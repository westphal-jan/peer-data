{"id": "1602.02666", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "8-Feb-2016", "title": "A Variational Analysis of Stochastic Gradient Algorithms", "abstract": "Stochastic Gradient Descent (SGD) is an important algorithm in machine learning. With constant learning rates, it is a stochastic process that, after an initial phase of convergence, generates samples from a stationary distribution. We show that SGD with constant rates can be effectively used as an approximate posterior inference algorithm for probabilistic modeling. Specifically, we show how to adjust the tuning parameters of SGD such as to match the resulting stationary distribution to the posterior. This analysis rests on interpreting SGD as a continuous-time stochastic process and then minimizing the Kullback-Leibler divergence between its stationary distribution and the target posterior. (This is in the spirit of variational inference.) In more detail, we model SGD as a multivariate Ornstein-Uhlenbeck process and then use properties of this process to derive the optimal parameters. This theoretical framework also connects SGD to modern scalable inference algorithms; we analyze the recently proposed stochastic gradient Fisher scoring under this perspective. We demonstrate that SGD with properly chosen constant rates gives a new way to optimize hyperparameters in probabilistic models. In the same paper, we describe the experimental parameters and their applicability using a series of simulations of simulated SGD. (We further discuss a few more articles.) We provide a simple statistical model with a common linear classification scheme.\n\n\n\nThe following simulations show that we can efficiently extract a latent distribution of SGD from a random distribution and find a set of covariates. (In a case of the H2N2S clustering between the three models in Fig. 1), we use a Gaussian distribution to estimate a latent distribution of the model's values. This approach is not required for the general rule of thumb and is not always accurate with statistical models. We consider two approaches that require the use of Gaussian Gaussian distributions in a single model. The first is based on a Gaussian distribution and a Gaussian distribution of the model's values. (In this context, we find that we can obtain a simple GASP model without requiring Gaussian Gaussian Gaussian Gaussian Gaussian Gaussian Gaussian Gaussian Gaussian Gaussian Gaussian Gaussian Gaussian Gaussian Gaussian Gaussian Gaussian Gaussian Gaussian Gaussian Gaussian Gaussian Gaussian Gaussian Gaussian Gaussian Gaussian Gaussian Gaussian Gaussian Gaussian Gaussian Gaussian Gaussian Gaussian Gaussian Gaussian Gaussian Gaussian Gaussian Gaussian Gaussian Gaussian Gaussian Ga", "histories": [["v1", "Mon, 8 Feb 2016 17:46:18 GMT  (1543kb,D)", "http://arxiv.org/abs/1602.02666v1", "8 pages, 3 figures"]], "COMMENTS": "8 pages, 3 figures", "reviews": [], "SUBJECTS": "stat.ML cs.LG", "authors": ["stephan mandt", "matthew d hoffman", "david m blei"], "accepted": true, "id": "1602.02666"}, "pdf": {"name": "1602.02666.pdf", "metadata": {"source": "META", "title": "A Variational Analysis of Stochastic Gradient Algorithms", "authors": ["Stephan Mandt", "Matthew D. Hoffman"], "emails": ["sm3976@columbia.edu", "mathoffm@adobe.com", "david.blei@columbia.edu"], "sections": [{"heading": "1. Introduction", "text": "Stochastic gradient descent (SGD) has become crucial to modern machine learning. SGD optimizes a function by following noisy gradients with a decreasing step size. The\nclassical result of Robbins and Monro (1951) is that this procedure provably reaches the optimum of the function (or local optimum, when it is nonconvex). Recent studies investigate the merits of adaptive step sizes (Duchi et al., 2011; Tieleman and Hinton, 2012), gradient or iterate averaging (Toulis et al.; De\u0301fossez and Bach, 2015), and constant step-sizes (Bach and Moulines, 2013; Flammarion and Bach, 2015). Stochastic gradient descent has enabled efficient optimization with massive data.\nRecently, stochastic gradients (SG) have also been used in the service of scalable Bayesian Markov Chain MonteCarlo (MCMC) methods, where the goal is to generate samples from a conditional distribution of latent variables given a data set. In Bayesian inference, we assume a probabilistic model p(\u03b8, x) with data x and hidden variables \u03b8; our goal is to approximate the posterior\np(\u03b8 | x) = exp{log p(\u03b8, x) \u2212 log p(x)}. (1)\nNew scalable MCMC algorithms\u2014such as SG Langevin dynamics (Welling and Teh, 2011), SG Hamiltonian Monte-Carlo (Chen et al., 2014), and SG Fisher scoring (Ahn et al., 2012)\u2014employ stochastic gradients of log p(\u03b8, x) to improve convergence and computation of existing sampling algorithms. Also see Ma et al. (2015) for a complete classification of these algorithms.\nThese methods all take precautions to sample from an asymptotically exact posterior. In contrast to this and specifically in the limit of large data, we will show how to effectively use the simplest stochastic gradient descent algorithm as a sensible approximate Bayesian inference method. Specifically, we consider SGD with a constant learning rate (constant SGD). Constant SGD first marches toward an optimum of the objective function and then bounces around its vicinity because of the sampling noise in the gradient. (In contrast, traditional SGD converges to the optimum by decreasing the learning rate.) Our analysis below rests on the idea that constant SGD can be inter-\nar X\niv :1\n60 2.\n02 66\n6v 1\n[ st\nat .M\nL ]\n8 F\neb 2\npreted as a stochastic process with a stationary distribution, one that is centered on the optimum and that has a certain covariance structure. The main idea is that we can use this stationary distribution to approximate a posterior.\nHere is how it works. The particular profile of the stationary distribution depends on the parameters of the algorithm\u2014the constant learning rate, the preconditioning matrix, and the minibatch size, all of which affect the noise and the gradients. Thus we can set log p(\u03b8, x) as the objective function and set the parameters of constant SGD such that its stationary distribution is close to the exact posterior (Eq. 1). Specifically, in the spirit of variational Bayes (Jordan et al., 1999b), we set those parameters to minimize the Kullback-Leibler (KL) divergence. With those settings, we can perform approximate inference by simply running constant SGD.\nIn more detail, we make the following contributions:\n\u2022 First, we develop a variational Bayesian view of stochastic gradient descent. Based on its interpretation as a continuous-time stochastic process\u2014specifically a multivariate Ornstein-Uhlenbeck (OU) process (Uhlenbeck and Ornstein, 1930; Gardiner et al., 1985)\u2014we compute stationary distributions for a large class of SGD algorithms, all of which converge to a Gaussian distribution with a non-trivial covariance matrix. The stationary distribution is parameterized by the learning rate, minibatch size, and preconditioning matrix.\nResults about the multivariate OU process enable us to compute the KL divergence between the stationary distribution and the posterior analytically. Minimizing the KL, we can relate the optimal step size or preconditioning matrix to the Hessian and noise covariances near the optimum. The resulting criteria strongly resemble AdaGrad (Duchi et al., 2011), RMSProp (Tieleman and Hinton, 2012), and classical Fisher scoring (Longford, 1987). We demonstrate how these different optimization methods compare, when used for approximate inference.\n\u2022 Then, we analyze scalable MCMC algorithms. Specifically, we use the stochastic process perspective to compute the stationary sampling distribution of stochastic gradient Fisher scoring (Ahn et al., 2012). The view from the multivariate OU process reveals a simple justification for this method: we show that the preconditioning matrix suggested in SGFS is indeed optimal. We also derive a criterion for the free noise parameter in SGFS such as to enhance numerical stability, and we show how the stationary distribution is modified when the preconditioner is approximated with a diagonal matrix (as is often done in practice for high-dimensional problems).\n\u2022 Finally, we show how using SGD with a constant learning rate confers an important practical advantage: it al-\nlows simultaneous inference of the posterior and optimization of meta-level parameters, such as hyperparameters in a Bayesian model. We demonstrate this technique on a Bayesian multinomial logistic regression model with normal priors.\nOur paper is organized as follows. In section 2 we review the continuous-time limit of SGD, showing that it can be interpreted as an OU process. In section 3 we present consequences of this perspective: the interpretation of SGD as variational Bayes and results around stochastic gradient Fisher Scoring (Ahn et al., 2012). In the empirical study (section 4), we show that our theoretical assumptions are satisfied for different models, and that we can use SGD to perform gradient-based hyperparameter optimization."}, {"heading": "2. Continuous-Time Limit Revisited", "text": "We first review the theoretical framework that we use throughout the paper. Our goal is to characterize the behavior of SGD when using a constant step size. To do this, we approximate SGD with a continuous-time stochastic process (Kushner and Yin, 2003; Ljung et al., 2012)."}, {"heading": "2.1. Problem setup", "text": "Consider loss functions of the following form: L(\u03b8) = 1N \u2211N n=1`n(\u03b8), g(\u03b8) \u2261 \u2207\u03b8L(\u03b8). (2)\nSuch loss functions are common in machine learning, where L(\u03b8) \u2261 L(\u03b8, x) is a loss function that depends on data x and parameters \u03b8. Each `n(\u03b8) \u2261 `(\u03b8, xn) is the contribution to the overall loss from a single observation. For example, when finding a maximum-a-posteriori estimate of a model, the contributions to the loss may be\n`n(\u03b8) = \u2212 log p(xn | \u03b8) + 1 N log p(\u03b8), (3)\nwhere p(xn | \u03b8) is the likelihood and p(\u03b8) is the prior. For simpler notation, we will suppress the dependence of the loss on the data.\nFrom this loss we construct stochastic gradients. Let S be a set of S random indices drawn uniformly at random from the set {1, . . . ,N}. This set indexes functions `n(\u03b8), and we call S a \u201cminibatch\u201d of size S . Based on the minibatch, we used the indexed functions to form a stochastic estimate of the loss and a stochastic gradient,\nL\u0302S (\u03b8) = 1S \u2211 n\u2208S `n(\u03b8), g\u0302S (\u03b8) = \u2207\u03b8L\u0302S (\u03b8). (4)\nIn expectation the stochastic gradient is the full gradient, i.e., g(\u03b8) = E[g\u0302S (\u03b8)]. We use this stochastic gradient in the SGD update\n\u03b8(t + 1) = \u03b8(t) \u2212 g\u0302S (\u03b8(t)). (5)\nEquations 4 and 5 define the discrete-time process that SGD simulates from. We will approximate it with a continuous-time process that is easier to analyze."}, {"heading": "2.2. SGD as a Ornstein-Uhlenbeck process", "text": "We now show how to approximate the discrete-time Eq. 5 with the continuous-time Ornstein-Uhlenbeck process (Uhlenbeck and Ornstein, 1930). This leads to the stochastic differential equation below in Eq. 11. To justify the approximation, we make four assumptions. We verify its accuracy in Section 4.\nAssumption 1. Observe that the stochastic gradient is a sum of S independent, uniformly sampled contributions. Invoking the central limit theorem, we assume that the gradient noise is Gaussian with variance \u221d 1/S :\ng\u0302S (\u03b8) \u2248 g(\u03b8) + 1\u221aS \u2206g(\u03b8), \u2206g(\u03b8) \u223c N(0,C(\u03b8)). (6)\nAssumption 2. We assume that the noise covariance is approximately constant. Further, we decompose the constant noise covariance into a product of two constant matrices:\nC = BB>. (7)\nThis assumption is justified when the iterates of SGD are confined to a small enough region around a local optimum of the loss that the noise covariance does not vary significantly in that region.\nAssumption 3. We now define \u2206\u03b8(t) = \u03b8(t + 1) \u2212 \u03b8(t) and combine Eqs. 5, 6, and 7 to rewrite the process as\n\u2206\u03b8(t) = \u2212 g(\u03b8(t)) + \u221a\nS B \u2206W, \u2206W \u223c N (0, I) . (8)\nThis is a discretization of the following continuous-time stochastic differential equation: 1\nd\u03b8(t) = \u2212g(\u03b8)dt + \u221a\nS B dW(t). (9)\nWe assume that this continuous-time limit is approximately justified and that we can neglect the discretization errors.\nAssumption 4. Finally, we assume that the stationary distribution of the iterates is constrained to a region where the loss is well approximated by a quadratic function,\nL(\u03b8) = 12 \u03b8>A\u03b8. (10)\n(Without loss of generality, we assume that a minimum of the loss is at \u03b8 = 0.) This assumption makes sense when the\n1We performed the conventional substitution rules when discretizing a continuous-time stochastic process. These substitution rules are \u2206\u03b8(t) \u2192 d\u03b8(t), \u2192 dt and \u2206W \u2192 dW, see e.g. (Gardiner et al., 1985).\nloss function is smooth and the stochastic process reaches a low-variance quasi-stationary distribution around a deep local minimum. The exit time of a stochastic process is typically exponential in the height of the barriers between minima, which can make local optima very stable even in the presence of noise (Kramers, 1940).\nSGD as an Ornstein-Uhlenbeck process. For what follows, define B /S = \u221a S B. The four assumptions above result in a specific kind of stochastic process, the multivari-\nate Ornstein-Uhlenbeck process (Uhlenbeck and Ornstein, 1930). It is\nd\u03b8(t) = \u2212A \u03b8(t)dt + B /S dW(t) (11)\nThis connection helps us analyze properties of SGD because the Ornstein-Uhlenbeck process has an analytic stationary distribution q(\u03b8) that is Gaussian. This distribution will be the core analytic tool of this paper:\nq(\u03b8) \u221d exp { \u2212 12\u03b8>\u03a3\u22121\u03b8 } . (12)\nThe covariance \u03a3 satisfies\n\u03a3A> + A\u03a3 = S BB >. (13)\nWithout explicitly solving this equation, we see that the resulting covariance is proportional to the learning rate and inversely proportional to the magnitude of A and minibatch size S . (More details are in the Appendix.) This characterizes the stationary distribution of running SGD with a constant step size."}, {"heading": "3. SGD as Approximate Inference", "text": "We discussed a continuous-time interpretation of SGD with a constant step size (constant SGD). We now discuss how to use constant SGD as an approximate inference algorithm. To repeat the set-up from the introduction, consider a probabilistic model p(\u03b8, x) with data x and hidden variables \u03b8; our goal is to approximate the posterior in Eq. 1.\nWe set the loss to be proportional to the negative log-joint distribution (Eqs. 2 and 3), which equals the posterior up to an additive constant. The classical goal of SGD is to minimize this loss, leading us to a maximum-a-posteriori point estimate of the parameters. This is how SGD is used in\nmany statistical models, including logistic regression, linear regression, matrix factorization, neural network classifiers, and regressors. In contrast, our goal here is to tune the parameters of SGD such that we approximate the posterior with its stationary distribution. Thus we use SGD as a posterior inference algorithm.\nFig. 1 shows an example. Here we illustrate two Bayesian posteriors\u2014from a linear regression problem (left) and a logistic regression problem (right)\u2014along with iterates from a constant SGD algorithm. In these figures, we set the parameters of the optimization to values that minimize the Kullback-Leibler (KL) divergence between the stationary distribution of the OU process and the posterior\u2014these results come from our theorems below. The top plots optimize both a preconditioning matrix and the step size; the middle plots optimize only the step size. (The middle plots are from a more efficient algorithm, but it is less accurate.) We can see that the stationary distribution of constant SGD can be made close to the exact posterior.\nFig. 1 also compares the empirical covariance of the iterates with the predicted covariance in terms of Eq. 13. The close match supports the assumptions of Sec. 2.\nWe will use this perspective in two ways. First, we develop optimal algorithmic conditions for constant SGD to best approximate the posterior, connecting to well-known results around adaptive learning rates and preconditioners. Second, we use it to analyze Stochastic Gradient Fisher Scoring (Ahn et al., 2012), both in its exact form and its more efficient approximate form."}, {"heading": "3.1. Constant stochastic gradient descent", "text": "First, we show how to tune constant SGD\u2019s parameters to minimize the KL divergence to the posterior; this is a type of variational inference (Jordan et al., 1999a). Based on this analysis, we derive three versions of constant SGD\u2014 one with a constant step size, one with a full preconditioning matrix, and one with a diagonal preconditioning matrix. Each one yields samples from an approximate posterior, and each trades of efficiency and accuracy in a different way. Finally, we show how to use these algorithms to learn hyperparameters.\nAssumption 4 from Sec. 2 says that the posterior is approximately Gaussian in the region that the stationary distribution focuses on,\nf (\u03b8) \u221d exp { \u2212N2 \u03b8>A\u03b8 } . (14)\n(The scalar N corrects the averaging in equation 2.) In setting the parameters of SGD, we minimize the KL divergence between the posterior f (\u03b8) and the stationary distribution q(\u03b8) (Eqs. 12 and 13) as a function of the learning rate and minibatch size S . We can optionally include a\npreconditioning matrix H, i.e. a matrix that premultiplies the stochastic gradient to modify its convergence behavior.\nHence, we minimize\n{ \u2217, S \u2217,H\u2217} = arg min ,S ,H KL(q(\u03b8) || f (\u03b8)). (15)\nThe distributions f (\u03b8) and q(\u03b8) are both Gaussians. Their means coincide, at the minimum of the loss, and so their KL divergence is\nKL(q|| f ) = Eq(\u03b8)[log f (\u03b8)] \u2212 Eq(\u03b8)[log q(\u03b8)] (16) = 12 ( NTr(A\u03a3) \u2212 log |NA| \u2212 log |\u03a3| \u2212 D) ,\nwhere | \u00b7 | is the determinant and D is the dimension of \u03b8. We suggest three variants of constant SGD that generate samples from an approximate posterior.\nTheorem 1 (constant SGD). Under assumptions [A1A4], the constant learning rate which minimizes KL divergence from the stationary distribution of SGD to the posterior is\n\u2217 = 2DSNTr(BB>) . (17)\nTo prove this claim, we face the problem that the covariance of the stationary distribution depends indirectly on through Eq. 13. Inspecting this equation reveals that \u03a30 \u2261 S \u03a3 is independent of S and . This simplifies the entropy term log |\u03a3| = D log( /S ) + log |\u03a30|. Since \u03a30 is constant, we can neglect it when minimizing KL divergence.\nWe also need to simplify the term Tr(A\u03a3), which still depends on and S through \u03a3. To do this, we again use Eq. 13, from which follows that Tr(A\u03a3) = 12 (Tr(A\u03a3) + Tr(\u03a3A>)) = 2S Tr(BB\n>). The KL divergence is therefore, up to constant terms,\nKL(q|| f ) c= N2S Tr(BB>) \u2212 D log( /S ) (18)\nMinimizing KL divergence over /S results in Eq. 17 for the optimal learning rate.\nTheorem 1 suggests that the learning rate should be chosen inversely proportional to the average of diagonal entries of the noise covariance. We can also precondition SGD with a diagonal matrix H. This gives more tuning parameters to better approximate the posterior. Under the same assumptions, we ask for the optimal diagonal preconditioner.\nTheorem 2 (preconditioned constant SGD). The constant preconditioning matrix for SGD that minimizes KL divergence from the sampling distribution to the posterior is\nH\u2217 = 2S N (BB >)\u22121 (19)\nTo prove this claim, we need the Ornstein-Uhlenbeck process which corresponds to preconditioned SGD. Preconditioning Eq. 11 with H results in\nd\u03b8(t) = \u2212HA \u03b8(t)dt + HB /S dW(t). (20)\nAll our results carry over after substituting A\u2190 HA, B\u2190 HB. Eq. 13, after the transformation and multiplication by H\u22121 from the left, becomes\nA\u03a3 + H\u22121\u03a3A>H = S BB >H (21)\nUsing the cyclic property of the trace, this implies that Tr(A\u03a3) = 12 (Tr(A\u03a3) + Tr(H \u22121A\u03a3H) = 2S Tr(BB >H). Hence up to constant terms, the KL divergence is\nKL(q|| f ) c= N2S Tr(BB>H) + 1 2 log ( S |H\u03a3\u22121H| ) (22)\n= N2S Tr(BB >H) + Tr log(H) + D2 log S \u2212 1 2 log |\u03a3|.\n(We used that log(det H) = Tr log H.) Taking derivatives with respect to the entries of H results in Eq. 19.\nIn high-dimensional applications where working with large dense matrices is impractical, the preconditioner may be constrained to be diagonal. The following corollary is a direct consequence of Eq. 22 when constraining the preconditioner to be diagonal:\nCorollary 1 The optimal diagonal preconditioning matrix for SGD that minimizes KL divergence is\nH\u2217kk = 2S\nNBB>kk . (23)\nWe showed that the optimal diagonal preconditioner is the inverse of the diagonal part of the noise matrix. Similar preconditioning matrices have been suggested earlier in optimal control theory based on very different arguments, see (Widrow and Stearns, 1985). Our result also relates to AdaGrad and its relatives (Duchi et al., 2011; Tieleman and Hinton, 2012), which also adjust the preconditioner based on the square root of the diagonal entries of the noise covariance. In the supplement we derive an optimal global learning rate for AdaGrad-style diagonal preconditioners.\nIn Sec. 4, we compare three versions of constant SGD for approximate posterior inference: one with a scalar step size, one with a diagonal preconditioner, and one with a dense preconditioner."}, {"heading": "3.2. Stochastic Gradient Fisher Scoring", "text": "We now investigate Stochastic Gradient Fisher Scoring (Ahn et al., 2012), a scalable Bayesian MCMC algorithm. We use the variational perspective to rederive the Fisher scoring update and identify it as optimal. We also\nanalyze the sampling distribution of the truncated algorithm, one with diagonal preconditioning (as it is used in practice), and quantify the bias that this induces.\nThe basic idea here is that the stochastic gradient is preconditioned and additional noise is added to the updates such that the algorithm approximately samples from the Bayesian posterior. More precisely, the update is\n\u03b8(t + 1) = \u03b8(t) \u2212 H g\u0302(\u03b8(t)) + \u221a HE W(t). (24)\nThe matrix H is a preconditioner and EW(t) is Gaussian noise; we control the preconditioner and the covariance EE> of the noise. Stochastic gradient Fisher scoring suggests a preconditioning matrix H that leads to samples from the posterior even if the learning rate is not asymptotically small. We show here that this preconditioner follows from our variational analysis.\nTheorem 3 (SGFS) Under assumptions A1-A4, the preconditioning matrix H in Eq. 24 that minimizes KL divergence between the stationary distribution of SGFS and the posterior is\nH\u2217 = 2N ( BB > + EE>)\u22121. (25)\nTo prove the claim, we go through the steps of section 2 to derive the corresponding Ornstein-Uhlenbeck process, d\u03b8(t) = \u2212HA\u03b8(t)dt + H [B + E] dW(t). For simplicity, we have set the minibatch size S to 1, hence B \u2261 \u221a B. In the appendix, we derive the following KL divergence between the posterior and the sampling distribution: KL(q||p) = \u2212N4 Tr(H(B B> +EE>))+ 1 2 log |T |+ 1 2 log |H|+ 1 2 log |NA|+ D 2 . We can now minimize this KL divergence over the parameters H and E. When E is given, minimizing over H gives Eq. 25 .\nThe solution given in Eq. 25 not only minimizes the KL divergence, but makes it 0, meaning that the stationary sampling distribution is the posterior. This solution corresponds to the suggested Fisher Scoring update in the idealized case when the sampling noise distribution is estimated perfectly (Ahn et al., 2012). Through this update, the algorithm thus generates posterior samples without decreasing the learning rate to zero. (This is in contrast to Stochastic Gradient Langevin Dynamics by Welling and Teh (2011).)\nIn practice, however, SGFS is often used with a diagonal approximation of the preconditioning matrix (Ahn et al., 2012; Ma et al., 2015). However, researchers have not explored how the stationary distribution is affected by this truncation, which makes the algorithm only approximately Bayesian. We can quantify its deviation from the exact posterior and we derive the optimal diagonal preconditioner.\nCorollary 2 (approximate SGFS). When approximating the Fisher scoring preconditioning matrix by a diagonal matrix or a scalar, respectively, then H\u2217kk = 2 N ( BB > kk +\nEE>kk) \u22121 and H\u2217scalar = 2D N ( \u2211 k[ BB>kk + EE > kk])\n\u22121, respectively.\nThis follows from the KL divergence in the proof of theorem 3.\nNote that we have not made any assumptions about the noise covariance E. To keep the algorithm stable, it may make sense to impose a maximum step size hmax, so that Hkk < hmax. We can satisfy Eq. 25 by choosing Hkk = hmax = 2N ( BB > kk + EE > kk) \u22121. Solving for E yields\nEkk = 2hmaxN \u2212 BB>kk. (26)\nHence, to keep the learning rates bounded in favor of stability, one can inject noise in dimensions where the variance of the gradient is too small. This guideline is opposite to the advice of choosing B proportional to E, as suggested by Ahn et al. (2012), but follows naturally from the variational analysis."}, {"heading": "3.3. Implications for hyperparameter optimization", "text": "One of the major benefits to the Bayesian approach is the ability to fit hyperparameters to data without expensive cross-validation runs by placing hyperpriors on those hyperparameters. Empirical Bayesian methods fit hyperparameters by finding the hyperparameters that maximize the marginal likelihood of the data, integrating out the main model parameters:\n\u03bb? = arg max\u03bb log p(y|x, \u03bb) = arg max\u03bb log \u222b \u03b8 p(y, \u03b8|x, \u03bb)d\u03b8.\nWhen this marginal log-likelihood is intractable, a common approach is to use variational expectation-maximization (VEM) (Bishop, 2006), which iteratively optimizes a variational lower bound on the marginal log-likelihood over \u03bb. If we approximate the posterior p(\u03b8|x, y, \u03bb) with some distribution q(\u03b8), then VEM tries to find a value for \u03bb that maximizes the expected log-joint probability Eq[log p(\u03b8, y|x, \u03bb)]. If we interpret the stationary distribution of SGD as a variational approximation to a model\u2019s posterior, then we can justify jointly optimizing parameters and hyperparameters as a VEM algorithm. This should avoid degenerate solutions, as long as we use the learning rates and preconditioners derived above. In the experimental section, we show that gradient-based hyperparameter learning is a cheap alternative to cross-validation in constant SGD."}, {"heading": "4. Experiments", "text": "We test our theoretical assumptions in section 4.1 and find good experimental evidence that they are correct. In this section, we compare against other approximate inference algorithms. In section 4.2 we show that constant SGD lets us optimize hyperparameters in a Bayesian model."}, {"heading": "4.1. Confirming the stationary distribution\u2019s covariance", "text": "In this section, we confirm empirically that the stationary distributions of SGD with KL-optimal constant learning rates are as predicted by the Ornstein-Uhlenbeck process.\nData. We first considered the following data sets. (1) The Wine Quality Data Set2, containing N = 4898 instances, 11 features, and one integer output variable (the wine rating). (2) A data set of Protein Tertiary Structure3, containing N = 45730 instances, 8 features and one output variable. (3) The Skin Segmentation Data Set4, containing N = 245057 instances, 3 features, and one binary output variable. We applied linear regression on data sets 1 and 2 and applied logistic regression on data set 3. We rescaled the feature to unit length and used a mini-batch of sizes S = 100, S = 100 and S = 10000, respectively. The quadratic regularizer was 1. The constant learning rate was adjusted according to Eq. 17.\nFig. 1 shows two-dimensional projections of samples from the posterior (blue) and the stationary distribution (cyan), where the directions were chosen two be the smallest and largest principal component of the posterior. Both distributions are approximately Gaussian and centered around the maximum of the posterior. To check our theoretical assumptions, we compared the covariance of the sampling distribution (yellow) against its predicted value based on the Ornstein-Uhlenbeck process (red), where very good agreement was found. Since the predicted covariance is based on approximating SGD as a multivariate OrnsteinUhlenbeck process, we conclude that our modeling assumptions are satisfied to a very good extent. The unprojected 11-dimensional covariances on wine data are also\n2P. Cortez, A. Cerdeira, F. Almeida, T. Matos and J. Reis, \u2019Wine Quality Data Set\u2019, UCI Machine Learning Repository.\n3Prashant Singh Rana, \u2019Protein Tertiary Structure Data Set\u2019, UCI Machine Learning Repository.\n4Rajen Bhatt, Abhinav Dhall, \u2019Skin Segmentation Dataset\u2019, UCI Machine Learning Repository.\ncompared in Fig. 2. The bottom row of Fig. 1 shows the sampling distributions of black box variational inference (BBVI) using the reparametrization trick (Kucukelbir et al., 2015). Our results show that the approximation to the posterior given by constant SGD is not worse than the approximation given by BBVI.\nWe also computed KL divergences between the posterior and stationary distributions of various algorithms: constant SGD with KL-optimal learning rates and preconditioners, Stochastic Gradient Langevin Dynamics, Stochastic Gradient Fisher Scoring (with and without diagonal approximation) and BBVI. For SG Fisher Scoring, we set the learning rate to \u2217 of Eq. 17, while for Langevin dynamics we chose the largest rate that yielded stable results ( = {10\u22123, 10\u22126, 10\u22125} for data sets 1, 2 and 3, respectively). We found that constant SGD can compete in approximating the posterior with the MCMC algorithms under consideration. This suggests that the most important factor is not the artificial noise involved in scalable MCMC, but rather the approximation of the preconditioning matrix."}, {"heading": "4.2. Optimizing hyperparameters", "text": "To test the hypothesis of Section 3.3, namely that constant SGD as a variational algorithm allows for gradient-based hyperparameter learning, we experimented with a Bayesian multinomial logistic (a.k.a. softmax) regression model with normal priors. The negative log-joint being optimized is\nL \u2261 \u2212 log p(y, \u03b8|x) = \u03bb2 \u2211 d,k \u03b8 2 dk \u2212 DK 2 log(\u03bb) + DK 2 log 2\u03c0\n+ \u2211 n log \u2211 k exp{ \u2211 d xnd\u03b8dk} \u2212 \u2211\nd xnd\u03b8dyn , (27)\nwhere n \u2208 {1, . . . ,N} indexes examples, d \u2208 {1, . . . ,D} indexes features and k \u2208 {1, . . . ,K} indexes classes. xn \u2208 RD is the feature vector for the nth example and yn \u2208 {1, . . . ,K} is the class for that example. Equation 27 has the degenerate maximizer \u03bb = \u221e, \u03b8 = 0, which has infinite posterior density which we hope to avoid in our approach.\nData. In all experiments, we applied this model to the MNIST dataset (60000 training examples, 10000 test examples, 784 features) and the cover type dataset (500000 training examples, 81012 testing examples, 54 features).\nFigure 3 shows the validation loss achieved by maximizing equation 27 over \u03b8 for various values of \u03bb, as well as the values of \u03bb selected by SGD and BBVI. The results suggest that this approach can be used as an inexpensive alternative to cross-validation or other VEM methods for hyperparameter selection."}, {"heading": "5. Related Work", "text": "Our paper relates to Bayesian inference and stochastic optimization.\nScalable MCMC. Recent work in Bayesian statistics focuses on making MCMC sampling algorithms scalable by using stochastic gradients. In particular, Welling and Teh (2011) developed stochastic gradient Langevin dynamics (SGLD). This algorithm samples from a Bayesian posterior by adding artificial noise to the stochastic gradient which, at long times, dominates the SGD noise. Also see Sato and Nakagawa (2014) for a detailed convergence analysis of the algorithm. Though elegant, one disadvantage of SGLD is that the learning rate must be decreased to achieve the correct sampling regime, and the algorithm can suffer from slow mixing times. Other research suggests improvements to this issue, using Hamiltonian Monte-Carlo (Chen et al., 2014) or thermostats (Ding et al., 2014). Ma et al. (2015) give a complete classification of possible stochastic gradient-based MCMC schemes.\nAbove, we analyzed properties of stochastic gradient Fisher scoring (Ahn et al., 2012). This algorithm speeds up mixing times in SGLD by preconditioning a gradient with the inverse sampling noise covariance. This allows constant learning rates, while maintaining long-run samples from the posterior. In contrast, we do not aim to sample exactly from the posterior. We describe how to tune the parameters of SGD such that its stationary distribution approximates the posterior.\nMaclaurin et al. (2015) also interpret SGD as a nonparametric variational inference scheme, but with different goals and in a different formalism. The paper proposes a way to track entropy changes in the implicit variational objective, based on estimates of the Hessian. As such, they mainly consider sampling distributions that are not stationary, whereas we focus on constant learning rates and distributions that have (approximately) converged. Note that\ntheir notion of hyperparameters does not refer to model parameters but to parameters of SGD.\nStochastic Optimization. Stochastic gradient descent is an active field (Zhang, 2004; Bottou, 1998). Many papers discuss constant step-size SGD. Bach and Moulines (2013); Flammarion and Bach (2015) discuss convergence rate of averaged gradients with constant step size, while De\u0301fossez and Bach (2015) analyze sampling distributions using quasi-martingale techniques. Toulis et al. (2014) calculate the asymptotic variance of SGD for the case of decreasing learning rates, assuming that the data is distributed according to the model. None of these papers use variational arguments.\nThe fact that optimal preconditioning (using a decreasing Robbins-Monro schedule) is achieved by choosing the inverse noise covariance was first shown in (Sakrison, 1965), but here we derive the same result based on different arguments and suggest a scalar prefactor. Note the optimal scalar learning rate of 2/Tr(BB>) can also be derived based on stability arguments, as was done in the context of least mean square filters (Widrow and Stearns, 1985).\nFinally, Chen et al. (2015a) also draw analogies between SGD and scalable MCMC. They suggest annealing the posterior over iterations to use scalable MCMC as a tool for global optimization. We follow the opposite idea and suggest to use constant SGD as an approximate sampler by choosing appropriate learning rate and preconditioners.\nStochastic differential equations. The idea of analyzing stochastic gradient descent with stochastic differential equations is well established in the stochastic approximation literature (Kushner and Yin, 2003; Ljung et al., 2012). Recent work focuses on dynamical aspects of the algorithm. Li et al. (2015) discuss several one-dimensional cases and momentum. Chen et al. (2015b) analyze stochastic gradient MCMC and studied their convergence properties using stochastic differential equations.\nOur work makes use of the same formalism but has a different focus. Instead of analyzing dynamical properties, we focus on stationary distributions. Further, our paper introduces the idea of minimizing KL divergence between multivariate sampling distributions and the posterior."}, {"heading": "6. Conclusions", "text": "We analyzed new optimization goals of stochastic gradient descent in the context of statistical machine learning. Instead of decreasing the learning rate to zero, we ask for optimal constant learning rates such that Kullback-Leibler divergence between the stationary distribution of SGD and the posterior is minimized. This goal leads to criteria for optimal learning rates, minibatch sizes and precondi-\ntioning matrices. To be able to compute stationary distributions and KL divergences, we approximated SGD in terms of a continuous-time stochastic process, leading to the Ornstein-Uhlenbeck process. We also presented a novel analysis of stochastic gradient Fisher scoring. Finally, we demonstrated that a simple SGD algorithm can compete with stochastic variational methods at empirical Bayesian hyperparameter optimization."}, {"heading": "A. Stationary Covariance", "text": "The Ornstein-Uhlenbeck process has an analytic solution in terms of the stochastic integral (Gardiner et al., 1985),\n\u03b8(t) = exp(\u2212At)\u03b8(0) + \u221a\nS \u222b t 0 exp[\u2212A(t \u2212 t\u2032)]BdW(t\u2032) (28)\nFollowing Gardiner\u2019s book we derive an algebraic relation for the stationary covariance of the multivariate OrnsteinUhlenbeck process. Define \u03a3 = E[\u03b8(t)\u03b8(t)>]. Using the formal solution for \u03b8(t) given in the main paper, we find\nA\u03a3 + \u03a3A> = S \u222b t \u2212\u221e A exp[\u2212A(t \u2212 t\u2032)]BB> exp[\u2212A>(t \u2212 t\u2032)]dt\u2032\n+ S \u222b t \u2212\u221e exp[\u2212A(t \u2212 t\u2032)]BB> exp[\u2212A>(t \u2212 t\u2032)]dt\u2032A>\n= S \u222b t \u2212\u221e d dt\u2032 ( exp[\u2212A(t \u2212 t\u2032)]BB> exp[\u2212A>(t \u2212 t\u2032)] ) = S BB >.\nWe used that the lower limit of the integral vanishes by the positivity of the eigenvalues of A."}, {"heading": "B. Stochastic Gradient Fisher Scoring", "text": "We start from the Ornstein-Uhlenbeck process\nd\u0398(t) = \u2212HA\u03b8(t)dt + H [B /S + E] dW(t) = \u2212A\u2032\u03b8(t)dt + B\u2032dW(t). (29)\nWe defined A\u2032 \u2261 HA and B\u2032 \u2261 H [B /S + E]. As derived in the paper, the variational bound is (up to a constant)\nKL c= N 2 Tr(A\u03a3) \u2212 log(|NA|). (30)\nTo evaluate it, the task is to remove the unknown covariance \u03a3 from the bound. To this end, as before, we use the identity for the stationary covariance A\u2032\u03a3 + \u03a3A\u2032> = B\u2032B\u2032>.\nThe criterion for the stationary covariance is equivalent to\nHA\u03a3 + \u03a3AH = HBB>H + HEE>H>\n\u21d4 A\u03a3 + H\u22121\u03a3AH = BB>H + EE>H \u21d2 Tr(A\u03a3) = 1\n2 Tr(H( BB> + EE>)) (31)\nWe can re-parametrize the covariance as \u03a3 = T H, such that T is now unknown. The KL divergence is therefore\nKL = \u2212N 2 Tr(A\u03a3) + log(|NA|)\n= N 4 Tr(H( BB> + EE>)) + 1 2 log |T |\n+ 1 2 log |H| + 1 2 log |NA| + D 2 , (32)\nwhich is the result we give in the main paper."}, {"heading": "C. Square root preconditioning", "text": "Finally, we analyze the case where we precondition with a matrix that is proportional to the square root of the diagonal entries of the noise covariance.\nWe define\nG = \u221a diag(BB>) (33)\nas the diagonal matrix that contains square roots of the diagonal elements of the noise covariance. We use an additional scalar learning rate .\nTheorem (taking square roots). Consider SGD preconditioned with G\u22121 as defined above. Under the previous assumptions, the constant learning rate which minimizes KL divergence between the stationary distribution of this process and the posterior is\n\u2217 = 2DSNTr(BB>G\u22121) . (34)\nFor the proof, we read off the appropriate KL divergence from the proof of Theorem 2 with G\u22121 \u2261 H:\nKL(q|| f ) c= N2S Tr(BB>G\u22121)\u2212Tr log(G) + D 2 log S \u2212 1 2 log |\u03a3| (35) Minimizing this KL divergence over the learning rate yields Eq. 34 ."}], "references": [{"title": "Bayesian posterior sampling via stochastic gradient fisher scoring", "author": ["S. Ahn", "A. Korattikara", "M. Welling"], "venue": "arXiv preprint arXiv:1206.6380.", "citeRegEx": "Ahn et al\\.,? 2012", "shortCiteRegEx": "Ahn et al\\.", "year": 2012}, {"title": "Non-strongly-convex smooth stochastic approximation with convergence rate o (1/n)", "author": ["F. Bach", "E. Moulines"], "venue": "Advances in Neural Information Processing Systems, pages 773\u2013781.", "citeRegEx": "Bach and Moulines,? 2013", "shortCiteRegEx": "Bach and Moulines", "year": 2013}, {"title": "Pattern Recognition and Machine Learning", "author": ["C. Bishop"], "venue": "Springer New York.", "citeRegEx": "Bishop,? 2006", "shortCiteRegEx": "Bishop", "year": 2006}, {"title": "Online learning and stochastic approximations", "author": ["L. Bottou"], "venue": "On-line learning in neural networks, 17(9):25.", "citeRegEx": "Bottou,? 1998", "shortCiteRegEx": "Bottou", "year": 1998}, {"title": "Bridging the gap between stochastic gradient mcmc and stochastic optimization", "author": ["C. Chen", "D. Carlson", "Z. Gan", "C. Li", "L. Carin"], "venue": "arXiv preprint arXiv:1512.07962.", "citeRegEx": "Chen et al\\.,? 2015a", "shortCiteRegEx": "Chen et al\\.", "year": 2015}, {"title": "On the convergence of stochastic gradient mcmc algorithms with high-order integrators", "author": ["C. Chen", "N. Ding", "L. Carin"], "venue": "Advances in Neural Information Processing Systems, pages 2269\u20132277.", "citeRegEx": "Chen et al\\.,? 2015b", "shortCiteRegEx": "Chen et al\\.", "year": 2015}, {"title": "Stochastic gradient hamiltonian monte carlo", "author": ["T. Chen", "E.B. Fox", "C. Guestrin"], "venue": "arXiv preprint arXiv:1402.4102.", "citeRegEx": "Chen et al\\.,? 2014", "shortCiteRegEx": "Chen et al\\.", "year": 2014}, {"title": "Averaged least-meansquares: Bias-variance trade-offs and optimal sampling distributions", "author": ["A. D\u00e9fossez", "F. Bach"], "venue": "Proceedings of the Eighteenth International Conference on Artificial Intelligence and Statistics, pages 205\u2013213.", "citeRegEx": "D\u00e9fossez and Bach,? 2015", "shortCiteRegEx": "D\u00e9fossez and Bach", "year": 2015}, {"title": "Bayesian sampling using stochastic gradient thermostats", "author": ["N. Ding", "Y. Fang", "R. Babbush", "C. Chen", "R.D. Skeel", "H. Neven"], "venue": "Advances in neural information processing systems, pages 3203\u20133211.", "citeRegEx": "Ding et al\\.,? 2014", "shortCiteRegEx": "Ding et al\\.", "year": 2014}, {"title": "Adaptive subgradient methods for online learning and stochastic optimization", "author": ["J. Duchi", "E. Hazan", "Y. Singer"], "venue": "The Journal of Machine Learning Research, 12:2121\u20132159.", "citeRegEx": "Duchi et al\\.,? 2011", "shortCiteRegEx": "Duchi et al\\.", "year": 2011}, {"title": "From averaging to acceleration, there is only a step-size", "author": ["N. Flammarion", "F. Bach"], "venue": "arXiv preprint arXiv:1504.01577.", "citeRegEx": "Flammarion and Bach,? 2015", "shortCiteRegEx": "Flammarion and Bach", "year": 2015}, {"title": "Handbook of stochastic methods, volume 4", "author": ["Gardiner", "C. W"], "venue": "Springer Berlin.", "citeRegEx": "Gardiner and W,? 1985", "shortCiteRegEx": "Gardiner and W", "year": 1985}, {"title": "Introduction to variational methods for graphical models", "author": ["M. Jordan", "Z. Ghahramani", "T. Jaakkola", "L. Saul"], "venue": "Machine Learning, 37:183\u2013233.", "citeRegEx": "Jordan et al\\.,? 1999a", "shortCiteRegEx": "Jordan et al\\.", "year": 1999}, {"title": "An introduction to variational methods for graphical models", "author": ["M.I. Jordan", "Z. Ghahramani", "T.S. Jaakkola", "L.K. Saul"], "venue": "Machine learning, 37(2):183\u2013233.", "citeRegEx": "Jordan et al\\.,? 1999b", "shortCiteRegEx": "Jordan et al\\.", "year": 1999}, {"title": "Brownian motion in a field of force and the diffusion model of chemical reactions", "author": ["H.A. Kramers"], "venue": "Physica, 7(4):284\u2013304.", "citeRegEx": "Kramers,? 1940", "shortCiteRegEx": "Kramers", "year": 1940}, {"title": "Automatic variational inference in stan", "author": ["A. Kucukelbir", "R. Ranganath", "A. Gelman", "D. Blei"], "venue": "Advances in Neural Information Processing Systems, pages 568\u2013576.", "citeRegEx": "Kucukelbir et al\\.,? 2015", "shortCiteRegEx": "Kucukelbir et al\\.", "year": 2015}, {"title": "Stochastic approximation and recursive algorithms and applications, volume 35", "author": ["H.J. Kushner", "G. Yin"], "venue": "Springer Science & Business Media.", "citeRegEx": "Kushner and Yin,? 2003", "shortCiteRegEx": "Kushner and Yin", "year": 2003}, {"title": "Dynamics of stochastic gradient algorithms. arXiv preprint arXiv:1511.06251", "author": ["Q. Li", "C Tai"], "venue": null, "citeRegEx": "Li and Tai,? \\Q2015\\E", "shortCiteRegEx": "Li and Tai", "year": 2015}, {"title": "Stochastic approximation and optimization of random systems, volume 17", "author": ["L. Ljung", "G.C. Pflug", "H. Walk"], "venue": "Birkh\u00e4user.", "citeRegEx": "Ljung et al\\.,? 2012", "shortCiteRegEx": "Ljung et al\\.", "year": 2012}, {"title": "A fast scoring algorithm for maximum likelihood estimation in unbalanced mixed models with nested random effects", "author": ["N.T. Longford"], "venue": "Biometrika, 74(4):817\u2013827.", "citeRegEx": "Longford,? 1987", "shortCiteRegEx": "Longford", "year": 1987}, {"title": "A complete recipe for stochastic gradient mcmc", "author": ["Ma", "Y.-A.", "T. Chen", "E.B. Fox"], "venue": "arXiv preprint arXiv:1506.04696.", "citeRegEx": "Ma et al\\.,? 2015", "shortCiteRegEx": "Ma et al\\.", "year": 2015}, {"title": "Early stopping is nonparametric variational inference", "author": ["D. Maclaurin", "D. Duvenaud", "R.P. Adams"], "venue": "arXiv preprint arXiv:1504.01344.", "citeRegEx": "Maclaurin et al\\.,? 2015", "shortCiteRegEx": "Maclaurin et al\\.", "year": 2015}, {"title": "A stochastic approximation method", "author": ["H. Robbins", "S. Monro"], "venue": "The annals of mathematical statistics, pages 400\u2013407.", "citeRegEx": "Robbins and Monro,? 1951", "shortCiteRegEx": "Robbins and Monro", "year": 1951}, {"title": "Efficient recursive estimation; application to estimating the parameters of a covariance function", "author": ["D.J. Sakrison"], "venue": "International Journal of Engineering Science, 3(4):461\u2013483.", "citeRegEx": "Sakrison,? 1965", "shortCiteRegEx": "Sakrison", "year": 1965}, {"title": "Approximation analysis of stochastic gradient langevin dynamics by using fokker-planck equation and ito process", "author": ["I. Sato", "H. Nakagawa"], "venue": "Proceedings of the 31st International Conference on Machine Learning (ICML-14), pages 982\u2013990.", "citeRegEx": "Sato and Nakagawa,? 2014", "shortCiteRegEx": "Sato and Nakagawa", "year": 2014}, {"title": "Lecture 6.5\u2014 RmsProp: Divide the Gradient by a Running Average of its Recent Magnitude. COURSERA: Neural Networks for Machine Learning", "author": ["T. Tieleman", "G. Hinton"], "venue": null, "citeRegEx": "Tieleman and Hinton,? \\Q2012\\E", "shortCiteRegEx": "Tieleman and Hinton", "year": 2012}, {"title": "Statistical analysis of stochastic gradient methods for generalized linear models", "author": ["P. Toulis", "E. Airoldi", "J. Rennie"], "venue": "Proceedings of the 31st International Conference on Machine Learning (ICML-14), pages 667\u2013675.", "citeRegEx": "Toulis et al\\.,? 2014", "shortCiteRegEx": "Toulis et al\\.", "year": 2014}, {"title": "On the theory of the brownian motion", "author": ["G.E. Uhlenbeck", "L.S. Ornstein"], "venue": "Physical review, 36(5):823.", "citeRegEx": "Uhlenbeck and Ornstein,? 1930", "shortCiteRegEx": "Uhlenbeck and Ornstein", "year": 1930}, {"title": "Bayesian learning via stochastic gradient langevin dynamics", "author": ["M. Welling", "Y.W. Teh"], "venue": "Proceedings of the 28th International Conference on Machine Learning (ICML-11), pages 681\u2013688.", "citeRegEx": "Welling and Teh,? 2011", "shortCiteRegEx": "Welling and Teh", "year": 2011}, {"title": "Adaptive signal processing", "author": ["B. Widrow", "S.D. Stearns"], "venue": "Englewood Cliffs, NJ, Prentice-Hall, Inc., 1985, 491 p., 1.", "citeRegEx": "Widrow and Stearns,? 1985", "shortCiteRegEx": "Widrow and Stearns", "year": 1985}, {"title": "Solving large scale linear prediction problems using stochastic gradient descent algorithms", "author": ["T. Zhang"], "venue": "Proceedings of the twenty-first international conference on Machine learning, page 116. ACM.", "citeRegEx": "Zhang,? 2004", "shortCiteRegEx": "Zhang", "year": 2004}], "referenceMentions": [{"referenceID": 9, "context": "Recent studies investigate the merits of adaptive step sizes (Duchi et al., 2011; Tieleman and Hinton, 2012), gradient or iterate averaging (Toulis et al.", "startOffset": 61, "endOffset": 108}, {"referenceID": 25, "context": "Recent studies investigate the merits of adaptive step sizes (Duchi et al., 2011; Tieleman and Hinton, 2012), gradient or iterate averaging (Toulis et al.", "startOffset": 61, "endOffset": 108}, {"referenceID": 7, "context": ", 2011; Tieleman and Hinton, 2012), gradient or iterate averaging (Toulis et al.; D\u00e9fossez and Bach, 2015), and constant step-sizes (Bach and Moulines, 2013; Flammarion and Bach, 2015).", "startOffset": 66, "endOffset": 106}, {"referenceID": 1, "context": "; D\u00e9fossez and Bach, 2015), and constant step-sizes (Bach and Moulines, 2013; Flammarion and Bach, 2015).", "startOffset": 52, "endOffset": 104}, {"referenceID": 10, "context": "; D\u00e9fossez and Bach, 2015), and constant step-sizes (Bach and Moulines, 2013; Flammarion and Bach, 2015).", "startOffset": 52, "endOffset": 104}, {"referenceID": 18, "context": "The classical result of Robbins and Monro (1951) is that this procedure provably reaches the optimum of the function (or local optimum, when it is nonconvex).", "startOffset": 24, "endOffset": 49}, {"referenceID": 28, "context": "New scalable MCMC algorithms\u2014such as SG Langevin dynamics (Welling and Teh, 2011), SG Hamiltonian Monte-Carlo (Chen et al.", "startOffset": 58, "endOffset": 81}, {"referenceID": 6, "context": "New scalable MCMC algorithms\u2014such as SG Langevin dynamics (Welling and Teh, 2011), SG Hamiltonian Monte-Carlo (Chen et al., 2014), and SG Fisher scoring (Ahn et al.", "startOffset": 110, "endOffset": 129}, {"referenceID": 0, "context": ", 2014), and SG Fisher scoring (Ahn et al., 2012)\u2014employ stochastic gradients of log p(\u03b8, x) to improve convergence and computation of existing sampling algorithms.", "startOffset": 31, "endOffset": 49}, {"referenceID": 0, "context": ", 2014), and SG Fisher scoring (Ahn et al., 2012)\u2014employ stochastic gradients of log p(\u03b8, x) to improve convergence and computation of existing sampling algorithms. Also see Ma et al. (2015) for a complete classification of these algorithms.", "startOffset": 32, "endOffset": 191}, {"referenceID": 13, "context": "Specifically, in the spirit of variational Bayes (Jordan et al., 1999b), we set those parameters to minimize the Kullback-Leibler (KL) divergence.", "startOffset": 49, "endOffset": 71}, {"referenceID": 27, "context": "Based on its interpretation as a continuous-time stochastic process\u2014specifically a multivariate Ornstein-Uhlenbeck (OU) process (Uhlenbeck and Ornstein, 1930; Gardiner et al., 1985)\u2014we compute stationary distributions for a large class of SGD algorithms, all of which converge to a Gaussian distribution with a non-trivial covariance matrix.", "startOffset": 128, "endOffset": 181}, {"referenceID": 9, "context": "The resulting criteria strongly resemble AdaGrad (Duchi et al., 2011), RMSProp (Tieleman and Hinton, 2012), and classical Fisher scoring (Longford, 1987).", "startOffset": 49, "endOffset": 69}, {"referenceID": 25, "context": ", 2011), RMSProp (Tieleman and Hinton, 2012), and classical Fisher scoring (Longford, 1987).", "startOffset": 17, "endOffset": 44}, {"referenceID": 19, "context": ", 2011), RMSProp (Tieleman and Hinton, 2012), and classical Fisher scoring (Longford, 1987).", "startOffset": 75, "endOffset": 91}, {"referenceID": 0, "context": "Specifically, we use the stochastic process perspective to compute the stationary sampling distribution of stochastic gradient Fisher scoring (Ahn et al., 2012).", "startOffset": 142, "endOffset": 160}, {"referenceID": 0, "context": "In section 3 we present consequences of this perspective: the interpretation of SGD as variational Bayes and results around stochastic gradient Fisher Scoring (Ahn et al., 2012).", "startOffset": 159, "endOffset": 177}, {"referenceID": 16, "context": "To do this, we approximate SGD with a continuous-time stochastic process (Kushner and Yin, 2003; Ljung et al., 2012).", "startOffset": 73, "endOffset": 116}, {"referenceID": 18, "context": "To do this, we approximate SGD with a continuous-time stochastic process (Kushner and Yin, 2003; Ljung et al., 2012).", "startOffset": 73, "endOffset": 116}, {"referenceID": 27, "context": "5 with the continuous-time Ornstein-Uhlenbeck process (Uhlenbeck and Ornstein, 1930).", "startOffset": 54, "endOffset": 84}, {"referenceID": 15, "context": "Rows: full-rank preconditioned constant SGD (top), constant SGD (middle), and BBVI (Kucukelbir et al., 2015) (bottom).", "startOffset": 83, "endOffset": 108}, {"referenceID": 14, "context": "The exit time of a stochastic process is typically exponential in the height of the barriers between minima, which can make local optima very stable even in the presence of noise (Kramers, 1940).", "startOffset": 179, "endOffset": 194}, {"referenceID": 28, "context": "8 SGLD (Welling and Teh, 2011) 2.", "startOffset": 7, "endOffset": 30}, {"referenceID": 0, "context": "5 SGFS-d (Ahn et al., 2012) 12.", "startOffset": 9, "endOffset": 27}, {"referenceID": 0, "context": "4 SGFS-f (Ahn et al., 2012) 0.", "startOffset": 9, "endOffset": 27}, {"referenceID": 15, "context": "3 BBVI (Kucukelbir et al., 2015) 44.", "startOffset": 7, "endOffset": 32}, {"referenceID": 27, "context": "ate Ornstein-Uhlenbeck process (Uhlenbeck and Ornstein, 1930).", "startOffset": 31, "endOffset": 61}, {"referenceID": 0, "context": "Second, we use it to analyze Stochastic Gradient Fisher Scoring (Ahn et al., 2012), both in its exact form and its more efficient approximate form.", "startOffset": 64, "endOffset": 82}, {"referenceID": 12, "context": "First, we show how to tune constant SGD\u2019s parameters to minimize the KL divergence to the posterior; this is a type of variational inference (Jordan et al., 1999a).", "startOffset": 141, "endOffset": 163}, {"referenceID": 29, "context": "Similar preconditioning matrices have been suggested earlier in optimal control theory based on very different arguments, see (Widrow and Stearns, 1985).", "startOffset": 126, "endOffset": 152}, {"referenceID": 9, "context": "Our result also relates to AdaGrad and its relatives (Duchi et al., 2011; Tieleman and Hinton, 2012), which also adjust the preconditioner based on the square root of the diagonal entries of the noise covariance.", "startOffset": 53, "endOffset": 100}, {"referenceID": 25, "context": "Our result also relates to AdaGrad and its relatives (Duchi et al., 2011; Tieleman and Hinton, 2012), which also adjust the preconditioner based on the square root of the diagonal entries of the noise covariance.", "startOffset": 53, "endOffset": 100}, {"referenceID": 0, "context": "We now investigate Stochastic Gradient Fisher Scoring (Ahn et al., 2012), a scalable Bayesian MCMC algorithm.", "startOffset": 54, "endOffset": 72}, {"referenceID": 0, "context": "This solution corresponds to the suggested Fisher Scoring update in the idealized case when the sampling noise distribution is estimated perfectly (Ahn et al., 2012).", "startOffset": 147, "endOffset": 165}, {"referenceID": 0, "context": "This solution corresponds to the suggested Fisher Scoring update in the idealized case when the sampling noise distribution is estimated perfectly (Ahn et al., 2012). Through this update, the algorithm thus generates posterior samples without decreasing the learning rate to zero. (This is in contrast to Stochastic Gradient Langevin Dynamics by Welling and Teh (2011).)", "startOffset": 148, "endOffset": 369}, {"referenceID": 0, "context": "In practice, however, SGFS is often used with a diagonal approximation of the preconditioning matrix (Ahn et al., 2012; Ma et al., 2015).", "startOffset": 101, "endOffset": 136}, {"referenceID": 20, "context": "In practice, however, SGFS is often used with a diagonal approximation of the preconditioning matrix (Ahn et al., 2012; Ma et al., 2015).", "startOffset": 101, "endOffset": 136}, {"referenceID": 0, "context": "This guideline is opposite to the advice of choosing B proportional to E, as suggested by Ahn et al. (2012), but follows naturally from the variational analysis.", "startOffset": 90, "endOffset": 108}, {"referenceID": 2, "context": "When this marginal log-likelihood is intractable, a common approach is to use variational expectation-maximization (VEM) (Bishop, 2006), which iteratively optimizes a variational lower bound on the marginal log-likelihood over \u03bb.", "startOffset": 121, "endOffset": 135}, {"referenceID": 15, "context": "1 shows the sampling distributions of black box variational inference (BBVI) using the reparametrization trick (Kucukelbir et al., 2015).", "startOffset": 111, "endOffset": 136}, {"referenceID": 6, "context": "Other research suggests improvements to this issue, using Hamiltonian Monte-Carlo (Chen et al., 2014) or thermostats (Ding et al.", "startOffset": 82, "endOffset": 101}, {"referenceID": 8, "context": ", 2014) or thermostats (Ding et al., 2014).", "startOffset": 23, "endOffset": 42}, {"referenceID": 22, "context": "In particular, Welling and Teh (2011) developed stochastic gradient Langevin dynamics (SGLD).", "startOffset": 15, "endOffset": 38}, {"referenceID": 19, "context": "Also see Sato and Nakagawa (2014) for a detailed convergence analysis of the algorithm.", "startOffset": 9, "endOffset": 34}, {"referenceID": 4, "context": "Other research suggests improvements to this issue, using Hamiltonian Monte-Carlo (Chen et al., 2014) or thermostats (Ding et al., 2014). Ma et al. (2015) give a complete classification of possible stochastic gradient-based MCMC schemes.", "startOffset": 83, "endOffset": 155}, {"referenceID": 0, "context": "Above, we analyzed properties of stochastic gradient Fisher scoring (Ahn et al., 2012).", "startOffset": 68, "endOffset": 86}, {"referenceID": 30, "context": "Stochastic gradient descent is an active field (Zhang, 2004; Bottou, 1998).", "startOffset": 47, "endOffset": 74}, {"referenceID": 3, "context": "Stochastic gradient descent is an active field (Zhang, 2004; Bottou, 1998).", "startOffset": 47, "endOffset": 74}, {"referenceID": 1, "context": "Bach and Moulines (2013); Flammarion and Bach (2015) discuss convergence rate of averaged gradients with constant step size, while D\u00e9fossez and Bach (2015) analyze sampling distributions using quasi-martingale techniques.", "startOffset": 0, "endOffset": 25}, {"referenceID": 1, "context": "Bach and Moulines (2013); Flammarion and Bach (2015) discuss convergence rate of averaged gradients with constant step size, while D\u00e9fossez and Bach (2015) analyze sampling distributions using quasi-martingale techniques.", "startOffset": 0, "endOffset": 53}, {"referenceID": 1, "context": "Bach and Moulines (2013); Flammarion and Bach (2015) discuss convergence rate of averaged gradients with constant step size, while D\u00e9fossez and Bach (2015) analyze sampling distributions using quasi-martingale techniques.", "startOffset": 0, "endOffset": 156}, {"referenceID": 1, "context": "Bach and Moulines (2013); Flammarion and Bach (2015) discuss convergence rate of averaged gradients with constant step size, while D\u00e9fossez and Bach (2015) analyze sampling distributions using quasi-martingale techniques. Toulis et al. (2014) calculate the asymptotic variance of SGD for the case of decreasing learning rates, assuming that the data is distributed according to the model.", "startOffset": 0, "endOffset": 243}, {"referenceID": 23, "context": "The fact that optimal preconditioning (using a decreasing Robbins-Monro schedule) is achieved by choosing the inverse noise covariance was first shown in (Sakrison, 1965), but here we derive the same result based on different arguments and suggest a scalar prefactor.", "startOffset": 154, "endOffset": 170}, {"referenceID": 29, "context": "Note the optimal scalar learning rate of 2/Tr(BB>) can also be derived based on stability arguments, as was done in the context of least mean square filters (Widrow and Stearns, 1985).", "startOffset": 157, "endOffset": 183}, {"referenceID": 16, "context": "The idea of analyzing stochastic gradient descent with stochastic differential equations is well established in the stochastic approximation literature (Kushner and Yin, 2003; Ljung et al., 2012).", "startOffset": 152, "endOffset": 195}, {"referenceID": 18, "context": "The idea of analyzing stochastic gradient descent with stochastic differential equations is well established in the stochastic approximation literature (Kushner and Yin, 2003; Ljung et al., 2012).", "startOffset": 152, "endOffset": 195}, {"referenceID": 4, "context": "Finally, Chen et al. (2015a) also draw analogies between SGD and scalable MCMC.", "startOffset": 9, "endOffset": 29}, {"referenceID": 4, "context": "Finally, Chen et al. (2015a) also draw analogies between SGD and scalable MCMC. They suggest annealing the posterior over iterations to use scalable MCMC as a tool for global optimization. We follow the opposite idea and suggest to use constant SGD as an approximate sampler by choosing appropriate learning rate and preconditioners. Stochastic differential equations. The idea of analyzing stochastic gradient descent with stochastic differential equations is well established in the stochastic approximation literature (Kushner and Yin, 2003; Ljung et al., 2012). Recent work focuses on dynamical aspects of the algorithm. Li et al. (2015) discuss several one-dimensional cases and momentum.", "startOffset": 9, "endOffset": 642}, {"referenceID": 4, "context": "Finally, Chen et al. (2015a) also draw analogies between SGD and scalable MCMC. They suggest annealing the posterior over iterations to use scalable MCMC as a tool for global optimization. We follow the opposite idea and suggest to use constant SGD as an approximate sampler by choosing appropriate learning rate and preconditioners. Stochastic differential equations. The idea of analyzing stochastic gradient descent with stochastic differential equations is well established in the stochastic approximation literature (Kushner and Yin, 2003; Ljung et al., 2012). Recent work focuses on dynamical aspects of the algorithm. Li et al. (2015) discuss several one-dimensional cases and momentum. Chen et al. (2015b) analyze stochastic gradient MCMC and studied their convergence properties using stochastic differential equations.", "startOffset": 9, "endOffset": 714}], "year": 2016, "abstractText": "Stochastic Gradient Descent (SGD) is an important algorithm in machine learning. With constant learning rates, it is a stochastic process that, after an initial phase of convergence, generates samples from a stationary distribution. We show that SGD with constant rates can be effectively used as an approximate posterior inference algorithm for probabilistic modeling. Specifically, we show how to adjust the tuning parameters of SGD such as to match the resulting stationary distribution to the posterior. This analysis rests on interpreting SGD as a continuoustime stochastic process and then minimizing the Kullback-Leibler divergence between its stationary distribution and the target posterior. (This is in the spirit of variational inference.) In more detail, we model SGD as a multivariate Ornstein-Uhlenbeck process and then use properties of this process to derive the optimal parameters. This theoretical framework also connects SGD to modern scalable inference algorithms; we analyze the recently proposed stochastic gradient Fisher scoring under this perspective. We demonstrate that SGD with properly chosen constant rates gives a new way to optimize hyperparameters in probabilistic models.", "creator": "LaTeX with hyperref package"}}}