{"id": "1602.03264", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "10-Feb-2016", "title": "A Theory of Generative ConvNet", "abstract": "The convolutional neural network (ConvNet or CNN) is a powerful discriminative learning machine. In this paper, we show that a generative random field model that we call generative ConvNet can be derived from the discriminative ConvNet. The probability distribution of the generative ConvNet model is in the form of exponential tilting of a reference distribution. Assuming re-lu non-linearity and Gaussian white noise reference distribution, we show that the generative ConvNet model contains a representational structure with multiple layers of binary activation variables. The model is non-Gaussian, or more precisely, piecewise Gaussian, where each piece is determined by an instantiation of the binary activation variables that reconstruct the mean of the Gaussian piece. The Langevin dynamics for synthesis is driven by the reconstruction error, and the corresponding gradient descent dynamics converges to a local energy minimum that is auto-encoding. As for learning, we show that the contrastive divergence learning tends to reconstruct the observed images. Finally, we show that the maximum likelihood learning algorithm can generate realistic natural images.\n\n\n\nThe neural network\nConvNet:\nConvNet:\nConvNet:\nConvNet:\nConvNet:\nconvNet:\nConvNet:\nConvNet:\nConvNet:\nConvNet:\nConvNet:\nConvNet:\nConvNet:\nConvNet:\nConvNet:\nConvNet:\nConvNet:\nConvNet:\nConvNet:\nConvNet:\nConvNet:\nConvNet:\nConvNet:\nConvNet:\nConvNet:\nConvNet:\nConvNet:\nConvNet:\nConvNet:\nConvNet:\nConvNet:\nConvNet:\nConvNet:\nConvNet:\nConvNet:\nConvNet:\nConvNet:\nConvNet:\nConvNet:\nConvNet:\nConvNet:\nConvNet:\nConvNet:\nConvNet:\nConvNet:\nConvNet:\nConvNet:\nConvNet:\nConvNet:\nConvNet:\nConvNet:\nConvNet:\nConvNet:\nConvNet:\nConvNet:\nConvNet:\nCon", "histories": [["v1", "Wed, 10 Feb 2016 04:46:45 GMT  (5030kb,D)", "http://arxiv.org/abs/1602.03264v1", null], ["v2", "Sun, 29 May 2016 05:52:10 GMT  (8236kb,D)", "http://arxiv.org/abs/1602.03264v2", null], ["v3", "Tue, 31 May 2016 06:02:19 GMT  (8236kb,D)", "http://arxiv.org/abs/1602.03264v3", null]], "reviews": [], "SUBJECTS": "stat.ML cs.LG", "authors": ["jianwen xie", "yang lu", "song-chun zhu", "ying nian wu"], "accepted": true, "id": "1602.03264"}, "pdf": {"name": "1602.03264.pdf", "metadata": {"source": "CRF", "title": "A Theory of Generative ConvNet", "authors": ["Jianwen Xie", "Yang Lu", "Song-Chun Zhu", "Ying Nian Wu"], "emails": [], "sections": [{"heading": "1 Introduction", "text": "The convolutional neural network (ConvNet or CNN) (LeCun et al., 1998; Krizhevsky et al., 2012) has proven to be a tremendously successful discriminative or predictive learning machine. Can the discriminative ConvNet be turned into a generative model and an unsupervised learning machine? It would be highly desirable if this can be achieved because generative models and unsupervised learning can be very useful when the training datasets are small or the labeled data are scarce. It would also be extremely satisfying from a conceptual point of view if both discriminative classifier and generative model, and both supervised learning and unsupervised learning, can be treated within a unified framework for ConvNet.\nIn this conceptual paper, we show that a generative random field model, which we call generative ConvNet, can be derived from the commonly used discriminative ConvNet. The probability distribution of the generative ConvNet model is in the form of an exponential tilting of a reference distribution, and the exponential tilting is defined by\n\u2217Equal contributions.\nConvNet that involves multiple layers of liner filtering and non-linear transformation. The generative ConvNet model can be viewed as a hierarchical version of the FRAME (Filters, Random field, And Maximum Entropy) model (Zhu et al., 1997), as well as the Product of Experts (PoE) (Hinton, 2002) and Field of Experts (FoE) (Roth & Black, 2005) models.\nBeing an exponential family model, the generative ConvNet may appear dull and opaque. The main purpose of this article is to show that the contrary is true. Assuming Gaussian white noise reference distribution and re-lu nonlinearity, we discover that the generative ConvNet contains a surprisingly explicit and exquisite representational structure. Specifically, it contains multiple layers of binary activation variables that indicate the presence or absence of the patterns modeled by the multiple layers of filters of the ConvNet. The generative ConvNet model is non-Gaussian, or more precisely, piecewise Gaussian, where each piece is determined by an instantiation of the binary activation variables. These binary variables are computed by a bottom-up process by the multiple layers of filters, and they reconstruct the mean of the Gaussian piece by a top-down process, where the multiple layers of filters serve as multiple layers of basis functions for image reconstruction.\nThe Langevin dynamics can be employed to synthesize images by sampling from the generative ConvNet. Interestingly, the dynamics is driven by the reconstruction error, i.e., the difference between the current image and the reconstruction by the binary activation variables mentioned above. Thus image synthesis and image reconstruction are connected.\nThe deterministic gradient descent counterpart of the Langevin dynamics was employed by Zhu & Mumford (1998) for exploring the local energy minima of the FRAME model. They called it the Gibbs Reaction And Diffusion Equation (GRADE). It defines an attractor dynamics that converges to a local energy minimum. The local energy minima are the means of the Gaussian pieces mentioned above, and they are auto-encoding. Thus the generative ConvNet hides an auto-encoder at its energy minima. This observation establishes a connection between the Hopfield network (Hopfield, 1982) and the auto-encoder, and may realize a hierarchically distributed form of memory.\nThe model can be learned by maximum likelihood or con-\nar X\niv :1\n60 2.\n03 26\n4v 1\n[ st\nat .M\nL ]\n1 0\nFe b\n20 16\ntrastive divergence (Hinton, 2002). For generative ConvNet, we show that the contrastive divergence tends to reconstruct the training images by the above mentioned auto-encoder.\nFinally, we show that the maximum likelihood learning algorithm can generate a wide variety of realistic natural image patterns, thus validating the generative capacity of the generative ConvNet."}, {"heading": "2 Contributions and Related Work", "text": "The following are the discoveries that we have made in this paper about the generative ConvNet. (1) It can be derived from the discriminative ConvNet. (2) It contains an explicit representational structure. (3) It is piecewise Gaussian. (4) It can be sampled by a reconstruction driven algorithm. (5) Its local energy minima are auto-encoding. (6) The contrastive divergence learning of it tends to reconstruct the observed images. (7) It is capable of generating realistic image patterns.\nThe model in the form of exponential tilting of a reference distribution where the exponential tilting is defined by ConvNet was first proposed by Dai et al. (2015). They learned the model by a non-parametric importance sampling scheme. Lu et al. (2016) proposed to learn the FRAME models based on pre-learned filters of existing ConvNets. They did not learn the models from scratch. The hierarchical energy-based models (LeCun et al., 2006) were studied by the pioneering work of Hinton et al. (2006a) and Ngiam et al. (2011). However, their models do not correspond directly to modern ConvNet.\nCompared to the above mentioned papers, we would like to emphasize the conceptual novelty of our work. Starting from a prototype model and then unfolding it, our work reveals a curious representational structure contained in the model that involves multiple layers of activation variables. Such a representational structure is unexpected for the exponential family models, and was not studied by the papers cited above.\nA main motivation for this paper is to reconcile the FRAME model (Zhu et al., 1997), where the Gabor wavelets play the role of bottom-up filters, and the Olshausen-Field model (Olshausen & Field, 1997), where the wavelets play the role of top-down basis functions, and unfold these models into a hierarchical sparse compositional model (Hong et al., 2013; Xie et al., 2014). The generative ConvNet may help solve this problem.\nThe representational structure in the generative ConvNet is similar to but subtly different from the deconvolution network of Zeiler et al. (2011). The top-down process of the generative ConvNet is controlled by multiple layers of binary activation variables computed by the bottom-up process. The generative ConvNet can synthesize new images in addition to reconstructing observed images.\nCompared to hierarchical models with explicit binary latent variables such as those based on the Boltzmann machine (Hinton et al., 2006b; Salakhutdinov & Hinton, 2009; Lee et al., 2009), the generative ConvNet is directly derived from the discriminative ConvNet. Our work seems to suggest that in searching for generative models and unsupervised learn-\ning machines, we need to look no further beyond the ConvNet.\nThere are two major classes of generative models. One consists of exponential family models such as FRAME model, and the other consists of latent variable models such as Olshausen-Field model. While the former class usually cannot reconstruct the observed data, the latter class typically needs to negotiate with the intractable inference. The generative ConvNet has both explicit bottom-up pass for computing binary variables and explicit top-down pass for reconstructing the image. It strikes a middle ground between the two classes of models.\nOne way to get around the intractable inference problem mentioned above is to use the wake-sleep algorithm (Hinton et al., 1995) or the variational auto-encoder (Kingma & Welling, 2014; Rezende et al., 2014; Mnih & Gregor, 2014). The parameters in the top-down generation model and the bottom-up recognition model of a variational auto-encoder are completely separate from each other. In generative ConvNet, there is a common set of parameters. More importantly, the generative ConvNet actually contains an autoencoder at the local minima of its energy landscape, and the encoding and decoding of this auto-encoder share the same set of parameters."}, {"heading": "3 Generative ConvNet", "text": "To fix notation, let I(x) be an image defined on the square (or rectangular) image domain D, where x = (x1, x2) indexes the coordinates of pixels. We can treat I(x) as a twodimensional function defined on D. We can also treat I as a vector if we fix an ordering for the pixels.\nFor a filter F , let F \u2217I denote the filtered image or feature map, and let [F \u2217 I](x) denote the filter response or feature at position x.\nA ConvNet is a composition of multiple layers of linear filtering and element-wise non-linear transformation as expressed by the following recursive formula:\n[F (l) k \u2217I](x) = h Nl\u22121\u2211 i=1 \u2211 y\u2208Sl w (l,k) i,y [F (l\u22121) i \u2217 I](x+ y) + bl,k  (1) where l \u2208 {1, 2, ...,L} indexes the layer. {F (l)k , k = 1, ..., Nl} are the filters at layer l, and {F (l\u22121)i , i = 1, ..., Nl\u22121} are the filters at layer l \u2212 1. k and i are used to index filters at layers l and l \u2212 1 respectively, and Nl and Nl\u22121 are the numbers of filters at layers l and l \u2212 1 respectively. The filters are locally supported, so the range of y is within a local support Sl (such as a 7\u00d77 image patch). At the bottom layer, [F (0)k \u2217 I](x) = Ik(x), where k \u2208 {R,G,B} indexes the three color channels. Sub-sampling may be implemented so that in [F (l)k \u2217 I](x), x \u2208 Dl \u2282 D. For notational simplicity, we do not make local max pooling explicit in (1).\nWe take h(r) = max(r, 0), the rectified linear unit (re-lu), that is commonly adopted in modern ConvNet (Krizhevsky et al., 2012). This crisp piecewise linear transformation is the\nroot of the binary activation variables and piecewise Gaussian form of the model. But some results in this paper can be extended to general non-linearity.\nLet (F (L)k ) be the top layer filters. The filtered images are usually 1 \u00d7 1 due to repeated sub-sampling. Suppose there are C categories. For category c \u2208 {1, ..., C}, the scoring function for classification is\nfc(I;w) = NL\u2211 k=1 wc,k[F (L) k \u2217 I], (2)\nwhere wc,k are the category-specific weight parameters for classification.\nDefinition 1 Discriminative ConvNet: We define the following conditional distribution as the discriminative ConvNet:\np(c|I;w) = exp[fc(I;w) + bc]\u2211C c=1 exp [fc(I;w) + bc] . (3)\nwhere bc is the bias term, and w collects all the weight and bias parameters at all the layers.\nThe discriminative ConvNet is a multinomial logistic regression (or soft-max) that is commonly used for classification (LeCun et al., 1998; Krizhevsky et al., 2012).\nDefinition 2 Generative ConvNet (fully connected version): We define the following random field model as the fully connected version of the generative ConvNet:\np(I|c;w) = pc(I;w) = 1\nZc(w) exp[fc(I;w)]q(I), (4)\nwhere q(I) is a reference distribution or the null model, assumed to be Gaussian white noise in this paper. Z(w) = Eq{exp[fc(I;w)]} is the normalizing constant.\nIn (4), pc(I;w) is obtained by the exponential tilting of q, and is the conditional distribution of image given category, p(I|c, w). The model was first proposed by Dai et al. (2015).\nProposition 1 Generative and discriminative ConvNets can be derived from each other:\n(a) Let \u03c1c be the prior probability of category c, if p(I|c;w) = pc(I;w) is defined according to model (4), then p(c|I;w) is given by model (3), with bc = log \u03c1c \u2212 logZc(w) + constant.\n(b) Suppose a base category c = 1 is generated by q(I), and suppose we fix the scoring function and the bias term of the base category f1(I;w) = 0, and b1 = 0. If p(c|I;w) is given by model (3), then p(I|c;w) = pc(I;w) is of the form of model (4), with bc = log \u03c1c \u2212 log \u03c11 + logZc(w).\nProposition 1 can be proved by a simple exercise of the Bayes rule. Result (a) has already been explained in Dai et al. (2015). Result (b) is stronger and is new. First, it is entirely reasonable to include Gaussian white noise images as a base category and demand the discriminative ConvNet (3) not to misclassify the Gaussian white noise as an object category. It is also reasonable to fix the scoring function and bias term of this base category at 0 in training for the sake\nof identifiability. In fact, in the binary (two-category) logistic regression, the scoring function and the bias term for the negative category are always fixed at 0. Then\np(c|I;w) p(c = 1|I;w) = exp[fc(I;w) + bc]. (5)\nMeanwhile\np(c|I;w) p(c = 1|I;w) = p(c, I|w) p(c = 1, I|w) = \u03c1cpc(I;w) \u03c11q(I) , (6)\nbecause p(c, I|w) = p(c|I;w)P (I;w), where the marginal distribution P (I;w) = \u2211C c=1 \u03c1cp(I|c;w) is the mixture of all the categories. Thus pc(I;w) is of the form of model (4). As to learning, we may use the discriminative loglikelihood based on log p(c|I;w), or we may use the generative log-likelihood based on log p(c, I|w) = log pc(I;w) + log \u03c1c. Because log p(c, I|w) = log p(c|I;w) + logP (I;w), the discriminative log-likelihood log p(c|I;w) is without the marginal log-likelihood logP (I;w), resulting in the loss of statistical efficiency.\nIf we only observe unlabeled data {Im,m = 1, ...,M}, we may still use the exponential tilting form to model and learn from them. A possible model is to learn filters at a certain convolutional layer L \u2208 {1, ...,L} of a ConvNet.\nDefinition 3 Generative ConvNet (convolutional version or FRAME version): we define the following Markov random field model as the convolutional version or the FRAME version of generative ConvNet:\np(I;w) = 1\nZ(w) exp [ K\u2211 k=1 \u2211 x\u2208DL [F (L) k \u2217 I](x) ] q(I), (7)\nwhere w consists of all the weight and bias terms that define the filters (F (L)k , k = 1, ...,K = NL), and q is the Gaussian white noise model.\nModel (7) corresponds to the exponential tilting model (4) with scoring function\nf(I;w) = K\u2211 k=1 \u2211 x\u2208DL [F (L) k \u2217 I](x). (8)\nEssentially the above model treats the images {Im} as coming from a single meta-category, which is to be discriminated from the base category q by the filters (F (L)k ) to be learned from the data. However, it may be too easy to discriminate {Im} from q, so that we cannot learn anything meaningful by the discriminative log-likelihood. In this case, we can learn w based on the generative log-likelihood L(w) =\u2211M m=1 log p(Im;w)/M , with p(I;w) defined by (7). The learning of filters {F (L)k } by the generative log-likelihood is considered to be unsupervised because the observed images are unlabeled.\nFor the rest of the paper, we shall focus on the model (7), but all the results can be easily extended to model (4)."}, {"heading": "4 Hierarchical FRAME Model", "text": "Model (7) is a Markov random field model (Besag, 1974), where the clique functions are [F (L)k \u2217 I](x),\u2200k, x. According to the Hammersley-Clifford theorem (Besag, 1974), a Markov random filed model can be written as\np(I) = 1\nZ exp [\u2211 C \u03bbC(I(C)) ] , (9)\nwhere C \u2282 D are the cliques, and each clique C consists of pixels that are neighbors of each other according to a pre-defined neighborhood system. I(C) are the intensities of pixels in clique C, and \u03bbC is the potential function. The challenge in developing a Markov random field model is to specify the clique functions \u03bbC and estimate them from the data. Model (7) solves this problem by assuming \u03bbC(I(C)) = [F (L)k \u2217 I](x) using the ConvNet filters. The Gaussian white noise q(I) contributes to cliques that consist of single pixels.\nAt the first glance, defining clique functions by ConvNet filters may appear to be arbitrary and ad hoc, but it is actually based on a rich tradition in generative modeling. The first model in the literature that represents the clique functions by non-linear transformations of linear filter responses is the FRAME (Filters, Random field, And Maximum Entropy) model (Zhu et al., 1997). The generative ConvNet (7) can be considered a hierarchical FRAME model, with alternating layers of linear filtering and non-linearity.\nMore importantly, the recursive form of equation (1) has an interesting justification by generative modeling. Based on filters {F (l\u22121)i ,\u2200i} at layer l \u2212 1, each filter F (l) k at layer l corresponds to a non-stationary FRAME model (Xie et al., 2014; Lu et al., 2016) of an image patch defined on the support of the filter, Sl, and centered at x:\np (l) k (I;w, x) =\n1\nZ (l) k (w, x)\nexp Nl\u22121\u2211 i=1 \u2211 y\u2208Sl w (l,k) i,y [F (l\u22121) i \u2217 I](x+ y)  q(I), (10)\nwhere (w(l,k)i,y ,\u2200i, y) are the parameters of the above exponential family model. The model is also a generative ConvNet model, and it can generate vivid object patterns (Lu et al., 2016). The bias term bl,k and the re-lu non-linearity h() in equation (1) can be justified by a mixture model P\n(l) k (I;w, x) = \u03b1p (l) k (I;w, x) + (1 \u2212 \u03b1)q(I), which is a mixture of presence and absence of the object pattern modeled by the non-stationary FRAME model (10). Writing P\n(l) k (I;w, x) = exp [ f (l) k (I;w, x) ] q(I) gives rise to softmax non-linearity log(1 + er) that can be approximated by re-lu max(0, r), and the bias term bl,k that is determined by \u03b1 andZ(l)k (w, x). Finally, taking the product of P (l) k (I;w, x) over k and x gives rise to a Product of Experts (PoE) model (Hinton, 2002), which is the generative ConvNet (7) using filters at layer l. See Lu et al. (2016) for details."}, {"heading": "5 A Prototype Model", "text": "We shall explain the key properties of the generative ConvNet by the simplest prototype model, which makes crystal clear most of the key elements of the generative ConvNet model (7). A similar model was studied by Xie et al. (2015). The generative ConvNet can be obtained from the prototype model by unfolding the latter both convolutionally and hierarchically, but with much more involved notation that is in danger of obscuring the key ideas. Hence it is helpful to start from the prototype model.\nIn our prototype model, we assume that the image domain D is small (e.g., 10 \u00d7 10). Suppose we want to learn a dictionary of filters or basis functions from a set of observed image patches {Im,m = 1, ...,M} defined on D. We denote these filters or basis functions by (wk, k = 1, ...,K), where each wk itself is an image patch defined on D. Let \u3008I,wk\u3009 = \u2211 x\u2208Dwk(x)I(x) be the inner product between image patches I and wk. It is also the response of I to the linear filter wk.\nDefinition 4 Prototype model: We define the following random field model as the prototype model:\np(I;w) = 1\nZ(w) exp [ K\u2211 k=1 h(\u3008I,wk\u3009+ bk) ] q(I), (11)\nwhere bk is the bias term, w = (wk, bk, k = 1, ...,K), and h(r) = max(r, 0). q(I) is the Gaussian white noise model,\nq(I) = 1\n(2\u03c0\u03c32)|D|/2 exp\n[ \u2212 1\n2\u03c32 ||I||2\n] , (12)\nwhere |D| counts the number of pixels in the domain D.\nThe following are our findings about the prototype model. (1) Piecewise Gaussian and binary activation variables: The prototype model (11) is a piecewise Gaussian distribution. Without loss of generality, let us assume \u03c32 = 1 in q(I). Define the binary activation variable \u03b4k(I;w) = 1 if \u3008I,wk\u3009+ bk > 0 and \u03b4k(I;w) = 0 otherwise, i.e.,\n\u03b4k(I;w) = 1(\u3008I,wk\u3009+ bk > 0), (13)\nwhere 1() is the indicator function. Then\nh(\u3008I,wk\u3009+ bk) = \u03b4k(I;w)(\u3008I,wk\u3009+ bk). (14)\nThe image space is divided into 2K pieces by the K hyperplanes, \u3008I,wk\u3009 + bk = 0, k = 1, ...,K, according to the values of the binary activation variables (\u03b4k(I;w), k = 1, ...,K). Consider the piece where \u03b4k(I;w) = \u03b4k for k = 1, ...,K. Here we abuse the notation slightly where \u03b4k \u2208 {0, 1} on the right hand side denotes the value of \u03b4k(I;w). Write \u03b4(I;w) = (\u03b4k(I;w), k = 1, ...,K), and \u03b4 = (\u03b4k, k = 1, ...,K) as an instantiation of \u03b4(I;w). We call \u03b4(I;w) the activation pattern of I. Let A(\u03b4;w) = {I : \u03b4(I;w) = \u03b4} be the piece of image space that consists of images sharing the same activation pattern \u03b4, then the prob-\nability density on this piece\np(I;w, \u03b4) \u221d exp [ K\u2211 k=1 \u03b4kbk + \u3008I, K\u2211 k=1 \u03b4kwk\u3009 \u2212 \u2016I\u20162 2 ]\n\u221d exp [ \u22121\n2 \u2016I\u2212 K\u2211 k=1\n\u03b4kwk\u20162 ] ,\n(15) which is N( \u2211 k \u03b4kwk,1) restricted to the piece A(\u03b4;w), where the bold font 1 is the identity matrix (recall we assume \u03c32 = 1). \u03b4 = (\u03b4k) are the binary activation variables that reconstruct the mean of this Gaussian piece, \u2211 k \u03b4kwk.\n(2) Synthesis via reconstruction: One can sample from p(I;w) in (11) by the Langevin dynamics:\nI\u03c4+1 = I\u03c4 \u2212 2\n2\n[ I\u03c4 \u2212\nK\u2211 k=1 \u03b4k(I\u03c4 ;w)wk\n] + Z\u03c4 , (16)\nwhere \u03c4 denotes the time step, denotes the step size, assumed to be sufficiently small throughout this paper, and Z\u03c4 \u223c N(0,1). The dynamics is driven by the reconstruction error I \u2212 \u2211 k \u03b4kwk, where the reconstruction is based on the binary activation variables (\u03b4k). This links synthesis to reconstruction.\n(3) Auto-encoding local modes: The deterministic part of the dynamics I\u03c4+1 = I\u03c4 \u2212 2\n2\n[ I\u03c4 \u2212 \u2211K k=1 \u03b4k(I\u03c4 ;w)wk ] will converge to a local energy minimum I\u0302, where\nI\u0302 = K\u2211 k=1 \u03b4k(I\u0302;w)wk. (17)\nThat is, I\u0302 is auto-encoding. The encoding process is bottomup and infers \u03b4k = \u03b4k(I\u0302;w) = 1(\u3008I\u0302,wk\u3009+ bk > 0). The decoding process is top-down and reconstructs I\u0302 = \u2211 k \u03b4kwk. In the encoding process, wk plays the role of filter. In the decoding process, wk plays the role of basis function.\nThe learning of w from training images {Im,m = 1, ...,M} can be accomplished by maximum likelihood. Define L(w) = \u2211M m=1 log p(I;w)/M , with p(I;w) defined in (11), then\n\u2202L(w)\n\u2202wk =\n1\nM M\u2211 m=1 \u03b4k(Im;w)I\u2212 Ew[\u03b4k(I;w)I],\n\u2202L(w)\n\u2202bk =\n1\nM M\u2211 m=1 \u03b4k(Im;w)\u2212 Ew[\u03b4k(I;w)],\n(18)\nwhere Ew is the expectation with respect to p(I;w), and can be approximated by Monte Carlo samples from the Langevin dynamics. At the maximum likelihood estimate of w, the model matches the observed images in terms of (1) the frequency that \u03b4k is on, and (2) the average of images on which \u03b4k is on, for every k.\nThe reference distribution is usually not emphasized in previous treatments of exponential family models. It plays an important role in our work. The Gaussian white noise\nreference distribution makes the density p(I;w) in (11) integrable, and leads to the reconstruction error interpretation in the Langevin dynamics. It is also crucial for the autoencoder form of the local modes.\nThe reason we choose Gaussian white noise model as the reference distribution is that it is the maximum entropy distribution with given marginal mean and variance. Thus it is the most featureless distribution. The exponential tilting seeks to explain the departure from the Gaussian distribution by learning non-Gaussian features.\nAnother justification for the Gaussian white noise distribution is that it is the limiting distribution if we zoom out a texture image due to the central limit theorem, a phenomenon called information scaling by Wu et al. (2008). The exponential tilting is to recover the non-Gaussian distribution before the central limit theorem takes effect."}, {"heading": "6 Properties of Generative ConvNet", "text": "In order to generalize the prototype model (11) to the generative ConvNet (7), we only need to add two elements: (1) Horizontal unfolding: make the filters (wk) convolutional. (2) Vertical unfolding: make the filters (wk) multi-layer or hierarchical. The results we have obtained for the prototype model can be unfolded accordingly.\nWe shall use vector notation for ConvNet in order to minimize the use of indices (although there are still plenty remaining). For filters at level l, the Nl filters are denoted by the compact notation F(l) = (F (l)k , k = 1, ..., Nl). The Nl filtered images or feature maps are denoted by the compact notation F(l) \u2217 I = (F (l)k \u2217 I, k = 1, ..., Nl). F(l) \u2217 I is a 3D image, containing all the Nl filtered images at layer l. In vector notation, the recursive formula (1) of ConvNet filters can be rewritten as\n[F (l) k \u2217 I](x) = h ( \u3008w(l)k,x,F (l\u22121) \u2217 I\u3009+ bl,k ) , (19)\nwhere w(l)k,x matches the dimension of F (l\u22121) \u2217 I, which is a 3D image containing all the Nl\u22121 filtered images at layer l \u2212 1. Specifically,\n\u3008w(l)k,x,F (l\u22121) \u2217 I\u3009 = Nl\u22121\u2211 i=1 \u2211 y\u2208Sl w (l,k) i,y [F (l\u22121) i \u2217 I](x+ y).\n(20) The 3D basis function w(l)k,x is locally supported (on x + Sl), and (w(l)k,x) are spatially translated copies for different positions x, i.e.,\nw (l) k,x,i(x+ y) = w (l,k) i,y , (21)\nfor i \u2208 {1, ..., Nl\u22121}, x \u2208 Dl and y \u2208 Sl. For instance, at layer l = 1, w(1)k,x is a Gabor-like wavelet of type k centered at position x. w\n(l) k,x is the unfolded version of wk in the prototype model, where x indexes the position for convolutional unfolding, and l indexes the layer for hierarchical unfolding.\nDefine the binary activation variable\n\u03b4 (l) k,x(I;w) = 1 ( \u3008w(l)k,x,F (l\u22121) \u2217 I\u3009+ bl,k > 0 ) . (22)\nSince F (l)k corresponds to a non-stationary FRAME model (10), \u03b4(l)k,x(I;w) is a decision maker based on the likelihood ratio test of H1 : p (l) k (I;w, x) vs H0 : q(I) for detecting the pattern modeled by F (l)k . According to (1), we have the following bottom-up process:\n[F (l) k \u2217 I](x) = \u03b4 (l) k,x(I;w) ( \u3008w(l)k,x,F (l\u22121) \u2217 I\u3009+ bl,k ) . (23) Let \u03b4(I;w) = (\u03b4(l)k,x(I;w),\u2200k, x, l) be the activation pattern at all the layers. The active pattern \u03b4(I;w) can be computed in the bottom-up process (22) and (23) of ConvNet.\nFor the scoring function f(I;w) = \u2211K k=1 \u2211 x\u2208DL [F (L) k \u2217 I](x) defined in (8) for the generative ConvNet, we can write it in terms of lower layers (l \u2264 L) of filter responses:\nf(I;w) = \u03b1l + \u3008B(l),F(l) \u2217 I\u3009\n= \u03b1l + Nl\u2211 k=1 \u2211 x\u2208Dl B (l) k (x)[F (l) k \u2217 I](x),\n(24)\nwhere B(l) = (B(l)k (x), k = 1, ..., Nl, x \u2208 Dl) consists of the maps of the coefficients at layer l. B(l) matches the dimension of F(l)\u2217I. When l = L, B(L) consists of maps of 1\u2019s, i.e., B(L)k (x) = 1 for k = 1, ...,K = NL and x \u2208 DL. According to equations (23) and (24), we have the following top-down process:\nB(l\u22121) = Nl\u2211 k=1 \u2211 x\u2208Dl B (l) k (x)\u03b4 (l) k,x(I;w)w (l) k,x, (25)\nwhere both B(l\u22121) and w(l)k,x match the dimension of F(l\u22121) \u2217 I. Equation (25) is a top-down deconvolution process, where B(l)k (x)\u03b4 (l) k,x serves as the coefficient of the basis function w(l)k,x. The top-down deconvolution process (25) is similar to but subtly different from that in Zeiler & Fergus (2014), because equation (25) is controlled by the multiple layers of activation variables \u03b4(l)k,x computed in the bottomup process of the ConvNet. Specifically, \u03b4(l)k,x turns on or off the basis function w(l)k,x, while \u03b4 (l) k,x is determined by F (l) k . The recursive relationship for \u03b1l can be similarly derived. In the bottom-up convolution process (23), (w(l)k,x) serve as filters. In the top-down deconvolution process (25), (w\n(l) k,x) serve as basis functions. Let B = B(0), \u03b1 = \u03b10. Since F(0) \u2217 I = I, we have f(I;w) = \u03b1+ \u3008I,B\u3009. Note that B depends on the activation pattern \u03b4(I;w) = (\u03b4(l)k,x(I;w),\u2200k, x, l), as well as w that collects the weight and bias parameters at all the layers.\nOn the piece of image space A(\u03b4;w) = {I : \u03b4(I;w) = \u03b4} of a fixed activation pattern (again we slightly abuse the notation where \u03b4 = (\u03b4(l)k,x \u2208 {0, 1},\u2200k, x, l) denotes an instantiation of the activation pattern), B and \u03b1 depend on \u03b4 and\nw. To make this dependency explicit, we denote B = Bw,\u03b4 and \u03b1 = \u03b1w,\u03b4 , thus\nf(I;w) = \u03b1w,\u03b4 + \u3008I,Bw,\u03b4\u3009. (26)\nSee Montufar et al. (2014) for an analysis of the number of linear pieces.\nProposition 2 Generative ConvNet is piecewise Gaussian defined by binary activation variables: With re-lu h() and Gaussian white noise q(I), p(I;w) of model (7) is piecewise Gaussian. On each piece A(\u03b4;w), the density is N(Bw,\u03b4,1) truncated to A(\u03b4;w).\nProposition 2 follows from the fact that on A(\u03b4;w), p(I;w, \u03b4) \u221d exp [ \u03b1w,\u03b4 + \u3008I,Bw,\u03b4\u3009 \u2212 \u2016I\u20162\n2 ] \u221d exp [ \u22121\n2 \u2016I\u2212Bw,\u03b4\u20162\n] ,\n(27)\nwhich is N(Bw,\u03b4,1) restricted to A(\u03b4;w). For each I, the binary activation variables in \u03b4 = \u03b4(I;w) are computed by the bottom-up detection process (22) and (23), and Bw,\u03b4 is computed by the top-down deconvolution process (25).\nOne can sample from p(I;w) of model (7) by the Langevin dynamics:\nI\u03c4+1 = I\u03c4 \u2212 2\n2\n[ I\u03c4 \u2212Bw,\u03b4(I\u03c4 ;w) ] + Z\u03c4 , (28)\nwhere Z\u03c4 \u223c N(0,1). Again, the dynamics is driven by the reconstruction error I\u2212Bw,\u03b4(I;w).\nThe deterministic part of the Langevin equation (28) was employed by Zhu & Mumford (1998) for exploring the local modes of the FRAME model. They called it the Gibbs Reaction and Diffusion Equation (GRADE). It defines an attractor dynamics that will converge to a local energy minimum.\nProposition 3 Generative ConvNet contains an autoencoder at its local energy minima: The GRADE attractor dynamics I\u03c4+1 = I\u03c4 \u2212 2\n2 [ I\u03c4 \u2212Bw,\u03b4(I\u03c4 ;w) ] converges to a\nlocal mode I\u0302 of p(I;w) of model (7), and I\u0302 is auto-encoding:\nBottom-up encoding: \u03b4 = \u03b4(I\u0302;w);\nTop-down decoding: I\u0302 = Bw,\u03b4. (29)\nThe local energy minima are the means of the Gaussian pieces in Proposition 2, but the reverse is not necessarily true because Bw,\u03b4 does not necessarily belong to A(\u03b4;w). But if Bw,\u03b4 \u2208 A(\u03b4;w), then Bw,\u03b4 must be a local mode.\nProposition 3 can be generalized to general non-linear h(), whereas Proposition 2 is true only for piecewise linear h() such as re-lu.\nProposition 3 is interestingly related to the Hopfield network (Hopfield, 1982) and attractor network (Seung, 1998). Proposition 3 shows that the Hopfied minima can be represented by a hierarchical auto-encoder.\nTo gain a more comprehensive understanding of the deconvolution process, we would like to treat it from a\nslightly different perspective. For each filter F (l)k defined in the recursive formulas (1) and (19), let [F\u0304 (l)k \u2217 I](x) = \u3008w(l)k,x, F (l\u22121) \u2217 I\u3009 + bl,k be the linear combination before re-lu non-linearity. On A(\u03b4;w) = {I : \u03b4(I;w) = \u03b4}, i.e., given the activation pattern \u03b4, [F\u0304 (l)k \u2217 I](x) becomes a linear filter, and we can write\n[F\u0304 (l) k \u2217 I](x) = a (l) k,x + \u3008I,b (l) k,x\u3009, (30)\nwhere b(l)k,x is the basis function or a basis image defined on D. According to (1),\nb (l) k,x = Nl\u22121\u2211 i=1 \u2211 y\u2208Sl w (l,k) i,y \u03b4 (l\u22121) i,x+yb (l\u22121) i,x+y,\na (l) k,x = Nl\u22121\u2211 i=1 \u2211 y\u2208Sl w (l,k) i,y \u03b4 (l\u22121) i,x+ya (l\u22121) i,x+y + bl,k.\n(31)\nAt the bottom layer, for color channel k \u2208 {R,G,B}, we have b(0)k,x(y) = 1 if y = x and b (0) k,x(y) = 0 otherwise, i.e., the delta-function. Also, b0,k = 0, and \u03b4 (0) k,x = 1.\nEquation (31) corresponds to back-propagation calculation in discriminative ConvNet. But it takes a novel representational role in generative ConvNet. There are two complementary views of equation (31).\n(1) Bottom-up composition: In view of the basis functions, (31) defines a composition process, where the higher layer basis function b(l)k,x is a composition of lower layer basis functions {b(l\u22121)i,x+y, i = 1, ..., Nl\u22121, y \u2208 Sl} with coefficients {w(l,k)i,y \u03b4 (l\u22121) i,x+y}. Note that the weight parametersw (l,k) i,y can be turned on or off by the activation variable \u03b4(l\u22121)i,x+y , so that the composition is reconfigurable and b(l)k,x is a reconfigurable basis function that follows an And-Or logic (Zhu & Mumford, 2006). Sparsifying the connections will make the compositions more explicit and meaningful (Xie et al., 2015; Wu et al., 2010).\n(2) Top-down decomposition: In view of the maps of coefficients, (31) defines a top-down deconvolution process, where the coefficients of lower layer basis functions b(l\u22121)i,x+y are obtained by expanding the coefficients of higher layer basis functions b(l)k,x. This view is consistent with the deconvolution process about B(l) in equation (25). On the piece A(\u03b4;w) = {I : \u03b4(I;w) = \u03b4}, f(I;w) =\u2211K k=1 \u2211 x\u2208DL [F (L) k \u2217 I](x) in (8) is linear, i.e., f(I;w) = \u03b1w,\u03b4 + \u3008I,Bw,\u03b4\u3009, where\nBw,\u03b4 = K\u2211 k=1 \u2211 x\u2208DL \u03b4 (L) k,xb (L) k,x ,\n\u03b1w,\u03b4 = K\u2211 k=1 \u2211 x\u2208DL \u03b4 (L) k,xa (L) k,x .\n(32)\nIn the prototype model, Bw,\u03b4 = \u2211 k \u03b4kwk and \u03b1w,\u03b4 =\u2211\nk \u03b4kbk."}, {"heading": "7 Learning Generative ConvNet", "text": "The learning ofw from training images {Im,m = 1, ...,M} can be accomplished by maximum likelihood. Let\nL(w) = 1\nM M\u2211 m=1 log p(I;w), (33)\nwith p(I;w) defined in (7), then\n\u2202L(w)\n\u2202w =\n1\nM M\u2211 m=1 \u2202 \u2202w f(Im;w)\u2212 Ew [ \u2202 \u2202w f(I;w) ] .\n(34) The expectation can be approximated by Monte Carlo samples (Younes, 1999) from the Langevin dynamics (28). See Algorithm 1 for a description of the learning and sampling algorithm.\nWe can build up the model layer by layer. Given the filters at layers below, the top layer weight and bias parameters can be learned according to\n\u2202L(w)\n\u2202w (L,k) i,y\n= 1\nM M\u2211 m=1 \u2211 x\u2208DL \u03b4 (L) k,x (Im;w)[F (L\u22121) i \u2217 Im](x+ y)\n\u2212 Ew [ \u2211 x\u2208DL \u03b4 (L) k,x (I;w)[F (L\u22121) i \u2217 I](x+ y) ] ,\n(35) and\n\u2202L(w)\n\u2202bL,k =\n1\nM M\u2211 m=1 \u2211 x\u2208DL \u03b4 (L) k,x (Im;w)\n\u2212 Ew [ \u2211 x\u2208DL \u03b4 (L) k,x (I;w) ] .\n(36)\nThe above equations are unfolded versions of (18). At the maximum likelihood estimate of w, the model matches the observed images in terms of (1) the average frequency that \u03b4 (L) k,x is on, and (2) the average of patches of the filtered image F (L\u22121)i \u2217 I on which \u03b4 (L) k,x is on, where the average is pooled over x, and the matching happens for every k. If we want to learn from big data, we may use contrastive divergence (Hinton, 2002) by starting the Langevin dynamics from the observed images. The behavior of contrastive divergence learning of generative ConvNet is very interesting.\nSuppose we start from an observed image Iobs, and run a small number of iterations of Langevin dynamics (28) to get a synthesized image Isyn. If both Iobs and Isyn share the same activation pattern \u03b4(Iobs;w) = \u03b4(Isyn;w) = \u03b4, then f(I;w) = aw,\u03b4 + \u3008I,Bw,\u03b4\u3009 for both Iobs and Isyn. Then the contribution of Iobs to the learning gradient is\n\u2202 \u2202w f(Iobs;w)\u2212 \u2202 \u2202w f(Isyn;w) = \u3008Iobs \u2212 Isyn, \u2202 \u2202w Bw,\u03b4\u3009. (37) If Isyn is close to the mean Bw,\u03b4 and if Bw,\u03b4 is a local mode, then the contrastive divergence tends to reconstruct Iobs by\nAlgorithm 1 Learning and sampling algorithm Input:\n(1) training images {Im,m = 1, ...,M} (2) number of synthesized images M\u0303 (3) number of Langevin steps L (4) number of learning iterations T\nOutput: (1) estimated parameters w (2) synthesized images {I\u0303m,m = 1, ..., M\u0303}\n1: Let t\u2190 0, initialize w(0) \u2190 0. 2: Initialize I\u0303m \u2190 0, for m = 1, ..., M\u0303 . 3: repeat 4: For each m, run L steps of Langevin dynamics to up-\ndate I\u0303m, i.e., starting from the current I\u0303m, each step follows equation (28).\n5: Calculate Hobs = \u2211M m=1 \u2202 \u2202wf(Im;w (t))/M , and\nHsyn = \u2211M\u0303 m=1 \u2202 \u2202wf(I\u0303m;w\n(t))/M\u0303 . 6: Update w(t+1) \u2190 w(t) + \u03b7(Hobs \u2212Hsyn), with step size \u03b7. 7: Let t\u2190 t+ 1 8: until t = T\nthe local mode Bw,\u03b4 , because the gradient\n\u2202\n\u2202w \u2016Iobs \u2212Bw,\u03b4\u20162/2 = \u2212\u3008Iobs \u2212Bw,\u03b4,\n\u2202\n\u2202w Bw,\u03b4\u3009. (38)\nHence contrastive divergence learns Hopfield network which memorizes observations by local modes.\nWe can establish a precise connection for one-step contrastive divergence.\nProposition 4 Contrastive divergence learns to reconstruct: If the one-step Langevin dynamics does not change the activation pattern, i.e., \u03b4(Iobs;w) = \u03b4(Isyn;w) = \u03b4, then the one-step contrastive divergence has an expected gradient that is proportional to the reconstruction gradient:\nE\n[ \u2202\n\u2202w f(Iobs;w)\u2212 \u2202 \u2202w f(Isyn;w) ] \u221d \u2202 \u2202w \u2016Iobs\u2212Bw,\u03b4\u20162.\n(39)\nThis is because\nIsyn = Iobs \u2212 2\n2\n[ Iobs \u2212Bw,\u03b4 ] + Z, (40)\nhence EZ [ Iobs \u2212 Isyn ] \u221d Iobs \u2212Bw,\u03b4, (41)\nand Proposition 4 follows from (37) and (38). Proposition 4 is related to score matching estimator of Hyva\u0308rinen (2005), whose connection with contrastive divergence based on one-step Langevin was studied by Hyva\u0308rinen (2007). The relationship between score matching and autoencoder was discovered by Vincent (2011) and Swersky et al. (2011). Our work can be considered a sharpened specialization of the above mentioned connection and relationship, where the piecewise linear structure in ConvNet greatly\nsimplifies the matter by getting rid of the complicated second derivative terms, so that the contrastive divergence gradient becomes exactly proportional to the gradient of the reconstruction error, which is not the case in general score matching estimator. Also, our work gives a novel hierarchical realization of the relationship between probability model and auto-encoder, as well as an explicit hierarchical realization of auto-encoder based sampling of Alain & Bengio (2014). The connection with Hopfied network also appears new.\nProposition 4 suggests that we may learn the weight parameters by directly minimizing the reconstruction error, without having to deal with Monte Caro fluctuations. As to the bias parameters, which threshold the likelihood ratio tests for pattern detection, we can simply set their values to constrain the sparsity (Olshausen & Field, 1997) of activations. Such an unsupervised learning method is as fast as training a discriminative ConvNet in supervised learning.\nIn general, it is possible for multi-step Langevin dynamics to move out of the Gaussian piece of Iobs and into another piece of a different activation pattern. But it is unlikely for the activation patterns of Iobs and Isyn to be very different. It may be particularly difficult to flip the activation variables at higher layers. In this case, contrastive divergence may be maximizing the pseudo-likelihood (Besag, 1974) conditioning on the higher layer binary variables. In any case, equation (36) makes it clear that we do not have much information to update the bias terms associated with these variables, but we can set their values by sparsity constraints as mentioned above."}, {"heading": "8 Synthesis and Reconstruction by Generative ConvNet", "text": "We show that the generative ConvNet is capable of learning and generating realistic natural image patterns. Such an empirical proof of concept validates the generative capacity of the model. We also show that contrastive divergence learning can indeed reconstruct the observed images, thus empirically validating Proposition 4.\nThe code in our experiments is based on the MatConvNet package of Vedaldi & Lenc (2014).\nUnlike Lu et al. (2016), the generative ConvNets in our experiments are learned from scratch without relying on the pre-learned filters of existing ConvNets.\nWhen learning the generative ConvNet, we grow the layers sequentially. Starting from the first layer, we sequentially add the layers one by one. Each time we learn the model and generate the synthesized images using Algorithm 1. While learning the new layer of filters, we can either fix lower layers of filters while updating the top layer weight and bias parameters according to equations (35) and (36), or we can additionally refine the lower layer filters by back-propagation at the same time. Both strategies work well for image synthesis. We adopt the latter strategy in our experiments.\nWe use M\u0303 = 16 parallel chains for Langevin sampling. The number of Langevin iterations between every two consecutive updates of parameters is L = 10. With each new added layer, the number of learning iterations is T = 700.\nWe follow the standard procedure to prepare the training images of size 224\u00d7224, whose intensities range from [0, 255], and we subtract the mean image. We fix \u03c32 = 1 in the reference distribution q(I).\nFor each of the 3 experiments, we use the same set of parameters for all the categories without tuning."}, {"heading": "8.1 Experiment 1: Generating texture patterns", "text": "We learn a 3-layer generative ConvNet. The first layer has 100 15\u00d7 15 filters with sub-sampling size of 3. The second layer has 64 5 \u00d7 5 filters with sub-sampling size of 1. The third layer has 30 3 \u00d7 3 filters with sub-sampling size of 1. We learn a generative ConvNet for each category from a single training image. Figure 1 displays the results. For each category, the first image is the training image, and the rest are 2 of the images generated by the learning algorithm."}, {"heading": "8.2 Experiment 2: Special case: generating aligned object patterns", "text": "Experiment 1 shows clearly that the generative ConvNet can learn from images without alignment. We can also specialize it to learning aligned object patterns by using a single top-layer filter that covers the whole image. It is actually a non-stationary FRAME model of the form (10), i.e., a convolutional filter at a fixed position before re-lu nonlinearity. We learn a 4-layer generative ConvNet from images of aligned objects. The first layer has 100 7 \u00d7 7 filters with sub-sampling size of 2. The second layer has 64 5 \u00d7 5 filters with sub-sampling size of 1. The third layer has 20 3 \u00d7 3 filters with sub-sampling size of 1. The fourth layer is a fully connected layer with a single filter that covers the whole image. When growing the layers, we always keep the top-layer single filter, and train it together with the existing layers. We learn a generative ConvNet for each category, where the number of training images for each category is around 10, and they are collected from Internet. Figure 2 shows the results. For each category, the first row displays 4 of the training images, and the second row shows 4 of the images generated by the learning algorithm."}, {"heading": "8.3 Experiment 3: Contrastive divergence learns to reconstruct", "text": "We evaluate one-step contrastive divergence learning on a small training set of 10 images collected from Internet. The ConvNet structure is the same as in experiment 1. For computational efficiency, we learn all the layers of filters simultaneously. The number of learning iterations is T = 1200. Starting from the observed images, the number of Langevin iterations is L = 1. Figure 3 shows the results. The first row displays 4 of the training images, and the second row displays the corresponding auto-encoding reconstructions with the learned parameters."}, {"heading": "9 General Non-Linearity", "text": "For general non-linearity, i.e., h(r) is not necessarily re-lu or piecewise linear, some of the above results still hold. Let h\u2032(r) be the derivative of h(r), then the activation variables become\n\u03b4 (l) k,x(I;w) = h \u2032 ( \u3008w(l)k,x, F (l\u22121) \u2217 I\u3009+ bl,k ) , (42)\nwhich is not binary in general, and the auto-encoding reconstruction\nBw,\u03b4(I;w) = \u2202\n\u2202I f(I;w), (43)\nwhere f(I;w) is defined by (8). The Langevin dynamics (28) and the Hopfield auto-encoder (29) in Proposition 3 still hold, but the piecewise Gaussian form (27) in Proposition 2 is lost. The exact proportionality in Proposition 4 is also lost. The crisp re-lu non-linearity is crucial for the simplicity of modeling and learning, and should be kept as much as possible.\nEquations (42) and (43) show that the top-down generation is actually the back-propagation derivative (with respect to I instead of w) of the bottom-up recognition, i.e., representation = back-propagation.\nWe can also incorporate max pooling in the bottom-up computation, whose derivative is arg-max retrieval in the top-down reconstruction."}, {"heading": "10 Conclusion", "text": "This paper derives the generative ConvNet from the discriminative ConvNet, and reveals a representational structure in the generative ConvNet.\nSome of the results in this paper can be mapped to backpropagation in the discriminative ConvNet, but our reinterpretation of them in terms of representation is novel and is richly expansive. Our paper unifies, reconciles or connects the following antagonizing or disparate pairs: (1) discriminative ConvNet and generative CongNet, (2) supervised learning and unsupervised learning, (3) exponential family models and latent variable models, (4) bottom-up filters (operation) and top-down basis functions (representation), (5) synthesis (dream) and reconstruction (memory), (6) hierarchal probability model and hierarchical auto-encoder, (7) Hopfield attractor network and auto-encoder, (8) Contrastive divergence (learning) and reconstruction (memory).\nThe generative ConvNet has the potential to learn from unlabeled data. In our future work, we shall scale up the\nunsupervised learning from big unlabeled data using reconstruction based methods.\nCode and data The code and training images can be downloaded from the project page: http://www.stat.ucla.edu/\u02dcywu/ GenerativeConvNet/main.html"}, {"heading": "Acknowledgement", "text": "The code in our work is based on the Matlab code of MatConvNet of Vedaldi & Lenc (2014). We thank the authors for making their code public.\nWe thank Jifeng Dai for earlier collaboration on generative ConvNet. We thank Wenze Hu for earlier collaboration on inhomogeneous FRAME model.\nThe work is supported by NSF DMS 1310391, ONR MURI N00014-10-1-0933 and DARPA SIMPLEX N6600115-C-4035."}], "references": [{"title": "What regularized auto-encoders learn from the data-generating distribution", "author": ["Alain", "Guillaume", "Bengio", "Yoshua"], "venue": "The Journal of Machine Learning Research,", "citeRegEx": "Alain et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Alain et al\\.", "year": 2014}, {"title": "Spatial interaction and the statistical analysis of lattice systems", "author": ["Besag", "Julian"], "venue": "Journal of the Royal Statistical Society. Series B (Methodological),", "citeRegEx": "Besag and Julian.,? \\Q1974\\E", "shortCiteRegEx": "Besag and Julian.", "year": 1974}, {"title": "Generative modeling of convolutional neural networks", "author": ["Dai", "Jifeng", "Lu", "Yang", "Wu", "Ying Nian"], "venue": "In ICLR,", "citeRegEx": "Dai et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Dai et al\\.", "year": 2015}, {"title": "Unsupervised discovery of nonlinear structure using contrastive backpropagation", "author": ["Hinton", "Geoffrey", "Osindero", "Simon", "Welling", "Max", "Teh", "Yee-Whye"], "venue": "Cognitive science,", "citeRegEx": "Hinton et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Hinton et al\\.", "year": 2006}, {"title": "Training products of experts by minimizing contrastive divergence", "author": ["Hinton", "Geoffrey E"], "venue": "Neural Computation,", "citeRegEx": "Hinton and E.,? \\Q2002\\E", "shortCiteRegEx": "Hinton and E.", "year": 2002}, {"title": "wake-sleep\u201d algorithm for unsupervised neural networks", "author": ["Hinton", "Geoffrey E", "Dayan", "Peter", "Frey", "Brendan J", "Neal", "Radford M. The"], "venue": null, "citeRegEx": "Hinton et al\\.,? \\Q1995\\E", "shortCiteRegEx": "Hinton et al\\.", "year": 1995}, {"title": "A fast learning algorithm for deep belief nets", "author": ["Hinton", "Geoffrey E", "Osindero", "Simon", "Teh", "Yee Whye"], "venue": "Neural Computation,", "citeRegEx": "Hinton et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Hinton et al\\.", "year": 2006}, {"title": "Unsupervised learning of compositional sparse code for natural image representation", "author": ["Hong", "Yi", "Si", "Zhangzhang", "Hu", "Wenze", "Zhu", "Song-Chun", "Wu", "YING NIAN"], "venue": "Quarterly of Applied Mathematics,", "citeRegEx": "Hong et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Hong et al\\.", "year": 2013}, {"title": "Neural networks and physical systems with emergent collective computational abilities", "author": ["Hopfield", "John J"], "venue": "Proceedings of the national academy of sciences,", "citeRegEx": "Hopfield and J.,? \\Q1982\\E", "shortCiteRegEx": "Hopfield and J.", "year": 1982}, {"title": "Estimation of non-normalized statistical models by score matching", "author": ["Hyv\u00e4rinen", "Aapo"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Hyv\u00e4rinen and Aapo.,? \\Q2005\\E", "shortCiteRegEx": "Hyv\u00e4rinen and Aapo.", "year": 2005}, {"title": "Connections between score matching, contrastive divergence, and pseudolikelihood for continuous-valued variables", "author": ["Hyv\u00e4rinen", "Aapo"], "venue": "Neural Networks, IEEE Transactions on,", "citeRegEx": "Hyv\u00e4rinen and Aapo.,? \\Q2007\\E", "shortCiteRegEx": "Hyv\u00e4rinen and Aapo.", "year": 2007}, {"title": "Auto-encoding variational bayes", "author": ["Kingma", "Diederik P", "Welling", "Max"], "venue": "ICLR,", "citeRegEx": "Kingma et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Kingma et al\\.", "year": 2014}, {"title": "Imagenet classification with deep convolutional neural networks", "author": ["Krizhevsky", "Alex", "Sutskever", "Ilya", "Hinton", "Geoffrey E"], "venue": "In NIPS, pp", "citeRegEx": "Krizhevsky et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Krizhevsky et al\\.", "year": 2012}, {"title": "A tutorial on energy-based learning", "author": ["Y. LeCun", "S. Chopra", "R. Hadsell", "M. Ranzato", "F.J. Huang"], "venue": null, "citeRegEx": "LeCun et al\\.,? \\Q2006\\E", "shortCiteRegEx": "LeCun et al\\.", "year": 2006}, {"title": "Gradient-based learning applied to document recognition", "author": ["LeCun", "Yann", "Bottou", "L\u00e9on", "Bengio", "Yoshua", "Haffner", "Patrick"], "venue": "Proceedings of the IEEE,", "citeRegEx": "LeCun et al\\.,? \\Q1998\\E", "shortCiteRegEx": "LeCun et al\\.", "year": 1998}, {"title": "Learning frame models using cnn filters", "author": ["Lu", "Yang", "Zhu", "Song-chun", "Wu", "Ying Nian"], "venue": "In Thirtieth AAAI Conference on Artificial Intelligence,", "citeRegEx": "Lu et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Lu et al\\.", "year": 2016}, {"title": "Neural variational inference and learning in belief networks", "author": ["Mnih", "Andriy", "Gregor", "Karol"], "venue": "In ICML,", "citeRegEx": "Mnih et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Mnih et al\\.", "year": 2014}, {"title": "On the number of linear regions of deep neural networks", "author": ["Montufar", "Guido F", "Pascanu", "Razvan", "Cho", "Kyunghyun", "Bengio", "Yoshua"], "venue": "In NIPS,", "citeRegEx": "Montufar et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Montufar et al\\.", "year": 2014}, {"title": "Learning deep energy models", "author": ["Ngiam", "Jiquan", "Chen", "Zhenghao", "Koh", "Pang Wei", "Ng", "Andrew Y"], "venue": "In ICML,", "citeRegEx": "Ngiam et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Ngiam et al\\.", "year": 2011}, {"title": "Sparse coding with an overcomplete basis set: A strategy employed by v1", "author": ["Olshausen", "Bruno A", "Field", "David J"], "venue": "Vision Research,", "citeRegEx": "Olshausen et al\\.,? \\Q1997\\E", "shortCiteRegEx": "Olshausen et al\\.", "year": 1997}, {"title": "Stochastic backpropagation and approximate inference in deep generative models", "author": ["Rezende", "Danilo J", "Mohamed", "Shakir", "Wierstra", "Daan"], "venue": "JMLR Workshop and Conference Proceedings,", "citeRegEx": "Rezende et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Rezende et al\\.", "year": 2014}, {"title": "Fields of experts: A framework for learning image priors", "author": ["Roth", "Stefan", "Black", "Michael J"], "venue": "In CVPR,", "citeRegEx": "Roth et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Roth et al\\.", "year": 2005}, {"title": "Deep boltzmann machines", "author": ["Salakhutdinov", "Ruslan", "Hinton", "Geoffrey E"], "venue": "In AISTATS,", "citeRegEx": "Salakhutdinov et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Salakhutdinov et al\\.", "year": 2009}, {"title": "Learning continuous attractors in recurrent networks", "author": ["Seung", "H. Sebastian"], "venue": "In NIPS,", "citeRegEx": "Seung and Sebastian.,? \\Q1998\\E", "shortCiteRegEx": "Seung and Sebastian.", "year": 1998}, {"title": "On autoencoders and score matching for energy based models", "author": ["Swersky", "Kevin", "Ranzato", "Marc\u2019Aurelio", "Buchman", "David", "Marlin", "Benjamin", "Freitas", "Nando"], "venue": "ICML, ICML", "citeRegEx": "Swersky et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Swersky et al\\.", "year": 2011}, {"title": "Matconvnet \u2013 convolutional neural networks for matlab", "author": ["A. Vedaldi", "K. Lenc"], "venue": "CoRR, abs/1412.4564,", "citeRegEx": "Vedaldi and Lenc,? \\Q2014\\E", "shortCiteRegEx": "Vedaldi and Lenc", "year": 2014}, {"title": "A connection between score matching and denoising autoencoders", "author": ["Vincent", "Pascal"], "venue": "Neural computation,", "citeRegEx": "Vincent and Pascal.,? \\Q2011\\E", "shortCiteRegEx": "Vincent and Pascal.", "year": 2011}, {"title": "From information scaling of natural images to regimes of statistical models", "author": ["Wu", "Ying Nian", "Zhu", "Song-Chun", "Guo", "Cheng-En"], "venue": "Quarterly of Applied Mathematics,", "citeRegEx": "Wu et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Wu et al\\.", "year": 2008}, {"title": "Learning active basis model for object detection and recognitio", "author": ["Wu", "Ying Nian", "Si", "Zhangzhang", "Gong", "Haifeng", "Zhu", "Song-Chun"], "venue": "International Journal of Computer Vision,", "citeRegEx": "Wu et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Wu et al\\.", "year": 2010}, {"title": "Learning sparse frame models for natural image patterns", "author": ["Xie", "Jianwen", "Hu", "Wenze", "Zhu", "Song-Chun", "Wu", "Ying Nian"], "venue": "International Journal of Computer Vision, pp", "citeRegEx": "Xie et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Xie et al\\.", "year": 2014}, {"title": "Inducing wavelets into random fields via generative boosting", "author": ["Xie", "Jianwen", "Lu", "Yang", "Zhu", "Song-Chun", "Wu", "Ying Nian"], "venue": "Journal of Applied and Computational Harmonic Analysis,", "citeRegEx": "Xie et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Xie et al\\.", "year": 2015}, {"title": "On the convergence of markovian stochastic algorithms with rapidly decreasing ergodicity rates", "author": ["Younes", "Laurent"], "venue": "Stochastics: An International Journal of Probability and Stochastic Processes,", "citeRegEx": "Younes and Laurent.,? \\Q1999\\E", "shortCiteRegEx": "Younes and Laurent.", "year": 1999}, {"title": "Visualizing and understanding convolutional neural networks", "author": ["Zeiler", "Matthew D", "Fergus", "Rob"], "venue": null, "citeRegEx": "Zeiler et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Zeiler et al\\.", "year": 2014}, {"title": "Adaptive deconvolutional networks for mid and high level feature learning", "author": ["Zeiler", "Matthew D", "Taylor", "Graham W", "Fergus", "Rob"], "venue": "In Computer Vision (ICCV),", "citeRegEx": "Zeiler et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Zeiler et al\\.", "year": 2011}, {"title": "Grade: Gibbs reaction and diffusion equations", "author": ["S.C. Zhu", "D.B. Mumford"], "venue": "In ICCV,", "citeRegEx": "Zhu and Mumford,? \\Q1998\\E", "shortCiteRegEx": "Zhu and Mumford", "year": 1998}, {"title": "A stochastic grammar of images", "author": ["Zhu", "Song Chun", "Mumford", "David"], "venue": "Foundations and Trends in Computer Graphics and Vision,", "citeRegEx": "Zhu et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Zhu et al\\.", "year": 2006}, {"title": "Minimax entropy principle and its application to texture modeling", "author": ["Zhu", "Song Chun", "Wu", "Ying Nian", "Mumford", "David"], "venue": "Neural Computation,", "citeRegEx": "Zhu et al\\.,? \\Q1997\\E", "shortCiteRegEx": "Zhu et al\\.", "year": 1997}], "referenceMentions": [{"referenceID": 14, "context": "1 Introduction The convolutional neural network (ConvNet or CNN) (LeCun et al., 1998; Krizhevsky et al., 2012) has proven to be a tremendously successful discriminative or predictive learning machine.", "startOffset": 65, "endOffset": 110}, {"referenceID": 12, "context": "1 Introduction The convolutional neural network (ConvNet or CNN) (LeCun et al., 1998; Krizhevsky et al., 2012) has proven to be a tremendously successful discriminative or predictive learning machine.", "startOffset": 65, "endOffset": 110}, {"referenceID": 36, "context": "The generative ConvNet model can be viewed as a hierarchical version of the FRAME (Filters, Random field, And Maximum Entropy) model (Zhu et al., 1997), as well as the Product of Experts (PoE) (Hinton, 2002) and Field of Experts (FoE) (Roth & Black, 2005) models.", "startOffset": 133, "endOffset": 151}, {"referenceID": 12, "context": ", 1998; Krizhevsky et al., 2012) has proven to be a tremendously successful discriminative or predictive learning machine. Can the discriminative ConvNet be turned into a generative model and an unsupervised learning machine? It would be highly desirable if this can be achieved because generative models and unsupervised learning can be very useful when the training datasets are small or the labeled data are scarce. It would also be extremely satisfying from a conceptual point of view if both discriminative classifier and generative model, and both supervised learning and unsupervised learning, can be treated within a unified framework for ConvNet. In this conceptual paper, we show that a generative random field model, which we call generative ConvNet, can be derived from the commonly used discriminative ConvNet. The probability distribution of the generative ConvNet model is in the form of an exponential tilting of a reference distribution, and the exponential tilting is defined by \u2217Equal contributions. ConvNet that involves multiple layers of liner filtering and non-linear transformation. The generative ConvNet model can be viewed as a hierarchical version of the FRAME (Filters, Random field, And Maximum Entropy) model (Zhu et al., 1997), as well as the Product of Experts (PoE) (Hinton, 2002) and Field of Experts (FoE) (Roth & Black, 2005) models. Being an exponential family model, the generative ConvNet may appear dull and opaque. The main purpose of this article is to show that the contrary is true. Assuming Gaussian white noise reference distribution and re-lu nonlinearity, we discover that the generative ConvNet contains a surprisingly explicit and exquisite representational structure. Specifically, it contains multiple layers of binary activation variables that indicate the presence or absence of the patterns modeled by the multiple layers of filters of the ConvNet. The generative ConvNet model is non-Gaussian, or more precisely, piecewise Gaussian, where each piece is determined by an instantiation of the binary activation variables. These binary variables are computed by a bottom-up process by the multiple layers of filters, and they reconstruct the mean of the Gaussian piece by a top-down process, where the multiple layers of filters serve as multiple layers of basis functions for image reconstruction. The Langevin dynamics can be employed to synthesize images by sampling from the generative ConvNet. Interestingly, the dynamics is driven by the reconstruction error, i.e., the difference between the current image and the reconstruction by the binary activation variables mentioned above. Thus image synthesis and image reconstruction are connected. The deterministic gradient descent counterpart of the Langevin dynamics was employed by Zhu & Mumford (1998) for exploring the local energy minima of the FRAME model.", "startOffset": 8, "endOffset": 2812}, {"referenceID": 13, "context": "The hierarchical energy-based models (LeCun et al., 2006) were studied by the pioneering work of Hinton et al.", "startOffset": 37, "endOffset": 57}, {"referenceID": 36, "context": "A main motivation for this paper is to reconcile the FRAME model (Zhu et al., 1997), where the Gabor wavelets play the role of bottom-up filters, and the Olshausen-Field model (Olshausen & Field, 1997), where the wavelets play the role of top-down basis functions, and unfold these models into a hierarchical sparse compositional model (Hong et al.", "startOffset": 65, "endOffset": 83}, {"referenceID": 7, "context": ", 1997), where the Gabor wavelets play the role of bottom-up filters, and the Olshausen-Field model (Olshausen & Field, 1997), where the wavelets play the role of top-down basis functions, and unfold these models into a hierarchical sparse compositional model (Hong et al., 2013; Xie et al., 2014).", "startOffset": 260, "endOffset": 297}, {"referenceID": 29, "context": ", 1997), where the Gabor wavelets play the role of bottom-up filters, and the Olshausen-Field model (Olshausen & Field, 1997), where the wavelets play the role of top-down basis functions, and unfold these models into a hierarchical sparse compositional model (Hong et al., 2013; Xie et al., 2014).", "startOffset": 260, "endOffset": 297}, {"referenceID": 5, "context": "One way to get around the intractable inference problem mentioned above is to use the wake-sleep algorithm (Hinton et al., 1995) or the variational auto-encoder (Kingma & Welling, 2014; Rezende et al.", "startOffset": 107, "endOffset": 128}, {"referenceID": 20, "context": ", 1995) or the variational auto-encoder (Kingma & Welling, 2014; Rezende et al., 2014; Mnih & Gregor, 2014).", "startOffset": 40, "endOffset": 107}, {"referenceID": 2, "context": "The model in the form of exponential tilting of a reference distribution where the exponential tilting is defined by ConvNet was first proposed by Dai et al. (2015). They learned the model by a non-parametric importance sampling scheme.", "startOffset": 147, "endOffset": 165}, {"referenceID": 2, "context": "The model in the form of exponential tilting of a reference distribution where the exponential tilting is defined by ConvNet was first proposed by Dai et al. (2015). They learned the model by a non-parametric importance sampling scheme. Lu et al. (2016) proposed to learn the FRAME models based on pre-learned filters of existing ConvNets.", "startOffset": 147, "endOffset": 254}, {"referenceID": 2, "context": "The model in the form of exponential tilting of a reference distribution where the exponential tilting is defined by ConvNet was first proposed by Dai et al. (2015). They learned the model by a non-parametric importance sampling scheme. Lu et al. (2016) proposed to learn the FRAME models based on pre-learned filters of existing ConvNets. They did not learn the models from scratch. The hierarchical energy-based models (LeCun et al., 2006) were studied by the pioneering work of Hinton et al. (2006a) and Ngiam et al.", "startOffset": 147, "endOffset": 503}, {"referenceID": 2, "context": "The model in the form of exponential tilting of a reference distribution where the exponential tilting is defined by ConvNet was first proposed by Dai et al. (2015). They learned the model by a non-parametric importance sampling scheme. Lu et al. (2016) proposed to learn the FRAME models based on pre-learned filters of existing ConvNets. They did not learn the models from scratch. The hierarchical energy-based models (LeCun et al., 2006) were studied by the pioneering work of Hinton et al. (2006a) and Ngiam et al. (2011). However, their models do not correspond directly to modern ConvNet.", "startOffset": 147, "endOffset": 527}, {"referenceID": 2, "context": "The model in the form of exponential tilting of a reference distribution where the exponential tilting is defined by ConvNet was first proposed by Dai et al. (2015). They learned the model by a non-parametric importance sampling scheme. Lu et al. (2016) proposed to learn the FRAME models based on pre-learned filters of existing ConvNets. They did not learn the models from scratch. The hierarchical energy-based models (LeCun et al., 2006) were studied by the pioneering work of Hinton et al. (2006a) and Ngiam et al. (2011). However, their models do not correspond directly to modern ConvNet. Compared to the above mentioned papers, we would like to emphasize the conceptual novelty of our work. Starting from a prototype model and then unfolding it, our work reveals a curious representational structure contained in the model that involves multiple layers of activation variables. Such a representational structure is unexpected for the exponential family models, and was not studied by the papers cited above. A main motivation for this paper is to reconcile the FRAME model (Zhu et al., 1997), where the Gabor wavelets play the role of bottom-up filters, and the Olshausen-Field model (Olshausen & Field, 1997), where the wavelets play the role of top-down basis functions, and unfold these models into a hierarchical sparse compositional model (Hong et al., 2013; Xie et al., 2014). The generative ConvNet may help solve this problem. The representational structure in the generative ConvNet is similar to but subtly different from the deconvolution network of Zeiler et al. (2011). The top-down process of the generative ConvNet is controlled by multiple layers of binary activation variables computed by the bottom-up process.", "startOffset": 147, "endOffset": 1590}, {"referenceID": 12, "context": "We take h(r) = max(r, 0), the rectified linear unit (re-lu), that is commonly adopted in modern ConvNet (Krizhevsky et al., 2012).", "startOffset": 104, "endOffset": 129}, {"referenceID": 14, "context": "The discriminative ConvNet is a multinomial logistic regression (or soft-max) that is commonly used for classification (LeCun et al., 1998; Krizhevsky et al., 2012).", "startOffset": 119, "endOffset": 164}, {"referenceID": 12, "context": "The discriminative ConvNet is a multinomial logistic regression (or soft-max) that is commonly used for classification (LeCun et al., 1998; Krizhevsky et al., 2012).", "startOffset": 119, "endOffset": 164}, {"referenceID": 2, "context": "The model was first proposed by Dai et al. (2015).", "startOffset": 32, "endOffset": 50}, {"referenceID": 2, "context": "Result (a) has already been explained in Dai et al. (2015). Result (b) is stronger and is new.", "startOffset": 41, "endOffset": 59}, {"referenceID": 36, "context": "The first model in the literature that represents the clique functions by non-linear transformations of linear filter responses is the FRAME (Filters, Random field, And Maximum Entropy) model (Zhu et al., 1997).", "startOffset": 192, "endOffset": 210}, {"referenceID": 29, "context": "Based on filters {F (l\u22121) i ,\u2200i} at layer l \u2212 1, each filter F (l) k at layer l corresponds to a non-stationary FRAME model (Xie et al., 2014; Lu et al., 2016) of an image patch defined on the support of the filter, Sl, and centered at x:", "startOffset": 124, "endOffset": 159}, {"referenceID": 15, "context": "Based on filters {F (l\u22121) i ,\u2200i} at layer l \u2212 1, each filter F (l) k at layer l corresponds to a non-stationary FRAME model (Xie et al., 2014; Lu et al., 2016) of an image patch defined on the support of the filter, Sl, and centered at x:", "startOffset": 124, "endOffset": 159}, {"referenceID": 15, "context": "The model is also a generative ConvNet model, and it can generate vivid object patterns (Lu et al., 2016).", "startOffset": 88, "endOffset": 105}, {"referenceID": 15, "context": "See Lu et al. (2016) for details.", "startOffset": 4, "endOffset": 21}, {"referenceID": 29, "context": "A similar model was studied by Xie et al. (2015). The generative ConvNet can be obtained from the prototype model by unfolding the latter both convolutionally and hierarchically, but with much more involved notation that is in danger of obscuring the key ideas.", "startOffset": 31, "endOffset": 49}, {"referenceID": 27, "context": "Another justification for the Gaussian white noise distribution is that it is the limiting distribution if we zoom out a texture image due to the central limit theorem, a phenomenon called information scaling by Wu et al. (2008). The exponential tilting is to recover the non-Gaussian distribution before the central limit theorem takes effect.", "startOffset": 212, "endOffset": 229}, {"referenceID": 17, "context": "See Montufar et al. (2014) for an analysis of the number of linear pieces.", "startOffset": 4, "endOffset": 27}, {"referenceID": 30, "context": "Sparsifying the connections will make the compositions more explicit and meaningful (Xie et al., 2015; Wu et al., 2010).", "startOffset": 84, "endOffset": 119}, {"referenceID": 28, "context": "Sparsifying the connections will make the compositions more explicit and meaningful (Xie et al., 2015; Wu et al., 2010).", "startOffset": 84, "endOffset": 119}, {"referenceID": 24, "context": "The relationship between score matching and autoencoder was discovered by Vincent (2011) and Swersky et al. (2011). Our work can be considered a sharpened specialization of the above mentioned connection and relationship, where the piecewise linear structure in ConvNet greatly simplifies the matter by getting rid of the complicated second derivative terms, so that the contrastive divergence gradient becomes exactly proportional to the gradient of the reconstruction error, which is not the case in general score matching estimator.", "startOffset": 93, "endOffset": 115}, {"referenceID": 24, "context": "The relationship between score matching and autoencoder was discovered by Vincent (2011) and Swersky et al. (2011). Our work can be considered a sharpened specialization of the above mentioned connection and relationship, where the piecewise linear structure in ConvNet greatly simplifies the matter by getting rid of the complicated second derivative terms, so that the contrastive divergence gradient becomes exactly proportional to the gradient of the reconstruction error, which is not the case in general score matching estimator. Also, our work gives a novel hierarchical realization of the relationship between probability model and auto-encoder, as well as an explicit hierarchical realization of auto-encoder based sampling of Alain & Bengio (2014). The connection with Hopfied network also appears new.", "startOffset": 93, "endOffset": 758}, {"referenceID": 15, "context": "Unlike Lu et al. (2016), the generative ConvNets in our experiments are learned from scratch without relying on the pre-learned filters of existing ConvNets.", "startOffset": 7, "endOffset": 24}], "year": 2016, "abstractText": "The convolutional neural network (ConvNet or CNN) is a powerful discriminative learning machine. In this paper, we show that a generative random field model that we call generative ConvNet can be derived from the discriminative ConvNet. The probability distribution of the generative ConvNet model is in the form of exponential tilting of a reference distribution. Assuming re-lu non-linearity and Gaussian white noise reference distribution, we show that the generative ConvNet model contains a representational structure with multiple layers of binary activation variables. The model is nonGaussian, or more precisely, piecewise Gaussian, where each piece is determined by an instantiation of the binary activation variables that reconstruct the mean of the Gaussian piece. The Langevin dynamics for synthesis is driven by the reconstruction error, and the corresponding gradient descent dynamics converges to a local energy minimum that is auto-encoding. As for learning, we show that the contrastive divergence learning tends to reconstruct the observed images. Finally, we show that the maximum likelihood learning algorithm can generate realistic natural images.", "creator": "LaTeX with hyperref package"}}}