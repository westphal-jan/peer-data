{"id": "1602.03348", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "10-Feb-2016", "title": "Iterative Hierarchical Optimization for Misspecified Problems (IHOMP)", "abstract": "Reinforcement Learning (RL) aims to learn an optimal policy for a Markov Decision Process (MDP). For complex, high-dimensional MDPs, it may only be feasible to represent the policy with function approximation. If the policy representation used cannot represent good policies, the problem is misspecified and the learned policy may be far from optimal. We introduce IHOMP as an approach for solving misspecified problems. IHOMP iteratively refines a set of specialized policies based on a limited representation. We refer to these policies as policy threads. At the same time, IHOMP stitches these policy threads together in a hierarchical fashion to solve a problem that was otherwise misspecified. We prove that IHOMP enjoys theoretical convergence guarantees and extend IHOMP to exploit Option Interruption (OI) enabling it to learn where policy threads can be reused. Our experiments demonstrate that IHOMP can find near-optimal solutions to otherwise misspecified problems and that OI can further improve the solutions. However, the complexity of OI can also lead to high-dimensional complexity which can lead to mispecified problems. IHOMP provides a mechanism for IHOMP to gain the computational power required to solve OI problems through an approach in which we can explore the complexity of IHOMP's own implementation. The following experiments show that in the IHOMP program, we can use OI to solve many problems in a single program, rather than building a system using arbitrary IHOMP operations. We can use IHOMP to increase the performance of many different implementations in the framework. Our first step is to use IHOMP for generating a simple algorithm in which a rule can be applied to a problem. For example, we can use a program like IHOMP to implement a new rule based on a simple IHOMP algorithm that we can see in our examples. The first step is to implement a code that can be rewritten using the algorithm for the IHOMP algorithm. We can then use IHOMP to implement the algorithm to implement the code using the algorithm as a class. Here, we can use the algorithm for the following simple algorithm: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54", "histories": [["v1", "Wed, 10 Feb 2016 12:27:04 GMT  (5461kb,D)", "https://arxiv.org/abs/1602.03348v1", "arXiv admin note: text overlap witharXiv:1506.03624"], ["v2", "Tue, 7 Jun 2016 20:05:14 GMT  (4958kb,D)", "http://arxiv.org/abs/1602.03348v2", "arXiv admin note: text overlap witharXiv:1506.03624"]], "COMMENTS": "arXiv admin note: text overlap witharXiv:1506.03624", "reviews": [], "SUBJECTS": "cs.LG cs.AI", "authors": ["daniel j mankowitz", "timothy a mann", "shie mannor"], "accepted": false, "id": "1602.03348"}, "pdf": {"name": "1602.03348.pdf", "metadata": {"source": "CRF", "title": "Iterative Hierarchical Optimization for Misspecified Problems (IHOMP)", "authors": ["Daniel J. Mankowitz", "Timothy A. Mann"], "emails": ["danielm@tx.technion.ac.il", "timothymann@google.com", "shie@ee.technion.ac.il"], "sections": [{"heading": "1 Introduction", "text": "Reinforcement Learning (RL) algorithms can learn near-optimal solutions to well-defined problems. However, real-world problems rarely come in the form of a concrete problem description. A human has to translate the poorly defined target problem into a concrete problem description. A Misspecified Problem (MP) occurs when an optimal solution to the problem description is inadequate in the target problem. Unfortunately, creating a well-defined problem description is a challenging art. Furthermore, MPs can have serious consequences in many domains ranging from smart-grids [Abiri-Jahromi et al., 2013, Wu et al., 2010] and robotics [Smart and Kaelbling, 2002] to inventory management systems [Mann and Mannor, 2014]. In this paper, we introduce a hierarchical approach that mitigates the consequences of problem misspecification.\nRL problems are often described as Markov Decision Processes [Sutton and Barto, 1998, MDPs]. A solution to a MDP is a function that generates an action when presented with the current state, called a policy. The solution to a MDP is any policy that maximizes the long term sum of rewards. For problems with continuous, high dimensional state-spaces, explicitly representing the policy is infeasible, thus for the remainder of this paper we restrict our discussion to linearly parametrized policy representations [Sutton, 1996, Roy and How, 2013].1\n1Our results are generalizable and complementary to non-linear parametric policy representations.\nar X\niv :1\n60 2.\n03 34\n8v 2\n[ cs\n.L G\nWhy are problems misspecified? While a problem description can be misspecified for many reasons, one important case is due to the state representation. It is well established in the machine learning [Levi and Weiss, 2004, Zhou et al., 2009] and RL [Konidaris et al., 2011] literature that \u201cgood\u201d features can have a dramatic impact on performance. Finding \u201cgood\u201d features to represent the state is a challenging domain specific problem that is generally considered outside of the scope of RL. Unfortunately, domain experts may not supply useful features either because they do not fully understand the target problem or the technicalities of reinforcement learning.\nIn addition, we may prefer a MP with a limited state representation for several reasons: (1) Regularization: We wish to have a limited feature representation to improve the generalization and avoid overfitting [Singh et al., 1995, Geramifard et al., 2012]. (2) Memory and System constraints: Only a finite number of the features can be used due to computational constraints [Roy and How, 2013, Singh et al., 1995]. In real-time systems, querying a feature may take too long. In physical systems, the sensor required to measure a desired feature may be prohibitively expensive. (3) Learning on Large Data: After learning on large amounts of data, augmenting a feature set with new features to get improved performance is non-trivial and often inefficient [Geramifard et al., 2012].\nHow can we mitigate misspecification? Learning a hierarchical policy can mitigate the problems associated with a MP and contrast this against a flat policy approach where a single, parameterized policy is used to solve the entire MDP.\nTo illustrate how learning a hierarchical policy can repair MPs, consider the S-shaped domain shown in Figure 1a. To solve the task the agent must move from the bottom left corner to the goal region denoted by the letter \u2018G\u2019 in the top right. The state representation only permits policies that move in a straight line. So the problem is misspecified, and it is not solvable with a flat policy approach (Figure 1a.i). However, if we break up the state-space, as shown in Figure 1a.ii, and learn one policy for each cell, the problem is solvable.\nThe partial policies shown in Figure 1a.ii are an example of abstract actions, called options [Sutton et al., 1999], macro-actions [Hauskrecht et al., 1998, He et al., 2011], or skills [Konidaris and Barto, 2009]. Learning useful options has been a topic of intense research [McGovern and Barto, 2001, Moerman, 2009, Konidaris and Barto, 2009, Brunskill and Li, 2014, Hauskrecht et al., 1998]. However, previous approaches have proposed algorithms for learning options to learn or plan faster. In contrast, our objective is to learn options to repair a MP.\nProposed Algorithm: We introduce a meta-algorithm, Iterative Hierarchical Optimization for Misspecified Problems (IHOMP), that uses an RL algorithm as a \u201cblack box\u201d to iteratively learn options that repair MPs. To force the options to specialize, IHOMP uses a partition of the state-space and trains one option for each class in the partition (Figure 1b). Any arbitrary partitioning scheme can be used, however the partition impacts performance. During an iteration of IHOMP, an RL algorithm updates each option. The options may be initialized arbitrarily, but after the first iteration options with access to a goal region or non-zero rewards will learn how to exploit those rewards (e.g., Figure 1b, Iteration 1). On further iterations, the newly acquired options propagate reward back to other regions of the state-space. Thus, options that previously had no reward signal exploit the rewards of other options that have received meaningful reward signals (e.g., Figure 1b, Iterations 2 and 5). Although each option is only learned over a single partition class, it can be initialized in any state.\nWhy partitions? If all options are trained on all data, then the options would not specialize defeating the purpose of learning multiple policies. Partitions are necessary to foster specialization. Natural partitionings arise in many different applications and are often easy to design. Consider navigation tasks (which we use in this paper for ease of visualization), which are ever-present in robotics [Smart and Kaelbling, 2002], where partitions naturally lead an agent from one location to another in the state space. In addition, partitions are well suited to cyclical tasks; that is, tasks that have repeatable cycles (For example, a yearly cycle of 12 months). Here the state space can be easily partitioned based on time. Examples include inventory management systems [Mann and Mannor, 2014] as well as maintenance scheduling of generation units and transmission lines in smart grids [Abiri-Jahromi et al., 2013, Wu et al., 2010].\nAutomatically Learning partitions: The availability of a pre-defined partitioning of the state space is a strong assumption in some domains. We have developed a relaxation to this assumption that can enable partitions to be learned automatically using Regularized Option Interruption (ROI) [Mankowitz et al., 2014, Sutton et al., 1999].\nContributions: Our main contributions are: (1) Introducing Iterative Hierarchical Optimization for Misspecified Problems (IHOMP), which learns options to repair and solve MPs. (2) Theorem 1 shows that IHOMP converges to a near-optimal solution relating the quality of the learned policy to the quality of the options learned by the \u201cblack box\u201d RL algorithm. (3) Theorem 2 proves that Regularized Option Interruption (ROI) can be safely incorporated into IHOMP. (4) Experiments demonstrating that, given a misspecified problem, IHOMP can learn options to repair and solve the problem. Experiments showing IHOMP-ROI learning partitions and discovering reusable options. This divide-and-conquer approach may also enable us to scale and solve larger MDPs."}, {"heading": "2 Background", "text": "Let M = \u3008S,A, P,R, \u03b3\u3009 be an MDP, where S is a (possibly infinite) set of states, A is a finite set of actions, P is a mapping from state-action pairs to probability distributions over next states, R maps each state-action pair to a reward in [0, 1], and \u03b3 \u2208 [0, 1) is the discount factor. A policy \u03c0(a|s) gives the probability of executing action a \u2208 A from state s \u2208 S. Let M be an MDP. The value function of a policy \u03c0 with respect to a state s \u2208 S is V \u03c0M (s) = E [\u2211\u221e\nt=1 \u03b3 t\u22121R(st, at)|s0 = s\n] where the expectation is taken with respect to the trajectory produced\nby following policy \u03c0. The value function of a policy \u03c0 can also be written recursively as V \u03c0M (s) = Ea\u223c\u03c0(\u00b7|s) [R(s, a)] + \u03b3Es\u2032\u223cP (\u00b7|s,\u03c0) [V \u03c0(s\u2032)] , (1) which is known as the Bellman equation. The optimal Bellman equation can be written as V \u2217M (s) = maxa E [R(s, a)] + \u03b3Es\u2032\u223cP (\u00b7|s,\u03c0) [V \u2217(s\u2032)] . Let \u03b5 > 0. We say that a policy \u03c0 is \u03b5-optimal if V \u03c0M (s) \u2265 V \u2217M (s)\u2212 \u03b5 for all s \u2208 S. The action-value function of a policy \u03c0 is defined by Q\u03c0M (s, a) = Ea\u223c\u03c0(\u00b7|s) [R(s, a)]+\u03b3Es\u2032\u223cP (\u00b7|s,\u03c0) [V \u03c0(s\u2032)] , for a state s \u2208 S and an action a \u2208 A, and the optimal action-value function is denoted by Q\u2217M (s, a). Throughout this paper, we will drop the dependence on M when it is clear from the context."}, {"heading": "3 Learning Options", "text": "An option is typically defined by a triple o = \u3008I, \u03c0, \u03b2\u3009. However, we want to learn options that are both specialized to specific regions of the state-space but potentially reusable if they are useful in more general contexts. We focus on a special case of options, where an option \u03c3 is defined by a tuple \u03c3 = \u3008\u03c0\u03b8, \u03b2\u3009, where \u03c0\u03b8 is a parametric policy with parameter vector \u03b8 and \u03b2 : S \u2192 {0, 1} indicates whether the option has finished (\u03b2(s) = 1) or not (\u03b2(s) = 0) given the current state s \u2208 S. Given a set of options \u03a3 with size m \u2265 1, the inter-option policy is defined by \u00b5 : S \u2192 [m] where S is the state-space and [m] is the index set over the options in \u03a3. An inter-option policy selects which options to execute from the current state by returning the index of one of the options. By defining inter-option policies to select an index (rather than the options), we can use the same policy even as the set of options is adapting.\nFigure 1b shows an arbitrary partitioning P , consisting of 5 sub-partitions {Pi|i = 1 \u00b7 \u00b7 \u00b7 5}, defined over the original MDP\u2019s state space. Each Pi is initialized with an arbitrary option and its corre-\nsponding Local-MDP M \u2032i . Local-MDP M \u2032i (see supplementary material for a full definition) is an episodic MDP that terminates once the agent escapes from Pi and upon terminating receives a reward equal to the value of the state the agent would have transitioned to in the original MDP. Therefore, we construct a modified MDP called a Local-MDP and apply a planning or RL algorithm to solve it. The resulting solution (policy) is a specialized option.\nGiven a \u201cgood\u201d set of options, planning can be significantly faster [Sutton et al., 1999, Mann and Mannor, 2014]. However, in many domains we may not be given a good set of options. Therefore it is necessary to learn and improve this set of options. In the next section, we introduce an algorithm for dynamically learning and improving options using iterative hierarchical optimization."}, {"heading": "4 Iterative Hierarchical Optimization for Misspecified Problems (IHOMP)", "text": "Iterative Hierarchical Optimization for Misspecified Problems (IHOMP, Algorithm 1) takes the original MDP M , a partition P over the state-space and a number of iterations K \u2265 1 and returns a pair \u3008\u00b5,\u03a3\u3009 containing an inter-option policy \u00b5 and a set of options \u03a3. The number of options m = |P| is equal to the number of classes (sub-partitions) in the partition P (line 1). The inter-option policy \u00b5 returned by IHOMP is defined (line 2) by \u00b5(s) = arg maxi\u2208[m] I {s \u2208 Pi} , where I{\u00b7} is the indicator function returning 1 if its argument is true and 0 otherwise and Pi denotes the ith class in the partition P . Thus \u00b5 simply returns the index of the option associated with the partition class containing the current state. On line 3, IHOMP initializes \u03a3 with arbitrary options (IHOMP can also be initialized with options that we believe might be useful to speed up learning).\nAlgorithm 1 Iterative Hierarchical Optimization for Misspecified Problems (IHOMP) Require: M{MDP}, P{Partitioning of S}, K{Iterations}\n1: m\u2190 |P| {# of partitions.} 2: \u00b5(s) = arg maxi\u2208[m] I{s \u2208 Pi} 3: Initialize \u03a3 with m options. {1 option per partition.} 4: for k = 1, 2, . . . ,K do {Do K iterations.} 5: for i = 1, 2, . . . ,m do {One update per option.} 6: Policy Evaluation: 7: Evaluate \u00b5 with \u03a3 to obtain V \u3008\u00b5,\u03a3\u3009M 8: Option Update: 9: Construct Local-MDP M \u2032i from M & V \u3008\u00b5,\u03a3\u3009 M\n10: Solve M \u2032i obtaining policy \u03c0\u03b8 11: \u03c3\u2032i \u2190 \u3008\u03c0\u03b8, \u03b2i\u3009 12: Replace \u03c3i in \u03a3 by \u03c3\u2032i 13: end for 14: end for 15: return \u3008\u00b5,\u03a3\u3009\nNext (lines 4\u201314), IHOMP performs K iterations. In each iteration, IHOMP updates the options in \u03a3 (lines 5\u201313). Note that the value of a option depends on how it is combined with other options. If we allowed all options to change simultaneously, the options could not reliably propagate value off of each other. Therefore, IHOMP updates each option individually. Multiple iterations are needed so that the option set can converge (Figure 1b).\nThe process of updating an option (lines 7\u201312) starts by evaluating \u00b5 with the current option-set \u03a3 (line 7). Any number of policy evaluation algorithms could be used here, such as TD(\u03bb) with function approximation [Sutton and Barto, 1998] or LSTD [Boyan, 2002], modified to be used with options. In our experiments, we used a straightforward variant of LSTD [Sorg and Singh, 2010]. Then we use the original MDP M to construct a Local-MDP M \u2032 (line 9). Next, IHOMP uses a planning or RL algorithm to approximately solve the Local-MDP M \u2032 returning a parametrized policy \u03c0\u03b8 (line 10). Any planning or RL algorithm for regular MDPs could fill this role provided that it produces a parametrized policy. However, in our experiments, we used a simple actor-critic PG algorithm, unless otherwise stated. Then a new option \u03c3\u2032i = \u3008\u03c0\u03b8, \u03b2i\u3009 is created (line 11) where \u03c0\u03b8 is the policy\nderived on line 10 and \u03b2i(s) = {\n0 if s \u2208 Pi 1 otherwise . The definition of \u03b2i means that the option will\nterminate only if it leaves the ith partition. Finally, we update the option set \u03a3 by replacing the ith\noption with \u03c3\u2032i (line 12). It is important to note that in IHOMP, updating an option is equivalent to solving a Local-MDP."}, {"heading": "5 Analysis of IHOMP", "text": "We provide the first convergence guarantee for combining hierarchically and iteratively learning options in a continuous state MDP using IHOMP (Lemma 1 and Lemma 2, proven in the supplementary material). We use this guarantee as well as Lemma 2 to prove Theorem 1. This theorem enables us to analyze the quality of the inter-option policy returned by IHOMP. It turns out that the quality of the policy depends critically on the quality of the option learning algorithm. An important parameter for determining the quality of a policy returned by IHOMP is the misspecification error defined below.\nDefinition 1 Let P be a partition over the target MDP\u2019s state-space. The misspecification error is \u03b7P = max\ni\u2208[m] \u03b7i , (2)\nwhere \u03b7i is the smallest \u03b7i \u2265 0, such that V \u2217M \u2032i (s)\u2212V \u03c0\u03b8 M \u2032i (s) \u2264 \u03b7i , for all s \u2208 Pi and \u03c0\u03b8 is the policy returned by the option learning algorithm executed on M \u2032i .\nThe misspecification error quantifies the quality of the Local-MDP solutions returned by our option learning algorithm. If we used an exact solver to learn options, then \u03b7P = 0. However, if we use an approximate solver, then \u03b7P will be non-zero and the quality will depend on the partition P . Generally, using finer grain partitions will decrease \u03b7P . However, Theorem 1 reveals that adding too many options can also negatively impact the returned policy\u2019s quality.\nTheorem 1 Let \u03b5 > 0. If we run IHOMP with partition P for K \u2265 log\u03b3 (\u03b5(1\u2212 \u03b3)) iterations, then the algorithm returns stitching policy \u03d5 = \u3008\u00b5,\u03a3\u3009 such that\n\u2016V \u2217M \u2212 V \u03d5 M\u2016\u221e \u2264 m\u03b7P (1\u2212 \u03b3)2 + \u03b5 , (3)\nwhere m is the number of partition classes in P .\nThe proof of Theorem 1 is divided into three parts (a complete proof is given in the supplementary material). The main challenge is that updating one option can impact the value of other options. Our analysis starts by bounding the impact of updating one option. Note that \u03a3 represents a option set and \u03a3i represents a option set where we have updated the ith option (corresponding to the ith partition class Pi) in the set. In the first part, we show that the error between V \u2217M , the globally optimal value function, and V \u3008\u00b5,\u03a3i\u3009M , is a contraction when s \u2208 Pi and is bound by \u2016V \u2217M \u2212 V \u3008\u00b5,\u03a3\u3009 M \u2016\u221e + \u03b7P 1\u2212\u03b3 otherwise (Lemma 1). In the second part, we apply an inductive argument to show that updating all m options results in a \u03b3 contraction over the entire state space (Lemma 2). In the third part, we apply this contraction recursively, which proves Theorem 1.\nThis provides the first theoretical guarantees of convergence to a near optimal solution when combining hierarchically, and iteratively learning, a set of options \u03a3 in a continuous state MDP. Theorem 1 tells us that when the misspecification error is small, IHOMP returns a near-optimal inter-option policy. The first term on the right hand side of (3) is the approximation error. This is the loss we pay for the parametrized class of policies that we learn options over. Since m represents the number of classes defined by the partition, we now have a formal way of analyzing the effect of the partitioning structure. In addition, complex options do not need to be designed by a domain expert; only the partitioning needs to be provided a-priori. The second term is the convergence error. It goes to 0 as the number of iterations K increases.\nThe guarantee provided by Theorem 1 may appear similar to [Hauskrecht et al., 1998, Theorem 1]. However, Hauskrecht et al. [1998] derive options only at the beginning of the learning process and do not update them. On the other hand, IHOMP updates its option-set dynamically by propagating value throughout the state space during each iteration. Thus, IHOMP does not require prior knowledge of the optimal value function.\nTheorem 1 does not explicitly present the effect of policy evaluation error, which occurs with any approximate policy evaluation technique. However, if the policy evaluation error is bounded by \u03bd > 0, then we can simply replace \u03b7P in (3) with (\u03b7P + \u03bd). Again, smaller policy evaluation error leads to smaller approximation error."}, {"heading": "6 Learning Partitions via Regularized Option Interruption", "text": "So far IHOMP has assumed a partition is given a-priori. However, it may be non-trivial to design a partition and, in many cases, the partition may be sub-optimal. To relax this assumption, we incorporate Regularized Option Interruption (ROI) [Mankowitz et al., 2014] into this work to enable IHOMP to automatically learn a near-optimal partition from an initially misspecified problem.\nIHOMP keeps track of the action value function Q\u3008\u00b5,\u03a3\u3009(s, j) which represents the expected value of being in state s \u2208 S and executing option j, given the inter-option policy \u00b5 and option set \u03a3. ROI uses this estimate of the action-value function to enable the agent to choose when to switch options according to the following termination rule:\n\u03b2j(s, t) = { 1 if Q\u3008\u00b5,\u03a3\u3009(s, j) < V \u3008\u00b5,\u03a3\u3009(s)\u2212 \u03c1 0 otherwise . (4)\nHere \u03b2j(s, t) corresponds to the termination probability of the jth option partition and V \u3008\u00b5,\u03a3\u3009(s) = maxi\u2208[m]Q\n\u3008\u00b5,\u03a3\u3009(s, i). This rule is illustrated in Figure 2. A user has designed a partition resulting in a MP (Figure 2a) compared to the optimal partition for this domain (Figure 2b). IHOMP applies ROI to \u2018modify\u2019 the initial partition into the optimal one. By learning the optimal action-value function Q\u3008\u2217,\u00b5,\u03a3\u3009(s, j), IHOMP builds a near-optimal partition (Figure 2c) that is implicitly stored within this action-value function. That is, if the agent is executing an option in partition class 1, and the value of continuing with option 1, Q\u3008\u00b5,\u03a3\u3009(s, 1), is less than V \u3008\u00b5,\u03a3\u3009(s)\u2212\u03c1 for some regularization function \u03c1 (see the x location in Figure 2c), then switch to the new option partition (\u03b2j(s, t) = 1). Otherwise, continue executing the current option (see the y location in Figure 2c).\nThis leads to a new algorithm IHOMP-ROI (IHOMP with Regularized Option Interruption). The algorithm can be found in the supplementary material. The key difference between IHOMP and IHOMP-ROI is applying ROI during the policy evaluation step after\neach of the m options have been updated. IHOMP-ROI automatically learns an improved partition between iterations. We show that ROI can be safely incorporated into IHOMP in Theorem 2. The theorem shows that incorporating ROI can only improve the policy produced by IHOMP. The full proof is given in the supplementary material.\nTheorem 2 (IHOMP-ROI Approximate Convergence) Eq. (3) also holds for IHOMP-ROI."}, {"heading": "7 Experiments and Results", "text": "We performed experiments on three well-known RL benchmarks: Mountain Car (MC), Puddle World (PW) [Sutton, 1996] and the Pinball domain [Konidaris and Barto, 2009]. We also perform experiments in a sub-domain of Minecraft 2. The MC and Minecraft domains have similar results to PW and therefore have been moved to the supplementary material. We use two variations for the Pinball domain, namely maze-world (moved to supplementary material), which we created, and pinball-world which is one of the standard pinball benchmark domains. Finally, we created a domain which we call the Two Rooms domain to demonstrate how IHOMP-ROI can improve partitions.\nIn each experiment, we defined a MP, where no flat policy is adequate, and in some of the tasks, cannot solve the task at all. These experiments simulate situations where the policy representation is\n2https://minecraft.net/en/\nconstrained to avoid overfitting, manage system constraints, or coping with poorly designed features. In each case, IHOMP learns a significantly better policy compared to the non-hierarchical approach. In the Two Rooms domain, IHOMP-ROI improves the initial partition. Our experiments demonstrate potential to scale up to higher dimensional domains by hierarchically combining options over simple representations (A Video of IHOMP solving the Pinball tasks and the Minecraft sub-domain can be found in the supplementary material).\nIHOMP is a meta-algorithm. We provide an algorithm for Policy Evaluation (PE) and Policy Learning (PL). For the MC and PW domains, we used SMDP-LSTD [Sorg and Singh, 2010] for PE and a modified version of Regular-Gradient Actor-Critic (RG-AC) [Bhatnagar et al., 2009] for PL (see supplementary material for details). In the Pinball domains, we used Nearest-Neighbor Function Approximation (NN-FA) for PE and UCB Random Policy Search (UCB-RPS) for PL. In the two rooms domain, we use a variation of LSTDQ with Option Interruption for PE and RG-AC for PL.\nFor the MC, PW, and Two Rooms domains, each intra-option policy is represented as a probability distribution over actions (independent of the state). We compare their performance to the original misspecified problem using a flat policy with the same representation. Grid-like partitions are generated for each task. Binary-grid features are used to estimate the value function. In the Pinball domains, each option is represented by 5 polynomial features corresponding to each state dimension and a bias term. The value function is represented by a KD-Tree containing 1000 state-value pairs uniformly sampled in the domain. A value for a particular state is obtained by assigning the value of the nearest neighbor to that state that is contained within the KD-tree. These are example representations. In principal, any value function and policy representation that is representative of the domain can be utilized.\nPuddle World: Puddle World is a continuous 2-dimensional world containing two puddles as shown in Figure 3a. A successful agent (red ball) should navigate to the goal location (blue square), avoiding the puddles. The state space is the \u3008x, y\u3009 location of the agent. Initially, the agent is provided with a misspecified problem. That is, a flat policy that can only move in a single direction (thus it cannot avoid the puddles). Figure 3b compares this flat policy with IHOMP (for a 2\u00d7 2 grid partition (Four options)). The flat policy achieves low average reward. However, IHOMP turns the flat policy into options and hierarchically composes these options together, resulting in a richer solution space and a higher average reward as seen in Figure 3b. This is comparable to the approximately optimal average reward attained by executing Approximate Value Iteration (AVI) for a huge number of iterations. In this experiment IHOMP is not initiated in the partition class containing the goal state but still achieves near-optimal convergence after only 2 iterations.\nFigure 3c compares the performance of different partitions where a 1 \u00d7 1 grid represents the flat policy of the initially misspecified problem. The option learning error \u03b7P is significantly smaller for all the partitions greater than 1\u00d7 1, resulting in lower cost. On the other hand, according to Theorem 1, adding more options m increases the cost. A trade off therefore exists between \u03b7P and m. In practice, \u03b7P tends to dominate m. In addition to the trade off, the importance of the partition design is evident when analyzing the cost of the 3\u00d7 3 and 4\u00d7 4 grids. In this scenario, the 3\u00d7 3 partition design is better suited to Puddle World than the 4\u00d7 4 partition, resulting in lower cost. Pinball: We tested IHOMP on the challenging pinball-world task (Figure 4a) Konidaris and Barto [2009]. The agent is initially provided with a 5-feature flat policy \u30081, x, y, x\u0307, y\u0307\u3009. This results in a misspecified problem as the agent is unable to solve the task using this limited representation as shown by the average reward in Figure 4b. Using IHOMP with a 4\u00d7 3\u00d7 1\u00d7 1 grid, 12 options were\nlearned. IHOMP clearly outperforms the flat policy as shown in Figure 4b. It is less than optimal but still manages to sufficiently perform the task (see value function, Figure 4c). The drop in performance is due to a complicated obstacle setup, non-linear dynamics and partition design. Nevertheless, this shows that IHOMP can produce a reasonable solution with a limited representation.\nImproving Partitions: Providing a \u2018good\u2019 option partitioning a-priori is a strong assumption. It may be non-trivial to design the partitioning especially in continuous, high-dimensional domains. A sub-optimal partitioning may still mitigate misspecification, but it will not result in a near-optimal solution. To relax this assumption, we have incorporated Regularized Option Interruption into IHOMP to produce the IHOMP-ROI Algorithm. This algorithm learns the options and improves the partition, effectively determining where the options should be executed in the state space.\nWe tested IHOMP-ROI on the two rooms domain shown in Figure 5a. The agent (red ball) needs to navigate to the goal region (blue square). The policy parameterization is limited to a distribution over actions (moving in a single direction). This limited representation results in a MP as the agent is unable to traverse between the two rooms. If we use IHOMP with a sub-optimal partitioning containing two options as shown by the red and green cells in Figure 5b, the problem is still misspecified. Here, the agent leaves the red cell and immediately gets trapped behind the wall whilst in the green cell. Using IHOMP-ROI, as shown in Figure 5c, the agent learns both the options and a partition such that the agent can navigate to the goal. The green region in the bottom left corner comes about as a function approximation error but does not prevent the agent from reaching the goal. If reader looks carefully in Figure 5c, they will notice something unexpected. The optimal partitioning learned for the Two Rooms domain includes executing the red option in region B. This is intuitive given the parameterizations learned for each of the options. The red option has a dominant right action whereas the green option has a dominant upward action. When the agent in region B, it makes more sense to execute the red option to reach the goal. Thus, IHOMP-ROI provides an effective way to, not only learn an optimal partition, but to also discover where options should be reused."}, {"heading": "8 Discussion", "text": "We introduced IHOMP a RL planning algorithm for iteratively learning options and an inter-option policy [Sutton et al., 1999] to repair a MP. We provide theoretical results for IHOMP that directly relate the quality of the final inter-option policy to the misspecification error. IHOMP is the first algorithm that provides theoretical convergence guarantees while iteratively learning a set of options in a continuous state space. In addition, we have developed IHOMP-ROI which makes use of regularized option interruption [Sutton et al., 1999, Mankowitz et al., 2014] to learn an improved partition to solve an initially misspecified problem. IHOMP-ROI is also able to discover regions in the state space where the options should be reused. In high-dimensional domains, partitions can be learned from expert demonstrations [Abbeel and Ng, 2005] and intra-option policies can be\nrepresented as Deep Q-Networks [Mnih, 2015]. Option reuse can be especially useful for transfer learning [Tessler et al., 2016] and multi-agent settings [Garant et al., 2015]."}], "references": [{"title": "Exploration and apprenticeship learning in reinforcement learning", "author": ["Pieter Abbeel", "Andrew Y Ng"], "venue": "In Proceedings of the 22nd International Conference on Machine Learning,", "citeRegEx": "Abbeel and Ng.,? \\Q2005\\E", "shortCiteRegEx": "Abbeel and Ng.", "year": 2005}, {"title": "A two-stage framework for power transformer asset maintenance management\u2014part i: Models and formulations", "author": ["Amir Abiri-Jahromi", "Masood Parvania", "Francois Bouffard", "Mahmud Fotuhi-Firuzabad"], "venue": "Power Systems, IEEE Transactions on,", "citeRegEx": "Abiri.Jahromi et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Abiri.Jahromi et al\\.", "year": 2013}, {"title": "Technical update: Least-squares temporal difference learning", "author": ["Justin A Boyan"], "venue": "Machine Learning,", "citeRegEx": "Boyan.,? \\Q2002\\E", "shortCiteRegEx": "Boyan.", "year": 2002}, {"title": "PAC-inspired option discovery in lifelong reinforcement learning", "author": ["Emma Brunskill", "Lihong Li"], "venue": null, "citeRegEx": "Brunskill and Li.,? \\Q2014\\E", "shortCiteRegEx": "Brunskill and Li.", "year": 2014}, {"title": "Accelerating Multi-agent Reinforcement Learning with Dynamic Co-learning", "author": ["Daniel Garant", "Bruno C. da Silva", "Victor Lesser", "Chongjie Zhang"], "venue": "Technical report,", "citeRegEx": "Garant et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Garant et al\\.", "year": 2015}, {"title": "A bayesian approach to finding compact representations for reinforcement learning", "author": ["A Geramifard", "S Tellex", "D Wingate", "N Roy", "JP How"], "venue": "In European Workshops on Reinforcement Learning (EWRL),", "citeRegEx": "Geramifard et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Geramifard et al\\.", "year": 2012}, {"title": "Hierarchical solution of markov decision processes using macro-actions", "author": ["Milos Hauskrecht", "Nicolas Meuleau", "Leslie Pack Kaelbling", "Thomas Dean", "Craig Boutilier"], "venue": "In Proceedings of the 14th Conference on Uncertainty in AI,", "citeRegEx": "Hauskrecht et al\\.,? \\Q1998\\E", "shortCiteRegEx": "Hauskrecht et al\\.", "year": 1998}, {"title": "Efficient planning under uncertainty with macro-actions", "author": ["Ruijie He", "Emma Brunskill", "Nicholas Roy"], "venue": "Journal of Artificial Intelligence Research,", "citeRegEx": "He et al\\.,? \\Q2011\\E", "shortCiteRegEx": "He et al\\.", "year": 2011}, {"title": "Value function approximation in reinforcement learning using the fourier basis", "author": ["G.D. Konidaris", "S. Osentoski", "P.S. Thomas"], "venue": "In Proceedings of the Twenty-Fifth AAAI Conference on Artificial Intelligence,", "citeRegEx": "Konidaris et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Konidaris et al\\.", "year": 2011}, {"title": "Skill discovery in continuous reinforcement learning domains using skill chaining", "author": ["George Konidaris", "Andrew G Barto"], "venue": "In NIPS", "citeRegEx": "Konidaris and Barto.,? \\Q2009\\E", "shortCiteRegEx": "Konidaris and Barto.", "year": 2009}, {"title": "Learning object detection from a small number of examples: the importance of good features", "author": ["Kobi Levi", "Yair Weiss"], "venue": "In Computer Vision and Pattern Recognition,", "citeRegEx": "Levi and Weiss.,? \\Q2004\\E", "shortCiteRegEx": "Levi and Weiss.", "year": 2004}, {"title": "Time regularized interrupting options", "author": ["Daniel J Mankowitz", "Timothy A Mann", "Shie Mannor"], "venue": null, "citeRegEx": "Mankowitz et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Mankowitz et al\\.", "year": 2014}, {"title": "Scaling up approximate value iteration with options: Better policies with fewer iterations", "author": ["Timothy A Mann", "Shie Mannor"], "venue": "In Proceedings of the 31 st ICML,", "citeRegEx": "Mann and Mannor.,? \\Q2014\\E", "shortCiteRegEx": "Mann and Mannor.", "year": 2014}, {"title": "Automatic Discovery of Subgoals in Reinforcement Learning using Diverse Density", "author": ["Amy McGovern", "Andrew G Barto"], "venue": "In Proceedings of the 18th ICML,", "citeRegEx": "McGovern and Barto.,? \\Q2001\\E", "shortCiteRegEx": "McGovern and Barto.", "year": 2001}, {"title": "Mnih. Human-level control through deep reinforcement learning", "author": ["Volodymyr et. al"], "venue": "Nature,", "citeRegEx": "al.,? \\Q2015\\E", "shortCiteRegEx": "al.", "year": 2015}, {"title": "Hierarchical reinforcement learning: Assignment of behaviours to subpolicies by selforganization", "author": ["Wilco Moerman"], "venue": "PhD thesis,", "citeRegEx": "Moerman.,? \\Q2009\\E", "shortCiteRegEx": "Moerman.", "year": 2009}, {"title": "A tutorial on linear function approximators for dynamic programming and reinforcement learning", "author": ["N Roy", "JP How"], "venue": null, "citeRegEx": "Roy and How.,? \\Q2013\\E", "shortCiteRegEx": "Roy and How.", "year": 2013}, {"title": "Reinforcement learning with soft state aggregation", "author": ["Satinder P Singh", "Tommi Jaakkola", "Michael I Jordan"], "venue": "Advances in neural information processing systems,", "citeRegEx": "Singh et al\\.,? \\Q1995\\E", "shortCiteRegEx": "Singh et al\\.", "year": 1995}, {"title": "Effective reinforcement learning for mobile robots", "author": ["William D Smart", "Leslie Pack Kaelbling"], "venue": "In Robotics and Automation,", "citeRegEx": "Smart and Kaelbling.,? \\Q2002\\E", "shortCiteRegEx": "Smart and Kaelbling.", "year": 2002}, {"title": "Generalization in reinforcement learning: Successful examples using sparse coarse coding", "author": ["Richard Sutton"], "venue": "In Advances in neural information processing systems,", "citeRegEx": "Sutton.,? \\Q1996\\E", "shortCiteRegEx": "Sutton.", "year": 1996}, {"title": "Reinforcement Learning: An Introduction", "author": ["Richard Sutton", "Andrew Barto"], "venue": null, "citeRegEx": "Sutton and Barto.,? \\Q1998\\E", "shortCiteRegEx": "Sutton and Barto.", "year": 1998}, {"title": "Between MDPs and semi-MDPs: A framework for temporal abstraction in reinforcement learning", "author": ["Richard S Sutton", "Doina Precup", "Satinder Singh"], "venue": null, "citeRegEx": "Sutton et al\\.,? \\Q1999\\E", "shortCiteRegEx": "Sutton et al\\.", "year": 1999}, {"title": "A deep hierarchical approach to lifelong learning in minecraft", "author": ["Chen Tessler", "Shahar Givony", "Tom Zahavy", "Daniel J Mankowitz", "Shie Mannor"], "venue": "arXiv preprint arXiv:1604.07255,", "citeRegEx": "Tessler et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Tessler et al\\.", "year": 2016}, {"title": "Security-constrained generation and transmission outage scheduling with uncertainties", "author": ["Lei Wu", "Mohammad Shahidehpour", "Yong Fu"], "venue": "Power Systems, IEEE Transactions on,", "citeRegEx": "Wu et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Wu et al\\.", "year": 2010}, {"title": "Object tracking using sift features and mean shift", "author": ["Huiyu Zhou", "Yuan Yuan", "Chunmei Shi"], "venue": "Computer vision and image understanding,", "citeRegEx": "Zhou et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Zhou et al\\.", "year": 2009}], "referenceMentions": [{"referenceID": 18, "context": ", 2010] and robotics [Smart and Kaelbling, 2002] to inventory management systems [Mann and Mannor, 2014].", "startOffset": 21, "endOffset": 48}, {"referenceID": 12, "context": ", 2010] and robotics [Smart and Kaelbling, 2002] to inventory management systems [Mann and Mannor, 2014].", "startOffset": 81, "endOffset": 104}, {"referenceID": 8, "context": ", 2009] and RL [Konidaris et al., 2011] literature that \u201cgood\u201d features can have a dramatic impact on performance.", "startOffset": 15, "endOffset": 39}, {"referenceID": 5, "context": "(3) Learning on Large Data: After learning on large amounts of data, augmenting a feature set with new features to get improved performance is non-trivial and often inefficient [Geramifard et al., 2012].", "startOffset": 177, "endOffset": 202}, {"referenceID": 21, "context": "ii are an example of abstract actions, called options [Sutton et al., 1999], macro-actions [Hauskrecht et al.", "startOffset": 54, "endOffset": 75}, {"referenceID": 9, "context": ", 2011], or skills [Konidaris and Barto, 2009].", "startOffset": 19, "endOffset": 46}, {"referenceID": 18, "context": "Consider navigation tasks (which we use in this paper for ease of visualization), which are ever-present in robotics [Smart and Kaelbling, 2002], where partitions naturally lead an agent from one location to another in the state space.", "startOffset": 117, "endOffset": 144}, {"referenceID": 12, "context": "Examples include inventory management systems [Mann and Mannor, 2014] as well as maintenance scheduling of generation units and transmission lines in smart grids [Abiri-Jahromi et al.", "startOffset": 46, "endOffset": 69}, {"referenceID": 20, "context": "Any number of policy evaluation algorithms could be used here, such as TD(\u03bb) with function approximation [Sutton and Barto, 1998] or LSTD [Boyan, 2002], modified to be used with options.", "startOffset": 105, "endOffset": 129}, {"referenceID": 2, "context": "Any number of policy evaluation algorithms could be used here, such as TD(\u03bb) with function approximation [Sutton and Barto, 1998] or LSTD [Boyan, 2002], modified to be used with options.", "startOffset": 138, "endOffset": 151}, {"referenceID": 6, "context": "The guarantee provided by Theorem 1 may appear similar to [Hauskrecht et al., 1998, Theorem 1]. However, Hauskrecht et al. [1998] derive options only at the beginning of the learning process and do not update them.", "startOffset": 59, "endOffset": 130}, {"referenceID": 11, "context": "To relax this assumption, we incorporate Regularized Option Interruption (ROI) [Mankowitz et al., 2014] into this work to enable IHOMP to automatically learn a near-optimal partition from an initially misspecified problem.", "startOffset": 79, "endOffset": 103}, {"referenceID": 19, "context": "We performed experiments on three well-known RL benchmarks: Mountain Car (MC), Puddle World (PW) [Sutton, 1996] and the Pinball domain [Konidaris and Barto, 2009].", "startOffset": 97, "endOffset": 111}, {"referenceID": 9, "context": "We performed experiments on three well-known RL benchmarks: Mountain Car (MC), Puddle World (PW) [Sutton, 1996] and the Pinball domain [Konidaris and Barto, 2009].", "startOffset": 135, "endOffset": 162}, {"referenceID": 9, "context": "Pinball: We tested IHOMP on the challenging pinball-world task (Figure 4a) Konidaris and Barto [2009]. The agent is initially provided with a 5-feature flat policy \u30081, x, y, \u1e8b, \u1e8f\u3009.", "startOffset": 75, "endOffset": 102}, {"referenceID": 21, "context": "8 Discussion We introduced IHOMP a RL planning algorithm for iteratively learning options and an inter-option policy [Sutton et al., 1999] to repair a MP.", "startOffset": 117, "endOffset": 138}, {"referenceID": 0, "context": "In high-dimensional domains, partitions can be learned from expert demonstrations [Abbeel and Ng, 2005] and intra-option policies can be", "startOffset": 82, "endOffset": 103}, {"referenceID": 22, "context": "Option reuse can be especially useful for transfer learning [Tessler et al., 2016] and multi-agent settings [Garant et al.", "startOffset": 60, "endOffset": 82}, {"referenceID": 4, "context": ", 2016] and multi-agent settings [Garant et al., 2015].", "startOffset": 33, "endOffset": 54}], "year": 2016, "abstractText": "For complex, high-dimensional Markov Decision Processes (MDPs), it may be necessary to represent the policy with function approximation. A problem is misspecified whenever, the representation cannot express any policy with acceptable performance. We introduce IHOMP : an approach for solving misspecified problems. IHOMP iteratively learns a set of context specialized options and combines these options to solve an otherwise misspecified problem. Our main contribution is proving that IHOMP enjoys theoretical convergence guarantees. In addition, we extend IHOMP to exploit Option Interruption (OI) enabling it to decide where the learned options can be reused. Our experiments demonstrate that IHOMP can find near-optimal solutions to otherwise misspecified problems and that OI can further improve the solutions.", "creator": "LaTeX with hyperref package"}}}