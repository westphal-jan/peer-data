{"id": "1602.03779", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "11-Feb-2016", "title": "Network of Bandits insure Privacy of end-users", "abstract": "The distribution of the best arm identification task on the user's devices offers several advantages for application purposes: scalability, reduction of deployment costs and privacy. We propose a distributed version of the algorithm Successive Elimination using a simple architecture based on a single server which synchronizes each task executed on the user's devices. We show that this algorithm is optimal in terms of transmitted number of bits and is optimal up to logarithmic factors in terms to number of pulls per player. Finally, we propose an extension of this approach to distribute the contextual bandit algorithm Bandit Forest, which is able to finely exploit the user's data while guaranteeing the privacy.\n\n\n\n\n\nThe goal of this paper is to demonstrate the effectiveness of the algorithm in providing an application layer based on the best solution to this task and the application layer of a user's device. We will discuss this topic at a later stage of the paper.\nAcknowledgments\nThe authors thank all the participants, and to the following members of the Google team.\n\nThis paper aims to demonstrate that the method for assigning a given item to a user can be applied to an application layer based on the best approach to the task.\nThe research was supported by the Research Excellence in Computing (RPI) and the Science Fund of the National Institute of Standards and Technology, a Department of the Department of the European Parliament.\nIn recognition of the research funding, the Fund of the Institute for Advanced Research in Computer Science (ISFT) awarded an M.D. in M.D. for the M.D. for the M.D. from the NCCA/G.S.S.C. in collaboration with the Institute for Advanced Research in Computer Science.\n\nThe paper is available at the end of this week's edition of IEEE Transactions on Computer Science (FDR), a collaboration of the IEEE Society for Electrical and Computer Engineering, and by the IEEE Research Foundation.", "histories": [["v1", "Thu, 11 Feb 2016 15:55:59 GMT  (182kb,D)", "http://arxiv.org/abs/1602.03779v1", null], ["v2", "Sun, 14 Feb 2016 16:28:45 GMT  (182kb,D)", "http://arxiv.org/abs/1602.03779v2", null], ["v3", "Mon, 22 Feb 2016 12:26:15 GMT  (182kb,D)", "http://arxiv.org/abs/1602.03779v3", null], ["v4", "Tue, 1 Mar 2016 17:03:16 GMT  (182kb,D)", "http://arxiv.org/abs/1602.03779v4", null], ["v5", "Wed, 30 Mar 2016 15:32:59 GMT  (182kb,D)", "http://arxiv.org/abs/1602.03779v5", null], ["v6", "Tue, 26 Apr 2016 07:37:45 GMT  (0kb,I)", "http://arxiv.org/abs/1602.03779v6", "Theorem 1 does not hold. The proof is wrong. The speed-up factor is not N/2. So, please withdraw the manuscript. Regards, Rapha\\\"el F\\'eraud"], ["v7", "Mon, 6 Jun 2016 12:56:21 GMT  (234kb,D)", "http://arxiv.org/abs/1602.03779v7", "Theorem 1 does not hold. The proof is wrong. The speed-up factor is not N/2. So, please withdraw the manuscript. Regards, Rapha\\\"el F\\'eraud"], ["v8", "Mon, 19 Sep 2016 14:10:21 GMT  (258kb,D)", "http://arxiv.org/abs/1602.03779v8", null], ["v9", "Tue, 11 Oct 2016 07:28:28 GMT  (244kb,D)", "http://arxiv.org/abs/1602.03779v9", null], ["v10", "Mon, 5 Dec 2016 15:10:40 GMT  (326kb,D)", "http://arxiv.org/abs/1602.03779v10", null], ["v11", "Sat, 17 Dec 2016 17:24:05 GMT  (365kb,D)", "http://arxiv.org/abs/1602.03779v11", null], ["v12", "Mon, 6 Feb 2017 13:09:27 GMT  (369kb,D)", "http://arxiv.org/abs/1602.03779v12", null], ["v13", "Mon, 20 Mar 2017 14:04:42 GMT  (367kb,D)", "http://arxiv.org/abs/1602.03779v13", null], ["v14", "Wed, 29 Mar 2017 09:42:40 GMT  (369kb,D)", "http://arxiv.org/abs/1602.03779v14", null]], "reviews": [], "SUBJECTS": "cs.AI cs.DC cs.LG", "authors": ["rapha\\\"el f\\'eraud"], "accepted": false, "id": "1602.03779"}, "pdf": {"name": "1602.03779.pdf", "metadata": {"source": "CRF", "title": "Network of Bandits", "authors": ["Rapha\u00ebl F\u00e9raud"], "emails": ["raphael.feraud@orange.com"], "sections": [{"heading": "1 Introduction", "text": "Since the beginning of this century a vast quantity of connected devices has been deployed. These devices dialog together through wired and wireless networks thus generating infinite streams of events. By interacting with streams of events, machine learning algorithms are used for instance to optimize the choice of ads on a website, to choose the best human machine interface, to recommend products on a web shop, to insure self-care of set top boxes, to assign the best wireless network to mobile phones. With the now rising internet of things, the number of decisions (or actions) to be taken by more and more autonomous devices further increases. In this context, the distribution of machine learning algorithms on the user\u2019s devices offers decisive advantages:\n\u2022 scalability, thanks to parallel processing, \u2022 reduction of deployment costs, thanks to the removal of the centralized processing archi-\ntecture,\n\u2022 privacy, by processing the user\u2019s data on his own device.\nHowever, the massive distribution of machine learning algorithms on datastreams raises two major concerns. Firstly, the cost of this massive distribution can be huge in terms of communication costs. Indeed, the naive distribution of machine learning algorithms transmits each incoming event on each device to all devices. The communication cost has to be precisely quantified and controlled. Secondly, the proliferation of autonomous machine learning algorithms, which choose actions for users, implies to control the potential cost of bad choices. We need algorithms with strong theoretical guarantees.\nMost of applications necessitate to take and optimize decisions with a partial feedback. This well known problem is called multi-armed bandits (MAB). In its most basic formulation, it can be stated as follows: there are K decisions, each having an unknown distribution of bounded rewards. At each step, one has to choose a decision and receives a reward. The performance of a MAB algorithm is assessed in term of regret (or opportunity loss) with regards to the unknown optimal decision. Optimal solutions have been proposed to solve this problem using a stochastic formulation in [Auer et al (2002)a], using a Bayesian formulation in [Kaufman et al (2012)], or using an adversarial formulation in [Auer et al (2002)b]. While these approaches focus on the minimization of the\nar X\niv :1\n60 2.\n03 77\n9v 1\n[ cs\n.A I]\n1 1\nFe b\n20 16\nexpected regret, the PAC setting (see [Vailant (1984)]) or the ( , \u03b4)-best-arm identification, focuses on the sample complexity (i.e. the number of time steps) needed to find an -approximation of the best arm with a failure probability of \u03b4. This formulation has been studied for MAB problem in [Even-Dar et al (2002), Bubeck et al (2009)], for voting bandit problem in [Urvoy et al (2013)], and for linear bandit problem in [Soare et al (2014)]. In most of these applications, a rich contextual information is also available. For instance, in ad-serving optimization we know the web page, the position in the web page, or the profile of the user. This contextual information must be exploited in order to decide which ad is the most relevant to display. This problem is called contextual bandit problem [Langford and Zhang (2007), Dudik et al (2011)]. Some recent advances have shown that contextual bandit problem can be reduced to several best arm identification problems by using hierarchical models such as decision trees and random forest [Fe\u0301raud et al (2016)]. In this paper, we focus on the basic brick, the distribution of best arm identification problem, then we discuss the distribution of contextual bandit algorithms.\nThe distribution of exploration of multi-armed bandits has been studied in [Hillel et al (2013)]. The analysis of algorithms is based on the sample complexity need to find the best arm with an approximation factor [Vailant (1984)]. When only one communication round is allowed, an algorithm with an optimal speed-up factor of \u221a N has been proposed, where N is the number of best arm identification tasks. The algorithmic approach has been extended to the case where multiple communication rounds are allowed. In this case a speed-up factor of N is obtained while the number of communication rounds is in O(log 1/ ). In this study, the authors focus on the trade-off between the number of communication rounds and the number of pulls per player. This analysis is natural when one would like to distribute best arm identification tasks on a centralized processing architecture. In this case, the bandwidth between processors is broad and the number of communication rounds is the true cost. The massive distribution that we would like to address is more challenging.\nFirst of all, when bandit algorithms are deployed on the user\u2019s devices, the bandwidth between the devices and the processors can no longer be considered as broad. In this case, rather than the number of communications, the cost of communications is modeled by the number of bits needed to transmit. The first part of this study focuses on the analysis of the trade-off between the number of bits needed to transmit and the number of pulls per player. In the next section, we propose a distributed best arm identification algorithm, which is optimal in terms of number of transmitted bits, which is optimal up to logarithmic factors in terms of number of pulls per player, and which benifits from a speed-up factor N .\nMore importantly, when bandit algorithms are deployed on the user\u2019s devices, we do not control the probability of each player. Indeed, a player can choose an action only when an uncontrolled event occurs such as: the device of the user is switched on, the user has launched a mobile phone application, the user connects to a web page... Here, we consider that each player is drawn from a distribution. In the last part of the analysis, we provide an upper bound of the number of draws of players needed to identify the best arm. This result allows to adapt the proposed algorithm to the uncertainty of the number of pulls of each player."}, {"heading": "2 Distributed Action Selection", "text": ""}, {"heading": "2.1 Principle", "text": "The massive distribution on user\u2019s devices of the best arm identification task is a collaborative game. For instance, when a mobile phone application plays an action such as personalize its interface, a feedback is received from the user. This feedback is used to update the estimated mean reward of the chosen action given the mobile phone. If the mobile phone application would like to eliminate the action, it transmits it to the synchronization system. If enough mobile phones would like to eliminate the action, the synchronization system sends the suppressed action to each mobile phone. From a theoretical point of view any device can be used as the synchronization system. However, in practice the synchronization system must be reachable at any time, and thus a server has to be used to share information between devices (see Figure 1)."}, {"heading": "2.2 Algorithm", "text": "Let N be the number of players (i.e. the number of user\u2019s devices). Without loss of generality, to model the distribution of the best arm identification task on the user\u2019s devices, we assume that at each time step a player n is drawn according to a probability P (z), with \u2211N n P (z = n) = 1. To simplify the notations, in the following we will use Pn instead of P (z = n). Let A be a set of K actions. Let yt \u2208 [0, 1]K be a vector of bounded rewards at time t, yk(t) be the reward of the action k at time t, and ynk (t) be the reward of the action k chosen by the player n at time t. Let \u00b5nk = E[y n k ] = E[yk \u00b7 1z=n] be the expected reward of the action k for the player n. We study the distribution the best arm identification task, when the following assumption holds:\nAssumption 1: \u2200n \u2208 {0, ..., N} \u00b5nk = \u00b5k. Using Assumption 1, we can derive a simple and efficient algorithm to distribute the best arm identification task (see Algorithm 1). Each task collaborates with each other by sharing the same set of remaining actions A. When a player n is drawn, it plays a remaining action k from A. It recieves a feedback ynk (t) and uses it to update its estimated mean reward \u00b5\u0302 n k . Rather than to transmit the estimated mean reward to other players, which would lead to use a broad bandwith, DISTRIBUTED ACTION SELECTION trasmits to the synchronisation server the action, that it would like to eliminate. The action is sent only once per player when the gap between the empirical mean rewards of the action k and the estimated best action is high enough. The only role of the synchronisation server is to count the number of flags per players and actions set to one (see Algorithm 2). When this count is high enough, the action is eliminated form the set A, and the synchronisation server sends this information to all players."}, {"heading": "3 Analysis", "text": "Theorem 1 states that in comparison to SUCCESSIVE ELIMINATION, the speed-up factor of DISTRIBUTED ACTION SELECTION is N , where N is the number of players.\nTheorem 1: when = 0 and V = N with a probability at least 1 \u2212 \u03b4, DISTRIBUTED ACTION SELECTION identifies the optimal arm, using\nt\u2217 = O\n( K\nN.\u220621 log\nK\n\u03b4.\u22061 ) ) pulls per player, where \u22061 = mink 6=k\u2217 \u00b5k\u2217 \u2212 \u00b5k.\nAlgorithm 1 DISTRIBUTED ACTION SELECTION 1: Inputs: K actions, N players, \u2208 [0, 1), \u03b4 \u2208 (0, 1], V \u2264 N 2: Output: an -approximation of the best arm 3: Initialization: \u03b4s = \u03b41/V , \u2200(k, n) tnk = 0, \u00b5\u0302nk = 0, \u03bbnk = 0 4: repeat 5: Draw a player n \u223c P (z) 6: while MessageFromServer(k) do 7: A = A \\ {k} 8: end while 9: Play k = round-robin (A) 10: Receive ynk 11: tnk = t n k + 1 12: \u00b5\u0302nk = ynk tnk + tnk\u22121 tnk\n\u00b5\u0302nk 13: k1 = arg maxk \u00b5\u0302nk 14: if k = LastAction(A) then 15: for all k \u2208 A do 16: if ( \u00b5\u0302nk1 \u2212 \u00b5\u0302 n k + \u2265 2 \u221a 1 2tnk log 4K(tnk ) 2 \u03b4s and\u03bbnk = 0 ) then 17: \u03bbnk = 1 18: SendToServer(k) 19: end if 20: end for 21: end if 22: until |A| = 1\nAlgorithm 2 SYNCHRONISATION SERVER 1: Inputs: K actions, N players, V \u2264 N 2: Output: an -approximation of the best arm 3: Initialization: \u2200(k, n) \u03bbnk = 0 4: repeat 5: if MessageFromPlayer(k,n) then 6: \u03bbkn = 1 7: if ( \u2211 n \u03bb n k \u2265 V ) then\n8: SendToPlayers(k) 9: \u2200n \u03bbkn = 0\n10: end if 11: end if 12: until |A| = 1\nProof. Theorem 3 in [Even-Dar et al (2002)] provides the upper bound of number of pulls of a single player needed to find the best arm with a probability 1 \u2212 \u03b4s. By setting \u03b4s = \u03b41/N , we provide the proof.\nThe upper bound of number of pulls per player is optimal up to logarithmic factors (see [Mannor and Tsitsiklis (2004)]). In the following we focus the analysis on the communication cost.\nAssumption 2: each transmitted message through the communication network is coded using a binary code.\nFor instance, when the synchronisation server notifies to all players that the action k = 7 is eliminated, it sends to all players the code \u2032111\u2032.\nTheorem 2: when = 0 and V = N , when a binary code is used, with a probability at least 1\u2212\u03b4, DISTRIBUTED ACTION SELECTION identifies the optimal arm, transmitting less than 2N.(K \u2212 1)dlog2Ke bits through the communication network.\nProof. The condition \u03bbnk = 0 in line 16 of algorithm 1 ensures that only one action per player is sent to the server. Thus the number of sent codes to the server is upper bounded by N.(K \u2212 1). Then, the statement \u2200n \u03bbkn = 0 line 9 of algorithm 2 and the fact that only one action per player is sent to the server ensure that the synchornization server sends each action only once. Thus the number of sent codes to players is upper bounded by 2(K \u2212 1).N . The optimal length of a binary code needed to code an alphabet of size K is dlog2Ke. Thus, the total number of transmitted bits is upper bounded by 2N.(K \u2212 1)dlog2Ke.\nTheorem 3 shows that the upper bound of the total number of transmitted bits is equal to the lower bound, and thus that DISTRIBUTED ACTION SELECTION is optimal in terms of communication costs.\nTheorem 3: It exists a distribution Dy such that any distributed algorithm on N nodes using a binary code, and finding the optimal arm on each node with a maximum number of pulls t\u2217 necessitates to transmit at least:\n2N.(K \u2212 1)dlog2Ke bits.\nProof. First it is easy to show that any algorithm that satisfies the conditions of Theorem 3 needs to transmit the code of eliminated actions. Indeed, the only alternartive is to transmit the values of estimated mean rewards such as in [Hillel et al (2013)]. The mean rewards are floating numbers and cannot be coded in a finite number of bits without losses. For instance in standard computers, the mean rewards is coded using 64 bits. So, as any K has to be smaller than 264 to be processed on standard computers, transmitting the code is more efficient than transmitting the value of estimated mean reward.\nThen, let us assume that it exists an algorithm that satisfies the conditions of Theorem 3 and transmits 2N.(K \u2212 1)dlog2Ke \u2212 1 bits. As to reach the upper bound the codes are transmitted only once from the players to the server and only once from the server to the players,\n1. this algorithm uses less than dlog2Ke to code the actions,\n2. or a player does not transmit an action to the server,\n3. or the server does not transmit an action to a player.\nThe condition 1 not possible when a binary code is used (see Assumption 2). 1\nIf a player does not transmit an action to the server (condition 2), then this action will not be eliminated. If this action is the optimal one, the algorithm fails.\nIf an action is not transmited to a player (condition 3), this player will not eliminated this action in time t\u2217.\nTheorem 4 provides an upper bound of the number of draws of players (i.e. the time step) needed to obtain the t\u2217 number of pulls per player.\n1Using a prefix code such as a troncated binary code or a Huffman code (see [Cover and Thomas (2006)]), it is possible to transmit slightly less bits. To simplify the exposition of ideas, we have restricted the analysis to binary code.\nTheorem 4: when = 0 and V = N with a probability at least 1 \u2212 \u03b4, DISTRIBUTED ACTION SELECTION identifies the optimal arm, using\nO ( N\u22121\u2211 n=0 1 Pn ( t\u2217 + \u221a (1\u2212 Pn)t\u2217 \u03b4 )) draws of players, where Pn is the probability of the player n, and t\u2217 the optimal number of pulls per player.\nProof. At each time step, a player n is drawn according to a probability Pn. Let Tn be the time step when the player n has been drawn tn times. Let fn be the number of times where the player n has not been drawn at time step Tn. fn follows a negative binomial distribution with parameters tn,1\u2212 Pn. By definition of the negative binomial distribution, We have:\nE[fn(T )] = (1\u2212 Pn)tn\nPn , and V[fn(T )] = (1\u2212 Pn)tn P 2n\nUsing the Chebyshev\u2019s inequality, we have:\nP ( P (fn) \u2265\n(1\u2212 Pn)tn Pn +\n) \u2264 (1\u2212 Pn)tn\n2P 2n = \u03b4\nThen, we have:\nP ( P (fn) \u2265\n(1\u2212 Pn)tn Pn + 1 Pn\n\u221a (1\u2212 Pn)tn\n\u03b4\n) \u2264 \u03b4\nLet T \u2217n be the time step needed to obtain t \u2217 draws of the player n. The number of draws T \u2217n is the sum of the number of draws of contexts containing n and not containing n. Hence, the following inequality is true with a probability 1\u2212 \u03b4:\nT \u2217n \u2264 t\u2217 + (1\u2212 Pn)t\u2217\nPn +\n1\nPn\n\u221a (1\u2212 Pn)t\u2217\n\u03b4\n\u21d4 T \u2217n \u2264 1\nPn\n( t\u2217 + \u221a (1\u2212 Pn)t\u2217\n\u03b4\n)\nSumming the T \u2217n over all the players, we provide the proof of the Theorem 4.\nTheorem 4 shows that the players, which have a low probability to be drawn, have a strong contribution to the time step (i.e. the number of draws of players ) where all players choose the best action. This is the price to pay to distribute the best arm identification task on user\u2019s devices. A simple way to handle the case where some players have a low or even a null probability is to bound the number of players used to choose the actions by V \u2264 N . The speed-up factor becomes V instead N , but using a global knowledge on the activity of players to set V = |{n < N,Pn > P}|, one can insure that the proposed algorithm stops and finds an -approximation of the best arm with high probability."}, {"heading": "4 Distribution of contextual bandit algorithms", "text": "The distribution of best arm identification task is a basic brick, which can be used in most of variant of multi-armed bandit problems. For application purposes, one of the most important variant is the contextual bandit problem. The distribution of contextual bandit algorithm is a significant milestone. Indeed, beside the scalability and the reduction of deployment costs, the most important point is that\nthe distribution of contextual bandit algorithms on the user\u2019s device guarantees the privacy: with the proposed architecture (see Figure 1), the user\u2019s information stays on the user\u2019s device.\nTo extend our results to the contextual bandit problem, we propose to distribute BANDIT FOREST algorithm [Fe\u0301raud et al (2016)]. The key concept of decision tree and random forest approaches is the decision stump. All results obtained on the decision stumps can be extended to models which combine them. In this section, we focus on the decision stump, and then we discuss its extension to BANDIT FOREST.\nIn its basic form, building a decision stump consists in a best variable identification task followed by one best arm identification task per value of the best variable (see Algorithm 3). To distribute the decision stump, we now need to distribute the best variable identification task. We remind below the best variable identification problem.\nAlgorithm 3 DISTRIBUTED DECISION STUMP 1: Inputs: M binary variables, K actions, N players, \u2208 [0, 1), \u03b4 \u2208 (0, 1], V \u2264 N 2: Output: an -approximation of the best decision stump 3: Run DISTRIBUTED VARIABLE SELECTION to find i1, the -approximation of the best variable i\u2217 with high probability.\n4: Run DISTRIBUTED ACTION SELECTION for each value of the variable i1 to find the corresponding -approximation of best arms with high probability.\nAlgorithm 4 DISTRIBUTED VARIABLE SELECTION 1: Inputs: M binary variables, K actions, N players, \u2208 [0, 1), \u03b4 \u2208 (0, 1], V \u2264 N 2: Output: an -approximation of the best variable 3: Initialization: \u03b4s = \u03b41/V , \u2200(k, n) tnk = 0, \u2200(i, n, v) \u00b5\u0302 n,i k,v = 0, \u2200(i, n) \u00b5\u0302n,i = 0, \u03bbni = 0\n4: repeat 5: Draw a player n \u223c P (z) 6: while MessageFromServer(i) do 7: S = S \\ {i} 8: end while 9: Play k = round-robin (A)\n10: Receive ynk 11: tnk = t n k + 1 12: for each remaining variable i \u2208 S do 13: for each value v do 14: \u00b5\u0302ik,v = ynk tnk 1xi=v + tnk\u22121 tnk\n\u00b5\u0302n,ik,v 15: end for 16: \u00b5\u0302n,i = \u2211 v\u2208{0,1}maxk \u00b5\u0302 n,i k,v 17: end for 18: i1 = arg maxi \u00b5\u0302n,i 19: if k = LastAction(A) then 20: for all i \u2208 S do 21: if ( \u00b5\u0302n,i1 \u2212 \u00b5\u0302n,i + \u2265 4 \u221a 1\n2tnk log\n4KM(tnk ) 2\n\u03b4s and\u03bbni = 0\n) then\n22: \u03bbni = 1 23: SendToServer(i) 24: end if 25: end for 26: end if 27: until |S| = 1\nLet xt \u2208 {0, 1}M be a vector of binary values describing the environment at time t. Let V be the set of indices of variables. Let \u00b5ik,v = E[yk \u00b7 1xi=v] be the expected reward of the action k and the value v of the binary variable xi. The expected reward when using variable xi to select the best\naction is the sum of expected rewards of the best actions for each of its possible values: \u00b5i = \u2211\nv\u2208{0,1}\nmax k\n\u00b5ik,v\nThe optimal variable to be used for selecting the best action is: i\u2217 = arg maxi\u2208V \u00b5i.\nAlgorithm 4 outputs an -approximation of the best variable with high probability using a synchronization server similar to Algorithm 2, expect that it synchronizes the elimination of variables.\nTheorem 5: when = 0 and V = N , when a binary code is used, with a probability at least 1\u2212 \u03b4, DISTRIBUTED VARIABLE SELECTION identifies the optimal variable, transmitting less than 2N.(M \u2212 1)dlog2Me bits through the communication network, and using\nt\u2217 = O\n( K\nN.\u220622 log\nKM\n\u03b4.\u22062 ) pulls per player, where \u22062 = mini 6=i\u2217 \u00b5i \u2217 \u2212 \u00b5i.\nProof. The number of transmitted bits is obtained using the same arguments than those provided for the proof of Theorem 2. Lemma 1 in [Fe\u0301raud et al (2016)] provides the upper bound of number of pulls of a single player needed to find the best variable with a probability 1 \u2212 \u03b4s. By setting \u03b4s = \u03b4 1/N , we provide the proof.\nTheorem 6: when = 0 and V = N , when a binary code is used, with a probability at least 1\u2212\u03b4, DISTRIBUTED DECISION STUMP identifies the optimal decision stump, using\nt\u2217 = O\n( K\nN.\u220622 log\nKM \u03b4.\u22062 + K N.\u220621 log K \u03b4.\u22061 ) pulls per player, and transmitting less than 2N((M\u22121)dlog2Me+2(K\u22121)dlog2Ke) bits through the communication network.\nProof. A decision stump is a best variable identification task followed by two best arm identification tasks. So to obtain the upper bound of the number of pulls per player and the number of transmitted bits, we just have to sum those of best variable identification and best arm identification tasks.\nThe upper bound of number of pulls per player is optimal up to logarithmic factors (see Theorem 2 in [Fe\u0301raud et al (2016)]). Using the same arguments than those provided in Theorem 3 it is straightforward to show that the transmitted number of bits is optimal.\nNow, it is easy to extend the obtained results to a decision tree of depth D which recursively combines 2D best variable identification tasks and then 2D best arm identification tasks. The obtained upper bound of the number of pulls per player is the one provided in Theorem 6 multiplied by 2D. It is optimal up to logarithmic factors (see Theorem 4 in [Fe\u0301raud et al (2016)]). The optimal number of transmitted bits is also the one provided in Theorem 6 multiplied by 2D.\nTo extend these results to BANDIT FOREST, one just has to add that the random seeds used to draw the available variables at each split must be shared between user\u2019s devices."}, {"heading": "5 Conclusion", "text": "We have proposed a simple architecture allowing to distribute the best arm identification task on user\u2019s devices. Based on this massively distributed architecture, we have proposed a distributed version of SUCCESSIVE ELIMINATION. DISTRIBUTED ACTION SELECTION obtains a speed-up factor linear in number of players. We have showed that its communication cost is optimal in terms of transmitted number of bits, while the number of pulls per player is optimal up to logarithmic factors. This algorithm is a key, which we used to address the distribution of BANDIT FOREST. Finally, we have proposed a contextual bandit algorithm which is able to finely exploit the user\u2019s data while guaranteeing the privacy."}], "references": [{"title": "Finite-time Analysis of the Multiarmed Bandit Problem", "author": ["P. Auer et al (2002)a] Auer", "N. Cesa Bianchi", "P. Fischer"], "venue": "Machine Learning,47,", "citeRegEx": "Auer et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Auer et al\\.", "year": 2002}, {"title": "The nonstochastic multiarmed bandit problem", "author": ["P. Auer et al (2002)b] Auer", "N. Cesa-Bianchi", "Y. Freund", "R.E. Schapire"], "venue": "SIAM J. COMPUT.,", "citeRegEx": "Auer et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Auer et al\\.", "year": 2002}, {"title": "Stoltz: Pure exploration in multi-armed bandits", "author": ["S. Bubeck et al (2009)] Bubeck", "T. Wang"], "venue": null, "citeRegEx": "Bubeck and Wang,? \\Q2009\\E", "shortCiteRegEx": "Bubeck and Wang", "year": 2009}, {"title": "T.:Efficient Optimal Learning for Contextual Bandits, UAI", "author": ["M. Dudik et al (2011)] Dud\u0131\u0301k", "D. Hsu", "S. Kale", "N. Karampatziakis", "J. Langford", "L. Reyzin", "Zhang"], "venue": null, "citeRegEx": "Dud\u0131\u0301k et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Dud\u0131\u0301k et al\\.", "year": 2011}, {"title": "PAC Bounds for Multi-armed Bandit and Markov Decision", "author": ["S. Mannor", "Mansour Y"], "venue": "Processes, COLT,", "citeRegEx": "Even.Dar et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Even.Dar et al\\.", "year": 2002}, {"title": "Random Forest for the Contextual Bandit Problem, AISTATS, 2016", "author": ["R. F\u00e9raud et al (2016)] F\u00e9raud", "R. Allesiardo", "T. Urvoy", "F. Cl\u00e9rot"], "venue": null, "citeRegEx": "F\u00e9raud et al\\.,? \\Q2016\\E", "shortCiteRegEx": "F\u00e9raud et al\\.", "year": 2016}, {"title": "Distributed Exploration in Multi-Armed Bandits", "author": ["E. Hillel et al (2013)] Hillel", "Z. Karnin", "Koren", "R.T. Lempel", "O. Somekh"], "venue": null, "citeRegEx": "Hillel et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Hillel et al\\.", "year": 2013}, {"title": "Thomson sampling: An asymptotically optimal finite time", "author": ["E. Kaufman et al (2012)] Kaufman", "N. Korda", "R. Munos"], "venue": null, "citeRegEx": "Kaufman et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Kaufman et al\\.", "year": 2012}, {"title": "The epoch-greedy algorithm for contextual multi-armed bandits", "author": ["Langford", "J. Zhang (2007)] Langford", "T. Zhang"], "venue": null, "citeRegEx": "Langford et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Langford et al\\.", "year": 2007}, {"title": "The Sample Complexity of Exploration in the Multi-Armed", "author": ["Mannor", "S. Tsitsiklis (2004)] Mannor", "J.N. Tsitsiklis"], "venue": "Bandit Problem,", "citeRegEx": "Mannor et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Mannor et al\\.", "year": 2004}, {"title": "Best-Arm Identification in Linear Bandits", "author": ["M. Soare et al (2014)] Soare", "L. Lazaric", "R. Munos"], "venue": null, "citeRegEx": "Soare et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Soare et al\\.", "year": 2014}, {"title": "Generic Exploration and K-armed Voting Bandits", "author": ["T. Urvoy et al (2013)] Urvoy", "F. Cl\u00e9rot", "R. F\u00e9raud", "S. Naamane"], "venue": null, "citeRegEx": "Urvoy et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Urvoy et al\\.", "year": 2013}], "referenceMentions": [], "year": 2017, "abstractText": "The distribution of the best arm identification task on the user\u2019s devices offers several advantages for application purposes: scalability, reduction of deployment costs and privacy. We propose a distributed version of the algorithm SUCCESSIVE ELIMINATION using a simple architecture based on a single server which synchronizes each task executed on the user\u2019s devices. We show that this algorithm is optimal in terms of transmitted number of bits and is optimal up to logarithmic factors in terms to number of pulls per player. Finally, we propose an extension of this approach to distribute the contextual bandit algorithm BANDIT FOREST, which is able to finely exploit the user\u2019s data while guaranteeing the privacy.", "creator": "LaTeX with hyperref package"}}}