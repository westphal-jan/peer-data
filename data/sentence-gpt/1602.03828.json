{"id": "1602.03828", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "11-Feb-2016", "title": "Community Recovery in Graphs with Locality", "abstract": "Motivated by applications in domains such as social networks and computational biology, we study the problem of community recovery in graphs with locality. In this problem, pairwise noisy measurements of whether two nodes are in the same community or different communities come mainly or exclusively from nearby nodes rather than uniformly sampled between all nodes pairs, as in most existing models. We present an algorithm that runs nearly linearly in the number of measurements and which achieves the information theoretic limit for exact recovery. This algorithm is based on an algorithm described in a previous article on the theoretical contribution of stochastic analysis to the problem. For example, when I first created the original graph, I used a statistical formula that assumes that nodes are in the same community. To achieve that estimate, I first used a statistical formula that assumes that nodes are in a different local community and not in the same state. We can obtain this by using a Gaussian function. In this technique, we first use a Gaussian function, using a Gaussian function that assumes that nodes are in a different local community. Using a Gaussian function, we first create a Gaussian function that takes the maximum number of nodes at each point in the graph. Then we can use Gaussian function to compute the number of nodes at each point in the graph. We can then define a Gaussian function with the parameter, where the max value is one node.\n\n\nThe results show that there is a reduction in population density by the rate of population change and the loss of large number of smaller clusters. The increase in population density is offset by the number of individual nodes in one cluster. In the analysis of this analysis, the decrease in population density is offset by the number of individual nodes in the whole cluster. In this analysis, we can identify a large number of cluster clusters. For example, in the graph above, the decrease in population density is offset by the number of individuals in one cluster. We can create a Gaussian function that takes the maximum number of nodes at each point in the graph.\nTo compute the number of clusters, we first use a Gaussian function that takes the maximum number of nodes at each point in the graph. The decrease in population density is offset by the number of individuals in one cluster. In the analysis of this analysis, the decrease in population density is offset by the number of individuals in one cluster. In this analysis, the decrease in population density is offset by the number of individuals in one cluster. In this analysis, the decrease in population density is offset by the number", "histories": [["v1", "Thu, 11 Feb 2016 19:13:20 GMT  (782kb,D)", "https://arxiv.org/abs/1602.03828v1", "Preliminary versions"], ["v2", "Mon, 15 Feb 2016 19:26:32 GMT  (783kb,D)", "http://arxiv.org/abs/1602.03828v2", "Preliminary versions; the proofs of all theorems are deferred to the supplemental materials (this http URL)"], ["v3", "Wed, 1 Jun 2016 18:18:04 GMT  (2488kb,D)", "http://arxiv.org/abs/1602.03828v3", "accepted in part to International Conference on Machine Learning (ICML), 2016"]], "COMMENTS": "Preliminary versions", "reviews": [], "SUBJECTS": "cs.IT cs.LG cs.SI math.IT math.ST q-bio.GN stat.TH", "authors": ["yuxin chen", "govinda m kamath", "changho suh", "david tse"], "accepted": true, "id": "1602.03828"}, "pdf": {"name": "1602.03828.pdf", "metadata": {"source": "CRF", "title": "Community Recovery in Graphs with Locality", "authors": ["Yuxin Chen", "Govinda Kamath", "Changho Suh", "David Tse"], "emails": [], "sections": [{"heading": null, "text": "the problem of community recovery in graphs with locality. In this problem, pairwise noisy measurements of whether two nodes are in the same community or different communities come mainly or exclusively from nearby nodes rather than uniformly sampled between all node pairs, as in most existing models. We present two algorithms that run nearly linearly in the number of measurements and which achieve the information limits for exact recovery."}, {"heading": "1 Introduction", "text": "Clustering of data is a central problem that is prevalent across all of science and engineering. One formulation that has received significant attention in recent years is community recovery [1\u20133], also referred to as correlation clustering [4] or graph clustering [5]. In this formulation, the objective is to cluster individuals into different communities based on pairwise measurements of their relationships, each of which gives some noisy information about whether two individuals belong to the same community or different communities. While this formulation applies naturally in social networks, it has a broad range of applications in other domains including protein complex detection [6], image segmentation [7,8], shape matching [9], etc. See [10] for an introduction of this topic.\nIn recent years, there has been a flurry of works on designing community recovery algorithms based on idealised generative models of the measurement process. A particular popular model is the Stochastic Block Model (SBM) [11,12], where the n individuals to be clustered are modeled as nodes on a random graph. In the simplest version of this model with two communities, this random graph is generated such that two nodes has an edge connecting them with probability p if they are in the same community and probability q if they belong to different communities. If p > q, then there are statistically more edges within a community than between two communities, which can provide discriminating information for recovering the communities. A closely related model is the Censored Block Model (CBM) [13], where one obtains noisy parity measurements on the edges of an Erd\u0151s-R\u00e9nyi graph [14]. Each edge measurement is 0 with probability 1 \u2212 \u03b8 and 1 with probability \u03b8 if the two incident vertices are in the same community, and vice versa if they are in different communities.\nBoth the SBM and the CBM can be unified into one model by viewing the measurement process as a two-step process. First, the edge locations where there are measurements are determined by randomly and uniformly sampling a complete graph between the nodes. Second, the value of each edge measurement is obtained as a noisy function of the communities of the two nodes the edge connects. The two models differ only in the noisy functions. Viewed in this light, it is seen that a central assumption underlying both models is that it is equally likely to obtain measurements between any pair of nodes. This is a very unrealistic assumption in many applications: nodes often have locality and it is more likely to obtain data on relationships between nearby nodes than far away nodes. For example, in friendship graphs, individuals that live close by are more likely to interact than nodes that are far away. \u2217Department of Statistics and of Electrical Engineering, Stanford University, Stanford, CA 94305, USA (email: yxchen@stanford.edu). \u2020Department of Electrical Engineering, Stanford University, Stanford, CA 94305, USA (email: gkamath@stanford.edu). \u2021Department of Electrical Engineering, KAIST, Daejeon 305-701, Korea (e-mail: chsuh@kaist.ac.kr). \u00a7Department of Electrical Engineering, Stanford University, Stanford, CA 94305, USA (email: dntse@stanford.edu).\nar X\niv :1\n60 2.\n03 82\n8v 3\n[ cs\n.I T\n] 1\nJ un\n2 01\n6\nThis paper focuses on the community recovery problem when the measurements are randomly sampled from graphs with locality structure rather than complete graphs. Our theory covers a broad range of graphs including rings, lines, 2-D grids, and small-world graphs (Fig. 1). Each of these graphs is parametrized by a locality radius r such that nodes within r hops are connected by an edge. We characterize the information limits for community recovery on these networks, i.e. the minimum number of measurements needed to exactly recover the communities as the number of nodes n scales. We propose two algorithms whose complexities are nearly linear in the number of measurements and which can achieve the information limits of all these networks for a very wide range of the radius r. In the special case when the radius r is so large that measurements at all locations are possible, we recover the exact recovery limit identified by [15] when measurements are randomly sampled from complete graphs.\nIt is worth emphasizing that various computationally feasible algorithms [5, 16\u201321] have been proposed for more general models beyond the SBM and the CBM, which accommodate multi-community models, the presence of outlier samples, the case where different edges are sampled at different rates, and so on. Most of these models, however, fall short of accounting for any sort of locality constraints. In fact, the results developed in prior literature often lead to unsatisfactory guarantees when applied to graphs with locality, as will be detailed in Section 3. Another recent work [22] has determined the order of the information limits in geometric graphs, with no tractable algorithms provided therein. In contrast, our findings uncover a curious phenomenon: the presence of locality does not lead to additional computational barriers: solutions that are information theoretically optimal can often be achieved computational efficiently and, perhaps more surprisingly, within nearly linear time.\nThe paper is structured as follows. We describe the problem formulation in Section 2, including a concrete application from computational biology\u2014called haplotype phasing\u2014which motivates much of our theory. Section 3 presents our main results, with extensions of the basic theory and numerical results provided in Sections 4 and 5 respectively. Section 6 concludes the paper with a few potential extensions. The proofs of all results are deferred to the appendices."}, {"heading": "2 Problem Formulation and A Motivating Application", "text": "This section is devoted to describing a basic mathematical setup of our problem, and to discussing a concrete application that comes from computational biology."}, {"heading": "2.1 Sampling Model", "text": "Measurement Graph. Consider a collection of n vertices V = {1, \u00b7 \u00b7 \u00b7 , n}, each represented by a binaryvalued vertex variable Xi \u2208 {0, 1}, 1 \u2264 i \u2264 n. Suppose it is only feasible to take pairwise samples over a restricted set of locations, as represented by a graph G = (V, E) that comprises an edge set E . Specifically, for each edge (i, j) \u2208 E one acquires Ni,j samples1 Y (l)i,j (1 \u2264 l \u2264 Ni,j), where each sample measures the parity of Xi and Xj . We will use G to encode the locality constraint of the sampling scheme, and shall pay particular attention to the following families of measurement graphs.\n\u2022 Complete graph: G is called a complete graph if every pair of vertices is connected by an edge; see Fig. 1(a).\n\u2022 Line: G is said to be a line Lr if, for some locality radius r, (i, j) \u2208 E iff |i\u2212 j| \u2264 r; see Fig. 1(b).\n\u2022 Ring : G = (V, E) is said to be a ring Rr if, for some locality radius r, (i, j) \u2208 E iff i \u2212 j \u2208 [\u2212r, r] (mod n); see Fig. 1(c). \u2022 Grid : G is called a grid if (1) all vertices reside within a \u221an \u00d7 \u221an square with integer coordinates, and (2) two vertices are connected by an edge if they are at distance not exceeding some radius r; see Fig. 1(d).\n\u2022 Small-world graphs: G is said to be a small-world graph if it is a superposition of a complete graph G0 = (V, E0) and another graph G1 = (V, E1) with locality. See Fig. 1(e) for an example.\n| {z } r\n1\n| {z } r\n1\n| {z\n} r1\n| {z } r\n1\n(a) a complete graph (b) a line Lr (c) a ring Rr | {z } r 1 | {z } r\n(d) a grid (e) a small-world graph\nFigure 1: Examples of a complete graph, a line, a ring, a 2-D grid, and a small-world graph.\nRandom Sampling. This paper focuses on a random sampling model, where the number of samples Ni,j taken over (i, j) \u2208 E is independently drawn and obeys2 Ni,j \u223c Poisson (\u03bb) for some average sampling rate \u03bb. This gives rise to an average total sample size\nm := \u2211\n(i,j)\u2208E E [Ni,j ] = \u03bb |E| . (1)\nWhen m is large, the actual sample size sharply concentrates around m with high probability.\nMeasurement Noise Model. The acquired parity measurements are assumed to be independent given Ni,j ; more precisely, conditional on Ni,j ,\nY (l) i,j = Y (l) j,i ind. = { Xi \u2295Xj , with probability 1\u2212 \u03b8 Xi \u2295Xj \u2295 1, else\n(2)\nfor some fixed error rate 0 < \u03b8 < 1, where \u2295 denotes modulo-2 addition. This is the same as the noise model in CBM [13]. The SBM corresponds to an asymmetric erasure model for the measurement noise, and we expect our results extend to that model as well."}, {"heading": "2.2 Goal: Optimal Algorithm for Exact Recovery", "text": "This paper centers on exact recovery, that is, to reconstruct all input variables X = [Xi]1\u2264i\u2264n precisely up to global offset. This is all one can hope for since there is absolutely no basis to distinguish X from X \u2295 1 := [Xi \u2295 1]1\u2264i\u2264n given only parity samples. More precisely, for any recovery procedure \u03c8 the probability of error is defined as\nPe (\u03c8) := max X\u2208{0,1}n\nP {\u03c8(Y ) 6= X and \u03c8(Y ) 6= X \u2295 1} ,\nwhere Y := {Y (l)i,j }. The goal is to develop an algorithm whose required sample complexity approaches the information limit m\u2217 (as a function of (n, \u03b8)), that is, the minimum sample size m under which inf\u03c8 Pe (\u03c8) vanishes as n scales. For notational simplicity, the dependency of m\u2217 on (n, \u03b8) shall often be suppressed when it is clear from the context.\n1Here and throughout, we adopt the convention that Ni,j \u2261 0 for any (i, j) /\u2208 E. 2All results presented in this paper hold under a related model where Ni,j \u223c Bernoulli (\u03bb), as long as |E| n logn and \u03bb \u2264 1 (which is the regime accommodated in all theorems). In short, this arises due to the tightness of Poisson approximation to the Binomial distribution. We omit the details for conciseness.\n3"}, {"heading": "2.3 Haplotype Phasing: A Motivating Application", "text": "Before proceeding to present the algorithms, we describe here a genome phasing application that motivates this research and show how it can be modeled as a community recovery problem on graphs with locality.\nHumans have 23 pairs of homologous chromosomes, one maternal and one paternal. Each pair are identical sequences of nucleotides A,G,C,T\u2019s except on certain documented positions called single nucleotide polymorphisms (SNPs), or genetic variants. At each of these positions, one of the chromosomes takes on one of A,G,C or T which is the same as the majority of the population (called the major allele), while the other chromosome takes on a variant (also called minor allele). The problem of haplotype phasing is that of determining which variants are on the same chromosome in each pair, and has important applications such as in personalized medicine and understanding poylogenetic trees. The advent of next generation sequencing technologies allows haplotype phasing by providing linking reads between multiple SNP locations [23\u201325].\nOne can formulate the problem of haplotype phasing as recovery of two communities of SNP locations, those with the variant on the maternal chromosome and those with the variant on the paternal chromosome [26, 27]. Each pair of linking reads gives a noisy measurement of whether two SNPs have the variant on the same chromosome or different chromosomes. While there are of the order of n = 105 SNPs on each chromosome, the linking reads are typically only several SNPs or at most 100 SNPs apart, depending on the specific sequencing technology. Thus, the measurements are sampled from a line graph like in Fig. 1(b) with locality radius r n."}, {"heading": "2.4 Other Useful Metrics and Notation", "text": "It is convenient to introduce some notations that will be used throughout. One key metric that captures the distinguishability between two probability measures P0 and P1 is the Chernoff information [28], defined as\nD\u2217 (P0, P1) := \u2212 inf 0\u2264\u03c4\u22641\nlog {\u2211\ny P \u03c40 (y)P 1\u2212\u03c4 1 (y)\n} . (3)\nFor instance, when P0 \u223c Bernoulli (\u03b8) and P1 \u223c Bernoulli (1\u2212 \u03b8), D\u2217 simplifies to\nD\u2217 = KL (0.5 \u2016 \u03b8) = 0.5 log 0.5 \u03b8 + 0.5 log 0.5 1\u2212 \u03b8 , (4)\nwhere KL (0.5 \u2016 \u03b8) is the Kullback-Leibler (KL) divergence between Bernoulli(0.5) and Bernoulli(\u03b8). Here and below, we shall use log (\u00b7) to indicate the natural logarithm.\nIn addition, we denote by dv and davg the vertex degree of v and the average vertex degree of G, respectively. We use \u2016M\u2016 to represent the spectral norm of a matrix M . Let 1 and 0 be the all-one and all-zero vectors, respectively. We denote by supp (x) (resp. \u2016x\u20160) the support (resp. the support size) of x. The standard notion f(n) = o (g(n)) means lim\nn\u2192\u221e f(n)/g(n) = 0; f(n) = \u03c9 (g(n)) means lim n\u2192\u221e g(n)/f(n) = 0;\nf(n) = \u2126 (g(n)) or f(n) & g(n) mean there exists a constant c such that f(n) \u2265 cg(n); f(n) = O (g(n)) or f(n) . g(n) mean there exists a constant c such that f(n) \u2264 cg(n); f(n) = \u0398 (g(n)) or f(n) g(n) mean there exist constants c1 and c2 such that c1g(n) \u2264 f(n) \u2264 c2g(n)."}, {"heading": "3 Main Results", "text": "This section describes two nearly linear-time algorithms and presents our main results. The proofs of all theorems are deferred to the appendices."}, {"heading": "3.1 Algorithms", "text": ""}, {"heading": "3.1.1 Spectral-Expanding", "text": "The first algorithm, called Spectral-Expanding, consists of three stages. For concreteness, we start by describing the procedure when the measurement graphs are lines / rings; see Algorithm 1 for a precise description of the algorithm and Fig. 2 for a graphical illustration.\nAlgorithm 1 : Spectral-Expanding\n1. Run spectral method (Algorithm 2) on a core subgraph induced by Vc, which yields estimates X\n(0) j , 1 \u2264 j \u2264 |Vc|.\n2. Progressive estimation: for i = |Vc|+ 1, \u00b7 \u00b7 \u00b7 , n,\nX (0) i \u2190 majority\n{ Y\n(l) i,j \u2295X (0) j | j : j < i, (i, j) \u2208 E , 1 \u2264 l \u2264 Ni,j\n} .\n3. Successive local refinement: for t = 0, \u00b7 \u00b7 \u00b7 , T \u2212 1, X\n(t+1) i \u2190 majority\n{ Y\n(l) i,j \u2295X (t) j | j : j 6= i, (i, j) \u2208 E , 1 \u2264 l \u2264 Ni,j\n} , 1 \u2264 i \u2264 n.\n4. Output X(T )i , 1 \u2264 i \u2264 n.\nHere, majority {\u00b7} represents the majority voting rule: for any sequence s1, \u00b7 \u00b7 \u00b7 , sk \u2208 {0, 1}, majority {s1, \u00b7 \u00b7 \u00b7 , sk} is equal to 1 if \u2211k i=1 si > k/2; and 0 otherwise.\nAlgorithm 2 : Spectral initialization\n1. Input: measurement graph G = (V, E), and samples { Y\n(l) i,j \u2208 {0, 1} | j : j < i, (i, j) \u2208 E , 1 \u2264 l \u2264 Ni,j\n} .\n2. Form a sample matrix A such that\nAi,j =\n{ 1 { Y\n(1) i,j = 0\n} \u2212 1 { Y\n(1) i,j = 1\n} , if (i, j) \u2208 E ;\n0, else.\n3. Compute the leading eigenvector u of A, and for all 1 \u2264 i \u2264 n set\nX (0) i = { 1, if ui \u2265 0, 0, else.\n4. Output X(0)i , 1 \u2264 i \u2264 n.\n\u2022 Stage 1: spectral metrod on a core subgraph. Consider a subgraph Gc induced by Vc := {1, \u00b7 \u00b7 \u00b7 , r}, and it is self-evident that Gc is a complete subgraph. We run a spectral method (e.g. [29]) on Gc using samples taken over Gc, in the hope of obtaining approximate recovery of {Xi | i \u2208 Vc}. Note that the spectral method can be replaced by other efficient algorithms, including semidefinite programming (SDP) [30] and a variant of belief propagation (BP) [31].\n\u2022 Stage 2: progressive estimation of remaining vertices. For each vertex i > |Vc|, compute an estimate of Xi by majority vote using backward samples\u2014those samples linking i and some j < i. The objective is to ensure that a large fraction of estimates obtained in this stage are accurate. As will be discussed later, the sample complexity required for approximate recovery is much lower than that required for exact recovery, and hence the task is feasible even though we do not use any forward samples to estimate Xi.\n\u2022 Stage 3: successive local refinement. Finally, we clean up all estimates using both backward and forward samples in order to maximize recovery accuracy. This is achieved by running local majority voting from the neighbors of each vertex until convergence. In contrast to many prior work, no sample splitting is required, namely, we reuse all samples in all iterations in all stages. As we shall see, this stage is the bottleneck for exact information recovery.\nRemark 1. The proposed algorithm falls under the category of a general non-convex paradigm, which starts\nV1 V2 V3 V4 V5 V6 core subgraph Vc\n1\nV1 V2 V3 V4 V5 V6 core subgraph Vc\n1\n1\n1\nwith an approximate estimate (often via spectral methods) followed by iterative refinement. This paradigm has been successfully applied to a wide spectrum of applications ranging from matrix completion [32,33] to phase retrieval [34] to community recovery [17,35,36].\nAn important feature of this algorithm is its low computational complexity. First of all, the spectral method can be performed within O (mc log n) time by means of the power method, where mc indicates the number of samples falling on Gc. Stage 2 entails one round of majority voting, whereas the final stage\u2014as we will demonstrate\u2014converges within at most O (log n) rounds of majority voting. Note that each round of majority voting can be completed in linear time, i.e. in time proportional to reading all samples. Taken collectively, we see that Spectral-Expanding can be accomplished within O (m log n) flops, which is nearly linear time.\nCareful readers will recognize that Stages 2-3 bear similarities with BP, and might wonder whether Stage 1 can also be replaced with standard BP. Unfortunately, we are not aware of any approach to analyze the performance of vanilla BP without a decent initial guess. Note, however, that the spectral method is already nearly linear-time, and is hence at least as fast as any feasible procedure.\nWhile the preceding paradigm is presented for lines / rings, it easily extends to a much broader family of graphs with locality. The only places that need to be adjusted are:\n1. The core subgraph Vc. One would like to ensure that |Vc| & davg and that the subgraph Gc induced by Vc forms a (nearly) complete subgraph, in order to guarantee decent recovery in Stage 1.\n2. The ordering of the vertices. Let Vc form the first |Vc| vertices of V, and make sure that each i > |Vc| is connected to at least an order of davg vertices in {1, \u00b7 \u00b7 \u00b7 , i\u2212 1}. This is important because each vertex needs to be incident to sufficiently many backward samples in order for Stage 2 to be successful."}, {"heading": "3.1.2 Spectral-Stitching", "text": "We now turn to the 2nd algorithm called Spectral-Stitching, which shares similar spirit as Spectral-Expanding and, in fact, differs from Spectral-Expanding only in Stages 1-2.\n\u2022 Stage 1: node splitting and spectral estimation. Split V into several overlapping subsets Vl (l \u2265 1) of size W , such that any two adjacent subsets share W/2 common vertices. We choose the size W of each Vl to be r for rings / lines, and on the order of davg for other graphs. We then run spectral methods separately on each subgraph Gl induced by Vl, in the hope of achieving approximate estimates {XVli | i \u2208 Vl}\u2014up to global phase\u2014for each subgraph.\n\u2022 Stage 2: stiching the estimates. The aim of this stage is to stitch together the outputs of Stage 1 computed in isolation for the collection of overlapping subgraphs. If approximate recovery (up to some global phase) has been achieved in Stage 1 for each Vl, then the outputs for any two adjacent subsets\nAlgorithm 3 : Spectral-Stitching\n1. Split all vertices into several (non-disjoint) vertex subsets each of size W as follows\nVl := {i | (i\u2212 1)W/2 + 1 \u2264 l \u2264 (i\u2212 1)W/2 +W } , l = 1, 2, \u00b7 \u00b7 \u00b7 ,\nand run spectral method (Algorithm 2) on each subgraph induced by Vl, which yields estimates {XVlj | j \u2208 Vl} for each l \u2265 1.\n2. Stitching: set X(0)j \u2190 XV1j for all j \u2208 V1; for l = 2, 3, \u00b7 \u00b7 \u00b7 ,\nX (0) j \u2190 XVlj (\u2200j \u2208 Vl) if \u2211 j\u2208Vl\u2229Vl\u22121 XVlj \u2295X Vl\u22121 j \u2264 0.5 |Vl \u2229 Vl\u22121| ;\nand X(0)j \u2190 XVlj \u2295 1 (\u2200j \u2208 Vl) otherwise.\n3. Successive local refinement and output X(T )i , 1 \u2264 i \u2264 n (see Steps 3-4 of Algorithm 1).\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\nare positively correlated only when they have matching global phases. This simple observation allows us to calibrate the global phases for all preceding estimates, thus yielding a vector {X(0)i }1\u2264i\u2264n that is approximately faithful to the truth modulo some global phase.\nThe remaining steps of Spectral-Stitching follow the same local refinement procedure as in SpectralExpanding, and we can employ the same ordering of vertices as in Spectral-Expanding. See Algorithm 3 and Fig. 3. As can be seen, the first 2 stages of Spectral-Stitching\u2014which can also be completed in nearly linear time\u2014are more \u201csymmetric\u201d than those of Spectral-Expanding. More precisely, Spectral-Expanding emphasizes a single core subgraph Gc and computes all other estimates based on Gc, while Spectral-Stitching treats each subgraph Gl almost equivalently. This symmetry nature might be practically beneficial when the acquired data deviate from our assumed random sampling model."}, {"heading": "3.2 Theoretical Guarantees: Rings", "text": "We start with the performance of our algorithms for rings. This class of graphs\u2014which is spatially invariant\u2014 is arguably the simplest model exhibiting locality structure.\n7"}, {"heading": "3.2.1 Minimum Sample Complexity", "text": "Encouragingly, the proposed algorithms succeed in achieving the minimum sample complexity, as stated below.\nTheorem 1. Fix \u03b8 > 0 and any small > 0. Let G be a ring Rr with locality radius r, and suppose\nm \u2265 (1 + )m\u2217, (5)\nwhere m\u2217 =\nn log n\n2 ( 1\u2212 e\u2212KL(0.5\u2016\u03b8) ) . (6)\nThen with probability approaching one3, Spectral-Expanding (resp. Spectral-Stitching) converges to the ground truth within T = O (log n) iterations, provided that r & log3 n (resp. r \u2265 n\u03b4 for an arbitrary constant \u03b4 > 0).\nConversely, if m < (1\u2212 )m\u2217, then the probability of error Pe(\u03c8) is approaching one for any algorithm \u03c8.\nRemark 2. When r = n\u2212 1, a ring reduces to a complete graph (or an equivalent Erd\u0151s-R\u00e9nyi model). For this case, computationally feasible algorithms have been extensively studied [5, 9, 18, 37, 38], most of which focus only on the scaling results. Recent work [15,39] succeeded in characterizing the sharp threshold for this case, and it is immediate to check that the sample complexity we derive in (6) matches the one presented in [15,39]. Remark 3. Theorem 1 requires r & poly log(n) because each node needs to be connected to sufficiently many neighbors in order to preclude \u201cbursty\u201d errors. The condition r & log3 n might be improved to a lower-order poly log (n) term using more refined analyses. When r . log n, one can compute the maximum likelihood (ML) estimate via dynamic programming [27] within polynomial time.\nTheorem 1 uncovers a surprising insensitivity phenomenon for rings: as long as the measurement graph is sufficiently connected, the locality constraint does not alter the sample complexity limit and the computational limit at all. This subsumes as special cases two regimes that exhibit dramatically different graph structures: (1) complete graphs, where the samples are taken in a global manner, and (2) rings with r = O(poly log (n)), where the samples are constrained within highly local neighborhood. In addition, Theorem 1 does not impose any assumption on the ground truth {Xi : 1 \u2264 i \u2264 n}; in fact, the success probability of the proposed algorithms is independent of the true community assignment.\nNotably, both [13] and [40] have derived general sufficient recovery conditions of SDP which, however, depend on the second-order graphical metrics of G [14] (e.g. the spectral gap or Cheeger constant). When applied to rings (or other graphs with locality), the sufficient sample complexity given therein is significantly larger than the information limit4. This is in contrast to our finding, which reveals that for many graphs with locality, both the information and computation limits often depend only upon the vertex degrees independent of these second-order graphical metrics."}, {"heading": "3.2.2 Bottlenecks for Exact Recovery", "text": "Before explaining the rationale of the proposed algorithms, we provide here some heuristic argument as to why n log n samples are necessary for exact recovery and where the recovery bottleneck lies.\nWithout loss of generality, assume X = [0, \u00b7 \u00b7 \u00b7 , 0]>. Suppose the genie tells us the correct labels of all nodes except v. Then all samples useful for recovering Xv reside on the edges connecting v and its neighbors, and there are Poisson(\u03bbdv) such samples. Thus, this comes down to testing between two conditionally i.i.d. distributions with a Poisson sample size of mean \u03bbdv. From the large deviation theory, the ML rule fails in recovering Xv with probability\nPe,v \u2248 exp { \u2212\u03bbdv(1\u2212 e\u2212D \u2217 ) } , (7)\n3More precisely, the proposed algorithms succeed with probability exceeding 1\u2212 c1r\u22129\u2212C2 exp{\u2212c2mn (1\u2212e \u2212D\u2217 )} for some\nconstants c1, c2, C2 > 0. 4For instance, the sufficient sample complexity given in [13] scales as n logn\nhGD\u2217 with hG denoting the Cheeger constant. Since\nhG = O(1/n) for rings / lines, this results in a sample size that is about n times larger than the information limit.\nwhere D\u2217 is the large deviation exponent. The above argument concerns a typical error event for recovering a single node v, and it remains to accommodate all vertices. Since the local neighborhoods of two vertices v and u are nearly non-overlapping, the resulting typical error events for recovering Xv and Xu become almost independent and disjoint. As a result, the probability of error of the ML rule \u03c8ml is approximately lower bounded by\nPe(\u03c8ml) & \u2211n\nv=1 Pe,v \u2248 n exp\n{ \u2212\u03bbdavg(1\u2212 e\u2212D \u2217 ) } , (8)\nwhere one uses the fact that dv \u2261 davg. Apparently, the right-hand side of (8) would vanish only if\n\u03bbdavg(1\u2212 e\u2212D \u2217 ) > log n. (9)\nSince the total sample size is m = \u03bb \u00b7 12ndavg, this together with (9) confirms the sample complexity lower bound\nm = 1\n2 \u03bbndavg >\nn log n\n2 (1\u2212 e\u2212D\u2217) = m \u2217.\nAs we shall see, the above error events\u2014in which only a single variable is uncertain\u2014dictate the hardness of exact recovery."}, {"heading": "3.2.3 Interpretation of Our Algorithms", "text": "The preceding argument suggests that the recovery bottleneck of an optimal algorithm should also be determined by the aforementioned typical error events. This is the case for both Spectral-Expanding and Spectral-Stitching, as revealed by the intuitive arguments below. While the intuition is provided for rings, it contains all important ingredients that apply to many other graphs.\nTo begin with, we provide an heuristic argument for Spectral-Expanding.\n(i) Stage 1 focuses on a core complete subgraph Gc. In the regime where m & n log n, the total number of samples falling within Gc is on the order of |Vc|n \u00b7m \u2265 |Vc| log n, which suffices in guaranteeing partial recovery using spectral methods [29]. In fact, the sample size we have available over Gc is way above the degrees of freedom of the variables in Gc (which is r).\n(ii) With decent initial estimates for Gc in place, one can infer the remaining pool of vertices one by one using existing estimates together with backward samples. One important observation is that each vertex is incident to many\u2014i.e. about the order of log n\u2014backward samples. That said, we are effectively operating in a high signal-to-noise ratio (SNR) regime. While existing estimates are imperfect, the errors occur only to a small fraction of vertices. Moreover, these errors are in some sense randomly distributed and hence fairly spread out, thus precluding the possibility of bursty errors. Consequently, one can obtain correct estimate for each of these vertices with high probability, leading to a vanishing fraction of errors in total.\n(iii) Now that we have achieved approximate recovery, all remaining errors can be cleaned up via local refinement using all backward and forward samples. For each vertex, since only a vanishingly small fraction of its neighbors contain errors, the performance of local refinement is almost the same as in the case where all neighbors have been perfectly recovered.\nThe above intuition extends to Spectral-Stitching. Following the argument in (i), we see that the spectral method returns nearly accurate estimates for each of the subgraph Gl induced by Vl, except for the global phases (this arises because each Gl has been estimated in isolation, without using any information concerning the global phase). Since any two adjacent Gl and Gl+1 have sufficient overlaps, this allows us to calibrate the global phases for {XVli : i \u2208 Vl} and {X Vl+1 i : i \u2208 Vl+1}. Once we obtain approximate recovery for all variables simultaneously, the remaining errors can then be cleaned up by Stage 3 as in Spectral-Expanding. We emphasize that the first two stages of both algorithms\u2014which aim at approximate recovery\u2014require only O (n) samples (as long as the pre-constant is sufficiently large). In contrast, the final stage is the bottleneck: it succeeds as long as local refinement for each vertex is successful. The error events for this stage are almost equivalent to the typical events singled out in Section 3.2.2, justifying the informationtheoretic optimality of both algorithms.\n1\n0.2 0.4 0.6 0.8 1\n1\n2\n3 4\nrings\nlines\ngrids\n(r = n )\nc m\u21e4 = c \u00b7 n log n 2(1 e D\u21e4 )\n1\n1\n2\n3\n6\n5\n4\n7\n8\n9\n12\n11\n10\n13\n14\n15\n18\n17\n16\nCommunity Recovery in Graphs with Locality\nAlgorithm 1\n1. Run spectral method (Algorithm 3 of (Chin et al., 2015)) on a core subgraph induced by Vc, which yields estimates X\n(0) j , 1  j  |Vc|.\n2. Progressive estimation: for i = |Vc| + 1, \u00b7 \u00b7 \u00b7 , n,\nX (0) i majority\nn Y\n(l) i,j X (0)\ni | j : j < i, (i, j) 2 E , 1  l  Ni,j\no .\n3. Successive local refinement: for t = 0, \u00b7 \u00b7 \u00b7 , T 1, X\n(t+1) i majority\nn Y\n(l) i,j X (t) j | j : j 6= i, (i, j) 2 E , 1  l  Ni,j\no , 1  i  n.\n4. Output X(T )i , 1  i  n.\nHere, majority {\u00b7} represents the majority voting rule: for any sequence s1, \u00b7 \u00b7 \u00b7 , sk 2 {0, 1}, majority {s1, \u00b7 \u00b7 \u00b7 , sk} is equal to 1 if Pk i=1 si > k/2; and 0 otherwise.\nr\n(a) Stage 1:\n(b) Stage 2:\n(c) Stage 3:\ncore subgraph Vc\n1\nr\n(a) Stage 1:\n(b) Stage 2:\n(c) Stage 3:\ncore subgraph Vc\n1\nr\n(a) Stage 1:\n(b) Stage 2:\n(c) Stage 3:\ncore subgraph Vc\nr\n(a) Stage 1:\n(b) Stage 2:\n(c) Stage 3:\ncore subgraph Gc\ninsert size\nnumber of fragments\n0 2000 4000 6000 8000 10000\n1\nFigure 2. Illustration of Algorithm 1: (a) Stage 1 concerns recovery in a core complete subgraph; (b) Stage 2 makes a forward pass by progressively propagating information through backward samples; (c) Stage 3 refines each Xv by employing all samples incident to v.\napproximate recovery is much lower than that required for exact recovery, and hence the task is feasible even though we do not use any forward samples to estimate Xi.\nStage 3: successive local refinement. Finally, we clean up all estimates using both backward and forward samples in order to maximize recovery accuracy. This is achieved by running local majority voting from the neighbors of each vertex until convergence. As we shall see, this stage is the bottleneck for exact information recovery. Remark 1. The proposed algorithm falls under the category of a general paradigm, which starts with an approximate estimate (often via spectral initialization) followed by iterative refinement. This paradigm has been successfully applied to a wide spectrum of applications ranging from matrix completion (Keshavan et al., 2010a; Jain et al., 2013) to phase retrieval (Candes et al., 2015; Chen & Candes, 2015; Netrapalli et al., 2013) to community recovery (Chaudhuri et al., 2012; Abbe et al., 2016; Mossel et al., 2015; Gao et al., 2015); see the discussion therein.\nAn important feature of the proposed algorithm is its low computational complexity. First of all, the spectral method can be performed within O (mc log n) time by means of the power method, where mc indicates the number of sam-\nples falling on Gc. Stage 2 entails one round of majority voting, whereas the final stage\u2014as we will demonstrate\u2014 converges within at most O (log n) rounds of majority voting. Note that each round of majority voting can be completed in linear time, i.e. in time proportional to reading all samples. Taken collectively, we see that Algorithm 1 can be accomplished within O (m log n) flops, which is nearly linear time.\nCareful readers will recognize that Stages 2-3 share similarities with BP, and might wonder whether Stage 1 can also be replaced with standard BP. Unfortunately, we are not aware of any approach to analyze the performance of vanilla BP without a decent initial guess. Note, however, that the spectral method is already nearly linear-time, and is hence at least as fast as any feasible procedure.\nWhile the preceding paradigm is presented for lines / rings, it easily extends to a much broader family of graphs with locality. The only places that need to be adjusted are:\n(1) The core subgraph Vc. One would like to ensure that |Vc| & davg and that the subgraph Gc induced by Vc forms a (nearly) complete subgraph, in order to guarantee decent recovery in Stage 1.\nFigure 5: Labeling / ordering of the vertex set for a grid, where the core subgraph consists of the r2 vertices on the bottom left."}, {"heading": "3.3 Theoretical Guarantees: Inhomogeneous Graphs", "text": "The proposed algorithms are guaranteed to succeed for a much broader class of graphs with locality beyond rings, including those that exhibit inhomogeneous vertex degrees. The following theorem formalizes this claim for two of the most important instances: lines and grids.\nTheorem 2. Theorem 1 continues to hold for the following families of measurement graphs: (1) Lines with r = n\u03b2 for some constant 0 < \u03b2 < 1, where\nm\u2217 = max {1/2, \u03b2}n log n\n1\u2212 e\u2212KL(0.5\u2016\u03b8) ; (10)\n(2) Grids with r = n\u03b2 for some constant 0 < \u03b2 < 0.5, where\nm\u2217 = max {1/2, 4\u03b2}n log n\n1\u2212 e\u2212KL(0.5\u2016\u03b8) . (11)\nRemark 4. Note that both Spectral-Expanding and Spectral-Stitching rely on the labeling / ordering of the vertex set V. For lines, it suffices to employ the same ordering and core subgraph as for rings. For grids, we can start by taking the core subgraph to be a subsquare of area r2 lying on the bottom left of the grid, and then follow a serpentine trajectory running alternately from the left to the right and then back again; see Fig. 5 for an illustration.\n10\nRemark 5. Careful readers will note that for lines (resp. grids), m\u2217 does not converge to n logn 2(1\u2212e\u2212KL(0.5\u2016\u03b8)) as \u03b2 \u2192 1 (resp. \u03b2 \u2192 0.5), which is the case of complete graphs. This arises because m\u2217 experiences a more rapid drop in the regime where \u03b2 = 1 (resp. \u03b2 = 0.5). For instance, for a line with r = \u03b3n for some constant \u03b3 > 0, one has m\u2217 = (1\u2212\u03b3/2)n logn\n1\u2212e\u2212KL(0.5\u2016\u03b8) .\nTheorem 2 characterizes the effect of locality radius upon the sample complexity limit; see Fig. 4 for a comparison of three classes of graphs. In contrast to rings, lines and grids are spatially varying models due to the presence of boundary vertices, and the degree of graph inhomogeneity increases in the locality radius r. To be more concrete, consider, for example, the first davg/ log n vertices of a line, which have degrees around davg/2. In comparison, the set of vertices lying away from the boundary have degrees as large as davg. This tells us that the first few vertices form a weakly connected component, thus presenting an additional bottleneck for exact recovery. This issue is negligible unless the size of the weakly connected component is exceedingly large. As asserted by Theorem 2, the minimum sample complexity for lines (resp. grids) is identical to that for rings unless r & \u221an (resp. r & n1/8). Note that the curves for lines and grids (Fig. 4) have distinct hinge points primarily because the vertex degrees of the corresponding weakly connected components differ.\nMore precisely, the insights developed in Section 3.2.2 readily carry over here. Since the error probability of the ML rule is lower bounded by (8), everything boils down to determining the smallest \u03bb (called \u03bb\u2217) satisfying \u2211n\nv=1 exp\n{ \u2212\u03bb\u2217dv ( 1\u2212 e\u2212D\u2217 )} \u2192 0,\nwhich in turn yields m\u2217 = 12\u03bb \u2217davgn. The two cases accommodated by Theorem 2 can all be derived in this way."}, {"heading": "3.4 Connection to Low-Rank Matrix Completion", "text": "One can aggregate all correct parities into a matrix Z = [Zi,j ]1\u2264i,j\u2264n such that Zi,j = 1 if Xi = Xj and Zi,j = \u22121 otherwise. It is straightforward to verify that rank (Z) = 1, with each Y (l)i,j being a noisy measurement of Zi,j . Thus, our problem falls under the category of low-rank matrix completion, a topic that has inspired a flurry of research (e.g. [41\u201345]). Most prior works, however, concentrated on samples taken over an Erd\u0151s\u2013R\u00c3 c\u00a9nyi model, without investigating sampling schemes with locality constraints. One exception is [46], which explored the effectiveness of SDP under general sampling schemes. However, the sample complexity required therein increases significantly as the spectral gap of the measurement graph drops, which does not deliver optimal guarantees. We believe that the approach developed herein will shed light on solving general matrix completion problems from samples with locality."}, {"heading": "4 Extension: Beyond Pairwise Measurements", "text": "The proposed algorithms are applicable to numerous scenarios beyond the basic setup in Section 2.1. This section presents two important extension beyond pairwise measurements."}, {"heading": "4.1 Sampling with Nonuniform Weight", "text": "In many applications, the sampling rate is nonuniform across different edges; for instance, it might fall off with distance between two incident vertices. In the haplotype phasing application, Fig. 7(a) gives an example of a distribution of the separation between mate-paired reads (insert size). One would naturally wonder whether our algorithm works under this type of more realistic models.\nMore precisely, suppose the number of samples over each (i, j) \u2208 E is independently generated obeying\nNi,j ind\u223c Poisson (\u03bbwi,j) , (12)\nwhere wi,j > 0 incorporates a sampling rate weighting for each edge. This section focuses on lines / grids / rings for concreteness, where we impose the following assumptions in order to make the sampling model more \u201csymmetric\u201d:\n(i) Lines / grids: wi,j depends only on the Euclidean distance between vertices i and j;\n(ii) Rings: wi,j depends only on i\u2212 j (mod n). Theorem 3. Theorems 1-2 continue to hold under the above nonuniform sampling model, provided that max(i,j)\u2208E wi,j min(i,j)\u2208E wi,j is bounded.\nTheorem 3 might be surprising at first glance: both the performance of our algorithms and the fundamental limit depend only on the weighted average of the vertex degrees, and are insensitive to the degree distributions. This can be better understood by examining the three stages of Spectral-Expanding and Spectral-Stitching. To begin with, Stages 1-2 are still guaranteed to work gracefully, since we are still operating in a high SNR regime irrespective of the specific values of {wi,j}. The main task thus amounts to ensuring the success of local clean-up. Repeating our heuristic treatment in Section 3.2.2, one sees that the probability of each singleton error event (i.e. false recovery of Xv when the genie already reveals the true labels of other nodes) depends only on the average number of samples incident to each vertex v, namely,\nE [Nv] := \u2211\nj E [Nv,j ] = \u2211 j:(v,j)\u2208E \u03bbwv,j .\nDue to the symmetry assumptions on {wi}, the total sample size m scales linearly with E [Nv], and hence the influence of {wi} is absorbed into m and ends up disappearing from the final expression.\nAnother prominent example is the class of small-world graphs. In various human social networks, one typically observes both local friendships and a (significantly lower) portion of long-range connections, and small-world graphs are introduced to incorporate this feature. To better illustrate the concept, we focus on the following spatially-invariant instance, but it naturally generalizes to a much broader family.\n\u2022 Small-world graphs. Let G be a superposition of a complete graph G0 = (V, E0) and a ring G1 with connectivity radius r. The sampling rate is given by\nNi,j ind.\u223c { Poisson (w0) , if (i, j) \u2208 E0; Poisson (w1) , else.\nWe assume that w0n 2\nw1nr = O (1), in order to ensure higher weights for local connections.\nTheorem 4. Theorem 1 continues to hold under the above small-world graph model, provided that r & log3 n."}, {"heading": "4.2 Beyond Pairwise Measurements", "text": "In some applications, each measurement may cover more than two nodes in the graph. In the haplotype phasing application, for example, a new sequencing technology called 10x [47] generates barcodes to mark reads from the same chromosome (maternal or paternal), and more than two reads can have the same barcode. For concreteness, we suppose the locality constraint is captured by rings, and consider the type of multiple linked samples as follows.\n\u2022 Measurement (hyper)-graphs. Let G0 = (V, E0) be a ring Rr, and let G = (V, E) be a hyper-graph such that (i) every hyper-edge is incident to L vertices in V, and (ii) all these L vertices are mutually connected in G0.\n\u2022 Noise model. On each hyper-edge e = (i1, \u00b7 \u00b7 \u00b7 , iL) \u2208 G, we obtain Ne ind.\u223c Poisson (\u03bb) multi-linked samples {Y (l)e | 1 \u2264 l \u2264 Ne}. Conditional on Ne, each sample Y (l)e is an independent copy of\nYe = { (Zi1 , \u00b7 \u00b7 \u00b7 , ZiL) , with prob. 0.5, (Zi1 \u2295 1, \u00b7 \u00b7 \u00b7 , ZiL \u2295 1) , else,\n(13)\nwhere Zi is a noisy measurement of Xi such that\nZi = { Xi, with probability 1\u2212 p; Xi \u2295 1, otherwise.\n(14)\nHere, p represents the error rate for measuring a single vertex. For the pairwise samples considered before, one can think of the parity error rate \u03b8 as P {Zi \u2295 Zj 6= Xi \u2295Xj} or, equivalently, \u03b8 = 2p(1\u2212p).\nWe emphasize that a random global phase is incorporated into each sample (13). That being said, each sample reveals only the relative similarity information among these L vertices, without providing further information about the absolute cluster membership.\nSince Algorithm 1 and Algorithm 3 operate only upon pairwise measurements, one alternative is to convert each L-wise sample Ye = (Yi1 , \u00b7 \u00b7 \u00b7 , YiL) into ( L 2 ) pairwise samples of the form Yij \u2295Yil (for all j 6= l), and then apply the spectral methods on these parity samples. In addition, the majority voting procedure specified in Algorithm 1 needs to be replaced by certain local maximum likelihood rule as well, in order to take advantage of the mutual data correlation within each L-wise measurement. The modified algorithms are summarized in Algorithms 4 and 5. Interestingly, these algorithms are still information-theoretically optimal, as asserted by the following theorem.\nTheorem 5. Fix L \u2265 2, and consider Algorithms 4 and 5 . Theorem 1 continues to hold under the above L-wise sampling model, with m\u2217 replaced by\nm\u2217 := n log n\nL ( 1\u2212 e\u2212D(P0,P1) ) .\nHere, { P0 = (1\u2212 p)Binomial (L\u2212 1, p) + pBinomial (L\u2212 1, 1\u2212 p) ; P1 = pBinomial (L\u2212 1, p) + (1\u2212 p)Binomial (L\u2212 1, 1\u2212 p) .\n(15)\nHere, the Chernoff information D(P0, P1) can be expressed in closed form as\nD(P0, P1) = \u2212 log { L\u22121\u2211\ni=0\n( L\u2212 1 i )\u221a{ pi (1\u2212 p)L\u2212i + (1\u2212 p)i pL\u2212i }{ pi+1 (1\u2212 p)L\u2212i\u22121 + (1\u2212 p)i+1 pL\u2212i\u22121 }} .\n(16) In particular, when L = 2, this reduces to5 D(P0, P1) = KL (0.5 \u2016 \u03b8) for \u03b8 := 2p(1 \u2212 p), which matches our results with pairwise samples.\nInterestingly, D(P0, P1) enjoys a very simple asymptotic limit as L scales, as stated in the following lemma.\nLemma 1. Fix any 0 < p < 1/2. The Chernoff information D(P0, P1) given in Theorem 5 obeys\nlim L\u2192\u221e\nD (P0, P1) = KL (0.5 \u2016 p) . (17)\nProof. See Appendix F.1.\nRemark 6. The asymptotic limit (17) admits a simple interpretation. Consider the typical event where only X1 is uncertain and X2 = \u00b7 \u00b7 \u00b7 = Xn = 0. Conditional on Z1, the L\u22121 parity samples (Z1 \u2295 Z2, \u00b7 \u00b7 \u00b7 , Z1 \u2295 ZL) are i.i.d., which reveals accurate information about Z1 \u2295 0 in the regime where L\u2192\u221e (by the law of large number). As a result, the uncertainty arises only because Z1 is a noisy version of X1, which behaves like passing X1 through a binary symmetric channel with crossover probability p. This essentially boils down to distinguishing between Bernoulli (p) (when X1 = 0) and Bernoulli (1\u2212 p) (when X1 = 1), for which the associated Chernoff information is known to be KL (0.5 \u2016 p).\nWith Theorem 5 in place, we can determine the benefits of multi-linked sampling. To enable a fair comparison, we evaluate the sampling efficiency in terms of Lm\u2217 rather than m\u2217, since Lm\u2217 captures the total number of vertices (including repetition) one needs to measure. As illustrated in Fig. 4, the sampling efficiency improves as L increases, but there exists a fundamental lower barrier given by n logn\n1\u2212e\u2212KL(0.5\u2016p) . This lower barrier, as plotted in the black curve of Fig. 4, corresponds to the case where L is approaching infinity.\n5This follows since, when L = 2, D(P0, P1) = \u2212 log { 2 \u221a ((1\u2212 p)2 + p2) (2p (1\u2212 p)) } = \u2212 log { 2 \u221a (1\u2212 \u03b8) \u03b8 } = KL (0.5 \u2016 \u03b8) .\nAlgorithm 4 : Spectral-Expanding for multi-linked samples\n1. Break each L-wise sample Ye = (Yi1 , \u00b7 \u00b7 \u00b7 , YiL) into ( L 2 ) pairwise samples of the form Yij \u2295 Yil (for all\nj 6= l), and run spectral method (Algorithm 2) on a core subgraph induced by Vc using these parity samples. This yields estimates X(0)j , 1 \u2264 j \u2264 |Vc|.\n2. Progressive estimation: for k = |Vc|+ 1, \u00b7 \u00b7 \u00b7 , n,\nX (0) k \u2190 local\u2212ML{X(0)i |1\u2264i<k} { Y (l)e | e = (i1, \u00b7 \u00b7 \u00b7 , iL) with iL = k, 1 \u2264 l \u2264 Ne } .\n3. Successive local refinement: for t = 0, \u00b7 \u00b7 \u00b7 , T \u2212 1,\nX (t+1) k \u2190 local\u2212ML{X(t)i |i 6=k} { Y (l)e | k \u2208 e, 1 \u2264 l \u2264 Ne } , 1 \u2264 k \u2264 n.\n4. Output X(T )k , 1 \u2264 k \u2264 n.\nHere, local\u2212MLX {\u00b7} represents the local maximum likelihood rule: for any sequence s1, \u00b7 \u00b7 \u00b7 , sN \u2208 {0, 1}L,\nlocal\u2212ML{Zi|1\u2264i<k} {s1, \u00b7 \u00b7 \u00b7 , sN} = { 1, if \u2211N j=1 log P{sj |Xk=1,Xi=Zi (1\u2264i<k)} P{sj |Xk=0,Xi=Zi (1\u2264i<k)} \u2265 0,\n0, else,\nand\nlocal\u2212ML{Zi|i6=k} {s1, \u00b7 \u00b7 \u00b7 , sN} = { 1, if \u2211N j=1 log P{sj |Xk=1,Xi=Zi (i 6=k)} P{sj |Xk=0,Xi=Zi (i6=k)} \u2265 0,\n0, else.\nAlgorithm 5 : Spectral-Stitching for multi-linked samples\n1. Break each L-wise sample Ye = (Yi1 , \u00b7 \u00b7 \u00b7 , YiL) into ( L 2 ) pairwise samples of the form Yij \u2295 Yil (for all\nj 6= l). Run Steps 1-2 of Algorithm 3 using these pairwise samples to obtain estimates X(0)j , 1 \u2264 j \u2264 n.\n2. Run Step 3 of Algorithm 4 and output X(T )k , 1 \u2264 k \u2264 n."}, {"heading": "5 Numerical Experiments", "text": "To verify the practical applicability of the proposed algorithms, we have conducted simulations in various settings. All these experiments focused on graphs with n = 100, 000 vertices, and used an error rate of \u03b8 = 10% unless otherwise noted. For each point, the empirical success rates averaged over 10 Monte Carlo runs are reported.\n(a) Regular rings. We ran Algorithm 1 on rings Rr for various values of locality radius r (Fig. 6(a)), with the runtime reported in Table 1;\n(b) Small-world graphs. We ran Algorithm 1 on small-world graphs, where the aggregate sampling rate for Rr is chosen to be on the same order as that for the complete graph (Fig. 6(b));\n(c) Rings with nonuniform sampling weight. We ran Algorithm 1 for rings with nonuniform sampling rate (Fig. 6(c)). Specifically, the distance between two vertices involved in a parity sample is drawn according to Poisson(r/2);\n(d) Rings with different error rates. We varied the error rate \u03b8 for rings with r = 18 = n0.25, and plotted the empirical success rate (Fig. 6(d)).\nWe have also simulated a model of the haplotype phasing problem by assuming that the genome has a SNP periodically every 1000 base pairs. The insert length distribution, i.e. the distribution of the genomic distance between the linking reads, is given in Fig. 6(c) for Illumina reads, and a draw from Poisson(3.5) truncated within the interval 1, \u00b7 \u00b7 \u00b7 , 9 is a reasonable approximation for the number of SNPs between two measured SNPs. We then ran the simulation on the rings R9, with non-uniform sampling weight. Using the\nnominal error rate of p = 1% for the short reads, the error rates of the measurements is 2p(1 \u2212 p) \u2248 2%. The empirical performance is shown in Fig. 6(d).\nAdditionally, we have simulated reads generated by 10x-Genomics [47] , which corresponds to the model in Section 4. Each measurement consists of multiple linked reads, which is generated by first randomly picking a segment of length 100 SNPs (called a fragment) on the line graph and then generating Poisson(9) number of linked reads uniformly located in this segment. The noise rate per read is p = 0.01. The empirical result is shown in Fig. 6(e). The information theoretic limit is calculated using Theorem 5, with L set to infinity (since the number of vertices involved in a measurement is quite large here).\nTo evaluate the performance of our algorithm on real data, we ran Spectral-Stitching for Chromosomes 1-22 on the NA12878 data-set made available by 10x-Genomics [49]. The nominal error rate per read is p = 1%, and the average number of SNPs touched by each sample is L \u2208 [6, 7]. The number of SNPs n ranges from 34240 to 191829, with the sample size m from 102633 to 574189. Here, we split all vertices into overlapping subsets of size W = 100. The performance is measured in terms of the switch error rate, that is, the fraction of positions where we need to switch the estimate to match the ground truth. The performance on Chromosomes 1-22 is reported in Tab. 2 and Fig. 8."}, {"heading": "6 Discussion", "text": "We have presented two efficient algorithms that are information theoretically optimal. Rather than resorting to a \u201cglobal\u201d method that attempts recovery of all nodes all at once, the proposed algorithms emphasize local subgraphs whose nodes are mutually well connected, and then propagate information across different parts of the graph. This way we are able to exploit the locality structure in a computationally feasible manner.\nThis paper leaves open numerous directions for further investigation. To begin with, the current work concentrates on the simplest setup where only two communities are present. It would be of interest to extend the results to the case with M > 2 communities, which naturally arises in many applications including haplotype phasing for polyploid species [50]. Furthermore, what would be the information and computation limits in the regime where the number M of communities scales with n? In fact, there often exists a computational barrier away from the information limit when the measurement graph is drawn from the Erd\u0151s-R\u00e9nyi model for large M (e.g. [51]). How will this computational barrier be influenced by the locality structure of the measurement graph? In addition, the present theory operates under the assumption that L is a fixed constant, namely, each multi-linked measurement entails only a small number of samples. Will the proposed algorithms still be optimal if L is so large that it has to scale with n?\nMore broadly, it remains to develop a unified and systematic approach to accommodate a broader family of graphs beyond the instances considered herein. In particular, what would be an optimal recovery scheme if the graph is far from spatially-invariant or if there exist a few fragile cuts? Finally, as mentioned before, it would be interesting to see how to develop more general low-rank matrix completion paradigms, when the revealed entries come from a sampling pattern that exhibits locality structure."}, {"heading": "A Preliminaries", "text": "Before continuing to the proofs, we gather a few facts that will be useful throughout. First of all, recall that the maximum likelihood (ML) decision rule achieves the lowest Bayesian probability of error, assuming uniform prior over two hypotheses of interest. The resulting error exponent is determined by the Chernoff information, as given in the following lemma.\nLemma 2. Fix any > 0. Suppose we observe a collection of Nz random variables Z = {Z1, \u00b7 \u00b7 \u00b7 , ZNz} that are i.i.d. given Nz. Consider two hypotheses H0: Zi \u223c P0 and H1: Zi \u223c P1 for two given probability measures P0 and P1. Assume that the Chernoff information D\u2217 = D (P0, P1) > 0 and the alphabet of Zi are both finite and fixed, and that maxz\nP1(z) P0(z) <\u221e. (a) Conditional on Nz, one has\nexp {\u2212 (1 + )NzD\u2217} \u2264 P0 ( P1 (Z) P0 (Z) \u2265 1 \u2223\u2223\u2223\u2223Nz ) \u2264 exp {\u2212NzD\u2217} , (18)\nwhere the lower bound holds when Nz is sufficiently large. (b) If Nz \u223c Poisson (N), then\nexp { \u2212 (1 + )N ( 1\u2212 e\u2212D\u2217 )} \u2264 P0 ( P1 (Z) P0 (Z) \u2265 1 ) \u2264 exp { \u2212N ( 1\u2212 e\u2212D\u2217 )} , (19)\nwhere the lower bound holds when N is sufficiently large.\nProof. See Appendix F.2.\nWe emphasize that the best achievable error exponent coincides with the Chernoff information D\u2217 when the sample size is fixed, while it becomes 1 \u2212 e\u2212D\u2217\u2014which is sometimes termed the Chernoff-Hellinger divergence\u2014when the sample size is Poisson distributed.\nThe next result explores the robustness of the ML test. In particular, we control the probability of error when the ML decision boundary is slightly shifted, as stated below.\nLemma 3. Consider any > 0, and let N \u223c Poisson (\u03bb). (a) Fix any 0 < \u03b8 < 0.5. Conditional on N , draw N independent samples Z1, \u00b7 \u00b7 \u00b7 , ZN such that Zi \u223c Bernoulli (\u03b8), 1 \u2264 i \u2264 N . Then one has\nP\n{ N\u2211\ni=1\nZi \u2265 1\n2 N \u2212 \u03bb\n} \u2264 exp ( \u00b7 2 log 1\u2212 \u03b8\n\u03b8 \u03bb\n) exp { \u2212\u03bb ( 1\u2212 e\u2212KL(0.5\u2016\u03b8) )} . (20)\n(b) Let P0 and P1 be two distributions obeying maxz P1(z) P0(z) <\u221e. Conditional on N , draw N independent samples Zi \u223c P0, 1 \u2264 i \u2264 N . Then one has\nP0    N\u2211\nj=1\nlog P1 (Zi)\nP0 (Zi) \u2265 \u2212 \u03bb    \u2264 exp ( \u03bb) exp { \u2212\u03bb ( 1\u2212 e\u2212D\u2217 )} , (21)\nwhere D\u2217 = D (P0, P1) denotes the Chernoff information between P0 and P1.\nProof. See Appendix F.3.\nFurther, the following lemma develops an upper bound on the tail of Poisson random variables.\nLemma 4. Suppose that N \u223c Poisson ( \u03bb) for some 0 < < 1. Then for any c1 > 2e, one has\nP { N \u2265 c1\u03bb\nlog 1\n} \u2264 2 exp { \u2212c1\u03bb\n2\n} .\nProof. See Appendix F.4.\nAdditionally, our analysis relies on the well-known Chernoff-Hoeffding inequality [52, Theorem 1, Eqn (2.1)].\nLemma 5 (Chernoff-Hoeffding Inequality). Suppose Z1, \u00b7 \u00b7 \u00b7 , Zn are independent Bernoulli random variables with mean E [Zi] \u2264 p. Then for any 1 > q \u2265 p, one has\nP { 1\nn \u2211n j=1 Zj \u2265 q } \u2264 exp {\u2212nKL (q \u2016 p)} ,\nwhere KL (q \u2016 p) := q log qp + (1\u2212 q) log 1\u2212q 1\u2212p .\nWe end this section with a lower bound on the KL divergence between two Bernoulli distributions.\nFact 1. For any 0 \u2264 q \u2264 \u03c4 \u2264 1,\nKL (\u03c4 \u2016 q) := \u03c4 log \u03c4 q + (1\u2212 \u03c4) log 1\u2212 \u03c4 1\u2212 q \u2265 \u03c4 log (\u03c4/q)\u2212 \u03c4.\nProof. By definition,\nKL (\u03c4 \u2016 q) (i) \u2265 \u03c4 log \u03c4\nq + (1\u2212 \u03c4) log (1\u2212 \u03c4)\n(ii) \u2265 \u03c4 log (\u03c4/q)\u2212 \u03c4,\nwhere (i) follows since log 11\u2212q \u2265 0, and (ii) arises since (1\u2212 \u03c4) log (1\u2212 \u03c4) \u2265 \u2212 (1\u2212 \u03c4) \u03c4 \u2265 \u2212\u03c4 ."}, {"heading": "B Performance Guarantees of Spectral-Expanding", "text": "The analyses for all the cases follow almost identical arguments. In what follows, we separate the proofs into two parts: (1) the optimality of Spectral-Expanding and Spectral-Stitching, and (2) the minimax lower bound, where each part accommodates all models studied in this work.\nWe start with the performance guarantee of Spectral-Expanding in this section. Without loss of generality, we will assume X1 = \u00b7 \u00b7 \u00b7 = Xn = 0 throughout this section. For simplicity of presentation, we will focus on the most challenging boundary case where m n log n, but all arguments easily extend to the regime where m n log n.\nB.1 Stage 1 gives approximate recovery for Gc This subsection demonstrates that the spectral method (Algorithm 2) is successful in recovering a portion 1\u2212 o (1) of the variables in Vc with high probability, as stated in the following lemma. Lemma 6. Fix \u03b8 > 0. Suppose that G = (V, E) is a complete graph and the sample size m & n log n. The estimate X(0) = [ X\n(0) i ] 1\u2264i\u2264n returned by Algorithm 2 obeys\nmin { \u2016X(0) \u2212X\u20160, \u2016X(0) + X\u20160 } = o (n) (22)\nwith probability exceeding 1\u2212O ( n\u221210 ) .\nProof. See Appendix F.6. Remark 7. Here, 1\u2212O ( n\u221210 ) can be replaced by 1\u2212O (n\u2212c) for any other positive constant c > 0.\nRemark 8. It has been shown in [29, Theorem 1.6] that a truncated version of the spectral method returns reasonably good estimates even in the sparse regime (i.e. m n). Note that truncation is introduced in [29, Theorem 1.6] to cope with the situation in which some rows of the sample matrix are \u201cover-represented\u201d. This becomes unnecessary in the regime where m & n log n, since the number of samples incident to each vertex concentrates around \u0398 (log n), thus precluding the existence of \u201cover-represented\u201d rows.\nAccording to Lemma 6, Stage 1 accurately recovers (1 \u2212 o (1))|Vc| variables in Vc modulo some global phase, as long as \u03bb|Vc|2 & |Vc| \u00b7 log n. Since \u03bb m/n2 and |Vc| davg, this condition is equivalent to\nm & n log n,\nwhich falls within our regime of interest. Throughout the rest of the section, we will assume without loss of generality that\n1\n|Vc|\nVc\u2211\ni=1\n1 { X\n(0) i 6= Xi\n} = o (1) ,\ni.e. the first stage obtains approximate recovery along with the correct global phase.\nB.2 Stage 2 yields approximate recovery for V\\Vc For concreteness, we start by establishing the achievability for lines and rings, which already contain all important ingredients for proving the more general cases.\nB.2.1 Lines / rings\nWe divide all vertices in V\\Vc into small groups {Vi}, each consisting of log3 n adjacent vertices6:\nVi := { |Vc|+ (i\u2212 1) log3 n+ 1, \u00b7 \u00b7 \u00b7 , |Vc|+ i \u00b7 log3 n } ,\nwhere > 0 is some arbitrarily small constant. In what follows, we will control the estimation errors happening within each group. For notational simplicity, we let V0 := Vc. An important vertex set for the progressive step, denoted by V\u2192i, is the one encompassing all vertices preceding and connected to Vi; see Fig. 9 for an illustration.\nThe proof is recursive, which mainly consists in establishing the claim below. To state the claim, we need to introduce a collection of events as follows\nA0 := { at most a fraction\n2 of progressive estimates\n{ X\n(0) j : j \u2208 Vc\n} is incorrect } ;\nAi := { at most a fraction of progressive estimates { X (0) j : j \u2208 Vi } is incorrect } , i \u2265 1.\nLemma 7. For any i \u2265 0, conditional on A0 \u2229 \u00b7 \u00b7 \u00b7 \u2229 Ai, one has\nP {Ai+1 | A0 \u2229 \u00b7 \u00b7 \u00b7 \u2229 Ai} \u2265 1\u2212O ( n\u221210 ) . (23)\nAs a result, one has\nP {\u2229i\u22650Ai} \u2265 1\u2212O ( n\u22129 ) . (24)\nApparently, A0 holds with high probability; see the analysis for Stage 1. Thus, if (23) holds, then (24) follows immediately from the union bound. In fact, (24) suggests that for any group Vi, only a small fraction of estimates obtained in this stage would be incorrect, thus justifying approximate recovery for this stage. Moreover, since the neighborhood N (v) of each node v \u2208 Vi is covered by at most O ( davg |Vi| ) groups, the event\n\u2229i\u22650Ai immediately suggests that there are no more than O ( \u00b7 |Vi|)O ( davg |Vi| ) = O ( davg) errors occurring to either the neighborhood N (v) or the backward neighborhood N (v) \u2229 V\u2192i. This observation will prove useful for analyzing Stage 3, and hence we summarize it in the following lemma.\n6Note that the errors occurring to distinct vertices are statistically dependent in the progressive estimation stage. The approach we propose is to look at a group of vertices simultaneously, and to bound the fraction of errors happening within this group. In order to exhibit sufficiently sharp concentration, we pick the group size to be at least log3 n. A smaller group is possible via more refined arguments.\n1\n1\n1\n1\n1\n1\n1\n1\n1\nLemma 8. There are at most O ( davg) errors occurring to either the neighborhood N (v) or the backward neighborhood N (v) \u2229 V\u2192i.\nThe rest of the section is devoted to establish the claim (23) in Lemma 7.\nProof of Lemma 7. As discussed above, it suffices to prove (23) (which in turn justifies (24)). The following argument is conditional on A0 \u2229 \u00b7 \u00b7 \u00b7 \u2229 Ai and all estimates for V0 \u222a \u00b7 \u00b7 \u00b7 \u222a Vi; we shall suppress the notation by dropping this conditional dependence whenever it is clear from the context.\nConsider any vertex u \u2208 Vi+1. In the progressive estimation step, each X(0)u relies on the preceding estimates { X\n(0) j | j : j < u, (j, u) \u2208 E } , as well as the set Bu of backward samples incident to u, that is,\nBu := { Y (l) u,j | j < u, (j, u) \u2208 E , 1 \u2264 l \u2264 Nu,j } ;\nsee Fig. 9(a). We divide Bu into two parts\n\u2022 Bgoodu : the set of samples Y (l)u,j in Bu such that (i) X (0) j = Xj , and (ii) j \u2208 V\u2192(i+1);\n\u2022 Bbadu : the remaining samples Bu\\Bgoodu ;\nand set Ngoodu := \u2223\u2223Bgoodu \u2223\u2223 and Nbadu := \u2223\u2223Bbadu \u2223\u2223 .\nIn words, Bgoodu is associated with those preceding estimates in V\u2192(i+1) that are consistent with the truth, while Bbadu entails the rest of the samples that might be unreliable. See Fig. 9(b) for an illustration. The purpose of this partition is to separate out Bbadu , which only accounts for a small fraction of all samples.\nWe now proceed to analyze the majority voting procedure which, by definition, succeeds if the total votes favoring the truth exceeds 12 ( Ngoodu +N bad u ) . To preclude the effect of Bbadu , we pay particular attention to the part of votes obtained over Bgoodu ; that is, the partial score\nscoregoodu := \u2211\nY (l) u,j\u2208B good u\nX (0) j \u2295 Y (l) u,j .\nIt is self-evident to check that the above success condition would hold if\nscoregoodu < 1\n2\n( Ngoodu +N bad u ) \u2212 \u2223\u2223Bbadu \u2223\u2223 = 1 2 Ngoodu \u2212 1 2 Nbadu ,\n21\nand we further define the complement event as\nDu := { scoregoodu \u2265 1\n2 Ngoodu \u2212\n1 2 Nbadu\n} .\nThe main point to work with Du is that conditional on all prior estimates in V\u2192(i+1), the Du\u2019s are independent across all u \u2208 Vi+1.\nWe claim that\nP {Du} = exp {\u2212\u0398 (log n)} := Pe,1. (25)\nIf this claim holds, then we can control the number of incorrect estimates within the group Vi+1 via the Chernoff-Hoeffding inequality. Specifically,\nP    1 log3 n \u2211 u\u2208Vi+1 1 { X(0)u 6= Xu } \u2265 1 log2 n    \u2264 P    1 log3 n \u2211 u\u2208Vi+1 1 {Du} \u2265 1 log2 n   \n(a) \u2264 exp { \u2212 log3 n \u00b7 KL ( 1\nlog2 n\n\u2225\u2225\u2225\u2225 E [1 {Du}] )} \u2264 exp { \u2212 log3 n \u00b7 KL ( 1\nlog2 n\n\u2225\u2225\u2225\u2225 Pe,1 )}\n(b) \u2264 exp { \u2212 log3 n \u00b7 1\nlog2 n\n( log\n1 Pe,1 log 2 n \u2212 1 )}\n(c) = exp { \u2212\u0398 ( log2 n )} < O\n( 1\nn10\n) ,\nwhere (a) follows from Lemma 5, (b) arises from Fact 1, and (c) is a consequence of (25). This reveals that the fraction of incorrect estimates for Vi is vanishingly small with high probability, thus establishing the claim (23).\nFinally, it remains to prove (25). To this end, we decouple Du into two events:\nP {Du} \u2264 P { Nbadu \u2265 c0 log n\nlog 1\n} + P { scoregoodu \u2265 1\n2 N trueu \u2212\n1\n2\nc0 log n\nlog 1\n} (26)\nfor some universal constant c0 > 0. Recall that each edge is sampled at a Poisson rate \u03bb m|E0| n logn ndavg logn davg\n, and that the average number of samples connecting u and other nodes in Vi+1 is atmost \u03bb \u00b7 O ( davg) (recalling our assumption that r & log3 n). On the event A0\u2229\u00b7 \u00b7 \u00b7\u2229Ai, the number of wrong labels in V\u2192(i+1) is O ( du), and hence\nE [ Nbadu ] \u2264 O (\u03bb du) \u2264 c2 log n (27)\nfor some constant c2 > 0. This further gives\nE [ Ngoodu ] \u2265 \u03bbc3du \u2212 E [ Nbadu ] \u2265 (1\u2212 c4 ) c3\u03bbdu\nfor some constants c3, c4 > 0. Thus, Lemma 4 and the inequality (27) taken collectively yield\nP { Nbadu \u2265 c1c2 log n\nlog 1\n} \u2264 2 exp { \u2212c1c2 log n\n2\n}\nfor any c1 > 2e. In addition, in view of Lemma 3, there exists some function \u03be\u0303 (\u00b7) such that\nP { scoregoodu \u2265 1\n2 N trueu \u2212\n1\n2\nc0 log n\nlog 1\n} \u2264 exp { \u2212 (1\u2212 on (1)) ( 1\u2212 \u03be\u0303 ( ) ) c3\u03bbdu ( 1\u2212 e\u2212D\u2217 )}\n= exp {\u2212\u0398 (log n)} ,\nwhere \u03be\u0303 ( ) is independent of n and vanishes as \u2192 0. Putting these bounds together reveals that: when \u03bb logndavg and c0 = c1c2, there exists some function \u03be\u0302 ( ) independent of n such that\n(26) \u2264 2 exp { \u2212c1c2 log n\n2\n} + exp { \u2212 (1\u2212 on (1)) ( 1\u2212 \u03be\u0303 ( ) ) \u03bbdavg 2 ( 1\u2212 e\u2212D\u2217 )}\n= exp {\u2212\u0398 (log n)} := Pe,1, (28)\nwhere \u03be\u0302 ( ) vanishes as \u2192 0. This finishes the proof.\nB.2.2 Beyond lines / rings\nThe preceding analysis only relies on very few properties of lines / rings, and can be readily applied to many other graphs. In fact, all arguments continue to hold as long as the following assumptions are satisfied:\n1. In G, each vertex v (v > |Vc|) is connected with at least \u0398(davg) vertices in {1, \u00b7 \u00b7 \u00b7 , v \u2212 1} by an edge; 2. For any v \u2208 Vi (i \u2265 1), its backward neighborhood N (v) \u2229 V\u2192i is covered by at most O ( davg |Vi| ) =\nO (\ndavg log3 n ) distinct groups among V1, \u00b7 \u00b7 \u00b7 ,Vi\u22121.\nIn short, the first condition ensures that the information of a diverse range of prior vertices can be propagated to each v, whereas the second condition guarantees that the estimation errors are fairly spread out within the backward neighborhood associated with each node.\nWe are now in position to look at grids, small-world graphs, as well as lines / rings with nonuniform weights.\n(a) It is straightforward to verify that the choices of Vc and the ordering of V suggested in Section 3.3 satisfy Conditions 1-2, thus establishing approximate recovery for grids.\n(b) Suppose max(i,j)\u2208E wi,jmin(i,j)\u2208E wi,j is bounded. Define the weighted degree as\ndwv := \u2211\ni:(i,v)\u2208E wi,v\nand let the average weighted degree be dwavg := 1 n \u2211 dwv . Then all arguments continue to hold if dv and davg are replaced by dwv and dwavg, respectively. This reveals approximate recovery for lines / rings / grids under sampling with nonuniform weight.\n(c) The proof for small-world graphs follows exactly the same argument as for rings.\n(d) For the case with multi-linked samples, we redefine several metrics as follows:\n\u2013 Bu: the set of backward samples { Y (l) e | u \u2208 e, j < u for all other j \u2208 e, 1 \u2264 l \u2264 Ne } , where e\nrepresents the hyper-edge;\n\u2013 Bgoodu : the set of samples Y (l)e in Bu such that (i) X(0)j = Xj for all j \u2208 e and j 6= u, and (ii) j \u2208 V\u2192(i+1) for all j \u2208 e and j 6= u;\n\u2013 Bbadu : the remaining samples Bu\\Bgoodu . \u2013 We also need to re-define the score scoregoodu as\nscoregoodu := \u2211\nY (l) e \u2208Bgoodu\nlog P { Y (l) e | Xu = 1, Xi = X(0)i (i \u2208 e, i 6= u) }\nP { Y\n(l) e | Xu = 0, Xi = X(0)i (i \u2208 e, i 6= u)\n}\nwith the decision boundary replaced by 0 and the event Du replaced by\nDu := { scoregoodu \u2265 0\u2212 smax \u00b7 |Bbadu | } = { scoregoodu \u2265 \u2212smaxNbadu } .\nHere, smax indicates the maximum possible likelihood ratio for each L-wise sample:\nsmax := max Ye,{Zi} \u2223\u2223\u2223\u2223log P {Ye | Xu = 1, Xi = Zi (i \u2208 e, i 6= u)} P {Ye | Xu = 0, Xi = Zi (i \u2208 e, i 6= u)} \u2223\u2223\u2223\u2223 .\nWith these metrics in place, all proof arguments for the basic setup carry over to the multi-linked sample case.\nB.3 Stage 3 achieves exact recovery We now turn to the last stage, and the goal is to prove that X(t) converges to X within O (log n) iterations. Before proceeding, we introduce a few more notations that will be used throughout.\n\u2022 For any vertex v, denote by N (v) the neighborhood of v in G, and let S (v) be the set of samples that involve v;\n\u2022 For any vector Z = [Z1, \u00b7 \u00b7 \u00b7 , Zn]> and any set I \u2286 {1, \u00b7 \u00b7 \u00b7 , n}, define the `0 norm restricted to I as follows\n\u2016Z\u20160,I := \u2211\ni\u2208I 1 {Zi 6= 0} .\n\u2022 Generalize the definition of the majority vote operator such that\nmajority (Z) = [majority1(Z1), \u00b7 \u00b7 \u00b7 ,majorityn(Zn)]>\nobtained by applying majorityv (\u00b7) component-wise, where\nmajorityv (Zv) := { 1, if Zv \u2265 12 |S (v) |; 0, else.\n\u2022 Let VZ (resp. VX) denote the local voting scores using Z = [Zi]1\u2264i\u2264n (resp. X = [Xi]1\u2264i\u2264n = 0) as the current estimates, i.e. for any 1 \u2264 u \u2264 n,\n(VZ)u = \u2211\nY (l) i,u\u2208S(u)\nY (l) i,u \u2295 Zi; (29)\n(VX)u = \u2211\nY (l) i,u\u2208S(u)\nY (l) i,u \u2295Xi =\n\u2211\ny (l) i,u\u2208S(u)\nY (l) i,u . (30)\nWith these notations in place, the iterative procedure can be succinctly written as\nX(t+1) = majority (VX(t)) .\nThe main subject of this section is to prove the following theorem.\nTheorem 6. Consider any 0 < \u2264 0, where 0 is some sufficiently small constant. Define\nZ := { Z \u2208 {0, 1}n | \u2200v : \u2016Z \u2212X\u20160,N (v) \u2264 dv } . (31)\nThen with probability approaching one,\nmajority (VZ) \u2208 Z 1 2 , \u2200Z \u2208 Z and \u2200 \u2208\n[ 1\ndmax , 0\n] .\nRemark 9. When the iterate falls within the set Z (cf. (31)), there exist only a small number of errors occurring to the neighborhood of each vertex. This essentially implies that (i) the fraction of estimation errors is low; (ii) the estimation errors are fairly spread out instead of clustering within the neighborhoods of a few nodes.\nRemark 10. This is a uniform result: it holds regardless of whether Z is statistically independent of the samples Y or not. This differs from many prior results (e.g. [17]) that employ fresh samples in each stage in order to decouple the statistical dependency.\nNote that the subscript of Z indicates the fraction of estimation errors allowed in an iterate. According to the analyses for the the preceding stages, Stage 3 is seeded with some initial guess X(0) \u2208 Z for some arbitrarily small constant > 0. This taken collectively with Theorem 6 gives rise to the following error contraction result: for any t \u2265 0,\n\u2016X(t+1) \u2212X\u20160,N (v) = \u2016majority (VX(t))\u2212X\u20160,N (v) \u2264 1\n2 \u2016X(t) \u2212X\u20160,N (v), 1 \u2264 v \u2264 n. (32)\nThis reveals the geometric convergence rate of X(t), namely, X(t) converges to the truth within O (log n) iterations, as claimed.\nThe rest of this section is devoted to proving Theorem 6. We will start by proving the result for any fixed candidate Z \u2208 Z independent of the samples, and then generalize to simultaneously accommodate all Z \u2208 Z . Our strategy is to first quantify VX (which corresponds to the score we obtain when only a single vertex is uncertain), and then control the difference between VX and VZ . We make the observation that all entries of VX are strictly below the decision boundary, as asserted by the following lemma.\nLemma 9. Fix any small constant \u03b4 > 0, and suppose that m n log n. Then one has\n(VX)u < 1 2 |S (u)| \u2212 \u03b4 log n = 1 2 |S (u)| \u2212 \u03b4 \u00b7O (\u03bbdu) , 1 \u2264 u \u2264 n\nwith probability exceeding 1 \u2212 C1 exp { \u2212c1mn ( 1\u2212 e\u2212D\u2217 )} for some constants C1, c1 > 0, provided that the following conditions are satisfied: (1) Rings with r & log2 n:\nm > ( 1 + \u03be (\u03b4) ) n log n 2 ( 1\u2212 e\u2212KL(0.5\u2016\u03b8) ) ;\n(2) Lines with r = n\u03b2 for some constant 0 < \u03b2 < 1:\nm > ( 1 + \u03be (\u03b4) ) max { \u03b2, 1\n2\n} n log n\n1\u2212 e\u2212KL(0.5\u2016\u03b8) ;\n(3) Lines with r = \u03b3n for some constant 0 < \u03b3 \u2264 1:\nm > ( 1 + \u03be (\u03b4) )( 1\u2212 1\n2 \u03b3\n) n log n\n1\u2212 e\u2212KL(0.5\u2016\u03b8) ;\n(4) Grids with r = n\u03b2 for some constant 0 < \u03b2 < 1/2:\nm > ( 1 + \u03be (\u03b4) ) max { 4\u03b2, 1\n2\n} n log n\n1\u2212 e\u2212KL(0.5\u2016\u03b8) ;\n(5) Small-world graphs:\nm > ( 1 + \u03be (\u03b4) ) n log n 2 ( 1\u2212 e\u2212KL(0.5\u2016\u03b8) ) ;\nIn all these cases, \u03be (\u00b7) is some function independent of n satisfying \u03be (\u03b4)\u2192 0 as \u03b4 \u2192 0. Here, we allow Cases (1), (2) and (4) to have nonuniform sampling weight over different edges, as long as max(i,j)\u2208E wi,jmin(i,j)\u2208E wi,j is bounded.\nProof. See Appendix F.7.\nIt remains to control the difference between VX and VZ :\n\u2206Z := VZ \u2212 VX .\nSpecifically, we would like to demonstrate that most entries of \u2206Z are bounded in magnitude by \u03b4 log n (or \u03b4 \u00b7O (\u03bbdu)), so that most of the perturbations are absolutely controlled. To facilitate analysis, we decouple the statistical dependency by writing\nVZ = FZ + BZ ,\nwhere FZ represents the votes using only forward samples, namely,\n(FZ)u = \u2211\ni>u, Y (l) i,u\u2208S(u)\nY (l) i,u \u2295 Zi, 1 \u2264 u \u2264 n.\nThis is more convenient to work with since the entries of FZ (orBZ) are jointly independent. In what follows, we will focus on bounding FZ , but all arguments immediately apply to BZ . To simplify presentation, we also decompose VX into two parts VX = FX + BX in the same manner.\nNote that the vth entry of the difference\n\u2206F := FZ \u2212 FX (33)\nis generated by those entries from indices in N (v) satisfying Zv 6= Xv. From the assumption (31), each \u2206Fv (1 \u2264 v \u2264 n) is dependent on at most O( dv) non-zero entries of Z \u2212X, and hence on average each \u2206Fv is only affected by O (\u03bb \u00b7 davg) samples. Moreover, each non-zero entry of Z \u2212X is bounded in magnitude by a constant. This together with Lemma 4 yields that: for any sufficiently large constant c1 > 0,\nP {\u2223\u2223\u2206Fi\n\u2223\u2223 \u2265 c1\u03bbdavg log 1\n} \u2264 2 exp {\u2212\u0398 (c1\u03bbdavg)} \u2264 2n\u2212c2 , (34)\nprovided that \u03bbdavg & log n (which is the regime of interest), where c2 = \u0398 (c1) is some absolute positive constant. In fact, for any index i, if \u2223\u2223\u2206Fi \u2223\u2223 \u2265 c1\u03bbdavg log 1 , then picking sufficiently small > 0 we have\n\u2223\u2223\u2206Fi \u2223\u2223 \u03bbdavg of \u2223\u2223\u2206Fi \u2223\u2223 log n,\nand hence (FZ)i and (FX)i become sufficiently close. The preceding bound only concerns a single component. In order to obtain overall control, we introduce a set of independent indicator variables {\u03b7i (Z)}:\n\u03b7i (Z) :=\n{ 1, if \u2223\u2223\u2206Fi \u2223\u2223 \u2265 c1\u03bbdavglog(1/ ) ,\n0, else.\nFor any 1 \u2264 v \u2264 n, applying Lemma 5 gives\nP    1\ndv\n\u2211\ni\u2208N (v) \u03b7i (Z) \u2265 \u03c4\n   \u2264 exp { \u2212dvKL ( \u03c4 \u2016 max i E [\u03b7i (Z)] )}\n\u2264 exp { \u2212dv ( \u03c4 log \u03c4 2n\u2212c2 \u2212 \u03c4 )} ,\nwhere the last line follows from Fact 1 as well as (34). For any \u03c4 \u2265 1/n,\n\u03c4 log \u03c4\n2n\u2212c2 \u2212 \u03c4 & \u03c4 log n,\nindicating that\nP    1\ndv\n\u2211\ni\u2208N (v) \u03b7i (Z) \u2265 \u03c4\n   \u2264 exp {\u2212c3\u03c4davg log n}\nfor some universal constant c3 > 0. If we pick > 0 and \u03c4 > 0 to be sufficiently small, we see that with high probability most of the entries have\n\u2223\u2223\u2206Fi \u2223\u2223 < c1\u03bbdavg\nlog 1 \u03bbdavg.\nWe are now in position to derive the results in a more uniform fashion. Suppose that dmax = Kdavg. When restricted to Z , the neighborhood of each v can take at most ( Kdavg Kdavg ) 2 Kdavg different possible values. If we set \u03c4 = 14 , then in view of the union bound,\nP   \u2203Z \u2208 Z s.t. 1\ndv\n\u2211\ni\u2208N (v) \u03b7i (Z) \u2265 \u03c4\n   \u2264 ( Kdavg Kdavg ) 2 Kdavg exp {\u2212c3\u03c4davg log n}\n\u2264 (2Kdavg) Kdavg exp {\u2212c3\u03c4davg log n} \u2264 exp {(1 + o (1)) Kdavg log n} exp { \u22121\n4 c3 davg log n\n}\n\u2264 exp { \u2212 ( 1\n4 c3 \u2212 (1 + o (1))K\n) davg log n } .\nSince Z,X \u2208 {0, 1}n, it suffices to consider the case where \u2208 {\ni dv | 1 \u2264 v \u2264 n, 1 \u2264 i \u2264 dv\n} , which has\nat most O ( n2 ) distinct values. Set c3 to be sufficiently large and apply the union bound (over both v and\n) to deduce that: with probability exceeding 1\u2212 exp (\u2212\u0398 ( davg log n)) \u2265 1\u2212O ( n\u221210 ) ,\ncard { i \u2208 N (v) : \u2223\u2223\u2206Fi \u2223\u2223 \u2265 c1\u03bbdavg\nlog 1\n} \u2264 1\n4 dv, 1 \u2264 v \u2264 n, (35)\nholds simultaneously for all Z \u2208 Z and all \u2265 1dmax 1 davg .\nThe uniform bound (35) continues to hold if \u2206F is replaced by \u2206B. Putting these together suggests that with probability exceeding 1\u2212 exp (\u2212\u0398 ( d log n)),\ncard { i \u2208 N (v) : |(\u2206Z)i| \u2265 2c1\u03bbdavg\nlog 1\n}\n\u2264 card { i \u2208 N (v) : \u2223\u2223(\u2206F ) i \u2223\u2223 \u2265 c1\u03bbdavg log 1 } + card { i \u2208 N (v) : \u2223\u2223(\u2206B ) i \u2223\u2223 \u2265 c1\u03bbdavg log 1 }\n\u2264 1 2 dv, 1 \u2264 v \u2264 n\nholds simultaneously for all Z \u2208 Z and all \u2265 1dmax . Taking \u03b4 to be 2c1/ log 1 in (74), we see that all but 1 2 dv entries of VZ = VX + \u2206Z at indices from N (v) exceed the voting boundary. Consequently, the majority voting yields\n\u2016majority (VZ)\u2212X\u20160,N (v) \u2264 1\n2 dv, 1 \u2264 v \u2264 n\nor, equivalently, majority (VZ) \u2208 Z 1\n2 , \u2200Z \u2208 Z\nas claimed. When it comes to the multi-linked reads, we need to make some modification to the vectors defined above. Specifically, we define the score vector VZ and VX to be\n(VZ)u = \u2211\nY (l) e \u2208S(u)\nlog P { Y (l) e | Xu = 1, Xi = Zi (for all i 6= u and u \u2208 e) }\nP { Y (l) e | Xu = 0, Xi = Zi (for all i 6= u and u \u2208 e) } , (36)\n(VX)u = \u2211\nY (l) e \u2208S(u)\nlog P { Y (l) e | Xu = 1, Xi = 0 (for all i 6= u and u \u2208 e) }\nP { Y (l) e | Xu = 0, Xi = 0 (for all i 6= u and u \u2208 e) } , (37)\nand replace the majority voting procedure as\nmajorityv (Zv) := { 1, if Zv \u2265 0; 0, else.\nWith these changes in place, the preceding proof extends to the multi-linked sample case with little modification, as long as L remains a constant. We omit the details for conciseness."}, {"heading": "C Performance Guarantees of Spectral-Stitching", "text": "We start from the estimates { XVlj : j \u2208 Vl } obtained in Stage 1. Combining Lemma 6 and the union bound, we get\n1\n|Vl| min    \u2211 j\u2208Vl 1 { XVlj 6= Xj } , \u2211 j\u2208Vl 1 { XVlj \u2295 1 6= Xj }    = o (1) , l = 1, 2, \u00b7 \u00b7 \u00b7\nwith probability exceeding 1 \u2212 O (n\u2212c) for any constant c > 0. In other words, we achieve approximate recovery\u2014up to some global phase\u2014for each vertex group Vl. The goal of Stage 2 is then to calibrate these estimates so as to make sure all groups enjoy the same global phase. Since each group suffers from a fraction o(1) of errors and any two adjacent groups share O(|Vl|) vertices, we can easily see that two groups of estimates { XVlj : j \u2208 Vl } and { X Vl\u22121 j : j \u2208 Vl\u22121 } have positive correlation, namely,\n\u2211\nj\u2208Vl\u2229Vl\u22121 XVlj \u2295X Vl\u22121 j \u2264 1 2 |Vl \u2229 Vl\u22121| ,\nonly when they share the same global phase. As a result, there are at most o(n) occurring to the estimates{ X\n(0) i | 1 \u2264 i \u2264 n } obtained in Stage 2. Moreover, the way we choose Vl ensures that the neighborhood Nv\nof each vertex v is contained within at most O ( davg |V1| ) groups, thus indicating that\n1\n|Nv| min    \u2211 j\u2208Nv 1 { X (0) j 6= Xj } , \u2211 j\u2208Nv 1 { X (0) j \u2295 1 6= Xj }    = o (1) , v = 1, \u00b7 \u00b7 \u00b7 , n;\nthat is, the estimation errors are fairly spread out across the network. Finally, Spectral-Expanding and Spectral-Stitching employ exactly the same local refinement stage, and hence the proof for Stage 3 in SpectralExpanding readily applies here. This concludes the proof."}, {"heading": "D Minimax Lower Bound", "text": "This section contains the proof for the converse parts of Theorems 1-5; that is, the minimax probability of error inf\u03c8 Pe (\u03c8)\u2192 1 unless m \u2265 (1\u2212 )m\u2217 in all of these theorems.\nD.1 Pairwise samples with uniform weight We begin with the simplest sampling model: pairwise measurements with uniform sampling rate at each edge, which are the scenarios considered in Theorems 1-2. The key ingredient to establish the minimax lower bounds is to prove the following lemma.\nLemma 10. Fix any constant > 0, and suppose that Ni,j ind.\u223c Poisson (\u03bb) for all (i, j) \u2208 E. Consider any vertex subset U \u2286 V with |U| \u2265 n , and denote by d\u0303 the maximum degree of the vertices lying within U . If\n\u03bbd\u0303 \u2264 (1\u2212 ) log |U| 1\u2212 e\u2212D\u2217 , (38)\nthen the probability of error inf\u03c8 Pe (\u03c8)\u2192 1 as n\u2192\u221e.\nProof. See Appendix F.5.\nWe are now in position to demonstrate how Lemma 10 leads to tight lower bounds. In what follows, we let davg denote the average vertex degree in G. \u2022 Rings. When G = (V, E) is a ring Rr with connectivity radius r, set U = V = {1, \u00b7 \u00b7 \u00b7 , n} and fix\nany small constant > 0. It is self-evident that d\u0303 = davg. Applying Lemma 10 leads to a necessary recovery condition\n\u03bbdavg > (1\u2212 ) log n\n1\u2212 e\u2212D\u2217 . (39)\nSince m = \u03bb|E| = 12\u03bbndavg, this condition (39) is equivalent to\nm > (1\u2212 ) \u00b7 n log n 2 (1\u2212 e\u2212D\u2217) .\n\u2022 Lines with r = n\u03b2 for some constant 0 < \u03b2 < 1. Take U = {1, \u00b7 \u00b7 \u00b7 , r} for some sufficiently small constant 0 < < \u03b2, which obeys |U| = n\u03b2 \u2265 n for large n and d\u0303 = (1 +O ( )) davg/2. In view of Lemma 10, a necessary recovery condition is\n\u03bbd\u0303 > (1\u2212 ) log |U| 1\u2212 e\u2212D\u2217 ,\n\u21d0\u21d2 1 2 \u03bbdavg > 1\u2212 1 +O ( ) \u00b7 \u03b2 log n+ log 1\u2212 e\u2212D\u2217 .\nIn addition, if we pick U = V, then d\u0303 = davg. Lemma 10 leads to another necessary condition:\n\u03bbdavg > (1\u2212 ) \u00b7 log n\n1\u2212 e\u2212D\u2217 .\nCombining these conditions and recognizing that can be arbitrarily small, we arrive at the following necessary recovery condition\n1 2 \u03bbdavg > (1\u2212 ) max\n{ \u03b2, 1\n2\n} n log n\n1\u2212 e\u2212D\u2217 . (40)\nWhen \u03b2 < 1, the edge cardinality obeys |E| = (1 + o(1))ndavg/2, allowing us to rewrite (40) as\nm = \u03bb|E| > 1\u2212 1 + o(1) max\n{ \u03b2, 1\n2\n} n log n\n1\u2212 e\u2212D\u2217 .\n\u2022 Lines with r = \u03b3n for some constant 0 < \u03b3 \u2264 1. Take U = {1, \u00b7 \u00b7 \u00b7 , r} for some sufficiently small constant > 0, which obeys |U| = \u03b3n \u2265 n for large n and d\u0303 = (1 +O ( )) r. Lemma 10 reveals the following necessary recovery condition:\n\u03bbd\u0303 > (1\u2212 ) log |U| 1\u2212 e\u2212D\u2217\n\u21d0\u21d2 \u03bbr > 1\u2212 1 +O ( ) \u00b7 log n+ log ( \u03b3) 1\u2212 e\u2212D\u2217 . (41)\nOn the other hand, the total number of edges in G is given by\n|E| = 1 + o (1) 2\n( n2 \u2212 (n\u2212 r)2 ) = (1 + o (1))nr ( 1\u2212 1\n2\nr\nn\n) = (1 + o (1))nr ( 1\u2212 1\n2 \u03b3\n) .\nThis taken collectively with (41) establishes the necessary condition\nm = \u03bb|E| = (1 + o (1))\u03bbnr (\n1\u2212 1 2 \u03b3\n) (42)\n> (1\u2212O ( )) (\n1\u2212 1 2 \u03b3\n) n log n\n1\u2212 e\u2212D\u2217 ,\nwhich completes the proof for this case by recognizing that can be arbitrary.\n\u2022 Grids with r = n\u03b2 for some constant 0 < \u03b2 < 1. Consider a sub-square of edge length r lying in the bottom left corner of the grid, and let U consist of all 2r2 vertices residing within the subsquare. This obeys |U| = 2n2\u03b2 > n for large n and small , and we also have d\u0303 = ( 1 +O ( 2 )) davg/4.\nAccording to Lemma 10, a necessary recovery condition is\n\u03bbd\u0303 > (1\u2212 ) log |U| 1\u2212 e\u2212D\u2217\nor, equivalently, 1\n4 \u03bbdavg > 1\u2212 1 +O ( 2) \u00b7 2 (\u03b2 log n+ log ) 1\u2212 e\u2212D\u2217 .\nIn addition, by taking U = V one has d\u0303 = davg; applying Lemma 10 requires\n\u03bbdavg > (1\u2212 ) \u00b7 log n\n1\u2212 e\u2212D\u2217\nfor exact recovery. Putting these two conditions together we derive\n\u03bbdavg > (1\u2212O ( )) max {8\u03b2, 1} n log n\n1\u2212 e\u2212D\u2217 ,\nwhich is equivalent to\nm = \u03bb|E| > (1\u2212O ( )) max { 4\u03b2, 1\n2\n} n log n\n1\u2212 e\u2212D\u2217\nsince |E| = (1 + o (1))ndavg/2.\nD.2 Pairwise samples with nonuniform weight The preceding analyses concerning the minimax lower bounds can be readily extended to the sampling model with nonuniform weight, which is the focus of Theorem 3. To be precise, defining the weighted degree of any node v as\ndwv := \u2211\ni:(i,v)\u2208E wi,v, (43)\nwe can generalize Lemma 10 as follows.\nLemma 11. Suppose that max(i,j)\u2208E wi,jmin(i,j)\u2208E wi,j is bounded. Then Lemma 10 continues to the hold for the sampling model with nonuniform weight, provided that d\u0303 is defined as the maximum weighted degree within V1 and that Ni,j ind.\u223c Poisson(\u03bbwi,j) for all (i, j) \u2208 E.\nProof. See Appendix F.5.\nThis lemma allows us to accommodate the following scenarios, as studied in Theorem 3.\n\u2022 Lines / rings / grids under nonuniform sampling. In view of Lemma 11, the preceding proof in Section D.1 continues to hold in the presence of nonuniform sampling weight, provided that davg is replaced with the average weighted degree 1n \u2211n v=1 d w v .\n\u2022 Small-world graphs. The proof for rings is applicable for small-world graphs as well, as long as davg is replaced by the average weighted degree.\nD.3 Multi-linked samples Finally, the above results immediately extend to the case with multi-linked samples.\nLemma 12. Consider the model with multi-linked samples introduced in the main text, and suppose that L and > 0 are both fixed constants. Let U \u2286 V be any vertex subset obeying |U| \u2265 n , and denote by d\u0303 the maximum degree (defined with respect to the hyper-edges) of the vertices within U . If\n\u03bbd\u0303 \u2264 (1\u2212 ) log |U| 1\u2212 e\u2212D\u2217 , (44)\nthen the probability of error inf\u03c8 Pe (\u03c8)\u2192 1 as n\u2192\u221e.\nProof. See Appendix F.5.\nLemma 13. Fix any constant > 0, and suppose that Ni,j ind.\u223c Poisson (\u03bb) for all (i, j) \u2208 E. Consider any vertex subset U \u2286 V with |U| \u2265 n , and denote by d\u0303 the maximum degree of the vertices lying within U . If\n\u03bbd\u0303 \u2264 (1\u2212 ) log |U| 1\u2212 e\u2212D\u2217 , (45)\nthen the probability of error inf\u03c8 Pe (\u03c8)\u2192 1 as n\u2192\u221e.\nWhen specialized to rings, setting U = {1, \u00b7 \u00b7 \u00b7 , n} with d\u0303 = davg gives rise to the necessary condition\n\u03bbdavg > (1\u2212 ) log n\n1\u2212 e\u2212D\u2217 , (46)\nwhere davg represents the average number of hyper-edge degree. Since each hyper-edge covers L vertices, accounting for the over-count factor gives m = 1Ln\u03bbdavg, allowing us to rewrite (46) as\nm > (1\u2212 ) n log n L (1\u2212 e\u2212D\u2217) .\nThis establishes the converse bound in the presence of multi-linked samples."}, {"heading": "E Chernoff Information for Multi-linked Samples", "text": "Suppose now that each vertex v is involved in Nv multi-linked samples or, equivalently, Nv (L\u2212 1) pairwise samples. Careful readers will note that these parity samples are not independent. The key step in dealing with such dependency is not to treat them as Nv (L\u2212 1) independent samples, but instead Nv independent groups. Thus, it suffices to compute the Chernoff information associated with each group, as detailed below.\nWithout loss of generality, suppose only X1 is uncertain and X2 = \u00b7 \u00b7 \u00b7 = Xn = 0. Consider a multi-linked sample that covers X1, \u00b7 \u00b7 \u00b7 , XL. According to our model, each L-wise sample is an independent copy of (13). Since we never observe the global phase in any sample, a sufficient statistic for Ye is given by\nY\u0303e = (Z1 \u2295 Z2, Z1 \u2295 Z3, \u00b7 \u00b7 \u00b7 , Z1 \u2295 ZL) .\nBy definition (3), the Chernoff information D\u2217 is the large-deviation exponent when distinguishing between the conditional distributions of\nY\u0303e | (X1, \u00b7 \u00b7 \u00b7 , XL) = (0, \u00b7 \u00b7 \u00b7 , 0) and Y\u0303e | (X1, \u00b7 \u00b7 \u00b7 , XL) = (1, \u00b7 \u00b7 \u00b7 , 0) , (47)\nwhich we discuss as follows.\n\u2022 When X1 = \u00b7 \u00b7 \u00b7 = XL = 0:\n\u2013 if Z1 = 0 (which occurs with probability 1\u2212 p), then Y\u0303e \u223c Binomial (L\u2212 1, p); \u2013 if Z1 = 1 (which occurs with probability p), then Y\u0303e \u223c Binomial (L\u2212 1, 1\u2212 p);\n\u2022 When X1 = 1 and X2 = \u00b7 \u00b7 \u00b7 = XL = 0:\n\u2013 if Z1 = 0 (which occurs with probability p), then Y\u0303e \u223c Binomial (L\u2212 1, p); \u2013 if Z1 = 1 (which occurs with probability 1\u2212 p), then Y\u0303e \u223c Binomial (L\u2212 1, 1\u2212 p).\nTo summarize, one has\nY\u0303e | (X1, \u00b7 \u00b7 \u00b7 , XL) = (0, 0, \u00b7 \u00b7 \u00b7 , 0) \u223c (1\u2212 p)Binomial (L\u2212 1, p) + pBinomial (L\u2212 1, 1\u2212 p) := P0; Y\u0303e | (X1, \u00b7 \u00b7 \u00b7 , XL) = (1, 0, \u00b7 \u00b7 \u00b7 , 0) \u223c pBinomial (L\u2212 1, p) + (1\u2212 p)Binomial (L\u2212 1, 1\u2212 p) := P1.\nTo derive a closed-form expression, we note that a random variable W0 \u223c P0 obeys\nP0 (W0 = i) = (1\u2212 p) ( L\u2212 1 i ) pi (1\u2212 p)L\u2212i\u22121 + p ( L\u2212 1 i ) (1\u2212 p)i pL\u2212i\u22121\n= ( L\u2212 1 i ){ pi (1\u2212 p)L\u2212i + (1\u2212 p)i pL\u2212i } . (48)\nSimilarly, if W1 \u223c P1, then\nP1 (W1 = i) = ( L\u2212 1 i ){ pi+1 (1\u2212 p)L\u2212i\u22121 + (1\u2212 p)i+1 pL\u2212i\u22121 } . (49)\nBy symmetry (i.e. P0 (W0 = i) = P1 (W1 = L\u2212 1\u2212 i)), one can easily verify that (3) is attained when \u03c4 = 1/2, giving\nD (P0, P1) = \u2212 log { L\u22121\u2211\ni=0\n\u221a P0 (W0 = i)P1 (W1 = i)\n}\n= \u2212 log { L\u22121\u2211\ni=0\n( L\u2212 1 i )\u221a{ pi (1\u2212 p)L\u2212i + (1\u2212 p)i pL\u2212i }{ pi+1 (1\u2212 p)L\u2212i\u22121 + (1\u2212 p)i+1 pL\u2212i\u22121 }} . (50)"}, {"heading": "F Proof of Auxiliary Lemmas", "text": "F.1 Proof of Lemma 1 For notational convenience, set\nbi :=\n\u221a{ pi (1\u2212 p)L\u2212i + (1\u2212 p)i pL\u2212i }{ pi+1 (1\u2212 p)L\u2212i\u22121 + (1\u2212 p)i+1 pL\u2212i\u22121 } . (51)\nFor any i < L2 \u2212 logL, one can verify that\npi (1\u2212 p)L\u2212i + (1\u2212 p)i pL\u2212i = pi (1\u2212 p)L\u2212i { 1 + ( p\n1\u2212 p\n)L\u22122i}\n= (1 + oL (1)) p i (1\u2212 p)L\u2212i\nand\npi+1 (1\u2212 p)L\u2212i\u22121 + (1\u2212 p)i+1 pL\u2212i\u22121 = pi+1 (1\u2212 p)L\u2212i\u22121 { 1 + ( p\n1\u2212 p\n)L\u22122i\u22122}\n= (1 + oL (1)) p i+1 (1\u2212 p)L\u2212i\u22121 .\nThese identities suggest that L/2\u2212logL\u2211\ni=0\n( L\u2212 1 i ) bi = (1 + oL (1)) L/2\u2212logL\u2211\ni=0\n( L\u2212 1 i )\u221a{ pi (1\u2212 p)L\u2212i }{ pi+1 (1\u2212 p)L\u2212i\u22121 }\n= (1 + oL (1)) \u221a p (1\u2212 p)\nL/2\u2212logL\u2211\ni=0\n( L\u2212 1 i ) pi (1\u2212 p)L\u2212i\u22121\n= (1 + oL (1)) \u221a p (1\u2212 p),\nwhere the last line makes use of the following fact.\nFact 2. Fix any 0 < p < 1/2. Then one has\nL/2\u2212logL\u2211\ni=0\n( L\u2212 1 i ) pi (1\u2212 p)L\u2212i\u22121 = 1\u2212 oL (1) .\nProof. To simplify writing, we concentrate on the case where L is even. From the binomial theorem, we see that\nL\u22121\u2211\ni=0\n( L\u2212 1 i ) pi (1\u2212 p)L\u2212i\u22121 = 1. (52)\nHence, it suffices to control \u2211L\u22121 i=L/2\u2212logL+1 ( L\u22121 i ) pi (1\u2212 p)L\u2212i\u22121. To this end, we first make the observation that L/2\u2212logL\u22122\u2211\ni=L/2\u2212logL+1\n( L\u2212 1 i ) pi (1\u2212 p)L\u2212i\u22121 \u2264 (2 logL) max\ni\u2265L2 \u2212logL+1\n( L\u2212 1 i )( p 1\u2212 p )i (1\u2212 p)L\u22121\n\u2264 (2 logL) \u00b7 ( L\u2212 1 L/2 )( p 1\u2212 p )L 2 \u2212logL+1 (1\u2212 p)L\u22121 (i) \u2264 { (2 logL) (1\u2212 p)2 logL\u22123 } \u00b7 2L (p (1\u2212 p))L2 \u2212logL+1\n(ii) \u2264 oL (1) \u00b7 [ 2 (p (1\u2212 p)) 12\u2212 logL L ]L\n= oL (1) , (53)\nwhere (i) comes from the inequalities ( L\u22121 L/2 ) \u2264 2L\u22121 \u2264 2L, and (ii) holds because logL (1\u2212 p)2 logL\u22123 = oL (1). The last identity is a consequence of the inequality \u221a p (1\u2212 p) < 1/2 (\u2200p < 1/2),\nas well as the fact that (p (1\u2212 p))\u2212 logL L \u2192 1 (L\u2192\u221e) and hence\n(p (1\u2212 p)) 12\u2212 logL L = \u221a p(1\u2212 p) (p (1\u2212 p))\u2212 logL L < 1/2.\nOn the other hand, the remaining terms can be bounded as\nL\u22121\u2211\ni=L2 +logL\u22121\n( L\u2212 1 i ) pi (1\u2212 p)L\u2212i\u22121 =\nL\u22121\u2211\ni=L2 +logL\u22121\n( L\u2212 1\nL\u2212 i\u2212 1\n) pi (1\u2212 p)L\u2212i\u22121\n=\nL 2 \u2212logL\u2211\ni=0\n( L\u2212 1 i ) pL\u2212i\u22121 (1\u2212 p)i = L 2 \u2212logL\u2211\ni=0\n( L\u2212 1 i ) pi (1\u2212 p)L\u2212i\u22121 \u00b7 ( p 1\u2212 p )L\u22122i\u22121\n= oL (1) \u00b7 L 2 \u2212logL\u2211\ni=0\n( L\u2212 1 i ) pi (1\u2212 p)L\u2212i\u22121 .\nPutting the above results together yields\n1 =\n  L/2\u2212logL\u2211\ni=0\n+\nL\u22121\u2211\ni=L2 +logL\u22121 +\nL/2\u2212logL\u22122\u2211\ni=L/2\u2212logL+1\n  ( L\u2212 1 i ) pi (1\u2212 p)L\u2212i\u22121\n= (1 + oL (1))\nL/2\u2212logL\u2211\ni=0\n( L\u2212 1 i ) pi (1\u2212 p)L\u2212i\u22121 + oL (1) ,\nwhich in turn gives\nL/2\u2212logL\u2211\ni=0\n( L\u2212 1 i ){ pi (1\u2212 p)L\u2212i\u22121 } = 1\u2212 oL (1)\nas claimed.\nFollowing the same arguments, we arrive at\nL\u22121\u2211\ni=L/2+logL\n( L\u2212 1 i ) bi = (1 + oL (1)) \u221a p (1\u2212 p).\nMoreover,\nL/2+logL\u22121\u2211\ni=L/2\u2212logL+1\n( L\u2212 1 i ) bi \u2264 L/2+logL\u22121\u2211\ni=L/2\u2212logL+1\n( L\u2212 1 i ){ pi (1\u2212 p)L\u2212i + (1\u2212 p)i pL\u2212i }\n+\nL/2+logL\u22121\u2211\ni=L/2\u2212logL+1\n( L\u2212 1 i ){ pi+1 (1\u2212 p)L\u2212i\u22121 + (1\u2212 p)i+1 pL\u2212i\u22121 }\n= O    L/2+logL\u22121\u2211\ni=L/2\u2212logL+1\n( L\u2212 1 i ){ pi (1\u2212 p)L\u2212i\u22121 }   \n= oL (1) ,\nwhere the last line follows the same step as in the proof of Fact 2 (cf. (53)). Taken together these results lead to\nL\u22121\u2211\ni=0\n( L\u2212 1 i ) bi =    L/2\u2212logL\u22121\u2211\ni=0\n+\nL\u22121\u2211\ni=L/2+logL\u22121 +\nL/2+logL\u2211\ni=L/2\u2212logL\n   ( L\u2212 1 i ) bi\n= 2 (1 + oL (1)) \u221a p (1\u2212 p),\nthus demonstrating that\nD (P0, P1) = \u2212 log { 2 (1 + oL (1)) \u221a p (1\u2212 p) } = (1 + oL (1))KL (0.5\u2016p) .\nF.2 Proof of Lemma 2 LetM be the alphabet size for Zi. The standard method of types result (e.g. [53, Chapter 2] and [28, Section 11.7-11.9]) reveals that\n1\n(Nz + 1) M\nexp { \u2212 ( 1 +\n2\n) NzD \u2217 } \u2264 P0 ( P1 (Z) P0 (Z) \u2265 1 \u2223\u2223\u2223\u2223Nz ) \u2264 exp {\u2212NzD\u2217} ; (54)\nhere, the left-hand side holds for sufficiently large Nz, while the right-hand side holds for arbitrary Nz (see [54, Exercise 2.12] or [55, Theorem 1] and recognize the convexity of the set of types under consideration). Moreover, since D\u2217 > 0 and M are fixed, one has 1\n(Nz+1) M = exp (\u2212M log (Nz + 1)) \u2265 exp { \u2212 12 NzD\u2217 } for\nany sufficiently large Nz, thus indicating that\nP0\n( P1 (Z) P0 (Z) \u2265 1 \u2223\u2223\u2223\u2223Nz ) \u2265 exp {\u2212 (1 + )NzD\u2217} (55)\nas claimed.\nWe now move on to the case where Nz \u223c Poisson (N). Employing (55) we arrive at\nP0\n( P1 (Z) P0 (Z) \u2265 1 ) = \u221e\u2211\nl=0\nP (Nz = l)P0 ( P1 (Z) P0 (Z) \u2265 1 \u2223\u2223\u2223\u2223Nz = l ) (56)\n\u2265 \u221e\u2211\nl=N\u0303\nN le\u2212N\nl! exp {\u2212 (1 + ) lD\u2217} (57)\n= e\u2212(N\u2212N0) \u221e\u2211\nl=N\u0303\nN l0 exp (\u2212N0) l!\n(58)\nfor any sufficiently large N\u0303 , where we have introduced N0 := Ne\u2212(1+ )D \u2217 . Furthermore, taking N\u0303 = logN0 we obtain \u221e\u2211\nl=N\u0303\nN l0 l! exp (\u2212N0) = 1\u2212 N\u0303\u2211\nl=0\nN l0 l! exp (\u2212N0) \u2265 1\u2212 N\u0303\u2211\nl=0\nN l0 exp (\u2212N0)\n\u2265 1\u2212 ( N\u0303 + 1 ) N N\u03030 exp (\u2212N0)\n= 1\u2212 (logN0 + 1)N logN00 exp (\u2212N0) = 1\u2212 oN (1) \u2265 0.5\nas long as N is sufficiently large. Substitution into (58) yields\nP0\n( P1 (Z) P0 (Z) \u2265 1 ) \u2265 0.5e\u2212(N\u2212N0) \u2265 exp ( \u2212 (1 + )N ( 1\u2212 e\u2212(1+ )D\u2217 )) . (59)\nThis finishes the proof of the lower bound in (19) since > 0 can be arbitrary. Additionally, applying the upper bound (54) we derive\n(56) \u2264 \u221e\u2211\nl=0\nN le\u2212N\nl! \u00b7 e\u2212lD\u2217 = exp\n( \u2212N ( 1\u2212 e\u2212D\u2217 )) ,\nestablishing the upper bound (19).\nF.3 Proof of Lemma 3 We start with the general case, and suppose that the Chernoff information (3) is attained by \u03c4 = \u03c4\u2217 \u2208 [0, 1]. It follows from the Chernoff bound that\nP0\n{ N\u2211\ni=1\nlog P1 (Zi)\nP0 (Zi) \u2265 \u2212 \u03bb \u2223\u2223\u2223\u2223\u2223N = k } = P0 { \u03c4\u2217 k\u2211\ni=1\nlog P1 (Zi) P0 (Zi) \u2265 \u2212\u03c4\u2217 \u00b7 \u03bb\n} \u2264\n\u220fk i=1 EP0 [( P1(Zi) P0(Zi) )\u03c4\u2217]\nexp (\u2212\u03c4\u2217 \u00b7 \u03bb)\n= exp (\u03c4\u2217 \u00b7 \u03bb) ( EP0 [( P1 (Zi)\nP0 (Zi)\n)\u03c4\u2217])k\n= exp (\u03c4\u2217 \u00b7 \u03bb) (\u2211\nz P 1\u2212\u03c4\n\u2217 0 (z)P 1\u2212\u03c4\u2217 1 (z)\n)k\n\u2264 exp ( \u03bb) exp (\u2212kD\u2217) . This suggests that\nP0\n{ N\u2211\ni=1\nlog P1 (Zi)\nP0 (Zi) \u2265 \u2212 \u03bb\n} = P0 { N\u2211\ni=1\nlog P1 (Zi)\nP0 (Zi) \u2265 \u2212 \u03bb \u2223\u2223\u2223\u2223\u2223N = k } P {N = k}\n\u2264 exp ( \u03bb)EN\u223cPoisson(\u03bb) [exp (\u2212ND\u2217)] = exp ( \u03bb) exp { \u2212\u03bb ( 1\u2212 e\u2212D\u2217 )} ,\nwhere the last identity follows from the moment generating function of Poisson random variables. This establishes the claim for the general case.\nWhen specialized to the Bernoulli case, the log-likelihood ratio is given by\nlog P1 (Zi)\nP0 (Zi) = I {Zi = 0} log\n\u03b8 1\u2212 \u03b8 + I {Zi = 1} log 1\u2212 \u03b8 \u03b8\n= {2I {Zi = 1} \u2212 1} log 1\u2212 \u03b8 \u03b8 .\nWhen 0 < \u03b8 < 1/2, this demonstrates the equivalence between the following two inequalities:\nN\u2211\ni=1\nlog P1 (Zi)\nP0 (Zi) \u2265 \u2212 \u03bb \u21d0\u21d2\nN\u2211\ni=1\nI {Zi = 1} \u2265 1 2 N \u2212 \u03bb\n2 log 1\u2212\u03b8\u03b8 .\nRecognizing that \u2211N i=1 Zi = \u2211N i=1 I {Zi = 1} and replacing with \u00b7 2 log 1\u2212\u03b8\u03b8 , we complete the proof.\nF.4 Proof of Lemma 4 For any constant c1 \u2265 2e,\nP { N \u2265 c1\u03bb\nlog 1\n} =\n\u2211\nk\u2265 c1\u03bb log(1/ )\nP {N = k} = \u2211\nk\u2265 c1\u03bb log(1/ )\n( \u03bb) k\nk! exp (\u2212 \u03bb)\n(i) \u2264\n\u2211\nk\u2265 c1\u03bb log(1/ )\n( \u03bb) k\n(k/e) k\n= \u2211\nk\u2265 c1\u03bb log(1/ )\n( e\u03bb\nk\n)k \u2264\n\u2211\nk\u2265 c1\u03bb log(1/ )\n( e\u03bb c1\u03bb\nlog(1/ )\n)k\n(ii) \u2264\n\u2211\nk\u2265 c1\u03bb log(1/ )\n( e \u221a\nc1\n)k (iii) \u2264 2 ( e \u221a\nc1\n) c1\u03bb log(1/ )\n\u2264 2 exp { \u2212 log ( c1 e \u00b7 1\u221a ) c1\u03bb log 1 } \u2264 2 exp { \u2212 log ( 1\u221a ) c1\u03bb log 1 }\n= 2 exp { \u2212c1\u03bb\n2\n} ,\nwhere (i) arises from the elementary inequality a! \u2265 ( a e )a, (ii) holds because log 1 \u2264 \u221a holds for any 0 < \u2264 1, and (iii) follows due to the inequality \u2211k\u2265K ak \u2264 a K 1\u2212a \u2264 2aK as long as 0 < a \u2264 1/2.\nF.5 Proof of Lemmas 10-12 (1) We start by proving Lemma 10, which contains all ingredients for proving Lemmas 11-12. First of all, we demonstrate that there are many vertices in U that are isolated in the subgraph induced by U . In fact, let U0 be a random subset of U of size |U|log3 n . By Markov\u2019s inequality, the number of samples with two endpoints lying in U0\u2014denoted by NU0\u2014is bounded above by\nNU0 . log n \u00b7 E [\u03bb \u00b7 |E (U0,U0)|] . \u03bb ( 1\nlog6 n |E (U ,U)|\n) log n . \u03bb ( 1\nlog6 n |U| d\u0303\n) log n\n(i) . |U|\nlog4 n = o (|U0|)\nwith high probability, where E(U , U\u0303) denotes the set of edges linking U and U\u0303 , and (i) follows from the assumption (38). As a consequence, one can find (1\u2212 o (1)) |U0| vertices in U0 that are involved in absolutely no sample falling within E (U0,U0). Let U1 be the set of these isolated vertices, which obeys\n|U1| = (1\u2212 o (1)) |U0| = (1\u2212 o (1)) |U| log3 n \u2265 |U| 2 log3 n (60)\nfor large n. We emphasize that the discussion so far only concerns the subgraph induced by U , which is independent of the samples taken over E ( U ,U ) .\nSuppose the ground truth is Xi = 0 (1 \u2264 i \u2264 n). For each vertex v \u2208 U1, construct a representative singleton hypothesis Xv such that\nXvi = { 1, if i = v, 0, else.\nLet P0 (resp. Pv) denote the output distribution given X = 0 (resp. X = Xv). Assuming a uniform prior over all candidates, it suffices to study the ML rule which achieves the best error exponent. For each v \u2208 U1, since it is isolated in U1, all information useful for differentiating X = Xv and X = 0 falls within the positions v \u00d7 U0, which in total account for at most d\u0303 entries. The main point is that for any v, u \u2208 U1, the corresponding samples over v \u00d7 U0 and u \u00d7 U0 are statistically independent, and hence the events { Pv(Y ) P0(Y ) \u2265 1 } v\u2208U1\nare independent conditional on U1. We now consider the probability of error conditional on U1. Observe that Pv (Y ) and P0 (Y ) only differ\nover those edges incident to v. Thus, Lemma 2 suggests that\nP0\n( Pv (Y ) P0 (Y ) \u2265 1 ) \u2265 exp { \u2212 (1 + o (1))\u03bbd\u0303 ( 1\u2212 e\u2212D\u2217 )}\nfor any v \u2208 U1. The conditional independence of the events { Pv(Y ) P0(Y ) \u2265 1 } gives\nPe (\u03c8ml) \u2265 P0 ( \u2203v \u2208 U1 : Pv (Y ) P0 (Y ) \u2265 1 ) = 1\u2212 \u220f\nv\u2208U1\n{ 1\u2212 P0 ( Pv (Y ) P0 (Y ) \u2265 1 )}\n\u2265 1\u2212 { 1\u2212 exp [ \u2212 (1 + o (1))\u03bbd\u0303 ( 1\u2212 e\u2212D\u2217 )]}|U1| (61)\n\u2265 1\u2212 { 1\u2212 exp [ \u2212 (1 + o (1))\u03bbd\u0303 ( 1\u2212 e\u2212D\u2217 )]} |U| 2 log3 n (62) \u2265 1\u2212 exp { \u2212 exp [ \u2212 (1 + o (1))\u03bbd\u0303 ( 1\u2212 e\u2212D\u2217\n)] |U| 2 log3 n } , (63)\nwhere (62) comes from (60), and the last inequality follows since 1\u2212 x \u2264 exp (\u2212x). With (63) in place, we see that ML fails with probability approaching one if\nexp { \u2212 (1 + o (1))\u03bbd\u0303 ( 1\u2212 e\u2212D\u2217 )} |U| log3 n \u2192\u221e,\nwhich would hold under the assumption (38). (2) Now we turn to Lemma 11. Without loss of generality, it is assumed that wi,j = \u0398 (1) for all (i, j) \u2208 E . The preceding argument immediately carries over to the sampling model with nonuniform weight, as long as all vertex degrees are replaced with the corresponding weighted degrees (cf. (43)).\n(3) Finally, the preceding argument remains valid for proving Lemma 12 with minor modification. Let U0 be a random subset of U of size |U|log3 n , denote by EU0 the collection of hyper-edges with at least two endpoints in U0, and let NU0 represent the number of samples that involve at least two nodes in U0. When L is a fixed constant, applying Markov\u2019s inequality one gets\nNU0 . log n \u00b7 E [\u03bb \u00b7 |EU0 |] . \u03bb (( L\n2\n)( 1\nlog3 n\n)2 |EU | ) log n . \u03bb ( 1\nlog6 n |U| d\u0303\n) log n\n(i) . |U|\nlog4 n = o (|U0|)\nwith high probability, where d\u0303 denotes the maximum vertex degree (defined w.r.t. the hyper-edges) in U . That said, there exist (1\u2212 o (1)) |U0| vertices in U0 that are involved in absolutely no sample falling within\nEU0 , and if we let U1 be the set of these isolated vertices, then\n|U1| = (1\u2212 o (1)) |U0| = (1\u2212 o (1)) |U| log3 n \u2265 |U| 2 log3 n (64)\nfor large n. Repeating the remaining argument in Part (1) finishes the proof.\nF.6 Proof of Lemma 6 To begin with, set \u03bb\u0303 = 1 \u2212 exp (\u2212\u03bb), which satisfies 1 \u2265 \u03bb\u0303 & log n/n under the assumption of this lemma. Then the sample matrix A generated in Algorithm 2 obeys\nE [A] = \u03bb\u0303 ( 11> \u2212 I ){ E [ Y\n(1) i,j = 0\n] \u2212 E [ Y\n(1) i,j = 1\n]}\n= \u03bb\u0303 (1\u2212 2\u03b8) 11> \u2212 \u03bb\u0303 (1\u2212 2\u03b8) I, (65)\nwhere the first term of (65) is the dominant component. Moreover, we claim for the time being that the fluctuation A\u0303 := A\u2212 E[A] obeys\n\u2016A\u0303\u2016 . \u221a \u03bb\u0303n (66)\nwith probability at least 1\u2212O ( n\u221210 ) . In view of the Davis-Kahan sin-\u0398 Theorem [56], the leading eigenvector u of A = \u03bb\u0303 (1\u2212 2\u03b8) 11> + A\u0303\u2212 \u03bb\u0303 (1\u2212 2\u03b8) I satisfies\nmin {\u2225\u2225\u2225\u2225u\u2212 1\u221a n 1 \u2225\u2225\u2225\u2225 , \u2225\u2225\u2225\u2225\u2212u\u2212 1\u221a n 1 \u2225\u2225\u2225\u2225 } . \u2016A\u0303\u2016+\n\u2225\u2225\u2225\u03bb\u0303 (1\u2212 2\u03b8) I \u2225\u2225\u2225\n\u03bb\u0303n (1\u2212 2\u03b8)\u2212 \u2016A\u0303\u2016 \u2212 \u2225\u2225\u2225\u03bb\u0303 (1\u2212 2\u03b8) I \u2225\u2225\u2225\n. \u221a \u03bb\u0303n+ \u03bb\u0303\n\u03bb\u0303n 1,\nwhich is sufficient to guarantee (22). In fact, suppose without loss of generality that \u2225\u2225\u2225u\u2212 1\u221an1 \u2225\u2225\u2225 \u2264 \u2225\u2225\u2225\u2212u\u2212 1\u221an1 \u2225\u2225\u2225. According to the rounding procedure,\nX (0) i = Xi = 1 if \u2223\u2223\u2223\u2223ui \u2212 1\u221a n \u2223\u2223\u2223\u2223 < 1 2 \u221a n ,\nleading to an upper bound on the Hamming error \u2225\u2225X(0) \u2212 1 \u2225\u2225 0\nn \u2264 1 n\nn\u2211\ni=1\nI { X\n(0) i 6= Xi\n} \u2264 1\nn\nn\u2211\ni=1\nI {\u2223\u2223\u2223\u2223ui \u2212 1\u221a n \u2223\u2223\u2223\u2223 \u2265 1 2 \u221a n }\n\u2264 1 n    \u2225\u2225\u2225u\u2212 1\u221an1 \u2225\u2225\u2225 2 (1/(2 \u221a n)) 2    . 1 \u03bb\u0303n + 1 n2 = o (1) ,\nas claimed in this lemma. It remains to justify the claim (66), for which we start by controlling the mean E[\u2016A\u0303\u2016]. The standard symmetrization argument [57, Page 133] reveals that\nE [ \u2016A\u0303\u2016 ] \u2264 \u221a 2\u03c0E [ \u2016A\u0303 \u25e6G\u2016 ] , (67)\nwhere G is a symmetric standard Gaussian matrix (i.e. {Gi,j | i \u2265 j} are i.i.d. standard Gaussian variables), and A\u0303 \u25e6G := [A\u0303i,jG\u0303i,j ]1\u2264i,j\u2264n represents the Hadamard product of A\u0303 and G. To control E[\u2016A\u0303 \u25e6G\u2016], it follows from [58, Theorem 1.1] that\nE [ \u2016A\u0303 \u25e6G\u2016 \u2223\u2223\u2223 A\u0303 ] . max\ni {\u221a\u2211n j=1 A\u03032i,j } + \u221a log n, (68)\ndepending on the size of maxi {\u221a\u2211n j=1 A\u0303 2 i,j } . First of all, with probability exceeding 1\u2212O ( n\u221210 ) one has\n\u221a\u2211n j=1 A\u03032i,j . \u03bb\u0303n, 1 \u2264 i \u2264 n,\nwhich arises by taking Chernoff inequality [59, Appendix A] together with the union bound, and recognizing that E[ \u2211n j=1 A\u0303 2 i,j ] \u2264 \u03bb\u0303n and \u03bb\u0303n & log n. In this regime, substitution into (68) gives\nE [ \u2016A\u0303 \u25e6G\u2016 \u2223\u2223\u2223 A\u0303 ] . \u221a \u03bb\u0303n+ \u221a log n. (69)\nFurthermore, the trivial bound \u221a\u2211n\nj=1 A\u0303 2 i,j \u2264 n taken together with (68) gives\nE [ \u2016A\u0303 \u25e6G\u2016 \u2223\u2223\u2223 A\u0303 ] . \u221a n+ \u221a log n (70)\nin the complement regime. Put together (67), (69) and (70) to arrive at E [ \u2016A\u0303\u2016 ] \u2264 E [ E [ \u2016A\u0303 \u25e6G\u2016 \u2223\u2223\u2223 A\u0303 ]]\n. P   maxi \u221a\u221a\u221a\u221a n\u2211\nj=1\nA\u03032i,j . \u03bb\u0303n    (\u221a \u03bb\u0303n+ \u221a log n ) +  1\u2212 P   maxi \u221a\u221a\u221a\u221a n\u2211\nj=1\nA\u03032i,j . \u03bb\u0303n      (\u221a n+ \u221a log n )\n. \u221a \u03bb\u0303n+ \u221a log n+ 1\nn10\n(\u221a n+ \u221a log n )\n\u221a \u03bb\u0303n, (71)\nwhere the last inequality holds as long as\n\u03bb\u0303 & log n n . (72)\nTo finish up, we shall connect \u2016A\u0303\u2016 with E[\u2016A\u0303\u2016] by invoking the Talagrand concentration inequality. Note that the spectral norm \u2016M\u2016 is a 1-Lipschitz function in M , which allows to apply [57, Theorem 2.1.13] to yield\nP {\u2223\u2223\u2223\u2016A\u0303\u2016 \u2212 E[\u2016A\u0303\u2016] \u2223\u2223\u2223 \u2265 c1 \u221a \u03bb\u0303n } \u2264 C2 exp ( \u2212c2c21\u03bb\u0303n ) (73)\nfor some constants c1, c2, C2 > 0. Combining (71)-(73) and taking c1 to be sufficiently large lead to\n\u2016A\u0303\u2016 . \u221a \u03bb\u0303n\nwith probability at least 1\u2212O ( n\u221210 ) , concluding the proof.\nF.7 Proof of Lemma 9 It follows from Lemma 3 that for any small constant \u03b4 > 0\nP {\n(VX)v \u2265 1\n2 |S(v)| \u2212 \u03b4\u03bbdv\n} \u2264 exp { \u2212 (1\u2212 o (1)) (1\u2212 \u03be (\u03b4))\u03bbdv ( 1\u2212 e\u2212D\u2217 )} ,\nwhere D\u2217 represents the Chernoff information. Recalling that \u03bbdv & log n for all 1 \u2264 v \u2264 n and applying the union bound, we get\nP { \u22031 \u2264 v \u2264 n : (VX)v \u2265 1\n2 |S(v)| \u2212 \u03b4 log n\n} \u2264 n\u2211\nv=1\nexp { \u2212 (1\u2212 o (1)) ( 1\u2212 \u03be\u0303 (\u03b4) ) \u03bbdv ( 1\u2212 e\u2212D\u2217 )} (74)\nfor some function \u03be\u0303 (\u03b4) that vanishes as \u03b4 \u2192 0. We can now analyze different sampling models on a case-bycase basis.\n(1) Rings. All vertices have the same degree davg. Since the sample complexity is m = 12\u03bbndavg, we arrive at\n(74) \u2264 n exp { \u2212 (1\u2212 o (1)) davg \u00b7 ( 1\u2212 \u03be (\u03b4) ) \u03bb ( 1\u2212 e\u2212D\u2217 )}\n= n exp { \u2212 (1\u2212 o (1)) ( 1\u2212 \u03be (\u03b4)\n) 2m n ( 1\u2212 e\u2212D\u2217 )} ,\nwhich tends to zero as long as\nm > 1 + \u03b4 1\u2212 \u03be\u0303 (\u03b4) \u00b7 n log n 2 (1\u2212 e\u2212D\u2217) .\n(2) Lines with r = n\u03b2 for some constant 0 < \u03b2 < 1. The first and the last r vertices have degrees at least (1\u2212 o (1)) 12davg, while all remaining n\u2212 2r vertices have degrees equal to (1\u2212 o (1)) davg. This gives\n(74) \u2264 2r \u00b7 exp { \u2212 (1\u2212 o (1)) ( 1\u2212 \u03be\u0303 (\u03b4) ) \u03bb \u00b7 1\n2 davg\n( 1\u2212 e\u2212D\u2217\n)}\n+ (n\u2212 2r) exp { \u2212 (1\u2212 o (1)) ( 1\u2212 \u03be\u0303 (\u03b4) ) \u03bbdavg ( 1\u2212 e\u2212D\u2217 )}\n\u2264 2 exp { \u03b2 log n\u2212 (1\u2212 o (1)) ( 1\u2212 \u03be\u0303 (\u03b4) ) m n ( 1\u2212 e\u2212D\u2217 )}\n+ exp { log n\u2212 (1\u2212 o (1)) ( 1\u2212 \u03be\u0303 (\u03b4) ) 2m n ( 1\u2212 e\u2212D\u2217 )} ,\nwhich converges to zero as long as    m > (1 + o (1)) 1+\u03b4 1\u2212\u03be\u0303(\u03b4) \u00b7 \u03b2n logn 1\u2212e\u2212D\u2217 ;\nm > (1 + o (1)) 1+\u03b4 1\u2212\u03be\u0303(\u03b4) \u00b7 n logn 2(1\u2212e\u2212D\u2217) .\n(3) Lines with r = \u03b3n for some constant 0 < \u03b3 \u2264 1. Each vertex has degree exceeding (1\u2212 o (1)) r, indicating that\n(74) \u2264 n exp { \u2212 (1\u2212 o (1)) ( 1\u2212 \u03be (\u03b4) ) \u03bbr ( 1\u2212 e\u2212D\u2217 )}\n\u2264 exp { log n\u2212 (1\u2212 o (1)) ( 1\u2212 \u03be (\u03b4) ) \u03bbr ( 1\u2212 e\u2212D\u2217 )}\n= exp { log n\u2212 (1\u2212 o (1)) ( 1\u2212 \u03be (\u03b4)\n) m n ( 1\u2212 12\u03b3 ) (\n1\u2212 e\u2212D\u2217 )}\nwhere the last line follows from (42). This converges to zero as long as\nm > 1 + \u03b4\n1\u2212 \u03be\u0303 (\u03b4)\n( 1\u2212 1\n2 \u03b3\n) n log n\n1\u2212 e\u2212D\u2217 .\n(4) Grids with r = n\u03b2 for some constant 0 < \u03b2 < 1. Note that davg r2 = n2\u03b2 . There are at least n\u2212 \u03c0r2 vertices with degrees equal to (1\u2212 o (1))davg, while the remaining vertices have degree at least (1\u2212 o (1))davg/4. This gives\n(74) \u2264 \u03c0r2 \u00b7 exp { \u2212(1\u2212 o (1)) ( 1\u2212 \u03be\u0303 (\u03b4) ) \u03bb davg\n4\n( 1\u2212 e\u2212D\u2217\n)}\n+ ( n\u2212 \u03c0r2 ) exp { \u2212(1\u2212 o (1)) ( 1\u2212 \u03be\u0303 (\u03b4) ) \u03bbdavg ( 1\u2212 e\u2212D\u2217 )}\n\u2264 4 exp { 2\u03b2 log n\u2212 (1\u2212 o (1)) ( 1\u2212 \u03be\u0303 (\u03b4) ) \u03bb \u00b7 davg\n4\n( 1\u2212 e\u2212D\u2217\n)}\n+ exp { log n\u2212 (1\u2212 o (1)) ( 1\u2212 \u03be\u0303 (\u03b4) ) \u03bbdavg ( 1\u2212 e\u2212D\u2217 )} ,\nwhich vanishes as long as { \u03bbdavg > (1 + o (1))\n1+\u03b4 1\u2212\u03be\u0303(\u03b4) \u00b7 8\u03b2 logn 1\u2212e\u2212D\u2217 ;\n\u03bbdavg > (1 + o (1)) 1+\u03b4 1\u2212\u03be\u0303(\u03b4) \u00b7 logn 1\u2212e\u2212D\u2217 .\nThis together with the fact that m = 12n\u03bbdavg establishes the proof for this case. Finally, for the cases of lines (with r = n\u03b2 for some constant 0 < \u03b2 < 1) / rings / grids with non-uniform sampling weight, it suffices to replace davg with the average weighted degree (see Section B.2.2). The case of small-world graphs follows exactly the same argument as in the case of rings with nonuniform weights."}], "references": [{"title": "Community structure in social and biological networks", "author": ["M. Girvan", "M. Newman"], "venue": "Proceedings of the national academy of sciences,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2002}, {"title": "Community detection in graphs", "author": ["S. Fortunato"], "venue": "Physics Reports,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2010}, {"title": "Correlation clustering", "author": ["N. Bansal", "A. Blum", "S. Chawla"], "venue": "Machine Learning,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2004}, {"title": "Clustering Partially Observed Graphs via Convex Optimization", "author": ["Ali Jalali", "Yudong Chen", "Sujay Sanghavi", "Huan Xu"], "venue": "In International Conference on Machine Learning (ICML),", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2011}, {"title": "Detecting functional modules in the yeast protein\u2013protein interaction", "author": ["Jingchun Chen", "Bo Yuan"], "venue": "network. Bioinformatics,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2006}, {"title": "Normalized cuts and image segmentation", "author": ["Jianbo Shi", "Jitendra Malik"], "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2000}, {"title": "How Hard is Inference for Structured Prediction", "author": ["Amir Globerson", "Tim Roughgarden", "David Sontag", "Cafer Yildirim"], "venue": "In ICML,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2015}, {"title": "Near-Optimal Joint Object Matching via Convex Relaxation", "author": ["Y. Chen", "L. Guibas", "Q. Huang"], "venue": "International Conference on Machine Learning (ICML),", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2014}, {"title": "Stochastic blockmodels: First steps", "author": ["Paul W Holland", "Kathryn Blackmond Laskey", "Samuel Leinhardt"], "venue": "Social networks,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 1983}, {"title": "Algorithms for graph partitioning on the planted partition model", "author": ["Anne Condon", "Richard M Karp"], "venue": "Random Structures and Algorithms,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2001}, {"title": "Decoding binary node labels from censored measurements: Phase transition and efficient recovery", "author": ["E. Abbe", "A. Bandeira", "A. Bracher", "A. Singer"], "venue": "IEEE Trans on Network Science and Engineering,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2015}, {"title": "Random graph dynamics, volume 200", "author": ["R. Durrett"], "venue": "Cambridge university press Cambridge,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2007}, {"title": "Achieving Exact Cluster Recovery Threshold via Semidefinite Programming: Extensions", "author": ["Bruce Hajek", "Yihong Wu", "Jiaming Xu"], "venue": "arXiv preprint arXiv:1502.07738,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2015}, {"title": "Graph partitioning via adaptive spectral techniques", "author": ["Amin Coja-Oghlan"], "venue": "Combinatorics, Probability and Computing,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2010}, {"title": "Spectral clustering of graphs with general degrees in the extended planted partition model", "author": ["K. Chaudhuri", "F. Chung", "A. Tsiatas"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2012}, {"title": "Weighted graph clustering with non-uniform uncertainties", "author": ["Yudong Chen", "Shiau H Lim", "Huan Xu"], "venue": "In International Conference on Machine Learning (ICML),", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2014}, {"title": "Community detection in general stochastic block models: fundamental limits and efficient recovery algorithms", "author": ["E. Abbe", "C. Sandon"], "venue": "arXiv preprint arXiv:1503.00609,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2015}, {"title": "Robust and computationally feasible community detection in the presence of arbitrary outlier nodes", "author": ["T Tony Cai", "Xiaodong Li"], "venue": "The Annals of Statistics,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2015}, {"title": "Density evolution in the degree-correlated stochastic block model", "author": ["Elchanan Mossel", "Jiaming Xu"], "venue": "arXiv preprint arXiv:1509.03281,", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2015}, {"title": "Information recovery from pairwise measurements: A Shannon-theoretic approach", "author": ["Y. Chen", "C. Suh", "A.J. Goldsmith"], "venue": "IEEE International Symposium on Information Theory, arXiv preprint arXiv:1504.01369,", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2015}, {"title": "Haplotype phasing: existing methods and new developments", "author": ["S. Browning", "B. Browning"], "venue": "Nature Reviews Genetics,", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2011}, {"title": "Hapsembler: an assembler for highly polymorphic genomes", "author": ["N. Donmez", "M. Brudno"], "venue": "In Research in Computational Molecular Biology,", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2011}, {"title": "Structured low-rank matrix factorization for haplotype assembly", "author": ["Changxiao Cai", "Sujay Sanghavi", "Haris Vikalo"], "venue": "IEEE Journal of Selected Topics in Signal Processing,", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2016}, {"title": "Haplotype assembly: An information theoretic view", "author": ["H. Si", "H. Vikalo", "S. Vishwanath"], "venue": "In IEEE Information Theory Workshop,", "citeRegEx": "26", "shortCiteRegEx": "26", "year": 2014}, {"title": "Optimal Haplotype Assembly from High-Throughput Mate-Pair Reads", "author": ["G. Kamath", "E. Sasoglu", "D. Tse"], "venue": "IEEE International Symposium on Information Theory,", "citeRegEx": "27", "shortCiteRegEx": "27", "year": 2015}, {"title": "Stochastic Block Model and Community Detection in the Sparse Graphs: A spectral algorithm with optimal rate of recovery", "author": ["Peter Chin", "Anup Rao", "Van Vu"], "venue": "arXiv preprint arXiv:1501.05021,", "citeRegEx": "29", "shortCiteRegEx": "29", "year": 2015}, {"title": "Phase Transitions in Semidefinite Relaxations", "author": ["Adel Javanmard", "Andrea Montanari", "Federico Ricci-Tersenghi"], "venue": "arXiv preprint arXiv:1511.08769,", "citeRegEx": "30", "shortCiteRegEx": "30", "year": 2015}, {"title": "Belief propagation, robust reconstruction, and optimal recovery of block models", "author": ["Elchanan Mossel", "Joe Neeman", "Allan Sly"], "venue": "arXiv preprint arXiv:1309.1380,", "citeRegEx": "31", "shortCiteRegEx": "31", "year": 2013}, {"title": "Matrix completion from noisy entries", "author": ["R.H. Keshavan", "A. Montanari", "S. Oh"], "venue": "The Journal of Machine Learning Research,", "citeRegEx": "32", "shortCiteRegEx": "32", "year": 2010}, {"title": "Low-rank matrix completion using alternating minimization", "author": ["Prateek Jain", "Praneeth Netrapalli", "Sujay Sanghavi"], "venue": "In Symposium on Theory of Computing (STOC),", "citeRegEx": "33", "shortCiteRegEx": "33", "year": 2013}, {"title": "Solving random quadratic systems of equations is nearly as easy as solving linear systems", "author": ["Y. Chen", "E. Candes"], "venue": "In Advances in Neural Information Processing Systems (NIPS),", "citeRegEx": "34", "shortCiteRegEx": "34", "year": 2015}, {"title": "Exact recovery in the stochastic block model", "author": ["Emmanuel Abbe", "Afonso S Bandeira", "Georgina Hall"], "venue": "IEEE Trans. on Information Theory,", "citeRegEx": "35", "shortCiteRegEx": "35", "year": 2016}, {"title": "Achieving optimal misclassification proportion in stochastic block model", "author": ["C. Gao", "Z. Ma", "A. Y Zhang", "H. Zhou"], "venue": "arXiv preprint arXiv:1505.03772,", "citeRegEx": "36", "shortCiteRegEx": "36", "year": 2015}, {"title": "Correlation clustering: maximizing agreements via semidefinite programming", "author": ["C. Swamy"], "venue": "In Symposium on Discrete Algorithms (SODA),", "citeRegEx": "37", "shortCiteRegEx": "37", "year": 2004}, {"title": "Improved graph clustering", "author": ["Yudong Chen", "Sujay Sanghavi", "Huan Xu"], "venue": "IEEE Transactions on Information Theory,", "citeRegEx": "38", "shortCiteRegEx": "38", "year": 2014}, {"title": "Information-theoretic bounds for exact recovery in weighted stochastic block models using the Renyi divergence", "author": ["Varun Jog", "Po-Ling Loh"], "venue": "arXiv preprint arXiv:1509.06418,", "citeRegEx": "39", "shortCiteRegEx": "39", "year": 2015}, {"title": "Exact recovery threshold in the binary censored block model", "author": ["Bruce Hajek", "Yihong Wu", "Jiaming Xu"], "venue": "In Information Theory Workshop,", "citeRegEx": "40", "shortCiteRegEx": "40", "year": 2015}, {"title": "Exact Matrix Completion via Convex Optimization", "author": ["E.J. Candes", "B. Recht"], "venue": "Foundations of Comp. Math.,", "citeRegEx": "41", "shortCiteRegEx": "41", "year": 2009}, {"title": "Matrix completion from a few entries", "author": ["R.H. Keshavan", "A. Montanari", "S. Oh"], "venue": "IEEE Trans on Info Theory,", "citeRegEx": "42", "shortCiteRegEx": "42", "year": 2010}, {"title": "Robust principal component analysis", "author": ["E.J. Cand\u00e8s", "X. Li", "Y. Ma", "J. Wright"], "venue": "Journal of ACM,", "citeRegEx": "43", "shortCiteRegEx": "43", "year": 2011}, {"title": "Rank-sparsity incoherence for matrix decomposition", "author": ["Venkat Chandrasekaran", "Sujay Sanghavi", "Pablo A Parrilo", "Alan S Willsky"], "venue": "SIAM Journal on Optimization,", "citeRegEx": "44", "shortCiteRegEx": "44", "year": 2011}, {"title": "Low-Rank Matrix Recovery From Errors and Erasures", "author": ["Yudong Chen", "A. Jalali", "S. Sanghavi", "C. Caramanis"], "venue": "IEEE Trans on Info Theory,", "citeRegEx": "45", "shortCiteRegEx": "45", "year": 2013}, {"title": "Universal matrix completion", "author": ["Srinadh Bhojanapalli", "Prateek Jain"], "venue": "International Conference on Machine Learning (ICML),", "citeRegEx": "46", "shortCiteRegEx": "46", "year": 2014}, {"title": "SDhaP: haplotype assembly for diploids and polyploids via semidefinite programming", "author": ["Shreepriya Das", "Haris Vikalo"], "venue": "BMC genomics,", "citeRegEx": "50", "shortCiteRegEx": "50", "year": 2015}, {"title": "Statistical-computational tradeoffs in planted problems and submatrix localization with a growing number of clusters and submatrices", "author": ["Yudong Chen", "Jiaming Xu"], "venue": "arXiv preprint arXiv:1402.1267,", "citeRegEx": "51", "shortCiteRegEx": "51", "year": 2014}, {"title": "Probability inequalities for sums of bounded random variables", "author": ["Wassily Hoeffding"], "venue": "Journal of the American statistical association,", "citeRegEx": "52", "shortCiteRegEx": "52", "year": 1963}, {"title": "Information theory and statistics: A tutorial", "author": ["Imre Csisz\u00e1r", "Paul C Shields"], "venue": "Communications and Information Theory,", "citeRegEx": "53", "shortCiteRegEx": "53", "year": 2004}, {"title": "Information theory: coding theorems for discrete memoryless systems", "author": ["Imre Csiszar", "J\u00e1nos K\u00f6rner"], "venue": null, "citeRegEx": "54", "shortCiteRegEx": "54", "year": 2011}, {"title": "Sanov property, generalized I-projection and a conditional limit theorem", "author": ["Imre Csisz\u00e1r"], "venue": "The Annals of Probability,", "citeRegEx": "55", "shortCiteRegEx": "55", "year": 1984}, {"title": "The rotation of eigenvectors by a perturbation", "author": ["Chandler Davis", "William Morton Kahan"], "venue": "iii. SIAM Journal on Numerical Analysis,", "citeRegEx": "56", "shortCiteRegEx": "56", "year": 1970}, {"title": "Topics in random matrix theory, volume 132", "author": ["Terence Tao"], "venue": "American Mathematical Soc.,", "citeRegEx": "57", "shortCiteRegEx": "57", "year": 2012}, {"title": "Sharp nonasymptotic bounds on the norm of random matrices with independent entries", "author": ["A.S. Bandeira", "R. van Handel"], "venue": "arXiv preprint arXiv:1408.6185,", "citeRegEx": "58", "shortCiteRegEx": "58", "year": 2014}, {"title": "The probabilistic method", "author": ["Noga Alon", "Joel H Spencer"], "venue": null, "citeRegEx": "59", "shortCiteRegEx": "59", "year": 2015}], "referenceMentions": [{"referenceID": 0, "context": "One formulation that has received significant attention in recent years is community recovery [1\u20133], also referred to as correlation clustering [4] or graph clustering [5].", "startOffset": 94, "endOffset": 99}, {"referenceID": 1, "context": "One formulation that has received significant attention in recent years is community recovery [1\u20133], also referred to as correlation clustering [4] or graph clustering [5].", "startOffset": 94, "endOffset": 99}, {"referenceID": 2, "context": "One formulation that has received significant attention in recent years is community recovery [1\u20133], also referred to as correlation clustering [4] or graph clustering [5].", "startOffset": 144, "endOffset": 147}, {"referenceID": 3, "context": "One formulation that has received significant attention in recent years is community recovery [1\u20133], also referred to as correlation clustering [4] or graph clustering [5].", "startOffset": 168, "endOffset": 171}, {"referenceID": 4, "context": "While this formulation applies naturally in social networks, it has a broad range of applications in other domains including protein complex detection [6], image segmentation [7,8], shape matching [9], etc.", "startOffset": 151, "endOffset": 154}, {"referenceID": 5, "context": "While this formulation applies naturally in social networks, it has a broad range of applications in other domains including protein complex detection [6], image segmentation [7,8], shape matching [9], etc.", "startOffset": 175, "endOffset": 180}, {"referenceID": 6, "context": "While this formulation applies naturally in social networks, it has a broad range of applications in other domains including protein complex detection [6], image segmentation [7,8], shape matching [9], etc.", "startOffset": 175, "endOffset": 180}, {"referenceID": 7, "context": "While this formulation applies naturally in social networks, it has a broad range of applications in other domains including protein complex detection [6], image segmentation [7,8], shape matching [9], etc.", "startOffset": 197, "endOffset": 200}, {"referenceID": 8, "context": "A particular popular model is the Stochastic Block Model (SBM) [11,12], where the n individuals to be clustered are modeled as nodes on a random graph.", "startOffset": 63, "endOffset": 70}, {"referenceID": 9, "context": "A particular popular model is the Stochastic Block Model (SBM) [11,12], where the n individuals to be clustered are modeled as nodes on a random graph.", "startOffset": 63, "endOffset": 70}, {"referenceID": 10, "context": "A closely related model is the Censored Block Model (CBM) [13], where one obtains noisy parity measurements on the edges of an Erd\u0151s-R\u00e9nyi graph [14].", "startOffset": 58, "endOffset": 62}, {"referenceID": 11, "context": "A closely related model is the Censored Block Model (CBM) [13], where one obtains noisy parity measurements on the edges of an Erd\u0151s-R\u00e9nyi graph [14].", "startOffset": 145, "endOffset": 149}, {"referenceID": 12, "context": "In the special case when the radius r is so large that measurements at all locations are possible, we recover the exact recovery limit identified by [15] when measurements are randomly sampled from complete graphs.", "startOffset": 149, "endOffset": 153}, {"referenceID": 3, "context": "It is worth emphasizing that various computationally feasible algorithms [5, 16\u201321] have been proposed for more general models beyond the SBM and the CBM, which accommodate multi-community models, the presence of outlier samples, the case where different edges are sampled at different rates, and so on.", "startOffset": 73, "endOffset": 83}, {"referenceID": 13, "context": "It is worth emphasizing that various computationally feasible algorithms [5, 16\u201321] have been proposed for more general models beyond the SBM and the CBM, which accommodate multi-community models, the presence of outlier samples, the case where different edges are sampled at different rates, and so on.", "startOffset": 73, "endOffset": 83}, {"referenceID": 14, "context": "It is worth emphasizing that various computationally feasible algorithms [5, 16\u201321] have been proposed for more general models beyond the SBM and the CBM, which accommodate multi-community models, the presence of outlier samples, the case where different edges are sampled at different rates, and so on.", "startOffset": 73, "endOffset": 83}, {"referenceID": 15, "context": "It is worth emphasizing that various computationally feasible algorithms [5, 16\u201321] have been proposed for more general models beyond the SBM and the CBM, which accommodate multi-community models, the presence of outlier samples, the case where different edges are sampled at different rates, and so on.", "startOffset": 73, "endOffset": 83}, {"referenceID": 16, "context": "It is worth emphasizing that various computationally feasible algorithms [5, 16\u201321] have been proposed for more general models beyond the SBM and the CBM, which accommodate multi-community models, the presence of outlier samples, the case where different edges are sampled at different rates, and so on.", "startOffset": 73, "endOffset": 83}, {"referenceID": 17, "context": "It is worth emphasizing that various computationally feasible algorithms [5, 16\u201321] have been proposed for more general models beyond the SBM and the CBM, which accommodate multi-community models, the presence of outlier samples, the case where different edges are sampled at different rates, and so on.", "startOffset": 73, "endOffset": 83}, {"referenceID": 18, "context": "It is worth emphasizing that various computationally feasible algorithms [5, 16\u201321] have been proposed for more general models beyond the SBM and the CBM, which accommodate multi-community models, the presence of outlier samples, the case where different edges are sampled at different rates, and so on.", "startOffset": 73, "endOffset": 83}, {"referenceID": 19, "context": "Another recent work [22] has determined the order of the information limits in geometric graphs, with no tractable algorithms provided therein.", "startOffset": 20, "endOffset": 24}, {"referenceID": 10, "context": "This is the same as the noise model in CBM [13].", "startOffset": 43, "endOffset": 47}, {"referenceID": 20, "context": "The advent of next generation sequencing technologies allows haplotype phasing by providing linking reads between multiple SNP locations [23\u201325].", "startOffset": 137, "endOffset": 144}, {"referenceID": 21, "context": "The advent of next generation sequencing technologies allows haplotype phasing by providing linking reads between multiple SNP locations [23\u201325].", "startOffset": 137, "endOffset": 144}, {"referenceID": 22, "context": "The advent of next generation sequencing technologies allows haplotype phasing by providing linking reads between multiple SNP locations [23\u201325].", "startOffset": 137, "endOffset": 144}, {"referenceID": 23, "context": "One can formulate the problem of haplotype phasing as recovery of two communities of SNP locations, those with the variant on the maternal chromosome and those with the variant on the paternal chromosome [26, 27].", "startOffset": 204, "endOffset": 212}, {"referenceID": 24, "context": "One can formulate the problem of haplotype phasing as recovery of two communities of SNP locations, those with the variant on the maternal chromosome and those with the variant on the paternal chromosome [26, 27].", "startOffset": 204, "endOffset": 212}, {"referenceID": 25, "context": "[29]) on Gc using samples taken over Gc, in the hope of obtaining approximate recovery of {Xi | i \u2208 Vc}.", "startOffset": 0, "endOffset": 4}, {"referenceID": 26, "context": "Note that the spectral method can be replaced by other efficient algorithms, including semidefinite programming (SDP) [30] and a variant of belief propagation (BP) [31].", "startOffset": 118, "endOffset": 122}, {"referenceID": 27, "context": "Note that the spectral method can be replaced by other efficient algorithms, including semidefinite programming (SDP) [30] and a variant of belief propagation (BP) [31].", "startOffset": 164, "endOffset": 168}, {"referenceID": 28, "context": "This paradigm has been successfully applied to a wide spectrum of applications ranging from matrix completion [32,33] to phase retrieval [34] to community recovery [17,35,36].", "startOffset": 110, "endOffset": 117}, {"referenceID": 29, "context": "This paradigm has been successfully applied to a wide spectrum of applications ranging from matrix completion [32,33] to phase retrieval [34] to community recovery [17,35,36].", "startOffset": 110, "endOffset": 117}, {"referenceID": 30, "context": "This paradigm has been successfully applied to a wide spectrum of applications ranging from matrix completion [32,33] to phase retrieval [34] to community recovery [17,35,36].", "startOffset": 137, "endOffset": 141}, {"referenceID": 14, "context": "This paradigm has been successfully applied to a wide spectrum of applications ranging from matrix completion [32,33] to phase retrieval [34] to community recovery [17,35,36].", "startOffset": 164, "endOffset": 174}, {"referenceID": 31, "context": "This paradigm has been successfully applied to a wide spectrum of applications ranging from matrix completion [32,33] to phase retrieval [34] to community recovery [17,35,36].", "startOffset": 164, "endOffset": 174}, {"referenceID": 32, "context": "This paradigm has been successfully applied to a wide spectrum of applications ranging from matrix completion [32,33] to phase retrieval [34] to community recovery [17,35,36].", "startOffset": 164, "endOffset": 174}, {"referenceID": 3, "context": "For this case, computationally feasible algorithms have been extensively studied [5, 9, 18, 37, 38], most of which focus only on the scaling results.", "startOffset": 81, "endOffset": 99}, {"referenceID": 7, "context": "For this case, computationally feasible algorithms have been extensively studied [5, 9, 18, 37, 38], most of which focus only on the scaling results.", "startOffset": 81, "endOffset": 99}, {"referenceID": 15, "context": "For this case, computationally feasible algorithms have been extensively studied [5, 9, 18, 37, 38], most of which focus only on the scaling results.", "startOffset": 81, "endOffset": 99}, {"referenceID": 33, "context": "For this case, computationally feasible algorithms have been extensively studied [5, 9, 18, 37, 38], most of which focus only on the scaling results.", "startOffset": 81, "endOffset": 99}, {"referenceID": 34, "context": "For this case, computationally feasible algorithms have been extensively studied [5, 9, 18, 37, 38], most of which focus only on the scaling results.", "startOffset": 81, "endOffset": 99}, {"referenceID": 12, "context": "Recent work [15,39] succeeded in characterizing the sharp threshold for this case, and it is immediate to check that the sample complexity we derive in (6) matches the one presented in [15,39].", "startOffset": 12, "endOffset": 19}, {"referenceID": 35, "context": "Recent work [15,39] succeeded in characterizing the sharp threshold for this case, and it is immediate to check that the sample complexity we derive in (6) matches the one presented in [15,39].", "startOffset": 12, "endOffset": 19}, {"referenceID": 12, "context": "Recent work [15,39] succeeded in characterizing the sharp threshold for this case, and it is immediate to check that the sample complexity we derive in (6) matches the one presented in [15,39].", "startOffset": 185, "endOffset": 192}, {"referenceID": 35, "context": "Recent work [15,39] succeeded in characterizing the sharp threshold for this case, and it is immediate to check that the sample complexity we derive in (6) matches the one presented in [15,39].", "startOffset": 185, "endOffset": 192}, {"referenceID": 24, "context": "log n, one can compute the maximum likelihood (ML) estimate via dynamic programming [27] within polynomial time.", "startOffset": 84, "endOffset": 88}, {"referenceID": 10, "context": "Notably, both [13] and [40] have derived general sufficient recovery conditions of SDP which, however, depend on the second-order graphical metrics of G [14] (e.", "startOffset": 14, "endOffset": 18}, {"referenceID": 36, "context": "Notably, both [13] and [40] have derived general sufficient recovery conditions of SDP which, however, depend on the second-order graphical metrics of G [14] (e.", "startOffset": 23, "endOffset": 27}, {"referenceID": 11, "context": "Notably, both [13] and [40] have derived general sufficient recovery conditions of SDP which, however, depend on the second-order graphical metrics of G [14] (e.", "startOffset": 153, "endOffset": 157}, {"referenceID": 10, "context": "4For instance, the sufficient sample complexity given in [13] scales as n logn hGD\u2217 with hG denoting the Cheeger constant.", "startOffset": 57, "endOffset": 61}, {"referenceID": 25, "context": "In the regime where m & n log n, the total number of samples falling within Gc is on the order of |Vc| n \u00b7m \u2265 |Vc| log n, which suffices in guaranteeing partial recovery using spectral methods [29].", "startOffset": 193, "endOffset": 197}, {"referenceID": 37, "context": "[41\u201345]).", "startOffset": 0, "endOffset": 7}, {"referenceID": 38, "context": "[41\u201345]).", "startOffset": 0, "endOffset": 7}, {"referenceID": 39, "context": "[41\u201345]).", "startOffset": 0, "endOffset": 7}, {"referenceID": 40, "context": "[41\u201345]).", "startOffset": 0, "endOffset": 7}, {"referenceID": 41, "context": "[41\u201345]).", "startOffset": 0, "endOffset": 7}, {"referenceID": 42, "context": "One exception is [46], which explored the effectiveness of SDP under general sampling schemes.", "startOffset": 17, "endOffset": 21}, {"referenceID": 4, "context": "The nominal error rate per read is p = 1%, and the average number of SNPs touched by each sample is L \u2208 [6, 7].", "startOffset": 104, "endOffset": 110}, {"referenceID": 5, "context": "The nominal error rate per read is p = 1%, and the average number of SNPs touched by each sample is L \u2208 [6, 7].", "startOffset": 104, "endOffset": 110}, {"referenceID": 43, "context": "It would be of interest to extend the results to the case with M > 2 communities, which naturally arises in many applications including haplotype phasing for polyploid species [50].", "startOffset": 176, "endOffset": 180}, {"referenceID": 44, "context": "[51]).", "startOffset": 0, "endOffset": 4}, {"referenceID": 14, "context": "[17]) that employ fresh samples in each stage in order to decouple the statistical dependency.", "startOffset": 0, "endOffset": 4}, {"referenceID": 0, "context": "3 Proof of Lemma 3 We start with the general case, and suppose that the Chernoff information (3) is attained by \u03c4 = \u03c4\u2217 \u2208 [0, 1].", "startOffset": 121, "endOffset": 127}, {"referenceID": 49, "context": "In view of the Davis-Kahan sin-\u0398 Theorem [56], the leading eigenvector u of A = \u03bb\u0303 (1\u2212 2\u03b8) 11> + \u00c3\u2212 \u03bb\u0303 (1\u2212 2\u03b8) I satisfies", "startOffset": 41, "endOffset": 45}], "year": 2016, "abstractText": "Motivated by applications in domains such as social networks and computational biology, we study the problem of community recovery in graphs with locality. In this problem, pairwise noisy measurements of whether two nodes are in the same community or different communities come mainly or exclusively from nearby nodes rather than uniformly sampled between all node pairs, as in most existing models. We present two algorithms that run nearly linearly in the number of measurements and which achieve the information limits for exact recovery.", "creator": "LaTeX with hyperref package"}}}