{"id": "1602.04341", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "13-Feb-2016", "title": "Attention-Based Convolutional Neural Network for Machine Comprehension", "abstract": "Understanding open-domain text is one of the primary challenges in natural language processing (NLP). Machine comprehension benchmarks evaluate the system's ability to understand text based on the text content only. In this work, we investigate machine comprehension on MCTest, a question answering (QA) benchmark that uses a real-world machine learning algorithm (RNN). Using RNN, we develop a process that uses simple words, words, and sentences to measure the accuracy of the results. For example, by comparing two data sets with a real-world text, we have found that a real-world text like \"A\", \"A\", \"B\" and \"C\" are similar across both data sets. This results in a greater accuracy of each word and a greater accuracy of each word. Using NLP, we use the approach of LAS, a machine learning algorithm that measures the accuracy of text based on the text content. The algorithms use RNN to evaluate the reliability of word-to-word information. Our algorithm uses a real-world machine learning algorithm, LAS, that measures the accuracy of words based on the text content and measures the accuracy of the words based on the text content. The algorithms use the approach of LAS, a machine learning algorithm that measures the accuracy of words based on the text content and measures the accuracy of the words based on the text content. The algorithm uses the approach of LAS, a machine learning algorithm that measures the accuracy of words based on the text content and measures the accuracy of the words based on the text content. The algorithms use the approach of LAS, a machine learning algorithm that measures the accuracy of words based on the text content and measures the accuracy of the words based on the text content and measures the accuracy of the words based on the text content and measures the accuracy of the words based on the text content and measures the accuracy of the words based on the text content and measures the accuracy of the words based on the text content and measures the accuracy of the words based on the text content and measures the accuracy of the words based on the text content and measures the accuracy of the words based on the text content and measures the accuracy of the words based on the text content and measures the accuracy of the words based on the text content and measures the accuracy of the words based on the text content and measures the accuracy of the words based on the text content and measures the accuracy of the words based on the text content and measures the accuracy of the words based on the text content and measures the accuracy of", "histories": [["v1", "Sat, 13 Feb 2016 14:38:47 GMT  (439kb,D)", "http://arxiv.org/abs/1602.04341v1", "7 pages, 4 figures"]], "COMMENTS": "7 pages, 4 figures", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["wenpeng yin", "sebastian ebert", "hinrich sch\\\"utze"], "accepted": false, "id": "1602.04341"}, "pdf": {"name": "1602.04341.pdf", "metadata": {"source": "CRF", "title": "Attention-Based Convolutional Neural Network for Machine Comprehension", "authors": ["Wenpeng Yin", "Sebastian Ebert", "Hinrich Sch\u00fctze"], "emails": ["ebert@cis.lmu.de"], "sections": [{"heading": "1 Introduction", "text": "Endowing machines with the ability to understand natural language is a long-standing goal in NLP and holds the promise of revolutionizing the way in which people interact with machines and retrieve information. Richardson et al. [2013] proposed the task of machine comprehension, along with MCTest, a question answering dataset for evaluation. The ability of the machine to understand text is evaluated by posing a series of questions, where the answer to each question can be found only in the associated text. Solutions typically focus on some semantic interpretation of the text, possibly with some form of probabilistic or logic inference, to answer the question. Despite intensive recent work [Weston et al., 2014; Weston et al., 2015; Hermann et al., 2015; Sachan et al., 2015], the problem is far from solved.\nMachine comprehension is an open-domain questionanswering problem which contains factoid questions, but the answers can be derived by extraction or induction of key clues. Figure 1 shows one example in MCTest. Each example consists of one document, four associated questions; each question is followed by four answer candidates in which\nonly one is correct. Questions in MCTest have two categories: \u201cone\u201d and \u201cmultiple\u201d. The label means one or multiple sentences from the document are required to answer this question. To correctly answer the first question in the example, the two blue sentences are required; for the second question instead, only the red sentence can help. The following observations hold for the whole MCTest. (i) Most of the sentences in the document are irrelavent for a given question. It hints that we need to pay attention to just some key regions. (ii) The answer candidates can be flexible text in length and abstraction level, and probably do not appear in the document. For example, candidate B for the second question is \u201coutside\u201d, which is one word and does not exist in the document, while the answer candidates for the first question are longer texts with some auxiliary words like \u201cBecause\u201d in the text. This requires our system to handle flexible texts via extraction as well as abstraction. (iii) Some questions require multiple sentences to infer the answer, and those vital sentences mostly appear close to each other (we call them snippet). Hence, our system should be able to make a choice or compromise between potential single-sentence clue and snippet clue.\nPrior work on this task is mostly based on feature engineering. This work, instead, takes the lead in presenting a deep neural network based approach without any linguistic features involved.\nConcretely, we propose HABCNN, a hierarchical attention-based convolutional neural network, to address this task in two roadmaps. In the first one, we project the docu-\nar X\niv :1\n60 2.\n04 34\n1v 1\n[ cs\n.C L\n] 1\n3 Fe\nb 20\n16\nment in two different ways, one based on question-attention, one based on answer-attention and then compare the two projected document representations to determine whether the answer matches the question. In the second one, every question-answer pair is reformatted into a statement, then the whole task is treated through textual entailment.\nIn both roadmaps, convolutional neural network (CNN) is explored to model all types of text. As human beings usually do for such a QA task, our model is expected to be able to detect the key snippets, key sentences, and key words or phrases in the document. In order to detect those informative parts required by questions, we explore an attention mechanism to model the document so that its representation contains required information intensively. In practice, instead of imitating human beings in QA task top-down, our system models the document bottom-up, through accumulating the most relevant information from word level to snippet level.\nOur approach is novel in three aspects. (i) A document is modeled by a hierarchical CNN for different granularity, from word to sentence level, then from sentence to snippet level. The reason of choosing a CNN rather than other sequence models like recurrent neural network [Mikolov et al., 2010], long short-term memory unit (LSTM [Hochreiter and Schmidhuber, 1997]), gated recurrent unit (GRU [Cho et al., 2014]) etc, is that we argue CNNs are more suitable to detect the key sentences within documents and key phrases within sentences. Considering again the second question in Figure 1, the original sentence \u201cThey sat by the fire and talked about he insects\u201d has more information than required, i.e, we do not need to know \u201cthey talked about the insects\u201d. Sequence modeling neural networks usually model the sentence meaning by accumulating the whole sequence. CNNs, with convolutionpooling steps, are supposed to detect some prominent features no matter where the features come from. (ii) In the example in Figure 1, apparently not all sentences are required given a question, and usually different snippets are required by different questions. Hence, the same document should have different representations based on what the question is. To this end, attentions are incorporated into the hierarchical CNN to guide the learning of dynamic document representations which closely match the information requirements by questions. (iii) Document representations at sentence and snippet levels both are informative for the question, a highway network is developed to combine them, enabling our system to make a flexible tradeoff.\nOverall, we make three contributions. (i) We present a hierarchical attention-based CNN system \u201cHABCNN\u201d. It is, to our knowledge, the first deep learning based system for this MCTest task. (ii) Prior document modeling systems based on deep neural networks mostly generate generic representation, this work is the first to incorporate attention so that document representation is biased towards the question requirement. (iii) Our HABCNN systems outperform other deep learning competitors by big margins."}, {"heading": "2 Related Work", "text": "Existing systems for MCTest task are mostly based on manually engineered features. Representative work includes\n[Narasimhan and Barzilay, 2015; Sachan et al., 2015; Wang and McAllester, 2015; Smith et al., 2015]. In these works, a common route is first to define a regularized loss function based on assumed feature vectors, then the effort focuses on designing effective features based on various rules. Even though these researches are groundbreaking for this task, their flexibility and their capacity for generalization are limited.\nDeep learning based approaches appeal to increasing interest in analogous tasks. Weston et al., [2014] introduce memory networks for factoid QA. Memory network framework is extended in [Weston et al., 2015; Kumar et al., 2015] for Facebook bAbI dataset. Peng et al. [2015]\u2019s Neural Reasoner infers over multiple supporting facts to generate an entity answer for a given question and it is also tested on bAbI. All of these works deal with some short texts with simple-grammar, aiming to generate an answer which is restricted to be one word denoting a location, a person etc.\nSome works also tried over other kinds of QA tasks. For example, Iyyer et al., [2014] present QANTA, a recursive neural network, to infer an entity based on its description text. This task is basically a matching between description and entity, no explicit question exist. Another difference with us lies in that all the sentences in the entity description actually contain partial information about the entity, hence a description is supposed to have only one representation. However in our task, the modeling of document should be dynamically changed according to the question analysis. Hermann et al., [2015] incorporate attention mechanism into LSTM for a QA task over news text. Still, their work does not handle some complex question types like \u201cWhy...\u201d, they merely aim to find the entity from the document to fill the slot in the query so that the completed query is true based on the document. Nevertheless, it inspires us to treat our task as a textual entailment problem by first reformatting question-answer pairs into statements.\nSome other deep learning systems are developed for answer selection task [Yu et al., 2014; Yang et al., 2015; Severyn and Moschitti, 2015; Shen et al., 2015; Wang et al., 2010]. Differently, this kind of question answering task does not involve document comprehension. They only try to match the question and answer candidate without any background information. Instead, we treat machine comprehension in this work as a question-answer matching problem under background guidance.\nOverall, for open-domain MCTest machine comprehension task, this work is the first to resort to deep neural networks."}, {"heading": "3 Model", "text": "We investigate this task by three approaches, illustrated in Figure 2. (i) We can compute two different document (D) representations in a common space, one based on question (Q) attention, one based on answer (A) attention, and compare them. This architecture we name HABCNN-QAP. (ii) We compute a representation of D based on Q attention (as before), but now we compare it directly with a representation of A. We name this architecture HABCNN-QP. (iii) We treat this QA task as textual entailment (TE), first reformatting Q-A pair into a statement (S), then matching S and D directly. This architecture we name HABCNN-TE. All three approaches are implemented in the common framework HABCNN."}, {"heading": "3.1 HABCNN", "text": "Recall that we use the abbreviations A (answer), Q (question), S (statement), D (document). HABCNN performs representation learning for triple (Q, A, D) in HABCNN-QP and HABCNN-QAP, for tuple (S, D) in HABCNN-TE. For convenience, we use \u201cquery\u201d to refer to Q, A, or S uniformly. HABCNN, depicted in Figure 3, has the following phases.\nInput Layer. The input is (query,D). Query is two individual sentences (for Q, A) or one single sentence (for S), D is a sequence of sentences. Words are initialized by ddimensional pre-trained word embeddings. As a result, each sentence is represented as a feature map with dimensionality of d \u00d7 s (s is sentence length). In Figure 3, each sentence in the input layer is depicted by a rectangle with multiple columns.\nSentence-CNN. Sentence-CNN is used for sentence representation learning from word level. Given a sentence of length s with a word sequence: v1, v2, . . . , vs, let vector ci \u2208 Rwd be the concatenated embeddings of w words vi\u2212w+1, . . . , vi where w is the filter width, d is the dimensionality of word representations and 0 < i < s + w. Embeddings for words vi, i < 1 and i > s, are zero padding. We then generate the representation pi \u2208 Rd1 for the phrase\nvi\u2212w+1, . . . , vi using the convolution weights W \u2208 Rd1\u00d7wd:\npi = tanh(W \u00b7 ci + b) (1)\nwhere bias b \u2208 Rd1 . d1 is called \u201ckernel size\u201d in CNN. Note that the sentence-CNNs for query and all document sentences share the same weights, so that the resulting sentence representations are comparable.\nSentence-Level Representation. The sentence-CNN generates a new feature map (omitted in Figure 3) for each input sentence, one column in the feature map denotes a phrase representation (i.e., pi in Equation (1)).\nFor the query and each sentence of D, we do element-wise 1-max-pooling (\u201cmax-pooling\u201d for short) [Collobert and Weston, 2008] over phrase representations to form their representations at this level.\nWe now treat D as a set of \u201cvital\u201d sentences and \u201cnoise\u201d sentences. We propose attention-pooling to learn the sentence-level representation of D as follows: first identify vital sentences by computing attention for each D\u2019s sentence as the cosine similarity between the its representation and the query representation, then select the k highest-attention sentences to do max-pooling over them. Taking Figure 3 as an example, based on the output of sentence-CNN layer, k = 2 important sentences with blue color are combined by maxpooling as the sentence-level representation vs of D; the other \u2013 white-color \u2013 sentence representations are neglected as they have low attentions. (If k = all, attention-pooling returns to the common max-pooling in [Collobert and Weston, 2008].) When the query is (Q,A), this step will be repeated, once for Q, once for A, to compute representations of D at the sentence level that are biased with respect to Q and A, respectively.\nSnippet-CNN. As the example in Figure 1 shows, to answer the first question \u201cwhy did Grandpa answer the door?\u201d, it does not suffice to compare this question only to the sentence \u201cGrandpa answered the door with a smile and welcomed Jimmy inside\u201d; instead, the snippet \u201cFinally, Jimmy arrived at Grandpa\u2019s house and knocked. Grandpa answered\nthe door with a smile and welcomed Jimmy inside\u201d should be used to compare. To this end, it is necessary to stack another CNN layer, snippet-CNN, to learn representations of snippets, i.e., units of one or more sentences. Thus, the basic units input to snippet-CNN (resp. sentence-CNN) are sentences (resp. words) and the output is representations of snippets (resp. sentences).\nConcretely, snippet-CNN puts all sentence representations in column sequence as a feature map and conducts another convolution operation over it. With filter width w, this step generates representation of snippet with w consecutive sentences. Similarly, we use the same CNN to learn higher-abstraction query representations (just treating query as a document with only one sentence, so that the higherabstraction query representation is in the same space with corresponding snippet representations).\nSnippet-Level Representation. For the output of snippetCNN, each representation is more abstract and denotes bigger granularity. We apply the same attention-pooling process to snippet level representations: attention values are computed as cosine similarities between query and snippets and the snippets with the k largest attentions are retained. Maxpooling over the k selected snippet representations then creates the snippet-level representation vt of D. Two selected snippets are shown as red in Figure 3.\nOverall Representation. Based on convolution layers at two different granularity, we have derived query-biased representations of D at sentence level (i.e., vs) as well as snippet level (i.e., vt). In order to create a flexible choice for open Q/A, we develop a highway network [Srivastava et al., 2015] to combine the two levels of representations as an overall representation vo of D:\nvo = (1\u2212 h) vs + h vt (2) where highway network weights h are learned by\nh = \u03c3(Whvs + b) (3)\nwhere Wh \u2208 Rd1\u00d7d1 . With the same highway network, we can generate the overall query representation, ri in Figure 3, by combining the two representations of the query at sentence and snippet levels."}, {"heading": "3.2 HABCNN-QP & HABCNN-QAP", "text": "HABCNN-QP/QAP computes the representation of D as a projection of D, either based on attention from Q or based on attention from A. We hope that these two projections of the document are close for a correct A and less close for an incorrect A. As we said in related work, machine comprehension can be viewed as an answer selection task using the document D as critical background information. Here, HABCNNQP/QAP do not compare Q and A directly, but they use Q and A to filter the document differently, extracting what is critical for the Q/A match by attention-pooling. Then they match the two document representations in the new space.\nFor ease of exposition, we have used the symbol vo so far, but in HABCNN-QP/QAP we compute two different document representations: voq , for which attention is computed with respect to Q; and voa for which attention is computed\nwith respect to A. ri also has two versions, one for Q: riq , one for A: ria.\nHABCNN-QP and HABCNN-QAP make different use of voq . HABCNN-QP compares voq with answer representation ria. HABCNN-QAP compares voq with voa. HABCNNQAP projects D twice, once based on attention from Q, once based on attention from A and compares the two projected representations, shown in Figure 2 (top). HABCNN-QP only utilizes the Q-based projection of D and then compares the projected document with the answer representation, shown in Figure 2 (middle)."}, {"heading": "3.3 HABCNN-TE", "text": "HABCNN-TE treats machine comprehension as textual entailment. We use the statements that are provided as part of MCTest. Each statement corresponds to a question-answer pair; e.g., the Q/A pair \u201cWhy did Grandpa answer the door?\u201d / \u201cBecause he saw the insects\u201d (Figure 1) is reformatted into the statement \u201cGrandpa answered the door because he saw the insects\u201d. The question answering task is then cast as: \u201cdoes the document entail the statement?\u201d\nFor HABCNN-TE, shown in Figure 2 (bottom), the input for Figure 3 is the pair (S,D). HABCNN-TE tries to match the S\u2019s representation ri with the D\u2019s representation vo."}, {"heading": "4 Experiments", "text": ""}, {"heading": "4.1 Dataset", "text": "MCTest1 has two subsets. MCTest-160 is a set of 160 items, each consisting of a document, four questions followed by one correct anwer and three incorrect answers (split into 70 train, 30 dev and 60 test) and MCTest-500 a set of 500 items (split into 300 train, 50 dev and 150 test)."}, {"heading": "4.2 Training Setup and Tricks", "text": "Our training objective is to minimize the following ranking loss function:\nL(d, a+, a\u2212) = max(0, \u03b1+ S(d, a\u2212)\u2212 S(d, a+)) (4)\nwhere S(\u00b7, \u00b7) is a matching score between two representation vectors. Cosine similarity is used throughout. \u03b1 is a constant.\nFor this common ranking loss, we also have two styles to utilize the data in view of each positive answer is accompanied with three negative answers. One is treating (d, a+, a\u22121 , a\u22122 , a \u2212 3 ) as a training example, then our loss function can have three \u201cmax()\u201d terms, each for a positive-negative pair; the other one is treating (d, a+, a\u2212i ) as an individual training example. In practice, we find the second way works better. We conjecture that the second way has more training examples, and positive answers are repeatedly used to balance the amounts of positive and negative answers.\nMultitask learning: Question typing is commonly used and proved to be very helpful in QA tasks [Sachan et al., 2015]. Inspired, we stack a logistic regression layer over question representation riq , with the purpose that this subtask can favor the parameter tuning of the whole system, and\n1http://requery.microsoft.com/mct\nfinally the question is better recognized and is able to find the answer more accurately.\nTo be specific, we classify questions into 12 classes: \u201chow\u201d, \u201chow much\u201d, \u201chow many\u201d, \u201cwhat\u201d, \u201cwho\u201d, \u201cwhere\u201d, \u201cwhich\u201d, \u201cwhen\u201d, \u201cwhose\u201d, \u201cwhy\u201d, \u201cwill\u201d and \u201cother\u201d. The question label is created by querying for the label keyword in the question. If more than one keyword appears in a question, we adopt the one appearing earlier and the more specific one (e.g., \u201chow much\u201d, not \u201chow\u201d). In case there is no match, the class \u201cother\u201d is assigned.\nWe train with AdaGrad [Duchi et al., 2011] and use 50- dimensional GloVe [Pennington et al., 2014] to initialize word representations,2 kept fixed during training. Table 1 gives hyperparameter values, tuned on dev.\nWe consider two evaluation metrics: accuracy (proportion of questions correctly answered) and NDCG4 [Ja\u0308rvelin and Keka\u0308la\u0308inen, 2002]. Unlike accuracy which evaluates if the question is correctly answered or not, NDCG4, being a measure of ranking quality, evaluates the position of the correct answer in our predicted ranking."}, {"heading": "4.3 Baseline Systems", "text": "This work focuses on the comparison with systems about distributed representation learning and deep learning:\nAddition. Directly compare question and answers without considering the D. Sentence representations are computed by element-wise addition over word representations.\nAddition-proj. First compute sentence representations for Q, A and all D sentences as the same way as Addition, then match the two sentences in D which have highest similarity with Q and A respectively.\nNR. The Neural Reasoner [Peng et al., 2015] has an encoding layer, multiple reasoning layers and a final answer layer. The input for the encoding layer is a question and the sentences of the document (called facts); each sentence is encoded by a GRU into a vector. In each reasoning layer, NR lets the question representation interact with each fact representation as reasoning process. Finally, all temporary reasoning clues are pooled as answer representation.\nAR. The Attentive Reader [Hermann et al., 2015] is implemented by modeling the whole D as a word sequence \u2013 without specific sentence / snippet representations \u2013 using an LSTM. Attention mechanism is implemented at word representation level.\nOverall, baselines Addition and Addition-proj do not involve complex composition and inference. NR and AR represent the top-performing deep neural networks in QA tasks.\n2http://nlp.stanford.edu/projects/glove/"}, {"heading": "4.4 HABCNN Variants", "text": "In addition to the main architectures described above, we also explore two variants of ABCHNN, inspired by [Peng et al., 2015] and [Hermann et al., 2015], respectively.\nVariant-I: As RNNs are widely recognized as a competitor of CNNs in sentence modeling, similar with [Peng et al., 2015], we replace the sentence-CNN in Figure 3 by a GRU while keeping other parts unchanged.\nVariant-II: How to model attention at the granularity of words was shown in [Hermann et al., 2015]; see their paper for details. We develop their attention idea and model attention at the granularity of sentence and snippet. Our attention gives different weights to sentences/snippets (not words), then computes the document representation as a weighted average of all sentence/snippet representations."}, {"heading": "4.5 Results", "text": "Table 2 lists the performance of baselines, HABCNNTE variants, HABCNN systems in the first, second and last block, respectively (we only report variants for topperforming HABCNN-TE). Consistently, our HABCNN systems outperform all baselines, especially surpass the two competitive deep learning based systems AR and NR. The margin between our best-performing ABHCNN-TE and NR is 15.6/16.5 (accuracy/NDCG) on MCTest-150 and 7.3/4.6 on MCTest-500. This demonstrates the promise of our architecture in this task.\nAs said before, both AR and NR systems aim to generate answers in entity form. Their designs might not suit this machine comprehension task, in which the answers are openlyformed based on summarizing or abstracting the clues. To be more specific, AR models D always at word level, attentions are also paid to corresponding word representations, which is applicable for entity-style answers, but is less suitable for comprehension at sentence level or even snippet level. NR contrarily models D in sentence level always, neglecting the discovering of key phrases which however compose most of answers. In addition, the attention of AR system and the question-fact interaction in NR system both bring large numbers of parameters, this potentially constrains their power in a dataset of limited size.\nFor Variant-I and Variant-II (second block of Table 2), we can see that both modifications do harm to the original HABCNN-TE performance. The first variant, i.e, replacing the sentence-CNN in Figure 3 as GRU module is not helpful for this task. We suspect that this lies in the fundamental function of CNN and GRU. The CNN models a sentence without caring about the global word order information, and max-pooling is supposed to extract the features of key phrases in the sentence no matter where the phrases are located. This property should be useful for answer detection, as answers are usually formed by discovering some key phrases, not all words in a sentence should be considered. However, a GRU models a sentence by reading the words sequentially, the importance of phrases is less determined by the question requirement. The second variant, using a more complicated attention scheme to model biased D representations than simple cosine similarity based attention used in our model, is less effective to detect truly informative sentences or snippet. We\ndoubt such kind of attention scheme when used in sentence sequences of large size. In training, the attention weights after softmax normalization have actually small difference across sentences, this means the system can not distinguish key sentences from noise sentences effectively. Our cosine similarity based attention-pooling, though pretty simple, is able to filter noise sentences more effectively, as we only pick top-k pivotal sentences to form D representation finally. This trick makes the system simple while effective."}, {"heading": "4.6 Case Study and Error Analysis", "text": "In Figure 4, we visualize the attention distribution at sentence level as well as snippet level for the statement \u201c Grandpa answered the door because Jimmy knocked\u201d whose corresponding question requires multiple sentences to answer. From its left part, we can see that \u201cGrandpa answered the door with a smile and welcomed Jimmy inside\u201d has the highest attention weight. This meets the intuition that this sentence has semantic overlap with the statement. And yet this sentence does not contain the answer. Look further the right part, in which the CNN layer over sentence-level representations is supposed to extract high-level features of snippets. In this level, the highest attention weight is cast to the best snippet \u201cFinally, Jimmy arrived...knocked. Grandpa answered the door...\u201d. And the neighboring snippets also get relatively higher attentions than other regions. Recall that our system chooses the one sentence with top attention at left part and choose top-3 snippets at right part (referring to k value in Table 1) to form D representations at different granularity, then uses a high-\nway network to combine both representations as an overall D representation. This visualization hints that our architecture provides a good way for a question to compromise key information from different granularity.\nWe also do some preliminary error analysis. One big obstacle for our systems is the \u201chow many\u201d questions. For example, for question \u201chow many rooms did I say I checked?\u201d and the answer candidates are four digits \u201c5,4,3,2\u201d which never appear in the D, but require the counting of some locations. However, these digital answers can not be modeled well by distributed representations so far. In addition, digital answers also appear for \u201cwhat\u201d questions, like \u201cwhat time did...\u201d. Another big limitation lies in \u201cwhy\u201d questions. This question type requires complex inference and long-distance dependencies. We observed that all deep lerning systems, including the two baselines, suffered somewhat from it."}, {"heading": "5 Conclusion", "text": "This work takes the lead in presenting a CNN based neural network system for open-domain machine comprehension task. Our systems tried to solve this task in a document projection way as well as a textual entailment way. The latter one demonstrates slightly better performance. Overall, our architecture, modeling dynamic document representation by attention scheme from sentence level to snippet level, shows promising results in this task. In the future, more finegrained representation learning approaches are expected to model complex answer types and question types."}], "references": [{"title": "Learning phrase representations using rnn encoder-decoder for statistical machine translation", "author": ["Cho et al", "2014] Kyunghyun Cho", "Bart Van Merri\u00ebnboer", "Caglar Gulcehre", "Dzmitry Bahdanau", "Fethi Bougares", "Holger Schwenk", "Yoshua Bengio"], "venue": "Proceedings of EMNLP,", "citeRegEx": "al. et al\\.,? \\Q2014\\E", "shortCiteRegEx": "al. et al\\.", "year": 2014}, {"title": "A unified architecture for natural language processing: Deep neural networks with multitask learning", "author": ["Ronan Collobert", "Jason Weston"], "venue": "Proceedings of ICML, pages 160\u2013167,", "citeRegEx": "Collobert and Weston. 2008", "shortCiteRegEx": null, "year": 2008}, {"title": "The Journal of Machine Learning Research", "author": ["John Duchi", "Elad Hazan", "Yoram Singer. Adaptive subgradient methods for online learning", "stochastic optimization"], "venue": "12:2121\u20132159,", "citeRegEx": "Duchi et al.. 2011", "shortCiteRegEx": null, "year": 2011}, {"title": "In Proceedings of NIPS", "author": ["Karl Moritz Hermann", "Tomas Kocisky", "Edward Grefenstette", "Lasse Espeholt", "Will Kay", "Mustafa Suleyman", "Phil Blunsom. Teaching machines to read", "comprehend"], "venue": "pages 1684\u20131692,", "citeRegEx": "Hermann et al.. 2015", "shortCiteRegEx": null, "year": 2015}, {"title": "Neural computation", "author": ["Sepp Hochreiter", "J\u00fcrgen Schmidhuber. Long short-term memory"], "venue": "9(8):1735\u20131780,", "citeRegEx": "Hochreiter and Schmidhuber. 1997", "shortCiteRegEx": null, "year": 1997}, {"title": "In Proceedings of EMNLP", "author": ["Mohit Iyyer", "Jordan Boyd-Graber", "Leonardo Claudino", "Richard Socher", "Hal Daum\u00e9 III. A neural network for factoid question answering over paragraphs"], "venue": "pages 633\u2013644,", "citeRegEx": "Iyyer et al.. 2014", "shortCiteRegEx": null, "year": 2014}, {"title": "ACM Transactions on Information Systems (TOIS)", "author": ["Kalervo J\u00e4rvelin", "Jaana Kek\u00e4l\u00e4inen. Cumulated gain-based evaluation of ir techniques"], "venue": "20(4):422\u2013446,", "citeRegEx": "J\u00e4rvelin and Kek\u00e4l\u00e4inen. 2002", "shortCiteRegEx": null, "year": 2002}, {"title": "Ask me anything: Dynamic memory networks for natural language processing", "author": ["Ankit Kumar", "Ozan Irsoy", "Jonathan Su", "James Bradbury", "Robert English", "Brian Pierce", "Peter Ondruska", "Ishaan Gulrajani", "Richard Socher"], "venue": "arXiv preprint arXiv:1506.07285,", "citeRegEx": "Kumar et al.. 2015", "shortCiteRegEx": null, "year": 2015}, {"title": "In Proceedings of INTERSPEECH", "author": ["Tomas Mikolov", "Martin Karafi\u00e1t", "Lukas Burget", "Jan Cernock\u1ef3", "Sanjeev Khudanpur. Recurrent neural network based language model"], "venue": "pages 1045\u20131048,", "citeRegEx": "Mikolov et al.. 2010", "shortCiteRegEx": null, "year": 2010}, {"title": "In Proceedings of ACL", "author": ["Karthik Narasimhan", "Regina Barzilay. Machine comprehension with discourse relations"], "venue": "pages 1253\u20131262,", "citeRegEx": "Narasimhan and Barzilay. 2015", "shortCiteRegEx": null, "year": 2015}, {"title": "CoRR", "author": ["Baolin Peng", "Zhengdong Lu", "Hang Li", "Kam-Fai Wong. Towards neural network-based reasoning"], "venue": "abs/1508.05508,", "citeRegEx": "Peng et al.. 2015", "shortCiteRegEx": null, "year": 2015}, {"title": "Glove: Global vectors for word representation", "author": ["Jeffrey Pennington", "Richard Socher", "Christopher D Manning"], "venue": "Proceedings of EMNLP, 12:1532\u20131543,", "citeRegEx": "Pennington et al.. 2014", "shortCiteRegEx": null, "year": 2014}, {"title": "Mctest: A challenge dataset for the open-domain machine comprehension", "author": ["Richardson et al", "2013] Matthew Richardson", "Christopher JC Burges", "Erin Renshaw"], "venue": null, "citeRegEx": "al. et al\\.,? \\Q2013\\E", "shortCiteRegEx": "al. et al\\.", "year": 2013}, {"title": "In Proceedings of ACL", "author": ["Mrinmaya Sachan", "Avinava Dubey", "Eric P Xing", "Matthew Richardson. Learning answerentailing structures for machine comprehension"], "venue": "pages 239\u2013249,", "citeRegEx": "Sachan et al.. 2015", "shortCiteRegEx": null, "year": 2015}, {"title": "In Proceedings of SIGIR", "author": ["Aliaksei Severyn", "Alessandro Moschitti. Learning to rank short text pairs with convolutional deep neural networks"], "venue": "pages 373\u2013382,", "citeRegEx": "Severyn and Moschitti. 2015", "shortCiteRegEx": null, "year": 2015}, {"title": "Question/answer matching for cqa system via combining lexical and sequential information", "author": ["Yikang Shen", "Wenge Rong", "Zhiwei Sun", "Yuanxin Ouyang", "Zhang Xiong"], "venue": "Proceedings of AAAI, pages 275\u2013 281,", "citeRegEx": "Shen et al.. 2015", "shortCiteRegEx": null, "year": 2015}, {"title": "In Proceedings of EMNLP", "author": ["Ellery Smith", "Nicola Greco", "Matko Bosnjak", "Andreas Vlachos. A strong lexical matching method for the machine comprehension test"], "venue": "pages 1693\u20131698,", "citeRegEx": "Smith et al.. 2015", "shortCiteRegEx": null, "year": 2015}, {"title": "In NIPS", "author": ["Rupesh K Srivastava", "Klaus Greff", "J\u00fcrgen Schmidhuber. Training very deep networks"], "venue": "pages 2368\u20132376,", "citeRegEx": "Srivastava et al.. 2015", "shortCiteRegEx": null, "year": 2015}, {"title": "frames", "author": ["Hai Wang", "Mohit Bansal Kevin Gimpel David McAllester. Machine comprehension with syntax"], "venue": "and semantics. In Proceedings of ACL, Volume 2: Short Papers, pages 700\u2013706,", "citeRegEx": "Wang and McAllester. 2015", "shortCiteRegEx": null, "year": 2015}, {"title": "In Proceedings of ACL", "author": ["Baoxun Wang", "Xiaolong Wang", "Chengjie Sun", "Bingquan Liu", "Lin Sun. Modeling semantic relevance for question-answer pairs in web social communities"], "venue": "pages 1230\u20131238,", "citeRegEx": "Wang et al.. 2010", "shortCiteRegEx": null, "year": 2010}, {"title": "Memory networks", "author": ["Jason Weston", "Sumit Chopra", "Antoine Bordes"], "venue": "Proceedings of ICLR,", "citeRegEx": "Weston et al.. 2014", "shortCiteRegEx": null, "year": 2014}, {"title": "Towards ai-complete question answering: a set of prerequisite toy tasks", "author": ["Jason Weston", "Antoine Bordes", "Sumit Chopra", "Tomas Mikolov"], "venue": "arXiv preprint arXiv:1502.05698,", "citeRegEx": "Weston et al.. 2015", "shortCiteRegEx": null, "year": 2015}, {"title": "Wikiqa: A challenge dataset for open-domain question answering", "author": ["Yi Yang", "Wen-tau Yih", "Christopher Meek"], "venue": "Proceedings of EMNLP, pages 2013\u2013 2018,", "citeRegEx": "Yang et al.. 2015", "shortCiteRegEx": null, "year": 2015}, {"title": "Deep learning for answer sentence selection", "author": ["Lei Yu", "Karl Moritz Hermann", "Phil Blunsom", "Stephen Pulman"], "venue": "ICLR workshop,", "citeRegEx": "Yu et al.. 2014", "shortCiteRegEx": null, "year": 2014}], "referenceMentions": [{"referenceID": 20, "context": "Despite intensive recent work [Weston et al., 2014; Weston et al., 2015; Hermann et al., 2015; Sachan et al., 2015], the problem is far from solved.", "startOffset": 30, "endOffset": 115}, {"referenceID": 21, "context": "Despite intensive recent work [Weston et al., 2014; Weston et al., 2015; Hermann et al., 2015; Sachan et al., 2015], the problem is far from solved.", "startOffset": 30, "endOffset": 115}, {"referenceID": 3, "context": "Despite intensive recent work [Weston et al., 2014; Weston et al., 2015; Hermann et al., 2015; Sachan et al., 2015], the problem is far from solved.", "startOffset": 30, "endOffset": 115}, {"referenceID": 13, "context": "Despite intensive recent work [Weston et al., 2014; Weston et al., 2015; Hermann et al., 2015; Sachan et al., 2015], the problem is far from solved.", "startOffset": 30, "endOffset": 115}, {"referenceID": 8, "context": "The reason of choosing a CNN rather than other sequence models like recurrent neural network [Mikolov et al., 2010], long short-term memory unit (LSTM [Hochreiter and Schmidhuber, 1997]), gated recurrent unit (GRU [Cho et al.", "startOffset": 93, "endOffset": 115}, {"referenceID": 4, "context": ", 2010], long short-term memory unit (LSTM [Hochreiter and Schmidhuber, 1997]), gated recurrent unit (GRU [Cho et al.", "startOffset": 43, "endOffset": 77}, {"referenceID": 9, "context": "[Narasimhan and Barzilay, 2015; Sachan et al., 2015; Wang and McAllester, 2015; Smith et al., 2015].", "startOffset": 0, "endOffset": 99}, {"referenceID": 13, "context": "[Narasimhan and Barzilay, 2015; Sachan et al., 2015; Wang and McAllester, 2015; Smith et al., 2015].", "startOffset": 0, "endOffset": 99}, {"referenceID": 18, "context": "[Narasimhan and Barzilay, 2015; Sachan et al., 2015; Wang and McAllester, 2015; Smith et al., 2015].", "startOffset": 0, "endOffset": 99}, {"referenceID": 16, "context": "[Narasimhan and Barzilay, 2015; Sachan et al., 2015; Wang and McAllester, 2015; Smith et al., 2015].", "startOffset": 0, "endOffset": 99}, {"referenceID": 21, "context": "Memory network framework is extended in [Weston et al., 2015; Kumar et al., 2015] for Facebook bAbI dataset.", "startOffset": 40, "endOffset": 81}, {"referenceID": 7, "context": "Memory network framework is extended in [Weston et al., 2015; Kumar et al., 2015] for Facebook bAbI dataset.", "startOffset": 40, "endOffset": 81}, {"referenceID": 23, "context": "Some other deep learning systems are developed for answer selection task [Yu et al., 2014; Yang et al., 2015; Severyn and Moschitti, 2015; Shen et al., 2015; Wang et al., 2010].", "startOffset": 73, "endOffset": 176}, {"referenceID": 22, "context": "Some other deep learning systems are developed for answer selection task [Yu et al., 2014; Yang et al., 2015; Severyn and Moschitti, 2015; Shen et al., 2015; Wang et al., 2010].", "startOffset": 73, "endOffset": 176}, {"referenceID": 14, "context": "Some other deep learning systems are developed for answer selection task [Yu et al., 2014; Yang et al., 2015; Severyn and Moschitti, 2015; Shen et al., 2015; Wang et al., 2010].", "startOffset": 73, "endOffset": 176}, {"referenceID": 15, "context": "Some other deep learning systems are developed for answer selection task [Yu et al., 2014; Yang et al., 2015; Severyn and Moschitti, 2015; Shen et al., 2015; Wang et al., 2010].", "startOffset": 73, "endOffset": 176}, {"referenceID": 19, "context": "Some other deep learning systems are developed for answer selection task [Yu et al., 2014; Yang et al., 2015; Severyn and Moschitti, 2015; Shen et al., 2015; Wang et al., 2010].", "startOffset": 73, "endOffset": 176}, {"referenceID": 1, "context": "For the query and each sentence of D, we do element-wise 1-max-pooling (\u201cmax-pooling\u201d for short) [Collobert and Weston, 2008] over phrase representations to form their representations at this level.", "startOffset": 97, "endOffset": 125}, {"referenceID": 1, "context": "(If k = all, attention-pooling returns to the common max-pooling in [Collobert and Weston, 2008].", "startOffset": 68, "endOffset": 96}, {"referenceID": 17, "context": "In order to create a flexible choice for open Q/A, we develop a highway network [Srivastava et al., 2015] to combine the two levels of representations as an overall representation vo of D:", "startOffset": 80, "endOffset": 105}, {"referenceID": 13, "context": "Multitask learning: Question typing is commonly used and proved to be very helpful in QA tasks [Sachan et al., 2015].", "startOffset": 95, "endOffset": 116}, {"referenceID": 2, "context": "We train with AdaGrad [Duchi et al., 2011] and use 50dimensional GloVe [Pennington et al.", "startOffset": 22, "endOffset": 42}, {"referenceID": 11, "context": ", 2011] and use 50dimensional GloVe [Pennington et al., 2014] to initialize word representations,2 kept fixed during training.", "startOffset": 36, "endOffset": 61}, {"referenceID": 6, "context": "We consider two evaluation metrics: accuracy (proportion of questions correctly answered) and NDCG4 [J\u00e4rvelin and Kek\u00e4l\u00e4inen, 2002].", "startOffset": 100, "endOffset": 131}, {"referenceID": 10, "context": "The Neural Reasoner [Peng et al., 2015] has an encod-", "startOffset": 20, "endOffset": 39}, {"referenceID": 3, "context": "The Attentive Reader [Hermann et al., 2015] is implemented by modeling the whole D as a word sequence \u2013 without specific sentence / snippet representations \u2013 using an", "startOffset": 21, "endOffset": 43}, {"referenceID": 10, "context": "In addition to the main architectures described above, we also explore two variants of ABCHNN, inspired by [Peng et al., 2015] and [Hermann et al.", "startOffset": 107, "endOffset": 126}, {"referenceID": 3, "context": ", 2015] and [Hermann et al., 2015], respectively.", "startOffset": 12, "endOffset": 34}, {"referenceID": 10, "context": "Variant-I: As RNNs are widely recognized as a competitor of CNNs in sentence modeling, similar with [Peng et al., 2015], we replace the sentence-CNN in Figure 3 by a GRU while keeping other parts unchanged.", "startOffset": 100, "endOffset": 119}, {"referenceID": 3, "context": "Variant-II: How to model attention at the granularity of words was shown in [Hermann et al., 2015]; see their paper for details.", "startOffset": 76, "endOffset": 98}], "year": 2016, "abstractText": "Understanding open-domain text is one of the primary challenges in natural language processing (NLP). Machine comprehension benchmarks evaluate the system\u2019s ability to understand text based on the text content only. In this work, we investigate machine comprehension on MCTest, a question answering (QA) benchmark. Prior work is mainly based on feature engineering approaches. We come up with a neural network framework, named hierarchical attention-based convolutional neural network (HABCNN), to address this task without any manually designed features. Specifically, we explore HABCNN for this task by two routes, one is through traditional joint modeling of document, question and answer, one is through textual entailment. HABCNN employs an attention mechanism to detect key phrases, key sentences and key snippets that are relevant to answering the question. Experiments show that HABCNN outperforms prior deep learning approaches by a big margin.", "creator": "LaTeX with hyperref package"}}}