{"id": "1602.04489", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "14-Feb-2016", "title": "Convolutional Tables Ensemble: classification in microseconds", "abstract": "We study classifiers operating under severe classification time constraints, corresponding to 1-1000 CPU microseconds, using Convolutional Tables Ensemble (CTE), an inherently fast architecture for object category recognition. The architecture is based on convolutionally-applied sparse feature extraction, using trees or ferns, and a linear voting layer. Several structure and optimization variants are considered, including novel decision functions, tree learning algorithm, and distillation from CNN to CTE architecture. The main limitation of this research is that the present study design involves a relatively small subset of computational experiments, and thus the analysis method used is difficult. However, the data from this study was obtained by searching for data about the underlying structures of each of the above domains.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n", "histories": [["v1", "Sun, 14 Feb 2016 19:21:17 GMT  (413kb,D)", "http://arxiv.org/abs/1602.04489v1", "10 pages"]], "COMMENTS": "10 pages", "reviews": [], "SUBJECTS": "cs.CV cs.LG", "authors": ["aharon bar-hillel", "eyal krupka", "noam bloom"], "accepted": false, "id": "1602.04489"}, "pdf": {"name": "1602.04489.pdf", "metadata": {"source": "CRF", "title": "Convolutional Tables Ensemble: classification in microseconds", "authors": ["Aharon Bar-Hillel", "Eyal Krupka"], "emails": ["aharonb@microsoft.com", "eyalk@microsoft.com", "t-noblo@microsoft.com"], "sections": [{"heading": "1. Introduction", "text": "Practical object recognition problems often have to be solved under severe computation and time constraints. Some examples of interest are natural user interfaces, automotive active safety, robotic vision or sensing for the Internet of Things (IoT). Often the problem is to obtain high accuracy in real time, on a low power platform, or in a background process that can only utilize a small fraction of the CPU. In other cases the classifier is part of a cascade, or a complex multiple-classifier system. The accuracy-speed trade-off has thus been widely discussed in the literature, and various architectures have been suggested [14, 35, 7, 4, 24, 31]. Here we focus on the extreme end of this trade-off and ask: how accurate can we get for classifiers working in 1\u2212 1000 CPU microseconds.\nAs a thought experiment, the fastest classifier possible would be the one concatenating all the pixel values into a single index, and then using this index to access a table listing the labels of all possible images. Of course, this is not feasible due to the exponential requirements of memory and\ntraining set size, but this limit case points us the direction to follow. The actual architecture we pursue compromises this limit idea in two main ways: first, instead of encoding the whole image with a single index, the image is treated as a dense set of patches where each patch is encoded using the same parameters. This is analogous to a convolutional layer in a convolutional neural network (CNN) [22], where the same set of filters is applied at each image position. Second, instead of describing a patch using a single long index, it is encoded with a set of short indices, that are used to access a set of reasonable-size tables. Votes of all tables at all positions are combined linearly to yield the classifier outputs. Variants of this architecture have been used successfully mainly for classification of depth images [20, 31]. Here we explore this regime for visual recognition in general, under the term Convolutional Tables Ensemble (CTE).\nThe idea of applying the same feature extraction on a dense locations grid is very old and influential in vision, and is a key tenet in CNNs, the state-of-the-art in object recognition. It provides a good structural prior in the form of translation invariance. Another advantage lies in enhanced sample size for learning local feature parameters, since these can be trained from (number of training images)\u00d7(number of image patches) instances. The architectures we consider here are not deep in the CNN sense, and correspond to a single convolutional layer, followed by spatial pooling.\nThe main vessel we use for obtaining high classification speed is the utilization of table-based feature extractors, instead of heavier computations such as applying a large set of filters in a convolutional layer. In table-based feature extraction, the patch is characterized using a set of fast bit functions, such as a comparison between two pixels. K bits are extracted and concatenated into a word. This word is then used as an index into a set of weight tables, one per class, and the weights extracted provide the classes support from this word. Support weights are accumulated across many tables and all image positions, and the label is decided according to the highest scoring class.\nThe power of this architecture is in the combination of fast-but-rich features with a high capacity classifier. Using\n1\nar X\niv :1\n60 2.\n04 48\n9v 1\n[ cs\n.C V\n] 1\n4 Fe\nb 20\n16\nK quick bit functions, the representation considers all their 2K combinations as features. The representation is highly sparse, with 1\n2K of the features active at each position. The\nclassifier is linear, but it operates over numerous highly non linear features. For M tables and C classes, the number of weights to optimize is M2KC, which can be very high even for modest values of M,K,C. The architecture hence requires a large training set to be used, and it effectively trades training sample size for speed and accuracy.\nPushing the speed-accuracy envelope using this architecture requires making careful structural and algorithmic choices. First, bit functions and image preprocessing should be chosen. We start with the simple functions employed in [20, 31], which were suitable for depth images, and extend them using gradient and color based channels and features employed in [9]. Another type of bit function introduced are spatial bits stating the rough location of the patch, which enable us to combine global and local pooling. A second important choice is between conditional computation of bit functions, leading to tree structures like used in [31], and unconditional computation as in fern structures [20]. While trees may enable higher accuracy, ferns are better suited for vector processing (such as SSE instructions) and thus provide significant speed advantages. We explore between these ends empirically using a \u2019long tree\u2019 structure, whose configuration enables testing intermediate structures.\nSeveral works have addressed the challenges of learning a tables-based classifier [12, 5, 34, 23, 6, 20, 28]. These vary in optimization effort from extremely random forests [12] to global optimization of table weights and greedy forward choice of bit functions [28, 20]. Our approach builds on previous approaches, mostly [20], and extends them with new possibilities. We learn the table ensemble by adding one table at a time, using a framework similar to the \u2019anyboost\u2019 algorithm [26, 2]. Training iterates between minimizing a global convex loss, differentiating this loss w.r.t. examples, and using these gradients to guide construction of the next table. For the global optimization we used two main options: an SVM loss as used in [20] and a softmax loss as commonly used in CNN training. For the optimization of the bit function parameters in a new fern/tree we developed several options: forward bit selection, iterative bit replacement, and iterative local refinement. In some cases, such as the threshold parameters of certain bits, an algorithm providing the optimal solution is suggested. The algorithms considered are described in Section 2.\nSince CTEs can be much faster than CNNs, while the latter excel at accuracy, one would naturally like to merge their advantages if possible. In several recent studies [17, 15, 29], the output of an accurate but computationally expensive classifier is used to train another classifier, with a different and often computationally cheaper architecture. We made a preliminary attempt to use this technique, termed distilla-\ntion in [15], to train a CTE classifier with a CNN teacher, with encouraging results on the MNIST data.\nIn Section 3.2 we present experiments demonstrating the performance gains of our techniques by comparison with the DFE method of [20], ablation studies, fern-tree trade-off experiments, and distillation results. We use several publicly available object recognition benchmarks: MNIST [22], CIFAR-10 [19], SVHN [27] and 3-HANDPOSE [20]. CTE achieves error improvements of 24 \u2212 45% over [20], with 38% improvement on 3-HANDPOSE, the original data used in [20]. For MNIST, we were able to train a CTE with 0.45% error and close to 100\u00b5s running time using the distillation technique. Even higher accuracy of 0.39% error can be obtained with a tree-based CTE, with some cost in running time.\nIn section 3.3 we systematically experimented with CTE configurations to obtain accuracy-speed trade-off graphs for the datasets mentioned. These graphs are compared to similar graphs obtained for CNNs. For the latter we trained NIN networks [25], combining state-of-the-art accuracy with significant speed advantages, and further accelerated them by scaling parameters of breadth, NIN output dimension and convolution stride. Our results indicate that for a highly restricted CPU budget CTEs provide significantly better accuracy than CNNs, or conversely, CTEs can obtain the same error with a CPU budget lower by 5\u2212200X . Typically this is true for classifiers operating below 100 microseconds on a single CPU thread. For the very low CPU bound domain CTE can still provide useful results, whereas CNNs completely break. This makes CTEs a natural architecture choice for that domain. Alternatives to CTE may be provided by the literature dealing with CNN acceleration [35, 36, 1]. However obtaining the speed gains made possible by CTEs using such techniques is far from trivial.\nIn summary, our main contribution in this paper is twofold: First, we develop new algorithms in the CTE framework, improving upon related similar art and extending the framework to general object recognition. Second, we pose an alternative to CNN which enables improved accuracy at the highly CPU constrained regime. Short concluding remarks are given in Section 4."}, {"heading": "2. Convolutional Tables Ensemble", "text": "We present the classifier structure in Section 2.1 and derive the learning algorithm in 2.2. The details and variations of structure and training appear in 2.3 and 2.4 respectively."}, {"heading": "2.1. Notation and classifier structure", "text": "A convolutional table ensemble is a classifier f : I \u2192 {1, .., C} where I \u2208 RSx\u00d7Sy\u00d7D and C is the number of classes. The image I may undergoes a preparation stage where additional feature channels (\u2019maps\u2019) may be added to it, so it is transformed to Ie \u2208 RSx\u00d7Sy\u00d7De with De \u2265\nAlgorithm 1 Convolutional Tables Ensemble: Classification Input: An image I of size Sx \u00d7 Sy \u00d7D, classifier parameters (\u0398m, Am,Wm,c, T c)M,Cm=1,c=1 Am \u2282 {1, .., Sx} \u00d7 {1, .., Sy}, Wm,c \u2208 R2 K\n,T c \u2208 R Output: A classifier decision in {1, ..C} Initialization: For c = 1, .., C Score[c] = \u2212T c Prepare extended image Ie \u2208 RSx\u00d7Sy\u00d7De For all tables m = 1, ..,M\nFor all pixels p \u2208 Am Compute F = B(IeN(p); \u0398\nm) \u2208 {0, 1}K For c = 1, .., C Score[c] = Score[c] +Wm,c[F ]\nReturn arg maxc Score[c]\nD. After preparation, the ensemble sums the votes of M convolutional tables, each in turn sums the votes of a word calculator over all pixels in a aggregation area. We now explain this process bottom-up.\nWord calculator: For an image location p = (x, y) \u2208 {1, .., Sx}\u00d7{1, ..Sy}, we denote its neighborhood byN(p), and by IN(p) the patch centered at location p. A word calculator is a feature extractor applied to such a patch and returning a K-bit index, i.e. a function W : Rl\u00d7l\u00d7De \u2192 {0, 1}K , where l is the neighborhood size. In a classifier we have M calculators denoted by Bm = B(\u00b7 ; \u0398m) where m = 1, ..,M and \u0398m are the calculator parameters. The calculator computes its output by applying K bit-functions to the patch, each producing a single bit. Several types of bit functions are discussed in Section 2.3.\nConvolutional table: Each word calculator is applied to all locations in an integration area, and each word extracted casts votes for the C output classes. Convolutional table m is hence a triplet (Bm, Am,Wm) where Am \u2282 {1, .., Sx} \u00d7 {1, ..Sy} is the integration area of word calculator Bm, and Wm \u2208 MC\u00d72K is its weight matrix. Denote by bmp = B(I e N(p); \u0398\nm) the word calculated at location p. We gather a histogram Hm = (Hm0 , ..,H m 2K\u22121) counting word occurrences; i.e.,\nHmb = \u2211 p\u2208Am \u03b4(bmp \u2212 b) (1)\nwith \u03b4 the discrete delta function. The class support of the convolutional table is the C-element vector Wm(Hm)t.\nConvolutional tables ensemble: The ensemble classification is done by accumulating the class support of all convolutional tables into a linear classifier with a bias term. Let H = [H1, ..,HM ] , and W = [W 1, ..WM ] \u2208 RC\u00d7M2K . The classifier\u2019s decision is given by\nC\u2217 = arg max c WHt \u2212 T t (2)\nwhere T = (T 1, ..TC) is a vector of class biases. Algorithm box 1 shows the classifier\u2019s test time flow. Note that the\nAlgorithm 2 Convolutional Tables Ensemble: Training Input: A labeled training set S = {Ii, yi}Ni=1 Parameters M,K,C, {Am}Mm=1 , convex loss L(S;W,T ) Output: A classifier (\u0398m, Am,Wm,c, T c)M,Cm=1,c=1 Initialization: gci = 1/|{Ii|yci = 1}| if yci = 1, gci = \u22121/|{Ii|yci = \u22121}| if yci = \u22121 For m = 1, ..,M\nTable addition: choose \u0398m to optimize: \u0398m = arg max\u0398 \u2211 c \u2211 b\u2208{0,1}K | \u2211 {i,p:bi,p(\u0398)=b} g c i | Update representation: \u2200i = 1, ..N, b \u2208 {0, 1}K Hmb [Ii] = \u2211 p \u03b4(b m p,i = b), H = [H,H\nm] Global optimization: train W,T by solving\narg minW,T L({Hi, yi}Ni=1;W,T ) If m < M get loss gradients: gci = dLdSci\nhistogramsHm are not accumulated in practice, and instead each word computed directly votes for all classes."}, {"heading": "2.2. Training", "text": "In [20, 28] instances of convolutional tables ensemble were discriminatively optimized for specific tasks and losses (hand pose recognition using SVM in [20], and face alignment using l2 regression in [28]). The main idea behind these methods is to iterate between solving a convex problem for a fixed representation, and augmenting the representation based on gradient signals from the obtained solution. Here we adapt these ideas to linear M -classification with an arbitrary l2-regularized convex loss function, using techniques from [26, 2]. Assume a labeled training sample with fixed representation {(Hi, yi)}Ni=1 whereHi \u2208 Rm2 K\n, yi \u2208 {1, .., C}, and denote the c-th row of the weight matrix W by Wc. We want to learn a linear classifier of the form C\u2217 = arg maxc s\nc with sc = WcHt \u2212 T c by minimizing a sample loss function of the form\nL({Hi, yi}Ni=1) = 1\n2 ||W ||2 + N\u2211 i=1 l({sci} C c=1, yi) (3)\nwith l({sc}Cc=1, y) a convex function of sc. L is strictly convex with a single global minimum, hence solvable using known techniques. Once the problem has been solved for the fixed representationHi, we want to extend the representation by incorporating a new table, effectively adding 2K new features. In order to choose the new features wisely, we consider how the loss changes if a new feature f+ is added to the representation with small class weights, regarded as a small perturbation of the existing model.\nDenote by f+i the value of a new feature candidate for example i.After incorporating the new feature, example i\u2019s representation changes from Hi to H+i = [Hi, f + i ] and weights vectors Wc are augmented to [Wc, w+c ] with w+c \u2208 R. Class scores sci are updated to s c,+ i =\nW+c (H + i ) t \u2212 tc = sci +w+c f+. Finally, the loss is changed to L+ = L({H+i , yi} N i=1) = 1 2 ||W || 2 + 12 \u2211C c=1 w + c\n2 +\u2211N\ni=1 l({s c,+ i }\nC c=1, yi). Denote W + = [w+1 , .., w + C ] the\nnew weights vector. We assume that the new feature is added with small weights; i.e., w+c \u2264 for all c. L+ can be Taylor approximated around W+ = 0, with the gradient dL+ dW+ |W+=0:\ndL+\ndW+ \u2223\u2223\u2223\u2223 W+=0 = w+ + N\u2211 i=1 dl({sc,+i }, yi) dsc,+i f+i \u2223\u2223\u2223\u2223 W+=0\n= N\u2211 i=1 dl({sci}, yi) dsci f+i\n(4)\nUsing the gradient in a Taylor approximation ofL+ gives\nL+ = L+W+( dL+\ndW+ )t +O(||W+||2)\n= L+ C\u2211 c=1 w+c N\u2211 i=1 dl({sci}, yi) dsci f+i +O(||W +||2)\n(5) Denote gci = dl({sci},yi) dsci . For loss minimization we\nwant to minimize \u2211C c=1 w + c \u2211N i=1 g c i f + i over W\n+ and f+. For fixed f+ minimizing over W+ is simple. Denoting Rc(f+) = \u2211n i=1 g c i f\n+ i , we have to minimize\u2211C\nc=1 w + c R c(f+) under the constraint w+c \u2264 , \u2200c. We can minimize each term in the sum independently to get w+,copt = \u2212 sign(Rc), and the value of the minimum is \u2212 \u2211C c=1 |Rc(f+)|. Hence, for a single feature addition,\nwe need to maximize the score \u2211C c=1 |Rc(f+)|.\nTo return to our scenario, we add 2K features at once, generated by a new word calculator B. The derivation above can be done for each of them independently, so for the addition of the features {H+b }b\u2208{0,1}K we get\nL+ \u2248 L\u2212 \u2211\nb\u2208{0,1}K \u2212 C\u2211 c=1 |Rc(H+b )|\n= L\u2212 C\u2211 c=1 \u2211 b\u2208{0,1}K | N\u2211 i=1 gci \u2211 p\u2208A+ \u03b4(b+i,p = b)| = L\u2212 C\u2211 c=1 \u2211 b\u2208{0,1}K | \u2211\n{i,p:b+i,p=b}\ngci | \u2206= L\u2212 R(B)\n(6) where we used Equation 1 for H+b and denoted b + i,p = B(Ii,N(p)). The resulting training algorithm, summarized in algorithm box 2, iterates between global classifier optimization and greedy optimization of the next convolutional table by maximizing R(B; \u0398)."}, {"heading": "2.3. Structural variants", "text": "The word calculator concept described in Section 2.1 is very general. Here we describe the bit functions and word calculator types we have explored.\nBit functions and input preparation: Word calculators compute an index descriptor of a patch P \u2208 Rl\u00d7l\u00d7De by applying K bit functions, each producing a single bit. Each such function is composed of a simple comparison operation, with a few parameters stating its exact operation. Specifically we use the following bit function forms:\n\u2022 One pixel: F (P ) = \u03c3(P (x, y, d)\u2212 t)\n\u2022 Two pixels: F (P ) = \u03c3(P (x1, y1, d)\u2212P (x2, y2, d)\u2212 t)\n\u2022 Get Bit l: F (P ) = (P (0, 0, d) << l)&1\n\u2022 Integral channel bit: F (p) = \u03c3(P (x1, y1, d) \u2212 P (x1, y2, d)\u2212 P (x2, y1, d) + P (x2, y2, d)\u2212 t)\nwhere \u03c3 is the Heaviside step function. The first two bit function types can be applied to any input channel d \u2208 {1, .., De}, while the latter two are meaningful only for specific channels. The channels we consider are as follows:\n\u2022 Original image channels: Gray scale and color channels, or depth and IR (multiplied by a depth-based mask) for depth images.\n\u2022 Gradient-based channels: Two kinds of gradient maps are computed from the original channels following [9]. A normalized gradient channel includes the norm of the gradient for each pixel location. In oriented gradient channels the gradient energy of a pixel is softly quantized into NO orientation maps.\n\u2022 Integral channels: Integral images [38] of channels from the previous two forms, again following [9]. Applying integral channel bits to these channels allows fast calculation of channel area sums.\n\u2022 Spatial channels: Two channels stating the horizontal and vertical location of a pixel in the image. These channels state the quantized location, using NH = blog2 Sxc and NV = blog2 Syc bits respectively.\nAfter preparation, the channels are optionally smoothed by a convolution with a triangle filter. Spatial channels enable the incorporation of patches\u2019 position in the word computed. They are used with a \u2019Get Bit l\u2019 bit function type, with l referring to the higher bits. This effectively puts a spatial grid over the image, thus turning the global summation pooling into local summation using a pyramid-like structure [21]. For example using two bit functions, checking for the NH -th horizontal bit and the NV -th vertical bit,\neffectively puts a 2\u00d72 grid over the image where words are summed independently and get different weights for each quarter. Similarly using 4 spatial bits one gets a 4\u00d7 4 pyramid, etc. We found that enforcing a different number of spatial bits in each convolutional table improves feature diversity and consequently the accuracy.\nWord calculator structure: The main design decision in this respect is the choice between ferns and trees. Ferns include only K bit functions, so the number of parameters is relatively small and over-fitting during local optimization is less likely. Trees are a much larger hypothesis family with up to 2K \u2212 1 bit functions in a full tree. Thus they are likely to enable higher accuracy, but also be more prone to overfit. We explored this trade-off using a \u2019long tree\u2019 structure enabling a gradual interplay between the fern and full tree extremes.\nIn a long tree the K bits to compute are divided into NS stages, with Ks bits computed at stage s = 1, .., NS , so\u2211NS s=1Ks = K. A tree of depth N\nS is built, where a node in stage s contains Ks bit functions computing a Ks-bits word. A node in stage s = 1, .., NS \u2212 1 has qs children, and it has a child-directing table of size 2Ks , with entries containing child indices in 1, .., qs. Computation starts at stage 1 at a root node, and after computation of the Ks bits in a node the produced word is used as an index to the child-directing table, whose output is the index of the child node to descend to. The tree structure is determined by the vectors (K1, ..,KNS ) and (q1, .., qNS\u22121) of stage size and stage split factors respectively.\nWhen speed is considered, the most important point is that ferns can be efficiently implemented using vector operations (like SSE), constructing the word in several locations at the same time. The efficiency arises because computing the same bit function for several contiguous patches involves access to contiguous pixels, which can be done without expensive gather operations. Conversely, for trees different bit functions are applied at contiguous patches so the accessed pixels are not contiguous in memory. As will be seen in Section 3, trees can be more accurate, but ferns provide considerably better accuracy-speed trade-off."}, {"heading": "2.4. Training variants", "text": "As stated in algorithm 2, training iterates between gradient based word calculator optimization and global optimization of table weights. We now describe the methods we explored for these two components.\nWord calculator optimization: We consider several mechanisms for the optimization ofR(B; \u0398), including forward bit function selection, optimal threshold finding, and iterative bit function replacement/refinement.\nIn forward selection, we optimize R(B) by adding one bit after the other. For fern growing there are K such stages. At stage l = 1, ..K, {F j}Ncj=1 candidate bit func-\ntions are generated, with their type and parameters drawn from a prior distribution. For each j, we augment the current word calculator B to B+ = [B,F j ] and choose the one with the highest score. However, we found that simple greedy computation of R(B+) at each stage is not the best way to optimize R(B), and an auxiliary score which additively normalizes the newly-introduced features does a better job. Denote the patch features of a word calculator B by \u03b4b(P ) = \u03b4(B(P ) = b), by \u03b4bi,p the value of \u03b4\nb for pixel p in image i and by Rc(f(P ))\u2206= \u2211 i,p g c i fi,p the score R induced by a patch feature f . The addition of a new bit effectively replaces the feature \u03b4b for b \u2208 {0, 2l \u2212 1} with 2 new features \u03b4(b,0) and \u03b4(b,1). If the gradients in cell b are not balanced; i.e., \u2211 i,p g c i \u03b4 b i,p = C0 6= 0, as is often the case, a feature \u03b4(b,0) may get a good Rc(\u03b4(b,0)) score of C0 even if the new bit function is constant, or otherwise uninformative. To handle this, we score a normalized version of the new features, with an average value of 0, which more effectively measures the added information in the new features. The following lemma shows that this is a valid, as well as computationally effective strategy:\nLemma 2.1 Let \u03b4\u0304(b,u) = \u03b4(b,u) \u2212 #\u03b4 (b,u)\n#\u03b4b \u03b4b for u = 0, 1 and #\u03b4a = \u2211 i,p \u03b4 a i,p. The following properties hold\n1. Using \u03b4\u0304(b,1), \u03b4b in a classifier is equivalent to using \u03b4(b,0),\u03b4(b,1); i.e, for any weight choice w0, w1 there are wb, w\u2206 such that w0\u03b4(b,0) + w1\u03b4(b,1) = wb\u03b4b + w\u2206\u03b4\u0304 (b,1)\n2. Rc(\u03b4\u0304(b,0)) = Rc(\u03b4\u0304(b,1)) 3. Rc(\u03b4\u0304(b,0)) = \u2211 i,p(g c i \u2212 E[gci |b])\u03b4(b,0) with\nE[gci |b] \u2206=\n\u2211 i,p g c i \u03b4 b\n#\u03b4b\nThe proofs are rather simple and appear in appendix 5. Property 1 shows that we may score \u03b4b, \u03b4\u0304(b,1) features instead of \u03b4(b,u) features. Since only \u03b4\u0304(b,1) is affected by the new candidate bit, we can score only those terms when selecting among candidates. Property 3 shows that we can normalize the gradient instead of the feature candidates, which is cheaper (as there are Nc candidates but only a single gradient vector). In summary, we optimize the next bit selection by maximizing\nR\u2206([B,F j ])\u2206= C\u2211 c=1 \u2211 b\u2208{0,1}l | \u2211 {i,p:bi,p=(b,1)} (gci \u2212 E[gci |b])|\n(7) over the choice of F j . The calculation requires a single histogram aggregation sweep over all patches (i, p). Most of the bit functions obtain their bit by comparing an underlying patch measurement to a threshold t. For such functions, the optimal threshold parameter can be found\nwith a small additional computational cost. This is done by sorting the underlying values of F j and computing the sum over i, p in Equation 7 by running with the sorted order. This way, a running statistic of the R\u2206 score can be maintained, computing the score for all possible thresholds and keeping the best.\nFor a long tree a similar algorithm is employed, with ferns internal to nodes optimized as full ferns, but tree splits requiring special treatment. Assume we are splitting a node in stage s, so the current word calculator has already computes a Ls = \u2211s i=1Ki-bit word, among which Ks were computed in the current node. We now choose the first bit functions of all the children, as well as the redirection table, to optimize R. Since different prefixes of the current calculator B are augmented by different bit functions we need to decompose the R score. Denote by a the index set {Ls\u2212Ks + 1, .., Ls} of bits computed by the current node, and by b(a) the limitation of a binary word b to indices a. For a Ks-bit word z \u2208 {0, 1}Ks , we define the component of R contributed by words with b(a) = z by\nRb(a)=z(B) \u2206= C\u2211 c=1 \u2211 b\u2208{0,1}Ls b(a)=z | \u2211 {i,p:bi,p=b} gci | (8)\nFor the tree split we draw a large set F of candidate bits, and choose the first bits of the qs children by optimizing\nmax G\u2282F |G|=qs \u2211 z\u2208{0,1}Ks max Fz\u2208G Rb(a)=z([B,Fz]) (9)\nwith G the set of chosen bits for the children and entry z in the redirection table set to the index of the child containing Fz . For this optimization we compute the score matrix S \u2208 M2Ks\u00d7|F| with S(i, j) = Rb(a)=i([B,Fj ]). Given a choice of G, amounting to a choice of column subset in S, the optimization over Fz is trivial and the score is easy to compute. We optimize over G by exhaustively trying all choices of G for |G| = 2, and greedily adding columns to G until it contains qs members.\nIn addition to forward bit selection, we implemented iterative bit replacement and refinement stages. The rationale\nfor this is the observation that while the last bit functions in a fern are chosen to complement the previous ones, the bits chosen at the beginning are not optimized to be complementary and may be suboptimal in a long word calculator. The bit replacement algorithm operates after forward bit selection. It runs over all the bit functions several times and attempts to replace each function with several randomly drawn candidates. A replacement step is accepted if it improves the R\u2206(B) score. In a similar manner, a bit refinement algorithm attempts to replace a bit function by small perturbations of its parameters, thus effectively implementing a local search. For trees, bit replacement/refinement is done only for bits inside a node, and once a split is made the node parameters are fixed.\nGlobal optimization: We considered two global loss functions in our classification experiments: an SVM-based loss, and a softmaxloss as typically used in neural networks optimization. In the SVM loss, we take the sum of C SVM programs, each minimizing a one-versus-all error. Let yi,c = 2 \u2217 \u03b4(yi, c)\u2212 1 be binary class labels. The loss is\nLSVM = 1\n2 ||W ||2 + \u039b C\u2211 c=1 N\u2211 i=1 max(1\u2212 yi,csci , 0) (10)\nThe loss aims for class separation in C independent classifiers. Its advantage lies in the availability of fast and scalable methods for solving large and sparse SVM programs [30, 16]. The loss gradients are gci = \u2212yi,c if example i is a support vector, and 0 otherwise. In [3] a first order approximation for minW LSVM is derived for new feature addition, in which the example gradients are \u2212\u03b1iyi,c with \u03b1i the dual SVM variables at the optimum. The two expressions are similar and we did not find noticeable difference between them empirically. The softmax loss is\nLLR = 1\n2 ||W ||2 \u2212 \u039b N\u2211 i=1 log exp(syii )\u2211 c exp(s c i )\n(11)\nThis loss provides a direct minimization of the M -class error. The gradients are gci = exp(s yi i )/ \u2211 c exp(s c i ) \u2212 \u03b4(yi, c). Conveniently, it can be extended to a distillation loss [15], which enables guidance of the classifier using an internal representation of a well-trained CNN classifier.\nFeatures in a word histogram have significant variance, as some words appear in large quantities in a single image. Without normalization such words may be arbitrarily preferred due to their lower regularization cost- they can be used with lower weights. Denote the column of a feature across all examples by Colmb = (H m b,1, ..H m b,N ). We found that normalizing each features column by the expected count of active examples L1(Colmb )/L0(Col m b ) improved accuracy and convergence speed in many cases."}, {"heading": "3. Empirical results", "text": "We discuss our experimental setup in 3.1. In Section 3.2 we compare to related art and evaluate the contribution of algorithmic components to the performance. Results of speed-accuracy trade-offs are presented in 3.3."}, {"heading": "3.1. Implementation and data details", "text": "The experiments were conducted on 4 publicly available datasets: MNIST, CIFAR-10, SVHN and 3-HANDPOSE. The first three are standard recognition benchmarks in grayscale (MNIST) or RGB (CIFAR-10,SVHN), with 10 classes each. 3-HANDPOSE are a 4-class dataset, with 3 hand poses and a fourth class of \u2019other\u2019, and its images contain depth and IR channels. The image sizes are between 28\u00d728 (MNIST) and 36 \u00d7 36 (3-HANDPOSE). The training set size ranges from 50000 (CIFAR-10) to 604000 (SVHN).\nCTE training code was written in Matlab, with some routines using code from the packages [8, 11, 37]. The test time classier was implemented and optimized in C. For ferns we implemented algorithm 1 with SSE operations. Words are computed for 8 neighboring pixels together, and voting is done for 8 classes at once. For trees we implemented a program generating efficient code of the bit computation loop for a specific tree, so the tree parameters are part of the code. This obtained an acceleration factor of 2X over standard C code. We also thread-parallelized the algorithm over the convolutional tables, with good a speed-up of 3.6\u00d7 obtained from 4 cores. However, we report and compare single thread performance to keep the methodology as simple as possible.\nCNN models were trained using MatConvNet [37]. The\nimplementation is efficient, reported to be comparable to Caffe [39] in [37], with the convolutional and global layers reduced to matrix multiplication done using an SSEoptimized BLAS package. When measuring execution time, we measured net run time of the convolutional, pooling and global layers alone, without Matlab overhead. Time measurements were made on a Lenovo Thinkpad W530 quad core laptop, with i7-3720QM core running at 2.6Ghz."}, {"heading": "3.2. Comparison and variation", "text": "Comparison with DFE: The Discriminative Ferns Ensemble (DFE) was suggested in [20] for classification of 3-HANDPOSE, and can be seen a baseline for CTE, which enhances it in many aspects. The first two columns in Table 1 present errors of DFE and CTE on the 4 datasets, using 50 ferns for MNIST, SVHN, 3-HANDPOSE and 100 for CIFAR-10. MNIST was trained with softmax distillation loss (see below for details), and the others with SVM loss. The aggregation area {Am}Mm=1 were chosen to be identical for all tables in a classifier, forming a centered square occupying most of the image. To enable the comparison, M-class error rates are extracted from DFE (in [20] such errors are not reported, and 3-class average true positive rates are reported instead). It can be seen that CTE base provides significant improvements of 24\u2212 45% error reduction over DFE, with 28% obtained for 3-HANDPOSE, where DFE was originally applied. Note that the CTE base is not the best choice for 3-HANDPOSE. With additional parameter tuning result of 2% can be obtained with 50 ferns, which is an improvement of 38% over DFE.\nAblation experiments: The accuracy obtained by a CTE is influenced by many small incremental improvements related to structural and algorithmic variations. Columns 3-9 in Table 1 show the contribution of some ingredients by removing them from the baseline CTE. For MNIST, where the effects are small due to the low error, results were averaged over 5 experiments varying in their random seed, a with seed-induced std of 0.1%. It can be seen that these ingredients consistently contribute to accuracy for non-depth data.\nTrees/Ferns trade-off: The trade-off between ferns\nand trees for MNIST and CIFAR-10 is presented in Figure 2(Right). For MNIST, the results were averaged over 5 experiments, with a seed induced std of 0.028%. It can be seen that trees provide better accuracy. However, the speed cost of using trees is significant, due to the inability to efficiently vectorize their implementation.\nDistillation experiments: We experimented with knowledge distillation from a CNN to a CTE using the method suggested in [15]. In such experiments, soft labels are taken from our best CNN model, and a CTE is trained to optimize a convex combination of the standard softmax loss and the Kullback-Leibler distance from the CNN-induced probabilities. We attempted this for MNIST and CIFAR10 using our best CNN models, providing 0.31% and 8.6% error respectively as distillation sources. For MNIST, this training methodology proved to be successful. Averaging over 5 seeds, the accuracy of a 50-fern CTE optimized for softmax was 0.66%(the std was 0.025%) without distillation, and 0.45%(0.029%) with distillation. For comparison, an SVM-optimized CTE with the same parameters obtained 0.61%(0.04%) error. For CIFAR-10 distillation did not consistently improve the results."}, {"heading": "3.3. Speed-Accuracy trade-off", "text": "We are interested in the trade-off or Pareto curves [10], showing the best accuracy obtainable for a specific speed constraint and vice versa. Since the design space for variations ofCNNs andCTE algorithms is huge, and the training time of the algorithms is considerable, we needed to sample it wisely to get a good curve approximation. Our sampling technique is based on two stages. In stage 1 we searched for the most accurate classifiers for CTE and CNN with loose speed constraints, so even slow classifiers were considered. We then used the few top accuracy variants of each architecture as baselines and accelerated them by systematically varying certain design parameters.\nOur CNN baseline architectures are variations of DeepCNiN(l,k) [13], with l = 3 \u2212 4 convolutional layers and k = 60\u2212 100, implying usage of i \u00b7k maps at the i-th layer. It was shown in [13] that higher l, k values provide better accuracy, but such architectures are much slower than 1 CPU\nmillisecond and so they are outside our domain of interest. We experimented with dropout [33], parametric RELU units [18], affine image transformations following [13], and HSV image transformations following [32]. Acceleration of the baseline architectures used three main parameters. The first was reducing parameter k controlling the network width. The second was reduction of the number of maps in the output of the NIN layers. This reduces the number of input maps for the next layer, and can dramatically save computation with relatively small loss of accuracy. The third was raising the convolution stride parameter from 1 to 2. For CTEs, our exploration space was sketched in Section 2, and it includes both ferns and trees. The best performing configurations were then accelerated using a single parameter: the number of tables in the ensemble.\nTrade-off graphs for the 4 datasets are shown in Figure 3. Classification speed in microseconds is displayed along the X-axis in log scale with base 10. For all datasets, there is a high speed regime where CTEs provide better accuracy than CNNs. Specifically CTEs are preferable for all datasets when less than 100 microseconds are available for computation. Starting from 1000 microseconds and up CNNs are usually better, with CTEs still providing comparable accuracy for MNIST and 3-HANDPOSE at the 1\u2212 10 milliseconds regime. Viewed conversely, for a wide range of error rates, if the error rate is obtainable by a CTE, it is obtainable with significant speedups over CNNs. Some examples of this phenomenon are given in Figure 2(Left). Note that while a working point of 0.25 error for CIFAR-10 may seem high, the majority of the one-versus-one errors of such a classifier are lower than 0.05, which may be good enough for many purposes."}, {"heading": "4. Conclusions and further work", "text": "We introduced improvements to the convolutional tables framework in terms of bit functions used, word calculator structure, calculator optimization and global optimization. We have shown that for highly computational constrained tasks CTE may provide accuracy higher than CNNs. A natural direction for future research is to replace the flat structure of CTEs with a layered approach, in order to try and\nenjoy the accuracy of CNNs with the speed of CTEs."}, {"heading": "5. Appendix", "text": "Here we prove the statements of lemma 2.1 from Section 2.4.\nProof 1. From the definition \u03b4\u0304(b,1) = \u03b4(b,1) \u2212 #\u03b4(b,1) #\u03b4b \u03b4b,\nso \u03b4(b,1) = #\u03b4 (b,1)\n#\u03b4b \u03b4b+ \u03b4\u0304(b,1). Also, since \u03b4b = \u03b4(b,0) +\n\u03b4(b,1), we have\n\u03b4(b,0) = \u03b4b \u2212 \u03b4(b,1)\n= \u03b4b \u2212 (#\u03b4 (b,1)\n#\u03b4b \u03b4b + \u03b4\u0304(b,1))\n= \u03b4b(1\u2212 #\u03b4 (b,1)\n#\u03b4b )\u2212 \u03b4\u0304(b,1)\n= \u03b4b #\u03b4(b,0)\n#\u03b4b \u2212 \u03b4\u0304(b,1)\nHence, for weights w0, w1,\nw0\u03b4 (b,0) + w1\u03b4 (b,1)\n= w0( #\u03b4(b,0)\n#\u03b4b \u03b4b \u2212 \u03b4\u0304(b,1))\n+ w1( #\u03b4(b,1)\n#\u03b4b \u03b4b + \u03b4\u0304(b,1))\n= (w0 + w1)\u03b4 b + (w1 \u2212 w0)\u03b4\u0304(b,1)\nSo wb = (w0 + w1) and w\u2206 = w1 \u2212 w0 fulfill the lemma\u2019s statement.\n2.\nRc(\u03b4\u0304(b,1)) = | \u2211 i,p gci \u03b4\u0304 (b,1) i,p |\n= | \u2211 i,p gci \u03b4 (b,1) i,p \u2212 #\u03b4(b,1) #\u03b4b \u2211 i,p gci \u03b4 b i,p|\nUsing \u03b4(b,0) = \u03b4b \u2212 \u03b4(b,1) we continue to\n= | \u2211 i,p gci (\u03b4 b i,p \u2212 \u03b4 (b,0) i,p )\u2212 #\u03b4b \u2212#\u03b4(b,0) #\u03b4b \u2211 i,p gci \u03b4 b i,p|\n= | \u2212 \u2211 i,p gci \u03b4 (b,0) i,p + #\u03b4(b,0) #\u03b4b \u2211 i,p gci \u03b4 b i,p|\n= Rc(\u03b4\u0304(b,0))\n3.\n| \u2211 i,p (gci \u2212 E[gci |b])\u03b4 (b,1) i,p |\n= | \u2211 i,p (gci \u2212 \u2211 i\u2032,p\u2032 g c i\u2032\u03b4 b i\u2032,p\u2032 #\u03b4b )\u03b4 (b,1) i,p |\n= | \u2211 i,p gci \u03b4 (b,1) i,p \u2212 #\u03b4(b,1) #\u03b4b \u2211 i,p gci \u03b4 b i,p|\n= Rc(\u03b4\u0304(b,1))"}], "references": [{"title": "Sparse convolutional neural networks", "author": ["L. Baoyuan", "W. Min", "F. Hassan", "T. Marshall", "P. Marianna"], "venue": "CVPR,", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2015}, {"title": "Object class recognition by boosting a part based model", "author": ["A. Bar-Hillel", "T. Hertz", "D. Weinshall"], "venue": "CVPR,", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2005}, {"title": "Partbased feature synthesis for human detection", "author": ["A. Bar-Hillel", "D. Levi", "E. Krupka", "C. Goldberg"], "venue": "ECCV,", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2010}, {"title": "Pedestrian detection at 100 frames per second", "author": ["R. Benenson", "M. Mathias", "R. Timofte", "L.J.V. Gool"], "venue": "CVPR,", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2012}, {"title": "Image classification using random forests and ferns", "author": ["A. Bosch", "A. Zisserman", "X. Mu\u00f1oz"], "venue": "ICCV, pages 1\u20138,", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2007}, {"title": "Decision forests for classification, regression, density estimation, manifold learning and semi-supervised learning", "author": ["A. Criminisi", "J. Shotton", "E. Konukoglu"], "venue": "Technical report, Microsoft Research,", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2011}, {"title": "Fast, accurate detection of 100,000 object classes on a single machine", "author": ["T. Dean", "M. Ruzon", "M. Segal", "J. Shlens", "S. Vijayanarasimhan", "J. Yagnik"], "venue": "Proceedings of IEEE Conference on Computer Vision and Pattern Recognition, Washington, DC, USA,", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2013}, {"title": "Integral channel features", "author": ["P. Dollar", "Z. Tu", "P. Perona", "S. Belongie"], "venue": "BMVC,", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2009}, {"title": "Liblinear: A library for large linear classification", "author": ["R.-E. Fan", "K.-W. Chang", "C.-J. Hsieh", "X.-R. Wang", "C.- J. Lin"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2008}, {"title": "Extremely randomized trees", "author": ["P. Geurts", "D. Ernst", "L. Wehenkel"], "venue": "Mach. Learn., 63(1):3\u201342, Apr.", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2006}, {"title": "Spatially-sparse convolutional neural networks", "author": ["B. Graham"], "venue": "CoRR, abs/1409.6070,", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2014}, {"title": "Spatial pyramid pooling in deep convolutional networks for visual recognition", "author": ["K. He", "X.S. Zhang", "Ren", "J. Sun"], "venue": "CoRR, abs/1406.4729v2,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2014}, {"title": "Distilling the knowledge in a neural network", "author": ["G. Hinton", "O. Vinyals", "J. Dean"], "venue": "CoRR, abs/1503.02531,", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2015}, {"title": "A dual coordinate descent method for largescale linear svm", "author": ["C.-J. Hsieh", "K.-W. Chang", "C.-J. Lin", "S.S. Keerthi", "S. Sundararajan"], "venue": "ICML,", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2008}, {"title": "Do deep nets really need to be deep", "author": ["B. Jimmy", "C. Rich"], "venue": "In NIPS,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2014}, {"title": "Delving deep into rectifiers: Surpassing human-level performance on imagenet classifcation", "author": ["H. Kaiming", "Z. Xiangyu", "R. Shaoqing", "S. Jian"], "venue": "CoRR, abs/1502.01852,", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2015}, {"title": "Learning multiple layers of features from tiny images", "author": ["A. Krizhevsky", "G. Hinton"], "venue": "Technical report, Masters thesis, Department of Computer Science, University of Toronto,", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2009}, {"title": "Discriminative ferns ensemble for hand pose recognition", "author": ["E. Krupka", "A. Vinnikov", "B. Klein", "A.B. Hillel", "D. Freedman", "S. Stachniak"], "venue": "CVPR,", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2014}, {"title": "Beyond bags of features: Spatial pyramid matching for recognizing natural scene categories", "author": ["S. Lazebnik", "C. Schmid", "J. Ponce"], "venue": "CVPR, pages 2169\u20132178,", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2006}, {"title": "Gradientbased learning applied to document recognition", "author": ["Y. LeCun", "L. Bottou", "Y. Bengio", "P. Haffner"], "venue": "Proceedings of the IEEE,", "citeRegEx": "22", "shortCiteRegEx": null, "year": 1998}, {"title": "Keypoint recognition using randomized trees", "author": ["V. Lepetit", "P. Fua"], "venue": "PAMI, 28:1465\u20131479,", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2008}, {"title": "Fast multiple-part based object detection using kd-ferns", "author": ["D. Levi", "S. Silberstein", "A. Bar-Hillel"], "venue": "CVPR,", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2013}, {"title": "Network in network", "author": ["M. Lin", "Q. Chen", "S. Yan"], "venue": "CoRR, abs/1312.4400,", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2014}, {"title": "Boosting algorithms as gradient descent", "author": ["L. Mason", "J. Baxter", "P. Bartlett", "M. Frean"], "venue": "NIPS, pages 512\u2013518,", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2000}, {"title": "Reading digits in natural images with unsupervised feature learning", "author": ["Y. Netzer", "T. Wang", "A. Coates", "A. Bissacco", "B. Wu", "A.Y. Ng"], "venue": "In NIPS Workshop on Deep Learning and Unsupervised Feature Learning,", "citeRegEx": "27", "shortCiteRegEx": "27", "year": 2011}, {"title": "Face alignment at 3000 fps via regressing local binary features", "author": ["S. Ren", "X. Cao", "Y. Wei", "J. Sun"], "venue": "CVPR, June", "citeRegEx": "28", "shortCiteRegEx": null, "year": 2014}, {"title": "Fitnets: Hints for thin deep nets", "author": ["A. Romero", "N. Ballas", "S.E. Kahou", "A. Chassang", "C. Gatta", "Y. Bengio"], "venue": "CoRR, abs/1412.6550,", "citeRegEx": "29", "shortCiteRegEx": null, "year": 2015}, {"title": "Pegasos: Primal estimated sub-gradient solver for svm", "author": ["S. Shalev-Shwartz", "Y. Singer", "N. Srebro"], "venue": "ICML,", "citeRegEx": "30", "shortCiteRegEx": null, "year": 2007}, {"title": "Real-time human pose recognition in parts from single depth images", "author": ["J. Shotton", "T. Sharp", "A. Kipman", "A.W. Fitzgibbon", "M. Finocchio", "A. Blake", "M. Cook", "R. Moore"], "venue": "Commun. ACM, 56(1):116\u2013124,", "citeRegEx": "31", "shortCiteRegEx": null, "year": 2013}, {"title": "Scalable bayesian optimization using deep neural networks", "author": ["J. Snoek", "O. Rippel", "K. Swersky", "R. Kiros", "N. Satish", "N. Sundaram", "M.A. Patwary", "Prabhat", "R.P. Adams"], "venue": "CoRR, abs/1502.05700,", "citeRegEx": "32", "shortCiteRegEx": "32", "year": 2014}, {"title": "Dropout: A simple way to prevent neural networks from overfitting", "author": ["N. Srivastava", "G. Hinton", "A. Krizhevsky", "I. Sutskever", "R. Salakhutdinov"], "venue": "J. Mach. Learn. Res., 15(1):1929\u2013 1958,", "citeRegEx": "33", "shortCiteRegEx": null, "year": 2014}, {"title": "A Fast Local Descriptor for Dense Matching", "author": ["E. Tola", "V.Lepetit", "P. Fua"], "venue": "In CVPR,", "citeRegEx": "34", "shortCiteRegEx": "34", "year": 2008}, {"title": "Speeding-up convolutional neural networks using fine-tuned cp-decomposition", "author": ["L. Vadim", "G. Yaroslav", "R. Maksim", "O. Ivan", "L. Victor"], "venue": "CoRR, abs/1412.6553,", "citeRegEx": "35", "shortCiteRegEx": null, "year": 2014}, {"title": "Improving the speed of neural networks on cpus", "author": ["V. Vanhoucke", "A. Senior", "M.Z. Mao"], "venue": "Deep Learning and Unsupervised Feature Learning Workshop, NIPS 2011,", "citeRegEx": "36", "shortCiteRegEx": null, "year": 2011}, {"title": "Matconvnet \u2013 convolutional neural networks for matlab", "author": ["A. Vedaldi", "K. Lenc"], "venue": "Proceeding of the ACM Int. Conf. on Multimedia,", "citeRegEx": "37", "shortCiteRegEx": null, "year": 2015}, {"title": "Rapid object detection using a boosted cascade of simple features", "author": ["P. Viola", "M. Jones"], "venue": "CVPR,", "citeRegEx": "38", "shortCiteRegEx": null, "year": 2001}, {"title": "Caffe: Convolutional architecture for fast feature embedding", "author": ["J. Yangqing", "S. Evan", "D. Jeff", "K. Sergey", "L. Jonathan", "G. Ross", "G. Sergio", "D. Trevor"], "venue": "Proceedings of the ACM International Conference on Multimedia,", "citeRegEx": "39", "shortCiteRegEx": null, "year": 2014}], "referenceMentions": [{"referenceID": 11, "context": "The accuracy-speed trade-off has thus been widely discussed in the literature, and various architectures have been suggested [14, 35, 7, 4, 24, 31].", "startOffset": 125, "endOffset": 147}, {"referenceID": 32, "context": "The accuracy-speed trade-off has thus been widely discussed in the literature, and various architectures have been suggested [14, 35, 7, 4, 24, 31].", "startOffset": 125, "endOffset": 147}, {"referenceID": 6, "context": "The accuracy-speed trade-off has thus been widely discussed in the literature, and various architectures have been suggested [14, 35, 7, 4, 24, 31].", "startOffset": 125, "endOffset": 147}, {"referenceID": 3, "context": "The accuracy-speed trade-off has thus been widely discussed in the literature, and various architectures have been suggested [14, 35, 7, 4, 24, 31].", "startOffset": 125, "endOffset": 147}, {"referenceID": 21, "context": "The accuracy-speed trade-off has thus been widely discussed in the literature, and various architectures have been suggested [14, 35, 7, 4, 24, 31].", "startOffset": 125, "endOffset": 147}, {"referenceID": 28, "context": "The accuracy-speed trade-off has thus been widely discussed in the literature, and various architectures have been suggested [14, 35, 7, 4, 24, 31].", "startOffset": 125, "endOffset": 147}, {"referenceID": 19, "context": "This is analogous to a convolutional layer in a convolutional neural network (CNN) [22], where the same set of filters is applied at each image position.", "startOffset": 83, "endOffset": 87}, {"referenceID": 17, "context": "Variants of this architecture have been used successfully mainly for classification of depth images [20, 31].", "startOffset": 100, "endOffset": 108}, {"referenceID": 28, "context": "Variants of this architecture have been used successfully mainly for classification of depth images [20, 31].", "startOffset": 100, "endOffset": 108}, {"referenceID": 17, "context": "We start with the simple functions employed in [20, 31], which were suitable for depth images, and extend them using gradient and color based channels and features employed in [9].", "startOffset": 47, "endOffset": 55}, {"referenceID": 28, "context": "We start with the simple functions employed in [20, 31], which were suitable for depth images, and extend them using gradient and color based channels and features employed in [9].", "startOffset": 47, "endOffset": 55}, {"referenceID": 7, "context": "We start with the simple functions employed in [20, 31], which were suitable for depth images, and extend them using gradient and color based channels and features employed in [9].", "startOffset": 176, "endOffset": 179}, {"referenceID": 28, "context": "A second important choice is between conditional computation of bit functions, leading to tree structures like used in [31], and unconditional computation as in fern structures [20].", "startOffset": 119, "endOffset": 123}, {"referenceID": 17, "context": "A second important choice is between conditional computation of bit functions, leading to tree structures like used in [31], and unconditional computation as in fern structures [20].", "startOffset": 177, "endOffset": 181}, {"referenceID": 9, "context": "Several works have addressed the challenges of learning a tables-based classifier [12, 5, 34, 23, 6, 20, 28].", "startOffset": 82, "endOffset": 108}, {"referenceID": 4, "context": "Several works have addressed the challenges of learning a tables-based classifier [12, 5, 34, 23, 6, 20, 28].", "startOffset": 82, "endOffset": 108}, {"referenceID": 31, "context": "Several works have addressed the challenges of learning a tables-based classifier [12, 5, 34, 23, 6, 20, 28].", "startOffset": 82, "endOffset": 108}, {"referenceID": 20, "context": "Several works have addressed the challenges of learning a tables-based classifier [12, 5, 34, 23, 6, 20, 28].", "startOffset": 82, "endOffset": 108}, {"referenceID": 5, "context": "Several works have addressed the challenges of learning a tables-based classifier [12, 5, 34, 23, 6, 20, 28].", "startOffset": 82, "endOffset": 108}, {"referenceID": 17, "context": "Several works have addressed the challenges of learning a tables-based classifier [12, 5, 34, 23, 6, 20, 28].", "startOffset": 82, "endOffset": 108}, {"referenceID": 25, "context": "Several works have addressed the challenges of learning a tables-based classifier [12, 5, 34, 23, 6, 20, 28].", "startOffset": 82, "endOffset": 108}, {"referenceID": 9, "context": "These vary in optimization effort from extremely random forests [12] to global optimization of table weights and greedy forward choice of bit functions [28, 20].", "startOffset": 64, "endOffset": 68}, {"referenceID": 25, "context": "These vary in optimization effort from extremely random forests [12] to global optimization of table weights and greedy forward choice of bit functions [28, 20].", "startOffset": 152, "endOffset": 160}, {"referenceID": 17, "context": "These vary in optimization effort from extremely random forests [12] to global optimization of table weights and greedy forward choice of bit functions [28, 20].", "startOffset": 152, "endOffset": 160}, {"referenceID": 17, "context": "Our approach builds on previous approaches, mostly [20], and extends them with new possibilities.", "startOffset": 51, "endOffset": 55}, {"referenceID": 23, "context": "We learn the table ensemble by adding one table at a time, using a framework similar to the \u2019anyboost\u2019 algorithm [26, 2].", "startOffset": 113, "endOffset": 120}, {"referenceID": 1, "context": "We learn the table ensemble by adding one table at a time, using a framework similar to the \u2019anyboost\u2019 algorithm [26, 2].", "startOffset": 113, "endOffset": 120}, {"referenceID": 17, "context": "For the global optimization we used two main options: an SVM loss as used in [20] and a softmax loss as commonly used in CNN training.", "startOffset": 77, "endOffset": 81}, {"referenceID": 14, "context": "In several recent studies [17, 15, 29], the output of an accurate but computationally expensive classifier is used to train another classifier, with a different and often computationally cheaper architecture.", "startOffset": 26, "endOffset": 38}, {"referenceID": 12, "context": "In several recent studies [17, 15, 29], the output of an accurate but computationally expensive classifier is used to train another classifier, with a different and often computationally cheaper architecture.", "startOffset": 26, "endOffset": 38}, {"referenceID": 26, "context": "In several recent studies [17, 15, 29], the output of an accurate but computationally expensive classifier is used to train another classifier, with a different and often computationally cheaper architecture.", "startOffset": 26, "endOffset": 38}, {"referenceID": 12, "context": "We made a preliminary attempt to use this technique, termed distillation in [15], to train a CTE classifier with a CNN teacher, with encouraging results on the MNIST data.", "startOffset": 76, "endOffset": 80}, {"referenceID": 17, "context": "2 we present experiments demonstrating the performance gains of our techniques by comparison with the DFE method of [20], ablation studies, fern-tree trade-off experiments, and distillation results.", "startOffset": 116, "endOffset": 120}, {"referenceID": 19, "context": "We use several publicly available object recognition benchmarks: MNIST [22], CIFAR-10 [19], SVHN [27] and 3-HANDPOSE [20].", "startOffset": 71, "endOffset": 75}, {"referenceID": 16, "context": "We use several publicly available object recognition benchmarks: MNIST [22], CIFAR-10 [19], SVHN [27] and 3-HANDPOSE [20].", "startOffset": 86, "endOffset": 90}, {"referenceID": 24, "context": "We use several publicly available object recognition benchmarks: MNIST [22], CIFAR-10 [19], SVHN [27] and 3-HANDPOSE [20].", "startOffset": 97, "endOffset": 101}, {"referenceID": 17, "context": "We use several publicly available object recognition benchmarks: MNIST [22], CIFAR-10 [19], SVHN [27] and 3-HANDPOSE [20].", "startOffset": 117, "endOffset": 121}, {"referenceID": 17, "context": "CTE achieves error improvements of 24 \u2212 45% over [20], with 38% improvement on 3-HANDPOSE, the original data used in [20].", "startOffset": 49, "endOffset": 53}, {"referenceID": 17, "context": "CTE achieves error improvements of 24 \u2212 45% over [20], with 38% improvement on 3-HANDPOSE, the original data used in [20].", "startOffset": 117, "endOffset": 121}, {"referenceID": 22, "context": "For the latter we trained NIN networks [25], combining state-of-the-art accuracy with significant speed advantages, and further accelerated them by scaling parameters of breadth, NIN output dimension and convolution stride.", "startOffset": 39, "endOffset": 43}, {"referenceID": 32, "context": "Alternatives to CTE may be provided by the literature dealing with CNN acceleration [35, 36, 1].", "startOffset": 84, "endOffset": 95}, {"referenceID": 33, "context": "Alternatives to CTE may be provided by the literature dealing with CNN acceleration [35, 36, 1].", "startOffset": 84, "endOffset": 95}, {"referenceID": 0, "context": "Alternatives to CTE may be provided by the literature dealing with CNN acceleration [35, 36, 1].", "startOffset": 84, "endOffset": 95}, {"referenceID": 17, "context": "In [20, 28] instances of convolutional tables ensemble were discriminatively optimized for specific tasks and losses (hand pose recognition using SVM in [20], and face alignment using l regression in [28]).", "startOffset": 3, "endOffset": 11}, {"referenceID": 25, "context": "In [20, 28] instances of convolutional tables ensemble were discriminatively optimized for specific tasks and losses (hand pose recognition using SVM in [20], and face alignment using l regression in [28]).", "startOffset": 3, "endOffset": 11}, {"referenceID": 17, "context": "In [20, 28] instances of convolutional tables ensemble were discriminatively optimized for specific tasks and losses (hand pose recognition using SVM in [20], and face alignment using l regression in [28]).", "startOffset": 153, "endOffset": 157}, {"referenceID": 25, "context": "In [20, 28] instances of convolutional tables ensemble were discriminatively optimized for specific tasks and losses (hand pose recognition using SVM in [20], and face alignment using l regression in [28]).", "startOffset": 200, "endOffset": 204}, {"referenceID": 23, "context": "Here we adapt these ideas to linear M -classification with an arbitrary l2-regularized convex loss function, using techniques from [26, 2].", "startOffset": 131, "endOffset": 138}, {"referenceID": 1, "context": "Here we adapt these ideas to linear M -classification with an arbitrary l2-regularized convex loss function, using techniques from [26, 2].", "startOffset": 131, "endOffset": 138}, {"referenceID": 7, "context": "\u2022 Gradient-based channels: Two kinds of gradient maps are computed from the original channels following [9].", "startOffset": 104, "endOffset": 107}, {"referenceID": 35, "context": "\u2022 Integral channels: Integral images [38] of channels from the previous two forms, again following [9].", "startOffset": 37, "endOffset": 41}, {"referenceID": 7, "context": "\u2022 Integral channels: Integral images [38] of channels from the previous two forms, again following [9].", "startOffset": 99, "endOffset": 102}, {"referenceID": 18, "context": "This effectively puts a spatial grid over the image, thus turning the global summation pooling into local summation using a pyramid-like structure [21].", "startOffset": 147, "endOffset": 151}, {"referenceID": 17, "context": "Comparison and ablation: Columns one and two present errors of a DFE [20] and a baseline CTE.", "startOffset": 69, "endOffset": 73}, {"referenceID": 27, "context": "Its advantage lies in the availability of fast and scalable methods for solving large and sparse SVM programs [30, 16].", "startOffset": 110, "endOffset": 118}, {"referenceID": 13, "context": "Its advantage lies in the availability of fast and scalable methods for solving large and sparse SVM programs [30, 16].", "startOffset": 110, "endOffset": 118}, {"referenceID": 2, "context": "In [3] a first order approximation for minW LSVM is derived for new feature addition, in which the example gradients are \u2212\u03b1iyi,c with \u03b1i the dual SVM variables at the optimum.", "startOffset": 3, "endOffset": 6}, {"referenceID": 12, "context": "Conveniently, it can be extended to a distillation loss [15], which enables guidance of the classifier using an internal representation of a well-trained CNN classifier.", "startOffset": 56, "endOffset": 60}, {"referenceID": 8, "context": "CTE training code was written in Matlab, with some routines using code from the packages [8, 11, 37].", "startOffset": 89, "endOffset": 100}, {"referenceID": 34, "context": "CTE training code was written in Matlab, with some routines using code from the packages [8, 11, 37].", "startOffset": 89, "endOffset": 100}, {"referenceID": 34, "context": "CNN models were trained using MatConvNet [37].", "startOffset": 41, "endOffset": 45}, {"referenceID": 36, "context": "The implementation is efficient, reported to be comparable to Caffe [39] in [37], with the convolutional and global layers reduced to matrix multiplication done using an SSEoptimized BLAS package.", "startOffset": 68, "endOffset": 72}, {"referenceID": 34, "context": "The implementation is efficient, reported to be comparable to Caffe [39] in [37], with the convolutional and global layers reduced to matrix multiplication done using an SSEoptimized BLAS package.", "startOffset": 76, "endOffset": 80}, {"referenceID": 17, "context": "Comparison with DFE: The Discriminative Ferns Ensemble (DFE) was suggested in [20] for classification of 3-HANDPOSE, and can be seen a baseline for CTE, which enhances it in many aspects.", "startOffset": 78, "endOffset": 82}, {"referenceID": 17, "context": "To enable the comparison, M-class error rates are extracted from DFE (in [20] such errors are not reported, and 3-class average true positive rates are reported instead).", "startOffset": 73, "endOffset": 77}, {"referenceID": 12, "context": "Distillation experiments: We experimented with knowledge distillation from a CNN to a CTE using the method suggested in [15].", "startOffset": 120, "endOffset": 124}, {"referenceID": 10, "context": "Our CNN baseline architectures are variations of DeepCNiN(l,k) [13], with l = 3 \u2212 4 convolutional layers and k = 60\u2212 100, implying usage of i \u00b7k maps at the i-th layer.", "startOffset": 63, "endOffset": 67}, {"referenceID": 10, "context": "It was shown in [13] that higher l, k values provide better accuracy, but such architectures are much slower than 1 CPU millisecond and so they are outside our domain of interest.", "startOffset": 16, "endOffset": 20}, {"referenceID": 30, "context": "We experimented with dropout [33], parametric RELU units [18], affine image transformations following [13], and HSV image transformations following [32].", "startOffset": 29, "endOffset": 33}, {"referenceID": 15, "context": "We experimented with dropout [33], parametric RELU units [18], affine image transformations following [13], and HSV image transformations following [32].", "startOffset": 57, "endOffset": 61}, {"referenceID": 10, "context": "We experimented with dropout [33], parametric RELU units [18], affine image transformations following [13], and HSV image transformations following [32].", "startOffset": 102, "endOffset": 106}, {"referenceID": 29, "context": "We experimented with dropout [33], parametric RELU units [18], affine image transformations following [13], and HSV image transformations following [32].", "startOffset": 148, "endOffset": 152}], "year": 2016, "abstractText": "We study classifiers operating under severe classification time constraints, corresponding to 1\u22121000 CPU microseconds, using Convolutional Tables Ensemble (CTE), an inherently fast architecture for object category recognition. The architecture is based on convolutionally-applied sparse feature extraction, using trees or ferns, and a linear voting layer. Several structure and optimization variants are considered, including novel decision functions, tree learning algorithm, and distillation from CNN to CTE architecture. Accuracy improvements of 24\u221245% over related art of similar speed are demonstrated on standard object recognition benchmarks. Using Pareto speed-accuracy curves, we show that CTE can provide better accuracy than Convolutional Neural Networks (CNN) for a certain range of classification time constraints, or alternatively provide similar error rates with 5\u2212 200\u00d7 speedup.", "creator": "LaTeX with hyperref package"}}}