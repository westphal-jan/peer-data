{"id": "1602.04921", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "16-Feb-2016", "title": "A diffusion and clustering-based approach for finding coherent motions and understanding crowd scenes", "abstract": "This paper addresses the problem of detecting coherent motions in crowd scenes and presents its two applications in crowd scene understanding: semantic region detection and recurrent activity mining. It processes input motion fields (e.g., a field) using semantic layer processing to detect the motion and to identify which is a valid position and whether the field is a valid position. It also analyzes the dynamics of the movement between a field and a group of participants to determine the location and activity of these participants (in order to analyze their behavior, for example, if there are two members of the group).\n\n\n\nThe most recent paper (see Appendix 2) identifies a wide range of methods for generating and generating the motion field (e.g., using semantic layer processing). For example, for instance, we first used a novel set of vector-based computational algorithms (NPMs), to compute motion properties. Since we first used these algorithms, the most recent paper (see Appendix 2) identifies a wide range of methods for generating and generating the motion field (e.g., using semantic layer processing). For example, for instance, we first used these algorithms, the most recent paper (see Appendix 2) identifies a wide range of methods for generating and generating the motion field (e.g., using semantic layer processing). For example, for instance, we first used these algorithms, the most recent paper (see Appendix 2) identifies a wide range of methods for generating and generating the motion field (e.g., using semantic layer processing). For example, for instance, we first used these algorithms, the most recent paper (see Appendix 2) identifies a wide range of methods for generating and generating the motion field (e.g., using semantic layer processing). For example, for instance, we first used these algorithms, the most recent paper (see Appendix 2) identifies a wide range of methods for generating and generating the motion field (e.g., using semantic layer processing). For example, for instance, we first used these algorithms, the most recent paper (see Appendix 2) identifies a wide range of methods for generating and generating the motion field (e.g., using semantic layer processing). For example, for instance, we first used these algorithms, the most recent paper (see Appendix 2) identifies a wide range of methods for generating and generating the motion field (e.g., using semantic layer processing). For example, for instance, we first used these algorithms, the most recent paper (see Appendix 2) identifies a wide range of methods for generating and generating", "histories": [["v1", "Tue, 16 Feb 2016 06:25:30 GMT  (16211kb,D)", "http://arxiv.org/abs/1602.04921v1", "This manuscript is the accepted version for TIP (IEEE Transactions on Image Processing), 2016"]], "COMMENTS": "This manuscript is the accepted version for TIP (IEEE Transactions on Image Processing), 2016", "reviews": [], "SUBJECTS": "cs.CV cs.AI cs.MM", "authors": ["weiyao lin", "yang mi", "weiyue wang", "jianxin wu", "jingdong wang", "tao mei"], "accepted": false, "id": "1602.04921"}, "pdf": {"name": "1602.04921.pdf", "metadata": {"source": "CRF", "title": "A Diffusion and Clustering-based Approach for Finding Coherent Motions and Understanding Crowd Scenes", "authors": ["Weiyao Lin", "Yang Mi", "Weiyue Wang", "Jianxin Wu", "Jingdong Wang", "Tao Mei"], "emails": ["laughter}@sjtu.edu.cn).", "wujx2001@nju.edu.cn).", "tmei}@microsoft.com)."], "sections": [{"heading": null, "text": "Index Terms\u2014Coherent Motion Detection, Semantic Region Construction, Recurrent Activity Mining\nI. INTRODUCTION\nCoherent motions, which represent coherent movements of massive individual particles, are pervasive in natural and social scenarios. Examples include traffic flows and parades of people (cf. Figs 1a and 2a). Since coherent motions can effectively decompose scenes into meaningful semantic parts and facilitate the analysis of complex crowd scenes, they are of increasing importance in crowd-scene understanding and activity recognition [2], [3], [4], [5], [6].\nIn this paper, we address the problem of detecting coherent motions in crowd scenes, and subsequently using them to understand input scenes. More specifically, we focus on 1) constructing an accurate coherent motion field to find coherent motions, 2) finding stable semantic regions based on the detected coherent motions and using them to recognize predefined activities (i.e., activities with labeled training data) in a crowd scene, and 3) automatically mining recurrent activities\nThis paper is supported in part by the following grants: Chinese National 973 Grants (2013CB329603), National Science Foundation of China (No. 61471235, 61422203, 61425011, 61527804), and Microsoft Research Asia Collaborative Research Award. The basic idea of this paper appeared in our conference version [1]. In this version, we propose a new cluster-andmerge process to mine recurrent activities in crowd scenes, carry out detailed analysis, and present more performance results.\nW. Lin, Y. Mi and W. Wang are with the Department of Electronic Engineering, Shanghai Jiao Tong University, China (email: {wylin, deyangmiyang, laughter}@sjtu.edu.cn).\nJ. Wu is with the National Key Laboratory for Novel Software Technology, Nanjing University, China (email: wujx2001@nju.edu.cn).\nJ. Wang and T. Mei are with the Microsoft Research, Beijing, China (email: {jingdw, tmei}@microsoft.com).\nin a crowd scene based on the detected coherent motions and semantic regions.\nFirst, constructing an accurate coherent motion field is crucial in detecting reliable coherent motions. In Fig. 1, (b) is the input motion field and (c) is the coherent motion field which is constructed from (b) using the proposed approach. In (b), the motion vectors of particles at the beginning of the Marathon queue are far different from those at the end, and there are many inaccurate optical flow vectors. Due to such variations and input errors, it is difficult to achieve satisfying coherent motion detection results directly from (b). However, by transferring (b) into a coherent motion field where the coherent motions among particles are suitably highlighted in (c), coherent motion detection is greatly facilitated. Although many algorithms have been proposed for coherent motion detection [7], [8], [9], [2], this problem is not yet effectively addressed. We argue that a good coherent motion field should effectively be able to 1) encode motion correlation among particles, such that particles with high correlations can be grouped into the same coherent region; and, 2) maintain motion information of individual particles, such that activities in crowd scenes can be effectively parsed by the extracted coherent motion field. Based on these intuitions, we propose a thermal-diffusion-based approach, which can extract accurate coherent motion fields.\nSecond, constructing meaningful semantic regions to describe activity patterns in a scene is also essential. Coherent motions at different times may vary widely. In Fig. 2a, changing of traffic lights will lead to different coherent motions. Coherent motions alone may not effectively describe the overall semantic patterns in a scene either. Therefore, semantic regions need to be extracted from these time-varying coherent\nar X\niv :1\n60 2.\n04 92\n1v 1\n[ cs\n.C V\n] 1\n6 Fe\nb 20\n16\nmotions to achieve stable and meaningful semantic patterns, as in Fig. 2b. However, most existing works only focus on the detection of coherent motions at some specific time, while the problem of handling time-varying coherent motions is less studied. We proposed a two-step clustering process for this purpose.\nThird, mining recurrent activities is another important issue. Many crowd scenes are composed of recurrent activities [10], [11], [12]. For example, the scene in Fig. 2 is composed of recurrent activities including vertical motion activities and horizontal motion activities, as in Fig. 2c. Automatically mining these recurrent activities is important in understanding scene contents and their dynamics. Although many researches have been done for parsing recurrent activities in low-crowd scenes [13], [14], [15], [16], this issue is not well addressed in crowd scene scenarios where reliable motion trajectories are unavailable. We proposed a cluster-and-merge process, which can effectively extract recurrent activities in crowd scenes.\nOur contributions to crowd scene understanding and activity recognition are summarized as follows.\n1) We introduce a coarse-to-fine thermal diffusion process to transfer an input motion field into a thermal energy field (TEF), which is a more accurate coherent motion field. TEF effectively encodes both motion correlation among particles and motion trends of individual particles. To our knowledge, this is the first work that introduces thermal diffusion to detect coherent motions in crowd scenes. We also introduce a triangulationbased scheme to effectively identify coherent motion components from the TEF. 2) We present a two-step clustering scheme to find semantic regions according to the correlations among coherent motions. The found semantic regions can effectively catch activity patterns in a scene. Thus good performance can be achieved when recognizing pre-defined crowd activities based on these semantic regions. 3) We propose a cluster-and-merge process to automatically mine recurrent activities by clustering and merging the coherent motions. The obtained recurrent activities can accurately describe recurrent motion patterns in a crowd scene.\nThe remainder of this paper is organized as follows. Section II reviews related works. Section III describes the framework of the proposed approach. Sections IV to VI\ndescribe the details of our proposed thermal diffusion process, triangulation scheme, two-step clustering scheme, and clusterand-merge process. Section VII shows the experimental results and Section VIII concludes the paper."}, {"heading": "II. RELATED WORKS", "text": "Many works [17], [7], [8], [9], [2], [18], [19], [20], [21], [22] have been proposed on coherent motion detection. Due to the complex nature of crowd scenes, they are not yet mature for accurate detection of coherent motion fields. Cremers and Soatto [20] and Brox et al. [21] model the intensity variation of optical flow by an objective functional minimization scheme. These methods are only suitable for motions with simple patterns and cannot effectively analyze complex crowd patterns such as the circular flow in Fig. 1a. Other works introduce external spatial-temporal correlation traits to model the motion coherency among particles [7], [8], [9]. Since these methods model particle correlations in more precise ways, they can achieve more satisfying results. However, most of these methods only consider short-distance particle motion correlation within a local region while neglecting long-distance correlation among distant particles, they have limitations in handling low-density or disconnected coherent motions where the long-distance correlation is essential. Furthermore, without the information from distant particles, these methods are also less effective in identifying coherent motion regions in the case when local coherent motion patterns are close to their neighboring backgrounds. One example of this kind of scenario is showcased in the region B in Fig. 1b.\nThere are also other works related to motion modeling. One line of related works is advanced optical flow estimation. These methods try to improve the estimation accuracy of the input motion field by including global constraints over particles [23], [24], [25], [26]. The focus of our approach is different from these methods. We focus on enhancing the correlation among coherent particles to facilitate coherent motion detection. Thus, the motion vectors of coherent particles are enhanced even if their actual motions are small, such as the region B in Figs 1b and 1c. In contrast, advanced optical flow estimation methods focus on estimating the actual motion of particles. They are still less capable of creating precise results when applied to coherent motion detection.\nThe anisotropic diffusion based methods, used in image segmentation, is also related to our work [27], [28], [29]. Our approach differs from these methods. First, our approach not only embeds the motion correlation among particles, but also suitably maintains the original motion information from the input motion vector field. Comparatively, the anisotropicdiffusion-based methods are more focused on enhancing the correlation among particles while neglecting the particles\u2019 original information. As aforementioned, maintaining particle motion information is important in parsing crowd scenes. More importantly, due to the complex nature of crowd scenes, many coherent region boundaries are vague, subtle and unrecognizable. Simply applying the anisotropic-diffusion methods cannot identify the ideal boundaries. The proposed thermal diffusion process can achieve more satisfying results by mod-\n3 eling the motion direction, strength, and spatial correlation among particles.\nBesides coherent motion detection, it is also important to utilize coherent motions to recognize pre-defined crowd activities. However, most existing coherent motion works only focus on the extraction of coherent motions while the recognition of crowd activities is much less studied. In [17], Ali and Shah detected instability regions in a scene by comparing with its normal coherent motions. However, they assume coherent motions to be stable, while in practice, many coherent motions may vary widely over time, making it difficult to construct stable normal coherent motions. Furthermore, besides the works on coherent motion, there are also other works which directly extract global features from the entire scene to recognize crowd activities [3], [30]. However, since they do not consider the semantic region correlations inside the scene, they have limitations in differentiating subtle differences among activities. Although there are some works [4], [31] which recognize crowd activities by segmenting scenes into semantic regions, our approach differs from them. Our approach finds the semantic regions by first extracting global coherent motion information, while these methods construct semantic regions from the particles\u2019 local features. As will be shown later, information from the coherent motions can effectively enhance the correlation among particles, resulting in more meaningful semantic regions to facilitate activity recognition.\nFurthermore, pre-defining or labeling crowd activities requires lots of human labors, making it desirable to automatically discover activity patterns in a crowd video without human intervention. In [15], Morris and Trivedi clustered trajectories into groups and modeled the spatio-temporal dynamic patterns of each trajectory group by Hidden Markov Models. Wang et al. [13] and Hu et al. [14] further introduced Dirichlet processes to model the activity patterns of different trajectory groups. However, since these methods extract recurrent activities from motion trajectories, they are not suitable for crowd scene scenarios where reliable trajectories are difficult to achieve. Besides using motion trajectories, other researches tried to find recurrent activities by extracting lowlevel or short-term motion features. For example, Zhou et al. [12] extracted fragments of trajectories (called tracklets) and utilized a Latent Dirichlet Allocation topic model to infer recurrent activities. Jagannadan et al. [11] and Emonet et al. [10] extracted low-level motion flows as motion descriptors and introduced a Probabilistic Latent Sequential Motif (PLSM) model to achieve recurrent activities. Although these methods can be applied in crowd scenes, they still have limitations in obtaining precise recurrent activity patterns under scenes with complex motions. Our approach differs from the previous methods in that 1) Our approach utilizes coherent motions to discover recurrent activities. Since coherent motions can effectively catch the local activity pattern in each frame, more precise recurrent activities can be achieved by our approach, 2) Our approach also extracts flow curves to describe and visualize recurrent activities. Compared with the previous methods which described recurrent activities by trajectory clusters or probability densities, the flow curves derived by our approach can visualize recurrent activity patterns in a clearer\nand more straightforward way."}, {"heading": "III. OVERVIEW OF THE APPROACH", "text": "The framework of the proposed approach is shown in Fig. 3. The optical flow fields [17], [32] are first extracted from input videos. Secondly, the coarse-to-fine thermal diffusion process is applied to transfer the input motion fields into coherent motion fields, i.e., thermal energy fields (TEFs). Thirdly, the triangulation-based scheme is applied to identify coherent motions. Fourthly, with the obtained coherent motions, the two-step clustering scheme is performed to cluster coherent motions from multiple TEFs and construct semantic regions for the target scene. Finally, based on these semantic regions, we can extract effective features to describe crowd activities in the scene and recognize pre-defined crowd activities accordingly. At the same time, the cluster-and-merge process is also applied based on the extracted coherent motions and semantic regions to discover recurrent activities in the target scene. These proposed techniques are described in the following sections in detail."}, {"heading": "IV. FINDING COHERENT MOTIONS", "text": "In order to find accurate coherent motions, it is important to construct a coherent motion field to highlight the motion correlation among particles while still maintaining the original motion information. To achieve this requirement, we introduce a thermal diffusion process to model particle correlations. Given an input optical flow field, we view each particle (i.e., each pixel in a frame) as a \u201cheat source\u201d and it can diffuse energies to influence other particles. By suitably modeling this thermal diffusion process, precise correlation among particles can be achieved. The formulation is motivated by the following intuitions:\n1) Particles farther from heat source should achieve fewer thermal energies; 2) Particles residing in the motion direction of the heat source particle should receive more thermal energies; 3) Heat source particles with larger motions should carry more thermal energies."}, {"heading": "A. Thermal Diffusion Process", "text": "Based on the above discussions, we borrow the idea from physical thermal propagation [33] and model the thermal diffusion process by Eq. 1:\n\u2202EP,l \u2202l = k2p ( \u22022EP,l \u2202x2 + \u22022EP,l \u2202y2 ) + FP (1)\nwhere EP,l = [ExP,l, E y P,l] is the thermal energy for the particle at location P = (px, py) after performing thermal diffusion for l seconds, FP = [fxP, f y P] is the input motion vector for particle P, kp is the propagation coefficient. The first term in Eq. 1 models the propagation of thermal energies over free space such that the spatial correlation among particles can be properly enhanced during thermal diffusion. The second term FP can be viewed as the external force added on the particle to affect its diffusion behavior, which preserves\n4\nthe original motion patterns. The inclusion of this term is one of the major differences between the proposed approach and the anisotropic-diffusion methods [29]. Without the FP term, Eq. 1 can be solved by:\nEP,l = 1\nwh \u2211 Q\u2208I,Q6=P eP,l (Q) (2)\nwhere EP,l is the final diffused thermal energy for particle P after l seconds, I is the set of all particles in the frame, w and h are width and height of the frame. The individual thermal energy eP,l (Q) = [exP,l (Q) , e y P,l (Q)] is diffused from the heat source particle Q = (qx, qy) to particle P after l seconds, defined as:\ne\u03b3P,l (Q) = u \u03b3 Q \u00b7 e\n\u2212kp l ||P\u2212Q|| 2\n(3)\nwhere \u03b3 \u2208 {x, y}, UQ = (uxQ, u y Q) is the current motion pattern for the heat source particle Q and it is initialized by UQ = FQ, ||P\u2212Q|| is the distance between particles P and Q. In this paper, we fix l to be 1 to eliminate its effect.\nHowever, when F in Eq. 1 is non-zero, it is difficult to get the exact solution for Eq. 1. So we introduce an additional term e\u2212kf |FQ\u00b7(P\u2212Q)| to approximate the influence of FQ where kf is a force propagation factor. Moreover, in order to prevent unrelated particles from accepting too much heat from Q, we restrict that only highly correlated particles will propagate energies to each other. The final individual thermal energy from Q to P is:\ne\u03b3P,l (Q) = u \u03b3 Q \u00d7 e \u2212kp||P\u2212Q||2 \u00d7 e\u2212kf |FQ\u00b7(P\u2212Q)| (4)\nif cos(FP,FQ) \u2265 \u03b8c and is 0 if otherwise, where FP and FQ are the input motion vectors of the current particle P and the heat source particle Q, and cos(FP,FQ) is the cosine similarity, \u03b8c is a threshold. In our experiments, kp, kf , and \u03b8c are set to be 0.2, 0.8, 0.7, which are decided from the experimental statistics.\nFrom Eq. 2, we see that the diffused thermal energy EP is the summation from all other particles, which encodes the correlation among P and all other particles in the frame. Furthermore, in Eq. 4, the first term preserves the motion pattern of the heat source. The second term considers the spatial correlation between source and target particles. The third term guarantees that particles along the motion direction of the heat source receives more thermal energies. Furthermore,\n(a) (b) (c) (d)\nFigure 4. (a),(b): One input optical flow field and its thermal energy field; (c), (d): Individual thermal diffusion result by diffusing from a single heat source particle A and B to the entire field.\nthe cosine similarity cos(FP,FQ) is introduced in Eq. 4 such that particle P will not accept energy from Q if their input motion vectors are far different (or less-coherent) from each other. That is, Eq. 4 successfully satisfies all the intuitions.\nFig. 4 shows one example of the thermal diffusion process, which reveals that:\n1) Comparing Figs 4b and 4a, the original motion information is indeed preseved in the TEF. Moreover, TEF further strengthens particle motion coherency by thermal diffusion, which integrates the influence among particles. Coherent motions become more recognizable, thus more accurate coherent motion extraction can be achieved. 2) From Fig. 4c, we can see that the thermal energy for each heat source particle is propagated in a sector shape. Particles along the motion direction of the heat source (C and D) receive more energies than particles outside the motion direction (such as E). In Fig. 4d, since particles on the lower side of the heat source B have small (cosine) motion similarities with B, they do not accept thermal energies."}, {"heading": "B. The Coarse-to-Fine Scheme", "text": "Although Eqs 2 and 4 can effectively strengthen the coherency among particles, it is based on a single input motion field, and only short-term motion information is considered, which is volatile and noisy. Thus, we propose a coarse-tofine scheme to include long-term motion information. The\n5 entire coarse-to-fine thermal diffusion process is described in Algorithm 1.\nAlgorithm 1 Coarse-to-Fine Thermal Diffusion Process 1: T = Tmax 2: Calculate the input motion vector field FP(T ) with T -frame\nintervals 3: UP = FP(T ) 4: for n = 0 to Numitr do 5: Use Eq. 2 to create the new thermal energy field EnP based on FP(T ) and UP 6: Normalize the vector magnitudes in EnP 7: UP = E n P 8: T = T \u2212 Tstep 9: if T > 0 then\n10: Calculate FP(T ) with the new T 11: end if 12: end for 13: Output EnP\nThe long-term motion vector field with a large frame interval Tmax is first calculated and used to create the thermal energy field. Then, the TEF is iteratively updated with shorterterm motion vector fields, i.e., FP(T ) with smaller T . Figs 5a to 5d show the TEF results after different iteration numbers. When more iterations are performed, more motion information with different intervals will be included in the thermal diffusion process. Thus, more precise results can be achieved in the TEF, as in Fig. 5d. Fig. 1c shows another TEF result after the entire coarse-to-fine thermal diffusion scheme. We find that:\n1) TEF is an enhanced version of the input motion where particles\u2019 energy directions in the TEF are similar to their original motion directions. Besides, since TEF include both the motion correlation among particles and the short-/long-term motion information among frames, coherent motions are effectively strengthened and highlighted in TEF. 2) As mentioned, input motion vectors may be disordered, e.g., region A in Fig 1b. However, the thermal energies from other particles can help recognize these disordered motion vectors and make them coherent, e.g., Fig. 1c. 3) Input motion vectors may be extremely small due to slow motion or occlusion by other objects (region B in Fig. 1b and region C in Fig. 5b). It is very difficult to include these particles into the coherent region by traditional methods [17], [7], [8], [9] because they are close to the background motion vector. However, TEF can strengthen these small motion vectors by diffusing thermal energies from distant particles with larger motions."}, {"heading": "C. Finding Coherent Motions through Triangulation", "text": "Coherent motion regions can be achieved by performing segmentation on the TEF. We propose a triangulation-based scheme as follows:\nStep 1: Triangulation. In this step, we randomly sample particles from the entire scene and apply the triangulation process [34] to link the sampled particles. The block labeled\n(a) (b)\nas \u201ctriangulation\u201d in Fig. 3 shows one triangulation result, where red dots are the sampled particles and the lines are links created by the triangulation process [34].\nStep 2: Boundary detection. We first obtain each triangulation link weight by:\n\u03c9 (P,Q) = ||EP \u2212EQ|| ||P\u2212Q||\n(5)\nwhere P and Q are two connected particles, EP and EQ are the thermal energy vectors of P and Q in the TEF. A large weight will be assigned if the connected particles are from different coherent motion regions (i.e., they have different thermal energy vectors). Thus, by thresholding on the link weights, we can find links crossing the boundaries. The block labeled as \u201cdetected region boundary\u201d in Fig. 3 shows one boundary detection result after step 2.\nStep 3: Coherent motion segmentation. Then, coherent motions can be easily segmented and we use the watershed algorithm [35]. The final coherent motions are shown in the block named \u201cdetected coherent motions\u201d in Fig. 3."}, {"heading": "V. CONSTRUCTING SEMANTIC REGIONS", "text": "With the extracted coherent motions, accurate motion information in a frame can be achieved. However, since coherent motions vary over time, it is essential to construct semantic regions from time-varying coherent motions to catch stable semantic patterns inside a scene. For this purpose, we propose a two-step clustering scheme. Assuming that in total M coherent motions (Cm, m = 1, ...,M ) from N TEFs extracted at N times, the two-step clustering scheme is:\nStep 1: Cluster coherent motion regions. The similarity between two coherent motions Cm and Ck is computed as:\nSC(Cm,Ck) = #{(P,Q)|P \u2208 Lm,Q \u2208 Lk, cos(EP,EQ) \u00b7 e\u2212kp||P\u2212Q|| 2 > \u03b8bp} (6)\nwhere #{\u00b7} is the number of elements in a set. \u03b8bp is a threshold which is set to be the same as \u03b8c in Eq. 4 in our experiments. Furthermore, Lm and Lk are the sets of \u201cindicative particles\u201d for Cm and Ck:\nLm = {P| cos(EP,VP) > \u03b8c,P is on the boundary of Cm} Lk = {Q| cos(EQ,VQ) > \u03b8c,Q is on the boundary of Ck} (7)\nwhere VP = [vxP, v y P] is the outer normal vector at P, i.e., perpendicular to the boundary and pointing outward the coherent motion region, \u03b8c is the same threshold as in the condition for Eq. 4. That is, only particles which are on the boundaries of the coherent motion region and whose thermal energy vectors sharply point outward the region are selected as the indicative particles. Thus, we can avoid noisy particles and substantially reduce the required computations.\nFrom Eq. 6, we can see that we first extract the indicative particles, then only utilize those high-correlation pairs, and the total number of such pairs are the similarity value between two coherent motions. It should be noted that the similarity will be calculated between any coherent motion pairs even if they belong to different TEFs.\nThen, we construct a similarity graph for the M coherent motions, and perform clustering [36] on this similarity graph with the optimal number of clusters being determined automatically, the cluster results are grouped coherent regions.\nStep 2: Cluster to find semantic regions. Each coherent motion is assigned a cluster label in Step 1, as illustrated in Fig. 6a. However, due to the variation of coherent motions at different times, there exist many ambiguous particles. For\nexample, in Fig. 6a, the yellow cross particle belongs to different coherent motion clusters in different TEFs. This makes it difficult to directly use the clustered coherent motion results to construct reliable semantic regions. In order to address this problem, we further propose to encode particles in each TEF by the cluster labels of the particles\u2019 affiliated coherent motions. And by concatenating the cluster labels over different TEFs, we can construct a \u201ccluster label\u201d vector for each particle, as in Fig. 6a And with these label vectors, the same spectral clustering process as Step 1 can be performed on the particles to achieve the final semantic regions, as in Fig. 6b.\nComparing with previous semantic region segmentation methods [4], [31] which perform clustering using local similarity among particles, our scheme utilizes the guidance from the global coherent motion clustering results to strengthen the correlations among particles. For example, in Fig. 7a, when directly segmenting the particles by their local features, its accuracy may be limited due to similar distances among particles. However, by utilizing cluster labels to encode the particles, similarities among particles can be suitably enhanced by the global coherent cluster information, as in Fig. 7b. Thus, more precise segmentation results can be achieved."}, {"heading": "A. Recognizing Pre-defined Activities", "text": "Based on the constructed semantic regions, we are able to recognize pre-defined activities (i.e., activities with labeled training data) in the scene. In this paper, we simply average the TEF vectors in each semantic region and concatenate these averaged TEF vectors as the final feature vector for describing the activity patterns in a TEF. Then, a linear support vector machine (SVM) [37] is utilized to train and recognize predefined activities. Experimental results show that with accurate TEF and precise semantic regions, we can achieve satisfying results using this simple method."}, {"heading": "B. Merging Disconnected Coherent Motions", "text": "Since TEF also includes long-distance correlations between distant particles, by performing our clustering scheme, we also have the advantage of effectively merging disconnected coherent motions, which may be caused by the occlusion from other objects or low density of the crowd. For examples, the two disconnected blue regions in the right-most figure in Fig. 6a are merged into the same cluster by our approach. Note that this issue is not well studied in the existing coherent motion research."}, {"heading": "VI. MINING RECURRENT ACTIVITIES", "text": "With the extracted coherent motions and constructed semantic regions, crowd activities can be recognized by constructing and pre-labeling training data, as in Section V-A. However, since pre-defining or labeling crowd activities take lots of human labors, it is also desirable to automatically mine recurrent activity patterns in a crowd scene without human intervention. For this purpose, we propose a cluster-and-marge process which includes three steps: frame-level clustering, coherent motion merging, and flow curve extraction.\n7"}, {"heading": "A. Frame-level Clustering", "text": "The frame-level clustering step clusters frames according to the extracted coherent motions and semantic regions, such that frames with the same recurrent activity pattern can be organized into the same group. In this paper, we first calculate inter-frame similarities for all frame pairs and then utilize spectral clustering [36] to cluster frames according to these inter-frame similarities.\nIn order to calculate the inter-frame similarity between frames t and t\u2212\u03c4 , the similarities between all coherent motions from frames t and t\u2212 \u03c4 are first calculated using Eq. 6. Then, the inter-frame similarity SF (t, t\u2212 \u03c4) can be achieved from these coherent motion similarities and the segmented semantic regions. More specifically, we define SF (t, t\u2212 \u03c4) as\nSF (t, t\u2212 \u03c4) = SFU (t, t\u2212 \u03c4) \u00b7 SFM (t, t\u2212 \u03c4) (8)\nwhere SFM (t, t\u2212 \u03c4) is the similarity for the matched coherent motion pairs between t and t \u2212 \u03c4 , SFU (t, t\u2212 \u03c4) is the similarity for the unmatched coherent motion regions in frames t and t\u2212 \u03c4 . SFM (t, t\u2212 \u03c4) and SFU (t, t\u2212 \u03c4) can be calculated by Eqs 9 and 10.\nFirst, SFM (t, t\u2212 \u03c4) is defined as\nSFM (t, t\u2212 \u03c4) =\n\u2211 (Ct,i,Ct\u2212\u03c4,j)\u2208Ht,t\u2212\u03c4 \u03bbi,jSC (Ct,i,Ct\u2212\u03c4,j)\nmax{nt, nt\u2212\u03c4} (9)\nwhere SC (Ct,i,Ct\u2212\u03c4,j) is the similarity between coherent motion regions Ct,i and Ct\u2212\u03c4,j , \u03bbi,j is the corresponding weight. nt and nt\u2212\u03c4 are the total number of coherent motion regions in frames t and t\u2212\u03c4 , respectively. Ht,t\u2212\u03c4 is the set of all matched coherent region pairs. In this paper, Ht,t\u2212\u03c4 and \u03bbi,j are calculated by the Hungarian algorithm [38] which can achieve optimal coherent motion matching results based on the input coherent motion similarities. Furthermore, in order to exclude dissimilar coherent motion pairs from the matching result, coherent motion pairs (Ct,i,Ct\u2212\u03c4,j) with small similarity values SC (Ct,i,Ct\u2212\u03c4,j) will be deleted from Ht,t\u2212\u03c4 . Fig. 8 shows an example of the matched coherent motion pairs.\nThe next term SFU (t, t\u2212 \u03c4) is defined as SFU (t, t\u2212 \u03c4) = \u220f\nCt\u2212\u03c4,j\u2208Dt\u2212\u03c4\n\u03b5 (Ct\u2212\u03c4,j) \u00b7 \u220f\nCt,i\u2208Dt\n\u03b5 (Ct,i) (10)\nwhere Dt\u2212\u03c4 and Dt are the sets of unmatched coherent regions in frames t\u2212 \u03c4 and t, as shown in Fig. 8. \u03b5 (C) is the unmatching cost for coherent motion region C:\n\u03b5 (C) =\n\u2211 Rk,Rk\u2229C6=\u2205 1\n\u03c1 (Rk)\n#{Rk|Rk \u2229C 6= \u2205} (11)\nwhere Rk is the k-th semantic region of the scene, the term #{Rk|Rk \u2229C 6= \u2205} represents the total number of semantic regions that have overlap with the coherent motion region C. \u03c1 (Rk) is the importance cost measuring whether semantic region Rk is important in distinguishing different recurrent activities. For example, assuming that a scene includes two recurrent activities, as in Fig. 9, it is obvious that the semantic region R2 on the right should have larger importance cost since the two recurrent activity patterns have different motion flows in R2. Comparatively, the semantic region R1 on the left should have smaller importance cost since both recurrent activity patterns have similar flows in R1. Therefore, when calculating the similarity between frames t and t \u2212 \u03c4 , if there exists an unmatched coherent region C in R2, a large importance cost \u03c1 (R2) will be applied to reduce the interframe similarity, indicating that frames t and t \u2212 \u03c4 have different recurrent activity patterns. On the contrary, if there exists an unmatched coherent region C in R1, the inter-frame similarity will be less affected since a coherent region in R1 is less indicative of the differences between recurrent activities.\nFor \u03c1 (Rk), we first perform a pre-clustering according to the matched coherent motion similarities SFM (t, t\u2212 \u03c4) which roughly clusters frames into different recurrent activity groups. Then a vector is constructed for each semantic region Rk: [NumRk,G1 , NumRk,G2 , ..., NumRk,GZ ] where NumRk,Gi is the total number of coherent motions located in Rk in the i-th pre-clustered recurrent activity group Gi, Z is the total number of pre-clustered recurrent activity groups. Finally, \u03c1 (Rk) can be calculated by:\n\u03c1 (Rk) = e ks\u00b7var{NumRk,G1 ,NumRk,G2 ,...,NumRk,GZ} (12)\nwhere var{\u00b7} is the variance operation, ks = 1Numf 2 where Numf is the total number of frames to be clustered. According to Eq. 12, if coherent motions appear evenly in Rk for different recurrent activities, i.e., the variance is smaller, it implies that Rk is less important in distinguishing different recurrent activities. On the contrary, if the appearance time of coherent motions in Rk has larger variation over different pre-clustered recurrent activity groups, a large \u03c1 (Rk) will be achieved to increase the importance of Rk. The complete process of frame-level clustering is illustrated in Algorithm 2.\n8 Algorithm 2 Frame-level Clustering Process Input: Coherent regions Ct,i extracted for each frame t, and semantic regions Rk of the scene Output: Recurrent activity groups including frames with similar recurrent activity patterns 1: Calculate similarities SC (Ct,i,Ct\u2212\u03c4,j) between all coherent\nmotion regions from different frames 2: Calculate SFM (t, t\u2212 \u03c4) for all frame pairs based on SC (Ct,i,Ct\u2212\u03c4,j) 3: Pre-cluster frames based on SFM (t, t\u2212 \u03c4) 4: Calculate importance cost \u03c1 (Rk) for all semantic regions based\non the pre-clustering result 5: Calculate SFU (t, t\u2212 \u03c4) for all frame pairs according to the\nunmatched coherent regions and \u03c1 (Rk) 6: Calculate inter-frame similarities SF (t, t\u2212 \u03c4) for all frame pairs\nusing SFM (t, t\u2212 \u03c4) and SFU (t, t\u2212 \u03c4) 7: With SF (t, t\u2212 \u03c4), cluster frames into recurrent activity groups 8: Output the clustering result in line 9"}, {"heading": "B. Coherent Motion Merging", "text": "After frame-level clustering, frames are clustered into different recurrent activity groups. Thus, by parsing frames in each recurrent activity group, complete motion patterns for each recurrent activity can be estimated. In this paper, we introduce a coherent motion merging step to merge similar coherent motions from the same recurrent activity group for achieving motion pattern regions. More specifically, we first apply the same operation as Step 1 in the two-step clustering scheme (Section V) to cluster coherent motion regions inside the same recurrent activity group. Then, coherent motions of the same cluster are merged together to form a motion pattern region. The merging process can be described by Eq. 13 and Fig. 10.\nEP,\u03a8j = \u2211\nCm\u2208\u03a8j\nEP,Cm #{\u03a8j}\n(13)\nif # {EP,Cm |EP,Cm 6= [0, 0],Cm \u2208 \u03a8j}\n#{\u03a8j} > \u03b8mf , and it is\n[0, 0] if otherwise, where \u03a8j the j-th coherent motion cluster. EP,\u03a8j = [ E x P,\u03a8j , E y P,\u03a8j ] is the merged motion vector result for \u03a8j at particle P. Cm is a coherent motion region belonging to coherent motion cluster \u03a8j . #{\u03a8j} is the total number of coherent motion regions in cluster \u03a8j . \u03b8mf is a threshold which is set as 0.4 in our experiments. EP,Cm is the TEF thermal energy for Cm at particle P. Note that EP,Cm is set to [0, 0] if P is outside the region of Cm. And # {EP,Cm |EP,Cm 6= [0, 0],Cm \u2208 \u03a8j} is the total number of non-zero TEF thermal energies at particle P and belonging to \u03a8j .\nAccording to Eq. 13, the merged motion pattern region R\u03a8j = {EP,\u03a8j} for a coherent motion cluster \u03a8j is basically the normalized summation over all coherent regions in \u03a8j . Besides, we further introduce a threshold \u03b8mf to filter out noisy or isolated particles which have low frequent motions in the coherent motion cluster \u03a8j . An example of merged motion pattern regions is shown in Fig. 10."}, {"heading": "C. Flow Curve Extraction", "text": "The motion pattern regions achieved in the previous step can represent the complete motion information for each recurrent activity. However, since motion pattern regions may overlap with each other and the contours of motion pattern regions may also be irregular, it is necessary to extract flow curves from these motion pattern regions such that recurrent activities can be more clearly described and visualized.\nOur proposed flow curve extraction process can be described by Algorithm 3 and Fig. 11. According to Algorithm 3 and Fig. 11, our approach first sequentially cuts a motion pattern region R\u03a8j into sub-regions along the motion direction in R\u03a8j . Then the centroids of sub-regions are linked together to achieve the output flow curve. With the above process, the extracted flow curve can accurately catch the major motion flow of a motion pattern region. Furthermore, it should be noted that in step 5 of Algorithm 3, if the line perpendicular to the motion vector EPK+1,\u03a8j at PK+1 is intersecting with a branched motion region (i.e., the motion region diverges around PK+1), multiple Pmov,s points will be achieved and the following flow curve extraction process will be performed on each Pmov,s respectively. In this way, we can properly achieve branched flow curves at the branch region."}, {"heading": "VII. EXPERIMENTAL RESULTS", "text": "Our approach is implemented by Matlab and the optical flow fields [32] are used as the input motion vector fields while each pixel in the frame is viewed as a particle. In order to achieve motion vector fields with T -frame intervals (T = 10 in our experiments), the particle advection method [17] is used which tracks the movement of each particle over T frames."}, {"heading": "A. Results for Coherent Motion Detection", "text": "We perform experiments on a dataset including 30 different crowd videos collected from the UCF dataset [17], the UCSD dataset [39], the CUHK dataset [9], and our own collected set. This dataset covers various real-world crowd scene scenarios with both low- and high-density crowds and both rapid and slow motion flows. Some example frames of the dataset is shown in Fig. 12.\nAlgorithm 3 Flow Curve Extraction Input: A motion pattern region R\u03a8j merged from coherent region cluster \u03a8j Output: A flow curve extracted from R\u03a8j 1: Calculate the skeleton of R\u03a8j [35] 2: Find the end point Ps of the skeleton which is on \u201cbackward\u201d\nposition to all other end points, where the \u201cbackward\u201d direction is defined as the reversed direction of the motion flows in R\u03a8j 3: PK = Ps, where PK is the current segmentation point 4: while PK+1 is inside R\u03a8j do 5: Pmov,s as the middle point of the line perpendicular to the motion vector EPK ,\u03a8j at PK 6: for n=0 to Nummov {Nummov is the number of movements} do 7: Move from Pmov,s to Pmov,e by EPmov,s,\u03a8j , where EPmov,s,\u03a8j is motion vector at Pmov,s in R\u03a8j 8: Pmov,s=Pmov,e 9: end for 10: PK+1= Pmov,s, where PK+1 is the next segmentation point\n11: Draw two straight lines perpendicular to the motion vectors of at PK and PK+1, respectively 12: Calculate the centroid of the sub-region segmented by the lines in line 13 13: PK = PK+1 14: end while 15: Sequentially link together all centroid points achieved by line 14\n16: Smooth the linked curve by line 17 17: Output the curve by line 18\nWe compare our approach with four state-of-the-art coherent motion detection algorithms: The Lagrangian particle dynamics approach [17], the local-translation domain segmentation approach [7], the coherent-filtering approach [8], and the collectiveness measuring-based approach [9]. In order to\n10\nfurther demonstrate the effectiveness of our approach, we also include the results of a general motion segmentation method [40] and an anisotropic-diffusion-based image segmentation method [28].\nQualitative comparison on coherent motion detection. Fig. 12 compares the coherent motion detection results for different methods. We include the manually labeled ground truth results in the first column. From Fig. 12, we can see that our approach can achieve better coherent motion extraction than the compared methods. For example, in sequence 1, our approach can effectively extract the circle-shape coherent motion. Comparatively, the method in [17] can only detect part of the circle while the methods in [8] and [9] fail to work since few reliable key points are extracted from this overcrowded scene. For sequences 2 and 4 where multiple complex motion flows exist, our approach can still precisely detect the small and less differentiable coherent motions, such as the pink region on the bottom and the blue region on the top in sequence 2 (a). The compared methods have low effectiveness in identifying these regions due to the interference from the neighboring motion regions. In sequences 3 and 6, since motions on the top of the frame are extremely small and close to the background, the compared methods fail to include these particles into the coherent motion region. However, in our approach, these small motions can be suitably strengthened and included through the thermal diffusion process. Furthermore, the methods in [40] and [28] do not show satisfying results, e.g., in sequences 5 and 6. This is because: (1) the crowd scenes are extremely complicated such that the extracted particle flows or trajectories become unreliable, thus making the general motion segmentation methods [40] difficult to create precise results; (2) Since many coherent region boundaries in the crowd motion fields are rather vague and unrecognizable, good boundaries cannot be easily achieved without suitably utilizing the characteristics of the motion vector fields. Thus, simply applying the existing anisotropicdiffusion segmentation methods [28] cannot achieve satisfying results.\nCapability to handle disconnected coherent motions. Sequences 5-8 in Fig. 12 compare the algorithms\u2019 capability in handling disconnected coherent motions. In sequence 7, we manually block one part of the coherent motion region while in sequences 5, 6, and 8, the red or green coherent motion regions are disconnected due to occlusion by other objects or low density. Since the disconnected regions are separated far from each other, most compared methods wrongly segment them into different coherent motion regions. However, with our thermal diffusion process and two-step clustering scheme, these regions can be successfully merged into one coherent region.\nQuantitative comparison. Table I compares the quantitative results for different methods. In Table I, the average Particle Error Rates (PERs) and the average Coherent Number Error (CNE) for all the sequences in our dataset are compared to measure the overall accuracy of coherent motion detection. PER is calculated by PER = # of Wrong Particles / Total # of Particles. CNE is calculated by CNE =\n\u03a3i1\nwhere Numd(i) and Numgt(i)\nare the numbers of detected and ground-truth coherent regions for sequence i, respectively, \u03a3i1 is the total number of sequences.\nTable I further demonstrates the effectiveness of our approach. In Table I, we can see that 1) Our approach can achieve smaller coherent detection error rates than the other methods, 2) Our approach can accurately obtain the coherent region numbers (close to the ground truth) while other methods often over-segment or under-segment the coherent regions.\nEffect of different parameter values. Finally, Fig. 13 shows the results of our approach under different parameter values, i.e., kp and kf in Eqs 3 and 4. From Figs 13a to 13c, we can see that kp mainly governs the thermal diffusion distance. A small kp will make the thermal energies to be diffused farther and thus can achieve larger coherent motion regions. When kp increases, the extracted coherent motion region will shrink. Furthermore, kf determines the directivity of thermal diffusion. When kf increases, the diffused thermal energies will concentrate more along the motion direction of the source heat particles. On the contrary, when kf decreases, the thermal energies will be propagated more uniformly to all directions around the heat source particle. Thus, the boundaries will shrink horizontally with larger kf , as in Fig. 13e. However, note that in all examples in Fig. 13, our approach can always suitably merge coherent regions together even when they become disconnected when the parameter value changes."}, {"heading": "B. Results for Semantic Region Construction and Pre-defined Activity Recognition", "text": "We perform experiments on two crowd videos in our dataset, as the first and second rows in Fig. 14. 400 video clips are selected from each video with each clip including 20 frames. Four crowd activities are defined for each video and the example frames for the crowd activities are shown in Fig. 14. Note that these videos are challenging in that: (1) the crowd density in the scene varies frequently including both high density as Fig. 14d and low density clips as Fig. 14c; (2) The motion patterns are varying for different activities, making it difficult to construct meaningful and stable semantic regions; (3) There are large numbers of irregular motions that disturb the normal motion patterns (e.g., people running the red lights or bicycle following irregular paths); (4) The number\nof clips in the dataset is small, which increases the difficulty of constructing reliable semantic regions. Moreover, in order to further demonstrate the effectiveness of our approach, we also perform experiments on a public QMUK Junction dataset [41] where five crowd activities are defined, as shown in the third row of Fig. 14.\nAccuracy on semantic region construction. For each video in Fig. 14, we randomly select 200 video clips and use them to construct the corresponding semantic regions. Fig. 15 compares the results of four methods: (1) Our approach (\u201cOur\u201d), (2) Directly cluster regions based on the particles\u2019 TEF vectors (\u201cDirect\u201d, note that our approach differs from this method by clustering over the cluster label vectors), (3) Use [7] to achieve coherent motion regions and then apply our two-step clustering scheme to construct semantic regions (\u201c[7]+Two-Step\u201d, we show the results of [7] because in our experiments, [7] has the best semantic region construction results among the compared methods in Table I), (4) The activity-based scene segmentation method in [4] (\u201c[4]\u201d). We also show original scene images and plot all major activity flows to ease the comparison (\u201coriginal scene\u201d).\nFig. 15 shows that the methods utilizing \u201ccoherent motion cluster label\u201d information (\u201cour\u201d and \u201c[7]+two-step\u201d) create more meaningful semantic regions than the other methods, e.g., successfully identifying the horizontal motion regions in the middle of the scene in Fig. 15b. This shows that our cluster label features can effectively strengthen the correlation among particles to facilitate semantic region construction. Furthermore, comparing our approach with the \u201c[7]+TwoStep\u201d method, it is obvious that the semantic regions by our approach are more accurate (e.g., more precise semantic region boundaries and more meaningful segmentations in the scene). This further shows that more precise coherent motion detection results can result in more accurate semantic region results.\nPerformances on recognizing pre-defined activities. In order to recognize the pre-defined activities in Fig. 14, for each video, we randomly select 200 video clips and construct semantic regions by the methods in Fig. 15. After that, we\nTable II RECOGNITION ACCURACY OF DIFFERENT METHODS\nOur (%) Our+ OF (%) Direct (%) [7]+TwoStep (%) [4] (%)\n[3] (%)\nFig. 14a video 92.2 87.75 77.0 89.5 79.2 67.0 Fig. 14e video 90.69 83.83 73.53 81.76 72.35 69.80 Fig. 14i video 93.58 91.03 83.42 88.32 84.83 82.69\nderive features from the TEF and train SVM classifiers by the method in Section V-A. Finally, we perform recognition on the other 200 video clips in the same video. Besides, we also include the results of two additional methods: (1) a state-ofthe-art dense-trajectory-based recognition method [3] (\u201cDenseTraj\u201d); (2) the method which uses our semantic regions but uses the input motion field (i.e., the optical flows) to derive the motion features in each semantic region (\u201cOur+OF\u201d). From the recognition accuracy shown in Table II, we observe that:\n1) Methods using more meaningful semantic regions (i.e., \u201cour\u201d, \u201cour+OF\u201d, and \u201c[7]+Two step\u201d) achieve better results than other methods. This shows that suitable semantic region construction can greatly facilitate activity recognition. 2) Approaches using TEF (\u201cOur\u201d) achieve better results than those using the input motion field (\u201cOur+OF\u201d). This demonstrates that compared with the input motion filed, our TEF can effectively improve the effectiveness in representing the semantic regions\u2019 motion patterns. 3) The dense-trajectory method [3] which extracts global features does not achieve satisfying results. This is because the global features still have limitations in differentiating the subtle differences among activities. This further implies the usefulness of semantic region decomposition in analyzing crowd scenes."}, {"heading": "C. Results for Recurrent Activity Mining", "text": "In this experiment, we use the same videos as in Fig. 14 for mining recurrent activities. For each video, we sample one frame per second, then calculate coherent motions for the sampled frames, and finally apply our cluster-and-merge process to achieve recurrent activity patterns. Note that the target for recurrent activity mining is to automatically discover\n12\n(a) Our (b) Direct Clustering (c) Pre-clustering\n(d) Our (e) Direct Clustering (f) Pre-clustering\nrecurrent activities from an input video without pre-defining activity types or pre-labeling training data. And ideally, good activity mining approaches should achieve similar activity patterns as the human-observed activity types in Fig. 14.\nPerformances on frame-level clustering. For each video, we apply our frame-level clustering step to cluster the sampled frames into four recurrent activity groups. Our clustering results are compared with two methods: (1) Direct clustering. Directly clustering based on the TEF difference between two frames (i.e., use the summation of absolute thermal energy differences between the co-located particles in two TEFs as the inter-frame similarity). (2) Pre-clustering. Using the matchedcoherent-motion similarities SFM (t, t\u2212 \u03c4) in Eq. 9 as the inter-frame similarity for clustering.\nFig. 16 compares the clustering confusion matrixes of different methods. From Fig. 16, we can see that since frames of the same recurrent activity may contain different parts of a complete activity flow (e.g., Fig. 10), their TEFs may have large differences. Therefore, directly using TEF difference for clustering (i.e., direct TEF clustering) cannot achieve satisfying results. Comparatively, by including coherent motions to evaluate inter-frame similarities (i.e., \u201cpre-clustering\u201d and \u201cour\u201d), the clustering accuracy can be improved. However, the pre-clustering method still have limitations in differentiating similar recurrent activities, e.g., HP and HU in Figs 14g and 14h. Comparatively, by introducing the importance cost of semantic regions to measure the effects of unmatched coherent motions, our frame-level clustering approach can have stronger capability in differentiating similar recurrent activity patterns.\nPerformances on coherent motion merging and flow curve extraction. Fig. 17 shows the results of our coherent\nmotion merging and flow curve extraction steps. Besides, we also compare our approach with a state-of-the-art activity mining method which utilizes a Probabilistic Latent Sequential Motif (PLSM) model to discover recurrent activities [11], which are shown as the last rows in Figs 17a, 17b, and 17c. From Fig. 17, we can have the following observations:\n1) The recurrent activities mined by our approach is similar to the human-observed activity types in Fig. 14. This demonstrates that our proposed cluster-and-merge process can effectively discover desired activity types from an input video. 2) Note that although the clustering result in our framelevel clustering step is not 100 percentage accurate (as in Fig. 16), the extracted flow curves are less affected by the wrongly clustered frames because: (i) The noisy or isolated thermal energy vectors from the wrongly clustered frames will be filtered by the threshold \u03b8mf in Eq. 13. (ii) The flow curve extraction process will further reduce the effects of wrong frames by dividing sub-regions to derive flow curves, as in Fig. 11a. 3) Comparing our approach with the PLSM-based method [11], we can see that: (i) By introducing coherent regions to measure inter-frame similarities and derive motion pattern regions, our approach can achieve cleaner activity flows which are more coherent with the humanobserved activity types in Fig. 14. Comparatively, results of the PLSM-based method still include noisy motion patterns, e.g., the last column in Fig. 17b. (ii) Our approach can precisely differentiate motion flows inside a recurrent activity. However, the PLSM-based method has limitations in differentiate motion flows when they are located close to each other, e.g., the second column in Fig. 17a. (iii) The differences between similar recurrent activities are clearly differentiated and visualized by our approach, while they are less obvious in the results of the PLSM-based method, e.g., the third and fourth columns in Fig. 17b."}, {"heading": "VIII. CONCLUSION", "text": "In this paper, we study the problem of coherent motion detection, semantic region construction, and recurrent activity mining in crowd scenes. A thermal-diffusion-based algorithm together with a two-step clustering scheme are introduced, which can achieve more meaningful coherent motion and semantic region results. Based on the extracted coherent motions and semantic regions, a cluster-and-merge process is further proposed which can effectively discover desirable activity patterns from a crowd video. Experiments on various videos show that our approach achieves the state-of-the-art performance."}], "references": [{"title": "Finding coherent motions and semantic regions in crowd scenes: a diffusion and clustering approach,", "author": ["W. Wang", "W. Lin", "Y. Chen", "J. Wu", "J. Wang", "B. Sheng"], "venue": "European Conf. Computer Vision (ECCV),", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2014}, {"title": "Learning motion patterns in crowded scenes using motion flow field,", "author": ["M. Hu", "S. Ali", "M. Shah"], "venue": "Intl. Conf. Pattern Recognition (ICPR), pp", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2008}, {"title": "Towards good practices for action video encoding,", "author": ["J. Wu", "Y. Zhang", "W. Lin"], "venue": "IEEE Conf. Computer Vision and Pattern Recognition (CVPR), pp. 2577\u20132584,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2014}, {"title": "Multi-camera activity correlation analysis,", "author": ["C.C. Loy", "T. Xiang", "S. Gong"], "venue": "IEEE Conf. Computer Vision and Pattern Recognition (CVPR),", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2009}, {"title": "Multimedia search reranking: A literature survey,", "author": ["T. Mei", "Y. Rui", "S. Li", "Q. Tian"], "venue": "ACM Computing Surveys,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2012}, {"title": "Finding perfect rendezvous on the go: accurate mobile visual localization and its applications to routing,", "author": ["H. Liu", "T. Mei", "J. Luo", "H. Li", "S. Li"], "venue": "ACM Intl. Conf. Multimedia (MM),", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2012}, {"title": "Crowd motion partitioning in a scattered motion field,", "author": ["S. Wu", "H. Wong"], "venue": "IEEE Trans. Systems, Man, and Cybernetics,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2012}, {"title": "Coherent filtering: detecting coherent motions from crowd clutters,", "author": ["B. Zhou", "X. Tang", "X. Wang"], "venue": "European Conf. Computer Vision (ECCV),", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2012}, {"title": "Measuring crowd collectiveness,", "author": ["B. Zhou", "X. Wang", "X. Tang"], "venue": "IEEE Conf. Computer Vision and Pattern Recognition (CVPR), pp. 3049\u20133056,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2013}, {"title": "Temporal analysis of motif mixtures using Dirichlet processes,", "author": ["R. Emonet", "J. Varadarajan", "J.-M. Odobez"], "venue": "IEEE Trans. Pattern Analysis and Machine Intelligence,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2013}, {"title": "A sequential topic model for mining recurrent activities from long term video logs,", "author": ["V. Jagannadan", "R. Emonet", "J.-M. Odobez"], "venue": "Intl. J. Computer Vision, vol. 103,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2013}, {"title": "Random field topic model for semantic region analysis in crowded scenes from tracklets,", "author": ["B. Zhou", "X. Wang", "X. Tang"], "venue": "IEEE Conf. Computer Vision and Pattern Recognition (CVPR),", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2011}, {"title": "Trajectory analysis and semantic region modeling using nonparametric Bayesian models,", "author": ["X. Wang", "K.T. Ma", "G. Ng", "E. Grimson"], "venue": "Intl. J. Computer Vision,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2011}, {"title": "An incremental DPMM-based method for trajectory clustering, modeling, and retrieval,", "author": ["W. Hu", "X. Li", "G. Tian", "S. Maybank", "Z. Zhang"], "venue": "IEEE Trans. Pattern Analysis and Machine Intelligence,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2013}, {"title": "Trajectory classification using switched dynamical Hidden Markov Models,", "author": ["J. Nascimento", "M.A.T. Figueiredo", "J.S. Marques"], "venue": "IEEE Trans. Image Processing,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2010}, {"title": "A Lagrangian particle dynamics approach for crowd flow segmentation and stability analysis,", "author": ["S. Ali", "M. Shah"], "venue": "IEEE Conf. Computer Vision and Pattern Recognition (CVPR), pp", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2007}, {"title": "Abnormal crowd behavior detection using social force model,", "author": ["R. Mehran", "A. Oyama", "M. Shah"], "venue": "IEEE Conf. Computer Vision and Pattern Recognition (CVPR),", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2009}, {"title": "Crowd analysis: a survey,", "author": ["B. Zhan", "D. Monekosso", "P. Remagnino", "S. Velastin", "L. Xu"], "venue": "Machine Vision and Applications,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2008}, {"title": "Motion competition: A variational approach to piecewise parametric motion segmentation,", "author": ["D. Cremers", "S. Soatto"], "venue": "Intl. J. Computer Vision, vol. 62,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2005}, {"title": "Colour, texture, and motion in level set based segmentation and tracking,", "author": ["T. Brox", "M. Rousson", "R. Deriche", "J. Weickert"], "venue": "Image and Vision Computing,", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2010}, {"title": "Abnormal detection using interaction energy potentials,", "author": ["X. Cui", "Q. Liu", "M. Gao", "D.N. Metaxas"], "venue": "IEEE Conf. Computer Vision and Pattern Recognition (CVPR),", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2011}, {"title": "Optical flow estimation for a periodic image sequence,", "author": ["L. Li", "Y. Yang"], "venue": "IEEE Trans. Image Processing,", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2010}, {"title": "Motion detail preserving optical flow estimation.", "author": ["L. Xu", "J. Jia", "Y. Matsushita"], "venue": "IEEE Trans. Pattern Analysis and Machine Intelligence,", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2012}, {"title": "Learning visual flows: a Lie algebraic approach,", "author": ["D. Lin", "E. Grimson", "J. Fisher"], "venue": "IEEE Conf. Computer Vision and Pattern Recognition (CVPR),", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2009}, {"title": "Optical flow computation using extended constraints,", "author": ["A. Bimbo", "P. Nesi", "J. Sanz"], "venue": "IEEE Trans. Image Processing,", "citeRegEx": "26", "shortCiteRegEx": "26", "year": 1996}, {"title": "Robust Anisotropic Diffusion,", "author": ["M.J. Black", "G. Sapiro", "D.H. Marimont", "D. Heeger"], "venue": "IEEE Trans. Image Processing,", "citeRegEx": "27", "shortCiteRegEx": "27", "year": 1998}, {"title": "Adaptive diffusion flow active contours for image segmentation,", "author": ["Y. Wu", "Y. Wang", "Y. Jia"], "venue": "Computer Vision and Image Understanding,", "citeRegEx": "28", "shortCiteRegEx": "28", "year": 2013}, {"title": "Anisotropic diffusion in image processing", "author": ["J. Weickert"], "venue": "Stuttgart: Teubner,", "citeRegEx": "29", "shortCiteRegEx": "29", "year": 1998}, {"title": "Single and multiple view detection, tracking and video analysis in crowded environments,", "author": ["T. Xu", "P. Peng", "X. Fang", "C. Su", "Y. Wang", "Y. Tian", "W. Zeng", "T. Huang"], "venue": "Intl. Conf. Advanced Video and Signal-Based Surveillance (AVSS),", "citeRegEx": "30", "shortCiteRegEx": "30", "year": 2012}, {"title": "Scene segmentation for behavior correlation,", "author": ["J. Li", "S. Gong", "T. Xiang"], "venue": "European Conf. Computer Vision (ECCV),", "citeRegEx": "31", "shortCiteRegEx": "31", "year": 2008}, {"title": "Lucas/Kanade meets Horn/Schunck: combining local and global optic flow methods,", "author": ["A. Bruh", "J. Weickert", "C. Schnorr"], "venue": "Intl. J. Computer Vision, vol. 61,", "citeRegEx": "32", "shortCiteRegEx": "32", "year": 2005}, {"title": "Conduction of heat in solids", "author": ["H. Carslaw", "J. Jaeger"], "venue": "Oxford Science Publications,", "citeRegEx": "33", "shortCiteRegEx": "33", "year": 1959}, {"title": "Incremental topological flipping works for regular triangulations,", "author": ["H. Edelsbrunner", "N. Shah"], "venue": "Algorithmica, vol. 15,", "citeRegEx": "34", "shortCiteRegEx": "34", "year": 1996}, {"title": "Digital image processing", "author": ["R. Gonzales", "R. Woods"], "venue": null, "citeRegEx": "35", "shortCiteRegEx": "35", "year": 2001}, {"title": "Inferring user image search goals under the implicit guidance of users,", "author": ["Z. Lu", "X. Yang", "W. Lin", "H. Zha", "X. Chen"], "venue": "IEEE Trans. Circuits and Systems for Video Technology,", "citeRegEx": "36", "shortCiteRegEx": "36", "year": 2014}, {"title": "LIBSVM: a library for support vector machines,", "author": ["C. Chang", "C. Lin"], "venue": "ACM Trans. Intelligent Systems Technology,", "citeRegEx": "37", "shortCiteRegEx": "37", "year": 2011}, {"title": "Object segmentation by long term analysis of point trajectories,", "author": ["T. Brox", "J. Malik"], "venue": "European Conf. Computer Vision (ECCV),", "citeRegEx": "40", "shortCiteRegEx": "40", "year": 2010}], "referenceMentions": [{"referenceID": 1, "context": "Since coherent motions can effectively decompose scenes into meaningful semantic parts and facilitate the analysis of complex crowd scenes, they are of increasing importance in crowd-scene understanding and activity recognition [2], [3], [4], [5], [6].", "startOffset": 228, "endOffset": 231}, {"referenceID": 2, "context": "Since coherent motions can effectively decompose scenes into meaningful semantic parts and facilitate the analysis of complex crowd scenes, they are of increasing importance in crowd-scene understanding and activity recognition [2], [3], [4], [5], [6].", "startOffset": 233, "endOffset": 236}, {"referenceID": 3, "context": "Since coherent motions can effectively decompose scenes into meaningful semantic parts and facilitate the analysis of complex crowd scenes, they are of increasing importance in crowd-scene understanding and activity recognition [2], [3], [4], [5], [6].", "startOffset": 238, "endOffset": 241}, {"referenceID": 4, "context": "Since coherent motions can effectively decompose scenes into meaningful semantic parts and facilitate the analysis of complex crowd scenes, they are of increasing importance in crowd-scene understanding and activity recognition [2], [3], [4], [5], [6].", "startOffset": 243, "endOffset": 246}, {"referenceID": 5, "context": "Since coherent motions can effectively decompose scenes into meaningful semantic parts and facilitate the analysis of complex crowd scenes, they are of increasing importance in crowd-scene understanding and activity recognition [2], [3], [4], [5], [6].", "startOffset": 248, "endOffset": 251}, {"referenceID": 0, "context": "The basic idea of this paper appeared in our conference version [1].", "startOffset": 64, "endOffset": 67}, {"referenceID": 6, "context": "Although many algorithms have been proposed for coherent motion detection [7], [8], [9], [2], this problem is not yet effectively addressed.", "startOffset": 74, "endOffset": 77}, {"referenceID": 7, "context": "Although many algorithms have been proposed for coherent motion detection [7], [8], [9], [2], this problem is not yet effectively addressed.", "startOffset": 79, "endOffset": 82}, {"referenceID": 8, "context": "Although many algorithms have been proposed for coherent motion detection [7], [8], [9], [2], this problem is not yet effectively addressed.", "startOffset": 84, "endOffset": 87}, {"referenceID": 1, "context": "Although many algorithms have been proposed for coherent motion detection [7], [8], [9], [2], this problem is not yet effectively addressed.", "startOffset": 89, "endOffset": 92}, {"referenceID": 9, "context": "Many crowd scenes are composed of recurrent activities [10], [11], [12].", "startOffset": 55, "endOffset": 59}, {"referenceID": 10, "context": "Many crowd scenes are composed of recurrent activities [10], [11], [12].", "startOffset": 61, "endOffset": 65}, {"referenceID": 11, "context": "Many crowd scenes are composed of recurrent activities [10], [11], [12].", "startOffset": 67, "endOffset": 71}, {"referenceID": 12, "context": "Although many researches have been done for parsing recurrent activities in low-crowd scenes [13], [14], [15], [16], this issue is not well addressed in crowd scene scenarios where reliable motion trajectories are unavailable.", "startOffset": 93, "endOffset": 97}, {"referenceID": 13, "context": "Although many researches have been done for parsing recurrent activities in low-crowd scenes [13], [14], [15], [16], this issue is not well addressed in crowd scene scenarios where reliable motion trajectories are unavailable.", "startOffset": 99, "endOffset": 103}, {"referenceID": 14, "context": "Although many researches have been done for parsing recurrent activities in low-crowd scenes [13], [14], [15], [16], this issue is not well addressed in crowd scene scenarios where reliable motion trajectories are unavailable.", "startOffset": 111, "endOffset": 115}, {"referenceID": 15, "context": "Many works [17], [7], [8], [9], [2], [18], [19], [20], [21], [22] have been proposed on coherent motion detection.", "startOffset": 11, "endOffset": 15}, {"referenceID": 6, "context": "Many works [17], [7], [8], [9], [2], [18], [19], [20], [21], [22] have been proposed on coherent motion detection.", "startOffset": 17, "endOffset": 20}, {"referenceID": 7, "context": "Many works [17], [7], [8], [9], [2], [18], [19], [20], [21], [22] have been proposed on coherent motion detection.", "startOffset": 22, "endOffset": 25}, {"referenceID": 8, "context": "Many works [17], [7], [8], [9], [2], [18], [19], [20], [21], [22] have been proposed on coherent motion detection.", "startOffset": 27, "endOffset": 30}, {"referenceID": 1, "context": "Many works [17], [7], [8], [9], [2], [18], [19], [20], [21], [22] have been proposed on coherent motion detection.", "startOffset": 32, "endOffset": 35}, {"referenceID": 16, "context": "Many works [17], [7], [8], [9], [2], [18], [19], [20], [21], [22] have been proposed on coherent motion detection.", "startOffset": 37, "endOffset": 41}, {"referenceID": 17, "context": "Many works [17], [7], [8], [9], [2], [18], [19], [20], [21], [22] have been proposed on coherent motion detection.", "startOffset": 43, "endOffset": 47}, {"referenceID": 18, "context": "Many works [17], [7], [8], [9], [2], [18], [19], [20], [21], [22] have been proposed on coherent motion detection.", "startOffset": 49, "endOffset": 53}, {"referenceID": 19, "context": "Many works [17], [7], [8], [9], [2], [18], [19], [20], [21], [22] have been proposed on coherent motion detection.", "startOffset": 55, "endOffset": 59}, {"referenceID": 20, "context": "Many works [17], [7], [8], [9], [2], [18], [19], [20], [21], [22] have been proposed on coherent motion detection.", "startOffset": 61, "endOffset": 65}, {"referenceID": 18, "context": "Cremers and Soatto [20] and Brox et al.", "startOffset": 19, "endOffset": 23}, {"referenceID": 19, "context": "[21] model the intensity variation of optical flow by an objective functional minimization scheme.", "startOffset": 0, "endOffset": 4}, {"referenceID": 6, "context": "Other works introduce external spatial-temporal correlation traits to model the motion coherency among particles [7], [8], [9].", "startOffset": 113, "endOffset": 116}, {"referenceID": 7, "context": "Other works introduce external spatial-temporal correlation traits to model the motion coherency among particles [7], [8], [9].", "startOffset": 118, "endOffset": 121}, {"referenceID": 8, "context": "Other works introduce external spatial-temporal correlation traits to model the motion coherency among particles [7], [8], [9].", "startOffset": 123, "endOffset": 126}, {"referenceID": 21, "context": "These methods try to improve the estimation accuracy of the input motion field by including global constraints over particles [23], [24], [25], [26].", "startOffset": 126, "endOffset": 130}, {"referenceID": 22, "context": "These methods try to improve the estimation accuracy of the input motion field by including global constraints over particles [23], [24], [25], [26].", "startOffset": 132, "endOffset": 136}, {"referenceID": 23, "context": "These methods try to improve the estimation accuracy of the input motion field by including global constraints over particles [23], [24], [25], [26].", "startOffset": 138, "endOffset": 142}, {"referenceID": 24, "context": "These methods try to improve the estimation accuracy of the input motion field by including global constraints over particles [23], [24], [25], [26].", "startOffset": 144, "endOffset": 148}, {"referenceID": 25, "context": "The anisotropic diffusion based methods, used in image segmentation, is also related to our work [27], [28], [29].", "startOffset": 97, "endOffset": 101}, {"referenceID": 26, "context": "The anisotropic diffusion based methods, used in image segmentation, is also related to our work [27], [28], [29].", "startOffset": 103, "endOffset": 107}, {"referenceID": 27, "context": "The anisotropic diffusion based methods, used in image segmentation, is also related to our work [27], [28], [29].", "startOffset": 109, "endOffset": 113}, {"referenceID": 15, "context": "In [17], Ali and Shah detected instability regions in a scene by comparing with its normal coherent motions.", "startOffset": 3, "endOffset": 7}, {"referenceID": 2, "context": "Furthermore, besides the works on coherent motion, there are also other works which directly extract global features from the entire scene to recognize crowd activities [3], [30].", "startOffset": 169, "endOffset": 172}, {"referenceID": 28, "context": "Furthermore, besides the works on coherent motion, there are also other works which directly extract global features from the entire scene to recognize crowd activities [3], [30].", "startOffset": 174, "endOffset": 178}, {"referenceID": 3, "context": "Although there are some works [4], [31] which recognize crowd activities by segmenting scenes into semantic regions, our approach differs from them.", "startOffset": 30, "endOffset": 33}, {"referenceID": 29, "context": "Although there are some works [4], [31] which recognize crowd activities by segmenting scenes into semantic regions, our approach differs from them.", "startOffset": 35, "endOffset": 39}, {"referenceID": 12, "context": "[13] and Hu et al.", "startOffset": 0, "endOffset": 4}, {"referenceID": 13, "context": "[14] further introduced Dirichlet processes to model the activity patterns of different trajectory groups.", "startOffset": 0, "endOffset": 4}, {"referenceID": 11, "context": "[12] extracted fragments of trajectories (called tracklets)", "startOffset": 0, "endOffset": 4}, {"referenceID": 10, "context": "[11] and Emonet et al.", "startOffset": 0, "endOffset": 4}, {"referenceID": 9, "context": "[10] extracted low-level motion flows as motion descriptors and introduced a Probabilistic Latent Sequential Motif (PLSM) model to achieve recurrent activities.", "startOffset": 0, "endOffset": 4}, {"referenceID": 15, "context": "The optical flow fields [17], [32] are first extracted from input videos.", "startOffset": 24, "endOffset": 28}, {"referenceID": 30, "context": "The optical flow fields [17], [32] are first extracted from input videos.", "startOffset": 30, "endOffset": 34}, {"referenceID": 31, "context": "physical thermal propagation [33] and model the thermal diffusion process by Eq.", "startOffset": 29, "endOffset": 33}, {"referenceID": 27, "context": "The inclusion of this term is one of the major differences between the proposed approach and the anisotropic-diffusion methods [29].", "startOffset": 127, "endOffset": 131}, {"referenceID": 15, "context": "It is very difficult to include these particles into the coherent region by traditional methods [17], [7], [8], [9] because they are", "startOffset": 96, "endOffset": 100}, {"referenceID": 6, "context": "It is very difficult to include these particles into the coherent region by traditional methods [17], [7], [8], [9] because they are", "startOffset": 102, "endOffset": 105}, {"referenceID": 7, "context": "It is very difficult to include these particles into the coherent region by traditional methods [17], [7], [8], [9] because they are", "startOffset": 107, "endOffset": 110}, {"referenceID": 8, "context": "It is very difficult to include these particles into the coherent region by traditional methods [17], [7], [8], [9] because they are", "startOffset": 112, "endOffset": 115}, {"referenceID": 32, "context": "In this step, we randomly sample particles from the entire scene and apply the triangulation process [34] to link the sampled particles.", "startOffset": 101, "endOffset": 105}, {"referenceID": 32, "context": "3 shows one triangulation result, where red dots are the sampled particles and the lines are links created by the triangulation process [34].", "startOffset": 136, "endOffset": 140}, {"referenceID": 33, "context": "algorithm [35].", "startOffset": 10, "endOffset": 14}, {"referenceID": 34, "context": "Then, we construct a similarity graph for the M coherent motions, and perform clustering [36] on this similarity graph with the optimal number of clusters being determined automatically, the cluster results are grouped coherent regions.", "startOffset": 89, "endOffset": 93}, {"referenceID": 3, "context": "Comparing with previous semantic region segmentation methods [4], [31] which perform clustering using local similarity among particles, our scheme utilizes the guidance from the global coherent motion clustering results to strengthen the correlations among particles.", "startOffset": 61, "endOffset": 64}, {"referenceID": 29, "context": "Comparing with previous semantic region segmentation methods [4], [31] which perform clustering using local similarity among particles, our scheme utilizes the guidance from the global coherent motion clustering results to strengthen the correlations among particles.", "startOffset": 66, "endOffset": 70}, {"referenceID": 35, "context": "Then, a linear support vector machine (SVM) [37] is utilized to train and recognize predefined activities.", "startOffset": 44, "endOffset": 48}, {"referenceID": 34, "context": "In this paper, we first calculate inter-frame similarities for all frame pairs and then utilize spectral clustering [36] to cluster frames according to these inter-frame similarities.", "startOffset": 116, "endOffset": 120}, {"referenceID": 30, "context": "Our approach is implemented by Matlab and the optical flow fields [32] are used as the input motion vector fields while each", "startOffset": 66, "endOffset": 70}, {"referenceID": 15, "context": "In order to achieve motion vector fields with T -frame intervals (T = 10 in our experiments), the particle advection method [17] is used which tracks the movement of each particle over T frames.", "startOffset": 124, "endOffset": 128}, {"referenceID": 15, "context": "We perform experiments on a dataset including 30 different crowd videos collected from the UCF dataset [17], the UCSD dataset [39], the CUHK dataset [9], and our own collected set.", "startOffset": 103, "endOffset": 107}, {"referenceID": 8, "context": "We perform experiments on a dataset including 30 different crowd videos collected from the UCF dataset [17], the UCSD dataset [39], the CUHK dataset [9], and our own collected set.", "startOffset": 149, "endOffset": 152}, {"referenceID": 15, "context": "(a): Ground Truth, (b): Results of our approach, (c): Results of [17], (d): Results of [7], (e): Results of [8], (f): Results of [9], (g): Results of [40], (h): Results of [28].", "startOffset": 65, "endOffset": 69}, {"referenceID": 6, "context": "(a): Ground Truth, (b): Results of our approach, (c): Results of [17], (d): Results of [7], (e): Results of [8], (f): Results of [9], (g): Results of [40], (h): Results of [28].", "startOffset": 87, "endOffset": 90}, {"referenceID": 7, "context": "(a): Ground Truth, (b): Results of our approach, (c): Results of [17], (d): Results of [7], (e): Results of [8], (f): Results of [9], (g): Results of [40], (h): Results of [28].", "startOffset": 108, "endOffset": 111}, {"referenceID": 8, "context": "(a): Ground Truth, (b): Results of our approach, (c): Results of [17], (d): Results of [7], (e): Results of [8], (f): Results of [9], (g): Results of [40], (h): Results of [28].", "startOffset": 129, "endOffset": 132}, {"referenceID": 36, "context": "(a): Ground Truth, (b): Results of our approach, (c): Results of [17], (d): Results of [7], (e): Results of [8], (f): Results of [9], (g): Results of [40], (h): Results of [28].", "startOffset": 150, "endOffset": 154}, {"referenceID": 26, "context": "(a): Ground Truth, (b): Results of our approach, (c): Results of [17], (d): Results of [7], (e): Results of [8], (f): Results of [9], (g): Results of [40], (h): Results of [28].", "startOffset": 172, "endOffset": 176}, {"referenceID": 33, "context": "Input: A motion pattern region R\u03a8j merged from coherent region cluster \u03a8j Output: A flow curve extracted from R\u03a8j 1: Calculate the skeleton of R\u03a8j [35] 2: Find the end point Ps of the skeleton which is on \u201cbackward\u201d position to all other end points, where the \u201cbackward\u201d direction is defined as the reversed direction of the motion flows in R\u03a8j 3: PK = Ps, where PK is the current segmentation point 4: while PK+1 is inside R\u03a8j do 5: Pmov,s as the middle point of the line perpendicular to the motion vector EPK ,\u03a8j at PK 6: for n=0 to Nummov {Nummov is the number of movements} do 7: Move from Pmov,s to Pmov,e by EPmov,s,\u03a8j , where EPmov,s,\u03a8j is motion vector at Pmov,s in R\u03a8j 8: Pmov,s=Pmov,e 9: end for", "startOffset": 147, "endOffset": 151}, {"referenceID": 15, "context": "dynamics approach [17], the local-translation domain segmentation approach [7], the coherent-filtering approach [8], and the collectiveness measuring-based approach [9].", "startOffset": 18, "endOffset": 22}, {"referenceID": 6, "context": "dynamics approach [17], the local-translation domain segmentation approach [7], the coherent-filtering approach [8], and the collectiveness measuring-based approach [9].", "startOffset": 75, "endOffset": 78}, {"referenceID": 7, "context": "dynamics approach [17], the local-translation domain segmentation approach [7], the coherent-filtering approach [8], and the collectiveness measuring-based approach [9].", "startOffset": 112, "endOffset": 115}, {"referenceID": 8, "context": "dynamics approach [17], the local-translation domain segmentation approach [7], the coherent-filtering approach [8], and the collectiveness measuring-based approach [9].", "startOffset": 165, "endOffset": 168}, {"referenceID": 36, "context": "include the results of a general motion segmentation method [40] and an anisotropic-diffusion-based image segmentation method [28].", "startOffset": 60, "endOffset": 64}, {"referenceID": 26, "context": "include the results of a general motion segmentation method [40] and an anisotropic-diffusion-based image segmentation method [28].", "startOffset": 126, "endOffset": 130}, {"referenceID": 15, "context": "Comparatively, the method in [17] can only detect part of the circle while the methods in [8] and [9] fail to work since few reliable key points are extracted from this overcrowded scene.", "startOffset": 29, "endOffset": 33}, {"referenceID": 7, "context": "Comparatively, the method in [17] can only detect part of the circle while the methods in [8] and [9] fail to work since few reliable key points are extracted from this overcrowded scene.", "startOffset": 90, "endOffset": 93}, {"referenceID": 8, "context": "Comparatively, the method in [17] can only detect part of the circle while the methods in [8] and [9] fail to work since few reliable key points are extracted from this overcrowded scene.", "startOffset": 98, "endOffset": 101}, {"referenceID": 36, "context": "Furthermore, the methods in [40] and [28] do not show satisfying results, e.", "startOffset": 28, "endOffset": 32}, {"referenceID": 26, "context": "Furthermore, the methods in [40] and [28] do not show satisfying results, e.", "startOffset": 37, "endOffset": 41}, {"referenceID": 36, "context": "This is because: (1) the crowd scenes are extremely complicated such that the extracted particle flows or trajectories become unreliable, thus making the general motion segmentation methods [40] difficult to create precise results; (2) Since many coherent region boundaries in the crowd motion fields are rather vague and unrecognizable, good boundaries cannot be easily achieved without suitably utilizing the characteristics of the motion vector fields.", "startOffset": 190, "endOffset": 194}, {"referenceID": 26, "context": "Thus, simply applying the existing anisotropicdiffusion segmentation methods [28] cannot achieve satisfying results.", "startOffset": 77, "endOffset": 81}, {"referenceID": 15, "context": "Proposed [17] [7] [8] [9] [40] [28] PER (%) 7.", "startOffset": 9, "endOffset": 13}, {"referenceID": 6, "context": "Proposed [17] [7] [8] [9] [40] [28] PER (%) 7.", "startOffset": 14, "endOffset": 17}, {"referenceID": 7, "context": "Proposed [17] [7] [8] [9] [40] [28] PER (%) 7.", "startOffset": 18, "endOffset": 21}, {"referenceID": 8, "context": "Proposed [17] [7] [8] [9] [40] [28] PER (%) 7.", "startOffset": 22, "endOffset": 25}, {"referenceID": 36, "context": "Proposed [17] [7] [8] [9] [40] [28] PER (%) 7.", "startOffset": 26, "endOffset": 30}, {"referenceID": 26, "context": "Proposed [17] [7] [8] [9] [40] [28] PER (%) 7.", "startOffset": 31, "endOffset": 35}, {"referenceID": 6, "context": "15 compares the results of four methods: (1) Our approach (\u201cOur\u201d), (2) Directly cluster regions based on the particles\u2019 TEF vectors (\u201cDirect\u201d, note that our approach differs from this method by clustering over the cluster label vectors), (3) Use [7] to achieve coherent motion regions and then apply our two-step clustering scheme to construct semantic regions (\u201c[7]+Two-Step\u201d, we show the results of [7] because in our experiments, [7] has the best semantic region construction results among the compared methods in Table I), (4) The activity-based scene segmentation method in [4] (\u201c[4]\u201d).", "startOffset": 246, "endOffset": 249}, {"referenceID": 6, "context": "15 compares the results of four methods: (1) Our approach (\u201cOur\u201d), (2) Directly cluster regions based on the particles\u2019 TEF vectors (\u201cDirect\u201d, note that our approach differs from this method by clustering over the cluster label vectors), (3) Use [7] to achieve coherent motion regions and then apply our two-step clustering scheme to construct semantic regions (\u201c[7]+Two-Step\u201d, we show the results of [7] because in our experiments, [7] has the best semantic region construction results among the compared methods in Table I), (4) The activity-based scene segmentation method in [4] (\u201c[4]\u201d).", "startOffset": 363, "endOffset": 366}, {"referenceID": 6, "context": "15 compares the results of four methods: (1) Our approach (\u201cOur\u201d), (2) Directly cluster regions based on the particles\u2019 TEF vectors (\u201cDirect\u201d, note that our approach differs from this method by clustering over the cluster label vectors), (3) Use [7] to achieve coherent motion regions and then apply our two-step clustering scheme to construct semantic regions (\u201c[7]+Two-Step\u201d, we show the results of [7] because in our experiments, [7] has the best semantic region construction results among the compared methods in Table I), (4) The activity-based scene segmentation method in [4] (\u201c[4]\u201d).", "startOffset": 401, "endOffset": 404}, {"referenceID": 6, "context": "15 compares the results of four methods: (1) Our approach (\u201cOur\u201d), (2) Directly cluster regions based on the particles\u2019 TEF vectors (\u201cDirect\u201d, note that our approach differs from this method by clustering over the cluster label vectors), (3) Use [7] to achieve coherent motion regions and then apply our two-step clustering scheme to construct semantic regions (\u201c[7]+Two-Step\u201d, we show the results of [7] because in our experiments, [7] has the best semantic region construction results among the compared methods in Table I), (4) The activity-based scene segmentation method in [4] (\u201c[4]\u201d).", "startOffset": 433, "endOffset": 436}, {"referenceID": 3, "context": "15 compares the results of four methods: (1) Our approach (\u201cOur\u201d), (2) Directly cluster regions based on the particles\u2019 TEF vectors (\u201cDirect\u201d, note that our approach differs from this method by clustering over the cluster label vectors), (3) Use [7] to achieve coherent motion regions and then apply our two-step clustering scheme to construct semantic regions (\u201c[7]+Two-Step\u201d, we show the results of [7] because in our experiments, [7] has the best semantic region construction results among the compared methods in Table I), (4) The activity-based scene segmentation method in [4] (\u201c[4]\u201d).", "startOffset": 579, "endOffset": 582}, {"referenceID": 3, "context": "15 compares the results of four methods: (1) Our approach (\u201cOur\u201d), (2) Directly cluster regions based on the particles\u2019 TEF vectors (\u201cDirect\u201d, note that our approach differs from this method by clustering over the cluster label vectors), (3) Use [7] to achieve coherent motion regions and then apply our two-step clustering scheme to construct semantic regions (\u201c[7]+Two-Step\u201d, we show the results of [7] because in our experiments, [7] has the best semantic region construction results among the compared methods in Table I), (4) The activity-based scene segmentation method in [4] (\u201c[4]\u201d).", "startOffset": 585, "endOffset": 588}, {"referenceID": 6, "context": "15 shows that the methods utilizing \u201ccoherent motion cluster label\u201d information (\u201cour\u201d and \u201c[7]+two-step\u201d) create more meaningful semantic regions than the other methods,", "startOffset": 92, "endOffset": 95}, {"referenceID": 6, "context": "Furthermore, comparing our approach with the \u201c[7]+TwoStep\u201d method, it is obvious that the semantic regions by our", "startOffset": 46, "endOffset": 49}, {"referenceID": 6, "context": "After that, we (a) Original (b) Our (c) Direct (d) [7] (e) [4]", "startOffset": 51, "endOffset": 54}, {"referenceID": 3, "context": "After that, we (a) Original (b) Our (c) Direct (d) [7] (e) [4]", "startOffset": 59, "endOffset": 62}, {"referenceID": 6, "context": "(f) Original (g) Our (h) Direct (i) [7] (j) [4]", "startOffset": 36, "endOffset": 39}, {"referenceID": 3, "context": "(f) Original (g) Our (h) Direct (i) [7] (j) [4]", "startOffset": 44, "endOffset": 47}, {"referenceID": 6, "context": "(k) Original (l) Our (m) Direct (n) [7] (o) [4]", "startOffset": 36, "endOffset": 39}, {"referenceID": 3, "context": "(k) Original (l) Our (m) Direct (n) [7] (o) [4]", "startOffset": 44, "endOffset": 47}, {"referenceID": 6, "context": "The caption \u201c[7]\u201d denotes the method \u201c[7]+Two step\u201d.", "startOffset": 13, "endOffset": 16}, {"referenceID": 6, "context": "The caption \u201c[7]\u201d denotes the method \u201c[7]+Two step\u201d.", "startOffset": 38, "endOffset": 41}, {"referenceID": 6, "context": "Our (%) Our+ OF (%) Direct (%) [7]+TwoStep (%) [4] (%) [3] (%) Fig.", "startOffset": 31, "endOffset": 34}, {"referenceID": 3, "context": "Our (%) Our+ OF (%) Direct (%) [7]+TwoStep (%) [4] (%) [3] (%) Fig.", "startOffset": 47, "endOffset": 50}, {"referenceID": 2, "context": "Our (%) Our+ OF (%) Direct (%) [7]+TwoStep (%) [4] (%) [3] (%) Fig.", "startOffset": 55, "endOffset": 58}, {"referenceID": 2, "context": "Besides, we also include the results of two additional methods: (1) a state-ofthe-art dense-trajectory-based recognition method [3] (\u201cDenseTraj\u201d); (2) the method which uses our semantic regions but uses the input motion field (i.", "startOffset": 128, "endOffset": 131}, {"referenceID": 6, "context": ", \u201cour\u201d, \u201cour+OF\u201d, and \u201c[7]+Two step\u201d) achieve better results than other methods.", "startOffset": 24, "endOffset": 27}, {"referenceID": 2, "context": "3) The dense-trajectory method [3] which extracts global features does not achieve satisfying results.", "startOffset": 31, "endOffset": 34}, {"referenceID": 10, "context": "Besides, we also compare our approach with a state-of-the-art activity mining method which utilizes a Probabilistic Latent Sequential Motif (PLSM) model to discover recurrent activities [11], which are shown as the last rows in Figs 17a, 17b, and 17c.", "startOffset": 186, "endOffset": 190}, {"referenceID": 10, "context": "3) Comparing our approach with the PLSM-based method [11], we can see that: (i) By introducing coherent regions to measure inter-frame similarities and derive motion pattern regions, our approach can achieve cleaner activity flows which are more coherent with the humanobserved activity types in Fig.", "startOffset": 53, "endOffset": 57}], "year": 2016, "abstractText": "This paper addresses the problem of detecting coherent motions in crowd scenes and presents its two applications in crowd scene understanding: semantic region detection and recurrent activity mining. It processes input motion fields (e.g., optical flow fields) and produces a coherent motion filed, named as thermal energy field. The thermal energy field is able to capture both motion correlation among particles and the motion trends of individual particles which are helpful to discover coherency among them. We further introduce a two-step clustering process to construct stable semantic regions from the extracted time-varying coherent motions. These semantic regions can be used to recognize pre-defined activities in crowd scenes. Finally, we introduce a cluster-and-merge process which automatically discovers recurrent activities in crowd scenes by clustering and merging the extracted coherent motions. Experiments on various videos demonstrate the effectiveness of our approach.", "creator": "LaTeX with hyperref package"}}}