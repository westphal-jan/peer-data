{"id": "1602.04936", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "16-Feb-2016", "title": "Reinforcement Learning approach for Real Time Strategy Games Battle city and S3", "abstract": "In this paper we proposed reinforcement learning algorithms with the generalized reward function. In our proposed method we use Q-learning and SARSA algorithms with generalised reward function to train the reinforcement learning agent. We evaluated the performance of our proposed algorithms on two real-time strategy games called BattleCity and S3: an AI game. The reward function performs a deep neural network and we show a demonstration with a simplified learning algorithm for reward learning. We also showed that the generalised reward function is able to reduce the amount of memory needed for a real-time game by more than 5% compared to the average reward function.\n\n\n\n\n\nThe algorithm is simple. The function uses a gradient learning algorithm to train the reinforcement learning agent and then it is used to train the reinforcement learning agent. In order to train the reward function in the real-time strategy game, the algorithm performs a deep neural network and we show a demonstration with a simplified learning algorithm for reward learning.\n\nThe algorithm is complex. The neural network consists of the following algorithms:\nFor example, we want to train a given problem for a task, and we want to train a specific strategy, and the target strategy will have a different response than the other problem. This task is a one-way train, which is set up in a very simple way. It is a non-linear one-way strategy, which is based on one-way classification with a specific goal and can be repeated in many different ways. It is called a non-linear one-way model, which is based on a one-way classification with a certain goal and can be repeated in many different ways.\nWe want to create a simple algorithm which trains the reward function for the task and does not require training. To train the reward function, we will require training of a different strategy for the reward function, but training of the reward function for the reward function for the reward function for the reward function for the reward function for the reward function for the reward function for the reward function for the reward function for the reward function for the reward function for the reward function for the reward function for the reward function for the reward function for the reward function for the reward function for the reward function for the reward function for the reward function for the reward function for the reward function for the reward function for the reward function for the reward function for the reward function for the reward function for the reward function for the reward function for the reward function for the reward function for the reward function for the reward function for the reward", "histories": [["v1", "Tue, 16 Feb 2016 08:17:17 GMT  (3449kb,D)", "http://arxiv.org/abs/1602.04936v1", "13 pages, vol 9 issue 4 of IJIP"]], "COMMENTS": "13 pages, vol 9 issue 4 of IJIP", "reviews": [], "SUBJECTS": "cs.AI", "authors": ["harshit sethy", "amit patel"], "accepted": false, "id": "1602.04936"}, "pdf": {"name": "1602.04936.pdf", "metadata": {"source": "CRF", "title": "Reinforcement Learning approach for Real Time Strategy Games Battle city and S3", "authors": ["Harshit Sethy", "Amit Patel"], "emails": ["hsethy1@gmail.com", "amtptl93@gmail.com"], "sections": [{"heading": null, "text": "Reinforcement Learning approach for Real Time Strategy Games Battle city and S3\nHarshit Sethya, Amit Patelb\naCTO of Gymtrekker Fitness Private Limited,Mumbai, India, Email: hsethy1@gmail.com\nbAssistant Professor, Department of Computer Science and Engineering, RGUKT IIIT Nuzvid, Krishna-521202 India, Email: amtptl93@gmail.com\nIn this paper we proposed reinforcement learning algorithms with the generalized reward function. In our proposed method we use Q-learning and SARSA algorithms with generalised reward function to train the reinforcement learning agent. We evaluated the performance of our proposed algorithms on two real-time strategy games called BattleCity and S3. There are two main advantages of having such an approach as compared to other works in RTS. (1) We can ignore the concept of a simulator which is often game specific and is usually hard coded in any type of RTS games (2) our system can learn from interaction with any opponents and quickly change the strategy according to the opponents and do not need any human traces as used in previous works.\nKeywords : Reinforcement learning, Machine learning, Real time strategy, Artificial intelligence."}, {"heading": "1. INTRODUCTION", "text": "Existence of a good artificial intelligence(AI) technique in the background of a game is one of the major factor for the fun and re-play ability in commercial computer games. Although AI has been applied successfully in several games such as chess, backgammon or checkers when it comes to real-time games the pre-defined scripts which is usually used to simulate the artificial intelligence in chess, backgammon etc [11]. does not seem to work. This is because in real-time games decisions has to be made in real-time as well as the search space is huge and as such they do not contain any true AI for learning [2]. Traditional planning approaches are difficult in case of RTS games because they have various factors like huge decision spaces, adversarial domains, partiallyobservable, non-deterministic and real-time, (real time means while deciding the best actions, the game continues running and states change simultaneously)."}, {"heading": "1.1. Real Time Strategy Games", "text": "Today game developing companies have started showing more interest in RTS games. Unlike turn based strategy games, where one has the ability to take ones own time, in real time strategy games,\nall movement, construction, combat etc., are all occurring in real time. In a typical RTS game, the screen contains a map area which consists of the game world with buildings, units and terrain. There are usually several players in an RTS game. Other than the players there are various game entities called participants, units and structures. These are under the control of the players and the players need to save their assets and/or destroy assets of the opponent players by making use of their control over the entities. We are using 2 RTS games (1) BattleCity and (2) S3 game for our evaluation. A snapshot of two RTS games called BattleCity and S3 are given in Figure 1."}, {"heading": "1.2. BattleCity Game", "text": "BattleCity is a multidirectional shooter video game, which can be played using two basic actions Move and Fire. The player, controlling a tank, must destroy enemy tanks or enemy base and also protect its own base. Player can move tank in four directions (left, right, up and down) and fire bullets in whichever direction the tank last moved, while bases are static. There are three types of obstacle. (1) Brick wall tank can destroy it by firing this type wall. (2) Marble wall tank cant destroy it by firing. (3) Water\n1\nar X\niv :1\n60 2.\n04 93\n6v 1\n[ cs\n.A I]\n1 6\nFe b\n20 16\nbodies tank can fire through it. Tank cant pass through any of above obstacle. Only brick wall can be destroyed by tank so after destroying tank can pass through it."}, {"heading": "1.3. S3 Game", "text": "S3 is a real-time strategy game where each players goal is to remain alive after destroying the rest of the players. Four basic actions in this game are Harvest : i.e., to gather resources (gold and wood), Build : to build buildings (Barrack, Blacksmith, Tower etc) ,Train: to produce troops (archers, footmen, catapults, knights), Attack : for attacking enemy.\nThis paper is structured as follows. Apart from introduction, there are five more sections.In section 2 highlights the review of related works. In section 3 we discuss about reinforcement learning techniques in real-time-strategy games and outline the various learning algorithms used in reinforcement learning. In section 4 we outline implementation details related to the proposed reinforcement learning algorithms with the generalized reward function for two real-time-strategy games (1) BattleCity and (2) S3 game. Section 5 discusses about the experimental result related to our proposed work for BattleCity and S3. We conclude with section 6."}, {"heading": "2. Related Work", "text": "One of the major works using Online casebased planning [6] techniques for Real Time Strategy Games was published in [9]. On-line casebased planning revises case based planning for strategic real-time domains involving on-line planning.\nIn [8] a case-based planning system called Darmok2 is introduced that can play RTS games. They introduced a set of algorithms that can be used to learn plans, represented as petri-nets, from one or more human demonstrations. Another work by the same authors which uses Darmok2 but addresses the issues of plan acquisition, on-line plan execution, interleaved planning and execution and on-line plan adaptation is [7].\nIn [3] the authors summarize their work in exploring the use of the first order inductive learning (FOIL) algorithm for learning rules which can be used to represent opponent strategies.\nIn [13] the authors improve Darmok2 using information related to sensors of the game. We refer to that work as PR-Model in this paper. PR-model is capable of learning how to play RTS games by observing human demonstrations. Using human traces PR-model makes plans to play games. Prioritize the plan according to the feedback of the game and feedbacks are decided using some rule which depends on the sensors of the game.\nDrawbacks of all case based learning [4] approaches as mentioned above are (1) It requires expert demonstrations for making plans (2) after training is done, no further learning takes place (3) to cover large state spaces it would require large number of rules in the plan base (4) no exploration for optimal solution. Only follows human traces.. Stefan Wender [5] uses Reinforcement Learning for City Site Selection in the TurnBased Strategy Game Civilization IV. Civilization IV is the strategy game it is a turn-based game while Battle City is Real time game.\nStefan Wender [5] uses Reinforcement Learning for City Site Selection in the Turn-Based Strategy Game Civilization IV. Civilization IV is the strategy game similar to S3 but it is a turn-based game while S3 is Real time multi agent game.\nIn this paper we aim to do away with the hard coded simulator and propose a learning approach based on Reinforcement Learning [1](RL) wherein sensor information from the current game-state is used to select the best action. Reinforcement learning is used because of its advantages over previous strategies. Specifically (1) RL cuts out the need to manually specify rules. RL agents learn simply by playing the game against other human players or even other RL agents (2) for large state spaces, RL can be combined with a function approximator such as a neural network, to approximate the evaluation function (3) RL agent always explores for optimal solution to reach the goal (4) RL has been applied widely to many other fields, such as robotics, board games ,turn based games and single agent games with great results, but hardly ever on RTS multi-agent games."}, {"heading": "3. Reinforcement Learning", "text": "Reinforcement Learning [1] is the field of Machine Learning which deals with what to do, how to map situations to actions so as to maximize a numerical reward signal.The learner does not know which actions to take, as in most forms of machine learning, but instead must discover which actions gives the most reward by applying them. In the most interesting and challenging cases, actions may affect not only the immediate\nreward but also the next situation and, through that, all subsequent rewards.\nWith comparing reinforcement learning [12] to RTS game environment an AI player learns by interacting with the environment and observing the feed-backs of these interactions. This is same as the fundamental way in which humans (and animals) learn. As a human, we can perform actions and observe the results of these actions on the environment. The same way RL-agent interacts with the environment and observes the result and assign the reward or penalty to state or state-action pair according to the desirability of the resultant state."}, {"heading": "3.1. Reinforcement Learning Architecture", "text": "RL Architecture has two main characteristics; one is learning and the other is playing with\nthe learnt experiences. Initially RLearner has no Knowledge about the game. So it does random actions and observe the resultant state using some sensor information of the game and give feedback (in the form of reward which is further used to calculate the Q-Values for the state-action pairs or Q-Table) of that action to the previous state according to the desirability of the current state. Q-Values of the state-action pairs are known as Q-Table which define a policy. After every action policy updates Q-Values for the state action pairs (Q-Table) this policy is used to predict the best action while playing the game. RL agent learns while playing so it again gives feedback and the whole process it going on till the end of the game."}, {"heading": "3.2. Basic components of RL", "text": "Reinforcement learning contains five basic components which are as listed below.\n1. a set of environment states S\n2. a set of actions A\n3. rules of transitioning between states\n4. rules that determine the scalar immediate reward of a transition (Reward Functions)\n5. rules that describe what the agent observes (Value Functions)"}, {"heading": "3.2.1. Reward Function", "text": "The scalar value which represents the degree to which a state or action is desirable is known as reward. This scalar reward is assigned to the action for the particular transition and the resultant state of the game. If the resultant state is desirable and safe then positive scalar value as reward will be assigned to that action otherwise if state is not safe or undesirable then some negative scalar value as negative reward will be assigned to that action. We are using 2 types of Reward function (1) Conditional Reward function (2) Generalised Reward function."}, {"heading": "3.2.2. Value Function", "text": "Value Functions are used for mapping from states or from state-action pairs to real numbers, where the value of a state represents the longterm reward achieved starting from that state (or\nstate-action), and executing a particular policy. It estimates how good a particular action will be in a given state, or what the return for that action is expected to be. There are two type of value functions.\n1. V \u03c0(s) is the value of a state \u2019s\u2019 under policy \u03c0. The expected return when starting in s and following \u03c0 thereafter.\n2. Q\u03c0(s, a) is the value of taking action \u2019a\u2019 in state \u2019s\u2019 under a policy \u03c0. The expected return when starting from s taking the action a and thereafter following policy \u03c0.\nThere are two methods to define these value functions:\n1. Monte Carlo [1] Method : In this method the agent would need to wait until the final reward was received before any state-action pair values can be updated. Once the final reward is received, the path taken to reach the final state would need to be traced back and each value updated.\nV (st)\u2190 V (st) + \u03b1[Rt \u2212 V (st)] (1)\nwhere st is the state visited at time t, Rt is the reward after time t and \u03b1 is a constant parameter.\n2. Temporal Difference [1] Method : It is used to estimate the value functions after each step. An estimate of the final reward is calculated at each state and the state-action value updated for every step of the way. This reflects a more realistic assignment of rewards to actions compared to MC, which updates all actions at the end directly. TD Learning is nothing but the combination of dynamic programming with the Monte Carlo method. The formula related to TD learning is given as\nV (st)\u2190 V (st)+\u03b1[rt+1+\u03b3V (st+1)\u2212V (st)](2)\nwhere rt+1 is the observed reward at time t+1."}, {"heading": "3.3. Sensor representation for S3 and BattleCity Game", "text": "We are using two types of sensor information for assigning reward in battle city game which are explained as follows;\n1. EnemyInline: If enemy position is directly in line with player without any block or wall then sensor is represented by number 2. If there is a wall or block between enemy and player then sensor is represented by number 1. If enemy position is not in line with player then sensor is 0.\n2. EnemyBaseInline: This sensor information is represented in the same way as above but instead of taking into consideration position\nof enemy, position of enemy-base is taken into account. If enemy-base position is directly in line with player without any block or wall then sensor is represented by number 2. If there is a wall or block between enemy-base and player then sensor is represented by number 1. If enemy-base position is not in line with player then sensor is 0.\nSensor information for S3 game\n1. Get the current map and store it in a two dimensional array.\n2. Gold and Wood sensors are retrieved from current game-state.\n3. Number of peasant and footmen entities for both enemies and player are retrieved from entities state.\n4. Update two dimensional array with static entities like goldmine position with \u2019g\u2019, and buildings with \u2019b\u2019.\nSo far we have outlined our method of obtaining sensor information related to two real-time strategy games, BattleCity and S3."}, {"heading": "3.4. Action Selection Policies", "text": "We have the following action selections policies which can be used to select desired action according to the behavior of that particular policy\n1. \u2212greedy : Most of the time the action with the highest estimated reward is chosen, called the greediest action. But, with a small probability , an action is selected at random to ensure optimal actions are discovered.\n2. \u2212soft : Very similar to \u2212greedy. The best action is selected with probability 1\u2212 and the rest of the time a random action is chosen uniformly.\n3. softmax : One drawback of the above methods is that they select random actions with some probability. So there is a case when the worst possible action is selected as the second best. Softmax remedies this\nby assigning a rank or weight to each of the actions, according to their action-value estimate. So the worst actions are unlikely to be chosen."}, {"heading": "3.5. Steps while learning", "text": "1. The Rlearner observes an input Game state.\n2. The Rlearner then creates a new policy based on the dimensions of the world.\n3. Set the parameters (\u03b1, \u03b3, and number of episodes) for the Rlearner and start learning.\n4. Start running epochs. You can optionally run each epoch individually.\nOne epoch contains following steps.\n1. An action is determined by a decision making function (e.g. \u2212greedy).\n2. The action is performed.\n3. The Rlearner receives a scalar reward or reinforcement from the environment according to reward function.\n4. Information about the reward given for that state / action pair is recorded.\n5. Update the Q-values in Q-table According to Learning Algorithm(e.g. Q-learning or SARSA)."}, {"heading": "4. Proposed learning algorithm", "text": "In this section we outline our proposed learning algorithms which we integrated into the two RTS games Battlecity and S3. We also provide the implementation details related to selection of parameters and reward functions."}, {"heading": "4.1. Parameters", "text": "This section contains the information regarding the reward algorithms and its parameters which we use for the two game BattleCity and S3.\n\u2022 Learning Rate \u03b1 : The learning rate 0 < \u03b1 < 1 determines what fraction of the old estimate will be updated with the new\nestimate. \u03b1 = 0 will stop the RL-agent from learning anything while \u03b1 = 1 will completely change the previous values with the new one.\n\u2022 Discount Factor \u03b3 : The discount factor 0 < \u03b3 < 1 determines what fraction of the upcoming reward values will be considered for evaluation. For \u03b3 = 0 all the upcoming rewards are ignored. For \u03b3 = 1 means the RL-Agent will consider the current and upcoming rewards as equal weightage.\n\u2022 Exploration Rate : In action selection policies there is one policy called as greedy method which uses the exploration rate 0 < < 1 for determining the ratio between the exploration and exploitation. We are using greedy method for selecting the best action and to maintain the balance between exploration and exploitation."}, {"heading": "4.2. Reward function for BattleCity", "text": "Algorithm 1: Reward function is for calculating reward after performing action on current state. According to the result of the action reward or penalty are assigned. In steps 1 to 9 get the positions (x-y co-ordinates) of the player, enemy and enemy base on the map. In steps 10 to 16 if game is over and winner is the RL-Agent (player) then add the reward to the total reward (newReward) else deduct penalty from the total reward. In steps 17 to 18 if enemy is in line with the RL-Agent deduct penalty from total reward so it always tries not to be in line with enemy. In steps 19 to 21 if enemy base is in line with the RL-Agent then calculate the distance between the enemy base and RL-Agent and deduct from 2 times of reward and add to total reward. So it pushes the RL-Agent to come closer to the enemy base. Steps 22 to 24 gives the generalized reward function which makes the RL-Agent quickly attack the enemy base and prevent attack by the enemy.\n%"}, {"heading": "4.3. Reward function for S3", "text": "Algorithm 2: In step 1 to 6 get the sensors related to total gold, total wood and size of troops\nAlgorithm 1: calcReward for BattleCity\nInput: state :- contains positions of entities, reward, penalty sensorsList :- contains sensors of game domain. gameState :- contains state of game is running or not Output: Reward\n1 Playerx = null, Playery = null, Enemyx = null, Enemyy = null ; 2 EnemyBasex = null, EnemyBasey = null, winner = null ; 3 newReward = 0, distance = 0 ; 4 Playerx = getPositionx(state,player) ; 5 Playery = getPositiony(state,player) ; 6 Enemyx = getPositionx(state,enemy) ; 7 Enemyy = getPositiony(state,enemy) ; 8 EnemyBasex = getPositionx(state,enemybase) ; 9 EnemyBasey = getPositiony(state,enemybase) ;\n10 if gameState == \"end\" then 11 winner = getWinner() ; 12 if winner == \"player\" then 13 newReward = newReward + reward ;\n14 else 15 newReward = newReward - penalty ;\n16 else 17 if sensorList[EnemyInline]==2 then 18 newReward = newReward - penalty ;\n19 if sensorList[EnemyBaseInline]==2 then 20 distance = 2 \u221a\n(EnemyBasex \u2212 Playerx)2 + (EnemyBasey \u2212 Playery)2 ; 21 newReward = newReward + 2 \u00d7 reward - distance ; 22 newReward = newReward - 4 \u00d7 distance ; 23 distance = 2 \u221a (Enemyx \u2212 Playerx)2 + (Enemyy \u2212 Playery)2 ; 24 newReward = newReward + 4 \u00d7 distance ; 25 return newReward ;\nof the player and enemy. In steps 7 to 11 if game is over and winner is the RL-Agent (player) then add the reward to the total reward (newReward) else deduct penalty from the total reward. In steps 12 to 14 and 17 to 18 if gold and wood for player is greater than enemy than add reward to the total reward otherwise deduct penalty from total reward so it always tries to increase the gold and wood with compare to enemy. In steps 21 to 22 if Player troop is bigger than the Enemy troop then add the twice of reward to the total reward (newReward) else deduct twice of penalty from the total reward. So it pushes the RL-Agent to Attack or build the army to increase the size of troop as compared to the enemy. In step 25\nReturn the total reward."}, {"heading": "5. Experimental Results", "text": "In the previous section we have discussed how we successfully applied reinforcement learning in two real-time strategy games called BattleCity and S3. In this section we outline the experimental results related to reinforcement learning in BattleCity and S3."}, {"heading": "5.1. BattleCity:", "text": "We evaluated the performance of RL-Agent with the help of various maps (e.g. Bridge-26x18, Bridge-metal-26x18, Bridges-34x26 ) as well as\nAlgorithm 2: calcReward for S3\nInput: state :- contains positions of entities, reward, penalty Global access to: sensorsList :- contains sensors of game domain gameState :- contains state of game is running or not Output: Reward\n1 Playerg = 0, Playerw = 0, Enemyg = 0, Enemyw = 0 EnemyTroopLength = 0, PlayerTroopLength = 0, winner = null newReward = 0 Playerg = player.getGold() ; 2 Playerw = player.getWood() ; 3 Enemyg = enemy.getGold() ; 4 Enemyw = enemy.getWood() ; 5 EnemyTroopLength = enemyTroop.size() ; 6 PlayerTroopLength = playerTroop.size() ; 7 if gameState == \"end\" then 8 winner = getWinner() if winner == \"player\" then 9 newReward = newReward + reward\n10 else 11 newReward = newReward - penalty\n12 else 13 if Playerg > Enemyg then 14 newReward = newReward + reward\n15 else 16 newReward = newReward - penalty\n17 if Playerw > Enemyw then 18 newReward = newReward + reward\n19 else 20 newReward = newReward - penalty\n21 if PlayerTroopLength > EnemyTroopLength then 22 newReward = newReward + 2*reward\n23 else 24 newReward = newReward - 2*penalty\n25 return newReward\nwith two types of opponents called AI-Random and AI-Follower in each map. We observed that the Reinforcement Learning Agent won more than 90% games when played against both opponents( AI-Random and AI-Follower) in simple maps and about 80% to 90% when played against AI-Random in complex maps and 60% to 80% when played against AI-Follower in complex maps. Statistics about the performance of the SARSA[1], Q-Learning[1] and Darmok2 in the various maps are represented below in the form of graphs. We observed that performance of RLAgent under SARSA Learning algorithm is better\nthan other techniques and also RL-agent trained by SARSA algorithm takes less time to win the game.\nWe performed our evaluation for BattleCity game against two opponents AI-Random and AIFollower with three different maps. AI-Random is the built-in AI which selects random action always and AI-Follower is tough to compete because it always follows the opponent and fires at it. It is clear from the experimental results that reinforcement learning agent with the SARSA [1] algorithm performs better than other techniques like Q-Learning [1] and online case based learn-\ning based on Darmok2 [10]. Statistics related to performance are given below in the form of graphs. Statistics are represented using two types of graphs. One is time (in milliseconds) taken to win the game versus episodes. X-axis represents the number of episode and Y-axis represents the time in milliseconds. The other is number of games won versus episode. Here also X-axis represents the number of episodes and Y-axis represents the total number of games won till that episode."}, {"heading": "5.1.1. Map: Bridge-26x18", "text": "This map size is 26x18 (refer Figure 4) so total state space for this map is total combination of the x \u2212 y co-ordinates of the player and enemy which is 262x182. This map has a marble wall in between which the tank cannot destroy by firing. So this is an advantage for the tank to hide from opponents and attack when opponents enters their side."}, {"heading": "5.1.2. Map: Bridges-34x24", "text": "This is the most complex map (refer Figure 9) among all on which we have performed our evaluation because of its size and the structure. It is a 34x24 map and it has 342x242 search spaces. It contains many brick wall and water bodies. Brick wall can be destroyed by firing. Its size and water bodies makes it a difficult and complex map.\nIn time versus episodes graph (refer Figure 10 and 11) the plot (refer Figure 6 and 5) is showing that time to win the game for all strategies varies for every episodes. This map has more water bodies so it is difficult to learn a strategy to win quickly. Against AI-random the performance of all the strategies are close while in case of AI-follower SARSA performs well and wins more game in compared to Q-learning and Darmok2."}, {"heading": "5.2. S3", "text": "The maps related to S3 are more complex than that of Battlecity. We evaluated our approach on various maps against several builtin AI player. In our experiments we built RL agent for S3 game using relative reward function with the Q-learning and SARSA approach as discussed earlier. RL-agent learn by playing 10 games against built-in-ai called ai-catapult-rush for the simple map NWTR1 (refer Figure 12) using two approaches Q-Learning and SARSA. The state-action pair values (Q-Values) are updated while playing (or Learning as discussed earlier RL-Agent also learns while playing). Using this updated Q-Values RL-Agent plays games against ai-catapult-rush as well as another type of builtin-ai called ai-rush.\n\u2022 ai-catapult-rush is the built-in-ai that builds barracks and lumber-mills at the starting, this has two peasants for harvesting gold, and two for harvesting wood. Then it starts building catapults nonstop and also attacks after a while. After sometime it increases the number of peasants to 3, and starts building the second barrack. It also looks for goldmines where there gold\nis still available. Also, it sends catapults to attack enemies.\n\u2022 ai-rush is the built-in-ai that builds a barrack at the starting. There are two peasants at the starting for harvesting gold and wood. After building the barrack ai-rush trains the footmen. When there are two trained footmen it starts attacking.\nFor our experiment we used three type of maps (refer Figure 3, 1 and 12) according to difficulty level (easy-NWTR2, medium-NWTR6 and difficult-GOW). We performed our experiments with five games against two built-in-ai wherein the two approaches are Q-Learning and SARSA for each map. The comparison statistics are given in Table 1. We observed that RL-agent with SARSA wins most of the games. Q-Learning and the previous approach (Darmok2) [10] performs almost the same but not better than SARSA. For S3 also SARSA gives the best results.\nTable 1 shows the results comparison. By analyzing the results shown in Table 1 we can see that in most of the maps SARSA has won or drawn the game. The maps where it has lost we found that the built-in-ai was a quick attacker and RLagent was not able to produce enough number\nof troops to defend while the enemy was attacking. The RL agent was basically trying to find a way to enter through the wall of trees. In some maps we have shown the results as drawn. This means that resources like wood and gold of both player and enemy got finished and only peasants were left out at both the sides and they cannot do anything without the gold and wood.\nWhen compared to previous research on Darmok2 [10], where pre-prepared strategies are used to play the game and plan adaption module is used to switch strategies in this research RL-Agent quickly switches the strategies while playing, even though we used a simple map for training the RL-Agent."}, {"heading": "6. Conclusions", "text": "In this paper we proposed a reinforcement learning model for real-time strategy games. In order to achieve this end we make use of two reinforcement learning algorithms SARSA and QLearning. The idea is to get the best action using one of the RL algorithms so as to not make use of the traces generated by the players. In previous\nworks on real-time strategy games using \u201don line case based learning\u201d human traces form an important component in the learning process. In the proposed method we are not making use of any previous knowledge like traces and therefore we follow an unsupervised approach. This research is with regard to getting the best action using two algorithms (SARSA and Q-Learning) which comes under Reinforcement Learning without the traces generated by the player as proposed in the previous work \u201don line case based learning\u201d using Darmok2. Another major contribution of our work is the reward function. Rewards are calculated by two types of reward functions called conditional and generalized reward function. The sensor information related to game is used for calculating the rewards. The reward values are further used by the two RL algorithms\nSARSA and Q-Learning. These algorithms make policies according to the reward for the state-action pair. RL agent choose the action using these policies. We evaluated our approach successfully in two different game domains (BattleCity and S3) and observed that reinforcement learning performs better than previous ap-\nproaches in terms of learning time and winning ratio. In particular SARSA algorithm takes lesser time to learn and start winning very quickly than Q-Learning and that too for complex maps."}], "references": [{"title": "Barto,Reinforcement Learning: An Introduction. A book publisher MIT", "author": ["A.G.R.S. Sutton"], "venue": null, "citeRegEx": "1", "shortCiteRegEx": "1", "year": 1998}, {"title": "Using first order inductive learning as an alternative to a simulator in a game arficial intelligence", "author": ["Katie Long Genter"], "venue": "under-graduate thesis. In Georgia Institute of Technology,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2009}, {"title": "Learning opponent strategies through first order induction", "author": ["Katie Long Genter", "Santiago Onta\u00f1\u00f3n", "Ashwin Ram"], "venue": "In : FLAIRS Conference,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2011}, {"title": "Mmpm: a generic platform for case-based planning research", "author": ["P.P. Gomez-Martin", "D. Llanso", "M.A. Gomez-Martin", "Santiago Onta\u00f1\u00f3n", "Ashwin Ram"], "venue": "In : ICCBR 2010 Workshop on Case-Based Reasoning for Computer Games,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2010}, {"title": "Using Reinforcement Learning for City Site Selection in the Turn-Based Strategy Game Civilization IV", "author": ["Stefan Wender", "Ian Watson"], "venue": "In : Computational Intelligence and Games (CIG-2008),", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2008}, {"title": "An introduction to casebased reasoning", "author": ["Janet L. Kolodner"], "venue": "In : Artificial Intelligence Review,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 1992}, {"title": "On-line casebased planning", "author": ["Santi Onta\u00f1\u00f3n", "Kinshuk Mishra", "Neha Sugandh", "Ashwin Ram"], "venue": "In : Computational Intelligence,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2010}, {"title": "Learning from human demonstrations for real-time casebased planning", "author": ["Santiago Onta\u00f1\u00f3n", "K.Bonnette", "P.Mahindrakar", "M.A. Gomez-Martin", "Katie Long Genter", "J.Radhakrishnan", "R.Shah", "Ashwin Ram"], "venue": null, "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2011}, {"title": "On-line case-based plan adaptation for real-time strategy games. In :Association for the Advancement of Artificial Intelligence", "author": ["Neha Sugandh", "Santiago Onta\u00f1\u00f3n", "Ashwin Ram"], "venue": null, "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2008}, {"title": "Improving Adaptive Game AI with Evolutionary Learning In Computer Games: Artificial Intelligence, Design and Education, pages", "author": ["Marc Ponsen", "Pieter Spronck"], "venue": null, "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2004}, {"title": "Concurrent hierarchical reinforcement learning Turn-Based Strategy Game Civilization IV", "author": ["Bhaskara Marthi", "Stuart Russell", "David Latham", "Carlos Guestrin"], "venue": "In International Joint Conference on Artificial Intelligence,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2005}], "referenceMentions": [{"referenceID": 9, "context": "Although AI has been applied successfully in several games such as chess, backgammon or checkers when it comes to real-time games the pre-defined scripts which is usually used to simulate the artificial intelligence in chess, backgammon etc [11].", "startOffset": 241, "endOffset": 245}, {"referenceID": 1, "context": "This is because in real-time games decisions has to be made in real-time as well as the search space is huge and as such they do not contain any true AI for learning [2].", "startOffset": 166, "endOffset": 169}, {"referenceID": 5, "context": "One of the major works using Online casebased planning [6] techniques for Real Time Strategy Games was published in [9].", "startOffset": 55, "endOffset": 58}, {"referenceID": 8, "context": "One of the major works using Online casebased planning [6] techniques for Real Time Strategy Games was published in [9].", "startOffset": 116, "endOffset": 119}, {"referenceID": 7, "context": "In [8] a case-based planning system called Darmok2 is introduced that can play RTS games.", "startOffset": 3, "endOffset": 6}, {"referenceID": 6, "context": "Another work by the same authors which uses Darmok2 but addresses the issues of plan acquisition, on-line plan execution, interleaved planning and execution and on-line plan adaptation is [7].", "startOffset": 188, "endOffset": 191}, {"referenceID": 2, "context": "In [3] the authors summarize their work in exploring the use of the first order inductive learning (FOIL) algorithm for learning rules which can be used to represent opponent strategies.", "startOffset": 3, "endOffset": 6}, {"referenceID": 3, "context": "Drawbacks of all case based learning [4] approaches as mentioned above are (1) It requires expert demonstrations for making plans (2) after training is done, no further learning takes place (3) to cover large state spaces it would require large number of rules in the plan base (4) no exploration for optimal solution.", "startOffset": 37, "endOffset": 40}, {"referenceID": 4, "context": "Stefan Wender [5] uses Reinforcement Learning for City Site Selection in the TurnBased Strategy Game Civilization IV.", "startOffset": 14, "endOffset": 17}, {"referenceID": 4, "context": "Stefan Wender [5] uses Reinforcement Learning for City Site Selection in the Turn-Based Strategy Game Civilization IV.", "startOffset": 14, "endOffset": 17}, {"referenceID": 0, "context": "In this paper we aim to do away with the hard coded simulator and propose a learning approach based on Reinforcement Learning [1](RL) wherein sensor information from the current game-state is used to select the best action.", "startOffset": 126, "endOffset": 129}, {"referenceID": 0, "context": "Reinforcement Learning [1] is the field of Machine Learning which deals with what to do, how to map situations to actions so as to maximize a numerical reward signal.", "startOffset": 23, "endOffset": 26}, {"referenceID": 10, "context": "With comparing reinforcement learning [12] to RTS game environment an AI player learns by interacting with the environment and observing the feed-backs of these interactions.", "startOffset": 38, "endOffset": 42}, {"referenceID": 0, "context": "Monte Carlo [1] Method : In this method the agent would need to wait until the final reward was received before any state-action pair values can be updated.", "startOffset": 12, "endOffset": 15}, {"referenceID": 0, "context": "Temporal Difference [1] Method : It is used to estimate the value functions after each step.", "startOffset": 20, "endOffset": 23}, {"referenceID": 0, "context": "Statistics about the performance of the SARSA[1], Q-Learning[1] and Darmok2 in the various maps are represented below in the form of graphs.", "startOffset": 45, "endOffset": 48}, {"referenceID": 0, "context": "Statistics about the performance of the SARSA[1], Q-Learning[1] and Darmok2 in the various maps are represented below in the form of graphs.", "startOffset": 60, "endOffset": 63}, {"referenceID": 0, "context": "It is clear from the experimental results that reinforcement learning agent with the SARSA [1] algorithm performs better than other techniques like Q-Learning [1] and online case based learn-", "startOffset": 91, "endOffset": 94}, {"referenceID": 0, "context": "It is clear from the experimental results that reinforcement learning agent with the SARSA [1] algorithm performs better than other techniques like Q-Learning [1] and online case based learn-", "startOffset": 159, "endOffset": 162}], "year": 2016, "abstractText": "In this paper we proposed reinforcement learning algorithms with the generalized reward function. In our proposed method we use Q-learning and SARSA algorithms with generalised reward function to train the reinforcement learning agent. We evaluated the performance of our proposed algorithms on two real-time strategy games called BattleCity and S3. There are two main advantages of having such an approach as compared to other works in RTS. (1) We can ignore the concept of a simulator which is often game specific and is usually hard coded in any type of RTS games (2) our system can learn from interaction with any opponents and quickly change the strategy according to the opponents and do not need any human traces as used in previous works.", "creator": "LaTeX with hyperref package"}}}