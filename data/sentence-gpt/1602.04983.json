{"id": "1602.04983", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "16-Feb-2016", "title": "Contextual Media Retrieval Using Natural Language Queries", "abstract": "The widespread integration of cameras in hand-held and head-worn devices as well as the ability to share content online enables a large and diverse visual capture of the world that millions of users build up collectively every day. We envision these images as well as associated meta information, such as GPS coordinates and timestamps, to form a collective visual memory that can be queried while automatically taking the ever-changing context of mobile users into account. As a first step towards this vision, in this work we present Xplore-M-Ego: a novel media retrieval system that allows users to query a dynamic database of images and videos using spatio-temporal natural language queries. We evaluate our system using a new dataset of real user queries as well as through a usability study. One key finding is that there is a considerable amount of inter-user variability, for example in the resolution of spatial relations in natural language utterances. We show that our retrieval system can cope with this variability using personalisation through an online learning-based retrieval formulation. In order to explore the use of multi-dimensional imagery in the field of digital media, the team created a new system for using human-readable data in a manner that is able to predict the spatial context of a user.\n\n\n\nThe results demonstrate that Xplore-M-Ego is not only useful in understanding a user's location information, but is also able to help them to understand the context of their interaction with the world as well. We report that Xplore-M-Ego is able to predict, in the present, how information might be collected from the user's view of an image. The result: the human-readable, digital form of images, videos, and videos will provide insight into how the user might interpret their interaction with the world.\nThis new approach has been applied to both user and device, where we are applying a new way to manage our user-generated content online:\nUser-generated content is the first form of data from a user's view of the world. In this work, we use user-generated content in a way that is easy to read, easily query, and interact with. The user-generated content is in the current form of a series of images, videos, and videos that users share, but are not connected to or interact with. The data is collected via a new approach by an app called Xplore-M-Ego. This approach offers user-generated content for users who can access and share", "histories": [["v1", "Tue, 16 Feb 2016 11:04:29 GMT  (7220kb,D)", "http://arxiv.org/abs/1602.04983v1", "8 pages, 9 figures, 1 table"]], "COMMENTS": "8 pages, 9 figures, 1 table", "reviews": [], "SUBJECTS": "cs.IR cs.AI cs.CL cs.CV cs.HC", "authors": ["sreyasi nag chowdhury", "mateusz malinowski", "reas bulling", "mario fritz"], "accepted": false, "id": "1602.04983"}, "pdf": {"name": "1602.04983.pdf", "metadata": {"source": "CRF", "title": "Contextual Media Retrieval Using Natural Language Queries", "authors": ["Sreyasi Nag Chowdhury", "Mateusz Malinowski", "Andreas Bulling", "Mario Fritz"], "emails": ["sreyasi@mpi-inf.mpg.de", "mmalinow@mpi-inf.mpg.de", "bulling@mpi-inf.mpg.de", "mfritz@mpi-inf.mpg.de"], "sections": [{"heading": "1. INTRODUCTION", "text": "Due to the widespread deployment of visual sensors in consumer products and Internet sharing platforms, we have collectively achieved a quite detailed visual capture of the world in space and time over the last years. In particular, mobile devices have changed the way we take pictures and new technology like life-logging devices will continue to do so in the future. With efficient search engines at our aid, viewing images and videos of unknown and distant places is just a few clicks away. These search engines do not allow for complex, natural language queries that include spatiotemporal references and they also largely ignore the users\u2019 local context.\nSimilar to how mobile devices have changed the way we take pictures, we ask how media search should be transformed to make use of the rich context available at query time. What if we quickly want to know what is behind the building in front of us? What if we want to know what a particular cafe looks like to quickly locate it in a busy market area? What if we want to see what our new neighborhood looks like in winter? Our approach makes use of the user\u2019s ever-changing context to retrieve results of a spatiotemporal query on a mobile device. The user is enabled to make references to his/her changing environment by allowing for queries in natural language. We have named our system Xplore-M-Ego (read \u201cExplore Amigo\u201d) \u2013 which stands for Exploration(Xplore) of Media(M) Egocentrically(Ego)\u201d."}, {"heading": "2. RELATED WORK", "text": "Previous research addressing the problems of media retrieval and machine understanding of natural language queries can be broadly classified into three groups.\nSpatio-temporal Media Retrieval: Spatio-temporal media retrieval is the browsing of media content captured in different geographical locations at various times in the past. Snavely et al. [17] proposed Photo tourism that constructs a sparse 3D geometric representation of the underlying scene from images. Using this system users could move in 3D space from one picture to another. To address challenges in the construction management industry, Wu and Tory [22] designed PhotoScope. It is an interactive tool to visualize spatio-temporal coverage of photos in a photo collection, which users can browse with space, time and standardized content specifications. Tompkin et al. [20] developed Vidicontexts that embeds videos in a panoramic frame of reference (context) and enables simultaneous visualization of videos in different foci. A similar system, VideoScapes [19] was implemented as a graph with videos as edges and portals (automatically identified transition opportunities) as vertices. When temporal context was relevant, videos were temporally aligned to offer correctly ordered transitions.\nIn contrast to these methods, our approach implements\nar X\niv :1\n60 2.\n04 98\n3v 1\n[ cs\n.I R\n] 1\n6 Fe\nb 20\n16\negocentrism by taking users\u2019 context (geographical location and viewing direction) into account, and interfaces with the users through natural language queries.\nNatural Language Query Processing: Successful answering of a natural language question by machines requires understanding its meaning, which is often realizable by a semantic parser that transforms the question into its formal representation. Traditional approaches to semantic parsing used supervised learning by training on questions with costly manually annotated logical forms [23, 21]. Modern approaches use more scalable techniques to train a semantic parser with more accessible textual question-answer pairs [2, 10, 1]. Malinowski and Fritz [14] proposed an architecture for question-answering based on real-world indoor images. They extended the work of Liang et al. [10] to include subjective interpretations of scenes. They also identified challenges that holistic architectures have to face, such as different frame of reference in spatial relations or ambiguities in the answers [14, 15].\nOur work differs in that we target a dynamic and egocentric environment in contrast to static geographical/job/image data.\nMedia Retrieval Using Natural Language Queries: Previous research on media retrieval using natural language queries varies considerably in the methods used to process the natural language utterances. Lum and Kim [11] presented a method that matches semantic network representations of queries with those of natural language descriptions of media data (manually annotated). Kucuktunc et al. [7] proposed a pattern matching approach based on Part-of-Speech (POS) tags. Other approaches are based on RDF-triples [6] and SPARQL queries [4]. Contrary to these research threads, our work does not involve any human annotations or additional processing steps for extracting descriptions of entities from images and videos. Instead, we extract media content simply based on its meta data such as geographical location (GPS coordinates) and textual questions.\nPrior research also looked into media retrieval with natural language questions containing spatial relations. Tellex and Roy [18] explored spatial relations in surveillance videos by a classification task which handles two prepositions,\u201cacross\u201d and \u201calong\u201d. Lan et al. [8] used structured queries that consists of two objects linked by a spatial relations chosen from a restricted set of spatial prepositions. In contrast, our media retrieval architecture aims to operate on rich natural language questions that liberate from any artificially imposed restrictions, such as fixed structure of the questions or a restricted vocabulary.\nTo the best of our knowledge, none of the previous works ventured into contextual media retrieval by taking into account the user\u2019s current location and viewing direction. The introduction of egocentrism and natural language queries in architectures developed for browsing large media collections have many practical applications. Not only does it open another unexplored dimension for media retrieval (vis-a\u0300-vis, \u201cegocentrism\u201d), but also aids in human interaction with the computer."}, {"heading": "3. CONTEXTUAL MEDIA RETRIEVAL", "text": "Our contextual media retrieval architecture allows users to explore a collective media collections in a spatio-temporal\ncontext through natural language questions such as \u201cWhat is there in front of the university bus terminal?\u201d, \u201cWhat is there to the left of the campus center?\u201d, \u201cWhat happened here five days ago?\u201d, \u201cWhat did this place look like in December?\u201d etc. In the following, we show how we formulate our architecture. Particular attention is payed on how to cope with the user\u2019s dynamic context and spatial references in natural language questions. We further describe how we collect a data set which relate to our initial motivation of building a Collective Visual Memory."}, {"heading": "3.1 Learning-based, Contextual Media Retrieval by Semantic Parsing", "text": "In this section, we describe how we approach learningbased, contextual media retrieval from natural language queries by a semantic parsing approach. First we describe the employed semantic parser architecture (inspired from Liang et al. [10]) and show how to extend it towards a contextual media retrieval task. The probabilistic model of our architecture in shown in Figure 2. A question x (uttered by a user) is mapped to a latent logical form z, which is then evaluated with respect to a world w (database of facts), producing an answer y. The world w consists of ws (a static database of geographical information) and wd (a dynamic database which stores user metadata and information about media files in the Collective Visual Memory). The logical forms z are represented as labeled trees and are induced automatically from question-answer (x, y) pairs.\n3.1.1 Question-Answering with Semantic Parsing We build our approach on a recently proposed framework\nfor semantic parsing [10] that has been shown to be able to answer questions about facts like geographical data and is trained solely on textual question-answer pairs. For example, the approach is capable of answering question like \u201cWhat are the major cities in California?\u201d with {San Fransisco, Los Angeles, San Diego, San Jose} as an answer. In the semantic parsing framework (left part of Figure 2 labeled Semantic Parsing and Interpretation): \u2018parsing\u2019 translates a question into its logical form z, and \u2018interpretation\u2019 executes z on the dataset of facts (word w) producing its denotation JzKw - an answer. Parameters \u03b8 are estimated solely on the training question-answer pairs (x, y) with an EM algorithm maximizing the following posterior distribution:\n\u03b8\u2217 := arg max \u03b8 \u2211 (x,y)\u223cD \u2211 z 1{y = JzKw}\ufe38 \ufe37\ufe37 \ufe38 Interpretation p(z|x, \u03b8)\ufe38 \ufe37\ufe37 \ufe38 SemanticParsing\n(1) where D denotes a training set, 1{a = b} is 1 if a condition a = b holds, and 0 otherwise. The posterior distribution marginalizes over a latent set of valid logical forms z. At test time, the answer is computed from the denotation Jz\u2217Kw that maximizes the following posteriori:\nz\u2217 := arg max z p(z|x, \u03b8\u2217) (2)\nThe logical forms follow a dependency-based compositional semantics (DCS) formalism [10] that consists of trees with nodes labeled with predicates and edges labeled with relations between the predicates. DCS is mainly introduced to efficiently encode feasible solutions.\nThe underlying principle of the parsing is built on two components \u2013 lexical semantics and compositional semantics. Lexical semantics learns a mapping from textual words into pre-defined predicates, and uses hand-designed lexical triggers that map specific parts-of-speech into a set of candidate predicates. Compositional semantics establishes relations between the predicates to generate the logical forms (DCS trees). The distribution over logical forms is modeled by a log linear distribution p\u03b8(z|x) \u221d e\u03c6(x,z) T \u03b8, where the feature vector \u03c6 measure compatibility between the question x and a logical form z. We perform a gradient descent scheme in order to optimize for parameters \u03b8. For a more detailed exposition of the semantic parser and parameter optimization in these models, we refer the reader to Liang et al. [10]. In the following, we discuss our decomposition of the world w into two parts: ws and wd (Figure 2).\n3.1.2 Static and Dynamic Worlds The existing works that use such a semantic parser are\nbased on a static environment [10, 1, 14]. In contrast to these, in our scenario a human user (the source of the query - user in Figure 2) relocates herself in space and time in a continuously changing environment. The pool of media content \u2013 Collective Visual Memory \u2013 also grows as new media is added (by multiple users - crowd icon in Figure 2). Such an environment leads us to our decomposition of the world w into a static part ws, which consists of geographical facts such as names of buildings or theirs GPS coordinates and inherits all the properties of the aforementioned previous works, and a dynamic and egocentric part wd (Figure 2).\nThe dynamic world wd decomposes even further into wdm that stores media metadata (timestamp, GPS coordinates) and is updated with continuously growing Collective Visual Memory, and wdu that models the user\u2019s context by storing\nher metadata (GPS coordinates, viewing direction). The latter is set anew for each query before it is fed into the semantic parser. Such representation renders the world w = ws+wd static to the semantic parser although it is constantly changing.\n3.1.3 Modelling User\u2019s Context The user\u2019s context is modelled through predicates per-\nson(LAT,LON,VIEW_DIR) where LAT, LON, VIEW_DIR represent the user\u2019s current latitude, longitude and viewing directions respectively. These predicates are stored in the dynamic database of user metadata wdu which is updated at query time for each query.\nUnderstanding egocentric spatial relations in natural language questions has for long intrigued the research community and forms a separate research area by itself [16, 8, 3, 13]. In our work, we approach ambiguity in the frame of reference [15] by defining predicates that resolve the spatial relations \u201cfront of\u201d, \u201cbehind\u201d, \u201cleft of\u201dand\u201cright of\u201dbased on the geomagnetic reference frame as well as the user-centric reference frame. Each of these spatial relations are modeled as twoargument predicates such as frontOf(A,B), behind(A,B), leftOf(A,B) and rightOf(A,B) where A denotes the GPS coordinates of the entity in question (extracted from ws) and B denotes the GPS coordinates of the media files in wdm .\nSimilarly, the temporal references in questions (e.g. \u201cwhat happened here five days ago?\u201d, \u201chow did this place look like in December?\u201d) are modelled through the predicate day(X) where X is the referenced time-stamp (for example 20150511). These are resolved by mapping to trigger a predicate view(A), where A is the list of media files having the same time-stamp as that in the predicate day(\u00b7).\nHowever, it is difficult to understand the hidden intent for contextual questions which includes an egocentric reference frame. This is because humans do not adhere to any\nN S EW\n\u201cfront of\u201d\n\u201cright of\u201d\u201cleft of\u201d\n\u201cbehind\u201d\nUser heading geomagnetic East\nOriginal Query: What is there in front of Postbank?\nAltered Query: What is there on the right of Postbank?\nFigure 3: Modification of spatial reference in query for integrating egocentrism to media retrieval\nconsistent reference frame. They may consider their own physical \u201cleft hand\u201d for \u201cleft of\u201d or the physical \u201cleft side\u201d of the geographical entity. The first possibility is tackled by programming the semantic parser to follow the geomagnetic reference frame. Then, with the assumption that the direction in which the human user faces is the local north, the spatial reference in the query is modified in a pre-processing step. This is explained in Figure 3 \u2013 if the user faces east and queries for \u201cWhat is there in front of postbank?\u201d, the question would be changed during pre-processing to \u201cWhat is there on the right of postbank?\u201d. The semantic parser would predict answer for this changed question. For simplicity we have narrowed down to only the four basic heading directions - north, south, east and west.\n3.1.4 Media Retrieval as Answers In contrast to previous work on question answering [10,\n14], we desire to retrieve media as answers to natural language questions instead of textual information. This can be modeled by generating references to media files as denotations of logical forms. For example, the question \u201cWhat is there on the right of the campus center?\u201d would be transformed into the following symbolic representation (after training the system): answer(A, (rightOf(A,B), const(B, \u2018campus_center\u2019)) with its denotation {\u2018image12\u2019, \u2018image58\u2019, \u2018image234\u2019, ...}, where \u2018image12\u2019, \u2018image58\u2019 and so on are references to images with visual contents depicting geographical entities on the right of the university campus center. The answers are predicted with respect to a world which consists of the name, timestamp, GPS coordinates and the month of the media file acquisition. Once the denotations of a logical form are predicted, the actual media files are extracted from the Collective Visual Memory (physically, a file system storing all captured media). These extracted media files are then returned to the user. Figure 1 shows examples of retrieved results."}, {"heading": "3.2 Data Collection", "text": "To enable the spatio-temporal exploration of a certain geographic area we inherently require a database which record physical features on the ground along with their types (e.g. building, cafe, highway, etc.), names and GPS locations \u2013\nsu p\nma C\nsu p\nma C\nEsupmaC EsupmaC\nsupmaC\nsu p\nma C\nsu p\nma C\nsu p\nma C\nEsupmaC\nEsupmaC\nDs up ma C\nCampus\nsupmaC\nDFKIParkplatz\nUniversit\u00e4t Mensa\nUniversit\u00e4t Mensa\nC2.2\nC4.3\nDeusches Forschungszentrum f\u00fcr K\u00fcnstliche Intelligenz\nC1.1\nDeusches Forschungszentrum\nf\u00fcr K\u00fcnstliche Intelligenz\nC1.2\nC1.3\nStud.Wohnheim D4.5\nAStA A5.2\nC2.1\nA5.1\nE2.6\nC2.3\nE2.4\nE2.9\nE2.7\nC4.2\nC4.1\nC4.5\nC4.4\n(a) A section of the OpenStreetMap view of the university campus\n\u3008nodeid =\u201c344240596\u201dvisible =\u201ctrue\u201dversion =\u201c6\u201dchangeset = \u201c9208001\u201d timestamp = \u201c2011-09-04T11 : 43 : 28Z\u201d user = \u201carnhar\u201d uid = \u201c495739\u201d lat = \u201c81.24279\u201d lon = \u201c35.18783\u201d\u3009 \u3008tagk = \u201camenity\u201d v = \u201cbus stop\u201d/\u3009 \u3008tagk = \u201cname\u201d v = \u201cUniversita\u0308t Mensa\u201d/\u3009 \u3008/node\u3009\n(b) XML rendition of the physical entity shown in Figure 4a\nbus stop(\u2018universitaet mensa\u2019,49.2562752,7.0436771).\n(c) Entry in ws corresponding to the entity in Figure 4b\nFigure 4: Example of OpenStreetMap data\nthis constitutes our static world ws. To support media retrieval we need a database of images and videos rich with metadata. We also require natural language queries paired with corresponding media content as retrievals for training and testing our query-retrieval model. In the absence of a suitable benchmark, we needed to record our own data set, where the geographical facts (ws) are obtained from OpenStreetMap, and the Collective Visual Memory and queryretrieval pairs are collected from regular users.\n3.2.1 Geographical Facts OpenStreetMap [5] is a freely-available and well-documented\ncollection of geographical data. The topological data structure used has four basic elements or data primitives \u2013 nodes, ways, relations and tags. Physical entities on the ground such as buildings, highways, ATMs, banks, restaurants etc. are registered in the map database in terms of these data primitives.\nIn our study, we restrict the spatial scope of our system to a university campus. Figure 4a shows the map view of a part of the university campus depicting a physical entity on the ground \u2013 a bus-stop named \u201cUniversita\u0308t Mensa\u201d. The XML rendition of this part of the map (available for download) in shown in Figure 4b. We used information such as the type of the physical entity (e.g. building, cafe, highway etc.), their names, and their GPS coordinates as our static database of facts (ws) (Figure 4c).\n3.2.2 Collective Visual Memory Participants were asked to capture media (images and\nvideos) at various locations of the university campus for a month using their mobile devices. In total our instance of the Collective Visual Memory consists of 1025 images and 175 videos. Metadata such as GPS coordinates and timestamp registered with each media file constitute our media database wdm .\nThe process of media acquisition was coupled with the col-\nlection of natural language questions. Participants were instructed to formulate a question and capture the photo(s)/ video(s) that they would expect as the corresponding answer. 1000 questions-answer pairs with spatial references were collected (one question could have multiple answers). Question-answer pairs with temporal references could not be collected because of the trivial infeasibility of capturing events from the past. The data set was randomized and divided into two parts \u2013 500 train questions and 500 test questions. To introduce sufficient amount of variations in natural language we chose participants from different cultural and linguistic background. We will make our data-set (the query-retrieval pairs, Collective Visual Memory and the geographical facts) publicly available at time of publication."}, {"heading": "4. EXPERIMENTS", "text": "For our experiments we use the geographical facts and a Collective Visual Memory as described in the previous section.We use a dataset consisting of query-retrieval pairs formulated by real-life users. It consists of user queries which follow no particular template and contains spatial relations in addition to those pre-defined as predicates, such \u201cnear\u201d, \u201cbeside\u201d, \u201cahead of\u201d, \u201copposite to\u201d etc.\nIn this section we describe the experiments conducted, state their results and discuss our observations. We further propose the concept of personalization of a media retrieval system to adapt to specific user perceptions. Finally, we provide a qualitative assessment of the usefulness of our contextual media retrieval system."}, {"heading": "4.1 Evaluation of Learning Procedure", "text": "To study the effect of learning on prediction accuracy we first trained a model with synthetically generated queryretrieval pairs (SynthModel). The queries are generated by templates \u2013 \u201cwhat is there <spatial relation> of X ?\u201d, \u201cwhat happened here Y days/weeks/months/years ago?\u201d, \u201cwhat did this place look like in Z?\u201d, where <spatial relation> \u2208 {\u201cin front\u201d, \u201cbehind\u201d, \u201con the right\u201d, \u201con the left\u201d}, X \u2208 {names of buildings, cafes, restaurants etc.}, Y \u2208 {natural numbers} and Z \u2208 {names of months}. The contextual cues \u2018here\u2019, \u2018this place\u2019 are fixed to a particular location. The retrievals follow pre-defined rules to resolve spatial (according to the geomagnetic reference frame) and temporal relations. The untrained model was found to have a prediction accuracy of 11.23%. We observe a strong improvement of performance to 46% from as little as 200 training examples (Figure 5).\nRegular users use a variety of grammatical constructs as common in a spoken language. Therefore the queries collected from them were rich with a number of spatial relations not restrictive to the ones we represent as predicates (section 3.1.3). Also, the answers to similar queries were subjective. To account for the variability and subjectivity in this type of data, and to study the effect of learning on prediction accuracy, we trained a query-retrieval model on human queries and retrievals (HumanModel). As before, we used a weakly supervised learning approach that only requires query-retrieval pairs without any supervision on the logical forms. The model was trained through a humanin-the-loop training procedure using a relevance feedback mechanism. Since the human trainer was familiar with the geographic scope of our work, it was also possible to provide feedback on the retrievals of the temporal queries. We found\nthat during the training the query-retrieval model learned to associate different spatial relations to pre-defined predicates. For example, the parser has learned to map the spatial relation \u201cahead of\u201d to the predicate frontOf(\u00b7).\nA comparison to the previous model shows that the HumanModel (26.67%) yields greater recall than the SynthModel (15.88%) on queries collected from humans (Figure 6), where recall is defined as the percentage of relevant retrievals among all test queries. This shows that our HumanModel is able to learn and adapt to the variations in natural language utterances and also interpret a variety of spatial relations in spoken queries. We use this model for our evaluations described in the following sections."}, {"heading": "4.2 Model Evaluation", "text": "Since a contextual application calls for the involvement of prospective users and their satisfaction in using it, we decide on a qualitative assessment of the system.\nHumans are inherently inconsistent in their perception of directions and idea of reference frames [9, 15]. The nature of understanding/speaking English questions also has variations based on a person\u2019s socio-cultural background. Hence, a system relying on fixed question templates and a partic-\nular set of rules to resolve spatial references does not guarantee high accuracy. A satisfactory result for one person may prove to be irrelevant for another. To better understand these perceptual biases and yet efficiently analyze the system, a series of user studies were conducted."}, {"heading": "4.2.1 Evaluation of the Retrieved Results and Human Disagreements", "text": "The goal of this user study is to observe how accurate regular users found our system. Five users were asked to evaluate the retrieved results for 500 test questions as \u201crelevant\u201d or \u201cirrelevant\u201d. The study was conducted in a lab set-up. Users looked at retrieved results for each question on a computer screen and stated whether they find the retrievals relevant or irrelevant to the question. A canonical reference frame was used in this experiment to resolve spatial relationships in queries. According to this convention, \u201cfront of\u201d meant \u201cnorth of\u201d, \u201cbehind\u201d meant \u201csouth of\u201d, \u201cright of\u201d meant \u201ceast of\u201d and \u201cleft of\u201d meant \u201cwest of\u201d.\nWe observed that for each question the opinions varied. Based on this observation we divide the test questions into six groups \u2013 (5,0), queries for which all five users agreed that the retrievals were relevant; (4,1), queries for which four users found the retrievals relevant and one user found them irrelevant and likewise. Figure 7 depicts the result of this analysis. For 26.67% of the queries all five users deemed the retrievals relevant. However, if we consider the cases in which most of the users found the retrievals relevant, this number rises to 40%. The numbers in the middle region of the graph in Figure 7 point out the prominent difference in opinions among participants. This accounts for about 25% of all queries. We observed that the inter-user variability stems from the inherent inconsistencies with regards to reference frame resolution. This result also hints towards the difficulty of the problem at hand since satisfactory answers for one user may be unsatisfactory for others. The high agreement in the last column is because of some unavoidable factors - scanty media content (our geographic scope could not be well covered in images and videos due to lack of infrastructure), incorrect POS tagging (this resulted in incorrect retrievals type, for e.g. text), etc.\nFrom the observation from this user study \u2013 that human disagreed in their opinion of relevance and irrelevance \u2013 we\nconjecture that instead of using the geomagnetic reference frame, the use of user-centric reference frames for retrieving answers could improve the performance of the system. In the deployment of the user-centric reference frame we mean to follow the user\u2019s physical egocentric directions \u2013 for example, her \u2018right hand side\u2019 for \u201cright of\u201d etc. (explained in greater details in section 3.1.3)."}, {"heading": "4.2.2 Canonical and User-centric Reference Frame", "text": "In order to study the impact of using two different con-\nventions of spatial relations resolution, we conducted this user study. Users were given two sets of retrieved results for each question \u2013 one set of media files retrieved according to the geomagnetic reference frame and the second set retrieved according to the user-centric reference frame. The experimental settings are similar to the previous user study.\nFigure 8 shows the result of this user study. user1 and user3 remained neutral to the use of separate reference frames while the other users slightly preferred the canonical reference frame over the user-centric reference frame. This observation further highlights the subjectivity of the task.\n4.2.3 Personalization of Xplore-M-Ego Having observed this inter-person subjectivity, we hypoth-\nesize that personalization of our media retrieval system would increase its accuracy on a per user basis. The user study which we discuss in this section was conducted to investigate this hypothesis.\nBy using an online relevance feedback mechanism, five users (U1, U2, U3, U4, U5) were asked to train five different query-retrieval models (M1,M2, M3,M4,M5) with 500 questions from the data-set collected from regular users. Every user was then asked to evaluate all five models keeping the identity of the model trained by each of them hidden.\nThe quantitative analysis of this study \u2013 precision1, recall2 and F1-score 3 \u2013 are shown in Figure 9. The diagonals show the user-specific evaluation results and the rows depict interuser evaluation results. The difference in opinion among the\n1precision = relevant retrievals/media retrievals 2recall = relevant retrievals/total number of test queries 3F1 score = 2 \u2217 (precision \u2217 recall)/(precision + recall)\nusers is very prominent, highlighting the challenge involved in the machine understanding of hidden human intent in natural language. Nonetheless, it is clear from the figure that users deemed their own models more accurate than those trained by others. This observation leads us to believe that the query-retrieval model can be trained over time through relevance feedback to adapt to user-specific preferences of spatial relation resolution \u2013 hence, it should be personalized. This consolidates our hypothesis \u2013 personalization of our media retrieval system increases its accuracy on a per user basis.\n4.2.4 User Experience Evaluation To understand the usefulness of our contextual media re-\ntrieval system, we made an usability/desirability study. 10 participants were given the Google Glass installed with our client-side application and asked to walk around in the university campus while making voice queries that involve spatiotemporal references. Afterward they were asked to fill in the USE Questionnaire [12]. This questionnaire has four groups of questions \u2013 Usefulness, Ease of Use, Ease of Learning and Satisfaction. Each question can be rated on a scale from 1 to 7, 1 meaning \u2018strongly disagree\u2019 and 7 meaning \u2018strongly agree\u2019. 10 questions most representative of the entire questionnaire are chosen. The mean and standard deviation of the ratings of these questions are shown in Table 1. The result of this evaluation shows that regular users strongly agree that our contextual media retrieval application is useful in daily life. Moreover, they find the application easy to use, very easy to learn and they are satisfied with the outcome."}, {"heading": "5. DISCUSSION", "text": "Due to the complex nature of our problem that involves natural language queries, media and map data, human concepts, in particular of spatial and temporal language, and complex contextual cues, we have faced a wide range of challenges. We highlight three of these in this section and discuss limitations and future work.\nFrame of reference: Proper spatial resolution is required in a successful communication with machines. Unfortunately, there is no a unique frame of reference, and hence even a simple statement involving \u201cleft of\u201d has different meanings for different users. However, our findings suggests two promising research directions in the reference frame resolution task. First, our inspection of the user study shows that the users often resolve spatial relations in spoken language for the navigation task according to the frontal direction of the physical object (from observations of section 4.2.2). Hence, a suitable map database that stores information about the frontal direction of the objects would help. We are unaware of such database or efforts to augment existing map data with such meta information. Second, our study on personalized Xplore-M-Ego suggests a more individual approach where the architecture learns to understand spatial relations by interacting with the user. While our online learning approach shows a first promising step in this directions, more complex models of person specific biases and shared notions across users could further improve the learning.\nDiversity of Named Entities: Our approach uses a static database that contains information about the geographical entities, for instance the name of the entity, extracted from the OpenStreetMap. However, the participants in our study use a number of different names to refer to the same entity \u2013 the formal full name, an acronym, a popular name, or even a name in a different language. Handling such diversity is a complicated task for the semantic parser, and hence we resort to manually adding all possible common names for each entity in the database. However, such human annotation may still be incomplete. An alternate method for handling the coverage of the database is to use suitable knowledgebases containing acronyms and regional names of geographical entities or crawling additional web resources. To the best of our knowledge, such information about synonyms of map\nentities is currently not pursued, but would greatly benefit applications that relate to map data such as ours.\nScalability: The program induction step of the semantic parser, where a logical form z is searched over a large space of possible predicates and theirs relations (Eq. 1 and 2), is computationally demanding, and does not scale well with a large number of predicates representing geographical facts. We deal with this problem by reducing the spatial scope to a university campus. In deployment, we envision a system that directly works in a spatial scope of the user, and updates the database by geographical facts in ws while the user is relocating in space and time."}, {"heading": "6. CONCLUSION", "text": "In this paper we proposed Xplore-M-Ego \u2013 a novel system for media retrieval using spatio-temporal natural language queries in a dynamic setting. Our work brings forth a new direction to this paradigm by exploiting a user\u2019s current context. Our approach is based on a semantic parser that infers interpretations of the natural language queries. We contribute several extensions which enable the user to dynamically refer to his/her context by spatial and temporal concepts. We further analyzed the system in the various user studies that highlight the importance of our adaptive and personalized training approaches."}], "references": [{"title": "Semantic parsing via paraphrasing", "author": ["J. Berant", "P. Liang"], "venue": "In Proceedings of ACL,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2014}, {"title": "Driving semantic parsing from the world\u2019s response", "author": ["J. Clarke", "D. Goldwasser", "M.-W. Chang", "D. Roth"], "venue": "In Proceedings of the Fourteenth Conference on Computational Natural Language Learning,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2010}, {"title": "Grounding spatial relations for human-robot interaction", "author": ["S. Guadarrama", "L. Riano", "D. Golland", "D. Gouhring", "Y. Jia", "D. Klein", "P. Abbeel", "T. Darrell"], "venue": "In Intelligent Robots and Systems (IROS),", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2013}, {"title": "Semantic video search using natural language queries", "author": ["A. Hakeem", "M.W. Lee", "O. Javed", "N. Haering"], "venue": null, "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2009}, {"title": "Openstreetmap: Usergenerated street maps", "author": ["M. Haklay", "P. Weber"], "venue": "Pervasive Computing, IEEE,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2008}, {"title": "A method for processing the natural language query in ontologybased image retrieval system. In Adaptive Multimedia Retrieval: User, Context, and Feedback, pages", "author": ["M. Hwang", "H. Kong", "S. Baek", "P. Kim"], "venue": null, "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2007}, {"title": "A natural language-based interface for querying a video database", "author": ["O. Kucuktunc", "U. G\u00fcd\u00fckbay", "\u00d6. Ulusoy"], "venue": "IEEE MultiMedia,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2007}, {"title": "Image retrieval with structured object queries using latent ranking svm", "author": ["T. Lan", "W. Yang", "Y. Wang", "G. Mori"], "venue": "In Computer Vision\u2013ECCV", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2012}, {"title": "Space in language and cognition: Explorations in cognitive diversity, volume 5", "author": ["S.C. Levinson"], "venue": null, "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2003}, {"title": "Learning dependency-based compositional semantics", "author": ["P. Liang", "M.I. Jordan", "D. Klein"], "venue": "Computational Linguistics,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2013}, {"title": "Intelligent natural language processing for media data query", "author": ["V. Lum", "K.-c. K. Kim"], "venue": "In Proc. 2nd Int. Golden West Conf. on Intelligent Systems,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 1992}, {"title": "Measuring usability with the use questionnaire", "author": ["A.M. Lund"], "venue": "Usability interface,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2001}, {"title": "A pooling approach to modelling spatial relations for image retrieval and annotation", "author": ["M. Malinowski", "M. Fritz"], "venue": "[cs.CV],", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2014}, {"title": "A multi-world approach to question answering about real-world scenes based on uncertain input", "author": ["M. Malinowski", "M. Fritz"], "venue": "In Advances in Neural Information Processing Systems (NIPS),", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2014}, {"title": "Towards a visual turing challenge", "author": ["M. Malinowski", "M. Fritz"], "venue": "In NIPS Workshop on Learning Semantics,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2014}, {"title": "Grounding spatial language in perception: an empirical and computational investigation", "author": ["T. Regier", "L.A. Carlson"], "venue": "Journal of Experimental Psychology: General,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2001}, {"title": "Photo tourism: exploring photo collections in 3d", "author": ["N. Snavely", "S.M. Seitz", "R. Szeliski"], "venue": "ACM transactions on graphics (TOG),", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2006}, {"title": "Towards surveillance video search by natural language query", "author": ["S. Tellex", "D. Roy"], "venue": "In Proceedings of the ACM International Conference on Image and Video Retrieval,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2009}, {"title": "Videoscapes: exploring sparse, unstructured video collections", "author": ["J. Tompkin", "K.I. Kim", "J. Kautz", "C. Theobalt"], "venue": "ACM Transactions on Graphics (TOG),", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2012}, {"title": "Video collections in panoramic contexts", "author": ["J. Tompkin", "F. Pece", "R. Shah", "S. Izadi", "J. Kautz", "C. Theobalt"], "venue": "In Proceedings of the 26th annual ACM symposium on User interface software and technology,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2013}, {"title": "Learning synchronous grammars for semantic parsing with lambda calculus", "author": ["Y.W. Wong", "R.J. Mooney"], "venue": "In Annual Meeting-Association for computational Linguistics,", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2007}, {"title": "Photoscope: visualizing spatiotemporal coverage of photos for construction management", "author": ["F. Wu", "M. Tory"], "venue": null, "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2009}, {"title": "Learning to map sentences to logical form: Structured classification with probabilistic categorial grammars", "author": ["L.S. Zettlemoyer", "M. Collins"], "venue": "arXiv preprint arXiv:1207.1420,", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2012}], "referenceMentions": [{"referenceID": 16, "context": "[17] proposed Photo tourism that constructs a sparse 3D geometric representation of the underlying scene from images.", "startOffset": 0, "endOffset": 4}, {"referenceID": 21, "context": "To address challenges in the construction management industry, Wu and Tory [22] designed PhotoScope.", "startOffset": 75, "endOffset": 79}, {"referenceID": 19, "context": "[20] developed Vidicontexts that embeds videos in a panoramic frame of reference (context) and enables simultaneous visualization of videos in different foci.", "startOffset": 0, "endOffset": 4}, {"referenceID": 18, "context": "A similar system, VideoScapes [19] was implemented as a graph with videos as edges and portals (automatically identified transition opportunities) as vertices.", "startOffset": 30, "endOffset": 34}, {"referenceID": 22, "context": "Traditional approaches to semantic parsing used supervised learning by training on questions with costly manually annotated logical forms [23, 21].", "startOffset": 138, "endOffset": 146}, {"referenceID": 20, "context": "Traditional approaches to semantic parsing used supervised learning by training on questions with costly manually annotated logical forms [23, 21].", "startOffset": 138, "endOffset": 146}, {"referenceID": 1, "context": "Modern approaches use more scalable techniques to train a semantic parser with more accessible textual question-answer pairs [2, 10, 1].", "startOffset": 125, "endOffset": 135}, {"referenceID": 9, "context": "Modern approaches use more scalable techniques to train a semantic parser with more accessible textual question-answer pairs [2, 10, 1].", "startOffset": 125, "endOffset": 135}, {"referenceID": 0, "context": "Modern approaches use more scalable techniques to train a semantic parser with more accessible textual question-answer pairs [2, 10, 1].", "startOffset": 125, "endOffset": 135}, {"referenceID": 13, "context": "Malinowski and Fritz [14] proposed an architecture for question-answering based on real-world indoor images.", "startOffset": 21, "endOffset": 25}, {"referenceID": 9, "context": "[10] to include subjective interpretations of scenes.", "startOffset": 0, "endOffset": 4}, {"referenceID": 13, "context": "They also identified challenges that holistic architectures have to face, such as different frame of reference in spatial relations or ambiguities in the answers [14, 15].", "startOffset": 162, "endOffset": 170}, {"referenceID": 14, "context": "They also identified challenges that holistic architectures have to face, such as different frame of reference in spatial relations or ambiguities in the answers [14, 15].", "startOffset": 162, "endOffset": 170}, {"referenceID": 10, "context": "Lum and Kim [11] presented a method that matches semantic network representations of queries with those of natural language descriptions of media data (manually annotated).", "startOffset": 12, "endOffset": 16}, {"referenceID": 6, "context": "[7] proposed a pattern matching approach based on Part-of-Speech (POS) tags.", "startOffset": 0, "endOffset": 3}, {"referenceID": 5, "context": "Other approaches are based on RDF-triples [6] and SPARQL queries [4].", "startOffset": 42, "endOffset": 45}, {"referenceID": 3, "context": "Other approaches are based on RDF-triples [6] and SPARQL queries [4].", "startOffset": 65, "endOffset": 68}, {"referenceID": 17, "context": "Tellex and Roy [18] explored spatial relations in surveillance videos by a classification task which handles two prepositions,\u201cacross\u201d and \u201calong\u201d.", "startOffset": 15, "endOffset": 19}, {"referenceID": 7, "context": "[8] used structured queries that consists of two objects linked by a spatial relations chosen from a restricted set of spatial prepositions.", "startOffset": 0, "endOffset": 3}, {"referenceID": 9, "context": "[10]) and show how to extend it towards a contextual media retrieval task.", "startOffset": 0, "endOffset": 4}, {"referenceID": 9, "context": "We build our approach on a recently proposed framework for semantic parsing [10] that has been shown to be able to answer questions about facts like geographical data and is trained solely on textual question-answer pairs.", "startOffset": 76, "endOffset": 80}, {"referenceID": 9, "context": "The logical forms follow a dependency-based compositional semantics (DCS) formalism [10] that consists of trees with nodes labeled with predicates and edges labeled with relations between the predicates.", "startOffset": 84, "endOffset": 88}, {"referenceID": 9, "context": "[10].", "startOffset": 0, "endOffset": 4}, {"referenceID": 9, "context": "The existing works that use such a semantic parser are based on a static environment [10, 1, 14].", "startOffset": 85, "endOffset": 96}, {"referenceID": 0, "context": "The existing works that use such a semantic parser are based on a static environment [10, 1, 14].", "startOffset": 85, "endOffset": 96}, {"referenceID": 13, "context": "The existing works that use such a semantic parser are based on a static environment [10, 1, 14].", "startOffset": 85, "endOffset": 96}, {"referenceID": 15, "context": "Understanding egocentric spatial relations in natural language questions has for long intrigued the research community and forms a separate research area by itself [16, 8, 3, 13].", "startOffset": 164, "endOffset": 178}, {"referenceID": 7, "context": "Understanding egocentric spatial relations in natural language questions has for long intrigued the research community and forms a separate research area by itself [16, 8, 3, 13].", "startOffset": 164, "endOffset": 178}, {"referenceID": 2, "context": "Understanding egocentric spatial relations in natural language questions has for long intrigued the research community and forms a separate research area by itself [16, 8, 3, 13].", "startOffset": 164, "endOffset": 178}, {"referenceID": 12, "context": "Understanding egocentric spatial relations in natural language questions has for long intrigued the research community and forms a separate research area by itself [16, 8, 3, 13].", "startOffset": 164, "endOffset": 178}, {"referenceID": 14, "context": "In our work, we approach ambiguity in the frame of reference [15] by defining predicates that resolve the spatial relations \u201cfront of\u201d, \u201cbehind\u201d, \u201cleft of\u201dand\u201cright of\u201dbased on the geomagnetic reference frame as well as the user-centric reference frame.", "startOffset": 61, "endOffset": 65}, {"referenceID": 9, "context": "In contrast to previous work on question answering [10, 14], we desire to retrieve media as answers to natural language questions instead of textual information.", "startOffset": 51, "endOffset": 59}, {"referenceID": 13, "context": "In contrast to previous work on question answering [10, 14], we desire to retrieve media as answers to natural language questions instead of textual information.", "startOffset": 51, "endOffset": 59}, {"referenceID": 4, "context": "OpenStreetMap [5] is a freely-available and well-documented collection of geographical data.", "startOffset": 14, "endOffset": 17}, {"referenceID": 8, "context": "Humans are inherently inconsistent in their perception of directions and idea of reference frames [9, 15].", "startOffset": 98, "endOffset": 105}, {"referenceID": 14, "context": "Humans are inherently inconsistent in their perception of directions and idea of reference frames [9, 15].", "startOffset": 98, "endOffset": 105}, {"referenceID": 11, "context": "Afterward they were asked to fill in the USE Questionnaire [12].", "startOffset": 59, "endOffset": 63}], "year": 2016, "abstractText": "The widespread integration of cameras in hand-held and head-worn devices as well as the ability to share content online enables a large and diverse visual capture of the world that millions of users build up collectively every day. We envision these images as well as associated meta information, such as GPS coordinates and timestamps, to form a collective visual memory that can be queried while automatically taking the ever-changing context of mobile users into account. As a first step towards this vision, in this work we present Xplore-M-Ego: a novel media retrieval system that allows users to query a dynamic database of images and videos using spatio-temporal natural language queries. We evaluate our system using a new dataset of real user queries as well as through a usability study. One key finding is that there is a considerable amount of inter-user variability, for example in the resolution of spatial relations in natural language utterances. We show that our retrieval system can cope with this variability using personalisation through an online learning-based retrieval formulation.", "creator": "LaTeX with hyperref package"}}}