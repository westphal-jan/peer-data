{"id": "1602.05703", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "18-Feb-2016", "title": "Adaptive Least Mean Squares Estimation of Graph Signals", "abstract": "In many applications spanning from sensor to social networks, transportation systems, gene regulatory networks or big data, the signals of interest are defined over the vertices of a graph. The aim of this paper is to propose a least mean square (LMS) strategy for adaptive estimation of signals defined over graphs. Assuming the graph signal to be band-limited, over a known bandwidth, the method enables reconstruction, with guaranteed performance in terms of mean-square error, and tracking from a limited number of observations over a subset of vertices. The concept of a system that captures data via a given bandwidth allows the technique to generate data with a minimum of multiple possible values in order to optimize the accuracy and accuracy of the measurement.\n\n\n\nThe basic approach that follows is the most successful in predicting the signal-to-data distribution of signals across different networks of traffic. The technique provides a method of using data from the top end of a graph, providing a better estimate of what has happened in the past or a different network in the future. It is known that this method is well understood and used by the community as an adjunct to other approaches for estimating signal-to-data. It was first used by some of the major sensor networks in the history of this field and is available at www.smartdata.org/publications/fMRI. The goal of this paper is to provide a more comprehensive understanding of signal-to-data.\nA single approach that captures signals between a set of different nodes in the network is based on a linear and linear approach. Using linear approaches, the system learns from the distributed information that can be produced. For example, each node is given a unique network state with a single information value; each node then has an associated network state and has an associated network state.\nIn addition to a linear approach, a network network learns from its peers by using random-effects to model the number of neurons (e.g., a block size in a network) and the current network state. Using random-effects to model the number of neurons and nodes in a network, the model learns by having a set of random-effects to model the number of neurons (e.g., a block size in a network) and the current network state. Using random-effects to model the number of neurons and nodes in a network, the model learns by having a set of random-effects to model the number of neurons (e.g., a block size in a network) and the current network state. Using random-effects", "histories": [["v1", "Thu, 18 Feb 2016 07:34:04 GMT  (290kb)", "http://arxiv.org/abs/1602.05703v1", "Submitted to IEEE Transactions on Signal and Information Processing over Networks. arXiv admin note: text overlap witharXiv:1211.6950by other authors without attribution"], ["v2", "Fri, 19 Feb 2016 10:15:47 GMT  (171kb)", "http://arxiv.org/abs/1602.05703v2", "Submitted to IEEE Transactions on Signal and Information Processing over Networks"], ["v3", "Mon, 11 Jul 2016 15:40:59 GMT  (120kb)", "http://arxiv.org/abs/1602.05703v3", "Submitted to IEEE Transactions on Signal and Information Processing over Networks"]], "COMMENTS": "Submitted to IEEE Transactions on Signal and Information Processing over Networks. arXiv admin note: text overlap witharXiv:1211.6950by other authors without attribution", "reviews": [], "SUBJECTS": "cs.LG cs.SY", "authors": ["paolo di lorenzo", "sergio barbarossa", "paolo banelli", "stefania sardellitti"], "accepted": false, "id": "1602.05703"}, "pdf": {"name": "1602.05703.pdf", "metadata": {"source": "CRF", "title": "Least Mean Squares Estimation of Graph Signals", "authors": ["Paolo Di Lorenzo"], "emails": ["paolo.dilorenzo@unipg.it,", "sergio.barbarossa@uniroma1.it,", "paolo.banelli@unipg.it,", "stefania.sardellitti@uniroma1.it"], "sections": [{"heading": null, "text": "ar X\niv :1\n60 2.\n05 70\n3v 1\n[ cs\n.L G\n] 1\n8 Fe\nb 20\nIndex Terms\u2014Least mean square estimation, graph signal processing, sampling on graphs, cognitive networks.\nI. INTRODUCTION\nIn many applications, from sensor to social networks, vehicular networks, big data or biological networks, the signals of interest are defined over the vertices of a graph [1]. Over the last few years, a series of papers produced a significant advancement in the development of processing tools for the analysis of signals defined over a graph, or graph signals for short [1]\u2013[3]. Graph signal processing (GSP) extends classical discrete-time signal processing to signals defined over a discrete domain having a very general structure, represented by a graph, which subsumes discrete-time as a very simple case. Since the signal domain is not defined once for all, but it comes to depend on the graph topology, one of the most interesting and intriguing aspects of GSP is that the analysis tools come to depend on the graph topology as well. This paves the way to a plethora of methods, each emphasizing different aspects of the problem. An important feature of graph signals is that the signal domain is not a metric space, as for example with biological networks, where the vertices may be genes, proteins, enzymes, etc, and the presence of an edge between two molecules means that those molecules undergo a chemical reaction. This marks a fundamental difference with\nThis work has been supported by TROPIC Project, Nr. ICT-318784.\nrespect to time signals where the time domain is inherently a metric space. Processing signals defined over a graph has been considered in [2], [4]\u2013[6]. A central role in GSP is of course played by spectral analysis of graph signals, which passes through the introduction of the so called Graph Fourier Transform (GFT). Alternative definitions of GFT have been proposed, depending on the different perspectives used to extend classical tools [7], [8], [1], [9], [2]. Two basic approaches are available, proposing the projection of the graph signal onto the eigenvectors of either the graph Laplacian, see, e.g., [7], [1], [9] or of the adjacency matrix, see, e.g. [2], [10]. The first approach applies to undirected graphs and builds on the spectral clustering properties of the Laplacian eigenvectors and the minimization of the \u21132 norm graph total variation; the second approach was proposed to handle also directed graphs and it is based on the interpretation of the adjacency operator as the graph shift operator, which lies at the heart of all linear shift-invariant filtering methods for graph signals [11], [12]. A further very recent contribution proposes to build the graph Fourier basis as the set of orthonormal signals that minimize the (directed) graph cut size [13].\nAfter the introduction of the GFT, an uncertainty principle for graph signals was derived in [14] and, more recently [15], [16], [17], [18]. The aim of these works is to assess the link between the spread of a signal on the vertices of the graph and on its dual domain, as defined by the GFT. In particular, in [18], the authors give simple closed form expressions for the fundamental tradeoff between the concentrations of a signal in the graph and the transformed domains.\nOne of the basic problems in GSP is the development of a graph sampling theory, whose aim is to recover a band-limited (or approximately band-limited) graph signal from a subset of its samples. A seminal contribution was given in [7], later extended in [19] and, very recently, in [10], [18], [20], [21], [22]. Dealing with graph signals, the recovery problem may easily become ill-conditioned, depending on the location of the samples. Hence, for any given number of samples enabling signal recovery, the identification of the sampling set plays a key role in the conditioning of the recovery problem. It is then particularly important to devise strategies to optimize the selection of the sampling set. Alternative signal reconstuction methods have been proposed, either iterative as in [23], [20], [24], or single shot, as in [10], [18]. Frame-based approaches to reconstruct signals from subsets of samples have been proposed in [7], [20], [18].\n2 The theory developed in the last years for GSP was then applied to solve specific learning tasks, such as semi-supervised classification on graphs [25]\u2013[27], graph dictionary learning [28], [29], learning graphs structures [30], smooth graph signal recovery from random samples [31], [32], inpainting [33], denoising [34], and community detection on graphs [35]. Finally, in [36], [37], the authors proposed signal recovery methods aimed to recover graph signals that are assumed to be smooth with respect to the underlying graph, from sampled, noisy, missing, or corrupted measurements. Contribution: The goal of this paper is to propose LMS strategies for the adaptive estimation of signals defined on graphs. To the best of our knowledge, this is the first attempt to merge the well established theory of adaptive filtering [38] with the emerging field of signal processing on graphs. The proposed method hinges on the graph structure describing the observed signal and, under a band-limited assumption, it enables online reconstruction and tracking from a limited number of observations taken over a subset of vertices. An interesting feature of our proposed strategy is that this subset is allowed to vary over time, in adaptive manner. A detailed mean square analysis illustrates the role of the sampling strategy on the reconstruction capability, stability, and meansquare performance of the proposed algorithm. Based on these results, we also derive adaptive sampling strategies for LMS estimation of graph signals. Several numerical results confirm the theoretical findings, and assess the performance of the proposed strategies. Furthermore, we consider the case where the graph signal is band-limited but the bandwidth is not known beforehand; this case is critical because the selection of the sampling strategy fundamentally depends on such prior information. To cope with this issue, we propose an LMS method with adaptive sampling, which estimates and track the signal support in the (graph) frequency domain, while at the same time adapting the graph sampling strategy. Numerical results illustrate the tracking capability of the aforementioned method in the presence of time-varying graph signals. As an example, we apply the proposed strategy to estimate and track the spatial distribution of the electromagnetic power in a cognitive radio framework. The resulting graph signal turns out to be smooth, i.e. the largest part of its energy is concentrated at low frequencies, but it is not perfectly band-limited. As a consequence, recovering the overall signal from a subset of samples is inevitably affected by aliasing [22]. Numerical results show the tradeoff between complexity, i.e. number of samples used for processing, and mean-square performance of the proposed strategy, when applied to such cartography task. Intuitively, processing with a larger bandwidth and a (consequent) larger number of samples, improves the performance of the algorithm, at the price of a larger complexity.\nThe paper is organized as follows. In Sec. II, we introduce some basic graph signal processing tools, which will be useful for the following derivations. Sec. III introduces the proposed LMS algorithm for graph signals, illustrates its mean-square analysis, and derives useful graph sampling strategies. Then, in Sec. IV we illustrate the proposed LMS strategy with adaptive sampling, while Sec. V considers the application to power density cartography. Finally, Sec. VI draws some conclusions."}, {"heading": "II. GRAPH SIGNAL PROCESSING TOOLS", "text": "We consider a graph G = (V , E) consisting of a set of N nodes V = {1, 2, ..., N}, along with a set of weighted edges E = {aij}i,j\u2208V , such that aij > 0, if there is a link from node j to node i, or aij = 0, otherwise. The adjacency matrix A of a graph is the collection of all the weights aij , i, j = 1, . . . , N . The degree of node i is ki := \u2211N j=1 aij . The degree matrix K is a diagonal matrix having the node degrees on its diagonal. The Laplacian matrix is defined as:\nL = K\u2212A. (1)\nIf the graph is undirected, the Laplacian matrix is symmetric and positive semi-definite, and admits the eigendecomposition L = U\u039bUH , where U collects all the eigenvectors of L in its columns, whereas \u039b is a diagonal matrix containing the eigenvalues of L. It is well known from spectral graph theory [39] that the eigenvectors of L are well suited for representing clusters, since they minimize the \u21132 norm graph total variation.\nA signal x over a graph G is defined as a mapping from the vertex set to the set of complex numbers, i.e. x : V \u2192 C. In many applications, the signal x admits a compact representation, i.e., it can be expressed as:\nx = Us (2)\nwhere s is exactly (or approximately) sparse. As an example, in all cases where the graph signal exhibits clustering features, i.e. it is a smooth function within each cluster, but it is allowed to vary arbitrarily from one cluster to the other, the representation in (2) is compact, i.e. the only nonzero (or approximately nonzero) entries of s are the ones associated to the clusters.\nThe GFT s of a signal x is defined as the projection onto the orthogonal set of vectors {ui}i=1,...,N [1], i.e.\nGFT: s = UHx. (3)\nThe GFT has been defined in alternative ways, see, e.g., [1], [9], [2], [10]. In this paper, we basically follow the approach based on the Laplacian matrix, assuming an undirected graph structure, but the theory could be extended to handle directed graphs with minor modifications. We denote the support of s in (2) as F = {i \u2208 {1, . . . , N} : si 6= 0}, and the bandwidth of the graph signal x is defined as the cardinality of F , i.e. |F|. Clearly, combining (2) with (3), if the signal x exhibits a clustering behavior, in the sense specified above, the GFT is the way to recover the sparse vector s. Localization Operators: Given a subset of vertices S \u2286 V , we define a vertex-limiting operator as the diagonal matrix\nDS = diag{1S}, (4)\nwhere 1S is the set indicator vector, whose i-th entry is equal to one, if i \u2208 S , or zero otherwise. Similarly, given a subset of frequency indices F \u2286 V , we introduce the filtering operator\nBF = U\u03a3FU H , (5)\nwhere \u03a3F is a diagonal matrix defined as \u03a3F = diag{1F}. It is immediate to check that both matrices DS and BF are selfadjoint and idempotent, and then they represent orthogonal\n3 projectors. The space of all signals whose GFT is exactly supported on the set F is known as the Paley-Wiener space for the set F [7]. We denote by BF \u2286 L2(G) the set of all finite \u21132-norm signals belonging to the Paley-Wiener space associated to F . Similarly, we denote by DS \u2286 L2(G) the set of all finite \u21132-norm signals with support on the vertex subset S. In the rest of the paper, whenever there will be no ambiguities in the specification of the sets, we will drop the subscripts referring to the sets. Finally, given a set S, we denote its complement set as S , such that V = S \u222a S and S \u2229 S = \u2205. Similarly, we define the complement set of F as F . Thus, we define the vertex-projector onto S as D and, similarly, the frequency projector onto the frequency domain F as B.\nExploiting the localization operators in (4) and (5), we say that a vector x is perfectly localized over the subset S \u2286 V if\nDx = x, (6)\nwith D defined as in (4). Similarly, a vector x is perfectly localized over the frequency set F if\nBx = x, (7)\nwith B given in (5). As previously stated, |F| represents the (not necessarily contiguous) bandwidth of the graph signal. The localization properties of graph signals were studied in [22] and later extended in [18] to derive the fundamental tradeoff between the localization of a signal in the graph and on its dual domain. An interesting consequence of that theory is that, differently from continuous-time signals, a graph signal can be perfectly localized in both vertex and frequency domains. The conditions for having perfect localization are stated in the following theorem, which we report here for completeness of exposition; its proof can be found in [22].\nTheorem 1: There is a vector x, perfectly localized over both vertex set S and frequency set F (i.e. x \u2208 BF \u2229 DS) if and only if the operator BDB (or DBD) has an eigenvalue equal to one; in such a case, x is an eigenvector of BDB associated to the unitary eigenvalue.\nEquivalently, the perfect localization properties can be expressed in terms of the operators BD and DB. Indeed, since the operators BD and DB have the same singular values [18], perfect localization onto the sets S and F can be achieved if and only if\n\u2016BD\u20162 = \u2016DB\u20162 = 1. (8)\nBuilding on these previous results on GSP, in the next section we introduce the proposed LMS strategy for adaptive estimation of graph signals."}, {"heading": "III. LMS ESTIMATION OF GRAPH SIGNALS", "text": "The least mean square algorithm, introduced by Widrow and Hoff [40], is one of the most popular methods for adaptive filtering. Its applications include echo cancelation, channel equalization, interference cancelation and so forth. Although there exist algorithms with faster convergence rates such as the Recursive Least Square (RLS) methods [38], LMS-type methods are popular because of its ease of implementation,\nlow computational costs and robustness. For these reasons, a huge amount of research was produced in the last decades focusing on improving the performance of LMS-type methods, exploiting in many cases some prior information that is available on the observed signals. For instance, if the observed signal is known to be sparse in some domain, such prior information can help improve the estimation performance, as demonstrated in many recent efforts in the area of compressed sensing [41], [42]. Some of the early works that mix adaptation with sparsity-aware reconstruction include methods that rely on the heuristic selection of active taps [43], and on sequential partial updating techniques [44]; some other methods assign proportional step-sizes to different taps according to their magnitudes, such as the proportionate normalized LMS (PNLMS) algorithm and its variations [45]. In subsequent studies, motivated by the LASSO technique [46] and by connections with compressive sensing [42], [47], several algorithms for sparse adaptive filtering have been proposed based on LMS [48], RLS [49], and projection-based methods [50]. Finally, sparsity aware distributed methods were proposed in [51]\u2013[55].\nIn this paper, we aim to exploit the intrinsic sparsity that is present in band-limited graph signals, thus designing proper sampling strategies that guarantee adaptive reconstruction of the signal, with guaranteed mean-square performance, from a limited number of observation sampled from the graph. To this aim, let us consider a signal x0 \u2208 CN defined over the graph G = (V , E). The signal is initially assumed to be perfectly band-limited, i.e. its spectral content is different from zero only on a limited set of frequencies F . Later on, we will relax such an assumption. Let us consider partial observations of signal x0, i.e. observations over only a subset of nodes. Denoting with S the sampling set (observation subset), the observed signal at time n can be expressed as:\ny[n] =D (x0 + v[n]) = DBx0 +Dv[n] (9)\nwhere D is the vertex-limiting operator defined in (4), which takes nonzero values only in the set S, and v[n] is a zeromean, additive noise with covariance matrix Cv . The second equality in (9) comes from the bandlimited assumption, i.e. Bx0 = x0, with B denoting the operator in (5) that projects onto the (known) frequency set F . The estimation task consists in recovering the band-limited graph signal x0 from the noisy, streaming, and partial observations y[n] in (9). Following an LMS approach, the optimal estimate for x0 can be found as the vector that solves the following optimization problem:\nmin x\nE \u2016y[n]\u2212DBx\u20162 (10)\ns.t. Bx = x\nwhere E(\u00b7) denotes the expectation operator. The solution of problem (10) minimizes the mean-squared error and has a bandwidth limited to the frequency set F . For stationary y[n], the optimal (band-limited) solution of (10) is given by the vector x0 that satisfies the normal equations:\nBDBx0 = BDE{y[n]}. (11)\nNevertheless, in many linear regression applications involving online processing of data, the expectation E{y[n]} may be\n4 Algorithm 1: LMS algorithm for graph signals\nStart with x[0] \u2208 BF chosen at random. Given a sufficiently small step-size \u00b5 > 0, for each time n > 0, repeat:\nx[n+ 1] = x[n] + \u00b5BD (y[n]\u2212 x[n]) (12)\neither unavailable or time-varying, and thus impossible to update continuously. For this reason, adaptive solutions relying on instantaneous information are usually adopted in order to avoid the need to know the signal statistics beforehand. A typical solution proceeds to optimize (10) by means of a steepest-descent procedure. Thus, letting x[n] be the instantaneous estimate of vector x0, the LMS algorithm for graph signals evolves as illustrated in Algorithm 1, where \u00b5 > 0 is a (sufficiently small) step-size, and we have exploited the fact that D is an idempotent operator, and Bx[n] = x[n] (i.e., x[n] is band-limited) for all n. Algorithm 1 starts from an initial signal that belongs to the Paley-Wiener space for the set F , and then evolves implementing an alternating orthogonal projection onto the vertex set S (through D) and the frequency set F (through B). The properties of the LMS recursion in (12) crucially depend on the choice of the sampling set S, which defines the structure of the operator D [cf. (4)]. To shed light on the theoretical behavior of Algorithm 1, in the following sections we illustrate how the choice of the operator D affects the reconstruction capability, mean-square stability, and steady-state performance of the proposed LMS strategy."}, {"heading": "A. Reconstruction Properties", "text": "It is well known from adaptive filters theory [38] that the LMS algorithm in (12) is a stochastic approximation method for the solution of problem (10), which enables convergence in the mean-sense to the true vector x0 (if the step-size \u00b5 is chosen sufficiently small), while guaranteing a bounded meansquare error (as we will see in the sequel). However, since the existence of a unique band-limited solution for problem (12) depends on the adopted sampling strategy, the first natural question to address is: What conditions must be satisfied by the sampling operator D to guarantee reconstruction of signal x0 from the selected samples? The answer is given in the following Theorem, which gives a necessary and sufficient condition to reconstruct graph signals from partial observations using Algorithm 1.\nTheorem 2: Problem (10) admits a unique solution, i.e. any band-limited signal x0 can be reconstructed from its samples taken in the set S , if and only if\n\u2225\u2225DB \u2225\u2225 2 < 1, (13)\ni.e. if the matrix BDB does not have any eigenvector that is perfectly localized on S and bandlimited on F .\nProof. From (11), exploiting the relation D = I\u2212D, it holds ( I\u2212BDB ) x0 = BDE{y[n]}. (14)\nHence, it is possible to recover x0 from (14) if I\u2212BDB is invertible. This happens if the sufficient condition (13) holds\ntrue. Conversely, if \u2016DB\u20162 = 1 (or, equivalently, \u2016BD\u20162 = 1), from (8) we know that there exist band-limited signals that are perfectly localized over S . This implies that, if we sample one of such signals over the set S , we get only zero values and then it would be impossible to recover x0 from those samples. This proves that condition (13) is also necessary.\nA necessary condition that enables reconstruction, i.e. the non-existence of a non-trivial vector x satisfying DBx = 0, is that |S| \u2265 |F|. However, this condition is not sufficient, because matrix DB in (9) may loose rank, or easily become ill-conditioned, depending on the graph topology and sampling strategy defined by D. This suggests that the location of samples plays a key role in the performance of the LMS reconstruction algorithm in (12). For this reason, In Section III.D we will consider a few alternative sampling strategies satisfying different optimization criteria."}, {"heading": "B. Mean-Square Analysis", "text": "When condition (13) holds true, Algorithm 1 can reconstruct the graph signal from a subset of samples. In this section, we study the mean-square behavior of the proposed LMS strategy, illustrating how the sampling operator D affects its stability and steady-state performance. From now on, we view the estimates x[n] as realizations of a random process and analyze the performance of the LMS algorithm in terms of its meansquare behavior. Let x\u0303[n] = x[n]\u2212 x0 be the error vector at time n. Subtracting x0 from the left and right hand sides of (12), using (9) and relation Bx\u0303[n] = x\u0303[n], we obtain:\nx\u0303[n+ 1] = (I\u2212 \u00b5BDB) x\u0303[n] + \u00b5BDv[n]. (15)\nApplying a GFT to each side of (15) (i.e., multiplying by UH ), and exploiting the structure of matrix B in (5), we obtain\ns\u0303[n+ 1] = (I\u2212 \u00b5\u03a3UHDU\u03a3) s\u0303[n] + \u00b5\u03a3UHDv[n], (16)\nwhere s\u0303[n] = UH x\u0303[n] is the GFT of the error x\u0303[n]. From (16) and the definition of \u03a3 in (5), since s\u0303i[n] = 0 for all i /\u2208 F , we can analyze the behavior of the error recursion (16) only on the support of s\u0303[n], i.e. s\u0302[n] = {s\u0303i[n], i \u2208 F} \u2208 C|F|. Thus, letting UF \u2208 CN\u00d7|F| be the matrix having as columns the eigenvectors of the Laplacian matrix associated to the frequency indices F , the error recursion (16) can be rewritten in compact form as:\ns\u0302[n+ 1] = (I\u2212 \u00b5UHFDUF ) s\u0302[n] + \u00b5U H FDv[n]. (17)\nThe evolution of the error s\u0302[n] = UHF x\u0303[n] in the compact transformed domain is totally equivalent to the behavior of x\u0303[n] from a mean-square error point of view. Thus, using energy conservation arguments [56], we consider a general weighted squared error sequence s\u0302[n]H\u03a6s\u0302[n], where \u03a6 \u2208 C|F|\u00d7|F| is any Hermitian nonnegative-definite matrix that we are free to choose. In the sequel, it will be clear the role played by a proper selection of the matrix \u03a6. Then, from (17) we can establish the following variance relation:\nE\u2016s\u0302[n+ 1]\u20162\u03a6 = E\u2016s\u0302[n]\u2016 2 \u03a6\u2032 + \u00b5 2 E{v[n]HDUF\u03a6U H FDv[n]}\n= E\u2016s\u0302[n]\u20162\u03a6\u2032 + \u00b5 2 Tr(\u03a6UHFDCvDUF) (18)\n5 where Tr(\u00b7) denotes the trace operator, and\n\u03a6\u2032 = ( I\u2212 \u00b5UHFDUF ) \u03a6 ( I\u2212 \u00b5UHFDUF ) . (19)\nLet \u03d5 = vec(\u03a6) and \u03d5\u2032 = vec(\u03a6\u2032), where the notation vec(\u00b7) stacks the columns of \u03a6 on top of each other and vec\u22121(\u00b7) is the inverse operation. We will use interchangeably the notation \u2016s\u0302\u20162\u03a6 and \u2016s\u0302\u2016 2 \u03d5\nto denote the same quantity s\u0302H\u03a6s\u0302. Exploiting the Kronecker product property\nvec(X\u03a6Y) = (YH \u2297X)vec(\u03a6),\nand the trace property\nTr(\u03a6X) = vec(XH)T vec(\u03a6),\nin the relation (18), we obtain:\nE\u2016s\u0302[n+ 1]\u20162 \u03d5 = E\u2016s\u0302[n]\u20162Q\u03d5 + \u00b5 2vec(G)T\u03d5 (20)\nwhere\nG = UHFDCvDUF (21) Q = (I\u2212 \u00b5UHFDUF)\u2297 (I\u2212 \u00b5U H FDUF ). (22)\nThe following theorem guarantees the asymptotic mean-square stability (i.e., convergence in the mean and mean-square error sense) of the LMS algorithm in (12).\nTheorem 3: Assume model (9) holds. Then, for any bounded initial condition, the LMS strategy (12) asymptotically converges in the mean-square error sense if the step-size \u00b5 and the sampling operator D are chosen to satisfy:\n0 < \u00b5 < 2\n\u03bbmax ( UHFDUF ) , (23)\nwith \u03bbmax(A) denoting the maximum eigenvalue of the symmetric matrix A.\nProof. Letting r = \u00b52vec(G), recursion (18) can be equivalently recast as:\nE\u2016s\u0302[n]\u20162 \u03d5 = E\u2016s\u0302[0]\u20162Qn\u03d5 + r T\nn\u22121\u2211\nl=0\nQl\u03d5 (24)\nwhere E\u2016s\u0302\u2016 denotes the initial condition. We first note that if Q is stable, Qn \u2192 0 as n \u2192 \u221e. In this way, the first term on the RHS of (24) vanishes asymptotically. At the same time, the convergence of the second term on the RHS of (24) depends only on the geometric series of matrices \u2211n\u22121 l=0 Q\nl, which is known to be convergent to a finite value if the matrix Q is a stable matrix [57]. In summary, since the RHS of (24) asymptotically converges to a finite value, we conclude that E\u2016s\u0302[n]\u20162\n\u03d5 will converge to a steady-state value. From (22), we\ndeduce that Q is stable if matrix I\u2212 \u00b5UHFDUF is stable as well. Now recalling that, for any Hermitian matrix X, it holds \u2016X\u2016 = \u03c1(X) [57], with \u03c1(X) denoting the spectral radius of X, we obtain that Q is stable if \u2016I\u2212UHFDUF\u2016 < 1, which holds true for any step-sizes satisfying (23)."}, {"heading": "C. Steady-State Performance", "text": "Taking the limit of (20) as n \u2192 \u221e (assuming condition (23) holds true), we obtain:\nlim n\u2192\u221e\nE\u2016s\u0302[n]\u20162(I\u2212Q)\u03d5 = \u00b5 2vec(G)T\u03d5. (25)\nExpression (25) is a useful result: it allows us to derive several performance metrics through the proper selection of the free weighting parameter \u03d5 (or \u03a6). For instance, let us assume that one wants to evaluate the steady-state mean square deviation (MSD) of the LMS strategy (12). Thus, selecting \u03d5 = (I \u2212 Q)\u22121vec(I) in (25), we obtain\nMSD = lim n\u2192\u221e E\u2016x\u0303[n]\u20162 = lim n\u2192\u221e E\u2016s\u0302[n]\u20162\n= \u00b52vec(G)T (I\u2212Q)\u22121vec(I). (26)\nIf instead one is interested in evaluating the mean square deviation obtained by the LMS algorithm (12) when reconstructing the value of the signal associated to k-th vertex of the graph, selecting \u03d5 = (I\u2212Q)\u22121vec(UHFEkUF ) in (25), we obtain\nMSDk = lim n\u2192\u221e E\u2016x\u0303[n]\u20162Ek = limn\u2192\u221e E\u2016s\u0302[n]\u20162 UH F EkUF\n= \u00b52vec(G)T (I\u2212Q)\u22121vec(UHFEkUF ), (27)\nwhere Ek = diag{ek}, with ek \u2208 RN denoting the k-th canonical vector. In the sequel, we will confirm the validity of these theoretical expressions by comparing them with numerical simulations."}, {"heading": "D. Sampling Strategies", "text": "As illustrated in the previous sections, the properties of the proposed LMS algorithm in (12) strongly depend on the choice of the sampling set S, i.e. on the vertex limiting operator D. Indeed, building on the previous analysis, it is clear that the sampling strategy must be carefully designed in order to: a) enable reconstruction of the signal; b) guarantee stability of the algorithm; and c) impose a desired mean-square error at convergence. In particular, we will see that, when sampling signals defined on graphs, besides choosing the right number of samples, whenever possible it is also fundamental to have a strategy indicating where to sample, as the samples\u2019 location plays a key role in the performance of the reconstruction algorithm in (12). To select the best sampling strategy, one should optimize some performance criterion, e.g. the MSD in (26), with respect to the sampling set S, or, equivalently, the vertex limiting operator D. However, since this formulation translates inevitably into a selection problem, whose solution in general requires an exhaustive search over all the possible combinations, the complexity of such procedure becomes intractable also for graph signals of moderate dimensions. Thus, in the sequel we will provide some numerically efficient, albeit sub-optimal, greedy algorithms to tackle the problem of selecting the sampling set. Greedy Selection - Minimum MSD: The first strategy that we consider aims at minimizing the MSD in (26) via a greedy approach: the method iteratively selects the samples from the graph that lead to the largest reduction in terms of MSD. The resulting algorithm is summarized in the table entitled\n6 Sampling strategy 1: Minimization of MSD\nInput Data : M , the number of samples. Output Data : S, the sampling set.\nFunction : initialize S \u2261 \u2205 while |S| < M s = argmin\nj MSD(DS\u222a{j});\nS \u2190 S \u222a {s}; end\n\u201cSampling strategy 1\u201d. In the sequel, we will refer to this method as the Min-MSD strategy. Greedy Selection - Maximum |UHFDUF |: In this case, the strategy aims at maximizing the determinant of the matrix UHFDUF . This is equivalent to selecting the set S of rows of the matrix UF that maximize the (squared) volume of the parallelepiped built with such vectors. In the sequel, we motivate the rationale underlying this strategy.\nLet us consider the eigendecomposition Q = V\u039bVH , where V is the eigenvectors matrix, and \u039b is the diagonal matrix containing its eigenvalues. From (26), we obtain:\nMSD = \u00b52vec(G)T (I\u2212Q)\u22121vec(I)\n= \u00b52vec(G)TV(I\u2212\u039b)\u22121VHvec(I)\n= \u00b52 |F|2\u2211\ni=1\npi \u00b7 qi 1\u2212 \u03bbi(Q)\n(28)\nwhere p = {pi} = V Hvec(G), q = {qi} = V Hvec(I). From (28), we notice how the MSD of the LMS algorithm in (12) strongly depends on the values assumed by the eigenvalues \u03bbi(Q), i = 1, . . . , |F|2. In particular, we would like to design matrix Q in (22) such that its eigenvalues are as far as possible from 1. From (22), it is easy to understand that\n\u03bbi(Q) = ( 1\u2212 \u00b5\u03bbk(U H FDUF) )( 1\u2212 \u00b5\u03bbl(U H FDUF ) )\nk, l = 1, . . . , |F|. Thus, requiring \u03bbi(Q), i = 1, . . . , |F|2, to be as far as possible from 1 translates in designing the matrix UHFDUF \u2208 C\n|F|\u00d7|F| such that its eigenvalues are as far as possible from zero. Thus, a possible surrogate criterion for the approximate minimization of (28) can be formulated as the selection of the sampling set S (i.e. operator D) that maximizes the determinant (i.e. the product of all eigenvalues) of the matrix UHFDUF . Also in this case, we propose a greedy approach, as described in the following table. The algorithm starts including the column with the largest norm in UF , and then it adds, iteratively, the columns having the largest norm and, at the same time, are as orthogonal as possible to the vectors already in S . This is equivalent to maximize the volume of the parallelepiped build with the selected vectors. The resulting algorithm is summarized in the table entitled \u201cSampling strategy 2\u201d. This strategy can be also interpreted as the selection of a well suited basis for the graph signal that the want to estimate. In the sequel, we will refer to this method as the Max-Det sampling strategy. Greedy Selection - Maximum \u03bbmin(U H FDUF ): Finally, using similar arguments as before, a further surrogate criterion\nSampling strategy 2: Maximization of \u2223\u2223\u2223UHFDUF \u2223\u2223\u2223\nInput Data : M , the number of samples. Output Data : S, the sampling set.\nFunction : initialize S \u2261 \u2205 while |S| < M\ns = argmax j\n\u2223\u2223\u2223UHFDS\u222a{j}UF \u2223\u2223\u2223;\nS \u2190 S \u222a {s}; end\nfor the minimization of (28) can be formulated as the maximization of the minimum eigenvalue of the matrix UHFDUF . This greedy strategy was already introduced in [10] in the case of batch signal reconstruction. The resulting algorithm is summarized in the table entitled \u201cSampling strategy 3\u201d. We will refer to this method as the Max-\u03bbmin sampling strategy. Sampling strategy 3: Maximization of \u03bbmin ( UHFDUF )\nInput Data : M , the number of samples. Output Data : S, the sampling set.\nFunction : initialize S \u2261 \u2205 while |S| < M\ns = argmax j \u03bbmin\n( UHFDS\u222a{j}UF ) ;\nS \u2190 S \u222a {s}; end\nIn the sequel, we will illustrate some numerical results aimed at comparing the performance achieved by the proposed LMS algorithm using the aforementioned sampling strategies."}, {"heading": "E. Numerical Results", "text": "In this section, we first illustrate some numerical results aimed at confirming the theoretical results in (26) and (27). Then, we will illustrate how the sampling strategy affects the performance of the proposed LMS algorithm in (12). Finally, we will evaluate the effect of a graph mismatching in the performance of the proposed algorithm. Performance: Let us consider the graph signal shown in Fig. 1 and composed of N = 50 nodes, where the color of each vertex denotes the value of the signal associated to it. The signal has a spectral content limited to the first ten eigenvectors of the Laplacian matrix of the graph in Fig. 1, i.e. |F| = 10. The observation noise in (9) is zero-mean, Gaussian, with a diagonal covariance matrix, where each element is chosen uniformly random between 0 and 0.01. An example of graph sampling, obtained selecting |S| = 10 vertexes using the MaxDet sampling strategy, is also illustrated in Fig. 1, where the sampled vertexes have thicker marker edge. To validate the theoretical results in (27), in Fig. 2 we report the behavior of the theoretical MSD values achieved at each vertex of the graph, comparing them with simulation results, obtained averaging over 200 independent simulations and 100 samples of squared error after convergence of the algorithm. The stepsize is chosen equal to \u00b5 = 0.5 and, together with the selected\n7\nsampling strategy D, they satisfy the reconstruction and stability conditions in (13) and (23). As we can notice from Fig. 2, the theoretical predictions match well the simulation results. Effect of sampling strategies: It is fundamental to assess the performance of the LMS algorithm in (12) with respect to the adopted sampling set S. As a first example, using the Max-Det sampling strategy, in Fig. 3 we report the transient behavior of the MSD, considering different number of samples taken from the graph, i.e. different cardinalities |S| of the sampling set. The results are averaged over 200 independent simulations, and the step-sizes are tuned in order to have the same steadystate MSD for each value of |S|. As expected, from Fig. 3 we notice how the learning rate of the algorithm improves by increasing the number of samples. Finally, in Fig. 4 we illustrate the steady-state MSD of the LMS algorithm in (12) comparing the performance obtained by four different sampling strategies, namely: a) the Max-Det strategy; b) the Max-\u03bbmin strategy; c) the Min-MSD strategy; and d) the random sampling strategy, which simply picks at random |S| nodes. We consider the same parameter setting of the previous simulation. The results are averaged over 200 independent simulations. As we can notice from Fig. 4, the LMS algorithm with random sampling can perform quite poorly, especially at low number of samples. This poor result of random sampling emphasizes that, when sampling a graph signal, what matters is not only the number\nof samples, but also (and most important) where the samples are taken. Comparing the other sampling strategies, we notice from Fig. 4 that the Max-det strategy performs well also at low number of samples (|S| = 10 is the minimum number of samples that allows signal reconstruction), where all the other methods fail. As expected, the Max-Det strategy outperforms the Max-\u03bbmin strategy, because it considers all the modes of the MSD in (28), as opposed to the single mode associated to the minimum eigenvalue considered by the Max-\u03bbmin strategy. It is indeed remarkable that, for low number of samples, Max-Det outperforms also Min-MSD, even if the performance metric is MSD. There is no contradiction here because we need to remember that all the proposed methods are greedy strategies, so that there is no claim of optimality in all of them. However, as the number of samples increases, the Min-MSD strategy outperforms all other methods. This is not only due to the fact that the Min-MSD strategy is matched to the metric used for comparison. It is also a consequence of the fact that Min-MSD exploits information about the spatial distribution of the observation noise (cf. (26)). Thus, increasing the number\n8 0 50 100 150 200 250 300 350 400 \u221220 \u221218 \u221216 \u221214 \u221212 \u221210 \u22128 \u22126 \u22124 \u22122 0\nIteration index\nT ra\nns ie\nnt M\nSD (\ndB )\nLink 1 removed Link 2 removed Link 3 removed Link 4 removed Theoretical MSD\nFig. 5: Transient MSD versus iteration index, for different links removed from the original graph in Fig. 1.\nof samples, this strategy avoids to select the most noisy vertexes of the graph, thus improving the overall performance of the LMS algorithm in (12). This analysis suggests that an optimal design of the sampling strategy for graph signals should take into account processing complexity (in terms of number of samples), prior knowledge (e.g., graph structure, noise distribution), and achievable performance.\nEffect of graph mismatching: In this last example, we aim at illustrating how the performance of the proposed method is affected by a graph mismatching during the processing. To this aim, we take as a benchmark the graph signal in Fig. 1, where the signal bandwidth is set equal to |F| = 10. The bandwidth defines also the sampling operator D, which is selected through the Max-Det strategy, introduced in Sec. III.D, using |S| = 10 samples. Now, we assume that the LMS processing in (12) is performed keeping fixed the sampling operator D, while adopting an operator B in (5) that uses the same bandwidth as for the benchmark case (i.e., the same matrix \u03a3F ), but different GFT operators U, which are generated as the eigenvectors of Laplacian matrices associated to graphs that differs from the benchmark in Fig. 1 for one (removed) link. The aim of this simulation is to quantify the effect of a single link removal on the performance of the LMS strategy in (12). Thus, in Fig. 5, we report the transient MSD versus the iteration index of the proposed LMS strategy, considering four different links that are removed from the original graph. The four removed links are those shown in Fig. 1 using ticker lines; the colors and line styles are associated to the four behaviors of the transient MSD in Fig. 5. The results are averaged over 100 independent simulations, using a stepsize \u00b5 = 0.5. The theoretical performance in (26) achieved by the ideal LMS, i.e. the one perfectly matched to the graph, are also reported as a benchmark. As we can see from Fig. 5, the removal of different links from the graph leads to very different performance obtained by the algorithm. Indeed, while removing Link 1 (i.e., the red one), the algorithm performs as in the ideal case, the removal of links 2, 3, and 4, progressively determine a worse performance loss. This happens because the\nstructure of the eigenvectors of the Laplacian of the benchmark graph is more or less preserved by the removal of specific links. Some links have almost no effects (e.g., Link 1), whereas some others (e.g., Link 4) may lead to deep modification of the structure of such eigenvectors, thus determining the mismatching of the LMS strategy in (12) and, consequently, its performance degradation. This example opens new theoretical questions that aim at understanding which links affect more the graph signals\u2019 estimation performance in situations where both the signal and the graph are jointly time-varying. We plan to tackle this exciting case in a future work."}, {"heading": "IV. LMS ESTIMATION WITH ADAPTIVE GRAPH SAMPLING", "text": "The LMS strategy in (12) assumes perfect knowledge of the support where the signal is defined in the graph frequency domain, i.e. F . Indeed, this prior knowledge allows to define the projector operator B in (5) in a unique manner, and to implement the sampling strategies introduced in Sec. III.D. However, in many practical situations, this prior knowledge is unrealistic, due to the possible variability of the graph signal over time at various levels: the signal can be time varying according to a given model; the signal model may vary over time, for a given graph topology; the graph topology may vary as well over time. In all these situations, we cannot always assume to have prior information about the frequency support F , which must then be inferred directly from the streaming data y[n] in (9). Here, we consider the important case where the graph is fixed, and the spectral content of the signal can vary over time in unknown manner. Exploiting the definition of GFT in (3), the signal observations in (9) can be recast as:\ny[n] =DUs0 +Dv[n]. (29)\nThe problem then translates in estimating the coefficients of the GFT s0, while identifying its support, i.e. the set of indexes where s0 is different from zero. The support identification is deeply related to the selection of the sampling set. Thus, the overall problem can be formulated as the joint estimation of sparse representation s and sampling strategy D from the observations y[n] in (29), i.e.,\nmin s,D\u2208D\nE\u2016y[n]\u2212DUs\u20162 + \u03bb f(s), (30)\nwhere D is the (discrete) set that constraints the selection of the sampling strategy D, f(\u00b7) is a sparsifying penalty function (typically, \u21130 or \u21131 norms), and \u03bb > 0 is a parameter that regulates how sparse we want the optimal GFT vector s. Problem (30) is a mixed integer nonconvex program, which is very complicated to solve, especially in the adaptive context considered in this paper. Thus, to favor low complexity online solutions for (30), we propose an algorithm that alternates between the optimization of the vector s and the selection of the sampling operator D. The rationale behind this choice is that, given an estimate for the support of vector s, i.e. F , we can select the sampling operator D in a very efficient manner through one of the sampling strategies illustrated in Sec. III.D. Then, starting from a random initialization for s and a full sampling for D (i.e., D = I), the algorithm iteratively proceeds as follows. First, fixing the value of the\n9 Algorithm 2: LMS with Adaptive Graph Sampling\nStart with s[0] chosen at random, D[0] = I, and F [0] = V . Given \u00b5 > 0, for each time n > 0, repeat:\n1) s[n+ 1] = T\u03bb\u00b5 ( s[n] + \u00b5UHD[n] (y[n]\u2212Us[n]) ) ;\n2) Set F [n+ 1] = {i \u2208 {1, . . . , N} : si[n+ 1] 6= 0};\n3) Given UF [n+1], select D[n+1] according to one of the criteria proposed in Sec. III.D;\nsampling operator D[n] at time n, we update the estimate of the GFT vector s using an online version of the celebrated ISTA algorithm [58], [59], which proceeds as:\ns[n+ 1] = T \u03bb\u00b5\n( s[n] + \u00b5UHD[n] (y[n]\u2212Us[n]) ) , (31)\nn \u2265 0, where \u00b5 > 0 is a (sufficiently small) step-size, and T \u03b3(s) is a thresholding function that depends on the sparsityinducing penalty f(\u00b7) in (30). Several choices are possible, as we will illustrate in the sequel. The aim of recursion (31) is to estimate the GFT s0 of the graph signal x0 in (9), while selectively shrinking to zero all the components of s0 that are outside its support, i.e., which does not belong to the bandwidth of the graph signal. Then, the online identification of the support of the GFT s0 enables the adaptation of the sampling strategy, which can be updated using one of the strategies illustrated in Sec. III.D. Intuitively, the algorithm will increase (reduce) the number of samples used for the estimation, depending on the increment (reduction) of the current signal bandwidth. The main steps of the LMS algorithm with adaptive graph sampling are listed in Algorithm 2. Thresholding functions : Several different functions can be used to enforce sparsity. A commonly used thresholding function comes directly by imposing an \u21131 norm constraint in (30), which is commonly known as the Lasso [46]. In this case, the vector threshold function T \u03b3(s) is the componentwise thresholding function T\u03b3(sm) applied to each element of vector s, with\nT\u03b3(sm) =   \nsm \u2212 \u03b3, sm > \u03b3; 0, \u2212\u03b3 \u2264 sm \u2264 \u03b3; sm + \u03b3, sm < \u2212\u03b3.\n(32)\nThe function T \u03b3(s) in (32) tends to shrink all the components of the vector s and, in particular, sets to zero the components whose magnitude is within the threshold \u03b3. Since the Lasso constraint is known for introducing a large bias in the estimate, the performance would deteriorate for vectors that are not sufficiently sparse, i.e. graph signals with large bandwidth. To reduce the bias introduced by the Lasso constraint, several other thresholding functions can be adopted to improve the performance also in the case of less sparse systems. A potential improvement can be made by considering the non-negative Garotte estimator as in [60], whose thresholding function is defined as a vector whose entries are derived applying the threshold\nT\u03b3(sm) =\n{ sm (1 \u2212 \u03b32/s2m), |sm| > \u03b3;\n0, |sm| \u2264 \u03b3; (33)\nm = 1, . . . ,M . Finally, to completely remove the bias over the large components, we can implement a hard thresholding mechanism, whose function is defined as a vector whose entries are derived applying the threshold\nT\u03b3(sm) =\n{ sm, |sm| > \u03b3;\n0, |sm| \u2264 \u03b3; (34)\nIn the sequel, numerical results will illustrate how different thresholding functions as (32), (33), and (34), affect the performance of Algorithm 2.\nNumerical Results\nIn this section, we illustrate some numerical results aimed at assessing the performance of the proposed LMS method with adaptive graph sampling, i.e. Algorithm 2. In particular, to illustrate the adaptation capabilities of the algorithm, we simulate a scenario with a time-varying graph signal with N = 50 nodes, which has the same topology shown in Fig. 1, and spectral content that switches between the first 5, 15, and 10 eigenvectors of the Laplacian matrix of the graph. The elements of the GFT s0 inside the support are chosen to be equal to 1. The observation noise in (9) is zero-mean, Gaussian, with a diagonal covariance matrix Cv = \u03c32vI, with \u03c32v = 4\u00d7 10\n\u22124. In Fig. 6 we report the transient behavior of the normalized Mean-Square Deviation (NMSD), i.e.\nNMSD[n] = \u2016s[n]\u2212 s0\u20162\n\u2016s0\u20162 ,\nversus the iteration index, considering the evolution of Algorithm 2 with three different thresholding functions, namely: a) the Lasso threshold in (32), the Garotte threshold in (33), and the hard threshold in (34). Also, in Fig. 7, we illustrate the behavior of the estimate of the cardinality of F versus the iteration index (cf. Step 2 of Algorithm 2), obtained by the three aforementioned strategies at each iteration. The value of the cardinality of F of the true underlying graph signal is also reported as a benchmark. The curves are averaged over 100 independent simulations. The step-size is chosen to be \u00b5 = 0.5, the sparsity parameter \u03bb = 0.1, and thus the threshold is equal to \u03b3 = \u00b5\u03bb = 0.05 for all strategies. The sampling strategy used in Step 3 of Algorithm 2 is the MaxDet method introduced in Sec. III.D, where the number of samples M [n] to be selected at each iteration is chosen to be equal to the current estimate of cardinality of the set F [n]. As we can notice from Fig. 6, the LMS algorithm with adaptive graph sampling is able to track time-varying scenarios, and its performance is affected by the adopted thresholding function. In particular, from Fig. 6, we notice how the algorithm based on the hard thresholding function in (34) outperforms the other strategies in terms of steady-state NMSD, while having the same learning rate. The Garotte based algorithm has slightly worse performance with respect to the method exploiting hard thresholding, due to the residual bias introduced at large values by the thresholding function in (33). Finally, we can notice how the LMS algorithm based on Lasso may lead to very poor performance, due to misidentifications of the true graph bandwidth. This can be noticed from Fig. 7 where, while\n10\nthe Garotte and hard thresholding strategies are able to learn exactly the true bandwidth of the graph signal (thus leading to very good performance in terms of NMSD, see Fig. 6), the Lasso strategy overestimates the bandwidth of the signal, i.e. the cardinality of the set F (thus leading to poor estimation performance, see Fig. 6). Finally, to illustrate an example of adaptive sampling, in Figs. 8, 9, and 10 we report the samples (depicted as black nodes) chosen by the proposed LMS algorithm based on hard thresholding at iterations n = 80, n = 180, and n = 280. As we can notice from Figs. 6, 7 and 8, 9, and 10, the algorithm always selects a number of samples equal to the current value of the signal bandwidth, while guaranteeing good reconstruction performance."}, {"heading": "V. APPLICATION TO POWER SPATIAL DENSITY", "text": "ESTIMATION IN COGNITIVE NETWORKS\nThe advent of intelligent networking of heterogeneous devices such as those deployed to monitor the 5G networks, power grid, transportation networks, and the Internet, will have\na strong impact on the underlying systems. Ensuring compliance to service-level agreements necessitates breakthrough management and monitoring tools providing operators with a comprehensive view of the network landscape. Situational awareness provided by such tools will be the key enabler for effective information dissemination, routing and congestion control, network health management, risk analysis, and security assurance. The vision is for ubiquitous smart network devices to enable data-driven statistical learning algorithms for distributed, robust, and online network operation and\n11\nmanagement, adaptable to the dynamically evolving network landscape with minimal need for human intervention. In this context, the unceasing demand for continuous situational awareness in cognitive radio (CR) networks calls for innovative signal processing algorithms, complemented by sensing platforms to accomplish the objectives of layered sensing and control. These challenges are embraced in the study of power cartography, where CRs collect data to estimate the distribution of power across space, namely the power spatial density (PSD). Knowing the PSD at any location allows CRs to dynamically implement a spatial reuse of idle bands. The estimated PSD map need not be extremely accurate, but precise enough to identify idle spatial regions.\nIn this section, we apply the proposed framework for LMS estimation of graph signals to spectrum cartography in cognitive networks. We consider a 5G scenario, where a dense deployment of radio access points (RAPs) is envisioned to provide a service environment characterized by very low latency and high rate access. Each RAP collects streaming data related to the spectrum utilization of primary users (PU\u2019s) at its geographical position. This information can then be sent to a processing center, which collects data from the entire system, through high speed wired links. The aim of the center is then to build a spatial map of the spectrum usage, while processing the received data on the fly and envisaging proper sampling techniques that enable a proactive sensing of the system from only a limited number of RAP\u2019s measurements. As we will see in the sequel, the proposed approach hinges on the graph structure of the signal received from the RAP\u2019s, thus enabling real-time PSD estimation from a small set of observations that are smartly sampled from the graph. Numerical examples: Let us consider an operating region where 100 RAPs are randomly deployed to produce a map of the spatial distribution of power generated by the transmissions of two active primary users. The PU\u2019s emit electromagnetic radiation with power equal to 1 Watt. For simplicity, the propagation medium is supposed to introduce a free-space path\nloss attenuation on the PU\u2019s transmissions. The graph among RAPs is built from a distance based model, i.e. stations that are sufficiently close to each other are connected through a link (i.e. aij = 1, if nodes i and j are neighbors). In Fig. 11, we illustrate a pictorial description of the scenario, and of the resulting graph signal. We assume that each RAP is equipped with an energy detector, which estimates the received signal using 100 samples, considering an additive white Gaussian noise with variance \u03c32v = 10\n\u22124. The resulting signal is not perfectly band-limited, but it turns out to be smooth over the graph, i.e. neighbor nodes observe similar values. This implies that sampling such signals inevitably introduces aliasing during the reconstruction process. However, even if we cannot find a limited (lower than N ) set of frequencies where the signal is completely localized, the greatest part of the signal energy is concentrated at low frequencies. This means that if we process the data using a sufficient number of observations and (low) frequencies, we should still be able to reconstruct the signal with a satisfactory performance.\nTo illustrate an example of cartography based on the LMS algorithm (12), in Fig. 12 we report the behavior of the steadystate NMSD versus the number of samples taken from the graph, for different bandwidths used for processing. The stepsize is chosen equal to 0.5, while the adopted sampling strategy is the Max-Det method introduced in Sec. III.D. The results are averaged over 200 independent simulations. As expected, from Fig. 12, we notice that the steady-state NMSD of the LMS algorithm in (12) improves by increasing the number of samples and bandwidths used for processing. Interestingly, in Fig. 12 we can see a sort of threshold behavior: the NMSD is large for |S| < |F|, when the signal is undersampled, whereas the values become lower and stable as soon as |S| > |F|. Finally, we illustrate an example that shows the tracking capability of the proposed method to time-varying scenarios. In particular, we simulate a situation the two PU\u2019s switch between idle and active modes: for 0 \u2264 n < 133 only the first PU transmits; for 133 \u2264 n < 266 both PU\u2019s transmit; for 266 \u2264 n \u2264 400 only the second PU\u2019s transmits. In Fig. 13\n12\nwe show the behavior of the transient NMSD versus iteration index, for different number of samples and bandwidths used for processing. The results are averaged over 200 independent simulations. From Fig. 13, we can see how the proposed technique can track time-varying scenarios. Furthermore, its steady-state performance improves by increasing the number of samples and bandwidths used for processing. These results, together with those achieved in Fig. 12, illustrate an existing tradeoff between complexity, i.e. number of samples used for processing, and mean-square performance of the proposed LMS strategy. In particular, using a larger bandwidth and a (consequent) larger number of samples for processing, the performance of the algorithm improves, at the price of a larger computational complexity."}, {"heading": "VI. CONCLUSIONS", "text": "In this paper we have proposed LMS strategies for the online estimation of signals defined over graphs. The proposed strategies are able to exploit the underlying structure of the graph signal, which can be reconstructed from a limited number of observations properly sampled from a subset of vertexes, under a band-limited assumption. A detailed mean square analysis illustrates the deep connection between sampling strategy and the properties of the proposed LMS algorithm in terms of reconstruction capability, stability, and mean-square error performance. From this analysis, some sampling strategies for adaptive estimation of graph signals are also derived. Furthermore, to cope with time-varying scenarios, we also propose an LMS method with adaptive graph sampling, which estimates and track the signal support in the (graph)frequency domain, while at the same time adapting the graph sampling strategy. Several numerical simulations confirm the theoretical findings, and illustrate the potential advantages achieved by these strategies for online estimation of band-limited graph signals. Finally, we apply the proposed method to estimate and track the spatial distribution of power transmitted by primary users in a cognitive network environment, thus illustrating\nthe existing tradeoff between complexity and mean-square performance of the proposed strategy.\nWe expect that such processing tools will represent a key technology for the design and proactive sensing of Cyber Physical Systems, where a proper adaptive control mechanism requires the availability of data driven sampling strategies able to control the overall system by only checking a limited number of nodes, in order to collect correct information at the right time, in the right place, and for the right purpose."}], "references": [{"title": "The emerging field of signal processing on graphs: Extending high-dimensional data analysis to networks and other irregular domains", "author": ["D.I. Shuman", "S.K. Narang", "P. Frossard", "A. Ortega", "P. Vandergheynst"], "venue": "IEEE Signal Proc. Mag., vol. 30, no. 3, pp. 83\u201398, 2013.", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2013}, {"title": "Discrete signal processing on graphs", "author": ["A. Sandryhaila", "J.M.F. Moura"], "venue": "IEEE Trans. on Signal Processing, vol. 61, pp. 1644\u20131656, 2013.", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2013}, {"title": "Big data analysis with signal processing on graphs: Representation and processing of massive data sets with irregular structure", "author": ["\u2014\u2014"], "venue": "IEEE Signal Proc. Mag., vol. 31, no. 5, pp. 80\u201390, 2014.", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2014}, {"title": "Discrete signal processing on graphs: Frequency analysis", "author": ["A. Sandryhaila", "J.M. Moura"], "venue": "IEEE Transactions on Signal Processing, vol. 62, no. 12, pp. 3042\u20133054, 2014.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2014}, {"title": "Perfect reconstruction two-channel wavelet filter banks for graph structured data", "author": ["S.K. Narang", "A. Ortega"], "venue": "IEEE Transactions on Signal Processing, vol. 60, no. 6, pp. 2786\u20132799, 2012.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2012}, {"title": "Compact support biorthogonal wavelet filterbanks for arbitrary undirected graphs", "author": ["\u2014\u2014"], "venue": "IEEE Transactions on Signal Processing, vol. 61, no. 19, pp. 4673\u20134685, 2013.", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2013}, {"title": "Sampling in Paley-Wiener spaces on combinatorial graphs", "author": ["I.Z. Pesenson"], "venue": "Trans. of the American Mathematical Society, vol. 360, no. 10, pp. 5603\u20135627, 2008.", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2008}, {"title": "Sampling, filtering and sparse approximations on combinatorial graphs", "author": ["I.Z. Pesenson", "M.Z. Pesenson"], "venue": "Journal of Fourier Analysis and Applications, vol. 16, no. 6, pp. 921\u2013942, 2010.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2010}, {"title": "Approximating signals supported on graphs", "author": ["X. Zhu", "M. Rabbat"], "venue": "IEEE Int. Conf. on Acoustics, Speech and Signal Processing (ICASSP), March 2012, pp. 3921\u20133924.", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2012}, {"title": "Discrete signal processing on graphs: Sampling theory", "author": ["S. Chen", "R. Varma", "A. Sandryhaila", "J. Kova\u010devi\u0107"], "venue": "IEEE Trans. on Signal Proc., vol. 63, pp. 6510\u20136523, Dec. 2015.", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2015}, {"title": "Algebraic signal processing theory: Foundation and 1-D time", "author": ["M. P\u00fcschel", "J.M.F. Moura"], "venue": "IEEE Trans. Signal Process., vol. 56, pp. 3572\u20133585, 2008.", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2008}, {"title": "Algebraic signal processing theory: 1-D space", "author": ["\u2014\u2014"], "venue": "IEEE Trans. on Signal Processing, pp. 3586\u20133599, 2008.", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2008}, {"title": "On the graph fourier transform for directed graphs", "author": ["S. Sardellitti", "S. Barbarossa"], "venue": "available at: http://arxiv.org/abs/1601.05972, 2016.", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2016}, {"title": "A spectral graph uncertainty principle", "author": ["A. Agaskar", "Y.M. Lu"], "venue": "IEEE Trans. on Inform. Theory, vol. 59, no. 7, pp. 4338\u20134356, 2013.", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2013}, {"title": "Toward an uncertainty principle for weighted graphs", "author": ["B. Pasdeloup", "R. Alami", "V. Gripon", "M. Rabbat"], "venue": "online: arXiv:1503.03291, 2015.", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2015}, {"title": "Graph theoretic uncertainty principles", "author": ["J.J. Benedetto", "P.J. Koprowski"], "venue": "http://www.math.umd.edu/ jjb/graph theoretic UP April 14.pdf, 2015.", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2015}, {"title": "Finite frames and graph theoretic uncertainty principles", "author": ["P.J. Koprowski"], "venue": "Ph.D. dissertation, 2015. [Online]. Available: http://hdl.handle.net/1903/16666", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2015}, {"title": "Signals on graphs: Uncertainty principle and sampling", "author": ["M. Tsitsvero", "S. Barbarossa", "P. Di Lorenzo"], "venue": "submitted to IEEE Trans. on Signal Processing; available at http://arxiv.org/abs/1507.08822, 2015.", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2015}, {"title": "Signal processing techniques for interpolation in graph structured data", "author": ["S. Narang", "A. Gadde", "A. Ortega"], "venue": "IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), May 2013, pp. 5445\u20135449.", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2013}, {"title": "Local-set-based graph signal reconstruction", "author": ["X. Wang", "P. Liu", "Y. Gu"], "venue": "IEEE Trans. on Signal Processing, vol. 63, no. 9, pp. 2432\u20132444, 2015.", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2015}, {"title": "Sampling of graph signals with successive local aggregations", "author": ["A.G. Marquez", "S. Segarra", "G. Leus", "A. Ribeiro"], "venue": "To appear on IEEE Trans. Signal Process., 2016.  13", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2016}, {"title": "On the degrees of freedom of signals on graphs", "author": ["M. Tsitsvero", "S. Barbarossa"], "venue": "2015 European Signal Proc. Conf. (Eusipco 2015), Sep. 2015, pp. 1521\u20131525.", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2015}, {"title": "Localized iterative methods for interpolation in graph structured data", "author": ["S.K. Narang", "A. Gadde", "E. Sanou", "A. Ortega"], "venue": "IEEE Global Conference on Signal and Information Processing, 2013.", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2013}, {"title": "A distributed tracking algorithm for reconstruction of graph signals", "author": ["X. Wang", "M. Wang", "Y. Gu"], "venue": "IEEE Journal of Selected Topics in Signal Processing, vol. 9, no. 4, pp. 728\u2013740, 2015.", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2015}, {"title": "Semi-supervised multiresolution classification using adaptive graph filtering with application to indirect bridge structural health monitoring", "author": ["S. Chen", "F. Cerda", "P. Rizzo", "J. Bielak", "J.H. Garrett", "J. Kovacevic"], "venue": "IEEE Trans. on Signal Processing, vol. 62, no. 11, pp. 2879\u20132893, 2014.", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2014}, {"title": "Classification via regularization on graphs.", "author": ["A. Sandryhaila", "J.M. Moura"], "venue": "Proc. of IEEE Global conference on Signal and Information Processing,", "citeRegEx": "26", "shortCiteRegEx": "26", "year": 2013}, {"title": "Waveletregularized graph semi-supervised learning", "author": ["V.N. Ekambaram", "G. Fanti", "B. Ayazifar", "K. Ramchandran"], "venue": "IEEE Global Conference on Signal and Information Processing, 2013, pp. 423\u2013426.", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2013}, {"title": "Learning of structured graph dictionaries", "author": ["X. Zhang", "X. Dong", "P. Frossard"], "venue": "IEEE International Conference on Acoustics, Speech and Signal Processing, 2012, pp. 3373\u20133376.", "citeRegEx": "28", "shortCiteRegEx": null, "year": 2012}, {"title": "Parametric dictionary learning for graph signals", "author": ["D. Thanou", "D.I. Shuman", "P. Frossard"], "venue": "IEEE Global Conference on Signal and Information Processing, 2013, pp. 487\u2013490.", "citeRegEx": "29", "shortCiteRegEx": null, "year": 2013}, {"title": "Learning graphs from signal observations under smoothness prior", "author": ["X. Dong", "D. Thanou", "P. Frossard", "P. Vandergheynst"], "venue": "submitted to: IEEE Transacctions on Signal Processing, 2014.", "citeRegEx": "30", "shortCiteRegEx": null, "year": 2014}, {"title": "A regularization framework for learning from graph data", "author": ["D. Zhou", "B. Sch\u00f6lkopf"], "venue": "ICML workshop on statistical relational learning and Its connections to other fields, vol. 15, 2004, pp. 67\u201368.", "citeRegEx": "31", "shortCiteRegEx": null, "year": 2004}, {"title": "Manifold regularization: A geometric framework for learning from labeled and unlabeled examples", "author": ["M. Belkin", "P. Niyogi", "V. Sindhwani"], "venue": "The J. of Machine Learning Research, vol. 7, pp. 2399\u20132434, 2006.", "citeRegEx": "32", "shortCiteRegEx": null, "year": 2006}, {"title": "Signal inpainting on graphs via total variation minimization", "author": ["S. Chen", "A. Sandryhaila", "G. Lederman", "Z. Wang", "J.M. Moura", "P. Rizzo", "J. Bielak", "J.H. Garrett", "J. Kovacevic"], "venue": "IEEE International Conference on Acoustics, Speech and Signal Processing, 2014, pp. 8267\u20138271.", "citeRegEx": "33", "shortCiteRegEx": null, "year": 2014}, {"title": "Signal denoising on graphs via graph filtering", "author": ["S. Chen", "A. Sandryhaila", "J.M. Moura", "J. Kovacevic"], "venue": "IEEE Global Conference on Signal and Information Processing, 2014, pp. 872\u2013876.", "citeRegEx": "34", "shortCiteRegEx": null, "year": 2014}, {"title": "Local fiedler vector centrality for detection of deep and overlapping communities in networks", "author": ["P.-Y. Chen", "A.O. Hero"], "venue": "IEEE International Conference on Acoustics, Speech and Signal Processing, 2014, pp. 1120\u20131124.", "citeRegEx": "35", "shortCiteRegEx": null, "year": 2014}, {"title": "Signal recovery on graphs: Variation minimization", "author": ["S. Chen", "A. Sandryhaila", "J.M. Moura", "J. Kovacevic"], "venue": "IEEE Transactions on Signal Processing, vol. 63, no. 17, pp. 4609\u20134624, 2015.", "citeRegEx": "36", "shortCiteRegEx": null, "year": 2015}, {"title": "Signal recovery on graphs: Fundamental limits of sampling strategies", "author": ["S. Chen", "R. Varma", "A. Singh", "J. Kova\u010devi\u0107"], "venue": "arXiv preprint arXiv:1512.05405, 2015.", "citeRegEx": "37", "shortCiteRegEx": null, "year": 2015}, {"title": "Adaptive filters", "author": ["A.H. Sayed"], "venue": null, "citeRegEx": "38", "shortCiteRegEx": "38", "year": 2011}, {"title": "Adaptive signal processing", "author": ["B. Widrow", "S.D. Stearns"], "venue": "Englewood Cliffs, NJ, Prentice-Hall, Inc.,, vol. 1, 1985.", "citeRegEx": "40", "shortCiteRegEx": null, "year": 1985}, {"title": "Compressed sensing", "author": ["D.L. Donoho"], "venue": "IEEE Transactions on Information Theory, vol. 52, no. 4, pp. 1289\u20131306, 2006.", "citeRegEx": "41", "shortCiteRegEx": null, "year": 2006}, {"title": "Compressive sensing", "author": ["R.G. Baraniuk"], "venue": "IEEE signal processing magazine, vol. 24, no. 4, 2007.", "citeRegEx": "42", "shortCiteRegEx": null, "year": 2007}, {"title": "LMS estimation via structural detection", "author": ["J. Homer", "I. Mareels", "R.R. Bitmead", "B. Wahlberg", "F. Gustafsson"], "venue": "IEEE Transactions on Signal Processing, vol. 46, no. 10, pp. 2651\u20132663, 1998.", "citeRegEx": "43", "shortCiteRegEx": null, "year": 1998}, {"title": "Partial update LMS algorithms", "author": ["M. Godavarti", "A.O. Hero III"], "venue": "IEEE Trans. on Signal Processing, vol. 53, no. 7, pp. 2382\u20132399, 2005.", "citeRegEx": "44", "shortCiteRegEx": null, "year": 2005}, {"title": "Proportionate normalized least-mean-squares adaptation in echo cancelers", "author": ["D.L. Duttweiler"], "venue": "IEEE Transactions on Speech and Audio Processing, vol. 8, no. 5, pp. 508\u2013518, 2000.", "citeRegEx": "45", "shortCiteRegEx": null, "year": 2000}, {"title": "Regression shrinkage and selection via the lasso", "author": ["R. Tibshirani"], "venue": "Journal of the Royal Statistical Society. Series B (Methodological), pp. 267\u2013288, 1996.", "citeRegEx": "46", "shortCiteRegEx": null, "year": 1996}, {"title": "Enhancing sparsity by reweighted l 1 minimization", "author": ["E.J. Candes", "M.B. Wakin", "S.P. Boyd"], "venue": "Journal of Fourier analysis and applications, vol. 14, no. 5-6, pp. 877\u2013905, 2008.", "citeRegEx": "47", "shortCiteRegEx": null, "year": 2008}, {"title": "Sparse LMS for system identification", "author": ["Y. Chen", "Y. Gu", "A.O. Hero III"], "venue": "IEEE International Conference on Acoustics, Speech and Signal Processing, 2009, pp. 3125\u20133128.", "citeRegEx": "48", "shortCiteRegEx": null, "year": 2009}, {"title": "Online adaptive estimation of sparse signals: Where RLS meets the-norm", "author": ["D. Angelosante", "J.A. Bazerque", "G.B. Giannakis"], "venue": "IEEE Transactions on Signal Processing, vol. 58, no. 7, pp. 3436\u20133447, 2010.", "citeRegEx": "49", "shortCiteRegEx": null, "year": 2010}, {"title": "Online sparse system identification and signal reconstruction using projections onto weighted balls", "author": ["Y. Kopsinis", "K. Slavakis", "S. Theodoridis"], "venue": "IEEE Transactions on Signal Processing, vol. 59, no. 3, pp. 936\u2013 952, 2011.", "citeRegEx": "50", "shortCiteRegEx": null, "year": 2011}, {"title": "A sparsity promoting adaptive algorithm for distributed learning", "author": ["S. Chouvardas", "K. Slavakis", "Y. Kopsinis", "S. Theodoridis"], "venue": "IEEE Transactions on Signal Processing, vol. 60, no. 10, pp. 5412\u20135425, 2012.", "citeRegEx": "51", "shortCiteRegEx": null, "year": 2012}, {"title": "Sparse distributed learning based on diffusion adaptation", "author": ["P. Di Lorenzo", "A.H. Sayed"], "venue": "IEEE Transactions on Signal Processing, vol. 61, no. 6, pp. 1419\u20131433, 2013.", "citeRegEx": "52", "shortCiteRegEx": null, "year": 2013}, {"title": "Distributed spectrum estimation for small cell networks based on sparse diffusion adaptation", "author": ["P. Di Lorenzo", "S. Barbarossa", "A.H. Sayed"], "venue": "IEEE Signal Processing Letters, vol. 20, no. 12, pp. 1261\u20131265, 2013.", "citeRegEx": "53", "shortCiteRegEx": null, "year": 2013}, {"title": "Diffusion adaptation strategies for distributed estimation over gaussian markov random fields", "author": ["P. Di Lorenzo"], "venue": "IEEE Transactions on Signal Processing, vol. 62, no. 21, pp. 5748\u20135760, 2014.", "citeRegEx": "54", "shortCiteRegEx": null, "year": 2014}, {"title": "Distributed Detection and Estimation in Wireless Sensor Networks", "author": ["S. Barbarossa", "S. Sardellitti", "P. Di Lorenzo"], "venue": "Academic Press Library in Signal Processing,", "citeRegEx": "55", "shortCiteRegEx": "55", "year": 2014}, {"title": "Energy conservation and the learning ability of LMS adaptive filters", "author": ["A.H. Sayed", "V.H. Nascimento"], "venue": null, "citeRegEx": "56", "shortCiteRegEx": "56", "year": 2003}, {"title": "An iterative thresholding algorithm for linear inverse problems with a sparsity constraint", "author": ["I. Daubechies", "M. Defrise", "C. De Mol"], "venue": "Communications on pure and applied mathematics, vol. 57, no. 11, pp. 1413\u2013 1457, 2004.", "citeRegEx": "58", "shortCiteRegEx": null, "year": 2004}, {"title": "A fast iterative shrinkage-thresholding algorithm for linear inverse problems", "author": ["A. Beck", "M. Teboulle"], "venue": "SIAM journal on imaging sciences, vol. 2, no. 1, pp. 183\u2013202, 2009.", "citeRegEx": "59", "shortCiteRegEx": null, "year": 2009}, {"title": "On the non-negative garrotte estimator", "author": ["M. Yuan", "Y. Lin"], "venue": "Journal of the Royal Statistical Society: Series B (Statistical Methodology), vol. 69, no. 2, pp. 143\u2013161, 2007.", "citeRegEx": "60", "shortCiteRegEx": null, "year": 2007}], "referenceMentions": [{"referenceID": 0, "context": "In many applications, from sensor to social networks, vehicular networks, big data or biological networks, the signals of interest are defined over the vertices of a graph [1].", "startOffset": 172, "endOffset": 175}, {"referenceID": 0, "context": "Over the last few years, a series of papers produced a significant advancement in the development of processing tools for the analysis of signals defined over a graph, or graph signals for short [1]\u2013[3].", "startOffset": 195, "endOffset": 198}, {"referenceID": 2, "context": "Over the last few years, a series of papers produced a significant advancement in the development of processing tools for the analysis of signals defined over a graph, or graph signals for short [1]\u2013[3].", "startOffset": 199, "endOffset": 202}, {"referenceID": 1, "context": "Processing signals defined over a graph has been considered in [2], [4]\u2013[6].", "startOffset": 63, "endOffset": 66}, {"referenceID": 3, "context": "Processing signals defined over a graph has been considered in [2], [4]\u2013[6].", "startOffset": 68, "endOffset": 71}, {"referenceID": 5, "context": "Processing signals defined over a graph has been considered in [2], [4]\u2013[6].", "startOffset": 72, "endOffset": 75}, {"referenceID": 6, "context": "Alternative definitions of GFT have been proposed, depending on the different perspectives used to extend classical tools [7], [8], [1], [9], [2].", "startOffset": 122, "endOffset": 125}, {"referenceID": 7, "context": "Alternative definitions of GFT have been proposed, depending on the different perspectives used to extend classical tools [7], [8], [1], [9], [2].", "startOffset": 127, "endOffset": 130}, {"referenceID": 0, "context": "Alternative definitions of GFT have been proposed, depending on the different perspectives used to extend classical tools [7], [8], [1], [9], [2].", "startOffset": 132, "endOffset": 135}, {"referenceID": 8, "context": "Alternative definitions of GFT have been proposed, depending on the different perspectives used to extend classical tools [7], [8], [1], [9], [2].", "startOffset": 137, "endOffset": 140}, {"referenceID": 1, "context": "Alternative definitions of GFT have been proposed, depending on the different perspectives used to extend classical tools [7], [8], [1], [9], [2].", "startOffset": 142, "endOffset": 145}, {"referenceID": 6, "context": ", [7], [1], [9] or of the adjacency matrix, see, e.", "startOffset": 2, "endOffset": 5}, {"referenceID": 0, "context": ", [7], [1], [9] or of the adjacency matrix, see, e.", "startOffset": 7, "endOffset": 10}, {"referenceID": 8, "context": ", [7], [1], [9] or of the adjacency matrix, see, e.", "startOffset": 12, "endOffset": 15}, {"referenceID": 1, "context": "[2], [10].", "startOffset": 0, "endOffset": 3}, {"referenceID": 9, "context": "[2], [10].", "startOffset": 5, "endOffset": 9}, {"referenceID": 10, "context": "The first approach applies to undirected graphs and builds on the spectral clustering properties of the Laplacian eigenvectors and the minimization of the l2 norm graph total variation; the second approach was proposed to handle also directed graphs and it is based on the interpretation of the adjacency operator as the graph shift operator, which lies at the heart of all linear shift-invariant filtering methods for graph signals [11], [12].", "startOffset": 433, "endOffset": 437}, {"referenceID": 11, "context": "The first approach applies to undirected graphs and builds on the spectral clustering properties of the Laplacian eigenvectors and the minimization of the l2 norm graph total variation; the second approach was proposed to handle also directed graphs and it is based on the interpretation of the adjacency operator as the graph shift operator, which lies at the heart of all linear shift-invariant filtering methods for graph signals [11], [12].", "startOffset": 439, "endOffset": 443}, {"referenceID": 12, "context": "A further very recent contribution proposes to build the graph Fourier basis as the set of orthonormal signals that minimize the (directed) graph cut size [13].", "startOffset": 155, "endOffset": 159}, {"referenceID": 13, "context": "After the introduction of the GFT, an uncertainty principle for graph signals was derived in [14] and, more recently [15], [16], [17], [18].", "startOffset": 93, "endOffset": 97}, {"referenceID": 14, "context": "After the introduction of the GFT, an uncertainty principle for graph signals was derived in [14] and, more recently [15], [16], [17], [18].", "startOffset": 117, "endOffset": 121}, {"referenceID": 15, "context": "After the introduction of the GFT, an uncertainty principle for graph signals was derived in [14] and, more recently [15], [16], [17], [18].", "startOffset": 123, "endOffset": 127}, {"referenceID": 16, "context": "After the introduction of the GFT, an uncertainty principle for graph signals was derived in [14] and, more recently [15], [16], [17], [18].", "startOffset": 129, "endOffset": 133}, {"referenceID": 17, "context": "After the introduction of the GFT, an uncertainty principle for graph signals was derived in [14] and, more recently [15], [16], [17], [18].", "startOffset": 135, "endOffset": 139}, {"referenceID": 17, "context": "In particular, in [18], the authors give simple closed form expressions for the fundamental tradeoff between the concentrations of a signal in the graph and the transformed domains.", "startOffset": 18, "endOffset": 22}, {"referenceID": 6, "context": "A seminal contribution was given in [7], later extended in [19] and, very recently, in [10], [18], [20], [21], [22].", "startOffset": 36, "endOffset": 39}, {"referenceID": 18, "context": "A seminal contribution was given in [7], later extended in [19] and, very recently, in [10], [18], [20], [21], [22].", "startOffset": 59, "endOffset": 63}, {"referenceID": 9, "context": "A seminal contribution was given in [7], later extended in [19] and, very recently, in [10], [18], [20], [21], [22].", "startOffset": 87, "endOffset": 91}, {"referenceID": 17, "context": "A seminal contribution was given in [7], later extended in [19] and, very recently, in [10], [18], [20], [21], [22].", "startOffset": 93, "endOffset": 97}, {"referenceID": 19, "context": "A seminal contribution was given in [7], later extended in [19] and, very recently, in [10], [18], [20], [21], [22].", "startOffset": 99, "endOffset": 103}, {"referenceID": 20, "context": "A seminal contribution was given in [7], later extended in [19] and, very recently, in [10], [18], [20], [21], [22].", "startOffset": 105, "endOffset": 109}, {"referenceID": 21, "context": "A seminal contribution was given in [7], later extended in [19] and, very recently, in [10], [18], [20], [21], [22].", "startOffset": 111, "endOffset": 115}, {"referenceID": 22, "context": "Alternative signal reconstuction methods have been proposed, either iterative as in [23], [20], [24], or single shot, as in [10], [18].", "startOffset": 84, "endOffset": 88}, {"referenceID": 19, "context": "Alternative signal reconstuction methods have been proposed, either iterative as in [23], [20], [24], or single shot, as in [10], [18].", "startOffset": 90, "endOffset": 94}, {"referenceID": 23, "context": "Alternative signal reconstuction methods have been proposed, either iterative as in [23], [20], [24], or single shot, as in [10], [18].", "startOffset": 96, "endOffset": 100}, {"referenceID": 9, "context": "Alternative signal reconstuction methods have been proposed, either iterative as in [23], [20], [24], or single shot, as in [10], [18].", "startOffset": 124, "endOffset": 128}, {"referenceID": 17, "context": "Alternative signal reconstuction methods have been proposed, either iterative as in [23], [20], [24], or single shot, as in [10], [18].", "startOffset": 130, "endOffset": 134}, {"referenceID": 6, "context": "Frame-based approaches to reconstruct signals from subsets of samples have been proposed in [7], [20], [18].", "startOffset": 92, "endOffset": 95}, {"referenceID": 19, "context": "Frame-based approaches to reconstruct signals from subsets of samples have been proposed in [7], [20], [18].", "startOffset": 97, "endOffset": 101}, {"referenceID": 17, "context": "Frame-based approaches to reconstruct signals from subsets of samples have been proposed in [7], [20], [18].", "startOffset": 103, "endOffset": 107}, {"referenceID": 24, "context": "The theory developed in the last years for GSP was then applied to solve specific learning tasks, such as semi-supervised classification on graphs [25]\u2013[27], graph dictionary learning [28], [29], learning graphs structures [30], smooth graph signal recovery from random samples [31], [32], inpainting [33], denoising [34], and community detection on graphs [35].", "startOffset": 147, "endOffset": 151}, {"referenceID": 26, "context": "The theory developed in the last years for GSP was then applied to solve specific learning tasks, such as semi-supervised classification on graphs [25]\u2013[27], graph dictionary learning [28], [29], learning graphs structures [30], smooth graph signal recovery from random samples [31], [32], inpainting [33], denoising [34], and community detection on graphs [35].", "startOffset": 152, "endOffset": 156}, {"referenceID": 27, "context": "The theory developed in the last years for GSP was then applied to solve specific learning tasks, such as semi-supervised classification on graphs [25]\u2013[27], graph dictionary learning [28], [29], learning graphs structures [30], smooth graph signal recovery from random samples [31], [32], inpainting [33], denoising [34], and community detection on graphs [35].", "startOffset": 184, "endOffset": 188}, {"referenceID": 28, "context": "The theory developed in the last years for GSP was then applied to solve specific learning tasks, such as semi-supervised classification on graphs [25]\u2013[27], graph dictionary learning [28], [29], learning graphs structures [30], smooth graph signal recovery from random samples [31], [32], inpainting [33], denoising [34], and community detection on graphs [35].", "startOffset": 190, "endOffset": 194}, {"referenceID": 29, "context": "The theory developed in the last years for GSP was then applied to solve specific learning tasks, such as semi-supervised classification on graphs [25]\u2013[27], graph dictionary learning [28], [29], learning graphs structures [30], smooth graph signal recovery from random samples [31], [32], inpainting [33], denoising [34], and community detection on graphs [35].", "startOffset": 223, "endOffset": 227}, {"referenceID": 30, "context": "The theory developed in the last years for GSP was then applied to solve specific learning tasks, such as semi-supervised classification on graphs [25]\u2013[27], graph dictionary learning [28], [29], learning graphs structures [30], smooth graph signal recovery from random samples [31], [32], inpainting [33], denoising [34], and community detection on graphs [35].", "startOffset": 278, "endOffset": 282}, {"referenceID": 31, "context": "The theory developed in the last years for GSP was then applied to solve specific learning tasks, such as semi-supervised classification on graphs [25]\u2013[27], graph dictionary learning [28], [29], learning graphs structures [30], smooth graph signal recovery from random samples [31], [32], inpainting [33], denoising [34], and community detection on graphs [35].", "startOffset": 284, "endOffset": 288}, {"referenceID": 32, "context": "The theory developed in the last years for GSP was then applied to solve specific learning tasks, such as semi-supervised classification on graphs [25]\u2013[27], graph dictionary learning [28], [29], learning graphs structures [30], smooth graph signal recovery from random samples [31], [32], inpainting [33], denoising [34], and community detection on graphs [35].", "startOffset": 301, "endOffset": 305}, {"referenceID": 33, "context": "The theory developed in the last years for GSP was then applied to solve specific learning tasks, such as semi-supervised classification on graphs [25]\u2013[27], graph dictionary learning [28], [29], learning graphs structures [30], smooth graph signal recovery from random samples [31], [32], inpainting [33], denoising [34], and community detection on graphs [35].", "startOffset": 317, "endOffset": 321}, {"referenceID": 34, "context": "The theory developed in the last years for GSP was then applied to solve specific learning tasks, such as semi-supervised classification on graphs [25]\u2013[27], graph dictionary learning [28], [29], learning graphs structures [30], smooth graph signal recovery from random samples [31], [32], inpainting [33], denoising [34], and community detection on graphs [35].", "startOffset": 357, "endOffset": 361}, {"referenceID": 35, "context": "Finally, in [36], [37], the authors proposed signal recovery methods aimed to recover graph signals that are assumed to be smooth with respect to the underlying graph, from sampled, noisy, missing, or corrupted measurements.", "startOffset": 12, "endOffset": 16}, {"referenceID": 36, "context": "Finally, in [36], [37], the authors proposed signal recovery methods aimed to recover graph signals that are assumed to be smooth with respect to the underlying graph, from sampled, noisy, missing, or corrupted measurements.", "startOffset": 18, "endOffset": 22}, {"referenceID": 37, "context": "To the best of our knowledge, this is the first attempt to merge the well established theory of adaptive filtering [38] with the emerging field of signal processing on graphs.", "startOffset": 115, "endOffset": 119}, {"referenceID": 21, "context": "As a consequence, recovering the overall signal from a subset of samples is inevitably affected by aliasing [22].", "startOffset": 108, "endOffset": 112}, {"referenceID": 0, "context": ",N [1], i.", "startOffset": 3, "endOffset": 6}, {"referenceID": 0, "context": ", [1], [9], [2], [10].", "startOffset": 2, "endOffset": 5}, {"referenceID": 8, "context": ", [1], [9], [2], [10].", "startOffset": 7, "endOffset": 10}, {"referenceID": 1, "context": ", [1], [9], [2], [10].", "startOffset": 12, "endOffset": 15}, {"referenceID": 9, "context": ", [1], [9], [2], [10].", "startOffset": 17, "endOffset": 21}, {"referenceID": 6, "context": "The space of all signals whose GFT is exactly supported on the set F is known as the Paley-Wiener space for the set F [7].", "startOffset": 118, "endOffset": 121}, {"referenceID": 21, "context": "The localization properties of graph signals were studied in [22] and later extended in [18] to derive the fundamental tradeoff between the localization of a signal in the graph and on its dual domain.", "startOffset": 61, "endOffset": 65}, {"referenceID": 17, "context": "The localization properties of graph signals were studied in [22] and later extended in [18] to derive the fundamental tradeoff between the localization of a signal in the graph and on its dual domain.", "startOffset": 88, "endOffset": 92}, {"referenceID": 21, "context": "The conditions for having perfect localization are stated in the following theorem, which we report here for completeness of exposition; its proof can be found in [22].", "startOffset": 163, "endOffset": 167}, {"referenceID": 17, "context": "Indeed, since the operators BD and DB have the same singular values [18], perfect localization onto the sets S and F can be achieved if and only if", "startOffset": 68, "endOffset": 72}, {"referenceID": 38, "context": "The least mean square algorithm, introduced by Widrow and Hoff [40], is one of the most popular methods for adaptive filtering.", "startOffset": 63, "endOffset": 67}, {"referenceID": 37, "context": "Although there exist algorithms with faster convergence rates such as the Recursive Least Square (RLS) methods [38], LMS-type methods are popular because of its ease of implementation, low computational costs and robustness.", "startOffset": 111, "endOffset": 115}, {"referenceID": 39, "context": "For instance, if the observed signal is known to be sparse in some domain, such prior information can help improve the estimation performance, as demonstrated in many recent efforts in the area of compressed sensing [41], [42].", "startOffset": 216, "endOffset": 220}, {"referenceID": 40, "context": "For instance, if the observed signal is known to be sparse in some domain, such prior information can help improve the estimation performance, as demonstrated in many recent efforts in the area of compressed sensing [41], [42].", "startOffset": 222, "endOffset": 226}, {"referenceID": 41, "context": "Some of the early works that mix adaptation with sparsity-aware reconstruction include methods that rely on the heuristic selection of active taps [43], and on sequential partial updating techniques [44]; some other methods assign proportional step-sizes to different taps according to their magnitudes, such as the proportionate normalized LMS (PNLMS) algorithm and its variations [45].", "startOffset": 147, "endOffset": 151}, {"referenceID": 42, "context": "Some of the early works that mix adaptation with sparsity-aware reconstruction include methods that rely on the heuristic selection of active taps [43], and on sequential partial updating techniques [44]; some other methods assign proportional step-sizes to different taps according to their magnitudes, such as the proportionate normalized LMS (PNLMS) algorithm and its variations [45].", "startOffset": 199, "endOffset": 203}, {"referenceID": 43, "context": "Some of the early works that mix adaptation with sparsity-aware reconstruction include methods that rely on the heuristic selection of active taps [43], and on sequential partial updating techniques [44]; some other methods assign proportional step-sizes to different taps according to their magnitudes, such as the proportionate normalized LMS (PNLMS) algorithm and its variations [45].", "startOffset": 382, "endOffset": 386}, {"referenceID": 44, "context": "In subsequent studies, motivated by the LASSO technique [46] and by connections with compressive sensing [42], [47], several algorithms for sparse adaptive filtering have been proposed based on LMS [48], RLS [49], and projection-based methods [50].", "startOffset": 56, "endOffset": 60}, {"referenceID": 40, "context": "In subsequent studies, motivated by the LASSO technique [46] and by connections with compressive sensing [42], [47], several algorithms for sparse adaptive filtering have been proposed based on LMS [48], RLS [49], and projection-based methods [50].", "startOffset": 105, "endOffset": 109}, {"referenceID": 45, "context": "In subsequent studies, motivated by the LASSO technique [46] and by connections with compressive sensing [42], [47], several algorithms for sparse adaptive filtering have been proposed based on LMS [48], RLS [49], and projection-based methods [50].", "startOffset": 111, "endOffset": 115}, {"referenceID": 46, "context": "In subsequent studies, motivated by the LASSO technique [46] and by connections with compressive sensing [42], [47], several algorithms for sparse adaptive filtering have been proposed based on LMS [48], RLS [49], and projection-based methods [50].", "startOffset": 198, "endOffset": 202}, {"referenceID": 47, "context": "In subsequent studies, motivated by the LASSO technique [46] and by connections with compressive sensing [42], [47], several algorithms for sparse adaptive filtering have been proposed based on LMS [48], RLS [49], and projection-based methods [50].", "startOffset": 208, "endOffset": 212}, {"referenceID": 48, "context": "In subsequent studies, motivated by the LASSO technique [46] and by connections with compressive sensing [42], [47], several algorithms for sparse adaptive filtering have been proposed based on LMS [48], RLS [49], and projection-based methods [50].", "startOffset": 243, "endOffset": 247}, {"referenceID": 49, "context": "Finally, sparsity aware distributed methods were proposed in [51]\u2013[55].", "startOffset": 61, "endOffset": 65}, {"referenceID": 53, "context": "Finally, sparsity aware distributed methods were proposed in [51]\u2013[55].", "startOffset": 66, "endOffset": 70}, {"referenceID": 37, "context": "It is well known from adaptive filters theory [38] that the LMS algorithm in (12) is a stochastic approximation method for the solution of problem (10), which enables convergence in the mean-sense to the true vector x0 (if the step-size \u03bc is chosen sufficiently small), while guaranteing a bounded meansquare error (as we will see in the sequel).", "startOffset": 46, "endOffset": 50}, {"referenceID": 54, "context": "Thus, using energy conservation arguments [56], we consider a general weighted squared error sequence \u015d[n]\u03a6\u015d[n], where \u03a6 \u2208 C is any Hermitian nonnegative-definite matrix that we are free to choose.", "startOffset": 42, "endOffset": 46}, {"referenceID": 9, "context": "This greedy strategy was already introduced in [10] in the case of batch signal reconstruction.", "startOffset": 47, "endOffset": 51}, {"referenceID": 55, "context": "sampling operator D[n] at time n, we update the estimate of the GFT vector s using an online version of the celebrated ISTA algorithm [58], [59], which proceeds as:", "startOffset": 134, "endOffset": 138}, {"referenceID": 56, "context": "sampling operator D[n] at time n, we update the estimate of the GFT vector s using an online version of the celebrated ISTA algorithm [58], [59], which proceeds as:", "startOffset": 140, "endOffset": 144}, {"referenceID": 44, "context": "A commonly used thresholding function comes directly by imposing an l1 norm constraint in (30), which is commonly known as the Lasso [46].", "startOffset": 133, "endOffset": 137}, {"referenceID": 57, "context": "A potential improvement can be made by considering the non-negative Garotte estimator as in [60], whose thresholding function is defined as a vector whose entries are derived applying the threshold", "startOffset": 92, "endOffset": 96}], "year": 2017, "abstractText": "In many applications spanning from sensor to social networks, transportation systems, gene regulatory networks or big data, the signals of interest are defined over the vertices of a graph. The aim of this paper is to propose a least mean square (LMS) strategy for adaptive estimation of signals defined over graphs. Assuming the graph signal to be band-limited, over a known bandwidth, the method enables reconstruction, with guaranteed performance in terms of mean-square error, and tracking from a limited number of observations over a subset of vertices. A detailed mean square analysis provides the performance of the proposed method, and leads to several insights for designing useful sampling strategies for graph signals. Numerical results validate our theoretical findings, and illustrate the performance of the proposed method. Furthermore, to cope with the case where the bandwidth is not known beforehand, we propose a method that performs a sparse online estimation of the signal support in the (graph) frequency domain, which enables online adaptation of the graph sampling strategy. Finally, we apply the proposed method to build the power spatial density cartography of a given operational region in a cognitive network environment.", "creator": "LaTeX with hyperref package"}}}