{"id": "1602.06727", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "22-Feb-2016", "title": "Improving Trajectory Modelling for DNN-based Speech Synthesis by using Stacked Bottleneck Features and Minimum Generation Error Training", "abstract": "We propose two novel techniques --- stacking bottleneck features and minimum trajectory error training criterion --- to improve the performance of deep neural network (DNN)-based speech synthesis. The techniques address the related issues of frame-by-frame independence and ignorance of the relationship between static and dynamic features, within current typical DNN-based synthesis frameworks. Stacking bottleneck features, which are an acoustically--informed linguistic representation, provides an efficient way to include more detailed linguistic context at the input or output level, and also allows for a faster read times and larger read times. These include a number of structural features such as the use of parallelization algorithms, which are commonly used to explain the difference between DNN-based and DNN-based speech synthesis.\n\n\n\nThe main issue with the proposed approach is that the current DNN-based DNN algorithm is designed to perform more complex speech synthesis. The first proposed approach is that the underlying DNN algorithm is to store and store the output data for a number of discrete stages, and to use a local system as the primary input. The second approach is to use a local network (i.e., a local system that processes speech by itself). The third approach is to store all the output and input data.\nIn general, DNN-based speech synthesis is a great application of these approaches to learning and processing. DNN-based speech synthesis is also used in practice to support computational model inference in general.\nThe current proposal aims to improve the performance of Deep Neural Network (DNN) by implementing the DNN-based DNN-based approach, as it facilitates more efficient and flexible parsing of input data using a local network in order to perform better processing. In particular, it is intended to enable the performance of deep learning, such as image recognition, prediction of complex speech patterns, and learning patterns that require a small and highly structured pipeline of data.\nCurrently, these approaches require a number of layers of layers, which are typically represented by multiple layers (such as a network of layers), and a range of different layers of processing. The proposed method is to use a finite number of layers to implement the DNN-based DNN-based algorithm, to store and store the output data for the discrete stages. It is currently in the development stage and may be feasible to use the DNN-based algorithm for language translation.", "histories": [["v1", "Mon, 22 Feb 2016 11:11:04 GMT  (192kb,D)", "https://arxiv.org/abs/1602.06727v1", "submitted to IEEE/ACM Transactions on Audio, Speech and Language Processing 2016 (Under second round review)"], ["v2", "Mon, 4 Apr 2016 11:18:07 GMT  (199kb,D)", "http://arxiv.org/abs/1602.06727v2", "submitted to IEEE/ACM Transactions on Audio, Speech and Language Processing 2016 (AQ)"], ["v3", "Tue, 5 Apr 2016 11:31:02 GMT  (199kb,D)", "http://arxiv.org/abs/1602.06727v3", "submitted to IEEE/ACM Transactions on Audio, Speech and Language Processing 2016 (AQ)"]], "COMMENTS": "submitted to IEEE/ACM Transactions on Audio, Speech and Language Processing 2016 (Under second round review)", "reviews": [], "SUBJECTS": "cs.SD cs.CL cs.NE", "authors": ["zhizheng wu", "simon king"], "accepted": false, "id": "1602.06727"}, "pdf": {"name": "1602.06727.pdf", "metadata": {"source": "CRF", "title": "Improving Trajectory Modelling for DNN-based Speech Synthesis by using Stacked Bottleneck Features and Minimum Generation Error Training", "authors": ["Zhizheng Wu"], "emails": ["mon.king}@ed.ac.uk"], "sections": [{"heading": null, "text": "Index Terms\u2014Speech synthesis, acoustic modelling, deep neural network, bottleneck, minimum generation error\nI. INTRODUCTION\nStatistical parametric speech synthesis (SPSS) [1] has advanced particularly rapidly in the last decade, as seen across the annual Blizzard Challenges [2], and can produce highlyintelligible synthesised speech with acceptable naturalness. However, although it offers greater flexibility than the other mainstream technique of unit selection [3], the naturalness of speech generated by SPSS is still too low.\nThere are many factors that underlie this, and acoustic modelling is a key one, as discussed in [1]. The task of modelling the complex relationship between linguistic representations derived from text input and acoustic features computed from speech waveforms is of course very difficult. In this work, we propose two novel techniques to improve this acoustic modelling. Both techniques target improved modelling of the temporal natural of speech, but in different ways: one via the input linguistic features, the other via the output speech parameters. Separately, each of them results in improvements to the subjective naturalness of the synthesised speech, and their combination gives a further improvement.\nThis work was supported by EPSRC under Programme Grant EP/I031022/1 (Natural Speech Technology).\nZ. Wu and S. King are with the Centre for Speech Technology Research (CSTR), University of Edinburgh, UK. Email: {zhizheng.wu, simon.king}@ed.ac.uk"}, {"heading": "A. Related work", "text": "Very substantial effort has been devoted to acoustic modelling in the hidden Markov model (HMM) speech synthesis framework. Amongst the many proposed techniques, we highlight just a few of the most influential. In [4], a minimum generation error training criterion was proposed to address an inconsistency between training and generation criteria, and the lack of interaction between static and dynamic features during training. In [5], the so-called trajectory HMM was proposed to explicitly model the relationships between static and dynamic features. As a complement to improving the acoustic model itself, enhancement techniques such as global variance [6] and modulation spectrum enhancement [7] aim to mitigate the lack of variation in generated parameter trajectories that results from using an incorrect acoustic model. Although such enhancement techniques do not reduce objective error (e.g., lower spectral distortion w.r.t. a natural speech reference), significant improvements in subjective naturalness are obtained. However, none of the above techniques address what is perhaps the most fundamental problem of HMM-based speech synthesis: across-context averaging via decision tree clustering, which has been identified as a major contributing factor to reduced naturalness [8].\nMore recently, following on from successes in automatic speech recognition [9], artificial neural networks have reemerged as acoustic models for SPSS [10]. By the 1990s, artificial neural networks had already been employed as feature extractors from text input to produce linguistic features [11], as acoustic models to map linguistic features to vocoder parameters [12], [13], [14], and to predict segment durations [15]. One prominent theme in more recent studies is the use of neural architectures to replace Gaussian mixture models (GMMs) associated with leaf nodes of decision trees, such as the restricted Boltzmann machines (RBMs) in [16], where RBMs were claimed to better learn spectral detail, resulting in better quality synthesised speech. In [17], [18], a deep belief network (DBN) was employed as a deep generative model of the joint probability distribution between linguistic and acoustic features. Other variants on the neural architectures applied to SPSS include the use of deep mixture density networks to predict probability density functions over acoustic features given the corresponding linguistic features [19] and a trajectory real-valued neural autoregressive density estimator to model acoustic parameter trajectories as well as acrossfeature dependencies [20]. Neural approaches have been apar X iv :1\n60 2.\n06 72\n7v 3\n[ cs\n.S D\n] 5\nA pr\n2 01\n6\nplied to enhancement too, such as the deep generative model in [21] acting as a post-filter to enhance the quality of speech synthesised from an HMM-based system.\nThe most popular way to use neural networks in SPSS is with a deep feed-forward neural network (DNN) as a conditional model to map linguistic features to vocoder parameters directly [22], [23], [24], [25], [26]. This can be viewed as replacing the decision tree used in HMM-based speech synthesis with a more powerful regression model [22], [27]. DNNs have other advantages, including the ability to model highdimensional acoustic parameters (e.g., the spectrum [28]), and the availability of techniques such as multi-task learning [25], [29]. However, a limitation of standard DNN implementations is that the mapping is performed frame by frame without considering contextual constraints, other than those encoded in the input linguistic features. Even though dynamic features are part of the output acoustic feature vector \u2013 because they are needed as a constraint to generate smooth parameter trajectories at synthesis time \u2013 contextual constraints between statics and deltas (or equivalently between successive frames of statics) are not explicitly modelled during training.\nOne way to model contextual constraints is proposed in [30]: a bidirectional long short-term memory (LSTM)-based recurrent neural network (RNN) to map a sequence of linguistic features to the corresponding sequence of acoustic features. An LSTM with a recurrent output layer is proposed in [31] to smooth acoustic features across consecutive frames. A systematic investigation on the architectures of gated recurrent neural network can be found in [32]. These studies formulate speech synthesis as a sequence-to-sequence mapping problem and provide evidence that a better model of speech parameter trajectories results in better synthetic speech."}, {"heading": "B. Contributions of this work", "text": "We propose some alternative way to include contextual temporal constraints during both training and generation. The proposed framework is easy to train and has an additional benefit that parts of the model can be trained on out-of-domain, lower-quality data (e.g., corpora used in speech recognition). We offer two contributions:\nFirst, we propose stacked bottleneck features as a way to include more detailed linguistic contextual constraints at the input1. We train a first network with a bottleneck hidden layer, which has a much smaller number of units than the other hidden layers. The input to this network is the usual set of linguistic features [33], and the output is some representation of the corresponding speech signal (e.g., the usual vocoder parameters). The activations of the units in the bottleneck layer are thus a lower-dimensional embedding of the input linguistic features that captures information useful for predicting the acoustic features (due to the supervised training) but discards irrelevant or erroneous information (i.e., denoises the features). Then, we stack bottleneck features from multiple\n1Preliminary results were published in [25]. Here, we significantly extend that work with a systematic analysis of the bottleneck features (e.g., the positioning of the bottleneck layer and the contextual width of bottleneck features) and an analysis of the use of out-of-domain data to train the bottleneck feature extractor.\nconsecutive frames around the central frame, and concatenate these stacked bottlenecks with the usual linguistic features. We use these combined features as input to a second neural network to predict vocoder parameters and thus to perform speech synthesis. Note that, because the bottleneck layer size is small (e.g., 32), stacking bottleneck features from multiple consecutive frames does not increase the dimensionality of the input features very much.\nSecond, we apply a sequential training criterion \u2013 minimum generation error (MGE) \u2014 for DNNs2, which is inspired by minimum generation error for HMM-based speech synthesis [4] and sequence error minimisation for voice conversion [36]. In a typical conventional implementation of a DNN for speech synthesis, dynamic features (extracted from the sequence of static features) are included as part of the output vector; but, the relationship between the static and dynamic features is neglected during training. The MGE criterion minimises the utterance-level vocoder parameter trajectory error rather than the sum of frame-wise mean squared errors. In this way, the MGE criterion explicitly accounts for the relationship between static and dynamic features and correctly uses the dynamic constraints in the training phase.\nBecause the two techniques proposed in this paper are applied at different places in the architecture, namely the linguistic input layer and acoustic output layer, it is possible and natural to combine them. We provide experimental results for this combination."}, {"heading": "II. PROBLEM STATEMENT", "text": "To place our proposed methods in context, we briefly review DNN-based speech synthesis and discuss the limitations of typical DNNs as used for speech synthesis, that our proposed methods address."}, {"heading": "A. DNN-based speech synthesis", "text": "DNN-based speech synthesis comprises offline training and runtime generation phases. During training, a DNN learns the complex relationship between input linguistic features xt and corresponding output acoustic features ot :\not = F(xt) + e, (1)\nwhere F(\u00b7) is the mapping function realised by the trained DNN, and e is the modelling error. Usually, the acoustic features ot consist of static features ct, also called vocoder parameters, and corresponding dynamic features \u2206ct and \u22062ct, written as\not = [c > t ,\u2206c > t ,\u2206 2c>t ] >. (2)\nThe dynamic features are used as a constraint to produce smooth parameter trajectories during generation. The dynamic features are computed from the sequence of static features. Hence, a sequence of observed acoustic features\n2Preliminary results were published in [34]. Here we add a comprehensive description of the theory, plus further implementation details and experimental analysis. A similar idea, called sequence generation error, has been independently proposed by Fan et. al. [35], and published at the same time as [34].\nO = [o>1 ,o > 2 , \u00b7 \u00b7 \u00b7 ,o>T ]> can be calculated from a sequence of static features C = [c>1 , c > 2 , \u00b7 \u00b7 \u00b7 , c>T ]> by\nO = WC, (3)\nwhere W is a matrix that contains the coefficients used to compute static, delta and delta-delta features from a sequence of static features C. Details can be found in [37].\nTo train a DNN, the usual objective is to minimise, in a frame-wise fashion, the error D(o\u0302t,ot) between predicted o\u0302t and observed acoustic features ot; this objective function can be written as\nD(o\u0302t,ot) = (o\u0302t \u2212 ot)>(o\u0302t \u2212 ot). (4)\nwhere D(o\u0302t,ot) is frame-wise error computed at the output. To minimise this error, the classic gradient descent algorithm back-propagation [38] is typically used. The gradients of the DNN parameters can be calculated by taking derivatives of D(o\u0302t,ot) with respect to model parameters \u03bb :\n\u2202D(o\u0302t,ot) \u2202\u03bb = \u2202D(o\u0302t,ot)\n\u2202ot\n\u2202ot \u2202\u03bb , (5)\nwhere\n\u2202D(o\u0302t,ot)\n\u2202ot = o\u0302t \u2212 ot (6)\nis the error to be back-propagated through the network from the top output layer to the bottom input layer, and the gradients of model parameters at each layer can be calculated through this back-propagation process. In practice, a mini-batch gradient descent method is usually applied, for faster convergence and more stable behaviour [39]; this is possible because the error at each frame can be calculated independently.\nAt generation time, given a sequence of linguistic features X, the corresponding acoustic features O\u0302 can be generated from the trained DNN by performing a forward propagation once per frame. To generate smooth parameter trajectories, the maximum likelihood parameter generation (MLPG) algorithm [37] is used, to take the dynamic feature constraints into account. Recall that the DNN predicts both static and dynamic features (although without ensuring consistency between them). The MLPG algorithm can be expressed as\nC\u0302 = (W>U\u22121W)\u22121W>U\u22121O\u0302, (7)\nwhere C\u0302 is the predicted static acoustic feature sequence (i.e., trajectory), which will be used to reconstruct the speech waveform, and U is the covariance matrix, which is computed from the training data in DNN-based framework. Using MLPG is important for good quality synthesised speech [40], [41]."}, {"heading": "B. Limitations", "text": "Although DNNs have been reported to achieve significant improvements over HMMs for speech synthesis, as we reviewed in Section I-A, there are at least two limitations in current DNN implementations:\n1) Frame-by-frame independence: Each frame\u2019s acoustic features are predicted from that frame\u2019s linguistic features without any contextual constraints other than those encoded in the linguistic features. Acoustic context, which is so important in speech, is not explicitly considered either during training or in the forward propagation step of generation.\n2) Neglecting the relationship between static and dynamic features: During the generation process, dynamic features are used by the MLPG algorithm to generate smooth parameter trajectories. But it relies on potentially inconsistent static and dynamic features predicted by the DNN. It should be beneficial to include the dynamic feature constraints during training.\nThe two techniques that we propose for addressing these limitations are now introduced in Sections III and IV."}, {"heading": "III. STACKED BOTTLENECK FEATURES", "text": "A straightforward approach might be to stack linguistic input features from several consecutive frames at the input. These linguistic features are usually extracted per phone and they include more slowly-changing word and phrase level information; then, information from forced-alignment is used to interpolate the phoneme-level linguistic features to obtain a frame-level input for the DNN. Therefore, stacking multiple frames of linguistic features would result in high-dimensional, sparse and highly-redundant features, which still may not be effective in capturing contextual constraints.\nSo, we propose instead to stack acoustically-informed bottleneck features, which are intended to capture all the relevant information from the input linguistic representation. Bottleneck features are the activations at a bottleneck layer in a DNN. This layer has a relatively small number of hidden units compared to the other hidden layers in the same network. Bottleneck features have been extensively employed in automatic speech recognition (ASR) as a compact representation of acoustic features [42], [43], [44]. For speech synthesis, bottleneck features can be viewed as a compressive transform of the linguistic features, extracted at the frame level. Because the network, in which the bottleneck layer is situated, is trained in a supervised fashion using acoustic features (which of course change every frame, reflecting the continuous nature of speech signals) the bottleneck features can also capture finegrained sub-phonetic temporal variations.\nFig. 1 illustrates the architecture of a DNN system that employs stacked bottleneck features. The left-hand network is a bottleneck network with four hidden layers of which the second layer is the bottleneck layer3. The left-hand network is used to extract bottleneck features, which are then stacked as input to the right-hand network. Since the dimensionality of the bottleneck features is small (e.g., 32), stacking such features from multiple frames does not increase input dimensionality much, nor does it increese the computational complexity of the synthesis network (the right-hand network in Fig. 1). The method proceeds as follows: (a) Train a network with a bottleneck layer. The input\ncomprises the linguistic features and the output is the corresponding acoustic features;\n3The numbers of layers in the example networks are only for illustration. We used a different setting in the experiments detailed in Section V.\n(b) Given a sequence of linguistic features, perform a forward propagation through the bottleneck network to generate bottleneck features, frame by frame; (c) Stack these bottleneck features from several consecutive frames around the current frame alongside the linguistic features; (d) With linguistic features and stacked bottlenecks as input, train a synthesis network to predict vocoder parameters; (e) To perform synthesis from a sequence of linguistic features, follow steps (b) and (c) to obtain input features, then make a forward pass through the synthesis network to generate vocoder parameters, and thence synthetic speech.\nIt is important to note that the bottleneck network is never used to generate synthetic speech. So, the output of the bottleneck network may be any kind of acoustic feature (e.g., MelFrequency Cepstral Coefficients (MFCCs)). Neither does the bottleneck network have to be trained on the same data as the synthesis network; for example, it is possible to use additional data from other speakers to train the bottleneck network. We will investigate the performance of various such system configurations in Section V."}, {"heading": "IV. MINIMUM GENERATION ERROR TRAINING CRITERION", "text": "In the previous section, we introduced the idea of stacking bottleneck features to explicitly include contextual constraints at the input to a DNN. We now turn to the output features, where the interaction between the static and dynamic features at the output is still neglected in conventional DNN training. The second contribution of this paper is a novel training criterion: minimum generation error (MGE). The MGE criterion minimises the utterance-level trajectory error rather than frame-by-frame error. This is because it explicitly accounts for the interaction between the static and dynamic features, whereas conventional DNNs for SPSS treat the dynamic features no differently to static features.\nThe basic idea of the MGE criterion is to minimise the error of the output after MLPG; i.e., the vocoder parameter trajectories which will actually be used to reconstruct the speech waveform, rather than to minimise the error of the input to MLPG, namely the static and dynamic acoustic features. We\ndefine the trajectory error as the Euclidean distance between the predicted C\u0302 and reference C static parameter trajectories, and the new objective function is defined as\nD(C\u0302,C) = (C\u0302\u2212C)>(C\u0302\u2212C) (8) = (RO\u0302\u2212C)>(RO\u0302\u2212C), (9)\nwhere R = (W>U\u22121W)\u22121W>U\u22121\nis the matrix that performs MLPG, given static and dynamic features, similar to that in Eq. (7). In practice, mean-variance normalisation is performed to C\u0302 and C for trajectory error calculation. The mean and variance are pre-calculated from the training data.\nCompared to Eq. (4), the new objective function D(C\u0302,C) is calculated from the smoothed trajectory (after MLPG) rather that the direct output of the DNN.\nAs with any conventionally-trained DNN, a gradient descent algorithm can be employed to train the network. With the new error function, the gradients of the DNN model parameters can be calculated as\n\u2202D(C\u0302,C)\n\u2202\u03bb =\n\u2202D(C\u0302,C)\n\u2202O\u0302\n\u2202O\u0302 \u2202\u03bb (10)\n= \u2202D(RO\u0302,C)\n\u2202O\u0302\n\u2202O\u0302 \u2202\u03bb , (11)\nwhere\n\u2202D(RO\u0302,C)\n\u2202O\u0302 = (C\u0302\u2212C)>R. (12)\nThe only difference between the new criterion and the conventional frame-wise mean squared error criterion is the way in which the output errors to be back-propagated through the network are calculated. The difference can be seen in Eq. (12) and Eq. (6). The method for computing the gradients at lower layers is unchanged, that is to compute the gradients through \u2202O\u0302 \u2202\u03bb . Similar to Eq. (5), only \u2202O\u0302 \u2202\u03bb is directly related to the model parameters. Performing back-propagation with the new criterion involves the following steps: (a) Initialise the weights for the MGE-DNN from a\nconventionally-trained DNN (i.e., using MMSE); (b) Given a sequence of input linguistic features, perform a\nforward propagation step just as in conventional training, to predict observation O\u0302; (c) Restore the mean and variance for O\u0302 (because meanvariance normalisation is performance for the acoustic features before training); (d) Perform MLPG to generate acoustic feature trajectories C\u0302 using Eq.(7); (e) Perform mean-variance normalisation to the predicted trajectories C\u0302 and the reference trajectories C\u0302; (f) Calculate the trajectory error using Eq. (12) with the mean-variance normalised trajectories; (g) Perform backward propagation just as in the conventional algorithm, except using the error calculated at step (f).\nIn practice, steps (d) to (f), corresponding to Eq. (12), are performed dimension by dimension. After the errors of all the\ndimensions are calculated, we perform step (g). The gradient update process is the same as in the conventional training algorithm.\nIn conventional training, it is usual to employ mini-batches, in which each mini-batch contains a fixed number of frames from a randomly shuffled version of the training data. With the new criterion, we need to keep trajectories intact, and therefore all frames from each utterance much be in the same minibatch, in the original order. We use individual utterances as the mini-batches, so the sizes of mini-batches vary. For synthesis, we proceed exactly as with a conventionally-trained DNN.\nIn our implementation, most of the computational cost arises from the calculation of (W>U\u22121W)\u22121. As U is diagonal, W>U\u22121W becomes a banded matrix, and the computational costs can be reduced considerably; we used the bandmat Python library4 to perform inversion of this banded matrix."}, {"heading": "V. EXPERIMENTS", "text": ""}, {"heading": "A. Experimental setups", "text": "We conducted experiments using a corpus recorded from a British male professional speaker, divided into three subsets: 2400 utterances as training set, 70 utterances as development set, and 72 utterances as testing set. The waveform sampling rate of the corpus is 48 kHz. We used the STRAIGHT vocoder [45] to extract vocoder parameters \u2014 60-dimensional Mel-Cepstral Coefficients (MCCs), 25 band aperiodicities (BAPs), and log-scale fundamental frequency (logF0 at a 5ms frame step, and we employed the same vocoder to reconstruct speech waveforms during synthesis.\nAs reported in our previous work [25] and other previous studies [24], [41], DNN-based systems are generally significantly better than HMM-based ones, and therefore we only included DNN and LSTM baselines, and no HMM systems. We also implemented intermediate methods incorporating individual techniques of the proposed framework to examine the effectiveness of each of them, as well as their combination. The systems implemented and compared were: \u2022 DNN: a baseline system based a normal feed-forward\ndeep neural network trained using the conventional frame-by-frame minimum mean squared error (MMSE) criterion. The network has six hidden layers, each of which has 1024 hidden units. The hyperbolic Tangent activation functions are employed in the lower layers, and a linear activation function at the output layer. \u2022 LSTM: a second baseline system based on a long shortterm memory (LSTM) network with three feed-forward lower hidden layers each of 1024 units with tangent activation functions (intended to extract features, as suggested in [30]) plus one LSTM layer with 768 units on top of these feed-forward layers, and finally a linear regression output layer. \u2022 BN-DNN: similar to the DNN system, and also trained with frame-wise MMSE, but using stacked bottleneck features and linguistic features as in Fig. 1. The same vocoder parameters as for the BN-DNN system were used\n4https://pypi.python.org/pypi/bandmat/0.5\nas the acoustic output of the bottleneck network. The architecture of the synthesis network was the same as the DNN system. \u2022 BN-DNN-VB: same as BN-DNN system, except using a different database (the voice bank database [46]) to train the bottleneck network. \u2022 BN-DNN-MFC: same as BN-DNN-VB, except the output features of the bottleneck network were 21- dimensional Mel-Frequency Cepstral Coefficients (MFCCs) and their delta, delta-delta features, in total 63- D, extracted from waveforms that had been downsampled to 16 kHz. \u2022 BN-DNN-WSJ: same as BN-DNN-MFC, except using the Wall Street Journal (WSJ0+WSJ1) database [47] to train the bottleneck network. \u2022 MGE-DNN: same as the DNN system, but now employing the proposed minimum generation error (MGE) training criterion. The model parameters were initialised from the fully-trained DNN system above. \u2022 MGE-BN-DNN: same as the BN-DNN system, but now employing the MGE training criterion. The model parameters were initialised from the fully-trained BN-DNN system. Note that the MGE training is only applied to the synthesis network; the bottleneck network is simply taken from the BN-DNN system.\nAll the systems described above employed the same frontend to extract input linguistic features, which comprised 592 binary and 9 numerical features. The binary features were derived from the linguistic features such as quinphone identities, part-of-speech (POS), and positional information of phoneme, syllable, word and phrase. The input features were normalised to the range [0.01 0.99].\nFor the output vocoder parameters, F0 was linearly interpolated before modelling, and a binary feature was used to record the voiced/unvoiced information for each frame. Delta and delta-delta features were calculated for MCCs, BAPs and F0. In total, there were 259 features in the output. We applied mean-variance normalisation to the output acoustic features such that they had zero mean and unit variance across the training set. Similar normalisation was also employed to the MFCCs.\nThe hyper-parameters (i.e. the number of hidden layers, the number of hidden units, learning rate, momentum) of all neural networks were tuned on the development set. For all the systems except LSTM, our implementation employed the CUDAMat library5, which is a Python module for matrix calculations on GPU using CUDA, while for LSTM, we employed the Theano library6."}, {"heading": "B. Objective evaluation", "text": "We employed objective measures to tune the systems. Although these objective measures might not always be well correlated with human perception, they provide a practical and effective way to optimise the systems, especially for tuning hyper-parameters. We employed four measures:\n5https://github.com/cudamat/cudamat 6Theano version 0.7: http://deeplearning.net/software/theano/\n\u2022 MCD: Mel-Cepstral Distortion (MCD) to measure MCC prediction performance. \u2022 BAP: a distortion measure for BAPs. \u2022 F0 RMSE: Root Mean Squared Error (RMSE) to mea-\nsure F0 prediction performance. We note that F0 was modelled on a log-scale, but the error was calculated on a linear-scale. \u2022 V/UV: to measure voiced/unvoiced error. For all objective measures, a lower value indicates better performance.\n1) Effect of the position of the bottleneck layer: We started with experiments to examine the effects of the position of the bottleneck layer for the BN-DNN system. We fixed the context size (number of bottleneck frames that are stacked for input to the synthesis network) to 9, and varied the placement of the bottleneck layer from the bottom hidden layer to the top hidden layer. Examining all combinations of context size and bottleneck layer placement would involve too many experiments to be practical.\nWhen the bottleneck layer is close to the input layer, the bottleneck features are presumed to represent something more akin to the linguistic features than the acoustic features, and vice versa. The objective measure MCD, measured at the output of the synthesis network, is plotted as a function of bottleneck layer position in Fig. 2 and shows that lower (closer to the input) positioning of the bottleneck results in lower distortion. In particular, placing the bottleneck layer as the second or third hidden layer works best. BAP distortions also showed a similar pattern, whereas the other objective measures did not substantially vary with bottleneck layer position. We hence place the bottleneck at the second hidden layer in all remaining experiments. We consider the bottleneck layer activations to be a non-linear compression of the linguistic input features, with the compressive transform learned in a supervised way to minimise an acoustic distortion.\n2) Effect of the contextual size of stacked bottlenecks: We next conducted experiments to examine the effect of the contextual size of the stacked bottleneck features being presented at the input of the synthesis network, again for the BN-DNN system. With the bottleneck always at the second hidden layer of the first network, we varied the contextual size from 1 to 25 and measured MCD at the output of the synthesis network, which is plotted in Fig. 3. In this figure, we also show\nthe effect of stacking up multiple frames of linguistic features at the input to the synthesis network (there are no bottleneck features in this case). The MCD does initially fall, as expected, but not as quickly as when stacking bottleneck features, and plateauing out after a contextual size of around 7 frames. Of course, the dimensionality of the stacked linguistic features becomes very high. For example, for a contextual size of 11: 601\u00d7 11 = 6611.\nFor a bottleneck layer size of 128, the MCD keeps falling until a contextual size of about 15. For a bottleneck layer size of 32, the MCD continues falling until a contextual size of 23; in this case the dimensionality of the stacked bottleneck features is 32 \u00d7 23 = 736 \u2013 much smaller than that of stacked linguistic features, and smaller than for 128- dimensional bottleneck features stacked to a contextual size of 15 (128\u00d7 15 = 1920).\nAn equivalent plot of RMSE F0 is presented in Fig. 4. The behaviour is largely similar to that of MCC, and again a low error is obtained using 32-dimensional bottleneck features stacked to a contextual size of 23.\nIn summary, a highly effective way to include richer linguistic context is by stacking bottleneck features. A relatively small bottleneck size of 32 provides good performance when stacking 23 contextual frames; this is the configuration used in all remaining experiments.\n3) Effect of training bottleneck network using out-ofdomain data: As previously noted, the bottleneck layer is providing an acoustically-supervised compression of the linguistic input features. It is interesting to know whether this acoustic\nsupervision has to come from exactly the same data as will later be used to train the synthesis network, and whether the data have to be parametrised in exactly the same way.\nWe therefore conducted experiments to assess the effects of using out-of-domain data (i.e., different to the main singlespeaker British English dataset used to train the synthesis network) to train the bottleneck network.\nWe considered three settings, enumerated in Section V-A: BN-DNN-VB, BN-DNN-MFC, BN-DNN-WSJ. The BNDNN-VB and BN-DNN-MFC systems both used the same out-of-domain speech dataset that has been recorded in a highquality studio for speech synthesis purposes, albeit from nonprofessional speakers (voice bank), but parametrised differently. BN-DNN-VB used vocoder features extracted from 48 kHz waveforms whilst BN-DNN-MFC used MFCC extracted from 16 kHz waveforms. BN-DNN-WSJ used a database designed for speech recognition and containing American English accented speech. In the voice bank data, there are 96 speakers (41 male, 55 female), each saying about 300 utterances. In total there are 36800 utterances for training the bottleneck network. In the WSJ corpus, there are 283 speakers and a total of about 37000 utterances for training the bottleneck network.\nObjective results for the three systems are presented in Table I. Compared to BN-DNN (which does not use out-ofdomain data), the three systems all reduce all four objective measures, except that BN-DNN-VB slightly increases V/UV error. Comparing BN-DNN-VB with BN-DNN-MFC, we see that using data at a lower-sampling rate and simpler acoustic features (MFCCs instead of vocoder parameters) has no effect on the objective measures. Even when using a speech recognition database, containing speakers of a difference accent, to train the bottleneck network (BN-DNN-WSJ), we still get lower distortions than for BN-DNN.\nIn summary, these objective results demonstrate that using a relatively large amount of out-of-domain speech data to train the bottleneck network improves the synthesis performance compared to only using the smaller single-speaker synthesis data. The sampling rate (and therefore bandwidth) and parametrisation of the speech data has little effect.\n4) Effectiveness of minimum generation error training criterion: The second contribution of this paper is the novel minimum generation error (MGE) training criterion of Section IV, operating on the output features of the synthesis network. We performed initial experiments to examine the\nconvergence property of training under the MGE criterion, using the objective measure of mean squared error between predicted and reference vocoder parameter trajectories. The error is calculated after MLPG, because the vocoder parameter trajectories after MLPG are those that will be used to reconstruct the speech waveform. The results are presented in Fig. 5. It is observed that the trajectory error is reduced consistently by the MGE criterion, and converges after about 15 epochs. The \u2018jump\u2019 in the trajectory errors at the 11th iteration is expected: we increased the value of momentum at that point. This phenomenon has been reported in [39]. Similar convergence properties were also observed in the MGE-BNDNN system, and is consistent with that reported in our previous work [34].\nWe next compared the objective error of the DNN and BNDNN architectures with and without MGE training. The objective results are presented in Table I. Compared to the DNN system without MGE training, MGE-DNN reduces MCD and F0 RMSE from 4.19 dB and 9.13 Hz to 4.12 dB and 8.93 Hz, respectively. In comparison with BN-DNN, both MCD and F0 RMSE measures for MGE-BN-DNN are reduced from 4.00 dB and 8.90 Hz to 3.97 dB and 8.89 Hz, respectively. The distortion reduction for BN-DNN is less than for DNN, and we think this is because BN-DNN already includes contextual constraints via the stacked bottleneck features at the input, and that these already improve the output trajectories.\nAfter that, we compared the performance of MGE-DNN and BN-DNN. In comparison with BN-DNN, MGE-DNN achieves higher distortions for all the measures. This indicates that stacking bottleneck features is more effective than the MGE criterion in improving the model accuracy. As we discussed above, MGE-BN-DNN can reduce the distortion further. This implies that even though MGE criterion alone is not as effective as stacking bottlenecks, it is complementary to stacking bottlenecks and the two techniques can be easily integrated to boost the performance. In summary, the objective results confirm the effectiveness of the proposed MGE criterion in improving DNN accuracy, and that MGE is complementary to the stacking of bottleneck features. Fig. 6 provides an example trajectory.\n5) Summary of objective evaluation: Objective results for all systems on the evaluation set are presented in Table I. The performance of all the systems was optimised on the\ndevelopment set. Between the two baselines, LSTM is consistently better than DNN under all objective measures, consistent with [30]. MGEDNN does not outperform LSTM objectively. The proposed MGE criterion only models the interaction between static and dynamic features, while the LSTM explicitly models temporal dependency in speech.\nAll systems employing stacked bottlenecks and/or MGE training objectively outperform the DNN baseline. All systems employing stacked bottleneck features achieve lower MCD and BAP distortion than LSTM, although LSTM achieves slightly lower F0 RMSE than BN-DNN and MGE-BN-DNN."}, {"heading": "C. Subjective evaluation", "text": "Whilst objective measures are useful in system development, they are not always reliable predictors of listeners\u2019 preferences, so we also conducted a series of subjective preference tests. 30 paid native English speakers participated in each test, in which they each listened 20 randomly selected pairs of utterances and decided which item in each pair sounded more natural (or chose a \u201cno preference\u201d option). The utterances within each pair came from differing systems but had the same linguistic content.\nThe preference scores for BN-DNN vs. DNN are presented in Fig. 7 and confirm that synthetic speech from the stacked bottleneck BN-DNN system is significantly more natural than DNN (60% vs. 27%).\nRecall from Section V-A that there are several different ways to train the bottleneck network. Fig. 8 presents the results for the pairwise comparisons we made between these variants.\nThe difference between BN-DNN-VB in Fig. 8(a) and BNDNN-MFC in Fig. 8(b) is in the acoustic feature representations and sampling rate (refer to Section V-A for details) and the results suggest that we can equally well use lower quality data and mismatched acoustic features to train the bottleneck network. Fig. 8(c) shows that even out-of-domain data (a North American English speech recognition database)\nis effective for training the bottleneck network. There is an overall tendency that using more data (which is always also from multiple speakers) to train the bottleneck network is helpful, although the improvements over BN-DNN are not statistically significant.\nThat the data used to train the bottleneck network does not need to be speaker- or accent-specific is encouraging. Whether it needs to be language-specific is a question left for future experimentation.\nThe performance of the proposed MGE criterion was examined first in comparison to the baseline: preference scores for MGE-DNN vs. DNN are presented in Fig. 10 and show that MGE-DNN is significantly better than DNN in terms of naturalness.\nNext, we compared the performance of the two techniques proposed in this paper. In BN-DNN, contextual constraints are included at the input by stacking bottleneck features, while in MGE-DNN, contextual constraints are considered at the output by explicitly modelling the relationship between static and dynamic feature via the MGE training criterion. The preference scores in Fig. 9 show that BN-DNN is very substantially (and statistically significantly) better than MGEDNN: stacking bottleneck features is much more effective than MGE training.\nOf course, the two techniques can be combined, and the preference scores for BN-DNN vs. MGE-BN-DNN are pre-\nsented in Fig. 11. These indicate that MGE may further improve naturalness on top of the improvements already obtained by stacking bottleneck features, although the difference is not significant. Overall, these preference tests demonstrate the effectiveness of the proposed MGE criterion, and reconfirm our findings reported earlier in [34].\nFinally, we compared the combined proposed techniques (MGE-BN-DNN) to the baseline LSTM system. Fig. 12 shows that in our setting MGE-BN-DNN is significantly better than LSTM in terms of naturalness (50% vs. 35%).\nOne interesting observation is that the preference scores just presented are in fact consistent with the objective results presented earlier (especially MCD), despite our own feeling (widely shared across the community) that objective measures do not reliably correlate with human judgements."}, {"heading": "VI. DISCUSSION", "text": "Our experimental results confirm the effectiveness of both proposed techniques as ways to include contextual constraints in neural network-based speech synthesis. Although LSTMbased recurrent neural networks may provide an elegant way to include temporal constraints, there are at least two reasons to choose our proposed framework of combining stacked bottleneck features with MGE training.\nFirst, the proposed framework can make use of out-ofdomain data in a very straightforward way: the bottleneck network can be trained using relatively low-quality speech, from multiple speakers. Since thousands of hours of speech data are available for training speech recognition systems, these data might also be used to train the bottleneck network.\nUsing such large amounts of data might further improve the results presented here, where we used tens of hours data to train the bottleneck network.\nSecond, although the total training time of the proposed framework is close to that of the LSTM system, at synthesis time the computational complexity of the proposed framework is considerably lower. In particular, to generate 142 utterances (all the utterances in the development and evaluation sets), the LSTM system took 215 seconds7, while the proposed MGEBN-DNN system took only 8 seconds. The computational cost of the LSTM system is about 27 times higher than the proposed framework, during synthesis."}, {"heading": "VII. CONCLUSION", "text": "We propose two techniques to improve the performance of DNN-based speech synthesis, namely stacked bottlenecks and a minimum generation error training criterion. The two techniques can be easily combined in a single DNN speech synthesis framework. This novel framework allows us a) to benefit from additional out-of-domain data to improve the synthesis performance; and b) to include contextual constraints without much increase in computational complexity at synthesis time. Both objective and subjective results confirm the effectiveness of the proposed system over both DNN and LSTM baselines. To summarise the main findings: \u2022 Stacking bottleneck features provides an effective way\nto include contextual constraints. As shown in the experiments, by setting the size of the bottleneck layer to 32, we can effectively include contextual constraints from 23 consecutive frames, which span a segment of 23 \u00d7 5 = 115 ms. Because the dimensionality is low, the computational cost of the synthesis network does not increase significantly. \u2022 Out-of-domain data (e.g., lower quality data collected for speech recognition) can be used to train the bottleneck network. This provides a flexible way to benefit from the readily-available large quantities of such data. \u2022 The minimum generation error training criterion is effective and can improve model accuracy, as shown in the experiments. As the criterion is only employed at the training stage, it does not introduce any additional computational complexity at synthesis time. \u2022 The two techniques, stacked bottleneck features and minimum generation error training criterion, are complementary. The techniques are applied at the input and output, respectively, and can be easily combined.\nCurrently, the bottleneck network and the synthesis network are trained independently. The performance might be boosted if the two networks were to be optimised jointly. Combining stacked bottleneck features with an LSTM-based synthesis network might also be effective. Our preliminary results show that the proposed bottleneck features can also be used to guide rich-context model selection [48], and waveform unit\n7Times reported are for neural network computations only and do not include lingusitic feature extraction or vocoding to reconstruct speech waveform. The hardware used here is Nvidia GTX TITAN with 2688 cores 6G RAM.\nselection [49]. We will leave those directions of research as follow-up work for the future."}], "references": [{"title": "Statistical parametric speech synthesis", "author": ["H. Zen", "K. Tokuda", "A.W. Black"], "venue": "Speech Communication, vol. 51, no. 11, pp. 1039\u20131064, 2009.", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2009}, {"title": "Measuring a decade of progress in text-to-speech", "author": ["S. King"], "venue": "Loquens, vol. 1, no. 1, 2014.", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2014}, {"title": "Unit selection in a concatenative speech synthesis system using a large speech database", "author": ["A.J. Hunt", "A.W. Black"], "venue": "Proc. IEEE Int. Conf. on Acoustics, Speech, and Signal Processing (ICASSP), vol. 1, 1996, pp. 373\u2013376.", "citeRegEx": "3", "shortCiteRegEx": null, "year": 1996}, {"title": "Minimum generation error training for HMM-based speech synthesis", "author": ["Y.-J. Wu", "R.-H. Wang"], "venue": "Proc. IEEE Int. Conf. on Acoustics, Speech, and Signal Processing (ICASSP), 2006, pp. 89\u201392.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2006}, {"title": "Reformulating the HMM as a trajectory model by imposing explicit relationships between static and dynamic feature vector sequences", "author": ["H. Zen", "K. Tokuda", "T. Kitamura"], "venue": "Computer Speech & Language, vol. 21, no. 1, pp. 153\u2013173, 2007.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2007}, {"title": "A speech parameter generation algorithm considering global variance for HMM-based speech synthesis", "author": ["T. Toda", "K. Tokuda"], "venue": "IEICE Transactions on Information and Systems, vol. 90, no. 5, pp. 816\u2013824, 2007.", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2007}, {"title": "A postfilter to modify the modulation spectrum in HMM-based speech synthesis", "author": ["S. Takamichi", "T. Toda", "G. Neubig", "S. Sakti", "S. Nakamura"], "venue": "Proc. IEEE Int. Conf. on Acoustics, Speech, and Signal Processing (ICASSP), 2014, pp. 290\u2013294.", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2014}, {"title": "Attributing modelling errors in HMM synthesis by stepping gradually from natural to modelled speech", "author": ["T. Merritt", "J. Latorre", "S. King"], "venue": "Proc. IEEE Int. Conf. on Acoustics, Speech, and Signal Processing (ICASSP), 2015, pp. 4220\u20134224.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2015}, {"title": "Deep neural networks for acoustic modeling in speech recognition: The shared views of four research groups", "author": ["G. Hinton", "L. Deng", "D. Yu", "G. Dahl", "A. Mohamed", "N. Jaitly", "A. Senior", "V. Vanhoucke", "P. Nguyen", "T. Sainath", "B. Kingsbury"], "venue": "IEEE Signal Processing Magazine, vol. 29, no. 6, pp. 82\u201397, 2012.", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2012}, {"title": "Deep learning for acoustic modeling in parametric speech generation: A systematic review of existing techniques and future trends", "author": ["Z.-H. Ling", "S.-Y. Kang", "H. Zen", "A. Senior", "M. Schuster", "X.-J. Qian", "H.M. Meng", "L. Deng"], "venue": "IEEE Signal Processing Magazine, vol. 32, no. 3, pp. 35\u201352, 2015.", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2015}, {"title": "A high quality text-to-speech system composed of multiple neural networks", "author": ["O. Karaali", "G. Corrigan", "N. Massey", "C. Miller", "O. Schnurr", "A. Mackie"], "venue": "Proc. IEEE Int. Conf. on Acoustics, Speech, and Signal Processing (ICASSP), vol. 2, 1998, pp. 1237\u20131240.", "citeRegEx": "11", "shortCiteRegEx": null, "year": 1998}, {"title": "Speech synthesis with artificial neural networks", "author": ["T. Weijters", "J. Thole"], "venue": "Proc. Int. Conf. on Neural Networks, 1993, pp. 1764\u20131769.", "citeRegEx": "12", "shortCiteRegEx": null, "year": 1993}, {"title": "LSP speech synthesis using backpropagation networks", "author": ["G. Cawley", "P. Noakes"], "venue": "Proc. Third Int. Conf. on Artificial Neural Networks, 1993, pp. 291\u2013294.", "citeRegEx": "13", "shortCiteRegEx": null, "year": 1993}, {"title": "Speech synthesis using artificial neural networks trained on cepstral coefficients.", "author": ["C. Tuerk", "T. Robinson"], "venue": "in Proc. European Conference on Speech Communication and Technology (Eurospeech),", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 1993}, {"title": "A neural-network-based model of segmental duration for speech synthesis", "author": ["M. Riedi"], "venue": "Proc. European Conference on Speech Communication and Technology (Eurospeech), 1995, pp. 599\u2013602.", "citeRegEx": "15", "shortCiteRegEx": null, "year": 1995}, {"title": "Modeling spectral envelopes using Restricted Boltzmann Machines and Deep Belief Networks for statistical parametric speech synthesis", "author": ["Z.-H. Ling", "L. Deng", "D. Yu"], "venue": "IEEE Transactions on Audio, Speech, and Language Processing, vol. 21, no. 10, pp. 2129\u20132139, 2013.", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2013}, {"title": "Multi-distribution deep belief network for speech synthesis", "author": ["S. Kang", "X. Qian", "H. Meng"], "venue": "Proc. IEEE Int. Conf. on Acoustics, Speech, and Signal Processing (ICASSP), 2013, pp. 8012\u20138016.", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2013}, {"title": "Statistical parametric speech synthesis using weighted multi-distribution deep belief network", "author": ["S. Kang", "H. Meng"], "venue": "Proc. Interspeech, 2014, pp. 1959\u20131963.", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2014}, {"title": "Deep mixture density networks for acoustic modeling in statistical parametric speech synthesis", "author": ["H. Zen", "A. Senior"], "venue": "Proc. IEEE Int. Conf. on Acoustics, Speech, and Signal Processing (ICASSP), 2014, pp. 3844\u20133848.", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2014}, {"title": "Modelling acoustic feature dependencies with artificial neural networks: Trajectory-rnade", "author": ["B. Uria", "I. Murray", "S. Renals", "C. Valentini"], "venue": "Proc. IEEE Int. Conf. on Acoustics, Speech, and Signal Processing (ICASSP), 2015, pp. 4465\u20134469.", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2015}, {"title": "A deep generative architecture for postfiltering in statistical parametric speech synthesis", "author": ["L.-H. Chen", "T. Raitio", "C. Valentini-Botinhao", "Z.-H. Ling", "J. Yamagishi"], "venue": "IEEE/ACM Trans. on Audio, Speech, and Language Processing, vol. 23, no. 11, pp. 2003\u20132014, 2015.", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2003}, {"title": "Statistical parametric speech synthesis using deep neural networks", "author": ["H. Zen", "A. Senior", "M. Schuster"], "venue": "Proc. IEEE Int. Conf. on Acoustics, Speech, and Signal Processing (ICASSP), 2013, pp. 7962\u2013 7966.", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2013}, {"title": "Combining a vector space representation of linguistic context with a deep neural network for text-to-speech synthesis", "author": ["H. Lu", "S. King", "O. Watts"], "venue": "Proc. the 8th ISCA Speech Synthesis Workshop (SSW), pp. 281\u2013285, 2013.", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2013}, {"title": "On the training aspects of deep neural network (DNN) for parametric TTS synthesis", "author": ["Y. Qian", "Y. Fan", "W. Hu", "F.K. Soong"], "venue": "Proc. IEEE Int. Conf. on Acoustics, Speech, and Signal Processing (ICASSP), 2014, pp. 3829\u20133833.", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2014}, {"title": "Deep neural networks employing multi-task learning and stacked bottleneck features for speech synthesis", "author": ["Z. Wu", "C. Valentini-Botinhao", "O. Watts", "S. King"], "venue": "Proc. IEEE Int. Conf. on Acoustics, Speech, and Signal Processing (ICASSP), 2015, pp. 4460\u20134464.", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2015}, {"title": "From hmms to dnns: where do the improvements come from?", "author": ["O. Watts", "G.E. Henter", "T. Merritt", "Z. Wu", "S. King"], "venue": "in Proc. IEEE Int. Conf. on Acoustics, Speech, and Signal Processing (ICASSP),", "citeRegEx": "26", "shortCiteRegEx": "26", "year": 2016}, {"title": "A reading list of recent advances in speech synthesis", "author": ["S. King"], "venue": "Proc. ICPhS, 2015.", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2015}, {"title": "Towards minimum perceptual error training for DNN-based speech synthesis", "author": ["C. Valentini-Botinhao", "Z. Wu", "S. King"], "venue": "Proc. Interspeech, 2015, pp. 869\u2013873.", "citeRegEx": "28", "shortCiteRegEx": null, "year": 2015}, {"title": "Fusion of multiple parameterisations for DNN-based sinusoidal speech synthesis with multi-task learning", "author": ["Q. Hu", "Z. Wu", "K. Richmond", "J. Yamagishi", "Y. Stylianou", "R. Maia"], "venue": "Proc. Interspeech, 2015, pp. 854\u2013 858.", "citeRegEx": "29", "shortCiteRegEx": null, "year": 2015}, {"title": "TTS synthesis with bidirectional LSTM based recurrent neural networks", "author": ["Y. Fan", "Y. Qian", "F. Xie", "F.K. Soong"], "venue": "Proc. Interspeech, 2014, pp. 1964\u20131968.", "citeRegEx": "30", "shortCiteRegEx": null, "year": 2014}, {"title": "Unidirectional long short-term memory recurrent neural network with recurrent output layer for low-latency speech synthesis", "author": ["H. Zen", "H. Sak"], "venue": "Proc. IEEE Int. Conf. on Acoustics, Speech, and Signal Processing (ICASSP), 2015, pp. 4470\u20134474.", "citeRegEx": "31", "shortCiteRegEx": null, "year": 2015}, {"title": "Investigating gated recurrent neural networks for speech synthesis", "author": ["Z. Wu", "S. King"], "venue": "Proc. IEEE Int. Conf. on Acoustics, Speech, and Signal Processing (ICASSP), 2016.", "citeRegEx": "32", "shortCiteRegEx": null, "year": 2016}, {"title": "An hmm-based speech synthesis system applied to english", "author": ["K. Tokuda", "H. Zen", "A.W. Black"], "venue": "Proc. the 2002 IEEE Workshop on Speech Synthesis, 2002, pp. 227\u2013230.", "citeRegEx": "33", "shortCiteRegEx": null, "year": 2002}, {"title": "Minimum trajectory error training for deep neural networks, combined with stacked bottleneck features", "author": ["Z. Wu", "S. King"], "venue": "Proc. Interspeech, 2015, pp. 309\u2013313.", "citeRegEx": "34", "shortCiteRegEx": null, "year": 2015}, {"title": "Sequence generation error (SGE) minimization based deep neural networks training for text-tospeech synthesis", "author": ["Y. Fan", "Y. Qian", "F.K. Soong", "L. He"], "venue": "Proc. Interspeech, 2015, pp. 864\u2013868.", "citeRegEx": "35", "shortCiteRegEx": null, "year": 2015}, {"title": "Sequence error (SE) minimization training of neural network for voice conversion", "author": ["F.-L. Xie", "Y. Qian", "Y. Fan", "F.K. Soong", "H. Li"], "venue": "Proc. Interspeech, 2014, pp. 2283\u20132287.", "citeRegEx": "36", "shortCiteRegEx": null, "year": 2014}, {"title": "Speech parameter generation algorithms for HMM-based speech synthesis", "author": ["K. Tokuda", "T. Yoshimura", "T. Masuko", "T. Kobayashi", "T. Kitamura"], "venue": "Proc. IEEE Int. Conf. on Acoustics, Speech, and Signal Processing (ICASSP), vol. 3, 2000, pp. 1315\u20131318.", "citeRegEx": "37", "shortCiteRegEx": null, "year": 2000}, {"title": "Learning representations by back-propagating errors", "author": ["D.E. Rumelhart", "G.E. Hinton", "R.J. Williams"], "venue": "Nature, vol. 323, no. 6088, pp. 533\u2013536, 1986.", "citeRegEx": "38", "shortCiteRegEx": null, "year": 1986}, {"title": "A practical guide to training restricted Boltzmann machines", "author": ["G. Hinton"], "venue": "University of Toronto, Tech. Rep., 2010.", "citeRegEx": "39", "shortCiteRegEx": null, "year": 2010}, {"title": "A perceptual study of acceleration parameters in HMM-based TTS", "author": ["Y. Chen", "Z.-J. Yan", "F.K. Soong"], "venue": "Proc. Interspeech, 2010, pp. 426\u2013 429.", "citeRegEx": "40", "shortCiteRegEx": null, "year": 2010}, {"title": "The effect of neural networks in statistical parametric speech synthesis", "author": ["K. Hashimoto", "K. Oura", "Y. Nankaku", "K. Tokuda"], "venue": "Proc. IEEE Int. Conf. on Acoustics, Speech, and Signal Processing (ICASSP), 2015, pp. 4455\u20134459.", "citeRegEx": "41", "shortCiteRegEx": null, "year": 2015}, {"title": "Optimizing bottle-neck features for LVCSR", "author": ["F. Gr\u00e9zl", "P. Fousek"], "venue": "Proc. IEEE Int. Conf. on Acoustics, Speech, and Signal Processing (ICASSP), 2008, pp. 4729\u20134732.", "citeRegEx": "42", "shortCiteRegEx": null, "year": 2008}, {"title": "Improved bottleneck features using pretrained deep neural networks", "author": ["D. Yu", "M.L. Seltzer"], "venue": "Proc. Interspeech, 2011, pp. 237\u2013240.  SUBMITTED TO IEEE/ACM TRANSACTIONS ON AUDIO, SPEECH, AND LANGUAGE PROCESSING 2016  11", "citeRegEx": "43", "shortCiteRegEx": null, "year": 2011}, {"title": "Auto-encoder bottleneck features using deep belief networks", "author": ["T.N. Sainath", "B. Kingsbury", "B. Ramabhadran"], "venue": "Proc. IEEE Int. Conf. on Acoustics, Speech, and Signal Processing (ICASSP), 2012, pp. 4153\u2013 4156.", "citeRegEx": "44", "shortCiteRegEx": null, "year": 2012}, {"title": "Restructuring speech representations using a pitch-adaptive time\u2013frequency smoothing and an instantaneous-frequency-based F0 extraction: Possible role of a repetitive structure in sounds", "author": ["H. Kawahara", "I. Masuda-Katsuse", "A. de Cheveign\u00e9"], "venue": "Speech communication, vol. 27, no. 3, pp. 187\u2013207, 1999.", "citeRegEx": "45", "shortCiteRegEx": null, "year": 1999}, {"title": "The voice bank corpus: Design, collection and data analysis of a large regional accent speech database", "author": ["C. Veaux", "J. Yamagishi", "S. King"], "venue": "Int. Conf. Oriental COCOSDA, 2013, pp. 1\u20134.", "citeRegEx": "46", "shortCiteRegEx": null, "year": 2013}, {"title": "The design for the Wall Street Journalbased CSR corpus", "author": ["D.B. Paul", "J.M. Baker"], "venue": "the Workshop on Speech and Natural Language, 1992, pp. 357\u2013362.", "citeRegEx": "47", "shortCiteRegEx": null, "year": 1992}, {"title": "Deep neural network context embeddings for model selection in rich-context HMM synthesis", "author": ["T. Merritt", "J. Yamagishi", "Z. Wu", "O. Watts", "S. King"], "venue": "Proc. Interspeech, 2015.", "citeRegEx": "48", "shortCiteRegEx": null, "year": 2015}, {"title": "Deep neural network-guided unit selection synthesis", "author": ["T. Merritt", "R.A. Clark", "Z. Wu", "J. Yamagishi", "S. King"], "venue": "Proc. IEEE Int. Conf. on Acoustics, Speech, and Signal Processing (ICASSP), 2016.", "citeRegEx": "49", "shortCiteRegEx": null, "year": 2016}], "referenceMentions": [{"referenceID": 0, "context": "Statistical parametric speech synthesis (SPSS) [1] has advanced particularly rapidly in the last decade, as seen across the annual Blizzard Challenges [2], and can produce highlyintelligible synthesised speech with acceptable naturalness.", "startOffset": 47, "endOffset": 50}, {"referenceID": 1, "context": "Statistical parametric speech synthesis (SPSS) [1] has advanced particularly rapidly in the last decade, as seen across the annual Blizzard Challenges [2], and can produce highlyintelligible synthesised speech with acceptable naturalness.", "startOffset": 151, "endOffset": 154}, {"referenceID": 2, "context": "However, although it offers greater flexibility than the other mainstream technique of unit selection [3], the naturalness of speech generated by SPSS is still too low.", "startOffset": 102, "endOffset": 105}, {"referenceID": 0, "context": "There are many factors that underlie this, and acoustic modelling is a key one, as discussed in [1].", "startOffset": 96, "endOffset": 99}, {"referenceID": 3, "context": "In [4], a minimum generation error training criterion was proposed to address an inconsistency between training and generation criteria, and the lack of interaction between static and dynamic features during training.", "startOffset": 3, "endOffset": 6}, {"referenceID": 4, "context": "In [5], the so-called trajectory HMM was proposed to explicitly model the relationships between static and dynamic features.", "startOffset": 3, "endOffset": 6}, {"referenceID": 5, "context": "As a complement to improving the acoustic model itself, enhancement techniques such as global variance [6] and modulation spectrum enhancement [7] aim to mitigate the lack of variation in generated parameter trajectories that results from using an incorrect acoustic model.", "startOffset": 103, "endOffset": 106}, {"referenceID": 6, "context": "As a complement to improving the acoustic model itself, enhancement techniques such as global variance [6] and modulation spectrum enhancement [7] aim to mitigate the lack of variation in generated parameter trajectories that results from using an incorrect acoustic model.", "startOffset": 143, "endOffset": 146}, {"referenceID": 7, "context": "However, none of the above techniques address what is perhaps the most fundamental problem of HMM-based speech synthesis: across-context averaging via decision tree clustering, which has been identified as a major contributing factor to reduced naturalness [8].", "startOffset": 257, "endOffset": 260}, {"referenceID": 8, "context": "More recently, following on from successes in automatic speech recognition [9], artificial neural networks have reemerged as acoustic models for SPSS [10].", "startOffset": 75, "endOffset": 78}, {"referenceID": 9, "context": "More recently, following on from successes in automatic speech recognition [9], artificial neural networks have reemerged as acoustic models for SPSS [10].", "startOffset": 150, "endOffset": 154}, {"referenceID": 10, "context": "By the 1990s, artificial neural networks had already been employed as feature extractors from text input to produce linguistic features [11], as acoustic models to map linguistic features to vocoder parameters [12], [13], [14], and to predict segment durations [15].", "startOffset": 136, "endOffset": 140}, {"referenceID": 11, "context": "By the 1990s, artificial neural networks had already been employed as feature extractors from text input to produce linguistic features [11], as acoustic models to map linguistic features to vocoder parameters [12], [13], [14], and to predict segment durations [15].", "startOffset": 210, "endOffset": 214}, {"referenceID": 12, "context": "By the 1990s, artificial neural networks had already been employed as feature extractors from text input to produce linguistic features [11], as acoustic models to map linguistic features to vocoder parameters [12], [13], [14], and to predict segment durations [15].", "startOffset": 216, "endOffset": 220}, {"referenceID": 13, "context": "By the 1990s, artificial neural networks had already been employed as feature extractors from text input to produce linguistic features [11], as acoustic models to map linguistic features to vocoder parameters [12], [13], [14], and to predict segment durations [15].", "startOffset": 222, "endOffset": 226}, {"referenceID": 14, "context": "By the 1990s, artificial neural networks had already been employed as feature extractors from text input to produce linguistic features [11], as acoustic models to map linguistic features to vocoder parameters [12], [13], [14], and to predict segment durations [15].", "startOffset": 261, "endOffset": 265}, {"referenceID": 15, "context": "One prominent theme in more recent studies is the use of neural architectures to replace Gaussian mixture models (GMMs) associated with leaf nodes of decision trees, such as the restricted Boltzmann machines (RBMs) in [16], where", "startOffset": 218, "endOffset": 222}, {"referenceID": 16, "context": "In [17], [18], a deep belief network (DBN) was employed as a deep generative model of the joint probability distribution between linguistic and acoustic features.", "startOffset": 3, "endOffset": 7}, {"referenceID": 17, "context": "In [17], [18], a deep belief network (DBN) was employed as a deep generative model of the joint probability distribution between linguistic and acoustic features.", "startOffset": 9, "endOffset": 13}, {"referenceID": 18, "context": "applied to SPSS include the use of deep mixture density networks to predict probability density functions over acoustic features given the corresponding linguistic features [19] and a trajectory real-valued neural autoregressive density estimator to model acoustic parameter trajectories as well as acrossfeature dependencies [20].", "startOffset": 173, "endOffset": 177}, {"referenceID": 19, "context": "applied to SPSS include the use of deep mixture density networks to predict probability density functions over acoustic features given the corresponding linguistic features [19] and a trajectory real-valued neural autoregressive density estimator to model acoustic parameter trajectories as well as acrossfeature dependencies [20].", "startOffset": 326, "endOffset": 330}, {"referenceID": 20, "context": "plied to enhancement too, such as the deep generative model in [21] acting as a post-filter to enhance the quality of speech synthesised from an HMM-based system.", "startOffset": 63, "endOffset": 67}, {"referenceID": 21, "context": "The most popular way to use neural networks in SPSS is with a deep feed-forward neural network (DNN) as a conditional model to map linguistic features to vocoder parameters directly [22], [23], [24], [25], [26].", "startOffset": 182, "endOffset": 186}, {"referenceID": 22, "context": "The most popular way to use neural networks in SPSS is with a deep feed-forward neural network (DNN) as a conditional model to map linguistic features to vocoder parameters directly [22], [23], [24], [25], [26].", "startOffset": 188, "endOffset": 192}, {"referenceID": 23, "context": "The most popular way to use neural networks in SPSS is with a deep feed-forward neural network (DNN) as a conditional model to map linguistic features to vocoder parameters directly [22], [23], [24], [25], [26].", "startOffset": 194, "endOffset": 198}, {"referenceID": 24, "context": "The most popular way to use neural networks in SPSS is with a deep feed-forward neural network (DNN) as a conditional model to map linguistic features to vocoder parameters directly [22], [23], [24], [25], [26].", "startOffset": 200, "endOffset": 204}, {"referenceID": 25, "context": "The most popular way to use neural networks in SPSS is with a deep feed-forward neural network (DNN) as a conditional model to map linguistic features to vocoder parameters directly [22], [23], [24], [25], [26].", "startOffset": 206, "endOffset": 210}, {"referenceID": 21, "context": "This can be viewed as replacing the decision tree used in HMM-based speech synthesis with a more powerful regression model [22], [27].", "startOffset": 123, "endOffset": 127}, {"referenceID": 26, "context": "This can be viewed as replacing the decision tree used in HMM-based speech synthesis with a more powerful regression model [22], [27].", "startOffset": 129, "endOffset": 133}, {"referenceID": 27, "context": ", the spectrum [28]), and the availability of techniques such as multi-task learning [25], [29].", "startOffset": 15, "endOffset": 19}, {"referenceID": 24, "context": ", the spectrum [28]), and the availability of techniques such as multi-task learning [25], [29].", "startOffset": 85, "endOffset": 89}, {"referenceID": 28, "context": ", the spectrum [28]), and the availability of techniques such as multi-task learning [25], [29].", "startOffset": 91, "endOffset": 95}, {"referenceID": 29, "context": "One way to model contextual constraints is proposed in [30]: a bidirectional long short-term memory (LSTM)-based recurrent neural network (RNN) to map a sequence of linguistic features to the corresponding sequence of acoustic features.", "startOffset": 55, "endOffset": 59}, {"referenceID": 30, "context": "An LSTM with a recurrent output layer is proposed in [31] to smooth acoustic features across consecutive frames.", "startOffset": 53, "endOffset": 57}, {"referenceID": 31, "context": "A systematic investigation on the architectures of gated recurrent neural network can be found in [32].", "startOffset": 98, "endOffset": 102}, {"referenceID": 32, "context": "The input to this network is the usual set of linguistic features [33], and the output is some representation of the corresponding speech signal (e.", "startOffset": 66, "endOffset": 70}, {"referenceID": 24, "context": "1Preliminary results were published in [25].", "startOffset": 39, "endOffset": 43}, {"referenceID": 3, "context": "Second, we apply a sequential training criterion \u2013 minimum generation error (MGE) \u2014 for DNNs2, which is inspired by minimum generation error for HMM-based speech synthesis [4] and sequence error minimisation for voice conversion [36].", "startOffset": 172, "endOffset": 175}, {"referenceID": 35, "context": "Second, we apply a sequential training criterion \u2013 minimum generation error (MGE) \u2014 for DNNs2, which is inspired by minimum generation error for HMM-based speech synthesis [4] and sequence error minimisation for voice conversion [36].", "startOffset": 229, "endOffset": 233}, {"referenceID": 33, "context": "2Preliminary results were published in [34].", "startOffset": 39, "endOffset": 43}, {"referenceID": 34, "context": "[35], and published at the same time as [34].", "startOffset": 0, "endOffset": 4}, {"referenceID": 33, "context": "[35], and published at the same time as [34].", "startOffset": 40, "endOffset": 44}, {"referenceID": 36, "context": "Details can be found in [37].", "startOffset": 24, "endOffset": 28}, {"referenceID": 37, "context": "To minimise this error, the classic gradient descent algorithm back-propagation [38] is typically used.", "startOffset": 80, "endOffset": 84}, {"referenceID": 38, "context": "In practice, a mini-batch gradient descent method is usually applied, for faster convergence and more stable behaviour [39]; this is possible because the error at each frame can be calculated independently.", "startOffset": 119, "endOffset": 123}, {"referenceID": 36, "context": "To generate smooth parameter trajectories, the maximum likelihood parameter generation (MLPG) algorithm [37] is used, to take the dynamic feature constraints into account.", "startOffset": 104, "endOffset": 108}, {"referenceID": 39, "context": "Using MLPG is important for good quality synthesised speech [40], [41].", "startOffset": 60, "endOffset": 64}, {"referenceID": 40, "context": "Using MLPG is important for good quality synthesised speech [40], [41].", "startOffset": 66, "endOffset": 70}, {"referenceID": 41, "context": "Bottleneck features have been extensively employed in automatic speech recognition (ASR) as a compact representation of acoustic features [42], [43], [44].", "startOffset": 138, "endOffset": 142}, {"referenceID": 42, "context": "Bottleneck features have been extensively employed in automatic speech recognition (ASR) as a compact representation of acoustic features [42], [43], [44].", "startOffset": 144, "endOffset": 148}, {"referenceID": 43, "context": "Bottleneck features have been extensively employed in automatic speech recognition (ASR) as a compact representation of acoustic features [42], [43], [44].", "startOffset": 150, "endOffset": 154}, {"referenceID": 44, "context": "We used the STRAIGHT vocoder [45] to extract vocoder parameters \u2014 60-dimensional Mel-Cepstral Coefficients (MCCs), 25 band aperiodicities (BAPs), and log-scale fundamental frequency (logF0 at a 5ms frame step, and we employed the same vocoder to reconstruct speech waveforms during synthesis.", "startOffset": 29, "endOffset": 33}, {"referenceID": 24, "context": "As reported in our previous work [25] and other previous studies [24], [41], DNN-based systems are generally significantly better than HMM-based ones, and therefore we only included DNN and LSTM baselines, and no HMM systems.", "startOffset": 33, "endOffset": 37}, {"referenceID": 23, "context": "As reported in our previous work [25] and other previous studies [24], [41], DNN-based systems are generally significantly better than HMM-based ones, and therefore we only included DNN and LSTM baselines, and no HMM systems.", "startOffset": 65, "endOffset": 69}, {"referenceID": 40, "context": "As reported in our previous work [25] and other previous studies [24], [41], DNN-based systems are generally significantly better than HMM-based ones, and therefore we only included DNN and LSTM baselines, and no HMM systems.", "startOffset": 71, "endOffset": 75}, {"referenceID": 29, "context": "\u2022 LSTM: a second baseline system based on a long shortterm memory (LSTM) network with three feed-forward lower hidden layers each of 1024 units with tangent activation functions (intended to extract features, as suggested in [30]) plus one LSTM layer with 768 units on top of these feed-forward layers, and finally a linear regression output layer.", "startOffset": 225, "endOffset": 229}, {"referenceID": 45, "context": "\u2022 BN-DNN-VB: same as BN-DNN system, except using a different database (the voice bank database [46]) to train the bottleneck network.", "startOffset": 95, "endOffset": 99}, {"referenceID": 46, "context": "\u2022 BN-DNN-WSJ: same as BN-DNN-MFC, except using the Wall Street Journal (WSJ0+WSJ1) database [47] to train the bottleneck network.", "startOffset": 92, "endOffset": 96}, {"referenceID": 38, "context": "This phenomenon has been reported in [39].", "startOffset": 37, "endOffset": 41}, {"referenceID": 33, "context": "Similar convergence properties were also observed in the MGE-BNDNN system, and is consistent with that reported in our previous work [34].", "startOffset": 133, "endOffset": 137}, {"referenceID": 29, "context": "Between the two baselines, LSTM is consistently better than DNN under all objective measures, consistent with [30].", "startOffset": 110, "endOffset": 114}, {"referenceID": 33, "context": "Overall, these preference tests demonstrate the effectiveness of the proposed MGE criterion, and reconfirm our findings reported earlier in [34].", "startOffset": 140, "endOffset": 144}, {"referenceID": 47, "context": "Our preliminary results show that the proposed bottleneck features can also be used to guide rich-context model selection [48], and waveform unit", "startOffset": 122, "endOffset": 126}, {"referenceID": 48, "context": "selection [49].", "startOffset": 10, "endOffset": 14}], "year": 2016, "abstractText": "We propose two novel techniques \u2014 stacking bottleneck features and minimum generation error training criterion \u2014 to improve the performance of deep neural network (DNN)based speech synthesis. The techniques address the related issues of frame-by-frame independence and ignorance of the relationship between static and dynamic features, within current typical DNNbased synthesis frameworks. Stacking bottleneck features, which are an acoustically\u2013informed linguistic representation, provides an efficient way to include more detailed linguistic context at the input. The minimum generation error training criterion minimises overall output trajectory error across an utterance, rather than minimising the error per frame independently, and thus takes into account the interaction between static and dynamic features. The two techniques can be easily combined to further improve performance. We present both objective and subjective results that demonstrate the effectiveness of the proposed techniques. The subjective results show that combining the two techniques leads to significantly more natural synthetic speech than from conventional DNN or long short-term memory (LSTM) recurrent neural network (RNN) systems.", "creator": "LaTeX with hyperref package"}}}