{"id": "1602.06863", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "22-Feb-2016", "title": "Higher-Order Low-Rank Regression", "abstract": "This paper proposes an efficient algorithm (HOLRR) to handle regression tasks where the outputs have a tensor structure. We formulate the regression problem as the minimization of a least square criterion under a multilinear rank constraint, a difficult non convex problem. HOLRR computes efficiently an approximate solution of this problem, with solid theoretical guarantees. This is demonstrated by a model which consists of three dimensions of a linear matrix. The main criterion is the assumption of a linear matrix of four dimension boundaries of the matrix, including the first and the second. If two dimensions are equal in the same dimension then the distribution of the two dimensions is the same as if a linear matrix of three dimensions is equal to a linear matrix of four dimensions. The second criterion is the assumption of a linear matrix of four dimensions in a linear matrix of four dimensions in a linear matrix of four dimensions in a linear matrix of four dimensions in a linear matrix of four dimensions in a linear matrix of four dimensions in a linear matrix of four dimensions in a linear matrix of four dimensions in a linear matrix of four dimensions in a linear matrix of four dimensions in a linear matrix of four dimensions in a linear matrix of four dimensions in a linear matrix of four dimensions in a linear matrix of four dimensions in a linear matrix of four dimensions in a linear matrix of four dimensions in a linear matrix of four dimensions in a linear matrix of four dimensions in a linear matrix of four dimensions in a linear matrix of four dimensions in a linear matrix of four dimensions in a linear matrix of four dimensions in a linear matrix of four dimensions in a linear matrix of four dimensions in a linear matrix of four dimensions in a linear matrix of four dimensions in a linear matrix of four dimensions in a linear matrix of four dimensions in a linear matrix of four dimensions in a linear matrix of four dimensions in a linear matrix of four dimensions in a linear matrix of four dimensions in a linear matrix of four dimensions in a linear matrix of four dimensions in a linear matrix of four dimensions in a linear matrix of four dimensions in a linear matrix of four dimensions in a linear matrix of four dimensions in a linear matrix of four dimensions in a linear matrix of four dimensions in a linear matrix of four dimensions in a linear matrix of four dimensions in a linear matrix of four dimensions in a linear matrix of four dimensions in a linear matrix of four dimensions in a linear matrix of four dimensions in a linear matrix of four dimensions in a linear matrix of four dimensions in a linear matrix of four dimensions in a linear matrix of four dimensions in a linear matrix of four dimensions", "histories": [["v1", "Mon, 22 Feb 2016 17:21:11 GMT  (4395kb,D)", "http://arxiv.org/abs/1602.06863v1", "submitted to ICML 2016"]], "COMMENTS": "submitted to ICML 2016", "reviews": [], "SUBJECTS": "cs.LG", "authors": ["guillaume rabusseau", "hachem kadri"], "accepted": false, "id": "1602.06863"}, "pdf": {"name": "1602.06863.pdf", "metadata": {"source": "CRF", "title": "Higher-Order Low-Rank Regression", "authors": ["Guillaume Rabusseau", "Hachem Kadri"], "emails": ["guillaume.rabusseau@lif.univ-mrs.fr."], "sections": [{"heading": null, "text": "sion tasks where the outputs have a tensor structure. We formulate the regression problem as the minimization of a least square criterion under a multilinear rank constraint, a difficult non convex problem. HOLRR computes efficiently an approximate solution of this problem, with solid theoretical guarantees. A kernel extension is also presented. Experiments on synthetic and real data show that HOLRR outperforms multivariate and multilinear regression methods and is considerably faster than existing tensor methods."}, {"heading": "1 Introduction", "text": "Recently, there has been an increasing interest in adapting machine learning and statistical methods to tensors. Data with a natural tensor structure is encountered in many scientific areas including neuroimaging (Zhou et al., 2013), signal processing (Cichocki et al., 2009), spatio-temporal analysis (Bahadori et al., 2014) and computer vision (Lu et al., 2013). Extending multivariate regression methods to tensors is one of the challenging task in this area. Most existing works extend linear models to the multilinear setting and focus on the tensor structure of the input data (e.g. Signoretto et al. (2013)). Little has been done however to investigate learning methods for tensor-structured output data.\nWe consider a multilinear regression task where inputs are vectors and outputs are tensors. In order to leverage the tensor structure of the output data, we formulate the problem as the minimization of a least squares criterion subject to a multilinear rank constraint on the regression tensor. The rank constraint enforces the model to capture the low-rank structure of the outputs and to explain the dependencies between inputs and outputs in a low-dimensional multilinear subspace.\nUnlike previous work we do not use a convex relaxation of this difficult non-convex optimization problem. Instead we design an efficient approximation algorithm (HOLRR) for which we are able to provide good approximation\n\u2217Contact author: guillaume.rabusseau@lif.univ-mrs.fr.\nar X\niv :1\n60 2.\n06 86\n3v 1\n[ cs\n.L G\n] 2\n2 Fe\nguarantees. We also present a kernelized version of HOLRR which extends our model to the nonlinear setting. Experiments on synthetic and real data shows that HOLRR obtains better predictive accuracy while being computationally very competitive. We also present an image recovery experiment which gives an illustrative insight on the effects of multilinear rank regularization.\nRelated work. In the context of multi-task learning, Romera-Paredes et al. (2013) have proposed a linear model using a tensor-rank penalization of a least squares criterion to take into account the multi-modal interactions between tasks. Their approach relies on a convex relaxation of the multlinear rank constraint using the trace norms of the matricizations. They also propose a non-convex approach (MLMT-NC) but it is computationally very expensive. Bahadori et al. (2014) have proposed a greedy algorithm to solve a low-rank tensor learning problem in the context of multivariate spatio-temporal data analysis. The linear model they assume is different from the one we propose, it is specifically designed for spatio-temporal data and does not fit into the general tensor-valued regression framework we consider here.\nPaper outline. We provide some background on low-rank regression and tensors in Section 2. In Section 3 we introduce the tensor response regression problem, we develop the HOLRR algorithm to tackle the minimization problem with mulitlinear rank constraint, and we prove that this algorithm has good approximation guarantees. A kernelized version of HOLRR is provided in Section 4. Finally, we assess the performances of our method through simulation study and real data analysis in Section 5."}, {"heading": "2 Preliminaries", "text": "We begin by introducing some notations. For any integer k we use [k] to denote the set of integers from 1 to k. We use lower case bold letters for vectors (e.g. v \u2208 Rd1), upper case bold letters for matrices (e.g. M \u2208 Rd1\u00d7d2) and bold calligraphic letters for higher order tensors (e.g. T \u2208 Rd1\u00d7d2\u00d7d3). The identity matrix will be written as I. The ith row (resp. column) of a matrix M will be denoted by Mi,: (resp. M:,i). This notation is extended to slices of a tensor in the straightforward way. If v \u2208 Rd1 and v\u2032 \u2208 Rd2 , we use v \u2297 v\u2032 \u2208 Rd1\u00b7d2 to denote the Kronecker product between vectors, and its straightforward extension to matrices and tensors. Given a matrix M \u2208 Rd1\u00d7d2 , we use vec(M) \u2208 Rd1\u00b7d2 to denote the column vector obtained by concatenating the columns of M."}, {"heading": "2.1 Tensors and Tucker Decomposition", "text": "We first recall some basic definitions of tensor algebra; more details can be found in Kolda & Bader (2009). A tensor T \u2208 Rd1\u00d7\u00b7\u00b7\u00b7\u00d7dp is a collection of real numbers (T i1,\u00b7\u00b7\u00b7 ,ip : in \u2208 [dn], n \u2208 [p]). The mode-n fibers of T are the vectors obtained by fixing all indices except for the nth one, e.g. T :,i2,\u00b7\u00b7\u00b7 ,ip \u2208 Rd1 .\nThe nth mode matricization of T is the matrix having the mode-n fibers of T for columns and is denoted by T(n) \u2208 Rdn\u00d7d1\u00b7\u00b7\u00b7dn\u22121dn+1\u00b7\u00b7\u00b7dp . The vectorization of a tensor is defined by vec(T ) = vec(T(1)). The inner product between two tensors S and T (of the same size) is defined by \u3008S,T \u3009 = \u3008vec(S), vec(T )\u3009 and the Frobenius norm is defined by \u2016T \u20162F = \u3008T ,T \u3009. In the following T always denotes a tensor of size d1 \u00d7 \u00b7 \u00b7 \u00b7 \u00d7 dp.\nmode-n product. The mode-n matrix product of the tensor T and a matrix X \u2208 Rm\u00d7dn is a tensor denoted by T \u00d7n X. It is of size d1 \u00d7 \u00b7 \u00b7 \u00b7 \u00d7 dn\u22121 \u00d7m\u00d7 dn+1\u00d7\u00b7 \u00b7 \u00b7\u00d7dp and is defined by the relation Y = T \u00d7nX\u21d4 Y(n) = XT(n). The mode-n vector product of the tensor T and a vector v \u2208 Rdn is a tensor defined by T \u2022n v = T \u00d7n v> \u2208 Rd1\u00d7\u00b7\u00b7\u00b7\u00d7dn\u22121\u00d7dn+1\u00d7\u00b7\u00b7\u00b7\u00d7dp . Given tensors S and T and matrices X,A and B, it is easy to check that \u3008T \u00d7n X,S\u3009 = \u3008T ,S\u00d7n X>\u3009 and (T \u00d7n A)\u00d7n B = T \u00d7n BA (where we assumed conforming dimensions of the tensors and matrices).\nMultilinear rank. The mode-n rank of T is the dimension of the space spanned by its mode-n fibers, that is rankn(T ) = rank(T(n)). The multilinear rank of T , denoted by rank(T ), is the tuple of mode-n ranks of T : rank(T ) = (R1, \u00b7 \u00b7 \u00b7 , Rp) where Rn = rankn(T ) for n \u2208 [p]. We will write rank(T ) \u2264 (S1, \u00b7 \u00b7 \u00b7 , Sp) whenever rank1(T ) \u2264 S1, rank2(T ) \u2264 S2, \u00b7 \u00b7 \u00b7 , rankp(T ) \u2264 Sp.\nTucker decomposition. The Tucker decomposition is a form of higher-order principal component analysis. It decomposes a tensor T into a core tensor G transformed by an orthogonal matrix along each mode:\nT = G \u00d71 U1 \u00d72 U2 \u00d73 \u00b7 \u00b7 \u00b7 \u00d7p Up , (1)\nwhere G \u2208 RR1\u00d7R2\u00d7\u00b7\u00b7\u00b7\u00d7Rp , Ui \u2208 Rdi\u00d7Ri for i \u2208 [p] and U>i Ui = I for all i \u2208 [p]. The number of parameters involved in a Tucker decomposition can be considerably smaller than d1d2 \u00b7 \u00b7 \u00b7 dp. We have the following identities when matricizing and vectorizing a Tucker decomposition\nT(n) = UnG(n)(Up \u2297 \u00b7 \u00b7 \u00b7 \u2297Un+1 \u2297Un\u22121 \u2297 \u00b7 \u00b7 \u00b7 \u2297U1)>\nvec(T ) = (Up \u2297Up\u22121 \u2297 \u00b7 \u00b7 \u00b7 \u2297U1)vec(G) . (2)\nIt is well known that T admits the Tucker decomposition (1) if and only if rank(T ) \u2264 (R1, \u00b7 \u00b7 \u00b7 , Rp) (see e.g. Kolda & Bader (2009)). Finding an exact Tucker decomposition can be done using the higher-order SVD algorithm (HOSVD) introduced by De Lathauwer et al. (2000). Although finding the best approximation of multilinear rank (R1, \u00b7 \u00b7 \u00b7 , Rp) of a tensor T is a difficult problem, the truncated HOSVD algorithm provides good approximation guarantees and often performs well in practice."}, {"heading": "2.2 Low-Rank Regression", "text": "Multivariate regression is the task of recovering a function f : Rd \u2192 Rp from a set of input-output pairs {(x(n),y(n))}Nn=1, where the outputs are sampled from the model with an additive noise y = f(x) + \u03b5, where \u03b5 is the error term. To solve this problem, the ordinary least squares (OLS) approach assumes a linear dependence between input and output data and boils down to finding a matrix W \u2208 Rd\u00d7p that minimizes the squared error \u2016XW\u2212Y\u20162F , where X \u2208 RN\u00d7d and Y \u2208 RN\u00d7p denote the input and the output matrices. To prevent overfitting and to avoid numerical instabilities a ridge regularization term (i.e. \u03b3\u2016W\u20162F ) is often added to the objective function, leading to the regularized least squares (RLS) method. The main drawback of this method is that it does not take into account the dependencies between the components of the response. It performs poorly when the outputs are correlated and the true dimension of the response is less than p. Indeed, it is easy to see that the OLS/RLS approach in the multivariate setting is equivalent to performing p linear regressions for each scalar output {yj}pj=1 independently.\nLow-rank regression (or reduced-rank regression) addresses this issue by solving the following rank penalized problem\nmin W\u2208Rd\u00d7p \u2016XW\u2212Y\u20162F + \u03b3\u2016W\u20162F s.t. rank(W) \u2264 R , (3)\nfor a given integer R. Suppose that f(x) = Wx. The constraint rank(W) = R implies linear restrictions on W, i.e. there must exist p \u2212 R linearly independent vectors v1, \u00b7 \u00b7 \u00b7 ,vp\u2212R \u2208 Rp such that Wvi = 0, which, in turn, implies that y>vi = \u03b5>vi for each i \u2208 [p \u2212 R]. Intuitively this means that the linear subspace generated by the vi\u2019s only contains noise. Another way to see this is to write W = AB with A \u2208 Rd\u00d7R and B \u2208 RR\u00d7p, which implies that the relation between x and y is explained in an R-dimensional latent space.\nThe rank constraint in (3) was first proposed in (Anderson, 1951), whereas the term reduced-rank regression was introduced in (Izenman, 1975). Adding a ridge regularization to the rank penalized problem was proposed in Mukherjee & Zhu (2011). In the rest of the paper we will refer to this approach as lowrank regression (LRR). The solution of minimization problem (3) is given by projecting the RLS solution onto the space spanned by the top R eigenvectors of Y>PY where P is the orthogonal projection matrix onto the column space of X, that is WLRR = WRLS\u03a0 where \u03a0 is the matrix of the aforementioned projection. For more description and discussion of reduced-rank regression, we refer the reader to the books of Reinsel & Velu (1998) and Izenman (2008)."}, {"heading": "3 Low-Rank Regression for Tensor-Valued Functions", "text": ""}, {"heading": "3.1 Problem Formulation", "text": "We consider a multivariate regression task where the response has a tensor structure. Let f : Rd0 \u2192 Rd1\u00d7d2\u00d7\u00b7\u00b7\u00b7\u00d7dp be the function we want to learn from a sample of input-output data {(x(n),Y(n))}Nn=1 drawn from the model Y = f(x) + E, where E is an error term. We assume that the function f is linear, that is f(x) = W \u20221 x for some regression tensor W \u2208 Rd0\u00d7d1\u00d7\u00b7\u00b7\u00b7\u00d7dp . Note that the vectorization of this relation leads to vec(f(x)) = W>(1)x showing that this model is equivalent to the standard multivariate linear model.\nVectorization of the outputs. One way to tackle this linear regression task for tensor responses would be to vectorize each output sample and to perform a standard multivariate low-rank regression on the data {(x(n), vec(Y(n)))}Nn=1 \u2282 Rd0 \u00d7 Rd1\u00b7\u00b7\u00b7dp . A major drawback of this approach is that the tensor structure of the output is lost in the vectorization step. The low-rank model tries to capture linear dependencies between components of the output but it ignores higher level dependencies that could be present in a tensor-structured output. For illustration, suppose the output is a matrix encoding the samples of d1 continuous variables at d2 different time steps, one could expect structural relations between the d1 time series, for example linear dependencies between the rows of the output matrix.\nLow-rank regression for tensor responses. To overcome the limitation described above we propose an extension of the low-rank regression method for tensor-structured responses by enforcing low multilinear rank of the regression tensor W . Let {(x(n),Y(n))}Nn=1 \u2282 Rd0 \u00d7 Rd1\u00d7d2\u00d7\u00b7\u00b7\u00b7\u00d7dp be a training sample of input/output data drawn from the model f(x) = W \u20221 x + E where W is assumed of low multilinear rank. Considering the framework of empirical risk minimization, we want to find a low-rank regression tensor W minimizing the loss on the training data. To avoid numerical instabilities and to prevent overfitting we add a ridge regularization to the objective function, leading to the following minimization problem\nmin W\u2208Rd0\u00d7\u00b7\u00b7\u00b7\u00d7dp N\u2211 n=1 `(W \u20221 x(n),Y(n)) + \u03b3\u2016W\u20162F (4)\ns.t. rank(W) \u2264 (R0, R1, \u00b7 \u00b7 \u00b7 , Rp) ,\nfor some given integers R0, R1, \u00b7 \u00b7 \u00b7 , Rp and where ` is a loss function. In this paper, we consider the squared error loss between tensors defined by `(T , T\u0302 ) =\n\u2016T \u2212 T\u0302 \u20162F . Using this loss we can rewrite problem (4) as\nmin W\u2208Rd0\u00d7d1\u00d7\u00b7\u00b7\u00b7\u00d7dp \u2016W \u00d71 X\u2212Y\u20162F + \u03b3\u2016W\u20162F (5)\ns.t. rank(W) \u2264 (R0, R1, \u00b7 \u00b7 \u00b7 , Rp) ,\nwhere the input matrix X \u2208 RN\u00d7d0 and the output tensor Y \u2208 RN\u00d7d1\u00d7\u00b7\u00b7\u00b7\u00d7dp are defined by Xn,: = (x(n))>, Yn,:,\u00b7\u00b7\u00b7 ,: = Y(n) for n = 1, \u00b7 \u00b7 \u00b7 , N (Y is the tensor obtained by stacking the output tensors along the first mode).\nLow-rank regression function. Let W\u2217 be a solution of problem (5), it follows from the multilinear rank constraint that W\u2217 = G\u00d71 U0\u00d72 \u00b7 \u00b7 \u00b7\u00d7p+1 Up for some core tensor G \u2208 RR0\u00d7\u00b7\u00b7\u00b7\u00d7Rp and orthogonal matrices Ui \u2208 Rdi\u00d7Ri for 0 \u2264 i \u2264 p. The regression function f\u2217 : x 7\u2192W\u2217 \u20221 x can thus be written as\nf\u2217 : x 7\u2192 G \u00d71 x>U0 \u00d72 \u00b7 \u00b7 \u00b7 \u00d7p+1 Up .\nThis implies several interesting properties. First, for any x \u2208 Rd0 we have f\u2217(x) = T x \u00d71 U1 \u00d72 \u00b7 \u00b7 \u00b7 \u00d7p Up with T x = G \u20221 U>0 x, which implies rank(f\u2217(x)) \u2264 (R1, \u00b7 \u00b7 \u00b7 , Rp), that is the image of f\u2217 is a set of tensors with low multilinear rank. Second, the relation between x and Y = f\u2217(x) is explained in a low dimensional subspace of size R0 \u00d7R1 \u00d7 \u00b7 \u00b7 \u00b7 \u00d7Rp. Indeed one can decompose the mapping f\u2217 into the following steps: (i) project x in RR0 as x\u0304 = U>0 x, (ii) perform a low-dimensional mapping Y\u0304 = G \u20221 x\u0304, (iii) project back into the output space to get Y = Y\u0304 \u00d71 U1 \u00d72 \u00b7 \u00b7 \u00b7 \u00d7p Up.\nComparison with LRR. First, it is obvious that the function learned by vectorizing the outputs and performing LRR does not enforce any multilinear structure in the predictions. That is, the tensor obtained by reshaping a vectorized prediction will not necessarily have a low multilinear rank. This is well illustrated in the experiment from Section 5.2 where this fact is particularly striking when we try to learn an image from noisy measurements with a rank constraint set to 1 (see Figure 2, left). Second, since the multilinear rank is defined with respect to the ranks of the matricizations there are some scenarios where the rank constraint in LRR after vectorizing the output will be (almost) equivalent to the multilinear rank constraint in (5). This is clear if the regression tensor is of low mode-1 rank and of full mode-n rank for n > 1, but even when R0 Ri for i \u2265 1 we can expect LRR to be able to capture most of the low-rank structure which is concentrated along the first mode."}, {"heading": "3.2 Higher-Order Low-Rank Regression", "text": "We now propose an efficient algorithm to tackle problem (5). Theoretical approximation guarantees are given in the next section.\nProblem reformulation. We first show that the ridge regularization term in (5) can be incorporated in the data fitting term. Let X\u0303 \u2208 R(N+d0)\u00d7d0 and Y\u0303 \u2208 R(N+d0)\u00d7d1\u00d7\u00b7\u00b7\u00b7\u00d7dp be defined by X\u0303> = (X | \u03b3I)> and Y\u0303>(1) = ( Y(1) | 0 )>. It is easy to check that the objective function in (5) is equal to \u2016W\u00d71 X\u0303\u2212 Y\u0303\u20162F . Minimization problem (5) can then be rewritten as\nmin G\u2208RR0\u00d7R1\u00d7\u00b7\u00b7\u00b7\u00d7Rp ,\nUi\u2208R di\u00d7Ri for 0\u2264i\u2264p\n\u2016W \u00d71 X\u0303\u2212 Y\u0303\u20162F (6)\ns.t. W = G \u00d71 U0 \u00d72 U1 \u00b7 \u00b7 \u00b7 \u00d7p+1 Up, U>i Ui = I for 0 \u2264 i \u2264 p .\nWe now show that this minimization problem can be reduced to finding p+ 1 projection matrices onto subspaces of dimension R0, R1, \u00b7 \u00b7 \u00b7 , Rp. We start by showing that the core tensor G solution of (6) is determined by the factor matrices U0, \u00b7 \u00b7 \u00b7 ,Up.\nTheorem 1. For given orthogonal matrices U0, \u00b7 \u00b7 \u00b7 ,Up the tensor G that minimizes (6) is given by\nG = Y\u0303 \u00d71 (U>0 X\u0303>X\u0303U0)\u22121U>0 X\u0303> \u00d72 U>1 \u00d73 \u00b7 \u00b7 \u00b7 \u00d7p+1 U>p .\nProof. Using Eq. 2 the objective function in (6) can be written as\n\u2016(Up \u2297Up\u22121 \u2297 \u00b7 \u00b7 \u00b7 \u2297U1 \u2297 X\u0303U0)vec(G)\u2212 vec(Y\u0303)\u20162F .\nLet M = Up \u2297 Up\u22121 \u2297 \u00b7 \u00b7 \u00b7 \u2297 U1 \u2297 X\u0303U0. The solution w.r.t. vec(G) of this classical linear least-squares problem is given by (M>M)\u22121M>. Using the mixed-product and inverse properties of the Kronecker product and the columnwise orthogonality of U1, \u00b7 \u00b7 \u00b7 ,Up we obtain\nvec(G) = ( Up \u2297 \u00b7 \u00b7 \u00b7 \u2297U1 \u2297 (U>0 X\u0303>X\u0303U0)\u22121U>0 X\u0303> ) vec(Y\u0303) .\nIt follows from Theorem 1 that problem (5) can be written as\nmin Ui\u2208R\ndi\u00d7Ri , 0\u2264i\u2264p\n\u2016Y\u0303 \u00d71 \u03a00 \u00d72 \u00b7 \u00b7 \u00b7 \u00d7p+1 \u03a0p \u2212 Y\u0303\u20162F (7)\ns.t. U>i Ui = I for 0 \u2264 i \u2264 p, \u03a00 = X\u0303U0 ( U0X\u0303>X\u0303U>0 )\u22121 U>0 X\u0303T , \u03a0i = UiU>i for 1 \u2264 i \u2264 p ,\nwhere we used W \u00d71 X\u0303 = Y\u0303 \u00d71 \u03a00 \u00d72 \u00b7 \u00b7 \u00b7 \u00d7p+1 \u03a0p. Note that \u03a00 is the orthogonal projection onto the space spanned by the columns of X\u0303U0 and \u03a0i is the orthogonal projection onto the column space of Ui for i \u2265 1. Hence solving problem (5) is equivalent to finding p+ 1 low-dimensional subspaces U0, \u00b7 \u00b7 \u00b7 , Up such that projecting Y\u0303 onto the spaces X\u0303U0, U1, \u00b7 \u00b7 \u00b7 , Up along the corresponding modes is close to Y\u0303 .\nHOLRR algorithm. Since solving problem (7) for the p+1 projections simultaneously is a difficult non-convex optimization problem we propose to solve it independently for each projection. This approach has the benefits of both being computationally efficient and providing good theoretical approximation guarantees (see Theorem 2). The following proposition gives the analytic solutions of (7) when each projection is considered independently.\nProposition 1. For 0 \u2264 i \u2264 p, using the definition of \u03a0i in (7), the optimal solution of\nmin Ui\u2208Rdi\u00d7Ri \u2016Y\u0303 \u00d7i+1 \u03a0i \u2212 Y\u0303\u20162F s.t. U>i Ui = I\nis given by the eigenvectors of{ (X\u0303>X\u0303)\u22121X\u0303>Y\u0303(1)Y\u0303>(1)X\u0303 if i = 0 Y\u0303(i)Y\u0303>(i) otherwise\nthat corresponds to the Ri largest eigenvalues.\nProof. For any 0 \u2264 i \u2264 p, since \u03a0i is a projection we have \u3008Y\u0303 \u00d71 \u03a0i, Y\u0303\u3009 = \u3008\u03a0iY\u0303(i), Y\u0303(i)\u3009 = \u2016\u03a0iY\u0303(i)\u20162F , thus minimizing \u2016Y\u0303 \u00d7i \u03a0i \u2212 Y\u0303\u20162F is equivalent to minimizing \u2016\u03a0iY\u0303(i)\u20162F \u2212 2\u3008\u03a0iY\u0303(i), Y\u0303(i)\u3009 = \u2212\u2016\u03a0iY\u0303(i)\u20162F . For i \u2265 1, we have \u2016\u03a0iY\u0303(i)\u20162F = Tr(U>i Y\u0303(i)Y\u0303>(i)Ui) which is maximized by letting the columns of Ui be the top Ri eigenvectors of the matrix Y\u0303(i)Y\u0303>(i). For i = 0 we have\n\u2016\u03a00Y\u0303(i)\u20162F = Tr(\u03a00Y\u0303(1)Y\u0303>(1)\u03a0 > 0 ) = Tr ( (U>0 AU0)\u22121U>0 BU0 ) with A = X\u0303>X\u0303 and B = X\u0303>Y\u0303(1)Y\u0303>(1)X\u0303, which is maximized by the top R0 eigenvectors of A\u22121B.\nThe results from Theorem 1 and Proposition 1 can be rewritten using the original input matrix X and output tensor Y . Since X\u0303>X\u0303 = X>X + \u03b3I and Y\u0303 \u00d71 X\u0303> = Y \u00d71 X>, we can rewrite the solution of Theorem 1 as\nG = Y \u00d71 (U>0 (X>X + \u03b3I)U0)\u22121U>0 XT \u00d72 U>1 \u00d73 \u00b7 \u00b7 \u00b7 \u00d7p+1 U>p .\nSimilarly for Proposition 1 we have\n(X\u0303>X\u0303)\u22121X\u0303>Y\u0303(1)Y\u0303>(1)X\u0303 = (X>X + \u03b3I)\u22121X>Y(1)Y>(1)X\nand one can check that Y\u0303(i)Y\u0303>(i) = Y(i)Y>(i) for any i \u2265 1. The overall Higher-Order Low-Rank Regression procedure (HOLRR) is summarized in Algorithm 1. Note that the Tucker decomposition of the solution returned by HOLRR could be a good initialization point for an Alternative Least Square method. However, studying the theoretical and experimental properties of this approach is beyond the scope of this paper and is left for future work.\nAlgorithm 1 HOLRR Input: X \u2208 RN\u00d7d0 , Y \u2208 RN\u00d7d1\u00d7\u00b7\u00b7\u00b7\u00d7dp , rank (R0, R1, \u00b7 \u00b7 \u00b7 , Rp) and regulariza-\ntion parameter \u03b3. 1: U0 \u2190 top R0 eigenvectors of (X>X + \u03b3I)\u22121X>Y(1)Y>(1)X 2: for i = 1 to p do 3: Ui \u2190 top Ri eigenvectors of Y(i)Y>(i) 4: end for 5: M = ( U>0 (X>X + \u03b3I)U0\n)\u22121 U>0 X> 6: G \u2190 Y \u00d71 M\u00d72 U>1 \u00d73 \u00b7 \u00b7 \u00b7 \u00d7p+1 U>p 7: return G \u00d71 U0 \u00d72 \u00b7 \u00b7 \u00b7 \u00d7p+1 Up"}, {"heading": "3.3 Theoretical Analysis", "text": "Complexity analysis. We compare the computational complexity of the LRR and HOLRR algorithms. For both algorithms the computational cost for matrix multiplications is asymptotically the same (dominated by the products X>X and Y(1)Y>(1)). We thus focus our analysis on the computational cost of matrix inversions and truncated singular value decompositions. For the two methods the inversion of the matrix X>X + \u03b3I can be done in O((d0)3). The LRR method needs to compute the R dominant eigenvectors of a matrix of the same size as Y>(1)Y(1) (see Section 2.2) which can be done in O(R(d1d2 \u00b7 \u00b7 \u00b7 dp)2). The HOLRR algorithm needs to compute the Ri dominant eigenvectors of the matrix Y(i)Y>(i) for i \u2208 [p] and to invert the extra matrix U>0 (X>X + \u03b3I)U0 of size R0 \u00d7 R0 leading to a complexity of O((R0)3 + maxi{Rid2i }). We can conclude that HOLRR is far more efficient than LRR when the output dimensions d1, \u00b7 \u00b7 \u00b7 , dp are large.\nApproximation guarantees. The following theorem shows that the HOLRR algorithm proposed in the previous section is a (p+1)-approximation algorithm for problem (5).\nTheorem 2. Let W\u2217 be a solution of problem (5) and let W be the regression tensor returned by Algorithm 1. If L : Rd0\u00d7\u00b7\u00b7\u00b7\u00d7dp \u2192 R denotes the objective function of (5) w.r.t. W then L(W) \u2264 (p+ 1)L(W\u2217).\nProof. Let U0, \u00b7 \u00b7 \u00b7 ,Up be the matrices defined in Algorithm 1 and let \u03a00, \u00b7 \u00b7 \u00b7 ,\u03a0p be the projection matrices defined in problem (7). We have\nL(W) = \u2016W \u00d71 X\u0303\u2212 Y\u0303\u20162F = \u2016Y\u0303 \u00d71 \u03a00 \u00d72 \u00b7 \u00b7 \u00b7 \u00d7p+1 \u03a0p \u2212 Y\u0303\u20162F \u2264 \u2016Y\u0303 \u00d71 \u03a00 \u00d72 \u00b7 \u00b7 \u00b7 \u00d7p+1 \u03a0p \u2212 Y\u0303 \u00d71 \u03a00 \u00d72 \u00b7 \u00b7 \u00b7 \u00d7p \u03a0p\u22121\u20162F + \u2016Y\u0303 \u00d71 \u03a00 \u00d72 \u00b7 \u00b7 \u00b7 \u00d7p \u03a0p\u22121 \u2212 Y\u0303 \u00d71 \u03a00 \u00d72 \u00b7 \u00b7 \u00b7 \u00d7p\u22121 \u03a0p\u22122\u20162F + \u00b7 \u00b7 \u00b7+ \u2016Y\u0303 \u00d71 \u03a00 \u2212 Y\u0303\u20162F \u2264 \u2016Y\u0303 \u00d7p+1 \u03a0p \u2212 Y\u0303\u20162F + \u2016Y\u0303 \u00d7p \u03a0p\u22121 \u2212 Y\u0303\u20162F + \u00b7 \u00b7 \u00b7+ \u2016Y\u0303 \u00d71 \u03a00 \u2212 Y\u0303\u20162F ,\nwhere we used the fact that \u2016\u03a0M\u2016F \u2264 \u2016M\u2016F for any projection matrix \u03a0. Furthermore, it follows from Theorem 1 that W\u2217 = Y\u00d71 \u03a0\u22170\u00d72 \u00b7 \u00b7 \u00b7\u00d7p+1 \u03a0\u2217p for some projection matrices \u03a0\u2217i satisfying the constraints from problem (7). By Proposition 1 each summand in the last inequality is minimal w.r.t. \u03a0i, hence\n\u2016Y\u0303 \u00d7i \u03a0i \u2212 Y\u0303\u20162F \u2264 \u2016Y\u0303 \u00d7i \u03a0\u2217i \u2212 Y\u0303\u20162F \u2264 \u2016Y\u0303 \u00d71 \u03a0\u22170 \u00d72 \u00b7 \u00b7 \u00b7 \u00d7p+1 \u03a0\u2217p \u2212 Y\u0303\u20162F = \u2016W\u2217 \u00d71 X\u0303\u2212 Y\u0303\u20162F = L(W\u2217)\nwhich concludes the proof.\nOne direct consequence of this theorem is that if there exists a W\u2217 such that W\u2217 \u00d71 X = Y then Algorithm 1 (using \u03b3 = 0) will return a tensor W satisfying W \u00d71 X = Y ."}, {"heading": "4 HOLRR Kernel Extension", "text": "In this section we provide a kernelized version of the HOLRR algorithm. We proceed by analyzing how the algorithm would be instantiated in a feature space and we show that all the steps involved can be performed using the Gram matrix of the input data without having to explicitly compute the feature map.\nLet \u03c6 : Rd0 \u2192 RL be a feature map and let \u03a6 \u2208 Rn\u00d7L be the matrix with lines \u03c6(x(n))> for n \u2208 [N ]. The higher-order low-rank regression problem in the feature space boils down to the minimization problem\nmin W\u2208RL\u00d7d1\u00d7\u00b7\u00b7\u00b7\u00d7dp \u2016W \u00d71 \u03a6\u2212Y\u20162F + \u03b3\u2016W\u20162F (8)\ns.t. rank(W) \u2264 (R0, R1, \u00b7 \u00b7 \u00b7 , Rp) .\nFollowing the HOLRR algorithm proposed in Section 3.2 one needs to compute the top R0 eigenvectors of the L\u00d7L matrix (\u03a6>\u03a6 + \u03b3I)\u22121\u03a6>Y(1)Y>(1)\u03a6. The following proposition shows that this can be done using the Gram matrix K = \u03a6\u03a6> without having to explicitly compute the feature map \u03c6.\nProposition 2. If \u03b1 \u2208 RN is an eigenvector with eigenvalue \u03bb of the matrix\n(K + \u03b3I)\u22121Y(1)Y>(1)K , (9)\nthen v = \u03a6>\u03b1 \u2208 RL is an eigenvector with eigenvalue \u03bb of the matrix (\u03a6>\u03a6 + \u03b3I)\u22121\u03a6>Y(1)Y>(1)\u03a6.\nProof. Let \u03b1 \u2208 RN be the eigenvector from the hypothesis. We have\n\u03bbv = \u03a6>(\u03bb\u03b1) = \u03a6> ( (K + \u03b3I)\u22121Y(1)Y>(1)K ) \u03b1\n= \u03a6>(\u03a6\u03a6> + \u03b3I)\u22121Y(1)Y>(1)\u03a6\u03a6 >\u03b1 = ( (\u03a6>\u03a6 + \u03b3I)\u22121\u03a6>Y(1)Y>(1)\u03a6 ) v .\nLet A be the top R0 eigenvectors of the matrix (9). When working with the feature map \u03c6, it follows from the previous proposition that line 1 in Algorithm 1 is equivalent to choosing U0 = \u03a6>A \u2208 RL\u00d7R0 , while the updates in line 3 stay the same. The regression tensor W \u2208 RL\u00d7d1\u00d7\u00b7\u00b7\u00b7\u00d7dp returned by this algorithm is then equal to\nW = Y \u00d71 P\u00d72 U1U>1 \u00d72 \u00b7 \u00b7 \u00b7 \u00d7p+1 UpU>p ,\nwhere\nP = \u03a6>A ( A>\u03a6(\u03a6>\u03a6 + \u03b3I)\u03a6>A )\u22121 A>\u03a6\u03a6>\n= \u03a6>A ( A>\u03a6\u03a6>(\u03a6\u03a6> + \u03b3I)A )\u22121 A>\u03a6\u03a6>\n= \u03a6>A ( A>K(K + \u03b3I)A )\u22121 A>K . Suppose now that the feature map \u03c6 is induced by a kernel k : Rd0\u00d7Rd0 \u2192 R. The prediction for an input vector x is then given by W \u20221 x = C \u20221 kx where the nth component of kx \u2208 RN is k(xn,x) and the tensor C \u2208 RN\u00d7d1\u00d7\u00b7\u00b7\u00b7\u00d7dp is defined by\nC = G \u00d71 A\u00d72 U1 \u00d72 \u00b7 \u00b7 \u00b7 \u00d7p+1 U>p ,\nwith G = Y \u00d71 ( A>K(K + \u03b3I)A )\u22121 A>K\u00d72 U>2 \u00d73 \u00b7 \u00b7 \u00b7 \u00d7p+1 Up. Now, letH be the reproducing kernel Hilbert space associated with the kernel k. The overall procedure for kernelized HOLRR is summarized in Algorithm 2. This algorithm returns the tensor C \u2208 RN\u00d7d1\u00d7\u00b7\u00b7\u00b7\u00d7dp defining the regression function\nf : x 7\u2192 C \u20221 kx = N\u2211\nn=1 k(x,x(n))C(n) ,\nwhere C(n) = Cn:\u00b7\u00b7\u00b7: \u2208 Rd1\u00d7\u00b7\u00b7\u00b7\u00d7dp .\nAlgorithm 2 Kernelized HOLRR Input: Gram matrix K \u2208 RN\u00d7N , Y \u2208 RN\u00d7d1\u00d7\u00b7\u00b7\u00b7\u00d7dp , rank (R0, R1, \u00b7 \u00b7 \u00b7 , Rp)\nsize of W RLS LRR ADMM MLMT-NC HOLRR 10\u00d7 10\u00d7 10\u00d7 10 0.012 0.45 12.92 945.79 0.04\n3\u00d7 70\u00d7 70 0.04 10.46 \u2212 \u2212 0.08 160\u00d7 5\u00d7 16\u00d7 5 2.07 2.24 40.23 \u2212 1.67"}, {"heading": "5 Experiments", "text": "We present and analyze the experimental results obtained on a regression task on synthetic data, an image reconstruction task and a meteorological forecasting task on real data1. We compare the predictive accuracy of HOLRR with the following methods: - RLS: Regularized least squares. - LRR: Low-rank regression (see Section 2.2). - ADMM: a multilinear approach based on tensor trace norm regularization introduced in (Gandy et al., 2011) and (Romera-Paredes et al., 2013). - MLMT-NC: a nonconvex approach proposed in (Romera-Paredes et al., 2013) in the context of multilinear multitask learning.\nFor experiments with kernel algorithms we use the readily available kernelized RLS and the LRR kernel extension proposed in (Mukherjee & Zhu, 2011) (note that ADMM and MLMT-NC only consider a linear dependency between inputs and outputs).\nAlthough MLMLT-NC is perhaps the closest algorithm to ours, we applied it only to simulated data. This is because MLMLT-NC is computationally very expensive and becomes intractable for large data sets. To give an idea on the running times, averages for some of the experiments are reported in Table 1.\n1The code and data used in the experiments will be made available by the authors."}, {"heading": "5.1 Synthetic Data", "text": "We generate both linear and nonlinear data. The linear data is drawn from the model Y = W \u20221 x+E where W \u2208 R10\u00d710\u00d710\u00d710 is a tensor of multilinear rank (6, 4, 4, 8) drawn at random, x \u2208 R10 is drawn from N (0, I) and each component of the error tensor E is drawn from N (0, 0.1). The nonlinear data is drawn from the model Y = W \u20221 (x\u2297 x) + E where W \u2208 R25\u00d710\u00d710\u00d710 is a tensor of rank (5, 6, 4, 2) drawn at random and x \u2208 R5 and E are generated as above. The hyper-parameters for all algorithms are selected using 3-fold cross validation on the training data.\nThese experiments have been carried out for different sizes of the training data set, 20 trials have been executed for each size. The average RMSEs on a test set of size 100 for the 20 trials are reported in Figure 1. We see that the HOLRR algorithm clearly outperforms the other methods on the linear data. MLMT-NC is the method obtaining the better accuracy after ours, it is however much more computationally expensive (see Table 1). On the nonlinear data the LRR method achieves good performances but HOLRR is still significantly more accurate for small data set sizes."}, {"heading": "5.2 Image Reconstruction from Noisy Measurements", "text": "To give an illustrative intuition on the differences between matrix and multilinear rank regularization we generate data from the model Y = W \u20221 x+E where the tensor W is a color image of size m\u00d7 n encoded with three color channels RGB. We consider two different tasks depending on the input dimension: (i) W \u2208 R3\u00d7m\u00d7n, x \u2208 R3 and (ii) W \u2208 Rn\u00d7m\u00d73, x \u2208 Rn. In both tasks the components of both x and E are drawn from N (0, 1) and the regression tensor W is learned from a training set of size 200.\nThis experiment allows us to visualize the tensors returned by the RLS, LRR and HOLRR algorithms. The results are shown in Figure 2 for three images: a green cross (of size 50 \u00d7 50), a thumbnail of a Rothko painting (44 \u00d7 70) and a square made of triangles (70\u00d7 70), note that the first two images have a low rank structure which is not the case for the third one.\nWe first see that HOLRR clearly outperforms LRR on the task where the\ninput dimension is small (task (i)). This is to be expected since the rank of the matrix W(1) is at most 3 and LRR is unable to enforce a low-rank structure on the output modes of W . When the rank constraint is set to 1 for LRR and (3, 1, 1) for HOLRR we clearly see that unlike HOLRR the LRR approach does not enforce any low-rank structure on the regression tensor along the output modes. On task (ii) the difference is more subtle, but we can see that setting a rank constraint of 2 for the LRR algorithm prevents the model from capturing the white border around the green cross and creates the vertical lines artifact in the Rothko painting. For higher values of the rank the model starts to learn the noise. The tensor returned by HOLRR with rank (4, 4, 3) does not exhibit these behaviors and gives better results on these two images. On the square image which does not have a low-rank structure both algorithms do not perform very well. Overall, we see that capturing the mutlilinear low-rank structure of the output data allows HOLRR to separate the noise from the true signal better than RLS and LRR."}, {"heading": "5.3 Real Data", "text": "We compare our algorithm with other methods on the task of meteo forecasting. We collected data from the meteorological office of the UK 2: monthly average measurements of 5 variables in 16 stations across the UK from 1960 to 2000. The forecasting task consists in predicting the values of the 5 variables in the 16 stations from their values in the preceding months. We use the values of all the variables from the past 2 months as covariates and consider the tasks of predicting the values of all variables for the next k months, the output tensors are thus in Rk\u00d716\u00d75.\nWe randomly split the available data into a training set of size N , a test set of size 50 and a validation set of size 20, for all methods hyper-parameters are chosen w.r.t. to their performance on the validation set. The average test RMSE over 10 runs of this experiment for different values of N and k are reported in Table 2. We see that the HOLRR has overall a better predictive accuracy than the other methods, especially when the tensor structure of the output data gets richer (k = 3 and k = 5). On this dataset, nonlinear methods only improve the results by a small margin with the RBF kernel, and the polynomial kernel performs poorly."}, {"heading": "6 Conclusion", "text": "We proposed a low-rank multilinear regression model for tensor-structured output data. We developed a fast and efficient algorithm to tackle the multilinear rank penalized minimization problem and provided theoretical approximation guarantees. Experimental results showed that capturing low-rank structure in the output data can help to improve tensor regression performance.\n2http://www.metoffice.gov.uk/public/weather/climate-historic/"}], "references": [{"title": "Estimating linear restrictions on regression coefficients for multivariate normal distributions", "author": ["T.W. Anderson"], "venue": "Annals of Mathematical Statistics,", "citeRegEx": "Anderson,? \\Q1951\\E", "shortCiteRegEx": "Anderson", "year": 1951}, {"title": "Fast multivariate spatio-temporal analysis via low rank tensor learning", "author": ["M.T. Bahadori", "Q.R. Yu", "Y. Liu"], "venue": "In NIPS", "citeRegEx": "Bahadori et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Bahadori et al\\.", "year": 2014}, {"title": "Nonnegative Matrix and Tensor Factorizations. Applications to Exploratory Multi-way Data Analysis and Blind Source Separation", "author": ["A. Cichocki", "R. Zdunek", "A.H. Phan", "S.I. Amari"], "venue": null, "citeRegEx": "Cichocki et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Cichocki et al\\.", "year": 2009}, {"title": "Tensor completion and low-n-rank tensor recovery via convex optimization", "author": ["Gandy", "Silvia", "Recht", "Benjamin", "Yamada", "Isao"], "venue": "Inverse Problems,", "citeRegEx": "Gandy et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Gandy et al\\.", "year": 2011}, {"title": "Reduced-rank regression for the multivariate linear model", "author": ["A.J. Izenman"], "venue": "Journal of Multivariate Analysis,", "citeRegEx": "Izenman,? \\Q1975\\E", "shortCiteRegEx": "Izenman", "year": 1975}, {"title": "Modern Multivariate Statistical Techniques: Regression, Classification, and Manifold Learning", "author": ["A.J. Izenman"], "venue": null, "citeRegEx": "Izenman,? \\Q2008\\E", "shortCiteRegEx": "Izenman", "year": 2008}, {"title": "Tensor decompositions and applications", "author": ["T.G. Kolda", "B.W. Bader"], "venue": "SIAM review,", "citeRegEx": "Kolda and Bader,? \\Q2009\\E", "shortCiteRegEx": "Kolda and Bader", "year": 2009}, {"title": "Multilinear Subspace Learning: Dimensionality Reduction of Multidimensional Data", "author": ["H. Lu", "K.N. Plataniotis", "A. Venetsanopoulos"], "venue": null, "citeRegEx": "Lu et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Lu et al\\.", "year": 2013}, {"title": "Reduced rank ridge regression and its kernel extensions", "author": ["A. Mukherjee", "J. Zhu"], "venue": "Statistical analysis and data mining,", "citeRegEx": "Mukherjee and Zhu,? \\Q2011\\E", "shortCiteRegEx": "Mukherjee and Zhu", "year": 2011}, {"title": "Multivariate reduced-rank regression: theory and applications", "author": ["G.C. Reinsel", "R.P. Velu"], "venue": "Lecture Notes in Statistics. Springer,", "citeRegEx": "Reinsel and Velu,? \\Q1998\\E", "shortCiteRegEx": "Reinsel and Velu", "year": 1998}, {"title": "Multilinear multitask learning", "author": ["B. Romera-Paredes", "M.H. Aung", "N. Bianchi-Berthouze", "M. Pontil"], "venue": "In Proceedings of the 30th International Conference on Machine Learning", "citeRegEx": "Romera.Paredes et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Romera.Paredes et al\\.", "year": 2013}, {"title": "Learning with tensors: a framework based on convex optimization and spectral regularization", "author": ["M. Signoretto", "Q.T. Dinh", "L. De Lathauwer", "J.K. Suykens"], "venue": "Machine Learning,", "citeRegEx": "Signoretto et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Signoretto et al\\.", "year": 2013}, {"title": "Tensor regression with applications in neuroimaging data analysis", "author": ["H. Zhou", "L. Li", "H. Zhu"], "venue": "Journal of the American Statistical Association,", "citeRegEx": "Zhou et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Zhou et al\\.", "year": 2013}], "referenceMentions": [{"referenceID": 12, "context": "Data with a natural tensor structure is encountered in many scientific areas including neuroimaging (Zhou et al., 2013), signal processing (Cichocki et al.", "startOffset": 100, "endOffset": 119}, {"referenceID": 2, "context": ", 2013), signal processing (Cichocki et al., 2009), spatio-temporal analysis (Bahadori et al.", "startOffset": 27, "endOffset": 50}, {"referenceID": 1, "context": ", 2009), spatio-temporal analysis (Bahadori et al., 2014) and computer vision (Lu et al.", "startOffset": 34, "endOffset": 57}, {"referenceID": 7, "context": ", 2014) and computer vision (Lu et al., 2013).", "startOffset": 28, "endOffset": 45}, {"referenceID": 1, "context": ", 2009), spatio-temporal analysis (Bahadori et al., 2014) and computer vision (Lu et al., 2013). Extending multivariate regression methods to tensors is one of the challenging task in this area. Most existing works extend linear models to the multilinear setting and focus on the tensor structure of the input data (e.g. Signoretto et al. (2013)).", "startOffset": 35, "endOffset": 346}, {"referenceID": 9, "context": "In the context of multi-task learning, Romera-Paredes et al. (2013) have proposed a linear model using a tensor-rank penalization of a least squares criterion to take into account the multi-modal interactions between tasks.", "startOffset": 39, "endOffset": 68}, {"referenceID": 1, "context": "Bahadori et al. (2014) have proposed a greedy algorithm to solve a low-rank tensor learning problem in the context of multivariate spatio-temporal data analysis.", "startOffset": 0, "endOffset": 23}, {"referenceID": 0, "context": "The rank constraint in (3) was first proposed in (Anderson, 1951), whereas the term reduced-rank regression was introduced in (Izenman, 1975).", "startOffset": 49, "endOffset": 65}, {"referenceID": 4, "context": "The rank constraint in (3) was first proposed in (Anderson, 1951), whereas the term reduced-rank regression was introduced in (Izenman, 1975).", "startOffset": 126, "endOffset": 141}, {"referenceID": 0, "context": "The rank constraint in (3) was first proposed in (Anderson, 1951), whereas the term reduced-rank regression was introduced in (Izenman, 1975). Adding a ridge regularization to the rank penalized problem was proposed in Mukherjee & Zhu (2011). In the rest of the paper we will refer to this approach as lowrank regression (LRR).", "startOffset": 50, "endOffset": 242}, {"referenceID": 0, "context": "The rank constraint in (3) was first proposed in (Anderson, 1951), whereas the term reduced-rank regression was introduced in (Izenman, 1975). Adding a ridge regularization to the rank penalized problem was proposed in Mukherjee & Zhu (2011). In the rest of the paper we will refer to this approach as lowrank regression (LRR). The solution of minimization problem (3) is given by projecting the RLS solution onto the space spanned by the top R eigenvectors of Y>PY where P is the orthogonal projection matrix onto the column space of X, that is WLRR = WRLS\u03a0 where \u03a0 is the matrix of the aforementioned projection. For more description and discussion of reduced-rank regression, we refer the reader to the books of Reinsel & Velu (1998) and Izenman (2008).", "startOffset": 50, "endOffset": 737}, {"referenceID": 0, "context": "The rank constraint in (3) was first proposed in (Anderson, 1951), whereas the term reduced-rank regression was introduced in (Izenman, 1975). Adding a ridge regularization to the rank penalized problem was proposed in Mukherjee & Zhu (2011). In the rest of the paper we will refer to this approach as lowrank regression (LRR). The solution of minimization problem (3) is given by projecting the RLS solution onto the space spanned by the top R eigenvectors of Y>PY where P is the orthogonal projection matrix onto the column space of X, that is WLRR = WRLS\u03a0 where \u03a0 is the matrix of the aforementioned projection. For more description and discussion of reduced-rank regression, we refer the reader to the books of Reinsel & Velu (1998) and Izenman (2008).", "startOffset": 50, "endOffset": 756}, {"referenceID": 3, "context": "- ADMM: a multilinear approach based on tensor trace norm regularization introduced in (Gandy et al., 2011) and (Romera-Paredes et al.", "startOffset": 87, "endOffset": 107}, {"referenceID": 10, "context": ", 2011) and (Romera-Paredes et al., 2013).", "startOffset": 12, "endOffset": 41}, {"referenceID": 10, "context": "- MLMT-NC: a nonconvex approach proposed in (Romera-Paredes et al., 2013) in the context of multilinear multitask learning.", "startOffset": 44, "endOffset": 73}], "year": 2016, "abstractText": "This paper proposes an efficient algorithm (HOLRR) to handle regression tasks where the outputs have a tensor structure. We formulate the regression problem as the minimization of a least square criterion under a multilinear rank constraint, a difficult non convex problem. HOLRR computes efficiently an approximate solution of this problem, with solid theoretical guarantees. A kernel extension is also presented. Experiments on synthetic and real data show that HOLRR outperforms multivariate and multilinear regression methods and is considerably faster than existing tensor methods.", "creator": "LaTeX with hyperref package"}}}