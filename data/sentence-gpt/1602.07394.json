{"id": "1602.07394", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "24-Feb-2016", "title": "Improved Accent Classification Combining Phonetic Vowels with Acoustic Features", "abstract": "Researches have shown accent classification can be improved by integrating semantic information into pure acoustic approach. In this work, we combine phonetic knowledge, such as vowels, with enhanced acoustic features to build an improved accent classification system. The classifier is based on Gaussian Mixture Model-Universal Background Model (GMM-UBM), with normalized Perceptual Linear Predictive (PLP) features. The features are further optimized by Principle Component Analysis (PCA) and Hetroscedastic Linear Discriminant Analysis (HLDA). Using 7 major types of accented speech from the Foreign Accented English (FAE) corpus, the system achieves classification accuracy 54% with input test data as short as 20 seconds, which is competitive to the state of the art in this field. This has implications for the accuracy of the FFAE corpus, particularly for high-level data. In this study, we use 2 types of audio-encoded text for all spoken-language audio: A non-English or an English speaking native language, but instead of a standard native language of native languages of English or an English spoken native language of English or an English spoken native language of English or an English spoken native language of English or an English spoken native language of English or an English spoken native language of English or an English spoken native language of English or an English spoken native language of English or an English spoken native language of English or an English spoken native language of English or an English spoken native language of English or an English spoken native language of English or an English spoken native language of English or an English spoken native language of English or an English spoken native language of English or an English spoken native language of English or an English spoken native language of English or an English spoken native language of English or an English spoken native language of English or an English spoken native language of English or an English spoken native language of English or an English spoken native language of English or an English spoken native language of English or an English spoken native language of English or an English spoken native language of English or an English spoken native language of English or an English spoken native language of English or an English spoken native language of English or an English spoken native language of English or an English spoken native language of English or an English spoken native language of English or an English spoken native language of English or an English spoken native language of English or an English spoken native language of English or an English spoken native language of English or an English spoken native language of English or an English spoken native language of English", "histories": [["v1", "Wed, 24 Feb 2016 04:33:49 GMT  (520kb,D)", "http://arxiv.org/abs/1602.07394v1", "International Congress on Image and Signal Processing (CISP) 2015"]], "COMMENTS": "International Congress on Image and Signal Processing (CISP) 2015", "reviews": [], "SUBJECTS": "cs.SD cs.CL", "authors": ["zhenhao ge"], "accepted": false, "id": "1602.07394"}, "pdf": {"name": "1602.07394.pdf", "metadata": {"source": "CRF", "title": "Improved Accent Classification Combining Phonetic Vowels with Acoustic Features", "authors": ["Zhenhao Ge"], "emails": [], "sections": [{"heading": null, "text": "Index Terms\u2014Accent Classification, Vowel Representation, GMM-UBM, Feature Optimization.\nI. INTRODUCTION\nAs businesses become more international, accent verification and classification gains more attention recently, probably because of the increasing demand for better recognizing nonnative speakers and their accented speech. However, this problem is still very challenging, since there are many types of accents and the response time allowed for accent detection is usually very short. Choueiter et al. achieved accuracy of 32% classifying 23 types of accented English [1], using methods in language identification (LID), such as Maximum Mutual Information (MMI) training and Gaussian tokenization. Omar et al. recently integrated Universal Background Model (UBM) into Support Vector Machine (SVM) classifier and claimed that it outperformed the results in [1] by 75.3% relatively [2]. Another work for German vs. Spanish classification in [3] reported classification rates of 73% and 58.9%, using GMMs and naive Bayes classification respectively. In addition, classification rates of 36.2%, 17.7% and 13.2% were reported in [3], for 4-, 13- and 23-way classification using naive Bayes. To the best of our knowledge, these are the only three works, which used the same dataset used in this work.\nIn this work, we first created a baseline accent classifier for 7 selected types of English accents, using Gaussian Mixture Model-Universal Background Model (GMM-UBM), with normalized Perceptual Linear Predictive (PLP) features. The feature were then dimension-reduced and discriminatively optimized using Principle Component Analysis (PCA) and Heteroscedastic Linear Discriminant Analysis (HLDA). Since most identifiable accents are presented from the pronunciation of vowels rather than consonants [4], multiple vowel-specific GMMs were computed with features of the vowel components,\nextracted either from phoneme alignment (in system development) or phoneme recognition (in system test). Compared with the baseline with pure acoustic information, the improved 7-way classification system increases accuracy from 42% to 54%, using only up to 20 seconds speech data.\nThis work was initiated during the author\u2019s internship at Interactive Intelligence (ININ) [5], and the algorithm and experiments were later refined for better accuracy and efficiency [6]. This paper reviews the major components of the accent classification system, with highlights on the recent improvements in feature generation and constructing baseline and improved classifiers. The remaining paper is organized as follows: Sec. II introduces the database used in this work and a process of feature optimization and dimension reduction. In Sec. III, the main concept of creating accent-adapted features based on phonetic vowels is demonstrated. Then, the baseline classifier and improved version with vowel extraction are described in Sec. IV, followed by the results, conclusion and future work in Sec. V."}, {"heading": "II. DATA AND FEATURE PREPARATION", "text": "Preprocessing such as data and feature preparation, significantly impact the performance of classification. In this section, we introduce the database used in this work, discuss the feature extraction including normalization and Gaussianization, and feature optimization and dimension reduction with PCA and HLDA. The whole process is illustrated in Fig.1."}, {"heading": "A. Database", "text": "The Foreign Accented English (FAE) corpus from Linguistic Data Consortium (LDC) with catalog number LDC2007S08 is used in this work. It is one of the most comprehensive accented English speech database currently available, which contains 4925 sentences of 23 types of accents, with 20 second duration on average.\nAccents are divided into 7 major categories based on their relationships shown in Table I and one accent from each group is selected for developing a 7-way accent classifier. Table II\nar X\niv :1\n60 2.\n07 39\n4v 1\n[ cs\n.S D\n] 2\n4 Fe\nb 20\n16\nsummarizes some statistics in each of these accent groups, such as 1) the number of utterances; 2) their proportion in the entire FAE corpus, 3) the total durations before and after silence removal, and 4) their corresponding compression ratio (Dir.2/Dir.1). Since there is no transcription comes along with the speech in FAE, we also transcribed the audio data of these 7 major accents, in order to perform vowel extraction from speech using phoneme alignment later. The selected partial dataset of FAE were randomly divided into training, development and testing with ratio 70 : 15 : 15."}, {"heading": "B. Silence Removal", "text": "In practice, only the high signal-to-noise ratio (SNR) regions of the waveform are retained for classification. Therefore, silence removal or so-called voice activity detection (VAD) is often performed before feature extraction. Here we use the method described in [7], which detects the silence by thresholding on the short-time energy rate and spectral centroids of the speech. One can also use either Auditory Toolbox [8] or Voicebox [9] for the same purpose.\nGiven si(n), n \u2208 [1, N ] as the audio samples in the ith frame, its short-time energy rate ei, can be formulated as\nei = 1\nN N\u2211 n=1 |si(n)|2, (1)\nwhere N is the number of samples in one frame. The spectral centroid can be defined as\nci = \u2211K k=1(k + 1)Si(k)\u2211K\nk=1 Si(k) , (2)\nwhere Si(k), k \u2208 [1,K] is the Discrete Fourier Transform (DFT) coefficients of si. ei is used to discriminate silence with environmental noise, while ci is used to remove nonspeech noise, such as coughing, because of its lower energy concerntration in the spectrum, relative to that of normal human speech.\nFig.2 shows an example of silence removal with both measurements of short-time energy rate and spectral centroids on data file FAR00035.wav in FAE corpus with Arabic (AE) accents. It is considered to be silence if either of these 2 measurements is lower than its threshold. As shown in Table II, the total duration of recording for each type of accents were reduced after silence removal."}, {"heading": "C. Feature Extraction and Optimization", "text": "After silence removal, the data of the selected accents were then transformed to 39-dimensional PLP windowed feature frames with 10 millisecond each, using the method in [10]. Feature Mean and Variance Normalization (MVN) and shortterm Gaussianization (a.k.a feature warping) were applied afterwards using the method in [11]. The latter warps the distribution of the feature to a standard normal distribution to mitigate the effects of locally linear channel mismatch. This is specially useful because the features distribution here can be modeled by Gaussians [12].\nThe normalized and wrapped features were further improved by PCA and HLDA for dimension reduction and optimization. PCA is commonly used for dimension reduction, which preserves the data dimensions with larger variations in the eigenspace. It has been applied to many applications, such as face recognition [13] and speech evaluation [14], etc. It also helps to regularize the data and avoid over-fitting in HLDA which is performed afterwards [15]. Here the feature dimension is reduced from 39 to 30 after applying PCA.\nCompared with PCA, LDA reduces dimensions by mapping data into a subspace while maximizing the discriminative information. It overcomes the weakness of PCA, when the\ndiscriminative information is actually in the dimension with less variation. It has been also applied to many problems, such as face recognition [16] and speaker recognition [17]. Assume there are K = \u2211S s=1Ks number of M -dimensional data vectors xk in S classes, where Ks is the number of vectors in class s \u2208 [1, S]. Let the global mean \u03a6 over all classes be 1K \u2211K k xk and the local mean \u03a6s for each class\ns be 1Ks \u2211\nxk\u2208s xk, the between-class scatter SB and withinclass scatter SW can be defined as\nSB = 1\nK K\u2211 k=1 (xk \u2212\u03a6)(xk \u2212\u03a6)T or\nSB = 1\nS S\u2211 s=1 (\u03a6s \u2212\u03a6)(\u03a6s \u2212\u03a6)T , (3)\nand\nSW = 1\nS S\u2211 s=1 \u2211 xk\u2208s (xk \u2212\u03a6s)(xk \u2212\u03a6s)T or\nSW = 1\nS S\u2211 s=1 1 Ks \u2211 xk\u2208s (xk \u2212\u03a6s)(xk \u2212\u03a6s)T . (4)\nThe first definitions of Eq. (3) and Eq. (4) consider the class weights, i.e. the sizes of each class s, while the second does not. The first definitions are consistent with the LDA definition used in Kumar\u2019s HLDA work [18] and are used in this work. However, the second definitions of both formulas are also provided for completeness. SW is likely to be singular if there is not enough data in that class, however, by applying PCA first, this problem can be significantly alleviated.\nDefine w as a direction in the underlying W to be transformed to, wTSBw and wTSWw are the projections of SB and SW onto w and searching for directions w for the best class discrimination is equivalent to maximizing the ratio of (wTSBw)/(w\nTSWw) subject to wTSWw = 1. The latter is called the Fisher Discriminant function and can be converted by Lagrange multipliers and solved by eigen-decomposition of S\u22121W SB . By selecting eigenvectors associated with the most significant m eigenvalues of S\u22121W SB , one can map the original M -dimensional data into a m-dimensional subspace for discriminative feature reduction.\nLDA is derived with the assumption that features in various dimensions have the same variance, but in real scenario, there are examples illustrating LDA may transform data into a suboptimal space when the dimension variances are different. Hetroscedastic LDA (HLDA) is a generalization of LDA using Maximum Likelihood Estimation (MLE) on Gaussian ditributions, which removes this assumption. An improved version of Kumar\u2019s HLDA algorithm with more flexibility and higher efficiency was developed in MATLAB and used in this work [19]. The context size C is set to 1 and the feature dimension is further reduced from 30 to 20 after applying HLDA."}, {"heading": "III. PHONETIC VOWEL REPRESENTATION", "text": "Minematsu et al. [20] and Suzuki et al. [4] demonstrated that, for a particular speaker, the location of 5 fundamental vowels in the feature space of a target language (such as English in this work), is relatively consistent. Therefore, they can be extracted as accent-adapted features and used for identifying accent of that speaker.\nFig.3 is a simple demonstration of 5 vowels from both accented and non-accented (standard) languages in the reduced 2-dimensional feature space [20]. The center in each pentagon is the weighted average of five vowels based on their positions in feature space and frequency of appearance in the corpus. By matching the center of the pentagon of the standard and the accented language into the overlapped pentagon in the bottom of Figure 3, the Bhattacharyya distances [21] between each pair of corresponding vowels and their angles can be computed and stored in a vector. This vector Vi represents the difference from the accented language Li to the standard one L.\nTo classify the test speech into one of the accent categories L1, L2, . . . , LN , where N is the number of accent categories, the difference from Vj to Vi, i \u2208 [1, N ] and V (category of standard language) are computed, compared and classified to the nearest category of accent. Comparison\u00a0of\u00a05\u00a0vowels\u00a0locations\u00a0in\u00a0 standard\u00a0and\u00a0accented\u00a0language"}, {"heading": "A. Phoneme Alignment in System Development", "text": "In order to extract vowels from speech data, phoneme alignment was performed during system development, with the in-house transcriptions of partial FAE corpus covering the 7 accents, each of which is from one major type of the accent groups. We prepared the dictionary needed for phoneme alignment using HVite in HTK [22], through a procedure including transcription cleaning, word collection, word-to-pronunciation conversion, etc. Figure 4 demonstrates the process of dictionary preparation and phoneme alignment for FAE corpus. The dictionary file is a list of pairs of words and pronunciations in HTK format, which can be obtained through the process of word collection, word-to-pronunciation conversion with ININ Lexicon Tester and HTK dictionary file creation. In Phoneme alignment, the HTK configuration file, HMM model definition and tired list were all trained using Fisher corpus.\nwww.inin.com \u00a92012\u00a0Interactive\u00a0Intelligence\u00a0Group\u00a0Inc.\nDictionary\u00a0Preparation\u00a0and\u00a0Phoneme\u00a0Alignment"}, {"heading": "B. Phoneme Recognition in System Test", "text": "During system test, there is no transcription available. To find features corresponding to vowels, phoneme recognition was performed on the test accented speech using HTK. Since the recognition cannot be perfect, only a subset of recognized vowels with level of confidence score higher than a threshold based on the n-gram log likelihood were used. This threshold was predefined with the training and development data."}, {"heading": "IV. GMM-UBM FRAMEWORK FOR ACCENT CLASSIFICATION", "text": "The Gaussian Mixture Model-Universal Background Model (GMM-UBM) framework has been sussessfully applied to speech verification and classification systems [23]. The accent classification algorithm developed in this paper treats accents as speakers, and models the attributes of accents using GMMUBM, which is similar to modeling the attributes of speakers in speaker classification systems. Modern speaker classification systems train a general Gaussian Mixture Model (GMM) with data from all speakers, so-called Universal Background Model (UBM) and then generate individual GMMs for each speaker by adapting UBM with features from individual speakers. Subsec. IV-A and IV-B provide an outline of applying the similar framework to an accent classification problem."}, {"heading": "A. Universal Background Model (UBM)", "text": "The Universal Background Model (UBM) is a general GMM trained with features from all types of accents. Given a GMM \u03bb = {wi,\u00b5i,\u03a3i}, i \u2208 [1, N ] and N is the number of mixture components, the likelihood function for a feature frame x can be formulated as\np(x|\u03bb) = N\u2211 i=1 wipi(x), (5)\nwhere\npi(x) = 1 (2\u03c0)M/2|\u03a3i|1/2 exp{\u22121 2 (x\u2212 \u00b5i)T \u03a3\u22121i (x\u2212 \u00b5i)}.\n(6) The parameters of GMM \u03bb, including weight wi, \u00b5i and covariance matrix \u03a3i can be optimized by ExpectationMaximization (EM) algorithm [24]. Here \u03a3i is restricted to be diagonal. Usually the feature vectors are assumed independent,\nso the log-likelihood of a GMM \u03bb for a sequence of K feature vectors, X = {x1,x2, . . . ,xK} is computed as\nlog p(X|\u03bb) = K\u2211\nm=1\nlog p(xk|\u03bb). (7)"}, {"heading": "B. Adaptation of Accent Model", "text": "In the GMM-UBM system, we derive the individual accent GMMs by adapting the parameters of the UBM \u03bbUBM using the training speech X = {x1,x2, . . . ,xK} of each accents and a form of Bayesian adaptation. The adaptation is a two step process and the first step is identical to the E-step of the EM algorithm, where we determine the probabilistic alignment of X into the UBM mixtures. That is for ith component of the UBM, we compute\nPr(i|xk) = wipi(xk)\u2211N\nn=1 wnpn(xk) . (8)\nThen, the weight, mean and variance can be computed by\nni = K\u2211 k=1 Pr(i|xk), (9)\nEi(x) = 1\nni K\u2211 k=1 Pr(i|xk)xk, (10)\nEi(x 2) =\n1\nni K\u2211 k=1 Pr(i|xk)x2k, (11)\nwhere x2 is shorthand for diag(xxT ). Finally, these new sufficient statistics from the training data are sued to update UBM for mixture i, to create the adapted parameters for mixture i in the accent GMMs, with the equations:\nw\u0302i = [\u03b1 w i ni/T + (1\u2212 \u03b1wi )wi]\u03b3 (12)\n\u00b5\u0302i = \u03b1 m i Ei(x) + (1\u2212 \u03b1mi )\u00b5i (13)\n\u03c3\u03022i = \u03b1 v iEi(x 2) + (1\u2212 \u03b1vi )(\u03c32i + \u00b52i )\u2212 \u00b5\u03022i , (14) where {\u03b1wi , \u03b1mi , \u03b1vi } are the adaptation coefficients for the weights, means and variances respectively, controlling the balance between old and new estimates. They can be derived from the Maximum a posteriori (MAP) estimation equations for a GMM using constraints on the prior distribution described in [25]. The scale factor \u03b3 is computed over all adapted mixture weights to ensure they sum to 1."}, {"heading": "C. Baseline Classifier", "text": "After obtaining the adapted GMM parameter set \u03bbs through GMM-UBM framework for accent class s \u2208 [1, S], the GMMbased classifier, which maximize a posteriori probability for K M -dimensional feature vectors X (M\u00d7K) can be formulated as:\nS\u0302 = arg max s\u2208[1,S] Pr(\u03bbs|X) = arg max s\u2208[1,S] p(X|\u03bbs)\u03bbs p(X)\n\u221d arg max s\u2208[1,S] p(X|\u03bbs)\n\u221d arg max s\u2208[1,S] K\u2211 k=1 logp(xk|\u03bbs). (15)\nThe first equation is due to Bayes\u2019 rule. The first proportion is assuming Pr(\u03bbs) = 1/S and p(X) is the same for all accent models. The second proportion uses logarithm and independence between input samples xk, k \u2208 [1,K], explained before in Eq. (7). Providing accent features for UBM adapation, Fig.5 shows the diagram for baseline classification with accent GMM classifiers.\nProvide\u00a0features\u00a0to\u00a0UBM\u00a0and\u00a0 adapt\u00a0the\u00a0individual\u00a0GMMs\nD. Improved Classifier\nThe improvement from the baseline is mainly contributed from the vowel extraction. To construct the classifier with vowel representation, instead of directly measureing the shift of vowels from the standard speech to the accented one, the speech segments from the same vowel of various types of accents were concatenated and used to train vowel-specific UBMs. Fig.6 shows that each of the T UBMs was then adapted to S separated GMMs using data from various accent types. Here T is the number of vowels used in this work. Instead of\nusing only the fundamental 5 vowels described in Sec. III, the same concept was generalized and all 15 vowels in Arpabet [26] were used, which are listed in Table III. S is the number\nof selected accent types from FAE corpus (Table II), which is 7 in this work. Given extracted features of T types of vowels\n[X(1), X(2), . . . , X(t), . . . , X(T )] from accented test feature X , the improved GMM accent classifier as the combination of GMM classifiers of all vowels (shown at the bottom box in Fig.6) can be formulated as\nS\u0302 = arg max s\u2208[1,S] T\u2211 t=1 wtPr(\u03bbs,t|X(t))\n\u221d arg max s\u2208[1,S] T\u2211 t=1 wt K(t)\u2211 k=1 logp(x (t) k |\u03bbs,t). (16)\nwhere \u03bbs,t is the GMM for sth accent and tth type of vowels, and wt is the weight of the vowel-specific GMM classifier for tth vowel. Adding this additional layer on the GMM classifier is critical to find the vowel sets which preserve the accents and later shown to improve on classifying accents.\nThere are two factors considered in the vowel weight wt in Eq. (16), which are 1) the popularity (proportion) of tth vowels in the whole vowel set rt, and 2) the discriminativeness dt, i.e. the difference in the distributions of the same vowels extracted from different accents. The first factor is based on the assumption GMMs trained with more data is more reliable than the ones with less data. Fig.7 shows the popularity of vowels in descending order. It show the vowel ah is much more popular (frequent) than the vowel oy in the selected dataset. The second factor is based on the assumption that\nvowels are more discriminative if the distributions of GMMs of the same vowel but different accents are far apart. For example, Fig.8 shows the Hellinger distances [27] computed between any 2 GMM distributions of 7 different accent types for the vowel aa. The discriminativeness factor dt for aa is just the reciprocal of the mean of these distances from ( 7 2\n) combinations (the smaller mean, the more weight). Here we simply used wt = rtdt to compute the weights of vowels, which assumes both factors are equally important."}, {"heading": "V. RESULTS AND CONCLUSION", "text": "The baseline classification was based on accent GMM classifier with 256 mixtures, adapted from UBM, using normalized and warped 39-dimensional PLPs. The features were then optimized using PCA and HLDA with context size C = 1, and the dimension was reduced from 39 to 20. With the enhanced feature, the baseline accuracy was increased from 42.3% to 47.9%. The main contribution of accuracy improvement was from the classifier of the combination of weighted vowels, which further increased the 7-way classification rate to 53.7%. Table IV shows the performance of all these 3 experiments with various models and features.\nThis work demonstrates that methods in speaker recognition can be used for accent classification. With several feature optimization techniques and phonetic vowel information, the accuracy obtained from accented speech as short as 20 seconds, is competitive compared with the state of the art in [1], [2] and [3]. In the future, more recent classification methods such as i-vector (eigenvoice component) [28], or neural network classifier [29] can be explored, used or combined with the current methods. More data-driven techniques can be experimented, such as 1) training distinct UBMs for male and female accented speakers, 2) using tri-phone vowel set instead of the current mono-phone vowel set for more refined classification, 3) selecting a subset of vowels rather than using all 15 vowels in Arpabet by experiment for better classification results, etc."}], "references": [{"title": "An empirical study of automatic accent classification", "author": ["G. Choueiter", "G. Zweig", "P. Nguyen"], "venue": "ICASSP 2008. IEEE International Conference on. IEEE.", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2008}, {"title": "A novel approach to detecting non-native speakers and their native language", "author": ["M.K. Omar", "J. Pelecanos"], "venue": "ICASSP, 2010 IEEE International Conference on. IEEE.", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2010}, {"title": "Acoustic adaptation and accent identification in the ICSI MR and FAE corpora", "author": ["J. Mac\u0131\u0301as-Guarasa"], "venue": "ICSI Meeting slides, 2003.", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2003}, {"title": "Improved structurebased automatic estimation of pronunciation proficiency", "author": ["M. Suzuki", "L. Dean", "N. Minematsu", "K. Hirose"], "venue": "Proc. SLaTE, vol. 5, 2009.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2009}, {"title": "Mispronunciation Detection with Multiple Applications: for Language Learning and Speech Recognition Adaptation and Improvement", "author": ["Z. Ge"], "venue": null, "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2014}, {"title": "Accent classification with phonetic vowel representation", "author": ["Z. Ge", "Y. Tan", "A. Ganapathiraju"], "venue": "Pattern Recognition (ACPR), 2015 3rd IAPR Asian Conference on. IEEE, 2015.", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2015}, {"title": "A method for silence removal and segmentation of speech signals, implemented in matlab", "author": ["T. Giannakopoulos"], "venue": "University of Athens, Athens, 2009.", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2009}, {"title": "Auditory toolbox", "author": ["M. Slaney"], "venue": "Interval Research Corporation, Tech. Rep, vol. 10, p. 1998, 1998.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 1998}, {"title": "Voicebox: Speech processing toolbox for matlab", "author": ["M. Brookes"], "venue": "Software, available [Mar. 2011] from www. ee. ic. ac. uk/hp/staff/dmb/voicebox/voicebox. html, 1997.", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2011}, {"title": "PLP and RASTA (and MFCC, and inversion) in MATLAB", "author": ["D. Ellis"], "venue": "http://labrosa.ee.columbia.edu/matlab/rastamat/, accessed 2015-07-01.", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2015}, {"title": "MSR identity toolbox v1. 0: A matlab toolbox for speaker recognition research", "author": ["S.O. Sadjadi", "M. Slaney", "L. Heck"], "venue": "Speech and Language Processing Technical Committee Newsletter, 2013.", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2013}, {"title": "Feature warping for robust speaker verification", "author": ["J. Pelecanos", "S. Sridharan"], "venue": "2001.", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2001}, {"title": "Face recognition using eigenfaces", "author": ["M. Turk", "A.P. Pentland"], "venue": "Computer Vision and Pattern Recognition, 1991. Proceedings CVPR\u201991., IEEE Computer Society Conference on. IEEE, 1991, pp. 586\u2013591.", "citeRegEx": "13", "shortCiteRegEx": null, "year": 1991}, {"title": "PCA method for automated detection of mispronounced words", "author": ["Z. Ge", "S.R. Sharma", "M.J. Smith"], "venue": "SPIE Defense, Security, and Sensing. International Society for Optics and Photonics, 2011, pp. 80 581D\u201380 581D.", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2011}, {"title": "Why can LDA be performed in PCA transformed space?", "author": ["J. Yang", "J.-y. Yang"], "venue": "Pattern recognition,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2003}, {"title": "Face recognition using LDA-based algorithms", "author": ["J. Lu", "K.N. Plataniotis", "A.N. Venetsanopoulos"], "venue": "Neural Networks, IEEE Transactions on, vol. 14, no. 1, pp. 195\u2013200, 2003.", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2003}, {"title": "PCA/LDA approach for text-independent speaker recognition", "author": ["Z. Ge", "S.R. Sharma", "M.J. Smith"], "venue": "SPIE Defense, Security, and Sensing. International Society for Optics and Photonics, 2012, pp. 840 108\u2013840 108.", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2012}, {"title": "Investigation of silicon auditory models and generalization of linear discriminant analysis for improved speech recognition", "author": ["N. Kumar", "A.G. Andreou"], "venue": "Ph.D. dissertation, Johns Hopkins University, 1997.", "citeRegEx": "18", "shortCiteRegEx": null, "year": 1997}, {"title": "Mispronunciation detection for language learning and speech recognition adaptation", "author": ["Z. Ge"], "venue": "2013.", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2013}, {"title": "Yet another acoustic representation of speech sounds", "author": ["N. Minematsu"], "venue": "ICASSP\u201904. IEEE International Conference on. IEEE.", "citeRegEx": "20", "shortCiteRegEx": null, "year": 0}, {"title": "On a measure of divergence between two multinomial populations", "author": ["A. Bhattacharyya"], "venue": "Sankhy\u0101: The Indian Journal of Statistics (1933- 1960), vol. 7, no. 4, pp. 401\u2013406, 1946.", "citeRegEx": "21", "shortCiteRegEx": null, "year": 1933}, {"title": "The HTK book (for HTK version 3.4)", "author": ["S. Young", "G. Evermann", "M. Gales", "T. Hain", "D. Kershaw", "X.A. Liu", "G. Moore", "J. Odell", "D. Ollason", "D. Povey"], "venue": "2006.", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2006}, {"title": "Speaker verification using adapted gaussian mixture models", "author": ["D.A. Reynolds", "T.F. Quatieri", "R.B. Dunn"], "venue": "Digital signal processing, vol. 10, no. 1, pp. 19\u201341, 2000.", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2000}, {"title": "Maximum likelihood from incomplete data via the em algorithm", "author": ["A. Dempster", "N. Laird", "D. Rubin"], "venue": "Journal of the Royal Statistical Society. Series B (Methodological), pp. 1\u201338, 1977.", "citeRegEx": "24", "shortCiteRegEx": null, "year": 1977}, {"title": "Maximum a posteriori estimation for multivariate gaussian mixture observations of markov chains", "author": ["J.-L. Gauvain", "C.-H. Lee"], "venue": "Speech and audio processing, ieee transactions on, vol. 2, no. 2, pp. 291\u2013298, 1994.", "citeRegEx": "25", "shortCiteRegEx": null, "year": 1994}, {"title": "Arpabet", "author": ["Wikipedia"], "venue": "http://en.wikipedia.org/wiki/Arpabet, August 2011, accessed 2015-07-01.", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2011}, {"title": "Multivariate online kernel density estimation with gaussian kernels", "author": ["M. Kristan", "A. Leonardis", "D. Sko\u010daj"], "venue": "Pattern Recognition, vol. 44, no. 10, pp. 2630\u20132642, 2011.", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2011}, {"title": "Full-covariance UBM and heavy-tailed PLDA in i-vector speaker verification", "author": ["P. Mat\u011bjka", "O. Glembek", "F. Castaldo", "M.J. Alam", "O. Plchot", "P. Kenny", "L. Burget", "J.H. \u010cernocky"], "venue": "Acoustics, Speech and Signal Processing, 2011 IEEE International Conference on. IEEE, 2011.", "citeRegEx": "28", "shortCiteRegEx": null, "year": 2011}, {"title": "Sleep stages classification using neural networks with multi-channel neural data", "author": ["Z. Ge", "Y. Sun"], "venue": "Brain Informatics and Health. Springer, 2015, pp. 306\u2013316.", "citeRegEx": "29", "shortCiteRegEx": null, "year": 2015}], "referenceMentions": [{"referenceID": 0, "context": "achieved accuracy of 32% classifying 23 types of accented English [1], using methods in language identification (LID), such as Maximum Mutual Information (MMI) training and Gaussian tokenization.", "startOffset": 66, "endOffset": 69}, {"referenceID": 0, "context": "recently integrated Universal Background Model (UBM) into Support Vector Machine (SVM) classifier and claimed that it outperformed the results in [1] by 75.", "startOffset": 146, "endOffset": 149}, {"referenceID": 1, "context": "3% relatively [2].", "startOffset": 14, "endOffset": 17}, {"referenceID": 2, "context": "Spanish classification in [3] reported classification rates of 73% and 58.", "startOffset": 26, "endOffset": 29}, {"referenceID": 2, "context": "2% were reported in [3], for 4-, 13- and 23-way classification using naive Bayes.", "startOffset": 20, "endOffset": 23}, {"referenceID": 3, "context": "Since most identifiable accents are presented from the pronunciation of vowels rather than consonants [4], multiple vowel-specific GMMs were computed with features of the vowel components, extracted either from phoneme alignment (in system development) or phoneme recognition (in system test).", "startOffset": 102, "endOffset": 105}, {"referenceID": 4, "context": "This work was initiated during the author\u2019s internship at Interactive Intelligence (ININ) [5], and the algorithm and experiments were later refined for better accuracy and efficiency [6].", "startOffset": 90, "endOffset": 93}, {"referenceID": 5, "context": "This work was initiated during the author\u2019s internship at Interactive Intelligence (ININ) [5], and the algorithm and experiments were later refined for better accuracy and efficiency [6].", "startOffset": 183, "endOffset": 186}, {"referenceID": 6, "context": "Here we use the method described in [7], which detects the silence by thresholding on the short-time energy rate and spectral centroids of the speech.", "startOffset": 36, "endOffset": 39}, {"referenceID": 7, "context": "One can also use either Auditory Toolbox [8] or Voicebox [9] for the same purpose.", "startOffset": 41, "endOffset": 44}, {"referenceID": 8, "context": "One can also use either Auditory Toolbox [8] or Voicebox [9] for the same purpose.", "startOffset": 57, "endOffset": 60}, {"referenceID": 9, "context": "After silence removal, the data of the selected accents were then transformed to 39-dimensional PLP windowed feature frames with 10 millisecond each, using the method in [10].", "startOffset": 170, "endOffset": 174}, {"referenceID": 10, "context": "a feature warping) were applied afterwards using the method in [11].", "startOffset": 63, "endOffset": 67}, {"referenceID": 11, "context": "This is specially useful because the features distribution here can be modeled by Gaussians [12].", "startOffset": 92, "endOffset": 96}, {"referenceID": 12, "context": "It has been applied to many applications, such as face recognition [13] and speech evaluation [14], etc.", "startOffset": 67, "endOffset": 71}, {"referenceID": 13, "context": "It has been applied to many applications, such as face recognition [13] and speech evaluation [14], etc.", "startOffset": 94, "endOffset": 98}, {"referenceID": 14, "context": "It also helps to regularize the data and avoid over-fitting in HLDA which is performed afterwards [15].", "startOffset": 98, "endOffset": 102}, {"referenceID": 15, "context": "It has been also applied to many problems, such as face recognition [16] and speaker recognition [17].", "startOffset": 68, "endOffset": 72}, {"referenceID": 16, "context": "It has been also applied to many problems, such as face recognition [16] and speaker recognition [17].", "startOffset": 97, "endOffset": 101}, {"referenceID": 17, "context": "The first definitions are consistent with the LDA definition used in Kumar\u2019s HLDA work [18] and are used in this work.", "startOffset": 87, "endOffset": 91}, {"referenceID": 18, "context": "An improved version of Kumar\u2019s HLDA algorithm with more flexibility and higher efficiency was developed in MATLAB and used in this work [19].", "startOffset": 136, "endOffset": 140}, {"referenceID": 19, "context": "[20] and Suzuki et al.", "startOffset": 0, "endOffset": 4}, {"referenceID": 3, "context": "[4] demonstrated that, for a particular speaker, the location of 5 fundamental vowels in the feature space of a target language (such as English in this work), is relatively consistent.", "startOffset": 0, "endOffset": 3}, {"referenceID": 19, "context": "3 is a simple demonstration of 5 vowels from both accented and non-accented (standard) languages in the reduced 2-dimensional feature space [20].", "startOffset": 140, "endOffset": 144}, {"referenceID": 20, "context": "By matching the center of the pentagon of the standard and the accented language into the overlapped pentagon in the bottom of Figure 3, the Bhattacharyya distances [21] between each pair of corresponding vowels and their angles can be computed and stored in a vector.", "startOffset": 165, "endOffset": 169}, {"referenceID": 21, "context": "We prepared the dictionary needed for phoneme alignment using HVite in HTK [22], through a procedure including transcription cleaning, word collection, word-to-pronunciation conversion, etc.", "startOffset": 75, "endOffset": 79}, {"referenceID": 22, "context": "The Gaussian Mixture Model-Universal Background Model (GMM-UBM) framework has been sussessfully applied to speech verification and classification systems [23].", "startOffset": 154, "endOffset": 158}, {"referenceID": 23, "context": "(6) The parameters of GMM \u03bb, including weight wi, \u03bci and covariance matrix \u03a3i can be optimized by ExpectationMaximization (EM) algorithm [24].", "startOffset": 137, "endOffset": 141}, {"referenceID": 24, "context": "They can be derived from the Maximum a posteriori (MAP) estimation equations for a GMM using constraints on the prior distribution described in [25].", "startOffset": 144, "endOffset": 148}, {"referenceID": 25, "context": "III, the same concept was generalized and all 15 vowels in Arpabet [26] were used, which are listed in Table III.", "startOffset": 67, "endOffset": 71}, {"referenceID": 26, "context": "8 shows the Hellinger distances [27] computed between any 2 GMM distributions of 7 different accent types for the vowel aa.", "startOffset": 32, "endOffset": 36}, {"referenceID": 0, "context": "With several feature optimization techniques and phonetic vowel information, the accuracy obtained from accented speech as short as 20 seconds, is competitive compared with the state of the art in [1], [2] and [3].", "startOffset": 197, "endOffset": 200}, {"referenceID": 1, "context": "With several feature optimization techniques and phonetic vowel information, the accuracy obtained from accented speech as short as 20 seconds, is competitive compared with the state of the art in [1], [2] and [3].", "startOffset": 202, "endOffset": 205}, {"referenceID": 2, "context": "With several feature optimization techniques and phonetic vowel information, the accuracy obtained from accented speech as short as 20 seconds, is competitive compared with the state of the art in [1], [2] and [3].", "startOffset": 210, "endOffset": 213}, {"referenceID": 27, "context": "In the future, more recent classification methods such as i-vector (eigenvoice component) [28], or neural network classifier [29] can be explored, used or combined with the current methods.", "startOffset": 90, "endOffset": 94}, {"referenceID": 28, "context": "In the future, more recent classification methods such as i-vector (eigenvoice component) [28], or neural network classifier [29] can be explored, used or combined with the current methods.", "startOffset": 125, "endOffset": 129}], "year": 2016, "abstractText": "Researches have shown accent classification can be improved by integrating semantic information into pure acoustic approach. In this work, we combine phonetic knowledge, such as vowels, with enhanced acoustic features to build an improved accent classification system. The classifier is based on Gaussian Mixture Model-Universal Background Model (GMM-UBM), with normalized Perceptual Linear Predictive (PLP) features. The features are further optimized by Principle Component Analysis (PCA) and Hetroscedastic Linear Discriminant Analysis (HLDA). Using 7 major types of accented speech from the Foreign Accented English (FAE) corpus, the system achieves classification accuracy 54% with input test data as short as 20 seconds, which is competitive to the state of the art in this field.", "creator": "LaTeX with hyperref package"}}}