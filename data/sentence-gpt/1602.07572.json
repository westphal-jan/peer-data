{"id": "1602.07572", "review": {"conference": "HLT-NAACL", "VERSION": "v1", "DATE_OF_SUBMISSION": "24-Feb-2016", "title": "Ultradense Word Embeddings by Orthogonal Transformation", "abstract": "Embeddings are generic representations that are useful for many NLP tasks. In this paper, we introduce DENSIFIER, a method that learns an orthogonal transformation of the embedding space that focuses the information relevant for a task in an ultradense subspace of a dimensionality that is smaller by a factor of 100 than the original space. We show that ultradense embeddings generated by DENSIFIER reach state of the art on a lexicon creation task in which words are annotated with three types of lexical information - sentiment, concreteness and frequency.\n\n\n\n\n\nWe describe DENSIFIER using two basic features:\n- DENSIFIER generates an orthogonal transformation of the embedding space where words are annotated with an array of words\n- DENSIFIER generates an orthogonal transformation of the embedding space where words are annotated with an array of words. In DENSIFIER, this is just one of four key functions that allow DENSIFIER to generate a dynamic type of DENSIFIER and then use DENSIFIER as the output from the same dimension of the DENSIFIER input. This is the second step in our work, as we will explain.\n\nThe DENSIFIER input consists of the four components: a set of words and a set of words and a set of words.\n- DENSIFIER generates an orthogonal transformation of the embedding space where words are annotated with an array of words\n- DENSIFIER generates an orthogonal transformation of the embedding space where words are annotated with an array of words. This is the second step in our work, as we will explain.\nThe DENSIFIER input consists of the four components: a set of words and a set of words.\n- DENSIFIER generates an orthogonal transformation of the embedding space where words are annotated with an array of words. This is the second step in our work, as we will explain.\nThis is the second step in our work, as we will explain. A common task in DENSIFIER, the one that has to be performed in tandem (as shown in the figure below) is to generate an orthogonal transformation of the embedding space where words are annotated with an array of words. In DENSIFIER, this is the second step in our work, as we will explain.\nThe DENSIFIER", "histories": [["v1", "Wed, 24 Feb 2016 16:06:25 GMT  (54kb,D)", "http://arxiv.org/abs/1602.07572v1", null], ["v2", "Sun, 8 May 2016 08:50:11 GMT  (53kb,D)", "http://arxiv.org/abs/1602.07572v2", null]], "reviews": [], "SUBJECTS": "cs.CL", "authors": ["sascha rothe", "sebastian ebert", "hinrich sch\u00fctze"], "accepted": true, "id": "1602.07572"}, "pdf": {"name": "1602.07572.pdf", "metadata": {"source": "CRF", "title": "Ultradense Word Embeddings by Orthogonal Transformation", "authors": ["Sascha Rothe", "Sebastian Ebert"], "emails": ["sascha@cis.lmu.de", "ebert@cis.lmu.de"], "sections": [{"heading": null, "text": "Embeddings are generic representations that are useful for many NLP tasks. In this paper, we introduce DENSIFIER, a method that learns an orthogonal transformation of the embedding space that focuses the information relevant for a task in an ultradense subspace of a dimensionality that is smaller by a factor of 100 than the original space. We show that ultradense embeddings generated by DENSIFIER reach state of the art on a lexicon creation task in which words are annotated with three types of lexical information \u2013 sentiment, concreteness and frequency. On the SemEval2015 10B sentiment analysis task we show that no information is lost when the ultradense subspace is used, but training is an order of magnitude more efficient due to the compactness of the ultradense space."}, {"heading": "1 Introduction", "text": "Embeddings are useful for many tasks, including word similarity (e.g., Pennington et al. (2014a)), named entity recognition (NER) (e.g., Collobert et al. (2011)) and sentiment analysis (e.g., Kim (2014), Kalchbrenner et al. (2014), Severyn and Moschitti (2015)). Embeddings are generic representations, containing different types of information about a word. Statistical models can be trained to make best use of these generic representations for a specific application like NER or sentiment analysis.\nOur hypothesis in this paper is that the information useful for any given task is contained in an ultradense subspace Ru. We propose the new method\nDENSIFIER to identify Ru. Given a set of word embeddings, DENSIFIER learns an orthogonal transformation of the original space Ro on a task-specific training set. Orthogonal transformations \u201creorder\u201d the space without adding or removing information and preserve the bilinear form, i.e., Euclidean distance and cosine. The transformed embeddings concentrate all information relevant for the task in Ru. The benefits of Ru compared to Ro are (i) highquality and (ii) efficient representations. (i) DENSIFIER moves non-task-related information outside of Ru, i.e., into the orthogonal complement of Ru. As a result, Ru provides higher-quality representations for the task than Ro; e.g., noise that could result in overfitting is reduced in Ru compared to Ro. (ii) Ru has a dimensionality smaller by a factor of 100 in our experiments. As a result, training statistical models on these embeddings is much faster. These models also have many fewer parameters, thus again helping to prevent overfitting, especially for complex, deep neural networks.\nWe show the benefits of ultradense representations in two text polarity classification tasks (SemEval2015 Task 10B, Czech movie reviews).\nIn the most extreme form, ultradense representations \u2013 i.e., Ru \u2013 have a single dimension. We exploit this for creating lexicons in which words are annotated with lexical information, e.g., with sentiment. Specifically, we create high-coverage lexicons with up to 3 million words (i) for three lexical properties: for sentiment, concreteness and frequency; (ii) for five languages: Czech, English, French, German and Spanish; (iii) for two domains, Twitter and News, in a domain adaptation setup.\nar X\niv :1\n60 2.\n07 57\n2v 1\n[ cs\n.C L\n] 2\n4 Fe\nb 20\nThe main advantages of this method of lexicon creation are: (i) We need a training lexicon of only a few hundred words, thus making our method effective for new domains and languages and requiring only a minimal manual annotation effort. (ii) The method is applicable to any set of embeddings, including phrase and sentence embeddings. Assuming the availability of a small hand-labeled lexicon, DENSIFIER automatically creates a domain dependent lexicon based on a set of embeddings learned on a large corpus of the domain. (iii) While the input lexicon is discrete \u2013 e.g., positive (+1) and negative (-1) sentiment \u2013 the output lexicon is continuous and this more fine-grained assessment is potentially more informative than a simple binary distinction.\nWe show that lexicons created by DENSIFIER beat the state of the art on SemEval2015 Task 10E (determining association strength).\nOne of our goals is to make embeddings more interpretable. The work on sentiment, concreteness and frequency we present in this paper is a first step towards a general decomposition of embedding spaces into meaningful, dense subspaces. This would lead to cleaner and more easily interpretable representations \u2013 as well as representations that are more effective and efficient."}, {"heading": "2 Model", "text": "Let Q \u2208 Rd\u00d7d be an orthogonal matrix that transforms the original word embedding space into a space in which certain types of information are represented by a small number of dimensions. Concretely, we learn Q such that the dimensions Ds \u2282 {1, . . . , d} of the resulting space correspond to a word\u2019s sentiment information and the {1, . . . , d}\\Ds remaining dimensions correspond to non-sentiment information. Analogously, the sets of dimensions Dc and Df correspond to a word\u2019s concreteness information and frequency information, respectively.1 If wn \u2208 Rd is the original embedding of word n, the transformed representation is Qwn. We use \u2217 as a placeholder for s, c and f and call d\u2217 = |D\u2217| the dimensionality of the ultradense subspace of property \u2217. For each ultradense subspace,\n1In this paper, we assume that these properties do not correlate and therefore the ultradense subspaces do not overlap, e.g., Ds\u2229Dc = \u2205. However, this might not be true for other settings, e.g., sentiment and semantic information.\nwe create P \u2217 \u2208 Rd\u2217\u00d7d, an identity matrix for the dimensions in D\u2217 \u2282 {1, . . . , d}. Thus, the ultradense representation u\u2217n \u2208 Rd \u2217 of wn is defined as:\nu\u2217n := P \u2217Qwn"}, {"heading": "2.1 Separating Words of Different Groups", "text": "We assume we have a lexicon resource L in which each word w is annotated for a certain property as either L(w) = +1 (positive, concrete, frequent) or L(w) = \u22121 (negative, abstract, infrequent). Let M\u2217diff be a set of word index pairs (m,n) for which L(wm) 6= L(wn) holds. We want to maximize:\u2211\n(m,n)\u2208M\u2217diff\n\u2016u\u2217n \u2212 u\u2217m\u2016 (1)\nThus, our objective is given by:\nargmax Q \u2211 (m,n)\u2208M\u2217diff \u2016P \u2217Q(wn \u2212 wm)\u2016 (2)\nor, equivalently, by: argmin\nQ\n\u2211 (m,n)\u2208M\u2217diff \u2212\u2016P \u2217Q(wn \u2212 wm)\u2016\nsubject to Q being an orthogonal matrix."}, {"heading": "2.2 Aligning Words of the Same Group", "text": "Another goal is to minimize the distance of two words of the same group. Let M\u2217same be a set of word index pairs (m,n) for which L(wm) = L(wn) holds. In contrast to Eq. 2, we now want to minimize each distance. Thus, the objective is given by:\nargmin Q \u2211 (m,n)\u2208M\u2217same \u2016P \u2217Q(wn \u2212 wm)\u2016 (3)\nsubject to Q being an orthogonal matrix. The intuition behind the two objectives is graphically depicted in Figure 1."}, {"heading": "2.3 Training", "text": "We combine the two objectives in Eqs. 2/3 for each subspace, i.e., for sentiment, concreteness and frequency, and weight them with \u03b1\u2217 and 1\u2212\u03b1\u2217. Hence, there is one hyperparameter \u03b1\u2217 for each subspace. We then perform stochastic gradient descent (SGD). Batch size is 100 and starting learning rate is 5, multiplied by .99 in each iteration."}, {"heading": "2.4 Orthogonalization", "text": "Each step of SGD updates Q. The updated matrix Q\u2032 is in general no longer orthogonal. We therefore reorthogonalize Q\u2032 in each step based on singular value decomposition: Q\u2032 = USV T where S is a diagonal matrix, and U and V are orthogonal matrices. The matrixQ := UV T is the nearest orthogonal matrix to Q\u2032 in both the 2-norm and the Frobenius norm (Fan and Hoffman, 1955).\nSGD for this problem is sensitive to the learning rate. If the learning rate is too large, a large jump results and the reorthogonalized matrix Q basically is a random new point in the parameter space. If the learning rate is too small, then learning can take long. We found that our training regime of starting at a high learning rate (5) and multiplying by .99 in each iteration is effective. Typically, the cost initially stays about constant (random jumps in parameter space), then cost steeply declines in a small number of about 50 iterations (sweet spot); the curve flattens after that. Training Q took less than 5 minutes per experiment for all experiments in this paper."}, {"heading": "3 Lexicon Creation", "text": "For lexicon creation, the input is a set of embeddings and a lexicon resource L, in which words are annotated for a lexical property such as sentiment, concreteness or frequency. DENSIFIER is then trained to produce a one-dimensional ultradense subspace. The output is an output lexicon. It consists of all words covered by the embedding set, each associ-\nated with its one-dimensional ultradense subspace representation (which is simply a real number), an indicator of the word\u2019s strength for that property.\nThe embeddings and lexicon resources used in this paper cover five languages and three domains (Table 1). The Google News embeddings for English2 and the FrWac embeddings for French3 are publicly available. We use word2vec to train 400- dimensional embeddings for English on a 2013 Twitter corpus of size 5.4 109. For Czech, German and Spanish, we train embeddings on web data of sizes 3.3, 8.0 and 3.8 109, respectively. We use the following lexicon resources for sentiment: SubLex 1.0 (Veselovska\u0301 and Bojar, 2013) for Czech; WHM for English [the combination of MPQA (Wilson et al., 2005), Opinion Lexicon (Hu and Liu, 2004) and NRC Emotion lexicons (Mohammad and Turney, 2013)]; FEEL (Abdaoui et al., 2014) for French; German Polarity Clues (Waltinger, 2010) for German; and the sentiment lexicon of Pe\u0301rez-Rosas et al. (2012) for Spanish. For concreteness, we use BWK, a lexicon of 40,000 English words (Brysbaert et al., 2014). For frequency, we exploit the fact that word2vec stores words in frequency order; thus, the ranking provided by word2vec is our lexicon resource for frequency.\nFor a resource/embedding-set pair (L,E), we intersect the vocabulary of L with the top 80,000 words of E to filter out noisy, infrequent words that\n2https://code.google.com/p/word2vec/ 3http://fauconnier.github.io/\ntend to have low quality embeddings. For the sentiment and concreteness resources, L\u2217(w) \u2208 {\u22121, 1} for all words w covered. We create a resource Lf for frequency by setting Lf (w) = 1 for the 2000 most frequent words and Lf (w) = \u22121 for words at ranks 20000-22000. 1000 words randomly selected from the 5000 most frequent are the test set.4 We designate three sets of dimensions Ds, Dc and Df to represent sentiment, concreteness and frequency, respectively, and arbitrarily set Dc = {11} for English and Dc = \u2205 for the other languages since we do not have concreteness resources for them, Ds = {1} and Df = {21}. Referring to the lines in Table 1, we then learn six orthogonal transformation matrices Q: for CZ-web (1), DE-web (2), ES-web (3), FR-web (4, 9), ENtwitter (5) and EN-news (6, 7, 8)."}, {"heading": "4 Evaluation", "text": ""}, {"heading": "4.1 Top-Ranked Words", "text": "Table 2 shows the top 10 positive/negative words (i.e., most extreme values on dimension Ds) when we apply the transformation to the corpora Entwitter, EN-news and DE-web and the top 10 concrete/abstract words (i.e., most extreme values on dimension Dc) for EN-news. For EN-twitter (leftmost double column), the selected words look promising: they contain highly domain-specific words such as hashtags (e.g., #happy). This is surprising because\n4The main result of the frequency experiment below is that \u03c4 is low even in a setup that is optimistic due to train/test overlap; presumably it would be even lower without overlap. For the camera-ready, we would create a disjoint test set.\nthere is not a single hashtag in the lexicon resource WHM that DENSIFIER was trained on. Results for the other three double columns show likewise extreme examples for the corresponding property and language. This initial evaluation indicates that our method effectively learns high quality lexicons for new domains. Figure 2 depicts values for selected words for the three properties. Illustrative examples are \u201cbrother\u201d / \u201cbrotherhood\u201d for concreteness and \u201chate\u201d / \u201clove\u201d for sentiment."}, {"heading": "4.2 Quality of Predictions", "text": "Table 1 presents experimental results. In each case, we split the resource into train/test, except for Twitter where we used the trial data of SemEval2015 Task 10E for test. We train DENSIFIER on train and compute Kendall\u2019s \u03c4 on test. The size of the lexicon resource has no big effect; e.g., results for Spanish (small resource; line 3) and French (large resource; line 4) are about the same. See Section 5 for a more detailed analysis of the effect of resource size.\nThe quality of the output lexicon depends strongly on the quality of the underlying word embeddings; e.g., results for French (small embedding training corpus; line 4) are worse than results for English (large embedding training corpus; line 6) even though the lexicon resources have comparable size.\nIn contrast to sentiment/concreteness, \u03c4 values for frequency are low (lines 8-9). For the other three languages, \u03c4 \u2208 [.34, .46] for frequency (not shown). This suggests that word embeddings represent sentiment and concreteness much better than frequency.\nWe make the nine output lexicons in Table 1 \u2013 each consisting of a list of words annotated with pre-\ndicted strength on one of three properties \u2013 available for download (see supplementary)."}, {"heading": "4.3 Determining Association Strength", "text": "We also evaluate lexicon creation on SemEval2015 Task 10E. As before, the task is to predict the sentiment score of words/phrases. We use the trial data of the task to tune the hyperparameter, \u03b1s = .4. Out-of-vocabulary words were predicted as neutral (7/1315). Table 3 shows that the lexicon computed by DENSIFIER (line 5, Table 1) has a \u03c4 of .654 (line 6, column all), significantly better than all other systems, including the winner of SemEval 2015 (\u03c4 = .626, line 1). DENSIFIER also beats Sentiment140 (Mohammad et al., 2013), a widely used semi-automatic sentiment lexicon. The last column is \u03c4 on the intersection of DENSIFIER and Sentiment140. It shows that DENSIFIER again performs significantly better than Sentiment140."}, {"heading": "4.4 Text Polarity Classification", "text": "We now show that ultradense embeddings decrease model training times without any noticeable decrease in performance compared to the original em-\nbeddings. We evaluate on SemEval2015 Task 10B, classification of Twitter tweets as positive, negative or neutral. We reimplement the linguisticallyinformed convolutional neural network (lingCNN) of Ebert et al. (2015) that has close to state-of-the-art performance on the task.5 We initialize the first layer of lingCNN, the embedding layer, in three different ways: (i) 400-dimensional Twitter embeddings (Section 3); (ii) 40-dimensional ultradense embeddings derived from (i); (iii) 4-dimensional ultradense embeddings derived from (i). The objective weighting is \u03b1s = .4, optimized on the development set.\nThe embedding layer converts a sentence into a matrix of word embeddings. We also add linguistic features for words, such as sentiment lexicon scores. The combination of embeddings and linguistic features is the input for a convolution layer with filters spanning 2-5 words (100 filters each). This is followed by a max pooling layer, a rectifier nonlinearity (Nair and Hinton, 2010) and a fully connected softmax layer predicting the final label. The model is trained with SGD using AdaGrad (Duchi et al., 2011) and `2 regularization (\u03bb = 5 \u2217 10\u22125). Learning rate is 0.01. Mini-batch size is 100.\nWe follow the official guidelines and use the SemEval2013 training and development sets as training set, the SemEval2013 test set as development set and the SemEval2015 test set to report final scores (Nakov et al., 2013; Rosenthal et al., 2015). We report macro F1 of positive and negative classes (the official SemEval evaluation metric) and accuracy over the three classes. Table 4 shows that 40- dimensional ultradense embeddings perform almost as well as the full 400-dimensional embeddings (no significant difference according to sign test). Train-\n5We do not use sentence-based features to focus on the evaluation of the embeddings.\ning time is shorter by a factor of 21 (85/4 examples/second). The 4-dimensional ultradense embeddings lead to only a small loss of 1.5% even though the size of the embeddings is smaller by a factor of 100 (again not a significant drop). Training time is shorter by a factor of 44 (178/4).\nWe perform the same experiment on CSFD, a Czech movie review dataset (Habernal et al., 2013), to show the benefits of ultradense embeddings for a low-resource language where only one rather small lexicon is available. As original word embeddings we train new 400 dimensional embeddings on a large Twitter corpus (3.3 109 tokens). We use DENSIFIER to create 40 and 4 dimensional embeddings out of these embeddings and SubLex 1.0 (Veselovska\u0301 and Bojar, 2013). Word polarity features are also taken from SubLex. A simple binary negation indicator is implemented by searching for all tokens beginning with \u201cne\u201d. Since that includes superlative forms having the prefix \u201cnej\u201d, we remove them with the exception of common negated words, such as \u201cnejsi\u201d (engl. \u201cyou are not\u201d). We randomly split the 91k dataset instances into 90% train and 10% test and report accuracy and macro F1 score over all three classes.\nTable 4 shows that what we found for English is also true for Czech. There is only a small performance drop when using ultradense embeddings (not significant for 40 dimensional embeddings) while the speed improvement is substantial."}, {"heading": "5 Parameter Analysis", "text": "In this section, we analyze two parameters: size of ultradense subspace and size of lexicon resource. We leave an evaluation of another parameter, the size of the embedding training corpus, for future work, but empirical results suggest that this corpus should ideally have a size of several billion tokens.\nSize of Subspace. With the exception of the two\ntext polarity classification experiments, all our subspaces have dimensionality d\u2217 = 1. The question arises: does a one-dimensional space perhaps have too low a capacity to encode all relevant information and could we further improve our results by increasing the dimensionality of the subspace to values d\u2217 > 1? The lexicon resources that we train and test on are all binary; thus, if we use values d\u2217 > 1, then we need to map the subspace embeddings to a one-dimensional scale for evaluation. We do this by training, on the train part of the resource, a linear transformation from the ultradense subspace to the one-dimensional scale (e.g., to the sentiment scale).\nFigure 3 compares different values of ds for three different types of subspaces in this setup, i.e., the setup in which the subspace representations are mapped via linear transformation to a onedimensional sentiment value: (i) Random: we take the first ds dimensions of the original embeddings; (ii) PCA: we compute a PCA and take the first ds principal components; (iii) Ultradense subspace of dimensionality ds. We use the word embeddings and lexicon resources of line 6 in Table 1. For random, the performance starts dropping when the subspace is smaller than 200 dimensions. For PCA, the performance is relatively stable until the subspace becomes smaller than 100. In contrast, ultradense subspaces have almost identical performance for all values of ds, even for ds = 1. This suggests that a single dimension is sufficient to encode all sentiment information needed for sentiment lexicon creation. However, for other sentiment tasks more dimensions may be needed, e.g., for modeling different emotional dimensions of polarity: fear, sadness, anger etc.\nAn alternative approach to creating a lowdimensional space is to simply train lowdimensional word2vec embeddings. The following experiment suggests that this does not work very well. We used word2vec to train 60-dimensional twitter embeddings with the same settings as on line 5 in Table 1. While the correlation for 400-dimensional embeddings shown in Table 1 is .661, the correlation of 60-dimensional embeddings is only .568. Thus, even though we show that the information in 400-dimensional embeddings that is relevant for sentiment can be condensed into a single dimension, hundreds of dimensions seem to\nbe needed if we use word2vec to collect sentiment information. If we run word2vec with a small dimensionality, only a subset of available sentiment information is \u201charvested\u201d from the corpus.\nNext, we analyze what size of training resource is required to learn a good transformation Q. Labeled resources that cover many words are not always available or might suffer from lack of quality. We use the settings of lines 6 (sentiment) and 7 (concreteness) in Table 1. Figure 3 shows that a small training resource of 300 entries is sufficient for high performance. This suggests that DENSIFIER can create a high quality output lexicon for a new language by hand-labeling only 300 words; and that a small, high-quality resource may be preferable to a large lower-quality resource (semi-automatic or out of domain). To provide further evidence, we train DENSIFIER on only the trial data of SemEval2015 task 10E. To convert the continuous trial data to binary \u22121 / 1 labels, we discard all words with sentiment values between \u22120.5 and 0.5 and round the remaining values, giving us 39 positive and 38 negative training words. The resulting lexicon has \u03c4 = .627 (Table 3, line 8).6 This is worse than \u03c4 = .654 (line 6) for the setup in which we used several large resources, but still better than all previous work. This indicates that DENSIFIER is especially suited for languages or\n6 Here, we tune \u03b1s on train (not on trial as before). This seems to work due to the different objectives for training (maximize/minimize difference) and development (correlation).\ndomains for which little training data is available."}, {"heading": "6 Related Work", "text": "To the best of our knowledge, this paper is the first to train an orthogonal transformation to reorder word embedding dimensions into ultradense subspaces. However, there is much prior work on postprocessing word embeddings. Faruqui et al. (2015) perform postprocessing based on a semantic lexicon with the goal of fine-tuning word embeddings. Their transformation is not orthogonal and therefore does not preserve distances. They show that their approach optimizes word embeddings for a given application, i.e., word similarity, but also that it worsens them for other applications like detecting syntactic relations. Faruqui et al. (2015)\u2019s approach also does not have the benefit of ultradense embeddings, in particular the benefit of increased efficiency. In a tensor framework, Rothe and Schu\u0308tze (2015) transform the word embeddings of a language to sense (synset) embeddings. In their work, all embeddings live in the same space whereas we explicitly want to change the embedding space to create ultradense embeddings with several desirable properties.\nXing et al. (2015) restricted the work of Mikolov et al. (2013) to an orthogonal transformation to ensure that normalized embeddings stay normalized. This transformation is learned between two embedding spaces of different languages to exploit similarities. They normalized word embeddings in a first\nstep, something that did not improve our results. Sentiment lexicons are often created semiautomatically, e.g., by extending a manually labeled seed set of sentiment words or by adding for each word its synonyms/antonyms. Another approach is to start with a seed set of manually labeled sentiment words and search for frequently cooccurring words (Turney, 2002; Kiritchenko et al., 2014). Heerschop et al. (2011) used WordNet together with a PageRank-based algorithm to propagate the sentiment of the seed set to unknown words. Scheible (2010) presented a semi-automatic approach based on machine translation of sentiment lexicons. The winning system of SemEval2015 10E (Amir et al., 2015) was based on structured skip-gram embeddings with 600 dimensions and support vector regression with RBF kernels. Hamdan et al. (2015), the second ranked team, used the average of six sentiment lexicons as a final sentiment score, a method that cannot be applied to low resource languages. We showed that the lexicons created by DENSIFIER achieve better performance than other semi-automatically created lexicons.\nTang et al. (2014b) train sentiment specific embeddings by extending Collobert & Weston\u2019s model and Tang et al. (2014a)\u2019s skip-gram model. The first model automatically labels tweets as positive/negative based on emoticons, a process that cannot be easily transferred to other domains like news. The second uses the Urban Dictionary to expand a small list of 350 sentiment seeds. In our work, we showed that a training resource of about the same size is sufficient without an additional dictionary. DENSIFIER differs from this work in that it does not need a text corpus, but can transform existing, publicly available word embeddings. DENSIFIER is independent of the embedding learning algorithm and therefore extensible to other word embedding mod-\nels like GloVe (Pennington et al., 2014b), to phrase embeddings (Yu and Dredze, 2015) and even to sentence embeddings (Kiros et al., 2015)."}, {"heading": "7 Conclusion", "text": "We have introduced DENSIFIER, a method that is trained to focus embeddings used for an application to an ultradense subspace that contains the information relevant for the application. In experiments on SemEval, we demonstrate two benefits of the ultradense subspace. (i) Information is preserved even if we focus on a subspace that is smaller by a factor of 100 than the original space. This means that unnecessary noisy information is removed from the embeddings and robust learning without overfitting is better supported. (ii) Since the subspace is 100 times smaller, models that use the embeddings as their input representation can be trained more efficiently and have a much smaller number of parameters. The subspace can be learned with just 80\u2212300 training examples, achieving state-of-the-art results on lexicon creation.\nWe have shown in this paper that up to three orthogonal ultradense subspaces can be created. In future work, we will explore the possibility of factoring all information present in an embedding into a dozen or so orthogonal subspaces. This factorization would not change the information embeddings contain, but it would make them more compact for any given application, more meaningful and more interpretable.\nThe nine large DENSIFIER lexicons shown in Table 1 are publicly available under http://www. cis.lmu.de/\u02dcsascha/Ultradense/."}], "references": [{"title": "Sandra Bringay", "author": ["Amine Abdaoui", "J\u00e9r\u00f4me Az\u00e9"], "venue": "and Pascal Poncelet.", "citeRegEx": "Abdaoui et al.2014", "shortCiteRegEx": null, "year": 2014}, {"title": "Silva", "author": ["Silvio Amir", "Ram\u00f3n Astudillo", "Wang Ling", "Bruno Martins", "Mario J"], "venue": "and Isabel Trancoso.", "citeRegEx": "Amir et al.2015", "shortCiteRegEx": null, "year": 2015}, {"title": "Amy Beth Warriner", "author": ["Marc Brysbaert"], "venue": "and Victor Kuperman.", "citeRegEx": "Brysbaert et al.2014", "shortCiteRegEx": null, "year": 2014}, {"title": "2011", "author": ["Ronan Collobert", "Jason Weston", "L\u00e9on Bottou", "Michael Karlen", "Koray Kavukcuoglu", "Pavel Kuksa"], "venue": "Natural language processing (almost) from scratch. J. Mach. Learn. Res., 12:2493\u2013", "citeRegEx": "Collobert et al.2011", "shortCiteRegEx": null, "year": 2537}, {"title": "Elad Hazan", "author": ["John C. Duchi"], "venue": "and Yoram Singer.", "citeRegEx": "Duchi et al.2011", "shortCiteRegEx": null, "year": 2011}, {"title": "Ngoc Thang Vu", "author": ["Sebastian Ebert"], "venue": "and Hinrich Sch\u00fctze.", "citeRegEx": "Ebert et al.2015", "shortCiteRegEx": null, "year": 2015}, {"title": "Some metric inequalities in the space of matrices", "author": ["Fan", "Hoffman1955] Ky Fan", "Alan J Hoffman"], "venue": null, "citeRegEx": "Fan et al\\.,? \\Q1955\\E", "shortCiteRegEx": "Fan et al\\.", "year": 1955}, {"title": "and Noah A", "author": ["Manaal Faruqui", "Jesse Dodge", "Sujay Kumar Jauhar", "Chris Dyer", "Eduard Hovy"], "venue": "Smith.", "citeRegEx": "Faruqui et al.2015", "shortCiteRegEx": null, "year": 2015}, {"title": "Tom\u00e1\u0161 Pt\u00e1\u010dek", "author": ["Ivan Habernal"], "venue": "and Josef Steinberger.", "citeRegEx": "Habernal et al.2013", "shortCiteRegEx": null, "year": 2013}, {"title": "Patrice Bellot", "author": ["Hussam Hamdan"], "venue": "and Frederic Bechet.", "citeRegEx": "Hamdan et al.2015", "shortCiteRegEx": null, "year": 2015}, {"title": "Alexander Hogenboom", "author": ["Bas Heerschop"], "venue": "and Flavius Frasincar.", "citeRegEx": "Heerschop et al.2011", "shortCiteRegEx": null, "year": 2011}, {"title": "Mining and Summarizing Customer Reviews", "author": ["Hu", "Liu2004] Minqing Hu", "Bing Liu"], "venue": null, "citeRegEx": "Hu et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Hu et al\\.", "year": 2004}, {"title": "Edward Grefenstette", "author": ["Nal Kalchbrenner"], "venue": "and Phil Blunsom.", "citeRegEx": "Kalchbrenner et al.2014", "shortCiteRegEx": null, "year": 2014}, {"title": "Convolutional Neural Networks for Sentence Classification", "author": ["Yoon Kim"], "venue": "In Proceedings of EMNLP", "citeRegEx": "Kim.,? \\Q2014\\E", "shortCiteRegEx": "Kim.", "year": 2014}, {"title": "Xiaodan Zhu", "author": ["Svetlana Kiritchenko"], "venue": "and Saif M Mohammad.", "citeRegEx": "Kiritchenko et al.2014", "shortCiteRegEx": null, "year": 2014}, {"title": "Antonio Torralba", "author": ["Ryan Kiros", "Yukun Zhu", "Ruslan R Salakhutdinov", "Richard Zemel", "Raquel Urtasun"], "venue": "and Sanja Fidler.", "citeRegEx": "Kiros et al.2015", "shortCiteRegEx": null, "year": 2015}, {"title": "Quoc V Le", "author": ["Tomas Mikolov"], "venue": "and Ilya Sutskever.", "citeRegEx": "Mikolov et al.2013", "shortCiteRegEx": null, "year": 2013}, {"title": "Mohammad and Peter D", "author": ["M Saif"], "venue": "Turney.", "citeRegEx": "Mohammad and Turney2013", "shortCiteRegEx": null, "year": 2013}, {"title": "Svetlana Kiritchenko", "author": ["Saif M. Mohammad"], "venue": "and Xiaodan Zhu.", "citeRegEx": "Mohammad et al.2013", "shortCiteRegEx": null, "year": 2013}, {"title": "Rectified Linear Units Improve Restricted Boltzmann Machines", "author": ["Nair", "Hinton2010] Vinod Nair", "Geoffrey E. Hinton"], "venue": "In Proceedings of ICML", "citeRegEx": "Nair et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Nair et al\\.", "year": 2010}, {"title": "Alan Ritter", "author": ["Preslav Nakov", "Sara Rosenthal", "Zornitsa Kozareva", "Veselin Stoyanov"], "venue": "and Theresa Wilson.", "citeRegEx": "Nakov et al.2013", "shortCiteRegEx": null, "year": 2013}, {"title": "Clac-sentipipe: Semeval2015 subtasks 10 b,e, and task", "author": ["\u00d6zdemir", "Bergler2015] Canberk \u00d6zdemir", "Sabine Bergler"], "venue": "Proceedings of SemEval", "citeRegEx": "\u00d6zdemir et al\\.,? \\Q2015\\E", "shortCiteRegEx": "\u00d6zdemir et al\\.", "year": 2015}, {"title": "GloVe: Global Vectors for Word Representation", "author": ["Richard Socher", "Christopher D. Manning"], "venue": "In Proceedings of EMNLP", "citeRegEx": "Pennington et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Pennington et al\\.", "year": 2014}, {"title": "Glove: Global vectors for word representation", "author": ["Richard Socher", "Christopher D. Manning"], "venue": "In Proceedings of EMNLP", "citeRegEx": "Pennington et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Pennington et al\\.", "year": 2014}, {"title": "Carmen Banea", "author": ["Ver\u00f3nica P\u00e9rez-Rosas"], "venue": "and Rada Mihalcea.", "citeRegEx": "P\u00e9rez.Rosas et al.2012", "shortCiteRegEx": null, "year": 2012}, {"title": "Natalie Dykes", "author": ["Nataliia Plotnikova", "Micha Kohl", "Kevin Volkert", "Stefan Evert", "Andreas Lerner"], "venue": "and Heiko Ermer.", "citeRegEx": "Plotnikova et al.2015", "shortCiteRegEx": null, "year": 2015}, {"title": "Alan Ritter", "author": ["Sara Rosenthal", "Preslav Nakov", "Svetlana Kiritchenko", "Saif M. Mohammad"], "venue": "and Veselin Stoyanov.", "citeRegEx": "Rosenthal et al.2015", "shortCiteRegEx": null, "year": 2015}, {"title": "Autoextend: Extending word embeddings to embeddings for synsets and lexemes", "author": ["Rothe", "Sch\u00fctze2015] Sascha Rothe", "Hinrich Sch\u00fctze"], "venue": "In Proceedings of ACL", "citeRegEx": "Rothe et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Rothe et al\\.", "year": 2015}, {"title": "Sentiment translation through lexicon induction", "author": ["Christian Scheible"], "venue": "ACL", "citeRegEx": "Scheible.,? \\Q2010\\E", "shortCiteRegEx": "Scheible.", "year": 2010}, {"title": "UNITN: Training Deep Convolutional Neural Network for Twitter Sentiment Classification", "author": ["Severyn", "Moschitti2015] Aliaksei Severyn", "Alessandro Moschitti"], "venue": "In Proceedings of SemEval", "citeRegEx": "Severyn et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Severyn et al\\.", "year": 2015}, {"title": "Building large-scale twitter-specific sentiment lexicon : A representation learning approach", "author": ["Tang et al.2014a] Duyu Tang", "Furu Wei", "Bing Qin", "Ming Zhou", "Ting Liu"], "venue": "Proceedings of COLING", "citeRegEx": "Tang et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Tang et al\\.", "year": 2014}, {"title": "Learning Sentiment-Specific Word Embedding for Twitter Sentiment Classification", "author": ["Tang et al.2014b] Duyu Tang", "Furu Wei", "Nan Yang", "Ming Zhou", "Ting Liu", "Bing Qin"], "venue": "Proceedings of ACL", "citeRegEx": "Tang et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Tang et al\\.", "year": 2014}, {"title": "Thumbs Up or Thumbs Down? Semantic Orientation Applied to Unsupervised Classification of Reviews", "author": ["Peter D. Turney"], "venue": "In Proceedings of ACL", "citeRegEx": "Turney.,? \\Q2002\\E", "shortCiteRegEx": "Turney.", "year": 2002}, {"title": "Janyce Wiebe", "author": ["Theresa Wilson"], "venue": "and Paul Hoffmann.", "citeRegEx": "Wilson et al.2005", "shortCiteRegEx": null, "year": 2005}, {"title": "Chao Liu", "author": ["Chao Xing", "Dong Wang"], "venue": "and Yiye Lin.", "citeRegEx": "Xing et al.2015", "shortCiteRegEx": null, "year": 2015}, {"title": "Learning composition models for phrase embeddings. Transactions of the Association for Computational Linguistics, 3:227\u2013242", "author": ["Yu", "Dredze2015] Mo Yu", "Mark Dredze"], "venue": null, "citeRegEx": "Yu et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Yu et al\\.", "year": 2015}, {"title": "Guoshun Wu", "author": ["Zhihua Zhang"], "venue": "and Man Lan.", "citeRegEx": "Zhang et al.2015", "shortCiteRegEx": null, "year": 2015}], "referenceMentions": [], "year": 2017, "abstractText": "Embeddings are generic representations that are useful for many NLP tasks. In this paper, we introduce DENSIFIER, a method that learns an orthogonal transformation of the embedding space that focuses the information relevant for a task in an ultradense subspace of a dimensionality that is smaller by a factor of 100 than the original space. We show that ultradense embeddings generated by DENSIFIER reach state of the art on a lexicon creation task in which words are annotated with three types of lexical information \u2013 sentiment, concreteness and frequency. On the SemEval2015 10B sentiment analysis task we show that no information is lost when the ultradense subspace is used, but training is an order of magnitude more efficient due to the compactness of the ultradense space.", "creator": "LaTeX with hyperref package"}}}