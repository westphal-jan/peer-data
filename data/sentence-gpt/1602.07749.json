{"id": "1602.07749", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "24-Feb-2016", "title": "Toward Mention Detection Robustness with Recurrent Neural Networks", "abstract": "One of the key challenges in natural language processing (NLP) is to yield good performance across application domains and languages. In this work, we investigate the robustness of the mention detection systems, one of the fundamental tasks in information extraction, via recurrent neural networks (RNNs). The advantage of RNNs over the traditional approaches is their capacity to capture long ranges of context and implicitly adapt the word embeddings, trained on a large corpus, into a task-specific word representation, but still preserve the original semantic generalization to be helpful across domains. Our systematic evaluation for RNN architectures demonstrates that RNNs not only outperform the best reported systems (up to 9\\% relative error reduction) in the general setting but also achieve the state-of-the-art performance in the cross-domain setting for English. Regarding other languages, RNNs are significantly better than the traditional methods on the similar task of named entity recognition for Dutch (up to 22\\% relative error reduction).\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n", "histories": [["v1", "Wed, 24 Feb 2016 23:14:01 GMT  (44kb)", "http://arxiv.org/abs/1602.07749v1", "13 pages, 11 tables, 3 figures"]], "COMMENTS": "13 pages, 11 tables, 3 figures", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["thien huu nguyen", "avirup sil", "georgiana dinu", "radu florian"], "accepted": false, "id": "1602.07749"}, "pdf": {"name": "1602.07749.pdf", "metadata": {"source": "CRF", "title": "Toward Mention Detection Robustness with Recurrent Neural Networks", "authors": ["Thien Huu Nguyen", "Avirup Sil", "Georgiana Dinu", "Radu Florian"], "emails": ["thien@cs.nyu.edu,{avi,gdinu,raduf}.us.ibm.com"], "sections": [{"heading": null, "text": "ar X\niv :1\n60 2.\n07 74\n9v 1\n[ cs\n.C L\n] 2\n4 Fe\nb 20\n16"}, {"heading": "1 Introduction", "text": "One of the crucial steps toward understanding natural languages is mention detection (MD), whose goal is to identify entity mentions, whether named, nominal (the president) or pronominal (he, she), and classify them into some predefined types of interest in text such as PERSON, ORGANIZATION or LOCATION. This is an extension of the named entity recognition (NER) task which only aims to extract entity names. MD is necessary for many higher-level applications such as relation extraction, knowledge population, information retrieval, question answering and so on.\n\u2217Work carried out during an internship at IBM\nTraditionally, both MD and NER are formalized as sequential labeling problems, thereby being solved by some linear graphical models such as Hidden Markov Models (HMMs), Maximum Entropy Markov Models (MEMMs) or Conditional Random Fields (CRFs) (Lafferty et al., 2001). Although these graphical models have been adopted well to achieve the top performance for MD, there are still at least three problems we want to focus in this work:\n(i) The first problem is the performance loss of the mention detectors when they are trained on some domain (the source domain) and applied to other domains (the target domains). The problem might originate from various mismatches between the source and the target domains (domain shifts) such as the vocabulary difference, the distribution mismatches etc (Blitzer et al., 2006; Daume, 2007; Plank and Moschitti, 2013).\n(ii) Second, in mention detection, we might need to capture a long context, possibly covering the whole sentence, to correctly predict the type for a word. For instance, consider the following sentence with the pronominal \u201cthey\u201d:\nNow, the reason that France, Russia and Germany are against war is because they have suffered much from the past war.\nIn this sentence, the correct type GPE1 for \u201cthey\u201d can only be inferred from its GPE references: \u201cFrance\u201d, \u201cRussia\u201d and \u201cGermany\u201d which are far way from the pronominal \u201cthey\u201d of interest. The challenge is come up with the models that can encode and utilize these long-range dependency context effectively.\n(iii) The third challenge is to be able to quickly adapt the current techniques for MD so that they can perform well on new languages.\nIn this paper, we propose to address these problems for MD via recurrent neural networks (RNNs) which offer a decent recurrent mechanism to embed the sentence context into a dis-\n1Geographical Political Entity\ntributed representation and employ it to decode the sentences. Besides, as RNNs replace the symbolic forms of words in the sentences with their word embeddings, the distributed representation that captures the general syntactic and semantic properties of words (Collobert and Weston, 2008; Mnih and Hinton, 2008; Turian et al., 2010), they can alleviate the lexical sparsity, induce more general feature representation, thus generalizing well across domains (Nguyen and Grishman, 2015b). This also helps RNNs to quickly and effectively adapt to new languages which just require word embeddings as the only new knowledge we need to obtain. Finally, we can achieve the task-specific word embeddings for MD to improve the overall performance by updating the initial pre-trained word embeddings during the course of training in RNNs.\nThe recent emerging interest in deep learning has produced many successful applications of RNNs for NLP problems such as machine translation (Cho et al., 2014a; Bahdanau et al., 2015), semantic role labeling (Zhou and Xu, 2015) etc. However, to the best of our knowledge, there has been no previous work employing RNNs for MD on the cross-domain and language settings so far. To summarize, the main contributions of this paper are as follow:\n1. We perform a systematic investigation on various RNN architectures and word embedding techniques that are motivated from linguistic observations for MD.\n2. We achieve the state-of-the-art performance for MD both in the general setting and in the crossdomain setting with the bidirectional modeling applied to RNNs.\n3. We demonstrate the portability of the RNN models for MD to new languages by their significant improvement with large margins over the best reported system for named entity recognition in Dutch."}, {"heading": "2 Related Work", "text": "Both named entity recognition (Bikel et al., 1997; Borthwick et al., 1997; Tjong Kim Sang and De Meulder, 2003; Florian et al., 2003; Miller et al., 2004; Ando and Zhang, 2005; Suzuki and Isozaki, 2008; Ratinov and Roth, 2009; Lin and Wu, 2009; Turian et al., 2010; Ritter et al., 2011; Passos et al., 2014; Cherry and Guo, 2015) and mention detection (Florian et al., 2004)\nhave been extensively studied with various evaluation in the last decades: MUC6, MUC7, CoNLL\u201902, CoNLL\u201903 and ACE. The previous work on MD has examined the cascade models (Florian et al., 2006), transferred knowledge from rich-resource languages to low-resource ones via machine translation (Zitouni and Florian, 2008) or improved the systems on noisy input (Florian et al., 2010). Besides, some recent work also tries to solve MD jointly with other tasks such as relation or event extraction to benefit from their inter-dependencies (Roth and Yih, 2007; Kate and Mooney, 2010; Li and Ji, 2014a; Li et al., 2014b). However, none of these work investigates RNNs for MD on the cross-domain and language settings as we do in this paper.\nRegarding neural networks, a large volume of work has devoted to the application of deep learning to NLP in the last few years, centering around several network architecture such as convolutional neural networks (CNNs) (Yih et al., 2014; Shen et al., 2014; Kalchbrenner et al., 2014; Kim, 2014; Zeng et al., 2014; dos Santos et al., 2015a; dos Santos and Guimar\u00e3es, 2015b), recurrent/recursive neural networks (Socher et al., 2012; Cho et al., 2014a; Bahdanau et al., 2015; Zhou and Xu, 2015; Tai et al., 2015), to name a few. For NER, Collobert et al. (2011) propose a CNN-based framework while Mesnil et al. (2013) and Yao et al. (2013; 2014) investigate the RNNs for the slot filling problem in spoken language understanding. Although our work also examines the RNNs, we consider the mention detection problem with an emphasis on the robustness of the models in the domain shifts and language changes which has never been explored in the literature before.\nFinally, for the robustness in the domain adaptation setting, the early work has focused on the sequential labeling tasks such as part-of-speech tagging or name tagging (Blitzer et al., 2006; Huang and Yates, 2010; Daume, 2007; Xiao and Guo, 2013; Schnabel and Sch\u00fctze, 2014). Recent work has drawn attention to relation extraction (Plank and Moschitti, 2013; Nguyen et al., 2015a; Gormley et al., 2015). In the field of neural networks, to the best of our knowledge, there is only one work from Nguyen and Grishman (2015b) that evaluates CNNs for event detection in the cross-domain setting."}, {"heading": "3 Models", "text": "We formalize the mention detection problem as a sequential labeling task. Given a sentence X = w1w2 . . . wn, where wi is the i-th word and n is the length of the sentence, we want to predict the label sequence Y = y1y2 . . . yn for X, where yi is the label for wi. The labels yi follow the BIO2 encoding to capture the entity mentions in X (Ratinov and Roth, 2009).\nIn order to prepare the sentence for RNNs, we first transform each word wi into a real-valued vector using the concatenation of two vectors ei and fi: wi = [ei, fi]2, where:\n\u2022 ei is the word embedding vector of wi, obtained by training a language model on a large corpus (discussed later).\n\u2022 fi is a binary vector encompassing different features for wi. In this work, we are utilizing four types of features: capitalization, gazetteers, triggers (whether wi is present in a list of trigger words3 or not) and cache (the label that is assigned to wi sometime before in the document).\nWe then enrich this vector representation by including the word vectors in a context window of vc for each word in the sentence to capture the short-term dependencies for prediction (Mesnil et al., 2013). This effectively converts wi into the context window version of the concatenated vectors: xi = [wi\u2212vc , . . . , wi, . . . , wi+vc ].\nGiven the new input representation, we describe the RNNs to be investigated in this work below."}, {"heading": "3.1 The Basic Models", "text": "In standard recurrent neural networks, at each time step (word position in sentence) i, we have three main vectors: the input vector xi \u2208 RI , the hidden vector hi \u2208 RH and the output vector oi \u2208 RO (I , H and O are the dimensions of the input vectors, the dimension of the hidden vectors and the number of possible labels for each word respectively). The output vector oi is the probabilistic distribution over the possible labels for the word xi and obtained from hi via the softmax function \u03d5:\noi = \u03d5(Whi), \u03d5(zm) = ezm\n\u2211 k e zk\n2For simplicity, we are using the word wi and its realvalued vector representation interchangeably.\n3Trigger words are the words that are often followed by entity names in sentences such as \u201cpresident\u201d, \u201cMr.\u201d etc.\nRegarding the hidden vectors or units hi, there are two major methods to obtain them from the current input and the last hidden and output vectors, leading to two different RNN variants:\n\u2022 In the Elman model (Elman, 1990), called ELMAN, the hidden vector from the previous step hi\u22121, along with the input in the current step xi, constitute the inputs to compute the current hidden state hi:\nhi = \u03a6(Uxi + V hi\u22121) (1)\n\u2022 In the Jordan model (Jordan, 1986), called JORDAN, the output vector from the previous step oi\u22121 is fed into the current hidden layer rather than the hidden vector from the previous steps hi\u22121. The rationale in this topology is to introduce the label from the preceding step as a feature for current prediction:\nhi = \u03a6(Uxi + V oi\u22121) (2)\nIn the formula above, \u03a6 is the sigmoid activation function: \u03a6(z) = 1\n1+e\u2212z and W , U , V are\nthe same weight matrices for all time steps, to be learned during training. The unfolded dependency graphs for the two models are given in Figure 1."}, {"heading": "3.2 Gated Recurrent Units", "text": "The hidden units in the two basic models above are essentially the standard feed-forward neural networks that take the vectors hi\u22121, oi\u22121 and xi as inputs and do a linear transformation followed by a nonlinearity to generate the hidden vector hi. The ELMAN and JORDAN models are then basically a stack of these hidden units. Unfortunately, this staking mechanism causes the so-called \u201cvanishing gradient\u201d and \u201cexploding gradient\u201d problems (Bengio et al., 1994), making it challenging to train the networks properly in practice (Pascanu et al., 2012). These problems are addressed by the long-short term memory units (LSTM) (Hochreiter and Schmidhuber, 1997; Graves et al., 2009) that propose the idea of memory cells with four gates to allow the information storage and access over a long period of time.\nIn this work, we apply another version of memory units with only two gates (reset and update), called Gated Recurrent Units (GRUs) from Cho et al. (2014a; 2015). GRU is shown to be much simpler than LSTM in terms of computation and implementation but still achieves the\ncomparable performance in machine translation (Bahdanau et al., 2015).\nThe introduction of GRUs into the models ELMAN and JORDAN amounts to two new models, named ELMAN_GRU and JORDAN_GRU respectively, with two new methods to compute the hidden vectors hi. The formula for ELMAN_GRU is adopted directly from Cho et al. (2014b) and given below:\nhi = zi \u2299 h\u0302i + (1\u2212 zi)\u2299 hi\u22121\nh\u0302i = \u03a6(Whxi + Uh(ri \u2299 hi\u22121))\nzi = \u03a6(Wzxi + Uzhi\u22121) (3)\nri = \u03a6(Wrxi + Urhi\u22121)\nwhere Wh,Wz,Wr \u2208 RH\u00d7I , Uh, Uz, Ur \u2208 R\nH\u00d7H and \u2299 is the element-wise multiplication operation.\nWe cannot directly apply the formula above to the JORDAN_GRU model since the dimensions of the output vectors oi and the hidden vector hi are different in general. For JORDAN_GRU, we first need to transform the output vector oi into the hidden vector space, leading to the following formula:\nhi = zi \u2299 o\u0302i + (1\u2212 zi)\u2299 ti\u22121\nti\u22121 = Toi\u22121\no\u0302i = \u03a6(Woxi + Uo(ri \u2299 ti\u22121))\nzi = \u03a6(Wzxi + Uzti\u22121) (4)\nri = \u03a6(Wrxi + Urti\u22121)\nwhere T \u2208 RH\u00d7O."}, {"heading": "3.3 The Extended Networks", "text": "One of the limitations of the four basic models presented above is their incapacity to incorporate the future context information that might be crucial to the prediction in the current step. For instance, consider the first word \u201cLiverpool\u201d in the following sentence:\nLiverpool suffered an upset first home league defeat of the season, beaten 1-0 by a Guy Whittingham goal for Sheffield Wednesday.\nIn this case, the correct label ORGANIZATION can only be detected if we first go over the whole sentence and then utilize the context words after \u201cLiverpool\u201d to decide its label.\nThe limitation of the four models is originated from their mechanism to perform a single pass over the sentences from left to right and make the prediction for a word once they first encounter it. In the following, we investigate two different networks to overcome this limitation."}, {"heading": "3.3.1 The Contextual Networks", "text": "The contextual networks are motivated by the RNN Encoder-Decoder models that have become very popular in neural machine translation recently (Cho et al., 2014a; Bahdanau et al., 2015). In these networks, we first run a RNN Re over the whole sentence X = x1x2 . . . xn to collect the hidden vector sequence c1, c2, . . . , cn, where ci is the hidden vector for the i-th step in the sentence. For convenience, this process is denoted by:\nRe(x1x2 . . . xn) = c1, c2, . . . , cn\nThe final hidden vector cn is then considered as a distributed representation of X, encoding the global context or topic information for X (the encoding phrase) and thus possibly being helpful for the label prediction of each word in X. Consequently, we perform the second RNN Rd over X to decode the sentence in which cn is used as an additional input in computing the hidden units for\nevery time step (the decoding phrase). Note that Re (the encoding model) should be an Elman model4 while Rd (the decoding model) can be any Elman or Jordan model. As an example, the formula for Rd = ELMAN is:\nhi = \u03a6(Uxi + V hi\u22121 + Scn)"}, {"heading": "3.3.2 The Bidirectional Networks", "text": "The bidirectional networks involve three passes over the sentence, in which the first two passes are designated to encode the sentence while the third pass is responsible for decoding. The procedure for the sentence X = x1x2 . . . xn is below:\n(i) Run the first RNN Ref from left to right over x1x2 . . . xn to obtain the first hidden vector or output vector sequence (depending on whether Ref is an Elman or Jordan network respectively): Ref (x1x2 . . . xn) = l1, l2, . . . , ln (forward encoding).\n(ii) Run the second RNN Reb from right to left over x1x2 . . . xn to obtain the second hidden vector or output vector sequence: Reb(xnxn\u22121 . . . x1) = rn, rn\u22121, . . . , r1 (backward encoding).\n(iii) Obtain the concatenated sequence \u03b1 = \u03b11, \u03b12, . . . , \u03b1n where \u03b1i = [li, ri].\n(iv) Decode the sentence with the third RNN Rd (the decoding model) using \u03b1 as the input vector, i.e, replacing xi by \u03b1i in the formula (1), (2), (3) and (4).\nConceptually, the encoding RNNs Ref and Reb can be different but in this work, for simplicity and consistency, we assume that we only have a single encoding model, i.e, Ref = Reb = Re. Once\n4From now on, for convenience, the term \u201cElman models\u201d refers to the ELMAN and ELMAN_GRU models. The same implication applies for the Jordan models.\nagain, Re and Rd can be any model in {ELMAN, JORDAN, ELMAN_GRU, JORDAN_GRU}.\nThe observation is, at the time step i, the forward hidden vector li represents the encoding for the past word context (from position 1 to i) while the backward hidden vector ri is the summary for the future word context (from position n to i). Consequently, the concatenated vector \u03b1i = [li, ri] constitutes a distributed representation that is specific to the word at position i but still encapsulates the context information over the whole sentence at the same time. This effectively provides the networks a much richer representation to decode the sentence. The bidirectional network for Re = ELMAN and Rd = JORDAN is given on the left of Figure 2.\nWe notice that Mesnil et al. (2013) also investigate the bidirectional models for the task of slot filling in spoken language understanding. However, compared to the work presented here, Mesnil et al. (2013) does not use any special transition memory cells (like the GRUs we are employing in this paper) to avoid numerical stability issues (Pascanu et al., 2012). Besides, they form the inputs \u03b1 for the decoding phase from a larger context of the forward and backward encoding outputs, while performing word-wise, independent classification; in contrast, we use only the current output vectors in the forward and backward encodings for \u03b1, but perform recursive computations to decode the sentence via the RNN model Rd (demonstrated on the right of Figure 2)."}, {"heading": "3.4 Training and Inference", "text": "We train the networks locally. In particular, each training example consists of a word xi and its corresponding label yi in a sentence X = x1x2 . . . xn (denoted by E = (xi, yi,X)). In the encoding phase, we first compute the necessary inputs ac-\ncording to the specific model of interest. This can be the original input vectors x1, x2, . . . , xn in the four basic models or the concatenated vectors \u03b11, \u03b12, . . . , \u03b1n in the bidirectional models. For the contextual models, we additionally have the context vector cn. Eventually, in the decoding phase, an sequence of vd input vectors preceding the current position i is fed into the decoding network Rd to obtain the output vector sequence. The last vector in this output sequence corresponds to the probabilistic label distribution for the current position i, to be used to compute the objective function. For example, in the bidirectional models, the input sequence for the decoding phase is \u03b1i\u2212vd\u03b1i\u2212vd+1 . . . \u03b1i while the output sequence is: Re(\u03b1i\u2212vd\u03b1i\u2212vd+1 . . . \u03b1i) = oi\u2212vdoi\u2212vd+1 . . . oi.\nIn this work, we employ the stochastic gradient descent algorithm5 to update the parameters via minimizing the negative log-likelihood objective function: nll(E) = \u2212 log(oi[yi]).\nFinally, besides the weight matrices in the networks, the word embeddings are also optimized during training to obtain the task-specific word embeddings for MD. The gradients are computed using the back-propagation through time algorithm (Mozer, 1989) and inference is performed by running the networks over the whole sentences and taking argmax over the output sequence: yi = argmax(oi)."}, {"heading": "4 Word Representation", "text": "Following Collobert et al. (2011), we pre-train word embeddings from a large corpus and employ them to initialize the word representations in the models. One of the state-of-the-art mod-\n5We try the AdaDelta algorithm (Zeiler, 2012) and the dropout regularization but do not see much difference.\nels to train word embeddings have been proposed recently in Mikolov et al. (2013a; 2013b) that introduce two log-linear models, i.e the continuous bag-of-words model (CBOW) and the continuous skip-gram model (Skip-gram). The CBOW model attempts to predict the current word based on the average of the context word vectors while the Skip-gram model aims to predict the surrounding words in a sentence given the current word. In this work, besides the CBOW and skip-gram models, we examine a concatenation-based variant of CBOW (C-CONCAT) to train word embeddings and compare the three models to gain insights into which kind of model is effective to obtain word representations for the MD task. The objective of C-CONCAT is to predict the target word using the concatenation of the vectors of the words surrounding it, motivated from our strategy to decide the label for a word based on the concatenated context vectors. Intuitively, the C-CONCAT model would perform better than CBOW due to the close relatedness between the decoding strategies of C-CONCAT and the MD methods. CBOW, Skip-gram and C-CONCAT are illustrated in Figure 3."}, {"heading": "5 Experiments", "text": ""}, {"heading": "5.1 Dataset", "text": "Our main focus in this work is to evaluate the robustness of the MD systems across domains and languages. In order to investigate the robustness across domains, following the prior work (Plank and Moschitti, 2013; Nguyen et al., 2015a), we utilize the ACE 2005 dataset which contains 6 domains: broadcast news (bn), newswire (nw), broadcast conversation (bc),\ntelephone conversation (cts), weblogs (wl), usenet (un) and 7 entity types: person, organization, GPE, location, facility, weapon, vehicle. The union of bn and nw is considered as a single domain, called news. We take half of bc as the only development data and use the remaining data and domains for evaluation. Some statistics about the domains are given in Table 1. As shown in Plank and Moschitti (2013), the vocabulary of the domains is quite different.\nFor completeness, we also test the RNNs system on the Named Entity Recognition for English using the CoNLL 2003 dataset6 (Florian et al., 2003; Tjong Kim Sang and De Meulder, 2003) and compare the performance with the state-ofthe-art neural network system on this dataset (Collobert et al., 2011). Regarding the robustness across languages, we further evaluate the RNN models on the CoNLL 2002 dataset for Dutch Named Entity Recognition7 (Carreras et al., 2002; Tjong Kim Sang, 2002). Both CoNLL datasets come along with the training data, validation data and test data, annotated for 4 types of entities: person, organization, location and miscellaneous.\nFinally, we use the standard IOB2 tagging schema for the ACE 2005 dataset and the Dutch CoNLL dataset while the IOBES tagging schema is applied for the English CoNLL dataset to ensure the compatibility with Collobert et al. (2011)."}, {"heading": "5.2 Resources and Parameters", "text": "In all the experiments for RNNs below, we employ the context window vc = 5, the decoding window vd = 9. We find that the optimal number of hidden units (or the dimension of the hidden vectors) and the learning rate vary according to the dataset. For the ACE 2005 dataset, we utilize 200 hidden units with learning rate = 0.01 while these numbers are 100 and 0.06 respectively for the CoNLL datasets. Note that the number of hidden units is kept the same in both the encoding phase and the decoding phase.\n6 http://www.cnts.ua.ac.be/conll2003/ner/ 7 http://www.cnts.ua.ac.be/conll2002/ner/\nFor word representation, we train the word embeddings for English from the Gigaword corpus augmented with the newsgroups data from BOLT (Broad Operational Language Technologies) (6 billion tokens) while the entire Dutch Wikipedia pages (310 million tokens) are extracted to train the Dutch word embeddings. We utilize the word2vec toolkit8 (modified to add the C-CONCAT model) to learn the word representations. Following Baroni et al. (2014), we use the context window of 5, subsampling set to 1e-05 and negative sampling with the number of instances set to 10. The dimension of the vectors is set to 300 to make it comparable with the word2vec toolkit."}, {"heading": "5.3 Model Architecture Experiments", "text": ""}, {"heading": "5.3.1 Model Architecture Evaluation", "text": "In this section, we evaluate different RNN models by training the models on the news domain and report the performance on the development set. As presented in the previous sections, we have 4 basic models M = {ELMAN, JORDAN, ELMAN_GRU, JORDAN_GRU}, 8 contextual models (two choices for the encoding model Re in {ELMAN, ELMAN_GRU} and 4 choices for the decoding model Rd \u2208 M ), and 16 bidirectional models (4 choices for the encoding and decoding models Re, Rd in M). The performance for the basic models, the contextual models and the bidirectional models are shown in Table 2, Table 3 and Table 4 respectively9 .\nThere are several important observations from the three tables:\n-Elman vs Jordan: In the encoding phase, the Elman models consistently outperform the Jordan models when the same decoding model is applied\n8 https://code.google.com/p/word2vec/\n9The experiments in this section use C-CONCAT to pretrain word embeddings.\nin the bidirectional architecture. In the decoding phase, however, it turns out that the Jordan models are better most of the time over different model architectures (basic, contextual or bidirectional).\n-With vs Without GRUs: The trends are quite mixed in the comparison between the cases with and without GRUs. In particular, for the encoding part, given the same decoding model, GRUs are very helpful in the bidirectional architecture while this is not the case for the contextual architecture. For the decoding part, we can only see the clear benefit of GRUs in the basic models and the bidirectional architecture when Re is a Jordan model.\n-Regarding different model architectures, in general, the bidirectional models are more effective than the contextual models and the basic models, confirming the effectiveness of bidirectional modeling to achieve a richer representation for MD.\nThe best basic model (F1 = 81.06%), the best contextual model (F1 = 80.77%) and the best bidirectional model with (F1 = 82.37%) are called BASIC, CONTEXT and BIDIRECT respectively. In the following, we only focus on these best models in the experiments."}, {"heading": "5.3.2 Comparison to other Bidirectional RNN Work", "text": "Mesnil et al. (2013) also present a RNN system with bidirectional modeling for the slot filling task. As described in Section 3.3.2, the major difference between the bidirectional models in this work and Mesnil el al. (2013)\u2019s is the recurrence in our decoding phase. Table 5 compares the performance of the bidirectional model from Mesnil et al. (2013), called MESNIL, and the BIDIRECT model. In order to verify the effectiveness of recurrence in decoding, the performance of MESNIL incorporated with the JORDAN_GRU model in the decoding phase (MESNIL+JORDAN_GRU) is also reported.\nIn general, we see that the bidirectional model in this work is much better than the model in Mesnil et al. (2013) for MD. This is significant with"}, {"heading": "5.4 Word Embedding Evaluation", "text": "The section investigates the effectiveness of different techniques to learn word embeddings to initialize the RNNs for MD. Table 6 presents the performance of the BASIC, CONTEXT and BIDIRECT models on the development set (trained on news) when the CBOW, SKIP-GRAM and C-CONCAT techniques are utilized to obtain word embeddings from the same English corpus. We also report the performance of the models when they are initialized with the word2vec word embeddings from Mikolov et al. (2013a; 2013b) (trained with the Skip-gram model on 100 billion words of Google News) (WORD2VEC). All of these word embeddings are updated during the training of the RNNs to induce the task-specific word embeddings . Finally, for comparison purpose, the performance for the two following scenarios is also included: (i) the word vectors are initialized randomly (not using any pre-trained word embeddings) (RANDOM), and (ii) the word vectors are loaded from the C-CONCAT pre-trained word embeddings but fixed during the RNN training (FIXED).\nThe first observation is that we need to borrow some pre-trained word embeddings and update them during the training process to improve the MD performance (comparing C-CONCAT, RANDOM and FIXED). Second, C-CONCAT is much better than CBOW, confirming our hypothesis about the similarity between the decodings of C-BOW and MD in Section 4. Third, we do not see much difference in terms of MD performance when we enlarge the corpus to learn\nword embeddings (comparing SKIP-GRAM and WORD2VEC that is trained with the skip-gram model on a much larger corpus). Finally, we achieve the best performance when we apply the C-CONCAT technique in the BIDIRECT model. From now on, for consistency, we use the CCONCAT word embeddings in all the experiments below."}, {"heading": "5.5 Comparison to the State-of-the-art", "text": ""}, {"heading": "5.5.1 The ACE 2005 Dataset for Mention Detection", "text": "The state-of-the-art systems for mention detection on the ACE 2005 dataset have been the joint extraction system for entity mentions and relations from Li and Ji (2014a) and the information networks to unify the outputs of three information extraction tasks: entity mentions, relations and events using structured perceptron from Li et al. (2014b). They extensively hand-design a large set of features (parsers, gazetteers, word clusters, coreference etc) to capture the inter-dependencies between different tasks. In this section, besides comparing the RNN systems above with these state-of-the-art systems, we also implement a Maximum Entropy Markov Model (MEMM) system10, following the description and features in Florian et al. (2004; 2006), and include it in the comparison for completeness11 . For this comparison, following Li and Ji (2014a), we remove the documents from the two informal domains cts and un, and then randomly split the remaining 511 documents into 3 parts: 351 for training, 80 for development, and the rest 80 for blind test. The performance of the systems on the blind test set is presented in Table 7.\nThere are two main conclusions from the table. First, our MEMM system is already better than the state-of-the-art system on this dataset, possibly due to the superiority of the features we are em-\n10We tried the CRF model, but it is worse than MEMM in our case.\n11We notice that the four features we are using in the RNN models (Section 3) are also included in the feature set of the implemented MEMM system.\nploying in this system. Consequently, from now on, we would utilize this MEMM system as the baseline in the following experiments. Second, all the three RNN systems: BASIC, CONTEXT and BIDIRECT substantially outperform the stateof-the-art system with large margins. In fact, we achieve the best performance on this dataset with the BIDIRECT model, once again testifying to the benefit of bidirectional modeling for MD."}, {"heading": "5.5.2 The CoNLL 2003 Dataset for English Named Entity Recognition", "text": "This section further assess the RNN systems on the similar task of Named Entity Recognition for English to compare them with other neural network approaches for completeness. On the CoNLL 2003 dataset for English NER, the best neural network system so far is Collobert et al. (2011). This system, called CNN-Sentence, employs convolutional neural networks to encode the sentences and then decodes it at the sentence level. Table 8 shows the performance of CNN-Sentence and our RNN systems on this dataset.\nAs we can see from the table, the RNN systems are on par with the CNN-Sentence system from Collobert et al. (2011) except the CONTEXT system that is worse in this case. We actually accomplish the best performance with the BIDIRECT model, thus further demonstrating its virtue."}, {"heading": "5.6 Cross-Domain Experiments", "text": "One of the main problems we want to address in this work is the robustness across domains of the MD systems. This section tests the MEMM (the baseline) and the RNN systems on the cross-domain settings to gain an insights into their operation when the domain changes. In particular, for the first experiment, following the previous work of domain adaptation on the ACE 2005 dataset (Plank and Moschitti, 2013; Nguyen and Grishman, 2014; Nguyen et al., 2015a), we treat news as the source domain and the other domains: bc, cts, wl and un as the target domains. We then examine the systems on two scenarios: (i) the systems are trained and tested on the source domain via 5-fold cross validation (in-domain performance), and\n(ii) the systems are trained on the source domain but evaluated on the target domains. Besides, in order to understand the effect of the features on the systems, we report the systems\u2019 performance in both the inclusion and exclusion of the features described in Section 3. Table 9 presents the results.\nTo summarize, we find that the RNN systems significantly outperform the MEMM system across all the target domains when the features are not applied. The BIDIRECT system still yields the best performance among systems being investigated (except in domain bc). This is also the case in the inclusion of features and demonstrates the robustness of the BIDIRECT model in the domain shifts. We further support this result in Table 10 where we report the performance of the MEMM and BIDIRECT systems (with features) on different domain assignments for the source and the target domains. Finally, we also see that the features are very useful for both the MEMM and the RNNs."}, {"heading": "5.7 Named Entity Recognition for Dutch", "text": "The previous sections have dealt with mention detection for English. In this section, we want to explores the capacity of the systems to quickly and effectively adapt to a new language. In particular, we evaluate the systems on the named entity recognition task (the simplified version of the MD task) for Dutch using the CoNLL 2002 dataset. The state-of-the-art performance for this dataset is due to Carreras et al. (2002) in the CoNLL 2002 evaluation and Nothman et al. (2013). Very recently, while we were preparing this paper, Gillick el al. (2015) introduce a multilingual language processing system and also report the performance\non this dataset. Table 11 compares the systems.\nNote that the systems in Gillick el al. (2015) are also based on RNNs and the row labeled with * for Gillick el al. (2015) corresponds to the system trained on multiple datasets instead of the single CoNLL dataset for Dutch, so not being comparable to ours.\nThe most important conclusion from the table is that the RNN models in this work significantly outperform MEMM as well as the other comparable system by large margins (up to 22% reduction in relative error). This proves that the proposed RNN systems are less subject to the language changes than MEMM and the other systems. Finally, BIDIRECT is also significantly better than BASIC, testifying to its robustness across languages."}, {"heading": "6 Conclusion", "text": "We systematically investigate various RNNs to solve the MD problem which suggests that bidirectional modeling is a very helpful mechanism for this task. The comparison between the RNN models and the state-of-the-art systems in the literature reveals the strong promise of the RNN models. In particular, the bidirectional model achieves the best performance in the general setting (up to 9% reduction in relative error) and outperforms\na very strong baseline of the feature-based exponential models in the cross-domain setting, thus demonstrating its robustness across domains. We also show that the RNN models are more portable to new languages as they are significantly better than the best reported systems for NER in Dutch (up to 22% reduction in relative error). In the future, we plan to apply the bidirectional modeling technique to other tasks as well as study the combination of different network architectures and resources to further improve the performance of the systems."}, {"heading": "Acknowledgment", "text": "We would like to thank Ralph Grishman for valuable suggestions."}], "references": [{"title": "A high-performance semi-supervised learning method for text chunking", "author": ["Ando", "Zhang2005] Rie Ando", "Tong Zhang"], "venue": null, "citeRegEx": "Ando et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Ando et al\\.", "year": 2005}, {"title": "Neural machine translation by jointly learning to align and translate", "author": ["Kyunghyun Cho", "Yoshua Bengio"], "venue": "In ICLR", "citeRegEx": "Bahdanau et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Bahdanau et al\\.", "year": 2015}, {"title": "Don\u2019t count, predict! a systematic comparison of context-counting vs. context-predicting semantic vectors", "author": ["Baroni et al.2014] Marco Baroni", "Georgiana Dinu", "Germ\u00e1n Kruszewski"], "venue": null, "citeRegEx": "Baroni et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Baroni et al\\.", "year": 2014}, {"title": "Learning long-term dependencies with gradient descent is difficult", "author": ["Bengio et al.1994] Yoshua Bengio", "Patrice Simard", "Paolo Frasconi"], "venue": "In Journal of Machine Learning Research", "citeRegEx": "Bengio et al\\.,? \\Q1994\\E", "shortCiteRegEx": "Bengio et al\\.", "year": 1994}, {"title": "Nymble: a high-performance learning name-finder", "author": ["Scott Miller", "Richard Schwartz", "Ralph Weischedel"], "venue": "ANLP", "citeRegEx": "Bikel et al\\.,? \\Q1997\\E", "shortCiteRegEx": "Bikel et al\\.", "year": 1997}, {"title": "Domain adaptation with structural correspondence learning", "author": ["Blitzer et al.2006] John Blitzer", "Ryan McDonald", "Fernando Pereira"], "venue": null, "citeRegEx": "Blitzer et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Blitzer et al\\.", "year": 2006}, {"title": "Exploiting diverse knowledge sources via maximum entropy in named entity recognition", "author": ["John Sterling", "Eugene Agichtein", "Ralph Grishman"], "venue": "In Sixth Workshop on Very Large Corpora", "citeRegEx": "Borthwick et al\\.,? \\Q1997\\E", "shortCiteRegEx": "Borthwick et al\\.", "year": 1997}, {"title": "Named entity extraction using adaboost", "author": ["Llu\u00eds M\u00e0rques", "Llu\u00eds Padr\u00f3"], "venue": "In CoNLL", "citeRegEx": "Carreras et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Carreras et al\\.", "year": 2002}, {"title": "The unreasonable effectiveness of word representations for twitter named entity recognition", "author": ["Cherry", "Guo2015] Colin Cherry", "Hongyu Guo"], "venue": null, "citeRegEx": "Cherry et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Cherry et al\\.", "year": 2015}, {"title": "Learning phrase representations using rnn encoder\u2013decoder for statistical machine translation", "author": ["Cho et al.2014a] Kyunghyun Cho", "Bart van Merrienboer", "Caglar Gulcehre", "Dzmitry Bahdanau", "Fethi Bougares", "Holger Schwenk", "Yoshua Bengio"], "venue": null, "citeRegEx": "Cho et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Cho et al\\.", "year": 2014}, {"title": "Quick introduction to natural language processing with neural networks", "author": ["Kyunghyun Cho"], "venue": "In Lecture at the Ecole Polytechnique de Montreal", "citeRegEx": "Cho.,? \\Q2014\\E", "shortCiteRegEx": "Cho.", "year": 2014}, {"title": "A unified architecture for natural language processing: deep neural networks with multitask learning", "author": ["Collobert", "Weston2008] Ronan Collobert", "Jason Weston"], "venue": null, "citeRegEx": "Collobert et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Collobert et al\\.", "year": 2008}, {"title": "Natural language processing (almost) from scratch", "author": ["Jason Weston", "L\u00c3l\u2019on Bottou", "Michael Karlen", "Koray Kavukcuoglu", "Pavel P. Kuksa"], "venue": "In CoRR", "citeRegEx": "Collobert et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Collobert et al\\.", "year": 2011}, {"title": "Boosting named entity recognition with neural character embeddings. In the Fifth Named Entity Workshop, ACL-IJCNLP", "author": ["dos Santos", "Victor Guimar\u00e3es"], "venue": null, "citeRegEx": "Santos et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Santos et al\\.", "year": 2015}, {"title": "Classifying relations by ranking with convolutional neural networks. In ACL-IJCNLP", "author": ["Bing Xiang", "Bowen Zhou"], "venue": null, "citeRegEx": "Santos et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Santos et al\\.", "year": 2015}, {"title": "Finding structure in time", "author": ["Jeffrey L. Elman"], "venue": "In Cognitive Science", "citeRegEx": "Elman.,? \\Q1990\\E", "shortCiteRegEx": "Elman.", "year": 1990}, {"title": "Named entity recognition through classifier combination", "author": ["Florian et al.2003] Radu Florian", "Abe Ittycheriah", "Hongyan Jing", "Tong Zhang"], "venue": null, "citeRegEx": "Florian et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Florian et al\\.", "year": 2003}, {"title": "A statistical model for multilingual entity detection and tracking", "author": ["Florian et al.2004] R Florian", "H Hassan", "A Ittycheriah", "H Jing", "N Kambhatla", "X Luo", "N Nicolov", "S Roukos"], "venue": "In HLT-NAACL", "citeRegEx": "Florian et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Florian et al\\.", "year": 2004}, {"title": "Factorizing complex models: A case study in mention detection", "author": ["Florian et al.2006] Radu Florian", "Hongyan Jing", "Nanda Kambhatla", "Imed Zitouni"], "venue": null, "citeRegEx": "Florian et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Florian et al\\.", "year": 2006}, {"title": "Improving mention detection robustness to noisy input", "author": ["Florian et al.2010] Radu Florian", "John Pitrelli", "Salim Roukos", "Imed Zitouni"], "venue": null, "citeRegEx": "Florian et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Florian et al\\.", "year": 2010}, {"title": "Multilingual language processing from bytes", "author": ["Gillick et al.2015] Dan Gillick", "Cliff Brunk", "Oriol Vinyals", "Amarnag Subramanya"], "venue": "In arXiv preprint arXiv:1512.00103", "citeRegEx": "Gillick et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Gillick et al\\.", "year": 2015}, {"title": "Improved relation extraction with feature-rich compositional embedding models", "author": ["Mo Yu", "Mark Dredze"], "venue": null, "citeRegEx": "Gormley et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Gormley et al\\.", "year": 2015}, {"title": "A novel connectionist system for unconstrained handwriting recognition", "author": ["Graves et al.2009] A. Graves", "Marcus EichenbergerLiwicki", "S. Fernandez", "R. Bertolami", "H. Bunke", "J. Schmidhuber"], "venue": "In IEEE Transactions on Pattern Analysis and Machine", "citeRegEx": "Graves et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Graves et al\\.", "year": 2009}, {"title": "Long short-term memory", "author": ["Hochreiter", "Schmidhuber1997] Sepp Hochreiter", "Jurgen Schmidhuber"], "venue": "In Neural Computation", "citeRegEx": "Hochreiter et al\\.,? \\Q1997\\E", "shortCiteRegEx": "Hochreiter et al\\.", "year": 1997}, {"title": "Exploring representation-learning approaches to domain adaptation. In The ACL Workshop on Domain Adaptation for Natural Language Processing (DANLP)", "author": ["Huang", "Yates2010] Fei Huang", "Alexander Yates"], "venue": null, "citeRegEx": "Huang et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Huang et al\\.", "year": 2010}, {"title": "Serial order: A parallel distributed processing approach", "author": ["Michael I. Jordan"], "venue": "In Tech. Rep. No. 8604", "citeRegEx": "Jordan.,? \\Q1986\\E", "shortCiteRegEx": "Jordan.", "year": 1986}, {"title": "A convolutional neural network for modelling sentences", "author": ["Edward Grefenstette", "Phil Blunsom"], "venue": null, "citeRegEx": "Kalchbrenner et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Kalchbrenner et al\\.", "year": 2014}, {"title": "Joint entity and relation extraction using card-pyramid parsing", "author": ["Kate", "Mooney2010] J. Rohit Kate", "Raymond Mooney"], "venue": "In CoNLL", "citeRegEx": "Kate et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Kate et al\\.", "year": 2010}, {"title": "Convolutional neural networks for sentence classification", "author": ["Yoon Kim"], "venue": "In EMNLP", "citeRegEx": "Kim.,? \\Q2014\\E", "shortCiteRegEx": "Kim.", "year": 2014}, {"title": "Conditional random fields: Probabilistic models for segmenting and labeling sequence data", "author": ["Andrew McCallum", "Fernando Pereira"], "venue": null, "citeRegEx": "Lafferty et al\\.,? \\Q2001\\E", "shortCiteRegEx": "Lafferty et al\\.", "year": 2001}, {"title": "Incremental joint extraction of entity mentions and relations", "author": ["Li", "Ji2014a] Qi Li", "Heng Ji"], "venue": null, "citeRegEx": "Li et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Li et al\\.", "year": 2014}, {"title": "Constructing information networks using one single model", "author": ["Li et al.2014b] Qi Li", "Heng Ji", "Yu Hong", "Sujian Li"], "venue": null, "citeRegEx": "Li et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Li et al\\.", "year": 2014}, {"title": "Phrase clustering for discriminative learning", "author": ["Lin", "Wu2009] Dekang Lin", "Xiaoyun Wu"], "venue": "In ACL-IJCNLP", "citeRegEx": "Lin et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Lin et al\\.", "year": 2009}, {"title": "Investigation of recurrent neural network architectures and learning methods for spoken language understanding", "author": ["Xiaodong He", "Li Deng", "Yoshua Bengio"], "venue": "In Interspeech", "citeRegEx": "Mesnil et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Mesnil et al\\.", "year": 2013}, {"title": "Efficient estimation of word representations in vector space. In ICLR", "author": ["Kai Chen", "Greg Corrado", "Jeffrey Dean"], "venue": null, "citeRegEx": "Mikolov et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Mikolov et al\\.", "year": 2013}, {"title": "Distributed representations of words and phrases and their compositionality", "author": ["Ilya Sutskever", "Kai Chen", "Greg Corrado", "Jeffrey Dean"], "venue": null, "citeRegEx": "Mikolov et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Mikolov et al\\.", "year": 2013}, {"title": "Name tagging with word clusters and discriminative training", "author": ["Miller et al.2004] Scott Miller", "Jethran Guinness", "Alex Zamanian"], "venue": "HLTNAACL", "citeRegEx": "Miller et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Miller et al\\.", "year": 2004}, {"title": "A scalable hierarchical distributed language model", "author": ["Mnih", "Hinton2008] Andriy Mnih", "Geoffrey Hinton"], "venue": null, "citeRegEx": "Mnih et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Mnih et al\\.", "year": 2008}, {"title": "A focused backpropagation algorithm for temporal pattern recognition", "author": ["Michael C. Mozer"], "venue": "In Complex Systems", "citeRegEx": "Mozer.,? \\Q1989\\E", "shortCiteRegEx": "Mozer.", "year": 1989}, {"title": "Employing word representations and regularization for domain adaptation of relation extraction", "author": ["Nguyen", "Grishman2014] Thien Huu Nguyen", "Ralph Grishman"], "venue": null, "citeRegEx": "Nguyen et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Nguyen et al\\.", "year": 2014}, {"title": "2015b. Event detection and domain adaptation with convolutional neural networks. In ACL-IJCNLP", "author": ["Nguyen", "Grishman2015b] Thien Huu Nguyen", "Ralph Grishman"], "venue": null, "citeRegEx": "Nguyen et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Nguyen et al\\.", "year": 2015}, {"title": "Semantic representations for domain adaptation: A case study on the tree kernel-based method for relation extraction", "author": ["Barbara Plank", "Ralph Grishman"], "venue": "ACL-IJCNLP", "citeRegEx": "Nguyen et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Nguyen et al\\.", "year": 2015}, {"title": "Learning multilingual named entity recognition from wikipedia", "author": ["Nothman et al.2013] Joel Nothman", "Nicky Ringland", "Will Radford", "Tara Murphy", "James R Curran"], "venue": null, "citeRegEx": "Nothman et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Nothman et al\\.", "year": 2013}, {"title": "On the difficulty of training recurrent neural networks", "author": ["Tomas Mikolov", "Yoshua Bengio"], "venue": "In arXiv preprint arXiv:1211.5063", "citeRegEx": "Pascanu et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Pascanu et al\\.", "year": 2012}, {"title": "Lexicon infused phrase embeddings for named entity resolution", "author": ["Vineet Kumar", "Andrew McCallum"], "venue": "In Proceedings of the Eighteenth Conference on Computational Natural Language Learning", "citeRegEx": "Passos et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Passos et al\\.", "year": 2014}, {"title": "Embedding semantic similarity in tree kernels for domain adaptation of relation extraction", "author": ["Plank", "Moschitti2013] Barbara Plank", "Alessandro Moschitti"], "venue": null, "citeRegEx": "Plank et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Plank et al\\.", "year": 2013}, {"title": "Design challenges and misconceptions in named entity recognition", "author": ["Ratinov", "Roth2009] Lev Ratinov", "Dan Roth"], "venue": "In CoNLL", "citeRegEx": "Ratinov et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Ratinov et al\\.", "year": 2009}, {"title": "Named entity recognition in tweets: An experimental study", "author": ["Ritter et al.2011] Alan Ritter", "Sam Clark", "Mausam", "Oren Etzioni"], "venue": null, "citeRegEx": "Ritter et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Ritter et al\\.", "year": 2011}, {"title": "Global inference for entity and relation identification via a linear programming formulation", "author": ["Roth", "Yih2007] D. Roth", "W. Yih"], "venue": null, "citeRegEx": "Roth et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Roth et al\\.", "year": 2007}, {"title": "Flors: Fast and simple domain adaptation for part-of-speech tagging", "author": ["Schnabel", "Sch\u00fctze2014] Tobias Schnabel", "Hinrich Sch\u00fctze"], "venue": "In Transactions of the Association of Computational Linguistics", "citeRegEx": "Schnabel et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Schnabel et al\\.", "year": 2014}, {"title": "Learning semantic representations using convolutional neural networks for web", "author": ["Shen et al.2014] Yelong Shen", "Xiaodong He", "Jianfeng Gao", "Li Deng", "Gregoire Mesnil"], "venue": null, "citeRegEx": "Shen et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Shen et al\\.", "year": 2014}, {"title": "Semantic compositionality through recursive matrix-vector spaces", "author": ["Brody Huval", "Christopher D. Manning", "Andrew Y. Ng"], "venue": null, "citeRegEx": "Socher et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Socher et al\\.", "year": 2012}, {"title": "Semi-supervised sequential labeling and segmentation using giga-word scale unlabeled data", "author": ["Suzuki", "Isozaki2008] Jun Suzuki", "Hideki Isozaki"], "venue": "In ACL-HLT", "citeRegEx": "Suzuki et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Suzuki et al\\.", "year": 2008}, {"title": "Improved semantic representations from tree-structured long short-term memory networks. In ACL-IJCNLP", "author": ["Tai et al.2015] Kai Sheng Tai", "Richard Socher", "Christopher D. Manning"], "venue": null, "citeRegEx": "Tai et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Tai et al\\.", "year": 2015}, {"title": "Introduction to the conll-2003 shared task: Languageindependent named entity recognition", "author": ["Tjong Kim Sang", "Fien De Meulder"], "venue": "In CoNLL", "citeRegEx": "Sang et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Sang et al\\.", "year": 2003}, {"title": "Introduction to the conll-2002 shared task: Language-independent named entity recognition", "author": [], "venue": "In CoNLL", "citeRegEx": "Sang.,? \\Q2002\\E", "shortCiteRegEx": "Sang.", "year": 2002}, {"title": "Word representations: A simple and general method for semi-supervised learning", "author": ["Turian et al.2010] Joseph Turian", "Lev-Arie Ratinov", "Yoshua Bengio"], "venue": null, "citeRegEx": "Turian et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Turian et al\\.", "year": 2010}, {"title": "Domain adaptation for sequence labeling tasks with a probabilistic language adaptation model", "author": ["Xiao", "Guo2013] Min Xiao", "Yuhong Guo"], "venue": null, "citeRegEx": "Xiao et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Xiao et al\\.", "year": 2013}, {"title": "Recurrent neural networks for language understanding", "author": ["Yao et al.2013] Kaisheng Yao", "Geoffrey Zweig", "MeiYuh Hwang", "Yangyang Shi", "Dong Yu"], "venue": "In Interspeech. Interspeech", "citeRegEx": "Yao et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Yao et al\\.", "year": 2013}, {"title": "Spoken language understanding using long shortterm memory", "author": ["Yao et al.2014] Kaisheng Yao", "Baolin Peng", "Yu Zhang", "Dong Yu", "Geoffrey Zweig", "Yangyang Shi"], "venue": null, "citeRegEx": "Yao et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Yao et al\\.", "year": 2014}, {"title": "Semantic parsing for single-relation question answering", "author": ["Yih et al.2014] Wen-tau Yih", "Xiaodong He", "Christopher Meek"], "venue": null, "citeRegEx": "Yih et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Yih et al\\.", "year": 2014}, {"title": "Adadelta: An adaptive learning rate method", "author": ["Matthew D. Zeiler"], "venue": "In CoRR,", "citeRegEx": "Zeiler.,? \\Q2012\\E", "shortCiteRegEx": "Zeiler.", "year": 2012}, {"title": "Relation classification via convolutional deep neural network. In COLING", "author": ["Zeng et al.2014] Daojian Zeng", "Kang Liu", "Siwei Lai", "Guangyou Zhou", "Jun Zhao"], "venue": null, "citeRegEx": "Zeng et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Zeng et al\\.", "year": 2014}, {"title": "Endto-end learning of semantic role labeling using recurrent neural networks. In ACL-IJCNLP", "author": ["Zhou", "Xu2015] Jie Zhou", "Wei Xu"], "venue": null, "citeRegEx": "Zhou et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Zhou et al\\.", "year": 2015}, {"title": "Mention detection crossing the language barrier", "author": ["Zitouni", "Florian2008] Imed Zitouni", "Radu Florian"], "venue": null, "citeRegEx": "Zitouni et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Zitouni et al\\.", "year": 2008}], "referenceMentions": [{"referenceID": 29, "context": "Random Fields (CRFs) (Lafferty et al., 2001).", "startOffset": 21, "endOffset": 44}, {"referenceID": 5, "context": "The problem might originate from various mismatches between the source and the target domains (domain shifts) such as the vocabulary difference, the distribution mismatches etc (Blitzer et al., 2006; Daume, 2007; Plank and Moschitti, 2013).", "startOffset": 177, "endOffset": 239}, {"referenceID": 1, "context": "The recent emerging interest in deep learning has produced many successful applications of RNNs for NLP problems such as machine translation (Cho et al., 2014a; Bahdanau et al., 2015), semantic role labeling (Zhou and Xu, 2015) etc.", "startOffset": 141, "endOffset": 183}, {"referenceID": 17, "context": ", 2014; Cherry and Guo, 2015) and mention detection (Florian et al., 2004) have been extensively studied with various evaluation in the last decades: MUC6, MUC7, CoNLL\u201902, CoNLL\u201903 and ACE.", "startOffset": 52, "endOffset": 74}, {"referenceID": 18, "context": "The previous work on MD has examined the cascade models (Florian et al., 2006), transferred knowledge from rich-resource languages to", "startOffset": 56, "endOffset": 78}, {"referenceID": 19, "context": "low-resource ones via machine translation (Zitouni and Florian, 2008) or improved the systems on noisy input (Florian et al., 2010).", "startOffset": 109, "endOffset": 131}, {"referenceID": 51, "context": "dos Santos and Guimar\u00e3es, 2015b), recurrent/recursive neural networks (Socher et al., 2012; Cho et al., 2014a; Bahdanau et al., 2015; Zhou and Xu, 2015; Tai et al., 2015), to name a few.", "startOffset": 70, "endOffset": 170}, {"referenceID": 1, "context": "dos Santos and Guimar\u00e3es, 2015b), recurrent/recursive neural networks (Socher et al., 2012; Cho et al., 2014a; Bahdanau et al., 2015; Zhou and Xu, 2015; Tai et al., 2015), to name a few.", "startOffset": 70, "endOffset": 170}, {"referenceID": 53, "context": "dos Santos and Guimar\u00e3es, 2015b), recurrent/recursive neural networks (Socher et al., 2012; Cho et al., 2014a; Bahdanau et al., 2015; Zhou and Xu, 2015; Tai et al., 2015), to name a few.", "startOffset": 70, "endOffset": 170}, {"referenceID": 1, "context": ", 2014a; Bahdanau et al., 2015; Zhou and Xu, 2015; Tai et al., 2015), to name a few. For NER, Collobert et al. (2011) propose a CNN-based framework while Mesnil et al.", "startOffset": 9, "endOffset": 118}, {"referenceID": 1, "context": ", 2014a; Bahdanau et al., 2015; Zhou and Xu, 2015; Tai et al., 2015), to name a few. For NER, Collobert et al. (2011) propose a CNN-based framework while Mesnil et al. (2013) and Yao et al.", "startOffset": 9, "endOffset": 175}, {"referenceID": 21, "context": "Recent work has drawn attention to relation extraction (Plank and Moschitti, 2013; Nguyen et al., 2015a; Gormley et al., 2015).", "startOffset": 55, "endOffset": 126}, {"referenceID": 21, "context": ", 2015a; Gormley et al., 2015). In the field of neural networks, to the best of our knowledge, there is only one work from Nguyen and Grishman (2015b) that evaluates CNNs for event detection in the cross-domain setting.", "startOffset": 9, "endOffset": 151}, {"referenceID": 33, "context": "dow of vc for each word in the sentence to capture the short-term dependencies for prediction (Mesnil et al., 2013).", "startOffset": 94, "endOffset": 115}, {"referenceID": 15, "context": "\u2022 In the Elman model (Elman, 1990), called ELMAN, the hidden vector from the previous step hi\u22121, along with the input in the current step xi, constitute the inputs to compute the current hidden state hi:", "startOffset": 21, "endOffset": 34}, {"referenceID": 25, "context": "\u2022 In the Jordan model (Jordan, 1986), called JORDAN, the output vector from the previous step oi\u22121 is fed into the current hidden layer rather than the hidden vector from the previous steps hi\u22121.", "startOffset": 22, "endOffset": 36}, {"referenceID": 3, "context": "\u201cvanishing gradient\u201d and \u201cexploding gradient\u201d problems (Bengio et al., 1994), making it challenging to train the networks properly in practice (Pascanu et al.", "startOffset": 55, "endOffset": 76}, {"referenceID": 43, "context": ", 1994), making it challenging to train the networks properly in practice (Pascanu et al., 2012).", "startOffset": 74, "endOffset": 96}, {"referenceID": 22, "context": "These problems are addressed by the long-short term memory units (LSTM) (Hochreiter and Schmidhuber, 1997; Graves et al., 2009) that propose the idea of memory cells with four gates to allow the information storage and access over a long period of time.", "startOffset": 72, "endOffset": 127}, {"referenceID": 1, "context": "comparable performance in machine translation (Bahdanau et al., 2015).", "startOffset": 46, "endOffset": 69}, {"referenceID": 1, "context": "comparable performance in machine translation (Bahdanau et al., 2015). The introduction of GRUs into the models ELMAN and JORDAN amounts to two new models, named ELMAN_GRU and JORDAN_GRU respectively, with two new methods to compute the hidden vectors hi. The formula for ELMAN_GRU is adopted directly from Cho et al. (2014b) and given below:", "startOffset": 47, "endOffset": 326}, {"referenceID": 1, "context": "The contextual networks are motivated by the RNN Encoder-Decoder models that have become very popular in neural machine translation recently (Cho et al., 2014a; Bahdanau et al., 2015).", "startOffset": 141, "endOffset": 183}, {"referenceID": 33, "context": "The model on the right is from Mesnil et al. (2013) with the forward and backward context size of 1.", "startOffset": 31, "endOffset": 52}, {"referenceID": 33, "context": "We notice that Mesnil et al. (2013) also investigate the bidirectional models for the task of slot filling in spoken language understanding.", "startOffset": 15, "endOffset": 36}, {"referenceID": 33, "context": "We notice that Mesnil et al. (2013) also investigate the bidirectional models for the task of slot filling in spoken language understanding. However, compared to the work presented here, Mesnil et al. (2013) does not use any special transition memory cells (like the GRUs we are employing", "startOffset": 15, "endOffset": 208}, {"referenceID": 43, "context": "in this paper) to avoid numerical stability issues (Pascanu et al., 2012).", "startOffset": 51, "endOffset": 73}, {"referenceID": 38, "context": "The gradients are computed using the back-propagation through time algorithm (Mozer, 1989) and inference is performed by running the networks over the whole sentences and taking argmax over the output sequence: yi = argmax(oi).", "startOffset": 77, "endOffset": 90}, {"referenceID": 11, "context": "Following Collobert et al. (2011), we pre-train word embeddings from a large corpus and em-", "startOffset": 10, "endOffset": 34}, {"referenceID": 61, "context": "We try the AdaDelta algorithm (Zeiler, 2012) and the dropout regularization but do not see much difference.", "startOffset": 30, "endOffset": 44}, {"referenceID": 12, "context": "Tjong Kim Sang and De Meulder, 2003) and compare the performance with the state-ofthe-art neural network system on this dataset (Collobert et al., 2011).", "startOffset": 128, "endOffset": 152}, {"referenceID": 7, "context": "Regarding the robustness across languages, we further evaluate the RNN models on the CoNLL 2002 dataset for Dutch Named Entity Recognition7 (Carreras et al., 2002; Tjong Kim Sang, 2002).", "startOffset": 140, "endOffset": 185}, {"referenceID": 11, "context": "is applied for the English CoNLL dataset to ensure the compatibility with Collobert et al. (2011).", "startOffset": 74, "endOffset": 98}, {"referenceID": 2, "context": "Following Baroni et al. (2014), we use the context window of 5, subsampling set to 1e-05 and negative sampling with the number of instances set to 10.", "startOffset": 10, "endOffset": 31}, {"referenceID": 33, "context": "Table 5 compares the performance of the bidirectional model from Mesnil et al. (2013), called MESNIL, and the BIDIRECT model.", "startOffset": 65, "endOffset": 86}, {"referenceID": 33, "context": "Table 5 compares the performance of the bidirectional model from Mesnil et al. (2013), called MESNIL, and the BIDIRECT model. In order to verify the effectiveness of recurrence in decoding, the performance of MESNIL incorporated with the JORDAN_GRU model in the decoding phase (MESNIL+JORDAN_GRU) is also reported. In general, we see that the bidirectional model in this work is much better than the model in Mesnil et al. (2013) for MD.", "startOffset": 65, "endOffset": 430}, {"referenceID": 33, "context": "Table 5 compares the performance of the bidirectional model from Mesnil et al. (2013), called MESNIL, and the BIDIRECT model. In order to verify the effectiveness of recurrence in decoding, the performance of MESNIL incorporated with the JORDAN_GRU model in the decoding phase (MESNIL+JORDAN_GRU) is also reported. In general, we see that the bidirectional model in this work is much better than the model in Mesnil et al. (2013) for MD. This is significant with Model P R F1 MESNIL (2013) 81.", "startOffset": 65, "endOffset": 490}, {"referenceID": 33, "context": "Table 5: Comparison to Mesnil et al. (2013).", "startOffset": 23, "endOffset": 44}, {"referenceID": 16, "context": "In this section, besides comparing the RNN systems above with these state-of-the-art systems, we also implement a Maximum Entropy Markov Model (MEMM) system10, following the description and features in Florian et al. (2004; 2006), and include it in the comparison for completeness11 . For this comparison, following Li and Ji (2014a), we remove the documents from the two informal domains cts", "startOffset": 202, "endOffset": 334}, {"referenceID": 11, "context": "On the CoNLL 2003 dataset for English NER, the best neural network system so far is Collobert et al. (2011). This system, called CNN-Sentence, em-", "startOffset": 84, "endOffset": 108}, {"referenceID": 11, "context": "As we can see from the table, the RNN systems are on par with the CNN-Sentence system from Collobert et al. (2011) except the CONTEXT system that is worse in this case.", "startOffset": 91, "endOffset": 115}, {"referenceID": 7, "context": "The state-of-the-art performance for this dataset is due to Carreras et al. (2002) in the CoNLL 2002 evaluation and Nothman et al.", "startOffset": 60, "endOffset": 83}, {"referenceID": 7, "context": "The state-of-the-art performance for this dataset is due to Carreras et al. (2002) in the CoNLL 2002 evaluation and Nothman et al. (2013). Very recently, while we were preparing this paper, Gillick el al.", "startOffset": 60, "endOffset": 138}, {"referenceID": 7, "context": "The state-of-the-art performance for this dataset is due to Carreras et al. (2002) in the CoNLL 2002 evaluation and Nothman et al. (2013). Very recently, while we were preparing this paper, Gillick el al. (2015) introduce a multilingual language processing system and also report the performance on this dataset.", "startOffset": 60, "endOffset": 212}, {"referenceID": 42, "context": "05 Nothman et al. (2013) - - 78.", "startOffset": 3, "endOffset": 25}, {"referenceID": 42, "context": "05 Nothman et al. (2013) - - 78.60 Gillick el al. (2015) - - 78.", "startOffset": 3, "endOffset": 57}, {"referenceID": 42, "context": "05 Nothman et al. (2013) - - 78.60 Gillick el al. (2015) - - 78.08 Gillick el al. (2015)* - - 82.", "startOffset": 3, "endOffset": 89}], "year": 2016, "abstractText": "One of the key challenges in natural language processing (NLP) is to yield good performance across application domains and languages. In this work, we investigate the robustness of the mention detection systems, one of the fundamental tasks in information extraction, via recurrent neural networks (RNNs). The advantage of RNNs over the traditional approaches is their capacity to capture long ranges of context and implicitly adapt the word embeddings, trained on a large corpus, into a task-specific word representation, but still preserve the original semantic generalization to be helpful across domains. Our systematic evaluation for RNN architectures demonstrates that RNNs not only outperform the best reported systems (up to 9% relative error reduction) in the general setting but also achieve the state-of-the-art performance in the cross-domain setting for English. Regarding other languages, RNNs are significantly better than the traditional methods on the similar task of named entity recognition for Dutch (up to 22% relative error reduction).", "creator": "LaTeX with hyperref package"}}}