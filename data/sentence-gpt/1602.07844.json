{"id": "1602.07844", "review": {"conference": "AAAI", "VERSION": "v1", "DATE_OF_SUBMISSION": "25-Feb-2016", "title": "Fast Nonsmooth Regularized Risk Minimization with Continuation", "abstract": "In regularized risk minimization, the associated optimization problem becomes particularly difficult when both the loss and regularizer are nonsmooth. Existing approaches either have slow or unclear convergence properties, are restricted to limited problem subclasses, or require careful setting of a smoothing parameter. In this paper, we propose a continuation algorithm that is applicable to a large class of nonsmooth regularized risk minimization problems, can be flexibly used with a number of existing solvers for the underlying smoothed subproblem, and with convergence results on the whole algorithm rather than just one of its subproblems. In particular, when accelerated solvers are used, the proposed algorithm achieves the fastest known rates of $O(1/T^2)$ on strongly convex problems, and $O(1/T)$ on general convex problems. Experiments on nonsmooth classification and regression tasks demonstrate that the proposed algorithm outperforms the state-of-the-art. The proposed algorithm simplifies the state of the state of the entire process by approximating the probability of the algorithm being in the state of a class of nonsmooth regularized risk minimization problems and the performance of the predicted model using a large class of nonsmooth regularized risk minimization problems. In particular, if the proposed algorithm outperforms the state of the whole process, it should outperform the state of the entire process by approximating the probability of the algorithm being in the state of a class of nonsmooth regularized risk minimization problems and the performance of the predicted model using a large class of nonsmooth regularized risk minimization problems and the performance of the predicted model using a large class of nonsmooth regularized risk minimization problems and the performance of the predicted model using a large class of nonsmooth regularized risk minimization problems and the performance of the predicted model using a large class of nonsmooth regularized risk minimization problems and the performance of the predicted model using a large class of nonsmooth regularized risk minimization problems and the performance of the predicted model using a large class of nonsmooth regularized risk minimization problems and the performance of the predicted model using a large class of nonsmooth regularized risk minimization problems and the performance of the predicted model using a large class of nonsmooth regularized risk minimization problems and the performance of the predicted model using a large class of nonsmooth regularized risk minimization problems and the performance of the predicted model using a large class of nonsmooth", "histories": [["v1", "Thu, 25 Feb 2016 08:34:59 GMT  (258kb,D)", "http://arxiv.org/abs/1602.07844v1", "AAAI-2016"]], "COMMENTS": "AAAI-2016", "reviews": [], "SUBJECTS": "cs.LG math.OC stat.ML", "authors": ["shuai zheng 0004", "ruiliang zhang", "james t kwok"], "accepted": true, "id": "1602.07844"}, "pdf": {"name": "1602.07844.pdf", "metadata": {"source": "CRF", "title": "Fast Nonsmooth Regularized Risk Minimization with Continuation", "authors": ["Shuai Zheng", "Ruiliang Zhang", "James T. Kwok"], "emails": ["jamesk}@cse.ust.hk"], "sections": [{"heading": "Introduction", "text": "In regularized risk minimization, one has to minimize the sum of an empirical loss and a regularizer. When both are smooth, it can be easily optimized by a variety of solvers (Nesterov 2004). In particular, a popular choice for big data applications is stochastic gradient descent (SGD), which is easy to implement and highly scalable (Kushner and Yin 2003). For many nonsmooth regularizers (such as the `1 and nuclear norm regularizers), the corresponding regularized risks can still be efficiently minimized by the proximal gradient algorithm and its accelerated variants (Nesterov 2013). However, when the regularizer is smooth but the loss is nonsmooth (e.g., the hinge loss and absolute loss), or when both the loss and regularizer are nonsmooth, proximal gradient algorithms are not directly applicable.\nOn nonsmooth problems, SGD can still be used, by simply replacing the gradient with subgradient. However, the information contained in the subgradient is much less informative (Nemirovski and Yudin 1983), and convergence is hindered. On general convex problems, SGD converges at a rate of O(log T/ \u221a T ), where T is the number of iterations; whereas on strongly convex problems, the rate is O(log T/T ). In contrast, its smooth counterparts converge\nCopyright c\u00a9 2016, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved.\nwith the much faster O(1/ \u221a T ) and O(1/T ) rates, respectively (Rakhlin, Shamir, and Sridharan 2012; Shamir and Zhang 2013). Recently, Shamir and Zhang (2013) recovered these rates by using a polynomial-decay averaging scheme on the SGD iterates. However, a major drawback is that it does not exploit properties of the regularizer. For example, when used with a sparsity-inducing regularizer, its solution obtained may not be sparse (Duchi and Singer 2009).\nNesterov (2005b) proposed to smooth the nonsmooth objective so that it can then be efficiently optimized. This smoothing approach is now popularly used for nonsmooth optimization. However, the optimal smoothness parameter needs to be known in advance. This restriction is later avoided by the (batch) excessive gap algorithm (Nesterov 2005a). In the stochastic setting, Ouyang and Gray (2012) combined Nesterov\u2019s smoothing with SGD. Though these methods achieve the fastest known convergence rates in the batch and stochastic settings respectively, they assume a Lipschitz-smooth regularizer, and nonsmooth regularizers (such as the sparsity-inducing regularizers) cannot be used.\nRecently, based on the observation that the training set is indeed finite, a number of fast stochastic algorithms are proposed for both smooth and composite optimization problems (Schmidt, Roux, and Bach 2013; Johnson and Zhang 2013; Xiao and Zhang 2014; Mairal 2013; Defazio, Bach, and Lacoste-Julien 2014). They are based on the idea of variance reduction, and attain comparable convergence rates as their batch counterparts. However, they are not applicable when both the loss and regularizer are nonsmooth. To alleviate this, Shalev-Shwartz and Zhang (2014) suggested running these algorithms on the smoothed approximation obtained by Nesterov\u2019s smoothing. However, as in (Nesterov 2005b), it requires a careful setting of the smoothness parameter. Over-smoothing deteriorates solution quality, while under-smoothing slows down convergence.\nThe problem of setting the smoothness parameter can be alleviated by continuation (Becker, Bobin, and Cande\u0300s 2011). It solves a sequence of smoothed problems, in which the smoothing parameter is gradually reduced from a large value (and the corresponding smoothed problem is easy to solve) to a small value (which leads to a solution closer to that of the original nonsmooth problem). Moreover, solution of the intermediate problem is used to warm-start the next smoothed problem. This approach is also similar to that\nar X\niv :1\n60 2.\n07 84\n4v 1\n[ cs\n.L G\n] 2\n5 Fe\nb 20\n16\nof gradually changing the regularization parameter in (Hale, Yin, and Zhang 2007; Wen et al. 2010; Mazumder, Hastie, and Tibshirani 2010). Empirically, continuation converges much faster than the use of a fixed smoothing parameter (Becker, Bobin, and Cande\u0300s 2011). However, the theoretical convergence rate obtained in (Becker, Bobin, and Cande\u0300s 2011) is only for one stage of the continuation algorithm (i.e., on the smoothed problem with a particular smoothing parameter), while the convergence properties for the whole algorithm are not clear. Recently, Xiao and Zhang (2012) obtained a linear convergence rate for their continuation algorithm, though only for the special case of `1-regularized least squares regression.\nIn this paper, we consider the general nonsmooth optimization setting, in which both the loss and regularizer may be nonsmooth. The proposed continuation algorithm can be flexibly used with a variety of existing batch/stochastic solvers in each stage. Theoretical analysis shows that the proposed algorithm, with this wide class of solvers, achieves the rate of O(1/T 2) on strongly convex problems, and O(1/T ) on general convex problems. These are the fastest known rates for nonsmooth optimization. Note that these rates are for the whole algorithm, not just one of its stages as in (Becker, Bobin, and Cande\u0300s 2011). Experiments on nonsmooth classification and regression models demonstrate that the proposed algorithm outperforms the state-of-the-art.\nNotation. For x, y \u2208 Rd, \u2016x\u20162 = \u221a\u2211d i=1 x 2 i is its `2-norm,\n\u2016x\u20161 = \u2211d i=1 |xi| is its `1-norm, and \u3008x, y\u3009 is the dot product between x, y. Moreover, \u2202f denotes the subdifferential of a nonsmooth function f , if f is differentiable, then \u2207f denotes its gradient. I is the identity matrix."}, {"heading": "Related Work", "text": "Consider nonsmooth functions of the form\ng(x) = g\u0302(x) + max u\u2208U\n[\u3008Ax, u\u3009 \u2212Q(u)], (1)\nwhere g\u0302 is convex, continuously differentiable with L\u0302Lipschitz-continuous gradient, U \u2286 Rp is convex, A \u2208 Rp\u00d7d, and Q is a continuous convex function. Nesterov (2005b) proposed the following smooth approximation:\ng\u0303\u03b3(x) = g\u0302(x) + max u\u2208U\n[\u3008Ax, u\u3009 \u2212Q(u)\u2212 \u03b3\u03c9(u)] , (2)\nwhere \u03b3 is a smoothness parameter, and \u03c9 is a nonnegative \u03b6-strongly convex function.\nFor example, consider the hinge loss g(x) = max(0, 1 \u2212 yiz T i x), where x is the linear model parameter, and (zi, yi) is the ith training sample with yi \u2208 {\u00b11}. Using \u03c9(u) = 1 2\u2016u\u2016 2 2, g can be smoothed to (Ouyang and Gray 2012)\ng\u0303\u03b3(x) =  0 yiz T i x \u2265 1 1\u2212 yizTi x\u2212 \u03b3 2 yiz T i x < 1\u2212 \u03b3\n1 2\u03b3 (1\u2212 yiz T i x)\n2 otherwise . (3)\nSimilarly, the `1 loss g(x) = |yi \u2212 zTi x| can be smoothed to\ng\u0303\u03b3(x) =  yi \u2212 zTi x\u2212 \u03b3 2 yi \u2212 z T i x \u2265 \u03b3 \u2212(yi \u2212 zTi x)\u2212 \u03b3 2 yi \u2212 z T i x < \u2212\u03b3\n1 2\u03b3 (yi \u2212 z T i x)\n2 otherwise . (4)\nOther examples in machine learning include popular regularizers such as the `1, total variation (Becker, Bobin, and Cande\u0300s 2011), overlapping group lasso, and graph-guided fused lasso (Chen et al. 2012).\nMinimization of the smooth (and convex) g\u0303\u03b3 can be performed efficiently using first-order methods, including the so-called \u201coptimal method\u201d and its variants (Nesterov 2005b) that achieve the optimal convergence rate."}, {"heading": "Nesterov Smoothing with Continuation", "text": "Consider the following nonsmooth minimization problem\nmin x P (x) \u2261 f(x) + r(x), (5)\nwhere both f and r are convex and nonsmooth. In machine learning, x usually corresponds to the model parameter, f is the loss, and r the regularizer. We assume that the loss f on a set of n training samples can be decomposed as f(x) = 1n \u2211n i=1 fi(x), where fi is the loss value on the ith sample. Moreover, each fi can be written as in (1), i.e., fi(x) = f\u0302i(x) + maxu\u2208U [\u3008Aix, u\u3009 \u2212Q(u)]. One can then apply Nesterov\u2019s smoothing, and P (x) in (5) is smoothed to\nP\u0303 (x) = f\u0303\u03b3(x) + r(x), (6) where f\u0303\u03b3(x) = 1n \u2211n i=1 f\u0303i(x) and\nf\u0303i(x) = f\u0302i(x) + max u\u2208U\n[\u3008Aix, u\u3009 \u2212Q(u)\u2212 \u03b3\u03c9(u)] . (7)\nAs for r, we assume that it is \u201csimple\u201d, namely that its proximal operator, prox\u03bbr(\u00b7) \u2261 argminx 12\u2016x\u2212 \u00b7\u2016\n2 + \u03bbr(x) for any \u03bb > 0, can be easily computed (Parikh and Boyd 2014)."}, {"heading": "Strongly Convex Objectives", "text": "In this section, we assume that P is \u00b5-strongly convex. This strong convexity may come from f (e.g., `2-regularized hinge loss) or r (e.g., elastic-net regularizer) or both. Assumption 1. P is \u00b5-strongly convex, i.e., there exists \u00b5 > 0 such that P (y) \u2265 P (x) + \u03beT (y \u2212 x) + \u00b52 \u2016y \u2212 x\u2016 2 2,\u2200\u03be \u2208 \u2202P (x) and x, y \u2208 Rd. The proposed algorithm is based on continuation. It proceeds in stages, and a smoothed problem is solved in each stage (Becker, Bobin, and Cande\u0300s 2011). The smoothness parameter is gradually reduced across stages, so that the smoothed problem becomes closer and closer to the original one. In each stage, an iterative solverM is used to solve the smoothed problem. It returns an approximate solution, which is then used to warm-start the next stage.\nIn stage s, let the smoothness parameter be \u03b3s, the smoothed objective in (6) be P\u0303s(x), x\u2217s = argminx P\u0303s(x), and x\u0303s be the solution returned by M. As M is warmstarted by x\u0303s\u22121, the error before runningM is P\u0303s(x\u0303s\u22121) \u2212 P\u0303s(x \u2217 s). At the end of stage s, we assume that the error is reduced by a factor of \u03c1s. The expectation E below is over the stochastic choice of training samples for a stochastic solver. For a deterministic solver, this expectation can be dropped.\nAssumption 2. EP\u0303s(x\u0303s) \u2212 P\u0303s(x\u2217s) \u2264 \u03c1s(P\u0303s(x\u0303s\u22121) \u2212 P\u0303s(x \u2217 s)), where \u03c1s \u2208 (0, 1).\nWe consider two types of solvers, which differ in the number of iterations (Ts) it takes to satisfy Assumption 2.\n1. Non-accelerated solvers: Ts = a\u03bas\u03c6(\u03c1s) + b\u03c6(\u03c1s) + c; 2. Accelerated solvers: Ts = a \u221a \u03bas\u03c6(\u03c1s) + b\u03c6(\u03c1s) + c.\nHere, \u03bas is the condition number of the objective, a, b, c \u2265 0 are constants not related to \u03bas and \u03c6(\u03c1s). Moreover, \u03c6 satisfies (i) \u03c6(\u03c1s) > 0 and non-increasing for \u03c1s \u2208 (0, 1); (ii) \u03c6(\u03c1s) is not related to \u03bas. Note that when \u03bas is large (as is typical when the smoothed problem approaches the original problem), non-accelerated solvers need a larger Ts than accelerated solvers. Table 1 shows some non-accelerated and accelerated solvers popularly used in machine learning.\nAlgorithm 1 shows the proposed procedure, which will be called CNS (Continuation for NonSmooth optimization). It is similar to that in (Becker, Bobin, and Cande\u0300s 2011), which however does not have convergence results. Moreover, a small but important difference is that Algorithm 1 specifies how Ts should be updated across stages, and this is essential for proving convergence. Note the different update options for non-accelerated and accelerated solvers.\nAlgorithm 1 CNS algorithm for strongly convex problems. 1: Input: number of iterations T1 and smoothness param-\neter \u03b31 for stage 1, and shrinking parameter \u03c4 > 1. 2: Initialize: x\u03030. 3: for s = 1, 2, . . . do 4: P\u0303s \u2190 smooth P with smoothing parameter \u03b3s; 5: x\u0303s \u2190 minimize P\u0303s(x) by running M for Ts iterations; 6: \u03b3s+1 = \u03b3s/\u03c4 ; 7: Option I (non-accelerated solvers): Ts+1 = \u03c4Ts; 8: Option II (accelerated solvers): Ts+1 = \u221a \u03c4Ts;\n9: end for 10: Output: x\u0303s.\nThe following Lemma shows that when T1 is large enough, error reduction can be guaranteed across all stages.\nLemma 1. For both non-accelerated and accelerated solvers, if T1 is large enough such that \u03c11 \u2264 1/\u03c42, then \u03c1s \u2264 1/\u03c42 for all s > 1.\nIf \u03ba1 is known, a sufficiently large T1 can be obtained from Table 1; otherwise, we can obtain T1 by ensuring P\u03031(x\u03031) \u2264 P\u03031(x\u03030)/\u03c42, which then implies P\u03031(x\u03031) \u2212 P\u03031(x \u2217 1) \u2264 (P\u03031(x\u03030)\u2212 P\u03031(x\u22171))/\u03c42.\nConvergence when Non-Accelerated Solver is used Let x\u2217 = argminx P (x), and Du = maxu\u2208U \u03c9(u). The following Lemma shows that if x is an -accurate solution of the P\u0303s (i.e., P\u0303s(x)\u2212 P\u0303s(x\u2217s) \u2264 ), it is also an ( + \u03b3sDu)accurate solution of the original objective P .\nLemma 2. P\u0303s(x) \u2212 P\u0303s(x\u2217s) \u2212 \u03b3sDu \u2264 P (x) \u2212 P (x\u2217) \u2264 P\u0303s(x)\u2212 P\u0303s(x\u2217s) + \u03b3sDu.\nSince Lemma 2 holds for any x, it also holds in expectation, i.e., EP\u0303s(x\u0303s)\u2212 P\u0303s(x\u2217s)\u2212 \u03b3sDu \u2264 EP (x\u0303s)\u2212P (x\u2217) \u2264 EP\u0303s(x\u0303s)\u2212 P\u0303s(x\u2217s) + \u03b3sDu. Theorem 1. Assume that T1 in Algorithm 1 is large enough so that \u03c11 \u2264 1/\u03c42. When non-accelerated solvers are used,\nEP (x\u0303S)\u2212P (x\u2217)\u2264 ( S\u220f s=1 \u03c1s ) (P (x\u03030)\u2212P (x\u2217))+O ( \u03b31Du T ) , (8)\nwhere S is the number of stages, T = \u2211S s=1 Ts, and\u220fS\ns=1 \u03c1s = O(1/T 2).\nThe first term on the RHS of (8) reflects the cumulative decrease of the objective after S stages, while the second term is due to smoothing. The condition \u03c11 \u2264 1/\u03c42 is used to obtain theO(1/T ) rate in the last term of (8). If we instead require that \u03c11 \u2264 1/\u03c4 , it can be shown that the rate will be slowed to O(log T/T ); if \u03c11 \u2264 1/ \u221a \u03c4 , it degrades further to\nO(1/ \u221a T ). On the other hand, if \u03c11 \u2264 1/\u03c4 c with c > 2, the rate will not be improved. Corollary 1. Together with Lemma 1, we have\nEP (x\u0303S)\u2212 P (x\u2217)\u2264 P (x\u03030)\u2212 P (x\u2217)\n\u03c42S +O ( \u03b31Du T ) , (9)\nwhere 1/\u03c42S = O(1/T 2). Existing stochastic algorithms such as SGD, FOBOS and RDA have a convergence rate of O(log T/T ) (Rakhlin, Shamir, and Sridharan 2012; Duchi and Singer 2009; Xiao 2009), while here we have the faster O(1/T ) rate. Recent\nworks in (Shamir and Zhang 2013; Ouyang and Gray 2012) also achieve a O(1/T ) rate. However, Shamir and Zhang (2013) use stochastic subgradient, and do not exploit properties of the regularizer (such as sparsity). This can lead to inferior performance (Duchi and Singer 2009; Xiao 2009; Mazumder, Hastie, and Tibshirani 2010). On the other hand, (Ouyang and Gray 2012) is restricted to r \u2261 0 in (5).\nNext, we compare with the case where continuation is not used (i.e., \u03b3s is a constant). Equivalently, this corresponds to setting \u03c4 = 1 in Algorithm 1. Proposition 1. When continuation is not used, let \u03c1 \u2208 (0, 1) be the error reduction factor at each stage, and \u03b3 > 0 be the fixed smoothing parameter. When either an accelerated or non-accelerated solver is used,\nEP (x\u0303S)\u2212P (x\u2217)\u2264\u03c1S(P (x\u03030)\u2212P (x\u2217))+(1+\u03c1S)\u03b3Du.(10) Proposition 2. Assume that the two terms on the RHS of (9) and (10) are equal to \u03b1 and (1 \u2212 \u03b1) , respectively, where \u03b1 > 0 and > 0. Let \u03c11 = \u03c1 = 1/\u03c42 in (8) and (10). Assume that Algorithm 1 needs a total of T iterations to obtain an -accurate solution, while its fixed-\u03b3s variant takes T \u2032 iterations. Then,\nT \u2265 \u03c4 S \u2212 1 \u03c4 \u2212 1\n( a\u03ba1\u03c6 ( 1\n\u03c42\n) +b\u03c6 ( 1\n\u03c42\n) +c ) ,\nT \u2032\u2265S ( a ( \u03c42S + 1\n\u03c4S+1 + \u03c4S \u03ba1+C\n) \u03c6 ( 1\n\u03c42\n) +b\u03c6 ( 1\n\u03c42\n) +c ) ,\nwhere S \u2265 log (\n\u03b1 (P (x\u03030)\u2212P (x\u2217))\n) / log ( 1 \u03c42 ) , C =(\n1\u2212 \u03c4 2S+1\n\u03c4S+1+\u03c4S ) K \u00b5 , and K is a constant,\nT and T \u2032 are usually dominated by the a\u03ba1\u03c6(1/\u03c42) term, and T \u2032 is roughly S times that of T . This is also consistent with empirical observations that continuation is much faster than fixed smoothing (Becker, Bobin, and Cande\u0300s 2011).\nConvergence when Accelerated Solver is used Theorem 2. Assume that T1 in Algorithm 1 is large enough so that \u03c11 \u2264 1/\u03c42. When accelerated solvers are used,\nEP (x\u0303S)\u2212P (x\u2217)\u2264 ( S\u220f s=1 \u03c1s ) (P (x\u03030)\u2212P (x\u2217))+O ( \u03b31Du T 2 ) .\nwhere T = \u2211S s=1 Ts, and \u220fS s=1 \u03c1s = O(1/T\n4). As the \u03c1s\u2019s for non-accelerated and accelerated solvers\nare different, the \u220fS s=1 \u03c1s term here is different from that in Theorem 1. Moreover, the last term is improved from O(1/T ) in Theorem 1 to O(1/T 2) with accelerated solvers. This is also better than the rates of existing stochastic algorithms (O(log T/T ) in (Duchi and Singer 2009; Xiao 2009) andO(1/T ) in (Rakhlin, Shamir, and Sridharan 2012; Shamir and Zhang 2013; Ouyang and Gray 2012)). Besides, the black-box lower bound of O(1/T ) for strongly convex problems (Agarwal et al. 2009) does not apply here, as we have additional assumptions that the objective is of the form in (1) and the number of training samples is finite. Though the (batch) excessive gap algorithm (Nesterov 2005a) also has a O(1/T 2) rate, it is limited to r \u2261 0 in (5).\nAs in Proposition 2, the following shows that if continuation is not used, the algorithm is roughly S times slower. Proposition 3. With the same assumptions in Proposition 2,\nT \u2265 \u221a \u03c4 S \u2212 1\u221a \u03c4 \u2212 1 ( a \u221a \u03ba1\u03c6 ( 1 \u03c42 ) +b\u03c6 ( 1 \u03c42 ) +c ) ,\nT \u2032\u2265S a \u221a \u03c42S + 1\n\u03c4S+1 + \u03c4S \u03ba1+C\u03c6\n( 1\n\u03c42\n) +b\u03c6 ( 1\n\u03c42\n) +c  , where S,C are as defined in Proposition 2."}, {"heading": "General Convex Objectives", "text": "When P is not strongly convex, we add to it a small `2 term (with weight \u03bbs). We then gradually decrease \u03b3s and \u03bbs simultaneously to approach the original problem. The revised procedure is shown in Algorithm 2.\nAlgorithm 2 CNS algorithm for general convex problems. 1: Input: number of iterations T1, smoothness parameter \u03b31 and strong convexity parameter \u03bb1 for stage 1, and shrinking parameter \u03c4 > 1.\n2: Initialize: x\u03030. 3: for s = 1, 2, . . . do 4: P\u0303s \u2190 smooth P with smoothing parameter \u03b3s; 5: x\u0303s \u2190 minimize P\u0303s(x) + \u03bbs2 \u2016x\u2016 2 2 by runningM for Ts iterations; 6: \u03b3s+1 = \u03b3s/\u03c4 ; \u03bbs+1 = \u03bbs/\u03c4 ; 7: Option I (non-accelerated solvers): Ts+1 = \u03c42Ts; 8: Option II (accelerated solvers): Ts+1 = \u03c4Ts; 9: end for\n10: Output: x\u0303s.\nWe assume that there exists R > 0 such that \u2016x\u2217\u20162 \u2264 R, and \u2016x\u2217s\u20162 \u2264 R for all s. Define Hs(x) = P\u0303s(x) + \u03bbs2 \u2016x\u2016 2 2, and let x\u2217s = argminxHs(x). The following assumption is similar to that for strongly convex problems. Assumption 3. EHs(x\u0303s) \u2212 Hs(x\u2217s) \u2264 \u03c1s(Hs(x\u0303s\u22121) \u2212 Hs(x \u2217 s)), where \u03c1s \u2208 (0, 1).\nTheorem 3. Assume that T1 in Algorithm 2 is large enough so that \u03c11 \u2264 1/\u03c42. When non-accelerated solvers are used,\nEP (x\u0303S)\u2212P (x\u2217) \u2264 ( S\u220f s=1 \u03c1s )( P (x\u03030)\u2212P (x\u2217)+ \u03bb1 2 \u2016x\u03030\u201622 ) +O ( \u03bb1R\n2/2\u221a T\n) +O ( \u03b31Du\u221a T ) , (11)\nwhere \u220fS s=1 \u03c1s = O( 1 T ). For accelerated solvers,\nEP (x\u0303S)\u2212P (x\u2217) \u2264 ( S\u220f s=1 \u03c1s )( P (x\u03030)\u2212P (x\u2217)+ \u03bb1 2 \u2016x\u03030\u201622 ) +O ( \u03bb1R 2/2\nT\n) +O ( \u03b31Du T ) , (12)\nwhere \u220fS s=1 \u03c1s = O( 1 T 2 ).\nFor non-accelerated solvers, the O(1/ \u221a T ) convergence rate in (11) is only as good as that obtained in (Xiao 2009; Duchi and Singer 2009; Ouyang and Gray 2012; Shamir and Zhang 2013). Hence, they will not be studied further in the sequel. However, for accelerated solvers, the O(1/T ) convergence rate in (12) is faster than the O(1/ \u221a T ) rate in (Xiao 2009; Duchi and Singer 2009; Ouyang and Gray 2012; Shamir and Zhang 2013) and the O( 1T 2 + log T T ) rate in (Orabona, Argyriou, and Srebro 2012). The O(1/T ) convergence rate is also obtained in (Nesterov 2005a; Nesterov 2005b), but again only for r \u2261 0 in (5).\nWhen continuation is not used, the following results are analogous to those obtained in the previous section. Proposition 4. Let x\u03030 = 0. When continuation is not used, let \u03c1 be the error reduction factor at each stage. When either an accelerated or non-accelerated solver is used,\nEP (x\u0303S)\u2212 P (x\u2217) \u2264 \u03c1S(P (x\u03030)\u2212 P (x\u2217))\n+(1 + \u03c1S)\u03b3Du + \u03bb\n2 R2. (13)\nProposition 5. Let x\u03030 = 0. Suppose that the three terms on the RHS of (13) are equal to \u03b1 , \u03b2 and \u03b6 , respectively, where \u03b1, \u03b2, \u03b6 > 0 and \u03b1 + \u03b2 + \u03b6 = 1. Let \u03c11 = \u03c1 = 1/\u03c42 in (12) and (13). Assume that Algorithm 2 (with accelerated solver) needs a total of T iterations to obtain an -accurate solution, while its fixed-\u03b3s variant takes T \u2032 iterations. Then,\nT \u2265 \u03c4 S \u2212 1 \u03c4 \u2212 1\n( a \u221a \u03ba1\u03c6 ( 1\n\u03c42\n) + b\u03c6 ( 1\n\u03c42\n) + c ) ,\nT \u2032 \u2265 S ( a \u221a \u03c42S + 1\n(\u03c4 + 1)2 \u03ba1+C\u03c6\n( 1\n\u03c42\n) + b\u03c6 ( 1\n\u03c42\n) + c ) ,\nwhere S \u2265 log (\n\u03b1 (P (x\u03030)\u2212P (x\u2217))\n) / log ( 1 \u03c42 ) , C =(\n1\u2212 \u03c4 2S+1\n(\u03c4+1)2\n) + ( \u03c4S\n1+\u03c4 \u2212 \u03c42S+1 (\u03c4+1)2 ) K \u03bb1 and K is a constant.\nA summary of the convergence results is shown in Table 2. As can be seen, the convergence rates of the proposed CNS algorithm match the fastest known rates in nonsmooth optimization, but CNS is less restrictive and can exploit the composite structure of the optimization problem."}, {"heading": "Experiments", "text": "Because of the lack of space, we only report results on two data sets (Table 3) from the LIBSVM archive: (i) the pop-\nularly used classification data set rcv1; and (ii) YearPredictionMSD, the largest regression data in the LIBSVM archive, and is a subset of the Million Song data set. We use the hinge loss for classification, and `1 loss for regression. Both can be smoothed using Nesterov\u2019s smoothing (to (3) and (4), respectively). As for the regularizer, we use the\n1. elastic-net regularizer r(x) = \u03bd1\u2016x\u20161+ \u03bd22 \u2016x\u2016 2 2 (Zou and\nHastie 2005), and problem (5) is strongly convex; and 2. `1 regularizer r(x) = \u03bd1\u2016x\u20161, and (5) is (general) convex. Here, \u03bd1, \u03bd2 are tuned by 5-fold cross-validation. Obviously, all losses and regularizers are convex but nonsmooth. We use mini-batch for all methods. The mini-batch size b is 50 for rcv1, and 100 for YearPredictionMSD.\nNote that FOBOS, RDA and the proposed CNS can effectively make use of the composite structure of the problem, while Poly-SGD cannot. For each method, the stepsize is tuned by running on a subset containing 20% training data for a few epochs (for the proposed method, we tune \u03b71). All algorithms are implemented in Matlab."}, {"heading": "Strongly Convex Objectives", "text": "Figure 1 shows convergence of the objective and testing performance (classification error for rcv1 and `1-loss for YearPredictionMSD). The trends are consistent with Theorem 1. CNS-A is the fastest (with a of O(1/T 2)). This is followed by CNS-NA and Poly-SGD, both with O(1/T ) rate (from Theorem 1 and (Shamir and Zhang 2013)). The slowest are FOBOS and RDA, which converge at a rate of O(log T/T ) (Duchi and Singer 2009; Xiao 2009).\nFigure 2 compares with the case where continuation is not used. Two fixed smoothness settings, \u03b3 = 10\u22122 and \u03b3 = 10\u22123, are used. As can be seen, they are much slower\n(Propositions 2 and 3). Moreover, a smaller \u03b3 leads to slower convergence but better solution, while a larger \u03b3 leads to faster convergence but worse solution. This is also consistent with Proposition 1, as using a fixed \u03b3 only allows convergence to the optimal solution with a tolerance of (1+\u03c1S)\u03b3Du. Moreover, a smaller \u03b3 leads to a larger condition number, and convergence becomes slower."}, {"heading": "General Convex Objectives", "text": "We set \u03bb1 in Algorithm 2 to 10\u22125 for rcv1, and 10\u22127 for YearPredictionMSD. As can be seen from Figure 3, the trends are again consistent with Theorem 3. CNS-A is the fastest (O(1/T ) convergence rate), while the others all have a rate of O(1/ \u221a T ) (Duchi and Singer 2009; Xiao 2009; Shamir and Zhang 2013). Also, RDA shows better performance than FOBOS and Poly-SGD. Recall that Poly-SGD outperforms FOBOS and RDA on strongly convex problems. However, on general convex problems, Poly-SGD is the worst as its rate is only as good as others, and it does not exploit the composite structure of the problem.\nFigure 4 compares with the case where continuation is not used. As in the previous section, CNS-NA and CNS-A show\nfaster convergence than its fixed-smoothing counterparts."}, {"heading": "Conclusion", "text": "In this paper, we proposed a continuation algorithm (CNS) for regularized risk minimization problems, in which both the loss and regularizer may be nonsmooth. In each of its stages, the smoothed subproblem can be easily solved by either existing accelerated or non-accelerated solvers. Theoretical analysis establishes convergence results on the whole continuation algorithm, not just one of its stages. In particular, when accelerated solvers are used, the proposed CNS algorithm achieves the rate of O(1/T 2) on strongly convex problems, and O(1/T ) on general convex problems. These are the fastest known rates for nonsmooth optimization. However, CNS is advantageous in that it allows the use of a regularizer (unlike the fastest batch algorithm) and can exploit the composite structure of the optimization problem (unlike the fastest stochastic algorithm). Experiments on nonsmooth classification and regression models demonstrate that CNS outperforms the state-of-the-art."}, {"heading": "Acknowledgments", "text": "This research was supported in part by the Research Grants Council of the Hong Kong Special Administrative Region (Grant 614513)."}], "references": [{"title": "Informationtheoretic lower bounds on the oracle complexity of convex optimization", "author": ["Agarwal"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Agarwal,? \\Q2009\\E", "shortCiteRegEx": "Agarwal", "year": 2009}, {"title": "NESTA: A fast and accurate first-order method for sparse recovery", "author": ["Bobin Becker", "S. Cand\u00e8s 2011] Becker", "J. Bobin", "E.J. Cand\u00e8s"], "venue": "SIAM Journal on Imaging Sciences", "citeRegEx": "Becker et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Becker et al\\.", "year": 2011}, {"title": "Smoothing proximal gradient method for general structured sparse regression", "author": ["Chen"], "venue": "Annals of Applied Statistics", "citeRegEx": "Chen,? \\Q2012\\E", "shortCiteRegEx": "Chen", "year": 2012}, {"title": "SAGA: A fast incremental gradient method with support for non-strongly convex composite objectives", "author": ["Bach Defazio", "A. Lacoste-Julien 2014] Defazio", "F. Bach", "S. Lacoste-Julien"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Defazio et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Defazio et al\\.", "year": 2014}, {"title": "Efficient online and batch learning using forward backward splitting", "author": ["Duchi", "J. Singer 2009] Duchi", "Y. Singer"], "venue": "Journal of Machine Learning Research", "citeRegEx": "Duchi et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Duchi et al\\.", "year": 2009}, {"title": "A fixed-point continuation method for `1regularized minimization with applications to compressed sensing", "author": ["Yin Hale", "E. Zhang 2007] Hale", "W. Yin", "Y. Zhang"], "venue": "Technical Report CAAM TR07-07,", "citeRegEx": "Hale et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Hale et al\\.", "year": 2007}, {"title": "Accelerating stochastic gradient descent using predictive variance reduction", "author": ["Johnson", "R. Zhang 2013] Johnson", "T. Zhang"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Johnson et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Johnson et al\\.", "year": 2013}, {"title": "Stochastic Approximation and Recursive Algorithms and Applications, volume 35", "author": ["Kushner", "H.J. Yin 2003] Kushner", "G. Yin"], "venue": null, "citeRegEx": "Kushner et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Kushner et al\\.", "year": 2003}, {"title": "Spectral regularization algorithms for learning large incomplete matrices", "author": ["Hastie Mazumder", "R. Tibshirani 2010] Mazumder", "T. Hastie", "R. Tibshirani"], "venue": "Journal of Machine Learning Research", "citeRegEx": "Mazumder et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Mazumder et al\\.", "year": 2010}, {"title": "Problem Complexity and Method Efficiency in Optimization", "author": ["Nemirovski", "A. Yudin 1983] Nemirovski", "D. Yudin"], "venue": null, "citeRegEx": "Nemirovski et al\\.,? \\Q1983\\E", "shortCiteRegEx": "Nemirovski et al\\.", "year": 1983}, {"title": "PRISMA: Proximal iterative smoothing algorithm. Preprint arXiv:1206.2372", "author": ["Argyriou Orabona", "F. Srebro 2012] Orabona", "A. Argyriou", "N. Srebro"], "venue": null, "citeRegEx": "Orabona et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Orabona et al\\.", "year": 2012}, {"title": "Stochastic smoothing for nonsmooth minimizations: Accelerating SGD by exploiting structure", "author": ["Ouyang", "H. Gray 2012] Ouyang", "A.G. Gray"], "venue": "In Proceedings of the 29th International Conference on Machine Learning,", "citeRegEx": "Ouyang et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Ouyang et al\\.", "year": 2012}, {"title": "Proximal algorithms. Foundations and Trends in Optimization 1(3):127\u2013239", "author": ["Parikh", "N. Boyd 2014] Parikh", "S. Boyd"], "venue": null, "citeRegEx": "Parikh et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Parikh et al\\.", "year": 2014}, {"title": "Making gradient descent optimal for strongly convex stochastic optimization", "author": ["Shamir Rakhlin", "A. Sridharan 2012] Rakhlin", "O. Shamir", "K. Sridharan"], "venue": "In Proceedings of the 29th International Conference on Machine Learning,", "citeRegEx": "Rakhlin et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Rakhlin et al\\.", "year": 2012}, {"title": "Convergence rates of inexact proximal-gradient methods for convex optimization", "author": ["Roux Schmidt", "M. Bach 2011] Schmidt", "N.L. Roux", "F.R. Bach"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Schmidt et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Schmidt et al\\.", "year": 2011}, {"title": "Minimizing finite sums with the stochastic average gradient", "author": ["Roux Schmidt", "M. Bach 2013] Schmidt", "N.L. Roux", "F. Bach"], "venue": "Preprint arXiv:1309.2388", "citeRegEx": "Schmidt et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Schmidt et al\\.", "year": 2013}, {"title": "Accelerated proximal stochastic dual coordinate ascent for regularized loss minimization", "author": ["Shalev-Shwartz", "S. Zhang 2014] Shalev-Shwartz", "T. Zhang"], "venue": "In Proceedings of the 31st International Conference on Machine Learning,", "citeRegEx": "Shalev.Shwartz et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Shalev.Shwartz et al\\.", "year": 2014}, {"title": "Stochastic gradient descent for non-smooth optimization: Convergence results and optimal averaging schemes", "author": ["Shamir", "O. Zhang 2013] Shamir", "T. Zhang"], "venue": "In Proceedings of the 30th International Conference on Machine Learning,", "citeRegEx": "Shamir et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Shamir et al\\.", "year": 2013}, {"title": "A fast algorithm for sparse reconstruction based on shrinkage, subspace optimization, and continuation", "author": ["Wen"], "venue": "SIAM Journal on Scientific Computing", "citeRegEx": "Wen,? \\Q2010\\E", "shortCiteRegEx": "Wen", "year": 2010}, {"title": "A proximal-gradient homotopy method for the `1-regularized least-squares problem", "author": ["Xiao", "L. Zhang 2012] Xiao", "T. Zhang"], "venue": "In Proceedings of the 29th International Conference on Machine Learning,", "citeRegEx": "Xiao et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Xiao et al\\.", "year": 2012}, {"title": "A proximal stochastic gradient method with progressive variance reduction", "author": ["Xiao", "L. Zhang 2014] Xiao", "T. Zhang"], "venue": "SIAM Journal on Optimization", "citeRegEx": "Xiao et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Xiao et al\\.", "year": 2014}, {"title": "Regularization and variable selection via the elastic net", "author": ["Zou", "H. Hastie 2005] Zou", "T. Hastie"], "venue": "Journal of the Royal Statistical Society: Series B", "citeRegEx": "Zou et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Zou et al\\.", "year": 2005}], "referenceMentions": [{"referenceID": 18, "context": "of gradually changing the regularization parameter in (Hale, Yin, and Zhang 2007; Wen et al. 2010; Mazumder, Hastie, and Tibshirani 2010). Empirically, continuation converges much faster than the use of a fixed smoothing parameter (Becker, Bobin, and Cand\u00e8s 2011). However, the theoretical convergence rate obtained in (Becker, Bobin, and Cand\u00e8s 2011) is only for one stage of the continuation algorithm (i.e., on the smoothed problem with a particular smoothing parameter), while the convergence properties for the whole algorithm are not clear. Recently, Xiao and Zhang (2012) obtained a linear convergence rate for their continuation algorithm, though only for the special case of `1-regularized least squares regression.", "startOffset": 82, "endOffset": 579}], "year": 2016, "abstractText": "In regularized risk minimization, the associated optimization problem becomes particularly difficult when both the loss and regularizer are nonsmooth. Existing approaches either have slow or unclear convergence properties, are restricted to limited problem subclasses, or require careful setting of a smoothing parameter. In this paper, we propose a continuation algorithm that is applicable to a large class of nonsmooth regularized risk minimization problems, can be flexibly used with a number of existing solvers for the underlying smoothed subproblem, and with convergence results on the whole algorithm rather than just one of its subproblems. In particular, when accelerated solvers are used, the proposed algorithm achieves the fastest known rates of O(1/T ) on strongly convex problems, and O(1/T ) on general convex problems. Experiments on nonsmooth classification and regression tasks demonstrate that the proposed algorithm outperforms the state-of-the-art.", "creator": "LaTeX with hyperref package"}}}