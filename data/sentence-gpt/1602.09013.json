{"id": "1602.09013", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "29-Feb-2016", "title": "Beyond CCA: Moment Matching for Multi-View Models", "abstract": "We introduce three novel semi-parametric extensions of probabilistic canonical correlation analysis with identifiability guarantees. We consider moment matching techniques for estimation in these models. For that, by drawing explicit links between the new models and a discrete version of independent component analysis (DICA), we first extend the DICA cumulant tensors to the new discrete version of CCA for comparisons of the previously tested case (CSCS; see Supplementary S1). We then extend the DICA and the CCA to the new discrete version of CCA for comparison of the previously tested case (CSCS; see Supplementary S1). We use a priori technique that has previously been used to predict whether, in the context of uncertainty, the likelihood of a previous previous predicted prediction is proportional to the likelihood of a previous predicted prediction in the context of uncertainty, whereas, in the context of uncertainty, the likelihood of a previous prediction is proportional to the likelihood of a previous prediction in the context of uncertainty, whereas, in the context of uncertainty, the probability of a previous prediction is proportional to the probability of a previous prediction in the context of uncertainty, which is consistent with the likelihood of a previous prediction in the context of uncertainty. This model provides a model for the model\u2012s prediction that relies on data from CCA and CCA (see Supplementary S1).\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n", "histories": [["v1", "Mon, 29 Feb 2016 15:51:50 GMT  (117kb,D)", "http://arxiv.org/abs/1602.09013v1", "26 pages, under review"], ["v2", "Fri, 3 Jun 2016 14:06:23 GMT  (130kb,D)", "http://arxiv.org/abs/1602.09013v2", "Appears in: Proceedings of the 33rd International Conference on Machine Learning (ICML 2016). 22 pages"]], "COMMENTS": "26 pages, under review", "reviews": [], "SUBJECTS": "stat.ML cs.LG", "authors": ["anastasia podosinnikova", "francis r bach", "simon lacoste-julien"], "accepted": true, "id": "1602.09013"}, "pdf": {"name": "1602.09013.pdf", "metadata": {"source": "CRF", "title": "Beyond CCA: Moment Matching for Multi-View Models", "authors": ["Anastasia Podosinnikova", "Francis Bach", "Simon Lacoste-Julien"], "emails": [], "sections": [{"heading": "1 Introduction", "text": "Canonical correlation analysis (CCA), originally introduced by Hotelling (1936), is a common statistical tool for the analysis of multi-view data. Examples of such data include, for instance, representation of some text in two languages (e.g., Vinokourov et al., 2002) or images aligned with text data (e.g., Hardoon et al., 2004; Gong et al., 2014). Given two multidimensional variables (or datasets), CCA finds two linear transformations (factor loading matrices) that mutually maximize the correlations between the transformed variables (or datasets). Together with its kernelized version (see, e.g., Cristianini & Shawe-Taylor, 2000; Bach & Jordan, 2003), CCA has a wide range of applications (see, e.g., Hardoon et al. (2004) for an overview).\nBach & Jordan (2005) provide a probabilistic interpretation of CCA: they show that the maximum likelihood estimators of a particular Gaussian graphical model, which we refer to as Gaussian CCA, is equivalent to the classical CCA by Hotelling (1936). The key idea of Gaussian CCA is to allow some of the covariance in the two observed variables to be explained by a linear transformation of common independent sources, while the rest of the covariance of each view is explained by their own (unstructured) noises. Importantly, the dimension of the common sources is often significantly smaller than the dimensions of the observations and, potentially, than the dimensions of the noise. Examples of applications and extensions of Gaussian CCA are the works by Socher & Fei-Fei (2010), for mapping visual and textual features to the same latent space, and Haghighi et al. (2008), for machine translation applications.\nGaussian CCA is subject to some well-known unidentifiability issues, in the same way as the closely related factor analysis model (FA; Bartholomew, 1987; Basilevsky, 1994) and its special case, the probabilistic principal component analysis model (PPCA; Tipping & Bishop; Roweis, 1998). Indeed, as FA and PPCA are identifiable only up to multiplication by any orthogonal rotation matrix, Gaussian CCA is only identifiable up to multiplication by any invertible matrix. Although this unidentifiability does not affect the predictive performance of the model, it does affect the factor loading matrices and hence the interpretability of the latent factors. In FA and PPCA, one can enforce additional constraints to recover unique factor loading matrices (see, e.g., Murphy, 2012). A notable identifiable version of FA is independent component analysis (ICA; Jutten, 1987; Jutten & He\u0301rault, 1991; Comon & Jutten, 2010). One of our goals is to introduce identifiable versions of CCA.\nThe main contributions of this paper are as follows. We first introduce for the first time, to the best of our knowledge, three new formulations of CCA: discrete, non-Gaussian, and mixed (see Section 2.1). We then provide identifiability guarantees for the new models (see Section 2.2). Then, in order to use a moment matching framework for estimation, we first derive a new set of cumulant tensors for the discrete version of CCA (Section 3.1). We further replace these tensors with their approximations by generalized covariance matrices for all three new models (Section 3.2). Finally, as opposed to standard approaches, we use a particular type of non-orthogonal joint diagonalization algorithms for extracting the model parameters from the cumulant tensors or their approximations (Section 4).\nar X\niv :1\n60 2.\n09 01\n3v 1\n[ st\nat .M\nL ]\n2 9\nFe b\n20 16\nModels. The new CCA models are adapted to applications where one or both of the data-views are either counts, like in the bag-of-words representation for text, or continuous data, for instance, any continuous representation of images. A key feature of CCA compared to joint PCA is the focus on modeling the common variations of the two views, as opposed to modeling all variations (including joint and marginal ones).\nMoment matching. Regarding parameter estimation, we use the method of moments, also known as \u201cspectral methods\u201d. It recently regained popularity as an alternative to other estimation methods for graphical models, such as approximate variational inference or MCMC sampling. Estimation of a wide range of models is possible within the moment matching framework: ICA (e.g., Cardoso & Comon, 1996; Comon & Jutten, 2010), mixtures of Gaussians (e.g., Arora & Kannan, 2005; Hsu & Kakade, 2013), latent Dirichlet allocation and topic models (Arora et al., 2012, 2013; Anandkumar et al., 2012; Podosinnikova et al., 2015), supervised topic models (Wang & Zhu, 2014), Indian buffet process inference (Tung & Smola, 2014), stochastic languages (Balle et al., 2014), mixture of hidden Markov models (Su\u0308bakan et al., 2014), neural networks (see, e.g., Anandkumar & Sedghi, 2015; Janzamin et al., 2016), and other models (see, e.g., Anandkumar et al., 2014, and references therein).\nMoment matching algorithms for estimation in graphical models mostly consist of two main steps: (a) construction of moments or cumulants with a particular diagonal structure and (b) joint diagonalization of the sample estimates of the moments or cumulants in order to estimate the parameters.\nCumulants and generalized covariance matrices. By using the close connection between ICA and CCA, we first derive in Section 3.1 the cumulant tensors for the discrete version of CCA from the cumulant tensors of a discrete version of ICA (DICA) proposed by Podosinnikova et al. (2015). Extending the ideas from the ICA literature (Yeredor, 2000; Todros & Hero, 2013), we further generalize in Section 3.2 cumulants as the derivatives of the cumulant generating function. This allows us to replace cumulant tensors with \u201cgeneralized covariance matrices\u201d, while preserving the rest of the framework. As a consequence of working with the second order information only, the derivations and algorithms get significantly simplified and the sample complexity potentially improves.\nNon-orthogonal joint diagonalization. When estimating model parameters, both CCA cumulant tensors and generalized covariance matrices for CCA lead to non-symmetric approximate joint diagonalization problems. Therefore, the workhorses of the method of moments in similar context \u2014 orthogonal diagonalization algorithms, such as the tensor power method Anandkumar et al. (2014), and orthogonal joint diagonalization (Bunse-Gerstner et al., 1993; Cardoso & Souloumiac, 1996) \u2014 are not applicable. As an alternative, we use a particular type of non-orthogonal Jacobi-like joint diagonalization algorithms (see Section 4). Importantly, the joint diagonalization problem we deal with in this paper is conceptually different from the one considered, e.g., by Kuleshov et al. (2015) (and references therein) and, therefore, the respective algorithms are not applicable here."}, {"heading": "2 Multi-view models", "text": ""}, {"heading": "2.1 Extensions of Gaussian CCA", "text": "Gaussian CCA. Classical CCA (Hotelling, 1936) aims to find projections D1 \u2208 RM1\u00d7K and D2 \u2208 RM2\u00d7K , of two observation vectors x1 \u2208 RM1 and x2 \u2208 RM2 , each representing a data-view, such that the projected data,D>1 x1 andD > 2 x2, are maximally correlated. Similarly to classical PCA, the solution boils down to solving a generalized SVD problem. The following probabilistic interpretation of CCA is well known (Browne, 1979; Bach & Jordan, 2005; Klami et al., 2013). Given that K sources are i.i.d. standard normal random variables, \u03b1 \u223c N (0, IK), the Gaussian CCA model is given by\nx1 |\u03b1, \u00b51, \u03a81 \u223c N (D1\u03b1+ \u00b51, \u03a81), x2 |\u03b1, \u00b52, \u03a82 \u223c N (D2\u03b1+ \u00b52, \u03a82),\n(1)\nwhere the matrices \u03a81 \u2208 RM1\u00d7M1 and \u03a82 \u2208 RM2\u00d7M2 are positive semi-definite. Then, the maximum likelihood solution of (1) coincides (up to permutation, scaling, and multiplication by any invertible matrix) with the classical CCA solution. The model (1) is equivalent to\nx1 = D1\u03b1+ \u03b51, x2 = D2\u03b1+ \u03b52, (2)\nwhere the noise vectors are normal random variables, i.e. \u03b51 \u223c N (\u00b51,\u03a81) and \u03b52 \u223c N (\u00b52,\u03a82), and the following independence assumptions are made:\n\u03b11, . . . , \u03b1K are mutually independent, \u03b1 \u22a5\u22a5 \u03b51, \u03b52 and \u03b51 \u22a5\u22a5 \u03b52.\n(3)\nThe following three models are our novel semi-parametric extensions of Gaussian CCA (1)\u2013(2).\nMulti-view models. The first new model follows by dropping the Gaussianity assumption on \u03b1, \u03b51, and \u03b52. In particular, the non-Gaussian CCA model is defined as\nx1 = D1\u03b1+ \u03b51, x2 = D2\u03b1+ \u03b52, (4)\nwhere, as opposed to (2), no assumptions are made on the sources \u03b1 and the noise \u03b51 and \u03b52 except for the independence assumption (3).\nBy analogy with Podosinnikova et al. (2015), we can further \u201cdiscretize\u201d non-Gaussian CCA (4) by applying the Poisson distribution to each view (independently on each variable):\nx1 |\u03b1, \u03b51 \u223c Poisson(D1\u03b1+ \u03b51), x2 |\u03b1, \u03b52 \u223c Poisson(D2\u03b1+ \u03b52).\n(5)\nWe obtain the (non-Gaussian) discrete CCA (DCCA) model, which is adapted to count data (e.g., such as word counts in the bag-of-words model of text). In this case, the sources \u03b1, the noise \u03b51 and \u03b52, and the matrices D1 and D2 have non-negative components.\nFinally, by combining non-Gaussian and discrete CCA, we also introduce the mixed CCA (MCCA) model:\nx1 = D1\u03b1+ \u03b51,\nx2 |\u03b1, \u03b52 \u223c Poisson(D2\u03b1+ \u03b52), (6)\nwhich is adapted to a combination of discrete and continuous data (e.g., such as images represented as continuous vectors aligned with text represented as counts). Note that no assumptions are made on distributions of the sources \u03b1 except for independence (3).\nThe plate diagram for the models (4)\u2013(6) is presented in Fig. 1. Depending on the context, the matrices D1 and D2 are called differently: topic matrices1 in the topic learning context, factor loading or projection matrices in the FA and/or PPCA context, mixing matrices in the ICA context, or dictionaries in the dictionary learning context. In this paper, we will use the name factor loading matrices to refer to D1 and D2.\nRelation between PCA and CCA. The important difference between Gaussian CCA and the closely related FA/PPCA models is that the noise in each view of Gaussian CCA is not assumed to be isotropic unlike in FA/PPCA. In other words, the components of the noise are not assumed to be independent or, equivalently, the noise covariance matrix does not have to be diagonal and may exhibit a strong structure. In this paper, we never assume any diagonal structure of the covariance matrices of the noises of the models (4)\u2013(6).\nThe following example illustrates the mentioned relation. Assuming a linear structure for the noise, (non-) Gaussian CCA (NCCA) takes the form\nx1 = D1\u03b1+ F1\u03b21, x2 = D2\u03b1+ F2\u03b22, (7)\n1 Note that Podosinnikova et al. (2015) show that DICA is closely connected (and under some conditions is equivalent) to latent Dirichlet allocation (Blei et al., 2003). Due to the close relation of DCCA and DICA, the former is thus closely related to the multi-view topic models (see, e.g., Blei & Jordan, 2003).\nwhere \u03b51 = F1\u03b21 with \u03b21 \u2208 RK1 and \u03b52 = F2\u03b22 with \u03b22 \u2208 RK2 . By stacking the vectors on the top of each other\nx = ( x1 x2 ) , D = ( D1 F1 0 D2 0 F2 ) , z = \u03b1\u03b21 \u03b22  , (8) one can rewrite the model as x = Dz. If the noise sources \u03b21 and \u03b22 are assumed to have mutually independent components, ICA is recovered. If the sources z are further assumed to be Gaussian, x = Dz corresponds to PPCA. However, we do not assume the noise in Gaussian CCA (as well as in the models (4)\u2013(6)) to have a very specific low dimensional structure.\nRelated work. Some extensions of Gaussian CCA were proposed in the literature: exponential family CCA (Virtanen, 2010; Klami et al., 2010) and Bayesian CCA (see, e.g., Klami et al., 2013, and references therein). Although exponential family CCA can also be discretized, it assumes in practice that the prior of the sources is a specific combination of Gaussians. Bayesian CCA models the factor loading matrices and the covariance matrix of Gaussian CCA. Sampling or approximate variational inference are used for estimation and inference in both models. Both models, however, lack our identifiability guarantees and are quite different from the models (4)\u2013(6). Song et al. (2014) consider a multi-view framework to deal with non-parametric mixture components, while our approach is semi-parametric with an explicit linear structure (our loading matrices) and makes the explicit link with CCA."}, {"heading": "2.2 Identifiability", "text": "In this section, the identifiability of the factor loading matrices D1 and D2 is discussed. In general, for the type of models considered, the unidentifiability to permutation and scaling cannot be avoided. In practice, this unidentifiability is however easy to handle and, in the following, we only consider identifiability up to permutation and scaling.\nICA can be seen as an identifiable analog of FA/PPCA. Indeed, it is known that the mixing matrix D of ICA is identifiable if at most one source is Gaussian (Comon, 1994). The factor loading matrix of FA/PPCA is unidentifiable since it is defined only up to multiplication by any orthogonal rotation matrix.\nSimilarly, the factor loading matrices of Gaussian CCA (1), which can be seen as a multi-view extension of PPCA, are identifiable only up to multiplication by any invertible matrix (Bach & Jordan, 2005). We show the identifiability results for the new models (4)\u2013(6): The factor loading matrices of these models are identifiable if at most one source is Gaussian (see Appendix A for a proof).\nTheorem 1. Assume that matrices D1 \u2208 RM1\u00d7K and D2 \u2208 RM2\u00d7K , where K \u2264 min(M1, M2), have full rank. If the covariance matrices cov(x1) and cov(x2) exist and if at most one source \u03b1k, for k = 1, . . . ,K, is Gaussian and none of the sources are deterministic, then the models (4)\u2013(6) are identifiable (up to scaling and joint permutation).\nImportantly, the permutation unidentifiability does not destroy the alignment in the factor loading matrices, that is, for some permutation matrix P , if D1P is the factor loading matrix of the first view, than D2P must be the factor loading matrix of the second view. This property is important for the interpretability of the factor loading matrices and, in particular, is used in the experimental Section 5."}, {"heading": "3 The cumulants and generalized covariances", "text": "In this section, we first derive the cumulant tensors for the discrete CCA model (Section 3.1) and then generalized covariance matrices (Section 3.2) for the models (4)\u2013(6). We show that both cumulants and generalized covariances have a special diagonal form and, therefore, can be efficiently used within the moment matching framework (Section 4)."}, {"heading": "3.1 From discrete ICA to discrete CCA", "text": "In this section, we derive the DCCA cumulants as an extension of the cumulants of discrete independent component analysis (DICA; Podosinnikova et al., 2015).\nDiscrete ICA. Podosinnikova et al. (2015) consider the discrete ICA model (9), where x \u2208 RM has conditionally independent Poisson components with mean D\u03b1 and \u03b1 \u2208 RK has independent non-negative components: x |\u03b1 \u223c Poisson(D\u03b1). (9) For estimating the factor loading matrix D, Podosinnikova et al. (2015) propose an algorithm based on the moment matching method with the cumulants of the DICA model. In particular, they define the DICA Scovariance matrix and T-cumulant tensor as\nS := cov(x)\u2212 diag [Ex] , [T ]m1m2m3 := cum(x)m1m2m3 + [\u03c4 ]m1m2m3 ,\n(10)\nwhere the indices m1, m2, and m3 take the values in 1, . . . ,M , and\n[\u03c4 ]m1m2m3 = 2\u03b4m1m2m3Exm1 \u2212 \u03b4m2m3cov(x)m1m2 \u2212 \u03b4m1m3cov(x)m1m2 \u2212 \u03b4m1m2cov(x)m1m3\nwith \u03b4 being the Kronecker delta. For completeness, we outline the derivation by Podosinnikova et al. (2015) below. Denoting y := D\u03b1, one obtains by the law of total expectation that E(x) = E(x|y) = E(y) and by the law of total covariance:\ncov(x) = E[cov(x|y)] + cov[E(x|y), E(x|y)] = diag[E(y)] + cov(y),\nsince all the cumulants of a Poisson random variable with parameter y are equal to y. Therefore, S = cov(y). Similarly, from the law of total cumulance, T = cum(y). Then, by the multilinearity property for cumulants, one obtains\nS = D cov(\u03b1)D>, T = cum(\u03b1)\u00d71 D> \u00d72 D> \u00d73 D>, (11)\nwhich is called the diagonal form since the covariance cov(\u03b1) and cumulant cum(\u03b1) of the independent sources are diagonal. Note that \u00d7i denotes the i-mode tensor-matrix product (see, e.g., Kolda & Bader, 2009). This diagonal form is further used for estimation of D (see Section 4).\nNoisy discrete ICA. The following noisy version (12) of the DICA model reveals the connection between DICA and DCCA. Noisy discrete ICA is obtained by adding non-negative noise \u03b5, such that \u03b1 \u22a5\u22a5 \u03b5, to discrete ICA (9): x |\u03b1, \u03b5 \u223c Poisson (D\u03b1+ \u03b5) . (12) Let y := D\u03b1 + \u03b5 and S and T are defined as in (10). Then a simple extension of the derivations from above gives S = cov(y) and T = cum(y). Since the covariance matrix (cumulant tensor) of the sum of two independent multivariate random variables, D\u03b1 and \u03b5, is equal to the sum of the covariance matrices (cumulant tensors) of these variables, the \u201cperturbed\u201d version of the diagonal form (11) follows\nS = Dcov(\u03b1)D> + cov(\u03b5), T = cum(\u03b1)\u00d71 D> \u00d72 D> \u00d73 D> + cum(\u03b5). (13)\nDCCA cumulants. By analogy with (8), stacking the observations x = [x1; x2], the factor loading matrices D = [D1; D2], and the noise vectors \u03b5 = [\u03b51; \u03b52] of discrete CCA (5) gives a noisy version of discrete ICA with a particular form of the covariance matrix of the noise:\ncov(\u03b5) =\n( cov(\u03b51) 0\n0 cov(\u03b52)\n) , (14)\nwhich is due to the independence \u03b51 \u22a5\u22a5 \u03b52. Similarly, the cumulant cum(\u03b5) of the noise has only two diagonal blocks which are non-zero. Therefore, considering only those parts of the S-covariance matrix and T-cumulant tensor of noisy DICA that correspond to zero blocks of the covariance cov(\u03b5) and cumulant cum(\u03b5) gives immediately a matrix and tensor with a diagonal structure similar to the one in (11). Those blocks are the cross-covariance and cross-cumulants of x1 and x2.\nWe define the S-covariance matrix of discrete CCA2 as the cross-covariance matrix of x1 and x2:\nS12 := cov(x1, x2). (15)\nFrom (13) and (14), the matrix S12 has the following diagonal form\nS12 = D1cov(\u03b1)D > 2 . (16)\n2 Note that S21 := cov(x2, x1) is just the transpose of S12.\nSimilarly, we define the T-cumulant tensors of discrete CCA ( T121 \u2208 RM1\u00d7M2\u00d7M1 and T122 \u2208 RM1\u00d7M2\u00d7M2 ) through the cross-cumulants of x1 and x2, for j = 1, 2:\n[T12j ]m1m2m\u0303j := [cum(x1, x2, xj)]m1m2m\u0303j \u2212 \u03b4mjm\u0303j [cov(x1, x2)]m1m2 , (17)\nwhere the indices m1, m2, and m\u0303j take the values m1 \u2208 1, . . . ,M1, m2 \u2208 1, . . . ,M2, and m\u0303j \u2208 1, . . . ,Mj . From (11) and the mentioned block structure (14) of cov(\u03b5), the DCCA T-cumulants have the diagonal form:\nT121 = cum(\u03b1)\u00d71 D>1 \u00d72 D>2 \u00d73 D>1 , T122 = cum(\u03b1)\u00d71 D>1 \u00d72 D>2 \u00d73 D>2 .\n(18)\nIn Section 4, we show how to estimate the factor loading matrices D1 and D2 using the diagonal form (16) and (18). Before that, in Section 3.2, we first derive the generalized covariance matrices of discrete ICA and the CCA models (4)\u2013(6) as an extension of the ideas by Yeredor (2000); Todros & Hero (2013)."}, {"heading": "3.2 Generalized covariance matrices", "text": "In this section, we introduce the generalization of the S-covariance matrix for both DICA and the CCA models (4)\u2013(6), which are obtained through the Hessian of the cumulant generating function. We show that (a) the generalized covariance matrices can be used for approximation of the T-cumulant tensors using generalized derivatives and (b) in the DICA case, these generalized covariance matrices have the diagonal form analogous to (11), and, in the CCA case, they have the diagonal form analogous to (16). Therefore, generalized covariance matrices can be seen as a substitute for the T-cumulant tensors in the moment matching framework. This (a) significantly simplifies derivations and the final expressions used for implementation of resulting algorithms and (b) potentially improves the sample complexity, since only the second order information is used.\nGeneralized covariance matrices. The idea of generalized covariance matrices3 is inspired by the similar extension of the ICA cumulants by Yeredor (2000).\nThe cumulant generating function (CGF) of a multivariate random variable x \u2208 RM is defined as\nKx(t) = logE(et >x), (19)\nfor t \u2208 RM . The cumulants \u03bas(x), for s = 1, 2, 3, . . . , are the coefficients of the Taylor series expansion of the CGF evaluated at zero. Therefore, the cumulants are the derivatives of the CGF evaluated at zero: \u03bas(x) = \u2207sKx(0), s = 1, 2, 3, . . . , where \u2207sKx(t) is the s-th order derivative of Kx(t) with respect to t. Thus, the expectation of x is the gradient E(x) = \u2207Kx(0) and the covariance of x is the Hessian cov(x) = \u22072Kx(0) of the CGF evaluated at zero.\nThe extension of cumulants then follows immediately: for t \u2208 RM , we refer to the derivatives \u2207sKx(t) of the CGF as the generalized cumulants. The respective parameter t is called a processing point. In particular, the gradient, \u2207Kx(t), and Hessian, \u22072Kx(t), of the CGF are referred to as the generalized expectation and generalized covariance matrix, respectively:\nEx(t) := \u2207Kx(t) = E(xet>x) E(et>x) , (20) Cx(t) := \u22072Kx(t) = E(xx>et>x) E(et>x) \u2212 Ex(t)Ex(t)>. (21)\nSome properties of these statistics and their natural finite sample estimators are analyzed by Slapak & Yeredor (2012b).\nWe now outline the key ideas of this section. When a multivariate random variable \u03b1 \u2208 RK has independent components, its CGF K\u03b1(h) = logE(eh\n>\u03b1), for some h \u2208 RK , is equal to a sum of decoupled terms: K\u03b1(h) = \u2211 k logE(ehk\u03b1k). Therefore, the Hessian \u22072K\u03b1(h) of the CGF K\u03b1(h) is diagonal (see Appendix B.1). Like covariance matrices, these Hessians (a.k.a. generalized covariance matrices) are subject to the multilinearity property for linear transformations of a vector, hence the resulting diagonal structure of the form (11). This is essentially the previous ICA work (Yeredor, 2000; Todros & Hero, 2013). Below we generalize these ideas first to the discrete ICA case and then to the CCA models (4)\u2013(6).\n3We find the name \u201cgeneralized covariance matrix\u201d to be more meaningful than \u201ccharrelation\u201d matrix as was proposed by previous authors (see, e.g. Slapak & Yeredor, 2012a,b).\nDiscrete ICA generalized covariance matrices. Like covariance matrices, generalized covariance matrices of a vector with independent components are diagonal: they satisfy the multilinearity property CD\u03b1(h) = D C\u03b1(h)D>, and are equal to covariance matrices when h = 0. Therefore, we can expect that the derivations of the diagonal form (11) of the S-covariance matrices extends to the generalized covariance matrices case. By analogy with (10), we define the generalized S-covariance matrix of DICA:\nS(t) := Cx(t)\u2212 diag[Ex(t)]. (22)\nTo derive the analog of the diagonal form (11) for S(t), we have to compute all the expectations in (20) and (21) for a Poisson random variable x with the parameter y = D\u03b1. Just to provide some intuition, we compute here one of these expectations (see Appendix B.2 for further derivations):\nE(xx>et >x) = E[E(xx>et >x | y)]\n= diag[et]E(yy>ey >(et\u22121))diag[et]\n= ( diag[et]D ) E(\u03b1\u03b1>e\u03b1 >h(t)) ( diag[et]D )> ,\nwhere h(t) = D>(et \u2212 1) and et denotes an M -vector with the m-th component equal to etm . This gives\nS(t) = ( diag[et]D ) C\u03b1 (h(t)) ( diag[et]D )> , (23)\nwhich is a diagonal form similar (and equivalent for t = 0) to (11) since the generalized covariance matrix C\u03b1(h) of independent sources is diagonal (see (42) in Appendix B.1). Therefore, the generalized S-covariance matrices, estimated at different processing points t, can be used as a substitute of the T-cumulant tensors in the moment matching framework. Interestingly enough, the T-cumulant tensor (10) can be approximated by the generalized covariance matrix via its directional derivative (see Appendix B.5).\nCCA generalized covariance matrices. For the CCA models (4)\u2013(6), straightforward generalizations of the ideas from Section 3.1 leads to the following definition of the generalized CCA S-covariance matrix:\nS12(t) := E(x1x>2 et >x) E(et>x) \u2212 E(x1e t>x) E(et>x) E(x>2 et >x) E(et>x) , (24)\nwhere the vectors x and t are obtained by vertically stacking x1 & x2 and t1 & t2 as in (8). In the discrete CCA case, S12(t) is essentially the upper right block of the generalized S-covariance matrix S(t) of DICA and has the form\nS12(t) = ( diag[et1 ]D1 ) C\u03b1(h(t)) ( diag[et2 ]D2 )> , (25)\nwhere h(t) = D>(et\u2212 1) and the matrix D is obtained by vertically stacking D1 & D2 by analogy with (8). For non-Gaussian CCA, the diagonal form is\nS12(t) = D1 C\u03b1 (h(t)) D>2 , (26)\nwhere h(t) = D>1 t1 +D > 2 t2. Finally, for mixed CCA,\nS12(t) = D1 C\u03b1 (h(t)) ( diag[et2 ]D2 )> , (27)\nwhere h(t) = D>1 t1 +D > 2 (e t2\u22121). Since the generalized covariance matrix of the sources C\u03b1(\u00b7) is diagonal, expressions (25)\u2013(27) have the desired diagonal form (see Appendix B.4 for detailed derivations)."}, {"heading": "4 Joint diagonalization algorithms", "text": "The standard algorithms such as TPM or orthogonal joint diagonalization cannot be used for the estimation of D1 andD2. Indeed, even after whitening, the matrices appearing in the diagonal form (16)&(18) or (25)\u2013(27) are not orthogonal. As an alternative, we use Jacobi-like non-orthogonal diagonalization algorithms (Fu & Gao, 2006; Iferroudjene et al., 2009; Luciani & Albera, 2010). These algorithms are discussed in this section and in Appendix E.\nThe estimation of the factor loading matricesD1 andD2 of the CCA models (4)\u2013(6) via non-orthogonal joint diagonalization algorithms consists of the following steps: (a) construction of a set of matrices to be jointly diagonalized (using finite sample estimators), (b) a whitening step, (c) a non-orthogonal joint diagonalization step, and (d) the final estimation of the factor loading matrices (Appendix D.4).\nMatrices for diagonalization. There are two ways to construct matrices for subsequent joint diagonalization: either with the CCA S-matrices (15) and T-cumulants (17) (only DCCA) or the generalized covariance matrices (24) (D/N/MCCA). Given a dataset, these matrices are estimated using natural finite sample estimators (see Appendices C.1 and C.2).\nWhen dealing with S- and T-cumulants, the matrices are obtained via tensor projections. We define a projection T (v) \u2208 RM1\u00d7M2 of a third-order tensor T \u2208 RM1\u00d7M2\u00d7M3 onto a vector v \u2208 RM3 as\n[T (v)]m1m2 := M3\u2211 m3=1 [T ]m1m2m3vm3 . (28)\nNote that the projection T (v) is a matrix. Therefore, given 2P vectors {v11, v21, v12, v22, . . . , v1P , v2P }, one can construct 2P + 1 matrices\n{S12, T121(v1p), T122(v2p), for p = 1, . . . , P}, (29)\nwhich have the diagonal form (16) and (18). Importantly, the tensors are never constructed (see Anandkumar et al. (2012, 2014); Podosinnikova et al. (2015) and Appendix C.2).\nAlternatively to (29), the set of matrices can be constructed by estimating the generalized S-covariance matrices at P + 1 processing points 0, t1, . . . , tP \u2208 RM1+M2 :\n{S12 = S12(0), S12(t1), . . . , S12(tP )}, (30)\nwhich also have the diagonal form (25)\u2013(27). It is interesting to mention the connection between the Tcumulants and the generalized S-covariance matrices. The T-cumulant can be approximated via the directional derivative of the generalized covariance matrix (see Appendix B.5). However, in general, e.g., S12(t) with t = [t1; 0] is not exactly the same as T121(t1) and the former can be non-zero even when the latter is zero. This is important since order-4 and higher statistics are used with the method of moments when there is a risk that an order-3 statistic is zero. In general, the use of higher-order statistics increases the sample complexity and makes the resulting expressions quite complicated. Therefore, replacing the T-cumulants with the generalized S-covariance matrices is potentially beneficial.\nWhitening. The matrices W1 \u2208 RK\u00d7M1 and W2 \u2208 RK\u00d7M2 are called whitening matrices of S12 if\nW1S12W > 2 = IK , (31)\nwhere IK is the K-dimensional identity matrix. Such matrices W1 and W2 are only defined up to multiplication by any invertible matrix Q \u2208 RK\u00d7K , since any pair of matrices W\u03031 = QW1 and W\u03032 = Q\u2212>W2 also satisfy (31). In fact, using higher order information (i.e. the T-cumulants or the generalized covariances for t 6= 0) allows to solve this ambiguity.\nThe whitening matrices can be computed via SVD of S12 (see Appendix D.1). When M1 and M2 are too large, one can use a randomized SVD algorithm (see, e.g., Halko et al., 2011) to avoid the construction of the large matrix S12 and to decrease the computational time.\nNon-orthogonal joint diagonalization (NOJD). For simplicity, let us consider joint diagonalization of the generalized covariance matrices (30) (the same procedure holds for the S- and T-cumulants (29); see Appendix D.2). Given the whitening matrices W1 and W2, the transformation of the generalized covariance matrices (30) gives P + 1 matrices\n{W1S12W>2 , W1S12(tp)W>2 , p = 1, . . . , P}, (32)\nwhere each matrix is in RK\u00d7K and has reduced dimension since K < M1,M2. In practice, finite sample estimators are used to construct (30) (see Appendices C.1 and C.2).\nDue to the diagonal form (16) and (25)\u2013(27), each matrix in (30) has the form4 (W1D1) diag(\u00b7) (W2D2)>. Both D1 and D2 are (full) K-rank matrices and W1 and W2 are K-rank by construction. Therefore, the square matrices V1 = W1D1 and V2 = W2D2 are invertible. From (16) and (31), we get V1cov(\u03b1)V >2 = I and hence V2 = diag[var(\u03b1)\u22121]V \u221211 (the covariance matrix of the sources is diagonal and we assume they are non-deterministic, i.e. var(\u03b1) 6= 0). Substituting this into W1S12(t)W>2 and using the diagonal form (25)\u2013(27), we obtain that the matrices in (30) have the form V1diag(\u00b7)V \u221211 . Hence, we deal with the problem of the following type: Given P non-defective (a.k.a. diagonalizable) matrices B = {B1, . . . , BP }, where each matrix Bp \u2208 RK\u00d7K , find and invertible matrix Q \u2208 RK\u00d7K such that\nQBQ\u22121 = {QB1Q\u22121, . . . , QBPQ\u22121} (33) 4 Note that when the diagonal form has terms diag[et], we simply multiply the expression by diag[e\u2212t].\nare (jointly) as diagonal as possible. This can be seen as a joint non-symmetric eigenvalue problem. This problem should not be confused with classical joint diagonalization problem by congruence (JDC), where Q\u22121 is replaced by Q>, except when Q is an orthogonal matrix (Luciani & Albera, 2010). JDC is often used for ICA algorithms or moment matching based algorithms for graphical models when a whitening step is not desirable (see, e.g., Kuleshov et al. (2015) and references therein). However, neither JDC nor the orthogonal diagonalization-type algorithms (such as, e.g., the tensor power method Anandkumar et al., 2014) are applicable for the problem (33).\nTo solve the problem (33), we use the Jacobi-like non-orthogonal joint diagonalization (NOJD) algorithms (e.g., Fu & Gao, 2006; Iferroudjene et al., 2009; Luciani & Albera, 2010). These algorithms are an extension of the orthogonal joint diagonalization algorithms based on Jacobi (=Givens) rotations (Golub & Van Loan, 1996; Bunse-Gerstner et al., 1993; Cardoso & Souloumiac, 1996). Due to the space constraint, the description of the NOJD algorithms is moved to Appendix E. Although these algorithms are quite stable in practice, we are not aware of any theoretical guarantees about their convergence or stability to perturbation.\nSpectral algorithm. By analogy with the orthogonal case (Cardoso, 1989; Anandkumar et al., 2012), we can easily extend the idea of the spectral algorithm to the non-orthogonal one. Indeed, it amounts to performing whitening as before and constructing only one matrix with the diagonal structure, e.g., B = W1S12(t)W>2 for some t. Then, the matrix Q is obtained as the matrix of the eigenvectors of B. The vector t can be, e.g., chosen as t = Wu, where W = [W1; W2] and u \u2208 RK is a vector sampled uniformly at random.\nThis spectral algorithm and the NOJD algorithms are closely connected. In particular, when B has real eigenvectors, the spectral algorithm is equivalent to NOJD of B. Indeed, in such case, NOJD boils down to an algorithm for a non-symmetric eigenproblem (Eberlein, 1962; Ruhe, 1968). In practice, however, due to the presence of noise and finite sample errors, B may have complex eigenvectors. In such case, the spectral algorithm is different from NOJD. Importantly, the joint diagonalization type algorithms are known to be more stable in practice (see, e.g., Bach & Jordan, 2003; Podosinnikova et al., 2015).\nWhile deriving precise theoretical guarantees is beyond the scope of this paper, the techniques outlined by Anandkumar et al. (2012) for the spectral algorithm for latent Dirichlet Allocation can potentially be extended. The main difference is obtaining the analogue of the SVD accuracy (Lemma C.3, Anandkumar et al., 2013) for the eigen decomposition. This kind of analysis can potentially be extended with the techniques outlined in (Chapter 4, Stewart & Sun, 1990). Nevertheless, with appropriate parametric assumptions on the sources, we expect that the above described extension of the spectral algorithm should lead to similar guarantee as the spectral algorithm of Anandkumar et al. (2012).\nSome important implementation details, including the choice of the processing points, are discussed in Appendix D."}, {"heading": "5 Experiments", "text": "Synthetic data. We sample synthetic data to have ground truth information for comparison. We sample from linear DCCA which extends linear CCA (7) such that each view is xj \u223c Poisson(Dj\u03b1 + Fj\u03b2j). The sources \u03b1 \u223c Gamma(c, b) and the noise sources \u03b2j \u223c Gamma(cj , bj), for j = 1, 2, are sampled from the gamma distribution (where b is the rate parameter). Let sj \u223c Poisson(Dj\u03b1) be the part of the sample due to the sources and nj \u223c Poisson(Fj\u03b2j) be the part of the sample due to the noise (i.e., xj = sj + nj). Then we define the expected sample length due to the sources and noise, respectively, as Ljs := E[ \u2211 m sjm] and\nLjn := E[ \u2211\nm njm]. For sampling, the target values Ls = L1s = L2s and Ln = L1n = L2n are fixed and the parameters b and bj are accordingly set to ensure these values: b = Kc/Ls and bj = Kjcj/Ln (see Appendix B.2 of Podosinnikova et al. (2015)). For the larger dimensional example (Fig. 2, right), each column of the matrices Dj and Fj , for j = 1, 2, is sampled from the symmetric Dirichlet distribution with the concentration parameter equal to 0.5. For the smaller 2D example (Fig. 2, left), they are fixed: D1 = D2 with [D1]1 = [D1]2 = 0.5 and F1 = F2 with [F1]11 = [F1]22 = 0.9 and [F1]12 = [F1]21 = 0.1. For each experiment, Dj and Fj , for j = 1, 2, are sampled once and, then, the x-observations are sampled for different sample sizes N = {500, 1, 000, 2, 000, 5, 000, 10, 000}, 5 times for each N .\nMetric. The evaluation is performed on a matrix D obtained by stacking D1 and D2 vertically (see also the comment after Thm. 1). As in Podosinnikova et al. (2015), we use as evaluation metric the normalized `1-error between a recovered matrix D\u0302 and the true matrix D with the best permutation of columns err1(D\u0302,D) := min\u03c0\u2208PERM 1 2K \u2211 k \u2016d\u0302\u03c0k \u2212 dk\u20161 \u2208 [0, 1]. The minimization is over the possible permutations \u03c0 \u2208 PERM of the columns of D\u0302 and can be efficiently obtained with the Hungarian algorithm for bipartite matching. The (normalized) `1-error takes the values in [0, 1] and smaller values of this error indicate better performance of an algorithm.\nAlgorithms. We compare DCCA (implementation with the S- and T-cumulants) and DCCAg (implementation with the generalized S-covariance matrices and the processing points initialized as described in Appendix D.3) to DICA and the non-negative matrix factorization (NMF) algorithm with multiplicative updates for divergence (Lee & Seung, 2000). To run DICA or NMF, we use the stacking trick (8). DCCA is set to estimate K components. DICA is set to estimate either K0 = K + K1 + K2 or M = M1 + M2 components (whichever is the smallest, since DICA cannot work in the over-complete case). NMF is always set to estimate K0 components. For the evaluation of DICA/NMF, the K columns with the smallest `1-error are chosen. NMF\u25e6 stands for NMF initialized with a matrix D of the form (8) with induced zeros; otherwise NMF is initialized with (uniformly) random non-negative matrices.\nSynthetic experiment. We first perform an experiment with discrete synthetic data in 2D (Fig. 2) and then repeat the same experiment when the size of the problem is 10 times larger. In practice, we observed that for K0 < M all models work approximately equally well, except for NMF which breaks down in high dimensions. In the over-complete case as in Fig. 2, DCCA works better. A continuous analogue of this experiment is presented in Appendix F.1.\nReal data (translation). Following Vinokourov et al. (2002), we illustrate the performance of DCCA by extracting bilingual topics from the Hansard collection (Vinokourov & Girolami, 2002) with aligned English and French proceedings of the 36-th Canadian Parliament. We first pre-process the dataset with the NLTK toolbox by Bird et al. (2009) (see details in Appendix F.3) to obtain N = 12, 932 aligned documents with M1 = M2 = 5, 000 English and French words. We then run DCCA withK = 20 to extract the aligned topics. Some of the extracted topics are presented in Table 1 and all the topics as well as the detailed description of this experiment are presented in Appendix F.3.\nRunning time. For the real experiment above, the runtime of DCCA algorithm is 24 seconds including 22 seconds for SVD at the whitening step. In general, the computational complexity of the D/N/MCCA algorithms is bounded by the time of SVD plus O(RNK) + O(NK2), where R is the largest number of non-zero components in the stacked vector x = [x1; x2], plus the time of NOJD for the matrices of size K. In practice, DCCAg is faster than DCCA."}, {"heading": "Conclusion", "text": "We have proposed the first identifiable versions of CCA, together with moment matching algorithms which allow the identification of the loading matrices in a semi-parametric framework, where no assumptions are made regarding the distribution of the source or the noise. We also introduce a new sets of moments (our generalized covariance matrices), which could prove useful in other settings.\nAcknowledgments. This work was partially supported by the MSR-Inria Joint Center."}, {"heading": "6 Appendix", "text": "The appendix is organized as follows.\n- In Appendix A, we present the proof of Theorem 1 stating the identifiability of the CCA models (4)\u2013(6).\n- In Appendix B, we provide some details for the generalized covariance matrices: the form of the generalized covariance matrices of independent variables (Appendix B.1), the derivations of the diagonal form of the generalized covariance matrices of discrete ICA (Appendix B.3), the derivations of the diagonal form of the generalized covariance matrices of the CCA models (4)\u2013(6) (Appendix B.4), and approximation of the T-cumulants with the generalized covariance matrix (Appendix B.5).\n- In Appendix C, we provide expressions for natural finite sample estimators of the generalized covariance matrices and the T-cumulant tensors for the considered CCA models.\n- In Appendix D, we discuss some rather technical implementation details: computation of whitening matrices (Appendix D.1), selection of the projection vectors for the T-cumulants and the processing points for the generalized covariance matrices (Appendix D.3), and the final estimation of the factor loading matrices (Appendix D.4).\n- In Appendix E, we describe the non-orthogonal joint diagonalization algorithms used in this paper.\n- In Appendix F, we present some supplementary experiments: a continuous analog of the synthetic experiment from Section 5 (Appendix F.1), an experiment to analyze the sensitivity of the DCCA algorithm with the generalized S-covariance matrices to the choice of the processing points (Appendix F.2), and a detailed description of the experiment with the real data from Section 5 (Appendices F.3 and F.4).\nA Identifiability\nIn this section, we prove that the factor loading matrices D1 and D2 of the non-Gaussian CCA (4), discrete CCA (5), and mixed CCA (6) models are identifiable up to permutation and scaling if at most one source \u03b1k is Gaussian. We provide a complete proof for the non-Gaussian CCA case and show that the other two cases can be proved by analogy.\nA.1 Identifiability of non-Gaussian CCA (4)\nThe proof uses the notion of the second characteristic function (SCF) of a random variable x \u2208 RM :\n\u03c6x(t) = logE(eit >x),\nfor all t \u2208 RM . The SCF completely defines the probability distribution of x (see, e.g., Jacod & Protter, 2004). Important difference between the SCF and the cumulant generating function (19) is that the former always exists.\nThe following property of the SCF is of central importance for the proof: if two random variables, z1 and z2, are independent, then \u03c6A1z1+A2z2(t) = \u03c6z1(A > 1 t) + \u03c6z2(A > 2 t), where A1 and A2 are any matrices of compatible sizes.\nWe can now use our CCA model to derive an expression of \u03c6x(t). Indeed, defining a vector x by stacking the vectors x1 and x2, the SCF of x for any t = [t1; t2], takes the form\n\u03c6x(t) = logE(eit > 1 x1+it > 2 x2)\n(a) = logE(ei\u03b1 >(D>1 t1+D > 2 t2)+i\u03b5 > 1 t1+i\u03b5 > 2 t2)\n(b) = logE(ei\u03b1 >(D>1 t1+D > 2 t2))\n+ logE(ei\u03b5 > 1 t1) + logE(ei\u03b5 > 2 t2)\n= \u03c6\u03b1(D > 1 t1 +D > 2 t2) + \u03c6\u03b51(t1) + \u03c6\u03b52(t2),\nwhere in (a) we substituted the definition (4) of x1 and x2 and in (b) we used the independence \u03b1 \u22a5\u22a5 \u03b51 \u22a5\u22a5 \u03b52. Therefore, the blockwise mixed derivatives of \u03c6x are equal to\n\u22021\u22022\u03c6x(t) = D1\u03c6 \u2032\u2032 \u03b1(D > 1 t1 +D > 2 t2)D > 2 , (34)\nwhere \u22021\u22022\u03c6x(t) := \u2207t1\u2207t2\u03c6x(h(t1, t2)) \u2208 RM1\u00d7M2 and \u03c6\u2032\u2032\u03b1(u) := \u22072u\u03c6\u03b1(u), does not depend on the noise vectors \u03b51 and \u03b52.\nFor simplicity, we first prove the identifiability result when all components of the common sources are nonGaussian. The high level idea of the proof is as follows. We assume two different representations of x1 and x2 and using (34) and the independence of the components of \u03b1 and the noises, we first show that the two potential dictionaries are related by an orthogonal matrix (and not any invertible matrix), and then show that this implies that the two potential sets of independent components are (orthogonal) linear combinations of each other, which, for non-Gaussian components which are not reduced to point masses, imposes that this orthogonal transformation is the combination of a permutation matrix and marginal scaling\u2014a standard result from the ICA literature (Comon, 1994, Theorem 11).\nLet us then assume that two equivalent representations of non-Gaussian CCA exist:\nx1 = D1\u03b1+ \u03b51 = E1\u03b2 + \u03b71, x2 = D2\u03b1+ \u03b52 = E2\u03b2 + \u03b72, (35)\nwhere the other sources \u03b2 = (\u03b21, . . . , \u03b2K) are also assumed mutually independent and non-degenerate. As a standard practice in the ICA literature and without loss of generality as the sources have non-degenerate components, one can assume that the sources have unit variances, i.e. cov(\u03b1, \u03b1) = I and cov(\u03b2, \u03b2) = I , by respectively rescaling the columns of the factor loading matrices. Under this assumption, the two expressions of the cross-covariance matrix are\ncov(x1, x2) = D1D > 2 = E1E > 2 , (36)\nwhich, given that D1, D2 have full rank, implies that5\nE1 = D1Q, E2 = D2Q \u2212>, (37)\nwhere Q \u2208 RK\u00d7K is some invertible matrix. Substituting the representations (35) into the blockwise mixed derivatives of the SCF (34) and using the expressions (37) give\nD1\u03c6 \u2032\u2032 \u03b1(D > 1 t1 +D > 2 t2)D > 2\n= D1Q\u03c6 \u2032\u2032 \u03b2(Q >D>1 t1 +Q \u22121D>2 t2)Q \u22121D>2 ,\nfor all t1 \u2208 RM1 and t2 \u2208 RM2 . Since the matrices D1 and D2 have full rank, this can be rewritten as\n\u03c6\u2032\u2032\u03b1(D > 1 t1 +D > 2 t2)\n= Q\u03c6\u2032\u2032\u03b2(Q >D>1 t1 +Q \u22121D>2 t2)Q \u22121,\nwhich holds for all t1 \u2208 RM1 and t2 \u2208 RM2 . Moreover, still since D1 and D2 have full rank, we have, for any u1, u2 \u2208 RK the existence of t1 \u2208 RM1 and t2 \u2208 RM2 , such that u1 = D>1 t1 and u2 = D>2 t2, that is,\n\u03c6\u2032\u2032\u03b1(u1 + u2) = Q\u03c6 \u2032\u2032 \u03b2(Q >u1 +Q \u22121u2)Q \u22121, (38)\nfor all u1, u2 \u2208 RK .\nWe will now prove two facts:\n(F1) For any vector v \u2208 RK , then \u03c6\u2032\u2032\u03b2((Q>Q\u2212 I)v) = \u2212I , which will imply that QQ> = I because of the non-Gaussian assumptions.\n(F2) If QQ> = I , then \u03c6\u2032\u2032\u03b1(u) = \u03c6 \u2032\u2032 Q\u03b2(u) for any u \u2208 RK , which will imply that Q is the composition of a\npermutation and a scaling. This will end the proof.\nProof of fact (F1). By letting u1 = Qv and u2 = \u2212Qv, we get:\n\u03c6\u2032\u2032\u03b1(0) = Q\u03c6 \u2032\u2032 \u03b2((Q >Q\u2212 I)v)Q\u22121, (39)\nSince6 \u03c6\u2032\u2032\u03b1(0) = \u2212cov(\u03b1) = \u2212I , one gets\n\u03c6\u2032\u2032\u03b2((Q >Q\u2212 I)v) = \u2212I,\nfor any v \u2208 RK . 5The fact thatD1,D2 have full rank and thatE1,E2 haveK columns, combined with (36), implies thatE1,E2 have also full rank.\n6 Note that\u22072u\u03c6\u03b1(u) = \u2212 E(\u03b1\u03b1>eiu\n>\u03b1)\nE(eiu>\u03b1) + E\u03b1(u)E\u03b1(u)>, where E\u03b1(u) = E(\u03b1e\niu>\u03b1)\nE(eiu>\u03b1) .\nUsing the property that \u03c6\u2032\u2032A>\u03b2(v) = A >\u03c6\u2032\u2032\u03b2(Av)A for any matrix A, and in particular with A = Q >Q \u2212 I , we have that \u03c6\u2032\u2032A>\u03b2(v) = \u2212A >A, i.e. is constant.\nIf the second derivative of a function is constant, the function is quadratic. Therefore, \u03c6A>\u03b2(\u00b7) is a quadratic function. Since the SCF completely defines the distribution of its variable (see,e.g., Jacod & Protter (2004)), A>\u03b2 must be Gaussian (the SCF of a Gaussian random variable is a quadratic function). Given Lemma 9 from Comon (1994) (i.e., Cramer\u2019s lemma: a linear combination of non-Gaussian random variables cannot be Gaussian unless the coefficients are all zero), this implies that A = 0, and hence Q>Q = I , i.e., Q is an orthogonal matrix.\nProof of fact (F2). Plugging Q> = Q\u22121 into (38), with u1 = 0 and u2 = u, gives\n\u03c6\u2032\u2032\u03b1(u) = Q\u03c6 \u2032\u2032 \u03b2(Q >u)Q> = \u03c6\u2032\u2032Q\u03b2(u), (40)\nfor any u \u2208 RK . By integrating both sides of (40) and using \u03c6\u03b1(0) = \u03c6Q\u03b2(0) = 0, we get that \u03c6\u03b1(u) = \u03c6Q\u03b2(u) + i\u03b3\n>u for all u \u2208 RK for some constant vector \u03b3. Using again that the SCF completely defines the distribution, it follows that \u03b1 \u2212 \u03b3 and Q\u03b2 have the same distribution. Since both \u03b1 and \u03b2 have independent components, this is only possible when Q = \u039bP , where P is a permutation matrix and \u039b is some diagonal matrix (Comon, 1994, Theorem 11).\nA.2 Case of a single Gaussian source\nWithout loss of generality, we assume that the potential Gaussian source is the first one for \u03b1 and \u03b2. The first change is in the proof of fact (F1). We use the same argument up to the point where we conclude that A>\u03b2 is a Gaussian vector. As only \u03b21 can be Gaussian, Cramer\u2019s lemma implies that only the first row of A can have non-zero components, that isA = Q>Q\u2212I = e1f>, where e1 is the first basis vector and f any vector. Since Q>Q is symmetric, we must have\nQ>Q = I + ae1e > 1 ,\nwhere a is a constant scalar different than \u22121 as Q>Q is invertible. This implies that Q>Q is an invertible diagonal matrix \u039b, and hence Q\u039b\u22121/2 is an orthogonal matrix, which in turn implies that Q\u22121 = \u039b\u22121Q>.\nPlugging this into (38) gives, for any u1 and u2:\n\u03c6\u2032\u2032\u03b1(u1 + u2) = Q\u03c6 \u2032\u2032 \u03b2(Q >u1 + \u039b \u22121Q>u2)\u039b \u22121Q>.\nGiven that diagonal matrices commute and that \u03c6\u2032\u2032\u03b2 is diagonal for independent sources (see Appendix B.1), this leads to\n\u03c6\u2032\u2032\u03b1(u1 + u2) = Q\u039b \u22121/2\u03c6\u2032\u2032\u03b2(Q >u1 + \u039b \u22121Q>u2)\u039b \u22121/2Q>.\nFor any given v \u2208 RK , we are looking for u1 and u2 such that Q>u1 + \u039b\u22121Q>u2 = \u039b\u22121/2Q>v and u1 + u2 = v, which is always possible by setting Q>u2 = (\u039b\u22121/2 + I)\u22121Q>v and Q>u1 = Q>v \u2212Q>u2 by using the special structure of \u039b. Thus, for any v,\n\u03c6\u2032\u2032\u03b1(v) = Q\u039b \u22121/2\u03c6\u2032\u2032\u03b2(\u039b \u22121/2Q>v)\u039b\u22121/2Q> = \u03c6\u2032\u2032Q\u039b\u22121/2\u03b2(v).\nIntegrating as previously, this implies that the characteristic function of \u03b1 and Q\u039b\u22121/2\u03b2 differ only by a linear function i\u03b3>v, and thus, that \u03b1 \u2212 \u03b3 and Q\u039b\u22121/2\u03b2 have the same distribution. This in turn, from Comon (1994, Theorem 11), implies that Q\u039b\u22121/2 is a product of a scaling and a permutation, which ends the proof.\nA.3 Identifiability of discrete CCA (5) and mixed CCA (6)\nGiven the discrete CCA model, the SCF \u03c6x(t) takes the form\n\u03c6x(t) = \u03c6\u03b1(D > 1 (e it1 \u2212 1) +D>2 (eit2 \u2212 1)) + \u03c6\u03b51(e it1 \u2212 1) + \u03c6\u03b52(eit2 \u2212 1),\nwhere eitj , for j = 1, 2, denotes a vector with the m-th element equal to ei[tj ]m , and we used the arguments analogous with the non-Gaussian case. The rest of the proof extends with a correction that sometimes one has to replace Dj with diag[eitj ]Dj and that uj = D>j (e\nitj \u2212 1) for j = 1, 2. For the mixed CCA case, only the part related to x2 and D2 changes in the same way as for the discrete CCA case."}, {"heading": "B The generalized expectation and covariance matrix", "text": "B.1 The generalized expectation and covariance matrix of the sources\nThe sources \u03b1 = (\u03b11, . . . , \u03b1K) are mutually independent. Therefore, for some h \u2208 RK , their CGF (19) K\u03b1(h) = logE(e\u03b1 >h) takes the form\nK\u03b1(h) = \u2211 k log [ E(e\u03b1khk) ] .\nTherefore, the k-th element of the generalized expectation (20) of \u03b1 is (separable in \u03b1k)\n[E\u03b1(h)]k = E(\u03b1ke\u03b1khk) E(e\u03b1khk)\n(41)\nand the generalized covariance (21) of \u03b1 is diagonal due to the separability and its k-th diagonal element is\n[C\u03b1(h)]kk = E(\u03b12ke\u03b1khk) E(e\u03b1khk) \u2212 [E\u03b1(h)]2k . (42)\nB.2 Some expectations of a Poisson random variable\nLet x \u2208 RM be a multivariate Poisson random variable with mean y \u2208 RM+ . Then, for some t \u2208 RM ,\nE(et >x) = ey >(et\u22121),\nE(xmet >x) = yme tmey >(et\u22121), E(x2met >x) = [ yme tm + 1 ] yme tmey >(et\u22121),\nE(xmxm\u2032et >x) = yme tmym\u2032e tm\u2032 ey >(et\u22121), m 6= m\u2032,\nwhere et denotes an M -vector with the m-th element equal to etm .\nB.3 The generalized expectation and covariance matrix of discrete ICA\nIn this section, we use the expectations of a Poisson random variable presented in Appendix B.2.\nGiven the discrete ICA model (9), the generalized expectation (20) of x \u2208 RM takes the form\nEx(t) = E(xet>x) E(et>x)\n= E [ E(xet>x|\u03b1) ] E [ E(et>x|\u03b1)\n] = diag[et]D\nE(\u03b1e\u03b1>h(t)) E(e\u03b1>h(t))\n= diag[et]DE\u03b1(h(t)),\nwhere t \u2208 RM is a parameter, h(t) = D>(et \u2212 1), and et denotes an M -vector with the m-th element equal to etm . Note that in the last equation we used the definition (20) of the generalized expectation E\u03b1(\u00b7).\nFurther, the generalized covariance (21) of x takes the form\nCx(t) = E(xx>et>x) E(et>x) \u2212 Ex(t)Ex(t)>\n= E [ E(xx>et>x|\u03b1) ] E [ E(et>x|\u03b1)\n] \u2212 Ex(t)Ex(t)>. Plugging into this expression the expression for Ex(t) and\nE(xx>et >x|\u03b1) = diag[et]DE(\u03b1\u03b1>e\u03b1 >h(t))D>diag[et] + diag[et]diag [ DE(\u03b1e\u03b1 >h(t)) ]\nwe get Cx(t) = diag[Ex(t)] + diag[et]DC\u03b1(h(t))D>diag[et],\nwhere we used the definition (21) of the generalized covariance of \u03b1."}, {"heading": "B.4 The generalized CCA S-covariance matrix", "text": "In this section we sketch the derivation of the diagonal form (27) of the generalized S-covariance matrix of mixed CCA (6). Expressions (25) and (26) can be obtained in a similar way.\nDenoting x = [x1; x2] and t = [t1; t2] (i.e. stacking the vectors as in (8)), the CGF (19) of mixed CCA (6) can be written as\nKx(t) = logE(et > 1 x1+t > 2 x2) = logE [ E(et > 1 x1+t > 2 x2 |\u03b1, \u03b51, \u03b52) ] (a) = logE [ E(et > 1 x1 |\u03b1, \u03b51)E(et > 2 x2 |\u03b1, \u03b52)\n] (b) = logE ( et > 1 (D1\u03b1+\u03b51)e(D2\u03b1+\u03b52) >(et2\u22121) )\n(c) = logE ( e\u03b1 >h(t) ) + logE ( e\u03b5 > 2 (e t2\u22121) ) + logE(et > 1 \u03b51),\nwhere h(t) = (D>1 t1 +D > 2 (e t2 \u2212 1), in (a) we used the conditional independence of x1 and x2, in (b) we used the first expression from Appendix B.2, and in (c) we used the independence assumption (3).\nThe generalized CCA S-covariance matrix is defined as\nS12(t) := \u2207t2\u2207t1Kx(t).\nIts gradient with respect to t1 is\n\u2207t1Kx(t) = D1E(\u03b1e\u03b1\n>h(t))\nE(e\u03b1>h(t)) +\nE(\u03b51et > 1 \u03b51)\nE(et>1 \u03b51) ,\nwhere the last term does not depend on t2. Computing the gradient of this expression with respect to t2 gives\nS12(t) = D1C\u03b1(h(t)) ( diag[et2 ]D2 )> ,\nwhere we substituted expression (42) for the generalized covariance of the independent sources.\nB.5 Approximation of the T-cumulants with the generalized covariance matrix\nLet fmm\u2032(t) = [Cx(t)]mm\u2032 be a function R \u2192 RM corresponding to the (m,m\u2032)-th element of the generalized covariance matrix. Then the following holds for its directional derivative at t0 along the direction t:\n\u3008\u2207fmm\u2032(t0), t\u3009 = lim \u03b4\u21920 fmm\u2032(t0 + \u03b4t)\u2212 fmm\u2032(t0) \u03b4 ,\nwhere \u3008\u00b7, \u00b7\u3009 stands for the inner product. Therefore, when using the fact that \u2207f(t0) = \u2207Cx(t) is the generalized cumulant of x at t0 and the definition of a projection of a tensor onto a vector (28), one obtains for t0 = 0 the approximation of the cumulant cum(x) with the generalized covariance matrix Cx(t).\nLet us define v1 = W>1 u1 and v1 = W > 2 u2 for some u1, u2 \u2208 RK . Then, approximations for the Tcumulants (17) of discrete CCA take the following form: W1T121(v1)W2 is approximated by the generalized S-covariances (24) S12(t) via the following expression\nW1T121(v1)W2 \u2248 W1S12(\u03b4t1)W > 2 \u2212W1S12(0)W>2 \u03b4 \u2212W1diag(v1)S12W>2 ,\nwhere t1 = ( v1 0 ) and W1T122(v2)W2 is approximated by the generalized S-covariances S12(t) via\nW1T122(v2)W2 \u2248 W1S12(\u03b4t2)W > 2 \u2212W1S12(0)W>2 \u03b4 \u2212W1S12diag(v2)W>2 ,\nwhere t2 = (\n0 v2\n) and \u03b4 are chosen to be small."}, {"heading": "C Finite sample estimators", "text": "C.1 Finite sample estimators of the generalized expectation and covariance matrix\nFollowing Yeredor (2000); Slapak & Yeredor (2012b), we use the most direct way of defining the finite sample estimators of the generalized expectation (20) and covariance matrix (21).\nGiven a finite sample X = {x1, x2, . . . , xN}, an estimator of the generalized expectation is\nE\u0302x(t) = \u2211\nn xnwn\u2211 n wn\nwhere weights wn = et >xn and an estimator of the generalized covariance is\nC\u0302x(t) = \u2211 n xnx > nwn\u2211\nn wn \u2212 E\u0302x(t)E\u0302x(t)>.\nSimilarly, an estimator of the generalized S-covariance matrix is then\nC\u0302x1,x2(t) = \u2211 n x1nx > 2nwn\u2211 n wn \u2212 \u2211 n x1nwn\u2211 n wn \u2211 n x > 2nwn\u2211 n wn ,\nwhere x = [x1; x2] and t = [t1; t2] for some t1 \u2208 RM1 and t2 \u2208 RM2 .\nSome properties of these estimators are analyzed by Slapak & Yeredor (2012b).\nC.2 Finite sample estimators of the DCCA cumulants\nIn this section, we sketch the derivation of unbiased finite sample estimators for the CCA cumulants S12, T121, and T122. Since the derivation is nearly identical to the derivation of the estimators for the DICA cumulants (see Appendix F.2 of Podosinnikova et al. (2015)), all details are omitted.\nGiven a finite sample X1 = {x11, x12, . . . , x1N} and X2 = {x21, x22, . . . , x2N}, the finite sample estimator of the discrete CCA S-covariance (15), i.e., S12 := cum(x1, x2), takes the form\nS\u030212 = \u03b71 [ X1X > 2 \u2212N E\u0302(x1)E\u0302(x2)> ] , (43)\nwhere E\u0302(x1) = N\u22121 \u2211 n x1n, E\u0302(x2) = N\u22121 \u2211 n x2n, and \u03b71 = 1/(N \u2212 1).\nSubstitution of the finite sample estimators of the 2nd and 3rd cumulants (see, e.g., Appendix C.4 of Podosinnikova et al. (2015)) into the definition of the DCCA T-cumulants (17) leads to the following expressions\nW\u03021T\u030212j(vj)W\u0302 > 2 = \u03b72[(W\u03021X1)diag(X > j vj)]\u2297 (W\u03022X2)\n+ \u03b72\u3008vj , E\u0302(xj)\u30092N [W\u03021E\u0302(x1)]\u2297 [W\u03022E\u0302(x2)]\n\u2212 \u03b72\u3008vj , E\u0302(xj)\u3009(W\u03021X1)\u2297 (W\u03022X2)\n\u2212 \u03b72[(W\u03021X1)(X>j vj)]\u2297 [W\u03022E\u0302(x2)]\n\u2212 \u03b72[W\u03021E\u0302(x1)]\u2297 [(W\u03022X2)(X>j vj)]\n\u2212 \u03b71(W\u0302 (j)1 X1)\u2297 (W\u0302 (j) 2 X2)\n+ \u03b71N [W\u0302 (j) 1 E\u0302(x1)]\u2297 [W\u0302 (j) 2 E\u0302(x2)],\nwhere \u03b72 = N/((N \u2212 1)(N \u2212 2)) and W\u0302 (1)1 = W\u03021diag(v1), W\u0302 (1) 2 = W\u03022, W\u0302 (2) 1 = W\u03021, and W\u0302 (2) 2 = W\u03022diag(v2).\nIn the expressions above, W\u03021 and W\u03022 denote whitening matrices of S\u030212, i.e. such that W\u03021S\u030212W\u0302>2 = I .\nD Implementation details"}, {"heading": "D.1 Computation of whitening matrices", "text": "One can compute such whitening matrices (31) via the singular value decomposition (SVD) of S12. Let S12 = U\u03a3V\n> be the SVD of S12, then one can define W1 = U1:K\u039b and W2 = V1:K\u039b, where U1:K and V1:K are the first K left- and right-singular vectors and \u039b = diag(\u03c3 \u22121/2 1 , . . . , \u03c3 \u22121/2 K ) and \u03c31, . . . , \u03c3K are the K largest singular values.\nAlthough SVD is computed only once, the size of the matrix S12 can be significant even for storage. To avoid construction of this large matrix and speed up SVD, one can use randomized SVD techniques (Halko et al., 2011). Indeed, since the sample estimator S\u030212 has the form (43), one can reduce this matrix by sampling two Gaussian random matrices \u21261 \u2208 RK\u0303\u00d7M1 and \u21262 \u2208 RK\u0303\u00d7M2 , where K\u0303 is slightly larger than K. Now, if U and V are the K largest singular vectors of the reduced matrix \u21261S\u030212\u21262, then \u2126 \u2020 1U and \u2126\u20202V are approximately (and up to permutation and scaling of the columns) the K largest singular vectors of S\u030212.\nD.2 Applying whitening transform to DCCA T-cumulants\nTransformation of the T-cumulants (29) with whitening matrices W1 and W2 gives new tensors T\u030212j \u2208 RK\u00d7K\u00d7K :\nT\u030212j := T12j \u00d71 W>1 \u00d72 W>2 \u00d73 W>j , (44)\nwhere j = 1, 2. Combining this transformation with the projection (28), one obtains 2P + 1 matrices\nW1S12W > 2 , W1T12j(W > j ujp)W > 2 , (45)\nwhere p = 1, . . . , P and j = 1, 2 and we used vjp = W>j ujp to take into account whitening along the third direction. By choosing ujp \u2208 RK to be the canonical vectors of the RK , the number of tensor projections is reduced from M = M1 +M2 to 2K.\nD.3 Choice of projection vectors or processing points\nFor the T-cumulants (29), we choose theK projection vectors as v1p = W>1 ep and v2p = W > 2 ep, where ep is one of the columns of theK-identity matrix (i.e., a canonical vector). For the generalized S-covariances (30), we choose the processing points as t1p = \u03b41v1p and t2p = \u03b42v2p, where \u03b4j , for j = 1, 2 are set to a small value such as 0.1 divided by \u2211 m E(|xjm|)/Mj , for j = 1, 2.\nWhen projecting a tensor T12j onto a vector, part of the information contained in this tensor gets lost. To preserve all information, one could project a tensor T12j onto the canonical basis of RMj to obtain Mj matrices. However, this would be an expensive operation in terms of both memory and computational time. In practice, we use the fact, that the tensor T12j , for J = 1, 2, is transformed with whitening matrices (44). Hence, the projection vector has to include multiplication by the whitening matrices. Since they reduce the dimension to K, choosing the canonical basis in RK becomes sufficient. Hence, the choice v1p = W>1 ep and v2p = W>2 ep, where ep is one of the columns of the K-identity matrix.\nImportantly, in practice, the tensors are never constructed (see Appendix C.2).\nThe choice of the processing points of the generalized covariance matrices has to be done carefully. Indeed, if the values of t1 or t2 are too large, the exponents blow up. Hence, it is reasonable to maintain the values of the processing points very small. Therefore, for j = 1, 2, we set tjp = \u03b4jvjp where \u03b4j is proportional to a parameter \u03b4 which is set to a small value (\u03b4 = 0.1 by default), and the scale is determined by the inverse of the empirical average of the component of xj , i.e.:\n\u03b4j := \u03b4 NMj\u2211N\nn=1 \u2211Mj m=1[|Xj |]mn , (46)\nfor j = 1, 2. See Appendix F.2 for an experimental comparison of different values of \u03b4 (the default value used in other experiments is \u03b4 = 0.1)."}, {"heading": "D.4 Finalizing estimation of D1 and D2", "text": "The non-orthogonal joint diagonalization algorithm outputs an invertible matrix Q. If the estimated factor loading matrices are not supposed to be non-negative (continuous case of NCCA (4)), then\nD1 = W \u2020 1Q, D2 = W \u2020 2Q \u22121,\n(47)\nwhere \u2020 stands for the pseudo-inverse. For the spectral algorithm, where Q are eigenvectors of a nonsymmetric matrix and are not guaranteed to be real, only real parts are kept after evaluating matrices D1 and D2 in accordance with (47).\nIf the matrices D1 and/or D2 have to be non-negative (the discrete case of DCCA (5) and MCCA (6)), they have to be further mapped. For that, we select the sign of each column such that the vector (column) has less negative than positive components, which is measured by the sum of squares of the components of each sign, (this is necessary since the scaling unidentifiability includes the scaling by \u22121) and then truncate all negative values at 0.\nIn practice, due to the scaling unidentifiability, each column of the obtained matrices D1 and D2 can be further normalized to have the unit `1-norm. This is applicable in all cases (D/M/NCCA)."}, {"heading": "E Jacobi-like joint diagonalization of non-symmetric matrices", "text": "Given N non-defective (a.k.a. diagonalizable) not necessary normal7 matrices\nA = {A1, A2, . . . , AN} ,\nwhere each matrix An \u2208 RM\u00d7M , find such matrix Q \u2208 RM\u00d7M that matrices\nQ\u22121AQ = { Q\u22121A1Q, Q \u22121A2Q, . . . , Q \u22121ANQ } are (jointly) as diagonal as possible. We refer to this problem as a non-orthogonal joint diagonalization (NOJD) problem.8\nAlgorithm 1 Non-orthogonal joint diagonalization (NOJD) 1: Initialize: A(0) \u2190 A and Q(0) \u2190 IM and iterations ` = 0 2: for sweeps k = 1, 2, . . . do 3: for p = 1, . . . , M \u2212 1 do 4: for q = p+ 1, . . . , M do 5: Increase ` = `+ 1 6: Find the (approx.) shear parameter y\u2217 defined in (54) 7: Find the Jacobi angle \u03b8\u2217 defined in (53) 8: Update Q(`) \u2190 Q(`\u22121)S(`)\u2217 U (`)\u2217 9: Update A(`) \u2190 U (`)>\u2217 S(`)\u22121\u2217 A(`\u22121)S(`)\u2217 U (`)\u2217 10: end for 11: end for 12: end for 13: Output: Q(`)\nAlgorithm. Non-orthogonal Jacobi-like joint diagonalization algorithms have the high level structure which is outlined in Alg. 1.\nThe algorithm iteratively constructs the sequence of matrices A(`) = { A\n(`) 1 , A (`) 2 , . . . , A (`) N\n} , which is\ninitialized with A(0) = A. Each such iteration ` corresponds to a single update (Line (1) of Alg. (1)) of the matrices with the optimal shear S(`)\u2217 and unitary U (`) \u2217 transforms:\nA(`)n = U (`)> \u2217 S (`)\u22121 \u2217 A (`\u22121) n S (`) \u2217 U (`) \u2217 ,\n7A real matrix A is normal if A>A = AA>. 8An orthogonal joint diagonalization problem corresponds to the case where the matrices A1, A2, . . . , AN are normal and, hence,\ndiagonalizable by an orthogonal matrix Q.\nwhere S(`)\u2217 = S(`)(y\u2217) and U (`) \u2217 = U (`)(\u03b8\u2217) for the chosen in accordance with some rules (see below) optimal shear parameter y\u2217 and optimal Jacobi (=Givens) angle \u03b8\u2217.\nFor the theoretical analysis purposes, the two transforms are considered separately:\nA\u2032(`)n = S (`)\u22121(y)A(`\u22121)n S (`)(y),\nA(`)n = A \u2032\u2032(`) n = U (`)>(\u03b8)A\u2032(`)n U (`)(\u03b8).\n(48)\nEach such iteration ` is a combination of the iteration k and the pivots p and q (see Alg. 1). The iteration k is referred to as a sweep. Within each sweep k, M(M \u2212 1)/2 pivots p < q are chosen in accordance with the lexicographical rule. The rule for the choice of pivots can affect convergence as was analyzed for the single matrix case (see, e.g., Ruhe, 1968; Eberlein, 1962), where more sophisticated rules were proposed for the algorithm to have a quadratic convergence phase. However, up to our best knowledge, no such analysis was done for the several matrices case. We assume the simple lexicographical rule all over the paper.\nThe shear transform is defined by the hyperbolic rotation matrix S(`) = S(`)(y) which is equal to the identity matrix except for the following entries(\nS (`) pp S (`) pq S (`) qp S (`) qp\n) = ( cosh y sinh y sinh y cosh y ) , (49)\nwhere the shear parameter y \u2208 R. The unitary transform is defined by the Jacobi (=Givens) rotation matrix U (`) = U (`)(\u03b8) which is equal to the identity matrix except for the following entries(\nU (`) pp U (`) pq U (`) qp U (`) qp\n) = ( cos \u03b8 sin \u03b8 \u2212 sin \u03b8 cos \u03b8 ) , (50)\nwhere the Jacobi (=Givens) angle \u03b8 \u2208 [ \u2212\u03c04 , \u03c0 4 ] .\nThe following two objective functions are of the central importance for this type of algorithms: (a) the sum of squares of all the off-diagonal elements of the matrices9 A\u2032\u2032(`) which are the transformed with the unitary transform U (`) matrices A\u2032(`):\nOff ( A\u2032\u2032(`) ) = N\u2211 n=1 Off ( U (`)>A\u2032(`)n U (`) )\n(51)\nand (b) the sum of the squared Frobenius norms of the matrices A\u2032(`) which are the transformed with the share transform S(`) matrices A(`\u22121):\u2225\u2225\u2225A\u2032(`)\u2225\u2225\u22252\nF = N\u2211 n=1 \u2225\u2225\u2225S(`)\u22121A(`\u22121)n S(`)\u2225\u2225\u22252 F . (52)\nWe refer to (51) as the diagonality measure and to (52) as the normality measure.\nAll the considered algorithms find the optimal Jacobi angle \u03b8\u2217 as the minimizer of the diagonality measure of the (unitary transformed) matrices A\u2032\u2032(`) (48):\n\u03b8\u2217 = arg min \u03b8\u2208[\u2212\u03c04 , \u03c0 4 ]\nOff ( A\u2032\u2032(`) ) , (53)\nwhich admits a unique closed form solution (Cardoso & Souloumiac, 1996). The optimal shear parameter y\u2217 is found10 as a minimizer of the normality measure of the (shear transformed) matrices A\u2032(`) (48):\ny\u2217 = arg min y\u2208R \u2225\u2225\u2225A\u2032(`)\u2225\u2225\u22252 F . (54)\nAll the considered algorithms (Fu & Gao, 2006; Iferroudjene et al., 2009; Luciani & Albera, 2010) solve this step only approximately. In particular, the sh-rt algorithm (Fu & Gao, 2006) approximates the equation for finding the nulls of the gradient of the objective; the JUST algorithm (Iferroudjene et al., 2009) replaces the normality measure with the diagonality measure and provides a closed form solution for the resulting problem; and the JDTM algorithm (Luciani & Albera, 2010) replaces the normality measure with the sum of only two squared elements A\u2032n,pq and A \u2032 n,qp and provides a closed form solution for the resulting problem.\n9In the JUST algorithm (Iferroudjene et al., 2009), this objective function is also considered for the (shear transformed) matrixA\u2032(`). 10The JUST algorithm is an exception here, since it minimizes the diagonality measure Off[A\u2032(`)] of the (shear transformed) matrices A\u2032(`) with respect to y.\nThe three NOJD algorithms can have slightly different convergence properties, however, for the purposes of this paper their performance can hardly be distinguished. That is, the difference in the performance of the algorithms in terms of the `1-error of the factor loading matrices is hardly noticeable. For the experiments, we use the JDTM algorithm, the other two algorithms could be equally used. To the best of our knowledge, no theoretical analysis of the NOJD algorithms is available, except for the single matrix case when they boil down to the (non-symmetric) eigenproblem (Eberlein, 1962; Ruhe, 1968).\nThe following intuitively explains why the normality measure, i.e. the sum of the squared Frobenius norms, has to be minimized at the shear transform. As (Ruhe, 1968) mention, for every matrix A and non-singular Q:\ninf Q\n\u2225\u2225Q\u22121AQ\u2225\u22252 F = \u2016\u039b\u20162F ,\nwhere \u039b is the diagonal matrix containing the eigenvalues of A. Therefore, a diagonalized version of the matrix A must have the smallest Frobenius norm. Since the unitary transform does not change the Frobenius norm, it can only be minimized with the shear transform. Further, if a matrix is normal, i.e. A>A = AA> with a symmetric matrix as a particular case, the upper triangular matrix in its Schur decomposition is zero (Golub & Van Loan, 1996, Chapter 7) and then the Schur vectors correspond to the (orthogonal in this case) eigenvectors of this matrix. Therefore, a normal non-defective matrix can be diagonalized by an orthogonal matrix, which preserves the Frobenius norm. Hence, the shear transform by minimizing the normality measure decreases the deviation from normality and then the unitary transform by minimizing the diagonality measure decreases the deviation from diagonality."}, {"heading": "F Supplementary experiments", "text": ""}, {"heading": "F.1 Continuous synthetic data", "text": "This experiment is essentially a continuous analogue to the synthetic experiment with the discrete data from Section 5.\nSynthetic data. We sample synthetic data from the linear non-Gaussian CCA (NCCA) model (7) with each view xj = Dj\u03b1 + Fj\u03b2j . The (non-Gaussian) sources are \u03b1 \u223c z\u03b1Gamma(c, b), where z\u03b1 is a Rademacher random variable (i.e., takes the values \u22121 or 1 with the equal probabilities). The noise sources are \u03b2j \u223c z\u03b2jGamma(cj , bj), for j = 1, 2, where again z\u03b2j is a Rademacher random variable. Parameters of the gamma distribution are initialized by analogy with the discrete case (see Section 5). The elements of the matrices Dj and Fj , for j = 1, 2, are sampled i.i.d. for the uniform distribution in [\u22121, 1]. Each column of Dj and Fj , for j = 1, 2, is normalized to have the unit `1-norm.\nAlgorithms. We compare gNCCA (the implementation of NCCA with the generalized S-covariance matrices with the default values of the parameters \u03b41 and \u03b42 as described in Appendix D.3) the spectral algorithm for NCCA (also with the generalized S-covariance matrices) to the JADE algorithm11 (Cardoso & Souloumiac, 1993) for independent component analysis (ICA) and to classical CCA.\nSynthetic experiment. In Fig. 3 (left and middle), the results of the experiment for the different number of topics are presented. The error of the classical CCA is high due to the mentioned unidentifiability issues.\n11 The code is available at: http://perso.telecom-paristech.fr/ cardoso/Algo/Jade/jadeR.m\nF.2 Sensitivity of the generalized covariance matrices to the choice of the processing points\nIn this section, we experimentally analyze the performance of the DCCAg algorithm based on the generalized S-covariance matrices vs. the parameters \u03b41 and \u03b42. We use the experimental setup of the synthetic discrete data from Section 5 with K1 = K2 = K = 10. The results are presented in Fig. 3 (right).\nF.3 Real data experiment \u2013 translation topics\nFor the real data experiment, we estimate the factor loading matrices (topics, in the following) D1 and D2 of aligned proceedings of the 36-th Canadian Parliament in English and French languages. This Hansard collection can be found at http://www.isi.edu/natural-language/download/hansard/.\nAlthough going into details of natural language processing (NLP) related problems is not the goal of this paper, we do minor pre-processing (see Appendix F.4) of this text data to improve the presentation of the estimated bilingual topics D1 and D2.\nThe 20 topics obtained with DCCA are presented in Tables 2\u20136. For each topic, we display the 20 most frequent words (ordered from top to bottom in the decreasing order). Most of the topic have quite clear interpretation. Moreover, we can often observe the pairs of words which are each others translations in the topics. Take, e.g.,\n- the topic 10: the phrase \u201cpension plan\u201d can be translated as \u201cre\u0301gime de retraite\u201d, the word \u201cbenefits\u201d as \u201cprestations\u201d, and abbreviations \u201cCPP\u201d and \u201cRPC\u201d stand for \u201cCanada Pension Plan\u201d and \u201cRe\u0301gime de pensions du Canada\u201d, respectively;\n- the topic 3: \u201cOTAN\u201d is the French abbreviation for \u201cNATO\u201d, the word \u201cwar\u201d is translated as \u201cguerre\u201d, and the word \u201cpeace\u201d as \u201cpaix\u201d;\n- the topic 9: \u201cNisga\u201d is the name of an Indigenous (or \u201caboriginal\u201d) people in British Columbia, the word \u201caboriginal\u201d translates to French as \u201cautochtontes\u201d, and, e.g., the word \u201cright\u201d can be translated as \u201cdroit\u201d.\nNote also that, e.g., in topics 10, although the French words \u201cans\u201d and \u201canne\u0301es\u201d are present in the French topic, their English translation \u201cyear\u201d is not, since it was removed as one of the 15 most frequent words in English (see Appendix F.4)."}, {"heading": "F.4 Data preprocessing", "text": "For the experiment, we use House Debate Training Set of the Hansard collection. To process this text data, we perform case conversion, stemming, and removal of some stop words. For stemming, the SnowballStemmer of the NLTK toolbox by Bird et al. (2009) was used for both English and French languages. Although this stemmer has particular problems (such as mapping several different forms of a word to a single stem in one language but not in the other), they are left beyond our consideration. Moreover, in addition to the standard stop words of the NLTK toolbox, we also removed the following words that we consider to be stop words for our task12 (and their possible forms):\n- from English: ask, become, believe, can, could, come, cost, cut, do, done, follow, get, give, go, know, let, like, listen, live, look, lost, make, may, met, move, must, need, put, say, see, show, take, think, talk, use, want, will, also, another, back, day, certain, certainly, even, final, finally, first, future, general, good, high, just, last, long, major, many, new, next, now, one, point, since, thing, time, today, way, well, without;\n- from French (translations in brackets): demander (ask), doit (must), devenir (become), dit (speak, talk), devoir (have to), donner (give), ila (he has), met (put), parler (speak, talk), penser (think), pourrait (could), pouvoir (can), prendre (take), savoir (know), aller (go), voir (see), vouloir (want), actuellement, apre\u0300s (after), aujourd\u2019hui (today), autres (other), bien (good), beaucoup (a lot), besoin (need), cas (case), cause, cela (it), certain, chose (thing), de\u0301ja\u0300 (already), dernier (last), e\u0301gal (equal), entre (between), fac\u0327on (way), grand (big), jour (day), lorsque (when), neuf (new), passe\u0301 (past), plus, point, pre\u0301sent, pre\u0302ts (ready), prochain (next), quelque (some), suivant (next), unique.\n12This list of words was obtained by looking at words that appear in the top-20 words of a large number of topics in a first experiment. Removing these words did not change much the content of the topics, but made them much more interpretable.\nAfter stemming and removing stop words, several files had different number of documents in each language and had to be removed too. The numbers of these files are: 16, 36, 49 55, 88, 103, 110, 114, 123, 155, 159, 204, 229, 240, 2-17, 2-35.\nWe also removed the 15 most frequent words from each language. These include:\n- in English: Mr, govern, member, speaker, minist(er), Hon, Canadian, Canada, bill, hous(e), peopl(e), year, act, motion, question;\n- in French: gouvern(er), pre\u0301sident, loi, de\u0301put(e\u0301), ministr(e), canadien, Canada, projet, Monsieur, question, part(y), chambr(e), premi(er), motion, Hon.\nRemoving these words is not necessary, but improves the presentation of the learned topics significantly. Indeed, the most frequent words tend to appear in nearly every topic (often in pairs in both languages as translations of each other, e.g., \u201cmember\u201d and \u201cde\u0301pute\u0301\u201d or \u201cCanada\u201d in both languages, which confirms one more time the correctness of our algorithm).\nFinally, we select M1 = M2 = 5, 000 words for each language to form matrices X1 and X2 each containing N = 11, 969 documents in columns. As stemming removes the words endings, we map the stemmed words to the respective most frequent original words when showing off the topics in Tables 2-6."}, {"heading": "Supplementary References", "text": "Jacod, J. and Protter, P. Probability Essentials. Springer, 2004."}], "references": [], "referenceMentions": [], "year": 2016, "abstractText": "We introduce three novel semi-parametric extensions of probabilistic canonical correlation analysis with identifiability guarantees. We consider moment matching techniques for estimation in these models. For that, by drawing explicit links between the new models and a discrete version of independent component analysis (DICA), we first extend the DICA cumulant tensors to the new discrete version of CCA. By further using a close connection with independent component analysis, we introduce generalized covariance matrices, which can replace the cumulant tensors in the moment matching framework, and, therefore, improve sample complexity and simplify derivations and algorithms significantly. As the tensor power method or orthogonal joint diagonalization are not applicable in the new setting, we use non-orthogonal joint diagonalization techniques for matching the cumulants. We demonstrate performance of the proposed models and estimation techniques on experiments with both synthetic and real datasets.", "creator": "LaTeX with hyperref package"}}}