{"id": "1603.00162", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "1-Mar-2016", "title": "Convolutional Rectifier Networks as Generalized Tensor Decompositions", "abstract": "Convolutional rectifier networks, i.e. convolutional neural networks with rectified linear activation and max or average pooling, are the cornerstone of modern deep learning.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n", "histories": [["v1", "Tue, 1 Mar 2016 06:44:34 GMT  (827kb,D)", "http://arxiv.org/abs/1603.00162v1", null], ["v2", "Mon, 23 May 2016 12:52:16 GMT  (828kb,D)", "http://arxiv.org/abs/1603.00162v2", null]], "reviews": [], "SUBJECTS": "cs.NE cs.LG", "authors": ["nadav cohen", "amnon shashua"], "accepted": true, "id": "1603.00162"}, "pdf": {"name": "1603.00162.pdf", "metadata": {"source": "CRF", "title": "Convolutional Rectifier Networks as Generalized Tensor Decompositions", "authors": ["Nadav Cohen", "Amnon Shashua"], "emails": ["cohennadav@cs.huji.ac.il", "shashua@cs.huji.ac.il"], "sections": [{"heading": null, "text": "In this paper we describe a construction based on generalized tensor decompositions, that transforms convolutional arithmetic circuits into convolutional rectifier networks. We then use mathematical tools available from the world of arithmetic circuits to prove new results. First, we show that convolutional rectifier networks are universal with max pooling but not with average pooling. Second, and more importantly, we show that depth efficiency is weaker with convolutional rectifier networks than it is with convolutional arithmetic circuits. This leads us to believe that developing effective methods for training convolutional arithmetic circuits, thereby fulfilling their expressive potential, may give rise to a deep learning architecture that is provably superior to convolutional rectifier networks but has so far been overlooked by practitioners."}, {"heading": "1. Introduction", "text": "Deep neural networks are repeatedly proving themselves to be extremely effective machine learning models, providing state of the art accuracies on a wide range of tasks (see [17, 9]). Arguably, the most successful deep learning architecture to date is that of convolutional neural networks (ConvNets, [16]), which prevails in the field of computer vision, and is recently being harnessed for many other application domains as well (e.g. [25, 31, 2]). Modern Con-\nvNets are formed by stacking layers one after the other, where each layer consists of a linear convolutional operator followed by Rectified Linear Unit (ReLU [21]) activation (\u03c3(z) = max{0, z}), which in turn is followed by max or average pooling (P{cj} = max{cj} or P{cj} = mean{cj} respectively). Such models, which we refer to as convolutional rectifier networks, have driven the resurgence of deep learning ([15]), and represent the cutting edge of the ConvNet architecture ([28, 27]).\nDespite their empirical success, and the vast attention they are receiving, our theoretical understanding of convolutional rectifier networks is partial at best. It is believed that they enjoy depth efficiency, i.e. that when allowed to go deep, such networks can implement with polynomial size computations that would require super-polynomial size if the networks were shallow. However, formal arguments that support this are scarce. It is unclear to what extent convolutional rectifier networks leverage depth efficiency, or more formally, what is the proportion of weight settings that would lead a deep network to implement a computation that cannot be efficiently realized by a shallow network. We refer to the most optimistic situation, where this takes place for all weight settings but a negligible (zero measure) set, as complete depth efficiency.\nCompared to convolutional rectifier networks, our theoretical understanding of depth efficiency for arithmetic circuits, and in particular for convolutional arithmetic circuits, is much more developed. Arithmetic circuits (also known as Sum-Product Networks, [24]) are networks with two types of nodes: sum nodes, which compute a weighted sum of their inputs, and product nodes, computing the product of their inputs. The depth efficiency of arithmetic circuits has been studied by the theoretical computer science community for the last five decades, long before the resurgence of deep learning. Although many problems in the area remain open, significant progress has been made over the years, making use of various mathematical tools. Convolutional arithmetic circuits form a specific sub-class of arithmetic circuits. Namely, these are ConvNets with linear activation (\u03c3(z) = z) and product pooling (P{cj} = \u220f cj). Recently, [5] analyzed convolutional arithmetic circuits through ten-\nar X\niv :1\n60 3.\n00 16\n2v 1\n[ cs\n.N E\n] 1\nM ar\n2 01\n6\nsor decompositions, essentially proving, for the type of networks considered, that depth efficiency holds completely. Although convolutional arithmetic circuits are known to be equivalent to SimNets ([3]), a new deep learning architecture that has recently demonstrated promising empirical performance ([4]), they are fundamentally different from convolutional rectifier networks. Accordingly, the result established in [5] does not apply to models commonly used in practice.\nIn this paper we present a construction, based on the notion of generalized tensor decompositions, that transforms convolutional arithmetic circuits of the type described in [5] into convolutional rectifier networks. We then use the available mathematical tools from the world of arithmetic circuits to prove new results concerning the expressive power and depth efficiency of convolutional rectifier networks. Namely, we show that with ReLU activation, average pooling leads to loss of universality, whereas max pooling is universal but enjoys depth efficiency to a lesser extent than product pooling with linear activation (convolutional arithmetic circuits). These results indicate that from the point of view of expressive power and depth efficiency, convolutional arithmetic circuits (SimNets) have an advantage over the prevalent convolutional rectifier networks (ConvNets with ReLU activation and max or average pooling). This leads us to believe that developing effective methods for training convolutional arithmetic circuits, thereby fulfilling their expressive potential, may give rise to a deep learning architecture that is provably better than convolutional rectifier networks but has so far been overlooked by practitioners.\nThe remainder of this paper is organized as follows. In sec. 2 we review existing works relating to depth efficiency of arithmetic circuits and networks with ReLU activation. Sec. 3 presents our definition of generalized tensor decompositions, followed by sec. 4 which employs this concept to frame convolutional rectifier networks. In sec. 5 we make use of this framework for an analysis of the expressive power and depth efficiency of such networks. Finally, sec. 6 concludes."}, {"heading": "2. Related Work", "text": "The literature on the computational complexity of arithmetic circuits is far too wide to cover here, dating back over five decades. Although many of the fundamental questions in the field remain open, significant progress has been made over the years, developing and employing a vast share of mathematical tools from branches of geometry, algebra, analysis, combinatorics, and more. We refer the interested reader to [26] for a survey written in 2010, and mention here the more recent works [7] and [19] studying depth efficiency of arithmetic circuits in the context of deep learning (Sum-Product Networks). Compared to arithmetic cir-\ncuits, the literature on depth efficiency of neural networks with ReLU activation is far less developed, primarily since these models were only introduced several years ago ([21]). There have been some notable works on this line, but these employ dedicated mathematical machinery, not making use of the plurality of available tools from arithmetic circuits. [22] and [20] use combinatorial arguments to characterize the maximal number of linear regions in functions generated by ReLU networks, thereby establishing existence of depth efficiency. [30] uses semi-algebraic geometry to analyze the number of oscillations in functions realized by neural networks with semi-algebraic activations, ReLU in particular. The fundamental result proven in [30] is the existence, for every k \u2208 N, of functions realizable by networks with \u0398(k3) layers and \u0398(1) nodes per layer, which cannot be approximated by networks withO(k) layers unless these are exponentially large (have \u2126(2k) nodes). The work in [8] makes use of Fourier analysis to show existence of functions that are efficiently computable by depth-3 networks, yet require exponential size in order to be approximated by depth-2 networks. The result applies to various activations, including ReLU. [23] also compares the computational abilities of deep vs. shallow networks under different activations that include ReLU. However, the complexity measure considered in [23] is the VC dimension, whereas our interest lies in network size.\nNone of the analyses above account for convolutional networks 1, thus they do not apply to the deep learning architecture most commonly used in practice. Recently, [5] introduced convolutional arithmetic circuits, which may be viewed as ConvNets with linear activation and product pooling. These networks are shown to correspond to hierarchical tensor decompositions (see [11]). Tools from linear algebra, functional analysis and measure theory are then employed to prove that the networks are universal, and exhibit complete depth efficiency. Although similar in structure, convolutional arithmetic circuits are inherently different from convolutional rectifier networks (ConvNets with ReLU activation and max or average pooling). Accordingly, the analysis carried out in [5] does not apply to the networks at the forefront of deep learning.\nClosing the gap between the networks analyzed in [5] and convolutional rectifier networks is the topic of this paper. We achieve this by generalizing tensor decompositions, thereby opening the door to mathematical machinery as used in [5], harnessing it to analyze, for the first time, the depth efficiency of convolutional rectifier networks.\n1 By this we mean that in all analyses, the deep networks shown to benefit from depth (i.e. to realize functions that require super-polynomial size from shallow networks) are not ConvNets."}, {"heading": "3. Generalized Tensor Decompositions", "text": "We begin by establishing basic tensor-related terminology and notations. For our purposes, a tensor is simply a multi-dimensional array:\nAd1,...,dN \u2208 R , di \u2208 [Mi]\nThe order of a tensor is defined to be the number of indexing entries in the array, which are referred to as modes. The term dimension stands for the number of values an index can take in a particular mode. For example, the tensor A above has order N and dimension Mi in mode i, i \u2208 [N ]. The space of all possible configurations A can take is called a tensor space and is denoted, quite naturally, by RM1\u00d7\u00b7\u00b7\u00b7\u00d7MN .\nThe fundamental operator in tensor analysis is the tensor product, denoted by \u2297. It is an operator that intakes two tensors A \u2208 RM1\u00d7\u00b7\u00b7\u00b7\u00d7MP and B \u2208 RMP+1\u00d7\u00b7\u00b7\u00b7\u00d7MP+Q (orders P and Q respectively), and returns a tensor A \u2297 B \u2208 RM1\u00d7\u00b7\u00b7\u00b7\u00d7MP+Q (order P +Q) defined by:\n(A\u2297 B)d1,...,dP+Q = Ad1,...,dP \u00b7 BdP+1,...,dP+Q (1)\nNotice that in the case P = Q = 1, the tensor product reduces to the standard outer product between vectors, i.e. if u \u2208 RM1 and v \u2208 RM2 , then u \u2297 v is no other than the rank-1 matrix uv> \u2208 RM1\u00d7M2 .\nTensor decompositions (see [14] for a survey) may be viewed as schemes for expressing tensors using tensor products and weighted sums. For example, suppose we have a tensor A \u2208 RM1\u00d7\u00b7\u00b7\u00b7\u00d7MN given by:\nA = J\u2211\nj1...jN=1\ncj1...jN \u00b7 aj1,1 \u2297 \u00b7 \u00b7 \u00b7 \u2297 ajN ,N\nThis expression is known as a Tucker decomposition, parameterized by the coefficients {cj1...jN \u2208 R}j1...jN\u2208[J] and vectors {aj,i \u2208 RMi}i\u2208[N ],j\u2208[J]. It is different from the CP (rank-1) and Hierarchical Tucker decompositions our analysis will rely upon (see sec. 4). All decompositions however are closely related, specifically in the fact that they are based on iterating between tensor products and weighted sums.\nOur construction and analysis are facilitated by generalizing the tensor product, which in turn generalizes tensor decompositions. For an associative and commutative binary operator g, i.e. a function g : R \u00d7 R \u2192 R such that \u2200a, b, c \u2208 R : g(g(a, b), c) = g(a, g(b, c)) and \u2200a, b \u2208 R : g(a, b) = g(b, a), the generalized tensor product \u2297g , an operator intaking tensors A \u2208 RM1\u00d7\u00b7\u00b7\u00b7\u00d7MP ,B \u2208 RMP+1\u00d7\u00b7\u00b7\u00b7\u00d7MP+Q and returning tensor A\u2297g B \u2208 RM1\u00d7\u00b7\u00b7\u00b7\u00d7MP+Q , is defined as follows:\n(A\u2297g B)d1,...,dP+Q = g(Ad1,...,dP ,BdP+1,...,dP+Q) (2)\nGeneralized tensor decompositions are simply obtained by plugging in the generalized tensor product \u2297g in place of the standard tensor product \u2297."}, {"heading": "4. From Networks to Tensors", "text": "The ConvNet architecture analyzed in this paper is presented in fig. 1. The input to a network, denoted X , is composed of N patches x1 . . .xN \u2208 Rs. The first layer, referred to as representation, can be thought of as a generalized convolution. Namely, it consists of applying M representation functions f\u03b81 . . .f\u03b8M : Rs \u2192 R to the patches of the input, thereby creating M feature maps. In the case where the representation functions are standard neurons, i.e. f\u03b8d(x) = \u03c3(w > d x + bd) for parameters \u03b8d = (wd, bd) \u2208 Rs \u00d7 R and some chosen activation \u03c3(\u00b7), we obtain a conventional convolutional layer. More elaborate settings are possible, for example modeling the representation as a cascade of convolutional layers with pooling in-between.\nFollowing the representation, a network includes L hidden layers indexed by l = 0. . .L \u2212 1. Each hidden layer l begins with a 1\u00d71 conv operator, which is simply a 3D convolution with rl channels and receptive field 1\u00d7 1 followed by point-wise activation \u03c3(\u00b7). We allow the convolution to operate without weight sharing, in which case the filters that generate feature maps by sliding across the previous layer may have different coefficients at different spatial locations. This is often referred to in the deep learning community as a locally-connected layer (see [29]). We refer to it as the unshared case, in contrast to the shared case that gives rise to a standard 1\u00d71 convolution. The second (last) operator in a hidden layer is spatial pooling. Feature maps generated by 1\u00d7 1 conv are decimated, by applying the pooling operator P (\u00b7) (e.g. max or average) to non-overlapping 2D windows that cover the spatial extent. The last of the L hidden layers (l = L\u22121) reduces feature maps to singletons (i.e. its pooling operator is global), creating a vector of dimension rL\u22121. This vector is mapped into Y network outputs through a final dense linear layer.\nAltogether, the architectural parameters of a ConvNet are the type of representation functions (f\u03b8d ), the pooling window sizes (which in turn determine the number of hidden layers L), the setting of conv weights as shared or unshared, the number of channels in each layer (M for representation, r0. . .rL\u22121 for hidden layers, Y for output), and the choice of activation and pooling operators (\u03c3(\u00b7) and P (\u00b7) respectively). Given these architectural parameters, the learnable parameters of a network are the representation weights (\u03b8d), the conv weights (al,j,\u03b3 for hidden layer l, location j and channel \u03b3 in the unshared case; al,\u03b3 for hidden layer l and channel \u03b3 in the shared case), and the output weights (aL,1,y).\nThe choice of activation and pooling operators determines the type of network we arrive at. For linear acti-\nvation (\u03c3(z) = z) and product pooling (P{cj} = \u220f cj) we get a convolutional arithmetic circuit as analyzed in [5]. For ReLU activation (\u03c3(z) = max{0, z}) and max or average pooling (P{cj} = max{cj} or P{cj} = mean{cj} respectively) we get the commonly used convolutional rectifier networks, on which we focus in this paper.\nIn terms of pooling window sizes and network depth, we direct our attention to two special cases representing the extremes. The first is a shallow network that includes global pooling in its single hidden layer \u2013 see illustration in fig. 2. The second is the deepest possible network, in which all pooling windows cover only two entries, resulting in L = log2N hidden layers. These ConvNets, which we refer to as shallow and deep respectively, will be shown to correspond to canonical tensor decompositions. It is for this reason, and for the sake of simplicity, that we limit ourselves to these special cases. One may just as well consider networks of intermediate depths with different pooling window sizes, and that would correspond to other, nonstandard, tensor decompositions. The analysis carried out in sec. 5 can easily be adapted to such cases.\nIn a classification setting, the Y outputs of a network correspond to different categories, and prediction follows the output with highest activation. Specifically, if we denote by hy(\u00b7) the mapping from network input to output y, the predicted label for the instance X = (x1, . . . ,xN ) \u2208 (Rs)N is determined by the following classification rule:\ny\u0302 = argmax y\u2208[Y ] hy(X)\nWe refer to hy as the score function of category y. In this paper we study score functions through the notion of grid tensors. Given templates x(1) . . .x(M) \u2208 Rs, the grid tensor of hy , denoted A(hy), is the tensor of order N and di-\nmension M in each mode defined by:\nA(hy)d1...dN = hy(x(d1), . . . ,x(dN )) (3)\nThat is to say, the grid tensor of a score function under M templates, is a tensor of order N and dimension M in each mode, holding score values on an exponentially large grid of instances, where each instance in the grid has its N patches chosen from the set of M templates. Obviously two score functions are different if their grid tensors are different. We argue in app. A that in the case of structured compositional data (e.g. natural images), M \u2208 \u2126(100) suffices in order for the converse to hold as well. Specifically, if M is on the order of hundreds or more, the templates x(1) . . .x(M) can be chosen such that a score function is fully determined by its grid tensor, i.e. two score functions are effectively equivalent if their grid tensors are identical. Before heading on to our analysis of grid tensors generated by ConvNets, to simplify notation, we define F \u2208 RM\u00d7M to be the matrix holding the values taken by the representation functions f\u03b81 . . .f\u03b8M : Rs \u2192 R on the selected templates x(1) . . .x(M) \u2208 Rs:\nF :=  f\u03b81(x (1)) \u00b7 \u00b7 \u00b7 f\u03b8M (x(1)) ... . . .\n... f\u03b81(x (M)) \u00b7 \u00b7 \u00b7 f\u03b8M (x(M))  (4) To express the grid tensor of a ConvNet\u2019s score function using generalized tensor decompositions (see sec. 3), we set the underlying function g : R\u00d7R\u2192 R to be the activationpooling operator defined by:\ng(a, b) = P (\u03c3(a), \u03c3(b)) (5)\nwhere \u03c3(\u00b7) and P (\u00b7) are the network\u2019s activation and pooling functions, respectively. Notice that the activationpooling operator meets the associativity and commutativity\nrequirements under product pooling with linear activation (g(a, b) = a\u00b7b), and under max pooling with ReLU activation (g(a, b) = max{a, b, 0}). To account for the case of average pooling with ReLU activation, which a-priori leads to a non-associative activation-pooling operator, we simply replace average by sum, i.e. we analyze sum pooling with ReLU activation (g(a, b) = max{a, 0}+ max{b, 0}), which from the point of view of expressiveness is completely equivalent to average pooling with ReLU activation (scaling factors can always blend in to linear weights that follow pooling).\nWith the activation-pooling operator g in place, it is straightforward to see that the grid tensor of hSy \u2013 a score function generated by the shallow ConvNet (fig. 2), is given by the following generalized tensor decomposition:\nA ( hSy ) = Z\u2211 z=1 ayz \u00b7 (Faz,1)\u2297g \u00b7 \u00b7 \u00b7 \u2297g (Faz,N ) (6)\nZ here is the number of channels in the network\u2019s single hidden layer, {az,i \u2208 RM}z\u2208[Z],i\u2208[N ] are the weights in the hidden conv, and ay \u2208 RZ are the weights of output y. The factorization in eq. 6 generalizes the classic CP (CANDECOMP/PARAFAC) decomposition (see [14] for a historic survey), and we accordingly refer to it as the generalized CP decomposition.\nTurning to the deep ConvNet (fig. 1 with size-2 pooling windows and L = log2N hidden layers), the grid tensor of its score function hDy is given by the hierarchical generalized tensor decomposition below:\n\u03c61,j,\u03b3 = r0\u2211 \u03b1=1 a1,j,\u03b3\u03b1 (Fa 0,2j\u22121,\u03b1)\u2297g (Fa0,2j,\u03b1)\n\u00b7 \u00b7 \u00b7\n\u03c6l,j,\u03b3 = rl\u22121\u2211 \u03b1=1 al,j,\u03b3\u03b1 \u03c6 l\u22121,2j\u22121,\u03b1\ufe38 \ufe37\ufe37 \ufe38 order 2l\u22121 \u2297g \u03c6l\u22121,2j,\u03b1\ufe38 \ufe37\ufe37 \ufe38 order 2l\u22121\n\u00b7 \u00b7 \u00b7\n\u03c6L\u22121,j,\u03b3 = rL\u22122\u2211 \u03b1=1 aL\u22121,j,\u03b3\u03b1 \u03c6 L\u22122,2j\u22121,\u03b1\ufe38 \ufe37\ufe37 \ufe38\norder N4\n\u2297g \u03c6L\u22122,2j,\u03b1\ufe38 \ufe37\ufe37 \ufe38 order N4\nA ( hDy ) = rL\u22121\u2211 \u03b1=1\naL,1,y\u03b1 \u03c6 L\u22121,1,\u03b1\ufe38 \ufe37\ufe37 \ufe38 order N2 \u2297g \u03c6L\u22121,2,\u03b1\ufe38 \ufe37\ufe37 \ufe38 order N2\n(7)\nr0. . .rL\u22121 \u2208 N here are the number of channels in the network\u2019s hidden layers, {a0,j,\u03b3 \u2208 RM}j\u2208[N ],\u03b3\u2208[r0] are the weights in the first hidden conv, {al,j,\u03b3 \u2208 Rrl\u22121}l\u2208[L\u22121],j\u2208[N/2l],\u03b3\u2208[rl] are the weights in the following hidden convs, and aL,1,y \u2208 RrL\u22121 are the weights of output y. The factorization in eq. 7 generalizes the Hierarchical Tucker decomposition introduced in [10], and is accordingly referred to as the generalized HT decomposition.\nTo conclude this section, we presented a ConvNet architecture (fig. 1) whose activation and pooling operators may be chosen to realize convolutional arithmetic circuits (linear activation, product pooling) or convolutional rectifier networks (ReLU activation, max/average pooling). We then defined the grid tensor of a network\u2019s score function as a tensor holding function values on an exponentially large grid whose points are sequences with elements chosen from a finite set of templates. Then, we saw that the grid tensor of a shallow ConvNet (fig. 2) is given by the generalized CP decomposition (eq. 6), and a grid tensor of a deep ConvNet (fig. 1 with L = log2N ) is given by the generalized HT decomposition (eq. 7). In the next section we utilize the connection between ConvNets and generalized tensor decompositions for an analysis of the expressive power and depth efficiency of convolutional rectifier networks."}, {"heading": "5. Capacity Analysis", "text": "In this section we analyze score functions expressible by the shallow and deep ConvNets (fig. 2, and fig. 1 with L = log2N , respectively) under ReLU activation with max or average pooling (convolutional rectifier networks), comparing these settings against linear activation with product pooling (convolutional arithmetic circuits). Score functions are analyzed through grid tensors (eq. 3), represented by the generalized tensor decompositions established in the previous section: the generalized CP decomposition (eq. 6) corresponding to the shallow network, and the generalized HT decomposition (eq. 7) corresponding to the deep network. The analysis is organized as follows. In sec. 5.1 we present preliminary material required in order to follow our proofs. Sec. 5.2 discusses templates and representation functions, which form the bridge between score functions and generalized tensor decompositions. Sec. 5.3 presents matricization \u2013 a technical tool that facilitates the use of matrix theory for analyzing generalized tensor decompositions. The actual analysis begins in sec. 5.4, where we address the question of universality, i.e. of the ability of networks to realize any score function when their size is unlimited. This is followed by sec. 5.5 which studies depth effi-\nciency, namely, situations where functions efficiently computable by deep networks require shallow networks to have super-polynomial size. Finally, sec. 5.6 analyzes the case of coefficient sharing, in which the conv operators of our networks are standard convolutions (as opposed to the more general locally-connected layers)."}, {"heading": "5.1. Preliminaries", "text": "For evaluating the completeness of depth efficiency, and for other purposes as well, we are often interested in the \u201cvolume\u201d of sets in a Euclidean space, or more formally, in their Lebesgue measure. While an introduction to Lebesgue measure theory is beyond the scope of this paper (the interested reader is referred to [13]), we restate here several concepts and results our proofs will rely upon. A zero measure set can intuitively be thought of as having zero volume. A union of countably many zero measure sets is itself a zero measure set. If we randomize a point in space by some continuous distribution, the probability of hitting a zero measure set is always zero. A useful fact (proven in [1] for example) is that the zero set of a polynomial, i.e. the set of points on which a polynomial vanishes, is either the entire space (when the polynomial in question is the zero polynomial), or it must have measure zero. An open set always has positive measure, and when a point in space is drawn by a continuous distribution with non-vanishing continuous probability density function, the probability of hitting such a set is positive.\nApart from measure theory, we will also be using tools from the field of tensor analysis. Here too, a full introduction to the topic is beyond our scope (we refer the interested reader to [11]), and we only list some concepts and results that will be used. First, a fact that relates to abstract tensor products over function spaces is the following. If f\u03b81 . . .f\u03b8M : Rs \u2192 R are linearly independent functions, then the product functions {(x(1), . . . ,x(M)) 7\u2192\u220fM i=1 f\u03b8di (x\n(i))}d1...dM\u2208[M ] from (Rs)M to R are linearly independent as well. Back to tensors as we have defined them (multi-dimensional arrays), a very important concept is that of rank, which for order-2 tensors reduces to the standard notion of matrix rank. A tensor is said to have rank 1 if it may be written as a tensor product between non-zero vectors (A = v1 \u2297 \u00b7 \u00b7 \u00b7 \u2297 vN ). The rank of a general tensor is defined to be the minimal number of rank-1 tensors that may be summed up to produce it. A useful fact is that the rank of an order-N tensor with dimension Mi in each mode i \u2208 [N ], is no greater than \u220f iMi/maxiMi. On the other hand, all such tensors, besides a zero measure set, have rank equal to at least min{ \u220f i evenMi, \u220f i oddMi}. As in the special case of matrices, the rank is sub-additive, i.e. rank(A+B)\u2264rank(A)+rank(B) for any tensorsA,B of matching dimensions. The rank is sub-multiplicative w.r.t. the tensor product, i.e. rank(A \u2297 B)\u2264rank(A)\u00b7rank(B)\nfor any tensors A,B. Finally, we use the fact that permuting the modes of a tensor does not alter its rank."}, {"heading": "5.2. Templates and Representation Functions", "text": "The expressiveness of our ConvNets obviously depends on the possible forms that may be taken by the representation functions f\u03b81 . . .f\u03b8M : Rs \u2192 R. For example, if representation functions are limited to be constant, the ConvNets can only realize constant score functions. We denote by F := {f\u03b8 : Rs \u2192 R : \u03b8 \u2208 \u0398} the parametric family from which representation functions are chosen, and make two mild assumptions on this family:\n\u2022 Continuity: f\u03b8(x) is continuous w.r.t. both \u03b8 and x.\n\u2022 Non-degeneracy: For any x(1) . . .x(M) \u2208 Rs such that xi 6= xj \u2200i6=j, there exist f\u03b81 . . .f\u03b8M \u2208 F for which the matrix F defined in eq. 4 is non-singular.\nBoth of the assumptions above are met for most reasonable choices ofF . In particular, non-degeneracy holds when representation functions are standard neurons:\nClaim 1. The parametric family:\nF = { f\u03b8(x) = \u03c3(w >x + b) : \u03b8 = (w, b) \u2208 Rs \u00d7 R } (8) where \u03c3(\u00b7) is any sigmoidal activation 2 or the ReLU activation, meets the non-degeneracy condition (i.e. for any distinct x(1) . . .x(M) \u2208 Rs there exist f\u03b81 . . .f\u03b8M \u2208 F such that the matrix F defined in eq. 4 is non-singular).\nProof. We first show that given distinct x(1) . . .x(M) \u2208 Rs, there exists a vector w \u2208 Rs such that w>x(i) 6= w>x(j) for all 1\u2264i < j\u2264M . w satisfies this condition if it is not perpendicular to any of the finitely many non-zero vectors {x(i) \u2212 x(j) : 1\u2264i < j\u2264M}. If for every 1\u2264i < j\u2264M we denote by P (i,j) \u2282 Rs the set of points perpendicular to x(i) \u2212 x(j), we obtain that w satisfies the desired condition if it does not lie in the union \u22c3 1\u2264i<j\u2264M P (i,j). Each P (i,j) is the zero set of a non-zero polynomial, and in particular has measure zero. The finite union \u22c3 1\u2264i<j\u2264M P\n(i,j) thus has measure zero as well, and accordingly cannot cover the entire space. This implies that w \u2208 Rs \\ \u22c3 1\u2264i<j\u2264M P (i,j) indeed exists. Assume without loss of generality w>x(1) < . . . < w>x(M). We may then choose b1. . .bM \u2208 R such that \u2212w>x(M) < bM < . . . < \u2212w>x(1) < b1. For i, j \u2208 [M ], w>x(i) + bj is positive when j\u2264i and negative when j > i. Therefore, if \u03c3(\u00b7) is chosen as the ReLU activation, defining f\u03b8j (x) = \u03c3(w\n>x + bj) for every j \u2208 [M ] gives rise to a matrix F (eq. 4) that is lower triangular with non-zero\n2 \u03c3(\u00b7) is sigmoidal if it is monotonic with limz\u2192\u2212\u221e \u03c3(z) = c and limz\u2192+\u221e \u03c3(z) = C for some c 6=C in R.\nvalues on its diagonal. This proves the desired result for the case of ReLU activation.\nConsider now the case of sigmoidal activation, where \u03c3(\u00b7) is monotonic with limz\u2192\u2212\u221e \u03c3(z) = c and limz\u2192+\u221e \u03c3(z) = C for some c 6=C in R. Letting w \u2208 Rs and b1. . .bM \u2208 R be as above, we introduce a scaling factor \u03b1 > 0, and define f\u03b8j (x) = \u03c3(\u03b1w\n>x + \u03b1bj) for every j \u2208 [M ]. It is not difficult to see that as \u03b1 \u2192 +\u221e, the matrix F tends closer and closer to a matrix holding C on and below its diagonal, and c elsewhere. The latter matrix is non-singular, and in particular has non-zero determinant d 6= 0. The determinant of F converges to d as \u03b1 \u2192 +\u221e, so for large enough \u03b1, F is non-singular.\nNon-degeneracy means that given distinct templates, one may choose representation functions for which F is nonsingular. We may as well consider the opposite situation, where we are given representation functions, and would like to choose templates leading to non-singular F . Apparently, so long as the representation functions are linearly independent, this is always possible:\nClaim 2. Let f\u03b81 . . .f\u03b8M : Rs \u2192 R be any linearly independent continuous functions. Then, there exist x(1) . . .x(M) \u2208 Rs such that F (eq. 4) is non-singular.\nProof. We may view the determinant of F (eq. 4) as a function of (x(1), . . . ,x(M)):\ndetF (x(1), . . . ,x(M)) = \u2211 \u03b4\u2208SM sign(\u03b4) M\u220f i=1 f\u03b8\u03b4(i)(x (i))\nwhere SM stands for the permutation group on [M ], and sign(\u03b4) \u2208 {\u00b11} is the sign of the permutation \u03b4. This in particular shows that detF (x(1), . . . ,x(M)) is a non-zero linear combination of the product functions {(x(1), . . . ,x(M)) 7\u2192 \u220fM i=1 f\u03b8di (x\n(i))}d1...dM\u2208[M ]. Since these product functions are linearly independent (see sec. 5.1), detF (x(1), . . . ,x(M)) cannot be the zero function. That is to say, there exist x(1) . . .x(M) \u2208 Rs such that detF (x(1), . . . ,x(M)) 6= 0.\nAs stated previously, the analysis carried out in this paper studies score functions expressible by ConvNets through the notion of grid tensors. The translation of score functions into grid tensors is facilitated by the choice of templates x(1) . . .x(M) \u2208 Rs (eq. 3). For general templates, the correspondence between score functions and grid tensors is not injective \u2013 a score function corresponds to a single grid tensor, but a grid tensor may correspond to more than one score function. We use the term covering to refer to templates leading to an injective correspondence, i.e. to a situation where two score functions associated with the same grid tensor are effectively identical. In other words, the templates x(1) . . .x(M) are covering if\nthe value of score functions outside the exponentially large grid { Xd1...dN := (x (d1), . . . ,x(dN )) : d1. . .dN \u2208 [M ] }\nis irrelevant for classification. Some of the claims in our analysis will assume existence of covering templates (it will be stated explicitly when so). We argue in app. A that for structured compositional data (e.g. natural images), M \u2208 \u2126(100) suffices in order for this assumption to hold."}, {"heading": "5.3. Matricization", "text": "When analyzing grid tensors, we will often consider their arrangement as matrices. The matricization of a tensor A, denoted [A], is its arrangement as a matrix with rows corresponding to odd modes and columns corresponding to even modes. Specifically, if A \u2208 RM1\u00d7\u00b7\u00b7\u00b7\u00d7MN , and assuming for simplicity that the order N is even, the matricization [A] \u2208 R(M1\u00b7M3\u00b7...\u00b7MN\u22121)\u00d7(M2\u00b7M4\u00b7...\u00b7MN ) holds Ad1,...,dN in row index 1 + \u2211N/2 i=1(d2i\u22121\u2212 1) \u220fN/2 j=i+1M2j\u22121 and col-\numn index 1 + \u2211N/2 i=1(d2i \u2212 1) \u220fN/2 j=i+1M2j .\nThe matrix analogy of the tensor product \u2297 (eq. 1) is called the Kronecker product, and is denoted by . For A \u2208 RM1\u00d7M2 and B \u2208 RN1\u00d7N2 , A B is the matrix in RM1N1\u00d7M2N2 holding AijBkl in row index (i\u2212 1)N1 + k and column index (j \u2212 1)N2 + l. The relation [A \u2297 B] = [A] [B], where A and B are arbitrary tensors of even order, implies that the tensor and Kronecker products are indeed analogous, i.e. they represent the same operation under tensor and matrix viewpoints, respectively. We generalize the Kronecker product analogously to our generalization of the tensor product (eq. 2). For an associative and commutative binary operator g(\u00b7, \u00b7), the generalized Kronecker product g , is an operator that intakes matrices A \u2208 RM1\u00d7M2 and B \u2208 RN1\u00d7N2 , and returns a matrix A gB \u2208 RM1N1\u00d7M2N2 holding g(Aij , Bkl) in row index (i\u22121)N1 +k and column index (j\u22121)N2 + l. The relation between the tensor and Kronecker products holds for their generalized versions as well, i.e. [A\u2297g B] = [A] g [B] for arbitrary tensors A,B of even order.\nEquipped with the matricization operator [\u00b7] and the generalized Kronecker product g , we are now in a position to translate the generalized HT decomposition (eq. 7) to an expression for the matricization of a grid tensor generated by the deep ConvNet:\n\u03c61,j,\u03b3 = r0\u2211 \u03b1=1 a1,j,\u03b3\u03b1 (Fa 0,2j\u22121,\u03b1)\u2297g (Fa0,2j,\u03b1) (9)\n\u00b7 \u00b7 \u00b7[ \u03c6l,j,\u03b3 ] = rl\u22121\u2211 \u03b1=1 al,j,\u03b3\u03b1 [ \u03c6l\u22121,2j\u22121,\u03b1 ] \ufe38 \ufe37\ufe37 \ufe38 M2 l\u22122 -by-M2 l\u22122 g [ \u03c6l\u22121,2j,\u03b1 ] \ufe38 \ufe37\ufe37 \ufe38 M2 l\u22122 -by-M2 l\u22122\n\u00b7 \u00b7 \u00b7[ \u03c6L\u22121,j,\u03b3 ] = rL\u22122\u2211 \u03b1=1 aL\u22121,j,\u03b3\u03b1 [ \u03c6L\u22122,2j\u22121,\u03b1 ] \ufe38 \ufe37\ufe37 \ufe38 MN/8-by-MN/8 g [ \u03c6L\u22122,2j,\u03b1 ] \ufe38 \ufe37\ufe37 \ufe38 MN/8-by-MN/8[\nA ( hDy )] = rL\u22121\u2211 \u03b1=1 aL,1,y\u03b1 [ \u03c6L\u22121,1,\u03b1 ] \ufe38 \ufe37\ufe37 \ufe38 MN/4-by-MN/4 g [ \u03c6L\u22121,2,\u03b1 ] \ufe38 \ufe37\ufe37 \ufe38 MN/4-by-MN/4\nWe refer to this factorization as the matricized generalized HT decomposition. Notice that the expression above for \u03c61,j,\u03b3 is the same as in the original generalized HT decomposition, as order-2 tensors need not be matricized.\nFor the matricization of a grid tensor generated by the shallow ConvNet, we translate the generalized CP decomposition (eq. 6) into the matricized generalized CP decomposition:\n[ A ( hSy )] = (10)\nZ\u2211 z=1 ayz \u00b7 ( (Faz,1) g (Faz,3) g \u00b7 \u00b7 \u00b7 g (Faz,N\u22121) ) g(\n(Faz,2) g (Faz,4) g \u00b7 \u00b7 \u00b7 g (Faz,N ) )>\nThe matricized generalized CP and HT decompositions (eq. 10 and 9 respectively) will be used throughout our proofs to establish depth efficiency. This is generally done by providing a lower bound on rank[A(hDy )] \u2013 the rank of the deep ConvNet\u2019s matricized grid tensor, and an upper bound on rank[A(hSy )] \u2013 the rank of the shallow ConvNet\u2019s matricized grid tensor. The upper bound on rank[A(hSy )] will be linear in Z, and so requiring A(hSy ) = A(hDy ), and in particular rank[A(hSy )] = rank[A(hDy )], will give us a lower bound on Z. That is to say, we obtain a lower bound on the number of hidden channels in the shallow ConvNet, that must be met in order for this network to replicate a grid tensor generated by the deep ConvNet. Our analysis of depth efficiency is given in sec. 5.5. As a prerequisite, we first head on to sec. 5.4 to analyze universality."}, {"heading": "5.4. Universality", "text": "Universality refers to the ability of a network to realize (or approximate) any function of choice when no restrictions are imposed on its size. It is well-known that fullyconnected neural networks are universal under all types of non-linear activations typically used in practice, even if the number of hidden layers is restricted to one ([6, 12, 18]). To the best of our knowledge universality has never been studied in the context of convolutional rectifier networks. This is the purpose of the current section. Specifically, we analyze the universality of our shallow and deep ConvNets (fig. 2, and fig. 1 with L = log2N , respectively) under ReLU activation and max or average pooling.\nWe begin by stating a result similar to that given in [5], according to which convolutional arithmetic circuits are universal:\nClaim 3. Assuming covering templates exist, with linear activation and product pooling the shallow ConvNet is universal (hence so is the deep).\nProof. Let x(1) . . .x(M) \u2208 Rs be distinct covering templates, and f\u03b81 . . .f\u03b8M be representation functions for which F is invertible (non-degeneracy implies that such functions exist). With linear activation and product pooling the generalized CP decomposition (eq. 6) reduces to its standard version, which is known to be able to express any tensor when size is large enough (e.g. Z\u2265MN suffices). The shallow ConvNet can thus realize any grid tensor on covering templates, precisely meaning that it is universal. As for the deep ConvNet, setting r0 = \u00b7 \u00b7 \u00b7 = rL\u22121 = Z and al,j,\u03b3\u03b1 = 1 [\u03b1 = \u03b3], where l \u2208 [L \u2212 1] and 1 [\u00b7] is the indicator function, reduces its decomposition (eq. 7) to that of the shallow ConvNet (eq. 6). This implies that all grid tensors realizable by the shallow ConvNet are also realizable by the deep ConvNet.\nHeading on to convolutional rectifier networks, the following claim tells us that max pooling leads to universality:\nClaim 4. Assuming covering templates exist, with ReLU activation and max pooling the shallow ConvNet is universal (hence so is the deep).\nProof. The proof follows the same line as that of claim 3, except we cannot rely on the ability of the standard CP decomposition to realize any tensor of choice. Instead, we need to show that the generalized CP decomposition (eq. 6) with g(a, b) = max{a, b, 0} can realize any tensor, so long as Z is large enough. We will show that Z\u22652\u00b7MN suffices. For that, it is enough to consider an arbitrary indicator tensor, i.e. a tensor holding 1 in some entry and 0 in all other entries, and show that it can be expressed with Z = 2.\nLet A be an indicator tensor of order N and dimension M in each mode, its active entry being (d1, . . . , dN ). Denote by 1 \u2208 RM the vector holding 1 in all entries, and\nfor every i \u2208 [N ], let e\u0304di \u2208 RM be the vector holding 0 in entry di and 1 elsewhere. With the following weight settings, a generalized CP decomposition (eq. 6) with g(a, b) = max{a, b, 0} and Z = 2 produces A, as required:\n\u2022 ay1 = 1, a y 2 = \u22121\n\u2022 a1,1 = \u00b7 \u00b7 \u00b7 = a1,N = 1\n\u2022 \u2200i \u2208 [N ] : a2,i = e\u0304di\nAt this point we encounter the first somewhat surprising result, according to which convolutional rectifier networks are not universal with average pooling:\nClaim 5. With ReLU activation and average pooling, both the shallow and deep ConvNets are not universal.\nProof. Let x(1) . . .x(M) \u2208 Rs be any templates of choice, and consider grid tensors produced by the generalized CP and HT decompositions (eq. 6 and 7 respectively) with g(a, b) = max{a, 0}+ max{b, 0} (this corresponds to sum pooling and ReLU activation, but as stated in sec. 4, sum and average pooling are equivalent in terms of expressiveness). We will show that such grid tensors, when arranged as matrices, necessarily have low rank. This obviously implies that they cannot take on any value. Moreover, since the set of low rank matrices has zero measure in the space of all matrices (see sec. 5.1), the set of values that can be taken by the grid tensors has zero measure in the space of tensors with order N and dimension M in each mode.\nIn accordance with the above, we complete our proof by showing that with g(a, b) = max{a, 0} + max{b, 0}, the matricized generalized CP and HT decompositions (eq. 10 and 9 respectively) give rise to low-rank matrices. For the matricized generalized CP decomposition (eq. 10), corresponding to the shallow ConvNet, we have with g(a, b) = max{a, 0}+ max{b, 0}:[\nA ( hSy )] = v1> + 1u>\nwhere 1 is the vector in RMN/2 holding 1 in all entries, and v,u \u2208 RMN/2 are defined as follows:\nv := Z\u2211 z=1 ayz \u00b7max { (Faz,1) g \u00b7 \u00b7 \u00b7 g (Faz,N\u22121), 0 }\nu := Z\u2211 z=1 ayz \u00b7max { (Faz,2) g \u00b7 \u00b7 \u00b7 g (Faz,N ), 0 }\nObviously the matrix [ A ( hSy )] \u2208 RMN/2\u00d7MN/2 has rank 2 or less.\nTurning to the matricized generalized HT decomposition (eq. 9), which corresponds to the deep ConvNet, we have with g(a, b) = max{a, 0}+ max{b, 0}:[\nA ( hDy )] = V O +O U\nwhere is the standard Kronecker product (see definition in sec. 5.3), O \u2208 RMN/4\u00d7MN/4 is a matrix holding 1 in all entries, and the matrices V,U \u2208 RMN/4\u00d7MN/4 are given by:\nV := rL\u22121\u2211 \u03b1=1 aL,1,y\u03b1 max {[ \u03c6L\u22121,1,\u03b1 ] , 0 }\nU := rL\u22121\u2211 \u03b1=1 aL,1,y\u03b1 max {[ \u03c6L\u22121,2,\u03b1 ] , 0 }\nThe rank of O is obviously 1, and since the Kronecker product multiplies ranks, i.e. rank(A B) = rank(A)\u00b7rank(B) for any matrices A and B, we have that the rank of [ A ( hDy )] \u2208 RMN/2\u00d7MN/2 is at most 2\u00b7MN/4.\nIn particular, [ A ( hDy )] cannot have full rank.\nOne may wonder if perhaps the non-universality of ReLU activation and average pooling is merely an artifact of the conv operator in our ConvNets having 1\u00d71 receptive field. Apparently, as the following claim shows, expanding the receptive field does not remedy the situation, and indeed non-universality is an inherent property of convolutional rectifier networks with average pooling:\nClaim 6. Consider the network illustrated in fig. 3, obtained by expanding the conv receptive field in the shallow ConvNet from 1\u00d71 tow\u00d7h, wherew\u00b7h < N/2+1\u2212logM N (conv windows cover less than half the feature maps that precede them). Such a network, when equipped with ReLU activation and average pooling, is not universal.\nProof. Compare the original shallow ConvNet (fig. 2) to the shallow ConvNet with expanded receptive field that we consider in this claim (fig. 3). The original shallow ConvNet has 1\u00d71 receptive field, with conv entry in location i \u2208 [N ] and channel z \u2208 [Z] assigned through a cross-channel linear combination of the representation entries in the same location, the combination weights being az,i \u2208 RM . In the shallow ConvNet with receptive field expanded to w\u00d7h, linear combinations span multiple locations. In particular, conv entry in location i and channel z is now assigned through a linear combination of the representation entries at all channels that lie inside a spatial window revolving around i. We denote by {\u03c1(j; i)}j\u2208[w\u00b7h] the locations comprised by this window. More specifically, \u03c1(j; i) is the j\u2019th location in the window, and the linear weights that correspond to it are held in the j\u2019th column of the weight matrix Az,i \u2208 RM\u00d7w\u00b7h. We assume for simplicity that conv windows stepping out of\nbounds encounter zero padding 3, and adhere to the convention under which indexing the row of a matrix with d\u03c1(j;i) produces zero when location j of window i steps out of bounds.\nWe are interested in the case of ReLU activation (\u03c3(z) = max{0, z}) and average pooling (P{cj} = mean{cj}). Under this setting, for any selected templates x(1) . . .x(M) \u2208 Rs, the grid tensor of hS(w\u00d7h)y \u2013 network\u2019s y\u2019th score function, is given by:\nA(hS(w\u00d7h)y )d1,...,dN = N\u2211 i=1 Bid\u03c1(1;i),...,d\u03c1(w\u00b7h;i)\nwhere for every i \u2208 [N ], Bi is a tensor of order w\u00b7h and dimension M in each mode, defined by:\nBic1,...,cw\u00b7h = Z\u2211 z=1 ayz N max  w\u00b7h\u2211 j=1 (FAz,i)cj ,j , 0  Let O be a tensor of order N \u2212 w\u00b7h and dimension M in each mode, holding 1 in all entries. We may write:\nA(hS(w\u00d7h)y ) = N\u2211 i=1 pi(Bi \u2297O) (11)\nwhere for every i \u2208 [N ], pi(\u00b7) is an appropriately chosen operator that permutes the modes of an order-N tensor.\nWe now make use of some known facts related to tensor rank (see sec. 5.1), in order to show that eq. 11 is not universal, i.e. that there are many tensors which cannot be realized by A(hS(w\u00d7h)y ). Being tensors of order w\u00b7h and dimension M in each mode, the ranks of B1 . . .BN are bounded above byMw\u00b7h\u22121. SinceO is an all-1 tensor, and since permuting modes does not alter rank, we have: rank(pi(Bi \u2297 O))\u2264Mw\u00b7h\u22121 \u2200i \u2208 [N ]. Finally, from sub-additivity of the rank we get: rank(A(hS(w\u00d7h)y ))\u2264N \u00b7Mw\u00b7h\u22121. Now, we know by assumption that w\u00b7h < N/2 + 1 \u2212 logM N , and this implies: rank(A(hS(w\u00d7h)y )) < MN/2. Since there exist tensors of order N and dimension M in each mode having rank at least MN/2 (actually only a negligible set of tensors do not meet this), eq. 11 is indeed not universal. That is to say, the shallow ConvNet with conv receptive field expanded to w\u00d7h (fig. 3) cannot realize all grid tensors on the templates x(1) . . .x(M).\nWe conclude this section by noting that the nonuniversality result in claim 6 does not contradict the known universality of shallow (single hidden layer) fullyconnected neural networks. Indeed, a shallow fullyconnected network corresponds to the ConvNet considered\n3 Modifying our proof to account for different padding schemes (such as duplication or no padding at all) is trivial \u2013 we choose to work with zero padding merely for notational convenience.\nin claim 6 (fig. 3) with conv receptive field covering the entire spatial extent (w\u00b7h = N ), thereby effectively removing the pooling operator (assuming the latter realizes the identity on singletons). In claim 7 below we show that such a network, when equipped with ReLU activation, is universal. On the other hand, in claim 6 we assumed that the receptive field covers less than half the spatial extent (w\u00b7h < N/2 + 1 \u2212 logM N ), and have shown that with ReLU activation and average pooling, this leads to nonuniversality. Loosely speaking, our findings imply that for networks with ReLU activation, which are known to be universal when fully-connected, introducing locality disrupts universality with average pooling (and maintains it with max pooling).\nClaim 7. Assume there exist covering templates x(1) . . .x(M), and corresponding representation functions f\u03b81 . . .f\u03b8M leading to a matrix F (eq. 4) that has non-recurring rows and a constant non-zero column 4. Consider the fully-connected network illustrated in fig. 4, obtained by expanding the conv receptive field in the shallow ConvNet to cover the entire spatial extent. Such a network, when equipped with ReLU activation, is universal.\nProof. Let hS(fc)y be the y\u2019th score function of our shallow fully-connected network (fig. 4) when equipped with ReLU activation (\u03c3(z) = max{0, z}). We would like to show\n4 The assumption that such representation functions exist differs from our usual non-degeneracy assumption. The latter requires F to be nonsingular, whereas here we pose the weaker requirement of F having nonrecurring rows. On the other hand, here we also demand that F have a constant non-zero column, i.e. that there be a representation function f\u03b8d such that f\u03b8d (x (1)) = \u00b7 \u00b7 \u00b7 = f\u03b8d (x (M)) = c 6= 0. In claim 1 we showed that standard neurons meet the non-degeneracy assumption. A slight modification to its proof shows that they also meet the assumption made here. Namely, if we modify the constructions for the cases of ReLU activation and sigmoidal activation by setting f\u03b81 (x) = \u03c3(0\n>x+1) and f\u03b81 (x) = \u03c3(0\n>x+ \u03b1) respectively, we get matrices F that are not only non-singular, but also have a constant non-zero column.\nthat A(hS(fc)y ) \u2013 the grid tensor of hS(fc)y w.r.t. the covering templates x(1) . . .x(M), may take on any value when hidden and output weights ({Az}z\u2208[Z] and ay respectively) are chosen appropriately.\nFor any d1. . .dN \u2208 [M ], define the following matrix:\nF (d1...dN ) := f\u03b81(x (d1)) \u00b7 \u00b7 \u00b7 f\u03b8M (x(d1)) ... . . .\n... f\u03b81(x (dN )) \u00b7 \u00b7 \u00b7 f\u03b8M (x(dN ))  \u2208 RN\u00d7M In words, F (d1...dN ) is the matrix obtained by taking rows d1. . .dN from F (recurrence allowed), and stacking them one on top of the other. It holds that:\nA(hS(fc)y )d1...dN = Z\u2211 z=1 ayz max { 0, \u2329 F (d1...dN ), Az \u232a} where \u3008\u00b7, \u00b7\u3009 stands for the inner-product operator, i.e.\u2329 F (d1...dN ), Az \u232a := \u2211N i=1 \u2211M d=1 F (d1...dN ) i,d A z i,d.\nBy assumption F has a constant non-zero column. This implies that there exist j \u2208 [M ], c 6= 0 such that for any d1. . .dN \u2208 [M ], all entries in column j of F (d1...dN ) are equal to c. For every d1. . .dN \u2208 [M ] and z \u2208 [Z], denote by F\u0303 (d1...dN ) and A\u0303z the matrices obtained by removing the j\u2019th column from F (d1...dN ) and Az respectively. Defining b \u2208 RZ to be the vector whose z\u2019th entry is given by bz = c \u00b7 \u2211N i=1A z i,j , we may write: A(hS(fc)y )d1...dN = Z\u2211 z=1 ayz max { 0, \u2329 F\u0303 (d1...dN ), A\u0303z \u232a + bz\n} noting that for every z \u2208 [Z], A\u0303z and bz may take on any values with proper choice of Az . Since by assumption F has non-recurring rows, and since all rows hold the same value (c) in their j\u2019th entry, we have that F\u0303 (d1...dN ) 6= F\u0303 (d \u2032 1...d\n\u2032 N ) for (d1. . .dN ) 6= (d\u20321. . .d\u2032N ). An application of\nlemma 1 now shows that when Z\u2265MN , any value for the grid tensor A(hS(fc)y ) may be realized with proper assignment of {A\u0303z}z\u2208[Z], b and ay . Since {A\u0303z}z\u2208[Z] and b may be set arbitrarily through {Az}z\u2208[Z], we get that with proper choice of hidden and output weights ({Az}z\u2208[Z] and ay respectively), the grid tensor of our network w.r.t. the covering templates may take on any value, precisely meaning that universality holds.\nLemma 1. Let v1 . . .vk \u2208 RD be distinct vectors (vi 6= vj for i6=j), and c1. . .ck \u2208 R be any scalars. Then, there exist w1 . . .wk \u2208 RD, b \u2208 Rk and a \u2208 Rk such that \u2200i \u2208 [k]:\nk\u2211 j=1 aj max{0,w>j vi + bj} = ci (12)\nProof. As shown in the proof of claim 1, for distinct v1 . . .vk \u2208 RD there exists a vector u \u2208 RD such that u>vi 6= u>vj for all 1\u2264i < j\u2264k. We assume without loss of generality that u>v1 < . . . < u>vk, and set w1. . .wk, b and a as follows:\n\u2022 w1 = \u00b7 \u00b7 \u00b7 = wk = u\n\u2022 b1 = \u2212u>v1 + 1\n\u2022 bj = \u2212u>vj\u22121 for j = 2. . .k\n\u2022 a1 = c1\n\u2022 aj = cj\u2212cj\u22121u>vj\u2212u>vj\u22121 \u2212 \u2211j\u22121 t=1 at for j = 2. . .k\nTo complete the proof, we show below that this assignment meets the condition in eq. 12 for i = 1. . .k.\nThe fact that:\nw>j v1 + bj = { u>v1 \u2212 u>v1 + 1 = 1 , j = 1 u>v1 \u2212 u>vj\u22121 < 0 , 2\u2264j\u2264k\nimplies that the condition in eq. 12 indeed holds for i = 1: k\u2211 j=1 aj max{0,w>j v1 + bj} = a1\u00b71 + k\u2211 j=1 aj \u00b70 = a1 = c1\nFor i > 1 we have:\nw>j vi + bj =  u >vi \u2212 u>v1 + 1 > 0 , j = 1\nu>vi \u2212 u>vj\u22121 > 0 , 2\u2264j\u2264i u>vi \u2212 u>vj\u22121 \u2264 0 , i < j\u2264k\nwhich implies:\u2211k j=1 aj max{0,w>j vi + bj} =\na1(u >vi \u2212 u>v1 + 1) + \u2211i j=2 aj(u >vi \u2212 u>vj\u22121)\nComparing this to the same expression with i replaced by i\u2212 1 we obtain:\u2211k\nj=1 aj max{0,w>j vi + bj} =\u2211k j=1 aj max{0,w>j vi\u22121 + bj}+\n(u>vi \u2212 u>vi\u22121) \u2211i j=1 aj\nNow, if we follow an inductive argument and assume that the condition in eq. 12 holds for i \u2212 1, i.e. that\u2211k j=1 aj max{0,w>j vi\u22121 + bj} = ci\u22121, we get:\u2211k\nj=1 aj max{0,w>j vi + bj} =\nci\u22121 + (u >vi \u2212 u>vi\u22121) \u2211i j=1 aj\nPlugging in the definition ai = ci\u2212ci\u22121 u>vi\u2212u>vi\u22121 \u2212 \u2211i\u22121 j=1 aj gives: \u2211k j=1 aj max{0,w>j vi + bj} =\nci\u22121 + (u >vi \u2212 u>vi\u22121) ci\u2212ci\u22121u>vi\u2212u>vi\u22121 = ci\nThus the condition in eq. 12 holds for i as well. We have therefore shown by induction that our assignment of w1. . .wk, b and a meets the lemma\u2019s requirement."}, {"heading": "5.5. Depth Efficiency", "text": "The driving force behind deep learning is the expressive power that comes with depth. It is generally believed that deep networks with non-linear layers efficiently express functions that cannot be efficiently expressed by shallow networks, i.e. that would require the latter to have superpolynomial size. We refer to such scenario as depth efficiency. Being concerned with the minimal size required by a shallow network in order to realize (or approximate) a given function, the question of depth efficiency implicitly assumes universality, i.e. that there exists some (possibly exponential) size with which the shallow network is capable of expressing the target function.\nTo the best of our knowledge, at the time of this writing the only work to formally analyze depth efficiency in the context of ConvNets is [5]. This work focused on convolutional arithmetic circuits, showing that with such networks depth efficiency is complete, i.e. besides a negligible set, all functions realizable by a deep network enjoy depth efficiency. We frame this result in our setup:\nClaim 8 (adaptation of theorem 1 in [5]). Let f\u03b81 . . .f\u03b8M be any set of linearly independent representation functions for a deep ConvNet (fig. 1 with L = log2N ) with linear activation and product pooling. Suppose we randomize the linear weights (al,j,\u03b3) of the network by some continuous distribution. Then, with probability 1, we obtain score functions that cannot be realized by a shallow ConvNet (fig. 2) with linear activation and product pooling if the number of hidden channels in the latter (Z) is less than min{r0,M}N/2.\nProof. Let x(1) . . .x(M) \u2208 Rs be templates such that F is invertible (existence follows from claim 2). The deep network generates grid tensors on x(1) . . .x(M) through the standard HT decomposition (eq. 7 with g(a, b) = a\u00b7b). The proof of theorem 1 in [5] shows that when arranged as matrices, such tensors have rank at least min{r0,M}N/2 almost always, i.e. for all weight (al,j,\u03b3) settings but a set of (Lebesgue) measure zero. On the other hand, the shallow network generates grid tensors on x(1) . . .x(M) through the standard CP decomposition (eq. 6 with g(a, b) = a\u00b7b), possibly with a different matrix F (representation functions need not be the same). Such tensors, when arranged as matrices, are shown in the proof of theorem 1 in [5] to have\nrank at most Z. Therefore, for them to realize the grid tensors generated by the deep network, we almost always must have Z \u2265 min{r0,M}N/2.\nWe now turn to convolutional rectifier networks, for which depth efficiency has yet to be analyzed. In sec. 5.4 we saw that convolutional rectifier networks are universal with max pooling, and non-universal with average pooling. Since depth efficiency is only applicable to universal architectures, we focus on the former setting. The following claim establishes existence of depth efficiency for ConvNets with ReLU activation and max pooling:\nClaim 9. There exist weight settings for a deep ConvNet with ReLU activation and max pooling, giving rise to score functions that cannot be realized by a shallow ConvNet with ReLU activation and max pooling if the number of hidden channels in the latter (Z) is less than min{r0,M}N/2 \u00b7 2M \u00b7N .\nProof. The proof traverses along the following path. Letting x(1) . . .x(M) \u2208 Rs be any distinct templates, we show that when arranged as matrices, grid tensors on x(1) . . .x(M) generated by the shallow network have rank at most Z\u00b7M \u00b7N2 . Then, defining f\u03b81 . . .f\u03b8M to be representation functions for the deep network giving rise to an invertible F (non-degeneracy implies that such functions exist), we show explicit linear weight (al,j,\u03b3) settings under which the grid tensors on x(1) . . .x(M) generated by the deep network, when arranged as matrices, have rank at least min{r0,M}N/2.\nIn light of the above, the proof boils down to showing that with g(a, b) = max{a, b, 0}:\n\u2022 The matricized generalized CP decomposition (eq. 10) produces matrices with rank at most Z\u00b7M \u00b7N2 .\n\u2022 For an invertible F , there exists a weight (al,j,\u03b3) setting under which the matricized generalized HT decomposition (eq. 9) produces a matrix with rank at least min{r0,M}N/2.\nWe begin with the first point, showing that for every v1, . . . ,vN/2 \u2208 RM and u1, . . . ,uN/2 \u2208 RM : rank ( v1 g \u00b7 \u00b7 \u00b7 g vN\n2\n) g ( u1 g \u00b7 \u00b7 \u00b7 g uN\n2\n)> \u2264M \u00b7N\n2 (13)\nThis would imply that every summand in the matricized generalized CP decomposition (eq. 10) has rank at most M \u00b7N 2 , and the desired result readily follows. To prove eq. 13, note that each of the vectors v\u0304 := v1 g \u00b7 \u00b7 \u00b7 g vN 2 and u\u0304 := u1 g \u00b7 \u00b7 \u00b7 guN 2\nare of dimensionMN/2, but have only up to M \u00b7N2 unique values. Let \u03b4v, \u03b4u : [M\nN/2] \u2192 [MN/2] be permutations that arrange the entries of v\u0304 and u\u0304 in descending order. Permuting the rows of the matrix\nv\u0304 g u\u0304> via \u03b4v, and the columns via \u03b4u, obviously does not change its rank. On the other hand, we get a MN/2\u00d7MN/2 matrix with a M \u00b7N2 \u00d7 M \u00b7N 2 block structure, each block being constant (i.e. all entries of a block hold the same value). This implies that the rank of v\u0304 g u\u0304> is at most M \u00b7N2 , which is what we set out to prove.\nMoving on to the matricized generalized HT decomposition (eq. 9), for an invertible F we define the following weight setting (0 and 1 here denote the all-0 and all-1 vectors, respectively):\n\u2022 a0,j,\u03b3 = { F\u22121e\u0304\u03b3 , \u03b3\u2264M 0 , \u03b3 > M , where e\u0304\u03b3 \u2208 RM is\ndefined to be the vector holding 0 in entry \u03b3 and 1 in all other entries.\n\u2022 al,j,\u03b3 = {\n1 , \u03b3 = 1 , l \u2208 [L\u2212 1] 0 , \u03b3 > 1 , l \u2208 [L\u2212 1]\n\u2022 aL,1,y = 1 Under this setting, the produced matrix [ A ( hDy )]\nholds min{r0,M} everywhere besides min{r0,M}N/2 entries on its diagonal, where it holds min{r0,M} \u2212 1. The rank of this matrix is at least min{r0,M}N/2.\nNearly all results in the literature that relate to depth efficiency merely show its existence, and claim 9 is no different in that respect. From a practical perspective, the implications of such results are slight, as a-priori, it may be that only a small fraction of the functions realizable by a deep network enjoy depth efficiency, and for all the rest shallow networks suffice. In sec. 5.5.2 we extend claim 9, arguing that with ReLU activation and max pooling, depth efficiency becomes more and more prevalent as the number of hidden channels in the deep ConvNet grows. However, no matter how large the deep ConvNet is, with ReLU activation and max pooling depth efficiency is never complete \u2013 there is always positive measure to the set of weight configurations that lead the deep ConvNet to generate score functions efficiently realizable by the shallow ConvNet:\nClaim 10. Suppose we randomize the weights of a deep ConvNet with ReLU activation and max pooling by some continuous distribution with non-vanishing continuous probability density function. Then, assuming covering templates exist, with positive probability, we obtain score functions that can be realized by a shallow ConvNet with ReLU activation and max pooling having only a single hidden channel (Z = 1).\nProof. Let x(1) . . .x(M) \u2208 Rs be covering templates, and f\u03b81 . . .f\u03b8M be representation functions for the deep network under which F is invertible (non-degeneracy implies that such functions exist). We will show that there exists a linear weight (al,j,\u03b3) setting for the deep network with which\nit generates a grid tensor that is realizable by a shallow network with a single hidden channel (Z = 1). Moreover, we show that when the representation parameters (\u03b8d) and linear weights (al,j,\u03b3) are subject to small perturbations, the deep network\u2019s grid tensor can still be realized by a shallow network with a single hidden channel. Since templates are covering grid tensors fully define score functions. This, along with the fact that open sets in Lebesgue measure spaces always have positive measure (see sec. 5.1), imply that there is positive measure to the set of weight configurations leading the deep network to generate score functions realizable by a shallow network with Z = 1. Translating the latter statement from measure theoretical to probabilistic terms readily proves the result we seek after.\nIn light of the above, the proof boils down to the following claim, framed in terms of our generalized tensor decompositions. Fixing g(a, b) = max{a, b, 0}, per arbitrary invertible F there exists a weight (al,j,\u03b3) setting for the generalized HT decomposition (eq. 7), such that the produced tensor may be realized by the generalized CP decomposition (eq. 6) with Z = 1, and this holds even if the weights al,j,\u03b3 and matrix F are subject to small perturbations 5.\nWe will now show that the following weight setting meets our requirement (0 and 1 here denote the all-0 and all-1 vectors, respectively):\n\u2022 a0,j,\u03b3 = { F\u221211 , j odd 0 , j even\n\u2022 al,j,\u03b3 = {\n1 , j odd , l \u2208 [L\u2212 1] 0 , j even , l \u2208 [L\u2212 1]\n\u2022 aL,1,y = 1\nLet EF be an additive noise matrix applied to F , and { l,j,\u03b3}l,j,\u03b3 be additive noise vectors applied to {al,j,\u03b3}l,j,\u03b3 . We use the notation o( ) to refer to vectors that tend to 0 as EF \u2192 0 and l,j,\u03b3 \u2192 0, with the dimension of a vector to be understood by context. Plugging in the noisy variables into the generalized HT decomposition (eq. 7), we get for every j \u2208 [N/2] and \u03b1 \u2208 [r0]:\n((F + EF )(a0,2j\u22121,\u03b1 + 0,2j\u22121,\u03b1)) \u2297g((F + EF )(a0,2j,\u03b1 + 0,2j,\u03b1)) = ((F + EF )(F\u221211 + 0,2j\u22121,\u03b1)) \u2297g((F + EF )(0 + 0,2j,\u03b1))\n= (1 + o( ))\u2297g o( )\nIf the applied noise (EF , l,j,\u03b3) is small enough this is equal to (1 + o( )) \u2297 1 (recall that \u2297 stands for the standard\n5 Recall that by assumption representation functions are continuous w.r.t. their parameters (f\u03b8(x) is continuous w.r.t. \u03b8), and so small perturbations on representation parameters (\u03b8d) translate into small perturbations on the matrix F (eq. 4).\ntensor product), and we in turn get for every j \u2208 [N/4] and \u03b3 \u2208 [r1]:\n\u03c61,2j\u22121,\u03b3 \u2297g \u03c61,2j,\u03b3 = (\u2211r0\n\u03b1=1 a 1,2j\u22121,\u03b3 \u03b1 (1 + o( ))\u2297 1 ) \u2297g (\u2211r0 \u03b1=1 a 1,2j,\u03b3 \u03b1 (1 + o( ))\u2297 1\n) = (\u2211r0\n\u03b1=1(1 + 1,2j\u22121,\u03b3 \u03b1 )(1 + o( ))\u2297 1 ) \u2297g (\u2211r0 \u03b1=1 1,2j,\u03b3 \u03b1 (1 + o( ))\u2297 1\n) = ((r01 + o( ))\u2297 1)\u2297g (o( )\u2297 1)\nWith the applied noise (EF , l,j,\u03b3) small enough this becomes (r01+ o( )\u2297 1\u2297 1\u2297 1. Continuing in this fashion over the levels of the decomposition, we get that with small enough noise, for every l \u2208 [L \u2212 1], j \u2208 [N/2l+1] and \u03b3 \u2208 [rl]:\n\u03c6l,2j\u22121,\u03b3\u2297g\u03c6l,2j,\u03b3 = (\u220fl\u22121\nl\u2032=0 rl\u2032 \u00b7 1 + o( )\n) \u2297 ( \u22972\nl+1\u22121 i=1 1 ) where \u22972\nl+1\u22121 i=1 1 stands for the tensor product of the vector\n1 with itself 2l+1 \u2212 1 times. We readily conclude from this that with small enough noise, the tensor produced by the decomposition may be written as follows:\nA ( hDy ) = (\u220fL\u22121 l=0 rl \u00b7 1 + o( ) ) \u2297 ( \u2297N\u22121i=1 1 ) (14)\nTo finish our proof, it remains to show that a tensor as in eq. 14 may be realized by the generalized CP decomposition (eq. 6) with Z = 1 (and g(a, b) = max{a, b, 0}). Indeed, we may assume that the latter\u2019s F , which we denote by F\u0303 to distinguish from the matrix in the generalized HT decomposition (eq. 7), is invertible (non-degeneracy ensures that this may be achieved with proper choice of representation functions for the shallow ConvNet). Setting the weights of the generalized CP decomposition (eq. 6) through:\n\u2022 ay1 = 1\n\u2022 a1,i =\n{ F\u0303\u22121 (\u220fL\u22121 l=0 rl \u00b7 1 + o( ) ) , i = 1\n0 , i > 1 leads to A ( hSy ) = A ( hDy ) , as required.\nComparing claims 8 and 10, we see that depth efficiency is complete under linear activation with product pooling, and incomplete under ReLU activation with max pooling. We interpret this as indicating that convolutional arithmetic circuits benefit from the expressive power of depth more than convolutional rectifier networks do. This result is rather surprising, especially given the fact that convolutional rectifier networks are much more commonly used in practice. We attribute the discrepancy primarily to historical reasons, and conjecture that developing effective methods for training convolutional arithmetic circuits, thereby\nfulfilling their expressive potential, may give rise to a deep learning architecture that is provably better than convolutional rectifier networks but has so far been overlooked by practitioners.\nLoosely speaking, we have shown that the gap in expressive power between the shallow and deep ConvNets is greater with linear activation and product pooling than it is with ReLU activation and max pooling. One may wonder at this point if it is plausible to deduce from this which architectural setting is more expressive, as a-priori, altering the shallow vs. deep ConvNet comparisons such that one network has linear activation with product pooling and the other has ReLU activation with max pooling, may change the expressive gaps in favor of the latter. Claims 11 and 12 below show that this is not the case. Specifically, they show that the depth efficiency of the deep ConvNet with linear activation and product pooling remains complete when the shallow ConvNet has ReLU activation and max pooling (claim 11), and on the other hand, the depth efficiency of the deep ConvNet with ReLU activation and max pooling remains incomplete when the shallow ConvNet has linear activation and product pooling (claim 12). This affirms our stand regarding the expressive advantage of convolutional arithmetic circuits over convolutional rectifier networks.\nClaim 11. Let f\u03b81 . . .f\u03b8M be any set of linearly independent representation functions for a deep ConvNet with linear activation and product pooling. Suppose we randomize the weights of the network by some continuous distribution. Then, with probability 1, we obtain score functions that cannot be realized by a shallow ConvNet with ReLU activation and max pooling if the number of hidden channels in the latter (Z) is less than min{r0,M}N/2 \u00b7 2M \u00b7N .\nProof. The proof here follows readily from those of claims 8 and 9. Namely, in the proof of claim 8 we state that for templates x(1) . . .x(M) \u2208 Rs chosen such that F is invertible (these exist according to claim 2), a grid tensor produced by the deep ConvNet with linear activation and product pooling, when arranged as a matrix, has rank at least min{r0,M}N/2 for all linear weight (al,j,\u03b3) settings but a set of measure zero. That is to say, a matrix produced by the matricized generalized HT decomposition (eq. 9) with g(a, b) = a\u00b7b, has rank at least min{r0,M}N/2 for all weight (al,j,\u03b3) settings but a set of measure zero. On the other hand, we have shown in the proof of claim 9 that a shallow ConvNet with ReLU activation and max pooling generates grid tensors that when arranged as matrices, have rank at most Z\u00b7M \u00b7N2 . More specifically, we have shown that the matricized generalized CP decomposition (eq. 10) with g(a, b) = max{a, b, 0} produces matrices with rank at most Z\u00b7M \u00b7N2 . This implies that under almost all linear weight (al,j,\u03b3) settings for a deep ConvNet with linear activation and product pooling, the generated\ngrid tensor cannot be replicated by a shallow ConvNet with ReLU activation and max pooling if the latter has less than Z = min{r0,M}N/2 \u00b7 2M \u00b7N hidden channels.\nClaim 12. Suppose we randomize the weights of a deep ConvNet with ReLU activation and max pooling by some continuous distribution with non-vanishing continuous probability density function. Then, assuming covering templates exist, with positive probability, we obtain score functions that can be realized by a shallow ConvNet with linear activation and product pooling having only a single hidden channel (Z = 1).\nProof. The proof here is almost identical to that of claim 10. The only difference is where we show that a tensor as in eq. 14 may be realized by the generalized CP decomposition (eq. 6) with Z = 1. In the proof of claim 10 the underlying operation of the decomposition was g(a, b) = max{a, b, 0} (corresponding to ReLU activation and max pooling), whereas here it is g(a, b) = a\u00b7b (corresponding to linear activation and product pooling). To account for this difference, we again assume that F\u0303 \u2013 the matrix F of the generalized CP decomposition, is invertible (nondegeneracy ensures that this may be achieved with proper choice of representation functions for the shallow ConvNet), and modify the decomposition\u2019s weight setting as follows:\n\u2022 ay1 = 1\n\u2022 a1,i =\n{ F\u0303\u22121 (\u220fL\u22121 l=0 rl \u00b7 1 + o( ) ) , i = 1\nF\u0303\u221211 , i > 1\nThis leads to A ( hSy ) = A ( hDy ) , as required."}, {"heading": "5.5.1 Approximation", "text": "In their current form, the results in our analysis establishing depth efficiency (claims 8, 9, 11 and the analogous ones in sec. 5.6) relate to exact realization. Specifically, they provide a lower bound on the size of a shallow ConvNet required in order for it to realize exactly a grid tensor generated by a deep ConvNet. From a practical perspective, a more interesting question would be the size required by a shallow ConvNet in order to approximate the computation of a deep ConvNet. A-priori, it may be that although the size required for exact realization is exponential, the one required for approximation is only polynomial. As we briefly discuss below, this is not the case, and in fact all of the lower bounds we have provided apply not only to exact realization, but also to arbitrarily-well approximation.\nWhen proving that a grid tensor generated by a shallow ConvNet beneath a certain size cannot be equal to a grid tensor generated by a deep ConvNet, we always rely on matricization rank. Namely, we arrange the grid tensors as\nmatrices, and derive constants R, r \u2208 N, R > r, such that the matrix corresponding to the deep ConvNet has rank at least R, while that corresponding to the shallow ConvNet has rank at most r. While used in our proofs solely to show that the matrices are different, this actually entails information regarding the distance between them. Namely, if we denote the singular values of the matrix corresponding to the deep ConvNet by \u03c31 \u2265 \u03c32 \u2265 . . . \u2265 0, the squared Euclidean (Frobenius) distance between the matrices is at least \u03c32r+1 + \u00b7 \u00b7 \u00b7 + \u03c32R. Since the matrices are merely rearrangements of the grid tensors, we have a lower bound on the distance between the shallow ConvNet\u2019s grid tensor and the target grid tensor generated by the deep ConvNet, so in particular arbitrarily-well approximation is not possible."}, {"heading": "5.5.2 On the Incidence of Depth Efficiency", "text": "In claim 8 we saw that depth efficiency is complete with linear activation and product pooling. That is to say, with linear activation and product pooling, besides a negligible set, all weight settings for the deep ConvNet (fig. 1 with size-2 pooling windows and L = log2N hidden layers) lead to score functions that cannot be realized by the shallow ConvNet (fig. 2) unless the latter has super-polynomial size. We have also seen (claims 9 and 10) that replacing the activation and pooling operators by ReLU and max respectively, makes depth efficiency incomplete. There are still weight settings leading the deep ConvNet to generate score functions that require the shallow ConvNet to have super-polynomial size, but these do not occupy the entire space. In other words, there is now positive measure to the set of deep ConvNet weight configurations leading to score functions efficiently realizable by the shallow ConvNet. A natural question would then be just how frequent depth efficiency is under ReLU activation and max pooling. More formally, we may consider a uniform distribution over a compact domain in the deep ConvNet\u2019s weight space, and ask the following. Assuming weights for the deep ConvNet are drawn from this distribution, what is the probability that generated score functions exhibit depth efficiency, i.e. require super-polynomial size from the shallow ConvNet? In this appendix we address this question, arguing that the probability tends to 1 as the number of channels in the hidden layers of the deep ConvNet grows. We do not prove this formally, but nonetheless provide a framework we believe may serve as a basis for establishing formal results concerning the incidence of depth efficiency. The framework is not limited to ReLU activation and max pooling \u2013 it may be used under different choices of activation and pooling operators as well.\nThe central tool used in this paper for proving depth efficiency is the rank of grid tensors when these are arranged as matrices. We establish upper bounds on the rank\nof matricized grid tensors produced by the shallow ConvNet through the matricized generalized CP decomposition (eq. 10). These upper bounds are typically linear in the size of the input (N ) and the number of hidden channels in the network (Z). The challenge is then to derive a superpolynomial (in N ) lower bound on the rank of matricized grid tensors produced by the deep ConvNet through the matricized generalized HT decomposition (eq. 9). In the case of linear activation and product pooling (g(a, b) = a\u00b7b), the generalized Kronecker product g reduces to the standard Kronecker product , and the rank-multiplicative property of the latter (rank(A B) = rank(A)\u00b7rank(B)) can be used to show (see [5]) that besides in negligible (zero measure) cases, rank grows rapidly through the levels of the matricized generalized HT decomposition (eq. 9), to the point where the final produced matrix has exponential rank. This situation does not persist when the activation and pooling operators are replaced by ReLU and max (respectively). Indeed, in the proof of claim 10 we explicitly presented a non-negligible (positive measure) case where the matricized generalized HT decomposition (eq. 9) produces a matrix of rank 1. To study the incidence of depth efficiency under ReLU activation and max pooling, we assume the weights (al,j,\u03b3) of the matricized generalized HT decomposition (eq. 9) are drawn independently and uniformly from a bounded interval (e.g. [\u22121, 1]), and question the probability of the produced matrix [A ( hDy ) ] having rank superpolynomial in N .\nTo study rank[A ( hDy ) ], we sequentially traverse through the levels l = 1. . .L of the matricized generalized HT decomposition (eq. 9), at each level going over all locations j \u2208 [N/2l]. When at location j of level l, for each \u03b1 \u2208 [rl\u22121], we draw the weights al\u22121,2j\u22121,\u03b1 and al\u22121,2j,\u03b1 (independently of the previously drawn weights), and observe the random variable Rl,j,\u03b1, defined as the rank of the matrix [\u03c6l\u22121,2j\u22121,\u03b1] g [\u03c6l\u22121,2j,\u03b1]. Given the weights drawn while traversing through the previous levels of the decomposition, the random variables {Rl,j,\u03b1 \u2208 N}\u03b1\u2208[rl\u22121] are independent and identically distributed. The random variable Rl,j := max\u03b1\u2208[rl\u22121]{Rl,j,\u03b1} thus tends to concentrate on higher and higher values as rl\u22121 (number of channels in hidden layer l\u22121 of the deep ConvNet) grows. When the next level (l+1) of the decomposition will be traversed, the weights {al,j,\u03b3}\u03b3\u2208[rl] will be drawn, and the matrices {[\u03c6l,j,\u03b3 ]}\u03b3\u2208[rl] will be generated. According to claim 13 below, with probability 1, all of these matrices will have rank equal to at least Rl,j . And so, assuming the generalized Kronecker product g has the potential of increasing the rank of its operands, ranks will generally ascend across the levels of the matricized generalized HT decomposition (eq. 9), with steeper ascends being more and more probable as the number of channels in the hidden layers of the deep ConvNet (r0. . .rL\u22121) grows.\nThe main piece that is missing in order to complete the sketch we have outlined above into a formal proof, is the behavior of rank under the generalized Kronecker product g . This obviously depends on the choice of underlying operator g. In the case of linear activation and product pooling g(a, b) = a\u00b7b, the generalized Kronecker product g reduces to the standard Kronecker product , and ranks always increase multiplicatively, i.e. rank(A B) = rank(A)\u00b7rank(B) for any matrices A and B. The fact that there is a simple law governing the behavior of ranks makes this case relatively simple to analyze, and we indeed have a full characterization (claim 8). In the case of linear activation and max pooling the underlying operator is given by g(a, b) = max{a, b}, and it is not difficult to see that g does not decrease rank, i.e. rank(A gB)\u2265min{rank(A), rank(B)} for any ma-\ntrices A and B 6. For ReLU activation and max pooling, corresponding to the choice g(a, b) = max{a, b, 0}, there is no simple rule depicting the behavior of ranks under g , and in fact, for matrices A and B holding negative values, the rank of rank(A gB) necessarily drops to zero. Nonetheless, it seems reasonable to assume that at least in some cases, a non-linear operation such as g does increase rank, and as we have seen, benefiting from these cases is more probable when the hidden layers of the deep ConvNet include many channels. To this end, we provide in fig. 5 simulation results for the case of ReLU activation and max pooling (g(a, b) = max{a, b, 0}), demonstrating that indeed ranks produced by the matricized generalized HT decomposition (eq. 9) tend to be higher as r0. . .rL\u22121 grow. We leave a complete formal analysis of this phenomenon to future work.\nClaim 13. LetA1. . .Am be given matrices of the same size, having ranks r1. . .rm respectively. For every weight vector \u03b1 \u2208 Rm define the matrix A(\u03b1) := \u2211m i=1 \u03b1iAi, and suppose we randomize \u03b1 by some continuous distribution. Then, with probability 1, we obtain a matrix A(\u03b1) having rank at least maxi\u2208[m] ri.\nProof. Our proof relies on concepts and results from Lebesgue measure theory (see sec. 5.1 for a brief discussion). The result to prove is equivalent to stating that there is measure zero to the set of weight vectors \u03b1 for which rank(A(\u03b1)) < maxi\u2208[m] ri.\nAssume without loss of generality that maxi\u2208[m] ri is equal to r1, and that the top-left r1\u00d7r1 block of A1 is nonsingular. For every \u03b1 define p(\u03b1) := det(A(\u03b1)1:r1,1:r1), i.e. p(\u03b1) is the determinant of the r1\u00d7r1 top-left block of the matrix A(\u03b1). p(\u03b1) is obviously a polynomial in the entries of \u03b1, and by assumption p(e1) 6= 0, where e1 \u2208 Rm is the vector holding 1 in its first entry and 0 elsewhere. Since a non-zero polynomial vanishes only on a set of zero measure (see [1] for example), the set of weight vectors \u03b1 for which p(\u03b1) = 0 has measure zero. This implies that the top-left r1\u00d7r1 block of A(\u03b1) is non-singular almost everywhere, and in particular rank(A(\u03b1))\u2265r1 = maxi\u2208[m] ri almost everywhere."}, {"heading": "5.6. Shared Coefficients for Convolution", "text": "To this end, our analysis has focused on the unshared setting, where the coefficients of the 1 \u00d7 1 conv filters (see fig. 1) may vary across spatial locations. In practice, ConvNets typically enforce sharing, which in our framework implies that the coefficients of the 1\u00d7 1 conv filter in channel \u03b3 of hidden layer l, are the same for all locations j. In this section we analyze the shared setting, following a line\n6 To see this, simply note that under the choice g(a, b) = max{a, b} there is either a sub-matrix ofA gB that is equal toA, or one that is equal to B.\nsimilar to that of our analysis for the unshared setting given above. For brevity, we assume the reader is familiar with the latter, and do not repeat discussions given there.\nAs described in sec. 4, the shared setting refers to the case where the 1\u00d71 conv filters in our networks are spatially invariant, giving rise to standard convolutions (as opposed to the more general locally-connected operators). Specifically, the shallow ConvNet (fig. 2) would have a single weight vector az for every hidden channel z, as opposed to the unshared setting where it had a weight vector az,i for every location i in every hidden channel z. Grid tensors produced by the shallow ConvNet in the shared setting are given by what we call the shared generalized CP decomposition:\nA ( hSy ) = Z\u2211 z=1 ayz \u00b7 (Faz)\u2297g \u00b7 \u00b7 \u00b7 \u2297g (Faz)\ufe38 \ufe37\ufe37 \ufe38 N times\n(15)\nAs for the deep ConvNet (fig. 1 with size-2 pooling windows and L = log2N hidden layers), in the shared setting, instead of having a weight vector al,j,\u03b3 for every hidden layer l, channel \u03b3 and location j, there is a single weight vector al,\u03b3 for all locations of channel \u03b3 in hidden layer l. Produced grid tensors are then given by the shared generalized HT decomposition:\n\u03c61,\u03b3 = r0\u2211 \u03b1=1 a1,\u03b3\u03b1 (Fa 0,\u03b1)\u2297g (Fa0,\u03b1)\n\u00b7 \u00b7 \u00b7\n\u03c6l,\u03b3 = rl\u22121\u2211 \u03b1=1 al,\u03b3\u03b1 \u03c6 l\u22121,\u03b1\ufe38 \ufe37\ufe37 \ufe38\norder 2l\u22121 \u2297g \u03c6l\u22121,\u03b1\ufe38 \ufe37\ufe37 \ufe38 order 2l\u22121\n\u00b7 \u00b7 \u00b7\n\u03c6L\u22121,\u03b3 = rL\u22122\u2211 \u03b1=1 aL\u22121,\u03b3\u03b1 \u03c6 L\u22122,\u03b1\ufe38 \ufe37\ufe37 \ufe38\norder N4 \u2297g \u03c6L\u22122,\u03b1\ufe38 \ufe37\ufe37 \ufe38 order N4\nA ( hDy ) = rL\u22121\u2211 \u03b1=1 aL,y\u03b1 \u03c6 L\u22121,\u03b1\ufe38 \ufe37\ufe37 \ufe38\norder N2 \u2297g \u03c6L\u22121,\u03b1\ufe38 \ufe37\ufe37 \ufe38 order N2\n(16)\nWe now turn to analyze universality and depth efficiency in the shared setting."}, {"heading": "5.6.1 Universality", "text": "In the unshared setting we saw (sec. 5.4) that linear activation with product pooling and ReLU activation with max pooling both lead to universality, whereas ReLU activation with average pooling does not. We will now see that in the shared setting, no matter how the activation and pooling operators are chosen, universality is never met.\nA shallow ConvNet with shared weights produces grid tensors through the shared generalized CP decomposition\n(eq. 15). A tensorA generated by this decomposition is necessarily symmetric, i.e. for any permutation \u03b4 : [N ] \u2192 [N ] and indexes d1. . .dN it meets: Ad1...dN = A\u03b4(d1)...\u03b4(dN ). Obviously not all tensors share this property, so indeed a shallow ConvNet with weight sharing is not universal. A deep ConvNet with shared weights produces grid tensors through the shared generalized HT decomposition (eq. 16). For this decomposition, a generated tensor A is invariant to replacing the first and second halves of its modes, i.e. for any indexes d1. . .dN it meets: Ad1,...,dN = AdN/2+1,...,dN ,d1,...,dN/2 . Although this property is much less stringent than symmetry, it is still not met by most tensors, and so a deep ConvNet with weight sharing is not universal either."}, {"heading": "5.6.2 Depth Efficiency", "text": "Depth efficiency deals with the computational complexity of replicating a deep network\u2019s function using a shallow network. In order for this question to be applicable, we require that the shallow network be a universal machine. If this is not the case, then it is generally likely that the deep network\u2019s function simply lies outside the reach of the shallow network, and we do not obtain a quantitative insight into the true power of depth. Since our shallow ConvNets are not universal with shared weights (sec. 5.6.1), we evaluate depth efficiency of deep ConvNets with shared weights against shallow ConvNets with unshared weights. Specifically, we do this for the activation-pooling choices leading shallow ConvNets with unshared weights to be universal: linear activation with product pooling, and ReLU activation with max pooling (see sec. 5.4).\nFor linear activation with product pooling, the following claim, which is essentially a derivative of theorem 1 in [5], tells us that in the shared setting, as in the unshared setting, depth efficiency holds completely:\nClaim 14 (shared analogy of claim 8). Let f\u03b81 . . .f\u03b8M be any set of linearly independent representation functions for a deep ConvNet with linear activation, product pooling and weight sharing. Suppose we randomize the weights of the network by some continuous distribution. Then, with probability 1, we obtain score functions that cannot be realized by a shallow ConvNet with linear activation and product pooling (not limited by weight sharing), if the number of hidden channels in the latter (Z) is less than min{r0,M}N/2.\nProof. The proof here is almost identical to that of claim 8. The only difference is that in the latter, we used the fact that the generalized HT decomposition (eq. 7), when equipped with g(a, b) = a\u00b7b, almost always produces tensors whose matrix arrangements have rank at least min{r0,M}N/2, whereas here, we require an analogous result for the shared generalized HT decomposition (eq. 16). Such result is provided by the proof of theorem 1 in [5].\nHeading on to ReLU activation and max pooling, we will show that here too, the situation in the shared setting is the same as in the unshared setting. Specifically, depth efficiency holds, but not completely. We prove this via two claims, analogous to claims 9 and 10 from sec. 5.5:\nClaim 15 (shared analogy of claim 9). There exist weight settings for a deep ConvNet with ReLU activation, max pooling and weight sharing, giving rise to score functions that cannot be realized by a shallow ConvNet with ReLU activation and max pooling (not limited by weight sharing), if the number of hidden channels in the latter (Z) is less than min{r0,M}N/2 \u00b7 2M \u00b7N .\nProof. In the proof of claim 9 we have shown, for arbitrary distinct templates x(1) . . .x(M) \u2208 Rs, an explicit weight setting for the deep ConvNet with ReLU activation and max pooling, leading the latter to produce a grid tensor that cannot be realized by a shallow ConvNet with ReLU activation and max pooling, if that has less than min{r0,M}N/2 \u00b7 2M \u00b7N hidden channels. Since the given weight setting was location invariant, i.e. the assignment of al,j,\u03b3 did not depend on j, it applies as is to a deep ConvNet with weight sharing, and the desired result readily follows.\nClaim 16 (shared analogy of claim 10). Suppose we randomize the weights of a deep ConvNet with ReLU activation, max pooling and weight sharing by some continuous distribution with non-vanishing continuous probability density function. Then, assuming covering templates exist, with positive probability, we obtain score functions that can be realized by a shallow ConvNet with ReLU activation and max pooling having only a single hidden channel (Z = 1).\nProof. The proof is similar in spirit to that of claim 10, which dealt with incompleteness of depth efficiency under ReLU activation and max pooling in the unshared setting. Our focus here is on the shared setting, or more specifically, on the case where the deep ConvNet is limited by weight sharing while the shallow ConvNet is not. Accordingly, we would like to show the following. Fixing g(a, b) = max{a, b, 0}, per arbitrary invertible F there exists a weight (al,\u03b3) setting for the shared generalized HT decomposition (eq. 16), such that the produced tensor may be realized by the generalized CP decomposition (eq. 6) with Z = 1, and this holds even if the weights al,\u03b3 and matrix F are subject to small perturbations.\nBefore heading on to prove that a weight setting as above exists, we introduce a new definition that will greatly simplify our proof. We refer to a tensor A of order P and dimension M in each mode as basic, if there exists a vector u \u2208 RM with non-decreasing entries (u1\u2264 . . .\u2264uM ), such that A = u \u2297g \u00b7 \u00b7 \u00b7 \u2297g u (i.e. A is equal to the generalized tensor product of u with itself P times, with underlying operation g(a, b) = max{a, b, 0}). A basic tensor can\nobviously be realized by the generalized CP decomposition (eq. 6) with Z = 1 (given that non-degeneracy is used to ensure the latter\u2019s representation matrix is non-singular), and so it suffices to find a weight (al,\u03b3) setting for the shared generalized HT decomposition (eq. 16) that gives rise to a basic tensor, and in addition, ensures that small perturbations on the weights al,\u03b3 and matrix F still yield basic tensors. Two trivial facts that relate to basic tensors and will be used in our proof are: (i) the generalized tensor product of a basic tensor with itself is basic, and (ii) a linear combination of basic tensors with non-negative weights is basic.\nTurning to the main part of the proof, we now show that the following weight setting meets our requirement:\n\u2022 a0,\u03b3 = F\u22121v\n\u2022 al,\u03b3 = 1, l \u2208 [L\u2212 1]\n\u2022 aL,y = 1\nv here stands for the vector [1, 2, . . . ,M ]> \u2208 RM , and 1 is an all-1 vector with dimension to be understood by context. Let EF be an additive noise matrix applied to F , and { l,\u03b3}l,\u03b3 be additive noise vectors applied to {al,\u03b3}l,\u03b3 . We would like to prove that under the weight setting above, when applied noise (EF , l,\u03b3) is small enough, the grid tensor produced by the shared generalized HT decomposition (eq. 16) is basic.\nFor convenience, we adopt the notation o( ) as referring to vectors that tend to 0 as EF \u2192 0 and l,\u03b3 \u2192 0, with the dimension of a vector to be understood by context. Plugging in the noisy variables into the shared generalized HT decomposition (eq. 16), we get for every \u03b1 \u2208 [r0]:\n((F + EF )(a0,\u03b1 + 0,\u03b1))\u2297g ((F + EF )(a0,\u03b1 + 0,\u03b1)) = ((F + EF )(F\u22121v + 0,\u03b1))\u2297g ((F + EF )(F\u22121v + 0,\u03b1))\n= v\u0303\u03b1 \u2297g v\u0303\u03b1\nwhere v\u0303\u03b1 = v+o( ). If the applied noise (EF , l,\u03b3) is small enough the entries of v\u0303\u03b1 are non-decreasing and v\u0303\u03b1 \u2297g v\u0303\u03b1 is a basic tensor (matrix). Moving to the next level of the decomposition, we have for every \u03b3 \u2208 [r1]:\n\u03c61,\u03b3 = r0\u2211 \u03b1=1 (a1,\u03b3\u03b1 + 1,\u03b3 \u03b1 ) \u00b7 v\u0303\u03b1 \u2297g v\u0303\u03b1\nWhen applied noise (EF , l,\u03b3) is small enough the weights of this linear combination are non-negative, and together with the tensors (matrices) v\u0303\u03b1\u2297g v\u0303\u03b1 being basic, this leads \u03c61,\u03b3 to be basic as well. Continuing in this fashion over the levels of the decomposition, we get that with small enough noise, for every l \u2208 [L \u2212 1] and \u03b3 \u2208 [rl], \u03c6l,\u03b3 is a basic tensor. A final step in this direction shows that under small noise, the produced grid tensor A ( hDy )\nis basic as well. This is what we set out to prove.\nTo recapitulate this section, we have shown that introducing weight sharing into the 1 \u00d7 1 conv operators of our networks, thereby limiting the general locally-connected linear mappings to be standard convolutions, disrupts universality, but leaves depth efficiency intact \u2013 it remains to hold completely under linear activation with product pooling, and incompletely under ReLU activation with max pooling."}, {"heading": "6. Discussion", "text": "The contribution of this paper is twofold. First, we introduce a construction in the form of generalized tensor decompositions, that enables transforming convolutional arithmetic circuits into convolutional rectifier networks (ConvNets with ReLU activation and max or average pooling). This opens the door to various mathematical tools from the world of arithmetic circuits, now available for analyzing convolutional rectifier networks. As a second contribution, we make use of such tools to prove new results on the expressive properties that drive this important class of networks.\nOur analysis shows that convolutional rectifier networks are universal with max pooling, but not with average pooling. This implies that if non-linearity originates solely from ReLU activation, increasing network size alone is not sufficient for expressing arbitrary functions. More interestingly, we analyze the behavior of convolutional rectifier networks in terms of depth efficiency, i.e. of cases where a function generated by a deep network of polynomial size requires shallow networks to have super-polynomial size. It is known that convolutional arithmetic circuits exhibit complete depth efficiency, i.e. that besides a negligible (zero measure) set, all functions generated by deep networks of this type are depth efficient. We show that this is not the case with convolutional rectifier networks, for which depth efficiency exists, but is weaker in the sense that it is not complete (there is positive measure to the set of functions generated by a deep network that may be efficiently realized by shallow networks).\nDepth efficiency is believed to be the key factor behind the success of deep learning. Our analysis indicates that from this perspective, the widely used convolutional rectifier networks are inferior to convolutional arithmetic circuits. This leads us to believe that convolutional arithmetic circuits bear the potential to improve the performance of deep learning beyond what is witnessed today. Of course, a practical machine learning model is measured not only by its expressive power, but also by our ability to train it. Over the years, massive amounts of research have been devoted to training convolutional rectifier networks. Convolutional arithmetic circuits on the other hand received far less attention, although they have been successfully trained in recent works on the SimNet architecture ([3, 4]), demonstrating\nhow the enhanced expressive power can lead to state of the art performance in computationally limited settings.\nWe believe that developing effective methods for training convolutional arithmetic circuits, thereby fulfilling their expressive potential, may give rise to a deep learning architecture that is provably superior to convolutional rectifier networks but has so far been overlooked by practitioners."}, {"heading": "Acknowledgments", "text": "This work is partly funded by Intel grant ICRI-CI no. 9- 2012-6133 and by ISF Center grant 1790/12. Nadav Cohen is supported by a Google Fellowship in Machine Learning."}, {"heading": "A. Existence of Covering Templates", "text": "In this paper we analyze the expressiveness of networks, i.e. the functions they can realize, through the notion of grid tensors. Recall from sec. 4 that given templates x(1) . . .x(M) \u2208 Rs, the grid tensor of a score function hy : (Rs)N \u2192 R realized by some network, is defined to be a tensor of order N and dimension M in each mode, denoted A(hy), and given by eq. 3. In particular, it is a tensor holding the values of hy on all instances X = (x1, . . . ,xN ) \u2208 (Rs)N whose patches xi are taken from the set of templates {x(1) . . .x(M)} (recurrence allowed). Some of the claims in our analysis (sec. 5) assumed that there exist templates for which grid tensors fully define score functions. That is to say, there exist templates such that score function values outside the exponentially large grid {Xd1...dN := (x\n(d1), . . . ,x(dN )) : d1. . .dN \u2208 [M ]} are irrelevant for classification. Templates meeting this property where referred to as covering (see sec. 5.2). In this appendix we address the existence of covering templates.\nIf we allowM to grow arbitrarily large then obviously covering templates can be found. However, since in our construction M is tied to the number of channels in the first (representation) layer of a network (see fig. 1), such a trivial observation does not suffice, and in fact we would like to show that covering templates exist for values ofM that correspond to practical network architectures, i.e.M \u2208 \u2126(100). For such an argument to hold, assumptions must be made on the distribution of input data. Given that ConvNets are used primarily for processing natural images, we assume here that data is governed by their statistics. Specifically, we assume that an instance X = (x1, . . . ,xN ) \u2208 (Rs)N corresponds to a natural image, represented through N image patches around its pixels: x1. . .xN \u2208 Rs.\nIf the dimension of image patches is small then it seems reasonable to believe that relatively few templates can indeed cover the possible appearances of a patch. For example, in the extreme case where each patch is simply a gray-scale pixel (s = 1), having M = 256 templates may provide the standard 8-bit resolution, leading grid tensors to fully define score functions by accounting for all possible images. However, since in our construction input patches correspond to the receptive field in the first layer of a ConvNet (see fig. 1), we would like to establish an argument for image patch sizes that more closely correlate to typical receptive fields, e.g. 5\u00d75. For this we rely on various studies (e.g. [32]) characterizing the statistics of natural images, which have shown that for large ensembles of images, randomly cropped patches of size up to 16\u00d716 may be relatively well captured by Gaussian Mixture Models with as few as 64 components. This complies with the common belief that there is a moderate number of appearances taken by the vast majority of local image patches (edges, Gabor filters etc.). That is to say, it complies with our assumption that covering templates exist with a moderate value of M . We refer the reader to [5] for a more formal argument on this line."}], "references": [{"title": "The zero set of a polynomial", "author": ["Richard Caron", "Tim Traynor"], "venue": "WSMR Report 05-02,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2005}, {"title": "Teaching deep convolutional neural networks to play go", "author": ["Christopher Clark", "Amos Storkey"], "venue": "arXiv preprint arXiv:1412.3409,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2014}, {"title": "SimNets: A Generalization of Convolutional Networks", "author": ["Nadav Cohen", "Amnon Shashua"], "venue": "NIPS Deep Learning and Representation Learning Workshop,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2014}, {"title": "On the expressive power of deep learning: a tensor analysis", "author": ["Nadav Cohen", "Or Sharir", "Amnon Shashua"], "venue": "arXiv preprint arXiv:1509.05009,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2015}, {"title": "Approximation by superpositions of a sigmoidal function", "author": ["G Cybenko"], "venue": "Mathematics of Control, Signals and Systems,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 1989}, {"title": "Shallow vs. deep sumproduct networks", "author": ["Olivier Delalleau", "Yoshua Bengio"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2011}, {"title": "The power of depth for feedforward neural networks", "author": ["Ronen Eldan", "Ohad Shamir"], "venue": "arXiv preprint arXiv:1512.03965,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2015}, {"title": "Deep learning. Book in preparation for MIT Press, 2016", "author": ["Ian Goodfellow", "Yoshua Bengio", "Aaron Courville"], "venue": "URL http://goodfeli.github.io/dlbook/", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2016}, {"title": "A New Scheme for the Tensor Representation", "author": ["W Hackbusch", "S K\u00fchn"], "venue": "Journal of Fourier Analysis and Applications,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2009}, {"title": "Tensor Spaces and Numerical Tensor Calculus, volume 42 of Springer Series in Computational Mathematics", "author": ["Wolfgang Hackbusch"], "venue": null, "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2012}, {"title": "Multilayer feedforward networks are universal approximators", "author": ["Kurt Hornik", "Maxwell B Stinchcombe", "Halbert White"], "venue": "Neural networks,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 1989}, {"title": "Tensor Decompositions and Applications", "author": ["Tamara G Kolda", "Brett W Bader"], "venue": "SIAM Review (),", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2009}, {"title": "ImageNet Classification with Deep Convolutional Neural Networks", "author": ["Alex Krizhevsky", "Ilya Sutskever", "Geoffrey E Hinton"], "venue": "Advances in Neural Information Processing Systems,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2012}, {"title": "Convolutional networks for images, speech, and time series", "author": ["Yann LeCun", "Yoshua Bengio"], "venue": "The handbook of brain theory and neural networks,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 1995}, {"title": "Multilayer feedforward networks with a nonpolynomial activation function can approximate any function", "author": ["Moshe Leshno", "Vladimir Ya Lin", "Allan Pinkus", "Shimon Schocken"], "venue": "Neural networks,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 1993}, {"title": "On the expressive efficiency of sum product networks", "author": ["James Martens", "Venkatesh Medabalimi"], "venue": "arXiv preprint arXiv:1411.7717,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2014}, {"title": "On the number of linear regions of deep neural networks", "author": ["Guido F Montufar", "Razvan Pascanu", "Kyunghyun Cho", "Yoshua Bengio"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2014}, {"title": "Rectified linear units improve restricted boltzmann machines", "author": ["Vinod Nair", "Geoffrey E Hinton"], "venue": "In Proceedings of the 27th International Conference on Machine Learning (ICML-", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2010}, {"title": "On the number of inference regions of deep feed forward networks with piece-wise linear activations", "author": ["Razvan Pascanu", "Guido Montufar", "Yoshua Bengio"], "venue": "arXiv preprint arXiv,", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2013}, {"title": "Itheory on depth vs width: hierarchical function composition", "author": ["Tomaso Poggio", "Fabio Anselmi", "Lorenzo Rosasco"], "venue": "Technical report, Center for Brains, Minds and Machines (CBMM),", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2015}, {"title": "Sum-product networks: A new deep architecture", "author": ["Hoifung Poon", "Pedro Domingos"], "venue": "In Computer Vision Workshops (ICCV Workshops),", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2011}, {"title": "Learning semantic representations using convolutional neural networks for web search", "author": ["Yelong Shen", "Xiaodong He", "Jianfeng Gao", "Li Deng", "Gr\u00e9goire Mesnil"], "venue": "In Proceedings of the companion publication of the 23rd international conference on World wide web companion,", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2014}, {"title": "Arithmetic circuits: A survey of recent results and open questions", "author": ["Amir Shpilka", "Amir Yehudayoff"], "venue": "Foundations and Trends in Theoretical Computer Science,", "citeRegEx": "26", "shortCiteRegEx": "26", "year": 2010}, {"title": "Very deep convolutional networks for large-scale image recognition", "author": ["Karen Simonyan", "Andrew Zisserman"], "venue": "arXiv preprint arXiv:1409.1556,", "citeRegEx": "27", "shortCiteRegEx": "27", "year": 2014}, {"title": "Going Deeper with Convolutions", "author": ["Christian Szegedy", "Wei Liu", "Yangqing Jia", "Pierre Sermanet", "Scott Reed", "Dragomir Anguelov", "Dumitru Erhan", "Vincent Vanhoucke", "Andrew Rabinovich"], "venue": null, "citeRegEx": "28", "shortCiteRegEx": "28", "year": 2015}, {"title": "DeepFace: Closing the Gap to Human-Level Performance in Face Verification", "author": ["Yaniv Taigman", "Ming Yang", "Marc\u2019Aurelio Ranzato", "Lior Wolf"], "venue": "Proceedings of the 2014 IEEE Conference on Computer Vision and Pattern Recognition", "citeRegEx": "29", "shortCiteRegEx": "29", "year": 2014}, {"title": "Benefits of depth in neural networks", "author": ["Matus Telgarsky"], "venue": "arXiv preprint arXiv:1602.04485,", "citeRegEx": "30", "shortCiteRegEx": "30", "year": 2016}, {"title": "Atomnet: A deep convolutional neural network for bioactivity prediction in structure-based drug discovery", "author": ["Izhar Wallach", "Michael Dzamba", "Abraham Heifets"], "venue": "arXiv preprint arXiv:1510.02855,", "citeRegEx": "31", "shortCiteRegEx": "31", "year": 2015}], "referenceMentions": [{"referenceID": 7, "context": "Deep neural networks are repeatedly proving themselves to be extremely effective machine learning models, providing state of the art accuracies on a wide range of tasks (see [17, 9]).", "startOffset": 174, "endOffset": 181}, {"referenceID": 13, "context": "Arguably, the most successful deep learning architecture to date is that of convolutional neural networks (ConvNets, [16]), which prevails in the field of computer vision, and is recently being harnessed for many other application domains as well (e.", "startOffset": 117, "endOffset": 121}, {"referenceID": 21, "context": "[25, 31, 2]).", "startOffset": 0, "endOffset": 11}, {"referenceID": 27, "context": "[25, 31, 2]).", "startOffset": 0, "endOffset": 11}, {"referenceID": 1, "context": "[25, 31, 2]).", "startOffset": 0, "endOffset": 11}, {"referenceID": 17, "context": "Modern ConvNets are formed by stacking layers one after the other, where each layer consists of a linear convolutional operator followed by Rectified Linear Unit (ReLU [21]) activation (\u03c3(z) = max{0, z}), which in turn is followed by max or average pooling (P{cj} = max{cj} or P{cj} = mean{cj} respectively).", "startOffset": 168, "endOffset": 172}, {"referenceID": 12, "context": "Such models, which we refer to as convolutional rectifier networks, have driven the resurgence of deep learning ([15]), and represent the cutting edge of the ConvNet architecture ([28, 27]).", "startOffset": 113, "endOffset": 117}, {"referenceID": 24, "context": "Such models, which we refer to as convolutional rectifier networks, have driven the resurgence of deep learning ([15]), and represent the cutting edge of the ConvNet architecture ([28, 27]).", "startOffset": 180, "endOffset": 188}, {"referenceID": 23, "context": "Such models, which we refer to as convolutional rectifier networks, have driven the resurgence of deep learning ([15]), and represent the cutting edge of the ConvNet architecture ([28, 27]).", "startOffset": 180, "endOffset": 188}, {"referenceID": 20, "context": "Arithmetic circuits (also known as Sum-Product Networks, [24]) are networks with two types of nodes: sum nodes, which compute a weighted sum of their inputs, and product nodes, computing the product of their inputs.", "startOffset": 57, "endOffset": 61}, {"referenceID": 3, "context": "Recently, [5] analyzed convolutional arithmetic circuits through ten-", "startOffset": 10, "endOffset": 13}, {"referenceID": 2, "context": "Although convolutional arithmetic circuits are known to be equivalent to SimNets ([3]), a new deep learning architecture that has recently demonstrated promising empirical performance ([4]), they are fundamentally different from convolutional rectifier networks.", "startOffset": 82, "endOffset": 85}, {"referenceID": 3, "context": "Accordingly, the result established in [5] does not apply to models commonly used in practice.", "startOffset": 39, "endOffset": 42}, {"referenceID": 3, "context": "In this paper we present a construction, based on the notion of generalized tensor decompositions, that transforms convolutional arithmetic circuits of the type described in [5] into convolutional rectifier networks.", "startOffset": 174, "endOffset": 177}, {"referenceID": 22, "context": "We refer the interested reader to [26] for a survey written in 2010, and mention here the more recent works [7] and [19] studying depth efficiency of arithmetic circuits in the context of deep learning (Sum-Product Networks).", "startOffset": 34, "endOffset": 38}, {"referenceID": 5, "context": "We refer the interested reader to [26] for a survey written in 2010, and mention here the more recent works [7] and [19] studying depth efficiency of arithmetic circuits in the context of deep learning (Sum-Product Networks).", "startOffset": 108, "endOffset": 111}, {"referenceID": 15, "context": "We refer the interested reader to [26] for a survey written in 2010, and mention here the more recent works [7] and [19] studying depth efficiency of arithmetic circuits in the context of deep learning (Sum-Product Networks).", "startOffset": 116, "endOffset": 120}, {"referenceID": 17, "context": "Compared to arithmetic circuits, the literature on depth efficiency of neural networks with ReLU activation is far less developed, primarily since these models were only introduced several years ago ([21]).", "startOffset": 200, "endOffset": 204}, {"referenceID": 18, "context": "[22] and [20] use combinatorial arguments to characterize the maximal number of linear regions in functions generated by ReLU networks, thereby establishing existence of depth efficiency.", "startOffset": 0, "endOffset": 4}, {"referenceID": 16, "context": "[22] and [20] use combinatorial arguments to characterize the maximal number of linear regions in functions generated by ReLU networks, thereby establishing existence of depth efficiency.", "startOffset": 9, "endOffset": 13}, {"referenceID": 26, "context": "[30] uses semi-algebraic geometry to analyze the number of oscillations in functions realized by neural networks with semi-algebraic activations, ReLU in particular.", "startOffset": 0, "endOffset": 4}, {"referenceID": 26, "context": "The fundamental result proven in [30] is the existence, for every k \u2208 N, of functions realizable by networks with \u0398(k) layers and \u0398(1) nodes per layer, which cannot be approximated by networks withO(k) layers unless these are exponentially large (have \u03a9(2) nodes).", "startOffset": 33, "endOffset": 37}, {"referenceID": 6, "context": "The work in [8] makes use of Fourier analysis to show existence of functions that are efficiently computable by depth-3 networks, yet require exponential size in order to be approximated by depth-2 networks.", "startOffset": 12, "endOffset": 15}, {"referenceID": 19, "context": "[23] also compares the computational abilities of deep vs.", "startOffset": 0, "endOffset": 4}, {"referenceID": 19, "context": "However, the complexity measure considered in [23] is the VC dimension, whereas our interest lies in network size.", "startOffset": 46, "endOffset": 50}, {"referenceID": 3, "context": "Recently, [5] introduced convolutional arithmetic circuits, which may be viewed as ConvNets with linear activation and product pooling.", "startOffset": 10, "endOffset": 13}, {"referenceID": 9, "context": "These networks are shown to correspond to hierarchical tensor decompositions (see [11]).", "startOffset": 82, "endOffset": 86}, {"referenceID": 3, "context": "Accordingly, the analysis carried out in [5] does not apply to the networks at the forefront of deep learning.", "startOffset": 41, "endOffset": 44}, {"referenceID": 3, "context": "Closing the gap between the networks analyzed in [5] and convolutional rectifier networks is the topic of this paper.", "startOffset": 49, "endOffset": 52}, {"referenceID": 3, "context": "We achieve this by generalizing tensor decompositions, thereby opening the door to mathematical machinery as used in [5], harnessing it to analyze, for the first time, the depth efficiency of convolutional rectifier networks.", "startOffset": 117, "endOffset": 120}, {"referenceID": 11, "context": "Tensor decompositions (see [14] for a survey) may be viewed as schemes for expressing tensors using tensor products and weighted sums.", "startOffset": 27, "endOffset": 31}, {"referenceID": 25, "context": "This is often referred to in the deep learning community as a locally-connected layer (see [29]).", "startOffset": 91, "endOffset": 95}, {"referenceID": 3, "context": "Convolutional arithmetic circuits as analyzed in [5] correspond to linear activation (\u03c3(z) = z) and product pooling (P{cj} = \u220f cj).", "startOffset": 49, "endOffset": 52}, {"referenceID": 3, "context": "vation (\u03c3(z) = z) and product pooling (P{cj} = \u220f cj) we get a convolutional arithmetic circuit as analyzed in [5].", "startOffset": 110, "endOffset": 113}, {"referenceID": 11, "context": "6 generalizes the classic CP (CANDECOMP/PARAFAC) decomposition (see [14] for a historic survey), and we accordingly refer to it as the generalized CP decomposition.", "startOffset": 68, "endOffset": 72}, {"referenceID": 8, "context": "7 generalizes the Hierarchical Tucker decomposition introduced in [10], and is accordingly referred to as the generalized HT decomposition.", "startOffset": 66, "endOffset": 70}, {"referenceID": 0, "context": "A useful fact (proven in [1] for example) is that the zero set of a polynomial, i.", "startOffset": 25, "endOffset": 28}, {"referenceID": 9, "context": "Here too, a full introduction to the topic is beyond our scope (we refer the interested reader to [11]), and we only list some concepts and results that will be used.", "startOffset": 98, "endOffset": 102}, {"referenceID": 4, "context": "It is well-known that fullyconnected neural networks are universal under all types of non-linear activations typically used in practice, even if the number of hidden layers is restricted to one ([6, 12, 18]).", "startOffset": 195, "endOffset": 206}, {"referenceID": 10, "context": "It is well-known that fullyconnected neural networks are universal under all types of non-linear activations typically used in practice, even if the number of hidden layers is restricted to one ([6, 12, 18]).", "startOffset": 195, "endOffset": 206}, {"referenceID": 14, "context": "It is well-known that fullyconnected neural networks are universal under all types of non-linear activations typically used in practice, even if the number of hidden layers is restricted to one ([6, 12, 18]).", "startOffset": 195, "endOffset": 206}, {"referenceID": 3, "context": "We begin by stating a result similar to that given in [5], according to which convolutional arithmetic circuits are universal:", "startOffset": 54, "endOffset": 57}, {"referenceID": 3, "context": "To the best of our knowledge, at the time of this writing the only work to formally analyze depth efficiency in the context of ConvNets is [5].", "startOffset": 139, "endOffset": 142}, {"referenceID": 3, "context": "Claim 8 (adaptation of theorem 1 in [5]).", "startOffset": 36, "endOffset": 39}, {"referenceID": 3, "context": "The proof of theorem 1 in [5] shows that when arranged as matrices, such tensors have rank at least min{r0,M}N almost always, i.", "startOffset": 26, "endOffset": 29}, {"referenceID": 3, "context": "Such tensors, when arranged as matrices, are shown in the proof of theorem 1 in [5] to have rank at most Z.", "startOffset": 80, "endOffset": 83}, {"referenceID": 3, "context": "In the case of linear activation and product pooling (g(a, b) = a\u00b7b), the generalized Kronecker product g reduces to the standard Kronecker product , and the rank-multiplicative property of the latter (rank(A B) = rank(A)\u00b7rank(B)) can be used to show (see [5]) that besides in negligible (zero measure) cases, rank grows rapidly through the levels of the matricized generalized HT decomposition (eq.", "startOffset": 256, "endOffset": 259}, {"referenceID": 1, "context": "fr e q u e n cy numbers of hidden channels: [2, 2, 2]", "startOffset": 44, "endOffset": 53}, {"referenceID": 1, "context": "fr e q u e n cy numbers of hidden channels: [2, 2, 2]", "startOffset": 44, "endOffset": 53}, {"referenceID": 1, "context": "fr e q u e n cy numbers of hidden channels: [2, 2, 2]", "startOffset": 44, "endOffset": 53}, {"referenceID": 6, "context": "fr e q u e n cy numbers of hidden channels: [8, 8, 8]", "startOffset": 44, "endOffset": 53}, {"referenceID": 6, "context": "fr e q u e n cy numbers of hidden channels: [8, 8, 8]", "startOffset": 44, "endOffset": 53}, {"referenceID": 6, "context": "fr e q u e n cy numbers of hidden channels: [8, 8, 8]", "startOffset": 44, "endOffset": 53}, {"referenceID": 0, "context": "Since a non-zero polynomial vanishes only on a set of zero measure (see [1] for example), the set of weight vectors \u03b1 for which p(\u03b1) = 0 has measure zero.", "startOffset": 72, "endOffset": 75}, {"referenceID": 3, "context": "For linear activation with product pooling, the following claim, which is essentially a derivative of theorem 1 in [5], tells us that in the shared setting, as in the unshared setting, depth efficiency holds completely:", "startOffset": 115, "endOffset": 118}, {"referenceID": 3, "context": "Such result is provided by the proof of theorem 1 in [5].", "startOffset": 53, "endOffset": 56}, {"referenceID": 2, "context": "Convolutional arithmetic circuits on the other hand received far less attention, although they have been successfully trained in recent works on the SimNet architecture ([3, 4]), demonstrating", "startOffset": 170, "endOffset": 176}], "year": 2017, "abstractText": "Convolutional rectifier networks, i.e. convolutional neural networks with rectified linear activation and max or average pooling, are the cornerstone of modern deep learning. However, despite their wide use and success, our theoretical understanding of the expressive properties that drive these networks is partial at best. On other hand, we have a much firmer grasp of these issues in the world of arithmetic circuits. Specifically, it is known that convolutional arithmetic circuits posses the property of \u201dcomplete depth efficiency\u201d, meaning that besides a negligible set, all functions that can be implemented by a deep network of polynomial size, require exponential size in order to be implemented (or even approximated) by a shallow network. In this paper we describe a construction based on generalized tensor decompositions, that transforms convolutional arithmetic circuits into convolutional rectifier networks. We then use mathematical tools available from the world of arithmetic circuits to prove new results. First, we show that convolutional rectifier networks are universal with max pooling but not with average pooling. Second, and more importantly, we show that depth efficiency is weaker with convolutional rectifier networks than it is with convolutional arithmetic circuits. This leads us to believe that developing effective methods for training convolutional arithmetic circuits, thereby fulfilling their expressive potential, may give rise to a deep learning architecture that is provably superior to convolutional rectifier networks but has so far been overlooked by practitioners.", "creator": "LaTeX with hyperref package"}}}