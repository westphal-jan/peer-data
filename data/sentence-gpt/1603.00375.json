{"id": "1603.00375", "review": {"conference": "acl", "VERSION": "v1", "DATE_OF_SUBMISSION": "1-Mar-2016", "title": "Easy-First Dependency Parsing with Hierarchical Tree LSTMs", "abstract": "We suggest a compositional vector representation of parse trees that relies on a recursive combination of recurrent-neural network encoders. To demonstrate its effectiveness, we use the representation as the backbone of a greedy, bottom-up dependency parser, achieving state-of-the-art accuracies for English and Chinese, without relying on external word embeddings. The parser's implementation is available for download at the first author's webpage.\n\n\n\n\nCoder: Richard E. Vahiran, Michael A. Vahiran.\nAcknowledgements\nThis work is part of the MIT-Dasselblad Research Council, a collaborative effort. The authors gratefully thank D. Borten and his colleagues.\nFurther research could be found in this journal.", "histories": [["v1", "Tue, 1 Mar 2016 17:43:37 GMT  (155kb,D)", "http://arxiv.org/abs/1603.00375v1", null], ["v2", "Thu, 26 May 2016 10:11:44 GMT  (155kb,D)", "http://arxiv.org/abs/1603.00375v2", null]], "reviews": [], "SUBJECTS": "cs.CL", "authors": ["eliyahu kiperwasser", "yoav goldberg"], "accepted": true, "id": "1603.00375"}, "pdf": {"name": "1603.00375.pdf", "metadata": {"source": "CRF", "title": "Easy-First Dependency Parsing with Hierarchical Tree LSTMs", "authors": ["Eliyahu Kiperwasser", "Yoav Goldberg"], "emails": ["elikip@gmail.com", "yoav.goldberg@gmail.com"], "sections": [{"heading": "1 Introduction", "text": "Dependency-based syntactic representations of sentences are central to many language processing tasks (K\u00fcbler et al., 2008). Dependency parse-trees encode not only the syntactic structure of a sentence but also many aspects of its semantics.\nA recent trend in NLP is concerned with encoding sentences as a vectors (\u201csentence embeddings\u201d), which can then be used for further prediction tasks. Recurrent neural networks (RNNs) (Elman, 1990), and in particular methods based on the LSTM architecture (Hochreiter and Schmidhuber, 1997), work very well for modeling sequences, and constantly obtain state-of-the-art results on both language-modeling and prediction tasks (see, e.g. (Mikolov et al., 2010)).\nSeveral works attempt to extend recurrent neural networks to work on trees (see Section 8 for a brief overview), giving rise to the so-called recursive neural networks (Goller and K\u00fcchler, 1996; Socher et al., 2010). However, recursive neural networks\ndo not cope well with trees with arbitrary branching factors \u2013 most work require the encoded trees to be binary-branching, or have a fixed maximum arity. Other attempts allow arbitrary branching factors, in the expense of ignoring the order of the modifiers.\nIn contrast, we propose a tree-encoding that naturally supports trees with arbitrary branching factors, making it particularly appealing for dependency trees. Our tree encoder uses recurrent neural networks as a building block: we model the left and right sequences of modifiers using RNNs, which are composed in a recursive manner to form a tree (Section 3). We use our tree representation for encoding the partially-build parse trees in a greedy, bottom-up dependency parser which is based on the easy-first transition-system of Goldberg and Elhadad (2010).\nUsing the Hierarchical Tree LSTM representation, and without using any external embeddings, our parser achieves parsing accuracies of 92.6 UAS and 90.2 LAS on the PTB (Stanford dependencies) and 86.1 UAS and 84.4 LAS on the Chinese treebank, while relying on greedy decoding.\nTo the best of our knowledge, this is the first work to demonstrate competitive parsing accuracies for full-scale parsing while relying solely on recursive, compositional tree representations, and without using a reranking framework. We discuss related work in Section 8.\nWhile the parsing experiments demonstrate the suitability of our representation for capturing the structural elements in the parse tree that are useful for predicting parsing decisions, we are interested in exploring the use of the RNN-based compositional vector representation of parse trees also for seman-\nar X\niv :1\n60 3.\n00 37\n5v 1\n[ cs\n.C L\n] 1\nM ar\ntic tasks such as sentiment analysis (Socher et al., 2013b; Tai et al., 2015), sentence similarity judgements (Marelli et al., 2014) and textual entailment (Bowman et al., 2015)."}, {"heading": "2 Background and Notation", "text": ""}, {"heading": "2.1 Dependency-based Representation", "text": "A dependency-based syntactic representation is centered around syntactic modification relations between head words and modifier words. The result are trees in which each node is a word in the sentence, and every node except for one designated root node has a parent node. A dependency tree over a sentence with n words w1, . . . , wn can be represented as a list of n pairs of the form (h,m), where 0 \u2264 h \u2264 n and 1 \u2264 m \u2264 n. Each such pair represents an edge in the tree in which h is the index of a head word (including the special ROOT node 0), and m is the index of a modifier word. In order for the dependency trees to be useful for actual downstream language processing tasks, each edge is labeled with a syntactic relation. The tree representation then becomes a list of triplets (h,m, `), where 1 \u2264 ` \u2264 L is the index of a dependency relation out of a designated set of L syntactic relations.\nDependency trees tend to be relatively shallow, with some nodes having many children. Looking at trees in the PTB training set we find that 94% of the trees have a height of at most 10, and 49% of the trees a height of at most 6. In terms of width, 93% of the trees have at least one node with an arity of 4 or more, and 56% of the trees have at least one node with an arity of 6 or more."}, {"heading": "2.2 Recurrent Networks and LSTMs", "text": "Recurrent neural networks (RNNs), first proposed by Elman (1990) are statistical learners for modeling sequential data. In this work, we use the RNN abstraction as a building block, and recursively combine several RNNs to obtain our tree representation. We briefly describe the RNN abstraction below. For further detail on RNNs, the reader is referred to sources such as (Goldberg, 2015; Bengio et al., 2015; Cho, 2015).\nThe RNN abstraction is a function RNN that takes in a sequence of inputs vectors x1, . . . , xn (xi \u2208 Rdin), and produces a sequence of state vec-\ntors (also called output vectors) y1, . . . , yn (yi \u2208 Rdout). Each yi is conditioned on all the inputs x1, . . . , xi preceding it. Ignoring the intermediate outputs y1, . . . , yn\u22121, the RNN can be thought of as encoding the sequence x1, . . . , xn into a final state yn. Our notation in this paper follows this view.\nThe RNN is defined recursively using two functions:1\nRNN(s0, x1, . . . , xn) = yn = O(sn) si = N(si\u22121, xi)\nHere, a function N takes as input a vector xi and a state vector si\u22121 and returns as output a new state si. One can then extract an output vector yi from si using the function O (the function O is usually the identity function, or a function that returns a subset of the elements in si).\nTaking an algorithmic perspective, one can view the RNN as a state object with three operations: s = RNN.initial() returns a new initial state, s.advance(x) takes an input vector and returns a new state, and s.output() returns the output vector for the current state. When clear from the context, we abbreviate and use the state\u2019s name (s) instead of s.output() to refer to the output vector at the state.\nThe functions N and O defining the RNN are parameterized by parameters \u03b8 (matrices and vectors), which are trained from data. Specifically, one is usually interested in using some of the outputs yi for making predictions. The RNN is trained such that the encoding yi is good for the prediction task. That is, the RNN learns which aspects of the sequence x1, . . . , xi are informative for the prediction.\nWe use subscripts (i.e. RNNL, RNNR) to indicate different RNNs, that is, RNNs that have different sets of parameters.\nSpecific instantiations of N and O yield different recurrent network mechanisms. In this work we use the Long Short Term Memory (LSTM) variant (Hochreiter and Schmidhuber, 1997) which is shown to be a very capable sequence learner. However, our algorithm and encoding method do not rely on any specific property of the LSTM architecture, and the\n1We follow the notation of Goldberg (2015), with the exception of taking the output of the RNN to be a single vector rather than a sequence, and renaming R to N .\nLSTM can be transparently switched for any other RNN variant."}, {"heading": "3 Tree Representation", "text": "We now describe our method for representing a tree as a d-dimensional vector. We assume trees in which the children are ordered and there are kl \u2265 0 children before the parent node (left children) and kr \u2265 0 children after it (right children). Such trees correspond well to dependency tree structures. We refer to the parent node as a head, and to its children as modifiers. For a node t, we refer to its left modifiers as t.l1, t.l2, . . . , t.lkl and its right modifiers as t.r1, t.r2, . . . , t.rkr The indices of the modifier are always from the parent outward, that is t.l1 is the left modifier closest to the head t:\nt\nt.r1 t.r2 t.r3 t.r4t.l1t.l2t.l3\nThe gist of the idea is to treat the modifiers of a node as a sequence, and encode this sequence using an RNN. We separate left-modifiers from rightmodifiers, and use two RNNs: the first RNN encodes the sequence of left-modifiers from the head outwards, and the second RNN the sequence of right-modifiers from the head outwards. The first input to each RNN is the vector representation of the head word, and the last input is the vector representation of the left-most or the right-most modifier. The node\u2019s representation is then a concatenation of the RNN encoding of the left-modifiers with the RNN encoding of the right-modifiers. The encoding is recursive: the representation for each of the modifier nodes is computed in a similar fashion.\nt\nR\nL\nt.r1\nR\nt.r2\nR\nt.r3\nR\nt.r4\nR\nt t.l1\nL\nt.l2\nL\nt.l3\nL enc(t)\nRNNR\nRNNL concatenate\nand compress\nMore formally, consider a node t. Let i(t) be the sentence index of the word corresponding to the head node t, and let vi be a vector corresponding to the ith word in the sentence (this vector captures information such as the word form and its part of speech tag, and will be discussed shortly). The vector encoding of a node enc(t) \u2208 Rdenc is then de-\nfined as follows:\nenc(t) =g(W e \u00b7 (el(t) \u25e6 er(t)) + be) el(t) =RNNL(vi(t), enc(t.l1), . . . , enc(t.lkl))\ner(t) =RNNR(vi(t), enc(t.r1), . . . , enc(t.rkr))\nFirst, the sequences consisting of the head-vector vi(t) followed by left-modifiers and the head-vector followed by right-modifiers are encoded using two RNNs, RNNL andRNNR, resulting in RNN states el(t) \u2208 Rdout and er(t) \u2208 Rdout . Then, the RNN states are concatenated, resulting in a 2doutdimensional vector (el(t) \u25e6 er(t)), which is reduced back to d-dimensions using a linear transformation followed by a non-linear activation function g. The recursion stops at leaf nodes, for which:\nenc(leaf) =g(W e \u00b7 (el(leaf) \u25e6 er(leaf)) + be) el(leaf) =RNNL(vi(leaf))\ner(leaf) =RNNR(vi(leaf))\nFigure 1 shows the network used for encoding the sentence \u201cthe black fox who really likes apples did not jump over a lazy dog yesterday\u201d."}, {"heading": "3.1 Representing words", "text": "In the discussion above we assume a vector representation vi associated with the ith sentence word. What does vi look like? A sensible approach would be to take vi to be a function of the word-form and the part-of-speech (POS) tag of the ith word, that is:\nvi = g(W v \u00b7 (wi \u25e6 pi) + bv)\nwhere wi and pi are the embedded vectors of the word-form and POS-tag of the ith word.\nThis encodes each word in isolation, disregarding its context. The context of a word can be very informative regarding its meaning. One way of incorporating context is the Bidirectional RNN (Schuster and Paliwal, 1997). Bidirectional RNNs are shown to be an effective representation for sequence tagging (Irsoy and Cardie, 2014). Bidirectional RNNs represent a word in the sentence using a concatenation of the end-states of two RNNs, one running from the beginning of the sentence to the word and\nyesterday\u201d. Top: the network structure: boxed nodes represent LSTM cells, where L are cells belonging to the leftmodifiers sequence model RNNL, and R to the right-modifiers sequence model RNNR. Circle nodes represent a concatenation followed by a linear transformation and a non-linearity. Bottom: the dependency parse of the sentence.\nthe other running from the end to the word. The result is a vector representation for each word which captures not only the word but also its context.\nWe adopt the Bidirectional LSTM scheme to enrich our node vector representation, and for an nwords sentence compute the vector representations vi as follows:\nv\u2032i =g(W v \u00b7 (wi \u25e6 pi) + bv)\nfi =LSTMF (v \u2032 1, v \u2032 2, . . . , v \u2032 i) bi =LSTMB(v \u2032 n, v \u2032 n\u22121, . . . , v \u2032 i) vi =(fi \u25e6 bi)\nWe plug this word representation as word vectors, allowing each word vector vi to capture information regarding the word form and POS-tag, as well as the sentential context it appears in. The BILSTM encoder is trained jointly with the rest of the network towards the parsing objective, using backpropagation.\nEmbedding vectors The word and POS embeddings wi and pi are also trained together with the network. For the word embeddings, we experiment\nwith random initialization, as well as with initialization using pre-trained word embeddings. Our main goal in this work is not to provide top parsing accuracies, but rather to evaluate the ability of the proposed compositional architecture to learn and capture the structural cues that are needed for accurate parsing. Thus, we are most interested in the random initialization setup: what can the network learn from the training corpus alone, without relying on external resources.\nHowever, the ability to perform semi-supervised learning by initializing the word-embeddings with vectors that are pre-trained on large amount of unannotated data is an appealing property of the neuralnetwork approaches, and we evaluate our parser also in this semi-supervised setup. When using pretrained word embeddings, we follow (Dyer et al., 2015) and use embedding vectors which are trained using positional context (Ling et al., 2015), as these were shown to work better than traditional skipgram vectors for syntactic tasks such as part-ofspeech tagging and parsing."}, {"heading": "3.2 A note on the head-outward generation", "text": "Why did we choose to encode the children from the head outward, and not the other way around? The head outward generation order is needed to facilitate incremental tree construction and allowing for efficient parsing, as we show in section 4 below. Beside the efficiency considerations, using the headoutward encoding puts more emphasis on the outermost dependants, which are known to be the most informative for predicting parse structure.2 We rely on the RNN capability of extracting information from arbitrary positions in the sequence to incorporate information about the head word itself, which appears in the beginning of the sequence. This seem to work well, which is expected considering that the average maximal number of siblings in one direction in the PTB is 4.1, and LSTMs were demonstrated to capture much longer-range interactions. Still, when using the tree encoding in a situation where the tree is fully specified in advance, i.e. for sentence classification, sentence similarity or translation tasks, using a head-inward generation order (or even a bidirectional RNN) may prove to work better. We leave this line of inquiry to future work.\nThe head-outward modifier generation approach has a long history in the parsing literature, and goes back to at least Eisner (1996) and Collins (1997). In contrast to previous work in which each modifier could condition only on a fixed small number of modifiers preceding it, and in which the left- and right- sequences of modifiers were treated as independent from one another for computational efficiency reasons, our approach allows the model to access information from the entirety of both the left and the right sequences jointly."}, {"heading": "4 Parsing Algorithm", "text": "We now turn to explain how to parse using the tree encoder defined above. We begin by describing our bottom-up parsing algorithm, and then show how the\n2Features in transition-based dependency parsers often look at the current left-most and right-most dependents of a given node, and almost never look further than the second left-most or second right-most dependents. Second-order graph based dependency parsers (McDonald, 2006; Eisner, 2000) also condition on the current outermost dependent when generating its sibling.\nencoded vector representation can be built and maintained throughout the parsing process."}, {"heading": "4.1 Bottom-up Parsing", "text": "We follow a (projective) bottom-up parsing strategy, similar to the easy-first parsing algorithm of Goldberg and Elhadad (2010).\nThe main data-structure in the parser is a list of partially-built parse trees we call pending. For a sentence with words w1, . . . , wn, the pending list is initialized with n nodes, where pending[i] corresponds to word wi. The algorithm then chooses two neighbouring trees in the pending list pending[i] and pending[i + 1] and either attaches the root of pending[i+1] as the right-most modifier of the root of pending[i], or attaches the root of pending[i] as the left-most modifier of the root of pending[i+ 1]. The tree which was treated as modifier is then removed from the pending list, shortening it by one. The process ends after n \u2212 1 steps, at which we remain with a single tree in the pending list, which is taken to be the output parse tree. The parsing process is described in Algorithm 1.\nAlgorithm 1 Parsing 1: Input: Sentence w = w1, . . . , wn 2: for i \u2208 1, . . . , n do 3: pend[i].id\u2190 i 4: arcs\u2190 [] 5: while |pend| > 1 do 6: A\u2190 {(i, d) | 1 \u2264 i < |pend|, d \u2208 {l, r}} 7: i, d\u2190 select(A) 8: if d = l then 9: m,h\u2190 pend[i], pend[i+ 1] 10: pend.remove(i) 11: else 12: h,m\u2190 pend[i], pend[i+ 1] 13: pend.remove(i+ 1) 14: arcs.append(h.id,m.id) 15: return arcs\nThis parsing algorithm is both sound and complete with respect to the class of projective dependency trees (Goldberg and Elhadad, 2010). The algorithm depends on non-deterministic choices of an index in the pending list and an attachment direction (line 7). When parsing in practice, the non-\ndeterministic choice will be replaced by using a trained classifier to assign a score to each indexdirection pair, and selecting the highest scoring pair. We discuss the scoring function in Section 4.4, and the training algorithm in Section 5."}, {"heading": "4.2 Bottom-up Tree-Encoding", "text": "We would like the scoring function to condition on the vector encodings of the subtrees it aims to connect. Algorithm 2 shows how to maintain the vector encodings together with the parsing algorithm, so that at every stage in the parsing process each item pending[i] is associated with a vector encoding of the corresponding tree.\nAlgorithm 2 Parsing while maintaining tree representations\n1: Input: Sentence w = w1, . . . , wn 2: Input: Vectors vi corresponding to words wi 3: arcs\u2190 [] 4: for i \u2208 1, . . . , n do 5: pend[i].id\u2190 i 6: pend[i].el \u2190 RNNL.init().append(vi) 7: pend[i].er \u2190 RNNR.init().append(vi) 8: while |pend| > 1 do 9: A\u2190 {(i, d) | 1 \u2264 i < |pend|, d \u2208 {l, r}}\n10: i, d\u2190 select(A) 11: if d = l then 12: m,h\u2190 pend[i], pend[i+ 1] 13: m.c = m.el \u25e6m.er 14: m.enc = g(W (m.c) + b) 15: h.el.append(m.enc) 16: pend.remove(i) 17: else 18: h,m\u2190 pend[i], pend[i+ 1] 19: m.c = m.el \u25e6m.er 20: m.enc = g(W (m.c) + b) 21: h.er.append(m.enc) 22: pend.remove(i+ 1) 23: arcs.add(h.id,m.id) 24: return arcs"}, {"heading": "4.3 Labeled Tree Representation", "text": "The tree representation described above does not account for the relation labels ` the parsing algorithm assigns each edge. In cases the tree is fully specified in advance, the relation of each word to its head can\nbe added to the word representations vi. However, in the context of parsing, the labels become known only when the modifier is attached to its parent. We thus extend the tree representation by concatenating the node vector representation with a vector representation assigned to the label connecting the subtree to its parent. Formally, only the final enc(t) equation changes:\nenc(t) = g(W e \u00b7 (el \u25e6 er \u25e6 `) + be)\nwhere ` is a learned embedding vector associated with the given label."}, {"heading": "4.4 Scoring Function", "text": "The parsing algorithm relies on a function select(A) for choosing the action to take at each stage. We model this function as:\nselect(A) = argmax(i,d,`)\u2208AScore(pend, i, d, `)\nwhere Score(.) is a learned function whose job is to assign scores to possible actions to reflect their quality. Ideally, it will not only score correct actions above incorrect ones, but also more confident (easier) actions above less confident ones, in order to minimize error propagation in the greedy parsing process.\nWhen scoring a possible attachment between a head h and a modifier m with relation `, the scoring function should attempt to reflect the following pieces of information: \u2022 Are the head words of h and m compatible un-\nder relation l? \u2022 Is the modifier m compatible with the already\nexisting modifiers of h? In other words, is m a good subtree to connect as an outer-most modifier in the subtree h? \u2022 Is m complete, in the sense that it already ac-\nquired all of its own modifiers? to this end, the scoring function looks at a window of k subtrees to each side of the head-modifier pair (pend[i\u2212 k], . . . , pend[i+1+ k]) where the neighbouring subtrees are used for providing hints regarding possible additional modifiers ofm and h that are yet to be acquired. We use k = 2 in our experiments, for a total of 6 subtrees in total. This window approach is also used in the Easy-First parser of Goldberg and Elhadad (Goldberg and Elhadad,\n2010) and works that extend it (Tratz and Hovy, 2011; Ma et al., 2012; Ma et al., 2013). However, unlike the previous work, which made use of extensive feature engineering and rich feature functions aiming at extracting the many relevant linguistic sub-structures from the 6 subtrees and their interactions, we provide the scoring function solely with the vector-encoding of the 6 subtrees in the window.\nModeling the labeled attachment score is more difficult than modeling the unlabeled score and is prone to more errors. Moreover, picking the label for an attachment will cause less cascading error in contrast to picking the wrong attachment, which will necessarily preclude the parser from reaching the correct tree structure. In order to partially overcome this issue, our scoring function is a sum of two auxiliary scoring function, one scoring unlabeled and the other scoring labeled attachments. The unlabeled attachment score term in the sum functions as a fallback which makes it easier for a parser to predict the attachment direction even when there is no sufficient certainty as to the label.\nScore(pend, i, d, `) = ScoreU (pend, i, d)\n+ ScoreL(pend, i, d, `)\nEach of ScoreU and ScoreL are modeled as multilayer perceptrons:\nScoreU (pend, i, d) =MLPU (xi)[d]\nScoreL(pend, i, d, `) =MLPL(xi)[(d, `)] xi = pend[i\u2212 2].c \u25e6 \u00b7 \u00b7 \u00b7 \u25e6 pend[i+ 3].c\nwhere MLPU and MLPL are standard multilayer perceptron classifiers with one hidden layer (MLPX(x) =W 2g(W 1x+ b1)+ b2) and have output layers with size 2 and 2L respectively, [.] is an indexing operation, and we assume the values of d and (d, `) are mapped to integer values."}, {"heading": "4.5 Computational Complexity", "text": "The Easy-First parsing algorithm works in O(n log n) time (Goldberg and Elhadad, 2010). The parser in this works differ by three aspects: running a BI-LSTM encoder prior to parsing (O(n)); maintaining the tree representation during parsing (lines 11\u201322 in Algorithm 2) which take a\nconstant time at each parsing step; and local scoring using an MLP rather than a linear classifier (again, a constant-time operation). Thus, the parser maintains the O(n log n) complexity of the Easy-First parser."}, {"heading": "5 Training Algorithm", "text": ""}, {"heading": "5.1 Loss and Parameter Updates", "text": "At each step of the parsing process we select the highest scoring action (i, d, `). The goal of training is to set the Score function such that correct actions are scored above incorrect ones. We use a marginbased objective, aiming to maximize the margin between the highest scoring correct action and the set of incorrect actions. Formally, we define a hinge loss for each parsing step as follows:\nmax{0, 1\u2212max(i,d,`)\u2208GScore(pend, i, d, `) +max(i\u2032,d\u2032,`\u2032)\u2208A\\GScore(pend, i, d, `)}\nwhere A is the set of all possible actions and G is the set of correct actions at the current stage.\nAs the scoring function depends on vectorencodings of the all the trees in the window, and each tree-encoding depends on the network\u2019s parameters, each parameter update will invalidate all the vector encodings, requiring a re-computation of the entire network. We thus sum the local losses throughout the parsing process, and update the parameter with respect to the sum of the losses at sentence boundaries. Furthermore, to increase gradient stability and training speed, we simulate mini-batch updates by only updating the parameters when the sum of local losses contains at least 50 non-zero elements. This assures us a sufficient number of gradients for every update thus minimizing the effect of gradient instability. The gradients of the entire network with respect to the sum of the losses are calculated using the backpropagation algorithm. Initial experiments with an SGD optimizer showed very instable results. We settled instead on using the ADAM optimizer (Kingma and Ba, 2014) which worked well without requiring fiddling with learning rates."}, {"heading": "5.2 Error-Exploration and Dynamic Oracle Training", "text": "At each stage in the training process, the parser assigns scores to all the possible actions (i, d, `) \u2208 A. It then selects an action, applies it, and moves to the\nnext step. Which action should be chosen? A sensible option is to defineG as the set of actions that can lead to the gold tree, and following the highest scoring actions in this set. However, using training in this manner tends to suffer from error propagation at test time. The parser sees only states that result from following correct actions. The lack of examples containing errors in the training phase makes it hard for the parser to infer the best action given partly erroneous trees. In order to cope with this, we follow the error exploration training strategy, in which we let the parser follow the highest scoring action in A during training even if this action is incorrect, exposing it to states that result from erroneous decisions. This strategy requires defining the set G such that the correct actions to take are well-defined also for states that cannot lead to the gold tree. Such a set G is called a dynamic oracle. Error-exploration and dynamic-oracles were introduced by Goldberg and Nivre (2012).\nThe Dynamic Oracle A dynamic-oracle for the easy-first parsing system we use is presented in (Goldberg and Nivre, 2013a). Briefly, the dynamicoracle version of G defines the set of gold actions as the set of actions which does not increase the number of erroneous attachments more than the minimum possible (given previous erroneous actions). The number of erroneous attachments is increased in three cases: (1) connecting a modifier to its head prematurely. Once the modifier is attached it is removed from the pending list and therefore can no longer acquire any of its own modifiers; (2) connecting a modifier to an erroneous head, when the correct head is still on the pending list; (3) connecting a modifier to a correct head, but an incorrect label.\nDealing with cases (2) and (3) is trivial. To deal with (1), we consider as correct only actions in which the modifier is complete. To efficiently identify complete modifiers we hold a counter for each word which is initialized to the number of modifiers the word has in the gold tree. When applying an attachment the counter of the modifier\u2019s gold head word is decreased. When the counter reaches 0, the sub-tree rooted at that word has no pending modifiers, and is considered complete.\nAggressive Exploration We found that even when using error-exploration, after one iteration the model\nremembers the training set quite well, and does not make enough errors to make error-exploration effective. In order to expose the parser to more errors, we employ a cost augmentation scheme: we sometimes follow incorrect actions also if they score below correct actions. Specifically, when the score of the correct action is greater than that of the wrong action but the difference is smaller than the margin constant, we chose to follow the wrong action with probability paug (we use paug = 0.1 in our experiments). Pseudocode for the entire training algorithm is given in the supplementary material."}, {"heading": "5.3 Out-of-vocabulary items and word-dropout", "text": "Due to the sparsity of natural language, we are likely to encounter at test time a substantial number of the words that did not appear in the training data (OOV words). OOV words are likely even when pre-training the word representations on a large unannotated corpora. A common approach is to designate a special \u201cunknown-word\u201d symbol, whose associated vector will be used as the word representation whenever an OOV word is encountered at test time. In order to train the unknown-word vector, a possible approach is to replace all the words appearing in the training corpus less than a certain number of times with the unknown-word symbol. This approach give a good vector representation for unknown words but in the expense of ignoring many of the words from the training corpus.\nWe instead propose a variant of the word-dropout approach (Iyyer et al., 2015). During training, we replace a word with the unknown-word symbol with probability that is inversely proportional to frequency of the word. Formally, we replace a word w appearing #(w) times in the training corpus with the unknown symbol with a probability:\npunk(w) = \u03b1\n#(w) + \u03b1\nUsing this approach we learn a vector representation for unknown words with minimal impact on the training of sparse words."}, {"heading": "6 Implementation Details", "text": "Our Python implementation will be made available at the first author\u2019s website. We use the PyCNN\nwrapper of the CNN library3 for building the computation graph of the network, computing the gradients using automatic differentiation, and performing parameter updates. We noticed the error on the development set does not improve after 20 iterations over the training set, therefore, we ran the training for 20 iterations. The sentences where shuffled between iterations. Non-projective sentences were skipped during training. We use the default parameters initialization, step sizes and regularization values provided by the PyCNN toolkit. The hyperparameters of the final networks used for all the reported experiments are detailed in Table 1.\nWeiss et al (2015) stress the importance of careful hyperparameter tuning for achieving top accuracy in neural network based parser. We did not follow this advice and made very few attempts at hyper-parameter tuning, using manual hill climbing until something seemed to work with reasonable accuracy, and then sticking with it for the rest of the experiments."}, {"heading": "7 Experiments and Results", "text": "We evaluated our parsing model to English and Chinese data. For comparison purposes we followed the setup of (Dyer et al., 2015).\nData For English, we used the Stanford Dependency (SD) (de Marneffe and Manning, 2008) conversion of the Penn Treebank (Marcus et al., 1993), using the standard train/dev/test splitswith the same predicted POS-tags as used in (Dyer et al., 2015;\n3https://github.com/clab/cnn/tree/ master/pycnn\nChen and Manning, 2014). This dataset contains a few non-projective trees. Punctuation symbols are excluded from the evaluation.\nFor Chinese, we use the Penn Chinese Treebank 5.1 (CTB5), using the train/test/dev splits of (Zhang and Clark, 2008; Dyer et al., 2015) with gold partof-speech tags, also following (Dyer et al., 2015; Chen and Manning, 2014).\nWhen using external word embeddings, we also use the same data as (Dyer et al., 2015).4\nExperimental configurations We evaluated the parser in several configuration. BOTTOMUPPARSER is the baseline parser, not using the tree-encoding, and instead representing each item in pending solely by the vector-representation (word and POS) of its head word. BOTTOMUPPARSER+HTLSTM is using our Hierarchical Tree LSTM representation. BOTTOMUPPARSER+HTLSTM+BI-LSTM is the Hierarchical Tree LSTM where we additionally use a BI-LSTM encoding for the head words. Finally, we added external, pre-trained word embeddings to the BOTTOMUPPARSER+HTLSTM+BI-LSTM setup. We also evaluated the final parsers in a \u2013POS setup, in which we did not feed the parser with any POS-tags.\nResults Results for English and Chinese are presented in Tables 2 and 3 respectively. For comparison, we also show the results of the Stack-LSTM transition-based parser model of Dyer et al (2015), which we consider to be state-of-the-art, with and without pre-trained embeddings, and with and without POS-tags.\n4We thank Dyer et al for sharing their data with us.\nThe trends are consistent across the two languages. The baseline Bottom-Up parser performs very poorly. This is expected, as only the head-word of each subtree is used for prediction. When adding the tree-encoding, results jump to near state-of-theart accuracy, suggesting that the composed vector representation indeed successful in capturing predictive structural information. Replacing the headwords with their BI-LSTM encodings result in another increase in accuracy for English, outperforming the Dyer et al (S-LSTM no external) models on the test-set. Adding the external pre-trained embeddings further improve the results for both our parer and Dyer et al\u2019s model, closing the gap between them. When POS-tags are not provided as input, the numbers for both parsers drop. The drop is small for English and large for Chinese, and our parser seem to suffer a little less than the Dyer et al model.\nImportance of the dynamic oracle We also evaluate the importance of using the dynamic oracle and error-exploration training, and find that they are indeed important for achieving high parsing accuracies with our model (Table 4).\nWhen training without error-exploration (that is,\nthe parser follows only correct actions during training and not using the dynamic aspect of the oracle), accuracies of unseen sentences drop by between 0.4 and 0.8 accuracy points (average 0.58). This is consistent with previous work on training with error-exploration and dynamic oracles (Goldberg and Nivre, 2013b), showing that the technique is not restricted to models trained with sparse linear models.\nComparison to other state-of-the-art parsers Our main point of comparison is the model of Dyer et al, which was chosen because it is (a) a very strong parsing model; and (b) is the closest to ours in the literature: a greedy parsing model making heavy use of LSTMs. To this end, we tried to make the comparison to Dyer et al as controlled as possible, using the same dependency annotation schemes, as well as the same predicted POS-tags and the pre-trained embeddings (when applicable).\nIt is also informative to position our results with respect to other state-of-the-art parsing results reported in the literature, as we do in Table 5. Here, some of the comparisons are less direct: some of the results use different dependency annotation schemes5, as well as different predicted POS-tags, and different pre-trained word embeddings. While the numbers are not directly comparable, they do give a good reference as to the expected range of state-of-the-art parsing results. Our system\u2019s English parsing results are in range of state-of-the-art and the Chinese parsing results surpass it. These numbers are achieved while using a greedy, bottom up parsing method without any search, and while relying solely on the compositional tree representations."}, {"heading": "8 Related Work", "text": "We survey two lines of related work: methods for encoding trees as vectors, and methods for parsing with vector representations.\nThe popular approach for encoding trees as vec-\n5Our English parsing experiments use the Stanford Dependencies scheme, while other work use less informative dependency relations which are based on the Penn2Malt converter, using the Yamada and Matsumoto head rules. From our experience, this conversion is somewhat easier to parse, resulting in numbers which are about 0.3-0.4 points higher than Stanford Dependencies.\ntors is using recursive neural networks (Goller and K\u00fcchler, 1996; Socher et al., 2010; Tai et al., 2015). Recursive neural networks represent the vector of a parent node in a tree as a function of its children nodes. However, the functions are usually restricted to having a fixed maximum arity (usually two) (Socher et al., 2010; Tai et al., 2015; Socher, 2014). While trees can be binarized to cope with the arity restriction, doing so result in deep trees which in turn lead to the vanishing gradient problem when training. To cope with the vanishing gradients, (Tai et al., 2015) enrich the composition function with a gating mechanism similar to that of the LSTM, resulting in the so-called Tree-LSTM model. Another approach is to allow arbitrary arities but ignoring the sequential nature of the modifiers, e.g. by using a bag-of-modifiers representation or a convolutional layer (Tai et al., 2015; Zhu et al., 2015). In contrast, our tree encoding method naturally allows for arbitrary branching trees by relying on the well established LSTM sequence model, and using it as a black box.\nIn terms of parsing with vector representations, there are four dominant approaches: search based parsers that use local features that are fed to a neural-network classifier (Pei et al., 2015; Durrett and Klein, 2015); greedy transition based parsers that use local features that are fed into a neuralnetwork classifier (Chen and Manning, 2014; Weiss\net al., 2015), sometimes coupled with a node composition function (Dyer et al., 2015; Watanabe and Sumita, 2015); bottom up parsers that rely solely on recursively combined vector encodings of subtrees (Socher et al., 2010; Stenetorp, 2013; Socher et al., 2013a); and parse-reranking approaches that first produced a k-best list of parses using a traditional parsing technique, and then scores the trees based on a recursive vector encoding of each node (Le and Zuidema, 2014; Le and Zuidema, 2015; Zhu et al., 2015).\nOur parser is a greedy, bottom up parser that rely on compositional vector encodings of subtrees as its sole set of features. Unlike the re-ranking approaches, we do not rely on an external parser to provide k-best lists. Unlike the bottom-up parser in (Socher et al., 2010) who only parses sentences of up to 15 words and the parser of (Stenetorp, 2013) who achieves very low parsing accuracies, we parse arbitrary sentences with state-of-the-art accuracy. Unlike the bottom up parser in (Socher et al., 2013a) we do not make use of a grammar. The parser of (Weiss et al., 2015) obtains exceptionally high results using local features and no composition function. Their secret sauce seems to be a very extensive tuning of hyper-parameters in order to squeeze every possible bit of accuracy. Due to our much more limited resources, we did not perform a methodological search over hyper-parameters, and explored only\na tiny space of the possible hyper-parameters. Finally, perhaps closest to our approach is the greedy, transition-based parser of (Dyer et al., 2015) that also works in a bottom-up fashion, and incorporates and LSTM encoding of the input tokens and hierarchical vector composition into its scoring mechanism. Indeed, that parser obtains similar scores to ours, although we obtain somewhat better results when not using pre-trained embeddings. We differ from the parser of Dyer et al by having a more elaborate vector-composition function, relying solely on the compositional representations, and performing fully bottom-up parsing without being guided by a stack-and-buffer control structure."}, {"heading": "9 Conclusions and Future Work", "text": "We suggest a compositional vector representation of parse trees that relies on a recursive combination of recurrent-neural network encoders, and demonstrate its effectiveness by integrating it in a bottom-up easy-first parser. Future extensions in terms of parsing include the addition of beam search, handling of unknown-words using character-embeddings, and adapting the algorithm to constituency trees. We also plan to establish the effectiveness of our Hierarchical Tree-LSTM encoder by applying it to more semantic vector representation tasks, i.e. training tree representation for capturing sentiment (Socher et al., 2013b; Tai et al., 2015), semantic sentence similarity (Marelli et al., 2014) or textual inference (Bowman et al., 2015).\nAcknowledgements This research is supported by the Intel Collaborative Research Institute for Computational Intelligence (ICRI-CI) and the Israeli Science Foundation (grant number 1555/15)."}, {"heading": "Appendix: Training Algorithm Pseudocode", "text": "Algorithm 3 Training on annotated corpus 1: Input: Sentences w1, . . . , wm 2: Input: Tree annotations T 1, . . . , Tm 3: Input: Number of epochs to train\n4: V \u2190 InitializeV ectors() 5: Loss\u2190 []\n6: for epoch \u2208 {1, . . . , Epochs} do 7: for S, T \u2208 {(w1, T 1), . . . , (wm, Tm)} do 8: Loss\u2190 TrainSentence (S, V [w1, . . . , wn], T, Loss)\n9: if |Loss| > 50 then 10: SumLoss\u2190 sum(Loss) 11: Call ADAM to minimize SumLoss 12: Loss\u2190 []\n(See Algorithm 4, training of a single sentence, on next page.)\nAlgorithm 4 Training on a single sentence with dynamic oracle algorithm 1: function TRAINSENTENCE(w, v, T, Loss) 2: Input: Sentence w = w1, . . . , wn 3: Input: Vectors vi corresponding to inputs wi 4: Input: Annotated tree T in the form of (h,m, rel) triplets 5: Input: List Loss to which loss expressions are added\n6: for i \u2208 1, . . . , n do 7: unassigned[i]\u2190 |Children(wi)| 8: pend[i].id\u2190 i 9: pend[i].el \u2190 RNNL.init().append(vi)\n10: pend[i].er \u2190 RNNR.init().append(vi)\n11: while |pend| > 1 do 12: G,W \u2190 {} , {}\n13: for (i, d, rel) \u2208 {1 \u2264 i < |pend|, d \u2208 {l, r}, rel \u2208 Relations} do 14: if d = l then m,h\u2190 pend[i], pend[i+ 1] 15: else m,h\u2190 pend[i+ 1], pend[i]\n16: if unassigned[m.id] 6= 0 \u2228 \u2203`6=rel(h,m, `) \u2208 T then 17: W.append((h,m, rel)) 18: else G.append((h,m, rel))\n19: hG,mG, relG \u2190 argmax(i,d,`)\u2208GScore(pend, i, d, `) 20: hW ,mW , relW \u2190 argmax(i,d,`)\u2208WScore(pend, i, d, `) 21: scoreG \u2190 Score(hG,mG, relG) 22: scoreW \u2190 Score(hW ,mW , relW )\n23: if scoreG \u2212 scoreW < 0 then 24: h,m, rel, score\u2190 hW ,mW , relW , scoreW 25: else if scoreG \u2212 scoreW > 1 \u2228 random() < paug then 26: h,m, rel, score\u2190 hG,mG, relG, scoreG 27: else 28: h,m, rel, score\u2190 hW ,mW , relW , scoreW 29: if scoreG \u2212 score < 1 then 30: Loss.append(1\u2212 scoreG + score)\n31: m.c = m.el \u25e6m.er 32: m.enc = g(W (m.c \u25e6 rel) + b) 33: if h.id < m.id then h.el.append(m.enc) 34: else h.er.append(m.enc)\n35: unassigned[TParent(m).id]\u2190 unassigned[TParent(m).id]\u2212 1 36: pend.remove(m) 37: return Loss"}], "references": [{"title": "Deep Learning", "author": ["Yoshua Bengio", "Ian J. Goodfellow", "Aaron Courville."], "venue": "Book in preparation for MIT Press.", "citeRegEx": "Bengio et al\\.,? 2015", "shortCiteRegEx": "Bengio et al\\.", "year": 2015}, {"title": "A large annotated corpus for learning natural language inference", "author": ["Samuel R. Bowman", "Gabor Angeli", "Christopher Potts", "Christopher D. Manning."], "venue": "Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing (EMNLP).", "citeRegEx": "Bowman et al\\.,? 2015", "shortCiteRegEx": "Bowman et al\\.", "year": 2015}, {"title": "A Fast and Accurate Dependency Parser using Neural Networks", "author": ["Danqi Chen", "Christopher Manning."], "venue": "Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 740\u2013750, Doha, Qatar, October. Association for", "citeRegEx": "Chen and Manning.,? 2014", "shortCiteRegEx": "Chen and Manning.", "year": 2014}, {"title": "Natural Language Understanding with Distributed Representation", "author": ["Kyunghyun Cho."], "venue": "arXiv:1511.07916 [cs, stat], November.", "citeRegEx": "Cho.,? 2015", "shortCiteRegEx": "Cho.", "year": 2015}, {"title": "Three Generative, Lexicalised Models for Statistical Parsing", "author": ["Michael Collins."], "venue": "Proceedings of the 35th Annual Meeting of the Association for Computational Linguistics, pages 16\u201323, Madrid, Spain, July. Association for Computational Linguistics.", "citeRegEx": "Collins.,? 1997", "shortCiteRegEx": "Collins.", "year": 1997}, {"title": "Stanford dependencies manual", "author": ["Marie-Catherine de Marneffe", "Christopher D. Manning."], "venue": "Technical report, Stanford University.", "citeRegEx": "Marneffe and Manning.,? 2008", "shortCiteRegEx": "Marneffe and Manning.", "year": 2008}, {"title": "Neural CRF Parsing", "author": ["Greg Durrett", "Dan Klein."], "venue": "Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing (Volume 1: Long Papers), pages 302\u2013312,", "citeRegEx": "Durrett and Klein.,? 2015", "shortCiteRegEx": "Durrett and Klein.", "year": 2015}, {"title": "TransitionBased Dependency Parsing with Stack Long ShortTerm Memory", "author": ["Chris Dyer", "Miguel Ballesteros", "Wang Ling", "Austin Matthews", "Noah A. Smith."], "venue": "Proceedings of the 53rd Annual Meeting of the Association for Computational Linguis-", "citeRegEx": "Dyer et al\\.,? 2015", "shortCiteRegEx": "Dyer et al\\.", "year": 2015}, {"title": "Three New Probabilistic Models for Dependency Parsing: An Exploration", "author": ["Jason M. Eisner."], "venue": "COLING 1996 Volume 1: The 16th International Conference on Computational Linguistics.", "citeRegEx": "Eisner.,? 1996", "shortCiteRegEx": "Eisner.", "year": 1996}, {"title": "Bilexical grammars and their cubictime parsing algorithms", "author": ["J. Eisner."], "venue": "Advances in Probabilistic and Other Parsing Technologies.", "citeRegEx": "Eisner.,? 2000", "shortCiteRegEx": "Eisner.", "year": 2000}, {"title": "Finding Structure in Time", "author": ["Jeffrey L. Elman."], "venue": "Cognitive Science, 14(2):179\u2013211, March.", "citeRegEx": "Elman.,? 1990", "shortCiteRegEx": "Elman.", "year": 1990}, {"title": "An Efficient Algorithm for Easy-First Non-Directional Dependency Parsing", "author": ["Yoav Goldberg", "Michael Elhadad."], "venue": "Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the Association for Computational Linguis-", "citeRegEx": "Goldberg and Elhadad.,? 2010", "shortCiteRegEx": "Goldberg and Elhadad.", "year": 2010}, {"title": "A dynamic oracle for the arc-eager system", "author": ["Yoav Goldberg", "Joakim Nivre."], "venue": "Proc. of COLING 2012.", "citeRegEx": "Goldberg and Nivre.,? 2012", "shortCiteRegEx": "Goldberg and Nivre.", "year": 2012}, {"title": "Easy-First Chinese POS Tagging and Dependency Parsing", "author": ["Ji Ma", "Tong Xiao", "Jingbo Zhu", "Feiliang Ren."], "venue": "Proceedings of COLING 2012, pages 1731\u20131746, Mumbai, India, December. The COLING 2012 Organizing Committee.", "citeRegEx": "Ma et al\\.,? 2012", "shortCiteRegEx": "Ma et al\\.", "year": 2012}, {"title": "Easy-First POS Tagging and Dependency Parsing with Beam Search", "author": ["Ji Ma", "Jingbo Zhu", "Tong Xiao", "Nan Yang."], "venue": "Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers), pages 110\u2013114, Sofia,", "citeRegEx": "Ma et al\\.,? 2013", "shortCiteRegEx": "Ma et al\\.", "year": 2013}, {"title": "Building a large annotated corpus of English: The penn Treebank", "author": ["Mitchell P. Marcus", "Beatrice Santorini", "Mary Ann Marchinkiewicz."], "venue": "Computational Linguistics, 19.", "citeRegEx": "Marcus et al\\.,? 1993", "shortCiteRegEx": "Marcus et al\\.", "year": 1993}, {"title": "Semeval-2014 task 1: Evaluation of compositional distributional semantic models on full sentences through semantic relatedness and textual entail", "author": ["Marco Marelli", "Luisa Bentivogli", "Marco Baroni", "Raffaella Bernardi", "Stefano Menini", "Roberto Zamparelli"], "venue": null, "citeRegEx": "Marelli et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Marelli et al\\.", "year": 2014}, {"title": "Turning on the turbo: Fast third-order nonprojective turbo parsers", "author": ["Andre Martins", "Miguel Almeida", "Noah A. Smith."], "venue": "Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers), pages 617\u2013622,", "citeRegEx": "Martins et al\\.,? 2013", "shortCiteRegEx": "Martins et al\\.", "year": 2013}, {"title": "Discriminative Training and Spanning Tree Algorithms for Dependency Parsing", "author": ["Ryan McDonald."], "venue": "Ph.D. thesis, University of Pennsylvania.", "citeRegEx": "McDonald.,? 2006", "shortCiteRegEx": "McDonald.", "year": 2006}, {"title": "Recurrent neural network based language model", "author": ["Tomas Mikolov", "Martin Karafi\u00e1t", "Lukas Burget", "Jan Cernocky", "Sanjeev Khudanpur."], "venue": "INTERSPEECH 2010, 11th Annual Conference of the International Speech Communication Association,", "citeRegEx": "Mikolov et al\\.,? 2010", "shortCiteRegEx": "Mikolov et al\\.", "year": 2010}, {"title": "An Effective Neural Network Model for Graph-based Dependency Parsing", "author": ["Wenzhe Pei", "Tao Ge", "Baobao Chang."], "venue": "Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Nat-", "citeRegEx": "Pei et al\\.,? 2015", "shortCiteRegEx": "Pei et al\\.", "year": 2015}, {"title": "Bidirectional recurrent neural networks", "author": ["M. Schuster", "Kuldip K. Paliwal."], "venue": "IEEE Transactions on Signal Processing, 45(11):2673\u20132681, November.", "citeRegEx": "Schuster and Paliwal.,? 1997", "shortCiteRegEx": "Schuster and Paliwal.", "year": 1997}, {"title": "Learning Continuous Phrase Representations and Syntactic Parsing with Recursive Neural Networks", "author": ["Richard Socher", "Christopher Manning", "Andrew Ng."], "venue": "Proceedings of the Deep Learning and Unsupervised Feature Learning Workshop of {NIPS}", "citeRegEx": "Socher et al\\.,? 2010", "shortCiteRegEx": "Socher et al\\.", "year": 2010}, {"title": "Parsing with Compositional Vector Grammars", "author": ["Richard Socher", "John Bauer", "Christopher D. Manning", "Ng Andrew Y."], "venue": "Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 455\u2013465,", "citeRegEx": "Socher et al\\.,? 2013a", "shortCiteRegEx": "Socher et al\\.", "year": 2013}, {"title": "Recursive Deep Models for Semantic Compositionality Over a Sentiment Treebank", "author": ["Richard Socher", "Alex Perelygin", "Jean Wu", "Jason Chuang", "Christopher D. Manning", "Andrew Ng", "Christopher Potts."], "venue": "Proceedings of the 2013 Conference on Empiri-", "citeRegEx": "Socher et al\\.,? 2013b", "shortCiteRegEx": "Socher et al\\.", "year": 2013}, {"title": "Recursive Deep Learning For Natural Language Processing and Computer Vision", "author": ["Richard Socher."], "venue": "Ph.D. thesis, Stanford University, August.", "citeRegEx": "Socher.,? 2014", "shortCiteRegEx": "Socher.", "year": 2014}, {"title": "Transition-based Dependency Parsing Using Recursive Neural Networks", "author": ["Pontus Stenetorp."], "venue": "Deep Learning Workshop at the 2013 Conference on Neural Information Processing Systems (NIPS), Lake Tahoe, Nevada, USA, December.", "citeRegEx": "Stenetorp.,? 2013", "shortCiteRegEx": "Stenetorp.", "year": 2013}, {"title": "Improved Semantic Representations From Tree-Structured Long Short-Term Memory Networks", "author": ["Kai Sheng Tai", "Richard Socher", "Christopher D. Manning."], "venue": "Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics", "citeRegEx": "Tai et al\\.,? 2015", "shortCiteRegEx": "Tai et al\\.", "year": 2015}, {"title": "A fast, effective, non-projective, semantically-enriched parser", "author": ["Stephen Tratz", "Eduard Hovy."], "venue": "Proc. of EMNLP.", "citeRegEx": "Tratz and Hovy.,? 2011", "shortCiteRegEx": "Tratz and Hovy.", "year": 2011}, {"title": "Transitionbased Neural Constituent Parsing", "author": ["Taro Watanabe", "Eiichiro Sumita."], "venue": "Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing (Volume", "citeRegEx": "Watanabe and Sumita.,? 2015", "shortCiteRegEx": "Watanabe and Sumita.", "year": 2015}, {"title": "Structured Training for Neural Network Transition-Based Parsing", "author": ["David Weiss", "Chris Alberti", "Michael Collins", "Slav Petrov."], "venue": "Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference", "citeRegEx": "Weiss et al\\.,? 2015", "shortCiteRegEx": "Weiss et al\\.", "year": 2015}, {"title": "A tale of two parsers: investigating and combining graph-based and transition-based dependency parsing using beamsearch", "author": ["Yue Zhang", "Stephen Clark."], "venue": "Proc. of EMNLP.", "citeRegEx": "Zhang and Clark.,? 2008", "shortCiteRegEx": "Zhang and Clark.", "year": 2008}, {"title": "Transition-based dependency parsing with rich non-local features", "author": ["Yue Zhang", "Joakim Nivre."], "venue": "Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies, pages 188\u2013193.", "citeRegEx": "Zhang and Nivre.,? 2011", "shortCiteRegEx": "Zhang and Nivre.", "year": 2011}, {"title": "A Re-ranking Model for Dependency Parser with Recursive Convolutional Neural Network", "author": ["Chenxi Zhu", "Xipeng Qiu", "Xinchi Chen", "Xuanjing Huang."], "venue": "Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th", "citeRegEx": "Zhu et al\\.,? 2015", "shortCiteRegEx": "Zhu et al\\.", "year": 2015}], "referenceMentions": [{"referenceID": 10, "context": "Recurrent neural networks (RNNs) (Elman, 1990), and in particular methods based on the LSTM architecture (Hochreiter and Schmidhuber, 1997), work very well for modeling sequences, and constantly obtain state-of-the-art results on both language-modeling and prediction tasks (see, e.", "startOffset": 33, "endOffset": 46}, {"referenceID": 19, "context": "(Mikolov et al., 2010)).", "startOffset": 0, "endOffset": 22}, {"referenceID": 22, "context": "Several works attempt to extend recurrent neural networks to work on trees (see Section 8 for a brief overview), giving rise to the so-called recursive neural networks (Goller and K\u00fcchler, 1996; Socher et al., 2010).", "startOffset": 168, "endOffset": 215}, {"referenceID": 11, "context": "We use our tree representation for encoding the partially-build parse trees in a greedy, bottom-up dependency parser which is based on the easy-first transition-system of Goldberg and Elhadad (2010).", "startOffset": 171, "endOffset": 199}, {"referenceID": 24, "context": "tic tasks such as sentiment analysis (Socher et al., 2013b; Tai et al., 2015), sentence similarity judgements (Marelli et al.", "startOffset": 37, "endOffset": 77}, {"referenceID": 27, "context": "tic tasks such as sentiment analysis (Socher et al., 2013b; Tai et al., 2015), sentence similarity judgements (Marelli et al.", "startOffset": 37, "endOffset": 77}, {"referenceID": 16, "context": ", 2015), sentence similarity judgements (Marelli et al., 2014) and textual entailment (Bowman et al.", "startOffset": 40, "endOffset": 62}, {"referenceID": 1, "context": ", 2014) and textual entailment (Bowman et al., 2015).", "startOffset": 31, "endOffset": 52}, {"referenceID": 0, "context": "For further detail on RNNs, the reader is referred to sources such as (Goldberg, 2015; Bengio et al., 2015; Cho, 2015).", "startOffset": 70, "endOffset": 118}, {"referenceID": 3, "context": "For further detail on RNNs, the reader is referred to sources such as (Goldberg, 2015; Bengio et al., 2015; Cho, 2015).", "startOffset": 70, "endOffset": 118}, {"referenceID": 8, "context": "Recurrent neural networks (RNNs), first proposed by Elman (1990) are statistical learners for modeling sequential data.", "startOffset": 52, "endOffset": 65}, {"referenceID": 21, "context": "One way of incorporating context is the Bidirectional RNN (Schuster and Paliwal, 1997).", "startOffset": 58, "endOffset": 86}, {"referenceID": 7, "context": "When using pretrained word embeddings, we follow (Dyer et al., 2015) and use embedding vectors which are trained using positional context (Ling et al.", "startOffset": 49, "endOffset": 68}, {"referenceID": 7, "context": "The head-outward modifier generation approach has a long history in the parsing literature, and goes back to at least Eisner (1996) and Collins (1997).", "startOffset": 118, "endOffset": 132}, {"referenceID": 4, "context": "The head-outward modifier generation approach has a long history in the parsing literature, and goes back to at least Eisner (1996) and Collins (1997). In contrast to previous work in which each modifier could condition only on a fixed small number of modifiers preceding it, and in which the left- and right- sequences of modifiers were treated as independent from one another for computational efficiency reasons, our approach allows the model to access information from the entirety of both the left and the right sequences jointly.", "startOffset": 136, "endOffset": 151}, {"referenceID": 18, "context": "Second-order graph based dependency parsers (McDonald, 2006; Eisner, 2000) also condition on the current outermost dependent when generating its sibling.", "startOffset": 44, "endOffset": 74}, {"referenceID": 9, "context": "Second-order graph based dependency parsers (McDonald, 2006; Eisner, 2000) also condition on the current outermost dependent when generating its sibling.", "startOffset": 44, "endOffset": 74}, {"referenceID": 11, "context": "We follow a (projective) bottom-up parsing strategy, similar to the easy-first parsing algorithm of Goldberg and Elhadad (2010).", "startOffset": 100, "endOffset": 128}, {"referenceID": 11, "context": "This parsing algorithm is both sound and complete with respect to the class of projective dependency trees (Goldberg and Elhadad, 2010).", "startOffset": 107, "endOffset": 135}, {"referenceID": 28, "context": "2010) and works that extend it (Tratz and Hovy, 2011; Ma et al., 2012; Ma et al., 2013).", "startOffset": 31, "endOffset": 87}, {"referenceID": 13, "context": "2010) and works that extend it (Tratz and Hovy, 2011; Ma et al., 2012; Ma et al., 2013).", "startOffset": 31, "endOffset": 87}, {"referenceID": 14, "context": "2010) and works that extend it (Tratz and Hovy, 2011; Ma et al., 2012; Ma et al., 2013).", "startOffset": 31, "endOffset": 87}, {"referenceID": 11, "context": "The Easy-First parsing algorithm works in O(n log n) time (Goldberg and Elhadad, 2010).", "startOffset": 58, "endOffset": 86}, {"referenceID": 3, "context": "Which action should be chosen? A sensible option is to defineG as the set of actions that can lead to the gold tree, and following the highest scoring actions in this set. However, using training in this manner tends to suffer from error propagation at test time. The parser sees only states that result from following correct actions. The lack of examples containing errors in the training phase makes it hard for the parser to infer the best action given partly erroneous trees. In order to cope with this, we follow the error exploration training strategy, in which we let the parser follow the highest scoring action in A during training even if this action is incorrect, exposing it to states that result from erroneous decisions. This strategy requires defining the set G such that the correct actions to take are well-defined also for states that cannot lead to the gold tree. Such a set G is called a dynamic oracle. Error-exploration and dynamic-oracles were introduced by Goldberg and Nivre (2012).", "startOffset": 23, "endOffset": 1008}, {"referenceID": 7, "context": "For comparison purposes we followed the setup of (Dyer et al., 2015).", "startOffset": 49, "endOffset": 68}, {"referenceID": 15, "context": "Data For English, we used the Stanford Dependency (SD) (de Marneffe and Manning, 2008) conversion of the Penn Treebank (Marcus et al., 1993), using the standard train/dev/test splitswith the same predicted POS-tags as used in (Dyer et al.", "startOffset": 119, "endOffset": 140}, {"referenceID": 31, "context": "1 (CTB5), using the train/test/dev splits of (Zhang and Clark, 2008; Dyer et al., 2015) with gold partof-speech tags, also following (Dyer et al.", "startOffset": 45, "endOffset": 87}, {"referenceID": 7, "context": "1 (CTB5), using the train/test/dev splits of (Zhang and Clark, 2008; Dyer et al., 2015) with gold partof-speech tags, also following (Dyer et al.", "startOffset": 45, "endOffset": 87}, {"referenceID": 7, "context": ", 2015) with gold partof-speech tags, also following (Dyer et al., 2015; Chen and Manning, 2014).", "startOffset": 53, "endOffset": 96}, {"referenceID": 2, "context": ", 2015) with gold partof-speech tags, also following (Dyer et al., 2015; Chen and Manning, 2014).", "startOffset": 53, "endOffset": 96}, {"referenceID": 7, "context": "When using external word embeddings, we also use the same data as (Dyer et al., 2015).", "startOffset": 66, "endOffset": 85}, {"referenceID": 32, "context": "The different systems and the numbers reported from them are taken from: ZhangNivre11: (Zhang and Nivre, 2011); Martins13: (Martins et al.", "startOffset": 87, "endOffset": 110}, {"referenceID": 17, "context": "The different systems and the numbers reported from them are taken from: ZhangNivre11: (Zhang and Nivre, 2011); Martins13: (Martins et al., 2013); Weiss15 (Weiss et al.", "startOffset": 123, "endOffset": 145}, {"referenceID": 30, "context": ", 2013); Weiss15 (Weiss et al., 2015); Pei15: (Pei et al.", "startOffset": 17, "endOffset": 37}, {"referenceID": 20, "context": ", 2015); Pei15: (Pei et al., 2015); LeZuidema14 (Le and Zuidema, 2014); Zhu15: (Zhu et al.", "startOffset": 16, "endOffset": 34}, {"referenceID": 33, "context": ", 2015); LeZuidema14 (Le and Zuidema, 2014); Zhu15: (Zhu et al., 2015).", "startOffset": 52, "endOffset": 70}, {"referenceID": 22, "context": "tors is using recursive neural networks (Goller and K\u00fcchler, 1996; Socher et al., 2010; Tai et al., 2015).", "startOffset": 40, "endOffset": 105}, {"referenceID": 27, "context": "tors is using recursive neural networks (Goller and K\u00fcchler, 1996; Socher et al., 2010; Tai et al., 2015).", "startOffset": 40, "endOffset": 105}, {"referenceID": 22, "context": "However, the functions are usually restricted to having a fixed maximum arity (usually two) (Socher et al., 2010; Tai et al., 2015; Socher, 2014).", "startOffset": 92, "endOffset": 145}, {"referenceID": 27, "context": "However, the functions are usually restricted to having a fixed maximum arity (usually two) (Socher et al., 2010; Tai et al., 2015; Socher, 2014).", "startOffset": 92, "endOffset": 145}, {"referenceID": 25, "context": "However, the functions are usually restricted to having a fixed maximum arity (usually two) (Socher et al., 2010; Tai et al., 2015; Socher, 2014).", "startOffset": 92, "endOffset": 145}, {"referenceID": 27, "context": "To cope with the vanishing gradients, (Tai et al., 2015) enrich the composition function with a gating mechanism similar to that of the LSTM, resulting in the so-called Tree-LSTM model.", "startOffset": 38, "endOffset": 56}, {"referenceID": 27, "context": "by using a bag-of-modifiers representation or a convolutional layer (Tai et al., 2015; Zhu et al., 2015).", "startOffset": 68, "endOffset": 104}, {"referenceID": 33, "context": "by using a bag-of-modifiers representation or a convolutional layer (Tai et al., 2015; Zhu et al., 2015).", "startOffset": 68, "endOffset": 104}, {"referenceID": 20, "context": "In terms of parsing with vector representations, there are four dominant approaches: search based parsers that use local features that are fed to a neural-network classifier (Pei et al., 2015; Durrett and Klein, 2015); greedy transition based parsers that use local features that are fed into a neuralnetwork classifier (Chen and Manning, 2014; Weiss et al.", "startOffset": 174, "endOffset": 217}, {"referenceID": 6, "context": "In terms of parsing with vector representations, there are four dominant approaches: search based parsers that use local features that are fed to a neural-network classifier (Pei et al., 2015; Durrett and Klein, 2015); greedy transition based parsers that use local features that are fed into a neuralnetwork classifier (Chen and Manning, 2014; Weiss et al.", "startOffset": 174, "endOffset": 217}, {"referenceID": 2, "context": ", 2015; Durrett and Klein, 2015); greedy transition based parsers that use local features that are fed into a neuralnetwork classifier (Chen and Manning, 2014; Weiss et al., 2015), sometimes coupled with a node composition function (Dyer et al.", "startOffset": 135, "endOffset": 179}, {"referenceID": 30, "context": ", 2015; Durrett and Klein, 2015); greedy transition based parsers that use local features that are fed into a neuralnetwork classifier (Chen and Manning, 2014; Weiss et al., 2015), sometimes coupled with a node composition function (Dyer et al.", "startOffset": 135, "endOffset": 179}, {"referenceID": 7, "context": ", 2015), sometimes coupled with a node composition function (Dyer et al., 2015; Watanabe and Sumita, 2015); bottom up parsers that rely solely on recursively combined vector encodings of subtrees (Socher et al.", "startOffset": 60, "endOffset": 106}, {"referenceID": 29, "context": ", 2015), sometimes coupled with a node composition function (Dyer et al., 2015; Watanabe and Sumita, 2015); bottom up parsers that rely solely on recursively combined vector encodings of subtrees (Socher et al.", "startOffset": 60, "endOffset": 106}, {"referenceID": 22, "context": ", 2015; Watanabe and Sumita, 2015); bottom up parsers that rely solely on recursively combined vector encodings of subtrees (Socher et al., 2010; Stenetorp, 2013; Socher et al., 2013a); and parse-reranking approaches that first produced a k-best list of parses using a traditional parsing technique, and then scores the trees based on a recursive vector encoding of each node (Le and Zuidema, 2014; Le and Zuidema, 2015; Zhu et al.", "startOffset": 124, "endOffset": 184}, {"referenceID": 26, "context": ", 2015; Watanabe and Sumita, 2015); bottom up parsers that rely solely on recursively combined vector encodings of subtrees (Socher et al., 2010; Stenetorp, 2013; Socher et al., 2013a); and parse-reranking approaches that first produced a k-best list of parses using a traditional parsing technique, and then scores the trees based on a recursive vector encoding of each node (Le and Zuidema, 2014; Le and Zuidema, 2015; Zhu et al.", "startOffset": 124, "endOffset": 184}, {"referenceID": 23, "context": ", 2015; Watanabe and Sumita, 2015); bottom up parsers that rely solely on recursively combined vector encodings of subtrees (Socher et al., 2010; Stenetorp, 2013; Socher et al., 2013a); and parse-reranking approaches that first produced a k-best list of parses using a traditional parsing technique, and then scores the trees based on a recursive vector encoding of each node (Le and Zuidema, 2014; Le and Zuidema, 2015; Zhu et al.", "startOffset": 124, "endOffset": 184}, {"referenceID": 33, "context": ", 2013a); and parse-reranking approaches that first produced a k-best list of parses using a traditional parsing technique, and then scores the trees based on a recursive vector encoding of each node (Le and Zuidema, 2014; Le and Zuidema, 2015; Zhu et al., 2015).", "startOffset": 200, "endOffset": 262}, {"referenceID": 22, "context": "Unlike the bottom-up parser in (Socher et al., 2010) who only parses sentences of up to 15 words and the parser of (Stenetorp, 2013) who achieves very low parsing accuracies, we parse arbitrary sentences with state-of-the-art accuracy.", "startOffset": 31, "endOffset": 52}, {"referenceID": 26, "context": ", 2010) who only parses sentences of up to 15 words and the parser of (Stenetorp, 2013) who achieves very low parsing accuracies, we parse arbitrary sentences with state-of-the-art accuracy.", "startOffset": 70, "endOffset": 87}, {"referenceID": 23, "context": "Unlike the bottom up parser in (Socher et al., 2013a) we do not make use of a grammar.", "startOffset": 31, "endOffset": 53}, {"referenceID": 30, "context": "The parser of (Weiss et al., 2015) obtains exceptionally high results using local features and no composition function.", "startOffset": 14, "endOffset": 34}, {"referenceID": 7, "context": "Finally, perhaps closest to our approach is the greedy, transition-based parser of (Dyer et al., 2015) that also works in a bottom-up fashion, and incorporates and LSTM encoding of the input tokens and hierarchical vector composition into its scoring mechanism.", "startOffset": 83, "endOffset": 102}, {"referenceID": 24, "context": "training tree representation for capturing sentiment (Socher et al., 2013b; Tai et al., 2015), semantic sentence similarity (Marelli et al.", "startOffset": 53, "endOffset": 93}, {"referenceID": 27, "context": "training tree representation for capturing sentiment (Socher et al., 2013b; Tai et al., 2015), semantic sentence similarity (Marelli et al.", "startOffset": 53, "endOffset": 93}, {"referenceID": 16, "context": ", 2015), semantic sentence similarity (Marelli et al., 2014) or textual inference (Bowman et al.", "startOffset": 38, "endOffset": 60}, {"referenceID": 1, "context": ", 2014) or textual inference (Bowman et al., 2015).", "startOffset": 29, "endOffset": 50}], "year": 2017, "abstractText": "We suggest a compositional vector representation of parse trees that relies on a recursive combination of recurrent-neural network encoders. To demonstrate its effectiveness, we use the representation as the backbone of a greedy, bottom-up dependency parser, achieving state-of-the-art accuracies for English and Chinese, without relying on external word embeddings. The parser\u2019s implementation is available for download at the first author\u2019s webpage.", "creator": "TeX"}}}