{"id": "1603.00423", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "1-Mar-2016", "title": "Quantifying the vanishing gradient and long distance dependency problem in recursive neural networks and recursive LSTMs", "abstract": "Recursive neural networks (RNN) and their recently proposed extension recursive long short term memory networks (RLSTM) are models that compute representations for sentences, by recursively combining word embeddings according to an externally provided parse tree. Both models thus, unlike recurrent networks, explicitly make use of the hierarchical structure of a sentence. In this paper, we demonstrate that RNNs nevertheless suffer from the vanishing gradient and long distance dependency problem, and that RLSTMs greatly improve over RNN's on these problems by learning from existing RNNs that they are not.\n\n\n\nIn addition to improving RNN's performance over RNN, RNNs also perform more computations than RNNs, such as recursively adding a sentence to the list.\nThe following model shows that RNNs have a more sophisticated model for the long term memory networks than RNNs, because it is more accurate, but lacks an overall model for RLSTMs, because they have more dependencies to make it more efficient. For example, RNNs currently support multiple input parameters, so it may not be feasible to use the same RNNs. However, we are also interested in how well they are implemented and how quickly they are implemented. We show that they have better predictions in the past.\nIn this post, we explain RNNs as a neural network model, to demonstrate how RNNs could be improved over RNNs, so that they may be improved over RNNs. This model is more accurate than previous models, because it is more efficient for re-use of RNNs, and to make use of the RLSTM models.", "histories": [["v1", "Tue, 1 Mar 2016 19:45:25 GMT  (201kb,D)", "http://arxiv.org/abs/1603.00423v1", null]], "reviews": [], "SUBJECTS": "cs.AI cs.CL cs.NE", "authors": ["phong le", "willem zuidema"], "accepted": false, "id": "1603.00423"}, "pdf": {"name": "1603.00423.pdf", "metadata": {"source": "CRF", "title": "Quantifying the vanishing gradient and long distance dependency problem in recursive neural networks and recursive LSTMs", "authors": ["Phong Le"], "emails": ["p.le@uva.nl", "zuidema@uva.nl"], "sections": [{"heading": "1 Introduction", "text": "The recursive neural network (RNN) model became popular since the work of Socher et al. (2010). It has been employed to tackle several NLP tasks, such as syntactic parsing (Socher et al., 2013a), machine translation (Liu et al., 2014), and word embedding learning (Luong et al., 2013). However, like traditional recurrent neural networks, the RNN seems to suffer from the vanish-\ning gradient problem, in which error signals propagating from the root in a parse tree to the child nodes shrink very quickly. Moreover, it encounters difficulties in capturing long range dependencies: information propagating from child nodes deep in a parse tree can be obscured before reaching the root node.\nIn the recurrent neural network world, the long short term memory (LSTM) architecture (Hochreiter and Schmidhuber, 1997) is often used as a solution to these two problems. A natural extension of the LSTM can be defined for tree structures, which we call Recursive LSTM (RLSTM), as proposed independently by Tai et al. (2015), Zhu et al. (2015), and Le and Zuidema (2015). However, while there is intensive research showing how the LSTM architecture can overcome those two problems compared to traditional recurrent models (e.g., Gers and Schmidhuber (2001)), such research is, to our knowledge, still absent for the comparison between RNNs and RLSTMs. Therefore, in the current paper we investigate the following two questions:\n1. Is the RLSTM more capable of capturing long range dependencies than the RNN?\n2. Does the RLSTM overcome the vanishing gradient problem more effectively than the RNN?\nSupervised learning requires annotated data, which is often expensive to collect. As a result, examining a model on natural data on many different aspects can be difficult because the portion of data that fits a specific aspect could not be sufficient. Moreover, studying individual aspects separately is hard since many aspects are often correlated with each other. This, unfortunately, is true in our case: answering those two questions requires us to evaluate the examined models on datasets of dif-\nar X\niv :1\n60 3.\n00 42\n3v 1\n[ cs\n.A I]\n1 M\nar 2\n01 6\nferent tree depths, in which the key nodes which contain decisive information in a parse tree must be identified. Using available annotated corpora such as the Stanford Sentiment Treebank (Socher et al., 2013b) and the Penn Treebank is thus inappropriate, as they are too small for this purpose (10k, 40k trees, respectively, compared to 240k trees in our experiments), and key nodes are not marked. Our solution is an artificial task where sentences and parse trees can be randomly generated under any arbitrary constraints on tree depth and key node\u2019s position."}, {"heading": "2 Background", "text": "Both the RNN and the RLSTM model are instances of a general framework which takes a sentence, syntactic tree, and vector representations for the words in the sentence as input, and applies a composition function to recursively compute vector representations for all the phrases in the tree and the complete sentence. Technically speaking, given a production p \u2192 x y, and x,y \u2208 Rn representing x, y, we compute p \u2208 Rn for p by p = F (x,y), where F is a composition function.\nIn the RNN, F is a one-layer feed-forward neural network. In the RLSTM, a node u is represented by the vector u = [ur;uc] resulting from concatenating a vector representing the phrase that the node covers and a memory vector. F could be any LSTM that can combine two such concatenation vectors, such as Structure-LSTM (Zhu et al., 2015), Tree-LSTM (Tai et al., 2015), and LSTM-RNN (Le and Zuidema, 2015). In the current paper, we use the implementation1 of Le and Zuidema (2015)."}, {"heading": "3 Experiments", "text": "We now examine how the two problems, the vanishing gradient problem and the problem of how to capture long range dependencies, affect the RLSTM model and the RNN model. To do so, we propose the following artificial task, which requires a model to distinguish useful signals from noise. We define:\n\u2022 a sentence is a sequence of tokens which are integer numbers in the range [0, 10000];\n\u2022 a sentence contains one and only one keyword token which is an integer number smaller than 1000;\n1https://github.com/lephong/lstm-rnn\n\u2022 a sentence is labeled with the integer resulting from dividing the keyword by 100. For instance, if the keyword is 607, the label is 6. In this way, there are 10 classes, ranging from 0 to 9.\nThe task is to predict the class of a sentence, given its binary parse tree (Figure 1). Because the label of a sentence is determined solely by the keyword, the two models need to identify the keyword in the parse tree and allow only the information from the leaf node of the keyword to affect the root node. It is worth noting that this task resembles sentiment analysis with simple cases in which the sentiment of a whole sentence is determined by one keyword (e.g. \u201cI like the movie\u201d). Simulating complex cases involving negation, composition, etc. is straightforward and for future work. But here we believe that the current task is adequate to answer our two questions raised in Section 1.\nThe two models, RLSTM and RNN, were implemented with the dimension of vector representations and vector memories 50. Following Socher et al. (2013b), we used tanh as the activation function, and initialized word vectors by randomly sampling each value from a uniform distribution U(\u22120.0001, 0.0001). We trained the two models using the AdaGrad method (Duchi et al., 2011) with a learning rate of 0.05 and a mini-batch size of 20 for the RNN and of 5 for the RLSTM. Development sets were employed for early stopping (training is halted when the accuracy on the development set is not improved after 5 consecutive epochs)."}, {"heading": "3.1 Experiment 1", "text": "We randomly generated 10 datasets. To generate a sentence of length l, we shuffle a list of randomly chosen l\u2212 1 non-keywords and one keyword. The i-th dataset contains 12k sentences of lengths from 10i\u22129 tokens to 10i tokens, and is split into train, dev, test sets with sizes of 10k, 1k, 1k sentences. We parsed each sentence by randomly generating a binary tree whose number of leaf nodes equals to the sentence length.\nThe test accuracies of the two models on the 10 datasets are shown in Figure 2; For each dataset we run each model 5 times and reported the highest accuracy for the RNN model, and the distribution of accuracies (via boxplot) for the RLSTM model. We can see that the RNN model performs reasonably well on very short sentences\n(less than 11 tokens). However, when the sentence length exceeds 10, the RNN\u2019s performance drops so quickly that the difference between it and the random guess\u2019 performance (10%) is negligible. Trying different learning rates, mini-batch sizes, and values for n (the dimension of vectors) did not give significant differences. On the other hand, the RLSTM model achieves more than 90% accuracy on sentences shorter than 31 tokens. Its performance drops when the sentence length increases, but is still substantially better than the random guess when the sentence length does not exceed 70. When the sentence length exceeds 70, both the RLSTM and RNN perform similarly."}, {"heading": "3.2 Experiment 2", "text": "In Experiment 1, it is not clear whether the tree size or the keyword depth is the main factor of the rapid drop of the RNN\u2019s performance. In this ex-\nperiment, we kept the tree size fixed and vary the keyword depth. We generated a pool of sentences of lengths from 21 to 30 tokens and parsed them by randomly generating binary trees. We then created 10 datasets each of which has 12k trees (10k for training, 1k for development, and 1k for testing). The i-th dataset consists of only trees in which distances from keywords to roots are i or i + 1 (to stop the networks from exploiting keyword depths directly).\nFigure 3 shows test accuracies of the two models on those 10 datasets. Similarly in Experiment 1, for each dataset we run each model 5 times and reported the highest accuracy for the RNN model, and the distribution of accuracies for the RLSTM model. As we can see, the RNN model achieves very high accuracies when the keyword depth does not exceed 3. Its performance then drops rapidly and gets close to the performance\nof the random guess. This is evidence that the RNN model has difficulty capturing long range dependencies. By contrast, the RLSTM model performs at above 90% accuracy until the depth of the keyword reaches 8. It has difficulty dealing with larger depths, but the performance is always better than the random guess."}, {"heading": "3.3 Experiment 3", "text": "We now examine whether the two models can encounter the vanishing gradient problem. To do so, we looked at the the back-propagation phase of each model in Experiment 1 on the third dataset (the one containing sentences of lengths from 21 to 30 tokens). For each tree, we calculated the ratio\n\u2016 \u2202J\u2202xkeyword \u2016\n\u2016 \u2202J\u2202xroot \u2016\nwhere the numerator is the norm of the error vector at the keyword node and the denominator is the norm of the error vector at the root node. This ratio gives us an intuition how the error signals develop when propagating backward to leaf nodes: if the ratio 1, the vanishing gradient problem occurs; else if the ratio 1, we observe the exploding gradient problem.\nFigure 4 reports the ratios w.r.t. the keyword node depth in each epoch of training the RNN model. The ratios in the first epoch are always very small. In each following epoch, the RNN model successfully lifts up the ratios steadily (see Figure 6a for a clear picture at the keyword depth\n10), but a clear decrease when the depth becomes larger is observable. For the RLSTM model (see Figure 5 and 6b), the story is somewhat different. The ratios go up after two epochs so rapidly that there are even some exploding error signals sent back to leaf nodes. They subsequently go down and remain stable with substantially less exploding error signals. This is, interestingly, concurrent with the performance of the RLSTM model on the development set (see Figure 6b). It seems that the RLSTM model, after one epoch, quickly locates the keyword node in a tree and relates it to the root by building a strong bond between them via error signals. After the correlation between the keyword and the label at the root is found, it tries to stabilize the training by reducing the error signals sent back to the keyword node. Comparing the two models by aligning Figure 4 with Figure 5, and Figure 6a with Figure 6b, we can see that the RLSTM model is more capable of transmitting error signals to leaf nodes.\nIt is worth noting that we do see the vanishing gradient problem happening when training the RNN model in Figure 4; but Figure 6a suggests that the problem can become less serious after a long enough training time. This might be because depth 10 is still manageable for the RNN model. (Notice that in the Stanford Sentiment Treebank, more than three quarters of leaf nodes are at depths less than 10.) The fact the the RNN model still doesnot perform better than random guessing can be explained using the arguments given by Bengio et al. (1994), who show that there is a trade-off\nbetween avoiding the vanishing gradient problem and capturing long term dependencies when training traditional recurrent networks."}, {"heading": "4 Conclusions", "text": "The experimental results show that the RLSTM is superior to the RNN in terms of overcoming the vanishing gradient problem and capturing long term dependencies. This is in parallel with general conclusions about the power of the LSTM architecture compared to traditional Recurrent neural networks. In future work we focus on more complex cases involving negation, composition, etc."}], "references": [{"title": "Learning long-term dependencies with gradient descent is difficult", "author": ["Yoshua Bengio", "Patrice Simard", "Paolo Frasconi."], "venue": "Neural Networks, IEEE Transactions on, 5(2):157\u2013166.", "citeRegEx": "Bengio et al\\.,? 1994", "shortCiteRegEx": "Bengio et al\\.", "year": 1994}, {"title": "Adaptive subgradient methods for online learning and stochastic optimization", "author": ["John Duchi", "Elad Hazan", "Yoram Singer."], "venue": "The Journal of Machine Learning Research, pages 2121\u20132159.", "citeRegEx": "Duchi et al\\.,? 2011", "shortCiteRegEx": "Duchi et al\\.", "year": 2011}, {"title": "Lstm recurrent networks learn simple context-free and context-sensitive languages", "author": ["Felix A Gers", "J\u00fcrgen Schmidhuber."], "venue": "Neural Networks, IEEE Transactions on, 12(6):1333\u20131340.", "citeRegEx": "Gers and Schmidhuber.,? 2001", "shortCiteRegEx": "Gers and Schmidhuber.", "year": 2001}, {"title": "Long short-term memory", "author": ["Sepp Hochreiter", "J\u00fcrgen Schmidhuber."], "venue": "Neural computation, 9(8):1735\u20131780.", "citeRegEx": "Hochreiter and Schmidhuber.,? 1997", "shortCiteRegEx": "Hochreiter and Schmidhuber.", "year": 1997}, {"title": "Compositional distributional semantics with long short term memory", "author": ["Phong Le", "Willem Zuidema."], "venue": "Proceedings of the Joint Conference on Lexical and Computational Semantics (*SEM). Association for Computational Linguistics.", "citeRegEx": "Le and Zuidema.,? 2015", "shortCiteRegEx": "Le and Zuidema.", "year": 2015}, {"title": "A recursive recurrent neural network for statistical machine translation", "author": ["Shujie Liu", "Nan Yang", "Mu Li", "Ming Zhou."], "venue": "Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1491\u2013", "citeRegEx": "Liu et al\\.,? 2014", "shortCiteRegEx": "Liu et al\\.", "year": 2014}, {"title": "Better word representations with recursive neural networks for morphology", "author": ["Minh-Thang Luong", "Richard Socher", "Christopher D Manning."], "venue": "CoNLL-2013, 104.", "citeRegEx": "Luong et al\\.,? 2013", "shortCiteRegEx": "Luong et al\\.", "year": 2013}, {"title": "Learning continuous phrase representations and syntactic parsing with recursive neural networks", "author": ["Richard Socher", "Christopher D. Manning", "Andrew Y. Ng."], "venue": "Proceedings of the NIPS-2010 Deep Learning and Unsupervised Feature Learning", "citeRegEx": "Socher et al\\.,? 2010", "shortCiteRegEx": "Socher et al\\.", "year": 2010}, {"title": "Parsing with compositional vector grammars", "author": ["Richard Socher", "John Bauer", "Christopher D Manning", "Andrew Y Ng."], "venue": "Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 455\u2013465.", "citeRegEx": "Socher et al\\.,? 2013a", "shortCiteRegEx": "Socher et al\\.", "year": 2013}, {"title": "Recursive deep models for semantic compositionality over a sentiment treebank", "author": ["Richard Socher", "Alex Perelygin", "Jean Y Wu", "Jason Chuang", "Christopher D Manning", "Andrew Y Ng", "Christopher Potts."], "venue": "Proceedings EMNLP.", "citeRegEx": "Socher et al\\.,? 2013b", "shortCiteRegEx": "Socher et al\\.", "year": 2013}, {"title": "Improved semantic representations from tree-structured long short-term memory networks", "author": ["Kai Sheng Tai", "Richard Socher", "Christopher D. Manning."], "venue": "Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics", "citeRegEx": "Tai et al\\.,? 2015", "shortCiteRegEx": "Tai et al\\.", "year": 2015}, {"title": "Long short-term memory over recursive structures", "author": ["Xiaodan Zhu", "Parinaz Sobhani", "Hongyu Guo."], "venue": "Proceedings of International Conference on Machine Learning, July.", "citeRegEx": "Zhu et al\\.,? 2015", "shortCiteRegEx": "Zhu et al\\.", "year": 2015}], "referenceMentions": [{"referenceID": 8, "context": "It has been employed to tackle several NLP tasks, such as syntactic parsing (Socher et al., 2013a), machine translation (Liu et al.", "startOffset": 76, "endOffset": 98}, {"referenceID": 5, "context": ", 2013a), machine translation (Liu et al., 2014), and word embedding learning (Luong et al.", "startOffset": 30, "endOffset": 48}, {"referenceID": 6, "context": ", 2014), and word embedding learning (Luong et al., 2013).", "startOffset": 37, "endOffset": 57}, {"referenceID": 3, "context": "In the recurrent neural network world, the long short term memory (LSTM) architecture (Hochreiter and Schmidhuber, 1997) is often used as a solution to these two problems.", "startOffset": 86, "endOffset": 120}, {"referenceID": 2, "context": "1 Introduction The recursive neural network (RNN) model became popular since the work of Socher et al. (2010). It has been employed to tackle several NLP tasks, such as syntactic parsing (Socher et al.", "startOffset": 89, "endOffset": 110}, {"referenceID": 2, "context": "In the recurrent neural network world, the long short term memory (LSTM) architecture (Hochreiter and Schmidhuber, 1997) is often used as a solution to these two problems. A natural extension of the LSTM can be defined for tree structures, which we call Recursive LSTM (RLSTM), as proposed independently by Tai et al. (2015), Zhu et al.", "startOffset": 87, "endOffset": 325}, {"referenceID": 2, "context": "In the recurrent neural network world, the long short term memory (LSTM) architecture (Hochreiter and Schmidhuber, 1997) is often used as a solution to these two problems. A natural extension of the LSTM can be defined for tree structures, which we call Recursive LSTM (RLSTM), as proposed independently by Tai et al. (2015), Zhu et al. (2015), and Le and Zuidema (2015).", "startOffset": 87, "endOffset": 344}, {"referenceID": 2, "context": "In the recurrent neural network world, the long short term memory (LSTM) architecture (Hochreiter and Schmidhuber, 1997) is often used as a solution to these two problems. A natural extension of the LSTM can be defined for tree structures, which we call Recursive LSTM (RLSTM), as proposed independently by Tai et al. (2015), Zhu et al. (2015), and Le and Zuidema (2015). However, while there is intensive research showing how the LSTM architecture can overcome those two problems compared to traditional recurrent models (e.", "startOffset": 87, "endOffset": 371}, {"referenceID": 2, "context": ", Gers and Schmidhuber (2001)), such research is, to our knowledge, still absent for the comparison between RNNs and RLSTMs.", "startOffset": 2, "endOffset": 30}, {"referenceID": 9, "context": "Using available annotated corpora such as the Stanford Sentiment Treebank (Socher et al., 2013b) and the Penn Treebank is thus inappropriate, as they are too small for this purpose (10k, 40k trees, respectively, compared to 240k trees in our experiments), and key nodes are not marked.", "startOffset": 74, "endOffset": 96}, {"referenceID": 11, "context": "F could be any LSTM that can combine two such concatenation vectors, such as Structure-LSTM (Zhu et al., 2015), Tree-LSTM (Tai et al.", "startOffset": 92, "endOffset": 110}, {"referenceID": 10, "context": ", 2015), Tree-LSTM (Tai et al., 2015), and LSTM-RNN (Le and Zuidema, 2015).", "startOffset": 19, "endOffset": 37}, {"referenceID": 4, "context": ", 2015), and LSTM-RNN (Le and Zuidema, 2015).", "startOffset": 22, "endOffset": 44}, {"referenceID": 4, "context": ", 2015), and LSTM-RNN (Le and Zuidema, 2015). In the current paper, we use the implementation1 of Le and Zuidema (2015).", "startOffset": 23, "endOffset": 120}, {"referenceID": 1, "context": "We trained the two models using the AdaGrad method (Duchi et al., 2011) with a learning rate of 0.", "startOffset": 51, "endOffset": 71}, {"referenceID": 6, "context": "Following Socher et al. (2013b), we used tanh as the activation function, and initialized word vectors by randomly sampling each value from a uniform distribution U(\u22120.", "startOffset": 10, "endOffset": 32}, {"referenceID": 0, "context": ") The fact the the RNN model still doesnot perform better than random guessing can be explained using the arguments given by Bengio et al. (1994), who show that there is a trade-off", "startOffset": 125, "endOffset": 146}], "year": 2016, "abstractText": "Recursive neural networks (RNN) and their recently proposed extension recursive long short term memory networks (RLSTM) are models that compute representations for sentences, by recursively combining word embeddings according to an externally provided parse tree. Both models thus, unlike recurrent networks, explicitly make use of the hierarchical structure of a sentence. In this paper, we demonstrate that RNNs nevertheless suffer from the vanishing gradient and long distance dependency problem, and that RLSTMs greatly improve over RNN\u2019s on these problems. We present an artificial learning task that allows us to quantify the severity of these problems for both models. We further show that a ratio of gradients (at the root node and a focal leaf node) is highly indicative of the success of backpropagation at optimizing the relevant weights low in the tree. This paper thus provides an explanation for existing, superior results of RLSTMs on tasks such as sentiment analysis, and suggests that the benefits of including hierarchical structure and of including LSTM-style gating are complementary.", "creator": "TeX"}}}