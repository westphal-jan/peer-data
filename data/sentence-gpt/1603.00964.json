{"id": "1603.00964", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "3-Mar-2016", "title": "Learning Tabletop Object Manipulation by Imitation", "abstract": "We aim to enable robot to learn tabletop object manipulation by imitation. Given external observations of demonstrations on object manipulations, we believe that two underlying problems to address in learning by imitation is 1) segment a given demonstration into skills that can be individually learned and reused, and 2) formulate the correct RL (Reinforcement Learning) problem that only considers the relevant aspects of each skill so that the policy for each skill can be effectively learned. Previous works made certain progress in this direction, but none has taken private information into account. Therefore, it is important to consider how the RL can be applied to our research and our knowledge about other systems and applications. We will address these issues in a separate project.\n\n\n\n\n\n\nThis article is part of a series of posts with the aim to explain what is being done by the RL in the field of robotic learning, the robotic-learning system, and related topics. We have been collecting and developing RL in the field of robotics, and have recently used it for research on the human intelligence, robotics, robotics, and human-computer interactions.\n\n\nIn 2014, I began a research project called Robot Science. It has been a part of my research on Robotics, Robotics, and Computer Evolution since 2012. It has included some examples from my work on robot biology, robotics, and human-computer interactions. In 2015, I started a project called Robots. It has been an ongoing project in the field of robotics and AI, with a goal of expanding the understanding of how AI learns from machines.\n\nSince its inception in 2010, robots have made extensive use of robotics in research, education, education, and other fields, such as robotics and robotics. Most of these are currently being used as learning, teaching, teaching and teaching robotics.\nTo provide the best results, the robots have been used for research on various systems, including robotics. However, these studies can only be done with our own training, which is often not possible with robotics. In particular, one can only compare the data collected in different countries and have a good idea of where the data collected for each skill and tool is.\nIn addition to studying, the current literature on the topic of artificial intelligence (AI) is divided into six parts:\nPrerequisites\nA robotic robot to learn and use the information collected from a human mind and is used to identify, train, and manage various machines in order to teach, train, and control human beings. To be taught, the robot will need to recognize, recognize,", "histories": [["v1", "Thu, 3 Mar 2016 03:49:02 GMT  (259kb,D)", "https://arxiv.org/abs/1603.00964v1", null], ["v2", "Fri, 26 Aug 2016 19:20:03 GMT  (259kb,D)", "http://arxiv.org/abs/1603.00964v2", null]], "reviews": [], "SUBJECTS": "cs.RO cs.AI", "authors": ["zhen zeng", "benjamin kuipers"], "accepted": false, "id": "1603.00964"}, "pdf": {"name": "1603.00964.pdf", "metadata": {"source": "CRF", "title": "Learning Tabletop Object Manipulation by Imitation", "authors": ["Zhen Zeng", "Benjamin Kuipers"], "emails": ["zengzhen@umich.edu,", "kuipers@umich.edu"], "sections": [{"heading": null, "text": "I. INTRODUCTION\nWhen a robot is presented with an unfamiliar object, the robot is not aware of what actions can cause what changes to the state of the object. Learning by imitation is an effective way for a robot to gain knowledge about possible useful actions on objects in its environment.\nGiven an observed behavior, usually formally defined as state variables describing the observed environment and action variables describing the observed actions, the robot should be able to learn a policy, which is a mapping between states and actions, such that it can select an action to execute based on its current state. RL methods are popular for policy learning, and the robot can improve its performance overtime through interaction with the world, thus we use RL methods in our work.\nTo learn a policy, we should first define a RL problem, usually formulated as an MDP (Markov Decision Process) with following components: the reward function, the action space, and the state space. The importance of formulating the\nThis work has taken place in the Intelligent Robotics Lab in the Computer Science and Engineering Division of the University of Michigan. Research of the Intelligent Robotics lab is supported in part by grants from the National Science Foundation (IIS-1111494 and IIS-1421168).\ncorrect RL problem, i.e., correctly defining each component is as explained below.\nWhat is the importance of correctly defining the reward function? For a robot to learn to effectively manipulate an unfamiliar object by imitating other\u2019s behavior, it is important for the robot to understand what to imitate, i.e., the goal of the behavior. For robotic experts, they can directly hand code the corresponding criteria to define the goal for the robot, but it is not realistic for most of the consumers of commercial robots to do so.\nThus it is important for the robot to automatically capture the goal of an observed behavior by defining the correct reward function. For example, if the goal of an observed behavior is to reach and grasp a cup, then the reward function can be defined as -1 everywhere except for a big positive reward when then cup is being grasped.\nWhat is the importance of correctly defining the state and action space? Think of the way we act in our daily lives, when we manipulate some objects of interest, we don\u2019t pay much attention to other objects in the environment. For example, if there are two blocks and a cup on a table, when we are stacking the two blocks, we don\u2019t really care about where the cup is; on the other hand, when we try to put one of the block into the cup, we don\u2019t care where the other block is. This reflects that even we are accessible to a bunch of information, we always abstract out the most important information relevant to our task by hand, and this is formally referred as abstraction in machine learning. RL problems that involve high-dimensional, continuous state and action space are difficult to solve, thus abstraction is a key to reduce the dimension of the problems. With the correct abstraction, the appropriate state and action space is defined for the RL problems.\nIn our work, we aim to find the abstraction that is able to 1) capture the important aspects of the behavior, and 2) chooses the correct reference frame (i.e., decides the target object). As in our experiments, if the observed behavior is hand reaching for a block with a basket as a noise object in the environment, then the block is the target object, the robot should be able to decide to abstract out the distance and angle of the hand in the block frame as the state space, and further RL methods can be applied with that state space.\nWhat if there are multiple RL problems involved? The observed behavior can underlie multiple policies, i.e., there are several skills involved in the behavior. For example, picking up a block, and then stacking it onto another one are two different skills. When the observed behavior contains multiple skills, the robot should be able to automatically segment the\nar X\niv :1\n60 3.\n00 96\n4v 2\n[ cs\n.R O\n] 2\n6 A\nug 2\n01 6\nbehavior into multiple pieces, each corresponds to a skill, and formulate different RL problems for each of them. The segmentation can be done based on how likely a temporal piece of the observed behavior is following the same policy, and this will be explained more in detail in later sections.\nWhat if there are important private information not accessible in the observed behavior? Given an externally observed behavior, private information such as tactile sensations are not available to the robot. If human demonstrates to rotate a jar\u2019s lid to open it, the observed behavior will be the hand rotating together with the jar\u2019s lid until it is open. With lack of private information, the robot cannot decide whether it is important or not by just observing the behavior. Once the robot understands the goal of the behavior, and selects the correct abstraction, i.e. the angles of the hand and lid w.r.t the frame of the jar\u2019s body in this case, it can use appropriate RL methods to learn a policy by temporarily assuming that the private information does not matter. As the robot is just grasping the jar\u2019s lid without controlling the holding force, the robot could fail to learn a policy to reliably open the jar\u2019s lid. Then the robot will propose to expand the state space in the original RL problem to include the private information, i.e., tactile sensation, and expand the action space to include the controller on holding force, then restart the learning based on the not-so-good policy learned previously.\nIn sum, given an observed behavior, it is important to segment it into multiple pieces (if there are multiple skills involved), and formulate the correct RL problem for each piece such that RL methods can be further applied to learn policies. One of the important step made in this direction is by Konidaris et al. [1], they provided an elegant approach for behavior segmentation, and abstraction selection for formulating the RL problems. We use a similar approach, and we have contributed in two new directions: \u2022 We provide a method to online select the appropriate\nabstraction such that 1) the observed behavior is captured, and 2) the correct reference frame and relevant objects are chosen. \u2022 When private information is not available in the observed\nbehavior, the robot is able to decide whether the private information is important, and reformulate the RL problems if needed.\nNext, section II will discuss some related works, section III, IV introduce notations and terminologies used in this paper, and then section V explains the methods involved in our framework, and section VI discuss our experiment setup, and the evaluation results."}, {"heading": "II. RELATED WORK", "text": ""}, {"heading": "A. Understanding Manipulation Behavior", "text": "A potentially useful manipulation behavior representation for artificial agents should satisfy several criteria. As pointed out in [2], the representation needs to be based on sensory signals and learnable by observation. From the point of view of learning by imitation, the representation should also satisfy that (1) it is not redundant in the sense that it should not encode information that exists only within certain observed behaviors,\nsuch as specific motion trajectories of human arm joints and objects; (2) it should be simple such that it can be easily interpreted as a roadmap that guide an agent to act. Previous works on human manipulation actions recognition have attempted to represent manipulation behavior in a probabilistic manner. Human poses, human-object context, and object-object context have been considered to solve the problem jointly as in [3]. Similarily in works by Kjellstrom et al.[4], pre-defined handobject features and manipulation features are extracted, and the semantic manipulation action-object dependencies are learned based on CRFs. Their representation of manipulation behavior is powerful in that they can recognize manipulation actions as well as object categories. Although previous works are robust in manipulation action recognition when presented by various view points and even occlusions, those representations of the behavior cannot be directly translated into any step by step guidance, or a roadmap that an agent can follow for effective imitation. Works by Aksoy\u2019s group [2] revealed an effective way to represent manipulation behavior. By observing different human demonstrations of the same manipulation behavior, they discovered that the manipulator, i.e. hand, and objects movement trajectory may vary, but there are certain moments that the spatial relations between the hand and objects are similar or identical across all the demonstrations, and these moments are referred as decisive moments. They introduced Semantic Event Chain (SEC) as a novel, generic representation of manipulation behavior, where they encoded the spatial relations between the manipulator and objects only at decisive moments."}, {"heading": "B. Learning by Imitation", "text": "Some previous works learn movements by imitating joint trajectories [5][6][7]. They mounted sensors on human body, and recorded the joint angles during the demonstrations to teach humanoid robot drumming and walking patterns. While in our work, the robot needs to learn a manipulation task, directly imitating exact joints trajectories for manipulation tasks won\u2019t generalize well. Two completely different sequences of joints trajectories can be performing the same manipulation task. For example, when we pick up an object, we can approach it from various angles and along various joint trajectories, but all these trajectories correspond the same manipulation task. Thus what really matters in the learning of a manipulation task is to capture and imitate the important aspects of the behavior rather than imitating the quantitative joints trajectories. For example, the important aspects of picking up an object are the hand needs to approach and get in contact with the object first, then grasp it to lift it up, no matter what the joints trajectories are. Another problem of imitating the joints trajectories is the corresponding issue. The corresponding issue is understood as the identification of a mapping from the demonstrator to the robot that allows the transfer of information. For example, in the case of robot learning to walk by observing human joint angles during demonstrations, before it can imitate the walking pattern, the robot needs to know the mapping between the human joint angles and its own joint angles.\nThere have been some works that directly learn the mapping from state to action. In robot domain, the state space is usually continuous, and the action space can be continuous or discrete. Continuous action space can be composed by available sensor values, such as moving speed, joint angular speed, and torque. Discrete action space can be composed by discrete low level incremental actions, such as move forward by 0.1 meter for a mobile robot, or composed by discrete high level primitive actions, such as reach, grasp an object for a manipulator.\nWhen the action space is discrete, the problem of learning the mapping is essentially a classification problem. For example, Chernova et al. [8][9] learns to navigate through corridors by observing the behavior generated by expert teleoperation. In their work, the states are continuous variables describing the distances of the closets walls, and the actions are discrete variables corresponding to pre-defined controllers that drives the robot forward, to the left, to the right, and u-turn. The mapping from state to action is learned based on GMM. Similar works have been focused on high-level primitive actions such as hand gestures, for learning box and ball sorting tasks [10][11]. When the action space is continuous, the problem of learning the mapping is then a regression problem. Grollman et al. [12] has applied locally weighted projection regression to soccer skill learning task on an AIBO robot.\nOur work focus on automatically formulating RL problems for the skills observed in demonstration, and solving the RL problems result in good policies that can generate behaviors that imitate the observed demonstration. The most similar work is by Konidaris et al. [1]. There are also some works that focus on learning the correspondence, i.e., the mapping between the observed states/actions and the robot state/actions. In our work, we assume that the correspondence is known. For more details on works that solves the correspondence issue, please refer to this survey[13]."}, {"heading": "III. NOTATIONS", "text": "First of all, S is the overall state space, and it divides into the overall public state space Spublic and the overall private state space Sprivate. In Spublic, each dimension is a public state variable whose value can be externally observed, such as the x coordinate of an object center, and a boolean state variable indicating whether the hand is open or not. In Sprivate, each dimension is a private state variable whose value cannot be externally observed, such as tactile sensations on the fingers.\nThe robot observes a sequence of states sampled at the frame rate,\nO = {s0, s1, \u00b7 \u00b7 \u00b7 , sfinal}\nwhere si \u2208 Spublic. More specifically, si is a vector composed of the pose vectors of objects in workspace, and the pose vector of hand,\nsi = [P o1 i , P o2 i , \u00b7 \u00b7 \u00b7 , P h i ] T\nwhere P oji is the pose vector of the jth visible object at ith sampled frame, and Phi is the pose vector of the hand at ith sampled frame. All the pose vectors in the observations are in the world frame, they can be transformed into any object frame or the hand frame given the object or hand pose.\nRobot action space A is composed by hand movements (translation and rotation in 3D), open and close gripper (with a default force), open and close gripper with commanded force. Given the observation, we define the action ai \u2208 A taken at state si to be\nai = [d(P h i , P h i+1), open(\u03b1)] T\nwhere function d returns the displacement between the hand poses Phi and P h i+1 and open(\u03b1) defines the opening or closing grippers with force \u03b1 (positive for opening, negative for closing, and zero for doing nothing). Note that the force \u03b1 in action open(\u03b1) is not externally observable, thus \u03b1 should be learned by robot explorations if that is considered an important aspect of the demonstrated behavior.\nDemonstrations on tabletop object manipulations can involve multiple skills, for example, grasp and pick up a cup, the series of actions can be segmented into three individual skills: first, approach the cup; second, grasp the cup; third, lift the cup. Each skill is formally defined as an option o, as introduced in [14]. An option includes three components: 1) an option policy \u03c0o(s,a) which gives the probability of executing each action in each state in which the option is defined; 2) an initiation set indicator function Io(s) which gives 1 for states where the option can be executed and 0 elsewhere; 3) termination condition \u03b2o(s) which gives the probability of option execution terminating in states where it is defined.\nWhen a demonstration is segmented into a sequence of skills, and each skill is represented as an option, the reward can be synthesized as negative at each step of action and positive when the option terminates. In this way, the goal of each skill is embedded in the termination condition, and can be captured by the synthesized rewards, as a simple instance of inverse reinforcement learning methods. These synthesized rewards are reasonable since many tabletop object manipulations involve skills with its own ending goal. If the real reward function is more complex than that, other inverse reinforcement learning methods [15] can be applied to infer the reward function from the demonstration.\nThe advantage of breaking the demonstration into a sequence of skills and learn the corresponding option is that: a) learned options can be reused in other tasks; b) to learn the option policy for each option, RL problem can be formulated with the state and action space composed by the most relevant state variables (can contain both public and private state variables) and action variables, instead of the overall state and action space.\nWe design a library of abstractions, where each abstraction M is a pair of functions \u3008\u03c3M , \u03c4M \u3009, where\n\u03c3M : S 7\u2192 SM\nis a state abstraction that maps the overall state space S to a smaller state space SM , and\n\u03c4M : A 7\u2192 AM\nis an action abstraction that maps the full action space A to a smaller action space AM . In our work, the abstracted space SM and AM are subset of S and A, with variables possibly\ndescribed in a different reference frame rather than the world frame.\nTo formulate the correct RL problem for each segmented skill, the pair of state and action abstraction M that involves the more important aspects of the skill should be selected."}, {"heading": "IV. LINEAR VALUE FUNCTION APPROXIMATION", "text": "The value function V maps a given state vector to expected return, and it can be approximated as a linear combination of a given set of basis functions \u03a6 = {\u03c61, \u00b7 \u00b7 \u00b7 , \u03c6k},\nV\u0302 (s) = k\u2211 i=1 wi\u03c6i(s)\nwhere the basis function are Fourier basis, a generic basis that generally exhibits good performance [16]. The zth order Fourier basis for d state variables in the set of basis function are defined as\n\u03c6i(s) = cos(\u03c0c i \u00b7 s)\nwhere ci = [c1, \u00b7 \u00b7 \u00b7 , cd], cj \u2208 [0, \u00b7 \u00b7 \u00b7 , z] with 1 \u2264 j \u2264 d. The set of basis functions is obtained by systematically varying the coefficients cj .\nFrom the synthesized rewards we can obtain a Monte Carlo sample of expected return from each appeared state s,\nR(s) = n\u2211 i=0 \u03b3iri\nwhere \u03b3 is the discount factor, and ri is the synthesized reward. And R(s) is the regression target for V\u0302 (s) when we are trying to approximate the value function of the underlying policy in the demonstration, with a given set of Fourier basis functions.\nEach abstraction M in the abstraction library has an associated set of Fourier basis functions \u03a6M defined over SM . Therefore, abstraction selection amounts to selecting the set of basis functions that can best represent the value function inferred from the demonstration."}, {"heading": "V. METHOD", "text": "We want to break the demonstration into multiple skills when it consist of underlying policies that use different abstractions, or it consists of policies that are too complex to be approximated using a single function approximator. Changepoint detection algorithm can be applied in this case to find the boundaries of each skill and select the best abstraction for each skill."}, {"heading": "A. Changepoint Detection", "text": "First, I\u2019ll introduce the statistical changepoint detection in a general regression setting. Given observed data and a set of candidate models Q, we assume that the data are sequentially generated by an instance of a single model, occasionally switching to a different model or switching to a different instance of the same model at certain points in time, called changepoints. The goal is to infer the number and positions of the changepoints and select an appropriate model instance for each segment.\nAn efficient changepoint detection algorithm was introduced by Fearnhead and Liu [17] that obtains the MAP changepoints and models via an online Viterbi algorithm: given data tuples (xt, yt) observed for times t \u2208 [1, 2, \u00b7 \u00b7 \u00b7 , T ], and a set of candidate models Q with prior p(q) for each q \u2208 Q. The marginal probability of a segment length l is modeled with probability mass function g(l) and cumulative mass function G(l) = \u2211 i = 1\nlg(l). And a segment from time j+ 1 to t can be fit using model q to obtain P (j, t, q), the probability of the data segment conditioned on q. Functions g(l) and P (j, t, q) is either given as a prior knowledge or pre-learned based on some training data.\nThis results in a HMM where the hidden state at time t is the model qt and the observed data is yt given xt, as shown in Figure 1. The transition from model qi to qj occurs with probability\nT (qi, qj) = g(j \u2212 i\u2212 l)p(qj)\nThe emission probability of an observed data segment starting at time t+ 1 and continuing through j using model q is given by\nP (yi+1 : y|q) = P (i, j, q)(1\u2212G(j \u2212 i\u2212 1))\nAn online Viterbi algorithm can be used to compute Pt(j, q), the probability of the changepoint previous to time t occurring at time j using model q (i.e., from time j + 1 to t the data is generated using model q) is\nPt(j, q) = (1\u2212G(t\u2212 j \u2212 1))P (i, j, q)p(q)PMAPj\nwhere PMAPj is the probability of the MAP changepoint at time j,\nPMAPj = maxi,q Pj(i, q)g(j \u2212 i)\n1\u2212G(j \u2212 i\u2212 1)\nThus at each time t, the algorithm computes Pt(j, q) for each model q and changepoint time j < t (using PMAPj ) and then computes and stores PMAPt . As P MAP t being recursively calculated, the MAP changepoint positions and models are stored. When PMAPT is calculated for the complete observed data sequence, the MAP changepoint positions and models for generating the observed data are identified as a result."}, {"heading": "B. Demonstration Segmentation and Abstraction Selection", "text": "Our framework for simultaneous demonstration segmentation and abstraction selection is as shown in Figure 2. We apply the changepoint detection algorithm to segment the demonstration at time points where the abstraction changes and select the appropriate abstraction for each segment such that, 1) the observed behavior can be effectively captured by the basis functions associated with the selected abstraction; 2) the observed trajectory in the state space is simple. By default, only public state and action variables are considered, if the RL problem formulated with the abstracted public state and action space cannot reproduce the observed behavior, we will re-formulate the RL problem by adding in private state and action variables.\nThe set of candidate models Q is composed by the sets of basis functions \u03a6M associated with each abstraction M , and R(st) at time t is the target variable yt. For the changepoint detection algorithm to work well, an appropriate model of expected segment length and an appropriate model for fitting the data are required. As introduced by Konidaris et al. [1], it is suggested to assume a geometric distribution for skill lengths with parameter p, so that\ng(l) = (1\u2212 p)l\u22121p\nG(l) = 1\u2212 (1\u2212 p)l\nand this provides a natural way to set p via k = 1/p, the expected skill length.\nA linear regression model with Gaussian noise is assumed to be the model of observed data, and the Gaussian noise prior has mean zero and an inverse gamma variance prior with parameters v/2 and u/2. The prior for each weight is a zeromean Gaussian with variance \u03c32\u03b4. The probability of the data segment from time j + 1 to t conditioned on abstraction q is\nP (j, t, q) = pV (j, t, q)ptraj(j, t, q)\nwhere PV (j, t, q) measures how well can the basis functions associated with model q approximate the value function V , and\nPtraj(j, t, q) measures how simple the observed trajectory is in the abstracted state space,\nPV (j, t, q) = \u03c0 n 2\n\u03b4m/2 |(Aq + D)\u22121| 1 2\nu v 2\n(yq + u) n+v 2\n\u0393(n+v2 )\n\u0393( v2 )\nPtraj(j, t, q) = N\u220f k=1 \u03c0 n 2 \u03b4m/2 |(A+D)\u22121| 12 u v 2 (yok + u) n+v 2 \u0393(n+v2 ) \u0393( v2 )\nwhere n = t\u2212j\u22121, q has m basis functions, \u0393 is the Gamma function, D is an m\u00d7m matrix with \u03b4\u22121 on the diagonal and zeros elsewhere, N is the number of relevant object indicated by the selected state abstraction. And\nAq = t\u2211 i=k+1 \u03a6q(si)\u03a6q(si) T\nyq = t\u2211 i=j R2i \u2212 bTq (A+q D)\u22121bq\nA = t\u2211\ni=k+1\n[1, 1\nt\u2212 j ,\n1\n(t\u2212 j)2 ]T [1,\n1\nt\u2212 j ,\n1\n(t\u2212 j)2 ]\nyok = t\u2211 i=j dok(si) 2 \u2212 bok T (Atraj + D)\u22121bok\nwhere \u03a6q(xi) is a vector of m basis functions associated with model q evaluated at state si, Ri = \u2211t j=i \u03b3\nj\u2212irj is the accumulated reward sample obtained from state si, and bq = \u2211t i=j Ri\u03a6q(si), bok = \u2211t i=j dok(si)[1, 1 t\u2212j , 1 (t\u2212j)2 ]\nT , dok(si) is the distance between the end effector of the agent and selected relevant object ok.\nUsing the probability functions defined above, the changepoint detection algorithm can segment the demonstration into multiple skills with selected abstraction. Once the RL problem is formulated for each skill, an initial policy can be learned based on the demonstration and the robot can improve upon the initial policy using appropriate RL methods. In our experiments, we use Sarsa(\u03bb) learning as our RL method, and if the policy learned after maximum number\nof trials cannot successfully reproduce the observed behavior, our framework handles that by automatically reformulate RL problem by adding in private state and action variables, e.g. tactile sensation state variable and gripping opening/closing action in our experiment."}, {"heading": "VI. EXPERIMENT", "text": ""}, {"heading": "A. Shape Sorter Game", "text": "Our motivating example is the shape sorter game, as shown in Figure 3(a), the goal is to pick up a block and align it with the corresponding shape hole and push it into the box. Now let\u2019s assume there are only the box and the green block within the visible field, as shown in Figure 3(b), and the demonstrator demonstrates the inserting the green block into the box.\nThe overall public state space will contain the pose of the block P block, the box P box and the hand Phand in a world frame. The overall private state space will contain the private tactile sensation on the left and the right fingers \u03bbl, \u03bbr. The action space is as introduced earlier. There are three coordinate frames available in this case, i.e., the world frame, the block frame, and the box frame."}, {"heading": "B. The Abstraction Library", "text": "We aim to design an abstraction library that is sufficient for the tabletop object manipulations such as assembly tasks, such that the proper abstraction exists for any tabletop object manipulation skill that the robot may decide to learn.\nWe define the mapping functions \u3008\u03c3M , \u03c4M \u3009 of an abstraction M by two sets of parameters \u3008P\u03c3,P\u03c4 \u3009:\nP\u03c3 = [IL, IO, I\u039b, Io1 , Io2 , \u00b7 \u00b7 \u00b7 , Ij , C]\nP\u03c4 = [IT , IR, IF ]\nwhere each parameter is a binary digit. In P\u03c4 , if IT=1, then hand translation action is included in the abstracted action space AM ; if IT = 0, then it is not included in AM . Similarly, IR indicates whether hand rotation action is included in AM , and I\u039b indicates whether opening/closing with commanded force is included in AM . In P\u03c3 , IL, IO, I\u039b indicate whether location variables, orientation variables, and tactile sensations are included in SM , and Io1 , Io2 , \u00b7 \u00b7 \u00b7 , Ih indicate whether information of objects o1, o2, \u00b7 \u00b7 \u00b7 and hand h are included in SM , and C indicates the reference frame in which the information should be represented. For example, P\u03c3 = [IL =\n1, IO = 0, I\u039b = 0, Iblock = 1, Ibox = 0, Ih = 1, C = Cbox] indicates that \u03c3M maps S to SM , where the state variables are locations of the block and hand expressed in the box frame.\nNote that when only hand translation action is included in A, i.e., P\u03c4 = [1, 0, 0], the state variables that the action can affect are the location variables, thus it is reasonable to constrain SM to include only location variables in this case, i.e., IL, IO, I\u039b = [1, 0, 0]. And the same principle applies to P\u03c4 = [0, 1, 0], P\u03c4 = [0, 0, 1], P\u03c4 = [1, 1, 0] and so on. As a result,\n[Il, IO,\u039b ] = [IT , IR, IF ] (1)\nis a rule of thumb for designing the abstraction library. By enumerating all possible pairs of parameters \u3008P\u03c3,P\u03c4 \u3009 under the constraint 1, the abstraction library is built with the resulting mapping functions \u3008\u03c3M , \u03c4M \u3009. The size of the abstraction library is 2|P |\u03c3\u22121K, where K is the number of available coordinate frames."}, {"heading": "C. Simplified 2D Manipulation Domain", "text": "Our toy example that captures the important properties in our motivating example is a 2D manipulation domain, where a robot, block, and a basket is involved. The observed behavior is the hand (or robot) reaches the block first and then translates it into the basket, as shown in Figure 4."}, {"heading": "D. Evaluation", "text": "Given the external observations of a demonstration as shown in Figure 5,6, and assuming reasonable perception errors, the evaluation is mostly concerned of 1) whether the demonstration is reasonably segmented into multiple skills, and 2) whether the correct RL problem is formulated for each segmented skill, including whether the private information is considered in the RL formulation when it is actually important. Videos of the robot learning to reproduce the observed behavior are available at https://www.dropbox.com/ s/eltjxl0solevnw0/QUAL2_ZHEN_slides.pptx?dl=0.\nGiven two different sample trajectories, the segmentation results were successful. And the selected abstraction is also correct. In both experiments, for the 1st segment, the abstraction is selected as the robot distance and angle w.r.t the block frame; for the 2nd segment, the abstraction is selected as robot and block distance and angle w.r.t the basket frame.\nAnd the RL problem initially formulated for the 2nd segment only involves public state and action variables, which is incorrect, since to carry the block to the destination basket, the robot needs to incorporate holding action. Thus the robot failed to reproduce the observed behavior in the 2nd segment,\nwhich result in reformulation of the RL problem by adding tactile state variable into originally selected SM , and adding holding action into originally selected AM . Then Sarsa(\u03bb) is applied again in the reformualted RL problem, when the policy converged, the robot learned to exert enough holding force onto the block while carrying it to the basket."}, {"heading": "VII. CONCLUSION", "text": "We show that based on our framework, given observed behavior, we can segment the behavior into multiple skills if needed, and select the appropriate abstraction such that 1) the observed behavior is captured, and 2) the correct reference frame and relevant objects are chosen. And when private information is not available in the observed behavior, the robot is able to decide whether the private information is important, and reformulate the RL problems if needed."}], "references": [{"title": "Robot learning from demonstration by constructing skill trees", "author": ["G. Konidaris", "S. Kuindersma", "R. Grupen", "A. Barto"], "venue": "The International Journal of Robotics Research, p. 0278364911428653, 2011.", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2011}, {"title": "Learning the semantics of object\u2013action relations by observation", "author": ["E.E. Aksoy", "A. Abramov", "J. D\u00f6rr", "K. Ning", "B. Dellen", "F. W\u00f6rg\u00f6tter"], "venue": "The International Journal of Robotics Research, p. 0278364911410459, 2011.", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2011}, {"title": "Learning human activities and object affordances from rgb-d videos", "author": ["H.S. Koppula", "R. Gupta", "A. Saxena"], "venue": "The International Journal of Robotics Research, vol. 32, no. 8, pp. 951\u2013970, 2013.", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2013}, {"title": "Simultaneous visual recognition of manipulation actions and manipulated objects", "author": ["H. Kjellstr\u00f6m", "J. Romero", "D. Mart\u00ednez", "D. Kragi\u0107"], "venue": "Computer Vision\u2013ECCV 2008, pp. 336\u2013349, Springer, 2008.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2008}, {"title": "Learning rhythmic movements by demonstration using nonlinear oscillators", "author": ["A.J. Ijspeert", "J. Nakanishi", "S. Schaal"], "venue": "Proceedings of the ieee/rsj int. conference on intelligent robots and systems (iros2002), no. BIOROB-CONF-2002-003, pp. 958\u2013963, 2002.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2002}, {"title": "Learning from demonstration and adaptation of biped locomotion", "author": ["J. Nakanishi", "J. Morimoto", "G. Endo", "G. Cheng", "S. Schaal", "M. Kawato"], "venue": "Robotics and Autonomous Systems, vol. 47, no. 2, pp. 79\u2013 91, 2004.", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2004}, {"title": "Movement imitation with nonlinear dynamical systems in humanoid robots", "author": ["A.J. Ijspeert", "J. Nakanishi", "S. Schaal"], "venue": "Robotics and Automation, 2002. Proceedings. ICRA\u201902. IEEE International Conference on, vol. 2, pp. 1398\u20131403, IEEE, 2002.", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2002}, {"title": "Confidence-based policy learning from demonstration using gaussian mixture models", "author": ["S. Chernova", "M. Veloso"], "venue": "Proceedings of the 6th international joint conference on Autonomous agents and multiagent systems, p. 233, ACM, 2007.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2007}, {"title": "Interactive policy learning through confidence-based autonomy", "author": ["S. Chernova", "M. Veloso"], "venue": "Journal of Artificial Intelligence Research, vol. 34, no. 1, p. 1, 2009.", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2009}, {"title": "Interactive task training of a mobile robot through human gesture recognition", "author": ["P.E. Rybski", "R.M. Voyles"], "venue": "Robotics and Automation, 1999. Proceedings. 1999 IEEE International Conference on, vol. 1, pp. 664\u2013669, IEEE, 1999.", "citeRegEx": "10", "shortCiteRegEx": null, "year": 1999}, {"title": "Teaching multi-robot coordination using demonstration of communication and state sharing", "author": ["S. Chernova", "M. Veloso"], "venue": "Proceedings of the 7th international joint conference on Autonomous agents and multiagent systems-Volume 3, pp. 1183\u20131186, International Foundation for Autonomous Agents and Multiagent Systems, 2008.", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2008}, {"title": "Learning robot soccer skills from demonstration", "author": ["D.H. Grollman", "O.C. Jenkins"], "venue": "Development and Learning, 2007. ICDL 2007. IEEE 6th International Conference on, pp. 276\u2013281, IEEE, 2007.", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2007}, {"title": "A survey of robot learning from demonstration", "author": ["B.D. Argall", "S. Chernova", "M. Veloso", "B. Browning"], "venue": "Robotics and autonomous systems, vol. 57, no. 5, pp. 469\u2013483, 2009.", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2009}, {"title": "Between mdps and semi-mdps: Learning, planning, and representing knowledge at multiple temporal scales", "author": ["R.S. Sutton"], "venue": "1998.", "citeRegEx": "14", "shortCiteRegEx": null, "year": 1998}, {"title": "Apprenticeship learning via inverse reinforcement learning", "author": ["P. Abbeel", "A.Y. Ng"], "venue": "Proceedings of the twenty-first international conference on Machine learning, p. 1, ACM, 2004.", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2004}, {"title": "Value function approximation in reinforcement learning using the fourier basis", "author": ["G. Konidaris"], "venue": "2008.", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2008}, {"title": "On-line inference for multiple changepoint problems", "author": ["P. Fearnhead", "Z. Liu"], "venue": "Journal of the Royal Statistical Society: Series B (Statistical Methodology), vol. 69, no. 4, pp. 589\u2013605, 2007.", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2007}], "referenceMentions": [{"referenceID": 0, "context": "[1], they provided an elegant approach for behavior segmentation, and abstraction selection for formulating the RL problems.", "startOffset": 0, "endOffset": 3}, {"referenceID": 1, "context": "out in [2], the representation needs to be based on sensory signals and learnable by observation.", "startOffset": 7, "endOffset": 10}, {"referenceID": 2, "context": "Human poses, human-object context, and object-object context have been considered to solve the problem jointly as in [3].", "startOffset": 117, "endOffset": 120}, {"referenceID": 3, "context": "[4], pre-defined handobject features and manipulation features are extracted, and the semantic manipulation action-object dependencies are learned based on CRFs.", "startOffset": 0, "endOffset": 3}, {"referenceID": 1, "context": "Works by Aksoy\u2019s group [2] revealed an effective way to represent manipulation behavior.", "startOffset": 23, "endOffset": 26}, {"referenceID": 4, "context": "Some previous works learn movements by imitating joint trajectories [5][6][7].", "startOffset": 68, "endOffset": 71}, {"referenceID": 5, "context": "Some previous works learn movements by imitating joint trajectories [5][6][7].", "startOffset": 71, "endOffset": 74}, {"referenceID": 6, "context": "Some previous works learn movements by imitating joint trajectories [5][6][7].", "startOffset": 74, "endOffset": 77}, {"referenceID": 7, "context": "[8][9] learns to navigate through corridors by observing the behavior generated by expert teleoperation.", "startOffset": 0, "endOffset": 3}, {"referenceID": 8, "context": "[8][9] learns to navigate through corridors by observing the behavior generated by expert teleoperation.", "startOffset": 3, "endOffset": 6}, {"referenceID": 9, "context": "Similar works have been focused on high-level primitive actions such as hand gestures, for learning box and ball sorting tasks [10][11].", "startOffset": 127, "endOffset": 131}, {"referenceID": 10, "context": "Similar works have been focused on high-level primitive actions such as hand gestures, for learning box and ball sorting tasks [10][11].", "startOffset": 131, "endOffset": 135}, {"referenceID": 11, "context": "[12] has applied locally weighted projection regression to soccer skill learning task on an AIBO robot.", "startOffset": 0, "endOffset": 4}, {"referenceID": 0, "context": "[1].", "startOffset": 0, "endOffset": 3}, {"referenceID": 12, "context": "For more details on works that solves the correspondence issue, please refer to this survey[13].", "startOffset": 91, "endOffset": 95}, {"referenceID": 13, "context": "Each skill is formally defined as an option o, as introduced in [14].", "startOffset": 64, "endOffset": 68}, {"referenceID": 14, "context": "If the real reward function is more complex than that, other inverse reinforcement learning methods [15] can be applied to infer the reward function from the demonstration.", "startOffset": 100, "endOffset": 104}, {"referenceID": 15, "context": "where the basis function are Fourier basis, a generic basis that generally exhibits good performance [16].", "startOffset": 101, "endOffset": 105}, {"referenceID": 0, "context": "HMM for changepoint detection [1].", "startOffset": 30, "endOffset": 33}, {"referenceID": 16, "context": "An efficient changepoint detection algorithm was introduced by Fearnhead and Liu [17] that obtains the MAP changepoints and models via an online Viterbi algorithm: given data tuples (xt, yt) observed for times t \u2208 [1, 2, \u00b7 \u00b7 \u00b7 , T ], and a set of candidate models Q with prior p(q) for each q \u2208 Q.", "startOffset": 81, "endOffset": 85}, {"referenceID": 0, "context": "[1], it is suggested to assume a geometric distribution for skill lengths with parameter p, so that", "startOffset": 0, "endOffset": 3}, {"referenceID": 0, "context": ", P\u03c4 = [1, 0, 0], the state variables that the action can affect are the location variables, thus it is reasonable to constrain SM to include only location variables in this case, i.", "startOffset": 7, "endOffset": 16}, {"referenceID": 0, "context": ", IL, IO, I\u039b = [1, 0, 0].", "startOffset": 15, "endOffset": 24}, {"referenceID": 0, "context": "And the same principle applies to P\u03c4 = [0, 1, 0], P\u03c4 = [0, 0, 1], P\u03c4 = [1, 1, 0] and so on.", "startOffset": 39, "endOffset": 48}, {"referenceID": 0, "context": "And the same principle applies to P\u03c4 = [0, 1, 0], P\u03c4 = [0, 0, 1], P\u03c4 = [1, 1, 0] and so on.", "startOffset": 55, "endOffset": 64}, {"referenceID": 0, "context": "And the same principle applies to P\u03c4 = [0, 1, 0], P\u03c4 = [0, 0, 1], P\u03c4 = [1, 1, 0] and so on.", "startOffset": 71, "endOffset": 80}, {"referenceID": 0, "context": "And the same principle applies to P\u03c4 = [0, 1, 0], P\u03c4 = [0, 0, 1], P\u03c4 = [1, 1, 0] and so on.", "startOffset": 71, "endOffset": 80}], "year": 2016, "abstractText": "We aim to enable robot to learn tabletop object manipulation by imitation. Given external observations of demonstrations on object manipulations, we believe that two underlying problems to address in learning by imitation is 1) segment a given demonstration into skills that can be individually learned and reused, and 2) formulate the correct RL (Reinforcement Learning) problem that only considers the relevant aspects of each skill so that the policy for each skill can be effectively learned. Previous works made certain progress in this direction, but none has taken private information into account. The public information is the information that is available in the external observations of demonstration, and the private information is the information that are only available to the agent that executes the actions, such as tactile sensations. Our contribution is that we provide a method for the robot to automatically segment the demonstration into multiple skills, and formulate the correct RL problem for each skill, and automatically decide whether the private information is an important aspect of each skill based on interaction with the world. Our motivating example is for a real robot to play the shape sorter game by imitating other\u2019s behavior, and we will show the results in a simulated 2D environment that captures the important properties of the shape sorter game. The evaluation is based on whether the demonstration is reasonably segmented, and whether the correct RL problems are formulated. In the end, we will show that robot can imitate the demonstrated behavior based on learned policies.", "creator": "LaTeX with hyperref package"}}}