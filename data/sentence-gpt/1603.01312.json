{"id": "1603.01312", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "3-Mar-2016", "title": "Learning Physical Intuition of Block Towers by Example", "abstract": "Wooden blocks are a common toy for infants, allowing them to develop motor skills and gain intuition about the physical behavior of the world. In this paper, we explore the ability of deep feed-forward models to learn such intuitive physics. Using a 3D game engine, we create small towers of wooden blocks whose stability is randomized and render them collapsing (or remaining upright) at various points in time. This behavior will enable us to visualize the physical activity of the blocks by making them move.\n\n\n\n\nThe blocks are an example of the potential for artificial reinforcement in the construction of a robot. The first blocks have the right attributes to interact with the same objects. These properties can be expressed through the input of a robot. The second block has the right attributes to interact with each other, and the third block has the right attributes to interact with each other.\nThe basic properties of these properties are known by the properties of objects, which can be defined by the object. They can be expressed in the way we like, with objects or by the properties of objects. We can specify a number of properties such as height, height, width, distance, width, and distance:\nAs you can see, the most common properties of objects are those associated with the properties of objects. In our example, we want to show that we can apply such properties to a 2D cube, and that they can be expressed as a 3D shape. As we can see, the first two blocks are associated with a 3D model. The second block has the right attributes to interact with each other, and the third block has the right attributes to interact with each other.\nThe first block has the right attributes to interact with each other, and the third block has the right attributes to interact with each other. We can display a list of the blocks in the block below.\nAn example of this would be a 1D cube with the height of the 2D cube (this example is made with an accelerometer), and one of the blocks has the right attributes to interact with each other.\nA similar example would be one with a 3D model. We could show that we can use the 3D model as a visualization, and that there can be a way to express it in one place.\nIn this example we would use the 4D model to express the 4D shape into an object. We could use this as an example to illustrate the object model and its properties. We could use the 3D model as a visual demonstration of how", "histories": [["v1", "Thu, 3 Mar 2016 22:59:35 GMT  (8028kb,D)", "http://arxiv.org/abs/1603.01312v1", null]], "reviews": [], "SUBJECTS": "cs.AI", "authors": ["adam lerer", "sam gross", "rob fergus"], "accepted": true, "id": "1603.01312"}, "pdf": {"name": "1603.01312.pdf", "metadata": {"source": "META", "title": "Learning Physical Intuition of Block Towers by Example", "authors": ["Adam Lerer", "Sam Gross", "Rob Fergus"], "emails": ["ALERER@FB.COM", "SGROSS@FB.COM", "ROBFERGUS@FB.COM"], "sections": [{"heading": "1. Introduction", "text": "Interaction with the world requires a common-sense understanding of how it operates at a physical level. For example, we can quickly assess if we can walk over a surface without falling, or how an object will behave if we push it. Making such judgements does not require us to invoke Newton\u2019s laws of mechanics \u2013 instead we rely on intuition, built up through interaction with the world.\nIn this paper, we explore if a deep neural network can capture this type of knowledge. While DNNs have shown remarkable success on perceptual tasks such as visual recognition (Krizhevsky et al., 2012) and speech understanding (Hinton et al., 2012), they have been rarely applied to problems involving higher-level reasoning, particularly those involving physical understanding. However, this is needed to move beyond object classification and detection to a true understanding of the environment, e.g. \u201cWhat will happen\nnext in this scene?\u201d Indeed, the fact that humans develop such physical intuition at an early age (Carey, 2009), well before most other types of high-level reasoning, suggests its importance in comprehending the world.\nTo learn this common-sense understanding, a model needs a way to interact with the physical world. A robotic platform is one option that has been explored e.g. (Agrawal et al., 2015), but inherent complexities limit the diversity and quantity of data that can be acquired. Instead, we use Unreal Engine 4 (UE4) (Epic Games, 2015), a platform for modern 3D game development, to provide a realistic environment. We chose UE4 for its realistic physics simulation, modern 3D rendering, and open source license. We integrate the Torch (Collobert et al., 2011) machine learning framework directly into the UE4 game loop, allowing for online interaction with the UE4 world.\nOne of the first toys encountered by infants, wooden blocks provide a simple setting for the implicit exploration of basic Newtonian concepts such as center-of-mass, stability and momentum. By asking deep models to predict the behavior of the blocks, we hope that they too might internalize such notions. Another reason for selecting this scenario is that it is possible to construct real world examples, enabling the generalization ability of our models to be probed (see Fig. 1).\nTwo tasks are explored: (i) will the blocks fall over or not? and (ii) where will the blocks end up? The former is a binary classification problem, based on the stability of the block configuration. For the latter we predict image masks that show the location of each block. In contrast to the first task, this requires the models to capture the dynamics of the system. Both tasks require an effective visual system to analyze the configuration of blocks. We explore models based on contemporary convolutional networks architectures (LeCun et al., 1989), notably Googlenet (Ioffe & Szegedy, 2015), DeepMask (Pinheiro et al., 2015) and\nar X\niv :1\n60 3.\n01 31\n2v 1\n[ cs\n.A I]\n3 M\nar 2\nResNets (He et al., 2015). While designed for classification or segmentation, we adapt them to our novel task, using an integrated approach where the lower layers perceive the arrangement of blocks and the upper layers implicitly capture their inherent physics.\nOur paper makes the following contributions:\nConvnet-based Prediction of Static Stability: We show that standard convnet models, refined on synthetic data, can accurately predict the stability of stacks of blocks. Crucially, these models successfully generalize to (i) new images of real-world blocks and (ii) new physical scenarios, not encountered during training. These models are purely bottom-up in nature, in contrast to existing approaches which rely on complex top-down graphics engines.\nPrediction of Dynamics: The models are also able to predict with reasonably accuracy the trajectories of the blocks as they fall, showing that they capture notions of acceleration and momentum, again in a purely feed-forward manner.\nComparison to Human Subjects: Evaluation of the test datasets by participants shows that our models match their performance on held-out real data (and are significantly better on synthetic data). Furthermore, the model predictions have a reasonably high correlation with human judgements.\nUETorch: We introduce an open-source combination of the Unreal game engine and the Torch deep learning environment, that is simple and efficient to use. UETorch is a viable environment for a variety of machine learning\nexperiments in vision, physical reasoning, and embodied learning."}, {"heading": "1.1. Related Work", "text": "The most closely related work to ours is (Battaglia et al., 2013) who explore the physics involved with falling blocks. A generative simulation model is used to predict the outcome of a variety of block configurations with varying physical properties, and is found to closely match human judgment. This work complements ours in that it uses a top-down approach, based on a sophisticated graphics engine which incorporates explicit prior knowledge about Newtonian mechanics. In contrast, our model is purely bottom-up, estimating stability directly from image pixels and is learnt from examples.\nOur pairing of top-down rendering engines for data generation with high capacity feed-forward regressors is similar in spirit to the Kinect body pose estimation work of (Shotton et al., 2013), although the application is quite different.\n(Wu et al., 2015) recently investigated the learning of simple kinematics, in the context of objects sliding down ramps. Similar to (Battaglia et al., 2013), they also used a top-down 3D physics engine to map from a hypothesis of object mass, shape, friction etc. to image space. Inference relies on MCMC, initialized to the output of convnet-based estimates of the attributes. As in our work, their evaluations are performed on real data and the model predictions correlate reasonably with human judgement.\nPrior work in reinforcement learning has used synthetic data from games to train bottom-up models. In particular, (Mnih et al., 2015) and (Lillicrap et al., 2015) trained deep convolutional networks with reinforcement learning directly on image pixels from simulations to learn policies for Atari games and the TORCS driving simulator, respectively.\nA number of works in cognitive science have explored intuitive physics, for example, in the context of liquid dynamics (Bates et al., 2015), ballistic motion (Smith et al., 2013) and gears and pulleys (Hegarty, 2004). The latter finds that people perform \u201cmental simulation\u201d to answer questions about gears, pulleys, etc., but some form of implicit bottom-up reasoning is involved too.\nIn computer vision, a number of works have used physical reasoning to aid scene understanding (Zheng et al., 2015; Koppula & Saxena, 2016). For example, (Jia et al., 2015) fit cuboids to RGBD data and use their centroids to search for scene interpretations that are statically stable."}, {"heading": "2. Methods", "text": ""}, {"heading": "2.1. UETorch", "text": "UETorch is a package that embeds the Lua/Torch machine learning environment directly into the UE4 game loop, allowing for fine-grained scripting and online control of UE4 simulations through Torch. Torch is well-suited for game engine integration because Lua is the dominant scripting language for games, and many games including UE4 support Lua scripting. UETorch adds additional interfaces to capture screenshots, segmentation masks, optical flow data, and control of the game through user input or direct modification of game state. Since Torch runs inside the UE4 process, new capabilities can be easily added through FFI without defining additional interfaces/protocols for interprocess communication. UETorch simulations can be run faster than real time, aiding large-scale training. The UETorch package can be downloaded freely at http: //github.com/facebook/UETorch."}, {"heading": "2.2. Data Collection", "text": "Synthetic\nA simulation was developed in UETorch that generated vertical stacks of 2, 3, or 4 colored blocks in random configurations. The block position and orientation, camera position, background textures, and lighting were randomized at each trial to improve the transferability of learned features. In each simulation, we recorded the outcome (did it fall?) and captured screenshots and segmentation masks at 8 frames/sec. Frames and masks from a representative 4- block simulation are shown in Fig. 2. A total of 180,000 simulations were performed, balanced across number of blocks and stable/unstable configurations. 12,288 examples were reserved for testing.\nReal\nFour wooden cubes were fabricated and spray painted red, green, blue and yellow respectively. Manufacturing imperfections added a certain level of randomness to the stability of the real stacked blocks, and we did not attempt to match the physical properties of the real and synthetic blocks. The blocks were manually stacked in configurations 2, 3 and 4 high against a white bedsheet. A tripod mounted DSLR camera was used to film the blocks falling at 60 frames/sec. A white pole was held against the top block in each example, and was then rapidly lifted upwards, allowing unstable stacks to fall (the stick can be see in Fig. 1, blurred due to its rapid motion). Note that this was performed even for stable configurations, to avoid bias. Motion of the blocks was only noticeable by the time the stick was several inches away from top block. 493 examples were captured, balanced between stable/unstable configurations. The totals for 2, 3 and 4 block towers were 115, 139 and 239 examples respectively."}, {"heading": "2.3. Human Subject Methodology", "text": "To better understand the challenge posed about our datasets, real and synthetic, we asked 10 human subjects to evaluate the images in a controlled experiment. Participants were asked to give a binary prediction regarding the outcome of the blocks (i.e. falling or not). During the training phase, consisting of 50 randomly drawn examples, participants were shown the final frame of each example, along with feedback as to whether their choice was correct or not (see Fig. 3). Subsequently, they were tested using 100 randomly drawn examples (disjoint from the training set). During the test phase, no feedback was provided to the individuals regarding the correctness of their responses."}, {"heading": "2.4. Model Architectures", "text": "We trained several convolutional network (CNN) architectures on the synthetic blocks dataset. We trained some ar-\nchitectures on the binary fall prediction task only, and others on jointly on the fall prediction and mask prediction tasks.\nFall Prediction\nWe trained the ResNet-34 (He et al., 2015) and Googlenet (Szegedy et al., 2014) networks on the fall prediction task. These models were pre-trained on the Imagenet dataset (Russakovsky et al., 2015). We replaced the final linear layer with a single logistic output and fine-tuned the entire network with SGD on the blocks dataset. Grid search was performed over learning rates.\nFall+Mask Prediction\nWe used deep mask networks to predict the segmentation trajectory of falling blocks at multiple future times (0s,1s,2s,4s) based on an input image. Each mask pixel is a multi-class classification across a background class and four foreground (block color) classes. A fall prediction is also computed.\nDeepMask (Pinheiro et al., 2015) is an existing mask prediction network trained for instance segmentation, and has the appropriate architecture for our purposes. We replaced the binary mask head with a multi-class SoftMax, and replicated thisN times for mask prediction at multiple points in time.\nWe also designed our own mask prediction network, PhysNet, that was suited to mask prediction rather than just segmentation. For block masks, we desired (i) spatially local and translation-invariant (i.e. convolutional) upsampling from coarse image features to masks, and (ii) more network depth at the coarsest spatial resolution, so the network could reason about block movement. Therefore, PhysNet take the 7 \u00d7 7 outputs from ResNet-34, and performs alternating upsampling and convolution to arrive at 56\u00d756 masks. The PhysNet architecture is shown in Fig. 4. We use the Resnet-34 trunk in PhysNet for historical reasons, but our experiments show comparable results with a Googlenet trunk.\nThe training loss for mask networks is the sum of a binary cross-entropy loss for fall prediction and a pixelwise multiclass cross-entropy loss for each mask. A hyperparameter controls the relative weight of these losses.\nBaselines As a baseline, we perform logistic regression either directly on image pixels, or on pretrained Googlenet features, to predict fall and masks. To reduce the number of parameters, the pixels-to-mask matrix is factored with an intermediate dimension 128. For fall prediction, we also try k-Nearest-Neighbors (k = 10) using Googlenet last-layer image features."}, {"heading": "2.5. Evaluation", "text": "We compare fall prediction accuracy on synthetic and real images, both between models and also between model and human performance. We also train models with a held-out block tower size and test them on the held out tower size, to evaluate the transfer learning capability of these models models to different block tower sizes.\nWe evaluate mask predictions with two criteria: mean mask IoU and log likelihood per pixel. We define mean mask IoU as the intersection-over-union of the mask label with the binarized prediction for the t = 4s mask, averaged over each foreground class present in the mask label.\nMIoU(m,q) = 1\nN N\u2211 n=1\n[ 1\n|Cn| \u2211 c\u2208Cn IoU(mnc, q\u0302nc) ] (1)\nwhere mnc is the set of pixels of class c in mask n, Cn = {c : c \u2208 {1, 2, 3, 4} \u2227 |mnc| > 0} is the set of foreground classes present in mask n, q\u0302nc is the set of pixels in model output n for which c is the highest-scoring class, and IoU(m1,m2) =\n|m1\u2229m2| |m1\u222am2| .\nThe mask IoU metric is intuitive but problematic because it uses binarized masks. For example, if the model predicts a mask with 40% probability in a region, the Mask IoU for that block will be 0 whether or not the block fell in that region. The quality of the predicted mask confidences is better captured by log likelihood.\nThe log likelihood per pixel is defined as the log likelihood of the correct final mask under the predicted (SoftMax) distribution, divided by the number of pixels. This is essentially the negative mask training loss.\nSince the real data has a small number of examples (N = 493 across all blocks sizes), we report an estimated confidence interval for the model prediction on real examples. We estimate this interval as the standard deviation of a binomial distribution with p approximated by the observed accuracy of the model."}, {"heading": "3. Results", "text": ""}, {"heading": "3.1. Fall Prediction Results", "text": "Table 1 compares the accuracy for fall prediction of several deep networks and baselines described in Section 2.4. Convolutional networks perform well at fall prediction, whether trained in isolation or jointly with mask prediction. The best accuracy on synthetic data is achieved with PhysNet, which is jointly trained on masks and fall prediction. Accuracy on real data for all convnets is within their standard deviation.\nAs an ablation study, we also measured the performance\nof Googlenet without Imagenet pretraining. Interestingly, while the model performed equally well on synthetic data with and without pretraining, only the pretrained model generalized well to real images (Table 1).\nOcclusion Experiments\nWe performed occlusion experiments to determine which regions of the block images affected the models\u2019 fall predictions. A Gaussian patch of gray pixels with standard deviation 20% of the image width was superimposed on the image in a 14 \u00d7 14 sliding window to occlude parts of the image, as shown in Fig. 5A. The PhysNet model was evaluated on each occluded image, and the difference in the fall\nprobability predicted from the baseline and occluded images were used to produce heatmaps, shown in Fig. 5B-D. These figures suggest that the model makes its prediction based on relevant local image features rather than memorizing the particular scene. For example, in Fig. 5B, the model prediction is only affected by the unstable interface between the middle and top blocks.\nModel vs. Human Performance\nFig. 6 compares PhysNet to 10 human subjects on the same set of synthetic and real test images. ROC curves comparing human and model performance are generated by using the fraction of test subjects predicting a fall as a proxy for confidence, and comparing this to model confidences.\nOverall, the model convincingly outperforms the human subjects on synthetic data, and is comparable on real data. Interestingly, the correlation between human and model confidences on both real and synthetic data (\u03c1 = (0.69, 0.45)) is higher than between human confidence and ground truth (\u03c1 = (0.60, 0.41)), showing that our model agrees quite closely with human judgement."}, {"heading": "3.2. Mask Prediction Results", "text": "Table 2 compares mask prediction accuracy of the DeepMask and PhysNet networks described in Section 2.4. PhysNet achieves the best performance on both Mean Mask IoU and Log Likelihood per pixel (see Section 2.5), substantially outperforming DeepMask and baselines. Predicting the mask as equal to the initial (t = 0) mask has a high Mask IoU due to the deficiencies in that metric described in Section 2.5.\nExamples of PhysNet mask outputs on synthetic and real data are shown in Fig. 7. We only show masks for examples that are predicted to fall, because predicting masks for\nstable towers is easy and the outputs are typically perfect. The mask outputs from PhysNet are typically quite reasonable for falling 2- and 3-block synthetic towers, but have more errors and uncertainty on 4-block synthetic towers and most real examples. In these cases, the masks are often highly diffuse, showing high uncertainty about the trajectory. On real examples, model predictions and masks are also skewed overstable, likely because of different physical properties of the real and simulated blocks."}, {"heading": "3.3. Evaluation on Held-Out Number of Blocks", "text": "Table 3 compares the performance of networks that had either 3- or 4-block configurations excluded from the training set. While the accuracy of these networks is lower on the untrained class relative to a fully-trained model, it\u2019s still relatively high \u2013 comparable to human performance. The predicted masks on the untrained number of blocks also continue to capture the fall dynamics with reasonably accuracy. Some examples are shown in Fig. 8."}, {"heading": "4. Discussion", "text": "Our results indicate that bottom-up deep CNN models can attain human-level performance at predicting how towers of blocks will fall. We also find that these models\u2019 performance generalizes well to real images if the models are pretrained on real data (Table 1).\nSeveral experiments provide evidence that the deep models we train are gaining knowledge about the dynamics of the block towers, rather than simply memorizing a mapping from configurations to outcomes. Most convincingly, the relatively small degradation in performance of the models on a tower size that is not shown during training (Table 3 & Fig. 8) demonstrates that the model must be making its prediction based on local features rather than memorized exact block configurations. The occlusion experiments in Fig. 5 also suggest that models focus on particular regions that confer stability or instability to a block configuration. Finally, the poor performance of k-nearest-neighbors on Googlenet features in Table 1 suggests that nearby configurations in Googlenet\u2019s pretrained feature space are not predictive of the stability of a given configuration.\nCompared to top-down, simulation-based models such as (Battaglia et al., 2013), deep models require far more training data \u2013 many thousands of examples \u2013 to achieve a high level of performance. Deep models also have difficulty generalizing to examples far from their training data. These difficulties arise because deep models must learn physics from scratch, whereas simulation-based models start with strong priors encoded in the physics simulation engine. Bottom-up and top-down approaches each have their advantages, and the precise combination of these systems in human reasoning is the subject of debate (e.g. (Davis & Marcus, 2016) and (Goodman et al., 2015)). Our results suggest that deep models show promise for directly capturing common-sense physical intuitions about the world that could lead to more powerful visual reasoning systems.\nWe believe that synthetic data from realistic physical sim-\nulations in UETorch are useful for other machine learning experiments in vision, physics, and agent learning. The combination of synthetic data and mask prediction constitutes a general framework for learning concepts such as object permanence, 3D extent, occlusion, containment, solidity, gravity, and collisions, that may be explored in the future."}, {"heading": "Acknowledgements", "text": "The authors would like to thank: Soumith Chintala and Arthur Szlam for early feedback on experimental design; Sainbayar Sukhbaatar for assistance collecting the realworld block examples; Y-Lan Boureau for useful advice regarding the human subject experiments; and Piotr Dollar for feedback on the manuscript."}], "references": [{"title": "Learning to see by moving", "author": ["Agrawal", "Pulkit", "Carreira", "Joao", "Malik", "Jitendra"], "venue": null, "citeRegEx": "Agrawal et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Agrawal et al\\.", "year": 2015}, {"title": "Humans predict liquid dynamics using probabilistic simulation", "author": ["C.J. Bates", "I. Yildirim", "J.B. Tenenbaum", "P.W. Battaglia"], "venue": "In In Proc. 37th Ann. Conf. Cognitive Science Society,", "citeRegEx": "Bates et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Bates et al\\.", "year": 2015}, {"title": "Simulation as an engine of physical scene understanding", "author": ["Battaglia", "Peter W", "Hamrick", "Jessica B", "Tenenbaum", "Joshua B"], "venue": "Proceedings of the National Academy of Sciences,", "citeRegEx": "Battaglia et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Battaglia et al\\.", "year": 2013}, {"title": "The origin of concepts", "author": ["Carey", "Susan"], "venue": null, "citeRegEx": "Carey and Susan.,? \\Q2009\\E", "shortCiteRegEx": "Carey and Susan.", "year": 2009}, {"title": "Torch7: A matlab-like environment for machine learning", "author": ["Collobert", "Ronan", "Kavukcuoglu", "Koray", "Farabet", "Cl\u00e9ment"], "venue": "In BigLearn, NIPS Workshop, number EPFL-CONF-192376,", "citeRegEx": "Collobert et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Collobert et al\\.", "year": 2011}, {"title": "The scope and limits of simulation in automated reasoning", "author": ["Davis", "Ernest", "Marcus", "Gary"], "venue": "Artificial Intelligence,", "citeRegEx": "Davis et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Davis et al\\.", "year": 2016}, {"title": "Relevant and robust a response to marcus and davis", "author": ["Goodman", "Noah D", "Frank", "Michael C", "Griffiths", "Thomas L", "Tenenbaum", "Joshua B", "Battaglia", "Peter W", "Hamrick", "Jessica B"], "venue": "Psychological science,", "citeRegEx": "Goodman et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Goodman et al\\.", "year": 2013}, {"title": "Deep Residual Learning for Image Recognition", "author": ["K. He", "X. Zhang", "S. Ren", "J. Sun"], "venue": "ArXiv e-prints,", "citeRegEx": "He et al\\.,? \\Q2015\\E", "shortCiteRegEx": "He et al\\.", "year": 2015}, {"title": "Mechanical reasoning by mental simulation", "author": ["Hegarty", "Mary"], "venue": "Trends in cognitive sciences,", "citeRegEx": "Hegarty and Mary.,? \\Q2004\\E", "shortCiteRegEx": "Hegarty and Mary.", "year": 2004}, {"title": "Batch normalization: Accelerating deep network training by reducing internal covariate", "author": ["Ioffe", "Sergey", "Szegedy", "Christian"], "venue": "shift. CoRR,", "citeRegEx": "Ioffe et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Ioffe et al\\.", "year": 2015}, {"title": "Consistent physics underlying ballistic motion prediction", "author": ["K.A. Smith", "P.W. Battaglia", "E. Vul"], "venue": "In Proc. 35th Ann. Conf. Cognitive Science Society,", "citeRegEx": "Smith et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Smith et al\\.", "year": 2013}, {"title": "Scene understanding by reasoning stability and safety", "author": ["Zheng", "Bo", "Zhao", "Yibiao", "Yu", "Joey", "Ikeuchi", "Katsushi", "Zhu", "Song-Chun"], "venue": "International Journal of Computer Vision,", "citeRegEx": "Zheng et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Zheng et al\\.", "year": 2015}], "referenceMentions": [{"referenceID": 0, "context": "(Agrawal et al., 2015), but inherent complexities limit the diversity and quantity of data that can be acquired.", "startOffset": 0, "endOffset": 22}, {"referenceID": 4, "context": "We integrate the Torch (Collobert et al., 2011) machine learning framework directly into the UE4 game loop, allowing for online interaction with the UE4 world.", "startOffset": 23, "endOffset": 47}, {"referenceID": 7, "context": "ResNets (He et al., 2015).", "startOffset": 8, "endOffset": 25}, {"referenceID": 2, "context": "The most closely related work to ours is (Battaglia et al., 2013) who explore the physics involved with falling blocks.", "startOffset": 41, "endOffset": 65}, {"referenceID": 2, "context": "Similar to (Battaglia et al., 2013), they also used a top-down 3D physics engine to map from a hypothesis of object mass, shape, friction etc.", "startOffset": 11, "endOffset": 35}, {"referenceID": 1, "context": "A number of works in cognitive science have explored intuitive physics, for example, in the context of liquid dynamics (Bates et al., 2015), ballistic motion (Smith et al.", "startOffset": 119, "endOffset": 139}, {"referenceID": 10, "context": ", 2015), ballistic motion (Smith et al., 2013) and gears and pulleys (Hegarty, 2004).", "startOffset": 26, "endOffset": 46}, {"referenceID": 11, "context": "In computer vision, a number of works have used physical reasoning to aid scene understanding (Zheng et al., 2015; Koppula & Saxena, 2016).", "startOffset": 94, "endOffset": 138}, {"referenceID": 7, "context": "We trained the ResNet-34 (He et al., 2015) and Googlenet (Szegedy et al.", "startOffset": 25, "endOffset": 42}, {"referenceID": 2, "context": "Compared to top-down, simulation-based models such as (Battaglia et al., 2013), deep models require far more training data \u2013 many thousands of examples \u2013 to achieve a high level of performance.", "startOffset": 54, "endOffset": 78}], "year": 2016, "abstractText": "Wooden blocks are a common toy for infants, allowing them to develop motor skills and gain intuition about the physical behavior of the world. In this paper, we explore the ability of deep feedforward models to learn such intuitive physics. Using a 3D game engine, we create small towers of wooden blocks whose stability is randomized and render them collapsing (or remaining upright). This data allows us to train large convolutional network models which can accurately predict the outcome, as well as estimating the block trajectories. The models are also able to generalize in two important ways: (i) to new physical scenarios, e.g. towers with an additional block and (ii) to images of real wooden blocks, where it obtains a performance comparable to human subjects.", "creator": "LaTeX with hyperref package"}}}