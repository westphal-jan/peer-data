{"id": "1603.01547", "review": {"conference": "ACL", "VERSION": "v1", "DATE_OF_SUBMISSION": "4-Mar-2016", "title": "Text Understanding with the Attention Sum Reader Network", "abstract": "Several large cloze-style context-question-answer datasets have been introduced recently: the CNN and Daily Mail news data and the Children's Book Test. Thanks to the size of these datasets, the associated text comprehension task is well suited for deep-learning techniques that currently seem to outperform all alternative approaches. We present a new, simple model that uses attention to directly pick the answer from the context as opposed to computing the answer using a blended representation of words in the document as is usual in similar models. This makes the model particularly suitable for question-answering problems where the answer is a single word from the document. Our model outperforms models previously proposed for these tasks by a large margin. Our model now presents a method where the answers on question-related question questions can be compared to one another by using the blended image for a sample of question-related questions and the same image for a sample of question-related questions.\n\n\n\nTo date, most current datasets of the same type used for this method have been used to model question-related questions. To date, most current datasets of the same type used for this method have been used to model question-related questions. To date, most current datasets of the same type used for this method have been used to model question-related questions. To date, most current datasets of the same type used for this method have been used to model question-related questions.\nThe first question-related question-related question-related question-related question-related question-related question-related question-related question-related question-related question-related question-related question-related question-related question-related question-related question-related question-related question-related question-related question-related question-related question-related question-related question-related question-related question-related question-related question-related question-related question-related question-related question-related question-related question-related question-related question-related question-related question-related question-related question-related question-related question-related question-related question-related question-related question-related question-related question-related question-related question-related question-related question-related question-related question-related question-related question-related question-related question-related question-related question-related question-related question-related question-related question-related question-related question-related question-related question-related question-related question-related question-related question-related question-", "histories": [["v1", "Fri, 4 Mar 2016 17:32:42 GMT  (366kb,D)", "http://arxiv.org/abs/1603.01547v1", null], ["v2", "Fri, 24 Jun 2016 13:04:47 GMT  (825kb,D)", "http://arxiv.org/abs/1603.01547v2", "Presented at ACL 2016"]], "reviews": [], "SUBJECTS": "cs.CL", "authors": ["rudolf kadlec", "martin schmid", "ondrej bajgar", "jan kleindienst"], "accepted": true, "id": "1603.01547"}, "pdf": {"name": "1603.01547.pdf", "metadata": {"source": "CRF", "title": "Text Understanding with the Attention Sum Reader Network", "authors": ["Rudolf Kadlec", "Martin Schmid", "Ondrej Bajgar", "Jan Kleindienst"], "emails": ["kadlec@cz.ibm.com", "martin.schmid@cz.ibm.com", "obajgar@cz.ibm.com", "jankle@cz.ibm.com"], "sections": [{"heading": "1 Introduction", "text": "Most of the information humanity has gathered up to this point is stored in the form of plain text. Hence the task of teaching machines how to understand this data is of utmost importance in the field of Artificial Intelligence. One way of testing the level of text understanding is simply to ask the system questions for which the answer can be inferred from the text. A well-known example of a system that could make use of a huge collection of unstructured documents to answer questions is for instance IBM\u2019s Watson system used for the Jeopardy challenge (Ferrucci et al., 2010).\nCloze style questions (Taylor, 1953), i.e. questions formed by removing a phrase from a sentence, are an appealing form of such questions. While the task is easy to evaluate, one can vary the context, the question sentence or the specific phrase missing in the question to dramatically change the task structure and difficulty.\nOne way of altering the task difficulty is to vary the word type being replaced, as in (Hill et al., 2015). The complexity of such variation comes from the fact that\n\u2217We would also like to thank Tim Klinger for providing us with masked softmax code that we used in our implementation.\nthe level of context understanding needed in order to correctly predict different types of words varies greatly. While predicting prepositions can easily be done using relatively simple models with very little context knowledge, predicting named entities requires a deeper understanding of the context.\nAlso, as opposed to selecting a random sentence from a text (as done in (Hill et al., 2015)), the questions can be formed from a specific part of a document, such as a short summary or a list of tags. Since such sentences often paraphrase in a condensed form what was said in the text, they are particularly suitable for testing text comprehension (Hermann et al., 2015).\nAn important property of cloze style questions is that a large amount of such questions can be automatically generated from real world documents. This opens the task to data-hungry techniques such as deep learning.\nIn the first part of this article we introduce the task at hand and the main aspects of the relevant datasets. Then we present our own model to tackle the problem. Subsequently we compare the model to previously proposed architectures and finally describe the experimental results on the performance of our model."}, {"heading": "2 Task and datasets", "text": "In this section we briefly introduce the task that we are seeking to solve and relevant large-scale datasets that have recently been introduced for this task."}, {"heading": "2.1 Formal Task Description", "text": "The task consists of answering a cloze style question, the answer to which is dependent on the understanding of a context document provided with the question. The model is also provided with a set of possible answers from which the correct one is to be selected. This can be formalized as follows:\nThe training data consist of tuples (q,d, a, A), where q is a question, d is a document that contains the answer to question q, A is a set of possible answers and a \u2208 A is the ground truth answer. Both q and d are sequences of words from vocabulary V . We also assume that all possible answers are words from the vocabulary, that is A \u2286 V , and that the ground truth answer a appears in the document, that is a \u2208 d.\nar X\niv :1\n60 3.\n01 54\n7v 1\n[ cs\n.C L\n] 4\nM ar\n2 01\n6"}, {"heading": "2.2 Datasets", "text": "We will now briefly summarize important features of the datasets."}, {"heading": "2.2.1 News Articles \u2014 CNN and Daily Mail", "text": "The first two datasets1 (Hermann et al., 2015) were constructed from a large number of news articles from the CNN and Daily Mail websites. The main body of each article forms a context, while the cloze style question is formed from one of short highlight sentences, appearing at the top of each article page. Specifically, the question is created by replacing a named entity from the summary sentence (e.g. \u201cProducer X will not press charges against Jeremy Clarkson, his lawyer says.\u201d).\nFurthermore the named entities in the whole dataset were replaced by anonymous tokens which were further shuffled for each example so that the model cannot build up any world knowledge about the entities and hence has to genuinely rely on the context document to search for an answer to the question."}, {"heading": "2.2.2 Children\u2019s Book Test", "text": "The third dataset2, the Children\u2019s Book Test (CBT) (Hill et al., 2015), is built from books that are freely available thanks to Project Gutenberg3. Each context document is formed by 20 consecutive sentences taken from a children\u2019s book story. Due to the lack of summary, the cloze style question is then constructed from the subsequent (21st) sentence.\nOne can also see how the task complexity varies with the type of the omitted word (named entity, common noun, verb, preposition). (Hill et al., 2015) have shown that while standard LSTM language models have human level performance on predicting verbs and prepositions, they lack behind on named entities and common nouns. In this article we therefore focus only on predicting the first two word types.\n1The CNN and Daily Mail datasets are available at https://github.com/deepmind/rc-data\n2The CBT dataset is available at http://www. thespermwhale.com/jaseweston/babi/ CBTest.tgz\n3https://www.gutenberg.org/\nBasic statistics about the CNN, Daily Mail and CBT datasets are summarized in Table 1."}, {"heading": "3 Our Model \u2014 Attention Sum Reader", "text": ""}, {"heading": "3.1 Motivation", "text": "Our model called the Attention Sum Reader (AS Reader) is tailor-made to leverage the fact that the answer is a word from the context document. This is a double-edged sword. While it achieves state-of-theart results on all of the mentioned datasets (where this assumption holds true), it cannot produce an answer which is not contained in the document. Intuitively, our model is structured as follows:\n1. We compute a vector embedding of the query.\n2. We compute a vector embedding of each individual word in the context of the whole document (contextual embedding).\n3. Using a dot product between the question embedding and the contextual embedding of each occurrence of a candidate answer in the document, we select the most likely answer."}, {"heading": "3.2 Formal Description", "text": "Our model uses one word embedding function and two encoder functions. The word embedding function e translates words into vector representations. The first encoder function is a document encoder f that encodes every word from the document d in the context of the whole document. We call this the contextual embedding. For convenience we will denote the contextual embedding of the i-th word in d as fi(d). The second encoder g is used to translate the query q into a fixed length representation of the same dimensionality as fi(d). Both encoders use word embeddings computed by e as their input. Then we compute a weight for every word in the document as the dot product of its contextual embedding and the query embedding. This weight might be viewed as an attention over the document d.\nTo form a proper probability distribution over the words in the document, we normalize the weights using the softmax function. This way we model probability si that the answer to query q appears at position i in the document d. In a functional form this is:\nsi \u221d exp (fi(d) \u00b7 g(q)) (1)\nFinally we compute the probability that word w is a correct answer as:\nP (w|q,d) = \u2211\ni\u2208I(w,d)\nsi (2)\nwhere I(w,d) is a set of positions where w appears in the document d. We call this mechanism pointer sum attention since we use attention as a pointer over discrete tokens in the context document and then we directly sum the word\u2019s attention across all the occurrences. This differs from the usual use of attention in sequence-to-sequence models (Bahdanau et al., 2015) where attention is used to blend representations of words into a new embedding vector. Our use of attention was inspired by Pointer Networks (PtrNets) (Vinyals et al., 2015).\nA high level structure of our model is shown in Figure 1."}, {"heading": "3.3 Model instance details", "text": "In our model the document encoder f is implemented as a bidirectional Gated Recurrent Unit (GRU) network (Cho et al., 2014) whose hidden states form the contextual word embeddings, that is fi(d) =\u2212\u2192 fi (d) || \u2190\u2212 fi (d), where || denotes vector concatenation and \u2212\u2192 fi and \u2190\u2212 fi denote forward and backward contextual embeddings from the respective recurrent networks. The query encoder g is implemented by another bidirectional GRU network. This time the last hidden state of the forward network is concatenated with the last hidden state of the backward network to form the\nquery embedding, that is g(q) = \u2212\u2192g|q|(q) || \u2190\u2212g1(q). The word embedding function e is implemented in a usual way as a look-up table V. V is a matrix whose rows can be indexed by words from the vocabulary, that is e(w) = Vw, w \u2208 V . Therefore, each row of V contains embedding of one word from the vocabulary. During training we jointly optimize parameters of f , g and e."}, {"heading": "4 Related Work", "text": "Two recent deep neural network architectures (Hermann et al., 2015; Hill et al., 2015) were applied to the task of text comprehension. Both these architectures use an attention mechanism that allows them to highlight places in the document that might be relevant to answering the question. We will now briefly describe these architectures and compare them to our approach."}, {"heading": "4.1 Attentive and Impatient Readers", "text": "Attentive and Impatient Readers were proposed in (Hermann et al., 2015). The simpler Attentive Reader is very similar to our architecture. It also uses bidirectional document and query encoders to compute an attention in a similar way we do. The more complex Impatient Reader computes attention over the document after reading every word of the query. However, empirical evaluation has shown that both models perform almost identically on the CNN and Daily Mail datasets.\nThe key difference between the Attentive Reader and our model is that the Attentive Reader uses attention to compute a fixed length representation r of the document d that is equal to a weighted sum of contextual embeddings of words in d, that is r = \u2211 i sifi(d). A joint query and document embedding is then a nonlinear function of r and the query embedding g(q). This joint embedding is in the end compared against all candidate answers a\u2032 \u2208 A using the dot product e(a\u2032) \u00b7 r, in the end the scores are normalized by softmax. That is: P (a\u2032|q,d) \u221d exp (e(a\u2032) \u00b7 r).\nIn contrast to the Attentive Reader, we select the answer from the context directly using the computed attention rather than using such attention for a weighted sum of the individual representations (see Eq. 2). The motivation for such simplification is the following.\nConsider a context \u201cBayern Munchen plays FC Barcelona tonight. Last time it played Liverpool\u201d 4 and question \u201cX is one of the candidates for tonight\u2019s victory.\u201d\nSince both Bayern Munchen and FC Barcelona are equally good candidates, the attention mechanism might put the same attention on both these candidates in the context. Because the resulting representation is computed as a weighted sum when using the above blending mechanism, the vector will represent something in between these two good candidates. It could very well be that the candidate with closest representation to the resulting blended embedding is Liverpool.\nBy using a sum rather than an average, our architecture would correctly propose Bayern and Barcelona as equally good candidates rather than potentially proposing a third one."}, {"heading": "4.2 Memory Networks", "text": "MemNNs (Weston et al., 2014) were applied to the task of text comprehension in (Hill et al., 2015).\nThe best performing memory networks model setup - window memory - uses windows of fixed length (8) centered around the candidate words as memory cells. Due to this limited context window, the model is unable to capture dependencies out of scope of this window.\n4In the text comprehension task as defined in (Hermann et al., 2015) these named entities will be anonymized. However, this problem can still emerge with anonymous word embeddings.\nFurthermore, the representation within such window is computed simply as the sum of embeddings of words in that window. By contrast, in our model the representation of each individual word is computed using a recurrent network, which not only allows it to capture context from the entire document but also the embedding computation is much more flexible than a simple sum.\nTo improve the initial accuracy, a heuristic approach called self supervision is used in (Hill et al., 2015) to help the network to select the right supporting \u201cmemories\u201d. Plain MemNNs without this heuristic are not competitive on these machine reading tasks. Our model does not need any similar heuristics."}, {"heading": "4.3 Pointer Networks", "text": "Our model architecture was inspired by PtrNets (Vinyals et al., 2015) in using an attention mechanism to select the answer in the context rather than to blend words from the context into an answer representation. While a Ptr-Net consists of an encoder as well as a decoder, which uses the attention to select the output at each step, our model outputs the answer in a single step. Furthermore, the pointer networks assume that no input in the sequence appears more than once, which is not the case in our settings."}, {"heading": "4.4 Summary", "text": "Our model combines the best features of the architectures mentioned above. We use recurrent networks to \u201cread\u201d the document and the query as does Attentive and Impatient readers and we use attention in a way similar to Ptr-Nets. We also use summation of atten-\ntion weights in a way similar to MemNNs (Hill et al., 2015)."}, {"heading": "5 Evaluation", "text": "In this section we evaluate our model on the CNN, Daily Mail and CBT datasets. We show that despite its simplicity the model achieves state-of-the-art performance on each of these datasets."}, {"heading": "5.1 Training Details", "text": "To train the model we used stochastic gradient descent with the ADAM update rule (Kingma and Ba, 2015) and learning rate of 0.001 or 0.0005. We used negative log-likelihood as the training objective.\nThe initial weights in the word embedding matrix were drawn randomly uniformly from the interval [\u22120.1, 0.1]. Weights in the GRU networks were initialized by random orthogonal matrices (Saxe et al., 2014) and biases were initialized to zero. The gradient clipping (Pascanu et al., 2012) threshold was set to 10 and the batch size to 32.\nDuring training we randomly shuffled all examples in each epoch. To speedup training, we always prefetched 10 batches worth of examples and sorted them according to the length of the document. This way each batch contained documents of roughly the same length.\nFor the CNN and Daily Mail datasets, for each batch we randomly reshuffled the assignment of named entities to the corresponding word embedding vectors to match the procedure proposed in (Hermann et al., 2015). This guaranteed that word embeddings of named entities were used only as semantically meaningless labels not encoding any intrinsic features of the represented entities. This forced the model to truly deduce the answer from the single context document associated with the question.\nDuring training we evaluated the model performance after each epoch and stopped the training when the error on the validation set started increasing.\nThe models usually converged after two epochs of training. Time needed to complete a single epoch of training on each dataset on an Nvidia K40 GPU is shown in Table 6.\nThe hyperparameters, namely the recurrent hidden layer dimension and the source embedding dimension, were chosen by grid search. We started with a range of 128 to 384 for both parameters and subsequently\nkept increasing the upper bound by 128 until we started observing a consistent decrease in validation accuracy. The region of the parameter space that we explored together with the parameters of the model with best validation accuracy are summarized in Table 7.\nOur model was implemented using Theano (Bastien et al., 2012) and Blocks (van Merrienboer et al., 2015)."}, {"heading": "5.2 Evaluation Method", "text": "We evaluated the proposed model both as a single model and using ensemble averaging.\nFor single models we are reporting results for the best model as well as the average of accuracies for the best 20% of models with best performance on validation data since single models display considerable variation of results due to random weight initialization, even for identical hyperparameter values. Single model performance may consequently prove difficult to reproduce.\nWhat concerns ensembles, we used simple averaging of the answer probabilities predicted by ensemble members5.\nThe ensemble models were chosen either as the top 70% of all trained models or using the following algorithm: We started with the best performing model according to validation performance. Then in each step we tried adding the best performing model that had not been previously tried. We kept it in the ensemble if it did improve its validation performance and discarded it otherwise. This way we gradually tried each model once. We call the resulting model a greedy ensemble."}, {"heading": "5.3 Results", "text": "Performance of our models on the CBT dataset is summarized in Table 5, Table 4 shows results on the CNN and Daily Mail datasets. The tables also list performance of other published models that were evaluated on these datasets. Ensembles of our models set the new state-of-the-art results on all evaluated datasets. However, even our best single models outperform the best previously reported results.\n5Attempts to use the Constrained Optimization By Linear Approximation (COBYLA) method (Powell, 1994) to optimize the weights lead to overfitting on the validation data with respect to which the optimization was done.\nTable 8 then measures accuracy as the proportion of test cases where the ground truth was among the top k answers proposed by the greedy ensemble model for k = 1, 2, 5.\nCNN. On the CNN dataset our single model with best validation accuracy achieves a test accuracy of 69.5%, this is 0.1% better than ensemble of models reported in (Hill et al., 2015). The average performance of the top 20% models according to validation accuracy is 69.9% which is even 0.5% better than the single bestvalidation model. This shows that there were many models that performed better on test set than the bestvalidation model. Fusing multiple models then gives a significant further increase in accuracy. Our simpleaverage ensemble outperforms the accuracy of the best previously reported ensemble (Hill et al., 2015) by 6%.\nDaily Mail. On the Daily Mail dataset our single model with accuracy of 73.9% outperforms the best previous result achieved by the Attentive Reader (Hermann et al., 2015) by 4.9% absolute and our averaging ensemble is by 8.7% absolute better.\nCBT. In named entity prediction our best single model with accuracy of 68.6% performs 2% absolute\nbetter than the MemNN with self supervision, the averaging ensemble performs 4% absolute better than the best previous result. In common noun prediction our single models is 0.4% absolute better than MemNN however the ensemble improves the performance to 69% which is 6% absolute better than MemNN."}, {"heading": "6 Analysis", "text": "To further analyze the properties of our model, we examined the dependence of accuracy on the length of the context document (Figure 2), the number of candidate\nanswers (Figure 3) and the frequency of the correct answer in the context (Figure 4).\nOn the CNN and Daily Mail datasets, the accuracy decreases with increasing document length (Figure 2a). We hypothesize this may be due to multiple factors. Firstly long documents may make the task more complex. Secondly such cases are quite rare in the training data (Figure 2b) which motivates the model to specialize on shorter contexts. Finally the context length is correlated with the number of named entities, i.e. the number of possible answers which is itself negatively correlated with accuracy (see Figure 3).\nOn the CBT dataset this negative trend seems to disappear (Fig. 2c). This supports the later two explanations since the distribution of document lengths is somewhat more uniform (Figure 2d) and the number of candidate answers is constant (10) for all examples in this dataset.\nThe effect of increasing number of candidate answers on the model\u2019s accuracy can be seen in Figure 3a. We can clearly see that as the number of candidate answers increases, the accuracy drops. On the other hand, the amount of examples with large number of candidate answers is quite small (Figure 3b).\nFinally, since the summation of attention in our model inherently favours frequently occurring tokens, we also visualize how the accuracy depends on the frequency of the correct answer in the document. Figure 4a shows that the accuracy significantly drops as the correct answer gets less and less frequent in the document compared to other candidate answers. On the other hand, the correct answer is likely to occur frequently (Fig. 4a)."}, {"heading": "6.1 Comparison to Weighted Average Blending", "text": "We hypothesized in Section 4.1 that the fact that the Attentive Reader uses attention to create a blended representation potentially harms its performance. In order to verify this intuition, we implemented blending into our model, bringing it closer to the Attentive Reader (Hermann et al., 2015).\nIn this modified model we compute attention weights si in the same way as in our original model (see Eq. 1). However, we replace Eq. 2 with the following equations:\nr = \u2211 i sie(wi) (3)\nP (a\u2032|q,d) \u221d exp (r \u00b7 e(a\u2032)) (4)\nwhere r is the blended response embedding and a\u2032 \u2208 A is a possible candidate response. This change in the architecture indeed lead to a significant decrease in accuracy across all four datasets.\nNamely, on each CBT dataset the difference was almost 15% while on CNN and Daily Mail the decrease was over 6% and 2% respectively. Besides the training time for the models with blending was several times\nlonger than our attention sum architecture both measured by the time per epoch and by the number of epochs required for the model to converge."}, {"heading": "7 Conclusion", "text": "In this article we presented a new neural network architecture for natural language text comprehension. While our model is simpler than previously published models, it gives a new state-of-the-art accuracy on all the evaluated datasets."}], "references": [{"title": "Kyunghyun Cho", "author": ["Dzmitry Bahdanau"], "venue": "and Yoshua Bengio.", "citeRegEx": "Bahdanau et al.2015", "shortCiteRegEx": null, "year": 2015}, {"title": "Nicolas Bouchard", "author": ["Fr\u00e9d\u00e9ric Bastien", "Pascal Lamblin", "Razvan Pascanu", "James Bergstra", "Ian J. Goodfellow", "Arnaud Bergeron"], "venue": "and Yoshua Bengio.", "citeRegEx": "Bastien et al.2012", "shortCiteRegEx": null, "year": 2012}, {"title": "Holger Schwenk", "author": ["Kyunghyun Cho", "Bart van Merrienboer", "Caglar Gulcehre", "Dzmitry Bahdanau", "Fethi Bougares"], "venue": "and Yoshua Bengio.", "citeRegEx": "Cho et al.2014", "shortCiteRegEx": null, "year": 2014}, {"title": "Nico Schlaefer", "author": ["David Ferrucci", "Eric Brown", "Jennifer Chu-Carroll", "James Fan", "David Gondek", "Aditya a. Kalyanpur", "Adam Lally", "J. William Murdock", "Eric Nyberg", "John Prager"], "venue": "and Chris Welty.", "citeRegEx": "Ferrucci et al.2010", "shortCiteRegEx": null, "year": 2010}, {"title": "Mustafa Suleyman", "author": ["Karl Moritz Hermann", "Tomas Kocisky", "Edward Grefenstette", "Lasse Espeholt", "Will Kay"], "venue": "and Phil Blunsom.", "citeRegEx": "Hermann et al.2015", "shortCiteRegEx": null, "year": 2015}, {"title": "Sumit Chopra", "author": ["Felix Hill", "Antoine Bordes"], "venue": "and Jason Weston.", "citeRegEx": "Hill et al.2015", "shortCiteRegEx": null, "year": 2015}, {"title": "Adam: a Method for Stochastic Optimization", "author": ["Kingma", "Ba2015] Diederik P. Kingma", "Jimmy Lei Ba"], "venue": "International Conference on Learning Representations,", "citeRegEx": "Kingma et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Kingma et al\\.", "year": 2015}, {"title": "Tomas Mikolov", "author": ["Razvan Pascanu"], "venue": "and Yoshua Bengio.", "citeRegEx": "Pascanu et al.2012", "shortCiteRegEx": null, "year": 2012}, {"title": "D", "author": ["J Michael"], "venue": "Powell,", "citeRegEx": "Powell1994", "shortCiteRegEx": null, "year": 1994}, {"title": "James L Mcclelland", "author": ["Andrew M Saxe"], "venue": "and Surya Ganguli.", "citeRegEx": "Saxe et al.2014", "shortCiteRegEx": null, "year": 2014}, {"title": "Cloze procedure: a new tool for measuring readability", "author": ["Wilson L Taylor"], "venue": "Journalism and Mass Communication", "citeRegEx": "Taylor.,? \\Q1953\\E", "shortCiteRegEx": "Taylor.", "year": 1953}, {"title": "Jan Chorowski", "author": ["Bart van Merrienboer", "Dzmitry Bahdanau", "Vincent Dumoulin", "Dmitriy Serdyuk", "David Warde-farley"], "venue": "and Yoshua Bengio.", "citeRegEx": "van Merrienboer et al.2015", "shortCiteRegEx": null, "year": 2015}, {"title": "Meire Fortunato", "author": ["Oriol Vinyals"], "venue": "and Navdeep Jaitly.", "citeRegEx": "Vinyals et al.2015", "shortCiteRegEx": null, "year": 2015}, {"title": "Sumit Chopra", "author": ["Jason Weston"], "venue": "and Antoine Bordes.", "citeRegEx": "Weston et al.2014", "shortCiteRegEx": null, "year": 2014}], "referenceMentions": [], "year": 2016, "abstractText": "Several large cloze-style context-questionanswer datasets have been introduced recently: the CNN and Daily Mail news data and the Children\u2019s Book Test. Thanks to the size of these datasets, the associated text comprehension task is well suited for deep-learning techniques that currently seem to outperform all alternative approaches. We present a new, simple model that uses attention to directly pick the answer from the context as opposed to computing the answer using a blended representation of words in the document as is usual in similar models. This makes the model particularly suitable for question-answering problems where the answer is a single word from the document. Our model outperforms models previously proposed for these tasks by a large margin.", "creator": "TeX"}}}