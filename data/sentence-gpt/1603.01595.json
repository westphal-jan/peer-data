{"id": "1603.01595", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "4-Mar-2016", "title": "Sentiment Analysis in Scholarly Book Reviews", "abstract": "So far different studies have tackled the sentiment analysis in several domains such as restaurant and movie reviews. But, this problem has not been studied in scholarly book reviews which is different in terms of review style and size. In this paper, we propose to combine different features in order to be presented to a supervised classifiers which extract the opinion target expressions and detect their polarities in scholarly book reviews. To assess whether these features have different interpretation of the data, we are interested in examining the accuracy and validity of these features.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n", "histories": [["v1", "Fri, 4 Mar 2016 20:04:31 GMT  (20kb)", "http://arxiv.org/abs/1603.01595v1", "10 pages"]], "COMMENTS": "10 pages", "reviews": [], "SUBJECTS": "cs.CL cs.AI", "authors": ["hussam hamdan", "patrice bellot", "frederic bechet"], "accepted": false, "id": "1603.01595"}, "pdf": {"name": "1603.01595.pdf", "metadata": {"source": "CRF", "title": "Sentiment Analysis in Scholarly Book Reviews", "authors": ["Hussam Hamdan", "Patrice Bellot", "Frederic Bechet"], "emails": ["hussam.hamdan@lsis.org", "patrice.bellot@lsis.org", "frederic.bechet@lif.univ-mrs.fr"], "sections": [{"heading": null, "text": "ar X\niv :1\n60 3.\n01 59\n5v 1\n[ cs\n.C L\n] 4\nM ar"}, {"heading": "1 Introduction", "text": "Classifying opinion texts at document or sentence levels is not sufficient for applications which need to identify the opinion targets. Even if the document is about one entity, many applications need to determine the opinion about each aspect of the entity. A user may express a positive opinion towards the food in a restaurant, but he may have a negative opinion towards other aspects as the ambiance. Therefore, we need to identify the aspects and determine whether the sentiment is positive, negative or neutral towards each one. This task is called Aspect-Based Sentiment Analysis or Feature-Based opinion mining as called in the early work [Hu and Liu, 2004].\nIn this work, we address the problem of sentiment analysis in scholarly book reviews. Our objective is to extract the opinion expressed towards a book in all its reviews. Therefore, given a collection of book reviews, we aim at finding out the aspects of the book and the sentiment expressed towards each aspect. This seems similar to aspect-based sentiment analysis in restaurant reviews where we have a set of aspects such as (food, drinks, service, ambiance, location), each aspect involves different aspect terms or opinion target expressions i.g. Pizza, Burger in food aspect.\nWhile it is not difficult to have a list of aspects in restaurant domain, it is ambiguous what may be the aspects of a book. When one thinks about the aspects of books, he may think about the quality of book, the number of pages, the discussed topics ...etc. But it is still not obvious as in restaurant reviews\nwhere all people may consider without doubt the food and drinks as aspects or categories. In fact, one can consider two methods to determine the aspects:\n1. Applying unsupervised method which is capable of extracting the facets or topics such as topic modeling in which we consider each topic related to an aspect. It is not obvious how we can evaluate the quality of this method and how each topic related to an aspect. 2. Asking domain experts to extract the aspects of books.\nWe have chosen the second method which can be evaluated at fine level of granularity, therefore we have asked the OpenEdition editorial team1, which deals with book reviews of social and human sciences, to enumerate the potential aspects that may be found in book reviews. They have listed the following aspects:\n1. Book presentation\n2. Problematic 3. Scientific context 4. Scientific method\n5. Author\u2019s arguments 6. Book organization 7. judgment about the book\nIn each aspect one can find a various opinion targets which describe or name an aspect which can not be listed, the annotators will specify them during their annotation."}, {"heading": "2 French Book Review Corpus Annotation", "text": "For creating an annotated corpus of French book reviews, OpenEdition team and we have selected 200 book reviews in French language. We have automatically segmented each review into sentences in order to annotate each sentence using Talismane2 syntax analyzer [Urieli and Tanguy, 2013]. The annotation should determine the 4 following elements:\n1. Target: a word or an expression which one can express an opinion toward it. 2. Polarity: the expressed sentiment towards the target(positive, negative or neutral). 3. Polarity terms: the words which allow us to judge the expressed sentiment (i.e. great indicates positive sentiment). 4. Category: one of the previous seven categories identified by the editorial team. 5. Occurrence: refers to the position of the target in the sentence. If the same target expression is repeated in the same sentence the first target occurrence is 1, the second repetition is 2 and so on.\n1 http://www.openedition.org/ 2 http://redac.univ-tlse2.fr/applications/talismane.html\nThree annotators have been asked to extract for each sentence all existing annotation elements. They have worked for 15 days, they annotated the same reviews, each one has annotated 7 reviews per day in average. The following box shows a part of book review. Ce livre, version pour la publication d\u2019un m\u00e9moire de DEA qui a re\u00e7u le prix Simone Genevois en\n2002, est consacr\u00e9 \u00e0 un sujet original et encore peu trait\u00e9 : le travail des conseillers historiques sur\nles films fran\u00e7ais des ann\u00e9es 1970 et 1980. Une dizaine de films sont envisag\u00e9s dans cette \u00e9tude.\nCe sont tous des films \u00ab historiques \u00bb fran\u00e7ais. L\u2019ensemble reste malgr\u00e9 tout un peu h\u00e9t\u00e9roclite\npuisque les deux films de Ren\u00e9 Allio consid\u00e9r\u00e9s (les Camisards et Moi, Pierre Rivi\u00e8re. . . ) ont \u00e9t\u00e9\nr\u00e9alis\u00e9s sans recours \u00e0 ce genre de sp\u00e9cialiste, mais l\u2019auteur s\u2019en justifie par l\u2019argument que les\nsc\u00e9narios sont tir\u00e9s d\u2019ouvrages d\u2019historiens renomm\u00e9s.\nThe automatic segmentation divides this part into sentences, then the annotators extract the annotations for each sentence as the following box shows: <review> <sentences> <sentence id=\"1\"> <text> Ce livre , version pour la publication d\u2019 un m\u00e9moire de DEA qui a re\u00e7u le prix Simone Genevois en 2002 , est consacr\u00e9 \u00e0 un sujet original et_encore peu trait\u00e9 : le travail des conseillers historiques sur les films fran\u00e7ais des ann\u00e9es 1970 et 1980 . </text> <Opinions> <Opinion target=\" livre\" category=\"presentation\" polarity=\"positive\" polarityterms=\"original ; peu traite\" occurrence=\"1\" /> </Opinions> </sentence> <sentence id=\"2\"> <text> Une dizaine de films sont envisag\u00e9s dans cette \u00e9tude . </text> <Opinions> <Opinion target=\"films\" category=\"presentation\" polarity=\"neutre\" polarityterms=\"NULL\" occurrence=\"1\" /> </Opinions> </sentence> <sentence id=\"3\"> <text> Ce sont tous des films \" historiques \" fran\u00e7ais . </text> <Opinions> <Opinion target=\"films\" category=\"presentation\" polarity=\"neutre\" polarityterms=\"NULL\" occurrence=\"1\" /> </Opinions> </sentence> <sentence id=\"4\"> <text> L\u2019 ensemble reste malgr\u00e9 tout un_peu h\u00e9t\u00e9roclite puisque les deux films de Ren\u00e9 Allio consid\u00e9r\u00e9s ( les Camisards et Moi , Pierre Rivi\u00e8re. . . ) ont \u00e9t\u00e9 r\u00e9alis\u00e9s sans recours \u00e0 ce genre de sp\u00e9cialiste , mais l\u2019 auteur s\u2019 en justifie par l\u2019 argument que les sc\u00e9narios sont tir\u00e9s d\u2019 ouvrages d\u2019 historiens renomm\u00e9s . </text> <Opinions> <Opinion target=\"ensemble\" category=\"presentation\" polarity=\"negative\" polarityterms=\"heteroclite\" occurrence=\"1\" /> <Opinion target=\"historiens\" category=\"methodology\" polarity=\"positive\" polarityterms=\"renommes\" occurrence=\"1\" /> </Opinions> </sentence> </sentences>\n</review>\nDuring the 15 days the three annotators have been annotated 97 common reviews. The first and second annotators have annotated 106 reviews while the third on has annotated 97. Table 1 shows the statistics on the annotated book reviews. We firstly count the number of targets, categories, polarities given by each annotator which represent the first three lines in Table 1. Note that the number of targets is a bit different from the number of categories or polarities because some sentences have been attributed to a category without determining a target and with or without the polarity. To measure the degree of agreement between the annotators, we have listed each possible combination among the three annotations, then the common targets, categories, polarities have been counted. We exclude 9 reviews when making the combination with the third annotator because he has annotated only 97 reviews.\nFrom the last four lines of Table 1, we remark that the number of common targets and categories is very low comparing to those produced by each annotation. The reasons may be: the annotators have different viewpoints:\n\u2013 Some annotators extract a word or an expression as a target others ignore it. \u2013 Some annotators extract the same target but use different writing (e.g. \"the man\" vs \"man\").\nThe category is a bit confused for the annotators, they attribute different categories to the same text. Obviously, the common polarity number seems to be enough acceptable for the common targets."}, {"heading": "3 Opinion Target Extraction", "text": "The objective of opinion target extraction is to extract all opinion target expressions in a book review, opinion target could be a word or multiple words. This extraction consists of the following steps:\n1. Review Segmentation This step segments each review into sentences.\n2. Sentence Tokenizing Each sentence is tokenized to get the terms. 3. Sentence Tagging Each term in the sentence should be tagged in order to be presented to a tagging classifier. We choose the IOB notation for representing each sentence in the review. Therefore, we distinguish the terms at the Beginning, the Inside and the Outside of opinion target. For example, for the following review sentence:\n\"Mais la m\u00e9thode avec laquelle il est pr\u00e9sent\u00e9 comme seule hypoth\u00e8se\nrecevable pose probl\u00e8me.\"\nWhere m\u00e9thode is a target. The tag of each word will be:\nMais:O la:O m\u00e9thode:B avec:O laquelle:O il:O est:O pr\u00e9sent\u00e9:O comme:O seule:O hypoth\u00e8se:O recevable:O pose:O probl\u00e8me:O.\n4. Feature Extraction This is the main step of opinion target extraction. We extract the following features for each term in the sentence: \u2013 the term itself. \u2013 term POS: We use Talismane parser to attach a part of speech tag to\neach term. \u2013 term shape: the shape of each character in the word (capital letter, small\nletter, digit, punctuation, other symbol) \u2013 term type: the type of the word (uppercase, digit, symbol, combination\n) \u2013 Prefixes (all prefixes having length between one to four ). \u2013 Suffixes (all suffixes having length between one to four).\nFor each term in the sentence, we make use of three group of feature values: (a) All the previous features for the term itself and the 2 and 3 previous and\nsubsequent terms, respectively. (b) the value of each two successive features in the the range -2,2 (the previ-\nous and subsequent two terms of actual word) for the following features: word surface, word POS,word shape, word type. (c) We extract the value of each three successive features in the the range -1,1 for the two features: word POS and word. 5. Training Method We have used a Conditional Random Field (CRF) as we have done in opinion target extraction in restaurant reviews."}, {"heading": "3.1 Experiments and Results", "text": "CRFsuite tool is used for this experiment with lbfgs algorithm. Table 2 shows the results of our experiments using different group of features. The first line represents the experiment when using only the terms as features which gives F1-score of 49.5%. The second line word+POS makes use of the term and POS\ntagging as features which improves the results to reach 61.2%. The third line exploits the term, POS tags, types and shape features which improve the previous run by 0.3%. The fourth line exploits all features including term, POS, shape, type and prefix and suffix which gives 61.5%. Thus, it should note that the word and POS features seem to be enough to produce a good result. The last line demonstrates the results obtaining from applying our system on the restaurant reviews provided by SemEval-2015 [Rosenthal et al., 2015]."}, {"heading": "3.2 Sentiment Polarity", "text": "For a given set of opinion targets within a sentence, we should determine whether the polarity of each opinion target is positive, negative or neutral. For example, the system should extract the polarity of m\u00e9thode in the following sentence:\n\"Mais la m\u00e9thode avec laquelle il est pr\u00e9sent\u00e9 comme seule hypoth\u00e8se recevable\npose probl\u00e8me.\" m\u00e9thode: negative\nWe propose to use a logistic regression with the following features:\n\u2013 Word n-grams Features\nUnigrams, bigrams and 3-grams are extracted for each word in the context without any stemming or stop-word removing, all terms with occurrence less than 3 are removed from the feature space.\n\u2013 Z score Features As described in [Hamdan et al., 2014a], we tested different thresholds for choosing the words which have the highest Z score, a grid search in the interval [-2..5] with step of 0.5 has been done. We found -0.5 is the best one for book reviews. Thus, we added the number of words having Z score higher than -0.5 in each class positive,negative and neutral in the restaurant and laptop sets, the two classes which have the maximum number and minimum numbers of words having Z score higher than the threshold. These 5 features have been added to the feature space."}, {"heading": "3.3 Experiments", "text": "We also trained a L1-regularized Logistic regression classifier implemented in LIBLINEAR. The classifier is trained on the training dataset using the previous features with the three polarities (positive, negative, and neutral) as labels.\nWe have used 10 fold cross-validation for evaluating our system. The results are shown in Table 3. The last two lines demonstrate the results obtaining from applying our system on the restaurant and laptop reviews provided by SemEval2015 [Rosenthal et al., 2015].\nThe first line represents the experiment which exploits only the terms as features, it gives accuracy score 70%. The remaining lines represent the experiments when exploiting the word and the Z score features, each line represents the same experiment but with a different Z threshold. We start by assigning 3 to Z score threshold and decrease this threshold until -1. The best result is given when using terms and Z score features with Z threshold of -0.5. The accuracy is 79% which seems fair enough when comparing with the results produced in restaurant reviews (about 75.5%)."}, {"heading": "4 Related Work", "text": "Aspect-Based Sentiment Analysis consists of several sub tasks. Some studies have proposed different methods for aspect detection and sentiment polarity analysis, others have proposed joint models in order to obtain the aspect and their polarities from the same model, these last models are generally unsupervised.\nThe early work on opinion target detection from on-line reviews presented by [Hu and Liu, 2004] used association rule mining based on Apriori algorithm [Agrawal and Srikant, 1994] to extract frequent noun phrases as product features. For polarity detection, they used two seed sets of 30 positive and negative\nadjectives, then WordNet has been used to find and add the synonyms of the seed words. Infrequent product features or opinion targets had been processed by finding the noun related to an opinionated word.\nOpinion Digger [Moghaddam and Ester, 2010] also used Apriori algorithm to extract the frequent opinion targets. kNN algorithm is applied to estimate the aspect rating scaling from 1 to 5 stands for (Excellent, Good, Average, Poor, Terrible).\nSupervised methods use normally Conditional Random Fields (CRF) or Hidden Markov models (HMM). [Jin and Ho, 2009] applied a HMMmodel to extract opinion targets using the words and their part-of-speech tags in order to learn a model, then unsupervised algorithm for determining the opinion targets polarity using the nearest opinion word to the opinion target and taking into account the polarity reversal words (such as not).\nA CRF model was used by [Jakob and Gurevych, 2010] with the following features: tokens, POS tags, syntactic dependency (if the opinion target has a relation with the opinionated word), word distance (the distance between the word in the closest noun phrase and the opinionated word), and opinion sentences (each token in the sentence containing an opinionated expression is labeled by this feature), the input of this method is also the opinionated expressions, they use these expressions for predicting the opinion target polarity using the dependency parsing for retrieving the pair target-expression from the training set. We also applied a CRF model with different features [Hamdan et al., 2014b,Hamdan et al., 2015].\nUnsupervised methods based on LDA (Latent Dirichlet allocation) have been proposed. [Brody and Elhadad, 2010] used LDA to figure out the opinion targets, determined the number of topics by applying a clustering method, then they used a similar method proposed by [Hatzivassiloglou and McKeown, 1997] to extract the conjunctive adjectives, but not the disjunctive due to the specificity of the domain.\n[Lin et al., 2012] proposed Joint model of Sentiment and Topic (JST) which extends the state-of-the-art topic model (LDA) by adding a sentiment layer, this model is fully unsupervised and it can detect sentiment and topic simultaneously.\n[Wei and Gulla, 2010] modeled the hierarchical relation between product aspects. They defined Sentiment Ontology Tree (SOT) to formulate the knowledge of hierarchical relationships among product attributes and tackled the problem of sentiment analysis as a hierarchical classification problem. Unsupervised hierarchical aspect Sentiment model (HASM) was proposed by [Kim et al., 2013] to discover a hierarchical structure of aspect-based sentiments from unlabeled online reviews."}, {"heading": "5 Conclusion and Future Work", "text": "We have constructed a corpus of book reviews, segmented each review into sentences and asked three annotators to extract the opinion targets and their polarities in each sentence. We trained a CRF model for opinion target extraction and\na logistic regression one for sentiment polarity. The obtaining results indicate that our systems perform as well as in restaurant reviews."}], "references": [{"title": "Semantic Evaluation (SemEval", "author": ["Hamdan et al", "H. 2015. Hamdan", "P. Bellot", "F. Bechet"], "venue": null, "citeRegEx": "al. et al\\.,? \\Q2015\\E", "shortCiteRegEx": "al. et al\\.", "year": 2015}, {"title": "Association for Computational Linguistics", "author": ["Denver", "Colorado"], "venue": null, "citeRegEx": "Denver and Colorado.,? \\Q1997\\E", "shortCiteRegEx": "Denver and Colorado.", "year": 1997}, {"title": "Opinion Digger: An Unsupervised Opinion Miner from Unstructured Product Reviews", "author": ["Moghaddam", "Ester", "S. 2010. Moghaddam", "M. Ester"], "venue": "In Proceedings of the 19th ACM International Conference on Information and Knowledge Management,", "citeRegEx": "Moghaddam et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Moghaddam et al\\.", "year": 2010}, {"title": "SemEval-2015 Task 10: Sentiment Analysis in Twitter", "author": ["Rosenthal et al", "S. 2015. Rosenthal", "P. Nakov", "S. Kiritchenko", "S.M. Mohammad", "A. Ritter", "V. Stoyanov"], "venue": "In Proceedings of the 9th International Workshop on Semantic Evaluation,", "citeRegEx": "al. et al\\.,? \\Q2015\\E", "shortCiteRegEx": "al. et al\\.", "year": 2015}, {"title": "L\u2019apport du faisceau dans l\u2019analyse syntaxique en d\u00e9pendances par transitions : \u00e9tudes de cas avec l\u2019analyseur Talismane", "author": ["Urieli", "Tanguy", "A. 2013. Urieli", "L. Tanguy"], "venue": "In Actes de la 20e confe\u0301rence sur le Traitement Automatique des Langues Naturelles", "citeRegEx": "Urieli et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Urieli et al\\.", "year": 2013}, {"title": "Sentiment learning on product reviews via sentiment ontology tree", "author": ["Wei", "Gulla", "W. 2010. Wei", "J.A. Gulla"], "venue": "In Proceedings of the 48th Annual Meeting of the ACL,", "citeRegEx": "Wei et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Wei et al\\.", "year": 2010}], "referenceMentions": [], "year": 2016, "abstractText": "So far different studies have tackled the sentiment analysis in several domains such as restaurant and movie reviews. But, this problem has not been studied in scholarly book reviews which is different in terms of review style and size. In this paper, we propose to combine different features in order to be presented to a supervised classifiers which extract the opinion target expressions and detect their polarities in scholarly book reviews. We construct a labeled corpus for training and evaluating our methods in French book reviews. We also evaluate them on English restaurant reviews in order to measure their robustness across the domains and languages. The evaluation shows that our methods are enough robust for English restaurant reviews and French book reviews.", "creator": "LaTeX with hyperref package"}}}