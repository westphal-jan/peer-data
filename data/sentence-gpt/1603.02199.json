{"id": "1603.02199", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "7-Mar-2016", "title": "Learning Hand-Eye Coordination for Robotic Grasping with Deep Learning and Large-Scale Data Collection", "abstract": "We describe a learning-based approach to hand-eye coordination for robotic grasping from monocular images. To learn hand-eye coordination for grasping, we trained a large convolutional neural network to predict the probability that task-space motion of the gripper will result in successful grasps, using only monocular camera images and independently of camera calibration or the current robot pose. This requires the network to observe the spatial relationship between the gripper and objects in the scene, thus learning hand-eye coordination. We then use this network to servo the gripper in real time to achieve successful grasps. To train our network, we collected over 800,000 grasp attempts over the course of two months, using between 6 and 14 robotic manipulators at any given time, with differences in camera placement and hardware. Our experimental evaluation demonstrates that our method achieves effective real-time control, can successfully grasp novel objects, and corrects mistakes by continuous servoing. These results provide a more efficient approach to human hand-eye coordination, and demonstrate the power of robotic grasping.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n", "histories": [["v1", "Mon, 7 Mar 2016 18:53:00 GMT  (5414kb,D)", "http://arxiv.org/abs/1603.02199v1", null], ["v2", "Thu, 24 Mar 2016 23:01:46 GMT  (6485kb,D)", "http://arxiv.org/abs/1603.02199v2", "revised related work, added additional experiments to evaluate data requirements"], ["v3", "Sat, 2 Apr 2016 23:50:24 GMT  (6485kb,D)", "http://arxiv.org/abs/1603.02199v3", "fixing some malformed citations"], ["v4", "Sun, 28 Aug 2016 23:32:37 GMT  (6486kb,D)", "http://arxiv.org/abs/1603.02199v4", "This is an extended version of \"Learning Hand-Eye Coordination for Robotic Grasping with Large-Scale Data Collection,\" ISER 2016. Draft modified to correct typo in Algorithm 1 and add a link to the publicly available dataset"]], "reviews": [], "SUBJECTS": "cs.LG cs.AI cs.CV cs.RO", "authors": ["sergey levine", "peter pastor", "alex krizhevsky", "deirdre quillen"], "accepted": false, "id": "1603.02199"}, "pdf": {"name": "1603.02199.pdf", "metadata": {"source": "META", "title": "Learning Hand-Eye Coordination for Robotic Grasping with Deep Learning and Large-Scale Data Collection", "authors": ["Sergey Levine", "Peter Pastor", "Deirdre Quillen"], "emails": ["SLEVINE@GOOGLE.COM", "PETERPASTOR@GOOGLE.COM", "AKRIZHEVSKY@GOOGLE.COM", "DEQUILLEN@GOOGLE.COM"], "sections": [{"heading": "1. Introduction", "text": "When humans and animals engage in object manipulation behaviors, the interaction inherently involves a fast feedback loop between perception and action. Even complex manipulation tasks, such as extracting a single object from a cluttered bin, can be performed with hardly any advance planning, relying instead on feedback from touch and vision. In contrast, robotic manipulation often (though not always) relies more heavily on advance planning and analysis, with relatively simple feedback, such as trajectory following, to ensure stability during execution (Srinivasa et al., 2012). Part of the reason for this is that incorporating complex sensory inputs such as vision directly into\na feedback controller is exceedingly challenging. Techniques such as visual servoing (Siciliano & Khatib, 2007) perform continuous feedback on visual features, but typically require the features to be specified by hand, and both open loop perception and feedback (e.g. via visual servoing) requires manual or automatic calibration to determine the precise geometric relationship between the camera and the robot\u2019s end-effector.\nIn this paper, we propose a learning-based approach to hand-eye coordination, which we demonstrate on a robotic grasping task. Our approach is data-driven and goalcentric: our method learns to servo a robotic gripper to\nar X\niv :1\n60 3.\n02 19\n9v 1\n[ cs\n.L G\n] 7\nM ar\n2 01\nposes that are likely to produce successful grasps, with endto-end training directly from image pixels to task-space gripper motion. By continuously recomputing the most promising motor commands, our method continuously integrates sensory cues from the environment, allowing it to react to perturbations and adjust the grasp to maximize the probability of success. Furthermore, the motor commands are issued in the frame of the robot, which is not known to the model at test time. This means that the model does not require the camera to be precisely calibrated with respect to the end-effector, but instead uses visual cues to determine the spatial relationship between the gripper and graspable objects in the scene.\nOur method consists of two components: a grasp success predictor, which uses a deep convolutional neural network (CNN) to determine how likely a given motion is to produce a successful grasp, and a continuous servoing mechanism that uses the CNN to continuously update the robot\u2019s motor commands. By continuously choosing the best predicted path to a successful grasp, the servoing mechanism provides the robot with fast feedback to perturbations and object motion, as well as robustness to inaccurate actuation.\nThe grasp prediction CNN was trained using a dataset of over 800,000 grasp attempts, collected using a cluster of similar (but not identical) robotic manipulators, shown in Figure 1, over the course of several months. Although the hardware parameters of each robot were initially identical, each unit experienced different wear and tear over the course of data collection, interacted with different objects, and used a slightly different camera pose relative to the robot base. These differences provided a diverse dataset for learning continuous hand-eye coordination for grasping.\nThe main contributions of this work are a method for learning continuous visual servoing for robotic grasping from monocular cameras, a novel convolutional neural network architecture for learning to predict the outcome of a grasp attempt, and a large-scale data collection framework for robotic grasps. Our experimental evaluation demonstrates that our convolutional neural network grasping controller achieves a high success rate when grasping in clutter on a wide range of objects, including objects that are large, small, hard, soft, deformable, and translucent. Supplemental videos of our grasping system show that the robot employs continuous feedback to constantly adjust its grasp, accounting for motion of the objects and inaccurate actuation commands. We also compare our approach to openloop alternative designs to demonstrate the importance of continuous feedback, as well as a hand-engineering grasping baseline that uses manual hand-to-eye calibration and depth sensing. Our method achieves the highest success rates in our experiments."}, {"heading": "2. Related Work", "text": "Robotic grasping is one of the most widely explored areas of manipulation. While a complete survey of grasping is outside the scope of this work, we refer the reader to standard surveys on the subject for a more complete treatment (Bohg et al., 2014). Broadly, grasping methods can be categorized as geometrically driven and data-driven. Geometric methods analyze the shape of a target object and plan a suitable grasp pose, based on criteria such as force closure (Weisz & Allen, 2012) or caging (Rodriguez et al., 2012). These methods typically need to understand the geometry of the scene, using depth or stereo sensors and matching of previously scanned models to observations (Goldfeder et al., 2009b). Data-driven methods take a variety of different forms, including purely human-supervised methods that predict grasp configurations (Herzog et al., 2014; Lenz et al., 2015) and methods that predict finger placement from geometric criteria computed offline (Goldfeder et al., 2009a). Both types of data-driven grasp selection have recently incorporated deep learning (Kappler et al., 2015; Lenz et al., 2015; Redmon & Angelova, 2015). Feedback has been incorporated into grasping primarily as a way to achieve the desired forces for force closure and other dynamic grasping criteria (Hudson et al., 2012), as well as in the form of standard servoing mechanisms, including visual servoing (described below) to servo the gripper to a pre-planned grasp pose (Kragic & Christensen, 2002). The method proposed in this work is entirely data-driven, and does not rely on any human annotation either at training or test time, in contrast to prior methods based on grasp points. Furthermore, our approach continuously adjusts the motor commands to maximize grasp success, providing continuous feedback. Comparatively little prior work has addressed direct visual feedback for grasping, most of which requires manually designed features to track the endeffector (Vahrenkamp et al., 2008; Hebert et al., 2012).\nOur approach is most closely related to recent work on self-supervised learning of grasp poses by Pinto & Gupta (2015). This prior work proposed to learn a network to predict the optimal grasp orientation for a given image patch, trained with self-supervised data collected using a heuristic grasping system based on object proposals. In contrast to this prior work, our approach achieves continuous handeye coordination by observing the gripper and choosing the best motor command to move the gripper toward a successful grasp, rather than making open-loop predictions. Furthermore, our approach does not require proposals or crops of image patches and, most importantly, does not require calibration between the robot and the camera, since the closed-loop servoing mechanism can compensate for offsets due to differences in camera pose by continuously adjusting the motor commands. We trained our method using over 800,000 grasp attempts on a very large variety of\nobjects, which is more than an order of magnitude larger than prior methods based on direct self-supervision (Pinto & Gupta, 2015) and more than double the dataset size of prior methods based on synthetic grasps from 3D scans (Kappler et al., 2015).\nAnother related area to our method is visual servoing, which addresses moving a camera or end-effector to a desired pose using visual feedback (Kragic & Christensen, 2002). In contrast to our approach, visual servoing methods are typically concerned with reaching a target pose relative to objects in the scene, and often (though not always) rely on manually designed or specified features for feedback control (Espiau et al., 1992; Wilson et al., 1996; Vahrenkamp et al., 2008; Hebert et al., 2012; Mohta et al., 2014). Photometric visual servoing uses a target image rather than features (Caron et al., 2013), and several visual servoing methods have been proposed that do not directly require prior calibration between the robot and camera (Yoshimi & Allen, 1994; Ja\u0308gersand et al., 1997; Kragic & Christensen, 2002). To the best of our knowledge, no prior learning-based method has been proposed that uses visual servoing to directly move into a pose that maximizes the probability of success on a given task (such as grasping).\nIn order to predict the optimal motor commands to maximize grasp success, we use convolutional neural networks (CNNs) trained on grasp success prediction. Although the technology behind CNNs has been known for decades (LeCun & Bengio, 1995), they have achieved remarkable success in recent years on a wide range of challenging computer vision benchmarks (Krizhevsky et al., 2012), becoming the de facto standard for computer vision systems. However, applications of CNNs to robotic control problems has been less prevalent, compared to applications to passive perception tasks such as object recognition (Krizhevsky et al., 2012; Wohlhart & Lepetit, 2015), localization (Girshick et al., 2014), and segmentation (Chen et al., 2014). Several works have proposed to use CNNs for deep reinforcement learning applications, including playing video games (Mnih et al., 2015), executing simple task-space motions for visual servoing (Lampe & Riedmiller, 2013), controlling simple simulated robotic systems (Watter et al., 2015; Lillicrap et al., 2016), and performing a variety of robotic manipulation tasks (Levine et al., 2015). Many of these applications have been in simple or synthetic domains, and all of them have focused on relatively constrained environments with small datasets."}, {"heading": "3. Overview", "text": "Our approach to learning hand-eye coordination for grasping consists of two parts. The first part is a prediction network g(It,vt) that accepts visual input It and a task-space\nmotion command vt, and outputs the predicted probability that executing the command vt will produce a successful grasp. The second part is a servoing function f(It) that uses the prediction network to continuously control the robot to servo the gripper to a success grasp. We describe each of these components below: Section 4.1 formally defines the task solved by the prediction network and describes the network architecture, Section 4.2 describes how the servoing function can use the prediction network to perform continuous control.\nBy breaking up the hand-eye coordination system into components, we can train the CNN grasp predictor using a standard supervised learning objective, and design the servoing mechanism to utilize this predictor to optimize grasp performance. The resulting method can be interpreted as a type of reinforcement learning, and we discuss this interpretation, together with the underlying assumptions, in Section 4.3.\nIn order to train our prediction network, we collected over 800,000 grasp attempts using a set of similar (but not identical) robotic manipulators, shown in Figure 1. We discuss the details of our hardware setup in Section 5.1, and discuss the data collection process in Section 5.2. To ensure generalization of the learned prediction network, the specific parameters of each robot varied in terms of the camera pose relative to the robot, providing independence to camera calibration. Furthermore, uneven wear and tear on each robot resulted in differences in the shape of the gripper fingers. Although accurately predicting optimal motion vectors in open-loop is not possible with this degree of variation, as demonstrated in our experiments, our continuous servoing method can correct mistakes by observing the outcomes of its past actions, achieving a high success rate even without knowledge of the precise camera calibration."}, {"heading": "4. Grasping with Convolutional Networks and Continuous Servoing", "text": "In this section, we discuss each component of our approach, including a description of the neural network architecture and the servoing mechanism, and conclude with an interpretation of the method as a form of reinforcement learning, including the corresponding assumptions on the structure of the decision problem."}, {"heading": "4.1. Grasp Success Prediction with Convolutional Neural Networks", "text": "The grasp prediction network g(It,vt) is trained to predict whether a given task-space motion vt will result in a successful grasp, based on the current camera observation It. In order to make accurate predictions, g(It,vt) must be able to parse the current camera image, locate the grip-\nper, and determine whether moving the gripper according to vt will put it in a position where closing the fingers will pick up an object. This is a complex spatial reasononing task that requires not only the ability to parse the geometry of the scene from monocular images, but also the ability to interpret material properties and spatial relationships between objects, which strongly affect the success of a given grasp. A pair of example input images for the network is shown in Figure 2, overlaid with lines colored accordingly to the inferred grasp success probabilities. Importantly, the movement vectors provided to the network are not transformed into the frame of the camera, which means that the method does not require hand-to-eye camera calibration. However, this also means that the network must itself infer the outcome of a task-space motor command by detemrining the orientation and position of the robot and gripper.\nData for training the CNN grasp predictor is obtained by attempting grasps using real physical robots. Each grasp consists of T time steps. At each time step, the robot records the current image Iit and the current pose p i t, and then chooses a direction along which to move the gripper. At the final time step T , the robot closes the gripper and evaluates the success of the grasp, producing a label `i. Each grasp attempt results in T training samples, given by (Iit ,p i T \u2212 pit, `i). That is, each sample includes the image observed at that time step, the vector from the current pose to the one that is eventually reached, and the success of the entire grasp. This process is illustrated in Figure 3. This procedure trains the network to predict whether moving a gripper along a given vector and then grasping will produce a successful grasp. Note that this differs from the standard reinforcement-learning setting, where the prediction is based on the current state and motor command, which in this case is given by pt+1 \u2212 pt. We discuss the inter-\npretation of this approach in the context of reinforcement learning in Section 4.3.\nThe architecture of our grasp prediction CNN is shown in Figure 4. The network takes the current image It as input, as well as an additional image I0 that is recorded before the grasp begins, and does not contain the gripper. This additional image provides an unoccluded view of the scene. The two input images are concatenated and processed by 5 convolutional layers with batch normalization (Ioffe & Szegedy, 2015), following by max pooling. After the 5th layer, we provide the vector vt as input to the network. The vector is represent by 5 values: a 3D translation vector, and a sine-cosine encoding of the change in orientation of the gripper about the vertical axis.1 To provide this vector to the convolutional network, we pass it through one fully connected layer and replicate it over the spatial dimensions of the response map after layer 5, concatenating it with the output of the pooling layer. After this concatenation, further convolution and pooling operations are applied, as described in Figure 4, followed by a set of small fully connected layers that output the probability of grasp success, trained with a cross-entropy loss to match `i, causing the network to output p(`i = 1). The input matches are 512 \u00d7 512 pixels, and we randomly crop the images to a 472\u00d7 472 region during training to provide for translation invariance.\nOnce trained the network g(It,vt) can predict the proba-\n1In this work, we only consider vertical pinch grasps, though extensions to other grasp parameterizations would be straightforward.\nbility of success of a given motor command, independently of the exact camera pose. In the next section, we discuss how this grasp success predictor can be used to continuous servo the gripper to a graspable object."}, {"heading": "4.2. Continuous Servoing", "text": "In this section, we describe the servoing mechanism f(It) that uses the grasp prediction network to choose the motor commands for the robot that will maximize the probability of a success grasp. The most basic operation for the servoing mechanism is to perform inference in the grasp predictor, in order to determine the motor command vt given an image It. The simplest way of doing this is to randomly sample a set of candidate motor commands vt and then evaluate g(It,vt), taking the command with the highest probability of success. However, we can obtain better results by running a small optimization on vt, which we perform using the cross-entropy method (CEM) (Rubinstein & Kroese, 2004). CEM is a simple derivative-free optimization algorithm that samples a batch of N values at each iteration, fits a Gaussian distribution to M < N of these samples, and then samples a new batch ofN from this Gaussian. We use N = 64 and M = 6 in our implementation, and perform three iterations of CEM to determine the best available command v?t and thus evaluate f(It). New motor commands are issued as soon as the CEM optimization completes, and the controller runs at around 2 to 5 Hz.\nOne appealing property of this sampling-based approach is that we can easily impose constraints on the types of grasps that are sampled. This can be used, for example, to incorporate user commands that require the robot to grasp in a particular location, keep the robot from grasping outside of\nthe workspace, and obey joint limits. It also allows the servoing mechanism to control the height of the gripper during each move. It is often desirable to raise the gripper above the objects in the scene to reposition it to a new location, for example when the objects move (due to contacts) or if errors due to lack of camera calibration produce motions that do not position the gripper in a favorable configuration for grasping.\nWe can use the predicted grasp success p(` = 1) produced by the network to inform a heuristic for raising and lowering the gripper, as well as to choose when to stop moving and attempt a grasp. We use two heuristics in particular: first, we close the gripper whenever the network predicts that (It, \u2205), where \u2205 corresponds to no motion, will succeed with a probability that is at least 90% of the best inferred motion v?t . The rationale behind this is to stop the grasp early if closing the gripper is nearly as likely to produce a successful grasp as moving it. The second heuristic is to raise the gripper off the table when (It, \u2205) has a probability of success that is less than 50% of v?t . The rationale behind this choice is that, if closing the gripper now is substantially worse than moving it, the gripper is most likely not positioned in a good configuration, and a large motion will be required. Therefore, raising the gripper off the table minimizes the chance of hitting other objects that are in the way. While these heuristics are somewhat ad-hoc, we found that they were effective for successfully grasping a wide range of objects in highly cluttered situations, as discussed in Section 6. Pseudocode for the servoing mechanism f(It) is presented in Algorithm 1. Further details on the servoing mechanism are presented in Appendix A.\nAlgorithm 1 Servoing mechanism f(It) 1: Given current image It and network g. 2: Infer v?t using g and CEM. 3: Evaluate p = g(It, \u2205)/g(It,v?t ). 4: if p = 0.9 then 5: Output \u2205, close gripper. 6: else if p \u2264 0.5 then 7: Modify v?t to raise gripper height and execute v?t . 8: else 9: Execute v?t . 10: end if"}, {"heading": "4.3. Interpretation as Reinforcement Learning", "text": "One interesting conceptual question raised by our approach is the relationship between training the grasp prediction network and reinforcement learning. In the case where T = 2, and only one decision is made by the servoing mechanism, the grasp network can be regarded as approximating the Q-function for the policy defined by the servoing mechanism f(It) and a reward function that is 1 when the grasp succeeds and 0 otherwise. Repeatedly deploying the latest grasp network g(It,vt), collecting additional data, and refitting g(It,vt) can then be regarded as fitted Q iteration (Antos et al., 2008). However, what happens when T > 2? In that case, fitted Q iteration would correspond to learning to predict the final probability of success from tuples of the form (It,pt+1 \u2212 pt), which is substantially harder, since pt+1 \u2212 pt doesn\u2019t tell us where the gripper will end up at the end, before closing (which is pT ).\nUsing pT \u2212pt as the action representation in fitted Q iteration therefore implies an additional assumption on the form of the dynamics. The assumption is that the actions induce a transitive relation between states: that is, that moving from p1 to p2 and then to p3 is equivalent to moving from p1 to p3 directly. This assumption does not always hold in the case of grasping, since an intermediate motion might move objects in the scene, but it is a reasonable approximation that we found works quite well in practice. The major advantage of this approximation is that fitting the Q function reduces to a prediction problem, and avoids the usual instabilities associated with Q iteration, since the previous Q function does not appear in the regression. An interesting and promising direction for future work is to combine our approach with more standard reinforcement learning formulations that do consider the effects of intermediate actions. This could enable the robot, for example, to perform nonprehensile manipulations to intentionally reorient and reposition objects prior to grasping."}, {"heading": "5. Large-Scale Data Collection", "text": "In order to collect training data to train the prediction network g(It,vt), we used between 6 and 14 robotic manipu-\nlators at any given time. An illustration of our data collection setup is shown in Figure 1. This section describes the robots used in our data collection process, as well as details of the data collection procedure."}, {"heading": "5.1. Hardware Setup", "text": "Our robotic manipulator platform consists of a lightweight 7 degree of freedom arm, a compliant, underactuated, twofinger gripper, and a camera mounted behind the arm looking over the shoulder. An illustration of a single robot is shown in Figure 5. The underactuated gripper provides some degree of compliance for oddly shaped objects, at the cost of producing a loose grip that is prone to slipping. An interesting property of this gripper was uneven ware and tear over the course of data collection, which lasted several months. Images of the grippers of various robots are shown in Figure 7, illustrating the range of variation in gripper wear and geometry. Furthermore, the cameras were mounted at slightly varying angles, providing a different viewpoint for each robot. The views from the cameras of all 14 robots during data collection are shown in Figure 6."}, {"heading": "5.2. Data Collection", "text": "We collected about 800,000 grasp attempts over the course of two months, using between 6 and 14 robots at any given point in time, without any manual annotation or supervision. The only human intervention into the data collection process was to replace the object in the bins in front of the robots and turn on the system. The data collection process\nstarted with random motor command selection and T = 3.2 When executing completely random motor commands, the robots were successful on 10% - 30% of the grasp attempts, depending on the particular objects in front of them. About half of the dataset was collected using random grasps, and the rest used the latest network fitted to all of the data collected so far. Over the course of data collection, we updated the network 4 times, and increased the number of steps from T = 3 at the beginning to T = 10 at the end.\nThe objects for grasping were chosen among common household and office items, and ranged from a 4 to 20 cm in length along the longest axis. Some of these objects are shown in Figure 6. The objects were placed in front of the robots into metal bins with sloped sides to prevent the\n2The last command is always vT = \u2205 and corresponds to closing the gripper without moving.\nobjects from becoming wedged into corners. The objects were periodically swapped out to increase the diversity of the training data.\nGrasp success was evaluated using two methods: first, we marked a grasp as successful if the position reading on the gripper was greater than 1 cm, indicating that the fingers had not closed fully. However, this method often missed thin objects, and we also included a drop test, where the robot picked up the object, recorded an image of the bin, and then dropped any object that was in the gripper. By comparing the image before and after the drop, we could determine whether any object had been picked up."}, {"heading": "6. Experiments", "text": "To evaluate our continuous grasping system, we conducted a series of quantitative experiments with novel objects that were not seen during training. The particular objects used in our evaluation are shown in Figure 8. This set of objects presents a challenging cross section of common office and household items, including objects that are heavy, such as staplers and tape dispensers, objects that are flat, such as\npost-it notes, as well as objects that are small, large, rigid, soft, and translucent.\nThe goal of our evaluation was to answer the following questions: (1) does continuous servoing significantly improve grasping accuracy and success rate? (2) how well does our learning-based system perform when compared to alternative approaches? To answer question (1), we compared our approach to an open-loop method that observes the scene prior to the grasp, extracts image patches, chooses the patch with the highest probability of a successful grasp, and then uses a known camera calibration to move the gripper to that location. This method is analogous to the approach proposed by Pinto & Gupta (2015), but uses the same network architecture as our method and the same training set. We refer to this approach as \u201copen loop,\u201d since it does not make use of continuous visual feedback. To answer question (2), we also compared our approach to a random baseline method, as well as a hand-engineered grasping system that uses depth images and heuristic positioning of the fingers. This hand-engineered system is described in Appendix B. Note that our method requires fewer assumptions than either of the two alternative methods: unlike Pinto & Gupta (2015), we do not require knowledge of the camera to hand calibration, and unlike the handengineered system, we do not require either the calibration or depth images.\nWe evaluated the methods using two experimental protocols. In the first protocol, the objects were placed into a bin in front of the robot, and it was allowed to grasp objects for 100 attempts, placing any grasped object back into the bin after each attempt. Grasping with replacement tests the ability of the system to pick up objects in cluttered settings, but it also allows the robot to repeatedly pick up easy objects. To address this shortcoming of the replacement condition, we also tested each system without replacement, as shown in Figure 8, by having it remove objects from a bin. For this condition, which we refer to as \u201cwithout replacement,\u201d we repeated each experiment 4 times, and we report success rates on the first 10, 20, and 30 grasp attempts.\nThe results are presented in Table 1. The success rate of our continuous servoing method exceeded the baseline and prior methods in all cases. For the evaluation without replacement, our method cleared the bin completely after 30 grasps on one of the 4 attempts, and had only one object left in the other 3 attempts (which was picked up on the 31st grasp attempt in 2 of the three cases, thus clearing the bin). The hand-engineered baseline struggled to accurately resolve graspable objects in clutter, since the camera was positioned about a meter away from the table, and its performance also dropped in the non-replacement case as the bin was emptied, leaving only small, flat objects that could not be resolved by the depth camera. Many practical grasping\nsystems use a wrist-mounted camera to address this difficulty (Leeper et al., 2014). In contrast, our approach did not require any special hardware modifications. The open-loop baseline was also substantially less successful. Although it benefitted from the large dataset collected by our parallelized data collection setup, which was more than an order of magnitude larger than in prior work (Pinto & Gupta, 2015), it was unable to react to perturbations, movement of objects in the scene, and variability in actuation and gripper shape.\nQualitatively, our method exhibited some interesting behaviors. Figure 9 shows the grasps that were chosen for soft and hard objects. Our system preferred to grasp softer objects by embedding the finger into the center of the object, while harder objects were grasped by placing the fingers on either side. Our method was also able to grasp a variety of challenging objects, some of which are shown\nin Figure 10. Other interesting grasp strategies, corrections, and mistakes can be seen in our supplementary video: https://youtu.be/cXaic_k80uM"}, {"heading": "7. Discussion and Future Work", "text": "We presented a method for learning hand-eye coordination for robotic grasping, using deep learning to build a grasp success prediction network, and a continuous servoing mechanism to use this network to continuously control a robotic manipulator. By training on over 800,000 grasp attempts from 14 distinct robotic manipulators with variation in camera pose, we can achieve invariance to camera calibration and small variations in the hardware. Unlike most grasping and visual servoing methods, our approach does not require calibration of the camera to the robot, instead using continuous feedback to correct any errors resulting from discrepancies in calibration. Our experimental results demonstrate that our method can effectively grasp a wide range of different objects, including novel objects not seen during training. Our results also show that our method can use continuous feedback to correct mistakes and reposition the gripper in response to perturbation and movement of objects in the scene.\nAs with all learning-based methods, our approach assumes that the data distribution during training resembles the distribution at test-time. While this assumption is reasonable for a large and diverse training set, such as the one used in this work, structural regularities during data collection can limit generalization at test time. For example, although our method exhibits some robustness to small variations in gripper shape, it would not readily generalize to new robotic platforms that differ substantially from those used during training. Furthermore, since all of our training grasp attempts were executed on flat surfaces, the proposed method is unlikely to generalize well to grasping on shelves, narrow cubbies, or other drastically different settings. These issues can be mitigated by increasing the diversity of the training setup, which we plan to explore as\nfuture work.\nOne of the most exciting aspects of the proposed grasping method is the ability of the learning algorithm to discover unconvential and nonobvious grasping strategies. We observed, for example, that the system tended to adopt a different approach for grasping soft objects, as opposed to hard ones. For hard objects, the fingers must be placed on either side of the object for a successful grasp. However, soft objects can be grasped simply by pinching into the object, which is most easily accomplised by placing one finger into the middle, and the other to the side. We observed this strategy for objects such as paper tissues and sponges. In future work, we plan to further explore the relationship between our self-supervised continuous grasping approach and reinforcement learning, in order to allow the methods to learn a wider variety of grasp strategies from large datasets of robotic experience.\nAt a more general level, our work explores the implications of large-scale data collection across multiple robotic platforms, demonstrating the value of this type of automatic large dataset construction for real-world robotic tasks. Although all of the robots in our experiments were located in a controlled laboratory environment, in the long term, this class of methods is particularly compelling for robotic systems that are deployed in the real world, and therefore are naturally exposed to a wide variety of environments, objects, lighting conditions, and wear and tear. For self-supervised tasks such as grasping, data collected and shared by robots in the real world would be the most representative of test-time inputs, and would therefore be the best possible training data for improving the real-world performance of the system. So a particularly exciting avenue for future work is to explore how our method would need to change to apply it to large-scale data collection across a large number of deployed robots engaged in real world tasks, including grasping and other manipulation skills."}, {"heading": "Acknowledgements", "text": "We would like to thank Kurt Konolige and Mrinal Kalakrishnan for additional engineering and insightful discussions, Jed Hewitt, Don Jordan, and Aaron Weiss for help with maintaining the robots, Max Bajracharya and Nicolas Hudson for providing us with a baseline perception pipeline, and Vincent Vanhoucke and Jeff Dean for support and organization."}, {"heading": "A. Servoing Implementation Details", "text": "In this appendix, we discuss the details of the inference procedure we use to infer the motor command vt with the highest probability of success, as well as additional details of the servoing mechanism.\nIn our implementation, we performed inference using three iterations of cross-entropy method (CEM). Each iteration of CEM consists of sampling 64 sample grasp directions vt from a Gaussian distribution with mean \u00b5 and covariance \u03a3, selecting the 6 best grasp directions (i.e. the 90th percentile), and refitting \u00b5 and \u03a3 to these 6 best grasps. The first iteration samples from a zero-mean Gaussian centered on the current pose of the gripper. All samples are constrained (via rejection sampling) to keep the final pose of the gripper within the workspace, and to avoid rotations of more than 180\u25e6 about the vertical axis. In general, these constraints could be used to control where in the scene the robot attempts to grasp, for example to impose user constraints and command grasps at particular locations.\nSince the CNN g(It,vt) was trained to predict the success of grasps on sequences that always terminated with the gripper on the table surface, we project all grasp directions vt to the table height (which we assume is known) before passing them into the network, although the actual grasp direction that is executed may move the gripper above the table, as shown in Algorithm 1. When the servoing algorithm commands a gripper motion above the table, we choose the height uniformly at random between 4 and 10 cm."}, {"heading": "B. Details of Hand-Engineered Grasping System Baseline", "text": "The hand-engineered grasping system baseline results reported in Table 1 were obtained using perception pipeline that made use of the depth sensor instead of the monocular camera, and required extrinsic calibration of the camera with respect to the base of the arm. The grasp configurations were computed as follows: First, the point clouds obtained from the depth sensor were accumulated into a voxel map. Second, the voxel map was turned into a 3D graph and segmented using standard graph based segmentation; individual clusters were then further segmented from top to bottom into \u201cgraspable objects\u201d based on the width and height of the region. Finally, a best grasp was computed that aligns the fingers centrally along the longer edges of the bounding box that represents the object. This grasp configuration was then used as the target pose for a taskspace controller, which was identical to the controller used for the open-loop baseline."}], "references": [{"title": "Fitted Q-Iteration in Continuous Action-Space MDPs", "author": ["A. Antos", "C. Szepesvari", "R. Munos"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Antos et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Antos et al\\.", "year": 2008}, {"title": "Photometric visual servoing for omnidirectional cameras", "author": ["G. Caron", "E. Marchand", "E. Mouaddib"], "venue": "Autonoumous Robots,", "citeRegEx": "Caron et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Caron et al\\.", "year": 2013}, {"title": "Semantic Image Segmentation with Deep Convolutional Nets and Fully Connected CRFs", "author": ["L. Chen", "G. Papandreou", "I. Kokkinos", "K. Murphy", "A.L. Yuille"], "venue": "arXiv preprint arXiv:1412.7062,", "citeRegEx": "Chen et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Chen et al\\.", "year": 2014}, {"title": "A New Approach to Visual Servoing in Robotics", "author": ["B. Espiau", "F. Chaumette", "P. Rives"], "venue": "IEEE Transactions on Robotics and Automation,", "citeRegEx": "Espiau et al\\.,? \\Q1992\\E", "shortCiteRegEx": "Espiau et al\\.", "year": 1992}, {"title": "Rich feature hierarchies for accurate object detection and semantic segmentation", "author": ["R. Girshick", "J. Donahue", "T. Darrell", "J. Malik"], "venue": "In IEEE Conference on Computer Vision and Pattern Recognition,", "citeRegEx": "Girshick et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Girshick et al\\.", "year": 2014}, {"title": "The Columbia Grasp Database", "author": ["C. Goldfeder", "M. Ciocarlie", "H. Dang", "P.K. Allen"], "venue": "In IEEE International Conference on Robotics and Automation,", "citeRegEx": "Goldfeder et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Goldfeder et al\\.", "year": 2009}, {"title": "Data-Driven Grasping with Partial Sensor Data", "author": ["C. Goldfeder", "M. Ciocarlie", "J. Peretzman", "H. Dang", "P.K. Allen"], "venue": "In IEEE/RSJ International Conference on Intelligent Robots and Systems,", "citeRegEx": "Goldfeder et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Goldfeder et al\\.", "year": 2009}, {"title": "Combined Shape, Appearance and Silhouette for Simultaneous Manipulator and Object Tracking", "author": ["P. Hebert", "N. Hudson", "J. Ma", "T. Howard", "T. Fuchs", "M. Bajracharya", "J. Burdick"], "venue": "In IEEE International Conference on Robotics and Automation,", "citeRegEx": "Hebert et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Hebert et al\\.", "year": 2012}, {"title": "Learning of Grasp Selection based on Shape-Templates", "author": ["A. Herzog", "P. Pastor", "M. Kalakrishnan", "L. Righetti", "J. Bohg", "T. Asfour", "S. Schaal"], "venue": "Autonomous Robots,", "citeRegEx": "Herzog et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Herzog et al\\.", "year": 2014}, {"title": "End-to-End Dexterous Manipulation with Deliberate Interactive Estimation", "author": ["N. Hudson", "T. Howard", "J. Ma", "A. Jain", "M. Bajracharya", "S. Myint", "C. Kuo", "L. Matthies", "P. Backes", "P. Hebert"], "venue": "In IEEE International Conference on Robotics and Automation,", "citeRegEx": "Hudson et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Hudson et al\\.", "year": 2012}, {"title": "Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift", "author": ["S. Ioffe", "C. Szegedy"], "venue": "In International Conference on Machine Learning,", "citeRegEx": "Ioffe and Szegedy,? \\Q2015\\E", "shortCiteRegEx": "Ioffe and Szegedy", "year": 2015}, {"title": "Experimental Evaluation of Uncalibrated Visual Servoing for Precision Manipulation", "author": ["M. J\u00e4gersand", "O. Fuentes", "R.C. Nelson"], "venue": "In IEEE International Conference on Robotics and Automation,", "citeRegEx": "J\u00e4gersand et al\\.,? \\Q1997\\E", "shortCiteRegEx": "J\u00e4gersand et al\\.", "year": 1997}, {"title": "Leveraging Big Data for Grasp Planning", "author": ["D. Kappler", "B. Bohg", "S. Schaal"], "venue": "In IEEE International Conference on Robotics and Automation,", "citeRegEx": "Kappler et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Kappler et al\\.", "year": 2015}, {"title": "Survey on Visual Servoing for Manipulation", "author": ["D. Kragic", "H.I. Christensen"], "venue": "Computational Vision and Active Perception Laboratory,", "citeRegEx": "Kragic and Christensen,? \\Q2002\\E", "shortCiteRegEx": "Kragic and Christensen", "year": 2002}, {"title": "ImageNet Classification with Deep Convolutional Neural Networks", "author": ["A. Krizhevsky", "I. Sutskever", "G.E. Hinton"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Krizhevsky et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Krizhevsky et al\\.", "year": 2012}, {"title": "Acquiring Visual Servoing Reaching and Grasping Skills using Neural Reinforcement Learning", "author": ["T. Lampe", "M. Riedmiller"], "venue": "In International Joint Conference on Neural Networks,", "citeRegEx": "Lampe and Riedmiller,? \\Q2013\\E", "shortCiteRegEx": "Lampe and Riedmiller", "year": 2013}, {"title": "Convolutional networks for images, speech, and time series", "author": ["Y. LeCun", "Y. Bengio"], "venue": "The Handbook of Brain Theory and Neural Networks,", "citeRegEx": "LeCun and Bengio,? \\Q1995\\E", "shortCiteRegEx": "LeCun and Bengio", "year": 1995}, {"title": "Using Near-Field Stereo Vision for Robotic Grasping in Cluttered Environments", "author": ["A. Leeper", "K. Hsiao", "E. Chu", "J.K. Salisbury"], "venue": "In Experimental Robotics,", "citeRegEx": "Leeper et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Leeper et al\\.", "year": 2014}, {"title": "Deep Learning for Detecting Robotic Grasps", "author": ["I. Lenz", "H. Lee", "A. Saxena"], "venue": "The International Journal of Robotics Research,", "citeRegEx": "Lenz et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Lenz et al\\.", "year": 2015}, {"title": "End-to-end Training of Deep Visuomotor Policies", "author": ["S. Levine", "C. Finn", "T. Darrell", "P. Abbeel"], "venue": "arXiv preprint arXiv:1504.00702,", "citeRegEx": "Levine et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Levine et al\\.", "year": 2015}, {"title": "Continuous control with deep reinforcement learning", "author": ["T. Lillicrap", "J. Hunt", "A. Pritzel", "N. Heess", "T. Erez", "Y. Tassa", "D. Silver", "D. Wierstra"], "venue": "In International Conference on Learning Representations,", "citeRegEx": "Lillicrap et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Lillicrap et al\\.", "year": 2016}, {"title": "Human-level control through deep reinforcement learning", "author": ["V Mnih"], "venue": "Nature, 518(7540):529\u2013533,", "citeRegEx": "Mnih,? \\Q2015\\E", "shortCiteRegEx": "Mnih", "year": 2015}, {"title": "Vision Based Control of a Quadrotor for Perching on Planes and Lines", "author": ["K. Mohta", "V. Kumar", "K. Daniilidis"], "venue": "In IEEE International Conference on Robotics and Automation,", "citeRegEx": "Mohta et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Mohta et al\\.", "year": 2014}, {"title": "Supersizing self-supervision: Learning to grasp from 50k tries and 700 robot", "author": ["L. Pinto", "A. Gupta"], "venue": "hours. CoRR,", "citeRegEx": "Pinto and Gupta,? \\Q2015\\E", "shortCiteRegEx": "Pinto and Gupta", "year": 2015}, {"title": "Real-time grasp detection using convolutional neural networks", "author": ["J. Redmon", "A. Angelova"], "venue": "In IEEE International Conference on Robotics and Automation,", "citeRegEx": "Redmon and Angelova,? \\Q2015\\E", "shortCiteRegEx": "Redmon and Angelova", "year": 2015}, {"title": "From Caging to Grasping", "author": ["A. Rodriguez", "M.T. Mason", "S. Ferry"], "venue": "The International Journal of Robotics Research,", "citeRegEx": "Rodriguez et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Rodriguez et al\\.", "year": 2012}, {"title": "The Cross-Entropy Method: A Unified Approach to Combinatorial Optimization, Monte-Carlo Simulation, and Machine Learning", "author": ["R. Rubinstein", "D. Kroese"], "venue": null, "citeRegEx": "Rubinstein and Kroese,? \\Q2004\\E", "shortCiteRegEx": "Rubinstein and Kroese", "year": 2004}, {"title": "Visual Servoing for Humanoid Grasping and Manipulation Tasks", "author": ["N. Vahrenkamp", "S. Wieland", "P. Azad", "D. Gonzalez", "T. Asfour", "R. Dillmann"], "venue": "In 8th IEEE-RAS International Conference on Humanoid Robots,", "citeRegEx": "Vahrenkamp et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Vahrenkamp et al\\.", "year": 2008}, {"title": "Embed to Control: A Locally Linear Latent Dynamics Model for Control from Raw Images", "author": ["M. Watter", "J. Springenberg", "J. Boedecker", "M. Riedmiller"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Watter et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Watter et al\\.", "year": 2015}, {"title": "Pose Error Robust Grasping from Contact Wrench Space Metrics", "author": ["J. Weisz", "P.K. Allen"], "venue": "In IEEE International Conference on Robotics and Automation,", "citeRegEx": "Weisz and Allen,? \\Q2012\\E", "shortCiteRegEx": "Weisz and Allen", "year": 2012}, {"title": "Relative End-Effector Control Using Cartesian Position Based Visual Servoing", "author": ["W.J. Wilson", "Hulls", "C.W. Williams", "G.S. Bell"], "venue": "IEEE Transactions on Robotics and Automation,", "citeRegEx": "Wilson et al\\.,? \\Q1996\\E", "shortCiteRegEx": "Wilson et al\\.", "year": 1996}, {"title": "Learning Descriptors for Object Recognition and 3D Pose Estimation", "author": ["P. Wohlhart", "V. Lepetit"], "venue": "In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition,", "citeRegEx": "Wohlhart and Lepetit,? \\Q2015\\E", "shortCiteRegEx": "Wohlhart and Lepetit", "year": 2015}, {"title": "Active, uncalibrated visual servoing", "author": ["B.H. Yoshimi", "P.K. Allen"], "venue": "In IEEE International Conference on Robotics and Automation,", "citeRegEx": "Yoshimi and Allen,? \\Q1994\\E", "shortCiteRegEx": "Yoshimi and Allen", "year": 1994}], "referenceMentions": [{"referenceID": 25, "context": "Geometric methods analyze the shape of a target object and plan a suitable grasp pose, based on criteria such as force closure (Weisz & Allen, 2012) or caging (Rodriguez et al., 2012).", "startOffset": 159, "endOffset": 183}, {"referenceID": 8, "context": "Data-driven methods take a variety of different forms, including purely human-supervised methods that predict grasp configurations (Herzog et al., 2014; Lenz et al., 2015) and methods that predict finger placement from geometric criteria computed offline (Goldfeder et al.", "startOffset": 131, "endOffset": 171}, {"referenceID": 18, "context": "Data-driven methods take a variety of different forms, including purely human-supervised methods that predict grasp configurations (Herzog et al., 2014; Lenz et al., 2015) and methods that predict finger placement from geometric criteria computed offline (Goldfeder et al.", "startOffset": 131, "endOffset": 171}, {"referenceID": 12, "context": "Both types of data-driven grasp selection have recently incorporated deep learning (Kappler et al., 2015; Lenz et al., 2015; Redmon & Angelova, 2015).", "startOffset": 83, "endOffset": 149}, {"referenceID": 18, "context": "Both types of data-driven grasp selection have recently incorporated deep learning (Kappler et al., 2015; Lenz et al., 2015; Redmon & Angelova, 2015).", "startOffset": 83, "endOffset": 149}, {"referenceID": 9, "context": "Feedback has been incorporated into grasping primarily as a way to achieve the desired forces for force closure and other dynamic grasping criteria (Hudson et al., 2012), as well as in the form of standard servoing mechanisms, including visual servoing (described below) to servo the gripper to a pre-planned grasp pose (Kragic & Christensen, 2002).", "startOffset": 148, "endOffset": 169}, {"referenceID": 27, "context": "Comparatively little prior work has addressed direct visual feedback for grasping, most of which requires manually designed features to track the endeffector (Vahrenkamp et al., 2008; Hebert et al., 2012).", "startOffset": 158, "endOffset": 204}, {"referenceID": 7, "context": "Comparatively little prior work has addressed direct visual feedback for grasping, most of which requires manually designed features to track the endeffector (Vahrenkamp et al., 2008; Hebert et al., 2012).", "startOffset": 158, "endOffset": 204}, {"referenceID": 12, "context": "objects, which is more than an order of magnitude larger than prior methods based on direct self-supervision (Pinto & Gupta, 2015) and more than double the dataset size of prior methods based on synthetic grasps from 3D scans (Kappler et al., 2015).", "startOffset": 226, "endOffset": 248}, {"referenceID": 3, "context": "In contrast to our approach, visual servoing methods are typically concerned with reaching a target pose relative to objects in the scene, and often (though not always) rely on manually designed or specified features for feedback control (Espiau et al., 1992; Wilson et al., 1996; Vahrenkamp et al., 2008; Hebert et al., 2012; Mohta et al., 2014).", "startOffset": 238, "endOffset": 346}, {"referenceID": 30, "context": "In contrast to our approach, visual servoing methods are typically concerned with reaching a target pose relative to objects in the scene, and often (though not always) rely on manually designed or specified features for feedback control (Espiau et al., 1992; Wilson et al., 1996; Vahrenkamp et al., 2008; Hebert et al., 2012; Mohta et al., 2014).", "startOffset": 238, "endOffset": 346}, {"referenceID": 27, "context": "In contrast to our approach, visual servoing methods are typically concerned with reaching a target pose relative to objects in the scene, and often (though not always) rely on manually designed or specified features for feedback control (Espiau et al., 1992; Wilson et al., 1996; Vahrenkamp et al., 2008; Hebert et al., 2012; Mohta et al., 2014).", "startOffset": 238, "endOffset": 346}, {"referenceID": 7, "context": "In contrast to our approach, visual servoing methods are typically concerned with reaching a target pose relative to objects in the scene, and often (though not always) rely on manually designed or specified features for feedback control (Espiau et al., 1992; Wilson et al., 1996; Vahrenkamp et al., 2008; Hebert et al., 2012; Mohta et al., 2014).", "startOffset": 238, "endOffset": 346}, {"referenceID": 22, "context": "In contrast to our approach, visual servoing methods are typically concerned with reaching a target pose relative to objects in the scene, and often (though not always) rely on manually designed or specified features for feedback control (Espiau et al., 1992; Wilson et al., 1996; Vahrenkamp et al., 2008; Hebert et al., 2012; Mohta et al., 2014).", "startOffset": 238, "endOffset": 346}, {"referenceID": 1, "context": "Photometric visual servoing uses a target image rather than features (Caron et al., 2013), and several visual servoing methods have been proposed that do not directly require prior calibration between the robot and camera (Yoshimi & Allen, 1994; J\u00e4gersand et al.", "startOffset": 69, "endOffset": 89}, {"referenceID": 11, "context": ", 2013), and several visual servoing methods have been proposed that do not directly require prior calibration between the robot and camera (Yoshimi & Allen, 1994; J\u00e4gersand et al., 1997; Kragic & Christensen, 2002).", "startOffset": 140, "endOffset": 215}, {"referenceID": 14, "context": "Although the technology behind CNNs has been known for decades (LeCun & Bengio, 1995), they have achieved remarkable success in recent years on a wide range of challenging computer vision benchmarks (Krizhevsky et al., 2012), becoming the de facto standard for computer vision systems.", "startOffset": 199, "endOffset": 224}, {"referenceID": 14, "context": "However, applications of CNNs to robotic control problems has been less prevalent, compared to applications to passive perception tasks such as object recognition (Krizhevsky et al., 2012; Wohlhart & Lepetit, 2015), localization (Girshick et al.", "startOffset": 163, "endOffset": 214}, {"referenceID": 4, "context": ", 2012; Wohlhart & Lepetit, 2015), localization (Girshick et al., 2014), and segmentation (Chen et al.", "startOffset": 48, "endOffset": 71}, {"referenceID": 2, "context": ", 2014), and segmentation (Chen et al., 2014).", "startOffset": 26, "endOffset": 45}, {"referenceID": 28, "context": ", 2015), executing simple task-space motions for visual servoing (Lampe & Riedmiller, 2013), controlling simple simulated robotic systems (Watter et al., 2015; Lillicrap et al., 2016), and performing a variety of robotic manipulation tasks (Levine et al.", "startOffset": 138, "endOffset": 183}, {"referenceID": 20, "context": ", 2015), executing simple task-space motions for visual servoing (Lampe & Riedmiller, 2013), controlling simple simulated robotic systems (Watter et al., 2015; Lillicrap et al., 2016), and performing a variety of robotic manipulation tasks (Levine et al.", "startOffset": 138, "endOffset": 183}, {"referenceID": 19, "context": ", 2016), and performing a variety of robotic manipulation tasks (Levine et al., 2015).", "startOffset": 64, "endOffset": 85}, {"referenceID": 0, "context": "Repeatedly deploying the latest grasp network g(It,vt), collecting additional data, and refitting g(It,vt) can then be regarded as fitted Q iteration (Antos et al., 2008).", "startOffset": 150, "endOffset": 170}, {"referenceID": 17, "context": "systems use a wrist-mounted camera to address this difficulty (Leeper et al., 2014).", "startOffset": 62, "endOffset": 83}], "year": 2016, "abstractText": "We describe a learning-based approach to handeye coordination for robotic grasping from monocular images. To learn hand-eye coordination for grasping, we trained a large convolutional neural network to predict the probability that task-space motion of the gripper will result in successful grasps, using only monocular camera images and independently of camera calibration or the current robot pose. This requires the network to observe the spatial relationship between the gripper and objects in the scene, thus learning hand-eye coordination. We then use this network to servo the gripper in real time to achieve successful grasps. To train our network, we collected over 800,000 grasp attempts over the course of two months, using between 6 and 14 robotic manipulators at any given time, with differences in camera placement and hardware. Our experimental evaluation demonstrates that our method achieves effective real-time control, can successfully grasp novel objects, and corrects mistakes by continuous servoing.", "creator": "LaTeX with hyperref package"}}}