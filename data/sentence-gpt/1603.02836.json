{"id": "1603.02836", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "9-Mar-2016", "title": "Faster learning of deep stacked autoencoders on multi-core systems using synchronized layer-wise pre-training", "abstract": "Deep neural networks are capable of modelling highly non-linear functions by capturing different levels of abstraction of data hierarchically. While training deep networks, first the system is initialized near a good optimum by greedy layer-wise unsupervised pre-training. However, with burgeoning data and increasing dimensions of the architecture, the time complexity of this approach becomes enormous. Also, greedy pre-training of the layers often turns detrimental by over-training a layer causing it to lose harmony with the rest of the network. In this paper a synchronized parallel algorithm for pre-training deep networks on multi-core machines has been proposed. Different layers are trained by parallel threads running on different cores with regular synchronization. Thus the pre-training process becomes faster and chances of over-training are reduced. This is experimentally validated using a stacked autoencoder for dimensionality reduction of MNIST handwritten digit database. The proposed algorithm achieved 26\\% speed-up compared to greedy layer-wise pre-training for achieving the same reconstruction accuracy substantiating its potential as an alternative.\n\n\n\nThis paper is a first in the series.", "histories": [["v1", "Wed, 9 Mar 2016 10:31:00 GMT  (91kb,D)", "http://arxiv.org/abs/1603.02836v1", null]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["anirban santara", "debapriya maji", "dp tejas", "pabitra mitra", "arobinda gupta"], "accepted": false, "id": "1603.02836"}, "pdf": {"name": "1603.02836.pdf", "metadata": {"source": "CRF", "title": "Faster learning of deep stacked autoencoders on multi-core systems using synchronized layer-wise pre-training", "authors": ["Anirban Santara", "Debapriya Maji", "DP Tejas", "Pabitra Mitra", "Arobinda Gupta"], "emails": ["santara@iitkgp.ac.in"], "sections": [{"heading": null, "text": "Keywords: Deep learning, stacked autoencoder, RBM, backpropagation, contrastive divergence, MNIST"}, {"heading": "1 Introduction", "text": "Artificial neural networks that have more than three hidden layers of neurons are called Deep Neural Networks (DNN) [1]. Every layer of a trained DNN represents a higher level abstraction of the input data than the previous one. DNNs are highly successful and provide the current state-of-the art results in many AI tasks. Stacked autoencoder is a class of DNN that is used for unsupervised representation learning and dimensionality reduction [2]. However, training deep architectures faced challenges ever since their introduction. The cost function being non-linear, gradient based routines usually get stuck in poor local minima for\nPresented as a paper at the Workshop on Parallel and Distributed Computing for Knowledge Discovery in Databases (PDCKDD 2015) as a part of European Conference on Machine Learning and Principles and Practices of Knowledge\nDiscovery in Databases (ECML/PKDD 2015)\nar X\niv :1\n60 3.\n02 83\n6v 1\n[ cs\n.L G\n] 9\nlarge initialization weights. On the other hand, with small initial weights backpropagated error gradients in the layers close to the input become tiny resulting in no appreciable training of those layers [3]. Greedy layer-wise pre-training was introduced with a motivation to approximately capture statistical abstractions of the input data in every layer and initialize the network parameters in a region near a good local optimum [4, 5]. This was followed by fine-tuning via backpropagation to push the solution deeper into the optimum. This approach of training deep architectures has brought great success in many areas of application [2, 3].\nThe time complexity of this process becomes forbiddingly large for big datasets and architectures. Hence there have been efforts at parallelization. These parallel algorithms fall in two broad categories [6]. The algorithms of the first category partition the neural network and train the parts in parallel. The neural network can be physically partitioned [7] or logically partitioned [8, 9]. The challenge is to ensure that the computational load is evenly distributed among the parallel processors and data communication is minimized. The second category of algorithms carry out calculations relative to the entire network but specific to subsets of data (single patterns or small batches of data) at different processing nodes [10]. This method is typically suitable for computer clusters as this requires very little communication. Each computing node i, generates its own update vector \u2206wi on the basis of the data-set Di assigned to it. At the end of an epoch when all the nodes are ready, the overall \u2206w is calculated by averaging the outputs of each of the nodes.\nAs mentioned earlier, the pre-training phase is crucial for convergence but to the best of our knowledge, all existing methodologies perform pre-training layer-bylayer in a greedy way. In this work we propose a synchronized parallel layer-wise pre-training algorithm. The layers are learnt on parallel running threads that are synchronized regularly by cascading of maturity information. It achieves faster learning due to parallelism and reduces chances of misfit among the layers owing to synchronization.\nSection 2 briefly introduces some theoretical concepts relevant to stacked autoencoders. The synchronized layer-wise pre-training algorithm is described in Section 3. Section 4 presents an experimental methodology to compare the proposed algorithm with greedy layer-wise pre-training and Section 5 discusses the results of the experiment. Finally Section 6 concludes the paper with a summary of the work."}, {"heading": "2 Preliminaries of stacked autoencoder", "text": "In this section we describe stacked autoencoder and state briefly about the tools and methodologies used in their training."}, {"heading": "2.1 Autoencoder", "text": "An autoencoder is a type of neural network that is operated in unsupervised learning mode mainly for representation learning and dimensionality reduction [11]. It is aimed at encoding input x into a representation c(x) so that the input can be reconstructed from the representation with minimum amount of deformation. Given a set of unlabelled training examples {x(1),x(2), ...} the autoencoder sets the outputs y(i) = x(i) and tries to learn a function hW,b such that hW,b(x) = x\u0302 \u2248 x.\nA stacked autoencoder [2, 3] has multiple hidden layers of neurons between the input and output layers. A stacked autoencoder of depth n has an input layer, 2n\u2212 3 hidden layers, and an output layer of neurons. Layers 1, 2, ..., n comprise the encoder and the remaining layers form the decoder. The nth layer is called the code layer.\nAutoencoder is usually trained by the backpropagation algorithm with an error criterion like mean squared error or cross entropy error [11]."}, {"heading": "2.2 Restricted Boltzmann Machine (RBM)", "text": "Restricted Boltzmann Machine (RBM) is commonly used for initializing deep neural networks like stacked autoencoder. An RBM is a Markov Random Field associated with an undirected graph that consists of m visible units V = (V1, . . . , Vm) comprising the visible layer and n hidden units H = (H1, . . . ,Hn) comprising the hidden layer [2, 12]. There exists no connection among the units of the same layer in the graph. The joint probability distribution associated with the model is given by the Gibbs distribution:\np(v,h) = 1\nZ e\u2212E(v,h) (1)\nwith the energy function:\nE(v,h) = \u2212 n\u2211\ni=1 m\u2211 j=1 wi,jhivj \u2212 m\u2211 j=1 bjvj \u2212 n\u2211 i=1 cihi (2)\nThe absence of connections among the nodes of the same layer implies that the hidden variables are conditionally independent given the state of the visible variables and vice versa:\np(h|v) = n\u220f\ni=1\np(hi|v)\nand\np(v|h) = m\u220f i=1 p(vi|h) (3)\nWe can use an RBM to model unknown probability distributions. The parameters are trained by gradient ascent on the log-likelihood of given data using Markov Chain Monte Carlo methods like Contrastive Divergence [13]."}, {"heading": "2.3 Greedy Layer-wise pre-training", "text": "Due to the presence of numerous local optima, proper initialization of the parameters is crucial for training deep neural networks. Greedy layer-wise pre-training was introduced with a motivation to roughly capture statistical abstractions of the input data in every layer and initialize the network parameters in a region near a good local optimum [5], [3]. Figure 1 outlines the greedy layer-wise pretraining algorithm.\nLet us consider a deep neural network with K hidden layers of neurons. Let the input dimension be Ni, output dimension be No and the hidden layer dimensions be N1, N2, . . . , NK . Let us denote the training set by S. Greedy layer-wise pre-training follows one of the two methodologies described below.\nPre-training using RBM: In this approach, first an RBM with Ni visible nodes and N1 hidden nodes is trained. After training, the hidden weights and biases are used to initialize the incoming weights and biases respectively of the first hidden layer of the deep neural network. The input data of dimensionality Ni is transformed to the dimension N1 of the hidden layer. For an RBM with Bernoulli hidden units this is done by calculating the activation probabilities of the hidden units corresponding to the input examples v \u2208 S as follows:\np(Hi = 1|v) = \u03c3  m\u2211 j=1 wi,jvj + ci  (4) Now another RBM with N1 visible units and N2 hidden units is trained with the transformed data to find weights and biases which go and initialize the second hidden layer. The data is transformed to the dimension of the second hidden layer and this procedure is repeated for all of the hidden layers of the deep neural network.\nPre-training using autoencoder: Deep networks can also be initialized using autoencoders. In this methodology, single hidden-layer autoencoders are trained for each of the layers of the deep architecture and their weights and biases are used to initialize the corresponding layers of the deep architecture."}, {"heading": "3 Synchronized layer-wise pre-training", "text": "This section describes the proposed algorithm for accelerated training of stacked autoencoders. Greedy layer-wise pre-training makes a layer wait for all the previous layers to finish learning before it can start itself and for all the following layers to finish after it has finished. The guiding philosophy of the proposed algorithm is to reduce the idle time of greedy layer-wise pre-training by introducing\nparallelism with synchronization. This algorithm is suitable for multi-core machines where different layers can learn in parallel on threads running in different cores of the CPU."}, {"heading": "3.1 Methodology", "text": "Let Ll denote the l th layer of an autoencoder of depth K. The thread assigned to pre-train Ll is denoted by Tl. The training and validation data for the n th epoch of its pre-training are denoted by DTl [n] and D V l [n] respectively. Let the activation function for Ll be fl(.) and the incoming weights and biases after the nth epoch be Wl[n] and bl[n] respectively [\u2200l = 1, 2, . . . ,K].\nThe training and validation data for the deep network are directly used for pretraining L1 and correspond to D T 1 [0] and D V 1 [0] respectively. These are considered mature right from the beginning and are kept constant throughout. \u2200n > 0:\nDT1 [n] = D T 1 [0]\nDV1 [n] = D V 1 [0] (5)\nAs the training and validation data for T1 are readily available, the algorithm starts with T1 beginning to learn the first layer of the autoencoder.\n\u2013 Due to data dependency Tl waits until Tl\u22121 has completed one epoch of training. Once Tl starts executing, every time it completes one epoch, it transforms DTl and D V l using the current weights and biases W l[n] and\nbl[n] and modifies DTl+1 and D V l+1 as:\nDTl+1[m+ 1] = fl(W l[n] \u2217DTl [n] + b l[n])\nDVl+1[m+ 1] = fl(W l[n] \u2217DVl [n] + b l[n]) (6)\nTl executes for a stipulated Nl epochs specified by the user. Cascading of knowledge synchronizes the threads and promotes harmonious training of the layers.\n\u2013 After completing Nl epochs, Tl goes to sleep. If Tl\u22121 modifies D T l and D V l\nafter that, Tl wakes up, executes one iteration of learning with the modified data and goes back to sleep. \u2013 Pre-training ends when all the threads have finished their stipulated number of iterations. \u2013 After pre-training has been completed, the entire architecture can be finetuned by backpropagation learning.\nPre-training can be done using RBM or autoencoder. Though described in terms of an autoencoder, this algorithm can also be used for pre-training other deep neural networks meant for classification, regression, etc."}, {"heading": "4 Experimental methodology", "text": "The synchronized layer-wise pre-training algorithm was tested on the task of dimensionality reduction of MNIST handwritten digit dataset using a stacked autoencoder. The reconstruction accuracy was measured using a mean squared error (MSE) crterion."}, {"heading": "4.1 Dataset", "text": "MNIST3 is a database of (8-bit) 28x28 gray-scale handwritten digits, popularly used for training and testing image processing and machine learning systems. The database contains 60, 000 training examples and 10, 000 test examples. For our experiments, the training set was further divided into 50, 000 training examples and 10, 000 validation examples. The validation set contained 1000 examples from each class of digits (0\u22129) which ensures equal participation from all of the classes. The training set was randomized and divided into minibatches of size 100 before every new epoch of pre-training and fine-tuning. Figure 5(a) shows some sample digits from the dataset.\n3 Available for free download at http://yann.lecun.com/exdb/mnist/"}, {"heading": "4.2 Parameters of the stacked autoencoder", "text": "Details of the architecture of the stacked autoencoder have been presented in Table 1. This is same as the architcture used by [3] for the same task with the only exception of sigmoid activation functions for all the layers."}, {"heading": "4.3 Parameters of the learning algorithms", "text": "The learning rate of both the weights and biases was set at 0.1 for contrastive divergence and 0.001 for backpropagation. In addition a momentum of 0.5 for the first 5 epochs and 0.9 for the remaining epochs was used in contrastive divergence learning.\nThe performances of greedy layer-wise pre-training (the baseline) [3] and the proposed synchronized layer-wise pre-training algorithm were compared on a system with 8 CPU cores (dual-hyperthreaded quad-core) and 8GB RAM in terms of reconstruction error calculated as the average squared reconstruction error per digit and the total training time. The baseline experiment was performed with 20 epochs of RBM pre-training for every layer followed by 10 epochs of fine-tuning of the entire architecture via backpropagation. Next, the proposed algorithm was executed for the same amount of pre-training and fine-tuning but the threads L2, L3 and L4 carried out 5, 20 and 40 additional epochs of pre-training respectively, every time the previous thread modified their data."}, {"heading": "5 Results and discussion", "text": "This section presents the results of the comparison and discusses a few aspects of the proposed algorithm."}, {"heading": "5.1 Convergence", "text": "Figure 6 shows the variation of reconstruction error of the validation set during the execution of the proposed algorithm. Table 2 gives the final reconstruction errors for the two experiments. Figure 5 compares the reconstructions of 25 randomly chosen samples from the test set by the two methods. The performances are almost equivalent."}, {"heading": "5.2 Speed-up", "text": "Table 5.1 compares the execution times of the two algorithms. The proposed algorithm achieves the same performance 1h26m49s faster than greedy layer-wise pre-training which is a 26.17% speedup. The four threads, T1 through T4 were run on four different cores of the same CPU. The time for communication of data among the threads was observed to be 5 to 6 orders of magnitude lesser than the time for an epoch of RBM pre-training.\nIn our experiment pre-training of the stacked autoencoder was concluded as soon as the RBM for the first layer (784\u2212 1000) completed its stipulated 20 epochs. Though it changed DT2 and D V 2 after the 20\nth epoch, the parameters of L2 or any of its data-dependent layers were not updated. This was just to keep the total pre-training time equal to the time taken for L1 (the largest and the slowest to train layer) to complete its prescribed amount of pre-training. Performing an extra round of update for L2, L3 and L4 prolonged the training but did not\nimprove the final reconstruction accuracy."}, {"heading": "5.3 Analysis of the space-time diagram", "text": "Figure 3 shows the space-time diagram for the proposed algorithm. The variation of reconstruction errors of the training and validation examples have been plotted for each layer. Both the errors show a decreasing trend for the first three layers indicating the parameters of these layers, getting tuned to the given data. Initially the training and validation errors differ for all the layers but eventually converge to almost equal values. This indicates that the parameters achieve generalization beyond the training set in the course of pre-training.\nThe behavior of the fourth layer is particularly interesting. Figure 4 zooms into its timing diagram (marked with a dotted rectangle in Figure 3). The validation and training errors differ by a large margin for quite sometime (throughout the stipulated amount of pre-training and beyond) but eventually converge as T4 updates its parameters every time D T 4 and D V 4 are changed by T3. Also both the training and validation errors show an increasing trend throughout the process. Low initial reconstruction error that increases with the progress of training indicates that the random values to which the weights and biases of L4 were initialized, overfit the data that was provided by L3 initially. Significant difference between the training and validation errors during this period of time further corroborates the overfitting. However with the progress of training, as the data matures, the overfitting slowly cures and the training and validation\nerrors converge to close values. The increasing trend of the reconstruction error implicitly indicates how synchronization forces the innermost layer to increase its own error to conform to the remaining of the network. The behavior of the fourth layer also indicates the necessity of updating the parameters even after the stipulated amount of pre-training has been completed. As the training data of a hidden layer changes throughout the process of pre-training, the parameters must also be updated regularly to keep up with the changing data. This does not add an overhead to the total pre-training time which is always dictated by the time taken by the largest (or the most complex) layer to complete the stipulated number of epochs of pre-training."}, {"heading": "6 Conclusion", "text": "In this work the existing methods of parallelizing deep architecture learning have been explored and a new algorithm for parallelizing the pre-training phase has been proposed. The convergence and performance of the algorithm have been tested on the task of dimensionality reduction of MNIST handwritten digit database using stacked autoencoder and 26% speed-up has been observed for achieving the same performance. The key feature of the proposed algorithm is that it is not greedy. This reduces chances of misfit among the layers, a common hazard of greedy learning and minimizes the idle time for each layer. The proposed algorithm can be seamlessly integrated with the existing methods of parallelization which mainly concentrate on the fine-tuning phase."}], "references": [{"title": "Why Does Unsupervised Pre-training Help Deep Learning ?", "author": ["D. Erhan", "A. Courville", "P. Vincent"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2010}, {"title": "Learning Deep Architectures for AI", "author": ["Y. Bengio"], "venue": "now, 2009,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2009}, {"title": "Reducing the Dimensionality of Data with", "author": ["G.E. Hinton", "R.R. Salakhutdinov"], "venue": "Neural Networks,\u201d Science,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2006}, {"title": "A fast learning algorithm for deep belief nets,", "author": ["G.E. Hinton", "S. Osindero"], "venue": "Neural computation,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2006}, {"title": "Greedy Layer-Wise Training of Deep Networks,", "author": ["Y. Bengio", "P. Lamblin", "D. Popovici", "H. Larochelle"], "venue": "Proceedings of Neural Information Processing Systems,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2007}, {"title": "Performance analysis of a pipelined backpropagation parallel algorithm,", "author": ["A. Petrowski", "C. Girault"], "venue": "IEEE Transactions on Neural Networks,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 1993}, {"title": "Pipelined Back-Propagation for Context-Dependent Deep Neural Networks,", "author": ["X. Chen", "A. Eversole", "G. Li", "D. Yu", "F. Seide"], "venue": null, "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2012}, {"title": "Deep Convex Net : A Scalable Architecture for Speech Pattern Classification,", "author": ["L. Deng", "D. Yu"], "venue": "ISCA, no. August,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2011}, {"title": "Parallelization of Deep Networks,", "author": ["M.D. DeGrazia", "I. Stoianov", "M. Zorzi", "M. De Filippo De Grazia"], "venue": "European Symposium on Artificial Neural Networks, Computational Intelligence and Machine Learning,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2012}, {"title": "Neural networks: A comprehensive foundation", "author": ["S. Haykin"], "venue": null, "citeRegEx": "11", "shortCiteRegEx": "11", "year": 1998}, {"title": "An Introduction to Restricted Boltzmann Machines,", "author": ["A. Fischer", "C. Igel"], "venue": "Progress in Pattern Recognition, Image Analysis2,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2012}, {"title": "Training Products of Experts by Minimizing Contrastive Divergence,", "author": ["G.E. Hinton"], "venue": "Neural computation,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2002}], "referenceMentions": [{"referenceID": 0, "context": "Artificial neural networks that have more than three hidden layers of neurons are called Deep Neural Networks (DNN) [1].", "startOffset": 116, "endOffset": 119}, {"referenceID": 1, "context": "Stacked autoencoder is a class of DNN that is used for unsupervised representation learning and dimensionality reduction [2].", "startOffset": 121, "endOffset": 124}, {"referenceID": 2, "context": "On the other hand, with small initial weights backpropagated error gradients in the layers close to the input become tiny resulting in no appreciable training of those layers [3].", "startOffset": 175, "endOffset": 178}, {"referenceID": 3, "context": "Greedy layer-wise pre-training was introduced with a motivation to approximately capture statistical abstractions of the input data in every layer and initialize the network parameters in a region near a good local optimum [4, 5].", "startOffset": 223, "endOffset": 229}, {"referenceID": 4, "context": "Greedy layer-wise pre-training was introduced with a motivation to approximately capture statistical abstractions of the input data in every layer and initialize the network parameters in a region near a good local optimum [4, 5].", "startOffset": 223, "endOffset": 229}, {"referenceID": 1, "context": "This approach of training deep architectures has brought great success in many areas of application [2, 3].", "startOffset": 100, "endOffset": 106}, {"referenceID": 2, "context": "This approach of training deep architectures has brought great success in many areas of application [2, 3].", "startOffset": 100, "endOffset": 106}, {"referenceID": 5, "context": "These parallel algorithms fall in two broad categories [6].", "startOffset": 55, "endOffset": 58}, {"referenceID": 6, "context": "The neural network can be physically partitioned [7] or logically partitioned [8, 9].", "startOffset": 49, "endOffset": 52}, {"referenceID": 7, "context": "The neural network can be physically partitioned [7] or logically partitioned [8, 9].", "startOffset": 78, "endOffset": 84}, {"referenceID": 8, "context": "The second category of algorithms carry out calculations relative to the entire network but specific to subsets of data (single patterns or small batches of data) at different processing nodes [10].", "startOffset": 193, "endOffset": 197}, {"referenceID": 9, "context": "An autoencoder is a type of neural network that is operated in unsupervised learning mode mainly for representation learning and dimensionality reduction [11].", "startOffset": 154, "endOffset": 158}, {"referenceID": 1, "context": "A stacked autoencoder [2, 3] has multiple hidden layers of neurons between the input and output layers.", "startOffset": 22, "endOffset": 28}, {"referenceID": 2, "context": "A stacked autoencoder [2, 3] has multiple hidden layers of neurons between the input and output layers.", "startOffset": 22, "endOffset": 28}, {"referenceID": 9, "context": "Autoencoder is usually trained by the backpropagation algorithm with an error criterion like mean squared error or cross entropy error [11].", "startOffset": 135, "endOffset": 139}, {"referenceID": 1, "context": ",Hn) comprising the hidden layer [2, 12].", "startOffset": 33, "endOffset": 40}, {"referenceID": 10, "context": ",Hn) comprising the hidden layer [2, 12].", "startOffset": 33, "endOffset": 40}, {"referenceID": 11, "context": "The parameters are trained by gradient ascent on the log-likelihood of given data using Markov Chain Monte Carlo methods like Contrastive Divergence [13].", "startOffset": 149, "endOffset": 153}, {"referenceID": 4, "context": "Greedy layer-wise pre-training was introduced with a motivation to roughly capture statistical abstractions of the input data in every layer and initialize the network parameters in a region near a good local optimum [5], [3].", "startOffset": 217, "endOffset": 220}, {"referenceID": 2, "context": "Greedy layer-wise pre-training was introduced with a motivation to roughly capture statistical abstractions of the input data in every layer and initialize the network parameters in a region near a good local optimum [5], [3].", "startOffset": 222, "endOffset": 225}, {"referenceID": 2, "context": "This is same as the architcture used by [3] for the same task with the only exception of sigmoid activation functions for all the layers.", "startOffset": 40, "endOffset": 43}, {"referenceID": 2, "context": "The performances of greedy layer-wise pre-training (the baseline) [3] and the proposed synchronized layer-wise pre-training algorithm were compared on a system with 8 CPU cores (dual-hyperthreaded quad-core) and 8GB RAM in terms of reconstruction error calculated as the average squared reconstruction error per digit and the total training time.", "startOffset": 66, "endOffset": 69}], "year": 2016, "abstractText": "Deep neural networks are capable of modelling highly nonlinear functions by capturing different levels of abstraction of data hierarchically. While training deep networks, first the system is initialized near a good optimum by greedy layer-wise unsupervised pre-training. However, with burgeoning data and increasing dimensions of the architecture, the time complexity of this approach becomes enormous. Also, greedy pre-training of the layers often turns detrimental by over-training a layer causing it to lose harmony with the rest of the network. In this paper a synchronized parallel algorithm for pre-training deep networks on multi-core machines has been proposed. Different layers are trained by parallel threads running on different cores with regular synchronization. Thus the pre-training process becomes faster and chances of overtraining are reduced. This is experimentally validated using a stacked autoencoder for dimensionality reduction of MNIST handwritten digit database. The proposed algorithm achieved 26% speed-up compared to greedy layer-wise pre-training for achieving the same reconstruction accuracy substantiating its potential as an alternative.", "creator": "LaTeX with hyperref package"}}}