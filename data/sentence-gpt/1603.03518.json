{"id": "1603.03518", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "11-Mar-2016", "title": "High-dimensional Black-box Optimization via Divide and Approximate Conquer", "abstract": "Divide and Conquer (DC) is conceptually well suited to high-dimensional optimization by decomposing a problem into multiple small-scale sub-problems. However, appealing performance can be seldom observed when the sub-problems are interdependent. This paper suggests that the major difficulty of tackling interdependent sub-problems lies in the precise evaluation of a partial solution (to a sub-problem), which can be overwhelmingly costly and thus makes sub-problems non-trivial to conquer. Thus, it is not a desirable solution to inter-problem problems, and there is more to it.\n\n\nThis paper reviews the current proposal for solving the problem problem by using the \u201cEfficient Complementary Analysis\u2021 approach to investigate the use of an algorithm to quantify the complexity of a particular solution. It proposes the algorithm to demonstrate a \u201cEfficient Complementary Analysis\u2021 approach to solve the problem by using the\u201cEfficient Complementary Analysis\u2021 approach to investigate the use of an algorithm to quantify the complexity of a particular solution. It proposes the algorithm to demonstrate a \u201cEfficient Complementary Analysis\u2021 approach to solve the problem by using the\u201cEfficient Complementary Analysis\u2021 approach to investigate the use of an algorithm to quantify the complexity of a particular solution. It proposes the algorithm to demonstrate a \u201cEfficient Complementary Analysis\u2021 approach to solve the problem by using the\u201cEfficient Complementary Analysis\u2021 approach to investigate the use of an algorithm to quantify the complexity of a particular solution. It proposes the algorithm to demonstrate a \u201cEfficient Complementary Analysis\u2021 approach to solve the problem by using the\u201cEfficient Complementary Analysis\u2021 approach to solve the problem by using the\u201cEfficient Complementary Analysis\u2021 approach to solve the problem by using the\u201cEfficient Complementary Analysis\u2021 approach to solve the problem by using the\u201cEfficient Complementary Analysis\u2021 approach to solve the problem by using the\u201cEfficient Complementary Analysis\u2021 approach to solve the problem by using the\u201cEfficient Complementary Analysis\u2021 approach to solve the problem by using the\u201cEfficient Complementary Analysis\u2021 approach to solve the problem by using the\u201cEfficient Complementary Analysis\u2021 approach to solve the problem by using the\u201cEfficient Complementary Analysis\u2021 approach to solve the problem by using the\u201cE", "histories": [["v1", "Fri, 11 Mar 2016 04:50:59 GMT  (2890kb,D)", "http://arxiv.org/abs/1603.03518v1", "7 pages, 2 figures, conference"], ["v2", "Mon, 21 Mar 2016 02:06:09 GMT  (2890kb,D)", "http://arxiv.org/abs/1603.03518v2", "7 pages, 2 figures, conference"]], "COMMENTS": "7 pages, 2 figures, conference", "reviews": [], "SUBJECTS": "cs.AI", "authors": ["peng yang", "ke tang", "xin yao"], "accepted": false, "id": "1603.03518"}, "pdf": {"name": "1603.03518.pdf", "metadata": {"source": "CRF", "title": "A Novel Divide and Conquer Approach for Large-scale Optimization Problems", "authors": ["Peng Yang", "Ke Tang", "Xin Yao"], "emails": ["trevor@mail.ustc.edu.cn;", "ketang@ustc.edu.cn;", "x.yao@cs.bham.ac.uk"], "sections": [{"heading": "1 Introduction", "text": "Developing Artificial Intelligence (AI) applications often encounters a key task of solving challenging optimization problems. Formally, it can be stated as: x\u2217 = argmaxx\u2208X f(x), where f : X \u2192 R denotes a function on a bounded solution space X \u2286 RD and x\u2217 denotes the global optimum in X . We consider f as a black-box function that the problem information is completely unknown beforehand, where only the function value of x \u2208 X can be directly provided if explicitly queried. Therefore, only derivative-free approaches can be brought to bear, which, however, often suffer from the curse of dimensionality if D is considerably large.\nAn intuitive idea to handle a high-dimensional optimization problem is to project its solution space onto lower dimensions, where traditional approaches perform well [Kaba\u0301n et al., 2015]. However, it is nontrivial to identify an appropriate projection. Typical approaches in this category, e.g., Random Embedding techniques [Wang et al., 2013; Qian and Yu, 2016], consider the high-dimensional problem having low effective dimensionality, for which a random projection would suffice to find the global optimal solution of a\nhigh-dimensional problem in a low-dimensional space. Although these algorithms also showed appealing performance in case the low effective dimensionality assumption is mildly violated, their performance may not be satisfactory on the wider range of irreducible problems.\nDivide-and-Conquer (DC) is another general idea for tackling large-scale problems. In case of high-dimensional blackbox optimization, DC can be implemented by dividing the original problem into multiple sub-problems (say of dimensionality di, where i = 1, ...,M , and di < D) [Yang et al., 2008a]. For the i-th sub-problem, x is optimized along di dimensions, while its values on the otherD\u2212di dimensions are fixed. That is, each sub-problem concerns a low-dimensional subspace of the original solution space. Applying an existing search method to the i-th sub-problem leads to a didimensional partial solution to the original high-dimensional problem. The solution to original problem can be obtained by combining the partial solutions achieved on all sub-problems.\nGiven an appropriate sub-problems optimizer, the abovedescribed DC strategy works well on the so-called separable problems, for which the global optimal optimum can be found by optimizing one dimension at a time regardless of the values taken on the other dimensions [Chen et al., 2010]. If this condition does not hold, the performance of DC heavily relies on the decomposition method [Omidvar et al., 2014], which aims to divide a black-box high-dimensional problem in such a way that the global optimum can still be obtained by solving the sub-problems in a fully independent manner. In the past few years, a large variety of decomposition methods have been proposed [Mahdavi et al., 2015]. Despite the performance enhancement brought by them, none of these methods are guaranteed to achieve the desired sub-problems. Meanwhile, a practical problem of interest may be fully nonseparable such that the ideal decomposition mentioned above does not even exist. Therefore, how to deal with (conquer) the interdependent sub-problems remains a challenge to DC in the context of high-dimensional optimization.\nIn this paper, we suggest that the major challenge of tackling interdependent sub-problems lies in the difficulty of evaluating the quality of a partial solution (to a sub-problem) during the search course. To be specific, as the quality of a partial solution (to the original problem) depends on the values taken on the dimensions involved in other sub-problems, precisely evaluating a partial solution requires overwhelming compu-\nar X\niv :1\n60 3.\n03 51\n8v 1\n[ cs\n.A I]\n1 1\nM ar\n2 01\n6\ntation cost, which increases exponentially with the number of interacting variables in other sub-problems. We propose an approximation approach for partial solution evaluation, which yields a novel framework, named Divide and Approximate Conquer (DAC). With DAC, the computational cost increases polynomially with the number of solutions generated in each iteration while the convergence to global optimum (of the original problem) is still guaranteed.\nThe major difficulty of tackling interdependent subproblems is analyzed in Section 2. The proposed DAC is detailed in Section 3. Section 4 reports empirical studies of DAC on five synthetic high-dimensional optimization problems and the hyper-parameters fine-tuning task for multiclass SVM. Section 5 concludes this work."}, {"heading": "2 Major challenge of dealing with interdependent sub-problems", "text": "The DC strategy consists of three steps: 1) (Divide) Decompose a problem into M di-dimensional\nsub-problems, where i = 1, ...,M and di < D; 2) (Conquer) Search the best partial solution in each sub-\nproblem by applying an existing search approach; 3) Merge the best partial solutions obtained on all sub-\nproblems as the final output. We restrict our discussions here to the Conquer phase. Usually, a derivative-free search process in each sub-problem is guided by the solutions with better function values, despite a few exceptions [Kirkpatrick et al., 1983; Tang et al., 2016]. Before a di-dimensional partial solution is evaluated, it must be complemented to D-dimensional by fixing the values for the variables in other sub-problems. Specially, we call a vector of such fixed D \u2212 di values as a complement to a partial solution. A partial solution will receive different function values with different complements. Fortunately, the indeterminate function values of partial solutions will not influence the search process unless the rank of partial solutions changes, on which the search direction is actually determined. The rank of partial solutions may change if sub-problems are interdependent, which is defined as follows: Definition 1. (Interacting Variables) [Chen et al., 2010] Given a D-dimensional problem, its i-th and j-th variables are said to be interacting, if the rank of two partial solutions xi and x\u2032i in the i-th dimension may change with different complements, e.g., xj and x\u2032j , in the j-th variable:\n\u2203x, x\u2032i, x\u2032j : f(x1, ..., xi, ..., xj , ..., xD) < f(x1, ..., x \u2032 i, ..., xj , ..., xD)\u2227 f(x1, ..., xi, ..., x \u2032 j , ..., xD) > f(x1, ..., x \u2032 i, ..., x \u2032 j , ..., xD).\nDefinition 2. (Interdependent Sub-problems) Given two arbitrary sub-problems, they are said to be interdependent if at least one variable in a sub-problem is interacting with at least one variable in the other sub-problem. Intuitively, two interacting variables of the 2-D Schwefel function are depicted in Figure 1, where the rank of two partial solutions x1 and x\u20321 in the first dimension varies by fixing different values in the second dimension, i.e., x2 and x\u20322.\nIf a problem is separable, where no interdependency exists between sub-problems, partial solutions can be complemented by arbitrary identical values in X , without perturbing their rank. However, for many non-separable problems, the sub-problems are interdependent, where the rank of partial solutions significantly relies on their complements. As a result, the search direction has a close relation to the choice of complements. Hence, the choice of complements to partial solutions should be carefully addressed. Otherwise, the search process in a sub-problem will run the risk of being misled, eventually resulting in an ineffective search.\nUnfortunately, we find that how to accurately complement partial solutions is a difficult optimization problem. Lemma 1. (The Difficulty of Accurately Complementing) Given a set of partial solutions in the di-dimensional i-th\nsub-problem, let D\u0302i be the set of all D variables except the ones in the i-th sub-problem, |D\u0302i| be the cardinality of D\u0302i, P D\u0302i(j) be the probability of fixing a correct value for the\nj-th variable in D\u0302i, and P be the arithmetic mean of all P D\u0302i(j)\n, j = 1, ..., D \u2212 di, then the probability of accurately complementing those partial solutions so that they can be correctly ranked, denoted as Pi, is:\nPi = |D\u0302i|\u220f j=1 P D\u0302i(j)\n\u2264 (\u2211|D\u0302i| j=1 PD\u0302i(j)\n|D\u0302i|\n)|D\u0302i| = PD\u2212di (1)\nProof. By Definition 1, we learn that fixing the correct values for variables are independent events. By Definition 2, we know that variables in the same sub-problem are non-\ninteracting. Thus, we can directly have Pi = \u220f|D\u0302i| j=1 PD\u0302i(j). Finally, according to the AM-GM Inequality, we have Eq.(1).\nNotice that, P D\u0302i(j) = 1 if the D\u0302i(j)-th variable is not interacting with any variable in the i-th sub-problem. Lemma 1 shows that the computational cost of accurately complementing partial solutions increases exponentially with the number of variables that are interacting with the current sub-problem. Hence, the interdependent sub-problems cannot be accurately conquered within a reasonable time budget."}, {"heading": "3 Divide and Approximate Conquer", "text": ""}, {"heading": "3.1 The Accurate Complement", "text": "According to Lemma 1, only the brute-force method is applicable to accurately complement partial solutions on interdependent sub-problems. However, the required computational costs are beyond being acceptable. On the other hand, it is an effective way to derive the mathematical formulation of a problem by observing its corresponding brute-force method, as such a method usually has to scan the whole solution space and thus reflects the problem characteristics naturally.\nThe core idea of the brute-force method is mathematically described as follow ( the maximization case is considered):\n\u2200xi \u2208 Si : f(x\u2217) = f(x\u2217i , x\n\u2217 r ) = maxxr\u2208Sr f(x\u2217i , xr) \u2265 maxxr\u2208Sr f(xi, xr), (2)\nwhere xi denotes a partial solution in the i-th sub-problem, i = 1, ...,M , and xr denotes its candidate complement, where r = [1, ..., i \u2212 1, i + 1, ...,M ] denotes all the sub-problems except the i-th one. Si and Sr denote the corresponding subspace of the solution space subjecting to |Si| \u00b7 |Sr| = |S|, where S \u2286 X and x\u2217 = (x\u2217i , x\u2217r ) is the optimal solution in S.\nEq.(2) states a fact that the correct rank of partial solutions can be obtained by comparing their best function values among all combinations with all possible complements. On this basis, we mathematically define the problem of accurately complementing partial solutions as follow: Definition 3. (The Accurate Complement) Given arbitrary xi \u2208 Si, a complement x\u2020r \u2208 Sr is said to be the accurate complement \u21d0\u21d2 x\u2020r = argmax xr\u2208Sr f(xi, xr).\nNotice that, every partial solution has its own accurate complement. To identify the accurate complement, the combinations of xi and all possible complements in Sr should be evaluated by f , among which the complement with the largest function value is chosen."}, {"heading": "3.2 DAC: an approximate approach to DC", "text": "In fact, Definition 3 allows us to approximate the accurate complements of partial solutions with only a limited set of candidate complements S\u2032r \u2286 Sr. That is,\nx\u2020r = argmax xr\u2208Sr f(xi, xr) x\u0303\u2020r = argmax x\u2032r\u2208S\u2032r\u2286Sr f(xi, x\u2032r). (3)\nwhere x\u2020r x\u0303\u2020r means that x \u2020 r is more accurate than x\u0303 \u2020 r , since,\nmax xr\u2208Sr f(xi, xr) \u2265 max x\u2032r\u2208S\u2032r\u2286Sr f(xi, x\u2032r). (4)\nBased on Eq.(4), it is reasonable to assume that good approximate complements will perturb the rank of partial solutions slightly. Eq.(3) thus gives rise to the proposed Divide and Approximate Conquer (DAC), as shown in Algorithm 1.\nDAC shares the same framework as the basic DC. The only difference between them is that: while complementing a partial solution, DAC always selects the complement associated with the largest function value among a given set of candidate\nAlgorithm 1 DAC(f , Tmax, N ) 1: Randomly initialize N solutions x1:N . 2: Divide f into M sub-problems. 3: For t = 1 to Tmax 4: For i = 1 to M 5: x\u20321:N ;i = SearchOperator(x1:N ;i). 6: For j = 1 to N 7: x\u0303\u2020j;r = argmax\nxr\u2208x1:N;r f(xj;i, xr).\n8: x\u0303\u2032 \u2020 j;r = argmax\nxr\u2208x1:N;r f(x\u2032j;i, xr).\n9: EndFor 10: x1:N ;i \u2190 {x1:N ;i; x\u20321:N ;i | x\u0303 \u2020 1:N ;r; x\u0303\u2032 \u2020 1:N ;r} . 11: x1:N ;r \u2190 {x\u0303\u20201:N ;r; x\u0303\u2032 \u2020 1:N ;r}. 12: EndFor 13: EndFor 14: Output the best solution found so far.\ncomplements. Specifically, DAC works by first randomly initializing N solutions x1:N (step 1). The problem f is decomposed into M sub-problems with a certain decomposition strategy (step 2). After that, without loss of generality, let us consider the i-th sub-problem. N new partial solutions x\u20321:N ;i are generated by applying some search operator to the current ones, i.e., x1:N ;i (step 5). To identify the approximate complement to the j-th partial solution xj;i, j = 1, ..., N , all the combinations of xj;i and the vectors of partial solutions in other sub-problems x1:N ;r will be evaluated by f . The vector associated with the largest function value is chosen as the approximate complement x\u0303\u2020j;r to xj;i (step 7). The same strategy is used to obtain the approximate complement x\u0303\u2032 \u2020 j;r to the j-th new partial solution x\u2032j;i (step 8). After that, according to a certain selection criterion, N partial solutions will be remained for the next iteration (step 10). Notice that, the selection of a partial solution is conditioned by its corresponding approximate complement. At last, for the j-th selected partial solution xj;i, its corresponding partial solutions in the rest sub-problems xj;r will be replaced with its approximate complement for further optimizing (step 11).\nAs a result, DAC consumes 2MN2 Function Evaluations (FEs) in each iteration, which is a significant reduction to the brute force method. Meanwhile, albeit the computational time is cut down, the convergence of DAC is still guaranteed. Lemma 2. (The Convergence of DAC) Given a search algorithm that can converge to the global optimum of each sub-problem (regarded as independent problems), DAC can approximately converge to the global optimum x\u2217 of the original problem.\nProof. With out loss of generality, let us consider the function values of the j-th solution at the t-th and t+1-th iteration, i.e., f(xtj) and f(x t+1 j ). For the i-th sub-problem, we have:\nf(xtj;i, x t j;r) \u2264 max xtr\u2208xt1:N;r f(xtj;i, x t r) \u2264 max xtr\u2208xt1:N;r f(xt+1j;i , x t r).\n(5)\nwhere the first \u2019\u2264\u2019 indicates the procedure of identifying the approximate complements for the current partial solutions (step 7 in Algorithm 1), and the second \u2019\u2264\u2019 represents the procedures of generating new partial solutions (step 5 in Algorithm 1), identifying their approximate complements (step 8 in Algorithm 1), and selecting better ones from candidate partial solutions, conditioned by their approximate complements (step 10 in Algorithm 1).\nThen by repeating Eq.(5) for i = 1, ...,M , we have that:\nf(xtj;1, ..., x t j;M ) \u2264 f(xt+1j;1 , ..., x t+1 j;M ), (6)\nwhich means the function value of the j-th solution f(xtj) monotonically increases with the iteration index t.\nNote that, the equality of Eq.(6) holds in two cases: 1) The approximate complement of a partial solution hap-\npens to be its corresponding partial solutions in the rest sub-problems, i.e., xj;r = x\u0303 \u2020 j;r;\n2) The search algorithm fails to produce new better solutions, i.e., max\nxr\u2208Sr f(xj;i, xr) \u2265 max xr\u2208Sr f(x\u2032j;i, xr).\nThe first case actually explains the term \u201dapproximate\u201d in DAC, as it happens at a probability of at least 1N . Hence, if the sub-problems optimizer of DAC can optimally solve each sub-problem separately, the global optimum value f(x\u2217) can be approximately approached by DAC."}, {"heading": "3.3 DAC-HC: an instantiation of DAC", "text": "An instantiation of DAC is presented to illustrate the detail steps of a DAC algorithm and for further empirical studies.\nTo instantiate DAC, both the decomposition strategy and the sub-problems optimizer should be specified. In order to highlight the advantages of the DAC framework further in empirical studies, the improvement of performance introduced by these two specified components should be kept to minimal. On this basis, we first decompose a problem f via random grouping [Yang et al., 2008a]. That is, in the beginning of each iteration, M equal-sized sub-problems are randomly generated. For the sub-problems optimizer, a Parallel Hill Climbing (PHC) method is employed, which thus yields the DAC-Hill Climbing (DAC-HC). The DAC-HC conducts N RLS processes on each sub-problem. Specifically, at each iteration of the i-th sub-problem, the j-th RLS produces one new partial solution x\u2032j;i by applying the Gaussian mutation operator to the current partial solution xj;i, using Eq.(7):\nx\u2032j;i = xj;i + I \u00b7 N (0, \u03c3j;i). (7) where N (0, \u03c3j;i) denotes a Gaussian random variable with zero mean and standard deviation \u03c3j;i, and I is the identity matrix of size di. Generally, the value of \u03c3j;i represents the search step-size that can be adaptively varied during the search and may also be distinct over RLSs or even dimensions. To keep it simple, all RLSs in DAC-HC are initially set to the same search step-size, i.e., 1.00. After that, each search step-size is adapted at every iteration in terms of the 1/5 successful rule [Kern et al., 2004], using Eq.(8):\n\u03c3j;i = \u03c3j;i \u00d7 exp 1\u221a D+1 (If(x\u2032j;i ,\u0303x\u2020j;r)\u2265f(xj;i ,\u0303x\u2020j;r) \u2212 1 5 ), (8)\nAlgorithm 2 DAC-HC(f , Tmax, N,M ) 1: Randomly initialize N solutions x1:N . 2: For t = 1 to Tmax 3: Randomly divide f into M equal-sized sub-problems. 4: For i = 1 to M 5: For j = 1 to N 6: x\u2032j;i = xj;i + I \u00b7 N (0, \u03c3j;i). 7: x\u0303\u2020j;r = argmax\nxr\u2208x1:N;r f(xj;i, xr).\n8: \u03c3j;i = \u03c3j;i\u00d7exp 1\u221a\nD+1 (If(x\u2032j;i ,\u0303x\u2020j;r)\u2265f(xj;i ,\u0303x\u2020j;r)\u2212 1 5 ).\n9: xj;i \u2190 {xj;i, x\u2032j;i | x\u0303 \u2020 j;r}.\n10: xj;r \u2190 x\u0303\u2020j;r. 11: EndFor 12: EndFor 13: EndFor 14: Output the best solution found so far.\nwhere Ia is a indicator function that returns 1 if a is true and 0 otherwise.\nDuring the selection procedure, each new partial solution in PHC only competes with its corresponding old one for survival. Based on this one-on-one relation, we further reduce the FEs consumption of DAC-HC to a half of DAC, i.e., MN2. This is conducted by letting two competing partial solutions share the same approximate complement. The reason behind this is that, by adopting RLSs with small search stepsizes, pairwise partial solutions can be close to each other in the solution space, in which case their respective approximate complements may also be similar. Lastly, the pseudo-code of DAC-HC is given in Algorithm 2 for illustration."}, {"heading": "4 Empirical Studies", "text": "DAC is proposed for solving non-separable high-dimensional optimization problems. That is where the empirical studies should concentrate on to verify the effectiveness of DAC-HC. For this purpose, two sets of non-separable high-dimensional optimization problems are employed."}, {"heading": "4.1 Varied numbers of interacting variables tests", "text": "The first set of problems is based on the fully non-separable functions, i.e., Schwefel\u2019s 1.2 and Rosenbrock [Tang et al., 2009], which are formulated as: fsch(x) = \u2211D i=1( \u2211i j=1 xj) 2\nand fros(x) = \u2211D\u22121 i=1 [100(x 2 i \u2212 xi+1)2 +(xi\u2212 1)2]. In these two functions, all variables are interacting. Meanwhile, it has also been observed that, in many real-world problems, only parts of variables are interacting [Friesen and Domingos, 2015]. Hence, it is a necessity to test DAC-HC with varied numbers of interacting variables. For this purpose, we further consider three problems that combine Schwefel\u2019s 1.2 function and the fully separable sphere function, i.e., fsph(x) = \u2211D i=1 x 2 i , in different formations. The dimensionality is set to 1000 for all 5 problems. All variables are randomly perturbed to avoid any potential bias. All problems are expected to be minimized to the global optimal value 0.00.\nSpecifically, f1(x) consists of a group of 50 interacting variables and 950 independent variables. f2(x) has 10 groups of 50 interacting variables and 500 independent variables. f3(x) and f5(x) compose of 20 groups of 50 interacting variables. f4(x) involves a group of 1000 interacting variables. The detailed formulations are listed in Table 1.\nOn these five problems, two groups of comparisons are conducted for different purposes.\nAdvantages of DAC-HC over existing approaches In the first group of comparison, DAC-HC is compared with CMA-ES [Hansen and Ostermeier, 2001], RESOO [Qian and Yu, 2016], and DECC-I [Omidvar et al., 2014], which are representatives of three basic ideas for high-dimensional optimization: the straightforward method, dimensionality reduction, and DC, respectively. Specifically, CMA-ES is widely endorsed as a powerful global optimizer that has been applied in many aspects. Here the basic version of CMA-ES is utilized. RESOO is a recently proposed approach built on the Random Embedding for reducing dimensionality. It should be noted that, DECC-I is not an algorithm for black-box optimization. It is an ideal approach that perfectly decomposes the problems using the priori knowledge of functions. Hence, all sub-problems of DECC-I are independent, while it is not the case for DAC-HC. Besides, the sub-problems of DECC-I are optimized by a variant of Differential Evolution [Yang et al., 2008b], which has been empirically observed more advanced than the employed RLSs [Tang et al., 2016]. On this basis, if DAC-HC outperforms DECC-I, it is reasonable to infer that the proposed DAC facilitates DC on non-separable high-dimensional optimization problems.\nAll algorithms are repeated 25 runs on each problem to diminish the noise introduced by their randomized search essence. The time budget for each run is set to 3e6 FEs. CMA-ES is parameterless that no parameter needs to be specified. For RESOO, after some coarse-tuning, the probability \u03b7 is set to 1/3, the restart times is set to 10, and the reduced dimensionality is set to 100. For DECC-I, the only parameter, i.e., population size N , is set to 100 as Omidvar et al. [2014] suggested. For DAC-HC, two parameters should be specified, i.e., the number of RLSs N and the number of sub-problems M . Recall that, the parameter N > 1 generally influences the approximate ability of DAC. To test the extreme case of DAC-HC, it is set to 2. To gain a relatively fair comparison with RESOO, we set M = 10 so that each sub-problems optimizer faces a 100-dimensional problem as RESOO does.\nThe mean and standard derivation of the final outputs in 25 runs are shown in Table 2. A gray cell indicates an algorithm achieves the best mean value on a problem, while a light gray cell indicates a second place. DAC-HC outperforms all the compared algorithms on f1, f4 and f5. On f2 and f3, though slightly inferior to CMA-ES, DAC-HC performs significantly better than DECC-I and RESOO. Since DAC-HC dominates DECC-I on all five problems, the effectiveness of DAC for promoting DC on non-separable high-dimensional optimization problems is confirmed. Besides, Lemma 1 is verified by observing that the solution qualities of DAC-HC deteriorates as the number of interacting variables increases. RESOO performs poorly because the tested problems are irreducible.\nEmpirical support to the convergence of DAC In the second group of comparison, the PHC with 2 RLSs is compared. The only difference between PHC and DAC-HC is that PHC complements a partial solution xj;i merely with the corresponding partial solutions in the rest sub-problems xj;r. On this basis, PHC does not satisfy Eq.(5), and its convergence is not guaranteed. The experimental protocol is set the same to the first group of comparison.\nThe convergence rates of both algorithms are shown in Figure 2, where the x-axis denotes the FEs and y-axis denotes the logarithm of function values. It can be seen that, DAC-HC always converges faster than PHC. Specially, log-linear convergence of DAC-HC is observed on the first two sphere function based problems. It should be noted that, the employed RLS has also been theoretically proved to converge log-linearly on the sphere function [Jebalia et al., 2008]. This coincidence actually supports the convergence of DAC stated in Lemma 2. Comparatively, the convergence rates of PHC are heavily retarded due to the unfit complements to partial solutions."}, {"heading": "4.2 Hyper-parameter tuning for multi-class SVMs", "text": "Given a set of labelled data {xi, yi}li=1, where xi \u2208 Rn, the classification task is to train a classifier in terms of {xi, yi}li=1 to predict the labels of incoming data. Support Vector Machines (SVMs) [Vapnik, 1998] is often considered as a family of powerful tools for classification. Here the SVM with linear kernel is considered. Let y \u2208 {\u22121,+1}, SVM requires to fine-tune three parameters w, b, \u03bb by solving the following optimization problem: minw,b,\u03bb 12w Tw + \u03bb \u2211l i=1 \u03bei, subject to yi(wT xi + b) \u2265 1 \u2212 \u03bei \u2227 \u03bei \u2265 0. Notice that, \u03bb > 0 is a hyper-parameter supplied by the user, which\npenalizes the error vector \u03be. When dealing with a multiclass classification problem, a typical idea is to divide it into multiple binary classification problems by adopting the oneon-one strategy. Let K be the number of class, then we have ( K 2 ) = K(K\u22121)2 binary classifiers to train, resulting in K(K\u22121) 2 hyper-parameters to tune.\nOf course, a simple strategy of specifying the same value of \u03bb for all binary classifiers works well [Chang and Lin, 2011]. It is also an intuition that varied \u03bb can facilitate a multi-class SVM better. On this basis, a potentially high-dimensional optimization problem needs to be solved for more advanced performance. Recall that, a final output of a multi-class SVM is based on the majority voting. Due to the overfitting risk on each binary classifier, the votes may introduce interdependencies in between. To sum up, the problem of hyperparameter tuning for multi-class SVMs is non-separable and high-dimensional in essence.\nWe thus apply the proposed DAC-HC to deal with it. RESOO and CMA-ES are again included as the compared algorithms. Since the ideal decomposition is no longer applicable for this problem, DECC-I will not be compared with DACHC. The grid search is also tested but on the assumption that all binary classifiers share the same value of \u03bb. That is , the grid search actually solves a 1-dimensional problem. Three data sets, i.e., usps [Hull, 1994], news20 [Lang, 1995], and letter [Hsu and Lin, 2002], are used for comparison, which contain 10, 20, 26 classes in each and yield three problems with 45, 190, 325 dimensions, respectively. All the features in each dataset are scaled to [\u22121, 1] or [0, 1]. The solution space for each binary classifier is bounded as [10\u22123, 102]. All algorithms are repeated for 20 runs on each problem, except the deterministic grid search. For each run, the hyper-parameters are tuned on the training set with the 5-fold cross-validation. The higher the accuracy is, the better the hyper-parameters\nare supposed to be. The best hyper-parameters obtained in a run will be tested on the testing set, and the testing accuracy is regarded as the performance of an algorithm in a run. The time budget for the training phase in each run is set to 100 FEs. For RESOO, the probability \u03b7 is set to 1/3, the restart times is set to 2, and the reduced dimensionality is set to 1/5 to the original dimensionality, i.e., 9, 38, 65, respectively. For the grid search, 100 candidate solutions are uniformly selected over the solution space. For DAC-HC, the parameters N and M are set to 2 and 3, respectively.\nTable 3 lists the mean and standard derivation of the testing accuracies of the multi-class SVMs tuned by each algorithm. It can be seen that, both RESOO and DAC-HC outperform the grid search, although the improvement is marginal. Hence, it can be inferred that tuning multiple hyper-parameters, rather than one shared hyper-parameter, is beneficial to the multiclass SVMs. DAC-HC outperforms all the compared algorithms on the usps and letter datasets. Although it shows slightly lower accuracy than RESOO on the news20 dataset, a more stable behavior is observed as its standard derivation is smaller. CMA-ES is inferior to the grid search on all three problems. This phenomenon suggests that, for tuning multiple hyper-parameters for multi-class SVMs, an appropriate optimization approach should be employed. Otherwise, it would be better to tune just one shared hyper-parameter. It is also worthwhile to notice that, CMA-ES does not adopt any special treatment for high-dimensional optimization."}, {"heading": "5 Conclusions and Future Directions", "text": "This work investigated the Divide and Conquer idea on highdimensional black-box optimization problems. We found that the interdependent sub-problems after decomposition actually cannot be accurately conquered. Instead, we proposed the Divide and Approximate Conquer (DAC) to solve each sub-problem approximately. The convergence of DAC was proved and empirically supported. For empirical studies, a simple instantiation of DAC, i.e., DAC-HC, was also proposed. The advantages of DAC-HC over existing representative approaches were verified on two sets of non-separable high-dimensional problems.\nFor future work, we are interested in: \u2022 Promoting the ability of DAC by adopting more ad-\nvanced sub-problems optimizers. \u2022 Theoretically analyzing the convergence rate of DAC."}], "references": [{"title": "LIBSVM: A library for support vector machines", "author": ["Chang", "Lin", "2011] Chih-Chung Chang", "Chih-Jen Lin"], "venue": null, "citeRegEx": "Chang et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Chang et al\\.", "year": 2011}, {"title": "pages 300\u2013309", "author": ["Wenxiang Chen", "Thomas Weise", "Zhenyu Yang", "Ke Tang. Large-scale global optimization using cooperative coevolution with variable interaction learning. In Parallel Problem Solving from Nature", "PPSN XI"], "venue": "Springer,", "citeRegEx": "Chen et al.. 2010", "shortCiteRegEx": null, "year": 2010}, {"title": "Recursive decomposition for nonconvex optimization", "author": ["Abram L Friesen", "Pedro Domingos"], "venue": "Proceedings of the 24th International Joint Conference on Artificial Intelligence,", "citeRegEx": "Friesen and Domingos. 2015", "shortCiteRegEx": null, "year": 2015}, {"title": "Evolutionary computation", "author": ["Nikolaus Hansen", "Andreas Ostermeier. Completely derandomized selfadaptation in evolution strategies"], "venue": "9(2):159\u2013195,", "citeRegEx": "Hansen and Ostermeier. 2001", "shortCiteRegEx": null, "year": 2001}, {"title": "IEEE Transactions on Neural Networks", "author": ["Chih-Wei Hsu", "Chih-Jen Lin. A comparison of methods for multiclass support vector machines"], "venue": "13(2):415\u2013425,", "citeRegEx": "Hsu and Lin. 2002", "shortCiteRegEx": null, "year": 2002}, {"title": "IEEE Transactions on Pattern Analysis and Machine Intelligence", "author": ["Jonathan J Hull. A database for handwritten text recognition research"], "venue": "16(5):550\u2013554,", "citeRegEx": "Hull. 1994", "shortCiteRegEx": null, "year": 1994}, {"title": "Log-linear convergence and optimal bounds for the (1+ 1)-ES", "author": ["Mohamed Jebalia", "Anne Auger", "Pierre Liardet"], "venue": "Artificial Evolution, pages 207\u2013218. Springer,", "citeRegEx": "Jebalia et al.. 2008", "shortCiteRegEx": null, "year": 2008}, {"title": "Toward large-scale continuous eda: A random matrix theory perspective", "author": ["Ata Kab\u00e1n", "Jakramate Bootkrajang", "Robert John Durrant"], "venue": "Evolutionary computation,", "citeRegEx": "Kab\u00e1n et al.. 2015", "shortCiteRegEx": null, "year": 2015}, {"title": "Learning probability distributions in continuous evolutionary algorithms\u2013a comparative review", "author": ["Stefan Kern", "Sibylle D M\u00fcller", "Nikolaus Hansen", "Dirk B\u00fcche", "Jiri Ocenasek", "Petros Koumoutsakos"], "venue": "Natural Computing, 3(1):77\u2013112,", "citeRegEx": "Kern et al.. 2004", "shortCiteRegEx": null, "year": 2004}, {"title": "Optimization by simulated annealing", "author": ["S. Kirkpatrick", "C.D. Gelatt", "M.P. Vecchi"], "venue": "Science, 220(4598):671\u2013680", "citeRegEx": "Kirkpatrick et al.. 1983", "shortCiteRegEx": null, "year": 1983}, {"title": "Newsweeder: Learning to filter netnews", "author": ["Ken Lang"], "venue": "Proceedings of the 12th international conference on machine learning, pages 331\u2013339,", "citeRegEx": "Lang. 1995", "shortCiteRegEx": null, "year": 1995}, {"title": "Metaheuristics in large-scale global continues optimization: A survey", "author": ["Sedigheh Mahdavi", "Mohammad Ebrahim Shiri", "Shahryar Rahnamayan"], "venue": "Information Sciences, 295:407\u2013428,", "citeRegEx": "Mahdavi et al.. 2015", "shortCiteRegEx": null, "year": 2015}, {"title": "IEEE Transactions on Evolutionary Computation", "author": ["Mohammad Nabi Omidvar", "Xiaodong Li", "Yi Mei", "Xin Yao. Cooperative co-evolution with differential grouping for large scale optimization"], "venue": "18(3):378\u2013393,", "citeRegEx": "Omidvar et al.. 2014", "shortCiteRegEx": null, "year": 2014}, {"title": "In Proceedings of the 30th AAAI Conference on Artificial Intelligence (AAAI 2016)", "author": ["Hong Qian", "Yang Yu. Scaling simultaneous optimistic optimization for high-dimensional non-convex functions with low effective dimensions"], "venue": "Phoenix, AZ,", "citeRegEx": "Qian and Yu. 2016", "shortCiteRegEx": null, "year": 2016}, {"title": "Benchmark functions for the cec2010 special session and competition on largescale global optimization", "author": ["Ke Tang", "Xiaodong Li", "P.N. Suganthan", "Zhenyu Yang", "Thomas Weise"], "venue": "Technical report, Nature Inspired Computation and Applications Laboratory,", "citeRegEx": "Tang et al.. 2009", "shortCiteRegEx": null, "year": 2009}, {"title": "34(3):542\u2013550", "author": ["Ke Tang", "Peng Yang", "Xin Yao. Negatively correlated search. IEEE Journal on Selected Areas in Communications"], "venue": "March", "citeRegEx": "Tang et al.. 2016", "shortCiteRegEx": null, "year": 2016}, {"title": "volume 1", "author": ["Vladimir Vapnik. Statistical learning theory"], "venue": "Wiley New York,", "citeRegEx": "Vapnik. 1998", "shortCiteRegEx": null, "year": 1998}, {"title": "Bayesian optimization in high dimensions via random embeddings", "author": ["Wang et al", "2013] Ziyu Wang", "Masrour Zoghi", "Frank Hutter", "David Matheson", "Nando De Freitas"], "venue": "In Proceedings of the Twenty-Third international joint conference on Artificial Intelligence (IJCAI 2013),", "citeRegEx": "al. et al\\.,? \\Q2013\\E", "shortCiteRegEx": "al. et al\\.", "year": 2013}, {"title": "Information Sciences", "author": ["Zhenyu Yang", "Ke Tang", "Xin Yao. Large scale evolutionary optimization using cooperative coevolution"], "venue": "178(15):2985\u20132999,", "citeRegEx": "Yang et al.. 2008a", "shortCiteRegEx": null, "year": 2008}, {"title": "2008 (IEEE World Congress on Computational Intelligence)", "author": ["Zhenyu Yang", "Ke Tang", "Xin Yao. Self-adaptive differential evolution with neighborhood search. In IEEE Congress on Evolutionary Computation"], "venue": "pages 1110\u20131116. IEEE,", "citeRegEx": "Yang et al.. 2008b", "shortCiteRegEx": null, "year": 2008}], "referenceMentions": [{"referenceID": 7, "context": "An intuitive idea to handle a high-dimensional optimization problem is to project its solution space onto lower dimensions, where traditional approaches perform well [Kab\u00e1n et al., 2015].", "startOffset": 166, "endOffset": 186}, {"referenceID": 13, "context": ", Random Embedding techniques [Wang et al., 2013; Qian and Yu, 2016], consider the high-dimensional problem having low effective dimensionality, for which a random projection would suffice to find the global optimal solution of a high-dimensional problem in a low-dimensional space.", "startOffset": 30, "endOffset": 68}, {"referenceID": 18, "context": ",M , and di < D) [Yang et al., 2008a].", "startOffset": 17, "endOffset": 37}, {"referenceID": 1, "context": "Given an appropriate sub-problems optimizer, the abovedescribed DC strategy works well on the so-called separable problems, for which the global optimal optimum can be found by optimizing one dimension at a time regardless of the values taken on the other dimensions [Chen et al., 2010].", "startOffset": 267, "endOffset": 286}, {"referenceID": 12, "context": "If this condition does not hold, the performance of DC heavily relies on the decomposition method [Omidvar et al., 2014], which aims to divide a black-box high-dimensional problem in such a way that the global optimum can still be obtained by solving the sub-problems in a fully independent manner.", "startOffset": 98, "endOffset": 120}, {"referenceID": 11, "context": "In the past few years, a large variety of decomposition methods have been proposed [Mahdavi et al., 2015].", "startOffset": 83, "endOffset": 105}, {"referenceID": 9, "context": "Usually, a derivative-free search process in each sub-problem is guided by the solutions with better function values, despite a few exceptions [Kirkpatrick et al., 1983; Tang et al., 2016].", "startOffset": 143, "endOffset": 188}, {"referenceID": 15, "context": "Usually, a derivative-free search process in each sub-problem is guided by the solutions with better function values, despite a few exceptions [Kirkpatrick et al., 1983; Tang et al., 2016].", "startOffset": 143, "endOffset": 188}, {"referenceID": 1, "context": "(Interacting Variables) [Chen et al., 2010] Given a D-dimensional problem, its i-th and j-th variables are said to be interacting, if the rank of two partial solutions xi and xi in the i-th dimension may change with different complements, e.", "startOffset": 24, "endOffset": 43}, {"referenceID": 18, "context": "On this basis, we first decompose a problem f via random grouping [Yang et al., 2008a].", "startOffset": 66, "endOffset": 86}, {"referenceID": 8, "context": "After that, each search step-size is adapted at every iteration in terms of the 1/5 successful rule [Kern et al., 2004], using Eq.", "startOffset": 100, "endOffset": 119}, {"referenceID": 2, "context": "Meanwhile, it has also been observed that, in many real-world problems, only parts of variables are interacting [Friesen and Domingos, 2015].", "startOffset": 112, "endOffset": 140}, {"referenceID": 3, "context": "In the first group of comparison, DAC-HC is compared with CMA-ES [Hansen and Ostermeier, 2001], RESOO [Qian and Yu, 2016], and DECC-I [Omidvar et al.", "startOffset": 65, "endOffset": 94}, {"referenceID": 13, "context": "In the first group of comparison, DAC-HC is compared with CMA-ES [Hansen and Ostermeier, 2001], RESOO [Qian and Yu, 2016], and DECC-I [Omidvar et al.", "startOffset": 102, "endOffset": 121}, {"referenceID": 12, "context": "In the first group of comparison, DAC-HC is compared with CMA-ES [Hansen and Ostermeier, 2001], RESOO [Qian and Yu, 2016], and DECC-I [Omidvar et al., 2014], which are representatives of three basic ideas for high-dimensional optimization: the straightforward method, dimensionality reduction, and DC, respectively.", "startOffset": 134, "endOffset": 156}, {"referenceID": 19, "context": "Besides, the sub-problems of DECC-I are optimized by a variant of Differential Evolution [Yang et al., 2008b], which has been empirically observed more advanced than the employed RLSs [Tang et al.", "startOffset": 89, "endOffset": 109}, {"referenceID": 15, "context": ", 2008b], which has been empirically observed more advanced than the employed RLSs [Tang et al., 2016].", "startOffset": 83, "endOffset": 102}, {"referenceID": 6, "context": "It should be noted that, the employed RLS has also been theoretically proved to converge log-linearly on the sphere function [Jebalia et al., 2008].", "startOffset": 125, "endOffset": 147}, {"referenceID": 16, "context": "Support Vector Machines (SVMs) [Vapnik, 1998] is often considered as a family of powerful tools for classification.", "startOffset": 31, "endOffset": 45}, {"referenceID": 5, "context": ", usps [Hull, 1994], news20 [Lang, 1995], and letter [Hsu and Lin, 2002], are used for comparison, which contain 10, 20, 26 classes in each and yield three problems with 45, 190, 325 dimensions, respectively.", "startOffset": 7, "endOffset": 19}, {"referenceID": 10, "context": ", usps [Hull, 1994], news20 [Lang, 1995], and letter [Hsu and Lin, 2002], are used for comparison, which contain 10, 20, 26 classes in each and yield three problems with 45, 190, 325 dimensions, respectively.", "startOffset": 28, "endOffset": 40}, {"referenceID": 4, "context": ", usps [Hull, 1994], news20 [Lang, 1995], and letter [Hsu and Lin, 2002], are used for comparison, which contain 10, 20, 26 classes in each and yield three problems with 45, 190, 325 dimensions, respectively.", "startOffset": 53, "endOffset": 72}], "year": 2017, "abstractText": "Divide and Conquer (DC) is conceptually well suited to high-dimensional optimization by decomposing a problem into multiple small-scale subproblems. However, appealing performance can be seldom observed when the sub-problems are interdependent. This paper suggests that the major difficulty of tackling interdependent sub-problems lies in the precise evaluation of a partial solution (to a sub-problem), which can be overwhelmingly costly and thus makes sub-problems nontrivial to conquer. Thus, we propose an approximation approach, named Divide and Approximate Conquer (DAC), which reduces the cost of partial solution evaluation from exponential time to polynomial time. Meanwhile, the convergence to the global optimum (of the original problem) is still guaranteed. The effectiveness of DAC is demonstrated empirically on two sets of non-separable high-dimensional problems.", "creator": "LaTeX with hyperref package"}}}