{"id": "1603.03714", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "11-Mar-2016", "title": "Distribution Free Learning with Local Queries", "abstract": "The model of learning with \\emph{local membership queries} interpolates between the PAC model and the membership queries model by allowing the learner to query the label of any example that is similar to an example in the training set. This model, recently proposed and studied by Awasthi, Feldman and Kanade, aims to facilitate practical use of membership queries. The models provide a simple learning model. This model can be applied to non-intervened classes of students who are interested in learning. These classes are also taught by non-intervened learners.\n\n\n\nAs the model of learning has evolved, many non-intervened learners have encountered many difficulties. In many cases, these difficulties may be overcome by learning more from different people. This is particularly difficult because students are exposed to different kinds of learning:\nCognitive tasks such as working memory, verbal language, and visual memory are all affected by learning differently from other people. For example, for example, an example of learning from a different country of the same generation as a group of learners may not be able to find any useful information on his or her language or language. Thus learning to learn from other people will become less difficult. Learning from other people will become more difficult. For example, learning from a different country of the same generation as a group of learners may not be able to find any useful information on his or her language or language. Thus learning to learn from other people will become less difficult. Learning from other people will become less difficult. Learning from other people will become less difficult. Learning from other people will become less difficult. Learning from other people will become less difficult. Learning from other people will become less difficult. Learning from other people will become less difficult. Learning from other people will become less difficult. Learning from other people will become less difficult. Learning from other people will become less difficult. Learning from other people will become less difficult. Learning from other people will become less difficult. Learning from other people will become less difficult. Learning from other people will become less difficult. Learning from other people will become less difficult. Learning from other people will become less difficult. Learning from other people will become less difficult. Learning from other people will become less difficult. Learning from other people will become less difficult. Learning from other people will become less difficult. Learning from other people will become less difficult. Learning from other people will become less difficult. Learning from other people will become less difficult. Learning from other people will become less difficult. Learning from other", "histories": [["v1", "Fri, 11 Mar 2016 18:23:44 GMT  (20kb,D)", "http://arxiv.org/abs/1603.03714v1", null]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["galit bary-weisberg", "amit daniely", "shai shalev-shwartz"], "accepted": false, "id": "1603.03714"}, "pdf": {"name": "1603.03714.pdf", "metadata": {"source": "CRF", "title": "Distribution Free Learning with Local Queries\u2217", "authors": ["Galit Bary-Weisberg", "Amit Daniely", "Shai Shalev-Shwartz"], "emails": [], "sections": [{"heading": null, "text": "We continue this line of work, proving both positive and negative results in the distribution free setting. We restrict to the boolean cube {\u22121, 1}n, and say that a query is q-local if it is of a hamming distance \u2264 q from some training example. On the positive side, we show that 1-local queries already give an additional strength, and allow to learn a certain type of DNF formulas. On the negative side, we show that even ( n0.99 ) -local queries cannot help to learn various classes including Automata, DNFs and more. Likewise, q-local queries for any constant q cannot help to learn Juntas, Decision Trees, Sparse Polynomials and more. Moreover, for these classes, an algorithm that uses ( log0.99(n) ) -local queries would lead to a breakthrough in the best known running times.\n\u2217This paper is based on the M.Sc. thesis [6] of the first author. The thesis offers a more elaborated discussion, as well as experiments. \u2020Matific inc. Most work was done while the author was an M.Sc. student at the Hebrew University, Jerusalem, Israel \u2021Google inc. Most work was done while the author was a Ph.D. student at the Hebrew University, Jerusalem, Israel \u00a7School of Computer Science and Engineering, The Hebrew University, Jerusalem, Israel\nar X\niv :1\n60 3.\n03 71\n4v 1\n[ cs\n.L G\n] 1\n1 M\nar 2"}, {"heading": "1 Introduction", "text": "A child typically learns to recognize a cat based on two types of input. The first is given by her parents, pointing at a cat and saying \u201cLook, a cat!\u201d. The second is given as a response to the child\u2019s frequent question \u201cWhat is that?\u201d. These two types of input were the basis for the learning model originally suggested by Valiant [21]. Indeed, in Valiant\u2019s model, the learner can randomly sample labelled examples from \u201cnature\u201d, but it can also make a membership query (MQ) for the label of any unseen example. Today, the acronym PAC stands for the restricted model in which MQ are forbidden, while the full model is called PAC+MQ. Much work has been done investigating the limits and the strengths of MQ. In particular, membership queries were proven stronger than the vanilla PAC model [1, 10, 12, 16]. Yet, MQ are rarely used in practice. This is commonly attributed to the fact that MQ algorithms query very artificial examples, that are uninterpretable by humans (e.g., [8]).\nAwasthi et al. [5] suggested a solution to the problem of unnatural examples. They considered a mid-way model that allows algorithms to make only local queries, i.e., query examples that are close to examples from the sample set. Hopefully, examples which are similar to natural examples will appear natural to humans. Awasti et al. considered the case where the instance space is {\u22121, 1}n and the distance between examples is the Hamming distance. They proved positive results on learning sparse polynomials with O(log(n))-local queries under what they defined as locally smooth distributions1. They also proposed an algorithm that learns DNF formulas under the uniform distribution in quasi-polynomial time using O(log(n))-local queries.\nOur work follows Awasthi et al. and investigates local queries in the distribution free setting, in which no explicit assumptions are made on the underlying distribution, but only on the learned hypothesis. We prove both positive and negative results in this context:\n\u2022 One of the strongest and most beautiful results in the MQ model shows that automata are learnable with membership queries [1]. We show that unfortunately, this is probably not the case with local queries. Concretely, we show that even (n0.99)-local queries cannot help to learn automata. Namely, such an algorithm will imply a standard PAC algorithm for automata. As learning automata is hard under several assumptions [17, 13], our result suggests that it is hard to learn automata even with (n0.99)-local queries.\n\u2022 We prove a similar result for several additional classes. Namely, we show that (n0.99)local queries cannot help to learn DNFs, intersection of halfspaces, decision lists, depthd circuits for any d \u2265 2, and depth-d threshold circuits for any d \u2265 2. Likewise, for any constant q, q-local queries cannot help to learn Juntas, Decision Trees, Sparse polynomials, and Sparse polynomial threshold functions. In fact, we show that even( log0.99(n) ) -local queries are unlikely to lead to polynomial time algorithms. Namely,\nany algorithm that uses ( log0.99(n) ) -local queries will result with a PAC algorithm whose running time significantly improves the state of the art in these well studied problems.\n\u2022 On the positive side we show that already 1-local queries are probably stronger than the vanilla PAC model. Concretely, we show that a certain simplistic but natural\n1A distribution is locally \u03b1-smooth for \u03b1 \u2265 1 if its density function is log(\u03b1)-Lipschitz.\nlearning problem, which we term learning DNFs with evident examples, is learnable with 1-local queries. Furthermore, we show that without queries, this problem is at least as hard as learning decision trees, that are conjectured to be hard to learn."}, {"heading": "2 Previous Work", "text": "Membership Queries Several concept classes are known to be learnable only if membership queries are allowed: Deterministic Finite Automatons [1], k-term DNF for k =\nlog(n) log(log(n)) [10], decision trees and k-almost monotone-DNF formulas [12], intersections of khalfspaces [7] and DNF formulas under the uniform distribution [16]. The last result builds on Freund\u2019s boosting algorithm [15] and the Fourier-based technique for learning using membership queries due to [18]. We note that there are cases in which MQ do not help. E.g., in the case of learning DNF and CNF formulas [2], assuming that one way functions exist, and in the case of distribution free agnostic learning [14].\nLocal Membership Queries Awasthi et al. focused on learning with O(log(n))-local queries. They showed that t-sparse polynomials are learnable under locally smooth distributions using O (log(n) + log(t))-local queries, and that DNF formulas are learnable under the uniform distribution in quasi-polynomial time (nO(log logn)) using O(log(n))-local queries. They also presented some results regarding the strength of local MQ. They proved that under standard cryptographic assumptions, (r+1)-local queries are more powerful than r-local queries (for every 1 \u2264 r \u2264 n \u2212 1). They also showed that local queries do not always help. They showed that if a concept class is agnostically learnable under the uniform distribution using k-local queries (for constant k) then it is also agnostically learnable (under the uniform distribution) in the PAC model.\nWe remark that besides local queries, there were additional suggestions to solve the problem of unnatural examples. For example, to allow \u201cI don\u2019t know\u201d answers, or to be tolerant to some incorrect answers [3, 4, 11, 20, 9]."}, {"heading": "3 Setting", "text": "We consider binary classification where the instance space is X = Xn = {\u22121, 1}n and the label space is Y = {0, 1}. A learning problem is defined by a hypothesis class H \u2282 {0, 1}X . The learner receives a training set\nS = {(x1, h?(x1)), (x2, h?(x2)), . . . , (xm, h?(xm))} \u2208 (X \u00d7 Y)m\nwhere the xi\u2019s are sampled i.i.d. from some unknown distribution D on X and h? : X \u2192 Y is some unknown hypothesis. The learner is also allowed to make membership queries. Namely, to call an oracle, which receives as input some x \u2208 X and returns h?(x). We say that a membership query for x \u2208 X is q-local if there exists a training example xi whose Hamming distance from x is at most q.\nThe learner returns (a description of) a hypothesis h\u0302 : X \u2192 Y . The goal is to approximate h?, namely to find h\u0302 : X \u2192 Y with loss as small as possible, where the loss is defined as\nLD,h?(h\u0302) = Prx\u223cD\n( h\u0302(x) 6= h?(x) ) . We will focus on the so-called realizable case where h? is\nassumed to be in H, and will require algorithms to return a hypothesis with loss < in time that is polynomial in n and 1 . Concretely,\nDefinition 3.1 (Membership-Query Learning Algorithm) We say that a learning algorithm A learns H with q-local membership queries if\n\u2022 There exists a function mA (n, ) \u2264 poly ( n, 1 ) , such that for every distribution D over\nX , every h? \u2208 H and every > 0, if A is given access to q-local membership queries, and a training sequence\nS = {(x1, h?(x1)), (x2, h?(x2)), . . . , (xm, h?(xm))}\nwhere the xi\u2019s are sampled i.i.d. from D and m \u2265 mA(n, ), then with probability of at least2 3\n4 (over the choice of S), the output h\u0302 of A satisfies LD,h?(h\u0302) < .\n\u2022 Given a training set of size m\n\u2013 A asks at most poly(m,n) membership queries. \u2013 A runs in time poly(m,n). \u2013 The hypothesis returned by A can be evaluated in time poly(m,n).\nWe remark that learning with 0-local queries is equivalent to PAC learning, while learning with n-local queries is equivalent to PAC+MQ learning."}, {"heading": "4 Learning DNFs with Evident Examples", "text": "Intuitively, when evaluating a DNF formula on a given example, we check a few conditions corresponding to the formula\u2019s terms, and deem the example positive if one of them holds. We will consider the case that for each of these conditions, there is a chance to see a \u201cprototype example\u201d, that satisfies it in a strong or evident way. In the sequel, we denote by hF : {\u22121, 1}n \u2192 {0, 1} the function induced by a DNF formula F over n variables.\nDefinition 4.1 Let F = T1 \u2228 T2 \u2228 . . . \u2228 Td be a DNF formula. We say that an example x \u2208 {\u22121, 1}n satisfies a term Ti (with respect to the formula F ) evidently and denote Ti(x) \u2261 1 if :\n\u2022 It satisfies Ti. (In particular, hF (x) = 1)\n\u2022 It does not satisfy any other term Tk (for k 6= i) from F.\n\u2022 No coordinate change will turn Ti False and another term Tk True. Concretely, if for j \u2208 [n] we denote x\u2295j = (x1, . . . , xj\u22121,\u2212xj, xj+1, . . . , xn), then for every coordinate j \u2208 [n], if x\u2295j satisfies F (i.e. if hF (x\u2295j) = 1) then x\u2295j satisfies Ti and only Ti.\n2The success probability can be amplified to 1\u2212 \u03b4 by repetition.\nDefinition 4.2 Let F = T1 \u2228 T2 \u2228 . . .\u2228 Td be a DNF formula. We say that h? : {\u22121, 1}n \u2192 {0, 1} is realized by F with evident examples w.r.t. a distribution D if h? = hF and for every term3 Ti, Prx\u223cD (Ti(x) \u2261 1|Ti(x) = 1) \u2265 1n .\nFor example, the definition holds whenever h? is realized by a DNF in which any pair of different terms contains two opposite literals. Also, functions that are realized by a decision tree can be also realized by a DNF in which every positive example satisfies a single term, corresponding to root-to-leaf path that ends with a leaf labelled by 1. Hence, the assumption holds for decision trees provided that for every such path, there is a chance to see an example satisfying it evidently, in the sense that flipping each variable in the path will turn the example negative."}, {"heading": "4.1 An algorithm", "text": "Algorithm 1 Input: S1, S2 \u2208 ({\u22121, 1}n \u00d7 {0, 1})m Output: A DNF formula H\nStart with an empty DNF formula H for all positive examples (x, y) \u2208 S1 do\nDefine T = x1 \u2227 x1 \u2227 x2 \u2227 x2 \u2227 . . . \u2227 xn \u2227 xn for 1 \u2264 j \u2264 n do\nQuery x\u2295j to get h?(x\u2295j) if h?(x\u2295j) = 1 then\nRemove xj and xj from T if h?(x\u2295j) = 0 then\nif xj = 1 then Remove xj from T if xj = 0 then Remove xj from T\nH = H \u2228 T for all T in H do\nif T (x) = 1 but y = 0 for some (x, y) \u2208 S2 then Remove T from H\nReturn H\nTheorem 4.3 Algorithm 1 learns with 1-local-queries poly-sized DNFs with evident examples.\nIdea The algorithm is based on the following claim that follows easily from definition 4.1.\n3We note that the quantity 1n in the following equation is arbitrary, and can be replaced by 1 nc for any\nc > 0.\nClaim 1 Let F = T1\u2228T2\u2228 . . .\u2228Td be a DNF formula over {\u22121, 1}n. For every x \u2208 {\u22121, 1}n that satisfies a term Ti evidently (with respect to F ), and for every j \u2208 [n] it holds that:\nhF (x \u2295j) = 1\u21d0\u21d2 the term Ti does not contain the variable xj\nBy this claim, if x is an evident example for a certain term T , one can easily reconstruct T . Indeed, by flipping the value of each variable and checking if the label changes, one can infer which variables appear in T . Furthermore, the sign of these variables can be inferred from their sign in x. Hence, after seeing an evident example for all terms, one can have a list of terms containing all terms in the DNF. This list might have terms that are not part of the DNF. Yet, such terms can be thrown away later by testing if they make wrong predictions.\nProof (of Theorem 4.3) We will show that algorithm 1 learns with 1-local-queries any function that is realized by a DNF of size \u2264 n2 with evident examples. Adapting the proof to hold with nc instead of n2, for any c > 0, is straight-forward. First, it is easy to see that this algorithm is efficient. Now, fix a distribution D and let h? : {\u22121, 1}n \u2192 {0, 1} be a hypothesis that is realized, w.r.t. D, by a DNF formula F = T1 \u2228 T2 \u2228 . . . \u2228 Td, d \u2264 n2 with evident examples. Let > 0, and suppose we run the algorithm on two indepent samples from D, denoted S1 = {(xi, h?(xi)}m1i=1 and S2 = {(x\u2032i, h?(x\u2032i)} m2 i=1. We will show that if m1 \u2265 32n 3 log 32n 2 \u2265 32nd log 32d and m2 \u2265 32m1 log 32m1\nthen with probability of at least 3 4 , the algorithm will return a function h\u0302 with LD,h?(h\u0302) < . Let H be the DNF formula returned by the algorithm, and let h\u0302 be the function induced by H. We have that\nLD,h?(h\u0302) = Pr x\u223cD\n( h?(x) 6= h\u0302(x) ) = Pr\nx\u223cD\n( h?(x) = 1 and h\u0302(x) = 0 ) + Pr\nx\u223cD\n( h?(x) = 0 and h\u0302(x) = 1 ) The proof will now follow from claims 2 and 3.\nClaim 2 With probability at least 7 8 over the choice of S1, S2 we have\nPr x\u223cD\n( h?(x) = 1 and h\u0302(x) = 0 ) \u2264\n2 (1)\nProof We first note that if there is an evident example x for a term Ti in S1, then Ti will be in the output formula. Indeed, in the for-loop that go over the examples in S1, when processing the example (x, h?(x)), it is not hard to see that Ti will be added. We furthermore claim that the term Ti won\u2019t be removed at the for loop that tests the terms collected in the first loop. Indeed, if for some (x, h?(x)) \u2208 S2 we have Ti(x) = 1, it must be the case that h?(x) = 1. Now, we say that the term Ti is revealed if we see an evident example for this term in S1. We also denote pi = Prx\u223cD (Ti(x) = 1). We have\nPr x\u223cD\n( h?(x) = 1 and h\u0302(x) = 0 ) \u2264 d\u2211 i=1 Pr x\u223cD ( Ti(x) = 1 and h\u0302(x) = 0 ) \u2264\n\u2211 i:Ti is not revealed pi\nNow, by the assumption that h? is realized with evident queries, the probability (over the choice of S1, S2) that Ti is not revealed is at most ( 1\u2212 pi\nn )m1 . Hence, if we denote by Ai the event that Ti is not revealed, we have\nE S1\u223cDm1 [ Pr x\u223cD ( h?(x) = 1 and h\u0302(x) = 0 )] \u2264 ES1\u223cDm1 [ d\u2211 i=1 pi \u00b7 1Ai ]\n= d\u2211 i=1 piES1\u223cDm1 [1Ai ]\n= d\u2211 i=1 piPrS1\u223cDm1 [Ai]\n= d\u2211 i=1 pi ( 1\u2212 pi n )m1 =\n\u2211 i|pi< 32d pi ( 1\u2212 pi n )m1 + \u2211 i|pi\u2265 32d pi ( 1\u2212 pi n )m1 \u2264\n\u2211 i|pi< 32d 32d + \u2211 i|pi\u2265 32d ( 1\u2212 pi n )m1 \u2264 d \u00b7\n32d + \u2211 i|pi\u2265 32d e\u2212 m1pi n\n\u2264 32\n+ d \u00b7 e\u2212 m1 32dn\nSince m1 \u2265 32dn log 32d\nthe last expression is bounded by 16 . By Markov\u2019s inequality we conclude that the probabilty over the coice of S1, S2 that (1) does not lold is less than 1 8 . 2\nClaim 3 With probability at least 7 8 over the choice of S1, S2 we have\nPr x\u223cD\n( h?(x) = 0 and h\u0302(x) = 1 ) \u2264\n2 (2)\nProof Let T\u03021, . . . , T\u0302r be the terms that were added to H at the first for-loop. Denote\nqi = Prx\u223cD ( T\u0302i(x) = 1 and h ?(x) = 0 ) . We have\nPr x\u223cD\n( h?(x) = 0 and h\u0302(x) = 1 ) \u2264 \u2211 i : T\u0302i is not removed qi\nNow, the probability that T\u0302i is not removed is (1\u2212 qi)m2 . Hence, using an argument similar to the one used in the proof of claim 2, and since m2 \u2265 32m1 log 32m1 \u2265 32r log 32r , the claim follows. 2 2"}, {"heading": "4.2 A matching Lower Bound", "text": "In this section we provide evidence that the use of queries in our algorithm is crucial. We will show that the problem of learning poly-sized decision trees can be reduced to the problem of learning DNFs with evident examples. As learning decision trees is conjectured to be intractable, this reduction serves as an indication that learning DNFs with strongly evident examples is hard without membership queries. In fact, we will show that learning decision trees can be even reduced to the case that all positive examples are evident.\nTheorem 4.4 Learning poly-sized DNFs with evident examples is as hard as learning polysized decision trees.\nWe denote by hT the function induced by a decision tree T . The proof will use the following claim:\nClaim 4 There exists a mapping (a reduction) \u03d5 : {\u22121, 1}n \u2192 {\u22121, 1}2n, that can be evaluated in poly(n) time so that for every decision tree T over {\u22121, 1}n there exists a DNF formula F over {\u22121, 1}2n such that the following holds:\n1. The number of terms in F is upper bounded by the number of leaves in T\n2. hT = hF \u25e6 \u03d5\n3. \u2200x such that hT (x) = 1 , \u03d5(x) satisfies some term in F evidently.\nProof Define \u03d5 as follows:\n\u2200x = (x1, x2, . . . , xn) \u2208 Xn \u03d5(x1, x2, . . . , xn) = (x1, x1, x2, x2, . . . , xn, xn)\nNow, for every tree T , we will build the desired DNF formula F as follows: First we build a DNF formula F \u2032 over {\u22121, 1}n . Every leaf labeled \u20191\u2019 in T will define the following termtake the path from the root to that leaf and form the logical AND of the literals describing the path. F \u2032 will be a disjunction of these terms. Now, for every term T in F \u2032 we will define a term \u03c6(T ) over X2n in the following way: Let PT = {i \u2208 [n] : xi appear in T} and NT = {i \u2208 [n] : xi appear in T}. So\nT = \u2227 j\u2208PT xj \u2227 j\u2208NT xj\nDefine\n\u03c6(T ) = \u2227 j\u2208PT x2j\u22121 \u2227 j\u2208PT x2j \u2227 j\u2208NT x2j\u22121 \u2227 j\u2208NT x2j\nFinally, define F to be the DNF formula over X2n given by F = \u2228 T\u2208F \u2032 \u03c6(T )\nWe will now prove that \u03d5 and F satisfy the required conditions. First, \u03d5 can be evaluated in linear time in n. Second, it is easy to see that hT = hF \u25e6\u03d5, and as every term in F matches one of T \u2019s leaves, the number of terms in F cannot exceed the number of leaves in T . It is left to show that the third requirement holds. Let there be an x such that hT (x) = 1, then x is matched to one and only one path from T \u2019s root to a leaf labeled \u20191\u2019. From the construction of F , x satisfies one and only one term in F \u2032. Regarding the last requirement, that no coordinate change will make one term from F False and another one True, we made sure this will not happen by \u201cdoubling\u201d each variable. By this construction, in order to change a term from False to True at least two coordinate must change their value. 2\nWe are now ready to prove theorem 4.4.\nProof [of theorem 4.4] Suppose that A learns size-n DNFs with evident examples. Using the reduction from claim 4 we will build an efficient algorithm B that learns size-n decision trees. For every training set\nS = {(x1, h?(x1)), (x2, h?(x2)), . . . , (xm, h?(xm))} \u2208 (Xn \u00d7 {0, 1})m\nwe define\n\u03d5(S) := {(\u03d5(x1), h?(x1)), (\u03d5(x2)), h?(x2)), . . . , (\u03d5(xm), h?(xm))} \u2208 (X2n \u00d7 {0, 1})m\nThe algorithm B will work as follows: Given a training set S, B will run A on \u03d5(S), and will return h\u0302 \u25e6 \u03d5, where h\u0302 is the hypothesis returned by A. Since \u03d5 can be evaluated in poly(n) time and A is efficient, B is also efficient. We will prove that B learns size-n trees. Since A learns size-n DNFs with evident examples, there exists a function mA (n, ) \u2264 poly ( n, 1 ) , such that if A is given a training sequence\nS = {(x1, h?(x1)), (x2, h?(x2)), . . . , (xm, h?(xm))} \u2208 (Xn \u00d7 {0, 1})m\nwhere the xi\u2019s are sampled i.i.d. from a distribution D, h? is realized by a poly-sized DNF with evident examples, and m \u2265 mA(n, ), then with probability of at least 34 (over the choice of S), the output h\u0302 of A satisfies LD,h?(h\u0302) \u2264 . Let D be a distribution on Xn and let hT be a hypothesis that can be realized by a tree with \u2264 n leafs. Define a distribution D\u0303 on X2n by,\n\u02dc(D)(z) = { D(x) if \u2203x \u2208 Xn such that z = \u03d5(x) 0 otherwise\nNow, since hT is realized by T , from the conditions that \u03d5 satisfies, we get that hT = h \u25e6\u03d5, where h is realized by a DNF of size \u2264 n with evident examples w.r.t. D\u0303. Now if S \u2208 (Xn \u00d7 {0, 1})m is an i.i.d. sample with m \u2265 mA(2n, ) we have that with probability of at least 3\n4 it holds that\nLD,hT (B(S)) = LD,hT (h\u0302 \u25e6 \u03d5) = Pr\nx\u223cD [hT (x) 6= h\u0302 \u25e6 \u03d5(x)]\n= Pr x\u223cD\n[h \u25e6 \u03d5(x) 6= h\u0302 \u25e6 \u03d5(x)]\n= Pr z\u223cD\u0303\n[h(z) 6= h\u0302(z)]\n= LD\u0303,h(h\u0302) = LD\u0303,h(A(\u03d5(S))) <\n2"}, {"heading": "5 Lower Bounds", "text": "We first present a general technique to prove lower bounds on learning with local queries. For A \u2282 Xn and q > 0 denote\nB(A, q) = {x \u2208 Xn | \u2203a \u2208 A, d(x, a) \u2264 q}\nWe say that a mapping \u03d5 : {\u22121, 1}n \u2192 {\u22121, 1}n\u2032 is a q-reduction of type A from a class H to a class H\u2032 if the following holds:\n1. \u03d5 is efficiently computable.\n2. For every h \u2208 H there is h\u2032 \u2208 H\u2032 such that\n(a) h = h\u2032 \u25e6 \u03d5 (b) The restriction of h\u2032 to B(\u03d5(Xn), q) \\ \u03d5(Xn) is the constant function 1.\nWe say that a mapping \u03d5 : {\u22121, 1}n \u2192 {\u22121, 1}n\u2032 is a q-reduction of type B if the same requirements hold, except that (2b) is replaced by the following condition: For every z \u2208 B(\u03d5(Xn), q) there is a unique x \u2208 Xn satisfying d(z, \u03d5(x)) \u2264 q, and furthermore, h\u2032(z) = h(x).\nLemma 5.1 Suppose that there is a q-reduction \u03d5 from H to H\u2032. Then, learning H\u2032 with q-local queries is as hard as PAC learning H.\nProof (sketch) Suppose that A\u2032 learns H\u2032 with q-local queries. We need to show that there is an algorithm that learns H. Indeed, by an argument similar to the one in theorem 4.4, it is not hard to verify that the following algorithm learns H. Given a sample {(x1, y1), . . . , (xm, ym)} \u2282 {\u22121, 1}n \u00d7 {0, 1}, run A\u2032 on the sample {(\u03d5(x1), y1), . . . , (\u03d5(xm), ym)} \u2282 {\u22121, 1}n\n\u2032 \u00d7 {0, 1}. Whenever A\u2032 makes a q-local query for z \u2208 {\u22121, 1}n\u2032 , respond 1 if \u03d5 is of type A. If \u03d5 is of type B, respond yi, where xi is the unique training sample satisfying d(z, \u03d5(xi)) \u2264 q. Finally, if A\u2032 returned the hypothesis h\u2032, return h\u2032 \u25e6 \u03d5. 2\nWe next use Lemma 5.1 to prove that for several classes, local queries are not useful. Namely, if the class is learnable with local queries then it is also learnable without queries. We will use the following terminology. We say that q-local queries cannot help to learn a class H if H is learnable if and only if it is learnable with q-local queries.\nCorollary 5.2 For every 0 > 0, (n 1\u2212 0)-local queries cannot help to learn poly-sized DNFs, intersection of halfspaces, decision lists, depth-d circuits for any d = d(n) \u2265 2, and depth-d threshold circuits for any d = d(n) \u2265 2.\nProof We will only prove the corollary for DNFs. The proofs for the remaining classes are similar. Also, for simplicity, we will assume that 0 = 1 3 . Consider the mapping \u03d5 : {\u22121, 1}n \u2192 {\u22121, 1}n3 that replicates each coordinate n2 times. To establish the corollary, we show that \u03d5 is an ( (n\u2032) 2 3 \u2212 1 ) -reduction of type A from poly-sized DNF to poly-sized\nDNF. Indeed, let F = T1 \u2228 . . . \u2228 Td be a DNF formula on n variables, consider the following formula on the n3 variables {xi,j}1\u2264i\u2264n,1\u2264j\u2264n2 . Let,\nT \u2032t({xi,j}i,j) = Tt(x1,1, . . . , xn,1) G\u2032({xi,j}i,j) = \u2228ni=1 \u2228n 2\u22121 j=1 (xi,j \u2227 \u00acxi,j+1) \u2228 (xi,j+1 \u2227 \u00acxi,j)\nF \u2032 = (T \u20321 \u2228 . . . \u2228 T \u2032d) \u2228G\u2032\nIt is not hard to verify that hF = hF \u2032\u25e6\u03d5. Moreover, if x = {xi,j}i,j \u2208 B(\u03d5(Xn), n2\u22121)\\\u03d5(Xn) then xi,j 6= xi,j+1 for some i, j, meaning that hG(x) = 1 and therefore also hF (x) = 1. 2\nCorollary 5.3 For every 0 > 0, (n 1\u2212 0)-local queries cannot help to learn poly-sized automata.\nProof Again, for simplicity, we assume that 0 = 1 3 , and consider the mapping \u03d5 : {\u22121, 1}n \u2192 {\u22121, 1}n3 that replicates each coordinate n2 times. To establish the corollary, we show that \u03d5 is an ( (n\u2032) 2 3 \u2212 1 ) -reduction of type A from poly-sized automata to\npoly-sized automata. Indeed, let A be an automaton on n variables. It is enough to show that there is an automaton A\u2032 on the n3 variables {xi,j}1\u2264i\u2264n,1\u2264j\u2264n2 such that (i) the size of A\u2032 is polynomial, (ii) A\u2032 accepts any string in B(\u03d5(Xn), n2\u2212 1) \\\u03d5(Xn), and (iii) A\u2032 accepts \u03d5(x) if and only if A accepts x. Now, by the product construction of automatons [19], if A\u20321, A \u2032 2 are automata that induce the functions hA\u20321 , hA\u20322 : {\u22121, 1} n3 \u2192 {0, 1}, then the function hA\u20321 \u2228 hA\u20322 can be induced by an automaton of size |A\u20321| \u00b7 |A\u20322|. Hence, it is enough to show that there are poly-sized automata A\u20321, A \u2032 2 that satisfies (ii) and (iii) respectively.\nA construction of a size O(n2) automaton that satisfies (ii) is a simple exercise. We next explain how to construct a poly-sized automaton A\u20322 satisfying (iii). The states of A \u2032 2 will be the S(A) \u00d7 [n2] (here, S(A) denotes the set of states of A). The start state of A\u20322 will be (\u03b10, 1), where \u03b10 is the start state of A, and the accept states of A \u2032 2 will be the cartesian product of the accept states of A with [n2]. Finally if \u03b4 : S(A) \u00d7 {\u22121, 1} \u2192 S(A) is the transition function of A, then the transition function of A\u20322 is defined by\n\u03b4\u2032((\u03b1, i), b) =\n{ (\u03b1, i+ 1) 1 \u2264 i < n2\n(\u03b4(\u03b1, b), 1) i = n2\nIt is not hard to verify that A\u20322 satisfies (iii). 2 In the next corollary, a Junta of size t is a function h : {\u22121, 1}n \u2192 {0, 1} that depends on \u2264 log(t) variables. The rational behind this definition of size, is the fact that in order to describe a general function that depends on K variables, at least 2K bits are required. Likewise, the sample complexity of learning Juntas is proportional to t rather than log(t)\nCorollary 5.4 For every constant q0 > 0, q0-local queries cannot help to learn poly-sized Juntas.\nProof Consider the mapping \u03d5 : {\u22121, 1}n \u2192 {\u22121, 1}(2q0+1)n that replicates each coordinate 2q0 + 1 times. To establish the corollary, we show that \u03d5 is a q0-reduction of type B from poly-sized Juntas to poly-sized Juntas. Indeed, let h : {\u22121, 1}n \u2192 {0, 1} be a function that depends on K variables. It is enough to show that there is a function h\u2032 on the variables {zi,j}i\u2208[n],j\u2208[2q0+1] that satisfies (i) h = h\u2032 \u25e6 \u03d5, (ii) for every x \u2208 X , if z \u2208 {\u22121, 1}(2q0+1)n is obtained from \u03d5(x) by modifying \u2264 q0 coordinates, then h\u2032(z) = h\u2032(\u03d5(x)) and (iii) h\u2032 depends on (2q0 + 1)K variables. It is not hard to check that the following function satisfies these requirements:\nh\u2032(z) = h (MAJ(z1,1, . . . , z1,2q0+1), . . . ,MAJ(zn,1, . . . , zn,2q0+1))\n2\nWe remark that by taking q0 = log 1\u2212 0(n) for some 0 > 0, and using a similar argument, it can be shown that an efficient algorithm for learning poly-sized Juntas with ( log1\u2212 0(n) ) -local queries would imply a PAC algorithm for poly-sized Juntas that runs in time nO(log 1\u2212 0 (n)).\nCorollary 5.5 For every constant q0 > 0, q0-local queries cannot help to learn poly-sized decision tress.\nProof As with Juntas, consider the mapping \u03d5 : {\u22121, 1}n \u2192 {\u22121, 1}(2q0+1)n that replicates each coordinate 2q0 + 1 times. To establish the corollary, we show that \u03d5 is a q0-reduction of type B from poly-sized decision trees to poly-sized decision trees. Indeed, let h : {\u22121, 1}n \u2192 {0, 1} be a function that is realized by a decision tree T . It is enough to show that there is a function h\u2032 on the variables {zi,j}i\u2208[n],j\u2208[2q0+1] that satisfies (i) h = h\u2032 \u25e6 \u03d5, (ii) for every x \u2208 X , if z \u2208 {\u22121, 1}(2q0+1)n is obtained from \u03d5(x) by modifying \u2264 q0 coordinates, then h\u2032(z) = h\u2032(\u03d5(x)) and (iii) h\u2032 can be realized by a tree of size |T |2q0+1. As we explain, this will hold for the following function:\nh\u2032(z) = MAJ (h(z1,1, . . . , zn,1), . . . , h(z1,2q0+1, . . . , zn,2q0+1))\nIt is not hard to check that h\u2032 satisfies (i) and (ii). As for (iii), consider the following tree T \u2032. First replicate T on the variables z1,1, . . . , zn,1. Then, on the obtained tree, replace each leaf by a replica of T on the variables z1,2, . . . , zn,2. Then, again, replace each leaf by a replica of T on the variables z1,3, . . . , zn,3. Continues doing so, until the leafs are replaced by a replica on the variables z1,2q0+1, . . . , zn,2q0+1. Now, each leaf in the resulted tree corresponds to (2q0 + 1) root-to-leaf paths in the original tree T . The label of such leaf will be the majority of the labels of these paths. 2\nAs with Juntas, we remark that by taking q0 = log 1\u2212 0(n) for some 0 > 0, and using a similar argument, it can be shown that an efficient algorithm for learning poly-sized trees with ( log1\u2212 0(n) ) -local queries would imply a PAC algorithm for poly-sized trees that runs in time nO(log 1\u2212 0 (n)).\nIn the sequel, we say that a function h : {\u22121, 1}n \u2192 {0, 1} is realized by a poly-sized polynomial, if it is the function induced by a polynomial with polynomially many nonzero coefficient and degree O (log(n)). A similar convention applies to the term poly-sized polynomial threshold function. We remark that the following corollary and its proof remain correct also if in our definition, we replace the number of coefficients with the `1 norm of the coefficients vector.\nCorollary 5.6 For every constant q0 > 0, q0-local queries cannot help to learn poly-sized polynomials, as well as poly-sized polynomial threshold functions.\nProof We will prove the corollary for polynomials. The proof for polynomial threshold functions is similar. Consider the mapping \u03d5 : {\u22121, 1}n \u2192 {\u22121, 1}(2q0+1)n that replicates each coordinate 2q0 + 1 times. To establish the corollary, we show that \u03d5 is a q0-reduction of type B from poly-sized polynomials to poly-sized polynomials. Indeed, let h : {\u22121, 1}n \u2192 {0, 1} be a poly-sized polynomial. It is enough to show that there is a poly-sized polynomial h\u2032 on the variables {zi,j}i\u2208[n],j\u2208[2q0+1] that satisfies (i) h = h\u2032 \u25e6 \u03d5, and (ii) for every x \u2208 X , if z \u2208 {\u22121, 1}(2q0+1)n is obtained from \u03d5(x) by modifying \u2264 q0 coordinates, then h\u2032(z) = h\u2032(\u03d5(x)). As we explain next, this will hold for the following polynomial:\nh\u2032(z) = h (MAJ(z1,1, . . . , z1,2q0+1), . . . ,MAJ(zn,1, . . . , zn,2q0+1))\nIt is not hard to check that h\u2032 satisfies (i) and (ii). It remains to show that h\u2032 is poly-sized. Indeed, the majority function on 2q0 +1 coordinates is a polynomial of degree \u2264 2q0 +1 with at most 22q0+1 non zero coefficients (this is true for any function on 2q0 + 1 coordinates). Hence, if we replace each variable in h by a polynomial of the form MAJ(zi,1, . . . , zi,2q0+1), the degree is multiplied by at most (2q0 + 1), while the number of non zero coefficients is multiplied by at most 22q0+1. 2\nAs with Juntas and decision trees, by taking q0 = log 1\u2212 0(n) for some 0 > 0, and using a similar argument, it can be shown that an efficient algorithm for learning poly-sized polynomials or polynomial threshold functions with ( log1\u2212 0(n) ) -local queries would imply a PAC algorithm for the same problem that runs in time nO(log 1\u2212 0 (n))."}, {"heading": "6 Conclusion and Future Work", "text": "We have shown that in the distribution free setting, for many hypothesis classes, local queries are not useful. As our proofs show, this stems from the fact that learning these classes without queries can be reduced to a case where local queries are pointless, in the sense that the answer to them is either always 1, or the label of the closest training example. On the other hand, the learning problem of DNFs with evident examples circumvents this property. Indeed, the underlying assumption enforces that local changes can change the label in a\nnon-trivial manner. While this assumption might be intuitive in some cases, it is certainly very restrictive. Therefore, a natural future research direction is to seek less restrictive assumptions, that still posses this property.\nMore concrete direction arising from our work concern classes for which we have shown that ( log0.99(n) ) -local queries are unlikely to lead to efficient algorithms. We conjecture that for some a > 0 even (na)-local queries won\u2019t lead to efficient distribution free algorithms for these classes."}], "references": [{"title": "Learning regular sets from queries and counterexamples", "author": ["Dana Angluin"], "venue": "Information and computation,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 1987}, {"title": "When won t membership queries help", "author": ["Dana Angluin", "Michael Kharitonov"], "venue": "Journal of Computer and System Sciences,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 1995}, {"title": "Randomly fallible teachers: Learning monotone dnf with an incomplete membership oracle", "author": ["Dana Angluin", "Donna K Slonim"], "venue": "Machine Learning,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 1994}, {"title": "Malicious omissions and errors in answers to membership queries", "author": ["Dana Angluin", "M\u0101rti\u0146\u0161 Kri\u0137is", "Robert H Sloan", "Gy\u00f6rgy Tur\u00e1n"], "venue": "Machine Learning,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 1997}, {"title": "Learning using local membership queries", "author": ["Pranjal Awasthi", "Vitaly Feldman", "Varun Kanade"], "venue": "arXiv preprint arXiv:1211.0996,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2012}, {"title": "Learning using 1-local membership queries", "author": ["Galit Bary"], "venue": "arXiv preprint arXiv:1512.00165,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2015}, {"title": "Neural net algorithms that learn in polynomial time from examples and queries", "author": ["Eric B Baum"], "venue": "Neural Networks, IEEE Transactions on,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 1991}, {"title": "Query learning can work poorly when a human oracle is used", "author": ["Eric B Baum", "Kenneth Lang"], "venue": "In International Joint Conference on Neural Networks,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 1992}, {"title": "Learning with errors in answers to membership queries", "author": ["Laurence Bisht", "Nader H Bshouty", "Lawrance Khoury"], "venue": "Journal of Computer and System Sciences,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2008}, {"title": "Fast learning of k-term dnf formulas with queries", "author": ["Avrim Blum", "Steven Rudich"], "venue": "In Proceedings of the twenty-fourth annual ACM symposium on Theory of computing,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 1992}, {"title": "Learning with unreliable boundary queries", "author": ["Avrim Blum", "Prasad Chalasani", "Sally A Goldman", "Donna K Slonim"], "venue": "In Proceedings of the eighth annual conference on Computational learning theory,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 1995}, {"title": "Exact learning boolean functions via the monotone theory", "author": ["Nader H Bshouty"], "venue": "Information and Computation,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 1995}, {"title": "Complexity theoretic limitations on learning dnf\u2019s", "author": ["Amit Daniely", "Shai Shalev-Shwatz"], "venue": "arXiv preprint arXiv:1404.3378,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2014}, {"title": "On the power of membership queries in agnostic learning", "author": ["Vitaly Feldman"], "venue": "The Journal of Machine Learning Research,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2009}, {"title": "Boosting a weak learning algorithm by majority", "author": ["Yoav Freund"], "venue": "Information and computation,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 1995}, {"title": "An efficient membership-query algorithm for learning dnf with respect to the uniform distribution", "author": ["Jeffrey Jackson"], "venue": "In Foundations of Computer Science,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 1994}, {"title": "Cryptographic limitations on learning boolean formulae and finite automata", "author": ["Michael Kearns", "Leslie Valiant"], "venue": "Journal of the ACM (JACM),", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 1994}, {"title": "Learning decision trees using the fourier spectrum", "author": ["Eyal Kushilevitz", "Yishay Mansour"], "venue": "SIAM Journal on Computing,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 1993}, {"title": "Learning with queries but incomplete information", "author": ["Robert H Sloan", "Gy\u00f6rgy Tur\u00e1n"], "venue": "In Proceedings of the seventh annual conference on Computational learning theory,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 1994}, {"title": "A theory of the learnable", "author": ["Leslie G Valiant"], "venue": "Communications of the ACM,", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 1984}], "referenceMentions": [{"referenceID": 4, "context": "[5], aims to facilitate practical use of membership queries.", "startOffset": 0, "endOffset": 3}, {"referenceID": 5, "context": "thesis [6] of the first author.", "startOffset": 7, "endOffset": 10}, {"referenceID": 19, "context": "These two types of input were the basis for the learning model originally suggested by Valiant [21].", "startOffset": 95, "endOffset": 99}, {"referenceID": 0, "context": "In particular, membership queries were proven stronger than the vanilla PAC model [1, 10, 12, 16].", "startOffset": 82, "endOffset": 97}, {"referenceID": 9, "context": "In particular, membership queries were proven stronger than the vanilla PAC model [1, 10, 12, 16].", "startOffset": 82, "endOffset": 97}, {"referenceID": 11, "context": "In particular, membership queries were proven stronger than the vanilla PAC model [1, 10, 12, 16].", "startOffset": 82, "endOffset": 97}, {"referenceID": 15, "context": "In particular, membership queries were proven stronger than the vanilla PAC model [1, 10, 12, 16].", "startOffset": 82, "endOffset": 97}, {"referenceID": 7, "context": ", [8]).", "startOffset": 2, "endOffset": 5}, {"referenceID": 4, "context": "[5] suggested a solution to the problem of unnatural examples.", "startOffset": 0, "endOffset": 3}, {"referenceID": 0, "context": "We prove both positive and negative results in this context: \u2022 One of the strongest and most beautiful results in the MQ model shows that automata are learnable with membership queries [1].", "startOffset": 185, "endOffset": 188}, {"referenceID": 16, "context": "As learning automata is hard under several assumptions [17, 13], our result suggests that it is hard to learn automata even with (n)-local queries.", "startOffset": 55, "endOffset": 63}, {"referenceID": 12, "context": "As learning automata is hard under several assumptions [17, 13], our result suggests that it is hard to learn automata even with (n)-local queries.", "startOffset": 55, "endOffset": 63}, {"referenceID": 0, "context": "Membership Queries Several concept classes are known to be learnable only if membership queries are allowed: Deterministic Finite Automatons [1], k-term DNF for k = log(n) log(log(n)) [10], decision trees and k-almost monotone-DNF formulas [12], intersections of khalfspaces [7] and DNF formulas under the uniform distribution [16].", "startOffset": 141, "endOffset": 144}, {"referenceID": 9, "context": "Membership Queries Several concept classes are known to be learnable only if membership queries are allowed: Deterministic Finite Automatons [1], k-term DNF for k = log(n) log(log(n)) [10], decision trees and k-almost monotone-DNF formulas [12], intersections of khalfspaces [7] and DNF formulas under the uniform distribution [16].", "startOffset": 184, "endOffset": 188}, {"referenceID": 11, "context": "Membership Queries Several concept classes are known to be learnable only if membership queries are allowed: Deterministic Finite Automatons [1], k-term DNF for k = log(n) log(log(n)) [10], decision trees and k-almost monotone-DNF formulas [12], intersections of khalfspaces [7] and DNF formulas under the uniform distribution [16].", "startOffset": 240, "endOffset": 244}, {"referenceID": 6, "context": "Membership Queries Several concept classes are known to be learnable only if membership queries are allowed: Deterministic Finite Automatons [1], k-term DNF for k = log(n) log(log(n)) [10], decision trees and k-almost monotone-DNF formulas [12], intersections of khalfspaces [7] and DNF formulas under the uniform distribution [16].", "startOffset": 275, "endOffset": 278}, {"referenceID": 15, "context": "Membership Queries Several concept classes are known to be learnable only if membership queries are allowed: Deterministic Finite Automatons [1], k-term DNF for k = log(n) log(log(n)) [10], decision trees and k-almost monotone-DNF formulas [12], intersections of khalfspaces [7] and DNF formulas under the uniform distribution [16].", "startOffset": 327, "endOffset": 331}, {"referenceID": 14, "context": "The last result builds on Freund\u2019s boosting algorithm [15] and the Fourier-based technique for learning using membership queries due to [18].", "startOffset": 54, "endOffset": 58}, {"referenceID": 17, "context": "The last result builds on Freund\u2019s boosting algorithm [15] and the Fourier-based technique for learning using membership queries due to [18].", "startOffset": 136, "endOffset": 140}, {"referenceID": 1, "context": ", in the case of learning DNF and CNF formulas [2], assuming that one way functions exist, and in the case of distribution free agnostic learning [14].", "startOffset": 47, "endOffset": 50}, {"referenceID": 13, "context": ", in the case of learning DNF and CNF formulas [2], assuming that one way functions exist, and in the case of distribution free agnostic learning [14].", "startOffset": 146, "endOffset": 150}, {"referenceID": 2, "context": "For example, to allow \u201cI don\u2019t know\u201d answers, or to be tolerant to some incorrect answers [3, 4, 11, 20, 9].", "startOffset": 90, "endOffset": 107}, {"referenceID": 3, "context": "For example, to allow \u201cI don\u2019t know\u201d answers, or to be tolerant to some incorrect answers [3, 4, 11, 20, 9].", "startOffset": 90, "endOffset": 107}, {"referenceID": 10, "context": "For example, to allow \u201cI don\u2019t know\u201d answers, or to be tolerant to some incorrect answers [3, 4, 11, 20, 9].", "startOffset": 90, "endOffset": 107}, {"referenceID": 18, "context": "For example, to allow \u201cI don\u2019t know\u201d answers, or to be tolerant to some incorrect answers [3, 4, 11, 20, 9].", "startOffset": 90, "endOffset": 107}, {"referenceID": 8, "context": "For example, to allow \u201cI don\u2019t know\u201d answers, or to be tolerant to some incorrect answers [3, 4, 11, 20, 9].", "startOffset": 90, "endOffset": 107}], "year": 2016, "abstractText": "The model of learning with local membership queries interpolates between the PAC model and the membership queries model by allowing the learner to query the label of any example that is similar to an example in the training set. This model, recently proposed and studied by Awasthi et al. [5], aims to facilitate practical use of membership queries. We continue this line of work, proving both positive and negative results in the distribution free setting. We restrict to the boolean cube {\u22121, 1}n, and say that a query is q-local if it is of a hamming distance \u2264 q from some training example. On the positive side, we show that 1-local queries already give an additional strength, and allow to learn a certain type of DNF formulas. On the negative side, we show that even ( n0.99 ) -local queries cannot help to learn various classes including Automata, DNFs and more. Likewise, q-local queries for any constant q cannot help to learn Juntas, Decision Trees, Sparse Polynomials and more. Moreover, for these classes, an algorithm that uses ( log(n) ) -local queries would lead to a breakthrough in the best known running times. \u2217This paper is based on the M.Sc. thesis [6] of the first author. The thesis offers a more elaborated discussion, as well as experiments. \u2020Matific inc. Most work was done while the author was an M.Sc. student at the Hebrew University, Jerusalem, Israel \u2021Google inc. Most work was done while the author was a Ph.D. student at the Hebrew University, Jerusalem, Israel \u00a7School of Computer Science and Engineering, The Hebrew University, Jerusalem, Israel ar X iv :1 60 3. 03 71 4v 1 [ cs .L G ] 1 1 M ar 2 01 6", "creator": "LaTeX with hyperref package"}}}