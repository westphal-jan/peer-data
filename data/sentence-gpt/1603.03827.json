{"id": "1603.03827", "review": {"conference": "HLT-NAACL", "VERSION": "v1", "DATE_OF_SUBMISSION": "12-Mar-2016", "title": "Sequential Short-Text Classification with Recurrent and Convolutional Neural Networks", "abstract": "Recent approaches based on artificial neural networks (ANNs) have shown promising results for short-text classification. However, many short texts occur in sequences (e.g., sentences in a document or utterances in a dialog), and most existing ANN-based systems do not leverage the preceding short texts when classifying a subsequent one. In this work, we present a model based on recurrent neural networks and convolutional neural networks that incorporates the preceding short texts. Our model achieves state-of-the-art results on three different datasets for dialog act prediction. We focus on the relative number of words by the word of the word and type of sentence. In summary, we propose a model based on the number of words by the word of the word. We use an initialization procedure to train model models based on an algorithm that calculates the probability of having different words or sentences per character for each subject (e.g., a sentence by the word of the word of the word, or a sentence by the word of the word). Finally, we apply an algorithm that combines a previous post with a new post with a new one, and reconstructs the number of words from the preceding post in order to identify the first term. In order to further develop our model, we want to introduce a second model based on multiple recurrent neural networks in which the first sentence represents an item of interest, and to explore the possibility of a second approach based on the previous one.", "histories": [["v1", "Sat, 12 Mar 2016 00:02:51 GMT  (75kb,D)", "http://arxiv.org/abs/1603.03827v1", "Accepted as a conference paper at NAACL 2016"]], "COMMENTS": "Accepted as a conference paper at NAACL 2016", "reviews": [], "SUBJECTS": "cs.CL cs.AI cs.LG cs.NE stat.ML", "authors": ["ji young lee", "franck dernoncourt"], "accepted": true, "id": "1603.03827"}, "pdf": {"name": "1603.03827.pdf", "metadata": {"source": "CRF", "title": "Sequential Short-Text Classification with Recurrent and Convolutional Neural Networks", "authors": ["Ji Young Lee", "Franck Dernoncourt"], "emails": ["jjylee@mit.edu", "francky@mit.edu"], "sections": [{"heading": "1 Introduction", "text": "Short-text classification is an important task in many areas of natural language processing, including sentiment analysis, question answering, or dialog management. Many different approaches have been developed for short-text classification, such as using Support Vector Machines (SVMs) with rule-based features (Silva et al., 2011), combining SVMs with naive Bayes (Wang and Manning, 2012), and building dependency trees with Conditional Random Fields (Nakagawa et al., 2010). Several recent studies using ANNs have shown promising results, including convolutional neural networks (CNNs) (Kim, 2014; Blunsom et al., 2014; Kalchbrenner et al., 2014) and recursive neural networks (Socher et al., 2012).\nMost ANN systems classify short texts in isolation, i.e., without considering preceding short texts.\n\u2217 These authors contributed equally to this work.\nHowever, short texts usually appear in sequence (e.g., sentences in a document or utterances in a dialog), therefore using information from preceding short texts may improve the classification accuracy. Previous works on sequential short-text classification are mostly based on non-ANN approaches, such as Hidden Markov Models (HMMs) (Reithinger and Klesen, 1997), (Stolcke et al., 2000), maximum entropy (Ang et al., 2005), and naive Bayes (Lendvai and Geertzen, 2007).\nInspired by the performance of ANN-based systems for non-sequential short-text classification, we introduce a model based on recurrent neural networks (RNNs) and CNNs for sequential short-text classification, and evaluate it on the dialog act classification task. A dialog act characterizes an utterance in a dialog based on a combination of pragmatic, semantic, and syntactic criteria. Its accurate detection is useful for a range of applications, from speech recognition to automatic summarization (Stolcke et al., 2000). Our model achieves state-of-the-art results on three different datasets."}, {"heading": "2 Model", "text": "Our model comprises two parts. The first part generates a vector representation for each short text using either the RNN or CNN architecture, as discussed in Section 2.1 and Figure 1. The second part classifies the current short text based on the vector representations of the current as well as a few preceding short texts, as presented in Section 2.2 and Figure 2.\nWe denote scalars with italic lowercases (e.g., k, bf ), vectors with bold lowercases (e.g., s, xi), and matrices with italic uppercases (e.g., Wf ). We\nar X\niv :1\n60 3.\n03 82\n7v 1\n[ cs\n.C L\n] 1\n2 M\nar 2\nuse the colon notation vi:j to denote the sequence of vectors (vi,vi+1, . . . ,vj)."}, {"heading": "2.1 Short-text representation", "text": "A given short text of length ` is represented as the sequence of m-dimensional word vectors x1:`, which is used by the RNN or CNN model to produce the n-dimensional short-text representation s."}, {"heading": "2.1.1 RNN-based short-text representation", "text": "We use a variant of RNN called Long Short Term Memory (LSTM) (Hochreiter and Schmidhuber, 1997). For the tth word in the short-text, an LSTM takes as input xt,ht\u22121, ct\u22121 and produces ht, ct based on the following formulas:\nit = \u03c3(Wixt + Uiht\u22121 + bi)\nft = \u03c3(Wfxt + Ufht\u22121 + bf )\nc\u0303t = tanh(Wcxt + Ucht\u22121 + bc)\nct = ft ct\u22121 + it c\u0303t ot = \u03c3(Woxt + Uoht\u22121 + bo)\nht = ot tanh(ct)\nwhere Wj \u2208 Rn\u00d7m, Uj \u2208 Rn\u00d7n are weight matrices and bj \u2208 Rn are bias vectors, for j \u2208 {i, f, c, o}.\nThe symbols \u03c3(\u00b7) and tanh(\u00b7) refer to the elementwise sigmoid and hyperbolic tangent functions, and is the element-wise multiplication. h0 = c0 = 0.\nIn the pooling layer, the sequence of vectors h1:` output from the RNN layer are combined into a single vector s \u2208 Rn that represents the short-text, using one of the following mechanisms: last, mean, and max pooling. Last pooling takes the last vector, i.e., s = h`, mean pooling averages all vectors, i.e., s = 1` \u2211` t=1 ht, and max pooling takes the elementwise maximum of h1:`."}, {"heading": "2.1.2 CNN-based short-text representation", "text": "Using a filter Wf \u2208 Rh\u00d7m of height h, a convolution operation on h consecutive word vectors starting from tth word outputs the scalar feature\nct = ReLU(Wf \u2022Xt:t+h\u22121 + bf )\nwhere Xt:t+h\u22121 \u2208 Rh\u00d7m is the matrix whose ith row is xi \u2208 Rm, and bf \u2208 R is a bias. The symbol \u2022 refers to the dot product and ReLU(\u00b7) is the elementwise rectified linear unit function.\nWe perform convolution operations with n different filters, and denote the resulting features as ct \u2208 Rn, each of whose dimensions comes from a distinct filter. Repeating the convolution operations\nfor each window of h consecutive words in the shorttext, we obtain c1:`\u2212h+1. The short-text representation s \u2208 Rn is computed in the max pooling layer, as the element-wise maximum of c1:`\u2212h+1."}, {"heading": "2.2 Sequential short-text classification", "text": "Let si be the n-dimensional short-text representation given by the RNN or CNN architecture for the ith short text in the sequence. The sequence si\u2212d1\u2212d2 : i is fed into a two-layer feedforward ANN that predicts the class for the ith short text. The hyperparameters d1, d2 are the history sizes used in the first and second layers, respectively.\nThe first layer takes as input si\u2212d1\u2212d2 : i and outputs the sequence yi\u2212d2 : i defined as\nyj = tanh ( d1\u2211 d=0 W\u2212d sj\u2212d + b1 ) , \u2200j \u2208 [i\u2212 d2, i]\nwhere W0,W\u22121,W\u22122 \u2208 Rk\u00d7n are the weight matrices, b1 \u2208 Rk is the bias vector, yj \u2208 Rk is the class representation, and k is the number of classes for the classification task.\nSimilarly, the second layer takes as input the sequence of class representations yi\u2212d2:i and outputs zi \u2208 Rk:\nzi = softmax  d2\u2211 j=0 W\u2212j yi\u2212j + b2  where U0, U\u22121, U\u22122 \u2208 Rk\u00d7k and b2 \u2208 Rk are the weight matrices and bias vector.\nThe final output zi represents the probability distribution over the set of k classes for the ith shorttext: the jth element of zi corresponds to the probability that the ith short-text belongs to the jth class."}, {"heading": "3 Datasets and Experimental Setup", "text": ""}, {"heading": "3.1 Datasets", "text": "We evaluate our model on the dialog act classification task using the following datasets:\n\u2022 DSTC 4: Dialog State Tracking Challenge 4 (Kim et al., 2015; Kim et al., 2016).\n\u2022 MRDA: ICSI Meeting Recorder Dialog Act Corpus (Janin et al., 2003; Shriberg et al., 2004). The 5 classes are introduced in (Ang et al., 2005).\n\u2022 SwDA: Switchboard Dialog Act Corpus (Jurafsky et al., 1997).\nFor MRDA, we use the train/validation/test splits provided with the datasets. For DSTC 4 and SwDA, only the train/test splits are provided.1 Table 1 presents statistics on the datasets."}, {"heading": "3.2 Training", "text": "The model is trained to minimize the negative loglikelihood of predicting the correct dialog acts of the utterances in the train set, using stochastic gradient descent with the Adadelta update rule (Zeiler, 2012). At each gradient descent step, weight matrices, bias vectors, and word vectors are updated. For regularization, dropout is applied after the pooling layer, and early stopping is used on the validation set with a patience of 10 epochs."}, {"heading": "4 Results and Discussion", "text": "To find effective hyperparameters, we varied one hyperparameter at a time while keeping the other ones fixed. Table 2 presents our hyperparameter choices.\nWe initialized the word vectors with the 300- dimensional word vectors pretrained with word2vec on Google News (Mikolov et al., 2013a; Mikolov et al., 2013b) for DSTC 4, and the 200-dimensional\n1All train/validation/test splits can be found at https:// github.com/Franck-Dernoncourt/naacl2016\nword vectors pretrained with GloVe on Twitter (Pennington et al., 2014) for MRDA and SwDA, as these choices yielded the best results among all publicly available word2vec, GloVe, SENNA (Collobert, 2011; Collobert et al., 2011) and RNNLM (Mikolov et al., 2011) word vectors.\nThe effects of the history sizes d1 and d2 for the short-text and the class representations, respectively, are presented in Table 3 for both the LSTM and CNN models. In both models, increasing d1 while keeping d2 = 0 improved the performances by 1.3- 4.2 percentage points. Conversely, increasing d2 while keeping d1 = 0 yielded better results, but the performance increase was less pronounced: incorporating sequential information at the short-text representation level was more effective than at the class representation level.\nUsing sequential information at both the shorttext representation level and the class representation level does not help in most cases and may even lower the performances. We hypothesize that shorttext representations contain richer and more general information than class representations due to their larger dimension. Class representations may not convey any additional information over shorttext representations, and are more likely to propagate errors from previous misclassifications.\nTable 4 compares our results with the state-of-theart. Overall, our model shows competitive results, while requiring no human-engineered features. Rigorous comparisons are challenging to draw, as many important details such as text preprocessing and train/valid/test split may vary, and many studies fail\nto perform several runs despite the randomness in some parts of the training process, such as weight initialization."}, {"heading": "5 Conclusion", "text": "In this article we have presented an ANN-based approach to sequential short-text classification. We demonstrate that adding sequential information improves the quality of the predictions, and the performance depends on what sequential information is used in the model. Our model achieves state-of-theart results on three different datasets for dialog act prediction."}], "references": [{"title": "Automatic dialog act segmentation and classification in multiparty meetings", "author": ["Ang et al.2005] Jeremy Ang", "Yang Liu", "Elizabeth Shriberg"], "venue": null, "citeRegEx": "Ang et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Ang et al\\.", "year": 2005}, {"title": "A convolutional neural network for modelling sentences", "author": ["Blunsom et al.2014] Phil Blunsom", "Edward Grefenstette", "Nal Kalchbrenner"], "venue": "In Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics. Proceedings of the 52nd", "citeRegEx": "Blunsom et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Blunsom et al\\.", "year": 2014}, {"title": "On the properties of neural machine translation: Encoderdecoder approaches", "author": ["Cho et al.2014] Kyunghyun Cho", "Bart van Merri\u00ebnboer", "Dzmitry Bahdanau", "Yoshua Bengio"], "venue": "arXiv preprint arXiv:1409.1259", "citeRegEx": "Cho et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Cho et al\\.", "year": 2014}, {"title": "Natural language processing (almost) from scratch", "author": ["Jason Weston", "L\u00e9on Bottou", "Michael Karlen", "Koray Kavukcuoglu", "Pavel Kuksa"], "venue": "The Journal of Machine Learning Research,", "citeRegEx": "Collobert et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Collobert et al\\.", "year": 2011}, {"title": "Deep learning for efficient discriminative parsing", "author": ["Ronan Collobert"], "venue": "In International Conference on Artificial Intelligence and Statistics,", "citeRegEx": "Collobert.,? \\Q2011\\E", "shortCiteRegEx": "Collobert.", "year": 2011}, {"title": "AdobeMIT submission to the DSTC 4 Spoken Language Understanding pilot task", "author": ["Ji Young Lee", "Trung H. Bui", "Hung H. Bui"], "venue": "In 7th International Workshop on Spoken Dialogue Systems (IWSDS)", "citeRegEx": "Dernoncourt et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Dernoncourt et al\\.", "year": 2016}, {"title": "Long short-term memory", "author": ["Hochreiter", "Schmidhuber1997] Sepp Hochreiter", "J\u00fcrgen Schmidhuber"], "venue": "Neural computation,", "citeRegEx": "Hochreiter et al\\.,? \\Q1997\\E", "shortCiteRegEx": "Hochreiter et al\\.", "year": 1997}, {"title": "The ICSI meeting corpus", "author": ["Janin et al.2003] Adam Janin", "Don Baron", "Jane Edwards", "Dan Ellis", "David Gelbart", "Nelson Morgan", "Barbara Peskin", "Thilo Pfau", "Elizabeth Shriberg", "Andreas Stolcke"], "venue": "In Acoustics, Speech, and Signal Processing,", "citeRegEx": "Janin et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Janin et al\\.", "year": 2003}, {"title": "Backoff model training using partially observed data: application to dialog act tagging", "author": ["Ji", "Bilmes2006] Gang Ji", "Jeff Bilmes"], "venue": "In Proceedings of the main conference on Human Language Technology Conference of the North American Chapter", "citeRegEx": "Ji et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Ji et al\\.", "year": 2006}, {"title": "Switchboard SWBDDAMSL shallow-discourse-function annotation coders manual", "author": ["Elizabeth Shriberg", "Debra"], "venue": "Biasca", "citeRegEx": "Jurafsky et al\\.,? \\Q1997\\E", "shortCiteRegEx": "Jurafsky et al\\.", "year": 1997}, {"title": "A convolutional neural network for modelling sentences", "author": ["Edward Grefenstette", "Phil Blunsom"], "venue": "arXiv preprint arXiv:1404.2188", "citeRegEx": "Kalchbrenner et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Kalchbrenner et al\\.", "year": 2014}, {"title": "Dialog State Tracking Challenge 4: Handbook", "author": ["Kim et al.2015] Seokhwan Kim", "Luis Fernando D\u2019Haro", "Rafael E. Banchs", "Jason Williams", "Matthew Henderson"], "venue": null, "citeRegEx": "Kim et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Kim et al\\.", "year": 2015}, {"title": "The Fourth Dialog State Tracking Challenge", "author": ["Kim et al.2016] Seokhwan Kim", "Luis Fernando D\u2019Haro", "Rafael E. Banchs", "Jason Williams", "Matthew Henderson"], "venue": "In Proceedings of the 7th International Workshop on Spoken Dialogue Systems (IWSDS)", "citeRegEx": "Kim et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Kim et al\\.", "year": 2016}, {"title": "Convolutional neural networks for sentence classification", "author": ["Yoon Kim"], "venue": "In Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing,", "citeRegEx": "Kim.,? \\Q2014\\E", "shortCiteRegEx": "Kim.", "year": 2014}, {"title": "Token-based chunking of turninternal dialogue act sequences", "author": ["Lendvai", "Geertzen2007] Piroska Lendvai", "Jeroen Geertzen"], "venue": "In Proceedings of the 8th SIGDIAL Workshop on Discourse and Dialogue,", "citeRegEx": "Lendvai et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Lendvai et al\\.", "year": 2007}, {"title": "Rnnlm-recurrent neural network language modeling toolkit", "author": ["Stefan Kombrink", "Anoop Deoras", "Lukar Burget", "Jan Cernocky"], "venue": "In Proc. of the 2011 ASRU Workshop,", "citeRegEx": "Mikolov et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Mikolov et al\\.", "year": 2011}, {"title": "Efficient estimation of word representations in vector space. arXiv preprint arXiv:1301.3781", "author": ["Kai Chen", "Greg Corrado", "Jeffrey Dean"], "venue": null, "citeRegEx": "Mikolov et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Mikolov et al\\.", "year": 2013}, {"title": "Distributed representations of words and phrases and their compositionality", "author": ["Ilya Sutskever", "Kai Chen", "Greg S Corrado", "Jeff Dean"], "venue": "In Advances in neural information processing systems,", "citeRegEx": "Mikolov et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Mikolov et al\\.", "year": 2013}, {"title": "Dependency tree-based sentiment classification using CRFs with hidden variables", "author": ["Kentaro Inui", "Sadao Kurohashi"], "venue": "In Human Language Technologies: The 2010 Annual Conference of the North American Chapter", "citeRegEx": "Nakagawa et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Nakagawa et al\\.", "year": 2010}, {"title": "GloVe: global vectors for word representation", "author": ["Richard Socher", "Christopher D Manning"], "venue": "Proceedings of the Empiricial Methods in Natural Language Processing", "citeRegEx": "Pennington et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Pennington et al\\.", "year": 2014}, {"title": "Dialogue act classification using language models", "author": ["Reithinger", "Klesen1997] Norbert Reithinger", "Martin Klesen"], "venue": "In EuroSpeech. Citeseer", "citeRegEx": "Reithinger et al\\.,? \\Q1997\\E", "shortCiteRegEx": "Reithinger et al\\.", "year": 1997}, {"title": "Dialog act tagging using memory-based learning", "author": ["Mihai Rotaru"], "venue": "Term project, University of Pittsburgh,", "citeRegEx": "Rotaru.,? \\Q2002\\E", "shortCiteRegEx": "Rotaru.", "year": 2002}, {"title": "Bidirectional recurrent neural networks", "author": ["Schuster", "Paliwal1997] Mike Schuster", "Kuldip K Paliwal"], "venue": "Signal Processing, IEEE Transactions", "citeRegEx": "Schuster et al\\.,? \\Q1997\\E", "shortCiteRegEx": "Schuster et al\\.", "year": 1997}, {"title": "The ICSI meeting recorder dialog act (MRDA", "author": ["Raj Dhillon", "Sonali Bhagat", "Jeremy Ang", "Hannah Carvey"], "venue": null, "citeRegEx": "Shriberg et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Shriberg et al\\.", "year": 2004}, {"title": "From symbolic to sub-symbolic information in question classification", "author": ["Silva et al.2011] Joao Silva", "Lu\u0131\u0301sa Coheur", "Ana Cristina Mendes", "Andreas Wichert"], "venue": "Artificial Intelligence Review,", "citeRegEx": "Silva et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Silva et al\\.", "year": 2011}, {"title": "Semantic compositionality through recursive matrixvector spaces", "author": ["Brody Huval", "Christopher D Manning", "Andrew Y Ng"], "venue": "In Proceedings of the 2012 Joint Conference on Empirical Methods in Natural", "citeRegEx": "Socher et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Socher et al\\.", "year": 2012}, {"title": "Dialogue act modeling for automatic tagging and recognition", "author": ["Klaus Ries", "Noah Coccaro", "Elizabeth Shriberg", "Rebecca Bates", "Daniel Jurafsky", "Paul Taylor", "Rachel Martin", "Carol Van EssDykema", "Marie Meteer"], "venue": null, "citeRegEx": "Stolcke et al\\.,? \\Q2000\\E", "shortCiteRegEx": "Stolcke et al\\.", "year": 2000}, {"title": "Baselines and bigrams: Simple, good sentiment and topic classification", "author": ["Wang", "Manning2012] Sida Wang", "Christopher D Manning"], "venue": "In Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics:", "citeRegEx": "Wang et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Wang et al\\.", "year": 2012}, {"title": "Adadelta: An adaptive learning rate method", "author": ["Matthew D Zeiler"], "venue": "arXiv preprint arXiv:1212.5701", "citeRegEx": "Zeiler.,? \\Q2012\\E", "shortCiteRegEx": "Zeiler.", "year": 2012}], "referenceMentions": [{"referenceID": 24, "context": "Many different approaches have been developed for short-text classification, such as using Support Vector Machines (SVMs) with rule-based features (Silva et al., 2011), combining SVMs with naive Bayes (Wang and Manning, 2012), and building dependency trees with Conditional Random Fields (Nakagawa et al.", "startOffset": 147, "endOffset": 167}, {"referenceID": 18, "context": ", 2011), combining SVMs with naive Bayes (Wang and Manning, 2012), and building dependency trees with Conditional Random Fields (Nakagawa et al., 2010).", "startOffset": 128, "endOffset": 151}, {"referenceID": 13, "context": "Several recent studies using ANNs have shown promising results, including convolutional neural networks (CNNs) (Kim, 2014; Blunsom et al., 2014; Kalchbrenner et al., 2014) and recursive neural networks (Socher et al.", "startOffset": 111, "endOffset": 171}, {"referenceID": 1, "context": "Several recent studies using ANNs have shown promising results, including convolutional neural networks (CNNs) (Kim, 2014; Blunsom et al., 2014; Kalchbrenner et al., 2014) and recursive neural networks (Socher et al.", "startOffset": 111, "endOffset": 171}, {"referenceID": 10, "context": "Several recent studies using ANNs have shown promising results, including convolutional neural networks (CNNs) (Kim, 2014; Blunsom et al., 2014; Kalchbrenner et al., 2014) and recursive neural networks (Socher et al.", "startOffset": 111, "endOffset": 171}, {"referenceID": 25, "context": ", 2014) and recursive neural networks (Socher et al., 2012).", "startOffset": 38, "endOffset": 59}, {"referenceID": 26, "context": "Previous works on sequential short-text classification are mostly based on non-ANN approaches, such as Hidden Markov Models (HMMs) (Reithinger and Klesen, 1997), (Stolcke et al., 2000), maximum entropy (Ang et al.", "startOffset": 162, "endOffset": 184}, {"referenceID": 0, "context": ", 2000), maximum entropy (Ang et al., 2005), and naive Bayes (Lendvai and Geertzen, 2007).", "startOffset": 25, "endOffset": 43}, {"referenceID": 26, "context": "Its accurate detection is useful for a range of applications, from speech recognition to automatic summarization (Stolcke et al., 2000).", "startOffset": 113, "endOffset": 135}, {"referenceID": 11, "context": "\u2022 DSTC 4: Dialog State Tracking Challenge 4 (Kim et al., 2015; Kim et al., 2016).", "startOffset": 44, "endOffset": 80}, {"referenceID": 12, "context": "\u2022 DSTC 4: Dialog State Tracking Challenge 4 (Kim et al., 2015; Kim et al., 2016).", "startOffset": 44, "endOffset": 80}, {"referenceID": 7, "context": "\u2022 MRDA: ICSI Meeting Recorder Dialog Act Corpus (Janin et al., 2003; Shriberg et al., 2004).", "startOffset": 48, "endOffset": 91}, {"referenceID": 23, "context": "\u2022 MRDA: ICSI Meeting Recorder Dialog Act Corpus (Janin et al., 2003; Shriberg et al., 2004).", "startOffset": 48, "endOffset": 91}, {"referenceID": 0, "context": "The 5 classes are introduced in (Ang et al., 2005).", "startOffset": 32, "endOffset": 50}, {"referenceID": 9, "context": "\u2022 SwDA: Switchboard Dialog Act Corpus (Jurafsky et al., 1997).", "startOffset": 38, "endOffset": 61}, {"referenceID": 28, "context": "The model is trained to minimize the negative loglikelihood of predicting the correct dialog acts of the utterances in the train set, using stochastic gradient descent with the Adadelta update rule (Zeiler, 2012).", "startOffset": 198, "endOffset": 212}, {"referenceID": 2, "context": "We also tried a variant of the LSTM model, gated recurrent units (Cho et al., 2014), but the results were generally lower than LSTM.", "startOffset": 65, "endOffset": 83}, {"referenceID": 19, "context": "word vectors pretrained with GloVe on Twitter (Pennington et al., 2014) for MRDA and SwDA, as these choices yielded the best results among all publicly available word2vec, GloVe, SENNA (Collobert, 2011; Collobert et al.", "startOffset": 46, "endOffset": 71}, {"referenceID": 4, "context": ", 2014) for MRDA and SwDA, as these choices yielded the best results among all publicly available word2vec, GloVe, SENNA (Collobert, 2011; Collobert et al., 2011) and RNNLM (Mikolov et al.", "startOffset": 121, "endOffset": 162}, {"referenceID": 3, "context": ", 2014) for MRDA and SwDA, as these choices yielded the best results among all publicly available word2vec, GloVe, SENNA (Collobert, 2011; Collobert et al., 2011) and RNNLM (Mikolov et al.", "startOffset": 121, "endOffset": 162}, {"referenceID": 15, "context": ", 2011) and RNNLM (Mikolov et al., 2011) word vectors.", "startOffset": 18, "endOffset": 40}, {"referenceID": 5, "context": "SVM: (Dernoncourt et al., 2016).", "startOffset": 5, "endOffset": 31}, {"referenceID": 26, "context": "HMM: (Stolcke et al., 2000).", "startOffset": 5, "endOffset": 27}, {"referenceID": 21, "context": "Memory-based Learning: (Rotaru, 2002).", "startOffset": 23, "endOffset": 37}], "year": 2016, "abstractText": "Recent approaches based on artificial neural networks (ANNs) have shown promising results for short-text classification. However, many short texts occur in sequences (e.g., sentences in a document or utterances in a dialog), and most existing ANN-based systems do not leverage the preceding short texts when classifying a subsequent one. In this work, we present a model based on recurrent neural networks and convolutional neural networks that incorporates the preceding short texts. Our model achieves state-of-the-art results on three different datasets for dialog act prediction.", "creator": "LaTeX with hyperref package"}}}