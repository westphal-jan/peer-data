{"id": "1603.05157", "review": {"conference": "HLT-NAACL", "VERSION": "v1", "DATE_OF_SUBMISSION": "16-Mar-2016", "title": "Comparing Convolutional Neural Networks to Traditional Models for Slot Filling", "abstract": "We address relation classification in the context of slot filling, the task of finding and evaluating fillers like \"Steve Jobs\" for the slot X in \"X founded Apple\". We propose a convolutional neural network which splits the input sentence into three parts according to the relation arguments and compare it to state-of-the-art and traditional approaches of relation classification. Finally, we combine different methods and show that the combination is better than individual approaches. We also analyze the effect of genre differences on performance. We also present our proposal to explain why the algorithm is more accurate and more efficient than conventional processing (R.J.R. and M.W.T.K.T.K.T.K.T.K.T.K.T.K.T.K.T.K.T.K.T.K.T.K.T.K.T.K.T.K.T.K.T.K.T.K.T.K.T.K.T.K.T.K.T.K.T.K.T.K.T.K.T.K.T.K.T.K.T.K.T.K.T.K.T.K.T.K.T.K.T.K.T.K.T.K.T.K.T.K.T.K.T.K.T.K.T.K.T.K.T.K.T.K.T.K.T.K.T.K.T.K.T.K.T.K.T.K.T.K.T.K.T.K.T.K.T.K.T.K.T.K.T.K.T.K.T.K.T.K.T.K.T.K.T.K.T.K.T.K.T.K.T.K.T.K.T.K.T.K.T.K.T.K.T.K.T.K.T.K.T.K.T.K.T.K.T.K.T.K.T.K.T.K.T.K.T.K.T.K.T.K.T.K.T.K.T.K.T.K.T.K.", "histories": [["v1", "Wed, 16 Mar 2016 16:02:03 GMT  (80kb,D)", "http://arxiv.org/abs/1603.05157v1", "NAACL 2016"], ["v2", "Mon, 4 Apr 2016 14:54:28 GMT  (80kb,D)", "http://arxiv.org/abs/1603.05157v2", "NAACL 2016"]], "COMMENTS": "NAACL 2016", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["heike adel", "benjamin roth", "hinrich sch\u00fctze"], "accepted": true, "id": "1603.05157"}, "pdf": {"name": "1603.05157.pdf", "metadata": {"source": "CRF", "title": "Comparing Convolutional Neural Networks to Traditional Models for Slot Filling", "authors": ["Heike Adel"], "emails": ["heike@cis.lmu.de"], "sections": [{"heading": "1 Introduction", "text": "Structured knowledge about the world is useful for many natural language processing (NLP) tasks, such as disambiguation, question answering or semantic search. However, the extraction of structured information from natural language text is challenging because one relation can be expressed in many different ways. The TAC Slot Filling (SF) Shared Task defines slot filling as extracting fillers for a set of predefined relations (\u201cslots\u201d) from a large corpus of text data. Exemplary relations are the city of birth of a person or the employees or founders of a company. Participants are provided with an evaluation corpus and a query file consisting of pairs of entities and slots. For each entity slot pair (e.g. \u201cApple\u201d and \u201cfounded by\u201d), the systems have to return the second argument (\u201cfiller\u201d) of the relation (e.g. \u201cSteve Jobs\u201d) as well as a supporting sentence from the evaluation corpus. The key challenge in slot\nfilling is relation classification: given a sentence s of the evaluation corpus containing the name of a queried entity (e.g., \u201cApple\u201d) and a filler candidate (e.g., \u201cSteve Jobs\u201d), we need to decide whether s expresses the relation (\u201cfounded by\u201d, in this case). We will refer to the mentions of the two arguments of the relation as name and filler. Performance on relation classification is crucial for slot filling since its effectiveness directly depends on it.\nIn this paper, we investigate three complementary approaches to relation classification.\nThe first approach is pattern matching, a leading approach in the TAC evaluations. Fillers are validated based on patterns. In this work, we consider patterns learned with distant supervision and patterns extracted from Universal Schema relations.\nThe second approach is support vector machines. We evaluate two different feature sets: a bag-ofword feature set (BOW) and more sophisticated skip n-gram features.\nOur third approach is a convolutional neural network (CNN). CNNs have been applied to NLP tasks like sentiment analysis, part-of-speech tagging and semantic role labeling. They can recognize phrase patterns independent of their position in the sentence. Furthermore, they make use of word embeddings that directly reflect word similarity (Mikolov et al., 2013). Hence, we expect them to be robust models for the task of classifying filler candidates and to generalize well to unseen test data. In this work, we train different variants of CNNs: As a baseline, we re-implement the recently developed piecewise CNN (Zeng et al., 2015). Then, we extend this model by splitting the contexts not only for ar X\niv :1\n60 3.\n05 15\n7v 1\n[ cs\n.C L\n] 1\n6 M\nar 2\npooling but also for convolution (contextwise CNN). Currently, there is no benchmark for slot filling. Therefore, it is not possible to directly compare results that were submitted to the Shared Task to new results. Comparable manual annotations for new results, for instance, cannot be easily obtained. There are also many different system components, such as document retrieval from the evaluation corpus and coreference resolution, that affect Shared Task performance and that are quite different in nature from relation classification. Even in the subtask of relation classification, it is not possible to directly use existing relation classification benchmarks (e.g. Riedel et al. (2013), Hendrickx et al. (2010)) since data and relations can be quite different. Many benchmark relations, for instance, correspond to Freebase relations but not all slots are modeled in Freebase and some slots even comprise more than one Freebase relation. While most relation classification benchmarks either use newswire or web data, the SF task includes documents from both domains (and discussion fora). Another difference to traditional relation classification benchmarks is the pipeline aspect of slot filling. Depending on the previous steps, the input for the relation classification models can be incomplete, noisy, include coreferent mentions, etc.\nThe official SF Shared Task evaluations only assess whole systems (with potential subsequent faults in their pipelines (Pink et al., 2014)). Thus, we expect component wise comparisons to be a valuable addition to the Shared Task: With comparisons of single components, teams would be able to improve their modules more specifically. To start with one of the most important components, we have created a benchmark for slot filling relation classification, based on 2012 \u2013 2014 TAC Shared Task data. It will be described below and published along with this paper. In addition to presenting model results on this benchmark dataset, we also show that these results correlate with end-to-end SF results. Hence, optimizing a model on this dataset will also help improving results in the end-to-end setting.\nIn our experiments, we found that our models suffer from large genre differences in the TAC data. Hence, the SF Shared Task is a task that conflates an investigation of domain (or genre) adaptation with the one of slot filling. We argue that both problems\nare important NLP problems and provide datasets and results for both within and across genres. We hope that this new resource will encourage others to test their models on our dataset and that this will help promote research on slot filling.\nIn summary, our contributions are as follows. (i) We investigate the complementary strengths and weaknesses of different approaches to relation classification and show that their combination can better deal with a diverse set of problems that slot filling poses than each of the approaches individually. (ii) We propose to split the context at the relation arguments before passing it to the CNN in order to better deal with the special characteristics of a sentence in relation classification. This outperforms the state-of-the-art piecewise CNN. (iii) We analyze the effect of genre on slot filling and show that it is an important conflating variable that needs to be carefully examined in research on slot filling. (iv) We provide a benchmark for slot filling relation classification that will facilitate direct comparisons of models in the future and show that results on this dataset are correlated with end-to-end system results."}, {"heading": "2 Related work", "text": "Slot filling. The participants of the SF Shared Task (Surdeanu, 2013) are provided with a large text corpus. For evaluation, they get a collection of queries and need to provide fillers for predefined relations and an offset of a context which can serve as a justification. Most participants apply pipeline based systems. Pink et al. (2014) analyzed sources of recall losses in these pipelines. The results of the systems show the difficulty of the task: In the 2014 evaluation, the top-ranked system had an F1 of .37 (Angeli et al., 2014a). To train their models, most groups use distant supervision (Mintz et al., 2009). The topranked systems apply machine learning based approaches rather than manually developed patterns or models (Surdeanu and Ji, 2014). The methods for extracting and scoring candidates range from pattern based approaches (Gonza\u0300lez et al., 2012; Liu and Zhao, 2012; Li et al., 2012; Qiu et al., 2012; Roth et al., 2014) over rule based systems (Varma et al., 2012) to classifiers (Malon et al., 2012; Roth et al., 2013). The top ranked system from 2013 used SVMs and patterns for evaluating filler candi-\ndates (Roth et al., 2013); their results suggest that n-gram based features are sufficient to build reliable classifiers for the relation classification module. They also show that SVMs outperform patterns.\nCNNs and baseline models for relation classification. Zeng et al. (2014) and Dos Santos et al. (2015) applied CNNs to the relation classification SemEval Shared Task data from 2010 and showed that CNNs outperform other models. We train CNNs on noisy distant supervised data since (in contrast to the SemEval Shared Task) clean training sets are not available. Malon et al. (2012) described a CNN for slot filling that is based on the output of a parser. We plan to explore parsing for creating a more linguistically motivated input representation in the future.\nIn this paper, we will compare our methods against traditional relation classification models: Mintz++ (Mintz et al., 2009; Surdeanu et al., 2012) and MIMLRE (Surdeanu et al., 2012). Mintz++ is a model based on the Mintz features (lexical and syntactic features for relation extraction). It was developed by Surdeanu et al. (2012) and used as a baseline model by them. MIMLRE is a graphical model designed to cope with multiple instances and multiple labels in distant supervised data. It is trained with Expectation Maximization.\nAnother baseline model which we use in this work is a piecewise convolutional neural network (Zeng et al., 2015). This recently published network is designed especially for the relation classification task which allows to split the context into three parts around the two relation arguments. While it uses the whole context for convolution, it performs max pooling over the three parts individually. In contrast, we propose to split the context even earlier and apply the convolutional filters to each part separately.\nGenre dependency. There are many studies showing the genre dependency of machine learning models. In 2012, the SANCL Shared Task focused on evaluating models on web data that have been trained on news data (Petrov and McDonald, 2012). The results showed that POS tagging performance can decline a lot when the genre is changed. For other NLP tasks like machine translation or sentiment analysis, this is also a well-known challenge and domain adaptation has been extensively studied (Glorot et al., 2011; Foster and Kuhn, 2007). We do not investigate domain adaptation per se, but\nshow that the genre composition of the slot filling source corpus poses challenges to genre independent models."}, {"heading": "3 Challenges of slot filling", "text": "Slot filling includes NLP challenges of various natures. Given a large evaluation corpus, systems first need to find documents relevant to the entity of the query. This involves challenges like alternate names for the same entity, misspellings of names and ambiguous names (different entities with the same name). Then for each relevant document, sentences with mentions of the entity need to be extracted, as well as possible fillers for the given slot. In most cases, coreference resolution and named entity recognition tools are used for these tasks. Finally, the systems need to decide which filler candidate to output as the solution for the given slot. This step can be reduced to relation classification. It is one of the most crucial parts of the whole pipeline since it directly influences the quality of the final output. The most important challenges for relation classification for slot filling are little or noisy (distant supervised) training data, data from different domains and test sentences which have been extracted with a pipeline of different NLP components. Thus, their quality directly depends on the performance of the whole pipeline. If, for example, sentence splitting failed, the input can be incomplete or too long. If coreference resolution or named entity recognition failed, the relation arguments can be wrong or incomplete."}, {"heading": "4 Models for relation classification", "text": "Patterns. The first approach we evaluate for relation classification is pattern matching. For a given sentence, the pattern matcher classifies the relation as correct if one of the patterns matches; otherwise the candidate is rejected. In particular, we apply two different pattern sets: The first set consists of patterns learned using distant supervision (PATdist). They have been used in the SF challenge by the topranked system in the 2013 Shared Task (Roth et al., 2013). The second set contains patterns from universal schema relations for the SF task (PATuschema). Universal schema relations are extracted based on matrix factorization (Riedel et al., 2013). In this\nwork, we apply the universal schema patterns extracted for slot filling by Roth et al. (2014).\nSupport vector machines (SVMs). Our second approach is support vector machines. We evaluate two different feature sets: bag-of-word features (SVMbow) and skip n-gram features (SVMskip). Based on the results of Roth et al. (2013), we will not use additional syntactic or semantic features for our classifiers. For SVMbow, the representation of a sentence consists of a flag and four bag-of-word vectors. Let m1 and m2 be the mentions of name and filler (or filler and name) in the sentence, with m1 occurring before m2. The binary flag indicates in which order name and filler occur. The four BOW vectors contain the words in the sentence to the left of m1, between m1 and m2, to the right of m2 and all words of the sentence. For SVMskip, we use the previously described BOW features and additionally a feature vector which contains skip n-gram features. They wildcard tokens in the middle of the n-gram (cf. (Roth et al., 2013)). In particular, we use skip 3-grams, skip 4-grams and skip 5-grams. A possible skip 4-gram of the context \u201c, founder and director of\u201d, for example, would be the string \u201cfounder of\u201d, a pattern that could not have been directly extracted from this context otherwise. We train one linear SVM (Fan et al., 2008) for each relation and feature set and tune parameter C on dev.\nConvolutional neural networks (CNNs). CNNs are increasingly applied in NLP (Collobert et al., 2011; Kalchbrenner et al., 2014). They extract ngram based features independent of the position in the sentence and create (sub-)sentence representations. The two most important aspects that make this possible are convolution and pooling. Max pooling (Collobert et al., 2011) detects the globally most relevant features obtained by local convolution.\nAnother promising aspect of CNNs for relation classification is that they use an embedding based input representation. With word embeddings, similar words are represented by similar vectors and, thus, we can recognize (near-)synonyms \u2013 synonyms of relation triggers as well as of other important context words. If the CNN has learned, for example, that the context \u201cis based in\u201d triggers the relation location of headquarters and that \u201cbased\u201d has a similar vector representation as \u201clocated\u201d, it may recognize the context \u201cis located in\u201d correctly as another\ntrigger for the same relation even if it has never seen it during training. In the following paragraphs, we describe the different variants of CNNs which we evaluate in this paper. For each variant, we train one binary CNN per slot and optimize the number of filters (\u2208 {300, 1000, 3000}), the size of the hidden layer (\u2208 {100, 300, 1000}) and the filter width (\u2208 {3, 5}) on dev. We use word2vec (Mikolov et al., 2013) to pre-train word embeddings (dimensionality d = 50) on a May-2014 English Wikipedia corpus.\nPiecewise CNN. Our baseline CNN is the model developed by Zeng et al. (2015). It represents the input sentence by a matrix of word vectors, applies several filters for convolution and then divides the resulting n-gram representation into left, middle and right context based on the positions m1 and m2 of name and filler (see SVM description). For each of the three parts, one max value is extracted by pooling. The results are passed to a softmax classifier.\nContextwise CNN. In contrast to the piecewise CNN, we propose to split the context before convolution. Hence, similar to our BOW vectors for the SVM, we split the original context words into left, middle and right context. Then, we apply convolution and pooling to each of the contexts separately. In contrast to the piecewise CNN, there is no convolution across relation arguments. Thus, the network learns to focus on the context words and cannot be distracted by the presence of (always present) rela-\ntion arguments. The filter weights W are shared for the three contexts. Our intuition is that the most important sequence features we want to extract by convolution can appear in two or three of the regions. Weight sharing also reduces the number of parameters and increases robustness.1 The results of convolution are pooled using k-max pooling (Kalchbrenner et al., 2014): only the k = 3 maximum values of each filter application are kept. The pooling results are then concatenated to a single vector and extended by a flag indicating whether the name or the filler appeared first in the sentence.\nIn initial experiments, we found that a fully connected hidden layer after convolution and pooling leads to a more powerful model. It connects the representations of the three contexts and, thus, can draw conclusions based on cooccurring patterns across contexts. Therefore, the result vector after convolution and pooling is fed into a fully connected hidden layer. A softmax layer makes the final decision.\nFor a fair comparison of models, we also add a hidden layer to the piecewise CNN and apply kmax pooling there as well. Thus, the number of parameters to learn is the same for both models. We call this model CNNpieceExt. The key difference between CNNpieceExt and CNNcontext is the time when the context is split into three parts: before or after convolution. This affects the windows of words to which the convolution filters are applied.\nModel combination (CMB). To combine a setM of models for classification, we perform a simple linear combination of the scores of the models: qCMB = \u2211 m=1...M \u03b1mqm where qm is the score of model m and \u03b1m is its weight (optimized on dev using grid search). All weights sum to 1.\nFor a comparison of different combination possibilities, see, for example, (Viswanathan et al., 2015)."}, {"heading": "5 Experiments and results", "text": ""}, {"heading": "5.1 Training data", "text": "We used distant supervision for generating training data. We created a set of (subject, relation, object) tuples by querying Freebase (Bollacker et al., 2008) for relations that correspond to one of the\n1In initial experiments, sharing filter weights across left, middle, right outperformed not sharing weights\nslot relations. Then we scanned the following corpora for sentences containing both arguments of a relation in the tuple set: (i) the TAC source corpus (TAC, 2014), (ii) a snapshot of Wikipedia (May 2014), (iii) the Freebase description fields, (iv) a subset of Clueweb2, (v) a New York Times corpus (LDC2008T19). The resulting sentences are positive training examples. Based on the tuple set, we selected negative examples by scanning the corpora for sentences that (i) contain a mention of a name occurring in a tuple, (ii) do not contain the correct filler, (iii) contain a mention different from the correct filler, but with the same named entity type (based on CoreNLP NER (Manning et al., 2014)). All negative examples for date slots, for instance, are sentences containing an incorrect date.\nThis procedure gave us a large but noisy training set for most slots. In order to reduce incorrect labels, we applied a self-training procedure: We trained SVMs on the SF dataset created by Angeli et al. (2014b). With the resulting SVMs, we predicted labels for our training set. If the predicted label did not match the distant supervised label, we deleted the corresponding training example (Min et al., 2012). This procedure was conducted in several iterations on different chunks of the training set. Finally, the SF dataset and the filtered training examples were merged.3 Since their contexts are similar, we also merged city, state-or-province and country slots to one location slot."}, {"heading": "5.2 Evaluation data", "text": "One of the main challenges in building and evaluating relation classification models for SF is the shortage of training and evaluation data. Each group has their own datasets and comparisons across groups are difficult. Therefore, we have developed a script that creates a clean dataset based on manually annotated system outputs from previous Shared Task evaluations. In the future, it can be used by all participants to evaluate components of their slot filling systems.4 The script only extracts sentences that\n2http://lemurproject.org/clueweb12 3We do not use the SF dataset directly because (i) it provides few examples per slot (min: 1, max: 4960) and (ii) it consists of examples for which the classifiers of Angeli et al. (2014b) were indecisive, i.e., presumably contexts that are hard to classify.\n4 We publish scripts since we cannot distribute data.\ncontain mentions of both name and filler. It conducts a heuristic check based on NER tags to determine whether the name in the sentence is a valid mention of the query name or is referring to another entity. In the latter case, the example is filtered out. One difficulty is that many published offsets are incorrect. We tried to match these using heuristics. In general, we apply filters that ensure high quality of the resulting evaluation data even if that means that a considerable part of the TAC system output is discarded. In total, we extracted 39,386 high-quality evaluation instances out of the 59,755 system output instances published by TAC and annotated as either completely correct or completely incorrect.\nA table in the supplementary material gives statistics: the number of positive and negative examples per slot and year (without duplicates). For 2013, the most examples were extracted. The lower number for 2014 is probably due to the newly introduced inference across documents. This limits the number of sentences with mentions of both name and filler. The average ratio of positive to negative examples is 1:4. The number of positive examples per slot and year ranges from 0 (org:member of, 2014) to 581 (per:title, 2013), the number of negative examples from 5 (org:website, 2014) to 1886 (per:title, 2013).\nIn contrast to other relation classification benchmarks, this dataset is not based on a knowledge base (such as Freebase) and unrelated text (such as web documents) but directly on the SF assessments. Thus, it includes exactly the SF relations and addresses the challenges of the end-to-end task: noisy data, possibly incomplete extractions of sentences and data from different domains.\nWe use the data from 2012/2013 as development and the data from 2014 as evaluation set."}, {"heading": "5.3 Experiments", "text": "We evaluate the models described in Section 4, select the best model of each method and combine them.\nExperiments with patterns. First, we compare the performance of PATdist and PATuschema on our dataset. We evaluate the pattern matchers on all slots presented in Table 1 and calculate their average F1 scores on dev. PATdist achieves a score of .35, PATuschema of .33. Since it performs better, we use PATdist in the following experiments.\nExperiments with SVMs. Second, we train and evaluate SVMbow and SVMskip. Average F1 of SVMskip and SVMbow are .62 and .59, respectively. Thus, we use SVMskip. We expected that SVMskip beats SVMbow due to its richer feature set, but SVMbow performs surprisingly well.\nExperiments with CNNs. Finally, we compare the performance of CNNpiece, CNNpieceExt and CNNcontext. While the baseline network CNNpiece (Zeng et al., 2015) achieves F1 of .52 on dev, CNNpieceExt has an F1 score of .55 and CNNcontext an F1 of .60. The difference of CNNpiece and CNNpieceExt is due to the additional hidden layer and k-max pooling. The considerable difference in performance of CNNpieceExt and CNNcontext shows that splitting the context for convolution has a positive effect on the performance of the network.\nOverall results. Table 1 shows the slot wise results of the best patterns (PATdist), SVMs (SVMskip) and CNNs (CNNcontext). Furthermore, it provides a comparison with two baseline models: Mintz++ and MIMLRE. SVM and CNN clearly outperform these baselines. They also outperform PAT for almost all slots. The difference between dev and eval results varies a lot among the slots. We suspect that this is a result of genre differences in the data and analyze this in Section 6.4.\nSlot-wise results of the other models (PATuschema, SVMbow, CNNpiece, CNNpieceExt) can be found in the supplementary material.\nComparing PAT, SVM and CNN5, different patterns emerge for different slots. Each is best on a subset of the slots (see bold numbers). This indicates that relation classification for slot filling is not a uniform problem: each slot has special properties and the three approaches are good at modeling a different subset of these properties. Given the big differences, we expect to gain performance by combining the three approaches. Indeed, CMB (PATdist + SVMskip + CNNcontext), the combination of the three best performing models, obtains the best results for most slots (in bold).\nSection 6.3 shows that the performance on our\n5In prior experiments, we also compared with recurrent neural network models: The RNNs achieved over-all performances comparable to the CNNs but required much more training time and parameter tuning. Therefore, we decided to focus on CNNs in this paper.\ndataset is highly correlated with SF end-to-end performance. Thus, our results indicate that a combination of different models ist the most promising approach to getting good performance on slot filling."}, {"heading": "6 Analysis", "text": ""}, {"heading": "6.1 Contribution of each model", "text": "To see how much each model contributes to CMB, we count how often each weight between 0.0 and 1.0 is selected for the linear interpolation. The results are plotted as a histogram (Figure 2). A weight\nof 0.0 means that the corresponding model does not contribute to CMB. We see that all three models contribute to CMB for most of the slots. The CNN, for instance, is included in the combination for 14 of 24 slots and, thus, improved performance on them."}, {"heading": "6.2 Comparison of CNN to traditional models", "text": "Our motivation for using a CNN is that convolution and max pooling can recognize important n-grams independent of their position in the sentence. To investigate this effect, we select for each CNN the top five kernels whose activations are the most correlated with the final score of the positive class. Then we calculate which n-grams are selected by these kernels in the max pooling step. This corresponds to those n-grams which are recognized by the kernel to be the most informative for the given slot. Figure 3 shows the result for an example sentence expressing the slot relation org:parents. The height of a bar is the number of times that the 3-gram around the corresponding word was selected by k-max pooling; e.g., the bar above \u201cnewest\u201d corresponds to the tri-\ngram \u201cits newest subsidiary\u201d. The figure shows that the convolutional filters are able to learn phrases that trigger a relation, e.g., \u201cits subsidiary\u201d. In contrast to patterns, they do not rely on exact matches. The first reason is embeddings. They generalize similar words and phrases by assigning similar word vectors to them. For PAT and BOW, this type of generalization is more difficult. The second type of generalization that the CNN learns concerns insertions. The recognition of important phrases in convolution is robust against insertions. An example is \u201cnewest\u201d in Figure 3, a word that is not important for the slot.\nA direct comparison of results with PAT shows that the CNN has better eval scores for about 73% of the slots (see Table 1). Our reasoning above can explain this. Compared to the SVM, the CNN generalizes better to unseen data in only half of all cases. The fact that this does not happen in more cases shows the power of the skip n-gram features of the SVM: they also provide a kind of generalization against insertions. The SVM might also need less data to train than the CNN. Nevertheless, the final scores show that the CNN performs almost as well as the SVM in average (.60 vs .62 on dev, .46 vs .48 on eval) and contributes to a better combination score."}, {"heading": "6.3 Correlation with end-to-end results", "text": "In this section, we show that using the dataset we provide with this paper allows tuning classification models for the end-to-end SF task. For each model and each possible combination of models, we calculate average results on our evaluation set as well as final F1 scores when running the whole slot filling pipeline with our in-house system. The best results of our slot filling system are an F1 of .290 on the\n2013 queries and of .250 on the 2014 queries. We calculate Pearson\u2019s correlation coefficient to assess correlation of relation classification and end-to-end performances for the n different system configurations (i.e. model combinations). The correlation of the results on our eval dataset with the SF results on 2013 queries is .89, the correlation with the SF results on 2014 queries is .82. This confirms that good results on the dataset we propose lead to good results on the slot filling end-to-end task."}, {"heading": "6.4 Effect of genre and time", "text": "The TAC source corpus consists of about 1M news documents, 1M web documents and 100K documents from discussion forums (TAC, 2014). The distribution of these different genres in the extracted assessment data is as follows:\n2012/3 2014 news 87.5% 73.4% web + forums 12.5% 26.6%\nThe proportion of non-news more than doubled from 12.5% to 26.6%. Thus, when using 2012/2013 as the development and 2014 as the test set, we are faced with a domain adaptation problem.\nIn this section, we show the effect of domain differences on our models in more detail. For our genre analysis, we re-train our models on genre specific training sets WEB and NEWS\u22826 and show withingenre as well as cross-genre evaluations.\n6To avoid performance differences due to different training set sizes, we reduced the news training set to the same size as the web training set. We refer to this subset as NEWS\u2282.\nCross-genre evaluation. Table 2 shows results of testing models trained on genre-specific data: on data of the same genre and on data of the other genre. We present results only for a subset of relations in this paper, however, the numbers for the other slots follow the same trends.\nModels trained on news (left part) show clearly higher performance in the within-genre evaluation than cross-genre. For models trained on web (right part), this is different. We suspect that the reason is that web data are much noiser and thus less predictable, even for models trained on web. For all evaluations, the differences among dev and eval are quite large. Especially for slot filling on web (bottom part of Table 2), the results on dev do not seem much related to the results on eval. This domain effect increases the difficulties of training robust relation classification models for slot filling. It can also explain why optimizing models for unseen data (with unknown genre distributions) as in Table 1 is challenging. Since slot filling by itself is a challenging task, even in the absence of domain differences, we will distribute two splits: a split by year and a split by genre. For training and tuning models for the slot filling research challenge, the year split can be used to cover the challenge of mixing different genres. For experiments on domain adaptation or genre-specific effects, our genre split can be used."}, {"heading": "7 Conclusion", "text": "In this paper, we presented different approaches to slot filling relation classification: patterns, support vector machines and convolutional neural networks. We investigated their complementary strengths and weaknesses and showed that their combination can better deal with a diverse set of problems that slot filling poses than each of the approaches individually. We proposed a contextwise CNN which outperforms the recent state-of-the-art piecewise CNN. Furthermore, we analyzed the effect of genre on slot filling and showed that it needs to be carefully examined in research on slot filling. Finally, we provided a benchmark for slot filling relation classification that will facilitate direct comparisons of approaches in the future."}, {"heading": "8 Additional Resources", "text": "We publish the scripts that we developed to extract the annotated evaluation data and our splits by genre and by year as well as the dev/eval splits. Furthermore, we provide a detailed distribution of positive and negative examples in the evaluation data."}, {"heading": "Acknowledgments", "text": "Heike Adel is a recipient of the Google European Doctoral Fellowship in Natural Language Processing and this research is supported by this fellowship.\nWe would like to thank Gabor Angeli for his eager support with the Mintz++ and MIMLRE models."}], "references": [{"title": "Combining distant and partial supervision for relation extraction", "author": ["Angeli et al.2014b] Gabor Angeli", "Julie Tibshirani", "Jean Y. Wu", "Christopher D. Manning"], "venue": null, "citeRegEx": "Angeli et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Angeli et al\\.", "year": 2014}, {"title": "Freebase: a collaboratively created graph database for structuring human knowledge", "author": ["Colin Evans", "Praveen Paritosh", "Tim Sturge", "Jamie Taylor"], "venue": "In ACM SIGMOD", "citeRegEx": "Bollacker et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Bollacker et al\\.", "year": 2008}, {"title": "Natural language processing (almost) from scratch. JMLR", "author": ["Jason Weston", "L\u00e9on Bottou", "Michael Karlen", "Koray Kavukcuoglu", "Pavel Kuksa"], "venue": null, "citeRegEx": "Collobert et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Collobert et al\\.", "year": 2011}, {"title": "Classifying relations by ranking with convolutional neural networks", "author": ["Bing Xiang", "Bowen Zhou"], "venue": null, "citeRegEx": "Santos et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Santos et al\\.", "year": 2015}, {"title": "LIBLINEAR: A library for large linear classification", "author": ["Fan et al.2008] Rong-En Fan", "Kai-Wei Chang", "Cho-Jui Hsieh", "Xiang-Rui Wang", "Chih-Jen Lin"], "venue": null, "citeRegEx": "Fan et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Fan et al\\.", "year": 2008}, {"title": "Mixture-model adaptation for SMT", "author": ["Foster", "Kuhn2007] George Foster", "Roland Kuhn"], "venue": "In Workshop on SMT", "citeRegEx": "Foster et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Foster et al\\.", "year": 2007}, {"title": "Domain adaptation for largescale sentiment classification: A deep learning approach", "author": ["Glorot et al.2011] Xavier Glorot", "Antoine Bordes", "Yoshua Bengio"], "venue": null, "citeRegEx": "Glorot et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Glorot et al\\.", "year": 2011}, {"title": "The TALP participation at TAC-KBP", "author": ["E. Sapena", "M. Vila", "M.A. Mart\u0131"], "venue": "In TAC", "citeRegEx": "Sapena et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Sapena et al\\.", "year": 2012}, {"title": "Semeval-2010 task 8: Multi-way classification of semantic relations", "author": ["Su Nam Kim", "Zornitsa Kozareva", "Preslav Nakov", "Diarmuid \u00d3 S\u00e9aghdha", "Sebastian Pad\u00f3", "Marco Pennacchiotti", "Lorenza Romano", "Stan Szpakowicz"], "venue": null, "citeRegEx": "Hendrickx et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Hendrickx et al\\.", "year": 2010}, {"title": "A convolutional neural network for modelling sentences", "author": ["Edward Grefenstette", "Phil Blunsom"], "venue": null, "citeRegEx": "Kalchbrenner et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Kalchbrenner et al\\.", "year": 2014}, {"title": "Sweat2012: Pattern based English slot filling system for knowledge base population at TAC", "author": ["Liu", "Zhao2012] Fang Liu", "Jun Zhao"], "venue": "In TAC", "citeRegEx": "Liu et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Liu et al\\.", "year": 2012}, {"title": "Slot-filling by substring extraction at tac kbp 2012 (team papelo)", "author": ["Bing Bai", "Kazi Saidul Hasan"], "venue": "In TAC", "citeRegEx": "Malon et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Malon et al\\.", "year": 2012}, {"title": "The Stanford CoreNLP natural language processing toolkit", "author": ["Mihai Surdeanu", "John Bauer", "Jenny Finkel", "Steven J. Bethard", "David McClosky"], "venue": null, "citeRegEx": "Manning et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Manning et al\\.", "year": 2014}, {"title": "Efficient estimation of word representations in vector space. In Workshop at ICLR", "author": ["Kai Chen", "Greg Corrado", "Jeffrey Dean"], "venue": null, "citeRegEx": "Mikolov et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Mikolov et al\\.", "year": 2013}, {"title": "New york university 2012 system for KBP slot filling", "author": ["Min et al.2012] Bonan Min", "Xiang Li", "Ralph Grishman", "Ang Sun"], "venue": "In TAC", "citeRegEx": "Min et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Min et al\\.", "year": 2012}, {"title": "Distant supervision for relation extraction without labeled data", "author": ["Mintz et al.2009] Mike Mintz", "Steven Bills", "Rion Snow", "Dan Jurafsky"], "venue": "ACL/IJCNLP", "citeRegEx": "Mintz et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Mintz et al\\.", "year": 2009}, {"title": "Overview of the 2012 shared task on parsing the web", "author": ["Petrov", "McDonald2012] Slav Petrov", "Ryan McDonald"], "venue": "SANCL", "citeRegEx": "Petrov et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Petrov et al\\.", "year": 2012}, {"title": "Analysing recall loss in named entity slot filling", "author": ["Pink et al.2014] Glen Pink", "Joel Nothman", "James R Curran"], "venue": null, "citeRegEx": "Pink et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Pink et al\\.", "year": 2014}, {"title": "GDUFS at slot filling TAC-KBP", "author": ["Qiu et al.2012] Xin Ying Qiu", "Xiaoting Li", "Weijian Mo", "Manli Zheng", "Zhuhe Zheng"], "venue": "In TAC", "citeRegEx": "Qiu et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Qiu et al\\.", "year": 2012}, {"title": "Relation extraction with matrix factorization and universal schemas", "author": ["Limin Yao", "Andrew McCallum", "Benjamin M Marlin"], "venue": "In HLT-NAACL", "citeRegEx": "Riedel et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Riedel et al\\.", "year": 2013}, {"title": "Effective slot filling based on shallow distant supervision methods", "author": ["Roth et al.2013] Benjamin Roth", "Tassilo Barth", "Michael Wiegand", "Mittul Singh", "Dietrich Klakow"], "venue": "In TAC", "citeRegEx": "Roth et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Roth et al\\.", "year": 2013}, {"title": "Universal schema for slotfilling, cold-start kbp and event argument extraction: UMAss IESL at TAC KBP", "author": ["Roth et al.2014] Benjamin Roth", "Emma Strubell", "John Sullivan", "Lakshmi Vikraman", "Kate Silverstein", "Andrew McCallum"], "venue": "In TAC", "citeRegEx": "Roth et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Roth et al\\.", "year": 2014}, {"title": "Overview of the English slot filling track at the TAC 2014 knowledge base population evaluation", "author": ["Surdeanu", "Ji2014] Mihai Surdeanu", "Heng Ji"], "venue": "In TAC", "citeRegEx": "Surdeanu et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Surdeanu et al\\.", "year": 2014}, {"title": "Multi-instance multi-label learning for relation extraction", "author": ["Julie Tibshirani", "Ramesh Nallapati", "Christopher D Manning"], "venue": "In Joint Conference on EMNLP and CoNLL", "citeRegEx": "Surdeanu et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Surdeanu et al\\.", "year": 2012}, {"title": "Overview of the tac2013 knowledge base population evaluation: English slot filling and temporal slot filling", "author": ["Mihai Surdeanu"], "venue": "In TAC", "citeRegEx": "Surdeanu.,? \\Q2013\\E", "shortCiteRegEx": "Surdeanu.", "year": 2013}, {"title": "Stacked ensembles of information extractors for knowledge-base population", "author": ["Viswanathan", "Nazneen Fatema Rajani", "Yinon Bentor", "Raymond Mooney."], "venue": "ACL.", "citeRegEx": "Viswanathan et al\\.,? 2015", "shortCiteRegEx": "Viswanathan et al\\.", "year": 2015}, {"title": "Relation classification via convolutional deep neural network. In COLING", "author": ["Zeng et al.2014] Daojian Zeng", "Kang Liu", "Siwei Lai", "Guangyou Zhou", "Jun Zhao"], "venue": null, "citeRegEx": "Zeng et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Zeng et al\\.", "year": 2014}, {"title": "Distant supervision for relation extraction via piecewise convolutional neural networks", "author": ["Zeng et al.2015] Daojian Zeng", "Kang Liu", "Yubo Chen", "Jun Zhao"], "venue": null, "citeRegEx": "Zeng et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Zeng et al\\.", "year": 2015}], "referenceMentions": [{"referenceID": 13, "context": "Furthermore, they make use of word embeddings that directly reflect word similarity (Mikolov et al., 2013).", "startOffset": 84, "endOffset": 106}, {"referenceID": 27, "context": "In this work, we train different variants of CNNs: As a baseline, we re-implement the recently developed piecewise CNN (Zeng et al., 2015).", "startOffset": 119, "endOffset": 138}, {"referenceID": 18, "context": "Riedel et al. (2013), Hendrickx et al.", "startOffset": 0, "endOffset": 21}, {"referenceID": 8, "context": "(2013), Hendrickx et al. (2010)) since data and relations can be quite different.", "startOffset": 8, "endOffset": 32}, {"referenceID": 17, "context": "The official SF Shared Task evaluations only assess whole systems (with potential subsequent faults in their pipelines (Pink et al., 2014)).", "startOffset": 119, "endOffset": 138}, {"referenceID": 24, "context": "The participants of the SF Shared Task (Surdeanu, 2013) are provided with a large text corpus.", "startOffset": 39, "endOffset": 55}, {"referenceID": 15, "context": "To train their models, most groups use distant supervision (Mintz et al., 2009).", "startOffset": 59, "endOffset": 79}, {"referenceID": 18, "context": "The methods for extracting and scoring candidates range from pattern based approaches (Gonz\u00e0lez et al., 2012; Liu and Zhao, 2012; Li et al., 2012; Qiu et al., 2012; Roth et al., 2014) over rule based systems (Varma et al.", "startOffset": 86, "endOffset": 183}, {"referenceID": 21, "context": "The methods for extracting and scoring candidates range from pattern based approaches (Gonz\u00e0lez et al., 2012; Liu and Zhao, 2012; Li et al., 2012; Qiu et al., 2012; Roth et al., 2014) over rule based systems (Varma et al.", "startOffset": 86, "endOffset": 183}, {"referenceID": 11, "context": ", 2012) to classifiers (Malon et al., 2012; Roth et al., 2013).", "startOffset": 23, "endOffset": 62}, {"referenceID": 20, "context": ", 2012) to classifiers (Malon et al., 2012; Roth et al., 2013).", "startOffset": 23, "endOffset": 62}, {"referenceID": 14, "context": "Pink et al. (2014) analyzed sources of recall losses in these pipelines.", "startOffset": 0, "endOffset": 19}, {"referenceID": 20, "context": "dates (Roth et al., 2013); their results suggest that n-gram based features are sufficient to build reliable classifiers for the relation classification module.", "startOffset": 6, "endOffset": 25}, {"referenceID": 19, "context": "dates (Roth et al., 2013); their results suggest that n-gram based features are sufficient to build reliable classifiers for the relation classification module. They also show that SVMs outperform patterns. CNNs and baseline models for relation classification. Zeng et al. (2014) and Dos Santos et al.", "startOffset": 7, "endOffset": 280}, {"referenceID": 3, "context": "(2014) and Dos Santos et al. (2015) applied CNNs to the relation classification SemEval Shared Task data from 2010 and showed", "startOffset": 15, "endOffset": 36}, {"referenceID": 11, "context": "Malon et al. (2012) described a CNN for slot filling that is based on the output of a parser.", "startOffset": 0, "endOffset": 20}, {"referenceID": 15, "context": "Mintz++ (Mintz et al., 2009; Surdeanu et al., 2012) and MIMLRE (Surdeanu et al.", "startOffset": 8, "endOffset": 51}, {"referenceID": 23, "context": "Mintz++ (Mintz et al., 2009; Surdeanu et al., 2012) and MIMLRE (Surdeanu et al.", "startOffset": 8, "endOffset": 51}, {"referenceID": 23, "context": ", 2012) and MIMLRE (Surdeanu et al., 2012).", "startOffset": 19, "endOffset": 42}, {"referenceID": 15, "context": "Mintz++ (Mintz et al., 2009; Surdeanu et al., 2012) and MIMLRE (Surdeanu et al., 2012). Mintz++ is a model based on the Mintz features (lexical and syntactic features for relation extraction). It was developed by Surdeanu et al. (2012) and used as a base-", "startOffset": 9, "endOffset": 236}, {"referenceID": 27, "context": "Another baseline model which we use in this work is a piecewise convolutional neural network (Zeng et al., 2015).", "startOffset": 93, "endOffset": 112}, {"referenceID": 6, "context": "ied (Glorot et al., 2011; Foster and Kuhn, 2007).", "startOffset": 4, "endOffset": 48}, {"referenceID": 20, "context": "They have been used in the SF challenge by the topranked system in the 2013 Shared Task (Roth et al., 2013).", "startOffset": 88, "endOffset": 107}, {"referenceID": 19, "context": "Universal schema relations are extracted based on matrix factorization (Riedel et al., 2013).", "startOffset": 71, "endOffset": 92}, {"referenceID": 20, "context": "work, we apply the universal schema patterns extracted for slot filling by Roth et al. (2014). Support vector machines (SVMs).", "startOffset": 75, "endOffset": 94}, {"referenceID": 20, "context": "work, we apply the universal schema patterns extracted for slot filling by Roth et al. (2014). Support vector machines (SVMs). Our second approach is support vector machines. We evaluate two different feature sets: bag-of-word features (SVMbow) and skip n-gram features (SVMskip). Based on the results of Roth et al. (2013), we", "startOffset": 75, "endOffset": 324}, {"referenceID": 20, "context": "(Roth et al., 2013)).", "startOffset": 0, "endOffset": 19}, {"referenceID": 4, "context": "We train one linear SVM (Fan et al., 2008) for each relation and feature set and tune parameter C on dev.", "startOffset": 24, "endOffset": 42}, {"referenceID": 2, "context": "CNNs are increasingly applied in NLP (Collobert et al., 2011; Kalchbrenner et al., 2014).", "startOffset": 37, "endOffset": 88}, {"referenceID": 9, "context": "CNNs are increasingly applied in NLP (Collobert et al., 2011; Kalchbrenner et al., 2014).", "startOffset": 37, "endOffset": 88}, {"referenceID": 2, "context": "Max pooling (Collobert et al., 2011) detects the globally most relevant features obtained by local convolution.", "startOffset": 12, "endOffset": 36}, {"referenceID": 26, "context": "Our baseline CNN is the model developed by Zeng et al. (2015). It represents the", "startOffset": 43, "endOffset": 62}, {"referenceID": 9, "context": "1 The results of convolution are pooled using k-max pooling (Kalchbrenner et al., 2014): only the k = 3 maximum values of each filter application are kept.", "startOffset": 60, "endOffset": 87}, {"referenceID": 25, "context": "For a comparison of different combination possibilities, see, for example, (Viswanathan et al., 2015).", "startOffset": 75, "endOffset": 101}, {"referenceID": 1, "context": "We created a set of (subject, relation, object) tuples by querying Freebase (Bollacker et al., 2008) for relations that correspond to one of the", "startOffset": 76, "endOffset": 100}, {"referenceID": 12, "context": "for sentences that (i) contain a mention of a name occurring in a tuple, (ii) do not contain the correct filler, (iii) contain a mention different from the correct filler, but with the same named entity type (based on CoreNLP NER (Manning et al., 2014)).", "startOffset": 230, "endOffset": 252}, {"referenceID": 0, "context": "In order to reduce incorrect labels, we applied a self-training procedure: We trained SVMs on the SF dataset created by Angeli et al. (2014b). With the resulting SVMs, we predicted labels for our training set.", "startOffset": 120, "endOffset": 142}, {"referenceID": 14, "context": "label did not match the distant supervised label, we deleted the corresponding training example (Min et al., 2012).", "startOffset": 96, "endOffset": 114}, {"referenceID": 0, "context": "org/clueweb12 We do not use the SF dataset directly because (i) it provides few examples per slot (min: 1, max: 4960) and (ii) it consists of examples for which the classifiers of Angeli et al. (2014b) were indecisive, i.", "startOffset": 180, "endOffset": 202}, {"referenceID": 27, "context": "While the baseline network CNNpiece (Zeng et al., 2015) achieves F1 of .", "startOffset": 36, "endOffset": 55}], "year": 2017, "abstractText": "We address relation classification in the context of slot filling, the task of finding and evaluating fillers like \u201cSteve Jobs\u201d for the slot X in \u201cX founded Apple\u201d. We propose a convolutional neural network which splits the input sentence into three parts according to the relation arguments and compare it to state-ofthe-art and traditional approaches of relation classification. Finally, we combine different methods and show that the combination is better than individual approaches. We also analyze the effect of genre differences on performance.", "creator": "LaTeX with hyperref package"}}}