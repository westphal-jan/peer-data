{"id": "1603.05643", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "17-Mar-2016", "title": "Variance Reduction for Faster Non-Convex Optimization", "abstract": "We consider the fundamental problem in non-convex optimization of efficiently reaching a stationary point. In contrast to the convex case, in the long history of this basic problem, the only known theoretical results on first-order non-convex optimization remain to be full gradient descent that converges in $O(1/\\varepsilon)$ iterations for smooth objectives, and stochastic gradient descent that converges in $O(1/\\varepsilon^2)$ iterations for objectives that are sum of smooth functions.\n\n\n\nIn the short version of our experiment we found that when we first found that if we can create one or more parallel functions for each dimension of our data in two dimensions, they can both achieve a smooth goal as follows. For a more detailed view of this important part of our experiment, see:\nIn addition to a simple model of the problem, we looked at the effect of the gradient descent on a non-convex task that would be applied to all of our solutions to that problem.\nSince the gradient descent is an optimization solution, we now consider the problem of estimating an optimal value of the gradient descent function. We can now compute the problem of estimating the optimal value for the gradient descent function. By performing a linear regression, we can estimate that the best optimal value is computed using the gradient descent function with the shortest possible time.\nTo determine how much the gradient descent function does, we used the Gaussian function. We will explain how the Gaussian function can be calculated using a Gaussian function.\nWe will explain the Gaussian function. The Gaussian function, for all its other important details, is a Gaussian function. As usual, we will assume that the first parameter that we define is the Gaussian function (this parameter is the constant of our Gaussian function). We define the Gaussian function (this parameter is the constant of the function).\nThe first parameter is the constant of our Gaussian function. This parameter is the constant of our Gaussian function. The Gaussian function is a constant of our Gaussian function. This parameter is the constant of our Gaussian function.\nWe will explain the Gaussian function.\nWe will explain how the Gaussian function is used to calculate the optimal value of the gradient descent function. Since all the parameters for this function are known, it is possible to compute the optimal value for the gradient descent function. For more detailed details, see:\nIn addition to a simple model of", "histories": [["v1", "Thu, 17 Mar 2016 19:55:12 GMT  (4239kb,D)", "https://arxiv.org/abs/1603.05643v1", null], ["v2", "Thu, 25 Aug 2016 02:34:00 GMT  (4254kb,D)", "http://arxiv.org/abs/1603.05643v2", "polished writing"]], "reviews": [], "SUBJECTS": "math.OC cs.DS cs.LG cs.NE stat.ML", "authors": ["zeyuan allen zhu", "elad hazan"], "accepted": true, "id": "1603.05643"}, "pdf": {"name": "1603.05643.pdf", "metadata": {"source": "CRF", "title": "Variance Reduction for Faster Non-Convex Optimization", "authors": ["Zeyuan Allen-Zhu", "Elad Hazan"], "emails": ["zeyuan@csail.mit.edu", "ehazan@cs.princeton.edu"], "sections": [{"heading": null, "text": "We provide the first improvement in this line of research. Our result is based on the variance reduction trick recently introduced to convex optimization, as well as a brand new analysis of variance reduction that is suitable for non-convex optimization. For objectives that are sum of smooth functions, our first-order minibatch stochastic method converges with an O(1/\u03b5) rate, and is faster than full gradient descent by \u2126(n1/3).\nWe demonstrate the effectiveness of our methods on empirical risk minimizations with nonconvex loss functions and training neural nets."}, {"heading": "1 Introduction", "text": "Numerous machine learning problems are naturally formulated as non-convex optimization problems. Examples include inference in graphical models, unsupervised learning models such as topic models, dictionary learning, and perhaps most notably, training of deep neural networks. Indeed, non-convex optimization for machine learning is one of the fields\u2019 main research frontiers.\nSince global minimization of non-convex functions is NP-hard, various alternative approaches are applied. For some models, probabilistic and other assumptions on the input can be used to give specially designed polynomial-time algorithms [5, 6, 15].\nHowever, the multitude and diversity of machine learning applications require a robust, generic optimization method that can be applied as a tool rather than reinvented per each specific model. One approach is the design of global non-convex heuristics such as simulated annealing or bayesian optimization. Although believed to fail in the worst case due to known complexity results, such heuristics many times perform well in practice for certain problems.\nAnother approach, which is based on more solid theoretical foundation and is gaining in popularity, is to drop the \u201cglobal optimality\u201d requirement and attempt to reach more modest solution concepts. The most popular of these is the use of iterative optimization methods to reach a stationary point. The use of stochastic first-order methods is the primary focus of this approach, which has become the most common method for training deep neural nets.\n\u2217First circulated on this date, and first appeared on arXiv on March 17, 2016.\nar X\niv :1\n60 3.\n05 64\n3v 2\n[ m\nat h.\nO C\n] 2\n5 A\nug 2\nFormally, in this paper we consider the unconstrained minimization problem\nmin x\u2208Rd\n{ f(x) def = 1\nn\nn\u2211\ni=1\nfi(x) } , (1.1)\nwhere each fi(x) is differentiable, possibly non-convex, and has L-Lipschitz continuous gradient (a.k.a. L-smooth) for some parameter L > 0.1 Many machine learning/imaging processing problems fall into Problem (1.1), including training neural nets, ERM (empirical risk minimization) with non-convex losses, and many others.\nFollowing the classical benchmark for non-convex optimization (see for instance [13]), we focus on algorithms that can efficiently find an approximate stationary point x satisfying \u2016\u2207f(x)\u20162 \u2264 \u03b5.\nUnlike convex optimization, a point with small gradient may only be close to a saddle point or a local minimum, rather than the global minimum. Therefore, such an algorithm is usually combined with saddle-point or local-minima escaping schemes, such as genetic algorithms or simulated annealing. More recently, Ge et al. [12] also demonstrated that a simple noise-addition scheme is sufficient for stochastic gradient descent to escape from saddle points.\nHowever, for the general problem (1.1) where smoothness is the only assumption and finding approximate stationary point is the simple goal, the only known theoretical convergence results remain to be that for gradient descent (GD) and stochastic gradient descent (SGD).\n\u2022 Given a starting point x0, GD applies an update x\u2032 \u2190 x \u2212 1L\u2207f(x) with a fixed step length 1/L per iteration. In order to produce an output x that is an \u03b5-approximate stationary point,\nGD needs T = O (L(f(x0)\u2212f(x\u2217))\n\u03b5\n) iterations where x\u2217 is the global minimizer of f(\u00b7). This is\na folklore result in optimization and included for instance in [13].\n\u2022 SGD applies an update x\u2032 \u2190 x\u2212 \u03b7\u2207fi(x) per iteration, where i chosen uniformly at random from [n] def = {1, 2, . . . , n}. If \u03b7 is properly tuned, one can obtain an \u03b5-approximate stationary\npoint in T = O ((\nL \u03b5 +\nL\u03c32\n\u03b52\n) \u00b7(f(x0)\u2212f(x\u2217)) ) iterations, where \u03c3 is the variance of the stochastic\ngradient. This result is perhaps first formalized by Ghadimi and Lan [13].\nSince computing the full gradient \u2207f(\u00b7) is usually n times slower than that of \u2207fi(x), each iteration of SGD is usually n times faster than that of GD, but the total number of iterations for SGD is very poor.\nBefore our work, it is an open question to design a first-order method that is faster than both GD and SGD."}, {"heading": "1.1 Our Result", "text": "We prove that variance reduction techniques, based on the SVRG method [16], produce an \u03b5stationary point in only O (n2/3L(f(x0)\u2212f(x\u2217))\n\u03b5\n) iterations. Since each iteration of SVRG is as fast as\nSGD and n times faster than that of GD, SVRG is guaranteed to be at least \u2126(n1/3) times faster than GD. Among first-order methods, this is the first time the performance of GD is outperformed in theory for problem (1.1) without any additional assumption, and also the first time that stochasticgradient based methods are shown to have a non-trivial2 1/\u03b5 convergence rate independent of the variance \u03c32.\n1Even if each fi(x) is not smooth but only Lipschitz continuous, standard smoothing techniques such as Chapter 2.3 of [14] usually turn each fi(x) into a smooth function without sacrificing too much accuracy.\n2Note however, designing a stochastic-gradient method with a trivial 1/\u03b5 rate is obvious. For instance, it is\nstraightforward to design such a method that converges in O (nL(f(x0)\u2212f(x\u2217))\n\u03b5\n) iterations. However, this is never\nfaster than GD.\nOur proposed algorithm is very analogous to SVRG of [16]. Recall that SVRG has an outer loop of epochs, where at the beginning of each epoch, SVRG defines a snapshot vector x\u0303 to be the average vector of the previous epoch,3 and computes its full gradient \u2207f(x\u0303). Each epoch of SVRG consists of m inner iterations, where the choice of m usually depends on the objective\u2019s strong convexity. In each inner iteration inside an epoch, SVRG picks a random i \u2208 [n], defines the gradient estimator\n\u2207\u0303k def= 1 n\nn\u2211\nj=1\n\u2207fj(x\u0303) +\u2207fi(xk)\u2212\u2207fi(x\u0303) , (1.2)\nand performs an update x\u2032 \u2190 x \u2212 \u03b7\u2207\u0303k for some fixed step length \u03b7 > 0 across all iterations and epochs.\nIn order to prove our theoretical result in this paper, we make the following changes to SVRG. First, we set the number of inner iterations m as a constant factor times n. Second, we pick the snapshot point to be a non-uniform average of the last m2/3 elements of the previous epoch. Finally, we prove that the average norm \u2016\u2207f(xk)\u20162 of the encountered vectors xk across all iterations is small, so it suffices to output xk for a random k.\nOur Technique. To prove our result, we need different techniques from all known results on variance reduction. The key idea used by previous authors is to show that the variance of the gradient estimator \u2207\u0303k is upper bounded by either O(f(xk)\u2212f(x\u2217)) or O(\u2016xk\u2212x\u2217\u20162), and therefore it converges to zero for convex functions. This analysis fails to apply in the non-convex setting because gradient-based methods do not converge to the global minimum.\nWe observe in this paper that the variance is upper bounded by O(\u2016xk \u2212 x\u0303\u20162), the squared distance between the current point and the most recent snapshot. By dividing an epoch into m1/3 subepochs of length m2/3, and performing a mirror-descent analysis for each subepoch, we further show that this squared distance is related to the objective decrease f(x\u0303)\u2212f(xk). This would suffice for proving our theorem: whenever this squared distance is small the objective is decreased by a lot due to the small variance, or otherwise if this squared distance is large we still experience a large objective decrease because it is related to f(x\u0303)\u2212 f(xk). Applications. There are many machine learning problems that fall into category (1.1). To mention just two:\n\u2022 Non-Convex Loss in ERM Empirical risk minimization (ERM) problems naturally fall into the category of (1.1) if the loss functions are non-convex. For instance, for binary classification problems, the sigmoid function \u2014or more broadly, any natural smoothed variant of the 0-1 loss function\u2014 is not only a more natural choice than artificial ones such as hinge loss, logistic loss, squared loss, but also generalize better in terms of testing accuracy especially when there are outliers [25].\nHowever, since sigmoid loss is non-convex, it was previously considered hard to train an ERM problem with it. Shalev-Shwartz, Shamir and Sridharan [25] showed that this minimization problem is still solvable in the improper learning sense, with the help from kernel methods and gradient descent. However, their theoretical convergence has a poor polynomial dependence on 1/\u03b5 and exponential dependence on the smoothness parameter of the loss function.\nOur result in this paper applies to ERM problems with non-convex loss. Suppose we are given n training examples {(a1, `1), . . . (an, `n)}, where each ai \u2208 Rd is the feature vector of example i and 3More precisely, SVRG provides two options, one defining x\u0303 to be the average vector of the previous epoch, and the other defining x\u0303 to be the last iterate of the previous epoch. While the authors only prove theoretical results for the \u201caverage\u201d definition, experimental results suggest that choosing the last iterate is better.\neach li \u2208 {\u22121,+1} is the binary label of example i. By setting \u03c6(t) def= 11+et to be the sigmoid loss function and setting fi(x) def = \u03c6(li\u3008ai, x\u3009) + \u03bb2\u2016x\u20162, problem (1.1) becomes `2 ERM with sigmoid loss. We shall demonstrate in our experiment section that, by using SVRG to train ERM with sigmoid loss, its running time is as good as using SVRG to train ERM with other convex loss functions, but the testing accuracy can be significantly better.\n\u2022 Neural Network Training neural nets can also be formalized into problem (1.1). For instance, as long as the activation function of each neural node is smooth, say the sigmoid function or a smooth version of the rectified linear unit (ReLU) function (for instance, the softplus alternative), we can define fi(x) to be the training loss with respect to the i-th data input. In this language, computing the stochastic gradient \u2207fi(x) for some random i \u2208 [n] corresponds to performing one forwardbackward prorogation on the neural net with respect to sample i. We shall demonstrate in our experiment that using SVRG to train neural nets can enjoy a much faster running time comparing to SGD or SVRG."}, {"heading": "1.2 Extensions", "text": "Mini-Batch. Our result in this paper trivially extends to the mini-batch setting: if in each iteration we select fi(\u00b7) for more than one random indices i, then we can accordingly define the gradient estimator and the result of this paper still holds. Note that the speed up that we obtain in this case comparing to gradient descent is O((n/b)1/3) where b is the mini-batch size. Therefore, the smaller b is the better sequential running time we expect to see (which is also observed in our experiments). Other Smoothness Assumptions. Our result generalizes to the setting when each fi(\u00b7) enjoys a different smoothness parameter. In this setting one needs to select a random index i \u2208 [n] with a non-uniform distribution in order to obtain a faster running time. Our result also generalizes to the upper-lower smoothness setting. Instead of requiring each fi(\u00b7) to be L-smooth, one can assume it is L-upper smooth and l-lower smooth, a notation introduced by [4]; in such a case, faster results can also be obtained using our same proof techniques.\nSum-of-Non-Convex Objectives. Our analogous proof also applies to the sum-of-non-convex setting which is the same Problem (1.1) except f(\u00b7) is guaranteed to be \u03c3-strongly convex. Our obtained running time is O\u0303(n+ \u221a nL/\u03c3) for SVRG. This is faster than the previous running time on SVRG which is O\u0303(n+L2/\u03c32), however, it is not faster than using SVRG+Catalyst which gives O\u0303(n + n3/4 \u221a L/ \u221a \u03c3), see discussion in [4]. We do not include the details about this proof because it does not outperform SVRG+Catalyst.\nOther Variance-Reduction Methods. Our proof generalizes to all variance-reduction methods. However, for brevity we demonstrate it only for the SVRG algorithm."}, {"heading": "1.3 Other Related Works", "text": "For convex objectives, finding stationary points (or equivalently the global minimum) for problem (1.1) has received lots of attentions across machine learning and optimization communities; many first-order methods [8, 16, 23, 26] as well as their accelerations [1, 3, 18, 27, 28] have been proposed in the past a few years. Even in the case when f(\u00b7) is convex but each fi(\u00b7) is non-convex, the problem (1.1) can be solved easily [4, 11, 24].\nAlgorithm 1 Simplified SVRG method in the non-convex setting Input: x\u03c6 a starting vector, S number of epochs, m number of iterations per epoch, \u03b7 step length. 1: x10 \u2190 x\u03c6 2: for s\u2190 1 to S do 3: \u00b5\u0303\u2190 \u2207f(xs0) 4: for k \u2190 0 to m\u2212 1 do 5: Pick i uniformly at random in {1, \u00b7 \u00b7 \u00b7 , n}. 6: \u2207\u0303 \u2190 \u2207fi(xsk)\u2212\u2207fi(xs0) + \u00b5\u0303 7: xsk+1 = x s k \u2212 \u03b7\u2207\u0303\n8: end for 9: xs+10 \u2190 xsm\n10: end for 11: return xsk for some random s \u2208 {1, 2, . . . , S} and random k \u2208 {1, 2, . . . ,m}.\nThe results of Li and Lin [17] and Ghadimi and Lan [13] unify the theory of non-convex and convex optimization in the following sense. They provide general first-order schemes such that, if the parameters are tuned properly, the schemes can converge (1) as fast as gradient descent in terms of finding an approximate stationary point; and (2) as fast as accelerated gradient descent [20] in terms of minimizing the objective if the function is convex. For the class of (1.1), their methods are only as slow as GD; in contrast, in this paper we prove theoretical convergence that is strictly faster than GD, which is both interesting and unknown.\nA few days after the first version of this paper appeared on arXiv, we became aware of another group of authors that have independently obtained essentially the same result [21, 22]. 4"}, {"heading": "2 Notations and Algorithm", "text": "A differentiable function f : Rn \u2192 R is L-smooth if for all pairs x, y \u2208 Rn it satisfies \u2016\u2207f(x) \u2212 \u2207f(y)\u2016 \u2264 L\u2016x\u2212 y\u2016. An equivalent definition says for for all x, y \u2208 Rn:\n\u2212 L 2 \u2016x\u2212 y\u20162 \u2264 f(x)\u2212 f(y)\u2212 \u3008\u2207f(y), x\u2212 y\u3009 \u2264 L 2 \u2016x\u2212 y\u20162 . (2.1)\nThe main body of this paper proves our result based on three false simplification assumptions 4.2, 4.3 and 4.4 for the sake of sketching the high-level intuitions and highlighting the differences between our proof and known results. Our formal convergence proof is quite technical and included in the full paper.\nIn this high-level proof, we consider Algorithm 1, a simplified version of our final SVRG method for the non-convex setting. Note that both the snapshot point and the starting iterate xs0 of the s-th epoch have been chosen as the last iterate of the previous epoch in Algorithm 1.\nRemark 2.1. In our final proof, we instead choose xs0 to be a weighted average of the last m 2/3 iterates from the previous epoch. See Algorithm 2 in the full paper. We demonstrate in Section 6 that this also a better choice in practice.\n4These results also address gradient dominated functions, for which our main theorem also applies as follows. A non-convex function f(\u00b7) is \u03c4 -gradient dominated if f(x)\u2212 f(x\u2217) \u2264 \u03c4\u2016\u2207f(x)\u20162 for every x. Since our main theorem implies one can obtain x satisfying \u2016\u2207f(x)\u20162 \u2264 1\n2\u03c4 (f(x0) \u2212 f(x\u2217)) using O\n( n + n2/3L\u03c4 ) stochastic gradients, by\nrepeating it log2(1/\u03b5) times, we obtain an \u03b5-minimizer of f(\u00b7) in O ( (n+ n2/3L\u03c4) log(1/\u03b5) ) stochastic gradients.\nThroughout this paper we denote by xsk the k-th iterate of the s-th epoch, by \u2207sk = \u2207f(xsk) the full gradient at this iterate, and by \u2207\u0303sk = \u2207f(xs0)+\u2207if(xsk)\u2212\u2207if(xs0) the gradient estimator which clearly satisfies Ei[\u2207\u0303sk] = \u2207sk. We denote by isk the random index i chosen at iteration k of epoch s. We also denote by (\u03c3sk)\n2 def= \u2016\u2207sk \u2212 \u2207\u0303sk\u20162 the variance of the gradient estimator \u2207\u0303sk. Under these notations, our simplified SVRG algorithm in Algorithm 1 simply performs update xsk+1 \u2190 xsk\u2212\u03b7\u2207\u0303sk for a fixed step length \u03b7 > 0 that shall be specified later.\nSince we focus mostly on analyzing a single epoch, when it is clear from the context, we drop the superscript s and denote by xk, ik, \u2207k, \u2207\u0303k, \u03c32k respectively for xsk, isk, \u2207sk, \u2207\u0303sk, (\u03c3sk)2. We also denote by H2k def = \u2016\u2207k\u201622, \u03c32i:j def = \u2211j k=i \u03c3 2 k and H2i:j def = \u2211j k=iH2k for notational simplicity."}, {"heading": "3 Two Useful Lemmas", "text": "We first observe two simple lemmas. The first one describes the expected objective decrease between two consecutive iterations. This is a standard step that is used in analyzing gradient descent for smooth functions, and we additionally take into account the variance of the gradient estimator.\nLemma 3.1 (gradient descent). If xk+1 = xk \u2212 \u03b7\u2207\u0303k for some gradient estimator \u2207\u0303k satisfying E[\u2207\u0303k] = \u2207k = \u2207f(xk), and if the step length \u03b7 \u2264 1L , we have\nf(xk)\u2212 E[f(xk+1)] \u2265 \u03b7\n2 \u22072k \u2212\n\u03b72L 2 E [ \u03c32k ] .\nProof. By the smoothness of the function, we have\nE[f(xk+1)] \u2264 f(xk) + E [ \u3008\u2207f(xk), xk+1 \u2212 xk\u3009 ] + E [L 2 \u2016xk+1 \u2212 xk\u20162 ]\n= f(xk)\u2212 \u03b7\u2016\u2207f(xk)\u20162 + \u03b72L 2 E [ \u2016\u2207\u0303k\u20162 ] = f(xk)\u2212 \u03b7\u2016\u2207f(xk)\u20162 + \u03b72L\n2 E [ \u2016\u2207f(xk)\u20162 + \u2016\u2207\u0303k \u2212\u2207f(xk)\u20162 ] .\nThis immediately yields Lemma 3.1 by using the assumption that \u03b7 \u2264 1L . The next lemma follows from the classical analysis of mirror descent methods.5 However, we make novel use of it on top of a non-convex but smooth function.\nLemma 3.2 (mirror descent). If xk+1 = xk \u2212 \u03b7\u2207\u0303k for some gradient estimator \u2207\u0303k satisfying E[\u2207\u0303k] = \u2207k = \u2207f(xk), then for every u \u2208 Rd it satisfies\nf(xk)\u2212 f(u) \u2264 \u03b7\n2\n( H2k + E[\u03c32k] ) + ( 1\n2\u03b7 + L 2\n) \u2016xk \u2212 u\u20162 \u2212 1 2\u03b7 E [ \u2016xk+1 \u2212 u\u20162 ] .\nProof. We first write the following inequality which follows from classical mirror-descent analysis.\n5Mirror descent is a terminology mostly used in optimization literature, see for instance the textbook [7]. In machine learning contexts, mirror-descent analysis is essentially identical to regret analysis. In our SVRG method, the descent step xsk+1 \u2190 xsk \u2212 \u03b7\u2207\u0303sk can be interpreted as a mirror descent step in the Euclidean space (see for instance [2]), and therefore mirror-descent analysis applies.\nFor every u \u2208 Rd, it satisfies\n\u3008\u2207k, xk \u2212 u\u3009 = E[\u3008\u2207\u0303k, xk \u2212 u\u3009] = E[\u3008\u2207\u0303k, xk \u2212 xk+1\u3009+ \u3008\u2207\u0303k, xk+1 \u2212 u\u3009] = E[\u3008\u2207\u0303k, xk \u2212 xk+1\u3009 \u2212 1\n2\u03b7 \u2016xk \u2212 xk+1\u20162 +\n1\n2\u03b7 \u2016xk \u2212 u\u20162 \u2212\n1\n2\u03b7 \u2016xk+1 \u2212 u\u20162]\n\u2264 E [\u03b7 2 \u2016\u2207\u0303k\u20162 + 1 2\u03b7 \u2016xk \u2212 u\u20162 \u2212 1 2\u03b7 \u2016xk+1 \u2212 u\u20162 ] . (3.1)\nAbove, \u3008\u2207\u0303k, xk+1 \u2212 u\u3009 = \u2212 12\u03b7\u2016xk \u2212 xk+1\u20162 + 12\u03b7\u2016xk \u2212 u\u20162 \u2212 12\u03b7\u2016xk+1 \u2212 u\u20162 is known as the threepoint equality of Bregman divergence (in the special case of Euclidean space). The only inequality is because we have 12\u2016a\u20162 + 12\u2016b\u20162 \u2265 \u3008a, b\u3009 for any two vectors a, b.\nClassically in convex optimization, one would lower bound the left hand side of (3.1) by f(xk)\u2212 f(u) using the convexity of f(\u00b7). We take a different path here because our objective f(\u00b7) is not convex. Instead, we use the lower smoothness property of function f which is the first inequality in (2.1) to deduce that \u3008\u2207k, xk \u2212 u\u3009 \u2265 f(xk)\u2212 f(u)\u2212 L2 \u2016xk \u2212 u\u20162. Combining this with inequality (3.1), and taking into account E[\u2016\u2207\u0303k\u20162] = \u2016\u2207k\u20162 + E[\u03c32k] by the definition of variance, we finish the proof of Lemma 3.2.\nOur main theorem is motivated by the linear-coupling framework [2]. In particular, we linearly couple the above gradient and mirror descent lemmas, together with a variance upper-bound lemma described in the next section."}, {"heading": "4 Upper Bounding the Variance", "text": "High-Level Ideas. The key idea behind all variance-reduction literatures (such as SVRG [16], SAGA [8], and SAG [23]) is to prove that the variance E[(\u03c3sk)2] decreases as s or k increases. However, the only known technique to achieve so is to upper bound E[(\u03c3sk)2] \u201cessentially\u201d by O ( f(xsk) \u2212 f(x\u2217) ) , the objective distance to the minimum. Perhaps the only exception is the work on sum-of-non-convex but strongly-convex objectives [4, 24], where the authors upper bound E[(\u03c3sk)2] by O ( \u2016xsk \u2212 x\u2217\u20162 ) , the squared vector distance to the minimum. Such techniques fail to apply in our non-convex setting, because gradient-descent based methods do not necessarily converge to the global minimum.\nWe take a different path in this paper. We upper bound E[(\u03c3sk)2] by O ( \u2016xsk\u2212xs0\u20162 ) , the squared vector distance between the current vector xsk and the first vector (i.e., the snapshot) x s 0 of the\ncurrent epoch s. This is certainly tighter than O ( \u2016xsk \u2212 x\u2217\u20162 ) from prior work.6 Moreover, the less we move away from the snapshot, the better upper bound we obtain on the variance. This is conceptually different from all existing literatures.\nFurthermore, we in turn argue that \u2016xsk\u2212xs0\u20162 is at most some constant times f(xsk)\u2212f(xs0). To prove so, we wish to select u = xs0 in Lemma 3.2 and telescope it for multiple iterations k, ideally for all the iterations k within the same epoch. This is possible for convex objectives but impossible for non-convex ones. More specifically, the ratio between (1/2\u03b7+L/2) and (1/2\u03b7) can be much larger than 1, preventing us from telescoping more than O(1/\u03b7L) iterations in any meaningful manner (see (4.1)). In contrast, this ratio would be identical to 1 in the convex setting, or even smaller than 1 in the strongly convex setting. For this reason, we define \u03b7 = 1\nm2/3L , divide each epoch\ninto O(m1/3) subepochs each consisting of O(m2/3) consecutive iterations. Now we can telescope\n6This new technique has also been applied to convex settings recently [1].\nLemma 3.2 for all the iterations within a subepoch because m2/3 \u2264 O(1/\u03b7L). Finally, we use vector inequalities (see (4.5)) to combine these upper bounds for sub-epochs into an upper bound on the entire epoch. Technical Details. We choose \u03b7 = 1m0L for some parameter m0 that divides m. We will set m0 to be m2/3 and the reason will become clear at the end of this section. Define d = m/m0 so an epoch is divided into d sub-epochs.\nWe make the following parameter choices\nDefinition 4.1. Define \u03b20 = 1 and \u03b2t def = (1 + \u03b7L)\u2212t = (1 + 1/m0)\u2212t for t = 1, . . . ,m0 \u2212 1. We have 1 \u2265 \u03b2t \u2265 1/e > 1/3. For each k = 0, 1, . . . ,m \u2212 m0, we sum up Lemma 3.2 for iterations k, k + 1, . . . , k + m0 \u2212 1 with multiplicative weights \u03b20, . . . , \u03b2m0\u22121 respectively. The norm square terms shall telescope in this summation, and we arrive at\nm0\u22121\u2211\nt=0\n\u03b2t ( f(xk+t)\u2212 f(u) ) \u2264 \u03b7\n2\nm0\u22121\u2211\nt=0\n\u03b2t ( H2k+t + \u03c32k+t ) + ( 1\n2\u03b7 + L 2\n) \u2016xk \u2212 u\u20162 \u2212\n\u03b2m0\u22121 2\u03b7 \u2016xk+m0 \u2212 u\u20162 .\n(4.1)\nSimplification 4.2. Since the weights \u03b20, . . . , \u03b2m0\u22121 are within each other by a constant factor, let us assume for simplicity that they are all equal to 1.\nIf we choose u = xk and assume \u03b2t = 1 for all t, we can rewrite (4.1) as\n1\nm0\nm0\u22121\u2211\nt=0\n( f(xk+t)\u2212 f(xk) ) \u2264 \u03b7\n2\n1\nm0\n( H2k:k+m0\u22121 + \u03c3 2 k:k+m0\u22121 ) \u2212 1\n6\u03b7m0 \u2016xk+m0 \u2212 xk\u20162 . (4.2)\nSimplification 4.3. Since the left hand side of (4.2) is describing the average objective value f(xk), f(xk+1), . . . , f(xk+m0\u22121) which is hard to analyze, let us assume for simplicity that it can be replaced with the last iteration in this subepoch, that is\nf(xk+m0)\u2212 f(xk) \u2264 \u03b7\n2\n1\nm0\n( H2k:k+m0\u22121 + \u03c3 2 k:k+m0\u22121 ) \u2212 1\n6\u03b7m0 \u2016xk+m0 \u2212 xk\u20162 . (4.3)\nUsing the above inequality we provide a novel upper bound on the variance of the gradient estimator:\nEit [ \u03c32t ] = Eit [\u2225\u2225(\u2207fit(xt)\u2212\u2207fit(x0) ) \u2212 ( \u2207f(xt)\u2212\u2207f(x0) )\u2225\u22252]\n\u2264 Eit [\u2225\u2225\u2207fit(xt)\u2212\u2207fit(x0) \u2225\u22252] \u2264 L2\u2016xt \u2212 x0\u20162 . (4.4)\nAbove, the first inequality is because for any random vector \u03b6 \u2208 Rd, it holds that E\u2016\u03b6 \u2212 E\u03b6\u20162 = E\u2016\u03b6\u20162 \u2212 \u2016E\u03b6\u20162, and the second inequality is by the smoothness of each fi(\u00b7).\nIn particular, for t = m, we can upper bound \u03c32m using (4.4) and multiple times of (4.3):\nE[\u03c32m] \u2264 L2E [ \u2016xm \u2212 x0\u20162 ] \u2264 L2dE [ \u2016xm \u2212 xm\u2212m0\u20162 + \u2016xm\u2212m0 \u2212 xm\u22122m0\u20162 + \u00b7 \u00b7 \u00b7+ \u2016xm0 \u2212 x0\u20162 ]\n\u2264 L2dE [ 6\u03b7m0 ( f(x0)\u2212 f(xm) ) + 3\u03b72 ( H20:m\u22121 + \u03c320:m\u22121 )] . (4.5)\nAbove, the first inequality follows from the vector inequality \u2016v1+\u00b7 \u00b7 \u00b7+vd\u20162 \u2264 d ( \u2016v1\u20162+\u00b7 \u00b7 \u00b7+\u2016vd\u20162 ) , and the second inequality follows from (4.3).\nSimplification 4.4. Suppose that (4.5) holds not only for \u03c32m but actually for all \u03c3 2 0, . . . , \u03c3 2 m\u22121, then it satisfies\n1\nm E[\u03c320:m\u22121] \u2264 L2dE\n[ 6\u03b7m0 ( f(x0)\u2212 f(xm) ) + 3\u03b72 ( H20:m\u22121 + \u03c320:m\u22121 )] . (4.6)\nAs long as 3\u03b72L2d \u2264 12m , (4.6) further implies\n1\nm E[\u03c320:m\u22121] \u2264 12\u03b7m0L2d \u00b7 E\n[ f(x0)\u2212 f(xm) + \u03b7\n2m0 H20:m\u22121\n] . (4.7)\nThis concludes our goal in this section which is to provide an upper bound (4.7) on the (average) variance by a constant times the objective difference f(x0)\u2212 f(xm)."}, {"heading": "5 Final Theorem", "text": "By applying the gradient descent guarantee Lemma 3.1 to the entire epoch. We obtain that\nf(x0)\u2212 E[f(xm)] \u2265 E [\u03b7\n2 H20:m\u22121 \u2212\n\u03b72L\n2 \u03c320:m\u22121\n] .\nCombining this with the variance upper bound (4.7), we immediately have\nf(x0)\u2212 E[f(xm)] \u2265 \u03b7\n2 E[H20:m\u22121]\u2212 6\u03b73m0mL3d \u00b7 E[f(x0)\u2212 f(xm) +\n\u03b7\n2m0 H20:m\u22121] . (5.1)\nIn other words, as long as 6\u03b73m0mL 3d \u2264 12 , we arrive at\nf(x0)\u2212 E[f(xm)] \u2265 \u03b7\n6 E[H20:m\u22121] . (5.2)\nNote that (5.2) is only for a single epoch and can be written as f(xs0)\u2212E[f(xsm)] \u2265 \u03b76E[ \u2211m\u22121\nt=0 \u2016\u2207f(xst )\u20162] in the general notation. Therefore, we can telescope it over all epochs s = 1, 2, . . . , S. Since we have chosen xs0, the initial vector in epoch s, to be x s\u22121 m , the last vector of the previous epoch, we obtain that\n1\nSm\nS\u2211\ns=1\nm\u22121\u2211\nt=0\nE [ \u2016\u2207f(xst )\u20162 ] \u2264 6 \u03b7Sm (f(x10)\u2212 f(xSm)) \u2264 6(f(x\u03c6)\u2212minx f(x)) \u03b7Sm .\nAt this point, if we randomly select s \u2208 [S] and t \u2208 [m] at the end of the algorithm, we conclude that\nE[\u2016\u2207f(xst )\u20162] \u2264 6(f(x\u03c6)\u2212minx f(x))\n\u03b7Sm .\n(We remark here that selecting an average iterate to output is a common step also used by GD or SGD for non-convex optimization. This step is often unnecessarily in practice.)\nFinally, let use choose the parameters properly. We simply let m = n be the epoch length. Since we have required 3\u03b72L2d \u2264 12m and 6\u03b73m0mL3d \u2264 12 in the previous section, and both these requirements can be satisfied when m30 \u2265 12m2, we set m0 = \u0398(m2/3) = \u0398(n2/3). Accordingly \u03b7 = 1m0L = \u0398 ( 1 n2/3L ) . In sum,\nTheorem 5.1. Under the simplification assumptions 4.2, 4.3 and 4.4, by choosing m = n and \u03b7 = \u0398 ( 1\nn2/3L\n) , the produced output x of Algorithm 1 satisfies that7\nE[\u2016\u2207f(x)\u20162] \u2264 O (L(f(x\u03c6)\u2212minx f(x))\nSn1/3\n) ."}, {"heading": "In other words, to obtain a point x satisfying \u2016\u2207f(x)\u20162 \u2264 \u03b5, the total number of iterations needed", "text": "for Algorithm 1 is\nSn = O (n2/3L(f(x\u03c6)\u2212minx f(x))\n\u03b5\n) .\nThe amortized per-iteration complexity of SVRG is at most twice of SGD. Therefore, this is a factor of \u2126(n1/3) faster than the full gradient descent method on solving (1.1)."}, {"heading": "6 Experiments", "text": ""}, {"heading": "6.1 Empirical Risk Minimization with Non-Convex Loss", "text": "We consider binary classification on four standard datasets that can be found on the LibSVM website [10]:\n\u2022 the adult (a9a) dataset (32, 561 training samples, 16, 281 testing samples, and 123 features). \u2022 the web (w8a) dataset (49, 749 training samples, 14, 951 testing samples, and 300 features). \u2022 the rcv1 (rcv1.binary) dataset (20, 242 training samples, 677, 399 testing samples, and 47, 236\nfeatures).\n\u2022 the mnist (class 1) dataset (60, 000 training samples, 10, 000 testing samples, and 780 features, Accuracy Experiment. In the first experiment we apply SVRG on training the `2-regularized ERM problem with six loss functions: logistic loss, squared loss, smoothed hinge loss (with smoothing parameters 0.01, 0.1 and 1 resp.), and smoothed zero-one loss (also known as sigmoid loss).8 We wish to see how non-convex loss compares to convex ones in terms of testing accuracy (and thus in terms of the generalization error).\nFor each of the four datasets, we also randomly flip 1/4 fraction, 1/8 fraction, or zero fraction of the training example labels. The purpose of this manipulation is to introduce outliers to the training set. We therefore have 4 \u00d7 3 = 12 datasets in total. We choose epoch length m = 2n as suggested by the paper SVRG for ERM experiments, and use the simple Algorithm 1 for both convex and non-convex loss functions.\nWe present the accuracy results partially in Figure 1 (and the full version can be found in Figure 4 in the appendix). The y-axis represents the classification testing accuracy, and the x-axis represents the number of passes to the dataset. (Each iteration of SVRG counts as 1/n pass and the full-gradient computation of SVRG counts as 1 pass.)\nThese plots are produced based on a fair and careful parameter-tuning and parameter-validation procedure that can be described in the following four steps. Step I: for each of the 12 datasets, we partition the training samples randomly into a training set of size 4/5 and a validation set of size\n7Like in SGD, one can easily apply a Markov inequality to conclude that with probability at least 2/3 we have the same asymptotic upper bound on the deterministic quantity \u2016\u2207f(x)\u20162.\n8For the sigmoid loss, we scale it properly so that its smoothness parameter is exactly 1. Unlike hinge loss, it is unnecessary to consider sigmoid loss with different smoothness parameters: one can carefully verify that by scaling up or down the weight of the `2 regularizer, it is equivalent to changing the smoothness of the sigmoid loss.\n1/5. Step II: for each of the 12 datasets and each loss function, we enumerate over 10 choices of \u03bb, the regularization parameter. For each \u03bb, we tune SVRG on the training set with different step lengths \u03b7 and choose the best \u03b7 that gives the fastest training speed. Step III: for each of the 12 datasets and each loss function, we tune the best \u03bb using the validation set. That is, we use the selected \u03b7 from Step II to train the linear predictor, and apply it on the validation set to obtain the testing accuracy. We then select the \u03bb that gives the best testing accuracy for the validation set. Step IV: for each of the 12 datasets and each loss function, we apply the validated linear predictor to the testing set and present it in Figure 1 and Figure 4.\nWe make the following observations from this experiment.\n\u2022 Although sigmoid loss is only comparable to hinge loss or logistic loss for \u201cno flip\u201d datasets, however, when the input has a lot of outliers (see \u201cflip 1/8\u201d and \u201cflip 1/4\u201d), sigmoid loss is undoubtedly the best choice. Square loss is almost always dominated because it is not necessarily a good choice for binary classification.\n\u2022 The running time needed for SVRG on these datasets are quite comparable, regardless of the loss function being convex or not.\nRunning-Time Experiment. In this second experiment, we fix the regularization parameter \u03bb and compare the training objective convergence between SGD and SVRG for sigmoid loss only.9 We choose four different \u03bb per dataset and present our plots partially in Figure 2 (and the full plots can be found in Figure 5 in the appendix). In these plots, the y-axis represents the training objective value, and the x-axis represents the number of passes to the dataset.\nFor a fair comparison we need to tune the step length \u03b7 for each dataset and each choice of \u03bb. For SGD, we enumerate over polynomial learning rates \u03b7k = \u03b1 \u00b7 (1 + k/n)\u03b2 where k is the number of iterations passed; we have made 10 choices of \u03b1, considered \u03b2 = 0, 0.1, . . . , 1.0, and selected the learning rate that gives a fastest convergence. For SVRG, we first consider the vanilla SVRG using a constant \u03b7 throughout all iterations, and select the best \u03b7 that gives the fastest convergence. This curve is presented in dashed blue in Figure 5. We also implement SVRG with polynomial learning rates \u03b7k = \u03b1 \u00b7 (1 + k/n)\u03b2 and tune the best \u03b1, \u03b2 parameters and present the results in dashed black curves in Figure 5.\nWe make the following observations from this experiment.\n\u2022 Consistent with theory, SVRG is not necessarily a better choice than SGD for large training error \u03b5. However, SVRG enjoys a very fast convergence especially for small \u03b5.\n\u2022 The smaller \u03bb is, the more \u201cnon-convex\u201d the objective function becomes. We see that SGD performs more poorer than SVRG in these cases.10 \u2022 With only one exception (dataset web with \u03bb = 10\u22126), choosing a polynomial learning rate does not seem necessary in terms of improving the running time for training ERM problems with non-convex loss.\n\u2022 Although not presented in Figure 5, the best-tuned polynomial learning rates for SVRG have smaller exponents \u03b2 as compared to SGD in each of the 16 plots."}, {"heading": "6.2 Neural Network", "text": "We consider the multi-class (in fact, 10-class) classification problem on CIFAR-10 (60, 000 training samples) and MNIST (10, 000 training samples), two standard image datasets for neural net studies. We construct a toy two-layered neural net, with (1) 64 hidden nodes in the first layer, each connecting to a uniformly distributed 4x4 or 5x5 pixel block of the input image and having a smoothed relu (also known as softplus) activation function; (2) 10 output nodes on the second\n9This experiment is the minimization problem with respect to all training samples since there is no need to perform validation.\n10We note that the plots for different values \u03bb are presented with different vertical scales. For instance, at 100 passes of the dataset, the objective difference between SVRG and SGD is more than 2\u00d710\u22124 for \u03bb = 10\u22126 on dataset web, but less than 5\u00d7 10\u22126 for \u03bb = 10\u22123.\nlayer, fully connected to the first layer and each representing one of the ten classification outputs. We consider training such neural networks with the multi-class logistic loss that is a function on the 10 outputs and the correct label. For each of the two datasets, we consider both training the unregularized version, as well as the `2 regularized version with weight 10\n\u22123 for CIFAR-10 and 10\u22124 for MNIST, two parameters suggested by [16].\nWe implement two classical algorithms: stochastic gradient descent (SGD) with the best tuned polynomial learning rate and adaptive subGradient method (AdaGrad) of [9, 19] which is essentially SGD but with an adaptive learning rate. We choose a mini-batch size of 100 for both these methods. We consider four variants of SVRG, all of which use epoch length m = 5n/b if b is the mini-batch size: \u2022 SVRG-1, the simple Algorithm 1 with a best tuned polynomial learning rate and b = 100. \u2022 SVRG-2, our full Algorithm 2 with a best tuned polynomial learning rate and b = 100.11\n\u2022 SVRG-3, using adaptive learning rate (similar to AdaGrad) on top of SVRG-2 with b = 100. \u2022 SVRG-4, same as SVRG-3 but with b = 16. Our training error performance is presented in Figure 3. We also include the testing accuracy in Figure 6 in the appendix. In these plots the y axis represents the training objective value, and the x axis represents the number of passes to the dataset. Each iteration of SGD or SVRG counts as b/n pass of the dataset, and the snapshot full-gradient computation counts as 1 pass.12 From the plots we clearly see a performance advantage for using SVRG-based algorithms as compared to SGD or AdaGrad. Furthermore, we observe that the following three features on top of SVRG could further improve its running time:\n1. Comparing SVRG-2 with SVRG-1, we see that setting the epoch initial vector to be a weighted average of the last a few iterations of the previous epoch is recommended.\n2. Comparing SVRG-3 with SVRG-2, we see that using adaptive learning rates comparing to tuning the best polynomial learning rate is recommended.\n3. Comparing SVRG-4 with SVRG-3, we see that a smaller mini-batch size is recommended in terms of the total complexity. In contrast, reducing the mini-batch size is discouraged for SGD or AdaGrad because the variance could blow up and the performances would be decreased (this is also observed by our experiment but not included in the plots).\nWe hope that the above observations provide new insights for experimentalists working on deep learning.\n11That is, we set the initial vector of each epoch to be weighted average of the last (m/b)2/3 vectors from the previous epoch.\n12The number of passes to the dataset is a traditional unit for comparing stochastic methods. For ERM problems, it is natural to count each iteration of SVRG as b/n passes of the data rather than 2b/n, because the computation of \u2207fi(x\u0303) is free if one efficiently stores \u2207fi(x\u0303) when the full gradient was computed at x\u0303. However, after our paper has appeared online, we noticed this measurement may not be fair for SGD on training neural networks, because it is memory-inefficient to store \u2207fi(x\u0303) when fi comes from a large-scale neural network. For this reason, the sequential per-iteration cost of SVRG can be a factor (2 + 1/5)/(1 + 1/5) = 11/6 greater than SGD. Nevertheless, the extra cost on computing \u2207fi(x\u0303) is totally parallelizable (and can be viewed as doubling the mini-batch size), so this may not affect the GPU-based running time of SVRG by that much. We leave it a future work to run SVRG on large-scale network networks because it is beyond the scope of this paper.\nAlgorithm 2 Our full SVRG method in the non-convex setting Input: x\u03c6 a starting vector, S number of epochs, m number of iterations per epoch, m0 subepoch length, \u03b7 step length.\n1: x10 \u2190 x\u03c6 2: for s\u2190 1 to S do 3: \u00b5\u0303\u2190 \u2207f(xs0) 4: for k \u2190 0 to m\u2212 1 do 5: Pick i uniformly at random in {1, \u00b7 \u00b7 \u00b7 , n}. 6: \u2207\u0303 \u2190 \u2207fi(xsk)\u2212\u2207fi(xs0) + \u00b5\u0303 7: xsk+1 = x s k \u2212 \u03b7\u2207\u0303 8: end for 9: Define \u03b20, \u03b21, . . . , \u03b2m0\u22121 following Definition 4.1\n10: Select a random ms \u2208 {m,m\u2212 1, . . . ,m\u2212m0 + 1} with probability proportional to { \u03b2m0\u22121, 10\n9 \u03b2m0\u22121,\n10\n9 (\u03b2m0\u22121 + \u03b2m0\u22122), . . . ,\n10\n9 (\u03b2m0\u22121 + \u00b7 \u00b7 \u00b7+ \u03b21)\n} .\n11: xs+10 \u2190 xsms . 12: end for 13: return a vector uniformly at random from the set {xst\u22121 : s \u2208 [S], t \u2208 [ms]}"}, {"heading": "Acknowledgements", "text": "E. Hazan acknowledges support from the National Science Foundation grant IIS-1523815 and a Google research award. Z. Allen-Zhu acknowledges support from a Microsoft research award, no. 0518584.\nAppendix"}, {"heading": "A Detailed Proof", "text": "In the detailed proof, we again first concentrating on analyzing a single epoch. We choose as before \u03b7 = 1m0L for some parameter m0 that divides m. The natural choice of m0 shall become clear at the end of this section, and would be set to around m2/3. Define d = m/m0 so an epoch is divided into d sub-epochs. We denote by x\u2212m0+1 = \u00b7 \u00b7 \u00b7 = x\u22121 def = x0 for notational convenience, and similarly define \u2207\u2212m0+1 = \u00b7 \u00b7 \u00b7 = \u2207\u22121 = \u2207\u0303\u2212m0+1 = \u00b7 \u00b7 \u00b7 = \u2207\u0303\u22121 = \u03c3\u2212m0+1 = \u00b7 \u00b7 \u00b7 = \u03c3\u22121 = 0. Throughout this section, we also drop the expectation sign E[\u00b7] for notational simplicity.\nOur full algorithm for the non-convex setting is slightly different from our sketched proof, see Algorithm 2. Most importantly, instead of setting the first vector of each epoch to be the last iterate of the previous epoch, we set it to be a non-uniform random iterate in the last subepoch of the previous epoch. This step is crucial for our analysis to hold without the simplification assumptions.\nA.1 Upper Bounding the Variance\nThe following lemma is a simple counterpart to (4.4) in our sketched-proof section. It upper bounds the average variance inside an epoch by the average squared distances between vectors that are m0 iterations away from each other.\nLemma A.1. m\u22121\u2211\nt=0\n\u03c32t \u2264 L2d2 m\u22121\u2211\nt=0\n\u2016xt+1 \u2212 xt+1\u2212m0\u20162\nProof. Recall that we have that for every t \u2208 {0, 1, . . . ,m\u2212 1}, we have\n\u03c32t \u2264 L2\u2016xt \u2212 x0\u20162 \u2264 L2d(\u2016xt \u2212 xt\u2212m0\u20162 + \u2016xt\u2212m0 \u2212 xt\u22122m0\u20162 + \u00b7 \u00b7 \u00b7 )\nSumming this up over all possible t\u2019s, we have\nm\u22121\u2211\nt=0\n\u03c32t \u2264 L2d2 m\u22121\u2211\nt=0\n\u2016xt+1 \u2212 xt+1\u2212m0\u20162 .\nWe emphasize that this analysis relies on our careful choice of x\u2212m0+1 = \u00b7 \u00b7 \u00b7 = x\u22121 def = x0 which simplifies our notations. We next state a simple variant of Lemma 3.2 that allows negative indices:\nLemma A.2. For every k \u2208 {\u2212m0 + 1, . . . ,m\u2212m0}, every t \u2208 {0, . . . ,m0\u2212 1}, and every u \u2208 Rd, we have\nf(xk+t)\u2212 f(u) \u2264 \u03b7\n2\n( H2k+t + \u03c32k+t ) + ( 1\n2\u03b7 + L 2\n) \u2016xk+t \u2212 u\u20162 \u2212 1\n2\u03b7 \u2016xk+t+1 \u2212 u\u20162\nWe define the same sequence of \u03b20, \u03b21, . . . , \u03b2m0\u22121 as before:\nDefinition A.3. Define \u03b20 = 1 and \u03b2t def = (1 + \u03b7L)\u2212t = (1 + 1/m0)\u2212t for t = 1, . . . ,m0 \u2212 1. We have 1 \u2265 \u03b2t \u2265 1/e > 1/3. By summing up Lemma A.2 with multiplicative ratios \u03b2t for each t = 0, 1, . . . ,m0\u22121, we arrive at the following lemma which is a counterpart of (4.1) in the sketched-proof section.\nLemma A.4. For every k \u2208 {\u2212m0 + 1, . . . ,m\u2212m0}, and every u, m0\u22121\u2211\nt=0\n\u03b2t ( f(xk+t)\u2212 f(u) ) \u2264 \u03b7\n2\nm0\u22121\u2211\nt=0\n\u03b2t ( H2k+t + \u03c32k+t ) + ( 1\n2\u03b7 + L 2\n) \u2016xk \u2212 u\u20162 \u2212\n\u03b2m0\u22121 2\u03b7 \u2016xk+m0 \u2212 u\u20162\nIn particular, if we select u = xk, we obtain that\nm0\u22121\u2211\nt=1\n\u03b2t ( f(xk+t)\u2212 f(xk) ) \u2264 \u03b7\n2\nm0\u22121\u2211\nt=0\n\u03b2t ( H2k+t + \u03c32k+t ) \u2212 1\n6\u03b7 \u2016xk+m0 \u2212 xk\u20162 . (A.1)\nThe next lemma sums up (A.1) over all possible values of k. It can be viewed as a weighted, more sophisticated version of (4.6) in our sketched-proof section.\nLemma A.5. As long as m0 \u2264 16\u03b72L2d2 , we have\n\u03b2m0\u22121 ( f(xm\u22121)\u2212 f(x0)\u2212 2\u03b7H20:m\u22122 )\n+ (\u03b2m0\u22121 + \u03b2m0\u22122) ( f(xm\u22122)\u2212 f(x0)\u2212 2\u03b7H20:m\u22123 ) + \u00b7 \u00b7 \u00b7 + (\u03b21 + \u00b7 \u00b7 \u00b7+ \u03b2m0\u22121) ( f(xm\u2212m0+1)\u2212 f(x0)\u2212 2\u03b7H20:m\u2212m0 )\n\u2264 \u03b7 2 \u03b2m0\u22121H2m\u22121 \u2212\n1\n12\u03b7L2d2\nm\u22121\u2211\nt=0\n\u03c32t .\nProof. By carefully summing up (A.1) for k \u2208 {\u2212m0 + 1, . . . ,m\u2212m0}, we have that\n\u03b2m0\u22121f(xm\u22121) + (\u03b2m0\u22121 + \u03b2m0\u22122)f(xm\u22122) + \u00b7 \u00b7 \u00b7+ (\u03b21 + \u00b7 \u00b7 \u00b7+ \u03b2m0\u22121)f(xm\u2212m0+1) \u2212 (\u03b21 + \u00b7 \u00b7 \u00b7+ \u03b2m0\u22121)f(x\u2212m0+1)\u2212 (\u03b22 + \u00b7 \u00b7 \u00b7+ \u03b2m0\u22121)f(x\u2212m0+2)\u2212 \u00b7 \u00b7 \u00b7 \u2212 \u03b2m0\u22121f(x\u22121)\n\u2264 \u03b7 2\n(m0\u22121\u2211\nt=0\n\u03b2t )(m\u22121\u2211\nt=0\n\u03c32t ) + \u03b7\n2\n(m0\u22121\u2211\nt=0\n\u03b2t )(m\u2212m0\u2211\nt=0\nH2t )\n+ \u03b7\n2\n( \u03b2m0\u22121H2m\u22121 + (\u03b2m0\u22121 + \u03b2m0\u22122)H2m\u22122 + \u00b7 \u00b7 \u00b7+ (\u03b21 + \u00b7 \u00b7 \u00b7+ \u03b2m0\u22121)H2m\u2212m0+1 )\n\u2212 1 6\u03b7\nm\u22121\u2211\nt=0\n\u2016xt+1 \u2212 xt+1\u2212m0\u20162 .\nUsing the fact that x\u2212m0+1 = \u00b7 \u00b7 \u00b7 = x\u22121 def = x0, we can rewrite the left hand side and get\n\u03b2m0\u22121 ( f(xm\u22121)\u2212 f(x0) ) + (\u03b2m0\u22121 + \u03b2m0\u22122) ( f(xm\u22122)\u2212 f(x0) ) + \u00b7 \u00b7 \u00b7 + (\u03b21 + \u00b7 \u00b7 \u00b7+ \u03b2m0\u22121) ( f(xm\u2212m0+1)\u2212 f(x0) )\n\u2264 \u03b7 2\n(m0\u22121\u2211\nt=0\n\u03b2t )(m\u22121\u2211\nt=0\n\u03c32t ) + \u03b7\n2\n(m0\u22121\u2211\nt=0\n\u03b2t )(m\u2212m0\u2211\nt=0\nH2t )\n+ \u03b7\n2 ( \u03b2m0\u22121H2m\u22121 + (\u03b2m0\u22121 + \u03b2m0\u22122)H2m\u22122 + \u00b7 \u00b7 \u00b7+ (\u03b21 + \u00b7 \u00b7 \u00b7+ \u03b2m0\u22121)H2m\u2212m0+1\ufe38 \ufe37\ufe37 \ufe38\n\u00ac\n)\n\u2212 1 6\u03b7\nm\u22121\u2211\nt=0\n\u2016xt+1 \u2212 xt+1\u2212m0\u20162 .\nUsing the specific values of \u03b2t\u2019s, we can relax the terms in \u00ac above and rewrite the above inequality as\n\u03b2m0\u22121 ( f(xm\u22121)\u2212 f(x0)\u2212 2\u03b7H2m\u22122 ) + (\u03b2m0\u22121 + \u03b2m0\u22122) ( f(xm\u22122)\u2212 f(x0)\u2212 2\u03b7H2m\u22123 ) + \u00b7 \u00b7 \u00b7 + (\u03b21 + \u00b7 \u00b7 \u00b7+ \u03b2m0\u22121) ( f(xm\u2212m0+1)\u2212 f(x0)\u2212 2H2m\u2212m0 )\n\u2264 \u03b7 2\n(m0\u22121\u2211\nt=0\n\u03b2t )(m\u22121\u2211\nt=0\n\u03c32t ) + \u03b7\n2\n(m0\u22121\u2211\nt=0\n\u03b2t )(m\u2212m0\u22121\u2211\nt=0\nH2t )\n\ufe38 \ufe37\ufe37 \ufe38 \n+ \u03b7\n2 \u03b2m0\u22121H2m\u22121 \u2212\n1\n6\u03b7\nm\u22121\u2211\nt=0\n\u2016xt+1 \u2212 xt+1\u2212m0\u20162 .\nNow we further relax the terms in  above and further conclude that\n\u03b2m0\u22121 ( f(xm\u22121)\u2212 f(x0)\u2212 2\u03b7H20:m\u22122 ) + (\u03b2m0\u22121 + \u03b2m0\u22122) ( f(xm\u22122)\u2212 f(x0)\u2212 2\u03b7H20:m\u22123 ) + \u00b7 \u00b7 \u00b7 + (\u03b21 + \u00b7 \u00b7 \u00b7+ \u03b2m0\u22121) ( f(xm\u2212m0+1)\u2212 f(x0)\u2212 2\u03b7H20:m\u2212m0 )\n\u2264 \u03b7 2\n(m0\u22121\u2211\nt=0\n\u03b2t )(m\u22121\u2211\nt=0\n\u03c32t ) + \u03b7\n2 \u03b2m0\u22121H2m\u22121 \u2212\n1\n6\u03b7\nm\u22121\u2211\nt=0\n\u2016xt+1 \u2212 xt+1\u2212m0\u20162\n\ufe38 \ufe37\ufe37 \ufe38 \u00ae\n.\nApplying the variance bound Lemma A.1 on the summation \u00ae, we have\n\u03b2m0\u22121 ( f(xm\u22121)\u2212 f(x0)\u2212 2\u03b7H20:m\u22122 ) + (\u03b2m0\u22121 + \u03b2m0\u22122) ( f(xm\u22122)\u2212 f(x0)\u2212 2\u03b7H20:m\u22123 ) + \u00b7 \u00b7 \u00b7 + (\u03b21 + \u00b7 \u00b7 \u00b7+ \u03b2m0\u22121) ( f(xm\u2212m0+1)\u2212 f(x0)\u2212 2\u03b7H20:m\u2212m0 )\n\u2264 \u03b7 2\n(m0\u22121\u2211\nt=0\n\u03b2t )(m\u22121\u2211\nt=0\n\u03c32t ) + \u03b7\n2 \u03b2m0\u22121H2m\u22121 \u2212\n1\n6\u03b7L2d2\nm\u22121\u2211\nt=0\n\u03c32t .\nFinally, as long as \u2211m0\u22121\nt=0 \u03b2t \u2264 16\u03b72L2d2 (which can be satisfied because m0 \u2264 16\u03b72L2d2 ), we have\n\u03b2m0\u22121 ( f(xm\u22121)\u2212 f(x0)\u2212 2\u03b7H20:m\u22122 ) + (\u03b2m0\u22121 + \u03b2m0\u22122) ( f(xm\u22122)\u2212 f(x0)\u2212 2\u03b7H20:m\u22123 ) + \u00b7 \u00b7 \u00b7 + (\u03b21 + \u00b7 \u00b7 \u00b7+ \u03b2m0\u22121) ( f(xm\u2212m0+1)\u2212 f(x0)\u2212 2\u03b7H20:m\u2212m0 )\n\u2264 \u03b7 2 \u03b2m0\u22121H2m\u22121 \u2212\n1\n12\u03b7L2d2\nm\u22121\u2211\nt=0\n\u03c32t .\nThis finishes the proof of Lemma A.5.\nA.2 Objective Decrease using Gradient Descent\nThe following lemma is a variant of (5.1). However, instead of lower bounding the objective decrease f(x0)\u2212f(xm) for the entire epoch as in the sketched-proof section, we have to carefully lower bound a weighted sum of f(x0)\u2212 f(xt) for t \u2208 {m,m\u2212 1, . . . ,m\u2212m0 + 1}, in order to make it consistent with the left hand side of Lemma A.5.\nLemma A.6.\n\u03b2m0\u22121 ( f(x0)\u2212 f(xm)\u2212 \u03b7\n2 H20:m\u22121\n) + \u03b2m0\u22121 ( f(x0)\u2212 f(xm\u22121)\u2212 \u03b7\n2 H20:m\u22122\n)\n+ (\u03b2m0\u22121 + \u03b2m0\u22122) ( f(x0)\u2212 f(xm\u22122)\u2212 \u03b7\n2 H20:m\u22123\n) + \u00b7 \u00b7 \u00b7\n+ (\u03b21 + \u00b7 \u00b7 \u00b7+ \u03b2m0\u22121) ( f(x0)\u2212 f(xm\u2212m0+1)\u2212 \u03b7\n2 H20:m\u2212m0\n)\n\u2265 \u2212\u03b7 2Lm0\n2\nm\u22121\u2211\nt=0\n\u03c32t\nProof. For each j = 1, 2, . . . ,m0, by telescoping Lemma 3.1 across iterations k = 0, 1, . . . ,m \u2212 j, we arrive at inequality\nf(x0)\u2212 f(xm\u2212j+1) \u2265 \u03b7\n2 H20:m\u2212j \u2212\n\u03b72L\n2\nm\u22121\u2211\nt=0\n\u03c32t .\nNow we write down these m0 inequalities separately, and multiply each of them by a positive weight:\n\u03b2m0\u22121 \u00d7 { f(x0)\u2212 f(xm) \u2265 \u03b7\n2 H20:m\u22121 \u2212\n\u03b72L\n2\nm\u22121\u2211\nt=0\n\u03c32t\n}\n\u03b2m0\u22121 \u00d7 { f(x0)\u2212 f(xm\u22121) \u2265 \u03b7\n2 H20:m\u22122 \u2212\n\u03b72L\n2\nm\u22121\u2211\nt=0\n\u03c32t\n}\n(\u03b2m0\u22122 + \u03b2m0\u22121)\u00d7 { f(x0)\u2212 f(xm\u22122) \u2265 \u03b7\n2 H20:m\u22123 \u2212\n\u03b72L\n2\nm\u22121\u2211\nt=0\n\u03c32t\n} \u00b7 \u00b7 \u00b7\n(\u03b21 + \u00b7 \u00b7 \u00b7+ \u03b2m0\u22121)\u00d7 { f(x0)\u2212 f(xm\u2212m0+1) \u2265 \u03b7\n2 H20:m\u2212m0 \u2212\n\u03b72L\n2\nm\u22121\u2211\nt=0\n\u03c32t\n}\nSumming these inequalities up, we obtain our desired inequality\n\u03b2m0\u22121 ( f(x0)\u2212 f(xm)\u2212 \u03b7\n2 H20:m\u22121\n) + \u03b2m0\u22121 ( f(x0)\u2212 f(xm\u22121)\u2212 \u03b7\n2 H20:m\u22122\n)\n+ (\u03b2m0\u22121 + \u03b2m0\u22122) ( f(x0)\u2212 f(xm\u22122)\u2212 \u03b7\n2 H20:m\u22123\n) + \u00b7 \u00b7 \u00b7\n+ (\u03b21 + \u00b7 \u00b7 \u00b7+ \u03b2m0\u22121) ( f(x0)\u2212 f(xm\u2212m0+1)\u2212 \u03b7\n2 H20:m\u2212m0\n)\n\u2265 \u2212\u03b7 2Lm20\n2\nm\u22121\u2211\nt=0\n\u03c32t .\nA.3 Final Theorem\nLet us now put together Lemma A.5 and Lemma A.6, and derive the following lemma:\nLemma A.7. As long as 6\u03b73L3m20d 2 = 1/9, we have\n\u03b2m0\u22121 ( f(x0)\u2212 f(xm)\u2212 \u03b7\n4 H20:m\u22121\n) +\n10\u03b2m0\u22121 9\n( f(x0)\u2212 f(xm\u22121)\u2212 \u03b7\n4 H20:m\u22122\n)\n+ 10\n9 (\u03b2m0\u22121 + \u03b2m0\u22122)\n( f(x0)\u2212 f(xm\u22122)\u2212 \u03b7\n4 H20:m\u22123\n) + \u00b7 \u00b7 \u00b7\n+ 10\n9 (\u03b21 + \u00b7 \u00b7 \u00b7+ \u03b2m0\u22121)\n( f(x0)\u2212 f(xm\u2212m0+1)\u2212 \u03b7\n4 H20:m\u2212m0\n) \u2265 0 .\nProof. By directly combining Lemma A.5 and Lemma A.6, we have\n\u03b2m0\u22121 ( f(x0)\u2212 f(xm)\u2212 \u03b7\n2 H20:m\u22121\n) + \u03b2m0\u22121 ( f(x0)\u2212 f(xm\u22121)\u2212 \u03b7\n2 H20:m\u22122\n)\n+ (\u03b2m0\u22121 + \u03b2m0\u22122) ( f(x0)\u2212 f(xm\u22122)\u2212 \u03b7\n2 H20:m\u22123\n) + \u00b7 \u00b7 \u00b7\n+ (\u03b21 + \u00b7 \u00b7 \u00b7+ \u03b2m0\u22121) ( f(x0)\u2212 f(xm\u2212m0+1)\u2212 \u03b7\n2 H20:m\u2212m0\n)\n\u2265 12\u03b7L2d2 \u00b7 \u03b7 2Lm20 2 \u00b7 ( \u03b2m0\u22121 ( f(xm\u22121)\u2212 f(x0)\u2212 2\u03b7H20:m\u22122 )\n+ (\u03b2m0\u22121 + \u03b2m0\u22122) ( f(xm\u22122)\u2212 f(x0)\u2212 2\u03b7H20:m\u22123 ) + \u00b7 \u00b7 \u00b7 + (\u03b21 + \u00b7 \u00b7 \u00b7+ \u03b2m0\u22121) ( f(xm\u2212m0+1)\u2212 f(x0)\u2212 2\u03b7H20:m\u2212m0 ) \u2212 \u03b7\n2 \u03b2m0\u22121H2m\u22121\n)\nSuppose we have 12\u03b7L2d2 \u00b7 \u03b7 2Lm20 2 = 6\u03b7 3L3m20d 2 = 1/9, then it satisfies that\n\u03b2m0\u22121 ( f(x0)\u2212 f(xm)\u2212 \u03b7\n2 H20:m\u22121\n) + \u03b2m0\u22121 ( f(x0)\u2212 f(xm\u22121)\u2212 \u03b7\n2 H20:m\u22122\n)\n+ (\u03b2m0\u22121 + \u03b2m0\u22122) ( f(x0)\u2212 f(xm\u22122)\u2212 \u03b7\n2 H20:m\u22123\n) + \u00b7 \u00b7 \u00b7\n+ (\u03b21 + \u00b7 \u00b7 \u00b7+ \u03b2m0\u22121) ( f(x0)\u2212 f(xm\u2212m0+1)\u2212 \u03b7\n2 H20:m\u2212m0\n)\n\u2265 1 9\n( \u03b2m0\u22121 ( f(xm\u22121)\u2212 f(x0)\u2212 2\u03b7H20:m\u22122 )\n+ (\u03b2m0\u22121 + \u03b2m0\u22122) ( f(xm\u22122)\u2212 f(x0)\u2212 2\u03b7H20:m\u22123 ) + \u00b7 \u00b7 \u00b7 + (\u03b21 + \u00b7 \u00b7 \u00b7+ \u03b2m0\u22121) ( f(xm\u2212m0+1)\u2212 f(x0)\u2212 2\u03b7H20:m\u2212m0 ) \u2212 \u03b7\n2 \u03b2m0\u22121H2m\u22121\n)\nAfter rearranging, we have\n\u03b2m0\u22121 ( f(x0)\u2212 f(xm)\u2212 \u03b7\n2 H20:m\u22121\n) +\n10\u03b2m0\u22121 9\n( f(x0)\u2212 f(xm\u22121)\u2212 \u03b7\n4 H20:m\u22122\n)\n+ 10\n9 (\u03b2m0\u22121 + \u03b2m0\u22122)\n( f(x0)\u2212 f(xm\u22122)\u2212 \u03b7\n4 H20:m\u22123\n) + \u00b7 \u00b7 \u00b7\n+ 10\n9 (\u03b21 + \u00b7 \u00b7 \u00b7+ \u03b2m0\u22121)\n( f(x0)\u2212 f(xm\u2212m0+1)\u2212 \u03b7\n4 H20:m\u2212m0\n) \u2265 \u2212 \u03b7\n18 \u03b2m0\u22121H2m\u22121 .\nAfter relaxing the right hand side, we conclude that\n\u03b2m0\u22121 ( f(x0)\u2212 f(xm)\u2212 \u03b7\n4 H20:m\u22121\n) +\n10\u03b2m0\u22121 9\n( f(x0)\u2212 f(xm\u22121)\u2212 \u03b7\n4 H20:m\u22122\n)\n+ 10\n9 (\u03b2m0\u22121 + \u03b2m0\u22122)\n( f(x0)\u2212 f(xm\u22122)\u2212 \u03b7\n4 H20:m\u22123\n) + \u00b7 \u00b7 \u00b7\n+ 10\n9 (\u03b21 + \u00b7 \u00b7 \u00b7+ \u03b2m0\u22121)\n( f(x0)\u2212 f(xm\u2212m0+1)\u2212 \u03b7\n4 H20:m\u2212m0\n) \u2265 0 .\nThis finishes the proof of Lemma A.7. Lemma A.8 naturally implies that if we select a random stopping vector for this epoch, we have the following corollary which is a counterpart of (5.2) in our sketch-proof section:\nCorollary A.8. If we set ms to be a random variable in {m,m\u22121, . . . ,m\u2212m0+1}, with probabilities proportional to { \u03b2m0\u22121, 10 9 \u03b2m0\u22121, 10 9 (\u03b2m0\u22121 + \u03b2m0\u22122), . . . , 10 9 (\u03b2m0\u22121 + \u00b7 \u00b7 \u00b7+ \u03b21) } , then Lemma A.7 implies that we have\nE [ f(x0)\u2212 f(xms)\u2212 \u03b7\n4 H20:ms\u22121\n] \u2265 0 .\nNote that Corollary A.8 is only for a single epoch and can be written as\nE[f(xs0)\u2212 f(xsms)] \u2265 \u03b7 4 E [ms\u22121\u2211\nt=0\n\u2016\u2207f(xst )\u20162 ]\nin the general notation. Therefore, we are now ready to telescope it over all the epochs s = 1, 2, . . . , S. Recall that we have chosen xs0, the initial vector in epoch s, to be x s\u22121 ms\u22121 , the random stopping vector from the previous epoch. Therefore, we obtain that\n1 m1 + \u00b7 \u00b7 \u00b7+mS S\u2211\ns=1\nms\u22121\u2211\nt=0\nE [ \u2016\u2207f(xst )\u20162 ] \u2264 4 \u03b7S(m1 + \u00b7 \u00b7 \u00b7+mS) ( f(x10)\u2212 E[f(xSmS )] )\n\u2264 O (f(x\u03c6)\u2212minx f(x)\n\u03b7Sm\n) .\nAt this point, if we select uniformly at random an output vector x from the set {xst\u22121 : s \u2208 [S], t \u2208 [ms]}, we conclude that\nE[\u2016\u2207f(xst )\u20162] \u2264 O (f(x\u03c6)\u2212minx f(x)\n\u03b7Sm\n) .\nFinally, let use choose the parameters properly. We simply let m = n be the epoch length. Since we have required m0 \u2264 16\u03b72L2d2 and 6\u03b73L3m20d2 = 1/9 in Lemma A.5 and Lemma A.7 respectively, and both these requirements can be satisfied when m30 \u2265 54m2, we set m0 = \u0398(m2/3) = \u0398(n2/3). Accordingly \u03b7 = 1m0L = O ( 1 n2/3L ) . In sum, Theorem A.9 (Formal statement of Theorem 5.1). By choosing m = n and \u03b8 = \u0398 (\n1 n2/3L\n) , the\nproduced output x of Algorithm 2 satisfies that13\nE[\u2016\u2207f(x)\u20162] \u2264 O (L(f(x\u03c6)\u2212minx f(x))\nSn1/3\n) ."}, {"heading": "In other words, to obtain a point x satisfying \u2016\u2207f(x)\u20162 \u2264 \u03b5, the total number of iterations needed", "text": "for Algorithm 1 is\nSn = O (n2/3L(f(x\u03c6)\u2212minx f(x))\n\u03b5\n) .\n13Like in SGD, one can easily apply a Markov inequality to conclude that with probability at least 2/3 we have the same asymptotic upper bound on the deterministic quantity \u2016\u2207f(x)\u20162."}], "references": [{"title": "Katyusha: The First Direct Acceleration of Stochastic Gradient Methods", "author": ["Zeyuan Allen-Zhu"], "venue": "ArXiv e-prints,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2016}, {"title": "Linear coupling: An ultimate unification of gradient and mirror descent", "author": ["Zeyuan Allen-Zhu", "Lorenzo Orecchia"], "venue": "ArXiv e-prints,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2014}, {"title": "Even Faster Accelerated Coordinate Descent Using Non-Uniform Sampling", "author": ["Zeyuan Allen-Zhu", "Peter Richt\u00e1rik", "Zheng Qu", "Yang Yuan"], "venue": "In ICML,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2016}, {"title": "Improved SVRG for Non-Strongly-Convex or Sum-of-Non- Convex Objectives", "author": ["Zeyuan Allen-Zhu", "Yang Yuan"], "venue": "In ICML,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2016}, {"title": "A practical algorithm for topic modeling with provable guarantees", "author": ["Sanjeev Arora", "Rong Ge", "Yonatan Halpern", "David M. Mimno", "Ankur Moitra", "David Sontag", "Yichen Wu", "Michael Zhu"], "venue": "In ICML,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2013}, {"title": "New algorithms for learning incoherent and overcomplete dictionaries", "author": ["Sanjeev Arora", "Rong Ge", "Ankur Moitra"], "venue": "In COLT,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2014}, {"title": "Lectures on Modern Convex Optimization", "author": ["Aharon Ben-Tal", "Arkadi Nemirovski"], "venue": "Society for Industrial and Applied Mathematics,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2013}, {"title": "SAGA: A Fast Incremental Gradient Method With Support for Non-Strongly Convex Composite Objectives", "author": ["Aaron Defazio", "Francis Bach", "Simon Lacoste-Julien"], "venue": "In NIPS,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2014}, {"title": "Adaptive subgradient methods for online learning and stochastic optimization", "author": ["John Duchi", "Elad Hazan", "Yoram Singer"], "venue": "The Journal of Machine Learning Research,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2011}, {"title": "LIBSVM Data: Classification, Regression and Multi-label", "author": ["Rong-En Fan", "Chih-Jen Lin"], "venue": null, "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2015}, {"title": "Fast and simple PCA via convex optimization", "author": ["Dan Garber", "Elad Hazan"], "venue": "ArXiv e-prints,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2015}, {"title": "Escaping from saddle points\u2014online stochastic gradient for tensor decomposition", "author": ["Rong Ge", "Furong Huang", "Chi Jin", "Yang Yuan"], "venue": "In COLT,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2015}, {"title": "Accelerated gradient methods for nonconvex nonlinear and stochastic programming", "author": ["Saeed Ghadimi", "Guanghui Lan"], "venue": "Mathematical Programming,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2015}, {"title": "DRAFT: Introduction to online convex optimimization", "author": ["Elad Hazan"], "venue": "Foundations and Trends in Machine Learning,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2015}, {"title": "A spectral algorithm for learning hidden markov models", "author": ["Daniel Hsu", "Sham M. Kakade", "Tong Zhang"], "venue": "J. Comput. Syst. Sci.,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2012}, {"title": "Accelerating stochastic gradient descent using predictive variance reduction", "author": ["Rie Johnson", "Tong Zhang"], "venue": "In NIPS,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2013}, {"title": "Accelerated Proximal Gradient Methods for Nonconvex Programming", "author": ["Huan Li", "Zhouchen Lin"], "venue": "In NIPS,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2015}, {"title": "An Accelerated Proximal Coordinate Gradient Method and its Application to Regularized Empirical Risk Minimization", "author": ["Qihang Lin", "Zhaosong Lu", "Lin Xiao"], "venue": "In NIPS,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2014}, {"title": "Adaptive Bound Optimization for Online Convex Optimization", "author": ["H. Brendan McMahan", "Matthew Streeter"], "venue": "In Proceedings of the 23rd Annual Conference on Learning Theory - COLT", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2010}, {"title": "Introductory Lectures on Convex Programming Volume: A Basic course, volume I", "author": ["Yurii Nesterov"], "venue": "Kluwer Academic Publishers,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2004}, {"title": "Stochastic variance reduction for nonconvex optimization", "author": ["Sashank J. Reddi", "Ahmed Hefny", "Suvrit Sra", "Barnabas Poczos", "Alex Smola"], "venue": "ArXiv e-prints,", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2016}, {"title": "Fast incremental method for nonconvex optimization", "author": ["Sashank J. Reddi", "Suvrit Sra", "Barnabas Poczos", "Alex Smola"], "venue": "ArXiv e-prints,", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2016}, {"title": "Minimizing finite sums with the stochastic average gradient", "author": ["Mark Schmidt", "Nicolas Le Roux", "Francis Bach"], "venue": "arXiv preprint arXiv:1309.2388,", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2013}, {"title": "SDCA without Duality", "author": ["Shai Shalev-Shwartz"], "venue": "arXiv preprint arXiv:1502.06177,", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2015}, {"title": "Learning kernel-based halfspaces with the 0-1 loss", "author": ["Shai Shalev-Shwartz", "Ohad Shamir", "Karthik Sridharan"], "venue": "SIAM Journal on Computing,", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2011}, {"title": "Stochastic dual coordinate ascent methods for regularized loss minimization", "author": ["Shai Shalev-Shwartz", "Tong Zhang"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "26", "shortCiteRegEx": "26", "year": 2013}, {"title": "Accelerated Proximal Stochastic Dual Coordinate Ascent for Regularized Loss Minimization", "author": ["Shai Shalev-Shwartz", "Tong Zhang"], "venue": "In ICML,", "citeRegEx": "27", "shortCiteRegEx": "27", "year": 2014}, {"title": "Stochastic Primal-Dual Coordinate Method for Regularized Empirical Risk Minimization", "author": ["Yuchen Zhang", "Lin Xiao"], "venue": "In ICML,", "citeRegEx": "28", "shortCiteRegEx": "28", "year": 2015}], "referenceMentions": [{"referenceID": 4, "context": "For some models, probabilistic and other assumptions on the input can be used to give specially designed polynomial-time algorithms [5, 6, 15].", "startOffset": 132, "endOffset": 142}, {"referenceID": 5, "context": "For some models, probabilistic and other assumptions on the input can be used to give specially designed polynomial-time algorithms [5, 6, 15].", "startOffset": 132, "endOffset": 142}, {"referenceID": 14, "context": "For some models, probabilistic and other assumptions on the input can be used to give specially designed polynomial-time algorithms [5, 6, 15].", "startOffset": 132, "endOffset": 142}, {"referenceID": 12, "context": "Following the classical benchmark for non-convex optimization (see for instance [13]), we focus on algorithms that can efficiently find an approximate stationary point x satisfying \u2016\u2207f(x)\u20162 \u2264 \u03b5.", "startOffset": 80, "endOffset": 84}, {"referenceID": 11, "context": "[12] also demonstrated that a simple noise-addition scheme is sufficient for stochastic gradient descent to escape from saddle points.", "startOffset": 0, "endOffset": 4}, {"referenceID": 12, "context": "This is a folklore result in optimization and included for instance in [13].", "startOffset": 71, "endOffset": 75}, {"referenceID": 12, "context": "This result is perhaps first formalized by Ghadimi and Lan [13].", "startOffset": 59, "endOffset": 63}, {"referenceID": 15, "context": "1 Our Result We prove that variance reduction techniques, based on the SVRG method [16], produce an \u03b5stationary point in only O (n2/3L(f(x0)\u2212f(x\u2217)) \u03b5 ) iterations.", "startOffset": 83, "endOffset": 87}, {"referenceID": 13, "context": "3 of [14] usually turn each fi(x) into a smooth function without sacrificing too much accuracy.", "startOffset": 5, "endOffset": 9}, {"referenceID": 15, "context": "Our proposed algorithm is very analogous to SVRG of [16].", "startOffset": 52, "endOffset": 56}, {"referenceID": 24, "context": "For instance, for binary classification problems, the sigmoid function \u2014or more broadly, any natural smoothed variant of the 0-1 loss function\u2014 is not only a more natural choice than artificial ones such as hinge loss, logistic loss, squared loss, but also generalize better in terms of testing accuracy especially when there are outliers [25].", "startOffset": 339, "endOffset": 343}, {"referenceID": 24, "context": "Shalev-Shwartz, Shamir and Sridharan [25] showed that this minimization problem is still solvable in the improper learning sense, with the help from kernel methods and gradient descent.", "startOffset": 37, "endOffset": 41}, {"referenceID": 3, "context": "Instead of requiring each fi(\u00b7) to be L-smooth, one can assume it is L-upper smooth and l-lower smooth, a notation introduced by [4]; in such a case, faster results can also be obtained using our same proof techniques.", "startOffset": 129, "endOffset": 132}, {"referenceID": 3, "context": "This is faster than the previous running time on SVRG which is \u00d5(n+L2/\u03c32), however, it is not faster than using SVRG+Catalyst which gives \u00d5(n + n3/4 \u221a L/ \u221a \u03c3), see discussion in [4].", "startOffset": 178, "endOffset": 181}, {"referenceID": 7, "context": "1) has received lots of attentions across machine learning and optimization communities; many first-order methods [8, 16, 23, 26] as well as their accelerations [1, 3, 18, 27, 28] have been proposed in the past a few years.", "startOffset": 114, "endOffset": 129}, {"referenceID": 15, "context": "1) has received lots of attentions across machine learning and optimization communities; many first-order methods [8, 16, 23, 26] as well as their accelerations [1, 3, 18, 27, 28] have been proposed in the past a few years.", "startOffset": 114, "endOffset": 129}, {"referenceID": 22, "context": "1) has received lots of attentions across machine learning and optimization communities; many first-order methods [8, 16, 23, 26] as well as their accelerations [1, 3, 18, 27, 28] have been proposed in the past a few years.", "startOffset": 114, "endOffset": 129}, {"referenceID": 25, "context": "1) has received lots of attentions across machine learning and optimization communities; many first-order methods [8, 16, 23, 26] as well as their accelerations [1, 3, 18, 27, 28] have been proposed in the past a few years.", "startOffset": 114, "endOffset": 129}, {"referenceID": 0, "context": "1) has received lots of attentions across machine learning and optimization communities; many first-order methods [8, 16, 23, 26] as well as their accelerations [1, 3, 18, 27, 28] have been proposed in the past a few years.", "startOffset": 161, "endOffset": 179}, {"referenceID": 2, "context": "1) has received lots of attentions across machine learning and optimization communities; many first-order methods [8, 16, 23, 26] as well as their accelerations [1, 3, 18, 27, 28] have been proposed in the past a few years.", "startOffset": 161, "endOffset": 179}, {"referenceID": 17, "context": "1) has received lots of attentions across machine learning and optimization communities; many first-order methods [8, 16, 23, 26] as well as their accelerations [1, 3, 18, 27, 28] have been proposed in the past a few years.", "startOffset": 161, "endOffset": 179}, {"referenceID": 26, "context": "1) has received lots of attentions across machine learning and optimization communities; many first-order methods [8, 16, 23, 26] as well as their accelerations [1, 3, 18, 27, 28] have been proposed in the past a few years.", "startOffset": 161, "endOffset": 179}, {"referenceID": 27, "context": "1) has received lots of attentions across machine learning and optimization communities; many first-order methods [8, 16, 23, 26] as well as their accelerations [1, 3, 18, 27, 28] have been proposed in the past a few years.", "startOffset": 161, "endOffset": 179}, {"referenceID": 3, "context": "1) can be solved easily [4, 11, 24].", "startOffset": 24, "endOffset": 35}, {"referenceID": 10, "context": "1) can be solved easily [4, 11, 24].", "startOffset": 24, "endOffset": 35}, {"referenceID": 23, "context": "1) can be solved easily [4, 11, 24].", "startOffset": 24, "endOffset": 35}, {"referenceID": 16, "context": "The results of Li and Lin [17] and Ghadimi and Lan [13] unify the theory of non-convex and convex optimization in the following sense.", "startOffset": 26, "endOffset": 30}, {"referenceID": 12, "context": "The results of Li and Lin [17] and Ghadimi and Lan [13] unify the theory of non-convex and convex optimization in the following sense.", "startOffset": 51, "endOffset": 55}, {"referenceID": 19, "context": "They provide general first-order schemes such that, if the parameters are tuned properly, the schemes can converge (1) as fast as gradient descent in terms of finding an approximate stationary point; and (2) as fast as accelerated gradient descent [20] in terms of minimizing the objective if the function is convex.", "startOffset": 248, "endOffset": 252}, {"referenceID": 20, "context": "A few days after the first version of this paper appeared on arXiv, we became aware of another group of authors that have independently obtained essentially the same result [21, 22].", "startOffset": 173, "endOffset": 181}, {"referenceID": 21, "context": "A few days after the first version of this paper appeared on arXiv, we became aware of another group of authors that have independently obtained essentially the same result [21, 22].", "startOffset": 173, "endOffset": 181}, {"referenceID": 6, "context": "Mirror descent is a terminology mostly used in optimization literature, see for instance the textbook [7].", "startOffset": 102, "endOffset": 105}, {"referenceID": 1, "context": "In our SVRG method, the descent step xk+1 \u2190 xk \u2212 \u03b7\u2207\u0303k can be interpreted as a mirror descent step in the Euclidean space (see for instance [2]), and therefore mirror-descent analysis applies.", "startOffset": 139, "endOffset": 142}, {"referenceID": 1, "context": "Our main theorem is motivated by the linear-coupling framework [2].", "startOffset": 63, "endOffset": 66}, {"referenceID": 15, "context": "The key idea behind all variance-reduction literatures (such as SVRG [16], SAGA [8], and SAG [23]) is to prove that the variance E[(\u03c3s k)] decreases as s or k increases.", "startOffset": 69, "endOffset": 73}, {"referenceID": 7, "context": "The key idea behind all variance-reduction literatures (such as SVRG [16], SAGA [8], and SAG [23]) is to prove that the variance E[(\u03c3s k)] decreases as s or k increases.", "startOffset": 80, "endOffset": 83}, {"referenceID": 22, "context": "The key idea behind all variance-reduction literatures (such as SVRG [16], SAGA [8], and SAG [23]) is to prove that the variance E[(\u03c3s k)] decreases as s or k increases.", "startOffset": 93, "endOffset": 97}, {"referenceID": 3, "context": "Perhaps the only exception is the work on sum-of-non-convex but strongly-convex objectives [4, 24], where the authors upper bound E[(\u03c3s k)] by O ( \u2016xk \u2212 x\u2217\u20162 ) , the squared vector distance to the minimum.", "startOffset": 91, "endOffset": 98}, {"referenceID": 23, "context": "Perhaps the only exception is the work on sum-of-non-convex but strongly-convex objectives [4, 24], where the authors upper bound E[(\u03c3s k)] by O ( \u2016xk \u2212 x\u2217\u20162 ) , the squared vector distance to the minimum.", "startOffset": 91, "endOffset": 98}, {"referenceID": 0, "context": "Now we can telescope This new technique has also been applied to convex settings recently [1].", "startOffset": 90, "endOffset": 93}, {"referenceID": 9, "context": "1 Empirical Risk Minimization with Non-Convex Loss We consider binary classification on four standard datasets that can be found on the LibSVM website [10]: \u2022 the adult (a9a) dataset (32, 561 training samples, 16, 281 testing samples, and 123 features).", "startOffset": 151, "endOffset": 155}, {"referenceID": 15, "context": "For each of the two datasets, we consider both training the unregularized version, as well as the `2 regularized version with weight 10 \u22123 for CIFAR-10 and 10\u22124 for MNIST, two parameters suggested by [16].", "startOffset": 200, "endOffset": 204}, {"referenceID": 8, "context": "We implement two classical algorithms: stochastic gradient descent (SGD) with the best tuned polynomial learning rate and adaptive subGradient method (AdaGrad) of [9, 19] which is essentially SGD but with an adaptive learning rate.", "startOffset": 163, "endOffset": 170}, {"referenceID": 18, "context": "We implement two classical algorithms: stochastic gradient descent (SGD) with the best tuned polynomial learning rate and adaptive subGradient method (AdaGrad) of [9, 19] which is essentially SGD but with an adaptive learning rate.", "startOffset": 163, "endOffset": 170}], "year": 2016, "abstractText": "We consider the fundamental problem in non-convex optimization of efficiently reaching a stationary point. In contrast to the convex case, in the long history of this basic problem, the only known theoretical results on first-order non-convex optimization remain to be full gradient descent that converges in O(1/\u03b5) iterations for smooth objectives, and stochastic gradient descent that converges in O(1/\u03b5) iterations for objectives that are sum of smooth functions. We provide the first improvement in this line of research. Our result is based on the variance reduction trick recently introduced to convex optimization, as well as a brand new analysis of variance reduction that is suitable for non-convex optimization. For objectives that are sum of smooth functions, our first-order minibatch stochastic method converges with an O(1/\u03b5) rate, and is faster than full gradient descent by \u03a9(n). We demonstrate the effectiveness of our methods on empirical risk minimizations with nonconvex loss functions and training neural nets.", "creator": "LaTeX with hyperref package"}}}