{"id": "1603.06459", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "12-Mar-2016", "title": "Characterization of neighborhood behaviours in a multi-neighborhood local search algorithm", "abstract": "We consider a multi-neighborhood local search algorithm with a large number of possible neighborhoods. Each neighborhood is accompanied by a weight value which represents the probability of being chosen at each iteration. These weights are fixed before the algorithm runs, and are considered as parameters of the algorithm. Given a set of instances, off-line tuning of the algorithm's parameters can be done by automated algorithm configuration tools (e.g., SMAC). However, the large number of neighborhoods can make the tuning expensive and difficult even when the number of parameters has been reduced by some intuition. In this work, we propose a systematic method to characterize each neighborhood's behaviours, representing them as a feature vector, and using cluster analysis to form similar groups of neighborhoods. The novelty of our characterization method is the ability of reflecting changes of behaviours according to hardness of different solution quality regions. We show that using neighborhood clusters instead of individual neighborhoods helps to reduce the parameter configuration space without misleading the search of the tuning procedure. Moreover, this method is problem-independent and potentially can be applied in similar contexts.", "histories": [["v1", "Sat, 12 Mar 2016 12:38:32 GMT  (95kb)", "http://arxiv.org/abs/1603.06459v1", "13 pages"]], "COMMENTS": "13 pages", "reviews": [], "SUBJECTS": "cs.AI", "authors": ["nguyen thi thanh dang", "patrick de causmaecker"], "accepted": false, "id": "1603.06459"}, "pdf": {"name": "1603.06459.pdf", "metadata": {"source": "CRF", "title": "Characterization of neighborhood behaviours in a multi-neighborhood local search algorithm", "authors": ["Nguyen Thi Thanh Dang"], "emails": ["nguyenthithanh.dang@kuleuven-kulak.be", "patrick.decausmaecker@kuleuven-kulak.be"], "sections": [{"heading": null, "text": "ar X\niv :1\n60 3.\n06 45\n9v 1\n[ cs\n.A I]\n1 2\nM ar\n2 01\n6\nKeywords: algorithm configuration, clustering, multi-neighborhood local search"}, {"heading": "1 Introduction", "text": "Because optimization algorithms are usually highly parameterized, algorithm parameter tuning/configuration is an important task. Given a distribution of problem instances, we need to find parameter configurations that optimize a predefined performance measure over the distribution, such as mean of optimality gap. For the last fifteen years, automated algorithm configuration has been extensively studied [1]. General-purpose automated algorithm configuration tools such as SMAC [2] and irace [3] have been successfully applied in several studies.\nIn this work, we consider the parameter tuning problem of a multi-neighborhood local search algorithm [4], which consists of a large number of neighborhoods. The algorithm is the winner of the Verolog Solver Challenge 2014 [5]. At each iteration, a neighbor solution is generated by a randomly chosen neighborhood with a probability defined by a weight value in the range of [0,1]. Weights of all\nneighborhoods are fixed before the algorithm runs, and are considered as algorithm parameters. Given a set of six (large) instances provided by the challenge, automated algorithm configuration tools can be used to tune the algorithm parameters. However, the large number of parameters (28 real parameters for the weight values and 2 integer parameters for the local search) might deteriorate the tuning tool\u2019s efficiency, especially in our case where each run of the algorithm is not computationally cheap (600 seconds per run for each instance). A potential solution is to cluster neighborhoods into groups and assign a common weight value to each. It can help to reduce the algorithm configuration space, hoping to make use of available tuning budgets more efficiently. The key question raised while doing such a clustering is how to characterize each neighborhood behaviours over a set of instances and represent them as a feature vector. In this paper, we propose a method to do so. This method is problem-independent and does not depend on any specific local search. Moreover, it can be done during stages of algorithm development, e.g., testing, manual/automated tuning.\nThis paper is organized as follows. We describe the tuning problem in more detail in section 2. The method for characterizing neighborhoods\u2019 behaviours and clustering them is explained in section 3. Section 4 shows the advantage of using clustering in automated parameter tuning and experimental results. Finally, section 5 gives conclusion and discussion on future work."}, {"heading": "2 Parameter tuning for a multi-neighborhood local search algorithm", "text": "The algorithm considered in this work, which was developed by CODeS group\u2019s members of the University of Leuven (Belgium) [4], tackles the Swap-body Vehicle Routing problem. It is an iterated local search [6] algorithm that uses late acceptance hill climbing [7] as the local search component. At each iteration of the late acceptance hill climbing, a neighborhood Nk is randomly chosen from a large set of neighborhoods, and a neighbor solution s is generated according to Nk. The probability that a neighborhood Nk is chosen is proportional to its weight value wk. These weight values are fixed during each algorithm\u2019s run, and sum up to one. In addition, there are two integer parameters that control the late acceptance hill climbing: this local search component is stopped after a number of itWI consecutive iterations without any improvement on the current solution, and the parameter laList represents the size of the saved memory.\nThe algorithm consists of 42 neighborhoods, which were generated from 18 neighborhood types. Some of them are specially designed for the Swap-body Vehicle Routing problem (e.g., Convert-to-sub-route) while the others are taken from the Vehicle Routing Problem literature (e.g., Cheapest-insertion). Some neighborhood types can be parameterized by their sizes. For example: the size of a Cheapest-insertion neighborhood is defined by the number of customers that will be removed and re-inserted back into the current solution. We can have a small Cheapest-insertion neighborhood with the size of 2, and a large Cheapest-insertion neighborhood with the size of 25.\nIntuition can be used to reduce the number of weights to 28: some neighborhoods that belong to the same neighborhood type and have similar sizes can be grouped into one. The list of neighborhood types and their groups of sizes are listed in Table 1. Parameter tuning is done on six (large) problem instances (large normal, large with, large without, new normal, new with, new without) provided by the competition. An algorithm run on each instance takes 600 seconds. Note that the algorithm considered in this paper is actually not the same as the one that won the competition. The winning one is multi-threaded (4 independent parallel runs) while the one we use here is single-threaded. This is because the aim of our work is not to beat the winning algorithm, but to use this case study as a proof of concept for our characterization method."}, {"heading": "3 Neighborhood characterization and clustering", "text": "Inspired by the idea from OSCAR [8], which is an automated approach for online selection of algorithm portfolio, we characterize each neighborhood Nk\u2019s behaviours on an instance I based on the following six observables:\n\u2013 Probabilities that Nk improves, worsens or does nothing on a solution of I, denoted as rimprove, rworsen, rnothing, where\nrimprove + rworsen + rnothing = 1\n\u2013 Magnitudes of improvement and worsening, denoted as aimprove and aworsen \u2013 Nk\u2019s running time (used for tie-breaking, as explained in section 3.3)\nThe novelty of our method is that we will represent Nk using the estimated values of those observables on different solution quality regions, as they reflect changes of Nk\u2019s behaviours according to the hardness of the solution that it is dealing with. An illustration of such changes of rimprove, rworsen and rnothing for four neighborhoods on an instance is visualized in Figure 1. The x-axis represents solution quality (the larger the value, the better the corresponding solution is) and the y-axis represents values of the three observables. In order to draw those plots, we divide the range of solution quality into intervals, collect necessary information during algorithm runs, and group every ten intervals into one. Details on how to collect information for such a visualization will be described in section 3.1. In Figure 1c, we can see that when the solution quality is low, i.e., the local search is in easy-to-improve region of the solution quality space, the Mergeroute neighborhood has a very high probability of improving the solution it is tackling. This probability drastically decreases when the neighborhood reaches a good solution quality region, and the probability of worsening the current solution starts reaching one from that point. On the other hand, the Remove-route neighborhood in Figure 1d shows a similar behaviour in the low-solution quality region. However, in the good-solution quality region, the neighborhood tends to preserve the quality of the current solution rather than worsen it. Even neighborhoods belonging to the same neighborhood type can behave differently in different regions. As shown in Figure 1a and 1b, the small Cheapest-insertion neighborhood with size 2 has a much smaller probability of worsening a solution in the hard-to-improve region compared with the large Cheapest-insertion neighborhood with size 25.\nIn the rest of this section, we introduce four steps to characterize and cluster neighborhoods. Firstly, necessary information is collected during algorithms runs. Then solution quality regions are automatically identified. Next, collected information on each region is aggregated to build neighborhoods feature vectors. Finally, we carry on cluster analysis."}, {"heading": "3.1 Collect necessary information during algorithm runs", "text": "In this part, we describe the procedure of collecting all necessary information for characterizing neighborhood behaviours. Given a problem instance, we assume\nthat an upper bound and a lower bound of the optimal solution quality are available. Since these bounds do not need to be tight, this assumption is not hard to be satisfied. For example, the upper bound could be obtained from a random solution or a solution generated by some greedy algorithm and the lower bound could result from solving a linear programming relaxation of the problem. In the algorithm considered in this work, the initial solution for each instance is produced by creating one route for each customer. We take that solution\u2019s value as the upper bound. A lower bound for each instance is provided by the authors of the algorithm, as the best solution obtained from running their best algorithm configuration (the multi-threaded version) in six hours.\nWe divide the range between the upper bound and the lower bound into a large number of small intervals (here we set it as 1000). Because higher quality solutions in general are harder to improve, we let the size of the intervals decrease exponentially. Each next interval has a size 0.99 the size of the previous interval.\nNow every time a neighborhood Nk is applied on a solution that has quality value belonging to an interval Ij , the following values are accumulatively collected for the pair of (Nk, Ij):\n\u2013 niters: the number of times Nk is applied, \u2013 nI , nSN , nW : the numbers of times Nk improves, does nothing, or worsens\nsolutions, respectively, \u2013 sI , sW : sums of the amount of improvement and worsening, \u2013 stime: sum of Nk\u2019s running time.\nSince the collection of these values is independent of algorithm configuration, it can be done during any algorithm runs, such as during testing, manual parameter tuning, or automated algorithm tuning. The more runs there are, the\nbetter the estimated values of the observables. In this work, we collect them from running two algorithm configurations on all instances, with 10 runs per instance, so the total number of algorithm runs is 240. We use a little bit longer running time (900 seconds per run) to make sure that the collected information can cover hard parts of the solution quality space."}, {"heading": "3.2 Identify solution quality regions as frames", "text": "Intervals are grouped into frames based on sum nIters, the sum of all neighborhoods\u2019 niters values on each interval. Figure 2 shows plots of sum nIters on each instance. Note that because lower bounds on solution quality are not reached, intervals with zero sum nIters at the end are removed. In this figure, there is a high peak in every plot, representing the interval where the algorithm stays most of the time. We thus conjecture that local optima or plateau should lie there. We can interpret the solution quality regions with low sum nIters values before that peak as easy-to-reach and easy-to-escape, whereas regions around that peak as easy-to-reach and hard-to-escape and regions after that peak as hard-to-reach. The smaller peaks of two instances new with and large with should indicate second local optima or plateau. We propose Algorithm 1 for grouping intervals into nframes regions (frames) that tries to reflect such an interpretation.\nFigure 3 shows identified frames with nframes = 5, which is used in our experiments, on the six provided instances.\nAlgorithm 1 Group intervals into frames\nInput: A: the array of sum nIters values, nintervals: the number of intervals after removing empty ending intervals, nframes: the number of frames Output: E: an nframes-element array, each element contains the index of the last interval of each frame\n\u2211nintervals"}, {"heading": "3.3 Characterize neighborhood behaviours as feature vectors by aggregating collected information into frames", "text": "For the first three observables, rimprove, rworsen and rnothing, we just simply sum the three values nI , nW and nSN for all intervals belonging to the same frame. We then divide them by the sum of niters to get the ratios. For the other two observables aimprove and aworsen, aggregation is more complicated. We can not sum sI or sW values over intervals and get the average due to the fact that their values are incomparable among different intervals. For example, we cannot say that an amount of improvement of 10 in the two intervals [33762, 33621) and [33621, 33482) are equal since hardness of solutions belonging to those is probably not the same. Therefore, we translate them into ranks before doing aggregation. For each interval, neighborhoods are ranked based on the averages of their corresponding sI , sW values. Because ties can happen, e.g., some neighborhoods might never make any improvement in the hard solution quality regions, the average value of stime in the corresponding interval is used for tie-breaking. Since the intervals are fine, the resulting ranked lists are possibly:\n\u2013 noisy: at some intervals, niters of some neighborhoods might be very small, so that their estimated values of aimprove and aworsen might be inaccurate. \u2013 partial: niters of some neighborhoods might be equal to zero at some intervals, i.e., we do not have information of those neighborhoods in those intervals.\nTherefore, we aggregate them using the R package RobustRankAggreg - a robust ranking aggregation method [9] specially designed for similar situations in bioinformatics. Eventually, for each neighborhood, we have a feature vector composing of 150 components, which is a combination of 5 observables, 5 frames and 6 instances."}, {"heading": "3.4 Cluster analysis on neighborhoods", "text": "The first three observables, rimprove, rworsen and rnothing , sum up to one. As a result, their corresponding vector components belong to a special class named compositional data. As explained in [10] \u201csample space for compositional vectors is radically different from the real Euclidean space associated with unconstrained data\u201d, multivariate statistical methodology designed for unconstrained data could not be applied directly. To convert them back to the Euclidean space, we apply the isometric log-ratio transformation proposed in [11]. After the transformation, since the three observables are reduced to two, each feature vector is now 120-dimensional. We can start doing cluster analysis on neighborhoods based on those vectors. Since the number of dimensions (120) is larger than the number of individuals (42), the clustering method High-Dimensional Data Clustering (HDDC) [12]), which is implemented in the R package HDclassif [13], is used for cluster analysis. This method has two desirable properties: the ability of dealing with high-dimensional low-sample data, and the optimal number of clusters automatically decided based on Bayesian Information Criterion. In the end, 42 neighborhoods are grouped into 9 clusters:\n\u2013 Ejection-chain 3, 4, 5; Remove-chain 1, 2, 3, 6, 7, 8; Remove-sub-routewith-cheapest-insertion; \u2013 Swap; Inter-route-two-opt \u2013 Cheapest-insertion 10, 15, 20, 25, 35, 50; Each-sequence-cheapest-insertion\n(2,5), (4,4), (5,2); Remove-chain 4 \u2013 Cheapest-insertion 1, 2, 3, 4, 5 \u2013 Change-swap-location; Merge-route \u2013 Add-sub-route; Convert-to-sub-route \u2013 Ejection-chain 10, 15, 35; Remove-chain 5; Intra-route-two-opt \u2013 Ruin-recreate 2, 3 \u2013 Convert-to-route; Remove-sub-route; Remove-route; Split-to-sub-route\nIt might be interesting to have a look at some of the resulting clusters. The two neighborhoods Merge-route and Remove-route behaves quite differently in the second-half region as shown in Figure 1, and they are indeed clustered into two different groups. By taking a look into the neighborhoods\u2019 implementation, we know that Add-sub-route and Convert-to-sub-route have an extreme behaviour when compared to the others: they will add an additional cost into the current solution and worsen it most of the time (it can also be seen in plots of their observables, which are similar to the ones shown in Figure 1). We can say that the cluster analysis does recognize this extremeness, as the two neighborhoods are grouped into a separated cluster. In addition to reflecting knowledge that can be guessed by looking at the neighborhoods\u2019 implementation, the cluster analysis also does some grouping that is not intuitive from the neighborhoods\u2019 structure, e.g., the grouping of Ejection-chain 10,15,35 and Intra-route-two-opt ."}, {"heading": "4 Experimental results", "text": "Our hypothesis is that the proposed characterization method does reflect neighborhood behaviours on the given set of instances, so that the generated feature vectors should correctly represent the neighborhoods and the clusters we obtained are meaningful. To test this hypothesis, we applied the automated tuning tool SMAC [2] to two configuration scenarios: the first one, dubbed basic, uses the 28 groups of neighborhoods described in Table 1, the second one, dubbed clustered uses the 9 clusters of neighborhoods generated from our characterization method. We carried out 18 runs of SMAC on each scenario. Each one has a budget of 2000 algorithm runs (13.9 CPU days). Due to the large CPU time each SMAC run requires, we use the shared-model-mode offered by SMAC with 10 cores (walltime is 1.39 days), and take the configuration which has the best training performance as the final one. Mean of optimality gaps (in percentage) on the instance set is used as tuning performance measure. Optimality gap on each instance is calculated by:\noptimalityGap = 100 \u2217 (solutionCost\u2212 lowerBound)/lowerBound\nwhere lowerBound is provided by the algorithm\u2019s authors, and is the best solution cost obtained after running the multi-threaded version of the algorithm on\nthe corresponding instance in 6 hours. The best algorithm configuration from each SMAC run is evaluated using test performance, which is the mean of optimality gaps obtained from 30 runs of the configuration on the instance set (5 runs/instance). Box-plots of the 18 SMAC runs on each scenario are shown in Figure 4, in which the clustered scenario offers advantage over the basic scenario. A paired t-test is conducted and gives a p\u2212value of 0.009258918, indicating statistical significance.\nIn the hyper-heuristic community, in particular the Selection Hyper-heuristic class, in which the aim is to manage a set of low-level heuristics during the search by selecting one of them at each iteration, the Simple Random (SR) heuristic selection mechanism is often used as a baseline [14]. In our setting, SR is equivalent to the parameter configuration that has identical weights for all neighborhoods. It will be interesting to compare SR with the resulting configurations obtained from the off-line tuning: for each scenario, the 18 best tuned configurations are taken and the neighborhood weights inside them are set to identical. Their test performance values are shown as basic with identical weights and clustered with identical weights in Figure 4. The horizontal line represents test performance of the algorithm configuration in which neighborhood weights are identical and laList and itIW are set to values recommended by the algorithm\u2019s authors. This configuration is also used as the default configuration for all SMAC runs mentioned above. We can see that the SR versions in both scenarios give worst test performance. A paired t-test is conducted for each scenario:\n\u2013 basic and basic with identical weights : p-value = 0.07464\n\u2013 clustered and clustered with identical weights : p-value = 0.000459\nThe p-value from the second t-test indicates that the neighborhoods\u2019 weights do have influence on the algorithm performance. Those tests also reflect the hardness of tuning those weights (as the basic tuning fails to show significant improvement over the identical-weight configurations), and the advantage of clustered over basic."}, {"heading": "5 Conclusion and future work", "text": "In this paper, we have proposed a systematic method to characterize neighborhood behaviours in a multi-neighborhood local search framework, where the probability of choosing a neighborhood at each iteration is chosen in an off-line manner. The characterization is based on the probabilities that a neighborhood will improve, worsen or do nothing on a solution, on the magnitudes of its improvement and worsening, and on its running time. We have observed that these characteristics change according to hardness of different regions in solution quality space. As a result, we design our method such that it tries to detect these regions based on collected information and represent neighborhood behaviours on them as feature vectors. Cluster analysis is then applied to form groups of similar neighborhoods. A tuning experiment with the automated algorithm configuration tool SMAC [2] shows that using these clusters gives a statistically significant improvement on test performances of the obtained algorithm configurations over the non-clustered version. It verifies the hypothesis that our characterization method is able to correctly reflect neighborhood behaviours on the given instance set. Since the information used in this method does not depend\non a specific problem, the characterization and clustering procedure potentially can be applied in similar contexts. A first version of our method\u2019s implementation has been available as a toolbox, and can be obtained by sending a request to the corresponding author. The toolbox receives log files containing necessary information collected during algorithm runs as input, and returns results of the cluster analysis, as well as box-plots and graphs for the visualization of observables and solution quality regions.\nFor future work, a multi-level tuning might be interesting. Firstly, a postanalysis on the importance of each cluster using fANOVA [15], which is an efficient approach to \u201cquantify the effect of algorithm parameters\u201d, can be applied. Then finer tuning on neighborhoods that belong to the most important clusters can be done. In addition, since our current method are only limited to a small number of instances, we are seeking for the possibility of an extension to a large set of instances. We might want to exploit problem-specific expert knowledge, e.g., instance features, in such a case."}, {"heading": "Acknowledgement", "text": "This work is funded by COMEX (Project P7/36), a BELSPO/IAP Programme. We would like to thank Tu\u0301lio Toffolo for his great support during the course of this research, Thomas Stu\u0308tzle and Jan Verwaeren for their valuable remarks. The computational resources and services used in this work were provided by the VSC (Flemish Supercomputer Center), funded by the Hercules Foundation and the Flemish Government department EWI."}], "references": [{"title": "Automated algorithm configuration and parameter tuning", "author": ["Holger H Hoos"], "venue": "In Autonomous search,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2012}, {"title": "Sequential model-based optimization for general algorithm configuration", "author": ["Frank Hutter", "Holger H Hoos", "Kevin Leyton-Brown"], "venue": "In Learning and Intelligent Optimization,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2011}, {"title": "The irace package, iterated race for automatic algorithm configuration", "author": ["Manuel L\u00f3pez-Ib\u00e1nez", "J\u00e9r\u00e9mie Dubois-Lacoste", "Thomas St\u00fctzle", "Mauro Birattari"], "venue": "Technical report, Citeseer,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2011}, {"title": "The winning approach for the verolog solver challenge 2014: the swap-body vehicle routing problem", "author": ["Tony Wauters", "T\u00falio Toffolo", "Jan Christiaens", "Sam Van Malderen"], "venue": "Proceedings of ORBEL29,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2015}, {"title": "Verolog solver challenge 2014\u2013vsc2014 problem description. VeRoLog (EURO Working Group on Vehicle Routing and Logistics Optimization) and PTV", "author": ["W Heid", "G Hasle", "D Vigo"], "venue": null, "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2014}, {"title": "Iterated local search: Framework and applications", "author": ["Helena R Louren\u00e7o", "Olivier C Martin", "Thomas St\u00fctzle"], "venue": "In Handbook of Metaheuristics,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2010}, {"title": "A late acceptance strategy in hill-climbing for exam timetabling problems", "author": ["Edmund K Burke", "Yuri Bykov"], "venue": "In PATAT 2008 Conference, Montreal, Canada,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2008}, {"title": "Robust rank aggregation for gene list integration and meta-analysis", "author": ["Raivo Kolde", "Sven Laur", "Priit Adler", "Jaak Vilo"], "venue": null, "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2012}, {"title": "A concise guide to compositional data analysis", "author": ["John Aitchison"], "venue": "In 2nd Compositional Data Analysis Workshop,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2005}, {"title": "Isometric logratio transformations for compositional data analysis", "author": ["Juan Jos\u00e9 Egozcue", "Vera Pawlowsky-Glahn", "Gl\u00f2ria Mateu-Figueras", "Carles Barcelo-Vidal"], "venue": "Mathematical Geology,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2003}, {"title": "High-dimensional data clustering", "author": ["Charles Bouveyron", "St\u00e9phane Girard", "Cordelia Schmid"], "venue": "Computational Statistics & Data Analysis,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2007}, {"title": "Hdclassif: An r package for model-based clustering and discriminant analysis of high-dimensional data", "author": ["Laurent Berg\u00e9", "Charles Bouveyron", "St\u00e9phane Girard"], "venue": null, "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2012}, {"title": "Hyper-heuristics: A survey of the state of the art", "author": ["Edmund K Burke", "Michel Gendreau", "Matthew Hyde", "Graham Kendall", "Gabriela Ochoa", "Ender \u00d6zcan", "Rong Qu"], "venue": "Journal of the Operational Research Society,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2013}, {"title": "An efficient approach for assessing hyperparameter importance", "author": ["Frank Hutter", "Holger Hoos", "Kevin Leyton-Brown"], "venue": "In Proceedings of the 31st International Conference on Machine Learning", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2014}], "referenceMentions": [{"referenceID": 0, "context": "For the last fifteen years, automated algorithm configuration has been extensively studied [1].", "startOffset": 91, "endOffset": 94}, {"referenceID": 1, "context": "General-purpose automated algorithm configuration tools such as SMAC [2] and irace [3] have been successfully applied in several studies.", "startOffset": 69, "endOffset": 72}, {"referenceID": 2, "context": "General-purpose automated algorithm configuration tools such as SMAC [2] and irace [3] have been successfully applied in several studies.", "startOffset": 83, "endOffset": 86}, {"referenceID": 3, "context": "In this work, we consider the parameter tuning problem of a multi-neighborhood local search algorithm [4], which consists of a large number of neighborhoods.", "startOffset": 102, "endOffset": 105}, {"referenceID": 4, "context": "The algorithm is the winner of the Verolog Solver Challenge 2014 [5].", "startOffset": 65, "endOffset": 68}, {"referenceID": 0, "context": "At each iteration, a neighbor solution is generated by a randomly chosen neighborhood with a probability defined by a weight value in the range of [0,1].", "startOffset": 147, "endOffset": 152}, {"referenceID": 3, "context": "The algorithm considered in this work, which was developed by CODeS group\u2019s members of the University of Leuven (Belgium) [4], tackles the Swap-body Vehicle Routing problem.", "startOffset": 122, "endOffset": 125}, {"referenceID": 5, "context": "It is an iterated local search [6] algorithm that uses late acceptance hill climbing [7] as the local search component.", "startOffset": 31, "endOffset": 34}, {"referenceID": 6, "context": "It is an iterated local search [6] algorithm that uses late acceptance hill climbing [7] as the local search component.", "startOffset": 85, "endOffset": 88}, {"referenceID": 7, "context": "Therefore, we aggregate them using the R package RobustRankAggreg - a robust ranking aggregation method [9] specially designed for similar situations in bioinformatics.", "startOffset": 104, "endOffset": 107}, {"referenceID": 8, "context": "As explained in [10] \u201csample space for compositional vectors is radically different from the real Euclidean space associated with unconstrained data\u201d, multivariate statistical methodology designed for unconstrained data could not be applied directly.", "startOffset": 16, "endOffset": 20}, {"referenceID": 9, "context": "To convert them back to the Euclidean space, we apply the isometric log-ratio transformation proposed in [11].", "startOffset": 105, "endOffset": 109}, {"referenceID": 10, "context": "Since the number of dimensions (120) is larger than the number of individuals (42), the clustering method High-Dimensional Data Clustering (HDDC) [12]), which is implemented in the R package HDclassif [13], is used for cluster analysis.", "startOffset": 146, "endOffset": 150}, {"referenceID": 11, "context": "Since the number of dimensions (120) is larger than the number of individuals (42), the clustering method High-Dimensional Data Clustering (HDDC) [12]), which is implemented in the R package HDclassif [13], is used for cluster analysis.", "startOffset": 201, "endOffset": 205}, {"referenceID": 1, "context": "To test this hypothesis, we applied the automated tuning tool SMAC [2] to two configuration scenarios: the first one, dubbed basic, uses the 28 groups of neighborhoods described in Table 1, the second one, dubbed clustered uses the 9 clusters of neighborhoods generated from our characterization method.", "startOffset": 67, "endOffset": 70}, {"referenceID": 12, "context": "In the hyper-heuristic community, in particular the Selection Hyper-heuristic class, in which the aim is to manage a set of low-level heuristics during the search by selecting one of them at each iteration, the Simple Random (SR) heuristic selection mechanism is often used as a baseline [14].", "startOffset": 288, "endOffset": 292}, {"referenceID": 1, "context": "A tuning experiment with the automated algorithm configuration tool SMAC [2] shows that using these clusters gives a statistically significant improvement on test performances of the obtained algorithm configurations over the non-clustered version.", "startOffset": 73, "endOffset": 76}, {"referenceID": 13, "context": "Firstly, a postanalysis on the importance of each cluster using fANOVA [15], which is an efficient approach to \u201cquantify the effect of algorithm parameters\u201d, can be applied.", "startOffset": 71, "endOffset": 75}], "year": 2016, "abstractText": "We consider a multi-neighborhood local search algorithm with a large number of possible neighborhoods. Each neighborhood is accompanied by a weight value which represents the probability of being chosen at each iteration. These weights are fixed before the algorithm runs, and are considered as parameters of the algorithm. Given a set of instances, off-line tuning of the algorithm\u2019s parameters can be done by automated algorithm configuration tools (e.g., SMAC). However, the large number of neighborhoods can make the tuning expensive and difficult even when the number of parameters has been reduced by some intuition. In this work, we propose a systematic method to characterize each neighborhood\u2019s behaviours, representing them as a feature vector, and using cluster analysis to form similar groups of neighborhoods. The novelty of our characterization method is the ability of reflecting changes of behaviours according to hardness of different solution quality regions. We show that using neighborhood clusters instead of individual neighborhoods helps to reduce the parameter configuration space without misleading the search of the tuning procedure. Moreover, this method is problem-independent and potentially can be applied in similar contexts.", "creator": "LaTeX with hyperref package"}}}