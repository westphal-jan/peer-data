{"id": "1603.06478", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "21-Mar-2016", "title": "Hard-Clustering with Gaussian Mixture Models", "abstract": "Training the parameters of statistical models to describe a given data set is a central task in the field of data mining and machine learning. A very popular and powerful way of parameter estimation is the method of maximum likelihood estimation (MLE). Among the most widely used families of statistical models are mixture models, especially, mixtures of Gaussian distributions, Gaussian Gaussian distribution, and nonparametric general linear distributions (GRMs).\n\n\n\nThe data mining method can easily be used in a wide range of fields including data mining, machine learning, machine learning and machine learning. Some of the most widely used models include a large number of nonlinear distributions in the form of generalized latent variables.\nHowever, for general statistical models, the approach is particularly suited to modeling large-scale data sets such as the latent variables. There is also a large variety of techniques in which statistical models can be used. For example, for example, using generalized latent variables, we can use convolutional linear distributions as described in the paper, a generalized latent variable that includes the latent variables and convolutional linear distributions as described in the paper.\nIn general, the model for a typical case of data mining is a model for the following parameters: a matrix with parameters that can be used to predict the model. The most commonly used parameters can be used to evaluate the model. The most commonly used parameters can be used to predict the model. The most commonly used parameters can be used to predict the model. The most commonly used parameters can be used to predict the model. The most commonly used parameters can be used to predict the model. The most commonly used parameters can be used to predict the model. The most commonly used parameters can be used to predict the model. The most commonly used parameters can be used to predict the model. The most commonly used parameters can be used to predict the model. The most commonly used parameters can be used to predict the model. The most commonly used parameters can be used to predict the model. The most commonly used parameters can be used to predict the model. The most commonly used parameters can be used to predict the model. The most commonly used parameters can be used to predict the model. The most commonly used parameters can be used to predict the model. The most commonly used parameters can be used to predict the model. The most commonly used parameters can be used to predict the model. The most commonly used parameters can be used to predict the model. The most commonly used parameters can be used to predict the model. The most commonly used", "histories": [["v1", "Mon, 21 Mar 2016 16:02:27 GMT  (14kb)", "http://arxiv.org/abs/1603.06478v1", null]], "reviews": [], "SUBJECTS": "cs.LG cs.DS", "authors": ["johannes bl\\\"omer", "sascha brauer", "kathrin bujna"], "accepted": false, "id": "1603.06478"}, "pdf": {"name": "1603.06478.pdf", "metadata": {"source": "CRF", "title": "Hard-Clustering with Gaussian Mixture Models", "authors": ["Johannes Bl\u00f6mer", "Sascha Brauer", "Kathrin Bujna"], "emails": [], "sections": [{"heading": null, "text": "ar X\niv :1\n60 3.\n06 47\n8v 1\n[ cs\n.L G\n] 2\n1 M\nar 2\n01 6\nHard-Clustering with Gaussian Mixture Models\nJohannes Blo\u0308mer, Sascha Brauer, and Kathrin Bujna\nDepartment of Computer Science\nPaderborn University\n33102 Paderborn, Germany\nMarch 22, 2016\nTraining the parameters of statistical models to describe a given data set is a central task in the field of data mining and machine learning. A very popular and powerful way of parameter estimation is the method of maximum likelihood estimation (MLE). Among the most widely used families of statistical models are mixture models, especially, mixtures of Gaussian distributions.\nA popular hard-clustering variant of the MLE problem is the so-called completedata maximum likelihood estimation (CMLE) method. The standard approach to solve the CMLE problem is the Classification-Expectation-Maximization (CEM) algorithm [CG92]. Unfortunately, it is only guaranteed that the algorithm converges to some (possibly arbitrarily poor) stationary point of the objective function.\nIn this paper, we present two algorithms for a restricted version of the CMLE problem. That is, our algorithms approximate reasonable solutions to the CMLE problem which satisfy certain natural properties. Moreover, they compute solutions whose cost (i.e. complete-data log-likelihood values) are at most a factor (1 + \u03b5) worse than the cost of the solutions that we search for. Note the CMLE problem in its most general, i.e. unrestricted, form is not well defined and allows for trivial optimal solutions that can be thought of as degenerated solutions.\n1 Preliminaries\nGiven set of observations, the objective of the CMLE problem is to find a Gaussian mixture model and a hard clustering with maximum complete-data likelihood. In this section, we will first describe and define this objective function. Then, we will present an alternating optimization scheme for this problem. However, the problem is not well-defined. Hence, we will restrict the problem to reasonable instances and solutions.\n1.1 Complete-Data Log-Likelihood\nLet X \u2282 Rd be a finite set of observations. Given a spherical Gaussian distribution Nd(\u00b5, \u03c3), the likelihood that all x \u2208 X have been drawn according to Nd(\u00b5, \u03c3) is given by\n\u220f\nx\u2208X\nNd(x|\u00b5, \u03c3) ,\nassuming that the observations have been drawn independently at random.\nDefinition 1. Given a finite set X \u2282 Rd and a spherical Gaussian distribution with mean \u00b5 \u2208 Rd and variance \u03c32 \u2208 R, let\nLX(\u00b5, \u03c3 2) := \u2212 ln\n(\n\u220f\nx\u2208X\np(x|\u00b5, \u03c32)\n)\n= |X|d\n2 ln(2\u03c0\u03c32k) +\n1\n2\u03c32k\n\u2211\nx\u2208X\n\u2016x\u2212 \u00b5k\u2016 2 .\nWe denote the minimal value by OPT (X, 1) = min(\u00b5,\u03c32)LX(\u00b5, \u03c3 2).\nNow consider a Gaussian mixture model (GMM) given by parameters \u03b8 = {(wk, \u00b5k, \u03c3 2 k)} K k=1.\nDrawing an observation xn according to a GMM corresponds to a two-step process:\n1. Draw a component zn \u2208 [K] with probability p(zn = k|\u03b8) = wk.\n2. Draw an observation xn \u2208 X according to Nd(\u00b5zn , \u03c3zn).\nNote that the assignment zn \u2208 [K] is a (latent) random variable in this two-step process. With the help of this random variable, we can compute the likelihood that observation x \u2208 X has been generated by the k-th component of the GMM, i.e.\np(xn, zn = k|\u03b8) = p(zn = k|\u03b8) \u00b7 p(xn|zn = k, \u03b8) = wk \u00b7 Nd(x|\u00b5k, \u03c3k) .\nSince xn and zn completely describe the two-step process, the likelihood p(xn, zn|\u03b8) is also called complete-data likelihood, while p(xn|\u03b8) = \u2211K zn=1 p(xn, zn|\u03b8) is refered to as (marginal) likelihood.\nAssume, we are given a set of observations X = {xn} N n=1 and assignments {zn} N n=1. Then, the likelihood that all observations have been drawn according to a GMM \u03b8 and that each xn has been generated by the zn-th component, is given by\nN \u220f\nn=1\np(xn, zn|\u03b8) = N \u220f\nn=1\nwzn \u00b7 Nd(xn|\u00b5zn , \u03c3zn) , (1)\nassuming that the observations have been drawn inpendently at random. Note that the assignments {zn} N n=1 define a partition C = \u222a\u0307 K k=1Ck via xn \u2208 Ck iff zn = k. Hence, we can also rewrite Equation (1) as\nK \u220f\nk=1\n\u220f\nxn\u2208Ck\np(xn, zn = k|\u03b8) = K \u220f\nk=1\n\u220f\nxn\u2208Ck\nwk \u00b7 Nd(xn|\u00b5k, \u03c3k) .\nBy taking (negative) logarithm of this expression, we obtain\n\u2212 log\n\n\nK \u220f\nk=1\n\u220f\nxn\u2208Ck\np(xn, zn = k|\u03b8)\n\n\n= K \u2211\nk=1\n\u2211\nxn\u2208Ck\n(ln(wk) + ln (Nd(xn|\u00b5k,\u03a3k))\n=\nK \u2211\nk=1\nLCk(\u00b5k, \u03c3 2 k)\u2212 ln(wk) \u00b7 |Ck| .\nDefinition 2. Given a finite set X \u2282 Rd, a partition C = {C1, . . . , CK} of X, and a mixture of spherical Gaussians with parameters \u03b8 = {(wk, \u00b5k, \u03c3 2 k)} K k=1, we call\nLX(\u03b8, C) := K \u2211\nk=1\nLCk(\u00b5k, \u03c3 2 k)\u2212 ln(wk) \u00b7 |Ck|\nthe complete-data negative log-likelihood.\nNote that a solution maximizing the complete-data likelihood also minimizes the completedata negative log-likelihood, and vice versa. Therefore, we define the complete-cata maximum likelihood estimation (CMLE) problem as follows.\nProblem 3 (CMLE). Given a finite set X \u2282 Rd and an integer K \u2208 N, find a partition C = {C1, . . . , CK} of X and a mixture of spherical Gaussians with parameters \u03b8 = {(wk, \u00b5k, \u03c3 2 k)} K k=1 minimizing LX(\u03b8, C). We denote the minimal value by OPT (X,K). For a fixed model \u03b8, we let LX(\u03b8) = minC LX(\u03b8, C). Analogously, for a fixed clustering C, we let LX(C) = min\u03b8 LX(\u03b8, C).\nDefinition 4. Given parameters (wk, \u00b5k, \u03c3 2 k) and a cluster Ck \u2286 X, we let\nLx(wk, \u00b5k, \u03c3 2 k) :=\nd 2 ln(2\u03c0\u03c32k) + 1\n2\u03c32k \u2016x\u2212 \u00b5k\u2016\n2 \u2212 ln(wk),\nand LCk(wk, \u00b5k, \u03c3 2 k) := \u2211\nx\u2208Ck\nLx(wk, \u00b5k, \u03c3 2 k) .\nRemark 5. For all partitions C = {C1, . . . , CK}, we have\nLX(C) = K \u2211\nk=1\nOPT (Ck, 1)\u2212 ln\n(\n|Ck| |X|\n)\n\u00b7 |Ck| .\nFor all \u03b8 = {(w1, \u00b51, \u03c3 2 1), . . . , (wK , \u00b5K , \u03c3 2 K)}, we have\nLX(\u03b8) = N \u2211\nn=1\nargmink\u2208[K]{Lx(wk, \u00b5k, \u03c3 2 k)} .\n1.2 Alternating Optimization Scheme (CEM algorithm)\nAn alternating optimization algorithm for this problem is given by the following first order optimality conditions. Fixing the partition C = {Ck} K k=1, the optimal mixture of spherical Gaussians is given by \u03b8 = {(wk, \u00b5k, \u03c3 2 k)} K k=1 with\nwk = |Ck|\n|X| , \u00b5k =\n1\n|Ck|\n\u2211\nxn\u2208Ck\nxn , \u03c3 2 k =\n1\nd|Ck|\n\u2211\nxn\u2208Ck\n\u2016xn \u2212 \u00b5k\u2016 2 .\nFixing the Gaussian mixture model \u03b8 = {(wk, \u00b5k, \u03c3 2 k)} K k=1, the optimal partition C = {Ck} K k=1 is given by assigning each point to its most likely component, i.e.\nxn \u2208 Ck \u21d4 k = argmaxl\u2208[K] p(zn = l|xn, \u03b8) ,\nwhere\np(zn = k|xn, \u03b8) = wkN (xn|\u00b5k, \u03c3\n2 k)\n\u2211K l=1wlN (xn|\u00b5l, \u03c3 2 l )\n,\nwhich is the posterior probability that xn has been generated by the k-th component of the given mixture.\nIf we repeatedly compute these update formulas, the solution converges to a local extremum or a saddlepoint of the likelihood function.\nA proof of the correctenss of these update formulas (which we omit here) uses the following lemma.\nLemma 6. Let X \u2282 Rd be a finite set. Define\n\u00b5(X) = 1\n|X|\n\u2211\nx\u2208X\nx .\nThen, for all y \u2208 Rd\n\u2211\nx\u2208X\n\u2016x\u2212 y\u20162 = \u2211\nx\u2208X\n\u2016x\u2212 \u00b5(X)\u20162 + |X| \u00b7 \u2016y \u2212 \u00b5(X)\u20162 .\nIn particular, \u00b5(X) = argminy\u2208Rd \u2211 x\u2208X\u2016x\u2212 y\u2016 2.\nNote that an optimal CMLE solution is not changed by this algorithm. Hence, an optimal CMLE solution is completely defined by a partition or a Gaussian mixture model. Similarly, if we refer to a partition or a Gaussian mixture as a CMLE solution we assume that the missing parameters are as defined by the update formulas given above, respectively.\n1.3 Well-Defined Instances\nUnfortunately, the CMLE problem is not well defined in this form. For example, you could choose C1 = {x} and \u00b51 = x for some x \u2208 X. Then, as \u03c31 \u2192 0 we get that LK(X) \u2192 \u2212\u221e. Consequently, we impose the following restrictions on instances.\nDefinition 7. We call X = \u02d9 \u22c3K\nk=1Ck a well-defined partition if\n1. for all k \u2208 [K] : |Ck| \u2265 2.\nWe call X itself a well-defined instance if\n2. \u2200x, y \u2208 X,x 6= y : \u2016x\u2212 y\u20162 \u2265 4d\u03c0 .\nWe denote X = \u02d9 \u22c3K\nk=1Ck as a well-defined solution if X is a well-defined instance and {Ck} K k=1\nis a well-defined partition.\nIn the following, we prove that, with these restrictions, the CMLE problem is well defined. That is, the minimum in Problem 3 is well defined (LK(X) > \u2212\u221e). Moreover, we will see (Lemma 9) that for the optimal solution we have \u03c32k \u2265 1 2\u03c0 or\n2\u03c0\u03c32k \u2265 1 for k \u2208 [K]. (2)\nFirst of all, note that the sum of squared distances between the points in X and the mean \u00b5(X) can be rewritten using pairwise distances (which are lower bounded in Restriction 2).\nLemma 8. Let X \u2282 Rd be a finite set and \u00b5(X) := 1|X| \u2211 x\u2208X its mean, then\n\u2211\nx\u2208X\n\u2016x\u2212 \u00b5(X)\u20162 = 1\n2|X|\n\u2211\nx\u2208X\n\u2211\ny\u2208X\n\u2016x\u2212 y\u20162.\nProof.\n\u2211\nx\u2208X\n\u2211\ny\u2208X\n\u2016x\u2212 y\u20162 = \u2211\nx\u2208X\n\u2211\ny\u2208X\n\u3008x\u2212 y, x\u2212 y\u3009\n= \u2211\nx\u2208X\n\u2211\ny\u2208X\n(\u3008x, x\u3009+ \u3008y, y\u3009 \u2212 2 \u3008x, y\u3009\n= 2|X| \u2211\nx\u2208X\n\u3008x, x\u3009 \u2212 2 \u2211\nx\u2208X\n\u2211\ny\u2208X\n\u3008x, y\u3009\n= 2|X| \u2211\nx\u2208X\n\u3008x, x\u3009 \u2212 2|X| \u2211\nx\u2208X\n\u3008x, \u00b5(X)\u3009\n= 2|X| \u2211\nx\u2208X\n\u3008x, x\u2212 \u00b5(X)\u3009\n= 2|X| \u2211\nx\u2208X\n\u3008x\u2212 \u00b5(X), x \u2212 \u00b5(X)\u3009 (using |X| \u2211\nx\u2208X \u3008\u00b5(X), x\u2212 \u00b5(X)\u3009 = 0)\n= 2|X| \u2211\nx\u2208X\n\u2016x\u2212 \u00b5(X)\u20162.\nNow using the restriction on the minimum pairwise difference between points (Restriction 2) and on the minimum number of points (Restriction 1) in a cluster, we can lower bound the variance of each cluster. This directly yields Equation (2) and our claim that the problem is well-defined under the restrictions given in Definition 7.\nLemma 9. Let Y be a subset of a set X that satisfies Restriction 2 from Definition 7 and that contains at least two different elements. Then,\n\u03c3(Y )2 = 1\n|Y |d\n\u2211\ny\u2208Y\n\u2016y \u2212 \u00b5(Y )\u20162 \u2265 1\n2\u03c0 .\nProof.\n\u03c3(Y )2 = 1\n|Y |d\n\u2211\ny\u2208Y\n\u2016y \u2212 \u00b5(Y )\u20162\n= 1\n2|Y |2d\n\u2211\nx\u2208Y\n\u2211\ny\u2208Y\n\u2016x\u2212 y\u20162 (using Lemma 8)\n\u2265 1\n2|Y |2d\n(\n|Y |\n2\n)\nmin x,y\u2208Y,x 6=y\n\u2016x\u2212 y\u20162\n\u2265 1\n8d min x,y\u2208Y,x 6=y \u2016x\u2212 y\u20162\n\u2265 1\n2\u03c0 (using Restriction 2)\nThroughout the rest of this paper, we will restrict the search space of CMLE to well-defined solutions. In particular, we only consider the optimal solution among all well-defined solutions.\n1.4 Well-Balanced Instances\nA central idea behind the algorithms that we present in this paper is that we do not allow somewhat degenerate instances. This means that we can find a function f in the number of clusters that can be used to lower bound the number of points in a cluster and a function g that can be used to lower bound the costs OPT (Ck, 1) of optimal clusters Ck. Definition 10 (well-balanced). Let f, g : N \u2192 R. We denote a partition X = \u02d9 \u22c3K\nk=1Ck as f -balanced if for all k \u2208 [K]\n|Ck| \u2265 |X|\nf(K) .\nFurthermore, we denote the partition as an (f, g)-balanced CMLE solution if it is f -balanced and additionally for all k \u2208 [K]\nOPT (Ck, 1) \u2265 1\ng(K) \u00b7\nK \u2211\nk=1\nOPT (Ck, 1) .\nDefinition 11. Given a finite set X \u2282 Rd and K \u2208 N, we let\nOPTdiam(X,K) = min {C1,...,CK},\n\u222a\u0307Kk=1Ck=X\nmax k\u2208[K] max x,y\u2208Ck \u2016x\u2212 y\u2016 .\nLemma 12 (From f -balanced to (f, g)-balanced). An f -balanced solution X = \u02d9 \u22c3K\nk=1Ck is also an (f,\u0393 \u00b7 f)-balanced CMLE solution, where \u0393 \u2264 2 \u00b7 ln (32\u03c0 \u00b7 OPTdiam(X,K)) + ln(K) + 1.\nProof.\nOPT (Ck, 1) \u2265 |Ck|d\n2 \u2265\n1\nf(K)\n|X|d\n2 (due to Lemma 20 and f balanced)\n\u2265 1\nf(K) \u00b7 \u0393 LK(X) (due to Lem. 21)\n\u2265 1\nf(K) \u00b7 \u0393\nK \u2211\nk=1\nOPT (Ck, 1) .\n2 Main Results (Theorems 13 and 15)\nTheorem 13. Let X \u2282 Rd, K \u2208 N and \u03b4, \u03b5 \u2208 [0, 1]. If X has an (f, g)-balanced optimal CMLE solution, then there exists an algorithm which computes a mixture of K spherical Gaussians \u03b8 = {(wk, \u00b5k, \u03c3 2 k)} K k=1, such that\nPr [LX(\u03b8) \u2264 (1 + \u03b5)OPT (X,K)] \u2265 1\u2212 \u03b4 .\nThe runtime of the algorithm is bounded by\n|X| \u00b7K \u00b7 log(\u0393) \u00b7 log(g(K)) \u00b7 2 O\u0303 ( f(K) \u03b5\u03b4 )\nwhere \u0393 \u2264 2 \u00b7 ln (32\u03c0 \u00b7 OPTdiam(X,K)) + ln(K) + 1.\nCorollary 14. Let X \u2282 Rd, K \u2208 N and \u03b4, \u03b5 \u2208 [0, 1]. If X has an f -balanced optimal CMLE solution, then there exists an algorithm which computes a mixture of K spherical Gaussians \u03b8, such that\nPr [LX(\u03b8) \u2264 (1 + \u03b5)OPT (X,K)] \u2265 1\u2212 \u03b4 .\nThe runtime of the algorithm is bounded by\n|X| \u00b7K \u00b7 log(\u0393)2 \u00b7 2 O\u0303 ( f(K) \u03b5\u03b4 )\nwhere \u0393 \u2264 2 \u00b7 ln (32\u03c0 \u00b7 OPTdiam(X,K)) + ln(K) + 1.\nTheorem 15. Let X \u2282 Rd, K \u2208 N, and \u03b4, \u03b5 > 0. Let C = \u02d9 \u22c3K\nk=1Ck be a well-defined solution for the CMLE problem. There is an algorithm that computes a mixture of K spherical Gaussians \u03b8, such that\nPr [LX(\u03b8) \u2264 (1 + \u03b5)LX(C)] \u2265 1\u2212 \u03b4 .\nThe running time of the algorithm is bounded by\n|X| d log\n(\n1 \u03b4\n)\n2 O ( K \u03b5 \u00b7log ( K \u03b52 )) ( log(log(\u22062)) + 1 )K (log(f(K)))K ,\nwhere \u22062 = maxx,y\u2208X{\u2016x\u2212 y\u2016 2}.\n3 Proof of Theorem 13\nIn the following we prove Theorem 13.\n\u2022 In Section 3.1 we show that, if the parameters of a CMLE solution are sufficently close to those of an optimal CMLE solution, then its complete-data log-likelihood is close to that of the optimal CMLE solution. In Sections 3.2 and 3.3 we then show how to obtain such parameter estimates.\n\u2022 In Section 3.2 we deal with the problem of estimating the means. We use the superset sampling technique introduced by [IKI94] to compute a set of candidate means which contains a good candidate, i.e. a good estimation to the mean parameters of an optimal solution.\n\u2022 In Section 3.3 we use a grid search to obtain estimates of the weights and variances. The core idea is to simply test all solutions lying on a specific grid in the search space. By choosing a grid that is dense enough, we ensure that there are solutions on the grid which are sufficiently close to the parameters that we search for.\n3.1 Estimate the Costs of Parameter Estimates\nFor an optimal (f, g)-balanced CMLE solutions, we can estimate the parameters of the the respective optimal Gaussian mixture model and the likelihood of the optimal clusters. We can show that the CMLE solution determined by these parameter estimates yields an approximation with respect to the complete data log-likelihood.\nTheorem 16. Let X \u2282 Rd, K \u2208 N and \u03b5 > 0. Assume X has an f -balanced optimal CMLE solution X = \u02d9 \u22c3K\nk=1Ck and let (\u00b5\u03031, . . . , \u00b5\u0303K) such that for all k \u2208 [K]\n\u2016\u00b5\u0303k \u2212 \u00b5(Ck)\u2016 2 \u2264\n\u03b5\n|Ck|\n\u2211\nx\u2208Ck\n\u2016x\u2212 \u00b5(Ck)\u2016 2 .\nLet (n1, . . . , nK), such that for all k \u2208 [K]\n|Ck| \u2264 nk \u2264 (1 + \u03b5)|Ck| . (3)\nand ~\u0303\u03c3 = (\u03c3\u030321 , . . . , \u03c3\u0303 2 K) \u2208 R K , such that for all k \u2208 [K] it holds\n\u03c3\u03032k \u2265 \u03c3 2 k (4)\nand\nln(\u03c3\u03032k)\u2212 ln(\u03c3 2 k) \u2264\n( (1 + \u03b5)2 \u2212 1 ) 2\n|Ck|d OPT (Ck, 1) . (5)\nDefine \u03b8\u0303 = {(w\u0303k, \u00b5\u0303k, \u03c3\u0303 2 k)}k=1,...,K , where w\u0303k = nk \u2211K\nl=1 nl . Then,\nLX(\u03b8\u0303) \u2264 (1 + \u03b5) 4OPT (X,K).\nProof. Using that |Cl| \u2264 nl \u2264 (1+\u03b5)|Cl| for all l = 1, . . . ,K, we obtain w\u0303k \u2265 1 (1+\u03b5) \u00b7 |Ck | |X| . Hence,\n\u2212 ln(w\u0303k) \u00b7 |Ck| \u2264 \u2212 ln\n(\n1 (1 + \u03b5) \u00b7 |Ck| |X|\n)\n|Ck| (by Equation (3))\n\u2264 ln(1 + \u03b5)|Ck| \u2212 ln\n(\n|Ck| |X|\n)\n\u00b7 |Ck|\n\u2264 \u03b5|Ck| \u2212 ln\n(\n|Ck| |X|\n)\n\u00b7 |Ck| (since ln(1 + \u03b5) \u2264 \u03b5)\n\u2264 2\u03b5\nd OPT (Ck, 1)\u2212 ln\n(\n|Ck| |X|\n)\n\u00b7 |Ck| (since OPT (Ck, 1) \u2265 |Ck |\u00b7d\n2 )\nFurthermore, observe that\nLCk(\u00b5\u0303k, \u03c3\u0303k) = |Ck|d\n2 ln(2\u03c0\u03c3\u03032k) +\n1\n2\u03c3\u03032k\n\u2211\nx\u2208Ck\n\u2016x\u2212 \u00b5\u0303k\u2016 2\n(4) \u2264 |Ck|d\n2 ln(2\u03c0\u03c3\u03032k) +\n1\n2\u03c32k\n\u2211\nx\u2208Ck\n\u2016x\u2212 \u00b5\u0303k\u2016 2\n\u2264 |Ck|d\n2 ln(2\u03c0\u03c3\u03032k) +\n1\n2\u03c32k (1 + \u03b5)\n\u2211\nx\u2208Ck\n\u2016x\u2212 \u00b5k\u2016 2 (By Lemma 6 and property of \u00b5\u0303k)\n= |Ck|d\n2 ln(2\u03c0\u03c3\u03032k) + (1 + \u03b5)\n|Ck|d\n2 (By def. of \u00b5k)\n= |Ck|d\n2 (ln(2\u03c0) + ln(\u03c3\u03032k)) + (1 + \u03b5)\n|Ck|d\n2 (5) = |Ck|d\n2\n(\nln(2\u03c0) + ( (1 + \u03b5)2 \u2212 1 ) 2\n|Ck|d OPT (Ck, 1) + ln(\u03c3\n2 k)\n)\n+ (1 + \u03b5) |Ck|d\n2\n= |Ck|d\n2 ln(2\u03c0\u03c32k) + (1 + \u03b5)\n|Ck|d\n2 +\n( (1 + \u03b5)2 \u2212 1 )\nOPT (Ck, 1)\n\u2264 (1 + \u03b5)OPT (Ck, 1) + ( (1 + \u03b5)2 \u2212 1 ) OPT (Ck, 1) \u2264 ( (1 + \u03b5)2 + \u03b5 )\nOPT (Ck, 1)\n\u2264 (1 + \u03b5)3OPT (Ck, 1)\nOverall, we have\nLX(\u03b8\u0303) = K \u2211\nk=1\nLCk(\u00b5k, \u03c3 2 k)\u2212 ln(wk) \u00b7 |Ck|\n\u2264 K \u2211\nk=1\n(1 + \u03b5)3OPT (Ck, 1) + 2\u03b5\nd OPT (Ck, 1) \u2212 ln\n(\n|Ck| |X|\n)\n\u00b7 |Ck|\n= K \u2211\nk=1\n(\n(1 + \u03b5)3 + 2\u03b5\nd\n)\nOPT (Ck, 1)\u2212 ln\n(\n|Ck| |X|\n)\n\u00b7 |Ck|\n\u2264\n(\n(1 + \u03b5)3 + 2\u03b5\nd\n) K \u2211\nk=1\nOPT (Ck, 1)\u2212 ln\n(\n|Ck| |X|\n)\n\u00b7 |Ck|\n=\n(\n(1 + \u03b5)3 + 2\u03b5\nd\n)\nOPT (X,K)\n\u2264 (1 + \u03b5)4OPT (X,K)\n3.2 Generate Candidate Means by Sampling\nWe reuse the following well-known lemma on superset sampling.\nLemma 17 (superset-sampling). Let X \u2282 Rd be a finite set, \u03b1 < 1 and X \u2032 \u2282 X with |X \u2032| \u2265 \u03b1|X|. Let S \u2286 X be a uniform sample multiset of size at least 2\u03b1\u03b5\u03b4 . Then with probability at\nleast 1\u2212\u03b45 there is a subset S \u2032 \u2286 S with |S\u2032| = 1\u03b5\u03b4 such that\n\u2016\u00b5(S\u2032)\u2212 \u00b5(X \u2032)\u20162 \u2264 \u03b5\n|X \u2032|\n\u2211\nx\u2208X\u2032\n\u2016x\u2212 \u00b5(X \u2032)\u20162.\nIf we plug our notion of f -balanced solutions into this lemma, then we receive an algorithm that samples good approximative means.\nTheorem 18 (sampling means). For a finite set X \u2282 Rd, K \u2208 N and \u03b5, \u03b4 > 0, if X = \u02d9 \u22c3K\nk=1Ck\nis an f -balanced partition, then there is an algorithm that computes a set of log(1/\u03b4)\u00b72 K \u03b5\u03b4\n\u00b7log (\nf(K) \u03b5\u03b4\n)\nK-tuples of points from Rd, such that with probability 1\u2212 \u03b4 for one of these tuples it holds that for all k \u2208 [K]\n\u2016\u00b5k \u2212 \u00b5(Ck)\u2016 2 \u2264\n\u03b5\n|Ck|\n\u2211\nx\u2208Ck\n\u2016x\u2212 \u00b5(Ck)\u2016 2 .\nThe runtime of the algorithm is bounded by log(1/\u03b4) \u00b7K \u00b7\n(\n|X|+ 2 K \u03b5\u03b4\n\u00b7log (\nf(K) \u03b5\u03b4\n))\n.\nProof. Consider the following algorithm, which computes a candidate set of tuples of means.\nAlgorithm 1: Approx-Means(X,K)\nInput: X \u2282 Rd : input points\nK \u2208 N : number of clusters\nOutput: set of candidate tuples of means\nP \u2190 \u2205;\nfor k = 1, . . . ,K do sample a multiset S of size 1\u03b1\u03b5\u03b4 from X;\nT \u2190 { \u00b5(S\u2032)|S\u2032 \u2282 S, |S\u2032| = \u2308 1\u03b5\u03b4\u2309 } ; P \u2190 P \u00d7 T ;\nend\nreturn P ;\nUsing Lemma 17 with \u03b1 = 1f(K) , we know that the output of a single run of Approx-Means\ncontains a tuple with the desired property with probability ( 1\u2212\u03b4 5 )K .\nWe know that\n|T | \u2264\n(\n1\n\u03b1\u03b5\u03b4\n) 1 \u03b5\u03b4\n,\nthus\n|P | = |T |K \u2264 2 K \u03b5\u03b4\n\u00b7log (\nf(K) \u03b5\u03b4\n)\n.\nThe runtime is bounded by\nK \u00b7 |X|+ K \u2211\nk=1\n|T |k \u2264 K\n(\n|X|+ 2 K \u03b5\u03b4\n\u00b7log (\nf(K) \u03b5\u03b4\n))\n.\nBy executing Approx-Means log(1/\u03b4) times we receive the desired success probability.\n3.3 Generate Candidate Cluster Sizes and Variances by Using Grids\nSo far, we have formulated an algorithm that gives us good means. In the following, we will use the gridding technique to determine a set of candidates for the the cluster sizes and variances. First of all, we generate a set of cluster sizes that contains good approximations of the cluster sizes of any f -balanced solutions. Then, we approximate the negative log-likelihood of optimal CMLE clusters, i.e.\n\u2211K k=1OPT (Ck, 1) where the Ck are the optimal CMLE clusters. Then,\nwe present how to construct a candidate set of variances that contains good estimates of the variances of any (f, g)-balanced optimal CMLE solution.\n3.3.1 Grid Search for Cluster Sizes Theorem 19. Let X \u2282 Rd, K \u2208 N and let X = \u02d9 \u22c3K\nk=1Ck be an f -balanced partition. Then\nthere exists an algorithm that outputs a set S \u2286 NK , |S| = (\nlog(f(K)) log(1+\u03b5)\n)K , that contains a tuple\n(n1, . . . , nK) \u2208 S such that\n|Ck| \u2264 nk \u2264 (1 + \u03b5)|Ck|. (6)\nfor all k \u2208 [K].\nProof. Since we assume a f -balanced solution, we know that for all k \u2208 [K]\n|X|\nf(K) \u2264 |Ck| \u2264 |X|.\nThus, there exist a value i\u2217 \u2208 {1, . . . , \u2308log1+\u03b5(f(K))\u2309} such that\n(1 + \u03b5)i \u2217\u22121 |X|\nf(K) \u2264 |Ck| \u2264 (1 + \u03b5)\ni\u2217 |X|\nf(K) .\nThus, we receive \u2308log1+\u03b5(f(K))\u2309 many values for each cluster size nk. The algorithm outputs all possible combinations of these values.\n3.3.2 Bounds on the Log-Likelihood of optimal CMLE clusters\nLemma 9 provides us with a lower bound on the negative log-likelihood of a cluster.\nCorollary 20 (Lower Bound on the Optimal Log-Likelihood). Let X = \u02d9 \u22c3K\nk=1Ck be an optimal\nCMLE solution. Then, OPT (Ck, 1) \u2265 |Ck |d 2 .\nThe next step is to find an upper bound on the optimal complete-data likelihood value. We use Gonzales algorithm to compute a value that gives us a tighter bound than just the maximum spread (over the dimensions of the vectors in the data set).\nLemma 21 (Upper Bound on the Optimal Complete-Data Log-Likelihood). Let X \u2282 Rd and K \u2208 N. A Value \u0393 can be computed in time O(K \u00b7d \u00b7 |X|) such that the complete-data likelihood of an optimal CMLE solution can be bounded by\nOPT (X,K) \u2264 |X|d\n2 \u00b7 \u0393\nand \u0393 = ln(2\u03c0s2) + 1 + ln(K) for some s \u2264 4 \u00b7OPTdiam(X).\nProof. Run Gonzales algorithm. The output is a set of K points p1, . . . , pK \u2208 X. Compute the point z with maximum distance to its closest point in {p1, . . . , pK} and set s := mink=1,...,K\u2016z\u2212 pk\u2016. Consider the solution where the pk are the centers. Partition the points into point sets C = {C1, . . . , CK}, with \u2016x\u2212pk\u2016 = mini=1,...,K\u2016x\u2212pi\u2016 for all x \u2208 Ck. Notice that the distances between any point and its center is at most s. Thus, when computing the optimal variance in each cluster, it is at most s2. Then, for \u03b8 = {(\n1 K , pk, \u03c3(Xk, pk)\n)}K\nk=1 we have\nOPT (X,K) \u2264 LX(\u03b8, C) = K \u2211\nk=1\n|Ck|d\n2 ln(2\u03c0\u03c3(Ck, pk)\n2) + |Ck|d\n2 \u2212 ln(wk) \u00b7 |Ck|\n\u2264\n(\nK \u2211\nk=1\n|Ck|d\n2 ln(2\u03c0s2) +\n|Ck|d\n2\n)\n\u2212 ln\n(\n1\nK\n)\n\u00b7 |X|\n= |X|d\n2 ln(2\u03c0s2) +\n|X|d\n2 + ln(K) \u00b7 |X|\n\u2264 |X|d\n2\n( ln(2\u03c0s2) + 1 + ln(K) )\nGiven two bounds, we can find a constant factor approximation of the the sum of the negative log-likelihoods of optimal CMLE clusters, i.e.\n\u2211K k=1OPT (Ck, 1), using a grid search.\nLemma 22 (Estimating the Optimal Log-Likelihood). Let X \u2282 Rd, K \u2208 N, and \u03b5 > 0. Let X = \u222a\u0307Kk=1Ck be an optimal CMLE solution. Then, there exists a set of log(3\u0393/d)/ log(1 + \u03b5) many values which contains a value Nest with\n1\n1 + \u03b5 Nest \u2264\nK \u2211\nk=1\nOPT (Ck, 1) \u2264 Nest .\nProof. Combining Corollary 20 and Lemma 21, we know that\n|X|d\n2 \u2264\nK \u2211\nk=1\nOPT (Ck, 1) \u2264 OPT (X,K) \u2264 |X|d\n2 \u0393.\nThus, there exist a value i\u2217 \u2208 {1, . . . , \u2308log1+\u03b5(\u0393)\u2309} such that\n(1 + \u03b5)i \u2217\u22121 |X|d\n2 \u2264\nK \u2211\nk=1\nOPT (Ck, 1) \u2264 (1 + \u03b5) i\u2217 |X|d\n2 .\nThe algorithm outputs all \u2308log1+\u03b5(\u0393)\u2309 values.\nGiven this approximation of the sum of the negative log-likelihoods, we will be able to find an approximation of the negative log-likelihoods of a single cluster as we will see in the next section.\n3.3.3 Grid Search for Variances\nGiven the approximations of the size of the clusters and their negative log-likelihod, we are now able to find estimates of the variances.\nTheorem 23. Let X \u2282 Rd, K \u2208 N and \u03b5 > 0. Assume X has an (f, g)-balanced CMLE solution X = \u02d9 \u22c3K\nk=1Ck. Let additionally Nest \u2208 R, with\n1\n1 + \u03b5 Nest \u2264\nK \u2211\nk=1\nOPT (Ck, 1) \u2264 Nest, (7)\nand (n1, . . . , nK), such that for all k \u2208 [K]\n|Ck| \u2264 nk \u2264 (1 + \u03b5)|Ck|. (8)\nThen there exists an algorithm that computes a set of size K \u00b7 log(g(K))log(1+\u03b5) , that contains a tuple (\u03c3\u030321 , . . . , \u03c3\u0303 2 K), such that for all k \u2208 [K] it holds\n\u03c3\u03032k \u2265 \u03c3 2 k (9)\nand\nln(\u03c3\u03032k)\u2212 ln(\u03c3 2 k) \u2264\n( (1 + \u03b5)2 \u2212 1 ) 2\n|Ck|d OPT (Ck, 1) . (10)\nProof. Observe that\n1\ng(K)(1 + \u03b5) Nest \u2264\n1\ng(K)\nK \u2211\nk=1\nOPT (Ck, 1) Def. 10 \u2264 OPT (Ck, 1) \u2264\nK \u2211\nk=1\nOPT (Ck, 1) \u2264 Nest.\nThus, there exists a value j\u2217 \u2208 { \u2308\u2212 log1+\u03b5(g(K))\u2309, . . . , 0 } which satisfies\n(1 + \u03b5)j \u2217\u22121Nest \u2264 OPT (Ck, 1) \u2264 (1 + \u03b5) j\u2217Nest .\nDenote the upper bound by N\u0302 := (1 + \u03b5)j \u2217 Nest and set \u03c3\u0303 2 k := exp\n(\n2(1+\u03b5) nkd\nN\u0302 \u2212 ln(2\u03c0)\u2212 1 ) .\nNotice that\nOPT (Ck, 1) = LCk(\u00b5k, \u03c3 2 k) =\n|Ck|d\n2\n( ln(2\u03c0\u03c32k + 1) )\n\u21d4 ln(\u03c32k) = 2\n|Ck|d OPT (Ck, 1) \u2212 ln(2\u03c0) \u2212 1\nThus,\nln(\u03c3\u03032k) = 2(1 + \u03b5)\nnkd N\u0302 \u2212 ln(2\u03c0) \u2212 1 \u2265\n2\n|Ck|d OPT (Ck, 1) \u2212 ln(2\u03c0) \u2212 1 = ln(\u03c3\n2 k)\nand\nln(\u03c3\u03032k)\u2212 ln(\u03c3 2 k) =\n2(1 + \u03b5)\nnkd N\u0302 \u2212\n2\n|Ck|d OPT (Ck, 1)\n\u2264 2(1 + \u03b5)2\n|Ck|d OPT (Ck, 1) \u2212\n2\n|Ck|d OPT (Ck, 1)\n= ( (1 + \u03b5)2 \u2212 1 ) 2\n|Ck|d OPT (Ck, 1)\n4 Proof of Theorem 15\nIn the following we present the proof of Theorem 15.\n\u2022 In Section 4.1 we show how to estimate the variances and the cluster sizes of a well-defined CMLE solution via gridding. The idea behind a grid search is simply to test all solutions lying on a grid in the search space. By choosing a grid that is dense enough, we ensure that there are solutions on the grid which are sufficiently close to the parameters that we search for.\n\u2022 In Section 4.2, we show how one can find good estimates of the means when given good estimates of the weights and covariances. To this end, we adapt the sample-and-prune technique presented in [ABS10].\n4.1 Generate Candidates for Variances and Weights\nLemma 24. Let X \u2282 Rd, and {Ck} K k=1 be a well-defined CMLE solution for X, with corresponding variances {\u03c32k} K k=1. Then, there exists an algorithm which outputs a set of at most (\nlog(log(\u22062))+1 log(1+\u03b5) )K tuples of variances, which contains a tuple (\u03c3\u03032k) K k=1, such that\n\u2200k \u2208 [K] : \u03c32k \u2264 \u03c3\u0303 2 k \u2264 (\u03c3 2 k) (1+\u03b5) ,\nwhere \u22062 = maxx,y\u2208X{\u2016x\u2212 y\u2016 2}.\nProof. We know that optimal variances \u03c32k of a well-defined solution are bounded from below by\n\u2200k \u2208 [K] : 1\n2\u03c0 \u2264 \u03c32k.\nFurthermore, we know that these are also bounded from above by\n\u2200k \u2208 [K] : \u03c32k = 1\n|Ck|d\n\u2211\nx\u2208Ck\n\u2016x\u2212 \u00b5(Ck)\u2016 2 \u2264\n1\n|Ck|d\n\u2211\nx\u2208Ck\n\u22062 \u2264 \u22062 .\nBecause 1/(2\u03c0) \u2264 \u03c32k \u2264 \u2206 2, there exists a value\nk\u2217 \u2208 {1, . . . , log1+\u03b5(\u2212 log1/(2\u03c0)(\u2206 2))}\nsuch that (1/(2\u03c0))(1+\u03b5) k\u2217\u22121\n\u2264 \u03c32i \u2264 (1/(2\u03c0)) (1+\u03b5)k\n\u2217\n.\nThus, we receive \u2308 log(log(\u22062))\u2212log(log(2\u03c0)) log(1+\u03b5) \u2309 many values for each variance. The algorithm outputs all possible combinations of these values.\nThe following result is the same as in Section 3.3.\nTheorem 25. Let X \u2282 Rd, K \u2208 N and let C = \u02d9 \u22c3K\nk=1Ck be an f -balanced partition. Then there\nexists an algorithm that outputs a set S \u2286 NK , |S| = (\nlog(f(K)) log(1+\u03b5) )K , that contains {n1, . . . , nK} \u2282\nS such that\n|Ck| \u2264 nk \u2264 (1 + \u03b5)|Ck|. (11)\nfor all k \u2208 [K].\nAlgorithm 2: Approx-Means(R, l,Mk\u2212l,\u03a3)\nInput: R \u2282 X \u2282 Rd : set of remaining input points\nl \u2208 N : number of means yet to be found\n~\u00b5 = (\u00b51, . . . , \u00b5j) : tuple of j \u2264 k \u2212 l candidate means (\u03c3\u030321 , . . . , \u03c3\u0303 2 k) : vector of k variances (w\u030321, . . . , w\u0303 2 k) : vector of k weights Notation: ~S : vector containing the elements of set S in arbitrary order\n~x \u25e6 ~y : concatenation of vectors, i.e. for ~x = (x1, . . . , xn) and ~y = (y1, . . . , ym), ~x \u25e6 ~y = (x1, . . . , xn, y1, . . . , ym) Output: \u03b8 = {(wi, \u00b5i, \u03c3i)} containing at most k tuples of mean and variance if l = 0 then\nreturn ~P ;\nelse\nif l \u2265 |R| then\nreturn \u03b8 = {(\u00b5i, \u03c3i)}i where ~\u00b5 \u25e6 ~R = (\u00b5i)i;\nelse\n/* sampling phase */ ; sample a multiset S of size 1\u03b1\u03b5\u03b4 from R; T \u2190 {\n\u00b5(S\u2032)|S\u2032 \u2282 S, |S\u2032| = 1\u03b5\u03b4 } ;\nMk \u2190 \u2205; for t \u2208 T do Mk \u2190 Mk \u222aApprox-Means(R, l \u2212 1, {~\u00b5 \u25e6 (t)|~\u00b5 \u2208 Mk\u2212l},\u03a3); end\n/* pruning phase */ ; N \u2190 set of |R|2 points x from R with smallest minimum negative complete-data log-likelihood cost wrt. the weighted component given by (w\u0303i, \u00b5i, \u03c3\u0303 2 i ) for i \u2208 [j], i.e.\nmin i\u2208[j]\n{\nd 2 ln(2\u03c0\u03c3\u03032i ) + 1\n2\u03c3\u03032i \u2016x\u2212 \u00b5i\u2016\n2 \u2212 ln(w\u0303i)\n}\nMk \u2190 Mk \u222aApprox-Means(R \\N, l,Mk\u2212l,\u03a3); return the candidate \u03b8 = {(wi, \u00b5i, \u03c3i)}i, (\u00b5i) \u2208 Mk, which has minimal cost LX(\u03b8) ;\nend\nend\n4.2 Applying the ABS Algorithm\nIn the following we analyze Algorithm 2. We show that the algorithm can be used to construct means such that, together with appropriate approximations of the weights and variances, we obtain a CMLE solution with costs close to the costs of the given CMLE solution.\nTheorem 26. Let \u03c3\u0303i \u2208 [\u03c3 2 i , (\u03c3 2 i ) (1+\u03b5)] and w\u0303k \u2265 1 (1+\u03b5)wk for i \u2208 [k]. Algorithm 2 started with (X, k, \u2205, (\u03c3\u030321 , . . . , \u03c3\u0303 2 k)) computes a tuple (\u00b5\u03031, . . . , \u00b5\u0303k) such that with probability at least ( 1\u2212\u03b4 5 )k\nLX((w\u0303i, \u00b5\u0303i, \u03c3\u0303 2 i )i\u2208[k]) \u2264 (1 + \u03b5)L(X) .\nThe running time of the algorithm is bounded by |X| d 2O(k/\u03b5\u00b7log(k/\u03b5 2)).\nLet \u02d9 \u22c3k\ni=1Ci be a partition of X into optimal CMLE clusters. We introduce\nC[i,j] = \u02d9\u22c3j\nt=i Ct\nas a short notation for the disjoint union of clusters i through j. We assume that the Ci are numbered by the order their approximate means \u00b5\u0303i are found by the superset-sampling technique.\nNow, let X = R0 \u2287 Ri \u2287 \u00b7 \u00b7 \u00b7 \u2287 Rk\u22121 be a sequence of input sets computed by the algorithm, such that\n|Ci \u2229Ri\u22121| \u2265 \u03b1|Ri\u22121|.\nWithout loss of generality assume that each Ri is the largest of these sets with this property. By using Lemma 17, we obtain the following Lemma.\nLemma 27 (By Superset-Sampling). With probability at least ((1 \u2212 \u03b4)/5)k we have\n\u2016\u00b5\u0303i \u2212 \u00b5(Ci \u2229Ri\u22121)\u2016 2 \u2264\n\u03b5\n|Ci \u2229Ri\u22121|\n\u2211\nx\u2208Ci\u2229Ri\u22121\n\u2016x\u2212 \u00b5(Ci \u2229Ri\u22121)\u2016 2\nfor all i \u2208 [K].\nBy Ni := Ri\u22121 \\Ri we denote the set of points remove between two sampling phases. Using these definitions we can see that\n\u02d9\u22c3k\ni=1 (Ci \u2229Ri\u22121) \u222a\u0307\n\u02d9\u22c3k\ni=1\n( C[i+1,k] \u2229Ni )\nis a disjoint partition of X. Each set Ci\u2229Ri\u22121 on the left side contains the points that the mean \u00b5\u0303i has been sampled from. The sets C[i+1,k] \u2229 Ni on the right side contain points incorrectly assigned to {\u00b5\u03031, . . . , \u00b5\u0303i} during the pruning phases between the sampling of \u00b5\u0303i and \u00b5\u0303i+1.\nDenote by \u03b8i the parameters of the first i weighted Gaussians obtained by the algorithm, i.e.\n\u03b8\u0303i = ((w\u03031, \u00b5\u03031, \u03c3\u03031), . . . , (w\u0303i, \u00b5\u0303i, \u03c3\u0303i)) .\nLemma 28 (cf. Claim 4.8 in [Ack09]).\nLC[i+1,k]\u2229Ni(\u03b8\u0303i) \u2264 8\u03b1kLC[1,i]\u2229Ri\u22121(\u03b8\u0303i)\nProof. As in [Ack09, p. 70ff], with \u201ccost\u201c replaced by \u201dL\u201c.\nDenote by cost(P,C) the k-means cost of a point set P wrt. a set of means C.\nLemma 29 (cf. Claim 4.9 in [Ack09]). For every i \u2208 [k] we have\ncost(Ci \u2229Ri\u22121, \u00b5\u0303i) \u2264 (1 + \u03b5) cost(Ci, \u00b5i) .\nProof. As in [Ack09, p. 70ff], using that optimal means in CMLE are means of the optimal CMLE clusters.\nGiven appropriate approximate variances, we can conclude that a similar bound holds wrt. the complete-data log-likelihood.\nLemma 30. Given \u03c3\u0303i \u2208 [\u03c3 2 i , (\u03c3 2 i ) (1+\u03b5)] and w\u0303i = ni |X| with ni \u2208 [|Ci|, (1 + \u03b5)|Ci|], we have\nLCi\u2229Ri\u22121(w\u0303i, \u00b5\u0303i, \u03c3\u0303 2 i ) \u2264 (1 + \u03b5)LCi(wi, \u00b5i, \u03c3 2 i ) .\nProof.\nLCi\u2229Ri\u22121(\u00b5\u0303i, \u03c3\u0303 2 i ) =\n|Ci \u2229Ri\u22121|d\n2 ln(2\u03c0\u03c3\u03032i ) +\n1\n2\u03c3\u03032i cost(Ci \u2229Ri\u22121, \u00b5\u0303i)\u2212 |Ci \u2229Ri\u22121| ln(w\u0303i) .\nWe have\nln(2\u03c0\u03c3\u03032i ) \u2264 ln(2\u03c0(\u03c3 2 i ) (1+\u03b5)) = (1 + \u03b5) ln(2\u03c0\u03c32i ) .\nFurthermore, Using that |Cl| \u2264 nl \u2264 (1+\u03b5)|Cl| for all l = 1, . . . ,K, we obtain w\u0303k \u2265 |Ck| |X| . Hence,\n\u2212 ln(w\u0303i) \u00b7 |Ci \u2229Ri\u22121| \u2264 \u2212 ln(w\u0303i) \u00b7 |Ci|\n\u2264 \u2212 ln\n(\n|Ci| |X|\n)\n|Ci| (by Equation (3))\n= \u2212 ln (wi) \u00b7 |Ci|\nBy Lemma 29 and \u03c3\u03032i \u2265 \u03c3 2 i ,\n1\n2\u03c3\u03032i cost(Ci \u2229Ri\u22121, \u00b5\u0303i) \u2264 (1 + \u03b5)\n1\n2\u03c32i cost(Ci, \u00b5i) .\nFrom this and by using that \u03c32i = 1 |Ci|d cost(Ci, \u00b5i), we conclude\nLCi\u2229Ri\u22121(\u00b5\u0303i, \u03c3\u0303 2 i ) \u2264 (1 + \u03b5)\n|Ci|d\n2 ln(2\u03c0\u03c32i ) + (1 + \u03b5)\n1\n2\u03c32i cost(Ci, \u00b5i)\u2212 ln(wi)|Ci|\n\u2264 (1 + 2\u03b5)N1(Ci)\u2212 ln(wi)|Ci| \u2264 (1 + 2\u03b5)LCi(\u00b5i, \u03c3 2 i ) .\nRunning Algorithm 2 with \u03b5/3 instead of \u03b5 yields the claim.\nAnalogously to [Ack09], we can prove Theorem 26 as follows.\nProof of Theorem 26. Let \u03b8\u0303k = (\u00b5\u0303i, \u03c3\u0303 2 i )i\u2208[k]. Then,\nLX(\u03b8\u0303k) \u2264 k \u2211\ni=1\nLCi\u2229Ri\u22121(\u00b5\u0303i, \u03c3\u0303 2 i ) +\nk\u22121 \u2211\ni=1\nLC[i+1,k]\u2229Ni(\u03b8\u0303k)\n\u2264 k \u2211\ni=1\nLCi\u2229Ri\u22121(\u00b5\u0303i, \u03c3\u0303 2 i ) + 8\u03b1k\nk\u22121 \u2211\ni=1\nLC[1,i]\u2229Ri\u22121(\u03b8\u0303k) (due to Lemma 28)\n\u2264 k \u2211\ni=1\nLCi\u2229Ri\u22121(\u00b5\u0303i, \u03c3\u0303 2 i ) + 8\u03b1k\nk\u22121 \u2211\ni=1\ni \u2211\nt=1\nLCt\u2229Ri\u22121(\u00b5\u0303t, \u03c3\u0303 2 t ) .\nSince Ri \u2286 Ri\u22121, we have Ct \u2229Ri\u22121 \u2286 Ct \u2229Rt\u22121. Hence,\nk\u22121 \u2211\ni=1\ni \u2211\nt=1\nLCt\u2229Ri\u22121(\u00b5\u0303t, \u03c3\u0303 2 t ) \u2264\nk\u22121 \u2211\ni=1\ni \u2211\nt=1\nLCt\u2229Rt\u22121(\u00b5\u0303t, \u03c3\u0303 2 t )\n\u2264 k k\u22121 \u2211\ni=1\nLCi\u2229Ri\u22121(\u00b5\u0303i, \u03c3\u0303 2 i ) .\nThus,\nLX(\u03b8\u0303k) \u2264 k \u2211\ni=1\nLCi\u2229Ri\u22121(\u00b5\u0303i, \u03c3\u0303 2 i ) + 8\u03b1k\n2 k\u22121 \u2211\ni=1\nLCi\u2229Ri\u22121(\u00b5\u0303i, \u03c3\u0303 2 i )\n\u2264 (1 + 8\u03b1k2) k \u2211\ni=1\nLCi\u2229Ri\u22121(\u00b5\u0303i, \u03c3\u0303 2 i )\n\u2264 (1 + 8\u03b1k2)(1 + \u03b5)L(X) . (by Lemma 30)\nFinally, running the algorithm for \u03b5 := \u03b5/2 and \u03b1 = \u03b8(\u03b5/k2) yields the theorem.\n5 Special Cases\n5.1 Weighted K-Means (Identical Covariances)\nIn this section we consider a restricted version of the CMLE problem where we are only interested in Gaussian mixture models where all components share the same fixed spherical covariance matrix, i.e. parameters \u03b8 = {(wk, \u00b5k,\u03a3k)}k\u2208[K] where \u03a3k = 1 2\u03b2 Id for all k \u2208 [K]. We call this problem the Weighted K-Means (WKM) problem.\nProblem 31 (WKM). Given a finite set X \u2282 Rd and an integer K \u2208 N, find a partition C = {C1, . . . , CK} of X into K disjoint subsets and K weighted means \u03b8 = {(wk, \u00b5k)} K k=1, where \u00b5k \u2208 R D, wk \u2208 R, and \u2211K k=1wk = 1, minimizing\nLwmX (\u03b8, C) = K \u2211\nk=1\n\u03b2\n\n\n\u2211\nx\u2208Ck\n\u2016x\u2212 \u00b5k\u2016 2\n\n\u2212 ln(wk) \u00b7 |Ck| .\nWe denote the minimal value by OPTwm(X,K).\nCorollary 32. Let X \u2282 Rd, K \u2208 N, and \u03b4, \u03b5 > 0. Let X = \u02d9 \u22c3K\nk=1Ck be a well-defined solution for the WKM problem. There is an algorithm that computes K weighted means \u03b8 = {(w\u0303k, \u00b5\u0303k)} K k=1 such that with probability at least 1\u2212 \u03b4\nLwmX ((w\u0303i, \u00b5\u0303i)i\u2208[K]) \u2264 (1 + \u03b5)OPTwm(X,K) .\nThe running time of the algorithm is bounded by\n|X| d 2O(K/\u03b5\u00b7log(K/\u03b5 2)) \u00b7 (log(f(K)))K .\nProof. Use a grid search to obtain candidates for the weights, then apply the ABS algorithm.\n5.2 Uniform Weights\nIn this section we consider a restricted version of the CMLE problem where we are only interested in Gaussian mixture models with fixed uniform weights, i.e. parameters \u03b8 = {(wk, \u00b5k,\u03a3k)}k\u2208[K] where wk = 1/K for all k \u2208 [K]. We denote this problem by Uniform Complete-Data Maximum Likelihood Estimation (UCMLE).\nProblem 33 (UCMLE). Given a finite set X \u2282 Rd and an integer K \u2208 N, find a partition C = {C1, . . . , CK} of X into K disjoint subsets and K spherical Gaussians with parameters \u03b8 = {(\u00b5k, \u03c3 2 k)} K k=1 minimizing\nLunifX (\u03b8, C) = K \u2211\nk=1\nLCk(\u00b5k, \u03c3 2 k)\n= K \u2211\nk=1\n|Ck|d\n2 ln(2\u03c0\u03c32k) +\n1\n2\u03c32k\n\n\n\u2211\nx\u2208Ck\n\u2016x\u2212 \u00b5k\u2016 2\n\n .\nWe denote the minimal value by OPTunif (X,K).\nCorollary 34. Let X \u2282 Rd, K \u2208 N, and \u03b4, \u03b5 > 0. Let X = \u02d9 \u22c3K\nk=1Ck be a well-defined solution for the UCMLE problem. There is an algorithm that computes K spherical Gaussians \u03b8 = {(\u00b5\u0303k, \u03c3\u0303 2 k)} K k=1 such that with probability at least 1\u2212 \u03b4\nLunifX ((\u00b5\u0303i, \u03c3\u0303 2 i )i\u2208[K]) \u2264 (1 + \u03b5)OPTunif (X,K) .\nThe running time of the algorithm is bounded by\n|X| d log(1/\u03b4) 2O(K/\u03b5\u00b7log(K/\u03b5 2)) ( log(log(\u22062)) + 1 )K ,\nwhere \u22062 = maxx,y\u2208X{\u2016x\u2212 y\u2016 2}.\nProof. Use a grid search to obtain candidates for the variances, then apply the ABS algorithm.\nReferences\n[ABS10] Marcel R. Ackermann, Johannes Blo\u0308mer, and Christian Sohler. Clustering for metric and nonmetric distance measures. ACM Trans. Algorithms, 6(4):59:1\u201359:26, September 2010.\n[Ack09] Marcel R. Ackermann. Algorithms for the Bregman k-Median Problem. PhD thesis, University of Paderborn, 2009.\n[CG92] Celeux and Govaert. A Classification EM Algorithm for Clustering and Two Stochastic Versions. Comput. Stat. Data Anal., 14(3), 1992.\n[IKI94] M. Inaba, N. Katoh, and H. Imai. Applications of Weighted Voronoi Diagrams and Randomization to Variance-based K-clustering. In Proceedings of the Tenth Annual Symposium on Computational Geometry, SoCG \u201994, pages 332\u2013339, New York, NY, USA, 1994. ACM."}], "references": [{"title": "Clustering for metric and nonmetric distance measures", "author": ["Marcel R. Ackermann", "Johannes Bl\u00f6mer", "Christian Sohler"], "venue": "ACM Trans. Algorithms,", "citeRegEx": "Ackermann et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Ackermann et al\\.", "year": 2010}, {"title": "Algorithms for the Bregman k-Median Problem", "author": ["Marcel R. Ackermann"], "venue": "PhD thesis, University of Paderborn,", "citeRegEx": "Ackermann.,? \\Q2009\\E", "shortCiteRegEx": "Ackermann.", "year": 2009}, {"title": "A Classification EM Algorithm for Clustering and Two Stochastic Versions", "author": ["Celeux", "Govaert"], "venue": "Comput. Stat. Data Anal.,", "citeRegEx": "Celeux and Govaert.,? \\Q1992\\E", "shortCiteRegEx": "Celeux and Govaert.", "year": 1992}, {"title": "Applications of Weighted Voronoi Diagrams and Randomization to Variance-based K-clustering", "author": ["M. Inaba", "N. Katoh", "H. Imai"], "venue": "In Proceedings of the Tenth Annual Symposium on Computational Geometry,", "citeRegEx": "Inaba et al\\.,? \\Q1994\\E", "shortCiteRegEx": "Inaba et al\\.", "year": 1994}], "referenceMentions": [], "year": 2016, "abstractText": "Training the parameters of statistical models to describe a given data set is a central task in the field of data mining and machine learning. A very popular and powerful way of parameter estimation is the method of maximum likelihood estimation (MLE). Among the most widely used families of statistical models are mixture models, especially, mixtures of Gaussian distributions. A popular hard-clustering variant of the MLE problem is the so-called completedata maximum likelihood estimation (CMLE) method. The standard approach to solve the CMLE problem is the Classification-Expectation-Maximization (CEM) algorithm [CG92]. Unfortunately, it is only guaranteed that the algorithm converges to some (possibly arbitrarily poor) stationary point of the objective function. In this paper, we present two algorithms for a restricted version of the CMLE problem. That is, our algorithms approximate reasonable solutions to the CMLE problem which satisfy certain natural properties. Moreover, they compute solutions whose cost (i.e. complete-data log-likelihood values) are at most a factor (1 + \u03b5) worse than the cost of the solutions that we search for. Note the CMLE problem in its most general, i.e. unrestricted, form is not well defined and allows for trivial optimal solutions that can be thought of as degenerated solutions.", "creator": "LaTeX with hyperref package"}}}