{"id": "1603.06708", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "22-Mar-2016", "title": "A Self-Paced Regularization Framework for Multi-Label Learning", "abstract": "In this paper, we propose a novel multi-label learning framework, called Multi-Label Self-Paced Learning (MLSPL), in an attempt to incorporate the self-paced learning strategy into multi-label learning regime. In light of the benefits of adopting the easy-to-hard strategy proposed by self-paced learning, the devised MLSPL aims to learn multiple labels jointly by gradually including label learning tasks and instances into model training from the easy to the hard. We first introduce a self-paced function as a regularizer in the multi-label learning formulation, so as to simultaneously rank priorities of the label learning tasks and the instances in each learning iteration. Considering that different multi-label learning scenarios often need different self-paced schemes during optimization, we thus propose a general way to find the desired self-paced functions. Experimental results on three benchmark datasets suggest the state-of-the-art performance of our approach. In addition to using a self-paced approach, our method offers a very intuitive algorithm for self-paced training, the most advanced in our design and in this case, the model-based training algorithm. We also propose a novel single-label learning system, that is not only in line with the new algorithm described in the previous work, but also in line with existing model training.\n\n\n\n\nWe propose the best-designed MLSPL learning framework of any kind, consisting of three methods: an algorithm for training and two types of self-paced learning tasks, as well as an additional two-level self-paced learning process. The first method offers a single-label learning model, consisting of three methods: an algorithm for learning and two types of self-paced learning tasks, as well as an additional two-level self-paced learning process. The second method offers a single-label learning model, consisting of three types of self-paced learning tasks, as well as an additional two-level self-paced learning process. The second method offers a single-label learning model, consisting of three types of self-paced learning tasks, as well as an additional two-level self-paced learning process. The third method offers a single-label learning model, consisting of three types of self-paced learning tasks, as well as an additional two-level self-paced learning process. The first method offers a single-label learning model, consisting of three types of self-paced learning tasks, as well as an additional two-level self-paced learning process. The second method offers a single", "histories": [["v1", "Tue, 22 Mar 2016 09:03:40 GMT  (434kb,D)", "http://arxiv.org/abs/1603.06708v1", null], ["v2", "Wed, 6 Apr 2016 14:54:28 GMT  (434kb,D)", "http://arxiv.org/abs/1603.06708v2", null]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["changsheng li", "fan wei", "junchi yan", "weishan dong", "qingshan liu", "xiaoyu zhang", "hongyuan zha"], "accepted": false, "id": "1603.06708"}, "pdf": {"name": "1603.06708.pdf", "metadata": {"source": "CRF", "title": "A Self-Paced Regularization Framework for Multi-Label Learning", "authors": ["Changsheng Li", "Fan Wei", "Junchi Yan", "Weishan Dong", "Qingshan Liu", "Xiaoyu Zhang"], "emails": ["dongweis}@cn.ibm.com.", "fanwei@stanford.edu.", "jcyan@sei.ecnu.edu.cn", "qsliu@nuist.edu.cn.", "zhangxiaoyu@iie.ac.cn"], "sections": [{"heading": null, "text": "I. INTRODUCTION\nMulti-label learning has attracted much attention in the past decade [1], [2]. Its goal is to learn a classifier to map the input instance into a label vector space, where each instance is associated with multiple labels instead of one single label. Different from multi-class learning, multiple labels in multi-label learning are often assumed to be correlated with each other. Often, this correlation among labels is beneficial to accurately predicting labels of test instances. Due to its empirical success, multi-label learning has been widely applied to various domains including image annotation [3], video concept detection [4], web page categorization [5], and visual object recognition [6].\nDuring the past years, many multi-label learning algorithms have been proposed. One simplified approach is to decompose multi-label learning into multiple independent binary classification problems (one per label or category). However, such a solution does not consider the relationship among labels, whereas previous studies [7], [1] have revealed that the label relationship is quite helpful and should be considered. Therefore, several approaches attempt to exploit label correlations by incorporating external prior knowledge [8], [9], [10], [11]. Considering that the prior knowledge is often unavailable in real applications, many other approaches [7], [12], [13], [14],\nC. Li and W. Dong are with IBM Research-China, Beijing 100094, China. Email: {lcsheng, dongweis}@cn.ibm.com.\nF. Wei is with Department of Mathematics, Stanford University. E-mail: fanwei@stanford.edu.\nJ. Yan is with East China Normal University, Shanghai, China. E-mail: jcyan@sei.ecnu.edu.cn\nQ. Liu is with Nanjing University of Information Science and Technology, Nanjing 210014, China. Email: qsliu@nuist.edu.cn.\nX. Zhang is with Institute of Information Engineering, Chinese Academy of Sciences, China. E-mail: zhangxiaoyu@iie.ac.cn\n[15], [16], [1] try to mine label relationships based on training data and incorporate the label correlations into the learning process of multi-label model. In addition, there are also many works focusing on leveraging other learning techniques for multi-label learning, such as multi-instance multi-label learning [17], active learning for multi-label learning [18], and multi-label learning combined with multi-kernel learning [19].\nThe algorithms above treat all the categories equally and also treat all the training instances per category equally when training the model. However, in real-world scenarios, the complexities of different label learning tasks may differ quite much, and the same for complexities of different training instances in one label learning task. For example, as shown in Figure 1, when learning the label tiger, image (b) is clearly harder than image (a), since the color of the tiger in image (b) is quite similar to the background, and the tiger in (b) is partially occlusive by the trees. Moreover, in image (a), the label Siberian tiger is more difficult to learn than the label tiger, since Siberian tiger is a subclass of tiger. In addition, many multi-label learning methods are associated with nonconvex objective functions, which is prone to local minima especially in the presence of large corruption and bad starting point.\nInspired by how children learn concepts, self-paced learning [20] advocates a paradigm that learning should first consider \u2018simple\u2019 or \u2018easy\u2019 instances, and then gradually take \u2018complex\u2019 or \u2018hard\u2019 instances into account. By simulating such a process of human learning, it has been empirically verified that self-paced learning can mitigate the problem of local-minima during iterative learning [20], [21], and exhibit better generalization behavior in many tasks, such as matrix factorization [22], multi-view learning [23], and multi-instance learning [24]. Based on these facts, we conclude that adding the training instances into the learning process in order of complexities can produce a more robust and accurate multilabel learning model.\nar X\niv :1\n60 3.\n06 70\n8v 1\n[ cs\n.L G\n] 2\n2 M\nar 2\n01 6\n2 This paper proposes a novel multi-label learning framework, called Multi-Label Self-Paced Learning (MLSPL), which is an effort to build a connection between multi-label learning and self-paced learning, in a principled fashion. MLSPL aims to learn multi-label model by introducing a self-paced function as a regularizer that can simultaneously take into consideration the complexities of both instances and labels during learning. Similar to human\u2019s learning mechanism, MLSPL should use different learning schemes for different multi-label learning scenarios. To achieve this, we present a general way to find the self-paced functions for the desired learning schemes. Finally, we tailor a simple yet effective algorithm to solve the optimization problem. Experimental results on three benchmark datasets demonstrate the effectiveness of the proposed approach, compared to the state-of-the-art methods."}, {"heading": "II. BACKGROUND", "text": "We first define notations and then briefly introduce the work [1] that our approach is originated from. Let X = Rd be the d-dimensional input feature space and Y = {\u22121,+1}L the finite set of L possible labels. Given a multi-label training set D = {(xi,yi)}ni=1, where xi = [xi1, . . . , xid] \u2208 X is the i-th instance and yi = [yi1, . . . , yiL] \u2286 Y is the label vector associated with xi. yij is +1 if xi has the j-th label and \u22121 otherwise. The goal of multi-label learning is to learn a multi-label learner h : X \u2192 2L from the training set D, so that we can predict a set of labels for each unseen instance.\nAs mentioned earlier, most existing multi-label learning methods attempt to exploit the correlations among labels to help learn the classifier h. Among these methods, one representative algorithm is ML-LOC [1] which tries to exploit the correlations locally; it assumes that the instances are partitioned into m different clusters and each cluster shares a subset of label correlations. Let W = [w1, . . . ,wL] \u2208 Rn\u00d7L, Q = [q1, . . . ,qn] \u2208 Rm\u00d7n, and A = [a1, . . . ,am] \u2208 RL\u00d7m where aj \u2208 RL is the mean of all the label vectors in the j-th cluster. Before explaining the variables, we first provide its formulation.\nmin W,A,\nQ\u2208[0,1]m\u00d7n L\u2211 l=1 L(wl,Q;D) + \u03b1\u0393(W) + \u03b2\u2126(A) (1)\n= L\u2211 l=1 L(wl,Q;D)+\u03b1 L\u2211 l=1 \u2016wl\u20162+\u03b2 n\u2211 i=1 m\u2211 j=1 qij\u2016yi\u2212aj\u20162\ns.t. m\u2211 j=1 qij = 1,\u2200i \u2208 [1, n],\nwhere W is the learned weight matrix with each column representing the weight vector for the corresponding task. L(wl,Q;D) is the empirical loss on the training set D for the l-th label, defined as L(wl,Q;D) = \u2211n i=1 L(wl,qi;xi, yil). Let zi = [\u03c6(xi);qi]1 where \u03c6 is a feature mapping induced by a kernel \u03ba. Then L(wl,qi;xi, yil) = max(0, 1\u2212yilwTl zi). The idea of ML-LOC is as follows. If the label vector yi is close to aj , then it is more likely that xi belongs to the j-th\n1The symbol [u;v] means flatten u,v into one single column vector.\ncluster. Moreover, minimizing the third term in (1) will result in a larger qij when yi is closer to aj . Thus, Q encodes the similarity between instances and clusters. The first term aims to take the similarity information as additional features for the instance, and incorporates them into the learning process of the classifier. By jointly optimizing these terms, the global discrimination fitting and local correlation sensitivity can be realized in a unified framework."}, {"heading": "III. MULTI-LABEL SELF-PACED LEARNING", "text": "Here, we will first present a general multi-label learning formulation with self-paced paradigm. Then we give a principled way to find the self-paced functions for realizing desired self-paced schemes. Last, an efficient algorithm is designed to solve the proposed optimization problem."}, {"heading": "A. Proposed Formulation", "text": "As can be seen from (1), the objective function treats all the training instances and all the label learning tasks equally. However, we should prioritize learning the easier instances and the easier labels. Moreover, the non-convexity of problem (1) renders the issue of bad local minima. To overcome these shortcomings, one interesting principle is that learning should be first done on the easy instances, and then gradually take the hard. This coincides with the idea for self-paced learning which is inspired by the way humans learn. Indeed, selfpaced learning has empirically demonstrated its usefulness for mitigating bad local minima and achieving better model generalization [20], [25]. More specifically, here we have \u2018easy\u2019 and \u2018hard\u2019 labels, as well as \u2018easy\u2019 and \u2018hard\u2019 instances. With the aim to incorporate the self-paced learning paradigm into the multi-label learning regime, we propose the following objective function by considering the complexities of labels and instances in a unified setting:\nmin W,A,\nQ\u2208[0,1]m\u00d7n , V\u2208[0,1]n\u00d7L\nn\u2211 i=1 L\u2211 l=1 v (l) i L(wl,qi;xi, yil) + \u03b1\u0393(W)\n+\u03b2\u2126(A) + f(V, \u03bb) (2)\ns.t. m\u2211 j=1 qij = 1,\u2200i \u2208 [1, n],\nwhere v(l) = [v(l)1 , . . . , v (l) n ]T consists of the weights of n instances for the l-th label. V = [v(1), . . . ,v(L)]. Different from (1), the first term in (2) is a weighted loss term on the training data D. f(V, \u03bb) denotes the self-paced function or self-paced regularizer used to determine which label learning tasks and the corresponding instances to be selected during training: we can select \u2018easy\u2019 labels and \u2018easy\u2019 instances for learning at the beginning of training. As the learning is iteratively proceeded, we can gradually add \u2018hard\u2019 labels and \u2018hard\u2019 instances into the process.\nMore importantly, since different problems often need different self-paced learning schemes during training, there is no universal self-paced function for all applications. Although many self-paced regularizers have been proposed for various\n3 applications [20], [24], [25], there lacks of a general method to derive the self-paced functions. In the following, we provide a general method to find the appropriate self-paced functions."}, {"heading": "B. Self-Paced Function", "text": "First, we introduce definition of the self-paced function from the recent work [22]:\nDefinition 1. Suppose that v is a weight variable, l is the loss, and \u03bb is the learning pace parameter. f(v, \u03bb) is called self-paced function, if\n1) f(v, \u03bb) is convex with respect to v \u2208 [0, 1]; 2) v\u2217 is monotonically decreasing with respect to l, and it\nholds that liml\u21920 v\u2217 \u2264 1, liml\u2192\u221e v\u2217 = 0; 3) v\u2217 is monotonically increasing with respect to \u03bb, and it\nholds that lim\u03bb\u21920 v\u2217 = 0, lim\u03bb\u2192\u221e v\u2217 \u2264 1; where v\u2217(l, \u03bb) = arg minv\u2208[0,1] vl + f(v, \u03bb) for fixed l, \u03bb.\nWe can see that v\u2217(l, \u03bb) is an S-shaped function in l. In order to find the self-paced function f , we first find a family of S-shaped functions g\u03bb(l) with range in [0, 1] such that it is monotonically decreasing with respect to l, and that liml\u2192\u221e g\u03bb(l) = 0 as well as liml\u21920 g\u03bb(l) \u2264 1. We further want g\u03bb(l) to increase with respect to \u03bb when keeping l fixed. Let v\u2217 = g\u03bb(l) be the arg min in Definition 1. Thus Conditions 2 and 3 in Definition 1 are satisfied. Let l = s(\u03bb, v) be the inverse function of g\u03bb(l). Then we propose the original lemma based on Definition 1.\nLemma 1. A smooth function f(v, \u03bb) is a self-paced function corresponding to v\u2217 if and only if \u2202f(v,\u03bb)\u2202v = \u2212s(\u03bb, v) and \u2202s(v,\u03bb) \u2202v \u2264 0 for v \u2208 [0, 1]. Proof. Since v\u2217 = arg min vl + f(v, \u03bb). We need\n\u2202(vl + f(v, \u03bb))\n\u2202v = 0.\nBy plugging in v\u2217, we have\nl + \u2202f(v\u2217, \u03bb)\n\u2202v = 0.\nBy our definition above, we know that l = s(\u03bb, v). So we need to have that \u2202f(v,\u03bb)\u2202v = \u2212s(\u03bb, v). In this way we have related f(v, \u03bb) with v\u2217 which satisfies Conditions 2 and 3.\nNow we also need Condition 1 to be satisfied. It is equivalent to say that \u2202\n2f(v,\u03bb) \u2202v2 \u2265 0. Thus we have\n\u22022f(v, \u03bb) \u2202v2 = \u2202(\u2212s(\u03bb, v)) \u2202v \u2265 0.\nSince different multi-label learning scenarios often need different self-paced schemes, it is necessary to develop more schemes for exploring this interesting direction. Next, we discuss some examples of self-paced learning schemes.\nExample 1. We choose v\u2217 as the arctan function, which is a classical S-shaped activation function.\nv\u2217(l, \u03bb) = \u2212arctan(l \u2212 \u03bb) + \u03c0/2\n\u03c0 . (3)\nThis function is centrally symmetric around the axis l = \u03bb. And v\u2217 is invairant under l \u2212 \u03bb.\nIn order to obtain the inverse function of v\u2217, we have that cot(\u03c0v\u2217) + \u03bb = l. Therefore s(\u03bb, v) = \u03bb+ cot(\u03c0v). Thus\n\u2212s(\u03bb, v) = \u2202f(v, \u03bb) \u2202v = \u2212\u03bb\u2212 cot(\u03c0v).\nIntegrating, we can obtain f(v, \u03bb) by f(v, \u03bb) = \u2212 \u222b v s(v, \u03bb) = \u2212\u03bbv \u2212 ln | sin(\u03c0v)|/\u03c0. (4) We also need to check that f is convex. Therefore it suffices to check that \u2202s(v,\u03bb)\u2202v = \u2212\u03c0\n2/ sin2(\u03c0v) \u2264 0. Thus we have checked that f(v, \u03bb) is a self-paced function corresponding to v\u2217(l, \u03bb).\nExample 2. v\u2217 is the classical sigmoid function. We want\nv\u2217(l, \u03bb) = 2\n1 + el/\u03bb . (5)\nThis v\u2217 is invariant under l/\u03bb Clearly v\u2217(l, \u03bb) \u2208 [0, 1] as l \u2265 0. By solving l in terms of v\u2217 and \u03bb, we have l = s(v, \u03bb) = \u03bb ln(2/v\u22121). Thus we have\nf(v, \u03bb) = \u2212 \u222b v s(v, \u03bb) = \u03bb((2\u2212 v) ln(2\u2212 v) + v ln v). (6)\nTo check that f is convex in v, we just need \u2202s\u2202v \u2264 0. This is the case since \u2202s\u2202v = 2\u03bb (v\u22122)v \u2264 0 when v \u2208 [0, 1]. Example 3. v\u2217 is the classical tanh function, another wellknown activation function.\nv\u2217(l, \u03bb) = 1\n1 + e2(l\u2212\u03bb) . (7)\nThis v\u2217 is invariant under l \u2212 \u03bb. Therefore l = s(v, \u03bb) = 1 2 ln(1/v \u2212 1) + \u03bb. By f(v, \u03bb) = \u2212 \u222b v s(v, \u03bb), we have\nf(v, \u03bb) = 1\n2 ((1\u2212 v) ln(1\u2212 v) + v ln v)\u2212 \u03bbv. (8)\nTo check that f is convex in v, we just need \u2202s\u2202v \u2264 0. This is the case since \u2202s\u2202v = 1 2(v\u22121)v \u2264 0 when v \u2208 [0, 1]. Example 4. v\u2217 is another well-known activation function, the exponential function, defined by\nv\u2217(l, \u03bb) = e\u2212l/\u03bb. (9)\nClearly this is an S-shaped curve in l. When l \u2192 0 we have v\u2217(l, \u03bb) \u2192 1. Also as l \u2192 \u221e, we have v\u2217(l, \u03bb) \u2192 0. v\u2217 is invariant under l/\u03bb. Similar to before, we have l = s(v, \u03bb) = \u03bb \u221a \u2212 ln v. Thus by f(v, \u03bb) = \u2212 \u222b v s(v, \u03bb) we have\nf(v, \u03bb) = \u03bb ( \u22122v \u2212 \u221a \u03c0Erf(\u2212 \u221a \u2212 ln v)\u221a\n\u2212 ln v\n)\u221a \u2212 ln v/2, (10)\nwhere Erf(x) is the error function. To check that f is convex in v, we just need \u2202s\u2202v \u2264 0. This is the case since \u2202s \u2202v = \u2212 1 2v \u221a \u2212 ln v which is no greater than 0 when v \u2208 [0, 1].\nIn general, after fixing one S-shaped function in terms of l, we can create a family of S-shaped functions g\u03bb(l) by deciding whether it is invariant under l/\u03bbc for some positive constant c or l\u2212\u03bb as shown in the examples above. By integrating the corresponding inverse function s(v, \u03bb) and then integrating s against v, we can find the self-paced function f(v, \u03bb).\n4 Algorithm 1 Multi-Label Self-Paced Learning Algorithm Input: Data matrix D, Number of groups m;\nRegularization parameters \u03b1 and \u03b2; Self-paced parameters: \u03bb, \u00b5 > 1;\n1. Initialize Q and A by K-means, and initialize W by bi-class SVM; 2. while not converge do 3. Update V by (3), (5), (7) or (9); 4. Update W by solving (13); 5. Update Q by solving (14); 6. Update A by (16); 7. \u03bb\u2190 \u03bb\u00b5; % update the learning pace 8. end while Output: W."}, {"heading": "C. Optimization", "text": "We adopt an alternating strategy to solve the optimization problem (2). We first rewrite the objective function (2) as:\nmin W,A,\nQ\u2208[0,1]m\u00d7n , V\u2208[0,1]n\u00d7L\nn\u2211 i=1 L\u2211 l=1 v (l) i L(wl,qi;xi, yil)+\u03b1 L\u2211 l=1 \u2016wl\u20162\n+\u03b2 n\u2211 i=1 m\u2211 j=1 qij\u2016yi\u2212aj\u20162 + n\u2211 i=1 L\u2211 l=1 f(v (l) i , \u03bb) (11)\ns.t. m\u2211 j=1 qij = 1,\u2200i \u2208 [1, n],\nwhere f(v(l)i , \u03bb) can be any function of (4), (6), (8) and (10) as the self-paced regularizer. i) Solving V with other variables fixed: the optimization function (11) can be decomposed in n\u00d7L individual problems for v(l)i as:\nmin v (l) i \u2208[0,1]\nv (l) i L(wl,qi;xi, yil) + f(v (l) i , \u03bb) (12)\nAccording to the discussion in Sec. III-B, the optimal solution v(l)i can be written in a closed form as (3), (5), (7), (9). ii) Solving W with other variables fixed: problem (11) can be decomposed into L individual problems for wl as:\nmin wl n\u2211 i=1 v (l) i L(wl,qi;xi, yil)+ \u03b1\u2016wl\u2016 2 (13)\nThis is a cost-sensitive SVM model, which can be solved by LIBSVM [26] software package. iii) Solving Q with other variables fixed: the optimization function (11) can be decomposed in n individual problems for qi as:\nmin qi L\u2211 l=1 v (l) i L(wl,qi;xi, yil) + \u03b2 m\u2211 j=1 qij\u2016yi\u2212aj\u20162 (14)\ns.t. m\u2211 j=1 qij = 1,qi \u2208 [0, 1]m\nThis is a linear programming program, which can be solved efficiently.\niv) Solving A with other variables fixed: To obtain A, we can optimize the following objective function:\nmin aj n\u2211 i=1 qij\u2016yi \u2212 aj\u20162 (15)\nTaking the derivative of (15) with respect to aj , and setting it to zero, we have\naj = n\u2211 i=1 qijyi (16)\nWe repeat the above process until the algorithm converges. Algorithm 1 summarizes the algorithm of multi-label selfpaced learning. For testing, we adopt the same strategy as that of ML-LOC, i.e., first predict the code qi for unseen test data xi, and then predict its labels yi based on xi and qi."}, {"heading": "IV. EXPERIMENTS", "text": "To verify the effectiveness of the proposed MLSPL, we perform our method on three benchmark datasets: the flags dataset, the scene dataset, and the emotions dataset.2 flags and scene are two image datasets, and emotions is a music dataset. They have 194, 593, and 2407 instances and 7, 6, 6 possible labels, respectively. To further evaluate MLSPL\u2019s performance, we compare it with several state-of-the-art multilabel learning algorithms3. We first compare with ML-LOC [1] that is the most related multi-label learning approach to ours. We also compare with ML-kNN [27] and RankSVM [7] that consider first-order and second order correlations, respectively. In addition, we compare with TRAM [28] that is proposed recently. Finally, we compare with another baseline BSVM [12] that learns a binary SVM for each label. LibSVM [26] is used to implement the SVM models for BSVM, MLLOC and MLSPL. For the compared methods, the parameters recommended in the corresponding literatures are used. In our method, the regularization parameters \u03b1 and \u03b2 are set the same with ML-LOC. The initial self-paced parameter \u03bb and \u00b5 is searched from {10\u22125, 10\u22124, 10\u22123, 10\u22122} and {1.1, 1.2, 1.3, 1.4, 1.5}, and then \u03bb is iteratively increased to make \u2018harder\u2019 label tasks and instances included gradually. We evaluate the performances of the compared approaches with five commonly used multi-label criteria: hamming loss, ranking loss, one error, coverage, and average precision. These criteria measure the performance from different aspects and the detailed definitions can be found in [29], [30]. In the following experiments, on each data set, we randomly select 30% instances as the training data, and use the rest 70% instances as the testing data. We repeat each experiment 10 times, and report the average results as well as standard deviations over the 10 repetitions.\n2These datasets can be downloaded from http://mulan.sourceforge.net/datasets-mlc.html\n3The codes of the compared methods are obtained from the corresponding authors.\n5\n6"}, {"heading": "A. General Performance", "text": "In this section, we test the general performance of our method on the three datasets. We use the sigmod activation function as our self-paced learning scheme in this experiment. Table I, II, and III summarize the performances of different methods in terms of the five evaluation criteria. Notice that for average precision, a larger value means a better performance, whereas for the other four criteria, the smaller, the better. From these tables, we can see that our method outperforms the other approaches significantly on all the three datasets. For example, on the flags dataset, our method achieves 13.4%, 8.4%, 25.8%, 4.1%, and 1.7% relative improvement in terms of the five evaluation criteria over TRAM that obtains the second best results on this dataset. In addition, as can be seen from the objective functions (1) and (2), ML-LOC is a special case to our method: when all the entries in V are 1, our method is reduced to ML-LOC. This shows that our method can improve the prediction performance of the model by jointly considering the complexities of the labels and the instances."}, {"heading": "B. Studying the Performance of Self-Paced Functions", "text": "In this section, we study the performance of different selfpaced functions on the three datasets. The results are shown in Figure 2. First of all, we can see that our method with different self-paced functions can achieve good performance. This shows that the self-paced functions we provide in the Sect. 2 are effective for multi-label learning. In addition, we observe that on the flags dataset, exponential function has the best performance in terms of all the five criteria except the Hamming distance, while on the emotions dataset, sigmod function outperforms all the other functions. Tanh function performs similar to Arctan in terms of all the five criteria on the three datasets. These points indicate that different scenarios indeed need different self-paced learning schemes. Therefore, it is necessary to develop more self-paced functions for multilabel learning."}, {"heading": "V. CONCLUSION", "text": "We proposed a novel multi-label learning algorithm, namely MLSPL. By introducing a self-paced regularizer, MLSPL can learn labels according to the order of labels and instances from easy to hard. Considering that real-world scenarios usually needs different learning schemes, we propose a general way to find the desired self-paced regularizer. Experiments on\nbenchmark datasets have demonstrated the effectiveness of SPMTL, compared to the state-of-the-art methods."}], "references": [{"title": "Multi-label learning by exploiting label correlations locally.", "author": ["S.-J. Huang", "Z.-H. Zhou"], "venue": "in AAAI,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2012}, {"title": "A review on multi-label learning algorithms", "author": ["M.-L. Zhang", "Z.-H. Zhou"], "venue": "IEEE Trans. on Knowledge and Data Engineering, vol. 26, no. 8, pp. 1819\u20131837, 2014.", "citeRegEx": "2", "shortCiteRegEx": null, "year": 1819}, {"title": "Multi-modal image annotation with multi-instance multi-label lda", "author": ["C.-T. Nguyen", "D.-C. Zhan", "Z.-H. Zhou"], "venue": "IJCAI, 2013.", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2013}, {"title": "Transductive multi-label learning for video concept detection", "author": ["J. Wang", "Y. Zhao", "X. Wu", "X.-S. Hua"], "venue": "MIR, 2008.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2008}, {"title": "Extracting shared subspace for multilabel classification", "author": ["S. Ji", "L. Tang", "S. Yu", "J. Ye"], "venue": "KDD, 2008.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2008}, {"title": "Multi-label multiple kernel learning by stochastic approximation: Application to visual object recognition", "author": ["S. Bucak", "R. Jin", "A.K. Jain"], "venue": "NIPS, 2010.", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2010}, {"title": "A kernel method for multi-labelled classification", "author": ["A. Elisseeff", "J. Weston"], "venue": "NIPS, 2001.", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2001}, {"title": "Hierarchical document categorization with support vector machines", "author": ["L. Cai", "T. Hofmann"], "venue": "CIKM, 2004.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2004}, {"title": "Learning hierarchical multi-category text classification models", "author": ["J. Rousu", "C. Saunders", "S. Szedmak", "J. Shawe-Taylor"], "venue": "JMLR, vol. 7, pp. 1601\u20131626, 2006.", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2006}, {"title": "Hierarchical classification: combining bayes with svm", "author": ["N. Cesa-Bianchi", "C. Gentile", "L. Zaniboni"], "venue": "ICML, 2006.", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2006}, {"title": "Large scale max-margin multi-label classification with priors", "author": ["B. Hariharan", "L. Zelnik-Manor", "M. Varma", "S. Vishwanathan"], "venue": "ICML, 2010.", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2010}, {"title": "Learning multi-label scene classification", "author": ["M.R. Boutell", "J. Luo", "X. Shen", "C.M. Brown"], "venue": "Pattern recognition, vol. 37, pp. 1757\u20131771, 2004.", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2004}, {"title": "Multi-labelled classification using maximum entropy method", "author": ["S. Zhu", "X. Ji", "W. Xu", "Y. Gong"], "venue": "SIGIR, 2005.", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2005}, {"title": "Model-shared subspace boosting for multi-label classification", "author": ["R. Yan", "J. Tesic", "J.R. Smith"], "venue": "KDD, 2007.", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2007}, {"title": "Correlative multi-label video annotation", "author": ["G.-J. Qi", "X.-S. Hua", "Y. Rui", "J. Tang", "T. Mei", "H.-J. Zhang"], "venue": "MM, 2007.", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2007}, {"title": "Multi-label learning by exploiting label dependency", "author": ["M.-L. Zhang", "K. Zhang"], "venue": "KDD, 2010.", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2010}, {"title": "Multi-instance multi-label learning with weak label", "author": ["S.-J. Yang", "Y. Jiang", "Z.-H. Zhou"], "venue": "IJCAI, 2013.", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2013}, {"title": "Active learning with multi-label svm classification", "author": ["X. Li", "Y. Guo"], "venue": "IJCAI, 2013.", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2013}, {"title": "Multi-label multiple kernel learning", "author": ["S. Ji", "L. Sun", "R. Jin", "J. Ye"], "venue": "NIPS, 2009.", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2009}, {"title": "Self-paced learning for latent variable models", "author": ["M.P. Kumar", "B. Packer", "D. Koller"], "venue": "NIPS, 2010.", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2010}, {"title": "Selfpaced learning with diversity", "author": ["L. Jiang", "D. Meng", "S.-I. Yu", "Z. Lan", "S. Shan", "A. Hauptmann"], "venue": "NIPS, 2014.", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2014}, {"title": "Self-paced learning for matrix factorization", "author": ["Q. Zhao", "D. Meng", "L. Jiang", "Q. Xie", "Z. Xu", "A.G. Hauptmann"], "venue": "AAAI, 2015.", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2015}, {"title": "Multi-view self-paced learning for clustering", "author": ["C. Xu", "D. Tao", "C. Xu"], "venue": "IJCAI, 2015.", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2015}, {"title": "A selfpaced multiple-instance learning framework for co-saliency detection", "author": ["D. Zhang", "D. Meng", "C. Li", "L. Jiang", "Q. Zhao", "J. Han"], "venue": "ICCV, 2015.", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2015}, {"title": "Self-paced curriculum learning", "author": ["L. Jiang", "D. Meng", "Q. Zhao", "S. Shan", "A.G. Hauptmann"], "venue": "AAAI, 2015.  7", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2015}, {"title": "Libsvm: A library for support vector machines", "author": ["C.-C. Chang", "C.-J. Lin"], "venue": "TIST, vol. 2, no. 3, p. 27, 2011.", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2011}, {"title": "Ml-knn: A lazy learning approach to multi-label learning", "author": ["M.-L. Zhang", "Z.-H. Zhou"], "venue": "Pattern recognition, vol. 40, no. 7, pp. 2038\u20132048, 2007.", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2007}, {"title": "Transductive multilabel learning via label set propagation", "author": ["X. Kong", "M.K. Ng", "Z.-H. Zhou"], "venue": "TKDE, vol. 25, no. 3, pp. 704\u2013719, 2013.", "citeRegEx": "28", "shortCiteRegEx": null, "year": 2013}, {"title": "Boostexter: A boosting-based system for text categorization", "author": ["R.E. Schapire", "Y. Singer"], "venue": "Machine learning, vol. 39, no. 2, pp. 135\u2013168, 2000.", "citeRegEx": "29", "shortCiteRegEx": null, "year": 2000}, {"title": "Multi-instance multi-label learning", "author": ["Z.-H. Zhou", "M.-L. Zhang", "S.-J. Huang", "Y.-F. Li"], "venue": "Artificial Intelligence, vol. 176, no. 1, pp. 2291\u2013 2320, 2012.", "citeRegEx": "30", "shortCiteRegEx": null, "year": 2012}], "referenceMentions": [{"referenceID": 0, "context": "INTRODUCTION Multi-label learning has attracted much attention in the past decade [1], [2].", "startOffset": 82, "endOffset": 85}, {"referenceID": 1, "context": "INTRODUCTION Multi-label learning has attracted much attention in the past decade [1], [2].", "startOffset": 87, "endOffset": 90}, {"referenceID": 2, "context": "Due to its empirical success, multi-label learning has been widely applied to various domains including image annotation [3], video concept detection [4], web page categorization [5], and visual object recognition [6].", "startOffset": 121, "endOffset": 124}, {"referenceID": 3, "context": "Due to its empirical success, multi-label learning has been widely applied to various domains including image annotation [3], video concept detection [4], web page categorization [5], and visual object recognition [6].", "startOffset": 150, "endOffset": 153}, {"referenceID": 4, "context": "Due to its empirical success, multi-label learning has been widely applied to various domains including image annotation [3], video concept detection [4], web page categorization [5], and visual object recognition [6].", "startOffset": 179, "endOffset": 182}, {"referenceID": 5, "context": "Due to its empirical success, multi-label learning has been widely applied to various domains including image annotation [3], video concept detection [4], web page categorization [5], and visual object recognition [6].", "startOffset": 214, "endOffset": 217}, {"referenceID": 6, "context": "However, such a solution does not consider the relationship among labels, whereas previous studies [7], [1] have revealed that the label relationship is quite helpful and should be considered.", "startOffset": 99, "endOffset": 102}, {"referenceID": 0, "context": "However, such a solution does not consider the relationship among labels, whereas previous studies [7], [1] have revealed that the label relationship is quite helpful and should be considered.", "startOffset": 104, "endOffset": 107}, {"referenceID": 7, "context": "Therefore, several approaches attempt to exploit label correlations by incorporating external prior knowledge [8], [9], [10], [11].", "startOffset": 110, "endOffset": 113}, {"referenceID": 8, "context": "Therefore, several approaches attempt to exploit label correlations by incorporating external prior knowledge [8], [9], [10], [11].", "startOffset": 115, "endOffset": 118}, {"referenceID": 9, "context": "Therefore, several approaches attempt to exploit label correlations by incorporating external prior knowledge [8], [9], [10], [11].", "startOffset": 120, "endOffset": 124}, {"referenceID": 10, "context": "Therefore, several approaches attempt to exploit label correlations by incorporating external prior knowledge [8], [9], [10], [11].", "startOffset": 126, "endOffset": 130}, {"referenceID": 6, "context": "Considering that the prior knowledge is often unavailable in real applications, many other approaches [7], [12], [13], [14],", "startOffset": 102, "endOffset": 105}, {"referenceID": 11, "context": "Considering that the prior knowledge is often unavailable in real applications, many other approaches [7], [12], [13], [14],", "startOffset": 107, "endOffset": 111}, {"referenceID": 12, "context": "Considering that the prior knowledge is often unavailable in real applications, many other approaches [7], [12], [13], [14],", "startOffset": 113, "endOffset": 117}, {"referenceID": 13, "context": "Considering that the prior knowledge is often unavailable in real applications, many other approaches [7], [12], [13], [14],", "startOffset": 119, "endOffset": 123}, {"referenceID": 14, "context": "[15], [16], [1] try to mine label relationships based on training data and incorporate the label correlations into the learning process of multi-label model.", "startOffset": 0, "endOffset": 4}, {"referenceID": 15, "context": "[15], [16], [1] try to mine label relationships based on training data and incorporate the label correlations into the learning process of multi-label model.", "startOffset": 6, "endOffset": 10}, {"referenceID": 0, "context": "[15], [16], [1] try to mine label relationships based on training data and incorporate the label correlations into the learning process of multi-label model.", "startOffset": 12, "endOffset": 15}, {"referenceID": 16, "context": "In addition, there are also many works focusing on leveraging other learning techniques for multi-label learning, such as multi-instance multi-label learning [17], active learning for multi-label learning [18], and multi-label learning combined with multi-kernel learning [19].", "startOffset": 158, "endOffset": 162}, {"referenceID": 17, "context": "In addition, there are also many works focusing on leveraging other learning techniques for multi-label learning, such as multi-instance multi-label learning [17], active learning for multi-label learning [18], and multi-label learning combined with multi-kernel learning [19].", "startOffset": 205, "endOffset": 209}, {"referenceID": 18, "context": "In addition, there are also many works focusing on leveraging other learning techniques for multi-label learning, such as multi-instance multi-label learning [17], active learning for multi-label learning [18], and multi-label learning combined with multi-kernel learning [19].", "startOffset": 272, "endOffset": 276}, {"referenceID": 19, "context": "Inspired by how children learn concepts, self-paced learning [20] advocates a paradigm that learning should first consider \u2018simple\u2019 or \u2018easy\u2019 instances, and then gradually take \u2018complex\u2019 or \u2018hard\u2019 instances into account.", "startOffset": 61, "endOffset": 65}, {"referenceID": 19, "context": "By simulating such a process of human learning, it has been empirically verified that self-paced learning can mitigate the problem of local-minima during iterative learning [20], [21], and exhibit better generalization behavior in many tasks, such as matrix factorization [22], multi-view learning [23], and multi-instance learning [24].", "startOffset": 173, "endOffset": 177}, {"referenceID": 20, "context": "By simulating such a process of human learning, it has been empirically verified that self-paced learning can mitigate the problem of local-minima during iterative learning [20], [21], and exhibit better generalization behavior in many tasks, such as matrix factorization [22], multi-view learning [23], and multi-instance learning [24].", "startOffset": 179, "endOffset": 183}, {"referenceID": 21, "context": "By simulating such a process of human learning, it has been empirically verified that self-paced learning can mitigate the problem of local-minima during iterative learning [20], [21], and exhibit better generalization behavior in many tasks, such as matrix factorization [22], multi-view learning [23], and multi-instance learning [24].", "startOffset": 272, "endOffset": 276}, {"referenceID": 22, "context": "By simulating such a process of human learning, it has been empirically verified that self-paced learning can mitigate the problem of local-minima during iterative learning [20], [21], and exhibit better generalization behavior in many tasks, such as matrix factorization [22], multi-view learning [23], and multi-instance learning [24].", "startOffset": 298, "endOffset": 302}, {"referenceID": 23, "context": "By simulating such a process of human learning, it has been empirically verified that self-paced learning can mitigate the problem of local-minima during iterative learning [20], [21], and exhibit better generalization behavior in many tasks, such as matrix factorization [22], multi-view learning [23], and multi-instance learning [24].", "startOffset": 332, "endOffset": 336}, {"referenceID": 0, "context": "BACKGROUND We first define notations and then briefly introduce the work [1] that our approach is originated from.", "startOffset": 73, "endOffset": 76}, {"referenceID": 0, "context": "Among these methods, one representative algorithm is ML-LOC [1] which tries to exploit the correlations locally; it assumes that the instances are partitioned into m different clusters and each cluster shares a subset of label correlations.", "startOffset": 60, "endOffset": 63}, {"referenceID": 0, "context": "min W,A, Q\u2208[0,1]m\u00d7n L \u2211", "startOffset": 11, "endOffset": 16}, {"referenceID": 19, "context": "Indeed, selfpaced learning has empirically demonstrated its usefulness for mitigating bad local minima and achieving better model generalization [20], [25].", "startOffset": 145, "endOffset": 149}, {"referenceID": 24, "context": "Indeed, selfpaced learning has empirically demonstrated its usefulness for mitigating bad local minima and achieving better model generalization [20], [25].", "startOffset": 151, "endOffset": 155}, {"referenceID": 0, "context": "min W,A, Q\u2208[0,1]m\u00d7n , V\u2208[0,1]n\u00d7L n \u2211", "startOffset": 11, "endOffset": 16}, {"referenceID": 0, "context": "min W,A, Q\u2208[0,1]m\u00d7n , V\u2208[0,1]n\u00d7L n \u2211", "startOffset": 24, "endOffset": 29}, {"referenceID": 19, "context": "applications [20], [24], [25], there lacks of a general method to derive the self-paced functions.", "startOffset": 13, "endOffset": 17}, {"referenceID": 23, "context": "applications [20], [24], [25], there lacks of a general method to derive the self-paced functions.", "startOffset": 19, "endOffset": 23}, {"referenceID": 24, "context": "applications [20], [24], [25], there lacks of a general method to derive the self-paced functions.", "startOffset": 25, "endOffset": 29}, {"referenceID": 21, "context": "Self-Paced Function First, we introduce definition of the self-paced function from the recent work [22]:", "startOffset": 99, "endOffset": 103}, {"referenceID": 0, "context": "f(v, \u03bb) is called self-paced function, if 1) f(v, \u03bb) is convex with respect to v \u2208 [0, 1]; 2) v\u2217 is monotonically decreasing with respect to l, and it holds that liml\u21920 v\u2217 \u2264 1, liml\u2192\u221e v\u2217 = 0; 3) v\u2217 is monotonically increasing with respect to \u03bb, and it holds that lim\u03bb\u21920 v\u2217 = 0, lim\u03bb\u2192\u221e v\u2217 \u2264 1; where v\u2217(l, \u03bb) = arg minv\u2208[0,1] vl + f(v, \u03bb) for fixed l, \u03bb.", "startOffset": 83, "endOffset": 89}, {"referenceID": 0, "context": "f(v, \u03bb) is called self-paced function, if 1) f(v, \u03bb) is convex with respect to v \u2208 [0, 1]; 2) v\u2217 is monotonically decreasing with respect to l, and it holds that liml\u21920 v\u2217 \u2264 1, liml\u2192\u221e v\u2217 = 0; 3) v\u2217 is monotonically increasing with respect to \u03bb, and it holds that lim\u03bb\u21920 v\u2217 = 0, lim\u03bb\u2192\u221e v\u2217 \u2264 1; where v\u2217(l, \u03bb) = arg minv\u2208[0,1] vl + f(v, \u03bb) for fixed l, \u03bb.", "startOffset": 319, "endOffset": 324}, {"referenceID": 0, "context": "In order to find the self-paced function f , we first find a family of S-shaped functions g\u03bb(l) with range in [0, 1] such that it is monotonically decreasing with respect to l, and that liml\u2192\u221e g\u03bb(l) = 0 as well as liml\u21920 g\u03bb(l) \u2264 1.", "startOffset": 110, "endOffset": 116}, {"referenceID": 0, "context": "A smooth function f(v, \u03bb) is a self-paced function corresponding to v\u2217 if and only if \u2202f(v,\u03bb) \u2202v = \u2212s(\u03bb, v) and \u2202s(v,\u03bb) \u2202v \u2264 0 for v \u2208 [0, 1].", "startOffset": 135, "endOffset": 141}, {"referenceID": 0, "context": "This v\u2217 is invariant under l/\u03bb Clearly v\u2217(l, \u03bb) \u2208 [0, 1] as l \u2265 0.", "startOffset": 50, "endOffset": 56}, {"referenceID": 0, "context": "This is the case since \u2202s \u2202v = 2\u03bb (v\u22122)v \u2264 0 when v \u2208 [0, 1].", "startOffset": 54, "endOffset": 60}, {"referenceID": 0, "context": "This is the case since \u2202s \u2202v = 1 2(v\u22121)v \u2264 0 when v \u2208 [0, 1].", "startOffset": 54, "endOffset": 60}, {"referenceID": 0, "context": "This is the case since \u2202s \u2202v = \u2212 1 2v \u221a \u2212 ln v which is no greater than 0 when v \u2208 [0, 1].", "startOffset": 83, "endOffset": 89}, {"referenceID": 0, "context": "min W,A, Q\u2208[0,1]m\u00d7n , V\u2208[0,1]n\u00d7L n \u2211", "startOffset": 11, "endOffset": 16}, {"referenceID": 0, "context": "min W,A, Q\u2208[0,1]m\u00d7n , V\u2208[0,1]n\u00d7L n \u2211", "startOffset": 24, "endOffset": 29}, {"referenceID": 0, "context": "min v (l) i \u2208[0,1] v (l) i L(wl,qi;xi, yil) + f(v (l) i , \u03bb) (12)", "startOffset": 13, "endOffset": 18}, {"referenceID": 25, "context": "This is a cost-sensitive SVM model, which can be solved by LIBSVM [26] software package.", "startOffset": 66, "endOffset": 70}, {"referenceID": 0, "context": "j=1 qij = 1,qi \u2208 [0, 1]", "startOffset": 17, "endOffset": 23}, {"referenceID": 0, "context": "We first compare with ML-LOC [1] that is the most related multi-label learning approach to ours.", "startOffset": 29, "endOffset": 32}, {"referenceID": 26, "context": "We also compare with ML-kNN [27] and RankSVM [7] that consider first-order and second order correlations, respectively.", "startOffset": 28, "endOffset": 32}, {"referenceID": 6, "context": "We also compare with ML-kNN [27] and RankSVM [7] that consider first-order and second order correlations, respectively.", "startOffset": 45, "endOffset": 48}, {"referenceID": 27, "context": "In addition, we compare with TRAM [28] that is proposed recently.", "startOffset": 34, "endOffset": 38}, {"referenceID": 11, "context": "Finally, we compare with another baseline BSVM [12] that learns a binary SVM for each label.", "startOffset": 47, "endOffset": 51}, {"referenceID": 25, "context": "LibSVM [26] is used to implement the SVM models for BSVM, MLLOC and MLSPL.", "startOffset": 7, "endOffset": 11}, {"referenceID": 28, "context": "These criteria measure the performance from different aspects and the detailed definitions can be found in [29], [30].", "startOffset": 107, "endOffset": 111}, {"referenceID": 29, "context": "These criteria measure the performance from different aspects and the detailed definitions can be found in [29], [30].", "startOffset": 113, "endOffset": 117}], "year": 2017, "abstractText": "In this paper, we propose a novel multi-label learning framework, called Multi-Label Self-Paced Learning (MLSPL), in an attempt to incorporate the self-paced learning strategy into multi-label learning regime. In light of the benefits of adopting the easy-to-hard strategy proposed by self-paced learning, the devised MLSPL aims to learn multiple labels jointly by gradually including label learning tasks and instances into model training from the easy to the hard. We first introduce a self-paced function as a regularizer in the multi-label learning formulation, so as to simultaneously rank priorities of the label learning tasks and the instances in each learning iteration. Considering that different multi-label learning scenarios often need different self-paced schemes during optimization, we thus propose a general way to find the desired self-paced functions. Experimental results on three benchmark datasets suggest the state-of-the-art performance of our approach.", "creator": "LaTeX with hyperref package"}}}