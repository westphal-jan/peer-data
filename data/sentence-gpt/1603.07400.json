{"id": "1603.07400", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "24-Mar-2016", "title": "A Reconfigurable Low Power High Throughput Architecture for Deep Network Training", "abstract": "General purpose computing systems are used for a large variety of applications. Extensive supports for flexibility in these systems limit their energy efficiencies. Given that big data applications are among the main emerging workloads for computing systems, specialized architectures for big data processing are needed to enable low power and high throughput execution. This can lead to a more complex process, with high performance applications.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n", "histories": [["v1", "Thu, 24 Mar 2016 00:52:22 GMT  (1436kb)", "http://arxiv.org/abs/1603.07400v1", "11 pages"], ["v2", "Wed, 15 Jun 2016 01:26:31 GMT  (1111kb)", "http://arxiv.org/abs/1603.07400v2", "9 pages"]], "COMMENTS": "11 pages", "reviews": [], "SUBJECTS": "cs.LG cs.AR cs.DC", "authors": ["raqibul hasan", "tarek taha"], "accepted": false, "id": "1603.07400"}, "pdf": {"name": "1603.07400.pdf", "metadata": {"source": "CRF", "title": null, "authors": [], "emails": [], "sections": [{"heading": null, "text": "large variety of applications. Extensive supports for flexibility in these systems limit their energy efficiencies. Given that big data applications are among the main emerging workloads for computing systems, specialized architectures for big data processing are needed to enable low power and high throughput execution. Several big data applications are particularly focused on classification and clustering tasks. In this paper we propose a multicore heterogeneous architecture for big data processing. This system has the capability to process key machine learning algorithms such as deep neural network, autoencoder, and kmeans clustering. Memristor crossbars are utilized to provide low power high throughput execution of neural networks. The system has both training and recognition (evaluation of new input) capabilities. The proposed system could be used for classification, unsupervised clustering, dimensionality reduction, feature extraction, and anomaly detection applications. The system level area and power benefits of the specialized architecture is compared with the NVIDIA Telsa K20 GPGPU. Our experimental evaluations show that the proposed architecture can provide four to six orders of magnitude more energy efficiency over GPGPUs for big data processing.\nKeywords\u2013Low power architecture; memristor crossbars;\nautoencoder; unsupervised training; big data.\nI. INTRODUCTION\nith the increasingly large volumes of data being generated in almost all fields including commerce,\nscience, medicine, security, and social networks, one of the\nkey application drivers of future computing systems is the processing of these data to decipher patterns within it to help\nmake better decisions. This process is typically described as\nbig data analytics and involves machine learning to analyze and categorize the data.\nTwo broad categories of machine learning algorithms are supervised and unsupervised learning. In supervised learning,\nwe know exactly what classes of data exist and a sample of\ndata for each class is available (this is described as labeled data). Supervised machine learning algorithms are trained on\nthis labeled datasets, and once trained, the algorithms can then sort out new unlabeled data into the existing set of classes.\nA more challenging, and perhaps more common, form of\nbig data analytics involves unlabeled data. In this case the\nmachine learning algorithms need to examine large volumes\nof data to uncover hidden patterns, unknown correlations, and other useful information. Some of the most commonly used\nunsupervised processing includes, autoencoder, restricted\nBoltzmann machines, self organizing maps, and k-means clustering.\nBig data processing using machine learning algorithms have traditionally been run on clusters of GPUs. One of the\nmajor problems with these systems is their high power\nconsumption and the need for higher throughput. Due to the limits of Dennard scaling and Moore\u2019s Law, alternative\narchitectures are gaining importance. Several studies have examined low power, high throughput architectures for\nmachine learning applications. Examples of this include\nDaDianNao [1] and PuDianNao [2], both of which accelerate machine learning algorithms through digital circuits. In\nPuDianNao neuron synaptic weights are stored in the off-chip\nmemory, and hence requires back and forth data movement\nduring training, between off-chip memory and the processing\nchip. In DaDianNao neuron synaptic weights are stored in eDRAM and are later brought into Neural Functional Units\nfor execution. Input data are initially stored in the central\neDRAM. This system utilized dynamic wormhole routing for data transfers.\nThis paper presents a memristor crossbar based multicore architecture to implement unsupervised training of streaming\ndata. It is a heterogeneous architecture consisting of\nmemristor neural cores, a digital unit for clustering, a RISC core for system configuration and initialization, and an on-\nchip routing network for data communication. The key\nbenefits of this approach are high throughput at very low power and area consumption. In this paper we utilize\nmemristor crossbars for neural network based system design\nand examine supervised and unsupervised learning of the\nsystem. The proposed system could be used for classification,\nunsupervised clustering, dimensionality reduction, feature extraction and anomaly detection applications.\nMemristors [3,4] have received significant attention as a potential building block for neuromorphic systems [5,6]. In\nthese systems memristors are used in a crossbar structure to\nefficiently evaluate multiply-add operations in parallel in the analog domain (these are the dominant operations in neural\nW\nnetworks). This enables highly dense neuromorphic systems\nwith great computational efficiency [7]. We are using\nmemristor crossbars which provide high synaptic weight\ndensity and parallel analog processing consuming very low energy. Memristor based synapses are 146 times denser than\ntraditional digital SRAM synapse (assuming 8 bits precision\nin both cases). In this system processing happens at physical location of the data. Data transfer energy and functional unit\nenergy consumptions are saved significantly.\nThe most recent results for memristor based neuromorphic systems can be seen in [12-15]. These works do not examine\nmulticore design and system level evaluations are not studied. [12,13] examined only linearly separable problems. [16]\nexamined memristor based neural networks utilizing four\nmemristors per synapse where the proposed systems utilize two memristors per synapse. They propose deployment of the\nmemristor based system as neural accelerator with RISC system while proposed systems are standalone embedded\nprocessing architecture. Furthermore, the proposed system\nhas on-chip unsupervised learning capability.\nDetailed circuit level simulations (in SPICE) of memristor crossbars were carried out to verify the neural operations. Both the programming and the recognition phase of the neural networks were examined. These simulations utilized an accurate memristor device model, and take low level circuit details such as wire resistance, capacitance, and driver circuits into consideration. We have examined the power, area, and performance of the proposed multicore system and compared them with a GPU based system. Our results indicate that the memristor based architecture can provide up to six orders of magnitude more energy efficiency over GPU for the selected benchmarks.\nThe rest of the paper is organized as follows: section II\ndemonstrates overall heterogeneous multicore architecture. Section III describes memristor device, neuron circuit design,\nautoencoder and memristor based autoencoder design.\nSection IV describes designs of the heterogeneous cores. Sections V and VI describe experimental setup and results\nrespectively. Section VII describes related works in the area\nand finally in section VIII we summarize our work."}, {"heading": "II. SYSTEM OVERVIEW", "text": "In this paper we propose an energy efficient multicore heterogeneous architecture to accelerate both supervised and unsupervised machine learning applications. The architecture has implementation of the backpropagation training algorithm, autoencoder, deep neural network, and k-means clustering algorithm. These are some of the fundamental algorithms in deep learning and are used for big data processing.\nA common approach for unsupervised learning is clustering which uses dimensionality reduction and feature extraction as preprocessing. Dimensionality reduction allows the data of large dimension to be represented using a smaller set of features, and thus allow a simpler clustering algorithm to process an unlabeled dataset. In this paper, we implement the commonly used autoencoder algorithm for dimensionality\nreduction and the k-means algorithm for clustering. The autoencoder is based on multilayered neural networks and thus is implemented using memristor based neural processing cores. As the input data can be of high dimensionality (such as a high resolution image), multiple neural cores are needed to implement the autoencoder.\nWe utilize the back-propagation algorithm for training the autoencoder. Due to the use of the autoencoder, a reduced dimension k-means implementation is sufficient, and hence we utilize a single digital core to implement k-means clustering. Deep neural network applications for supervised training could also be implemented in the proposed system. For deep network, autoencoder is used for the unsupervised layer wise pre training and supervised fine tuning is performed on the pre trained weights utilizing the backpropagation algorithm. The system has both training and inference capabilities. It can be used for classification, unsupervised clustering, dimensionality reduction, feature extraction, and anomaly detection applications.\nwith several neural cores (NC), one clustering core connected through a 2-D\nmesh routing network. (R: router).\nFig. 1 shows the overall system architecture which includes memristor crossbar based neural cores (NC) and a digital unit for clustering. Memristor crossbar neural cores are utilized to provide low power, high throughput execution of neural networks. The cores in the system are connected through an on-chip routing network (R). The neural cores receive their inputs from main memory holding the training data through the on-chip routers and a buffer between the routing and main memory as shown in Fig. 1.\nSince the proposed architecture is geared towards machine learning applications (including big data processing), the training data will be used multiple times during the training phase. As a result, access to a high bandwidth memory system is needed, and thus we propose the use of 3D stacked DRAM. Using through Silicon vias reduces memory access time and energy, thereby reducing overall system power. To allow efficient access to the main memory utilizing a DMA controller that is initialized by a RISC processing core. A single issue pipelined RISC core is used to reduce power consumption.\nAn on-chip routing network is needed to transfer neuron outputs among cores in a multicore system. In feed-forward neural networks, the outputs of a neuron layer are sent to the following layer after every iteration (as opposed to a spiking network, where outputs are sent only if a neuron fires). This means that the communication between neurons is deterministic and hence a static routing network can be used for the core-to-core communications. In this study, we assumed a static routing network as this would be lower power consuming than a dynamic routing network. The network is statically time multiplexed between cores for exchanging multiple neuron outputs.\nSRAM based static routing is utilized to facilitate reprogrammability in the switches. Fig. 2 shows the routing switch design. Note carefully that the switch allows outputs from a core to be routed back into the core to implement recurrent networks or multi-layer networks where all the neuron layers are within the same core."}, {"heading": "III. AUTOENCODER AND MEMRISTOR NEURON CIRCUIT", "text": ""}, {"heading": "A. Memristor Devices", "text": "The memristor device was first theorized in 1971 by Dr. Leon Chua [3]. Memristors are essentially nanoscale variable resistors, where the resistance is modified if a voltage greater than a write threshold voltage (Vth) is applied [17]. Hence their resistance can be read by applying a voltage below Vth across them without changing the device state. Several research groups have demonstrated memristive behavior using different materials. One such device, composed of layers of HfOx and AlOx [18], has a high on state resistance (RON\u224810k\u2126), a very high resistance ratio (ROFF/RON\u22481000) and a write threshold voltage of about 1.3 V. Physical memristors can be laid out in a high density grid known as a crossbar. The schematic and layout of a memristor crossbar can each be seen in Fig. 3. A memristor in the crossbar structure occupies 4F2 area (where F is the device feature size). This is 36 times smaller than a SRAM memory cell. A memristor crossbar can perform many multiply-add operations in parallel and the conductance of multiple memristors in a crossbar can be updated in parallel. Multiplyadd operations are the dominant operations in neural networks and training of neural networks require update of synaptic weights iteratively. As a consequence, memristors have a\ngreat potential as a synaptic element in a neural network based system design."}, {"heading": "B. Neuron circuit", "text": "In this paper we have utilized memristors as neuron synaptic weights. The circuit in Fig. 5 shows the memristor based neuron circuit design utilized in this paper. In this circuit, each data input is connected to two virtually grounded op-amps (operational amplifiers) through a pair of memristors. For a given row, if the conductance of a memristor connected to the first column (\u03c3A+) is higher than the conductance of the memristor connected to the second column (\u03c3A-), then the pair of memristors represents a positive synaptic weight. In the inverse situation, when \u03c3A+ < \u03c3A-, the memristor pair represents a negative synaptic weight. As the op-amps in Fig. 5 virtually ground the two neuron columns, the currents through the first and second columns can be calculated as (\ud835\udc34\ud835\udf0e\ud835\udc34+ + \u22ef + \ud835\udefd\ud835\udf0e\ud835\udefd+) and (\ud835\udc34\ud835\udf0e\ud835\udc34\u2212 + \u22ef + \ud835\udefd\ud835\udf0e\ud835\udefd\u2212) respectively.\nBA\nAssume that\n\ud835\udc37\ud835\udc43\ud835\udc57 = 4\ud835\udc45\ud835\udc53[{\ud835\udf0e\ud835\udc34+ + \u22ef + \ud835\udefd\ud835\udf0e\ud835\udefd+} \u2212 {\ud835\udf0e\ud835\udc34\u2212 + \u22ef + \ud835\udefd\ud835\udf0e\ud835\udefd\u2212}]\n= 4\ud835\udc45\ud835\udc53[\ud835\udc34(\ud835\udf0e\ud835\udc34+ \u2212 \ud835\udf0e\ud835\udc34\u2212) + \u22ef + \ud835\udefd(\ud835\udf0e\ud835\udefd+ \u2212 \ud835\udf0e\ud835\udefd\u2212)]\nThe output, yj of the op-amp connected directly with the second column represents the neuron output. When the power rails of the op-amps, VDD and VSS are set to 0.5V and -0.5V respectively, the neuron circuit implements the activation function h(x) as in Eq. (3) where \ud835\udc65 = 4\ud835\udc45\ud835\udc53[\ud835\udc34(\ud835\udf0e\ud835\udc34+ \u2212 \ud835\udf0e\ud835\udc34\u2212) + \u22ef + \ud835\udefd(\ud835\udf0e\ud835\udefd+ \u2212 \ud835\udf0e\ud835\udefd\u2212)]. This implies, the neuron output can be expressed as h(DPj). Fig. 6 shows that h(x) closely approximates the activation function, \ud835\udc53(\ud835\udc65) = 1 1+\ud835\udc52\u2212\ud835\udc65 \u2212 0.5. The values of VDD and VSS are chosen such that no memristor gets a voltage greater than Vth across it during evaluation."}, {"heading": "C. Autoencoder", "text": "Several big data applications are particularly focused on classification and clustering tasks. The robustness of such systems depends on how well they can extract important features from the raw data. For big data processing we are interested for a generic feature extraction mechanism which will work for a variety of applications. The autoencoder [19] is a popular method for dimensionality reduction and feature extraction approach which automatically learns features from unlabeled data. This means, it is an unsupervised approach and no supervision is needed.\nThe architecture of the autoencoder is similar to a multilayer neural network as shown in Fig. 7. An autoencoder tries to learn the function hW,b(x) \u2248 x. That is, it tries to learn an approximation to the identity function, such that output x\u2019 is similar to input x. By placing constraints on the network, such\nas by limiting the number of hidden units, we can discover useful patterns within the data.\nAs a concrete example, suppose the inputs x are the pixel intensity values from a 10\u00d710 image (100 pixels) so n = 100. An autoencoder determining compressed representation of these data has 100 inputs, 50 neurons in the hidden layer and 100 output neurons. The network is trained such that output layer neurons generate same values as the applied inputs. Since there are only 50 hidden units, the network is forced to\nlearn a compressed representation of the input. This paper utilizes memristor devices to develop novel extreme low power implementations of on-chip training circuitry for unsupervised training of the autoencoder."}, {"heading": "D. Memristor Based Neural Network Implementation", "text": "The structures of a multi-layer neural network and an autoencoder are similar. Both can be viewed as a feed forward neural network. Training of the two systems are different. The autoencoder is trained layer by layer. The training of each layer is similar to a two layer neural network training where a temporarily added second layer tries to learn the inputs applied to first layer (detail in previous subsection). Fig. 7 shows a simple two layer feed forward neural network with four inputs, four outputs, and three hidden layer neurons. Fig. 8 shows a memristor crossbar based circuit that can be used to evaluate the neural network in Fig. 7. There are two memristor crossbars in this circuit, each representing a layer of neurons. Each crossbar utilizes the neuron circuit shown in Fig. 5.\nIn Fig. 8, the first layer of the neurons is implemented using an 5\u00d76 crossbar. The second layer of two neurons is implemented using a 4\u00d78 memristor crossbar, where 3 of the inputs are coming from the 3 neuron outputs of the first crossbar. One additional input is used for bias. Applying the inputs to a crossbar, an entire layer of neurons can be processed in parallel within one cycle."}, {"heading": "E. The Training Algorithm", "text": "In order to provide proper functionality, a multi-layer neural network needs to be trained using a training algorithm. Back-propagation (BP) [20] and the variants of the BP algorithm are widely used for training such networks. The stochastic BP algorithm is used to train the memristor based multi-layer neural network and is described below:\n1) Initialize the memristors with high random resistances. 2) For each input pattern x:\ni) Apply the input pattern x to the crossbar circuit and evaluate the DPj values and outputs (yj) of all\nneurons (hidden neurons and output neurons).\nii) For each output layer neuron j, calculate the error, \u03b4j, between the neuron output (yj) and the target\noutput (tj). \ud835\udeff\ud835\udc57 = (\ud835\udc61\ud835\udc57 \u2212 \ud835\udc66\ud835\udc57) (4) iii) Back propagate the error for each hidden layer neuron j.\n\ud835\udeff\ud835\udc57 = \u2211 \ud835\udeff\ud835\udc58\ud835\udc64\ud835\udc58,\ud835\udc57\ud835\udc58 (5) where neuron k is connected to the previous layer neuron j. iv) Determine the amount, \u0394w, that each neuron\u2019s synapses should be changed (2\u03b7 is the learning\nrate and f is the activation function): \u0394\ud835\udc64\ud835\udc57 = 2\ud835\udf02 \u00d7 \ud835\udeff\ud835\udc57 \u00d7 \ud835\udc53\u2032(\ud835\udc37\ud835\udc43\ud835\udc57) \u00d7 \ud835\udc65 (6) If the error in the output layer has not converged to a sufficiently small value, goto step 2."}, {"heading": "F. Circuit Implementation of the Training Algorithm", "text": "The circuit implementation of the training algorithm for the neural network in Fig. 8 can be broken down into following major steps:\n1. Apply inputs to layer 1 and record the layer 2 neuron output errors. 2. Back-propagate layer 2 errors through the second\nlayer weights and record the layer 1 errors.\n3. Update the synaptic weights.\nThe circuit implementations of these steps are detailed below:\nStep 1: A set of inputs is applied to the layer 1 neurons, and the layer 2 neuron outputs are measured. This process is\nshown in Fig. 8. Errors are discretized into 8 bit representations (one sign bit and 7 bits for magnitude).\nStep 2: The layer 2 errors (\u03b4L2,1 and \u03b4L2,2) are applied to the layer 2 weights after conversion from digital to analog form as shown in Fig. 9 to generate the layer 1 errors (\u03b4L1,1 to \u03b4L1,6). The memristor crossbar in Fig. 9 is the same as the layer 2 crossbar in Fig. 8. Assume that the synaptic weight associated with input i, neuron j (second layer neuron) is wij=\u03c3ij+ - \u03c3ij- for i=1,2,3 and j=1,2,..,4. In the backward phase we want to evaluate\n\u03b4L1,i = \u03a3jwij \u03b4L2,j for i=1,2,3 and j=1,2,..,4.\n= \u03a3j(\u03c3ij+ - \u03c3ij-)\u03b4L2,j = \u03a3j\u03c3ij+\u03b4L2,j - \u03a3j\u03c3ij-\u03b4L2,j (7)\nThe circuit in Fig. 9 is essentially evaluating the same operations as Eq. (7), applying both \u03b4L2,j and -\u03b4L2,j to the crossbar columns for j=1,2,3. The back propagated errors are discretized using ADCs and are stored in buffers. To reduce the training circuit overhead, we can multiplex the back propagated error generation circuit as shown in Fig. 10. In this circuit, enabling the appropriate pass transistors, back propagated errors are sequentially generated and stored in the buffer. Access to the pass transistors will be controlled by a shift register.\nStep 3: The weight update procedures for both layers are similar. They take neuron inputs, errors, and intermediate neuron outputs (DPj) to generate a set of training pulses. The training unit essentially implements Eq. (6). To update a synaptic weight by an amount \u0394w, the conductance of the memristor connected to the first column of the corresponding neuron will be updated by amount \u0394w/2 (where \u0394w/2=\ud835\udf02 \u00d7 \ud835\udeff\ud835\udc57 \u00d7 \ud835\udc53\u2032(\ud835\udc37\ud835\udc43\ud835\udc57) \u00d7 \ud835\udc65\ud835\udc56) and the memristor connected to the second column of the corresponding neuron by amount -\u0394w/2. We will describe the weight update procedure for the first columns of the neurons (odd crossbar columns in Fig. 8). The weight update procedure for the second columns of the neurons (even columns) is similar to the procedure for the first columns, except that we need to multiply the neuron errors (\ud835\udeff\ud835\udc57) by -1.\nIn Eq. (6) we need to evaluate the derivative of the activation function for the dot product of the neuron inputs and weights (DPj). The DPj value of neuron j is essentially the scaled difference of the currents through the two columns implementing the neuron (see section III(B)). Applying the inputs to the neuron again, the difference of column currents are stored in the buffer after converting into digital form. The\nderivative of the activation function (f\u2019) is evaluated using a lookup table. A digital multiplier is utilized to multiply the neuron error (\u03b4j) and f\u2019(DPj). The number \u03b4j\u00d7f\u2019(DPj) is converted into analog form and is applied to the training pulse generation unit (see Fig. 11(b)). For training, pulses of variable amplitude and variable duration are produced. The amplitude of the training signal is modulated by the neuron input (xi) and is applied to the row wire connecting the desired memristor (see Fig. 11(a)). The duration of the training pulse is modulated by \u03b7\u00d7\u03b4j\u00d7f\u2019(DPj) and is applied to the column wire connecting the desired memristor (see Fig. 11(b)). The combined effect of the two voltages applied across the memristor will update the conductance by an amount proportional to \ud835\udf02 \u00d7 \ud835\udeff\ud835\udc57 \u00d7 \ud835\udc53\u2032(\ud835\udc37\ud835\udc43\ud835\udc57) \u00d7 \ud835\udc65\ud835\udc56."}, {"heading": "IV. HETEROGENEOUS CORES", "text": "The proposed system has a RISC core, a digital core for clustering and memristor neural cores. These cores are connected through an on-chip routing network. The memristor neural cores could be used to implement autoencoders and feed forward neural networks of different configurations."}, {"heading": "A. Memristor Neural Core", "text": "Fig. 12 shows the memristor based single neural core architecture. It consists of a memristor crossbar of size 400\u00d7200, input and output buffers, a training unit, and a control unit. Our objective was to take as big crossbar as possible, because this enables more computations to be done in parallel. Doing experiment on different crossbar sizes, we observed that 400\u00d7200 crossbar has very little impact of sneak paths for the memristor device considered (high resistance values). The control unit will manage the input and output buffers and will interact with the specific routing switch associated with the core. The control unit will be implemented as a finite state machine and thus will be of low overhead. Processing in the core is analog and entire core process in one cycle for an input. The neural cores communicate between each other in digital form as it is expensive to exchange and latch analog signals. Neuron outputs are discretized using a three bit ADC converter for each neuron and are stored in the output buffer for routing. Inputs come to the neural core through the routing network in digital form and are stored in the input buffer. Inputs are applied to the memristor crossbar, converting them into analog form."}, {"heading": "B. Digital Clustering Core", "text": "We have designed a digital core for performing k-means clustering on the feature representation output of the autoencoder. The core could be configured to generate up to 32 clusters and maximum input dimension could be 32 after dimensionality reduction using the autoencoder. This system performs clustering based on Manhattan distance calculations.\nAssume that our data dimension is n and the number of clusters is m. For an element of the data sample, the corresponding Manhattan distances for the current m cluster centers are evaluated in parallel and are accumulated in the dist. registers (Fg. 13). If a subtractor (in the first row of subtractors) output is negative, it is subtracted from the corresponding dist. register otherwise it is added. After n iterations (one for each element of the input sample), the Manhattan distance between the input sample and the current cluster centers will be in dist. registers. In the next m cycles, the minimum distance value and the corresponding cluster center index will be evaluated using the circuit shown in the figure at right.\nThe system has another set of registers for each cluster center (Fig. 13) to store the accumulated values of the data samples belonging to that cluster. One counter is taken for each cluster to keep track of how many data samples belong to that cluster. The data sample is accumulated in the center accumulator register corresponding to the minimum distance cluster. To minimize hardware costs and processing time, this operation is overlapped with the distance calculation step for the next data pattern in Fig. 13. When all the training data samples are assigned to the clusters based on the minimum distances, new cluster center will be evaluated dividing the center accumulator registers by the corresponding sample counter values. After this, new cluster centers will be updated and operations for the next epoch will be performed based on the new centers."}, {"heading": "V. EXPERIMENTAL SETUP", "text": ""}, {"heading": "A. Applications and Datasets", "text": "We have examined the MNIST [21] and ISOLET [22] datasets for classification and clustering tasks. The classification applications utilized deep networks where the autoencoder was utilized for unsupervised layer-wise pretraining of the networks. For clustering applications, dimensionality reduction and feature extraction tasks are performed using the autoencoder. After that the k-means clustering algorithm is applied on the data generated by the autoencoder. An autoencoder based anomaly detection was examined on the KDD dataset [23]. Table I shows the neural network configurations for different datasets and applications."}, {"heading": "B. Mapping Neural Networks to Cores", "text": "The neural hardware are not able to time multiplex neurons as their synaptic weights are stored directly within the neural circuits. Hence a neural network\u2019s structure may need to be modified to fit into a neural core. In cases where the networks were significantly smaller than the neural core memory, multiple neural layers were mapped to a core. In this case, the layers executed in a pipelined manner, where the outputs of layer 1 were fed back into layer 2 on the same core through the core\u2019s routing switch.\nWhen a software network layer was too large to fit into a core (either because it needed too many inputs or it had too many neurons), the network layer was split amongst multiple cores. Splitting a layer across multiple cores due to a large number of output neurons is trivial. When there were too many inputs per neuron for a core, each neuron was split into\nmultiple smaller neurons as shown in Fig. 14. When splitting a neuron, the network needs to be trained based on the new network topology. As the network topology is determined prior to training (based on the neural hardware architecture), the split neuron weights are trained correctly."}, {"heading": "C. Area Power Calculations", "text": "The area, power, and timing of the SRAM array, used in the clustering core, were calculated using CACTI [24] with the low operating power transistor option utilized. We assumed a 45 nm process in our area power calculations. Components of a typical cache that would not be needed in the neural core (such as the tag array and tag comparator) were not included in the calculations. The clustering core also requires adders, registers, multiplexers, and a control unit. The power of these basic components were determined through SPICE simulations. A frequency of 200 MHz was assumed for the digital system to keep power consumption low.\nFor the memristor cores, detailed SPICE simulations were used for power and timing calculations of the analog circuits (drivers, crossbar, and activation function circuits). These simulations considered the wire resistance and capacitance within the crossbar as well. The results show that the crossbar required 20 ns to be evaluated. As the memristor crossbars evaluate all neurons in one step, the majority of time in these systems is spent in transferring neuron outputs between cores through the routing network. We assumed that routing would run at 200 MHz clock resulting in 4 cycles needed for crossbar processing. The routing link power was calculated using Orion [25] (assuming 8 bits per link). Off-chip I/O energy was also considered as described in section II. Data transfer energy via TSV was assumed to be 0.05 pJ/bit [26]."}, {"heading": "VI. RESULTS", "text": ""}, {"heading": "A. Supervised Training Result", "text": "We have performed detailed simulations of the memristor crossbar based neural network systems. MATLAB (R2014a) and SPICE (LTspice IV) were used to develop a simulation framework for applying the training algorithm to a multi-layer neural network circuit. SPICE was mainly used for detailed analog simulations of the memristor crossbar array and MATLAB was used to simulate the rest of the system. The circuits were trained by applying input patterns one by one until the errors were below the desired levels.\nSimulation of the memristor device used an accurate model of the device published in [27]. The memristor device simulated for this circuit was published in [18] and the\nswitching characteristics for the model are displayed in Fig. 15. This device was chosen for its high minimum resistance value and large resistance ratio. According to the data presented in [18] this device has a minimum resistance of 10 k\u2126, a resistance ratio of 103, and the full resistance range of the device can be switched in 20 \u03bcs by applying 2.5 V across the device.\nSPICE simulations of large memristor crossbars are very time consuming. Therefore, we examined the on-chip supervised training approach with the Iris dataset [28] which utilizes crossbars of manageable sizes. A two layer network with four inputs, ten hidden neurons and one output neuron was utilized. Fig. 16 shows the Matlab-SPICE training simulation on the Iris dataset. It shows that the neural network was able to learn the desired classifiers."}, {"heading": "B. Unsupervised Training Result", "text": "Unsupervised training of the autoencoder was performed on the Iris dataset. The network configuration utilized was 4\u21922\u21924. After training, the two hidden neuron outputs give the feature representation of the corresponding input data in a reduced dimension of two. Fig. 17 shows the distribution of the data of three different classes (setosa, versicolor, virginica) in the feature space. We can observe that data belonging to the same class appears closely in the feature space and could potentially be linearly separated."}, {"heading": "C. Anomaly Detection", "text": "We have examined autoencoder based anomaly detection on the KDD dataset. The network configuration for this application was 39->15->39. As the SPICE simulation of this network size takes a very long time, we did this experiment in MATLAB. During training the autoencoder was trained using only normal data packets. It is expected that during evaluation, for normal data packets the differences between input and reconstruction will be smaller compared to the differences for the attack packets. The network was trained only with 5292 normal packet data (no anomalous packets were used during training).\nFigures 18 and 19 show the distribution of the distances between inputs and the corresponding reconstructions for the normal packets and the attack packets respectively. From Fig. 20 it is seen that the system can detect about 96.6% of the anomalous packets with a 4% false detection rate. Similar approaches could be used for other big data applications where the objective is to detect anomalous patterns from large volumes of continuously streaming data in real time at low power.\nFig. 19. Distance between original data and reconstructed data for attack packets.\nD. Impact of System Constraints on Application Accuracy\nThe hardware neural network training circuit differs from\nthe software implementations in the following aspects:\n0 500 1000 1500 2000 2500 0\n0.2\n0.4\n0.6\n0.8\n1\n1.2\nAnomalous packet\nD is\nta n c e b\ne tn\ni n p u t\n& r\ne c o n s tr\nu c ti o n"}, {"heading": "E. Single Core Area and Power", "text": "The area of the digital clustering core is 0.039 mm2 and its power consumption is 1.36 mW. The training of 1000 samples for one epoch in this core takes 0.32 \u03bcs time.\nThe memristor neural core configuration is 400\u00d7100, i.e. it can take a maximum of 400 inputs and can process a maximum of 100 neurons. The area of a single memristor neural core is 0.0163 mm2. Table II shows a single memristor core power and timing at different steps of execution. The RISC core is considered to be used only for configuring the cores, routers, and DMA engine. As a result, we assume that the RISC core is turned off afterwards during the actual training or evaluation phases."}, {"heading": "F. System Level Evaluations", "text": "Total system area: The whole multicore system includes 144 memristor neural cores, one digital clustering core, one RISC core, one DMA controller, 4 kB of input buffer and 1 kB of output buffer. The RISC core area was evaluated using McPat [36] and came to 0.52 mm2. The total system area was 2.94 mm2.\nEnergy efficiency: We have compared the throughput energy benefits of the proposed architecture over an Nvidia Tesla K20 GPU. The system consumes 225 W power. The area of the GPU is 561 mm2 using a 28 nm process. Figures 22 and 23 show the throughput and energy efficiencies respectively of the proposed system over GPU for different applications during training. For training the proposed architecture provides up to 30x speedup and four to six orders of magnitude more energy efficiency over GPU.\nFigures 24 and 25 show the throughput and energy efficiencies of the proposed system over GPU for different applications respectively during evaluation of new inputs. For recognition, the proposed architecture provides up to 50\u00d7 speedup and five to six orders of magnitude more energy efficiency over the GPU.\nFig. 24. Application speedup over GPU for recognition.\nTable III shows the number of cores used, the time and the energy for a single training data item in one iteration. Table IV shows the evaluation time and energy for a single test data. In both training and recognition phases the computation energy dominated the total system energy consumption.\n0\n10\n20\n30\n40\n50\nSp ee\nd u\np o\nve r\nG P\nU"}, {"heading": "VII. RELATED WORK", "text": ""}, {"heading": "A. Digital Systems", "text": "High performance computing platforms can simulate a large number of neurons, but are very expensive from a power\nconsumption standpoint. Specialized architectures [29-32] can significantly reduce the power consumption for neural\nnetwork applications and yet provide high performance\nIBM\u2019s TrueNorth chip consists of 5.4 billion transistors [33]. It has 4,096 neurosynaptic cores interconnected via an\nintra-chip network that integrates one million programmable\nspiking neurons and 256 million configurable synapses. The basic building block is a core, a self-contained neural network\nwith 256 input lines (axons), and 256 outputs (neurons) connected via 256\u00d7256 directed, programmable synaptic\nconnections. With a 400\u00d7240-pixel video input at 30 frames-\nper-second, the chip consumes 63mW. Key differences\nbetween this system and our system are that TrueNorth uses\nspiking neurons, has asynchronous communications, and\nallows the SRAM arrays to enter ultra low leakage modes. We examine the impact of using leakage-less SRAM arrays in our\nstudy.\nDaDianNao and PuDianNao [1,2] are accelerator for deep neural networks (DNN) and convolutional neural networks (CNN). These systems are based on a fully digital design. In PuDianNao [2] neuron synaptic weights are stored in off-chip memory which requires data movement during training back and forth between the off-chip memory and the processing chip. In DaDianNao [1], neuron synaptic weights are stored in eDRAM and are later brought into a neural functional unit for execution. Input data are initially stored in a central eDRAM. This system utilized dynamic wormhole routing for data transfers. It was able to achieve 150\u00d7 energy reduction over GPUs. ShiDianNao is an accelerator only for CNNs. As CNNs utilize shared weights, a small on-chip SRAM cache was able to store the whole network\u2019s weights.\nSt. Amant et al. [34] presented a compilation approach\nwhere a user would annotate a code segment for conversion to a neural network. This study examined transformation of\nseveral kernels including FFT, JPEG, K-means, and the Sobel edge detector into equivalent neural networks. In this work\nthey utilized analog neural cores to accelerate a RISC\nprocessor for general purpose codes. Our approach varies from this in that we are examining standalone neural cores for\nembedded applications, and using memristors for the processing.\nBelhadj et al. [35] proposed multicore architectures for\nspiking neurons and evaluated them for embedded signal processing applications. They stored synaptic weights in\ndigital form and used digital to analog converters to process\nthe neurons. There were significant differences in the neuron circuit designs and neural algorithm compared to our system."}, {"heading": "B. Memristor Based Systems", "text": "The most recent results for memristor based neuromorphic systems can be seen in [12-15]. Alibart [12] and Preioso [13]\nexamined only linearly separable problems. Boxun [15] demonstrates in-situ training of memristor crossbar based\nneural networks for nonlinear classifier designs. They\nproposed having two copies of the same synaptic weights, one\nfor the forward pass and another transposed version for the\nbackward pass. However it is practically not easy to have an exact copy of a memristor crossbar because of memristor\ndevice stocasticity. Soudry et al. [14] proposed\nimplementation of gradient descent based training on memristor crossbar neural networks. They utilized two\ntransistors and one memristor per synapse. Their synaptic\nweight precision is less than that of the two memristors per synapse neuron design utilized in this paper.\nLiu [16] examined memristor based neural networks utilizing four memristors per synapse while the proposed systems utilized two memristors per synapse. They proposed the deployment of the system as a neural accelerator with a RISC processor. The proposed system executes data directly coming from a 3D stacked sensor chip."}, {"heading": "VIII. CONCLUSION", "text": "In this paper we have proposed a multicore heterogeneous architecture for big data processing. This system has the capability to process key machine learning algorithms such as deep neural network, autoencoder, and k-means clustering. The system has both training and recognition (evaluation of new input) capabilities. We utilized memristor crossbars to implement energy efficient neural cores. Our experimental evaluations show that the proposed architecture can provide four to six orders of magnitude more energy efficiency over GPUs for big data processing."}], "references": [{"title": "DaDianNao: A Machine-Learning Supercomputer", "author": ["Yunji Chen", "Tao Luo", "Shaoli Liu", "Shijin Zhang", "Liqiang He", "Jia Wang", "Ling Li", "Tianshi Chen", "Zhiwei Xu", "Ninghui Sun", "Olivier Temam"], "venue": "In Proceedings of the 47th Annual IEEE/ACM International Symposium on Microarchitecture (MICRO-47)", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2014}, {"title": "PuDianNao: A Polyvalent Machine Learning Accelerator", "author": ["Daofu Liu", "Tianshi Chen", "Shaoli Liu", "Jinhong Zhou", "Shengyuan Zhou", "Olivier Teman", "Xiaobing Feng", "Xuehai Zhou", "Yunji Chen"], "venue": "In Proceedings of the Twentieth International Conference on Architectural Support for Programming Languages and Operating Systems (ASPLOS '15)", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2015}, {"title": "Memristor\u2014The Missing Circuit Element", "author": ["L.O. Chua"], "venue": "IEEE Transactions on Circuit Theory, vol. 18, no. 5, pp. 507\u2013519 (1971).", "citeRegEx": "3", "shortCiteRegEx": null, "year": 1971}, {"title": "Robust Neural Logic Block (NLB) Based on Memristor Crossbar Array", "author": ["D. Chabi", "W. Zhao", "D. Querlioz", "J.-O. Klein"], "venue": "IEEE/ACM International Symposium on Nanoscale Architectures,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2011}, {"title": "On spike-timing-dependent-plasticity, memristive devices, and building a self-learning visual cortex", "author": ["C. Zamarre\u00f1o-Ramos", "L.A. Camu\u00f1as-Mesa", "J.A. P\u00e9rez-Carrasco", "T. Masquelier", "T. Serrano-Gotarredona", "B. Linares-Barranco"], "venue": "Frontiers in Neuroscience, Neuromorphic Engineering, vol. 5, Article 26, pp. 1-22, Mar. 2011.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2011}, {"title": "The missing Memristor found", "author": ["D.B. Strukov", "G.S. Snider", "D.R. Stewart", "R.S. Williams"], "venue": "Nature, vol. 453, pp. 80\u201383 (2008).", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2008}, {"title": "Exploring the Design Space of Specialized Multicore Neural Processors", "author": ["T.M. Taha", "R. Hasan", "C. Yakopcic", "M.R. McLean"], "venue": "IEEE International Joint Conference on Neural Networks (IJCNN), 2013.", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2013}, {"title": "Memristor Crossbar-Based Neuromorphic Computing System: A Case Study.", "author": ["M. Hu", "H. Li", "Y. Chen", "Q. Wu", "G.S. Rose", "R.W. Linderman"], "venue": "IEEE Trans. Neural Netw. Learning Syst", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2014}, {"title": "Contrastive divergence for memristor-based restricted Boltzmann machine", "author": ["A.M. Sheri", "A. Rafique", "W. Pedrycz", "M. Jeon"], "venue": "Engineering Applications of Artificial Intelligence", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2015}, {"title": "Memristor-Based Multilayer Neural Networks With Online Gradient Descent Training", "author": ["D. Soudry", "D.D. Castro", "A. Gal", "A. Kolodny", "S. Kvatinsky"], "venue": "IEEE Trans. on Neural Networks and Learning Systems, accepted.", "citeRegEx": "10", "shortCiteRegEx": null, "year": 0}, {"title": "A Heterogeneous Computing System with Memristor-Based Neuromorphic Accelerators.\" HPEC", "author": ["Liu", "Xiaoxiao"], "venue": null, "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2014}, {"title": "Strukov, \"Pattern classification by memristive crossbar circuits with ex-situ and in-situ training", "author": ["F. Alibart", "E. Zamanidoost", "D.B"], "venue": "Nature Communications,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2013}, {"title": "Training and operation of an integrated neuromorphic network based on metal-oxide memristors", "author": ["M. Prezioso", "F. Merrikh-Bayat", "B.D. Hoskins", "G.C. Adam", "K.K. Likharev", "D.B. Strukov"], "venue": "Nature, 521(7550), 61-64, 2015.", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2015}, {"title": "Memristor-Based Multilayer Neural Networks With Online Gradient Descent Training", "author": ["D. Soudry", "D.D. Castro", "A. Gal", "A. Kolodny", "S. Kvatinsky"], "venue": "IEEE Trans. on Neural Networks and Learning Systems, issue 99, 2015.", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2015}, {"title": "Training itself: Mixed-signal training acceleration for memristorbased neural network", "author": ["Boxun Li", "Yuzhi Wang", "Yu Wang", "Chen, Y.", "Huazhong Yang"], "venue": "Design Automation Conference (ASP-DAC), 2014 19th Asia and South Pacific , vol., no., 20-23 Jan. 2014.", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2014}, {"title": "RENO: A high-efficient reconfigurable neuromorphic computing accelerator design", "author": ["Xiaoxiao Liu", "Mengjie Mao", "Beiye Liu", "Hai Li", "Yiran Chen", "Boxun Li", "Yu Wang", "Hao Jiang", "Barnell, M.", "Qing Wu", "Jianhua Yang"], "venue": "Design Automation Conference (DAC), 2015 52nd ACM/EDAC/IEEE , vol., no., pp.1-6, 8-12 June 2015", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2015}, {"title": "NVSim: A Circuit-Level Performance, Energy, and Area Model for Emerging Nonvolatile Memory", "author": ["X. Dong", "C. Xu", "S. Member", "Y. Xie", "N.P. Jouppi"], "venue": "IEEE Trans. on Computer Aided Design of Integrated Circuits and Systems, vol. 31, no. 7, pp. 994-1007, July, 2012.", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2012}, {"title": "Investigating the switching dynamics and multilevel capability of bipolar metal oxide resistive switching memory", "author": ["S. Yu", "Y. Wu", "H.-S.P. Wong"], "venue": "Applied Physics Letters 98, 103514 (2011).", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2011}, {"title": "Stacked denoising autoencoders: Learning useful representations in a deep network with a local denoising criterion", "author": ["P. Vincent", "H. Larochelle", "I. Lajoie", "Y. Bengio", "P.A. Manzagol"], "venue": "The Journal of Machine Learning Research, vo. 11, pp. 3371-3408, 2010.", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2010}, {"title": "Artificial Intelligence: A Modern Approach (2nd Edition)", "author": ["S. Russell", "P. Norvig"], "venue": null, "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2002}, {"title": "Optimizing NUCA Organizations and Wiring Alternatives for Large Caches with CACTI 6.0", "author": ["N. Muralimanohar", "R. Balasubramonian", "N. Jouppi"], "venue": "In Proceedings of the 40th Annual IEEE/ACM International Symposium on Microarchitecture (MICRO 40),", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2007}, {"title": "ORION 2.0: A fast and accurate NoC power and area model for early-stage design space exploration", "author": ["A.B. Kahng", "B. Li", "L.S. Peh", "K. Samadi"], "venue": "Design, Automation & Test in Europe Conference & Exhibition, pp.423-428, 20-24 April 2009.", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2009}, {"title": "An energy efficient multi-bit TSV transmitter using capacitive coupling", "author": ["J. Gorner", "S. Hoppner", "D. Walter", "M. Haas", "D. Plettemeier", "R. Schuffny"], "venue": "Electronics, Circuits and Systems (ICECS), 2014 21st IEEE International Conference on , vol., no., pp.650,653, 7-10 Dec. 2014.", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2014}, {"title": "Memristor SPICE Model and Crossbar Simulation Based on Devices with Nanosecond Switching Time", "author": ["C. Yakopcic", "T.M. Taha", "G. Subramanyam", "R.E. Pino"], "venue": "IEEE International Joint Conference on Neural Networks (IJCNN), August 2013.", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2013}, {"title": "High-Performance Computing for Systems of Spiking Neurons", "author": ["S.B. Furber", "S. Temple", "A.D. Brown"], "venue": "Proceedings of AISB'06 workshop on GC5: Architecture of Brain and Mind, vol.2, pp 29-36, Bristol, April, 2006.", "citeRegEx": "29", "shortCiteRegEx": null, "year": 2006}, {"title": "Wafer-Scale Integration of Analog Neural Networks", "author": ["J. Schemmel", "J. Fieres", "K. Meier"], "venue": "IEEE International Joint Conference on Neural Networks (IJCNN),", "citeRegEx": "30", "shortCiteRegEx": "30", "year": 2008}, {"title": "A digital neurosynaptic core using embedded crossbar memory with  45pJ per spike in 45nm", "author": ["P. Merolla", "J. Arthur", "F. Akopyan", "N. Imam", "R. Manohar", "D.S. Modha"], "venue": "IEEE Custom Integrated Circuits Conference (CICC), vol., no., pp.1-4, 19-21 Sept. 2011.", "citeRegEx": "31", "shortCiteRegEx": null, "year": 2011}, {"title": "Building block of a programmable neuromorphic substrate: A digital neurosynaptic core", "author": ["J.V. Arthur", "P.A. Merolla", "F. Akopyan", "R. Alvarez", "A. Cassidy", "S. Chandra", "S.K. Esser", "N. Imam", "W. Risk", "D.B.D. Rubin", "R. Manohar", "D.S. Modha"], "venue": "The International Joint Conference on Neural Networks (IJCNN), pp.1-8, June 2012.", "citeRegEx": "32", "shortCiteRegEx": null, "year": 2012}, {"title": "A million spikingneuron integrated circuit with a scalable communication network and interface.", "author": ["P. A Merolla", "J.V. Arthur", "R. Alvarez-Icaza", "A.S. Cassidy", "Jun Sawada", "F. Akopyan", "B.L. Jackson", "N. Imam", "C. Guo", "Y. Nakamura", "B. Brezzo", "Ivan Vo", "S.K. Esser", "R. Appuswamy", "B. Taba", "A. Amir", "M.D. Flickne", "W.P. Risk", "R. Manohar", "D.S. Modha"], "venue": "Science 345,", "citeRegEx": "33", "shortCiteRegEx": "33", "year": 2014}, {"title": "General-purpose code acceleration with limited-precision analog computation,", "author": ["R.S. Amant", "A. Yazdanbakhsh", "J. Park", "B. Thwaites", "H. Esmaeilzadeh", "A. Hassibi", "L. Ceze", "D. Burger"], "venue": "In Proceeding of the 41st annual international symposium on Computer architecuture (ISCA", "citeRegEx": "34", "shortCiteRegEx": "34", "year": 2014}, {"title": "Continuous realworld inputs can open up alternative accelerator designs", "author": ["B. Belhadj", "A.J.L. Zheng", "R. H\u00e9liot", "O. Temam"], "venue": "SIGARCH Comput. Archit. News 41, 3 (June 2013).", "citeRegEx": "35", "shortCiteRegEx": null, "year": 2013}, {"title": "McPAT: An integrated power, area, and timing modeling framework for multicore and manycore architectures", "author": ["Sheng Li", "Jung Ho Ahn", "R.D. Strong", "J.B. Brockman", "D.M. Tullsen", "N.P. Jouppi"], "venue": "Microarchitecture, 2009. MICRO-42. 42nd Annual IEEE/ACM International Symposium on , vol., no., pp.469-480, 12-16 Dec. 2009", "citeRegEx": "36", "shortCiteRegEx": null, "year": 2009}], "referenceMentions": [{"referenceID": 0, "context": "Examples of this include DaDianNao [1] and PuDianNao [2], both of which accelerate machine learning algorithms through digital circuits.", "startOffset": 35, "endOffset": 38}, {"referenceID": 1, "context": "Examples of this include DaDianNao [1] and PuDianNao [2], both of which accelerate machine learning algorithms through digital circuits.", "startOffset": 53, "endOffset": 56}, {"referenceID": 2, "context": "Memristors [3,4] have received significant attention as a potential building block for neuromorphic systems [5,6].", "startOffset": 11, "endOffset": 16}, {"referenceID": 3, "context": "Memristors [3,4] have received significant attention as a potential building block for neuromorphic systems [5,6].", "startOffset": 11, "endOffset": 16}, {"referenceID": 4, "context": "Memristors [3,4] have received significant attention as a potential building block for neuromorphic systems [5,6].", "startOffset": 108, "endOffset": 113}, {"referenceID": 5, "context": "Memristors [3,4] have received significant attention as a potential building block for neuromorphic systems [5,6].", "startOffset": 108, "endOffset": 113}, {"referenceID": 6, "context": "This enables highly dense neuromorphic systems with great computational efficiency [7].", "startOffset": 83, "endOffset": 86}, {"referenceID": 11, "context": "The most recent results for memristor based neuromorphic systems can be seen in [12-15].", "startOffset": 80, "endOffset": 87}, {"referenceID": 12, "context": "The most recent results for memristor based neuromorphic systems can be seen in [12-15].", "startOffset": 80, "endOffset": 87}, {"referenceID": 13, "context": "The most recent results for memristor based neuromorphic systems can be seen in [12-15].", "startOffset": 80, "endOffset": 87}, {"referenceID": 14, "context": "The most recent results for memristor based neuromorphic systems can be seen in [12-15].", "startOffset": 80, "endOffset": 87}, {"referenceID": 11, "context": "[12,13] examined only linearly separable problems.", "startOffset": 0, "endOffset": 7}, {"referenceID": 12, "context": "[12,13] examined only linearly separable problems.", "startOffset": 0, "endOffset": 7}, {"referenceID": 15, "context": "[16] examined memristor based neural networks utilizing four memristors per synapse where the proposed systems utilize two memristors per synapse.", "startOffset": 0, "endOffset": 4}, {"referenceID": 2, "context": "Leon Chua [3].", "startOffset": 10, "endOffset": 13}, {"referenceID": 16, "context": "Memristors are essentially nanoscale variable resistors, where the resistance is modified if a voltage greater than a write threshold voltage (Vth) is applied [17].", "startOffset": 159, "endOffset": 163}, {"referenceID": 17, "context": "One such device, composed of layers of HfOx and AlOx [18], has a high on state resistance (RON\u224810k\u03a9), a very high resistance ratio (ROFF/RON\u22481000) and a write threshold voltage of about 1.", "startOffset": 53, "endOffset": 57}, {"referenceID": 18, "context": "The autoencoder [19] is a popular method for dimensionality reduction and feature extraction approach which automatically learns features from unlabeled data.", "startOffset": 16, "endOffset": 20}, {"referenceID": 19, "context": "Back-propagation (BP) [20] and the variants of the BP algorithm are widely used for training such networks.", "startOffset": 22, "endOffset": 26}, {"referenceID": 20, "context": "The area, power, and timing of the SRAM array, used in the clustering core, were calculated using CACTI [24] with the low operating power transistor option utilized.", "startOffset": 104, "endOffset": 108}, {"referenceID": 21, "context": "The routing link power was calculated using Orion [25] (assuming 8 bits per link).", "startOffset": 50, "endOffset": 54}, {"referenceID": 22, "context": "05 pJ/bit [26].", "startOffset": 10, "endOffset": 14}, {"referenceID": 23, "context": "Simulation of the memristor device used an accurate model of the device published in [27].", "startOffset": 85, "endOffset": 89}, {"referenceID": 17, "context": "The memristor device simulated for this circuit was published in [18] and the m cluster centers", "startOffset": 65, "endOffset": 69}, {"referenceID": 17, "context": "According to the data presented in [18] this device has a minimum resistance of 10 k\u03a9, a resistance ratio of 10, and the full resistance range of the device can be switched in 20 \u03bcs by applying 2.", "startOffset": 35, "endOffset": 39}, {"referenceID": 23, "context": "Simulation results displaying the input voltage and current waveforms for the memristor model [27] that was based on the device in [18].", "startOffset": 94, "endOffset": 98}, {"referenceID": 17, "context": "Simulation results displaying the input voltage and current waveforms for the memristor model [27] that was based on the device in [18].", "startOffset": 131, "endOffset": 135}, {"referenceID": 31, "context": "The RISC core area was evaluated using McPat [36] and came to 0.", "startOffset": 45, "endOffset": 49}, {"referenceID": 24, "context": "Specialized architectures [29-32] can significantly reduce the power consumption for neural network applications and yet provide high performance IBM\u2019s TrueNorth chip consists of 5.", "startOffset": 26, "endOffset": 33}, {"referenceID": 25, "context": "Specialized architectures [29-32] can significantly reduce the power consumption for neural network applications and yet provide high performance IBM\u2019s TrueNorth chip consists of 5.", "startOffset": 26, "endOffset": 33}, {"referenceID": 26, "context": "Specialized architectures [29-32] can significantly reduce the power consumption for neural network applications and yet provide high performance IBM\u2019s TrueNorth chip consists of 5.", "startOffset": 26, "endOffset": 33}, {"referenceID": 27, "context": "Specialized architectures [29-32] can significantly reduce the power consumption for neural network applications and yet provide high performance IBM\u2019s TrueNorth chip consists of 5.", "startOffset": 26, "endOffset": 33}, {"referenceID": 28, "context": "4 billion transistors [33].", "startOffset": 22, "endOffset": 26}, {"referenceID": 0, "context": "DaDianNao and PuDianNao [1,2] are accelerator for deep neural networks (DNN) and convolutional neural networks (CNN).", "startOffset": 24, "endOffset": 29}, {"referenceID": 1, "context": "DaDianNao and PuDianNao [1,2] are accelerator for deep neural networks (DNN) and convolutional neural networks (CNN).", "startOffset": 24, "endOffset": 29}, {"referenceID": 1, "context": "In PuDianNao [2] neuron synaptic weights are stored in off-chip memory which requires data movement during training back and forth between the off-chip memory and the processing chip.", "startOffset": 13, "endOffset": 16}, {"referenceID": 0, "context": "In DaDianNao [1], neuron synaptic weights are stored in eDRAM and are later brought into a neural functional unit for execution.", "startOffset": 13, "endOffset": 16}, {"referenceID": 29, "context": "[34] presented a compilation approach where a user would annotate a code segment for conversion to a neural network.", "startOffset": 0, "endOffset": 4}, {"referenceID": 30, "context": "[35] proposed multicore architectures for spiking neurons and evaluated them for embedded signal processing applications.", "startOffset": 0, "endOffset": 4}, {"referenceID": 11, "context": "The most recent results for memristor based neuromorphic systems can be seen in [12-15].", "startOffset": 80, "endOffset": 87}, {"referenceID": 12, "context": "The most recent results for memristor based neuromorphic systems can be seen in [12-15].", "startOffset": 80, "endOffset": 87}, {"referenceID": 13, "context": "The most recent results for memristor based neuromorphic systems can be seen in [12-15].", "startOffset": 80, "endOffset": 87}, {"referenceID": 14, "context": "The most recent results for memristor based neuromorphic systems can be seen in [12-15].", "startOffset": 80, "endOffset": 87}, {"referenceID": 11, "context": "Alibart [12] and Preioso [13] examined only linearly separable problems.", "startOffset": 8, "endOffset": 12}, {"referenceID": 12, "context": "Alibart [12] and Preioso [13] examined only linearly separable problems.", "startOffset": 25, "endOffset": 29}, {"referenceID": 14, "context": "Boxun [15] demonstrates in-situ training of memristor crossbar based neural networks for nonlinear classifier designs.", "startOffset": 6, "endOffset": 10}, {"referenceID": 13, "context": "[14] proposed implementation of gradient descent based training on memristor crossbar neural networks.", "startOffset": 0, "endOffset": 4}, {"referenceID": 15, "context": "Liu [16] examined memristor based neural networks utilizing four memristors per synapse while the proposed systems utilized two memristors per synapse.", "startOffset": 4, "endOffset": 8}], "year": 2016, "abstractText": "General purpose computing systems are used for a large variety of applications. Extensive supports for flexibility in these systems limit their energy efficiencies. Given that big data applications are among the main emerging workloads for computing systems, specialized architectures for big data processing are needed to enable low power and high throughput execution. Several big data applications are particularly focused on classification and clustering tasks. In this paper we propose a multicore heterogeneous architecture for big data processing. This system has the capability to process key machine learning algorithms such as deep neural network, autoencoder, and kmeans clustering. Memristor crossbars are utilized to provide low power high throughput execution of neural networks. The system has both training and recognition (evaluation of new input) capabilities. The proposed system could be used for classification, unsupervised clustering, dimensionality reduction, feature extraction, and anomaly detection applications. The system level area and power benefits of the specialized architecture is compared with the NVIDIA Telsa K20 GPGPU. Our experimental evaluations show that the proposed architecture can provide four to six orders of magnitude more energy efficiency over GPGPUs for big data processing. Keywords\u2013Low power architecture; memristor crossbars; autoencoder; unsupervised training; big data.", "creator": "Microsoft\u00ae Word 2013"}}}