{"id": "1603.07646", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "24-Mar-2016", "title": "Recursive Neural Language Architecture for Tag Prediction", "abstract": "We consider the problem of learning distributed representations for tags from their associated content for the task of tag recommendation. Considering tagging information is usually very sparse, effective learning from content and tag association is very crucial and challenging task. Recently, various neural representation learning models such as WSABIE and its variants show promising performance, mainly due to compact feature representations learned in a semantic space. However, their capacity is limited by a linear compositional approach for representing tags as sum of equal parts and hurt their performance. In this work, we propose a neural feedback relevance model for learning tag representations with weighted feature representations. Our experiments on two widely used datasets show significant improvement for quality of recommendations over various baselines.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n", "histories": [["v1", "Thu, 24 Mar 2016 16:39:37 GMT  (501kb,D)", "http://arxiv.org/abs/1603.07646v1", null]], "reviews": [], "SUBJECTS": "cs.IR cs.CL cs.LG cs.NE", "authors": ["saurabh kataria"], "accepted": false, "id": "1603.07646"}, "pdf": {"name": "1603.07646.pdf", "metadata": {"source": "CRF", "title": "Recursive Neural Language Architecture for Tag Prediction", "authors": ["Saurabh Kataria"], "emails": [], "sections": [{"heading": "Introduction", "text": "With the advent of Web 2.0 and explosive growth of user generated content, tagging \u2013a social bookmark indexing activity\u2013 has become a preferred medium for conceptually organizing and summarizing information. However, from a user perspective, it is often difficult to compose a set of words that can represent associated content. Moreover, from a content sharing website perspective, tagging information can quickly become inconsistent and idiosyncratic because of disparate phrasing styles of the users, which leads to ineffective utilization of tag information. Therefore, automatic tag recommendation has become a popular choice both for content sharing websites as well as its users.\nTag recommendation remains a challenging task, mainly due to extremely sparse tag information present\nfor the underlying content. Recent advances in learning latent representations, or embeddings based upon various neural architectures are proving to be successful in tag recommendation tasks [Wang et al., 2015; Weston et al., 2014], mainly due to compact representations of words and tags in continuous low dimensional space. Unsupervised word embedding methods train with a reconstruction objective in which the embeddings are used to predict the original text. For example, word2vec [Mikolov et al., 2013] tries to predict all the words in a document, given the embeddings of surrounding words. As a result, word embeddings carry semantic information where words that are contextually similar are \u201ccloser\u201d compared to dissimilar ones. In contrast, supervised embedding methods, e.g. WSABIE [Weston et al., 2011] and its variants [Weston et al., 2013b], embed both the labels (or tags) and documents in a shared semantic space where document embedding predicts the \u201cclosest\u201d label embedding. The document embeddings are obtained by combining the embeddings of its words using a model-dependent, possibly learned function, producing a point in the same embedding space.\nAlthough supervised embedding methods are easy to implement and comprehend, these models are limited by a simplistic assumption that each underlying word embedding contribute equally to the document embedding. Consequently, the burden lies on the ranking objective (e.g. WARP [Weston et al., 2011]) to discriminate among the relevant and non-relevant features while learning and making a prediction, causing the model to underfit. Moreover, in the case of tag recommendation, ambiguity among certain tags (e.g. \u201capple\u201d as a tag in context of a technology article vs. a nutrition article) makes the learning even harder as two separate classes of words compete to achieve same representation for an ambiguous tag.\nIn this work, we relax the simplistic compositional assumption where word embeddings combine linearly to form tag embeddings. Specifically, we extend supervised linear embedding architecture (e.g. WSABIE) with a neural relevance feedback layer that weights each word in a document based upon its relevance to the tag it is composed to. Our formulation is flexible as it can accommodate bilinear network layer that can implicitly assign\nar X\niv :1\n60 3.\n07 64\n6v 1\n[ cs\n.I R\n] 2\n4 M\nar 2\n01 6\ntypes to words and tags representations based on different contextual usage and calculate their relevance multiplicatively. Moreover, parameters of the relevance layer can be updated jointly with supervised tag embedding layer providing an easy and scalable learning approach.\nContributions of our work are as follows: we propose a relevance feedback based extension to popular supervised representation learning framework a.k.a WSABIE. Our proposed approach works by reweighting, using the relevance function, each part of the sum (i.e., words in a document) that comprises the embedding of tags for a given set of documents. We propose several classes of neural networks based relevance functions that capture similarity between a word in document and its associated tag. As a result, we show that relevance functions with higher capacity can help disambiguating context in which a tag is associated with a document and, therefore, helps improving tag recommendation tasks. Lastly, we apply our relevance feedback based neural network to tag recommendation tasks to two publically available data set and show significant improvements over various baselines."}, {"heading": "Related Work", "text": "We divide related work in two areas of research:(1) distributed representation learning from content and tags, and (2) tag recommendation for documents. Distributed Representation Learning from Content and Tags Earlier research in distributed representation learning [Bengio et al., 2003] has focused on using probabilistic neural networks to build general representations of words that improve upon the classic ngram language models. More recently, this approach has been extended with two popular neural language models [Mikolov et al., 2013] for learning distributed representations of words, that are (1) continuous bag-ofwords model (CBOW) and (2) Skip-gram, collectively known as word2vec. Although Word2vec and its document based extensions [Le and Mikolov, 2014], learns unsupervised embeddings that are shown to be successful for various NLP related tasks [Djuric et al., 2015; Le and Mikolov, 2014], supervised embeddings such as WSABIE [Weston et al., 2011] directly learns from tagword associations and outperform word2vec for prediction tasks [Weston et al., 2014].WSABIE has been shown to perform well on various recommendation tasks as well, e.g., music annotation with textual tags [Weston et al., 2012], personalized video recommendation [Weston et al., 2013a], image annotation with labels, i.e. ImageNet [Weston et al., 2013c], personalized tag recommendation for images [Denton et al., 2015]. [Weston et al., 2014] extended WSABIE with a convolutional neural network based document representation that can take word ordering into account in a supervised embedding framework. Tag Recommendation for Documents Tag recommendation methods can roughly be categorized into three classes [Wang et al., 2012]: content-based meth-\nods, co-occurrence based methods, and hybrid methods. Content-based methods [Shen and Fan, 2010; Landia, 2012; Lu et al., 2009] utilize only the content information (e.g., abstracts of articles, image pixels, and music content) for tag recommendation. Co-occurrence based methods [Garg and Weber, 2008; Rendle et al., 2009] are similar to collaborative filtering (CF) methods [Li and Yeung, 2009]. The co-occurrence of tags among items, usually represented as a tag-item matrix, is used for tagging. The third class of methods [Wu et al., 2009; Wang and Blei, 2011; Wang et al., 2013; 2015], also the most popular and effective ones, consists of hybrid methods. They make use of both tagging (co-occurrence) information (the tag-item matrix) and item content information for recommendation.\nLearning item representations becomes crucial in tag recommendation especially when the tag-item matrix is extremely sparse. Recently, models such as collaborative topic regression (CTR) [Wang and Blei, 2011] and its variants [Wang et al., 2013; Purushotham et al., 2012] have been proposed and adapted for tag recommendation to achieve promising performance. These models use latent Dirichlet allocation (LDA) [Blei et al., 2003] as the key component for learning item representations and use probabilistic matrix factorization (PMF) [Salakhutdinov and Mnih, 2008] to process the co-occurrence matrix (tag-item matrix). Although powerful approaches for tag recommendation, CTR and its variants suffer from inconsistent low dimensions corresponding to content topic space and co-occurrence matrix\u2019s latent factor space which typically have different sparsity and capacities. Deep learning based generative models, such as deep generative autoencoders [Wang et al., 2015] remedy this drawback by representing both content and tag co-occurrences in same lower dimensional space and show improvements over CTR. However, a cubic learning time complexity (in terms of low dimensional space) for these models prohibit effective posterior estimation and resort to approximations [Li and Yeung, 2009; Wang et al., 2015]."}, {"heading": "Approach", "text": "In this section we present our approach for learning distributed representations for tags and words in a common, low-dimensional embedding space. We consider the learning setting where training documents are annotated with their tags. Formally, we are given a corpus M of |M| documents {dm}|M|m=1 where each document dm is annotated with Tm tags, i.e. Tm = {tm,i}Tmi=1, that belong to a tag vocabulary of size T . Furthermore, each document dm is a sequence of Nm words, i.e. (wm,1, .., wm,Nm) coming from a word vocabulary of size V . Hereafter, we drop the document index m from its words and tags indices as it should be clear from the context. Given such data, our goal is to learn a D-dimensional continuous vector representations of words and tags that can be used to rank tags for a given document. These representations form matrices U \u2208 R{D\u00d7V } and V \u2208 R{D\u00d7T} Our pro-\nposed relevance feedback based representation learning model defines each tag (i.e. its embedding) that appears in the context of document m as:\nV(:,t) = Nm\u2211\ni\nf(U(:,wi),V(:,t)) Nm U(:,wi) (1)\nHere, f(U(:,wi),V(:,t)) (referred as f(w,t) hereafter) defines a similarity (or relevance) function between w and t based upon the word and tag representations, respectively. Note that unlike topic models, f(.) does not corresponds to a mixture model providing latent variable based association between a word and tag. Instead, the relevance model directly computes a semantic similarity between word and tag in representation space. Moreover, division by document length Nm avoids learning bias towards longer document. The recursive representation of tags can be seen as a feedback loop (depicted in Figure 1), and is learned iteratively. Also, note that if we set f(w, t) = 1, Eq. 1 refers to WSABIE [Weston et al., 2011].\nIn this work, we define three similarity functions in the order of complexity and expressive power: \u2022 Scalar Product: We define f(w, t) = g(w>t), where g = sigm as the scalar product of word and tag representations. This similarity measure weights each word based upon how similar it is with tag in the original representation space. The associated non-linearity helps switch-on relevant features while marginalizing non-relevant ones. This similarity model does not assume any complex interaction between word and tag. \u2022 Single Feedforward Layer: We define\nf(w, t) = r>g(R1.w\u2212R2.t)\nwhere g = sigm and r \u2208 R{1\u00d7k}, R1 \u2208 R{k\u00d7D}, R2 \u2208 R{k\u00d7D} are the parameters of the feedforward\nnetwork for similarity function. This similarity function computes similarity in a linearly transformed low dimensional space (similar to PCA). \u2022 Neural Tensor Layer: We define\nf(w, t) = r>g(w>M[1:k]t t + bt)\nwhere r \u2208 R{1\u00d7k}, Mt \u2208 R{D\u00d7D\u00d7k},bt \u2208 R{1\u00d7k} are the parameters of the tensor layer. Neural tensor layer computes the similarity between a tag-word pair in multiple contexts (or senses) where each slice of Mt corresponding to tag t defines a context along which to compute the similarity score. For example, with context technology, the similarity of tag apple will be higher to word device as opposed to word fruit. Also, this tensor layer let word and tag interact multiplicatively along different contexts and produce context dependent similarity score. Fig. 2 depicts the neural tensor based similarity function.\nAlthough tensor factorization has been shown to be successful in modeling various ternary relations (e.g. entity relation modeling [Socher et al., 2013], word sense disambiguation [Liu et al., 2015]), introducing tensor based parameters per tag is computation prohibitive and, due to addition of millions of parameters, can lead to overfitting. Therefore, we make two modifications: (i) we use same tensor for all the tags, and (ii) apply a tensor factorization approach that factorizes each tensor slice as the product of two low-rank matrices. Formally, each tensor slice M [i] \u2208 R{D\u00d7D} is factorized into two low rank matrices, P [i] \u2208 R{D\u00d7p} and Q[i] \u2208 R{p\u00d7D}, i.e.\nM [i] = P [i]Q[i], 1 \u2264 i \u2264 k, p << D"}, {"heading": "Objective function and Optimization", "text": "Based upon the tag representation mentioned above, we define a scoring function for a given document tag pair, (m, t) as\nL(m, t) = [f(U,V(:,t))]>Im[U>V(:,t)] (2)\nwhere Im \u2208 R{V\u00d7V } defines a diagonal matrix where diagonal entries are normalized word counts appearing in document m.\nWe use the contrastive max-margin criterion [Socher et al., 2013] to train our model 1. The main idea is that each pair (m, t) coming from the training corpus should receive a higher score than a pair (m, t\u2212) in which tag t\u2212 is a random tag. Let the set of all parameters be \u03a8, we minimize the following objective:\nJ (\u03a8) = \u2211\nM\n\u2211\nM\u2212 max\n{ 0, 1\u2212 L(m, t) + L(m, t\u2212) } + \u03bb||\u03a8||22\n(3) where M is the set of doc-tag pairs from training corpus (and M\u2212 corrupted pairs, i.e. (m, t\u2212)\u2019s where t\u2212 is picked randomly) and we score the correct pair higher than its corrupted one up to margin of 1. For each correct triplet we sample neg = 5 random corrupted triplets. We used standard L2 regularization of all the parameters, weighted by the hyperparameter \u03bb."}, {"heading": "Experiments", "text": "Datasets: For our experiments, we use two real-world datasets with one from Citeulike 2 and one from MovieLens 3 4: CiteULike: Our first dataset is originally from [Wang and Blei, 2011] and is collected from CiteULike database dump for over six years from 2004 to 2010. This dataset was originally designed for personalized document recommendation and consists of documents tagged by users with at least 10 articles. [Wang et al., 2013] further extended this dataset with corresponding tag information from citeulike website. Each article is mapped to papers that are indexed in CiteSeerX to extract their titles and abstracts, resulting in 16980 articles, 7386 tags, and 204987 tag-item pairs. Tags with frequencies less than 5 have also been removed from the dataset. MovieLens dataset : For our second dataset, we combined two publicly available datasets:(1) movie ratings dataset MovieLens 10M 5, consisting of movie ratings as well as tags for around 10K movies with around 25K unique tags; (2) movie synopses data set from Internet Movie DataBase (IMDB) 6. After mapping movies from MovieLens data with IMDB data, there are 5,333 distinct movies plots and 15,558 distinct tags."}, {"heading": "Baselines", "text": "Latent Space based Representation: Latent space based tag recommendation methods typically combine Latent Dirichlet Allocation [Blei et al., 2003] that learns item representations in one latent space with probabilistic matrix factorization (PMF) [Salakhutdinov and\n1Note that other choices of ranking criterion include WARP [Weston et al., 2011], however, we select max-margin criteria for its wide applicability and fair to baselines.\n2http://www.citeulike.org/faq/data.adp 3http://grouplens.org/datasets/movielens/ 4Datasets used and code are available at https://github.com/ktsaurabh/\nrecursive_WSABIE 5grouplens.org/datasets/movielens/ 6availabe at: ftp.fu-berlin.de/pub/misc/movies/database/\nMnih, 2008] that learns from tag-item co-occurrence matrix in another latent space. Collaborative topic regression (CTR) [Wang and Blei, 2011] learns from these two latent spaces simultaneously and achieve significant improvements over content, co-occurrence based, and hybrid methods for tag recommendation methods [Wang et al., 2013; Wang and Blei, 2011; Purushotham et al., 2012; Wang et al., 2015]. Therefore, we use CTR as our topical representations based tag recommendation baseline. Neural network based Representations: For our neural network based baselines, we use two broad categories of baselines: Generative Neural Architectures: Generative neural architecture such as Probabilistic Stacked Denoising Autoencoders [Wang et al., 2015] are generative variants of denoising autoencoders [Vincent et al., 2010] that tries to reconstruct a noisy version of the input by learning to predict clean input with an intermediate lowdimensional bottleneck layer. [Wang et al., 2015] extended the framework of denoising autoencoders by (i) proposing a deep architecture, and (2) assigning a data generating distributions to decoding layer of autoencoders and maximizing posterior probability of denoised input. [Wang et al., 2015](referred as SDAE) shows significant improvements over generative neural architectures for tag recommendation. We use SDAE as our second baseline. Supervised Neural Architectures: As mentioned earler, WSABIE [Weston et al., 2011] is one of the most popular supervised embeddings based representation learning approach and it\u2019s variants has been shown to perform well on various annotation and recommendation task e.g. image annotation with labels [Weston et al., 2013c], personalized tag recommendation [Denton et al., 2015], etc. Therefore, we use following variants of WSABIE (along with itself) as our baselines: \u2022 TagSpace: TagSpace [Weston et al., 2014] extends WSABIE with a convolution neural network (CNN) over word sequences in a document to get a document representation. WSABIE does not learn from sequence information present in a document which is captures by a CNN with variable length window over document\u2019s text. TagSpace has shown significant improvements over WSABIE for large scale hashtag representation [Weston et al., 2014]. \u2022 Affinity Weighted Embeddings: AWE [Weston et al., 2013b] provides a framework similar to ours which is suitable for feature modalities other than text. Their approach differs from ours in two main ways: (1) AWE does not account for document length normalization which penalizes shorter documents. (2) AWE does not account for complex multiplicative interaction between tags and words due to document\u2019s context. \u2022 CSRW: Compositional Semantic Relevance\nWeighted embeddings are our proposed repre-\n(c) a\n(d) b\nsentations for tag recommendation. We use three variants of CSRW corresponding to three relevance functions defined in previous section. That is, (i) CSRW-d for dot product based similarity function, (ii)CSRW-s for single layer feedforward network based similarity function, and (iii)CSRW-t for neural tensor based similarity function."}, {"heading": "Evaluation Settings", "text": "In each dataset, similar to [Wang et al., 2013; 2015], P items associated with each tag are randomly selected to form the training set and all the rest of the dataset is used as the test set. P is set to 1 and 10, respectively, to evaluate and compare the models under both sparse and dense settings in the experiments. For each value of P , the evaluation is repeated five times with different randomly selected training sets and the average performance is reported. Following [Wang et al., 2015; Wang and Blei, 2011; Wang et al., 2013], we use recall\nas the performance measure since the rating information appears in the form of implicit feedback [Rendle et al., 2009], which means a zero entry may be due to irrelevance between the tag and the item or the user\u2019s ignorance of the tags when tagging items. As such, precision is not suitable as a performance measure. Like most recommender systems, we sort the predicted ratings of the candidate tags and recommend the top K tags to the target item. The recall @ K for each item is defined as:\nRecall @ K = number of tags the item is associated with in top Ktotal number of tags the item is associated with For evaluating overall tag recommendation performance, we also use Mean Average Precision (MAP) as another metric. Apart from taking overall recall into consideration, it also measures overall precision of recommendation system. MAP is defined as:\nMAP = 1|M| \u2211\nm\u2208M\n1 |tm|\n\u2211\nj\u2208tm Precision(Rmj)\nwhere tm is the set of all the test tags for document m and R defines a ranking order on all the test tags of a document."}, {"heading": "Parameter settings:", "text": "For CTR and SDAE, we set parameters as described by [Wang et al., 2013] and [Wang et al., 2015] respectively. Specifically, for CTR we fix the topic size to be 50 and choose hyperparameter based upon a validation set. For SDAE, we use a 2-layer architecture (described as setting \u201820000-200-50-200-20000\u2019 in [Wang et al., 2015]). For WSABIE and AWE, we use the same settings as for CSRW based approaches. That is, we find the best performing dimension size (=200) based upon a grid search with a 5-fold cross-validation scheme. We use same objective function (i.e., Eq.2) for all WSABIE based baselines and CSRW models. We use SGD as our learning algorithm. For CSRW-s, we use the layer size, i.e., k = 16. For CSRW-t, we use tensor slices size, i.e., k=4 and factorization size, i.e., p = 16.\nResults: Fig. 3 shows the recall @ K, where K ranges from 50 to 300, for two datasets. For recall, our 95% confidence interval indicates between 3-5 %, for P = 10, and 4-6%, for P = 1, deviation from mean reported (higher for Movielens data), indicating statistical significance of results. Evidently, CSRW based approaches outperforms WSABIE variants as well as topic modeling based CTR and probabilistic deep network based SDAE. Interestingly, performance of SDAE is comparable to variants of WSABIE (i.e., AWE and TagSpace). AWE outperforms TagSpace indicating that non-linear weighting of word features provides a better fit to data. Furthermore, the difference between AWE and CSRW-d (which is equivalent to document normalized AWE) clearly highlights the significance of document normalization. The good performance of neural tensor based relevance layer can be attributed to a better capacity for tags and words to\ninteract multiplicatively. Sparsity has effect on the performance of all the models as expected. However, relative improvements for CSRW models are consistent with improvements upto 36% for higher K and 47% for lower K\u2019s for P = 1 and upto 32% for higher K and 65% for lower K\u2019s for P = 10. Table 1 shows the MAP for two datasets which follows a similar trend. Since MAP accommodates for Precision as well as overall performance of recommender system, CSRW provides significant improvements overall.\nTo elaborate the multi-factor interaction of tags and words, we present most similar tags to words based upon different slices of tensor M[k]. Table 2 shows anecdotal evidences of separation of senses of tags that corresponds to same keywords. Here, we rank tags based upon f(w, t)[i] = w>P[i]Q[i]>t and show top ranked tags in representative i. Clearly, the tags tend to cluster into type of usages, for example, tags corresponding to the keyword \u201cneural\u201d are clustered into computation and biological neural networks."}, {"heading": "Conclusion", "text": "We have presented a novel neural model based architecture that takes advantage of tags associated with documents to find meaningful representation of content and tags. We have presented a relevance feedback based neural representation learning framework where tags representations are learned by weighting words representations with its associated tags in a recursive fashion. We have shown that our modeling scheme outperform several state of the art baselines for tag recommendation task."}], "references": [{"title": "Learn", "author": ["Yoshua Bengio", "R\u00e9jean Ducharme", "Pascal Vincent", "Christian Janvin. A neural probabilistic language model. J. Mach"], "venue": "Res., pages 1137\u20131155,", "citeRegEx": "Bengio et al.. 2003", "shortCiteRegEx": null, "year": 2003}, {"title": "JMLR", "author": ["David M Blei", "Andrew Y Ng", "Michael I Jordan. Latent dirichlet allocation"], "venue": "3:993\u20131022,", "citeRegEx": "Blei et al.. 2003", "shortCiteRegEx": null, "year": 2003}, {"title": "User conditional hashtag prediction for images", "author": ["Denton et al", "2015] Emily Denton", "Jason Weston", "Manohar Paluri", "Lubomir Bourdev", "Rob Fergus"], "venue": "In Proceedings of the 21th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining,", "citeRegEx": "al. et al\\.,? \\Q2015\\E", "shortCiteRegEx": "al. et al\\.", "year": 2015}, {"title": "and Narayan Bhamidipati", "author": ["Nemanja Djuric", "Hao Wu", "Vladan Radosavljevic", "Mihajlo Grbovic"], "venue": "Hierarchical neural language models for joint representation of streaming documents and their content,", "citeRegEx": "Djuric et al.. 2015", "shortCiteRegEx": null, "year": 2015}, {"title": "interactive tag recommendation for flickr", "author": ["Nikhil Garg", "Ingmar Weber. Personalized"], "venue": "Proceedings of the 2008 ACM conference on Recommender systems, pages 67\u201374. ACM,", "citeRegEx": "Garg and Weber. 2008", "shortCiteRegEx": null, "year": 2008}, {"title": "RecSys \u201912", "author": ["Nikolas Landia. Utilising document content for tag recommendation in folksonomies. In ACM Recsys"], "venue": "pages 325\u2013328. ACM,", "citeRegEx": "Landia. 2012", "shortCiteRegEx": null, "year": 2012}, {"title": "In ICML", "author": ["Quoc V. Le", "Tomas Mikolov. Distributed representations of sentences", "documents"], "venue": "pages 1188\u20131196,", "citeRegEx": "Le and Mikolov. 2014", "shortCiteRegEx": null, "year": 2014}, {"title": "Relation regularized matrix factorization", "author": ["Wu-Jun Li", "Dit-Yan Yeung"], "venue": "IJCAI,", "citeRegEx": "Li and Yeung. 2009", "shortCiteRegEx": null, "year": 2009}, {"title": "Learning context-sensitive word embeddings with neural tensor skip-gram model", "author": ["Pengfei Liu", "Xipeng Qiu", "Xuanjing Huang"], "venue": "Twenty-Fourth International Joint Conference on Artificial Intelligence,", "citeRegEx": "Liu et al.. 2015", "shortCiteRegEx": null, "year": 2015}, {"title": "A content-based method to enhance tag recommendation", "author": ["Yu-Ta Lu", "Shoou-I Yu", "Tsung-Chieh Chang", "Jane Yung-jen Hsu"], "venue": "IJCAI,", "citeRegEx": "Lu et al.. 2009", "shortCiteRegEx": null, "year": 2009}, {"title": "In NIPS", "author": ["Tomas Mikolov", "Ilya Sutskever", "Kai Chen", "Greg S Corrado", "Jeff Dean. Distributed representations of words", "phrases", "their compositionality"], "venue": "pages 3111\u20133119,", "citeRegEx": "Mikolov et al.. 2013", "shortCiteRegEx": null, "year": 2013}, {"title": "In Proceedings of the 29th International Conference on Machine Learning (ICML-12)", "author": ["Sanjay Purushotham", "Yan Liu", "C-c J Kuo. Collaborative topic regression with social matrix factorization for recommendation systems"], "venue": "pages 759\u2013766,", "citeRegEx": "Purushotham et al.. 2012", "shortCiteRegEx": null, "year": 2012}, {"title": "pages 727\u2013736", "author": ["Steffen Rendle", "Leandro Balby Marinho", "Alexandros Nanopoulos", "Lars Schmidt-Thieme. Learning optimal ranking with tensor factorization for tag recommendation. In SIGKDD"], "venue": "ACM,", "citeRegEx": "Rendle et al.. 2009", "shortCiteRegEx": null, "year": 2009}, {"title": "In Proceedings of the 25th international conference on Machine learning", "author": ["Ruslan Salakhutdinov", "Andriy Mnih. Bayesian probabilistic matrix factorization using markov chain monte carlo"], "venue": "pages 880\u2013887. ACM,", "citeRegEx": "Salakhutdinov and Mnih. 2008", "shortCiteRegEx": null, "year": 2008}, {"title": "In Proceedings of the 18th ACM international conference on Multimedia", "author": ["Yi Shen", "Jianping Fan. Leveraging loosely-tagged images", "inter-object correlations for tag recommendation"], "venue": "pages 5\u201314. ACM,", "citeRegEx": "Shen and Fan. 2010", "shortCiteRegEx": null, "year": 2010}, {"title": "In Advances in Neural Information Processing Systems", "author": ["Richard Socher", "Danqi Chen", "Christopher D Manning", "Andrew Ng. Reasoning with neural tensor networks for knowledge base completion"], "venue": "pages 926\u2013934,", "citeRegEx": "Socher et al.. 2013", "shortCiteRegEx": null, "year": 2013}, {"title": "Stacked denoising autoencoders: Learning useful representations in a deep network with a local denoising criterion", "author": ["Vincent et al", "2010] Pascal Vincent", "Hugo Larochelle", "Isabelle Lajoie", "Yoshua Bengio", "Pierre-Antoine Manzagol"], "venue": "The Journal of Machine Learning Research,", "citeRegEx": "al. et al\\.,? \\Q2010\\E", "shortCiteRegEx": "al. et al\\.", "year": 2010}, {"title": "In Proceedings of the 17th ACM SIGKDD international conference on Knowledge discovery and data mining", "author": ["Chong Wang", "David M Blei. Collaborative topic modeling for recommending scientific articles"], "venue": "pages 448\u2013456. ACM,", "citeRegEx": "Wang and Blei. 2011", "shortCiteRegEx": null, "year": 2011}, {"title": "Assistive tagging: A survey of multimedia tagging with human-computer joint exploration", "author": ["Meng Wang", "Bingbing Ni", "Xian-Sheng Hua", "Tat-Seng Chua"], "venue": "ACM Computing Surveys (CSUR), 44(4):25,", "citeRegEx": "Wang et al.. 2012", "shortCiteRegEx": null, "year": 2012}, {"title": "and Wu-Jun Li", "author": ["Hao Wang", "Binyi Chen"], "venue": "Collaborative topic regression with social regularization for tag recommendation.", "citeRegEx": "Wang et al.. 2013", "shortCiteRegEx": null, "year": 2013}, {"title": "Relational stacked denoising autoencoder for tag recommendation", "author": ["Hao Wang", "Xingjian Shi", "Dit-Yan Yeung"], "venue": "AAAI,", "citeRegEx": "Wang et al.. 2015", "shortCiteRegEx": null, "year": 2015}, {"title": "Wsabie: Scaling up to large vocabulary image annotation", "author": ["Jason Weston", "Samy Bengio", "Nicolas Usunier"], "venue": "Twenty-Second International Joint Conference on Artificial Intelligence,", "citeRegEx": "Weston et al.. 2011", "shortCiteRegEx": null, "year": 2011}, {"title": "Large-scale music annotation and retrieval: Learning to rank in joint semantic spaces", "author": ["Jason Weston", "Samy Bengio", "Philippe Hamel"], "venue": "Journal of New Music Research,", "citeRegEx": "Weston et al.. 2012", "shortCiteRegEx": null, "year": 2012}, {"title": "In Proceedings of the 30th International Conference on Machine Learning (ICML-13)", "author": ["Jason Weston", "Ameesh Makadia", "Hector Yee. Label partitioning for sublinear ranking"], "venue": "pages 181\u2013189,", "citeRegEx": "Weston et al.. 2013a", "shortCiteRegEx": null, "year": 2013}, {"title": "Affinity weighted embedding", "author": ["Jason Weston", "Ron Weiss", "Hector Yee"], "venue": "arXiv preprint arXiv:1301.4171,", "citeRegEx": "Weston et al.. 2013b", "shortCiteRegEx": null, "year": 2013}, {"title": "In Proceedings of the 7th ACM conference on Recommender systems", "author": ["Jason Weston", "Hector Yee", "Ron J Weiss. Learning to rank recommendations with the k-order statistic loss"], "venue": "pages 245\u2013248. ACM,", "citeRegEx": "Weston et al.. 2013c", "shortCiteRegEx": null, "year": 2013}, {"title": "and Keith Adams", "author": ["Jason Weston", "Sumit Chopra"], "venue": "# tagspace: Semantic embeddings from hashtags.", "citeRegEx": "Weston et al.. 2014", "shortCiteRegEx": null, "year": 2014}, {"title": "In Proceedings of the 18th international conference on World wide web", "author": ["Lei Wu", "Linjun Yang", "Nenghai Yu", "Xian-Sheng Hua. Learning to tag"], "venue": "pages 361\u2013370. ACM,", "citeRegEx": "Wu et al.. 2009", "shortCiteRegEx": null, "year": 2009}], "referenceMentions": [{"referenceID": 20, "context": "Recent advances in learning latent representations, or embeddings based upon various neural architectures are proving to be successful in tag recommendation tasks [Wang et al., 2015; Weston et al., 2014], mainly due to compact representations of words and tags in continuous low dimensional space.", "startOffset": 163, "endOffset": 203}, {"referenceID": 26, "context": "Recent advances in learning latent representations, or embeddings based upon various neural architectures are proving to be successful in tag recommendation tasks [Wang et al., 2015; Weston et al., 2014], mainly due to compact representations of words and tags in continuous low dimensional space.", "startOffset": 163, "endOffset": 203}, {"referenceID": 10, "context": "For example, word2vec [Mikolov et al., 2013] tries to predict all the words in a document, given the embeddings of surrounding words.", "startOffset": 22, "endOffset": 44}, {"referenceID": 21, "context": "WSABIE [Weston et al., 2011] and its variants [Weston et al.", "startOffset": 7, "endOffset": 28}, {"referenceID": 24, "context": ", 2011] and its variants [Weston et al., 2013b], embed both the labels (or tags) and documents in a shared semantic space where document embedding predicts the \u201cclosest\u201d label embedding.", "startOffset": 25, "endOffset": 47}, {"referenceID": 21, "context": "WARP [Weston et al., 2011]) to discriminate among the relevant and non-relevant features while learning and making a prediction, causing the model to underfit.", "startOffset": 5, "endOffset": 26}, {"referenceID": 0, "context": "Distributed Representation Learning from Content and Tags Earlier research in distributed representation learning [Bengio et al., 2003] has focused on using probabilistic neural networks to build general representations of words that improve upon the classic ngram language models.", "startOffset": 114, "endOffset": 135}, {"referenceID": 10, "context": "More recently, this approach has been extended with two popular neural language models [Mikolov et al., 2013] for learning distributed representations of words, that are (1) continuous bag-ofwords model (CBOW) and (2) Skip-gram, collectively known as word2vec.", "startOffset": 87, "endOffset": 109}, {"referenceID": 6, "context": "Although Word2vec and its document based extensions [Le and Mikolov, 2014], learns unsupervised embeddings that are shown to be successful for various NLP related tasks [Djuric et al.", "startOffset": 52, "endOffset": 74}, {"referenceID": 3, "context": "Although Word2vec and its document based extensions [Le and Mikolov, 2014], learns unsupervised embeddings that are shown to be successful for various NLP related tasks [Djuric et al., 2015; Le and Mikolov, 2014], supervised embeddings such as WSABIE [Weston et al.", "startOffset": 169, "endOffset": 212}, {"referenceID": 6, "context": "Although Word2vec and its document based extensions [Le and Mikolov, 2014], learns unsupervised embeddings that are shown to be successful for various NLP related tasks [Djuric et al., 2015; Le and Mikolov, 2014], supervised embeddings such as WSABIE [Weston et al.", "startOffset": 169, "endOffset": 212}, {"referenceID": 21, "context": ", 2015; Le and Mikolov, 2014], supervised embeddings such as WSABIE [Weston et al., 2011] directly learns from tagword associations and outperform word2vec for prediction tasks [Weston et al.", "startOffset": 68, "endOffset": 89}, {"referenceID": 26, "context": ", 2011] directly learns from tagword associations and outperform word2vec for prediction tasks [Weston et al., 2014].", "startOffset": 95, "endOffset": 116}, {"referenceID": 22, "context": ", music annotation with textual tags [Weston et al., 2012], personalized video recommendation [Weston et al.", "startOffset": 37, "endOffset": 58}, {"referenceID": 23, "context": ", 2012], personalized video recommendation [Weston et al., 2013a], image annotation with labels, i.", "startOffset": 43, "endOffset": 65}, {"referenceID": 25, "context": "ImageNet [Weston et al., 2013c], personalized tag recommendation for images [Denton et al.", "startOffset": 9, "endOffset": 31}, {"referenceID": 26, "context": "[Weston et al., 2014] extended WSABIE with a convolutional neural network based document representation that can take word ordering into account in a supervised embedding framework.", "startOffset": 0, "endOffset": 21}, {"referenceID": 18, "context": "Tag Recommendation for Documents Tag recommendation methods can roughly be categorized into three classes [Wang et al., 2012]: content-based methods, co-occurrence based methods, and hybrid methods.", "startOffset": 106, "endOffset": 125}, {"referenceID": 14, "context": "Content-based methods [Shen and Fan, 2010; Landia, 2012; Lu et al., 2009] utilize only the content information (e.", "startOffset": 22, "endOffset": 73}, {"referenceID": 5, "context": "Content-based methods [Shen and Fan, 2010; Landia, 2012; Lu et al., 2009] utilize only the content information (e.", "startOffset": 22, "endOffset": 73}, {"referenceID": 9, "context": "Content-based methods [Shen and Fan, 2010; Landia, 2012; Lu et al., 2009] utilize only the content information (e.", "startOffset": 22, "endOffset": 73}, {"referenceID": 4, "context": "Co-occurrence based methods [Garg and Weber, 2008; Rendle et al., 2009] are similar to collaborative filtering (CF) methods [Li and Yeung, 2009].", "startOffset": 28, "endOffset": 71}, {"referenceID": 12, "context": "Co-occurrence based methods [Garg and Weber, 2008; Rendle et al., 2009] are similar to collaborative filtering (CF) methods [Li and Yeung, 2009].", "startOffset": 28, "endOffset": 71}, {"referenceID": 7, "context": ", 2009] are similar to collaborative filtering (CF) methods [Li and Yeung, 2009].", "startOffset": 60, "endOffset": 80}, {"referenceID": 27, "context": "The third class of methods [Wu et al., 2009; Wang and Blei, 2011; Wang et al., 2013; 2015], also the most popular and effective ones, consists of hybrid methods.", "startOffset": 27, "endOffset": 90}, {"referenceID": 17, "context": "The third class of methods [Wu et al., 2009; Wang and Blei, 2011; Wang et al., 2013; 2015], also the most popular and effective ones, consists of hybrid methods.", "startOffset": 27, "endOffset": 90}, {"referenceID": 19, "context": "The third class of methods [Wu et al., 2009; Wang and Blei, 2011; Wang et al., 2013; 2015], also the most popular and effective ones, consists of hybrid methods.", "startOffset": 27, "endOffset": 90}, {"referenceID": 17, "context": "Recently, models such as collaborative topic regression (CTR) [Wang and Blei, 2011] and its variants [Wang et al.", "startOffset": 62, "endOffset": 83}, {"referenceID": 19, "context": "Recently, models such as collaborative topic regression (CTR) [Wang and Blei, 2011] and its variants [Wang et al., 2013; Purushotham et al., 2012] have been proposed and adapted for tag recommendation to achieve promising performance.", "startOffset": 101, "endOffset": 146}, {"referenceID": 11, "context": "Recently, models such as collaborative topic regression (CTR) [Wang and Blei, 2011] and its variants [Wang et al., 2013; Purushotham et al., 2012] have been proposed and adapted for tag recommendation to achieve promising performance.", "startOffset": 101, "endOffset": 146}, {"referenceID": 1, "context": "These models use latent Dirichlet allocation (LDA) [Blei et al., 2003] as the key component for learning item representations and use probabilistic matrix factorization (PMF) [Salakhutdinov and Mnih, 2008] to process the co-occurrence matrix (tag-item matrix).", "startOffset": 51, "endOffset": 70}, {"referenceID": 13, "context": ", 2003] as the key component for learning item representations and use probabilistic matrix factorization (PMF) [Salakhutdinov and Mnih, 2008] to process the co-occurrence matrix (tag-item matrix).", "startOffset": 112, "endOffset": 142}, {"referenceID": 20, "context": "Deep learning based generative models, such as deep generative autoencoders [Wang et al., 2015] remedy this drawback by representing both content and tag co-occurrences in same lower dimensional space and show improvements over CTR.", "startOffset": 76, "endOffset": 95}, {"referenceID": 7, "context": "However, a cubic learning time complexity (in terms of low dimensional space) for these models prohibit effective posterior estimation and resort to approximations [Li and Yeung, 2009; Wang et al., 2015].", "startOffset": 164, "endOffset": 203}, {"referenceID": 20, "context": "However, a cubic learning time complexity (in terms of low dimensional space) for these models prohibit effective posterior estimation and resort to approximations [Li and Yeung, 2009; Wang et al., 2015].", "startOffset": 164, "endOffset": 203}, {"referenceID": 21, "context": "1 refers to WSABIE [Weston et al., 2011].", "startOffset": 19, "endOffset": 40}, {"referenceID": 15, "context": "entity relation modeling [Socher et al., 2013], word sense disambiguation [Liu et al.", "startOffset": 25, "endOffset": 46}, {"referenceID": 8, "context": ", 2013], word sense disambiguation [Liu et al., 2015]), introducing tensor based parameters per tag is computation prohibitive and, due to addition of millions of parameters, can lead to overfitting.", "startOffset": 35, "endOffset": 53}, {"referenceID": 15, "context": "We use the contrastive max-margin criterion [Socher et al., 2013] to train our model 1.", "startOffset": 44, "endOffset": 65}, {"referenceID": 17, "context": "CiteULike: Our first dataset is originally from [Wang and Blei, 2011] and is collected from CiteULike database dump for over six years from 2004 to 2010.", "startOffset": 48, "endOffset": 69}, {"referenceID": 19, "context": "[Wang et al., 2013] further extended this dataset with corresponding tag information from citeulike website.", "startOffset": 0, "endOffset": 19}, {"referenceID": 1, "context": "Latent Space based Representation: Latent space based tag recommendation methods typically combine Latent Dirichlet Allocation [Blei et al., 2003] that learns item representations in one latent space with probabilistic matrix factorization (PMF) [Salakhutdinov and", "startOffset": 127, "endOffset": 146}, {"referenceID": 21, "context": "1Note that other choices of ranking criterion include WARP [Weston et al., 2011], however, we select max-margin criteria for its wide applicability and fair to baselines.", "startOffset": 59, "endOffset": 80}, {"referenceID": 17, "context": "Collaborative topic regression (CTR) [Wang and Blei, 2011] learns from these two latent spaces simultaneously and achieve significant improvements over content, co-occurrence based, and hybrid methods for tag recommendation methods [Wang et al.", "startOffset": 37, "endOffset": 58}, {"referenceID": 19, "context": "Collaborative topic regression (CTR) [Wang and Blei, 2011] learns from these two latent spaces simultaneously and achieve significant improvements over content, co-occurrence based, and hybrid methods for tag recommendation methods [Wang et al., 2013; Wang and Blei, 2011; Purushotham et al., 2012; Wang et al., 2015].", "startOffset": 232, "endOffset": 317}, {"referenceID": 17, "context": "Collaborative topic regression (CTR) [Wang and Blei, 2011] learns from these two latent spaces simultaneously and achieve significant improvements over content, co-occurrence based, and hybrid methods for tag recommendation methods [Wang et al., 2013; Wang and Blei, 2011; Purushotham et al., 2012; Wang et al., 2015].", "startOffset": 232, "endOffset": 317}, {"referenceID": 11, "context": "Collaborative topic regression (CTR) [Wang and Blei, 2011] learns from these two latent spaces simultaneously and achieve significant improvements over content, co-occurrence based, and hybrid methods for tag recommendation methods [Wang et al., 2013; Wang and Blei, 2011; Purushotham et al., 2012; Wang et al., 2015].", "startOffset": 232, "endOffset": 317}, {"referenceID": 20, "context": "Collaborative topic regression (CTR) [Wang and Blei, 2011] learns from these two latent spaces simultaneously and achieve significant improvements over content, co-occurrence based, and hybrid methods for tag recommendation methods [Wang et al., 2013; Wang and Blei, 2011; Purushotham et al., 2012; Wang et al., 2015].", "startOffset": 232, "endOffset": 317}, {"referenceID": 20, "context": "Generative Neural Architectures: Generative neural architecture such as Probabilistic Stacked Denoising Autoencoders [Wang et al., 2015] are generative variants of denoising autoencoders [Vincent et al.", "startOffset": 117, "endOffset": 136}, {"referenceID": 20, "context": "[Wang et al., 2015] extended the framework of denoising autoencoders by (i) proposing a deep architecture, and (2) assigning a data generating distributions to decoding layer of autoencoders and maximizing posterior probability of denoised input.", "startOffset": 0, "endOffset": 19}, {"referenceID": 20, "context": "[Wang et al., 2015](referred as SDAE) shows significant improvements over generative neural architectures for tag recommendation.", "startOffset": 0, "endOffset": 19}, {"referenceID": 21, "context": "WSABIE [Weston et al., 2011] is one of the most popular supervised embeddings based representation learning approach and it\u2019s variants has been shown to perform well on various annotation and recommendation task e.", "startOffset": 7, "endOffset": 28}, {"referenceID": 25, "context": "image annotation with labels [Weston et al., 2013c], personalized tag recommendation [Denton et al.", "startOffset": 29, "endOffset": 51}, {"referenceID": 26, "context": "\u2022 TagSpace: TagSpace [Weston et al., 2014] extends", "startOffset": 21, "endOffset": 42}, {"referenceID": 26, "context": "TagSpace has shown significant improvements over WSABIE for large scale hashtag representation [Weston et al., 2014].", "startOffset": 95, "endOffset": 116}, {"referenceID": 19, "context": "In each dataset, similar to [Wang et al., 2013; 2015], P items associated with each tag are randomly selected to form the training set and all the rest of the dataset is used as the test set.", "startOffset": 28, "endOffset": 53}, {"referenceID": 20, "context": "Following [Wang et al., 2015; Wang and Blei, 2011; Wang et al., 2013], we use recall as the performance measure since the rating information appears in the form of implicit feedback [Rendle et al.", "startOffset": 10, "endOffset": 69}, {"referenceID": 17, "context": "Following [Wang et al., 2015; Wang and Blei, 2011; Wang et al., 2013], we use recall as the performance measure since the rating information appears in the form of implicit feedback [Rendle et al.", "startOffset": 10, "endOffset": 69}, {"referenceID": 19, "context": "Following [Wang et al., 2015; Wang and Blei, 2011; Wang et al., 2013], we use recall as the performance measure since the rating information appears in the form of implicit feedback [Rendle et al.", "startOffset": 10, "endOffset": 69}, {"referenceID": 12, "context": ", 2013], we use recall as the performance measure since the rating information appears in the form of implicit feedback [Rendle et al., 2009], which means a zero entry may be due to irrelevance between the tag and the item or the user\u2019s ignorance of the tags when tagging items.", "startOffset": 120, "endOffset": 141}, {"referenceID": 19, "context": "For CTR and SDAE, we set parameters as described by [Wang et al., 2013] and [Wang et al.", "startOffset": 52, "endOffset": 71}, {"referenceID": 20, "context": ", 2013] and [Wang et al., 2015] respectively.", "startOffset": 12, "endOffset": 31}, {"referenceID": 20, "context": "For SDAE, we use a 2-layer architecture (described as setting \u201820000-200-50-200-20000\u2019 in [Wang et al., 2015]).", "startOffset": 90, "endOffset": 109}], "year": 2016, "abstractText": "We consider the problem of learning distributed representations for tags from their associated content for the task of tag recommendation. Considering tagging information is usually very sparse, effective learning from content and tag association is very crucial and challenging task. Recently, various neural representation learning models such as WSABIE and its variants show promising performance, mainly due to compact feature representations learned in a semantic space. However, their capacity is limited by a linear compositional approach for representing tags as sum of equal parts and hurt their performance. In this work, we propose a neural feedback relevance model for learning tag representations with weighted feature representations. Our experiments on two widely used datasets show significant improvement for quality of recommendations over various baselines.", "creator": "LaTeX with hyperref package"}}}