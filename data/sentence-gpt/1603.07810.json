{"id": "1603.07810", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "25-Mar-2016", "title": "Conditional Similarity Networks", "abstract": "In typical perceptual tasks, higher-order concepts are inferred from visual features to assist with perceptual decision making. However, there is a multitude of visual concepts which can be inferred from a single stimulus. When learning nonlinear embeddings with siamese or triplet networks from similarities, we typically assume they are sourced from a single visual concept, instead of relying upon the visual information of the stimuli. This is a common example of low-level inferential inference.\n\n\n\n\nThe basic structure of the neural network\nThe neural network is often used for learning multiple stimuli at a single time. For example, if you see a scene on a screen with two different visual elements (such as the white eye, green eyes) with different visual elements, you can expect two distinct images (or two different types of images) and the same result in one particular word of the same word. While we may expect different images, we are always able to infer patterns of sensory inputs by using this same neural network. In some cases, the brain can infer a single sensory input and thus perform more of the same perceptual functions in multiple situations.\nThe neural network consists of four regions of the brain called the sub-lateral brain (RBD) and includes four sub-lateral regions of the cortex (CFC). The sub-lateral area includes two sub-lateral areas (SC). This is the most important area in the brain that is involved in processing visual information. Therefore, if you have a visual component, you need to consider a task.\nThe task consists of three sub-lateral regions that are involved in processing visual information. Each sub-lateral region contains three sub-lateral areas (SC). Each sub-lateral region contains three sub-lateral areas (SC). Each sub-lateral region contains four sub-lateral areas (SC). Each sub-lateral area contains two sub-lateral areas (SC). Each sub-lateral area contains three sub-lateral areas (SC). Each sub-lateral area contains three sub-lateral areas (SC).\nIf you have a visual component, you can expect to infer patterns of sensory inputs by using this same neural network. If you have a visual component, you can expect to infer patterns of sensory inputs by using this same neural network. In some cases, the brain can infer a single sensory input and thus perform more of the same perceptual functions in multiple situations. However, there is a multitude of visual concepts which can", "histories": [["v1", "Fri, 25 Mar 2016 02:52:02 GMT  (1639kb,D)", "http://arxiv.org/abs/1603.07810v1", null], ["v2", "Mon, 19 Dec 2016 12:41:01 GMT  (2061kb,D)", "http://arxiv.org/abs/1603.07810v2", null], ["v3", "Mon, 10 Apr 2017 15:18:21 GMT  (2079kb,D)", "http://arxiv.org/abs/1603.07810v3", "CVPR 2017"]], "reviews": [], "SUBJECTS": "cs.CV cs.AI cs.LG", "authors": ["andreas veit", "serge belongie", "theofanis karaletsos"], "accepted": false, "id": "1603.07810"}, "pdf": {"name": "1603.07810.pdf", "metadata": {"source": "CRF", "title": "Disentangling Nonlinear Perceptual Embeddings With Multi-Query Triplet Networks", "authors": ["Andreas Veit", "Serge Belongie", "Theofanis Karaletsos"], "emails": ["sjb}@cs.cornell.edu,", "karaletsos@cbio.mskcc.org"], "sections": [{"heading": null, "text": "Keywords: Similarity based learning, Triplet Networks, Deep Learning, Representation Learning"}, {"heading": "1 Introduction", "text": "When learning nonlinear features from constraints of object similarities and dissimilarities, objects are embedded in a feature-vector space, in which their distances preserve the relative dissimilarity. The similarities can be encoded pairwise or in higher order relationships such as triplets. Commonly, convolutional neural networks are trained to transform the visual stimuli into the featurevectors. When learning from (dis-)similarities, a few striking simplifying key assumptions are commonly made: it is assumed that the measure of similarity is known, clear and unique. However, in practice objects can be compared according to a multitude of semantic similarities. If they contradict, it is not possible to find a single space capturing the plethora of semantic subtleties present in most perceptual tasks.\nar X\niv :1\n60 3.\n07 81\n0v 1\n[ cs\nAn illustrative example to consider is the comparison of coloured geometric shapes, a task toddlers are regularly exposed to with benefits to concept learning. Consider, that a red triangle and a red circle are very similar in terms of colour, more so than a red triangle and a blue triangle. However, the triangles are more similar to one another in terms of shape than the triangle and the circle. Nonlinear embeddings are trained to minimize distances between perceptually similar objects. As such, a successful embedding necessarily needs to be aware of the visual concept the perceiving agent attends to when judging a similarity.\nOne way to address this issue is to learn separate models or embeddings for each visual task, resulting in a multitude of weakly informed nonlinear feature extractors and embeddings, but avoiding the problem of dealing with contradictions in the semantics they are supposed to capture. However, the idea of building a separate perceptual system for each visual task is wasteful in terms of parameters needed as well as the associated need for training-data and redundancy in parameters, not to mention training time. Given the wealth of recent results on distilling nonlinear ensembles into compact models, the question arises whether this solution can be combined into a joint system, which can learn richer and semantically stronger visual features in conjuncture with concept-aware embeddings. In principle, such a system could model the correlations of nonlinear visual features when attending to different abstract concepts and thus deal with contradicting similarity measures.\nIn this work, we propose Multi-Query Networks (MQNs) to tackle this problem with a joint nonlinear architecture that is trained end-to-end. We leverage recent results in representation learning [1] to port the idea of factorized triplet embeddings to convolutional triplet networks and explore applications to computer vision tasks. Figure 1 provides an overview of the framework. Our proposed system combines an arbitrary amount of different given notions of similarity into one compact system. We show that the resulting model learns visually relevant and semantically powerful subspaces without compromising the discriminative\npower of the model. MQNs clearly outperform single triplet networks, and even sets of separate models for each visual task.\nOur contributions are a) formulating a simple way for end-to-end training of convolutional feature learning and embedding systems using multiple triplet queries jointly which we term triplet distillation, b) demonstrating that these models outperform strong baselines in a variety of hard predictive visual tasks and c) demonstrating that the proposed models successfully disentangle the embedding features into meaningful dimensions, providing semantically consistent explanations for the data."}, {"heading": "2 Related Work", "text": "Similarity based learning has emerged as a broad field of interest in modern computer vision and has been used in many contexts. Disconnected from the input image, triplet based similarity embeddings, can be learned using crowdkernels [2]. Further, Tamuz et al. [3] introduce a probabilistic treatment for triplets and learn an adaptive crowd kernel. Similar work has been generalized to multiple-views and clustering settings by Amid and Ukkonen [4] as well as Van der Maaten and Hinton [5]. A combination of triplet embeddings with input kernels was presented by Wilber et al. [6], but this work did not include joint feature and embedding learning. An early approach to connect input features with embeddings has been to learn image similarity functions through ranking [7].\nA foundational line of work combining similarities with neural network models to learn visual features from similarities revolves around Siamese networks [8,9], which use pairwise distances to learn embeddings discriminatively. In contrast to pairwise comparisons, triplets have a key advantage due to their flexibility in capturing a variety of higher-order similarity constraints rather than the binary similar/dissimilar statement for pairs. Neural networks to learn visual features from triplet based similarities have been used by Wang et al. [10] and Schroff et al. [11], where use of supervised features with crowd-inferred similarities boosts performance in face verification and fine-grained visual categorization tasks. A key insight from these works is that semantics as captured by triplet embeddings are a natural way to represent complex class-structures when dealing with problems of high-dimensional categorization and greatly boost the ability of models to share information between classes.\nDisentangling representations is a major topic in the recent machine learning literature and has for example been tackled using Boltzmann Machines by Reed et al. [12]. Within this stream of research, the work closest to ours is that of Karaletsos et al. [1] on representation learning which introduces a joint generative model over inputs and triplets to learn a factorized latent space. However, the focus of that work is the generative aspect of disentangling representations and proof of concept applications to low-dimensional data. Our work introduces a convolutional embedding architecture that forgoes the generative pathway in favor of exploring applications to embedding high-dimensional image data. We thus demonstrate that the generative interpretation is not required to reap the\nbenefits of Multi-Query Networks and demonstrate in particular their use in common computer vision tasks.\nA theme in our work is the goal of modeling separate learning signals within the same system by factorizing latent spaces. We note the relation of these goals to a variety of approaches used in learning representations. Multi-view learning [13,14] has been used for 3d shape inference and shown to generically be a good way to learn factorized latent spaces. Multiple kernel learning [15,16] employs information encoded in different kernels to provide predictions using the synthesized complex feature space and has also been used for similarity-based learning by McFee and Lanckriet [17]. Multi-task learning approaches [18] are used when information from disparate sources or using differing assumptions can be combined beneficially for a final prediction task. However, we focus on the setting of multi-view weak supervision using triplets as a distinctive feature of our work. We consider disentangled latent spaces, which is also considered in multilinear network architectures [19] focusing on factorizations of feature spaces in neural networks, but differ in many details. An interesting link also exists to multiple similarity learning [20], where category specific similarities are used to approximate a fine-grained global embedding. Our global factorized embeddings can be thought of as an approach to capture similar information in a shared space directly through feature learning.\nFurthermore we discuss the notion of attention in our work, since we employ gates in order to attend to the mentioned subspaces of the inferred embeddings when focusing on particular visual tasks. This term may be confused with spatial attention [21], but bears similarity insofar as it shows that the ability to gate the focus of the model on relevant dimensions (in our case in latent space rather than observed space) is beneficial both to the semantics and to the quantitative performance of our model."}, {"heading": "3 Multi-Query Triplet Networks", "text": "Our goal is to learn a nonlinear feature embedding f(x), from an image x into a feature space Rd, such that for a pair of images x1 and x2, the Euclidean distance between f(x1) and f(x2) reflects their semantic dis-similarity. In particular, we strive for the distance between images of semantically similar objects to be small, and the distance between images of semantically different objects to be large. This relationship should hold independent of imaging conditions.\nWe consider y = Wg(x) to be an embedding of observed images x into coordinates in a feature space y. Here, f(x) = Wg(x) clarifies that the embedding function is a composition of an arbitrarily nonlinear function g(\u00b7) and a linear projection W , for W \u2208 Rd\u00d7b, where d denotes the dimensions of the embedding and b stands for the dimensions of the output of the nonlinear function g(\u00b7). In general, we denote the parameters of function f(x) by \u03b8, denoting all the filters and weights.\nMulti-Query Networks 5{\nembedding features\nmasks\nquery selects mask\nmasked features for specific subspace\nFig. 2. The masking operation selects relevant embedding dimensions, given a query index. Masking can be seen as a soft gating function, to attend to a particular concept.\nQuery-Dependent Triplets Apart from observing images x, we are also given a set of triplet constraints stemming from an unknown oracle distribution we can sample from, such as a crowd. We define triplet constraints in the following.\nGiven an unknown query-dependent similarity function sQ(\u00b7, \u00b7) for an unobserved feature space \u03a6, an unknown oracle such as a crowd can compare observed images x1, x2 and x3 with reference to x1 by applying sQ(y1, y2) = sQ(\u03a6(x1), \u03a6(x2)) and sQ(y1, y3) = sQ(\u03a6(x1), \u03a6(x3)). It then returns an ordering over these two distances, which we call a triplet t, defined as the set of indices {1, 3, 2} if sQ(y1, y3) is larger than sQ(y1, y2). The query Q serves as a switch between attented visual concepts and can effectively gate between different similarity functions sQ.\nWe define the set of all triplets related to query Q as:\nTQ = {(i, j, l;Q) | sQ(yi, yj) > sQ(yi, yl)}. (1)\nWe do not have access to the exhaustive set TQ, but can sample K-times from it using the oracle to yield a finite sample TQK = {tk}Kk=1.\nTriplets have also been trivially generalized to quadruplets to encode analogical reasoning, but the key insight is that using more than pairwise relationships encodes higher-order knowledge with weak and distant supervision effectively. Therefore, triplets are at the core of the exhibition of our method as the simplest sufficient element to use implicit oracles for learning.\nLearning From Triplets The feature space spanned by our model is given by function f(\u00b7). We guide learning this nonlinear function such that it represents concepts in a functionally similar way as \u03a6. i.e., to be consistent with respect to the observed triplets. We define an energy function ET (\u00b7) over triplets as a means to model the similarity structure over observed images given by triplets. To be able to model the query-dependence, we introduce masks m over the embedding with m \u2208 Rb\u00d7nq where nq is the number of possible visual concepts we wish to obtain triplets for. We also define parameters \u03b2m of the same dimension as m such that m = \u03c3(\u03b2) for \u03c3 denoting the sigmoid function which ensures that mask values lie between 0 and 1. The mask plays the role of a gating function selecting the relevant dimensions of the embedding required to attend to a particular concept. As such, we denote mq to be the selection of the q-th mask column of\ndimension b (in pseudocode mq = m[:, q]). Given an observed triplet t = {i, j, k} defined over indices strictly pointing to a subset of the observed images and a corresponding query-index q and a scalar margin h warding against trivial solutions, the energy function ET (\u00b7) is given by:\nET (xi, xj , xk, q;m, \u03b8) = max{0, D(xi, xj ;mq, \u03b8)\u2212D(xi, xk;mq, \u03b8) + h} (2)\ngiven a distance D between two images defined as:\nD(xi, xj ;mq) = \u2016f(xi; \u03b8)mq \u2212 f(xj ; \u03b8)mq\u20162. (3)\nAs we see, Equation 2 follows the same general structure as generic triplet loss-functions commonly used (where the distance function isD(xi, xj) = \u2016f(xi; \u03b8)\u2212 f(xj ; \u03b8)\u20162.), but involves a query-aware masked distance function given by Equation 3. While appearing to be a small technical change, the inclusion of a masking or gating mechanism for the triplet-loss has a highly non-trivial effect. The role of the masking operation is visually sketched in Figure 2. The query index selects a mask which induces a subspace over the relevant embedding dimensions, effectively attending only to the relevant dimensions for the visual concept being queried by performing relevance determination during training. In the energy function above, that translates into a modulated energy cost phasing out Euclidian distances between irrelevant feature-dimensions while preserving the loss-structure of the relevant ones. We borrow a term from neural network ensemble training and coin this procedure triplet distillation, since it leads to one joint network distilling multiple triplet networks into a compact architecture.\nEncouraging Regular Embeddings The embedding function we use is y = f(x). We want to encourage embeddings to be drawn from a unit ball in order to maintain a regularity in the latent space. We encode that in an embedding energy function EW given by:\nEW (x; \u03b8) = \u2016f(x; \u03b8)\u201622 = \u2016y\u201622 (4)\nWithout this term, an optimization scheme may choose to inflate embeddings to create space for new data points instead of learning appropriate parameters for the learned feature extractors.\nJoint Formulation For Convolutional MQNs We define a loss-function LMQN for training MQNs by putting together the defined energy functions concerning the valid embedding of observed images and the valid constraints given by sampled triplets:\nLMQN (x, {t,q}; m, \u03b8) = ET (xt0 , xt1 , xt2 , q; m, \u03b8) + \u03bbEW (x, \u03b8) (5)\nThe parameter \u03bb weights the contributions of the triplet terms against the regular embedding terms. In our paper, f(x) = Wg(x), where g(x) is a convolutional neural network.\nThe masked learning procedure leads f(\u00b7) towards solutions which are consistent with triplets over varying abstract concepts. As a consequence, embedding dimensions encode features associated to specific semantic concepts. Then, during test time each observed image can be mapped into this embedding by f(\u00b7) in a way that abstracts the images and predicts query-related concepts. We call a feature space spanned by a function with this property disentangled , as it preserves the concept separation through test time."}, {"heading": "4 Results", "text": "In this section, we introduce the datasets used and present qualitative as well as quantitative evaluations of our work. We generate triplet constraints using datasets with annotated attributes. We learn fine-grained embeddings using MQNs and strong competing baselines. We focus on evaluating the quality of the embeddings and the learned convolutional filters, both separately and jointly. We show experiments highlighting the semantic structure of embeddings and the subspaces in particular, the ability to predict unseen triplets and fine-grained out-of-task classification performance. We describe our experimental setup in more detail in the supplemetary material."}, {"heading": "4.1 Datasets", "text": "We perform experiments on two different datasets. First, we consider a dataset of fonts1 collected by Bernhardsson. The dataset contains 3.1 million images of\n1 http://erikbern.com/2016/01/21/analyzing-50k-fonts-using-deep-neural-networks/\nsingle characters in gray scale with a size of 64 by 64 pixels. The dataset exhibits variations according to font style and character type, e.g., \u2018a\u2019, \u2018b\u2019. In particular, it contains 62 types of characters and 50,000 fonts. For our experiments we only use the first 1,000 fonts. We also consider the Zappos50k shoe dataset [22] collected by Yu and Grauman. The dataset contains 50,000 images of individual richly annotated shoes exhibiting multiple complex variations. The images are iconic and have a size of 136 by 102 pixels, which we resize to 112 by 112 pixels in our experiments. In particular, we are looking into four different characteristics: the type of the shoes (i.e., shoes, boots, sandals or slippers), the suggested gender of the shoes (i.e., for women, men, girls or boys), the height of the shoes\u2019 heels (numerical measurements from 0 to 5 inches) and the closing mechanism of the shoes (buckle, pull on, slip on, hook and loop or laced up). We also use the shoes\u2019 brand information to perform a fine-grained classification test."}, {"heading": "4.2 Visual Exploration of the Learned Subspaces", "text": "We visually explore learned embeddings regarding their consistency according to respective queries. Figure 3 shows embeddings of the two subspaces in the Fonts dataset, which we project down to two dimensions using t-SNE [23]. On the left,\nwe show the embedding according to character type and on the right according to font style. The learned features are disentangled such that those selected by the first mask group images by character type and those selected by the second mask by font style. Figures 4 and 5 show embeddings of the four subspaces learned with an MQN on the Zappos50k dataset. Figure 4(a) shows the embedding corresponding to the closure mechanism of the shoes. Figure 4(b) shows the embedding attending to type of the shoes. The embedding clearly separates the different types of shoes into boots, slippers and so on. Highlighted areas reveal some interesting details. For example, the highlighted region on the upper right side shows nearby images of the same type (\u2019shoes\u2019) that are completely different in all other aspects. Figure 5(a) shows the subspace for suggested gender for the shoes. The subspace separates shoes that are for female and male buyers as well as shoes for adult or youth buyers. The learned submanifold occupies a rotated square with axes defined by gender and age. Finally, Figure 5(b) shows a continuous embedding of heel heights, which is a subtle visual feature.\nWe stress that all of these semantic representations are taking place within a shared space produced by the same network. It is disentangled by the masking operation during learning such as to yield both semantic embeddings as well as a feature-selection mechanism (the masks) helpful in analyzing those subspaces."}, {"heading": "4.3 Qualitative Analysis Of Subspaces", "text": "The key feature of MQNs is the fact that they share the same convolutional filters but can adaptively learn compartmentalized semantic subspaces in the embeddings. As seen in Section 3, the driving force for that separation is both the existence of triplet information regarding multiple concepts as well as the masking mechanism allowing the model to learn from all these concepts jointly. We visualize the masks for our common model choices in Figure 6. We show the traditional triplet loss, a manually set mask factorizing the embeddings into disjoint features and a fully learned mask, which shows that the model itself disentangles masks into private and some shared features for each query. This speaks for the mechanism as an exploratory tool as it reveals the structure of the shared spaces. Unsurprisingly features are not as heavily shared as a fully joint space and not as disjoint as a fully factorized space."}, {"heading": "4.4 Quantitative Analysis Using Triplet Prediction", "text": "We evaluate the performance of our model on fine-grained similarity judgments in terms of triplet prediction on held-out triplets. Further, we evaluate the impact of the number of unique triplets available during training on the performance.\nWe compare different types of triplet networks. All networks are trained on the same set of triplets and only differ in the way they are trained. First, we compare our model to the state-of-the-art in triplet networks, which aim to learn from all available triplets jointly as if they belonged to the same concept. We denote this architecture as Single Query Network (SQN). Second, we compare to task-specific networks (TSN), an ensemble of separate specialist networks, each of which is trained on a single notion of similarity with corresponding triplets. This can be considered a high watermark as it is the best model achievable with currently available methods. We explore different Multi-Query Networks with\nrising level of integration into learning. First, we keep the convolutional features fixed to be imagenet-initialized and just learn the embedding-layer using the MQN triplet loss with pre-fixed masks (in-MQN-fm). Second, we jointly learn convolutional filters and embeddings with pre-fixed masks in order to test the effect of the feature disentanglement (MQN-fm). Third, we jointly learn filters, embeddings and masks from data in end-to-end fashion (MQN-lm).\nWe first train each model on fixed sets of triplets with one set for each query. After convergence, we evaluate the models on a fixed separate holdout testset. We report performance in terms of prediction error. Since triplet prediction is a binary task, random guessing would perform at 50%. We compare the performance of models trained on 10k, 50k, 100k and all available unique triplets.\nFigure 7 shows that triplet networks generally improve with more available triplets. SQN fail to capture fine-grained similarity and only reach a top error rate of 15.81%. TSN greatly improve on that, achieving an error rate of 1.29%. This shows that learning a single space is not capable of capturing multiple similarity notions. MQN-fm achieve a top error rate of 0.74%, clearly outperforming both the single query networks as well as the set of specialist networks. This means that in addition to learning subspaces that successfully capture the visual characteristics for the different task, MQNs benefit from learning all concept within one model. We expected MQN-lm to beat all models following this trend by also being able to learn masks. However, it reaches an error performance of 6.68% in-between the baselines and the MQN models. We attribute this to either a higher need for training data to resolve the ambiguities in mask training for all 4 tasks or to the slower convergence due to the harder optimization problem being solved. This is further supported by the observation of unfavourable performance of MQN-lm in low-triplet regimes compared to SQN. Pre-fixing the masks resolves a lot of uncertainty in the models, allowing the optimization to focus on disentangling the networks\u2019 output into semantically meaningful fea-\ntures. in-MQN-fm have an error of 16.53%. We observe that they outperform both SQN and MQN-lm in the low-triplet regime, indicating that learning the masked embedding alone does not require many triplets. Further, it shows that imageNet pretrained features are a non-trivial baseline for triplet prediction. Lastly, the similar performance of no convolutional fine-tuning to SQN in the high-label regime indicates that it is the semantic disentangling of the features that lead to the great performance improvement of MQN.\nSummarizing the results, we see that MQNs outperform all models, even the task-specific networks which have a lot more parameters available for learning. This points towards knowledge transfer from the different tasks, supporting learning of more appropriate convolutional filters than in cases where the differing tasks are not distilled into the same model. The effects come from the combination of jointly learning masked embeddings and semantically disentangled features from multiple queries."}, {"heading": "4.5 Quantitative Analysis Of Feature Extractors Using Off-Task Classification", "text": "We evaluate the performance of MQNs and baselines on an off-task fine-grained classification setting to test their generic value as components in other vision tasks. In particular, we perform brand classification on the Zappos dataset, which is a task we did not collect triplets for. Intuitively, if triplet knowledge-transfer is useful to explain away image-content, we would expect triplet models to resolve uncertainties arising in classification with less required label data, similar to semi-supervised learning. However, features learned for similarity comparisons in embeddings generally trade-off discriminating features for fine-grained descriptive features, which could compromise classification performance.\nWe compare three different models, SQN, imageNet and MQN-fm. All of them are residual networks of the same size, the second is fixed in its pretrained state while the others are refined using the same triplets and their respective losses as described in Section 4.4. None of them has seen any information about the brand of the shoes during training. We picked the 30 brands in the Zappos dataset which have the most examples and select 254 each, split into training, validation and test sets (194, 20 and 40). We replace the last linear layer for each network with one hidden and one 30-way classification layer. Then, we only train these layers for brand classification and keep the conv filters fixed, to focus our evaluation on the classification capacities of the feature extractors. We compare the performance of the approaches for different amounts of available training labels (1000, 3000 and 5820 labels).\nSQN and MQN-fm start from the same initialization (imageNet) and are trained using the same triplets. Figure 8 shows the classification accuracy as a function of available labels. It is striking, that SQN performs quite poorly compared to not just MQN-fm, but especially compared to the baseline imageNet, which it also was initialized to. This documents that training one architecture with triplets generated through different queries naively actually hurts generalization performance on discriminative tasks in addition to the poor performance in triplet prediction tasks shown in Section 4.4. Another interesting conclusion our experiments suggest is that MQN-fm actually improves the convolutional features, since it outperforms imageNet on an off-task in the low-label setting while maintaining comparable performance when heavily supervised. This suggests that it is feasible to refine convolutional features descriptively to perform well on similarity tasks without compromising discriminative accuracy and potentially even improving it, when using an appropriate method of training."}, {"heading": "5 Conclusion", "text": "In this work we propose Multi-Query Networks a joint nonlinear architecture for jointly triplet-based learning of convolutional features and embeddings using multiple triplet queries. We demonstrated the utility of MQNs in on-task and off-task predictive settings. They outperform dedicated task-specific networks and basic triplet networks in prediction of unseen triplets and in and off-task classification settings. In classification settings, they also indicate to be able to perform knowledge transfer and improve on imagenet-trained features in low-label settings. Further, instead of being a black-box predictor, MQNs are qualitatively highly interpretable as evidenced by our exhibition of the semantic submanifolds they learn. Moreover, they provide a feature-exploration mechanism through the masks which facilitate visual and quantified analysis of the dependence structure of features in relation to queried concepts. MQNs are simple to implement, intuitively pleasing and easy to train end-to-end, while beating very strong baselines. We note that such models are especially useful in combination with crowdsourcing in order to learn complex embeddings of large\nvisual datasets with weak and cheap supervision. Our example tasks involving fashion images suggest usefulness in retrieval and recommendation systems.\nIn addition, our findings suggest that when training triplet networks it is harmful to use triplets arising from conflicting queries without an appropriate loss function. When the various queries are known, such as in crowdsourcing, we propose a model which resolves these problems with strong empirical performance gains. We provide a principled avenue towards using heterogeneous triplet-based information within one compact model. Further, our classification experiments suggest that future work exploring hybrid systems fusing weak and strong supervision from multiple sources on multiple tasks can yield advantages when learning neural networks at potentially lower label requirements.\nIn future work, it would be interesting to generalize our work and consider the setting of learning from unlabeled triplets by introducing a clustering mechanism for triplets and masks in order to discover similarity substructures in an unsupervised way.\nThe world is littered with perceptual ambiguities and conflicting similarity measures. We find, that selecting the right ones to pay attention to means learning from them."}, {"heading": "Acknowledgements", "text": "We would like to thank Michael Wilber and Sam Kwak for feedback on the manuscript. This work is partly funded by AOL-Program for Connected Experiences."}, {"heading": "6 Supplementary Material", "text": "The supplementary material contains a detailed description of the experimental setup."}, {"heading": "6.1 Experimental Setup", "text": "Network Architectures: We use different feature extractors for the two datasets. For the font dataset, we use a variant of the VGG architecture [24] with 9 layers of 3 by 3 convolutions and two fully connected layers. As feature extractor for the Zappos dataset we choose the deep residual network architecture [25] for its strong empirical performance. In particular, we fine-tune an 18 layer model that is pretrained2 on Imagenet [26] and reduced by one residual module to adjust for the smaller image size of 112x112. For all our experiments we set embedding dimension to 128. We use two masks for the fonts dataset and four masks for the Zappos dataset corresponding to the number of conceptspecific queries respectively.\n2 https://github.com/facebook/fb.resnet.torch\nSetup of Masking Module: Training consists of learning parameters \u03b8 of f(\u00b7) and \u03b2m of the masks. We explore fixing these components to clarify their contribution to overall performance. For fixed masks, we allocate 1/nq th of the embedding dimensions to one task. When learning masks, we initialize \u03b2m using a normal distribution with 0 mean and 0.005 variance. Following the sigmoid, this results in initial mask m values centered closely around 0.5, representing uncertainty about whether a latent dimension is important or not to a particular concept. When observing Equation 2 from the manuscript, it becomes evident that the model will allocate higher likelihood to triplets for masked values which are either close to 1 or 0 by selecting relevant features and deselecting irrelevant ones, as uncertain states provide less reward.\nTriplet Generation Procedure: Triplets are drawn from the annotations of the datasets. For the font dataset, we sample two characters of the same type and one from a different type to simulate triplets and analogously for font-style. For the Zappos dataset, we sample four types of triplets. Analogous to the triplets described above we sample triplets for the shoe types, suggested genders and closure mechanisms. For the heel heights we sample coarse-grained triplets and fine-grained triplets to represent the numerical ranges well. For coarse-grained triplets we randomly pick three shoes with different heel heights. For fine-grained triplets, we explicitly sample shoes that have closer heel heights. Overall, we sample 500k triplets for the type of shoes, 400k triplets for the heel heights and 200k triplets for the suggested gender as well as for the closure mechanism. We split each of these sets into three parts so that 70% are in the training set, 10% in the validation set and 20% in the test set.\nOptimization Parameters: We train the MQNs with a mini-batch size of 96 and optimize using ADAM [27]. As parameters we use \u03b1 = 1e-5, \u03b21 = 0.1 and \u03b22 = 0.001. For the embedding energy function in our objective we choose a \u03bb of 5e-4. In each mini-batch we sample triplets for each question in equal proportions and sample the triplets uniformly. We perform early stopping in our experiments once the validation set performance decreases over the duration of 400,000 triplets.\nExperiment System: We perform our experiments on Amazon EC2 g2.2xlarge instances with Torch73 and cuDNN 44.\n3 https://github.com/torch/torch7 4 https://developer.nvidia.com/cudnn"}], "references": [{"title": "Bayesian representation learning with oracle constraints", "author": ["T. Karaletsos", "S. Belongie", "G. R\u00e4tsch"], "venue": "International Conference on Learning Representations (ICLR), San Juan, PR", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2016}, {"title": "Stochastic triplet embedding", "author": ["L. Van Der Maaten", "K. Weinberger"], "venue": "Machine Learning for Signal Processing (MLSP), 2012 IEEE International Workshop on, IEEE", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2012}, {"title": "Adaptively learning the crowd kernel", "author": ["O. Tamuz", "C. Liu", "O. Shamir", "A. Kalai", "S.J. Belongie"], "venue": "Proceedings of the 28th International Conference on Machine Learning (ICML-11).", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2011}, {"title": "Multiview triplet embedding: Learning attributes in multiple maps", "author": ["E. Amid", "A. Ukkonen"], "venue": "Proceedings of the 32nd International Conference on Machine Learning (ICML-15).", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2015}, {"title": "Visualizing non-metric similarities in multiple maps", "author": ["L. Van der Maaten", "G. Hinton"], "venue": "Machine learning 87(1)", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2012}, {"title": "Learning concept embeddings with combined human-machine expertise", "author": ["M. Wilber", "I.S. Kwak", "D. Kriegman", "S. Belongie"], "venue": "International Conference on Computer Vision (ICCV \u201915).", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2015}, {"title": "Large scale online learning of image similarity through ranking", "author": ["G. Chechik", "V. Sharma", "U. Shalit", "S. Bengio"], "venue": "The Journal of Machine Learning Research 11", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2010}, {"title": "Learning a similarity metric discriminatively, with application to face verification", "author": ["S. Chopra", "R. Hadsell", "Y. LeCun"], "venue": "Computer Vision and Pattern Recognition (CVPR \u201905). Volume 1., IEEE", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2005}, {"title": "Dimensionality reduction by learning an invariant mapping", "author": ["R. Hadsell", "S. Chopra", "Y. LeCun"], "venue": "Computer Vision and Pattern Recognition (CVPR \u201906). Volume 2., IEEE", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2006}, {"title": "Learning fine-grained image similarity with deep ranking", "author": ["J. Wang", "Y. Song", "T. Leung", "C. Rosenberg", "J. Wang", "J. Philbin", "B. Chen", "Y. Wu"], "venue": "Computer Vision and Pattern Recognition (CVPR \u201914).", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2014}, {"title": "Facenet: A unified embedding for face recognition and clustering", "author": ["F. Schroff", "D. Kalenichenko", "J. Philbin"], "venue": "Computer Vision and Pattern Recognition (CVPR \u201915).", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2015}, {"title": "Learning to disentangle factors of variation with manifold interaction", "author": ["S. Reed", "K. Sohn", "Y. Zhang", "H. Lee"], "venue": "Proceedings of the 31st International Conference on Machine Learning (ICML-14).", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2014}, {"title": "On deep multi-view representation learning", "author": ["W. Wang", "R. Arora", "K. Livescu", "J. Bilmes"], "venue": "Proceedings of the 32nd International Conference on Machine Learning (ICML-15).", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2015}, {"title": "Multi-view convolutional neural networks for 3d shape recognition", "author": ["H. Su", "S. Maji", "E. Kalogerakis", "E. Learned-Miller"], "venue": "International Conference on Computer Vision (ICCV \u201915).", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2015}, {"title": "Multiple kernel learning, conic duality, and the smo algorithm", "author": ["F.R. Bach", "G.R. Lanckriet", "M.I. Jordan"], "venue": "Proceedings of the 21st International Conference on Machine Learning (ICML-04), ACM", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2004}, {"title": "Large scale multiple kernel learning", "author": ["S. Sonnenburg", "G. R\u00e4tsch", "C. Sch\u00e4fer", "B. Sch\u00f6lkopf"], "venue": "The Journal of Machine Learning Research 7", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2006}, {"title": "Learning multi-modal similarity", "author": ["B. McFee", "G. Lanckriet"], "venue": "The Journal of Machine Learning Research 12", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2011}, {"title": "A unified architecture for natural language processing: Deep neural networks with multitask learning", "author": ["R. Collobert", "J. Weston"], "venue": "Proceedings of the 25st International Conference on Machine Learning (ICML-08), ACM", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2008}, {"title": "Bilinear cnn models for fine-grained visual recognition", "author": ["T.Y. Lin", "A. RoyChowdhury", "S. Maji"], "venue": "International Conference on Computer Vision (ICCV \u201915).", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2015}, {"title": "Similarity metrics for categorization: from monolithic to category specific", "author": ["B. Babenko", "S. Branson", "S. Belongie"], "venue": "International Conference on Computer Vision (ICCV \u201909), IEEE", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2009}, {"title": "Draw: A recurrent neural network for image generation", "author": ["K. Gregor", "I. Danihelka", "A. Graves", "D. Wierstra"], "venue": "arXiv preprint arXiv:1502.04623", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2015}, {"title": "Fine-Grained Visual Comparisons with Local Learning", "author": ["A. Yu", "K. Grauman"], "venue": "Computer Vision and Pattern Recognition (CVPR \u201914).", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2014}, {"title": "Visualizing data using t-sne", "author": ["L. Van der Maaten", "G. Hinton"], "venue": "Journal of Machine Learning Research 9(2579-2605)", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2008}, {"title": "Very deep convolutional networks for large-scale image recognition", "author": ["K. Simonyan", "A. Zisserman"], "venue": "arXiv preprint arXiv:1409.1556", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2014}, {"title": "Deep residual learning for image recognition", "author": ["K. He", "X. Zhang", "S. Ren", "J. Sun"], "venue": "arXiv preprint arXiv:1512.03385", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2015}, {"title": "Imagenet: A largescale hierarchical image database", "author": ["J. Deng", "W. Dong", "R. Socher", "L.J. Li", "K. Li", "L. Fei-Fei"], "venue": "Computer Vision and Pattern Recognition (CVPR \u201909), IEEE", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2009}, {"title": "Adam: A method for stochastic optimization", "author": ["D. Kingma", "J. Ba"], "venue": "arXiv preprint arXiv:1412.6980", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2014}], "referenceMentions": [{"referenceID": 0, "context": "We leverage recent results in representation learning [1] to port the idea of factorized triplet embeddings to convolutional triplet networks and explore applications to computer vision tasks.", "startOffset": 54, "endOffset": 57}, {"referenceID": 1, "context": "Disconnected from the input image, triplet based similarity embeddings, can be learned using crowdkernels [2].", "startOffset": 106, "endOffset": 109}, {"referenceID": 2, "context": "[3] introduce a probabilistic treatment for triplets and learn an adaptive crowd kernel.", "startOffset": 0, "endOffset": 3}, {"referenceID": 3, "context": "Similar work has been generalized to multiple-views and clustering settings by Amid and Ukkonen [4] as well as Van der Maaten and Hinton [5].", "startOffset": 96, "endOffset": 99}, {"referenceID": 4, "context": "Similar work has been generalized to multiple-views and clustering settings by Amid and Ukkonen [4] as well as Van der Maaten and Hinton [5].", "startOffset": 137, "endOffset": 140}, {"referenceID": 5, "context": "[6], but this work did not include joint feature and embedding learning.", "startOffset": 0, "endOffset": 3}, {"referenceID": 6, "context": "An early approach to connect input features with embeddings has been to learn image similarity functions through ranking [7].", "startOffset": 121, "endOffset": 124}, {"referenceID": 7, "context": "A foundational line of work combining similarities with neural network models to learn visual features from similarities revolves around Siamese networks [8,9], which use pairwise distances to learn embeddings discriminatively.", "startOffset": 154, "endOffset": 159}, {"referenceID": 8, "context": "A foundational line of work combining similarities with neural network models to learn visual features from similarities revolves around Siamese networks [8,9], which use pairwise distances to learn embeddings discriminatively.", "startOffset": 154, "endOffset": 159}, {"referenceID": 9, "context": "[10] and Schroff et al.", "startOffset": 0, "endOffset": 4}, {"referenceID": 10, "context": "[11], where use of supervised features with crowd-inferred similarities boosts performance in face verification and fine-grained visual categorization tasks.", "startOffset": 0, "endOffset": 4}, {"referenceID": 11, "context": "[12].", "startOffset": 0, "endOffset": 4}, {"referenceID": 0, "context": "[1] on representation learning which introduces a joint generative model over inputs and triplets to learn a factorized latent space.", "startOffset": 0, "endOffset": 3}, {"referenceID": 12, "context": "Multi-view learning [13,14] has been used for 3d shape inference and shown to generically be a good way to learn factorized latent spaces.", "startOffset": 20, "endOffset": 27}, {"referenceID": 13, "context": "Multi-view learning [13,14] has been used for 3d shape inference and shown to generically be a good way to learn factorized latent spaces.", "startOffset": 20, "endOffset": 27}, {"referenceID": 14, "context": "Multiple kernel learning [15,16] employs information encoded in different kernels to provide predictions using the synthesized complex feature space and has also been used for similarity-based learning by McFee and Lanckriet [17].", "startOffset": 25, "endOffset": 32}, {"referenceID": 15, "context": "Multiple kernel learning [15,16] employs information encoded in different kernels to provide predictions using the synthesized complex feature space and has also been used for similarity-based learning by McFee and Lanckriet [17].", "startOffset": 25, "endOffset": 32}, {"referenceID": 16, "context": "Multiple kernel learning [15,16] employs information encoded in different kernels to provide predictions using the synthesized complex feature space and has also been used for similarity-based learning by McFee and Lanckriet [17].", "startOffset": 225, "endOffset": 229}, {"referenceID": 17, "context": "Multi-task learning approaches [18] are used when information from disparate sources or using differing assumptions can be combined beneficially for a final prediction task.", "startOffset": 31, "endOffset": 35}, {"referenceID": 18, "context": "We consider disentangled latent spaces, which is also considered in multilinear network architectures [19] focusing on factorizations of feature spaces in neural networks, but differ in many details.", "startOffset": 102, "endOffset": 106}, {"referenceID": 19, "context": "An interesting link also exists to multiple similarity learning [20], where category specific similarities are used to approximate a fine-grained global embedding.", "startOffset": 64, "endOffset": 68}, {"referenceID": 20, "context": "This term may be confused with spatial attention [21], but bears similarity insofar as it shows that the ability to gate the focus of the model on relevant dimensions (in our case in latent space rather than observed space) is beneficial both to the semantics and to the quantitative performance of our model.", "startOffset": 49, "endOffset": 53}, {"referenceID": 21, "context": "We also consider the Zappos50k shoe dataset [22] collected by Yu and Grauman.", "startOffset": 44, "endOffset": 48}, {"referenceID": 22, "context": "Figure 3 shows embeddings of the two subspaces in the Fonts dataset, which we project down to two dimensions using t-SNE [23].", "startOffset": 121, "endOffset": 125}], "year": 2016, "abstractText": "In typical perceptual tasks, higher-order concepts are inferred from visual features to assist with perceptual decision making. However, there is a multitude of visual concepts which can be inferred from a single stimulus. When learning nonlinear embeddings with siamese or triplet networks from similarities, we typically assume they are sourced from a single visual concept. In this paper, we are concerned with the hypothesis that it can be potentially harmful to ignore the heterogeneity of concepts affiliated with observed similarities when learning these embedding networks. We demonstrate empirically that this hypothesis holds and suggest an approach that deals with these shortcomings, by combining multiple notions of similarities in one compact system. We propose Multi-Query Networks (MQNs) that leverage recent advances in representation learning on factorized triplet embeddings in combination with Convolutional Networks in order to learn embeddings differentiated into semantically distinct subspaces, which are learned with a latent space attention mechanism. We show that the resulting model learns visually relevant semantic subspaces with features that do not only outperform single triplet networks, but even sets of concept specific networks.", "creator": "LaTeX with hyperref package"}}}