{"id": "1603.08037", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "25-Mar-2016", "title": "On the Detection of Mixture Distributions with applications to the Most Biased Coin Problem", "abstract": "This paper studies the trade-off between two different kinds of pure exploration: breadth versus depth. The most biased coin problem asks how many total coin flips are required to identify a \"heavy\" coin from an infinite bag containing both \"heavy\" coins with mean $\\theta_1 \\in (0,1)$, and \"light\" coins with mean $\\theta_0 \\in (0,\\theta_1)$, where heavy coins are drawn from the bag with probability $\\alpha \\in (0,1/2)$. The key difficulty of this problem lies in distinguishing whether the two kinds of coins have very similar means, or whether heavy coins are just extremely rare. A second challenge, which involves distinguishing the types of coins that are rare, is the first problem with calculating the length of the total coins. It turns out that this is impossible to obtain without a good, simple, well-defined answer. It might be easier to find a simple answer with a few more parameters, such as the number of coins to draw in the bag, which is what you need to calculate.\nThe first two questions ask how many coins have very large coins. The answer is that most of the coins (the most important) are very rare (and thus, very rare) coins that can be considered only as rare.\nHere we want to use a small set of parameters to compute the total coin flips. In our case, we assume that, for every coin, a unique coin will be generated every 10 years and that all coins must be generated every 10 years. The problem is not as simple as we originally assumed, but it becomes more complicated. It is rather difficult to find an accurate solution for this problem. This is the problem of computing \"heavy\" coin flips. For example, suppose $0.05 = 10,000 coins with a maximum coin size of 1,000, and that every 10 years of the world is 100,000 coins with the maximum coin size of 1,000, and that every 10 years of the world is 50,000 coins with a maximum coin size of 1,000, and that each 10 years of the world is 50,000 coins with a maximum coin size of 1,000, and that every 10 years of the world is 50,000 coins with a maximum coin size of 1,000, and that each 10 years of the world is 50,000 coins with a maximum coin size of 1,000, and that each 10 years of the world is 50,000 coins with", "histories": [["v1", "Fri, 25 Mar 2016 21:22:59 GMT  (40kb)", "http://arxiv.org/abs/1603.08037v1", null]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["kevin jamieson", "daniel haas", "ben recht"], "accepted": false, "id": "1603.08037"}, "pdf": {"name": "1603.08037.pdf", "metadata": {"source": "CRF", "title": "On the Detection of Mixture Distributions with applications to the Most Biased Coin Problem", "authors": ["Kevin Jamieson"], "emails": ["kjamieson@eecs.berkeley.edu", "dhaas@eecs.berkeley.edu", "brecht@eecs.berkeley.edu"], "sections": [{"heading": null, "text": "ar X\niv :1\n60 3.\n08 03\n7v 1\n[ cs\n.L G\n] 2\n5 M"}, {"heading": "1 Introduction", "text": "The trade-off between exploration and exploitation has been an ever-present trope in the online learning literature. In contrast, this paper studies the trade-off between two different kinds of pure exploration: breadth versus depth. Consider a magic bag that contains an infinite number of two kinds of biased coins: \u201cheavy\u201d coins with mean \u03b81 \u2208 (0, 1) and \u201clight\u201d coins with mean \u03b80 \u2208 (0, \u03b81). When a player picks a coin from the bag, with probability \u03b1 the coin is \u201cheavy\u201d and with probability (1 \u2212 \u03b1) the coin is \u201clight.\u201d The player can flip any coin she picks from the bag as many times as she wants, and the goal is to identify a heavy coin. The key difficulty of this problem lies in distinguishing whether the two kinds of coins have very similar means, or whether heavy coins are just extremely rare. That is, how does one balance flipping an individual coin many times to better estimate its mean against considering many new coins to maximize the probability of observing a heavy one. It turns out that this toy problem is a useful abstraction to characterize the inherent difficulty of real-world problems including automated hiring of crowd workers for data processing tasks, anomaly and intrusion detection, and discovery of vacant frequencies in the radio spectrum.\nThe most biased coin problem first came to the attention of the authors of this work when it was presented at COLT 2014 (Chandrasekaran and Karp, 2014). In that work, it was shown that if \u03b1, \u03b81, and \u03b80 were known\nthen there exists an algorithm based on the sequential probability ratio test (SPRT) that is optimal in that it minimizes the expected number of total flips to find a \u201cheavy\u201d coin whose posterior probability of being heavy is at least 1\u2212 \u03b4, and the expected sample complexity of this algorithm was upper-bounded by\n16 (\u03b81 \u2212 \u03b80)2 ( 1\u2212 \u03b1 \u03b1 + log ( (1\u2212 \u03b1)(1 \u2212 \u03b4) \u03b1\u03b4 )) . (1)\nHowever, the practicality of the proposed algorithm is severely limited as it relies critically on knowing \u03b1, \u03b81, and \u03b80 exactly. In addition, the algorithm requires more than one coin to be outside the bag at a time ruling out some applications.\nMalloy et al. (2012) addressed some of the shortcomings of Chandrasekaran and Karp (2012) (a preprint of Chandrasekaran and Karp (2014)) by considering both an alternative SPRT procedure and a sequential thresholding procedure. Both of these proposed algorithms consider one coin at a time and never return to previous coins. However, the former requires knowledge of all relevant parameters \u03b1, \u03b80, \u03b81, and the latter requires knowledge of \u03b1, \u03b80. Moreover, these results are only presented for the asymptotic case where \u03b4 \u2192 0.\nIn this work we propose algorithms that are adaptive to partial or even no knowledge of \u03b1, \u03b80, \u03b81, are guaranteed to return a heavy coin with probability at least 1\u2212 \u03b4, and support the setting where just one coin is allowed outside the bag at any given time. In addition, we present lower bounds that nearly match the upper bounds shown for our algorithms.\nWhile coins are a useful analogy, all of our lower and upper bounds extend beyond Bernoulli coins to other distributions (e.g. distributions supported on the interval [0, 1]), though we return to the coin analogy throughout for concreteness. Indeed, in pursuit of bounds for the coin problem, we derive upper and lower bounds for a related problem, the detection of mixture distributions with applications to anomaly detection. As a concrete example of that kind of lower bound shown in this work, suppose we observe a sequence of random variables X1, . . . ,Xn and consider the following hypothesis test:\nProblem 1.\nH0 : \u2200i X1, . . . ,Xn \u223c N (\u03b8, \u03c32) for some \u03b8 \u2208 R, H1 : \u2200i X1, . . . ,Xn \u223c (1\u2212 \u03b1)N (\u03b80, \u03c32) + \u03b1 N (\u03b81, \u03c32)\nWe can show that if \u03b80, \u03b81, \u03b1 are known and \u03b8 = \u03b80, then it suffices to observe just max{1/\u03b1, \u03c32\n\u03b12(\u03b81\u2212\u03b80)2 log(1/\u03b4)} samples to determine the correct hypothesis with probability at least 1\u2212 \u03b4. However, if \u03b80, \u03b81, \u03b1 are unknown (and hence we cannot assume a value for \u03b8), we show that whenever (\u03b81\u2212\u03b80)2\n\u03c32 \u2264 1, at least max { 1/\u03b1, ( \u03c32 \u03b1(\u03b81\u2212\u03b80)2 )2 log(1/\u03b4) } samples in expectation are necessary to determine the correct hypothesis with probability at least 1 \u2212 \u03b4 (see Appendix C). The unknown parameter case has a simple interpretation for anomaly detection with a fixed mixing component \u03b1 that gets at the key insights of this work: if the anomalous distribution is well separated from the null distribution, then detecting an anomalous component is only about as hard as observing just one anomalous sample (i.e. 1/\u03b1\u2014no harder than if the parameters were known) since detection is nearly certain between well-separated distributions. However, when the two distributions are not well separated then the sample complexity to detect an anomaly scales like the inverse of the KL divergence squared!\nIn this work, we formally prove the above observations as special cases of more general statements about detecting mixtures. Our main contributions are the following:\n\u2022 We characterize the difficulty of distinguishing between a single-parameter distribution and a mixture of two such distributions. When the parameters are known, detecting the presence of a mixture requires a sample complexity that scales as the expected number of samples to differentiate between\nthe two distributions if given samples from each (i.e. the inverse KL divergence). However, when the distribution parameters are unknown, we prove lower bounds showing that detecting a mixture is quadratically harder if the distributions are not well-separated. We then show that this bound applies to any algorithm that solves the most biased coin problem by flipping each coin a fixed number of times (as in Malloy et al., 2012).\n\u2022 We propose and analyze the sample complexity of several algorithms for the most biased coin problem that are adaptive to partial or no knowledge of the distribution parameters, all of which come within log factors of the information-theoretic lower bound (see Table 1). These algorithms actually detect any heavy distribution supported on [0, 1], not just Bernoulli coins, and solve a particular instance of the infinite armed bandit problem. We believe both that our algorithms are the first fully adaptive solution to the most biased coin problem, and that the same approach can be reworked to solve more general instances of the infinite-armed bandit problem in the important case when the arm mean distributions are not fully known."}, {"heading": "1.1 Motivation and Related Work", "text": "Data labeling for machine learning applications is often performed by humans, and recent work in the crowdsourcing literature accelerates labeling by organizing workers into pools of labelers and paying them to wait for incoming data (Bernstein et al., 2011; Haas et al., 2015). Because workers hired on marketplaces such as Amazon\u2019s Mechanical Turk (MTurk) vary widely in skill, identifying high-quality workers is an important challenge. If we model each worker\u2019s performance (e.g. accuracy or speed) on a set of tasks as drawn from some distribution on [0, 1], then selecting a good worker is equivalent to identifying a worker with a high mean by taking as few total samples as possible from all workers. Note that we do not observe a worker\u2019s inherent skill or mean directly, we must give them tasks from which we estimate it (like repeatedly flipping a biased coin). That is, the identification of good workers is well-modeled by the most biased coin problem.\nOne can interpret the most biased coin problem as an infinite armed bandit problem where each coin is an arm. In that setting, Berry et al. (1997), Wang et al. (2009) and Bonald and Proutiere (2013) prove and refine bounds on the expected cumulative regret of the player, whereas Carpentier and Valko (2015) focus on the pure exploration setting. All of this work relies on the assumption that the distribution of the means is parametric and known (though Carpentier and Valko (2015) describes a method to estimate the relevant parameters first). Our setting relies on a different parameterization of the means (i.e. (1\u2212\u03b1)\u03b4\u03b80+\u03b1\u03b4\u03b81 where \u03b4x is a Dirac delta located at x), and we focus on settings in which the relevant parameters are unknown.\nOur lower bounds are based on the detection of the presence of a mixture of two parametric distributions versus just a single distribution of the same family. There has been extensive work in the estimation of mixture distributions (Hardt and Price, 2014; Freund and Mansour, 1999). This literature usually assumes that the mixture coefficient \u03b1 is bounded away from 0 and 1 to ensure that a sufficient amount of samples are observed from each distribution in the mixture. In contrast, we highlight the challenging regime when \u03b1 is arbitrarily small, as is the case in statistical anomaly detection (Eskin, 2000; Thatte et al., 2011; Agarwal, 2006). The current work differs primarily in that we are in an online setting where we choose to keep sampling or stop, and for the coin problem we must decide how many times to flip each coin, not just a stopping time."}, {"heading": "1.2 Preliminaries", "text": "Let P and Q be two probability distributions with a common measurable space. For simplicity, assume P and Q have the same support.\nDefinition 1. Define the KL Divergence between P and Q as KL(P,Q) = \u222b log ( dP dQ ) dP . Definition 2. Define the \u03c72 Divergence between P and Q as \u03c72(P,Q) = \u222b (\ndP dQ \u2212 1\n)2 dQ = \u222b (dP (x)\u2212dQ(x))2 dQ(x) dx.\nNote that by Jensen\u2019s inequality\nKL(P,Q) = EP\n[ log ( dP\ndQ\n)] \u2264 log ( EP [ dP\ndQ\n]) = log ( \u03c72(P,Q) + 1 ) \u2264 \u03c72(P,Q). (2)\nExample 1 (Gaussian). Let P = N (\u03b81, \u03c32) and Q = N (\u03b80, \u03c32). Then\nKL(P,Q) = (\u03b81\u2212\u03b80) 2\n2\u03c32 and \u03c7 2(P,Q) = e\n(\u03b81\u2212\u03b80) 2\n\u03c32 \u2212 1.\nExample 2 (Bernoulli). Let P = Bernoulli(\u03b81) and Q = Bernoulli(\u03b80). Then\nKL(P,Q) = \u03b81 log( \u03b81 \u03b80 ) + (1\u2212 \u03b81) log(1\u2212\u03b811\u2212\u03b80 ), and \u03c7 2(P,Q) = (\u03b81\u2212\u03b80) 2 \u03b80(1\u2212\u03b80) .\n\u2264 (\u03b81\u2212\u03b80)2/2\u03b80(1\u2212\u03b80)\u2212[(\u03b81\u2212\u03b80)(2\u03b80\u22121)]+"}, {"heading": "1.3 The Most Biased Coin Problem Statement", "text": "Let \u03b8 \u2208 \u0398 index a family of single-parameter probability density functions g\u03b8 and fix \u03b80, \u03b81 \u2208 \u0398, \u03b1 \u2208 [0, 1/2]. For any \u03b8 \u2208 \u0398 assume that g\u03b8 is known to the procedure. Consider a sequence of iid Bernoulli random variables \u03bei \u2208 {0, 1} for i = 1, 2, . . . where each P(\u03bei = 1) = 1 \u2212 P(\u03bei = 0) = \u03b1. Let Xi,j for j = 1, 2, . . . be a sequence of random variables drawn from g\u03b81 if \u03bei = 1 and g\u03b80 otherwise, and let {{Xi,j}Mij=1}Ni=1 represent the sampling history generated by a procedure for some N \u2208 N and (M1, . . . ,MN ) \u2208 NN . For any procedure, let N(\u03b1, \u03b80, \u03b81) be the random variable denoting the number of distributions each sampled Mi(\u03b1, \u03b80, \u03b81) times for all i when the procedure is applied to the problem defined by fixed (\u03b1, \u03b80, \u03b81).\nDefinition 3. We say a procedure is \u03b4-probably correct if for all (\u03b1, \u03b80, \u03b81) it identifies a \u201cheavy\u201d distribution with probability at least 1\u2212 \u03b4.\nFor all procedures that are \u03b4-probably correct and follow Algorithm 1, our goal is to provide lower and upper bounds on the quantity E[T (\u03b1, \u03b80, \u03b81)] = E[ \u2211N(\u03b1,\u03b80,\u03b81) i=1 Mi(\u03b1, \u03b80, \u03b81)] for any (\u03b1, \u03b80, \u03b81). Note that if g\u03b8 = Bernoulli(\u03b8), then E[T (\u03b1, \u03b80, \u03b81)] is equivalent to the expected number of total coin flips needed to find a most biased coin. To emphasize this, our results are stated generally, then tied to the special case of Bernoulli coins by way of corollaries. All proofs appear in the appendix."}, {"heading": "2 Lower bounds", "text": "In this section, we derive lower bounds on the sample complexity of valid procedures. Section 2.1 provides a lower bound for any adaptive procedure that may choose how many times to sample from each distribution independently, and Section 2.2 derives bounds for fixed sample size procedures that select an m \u2265 1 and sample from each distribution exactly m times. The results in Section 2.2.1 apply to procedures with full knowledge of \u03b1, \u03b80, \u03b81, and Section 2.2.2 demonstrates that without knowledge of these parameters, the sample complexity becomes much higher.\nInitialize an empty history (N = 0,M = {}). Repeat until heavy distribution declared:\nChoose one of 1. obtain an additional sample from distribution i = N so that Mi \u2190 Mi + 1 2. draw a sample from the (N + 1)st distribution so that N \u2190 N + 1, MN = 1 3. declare distribution i = N as heavy\nAlgorithm 1: Sequential procedure for identifying a heavy distribution. Only the last distribution drawn may be sampled or declared heavy, enforcing the rule that only one coin may be outside the bag at a time."}, {"heading": "2.1 Fully adaptive strategies", "text": "The following theorem, reproduced from Malloy et al. (2012), describes the sample complexity of any \u03b4probably correct algorithm for the most biased coin identification problem. Note that this lower bound holds for any procedure, regardless of how adaptive it is or if it returns to previously seen distributions to draw additional samples.\nTheorem 1. (Malloy et al., 2012, Theorem 2) Fix \u03b4 \u2208 (0, 1). Let T be the total number of samples taken of any procedure that is \u03b4-probably correct in identifying a heavy distribution. Then\nE[T ] \u2265 c1 max { 1\u2212 \u03b4 \u03b1 , (1\u2212 \u03b4)\n\u03b1KL(g\u03b80 |g\u03b81)\n}\nwhenever \u03b1 \u2264 c2\u03b4 where c1, c2 \u2208 (0, 1) are absolute constants. The above theorem is directly applicable to the special case where g\u03b8 is a Bernoulli distribution, implying\na lower bound of max {\n1\u2212\u03b4 \u03b1 , 2min{\u03b80(1\u2212\u03b80),\u03b81(1\u2212\u03b81)} \u03b1(\u03b81\u2212\u03b80)2\n} on the most biased coin problem. Our upper bounds\nfor adaptive procedures presented later should be compared to this result."}, {"heading": "2.2 The fixed sample size strategy and the detection of mixtures", "text": "The lower bounds of this section are based on two simple observations. The first observation is that identifying that a specific distribution i \u2264 N is heavy (i.e. \u03bei = 1) is at least as hard as detecting that any of the distributions up to time N is heavy. Thus, a lower bound on E[T (\u03b1, \u03b80, \u03b81)] for this strictly easier detection problem is also a lower bound for the identification problem. Thus, we\u2019ve reduced the problem to a sequential hypothesis test of whether all the observed samples all came from a single distribution or from a mixture of two distributions:\nProblem 2.\nH0 : \u2200i, j Xi,j \u223c g\u03b8 for some \u03b8 \u2208 \u0398\u0303 \u2286 \u0398, H1 : \u2200i \u03bei \u223c Bernoulli(\u03b1), \u2200i, j Xi,j \u223c { g\u03b80 if \u03bei = 0\ng\u03b81 if \u03bei = 1\nIf \u03b80 and \u03b81 are close to each other, or if \u03b1 is very small, or both, it can be very difficult to decide between H0 and H1 even if \u03b1, \u03b80, \u03b81 are known a priori. Note that if \u0398\u0303 = {\u03b80} and the parameters are known, any lower bound on the problem also bounds the most biased coin problem with known \u03b1, \u03b80, \u03b81. In what follows, for any event A, let Pi(A) and Ei[A] denote probability and expectation of A under hypothesis Hi for i \u2208 {0, 1} (the specific value of \u03b8 in H0 will be clear from context).\nThe second observation is characterized in the following claim:\nClaim 1. Any procedure that is \u03b4-probably correct also satisfies P(N(0, \u03b80, \u03b81) < \u221e) \u2264 \u03b4 for all \u03b80 < \u03b81. Claim 1 allows us to restrict our analysis of Problem 2 to procedures that in addition to deciding the hypothesis test, satisfy P0(N < \u221e) \u2264 \u03b4. This property is instrumental in our ability to prove tight bounds on the sample complexity of the procedures.\nThe fixed sample size strategy fixes an m \u2208 N prior to starting the game and samples each distribution exactly m times, i.e. Mi = m for all i \u2264 N . To simplify notation let f\u03b8 = g\u03b8 \u2297 \u00b7 \u00b7 \u00b7 \u2297 g\u03b8 be the m-wise product distribution for any \u03b8 \u2208 \u0398. Now our problem is more succinctly described as: Problem 3.\nH0 : \u2200i Xi \u223c f\u03b8 for some \u03b8 \u2208 \u0398\u0303 \u2286 \u0398, H1 : \u2200i \u03bei \u223c Bernoulli(\u03b1), \u2200i Xi \u223c { f\u03b80 if \u03bei = 0\nf\u03b81 if \u03bei = 1\nIn the special case where g\u03b8 is a Bernoulli distribution, f\u03b8 can be represented by a Binomial distribution with parameters (m, \u03b8)."}, {"heading": "2.2.1 Sample complexity when parameters are known", "text": "Theorem 2 characterizes the sample complexity of Problem 3 for any valid procedure. Note that when \u0398\u0303 = {\u03b80} and \u03b80, \u03b81, and \u03b1 are known, then lower bounding the problem also bounds any fixed sample size procedure that solves the most biased coin problem.\nTheorem 2. Fix \u03b4 \u2208 (0, 1). Consider the hypothesis test of Problem 3 for any fixed \u03b8 \u2208 \u0398\u0303 \u2286 \u0398. Let N be the random number of distributions considered before stopping and declaring a hypothesis. If a procedure satisfies P0(N < \u221e) \u2264 \u03b4 and P1(\u222aNi=1{\u03bei = 1}) \u2265 1\u2212 \u03b4, then\nE1[N ] \u2265 max { 1\u2212 \u03b4 \u03b1 , log(1\u03b4 ) KL(P1|P0) } \u2265 max { 1\u2212 \u03b4 \u03b1 , log(1\u03b4 ) \u03c72(P1|P0) } ."}, {"heading": "In addition, if \u0398\u0303 = {\u03b80} then", "text": "E1[N ] \u2265 max { 1\u2212 \u03b4 \u03b1 , log(1\u03b4 )\n\u03b12\u03c72(f\u03b81 |f\u03b80)\n} .\nThe next corollary relates Theorem 2 to the special case where distributions are Bernoulli coins and the objective is to find a heavy coin. The second result of the corollary is similar to that of Malloy et al. (2012, Theorem 4) that considers the limit as \u03b1 \u2192 0 and assumes m is sufficiently large (specifically, large enough for the Chernoff-Stein lemma to apply). In contrast, our result holds for all finite \u03b4, \u03b1,m.\nCorollary 1. Fix \u03b4 \u2208 (0, 1), m \u2208 N and consider the class of algorithms that flips each coin exactly m times and outputs a coin i \u2264 N as its estimate for a heavy coin. If an algorithm in this class is \u03b4-probably correct then\nE[Nm] \u2265 max    1\u2212 \u03b4 \u03b1 , log(1\u03b4 )\n\u03b12(e m\n(\u03b81\u2212\u03b80) 2 \u03b80(1\u2212\u03b80) \u2212 1)\n   \u2265 \u03b80(1\u2212 \u03b80) log(1\u03b4 ) m\u03b12(\u03b81 \u2212 \u03b80)2 1 m\u2264 \u03b80(1\u2212\u03b80)\n2(\u03b81\u2212\u03b80) 2\n,\nhowever, if we pick the best-case m:\nmin m\u2208N\nE[mNm] \u2265 (1\u2212 \u03b4) log\n( log(1/\u03b4)\n\u03b1\n)\n\u03b1\n\u03b80(1\u2212 \u03b80) (\u03b81 \u2212 \u03b80)2 .\nRemark 1. For all sufficiently small (\u03b81\u2212\u03b80) 2\n\u03b80(1\u2212\u03b80) , the expected number of flips of the fixed strategy to identify a\nheavy coin scales like \u2126( \u03b80(1\u2212\u03b80)\u03b12(\u03b81\u2212\u03b80)2 log(1/\u03b4)), a factor 1/\u03b1 more than (1) and the best adaptive algorithms we propose in Section 3 that can identify a heavy coin with just O( log(1/\u03b4) \u03b1(\u03b81\u2212\u03b80)2 ) total flips in expectation. Indeed, even the lower bound for the best case m is a factor of log(1/\u03b1) from the best upper bounds."}, {"heading": "2.2.2 Sample complexity when parameters are unknown", "text": "If \u03b1, \u03b80, and \u03b81 are unknown, we cannot test f\u03b80 against the mixture (1 \u2212 \u03b1)f\u03b80 + \u03b1f\u03b81 . Instead, we have the general composite test of any individual distribution against any mixture, which is at least as hard as the hypothesis test of Problem 3 with \u0398\u0303 = {\u03b8} for some particular worst-case setting of \u03b8. Without any specific form of f\u03b8, it is difficult to pick a worst case \u03b8 that will produce a tight bound. Consequently, in this section we appeal to single parameter exponential families (defined formally below) to provide us with a class of distributions in which we can reason about different possible values for \u03b8. Since exponential families include Bernoulli, Gaussian, exponential, and many other distributions, the following theorem is general enough to be useful in a wide variety of settings.\nTheorem 3. Suppose f\u03b8 for \u03b8 \u2208 \u0398 \u2282 R is a single parameter exponential family so that f\u03b8(x) = h(x) exp(\u03b7(\u03b8)x \u2212 b(\u03b7(\u03b8))) for some scalar functions h, b, \u03b7 where \u03b7 is strictly increasing. If E\u03b8[X] =\u222b xf\u03b8(x)dx then let Mk(\u03b8) = \u222b (x \u2212 E\u03b8[X])kf\u03b8(x)dx denote the kth centered moment under distribution f\u03b8. Define\n\u03b8\u2217 = \u03b7 \u22121((1\u2212 \u03b1)\u03b7(\u03b80) + \u03b1\u03b7(\u03b81) )\n\u03b8\u2212 = \u03b7 \u22121(\u03b7(\u03b80)\u2212 \u03b1(\u03b7(\u03b81)\u2212 \u03b7(\u03b80)) ) \u03b8+ = \u03b7 \u22121(\u03b7(\u03b81) + (1\u2212 \u03b1)(\u03b7(\u03b81)\u2212 \u03b7(\u03b80)) )\nand assume there exist finite \u03ba, \u03b3 such that\nsup y\u2208[\u03b80,\u03b81]\nb(2\u03b7(y) \u2212 \u03b7(\u03b8\u2217))\u2212 [2b(\u03b7(y)) \u2212 b(\u03b7(\u03b8\u2217))] \u2264 \u03ba,\nsup x\u2208[b\u0307(\u03b7(\u03b8\u2212)),b\u0307(\u03b7(\u03b8+))]\n\u03c6x(b\u0307 \u22121(x)) \u2264 \u03b3,\nwhere \u03c6x(\u03b7(\u03b8)) = f\u03b8(x). Then\n\u03c72((1 \u2212 \u03b1)f\u03b80(x) + \u03b1f\u03b81(x)|f\u03b8\u2217(x)) \u2264 c ( 1\n2 \u03b1(1\u2212 \u03b1)(\u03b7(\u03b81)\u2212 \u03b7(\u03b80))2\n)2\nwhere\nc = e\u03ba (\nsup \u03b8\u2208[\u03b80,\u03b81]\nM2(\u03b8) 2 ( 2 + \u03b3 ( b\u0307(\u03b7(\u03b8+))\u2212 b\u0307(\u03b7(\u03b8\u2212)) ))\n+ 8M4(\u03b8\u2212) + 8M4(\u03b8+) + 16 ( b\u0307(\u03b7(\u03b8+))\u2212 b\u0307(\u03b7(\u03b8\u2212)) )4 + 25\u03b3 ( b\u0307(\u03b7(\u03b8+))\u2212 b\u0307(\u03b7(\u03b8\u2212)) )5 ) .\nThus, if \u0398\u0303 = {\u03b8\u2217} and N is the stopping time of any procedure that satisfies P0(N < \u221e) \u2264 \u03b4 and P1(\u222aNi=1{\u03bei = 1}) \u2265 1\u2212 \u03b4, then\nE1[N ] \u2265 max { 1\u2212 \u03b4 \u03b1 , log(1\u03b4 )\nc ( 1 2\u03b1(1\u2212 \u03b1)(\u03b7(\u03b81)\u2212 \u03b7(\u03b80))2 )2\n} .\nTheorem 3 is difficult to interpret, so the following remark and corollary consider the special cases of Gaussian mixture model detection and the most biased coin problem, respectively.\nRemark 2. Recall that when \u03b1, \u03b80, \u03b81 are unknown, any procedure does not know how to choose \u0398\u0303 in Problem 3 and consequently it cannot rule out \u03b8 = \u03b8\u2217 for H0 where \u03b8\u2217 is defined in Theorem 3. If f\u03b8 = N (\u03b8, \u03c32) for known \u03c3, then whenever (\u03b81\u2212\u03b80)2\u03c32 \u2264 1 the constant c in Theorem 3 is an absolute constant and consequently, E1[N ] = \u2126 (( \u03c32 \u03b1(\u03b81\u2212\u03b80)2 )2 log(1/\u03b4) ) . Conversely, when \u03b1, \u03b80, \u03b81 are known, then we simply need to determine whether samples came from N (\u03b80, \u03c32) or (1\u2212\u03b1)N (\u03b80, \u03c32)+\u03b1N (\u03b81, \u03c32), and we show that it is sufficient to take just O ( \u03c32\n\u03b12(\u03b81\u2212\u03b80)2 log(1/\u03b4) ) samples (see Appendix C).\nCorollary 2. Fix \u03b4 \u2208 [0, 1],m \u2208 N and consider the class of algorithms that flips each coin exactly m times. Assume \u03b80, \u03b81 are bounded sufficiently far from {0, 1} such that 2(\u03b81 \u2212 \u03b80) \u2264 min{\u03b80(1\u2212 \u03b80), \u03b81(1\u2212 \u03b81)}. If an algorithm in this class is \u03b4-probably correct then\nE[N ] \u2265 c \u2032 min{ 1m , \u03b8\u2217(1\u2212 \u03b8\u2217)}\nm ( \u03b1(1 \u2212 \u03b1) (\u03b81\u2212\u03b80)2\u03b8\u2217(1\u2212\u03b8\u2217) )2 log( 1 \u03b4 ) whenever m \u2264\n\u03b8\u2217(1\u2212 \u03b8\u2217) (\u03b81 \u2212 \u03b80)2 .\nwhere c\u2032 is an absolute constant and \u03b8\u2217 = \u03b7\u22121 ((1\u2212 \u03b1)\u03b7(\u03b80) + \u03b1\u03b7(\u03b81)) \u2208 [\u03b80, \u03b81].\nRemark 3. We recall that if \u03b1, \u03b80, \u03b81 are unknown, then any fixed sample strategy would not know how to pick m sufficiently large a priori. Thus, the above corollary states that for any fixed m, whenever (\u03b81\u2212\u03b80) 2\n\u03b8\u2217(1\u2212\u03b8\u2217) is sufficiently small the number of samples necessary for this simple and intuitive strategy to identify the most biased coin scales like (\n\u03b8\u2217(1\u2212\u03b8\u2217) \u03b1(\u03b81\u2212\u03b80)2\n)2 log(1/\u03b4). However, in the next section we show that when \u03b1, \u03b80, \u03b81 are\nknown and m can be chosen by the algorithm, this same fixed sample strategy can identify the most biased coin using just log(1/(\u03b1\u03b4))\n\u03b1(\u03b81\u2212\u03b80)2 total flips in expectation, nearly matching the lower bound of Corollary 1. This is a striking example of the difference when parameters are known versus when they are not."}, {"heading": "3 Upper bounds and algorithms", "text": "Above we presented lower bounds on the difficulty of identifying a heavy distribution. In this section we prove the existence of algorithms that nearly match the lower bounds, even with only partial side knowledge. Table 1 summarizes the algorithms and their bounds. Our main result in Section 3.3 is Theorem 8 which describes the performance of an algorithm that has no prior knowledge of the parameters \u03b1, \u03b80, \u03b81 yet yields an upper bound that matches the lower bound of Theorem 1 up to logarithmic factors. In what follows, we assume that samples from heavy or light distributions are supported on [0, 1], and that drawn samples are independent and unbiased estimators of the mean, i.e., E[Xi,j] = \u00b5i for \u00b5i \u2208 {\u03b80, \u03b81}. All results can be easily extended to sub-Gaussian distributions. We begin with a fixed sample strategy and then turn our attention to adaptive sampling procedures."}, {"heading": "3.1 Fixed sample strategy for known \u03b1, \u03b80, \u03b81", "text": "A lower bound on \u03b1 tells us how many distributions we must consider and knowledge of the difference (\u03b81 \u2212 \u03b80) tells us how many times we should sample each distribution. The below theorem comes within a log(1/\u03b4) factor of the lower bound proved in Corollary 1 in general and is tight when \u03b1 \u2264 \u03b4. Theorem 4 (Fixed sample size, known \u03b1 and \u03b80, \u03b81). Fix \u03b4 \u2208 (0, 1/4) and set n\u0302 = \u2308 1 \u03b1 log( 2 \u03b4 ) \u2309\nand m =\u2308 2 log(4n\u0302/\u03b4) (\u03b81\u2212\u03b80)2 \u2309 . There exists a fixed sample size strategy with stopping time Nm \u2264 n\u0302 that is \u03b4-probably correct and satisfies\nE[mNm] \u2264 3 log(1/\u03b1) + log(12 log(6/\u03b4)/\u03b4) \u03b1(\u03b81 \u2212 \u03b80)2 \u2264 12 log(\n2 \u03b4\u03b1 )\n\u03b1(\u03b81 \u2212 \u03b80)2 ."}, {"heading": "3.2 Fully adaptive strategies when \u03b1 and/or \u03b80, \u03b81 are known", "text": "While the previous section considered a strategy that takes a constant number of samples from each distribution, this section allows the procedure to determine the number of times to sample a particular distribution adaptively based on the samples from that distribution. This section also shows that there exist simple procedures that adapt to the case when only a subset of \u03b1, \u03b80, \u03b81 are known using just a small number of samples more than if they had been known.\nConsider Algorithm 2, an SPRT-like procedure for finding a heavy distribution given \u03b4 and lower bounds on \u03b1 and \u01eb.\nTheorem 5. If Algorithm 2 is run with \u03b4 \u2208 (0, 1/4), \u03b10 \u2208 (0, 1/2), \u01eb0 \u2208 (0, 1), then the expected number of total samples taken by the algorithm is no more than\nc\u2032\u03b1 log(1/\u03b10) + c\u2032\u2032 log ( 1 \u03b4 )\n\u03b10\u01eb20\nfor some absolute constants c\u2032,c\u2032\u2032, and all of the following hold: 1) with probability at least 1 \u2212 \u03b4, a light distribution is not returned, 2) if \u01eb0 \u2264 \u03b81 \u2212 \u03b80 and \u03b10 \u2264 \u03b1, then with probability 45 a heavy distribution is returned, and 3) the procedure takes no more than c log(1/(\u03b10\u03b4))\n\u03b10\u01eb20 total samples.\nClearly, Theorem 5 applies when \u03b1, \u03b80, \u03b81 are known. The third claim of the theorem follows from a trivial bound of nm for the values of n and m stated in the algorithm (i.e. it holds with probability 1). The second claim holds only with constant probability (versus with probability 1\u2212 \u03b4) since the probability of observing a heavy distribution among the n = \u23082 log(4)/\u03b10\u2309 distributions considered only occurs with constant probability. One can boost this probability to 1\u2212\u03b4 by repeated application of the algorithm log(1/\u03b4)\nGiven \u03b4 \u2208 (0, 1/4), \u03b10 \u2208 (0, 1/2), \u01eb0 \u2208 (0, 1). Initialize n = \u23082 log(9)/\u03b10\u2309,m = \u230864\u01eb\u221220 log(14n/\u03b4)\u2309, A = \u22128\u01eb\u221210 log(21),\nB = 8\u01eb\u221210 log(14n/\u03b4), k1 = 5, k2 = \u23088\u01eb\u221220 log(2k1/min{\u03b4/8,m\u22121\u01eb\u221220 })\u2309. Draw k1 distributions and sample them each k2 times. Estimate \u03b8\u03020 = mini=1,...,k1 \u00b5\u0302i,k2 , \u03b3\u0302 = \u03b8\u03020 + \u01eb0/2. Repeat for i = 1, . . . , n:\nDraw distribution i. Repeat for j = 1, . . . ,m:\nSample distribution i and observe Xi,j . If \u2211j\nk=1(Xi,k \u2212 \u03b3\u0302) > B: Declare distribution i to be heavy and Output distribution i.\nElse if \u2211j\nk=1(Xi,k \u2212 \u03b3\u0302) < A: break.\nOutput null.\nAlgorithm 2: Adaptive strategy for heavy distribution identification with inputs \u03b10, \u01eb0, \u03b4\ntimes or alternatively, one can run the algorithm with n = \u0398( log(1/\u03b4)\u03b1 ) (with a straightforward modification of the proof). Moreover, with a slightly more sophisticated argument, one can show that if the algorithm is run with \u03b8\u03020 = \u03b80 (and the estimation step is skipped) and n = \u221e then the algorithm is nearly equivalent to the SPRT of Malloy et al. (2012) which succeeds with probability at least 1 \u2212 \u03b4 and achieves an expected sample complexity equivalent to (1).\nGiven \u03b4 \u2208 (0, 1), \u03b1 \u2208 (0, 1/2). Initialize k = 1 While Algorithm 2 run with inputs \u03b4/(2k2), \u03b10 = \u03b1, \u01eb0 = 2\n\u2212k returns null: Set k = k + 1.\nOutput distribution k.\nAlgorithm 3: Algorithm for unknown \u03b81 \u2212 \u03b80.\nGiven \u03b4 \u2208 (0, 1), \u01eb \u2208 (0, 1]. Initialize k = 1 While Algorithm 2 run with inputs \u03b4/(2k2), \u03b10 = 2\n\u2212k, \u01eb0 = \u01eb returns null: Set k = k + 1.\nOutput distribution k.\nAlgorithm 4: Algorithm for unknown \u03b1.\nWe now leverage Theorem 5 to design procedures that do not have knowledge of these parameters using the \u201cdoubling trick.\u201d. First we consider the case when \u03b1 is known but a lower bound on \u03b81 \u2212 \u03b80 is not. The following theorem characterizes the performance of Algorithm 3.\nTheorem 6 (Known \u03b1, unknown \u03b80, \u03b81). Fix \u03b4 \u2208 (0, 1). If Algorithm 3 is run with \u03b4, \u03b1 then with probability at least 1 \u2212 \u03b4 a heavy distribution is returned and the expected number of total samples taken is no more than\nc log ( log ( 1 (\u03b81\u2212\u03b80)2 ) /\u03b4 )\n\u03b1(\u03b81 \u2212 \u03b80)2 .\nfor an absolute constant c.\nNow we consider the case when \u03b81\u2212 \u03b80 is known but a lower bound on \u03b1 is not. The following theorem characterizes the performance of Algorithm 4.\nTheorem 7 (Unknown \u03b1, known \u03b80, \u03b81). Fix \u03b4 \u2208 (0, 1). If Algorithm 4 is run with \u03b4, \u03b81 \u2212 \u03b80 then with probability at least 1\u2212\u03b4 a heavy distribution is returned and the the expected number of total samples taken is no more than\nc log ( log ( 1 \u03b1 ) /\u03b4 )\n\u03b1(\u03b81 \u2212 \u03b80)2\nfor an absolute constant c."}, {"heading": "3.3 Fully adaptive strategies when \u03b1, \u03b80, \u03b81 are unknown", "text": "We now consider the most difficult setting in which no prior knowledge about \u03b1, \u03b80, \u03b81 are known. The algorithm for this setting, Algorithm 5, requires a more sophisticated argument than the simple \u201cdoubling trick\u201d used above when partial information was available. As far as we are aware this is the first result of its kind that does not require any prior estimation or knowledge of the unknown mean distribution parameters. We also remark that the placing of \u201clandmarks\u201d (\u03b1k, \u01ebk) throughout the search space as is done in Algorithm 5 can also be generalized to generic infinite armed bandit problems, perhaps providing a simple alternative to the two-stage approach of estimation then exploration of Carpentier and Valko (2015).\nGiven \u03b4 > 0. Initialize \u2113 = 1, heavy distribution h = null. Repeat until h is not null:\nSet \u03b3\u2113 = 2\u2113, \u03b4\u2113 = \u03b4/(2\u21133) Repeat for k = 0, . . . , \u2113:\nSet \u03b1k = 2 k\n\u03b3\u2113 , \u01ebk =\n\u221a 1\n2\u03b1k\u03b3\u2113\nRun Algorithm 2 with \u03b10 = \u03b1k, \u01eb0 = \u01ebk, \u03b4 = \u03b4\u2113 and Set h to its output. If h is not null break\nSet \u2113 = \u2113+ 1 Output h\nAlgorithm 5: Adaptive strategy for heavy distribution identification with unknown parameters\nTheorem 8 (Unknown \u03b1, \u03b80, \u03b81). Fix \u03b4 \u2208 (0, 1). If Algorithm 5 is run with \u03b4 then with probability at least 1\u2212 \u03b4 a heavy distribution is returned and the expected number of total samples taken is bounded by\nc log2(\n1 \u03b1\u01eb2 )\n\u03b1\u01eb2 (\u03b1 log2(\n1 \u01eb2 ) + log(log2( 1 \u03b1\u01eb2 )) + log(1/\u03b4))\nfor an absolute constant c."}, {"heading": "4 Conclusion", "text": "In this work, we prove upper and lower bounds on the complexity of detecting mixture distributions with partial or missing knowledge of the distribution parameters. We note that there is still a log-factor gap between several of our upper and lower bounds, and investigating whether either can be tightened remains an interesting problem. Importantly, in this work we considered mixtures of only two components, whereas the literature on infinite-armed bandits considers a continuous mixture. Extending the algorithms developed\nfor our upper bounds to the continuous mixture case is a promising direction, as it would represent the first such algorithm that does not rely on knowledge of the distribution parameters or estimating them first with a two-stage approach."}, {"heading": "Acknowledgments", "text": "Kevin Jamieson is generously supported by ONR awards N00014-15-1-2620, and N00014-13-1-0129. This research is supported in part by NSF CISE Expeditions Award CCF-1139158, DOE Award SN10040 de-sc0012463, and DARPA XData Award FA8750-12-2-0331, and gifts from Amazon Web Services, Google, IBM, SAP, The Thomas and Stacey Siebel Foundation, Apple Inc., Arimo, Blue Goji, Bosch, Cisco, Cray, Cloudera, Ericsson, Facebook, Fujitsu, Guavus, HP, Huawei, Intel, Microsoft, Pivotal, Samsung, Schlumberger, Splunk, State Farm and VMware."}, {"heading": "A Proofs of Lower Bounds", "text": ""}, {"heading": "A.1 Proof of Claim 1", "text": "Proof. Suppose there exists a \u03b4-probably correct procedure with P(N(0, \u03b80, \u03b81) < \u221e) > \u03b4. Then there exists a finite n\u0302 \u2208 N such that P(N(0, \u03b80, \u03b81) \u2264 n\u0302) > \u03b4. For some \u01eb \u2208 (0, 1) to be defined later, define \u03b1\u0302 = log( 1\n1\u2212\u01eb ) 2n\u0302 and note that for this \u03b1\u0302, P( \u22c2n\u0302 i=1{\u03bei = 0}) = (1\u2212 \u03b1\u0302)n\u0302 \u2265 e\u22122n\u0302\u03b1\u0302 \u2265 1\u2212 \u01eb. Thus, the probability\nthat the procedure terminates with a light distribution under \u03b1 = \u03b1\u0302 is at least\nP(N(\u03b1\u0302, \u03b80, \u03b81) \u2264 n\u0302,\u2229n\u0302i=1{\u03bei = 0}) = P(N(\u03b1\u0302, \u03b80, \u03b81) \u2264 n\u0302| \u2229n\u0302i=1 {\u03bei = 0})P(\u2229n\u0302i=1{\u03bei = 0}) = P(N(0, \u03b80, \u03b81) \u2264 n\u0302)P(\u2229n\u0302i=1{\u03bei = 0}) > \u03b4(1 \u2212 \u01eb).\nBecause we can make \u01eb arbitrarily small, the above display implies that the procedure makes a mistake with probability at least \u03b4, but this is a contradiction as the procedure is \u03b4-probably correct."}, {"heading": "A.2 Proof of Theorem 2", "text": "Proof. First, let N be the number of distributions considered at the stopping time T . Note that T \u2265 N . By assumption the procedure satisfies P1(N \u2265 n| \u2229n\u22121i=1 {\u03bei = 0}) \u2265 1\u2212 \u03b4 for all n \u2208 N. And\nP1(N \u2265 n) \u2265 P1(N \u2265 n,\u2229n\u22121i=1 {\u03bei = 0}) = P1(N \u2265 n| \u2229n\u22121i=1 {\u03bei = 0})P1(\u2229n\u22121i=1 {\u03bei = 0}) \u2265 (1\u2212 \u03b4)(1 \u2212 \u03b1)n\u22121\nThus, E1[N ] = \u2211\u221e n=1 P1(N \u2265 n) \u2265 (1 \u2212 \u03b4) \u2211\u221e\nn=1(1 \u2212 \u03b1)n\u22121 = 1\u2212\u03b4\u03b1 which results in the first argument of the max.\nApplying Theorem 2.38 of Siegmund (2013) we have\nE1[N ]\u03c7 2 (P1|P0)\nEqn. (2) \u2265 E1[N ]KL (P1|P0) Thm. 2.38 \u2265 log( 1P0(N<\u221e)) assumption \u2265 log(1\u03b4 ),\nwhich results in the second argument of the max. If \u0398\u0303 = {\u03b80} then \u03c72(P1|P0) = \u03c72((1\u2212 \u03b1)f\u03b80 + \u03b1f\u03b81 |f\u03b80) and\n\u03c72((1\u2212 \u03b1)f\u03b80 + \u03b1f\u03b81 |f\u03b80) = \u222b\n((1\u2212 \u03b1)f\u03b80(x) + \u03b1f\u03b81(x)\u2212 f\u03b80(x))2 f\u03b80(x) dx = \u03b12\u03c72(f\u03b81 |f\u03b80)\nThus, E1[N ] \u2265 log(\n1 \u03b4 )\n\u03b12\u03c72(f\u03b81 |f\u03b80) which results in the second part of the theorem."}, {"heading": "A.3 Proof of Corollary 1", "text": "Proof. For k = 0, 1 let g\u03b8k be a Bernoulli distribution with parameter \u03b8k and let f\u03b8k = g\u03b8k \u2297 \u00b7 \u00b7 \u00b7 \u2297 g\u03b8k be a product distribution composed of m g\u03b8k distributions. Then\n\u03c72(g\u03b81 |g\u03b80) = (\u03b81 \u2212 \u03b80)2 \u03b80(1\u2212 \u03b80) \u2264 e (\u03b81\u2212\u03b80) 2 \u03b80(1\u2212\u03b80) \u2212 1\nand\n\u03c72(f\u03b81 |f\u03b80) = ( 1 + \u03c72(g\u03b81 |g\u03b80) )m \u2212 1 \u2264 em (\u03b81\u2212\u03b80) 2 \u03b80(1\u2212\u03b80) \u2212 1.\nMoreover, e m\n(\u03b81\u2212\u03b80) 2 \u03b80(1\u2212\u03b80) \u2212 1 \u2264 m (\u03b81\u2212\u03b80)2\u03b80(1\u2212\u03b80) whenever m \u2264 \u03b80(1\u2212\u03b80) 2(\u03b81\u2212\u03b80)2 since e\nx/2 \u2212 1 \u2264 x for all x \u2208 [0, 1]. Applying Theorem 2 obtains the first result. The second result follows from loosening the integer constraint on m and minimizing the the lower bound on E[Nm] multiplied by m. To perform the minimization, we\nnote that the function max{1\u2212\u03b4\u03b1 , 2 log(1\u03b4 )/[\u03b12(e m\n(\u03b81\u2212\u03b80) 2\n\u03b80(1\u2212\u03b80) \u2212 1)]} reaches its minimum at the intersection of the two arguments and solve for m at that point."}, {"heading": "A.4 Proof of Theorem 3", "text": "Proof. Define \u03c6x(\u03b7) = h(x) exp(\u03b7x \u2212 b(\u03b7)). By the properties of scalar exponential families, note that b\u2032(\u03b7) and b\u2032\u2032(\u03b7) \u2265 0 represent the mean and variance of the distribution. We deduce that b\u2032 is monotonically increasing. Define \u03b70 = \u03b7(\u03b80), \u03b71 = \u03b7(\u03b81), and \u00b5 = (1\u2212 \u03b1)\u03b70 + \u03b1\u03b71. Noting that\n\u03c72((1 \u2212 \u03b1)\u03c6x(\u03b70) + \u03b1\u03c6x(\u03b71)|\u03c6x(\u00b5)) = \u222b \u03c6x(\u00b5) ( (1\u2212 \u03b1)\u03c6x(\u03b70) + \u03b1\u03c6x(\u03b71)\u2212 \u03c6x(\u00b5)\n\u03c6x(\u00b5)\n)2 dx\nwe will use a technique that was used in Pollard (2000) to approximate the divergence between a single Gaussian distribution and a mixture of them. Essentially, we will take the Taylor series of each \u03c6x(\u00b7) centered at \u00b5 and bound. We have\n\u03c6x(\u03b7) = h(x) exp(\u03b7x\u2212 b(\u03b7)) \u03c6\u2032x(\u03b7) = (x\u2212 b\u2032(\u03b7))\u03c6x(\u03b7) \u03c6\u2032\u2032x(\u03b7) = (\u2212b\u2032\u2032(\u03b7) + (x\u2212 b\u2032(\u03b7))2)\u03c6x(\u03b7)\nso that\n\u03c6x(y) = \u03c6x(\u00b5) [ 1 + (x\u2212 b\u2032(\u00b5))(y \u2212 \u00b5) + 12(\u2212b\u2032\u2032(\u00b5) + (x\u2212 b\u2032(\u00b5))2)(y \u2212 \u00b5)2 . . . ] .\nNoting that (\u03b70\u2212\u00b5) = \u2212\u03b1(\u03b71\u2212\u03b70), (\u03b71\u2212\u00b5) = (1\u2212\u03b1)(\u03b71\u2212\u03b70), and (1\u2212\u03b1)\u03b12+\u03b1(1\u2212\u03b1)2 = \u03b1(1\u2212\u03b1), we have\n\u2223\u2223\u2223\u2223 (1\u2212 \u03b1)\u03c6x(\u03b70) + \u03b1\u03c6x(\u03b71)\u2212 \u03c6x(\u00b5)\n\u03c6x(\u00b5)\n\u2223\u2223\u2223\u2223\n= \u2223\u2223\u2223\u2223 \u03c6\u2032x(\u00b5) \u03c6x(\u00b5) [(1\u2212 \u03b1)(\u03b70 \u2212 \u00b5) + \u03b1(\u03b71 \u2212 \u00b5)] + 1 2 \u03c6\u2032\u2032x(\u00b5) \u03c6x(\u00b5) [(1\u2212 \u03b1)(\u03b70 \u2212 \u00b5)2 + \u03b1(\u03b71 \u2212 \u00b5)2] + . . . \u2223\u2223\u2223\u2223\n= \u2223\u2223\u2223\u2223 1\n2 \u03c6\u2032\u2032x(\u00b5) \u03c6x(\u00b5)\n\u03b1(1\u2212 \u03b1)(\u03b71 \u2212 \u03b70)2 + . . . \u2223\u2223\u2223\u2223\n\u2264 sup z\u2208[\u03b70,\u03b71] |\u03c6\u2032\u2032x(z)| \u03c6x(\u00b5) 1 2 \u03b1(1 \u2212 \u03b1)(\u03b71 \u2212 \u03b70)2.\nThus,\n\u03c72((1 \u2212 \u03b1)\u03c6x(\u03b70) + \u03b1\u03c6x(\u03b71)|\u03c6x(\u00b5)) = \u222b \u03c6x(\u00b5) ( (1\u2212 \u03b1)\u03c6x(\u03b70) + \u03b1\u03c6x(\u03b71)\u2212 \u03c6x(\u00b5)\n\u03c6x(\u00b5)\n)2 dx\n\u2264 ( 1\n2 \u03b1(1 \u2212 \u03b1)(\u03b71 \u2212 \u03b70)2\n)2 \u222b sup\nz\u2208[\u03b70,\u03b71]\n|\u03c6\u2032\u2032x(z)|2 \u03c6x(\u00b5)2 \u03c6x(\u00b5)dx.\nBy distributing the square and noting that b\u2032\u2032(\u03b7) \u2265 0, we have \u222b\nsup z\u2208[\u03b70,\u03b71] |\u03c6\u2032\u2032x(z)|2 \u03c6x(\u00b5)2 \u03c6x(\u00b5)dx =\n\u222b sup\nz\u2208[\u03b70,\u03b71]\n( \u03c6x(z)\n\u03c6x(\u00b5)\n)2 (\u2212b\u2032\u2032(z) + (x\u2212 b\u2032(z))2)2 \u03c6x(\u00b5)dx\n\u2264 \u222b\nsup z\u2208[\u03b70,\u03b71]\n( \u03c6x(z)\n\u03c6x(\u00b5)\n)2 b\u2032\u2032(z)2 \u03c6x(\u00b5)dx+ \u222b sup\nz\u2208[\u03b70,\u03b71]\n( \u03c6x(z)\n\u03c6x(\u00b5)\n)2 (x\u2212 b\u2032(z))4 \u03c6x(\u00b5)dx\n\u2264 sup y\u2208[\u03b70,\u03b71]\nb\u2032\u2032(y)2 \u222b\nsup z\u2208[\u03b70,\u03b71]\n( \u03c6x(z)\n\u03c6x(\u00b5)\n)2 \u03c6x(\u00b5)dx+ \u222b sup\nz\u2208[\u03b70,\u03b71]\n( \u03c6x(z)\n\u03c6x(\u00b5)\n)2 (x\u2212 b\u2032(z))4 \u03c6x(\u00b5)dx.\nThe remainder of the proof bounds the integrals. Define \u03b7\u2212 = 2\u03b70\u2212\u00b5 = \u03b7(\u03b8\u2212) and \u03b7\u2212 = 2\u03b71\u2212\u00b5 = \u03b7(\u03b8+). Observe that\nsup z\u2208[\u03b70,\u03b71]\n( \u03c6x(z)\n\u03c6x(\u00b5)\n)2 \u03c6x(\u00b5)\n= sup z\u2208[\u03b70,\u03b71]\nh(x) exp ( (2z \u2212 \u00b5)x\u2212 (2b(z) \u2212 b(\u00b5)) )\n= sup z\u2208[\u03b70,\u03b71]\nh(x) exp ( (2z \u2212 \u00b5)x\u2212 b(2z \u2212 \u00b5) ) exp ( b(2z \u2212 \u00b5)\u2212 (2b(z) \u2212 b(\u00b5)) )\n\u2264 e\u03ba sup z\u2208[\u03b70,\u03b71]\nh(x) exp ( (2z \u2212 \u00b5)x\u2212 b(2z \u2212 \u00b5) )\n= e\u03ba sup z\u2208[2\u03b70\u2212\u00b5,2\u03b71\u2212\u00b5]\nh(x) exp ( zx\u2212 b(z) )\n= e\u03ba sup z\u2208[\u03b7\u2212,\u03b7+]\nh(x) exp ( zx\u2212 b(z) )\n\u2264 e\u03ba ( \u03c6x(\u03b7\u2212) + \u03c6x(\u03b7+) + \u03c6x(b\u0307\n\u22121(x))1x\u2208[b\u0307(\u03b7\u2212),b\u0307(\u03b7+)]\n)\n\u2264 e\u03ba ( \u03c6x(\u03b7\u2212) + \u03c6x(\u03b7+) + \u03b31x\u2208[b\u0307(\u03b7\u2212),b\u0307(\u03b7+)] )\nwhere the second inequality follows by observing that the maximum of the function \u03c6x(z) will occur either at an endpoint of the interval z \u2208 [\u03b7(\u03b8\u2212), \u03b7(\u03b8+)] or at the point where \u2202\u2202zg(z) = 0 (if that point occurs inside the interval), and loosely bounding the maximum by simply adding the function values at all three points.\nConsequently,\nsup y\u2208[\u03b70,\u03b71]\nb\u2032\u2032(y)2 \u222b\nsup z\u2208[\u03b70,\u03b71]\n( \u03c6x(z)\n\u03c6x(\u00b5)\n)2 \u03c6x(\u00b5)dx \u2264 sup\n\u03b8\u2208[\u03b80,\u03b81] M2(\u03b8)\n2e\u03ba ( 2 + \u03b3(b\u0307(\u03b7+)\u2212 b\u0307(\u03b7\u2212)) ) .\nBy Jensen\u2019s inequality, (a+ b)4 = 16(12a+ 1 2b) 4 \u2264 8(a4 + b4), so \u222b\nsup z\u2208[\u03b70,\u03b71]\n\u03c6x(\u03b7\u2212)(x\u2212 b\u0307(z))4dx = \u222b\nsup z\u2208[\u03b70,\u03b71]\n\u03c6x(\u03b7\u2212)(x\u2212 b\u0307(\u03b7\u2212) + b\u0307(\u03b7\u2212)\u2212 b\u0307(z))4dx\n\u2264 \u222b\n8\u03c6x(\u03b7\u2212)[(x\u2212 b\u0307(\u03b7\u2212))4 + sup z\u2208[\u03b70,\u03b71] (b\u0307(\u03b7\u2212)\u2212 b\u0307(z))4]dx\n\u2264 \u222b 8\u03c6x(\u03b7\u2212)[(x\u2212 b\u0307(\u03b7\u2212))4 + (b\u0307(\u03b7\u2212)\u2212 b\u0307(\u03b71))4]dx\n= 8[M4(\u03b8\u2212)\u2212 (b\u0307(\u03b7\u2212)\u2212 b\u0307(\u03b71))4].\nRepeating an analogous series of steps for \u03b7+, we have\n\u222b sup\nz\u2208[\u03b70,\u03b71]\n( \u03c6x(z)\n\u03c6x(\u00b5)\n)2 (x\u2212 b\u2032(z))4 \u03c6x(\u00b5)dx\n\u2264 e\u03ba \u222b (\n\u03c6x(\u03b7\u2212) + \u03c6x(\u03b7+) + \u03b31x\u2208[b\u0307(\u03b7\u2212),b\u0307(\u03b7+)]\n) sup\nz\u2208[\u03b70,\u03b71] (x\u2212 b\u0307(z))4dx\n\u2264 e\u03ba ( 8M4(\u03b8\u2212) + 8(b\u0307(\u03b71)\u2212 b\u0307(\u03b7\u2212))4 + 8M4(\u03b8+) + 8(b\u0307(\u03b7+)\u2212 b\u0307(\u03b70))4 + 25\u03b3(b\u0307(\u03b7+)\u2212 b\u0307(\u03b7\u2212))5 ) \u2264 e\u03ba ( 8M4(\u03b8\u2212) + 8M4(\u03b8+) + 16(b\u0307(\u03b7+)\u2212 b\u0307(\u03b7\u2212))4 + 25\u03b3(b\u0307(\u03b7+)\u2212 b\u0307(\u03b7\u2212))5 ) .\nThe final result holds by Theorem 2."}, {"heading": "A.5 Proof of Corollary 2", "text": "Proof. A binomial distribution for fixed m is an exponential family f\u03b8(x) = h(x) exp(\u03b7(\u03b8)x \u2212 b(\u03b7(\u03b8))) with h(x) = (m x ) , \u03b7(\u03b8) = log( \u03b81\u2212\u03b8 ), and b(\u03c4) = m log(1 + e \u03c4 ). Note that \u03b7 is monotonically increasing, b is m-Lipschitz, and b\u0307(\u03c4) = m(1 + e\u2212\u03c4 )\u22121 so that b\u0307(\u03b7(\u03b8)) = m\u03b8. Step 1: Relating \u03b8+, \u03b8\u2212 to \u03b81, \u03b80 We will make repeated use of the fact that if f is convex then f(y) \u2265 f(x)+ f \u2032(x)T (y\u2212x). Since x1\u2212x and 1\u2212x x are both convex, we have\ny 1\u2212 y \u2265 x 1\u2212 x + y \u2212 x (1\u2212 x)2 and 1\u2212 y y \u2265 1\u2212 x x \u2212 y \u2212 x x2\nfor all x, y \u2208 [0, 1]. To begin, note \u03b7\u22121(\u03bd) = (1+e\u2212\u03bd)\u22121 so that for any \u03b8 we have \u03b8(1\u2212\u03b8) = \u03b7\u22121(\u03b7(\u03b8))(1\u2212\u03b7\u22121(\u03b7(\u03b8))) =\ne\u2212\u03b7(\u03b8)\n(1+e\u2212\u03b7(\u03b8))2 . Observe that\n1 4 e\u2212|\u03b7(\u03b8)| \u2264 e\n\u2212\u03b7(\u03b8)\n(1 + e\u2212\u03b7(\u03b8))2 \u2264 e\u2212|\u03b7(\u03b8)|\nand recalling that \u03b8\u2217 = \u03b7\u22121((1\u2212 \u03b1)\u03b80 + \u03b1\u03b81) \u2208 [\u03b80, \u03b81] we have\n\u03b8+(1\u2212 \u03b8+) \u2265 1 4 e\u2212|\u03b7(\u03b8+)| = 1 4 e\u2212|2\u03b7(\u03b81)\u2212\u03b7(\u03b8\u2217)|\n= 141\u03b8+\u22641/2\n( \u03b81\n1\u2212 \u03b81 )2 (1\u2212 \u03b8\u2217 \u03b8\u2217 ) + 141\u03b8+>1/2 ( 1\u2212 \u03b81 \u03b81 )2( \u03b8\u2217 1\u2212 \u03b8\u2217 )\n\u2265 141\u03b8+\u22641/2 (\n\u03b81 1\u2212 \u03b81 )2 (1\u2212 \u03b81 \u03b81 ) + 141\u03b8+>1/2 ( 1\u2212 \u03b81 \u03b81 )2( \u03b80 1\u2212 \u03b80 )\n\u2265 141\u03b8+\u22641/2 (\n\u03b81 1\u2212 \u03b81\n) + 141\u03b8+>1/2 ( 1\u2212 \u03b81 \u03b81 )2( \u03b81 1\u2212 \u03b81 \u2212 \u03b81 \u2212 \u03b80 (1\u2212 \u03b81)2 )\n\u2265 141\u03b8+\u22641/2 (\n\u03b81 1\u2212 \u03b81\n) + 181\u03b8+>1/2 ( 1\u2212 \u03b81 \u03b81 ) \u2265 1 8 \u03b81(1\u2212 \u03b81)\nwhere the last line follows from the assumption that \u03b81(1\u2212 \u03b81) \u2265 2(\u03b81 \u2212 \u03b80). Analogously,\n\u03b8\u2212(1\u2212 \u03b8\u2212) \u2265 1 4 e\u2212|\u03b7(\u03b8\u2212)| = 1 4 e\u2212|2\u03b7(\u03b80)\u2212\u03b7(\u03b8\u2217)|\n= 141\u03b8\u2212\u22641/2\n( \u03b80\n1\u2212 \u03b80 )2 (1\u2212 \u03b8\u2217 \u03b8\u2217 ) + 141\u03b8\u2212>1/2 ( 1\u2212 \u03b80 \u03b80 )2( \u03b8\u2217 1\u2212 \u03b8\u2217 )\n\u2265 141\u03b8\u2212\u22641/2 (\n\u03b80 1\u2212 \u03b80 )2 (1\u2212 \u03b81 \u03b81 ) + 141\u03b8\u2212>1/2 ( 1\u2212 \u03b80 \u03b80 )2( \u03b80 1\u2212 \u03b80 )\n\u2265 141\u03b8\u2212\u22641/2 (\n\u03b80 1\u2212 \u03b80 )2 (1\u2212 \u03b80 \u03b80 \u2212 \u03b81 \u2212 \u03b80 \u03b820 ) + 141\u03b8\u2212>1/2 ( 1\u2212 \u03b80 \u03b80 )\n\u2265 181\u03b8\u2212\u22641/2 (\n\u03b80 1\u2212 \u03b80\n) + 141\u03b8\u2212>1/2 ( 1\u2212 \u03b80 \u03b80 ) \u2265 1 8 \u03b80(1\u2212 \u03b80)\nwhere the last line follows from the assumption that \u03b80(1\u2212 \u03b80) \u2265 2(\u03b81 \u2212 \u03b80). We conclude that\ninf \u03b8\u2208[\u03b8\u2212,\u03b8+] \u03b8(1\u2212 \u03b8) \u2265 1 8 inf \u03b8\u2208[\u03b80,\u03b81] \u03b8(1\u2212 \u03b8). (3)\nConversely,\nsup \u03b8\u2208[\u03b8\u2212,\u03b8+]\n\u03b8(1\u2212 \u03b8) \u2264 11/2\u2208[\u03b8\u2212,\u03b8+] 1\n4 + \u03b8+(1\u2212 \u03b8+)1\u03b8+\u22641/2 + \u03b8\u2212(1\u2212 \u03b8\u2212)1\u03b8\u2212>1/2.\nWe consider these three cases in turn. If \u03b8+ \u2264 1/2:\n\u03b8+(1\u2212 \u03b8+) \u2264 e\u2212|\u03b7(\u03b8+)| = e\u2212|2\u03b7(\u03b81)\u2212\u03b7(\u03b8\u2217)|\n=\n( \u03b81\n1\u2212 \u03b81 )2 (1\u2212 \u03b8\u2217 \u03b8\u2217 ) \u2264 ( \u03b81 1\u2212 \u03b81 )2 (1\u2212 \u03b80 \u03b80 ) \u2264 ( \u03b81 1\u2212 \u03b81 )2(1\u2212 \u03b81 \u03b81 + \u03b81 \u2212 \u03b80 \u03b820 )\n=\n( \u03b81\n1\u2212 \u03b81\n)( 1 +\n\u03b81(\u03b81 \u2212 \u03b80) (1\u2212 \u03b81)\u03b820\n) \u2264 ( \u03b81\n1\u2212 \u03b81\n)( 1 +\n\u03b81(1\u2212 \u03b80) 2(1 \u2212 \u03b81)\u03b80\n)\n=\n( \u03b81\n1\u2212 \u03b81\n)( 1 +\n\u03b80(1\u2212 \u03b80) + (\u03b81 \u2212 \u03b80)(1 \u2212 \u03b80) 2(1 \u2212 \u03b81)\u03b80\n)\n\u2264 (\n\u03b81 1\u2212 \u03b81\n)( 1 +\n\u03b80(1\u2212 \u03b80) + \u03b80(1\u2212 \u03b80)2/2 2(1\u2212 \u03b81)\u03b80\n) \u2264 5\n2\n( \u03b81\n1\u2212 \u03b81\n) \u2264 10\u03b81(1\u2212 \u03b81)\nusing the convexity of 1\u2212xx , the assumption that 2(\u03b81 \u2212 \u03b80) \u2264 \u03b80(1 \u2212 \u03b80), that \u03b81 \u2264 \u03b8+ \u2264 1/2, and that 1\u2212 \u03b80 \u2264 1. If \u03b8\u2212 > 1/2:\n\u03b8\u2212(1\u2212 \u03b8\u2212) \u2264 e\u2212|\u03b7(\u03b8\u2212)| = e\u2212|2\u03b7(\u03b80)\u2212\u03b7(\u03b8\u2217)|\n= ( 1\u2212 \u03b80 \u03b80 )2( \u03b8\u2217 1\u2212 \u03b8\u2217 ) \u2264 ( 1\u2212 \u03b80 \u03b80 )2 ( \u03b81 1\u2212 \u03b81 ) \u2264 ( 1\u2212 \u03b80 \u03b80 )2 ( \u03b80 1\u2212 \u03b80 + \u03b81 \u2212 \u03b80 (1\u2212 \u03b81)2 ) \u2264 ( 1\u2212 \u03b80 \u03b80 )( 1 + (1\u2212 \u03b80)(\u03b81 \u2212 \u03b80) \u03b80(1\u2212 \u03b81)2 ) \u2264 ( 1\u2212 \u03b80 \u03b80 )( 1 + (1\u2212 \u03b80)\u03b81/2 \u03b80(1\u2212 \u03b81) )\n= ( 1\u2212 \u03b80 \u03b80 )( 1 + (1\u2212 \u03b81)\u03b81 + (\u03b81 \u2212 \u03b80)\u03b81 2\u03b80(1\u2212 \u03b81) ) \u2264 ( 1\u2212 \u03b80 \u03b80 )( 1 + (1\u2212 \u03b81)\u03b81 + (1\u2212 \u03b81)\u03b821/2 2\u03b80(1\u2212 \u03b81) ) \u2264 5 2 ( 1\u2212 \u03b80 \u03b80 ) \u2264 10\u03b80(1\u2212 \u03b80)\nusing the same methods as above. From these two cases, we can conclude that if 1/2 /\u2208 [\u03b8\u2212, \u03b8+],\nsup \u03b8\u2208[\u03b8\u2212,\u03b8+] \u03b8(1\u2212 \u03b8) \u2264 10 sup \u03b8\u2208[\u03b80,\u03b81] \u03b8(1\u2212 \u03b8). (4)\nThe remaining case, when 1/2 \u2208 [\u03b8\u2212, \u03b8+], also satisfies (4), which we now demonstrate. When \u03b8+ = 1/2 we have 1/4 = \u03b8+(1 \u2212 \u03b8+) \u2264 10\u03b81(1 \u2212 \u03b81) so that \u03b81(1 \u2212 \u03b81) \u2265 1/40. Because \u03b81 is monotonically increasing in \u03b8+ and sup\u03b8\u2208[\u03b8\u2212,\u03b8+] \u03b8(1\u2212 \u03b8) \u2264 1/4 we conclude that (4) holds whenever \u03b81 \u2264 1/2. A similar argument follows for all \u03b80 \u2265 1/2. Finally, if 1/2 \u2208 [\u03b80, \u03b81], it must be true that sup\u03b8\u2208[\u03b8\u2212,\u03b8+] \u03b8(1 \u2212 \u03b8) \u2264 sup\u03b8\u2208[\u03b80,\u03b81] \u03b8(1 \u2212 \u03b8) because \u03b8\u2212 \u2264 \u03b80 \u2264 12 \u2264 \u03b81 \u2264 \u03b8+ and the function \u03b8(1 \u2212 \u03b8) is concave taking its maximum at 12 . Thus, (4) holds for all \u03b8\u2212, \u03b8+.\nWe now turn our attention to bounding \u03b8+ \u2212 \u03b8\u2212. Let g(y) = \u03b7\u22121(y) then g(y) = (1 + e\u2212y)\u22121 and g\u0307(y) = e\u2212y(1 + e\u2212y)\u22122. Observing that g\u0307(\u03b7(\u03b8)) = \u03b8(1\u2212 \u03b8) we have by Taylor\u2019s remainder theorem\n\u03b8+ \u2212 \u03b8\u2212 = \u03b7\u22121(\u03b7(\u03b8+))\u2212 \u03b7\u22121(\u03b7(\u03b8\u2212)) \u2264 (\u03b7(\u03b8+)\u2212 \u03b7(\u03b8\u2212)) sup y\u2208[\u03b7(\u03b8\u2212),\u03b7(\u03b8+)] e\u2212y(1 + e\u2212y)\u22122\n= (\u03b7(\u03b8+)\u2212 \u03b7(\u03b8\u2212)) sup \u03b8\u2208[\u03b8\u2212,\u03b8+] \u03b8(1\u2212 \u03b8) = 2 (\u03b7(\u03b81)\u2212 \u03b7(\u03b80)) sup \u03b8\u2208[\u03b8\u2212,\u03b8+] \u03b8(1\u2212 \u03b8) \u2264 20 (\u03b7(\u03b81)\u2212 \u03b7(\u03b80)) sup \u03b8\u2208[\u03b80,\u03b81] \u03b8(1\u2212 \u03b8).\nSince \u03b7(\u03b8) = log( \u03b81\u2212\u03b8 ) and \u03b7 \u2032(\u03b8) = 1\u03b8 + 1 1\u2212\u03b8 = 1 \u03b8(1\u2212\u03b8) , we have\n\u03b8+ \u2212 \u03b8\u2212 \u2264 20 (\u03b7(\u03b81)\u2212 \u03b7(\u03b80)) sup \u03b8\u2208[\u03b80,\u03b81] \u03b8(1\u2212 \u03b8) \u2264 20 (\u03b81 \u2212 \u03b80) sup\u03b8\u2208[\u03b80,\u03b81] \u03b8(1\u2212 \u03b8) inf\u03b8\u2208[\u03b80,\u03b81] \u03b8(1\u2212 \u03b8) .\nIf \u03b81(1\u2212 \u03b81) \u2265 \u03b80(1\u2212 \u03b80):\n\u03b81(1\u2212 \u03b81) \u03b80(1\u2212 \u03b80) = \u03b80(1\u2212 \u03b81) + (\u03b81 \u2212 \u03b80)(1\u2212 \u03b81) \u03b80(1\u2212 \u03b80)\n\u2264 \u03b80(1\u2212 \u03b81) + \u03b80(1\u2212 \u03b80)(1 \u2212 \u03b81)/2 \u03b80(1\u2212 \u03b80) \u2264 1 + (1\u2212 \u03b81)/2 \u2264 3/2,\nelse if \u03b80(1\u2212 \u03b80) \u2265 \u03b81(1\u2212 \u03b81)\n\u03b80(1\u2212 \u03b80) \u03b81(1\u2212 \u03b81) = \u03b80(1\u2212 \u03b81) + \u03b80(\u03b81 \u2212 \u03b80) \u03b81(1\u2212 \u03b81)\n\u2264 \u03b80(1\u2212 \u03b81) + \u03b80\u03b81(1\u2212 \u03b81)/2 \u03b81(1\u2212 \u03b81) \u2264 1 + \u03b80/2 \u2264 3/2.\nFinally, if 1/2 \u2208 [\u03b80, \u03b81] then sup\u03b8\u2208[\u03b80,\u03b81] \u03b8(1\u2212 \u03b8) = 1/4 taking its maximum at 1/4. To maximize the ratio of the sup to the inf, it suffices to just consider the case when \u03b80 = 1/2 or \u03b81 = 1/2. Thus, the above two bounds suffice for this case and we observe that\nsup\u03b8\u2208[\u03b80,\u03b81] \u03b8(1\u2212 \u03b8) inf\u03b8\u2208[\u03b80,\u03b81] \u03b8(1\u2212 \u03b8) \u2264 3/2. (5)\nThus, putting the pieces together, we conclude that\n\u03b8+ \u2212 \u03b8\u2212 \u2264 30(\u03b81 \u2212 \u03b80). (6)\nStep 2: Bounding \u03b3, \u03ba, c In what follows, define \u03b8h = arg sup\u03b8\u2208[\u03b80,\u03b81] \u03b8(1\u2212 \u03b8) and \u03b8l = arg inf\u03b8\u2208[\u03b80,\u03b81] \u03b8(1\u2212 \u03b8). We now continue to bound the terms of the theorem. Note\nsup x\u2208[b\u0307(\u03b7(\u03b8\u2212)),b\u0307(\u03b7(\u03b8+))]\n\u03c6x(b\u0307 \u22121(x)) = sup\nx\u2208[m\u03b8\u2212,m\u03b8+] \u03c6x(\u03b7(x/m))\n\u2264 sup x\u2208[m\u03b8\u2212,m\u03b8+] sup y\u2208[0,1] \u03c6x(\u03b7(y))\n= sup x\u2208[m\u03b8\u2212,m\u03b8+] sup y\u2208[0,1]\n\u0393(m+ 1)\n\u0393(m\u2212 x+ 1)\u0393(x+ 1)y x(1 \u2212 y)m\u2212x\n= sup \u03b8\u2208[\u03b8\u2212,\u03b8+] sup y\u2208[0,1]\n\u0393(m+ 1)\n\u0393(m(1\u2212 \u03b8) + 1)\u0393(m\u03b8 + 1)y m\u03b8(1\u2212 y)m(1\u2212\u03b8)\n\u2264 sup \u03b8\u2208[\u03b8\u2212,\u03b8+] sup y\u2208[0,1] e/2\u03c0\u221a m\u03b8(1\u2212 \u03b8) ym\u03b8(1\u2212 y)m(1\u2212\u03b8) \u03b8m\u03b8(1\u2212 \u03b8)m(1\u2212\u03b8)\n= sup \u03b8\u2208[\u03b8\u2212,\u03b8+] e/2\u03c0\u221a m\u03b8(1\u2212 \u03b8) \u2264 2\u221a m\u03b8l(1\u2212 \u03b8l) =: \u03b3\nby Stirling\u2019s approximation: \u221a 2\u03c0 \u2264 \u0393(s+1)\ne\u2212sss+1/2 \u2264 e (Spira, 1971) and (3). And for any y \u2208 [\u03b80, \u03b81]\nb(2\u03b7(y) \u2212 \u03b7(\u03b8\u2217))\u2212 (2b(\u03b7(y)) \u2212 b(\u03b7(\u03b8\u2217))) = m log(1 + e2\u03b7(y)\u2212\u03b7(\u03b8\u2217))\u2212 2m log(1 + e\u03b7(y)) +m log(1 + e\u03b7(\u03b8\u2217))\n= m log\n( (1 + e2\u03b7(y)\u2212\u03b7(\u03b8\u2217))(1 + e\u03b7(\u03b8\u2217))\n(1 + e\u03b7(y))2\n)\n= m log (( 1 + ( y\n1\u2212 y )2 1\u2212 \u03b8\u2217 \u03b8\u2217 )( 1 1\u2212 \u03b8\u2217 ) (1\u2212 y)2 )\n= m log ( (1\u2212 y)2 1\n1\u2212 \u03b8\u2217 + y2\n1\n\u03b8\u2217\n)\n= m log ( (1\u2212 2y + y2) \u03b8\u2217\n\u03b8\u2217(1\u2212 \u03b8\u2217) + y2 1\u2212 \u03b8\u2217 \u03b8\u2217(1\u2212 \u03b8\u2217)\n)\n= m log ( (1\u2212 2y) \u03b8\u2217\n\u03b8\u2217(1\u2212 \u03b8\u2217) + y2\n1\n\u03b8\u2217(1\u2212 \u03b8\u2217)\n)\n= m log ( 1 +\n(y \u2212 \u03b8\u2217)2 \u03b8\u2217(1\u2212 \u03b8\u2217)\n)\nso\nsup y\u2208[\u03b80,\u03b81]\nb(2\u03b7(y) \u2212 \u03b7(\u03b8\u2217))\u2212 (2b(\u03b7(y)) \u2212 b(\u03b7(\u03b8\u2217)))\n\u2264 sup y\u2208[\u03b80,\u03b81] m log\n( 1 +\n(y \u2212 \u03b8\u2217)2 \u03b8\u2217(1\u2212 \u03b8\u2217)\n) \u2264 m ( (\u03b81 \u2212 \u03b80)2 \u03b8\u2217(1\u2212 \u03b8\u2217) ) =: \u03ba.\nNoting that M2(\u03b8) = m\u03b8(1\u2212 \u03b8),\nsup y\u2208[\u03b80,\u03b81]\nM2(y) 2(2 + \u03b3(b\u0307(\u03b7(\u03b8+))\u2212 b\u0307(\u03b7(\u03b8\u2212)))) \u2264 m2 (\u03b8h(1\u2212 \u03b8h))2 (2 + \u03b3m(\u03b8+ \u2212 \u03b8\u2212))\n\u2264 m2 (\u03b8h(1\u2212 \u03b8h))2 ( 2 +\n2m\u221a m\u03b8l(1\u2212 \u03b8l)\n30(\u03b81 \u2212 \u03b80) )\n\u2264 m2 (\u03b8h(1\u2212 \u03b8h))2 ( 2 + 60 \u221a m (\u03b81 \u2212 \u03b80)2 \u03b8l(1\u2212 \u03b8l) ) .\nSince for any \u03b8 \u2208 [0, 1]\nM4(\u03b8) = m\u03b8(1\u2212 \u03b8) (3\u03b8(1\u2212 \u03b8)(m\u2212 2) + 1) < 3m2 (\u03b8(1\u2212 \u03b8))2 +m\u03b8(1\u2212 \u03b8),\nwe have\n8M4(\u03b8\u2212) + 8M4(\u03b8+) + 16 ( b\u0307(\u03b7(\u03b8+))\u2212 b\u0307(\u03b7(\u03b8\u2212)) )4 + 25\u03b3 ( b\u0307(\u03b7(\u03b8+))\u2212 b\u0307(\u03b7(\u03b8\u2212)) )5\n\u226424m2 (\u03b8\u2212(1\u2212 \u03b8\u2212))2 + 8m\u03b8\u2212(1\u2212 \u03b8\u2212) + 24m2 (\u03b8+(1\u2212 \u03b8+))2 + 8m\u03b8+(1\u2212 \u03b8+)\n+ 16m4(\u03b8+ \u2212 \u03b8\u2212)4 + 4/5\u221a\nm\u03b8l(1\u2212 \u03b8l) m5(\u03b8+ \u2212 \u03b8\u2212)5\n\u22644800m2 (\u03b8h(1\u2212 \u03b8h))2 + 160m\u03b8h(1\u2212 \u03b8h)\n+ 3240000m4(\u03b81 \u2212 \u03b80)4 + 19440000 \u221a\nm (\u03b81 \u2212 \u03b80)2 \u03b8l(1\u2212 \u03b8l) m4(\u03b81 \u2212 \u03b80)4\nwhere we have applied (4) and (6). Finally, recall from above that\n\u03b7(\u03b81)\u2212 \u03b7(\u03b80) \u2264 \u03b81 \u2212 \u03b80 \u03b8l(1\u2212 \u03b8l) \u2264 3 2 \u03b81 \u2212 \u03b80 \u03b8\u2217(1\u2212 \u03b8\u2217) .\nStep 3: Putting the pieces together Noting that \u03b8l(1 \u2212 \u03b8l) \u2264 \u03b8\u2217(1 \u2212 \u03b8\u2217) \u2264 \u03b8h(1 \u2212 \u03b8h) and \u03b8h(1\u2212\u03b8h)\u03b8l(1\u2212\u03b8l) \u2264 3/2 by (5), we can use \u03b8\u2217(1 \u2212 \u03b8\u2217) throughout at the cost of a constant. Putting it altogether, if m (\u03b81\u2212\u03b80) 2\n\u03b8\u2217(1\u2212\u03b8\u2217) \u2264 1 then \u03ba \u2264 1 and\nc \u2264 c\u2032 ( m2 (\u03b8\u2217(1\u2212 \u03b8\u2217))2 +m\u03b8\u2217(1\u2212 \u03b8\u2217) +m4(\u03b81 \u2212 \u03b80)4 )\n\u2264 c\u2032 ( m2 (\u03b8\u2217(1\u2212 \u03b8\u2217))2 +m\u03b8\u2217(1\u2212 \u03b8\u2217) )\nfor some absolute constant c\u2032. Thus,\nc ( 1 2\u03b1(1 \u2212 \u03b1) (\u03b7(\u03b81)\u2212 \u03b7(\u03b80)) 2 )2\n\u2264 c\u2032 ( m2 (\u03b8\u2217(1\u2212 \u03b8\u2217))2 +m\u03b8\u2217(1\u2212 \u03b8\u2217) )(9 8 \u03b1(1\u2212 \u03b1) (\u03b81 \u2212 \u03b80) 2 (\u03b8\u2217(1\u2212 \u03b8\u2217))2 )2 \u2264 c\u2032 ( m2 + m\n\u03b8\u2217(1\u2212 \u03b8\u2217)\n)( 9\n8 \u03b1(1 \u2212 \u03b1) (\u03b81 \u2212 \u03b80) 2 \u03b8\u2217(1\u2212 \u03b8\u2217)\n)2 ."}, {"heading": "B Proofs of Upper Bounds", "text": ""}, {"heading": "B.1 Proof of Theorem 4", "text": "Proof. Let \u00b5\u0302i be the empirical mean of the ith distribution sampled m times with mean \u00b5i \u2208 {\u03b80, \u03b81}. Let N be the minimum of n\u0302 and the first i \u2208 N such that \u00b5\u0302i \u2265 \u03b80+\u03b812 . Declare distribution N to be heavy. The total number of flips this procedure makes equals mN .\nDefine the events\n\u03be1 = n\u0302\u22c3\ni=1\n{\u00b5i = \u03b81}, and \u03be2 = n\u0302\u22c2\ni=1\n{|\u00b5\u0302i \u2212 \u00b5i| < \u03b81\u2212\u03b802 }.\nNote that P(\u03bec1) = P(\u00b51 = \u03b80) n\u0302 = (1 \u2212 \u03b1)n\u0302 \u2264 exp(\u2212\u03b1n\u0302) \u2264 \u03b4/2. And, by a union bound and Chernoff\u2019s inequality P (\u03bec2) \u2264 2n\u0302e\u2212m(\u03b81\u2212\u03b80) 2/2 \u2264 \u03b4/2. Thus, the probability that \u03be1 or \u03be2 fail to occur is less than \u03b4, so in what follows assume they succeed. Under \u03be1 at least one of the n\u0302 distributions is heavy. Under \u03be2, for any i \u2208 [n\u0302] with \u00b5i = \u03b80 we have \u00b5\u0302i < \u00b5i + \u03b81\u2212\u03b80 2 = \u03b80+\u03b81\n2 which implies that the procedure will never exit with a light distribution unless N = n\u0302. On the other hand, for the first i \u2208 [n\u0302] with \u00b5i = \u03b81 we have \u00b5\u0302i > \u00b5i \u2212 \u03b81\u2212\u03b802 = \u03b80+\u03b812 which means the algorithm will output distribution i at time N = i. Thus, N is equal to the first distribution that is heavy and\nE[N ] = n\u0302\u2211\nn=1\nP(N \u2265 n) = n\u0302\u2211\nn=1\nP(N \u2265 n, max i=1,...,n\u22121 \u00b5i 6= \u03b81) + P(N \u2265 n, max i=1,...,n\u22121 \u00b5i = \u03b81)\n\u2264 n\u0302\u2211\nn=1\nP( max i=1,...,n\u22121 \u00b5i 6= \u03b81) + P(\u222an\u22121i=1 {|\u00b5\u0302i \u2212 \u00b5i| > \u03b81\u2212\u03b802 }| maxi=1,...,n\u22121\u00b5i = \u03b81)P( maxi=1,...,n\u22121\u00b5i = \u03b81)\n\u2264 n\u0302\u2211\nn=1\nP( max i=1,...,n\u22121\n\u00b5i 6= \u03b81) + P(\u222an\u22121i=1 {|\u00b5\u0302i \u2212 \u00b5i| > \u03b81\u2212\u03b802 })\n\u2264 n\u0302\u2211\nn=1\n(1\u2212 \u03b1)n\u22121 + n\u22121n\u0302 \u03b4 2 \u2264 1 \u03b1 + n\u0302\u03b4/4 = 1 \u03b1 (1 + \u03b4 log(2e/\u03b4)4 ) \u2264 3/2 \u03b1 .\nMultiplying E[N ] by m yields the result."}, {"heading": "B.2 Proof of Theorem 5", "text": "First, we prove several technical lemmas necessary to analyze our algorithm.\nLemma 1. For i \u2208 N, let Xi \u2208 [ai, bi] for |bi \u2212 ai| \u2264 1 be a random variable with E[Xi] = 0. Then\nP\n( \u221e\u22c3\nn=1\n{ n\u2211\ni=1\nXi \u2265 \u03b1n+ \u03b2 }) \u2264 7 exp(\u2212\u03b1\u03b2/2)\nwhenever \u03b1\u03b2 \u2265 1.\nProof. First we will break the bound into two pieces:\nP\n( \u221e\u22c3\nn=1\n{ n\u2211\ni=1\nXi \u2265 \u03b1n+ \u03b2 })\n\u2264 min n0 P\n( n0\u22c3\nn=1\n{ n\u2211\ni=1\nXi \u2265 \u03b2 }) + P ( \u221e\u22c3\nn=n0+1\n{ n\u2211\ni=1\nXi \u2265 \u03b1n })\nwhere P ( \u22c3n0 n=1 { \u2211n\ni=1 Xi \u2265 \u03b2}) \u2264 exp(\u22122\u03b22/n0) by Doob-Hoeffding\u2019s maximal inequality. For any fixed k \u2208 N:\nP\n  2k\u2211\ni=1\nXi \u2265 \u03b12k/2   \u2264 exp(\u2212\u03b122k/2)\nand\nP\n  2k+1\u22c3\nn=2k+1\n   n\u2211\ni=2k+1\nXi \u2265 \u03b1n/2      \u2264 P   2k+1\u22c3\nn=2k+1\n   n\u2211\ni=2k+1\nXi \u2265 \u03b12k/2     \n= P\n  2k\u22c3\n\u2113=1\n{ \u2113\u2211\ni=1\nXi \u2265 \u03b12k/2 }  \u2264 exp(\u2212\u03b122k/2)\nby Hoeffding\u2019s and Doob-Hoeffding\u2019s maximal inequality, respectively. Thus\nP\n( \u221e\u22c3\nn=n0\n{ n\u2211\ni=1\nXi \u2265 \u03b1n }) = P   \u221e\u22c3\nn=n0\n   2\u2308log2(n)\u2309\u2211\ni=1\nXi +\nn\u2211\ni=2\u2308log2(n)\u2309+1\nXi \u2265 \u03b1n     \n= P\n \n\u221e\u22c3\nk=log2(n0)\n2k+1\u22c3\nn=2k+1\n   2k\u2211\ni=1\nXi + n\u2211\ni=2k+1\nXi \u2265 \u03b1n     \n\u2264 \u221e\u2211\nk=log2(n0)\nP\n  2k\u2211\ni=1\nXi \u2265 \u03b12k/2   + P   2k+1\u22c3\nn=2k+1\n   n\u2211\ni=2k+1\nXi \u2265 \u03b1n/2     \n\u2264 \u221e\u2211\nk=log2(n0)\n2 exp(\u2212\u03b122k/2) \u2264 2 \u222b \u221e\nlog2(n0) exp(\u2212(\u03b1/2)22x)dx\n= 2\nlog(2)\n\u222b \u221e\nn0\nu\u22121 exp(\u2212(\u03b1/2)2u)du \u2264 8 exp(\u2212(\u03b1/2) 2n0)\nn0\u03b12 log(2) .\nPutting the pieces together we have\nP\n( \u221e\u22c3\nn=1\n{ n\u2211\ni=1\nXi \u2265 \u03b1n+ \u03b2 })\n\u2264 min n0\nexp(\u22122\u03b22/n0) + 8 exp(\u2212(\u03b1/2)2n0)\nn0\u03b12 log(2)\n\u2264 exp(\u2212\u03b2\u03b1) + 4 exp(\u2212\u03b2\u03b1/2) \u03b2\u03b1 log(2) \u2264 7 exp(\u2212\u03b2\u03b1/2)\nwhere the last inequality holds with \u03b2\u03b1 \u2265 1.\nLemma 2. Given \u03b81 \u2212 \u03b3\u0302 \u2265 2Bm ,\nP ( max\nj=1,...,m\nj\u2211\ns=1\n(Xi,s \u2212 \u03b3\u0302) > B \u2223\u2223\u00b5i = \u03b81 ) \u2265 1\u2212 exp ( \u2212m(\u03b81 \u2212 \u03b3\u0302)2/2 ) .\nSimilarly, given \u03b3\u0302 \u2212 \u03b80 \u2265 2|A|m ,\nP ( min\nj=1,...,m\nj\u2211\ns=1\n(Xi,s \u2212 \u03b3\u0302) < A \u2223\u2223\u00b5i = \u03b80 ) \u2265 1\u2212 exp ( \u2212m(\u03b3\u0302 \u2212 \u03b80)2/2 ) .\nProof. We analyze the left hand side of the lemma:\nP ( max\nj=1,...,m\nj\u2211\ns=1\n(Xi,s \u2212 \u03b3\u0302) > B \u2223\u2223\u2223\u2223\u00b5i = \u03b81\n)\n= P\n  m\u22c3\nj=1\n{ j\u2211\ns=1\n(Xi,s \u2212 \u03b3\u0302) > B \u2223\u2223\u2223\u2223\u00b5i = \u03b81\n} \n\u2265 P ( m\u2211\ns=1\n(Xi,s \u2212 \u03b3\u0302) > B \u2223\u2223\u2223\u2223\u00b5i = \u03b81\n)\n= 1\u2212 P ( 1\nm\nm\u2211\ns=1\n(Xi,s \u2212 \u00b5i) \u2264 B\nm \u2212 (\u00b5i \u2212 \u03b3\u0302)\n\u2223\u2223\u2223\u2223\u00b5i = \u03b81 )\n= 1\u2212 P ( 1\nm\nm\u2211\ns=1\n(\u00b5i \u2212Xi,s) \u2265 (\u00b5i \u2212 \u03b3\u0302)\u2212 B\nm\n\u2223\u2223\u2223\u2223\u00b5i = \u03b81\n)\n\u2265 1\u2212 exp ( \u22122m [ (\u03b81 \u2212 \u03b3\u0302)\u2212 B\nm\n]2)\n\u2265 1\u2212 exp (\u2212m(\u03b81 \u2212 \u03b3\u0302)2\n2\n)\nWhere the second to last statement holds by Hoeffding\u2019s inequality, and the last uses the bound on B/m given in the lemma. A nearly identical argument yields the second half of the lemma.\nLemma 3. If \u03b81 \u2212 \u03b3\u0302 \u2265 2Bm then\nP\n( n\u22c3\ni=1\n{ \u00b5i = \u03b81, max\nj=1,...,m\nj\u2211\ns=1\n(Xi,s \u2212 \u03b3\u0302) > B, min j=1,...,m\nj\u2211\ns=1\n(Xi,s \u2212 \u03b3\u0302) > A })\n\u2265 1\u2212 exp [\u2212\u03b1n(1\u2212 exp (\u2212B(\u03b81 \u2212 \u03b3\u0302))\u2212 7 exp(\u2212|A|(\u03b3\u0302 \u2212 \u03b80)/2))]\nProof. Consider iid events \u2126i for i = 1, . . . , n. Then P( \u22c3n i=1\u2126i) = 1 \u2212 P( \u22c2n i=1\u2126 c i ) = 1 \u2212 P(\u2126c1)n = 1\u2212 (1\u2212 P(\u2126i))n \u2265 1\u2212 exp(\u2212nP(\u2126i)). We follow the same line of reasoning:\nP\n( n\u22c3\ni=1\n{ \u00b5i = \u03b81, max\nj=1,...,m\nj\u2211\ns=1\n(Xi,s \u2212 \u03b3\u0302) > B, min j=1,...,m\nj\u2211\ns=1\n(Xi,s \u2212 \u03b3\u0302) > A })\n= 1\u2212 ( 1\u2212 P ( \u00b5i = \u03b81, max\nj=1,...,m\nj\u2211\ns=1\n(Xi,s \u2212 \u03b3\u0302) > B, min j=1,...,m\nj\u2211\ns=1\n(Xi,s \u2212 \u03b3\u0302) > A ))n\n= 1\u2212 ( 1\u2212 \u03b1P ( max\nj=1,...,m\nj\u2211\ns=1\n(Xi,s \u2212 \u03b3\u0302) > B, min j=1,...,m\nj\u2211\ns=1\n(Xi,s \u2212 \u03b3\u0302) > A \u2223\u2223\u2223\u00b5i = \u03b81\n))n\n= 1\u2212 ( 1\u2212 \u03b1 ( 1\u2212 P ( max\nj=1,...,m\nj\u2211\ns=1\n(Xi,s \u2212 \u03b3\u0302) < B \u2223\u2223\u2223\u00b5i = \u03b81 ) \u2212 P ( min\nj=1,...,m\nj\u2211\ns=1\n(Xi,s \u2212 \u03b3\u0302) < A \u2223\u2223\u2223\u00b5i = \u03b81\n)))n\n\u2265 1\u2212 ( 1\u2212 \u03b1 ( 1\u2212 exp ( \u2212m(\u03b81 \u2212 \u03b3\u0302)2/2 ) \u2212 7 exp(\u2212|A|(\u03b3\u0302 \u2212 \u03b80)/2) ))n \u2265 1\u2212 exp [ \u2212\u03b1n(1\u2212 exp ( \u2212m(\u03b81 \u2212 \u03b3\u0302)2/2 ) \u2212 7 exp(\u2212|A|(\u03b3\u0302 \u2212 \u03b80)/2)) ] \u2265 1\u2212 exp [\u2212\u03b1n(1\u2212 exp (\u2212B(\u03b81 \u2212 \u03b3\u0302))\u2212 7 exp(\u2212|A|(\u03b3\u0302 \u2212 \u03b80)/2))]\nWhere the third-to-last inequality applies Lemmas 2 and 1.\nNow, we are ready to prove Theorem 5.\nProof. First, we consider the estimation of \u03b8\u03020 of Algorithm 2, then consider the sample complexity of the algorithm, and then prove correctness.\nLet \u03be0 = {\u03b8\u03020 \u2212 \u03b80 \u2265 \u2212 \u01eb04 } and \u03be1 = {\u03b8\u03020 \u2212 \u03b80 \u2264 \u01eb04 } be the events that we accurately estimate the parameter \u03b80. We will show that P(\u03be0) \u2265 1 \u2212 \u03b4\u2032 and P(\u03be1) \u2265 3/4 where \u03b4\u2032 = min{\u03b4/8, 1m\u01eb20}. Let k1 = 5 and k2 = 8\u01eb \u22122 0 log( 2k1 \u03b4\u2032 ). First note that\nP\n( k1\u22c3\ni=1\n{ |\u00b5\u0302i,k2 \u2212 \u00b5i| \u2265\n\u01eb0 4\n}) \u2264 2k1 exp(\u22122k2(\u01eb0/4)2) \u2264 \u03b4\u2032\nso that with probability at least 1\u2212\u03b4\u2032 we have \u03b8\u03020 = mini=1,...,k1 \u00b5\u0302i,k2 \u2265 mini=1,...,k1 \u00b5i\u2212\u01eb0/4 \u2265 \u03b80\u2212\u01eb0/4, and in particular, P(\u03be0) \u2265 1\u2212\u03b4\u2032. Let E = { \u22c3k1 i=1{\u00b5i = \u03b80}} be the event that at least one of the distributions is light. Then\nP (E) = 1\u2212 \u03b1k1 \u2265 1\u2212 2\u2212k1 \u2265 31/32,\nso that under E \u2229 \u03be0, we have \u03b8\u03020 = mini=1,...,k1 \u00b5\u0302i,k2 \u2264 mini=1,...,k1 \u00b5i + \u01eb0/4 = \u03b80 + \u01eb0/4 which means P(\u03bec1) \u2264 P(\u03bec0 \u222a Ec) \u2264 \u03b4/8 + 1/32 \u2264 1/16. Moreover, the total number of samples is bounded by k1k2 = c\u01eb \u22122 0 log(1/\u03b4\n\u2032) \u2264 c\u01eb\u221220 log(max{1\u03b4 , log( 1\u03b10\u03b4 )}) which is clearly dominated by log(1/\u03b4) \u03b10\u01eb20 .\nWe now turn our attention to the sample complexity. By Wald\u2019s identitity (Siegmund, 2013, Proposition 2.18),\nE[T ] = E\n[ N\u2211\ni=1\nMi ] = E[N ]E[M1] = E[N ]((1\u2212 \u03b1)E[M1|\u00b51 = \u03b80] + \u03b1E[M1|\u00b51 = \u03b81]).\nTrivially, E[N ] \u2264 n and E[M1|\u00b51 = \u03b81] \u2264 m, so we only need to bound E[M1|\u00b51 = \u03b80]. Clearly we have that\nE[M1|\u00b51 = \u03b80] = E[M1|\u03be0, \u00b51 = \u03b80]P(\u03be0) + E[M1|\u03bec0, \u00b51 = \u03b80]P(\u03bec0) \u2264 E[M1|\u03be0, \u00b51 = \u03b80] + \u03b4\u2032m\nso\nE[M1|\u03be0, \u00b51 = \u03b80] \u2264 \u221e\u2211\nt=1\nP ( argmin\nj\n{ j\u2211\ns=1\n(X1,s \u2212 \u03b3\u0302) < A \u2223\u2223\u2223\u03be0, \u00b51 = \u03b80 } \u2265 t )\n=\n\u221e\u2211\nt=1\n1\u2212 P (\nmin j=1,...,t\u22121\nj\u2211\ns=1\n(X1,s \u2212 \u03b3\u0302) < A \u2223\u2223\u2223\u03be0, \u00b51 = \u03b80\n)\n=\n\u221e\u2211\nt=0\n1\u2212 P (\nmin j=1,...,t\nj\u2211\ns=1\n(X1,s \u2212 \u03b3\u0302) < A \u2223\u2223\u2223\u03be0, \u00b51 = \u03b80\n)\n\u2264 \u221e\u2211\nt=0\n1\u2212 1 \u03b3\u0302\u2212\u03b80\u2265 2|A|t\n(1\u2212 exp ( \u2212t(\u03b3\u0302 \u2212 \u03b80)2/2 )\n\u2264 2|A| \u03b3\u0302 \u2212 \u03b80 + 2e1/2(\u03b3\u0302 \u2212 \u03b80)\u22122 exp (\u2212|A|(\u03b3\u0302 \u2212 \u03b80)) \u2264 3|A| \u03b3\u0302 \u2212 \u03b80 \u2264 293 \u01eb20 .\nwhere the second inequality follows by applying Lemma 2 and the last inequality holds by \u03be0 and the value of |A| since if \u03be0 holds, \u03b3\u0302 \u2212 \u03b80 = \u03b8\u03020 \u2212 \u03b80 + \u01eb02 \u2265 \u01eb04 . Thus\nE[M1] \u2264 (1\u2212 \u03b1) [( 293\n\u01eb20\n) + \u03b4\u2032m ] + \u03b1m \u2264 \u03b4\u2032m+ 1\n\u01eb20\n( 293 + 64\u03b1 log ( 14n \u03b4 )) \u2264\nc\u03b1 log (\n1 \u03b10\u03b4\n)\n\u01eb20\nfor some c where we use the fact that \u03b4\u2032m \u2264 \u01eb\u221220 . So we have\nE[T ] \u2264 nE[M1] \u2264 c\u2032\u03b1 log(1/\u03b10) + c\u2032\u2032 log\n( 1 \u03b4 )\n\u03b10\u01eb20 .\nNow, we analyze the correctness claims. Under \u03be0, \u03b3\u0302 \u2212 \u03b80 \u2265 \u01eb04 . Note that this event fails to occur with probability less than \u03b4/2, and if it is used in conjunction with some other event that fails to occur with probability \u03b4/2, we may conclude that either of these events fail with probability less than \u03b4.\nTo justify Claim 1, we apply Lemma 1 to observe that the probability that we output a light distribution is no greater than\nP(\u03bec0) + P\n( n\u22c3\ni=1\n{ max\nj=1,...,m\nj\u2211\ns=1\n(Xi,s \u2212 \u03b3\u0302) > B,\u00b5i = \u03b80 } \u2223\u2223\u2223\u03be0 ) P(\u03be0)\n\u2264 P(\u03bec0) + n(1\u2212 \u03b1)P (\nmax j=1,...,m\nj\u2211\ns=1\n(Xi,s \u2212 \u03b3\u0302) > B \u2223\u2223\u00b5i = \u03b80, \u03be0\n)\n\u2264 \u03b4/2 + 7n exp(\u2212B(\u03b3\u0302 \u2212 \u03b80)/2) \u2264 \u03b4\nwhere we have used \u03b3\u0302 \u2212 \u03b80 \u2265 \u01eb04 and plugged in the values of B and n. To justify Claim 2, assume \u03b10 \u2264 \u03b1 and \u01eb0 \u2264 \u03b81\u2212 \u03b80. We apply Lemma 3 to observe that the probability that we return a heavy distribution is at least\nP ( \u03be0 \u2229 \u03be1 \u2229 n\u22c3\ni=1\n{ \u00b5i = \u03b81, max\nj=1,...,m\nj\u2211\ns=1\n(Xi,s \u2212 \u03b3\u0302) > B, min j=1,...,m\nj\u2211\ns=1\n(Xi,s \u2212 \u03b3\u0302) > A })\n= P(\u03be0 \u2229 \u03be1)P ( n\u22c3\ni=1\n{ \u00b5i = \u03b81, max\nj=1,...,m\nj\u2211\ns=1\n(Xi,s \u2212 \u03b3\u0302) > B, min j=1,...,m\nj\u2211\ns=1\n(Xi,s \u2212 \u03b3\u0302) > A }\u2223\u2223\u2223\u2223\u03be0, \u03be1 )\n\u2265 P(\u03be0 \u2229 E)(1\u2212 exp [\u2212\u03b1n(1\u2212 exp (\u2212B(\u01eb0/4)) \u2212 7 exp(\u2212|A|(\u01eb0/4)/2))]) \u2265 (15/16)(1 \u2212 exp [ \u2212\u03b1n(1\u2212 ( \u03b414n )2 \u2212 1/3) ] ) \u2265 (15/16)(8/9) \u2265 4/5\nwhere we have used P(\u03be0 \u2229 E) \u2265 1\u2212 P(\u03bec0) \u2212 P(Ec) \u2265 15/16, ( \u03b414n)2 \u2264 1/6, \u03b1n \u2265 2 log(9) and plugged in the values for A and B.\nTo justify Claim 3, we simply observe that the algorithm always terminates after n\u00d7m steps."}, {"heading": "B.3 Proof of Theorem 6", "text": "Proof. On each stage k, Algorithm 2 is called with \u03b4/(2k2). By the guarantees of Theorem 5, the probability that Algorithm 3 ever outputs a light distribution is less than \u2211\u221e k=1 \u03b4/(2k\n2) \u2264 \u03b4. Thus, if a distribution is output, it is heavy with probability at least 1\u2212\u03b4. We now show that the expected number of samples taken before outputting a distribution is bounded.\nLet K be the random stage in which Algorithm 3 outputs a distribution and let k\u2217 be the smallest k \u2208 N that satisfies 2\u2212k \u2264 \u03b81 \u2212 \u03b80. By the guarantees of Theorem 5 and the independence of the stages k,\nP(K \u2265 k\u2217 + i) \u2264 \u2211\u221e \u2113=i( 1 5) \u2113 = (54)( 1 5 )\ni. Moreover, if Mk is the number of measurements taken at stage k, then by Wald\u2019s identity the expected number of measurements is bounded by\nE\n[ K\u2211\nk=1\nMk\n] = \u221e\u2211\nk=1\nE[Nk]P(K \u2265 k) \u2264 \u221e\u2211\nk=1\nc\u2032\u03b1 log(1/\u03b1) + c\u2032\u2032 log ( 2k2\n\u03b4\n)\n\u03b12\u22122k max{1, (54 )(15 )k\u2212k\u2217}\n\u2264 k\u2217\u2211\nk=1\nc\u2032\u2032\u2032 log ( 2k2\u2217 \u03b4 )\n\u03b1 4k + 5k\u2217 c\n\u2032\u2032\u2032 \u03b1\n\u221e\u2211\nk=k\u2217+1\n( 2 log(k) + log(2\u03b4 ) ) (45) k\n\u2264 c\u2032\u2032\u2032 log\n( 2k2\u2217 \u03b4 )\n\u03b1 4k\u2217+1 + 5k\u2217 c\n\u2032\u2032\u2032 \u03b1\n\u221e\u2211\nk=k\u2217+1\n( 2 log(k) + log(2\u03b4 ) ) (45) k \u2264 c \u2032\u2032\u2032\u2032 log\n( k\u2217 \u03b4 )\n\u03b1 (2k\u2217)2\nsince sup\u03b1 \u03b1 log(1/\u03b1) \u2264 e\u22121 and\n\u221e\u2211\nk=k\u2217\nlog(k)(45 ) k =\n2k\u2217\u22121\u2211\nk=k\u2217\nlog(k)(45 ) k +\n\u221e\u2211\nk=2k\u2217\nlog(k)(45 ) k/2(45 ) k/2\n\u2264 log(2k\u2217) 2k\u2217\u22121\u2211\nk=k\u2217\n(45) k +\n\u221e\u2211\nk=2k\u2217\n(45) k/2 \u2264 (log(2k\u2217) + 2)\n\u221e\u2211\nk=k\u2217\n(45 ) k = 5 log(2e2k\u2217)( 4 5 ) k\u2217\nsince supk log(k)( 4 5 ) k/2 \u2264 1. Noting that k\u2217 \u2264 log2( 1\u03b81\u2212\u03b80 ) + 1 completes the proof."}, {"heading": "B.4 Proof of Theorem 7", "text": "Proof. The proof of this result is nearly identical to that of Theorem 6 except the following changes. Let K be the random stage in which Algorithm 4 outputs a distribution and let k\u2217 be the smallest k \u2208 N that satisfies 2\u2212k \u2264 \u03b1. Moreover, if Mk is the number of measurements taken at stage k, then by Wald\u2019s identity expected number of measurements is bounded by\nE\n[ K\u2211\nk=1\nMk\n] = \u221e\u2211\nk=1\nE[Nk]P(K \u2265 k) \u2264 \u221e\u2211\nk=1\nc\u2032\u03b1 log(2k) + c\u2032\u2032 log ( 2k2\n\u03b4\n)\n2\u2212k\u01eb2 max{1, (54 )(15 )k\u2212k\u2217}\n\u2264 k\u2217\u2211\nk=1\nc\u2032\u2032\u2032 log ( 2k2\u2217 \u03b4 )\n\u01eb2 2k + 5k\u2217 c\n\u2032\u2032\u2032\n\u01eb2\n\u221e\u2211\nk=k\u2217+1\n( \u03b1k log(2) + 2 log(k) + log(2\u03b4 ) ) (25 ) k\n\u2264 c \u2032\u2032\u2032\u2032 (\u03b1k\u2217 + log ( k\u2217 \u03b4 ))\n\u01eb2 2k\u2217 \u2264 c \u2032\u2032\u2032\u2032\u2032 log (log(1/\u03b1)/\u03b4) \u03b1\u01eb2\nby the same series of steps as the proof of Theorem 6 and the fact that \u2211\u221e\nk=n ka k \u2264 nan (1\u2212a)2 for any a \u2208 (0, 1). The final inequality follows from k\u2217 \u2264 log2(1/\u03b1) + 1 and that \u03b1k\u2217 = \u03b1 log2(2/\u03b1) \u2264 2."}, {"heading": "B.5 Proof of Theorem 8", "text": "Proof. The proof is broken up into a few steps, summarized as follows. For any given \u03b10, \u01eb0, Theorem 5\ntakes just O ( \u03b1 log(1/\u03b10)+log(1/\u03b4)\n\u03b10\u01eb20\n) samples in expectation and the procedure makes an error (i.e. returns a\nlight distribution) with probability less than \u03b4. Define \u01eb = \u03b81 \u2212 \u03b80. In addition, if\u01eb = \u03b81 \u2212 \u03b80, \u03b1 \u2265 \u03b10, and \u01eb \u2265 \u01eb0 then with probability at least 4/5 a heavy distribution is returned after the same expected\nnumber of samples. We will leverage this result to show that if we are given an upper bound \u03b30 such that 1\u03b1\u01eb2 \u2264 \u03b30 then it is possible to identify a heavy distribution with probability at least 4/5 using just O (log2(\u03b30)\u03b30 [\u03b1 log2(\u03b30) + log(log2(\u03b30)/\u03b4)]) samples in expectation. Finally, we apply the \u201cdoubling trick\u201d to \u03b3 so that even though the tightest \u03b3 is not known a priori, we can adapt to it using only twice the number of samples as if we had known it. Because each of the stages is independent of one another, the probability that the procedure requires more than \u2113\u2217+ i stages is less than (1/5)i , which yields our expected sample complexity.\nFor all \u2113 \u2208 N define \u03b4\u2113 = \u03b42\u21133 and \u03b3\u2113 = 2\u2113. Fix some \u2113 and consider the set {(\u03b1, \u01eb) : 1\u03b1\u01eb2 = \u03b3\u2113}. Clearly, in this set, \u03b1 \u2208 [1/\u03b3\u2113, 1/2]. For all k \u2208 {0, . . . , \u2113\u2212 1}, define \u03b1k = 2 k\n\u03b3\u2113 and \u01ebk =\n\u221a 1\n2\u03b1k\u03b3\u2113 . The key\nobservation is that\n{(\u03b1, \u01eb) : 1 \u03b1\u01eb2\n\u2264 \u03b3\u2113} \u2286 log2 \u03b3\u2113\u22121\u22c3\nk=0\n{(\u03b1, \u01eb) : \u03b1 \u2265 \u03b1k, \u01eb \u2265 \u01ebk}. (7)\nTo see this, fix any (\u03b1\u2032, \u01eb\u2032) such that 1 \u03b1\u2032\u01eb\u20322 \u2264 \u03b3\u2113. Let k\u2217 be the integer that satisfies \u03b1k\u2217 \u2264 \u03b1\u2032 < 2\u03b1k\u2217 . Such a k\u2217 must exist since \u03b1\u2113\u22121 =\n1 2 \u2265 \u03b1\u2032 \u2265 1\u03b3\u2113\u01eb\u20322 \u2265 1 \u03b3\u2113 = \u03b10. Then \u03b3\u2113 \u2265 1\u03b1\u2032\u01eb\u20322 \u2265 12\u03b1k\u2217 \u01eb\u20322 which means \u01eb\u2032 \u2265 \u221a 1\n2\u03b1k\u2217\u03b3\u2113 = \u01ebk\u2217 which proves the claim of (7). Consequently, even if no information about \u03b1 or \u01eb\nindividually is known but 1 \u03b1\u01eb2 \u2264 \u03b3\u2113, one can cover the entire range of valid (\u03b1, \u01eb) with just log2(\u03b3\u2113) = \u2113 landmarks (\u03b1k, \u01ebk).\nFor any \u2113 \u2208 N and k \u2208 {0, . . . , \u2113 \u2212 1}, if Algorithm 2 is used with \u03b10 = \u03b1k, \u01eb0 = \u01ebk and \u03b4 = \u03b4\u2113 then the probability that a light distribution is returned, declared heavy is less than \u03b4\u2113. And the probability that a light distribution is returned, declared heavy for any \u2113 \u2208 N and k \u2208 {0, . . . , \u2113 \u2212 1} is less than\u2211\u221e\n\u2113=1 \u2113\u03b4\u2113 = \u03b4 \u2211\u221e \u2113=1 \u2113/(2\u2113 3) \u2264 \u03b4. Thus, given that Algorithm 5 terminates with a non-null distribution h, h is heavy with probability at least 1 \u2212 \u03b4. This proves correctness. We next bound the expected number of samples taken before the procedure terminates.\nWith the inputs given in the last paragraph for any k, \u2113, Algorithm 2 takes an expected number samples bounded by c\u03b3\u2113(\u03b1 log(1/\u03b1k)+log(1/\u03b4\u2113)). Let L \u2208 N be the random stage at which Algorithm 5 terminates with a non-null distribution h. Let \u2113\u2217 be the first integer such that there exists a k \u2208 {0, . . . , \u2113\u2217 \u2212 1} with \u03b1 \u2265 \u03b1k and \u01eb \u2265 \u01ebk (recall that in this case 1\u03b1k\u01eb2k \u2264 \u03b3\u2113\u2217). Then by the end of stage \u2113 \u2265 \u2113\u2217, at most c\u2113\u03b3\u2113(\u03b1 log(\u03b3\u2113) + log(1/\u03b4\u2113)) samples in expectation were taken on stage \u2113 and with probability at least 4/5 the procedure terminated with a heavy coin. By the independence of samples between rounds, observe that P(L \u2265 \u2113\u2217 + i) = \u2211\u221e j=i P(L = \u2113\u2217 + j) \u2264 (54 )(15 )i. Thus, if M\u2113 is the number of samples taken at stage \u2113\nthen by Wald\u2019s identify, the total expected number of samples taken before termination is bounded by\nE\n[ L\u2211\n\u2113=1\nc\u2113\u03b3\u2113(\u03b1 log(\u03b3\u2113) + log(1/\u03b4\u2113))\n] = \u221e\u2211\n\u2113=1\nE[M\u2113]P(L \u2265 \u2113) \u2264 \u221e\u2211\n\u2113=1\nc\u2113\u03b3\u2113(\u03b1 log(\u03b3\u2113) + log(1/\u03b4\u2113))P(L \u2265 \u2113)\n\u2264 \u2113\u2217\u2211\n\u2113=1\nc\u2113\u03b3\u2113(\u03b1 log(\u03b3\u2113) + log(1/\u03b4\u2113)) +\n\u221e\u2211\n\u2113=\u2113\u2217+1\nc\u2113\u03b3\u2113(\u03b1 log(\u03b3\u2113) + log(1/\u03b4\u2113))( 5 4 )( 1 5 )\n\u2113\u2212\u2113\u2217\n\u2264 \u2113\u2217\u2211\n\u2113=1\nc\u21132\u2113(\u03b1\u2113+ log(2\u21133/\u03b4)) +\n\u221e\u2211\n\u2113=\u2113\u2217+1\nc\u21132\u2113(\u03b1\u2113+ log(2\u21133/\u03b4))(54 )( 1 5 )\n\u2113\u2212\u2113\u2217\n\u2264 c\u2113\u2217(\u03b1\u2113\u2217 + log(2\u21133\u2217/\u03b4)) \u2113\u2217\u2211\n\u2113=1\n2\u2113 + c(54 )5 \u2113\u2217\n\u221e\u2211\n\u2113=\u2113\u2217+1\n( \u03b1\u21132(25) \u2113 + 3\u2113 log(\u2113)(25 ) \u2113 + log(2/\u03b4)\u2113(25 ) \u2113 )\n\u2264 2c\u2113\u22172\u2113\u2217(\u03b1\u2113\u2217 + log(2\u21133\u2217/\u03b4)) + c(54 )5 \u2113\u2217 ( 2\u03b1(\u2113\u2217 + 1) 2(25 ) \u2113\u2217 + 12 log(2e2\u2113\u2217)(\u2113\u2217 + 1)( 2 5 ) \u2113\u2217 + 4 log(2/\u03b4)(\u2113\u2217 + 1)( 2 5 ) \u2113\u2217 ) \u2264 c\u2032\u2113\u22172\u2113\u2217(\u03b1\u2113\u2217 + log(\u2113\u2217) + log(1/\u03b4))\nfor some absolute constant c\u2032 since \u2211\u221e\nk=n ka k \u2264 nan(1\u2212a)2 , \u2211\u221e k=n k\n2ak \u2264 n2an(1\u2212a)3 , and \u221e\u2211\n\u2113=\u2113\u2217+1\n\u2113 log(\u2113)(25 ) \u2113 \u2264 log(2\u2113\u2217)\n2\u2113\u2217\u2211\n\u2113\u2217+1\n\u2113(25) \u2113 +\n\u221e\u2211\n2\u2113\u2217+1\n\u2113(25) \u2113/2 ( log(\u2113)(25 ) \u2113/2 )\n\u2264 log(2e2\u2113\u2217) \u221e\u2211\n\u2113\u2217+1\n\u2113(25 ) \u2113 \u2264 4 log(2e2\u2113\u2217)(\u2113\u2217 + 1)(25 )\u2113\u2217\nsince maxx\u22651 log(x)(25 ) x/2 \u2264 1. Noting that \u2113\u2217 \u2264 log2( 1\u03b1\u01eb2 )+ 1, we have that the total number of samples, in expectation, is bounded by\nc\u2032\u2113\u22172 \u2113\u2217(\u03b1\u2113\u2217 + log(\u2113\u2217) + log(1/\u03b4)) \u2264 c\u2032\u2032\nlog2( 1 \u03b1\u01eb2 )\n\u03b1\u01eb2 (\u03b1 log2(\n1 \u03b1\u01eb2 ) + log(log2( 1 \u03b1\u01eb2 )) + log(1/\u03b4))\n\u2264 c\u2032\u2032\u2032 log2( 1 \u03b1\u01eb2 )\n\u03b1\u01eb2 (\u03b1 log2(\n1 \u01eb2 ) + log(log2( 1 \u03b1\u01eb2 )) + log(1/\u03b4))\nwhere we\u2019ve used the fact that sup\u03b1\u2208[0,1] \u03b1 log(1/\u03b1) \u2264 e\u22121."}, {"heading": "C Gaussians", "text": ""}, {"heading": "C.1 On the detection of a mixture of Gaussians", "text": "For known \u03c32, consider the hypothesis test of Problem 1 . In what follows, let \u03c72(\u03b81, \u03b80) and KL(\u03b81, \u03b80) be the chi-squared and KL divergences of the two distributions of H1. Note that for (\u03b81\u2212\u03b80)2\n\u03c3 \u2264 1, we have that \u03c72(\u03b81, \u03b80) = e (\u03b81\u2212\u03b80) 2\n\u03c32 \u2212 1 \u2264 2 (\u03b81\u2212\u03b80)2\u03c32 = 4KL(\u03b81, \u03b80) Theorem 2 says that for (\u03b81\u2212\u03b80) 2\n\u03c32 \u2264 1, a procedure that has maximum probability of error less than \u03b4\nrequires at least max {\n1\u2212\u03b4 \u03b1 , log(1/\u03b4) 4\u03b12KL(\u03b81,\u03b80)\n} samples to decide the above hypohesis test, even if \u03b1, \u03b80, \u03b81 are\nknown. The next subsection shows that if \u03b1, \u03b80, \u03b81 are unknown then one requires at least log(1/\u03b4)\n2[\u03b1KL(\u03b81,\u03b80)]2\nsamples to decide the above hypothesis test correctly with probability at least 1\u2212\u03b4. This is likely achievable using the method of moments (Hardt and Price, 2014)."}, {"heading": "C.2 Lower bounds", "text": "Theorem 9. For known \u03c32, consider the hypothesis test of Problem 1. If \u03b8\u2217 = (1 \u2212 \u03b1)\u03b80 + \u03b1\u03b81 and \u03b81\u2212\u03b80\n\u03c3 \u2264 1 then\n\u03c72((1\u2212 \u03b1)f\u03b80(x) + \u03b1f\u03b81(x)|f\u03b8\u2217(x)) \u2264 c\u2032 ( \u03b1(1\u2212 \u03b1)(\u03b81 \u2212 \u03b80) 2\n\u03c32\n)2\nfor some absolute constant c\u2032.\nProof. If f\u03b8 = N (\u03b8, \u03c32) then f\u03b8(x) = h(x) exp(\u03b7(\u03b8)x \u2212 b(\u03b8)) where h(x) = 1\u221a2\u03c0\u03c32 e \u2212 x2 2\u03c32 , \u03b7(\u03b8) = \u03b8 \u03c32 , and b(\u03b7(\u03b8)) = \u03b7(\u03b8) 2\u03c32\n2 = \u03b82 2\u03c32 . Thus,\n\u03b8\u2217 = \u03b7 \u22121((1\u2212 \u03b1)\u03b7(\u03b80) + \u03b1\u03b7(\u03b81) ) = (1\u2212 \u03b1)\u03b80 + \u03b1\u03b81\nand\nsup y\u2208[\u03b80,\u03b81] b(2\u03b7(y) \u2212 \u03b7(\u03b8\u2217))\u2212 (2b(\u03b7(y)) \u2212 b(\u03b7(\u03b8\u2217))) = sup y\u2208[\u03b80,\u03b81] (y \u2212 \u03b8\u2217)2 \u03c32 \u2264 (\u03b81 \u2212 \u03b80) 2 \u03c32 =: \u03ba\nand\nsup x\u2208[b\u0307(\u03b7(\u03b8\u2212)),b\u0307(\u03b7(\u03b8+))] fb\u0307\u22121(x)(x) = sup x\u2208[\u03b8\u2212,\u03b8+] sup \u03b8\u2208R 1\u221a 2\u03c0\u03c32 e\u2212 (x\u2212\u03b8)2 2\u03c32 \u2264 1\u221a 2\u03c0\u03c32 =: \u03b3.\nNote that for any \u03b8 < \u03b8\u2032 we have b\u0307(\u03b7(\u03b8\u2032)) \u2212 b\u0307(\u03b7(\u03b8)) = \u03b8\u2032 \u2212 \u03b8, M2(\u03b8) = \u03c32, and M4(\u03b8) = 3\u03c34. Plugging these values into the theorem we have\nc = e\u03ba (\nsup \u03b8\u2208[\u03b80,\u03b81]\nM2(\u03b8) 2 ( 2 + \u03b3 ( b\u0307(\u03b7(\u03b8+))\u2212 b\u0307(\u03b7(\u03b8\u2212)) ))\n+ 8M4(\u03b8\u2212) + 8M4(\u03b8+) + 16 ( b\u0307(\u03b7(\u03b8+))\u2212 b\u0307(\u03b7(\u03b8\u2212)) )4 + 25\u03b3 ( b\u0307(\u03b7(\u03b8+))\u2212 b\u0307(\u03b7(\u03b8\u2212))\n)5)\n=e (\u03b81\u2212\u03b80)\n2\n\u03c32 ( \u03c34 ( 2 +\n2(\u03b81 \u2212 \u03b80)\u221a 2\u03c0\u03c3 ) + 48\u03c34 + 256 (\u03b81 \u2212 \u03b80)4 + 645\u221a2\u03c0 (\u03b81 \u2212 \u03b80)5 \u03c3 )\nnoting that \u03b8+ \u2212 \u03b8\u2212 = 2(\u03b81 \u2212 \u03b80). If \u03b81\u2212\u03b80\u03c3 \u2264 1 then c = c\u2032\u03c34 for some absolute constant c\u2032 and (\u03b7(\u03b81)\u2212 \u03b7(\u03b80))2 = (\u03b81\u2212\u03b80) 2\n\u03c34 which yields the final result.\nC.3 Gaussian Upper bound for known \u03b1, \u03b80, \u03b81\nFor known \u03c32, consider the hypothesis test of Problem 1 with \u03b8 = \u03b80. We observe a sample X1, . . . ,Xn and are trying to establish whether it came from H0 or H1.\nConsider the test\n1\nn\nn\u2211\ni=1\n1Xi>\u03b81\nH1 \u2277 H0 P1(X1 > \u03b81) + P0(X1 > \u03b81) 2 =: \u03b3.\nIf \u01eb = P1(X1 > \u03b81)\u2212 P0(X1 > \u03b81) then\nP1\n( 1\nn\nn\u2211\ni=1\n1Xi>\u03b81 \u2264 \u03b3 ) = P1 ( 1\nn\nn\u2211\ni=1\n1Xi>\u03b81 \u2264 P1(X1 > \u03b81)\u2212 \u01eb/2 ) \u2264 e\u2212n\u01eb2/2\nand\nP0\n( 1\nn\nn\u2211\ni=1\n1Xi>\u03b81 \u2265 \u03b3 ) = P0 ( 1\nn\nn\u2211\ni=1\n1Xi>\u03b81 \u2265 P0(X1 > \u03b81) + \u01eb/2 ) \u2264 e\u2212n\u01eb2/2\nby sub-Gaussian tail bounds. If Q(x) = \u222b\u221e x 1\u221a 2\u03c0 e\u2212z 2/2dz and \u2206 = \u03b81\u2212\u03b80\u03c3 then\nP0(X1 > \u03b81) = Q (\u2206) P1(X1 > \u03b81) = (1\u2212 \u03b1)Q (\u2206) + \u03b1 1\n2\nso\n\u01eb = \u03b1\n( 1\n2 \u2212Q (\u2206)\n) = \u03b1 \u222b \u2206\n0\n1\u221a 2\u03c0 e\u2212x 2/2dx \u2265 min{ \u03b1\u2206 4 \u221a 2\u03c0 , 1 4 \u03b1}.\nThus, the test fails with probability at most\nexp [ \u2212n\u03b12min { (\u03b81 \u2212 \u03b80)2 64\u03c0\u03c32 , 1 32 }] .\nWe conclude that if \u2206 = \u03b81\u2212\u03b80\u03c3 \u2264 1 and n \u2265 (\u03b81\u2212\u03b80)2 log(1/\u03b4) 64\u03c0\u03b12\u03c32 = KL(P\u03b81 ,P\u03b80) log(1/\u03b4)\n64\u03c0\u03b12 the correct hypothesis is selected. The 1/\u03b1 sufficiency result holds for large enough \u2206 since one merely needs to observe just one sample since the probability of it coming from \u03b80 is negligible."}], "references": [], "referenceMentions": [], "year": 2016, "abstractText": "This paper studies the trade-off between two different kinds of pure exploration: breadth versus depth. The most biased coin problem asks how many total coin flips are required to identify a \u201cheavy\u201d coin from an infinite bag containing both \u201cheavy\u201d coins with mean \u03b81 \u2208 (0, 1), and \u201clight\u201d coins with mean \u03b80 \u2208 (0, \u03b81), where heavy coins are drawn from the bag with probability \u03b1 \u2208 (0, 1/2). The key difficulty of this problem lies in distinguishing whether the two kinds of coins have very similar means, or whether heavy coins are just extremely rare. This problem has applications in crowdsourcing, anomaly detection, and radio spectrum search. Chandrasekaran and Karp (2014) recently introduced a solution to this problem but it required perfect knowledge of \u03b80, \u03b81, \u03b1. In contrast, we derive algorithms that are adaptive to partial or absent knowledge of the problem parameters. Moreover, our techniques generalize beyond coins to more general instances of infinitely many armed bandit problems. We also prove lower bounds that show our algorithm\u2019s upper bounds are tight up to log factors, and on the way characterize the sample complexity of differentiating between a single parametric distribution and a mixture of two such distributions. As a result, these bounds have surprising implications both for solutions to the most biased coin problem and for anomaly detection when only partial information about the parameters is known.", "creator": "LaTeX with hyperref package"}}}