{"id": "1603.08789", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "29-Mar-2016", "title": "Using Enthymemes to Fill the Gap between Logical Argumentation and Revision of Abstract Argumentation Frameworks", "abstract": "In this paper, we present a preliminary work on an approach to fill the gap between logic-based argumentation and the numerous approaches to tackle the dynamics of abstract argumentation frameworks. Our idea is that, even when arguments and attacks are defined by means of a logical belief base, there may be some uncertainty about how accurate is the content of an argument, and so the presence (or absence) of attacks concerning it. We use enthymemes to illustrate this notion of uncertainty of arguments and attacks. Indeed, as argued in the literature, real arguments are often enthymemes instead of completely specified deductive arguments. This means that some parts of the pair (support, claim) may be missing because they are supposed to belong to some \"common knowledge\", and then should be deduced by the agent which receives the enthymeme. But the perception that agents have of the common knowledge may be wrong, and then a first agent may state an enthymeme that her opponent is not able to decode in an accurate way. It is likely that the decoding of the enthymeme by the agent leads to mistaken attacks between this new argument and the existing ones. In this case, the agent can receive some information about attacks or arguments acceptance statuses which disagree with her argumentation framework. We exemplify a way to incorporate this new piece of information by means of existing works on the dynamics of abstract argumentation frameworks.\n\nAcknowledgements\nThe author's main contribution is the discovery of some of the key concepts for the formulation of argumentation frameworks. These include, for example, the following principles:\n1. Argumentation frameworks are more often presented in informal and in formal contexts. They can be written by a programmer to be used with a formal argumentation framework and be used in formal contexts. In particular, if an argument or argument is explicitly stated explicitly, its contents may be interpreted in terms of arguments, but the use of these concepts may be restricted to the specific context. In this way, the semantics of argumentation frameworks can be applied as well. Thus, arguments in formal contexts may be interpreted in terms of arguments (in some cases, such as the following):\n1. A common-knowledge definition of argumentation frameworks is that argumentation frameworks have the same definition, or that an argument is not given, but that the same argument, and therefore the same argument, and therefore the same argument, and therefore the same argument, and therefore the same argument, and therefore the same argument, and therefore the", "histories": [["v1", "Tue, 29 Mar 2016 14:29:00 GMT  (24kb,D)", "http://arxiv.org/abs/1603.08789v1", null]], "reviews": [], "SUBJECTS": "cs.AI", "authors": ["jean-guy mailly"], "accepted": false, "id": "1603.08789"}, "pdf": {"name": "1603.08789.pdf", "metadata": {"source": "CRF", "title": "Using Enthymemes to Fill the Gap between Logical Argumentation and Revision of Abstract Argumentation Frameworks", "authors": ["Jean-Guy Mailly"], "emails": ["jmailly@dbai.tuwien.ac.at"], "sections": [{"heading": "Introduction", "text": "Argumentation frameworks (AFs) are a convenient way to represent conflicting information and to deduce which subset of the information can be inferred. For instance, they can be used to model dialogs between several agents (Amgoud and Hameurlain 2006) or to analyze on-line discussion between social network users (Leite and Martins 2011). Argumentation can also be useful in a mono-agent setting, for instance to infer non-trivial conclusions from an inconsistent knowledge base (Besnard and Hunter 2001).\nThe domain called dynamics of argumentation has become a hot topic in recent years, with numerous publications about it. The first ones consider really\nCopyright c\u00a9 2016, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved.\nclassical debate scenarios as the source of the dynamic process (Boella, Kaci, and van der Torre 2009a; Boella, Kaci, and van der Torre 2009b; Cayrol, de Saint-Cyr, and Lagasquie-Schiex 2010; Baumann and Brewka 2010; Bisquert et al. 2011; Bisquert et al. 2013; Baumann 2012; Booth et al. 2013). These approaches are perfectly wellsuited for classical exchange of arguments between agents. Then, some approaches have proposed to consider new scenarios, closer to what happens with belief change in logical settings (Alchourro\u0301n, Ga\u0308rdenfors, and Makinson 1985; Katsuno and Mendelzon 1991; Katsuno and Mendelzon 1992): these approaches propose to question the existing relation between arguments, and to modify this relation if it is required (Doutre, Herzig, and Perrussel 2014; Nouioua and Wu\u0308rbel 2014; Coste-Marquis et al. 2014a; Coste-Marquis et al. 2014b; Coste-Marquis et al. 2015; Baumann and Brewka 2015; Diller et al. 2015).\nThese works directly deal with the structure of the abstract AFs. An interesting question is \u201cWhat does AF revision mean when we consider logic-based AFs?\u201d. Indeed, it is not obvious that attacks between arguments can be changed, since they stem from the logical inference relation; for instance, if arguments a and b attack each other because their claims are the negation of each other (rebuttal attack), then it is not accurate to consider that the attack between a and b could be removed. But this is only the case when we consider completely specified deductive arguments (Besnard and Hunter 2001). As argued in the literature (Hunter 2007), the arguments which are used in real situations are often enthymemes, which are partially specified arguments: some parts of the support or some parts of the claim are not described, because it is supposed that they belong to some \u201ccommon knowledge\u201d. There may be different reasons for an agent not to share some part of her knowledge, such as some cost on the communication process. Then, the agent who receives an enthymeme must decide how to complete the content of the enthymeme to be able to use it. But if the missing formulae to complete the enthymeme are not part of the agent\u2019s beliefs (or at least, are not considered by the agent as the most accurate way to complete the enthymeme), then she will use a badly completed enthymeme in her argumentation framework. We see with this situation that, even with an underlying ar X iv :1 60 3. 08 78\n9v 1\n[ cs\n.A I]\n2 9\nM ar\n2 01\n6\nlogical belief base, the nature of arguments and attacks is not absolute; it depends on the agent\u2019s beliefs and on her way to complete enthymemes.\nSo we propose to consider the use of enthymemes in the argumentation process to explain the questionability of some attacks. We illustrate the possibility that a logic-based argumentation framework contains mistaken attacks. Then we show that the existing work on the dynamics of AFs can be used on such enthymeme-based AFs, as soon as a distinction between classical deductive arguments and enthymemes is done in the abstract AF, and that this distinction is used in the revision process.\nThe paper is organized as follows. The first section presents the background notions required to the understanding of the paper. In particular, we describe briefly belief revision, abstract argumentation and revision of AFs, and logicbased argumentation. Then in the second section, we focus on enthymemes in logic-based AFs; we explain how using enthymemes can be a source of mistaken attacks in the resulting AF. The following section illustrates the revision process on logic-based AFs which contain enthymemes. After the description of a basic approach in which each attack concerning an enthymeme is questionable, we propose a refinement of this approach based on the notion of fixed part of an enthymeme. Finally, the last section concludes the paper and sketches some interesting future work."}, {"heading": "Background", "text": ""}, {"heading": "Belief Revision", "text": "Belief revision is well-known when an agent\u2019s beliefs are represented in a logical setting. The intuitive idea is \u201cHow can an agent incorporate a new piece of information into her beliefs?\u201d, which is not a trivial question when the agent\u2019s previous beliefs and the new piece of information are conflicting. One of the most influencial works on this topic is the AGM framework (Alchourro\u0301n, Ga\u0308rdenfors, and Makinson 1985), which gives rationality postulates for belief change operators, when the beliefs are represented as deductively closed sets of formulae. Here we are interested in the adaptation of AGM revision to finite propositional logic by Katsuno and Mendelzon (1991). They explain that revising a formula \u03d5 by a formula \u03b1 is equivalent to selecting some models of \u03b1 which are minimal w.r.t. some plausibility relation. This relation has to satisfy some properties.\nDefinition 1 (Katsuno and Mendelzon 1991). A faithful assignment is a mapping from a formula \u03d5 to a total pre-order between interpretations \u2264\u03d5 such that:\n1. if I |= \u03d5 and I \u2032 |= \u03d5, then I '\u03d5 I \u2032; 2. if I |= \u03d5 and I \u2032 6|= \u03d5, then I <\u03d5 I \u2032; 3. if \u03d5 \u2261 \u03d5\u2032, then \u2264\u03d5=\u2264\u03d5\u2032 . Then, a KM revision operator \u25e6 is a mapping from two formulae \u03d5, \u03b1 to a new formula such that\nmod (\u03d5 \u25e6 \u03b1) = min( mod (\u03b1),\u2264\u03d5)\nFor instance, the Dalal revision operator can be defined through the pre-order built on the Hamming distance. Definition 2 (Hamming 1950; Dalal 1988). The Hamming distance between two propositional interpretations I, I \u2032 is the number of assignments which differ between I and I \u2032, formally: dH(I, I \u2032) = |(I\\I \u2032) \u222a (I \u2032\\I)|. The total pre-order \u2264dH\u03d5 is defined by\nI \u2264dH\u03d5 I \u2032 iff min J\u2208 mod (\u03d5) (dH(I, J)) \u2264 min J\u2208 mod (\u03d5) (dH(I \u2032, J))\nThe Dalal revision operator \u25e6D is a mapping from two formulae \u03d5, \u03b1 to a new formula such that\nmod (\u03d5 \u25e6D \u03b1) = min( mod (\u03b1),\u2264dH\u03d5 )\nLet us illustrate the behavior of the Dalal revision operator. Example 1. Consider V = {a, b, c, d} and \u03d5 = [(a \u2227 b) \u2228 (\u00aca \u2227 c) \u2228 \u00ac(b \u2228 (a \u2227 c))] \u2227 \u00acd. The models of \u03d5 are {{a}, {c}, {a, b}, {b, c}, {a, b, c}}. We revise \u03d5 by \u03b1 = a \u2227 \u00acb \u2227 c. The models of \u03b1 are {{a, c}, {a, c, d}}. Table 1 gives the Hamming distance between models of \u03d5 and models of \u03b1.\nSince the minimal Hamming distance between {a, c} and a model of \u03d5 is 1 (dH({a, c}, {a}) for instance), while the distance between {a, c, d} and any model of \u03d5 is at least 2, then {a, c} <dH\u03d5 {a, c, d}, and so mod (\u03d5 \u25e6Da \u03b1) = {{a, c}}."}, {"heading": "Abstract Argumentation and AF Revision", "text": "An abstract AF is a directed graph which represents the arguments and the attacks between them. The usual problem to solve with such an abstract AF is \u201cHow to determine which arguments are accepted?\u201d. This question is tackled in the seminal paper by Dung (1995). Definition 3 (Dung 1995). An argumentation framework (AF) is a pair F = \u3008A,R\u3009 where A is a set of abstract entities called arguments, and R \u2286 A \u00d7 A is the attack relation which represents the conflicts between arguments. Given a semantics \u03c3, the \u03c3-extensions of F , denoted \u03c3(F ), are subsets of A which can be accepted. An argument is then skeptically accepted by F w.r.t. \u03c3 iff it belongs to each \u03c3-extension of F .\nIn this paper, we illustrate our approach on the stable semantics: S \u2286 A is a stable extension of F (denoted by S \u2208 st(F )) iff\n\u2022 6 \u2203x, y \u2208 S s.t. (x, y) \u2208 R; \u2022 \u2200y \u2208 A\\S, \u2203x \u2208 S s.t. (x, y) \u2208 R. Example 2. Given the set of arguments A = {x, y, z, t, u}, the AF F1 = \u3008A,R\u3009 with R = {(x, y), (x, t), (y, x), (y, z), (z, u), (t, u)} is given in Figure 1.\nIts stable extensions are st(F1) = {{x, z}, {y, t}}. As explained in the introduction, the question of change in AFs has been tackled by several approaches. Here we use the translation-based revision from (Coste-Marquis et al. 2014b). The idea of this method is to translate the AF and the semantics into a propositional formula, to use a KM revision operator to perform the expected change, and then to decode the models of the revised formula to obtain a set of revised AFs. The propositional encoding is a generalization of a result from Besnard and Doutre (2004). They have defined a formula \u039e, built on propositional variables corresponding to the arguments, such that the set of models of \u039e exactly correspond to the set of stable extensions of an AF. Coste-Marquis et al. (2014b) generalize this encoding with the addition of two other kinds of variables V = {attx,y | x, y \u2208 A} \u222a {accx | x \u2208 A}. attx,y means that there is an attack from the argument x to the argument y, and accx means that the argument x is skeptically accepted. Definition 4 (Coste-Marquis et al. 2014b). Given an AF F = \u3008A = {x1, . . . , xn}, R\u3009, the stable encoding of F is\nfst(F ) = ( \u2227\n(x,y)\u2208R\nattx,y) \u2227 ( \u2227\n(x,y)/\u2208R\n\u00acattx,y) \u2227 thst(A)\nwhere thst(A) = \u2227 x\u2208A[accx \u21d4 \u2200x1, . . . ,\u2200xn,\n( \u2227 y\u2208A(y \u21d4 \u2227 z\u2208A(attz,y \u21d2 \u00acz))\u21d2 x)]\nIn general, f\u03c3(F ) can be defined for any semantics \u03c3 as soon as the formula \u039e exists; for semantics with a complexity higher than NP, we can consider for instance QBF encodings to define \u039e.\nThen, the revision operator is defined as follow: Definition 5 (Coste-Marquis et al. 2014b). Given \u25e6 a KM revision operator and \u03d5 a propositional formula built from the set of variables V , the translation-based revision operator ?\u25e6 is defined as\nF ?\u25e6 \u03d5 = dec(f\u03c3(F ) \u25e6 (\u03d5 \u2227 th\u03c3(A)))\nwith dec a mapping from a formula \u03c8 to a set of AFs F such that each AF F \u2032 \u2208 F corresponds to one of the models \u03c9 of \u03c8: (x, y) appears in F \u2032 iff attx,y is true in \u03c9.\nThis general definition allows to change any attack and argument status as long as it is compatible with \u03c3. If additional constraints should be satisfied,1 the use of a constrained version is possible: Definition 6 (Coste-Marquis et al. 2014b). Given \u25e6 a KM revision operator and \u03d5, \u00b5 two propositional formulae built from the set of variables V , the constrained translationbased revision operator ?\u00b5\u25e6 is defined as\nF ?\u00b5\u25e6 \u03d5 = dec(f\u03c3(F ) \u25e6 (\u03d5 \u2227 th\u03c3(A) \u2227 \u00b5))\nTo conclude, let us mention a particular revision operator proposed by (Coste-Marquis et al. 2014b): we call ?att (resp. ?\u00b5att) the translation-based (resp. constrained translationbased) revision operator which gives priority to the minimal change of the attack relation. This operator is similar to the Dalal-based revision revision operator ?\u25e6D , but it uses a weighted version of the Hamming distance such that changing the value of a single attx,y variable is more expensive than changing the value of each accx variable. Example 3. We consider the AF F1 given in Figure 1. We suppose the existence of an integrity constraint attt,u \u2227 attz,u, which means that the attacks from t and z to u must not be removed. The result of the revision F1 ? \u00b5 \u25e6D accu is the AF F2 described in Figure 2. Now the extensions are\nst(F2) = {{x, u}, {y, u}}, so u is skeptically accepted. We focus on this kind of revision operators because (Coste-Marquis et al. 2014b) already proposes a way to incorporate a constraint on the attack relation, which is required by our approach. Other revision or update operators could be used instead, but we should adapt their definition to take into account the constraint."}, {"heading": "Logic-based Arguments: Deductive Arguments", "text": "The question of the exact nature of arguments and attacks is tackled by several approaches which can be gathered under the name structural argumentation. Here we focus on one of the most prominent ones: deductive argumentation (Besnard and Hunter 2001). Definition 7 (Besnard and Hunter 2001). A deductive argument built from a belief base \u2206 is a pair \u3008\u03a6, \u03b1\u3009, where \u03a6 is called the support and \u03b1 the claim, such that:\n1. \u03a6 \u2286 \u2206, 2. \u03a6 6` \u22a5, 3. \u03a6 ` \u03b1,\n1Such as external constraint depending on the particular application, or some rules of the world.\n4. \u03a6 is minimal with respect to\u2286 among the sets of formulae which satisfy items 1. 2. and 3. There is an intuitive explanation to this definition. First\nthe agent is supposed to use her beliefs to justify her claim, which explains the first condition. The second and third conditions guarantee that the claim is actually supported by the beliefs of the agent, but not by conflicting beliefs (for instance, the sentence \u201cIt is raining and it is not raining, so I am the Queen of England.\u201d is not an argument at all). Finally, the last condition ensures that there is no useless piece of information in the support: \u201cIt is raining, when it is raining I should use an umbrella, and I love chocolate. So I will use my umbrella.\u201d is not accurate either.\nThe conflicts between deductive arguments may have different natures. The most general sort of conflict is defined as follow: Definition 8 (Besnard and Hunter 2001). A defeater for an argument \u3008\u03a6, \u03b1\u3009 is an argument \u3008\u03a6\u2032, \u03b1\u2032\u3009 such that \u03b1\u2032 ` \u00ac(\u03d51 \u2227 \u00b7 \u00b7 \u00b7 \u2227 \u03d5n), for some {\u03d51, . . . , \u03d5n} \u2286 \u03a6.\nIt is possible to use deductive arguments to build an argument tree with the arguments and counterarguments which attack and defend a given claim. We can also build a full argumentation framework from the set of deductive arguments generated from a belief base. Definition 9 (Besnard and Hunter 2014). Given A the set of deductive arguments generated from the belief base \u2206, the exhaustive graph associated with \u2206 is the AF F = \u3008A,R\u3009 with R = {(x, y) \u2208 A\u00d7A | x is a defeater for y}.\nHere we focus on the defeater relation, which is the most general one, but exhaustive graphs can be generated with another attack relation which guarantees additional properties for the defeaters (undercut, rebuttal, and so on). Moreover, these graphs may be infinite in general; Besnard and Hunter propose an approach to circumvent this problem. See (Besnard and Hunter 2014) for more details."}, {"heading": "Enthymemes and their Role in Mistaken Attacks", "text": ""}, {"heading": "Intuitive Explanation", "text": "Before formalizing our approach, we want to explain intuitively, with natural language arguments, why agents can disagree on the attack relation, and more generally why attacks could be questionable. Let us consider the following arguments: (c) The US army is preparing a secret plan to retreat from\nAfghanistan (source: Wikileaks). (b) Our informed sources say that the Wikileaks documents\nare fake (source: NY Times). (a) The media cannot be trusted on military issues (source:\nN. Chomsky). Now we consider three agents A1, A2, A3; each of them may have some personal beliefs which are not shared with the other agents.\n\u2022 A1 thinks that Chomsky is the most credible source, and considers that Wikileaks is a media more reliable than NY Times. So her AF is the one given in Fig. 3a.\n\u2022 A2 thinks that Chomsky is a more credible source than NY Times, and NY Times is a more credible source than Wikileaks. She also believes that Wikileaks cannot be seen as a media. So her AF is the one given in Fig. 3b.\n\u2022 Finally, A3 thinks that NY Times is the most credible source, and that Chomsky is not reliable on this topic. So her AF is the one given in Fig. 3c.\nThese personal AFs may depend on many different parameters (additional information which is not available to each agent, preferences, context, previous experience of each agent, and so on).\nOf course, under the assumption that the agents share all their knowledge and beliefs, the personal beliefs of the agents can be represented as additional arguments and we obtain a single AF representing the whole information about a topic. But we think that this assumption is too strong for at least three reasons. First, there may be technical issues with this information sharing; for instance, there may be some cost on communication between agents, or the global amount of information in the network may be too important to be stored in a centralized way. Then, for strategical reasons, agents may choose not to share their knowledge and beliefs. Also, if argument are mined from natural language (for instance, for an analysis of social networks debates), there are likely some implicit pieces of information used in the argumentation process. This explains why some attacks may be questionable.\nFor instance, if the agents A1 and A3 consider that agent A2 is trustworthy, then they could have to change the attack relation in their own AFs if they receive from agent A2 the information \u201cc should be accepted\u201d. On the opposite, if agents vote to determine the arguments statuses, there will be a majority of agents (A1 and A3) voting against c (meaning that c is rejected in their AFs), so agent A2 should modify the attack relation to incorporate this piece of information in her AF.\nEnthymemes with partial support Now let us formalize this notion of \u201carguments with partial knowledge\u201d, and their role in the existence of mistaken attacks. Hunter (2007) defines what he calls approximate arguments, which are pairs \u3008\u03a6, \u03b1\u3009 which do not satisfy the four conditions of deductive\narguments. He classifies them depending on which properties they satisfy, and then focuses on enthymemes. An enthymeme is a pair \u3008\u03a6, \u03b1\u3009 such that \u03a6 6` \u03b1, but there is a set \u03a8 \u2286 \u2206 such that \u3008\u03a6 \u222a \u03a8, \u03b1\u3009 is a deductive argument. Intuitively, \u03a8 represents some \u201ccommon knowledge\u201d that the agent supposes to be known by her opponents. Then it is not useful for the agent to state the full deductive argument to be able to exchange information and to reach her goal (persuading her opponent, helping to take a decision, negotiating, and so on).\nExample 4. To illustrate this concept, we borrow a simple example of real life use of enthymemes from (Hunter 2007). Let us consider John and his wife Yoko, who is going outside without an umbrella. If John tells her \u201cYou should take your umbrella, because the weather report predicts rain\u201d, there is no formal reason to consider that \u3008\u03a6, \u03b1\u3009 (with \u03a6 = {rain predicted} and \u03b1 = take umbrella) is an argument. It is in fact an enthymeme, because John supposes that \u03a8 = {rain predicted \u21d2 take umbrella} is part of the knowledge he shares with Yoko.\nOne of the questions tackled in (Hunter 2007) is \u201cHow does the agent knows that \u03a8 is actually part of the common knowledge?\u201d. Hunter supposes that each agent has a way to evaluate the plausibility that a given formula will be part of the knowledge shared between her and another agent.\nDefinition 10 (Hunter 2007). For each agent Ai whose beliefs are expressed in the propositional language L, \u2022 \u2206i \u2286 L denotes her own personal base, \u2022 and for each other agent Aj , \u00b5i,j is a mapping from the\nlanguage L to [0, 1], such that \u00b5i,j(\u03b1) represents the certainty that \u03b1 is common to both agents Ai and Aj .\nOn enthymemes and mistaken attacks This mapping \u00b5i,j is used by the agent to build her arguments and decide whether they should be fully specified deductive arguments, or whether enthymemes can be used. The idea is simply to keep only the formulae \u03d5 in the support such that the associated value \u00b5i,j(\u03d5) is less than a given threshold \u03c4 ; the other ones can be omited because they are supposed to be known by agent Aj .\nThis process may lead to some problems in the exchange of arguments. There are at least two sources of mistakes.\n1. The mapping \u00b5i,j describes the perception that agent Ai has of her common knowledge with Aj . If this perception is wrong, then there could be some exchange of enthymemes that the agent Aj cannot decode accurately.\n2. Even with a good evaluation of the common knowledge by \u00b5i,j , the choice of a bad threshold could also lead to enthymemes that the other agent cannot decode.\nIn both these situations, agent Aj receives some \u201cargument\u201d a = \u3008\u03a6, \u03b1\u3009 which is not fully specified, and then she has to complete the support with some \u03a8\u2032 from her own belief base, which could of course lead to the addition of some attacks from an existing argument b to this new argument a, for instance if the claim of b is the formula \u00ac\u03c8, for some \u03c8 \u2208 \u03a8\u2032. Even if for low-level treatments, it can be\nrepresented as a\u2032 = \u3008\u03a6 \u222a \u03a8\u2032, \u03b1\u3009 (for instance to determine if possibly new incoming arguments attack it), at a higher level it is still the argument a which is used. Indeed, this a\u2032 is not an argument that Aj has built by herself, since some of the premises are not part of her belief base.2 Then, agent Aj can receive a new piece of information about the argument a which is incompatible with the attack from b to a (the simplest example being \u201ca and b should be accepted together\u201d). So she has to build a new internal state a\u2032\u2032 from a subset \u03a8\u2032\u2032 from her belief base; for the same reason as previously, at a higher level it is still the argument a originally built from agent Ai\u2019s beliefs.\nEnthymemes with partial claim We have seen that enthymemes are a way to communicate arguments with partial support. Black and Hunter (2012) also give some examples of enthymemes with a partial claim. Borrowing their example, let us consider the sentence \u03b1 = \u201cJohn has bought The Times\u201d. The enthymeme \u3008{\u03b1},>\u3009 can be interpreted in at least two ways, which lead to different claims:\n1. \u3008{\u03b1, \u03b1 \u21d2 \u03b2}, \u03b2\u3009 with \u03b2 = \u201cJohn has bought a copy of the newspaper The Times\u201d;\n2. \u3008{\u03b1, \u03b1\u21d2 \u03b3}, \u03b3\u3009 with \u03b3 = \u201cJohn has bought the company which publishes the newspaper The Times\u201d.\nSimilarly to what we have described for enthymemes with a partial support, if an agent receives an argument a which is in fact an enthymeme with a partial claim, some mistakes in the attack relation can appear. For instance, she may consider that a attacks an argument b because some part of b\u2019s support is conflicting with the completed claim (either \u03b2 or \u03b3 in our example). If she later receives a piece of information which is not compatible with this attack, then she may have to consider a removal of this attack (for instance, because the chosen claim is not accurate).\nSo we can formally define the class of enthymemes (with partial claims and partial supports) as follows:\nDefinition 11 (Black and Hunter 2012). Given d = \u3008\u03a6, \u03b1\u3009 a deductive argument, an approximate argument \u3008\u03a6\u2032, \u03b1\u2032\u3009 is an enthymeme for d iff \u03a6\u2032 \u2282 \u03a6 and \u03b1 ` \u03b1\u2032.\nStated otherwise, the pair \u3008\u03a6, \u03b1\u3009 is an enthymeme for \u3008\u03a6 \u222a \u03a8, \u03b1 \u2227 \u03b2\u3009, with \u03a6 the partial support and \u03b1 the partial claim. In the rest of this paper, we call such a pair \u3008\u03a6, \u03b1\u3009 a non-completed enthymeme and \u3008\u03a6\u222a\u03a8, \u03b1\u2227\u03b2\u3009 a completed enthymeme. A completed enthymeme may not satisfy the conditions stated in Definition 7, since the set of formulae \u03a6 comes from another agent\u2019s belief base. Moreover, contrary to a fully specified argument stemming from the agent\u2019s beliefs, a completed enthymeme can be questioned.\n2To do this, it is a logical belief revision/expansion/update which should be performed, and this would likely have some side effects on the whole belief base, not only on the formulae involed in argument a."}, {"heading": "Dynamics of AFs and Enthymemes", "text": ""}, {"heading": "Building a Dung\u2019s AF from Enthymemes", "text": "For several reasons, the use of an abstract AF by the agent is interesting, even when she uses an underlying belief base. For instance, the developement of efficient approaches to solve abstract argumentation problems permits to obtain the conclusion of the agent\u2019s AF with respect to several semantics and inference policies (see for instance the competition of argumentation solvers (Thimm and Villata 2015)). But to avoid the loss of information about the nature of arguments and attacks, we propose to refine the definition of the AF. Definition 12. Given D and E which denote respectively the agent\u2019s deductive arguments and enthymemes, the agent\u2019s enthymeme-based AF is F (D,E) = \u3008A,R\u3009 with \u2022 A = D \u222a E; \u2022 R = RD \u222aRE ; \u2022 RD \u2286 D\u00d7D the set of certain attacks (between deductive\narguments); \u2022 RE \u2286 (A \u00d7 A)\\(D \u00d7D) the set of questionable attacks\n(concerning at least one enthymeme). Computing the extensions of such an AF is identical to the process for classical Dung\u2019s AFs; differentiating both kinds of attacks is useful only for the dynamics scenarios such as revision.\nIn this setting, each attack can be added or removed as soon as it concerns at least one enthymeme. We will refine this later. Example 5. Let F3 be the enthymeme-based AF presented in Fig. 4. Arguments with rounded corners are the enthymemes while the other ones are deductive arguments. Similarly, the dashed arrows represent the questionable attacks, while the other ones are the certain attacks. In this example, we suppose that the agent has received the enthymemes in the following way: \u2022 e1 = \u3008{\u03b1}, \u03b3\u3009, which has been completed by \u03a81 = {\u03b1\u21d2 \u03b2, \u03b2 \u21d2 \u03b3};\n\u2022 e2 = \u3008{\u03b7},>\u3009, which has been completed by \u03a82 = {\u03b7 \u21d2 \u00ac } in the support and \u00ac in the claim.\nWith this AF, the accepted arguments are {e2, d1}."}, {"heading": "Applying Dynamics of Abstract AFs to Enthymeme-based AFs", "text": "The existence of mistaken attacks in an enthymeme-based AF can be tackled through some approaches of the dynamics of abstract argumentation (Bisquert et al. 2013; Doutre, Herzig, and Perrussel 2014; Coste-Marquis et al. 2014a; Coste-Marquis et al. 2014b; Coste-Marquis et al. 2015). In the case when some arguments and the relations between them are certain (in particular, when they are fully specified arguments instead of enthymemes), integrity constraints can simply be added to these revision/update/enforcement operators to ensure that forbidden attacks will not be added, and mandatory attacks will not be removed. Since it is already defined by (Coste-Marquis et al. 2014b), we will exemplify the dynamics of argumentation with their constrained revision approach, presented previously. We can encode an integrity constraint to fix the attacks and non-attacks concerning the deductive arguments into the setting from (CosteMarquis et al. 2014b). Definition 13. Given F (D,E) an enthymeme-based AF, the integrity constraint on deductive arguments is\n\u00b5D = ( \u2227\n(x,y)\u2208RD\nattx,y) \u2227 ( \u2227\n(x,y)\u2208(D\u00d7D)\\RD\n\u00acattx,y)\nNow, if the agent receives some piece of information about the arguments statuses or the attack relation, then she can use the AF revision operator ?\u00b5Datt as defined previously in the case when this new piece of information disagrees with the current AF. This revision operator guarantees that the relations between deductive arguments will not be modified during the revision process, which is desirable since they are directly stemming from the logical inference relation. Example 5 Continued. We continue the previous example. The agent receives the piece of information \u201ce1 should be accepted\u201d, which corresponds to the formula acce1 . The integrity constraint is attd2,d1 \u2227 \u00acattd1,d2 , which ensures that the attacks between the deductive arguments d1 and d2 will not be modified. The possible results are given in Fig. 5.\nThe exact change operator which should be used depends on the the properties expected for the process, for instance it is well-known that performing an update (Bisquert et al. 2013; Doutre, Herzig, and Perrussel 2014) is accurate when the change is explained by an evolution of the world, while performing a revision (Coste-Marquis et al. 2014a; Coste-Marquis et al. 2014b) is accurate when the evolution only concerns the agent\u2019s beliefs about the world; thus\nthese operations do not satisfy the same properties. Similarly, among the different approaches in the state of the art about the dynamics of AFs, each of them do not have the same expressivity. For instance, the revision approach described in (Coste-Marquis et al. 2014a) permits to revise by a formula concerning the extensions, while the translationbased approach illustrated here permits to revise by a formula concerning skeptical acceptance of arguments and attacks at the same time. So, the choice of a change operator completely depends on the application and the agent\u2019s needs and preferences. In the following, we continue to consider revision to be consistent with the previous example, but update and extension enforcement (Baumann and Brewka 2010) could be considered as well."}, {"heading": "From Revised AFs to new Completed Enthymemes", "text": "After obtaining the result of the revision process, the agent should now decode this result to determine which of the revised AFs is the most plausible real AF corresponding to her beliefs, and which enthymemes should be internally modified (and how they should be internally modified) to ensure that the abstract AF and the logic-based AF coincide. Definition 14. Let F be the set of AFs obtained from the revision process. For each F \u2032 \u2208 F , F \u2032 is called an acceptable AF iff for each attack which differs between the original AF F and F \u2032, the agent\u2019s belief base contains some formulae which allow to complete the enthymemes s.t. this new completion is consistent with the attacks in F \u2032. Example 5 Continued. Continuing the previous example, let us suppose that the agent\u2019s belief base contains the formulae \u03a8\u2032 = {\u03b1 \u21d2 \u03b8, \u03b8 \u21d2 \u03b3}. Then the enthymeme e1 can be completed into \u3008{\u03b1, \u03b1\u21d2 \u03b8, \u03b8 \u21d2 \u03b3}, \u03b3\u3009, which leads to the acceptable AF F4 given in Fig. 6a. Similarly, if the agent\u2019s belief base contains the formulae \u03a8\u2032\u2032 = {\u03b7 \u21d2 \u03b9}, then the agent can consider the acceptable AF F5 given in Fig. 6b, since e2 can be completed into \u3008{\u03b7, \u03b7 \u21d2 \u03b9}, \u03b9\u3009.\nWhen the set of acceptable AFs is not a singleton, we can consider two different solutions: \u2022 the agent can keep the whole set as the result, to express\nthe uncertainy of the result of the revision, as suggested by (Bisquert et al. 2013; Coste-Marquis et al. 2014a; CosteMarquis et al. 2014b; Doutre, Herzig, and Perrussel 2014) which consider that revising or updating an AF can lead to a set of AFs;\n\u2022 the agent can use external information (preferences between AFs, preferences between formulae in the enthymemes, and so on) to select a single acceptable AF as the result.\nNone of them is in general more desirable than the other one, the choice depends on the situation (specific application, user\u2019s preferences, computational issues,. . . )."}, {"heading": "Refining Questionable Attacks", "text": "In the previous parts, we suppose that each attack concerning an enthymeme is questionable. But we can be more precise in the definition of the enthymeme-based AF. Indeed, even when we consider an enthymeme e, some of\ne1 = \u3008{\u03b1, \u03b1\u21d2 \u03b8, \u03b8 \u21d2 \u03b3}, \u03b3\u3009\nthe attacks concerning it may be certain. We know that some parts of e, that we have called previously the partial support and the partial claim, are fixed. If the reason of an attack between e and a deductive argument is a logical conflict involving one of these fixed parts of e, then this attack can be considered as certain. Similarly, when we consider another enthymeme e\u2032, if there is an attack between e and e\u2032 which is stemming from the fixed part of e and the fixed part of e\u2032, then this attack cannot be removed either.\nLet us first formalize this notion of fixed part.\nDefinition 15. If a = \u3008\u03a6, \u03b1\u3009 is a deductive argument or a non-completed enthymeme, then the fixed part of a is fix(a) = \u03a6 \u222a {\u03b1}. If a = \u3008\u03a6 \u222a \u03a8, \u03b1 \u2227 \u03b2\u3009 is a completed enthymeme, then fix(a) = \u03a6 \u222a {\u03b1}.\nSo if we consider a fully specified deductive argument or a non-completed enthymeme, the fixed part is the set of all the formulae involved in it. But when we consider an enthymeme completed with the agent\u2019s beliefs, then the fixed part is the set of formulae which appear in the enthymeme that the agent has originally received, but do not appear in the completed version of it.\nExample 5 Continued. Let us consider again the arguments\nd1, d2, e1 and e2. The fixed parts of the deductive arguments are trivially the union of their support and their claim. The result is more interesting for the enthymemes: \u2022 fix(e1) = {\u03b1, \u03b3}; \u2022 fix(e2) = {\u03b7,>};\nNow let us define the involved part of an argument in an attack. Definition 16. Let a = \u3008\u03a6, \u03b1\u3009 and b = \u3008\u03a6\u2032, \u03b1\u2032\u3009 be two arguments (deductive arguments, completed enthymemes or non-completed enthymemes). If there is an attack between a and b, then the involved part of a in the conflict between a and b, denoted by invb(a), is the set \u03a8 \u2286 \u03a6\u222a {\u03b1} such that ( \u2227 \u03c8\u2208\u03a8 \u03c8) \u2227 ( \u2227 \u03d5\u2032\u2208\u03a6\u2032\u222a{\u03b1\u2032} \u03d5\n\u2032) ` \u22a5 and \u03a8 is minimal w.r.t. \u2286. Otherwise, invb(a) = inva(b) = \u2205.\nSo, if we have a rebuttal conflict between a and b (meaning that the claims are the contradiction of each other) then invb(a) = {\u03b1} and inva(b) = {\u03b1\u2032}. If the conflict is an undercut from a to b (meaning that the claim \u03b1 of a is conflicting with some part \u03d5\u2032 of the support of b), then invb(a) = {\u03b1} and inva(b) = {\u03d5\u2032}. Example 5 Continued. Now we can see which parts of the arguments d1, d2, e1 and e2 are involved in conflicts. The certain attack (d2, d1) comes from the contradiction between \u03b4 and \u00ac\u03b4, so invd1(d2) = {\u00ac\u03b4} and invd2(d1) = {\u03b4}. Concerning the questionable attacks, we have: \u2022 inve1(d1) = {\u03b2 \u2227 \u00ac\u03b3} and invd1(e1) = {\u03b2 \u21d2 \u03b3}; \u2022 inve2(d2) = { } and invd2(e2) = {\u00ac }.\nNow we can refine the definition of an enthymeme-based AF. Definition 17. Given D and E which denote respectively the agent\u2019s deductive arguments and enthymemes, the agent\u2019s refined enthymeme-based AF is F (D,E) = \u3008A,R\u3009 with \u2022 A = D \u222a E; \u2022 R = RC \u222aRQ; \u2022 RC = {(x, y) \u2208 A \u00d7 A | invy(x) \u2286 fix(x) and invx(y) \u2286 fix(y)}: the set of certain attacks; \u2022 RQ \u2286 (A\u00d7A)\\RC : the set of questionable attacks. We use RD as a notation for RC \u2229 (D\u00d7D), which is the set of attacks between deductive arguments.\nOf course, if an argument is a fully specified deductive argument, then the part of it which is involved in conflicts is a fixed part. So to refine the AF, we need to check if it is the case with the enthymemes. Example 5 Continued. Studying the relations between involved parts and fixed parts for the enthymemes e1 and e2, we obtain the following: \u2022 invd1(e1) = {\u03b2 \u21d2 \u03b3} 6\u2286 fix(e1) = {\u03b1, \u03b3}; \u2022 invd2(e2) = {\u00ac } 6\u2286 fix(e2) = {\u03b7,>}. So none of the attacks (e2, d2) and (d1, e1) is certain.\nBut we can exhibit more interesting cases, for which the use of a refined enthymeme-based AF leads to another result than the basic enthymeme-based AF.\nExample 6. Let d3 = \u3008{\u03bd, \u03bd \u21d2 \u00ac\u03bb},\u00ac\u03bb\u3009 be a deductive argument, and e3 = \u3008{\u03ba}, \u03bb\u3009 an enthymeme, which can be completed for instance by the additional support \u03a6\u2032 = {\u03ba\u21d2 \u03bb}. It is easy to see here that invd3(e3) = {\u03bb} \u2286 fix(e3) = {\u03ba, \u03bb}, so the conflict between d3 and e3 is not questionable, and the AF corresponding to these arguments is F6 given in Fig. 7.\nWhen we consider these refined enthymeme-based AFs in the revision process, the integrity constraint must be adapted to take into account each certain attack, and not only the ones between deductive arguments:\nDefinition 18. Given F (D,E) a refined enthymeme-based AF, the integrity constraint on certain attacks is\n\u00b5C = ( \u2227\n(x,y)\u2208RC\nattx,y) \u2227 ( \u2227\n(x,y)\u2208(D\u00d7D)\\RD\n\u00acattx,y)\nThis new constraint ensures that a certain attack will not be removed during the revision process, and that attacks between deductive arguments will not be added if they do not belong to the original AF."}, {"heading": "Back to Chomsky Example", "text": "To conclude, let us formalize the intuitive \u201cChomsky example\u201d, showing the different completions of enthymemes which lead to the different agents AFs. We use the following propositional variables: retreat means that the US army will retreat;wkrmeans that the Wikileaks information about retreat is true; wkf means that the Wikileaks documents are fake; mnt means that media can not be trusted on military issues. As they are stated, the arguments a, b and c which are shared by the agents are these ones:\na = \u3008{mnt},>\u3009, b = \u3008{wkf},>\u3009, c = \u3008{wkr},>\u3009\nAll of them are enthymemes. For all the agents, the completion of c is \u3008{wkr,wkr \u21d2 retreat}, retreat\u3009. But they disagree on the completion of the other enthymemes. Agent A1 considers that a = \u3008{mnt,mnt \u21d2 \u00acwkf,mnt \u21d2 \u00acwkr},\u00acwkf \u2227 \u00acwkr\u3009 and b = \u3008{wkf},>\u3009. Agent A2 completes the enthymemes as follows: a = \u3008{mnt,mnt\u21d2 \u00acwkf},\u00acwkf\u3009 and b = \u3008{wkf,wkf \u21d2 \u00acwkr},\u00acwkr\u3009.\nFinally, agent A3 uses these completions of enthymemes: a = \u3008{mnt},>\u3009 and b = \u3008{wkf,wkf \u21d2 \u00acwkr},\u00acwkr\u3009.\nThese completions of enthymemes lead to the AFs described in Figure 3, with all arguments which are enthymemes, and all attacks which are questionable. So here, in case of a revision, the revision operator is used with the\nintegrity constraint >, which is equivalent to a revision without a constraint.\nWe mentioned in the introduction two scenarios which require to use dynamics of argumentation techniques. First, we suppose that agent A2 is considered to be trustworthy by other agents. Then, when she says that c should be accepted (which is represented by the formula accc), the other agents have to revise their AF with this new piece of information. The result of the revision for A1, with a corresponding completion of enthymemes which are modified because of the revision, is given in Figure 8.\na = \u3008{mnt,mnt\u21d2 \u00acwkf},\u00acwkf\u3009 b c\nFigure 8: Revision for Agent A1\nSimilarly, for A3, Figure 9 describes the possible revised AFs, with the modified enthymemes corresponding to it.\nFinally, the other scenario was a vote on the acceptance status of c. Since the majority of agents rejects c, A2 has to revise her AF by \u00acaccc to find an agreement with the majority. Possible results are described in Figure 10."}, {"heading": "Conclusion", "text": "In this paper, we argue that, in realistic situations, agents do not share all their knowledge and beliefs. There are different possible reasons, among them, technical and strategical reasons seem to be the most intuitive explanations. Also, implicit information is frequently used in natural language argumentation (on social networks for instance). When this situation occurs, there is some uncertainty in the resulting argumentation frameworks built by the agents. It is likely that agents\u2019 opinion about arguments\u2019 meaning and relations between arguments will differ; there may be some misunderstanding in the communication process which leads to mistakes in the generation of arguments and attacks. Here, this is formalized with the use of enthymemes,\ninstead of deductive arguments, to represent the uncertain nature of some arguments. For this reason, the reception of a new piece of information (supposed to be reliable) can force the agents to question the current attack relation to obtain a result which is compatible with the new piece of information. We have described formally how the use of enthymemes in the argumentation process can lead to the existence of these mistaken attacks, and how to define an argumentation framework which makes the distinction between the certain attacks and the questionable attacks. Then, we have seen that the existing works on the dynamics of abstract AFs can be used to perform a change on an enthymeme-based AF when it is required to incorporate a new piece of information. Here, we exemplify it with the translation-based revision from (Coste-Marquis et al. 2014b), but it can be adapted to any revision, update or enforcement approach as soon as it is possible to consider an integrity constraint on the attack relation.\nThis paper only presents some preliminary work on this question. Many future works can be envisioned. First, we want to model the uncertainty by other means than enthymemes. For instance, using weights could lead to the definition of original change operators which define the notion of minimal change w.r.t. these weights; it would then be more expensive to change a single attack which has a high weight than to change several attacks with low weights. Determining what can be the origin of these weights is particularly interesting. Combined with the use of enthymemes, we think that giving a low weight to an attack which is easy to modify (because there are many possible completions of enthymemes in the belief base) is an interesting way to tackle the problem. For the EAFs defined in this paper, as well as the weighted approach mentioned above, several questions are opened. We have made the simplifying hypothesis that the agents will have some possible completion of arguments at their disposal, which is not always the case in real world situations. Similarly, the revision operators may lead to an empty-set of results (because of the integrity constraint), or on the opposite, to a non-singleton set of results. All these cases must be investigated to ensure the possibility of some practical applications. The complexity of revising such framework, compared to the original revision approach in the abstract setting, will also be studied to be able to identify which approaches can be used for real applications. We also plan to use some of the existing pieces of software for the dynamics of AFs (in particular the one described in (Coste-Marquis et al. 2015; Wallner, Niskanen, and Ja\u0308rvisalo 2015) for extension enforcement), to study the scalability of the approaches on practical examples. But it requires first an important work to build argumentation graphs from logical knowledge-bases, since the existing works focus only on argumentation trees (Efstathiou and Hunter 2008; Efstathiou and Hunter 2011; Besnard et al. 2010)."}, {"heading": "Acknowledgments", "text": "This work as been supported by the Austrian Science Fund (FWF) under grants P25521 and I1102."}], "references": [{"title": "C", "author": ["Alchourr\u00f3n"], "venue": "E.; G\u00e4rdenfors, P.; and Makinson, D.", "citeRegEx": "Alchourr\u00f3n. G\u00e4rdenfors. and Makinson 1985", "shortCiteRegEx": null, "year": 1985}, {"title": "and Hameurlain", "author": ["L. Amgoud"], "venue": "N.", "citeRegEx": "Amgoud and Hameurlain 2006", "shortCiteRegEx": null, "year": 2006}, {"title": "and Brewka", "author": ["R. Baumann"], "venue": "G.", "citeRegEx": "Baumann and Brewka 2010", "shortCiteRegEx": null, "year": 2010}, {"title": "G", "author": ["R. Baumann", "Brewka"], "venue": "2015. AGM meets abstract argumentation: Expansion and revision for Dung frameworks. In Proc. of IJCAI", "citeRegEx": "Baumann and Brewka 2015", "shortCiteRegEx": null, "year": 2015}, {"title": "and Doutre", "author": ["P. Besnard"], "venue": "S.", "citeRegEx": "Besnard and Doutre 2004", "shortCiteRegEx": null, "year": 2004}, {"title": "and Hunter", "author": ["P. Besnard"], "venue": "A.", "citeRegEx": "Besnard and Hunter 2001", "shortCiteRegEx": null, "year": 2001}, {"title": "and Hunter", "author": ["P. Besnard"], "venue": "A.", "citeRegEx": "Besnard and Hunter 2014", "shortCiteRegEx": null, "year": 2014}, {"title": "MUS-based generation of arguments and counter-arguments", "author": ["Besnard"], "venue": "In Proc. of IRI", "citeRegEx": "Besnard,? \\Q2010\\E", "shortCiteRegEx": "Besnard", "year": 2010}, {"title": "Change in argumentation systems: Exploring the interest of removing an argument", "author": ["Bisquert"], "venue": "In Proc. of SUM", "citeRegEx": "Bisquert,? \\Q2011\\E", "shortCiteRegEx": "Bisquert", "year": 2011}, {"title": "Enforcement in argumentation is a kind of update", "author": ["Bisquert"], "venue": "In Proc. of SUM", "citeRegEx": "Bisquert,? \\Q2013\\E", "shortCiteRegEx": "Bisquert", "year": 2013}, {"title": "and Hunter", "author": ["E. Black"], "venue": "A.", "citeRegEx": "Black and Hunter 2012", "shortCiteRegEx": null, "year": 2012}, {"title": "Dynamics in argumentation with single extensions: Abstraction principles and the grounded extension", "author": ["Kaci Boella", "G. van der Torre 2009a] Boella", "S. Kaci", "L. van der Torre"], "venue": "In Proc. of ECSQARU", "citeRegEx": "Boella et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Boella et al\\.", "year": 2009}, {"title": "Dynamics in argumentation with single extensions: Attack refinement and the grounded extension", "author": ["Kaci Boella", "G. van der Torre 2009b] Boella", "S. Kaci", "L. van der Torre"], "venue": "In Proc. of AAMAS", "citeRegEx": "Boella et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Boella et al\\.", "year": 2009}, {"title": "A logical theory about dynamics in abstract argumentation", "author": ["Booth"], "venue": "In Proc. of SUM", "citeRegEx": "Booth,? \\Q2013\\E", "shortCiteRegEx": "Booth", "year": 2013}, {"title": "Change in abstract argumentation frameworks: Adding an argument", "author": ["de Saint-Cyr Cayrol", "C. Lagasquie-Schiex 2010] Cayrol", "F.D. de Saint-Cyr", "M.-C. Lagasquie-Schiex"], "venue": "Journal of Artificial Intelligence Research", "citeRegEx": "Cayrol et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Cayrol et al\\.", "year": 2010}, {"title": "On the revision of argumentation systems: Minimal change of arguments statuses", "author": ["Coste-Marquis"], "venue": "In Proc. of KR", "citeRegEx": "Coste.Marquis,? \\Q2014\\E", "shortCiteRegEx": "Coste.Marquis", "year": 2014}, {"title": "A translationbased approach for revision of argumentation frameworks", "author": ["Coste-Marquis"], "venue": "In Proc. of JELIA", "citeRegEx": "Coste.Marquis,? \\Q2014\\E", "shortCiteRegEx": "Coste.Marquis", "year": 2014}, {"title": "Extension enforcement in abstract argumentation as an optimization problem", "author": ["Coste-Marquis"], "venue": "In Proc. of IJCAI", "citeRegEx": "Coste.Marquis,? \\Q2015\\E", "shortCiteRegEx": "Coste.Marquis", "year": 2015}, {"title": "An extension-based approach to belief revision in abstract argumentation", "author": ["Diller"], "venue": "In Proc. of IJCAI", "citeRegEx": "Diller,? \\Q2015\\E", "shortCiteRegEx": "Diller", "year": 2015}, {"title": "A dynamic logic framework for abstract argumentation", "author": ["Herzig Doutre", "S. Perrussel 2014] Doutre", "A. Herzig", "L. Perrussel"], "venue": "In Proc. of KR", "citeRegEx": "Doutre et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Doutre et al\\.", "year": 2014}, {"title": "P", "author": ["Dung"], "venue": "M.", "citeRegEx": "Dung 1995", "shortCiteRegEx": null, "year": 1995}, {"title": "and Hunter", "author": ["V. Efstathiou"], "venue": "A.", "citeRegEx": "Efstathiou and Hunter 2008", "shortCiteRegEx": null, "year": 2008}, {"title": "and Hunter", "author": ["V. Efstathiou"], "venue": "A.", "citeRegEx": "Efstathiou and Hunter 2011", "shortCiteRegEx": null, "year": 2011}, {"title": "R", "author": ["Hamming"], "venue": "W.", "citeRegEx": "Hamming 1950", "shortCiteRegEx": null, "year": 1950}, {"title": "A", "author": ["H. Katsuno", "Mendelzon"], "venue": "O.", "citeRegEx": "Katsuno and Mendelzon 1991", "shortCiteRegEx": null, "year": 1991}, {"title": "A", "author": ["H. Katsuno", "Mendelzon"], "venue": "O.", "citeRegEx": "Katsuno and Mendelzon 1992", "shortCiteRegEx": null, "year": 1992}, {"title": "J", "author": ["J. Leite", "Martins"], "venue": "2011. Social abstract argumentation. In Proc. of IJCAI 2011, 2287\u2013", "citeRegEx": "Leite and Martins 2011", "shortCiteRegEx": null, "year": 2292}, {"title": "and Villata", "author": ["M. Thimm"], "venue": "S.", "citeRegEx": "Thimm and Villata 2015", "shortCiteRegEx": null, "year": 2015}, {"title": "J", "author": ["Wallner"], "venue": "P.; Niskanen, A.; and J\u00e4rvisalo, M.", "citeRegEx": "Wallner. Niskanen. and J\u00e4rvisalo 2015", "shortCiteRegEx": null, "year": 2015}], "referenceMentions": [], "year": 2016, "abstractText": "In this paper, we present a preliminary work on an approach to fill the gap between logic-based argumentation and the numerous approaches to tackle the dynamics of abstract argumentation frameworks. Our idea is that, even when arguments and attacks are defined by means of a logical belief base, there may be some uncertainty about how accurate is the content of an argument, and so the presence (or absence) of attacks concerning it. We use enthymemes to illustrate this notion of uncertainty of arguments and attacks. Indeed, as argued in the literature, real arguments are often enthymemes instead of completely specified deductive arguments. This means that some parts of the pair (support, claim) may be missing because they are supposed to belong to some \u201ccommon knowledge\u201d, and then should be deduced by the agent which receives the enthymeme. But the perception that agents have of the common knowledge may be wrong, and then a first agent may state an enthymeme that her opponent is not able to decode in an accurate way. It is likely that the decoding of the enthymeme by the agent leads to mistaken attacks between this new argument and the existing ones. In this case, the agent can receive some information about attacks or arguments acceptance statuses which disagree with her argumentation framework. We exemplify a way to incorporate this new piece of information by means of existing works on the dynamics of abstract argumentation frameworks.", "creator": "LaTeX with hyperref package"}}}