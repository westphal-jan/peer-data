{"id": "1603.09064", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "30-Mar-2016", "title": "Semi-Supervised Learning on Graphs through Reach and Distance Diffusion", "abstract": "Semi-supervised learning algorithms are an indispensable tool when labeled examples are scarce and there are many unlabeled examples [Blum and Chawla 2001, Zhu et. al. 2003] but many examples still remain in the same list [Brock et. al. 2008].\n\n\n\n\nTo view data, you have to visit the following tables:\n\nTo view the original results, click the \"Report for analysis\" button to start. (To see all the examples, click the \"Report for analysis\" button to start. The \"Results\" button is available at the top left corner of the page)\nTo view the original results, click the \"Report for analysis\" button to start. (To see all the examples, click the \"Report for analysis\" button to start. The \"Results\" button is available at the top left corner of the page)\nTo view the original results, click the \"Report for analysis\" button to start. (To see all the examples, click the \"Report for analysis\" button to start. The \"Results\" button is available at the top left corner of the page)\nTo view the original results, click the \"Report for analysis\" button to start. (To see all the examples, click the \"Report for analysis\" button to start. The \"Results\" button is available at the top left corner of the page)\nTo view the original results, click the \"Report for analysis\" button to start. (To see all the examples, click the \"Report for analysis\" button to start. The \"Results\" button is available at the top left corner of the page)\nTo view the original results, click the \"Report for analysis\" button to start. (To see all the examples, click the \"Report for analysis\" button to start. The \"Results\" button is available at the top left corner of the page)\nTo view the original results, click the \"Report for analysis\" button to start. (To see all the examples, click the \"Report for analysis\" button to start. The \"Results\" button is available at the top left corner of the page)\nTo view the original results, click the \"Report for analysis\" button to start. (To see all the examples, click the \"Report for analysis\" button to start. The \"Results\" button is available at the top left corner of the page)\nTo view the original results, click the \"Report for analysis\" button to start. (To", "histories": [["v1", "Wed, 30 Mar 2016 07:51:58 GMT  (251kb,D)", "http://arxiv.org/abs/1603.09064v1", "13 pages, 6 figures"], ["v2", "Mon, 23 May 2016 18:21:01 GMT  (420kb,D)", "http://arxiv.org/abs/1603.09064v2", "21 pages, 5 figures"], ["v3", "Sat, 13 Aug 2016 05:57:56 GMT  (417kb,D)", "http://arxiv.org/abs/1603.09064v3", "21 pages, 5 figures"], ["v4", "Mon, 26 Sep 2016 07:08:42 GMT  (418kb,D)", "http://arxiv.org/abs/1603.09064v4", "21 pages, 5 figures"], ["v5", "Fri, 20 Jan 2017 18:34:52 GMT  (436kb,D)", "http://arxiv.org/abs/1603.09064v5", "13 pages, 5 figures"]], "COMMENTS": "13 pages, 6 figures", "reviews": [], "SUBJECTS": "cs.LG", "authors": ["edith cohen"], "accepted": false, "id": "1603.09064"}, "pdf": {"name": "1603.09064.pdf", "metadata": {"source": "CRF", "title": "Semi-Supervised Learning for Asymmetric Graphs through Reach and Distance Diffusion", "authors": ["Edith Cohen"], "emails": ["edith@cohenwang.com"], "sections": [{"heading": null, "text": "Two powerful techniques to define such a kernel are \u201csymmetric\u201d spectral methods and Personalized Page Rank (PPR). With spectral methods, labels can be scalably learned using Jacobi iterations, but an inherent limiting issue is that they are applicable to symmetric (undirected) graphs, whereas often, such as with like, follow, or hyperlinks, relations between entities are inherently asymmetric. PPR naturally works with directed graphs but even with state of the art techniques does not scale when we want to learn billions of labels.\nAiming at both high scalability and handling of directed relations, we propose here Reach Diffusion and Distance Diffusion kernels. Our design is inspired by models for influence diffusion in social networks, formalized and spawned from the seminal work of [Kempe, Kleinberg, and Tardos 2003]. These models apply with directed interactions and are naturally suited for asymmetry. We tailor these models to define a natural asymmetric \u201ckernel\u201d and design highly scalable algorithms for parameter setting and label learning."}, {"heading": "1. INTRODUCTION", "text": "Semi-supervised learning [7, 55, 10] is a fundamental tool in machine learning, geared for applications when there are few labeled (seed) examples (xj , yj) j \u2264 n` and many nu n` unlabeled examples xi for i \u2208 (n`, n` + nu]. Semi-supervised learning algorithms utilize some auxiliary structure, for example a metric embedding or interaction graph on the space of examples, from which pairwise affinity scores are implied. The goal is to learn soft labels fi for the unlabeled examples that are as consistent as possible with seed labels yj and affinities \u2013 so that a learned label of an example is more similar to seed examples that are more strongly related to it.\nThe use of appropriate affinities is critical to the quality of the learned labels, but we momentarily defer this discussion and consider learning labels with respect to provided affinities \u03baij . A classic method is density estimation, where the learned labels minimize the cost function \u2211\ni>n` \u2211 j\u2264n` \u03baij ||fi \u2212 yj ||22 . (1)\nThe solution, which can be computed in isolation for each example\nand each coordinate, is a weighted average of the seed labels:\nfi =\n\u2211 j\u2264n`\n\u03baijyj\u2211 j\u2264n` \u03baij . (2)\nThis expression is also known as the Watson Nadaraya estimator [53, 40] which builds on kernel or Parzen-window density estimation [45, 43, 48]. With this estimator, two points with similar affinity relations to the seed points get similar learned labels.\nThis estimator is most often used when the affinities are a Gram matrix or another positive semi definite matrix, but here we view it more generally. In particular, the expression (2) is a solution of the optimization (1) also when \u03ba is asymmetric [48]."}, {"heading": "1.1 Strong and weak affinities", "text": "We now return to discussing derivations of good kernels. Density estimates are more effective for semi-supervised learning when the kernel meaningfully captures also the weak relations. This is because labeled examples are a small fraction of all examples, and relations of most points to seed points are weak. The raw data, however, typically only includes accurate strong relations wij . At the core of the semi-supervised learning techniques is the specification of a quality \u201call-range\u201d kernel that is computed from the strong relations.\nA typical raw data is provided as a set of pairwise interactions between entities: Friendships in a social network, word co-occurrence relations, product purchases, movie views by users, or features in images or documents. The interactions can have associated strengths determined by frequency, recency, confidence, or importance. The included interactions generally correspond to strong relations. A common approach to enhance such data is to embed the entities in a lower dimensional Euclidean space so that larger inner products, or closer distances, between the embeddings fit the provided interactions [30, 36]. Such embeddings are hugely successful in revealing other likely strong interactions that were not explicit in the data (and thus are useful for recommendations). They do define a dense kernel of pairwise similarities, but the weak relations do not seem to be captured accurately with this approach. A common technique in semi supervised learning is to sparsify this relation (often without explicitly computing the dense representation) by only retaining the strong relations \u2013 This is done by only including edges for pairs where one is the k nearest neighbor of another (i, j, if wij is one of the top k values in the ith row or wji is one of the top k in the jth row) or by using r-neighborhoods [52, 25, 54, 55]. The end product is an enhancement of the strong affinities we started with, but other techniques are still needed for capturing weak relations.\nTo visualize a kernel that is accurate for strong relations but not for weak ones, consider points that (approximately) form a dense manifold that lies in a higher dimensional space [46, 52, 55, 47, 5].\nar X\niv :1\n60 3.\n09 06\n4v 1\n[ cs\n.L G\n] 3\n0 M\nar 2\n01 6\nSemantically, we seek \u03ba that is with respect to distances over the manifold but our starting w corresponds to distances in the higher dimensional space and is therefore accurate only for strong relations."}, {"heading": "1.2 Capturing weak relations", "text": "Consider a graph representation where entities are nodes and weighted edges correspond to the strong affinities. The all-range relations \u03baij we seek can be viewed as depending on the ensemble of paths from i to j. The affinities should satisfy some intuitive desirable properties: Increase with the strength of edges, for shorter paths, and when there are more independent paths between entities. In addition, we often want to discount connections through high degree nodes, and be able to tune, via hyper parameters, the effect of each property. We now provide a brief survey of existing methods which specify an all-range kernel from such a graph.\nThe most popular methods for label learning are spectral. In perhaps the most familiar form, the learned labels are expressed as the solution of an optimization problem which has smoothness terms, which encourage learned labels of points with high wij to be more similar. To ensure a feasible solution, the objective also includes learned labels to the labeled points, and terms that encourages these labels to be close to the seed labels. These learned labels of labeled points serve only for the optimization and are eventually ignored. One such objective was proposed in an influential label propagation work by Zhu et al [54, 55]. Related objectives, adsorption and modified adsorption were studied for YouTube recommendations and named-entity recognition [4, 50, 51]. The cost function has the form\n1\n2 \u2211 i,j wij ||fi \u2212 fj ||2 + \u03bb \u2211 i\u2264n` ||fi \u2212 yi||2 . (3)\nThe solution can be expressed as a set of linear equations of a particular diagonally dominant form and computed by inverting a corresponding matrix. We can view this inverted matrix as an all-range kernel \u03ba (which does not depend on the labels of the seed nodes). The learned labels can then be expressed as density estimates with respect to \u03ba. Other interpretations of the solution are as a fixed point of a stochastic sharing process or the landing probability of a random walk [11, 29].\nIn practice, the explicit storage of the dense all-range \u03ba or performing a matrix inversion is too costly. Instead, the solution that is specific to the seed labels is approximated using the Jacobi method to approach the fixed point. The computation of each gradient update is linear in the number of edges, but typically, hundreds of iterations are needed even with various optimizations [22]. A further optimization sparsifies the set of unlabeled points using a smaller set of anchors that is large enough to preserve the short-range structure but is much smaller than the full set [19, 33]. Anchors are selected as samples or cluster representatives. Other unlabeled points are expressed as weighted combinations of anchors, removed from the \u201cspectral\u201d computation, and eventually inherit as their learned label, an appropriate linear combination, in essence a density estimate, of the learned labels of the anchors.\nWhile scalable, an inherent issue with these spectral techniques is that they assume symmetric (undirected) relations. Symmetry is needed for positive semi definiteness, which in turn, is needed for convergence and to ensure existence of a solution.\nA related and hugely successful set of techniques, which work with directed relations, are Personalized PageRank (PPR) and derivatives [11, 41, 27]. PPR is \u201cpersonalized\u201d to a node i or a distribution over subset of nodes. In a nutshell, PPR measures the frequency of visiting other nodes j when using short random walks\nfrom i. The PPR vectors naturally define an \u201call-range\u201d kernel, where \u03baij corresponds to the probability of visiting j from i. This PPR kernel has the desired qualitative properties we seek. To apply it to our context, however, we need to estimate visiting probabilities from all unlabeled i to sufficiently many seeds, and scalability is a game stopper even with state of the art techniques [34]. This is because, again, even the fastest designs aim to identify the nodes with largest visiting probabilities, whereas our density estimates would need the seed nodes with largest visiting probabilities. As mentioned, the seed set is a small fraction of all nodes and therefore the total visiting probability of the seed set is small. This means that any algorithm from basic Monte Carlo generation of walks to the bidirectional approach of [34] would spend most of its \u201cwork\u201d traversing non-seed nodes. Another more scalable use of PPR is performed on the \u201ctransposed\u201d graph [32]: All seeds with the same label are grouped and PPR is then personalized to each label, and applied to obtain similarities from labels to nodes. This method scales well when the label dimension is small (the number of optimization problems is the number of labels), but random walks are inherently not reversible and this transposed kernel has a very different semantics than \u201cforward\u201d PPR that is personalized to each unlabeled node."}, {"heading": "1.3 Contributions", "text": "We propose a novel approach to semi-supervised learning based on what we call reach diffusion and distance diffusion kernels. Our kernels share the intuitive desirable properties of the spectral and PPR kernels but they have the advantages of suitability for asymmetric relations, like PPR, and highly scalable algorithms, as with the symmetric spectral kernels. We present algorithms for setting parameters and computing approximate learned labels using computation that is near-linear in the size of the input, and can be parallelized efficiently. We establish statistical guarantees on the estimate quality of the approximate labels with respect to the exact ones as defined in the model. We also perform a preliminary experimental study that demonstrates the application and potential of our proposed models.\n1.3.1 Reach diffusion Our reach diffusion model is inspired by popular information\ndiffusion models [20, 28] and by reliability or survival analysis [37, 31] that is extensively used to analyze engineered and biological systems.\nInfluence diffusion, as motivated by Richardson and Domingos [20] and formalized by Kempe, Kleinberg, and Tardos [28], is defined for a network of directed pairwise interactions between entities. A probability distribution on the subset of active edges is constructed, and the influence of a node i is then measured as the expected number of nodes i it can reach through active edges. Independent Cascade (IC) [28], which uses independent activation probabilities pe to edges, is the simplest and most studied model. The influence of a node, when defined this way, satisfies the desirable properties of increasing when paths to other nodes are shorter and when there are more independent paths. The models also apply with asymmetric connections.\nTo apply this approach for semi supervised learning, we need to first define an appropriate kernel \u03baij that provides corresponding pairwise \u201cinfluence\u201d values. The straightforward attempt is working directly with the probability that i reaches j. But this will not scale up, for similar reasons to PPR not scaling up. Instead, we propose a refinement of the model that would both scale and satisfy natural desirable properties.\nWe borrow from reliability analysis, treating edges as a com-\nponent of a system connecting entities. We associate with edges continuous random variables \u00b5e that correspond to their lifetime (which can be thought of as inversely related to activation probabilities). Edges that correspond to more significant interactions have higher expected lifetimes. From this, we can define for each ordered pair of nodes (i, j) its survival time threshold random variable tij , which is the minimum \u03c4 such that j is reachable from i via edges with lifetime \u00b5e \u2265 \u03c4 .\nNote that we can express the IC model of [28] in terms of this reliability formulation by choosing independent lifetime variables that are exponential random variables with parameter 1/pe \u00b5e \u223c Exp[1/pe]. The influence of a node i in the IC model is then the expected number of nodes reachable from i via edges with \u00b5e \u2265 1.\nIn a Monte Carlo simulation of the model we obtain a set of lifetime values \u00b5e for edges which imply corresponding survival times tij for the connectivity from i to j. For a node i, we can consider an ordering of nodes j according to decreasing values of tij . Accordingly, we define \u03baij as inversely related to Nij , which is the position of j in this order. When j is not reachable from i we define \u03baij = 0. Note that our kernel \u03ba here is also a random variable, which assumes values in each simulation of the model. Our learned labels will be the expectation of the density estimates over the distribution of \u03ba.\nThe survival time tij depends on the ensemble of directed paths from i to j. To illustrate its properties, we can consider simpler structures. A directed path in isolation, where the survivability is the minimum lifetime of a path edge. We can also consider the set of out edges from a node, and the survivability of \u201cat least one out connection,\u201d which is the maximum lifetime of an out edge.\nThe position Nij does not only depend on the connectivity ensembles but also on how the ensemble relates to the corresponding ensembles of other nodes. For example, suppose i connects to j via a path of length 3 and to h via a path of length 2 with independent iid \u00b5e. Then we always have Pr[tij < tih] > Pr[tij > tih], that is, the shorter path has higher survival time. When the paths are independent, we can have, however, simulations with tij > tih. But when the 2-path is the prefix of the 3-path we always have tij < tih.\nWe can verify that the qualitative desiderata we seek are satisfied: Higher significance edges, shorter paths, and more independent paths would translate to a higher expected survival times tij and lower Nij .\nA typical choice for lifetime variables in reliability models is the Weibull distribution. If we use Weibull distributed \u00b5e, with shape parameter \u03b2 and scale parameter \u03bb equal to the significance of e, we obtain some compelling properties. Note that the Weibull family includes the exponential distribution which is Weibull with shape parameter \u03b2 = 1 and corresponds to \u201cmemoryless\u201d remaining lifetime. Parameters \u03b2 < 1 model higher probability of \u201cearly failures\u201d and \u03b2 > 1 to bell shaped lifetimes concentrated around the expectation. For two edges with iid lifetimes, the probability of one having a higher lifetime than the other is proportional to the ratio of their significances to the power of \u03b2. From the closure under minimum property of the Weibull distribution, the survival time of a directed path with independent Weibull lifetimes is also Weibull distributed with the same shape parameter \u03b2 and scale parameter equal to an inverse of the \u03b2-norm of the vector of inverted edge significances. For exponential distributions, the expected lifetime of each edge is the inverse of its significance, and the survival threshold of the path has parameter (which is the inverse of the Weibull parameter) equal to the sum of significances, which yields expected survival that is the inverse of that sum. The shape parameter \u03b2 allows us to tune the emphasis of lower significance edges\non the survival of the path.\n1.3.2 Distance diffusion Our distance diffusion kernels are inspired by a generalization,\nfirst proposed by Gomez-Rodriguez et al [24, 21, 17] of the influence model of Kempe et al [28] to a distance-based setting. They are also inspired by models of distance-based utility in networks [18, 6, 26] where the relevance of a node to another node decreases with the distance between them. In these influence models, edges have length random variables, which can be interpreted as propagation times. The influence of a node v is then defined as a function of elapsed \u201ctime\u201d T , as the expected number of nodes that are activated within a time T (the shortest-path distance from v is at most T ). Note that the \u201ctime\u201d here refers to propagation and activation times rather than \u201csurvival\u201d time, so shorter times correspond to stronger connections. To prevent confusion, we will use the terms edge lengths in the context of distance diffusion here and use time only in the context of reach diffusion.\nMore precisely, we associate length random variables `e with edges with expectation that decreases with the edge significance (shorter lengths when edges are more significant). In a Monte Carlo simulation of the model we obtain a set of lengths `e \u2265 0 for edges which induces a set of shortest paths distances dij between ordered pairs. Again, the random variable dij depends on the ensemble of directed graphs from i to j. A choice of Weibull distributed lengths with scale parameter equal to the inverse significance seems particularly natural [1, 21, 15]: The closest out connection from a node corresponds to the minimum length of an out edge. When edge lengths are Weibull, the minimum is also Weibull distributed with the same shape parameter and a scale parameter equal to the inverted \u03b2-norm of the edge significances.\nTo define a kernel \u03baij , we similarly consider the order on nodes j by increasing shortest-path distance dij from i. As we did with reach diffusion, we defineNij as the position of j in this order, and accordingly define \u03baij as inversely related to Nij . When j is not reachable from i we define \u03baij = 0.\n1.3.3 Scalable computation Our learned labels fi are the expectation, over simulations of\nthe model, of the density estimates (2). Exact computation of the learned labels fi is not feasible in the probabilistic model. We therefore use Monte Carlo simulations and average the learned labels we obtain from each simulation. A small number of simulations suffices to obtain a small error on the entries of fi.\nThe main algorithmic challenge is obtaining a scalable approximation of each simulation. Even per simulation, exact computation is prohibitive: The computation of Nij for all seed nodes j and unlabeled nodes i uses nu graph searches, which is O(|E|nu) operations and quadratic even for sparse graphs. We design highly scalable algorithm which approximates the entries of fi to within additive errors of with good concentration. Note that dominant entries would be approximated well.\nOur scalable approximation relies on a sketching technique of reachability sets and of neighborhoods of directed graphs [12, 13]. We will use these sketches, computed with respect to different base sets of nodes, for two different purposes. The first is to obtain estimates with small relative error on Nij from the survival threshold tij with reach diffusion and from the shortest-path distance dij with distance diffusion. These estimates replace the expensive exact computation of kernel entries \u03baij . The second is to obtain, for each node i, a small tailored weighted sample of seed nodes according to the kernel entries \u03baij . Since the sample is weighted, we can use only the sampled entries with inverse probability weights\nto approximate the density estimates and yet obtain a good approximation of the full sums.\nThe distance-sketching technique can be applied almost out of the box for distance diffusion. For reach diffusion, however, we need to sketch survival times and not distances. For the first part, we need to obtain sketches that will allow us to estimate the set sizes R\u03c4 (i) for all i and \u03c4 . For the second part, we need to obtain a weighted sample with respect to the reach diffusion kernel.\nTo do so, we design a threshold sketching algorithm which builds on the basic distance-sketching design [12, 13] but replaces the shortest-path searches by \u201csurvival threshold\u201d graph searches as a basic component. In a nutshell, the summation operation used for shortest paths length can be replaced (carefully) with a min operation for tracking survival thresholds instead of distances. We show that total computation of these threshold searches is near-linear and establish its correctness.\n1.3.4 Parameter setting The last component of our framework is a methodology for pa-\nrameter setting. There are multiple hyper parameters in our models including the significance weights and the lifetime or length random variables we associate with the graph components. Our sketches support leave-one-out cross validation on the seed nodes with the same cost of computing the sketches. Therefore the seed set itself can be used to set the hyper parameters."}, {"heading": "1.4 Overview", "text": "In Section 2 we present in detail our reach and distance diffusion kernels and their application to label learning. In Section 3 we show how we use Monte Carlo simulations and sketches to approximate the learned labels. We also analyze the worst-case statistical guarantees on approximation quality that we can obtain. In Section 4 we present algorithms to compute the approximate labels. Parameter settings methodology is discussed in Section 5. Section 6 contains some preliminary experiments."}, {"heading": "2. MODEL", "text": "Our input is specified as a graph G = (V,E), where the nodes V are entities and edges E (undirected or directed) correspond to interactions between entities. We associate weights we with edges e \u2208 E that reflect the strength of the interaction and inverse cost of connecting through the head entity. We can also associate weights wv with a node v that reflect the inverse cost of connecting through the entity. A common way to model the cost of connecting through an entity is as some non-decreasing function of its degree (number of interactions) \u2013 connections through higher degree nodes are less meaningful and thus costlier. The strength of an interaction can reflect its frequency or recency."}, {"heading": "2.1 Reach diffusion", "text": "We build a probabilistic model from this input by associating lifetime random variables with edges and nodes. A natural choice is to use for each component x, a Weibull or an exponentially distributed random variables tx \u223c Exp[1/wx] with parameter equal to its weight wx. Some components that are \u201cfixed\u201d have tx = +\u221e. Note that the expected lifetime is E[tx] = wx, so stronger interactions have longer lifetimes. In each Monte Carlo simulation of the model we obtain a set of lifetimes tx for the components of the graph (edges and nodes).\nFor a threshold parameter \u03c4 , the set of active components are the edges and nodes {x \u2208 E \u222a V | tx \u2265 \u03c4}. For a pair of nodes (i, j), we define the survival time tij of the connection from i to j tij as the maximum \u03c4 such that j is reachable from i using components\nwith tx \u2265 \u03c4 . To simplify notation, we overloaded t: When the subscript as the edge we refer to the lifetime. When the subscript is a pair we refer to the survival time. For an edge e = (i, j) we have tij \u2265 \u00b5e.\nWe use the notation\nR\u03c4 (i) = {j | tij \u2265 \u03c4}\nfor the set of nodes reachable from i via active components. Note that the R\u03c4 (i) is a random variables. Note that for a fixed simulation, the set of active components and the reachability sets R\u03c4 (i) are non increasing with \u03c4 .\nThe survival thresholds tij induce an order over nodes j. Intuitively, more related nodes to i, those with higher tij , would be earlier in this order. We capture this order with the random variable\nNij = |Rtij (i)| = |{h | tih \u2265 tij}| .\nFinally, we accordingly define the reach diffusion kernel\n\u03baij = \u03b1(Nij) ,\nwhere \u03b1 \u2265 0 is non-increasing. A natural default choice is \u03b1(x) = 1/x, where the affinity of j to i is inversely proportional to the number of nodes that precede it in the influence order. When j is not reachable from i, we define \u03baij = 0.\nIn the simplest scheme, the lifetimes tx of different components can be independent. Semantically, this achieves the effect of rewarding multiple edge-disjoint paths, even when they traverse the same nodes. (Nodes are not considered failure points). In general, however, we can also capture correlations between edges by correlating accordingly the lifetime random variables. For example, we can consider all edges with the same head entity as related and share the same lifetime t, or correlated lifetimes.\nA natural extension is to associate massmi \u2265 0 with nodes, that is interpreted as proportional to the importance of the example. For the case when entities correspond to consumers and goods and we are only interested in labeling goods, this flexibility allows us to assign positive mass only to \u201cgoods\u201d nodes and mi = 0 to \u201cconsumer\u201d nodes. The relevance of an example j to another i is then proportional to its mass, but inversely depends on the mass that is reached before j. To model this, we refine the definition to be\nNij = \u2211\nh|tih\u2265tij\nmh\nas the mass that is reached at the survival threshold of the connection (i, j). Our derivations and algorithms can be adapted to incorporate mass but for simplicity of presentation, we focus on the basic setting where mi \u2208 {0, 1}."}, {"heading": "2.2 Distance diffusion", "text": "We associate length nonnegative random variables with edges and nodes. In each Monte Carlo simulation of the model we obtain a fixed set of lengths `x for the components of the graph. We can now consider shortest-paths distances dij with respect to the lengths `. The length of a path is defined as the sum of the lengths of path edges and the lengths of middle nodes of the path. The distance dij is the length of the shortest path. For convenience here, we overload the notation we used for reach diffusion: For \u03c4 \u2265 0 and node i, we denote by R\u03c4 (i) = {j | dij \u2264 \u03c4} the set of nodes j within distance at most \u03c4 from i. For nodes i, j, we denote by Nij the number (or mass) of nodes h with dih \u2264 dij . The distance diffusion kernel is defined as \u03baij = \u03b1(Nij).\n2.3 Learned labels\nIn the semi-supervised learning setup, a subset of the nodes, those with j \u2264 n` have provided labels yj . For nodes i > n`, we compute learned labels using the kernel density estimate\nfi = E\n[\u2211 j\u2264n`\n\u03baijmjyj\u2211 j\u2264n` mj\u03baij\n] . (4)\nThe provided labels, and thus the learned labels, are vectors with nonnegative entries of dimension L such that ||yi||1 = 1.\nNote that we chose to use a respective \u201ckernel\u201d \u03baij in each simulation, and take the expectation over the density estimates. Another conceivable choice is instead to use \u03baij as the expectation over simulations and use that in a single density estimate. Our reasoning for the former choice is to preserve the dependencies when computing the density estimates in the relative location of seed nodes across simulations."}, {"heading": "3. APPROXIMATE LABELS", "text": "In this section we start tackling the issue of highly scalable computation of learned labels. We present our approach for computing the labels approximately, using Monte Carlo simulations and a novel use of sample-based sketches [12, 13] to approximate the results of each simulation. We also present the statistical guarantees we obtain on estimation quality.\nWe estimate the expectations (4) by taking the average over T Monte Carlo simulations of the model of\nf \u2032i =\n\u2211 i\u2264n`\n\u03bajimiyi\u2211 i\u2264n` mi\u03baji . (5)\nBefore we address the computation of f \u2032i , however, we consider the loss in quality due to the use of simulations versus the exact expectation (4). That is, the statistical guarantees we obtain for the average of T independent (exact) random variables f \u2032i as an estimate of fi.\nLEMMA 3.1. With T = \u22122, the average estimate of each component of fi has absolute error bound that is well concentrated around (probability of absolute error that exceeds c is at most 2 exp(\u22122c2)).\nPROOF. This is an immediate consequence of Hoeffding\u2019s inequality, noting that entries of our label vectors are in [0, 1].\nWe next consider computing (5) for a single simulation. An exact computation requires the values of the positions Nij for all i > n` and j \u2264 n`. With distances, it is widely believed that there is no subquadratic algorithm and even the representation alone is quadratic. With reach diffusion, on undirected (symmetric) graphs, all pairs tij can be represented efficiently using a single minimum weight spanning tree (MST) computation on a graph with edge weights 1/\u00b5e. The computation is near-linear in the number of edges. The graph cuts defined by the MST compactly specify tij for all pairs. Our interest here, however, is directed graphs, where the problem does not seem much easier than shortest paths computations: The computation of tij and Nij for one source node i and all j can be performed by a graph search from i, but it is seems that separate searches are needed for different source nodes, similarly to the corresponding problem with distances. Moreover, while n` searches suffices to compute tij , we seem to need nu n` searches to also compute Nij .\nWe approach this (for both reach and distance diffusions) by using instead estimates f\u0302 \u2032i of f \u2032 i , which can be scalably computed for all i > n`. We then estimate fi by averaging the T estimates f\u0302 \u2032i .\nOur estimates f\u0302 \u2032i are obtained by computing two sets of sketches for all nodes i:\n\u2022 The first set of sketches is with respect to the full set of nodes, or more precisely, all nodes h with mh > 0. These sketches are used to estimate the mass m(R\u03c4 (i)) for all i and for all \u03c4 .\n\u2022 The second set of sketches is with respect to seed nodes. They provide us, for each node i, a small tailored weighted sample of seed nodes S(i) \u2282 [n`]. The sampling is such that the inclusion probability of j is proportional to m(j) and inversely proportional to its position in the seed set when ordered by tij (dij for distances). For each j \u2208 S(i), the sketch also provides us with the exact value of tij (dij for distances) and a conditional inclusion probability pij .\nUsing these sketches, we compute our per-simulation label estimate f\u0302 \u2032i as follows. For each i and j \u2208 S(i), we have tij (dij for distances), and use the first set of sketches to compute the estimates\nN\u0302ij \u2261 m\u0302(Rtij (i)) .\nFor each i, we use the sample S(i) obtained in the second set of sketches to compute\nf\u0302 \u2032i =\n\u2211 j\u2208S(i)\n1 pij mj \u03ba\u0302ijyj\u2211\nj\u2208S(i) 1 pij mj \u03ba\u0302ij\n, (6)\nwhere \u03ba\u0302ij = \u03b1(N\u0302ij)."}, {"heading": "3.1 Sketches", "text": "The sketches we will use are MinHash and All-Distances Sketches (ADS), using state of the art optimal estimators [12, 13, 14]. To simplify and unify the presentation, we use bottom-k alldistances sketches [12, 13, 14] for the two uses of sketches. The sketch parameter k trades off sketch/sample size and estimation quality. Note that other variations can also be used and the representation can be simplified when sketches are only used for size estimation. For further simplicity, we assume here that mi \u2208 {0, 1}. See discussion in [13] for the handling of general m. We use the notation U for the set of nodes that are being sketched, which is the full set of nodes with positive mass for the first set of sketches and only the seed nodes for the second set.\nThe sketches are randomized structures that are defined with respect to a uniform random permutation \u03c0 of the sketched nodes U . We use the notation \u03c0j for the permutation position of j \u2208 U . A bottom-k MinHash sketch is defined for each \u03c4 and includes the k nodes with minimum \u03c0 in the set R\u03c4 (i) \u2229 U . The all-distances sketches S(i) we work with can be viewed as encoding MinHash sketches of R\u03c4 (i) \u2229 U for all values of \u03c4 . Formally,\nj \u2208 S(i) \u21d0\u21d2 \u03c0j \u2264 kth\u03c0{h \u2208 U | tih \u2265 tij} . (7)\nWith distances, the sketch is defined with the inequality reversed:\nj \u2208 S(i) \u21d0\u21d2 \u03c0j \u2264 kth\u03c0{h \u2208 U | dih \u2264 dij} . (8)\nThe definition is almost identical for reach diffusion and distance diffusion. Reach diffusion sketches are defined for survival times tij , which are stronger for higher values, whereas with distances we use dij , which are stronger for lower values. To reduce redundancy, we will focus the presentation on reach diffusion. To obtain the corresponding algorithms and sketches for distances, we need to reverse the inequality signs.\nFor each entry j in the sketch S(i), we also compute the conditional inclusion probabilities pij of j \u2208 S(i). In our context,\nwe use these probabilities for the mass estimates obtained from the first set of sketches and for the inverse probability estimate f\u0302 \u2032i that use the second set of sketches.\nThe probability pij is defined with respect to (is conditioned on) the permutation \u03c0 on U \\ {j}. It is the probability, over the |U | possible values of \u03c0j of having a value low enough so that j is included in S(i). More precisely, for j \u2208 S(i), we consider the set of nodes\nAij = {h \u2208 U \\ {j} | tih \u2265 tij} ,\nwhich includes all nodes in U other than j that have survival times at least tij . We then define\npij = { 1 : if |Aij | < k kth\u03c0(Aij)\u22121 |U| : Otherwise\n(9)\nWhere the operator kth\u03c0 returns the kth smallest permutation position of all elements in the set. The node j will always be included in the sketch if there are fewer than k other nodes with a lower tih. Otherwise, it will be included only if it has one of the lowest k permutation positions among the nodes U , which means that it has a strictly lower permutation position than the kth position in Aij .\nNote that the set Aij is usually contained in S(i), except for sometimes, when there are multiple elements h with same tih. In this case it is possible for pij to be defined by an element not in S(i). We refer to such elements that are not included in S(i) but are used to compute inclusion probabilities for other nodes as Z(i) nodes.\nWe now explain how the sketches are used for the two tasks. For a node i, the sketch S(i) can be viewed as a list of tuples of the form (j, tij , p(tij)). When U is the seed of seed nodes. The second set of sketches is computed with U being the set of seed nodes. In this case, the tuples S(i) are the sample we use to compute the approximate density estimates. The first set of sketches is computed with U being the set of all nodes with mi = 1. We use this sketch to obtain neighborhood estimation lists which we use to obtain the estimates m\u0302(R\u03c4 (i)). The neighborhood estimation list includes, for each represented t value, the entry\n(t, \u2211\nh\u2208S(i)|tih\u2264t\n1\nptih ) ,\nin sorted decreasing t order. This list can be computed by a linear pass over tuples (j, t, p) in decreasing t order. To query the list with value \u03c4 we look for the last tuple in the list that has t \u2265 \u03c4 and return the associated estimate."}, {"heading": "3.2 Estimation Error Analysis", "text": "The estimation quality of f\u0302 \u2032i (6) as an estimate of f \u2032 i (5) is affected by two sources of error. The first is the quality of the samplebased inverse probability estimate (6) as an estimate of\nf (\u03ba\u0302) i =\n\u2211 j\u2264n`\nyj \u03ba\u0302ij\u2211 j\u2264n` \u03ba\u0302ij . (10)\nThe second is the quality of \u03ba\u0302ij as an estimate of \u03baij . From the theory of MinHash and distance sketches, we obtain the following:\nTHEOREM 3.1. For a sketch parameter k:\n\u2022 The expected size of the samples is bounded by\nE[|S(i) \u222a Z(i)|] \u2264 k lnn`\nand the sizes are well concentrated.\n\u2022 If \u03ba\u0302ij are nonincreasing in tij , then each component of the vector f (\u03ba\u0302)i is estimated by (6) with mean square error (MSE) at most 1/k and good concentration.\nFor the second source of error we obtain:\nLEMMA 3.2. With sketch parameter k, the estimates N\u0302ij are unbiased with Coefficient of Variation (CV) at most 1/ \u221a 2k with good concentration.\nOne caveat is our use of \u03b1(\u03ba\u0302ij) as an estimate of \u03b1(\u03baij). Our estimates \u03ba\u0302ij have a small relative error with good concentration, but for \u03b1(\u03baij) to have this property we need it not to decay faster than polynomially. More precisely, when \u03b1\n\u2032(x)x \u03b1(x)\n\u2264 c then we obtain that the NRMSE is at most c times that of the estimate \u03ba\u0302ij . In particular, when \u03baij = 1/Nij , the estimates have NRMSE at most 1/ \u221a\n2k with good concentration. We can now state overall worst-case statistical guarantees on our estimates of fi as defined in (4). We use here the independence of our three sources of error to slightly tighten the bound.\nTHEOREM 3.2. When using \u22122 Monte Carlo simulations, and sketch parameter k = 1\n2 \u22122 and when \u03b1 \u2032(x)x \u03b1(x)\n\u2264 1, then each component of fi is approximated with RMSE \u221a 3 with good concentration.\nAlgorithm 1 Sketch survival thresholds Input: G = (V,E, \u00b5) a graph with nodes V , directed edges |E|, and lifetimes \u00b5e \u2265 0 for e \u2208 E ; Subset U \u2282 V of nodes Output: For i \u2208 V , a sketch S(i) of the set {(j, tij) | j \u2208 U} // Initialization foreach i \u2208 V do\nInitialize the sketch structure S(i) ; // Algorithm 2 Compute a random permutation \u03c0 : U \u2192 |U |\n// Main Loop: foreach j \u2208 U in increasing \u03c0j order do\nPerform a pruned single-source survival threshold search from j on the transposed graph; // Algorithm 3\n// Finalize foreach i \u2208 V do\nFinalize the sketch structure S(i); // Algorithm 2"}, {"heading": "4. ALGORITHMS", "text": "We now consider the computation of the bottom-k all-distances sketches. These sketches were originally developed to be used with shortest-paths distances dij and there are several algorithms and large scale implementations that can be used out of the box. They compute the sketches or the more restricted application of neighborhood size estimates [12, 42, 8, 9, 13]. The different algorithms are designed for distributed node-centric, multi-core, and other settings. Most of these approaches can be easily adapted to estimate m(R\u03c4 (i)), when mi \u2208 {0, 1} (see discussion in [13]) and there is a variation [13] that is suitable for general m. The component of obtaining the sample and probabilities is more subtle, but uses the same computation (See [13, 14]).\nFor reach diffusion, we do not work with distances but with the survival thresholds tij . As said, the sketches have the same definitions and form but we need to redesign the algorithms to compute the sketches with respect to thresholds.\nThe sketching algorithm we present here for survival thresholds builds on a sequential algorithm for ADS computation which is based on performing pruned Dijkstra searches [12, 13]. The algorithm for distance sketching performs O(|E|k ln |U |) edge traversals and the total computation bound is\nO(n logn+ (|E|+ n log k) ln |U |) ,\nwhere n is the total number of nodes. The algorithm has a parallel version designed to run on multi-core architectures [9].\nOur redesigned sketching algorithm for survival thresholds has the same bounds. Moreover, our redesign can be parallelized in the same way for multi-core architectures, but we do not provide the details here.\nA high level pseudocode of our sketching algorithm for survival thresholds is provided as Algorithm 1. We first initialize empty sketch structures S(i) for all nodes i. The algorithm builds the node sketches by processing nodes j in increasing permutation rank \u03c0j . A pruned graph search is then performed from the node j. This search has the property that it visits all nodes i where j \u2208 S(i) \u222a Z(i). The search updates the sketch for such nodes i and proceeds through them. The search is pruned when j 6\u2208 S(i) \u222a Z(i).\nWe now provide more details. The first component of this algorithm is building the sketches S(i). The pseudocode provided as Algorithm 2 builds on a state of the art design for computing universal monotone multi-objective samples [14]. The pseudocode includes the initialization, updates, and finalizing components of building the sketch for a single node i. The structure is initially empty and then tracks the set of pairs (j, tij) for the nodes j processed so far that are members of the sketch S(i). To build the sketch efficiently, the structure includes a min heap H of size k which contains the k largest tij values for processed j \u2208 S(i). The structure is presented with updates of the form (j, tij), which are in increasing \u03c0j order. A node j is inserted to S(i) when tij is one of the k largest tih values of nodes h already selected for the sketch. This is determined using the minimum priority in the heap H . If the node is inserted, the heap is updated by popping its min element and inserting tij . The update can also result in modifying the sketch in some cases when the node j is not included in S(i), but is in Z(i), meaning that some inclusion probability of other node(s) is set to p(tij). To facilitate the computation of inclusion probabilities, we define\nuj = \u03c0j \u2212 1 |U | . (11)\nThis is the probability for node with \u03c0h < \u03c0j to have permutation rank smaller than \u03c0j , when fixing the permutation order of all nodes except for j and computing the probability conditioned on that. We say that the update procedure for (j, tij) modified the sketch if and only if j \u2208 S(i) \u222a Z(i).\nThe sketch of i is computed correctly when the updates include all nodes j for which the sketch was modified: The computation of the sketch will not change if we do not process entries j that do not result in modifying the sketch.\nWe now describe the next component of the algorithm which is the pruned graph search from a node j. The searches are performed on the transposed graph, which has all edges reversed. Similar to the corresponding property of Dijkstra and distances, the search visits nodes i in order of non-increasing tji. The search is pruned at nodes where there were no updates to the sketch. A pseudocode for the pruned search is provided as Algorithm 3. The algorithm maintains a max heap that contains nodes i that are prioritized by lower bounds on tij . The heap maintains the property that the maximum priority i has the exact tij . The heap is initialized with the\nAlgorithm 2 Maintain sketch S(i), updates by increasing \u03c0 // Initialize: S(i)\u2190\u22a5; p\u2190\u22a5 H \u2190\u22a5 ; // min heap of size k, holding k largest (tij ,\u2212\u03c0ij) values processed so far (lex order) prevt\u2190\u22a5 ; // tij of most recent j popped from H // Process updates: for Update (j, tij , \u03c0j), given by increasing \u03c0j order do\nif |H| < k then S(i)\u2190 S(i) \u222a {j}; Insert j to H; Continue\ny \u2190 arg minz\u2208H(tiz , \u03c0z) ; // node with max (tiy , \u03c0y) if tij > tiy then\nS(i)\u2190 S(i) \u222a {j} ; // Add j to sample prevt\u2190 tiy if p(prevt) =\u22a5 then\np(prevt)\u2190 uj ; // As defined in Eq. (11) Delete y from H Insert j to H\nelse// Z node check if tij = tiy and tiy > prevt then\np(tij)\u2190 uj prevt\u2190 tiy\n// Finalize: for x \u2208 H do // keys with largest weights\nif p(tix) =\u22a5 then p(tix)\u2190 1\nnode j and priority +\u221e. The algorithm then repeats the following until the heap is empty. It removes the maximum priority i from the heap. It then updates the sketch of i with (j, tij). If the sketch was updated, all out edges e = (i, h) are processed as follows. If h is not on the heap, it is placed there with priority min{tij , te}. If h is in the heap, its priority is increased to the maximum of its current priority and min{tij , te}. If the sketch of i was not updated, the search is pruned at i and out edges are not processed. For correctness, note that min{tij , te} is trivially a lower bound on tih.\nWe now need to establish that the sketches are still constructed correctly with the pruning:\nLEMMA 4.1. The search from j reaches and processes all nodes i for such that j \u2208 S(i) \u222a Z(i). When the node i is processed, the update (j, tij) is with the correct survival threshold tij .\nPROOF. We show the claim by induction on permutation order. Suppose the sketches are correctly populated until just before j. Consider now a search from j and a node i such that j \u2208 S(i). There must exist a path P from i to j such that for any suffix P \u2032 of the path from some h to j, mine\u2208P \u2032 \u00b5e = thj .\nWe will show that the reverse search from j can not be pruned in any of the nodes in P . Therefore, i must be inserted into the search heap and subsequently be processed. Assume to the contrary that the search is pruned at h \u2208 P . For the pruning to occur, there must be a set of nodes Y \u2282 S(h) of size |Y | \u2265 k such that \u03c0y < \u03c0j and tyj \u2265 thj . Let P \u2032\u2032 = P \\ P \u2032 be the prefix of the path P from i to h and let T \u2032\u2032 = mine\u2208P \u2032\u2032 \u00b5e. Then by definition, for all y \u2208 Y ,\ntiy \u2265 min{T \u2032\u2032, tyj} \u2265 min{T \u2032\u2032, thj} = tij .\nSince there are at least |Y | \u2265 k nodes with \u03c0y < \u03c0j and tiy \u2265 tij , this implies that j 6\u2208 S(i), and we obtain a contradiction. A similar argument applies when j \u2208 Z(i).\nLastly, we need to argue that when node i is removed from the heap and processed, its priority is equal to tij . It is easy to verify that the heap maintains the property that the priorities are lower\nbounds on survival thresholds. This is because for any heap priority, there must be path to j with minimum \u00b5e equal to that priority.\nWe need to show that equality holds when i is processed. The nodes h \u2208 P on the path are in non-increasing order of tih. Let 0 = \u03c41 > \u03c42 > \u00b7 \u00b7 \u00b7 be the different survival threshold values on the path. We prove this by induction on \u03c4i. are processed not necessarily in path order, but in non-increasing order of tih. Initially the heap contains only (j,\u221e), which is the correct threshold. Assume now it holds for all nodes with survival thresholds \u2265 \u03c4i. Consider now the path edge e from a node h with thj = \u03c4i to a node h\u2032 with th\u2032j = \u03c4i+1. This edge must have lifetime \u00b5e = \u03c4i+1. When the node h is processed, h\u2032 is placed on the heap with priority min{\u03c4i, \u00b5e} = \u03c4i+1, which is equal to th\u2032j . If it was already on the heap, its priority is increased to th\u2032j . Consider now other path nodes h\u2032\u2032 with th\u2032\u2032j = \u03c4i+1. This nodes must be placed on the heap with the correct threshold when the previous path node is processed (it is possible for them to be placed with the correct priority also before that). Therefore, all path nodes with thj = \u03c4i+1 will be processed with the correct priority.\nAlgorithm 3 Pruned single-source survival threshold search Input: Source node j // Initialization: H \u2190\u22a5 ; // Empty max heap of nodes i. Priority is a lower bound on tij Put (j,+\u221e) in H ; // j with priority tjj = +\u221e // Main loop: while Heap H not empty do\nPop maximum priority (i, tij) from H Update the sketch S(i) with (j, tij) ; // Algorithm 2 if update modified sketch then\nforeach out edge e = (i, h) do if h 6\u2208 H then\nInsert (h,max{\u00b5e, tij}) to H else\nUpdate priority of h in H to the maximum of current priority and min{\u00b5e, tij}\nWe can now bound the computation performed by the algorithm.\nLEMMA 4.2. The sketching algorithm performs in expectation at most |E|k ln |U | edge traversals. The total computation is\nO(n logn+ (|E|+ n log k)k ln |U |)\nwhere n is the total number of nodes.\nPROOF. The number of times a node is processed by a pruned search (meaning that its out edges are processed) is equal to the number of times its sketch is modified, which is the size of the sketch. From the analysis of distance sketches, we have a bound on the number of visits. We obtain a bound of |E|k ln |U | on the number of edge traversals performed by the algorithm. The other summand is due to heap operations when updating the sketches and in the pruned searches."}, {"heading": "5. PARAMETER SETTING", "text": "Our models have several unspecified hyper parameters: With reach diffusion, the selection of the lifetime random variables of\nthe components and possible dependencies between them. With distance diffusion, the selection of the length random variables. Another parameter is the decay function \u03b1 applied to the position order of a node.\nOur use of sketches allows for an easy implementation of leaveone-out cross validation. For each memberm of the seed set U , we can compute a learned label fm with respect to U \\ {m} using the sample S(m) (withm itself omitted). We can then consider setting which minimizes the cost function\u2211\nm\u2208U\n||fm \u2212 ym||2 .\nNote that the selection of a decay function \u03b1 can utilize the same sets of simulations and sketches. Each evaluation for different component length/lifetime functions, however, requires a fresh set of simulations and sketches."}, {"heading": "6. EXPERIMENTS", "text": "We performed some preliminary experiments using the Movielens 1M [39] and political blogs [3] datasets. Our aim is two fold. First, to evaluate the quality of learned labels in some semi-supervised learning context. Second, to demonstrate a use case for our models and the selection of length or lifetime variables. Our evaluation here is not meant to assess scalability, as there are several highly scalable implementation of the basic distance and reachability sketching methods we use as our main component [42, 8, 16, 23, 17, 9]. We implemented the algorithms in Python and performed the experiments on a Macbook Air."}, {"heading": "6.1 Movielens 1M data", "text": "The data consists of about 1 million rating by 6,040 users of 3,952 movies. Each movie is a member of one or more of the 18 genres: Action, Adventure, Animation, Children\u2019s, Comedy, Crime, Documentary, Drama, Fantasy, Film-Noir, Horror, Musical, Mystery, Romance, Sci-Fi, Thriller, War, Western\nAbout 3.7K movies had both listed genres and ratings. This is our set M of examples. We represented the \u201ctrue\u201d label ym of a movie m with c listed genres as an L = 18 dimensional vector with weight 1/c on each listed genre and weight 0 otherwise. Note that the provided labels and also our learned labels have the form of probability vectors over genres. We use the notation \u0393(m) for the set of users that rated movie m and by \u0393(u) the set of movies rated by u.\nWe build a graph with a node for each movie and each user. For each user u and movie m \u2208 \u0393(u), we place two directed edges, (m,u) and (u,m) (We do not use the numeric ratings provided in the data set, and only consider presence or a rating).\nWe next specify our choices of length or lifetime random variables we associate with nodes and edges. In this preliminary evaluation, we used a small limited set of length/lifetime schemes, without attempting to optimize a choice of such a scheme.\nDistance diffusion lengths. Our randomized length schemes are specified by non-increasing functions g1, g2 \u2265 0 and an offset value \u03b4 \u2265 0. We associate with each user to movie edge e = (u,m) an independent exponential random variable `e \u223c Exp[g1(|\u0393(u)|)]. Each movie to user edge e = (m,u) has length `e = 0. Finally, with each movie m, we associate a pass-through length that is `m \u223c \u03b4 + Exp[g2(|\u0393(m)|)].\nThe functions gi allows us to tune the amount in which paths through higher degree nodes are discounted. With g(x) = 1/x, we have the property that the shortest out edge from a node v has\nlength distribution Exp[1] regardless of |\u0393(v)|. Functions with a slower decrease give more significance to higher degree nodes.\nThese length variables have a compelling interpretation when we consider, for a particular m \u2208 M , the order of 2-hop movies by increasing distance from m. This order is equivalent to the order of weighted sampling without replacement of movies according to the similarity of their users, when similarity is defined as:\nsim(m,m\u2032) = \u2211\nu\u2208\u0393(m)\u2229\u0393(m\u2032)\ng1(|\u0393(u)|) .\nWith g(x) = 1/ log(x) we obtain the Adamic-Adar similarity [2] popular in social network analysis. Note that our model captures these pairwise movie-movie relations while working with the original user-movie interactions, without explicit computation or approximation of these similarities.\nNote that our use of pass-through lengths with movie nodes and 0 lengths for (m,u) edges is equivalent to using pass-through lengths of 0 and using for all e = (m,u) edges identical lengths `m. We briefly explain the semantics of randomized edge versus node lengths. When edge lengths are independent, we reward multiple edge-disjoint paths even when they traverse the same node. When we randomize only node pass-throughs, we reward multiple paths only when they are node-disjoint.\nThe purpose of the offset parameter \u03b4 is to control the \u201ccost\u201d of additional hops: With a very high offset, a movie m\u2032 with a 4 hop path to m would always be farther than a movie m\u2032\u2032 2 hop away. With low offset, when the path ensemble from m to m\u2032\u2032 contains many independent paths through low degree nodes, and the ensemble from m to m\u2032 is sparse and involves one or few very high degree users, then m\u2032\u2032 would likely to be closer than m\u2032.\nThe specific lengths schemes we evaluated are listed in Table 1: (i) Randomized schemes with g1 \u2261 g2 \u2261 g and different offsets. (ii) A randomized scheme that sets the lengths of (m,u) edges independently as Exp[g(|\u0393(m)|)]+\u03b4 instead of using the same passthrough length for m. (iii) Fixed lengths schemes where the length of an edge (v, w) is a fixed non-decreasing function of |\u0393(v)|.\nReach diffusion lifetimes. We associate with each user to movie edge e = (u,m) an independent exponential random variables \u00b5e \u223c Exp[g1(|\u0393(u)|)]. For each movie m, we set all out edges e = (m,u) to a fixed +\u221e lifetime. We also use lifetime random variables for \u201cpassing through\u201d a movie node that are \u00b5mm \u223c Exp[g2(|\u0393(m)|)]. We used g1(x) = g2(x) \u2261 g as listed in Table 1. Note that with reach diffusion we use non-decreasing g.\nSeed sets. Our seed sets S are subsets of M selected uniformly at random. We use seed sets of sizes\ns \u2208 {20, 50, 100, 200, 500, 1000, 2000}\nwhich roughly correspond to 0.5% to 50% of all movies in M . We selected 5 random permutations of the examples M and sets of seeds that correspond to prefixes of the same permutation.\nQuality evaluation. We perform Monte Carlo simulations of the model. In each simulation, a fresh set of edge lengths is drawn and fresh sketches are computed with respect to the new lengths. In each simulation we obtain a set of labels, one for each movie in m \u2208 M . Our final learned labels fm are the average of the output of the different simulations. Our approximation quality improves on average with a larger sketch parameter (which controls the quality of the estimates obtained from the sketches) and the number of simulations. In our experiments we used i = {1, 2, 5, 10, 20} simulations and a sketch parameter of k = 16. With this smaller value of k we can only obtain very course estimates of the neighborhood sizes, but the computation is more efficient.\nNote that we also compute learned labels for seed movies, based only on the labels of other seed movies. As explained in Section 5, the sketches allow us to perform this computation very efficiently.\nWe measure the quality of our learned labels fm with respect to the true labels ym using the 2-norm of the difference\n||fm \u2212 ym||2 = \u221a\u2211 i\u2208[L] (fmi \u2212 ymi)2 .\nWe compare the error of fi to that of a simple prior: The average label of the seed set\ny(S) \u2261 1|S| \u2211 i\u2208S yi (12)\nand define success to be the event where the learned label is better than the average seed label:\n||fm \u2212 ym||2 < ||y(S)\u2212 ym||2 . (13)\nTo obtain precision recall (PR) tradeoffs, we consider the margin, which we define as the 2-norm of the difference between the learned label and the average label \u2206i = ||y(S) \u2212 fi||2. We then sweep a threshold value \u03c4 . The recall for \u03c4 is the fraction of examples (movies) i for which \u2206i \u2265 \u03c4 . The precision is the fraction of these examples with success.\n6.1.1 Movielens1M results Results were fairly consistent for the different seed sets selec-\ntions, with more variance as expected for the small seed sets. We discuss the trends and show some representative results.\nComparing schemes. Figure 1 illustrated the PR plots for different edge lengths schemes for 10 iterations (Monte Carlo simulations).\nThe schemes with fixed edge lengths (see bottom right plot for seed set of size 200) gave no or very weak signal for small seed sets s = 20, 50 but \u201ccatch up\u201d with the randomized schemes for larger seed sets. The worst performer by far throughout was fixed edge lengths of g(x) = 1. All schemes with increasing g(x) performed better on large seed sets with g(x) = \u221a x being the best performer. This shows that discounting the value of paths by the degree of the nodes they traverse is critical.\nThe first 5 plots show some of the randomized schemes in Table 1 and the fixed lengths scheme with g(x) = \u221a x, for seed\nset sizes between 20 and 500. Note that the randomized scheme provide a meaningful signal even for very small seed sets. The better performers were the randomized schemes with g(x) = 1/x and mid range \u03b4. The remaining schemes in the table, the randomized scheme with g(x) = 1/ log2(1 + x) and for randomized scheme with independent `mu lengths were outperformed by the other schemes and are not shown.\nOur results for reach diffusion showed the same patterns as distance diffusion, but where somewhat weaker.\nThe relative improvement of the fixed length schemes for larger seed sets can be explained by these schemes (which are not able to capture as well the full path ensembles) providing accurate \u201cshort range\u201d affinities, which is enough when many of the examples closest to them are in the seed set. These schemes are very inaccurate on the \u201clong range\u201d affinities, which are critical for sparse seed sets and are better captured by randomized schemes.\nNumber of simulations. We study quality as a function of the number of simulations. From the theory, we expect quality to increase for two reasons. First, with randomized edge lengths, the average over more simulation provides a closer estimate of the true expectation (4). Second, and this applies also for fixed-length schemes, more simulations mitigate the error of using sketch-based approximation and not the exact density estimate (5).\nSome representative plots of quality as a function of the number of simulations are presented in Figure 2. We observe that the improvement in quality with the number of simulations was significant for the randomized schemes, and in particular with the smaller seed sets. There was little or no improvement for the fixed lengths schemes.\nThe more significant improvement for the randomized schemes on smaller seed sets might be explained by the randomized model being able to better capture the deeper path ensembles between nodes and seeds.\nThe stronger improvement with the number of simulations for randomized versus deterministic schemes suggests that the main source of improvement is obtaining a better approximation of the true expectation. Therefore, it is more beneficial to increase the number of simulations than to increasing the sketch parameter k.\nSize of the seed set. Note that our measure of quality is relative to the average seed label. Therefore, the quality of this prior also improves for larger seed sets. We do observe, however, a consistent increase in quality for larger seed sets across schemes even with respect to this improving prior. Some representative results are provided in Figure 4 for the randomized scheme with g(x) = 1/x and \u03b4 = 50 and for the fixed-length scheme with g(x) = 1/ \u221a x."}, {"heading": "6.2 Political blogs data", "text": "The data consists of about 19,000 links between roughly 1200 blogs collected at the 2004 US presidential election. The blogs are labeled as liberal or conservative, with half the blogs in each category. About 62% of the blogs form a single strongly connected component, 21% can reach the component via links, and 16% can only be reached from the component.\nThe label dimension here is L = 2, as the provided label yi of a blog i is (1, 0) for liberal and (0, 1) for conservative. We used 10 sets of experiments. In each set, we select a different uniform random permutation of the blogs. We then take seed sets S of size s \u2208 [10, 1000] as prefixes of the same permutation. Note that our seed set sizes range from less than 0.1% to about 83% of all blogs. We then apply our algorithms to compute learned labels fj for all nodes j.\nTo make a prediction, we consider the average label of the seed set y(S) (defined in (12)) and the learned label fj . The prediction is then liberal if fj1 > y(S)1 and conservative if fj1 < y(S)1. If the prediction is equal to the true label, we count it as a success. We define the margin of our prediction as the 2-norm ||y(S)\u2212 yj ||2 of the difference between the average seed label and the learned label. When the margin is 0, which happens when our model provides no information (no reachable seed nodes), we take the success to be 0.5. We consider the precision (fraction of successful predictions) and recall (number of predictions as fraction of total), as a function of the margin.\nNote that hyperlinks are directed, and the direction has a concrete semantics. In our experiments, we separately worked with three sets of edges: forward (an edges with the same direction is generated for each hyperlink), reversed (a reversed edge is generated for each hyperlink), and undirected (two directed edges are generated for each hyperlink). We then consider distance and reach diffusion on these directed graphs. As we did with the Movielens1M dataset, we used a limited selection of fixed-length and randomized length (distance diffusion) and lifetime (reach diffusion) schemes, as outlined in Table 2. The function |\u0393(u)| is the outdegree of u, which is the number of hyperlinks to other blogs with forward, the number of hyperlinks to the blog with reversed, and the sum with undirected. We used a sketch parameter k = 32 and up to 40 Monte Carlo simulations.\nOn this data set, randomization of lengths did not provide an advantage. The fixed length schemes performed very well, with g(x) = x and g(x) = log2(1 + x) being more consistent and slightly better than g(x) = 1. For these schemes, there was no observable improvement with the number of simulations. The prediction success was typically over 90% even with the smallest seed sets (s = 10). This is explained by the two sets of blogs forming two distinct clusters, detectable by most clustering algorithms.\nThe best randomized distance scheme was g(x) = 1/x and \u03b4 = 5. The best randomized reach scheme was g(x) = x. Overall, the reach diffusion schemes gave much weaker predictions than the distance diffusion schemes and both were outperformed by the fixed-length schemes.\nThe randomized distance and reach diffusion schemes did show drastic improvement with the number of simulations (see Figure 6 (right)). All schemes were more accurate with larger seed sets (see Figure 6 (left and middle)). Performace did strongly depend on direction (see Figure 5 for representative results): Reversed was clearly inferior to forward. Undirected and forward were comparable and consistently best, with the former providing a higher recall. We also evaluated combined predictions (combo), which go with the prediction with the largest margin among forward and reversed. Prediction quality of combo was more consistent than reversed but was dominated by undirected and forward."}, {"heading": "7. EXTENSIONS", "text": ""}, {"heading": "7.1 Large label dimension", "text": "Our presentation focused on a small label dimension L, so that the soft learned labels can have a dense representation. When L is very large, we can use the variety of composable heavy-hitters sketches (sample-based, linear, deterministic) to represent and manipulate the labels. This sketches, roughly, have dimensionO( \u22121), regardless of the label dimension, and allow us to identify all entries in the (L1 normalized) soft labels of value at least .\nWith spectral label learning, this sketching approach was first applied by [49], which used Count-min sketches, and by [44], which used a composable version of Misra Gries sketches [38, 35]. Sketches of the label vectors can be seamlessly plugged-in with our density estimates here to obtain a respective linear sketch of the learned labels."}, {"heading": "7.2 Assigning confidence to seed labels", "text": "Our use of leave-one-out cross validation provides a cheap way of computing a learned label (based on other seeds) also for seed nodes. One application of that is to tune the hyper parameters of our framework. Another application, when the seed labels themselves are not of high quality, is to associate a confidence level \u201cmass\u201d with them based on the match between the seed and learned label for the seed."}, {"heading": "8. CONCLUSION", "text": "We proposed a new approach for graph-based semi-supervised learning through reach diffusion and distance diffusion kernels. Our models are inspired by well studied models of influence diffusion and address applications with asymmetric relations that require a highly scalable computation of the learned labels. We designed scalable algorithms and established correctness of the computation and of the statistical guarantees provided by our approximations. We conducted a promising preliminary experimental evaluation."}, {"heading": "Acknowledgment", "text": "We would like to thank Fernando Pereira for discussions, pointers to the literature, and sharing views and intuitions on real-world challenges which prompted the development of our proposed models."}, {"heading": "9. REFERENCES", "text": "[1] B. D. Abrahao, F. Chierichetti, R. Kleinberg, and\nA. Panconesi. Trace complexity of network inference. In KDD, 2013.\n[2] L. A. Adamic and E. Adar. How to search a social network. Social Networks, 27, 2005.\n[3] L. A. Adamic and N. Glance. The political blogosphere and the 2004 u.s. election: Divided they blog. In LinkKDD. ACM, 2005.\n[4] S. Baluja, R. Seth, D. Sivakumar, Y. Jing, J. Yagnik, S. Kumar, D. Ravichandran, and M. Aly. Video suggestion and discovery for youtube: Taking random walks through the view graph. In WWW, 2008.\n[5] A. Bijral, N. Ratliff, and N. Srebro. Semi-supervised learning with density based distances. In UAI, 2011.\n[6] F. Bloch and M. O. Jackson. The formation of networks with transfers among players. Journal of Economic Theory, 133(1):83\u2013110, 2007.\n[7] A. Blum and S. Chawla. Learning from labeled and unlabeled data using graph mincuts. In ICML, 2001.\n[8] P. Boldi, M. Rosa, and S. Vigna. HyperANF: Approximating the neighbourhood function of very large graphs on a budget. In WWW, 2011.\n[9] E. Buchnik and E. Cohen. Reverse ranking by graph structure: Model and scalable algorithms. Technical Report cs.SI/1506.02386, arXiv, 2015.\n[10] O. Chapelle, B. Sch\u00f6lkopf, and A. Zien. Semi-supervised learning. MIT Press, 2006.\n[11] F. R. K. Chung. Spectral Graph Theory. American Mathematical Society, 1997.\n[12] E. Cohen. Size-estimation framework with applications to transitive closure and reachability. J. Comput. System Sci., 55:441\u2013453, 1997.\n[13] E. Cohen. All-distances sketches, revisited: HIP estimators for massive graphs analysis. TKDE, 2015.\n[14] E. Cohen. Multi-objective weighted sampling. In HotWeb. IEEE, 2015. full version: http://arxiv.org/abs/1509.07445.\n[15] E. Cohen, D. Delling, F. Fuchs, A. Goldberg, M. Goldszmidt, and R. Werneck. Scalable similarity estimation in social networks: Closeness, node labels, and random edge lengths. In COSN. ACM, 2013.\n[16] E. Cohen, D. Delling, T. Pajor, and R. F. Werneck. Sketch-based influence maximization and computation: Scaling up with guarantees. In CIKM. ACM, 2014.\n[17] E. Cohen, D. Delling, T. Pajor, and R. F. Werneck. Distance-based influence in networks: Computation and maximization. Technical Report cs.SI/1410.6976, arXiv, 2015.\n[18] E. Cohen and H. Kaplan. Spatially-decaying aggregation over a network: Model and algorithms. J. Comput. System Sci., 73:265\u2013288, 2007. Full version of a SIGMOD 2004 paper.\n[19] O. Delalleau, Y. Bengio, and N. Le Roux. Efficient non-parametric function induction in semi-supervised learning. In Artificial Intelligence and Statistics, 2005.\n[20] P. Domingos and M. Richardson. Mining the network value of customers. In KDD. ACM, 2001.\n[21] N. Du, L. Song, M. Gomez-Rodriguez, and H. Zha. Scalable influence estimation in continuous-time diffusion networks. In NIPS. Curran Associates, Inc., 2013.\n[22] Y. Fujiwara and G. Irie. Efficient label propagation. In ICML, 2014.\n[23] K. Garimella, G. De Francisci Morales, A. Gionis, and M. Sozio. Scalable facility location for massive graphs on pregel-like systems. In CIKM. ACM, 2015.\n[24] M. Gomez-Rodriguez, D. Balduzzi, and B. Sch\u00f6lkopf. Uncovering the temporal dynamics of diffusion networks. In ICML, 2011.\n[25] T. Hastie, R. Tibshirani, and J. Friedman. The Elements of Statistical Learning. Springer New York Inc., 2001.\n[26] M. O. Jackson. Social and economic networks. Princeton University Press, 2010.\n[27] G. Jeh and J. Widom. Scaling personalized web search. In WWW. ACM, 2003.\n[28] D. Kempe, J. M. Kleinberg, and \u00c9. Tardos. Maximizing the spread of influence through a social network. In KDD. ACM, 2003.\n[29] R. I. Kondor and J. Laffery. Diffusion kernels on graphs and other discrete input spaces. In ICML, 2002.\n[30] Y. Koren, R. Bell, and C. Volinsky. Matrix factorization techniques for recommender systems. Computer, 42, 2009.\n[31] J. F. Lawless. Statistical models and methods for lifetime data, volume 362. John Wiley & Sons, 2011.\n[32] F. Lin and W. W. Cohen. The multirank bootstrap algorithm: Self-supervised political blog classification and ranking using semi-supervised link classification. In ICWSM, 2008.\n[33] W. Liu, J. He, and S.-F. Chang. Large graph construction for scalable semi-supervised learning. In ICML, 2010.\n[34] P. A. Lofgren, S. Banerjee, A. Goel, and C. Seshadhri. Fast-PPR: Scaling personalized pagerank estimation for large graphs. In KDD. ACM, 2014.\n[35] G. Manku and R. Motwani. Approximate frequency counts over data streams. In International Conference on Very Large Databases (VLDB), pages 346\u2013357, 2002.\n[36] T. Mikolov, I. Sutskever, K. Chen, G. S. Corrado, and J. Dean. Distributed representations of words and phrases and their compositionality. In NIPS, 2013.\n[37] R. G. Miller. Survival analysis. John Wiley & Sons, 1975. [38] J. Misra and D. Gries. Finding repeated elements. Technical\nreport, Cornell University, 1982. [39] Movielen 1M Dataset.\nhttp://grouplens.org/datasets/movielens/1m/. [40] E. A. Nadaraya. On estimating regression. Theory Prob.\nApplic., 9, 1964. [41] L. Page, S. Brin, R. Motwani, and T. Winograd. The\npagerank citation ranking: Bringing order to the web. Technical report, Stanford InfoLab, 1999.\n[42] C. R. Palmer, P. B. Gibbons, and C. Faloutsos. ANF: A fast and scalable tool for data mining in massive graphs. In KDD, 2002.\n[43] E. Parzen. On the estimation of a probability density function and the mode. Annals of Math. Stats., 33, 1962.\n[44] S. Ravi and Q. Diao. Large-scale semi-supervised learning using streaming approximation. In AISTATS, 2016.\n[45] M. Rosenblatt. Remarks on some nonparametric estimates of a density function. The Annals of Mathematical Statistics, 27(3):832, 1956.\n[46] S.T. Roweis and Saul L. K. Nonlinear dimensionality reduction by locally linear embedding. Science, 290, 2000.\n[47] Sajama and A. Orlitsky. Estimating and computing density based distance metrics. In ICML, 2005.\n[48] B. W. Silverman. Density estimation for statistics and data analysis. Monographs on statistics and applied probability. Chapman, 1986.\n[49] P. P. Talukdar and W. W. Cohen. Scaling graph-based semi supervised learning to large number of labels using count-min sketch. In AISTATS, 2014.\n[50] P. P. Talukdar and K. Crammer. New regularized algorithms for transductive learning. In ECML PKDD, 2009.\n[51] P. P. Talukdar and F. C. N. Pereira. Experiments in graph-based semi-supervised learning methods for class-instance acquisition. In ACL, 2010.\n[52] J. B. Tenenbaum, V. de Silva, and J. C. Langford. A global geometric framework for nonlinear dimensionality reduction. Science, 290, 2000.\n[53] G. S. Watson. Smooth regression analysis. Sankhya A, 26, 1964.\n[54] X. Zhu and Z. Ghahramani. Learning from labeled and unlabeled data with label propagation, 2002.\n[55] X. Zhu, Z. Ghahramani, and J. Laffery. Semi-supervised learning using Gaussian fields and harmonic functions. In ICML, 2003."}], "references": [{"title": "Trace complexity of network inference", "author": ["B.D. Abrahao", "F. Chierichetti", "R. Kleinberg", "A. Panconesi"], "venue": "In KDD,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2013}, {"title": "How to search a social network", "author": ["L.A. Adamic", "E. Adar"], "venue": "Social Networks,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2005}, {"title": "The political blogosphere and the 2004 u.s. election: Divided they blog", "author": ["L.A. Adamic", "N. Glance"], "venue": "In LinkKDD. ACM,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2005}, {"title": "Video suggestion and discovery for youtube: Taking random walks through the view graph", "author": ["S. Baluja", "R. Seth", "D. Sivakumar", "Y. Jing", "J. Yagnik", "S. Kumar", "D. Ravichandran", "M. Aly"], "venue": "In WWW,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2008}, {"title": "Semi-supervised learning with density based distances", "author": ["A. Bijral", "N. Ratliff", "N. Srebro"], "venue": "In UAI,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2011}, {"title": "The formation of networks with transfers among players", "author": ["F. Bloch", "M.O. Jackson"], "venue": "Journal of Economic Theory,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2007}, {"title": "Learning from labeled and unlabeled data using graph mincuts", "author": ["A. Blum", "S. Chawla"], "venue": "In ICML,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2001}, {"title": "HyperANF: Approximating the neighbourhood function of very large graphs on a budget", "author": ["P. Boldi", "M. Rosa", "S. Vigna"], "venue": "In WWW,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2011}, {"title": "Reverse ranking by graph structure: Model and scalable algorithms", "author": ["E. Buchnik", "E. Cohen"], "venue": "Technical Report cs.SI/1506.02386,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2015}, {"title": "Semi-supervised learning", "author": ["O. Chapelle", "B. Sch\u00f6lkopf", "A. Zien"], "venue": null, "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2006}, {"title": "Spectral Graph Theory", "author": ["F.R.K. Chung"], "venue": "American Mathematical Society,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 1997}, {"title": "Size-estimation framework with applications to transitive closure and reachability", "author": ["E. Cohen"], "venue": "J. Comput. System Sci.,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 1997}, {"title": "All-distances sketches, revisited: HIP estimators for massive graphs analysis", "author": ["E. Cohen"], "venue": null, "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2015}, {"title": "Multi-objective weighted sampling", "author": ["E. Cohen"], "venue": "In HotWeb. IEEE,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2015}, {"title": "Scalable similarity estimation in social networks: Closeness, node labels, and random edge lengths", "author": ["E. Cohen", "D. Delling", "F. Fuchs", "A. Goldberg", "M. Goldszmidt", "R. Werneck"], "venue": "In COSN. ACM,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2013}, {"title": "Sketch-based influence maximization and computation: Scaling up with guarantees", "author": ["E. Cohen", "D. Delling", "T. Pajor", "R.F. Werneck"], "venue": "In CIKM. ACM,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2014}, {"title": "Distance-based influence in networks: Computation and maximization", "author": ["E. Cohen", "D. Delling", "T. Pajor", "R.F. Werneck"], "venue": "Technical Report cs.SI/1410.6976,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2015}, {"title": "Spatially-decaying aggregation over a network: Model and algorithms", "author": ["E. Cohen", "H. Kaplan"], "venue": "J. Comput. System Sci.,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2007}, {"title": "Efficient non-parametric function induction in semi-supervised learning", "author": ["O. Delalleau", "Y. Bengio", "N. Le Roux"], "venue": "In Artificial Intelligence and Statistics,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2005}, {"title": "Mining the network value of customers", "author": ["P. Domingos", "M. Richardson"], "venue": "In KDD. ACM,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2001}, {"title": "Scalable influence estimation in continuous-time diffusion networks", "author": ["N. Du", "L. Song", "M. Gomez-Rodriguez", "H. Zha"], "venue": "In NIPS. Curran Associates, Inc.,", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2013}, {"title": "Efficient label propagation", "author": ["Y. Fujiwara", "G. Irie"], "venue": "In ICML,", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2014}, {"title": "Scalable facility location for massive graphs on pregel-like systems", "author": ["K. Garimella", "G. De Francisci Morales", "A. Gionis", "M. Sozio"], "venue": "In CIKM. ACM,", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2015}, {"title": "Uncovering the temporal dynamics of diffusion networks", "author": ["M. Gomez-Rodriguez", "D. Balduzzi", "B. Sch\u00f6lkopf"], "venue": "In ICML,", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2011}, {"title": "The Elements of Statistical Learning", "author": ["T. Hastie", "R. Tibshirani", "J. Friedman"], "venue": null, "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2001}, {"title": "Social and economic networks", "author": ["M.O. Jackson"], "venue": null, "citeRegEx": "26", "shortCiteRegEx": "26", "year": 2010}, {"title": "Scaling personalized web search", "author": ["G. Jeh", "J. Widom"], "venue": "In WWW. ACM,", "citeRegEx": "27", "shortCiteRegEx": "27", "year": 2003}, {"title": "Maximizing the spread of influence through a social network", "author": ["D. Kempe", "J.M. Kleinberg", "\u00c9. Tardos"], "venue": "In KDD. ACM,", "citeRegEx": "28", "shortCiteRegEx": "28", "year": 2003}, {"title": "Diffusion kernels on graphs and other discrete input spaces", "author": ["R.I. Kondor", "J. Laffery"], "venue": "In ICML,", "citeRegEx": "29", "shortCiteRegEx": "29", "year": 2002}, {"title": "Matrix factorization techniques for recommender systems", "author": ["Y. Koren", "R. Bell", "C. Volinsky"], "venue": null, "citeRegEx": "30", "shortCiteRegEx": "30", "year": 2009}, {"title": "Statistical models and methods for lifetime data, volume 362", "author": ["J.F. Lawless"], "venue": null, "citeRegEx": "31", "shortCiteRegEx": "31", "year": 2011}, {"title": "The multirank bootstrap algorithm: Self-supervised political blog classification and ranking using semi-supervised link classification", "author": ["F. Lin", "W.W. Cohen"], "venue": "In ICWSM,", "citeRegEx": "32", "shortCiteRegEx": "32", "year": 2008}, {"title": "Large graph construction for scalable semi-supervised learning", "author": ["W. Liu", "J. He", "S.-F. Chang"], "venue": "In ICML,", "citeRegEx": "33", "shortCiteRegEx": "33", "year": 2010}, {"title": "Fast-PPR: Scaling personalized pagerank estimation for large graphs", "author": ["P.A. Lofgren", "S. Banerjee", "A. Goel", "C. Seshadhri"], "venue": "In KDD. ACM,", "citeRegEx": "34", "shortCiteRegEx": "34", "year": 2014}, {"title": "Approximate frequency counts over data streams", "author": ["G. Manku", "R. Motwani"], "venue": "In International Conference on Very Large Databases (VLDB),", "citeRegEx": "35", "shortCiteRegEx": "35", "year": 2002}, {"title": "Distributed representations of words and phrases and their compositionality", "author": ["T. Mikolov", "I. Sutskever", "K. Chen", "G.S. Corrado", "J. Dean"], "venue": "In NIPS,", "citeRegEx": "36", "shortCiteRegEx": "36", "year": 2013}, {"title": "Survival analysis", "author": ["R.G. Miller"], "venue": null, "citeRegEx": "37", "shortCiteRegEx": "37", "year": 1975}, {"title": "Finding repeated elements", "author": ["J. Misra", "D. Gries"], "venue": "Technical report, Cornell University,", "citeRegEx": "38", "shortCiteRegEx": "38", "year": 1982}, {"title": "On estimating regression", "author": ["E.A. Nadaraya"], "venue": "Theory Prob. Applic.,", "citeRegEx": "40", "shortCiteRegEx": "40", "year": 1964}, {"title": "The pagerank citation ranking: Bringing order to the web", "author": ["L. Page", "S. Brin", "R. Motwani", "T. Winograd"], "venue": "Technical report, Stanford InfoLab,", "citeRegEx": "41", "shortCiteRegEx": "41", "year": 1999}, {"title": "ANF: A fast and scalable tool for data mining in massive graphs", "author": ["C.R. Palmer", "P.B. Gibbons", "C. Faloutsos"], "venue": "In KDD,", "citeRegEx": "42", "shortCiteRegEx": "42", "year": 2002}, {"title": "On the estimation of a probability density function and the mode", "author": ["E. Parzen"], "venue": "Annals of Math. Stats.,", "citeRegEx": "43", "shortCiteRegEx": "43", "year": 1962}, {"title": "Large-scale semi-supervised learning using streaming approximation", "author": ["S. Ravi", "Q. Diao"], "venue": "In AISTATS,", "citeRegEx": "44", "shortCiteRegEx": "44", "year": 2016}, {"title": "Remarks on some nonparametric estimates of a density function", "author": ["M. Rosenblatt"], "venue": "The Annals of Mathematical Statistics,", "citeRegEx": "45", "shortCiteRegEx": "45", "year": 1956}, {"title": "Nonlinear dimensionality reduction by locally linear embedding", "author": ["S.T. Roweis", "Saul L. K"], "venue": null, "citeRegEx": "46", "shortCiteRegEx": "46", "year": 2000}, {"title": "Estimating and computing density based distance metrics", "author": ["Sajama", "A. Orlitsky"], "venue": "In ICML,", "citeRegEx": "47", "shortCiteRegEx": "47", "year": 2005}, {"title": "Density estimation for statistics and data analysis", "author": ["B.W. Silverman"], "venue": "Monographs on statistics and applied probability", "citeRegEx": "48", "shortCiteRegEx": "48", "year": 1986}, {"title": "Scaling graph-based semi supervised learning to large number of labels using count-min sketch", "author": ["P.P. Talukdar", "W.W. Cohen"], "venue": "In AISTATS,", "citeRegEx": "49", "shortCiteRegEx": "49", "year": 2014}, {"title": "New regularized algorithms for transductive learning", "author": ["P.P. Talukdar", "K. Crammer"], "venue": "In ECML PKDD,", "citeRegEx": "50", "shortCiteRegEx": "50", "year": 2009}, {"title": "Experiments in graph-based semi-supervised learning methods for class-instance acquisition", "author": ["P.P. Talukdar", "F.C.N. Pereira"], "venue": "In ACL,", "citeRegEx": "51", "shortCiteRegEx": "51", "year": 2010}, {"title": "A global geometric framework for nonlinear dimensionality reduction", "author": ["J.B. Tenenbaum", "V. de Silva", "J.C. Langford"], "venue": null, "citeRegEx": "52", "shortCiteRegEx": "52", "year": 2000}, {"title": "Smooth regression analysis", "author": ["G.S. Watson"], "venue": "Sankhya A,", "citeRegEx": "53", "shortCiteRegEx": "53", "year": 1964}, {"title": "Learning from labeled and unlabeled data with label propagation", "author": ["X. Zhu", "Z. Ghahramani"], "venue": null, "citeRegEx": "54", "shortCiteRegEx": "54", "year": 2002}, {"title": "Semi-supervised learning using Gaussian fields and harmonic functions", "author": ["X. Zhu", "Z. Ghahramani", "J. Laffery"], "venue": "In ICML,", "citeRegEx": "55", "shortCiteRegEx": "55", "year": 2003}], "referenceMentions": [{"referenceID": 6, "context": "Semi-supervised learning [7, 55, 10] is a fundamental tool in machine learning, geared for applications when there are few labeled (seed) examples (xj , yj) j \u2264 n` and many nu n` unlabeled examples xi for i \u2208 (n`, n` + nu].", "startOffset": 25, "endOffset": 36}, {"referenceID": 53, "context": "Semi-supervised learning [7, 55, 10] is a fundamental tool in machine learning, geared for applications when there are few labeled (seed) examples (xj , yj) j \u2264 n` and many nu n` unlabeled examples xi for i \u2208 (n`, n` + nu].", "startOffset": 25, "endOffset": 36}, {"referenceID": 9, "context": "Semi-supervised learning [7, 55, 10] is a fundamental tool in machine learning, geared for applications when there are few labeled (seed) examples (xj , yj) j \u2264 n` and many nu n` unlabeled examples xi for i \u2208 (n`, n` + nu].", "startOffset": 25, "endOffset": 36}, {"referenceID": 51, "context": "This expression is also known as the Watson Nadaraya estimator [53, 40] which builds on kernel or Parzen-window density estimation [45, 43, 48].", "startOffset": 63, "endOffset": 71}, {"referenceID": 38, "context": "This expression is also known as the Watson Nadaraya estimator [53, 40] which builds on kernel or Parzen-window density estimation [45, 43, 48].", "startOffset": 63, "endOffset": 71}, {"referenceID": 43, "context": "This expression is also known as the Watson Nadaraya estimator [53, 40] which builds on kernel or Parzen-window density estimation [45, 43, 48].", "startOffset": 131, "endOffset": 143}, {"referenceID": 41, "context": "This expression is also known as the Watson Nadaraya estimator [53, 40] which builds on kernel or Parzen-window density estimation [45, 43, 48].", "startOffset": 131, "endOffset": 143}, {"referenceID": 46, "context": "This expression is also known as the Watson Nadaraya estimator [53, 40] which builds on kernel or Parzen-window density estimation [45, 43, 48].", "startOffset": 131, "endOffset": 143}, {"referenceID": 46, "context": "In particular, the expression (2) is a solution of the optimization (1) also when \u03ba is asymmetric [48].", "startOffset": 98, "endOffset": 102}, {"referenceID": 29, "context": "A common approach to enhance such data is to embed the entities in a lower dimensional Euclidean space so that larger inner products, or closer distances, between the embeddings fit the provided interactions [30, 36].", "startOffset": 208, "endOffset": 216}, {"referenceID": 35, "context": "A common approach to enhance such data is to embed the entities in a lower dimensional Euclidean space so that larger inner products, or closer distances, between the embeddings fit the provided interactions [30, 36].", "startOffset": 208, "endOffset": 216}, {"referenceID": 50, "context": "A common technique in semi supervised learning is to sparsify this relation (often without explicitly computing the dense representation) by only retaining the strong relations \u2013 This is done by only including edges for pairs where one is the k nearest neighbor of another (i, j, if wij is one of the top k values in the ith row or wji is one of the top k in the jth row) or by using r-neighborhoods [52, 25, 54, 55].", "startOffset": 400, "endOffset": 416}, {"referenceID": 24, "context": "A common technique in semi supervised learning is to sparsify this relation (often without explicitly computing the dense representation) by only retaining the strong relations \u2013 This is done by only including edges for pairs where one is the k nearest neighbor of another (i, j, if wij is one of the top k values in the ith row or wji is one of the top k in the jth row) or by using r-neighborhoods [52, 25, 54, 55].", "startOffset": 400, "endOffset": 416}, {"referenceID": 52, "context": "A common technique in semi supervised learning is to sparsify this relation (often without explicitly computing the dense representation) by only retaining the strong relations \u2013 This is done by only including edges for pairs where one is the k nearest neighbor of another (i, j, if wij is one of the top k values in the ith row or wji is one of the top k in the jth row) or by using r-neighborhoods [52, 25, 54, 55].", "startOffset": 400, "endOffset": 416}, {"referenceID": 53, "context": "A common technique in semi supervised learning is to sparsify this relation (often without explicitly computing the dense representation) by only retaining the strong relations \u2013 This is done by only including edges for pairs where one is the k nearest neighbor of another (i, j, if wij is one of the top k values in the ith row or wji is one of the top k in the jth row) or by using r-neighborhoods [52, 25, 54, 55].", "startOffset": 400, "endOffset": 416}, {"referenceID": 44, "context": "To visualize a kernel that is accurate for strong relations but not for weak ones, consider points that (approximately) form a dense manifold that lies in a higher dimensional space [46, 52, 55, 47, 5].", "startOffset": 182, "endOffset": 201}, {"referenceID": 50, "context": "To visualize a kernel that is accurate for strong relations but not for weak ones, consider points that (approximately) form a dense manifold that lies in a higher dimensional space [46, 52, 55, 47, 5].", "startOffset": 182, "endOffset": 201}, {"referenceID": 53, "context": "To visualize a kernel that is accurate for strong relations but not for weak ones, consider points that (approximately) form a dense manifold that lies in a higher dimensional space [46, 52, 55, 47, 5].", "startOffset": 182, "endOffset": 201}, {"referenceID": 45, "context": "To visualize a kernel that is accurate for strong relations but not for weak ones, consider points that (approximately) form a dense manifold that lies in a higher dimensional space [46, 52, 55, 47, 5].", "startOffset": 182, "endOffset": 201}, {"referenceID": 4, "context": "To visualize a kernel that is accurate for strong relations but not for weak ones, consider points that (approximately) form a dense manifold that lies in a higher dimensional space [46, 52, 55, 47, 5].", "startOffset": 182, "endOffset": 201}, {"referenceID": 52, "context": "One such objective was proposed in an influential label propagation work by Zhu et al [54, 55].", "startOffset": 86, "endOffset": 94}, {"referenceID": 53, "context": "One such objective was proposed in an influential label propagation work by Zhu et al [54, 55].", "startOffset": 86, "endOffset": 94}, {"referenceID": 3, "context": "Related objectives, adsorption and modified adsorption were studied for YouTube recommendations and named-entity recognition [4, 50, 51].", "startOffset": 125, "endOffset": 136}, {"referenceID": 48, "context": "Related objectives, adsorption and modified adsorption were studied for YouTube recommendations and named-entity recognition [4, 50, 51].", "startOffset": 125, "endOffset": 136}, {"referenceID": 49, "context": "Related objectives, adsorption and modified adsorption were studied for YouTube recommendations and named-entity recognition [4, 50, 51].", "startOffset": 125, "endOffset": 136}, {"referenceID": 10, "context": "Other interpretations of the solution are as a fixed point of a stochastic sharing process or the landing probability of a random walk [11, 29].", "startOffset": 135, "endOffset": 143}, {"referenceID": 28, "context": "Other interpretations of the solution are as a fixed point of a stochastic sharing process or the landing probability of a random walk [11, 29].", "startOffset": 135, "endOffset": 143}, {"referenceID": 21, "context": "The computation of each gradient update is linear in the number of edges, but typically, hundreds of iterations are needed even with various optimizations [22].", "startOffset": 155, "endOffset": 159}, {"referenceID": 18, "context": "A further optimization sparsifies the set of unlabeled points using a smaller set of anchors that is large enough to preserve the short-range structure but is much smaller than the full set [19, 33].", "startOffset": 190, "endOffset": 198}, {"referenceID": 32, "context": "A further optimization sparsifies the set of unlabeled points using a smaller set of anchors that is large enough to preserve the short-range structure but is much smaller than the full set [19, 33].", "startOffset": 190, "endOffset": 198}, {"referenceID": 10, "context": "A related and hugely successful set of techniques, which work with directed relations, are Personalized PageRank (PPR) and derivatives [11, 41, 27].", "startOffset": 135, "endOffset": 147}, {"referenceID": 39, "context": "A related and hugely successful set of techniques, which work with directed relations, are Personalized PageRank (PPR) and derivatives [11, 41, 27].", "startOffset": 135, "endOffset": 147}, {"referenceID": 26, "context": "A related and hugely successful set of techniques, which work with directed relations, are Personalized PageRank (PPR) and derivatives [11, 41, 27].", "startOffset": 135, "endOffset": 147}, {"referenceID": 33, "context": "To apply it to our context, however, we need to estimate visiting probabilities from all unlabeled i to sufficiently many seeds, and scalability is a game stopper even with state of the art techniques [34].", "startOffset": 201, "endOffset": 205}, {"referenceID": 33, "context": "This means that any algorithm from basic Monte Carlo generation of walks to the bidirectional approach of [34] would spend most of its \u201cwork\u201d traversing non-seed nodes.", "startOffset": 106, "endOffset": 110}, {"referenceID": 31, "context": "Another more scalable use of PPR is performed on the \u201ctransposed\u201d graph [32]: All seeds with the same label are grouped and PPR is then personalized to each label, and applied to obtain similarities from labels to nodes.", "startOffset": 72, "endOffset": 76}, {"referenceID": 19, "context": "Our reach diffusion model is inspired by popular information diffusion models [20, 28] and by reliability or survival analysis [37, 31] that is extensively used to analyze engineered and biological systems.", "startOffset": 78, "endOffset": 86}, {"referenceID": 27, "context": "Our reach diffusion model is inspired by popular information diffusion models [20, 28] and by reliability or survival analysis [37, 31] that is extensively used to analyze engineered and biological systems.", "startOffset": 78, "endOffset": 86}, {"referenceID": 36, "context": "Our reach diffusion model is inspired by popular information diffusion models [20, 28] and by reliability or survival analysis [37, 31] that is extensively used to analyze engineered and biological systems.", "startOffset": 127, "endOffset": 135}, {"referenceID": 30, "context": "Our reach diffusion model is inspired by popular information diffusion models [20, 28] and by reliability or survival analysis [37, 31] that is extensively used to analyze engineered and biological systems.", "startOffset": 127, "endOffset": 135}, {"referenceID": 19, "context": "Influence diffusion, as motivated by Richardson and Domingos [20] and formalized by Kempe, Kleinberg, and Tardos [28], is defined for a network of directed pairwise interactions between entities.", "startOffset": 61, "endOffset": 65}, {"referenceID": 27, "context": "Influence diffusion, as motivated by Richardson and Domingos [20] and formalized by Kempe, Kleinberg, and Tardos [28], is defined for a network of directed pairwise interactions between entities.", "startOffset": 113, "endOffset": 117}, {"referenceID": 27, "context": "Independent Cascade (IC) [28], which uses independent activation probabilities pe to edges, is the simplest and most studied model.", "startOffset": 25, "endOffset": 29}, {"referenceID": 27, "context": "Note that we can express the IC model of [28] in terms of this reliability formulation by choosing independent lifetime variables that are exponential random variables with parameter 1/pe \u03bce \u223c Exp[1/pe].", "startOffset": 41, "endOffset": 45}, {"referenceID": 23, "context": "Our distance diffusion kernels are inspired by a generalization, first proposed by Gomez-Rodriguez et al [24, 21, 17] of the influence model of Kempe et al [28] to a distance-based setting.", "startOffset": 105, "endOffset": 117}, {"referenceID": 20, "context": "Our distance diffusion kernels are inspired by a generalization, first proposed by Gomez-Rodriguez et al [24, 21, 17] of the influence model of Kempe et al [28] to a distance-based setting.", "startOffset": 105, "endOffset": 117}, {"referenceID": 16, "context": "Our distance diffusion kernels are inspired by a generalization, first proposed by Gomez-Rodriguez et al [24, 21, 17] of the influence model of Kempe et al [28] to a distance-based setting.", "startOffset": 105, "endOffset": 117}, {"referenceID": 27, "context": "Our distance diffusion kernels are inspired by a generalization, first proposed by Gomez-Rodriguez et al [24, 21, 17] of the influence model of Kempe et al [28] to a distance-based setting.", "startOffset": 156, "endOffset": 160}, {"referenceID": 17, "context": "They are also inspired by models of distance-based utility in networks [18, 6, 26] where the relevance of a node to another node decreases with the distance between them.", "startOffset": 71, "endOffset": 82}, {"referenceID": 5, "context": "They are also inspired by models of distance-based utility in networks [18, 6, 26] where the relevance of a node to another node decreases with the distance between them.", "startOffset": 71, "endOffset": 82}, {"referenceID": 25, "context": "They are also inspired by models of distance-based utility in networks [18, 6, 26] where the relevance of a node to another node decreases with the distance between them.", "startOffset": 71, "endOffset": 82}, {"referenceID": 0, "context": "A choice of Weibull distributed lengths with scale parameter equal to the inverse significance seems particularly natural [1, 21, 15]: The closest out connection from a node corresponds to the minimum length of an out edge.", "startOffset": 122, "endOffset": 133}, {"referenceID": 20, "context": "A choice of Weibull distributed lengths with scale parameter equal to the inverse significance seems particularly natural [1, 21, 15]: The closest out connection from a node corresponds to the minimum length of an out edge.", "startOffset": 122, "endOffset": 133}, {"referenceID": 14, "context": "A choice of Weibull distributed lengths with scale parameter equal to the inverse significance seems particularly natural [1, 21, 15]: The closest out connection from a node corresponds to the minimum length of an out edge.", "startOffset": 122, "endOffset": 133}, {"referenceID": 11, "context": "Our scalable approximation relies on a sketching technique of reachability sets and of neighborhoods of directed graphs [12, 13].", "startOffset": 120, "endOffset": 128}, {"referenceID": 12, "context": "Our scalable approximation relies on a sketching technique of reachability sets and of neighborhoods of directed graphs [12, 13].", "startOffset": 120, "endOffset": 128}, {"referenceID": 11, "context": "To do so, we design a threshold sketching algorithm which builds on the basic distance-sketching design [12, 13] but replaces the shortest-path searches by \u201csurvival threshold\u201d graph searches as a basic component.", "startOffset": 104, "endOffset": 112}, {"referenceID": 12, "context": "To do so, we design a threshold sketching algorithm which builds on the basic distance-sketching design [12, 13] but replaces the shortest-path searches by \u201csurvival threshold\u201d graph searches as a basic component.", "startOffset": 104, "endOffset": 112}, {"referenceID": 11, "context": "We present our approach for computing the labels approximately, using Monte Carlo simulations and a novel use of sample-based sketches [12, 13] to approximate the results of each simulation.", "startOffset": 135, "endOffset": 143}, {"referenceID": 12, "context": "We present our approach for computing the labels approximately, using Monte Carlo simulations and a novel use of sample-based sketches [12, 13] to approximate the results of each simulation.", "startOffset": 135, "endOffset": 143}, {"referenceID": 0, "context": "This is an immediate consequence of Hoeffding\u2019s inequality, noting that entries of our label vectors are in [0, 1].", "startOffset": 108, "endOffset": 114}, {"referenceID": 11, "context": "The sketches we will use are MinHash and All-Distances Sketches (ADS), using state of the art optimal estimators [12, 13, 14].", "startOffset": 113, "endOffset": 125}, {"referenceID": 12, "context": "The sketches we will use are MinHash and All-Distances Sketches (ADS), using state of the art optimal estimators [12, 13, 14].", "startOffset": 113, "endOffset": 125}, {"referenceID": 13, "context": "The sketches we will use are MinHash and All-Distances Sketches (ADS), using state of the art optimal estimators [12, 13, 14].", "startOffset": 113, "endOffset": 125}, {"referenceID": 11, "context": "To simplify and unify the presentation, we use bottom-k alldistances sketches [12, 13, 14] for the two uses of sketches.", "startOffset": 78, "endOffset": 90}, {"referenceID": 12, "context": "To simplify and unify the presentation, we use bottom-k alldistances sketches [12, 13, 14] for the two uses of sketches.", "startOffset": 78, "endOffset": 90}, {"referenceID": 13, "context": "To simplify and unify the presentation, we use bottom-k alldistances sketches [12, 13, 14] for the two uses of sketches.", "startOffset": 78, "endOffset": 90}, {"referenceID": 12, "context": "See discussion in [13] for the handling of general m.", "startOffset": 18, "endOffset": 22}, {"referenceID": 11, "context": "They compute the sketches or the more restricted application of neighborhood size estimates [12, 42, 8, 9, 13].", "startOffset": 92, "endOffset": 110}, {"referenceID": 40, "context": "They compute the sketches or the more restricted application of neighborhood size estimates [12, 42, 8, 9, 13].", "startOffset": 92, "endOffset": 110}, {"referenceID": 7, "context": "They compute the sketches or the more restricted application of neighborhood size estimates [12, 42, 8, 9, 13].", "startOffset": 92, "endOffset": 110}, {"referenceID": 8, "context": "They compute the sketches or the more restricted application of neighborhood size estimates [12, 42, 8, 9, 13].", "startOffset": 92, "endOffset": 110}, {"referenceID": 12, "context": "They compute the sketches or the more restricted application of neighborhood size estimates [12, 42, 8, 9, 13].", "startOffset": 92, "endOffset": 110}, {"referenceID": 12, "context": "Most of these approaches can be easily adapted to estimate m(R\u03c4 (i)), when mi \u2208 {0, 1} (see discussion in [13]) and there is a variation [13] that is suitable for general m.", "startOffset": 106, "endOffset": 110}, {"referenceID": 12, "context": "Most of these approaches can be easily adapted to estimate m(R\u03c4 (i)), when mi \u2208 {0, 1} (see discussion in [13]) and there is a variation [13] that is suitable for general m.", "startOffset": 137, "endOffset": 141}, {"referenceID": 12, "context": "The component of obtaining the sample and probabilities is more subtle, but uses the same computation (See [13, 14]).", "startOffset": 107, "endOffset": 115}, {"referenceID": 13, "context": "The component of obtaining the sample and probabilities is more subtle, but uses the same computation (See [13, 14]).", "startOffset": 107, "endOffset": 115}, {"referenceID": 11, "context": "The sketching algorithm we present here for survival thresholds builds on a sequential algorithm for ADS computation which is based on performing pruned Dijkstra searches [12, 13].", "startOffset": 171, "endOffset": 179}, {"referenceID": 12, "context": "The sketching algorithm we present here for survival thresholds builds on a sequential algorithm for ADS computation which is based on performing pruned Dijkstra searches [12, 13].", "startOffset": 171, "endOffset": 179}, {"referenceID": 8, "context": "The algorithm has a parallel version designed to run on multi-core architectures [9].", "startOffset": 81, "endOffset": 84}, {"referenceID": 13, "context": "The pseudocode provided as Algorithm 2 builds on a state of the art design for computing universal monotone multi-objective samples [14].", "startOffset": 132, "endOffset": 136}, {"referenceID": 2, "context": "We performed some preliminary experiments using the Movielens 1M [39] and political blogs [3] datasets.", "startOffset": 90, "endOffset": 93}, {"referenceID": 40, "context": "Our evaluation here is not meant to assess scalability, as there are several highly scalable implementation of the basic distance and reachability sketching methods we use as our main component [42, 8, 16, 23, 17, 9].", "startOffset": 194, "endOffset": 216}, {"referenceID": 7, "context": "Our evaluation here is not meant to assess scalability, as there are several highly scalable implementation of the basic distance and reachability sketching methods we use as our main component [42, 8, 16, 23, 17, 9].", "startOffset": 194, "endOffset": 216}, {"referenceID": 15, "context": "Our evaluation here is not meant to assess scalability, as there are several highly scalable implementation of the basic distance and reachability sketching methods we use as our main component [42, 8, 16, 23, 17, 9].", "startOffset": 194, "endOffset": 216}, {"referenceID": 22, "context": "Our evaluation here is not meant to assess scalability, as there are several highly scalable implementation of the basic distance and reachability sketching methods we use as our main component [42, 8, 16, 23, 17, 9].", "startOffset": 194, "endOffset": 216}, {"referenceID": 16, "context": "Our evaluation here is not meant to assess scalability, as there are several highly scalable implementation of the basic distance and reachability sketching methods we use as our main component [42, 8, 16, 23, 17, 9].", "startOffset": 194, "endOffset": 216}, {"referenceID": 8, "context": "Our evaluation here is not meant to assess scalability, as there are several highly scalable implementation of the basic distance and reachability sketching methods we use as our main component [42, 8, 16, 23, 17, 9].", "startOffset": 194, "endOffset": 216}, {"referenceID": 0, "context": "length distribution Exp[1] regardless of |\u0393(v)|.", "startOffset": 23, "endOffset": 26}, {"referenceID": 1, "context": "With g(x) = 1/ log(x) we obtain the Adamic-Adar similarity [2] popular in social network analysis.", "startOffset": 59, "endOffset": 62}, {"referenceID": 9, "context": "We then take seed sets S of size s \u2208 [10, 1000] as prefixes of the same permutation.", "startOffset": 37, "endOffset": 47}, {"referenceID": 47, "context": "With spectral label learning, this sketching approach was first applied by [49], which used Count-min sketches, and by [44], which used a composable version of Misra Gries sketches [38, 35].", "startOffset": 75, "endOffset": 79}, {"referenceID": 42, "context": "With spectral label learning, this sketching approach was first applied by [49], which used Count-min sketches, and by [44], which used a composable version of Misra Gries sketches [38, 35].", "startOffset": 119, "endOffset": 123}, {"referenceID": 37, "context": "With spectral label learning, this sketching approach was first applied by [49], which used Count-min sketches, and by [44], which used a composable version of Misra Gries sketches [38, 35].", "startOffset": 181, "endOffset": 189}, {"referenceID": 34, "context": "With spectral label learning, this sketching approach was first applied by [49], which used Count-min sketches, and by [44], which used a composable version of Misra Gries sketches [38, 35].", "startOffset": 181, "endOffset": 189}], "year": 2017, "abstractText": "Semi-supervised learning algorithms are an indispensable tool when labeled examples are scarce and there are many unlabeled examples [Blum and Chawla 2001, Zhu et. al. 2003]. With graph-based methods, entities (examples) correspond to nodes in a graph and edges correspond to related entities. The graph structure is used to infer implicit pairwise affinity values (kernel) which are used to compute the learned labels. Two powerful techniques to define such a kernel are \u201csymmetric\u201d spectral methods and Personalized Page Rank (PPR). With spectral methods, labels can be scalably learned using Jacobi iterations, but an inherent limiting issue is that they are applicable to symmetric (undirected) graphs, whereas often, such as with like, follow, or hyperlinks, relations between entities are inherently asymmetric. PPR naturally works with directed graphs but even with state of the art techniques does not scale when we want to learn billions of labels. Aiming at both high scalability and handling of directed relations, we propose here Reach Diffusion and Distance Diffusion kernels. Our design is inspired by models for influence diffusion in social networks, formalized and spawned from the seminal work of [Kempe, Kleinberg, and Tardos 2003]. These models apply with directed interactions and are naturally suited for asymmetry. We tailor these models to define a natural asymmetric \u201ckernel\u201d and design highly scalable algorithms for parameter setting and label learning.", "creator": "LaTeX with hyperref package"}}}