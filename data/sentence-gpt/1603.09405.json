{"id": "1603.09405", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "30-Mar-2016", "title": "Enhancing Sentence Relation Modeling with Auxiliary Character-level Embedding", "abstract": "Neural network based approaches for sentence relation modeling automatically generate hidden matching features from raw sentence pairs. However, the quality of matching feature representation may not be satisfied due to complex semantic relations such as entailment or contradiction. To address this challenge, we propose a new deep neural network architecture that jointly leverage pre-trained word embedding and auxiliary character embedding to learn sentence meanings. The two kinds of word sequence representations as inputs into multi-layer bidirectional LSTM to learn enhanced sentence representation. After that, we construct matching features followed by another temporal CNN to learn high-level hidden matching feature representations. Experimental results demonstrate that our approach consistently outperforms the existing methods on standard evaluation datasets.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n", "histories": [["v1", "Wed, 30 Mar 2016 22:39:59 GMT  (1224kb,D)", "http://arxiv.org/abs/1603.09405v1", null]], "reviews": [], "SUBJECTS": "cs.CL cs.AI cs.NE", "authors": ["peng li", "heng huang"], "accepted": false, "id": "1603.09405"}, "pdf": {"name": "1603.09405.pdf", "metadata": {"source": "CRF", "title": "Enhancing Sentence Relation Modeling with Auxiliary Character-level Embedding", "authors": ["Peng Li", "Heng Huang"], "emails": ["jerryli1981@gmail.com", "heng@uta.edu"], "sections": [{"heading": "1 Introduction", "text": "Traditional approaches (Lai and Hockenmaier, 2014; Zhao et al., 2014; Jimenez et al., 2014) for sentence relation modeling tasks such as paraphrase identification, question answering, recognized textual entailment and semantic textual similarity prediction usually build the supervised model using a variety of hand crafted features. Hundreds of features generated at different linguistic levels are exploited to boost classification. With the success of deep learning, there has been much interest in applying deep neural network based techniques to further improve the prediction performances (Socher et al., 2011b; Iyyer et al., 2014; Yin and Schutze, 2015).\nA key component of deep neural network is\nword embedding which serve as an lookup table to get word representations. From low level NLP tasks such as language modeling, POS tagging, name entity recognition, and semantic role labeling (Collobert et al., 2011; Mikolov et al., 2013), to high level tasks such as machine translation, information retrieval and semantic analysis (Kalchbrenner and Blunsom, 2013; Socher et al., 2011a; Tai et al., 2015). Deep word representation learning has demonstrated its importance for these tasks. All the tasks get performance improvement via further learning either word level representations or sentence level representations. On the other hand, some researchers have found character-level convolutional networks (Kim et al., 2016; Zhang et al., 2015) are useful in extracting information from raw signals for the task such as language modeling or text classification.\nIn this work, we focus on deep neural network based sentence relation modeling tasks. We explore treating each sentence as a kind of raw signal at character level, and applying temporal (one-dimensional) Convolution Neural Network (CNN) (Collobert et al., 2011), Highway Multilayer Perceptron (HMLP) and multi-layer bidirectional LSTM (Long Short Term Memory) (Graves et al., 2013) to learn sentence representations. We propose a new deep neural network architecture that jointly leverage pre-trained word embedding and character embedding to represent the meaning sentences. More specifically, our new approach first generates two kinds of word sequence representations. One kind of sequence representations are the composition of pre-trained word vectors. The other kind of sequence representation comprise word vectors that generating from character-level convolutional network. We then inject the two sequence representations into bidirectional LSTM, which means forward directional LSTM accept pre-trained word embedding output and backward directional LSTM accept auxar X iv :1 60 3. 09 40\n5v 1\n[ cs\n.C L\n] 3\n0 M\nar 2\n01 6\niliary character CNN embedding output. The final sentence representation is the concatenation of the two direction. After that, we construct matching features followed by another temporal CNN to learn high-level hidden matching feature representations. Figure 1 shows the neural network architecture for general sentence relation modeling.\nOur model shows that when trained on small size datasets, combining pre-trained word embeddings with auxiliary character-level embedding can improve the sentence representation. Word embeddings can help capturing general word semantic meanings, whereas char-level embedding can help modeling task specific word meanings. Note that auxiliary character-level embedding based sentence representation do not require the knowledge of words or even syntactic structure of a language. The enhanced sentence representation generated by multi-layer bidirectional LSTM will encapsulate the character and word levels informations. Furthermore, it may enhance matching features that generated by computing similarity measures on sentence pairs. Quantitative evaluations on standard dataset demonstrate the effectiveness and advantages of our method."}, {"heading": "2 Character-level Convolutional Neural Network", "text": "Besides pre-trained word vectors, we are also interested in generating word vectors from characters. To achieve that, we leverage deep convolutional neural network(ConvNets). The model accepts a sequence of encoded characters as input. The encoding si done by prescribing an alphabet of size m for the input language, and then quantize each character using one-hot encoding. Then, the sequence of characters is transformed to a sequence of such m sized vectors with fixed length l0. Any character exceeding length l0 is ignored, and any characters that are not in the alphabet are quantized as all-zero vectors. The alphabet used in our model consists of 36 characters, including 26 english letters and 10 digits. Below, we will introduce character-level temporal convolution neural network."}, {"heading": "2.1 Temporal Convolution", "text": "Temporal Convolution applies one-dimensional convolution over an input sequence. The onedimensional convolution is an operation between a vector of weights m \u2208 Rm and a vector of inputs viewed as a sequence x \u2208 Rn. The vector m is the filter of the convolution. Concretely, we think of x as the input token and xi \u2208 R as a single feature value associated with the i-th character in this token. The idea behind the one-dimensional convolution is to take the dot product of the vector m with each m-gram in the token x to obtain another sequence c:\ncj = m Txj\u2212m+1:j . (1)\nUsually, xi is not a single value, but a ddimensional vector so that x \u2208 Rd\u00d7n. There exist two types of 1d convolution operations. One is called Time Delay Neural Networks (TDNNs). The other one was introduced by (Collobert et al., 2011). In TDNN, weights m \u2208 Rd\u00d7m form a matrix. Each row of m is convolved with the corresponding row of x. In (Collobert et al., 2011) architecture, a sequence of length n is represented as:\nx1:n = x1 \u2295 x2 \u00b7 \u00b7 \u00b7 \u2295 xn , (2)\nwhere \u2295 is the concatenation operation. In general, let xi:i+j refer to the concatenation of characters xi,xi+1, . . . ,xi+j . A convolution operation involves a filter w \u2208 Rhk, which is applied to a\nwindow of h characters to produce the new feature. For example, a feature ci is generated from a window of characters xi:i+h\u22121 by:\nci = f(w \u00b7 xi:i+h\u22121 + b) . (3)\nHere b \u2208 R is a bias term and f is a nonlinear function such as the thresholding function f(x) = max{0, x}. This filter is applied to each possible window of characters in the sequence {x1:h,x2:h+1, . . . ,xn\u2212h+1:n} to produce a feature map:\nc = [c1, c2, . . . , cn\u2212h+1] , (4)\nwith c \u2208 Rn\u2212h+1."}, {"heading": "2.2 Highway MLP", "text": "On top of convolutional neural network layers, we build another Highway Multilayer Perceptron (HMLP) layer to further enhance character-level word embeddings. Conventional MLP applies an affine transformation followed by a nonlinearity to obtain a new set of features:\nz = g(Wy + b) . (5)\nOne layer of a highway network does the following:\nz = t g(WHy + bH) + (1\u2212 t) y , (6)\nwhere g is a nonlinearity, t = \u03c3(WTy + bT ) is called as the transform gate, and (1 \u2212 t) is called as the carry gate. Similar to the memory cells in LSTM networks, highway layers allow adaptively carrying some dimensions of the input directly to the input for training deep networks."}, {"heading": "3 Multi-Layer Bidirectional LSTM", "text": "Now that we have two kinds of word sequence representations. One kind of sequence representations are the composition of pre-trained word vectors. The other kind of sequence representation comprise word vectors that generating from character-level convolutional network. We can inject the two sequence representations into bidirectional LSTM to learn sentence representation. More specifically, forward directional LSTM accept pre-trained word embedding output and backward directional LSTM accept character CNN embedding output. The final sentence representation is the concatenation of the two direction."}, {"heading": "3.1 RNN vs LSTM", "text": "Recurrent neural networks (RNNs) are capable of modeling sequences of varying lengths via the recursive application of a transition function on a hidden state. For example, at each time step t, an RNN takes the input vector xt \u2208 Rn and the hidden state vector ht\u22121 \u2208 Rm, then applies affine transformation followed by an element-wise nonlinearity such as hyperbolic tangent function to produce the next hidden state vector ht:\nht = tanh(Wxt +Uht\u22121 + b) . (7)\nA major issue of RNNs using these transition functions is that it is difficult to learn longrange dependencies during training step because the components of the gradient vector can grow or decay exponentially (Bengio et al., 1994).\nThe LSTM architecture (Hochreiter and Schmidhuber, 1998) addresses the problem of learning long range dependencies by introducing a memory cell that is able to preserve state over long periods of time. Concretely, at each time step t, the LSTM unit can be defined as a collection of vectors in Rd: an input gate it, a forget gate ft, an output gate ot, a memory cell ct and a hidden state ht. We refer to d as the memory dimensionality of the LSTM. One step of an LSTM takes as input xt, ht\u22121, ct\u22121 and produces ht, ct via the following transition equations:\nit = \u03c3(W (i)xt +U (i)ht\u22121 + b (i)) ,\nft = \u03c3(W (f)xt +U (f)ht\u22121 + b (f)) ,\not = \u03c3(W (o)xt +U (o)ht\u22121 + b (o)) ,\nut = tanh(W (u)xt +U (u)ht\u22121 + b (u)) ,\nct = it ut + ft ct\u22121 , ht = ot tanh(ct) ,\n(8)\nwhere \u03c3(\u00b7) and tanh(\u00b7) are the element-wise sigmoid and hyperbolic tangent functions, is the element-wise multiplication operator."}, {"heading": "3.2 Model Description", "text": "One shortcoming of conventional RNNs is that they are only able to make use of previous context. In text entailment, the decision is made after the whole sentence pair is digested. Therefore, exploring future context would be better for sequence meaning representation. Bidirectional RNNs architecture (Graves et al., 2013) proposed a solution of making prediction based on future words.\nAt each time step t, the model maintains two hidden states, one for the left-to-right propagation \u2212\u2192 ht and the other for the right-to-left propagation \u2190\u2212 ht. The hidden state of the Bidirectional LSTM is the concatenation of the forward and backward hidden states. The following equations illustrate the main ideas:\n\u2212\u2192 ht = tanh( \u2212\u2192 Wxt + \u2212\u2192 U \u2212\u2192 h t\u22121 + \u2212\u2192 b ) \u2190\u2212 ht = tanh( \u2190\u2212 Wxt + \u2190\u2212 U \u2190\u2212 h t+1 + \u2190\u2212 b ) .\n(9)\nDeep RNNs can be created by stacking multiple RNN hidden layer on top of each other, with the output sequence of one layer forming the input sequence for the next. Assuming the same hidden layer function is used for all N layers in the stack, the hidden vectors hn are iteratively computed from n = 1 to N and t = 1 to T :\nhnt = tanh(Wh n\u22121 t +Uh n t\u22121 + b) . (10)\nMultilayer bidirectional RNNs can be implemented by replacing each hidden vector hn with the forward and backward vectors \u2212\u2192 hn and \u2190\u2212 hn, and ensuring that every hidden layer receives input from both the forward and backward layers at the level below. Furthermore, we can apply LSTM memory cell to hidden layers to construct multilayer bidirectional LSTM.\nFinally, we can concatenate sequence hidden matrix \u2212\u2192 M \u2208 Rn\u00d7d and reversed sequence hidden matrix \u2190\u2212 M \u2208 Rn\u00d7d to form the sentence representation. We refer to n is the number of layers, d as the memory dimensionality of the LSTM. In the next section, we will use the two matrixs to generate matching feature planes via linear algebra operations."}, {"heading": "4 Learning from Matching Features", "text": "Inspired by (Tai et al., 2015), we apply elementwise merge to first sentence matrix M1 \u2208 Rn\u00d72d and second sentence matrix M2 \u2208 Rn\u00d72d. Similar to previous method, we can define two simple matching feature planes (FPs) with below equations:\nFP1 =M1 M2 , FP2 = |M1 \u2212M2| ,\n(11)\nwhere is the element-wise multiplication. The FP1 measure can be interpreted as an elementwise comparison of the signs of the input representations. The FP2 measure can be interpreted as the distance between the input representations.\nIn addition to the above measures, we also found the following feature plane can improve the performance:\nFP3 = 1dConv(Reshape(Join(M1,M2))) , (12)\nIn FP3, the 1dConv means one-dimensional convolution. Join mean concatenate the two representation. The intuition behind FP3 is let the onedimensional convolution preserves the common information between sentence pairs."}, {"heading": "4.1 Reshape Feature Planes", "text": "Recall that the multi-layer bidirectional LSTM generates sentence representation matrix M \u2208 Rn\u00d72d by concatenating sentence hidden matrix\u2212\u2192 M \u2208 Rn\u00d7d and reversed sentence hidden matrix\u2190\u2212 M \u2208 Rn\u00d7d. Then we conduct element-wise merge to form feature plane Mfp \u2208 Rn\u00d72d. Therefore, the final input into temporal convolution layer is a 3D tensor I \u2208 Rf\u00d7n\u00d72d, where f is the number of matching feature plane, n is the number of layers, d as the memory dimensionality of the LSTM. Note that the 3D tensor convolutional layer input I can be viewed as an image where each feature plane is a channel. In computer vision and image processing communities, the spatial 2D convolution is often used over an input image composed of several input planes. In experiment section, we will compare 2D convolution with 1D convolution. In order to facilitate temporal convolution, we need reshape I to 2D tensor."}, {"heading": "4.2 CNN Topology", "text": "The matching feature planes can be viewed as channels of images in image processing. In our scenario, these feature planes hold the matching information. We will use temporal convolutional neural network to learn hidden matching features. The mechanism of temporal CNN here is the same as character-level temporal CNN. However, the kernels are totally different.\nIt\u2019s quite important to design a good topology for CNN to learn hidden features from heterogeneous feature planes. After several experiments, we found two topological graphs can be deployed in the architecture. Figure 2 and Figure 3 show the two CNN graphs. In Topology I, we stack temporal convolution with kernel width as 1 and tanh activation on top of each feature plane. After that, we deploy another temporal convolution and\ntanh activation operation with kernel width as 2. In Topology II, however, we first stack temporal convolution and tanh activation with kernel width as 2. Then we deploy another temporal convolution and tanh activation operation with kernel width as 1. Experiment results demonstrate that the Topology I is slightly better than the Topology II. This conclusion is reasonable. The feature planes are heterogeneous. After conducting convolution and tanh activation transformation, it makes sense to compare values across different feature planes."}, {"heading": "5 Experiments", "text": "We selected two related sentence relation modeling tasks: semantic relatedness task, which measures the degree of semantic relatedness of a sentence pair by assigning a relatedness score ranging from 1 (completely unrelated) to 5 ( very related); and textual entailment task, which determines whether the truth of a text entails the truth of another text called hypothesis. We use standard SICK (Sentences Involving Compositional Knowledge) dataset 1 for evaluation. It consists of about 10,000 English sentence pairs annotated for relatedness in meaning and entailment."}, {"heading": "5.1 Hyperparameters and Training Details", "text": "We first initialize our word representations using publicly available 300-dimensional Glove word vectors 2. LSTM memory dimension is 100, the number of layers is 2. On the other hand, for CharCNN model we use threshold activation function on top of each temporal convolution and max pooling pairs . The CharCNN input frame size equals alphabet size, output frame size is 100. The maximum sentence length is 37. The kernel width of each temporal convolution is set to 3, the step is 1, the hidden units of HighwayMLP is 50. Training is done through stochastic gradient descent over shuffled mini-batches with the AdaGrad update rule (Duchi et al., 2011). The learning rate is set to 0.05. The mini-batch size is 25. The model parameters were regularized with a per-minibatch L2 regularization strength of 10\u22124. Note that word embeddings were fixed during training.\n1http://alt.qcri.org/semeval2014/ task1/index.php?id=data-and-tools\n2http://nlp.stanford.edu/projects/ glove/"}, {"heading": "5.2 Objective Functions", "text": "The task of semantic relatedness prediction tries to measure the degree of semantic relatedness of a sentence pair by assigning a relatedness score ranging from 1 (completely unrelated) to 5 (very related). More formally, given a sentence pair, we wish to predict a real-valued similarity score in a range of [1,K], where K > 1 is an integer. The sequence 1, 2, ...,K is the ordinal scale of similarity, where higher scores indicate greater degrees of similarity. We can predict the similarity score y\u0302 by predicting the probability that the learned hidden representation xh belongs to the ordinal scale. This is done by projecting an input representation onto a set of hyperplanes, each of which corresponds to a class. The distance from the input to a hyperplane reflects the probability that the input will located in corresponding scale.\nMathematically, the similarity score y\u0302 can be written as:\ny\u0302 = rT \u00b7 p\u0302\u03b8(y|xh) = rT \u00b7 softmax(W \u00b7 xh + b)\n= rT \u00b7 e Wixh+bi\u2211 j e Wjxh+bj (13)\nwhere rT = [1 2 . . .K] and the weight matrix W and b are parameters.\nIn order to introduce the task objective function, we define a sparse target distribution p that satisfies y = rT p:\npi =  y \u2212 byc, i = byc+ 1 byc \u2212 y + 1, i = byc 0 otherwise\n(14)\nwhere 1 \u2264 i \u2264 K. The objective function then can be defined as the regularized KL-divergence between p and p\u03b8:\nJ(\u03b8) = \u2212 1 m m\u2211 k=1 KL(p(k)||pk\u03b8) + \u03bb 2 ||\u03b8||22 , (15)\nwhere m is the number of training pairs and the superscript k indicates the k-th sentence pair (Tai et al., 2015).\nReferring to textual entailment recognition task, we want to maximize the likelihood of the correct class. This is equivalent to minimizing the negative log-likelihood (NLL). More specifically, the label y\u0302 given the inputs xh is predicted by a softmax classifier that takes the hidden state hj at the\nnode as input:\np\u0302\u03b8(y|xh) = softmax(W \u00b7 xh + b) y\u0302 = argmax\ny p\u0302\u03b8(y|xh) (16)\nAfter that, the objective function is the negative log-likelihood of the true class labels yk:\nJ(\u03b8) = \u2212 1 m m\u2211 k=1 log p\u0302\u03b8(y k|xkh) + \u03bb 2 ||\u03b8||22 , (17)\nwhere m is the number of training pairs and the superscript k indicates the kth sentence pair."}, {"heading": "5.3 Results and Discussions", "text": "Table 1 and 2 show the Pearson correlation and accuracy comparison results of semantic relatedness and text entailment tasks. We can see that combining CharCNN with multi-layer bidirectional LSTM yields better performance compared with other traditional machine learning methods such as SVM and MaxEnt approach (Proisl and Evert, 2014; Lai and Hockenmaier, 2014) that served with many handcraft features. Note that our method doesn\u2019t need extra handcrafted feature extraction procedure. Also our method doesn\u2019t leverage external linguistic resources such as wordnet or parsing which get best results in (Tai et al., 2015). More importantly, both task prediction results close to the state-of-the-art results. It proved that our approaches successfully simultaneously predict heterogeneous tasks. Note that for semantic relatedness task, the latest research (Tai et al.,\n2015) proposed a tree-structure based LSTM, the Pearson correlation score of their system can reach 0.863. Compared with their approach, our method didn\u2019t use dependency parsing and can be used to predict tasks contains multiple languages.\nWe hope to point out that we implemented the method in (Tai et al., 2015), but the results are not as good as our method. Here we use the results reported in their paper. Based on our experiments, we believe the method in (Tai et al., 2015) is very sensitive to the initializations, thus it may not achieve the good performance in different settings. However, our method is pretty stable which may benefit from the joint tasks training."}, {"heading": "5.4 Tree LSTM vs Sequence LSTM", "text": "In this experiment, we will compare tree LSTM with sequential LSTM. A limitation of the sequence LSTM architectures is that they only allow for strictly sequential information propagation. However, tree LSTMs allow richer network topologies where each LSTM unit is able to incorporate information from multiple child units. As in standard LSTM units, each Tree-LSTM unit (indexed by j) contains input and output gates ij and oj , a memory cell cj and hidden state hj . The difference between the standard LSTM unit and tree LSTM units is that gating vectors and memory cell updates are dependent on the states of possibly many child units. Additionally, instead of a single forget gate, the tree LSTM unit contains one forget gate fjk for each child k. This allows the tree LSTM unit to selectively incorporate information\nfrom each child. We use dependency tree child-sum tree LSTM proposed by (Tai et al., 2015) as our baseline. Given a tree, let C(j) denote the set of children of node j. The child-sum tree LSTM transition equations are the following:\nh\u0303j = \u2211\nk\u2208C(j)\nhk ,\nij = \u03c3(W (i)xj +U (i)h\u0303j + b (i)) ,\nfjk = \u03c3(W (f)xj +U (f)hk + b (f)) ,\noj = \u03c3(W (o)xj +U (o)h\u0303j + b (o)) ,\nuj = tanh(W (u)xj +U (u)h\u0303j + b (u)) ,\ncj = ij uj + fjk ck , hj = oj tanh(cj) . (18)\nTable 3 show the comparisons between tree and sequential based methods. We can see that, if we don\u2019t deploy CNN, simple Tree LSTM yields better result than traditional LSTM, but worse than Bidirectional LSTM. This is reasonable due to the fact that Bidirectional LSTM can enhance sentence representation by concatenating forward and backward representations. We found that adding CNN layer will decrease the accuracy in this scenario. Because when feeding into CNN, we have to reshape the feature planes otherwise convolution will not work. For example, we set convolution kernel width as 2, the input 2D tensor will\nhave the shape lager than 2. To boost performance with CNN, we need more matching features. We found Multi-layer Bidirectional LSTM can incorporate more features and achieve best performance compared with single-layer Bidirectional LSTM."}, {"heading": "6 Related Work", "text": "Existing neural sentence models mainly fall into two groups: convolutional neural networks (CNNs) and recurrent neural networks (RNNs). In regular 1D CNNs (Collobert et al., 2011; Kalchbrenner and Blunsom, 2013; Kim, 2014), a fixedsize window slides over time (successive words in sequence) to extract local features of a sentence; then they pool these features to a vector, usually taking the maximum value in each dimension, for supervised learning. The convolutional unit, when combined with max-pooling, can act as the com-\npositional operator with local selection mechanism as in the recursive autoencoder (Socher et al., 2011b). However, semantically related words that are not in one filter can\u2019t be captured effectively by this shallow architecture. (Kalchbrenner et al., 2014) built deep convolutional models so that local features can mix at high-level layers. However, deep convolutional models may result in worse performance (Kim, 2014).\nOn the other hand, RNN can take advantage of the parsing or dependency tree of sentence structure information (Socher et al., 2011b; Socher et al., 2014). (Iyyer et al., 2014) used dependencytree recursive neural network to map text descriptions to quiz answers. Each node in the tree is represented as a vector; information is propagated recursively along the tree by some elaborate semantic composition. One major drawback of RNNs is the long propagation path of information near leaf nodes. As gradient may vanish when propagated through a deep path, such long dependency buries illuminating information under a complicated neural architecture, leading to the difficulty of training. To address this issue, (Tai et al., 2015) proposed a Tree-Structured Long Short-Term Memory Networks. This motivates us to investigate multi-layer bidirectional LSTM that directly models sentence meanings without parsing for RTE task."}, {"heading": "7 Conclusions", "text": "In this paper, we propose a new deep neural network architecture that jointly leverage pre-trained word embedding and character embedding to learn sentence meanings. Our new approach first generates two kinds of word sequence representations as inputs into bidirectional LSTM to learn sentence representation. After that, we construct matching features followed by another temporal CNN to learn high-level hidden matching feature representations. Our model shows that combining pre-trained word embeddings with auxiliary character-level embedding can improve the sentence representation. The enhanced sentence representation generated by multi-layer bidirectional LSTM will encapsulate the character and word levels informations. Furthermore, it may enhance matching features that generated by computing similarity measures on sentence pairs. Experimental results on benchmark datasets demonstrate that our new framework achieved the state-of-the-art\nperformance compared with other deep neural networks based approaches."}], "references": [{"title": "Learning long-term dependencies with gradient descent is difficult", "author": ["Bengio et al.1994] Yoshua Bengio", "Patrice Simard", "Paolo Fransconi"], "venue": "In IEEE Transactions on Neural Networks", "citeRegEx": "Bengio et al\\.,? \\Q1994\\E", "shortCiteRegEx": "Bengio et al\\.", "year": 1994}, {"title": "The Meaning Factory: Formal semantics for recognizing textual entailment and determining semantic similarity", "author": ["Johan Bos", "Rob van der Goot", "Malvina Nissim"], "venue": "In Proceedings of SemEval 2014: International", "citeRegEx": "Bjerva et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Bjerva et al\\.", "year": 2014}, {"title": "Natural language processing (almost) from scratch", "author": ["Jason Weston", "L\u00e9on Bottou", "Michael Karlen", "Koray Kavukcuoglu", "Pavel Kuksa"], "venue": "The Journal of Machine Learning Research,", "citeRegEx": "Collobert et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Collobert et al\\.", "year": 2011}, {"title": "Adaptive subgradient methods for online learning and stochastic optimization", "author": ["Duchi et al.2011] John Duchi", "Elad Hazan", "Yoram Singer"], "venue": "The Journal of Machine Learning Research,", "citeRegEx": "Duchi et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Duchi et al\\.", "year": 2011}, {"title": "Hybrid speech recognition with deep bidirectional lstm", "author": ["Graves et al.2013] Alex Graves", "Navdeep Jaitly", "Abdel rahman Mohamed"], "venue": "In IEEE Workshop on Au- tomatic Speech Recognition and Understanding (ASRU),", "citeRegEx": "Graves et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Graves et al\\.", "year": 2013}, {"title": "Long short-term memory", "author": ["Hochreiter", "Schmidhuber1998] Sepp Hochreiter", "J\u00fcrgen Schmidhuber"], "venue": "In Neural Computation", "citeRegEx": "Hochreiter et al\\.,? \\Q1998\\E", "shortCiteRegEx": "Hochreiter et al\\.", "year": 1998}, {"title": "A neural network for factoid question answering over paragraphs", "author": ["Iyyer et al.2014] Mohit Iyyer", "Jordan Boyd-Graber", "Leonardo Claudino", "Richard Socher", "Hal Daum\u00e9 III"], "venue": "In Empirical Methods in Natural Language Processing", "citeRegEx": "Iyyer et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Iyyer et al\\.", "year": 2014}, {"title": "UNAL-NLP: Combining soft cardinality features for semantic textual similarity, relatedness and entailment", "author": ["George Duenas", "Julia Baquero", "Alexander Gelbukh"], "venue": "In Proceedings of SemEval 2014:", "citeRegEx": "Jimenez et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Jimenez et al\\.", "year": 2014}, {"title": "Recurrent continuous translation models", "author": ["Kalchbrenner", "Blunsom2013] Nal Kalchbrenner", "Phil Blunsom"], "venue": "In Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing,", "citeRegEx": "Kalchbrenner et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Kalchbrenner et al\\.", "year": 2013}, {"title": "A convolutional neural network for modelling sentences", "author": ["Edward Grefenstette", "Phil Blunsom"], "venue": "Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics", "citeRegEx": "Kalchbrenner et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Kalchbrenner et al\\.", "year": 2014}, {"title": "Character-aware neural language models", "author": ["Kim et al.2016] Yoon Kim", "Yacine Jernite", "David Sontag", "Alexander M Rush"], "venue": "In Thirtieth AAAI Conference on Artificial Intelligence", "citeRegEx": "Kim et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Kim et al\\.", "year": 2016}, {"title": "Convolutional neural networks for sentence classification", "author": ["Yoon Kim"], "venue": "In Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP),", "citeRegEx": "Kim.,? \\Q2014\\E", "shortCiteRegEx": "Kim.", "year": 2014}, {"title": "Illinois-lh: A denotational and distributional approach to semantics", "author": ["Lai", "Hockenmaier2014] Alice Lai", "Julia Hockenmaier"], "venue": "In Proceedings of SemEval 2014: International Workshop on Semantic Evaluation", "citeRegEx": "Lai et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Lai et al\\.", "year": 2014}, {"title": "Distributed representations of words and phrases and their compositionality", "author": ["Ilya Sutskever", "Kai Chen", "Greg S Corrado", "Jeff Dean"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Mikolov et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Mikolov et al\\.", "year": 2013}, {"title": "Robust semantic similarity at multiple levels using maximum weight matching", "author": ["Proisl", "Evert2014] Thomas Proisl", "Stefan Evert"], "venue": "In Proceedings of SemEval 2014: International Workshop on Semantic Evaluation", "citeRegEx": "Proisl et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Proisl et al\\.", "year": 2014}, {"title": "Dynamic pooling and unfolding recursive autoencoders for paraphrase detection", "author": ["Eric H Huang", "Jeffrey Pennin", "Christopher D Manning", "Andrew Y Ng"], "venue": "In Advances in Neural Information Processing Sys-", "citeRegEx": "Socher et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Socher et al\\.", "year": 2011}, {"title": "Semi-supervised recursive autoencoders for predicting sentiment distributions", "author": ["Jeffrey Pennington", "Eric H Huang", "Andrew Y Ng", "Christopher D Manning"], "venue": "In Proceedings of the Conference on Empiri-", "citeRegEx": "Socher et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Socher et al\\.", "year": 2011}, {"title": "Grounded compositional semantics for finding and describing images", "author": ["Andrej Karpathy", "Quoc V Le", "Christopher D Manning", "Andrew Y Ng"], "venue": null, "citeRegEx": "Socher et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Socher et al\\.", "year": 2014}, {"title": "Improved semantic representations from tree-structured long short-term memory networks. arXiv preprint arXiv:1503.00075", "author": ["Tai et al.2015] Kai Sheng Tai", "Richard Socher", "Christopher D Manning"], "venue": null, "citeRegEx": "Tai et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Tai et al\\.", "year": 2015}, {"title": "Multigrancnn: An architecture for general matching of text chunks on multiple levels of granularity", "author": ["Yin", "Schutze2015] Wenpeng Yin", "Hinrich Schutze"], "venue": "In Proceedings of th 53rd Annual Meeting of the Association", "citeRegEx": "Yin et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Yin et al\\.", "year": 2015}, {"title": "Character-level convolutional networks for text classification", "author": ["Zhang et al.2015] Xiang Zhang", "Junbo Zhao", "Yann LeCun"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Zhang et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Zhang et al\\.", "year": 2015}, {"title": "ECNU: One stone two birds: Ensemble of heterogenous measures for semantic relatedness and textual entailment", "author": ["Zhao et al.2014] Jiang Zhao", "Tian Tian Zhu", "Man Lan"], "venue": "In Proceedings of SemEval 2014: International Workshop on Semantic Evalua-", "citeRegEx": "Zhao et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Zhao et al\\.", "year": 2014}], "referenceMentions": [{"referenceID": 21, "context": "Traditional approaches (Lai and Hockenmaier, 2014; Zhao et al., 2014; Jimenez et al., 2014) for sentence relation modeling tasks such as paraphrase identification, question answering, recognized textual entailment and semantic textual similarity prediction usually build the supervised model using a variety of hand crafted features.", "startOffset": 23, "endOffset": 91}, {"referenceID": 7, "context": "Traditional approaches (Lai and Hockenmaier, 2014; Zhao et al., 2014; Jimenez et al., 2014) for sentence relation modeling tasks such as paraphrase identification, question answering, recognized textual entailment and semantic textual similarity prediction usually build the supervised model using a variety of hand crafted features.", "startOffset": 23, "endOffset": 91}, {"referenceID": 6, "context": "With the success of deep learning, there has been much interest in applying deep neural network based techniques to further improve the prediction performances (Socher et al., 2011b; Iyyer et al., 2014; Yin and Schutze, 2015).", "startOffset": 160, "endOffset": 225}, {"referenceID": 2, "context": "From low level NLP tasks such as language modeling, POS tagging, name entity recognition, and semantic role labeling (Collobert et al., 2011; Mikolov et al., 2013), to high level tasks such as machine translation, information retrieval and semantic analysis (Kalchbrenner and Blunsom, 2013; Socher et al.", "startOffset": 117, "endOffset": 163}, {"referenceID": 13, "context": "From low level NLP tasks such as language modeling, POS tagging, name entity recognition, and semantic role labeling (Collobert et al., 2011; Mikolov et al., 2013), to high level tasks such as machine translation, information retrieval and semantic analysis (Kalchbrenner and Blunsom, 2013; Socher et al.", "startOffset": 117, "endOffset": 163}, {"referenceID": 18, "context": ", 2013), to high level tasks such as machine translation, information retrieval and semantic analysis (Kalchbrenner and Blunsom, 2013; Socher et al., 2011a; Tai et al., 2015).", "startOffset": 102, "endOffset": 174}, {"referenceID": 10, "context": "On the other hand, some researchers have found character-level convolutional networks (Kim et al., 2016; Zhang et al., 2015) are useful in extracting information from raw signals for the task such as language modeling or text classification.", "startOffset": 86, "endOffset": 124}, {"referenceID": 20, "context": "On the other hand, some researchers have found character-level convolutional networks (Kim et al., 2016; Zhang et al., 2015) are useful in extracting information from raw signals for the task such as language modeling or text classification.", "startOffset": 86, "endOffset": 124}, {"referenceID": 2, "context": "We explore treating each sentence as a kind of raw signal at character level, and applying temporal (one-dimensional) Convolution Neural Network (CNN) (Collobert et al., 2011), Highway Multilayer Perceptron (HMLP) and multi-layer bidirectional LSTM (Long Short Term Memory) (Graves et al.", "startOffset": 151, "endOffset": 175}, {"referenceID": 4, "context": ", 2011), Highway Multilayer Perceptron (HMLP) and multi-layer bidirectional LSTM (Long Short Term Memory) (Graves et al., 2013) to learn sentence representations.", "startOffset": 106, "endOffset": 127}, {"referenceID": 2, "context": "The other one was introduced by (Collobert et al., 2011).", "startOffset": 32, "endOffset": 56}, {"referenceID": 2, "context": "In (Collobert et al., 2011) architecture, a sequence of length n is represented as: x1:n = x1 \u2295 x2 \u00b7 \u00b7 \u00b7 \u2295 xn , (2)", "startOffset": 3, "endOffset": 27}, {"referenceID": 0, "context": "A major issue of RNNs using these transition functions is that it is difficult to learn longrange dependencies during training step because the components of the gradient vector can grow or decay exponentially (Bengio et al., 1994).", "startOffset": 210, "endOffset": 231}, {"referenceID": 4, "context": "Bidirectional RNNs architecture (Graves et al., 2013) proposed a solution of making prediction based on future words.", "startOffset": 32, "endOffset": 53}, {"referenceID": 18, "context": "Inspired by (Tai et al., 2015), we apply elementwise merge to first sentence matrix M1 \u2208 Rn\u00d72d and second sentence matrix M2 \u2208 Rn\u00d72d.", "startOffset": 12, "endOffset": 30}, {"referenceID": 3, "context": "Training is done through stochastic gradient descent over shuffled mini-batches with the AdaGrad update rule (Duchi et al., 2011).", "startOffset": 109, "endOffset": 129}, {"referenceID": 18, "context": "where m is the number of training pairs and the superscript k indicates the k-th sentence pair (Tai et al., 2015).", "startOffset": 95, "endOffset": 113}, {"referenceID": 18, "context": "Also our method doesn\u2019t leverage external linguistic resources such as wordnet or parsing which get best results in (Tai et al., 2015).", "startOffset": 116, "endOffset": 134}, {"referenceID": 18, "context": "Note that for semantic relatedness task, the latest research (Tai et al., 2015) proposed a tree-structure based LSTM, the Pearson correlation score of their system can reach 0.", "startOffset": 61, "endOffset": 79}, {"referenceID": 18, "context": "We hope to point out that we implemented the method in (Tai et al., 2015), but the results are not as good as our method.", "startOffset": 55, "endOffset": 73}, {"referenceID": 18, "context": "Based on our experiments, we believe the method in (Tai et al., 2015) is very sensitive to the initializations, thus it may not achieve the good performance in different settings.", "startOffset": 51, "endOffset": 69}, {"referenceID": 7, "context": "804 214 (Jimenez et al., 2014)", "startOffset": 8, "endOffset": 30}, {"referenceID": 1, "context": "827 32 (Bjerva et al., 2014)", "startOffset": 7, "endOffset": 28}, {"referenceID": 21, "context": "828 72 (Zhao et al., 2014)", "startOffset": 7, "endOffset": 26}, {"referenceID": 7, "context": "831 214 (Jimenez et al., 2014)", "startOffset": 8, "endOffset": 30}, {"referenceID": 21, "context": "836 72 (Zhao et al., 2014)", "startOffset": 7, "endOffset": 26}, {"referenceID": 18, "context": "We use dependency tree child-sum tree LSTM proposed by (Tai et al., 2015) as our baseline.", "startOffset": 55, "endOffset": 73}, {"referenceID": 2, "context": "In regular 1D CNNs (Collobert et al., 2011; Kalchbrenner and Blunsom, 2013; Kim, 2014), a fixedsize window slides over time (successive words in sequence) to extract local features of a sentence; then they pool these features to a vector, usually taking the maximum value in each dimension, for supervised learning.", "startOffset": 19, "endOffset": 86}, {"referenceID": 11, "context": "In regular 1D CNNs (Collobert et al., 2011; Kalchbrenner and Blunsom, 2013; Kim, 2014), a fixedsize window slides over time (successive words in sequence) to extract local features of a sentence; then they pool these features to a vector, usually taking the maximum value in each dimension, for supervised learning.", "startOffset": 19, "endOffset": 86}, {"referenceID": 9, "context": "(Kalchbrenner et al., 2014) built deep convolutional models so that local features can mix at high-level layers.", "startOffset": 0, "endOffset": 27}, {"referenceID": 11, "context": "However, deep convolutional models may result in worse performance (Kim, 2014).", "startOffset": 67, "endOffset": 78}, {"referenceID": 17, "context": "On the other hand, RNN can take advantage of the parsing or dependency tree of sentence structure information (Socher et al., 2011b; Socher et al., 2014).", "startOffset": 110, "endOffset": 153}, {"referenceID": 6, "context": "(Iyyer et al., 2014) used dependencytree recursive neural network to map text descriptions to quiz answers.", "startOffset": 0, "endOffset": 20}, {"referenceID": 18, "context": "To address this issue, (Tai et al., 2015) proposed a Tree-Structured Long Short-Term Memory Networks.", "startOffset": 23, "endOffset": 41}], "year": 2016, "abstractText": "Neural network based approaches for sentence relation modeling automatically generate hidden matching features from raw sentence pairs. However, the quality of matching feature representation may not be satisfied due to complex semantic relations such as entailment or contradiction. To address this challenge, we propose a new deep neural network architecture that jointly leverage pre-trained word embedding and auxiliary character embedding to learn sentence meanings. The two kinds of word sequence representations as inputs into multi-layer bidirectional LSTM to learn enhanced sentence representation. After that, we construct matching features followed by another temporal CNN to learn high-level hidden matching feature representations. Experimental results demonstrate that our approach consistently outperforms the existing methods on standard evaluation datasets.", "creator": "LaTeX with hyperref package"}}}