{"id": "1603.09420", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "31-Mar-2016", "title": "Minimal Gated Unit for Recurrent Neural Networks", "abstract": "Recently recurrent neural networks (RNN) has been very successful in handling sequence data. However, understanding RNN and finding the best practices for RNN is a difficult task, partly because there are many competing and complex hidden units (such as LSTM and GRU). We propose a gated unit for RNN, named as Minimal Gated Unit (MGU), since it only contains one gate, which is a minimal design among all gated hidden units. The design of MGU benefits from evaluation results on LSTM and GRU in the literature. Experiments on various sequence data show that MGU has comparable accuracy with GRU, but has a simpler structure, fewer parameters, and faster training. Hence, MGU is suitable in RNN's applications. Its simple architecture also means that it is easier to evaluate and tune, and in principle it is easier to study MGU's properties theoretically and empirically. Thus, the training and training is less difficult to obtain. This is the primary reason why MGU is suitable to be used for G-state classification. A simple example is the M-tree (M tree) that is an LSTM, which contains all possible parameters and can be evaluated using a simple calculation and its G-states as well as any other parameter. In a normal linear model, only a few parameters are left after an optimization of the m tree. We use one parameter of the M-tree (M tree) in order to compare the results. In the normal linear model, only one parameter of the M-tree (M tree) is left after an optimization of the m tree. The MLU is used for statistical comparison.\n\n\n\n\nFor example, a G-state classification would produce an approximate gradient. For example, we calculate a gradient which can be used to obtain the M-tree. For example, a Gaussian Gaussian Gaussian Gaussian Gaussian Gaussian Gaussian Gaussian Gaussian Gaussian Gaussian Gaussian Gaussian Gaussian Gaussian Gaussian Gaussian Gaussian Gaussian Gaussian Gaussian Gaussian Gaussian Gaussian Gaussian Gaussian Gaussian Gaussian Gaussian Gaussian Gaussian Gaussian Gaussian Gaussian Gaussian Gaussian Gaussian Gaussian Gaussian Gaussian Gaussian Gaussian Gaussian Gaussian Gaussian Gaussian Gaussian Gaussian Gaussian Gaussian Gaussian Gaussian Gaussian Gaussian Gaussian Gaussian Gaussian Gaussian Gaussian Gaussian Gaussian Gaussian Gaussian Gaussian Gaussian Ga", "histories": [["v1", "Thu, 31 Mar 2016 00:01:10 GMT  (178kb,D)", "http://arxiv.org/abs/1603.09420v1", null]], "reviews": [], "SUBJECTS": "cs.NE cs.LG", "authors": ["guo-bing zhou", "jianxin wu", "chen-lin zhang", "zhi-hua zhou"], "accepted": false, "id": "1603.09420"}, "pdf": {"name": "1603.09420.pdf", "metadata": {"source": "META", "title": "Minimal Gated Unit for Recurrent Neural Networks", "authors": ["Guo-Bing Zhou", "Jianxin Wu", "Chen-Lin Zhang", "Zhi-Hua Zhou"], "emails": ["ZHOUGB@LAMDA.NJU.EDU.CN", "WUJX@NJU.EDU.CN", "U-ZHANGCL@LAMDA.NJU.EDU.CN", "ZHOUZH@NJU.EDU.CN"], "sections": [{"heading": "1. Introduction", "text": "In recent years, deep learning models have been particularly effective in dealing with data that have complex internal structures. For example, convolutional neural networks (CNN) are very effective in handling image data in which 2D spatial relationships are critical among the set of raw pixel values in an image (LeCun et al., 1998; Krizhevsky et al., 2012). Another success of deep learning is handling sequence data, in which the sequential relationships within a variable length input sequence is crucial. In sequence modeling, recurrent neural networks (RNN) have been very successful in language translation (Cho et al., 2014; Sutskever et al., 2014; Bahdanau et al., 2015), speech recognition (Graves et al., 2013), image captioning, i.e.,\nSubmitted to ICML 2016.\nsummarizing the semantic meaning of an image into a sentence (Xu et al., 2015; Karpathy & Fei-Fei, 2015; Lebret et al., 2015), recognizing actions in videos (Donahue et al., 2015; Srivastava et al., 2015), or short-term precipitation prediction (Shi et al., 2015).\nThere is, however, one important difference between RNN and CNN. The key building blocks of CNN, such as nonlinear activation function, convolution and pooling operations, etc., have been extensively studied. The choices are becoming convergent, e.g., ReLU for nonlinear activation, small convolution kernels and max-pooling. Visualization also help us understand the semantic functionalities of different layers (Zeiler & Fergus, 2014), e.g., firing at edges, corners, combination of specific edge groups, object parts, and objects.\nThe community\u2019s understanding of RNN, however, is not as thorough, and the opinions are much less convergent. Proposed by Hochreiter and Schmidhuber (1997), the Long Short-Term Memory (LSTM) model and its variants have been the overall best performing RNN. Typical LSTM is complex, having 3 gates and 2 hidden states. Using LSTM as an representative example, we may find some obstacles that prevent us from reaching a consensus on RNN:\n\u2022 Depth with unrolling. Different from CNN which has physical layers, RNN is considered a deep model only when it is unrolled along the time axis. This property helps in reducing parameter size, but at the same time hinders visualization and understanding. This difficulty comes from the complexity of sequence data itself. We do not have a satisfactory solution for solving this difficulty, but we hope this paper will help in mitigating difficulties caused by the next one.\n\u2022 Competing and complex structures. Since its inception, the focus of LSTM-related research has been altering its structure to achieve higher performance. For example, a forget gate was added by Gers et al. (1999) to the original LSTM, and a peephole connection made it even more complex (Gers et al., 2002).\nar X\niv :1\n60 3.\n09 42\n0v 1\n[ cs\n.N E\n] 3\n1 M\nar 2\n01 6\nIt was not until recently that simplified models were proposed and studied, such as the Gated Recurrent Unit (GRU) by Cho et al. (2014). GRU, however, is still relatively complex because it has two gates. Very recently there have been empirical evaluations on LSTM, GRU, and their variants (Chung et al., 2014; Jozefowicz et al., 2015; Greff et al., 2015). Unfortunately, no consensus has yet been reached on the best LSTM-like RNN architecture.\nTheoretical analysis and empirical understanding of deep learning techniques are fundamental. However, it is very difficult if there are too many components in the structure. Simplifying the model structure is an important step to enable the learning theory analysis in the future.\nComplex RNN models not only hinder our understanding. It also means that more parameters are to be learned and more components to be tuned. As a natural consequence, more training sequences and (perhaps) more training time are needed. However, evaluations in (Chung et al., 2014) and (Jozefowicz et al., 2015) both show that more gates do not lead to better accuracy. On the contrary, the accuracy of GRU is usually higher than that of LSTM, albeit the fact that GRU has one less hidden state and one less gate than LSTM.\nIn this paper, we propose a new variant of GRU (which is also a variant of LSTM), which has minimal number of gates\u2013only one gate! Hence, the proposed method is named as the Minimal Gated Unit (MGU). Evaluations in (Chung et al., 2014; Jozefowicz et al., 2015; Greff et al., 2015) agreed that RNN with a gated unit works significantly better than a RNN with a simple tanh unit without any gate. The proposed method has the smallest possible number of gates in any gated unit, a fact giving rise to the name minimal gated unit.\nWith only one gate, we expect MGU will have significantly fewer parameters to learn than GRU or LSTM, and also fewer components or variations to tune. The learning process will be faster compared to them, which will be verified by our experiments on a diverse set of sequence data in Section 4. What is more, our experiments also showed that MGU has overall comparable accuracy with GRU, which once more concurs the observation that fewer gates reduces complexity but not necessarily accuracy.\nBefore we present the details of MGU, we want to add that we are not proposing a \u201cbetter\u201d RNN model in this paper.1 The purpose of MGU is two-fold. First, with a simpler model, we can reduce the requirement for training data, ar-\n1We also believe that: without a carefully designed common set of comprehensive benchmark datasets and evaluation criteria, it is not easy to get conclusive decisions as to which RNN model is better.\nchitecture tuning and CPU time, while at the same time maintaining accuracy. This characteristic may make MGU a good candidate in various applications. Second, a minimally designed gated unit will (in principle) make our analyses of RNN easier, and help us understand RNN, but this will be left as a future work."}, {"heading": "2. RNN: LSTM, GRU, and More", "text": "We start by introducing various RNN models, mainly LSTM and GRU, and their evaluation results. These studies in the literature have guided us in how to minimize a gated hidden unit in RNN.\nA recurrent neural network uses an index t to indicate different positions in an input sequence, and assumes that there is a hidden state ht to represent the system status at time t.2 RNN accepts input xt at time t, and the status is updated by a nonlinear mapping f from time to time:\nht = f(ht\u22121,xt) . (1)\nOne usual way of defining the recurrent unit f is a linear transformation plus a nonlinear activation, e.g.,\nht = tanh (W [ht\u22121,xt] + b) , (2)\nwhere we combined the parameters related to ht\u22121 and xt into a matrixW , and b is a bias term. The activation (tanh) is applied to every element of its input. The task of RNN is to learn the parameters W and b, and we call this architecture the simple RNN. An RNN may also have an optional output vector yt.\nLSTM. RNN in the simple form suffers from the vanishing or exploding gradient issue, which makes learning RNN using gradient descent very difficult in long sequences (Bengio et al., 1994; Hochreiter & Schmidhuber, 1997). LSTM solved the gradient issue by introducing various gates to control how information flows in RNN, which are summarized in Table 1, from Equation (4a) to (4f), in which\n\u03c3(x) = 1\n1 + exp(\u2212x) (3)\nis the logistic sigmoid function (applied to every component of the vector input) and is the component-wise product between two vectors.\nFigure 1a illustrates the data flow and operations in LSTM.\n\u2022 There is one more hidden state ct in addition to ht, which helps maintain long-term memories.\n\u2022 The forget gate f t decides the portion (between 0 and 1) of ct\u22121 to be remembered, determined by parameters Wf and bf .\n2We use boldface letters to denote vectors.\nLSTM learning follows typical stochastic gradient descent and back propagation (Graves & Schmidhuber, 2005).\nThere are a lot of variants of LSTM. The original LSTM (Hochreiter & Schmidhuber, 1997) does not include the forget gate, which was later introduced in (Gers et al., 1999). Gers et al. (2002) makes LSTM even more complicated by allowing the three gates (f t, it, ot) to take ct\u22121 or ct as an additional input, called the peephole connections. We choose the specific form of LSTM in Table 1 because of two reasons. First, as will soon be discussed, the forget gate is essential. Second, the peephole connection does not seem necessary, but it complicates the learning process.\nHowever, recently the trend is reversed: researchers found that simplifying the LSTM architecture may improve its\nperformance.\nLSTM with coupled gates. Greff et al. (2015) evaluated a variant of LSTM, which couples the forget and input gates into one:\nit = 1\u2212 f t , \u2200t , (7)\nas illustrated in Figure 1b. The coupling removed one gate and its parameters (Wi and bi), which leads to reduced computational complexity and slightly higher accuracy. Greff et al. (2015) also observed that removing the peephole connection has similar effects.\nGRU. The Gated Recurrent Unit (GRU) architecture further simplifies LSTM-like units (Cho et al., 2014). GRU contains two gates: an update gate z (whose role is similar to the forget gate) and a reset gate r (whose role loosely matches the input gate). GRU\u2019s update rules are shown as Equation (5a) to (5d), and the data flow and operations are illustrated in Figure 1c. Beyond removing the output gate from LSTM, GRU also removed the hidden (slowlychanging) cell state c. Note that GRU has appeared in different forms. When Cho et al. (2014) originally proposed GRU, its form is different from Equation 5d, as\nht = zt ht\u22121 + (1\u2212 zt) h\u0303t . (8)\nHowever, these two forms are mathematically equivalent. We adopt Equation 5d because it is more popular in the literature.\nEvaluations in (Chung et al., 2014) found that when LSTM and GRU have the same amount of parameters, GRU slightly outperforms LSTM. Similar observations were also corroborated in (Jozefowicz et al., 2015).\nSCRN. Instead of using gates to control information flow in RNN, the Structurally Constrained Recurrent Network (SCRN) added a hidden context vector st to simple RNN, which changes slowly over time if the parameter \u03b1 is large (Mikolov et al., 2015), as\nst = \u03b1st\u22121 + (1\u2212 \u03b1)Bxt , (9) ht = \u03c3(Pst +Axt +Rht\u22121) , (10)\nin which \u03b1was fixed to 0.95, and the hidden state ht hinges on three factors: st, xt and ht\u22121. SCRN has still fewer parameters than GRU, and has shown similar performance as LSTM in (Mikolov et al., 2015).\nIRNN. Le et al. (2015) showed that minor changes to the simple RNN architecture can significantly improve its accuracy. The key is to initialize the simple RNN\u2019s weight matrix to an identity matrix, and use ReLU (rectified linear unit) as the nonlinear activation function. This method (named as IRNN) has achieved accuracy that is much closer to LSTM than that of simple RNN. Especially in the MNIST dataset (LeCun et al., 1998), IRNN significantly\noutperforms LSTM. Similarly, Jozefowicz et al. (2015) also showed that proper initialization is also important for LSTM. They showed that the bias of the forget gate should be set to a large value (e.g., 1 or 2). The same initialization trick was used in (Le et al., 2015) too.\nLSTM variants. Greff et al. (2015) proposed, in evaluation of the importance of LSTM components, 8 variants of the LSTM architecture. The evaluation results, however, showed that none of them can outperform the vanilla LSTM model. Their vanilla LSTM architecture has the peephole connections. Hence, it is slightly different from the LSTM architecture used in this paper (cf. Table 1).\nGRU variants. Jozefowicz et al. (2015) proposed three variants of GRU. In these variants, they add the tanh nonlinearity in generating the gates, removing dependency to the hidden state in generating the gates, and make these changes while generating h\u0303t, etc. These variants sometimes achieve higher accuracy than GRU, but none of them can consistently outperform GRU.\nOverall, with all these recent results and proposals we feel that among them GRU has some advantage in learning recurrent neural networks. Although it is not always the model with the highest accuracy or fewest parameters, it has stable performance (and it is usually one of the most accurate models) and relatively small amount of parameters. We will use GRU as the baseline method and compare it with the proposed MGU (minimal gated unit, cf. next section) in our experiments."}, {"heading": "3. Minimal Gated Unit", "text": "As introduced in Section 1, we prefer an RNN architecture that has the smallest number of gates without losing LSTM\u2019s accuracy benefits. However, the choice of which gate to keep is no easy job. Fortunately, several recent evaluations have helped us make this decision. Now we briefly summarize knowledge from these evaluations.\nJozefowicz et al. (2015): the forget gate is critical (and its biases bf must be initialized to large values); the input gate is important, but the output gate is unimportant; GRU and LSTM have similar performance.\nGreff et al. (2015): The forget and output gates are critical, and many variants of LSTM (mainly simplified LSTM variants) act similarly to LSTM.\nChung et al. (2014): Gated units work better than simple units without any gate; GRU and LSTM has comparable accuracy with the same number of parameters.\nOne notable thing is that different evaluations may lead to inconsistent conclusions, e.g., on the importance of the output gate Jozefowicz et al. (2015) and Greff et al. (2015)\ndisagreed. This is inevitable because data with different properties have been used in different evaluations. However, at least we find the following consensus among these evaluations:\n\u2022 Having a gated unit is key to high performance of RNN architectures;\n\u2022 The forget gate is unanimously considered the most important one; and,\n\u2022 A simplified model may lower complexity and maintain comparable accuracy.\nHence, we propose the Minimal Gated Unit (MGU), which has the smallest possible number of gates in any gated unit. MGU only has 1 gate, which is the forget gate. MGU is based on GRU, and it further couples the input (reset) gate to the forget (update) gate, by specifying that\nrt = f t , \u2200t . (11)\nNote that we use f (instead of z) to denote the only gate, because it is treated as the forget gate (which can be considered as a renaming of the update gate z in GRU). The equations that define MGU are listed in Table 1 as Equations (6a) to (6c), and the data flow and operations are illustrated in Figure 1d.\nIn MGU, the forget gate f t is first generated, and the element-wise product between 1 \u2212 f t and ht\u22121 becomes part of the new hidden state ht. The portion of ht\u22121 that is \u201cforgotten\u201d (f t ht\u22121) is combined with xt to produce h\u0303t, the short-term response. A portion of h\u0303t (determined again by f t) form the second part of ht.\nComparing the equation sets in Table 1 and the parameterized operations in Figure 1, it is obvious that MGU is more simplified than LSTM or GRU. While LSTM has four sets of parameters that determine f , i, o and c\u0303, and GRU has three sets of parameters for z, r and h\u0303, MGU only has two sets of parameters, one for calculating f , the other for h\u0303. In other words, MGU has only roughly half the parameter size of that of LSTM, and 67% of that of GRU, because Wf (or Wz), Wi (or Wr), Wo, Wh have the same size. MGU also has slightly more parameters than SCRN, as will be shown in the example in Section 4.4. Since the parameter size of IRNN is the same as that of simple RNN, it must have the smallest number of parameters.\nIn short, MGU is a minimal design in any gated hidden unit for RNN. As we will show in the next section by experiments on a variety of sequence data, MGU also learns RNN for sequences without suffering from the gradient vanishing or gradient exploding problem (thanks to the forget gate in MGU). Because MGU only has few factors to tune, it is easier to find the best practices for MGU than for other gated units."}, {"heading": "4. Experimental Results", "text": "In this section, we will evaluate the effectiveness of MGU using four datasets. The simple adding problem is used as a sanity check in Section 4.1. The IMDB dataset and the MNIST dataset are sentiment and image classification problems with sequence inputs, presented in Section 4.2 and 4.3, respectively. Finally, we evaluate MGU on the Penn TreeBank (PTB) language modeling dataset in Section 4.4.\nAs was shown in the evaluations (Chung et al., 2014; Jozefowicz et al., 2015), GRU has comparable accuracy with LSTM, and has fewer parameters than LSTM. We will use GRU as a baseline architecture, and compare the proposed MGU with GRU. If not otherwise specified, we compare these two algorithms with the same number of hidden units. All RNNs are implemented with the Lasagne package in the Theano library.3\nThe dropout technique is not used in either GRU or MGU. Because the focus of this paper is not absolutely high accuracy, we did not evaluate model averaging (ensemble) either.\nThe metrics that are compared include accuracy (or error, or perplexity) computed from the test sets, the average running time per epoch, and the number of parameters in the hidden units. We only count the parameters of the hidden unit, i.e., the preprocessing and fully connected regression or classification layer\u2019s parameters are not counted."}, {"heading": "4.1. The Adding Problem", "text": "The adding problem was originally proposed in (Hochreiter & Schmidhuber, 1997), and we use the variant in (Le et al., 2015). The input has two components: one random number in the range [0 1], and the other is a mask in {+1, 0,\u22121}. In the sequence (whose length ranges from 50 to 55), only 2 numbers are with mask +1, and the output should be the sum of these two. Both MGU and GRU use 100 hidden units; batch size is 100, and the learning rate is 10\u22123. For this problem, we use a bidirectional network structure (Graves & Schmidhuber, 2005), which scans the sequence both from left to right and from right to left; the overall hidden state is the concatenation of the hidden state in both scans. On top of the last time step\u2019s hidden state, we add a fully connected layer to transform the last hidden state vector to a regression prediction.\nWe generated a training set of 10,000 examples and a test set of 1,000 examples. In Figure 2, we show the mean squared error of this simple regression task on the test set. MGU is slightly worse than GRU in the first 100 epochs.\n3http://deeplearning.net/software/theano/, http://lasagne.readthedocs.org.\nHowever, after 100 epochs these two algorithms have almost indistinguishable results. After 1,000 epochs, the mean squared regression error of both methods are below 0.005: 0.0041 for GRU and 0.0045 for MGU.\nThis simple experiment shows that MGU can smoothly deal with sequence input with a moderate length around 50. And, MGU has fewer parameters than GRU: 41,400 (MGU) vs. 62000 (GRU).\nMGU also trains faster, which takes on average 6.85 seconds per epoch, while GRU requires 8.60 seconds."}, {"heading": "4.2. IMDB", "text": "The second problem we study is sentiment classification in the IMDB movie reviews, whose task is to separate the reviews into positive and negative ones. This dataset was generated in (Maas et al., 2011).4 There are 25,000 movie reviews in the training set, another 25,000 for testing. We use the provided bag-of-words format as our sequence input. The maximum sequence length is 128. Both MGU and GRU have 100 hidden units; batch size is 16, and the learning rate is 10\u22128 with a 0.99 momentum. Similar to the adding problem, we use a fully connected layer on top of the last hidden state to classify a movie review as either positive or negative.\nWe show the accuracy of this binary classification problem in Figure 3, evaluated on the test set. In the x-axis of Figure 3, we show the epoch number divided by 100. Because both curves show that they converge after 15,000 epochs, it is too dense to show the results of the model after every training epoch.\nMGU consistently outperforms GRU in this example, al-\n4Available at http://ai.stanford.edu/ amaas/data/sentiment/.\nthough by a small margin. After convergence, the accuracy of GRU is 61.8%. MGU achieves an accuracy of 62.6%, obtaining a 1.3% relative improvement. The input of IMDB is longer than that of the adding problem. In this larger and longer dataset, MGU verifies again it has comparable (or slightly better in this case) accuracy with GRU.\nAgain, MGU has two thirds of the number of parameters in GRU. MGU has 20,400 parameters, while GRU has 30,600 parameters. However, MGU trains much faster than GRU in this problem. The average running time per epoch for MGU is 5.0 seconds, only 35% of that of GRU (which takes 14.1 seconds per epoch on average)."}, {"heading": "4.3. MNIST", "text": "The MNIST dataset by LeCun et al. (1998) contains images of handwritten digits (\u20180\u2019\u2013\u20189\u201d). All the images are of size 28\u00d7 28.5 There are 60,000 images in the training set, and 10,000 in the test set. The images are preprocessed such that the center of mass of these digits are at the central position of the 28\u00d7 28 image.\nMNIST has been a very popular testbed for deep neural network classification algorithms, but is not widely used in evaluating RNN models yet. We use this dataset in two ways. The first is to treat each row (28 pixels) as a single input in the input sequence. Hence, an image is a sequence with length 28, corresponding to the 28 image rows (from top to bottom). For this task, we use 100 hidden units and the batch size is 100. The learning rate is 10\u22128 with a 0.99 momentum. A fully connected layer transfer the last row\u2019s hidden state into an output vector with 10 elements. Accuracy of MGU and GRU on the test set in this task are shown in Figure 4, where the x-axis is the epoch number divided\n5Available at http://yann.lecun.com/exdb/mnist/\nby 100.\nThe performance on MNIST is similar to that on the adding problem (cf. Figure 2), but the role of MGU and GRU has reversed. In the first 1,000 epochs (x-axis value up to 10), GRU is slightly better than our MGU. However, from then on till both algorithms\u2019 convergence, MGU\u2019s accuracy is higher than that of GRU. In the end (16,000 epochs), GRU achieves a 87.53% accuracy, while MGU is slightly higher at 88.07%. For this task, MGU has 25,800 parameters, while GRU has 38,700 parameters. The average per-epoch training time of MGU is 158 seconds, faster than that of GRU (182 seconds).\nThe second task on MNIST treats every pixel as one component in the input sequence. An image then becomes a sequence of length 784, with pixels scanned from left to right, and top to bottom. This task tests how MGU works when the sequence length is long. In this task, GRU has 30,600 parameters while MGU has 20,400. Other settings (learning rate etc.) are the same as those in the first task.\nAfter 16,000 epochs, MGU\u2019s test set accuracy is 84.25%, while with the same input length (784) and epoch number, IRNN\u2019s accuracy was below 65% (see Le et al., 2015, Figure 3). After 32,000 epochs, IRNN\u2019s accuracy was roughly 80%, which is still worse than MGU\u2019s 84.25% at 16,000 epochs. Note that when the number of epochs continued till 500,000, IRNN reached a high accuracy of above 90%. Although we did not run MGU till this number of epochs, we have reasons to expect that MGU will similarly achieve a high accuracy if much longer training time is given. The accuracy of LSTM on this task is only around 65% even after 900,000 epochs (see Le et al., 2015, Figure 3).\nThe average training time per epoch of MGU is 48.1 sec-\nonds. However, GRU is much slower in this long sequence task, which takes 145.3 seconds per epoch.6 If the same training time budget is allocated, MGU has significantly higher accuracy than GRU."}, {"heading": "4.4. Penn TreeBank", "text": "The Penn TreeBank (PTB) dataset provides data for language modeling, which is released by Marcus et al. (1993). For this dataset, we work on the word-level prediction task, which is the same as the version in (Zaremba et al., 2014).7 There are 10,000 words in the vocabulary. It has 929K words in the training set, 73K in the validation set, and 82K in the test set. As in (Zaremba et al., 2014), we use two layers and the sequence length is 35. The batch size is 20, and the learning rate is 0.01. Because dropout is not used, we tested MGU and GRU on small networks, whose number of hidden nodes range in the set {50, 100, 200, 300, 400, 500, 600}. Without dropout, both units overfit when the number of hidden units exceed 500. A fully connected layer predicts one of the 10,000 words. As a direct comparison, we show the perplexity of MGU and GRU in Figure 5 when there are 500 hidden units. The x-axis is the epoch number divided by 1,316.\nGRU has a small advantage over MGU in this task. In the right end of Figure 5, GRU\u2019s perplexity on the test set is 101.64, while MGU\u2019s is higher at 105.59. We observe the same behavior on the validation set. We can also compare these two algorithms when they have roughly the same\n6We did not have enough time to wait for GRU to run to a high epoch number on this task. The accuracy of GRU is higher than that of MGU in the early training stage. However, MGU takes over after 710 epochs. This trend is similar to that of Figure 4.\n7Available at https://github.com/wojzaremba/lstm/, or from http://www.fit.vutbr.cz/imikolov/rnnlm/simple-examples.tgz.\nnumber of parameters, as was done in (Chung et al., 2014). The comparison results are shown in Table 2.\nMGU has one third less parameters than GRU. Thus, the number of parameters are roughly the same for MGU with 500 hidden units and GRU with 400 hidden units. When we compare these two algorithms using these settings, the gap between GRU and MGU becomes smaller (102.33 vs. 105.89 on the test set, and 107.92 vs. 110.68 on the validation set).\nIf we compare these two algorithms with the same amount of training time, MGU is faster than GRU. MGU with 500 units is roughly as fast as GRU with 300 units; and, MGU with 300 units is similar to GRU with 100 units. When the numbers of hidden units are same (e.g., 500), the proposed MGU can run more epochs than GRU given the same amount of training time, which we expect will continue to decrease the perplexity of MGU.\nWe can also compare MGU with the results in (Jozefowicz et al., 2015). When there are 400 hidden units, the total number of parameters (including all layers) of MGU is 4.8M, which can be fairly compared with the \u201c5M-tst\u201d result in (Jozefowicz et al., 2015, Table 3), so is GRU with 300 units. Our GRU implementation (with 300 hidden units) has a test set perplexity of 102.55, lower than the GRU (with 5M parameters) result in (Jozefowicz et al., 2015), which is 108.42 (= exp(4.684)). The proposed MGU (with 400 hidden units) achieves a test set perplexity of 106.02, also lower than the GRU result in (Jozefowicz et al., 2015).\nThe SCRN method has still fewer parameters than the proposed MGU. When there are 100 hidden units, MGU has 60,400 parameters. A similar SCRN architecture has 100 hidden units and 40 context units (see Mikolov et al., 2015, Table 1), which has 48,200 parameters, amounting to roughly 80% of that of MGU. On this dataset, however, SCRN seems to saturate at test set perplexity 115, because SCRN with 100 and 300 hidden units arrived at this same perplexity. MGU gets lower perplexity than SCRN on this dataset."}, {"heading": "4.5. Discussions", "text": "We have evaluated the proposed MGU on four different sequence data. The comparison is mainly against GRU, while results of IRNN and SCRN are also cited when appropriate. The input sequence length ranges short (35, 50\u201355), moderate (128), and long (784). The sequence data range from artificial to real-world, and the task domains are also diverse.\nThe proposed method is on par with GRU in terms of accuracy (or error, or perplexity). Given its minimal design of one gate, MGU has only two thirds of the parameters\nof GRU, and hence trains faster in all datasets. However, in some problems (e.g., Penn TreeBank), GRU converges faster than MGU. Overall, through these experimental results we believe MGU has proven itself as an attractive alternative in building RNN."}, {"heading": "5. Conclusions and Future Work", "text": "In this paper, we proposed a new hidden unit for recurrent neural networks. The proposed Minimal Gated Unit (MGU) has the minimal design in any gated hidden unit for RNN. It has only one gate (the forget gate) and does not involve the peephole connection. Hence, the number of parameters in MGU is only half of that in the Long Short-Term Memory (LSTM), or two thirds of that in the Gated Recurrent Unit (GRU). We compared MGU with GRU on several tasks that deal with sequence data in various domains. MGU has achieved comparable accuracy with GRU, and (thanks to the minimal design) trains faster than GRU.\nBased on our evaluations, MGU could be readily used as the hidden unit in an RNN, which may reduce memory footprint and training time in some applications. More importantly, the minimal design will facilitate our theoretical analysis or empirical observation (e.g., through visualization) of RNN models, and enhance our understanding and facilitate progresses in this domain. A minimal design also means that there are fewer possibilities of producing variants (which will complicate the analysis).\nAmple ways are possible to further this line of research. Beyond analysis and understanding, we will also run MGU with more epochs, in more diverse and complex tasks, and regularize MGU to improve its accuracy."}], "references": [{"title": "Nueral Machine Translation by Jointly Learning to Align and Translate", "author": ["D. Bahdanau", "K. Cho", "Y. Bengio"], "venue": "In Int\u2019l Conf Learning Represenations,", "citeRegEx": "Bahdanau et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Bahdanau et al\\.", "year": 2015}, {"title": "Learning Longterm Dependencies with Gradient Descent is Difficult", "author": ["Y. Bengio", "P. Simard", "P. Frasconi"], "venue": "IEEE Trans.Neural Networks,", "citeRegEx": "Bengio et al\\.,? \\Q1994\\E", "shortCiteRegEx": "Bengio et al\\.", "year": 1994}, {"title": "Learning to Forget: Continual Prediction with LSTM", "author": ["F.A. Gers", "J. Schmidhuber", "F. Cummins"], "venue": "In Proc. Int\u2019l Conf Artificial Neural Networks,", "citeRegEx": "Gers et al\\.,? \\Q1999\\E", "shortCiteRegEx": "Gers et al\\.", "year": 1999}, {"title": "Learning Precise Timing with LSTM Recurrent Networks", "author": ["F.A. Gers", "N.N. Schraudolph", "J. Schmidhuber"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Gers et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Gers et al\\.", "year": 2002}, {"title": "Framewise Phoneme Classification with Bidirectional LSTM Networks", "author": ["A. Graves", "J. Schmidhuber"], "venue": "Neural Networks,", "citeRegEx": "Graves and Schmidhuber,? \\Q2005\\E", "shortCiteRegEx": "Graves and Schmidhuber", "year": 2005}, {"title": "Speech Recognition with Deep Recurrent Neural Networks", "author": ["A. Graves", "Mohamed", "A.-R", "G. Hinton"], "venue": "In Proc. Int\u2019l Conf. Acoustics, Speech and Signal Processing (ICASSP),", "citeRegEx": "Graves et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Graves et al\\.", "year": 2013}, {"title": "Long Short-Term Memory", "author": ["S. Hochreiter", "J. Schmidhuber"], "venue": "Neural Computation,", "citeRegEx": "Hochreiter and Schmidhuber,? \\Q1997\\E", "shortCiteRegEx": "Hochreiter and Schmidhuber", "year": 1997}, {"title": "An Empirical Exploration of Recurrent Network Architectures", "author": ["R. Jozefowicz", "W. Zaremba", "I. Sutskever"], "venue": "In Proc. Int\u2019l Conf. on Machine Learning,", "citeRegEx": "Jozefowicz et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Jozefowicz et al\\.", "year": 2015}, {"title": "Deep visual-semantic alignments for generating image descriptions", "author": ["A. Karpathy", "L. Fei-Fei"], "venue": "In Proc. IEEE Int\u2019l Conf. on Computer Vision and Pattern Recognition,", "citeRegEx": "Karpathy and Fei.Fei,? \\Q2015\\E", "shortCiteRegEx": "Karpathy and Fei.Fei", "year": 2015}, {"title": "ImageNet Classification with Deep Convolutional Neural Networks", "author": ["A. Krizhevsky", "I. Sutskever", "G.E. Hinton"], "venue": "In Proc. Advances in Neural Information Processing Systems", "citeRegEx": "Krizhevsky et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Krizhevsky et al\\.", "year": 2012}, {"title": "A Simple Way to Initialize Recurrent Networks of Rectified Linear Units", "author": ["Q.V. Le", "N. Jaitly", "G.E. Hinton"], "venue": "arXiv preprint arXiv:1504.00941,", "citeRegEx": "Le et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Le et al\\.", "year": 2015}, {"title": "Phrasebased Image Captioning", "author": ["R. Lebret", "P.O. Pinheiro", "R. Collobert"], "venue": "In Proc. Int\u2019l Conf. on Machine Learning,", "citeRegEx": "Lebret et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Lebret et al\\.", "year": 2015}, {"title": "Gradientbased learning applied to document recognition", "author": ["Y. LeCun", "L. Bottou", "Y. Bengio", "P. Haffner"], "venue": "Proceedings of the IEEE,", "citeRegEx": "LeCun et al\\.,? \\Q1998\\E", "shortCiteRegEx": "LeCun et al\\.", "year": 1998}, {"title": "Learning Word Vectors for Sentiment Analysis", "author": ["A.L. Maas", "R.E. Daly", "P.T. Pham", "D. Huang", "A.Y. Ng", "C. Potts"], "venue": "In Proc. 49th Annual Meeting of the ACL,", "citeRegEx": "Maas et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Maas et al\\.", "year": 2011}, {"title": "Building a Large Annotated Corpus of English: The Penn Treebank", "author": ["M.P. Marcus", "B. Santorini", "M.A. Marcinkiewicz"], "venue": "Computational linguistics,", "citeRegEx": "Marcus et al\\.,? \\Q1993\\E", "shortCiteRegEx": "Marcus et al\\.", "year": 1993}, {"title": "Learning Longer Memory in Recurrent Neural Networks", "author": ["T. Mikolov", "A. Joulin", "S. Chopra", "M. Mathieu", "M. Ranzato"], "venue": "In Int\u2019l Conf Learning Represenations,", "citeRegEx": "Mikolov et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Mikolov et al\\.", "year": 2015}, {"title": "Convolutional LSTM Network: A Machine Learning Approach for Precipitation Nowcasting", "author": ["X. Shi", "Z. Chen", "H. Wang", "Yeung", "D.-Y", "W.K. Wong", "Woo", "W.-C"], "venue": "In Proc. Advances in Neural Information Processing Systems", "citeRegEx": "Shi et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Shi et al\\.", "year": 2015}, {"title": "Unsupervised Learning of Video Representations using LSTMs", "author": ["N. Srivastava", "E. Mansimov", "R. Salakhutdinov"], "venue": "In Proc. Int\u2019l Conf. on Machine Learning,", "citeRegEx": "Srivastava et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Srivastava et al\\.", "year": 2015}, {"title": "Sequence to Sequence Learning with Neural Networks", "author": ["I. Sutskever", "O. Vinyals", "Q.V. Le"], "venue": "In Proc. Advances in Neural Information Processing Systems", "citeRegEx": "Sutskever et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Sutskever et al\\.", "year": 2014}, {"title": "Show, Attend and Tell: Neural Image Caption Generation with Visual Attention", "author": ["K. Xu", "J. Ba", "R. Kiros", "K. Cho", "A. Courville", "R. Salakhudinov", "R. Zemel", "Y. Bengio"], "venue": null, "citeRegEx": "Xu et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Xu et al\\.", "year": 2015}, {"title": "Recurrent Neural Network Regularization", "author": ["W. Zaremba", "I. Sutskever", "O. Vinyals"], "venue": "arXiv preprint arXiv:1409.2329,", "citeRegEx": "Zaremba et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Zaremba et al\\.", "year": 2014}, {"title": "Visualizing and Understanding Convolutional Networks", "author": ["M.D. Zeiler", "R. Fergus"], "venue": "In Proc. European Conf. Computer Vision, volume LNCS", "citeRegEx": "Zeiler and Fergus,? \\Q2014\\E", "shortCiteRegEx": "Zeiler and Fergus", "year": 2014}], "referenceMentions": [{"referenceID": 12, "context": "For example, convolutional neural networks (CNN) are very effective in handling image data in which 2D spatial relationships are critical among the set of raw pixel values in an image (LeCun et al., 1998; Krizhevsky et al., 2012).", "startOffset": 184, "endOffset": 229}, {"referenceID": 9, "context": "For example, convolutional neural networks (CNN) are very effective in handling image data in which 2D spatial relationships are critical among the set of raw pixel values in an image (LeCun et al., 1998; Krizhevsky et al., 2012).", "startOffset": 184, "endOffset": 229}, {"referenceID": 18, "context": "In sequence modeling, recurrent neural networks (RNN) have been very successful in language translation (Cho et al., 2014; Sutskever et al., 2014; Bahdanau et al., 2015), speech recognition (Graves et al.", "startOffset": 104, "endOffset": 169}, {"referenceID": 0, "context": "In sequence modeling, recurrent neural networks (RNN) have been very successful in language translation (Cho et al., 2014; Sutskever et al., 2014; Bahdanau et al., 2015), speech recognition (Graves et al.", "startOffset": 104, "endOffset": 169}, {"referenceID": 5, "context": ", 2015), speech recognition (Graves et al., 2013), image captioning, i.", "startOffset": 28, "endOffset": 49}, {"referenceID": 19, "context": "summarizing the semantic meaning of an image into a sentence (Xu et al., 2015; Karpathy & Fei-Fei, 2015; Lebret et al., 2015), recognizing actions in videos (Donahue et al.", "startOffset": 61, "endOffset": 125}, {"referenceID": 11, "context": "summarizing the semantic meaning of an image into a sentence (Xu et al., 2015; Karpathy & Fei-Fei, 2015; Lebret et al., 2015), recognizing actions in videos (Donahue et al.", "startOffset": 61, "endOffset": 125}, {"referenceID": 17, "context": ", 2015), recognizing actions in videos (Donahue et al., 2015; Srivastava et al., 2015), or short-term precipitation prediction (Shi et al.", "startOffset": 39, "endOffset": 86}, {"referenceID": 16, "context": ", 2015), or short-term precipitation prediction (Shi et al., 2015).", "startOffset": 48, "endOffset": 66}, {"referenceID": 6, "context": "Proposed by Hochreiter and Schmidhuber (1997), the Long Short-Term Memory (LSTM) model and its variants have been the overall best performing RNN.", "startOffset": 12, "endOffset": 46}, {"referenceID": 3, "context": "(1999) to the original LSTM, and a peephole connection made it even more complex (Gers et al., 2002).", "startOffset": 81, "endOffset": 100}, {"referenceID": 2, "context": "For example, a forget gate was added by Gers et al. (1999) to the original LSTM, and a peephole connection made it even more complex (Gers et al.", "startOffset": 40, "endOffset": 59}, {"referenceID": 7, "context": "Very recently there have been empirical evaluations on LSTM, GRU, and their variants (Chung et al., 2014; Jozefowicz et al., 2015; Greff et al., 2015).", "startOffset": 85, "endOffset": 150}, {"referenceID": 7, "context": ", 2014) and (Jozefowicz et al., 2015) both show that more gates do not lead to better accuracy.", "startOffset": 12, "endOffset": 37}, {"referenceID": 7, "context": "Evaluations in (Chung et al., 2014; Jozefowicz et al., 2015; Greff et al., 2015) agreed that RNN with a gated unit works significantly better than a RNN with a simple tanh unit without any gate.", "startOffset": 15, "endOffset": 80}, {"referenceID": 1, "context": "RNN in the simple form suffers from the vanishing or exploding gradient issue, which makes learning RNN using gradient descent very difficult in long sequences (Bengio et al., 1994; Hochreiter & Schmidhuber, 1997).", "startOffset": 160, "endOffset": 213}, {"referenceID": 2, "context": "The original LSTM (Hochreiter & Schmidhuber, 1997) does not include the forget gate, which was later introduced in (Gers et al., 1999).", "startOffset": 115, "endOffset": 134}, {"referenceID": 2, "context": "The original LSTM (Hochreiter & Schmidhuber, 1997) does not include the forget gate, which was later introduced in (Gers et al., 1999). Gers et al. (2002) makes LSTM even more complicated by allowing the three gates (f t, it, ot) to take ct\u22121 or ct as an additional input, called the peephole connections.", "startOffset": 116, "endOffset": 155}, {"referenceID": 7, "context": "Similar observations were also corroborated in (Jozefowicz et al., 2015).", "startOffset": 47, "endOffset": 72}, {"referenceID": 15, "context": "Instead of using gates to control information flow in RNN, the Structurally Constrained Recurrent Network (SCRN) added a hidden context vector st to simple RNN, which changes slowly over time if the parameter \u03b1 is large (Mikolov et al., 2015), as", "startOffset": 220, "endOffset": 242}, {"referenceID": 15, "context": "SCRN has still fewer parameters than GRU, and has shown similar performance as LSTM in (Mikolov et al., 2015).", "startOffset": 87, "endOffset": 109}, {"referenceID": 12, "context": "Especially in the MNIST dataset (LeCun et al., 1998), IRNN significantly outperforms LSTM.", "startOffset": 32, "endOffset": 52}, {"referenceID": 10, "context": "The same initialization trick was used in (Le et al., 2015) too.", "startOffset": 42, "endOffset": 59}, {"referenceID": 9, "context": "Le et al. (2015) showed that minor changes to the simple RNN architecture can significantly improve its accuracy.", "startOffset": 0, "endOffset": 17}, {"referenceID": 7, "context": "Similarly, Jozefowicz et al. (2015) also showed that proper initialization is also important for LSTM.", "startOffset": 11, "endOffset": 36}, {"referenceID": 7, "context": "Jozefowicz et al. (2015) proposed three variants of GRU.", "startOffset": 0, "endOffset": 25}, {"referenceID": 7, "context": ", on the importance of the output gate Jozefowicz et al. (2015) and Greff et al.", "startOffset": 39, "endOffset": 64}, {"referenceID": 7, "context": ", on the importance of the output gate Jozefowicz et al. (2015) and Greff et al. (2015)", "startOffset": 39, "endOffset": 88}, {"referenceID": 7, "context": "As was shown in the evaluations (Chung et al., 2014; Jozefowicz et al., 2015), GRU has comparable accuracy with LSTM, and has fewer parameters than LSTM.", "startOffset": 32, "endOffset": 77}, {"referenceID": 10, "context": "The adding problem was originally proposed in (Hochreiter & Schmidhuber, 1997), and we use the variant in (Le et al., 2015).", "startOffset": 106, "endOffset": 123}, {"referenceID": 13, "context": "This dataset was generated in (Maas et al., 2011).", "startOffset": 30, "endOffset": 49}, {"referenceID": 12, "context": "The MNIST dataset by LeCun et al. (1998) contains images of handwritten digits (\u20180\u2019\u2013\u20189\u201d).", "startOffset": 21, "endOffset": 41}, {"referenceID": 20, "context": "For this dataset, we work on the word-level prediction task, which is the same as the version in (Zaremba et al., 2014).", "startOffset": 97, "endOffset": 119}, {"referenceID": 20, "context": "As in (Zaremba et al., 2014), we use two layers and the sequence length is 35.", "startOffset": 6, "endOffset": 28}, {"referenceID": 14, "context": "The Penn TreeBank (PTB) dataset provides data for language modeling, which is released by Marcus et al. (1993). For this dataset, we work on the word-level prediction task, which is the same as the version in (Zaremba et al.", "startOffset": 90, "endOffset": 111}, {"referenceID": 7, "context": "We can also compare MGU with the results in (Jozefowicz et al., 2015).", "startOffset": 44, "endOffset": 69}, {"referenceID": 7, "context": "55, lower than the GRU (with 5M parameters) result in (Jozefowicz et al., 2015), which is 108.", "startOffset": 54, "endOffset": 79}, {"referenceID": 7, "context": "02, also lower than the GRU result in (Jozefowicz et al., 2015).", "startOffset": 38, "endOffset": 63}], "year": 2016, "abstractText": "Recently recurrent neural networks (RNN) has been very successful in handling sequence data. However, understanding RNN and finding the best practices for RNN is a difficult task, partly because there are many competing and complex hidden units (such as LSTM and GRU). We propose a gated unit for RNN, named as Minimal Gated Unit (MGU), since it only contains one gate, which is a minimal design among all gated hidden units. The design of MGU benefits from evaluation results on LSTM and GRU in the literature. Experiments on various sequence data show that MGU has comparable accuracy with GRU, but has a simpler structure, fewer parameters, and faster training. Hence, MGU is suitable in RNN\u2019s applications. Its simple architecture also means that it is easier to evaluate and tune, and in principle it is easier to study MGU\u2019s properties theoretically and empirically.", "creator": "LaTeX with hyperref package"}}}