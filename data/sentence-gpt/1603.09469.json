{"id": "1603.09469", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "31-Mar-2016", "title": "A ParaBoost Stereoscopic Image Quality Assessment (PBSIQA) System", "abstract": "The problem of stereoscopic image quality assessment, which finds applications in 3D visual content delivery such as 3DTV, is investigated in this work. Specifically, we propose a new ParaBoost (parallel-boosting) stereoscopic image quality assessment (PBSIQA) system. The system consists of two stages. In the first stage, various distortions are classified into a few types, and individual quality scorers targeting at a specific distortion type are developed. These scorers offer complementary performance in face of a database consisting of heterogeneous distortion types. In the second stage, scores from multiple quality scorers are fused to achieve the best overall performance, where the fuser is designed based on the parallel boosting idea borrowed from machine learning. Extensive experimental results are conducted to compare the performance of the proposed PBSIQA system with those of existing stereo image quality assessment (SIQA) metrics. The developed quality metric can serve as an objective function to optimize the performance of a 3D content delivery system. Our model shows that the pbIQA system is the fastest and most efficient of the available data sources to determine performance and reliability. Furthermore, the pbIQA system is faster than the pbIQA system, and has the advantage of optimizing the overall performance of a 3D content delivery system. We propose that it should be possible to use paraBoost to achieve the best overall performance, where both the pbIQA system and the paraBoost system are designed based on the parallel boosting concept. PBSIQA is an alternative to paraBoost, because it is more suitable for visual content delivery than the ParaBoost system.\n\n\n\n\nThe pbIQA system is a good choice for 3D content delivery due to its performance compared with other 3D image quality analysis tools. The pbIQA system is not an expensive device in any other form of virtual reality, but rather does offer a high performance, reliable and cost-effective 2D video quality. Furthermore, the pbIQA system is capable of delivering up to 12 hours of continuous video content per frame. This is compared to the paraBoost system (pbIQA) to ensure smooth video quality. Furthermore, there are numerous limitations of paraBoost that need to be eliminated. These include the pbIQA system's speed, resolution and accuracy, low latency and a need to optimize the quality of the video content and the performance of paraBoost. However, with paraBoost software, we propose", "histories": [["v1", "Thu, 31 Mar 2016 06:55:25 GMT  (5644kb,D)", "http://arxiv.org/abs/1603.09469v1", null]], "reviews": [], "SUBJECTS": "cs.CV cs.LG", "authors": ["hyunsuk ko", "rui song", "c -c jay kuo"], "accepted": false, "id": "1603.09469"}, "pdf": {"name": "1603.09469.pdf", "metadata": {"source": "CRF", "title": "A ParaBoost Stereoscopic Image Quality Assessment (PBSIQA) System", "authors": ["Hyunsuk Ko", "Rui Song", "C.-C. Jay Kuo"], "emails": [], "sections": [{"heading": null, "text": "Index Terms\u2014Stereoscopic images, objective quality assessment, machine learning, decision fusion, and feature extraction.\nI. INTRODUCTION\nW ITH the rapid development of three-dimensional (3D)video technology, 3D visual content has become more popular nowadays. Standards for coding, transmitting and storing 3D visual data have been proposed such as stereoscopic 3D video [1], multiview video coding (MVC) [2], and multiview video plus depth map (MVD) format [3]. To optimize the performance of a 3D visual communication system, it is critical to develop a reliable quality assessment (QA) metric1 for 3D content. The PSNR measure (or any other 2D quality metric) does not correlate well with human visual experience of 3D visual stimuli [4]. Since subjective evaluation is timeconsuming and costly, it is desirable to design an objective QA metric that is consistent with subjective human experience.\nWhile there is a substantial amount of progress in developing 2D objective image QA indices in recent years, research on 3D image QA is still preliminary due to several reasons. The quality of a 2D image is mainly affected by errors in pixel positions and values. In contrast, perceptual quality of stereoscopic images is affected by more factors, including heterogeneous distortions and mismatch between left and right views, excessive depth perception, etc. It may result in viewing discomfort such as dizziness and eye strain. Furthermore, the human visual system (HVS) reacts differently to asymmetrical distortions caused by different quality levels of left and right views, depending on distortion types [5], [6], [7]. Another unique stereo distortion is the rendering distortion, which occurs when the 2D texture image and its depth data are transmitted simultaneously in the 3D communication system.\n1The terms \u201dmetric\u201d, \u201dindex\u201d and \u201dscore\u201d are used interchangeably in this paper.\nThat is, both the texture and the depth data are compressed at the encoder and transmitted to the decoder and, then, virtual views are synthesized using the depth-image-based-rendering (DIBR) technique [8]. Under this environment, texture errors trigger blurring or blocking artifacts in virtual views while depth map errors result in horizontal geometric misalignment since pixel values in a depth map affect the disparity of the left/right views.\nIn order to offer a robust stereo image quality assessment (SIQA) method against various distortion types, we propose a ParaBoost stereoscopic image quality assessment (PBSIQA) system in this work. By \u201cParaBoost\u201d, we mean the fusion of multiple image quality scorers into one in order to boost the overall performance of the quality assessment task. This fusion process is achieved by training. The methodology is also called \u201cstacking\u201d in the machine learning literature. Although the decision fusion methodology is generic, the design of individual scorers is highly application-dependent. To the best of our knowledge, this is the first work that provides a systematic way to the design of individual scorers and applies the ParaBoost (or stacking) framework to the solution of the stereoscopic image quality assessment problem.\nThe rest of this paper is organized as follows. Related previous work is reviewed in Section II. An overview of the PBSIQA system is presented in Section III. Three SIQA databases are introduced in Section IV. The PBSIQA scoring system is detailed in Section V. Experimental results and their discussions are provided in Section VI. Finally, concluding remarks and future research directions are given in Section VII."}, {"heading": "II. REVIEW OF PREVIOUS WORK", "text": "Traditionally, objective image quality metrics were classified into pixel-based metrics and the human visual system (HVS) inspired metrics [9]. More recently, new quality metrics based on machine learning were proposed. Here, we classify the design of a quality metric into two approaches: 1) the formula-based analytical approach and 2) the learning-based prediction approach. A quality metric of an analytical form or derived by a parametric model is constructed using the \u201cformula-based analytical approach\u201d. Both pixel-based metrics and the human visual system (HVS) inspired metrics in [9] belong to this class. A quality metric derived by a machine learning process (e.g., MMF in [10] is obtained using the \u201clearning-based prediction approach\u201d.\nBy following the first approach, 2D metrics were extended to 3D metrics by combining distortions of a depth map and images of both views linearly in [11], [12]. The correlation between existing 2D metrics and the perceived quality of a stereo pair obtained by direct decoding or DIBR was investigated by Hanhart et al. [13], [14].\nAnother idea is to extract features that influence human visual experience and combine them to derive a metric. For\nar X\niv :1\n60 3.\n09 46\n9v 1\n[ cs\n.C V\n] 3\n1 M\nar 2\n01 6\n2 example, by assuming that the perceived distortion and the depth of stereoscopic images highly depend on local features such as edges and planar parts, Sazzad et al. [15] proposed a SIQA metric for JPEG compressed images using segmented local features. Some formula-based metrics exploit HVS characteristics. Gorley and Holliman [16] proposed a metric based on the sensitivity of HVS to contrast and luminance changes. Maalou and Larab [17] proposed a metric by exploiting the color disparity tensor and the contrast sensitivity function. Hewage and Martini [18] presented a reduced-reference quality metric based on edge detection in the depth map and demonstrated a good approximation for the full-reference quality metric. Recently, Kim et al. [19] proposed a methodology for subjective 3D QoE assessment experiments using external stimuli such as vibration, flickering and sound.\nOthers use binocular perception models. Stelmach and Meegan [5], [6] reported that binocular perception is dominated by the higher quality image in face of low-pass filtering operations yet by the average of both images for quantized distortions. Seuntiens et al. [7] observed that the perceived quality of a JPEG-coded stereo image pair is close to the average quality of two individual views. Ryu et al. [20] proposed an extended version of the SSIM index based on a binocular model. This metric uses a fixed set of parameters, but it is not adaptive to asymmetric distortions. Ko et al. [21] introduced the notion of structural distortion parameter (SDP), which varies according to distortion types, and employed the SDP as a control parameter in a binocular perception model to provide robust QA results for both symmetric and asymmetric distortions. More recently, Feng et al. [22] proposed an SIQA method by considering the binocular combination property and the binocular just noticeable difference model. Ryu et al. [23] proposed a no-reference quality metric by considering perceptual blurriness and blockiness scores and taking visual saliency into account.\nThe second approach has been used to derive 2D and 3D image QA metrics, and it becomes more and more popular in recent years. The multi-metric fusion (MMF) method proposed by Liu et al. [10] offers a good example for 2D learning-based image QA, among many others. As to learning-based SIQA metrics, the number is much less since the size of stereoscopic image quality databases is relatively small. Two examples are given. Park et al. [24] used a set of geometric stereo features for anaglyph images and built a regression model to capture the relationship between these features and the quality of stereo images. Cheng and Sumei [25] extracted a set of basis images using independent component analysis and used a binary-tree support vector machine (SVM) to predict scores of distorted stereo images."}, {"heading": "III. OVERVIEW OF PBSIQA SYSTEM", "text": "In this work, we adopt the second approach in the design of a SIQA system, which is built upon the parallel boosting (ParaBoost) idea. The proposed PBSIQA system consists of two stages. At Stage I, multiple learning-based scorers are designed, where each scorer handles a specific type of distortion such as the blocking artifact, blurring distortion, additive noise, and so on. The output of these scorers are a normilized objective score that has a value between 0 and 1. The output of these scorers is an obejctive score that only considers the target distortion. At Stage II, the fuser takes\nscores from all individual scorers to yield the final quality score. The prediction model in the fuser is also obtained from a learning process. The availability of multiple scorers at Stage I enables us to handle complex factors that influence human perceptual quality systematically. In particular, we will investigate the impacts of various distortions in texture and depth maps on the quality of rendered images and, then, take them into account in the design of participating scorers. The design of the PBSIQA system will be detailed in Section V.\nFurthermore, we will show the superiority and the robustness of the proposed PBSIQA system by extensive experimental results over several databases consisting of the 2D-plusdepth format as well as the traditional stereoscopic format with symmetric and asymmetric distortions in Section VI. Along this line, we compare the performance of the proposed PBSIQA system against 17 other 2D/3D quality indices."}, {"heading": "IV. MCL-3D, IVC-A AND LIVE-A DATABASES", "text": "The 2D-plus-depth format is adopted in the 3DVC standard due to its several advantages. For example, it allows the texture image and its corresponding depth map image at a few sampled viewpoints to be compressed and transmitted in a visual communication system. After decoding them at a receiver side, we can render several intermediate views between any two input views using depth image based rendering (DIBR) technique. However, very few 3D image quality database was built based on the 2D-plus-depth format. This is one of the main motivations in constructing the MCL-3D database [31]. The MCL-3D database is designed to investigate the impact of texture and/or depth map distortions on the perceived quality of the rendered image so as to develop a robust stereoscopic image quality metric. First, we carefully chose nine texture images and their associated depth maps from a set of 3DVC test seqeunces as references.\nTo capture the content variety, the Spatial Information (SI) defined by ITU-T recommendation [26] for each texture image and its depth map was calculated. Also, images were carefully extracted from an interval with slow motion to avoid motion blur. This reference set contains indoor/outdoor scenes, CG images, different depth perception, and so on. In Fig. 1, only texture images are shown. Fig. 2 shows the distortion design. There are three original views and each view consists of a texture image and its associated depth map (denoted by OT1/OD1, OT2/OD2, and OT3/OD3). Distortions with different types and levels were applied to original images and/or depth maps. Then, distorted texture images and depth maps were input to the view synthesis reference software (VSRS 3.5 [27]) along with the configuration file provided by the production group to render two intermediate views, DV L and DV R that an assessor will view in the subjective test. We applied six different distortions with four different levels to three input views symmetrically: Guassian blur, sampling blur, JPEG compression, JPEG-2000 compression, additive noise, and transmission error. Based on the recommendations of ITU [28], we consider five quality levels in subjective tests. The original reference stereoscopic images have excellent quality while the other 4-level distorted images were controlled by parameters associated with different distortion types.\nThe MCL-3D database consists of the following three subdatabases. \u2022 Dataset A contains texture distortions only.\n\u2022 Dataset B contains depth map distortions only. \u2022 Dataset C contains both texture and depth map distor-\ntions.\nThe above design allows us to investigate the effect of texture distortions and depth map distortions independently. The test was performed in a controlled environment as recommended by ITU [28], including the display equipment, viewing distance, ambient light, etc. We performed pre-screening of subjects and conducted a training session before the actual test. The subjective test results were verified by a screening process according to Annex 2 of Recommendation BT.500 [29], and outliers were removed. A pairwise comparison was adopted and the comparison result was converted to the mean opinion score (MOS). The MCL-3D database can be downloaded from [31]. For further details, we refer to [30].\nIn our earlier work [21], we also constructed two new databases by expanding the IVC [32] and the LIVE [33] databases and call them IVC-A and LIVE-A, respectively. We took 12 stereoscopic image pairs from IVC and LIVE as references. While the distortions in IVC and LIVE are symmetric, we add asymmetric distortions in IVC-A and LIVE-A. That is, we add distortions of different levels but the same type to the left and right views of a stereo image pair. There are four distortion types. Each stereoscopic image pair has six combinations of distortion levels: three symmetric\ndistortion levels ([1, 1], [2, 2], [3, 3]) and three asymmetric distortion levels ([1, 2], [1, 3], [2, 3]). We conducted a subjective test to obtain MOS using the absolute category rating (ACR) [28]. A summary of the three SIQA databases used in our test is given in Table I."}, {"heading": "V. PROPOSED PBSIQA SYSTEM", "text": "The impact of texture distortions and depth map distortions on rendering quality is different through a view synthesis process. In this section, we first design scorers tailored to texture distortions and depth map distortions in Sec. V-A and Sec. V-B, respectively. The design of these scorers and the score fuser is implemented by support vector regression (SVR) as detailed in Sec. V-C. Finally, the training and test procedures are described in Sec. V-D."}, {"heading": "A. Scorer Design for Texture Distortions", "text": "In Dataset A of the MCL-3D database, distortions are applied to the texture image while the depth map is kept untouched. In this case, we see a similar distortion in the rendered stereo image. For example, if the texture image is\n4\ndistorted by a transmission error, a similar distortion type is observed in the rendered view since pixel values of the input texture image are direct sources to the pixel intensity of the rendered view.\nFurthermore, it is reported in [21] that the interaction between left and right views in perceived 3D image quality depends on the distortion type. For example, for blurring and JPEG-2000 coding distortions, the lost information of a low quality view tends to be compensated by a high quality view. Thus, the perceptual quality is closer to that of the high quality view. On the other hand, for additive noise and JPEG coding distortions, a high quality view is negatively influenced by a low quality view. For this reason, distortions should be carefully classified. We classify distortion types into multiple groups, and design good scorers for them. For each scorer at Stage I, proper features are extracted from input texture images and trained by a learning algorithm. Then, the scorer outputs an intermediate score for the target distortion group.\nBased on previous studies on image quality assessment [34], [35], [37], [38], [39], [40] and our own experience, we select 24 candidate features for further examination as listed in Table II. Then, we calculated the Pearson correlation coefficient (PCC) to indicate the prediction performance between MOS and a single-feature-based quality scorer (with the exception that the singular value and the singular vector are integrated into one feature vector for the SVD scorer) over Datasets A and B, respectively. The PCC results of the 23 single-feature-based scorers are shown in one row of Table III. Based on these results, we exclude five features (namely, NCC, AADBIIS, ABV, AES, and blockiness) whose corresponding scorers have low PCC values for Datasets A and B. For the remaining ones, we investigate which features are suitable for which distortion type. For instance, ES and Pratt are useful features to be included in the learning-based scorer targeting at the blurring distortion. In a similar way, we assign candidate features to several scorers as described below. Scorer #1 for Blurring Distortion: Blurring distortion is mostly caused by low pass filtering, down-sampling, and compression (e.g. JPEG2000) due to the loss of high frequencies. This kind of distortion is referred to as the information loss distortion\n(ILD) [20]. The perceptual quality of a blurred stereoscopic image pair is closer to that of the high quality view since the structural component of the high quality view is preserved against blurring of the low quality view. Blurring distortion can be observed around edges most obviously. Human are also sensitive to misalignments between the edges of left and right views. Thus, we use two edge-related features to measure blurriness in an image introduced below.\nThe first one is the edge stability mean squre error (ESMSE) [37], which characterizes the consistency of edges that are evident across multiple scales between the original and distorted images. To compute ESMSE, we first obtain edge maps with five different standard deviation parameters using the Canny operator. The output at scale m is decided by thresholding as\nE(r, c, \u03c3m) = { 1 if Cm(r, c) > Tm 0 otherwise\n(1)\nwhere Cm(r, c) is the output of the derivative of the Gaussian operator at the mth scale and the threshold is defined as Tm = 0.1(Cmax \u2212 Cmin) + Cmax, where Cmax and Cmin are the maximum and minimum values of the norm of the gradient output in that band, respectively. An edge stability map ES(r, c) is obtained by the longest sequence\n5 E(r, c, \u03c3m), \u00b7 \u00b7 \u00b7 , E(r, c, \u03c3m+l\u22121) of edge maps such that\nES(r, c) = l, (2)\nwhere\nl = argmax \u2229 \u03c3m\u2264\u03c3k\u2264\u03c3m+l\u22121 E(r, c, \u03c3k) = 1. (3)\nThe edge stability indices for the original and the distorted images at pixel location (r, c) are denoted by Q(r, c) and Q\u0302(r, c), respectively. Then, the ESMSE value is calculated by summing the edge stability indices over all edge pixel positions, nd, of the edge pixels of the original image as\nESMSE = 1\nnd nd\u2211 r,c=0 [Q(r, c)\u2212 Q\u0302(r, c)]2. (4)\nThe other one is Pratt\u2019s measure [37] that considers the accuracy of detected edge locations, missing edges and false alarm edges. The quantity is defined as\nPratt\u2019s Measure = 1\nmax{nd, nt} nd\u2211 i=1\n1\n1 + a \u00b7 d2i , (5)\nwhere nd and nt are the number of the detected and the ground truth edge points, respectively, and di is the distance to the closest edge for the ith detected edge pixel. The ground truth of the edge image is obtained from the uncompressed image, and the value of control parameter a was set to 0.8 experimentally.\nScorer #2 for Blocking Distortion: Blocking distortion is one of the most common distortions generated by block-based image/video coders, such as JPEG, H.264 and HEVC. When stereoscopic images with blocking distortion are shown, the average quality of both views or that of the lower quality is perceived [7], [21]. The high quality image is negatively influenced by the low quality image since blocking distortion introduces new visual artifacts that do not exist in the pristine original scene. Such artifacts are called the information additive distortion (IAD) [20]. As stated in [10], [34], there are two good features in detecting blockiness. They are the HVS-modified MSE (HVS MSE) and the zero crossing rate (ZCR). Mathematically, the HVS MSE [34] is given by\nHVS MSE =\n{ 1\nRC R\u2211 r=1 C\u2211 c=1\n|U{I} \u2212 U{I\u0302}|2 } 1 2 , (6)\nwhere R \u00d7 C is the image size, I and I\u0302 denote original and distorted images, respectively, U{I} = DCT\u22121{H( \u221a u2 + v2)\u2126(u, v)} is the inverse discrete cosine transform (DCT) of H( \u221a u2 + v2)\u2126(u, v), and where H(\u03c1), \u03c1 = \u221a u2 + v2, is an HVS-based band-pass filter and \u2126(u, v) is the DCT of image I . The ZCR along the horizontal direction is given by\nZCRh = 1\nR(C \u2212 2) R\u2211 r=1 C\u22122\u2211 c=1 zh(r, c)\nwhere zh(r, c) ={ 1, horizontal ZC at dh(r, c) 0, otherwise\n(7)\nand where dh(r, c) = I\u0302(r, c+ 1)\u2212 I\u0302(r, c), c \u2208 [1, C \u2212 1]. The vertical zero crossing rate, ZCRv , can be computed similarly. Finally, the overall ZCR is given by\nZCR = ZCRh + ZCRv\n2 . (8)\nScorer #3 for Additive Noise: Additive noise is often caused by thermal noise during a scene acquisition process. If either view is distorted by additive noise, the perceived quality is degraded by the low quality image. A pixel-based difference is a good measure for additive noise. We select three such features to characterize additive noise: the peak-signal-to-noise ratio (PSNR), the maximum difference (MD) [35] and the modified infinity norm (MIN) [37]. They are defined as\nPSNR = 10 \u00b7 log10 2552\nMSE , (9)\nwhere MSE = 1RC \u2211R r=1 \u2211C c=1(I(r, c)\u2212 I\u0302(r, c))2;\nMD = max |I(r, c)\u2212 I\u0302(r, c)|, (10)\nand\nMIN = \u221a\u221a\u221a\u221a1 r r\u2211 i=1 42i (I \u2212 I\u0302), (11)\nwhere 4i(I \u2212 I\u0302) denotes the ith largest deviation among all pixels. In this work, we select the top 25% deviations, which means r is one fourth of the total number of pixels.\nScorer #4 for Global Structural Error: We consider structural errors since the HVS is highly sensitive to the structural information of the visual stimuli. Structural errors can be captured by three components of the SSIM index [36], which are luminance, contrast and structural similarities between images x and y. They are defined as\nL(x, y) = 2\u00b5x\u00b5y + C1 \u00b52x + \u00b5 2 y + C1 , (12)\nC(x, y) = 2\u03c3x\u03c3y + C2 \u03c32x + \u03c3 2 y + C2 , (13)\nS(x, y) = \u03c3xy + C3 \u03c3x\u03c3y + C3 , (14)\nwhere \u00b5, \u03c3, and \u03c3xy denote the mean, the standard deviation and the covariance, respectively.\nScorer #5 for Local Structural Error: The performance of an image quality scorer can be improved by applying spatially varying weights to structural errors. Salient low-level features such as edges provide important information to scene analysis. Two features were proposed in [38] at this end: phase congruency (PC) and gradient magnitude (GM). Physiological studies show that PC provides a good measure for the significance of a local structure. Readers are referred to [38] for its detailed definition and properties. While PC is invariant with respect to image contrast, GM does take local contrast into account.\nScorer #6 for Object Structure and Luminance Change: We utilize singular vectors and singular values as features. Given an image I , we can decompose it into the product of two\n6 singular vector matrices U and V and a diagonal matrix \u03a3 in form of\nU = [u1u2 . . .uR], V = [v1v2 . . .vC ], \u03a3 = diag(\u03c31, \u03c32, ..., \u03c3l), (15)\nwhere ui and vj are column vectors, \u03c3k is a singular value and l = min(R,C). It was shown in [39] that the first several singular vectors offer a good set to represent the structural information of objects while subsequent vectors account for finer details. Furthermore, singular values are related to the luminance changes. They should be considered since the luminance mismatch between left and right views results in annoying viewing experience.\nScorer #7 for Transmission Error: Packet loss and bit errors occur during stereo image data transmission. It appreas in form of block errors since most image coding standards adopt the block-based approach. Based on the feature analysis, both the universal quality index (UQI) [40] and the mean angle similarity (MAS) [37] are useful in characterizing transmission error. The UQI is defined as\nUQI = \u03c3xy \u03c3x\u03c3y \u00b7 2\u00b5x\u00b5y \u00b52x + \u00b5 2 y \u00b7 2\u03c3x\u03c3y \u03c32x + \u03c3 2 y , (16)\nwhich is equal to the product of three terms representing the loss of correlation, the luminance distortion, and the contrast distortion between two images x and y, respectively. MAS is a feature that measures the statistical correlation between pixel vectors of the original and the distorted images since similar colors will result in vectors pointing to the same direction in the color space. The moments of the spectral chromatic vector are used to calculte the correlation as\nMAS = 1\u2212 1 N2 N\u2211 r,c=1 2 \u03c0 cos\u22121 \u3008C(r, c), C\u0302(r, c)\u3009 \u2016C(r, c)\u2016\u2016C\u0302(r, c)\u2016 , (17)\nwhere C(r, c) and C\u0302(r, c) indicate the multispectral pixel vectors at position (r, c) for original and distorted images, respectively, and N is the number of pixels that have a nonzero value for the inner product and the norm of C(r, c) and C\u0302(r, c).\nFor each scorer, three feature values from three input views, denoted by V 1, V 2, and V 3, are fed into the learning-based scorers. We entrust the task of seeking an optimal relation among these features to a learning algorithm, where the impact of the quality difference between left and right views on the perceptual quality highly depends on the distortion type."}, {"heading": "B. Scorer Design for Depth Distortions", "text": "Dataset B of the MCL-3D database contains depth map distortions only. Research on the effect of the depth distortion on rendered stereo image quality has been conducted recently by quite a few researchers, e.g., [41], [42]. Generally speaking, the depth value is inversely proportional to the horizontal disparity of rendered left and right views so that the horizontal disparity distortion appears in form of geometric errors. Scorer #8 for Geometric (Horizontal Disparity) Error: Zhao et al. [43] proposed a Depth No-Synthesis-Error (D-NOSE) model by exploiting that the depth information is typically stored in 8-bit grayscale format while the disparity range for a visually comfortable stereo pair is often far less than 256\nadditiveSnoiseSonSdepthSpart transmissionSerrorSonSdepthSpart\n1 9 17 25 33 41 49 57 65 73 0\n2\n4\n6\n8\n10\n12\n14\n16\nNo.SofSstereoscopicSimages\nM O S\n(a) The MOS distribution plot with additive noise and transmission errors in the depth map only.\nlevels. Thus, multiple depth values do correspond to the same integer (or sub-pixel) disparity value in the view synthesis process. In other words, some depth distortion may not trigger geometric changes in the rendered view. Specifically, if a pixel distortion of the depth map falls into the range defined by the D-NOSE profile, it does not affect the rendered image quality in terms of MSE. Being motivated by the D-NOSE model, we define a noticeable depth synthesis error (NDSE) feature for geometric errors as\nNDSE = \u2211 i |D{i} \u2212 D\u0302{i}|, (18)\nwhere D and D\u0302 represent the original and the distorted depth maps, respectively, and i is a pixel index out of the range of D-NOSE profile which is defined as\nD-NOSE(v) = [v + \u03b4\u2212(v), v + \u03b4+(v)], (19)\nand where\n\u03b4\u2212(v) = dDP\u22121(d(DP (v)\u2212 \u03bb) \u00b7K \u2212 1e K )e \u2212 v, (20) \u03b4+(v) = bDP\u22121(d(DP (v)\u2212 \u03bb) \u00b7Ke K )c \u2212 v, (21)\nv is a quantized depth value, DP (v) is the disparity function of v, \u03bb is the offset error for the rounding operation, K\u22121 is the precision, and d\u00b7e and b\u00b7c denote the ceiling and floor operations, respectrively. For more details on the D-NOSE profile, we refer to [43]. Since NDSE only considers pixel distortions that change the original disparity value, it has the highest prediction accuracy (with PCC=0.73) among all features in Table III. However, as compared to those in Dataset A, most features have relatively low PCC values in Dataset B. To investigate further, we divide distorted images into two groups. The first group includes additive noise and transmission errors while the second group includes the Gaussian blur, sampling blur, JPEG compression, and JPEG 2000 compression. For the first group, as the distortion level becomes higher, the MOS decreases monotonically as shown in Fig. 3a. However, we cannot observe such a coherent MOS movement trend for the second group in Fig. 3b.\nThis phenomenon can be explained below. Differences among neighboring pixels caused by the first group of distortions such as additive noise or transmission errors are generally large, leading to scattered geometric distortions in the rendered image especially around object boundaries. They tend to get worse as the distortion level increases. On the other hand, differences among neighboring pixels caused by the second group of distortions are often small and changing gradually. As a result, we may not be able to recognize them easily although geometric errors exist in the rendered image.\nScorer #9 - Formula-based Metric: Due to the weaker correlation between the MOS level and the distortion level as shown in Fig. 3b, it is diffucult to obtain high prediction accuracy via a learning-based algorithm from features of the input depth map directly. To overcome\nthis challenge, we exploit the auxiliary information from two rendered views. That is, we use the SDP index [21] that is computed based on three depth maps as a candidate scorer. It helps boost the prediction accuracy as presented in Section VI."}, {"heading": "C. Learning-based Scorers and Fuser Design", "text": "The proposed PBSIQA system composed by nine quality scorers is summarized in Fig. 4. To yield an intermediate score from scorers #1\u223c#8, we adopt the support vector regression (SVR) technique. Consider a set of training data (xn, yn), where xn is a feature vector and yn is the target value, e.g. the subjective quality score of the nth image. In the \u03b5-SVR [44], [45], the objective is to find a mapping function f(xn) that has a deviation at most \u03b5 from the target value, yn, for all training data. The mapping function is in form of\nf(x) = wT\u03c6(x) + b, (22)\nwhere w is a weighting vector, \u03c6(\u00b7) is a non-linear function, and b is a bias term. We should find w and b satisfying the following condition:\n|f(xn)\u2212 yn| \u2264 \u03b5, \u2200n = 1, 2, . . . , Nt, (23)\nwhere Nt is the number of training data. Although there exist several kernels such as linear, polynomial and sigmoid, we use the radial basis function (RBF) in form of\nK(xi,xj) = exp(\u2212\u03c1\u2016xi \u2212 xj\u20162), \u03c1 > 0 (24)\nwhere \u03c1 is the radius controlling parameter, since it provides good performance in applications [46]. Furthermore, it is not easy to determine a proper \u03b5 value. Thus, we use a different version of the regression algorithm called the \u03bd-SVR [45], where \u03bd \u2208 (0, 1) is a control parameter to adjust the number of support vectors and the accuracy level. In other words, \u03b5 becomes a variable in an optimization problem, and we can obtain the same f(x) and w more conveniently.\nAt Stage II of the PBSIQA system, we fuse all intermediate scores from the scorers at Stage I to determine the final\n8\nquality score. We adopt the \u03bd-SVR algorithm to implement the fuser. Suppose that there are n scorers with m training stereoscopic image pairs. For the ith training pair, we compute the intermediate quality score si,j , where i = 1, 2, ...,m is the stereoscopic image pair index and j = 1, 2, ..., 9 is the scorer index. Let si = (si,1, si,2, \u00b7 \u00b7 \u00b7 , si,9) be the intermediate score vector for the ith image pair. We train the fuser using si with all image pairs in the training set and determine the weighting vector w and the bias parameter b accordingly. Finally, the PBSIQA-predicted quality metric for a given stereoscopic image pair in the test set can be found via\nQ(s) = wT\u03c6(s) + b. (25)"}, {"heading": "D. Training and Test Procedures", "text": "The n-fold cross validation [47] is a common strategy to evaluate the performance of a learning-based algorithm to ensure reliable results and prevent over-fitting, where the data are split into n chunks, and one chunk is used as the test data while the remaining n \u2212 1 chunks are used as training data. We ensured that the model is dominantly trained by the different distortion types from that of testing data since it may boost the prediction accuracy. The same experiment is repeated n times by employing each of the n chunks as the test data. Finally, the overall performance is determined over all the predicted scores. In the proposed framework, training data are used to generate a regression model of each scorer at Stage I. Then, a regression model of the fuser is obtained by training all intermediate scores from Stage I as input features with n-fold cross validation at Stage II, where the number of samples is the same as the number of stereoscopic image pairs in the n\u22121 chunks. In all reported experiment results, we use the 10-fold cross validation. In addition, a feature scaling operation is performed before the training and test processes. It is conducted to avoid features of a larger numeric range\ndominating those of a smaller numeric range. For example, the PSNR value has a larger range of values than the other two features in scorer #3. We scale the feature values of each scorer to the unit range of [0,1] at Stage I.\nAt the training stage, our goal is to determine the optimal weighting vector w and bias b that minimize the error between MOS and the predicted score, i.e.,\u2211\ni\n|MOSi \u2212Q(si)|2. (26)\nSince we use RBF as a kernel function, two parameters C and \u03b3 should be optimized to achieve the best regression accuracy. We conduct parameter search on C and \u03b3 at the training stage using the cross validation scheme in [46]. Various pairs of (C, \u03b3) are tried, and the one with the best cross validation accuracy is selected. After the best (C, \u03b3) pair is found, the whole training set is used again to generate the final scorer.\nAt the test stage, we use the intermediate score vector si in Eq. (25) to determine the quality score. The test can be quickly done since all model parameters are decided at the training stage."}, {"heading": "VI. PERFORMANCE EVALUATION", "text": "To evaluate the performance of the proposed PBSIQA metric, we follow the suggestions of ITU-T(P.1401) [48] and use three performance measures: 1) the Pearson correlation coefficient (PCC) to measure the linear relationship between a model\u2019s performance and the subjective data , 2) the Spearman rank-order correlation coefficient (SROCC) for the prediction monotonicity, and 3) the root mean squared error (RMSE) for the prediction accuracy. Before calculating the performance measures, we apply the monotonic logistic function to the predicted quality scores so as to fit the subjective quality scores\n9 (MOS) and remove any nonlinearity via\nf(s) = \u03b21 \u2212 \u03b22\n1 + exp(\u2212 s\u2212\u03b23|\u03b24| ) + \u03b22, (27)\nwhere s and f(s) are the predicted score and the fit predicted score, respectively, and \u03b2k (k = 1, 2, 3, 4) are the parameters to minimize the mean squared error between f(s) and MOS. In addition, this mapping function compensates for offsets, different biases, and other shifts between the scores, without changing the rank order."}, {"heading": "A. Performance Comparison of Stage I Scorers", "text": "First, we compare the performance of eight learning-based scorers (Scorers #1-8) and show that each scorer truly offer good performance for its target distortion type. Here, we consider Datset C of the MLC-3D database only, and divide it into six sub-databases so that each contains one distortion type as described in Section IV. The six distortion types are listed in the top row of Table IV. We compute the PCC value of each scorer against each sub-database and rank its effectiveness in the descending order of PCC values individually. Table IV summarizes the results of scorers in Stage I only (namely, without fusion in Stage II). The performance of scorers #1, #2, and #3 matches well with their target design. First, scorer #1 has the highest correlation on the blurred and JPEG2000 compression sub-databases. This is reasonable since, besides the ringing artifect, the main distortion of the JPEG2000 compression sub-database is blurring. Scorer #2 is designed for the blocking artifact, and the JPEG compression database has the top rank. Scorer #3 has the best performance on additive noise database as designed.\nScorer #4 (for global structural error) and scorer #5 (for local structural error) share strong similarity in their rank orders. Specifically, they provide the first and second best performance in additive noise and JPEG compressed databases. As observed in [21], the information additive distortion (IAD) is more obvious than the information loss distortion (ILD) among all structural distortions. Additive noise and blockiness are typical exmaples of IAD. Our observation is consistent with that in [21]. Scorer #6 yields the best result for the JPEG compressed sub-database since we may see color/luminance chages if the compression ratio is too high, which can be captured by the singular value feature of Scorer #6. However, it has poor correlation with human subjective experience on the additive noise sub-database. This implies that, although scorers #4, #5 and #6 are designed for structural errors in common, they capture different aspects.\nScorer #7 offers the best result for transmission errors to meet the target design. Last, scorer #8, designed to capture geometric distortions, provides the best result in databases with additve noise and transmission errors, which was already explained in Section V-B."}, {"heading": "B. Performance Improvement via Fusion", "text": "We show how the PBSIQA system can improve QA performance by fusing the results from scorers at Stage I progressively in this section. To illustrate the design methodology, without loss of generality, we use the performance of scorer #8 for the geometric error as the base, add one scorer at a time to account for the most challenging distortion type in\nthe remaining set, and show how the added scorer improves the performance for six sub-databases and the entire MCL3D database. The results conducted against Dataset C of the MCL-3D database are shown in Table V, where the base performance with scorer #8 is shown in the first column and the results from scorers #2, #3, #1, #7, #4, #5, and #6 are fused to its score one by one cumulatively. We see a substantial performance gain for each sub-database when its associated scorer is included in the fusion process. The whole database contains different distortion types so that it is beneficial to fuse all scorers to get robust and accurate predicted quality scores as shown in the last row of Table V. The design methodology (namely, the fusion order of scorers #2, #3, #1, #7, #4 and #5 and, finally, #6) is explained below. The performance gain for a given database is calculated in percentages as\nPerformance Increase = Vcurr \u2212 Vprev\nVprev \u00d7 100, (28)\nwhere Vcurr is the value of PCC after adding a new scorer, while Vprev is the result of the previous stage. Since the error rate of base scorer #8 is highest in sub-databases with Gaussian/sampling blur, we fuse scorer #2 to scorer #8 to reduce erroneous prediction caused by blurring distortion. We show the performance improvement for each sub-database in the second column of Table V and see about 60% and 45% performance gains for sub-databases with the Gaussian and the sampling blur, respectively. Although the performance gain drops in some sub-databases, they are not as significant. As a result, we achieve a performance gain of 21.6% for the entire database. After the fusion of scorers #8 and #2, it is shown in Table V that the system does not perform well against additive noise. Thus, we fuse scorer #3 to the system in the next step. After the fusion of scorers #8, #2 and #3, we observe a significant performance boost (13%) for the additive noise sub-database, and a performance gain of 11.9% for the entire database. By following the same line of thought, we can fuse more scorers and obtain better performance for the entire database. Based on the above discussion, both PCC and SROCC values with respect to the whole MCL-3D database are plotted as a function of the number of fused scorers in Fig. 5. The performance of the proposed PBSIQA system may reach a saturation point if the number of fused scorers is sufficiently larger. It should also be emphasized that the proposed PBSIQA system can be extended systematically. That is, if new distortion types are introduced, we can design scorers to tailor to them and fuse new scorers into the system. In contrast, the traditional formula-based quality metric does not have such flexibility."}, {"heading": "C. Performance Comparison with Other Quality Indices", "text": "In this section, we compare the performance of the proposed PBSIQA metric with other QA metrics against Datasets A, B, and C of MCL-3D, IVC-A and LIVE-A. The latter two are more challenging than their sources, IVC [32] and LIVE [33], by including asymmetric distortions. Futhermore, we conduct a cross-database learning procedure to demonstrate the robustness of the proposed PBSIQA metric. That is, the PBSIQA trained with the MCL-3D database is used to predict the quality of stereo image pairs in IVC-A and LIVE-A. For performance benchmarking, we consider both 3D and 2D quality indices in the literature. The 3D indices include\n10\nthose denoted by Benoit [12], Campisi [11], RKS [20] and BQPNR [23]. The 2D indices include: the Signal to Noise Ratio (SNR), the Peak Signal to Noise Ratio (PSNR), the Mean Square Error (MSE), the Noise Quality Measure [49] (NQM), the Universal Quality Index [40] (UQI), the Structural Similarity Index [36] (SSIM), the pixel-based VIF [50] (VIFP), the visual signal-to-noise ratio [51] (VSNR), the Peak Signal to Noise Ratio taking into account CSF [52] (PSNR-HVS), C4 [53], and the image fidelity criterion [54] (IFC). Since 2D metrics are only applied to a single image, we use the average score of left and right rendered views to yield the overall quality metric. We first focus on performance comparison against the MCL-3D database. The results are summarized in Table VI, where the best and second best metrics in each column of are shown in bold. We have the following observations.\n\u2022 Dataset A of MCL-3D (texture distortion only) The PBSIQA system has the best performance in terms of PCC, SROCC and RMSE. It outperforms all other 2D and 3D indices by a significant margin. Among the 2D indices, those designed by considering HVS offer better performance such as C4, NQM, and PSNR HVS.\nbenchmarking indices by a significant margin.\nThe scatter plots of predicted scores vs. MOS along with their fitting curves approximated by function f(x) = axb + c with 95% confidence level for 18 objective quality metrics are shown in Fig. 6. These data are calculated based on Dataset C of MCL-3D. Each point on the plot represents one stereoscopic image pair. The horizontal axis gives the objective quality score while the vertical axis indicates the MOS value. The scatter plots confirm that the PBSIQA metric exhibits the best correlation with human perception. On the other hand, many of existing algorithms such as IFC, SSIM, UQI lose the monotonic property with respect to MOS as demonstrated in SROCC values of Table VI. Although the fitting curves of NQM and PSNR HVS show trends similar to that PBSIQA, their variances are much larger.\nNext, we show the performance comparison over IVC-A and LIVE-A databases in Table VII. We only fuse scorers #1\u223c#7 for texture distortions in the PBSIQA system since these two databases consist of the stereoscopic data format (i.e., the left and the right views without the depth map). As shown in Table VII, PBSIQA still outperforms existing metrics significantly. For example, the PCC values of PBSIQA for IVC-A and LIVE-A are 0.952 and 0.924, respectively while those of the second best metric (RSK) are 0.884 and 0.841. We see that the PCC values of PBSIQA for these two databases are higher than that for MCL-3D. This is due to the fact that\n11\nthe distortion types in IVC-A and LIVE-A are simpler than that of MCL-3D so that each scorer can generate more reliable scores for them.\nPractically, it is not convenient to train learning-based quality assessment indices to tailor to a new database every time. It is desirable to develop a quality metric that offers consistent performance when it is trained by one database yet tested by a different database. This is called cross-database validation. To conduct this task, we train the PBSIQA system using the MCL-3D database and then use it to predict the quality score of stereoscopic images in IVC-A and LIVE-A. The results are shown in Table VIII. Although its performance degrades slightly, the PBSIQA system still offers good results. As the SIQA database size becomes larger, its prediction accuracy goes higher. Finally, it is worthwhile to point out that, since the PBSIQA system consists of multiple individual scorers, its complexity is roughly equal to the total complexity of all contributing scorers. Basically, it trades higher complexity for\n12\nbetter performance."}, {"heading": "VII. CONCLUSION AND FUTURE WORK", "text": "In this work, we proposed a ParaBoost method to design a new stereoscopic image quality assessment system called PBSIQA, which includes a set of parallel quality scorers designed to address various distortions in Stage I and their scores are fused to yield the ultimate quality score in Stage II. The excellent performance of the PBSIQA system was collaborated by extensive experimental results. Our research has an impact to the 3D content delivery field since a reliable 3D quality assessment metric serves as a valuable objective function in optimizing the overall system performance.\nAs future extensions, it is interesting to incorporate the masking effect. For example, distortions in salient regions such as foreground objects have more negative impacts on perceptual quality. Also, textured regions are usually more robust to distortions, whereas distortions in homogeneous regions are more noticeable. We may further improve performance by considering content characteristics.\nAnother interesting extension is to consider the quality assessment of 3D video. On one hand, we believe that the ParaBoost methodology can be extended to the case of 3D video in principle. On the other hand, we do need sufficiently large 3D video quality assessment databases for the learningbased methodology to apply. The lack of publicly accessible 3D video quality assessment databases is the main barrier to this research and it is desired to develop such databases for the research community."}, {"heading": "ACKNOWLEDGMENT", "text": "Computation for the work described in this paper was supported by the University of Southern California\u2019s Center for High-Performance Computing (hpc.usc.edu)."}], "references": [{"title": "Text of ISO/IEC FDIS 23000-11 for Stereoscopic Video AF", "author": ["ISO/IEC JTC1/SC29/WG11"], "venue": "N10283, Busan, Korea, Oct. 2008.", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2008}, {"title": "Overview of the Stereo and Multiview Video Coding Extensions of the H.264/ MPEG-4 AVC Standard", "author": ["A. Vetro", "T. Wiegand", "G.J. Sullivan"], "venue": "Proc. of the IEEE, vol. 99, no. 4, pp. 626-642, Apr. 2011.", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2011}, {"title": "Wiegand,\u201c3-D Video Representation Using DepthMaps,", "author": ["K. Muller", "P. Merkle"], "venue": "Proc. of the IEEE,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2011}, {"title": "Quality assessment of stereoscopic 3D image compression by binocular integration behaviors", "author": ["L. Yu-Hsun", "JL. Wu"], "venue": "IEEE Transactions on Image Processing, vol. 23, pp. 1527-1542, Apr. 2014.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2014}, {"title": "Human perception of mismatched stereoscopic 3D inputs", "author": ["L.B. Stelmach", "W.J. Tam", "D.V. Meegan", "A. Vincent", "P. Corriveau"], "venue": "in Proc. IEEE ICIP,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2000}, {"title": "Unequal weighting of monocular inputs in binocular combination: Implications for the compression of stereoscopic imagery", "author": ["D.V. Meegan", "L.B. Stelmach", "W.J. Tam"], "venue": "Journal of Experimental Psychology: Applied,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2001}, {"title": "Perceived quality of compressed stereoscopic images: Effects of symmetric and asymmetric JPEG coding and camera separation", "author": ["P. Seuntiens", "L. Meesters", "W. Ijsselsteijn"], "venue": "ACM Transactions on Applied Perception (TAP), vol. 3, pp. 95-109, Apr. 2006.", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2006}, {"title": "Depth map creation and image based rendering for advanced 3DTV services providing interoperability and scalability", "author": ["P. Kauff", "N. Atzpadin", "C. Fehn", "M. Muller", "O. Schreer", "A. Smolic", "R. Tanger"], "venue": "Signal Process., Image Commun., vol. 22, no. 2, pp. 217-234, Feb. 2007.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2007}, {"title": "Digital video quality: vision models and metrics", "author": ["S. Winkler"], "venue": null, "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2005}, {"title": "Image quality assessment using multi-method fusion", "author": ["T.-J. Liu", "W. Lin", "C.-C.J. Kuo"], "venue": "IEEE Transactions on Image Processing, vol. 22, no. 5, pp. 1793-1807, May. 2013.", "citeRegEx": "10", "shortCiteRegEx": null, "year": 1807}, {"title": "Stereoscopic image quality assessment", "author": ["P. Campisi", "P. Le Callet", "E. Marini"], "venue": "Proc. 15th European Signal Processing Conference,, Sep. 2007", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2007}, {"title": "Using disparity for quality assessment of stereoscopic images", "author": ["A. Benoit", "P. Le Callet", "P. Campisi", "R. Cousseau"], "venue": "Proc. IEEE ICIP, pp. 389-392, Oct. 2008.", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2008}, {"title": "Quality assessment of a stereo pair formed from decoded and synthesized views using objective metrics", "author": ["P.Hanhart", "T. Ebrahimi"], "venue": "Proc. 3DTV-CON, pp. 1-4, Oct. 2012.", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2012}, {"title": "Quality assessment of a stereo pair formed from two synthesized views using objective metrics", "author": ["P. Hanhart", "T. Ebrahimi"], "venue": "7th VPQM, Jan. 2013.", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2013}, {"title": "Objective No- Reference Stereoscopic Image Quality Prediction Based on 2D Image Features and Relative Disparity", "author": ["Z.M.P. Sazzad", "R. Akhter", "J. Baltes", "Y. Horita"], "venue": "Advances in Multimedia, vol. 2012, Jan. 2012.", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2012}, {"title": "Stereoscopic Image Quality Metrics and Compression", "author": ["P. Gorley", "N. Holliman"], "venue": "International Society for Optics and Photonics Electronic Imaging, pp. 680305-680305, 2008.", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2008}, {"title": "CYCLOP: A stereo color image quality assessment metric", "author": ["A. Maalouf", "M.C. Larabi"], "venue": "IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), pp. 1161-1164, May, 2011.", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2011}, {"title": "Reduced-reference quality assessment for 3D video compression and transmission", "author": ["C.T. Hewage", "M.G. Martini"], "venue": "IEEE Transactions on Consumer Electronics, vol. 57, no. 3, pp. 1185-1193, 2011.", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2011}, {"title": "Multimodal interactive continuous scoring of subjective 3D video quality of experience", "author": ["T. Kim", "J. Kang", "S. Lee", "A. Bovik"], "venue": "IEEE Transactions on Multimedia, vol. 16, no. 2, pp. 387-402, 2014.", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2014}, {"title": "Stereoscopic image quality metric based on binocular perception model", "author": ["S. Ryu", "D. Kim", "K. Sohn"], "venue": "Proc. IEEE ICIP, vol. 1, pp. 609-612, Sep. 2012.", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2012}, {"title": "3D image quality index using SDP-based binocular perception model", "author": ["H. Ko", "C. Kim", "S. Choi", "C.-C.J. Kuo"], "venue": "IEEE 11th IVMSP Workshop, pp. 1-4, Jun. 2013.", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2013}, {"title": "Perceptual full-reference quality assessment of stereoscopic images by considering binocular visual characteristics", "author": ["F. Shao", "L. Weisi", "G. Shanbo", "J. Gangyi", "S. Thambipillai"], "venue": "IEEE Transactions on Image Processing, vol. 22, no. 5, pp. 1940-1953, May, 2013.", "citeRegEx": "22", "shortCiteRegEx": null, "year": 1940}, {"title": "No-Reference Quality Assessment for Stereoscopic Images Based on Binocular Quality Perception", "author": ["S. Ryu", "K. Sohn"], "venue": "IEEE Transactions on Circuits and Systems for Video Technology, vol. 24, no. 4, pp. 591-602, Apr, 2014.", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2014}, {"title": "Toward Assessing and Improving the Quality of Stereo Images", "author": ["M. Park", "L. Jiebo", "C.G. Andrew"], "venue": "IEEE Journal of Selected Topics in Signal Processing,, vol. 6, no. 5, pp. 460-470, 2012.", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2012}, {"title": "Objective Quality Assessment of Stereo Images Based on ICA and BT-SVM", "author": ["J. Cheng", "L. Sumei"], "venue": "IEEE 7th International Conference on Computer Science & Education (ICCSE), pp. 154-159, 2012.", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2012}, {"title": "Recommendation ITU-T P.910, Subjective video quality assessment methods for multimedia applications", "author": ["ITU"], "venue": "tech. rep., ITU-T, Geneva, 1996.", "citeRegEx": "26", "shortCiteRegEx": null, "year": 1996}, {"title": "Recommendation ITU-R BT.2021: Subjective methods for the assessment of stereoscopic 3DTV systems", "author": ["ITU-R"], "venue": "tehc.rep., 2012.", "citeRegEx": "28", "shortCiteRegEx": null, "year": 2012}, {"title": "BT.500-11: Methodology for the subjective assessment of the quality of television pictures", "author": ["ITU-R"], "venue": "tech. rep., 2002.", "citeRegEx": "29", "shortCiteRegEx": null, "year": 2002}, {"title": "MCL-3D: a database for stereoscopic image quality assessment using 2D-image-plus-depth source", "author": ["R. Song", "H. Ko", "C.-C.J. Kuo"], "venue": "accepted by Journal of Information Science and Engineering.", "citeRegEx": "30", "shortCiteRegEx": null, "year": 0}, {"title": "IRC- CyN/IVC 3D images database", "author": ["A. Benoit", "P.L. Callet", "P. Campisi", "R. Cousseau"], "venue": "ACM Transactions on Applied Perception (TAP), IVC-Database [Online]. Available: \u201chttp://www.irccyn.ecnantes.fr/spip.php?article876\u201d, 2008", "citeRegEx": "32", "shortCiteRegEx": null, "year": 2008}, {"title": "Subjective evaluation of stereoscopic image quality", "author": ["A.K. Moorthy", "C.C. Su", "A. Mittal", "A.C. Bovik"], "venue": "Signal Processing: Image Communication, LIVE-Database [Online]. Available: \u201clive.ece.utexas.edu/research/quality/live 3dimage phase1.html\u201d, 2012.", "citeRegEx": "33", "shortCiteRegEx": null, "year": 2012}, {"title": "Improving robustness of image quality measurement with degradation classification and machine learning", "author": ["T.H. Falk", "Y. Guo", "W. Y Chan"], "venue": "Proc. Conference Record of the Forty-First Asilomar Conference on Signals, Systems and Computers (ACSSC 2007), pp. 503-507, 2007.", "citeRegEx": "34", "shortCiteRegEx": null, "year": 2007}, {"title": "Image quality measures and their performance", "author": ["A.M. Eskicioglu", "P.S. Fisher"], "venue": "IEEE Transaction on Communications, vol. 43, pp. 29592965, Dec. 1995", "citeRegEx": "35", "shortCiteRegEx": null, "year": 1995}, {"title": "Image quality assessment: From error visibility to structural similarity", "author": ["Z. Wang", "A.C. Bovik", "H.R. Sheikh", "E.P. Simoncelli"], "venue": "IEEE Transactions on Image Processing, vol. 13, pp. 600-612, Apr. 2004.  13", "citeRegEx": "36", "shortCiteRegEx": null, "year": 2004}, {"title": "Statistical evaluation of image quality measures", "author": ["I. Avcibas", "B. Sankur", "K. Sayood"], "venue": "Journal of Electronic imaging, vol. 11, pp. 206-223, 2002.", "citeRegEx": "37", "shortCiteRegEx": null, "year": 2002}, {"title": "FSIM: a feature similarity index for image quality assessment,", "author": ["L. Zhang", "X. Mou", "D. Zhang"], "venue": "IEEE Transactions on Image Processing,", "citeRegEx": "38", "shortCiteRegEx": "38", "year": 2011}, {"title": "SVD-based quality metric for image and video using machine learning", "author": ["M. Narwaria", "W. Lin"], "venue": "IEEE Transactions on Systems, Man, and Cybernetics, Part B: Cybernetics, vol. 42, pp. 347-364, Apr. 2012.", "citeRegEx": "39", "shortCiteRegEx": null, "year": 2012}, {"title": "A Universal Image Quality Index", "author": ["Z. Wang", "A. Bovik"], "venue": "IEEE Signal Processing Letter, vol. 9, no 3, Mar. 2002.", "citeRegEx": "40", "shortCiteRegEx": null, "year": 2002}, {"title": "Depth map coding with distortion estimation of rendered view", "author": ["W.S. Kim", "A. Ortega", "P.L. Lai", "D. Tian", "C. Gomila"], "venue": "SPIE Vis. Inf. Process. Commun., 2010.", "citeRegEx": "41", "shortCiteRegEx": null, "year": 2010}, {"title": "Depth map coding based on synthesized view distortion function", "author": ["B.T. Oh", "J. Lee", "D.-S. Park"], "venue": "IEEE J. Sel. Topics Signal Process., vol. 5, no. 7, pp. 13441352, Nov. 2011.", "citeRegEx": "42", "shortCiteRegEx": null, "year": 2011}, {"title": "Depth no-synthesis-error model for view synthesis in 3-D video", "author": ["Y. Zhao", "C. Zhu", "Z. Chen", "L. Yu"], "venue": "IEEE Trans. Image Process., vol. 20, no. 8, pp. 22212228, Aug. 2011.", "citeRegEx": "43", "shortCiteRegEx": null, "year": 2011}, {"title": "Learning with kernels: support vector machines, regularization, optimization and beyond,", "author": ["B. Sch\u00f6lkopf", "A.J. Smola"], "venue": null, "citeRegEx": "44", "shortCiteRegEx": "44", "year": 2002}, {"title": "Patranabis \u201cSupport vector regression,", "author": ["D. Basak", "S. Pal", "D. C"], "venue": "Neural Information Processing-Letters and Reviews,", "citeRegEx": "45", "shortCiteRegEx": "45", "year": 2007}, {"title": "A practical guide to support vector classification,", "author": ["C.-W. Hsu", "C.-C. Chang", "C.-J Lin"], "venue": "Dept. Comput.Sci.,", "citeRegEx": "46", "shortCiteRegEx": "46", "year": 2010}, {"title": "Pattern recognition and machine learning", "author": ["C.M. Bishop"], "venue": "Vol. 1, New York: springer, 2006.", "citeRegEx": "47", "shortCiteRegEx": null, "year": 2006}, {"title": "P.1401: Statistical analysis, evaluation and reporting guidelines of quality measurements", "author": ["ITU-T"], "venue": "tech. rep., 2012.", "citeRegEx": "48", "shortCiteRegEx": null, "year": 2012}, {"title": "Image quality assessment based on a degradation model", "author": ["N. Damera-Venkata", "T.D. Kite", "W.S. Geisler", "B.L. Evans", "A.C. Bovik"], "venue": "IEEE Transactions on Image Processing, vol. 4, no. 4, pp. 636-650, Apr. 2000.", "citeRegEx": "49", "shortCiteRegEx": null, "year": 2000}, {"title": "Image information and visual quality", "author": ["H.R. Sheikh", "A.C. Bovik"], "venue": "IEEE Transactions on Image Processing, vol. 15, no. 2, pp. 430-444, Feb. 2006.", "citeRegEx": "50", "shortCiteRegEx": null, "year": 2006}, {"title": "VSNR: A wavelet-based visual signal-to-noise ratio for natural images", "author": ["D.M. Chandler", "S.S. Hemami"], "venue": "IEEE Transactions on Image Processing, vol. 16, no. 9, pp. 2284-2298, Sep. 2007.", "citeRegEx": "51", "shortCiteRegEx": null, "year": 2007}, {"title": "On between-coefficient contrast masking of dct basis functions,\u201din", "author": ["N. Ponomarenko", "F. Silvestri", "K. Egiazarian", "M. Carli", "J. Astola", "V. Lukin"], "venue": "Proc. 3rd Int. Workshop Video Process. Quality Metrics Consum. Electron.,", "citeRegEx": "52", "shortCiteRegEx": "52", "year": 2007}, {"title": "An image quality assessment method based on perception of structural information", "author": ["M. Carnec", "P. Le Callet", "D. Barba"], "venue": "Proc. IEEE ICIP, vol. 3, pp. III-185, Sep. 2003.", "citeRegEx": "53", "shortCiteRegEx": null, "year": 2003}], "referenceMentions": [{"referenceID": 0, "context": "Standards for coding, transmitting and storing 3D visual data have been proposed such as stereoscopic 3D video [1], multiview video coding (MVC) [2], and multiview video plus depth map (MVD) format [3].", "startOffset": 111, "endOffset": 114}, {"referenceID": 1, "context": "Standards for coding, transmitting and storing 3D visual data have been proposed such as stereoscopic 3D video [1], multiview video coding (MVC) [2], and multiview video plus depth map (MVD) format [3].", "startOffset": 145, "endOffset": 148}, {"referenceID": 2, "context": "Standards for coding, transmitting and storing 3D visual data have been proposed such as stereoscopic 3D video [1], multiview video coding (MVC) [2], and multiview video plus depth map (MVD) format [3].", "startOffset": 198, "endOffset": 201}, {"referenceID": 3, "context": "The PSNR measure (or any other 2D quality metric) does not correlate well with human visual experience of 3D visual stimuli [4].", "startOffset": 124, "endOffset": 127}, {"referenceID": 4, "context": "Furthermore, the human visual system (HVS) reacts differently to asymmetrical distortions caused by different quality levels of left and right views, depending on distortion types [5], [6], [7].", "startOffset": 180, "endOffset": 183}, {"referenceID": 5, "context": "Furthermore, the human visual system (HVS) reacts differently to asymmetrical distortions caused by different quality levels of left and right views, depending on distortion types [5], [6], [7].", "startOffset": 185, "endOffset": 188}, {"referenceID": 6, "context": "Furthermore, the human visual system (HVS) reacts differently to asymmetrical distortions caused by different quality levels of left and right views, depending on distortion types [5], [6], [7].", "startOffset": 190, "endOffset": 193}, {"referenceID": 7, "context": "That is, both the texture and the depth data are compressed at the encoder and transmitted to the decoder and, then, virtual views are synthesized using the depth-image-based-rendering (DIBR) technique [8].", "startOffset": 202, "endOffset": 205}, {"referenceID": 8, "context": "Traditionally, objective image quality metrics were classified into pixel-based metrics and the human visual system (HVS) inspired metrics [9].", "startOffset": 139, "endOffset": 142}, {"referenceID": 8, "context": "Both pixel-based metrics and the human visual system (HVS) inspired metrics in [9] belong to this class.", "startOffset": 79, "endOffset": 82}, {"referenceID": 9, "context": ", MMF in [10] is obtained using the \u201clearning-based prediction approach\u201d.", "startOffset": 9, "endOffset": 13}, {"referenceID": 10, "context": "By following the first approach, 2D metrics were extended to 3D metrics by combining distortions of a depth map and images of both views linearly in [11], [12].", "startOffset": 149, "endOffset": 153}, {"referenceID": 11, "context": "By following the first approach, 2D metrics were extended to 3D metrics by combining distortions of a depth map and images of both views linearly in [11], [12].", "startOffset": 155, "endOffset": 159}, {"referenceID": 12, "context": "[13], [14].", "startOffset": 0, "endOffset": 4}, {"referenceID": 13, "context": "[13], [14].", "startOffset": 6, "endOffset": 10}, {"referenceID": 14, "context": "[15] proposed a SIQA metric for JPEG compressed images using segmented local features.", "startOffset": 0, "endOffset": 4}, {"referenceID": 15, "context": "Gorley and Holliman [16] proposed a metric based on the sensitivity of HVS to contrast and luminance changes.", "startOffset": 20, "endOffset": 24}, {"referenceID": 16, "context": "Maalou and Larab [17] proposed a metric by exploiting the color disparity tensor and the contrast sensitivity function.", "startOffset": 17, "endOffset": 21}, {"referenceID": 17, "context": "Hewage and Martini [18] presented a reduced-reference quality metric based on edge detection in the depth map and demonstrated a good approximation for the full-reference quality metric.", "startOffset": 19, "endOffset": 23}, {"referenceID": 18, "context": "[19] proposed a methodology for subjective 3D QoE assessment experiments using external stimuli such as vibration, flickering and sound.", "startOffset": 0, "endOffset": 4}, {"referenceID": 4, "context": "Stelmach and Meegan [5], [6] reported that binocular perception is dominated by the higher quality image in face of low-pass filtering operations yet by the average of both images for quantized distortions.", "startOffset": 20, "endOffset": 23}, {"referenceID": 5, "context": "Stelmach and Meegan [5], [6] reported that binocular perception is dominated by the higher quality image in face of low-pass filtering operations yet by the average of both images for quantized distortions.", "startOffset": 25, "endOffset": 28}, {"referenceID": 6, "context": "[7] observed that the perceived quality of a JPEG-coded stereo image pair is close to the average quality of two individual views.", "startOffset": 0, "endOffset": 3}, {"referenceID": 19, "context": "[20] proposed an extended version of the SSIM index based on a binocular model.", "startOffset": 0, "endOffset": 4}, {"referenceID": 20, "context": "[21] introduced the notion of structural distortion parameter (SDP), which varies according to distortion types, and employed the SDP as a control parameter in a binocular perception model to provide robust QA results for both symmetric and asymmetric distortions.", "startOffset": 0, "endOffset": 4}, {"referenceID": 21, "context": "[22] proposed an SIQA method by considering the binocular combination property and the binocular just noticeable difference model.", "startOffset": 0, "endOffset": 4}, {"referenceID": 22, "context": "[23] proposed a no-reference quality metric by considering perceptual blurriness and blockiness scores and taking visual saliency into account.", "startOffset": 0, "endOffset": 4}, {"referenceID": 9, "context": "[10] offers a good example for 2D learning-based image QA, among many others.", "startOffset": 0, "endOffset": 4}, {"referenceID": 23, "context": "[24] used a set of geometric stereo features for anaglyph images and built a regression model to capture the relationship between these features and the quality of stereo images.", "startOffset": 0, "endOffset": 4}, {"referenceID": 24, "context": "Cheng and Sumei [25] extracted a set of basis images using independent component analysis and used a binary-tree support vector machine (SVM) to predict scores of distorted stereo images.", "startOffset": 16, "endOffset": 20}, {"referenceID": 25, "context": "To capture the content variety, the Spatial Information (SI) defined by ITU-T recommendation [26] for each texture image and its depth map was calculated.", "startOffset": 93, "endOffset": 97}, {"referenceID": 26, "context": "Based on the recommendations of ITU [28], we consider five quality levels in subjective tests.", "startOffset": 36, "endOffset": 40}, {"referenceID": 26, "context": "The test was performed in a controlled environment as recommended by ITU [28], including the display equipment, viewing distance, ambient light, etc.", "startOffset": 73, "endOffset": 77}, {"referenceID": 27, "context": "500 [29], and outliers were removed.", "startOffset": 4, "endOffset": 8}, {"referenceID": 28, "context": "For further details, we refer to [30].", "startOffset": 33, "endOffset": 37}, {"referenceID": 20, "context": "In our earlier work [21], we also constructed two new databases by expanding the IVC [32] and the LIVE [33] databases and call them IVC-A and LIVE-A, respectively.", "startOffset": 20, "endOffset": 24}, {"referenceID": 29, "context": "In our earlier work [21], we also constructed two new databases by expanding the IVC [32] and the LIVE [33] databases and call them IVC-A and LIVE-A, respectively.", "startOffset": 85, "endOffset": 89}, {"referenceID": 30, "context": "In our earlier work [21], we also constructed two new databases by expanding the IVC [32] and the LIVE [33] databases and call them IVC-A and LIVE-A, respectively.", "startOffset": 103, "endOffset": 107}, {"referenceID": 32, "context": "Pixel difference MD (Maximum Difference) [35]", "startOffset": 41, "endOffset": 45}, {"referenceID": 33, "context": "Structural Similarity SSIM index [36]", "startOffset": 33, "endOffset": 37}, {"referenceID": 34, "context": "MAE (Mean Absolute Error) [37] SSIM Luminance [36]", "startOffset": 26, "endOffset": 30}, {"referenceID": 33, "context": "MAE (Mean Absolute Error) [37] SSIM Luminance [36]", "startOffset": 46, "endOffset": 50}, {"referenceID": 33, "context": "PSNR SSIM Contrast [36] ABV (Average Block Variance) SSIM Similarity [36]", "startOffset": 19, "endOffset": 23}, {"referenceID": 33, "context": "PSNR SSIM Contrast [36] ABV (Average Block Variance) SSIM Similarity [36]", "startOffset": 69, "endOffset": 73}, {"referenceID": 34, "context": "MIN (Modified Infinity Norm) [37] UQI (Universal Quality Index) [40] Blockiness -", "startOffset": 29, "endOffset": 33}, {"referenceID": 37, "context": "MIN (Modified Infinity Norm) [37] UQI (Universal Quality Index) [40] Blockiness -", "startOffset": 64, "endOffset": 68}, {"referenceID": 36, "context": "SVD related Singular Value [39] AADBIIS (Average Absolute Difference Between In-Block Samples) [10] Singular Vector [39]", "startOffset": 27, "endOffset": 31}, {"referenceID": 9, "context": "SVD related Singular Value [39] AADBIIS (Average Absolute Difference Between In-Block Samples) [10] Singular Vector [39]", "startOffset": 95, "endOffset": 99}, {"referenceID": 36, "context": "SVD related Singular Value [39] AADBIIS (Average Absolute Difference Between In-Block Samples) [10] Singular Vector [39]", "startOffset": 116, "endOffset": 120}, {"referenceID": 34, "context": "related ES (Edge Stability) [37] Spectral", "startOffset": 28, "endOffset": 32}, {"referenceID": 9, "context": "Difference ZCR (Zero Crossing Rate) [10] AES (Average Edge Stability) [10] PC (Phase Congruency) [38]", "startOffset": 36, "endOffset": 40}, {"referenceID": 9, "context": "Difference ZCR (Zero Crossing Rate) [10] AES (Average Edge Stability) [10] PC (Phase Congruency) [38]", "startOffset": 70, "endOffset": 74}, {"referenceID": 35, "context": "Difference ZCR (Zero Crossing Rate) [10] AES (Average Edge Stability) [10] PC (Phase Congruency) [38]", "startOffset": 97, "endOffset": 101}, {"referenceID": 34, "context": "PRATT [37] Contrast measure GM (Gradient Magnitude) [38]", "startOffset": 6, "endOffset": 10}, {"referenceID": 35, "context": "PRATT [37] Contrast measure GM (Gradient Magnitude) [38]", "startOffset": 52, "endOffset": 56}, {"referenceID": 34, "context": "Correlation MAS (Mean Angle Similarity) [37] HVS HVS-MSE [34]", "startOffset": 40, "endOffset": 44}, {"referenceID": 31, "context": "Correlation MAS (Mean Angle Similarity) [37] HVS HVS-MSE [34]", "startOffset": 57, "endOffset": 61}, {"referenceID": 34, "context": "NCC (Normalized Cross Correlation) [37] View synthesis NDSE (Noticeable Depth Synthesis Error) [43]", "startOffset": 35, "endOffset": 39}, {"referenceID": 40, "context": "NCC (Normalized Cross Correlation) [37] View synthesis NDSE (Noticeable Depth Synthesis Error) [43]", "startOffset": 95, "endOffset": 99}, {"referenceID": 0, "context": "distortion levels ([1, 1], [2, 2], [3, 3]) and three asymmetric distortion levels ([1, 2], [1, 3], [2, 3]).", "startOffset": 19, "endOffset": 25}, {"referenceID": 0, "context": "distortion levels ([1, 1], [2, 2], [3, 3]) and three asymmetric distortion levels ([1, 2], [1, 3], [2, 3]).", "startOffset": 19, "endOffset": 25}, {"referenceID": 1, "context": "distortion levels ([1, 1], [2, 2], [3, 3]) and three asymmetric distortion levels ([1, 2], [1, 3], [2, 3]).", "startOffset": 27, "endOffset": 33}, {"referenceID": 1, "context": "distortion levels ([1, 1], [2, 2], [3, 3]) and three asymmetric distortion levels ([1, 2], [1, 3], [2, 3]).", "startOffset": 27, "endOffset": 33}, {"referenceID": 2, "context": "distortion levels ([1, 1], [2, 2], [3, 3]) and three asymmetric distortion levels ([1, 2], [1, 3], [2, 3]).", "startOffset": 35, "endOffset": 41}, {"referenceID": 2, "context": "distortion levels ([1, 1], [2, 2], [3, 3]) and three asymmetric distortion levels ([1, 2], [1, 3], [2, 3]).", "startOffset": 35, "endOffset": 41}, {"referenceID": 0, "context": "distortion levels ([1, 1], [2, 2], [3, 3]) and three asymmetric distortion levels ([1, 2], [1, 3], [2, 3]).", "startOffset": 83, "endOffset": 89}, {"referenceID": 1, "context": "distortion levels ([1, 1], [2, 2], [3, 3]) and three asymmetric distortion levels ([1, 2], [1, 3], [2, 3]).", "startOffset": 83, "endOffset": 89}, {"referenceID": 0, "context": "distortion levels ([1, 1], [2, 2], [3, 3]) and three asymmetric distortion levels ([1, 2], [1, 3], [2, 3]).", "startOffset": 91, "endOffset": 97}, {"referenceID": 2, "context": "distortion levels ([1, 1], [2, 2], [3, 3]) and three asymmetric distortion levels ([1, 2], [1, 3], [2, 3]).", "startOffset": 91, "endOffset": 97}, {"referenceID": 1, "context": "distortion levels ([1, 1], [2, 2], [3, 3]) and three asymmetric distortion levels ([1, 2], [1, 3], [2, 3]).", "startOffset": 99, "endOffset": 105}, {"referenceID": 2, "context": "distortion levels ([1, 1], [2, 2], [3, 3]) and three asymmetric distortion levels ([1, 2], [1, 3], [2, 3]).", "startOffset": 99, "endOffset": 105}, {"referenceID": 26, "context": "We conducted a subjective test to obtain MOS using the absolute category rating (ACR) [28].", "startOffset": 86, "endOffset": 90}, {"referenceID": 29, "context": "from IVC [32] database Six stereoscopic image pairs", "startOffset": 9, "endOffset": 13}, {"referenceID": 30, "context": "from LIVE [33] database # of test images 648 144", "startOffset": 10, "endOffset": 14}, {"referenceID": 0, "context": "-[1,1], [1,2], [1,3], [2,2], [2,3] and [3,3]", "startOffset": 1, "endOffset": 6}, {"referenceID": 0, "context": "-[1,1], [1,2], [1,3], [2,2], [2,3] and [3,3]", "startOffset": 1, "endOffset": 6}, {"referenceID": 0, "context": "-[1,1], [1,2], [1,3], [2,2], [2,3] and [3,3]", "startOffset": 8, "endOffset": 13}, {"referenceID": 1, "context": "-[1,1], [1,2], [1,3], [2,2], [2,3] and [3,3]", "startOffset": 8, "endOffset": 13}, {"referenceID": 0, "context": "-[1,1], [1,2], [1,3], [2,2], [2,3] and [3,3]", "startOffset": 15, "endOffset": 20}, {"referenceID": 2, "context": "-[1,1], [1,2], [1,3], [2,2], [2,3] and [3,3]", "startOffset": 15, "endOffset": 20}, {"referenceID": 1, "context": "-[1,1], [1,2], [1,3], [2,2], [2,3] and [3,3]", "startOffset": 22, "endOffset": 27}, {"referenceID": 1, "context": "-[1,1], [1,2], [1,3], [2,2], [2,3] and [3,3]", "startOffset": 22, "endOffset": 27}, {"referenceID": 1, "context": "-[1,1], [1,2], [1,3], [2,2], [2,3] and [3,3]", "startOffset": 29, "endOffset": 34}, {"referenceID": 2, "context": "-[1,1], [1,2], [1,3], [2,2], [2,3] and [3,3]", "startOffset": 29, "endOffset": 34}, {"referenceID": 2, "context": "-[1,1], [1,2], [1,3], [2,2], [2,3] and [3,3]", "startOffset": 39, "endOffset": 44}, {"referenceID": 2, "context": "-[1,1], [1,2], [1,3], [2,2], [2,3] and [3,3]", "startOffset": 39, "endOffset": 44}, {"referenceID": 0, "context": "& [1]: the strongest, [2]: moderate, and [3]: the weakest distortion.", "startOffset": 2, "endOffset": 5}, {"referenceID": 1, "context": "& [1]: the strongest, [2]: moderate, and [3]: the weakest distortion.", "startOffset": 22, "endOffset": 25}, {"referenceID": 2, "context": "& [1]: the strongest, [2]: moderate, and [3]: the weakest distortion.", "startOffset": 41, "endOffset": 44}, {"referenceID": 20, "context": "Furthermore, it is reported in [21] that the interaction between left and right views in perceived 3D image quality depends on the distortion type.", "startOffset": 31, "endOffset": 35}, {"referenceID": 31, "context": "Based on previous studies on image quality assessment [34], [35], [37], [38], [39], [40] and our own experience, we select 24 candidate features for further examination as listed in Table II.", "startOffset": 54, "endOffset": 58}, {"referenceID": 32, "context": "Based on previous studies on image quality assessment [34], [35], [37], [38], [39], [40] and our own experience, we select 24 candidate features for further examination as listed in Table II.", "startOffset": 60, "endOffset": 64}, {"referenceID": 34, "context": "Based on previous studies on image quality assessment [34], [35], [37], [38], [39], [40] and our own experience, we select 24 candidate features for further examination as listed in Table II.", "startOffset": 66, "endOffset": 70}, {"referenceID": 35, "context": "Based on previous studies on image quality assessment [34], [35], [37], [38], [39], [40] and our own experience, we select 24 candidate features for further examination as listed in Table II.", "startOffset": 72, "endOffset": 76}, {"referenceID": 36, "context": "Based on previous studies on image quality assessment [34], [35], [37], [38], [39], [40] and our own experience, we select 24 candidate features for further examination as listed in Table II.", "startOffset": 78, "endOffset": 82}, {"referenceID": 37, "context": "Based on previous studies on image quality assessment [34], [35], [37], [38], [39], [40] and our own experience, we select 24 candidate features for further examination as listed in Table II.", "startOffset": 84, "endOffset": 88}, {"referenceID": 19, "context": "(ILD) [20].", "startOffset": 6, "endOffset": 10}, {"referenceID": 34, "context": "The first one is the edge stability mean squre error (ESMSE) [37], which characterizes the consistency of edges that are evident across multiple scales between the original and distorted images.", "startOffset": 61, "endOffset": 65}, {"referenceID": 34, "context": "The other one is Pratt\u2019s measure [37] that considers the accuracy of detected edge locations, missing edges and false alarm edges.", "startOffset": 33, "endOffset": 37}, {"referenceID": 6, "context": "When stereoscopic images with blocking distortion are shown, the average quality of both views or that of the lower quality is perceived [7], [21].", "startOffset": 137, "endOffset": 140}, {"referenceID": 20, "context": "When stereoscopic images with blocking distortion are shown, the average quality of both views or that of the lower quality is perceived [7], [21].", "startOffset": 142, "endOffset": 146}, {"referenceID": 19, "context": "Such artifacts are called the information additive distortion (IAD) [20].", "startOffset": 68, "endOffset": 72}, {"referenceID": 9, "context": "As stated in [10], [34], there are two good features in detecting blockiness.", "startOffset": 13, "endOffset": 17}, {"referenceID": 31, "context": "As stated in [10], [34], there are two good features in detecting blockiness.", "startOffset": 19, "endOffset": 23}, {"referenceID": 31, "context": "Mathematically, the HVS MSE [34] is given by", "startOffset": 28, "endOffset": 32}, {"referenceID": 32, "context": "We select three such features to characterize additive noise: the peak-signal-to-noise ratio (PSNR), the maximum difference (MD) [35] and the modified infinity norm (MIN) [37].", "startOffset": 129, "endOffset": 133}, {"referenceID": 34, "context": "We select three such features to characterize additive noise: the peak-signal-to-noise ratio (PSNR), the maximum difference (MD) [35] and the modified infinity norm (MIN) [37].", "startOffset": 171, "endOffset": 175}, {"referenceID": 33, "context": "Structural errors can be captured by three components of the SSIM index [36], which are luminance, contrast and structural similarities between images x and y.", "startOffset": 72, "endOffset": 76}, {"referenceID": 35, "context": "Two features were proposed in [38] at this end: phase congruency (PC) and gradient magnitude (GM).", "startOffset": 30, "endOffset": 34}, {"referenceID": 35, "context": "Readers are referred to [38] for its detailed definition and properties.", "startOffset": 24, "endOffset": 28}, {"referenceID": 36, "context": "It was shown in [39] that the first several singular vectors offer a good set to represent the structural information of objects while subsequent vectors account for finer details.", "startOffset": 16, "endOffset": 20}, {"referenceID": 37, "context": "Based on the feature analysis, both the universal quality index (UQI) [40] and the mean angle similarity (MAS) [37] are useful in characterizing transmission error.", "startOffset": 70, "endOffset": 74}, {"referenceID": 34, "context": "Based on the feature analysis, both the universal quality index (UQI) [40] and the mean angle similarity (MAS) [37] are useful in characterizing transmission error.", "startOffset": 111, "endOffset": 115}, {"referenceID": 38, "context": ", [41], [42].", "startOffset": 2, "endOffset": 6}, {"referenceID": 39, "context": ", [41], [42].", "startOffset": 8, "endOffset": 12}, {"referenceID": 40, "context": "[43] proposed a Depth No-Synthesis-Error (D-NOSE) model by exploiting that the depth information is typically stored in 8-bit grayscale format while the disparity range for a visually comfortable stereo pair is often far less than 256 additiveSnoiseSonSdepthSpart transmissionSerrorSonSdepthSpart", "startOffset": 0, "endOffset": 4}, {"referenceID": 40, "context": "For more details on the D-NOSE profile, we refer to [43].", "startOffset": 52, "endOffset": 56}, {"referenceID": 20, "context": "That is, we use the SDP index [21] that is computed based on three depth maps as a candidate scorer.", "startOffset": 30, "endOffset": 34}, {"referenceID": 41, "context": "In the \u03b5-SVR [44], [45], the objective is to find a mapping function f(xn) that has a deviation at most \u03b5 from the target value, yn, for all training data.", "startOffset": 13, "endOffset": 17}, {"referenceID": 42, "context": "In the \u03b5-SVR [44], [45], the objective is to find a mapping function f(xn) that has a deviation at most \u03b5 from the target value, yn, for all training data.", "startOffset": 19, "endOffset": 23}, {"referenceID": 43, "context": "where \u03c1 is the radius controlling parameter, since it provides good performance in applications [46].", "startOffset": 96, "endOffset": 100}, {"referenceID": 42, "context": "Thus, we use a different version of the regression algorithm called the \u03bd-SVR [45], where \u03bd \u2208 (0, 1) is a control parameter to adjust the number of support vectors and the accuracy level.", "startOffset": 78, "endOffset": 82}, {"referenceID": 44, "context": "The n-fold cross validation [47] is a common strategy to evaluate the performance of a learning-based algorithm to ensure reliable results and prevent over-fitting, where the data are split into n chunks, and one chunk is used as the test data while the remaining n \u2212 1 chunks are used as training data.", "startOffset": 28, "endOffset": 32}, {"referenceID": 0, "context": "We scale the feature values of each scorer to the unit range of [0,1] at Stage I.", "startOffset": 64, "endOffset": 69}, {"referenceID": 43, "context": "We conduct parameter search on C and \u03b3 at the training stage using the cross validation scheme in [46].", "startOffset": 98, "endOffset": 102}, {"referenceID": 45, "context": "1401) [48] and use three performance measures: 1) the Pearson correlation coefficient (PCC) to measure the linear relationship between a model\u2019s performance and the subjective data , 2) the Spearman rank-order correlation coefficient (SROCC) for the prediction monotonicity, and 3) the root mean squared error (RMSE) for the prediction accuracy.", "startOffset": 6, "endOffset": 10}, {"referenceID": 20, "context": "As observed in [21], the information additive distortion (IAD) is more obvious than the information loss distortion (ILD) among all structural distortions.", "startOffset": 15, "endOffset": 19}, {"referenceID": 20, "context": "Our observation is consistent with that in [21].", "startOffset": 43, "endOffset": 47}, {"referenceID": 29, "context": "The latter two are more challenging than their sources, IVC [32] and LIVE [33], by including asymmetric distortions.", "startOffset": 60, "endOffset": 64}, {"referenceID": 30, "context": "The latter two are more challenging than their sources, IVC [32] and LIVE [33], by including asymmetric distortions.", "startOffset": 74, "endOffset": 78}, {"referenceID": 11, "context": "those denoted by Benoit [12], Campisi [11], RKS [20] and BQPNR [23].", "startOffset": 24, "endOffset": 28}, {"referenceID": 10, "context": "those denoted by Benoit [12], Campisi [11], RKS [20] and BQPNR [23].", "startOffset": 38, "endOffset": 42}, {"referenceID": 19, "context": "those denoted by Benoit [12], Campisi [11], RKS [20] and BQPNR [23].", "startOffset": 48, "endOffset": 52}, {"referenceID": 22, "context": "those denoted by Benoit [12], Campisi [11], RKS [20] and BQPNR [23].", "startOffset": 63, "endOffset": 67}, {"referenceID": 46, "context": "The 2D indices include: the Signal to Noise Ratio (SNR), the Peak Signal to Noise Ratio (PSNR), the Mean Square Error (MSE), the Noise Quality Measure [49] (NQM), the Universal Quality Index [40] (UQI), the Structural Similarity Index [36] (SSIM), the pixel-based VIF [50] (VIFP), the visual signal-to-noise ratio [51] (VSNR), the Peak Signal to Noise Ratio taking into account CSF [52] (PSNR-HVS), C4 [53], and the image fidelity criterion [54] (IFC).", "startOffset": 151, "endOffset": 155}, {"referenceID": 37, "context": "The 2D indices include: the Signal to Noise Ratio (SNR), the Peak Signal to Noise Ratio (PSNR), the Mean Square Error (MSE), the Noise Quality Measure [49] (NQM), the Universal Quality Index [40] (UQI), the Structural Similarity Index [36] (SSIM), the pixel-based VIF [50] (VIFP), the visual signal-to-noise ratio [51] (VSNR), the Peak Signal to Noise Ratio taking into account CSF [52] (PSNR-HVS), C4 [53], and the image fidelity criterion [54] (IFC).", "startOffset": 191, "endOffset": 195}, {"referenceID": 33, "context": "The 2D indices include: the Signal to Noise Ratio (SNR), the Peak Signal to Noise Ratio (PSNR), the Mean Square Error (MSE), the Noise Quality Measure [49] (NQM), the Universal Quality Index [40] (UQI), the Structural Similarity Index [36] (SSIM), the pixel-based VIF [50] (VIFP), the visual signal-to-noise ratio [51] (VSNR), the Peak Signal to Noise Ratio taking into account CSF [52] (PSNR-HVS), C4 [53], and the image fidelity criterion [54] (IFC).", "startOffset": 235, "endOffset": 239}, {"referenceID": 47, "context": "The 2D indices include: the Signal to Noise Ratio (SNR), the Peak Signal to Noise Ratio (PSNR), the Mean Square Error (MSE), the Noise Quality Measure [49] (NQM), the Universal Quality Index [40] (UQI), the Structural Similarity Index [36] (SSIM), the pixel-based VIF [50] (VIFP), the visual signal-to-noise ratio [51] (VSNR), the Peak Signal to Noise Ratio taking into account CSF [52] (PSNR-HVS), C4 [53], and the image fidelity criterion [54] (IFC).", "startOffset": 268, "endOffset": 272}, {"referenceID": 48, "context": "The 2D indices include: the Signal to Noise Ratio (SNR), the Peak Signal to Noise Ratio (PSNR), the Mean Square Error (MSE), the Noise Quality Measure [49] (NQM), the Universal Quality Index [40] (UQI), the Structural Similarity Index [36] (SSIM), the pixel-based VIF [50] (VIFP), the visual signal-to-noise ratio [51] (VSNR), the Peak Signal to Noise Ratio taking into account CSF [52] (PSNR-HVS), C4 [53], and the image fidelity criterion [54] (IFC).", "startOffset": 314, "endOffset": 318}, {"referenceID": 49, "context": "The 2D indices include: the Signal to Noise Ratio (SNR), the Peak Signal to Noise Ratio (PSNR), the Mean Square Error (MSE), the Noise Quality Measure [49] (NQM), the Universal Quality Index [40] (UQI), the Structural Similarity Index [36] (SSIM), the pixel-based VIF [50] (VIFP), the visual signal-to-noise ratio [51] (VSNR), the Peak Signal to Noise Ratio taking into account CSF [52] (PSNR-HVS), C4 [53], and the image fidelity criterion [54] (IFC).", "startOffset": 382, "endOffset": 386}, {"referenceID": 50, "context": "The 2D indices include: the Signal to Noise Ratio (SNR), the Peak Signal to Noise Ratio (PSNR), the Mean Square Error (MSE), the Noise Quality Measure [49] (NQM), the Universal Quality Index [40] (UQI), the Structural Similarity Index [36] (SSIM), the pixel-based VIF [50] (VIFP), the visual signal-to-noise ratio [51] (VSNR), the Peak Signal to Noise Ratio taking into account CSF [52] (PSNR-HVS), C4 [53], and the image fidelity criterion [54] (IFC).", "startOffset": 402, "endOffset": 406}], "year": 2016, "abstractText": "The problem of stereoscopic image quality assessment, which finds applications in 3D visual content delivery such as 3DTV, is investigated in this work. Specifically, we propose a new ParaBoost (parallel-boosting) stereoscopic image quality assessment (PBSIQA) system. The system consists of two stages. In the first stage, various distortions are classified into a few types, and individual quality scorers targeting at a specific distortion type are developed. These scorers offer complementary performance in face of a database consisting of heterogeneous distortion types. In the second stage, scores from multiple quality scorers are fused to achieve the best overall performance, where the fuser is designed based on the parallel boosting idea borrowed from machine learning. Extensive experimental results are conducted to compare the performance of the proposed PBSIQA system with those of existing stereo image quality assessment (SIQA) metrics. The developed quality metric can serve as an objective function to optimize the performance of a 3D content delivery system.", "creator": "LaTeX with hyperref package"}}}