{"id": "1604.00092", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "1-Apr-2016", "title": "Variational reaction-diffusion systems for semantic segmentation", "abstract": "A novel global energy model for multi-class semantic image segmentation is proposed that admits very efficient exact inference and derivative calculations for learning. Inference in this model is equivalent to MAP inference in a particular kind of vector-valued Gaussian Markov random field, and ultimately reduces to solving a linear system of linear PDEs known as a reaction-diffusion system. Solving this system can be achieved in time scaling near-linearly in the number of image pixels by reducing it to sequential FFTs, after a linear change of basis.\n\n\n\n\nThis paper examines the computational power of using a hierarchical class of learning. This paper proposes using a Gaussian MIPAR model as a model for multi-class semantic image segmentation. This is a new class of neural networks which provides the opportunity to learn an important number of important spatial and temporal features of neural networks.\nThe first step to understanding these features is to understand how many of the underlying functions of each neural network are related to one another, and how they affect others. For instance, in our case we have to model a group of different types of complex semantic images. The more the image is captured by a machine the more likely that the image is not a particular semantic class but a specific type of functional classification.\n\nThe first step in this paper is to understand how many of the underlying functions of each neural network are related to one another, and how they affect others. For instance, in our case we have to model a group of different types of complex semantic images. The more the image is captured by a machine the more likely that the image is not a particular semantic class but a specific type of functional classification. Neural networks of the networks associated with one another are known as networks. In our case we have to model a group of different kinds of dynamic semantic images. For instance, in our case we have to model a group of different types of dynamic semantic images. The more the image is captured by a machine the more likely that the image is not a particular semantic class but a specific type of functional classification. Neural networks of the networks associated with one another are known as networks. In our case we have to model a group of different types of dynamic semantic images. The more the image is captured by a machine the more likely that the image is not a particular semantic class but a specific type of functional classification. Neural networks of the networks associated with one another are known as networks. In our case we have to model a group of different types of dynamic semantic", "histories": [["v1", "Fri, 1 Apr 2016 01:04:31 GMT  (3681kb,D)", "http://arxiv.org/abs/1604.00092v1", null]], "reviews": [], "SUBJECTS": "cs.CV cs.LG", "authors": ["paul vernaza"], "accepted": false, "id": "1604.00092"}, "pdf": {"name": "1604.00092.pdf", "metadata": {"source": "CRF", "title": "Variational reaction-diffusion systems for semantic segmentation", "authors": ["Paul Vernaza"], "emails": ["<pvernaza@nec-labs.com>"], "sections": [{"heading": "1 Introduction", "text": "The focus of this work is the semantic segmentation problem, in which a learning system must be trained to predict a semantic label for each pixel of an input image. Recent advances in deep convolutional neural nets (CNNs), along with historical successes of global energy methods such as Conditional Random Fields (CRFs), have raised the natural question of how these methods might best be combined to achieve better results on difficult semantic segmentation problems. Although several proposals have recently emerged in this vein [2, 21, 16, 12], there is currently no clear consensus on how best to integrate these methods.\nAchieving tighter integration between and better joint training of global energy models and CNNs is the key motivator for this work. To that end, this paper proposes a novel global energy model for semantic segmentation, referred to here as Variational Reaction Diffusion (or VRD). VRD can be thought of as a vector-valued Gaussian Markov Random Field (GMRF) model over a continuous domain (as opposed to\nar X\niv :1\n60 4.\n00 09\n2v 1\n[ cs\n.C V\n] 1\na graph). Unlike most other energy-based methods, exact inference in VRD can be performed very efficiently by reducing the problem to sequential FFTs, after a linear change of basis. Backpropagation and parameter derivatives can also be computed efficiently, making it an attractive choice for integration with CNNs. The efficiency of VRD also raises some interesting new possibilities for tight integration of CNNs with global energy methods: instead of appending a relatively complex CRF model to an existing CNN architecture, VRD may actually be used as an internal, feature-generating layer, for example. This possibility is explored in the experiments.\nSince inference in VRD is linear in the inputs, an obvious concern is whether such a simple model manages to capture the most important features of seemingly more complex models requiring approximate inference or sophisticated combinatorial optimization. Although the possibility of layering somewhat negates this concern, Section 3 also provides some insight into this issue by showing how VRD can be considered a relaxation of other popular models. Experiments in Section 6 also shed some light on this question by showing that VRD compares favorably to more complex energy-based methods.\nThe name of the proposed method is a reference to the reaction-diffusion systems that initially inspired this work. Briefly, inference in VRD may be interpreted as evolving evidence (or class scores) under the dynamics of a reaction-diffusion process, as illustrated in Fig. 1. Intuitively, we might think of modeling evidence for one semantic class as being created by unary potentials (or the previous layer in a CNN), propagating across the image via diffusion, and reacting with evidence for other semantic classes. Each of these processes may locally create or suppress evidence for each class, and if we allow this process to reach an equilibrium, the sum of these effects must cancel at every point in the image (c.f. Eq. 2). By restricting the model to the class of such processes generating the solutions to convex, variational problems, we are essentially ensuring that such an equilibrium exists and is globally stable.\nThe rest of this paper is structured as follows. The next section gives a very brief overview of the method and a summary of the results that make inference and learning tractable. Section 3 motivates the model by comparing it to existing models, gives some intuition as to how inference works, and discusses other practical issues. The main results for inference and learning VRD are derived in Section 4. This is followed by a discussion of related work and experimental results."}, {"heading": "2 Method overview", "text": "This section gives a brief overview of the main ideas and results of the method. Details will be discussed subsequently."}, {"heading": "2.1 The model", "text": "Let I \u2282 R2 denote the image plane: i.e., a rectangular subset of R2 representing the domain of the image. VRD is given a spatially-varying set of Ni input features, represented here as a function si : I \u2192 RNi , and produces a set of No output scores so : I \u2192 RNo . For now, No might be thought of as the number of semantic classes, and we might think of sok(x) as a score associated with the kth class at x \u2208 I , with a prediction generated via arg maxk s o k(x). Throughout this paper, x will represent an arbitrary point in I .\nLet s = ( soT siT )T denote the concatenation of si and so into a single function\nI \u2192 RNi+No . VRD generates so by solving the following optimization problem. In the following, the dependence of s on x has been omitted for clarity.\narg min so \u222b I sTQs+ 2\u2211 k=1 \u2202s \u2202xk T B \u2202s \u2202xk dx. (1)\nHere, B and Q are assumed to be constant (i.e., independent of x) positive-definite parameter matrices. This is then an infinite-dimensional, convex, quadratic optimization problem in so."}, {"heading": "2.2 Inference", "text": "Just as the minimum of a convex, finite-dimensional quadratic function can be expressed as the solution to a linear system, the solution to this infinite-dimensional quadratic can be expressed as the solution to the following linear system of PDEs:\nBo\u2206so \u2212Qoso = Qisi \u2212Bi\u2206si, (2)\nwhere the dependence on x has again been omitted, \u2206 represents the vector Laplacian ((\u2206f)i := \u2211 j \u22022fi \u2202x2j ), and B and Q have been partitioned into submatrices Bo, Qo, Bi, and Qi such that sTQs = soTQoso + 2soTQisi + f(si) (and likewise for B). We can solve this system efficiently via a linear change of variables and a backsubstitution\nprocedure exactly analogous to the finite-dimensional case. Specifically, we first use the Schur decomposition to write (Bo)\u22121Qo = V UV T, where V is orthonormal and U is upper-triangular. We then perform the change of variables z = V Tso. Let sp := Qisi \u2212 Bi\u2206si. We then solve for z via backsubstitution, first solving the following scalar PDE for zNo , fixing it, solving for zNo\u22121, and proceeding thus backwards to z1:\n\u2206zk \u2212 Ukkzk = (V T(Bo)\u22121sp)k + No\u2211\nj=k+1\nUkjzj . (3)\nAfter solving for z, the output scores are obtained via so = V z. The scalar PDEs above may be discretized and solved either via the FFT or the multigrid method [1]. If L lattice points are used in the discretization, the total computational cost of solving (1) via FFT-based inference is O(No3 +No2L+NoL logL+NoNiL)."}, {"heading": "2.3 Learning", "text": "In order to learn the model, we assume some arbitrary, differentiable loss L(so) has been defined on the output scores so. Gradient-based learning is enabled by computing the derivatives of L with respect to the parameter matrices B, Q, and potentially the inputs si, allowing the model to be used in backpropagation.\nThe backpropagation derivative dLdsp : I \u2192 R No (with sp defined as above) can be computed by solving the same PDE system (2) as in the inference step, but replacing sp with dLdso . Specifically, we solve\nBo\u2206 dL dsp \u2212Qo dL dsp = dL dso (4)\nfor dLdsp , given dL dso : I \u2192 RNo , in the same way as in the inference step. The parameter derivatives can be expressed as simple functions of the backpropagation derivative. These are as follows:\ndL\ndBoij = \u2212\n\u2329 dL\ndspi ,\u2206soj\n\u232a (5)\ndL\ndQoij =\n\u2329 dL\ndspi , soj\n\u232a , (6)\nwhere the inner product is defined in the standard way, as \u3008f, g\u3009 := \u222b I f(x)g(x) dx, and so is that computed via inference for the current values of B and Q."}, {"heading": "3 Analysis", "text": ""}, {"heading": "3.1 Comparison to other energy-based models", "text": "Although the model (1) may appear quite different from other energy-based models due to the continuous formulation, the motivation for and structure of the model is\nvery similar to that for more familiar models. A typical CRF model for segmentation represents a distribution over (class-label-valued) functions defined on the nodes of a graph, which represent pixels or regions. Inference using the mean-field approximation reduces to finding a simplex-valued function on the graph (representing the pseudo-marginal label distributions), subject to some local self-consistency conditions. Unary potentials serve to anchor the distributions at each point, while binary potentials encourage smoothness of these distributions with respect to neighboring points.\nBy contrast, VRD produces a vector-valued score function defined on I . The derivatives in (1) can be replaced by finite-difference approximations to obtain a model defined over a regular lattice graph. This is illustrated in Fig. 2. Unary and binary potentials are both quadratic, as illustrated in Fig. 3a. Since all the potentials are quadratic, the overall energy of (1) can be thought of as the unnormalized loglikelihood of a Gaussian Markov random field, albeit a vector-valued variant.\nIt is therefore evident that the principal difference between the mean-field CRF and VRD models is that VRD relaxes the probability simplex constraints on its outputs, and instead produces unnormalized score functions. VRD also assumes the additional structure that the domain of the modeled function must be equivalent to Euclidean space, and the unary and binary potentials must be quadratic. These key assumptions enable very efficient, exact inference."}, {"heading": "3.2 Motivation for binary potential", "text": "The quadratic binary potential in (1) can be thought of as a natural extension of the standard Potts model commonly employed in energy-based methods. To make this clear, consider a finite-difference approximation of the quadratic term in (1). Denoting\nby \u03b4k a unit vector aligned to axis k, we have\n\u2202so\n\u2202xk\nT\nB \u2202so\n\u2202xk \u2248 \u22122\u2016so(x+ \u03b4k)\u2212 so(x)\u20162B , (7)\nwhere is a small step size. If so were a binary indicator vector (soj(x) = 1 \u21d0\u21d2 label(x) = j), and we had B = I , then this term would correspond exactly to the Potts potential 1{label(x) 6= label(x+ \u03b4k)}. Fig. 3b illustrates the effect of the binary potential for the general case: it essentially serves as a Gaussian prior on the difference between score vectors of neighboring points."}, {"heading": "3.3 Comparison with submodular combinatorial optimization", "text": "Here it is shown that the assumption of convexity of (1) is akin to the common assumption of submodular potentials in combinatorial global energy models. In particular, it is demonstrated that in the binary-label case, a discretized special case of (1) corresponds to a continuous relaxation of a binary, submodular optimization problem.\nFixing Qo = I , Qi = \u2212I , Bi = 0, discretizing (1) via finite differences, and defining an appropriate lattice graph with -spaced nodes N and edges E yields\narg min so \u2211 x\u2208N \u2016so(x)\u20162 \u2212 2so(x)Tsi(x)\n+ \u2211\n(x,x\u2032)\u2208E\n\u22122\u2016so(x\u2032)\u2212 so(x)\u20162Bo . (8)\nAn analogous combinatorial optimization can be defined by optimizing over binary indicator vectors instead of so. Let 1j \u2208 {0, 1}No denote the vector that is 1 in the jth position and 0 elsewhere. The analogous combinatorial optimization is then\narg min l \u2211 x\u2208N \u20161l(x)\u20162 \u2212 21Tl(x)s i(x) (9)\n+ \u2211\n(x,x\u2032)\u2208E\n\u22122\u20161l(x\u2032) \u2212 1l(x)\u20162Bo .\nThe term Eb(i, j) := \u20161i \u2212 1j\u20162Bo is referred to as the binary potential. In the binarylabel case (No = 2), this optimization is said to be submodular if the following condition holds:\nEb(0, 0) + Eb(1, 1) \u2264 Eb(0, 1) + Eb(1, 0). (10)\nIn our case, we have Eb(0, 0) = Eb(1, 1) = 0 and Eb(1, 0) = Eb(0, 1) = Bo00 + Bo11 \u2212Bo10 \u2212Bo01, which is nonnegative by the convexity assumption, since convexity requires that Bo be positive semidefinite. This implies that the combinatorial analog of (8) is submodular in the binary-label case. Equivalently, we may interpret (8) as a relaxation of a combinatorial optimiziation obtained by relaxing the integrality and simplex constraints on so. This may also be compared to LP relaxation, which would relax the integrality constraint, but retain the simplex constraints, at the expense of harder optimization."}, {"heading": "3.4 Intuition for inference", "text": "The key step in performing inference is the solution of the scalar PDEs (3). In the case of constant B and Q, this can be solved by taking the Fourier transform of both sides, solving algebraically for the transform of the solution, and then inverting the transform. This process is equivalent to convolving the right-hand side in the spatial domain with the Green\u2019s function, which is illustrated in Fig. 4. It is therefore evident that for large values of Ukk, solving (3) essentially convolves the right-hand side with a delta function (i.e., applying the identity function), while the solution for small values of Ukk convolves the right-hand side with an edge-preserving filter with a very wide support. Recalling that the Ukk are the (positive) eigenvalues of (Bo)\u22121Qo, it is also therefore evident that the amount of smoothing scales with the scale ofBo and inversely with the scale of Qo. Intuitively, this means that the smoothing decreases as the unary penalty grows and increases as the binary penalty grows, just as one might expect.\nIn practice, (3) is solved via discretization and discrete Fourier transforms. Specifically, the right-hand side of (3) is discretized over a regular grid, and the goal is then to obtain samples of zkk over the same grid. To do this, the Laplacian is first discretized in the usual way. Letting f denote the right-hand side of (3), assuming I has been discretized such that (xi, yj) represents the (i, j)th grid point, and assuming unit distance between adjacent grid points, this yields the following finite system of linear equations\n\u2200(i, j):\nf(xi, yj) =\u2212 (Ukk + 4)zkk(xi, yj) + \u2211\n\u2016\u03b4\u20161=1,\u03b4\u2208Z2 zkk(xi+\u03b41 , yj+\u03b42). (11)\nAssuming zero boundary conditions, this system can be solved by a discrete sine transform. Since the above expression can be written as a convolution of zkk with some filter F , this is a deconvolution problem to find zkk given f , and it can be solved by the aforementioned transform, multiply, inverse-transform method."}, {"heading": "3.5 Reparameterization", "text": "In practice, it was found that a naive parameterization of B and Q caused optimization problems in learning due to ill-conditioning. Figure 4 hints at the reason for this: the amount of smoothing varies dramatically as the eigenvalues of (Bo)\u22121Qo vary in a very small region around zero. An exponential change of coordinates helps to remedy this situation. Specifically, matrices B\u0304o and Q\u0304o are defined such thatBo = exp B\u0304o and Qo = exp Q\u0304o, where exp refers to the matrix exponential. The learning optimization is then performed in the variables B\u0304o and Q\u0304o.\nThe loss derivatives with respect to the new variables can be computed as follows. Define dLdQo as in (6). Let Q\u0304\no = U\u039bUT be an eigendecomposition of Q\u0304o, defining U and \u039b. Then, using known results for the derivative of the matrix exponential [14], it can be shown that\ndL\ndQ\u0304o = U (UT dL dQo T U ) \u03a6 UT, (12) where is the Hadamard (elementwise) product and \u03a6 is defined as follows (defining \u03bbi := \u039bii):\n\u03a6ij = { (e\u03bbi \u2212 e\u03bbj )/(\u03bbi \u2212 \u03bbj) if \u03bbi 6= \u03bbj e\u03bbi if \u03bbi = \u03bbj . (13)\nFor the above to hold, and for Qo to be positive definite, Q\u0304o must also be symmetric. This can be enforced by a final transformation, representing Q\u0304o as the sum of another\nmatrix and its transpose. It is then straightforward to apply the chain rule to find the derivative with respect to that parameterization. The expression for dL\ndB\u0304o is analogous\nto (12)."}, {"heading": "3.6 Computational complexity", "text": "Computationally, inference can be decomposed into three components: performing the Schur decomposition, performing a change of basis, and solving the scalar PDEs via backsubstitution. The cost of performing the Schur decomposition is O(No3) [7]. Let L denote the total number of points in the lattice discretization (i.e., the number of pixels in the image). The change of basis z = V Tso (and its inverse) consists of transforming aNo-dimensional vector via the square matrix V at each lattice point, at a cost of O(No2L). The backsubstitution procedure (3) consists of computing the right-hand side, which costsO(No2L), and solving theNo scalar PDEs. Solving each via the DST costs O(L logL). Comptuing sp costs an additional O(NoNiL). The total computational complexity of inference is therefore O(No3 + No2L + NoL logL + NoNiL). Computing the derivatives for learning requires the same amount of work, plus an additional O(L) work to compute each component of (6) and (5). The asymptotic complexity is therefore the same as for inference."}, {"heading": "4 Derivation", "text": "The results of Section 2 are now derived."}, {"heading": "4.1 Deriving the reaction-diffusion PDE", "text": "First, the convexity of (1) is easily shown under the assumption of positive-semidefiniteness of B and Q (by showing Jensen\u2019s inequality holds\u2014proof omitted). This implies that any stationary point of (1) must be optimal. We can find a stationary point by first finding the linear part of (1) for small variations (i.e., the Fre\u0301chet derivative) and then equating this to zero. Denote by J : C2(R2;RNi+No) \u2192 R the objective function in (1), which maps a twice-differentiable, vector-valued function on R2 to a scalar. Let the notation dfx represent the derivative of a function f at a point x, so that dJs : C\n2(R2;RNi+No) \u2192 R represents the derivative of J at s. dJs is identified as the coefficient of in J(s+ v), where v is an arbitrary variation. This yields\ndJs(v) = 2 \u222b I vTQs+ 2\u2211 k=1 \u2202v \u2202xk T B \u2202s \u2202xk dx, (14)\nwhich is verified to be the derivative, as it is linear in v. We now wish to express the term involving B as an inner product with v. To do so, we first rewrite this term (dropping the 2) as \u222b\nI Ni+No\u2211 i,j=1 \u2207vTi\u2207sjBij dx. (15)\nWe then apply Green\u2019s identity\u222b I \u03c8\u2206\u03c6+\u2207\u03c8T\u2207\u03c6dx = \u222b \u2202I \u03c8\u2207\u03c6Tn\u0302dS (16)\nfor \u03c6 = vi, \u03c8 = sj , use the fact that vi = 0 on \u2202I , and regroup terms to obtain\ndJs(v) = 2 \u222b I vT(Qs\u2212B\u2206s) dx. (17)\nStationarity requires that this be zero on the subspace of feasible variations, which consists of those variations that do not change the si components, as these are assumed\nfixed. Decomposing v as v = ( voT vi T )T\n, we have vi = 0 and\u222b I voT(Qoso +Qisi \u2212Bo\u2206so \u2212Bi\u2206si) dx = 0. (18)\nFinally, applying the fundamental lemma of the calculus of variations yields (2)."}, {"heading": "4.2 Solving the PDE system", "text": "The next step is to reduce the solution of (2) to a sequence of scalar PDE subproblems. Again defining sp = Qisi \u2212Bi\u2206si, we left-multiply (2) by (Bo)\u22121 to obtain\n\u2206so \u2212 (Bo)\u22121Qoso = (Bo)\u22121sp. (19)\nWe then use the Schur decomposition [7] to write (Bo)\u22121Qo = V UV T, where V is orthonormal. The assumption that B and Q are positive-definite implies that Bo and Qo are also positive definite, which implies that (Bo)\u22121Qo has a complete set of real, positive eigenvalues (equal to those of \u221a BoQo \u221a Bo, which is positive-definite). By the properties of the Schur decomposition, U will therefore be upper-triangular, with the positive eigenvalues of (Bo)\u22121Qo on the diagonal. Substituting the Schur decompsition and left-multiplying by V T yields\nV T\u2206so \u2212 UV Tso = V T(Bo)\u22121sp. (20)\nThe next, key step is to observe that the vector Laplacian commutes with constant linear transformations: i.e., V T\u2206so = \u2206V Tso. This is straightforward to show by expanding the definitions of matrix multiplication and the vector Laplacian. This allows us to perform the change of coordinates z = V Tso, and to solve for z instead. The fact that U is upper-triangular allows us to solve for z via the backsubstitution algorithm in (3)."}, {"heading": "4.3 Derivatives", "text": "Inference is regarded as a function mapping sp (as previously defined) to so by solving (2). The fact that this function is well-defined follows from the uniqueness of solutions of (2), which follows from the assumption that B and Q are positive definite, and the fact that the scalar PDEs (3) have unique solutions [5]; in other words, the\nlinear differential operator Bo\u2206\u2212Qo is invertible. Let G := (Bo\u2206\u2212Qo)\u22121. We now assume a loss L is defined on the output of G, and we wish to find the derivatives of the loss with respect to the input of G (i.e., the backpropagation derivative) as well as the derivatives with respect to the parameters Bo and Qo.\nFirst, we assume that the derivative of L is provided in the form of a function dL dso\n: I \u2192 RNo : (dLso)v = \u2329 dL dso , v \u232a , (21)\nwhere v is an arbitrary variation. Intuitively, dLdso represents the differential change in L due to a variation of so at the point given by its input. We would like to obtain the derivative of L\u25e6G in the same form. By the chain rule and the definition of the adjoint,\n(d(L \u25e6G)sp)v = \u2329 dL dso , (dGsp)v \u232a = \u2329 (dGsp) \u2217 dL dso , v \u232a . (22)\nSince G is linear in sp, dGsp = G. Furthermore, G is self-adjoint; this follows from the fact that Bo\u2206 \u2212 Qo is self-adjoint, which in turn can be shown given that Bo and Qo are self-adjoint (by the assumption of positive-definiteness) andBo commutes with \u2206. This implies dLdsp = G dL dso , which is equivalent to (4).\nTo obtain the parameter derivatives, we directly consider the effect of adding a small perturbation to each parameter. We first define the unperturbed solution so as the solution to (Bo\u2206 \u2212Qo)so = sp, given the input sp. We then define the perturbed solution s\u0303o as the solution to ((Bo + V )\u2206 \u2212 Qo)s\u0303o = sp, where V is a variation of Bo. We then find the following expansion of s\u0303o in . In the following, the notation GBo is used to refer to G evaluated with the parameter Bo.\n(Bo\u2206\u2212Qo)s\u0303o = sp \u2212 V\u2206s\u0303o\ns\u0303o = GBo(s p \u2212 V\u2206s\u0303o)\n= so \u2212 GBoV\u2206s\u0303o = so \u2212 GBoV\u2206(so \u2212 GBoV\u2206s\u0303o) = so \u2212 GBoV\u2206so +O( 2) (23)\nNote that the preceding two lines are obtained by recursive expansion. This implies that (dGBo)V = \u2212GBoV\u2206so, again abusing notation so that dGBo refers to the derivative of G as a function of the parameter Bo, and evaluated at the point Bo. The chain rule and adjoint property are applied again to obtain\n(d(L \u25e6G)Bo)V = \u2329 \u2212GBo dL\ndso , V\u2206so\n\u232a (24)\n= \u2329 \u2212 dL\ndsp , V\u2206so\n\u232a . (25)\nThe previous arguments apply even in the case that Bo is a function of I (i.e., depends on x). If Bo is constrained to be constant on I , however, the variations V must also be\nconstant, and we can write a basis for these in the form 1i1Tj , for (i, j) \u2208 {1, . . . , No}2. We then define dLdBoij = (d(L \u25e6G)Bo)1i1 T j . Evaluating this using the expression above then yields (5). Repeating the argument above mutatis mutandis for Qo yields (6)."}, {"heading": "5 Related work", "text": "The method proposed here is comparable to recent work on joint training of CNNs and CRFs. In particular, [21] and [16] both propose backpropagation-through-inference techniques in order to jointly train fully-connected CRFs (FC-CRFs) [11] and CNNs. Inference consists of performing a fixed number of message-passing iterations on the pseudo-marginals of the mean-field approximation. These message-passing steps are then regarded as a formal composition of functions, to which backpropagation is applied. The FC-CRF model is probably more expressive than VRD, in that it features non-local connections. However, this comes at the expense of having to resort to approximate inference. The asymptotic complexity of performing a single messagepassing round in the FC-CRF is comparable to that of completing the entire exact inference procedure in VRD. The method of [15] is also notable for jointly training a CNN with inference based on a global convex energy model, but inference and learning in this model rely on general optimization techniques that are costly when dealing with image-sized optimization problems.\nGaussian random field models have previously been applied to the task of semantic segmentation [20] and related tasks [19, 9]; however, exact inference in these methods scales at least quadratically in the number of pixels, making approximate inference necessary again.\nRecent work [3] learned reaction-diffusion processes for image restoration; however, this work relied on simulating the process in time to do inference and learning, resulting in a method very similar to the aforementioned backpropagation-throughinference techniques.\nSolving the PDE (2) is related to Wiener filtering. It is known that certain scalarvalued Gaussian MRFs may be solved via Wiener filtering [18]; to our knowledge, the inference procedure in this work is novel in that it essentially reduces inference in a vector-valued Gaussian MRF to repeated Wiener filtering."}, {"heading": "6 Experiments", "text": "VRD was implemented in Caffe [10] and compared to several other methods on two datasets: the KITTI road segmentation dataset [6] (a binary classification problem) and the Stanford background dataset [8] (an 8-way classification problem). The experiments were conducted with the following primary goals in mind: (1) to directly compare VRD to an global energy method based on \u201cbackpropagation-through-approximateinference;\u201d (2) to investigate whether composing multiple VRD layers is feasible and/or beneficial; and (3) to see whether integration of VRD with deep CNNs might provide any benefit.\nAll of the parameters (CRF, VRD, and convolution) in all of the experiments were trained jointly via backpropagation, using the AdaGrad algorithm [4]. Each experiment consisted of training all parameters from scratch\u2014i.e., using random initialization and no other training data besides\u2014and all experiments were run until the loss converged (using a stepwise-annealed learning rate). The softmax loss was used for all experiments. VRD was compared to the previously discussed method of [21], referred to here as CRF-RNN, which performs backpropagation through approximate inference for a FC-CRF. The default number of mean-field iterations (10) was used for this method. The authors\u2019 public implementation was used. The method of [17], referred to here as GoogleNet, was also evaluated. The public BVLC implementation was used, slightly modified for semantic segmentation by transforming it into a fullyconvolutional model [13]. A single random split of each dataset was chosen, with 80% of the data reserved for training and the remaining held-out. Each experiment reports the results of evaluation on this held-out set.\nEach experiment involved training a CNN with one of three general architectures, which are illustrated in Fig. 5. Each type of architecture was designed primarily with the goal of testing one of the questions mentioned above. Experiments with shallow architectures were designed to directly compare the merits of different global energy models. The Layered VRD experiments were intended to test whether layering VRD layers is feasible and provides any benefit (the layered baseline experiment is identical to Layered VRD, but exchanging the VRD layers with 1x1 convolutions). The GoogleNet experiments were intended to test whether VRD is useful in the context of joint training with a deep CNN.\nPrecision-recall curves for the KITTI experiments are shown in Fig. 7, and qualitative results are shown in Fig. 6. Table 1 lists evaluation results for both datasets: for KITTI, maximum F1 and AP are reported in the birds-eye view (as suggested in [6]); and for Stanford Background (abbreviated SBG), pixel-level accuracies are reported.\nComparing VRD and CRF-RNN, it was consistently observed that better results were obtained using VRD. The most significant benefits were observed with shallow architectures, although a significant benefit was also observed in the experiments with GoogleNet on SBG. Layering VRD proved to be both practical and beneficial. On the KITTI dataset, this method seemed to produce the best overall results, despite having far fewer parameters and a far simpler architecture than GoogleNet. However, this architecture had too few parameters to fit the SBG data well. Finally, joint training of VRD with GoogleNet was not beneficial for the KITTI dataset; however, a large benefit was seen on the more difficult SBG dataset.\nTo give a general idea of the computational efficiency of VRD, for an internal VRD layer with Ni = 64, No = 32, and an image size of 511x255 (L = 130305), forwardpass inference took 408 ms, and computing derivatives in the backwards pass took 760 ms. These timings are for a CPU implementation, although sp was computed via GPU convolutions."}, {"heading": "7 Conclusions", "text": "A global energy model for semantic segmentation featuring very efficient exact inference was proposed. Inference in VRD for a problem with No output labels reduces to a sequence of No convolutions, which can be implemented efficiently via the FFT, and backpropagation and parameter derivatives for learning can be computed just as efficiently, making it an attractive choice for joint training with CNNs.\nAnalysis revealed how VRD can be thought of as a relaxation of other global energy methods. Despite this, experiments demonstrated superior performance of VRD compared to a more complex FC-CRF-based model in the context of joint training with a CNN. This suggests that, at least in the tested scenarios, the benefits of exact inference may outweigh those of having a more expressive or sophisticated model. The experiments also demonstrated the feasibility of composing multiple VRD layers, and this yielded promising results.\nIn the short term, more work needs to be done to devise and test CNN architectures that are able to leverage the ability of VRD to efficiently produce and backpropagate through exact global inferences. In the longer term, it is hoped that the insights developed here will lead to a better general understanding of how best to integrate CNNs with global energy models."}], "references": [{"title": "Guide to multigrid development", "author": ["Achi Brandt"], "venue": "In Multigrid methods,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 1982}, {"title": "Semantic image segmentation with deep convolutional nets and 15  fully connected crfs", "author": ["Liang-Chieh Chen", "George Papandreou", "Iasonas Kokkinos", "Kevin Murphy", "Alan L Yuille"], "venue": "In ICLR,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2015}, {"title": "On learning optimized reaction diffusion processes for effective image restoration", "author": ["Yunjin Chen", "Wei Yu", "Thomas Pock"], "venue": "In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2015}, {"title": "Adaptive subgradient methods for online learning and stochastic optimization", "author": ["John Duchi", "Elad Hazan", "Yoram Singer"], "venue": "The Journal of Machine Learning Research,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2011}, {"title": "Partial differential equations", "author": ["Lawrence C Evans"], "venue": "American Mathematical Society,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2010}, {"title": "A new performance measure and evaluation benchmark for road detection algorithms", "author": ["Jannik Fritsch", "Tobias Kuehnl", "Andreas Geiger"], "venue": "In International Conference on Intelligent Transportation Systems (ITSC),", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2013}, {"title": "Decomposing a scene into geometric and semantically consistent regions", "author": ["Stephen Gould", "Richard Fulton", "Daphne Koller"], "venue": "In Computer Vision,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2009}, {"title": "Regression tree fieldsan efficient, non-parametric approach to image labeling problems", "author": ["Jeremy Jancsary", "Sebastian Nowozin", "Toby Sharp", "Carsten Rother"], "venue": "In Computer Vision and Pattern Recognition (CVPR),", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2012}, {"title": "Caffe: Convolutional architecture for fast feature embedding", "author": ["Yangqing Jia", "Evan Shelhamer", "Jeff Donahue", "Sergey Karayev", "Jonathan Long", "Ross Girshick", "Sergio Guadarrama", "Trevor Darrell"], "venue": "arXiv preprint arXiv:1408.5093,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2014}, {"title": "Efficient inference in fully connected crfs with gaussian edge potentials", "author": ["Philipp Kr\u00e4henb\u00fchl", "Vladlen Koltun"], "venue": "Advances in Neural Information Processing Systems", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2011}, {"title": "Efficient piecewise training of deep structured models for semantic segmentation", "author": ["Guosheng Lin", "Chunhua Shen", "Ian D. Reid", "Anton van den Hengel"], "venue": "CoRR, abs/1504.01013,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2015}, {"title": "Fully convolutional networks for semantic segmentation", "author": ["Jonathan Long", "Evan Shelhamer", "Trevor Darrell"], "venue": "In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2015}, {"title": "Derivatives of the matrix exponential and their computation", "author": ["Igor Najfeld", "Timothy F Havel"], "venue": "Advances in Applied Mathematics,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 1995}, {"title": "A deep variational model for image segmentation", "author": ["Ren\u00e9 Ranftl", "Thomas Pock"], "venue": "In Pattern Recognition,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2014}, {"title": "Fully connected deep structured networks", "author": ["Alexander G Schwing", "Raquel Urtasun"], "venue": "arXiv preprint arXiv:1503.02351,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2015}, {"title": "Computer vision: algorithms and applications", "author": ["Richard Szeliski"], "venue": "Springer Science & Business Media,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2010}, {"title": "Utilizing variational optimization to learn markov random fields", "author": ["Marshall F Tappen"], "venue": "In Computer Vision and Pattern Recognition,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2007}, {"title": "The logistic random fielda convenient graphical model for learning parameters for mrf-based labeling", "author": ["Marshall F Tappen", "Kegan GG Samuel", "Craig V Dean", "David M Lyle"], "venue": "In Computer Vision and Pattern Recognition,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2008}, {"title": "Conditional random fields as recurrent neural networks", "author": ["Shuai Zheng", "Sadeep Jayasumana", "Bernardino Romera-Paredes", "Vibhav Vineet", "Zhizhong Su", "Dalong Du", "Chang Huang", "Philip HS Torr"], "venue": "In Proceedings of the IEEE International Conference on Computer Vision,", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2015}], "referenceMentions": [{"referenceID": 1, "context": "Although several proposals have recently emerged in this vein [2, 21, 16, 12], there is currently no clear consensus on how best to integrate these methods.", "startOffset": 62, "endOffset": 77}, {"referenceID": 18, "context": "Although several proposals have recently emerged in this vein [2, 21, 16, 12], there is currently no clear consensus on how best to integrate these methods.", "startOffset": 62, "endOffset": 77}, {"referenceID": 14, "context": "Although several proposals have recently emerged in this vein [2, 21, 16, 12], there is currently no clear consensus on how best to integrate these methods.", "startOffset": 62, "endOffset": 77}, {"referenceID": 10, "context": "Although several proposals have recently emerged in this vein [2, 21, 16, 12], there is currently no clear consensus on how best to integrate these methods.", "startOffset": 62, "endOffset": 77}, {"referenceID": 0, "context": "The scalar PDEs above may be discretized and solved either via the FFT or the multigrid method [1].", "startOffset": 95, "endOffset": 98}, {"referenceID": 12, "context": "Then, using known results for the derivative of the matrix exponential [14], it can be shown that", "startOffset": 71, "endOffset": 75}, {"referenceID": 4, "context": "The fact that this function is well-defined follows from the uniqueness of solutions of (2), which follows from the assumption that B and Q are positive definite, and the fact that the scalar PDEs (3) have unique solutions [5]; in other words, the", "startOffset": 223, "endOffset": 226}, {"referenceID": 18, "context": "In particular, [21] and [16] both propose backpropagation-through-inference techniques in order to jointly train fully-connected CRFs (FC-CRFs) [11] and CNNs.", "startOffset": 15, "endOffset": 19}, {"referenceID": 14, "context": "In particular, [21] and [16] both propose backpropagation-through-inference techniques in order to jointly train fully-connected CRFs (FC-CRFs) [11] and CNNs.", "startOffset": 24, "endOffset": 28}, {"referenceID": 9, "context": "In particular, [21] and [16] both propose backpropagation-through-inference techniques in order to jointly train fully-connected CRFs (FC-CRFs) [11] and CNNs.", "startOffset": 144, "endOffset": 148}, {"referenceID": 13, "context": "The method of [15] is also notable for jointly training a CNN with inference based on a global convex energy model, but inference and learning in this model rely on general optimization techniques that are costly when dealing with image-sized optimization problems.", "startOffset": 14, "endOffset": 18}, {"referenceID": 17, "context": "Gaussian random field models have previously been applied to the task of semantic segmentation [20] and related tasks [19, 9]; however, exact inference in these methods scales at least quadratically in the number of pixels, making approximate inference necessary again.", "startOffset": 95, "endOffset": 99}, {"referenceID": 16, "context": "Gaussian random field models have previously been applied to the task of semantic segmentation [20] and related tasks [19, 9]; however, exact inference in these methods scales at least quadratically in the number of pixels, making approximate inference necessary again.", "startOffset": 118, "endOffset": 125}, {"referenceID": 7, "context": "Gaussian random field models have previously been applied to the task of semantic segmentation [20] and related tasks [19, 9]; however, exact inference in these methods scales at least quadratically in the number of pixels, making approximate inference necessary again.", "startOffset": 118, "endOffset": 125}, {"referenceID": 2, "context": "Recent work [3] learned reaction-diffusion processes for image restoration; however, this work relied on simulating the process in time to do inference and learning, resulting in a method very similar to the aforementioned backpropagation-throughinference techniques.", "startOffset": 12, "endOffset": 15}, {"referenceID": 15, "context": "It is known that certain scalarvalued Gaussian MRFs may be solved via Wiener filtering [18]; to our knowledge, the inference procedure in this work is novel in that it essentially reduces inference in a vector-valued Gaussian MRF to repeated Wiener filtering.", "startOffset": 87, "endOffset": 91}, {"referenceID": 8, "context": "VRD was implemented in Caffe [10] and compared to several other methods on two datasets: the KITTI road segmentation dataset [6] (a binary classification problem) and the Stanford background dataset [8] (an 8-way classification problem).", "startOffset": 29, "endOffset": 33}, {"referenceID": 5, "context": "VRD was implemented in Caffe [10] and compared to several other methods on two datasets: the KITTI road segmentation dataset [6] (a binary classification problem) and the Stanford background dataset [8] (an 8-way classification problem).", "startOffset": 125, "endOffset": 128}, {"referenceID": 6, "context": "VRD was implemented in Caffe [10] and compared to several other methods on two datasets: the KITTI road segmentation dataset [6] (a binary classification problem) and the Stanford background dataset [8] (an 8-way classification problem).", "startOffset": 199, "endOffset": 202}, {"referenceID": 3, "context": "All of the parameters (CRF, VRD, and convolution) in all of the experiments were trained jointly via backpropagation, using the AdaGrad algorithm [4].", "startOffset": 146, "endOffset": 149}, {"referenceID": 18, "context": "VRD was compared to the previously discussed method of [21], referred to here as CRF-RNN, which performs backpropagation through approximate inference for a FC-CRF.", "startOffset": 55, "endOffset": 59}, {"referenceID": 11, "context": "The public BVLC implementation was used, slightly modified for semantic segmentation by transforming it into a fullyconvolutional model [13].", "startOffset": 136, "endOffset": 140}, {"referenceID": 5, "context": "Table 1 lists evaluation results for both datasets: for KITTI, maximum F1 and AP are reported in the birds-eye view (as suggested in [6]); and for Stanford Background (abbreviated SBG), pixel-level accuracies are reported.", "startOffset": 133, "endOffset": 136}], "year": 2016, "abstractText": "A novel global energy model for multi-class semantic image segmentation is proposed that admits very efficient exact inference and derivative calculations for learning. Inference in this model is equivalent to MAP inference in a particular kind of vector-valued Gaussian Markov random field, and ultimately reduces to solving a linear system of linear PDEs known as a reaction-diffusion system. Solving this system can be achieved in time scaling near-linearly in the number of image pixels by reducing it to sequential FFTs, after a linear change of basis. The efficiency and differentiability of the model make it especially well-suited for integration with convolutional neural networks, even allowing it to be used in interior, feature-generating layers and stacked multiple times. Experimental results are shown demonstrating that the model can be employed profitably in conjunction with different convolutional net architectures, and that doing so compares favorably to joint training of a fully-connected CRF with a convolutional net.", "creator": "LaTeX with hyperref package"}}}