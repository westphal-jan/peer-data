{"id": "1604.00100", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "1-Apr-2016", "title": "A Compositional Approach to Language Modeling", "abstract": "Traditional language models treat language as a finite state automaton on a probability space over words. This is a very strong assumption when modeling something inherently complex such as language. In this paper, we challenge this by showing how the linear chain assumption inherent in previous work can be translated into a sequential composition tree. We then propose a new model that marginalizes over all possible composition trees thereby removing any underlying structural assumptions. As the partition function of this new model is intractable, we use a recently proposed sentence level evaluation metric Contrastive Entropy to evaluate our model. Given this new evaluation metric, we report more than 100% improvement across distortion levels over current state of the art recurrent neural network based language models. In other words, this is the first time we have implemented an algorithm that quantifies the probability of a word over a word on a set of nodes. Using our model we are able to apply the model to the entire dataset of 10,000 nodes that were trained for the past 24 hours. For more on how we should evaluate the model in the future, visit the links below for further updates on the model, which can be found here.\n\n\n\nThe model in question is a series of recurrent neural networks that are thought to be loosely related to network learning. Each model is a model that is distributed with tensor-fold similarity between neural networks and random-domain networks. The algorithm is a subset of the network learning algorithm, meaning that the recurrent neural network learning algorithm has a linear distribution.\nThe system's network learning algorithm combines a series of recurrent neural networks with a network learning algorithm and uses the original network learning algorithm to identify random-domain and deterministic regions. When we generate a random-domain random-domain random-domain model, the neural network learning algorithm generates a sequence of sequences that are a continuous list of regions. This produces an approximate representation of a matrix of the neural networks in each region, as described earlier. This is a good representation of the neural network learning algorithm as follows:\nTo identify the distribution of the neural networks, we generate a set of recurrent neural networks and apply the original neural network learning algorithm to a set of random-domain regions. The neural network learning algorithm has a linear distribution. The neural network learning algorithm uses the original neural network learning algorithm for a set of random-domain regions. These regions are a small subset of the neural networks that are thought to be distributed between the same regions, each of which contains random-domain regions. The neural network learning algorithm uses the original neural network", "histories": [["v1", "Fri, 1 Apr 2016 01:51:34 GMT  (300kb,D)", "http://arxiv.org/abs/1604.00100v1", "submitted to ACL 2016"]], "COMMENTS": "submitted to ACL 2016", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["kushal arora", "anand rangarajan"], "accepted": false, "id": "1604.00100"}, "pdf": {"name": "1604.00100.pdf", "metadata": {"source": "CRF", "title": "A Compositional Approach to Language Modeling", "authors": ["Kushal Arora", "Anand Rangarajan"], "emails": ["karora@cise.ufl.edu", "anand@cise.ufl.edu"], "sections": [{"heading": "1 Introduction", "text": "The objective of language modeling is to build a probability distribution over sequences of words. The traditional approaches, inspired by Shannon\u2019s game, has molded this problem into merely predicting the next word given the context. This leads to a linear chain model on words. In its simplest formulation, these conditional probabilities are estimated using frequency tables of word wi following the sequence w1i\u22121. There are two big issues with this formulation. First, the number of parameters rises exponentially with the size of the context. Second, it is impossible to see all such combinations in the training set, however large it may be. In traditional models, the first problem, famously called the curse of dimensionality, is tack-\nled by limiting the history to the previous n \u2212 1 words leading to an n-gram model. The second problem, one of sparsity, is tackled by redistributing the probability mass over seen and unseen examples usually by applying some kind of smoothing or interpolation techniques. A good overview of various smoothing techniques and their relative performance on language modeling tasks can be found in (Goodman, 2001).\nSmoothened n-gram language models fail on two counts: their inability to generalize and their failure to capture the longer context dependencies. The first one is due to the discrete nature of the problem and the lack of any kind of implicit measure of relatedness or context based clustering among words and phrases. The second problem\u2014the failure to capture longer context dependencies\u2014is due to the n-order Markov restriction applied to deal with the curse of dimensionality. There have been numerous attempts to address both the issues in the traditional ngram framework. Class based models (Brown et al., 1992; Baker and McCallum, 1998; Pereira et al., 1993) try to solve the generalization issue by deterministically or probabilistically mapping words to one or multiple classes based on manually designed or probabilistic criteria. The issue of longer context dependencies has been addressed using various approximations such as cache models (Kuhn and De Mori, 1990), trigger models (Lau et al., 1993) and structured language models (Charniak, 2001; Chelba et al., 1997; Chelba and Jelinek, 2000).\nNeural network based language models take an entirely different approach to solving the generalization problem. Instead of trying to solve the difficult task of modeling the probability distribution over discrete sets of words, they try to embed these words into a continuous space and then build a smooth probability distribution over it. Feedfor-\nar X\niv :1\n60 4.\n00 10\n0v 1\n[ cs\n.C L\n] 1\nA pr\n2 01\n6\nward neural network based models (Bengio et al., 2006; Mnih and Hinton, 2009; Morin and Bengio, 2005) embed the concatenated n-gram history in this latent space and then use a softmax layer over these embeddings to predict the next word. This solves the generalization issue by building a smoothly varying probability distribution but is still unable to capture longer dependencies beyond the Markovian boundary. Recurrent neural network based models (Mikolov et al., 2011a; Mikolov et al., 2010) attempt to address this by recursively embedding history in the latent space, predicting the next word based on it and then updating the history with the word. Theoretically, this means that the entire history can now be used to predict the next word, hence, the network has the ability to capture longer context dependencies.\nAll the models discussed above solve the two aforementioned issues to varying degrees of success but none of them actually challenge the underlying linear chain model assumption. Language is recursive in nature, and this along with the underlying compositional structure should play an important role in modeling language. The computational linguistics community has been working for years on formalizing the underlying structure of language in the form of grammars. A step in the right direction would be to look beyond simple frequency estimation-based methods and to use these compositional frameworks to assign probability to words and sentences.\nWe start by looking at n-gram models and show how they have an implicit sequential tree assumption. This brings us to the following questions: Is a sequential tree the best compositional structure to model language? If not, then, what is the best compositional structure? Further, do we even need to find one such structure, or can we marginalize over all structures to remove any underlying structural assumptions?\nIn this paper we take the latter approach. We model the probability of a sentence as the marginalized joint probability of words and composition trees (over all possible rooted trees). We use a probabilistic context free grammar (PCFG) to generate these trees and build a probability distribution on them. As generalization is still an issue, we use distributed representations of words and phrases and build a probability distribution on them in the latent space. A similar approach but in a different setting has been attempted in (Socher et\nal., 2013) for language parsing. The major difference between our approach and theirs is the way we handle the breaking of PCFG\u2019s independence assumption due to the distributed representation. Instead of approximating the marginalization using the n-best trees as in (Socher et al., 2013), we restore this independence assumption by averaging over phrasal representations leading to a single phrase representation. This single representation for phrases in turn allows us to use an efficient Inside-Outside (Lari and Young, 1990) algorithm for exact marginalization and training."}, {"heading": "2 Compositional View of an N-gram model", "text": "Let us consider a sequence of words wn1 . A linear chain model would factorize the probability of this sentence p(wn1 ) as a product of conditional probabilities p(wi|hi) leading to the following factorization:\np(wn1 ) = n\u220f i=1 p(wi|hi). (1)\nAll the models discussed in previous section differ in how the history or context hi is represented. For a linear chain model with no assumptions, the history hi would be the previous i \u2212 1 words, so the factorization is\np(wn1 ) = n\u220f i=1 p(wi|wi\u221211 ). (2)\nIf we move the probability space to sequences of words, the same factorization in equation (2)\ncan be written as:\np(wn1 , . . . , w i 1, . . . , w 2 1, wn, . . . , wi, . . . , w1) =\nn\u220f i=1 p(wi1|wi\u221211 , wi)p(wi). (3)\nFigure 1 shows the sequential compositional structure endowed by the factorization in equation (3). As the particular factorization is a byproduct of the underlying compositional structure, we can rewrite (3) as a probability density conditioned on this sequential tree t as follows:\np(wn1 |t) = n\u220f i=1 p(wi1|wi\u221211 , wi)p(wi). (4)\nHaving shown the sequential compositional structure assumption of the n-gram model, in the next section we try to remove this conditional density assumption by modeling the joint probability of the sentences and the compositional trees."}, {"heading": "3 The Compositional Language Model", "text": "In this section, we build the framework to carry out the marginalization over all possible trees. Let W be the sentence and T (W ) be the set of all compositional trees for sentence W . The probability of the sentence p(W ) can then be written in terms of the joint probability over the sentence and compositional structure as\np(W ) = \u2211\nt\u2208T (W )\np(W |t)p(t) (5)\nExamining (5), we see that we have two problems to solve: i) enumerating and building probability distributions over trees p(t) and ii) modeling the probability of sentences conditioned on compositional trees p(W |t).\nProbabilistic Context Free Grammars (PCFGs) fit the first use case perfectly. We define a PCFG as a quintuple:\n(G, \u03b8) = (N,T,R, S, P ) (6)\nwhere N is the set of non-terminal symbols, T is the set of terminal symbols, R is the finite set of production rules, S is the special start symbol which is always at the root of a parse tree and P is the set of probabilities on production rules.1 \u03b8 is a\n1We restrict our grammar to Chomsky Normal Form (CNF) to simplify the derivation and explanation.\nreal valued vector of length |R| with the rth index mapping to rule r \u2208 R and value \u03b8r \u2208 P .\nUsing this definition, we can write the probability of any tree t, p(t), as the product of the production rules used to derive the sentence W from S i.e.\np(t) = \u220f\nr\u2208R\u0303t(W )\n\u03b8r (7)\nwhere R\u0303t(W ) \u2208 R is the set of production rules used to derive tree t. As we are only interested in the compositional structure, R\u0303t(W ) only contains the binary rules for tree t."}, {"heading": "3.1 The Composition Tree Representation", "text": "We now focus our attention to the problem of modeling the sentence probability conditioned on the compositional tree i.e. p(W |t). Let t be the compositional tree shown in Figure 2. The factorization of w51 given t is\np(w51|t) =p(w51|w21, w53)p(w21|w1, w2) p(w53|w3w54)p(w54|w4, w5) p(w1)p(w2)p(w3)p(w4)p(w5). (8)\nWe now seek to represent any arbitrary tree t such that it can be factorized easily as we did in (8). We do this by representing the compositional tree t for a sentence W as a set of compositional rules and leaf nodes. Let us call this set Rt(W ). Using this abstraction, the rule set for the sentence w51 with compositional tree t, Rt(w51) is\nRt(w 5 1) = { w51 \u2190 w21 w53, w53 \u2190 w3 w54, w3w 5 4 \u2190 w4 w5, w4\nw21 \u2190 w1 w2, w1, w2 } . (9)\nWe now rewrite the factorization in (8) as\np(w51|t) = \u220f\nr\u2208Rt(w51)\np(r) (10)\nwhere p(pa\u2190 c1 c2) = p(pa|c1, c2). In more general terms, let t be the compositional tree for a sentence W . We can write the conditional probability p(W |t) as\np(W |t) = \u220f\nt\u2208Rt(W )\np(r). (11)"}, {"heading": "3.2 Computing the Sentence Probability", "text": "Using the definition of p(t) from (7) and the definition of p(W |t) from (11), we rewrite the joint probability p(W, t) as\np(W, t) = \u220f\nc\u2208Rt(W )\np(c) \u220f\nr\u2208R\u0303t(W )\n\u03b8r. (12)\nNow, as the compositional tree t is the same for both the production rule set R\u0303t(W ) and the compositional rule set Rt(W ), there is a one-to-one mapping between the binary rules in both the sets.\nAdding corresponding POS tags to phrase wji , we can merge both these sets. Let Rt(W ) be the merged set. The compositional rules in this new set can be re-written as\nA,wji \u2192 BC,w k i w j k+1. (13)\nUsing this new rule set, we can rewrite p(W, t) from (12) as\np(W, t) = \u220f\nr\u2208Rt(W )\n\u03b6r (14)\nand p(W ) from (5) as p(W ) = \u2211\nt\u2208T (W ) \u220f r\u2208Rt(W ) \u03b6r (15)\nwhere\n\u03b6r =\n{ p(r)\u03b8r binary rules\np(r) unary rules . (16)\nThe marginalization formulation in (15) is similar to one solved by the Inside algorithm.\nInside Algorithm: Let \u03c0(A,wji ) be the inside probability of A \u2208 N spanning wji . Using this definition, we can rewrite p(W ) in terms of the inside probability as\np(W ) = \u03c0(S,wn1 ). (17)\nWe can now recursively build \u03c0(S,wn1 ) using a dynamic programming (DP) based Inside algorithm in the following way:\nBase Case: For the unary rule, the inside probability \u03c0(A,wi) is the same as the production rule probability \u03b6A\u2192wi .\nRecursive Definition: Let \u03c0(B,wki ) and \u03c0(C,wjk+1) be the inside probabilities spanning wki rooted at B and w j k+1 rooted at C respectively. Let r = A,wji \u2192 BC,wki w j k+1 be the rule which composes wji from w k i and w j k+1, each one rooted at A, B and C respectively. The inside probability of rule r, \u03c0(r), can then be calculated as\n\u03c0(r) = \u03b6r\u03c0(B,w k i )\u03c0(C,w j k+1). (18)\nLet rA,i,k,j be the rule rooted at A spanning w j i splitting at k such that i < k < j. We can now calculate \u03c0(A,wji ) by summing over all possible splits between i, j.\n\u03c0(A,wji ) = \u2211 i\u2264k<j \u2211 rA,i,k,j\u2208R \u03c0(rA,i,k,j). (19)\nExamining equations (15) and (16), we see that we have reduced the problem of modeling p(W ) to modeling p(r), i.e. modeling the probability of compositional rules and leaf nodes. In the next section we carefully examine this problem. The approach we follow here is similar to the one taken in (Socher et al., 2010). We embed the words in a vocabulary V in a latent space of dimension d, use a compositional function f to build phrases in this latent space and then build a probability distribution function p over the leaf nodes and compositional rules."}, {"heading": "3.3 Modeling the compositional probability", "text": "The input to our model is an array of integers, each referring to the index of the word w in our vocabulary V . As a first step, we project each word into a continuous space using X , a d \u00d7 |V | embedding matrix, to obtain a continuous space vector xi = X[i] corresponding to word wi.\nA non terminal parent node pa is composed of children nodes c1 and c2 as\npa = f ( W [ c1 c2 ]) (20)\nwhere W is a parameter with dimensions d \u00d7 2d and f is a non-linear function like tanh or a sigmoid. The probability distribution p(r) over rule r \u2208 Rt(W ) is modeled as a Gibbs distribution\np(r) = 1\nZ exp {\u2212E(r)} (21)\nwhere E(r) is the energy for a compositional rule or a leaf node, and is modeled as\nE(r) = g(uT pa). (22)\nHere u is a scoring vector of dimension d\u00d71 and g is the identity function. From (20), (21) and (22), the parameters \u03b1 of p(r;\u03b1) are (u,X,W ).\nIn the next section we derive an approach for parameter estimation. We achieve this by formulating training as a maximum likelihood estimation problem and estimating \u03b1 by maximizing the probability over the training set D."}, {"heading": "4 Training", "text": "Let D be the set of training sentences. We can write the likelihood function as\nL(\u03b1;D) = \u220f\nWd\u2208D p(Wd;\u03b1). (23)\nThis leads to the negative log-likelihood objective function\nEML(\u03b1;D) = \u2212 \u2211 Wd\u2208D ln(p(Wd;\u03b1)). (24)\nSubstituting the definition of p(W ;\u03b1) from (5) and p(W, t;\u03b1) from (14) in (24), we get\nEML(\u03b1;D) = \u2212 \u2211 Wd\u2208D ln  \u2211 t\u2208T (Wd) \u220f r\u2208Rt(Wd) \u03b6r(\u03b1)  (25) The formulation in equation (25) is very similar to the standard expectation-maximization (EM) formulation where the compositional tree t can be seen as a latent variable."}, {"heading": "4.1 Expectation Step", "text": "In the E-step, we compute the expected loglikelihood Q(\u03b1;\u03b1old,W ) as follows.\nQ(\u03b1;\u03b1old,W ) = \u2212 \u2211\nt\u2208TG(W )\np(t|W ;\u03b1old) ln(p(t,W ;\u03b1)). (26)\nk w\nj\ni was\nused expand the subsequence to its left\nSubstituting p(t,W ) from (14) in (26), we can rewrite Q(\u03b1;W )2 as\nQ(\u03b1;W ) = \u2212 \u2211\nt\u2208TG(W )\np(t|W ) \u2211\nr\u2208Rt(W )\nln(\u03b6r(\u03b1)). (27)\nWe can simplify the expression further by taking summations over the trees inside leading to the following expression\nQ(\u03b1;W ) = \u2212 1 p(W ) \u2211 r\u2208R \u00b5(r) ln(\u03b6r(\u03b1)) (28)\nwhere \u00b5(r) = \u2211\nt\u2208TG(W ):r\u2208Rt(W )\np(t,W ). (29)\nThe term \u00b5(r) sums over all trees that contain rule r and can be calculated using the inside term \u03c0 and a new term\u2014the outside term \u03b2. Before computing \u00b5(r), let\u2019s examine how to compute this outside term \u03b2.\nOutside Algorithm: The Inside term \u03c0(A,wji ) is the probability of A \u2208 N spanning subsequence wji . The Outside term \u03b2(A,w j i ) is just the opposite. \u03b2(A,wji ) is the probability of expanding S to sentence wni such that sub-sequence wji rooted at A is left unexpanded. Similar to the inside probability, the outside probability can be calculated recursively as follows:\nBase Case: As the complete sentence is always rooted at S, \u03b2(S,wn1 ) is always 1. Moreover, as no other non-terminal A can be the root of the parse tree, \u03b2(A,wn1 ), A 6= S is zero.\n2Henceforth we drop the term \u03b1old in all our equations for the sake of brevity.\nRecursive Definition: To compute \u03b2(A,wji ) we need to sum up the probabilities both to the left and right of wji . Let\u2019s consider summing over the left side span wi\u221211 . Figure 3 shows one of the intermediate steps. Let \u03b2(rL) be the probability of expanding subsequence wi\u22121k rooted at B using rule rL = B,w j k \u2192 C A,w i\u22121 k w j i such that w j i rooted at A is left unexpanded. We can write \u03b2(rL) in terms of its parent\u2019s outside probability \u03b2(B,wjk), left sibling\u2019s inside probability \u03c0(C,wi\u22121k ) and \u03b6rL as\n\u03b2(rL) = \u03b6rL\u03c0(C,w i\u22121 k )\u03b2(B,w j k). (30)\nSimilarly, on the right hard side of wji , we can calculate \u03b2(rR) in terms of its parent\u2019s outside probability \u03b2(B,wki ), rule rR = B,w k i \u2192 AC,w j iw k j+1 probability \u03b6rR and inside probability of right sibling \u03c0(C,wkj+1) as\n\u03b2(rR) = \u03b6rR\u03c0(C,w k j+1)\u03b2(B,w k i ). (31)\nLet rA,k,i,j and rA,i,j,k be the rules spanning w j k andwki such thatA spanningw j i is its left and right child respectively. \u03b2(i, j, A) can then be calculated by summing rA,k,i,j and rA,i,j,k over all such rules and splits, i.e.\n\u03b2(A,wji ) = i\u22121\u2211 k=1 \u2211 rA,k,i,j\u2208R \u03b2(rA,k,i,j)\n+ n\u2211 k=j+1 \u2211 rA,i,j,k\u2208R \u03b2(rA,i,j,k). (32)\nNow, let\u2019s look at the definition of \u00b5(rA,i,k,j). Let rA,i,k,j be a rule rooted at A and span w j i . \u03c0(rA,i,k,j) contains probabilities of all the trees that uses production rule rA,i,k,j in their derivation. \u03b2(A,wji ) would contain the probability of everything except forA spanning wji . Hence, their product contains the probability of all parse trees that have rule rA,i,j in them, i.e. \u00b5(rA,i,j)."}, {"heading": "4.2 Minimization Step", "text": "In the M step, the objective is to minimize Q(\u03b1;\u03b1old) in order to estimate \u03b1\u2217 such that\n\u03b1? = argmin \u03b1 \u2212 \u2211 r\u2208R ln(\u03b6r(\u03b1))\u00b5(r) P (W ) . (33)\nSubstituting the definition of \u03b6r(\u03b1) and the value of p(r;\u03b1) from equation (21) and differentiating Q(\u03b1;\u03b1old) w.r.t. to \u03b1, we get\n\u2202Q \u2202\u03b1 = 1 P (W ) \u2211 r\u2208R { \u00b5(r) \u2202E(r;\u03b1) \u2202\u03b1 } . (34)\nUsing the definition of the energy function from (22), the partials \u2202E\u2202u , \u2202E \u2202W and \u2202E \u2202X are\n\u2202E(r;u,W,X)\n\u2202u = g\u2032(uT pa)pa, (35)\n\u2202E(r;u,W,X)\n\u2202W = g\u2032(uT pa)\n\u2202pa \u2202W , (36)\nand\n\u2202E(r;u,W,X)\n\u2202X = g\u2032(uT pa)\n\u2202pa \u2202X . (37)\nThe derivatives \u2202pa\u2202W and \u2202pa \u2202X can be recursively calculated as follows:\n\u2202pa/\u2202W: Base Case: For terminal node p, \u2202pa\u2202W is zero as there is no composition involved.\nRecursive Definition: Let \u2202c1\u2202W and \u2202c2 \u2202W be the partial derivatives of children c1 and c2 respectively. The derivative of parent embedding pa can then be built using \u2202c1\u2202W and \u2202c2 \u2202W as\n\u2202pa \u2202W = f \u2032\u00d7\n{ 1j \u25e6 [ c1 c2 ] +W [ \u2202c1 \u2202W \u2202c2 \u2202W ]} (38)\nwhere \u25e6 is the Hadamard product.\n\u2202pa/\u2202X: Base Case: Let xi be an embedding vector in X . For a terminal node p, there are two possibilities, either p is equal to xi or it is not. If p = xi, \u2202p\u2202xi is the identity matrix Id\u00d7d otherwise, it is zero.\nRecursive Definition: Let \u2202c1\u2202xi and \u2202c2 \u2202xi be the partial derivatives of children c1 and c2 respectively w.r.t. xi. The derivative of the parent embedding pa can be built using \u2202c1\u2202xi and \u2202c2 \u2202xi as\n\u2202pa \u2202xi = f \u2032 \u25e6W [ \u2202c1 \u2202xi \u2202c2 \u2202xi ] . (39)"}, {"heading": "4.3 Phrasal Representation", "text": "One of the inherent assumptions we made while using the Inside-Outside Algorithm was that each span has a unique representation. This is an important assumption because the states in the Inside and Outside algorithms are these spans. A\ndistributed representation breaks this assumption. To understand this, let\u2019s consider a three word sentence w31. Figure 4 shows possible derivations of this sentence. Embeddings for p{1{23}} and p{{12}3} are different as they follow different compositional paths despite both representing the same phrase w31. Generalizing this, any sentence or phrase of length 3 or greater would suffer from the multiple representation problem due to multiple possible compositional pathways.\nTo understand why this is an issue, let\u2019s examine the Inside algorithm recursion. Dynamic programming works while calculating \u03c0(A,wji ) because we assume that there is only one possible value for each of \u03c0(B,wki ) and \u03c0(C,w j k+1). As our compositional probability p(r) depends upon the phrase embeddings, so multiple possible phrase representations would mean multiple values for \u03b6r leading to multiple inside probabilities for each span. This can also be seen as breakage of the independence assumption of CFGs, as now, the probability of the parent node also depends upon how it\u2019s children were composed.\nWe restore the assumption by taking the expected value of phrasal embeddings w.r.t. its compositional inside probability \u03c0(wji \u2192 wki w j k+1) 3\nX(i, j) = E\u03c0[X(i, k, j)]. (40)\nWith this approximation, the phrasal embedding for w31 from Figure 4 is\nX(1, 3) = p{1{23}}\u03c0(w 3 1 \u2192 w1w32)+\np{{12}3}\u03c0(w 3 1 \u2192 w21w3). (41)\nThis representation is intuitive as well. If composition structures for a phrase leads to multi-\n3Inside probability of composition \u03c0(wji \u2192 w k i w j k+1) is\nInside rule probability \u03c0(A,wji \u2192 BC,w k i w j k+1) marginalized for all non terminals\nple representations, then the best way to represent that phrase would be an average representation weighted by the probability of each composition. Now, with the context free assumption restored, we can use the Inside-Outside algorithm for efficient marginalization and training.\nIn the above three sections, we have highlighted the inherent sequential tree assumption of the traditional models, proposed a compositional model and derived efficient algorithms to compute p(W ) and to train the parameters. In the next section, we look at how to evaluate our compositional model. For this we use a recently proposed discriminative metric Contrastive Entropy (Arora and Rangarajan, 2016) which doesn\u2019t not require the explicit computation of the partition function."}, {"heading": "5 Evaluation", "text": "The most commonly used metric for benchmarking language models is perplexity. Despite its widespread use, it cannot evaluate sentence level models like ours due to its word level model assumption and reliance on exact probabilities. A recently proposed discriminative metric Contrastive Entropy (Arora and Rangarajan, 2016) fits our evaluation use case perfectly. The goal of this new metric is to evaluate the ability of the model to discriminate between test sentences and their distorted version.\nThe Contrastive Entropy, HC , is defined as the difference between the entropy of the test sentence Wn, and the entropy of the distorted version of the test sentences W\u0302n i.e.\nHC(D; d) = 1\nN \u2211 Wn\u2208D H(W\u0302n; d)\u2212H(Wn) (42)\nHereD is the test set, d is the measure of distortion andN , the number of words or sentences for word level or sentence level models respectively.\nAs this measure is not scale invariant, we also report Contrastive Entropy Ratio HCR w.r.t. a baseline distortion level db i.e.\nHCR(D; db, d) = HC(D; d)\nHC(D; db) . (43)"}, {"heading": "5.1 Results", "text": "We use the example dataset provided with the RNNLM toolkit (Mikolov et al., 2011b) for evaluation purposes. The dataset is split into training, testing and validation set of sizes 10000, 1000 and\n1000 sentences respectively. The training set contains 3720 different words and the test set contains 206 vocabulary words. All reported values here are averaged over 10 runs.\nFigure 5 shows the monotonic increase in contrastive perplexity as the test distortion level increases. This is in line with hypothesis that the discriminative ability of a language model should increase with the test set distortion levels.\nTable 1 compares our language model to standard language modeling techniques. The n-gram language models here use Kneser Ney (KN5) smoothing and were generated and evaluated using the SRILM toolkit (Stolcke, 2002). The recurrent neural network language model (RNN) has a hidden layer of size 400 and was generated using the RNNLM toolkit. The compositional language models (CLM) in Table 1 have latent space size of 25 and 50 and were trained using Adagrad (Duchi et al., 2011) with an initial learning rate of 1 and `2 regularization coefficient of 0.1. CLMs show more than 100% improvement over RNNLM, the best performing baseline model, across all distortion levels. Table 2 confirms that CLM outperforms all the baseline models on entropy ratio and isn\u2019t impacted by scaling issues."}, {"heading": "6 Conclusion", "text": "In this paper we challenged the linear chain assumption of the traditional language models by building a model that uses the compositional structure endowed by context free grammars. We formulated it as a marginalization problem over the joint probability of sentences and structure and reduced it to one of modeling compositional rule probabilities p(r). To the best of our knowledge, this is the first model that looks beyond the linear chain assumption and uses the compositional structure to model the language. It is important to note that this compositional framework is much more general and the way this paper models p(r) is only one of many possible ways to do so.\nAlso, this paper proposed a compositional framework that recursively embeds phrases in a latent space and then builds a distribution over it. This provides us with a distributional language representation framework which, if trained properly, can be used as a base for various language processing tasks like NER, POS tagging and sentiment analysis. The assumption here is that most of the heavy lifting will be done by the representation and a simple classifier should be able to give good results over the benchmarks. We also hypothesize that phrasal embeddings generated using this model will be much more robust and will also exhibit interesting regularities due to the marginalization over all possible structures.\nAs the likelihood optimization proposed here is highly non linear, better initialization, and improved optimization and regularization techniques being developed for deep architectures can further improve these results. Another area of research is to study the effects of the choice of compositional functions and additional constraints on representation generated by the model and finally the performance of the classification layer built on top."}], "references": [{"title": "Contrastive entropy: A new evaluation metric for unnormalized language models. arXiv preprint arXiv:1601.00248", "author": ["Arora", "Rangarajan2016] Kushal Arora", "Anand Rangarajan"], "venue": null, "citeRegEx": "Arora et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Arora et al\\.", "year": 2016}, {"title": "Distributional clustering of words for text classification", "author": ["Baker", "McCallum1998] L Douglas Baker", "Andrew Kachites McCallum"], "venue": "In Proceedings of the 21st annual international ACM SIGIR conference on Research and development in in-", "citeRegEx": "Baker et al\\.,? \\Q1998\\E", "shortCiteRegEx": "Baker et al\\.", "year": 1998}, {"title": "Neural probabilistic language models", "author": ["Bengio et al.2006] Yoshua Bengio", "Holger Schwenk", "Jean-S\u00e9bastien Sen\u00e9cal", "Fr\u00e9deric Morin", "JeanLuc Gauvain"], "venue": "In Innovations in Machine Learning,", "citeRegEx": "Bengio et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Bengio et al\\.", "year": 2006}, {"title": "Class-based n-gram models of natural language", "author": ["Brown et al.1992] Peter F Brown", "Peter V Desouza", "Robert L Mercer", "Vincent J Della Pietra", "Jenifer C Lai"], "venue": null, "citeRegEx": "Brown et al\\.,? \\Q1992\\E", "shortCiteRegEx": "Brown et al\\.", "year": 1992}, {"title": "Immediatehead parsing for language models", "author": ["Eugene Charniak"], "venue": "In Proceedings of the 39th Annual Meeting on Association for Computational Linguistics,", "citeRegEx": "Charniak.,? \\Q2001\\E", "shortCiteRegEx": "Charniak.", "year": 2001}, {"title": "Structured language modeling", "author": ["Chelba", "Jelinek2000] Ciprian Chelba", "Frederick Jelinek"], "venue": "Computer Speech & Language,", "citeRegEx": "Chelba et al\\.,? \\Q2000\\E", "shortCiteRegEx": "Chelba et al\\.", "year": 2000}, {"title": "Structure and performance of a dependency language model", "author": ["David Engle", "Frederick Jelinek", "Victor Jimenez", "Sanjeev Khudanpur", "Lidia Mangu", "Harry Printz", "Eric Ristad", "Ronald Rosenfeld", "Andreas Stolcke"], "venue": null, "citeRegEx": "Chelba et al\\.,? \\Q1997\\E", "shortCiteRegEx": "Chelba et al\\.", "year": 1997}, {"title": "Adaptive subgradient methods for online learning and stochastic optimization", "author": ["Duchi et al.2011] John Duchi", "Elad Hazan", "Yoram Singer"], "venue": "The Journal of Machine Learning Research,", "citeRegEx": "Duchi et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Duchi et al\\.", "year": 2011}, {"title": "A bit of progress in language modeling", "author": ["Joshua T Goodman"], "venue": "Computer Speech & Language,", "citeRegEx": "Goodman.,? \\Q2001\\E", "shortCiteRegEx": "Goodman.", "year": 2001}, {"title": "A cache-based natural language model for speech recognition", "author": ["Kuhn", "De Mori1990] Roland Kuhn", "Renato De Mori"], "venue": "Pattern Analysis and Machine Intelligence, IEEE Transactions", "citeRegEx": "Kuhn et al\\.,? \\Q1990\\E", "shortCiteRegEx": "Kuhn et al\\.", "year": 1990}, {"title": "The estimation of stochastic context-free grammars using the inside-outside algorithm", "author": ["Lari", "Young1990] Karim Lari", "Steve J Young"], "venue": "Computer speech & language,", "citeRegEx": "Lari et al\\.,? \\Q1990\\E", "shortCiteRegEx": "Lari et al\\.", "year": 1990}, {"title": "Trigger-based language models: A maximum entropy approach", "author": ["Lau et al.1993] Raymond Lau", "Ronald Rosenfeld", "Salim Roukos"], "venue": null, "citeRegEx": "Lau et al\\.,? \\Q1993\\E", "shortCiteRegEx": "Lau et al\\.", "year": 1993}, {"title": "Recurrent neural network based language model", "author": ["Martin Karafi\u00e1t", "Lukas Burget", "Jan Cernock\u1ef3", "Sanjeev Khudanpur"], "venue": "In INTERSPEECH,", "citeRegEx": "Mikolov et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Mikolov et al\\.", "year": 2010}, {"title": "Extensions of recurrent neural network language model", "author": ["Stefan Kombrink", "Lukas Burget", "JH Cernocky", "Sanjeev Khudanpur"], "venue": "In Acoustics, Speech and Signal Processing (ICASSP),", "citeRegEx": "Mikolov et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Mikolov et al\\.", "year": 2011}, {"title": "RNNLM-recurrent neural network language modeling toolkit", "author": ["Stefan Kombrink", "Anoop Deoras", "Lukar Burget", "Jan Cernocky"], "venue": "In Proc. of the 2011 ASRU Workshop,", "citeRegEx": "Mikolov et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Mikolov et al\\.", "year": 2011}, {"title": "A scalable hierarchical distributed language model", "author": ["Mnih", "Hinton2009] Andriy Mnih", "Geoffrey E Hinton"], "venue": "In Advances in neural information processing systems,", "citeRegEx": "Mnih et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Mnih et al\\.", "year": 2009}, {"title": "Hierarchical probabilistic neural network language model", "author": ["Morin", "Bengio2005] Frederic Morin", "Yoshua Bengio"], "venue": "In AISTATS,", "citeRegEx": "Morin et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Morin et al\\.", "year": 2005}, {"title": "Distributional clustering of english words", "author": ["Naftali Tishby", "Lillian Lee"], "venue": "In Proceedings of the 31st annual meeting on Association for Computational Linguistics,", "citeRegEx": "Pereira et al\\.,? \\Q1993\\E", "shortCiteRegEx": "Pereira et al\\.", "year": 1993}, {"title": "Learning continuous phrase representations and syntactic parsing with recursive neural networks", "author": ["Christopher D Manning", "Andrew Y Ng"], "venue": "In Proceedings of the NIPS-2010 Deep Learning and Unsupervised", "citeRegEx": "Socher et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Socher et al\\.", "year": 2010}, {"title": "Parsing with compositional vector grammars", "author": ["John Bauer", "Christopher D Manning", "Andrew Y Ng"], "venue": "Proceedings of the ACL conference. Citeseer", "citeRegEx": "Socher et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Socher et al\\.", "year": 2013}, {"title": "Srilm-an extensible language modeling toolkit. In INTERSPEECH", "author": ["Andreas Stolcke"], "venue": null, "citeRegEx": "Stolcke.,? \\Q2002\\E", "shortCiteRegEx": "Stolcke.", "year": 2002}], "referenceMentions": [{"referenceID": 8, "context": "A good overview of various smoothing techniques and their relative performance on language modeling tasks can be found in (Goodman, 2001).", "startOffset": 122, "endOffset": 137}, {"referenceID": 3, "context": "Class based models (Brown et al., 1992; Baker and McCallum, 1998; Pereira et al., 1993) try to solve the generalization issue by deterministically or probabilistically mapping words to one or multiple classes based on manually designed or probabilistic criteria.", "startOffset": 19, "endOffset": 87}, {"referenceID": 17, "context": "Class based models (Brown et al., 1992; Baker and McCallum, 1998; Pereira et al., 1993) try to solve the generalization issue by deterministically or probabilistically mapping words to one or multiple classes based on manually designed or probabilistic criteria.", "startOffset": 19, "endOffset": 87}, {"referenceID": 11, "context": "The issue of longer context dependencies has been addressed using various approximations such as cache models (Kuhn and De Mori, 1990), trigger models (Lau et al., 1993) and structured language models (Charniak, 2001; Chelba et al.", "startOffset": 151, "endOffset": 169}, {"referenceID": 4, "context": ", 1993) and structured language models (Charniak, 2001; Chelba et al., 1997; Chelba and Jelinek, 2000).", "startOffset": 39, "endOffset": 102}, {"referenceID": 6, "context": ", 1993) and structured language models (Charniak, 2001; Chelba et al., 1997; Chelba and Jelinek, 2000).", "startOffset": 39, "endOffset": 102}, {"referenceID": 2, "context": "ward neural network based models (Bengio et al., 2006; Mnih and Hinton, 2009; Morin and Bengio, 2005) embed the concatenated n-gram history in this latent space and then use a softmax layer over these embeddings to predict the next word.", "startOffset": 33, "endOffset": 101}, {"referenceID": 12, "context": "Recurrent neural network based models (Mikolov et al., 2011a; Mikolov et al., 2010) attempt to address this by recursively embedding history in the latent space, predicting the next word based on it and then updating the history with the word.", "startOffset": 38, "endOffset": 83}, {"referenceID": 19, "context": "Instead of approximating the marginalization using the n-best trees as in (Socher et al., 2013), we restore this independence assumption by averaging over phrasal representations leading to a single phrase representation.", "startOffset": 74, "endOffset": 95}, {"referenceID": 18, "context": "The approach we follow here is similar to the one taken in (Socher et al., 2010).", "startOffset": 59, "endOffset": 80}, {"referenceID": 20, "context": "The n-gram language models here use Kneser Ney (KN5) smoothing and were generated and evaluated using the SRILM toolkit (Stolcke, 2002).", "startOffset": 120, "endOffset": 135}, {"referenceID": 7, "context": "The compositional language models (CLM) in Table 1 have latent space size of 25 and 50 and were trained using Adagrad (Duchi et al., 2011) with an initial learning rate of 1 and `2 regularization coefficient of 0.", "startOffset": 118, "endOffset": 138}], "year": 2016, "abstractText": "Traditional language models treat language as a finite state automaton on a probability space over words. This is a very strong assumption when modeling something inherently complex such as language. In this paper, we challenge this by showing how the linear chain assumption inherent in previous work can be translated into a sequential composition tree. We then propose a new model that marginalizes over all possible composition trees thereby removing any underlying structural assumptions. As the partition function of this new model is intractable, we use a recently proposed sentence level evaluation metric Contrastive Entropy to evaluate our model. Given this new evaluation metric, we report more than 100% improvement across distortion levels over current state of the art recurrent neural network based language models.", "creator": "LaTeX with hyperref package"}}}