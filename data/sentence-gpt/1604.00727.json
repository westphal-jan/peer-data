{"id": "1604.00727", "review": {"conference": "EMNLP", "VERSION": "v1", "DATE_OF_SUBMISSION": "4-Apr-2016", "title": "Character-Level Question Answering with Attention", "abstract": "We show that an encoder-decoder framework can be successfully be applied to question-answering with a structured knowledge base. In addition, we propose a new character-level modeling approach for this task, which we use to make our model robust to unseen entities and predicates. We use our model for single-relation question answering, and demonstrate the effectiveness of our novel approach on the SimpleQuestions dataset, where we improve state-of-the-art accuracy by 2% for both Freebase2M and Freebase5M subsets proposed. Importantly, we achieve these results even though our character-level model has 16x less parameters than an equivalent word-embedding model, uses significantly less training data than previous work which relies on data augmentation, and encounters only 1.18% of the entities seen during training when testing. To learn more about how we can use models to build our model, you can read more about why we built this model in our earlier work.\n\n\n\nThe first question we have asked is what kind of training is needed in this domain? When we ask questions, the answer is often simple:\n\u200f\u200f\u200f\u200f\u200f\u200f\u200f\u200f\u200f\u200f\u200f\u200f\u200f\u200f\u200f\u200f\u200f\u200f\u200f\u200f\u200f\u200f\u200f\u200f\u200f\u200f\u200f\u200f\u200f\u200f\u200f\u200f\u200f\u200f\u200f\u200f\u200f\u200f\u200f\u200f\u200f\u200f\u200f\u200f\u200f\u200f\u200f\u200f\u200f\u200f\u200f\u200f\u200f\u200f\u200f\u200f\u200f\u200f\u200f\u200f\u200f\u200f\u200f\u200f\u200f\u200f\u200f\u200f\u200f\u200f\u200f\u200f\u200f\u200f\u200f\u200f\u200f\u200f\u200f\u200f\u200f\u200f\u200f\u200f\u200f\u200f\u200f\u200f\u200f\u200f\u200f\u200f\u200f\u200f\u200f\u200f\u200f\u200f\u200f\u200f\u200f\u200f\u200f\u200f\u200f\u200f\u200f\u200f\u200f\u200f\u200f\u200f\u200f\u200f\u200f\u200f\u200f\u200f\u200f\u200f\u200f\u200f\u200f\u200f\u200f\u200f\u200f\u200f\u200f\u200f\u200f\u200f\u200f\u200f\u200f\u200f\u200f\u200f\u200f\ufffd", "histories": [["v1", "Mon, 4 Apr 2016 02:43:23 GMT  (646kb,D)", "http://arxiv.org/abs/1604.00727v1", null], ["v2", "Tue, 5 Apr 2016 23:09:31 GMT  (647kb,D)", "http://arxiv.org/abs/1604.00727v2", null], ["v3", "Fri, 8 Apr 2016 21:12:47 GMT  (647kb,D)", "http://arxiv.org/abs/1604.00727v3", null], ["v4", "Sun, 5 Jun 2016 02:02:10 GMT  (9011kb,D)", "http://arxiv.org/abs/1604.00727v4", null]], "reviews": [], "SUBJECTS": "cs.CL cs.AI cs.LG", "authors": ["xiaodong he", "david golub"], "accepted": true, "id": "1604.00727"}, "pdf": {"name": "1604.00727.pdf", "metadata": {"source": "CRF", "title": "Character-Level Question Answering with Attention", "authors": ["David Golub", "Xiaodong He"], "emails": ["golubd@cs.washington.edu", "xiaohe@microsoft.com"], "sections": [{"heading": "1 Introduction", "text": "Open-domain question answering remains a challenging and important problem that is largely unsolved. In this paper we focus on single-relation factoid questions, which are one of the most common forms of questions found in search query logs and community question answering websites (Yih et al., 2014), (Fader et al., 2013).\nWe assume that single-relation questions can be answered by querying a knowledge base such as Freebase with a single entity and predicate argument. For instance, the question \u201cWhen was Barack Obama born?\u201d can be answered by issuing the query:\n\u03bb(x).place of birth(Barack Obama, x)\nAdditional examples of questions in this Domain are \u201cWhere is Buckingham Palace located?\u201d and \u201cWhat is the language of France?\u201d.\nEven in the simpler domain of single-relation question answering, which does not include questions with multiple relations, such as \u201cWhen were Barack Obama\u2019s children born?\u201d, the mapping from natural language to a structured knowledgebase query remains non-trivial. First, there are many paraphrases of the same question. Second, even in a small knowledge base such as Freebase,\nar X\niv :1\n60 4.\n00 72\n7v 1\n[ cs\n.C L\n] 4\nA pr\n2 01\n6\nthere are millions of unique entities and thousands of predicates that can be used to construct a query, posing additional challenges for learning.\nTo address these challenges, we propose the use of an encoder-decoder framework with attention, which has already been successfully applied to machine translation, speech recognition, and image captioning (Bahdanau et al., 2014), (Amodei et al., 2015), (Xu et al., 2015). An encoder builds a compact representation for a question and can handle the paraphrase issue.\nMoreover, to make our model robust to unseen knowledge base entries and words, we extract high-level embeddings for questions, predicates, and entities purely from their characterlevel representations. During decoding, we use a general interaction function between these embeddings that measures their semantic relevance. The combined use of character-level modeling and a semantic relevance function allows us to successfully produce likelihood scores for entities and predicates that are not present in our vocabulary, a challenging task for standard encoder-decoder frameworks.\nOur novel, character-level encoder-decoder system is compact, requires significantly less data than previous work, and is able to generalize well to unseen entities during test time. In particular, we achieve 66.1% accuracy using Freebase2M and 65.7% accuracy using Freebase5M on the SimpleQuestions dataset, which beat the previous stateof-art of 63.9% by 2.2% and 1.7% respectively, even though we encounter less than 2% of the entities seen during training when testing. Moreover, in contrast to previous work, we only use the training questions provided to teach our model, without the use of any additional data."}, {"heading": "2 Related Work", "text": "Our work builds upon three major threads of research in machine learning: semantic-parsing for open-domain question answering, character-level modeling, and sequence to sequence learning.\nSemantic parsing for open-domain question answering, which translates a question into a structured knowledge-base query, is a key component in question answering with a knowledge base such as Freebase. While early approaches relied on building high-quality lexicons for domain-specific databases such as GeoQuery (Tang and Mooney, 2001), recent work has focused on building se-\nmantic parsing frameworks for general knowledge bases such as Freebase, (Yih et al., 2014), (Bordes et al., 2014), (Bordes et al., 2015), (Berant and Liang, 2014), (Fader et al., 2013).\nSemantic parsing frameworks for large-scale knowledge bases have to be able to successfully generate queries for the millions of entities and thousands of predicates in the KB, many of which are unseen during training. To address this issue, recent work relies on producing high-level embeddings for predicates and entities in a knowledge base based off of their textual descriptions, (Bordes et al., 2014), (Bordes et al., 2015), (Yih et al., 2014), (Yih et al., 2015). A general interaction function can then be used to measure the semantic similarity of the embeddings of knowledge base entries and the question, to produce a final likelihood score that can be used for query generation.\nHowever, these approaches still suffer from a large-scale vocabulary problem because they use word-level embeddings to encode entities and predicates. Consequently, they often rely on significant data augmentation from sources such as Paralex (Fader et al., 2013), which contains 18 million question-paraphrase pairs scraped from WikiAnswers, to have sufficient examples for each word they encounter.\nAs opposed to word-level modeling, characterlevel modeling can be used to handle the largescale vocabulary issue. While character-level modeling has never been applied to factoid question-answering, it has been successfully applied to sentiment analysis, classification, and named entity recognition (Zhang and LeCun, 2015), (Chung et al., 2015), (Klein et al., 2003). Moreover, in (Chung et al., 2015) the authors show that gated-feedback LSTMs on top of on character-level embeddings can capture long-term dependencies in language modeling.\nLastly, encoder-decoder methods can address the paraphrase issue commonly found in questionanswering. First introduced in (Sutskever et al., 2014), in an encoder-decoder network, a source sequence is first encoded into a fixed-length vector which intuitively captures \u201cmeaning\u201d, and then decoded into a desired target sequence. This approach has been used successfully in diverse domains such as machine translation, image captioning, and executing programs (Venugopalan et al., 2014), (Bahdanau et al., 2014), (Xu et al., 2015), (Zaremba and Sutskever, 2014).\nIn our work we intend to combine the proposed approaches from these three different areas of machine learning for the task of single-relation question-answering."}, {"heading": "3 Model", "text": "Given a question, the goal of our model is to generate a structured knowledge base query that can be used to retrieve the answer. In other words, our model assigns a conditional probability of predicting the next predicate or entity yt given an input question q and all the previously predicted knowledge base entries y0, \u00b7 \u00b7 \u00b7 , yt\u22121.\nIt is important to note that since we focus single-relation question answering in this work, our model decodes every question into exactly 2 outputs\u2013the source entity, and the predicate. Consequently, the goal of our model is to generate a single topic entity and a predicate from Freebase, such as (Barack Obama, /people/person/place of birth), which can then be used to retrieve the fact.\nOur end-to-end framework consists of two components. Like (Bordes et al., 2015), we have an approximate entity linking system that prunes the list of possible predicates and entities our model has to evaluate, and a prediction system which determines the most likely (entity, predicate) pair from the filtered list.\nWe use our encoder-decoder network two times in our end-to-end framework, to determine which part of the question to send to our entity-linking system and to assign a likelihood score to each (predicate, entity) tuple.\nWe will first describe how we represent entities and predicates from a structured knowledge-base, followed by our prediction system, and finally our entity-linking system."}, {"heading": "3.1 Representing Predicates and Entities", "text": "In a knowledge such as Freebase, entities are typically annotated with rich information such as descriptions, aliases in different languages, and types. However, to keep the experimental setup consistent with previous work such as (Bordes et al., 2015) and (Yih et al., 2014), we generate highlevel embeddings for each predicate and entity in Freebase using only their English aliases. For instance, to generate an embedding for the Freebase entity /m/02mjmr, we would take the characters of its English alias Barack Obama and feed it\nthrough our system. However, we do incorporate additional information from Freebase such as entity descriptions for our linking system to improve overall precision, like previous work in this area (Ling et al., 2015), (Lin et al., 2012), (Liang et al., 2013)."}, {"heading": "3.2 Prediction System", "text": "Our end-to-end prediction system is a function f(q, e1..n, p1..m) that takes as input a question q, a list of candidate entities e1..n, a list of candidate predicates p1..m and produces a likelihood score p(ei, pk|q) of seeing entity i and predicate j given q for all i \u2208 1..n, j \u2208 1..m. It consists of four components:\n1. An encoder for the question which produces an embedding vector for each character (Figure 1a).\n2. An encoder for the predicates/entities in a knowledge base which produces a single embedding vector of a fixed size N. (Figure 2)\n3. An LSTM with attention mechanisms whose output (Figure 1e) is also a single vector of size N (Figure 1d).\n4. A semantic relevance layer on top of the decoder LSTM to rerank the predicates on the first time step and entities on the second time step with respect to their distance to the LSTM output (Figure 1f). We use cosine similarity for our interaction function, as illustrated in Figure 2."}, {"heading": "3.3 Embeddings", "text": "To produce our high-level embeddings, we first have a one-hot encoding for each character of entities, predicates and questions. If we let the embedding matrix be E and the characters in a string be y1..n the output embedding for the ith character is E \u00d7 yi, where yi is a one-hot encoding for the respective character. We learn separate embedding matrices Ee, Ep, Eq for entity, predicate, and question characters respectively. This helps our model capture the fact that different characters may have different meanings depending on whether they come from an entity, predicate or a question."}, {"heading": "3.4 Question representation", "text": "We feed the embeddings of the question into a two-layered gated-feedback LSTM as described in\n(Chung et al., 2015) (Figure 1a) and take the outputs at each time step as the final embeddings for the question. These are the vectors h1...hn in Figure 1, and are the contexts for our attention LSTM."}, {"heading": "3.5 Predicate/Entity representation", "text": "To produce the latent semantic representations for the entity/predicates, we use a two-layer convolutional neural network (CNN) over character-level embeddings followed by a max-pooling layer, similarly to (Yih et al., 2014). The overall meaning of a predicate or entity is often determined by a few key words in its English alias, and the CNN + max pooling layer construction allows us to extract the most salient local features while forming a global feature vector of constant length.\nAs illustrated in Figure 2, we feed the character-level embeddings for all candidate entities/predicates through a temporal convolutional neural network:\nf(x1...n) = tanh(W3 \u2217max(tanh(W2\u2217 conv(tanhW1 \u2217 conv(x1..n)))))\nWhere n is the number of characters, f(x1...n) has size N , W3 has size RN\u2217h, conv represents a temporal convolutional neural network, max represents a max pooling layer in the temporal direction, and xi represents the embedding of the ith character of the entity/predicates."}, {"heading": "3.6 Attention-based LSTM", "text": "For our attention-based LSTM we use the same architecture as described in (Bahdanau et al., 2014) (Figure 1d). We find that the inclusion of an attention mechanism slightly boosts performance over mean pooling, and allows us to see that the softalignments produced by our model are reasonable.\nAt each time step, we feed in a context vector (Figure 1c) and an input into the LSTM (Figure 1g). To produce the context vector, an attention mechanism is used over the question embeddings (Figure 1b).\nLet si\u22121 be the hidden state of the LSTM at time i\u2212 1, and hj be the jth question character embedding. Then the context vector ci which represents the entire question is recomputed at each step by the alignment model as follows:\nci = Tx\u2211 j=1 \u03b1ij \u25e6 hj ,\nwhere\n\u03b1ij = exp (eij)\u2211Tx k=1 exp (eik)\neij =v > a tanh (Wasi\u22121 + Uahj) ,\naij are the attention vectors (Figure 1b) that are applied over each hidden unit hj in our diagram. To have fine-grained attention over each dimension of each hidden unit, va \u2208 Rn\u00d7n \u2032 . In contrast, to va \u2208 R1\u00d7n \u2032\nfor coarse-grained attention. For fine-grained attention with dropout k, each va \u2208 Rn\u00d7n \u2032 has a probability k of being zero. Wa \u2208 Rn \u2032\u00d7n and Ua \u2208 Rn\n\u2032\u00d7n are weight matrices. Lastly, for mean-pooling our attention coefficient becomes va = ~1.\nIn addition to the context vector, we also feed in an input into our LSTM at each time step (Figure 1g). As illustrated in Figure 1, at time t = 1 we feed a special input x\u2217 into our attention LSTM. We experiment with x\u2217 as both a learnable parameter and as a constant ~0. At time t = 2 we feed the\ninput ycorr during training and ypred during testing, where ycorr is the high-level embedding of the correct predicate, and ypred is the embedding of the predicted predicate at the previous time step. The embeddings, y, z are the outputs of a convolutional neural network, as illustrated in Figure 2."}, {"heading": "3.7 Prediction", "text": "Figure 2 provides an illustration of how our model generates predictions. Letting the new hidden states of the attention LSTM at times 1, 2 be o1 and o2 respectively (Figure 1e), we want to maximize the following values:\nexp(\u03bb \u2217 cos(o1, yc))\u2211n i=1 exp(\u03bb \u2217 cos(o1, yi)) \u2217 exp(\u03bb \u2217 cos(o2, zc))\u2211n i=1 exp(\u03bb \u2217 cos(o2, zi))\nwhere \u03bb is a constant and c is the index of the correct predicate/entity. A similar optimization objective was used to train the semantic similarity modules proposed in (Yih et al., 2014) and (Yih et al., 2015). During prediction time yi and zi are all of the entities and predicates returned by our entity-linking system, while during train time, they are randomly-sampled entities and predicates from the training set.\nFor our loss function we use a cross-entropy criterion, and we use back-propagation to propagate the errors back to all of the weights in our model."}, {"heading": "3.8 Entity Linking", "text": "For entity linking, we experiment with the official Freebase API, as well as an approximate entity linking system we build on top of the Freebase5M/Freebase2M subsets we build similar to (Bordes et al., 2015). For the FreebaseAPI we split the question into 1-5 ngrams and retrieve all facts where the topic entity matches any of the question ngrams. For the Freebase5M and Freebase2M subsets we store all entity English names and descriptions in a full-text search system and retrieve a subset of triples whose topic entity\u2019s English name or description matches a query. The triples are ordered according to tf-idf scores.\nTo generate our queries for the Freebase5M/2M subsets we only feed in a single question ngram into our entity-linking system. Specifically, we take all ngrams e3..6 of length 3-6 of the question, a dummy predicate p = \u201cfoo\u201d, the raw question, q, and feed them through our prediction system f(q, e3..6, p). We then take the ngram which our model model predicts is the most likely an entity, e\u2217 = argmaxe\u2208e3..6p(e, p|q), and use that as our query. Table 1 contains instances of generated queries."}, {"heading": "4 Dataset and Experimental Settings", "text": "We evaluate our models on the SimpleQuestions dataset (Bordes et al., 2015) which consists of 108,442 single-relation questions and their corresponding source entity, predicate, target entity triples from Freebase. The dataset is split into 21,687 test, 10,845 validation and 75,910 train questions respectively. Only 216 out of 18363 unique entities seen during testing were seen during training, and 886 out of 1034 predicates."}, {"heading": "4.1 Preprocessing and setup", "text": "We use no data augmentation and only use the 76k questions for training, and 21k questions for testing. We use RMSProp with a learning rate of 1e\u22124, character-level embeddings of size 200, and all of our LSTMs have hidden/memory states of size 100. We do not use any regularization, and we train our models on a standard Telsa gpu for 4-7 days and 19-30 epochs. During decoding we generate the predicate at the first time step, and the entity at the second time step."}, {"heading": "5 Results", "text": ""}, {"heading": "5.1 End-to-end results on SimpleQuestions", "text": "We achieve state of the art results on SimpleQuestions in experiments where we use either the FB2M or the FB5M subsets proposed, with accuracies of 66.2% and 65.7% respectively. Moreover, our character-level model is compact and has\nResults Raw question Reranked\n1.2M parameters as opposed to the 19.9M parameters of our word-embedding model, a 16x reduction in size. We also find that using our model to filter out candidate ngrams to send to our entity linking system boosts precision significantly, with a 12% boost at top-1 result, and 6% at top-50 results. Our results are summarized in Table 2, Table 3, and we provided example generated queries in Table 1.\nOur experimental results are slightly worse when we use FB5M as our knowledge base because all the test questions are sampled from FB2M. This means that when we use FB5M as our knowledge base, our model has more unnecessary entities to filter out."}, {"heading": "5.2 Ablation and word-embedding experiments", "text": "For our experiments in sections 5.3 and 5.4, we randomly sample 200 entities and predicates from\nthe test set. We then insert the correct entity, predicate into these noise sets, and determine the accuracy of our model in predicting the gold predicate/entity from this combined set."}, {"heading": "5.3 Performance of word-embedding models", "text": "In this experiment we explore using one-hot wordlevel encodings as an alternative to character level models to construct our high-level embeddings.\nOur results with models that have various attention mechanisms and character vs. word-level embeddings are summarized in Table 5 and Figure 3. We find that word-embeddings models have considerable difficulty generalizing to unseen entities, and after 11 epochs are only able to predict 40% of the entities accurately from the noise set, with a final joint accuracy of 34.01%. This demonstrates the difficulty word-embedding models have generalizing to unseen entities, as many entities are represented by words that may be rarely seen during training.\nIn contrast, character-level models have no such issues, and achieve a 96.25% accuracy in predicting the correct entity during negative sampling experiments. This empirically supports the hypothesis that character-level encoding models capture the semantic similarity between entity aliases in a knowledge base and how they are referenced to in natural language."}, {"heading": "5.4 Depth ablation study", "text": "In this experiment we perform an ablation study to determine whether it is necessary to use deeper models to produce high-level embeddings from character-level representations. Our results are summarized in Figure 4 and Table 6. In our ablation experiments we compare the effectiveness\nof using a single-layer LSTM vs. a multi-layer gated-feedback LSTM to encode the question, and a single-layer vs. two-layer convolutional neural network to encode the predicates/entities. We find that a 2-layered LSTM boosts joint accuracy by over 6%. The majority of accuracy gains come from improved predicate prediction, possibly because they require a more complicated alignment mechanism than entities do."}, {"heading": "5.5 Attention Mechanisms", "text": "We perform experiments with four different attention mechanisms\u2013mean pooling, fine-grained pooling, fine-grained pooling with dropout, and\ncoarse-grained pooling. Our results are summarized in Figure 3, Tables 4 and 5. During our negative sampling experiments, we find that models with coarse-grained attention slightly outperform those with mean pooling, which outperform those with fine-grained pooling and dropout. We postulate this is because fine-grained attention is harder to optimize, in spite of the fact that it gives an LSTM more control over what parts of the question to attend to.\nWe also visualize our attention over question characters for coarse-grained attention models. As illustrated in Table 4, we find that coarse-grained attention is able to learn where the predicates and entities are in a question. Attention typically peaks on the space after a sequence of words that correspond to a predicate or entity in a knowledge base. This demonstrates that a character-level LSTM has been able to infer that \u201cspace\u201d is a special character that separates words, and is able to learn meaningful representations on the word level, even though it reads in questions character by character."}, {"heading": "6 Summary", "text": "In conclusion, we proposed a new characterlevel, attention-based encoder-decoder model for question-answering with a structured knowledge base and demonstrated the promise of our approach on single-relation question answering, where we achieved state-of-the-art results on the SimpleQuestions dataset. Moreover, our compact model generalized well to unseen entities, in spite of the fact that we only used significantly less training data than previous work. In the future we would like to extend our model to handle multi-relation questions and use our characterlevel encoding to naturally handle spelling mistakes, which often occur in practice."}], "references": [{"title": "Deep speech 2: Endto-end speech recognition in english and mandarin", "author": ["Zhenyao Zhu"], "venue": null, "citeRegEx": "Zhu.,? \\Q2015\\E", "shortCiteRegEx": "Zhu.", "year": 2015}, {"title": "Neural machine translation by jointly learning to align and translate", "author": ["Dzmitry Bahdanau", "Kyunghyun Cho", "Yoshua Bengio."], "venue": "CoRR, abs/1409.0473.", "citeRegEx": "Bahdanau et al\\.,? 2014", "shortCiteRegEx": "Bahdanau et al\\.", "year": 2014}, {"title": "Semantic parsing via paraphrasing", "author": ["Jonathan Berant", "Percy Liang"], "venue": null, "citeRegEx": "Berant and Liang.,? \\Q2014\\E", "shortCiteRegEx": "Berant and Liang.", "year": 2014}, {"title": "Question answering with subgraph embeddings", "author": ["Antoine Bordes", "Sumit Chopra", "Jason Weston."], "venue": "CoRR, abs/1406.3676.", "citeRegEx": "Bordes et al\\.,? 2014", "shortCiteRegEx": "Bordes et al\\.", "year": 2014}, {"title": "Large-scale simple question answering with memory networks", "author": ["Antoine Bordes", "Nicolas Usunier", "Sumit Chopra", "Jason Weston."], "venue": "CoRR, abs/1506.02075.", "citeRegEx": "Bordes et al\\.,? 2015", "shortCiteRegEx": "Bordes et al\\.", "year": 2015}, {"title": "Gated feedback recurrent neural networks", "author": ["Junyoung Chung", "\u00c7aglar G\u00fcl\u00e7ehre", "KyungHyun Cho", "Yoshua Bengio."], "venue": "CoRR, abs/1502.02367.", "citeRegEx": "Chung et al\\.,? 2015", "shortCiteRegEx": "Chung et al\\.", "year": 2015}, {"title": "Paraphrase-driven learning for open question answering", "author": ["Anthony Fader", "Luke S Zettlemoyer", "Oren Etzioni."], "venue": "Citeseer.", "citeRegEx": "Fader et al\\.,? 2013", "shortCiteRegEx": "Fader et al\\.", "year": 2013}, {"title": "Named entity recognition with character-level models", "author": ["Dan Klein", "Joseph Smarr", "Huy Nguyen", "Christopher D Manning."], "venue": "Proceedings of the seventh conference on Natural language learning at HLT-NAACL 2003-Volume 4, pages 180\u2013183. Asso-", "citeRegEx": "Klein et al\\.,? 2003", "shortCiteRegEx": "Klein et al\\.", "year": 2003}, {"title": "Learning dependency-based compositional semantics", "author": ["Percy Liang", "Michael I Jordan", "Dan Klein."], "venue": "Computational Linguistics, 39(2):389\u2013446.", "citeRegEx": "Liang et al\\.,? 2013", "shortCiteRegEx": "Liang et al\\.", "year": 2013}, {"title": "Entity linking at web scale. In Proceedings of the Joint Workshop on Automatic Knowledge Base Construction and Web-scale Knowledge Extraction, pages 84\u201388", "author": ["Thomas Lin", "Oren Etzioni"], "venue": null, "citeRegEx": "Lin and Etzioni,? \\Q2012\\E", "shortCiteRegEx": "Lin and Etzioni", "year": 2012}, {"title": "Design challenges for entity linking", "author": ["Xiao Ling", "Sameer Singh", "Daniel S Weld."], "venue": "Transactions of the Association for Computational Linguistics, 3:315\u2013328.", "citeRegEx": "Ling et al\\.,? 2015", "shortCiteRegEx": "Ling et al\\.", "year": 2015}, {"title": "Sequence to sequence learning with neural networks", "author": ["Ilya Sutskever", "Oriol Vinyals", "Quoc V Le."], "venue": "Advances in neural information processing systems, pages 3104\u20133112.", "citeRegEx": "Sutskever et al\\.,? 2014", "shortCiteRegEx": "Sutskever et al\\.", "year": 2014}, {"title": "Using multiple clause constructors in inductive logic programming for semantic parsing", "author": ["Lappoon R Tang", "Raymond J Mooney."], "venue": "Machine Learning: ECML 2001, pages 466\u2013477. Springer.", "citeRegEx": "Tang and Mooney.,? 2001", "shortCiteRegEx": "Tang and Mooney.", "year": 2001}, {"title": "Translating videos to natural language using deep recurrent neural networks", "author": ["Subhashini Venugopalan", "Huijuan Xu", "Jeff Donahue", "Marcus Rohrbach", "Raymond Mooney", "Kate Saenko."], "venue": "arXiv preprint arXiv:1412.4729.", "citeRegEx": "Venugopalan et al\\.,? 2014", "shortCiteRegEx": "Venugopalan et al\\.", "year": 2014}, {"title": "Show, attend and tell: Neural image caption generation with visual attention", "author": ["Kelvin Xu", "Jimmy Ba", "Ryan Kiros", "Kyunghyun Cho", "Aaron C. Courville", "Ruslan Salakhutdinov", "Richard S. Zemel", "Yoshua Bengio."], "venue": "CoRR, abs/1502.03044.", "citeRegEx": "Xu et al\\.,? 2015", "shortCiteRegEx": "Xu et al\\.", "year": 2015}, {"title": "Semantic parsing for single-relation question answering", "author": ["Wen-tau Yih", "Xiaodong He", "Christopher Meek."], "venue": "Citeseer.", "citeRegEx": "Yih et al\\.,? 2014", "shortCiteRegEx": "Yih et al\\.", "year": 2014}, {"title": "Semantic parsing via staged query graph generation: Question answering with knowledge base", "author": ["Wen Yih", "Ming-Wei Chang", "Xiaodong He", "Jianfeng Gao."], "venue": "ACL.", "citeRegEx": "Yih et al\\.,? 2015", "shortCiteRegEx": "Yih et al\\.", "year": 2015}, {"title": "Learning to execute", "author": ["Wojciech Zaremba", "Ilya Sutskever."], "venue": "arXiv preprint arXiv:1410.4615.", "citeRegEx": "Zaremba and Sutskever.,? 2014", "shortCiteRegEx": "Zaremba and Sutskever.", "year": 2014}, {"title": "Text understanding from scratch", "author": ["Xiang Zhang", "Yann LeCun."], "venue": "CoRR, abs/1502.01710.", "citeRegEx": "Zhang and LeCun.,? 2015", "shortCiteRegEx": "Zhang and LeCun.", "year": 2015}], "referenceMentions": [{"referenceID": 4, "context": "We use our model for singlerelation question answering, and demonstrate the effectiveness of our novel approach on the SimpleQuestions dataset (Bordes et al., 2015), where we improve state-of-the-art accuracy by 2% for both Freebase2M and Freebase5M subsets proposed.", "startOffset": 143, "endOffset": 164}, {"referenceID": 15, "context": "In this paper we focus on single-relation factoid questions, which are one of the most common forms of questions found in search query logs and community question answering websites (Yih et al., 2014), (Fader et al.", "startOffset": 182, "endOffset": 200}, {"referenceID": 6, "context": ", 2014), (Fader et al., 2013).", "startOffset": 9, "endOffset": 29}, {"referenceID": 1, "context": "To address these challenges, we propose the use of an encoder-decoder framework with attention, which has already been successfully applied to machine translation, speech recognition, and image captioning (Bahdanau et al., 2014), (Amodei et al.", "startOffset": 205, "endOffset": 228}, {"referenceID": 14, "context": ", 2015), (Xu et al., 2015).", "startOffset": 9, "endOffset": 26}, {"referenceID": 12, "context": "While early approaches relied on building high-quality lexicons for domain-specific databases such as GeoQuery (Tang and Mooney, 2001), recent work has focused on building semantic parsing frameworks for general knowledge bases such as Freebase, (Yih et al.", "startOffset": 111, "endOffset": 134}, {"referenceID": 15, "context": "While early approaches relied on building high-quality lexicons for domain-specific databases such as GeoQuery (Tang and Mooney, 2001), recent work has focused on building semantic parsing frameworks for general knowledge bases such as Freebase, (Yih et al., 2014), (Bordes et al.", "startOffset": 246, "endOffset": 264}, {"referenceID": 3, "context": ", 2014), (Bordes et al., 2014), (Bordes et al.", "startOffset": 9, "endOffset": 30}, {"referenceID": 4, "context": ", 2014), (Bordes et al., 2015), (Berant and Liang, 2014), (Fader et al.", "startOffset": 9, "endOffset": 30}, {"referenceID": 2, "context": ", 2015), (Berant and Liang, 2014), (Fader et al.", "startOffset": 9, "endOffset": 33}, {"referenceID": 6, "context": ", 2015), (Berant and Liang, 2014), (Fader et al., 2013).", "startOffset": 35, "endOffset": 55}, {"referenceID": 3, "context": "To address this issue, recent work relies on producing high-level embeddings for predicates and entities in a knowledge base based off of their textual descriptions, (Bordes et al., 2014), (Bordes et al.", "startOffset": 166, "endOffset": 187}, {"referenceID": 4, "context": ", 2014), (Bordes et al., 2015), (Yih et al.", "startOffset": 9, "endOffset": 30}, {"referenceID": 15, "context": ", 2015), (Yih et al., 2014), (Yih et al.", "startOffset": 9, "endOffset": 27}, {"referenceID": 16, "context": ", 2014), (Yih et al., 2015).", "startOffset": 9, "endOffset": 27}, {"referenceID": 6, "context": "Consequently, they often rely on significant data augmentation from sources such as Paralex (Fader et al., 2013), which contains 18 million question-paraphrase pairs scraped from WikiAnswers, to have sufficient examples for each word they encounter.", "startOffset": 92, "endOffset": 112}, {"referenceID": 18, "context": "named entity recognition (Zhang and LeCun, 2015), (Chung et al.", "startOffset": 25, "endOffset": 48}, {"referenceID": 5, "context": "named entity recognition (Zhang and LeCun, 2015), (Chung et al., 2015), (Klein et al.", "startOffset": 50, "endOffset": 70}, {"referenceID": 7, "context": ", 2015), (Klein et al., 2003).", "startOffset": 9, "endOffset": 29}, {"referenceID": 5, "context": "Moreover, in (Chung et al., 2015) the authors show that gated-feedback LSTMs on top of on character-level embeddings can capture long-term dependencies in language modeling.", "startOffset": 13, "endOffset": 33}, {"referenceID": 11, "context": "First introduced in (Sutskever et al., 2014), in an encoder-decoder network, a source sequence is first encoded into a fixed-length vector which intuitively captures \u201cmeaning\u201d, and then decoded into a desired target sequence.", "startOffset": 20, "endOffset": 44}, {"referenceID": 13, "context": "This approach has been used successfully in diverse domains such as machine translation, image captioning, and executing programs (Venugopalan et al., 2014), (Bahdanau et al.", "startOffset": 130, "endOffset": 156}, {"referenceID": 1, "context": ", 2014), (Bahdanau et al., 2014), (Xu et al.", "startOffset": 9, "endOffset": 32}, {"referenceID": 14, "context": ", 2014), (Xu et al., 2015), (Zaremba and Sutskever, 2014).", "startOffset": 9, "endOffset": 26}, {"referenceID": 17, "context": ", 2015), (Zaremba and Sutskever, 2014).", "startOffset": 9, "endOffset": 38}, {"referenceID": 4, "context": "Like (Bordes et al., 2015), we have an approximate entity linking system that prunes the list of possible predicates and entities our model has to evaluate, and a prediction system which determines the most likely (entity, predicate) pair from the filtered list.", "startOffset": 5, "endOffset": 26}, {"referenceID": 4, "context": "However, to keep the experimental setup consistent with previous work such as (Bordes et al., 2015) and (Yih et al.", "startOffset": 78, "endOffset": 99}, {"referenceID": 15, "context": ", 2015) and (Yih et al., 2014), we generate highlevel embeddings for each predicate and entity in Freebase using only their English aliases.", "startOffset": 12, "endOffset": 30}, {"referenceID": 10, "context": "However, we do incorporate additional information from Freebase such as entity descriptions for our linking system to improve overall precision, like previous work in this area (Ling et al., 2015), (Lin et al.", "startOffset": 177, "endOffset": 196}, {"referenceID": 8, "context": ", 2012), (Liang et al., 2013).", "startOffset": 9, "endOffset": 29}, {"referenceID": 5, "context": "(Chung et al., 2015) (Figure 1a) and take the outputs at each time step as the final embeddings for the question.", "startOffset": 0, "endOffset": 20}, {"referenceID": 15, "context": "To produce the latent semantic representations for the entity/predicates, we use a two-layer convolutional neural network (CNN) over character-level embeddings followed by a max-pooling layer, similarly to (Yih et al., 2014).", "startOffset": 206, "endOffset": 224}, {"referenceID": 1, "context": "For our attention-based LSTM we use the same architecture as described in (Bahdanau et al., 2014) (Figure 1d).", "startOffset": 74, "endOffset": 97}, {"referenceID": 15, "context": "A similar optimization objective was used to train the semantic similarity modules proposed in (Yih et al., 2014) and (Yih et al.", "startOffset": 95, "endOffset": 113}, {"referenceID": 16, "context": ", 2014) and (Yih et al., 2015).", "startOffset": 12, "endOffset": 30}, {"referenceID": 4, "context": "For entity linking, we experiment with the official Freebase API, as well as an approximate entity linking system we build on top of the Freebase5M/Freebase2M subsets we build similar to (Bordes et al., 2015).", "startOffset": 187, "endOffset": 208}, {"referenceID": 4, "context": "We evaluate our models on the SimpleQuestions dataset (Bordes et al., 2015) which consists of 108,442 single-relation questions and their corresponding source entity, predicate, target entity triples from Freebase.", "startOffset": 54, "endOffset": 75}, {"referenceID": 4, "context": "See (Bordes et al., 2015).", "startOffset": 4, "endOffset": 25}], "year": 2016, "abstractText": "We show that an encoder-decoder framework can be successfully be applied to question-answering with a structured knowledge base. In addition, we propose a new character-level modeling approach for this task, which we use to make our model robust to unseen entities and predicates. We use our model for singlerelation question answering, and demonstrate the effectiveness of our novel approach on the SimpleQuestions dataset (Bordes et al., 2015), where we improve state-of-the-art accuracy by 2% for both Freebase2M and Freebase5M subsets proposed. Importantly, we achieve these results even though our character-level model has 16x less parameters than an equivalent word-embedding model, uses significantly less training data than previous work which relies on data augmentation, and encounters only 1.18% of the entities seen during training when testing.", "creator": "TeX"}}}