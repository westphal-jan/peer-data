{"id": "1604.01686", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "6-Apr-2016", "title": "Relationship between Variants of One-Class Nearest Neighbours and Creating their Accurate Ensembles", "abstract": "In one-class classification (OCC) problems, only the data for the target class is available, whereas the data for the non-target class may be completely absent. In this paper, we study one-class nearest neighbour (OCNN) classifiers and their different variants for the OCC problem. We present a theoretical analysis to show the equivalence among different variants of OCNN that may use different neighbours or thresholds to identify unseen examples of the non-target classifier. We demonstrate that OCC can improve their accuracy by using multiple approaches, but the two methods of the OCC problem are different in that OCC also outperform the OCC by significantly over-the-counter (or even less than 50%). These approaches may be useful for assessing the effect of OCCs on the accuracy of the OCC problem (for example, in our analysis in OCC the accuracy of OCCs is a better estimate for accuracy of OCCs than OCCs). In particular, OCCs may increase the accuracy of the OCCs in the OCCs by more than a tenth (Figure 1). Therefore, there is an increase in accuracy of the OCCs by more than a tenth (Figure 2). In contrast, OCCs may also be improved by more than a tenth (Figure 3). Therefore, there is an increase in accuracy of the OCCs by more than a tenth (Figure 4). Therefore, there is an increase in accuracy of the OCCs by more than a tenth (Figure 5). Furthermore, OCCs may also be improved by more than a tenth (Figure 6).", "histories": [["v1", "Wed, 6 Apr 2016 16:36:41 GMT  (371kb,D)", "http://arxiv.org/abs/1604.01686v1", "38 pages, 3 figures"], ["v2", "Fri, 28 Oct 2016 18:00:42 GMT  (385kb,D)", "http://arxiv.org/abs/1604.01686v2", "42 pages, 9 figures, 3 Tables"], ["v3", "Fri, 24 Feb 2017 21:30:42 GMT  (391kb,D)", "http://arxiv.org/abs/1604.01686v3", "14 pages, 9 figures, 3 Tables"], ["v4", "Wed, 22 Mar 2017 20:56:34 GMT  (391kb,D)", "http://arxiv.org/abs/1604.01686v4", "14 pages, 9 figures, 3 Tables"]], "COMMENTS": "38 pages, 3 figures", "reviews": [], "SUBJECTS": "cs.LG", "authors": ["shehroz s khan", "amir ahmad"], "accepted": false, "id": "1604.01686"}, "pdf": {"name": "1604.01686.pdf", "metadata": {"source": "CRF", "title": "Equivalence Among Different Variants of One-Class Nearest Neighbours and Creating Their Accurate Ensembles", "authors": ["Shehroz S. Khan", "Amir Ahmad"], "emails": ["s255khan@uwaterloo.ca", "amirahmad01@gmail.com"], "sections": [{"heading": null, "text": "\u2217Corresponding Author Email addresses: s255khan@uwaterloo.ca (Shehroz S. Khan),\namirahmad01@gmail.com (Amir Ahmad)\nPreprint submitted to Information Sciences September 4, 2017\nar X\niv :1\n60 4.\n01 68\n6v 1\n[ cs\n.L G"}, {"heading": "1. Introduction", "text": "One-Class Classification (OCC) [1] is a special case of general classification problem where the data of the positive (target) class is sufficiently available, whereas the negative class data is absent during the training time. This deficiency of data for the negative class may arise due several reasons, such as the difficulty in obtaining the data, higher cost in data collection or the rarity of their occurrence. Some examples for OCC are machine fault diagnosis, fraud detection, human fall detection, identifying rare disease. In most of these applications, it is easy to collect samples for the positive (target) class or the data describing the normal behaviour of the domain under consideration. However, the collection of negative samples may result in high cost in dollars, put health and safety of a person in danger. In other cases, the data in the negative class occurs rarely. Therefore, even if some samples are collected for the negative class, the training data will be severely skewed and it is difficult to build generalizable classifiers using the traditional supervised classification algorithms.\nWhen the data for the negative class is too few or absent, a normal practice is to generate artificial data for the negative class [2] and convert the classification problem as a binary classification problem. Another possibility is to estimated the parameters of the distribution of negative class based on varying the parameters of the positive class [3]. However, these techniques heavily depend on the choice of parameters for the distribution of the positive class or the unseen negative class. The literature on OCC offers several alternatives to build a classification boundary based on the positive samples only [4]. In this paper, we choose a one-class nearest neighbour (OCNN) approach for detecting unseen samples of the negative class, when they were not present during the training phase. In principle, an OCNN method finds the high and low density regions based on the local neighbourhood of a test sample. Using a decision threshold, an OCNN accepts or rejects a test sample as a member of the target class. In its simplest form, an OCNN finds the 1st nearest neighbour of a test sample in the target class and then finds the 1st nearest neighbour of this neighbour in the target class. If the ratio of these distances is lower than a user-defined threshold, it is accepted as a member of the target class [5]. This method is simple and effective in finding instances of the unseen negative class; however, it is very sensitive to the\nnoise in the target class. This method may also be not effective when the dimension of the data is very high because of the problems associated with the euclidean distance metric. In an OCNN, the number of nearest neighbours and the value of decision threshold can be optimized but the sensitivity of the classifier w.r.t. noise in the positive class may change. There exists several variants of OCNN in the literature (see Section 2); however, these methods do not make relation between the other OCNN methods and do not clearly explain the reasons as to why a particular variant works better than the other.\nA classifier ensembles is an approach to improve the performance of a classifier [6, 7] Classifier ensembles consist of many classifiers; the final result of a classifier ensemble depends on the combined results of individual classifiers of the ensemble. A classifier ensemble generally performs better than individual classifiers provided the member classifiers are accurate and diverse. As supervised K-Nearest Neighbour (KNN) classifiers are very robust to variations of the data set; therefore, ensemble techniques based on training data sampling (such as Bagging [8] and Boosting [9]) have not been successful to create their ensembles [10]. KNN classifiers are found sensitive to input features sampling; therefore, diverse NN classifiers can be created by using different feature space for different KNN classifiers to generate an accurate ensemble [10].\nIn this paper, we present a holistic view on the general problem of OCNN by discussing their different variants and creating accurate their ensembles. The main contributions of the paper are:\n\u2022 We theoretically show the equivalence and relationship between various OCNN approaches and discuss the impact of choosing nearest neighbours on the decision threshold.\n\u2022 We present two different types of ensemble methods \u2013 random subspace and random projection for the OCNN to study their performance when the feature space is changed. To the best of our knowledge, the suitability of random projection approach is investigated for the first time for OCNN.\n\u2022 We present present a cross-validation method and a modified thresholding algorithm that utilize the outliers from the target class to optimize the parameters of the OCNN.\nOur results on several benchmark and domain-specific real world data shows superior performance of the OCNN with random projection ensembles and give strong evidence that single OCNN may not be the right choice to detect unseen negative data during testing.\nThe rest of the paper is structured as follows. In Section 2, we present literature review on various variants of OCNN and their ensembles. Section 3 introduces two variants of OCNN that uses different numbers of nearest neighbours and decision threshold, followed by a theoretical analysis about their relation with other variants of OCNN and a brief discussion on various ensemble approaches that are used with the OCNN classifiers. In Section 4, we introduce a cross-validation method and a modified thresholding algorithm to optimize parameters for the OCNN classifiers using only the data from the target class. Experimental results are shown on various datasets in Section 5. Conclusions and future work are summarized in Section 6."}, {"heading": "2. Related Work", "text": "The research on one-class classification spans more than a decade and researchers have attempted various models using support vector machines, nearest neighbours, decision trees, neural networks, bayesian networks and classifier ensembles [4]. These models mostly differ in either learning with target examples only; learning with target examples and some amount of poorly sampled outlier examples or artificially generated outliers; learning with labelled target and unlabelled data; methodology used and application domains applied [1]. In this paper we restrict our literature review to one class nearest neighbour approaches and the methods that involve their ensemble.\nTax [5] presents a one-class nearest neighbour method called nearest neighbour description (NN-d). In NN-d, a test object is accepted as a member of the target class when its local density is larger or equal to the local density of its nearest neighbour in the training set. The general idea is to find the distance of a test object to its nearest neighbour in the target class, and find the nearest neighbour of this neighbour, and if the distance is greater than a threshold then it is identified as an outlier. Tax and Duin [11] propose a nearest neighbour method capable of finding data boundaries when the sample size is very low. The boundary thus constructed can be used to detect the targets and outliers. However, this method has the disadvantage that it relies on the individual positions of the objects in the target set. Their method seems to be useful in situations where the data is distributed in sub-\nspaces. They test the technique on both real and artificial data and find it to be useful when very small amounts of training data exist (fewer than 5 samples per feature). Haro-Garc\u0131a et al. [12] use NN-d along with other one-class classifiers for identifying plant/pathogen sequences. They find the suitability of above methods owing to the fact that genomic sequences of plant are easy to obtain in comparison to pathogens. Their basic idea is to optimize the threshold between the distances of a test object to its nearest neighbour and distance of this neighbour to its nearest neighbour. They order these distances and find the best threshold based on the percentile of the sorted sequence. Angiulli [13] present a prototype based domain description algorithm (PDD) for one class classification task, which is equivalent to NNd under the infinity Minkowski metric and generalizes statistical tests for outlier detection. Angiulli introduces the concept of PDD consistent subset prototypes, which is a subset of the original prototype set and show that computing a minimum size PDD consistent subset is in general not approximable within any constant factor. The paper present two algorithms for computing PDD consistent subset: one that guarantees logarithmic approxiimation factor and the other faster version to manage very large datasets. The faster version does not consider all the pairwise object distances but it is shown to return almost identical subset size at lower computational cost. Angiulli perform experiments on datasets from UCI repository [14] and show that under noisy conditions PDD algorithm performs better than NN-d, when the same value of relative subset size is considered. Krawczyk et al. [15] use prototype reduction techniques using evolutionary approaches that follow either selection or generation of examples from the training data and perform NN-d approach. They report no significant accuracy losses using their prototype reduction technique on several datasets from UCI repository. Cabral et al. [16] propose a one-class nearest neighbour data description using the concept of structural risk minimization. The nearest neighbour approach suffers with the drawback of storing all training samples as prototypes that would be used to classify an unseen test sample. Their paper is based on the idea of removing redundant samples from the training set, thereby obtaining a compact representation aiming at improving generalization performance of the classifier. Their results on artificial and UCI datasets have shown improved performance than the NN-d classifiers. Their method also achieved considerable reduction in the number of stored prototypes.\nCabral et al. [17] extend their work [16] and present another approach where they not only consider the 1 but K-nearest neighbours of a test object\nand find a nearest neighbour of each of these K neighbours. A decision is arrived based on majority voting. They tested their algorithm on artificial data, biomedical data and data from the UCI repository [50] and observe that K nearest neighbour version of their classifier outperforms the NN-d. However, it is not shown how the optimal value of K nearest neighbour is arrived at for better classification results. Munroe and Madden [18] extend the idea of NN-d to tackle the recognition of vehicles using a set of features extracted from their frontal view and presented high accuracy of classification. Instead of computing one neighbour of the nearest neighbour of the test sample in the target class, they compute the average of K nearest neighbours of the first nearest neighbour. However the computation of best fitting value of K is not not illustrated clearly in their work. Halvani et al. [19] use the similar idea for authorship verification task. They generate different types of features for a given document. For every feature type, the distance is calculated for its nearest neighbour and average distance of its K nearest neighbours, and a decision is taken for a given threshold. The decisions from all the feature types are combined using majority voting rule to take a final decision. Huang et al. [20] present an OCNN method for the identification of protein carbamylated as they play important role in a number of biological conditions. In their method they compute threshold from the positive class based on a fixed false negative ratio (wrongly classified positive samples), compute distance of a test sample with its K neighbours and if it is below a threshold, it is accepted as member of the positive class. The value of false negative rate is deduced from performance metric curves but the paper does not clearly state to handle the K nearest neighbour to arrive at final decision. Ges\u00f9 et al. [21] present an OCNN algorithm and tested it on synthetic data that simulate microarray data for the identification of nucleosomes and linker regions across DNA. They present a decision rule which states that if there are at least K data objects in the positive class that are dissimilar from the test object at most \u03c6, then it is classified as a member of the positive class. They propose to calculate optimal values for K and \u03c6 using the ROC curve. Their results show good recognition rate on synthetic data for nucleosome and linker regions across DNA. However, the paper does not explain the construction of validation set and parameters optimization in detail. Datta [22] modifies the standard nearest neighbour algorithm appropriate to learn the positive class. The modified algorithm learns a threshold, which is the maximum distance a test example can be from any learned example and still be considered a member of the positive class. Any test example that has a\ndistance greater than the threshold from any training example will not be considered a member of the positive class. Datta also experimented with another similar modification that involves learning a vector of thresholds, where each threshold corresponding to every sample in the positive class and then employing a classification rule based on them to accept or reject a test sample. Khan [23] presents a generalization of the NN-d method which finds the distance of J nearest neighbours of the test sample in the target class and compute the K nearest neighbours of each of the J neighbours in the target class. The distance ratio is computed for each of the J neighbours and a majority voting rule is used to take a final decision. They use kernel as a distance metrics instead of Euclidean distance and show that this method can perform better than the traditional NN-d on chemical spectral data.\nAs in traditional multi-class classification problems, one-class classifier might not capture all the characteristics of the data. However, using just the best classifier and discarding the classifiers with poorer performance might waste valuable information [24]. To improve the performance of different classifiers which may differ in complexity or in the underlying training algorithm used to construct them, an ensemble of classifiers is a viable solution. This may serve to increase the performance and also the robustness of the classification [25]. Pe\u0327kalska et al. [26] use the proximity of target object to its class as a \u2018dissimilarity representation\u2019 (DR) and show that the discriminative properties of various DRs can be enhanced by combining them properly. They use three types of one-class classifier, namely Nearest Neighbour, Generalized Mean Class and Linear Programming Dissimilarity Data Description. They make two types of ensembles: (i) combine different DR from individual one-class classifiers into one representation after proper scaling using fixed rules, for e.g. average, product and train single one-class classifier based on this information, (ii) combine different DR of training objects over several base classifiers using majority voting rule. Their results show that both methods perform significantly better than the OCC trained with a single representation. Segu\u00ed et al. [27] present an ensemble methods based on non-parametric weighted bagging strategy for OCC. Their method estimates a probability density based on a forest structure of the data instead of assuming uniform data distribution and constructs bootstrap samples according to the data density ranking, where higher density imply greater likelihood of being selected for a bootstrap of the ensemble. They use three types of one-class classifiers: NN-d, OSVM and Minimum Spanning Tree Class Descriptor (MST) [28] and show experimentally that\nthe ensemble bagging and non-parametric weighted bagging obtain better rankings than the base one-class classifier methods and show a statistically significant improvement for MST and NN-d. Menahem et al. [29] propose an ensemble meta-learning algorithm which learns a combining function upon aggregates of the ensemble-members prediction. This algorithm depends on the classification properties of the ensemble-members and not on the fix-rule scheme. They use four one-class classifiers; two of them are based on OCNN, the rest are based on density estimation and OSVM. They combine these classifiers by four ensembles rules (majority voting, mean voting, max rule and product rule) and show that their proposed ensemble algorithm outperform traditional ensemble schemes both in terms of classification performance and in correlation to the best ensemble-member. Ges\u00f9 and Bosco [30] present an ensemble method for combining one-class fuzzy OCNN classifiers. Their classifier combining method is based on a genetic algorithm optimization procedure by using different similarity measures. They test their method on two categorical datasets and show that whenever the optimal parameters are found, fuzzy combination of one-class classifiers may improve the overall recognition rate. Nanni [31] studies combining several one-class classifiers using the random subspace method for the problem of online signature verification. Nanni uses several one-class classifiers: Gaussian model description, Mixture of Gaussian Descriptions, Nearest Neighbour Method Description, PCA Description (PCAD), Linear Programming Description (LPD), SVDD and Parzen Window Classifier. It is shown that fusion of various classifiers can reduce the error and the best fusion method is the combination of LPD and PCAD.\nThe literature review suggests that the ensembles of OCNN (such as random subspace) and fusion with other one-class classifiers can perform better than the base OCNN. Some papers use first nearest neighbour in the target class, whereas others use more than one nearest neighbours to decide if a test instance is a member of the negative class or not. However, the intuition behind which method will work better under what conditions is not clear. There is not much work done on the optimization of the decision threshold of an OCNN, which is normally taken to be equal to 1 . A major challenge in most of the OCC methods is the unavailability of data from the negative class; therefore, optimization of parameters is very difficult [32]. Hence, choosing different nearest neighbours and the decision threshold are difficult to optimize in the OCNN approach."}, {"heading": "3. One-Class Nearest Neighbour Classifiers (OCNN)", "text": "Based on the literature review in Section 2, various OCNN methods can be categorized into the following four types based on the number of nearest neighbours they use to compute the decision threshold:\n(i) Find the first nearest neighbour of the test sample in the target class, and the first nearest neighbour of the first neighbour (11NN) [5, 12, 13, 15, 16].\n(ii) Find the first nearest neighbour of the test sample in the target class, and the K nearest neighbours of the first neighbours (1KNN) [18, 19, 20].\n(iii) Find the J nearest neighbours of the test sample in the target class, and the first nearest neighbours of the first J neighbours (J1NN) [17].\n(iv) Find the J nearest neighbours of the test sample in the target class, and the K nearest neighbours of the first J neighbours (JKNN) [23].\nThese different methods of OCNN can differ in the the distance metric, method of combining distances to arrive at a final decision and the choice of the value of the decision threshold. Figure 1 shows a graphical representation of these methods in 2D. The dark circles (\u2022) show the instances of the target class. The red star (?) shows a test sample. The solid line shows the distance of either 1 or J nearest neighbours of the test sample in the target class and the dotted lines show the distance between the 1 or K nearest neighbours of those 1 or J neighbours. In a JKNN classifier, if J and/or K is set to 1, then it condenses to 11NN , 1KNN or J1NN ."}, {"heading": "3.1. One-Class JK Nearest Neighbour", "text": "We now present a general OCNN (JKNN) method for detecting unseen members of the negative class. In this method, we\n1. Find the J nearest neighbour (NN trj (z)) of the test test sample (z) in the target class and find their average, D\u0304J .\n2. Find the K nearest neighbours of these J neighbours (NN trk (NN trj (z))) and find their average, D\u0304K .\nFor a given decision threshold, \u03b8, if D\u0304J D\u0304K < \u03b8, then the test data object is considered as a member of the target class or else rejected as a member of the negative class. Algorithm 2 shows the steps to classify a test data object as a member of target class or not. This algorithm is different from the work of\nKhan [23] in two ways. Firstly, they use kernel as a distance metric, whereas we use Euclidean as a distance metric. Secondly, they combine the decision using majority voting of the distances of different K neighbours of each J neighbours, whereas we take the average of the distances of the J and K nearest neighbours."}, {"heading": "3.2. Relationship Among Different OCNN approaches", "text": "In OCNN generally the decision threshold (\u03b8) is kept as 1. In this section, we will show that varying decision threshold (\u03b8) with 11NN is similar to other OCNN methods discussed in Section 3.\nIn 11NN method for one class problem, the distance between the new data point and its nearest neighbour data point is calculated and compared with the distance between the nearest neighbour data point and its nearest neighbour. In the given figure (Fig. 1a), for 11NN with \u03b8 equal to 1, if D1 more than D2 the new point D1 is outlier. In this case even if D1 is slightly more than D2 the test data point will be assigned to negative class. Intuitively, an outlier data point should be at much greater distance to its nearest neighbour (D1) than the distance between this nearest neighbour and its nearest neighbour (D2).\nMathematically this can be represented as\nD1 > \u03b8D2 (1)\nwhere \u03b8 > 1. In this cases, many data points which are outliers in first case (D1 > D2) will not be rejected. D1 > \u03b8D2 rule should perform better than D1 > D2 rule because an outlier data point dissimilarity (Euclidean distance) with its nearest data point should be more than the dissimilarity between the nearest data point to its nearest data point (we know that these two data points belong to positive class). However, finding the optimal value of \u03b8 is problematic as it depends on the properties of datasets. The optimal value of \u03b8 may be different for different datasets. The optimal value of \u03b8 will also depend on the performance measure. We will discuss how 1KNN and JKNN are related with this approach.\n1. 1KNN approach \u2013 There is another approach of OCNN, 1KNN , that may produce similar results as (\u03b8 > 1). From the given Fig. 1b, for a point of negative class, we can write distances as\nD1 > ((D21 +D22....+D2i, ......D2K)/K) (2)\nWhere D2i (K \u2265 i \u2265 1) is the distance between the nearest neighbour of the new data point and the ith nearest neighbour of this nearest neighbour (Fig. 1b) As that distance between a data point and ith nearest neighbour is greater than or equal to the distance to distance between the data point and (i\u2212 1)th nearest neighbour, D2i \u2265 D2(i\u22121). Hence D2i \u2265 D21\n(D21 +D22....+D23, ......D2K)/K \u2265 (D21 +D21....+D21......D21)/K (D21 +D22....+D23, ......D2K)/K \u2265 KD21/K (D21 +D22....+D23, ......D2K)/K \u2265 D21 (D21 +D22....+D23, ......D2K)/K = \u03b1D21(\u03b1 \u2265 1)\n(3)\nUsing Equations 2 and 3, we get\nD1 > \u03b1D21 (4)\nIt is highly unlikely that all the K nearest neighbours have same distances as the first nearest neighbour distance. Hence, \u03b1 > 1 for most of the cases. The optimal value of K will depend on the properties of datasets and performance measure. Equations 1 and 4 are same, this suggests that 1KNN is the same as the intuition based calculation for 11NN with \u03b8 > 1.\n2. J1NN approach - The calculation for assigning a class to a data point will be opposite to the 1KNN approach, hence the condition to assign a negative class to a data point will be\n\u03b1D1 > D21 (5)\nEq. 5 with Eq. 1 suggest that 1JNN approach is similar to \u03b8 \u2264 1. As we discussed that \u03b8 > 1 is a better choice for negative class prediction, 1JNN should not be a good candidate for accurate prediction.\n3. JKNN approach \u2013 In JKNN approach, the average distance of J nearest neighbours of a new data point is compared with the average distance of the K nearest neighbours of each of these J nearest neighbours. For a data point to be a negative class (Fig. 1d)\nD\u0304J > D\u0304K (6)\nUsing the same argument as discussed for 1KNN, we can write Equation 6 as, \u03b2D11 > \u03b3D21 (7) Where \u03b2 and \u03b3 \u2265 1. For most of the cases, \u03b2 and \u03b3 are more than 1\nD11 > (\u03b3/\u03b2)D21 (8)\nAs both \u03b3 and \u03b2 are more than 1. It is likely, they will cancel out each other effect for high values of J and K. Hence, for high values of J and K, they will generate a condition, D11 > D21; that is similar to 11NN approach with \u03b8 = 1.\nThe above arguments imply that 1JNN or JKNN with low J with \u03b8 = 1 will be similar to 11NN with \u03b8 > 1. Hence, they are more likely to produce better decision boundaries than 11NN with \u03b8 = 1. Whereas, J1NN or\nJKNN with high J may not produce accurate decision boundary as it is similar to 11NN with \u03b8 < 1, which does not represents the negative class properly. Finding the optimized values of J and K for JKNN with \u03b8 = 1 or the optimized value of \u03b8 for 11NN is an important step. A detailed study is carried out in this paper to understand this step properly. The parameter optimization techniques are discussed in Section 4."}, {"heading": "3.3. Classifier Ensemble", "text": "Classifier Ensembles are combination of many classifiers [6]. We propose that classifier ensembles approach can be used to improve the performance of OCNN methods. Creation of accurate and diverse classifiers are the key to the accurate ensembles[6]. Creating diverse feature spaces to create diverse NN classifiers is a popular method [10] for NN ensembles. We will apply similar approaches for ensembles of OCNN classifiers. In this section we discuss two approaches to create classifier ensembles; Random subspace and Random projection.\nRandom subspace is a popular ensembles creation technique [33]. In each run a classifier is trained on a randomly selected feature subspace of the original feature space. This creates diverse classifiers that in turn create an accurate ensemble. Random subspace technique has also be used to create ensembles for outlier detection [34]. However, ensembles of various OCNN methods created by Random Subspace method have not been extensively studied. Random projection (RP) is a method for dimensionality reduction problem [35, 36]. RP maps a number of points in a high-dimensional space into a low dimensional space with the property that the Euclidean distance of any two points is approximately preserved through the projection.\nDifferent random projections of a dataset create different new datasets with different features. Schclar and Rokach [37] introduce a classifier ensemble method for K-NN classifiers for multiclass problems by using random projections. In this method, new datasets are created by using different random projections. K-NN classifiers trained on these datasets are diverse. Hence, these K-NN classifiers are combined to produce an accurate ensemble. Random projections have been used to approximate the convex-hull in high dimensional spaces for one-class problem [38]. However, in this method Random Projection is used to create two or three dimensional low-dimensional spaces. Hence, there may be a large loss of information. The conceptual similarity between K-NN and OCNN methods motivate us to employ random projections for creating ensembles of OCNN methods."}, {"heading": "4. Parameter Optimization for OCNN", "text": "In this section, we will discuss a method by which noisy points of a one class dataset can be used to optimize the parameters. We note that the different variants of OCNN discussed in Section 3 (shown in Figure 1) can suffer from either or both of the following problems:\n1. Sensitivity to noise \u2013 The real world target data may contain noisy observations due to non-calibrated data collection devices, human errors in labelling or inadvertent artifacts [39]. The above discussed one-class nearest approaches ignores this fact and presence of such deviant observations in the target class can make these classifiers sensitive to noise. This case can arise when most of the target data is dense towards its center but some data objects are far from it. This behaviour can lead to outliers being accepted as members of the target class.\n2. False negatives \u2013 If noisy observations are removed from the dataset but the decision threshold is not properly set, OCNN can reject a target test sample as an outlier.\nYin et al. [40] mention that due to the scarcity of negative data, it is a challenging problem to design a detection system that can reduce both the false positives and false negatives. In general, OCC methods are very sensitive to the choice of parameters [41]. In the case of OCNN, choosing a particular number of J and K nearest neighbours or moving the decision threshold (\u03b8) can affect both the false positive and false negative rates. However, in a given OCC problem, a classifier\u2019s parameters cannot be optimized directly because there is no training set and validation set available for the negative class. Therefore, choosing the optimal parameters for a given OCC problem in the absence of non-target class is very challenging [42]. One approach to handle such scenario is to generate artificial data for the unseen negative class [43, 2] by assuming some distribution for the instances of the negative class. However, these approaches are prone to over-fitting and the classification boundaries depend upon the parameters of the distribution of features for the unseen class. Other possibility is to estimate the parameters of the negative class based on varying the parameters of the positive class [3, 44]. However, such approaches depend on the estimated parameters for the positive class. A true distribution for positive class is hard to estimate due to limited availability of data and the assumptions on the dataset. We\ntake an alternative view by attempting to find classification boundary from the existing training data for the target class. We now present a procedure for evaluating OCNN in the absence of training samples for the negative class by:\n1. Removing the noisy observations from the target class using a method based on Inter-Quartile Range.\n2. Using the rejected outliers as proxy for the unseen negative class and performing a new cross-validation method to optimize the best J and K neighbours and the decision threshold.\nThe details of the above stated steps are described in the following subsections."}, {"heading": "4.1. Removing noise from the target class", "text": "As discussed in the previous section, noise in the target data can arise due to various reasons and its presence can adversely affect the decision of NN-d based classifiers in terms of accepting outliers as the members of target class during the testing phase. We present a method to remove the noisy observations based on Inter-Quartile Range (IQR). Khan et al. [39] show that for the human fall detection problem, where the data for falls is difficult to obtain, deviant normal human motion sequences can be removed from the normal human activities using the IQR technique. These deviant sequences can act as a proxy for real falls and help in optimizing the parameters of the probabilistic classifiers. They use Hidden Markov Model (HMM) based classification technique, and obtain log-likelihoods of the training sequences from the normal activities and apply IQR technique on them to remove the noisy sequences. Although this technique is specifically developed for probabilistic classifiers, it can be modified to other types of classification techniques. In the nearest neighbour approach, we compute distances instead of probabilities and IQR technique can be adapted to work on non-sequential data. In order to obtain distances, we use a center based distance computation method [45]. Firstly, the center of the target data is calculating by computing the mean of all the N data objects present in the target class. Then the distance of each target object is calculated from the center, resulting in N distances. These distance are then ordered and the IQR technique is applied. The quartiles of a ranked set of data values are the three points that divide\nthe data set into four equal groups, where each group comprises of a quarter of the data. Given the distances of target data objects from the center, the lower quartile (Q1), the upper quartile (Q3) and the inter-quartile range (IQR = Q3 \u2212Q1), a point P is qualified as noise of the target class, if\nP > Q3 + \u03c9 \u00d7 IQR || P < Q1 \u2212 \u03c9 \u00d7 IQR (9)\nwhere \u03c9 is the rejection rate that represents the percentage of data points that are within the non-extreme limits of distances given the distances of target training data from the center. Based on \u03c9, the extreme values of distances that represent spurious target training data can be removed.\nFigure 3 shows a 2-D representation of a target class from which few spurious instances are removed based on a value of \u03c9. The instances that are far away from the center of the data are to be removed and used as proxy for outliers. The algorithmic steps to remove noisy observations from the target class is shown in the Figure 4. These rejected noisy observation can help in the optimization of the J and K neighbours and the decision threshold (\u03b8) for the OCNN. The cross-validation method is described next."}, {"heading": "4.2. Cross Validation", "text": "In a binary or multi-class classification problem, cross-validation is often employed to estimate the parameters of a classification algorithm on a validation set and these parameters are used during testing to classify the test samples. In OCC problems, estimating parameters using cross-validation becomes challenging due to the absence of negative data in the validation set. This happens because in OCC, the data of only the target class is available during the training and the data from negative class only appear during the testing (along with the target data).\nWe now introduce a cross-validation method to optimize the nearest neighbours J and K for the JKNN classifier when the decision threshold (\u03b8) is set to 1. Firstly, we choose the number of cross-validation folds, say F . Then we split the dataset such that target data from (F \u22121) folds are joined together and outliers (see Section 4.1) from (F \u2212 1) partitions are ignored \u2013 this set will be used during training and the remaining targets and outliers\nof the F th fold will be used during testing. In the combined target datasets, we employ the IQR technique (discussed in Section 4.1) to reject some noisy observation from the target class. These rejected noisy observations serves as a proxy for the negative class. Then we employ an inner cross-validation step, lets say with G folds. We split the target and rejected noisy data such that in one fold we have target data from all the (G\u2212 1) folds and tested on target and rejected noisy observations of the Gth fold. We use the one-class JKNN classifier for the inner cross-validation step and test it on different values of J and K neighbours and the values that gives the best average value for the performance metric over all G folds is chosen as the best value of the parameter. These values of best J and K neighbours are then used in the joined target datasets of all the (F \u2212 1) folds and tested on the F th partition of the outer cross-validation to test the performance on both target and outliers using the one-class JKNN classifier. We repeat the whole process F times and the average value of the performance metric over all the F folds is reported. The performance metric chosen for this task is Geometric Mean (gmean) and it is discussed in detail in Section 5.1.\nFigure 5 shows the block diagram for the overall procedure for evaluating OCNN that shows all the building blocks discussed above. For F -fold cross-validation, the block diagram will be repeated F times and average performance metric is reported."}, {"heading": "4.2.1. How many noisy observations to reject from the target class?", "text": "As discussed earlier, noisy observations can be removed from the training data using the IQR technique (Section 4.1) to create the validation set for testing the one-class JKNN classifier. The amount of observations to be rejected depends upon the coverage parameter \u03c9. A higher value of \u03c9 results in less rejections and smaller value means more rejections. In the inner cross-validation method described in Section 4.2, the validation set should have at least one observation as a proxy for outliers in each of the inner fold G. As a rule of thumb, the value of \u03c9 should be chosen such that at least G data objects are rejected from the target class as proxy for outlier class. Therefore, the value of \u03c9 can either be chosen using domain knowledge or hit-and-trial method. In our experiments, we mostly chose \u03c9 = 1.5, and if that did not yield desired number of rejected targets, it is reduced until the desired number of targets are rejected."}, {"heading": "4.3. Optimizing Decision Threshold", "text": "We discussed the equivalence relation between JKNN with \u03b8 = 1 and 11NN with variable \u03b8. To optimize the decision threshold \u03b8 for 11NN , we use the same strategy for optimizing J andK as discussed above, by using the rejected outliers from the positive class as proxy for unseen negative class.. Then we use a modified empirical thresholding algorithm [46] to optimize the decision threshold \u03b8. The general idea of the original thresholding algorithm is to select an empirical threshold from the training instances according to the misclassification cost. This method can convert any cost-insensitive algorithm to cost-sensitive one by searching for the probability that minimizes the misclassification costs across all the training instances as the threshold for predicting the test instances. The advantage with this method is that it is least sensitive when the difference in misclassification costs is high, it also does not require accurate estimation of probabilities, rather an accurate ranking is sufficient and an internal cross validation method can be used to search for an empirical threshold [47]. The internal cross validation method\nlooks for a threshold in the probability of an observation given each class and optimizes that by using an exhaustive search over all possible thresholds. A disadvantage with using this algorithm is that it takes additional time in searching for the best empirical threshold from all the probabilities corresponding to every training set. Due to limited training data, this technique may also lead to sub-optimal choice of the empirical threshold.\nIn the inner-cross validation step discussed in Section 4.2, we use the rejected outliers and inner target data in the G fold to compute decision threshold as the ratio of distance of 1st neighbours and its 1st neighbour (i.e. J = K = 1) in the combined inner target data of the (G\u22121) folds. After the completion of inner cross-validation step, we will have decision thresholds for both the outlier and the target class (sans outliers), jointly equal to the number of instances in the combined (F \u2212 1) folds. Then, for rejected outliers and inner target data, we compare each of these decision threshold and compute a performance metric. The value of decision threshold that gives the maximum value of performance metric is chosen as the empirical threshold and used with 11NN (instead of \u03b8 = 1). This is repeated for every F outer fold. Our method differs from the original empirical thresholding in two ways - we use distance instead of the probability estimates and gmean as performance metric (see Section 5.1 instead of the misclassification cost. The algorithmic steps for one outer F fold are presented in Figure 6."}, {"heading": "5. Results", "text": ""}, {"heading": "5.1. Performance Metric", "text": "There are several metrics that are widely used by machine learning researchers to measure the performance of classification algorithms. Some of the popular metrics are accuracy, precision, recall, F-measure, AUC etc. However, one-class classification presents a unique scenario when target data is sufficiently present during training and negative data is not. Therefore during testing the OCC, we would expect to observe a highly skewed distribution of negative data w.r.t. the target data. In that sense, during testing, the problem becomes the evaluation of the performance of a classifier with severely imbalanced test set. Due to this nature of test data, conventional performance metrics (e.g. accuracy) may not be directly employed the numbers it produce may be misleading. Other measures such as, F-measure depends on precision and recall and if all the test data is classified as target or negative, then it can give NaN values to precision/recall. Kubat and\nMatwin [48] use the Geometric Mean (gmean) of accuracies measured separately on each class i.e. it combines True Positive Rate (TPR) and True Negative Rate (TNR), defined as\ngmean = \u221a TPR \u2217 TNR\nAn important property of gmean is that it is independent of the distribution of positive and negative samples in the test data. This measure is more useful in our application where we have a skewed distribution of outliers w.r.t. target and we want to evaluate the performance on both the target and outliers. It is to be noted that in the extreme case when all the test data is either classified as belonging to target or outlier, gmean will become 0."}, {"heading": "5.2. Datasets", "text": "There are no standard datasets available for testing OCC algorithms; therefore we test our methods on datasets that have high imbalance ratio of the number of positive and negative labelled data. This is done to highlight the OCC problem where the data of the negative class is rare. However, in our analysis, only the data from the positive class is used during training and parameter optimization and no data from negative class is used during training. The data from negative and positive class is available during testing. We show the results on 9 benchmark datasets from KEEL repository [49], and four domain specific real-world datasets, which are described below"}, {"heading": "5.2.1. KEEL Benchmark datasets", "text": "KEEL dataset repository provides several datasets for testing machine learning algorithms. We choose 12 datasets, specifically the ones that have real valued attributes and high imbalance ratio. The details of these datasets are shown in Table 1"}, {"heading": "5.2.2. German Aerospace Center (DLR) [50]", "text": "This dataset is collected using the XSens MTx sensor, which is an Inertial Measurement Unit (IMU) with integrated 3D magnetometers. It has an embedded processor capable of calculating the orientation of the sensor in real time, and returns calibrated 3D linear acceleration, turn rate and magnetic field data. The orientation information of the IMU can be obtained through the direction cosine matrix and the sample frequency is set to 100 Hz. The\ndataset contains samples from 19 people of both genders of different age groups. The data is recorded in indoor and outdoor environments under semi-natural conditions. The sensor is placed on the belt either on the right or the left side of the body or in the right pocket in different orientations. In total the dataset contains labelled data of over 4 hours and 30 minutes of the following 7 activities: Standing, Sitting, Lying, Walking (up/downstairs, horizontal), Running/Jogging, Jumping and Falling. Each sample in the dataset consists of a 9-dimensional vector that has 3 readings each for the accelerometer, gyroscope and magnetometer in the x, y and z directions. One of the subjects did not perform fall activity; therefore, their data is omitted from the analysis."}, {"heading": "5.2.3. MobiFall (MF) [51]", "text": "This dataset is collected using a Samsung Galaxy S3 mobile device with inertial module integrated with 3D accelerometer and gyroscope. The mobile device was placed in a trouser pocket freely chosen by the subjects in random orientations. For falls, the subjects placed the mobile phone in the pocket on the opposite side of falling direction. All falls were monitored to be done in specific way. The data stores the timestamp to facilitate any convenient subsampling; however, mean sampling of 87 Hz is reported for the accelerometer and 200Hz for the gyroscope. The dataset is collected from 11 subjects performing various normal and fall activities and 2 subjects only performing falls activity; therefore, they are removed from the analysis. The following 8 normal activities are recorded in this dataset: step-in car, step-out car, jogging, jumping, sitting, standing, stairs (up and down grouped together) and walking. Four different types of falls are recorded \u2013 forward lying, front knees lying, sideward lying and back sitting chair. These data from different types of falls are joined together to make one separate class for falls."}, {"heading": "5.2.4. Coventry Dataset (COV) [52]", "text": "This dataset is collected using two SHIMMERTMsensor nodes strapped to the chest and thighs of subjects that consists of a 3D accelerometer, 3D gyroscope, and a Bluetooth device [53]. The data was gathered at 100 Hz and transmitted to a remote PC and annotated. Two protocols were followed to collect data from subjects. In Protocol 1, data for four types of falls, near\nfalls1, falls induced by applying a lateral force and a set of ADL (standing, sitting, walking and lying) is collected. Protocol 2 involved ascending and descending stairs. 42 young healthy individuals simulated various ADL and fall scenarios, with 32 took part in Protocol 1 and 10 in Protocol 2. For Protocol 1, the activities were collected in a real-life circumstances where subject would make phone calls, read books, or talk to others while maintaining various postures. The following normal ADL were collected in Protocol 1 \u2013 standing, lying, sitting on a chair or bed, walking, crouching and near falls. Six types of fall scenarios are captured \u2013 forward, backward, right, left, real fall-backward and real fall forward. The data for real fall-backward/forward is collected by standing the subject on a wobble board while they are blindfolded and try to balance themselves, then they are pushed from behind/front to fall forward/backward onto a cushion and remain lying down for 10 seconds. These data from different types of falls are joined together to make one separate class for falls. The subjects for Protocol 2 did not record corresponding fall data; therefore, the data from Protocol 2 is not used. In our analysis, we used accelerometer and gyroscope data from the sensor node strapped to the chest.\nThese above datasets are pre-processed and 31 time-domain and frequencydomain features are computed as discussed in Khan [54]."}, {"heading": "5.2.5. Aging-related bugs and software complexity metrics", "text": "This dataset data contains information on the aging-related bugs found in the the Linux kernel and MySQL DBMS open source projects [55]. This dataset is meant to investigate defect prediction for aging-related bugs by using software complexity metric and machine learning techniques. The data repository contains several datasets and we choose two of them, namely: Linux Drivers (LD) and MySQL InnoDB (MI) . Both the datasets contains 82 attributes (plus one class label) related to program size, aging-related bugs and software complexity metrics. The LI dataset contains 2283 positive instances and 9 negative instances (imbalance ratio of 253.66), whereas the MI dataset contains 370 positive instances and 32 negative instances (imbalance ratio of 11.56). The reason to use this dataset is to motivate OCC applications, where the examples of the negative class are very less than the\n1\u201cNear-falls\u201d are events that occur as a result of stumbles, trips or collisions with obstacles, but do not necessarily result in falls\u201d [52].\nnumber of positive class, in this case defect is rare than normal functioning of the software. In the LI dataset, 31 attributes have zero values and they are removed. In the MI dataset, 30 attributes have zero values that are removed. One attribute has all zero values except at 2 instances; however during 5-fold cross-validation, all the instances with zero attribute value may appear and will cause problem in the normalization of the data. Therefore, this attribute is also removed and both the datasets are used with 51 attributes."}, {"heading": "5.2.6. Breast Cancer Data", "text": "The original Breast Cancer data is available at the UCI repository [14] and modified by the German Research Center of Artificial Intelligence [56] to be used for unsupervised outlier detection. The modified data has 30 attributes with 357 normal instances and 10 negative samples with an imbalanced ratio of 35.7. We used the modified breast cancer data for our analysis."}, {"heading": "5.3. Experimental Setup", "text": "To perform the experiment, we set the following values of the parameters\n\u2022 5-fold Outer Cross-Validation\n\u2022 2-fold Inner Cross-Validation (for parameter optimization)\n\u2022 The maximum number of J and K neighbours in OCNN to optimize is set to 10. For the DLR, MF and COV dataset, this number is set to 5 because we could not obtain results in reasonable time with large number of nearest neighbours.\n\u2022 Size of random subspace is set to 50% and 75% of the total size of attributes that are selected randomly.\n\u2022 Size of the ensemble for random subspace, random projection is set to 25 [57].\n\u2022 Rejection rate is set to 1.5, for some datasets it is manually decreased until all the classifiers are able to reject at least 5 instances from the positive class.\n\u2022 The data is normalized using the minmax method to lie within [0, 1] [58], i.e. zi = xi\u2212min(x)max(x)\u2212min(x) , where x = (x1, x2, . . . , xn), n is the number of training data and zi is the normalized value.\nWe performed two kinds of experiments with the above mentioned datasets2:\n1. To compare the performance of different types of single OCNN methods i.e. 11NN vs 11NN(\u03b8) vs JKNN .\n2. To compare the performance of different types of ensembles of OCCNN i.e. RP and RS ensemble for each OCNN method."}, {"heading": "5.4. Experimental Analysis", "text": ""}, {"heading": "5.4.1. KEEL Benchmark Datasets", "text": "The results of the first experiment on the KEEL benchmark datasets are presented in Table 2 under the column \u2018single\u2019 for each of the specific OCNN technique i.e. 11NN with optimized \u03b8, JKNN and 11NN . The results show that 11NN performed best (gmean) for 6 out of 9 datasets. We observe that, 11NN gave high values of TNR for all the datasets, whereas 11NN(\u03b8) and JKNN produced better TPR in comparison to 11NN . This means that the class boundary set by 11NN(\u03b8) and JKNN favours accepting test samples as member of target class resulting in fewer false positives, whereas the boundary for 11NN favours rejecting more samples as negatives resulting in fewer false negatives. Since gmean combines both TPR and TNR with equal weights, it gave a higher value of performance metric to 11NN . Alternatively, we can infer that 11NN(\u03b8) and JKNN shift the decision boundary such that the these classifiers are biased for the positive class. We believe that KEEL benchmark datasets are not good representatives for the one class problems. Two classes in the given problems may not be far apart and not necessarily demonstrate the outlier concept. Hence, a large number of negative class data points were predicted as member of the positive class by 11NN(\u03b8) and JKNN and a lot of positive samples were predicted as members of the negative class by 11NN . Due to this large misclassification (i.e. more false positives or false negatives), the overall values of gmean are small and none of these methods can be considered for robust classification for both the targets and unseen outliers. If this misclassification can be reduced, then the gmean values will improve, this result is described next.\nThe results of the second experiment on the KEEL benchmark datasets are presented in Table 2 under the columns \u2018RS(50)\u2019, \u2018RS(75)\u2019 and \u2018RP\u2019\n2The source code for the procedure presented in the paper for evaluating different OCNN methods is available at http://anonymous.now.for.review\nfor each of the specific OCNN technique. The results show that the RP ensembles of 11NN with optimized \u03b8, JKNN and 11NN always performed better (gmean) than their single counterparts. The performance advantage is more for 11NN(\u03b8) and JKNN . RP ensembles of 11NN(\u03b8) performed best for all datasets. Results suggest that RP ensembles show the TNR value equal to 1, which means that these ensembles predicted all the members of the negative class correctly. As discussed above the 11NN(\u03b8) and JKNN have large TPR as they are biased for positive class. The combination of high TPR and high TNR for RP ensembles of 11NN(\u03b8) and JKNN leads to high gmean values for these ensembles.\nRS ensembles did not perform well. For most of the cases they performed (gmean) worse than single classifier. Though RS ensembles of 11NN(\u03b8) and JKNN generally have better TPR than that of single 11NN(\u03b8) and single JKNN respectively, lower values of TNR for ensembles of 11NN(\u03b8) and JKNN lead to lower values of gmean for these ensembles. An accurate ensemble consists of accurate and diverse classifiers. The RS method creates diverse classifiers however these classifiers may not necessarily be accurate. The single version trained on all the features gave poor values for gmean. Training these classifiers with smaller amount of features (and creating an ensemble) still suffer from classification boundary bias and the ensemble did not improve the results.\nThe performance of RP ensembles are best among all ensemble methods for various OCNN approaches. It has been shown [59] that the performance of supervised KNN classifiers is not affected much in feature space created by RP. The less loss of information can be the reason for it. In our case, we are creating OCNN classifiers that have the same characteristics of supervised KNN classifiers. Hence, these OCNN classifiers trained on feature spaces created by RPs are accurate. Different RPs are creating diverse classifiers. The combination of these accurate and diverse classifiers are helping RP ensembles to correctly predict the boundary line negative class points. 11NN(\u03b8 = 1) may not work appropriately because it depends on the choice of \u03b8 chosen empirically from training data. Since training data is limited, we can get sub-optimal choice for it. Therefore, in practice, the results of 11NN(\u03b8 = 1) may be different than JKNN with \u03b8=1.\nOne of the most important parts of the proposed methodology is finding the optimized values of \u03b8 for 11NN(\u03b8) and, J and K for JKNN . We observed that generally the optimized values of the \u03b8 for 11NN(\u03b8) were between 1 and 2. The optimal values of J were between 1 and 3 whereas\nthe optimized values of K were always 10 (the maximum set value in the experiment). These values verify our discussion in Section 3.2 that 11NN(\u03b8) with \u03b8 > 1, 1KNN and JKNN (for smaller values of J) have similar decision boundaries."}, {"heading": "5.4.2. Domain-Specific Real Datasets", "text": "The results of the first experiment on the MobiFall, DLR, Coventry, Linux-driver, MySQL-IDB and Breast-Cancer datasets are shown in Table 3 under the column \u2018single\u2019 for each of the specific OCNN technique i.e. 11NN with optimized \u03b8, JKNN and 11NN . Similar to the results on KEEL benchmark datasets, we observe that JKNN and 11NN(\u03b8) are more biased to the positive class than the 11NN , thus resulting in higher TPR at the cost of lower TNR. However, in these datasets, single JKNN always performed better than single 11NN in terms of gmean. For three datasets (DLR, Coventry and Breast-Cancer) single 11NN(\u03b8) performed better than single 11NN . These datasets better represent one class classification problems and the negative class may lie in low-density regions; however, we notice that 11NN(\u03b8) is most biased towards the positive class than the other two methods. This means that it gave highest TPR but at the cost of missing to identify negative samples in the test set (low TNR) leading to low gmean values. Nonetheless, JKNN and 11NN(\u03b8) show better performance against 11NN . In 11NN(\u03b8) method, the value 11NN(\u03b8) is calculated from the training dataset, we believe that 11NN(\u03b8) has overfitting problem, hence for some datasets 11NN perform better than 11NN(\u03b8).\nThe results of the second experiment on the MobiFall, DLR, Coventry, Linux-driver, MySQL-IDB and Breast-Cancer datasets are shown in Table 3 under the column \u2018RS(50)\u2019, \u2018RS(75)\u2019 and \u2018RP\u2019 for each of the specific OCNN technique i.e. 11NN with optimized \u03b8, JKNN and 11NN . The results about the ensembles suggest that for four datasets (MobiFall, DLR, Coventry, and Breast-Cancer) RP ensembles of 11NN(\u03b8) perform the best whereas for other two datasets (Linux-driver and MySQL-IDB) RP ensembles of JKNN perform best in terms of gmean. The RP ensembles for 11NN(\u03b8), JKNN and 11NN always gave high TNR and comparable TPR in comparison RS ensembles. Combining both these results in high gmean for RP ensembles in comparison to RS ensemble. The member classifiers in RP ensembles of 11NN(\u03b8) and JKNN are created independently. As ensembles in which classifiers are created independently are robust to overfitting problem we expect that RP ensembles of 11NN(\u03b8) and JKNN are robust to overfitting\nTa bl e 2:\nR es ul ts\nfo r di ffe\nre nt\nO C N N\nm et ho\nds (s in gl e an\nd en se m bl es ) fo r K EE\nL da\nta se ts .\nBo ld\nnu m be r sh ow th e be st re su lts . D at as et 11 N N w it h op ti m iz ed \u03b8 J K N N 11 N N m ea su re Si ng le R S( 50 ) R S( 75 ) R P Si ng le R S( 50 ) R S( 75 ) R P Si ng le R S( 50 ) R S( 75 ) R P G la ss 2 g m ea n 0. 22 8( 0. 33 4) 0. 30 9( 0. 28 6) 0. 33 8( 0. 32 6) 0. 89 4( 0. 04 2) 0. 32 8( 0. 31 8) 0. 19 8( 0. 27 2) 0. 32 6( 0. 32 1) 0. 87 6( 0. 04 3) 0. 40 3( 0. 29 6) 0. 52 8( 0. 12 8) 0. 40 0( 0. 27 6) 0. 71 9( 0. 06 0) T P R 0. 78 0( 0. 10 3) 0. 87 2( 0. 06 7) 0. 81 6( 0. 04 6) 0. 80 1( 0. 07 5) 0. 66 2( 0. 21 2) 0. 89 7( 0. 05 1) 0. 78 5( 0. 09 4) 0. 77 0( 0. 07 7) 0. 50 0( 0. 11 0) 0. 55 0( 0. 07 3) 0. 43 8( 0. 05 6) 0. 52 0( 0. 08 6) T N R 0. 18 3( 0. 29 1) 0. 18 3( 0. 17 0) 0. 25 0( 0. 27 6) 1. 00 0( 0. 00 0) 0. 38 3( 0. 43 9) 0. 11 6( 0. 16 2) 0. 25 0( 0. 27 6) 1. 00 0( 0. 00 0) 0. 43 3( 0. 39 7) 0. 51 6( 0. 19 0) 0. 46 6( 0. 40 2) 1. 00 0( 0. 00 0) G la ss 4 g m ea n 0. 29 1( 0. 27 7) 0. 37 4( 0. 36 0) 0. 59 3( 0. 37 8) 0. 87 6( 0. 18 4) 0. 47 8( 0. 32 7) 0. 59 5( 0. 35 1) 0. 57 9( 0. 39 9) 0. 83 1( 0. 24 8) 0. 48 0( 0. 34 3) 0. 64 3( 0. 19 8) 0. 62 5( 0. 18 7) 0. 60 1( 0. 23 0) T P R 0. 69 5( 0. 33 0) 0. 88 0( 0. 22 7) 0. 89 5( 0. 08 1) 0. 79 5( 0. 27 8) 0. 71 0( 0. 34 4) 0. 83 0( 0. 29 8) 0. 75 5( 0. 31 0) 0. 74 0( 0. 33 1) 0. 42 0( 0. 24 0) 0. 50 0( 0. 21 5) 0. 42 0( 0. 20 5) 0. 40 5( 0. 23 3) T N R 0. 33 3( 0. 40 8) 0. 30 0( 0. 29 8) 0. 53 3( 0. 44 7) 1. 00 0( 0. 00 0) 0. 53 3( 0. 38 0) 0. 60 0( 0. 36 5) 0. 66 6( 0. 47 14 ) 1. 00 0( 0. 00 0) 0. 80 0( 0. 44 7) 0. 90 0( 0. 22 3) 1. 00 0( 0. 00 0) 1. 00 0( 0. 00 0) G la ss 5 g m ea n 0. 57 6( 0. 32 6) 0. 13 9( 0. 31 2) 0. 13 7( 0. 30 8) 0. 91 3( 0. 09 7) 0. 59 0( 0. 11 6) 0. 41 3( 0. 37 7) 0. 39 9( 0. 36 4) 0. 82 1( 0. 24 4) 0. 59 6( 0. 15 9) 0. 54 6( 0. 30 9) 0. 61 4( 0. 17 9) 0. 62 6( 0. 18 4) T P R 0. 84 2( 0. 14 9) 0. 92 5( 0. 11 3) 0. 89 1( 0. 05 1) 0. 84 1( 0. 16 7) 0. 68 8( 0. 30 8) 0. 83 5( 0. 27 4) 0. 75 7( 0. 31 4) 0. 72 2( 0. 32 4) 0. 42 9( 0. 17 5) 0. 47 8( 0. 21 0) 0. 46 4( 0. 21 9) 0. 42 0( 0. 19 8) T N R 0. 50 0( 0. 35 3) 0. 10 0( 0. 22 3) 0. 10 0( 0. 22 3) 1. 00 0( 0. 00 0) 0. 60 0( 0. 22 3) 0. 30 0( 0. 27 3) 0. 30 0( 0. 27 3) 1. 00 0( 0. 00 0) 0. 90 0( 0. 22 3) 0. 70 0( 0. 44 7) 0. 90 0( 0. 22 36 ) 1. 00 0( 0. 00 0) W in e1 g m ea n 0. 44 8( 0. 25 1) 0. 35 0( 0. 19 6) 0. 36 2( 0. 21 2) 0. 93 3( 0. 02 2) 0. 55 9( 0. 14 3) 0. 59 5( 0. 12 0) 0. 62 9( 0. 05 7) 0. 85 6( 0. 03 5) 0. 48 6( 0. 04 9) 0. 52 0( 0. 03 3) 0. 45 9( 0. 04 8) 0. 50 3( 0. 05 6) T P R 0. 71 4( 0. 08 5) 0. 96 0( 0. 01 9) 0. 85 4( 0. 03 8) 0. 87 1( 0. 04 1) 0. 63 1( 0. 06 5) 0. 90 7( 0. 03 7) 0. 71 1( 0. 03 1) 0. 73 4( 0. 06 1) 0. 31 3( 0. 03 8) 0. 32 4( 0. 04 6) 0. 23 1( 0. 04 11 ) 0. 25 6( 0. 05 6) T N R 0. 36 0( 0. 21 9) 0. 16 0( 0. 08 9) 0. 20 0( 0. 14 1) 1. 00 0( 0. 00 0) 0. 52 0( 0. 22 8) 0. 40 (0 .1 41 ) 0. 56 0( 0. 08 9) 1. 00 0( 0. 00 0) 0. 76 0( 0. 08 9) 0. 84 0( 0. 08 9) 0. 92 0( 0. 10 9) 1. 00 0( 0. 00 0) W in e2 g m ea n 0. 37 9( 0. 21 5) 0. 11 5( 0. 25 7) 0. 48 9( 0. 29 5) 0. 94 6( 0. 04 0) 0. 50 3( 0. 29 1) 0. 11 2( 0. 25 1) 0. 45 9( 0. 27 7) 0. 86 0( 0. 04 7) 0. 47 6( 0. 26 9) 0. 45 2( 0. 09 4) 0. 41 6( 0. 23 7) 0. 57 2( 0. 06 0) T P R 0. 73 3( 0. 10 6) 0. 97 2( 0. 03 2) 0. 85 6( 0. 08 2) 0. 89 7( 0. 07 5) 0. 61 5( 0. 14 5) 0. 91 1( 0. 04 0) 0. 70 9( 0. 05 6) 0. 74 1( 0. 08 1) 0. 36 9( 0. 05 9) 0. 37 2( 0. 03 6) 0. 31 7( 0. 01 2) 0. 33 0( 0. 06 8) T N R 0. 23 3( 0. 13 6) 0. 06 6( 0. 14 9) 0. 35 0( 0. 25 2) 1. 00 (0 .0 00 ) 0. 55 0( 0. 38 9) 0. 06 6( 0. 14 91 ) 0. 38 3( 0. 27 3) 1. 00 0( 0. 00 0) 0. 75 0( 0. 43 3) 0. 56 6( 0. 19 9) 0. 68 3( 0. 40 99 ) 1. 00 0( 0. 00 0) W in e3 g m ea n 0. 30 4( 0. 43 5) 0. 33 4( 0. 46 8) 0. 45 6( 0. 42 9) 0. 94 0( 0. 02 0) 0. 40 7( 0. 38 9) 0. 54 0( 0. 30 2) 0. 61 7( 0. 04 4) 0. 87 6( 0. 03 6) 0. 51 2( 0. 11 9) 0. 54 6( 0. 06 9) 0. 52 5( 0. 05 4) 0. 53 3( 0. 05 3) T P R 0. 77 2( 0. 13 43 ) 0. 97 0( 0. 01 6) 0. 90 1( 0. 02 4) 0. 88 5( 0. 03 9) 0. 73 2( 0. 05 6) 0. 91 9( 0. 03 2) 0. 76 6( 0. 10 6) 0. 76 9( 0. 06 4) 0. 33 3( 0. 05 0) 0. 34 1( 0. 04 4) 0. 27 7( 0. 05 5) 0. 28 6( 0. 05 5) T N R 0. 30 0( 0. 44 7) 0. 30 0( 0. 44 7) 0. 40 0( 0. 41 8) 1. 00 0( 0. 00 0) 0. 40 0( 0. 41 8) 0. 40 0( 0. 22 3) 0. 50 0( 0. 00 0) 1. 00 0( 0. 00 0) 0. 80 0( 0. 27 3) 0. 90 0( 0. 22 3) 1. 00 0( 0. 00 0) 1. 00 (0 .0 00 ) Y ea st 4 g m ea n 0. 54 6( 0. 10 7) 0. 00 0( 0. 00 0) 0. 21 1( 0. 19 9) 0. 98 3( 0. 00 6) 0. 53 8( 0. 16 7) 0. 14 7( 0. 20 5) 0. 52 0( 0. 12 8) 0. 94 5( 0. 00 8) 0. 52 6( 0. 10 3) 0. 28 7( 0. 17 3) 0. 54 1( 0. 06 6) 0. 68 2( 0. 02 5) T P R 0. 81 9( 0. 07 3) 0. 99 3( 0. 00 8) 0. 96 0( 0. 02 3) 0. 96 7( 0. 01 2) 0. 81 3( 0. 04 1) 0. 98 3( 0. 01 3) 0. 90 7( 0. 03 4) 0. 89 3( 0. 01 5) 0. 46 3( 0. 02 8) 0. 90 1( 0. 01 1) 0. 53 7( 0. 02 4) 0. 46 6( 0. 03 4) T N R 0. 37 6( 0. 13 6) 0. 00 0( 0. 00 0) 0. 08 0( 0. 08 3) 1. 00 0( 0. 00 0) 0. 37 8( 0. 18 2) 0. 05 6( 0. 08 2) 0. 31 4( 0. 13 2) 1. 00 0( 0. 00 0) 0. 61 45 (0 .2 03 ) 0. 11 8( 0. 08 4) 0. 55 2( 0. 12 7) 1. 00 0( 0. 00 0) Y ea st 5 g m ea n 0. 32 9( 0. 22 7) 0. 16 4( 0. 22 8) 0. 36 6( 0. 22 2) 0. 98 0( 0. 01 1) 0. 43 1( 0. 16 0) 0. 15 9( 0. 22 3) 0. 43 0( 0. 17 6) 0. 94 2( 0. 02 4) 0. 54 1( 0. 09 7) 0. 62 0( 0. 08 2) 0. 53 6( 0. 10 8) 0. 64 6( 0. 01 8) T P R 0. 84 0( 0. 05 3) 0. 98 6( 0. 01 3) 0. 95 2( 0. 02 4) 0. 96 1( 0. 02 2) 0. 81 1( 0. 04 9) 0. 97 1( 0. 01 3) 0. 91 3( 0. 02 1) 0. 88 8( 0. 04 5) 0. 42 1( 0. 03 0) 0. 80 8( 0. 03 8) 0. 46 0( 0. 02 7) 0. 41 8( 0. 02 3) T N R 0. 17 7( 0. 16 8) 0. 06 9( 0. 10 1) 0. 18 3( 0. 12 9) 1. 00 0( 0. 00 0) 0. 25 0( 0. 18 2) 0. 06 6( 0. 09 9) 0. 22 7( 0. 19 2) 1. 00 0( 0. 00 0) 0. 70 8( 0. 22 7) 0. 48 0( 0. 11 2) 0. 63 6( 0. 21 3) 1. 00 0( 0. 00 0) Y ea st 6 g m ea n 0. 33 5( 0. 22 3) 0. 00 0( 0. 00 0) 0. 00 0( 0. 00 0) 0. 98 1( 0. 01 0) 0. 36 1( 0. 23 8) 0. 00 0( 0. 00 0) 0. 07 1( 0. 16 0) 0. 94 0( 0. 02 3) 0. 46 2( 0. 06 5) 0. 07 2( 0. 16 2) 0. 44 0( 0. 07 1) 0. 64 8( 0. 01 7) T P R 0. 82 3( 0. 07 3) 0. 99 3( 0. 00 7) 0. 95 9( 0. 02 2) 0. 96 4( 0. 01 9) 0. 80 6( 0. 05 8) 0. 99 1( 0. 00 9) 0. 90 1( 0. 02 7) 0. 88 3( 0. 04 3) 0. 42 9( 0. 03 8) 0. 93 6( 0. 02 6) 0. 53 4( 0. 02 0) 0. 42 1( 0. 02 2) T N R 0. 20 0( 0. 21 67 ) 0. 00 0( 0. 00 0) 0. 00 0( 0. 00 0) 1. 00 0( 0. 00 0) 0. 22 8( 0. 21 6) 0. 00 0( 0. 00 0) 0. 02 86 (0 .0 63 ) 1. 00 0( 0. 00 0) 0. 51 4( 0. 16 2) 0. 02 8( 0. 06 3) 0. 37 1( 0. 12 7) 1. 00 0( 0. 00 0)\nproblem. This could be one of the reasons for the accurate RP ensembles of 11NN(\u03b8) and JKNN ."}, {"heading": "6. Conclusions and Future Work", "text": "In this paper, presented a theoretical analysis to demonstrate the equivalence and relationship among different variants of OCNN. We further enhance the discriminatory power of OCNN classifier by creating accurate ensembles of different variants of OCNN. We presented a technique to optimize the parameters for the proposed OCNN that only uses data from the target class as the data for the negative data may be absent during the training phase. We tested the proposed methods on 15 benchmark and domain specific datasets and show that RP ensembles of JKNN and 11NN(\u03b8) are highly accurate, specially in identifying the unseen negative samples and having a good balance in identifying the members of the target class. The RP ensembles of 11NN(\u03b8) are quite accurate; the RP ensembles of JKNN are not far behind. The results also give definitive evidence that single OCNN classifiers are not a good choice for dealing with one-class classification problems when nearest neighbour approach is employed, rather their ensembles perform better. The success of these ensembles is attributed to two facts; (i) the proposed optimization method, accurate 11NN(\u03b8) and JKNN models are created and (ii) RP ensemble methods create diverse 11NN(\u03b8) and JKNN models. The combination of these diverse and accurate models create successful RP ensembles of 11NN(\u03b8) and JKNN . Thorough these experiments, we also found that finding the optimal values of J and K is a time consuming task in JKNN approach. However, on the basis of our experiments, we can suggest that only low values of J with high values of K could be used as a good choice for classifying unseen samples, especially when the negative concept is different from the the target class. In future, we would like to use kernel as a distance measure for OCNN and will study the effects of their ensembles."}], "references": [], "referenceMentions": [], "year": 2017, "abstractText": "In one-class classification (OCC) problems, only the data for the target class<lb>is available, whereas the data for the non-target class may be completely ab-<lb>sent. In this paper, we study one-class nearest neighbour (OCNN) classifiers<lb>and their different variants for the OCC problem. We present a theoreti-<lb>cal analysis to show the equivalence among different variants of OCNN that<lb>may use different neighbours or thresholds to identify unseen examples of<lb>the non-target class. We also present a method based on inter-quartile range<lb>for optimizing parameters used in OCNN in the absence of non-target data<lb>during training. Then, we propose to use two ensemble approaches based on<lb>random sub-space and random projection approaches to create accurate en-<lb>semble that significantly outperforms the baseline OCNN. We tested the pro-<lb>posed methods on various benchmark and real word domain-specific datasets<lb>to show their superior performance. The results give strong evidence that the<lb>random projection ensemble of the proposed OCNN with optimized param-<lb>eters variants perform significantly and consistently better than the single<lb>OCC on all the tested datasets.", "creator": "LaTeX with hyperref package"}}}