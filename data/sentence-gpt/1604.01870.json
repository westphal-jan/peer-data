{"id": "1604.01870", "review": {"conference": "NIPS", "VERSION": "v1", "DATE_OF_SUBMISSION": "7-Apr-2016", "title": "Efficient Globally Convergent Stochastic Optimization for Canonical Correlation Analysis", "abstract": "We study the stochastic optimization of canonical correlation analysis (CCA), whose objective is nonconvex and does not decouple over training samples. Although several stochastic optimization algorithms have been recently proposed to solve this problem, no global convergence guarantee was provided by any of them. Based on the alternating least squares formulation of CCA, we propose a globally convergent stochastic algorithm, which solves the resulting least squares problems approximately to sufficient accuracy with state-of-the-art stochastic gradient methods for convex optimization in the canonical correlations analysis (CSA). For each optimization procedure, we apply a weighted probability of positive effects in the canonical correlations analysis, which is not as effective as the generalized stochastic reduction technique. We propose the best way to perform this function by using a weighting model to model the posterior distribution, which is the equivalent of the Gaussian distribution for the number of positive effects. We propose a nonlinear gradient method to optimize the canonical correlations analysis for each evaluation in the canonical correlations analysis using a weighted probability of positive effects.\n\n\n\n\nThis paper shows that this approach can generate strong nonlinear correlation analysis in an even deeper context. Using a weighting model to model the posterior distribution, our approach can generate robust nonlinear correlation analysis in the canonical correlations analysis. The most common and most common correlations to evaluate the canonical correlations analysis (CSA), which does not include the nonlinear or nonlinear coefficients, are given in the following ways.\n\nIn contrast, we define the linear correlation analysis as a standard statistical procedure with several standard models that are used in multiple linear comparisons. For example, our method will generate a standard linear correlation analysis with the nonlinear coefficients, which can be used to compare the observed correlations between the predicted and predicted values.\n\nThe best way to measure this data is in a simplified form. For example, we consider the posterior distributions for all posterior distributions, and, in our study, use this measure as an independent method. For the posterior distribution, the posterior distribution and the nonlinear correlation coefficients are defined as:\nwhere \\(p \\overall P \\overall P \\overall P \\overall P \\overall P \\overall P \\overall P \\overall P \\overall P \\overall P \\overall P \\overall P \\overall P \\overall P \\overall P \\overall P \\overall P \\overall P \\overall P \\overall P \\overall P", "histories": [["v1", "Thu, 7 Apr 2016 04:14:54 GMT  (478kb)", "http://arxiv.org/abs/1604.01870v1", null], ["v2", "Wed, 20 Apr 2016 17:58:52 GMT  (461kb)", "http://arxiv.org/abs/1604.01870v2", null], ["v3", "Fri, 20 May 2016 03:09:29 GMT  (198kb)", "http://arxiv.org/abs/1604.01870v3", null], ["v4", "Mon, 14 Nov 2016 18:11:15 GMT  (648kb)", "http://arxiv.org/abs/1604.01870v4", "Accepted by NIPS 2016"]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["weiran wang", "jialei wang", "dan garber", "nati srebro"], "accepted": true, "id": "1604.01870"}, "pdf": {"name": "1604.01870.pdf", "metadata": {"source": "CRF", "title": "Globally Convergent Stochastic Optimization for Canonical Correlation Analysis", "authors": ["Weiran Wang", "Jialei Wang"], "emails": ["weiranwang@ttic.edu", "jialei@uchicago.edu", "nati@ttic.edu"], "sections": [{"heading": null, "text": "ar X\niv :1\n60 4.\n01 87\n0v 1\n[ cs\n.L G\n] 7\nA pr"}, {"heading": "1 Introduction", "text": "Canonical correlation analysis (CCA, [1]) and its extensions are ubiquitous techniques in scientific research areas for revealing the common sources of variability in multiple views of the same phenomenon. In CCA, the trainng set consists of paired observations from two views, denoted (x1,y1), . . . , (xN ,yN ), where N is the training set size, xi \u2208 Rdx and yi \u2208 Rdy for i = 1, . . . , N . We also denote the data matrices for each view1 by X = [x1, . . . ,xN ] \u2208 Rdx\u00d7N and Y = [y1, . . . ,yN ] \u2208 Rdy\u00d7N . The objective of (regularized) linear CCA is to find linear projections of each view such that the correlation between the projections is maximized:\nmax u,v\nu\u22a4\u03a3xyv (1)\ns.t. u\u22a4\u03a3xxu = v\u22a4\u03a3yyv = 1\nwhere \u03a3xy = 1NXY \u22a4 is the cross-covariance matrix, \u03a3xx = 1NXX \u22a4+\u03b3xI and \u03a3yy = 1NYY \u22a4+ \u03b3yI are the auto-covariance matrices, and (\u03b3x, \u03b3y) \u2265 0 are regularization parameters [2, 3]. We denote by (u\u2217,v\u2217) the global optimum of (1), which is unique up to a change of sign.\nThe CCA objective has a closed form solution as follows. Define\nT := \u03a3 \u2212 1\n2 xx \u03a3xy\u03a3\n\u2212 1 2\nyy \u2208 Rdx\u00d7dy , (2) and let (\u03c6,\u03c8) be the (unit-length) left and right singular vector pair associated with T\u2019s largest singular value \u03c11. Then the optimal objective value, i.e., the canonical correlation between the views, is \u03c11, achieved by (u\u2217, v\u2217) = (\u03a3 \u2212 1 2 xx \u03c6, \u03a3 \u2212 1 2 yy \u03c8). Note that\n\u03c11 = \u2016T\u2016 \u2264 \u2225 \u2225 \u2225 \u03a3 \u2212 1 2 xx X \u2225 \u2225 \u2225 \u2225 \u2225 \u2225 \u03a3 \u2212 1 2 yy Y \u2225 \u2225 \u2225 \u2264 1.\n\u2217The first two authors contributed equally. 1We assume that X and Y are centered at the origin for notational simplicity; if they are not, we can center\nthem as a pre-processing operation.\nAlgorithm 1 Alternating least squares for CCA.\nInput: Data matrices X \u2208 Rdx\u00d7N , Y \u2208 Rdy\u00d7N , regularization parameters (\u03b3x, \u03b3y). Initialize u\u03030 \u2208 Rdx , v\u03030 \u2208 Rdy . { \u03c6\u03030, \u03c8\u03030 }\nu0 \u2190 u\u03030/ \u221a u\u0303\u22a40 \u03a3xxu\u03030\n{ \u03c60 \u2190 \u03c6\u03030/ \u2225 \u2225 \u2225 \u03c6\u03030 \u2225 \u2225 \u2225 }\nv0 \u2190 v\u03030/ \u221a v\u0303\u22a40 \u03a3yyv\u03030\n{ \u03c80 \u2190 \u03c8\u03030/ \u2225 \u2225 \u2225 \u03c8\u03030 \u2225 \u2225 \u2225 }\nfor t = 1, 2, . . . , T do\nu\u0303t \u2190 \u03a3\u22121xx\u03a3xyvt\u22121 { \u03c6\u0303t \u2190 \u03a3 \u2212 1 2 xx \u03a3xy\u03a3 \u2212 1 2 yy \u03c8t\u22121 } v\u0303t \u2190 \u03a3\u22121yy \u03a3\u22a4xyut\u22121 { \u03c8\u0303t \u2190 \u03a3 \u2212 1 2 yy \u03a3 \u22a4 xy\u03a3 \u2212 1 2 xx \u03c6t\u22121 }\nut \u2190 u\u0303t/ \u221a u\u0303\u22a4t \u03a3xxu\u0303t\n{ \u03c6t \u2190 \u03c6\u0303t/ \u2225 \u2225 \u2225 \u03c6\u0303t \u2225 \u2225 \u2225 }\nvt \u2190 v\u0303t/ \u221a v\u0303\u22a4t \u03a3yyv\u0303t\n{ \u03c8t \u2190 \u03c8\u0303t/ \u2225 \u2225 \u2225 \u03c8\u0303t \u2225 \u2225 \u2225 }\nend for Output: (uT ,vT ) \u2192 (u\u2217,v\u2217) as T \u2192 \u221e. {(\u03c6T ,\u03c8T ) \u2192 (\u03c6,\u03c8)}\nFurthermore, we are guaranteed to have \u03c11 < 1 if (\u03b3x, \u03b3y) > 0.\nFor large high dimensional datasets, it is time and memory consuming to first explicitly form the matrix T (which requires eigenvalue decomposition of the covariance matrices) and then compute its singular value decomposition (SVD). For such problems, it is desirable to develop stochastic algorithms that have efficient updates, converges fast, and can take advantage of the input sparsity. There have been recent attempts to solve (1) based on stochastic gradient descent (SGD) methods [4, 5, 6], but none of these work provides rigorous convergence analysis for their stochastic CCA algorithms.\nFurther assume T has a positive gap between its first and second singular values \u03c11 and \u03c12. The main result of this paper is the proof of the following theorem.\nTheorem 1.1 (Main result). Fix \u03b7 > 0, and assume the initialization (u0,v0) satisfies u\u22a40 \u03a3xxu \u2217 6= 0 and v\u22a40 \u03a3yyv \u2217 6= 0. Then there exists an algorithm that starts with (u0,v0) and finds an approximate solution (u,v) to (1) such that u\u22a4\u03a3xxu = v\u22a4\u03a3yyv = 1, and\nmin ( (u\u22a4\u03a3xxu \u2217)2, (v\u22a4\u03a3yyv \u2217)2 ) \u2265 1\u2212 \u03b7, u\u22a4\u03a3xyv \u2265 \u03c11(1\u2212 2\u03b7) in total time\nO\u0303 ( d (N + \u03ba) \u03c121\n\u03c121 \u2212 \u03c122\n)\n,\nwhere d = dx+dy, \u03ba = max ( maxi\u2016xi\u2016 2\n\u03c3min(\u03a3xx) , maxi\u2016yi\u2016\n2\n\u03c3min(\u03a3yy)\n)\n, and the notation O\u0303(\u00b7) hides poly-logarithmic dependencies on d and 1/\u03b7."}, {"heading": "2 Alternating least squares", "text": "Our solution to (1) is inspired by the alternating least squares formulation of CCA [7, Algorithm 5.2], as shown in Algorithm 1. A similar algorithm is recently used by [8] for large scale linear CCA with high dimensional sparse inputs, and by [4, 5] for motivating stochastic CCA algorithms.\nLet the nonzero singular values of T be 1 \u2265 \u03c11 \u2265 \u03c12 \u2265 \u00b7 \u00b7 \u00b7 \u2265 \u03c1r > 0, where r = rank(T) \u2264 min(dx, dy), and the corresponding (unit-length) left and right singular vector pairs be (a1,b1), . . . , (ar,br), with a1=\u03c6 and b = \u03c8. Now define\nC =\n[\n0 T T\u22a4 0\n]\n\u2208 R(dx+dy)\u00d7(dx+dy).\nIt is straightforward to check that the nonzero eigenvalues of C are:\n\u03c11 \u2265 \u03c12 \u2265 \u00b7 \u00b7 \u00b7 \u2265 \u03c1r \u2265 \u2212\u03c1r \u2265 \u00b7 \u00b7 \u00b7 \u2265 \u2212\u03c11,\nwith corresponding eigenvectors [\na1 b1\n]\n, . . . ,\n[\nar br\n]\n,\n[\nar \u2212br\n]\n, . . . ,\n[\na1 \u2212b1\n]\n.\nThe key observation is that Algorithm 1 effectively runs a variant of power iterations on C to extract\nits top eigenvector\n[\n\u03c6 \u03c8\n]\n. To see this, make the change of variables \u03c6t = \u03a3 1 2 xxut, \u03c8t = \u03a3 1 2 yyvt,\nand similarly for their unnormalized version \u03c6\u0303t = \u03a3 1 2 xxu\u0303t, \u03c8\u0303t = \u03a3 1 2 yyv\u0303t. Then we can equivalently rewrite the steps of Algorithm 1 in terms of the new variables as in {} of each line. Observe that the iterates are updated as follows from step t\u2212 1 to step t:\n[\n\u03c6\u0303t \u03c8\u0303t\n] \u2190 [ 0 T\nT\u22a4 0\n] [\n\u03c6t\u22121 \u03c8t\u22121\n]\n,\n[\n\u03c6t \u03c8t\n] \u2190 [ \u03c6\u0303t/||\u03c6\u0303t|| \u03c8\u0303t/||\u03c8\u0303t|| ] . (3)\nTherefore, except for the special normalization steps (which rescale the two sets of variables separately), Algorithm 1 is very similar to the power iterations [9].\nWe now prove the convergence of this \u201cpower iterations\u201d. The first natural measure of progress is the objective of (1), i.e., u\u22a4t \u03a3xyvt, with the maximum value being \u03c11. Another measure is the alignment of \u03c6t to \u03c6, and the alignment of \u03c8t to \u03c8, i.e., (\u03c6 \u22a4 t \u03c6) 2 = (u\u22a4t \u03a3xxu \u2217)2 and (\u03c8\u22a4t \u03c8)\n2 = (v\u22a4t \u03a3yyv\n\u2217)2. The maximum value for such alignments is 1, achieved when the iterates completely align with the optimal solution.\nTheorem 2.1 (Convergence of Algorithm 1). Let \u00b5 := min ( (u\u22a40 \u03a3xxu \u2217)2, (v\u22a40 \u03a3yyv \u2217)2 ) > 0. Then for t \u2265 \u2308 \u03c1 2 1\n\u03c12 1 \u2212\u03c12 2\nlog (\n1 \u00b5\u03b7\n)\n\u2309, we have\nmin ( (u\u22a4t \u03a3xxu \u2217)2, (v\u22a4t \u03a3yyv \u2217)2 ) \u2265 1\u2212 \u03b7,\nand u\u22a4t \u03a3xyvt \u2265 \u03c11(1 \u2212 2\u03b7).\nRemarks We have assumed a nonzero singular value gap in Theorem 2.1 to obtain linear convergence in both the alignments and the objective. When there exists no singular value gap, the top singular vector pair is not unique and it is no longer meaningful to measure the alignments. Nonetheless, it is possible to extend our proof to obtain sublinear convergence for the objective in this case.\nObserve that, besides the steps of normalization to unit length, the basic operation in each iteration of Algorithm 1 is of the form u\u0303t \u2190 \u03a3\u22121xx\u03a3xyvt\u22121 = ( 1NXX\u22a4 + \u03b3xI)\u22121 1NXY\u22a4vt\u22121, which is equivalent to solving the following regularized least squares (ridge regression) problem\nmin u\n1\n2N\n\u2225 \u2225u\u22a4X\u2212 v\u22a4t\u22121Y \u2225 \u2225 2 + \u03b3x 2 \u2016u\u20162 = 1 N\nN \u2211\ni=1\n1\n2\n\u2223 \u2223u\u22a4xi \u2212 v\u22a4t\u22121yi \u2223 \u2223 2 + \u03b3x 2 \u2016u\u20162 , (4)\nwhich has the finite sum structure. In the next section, we show that, to maintain the convergence rate of power iterations, it is unnecessary to solve the least squares problem exactly. This enables us to use state-of-art stochastic gradient descent methods for solving the subproblem (4) to sufficient accuracy, and to obtain a globally convergent stochastic algorithm for CCA."}, {"heading": "3 Our algorithm", "text": "Our algorithm consists of two nested loops. The outer loop runs inexact power iterations while the inner loop uses advanced stochastic optimization methods, e.g., stochastic variance reduced gradient (SVRG, [10]) to obtain approximate matrix/vector product for power iterations. A sketch of our algorithm is provided in Algorithm 2.\nWe make the following observations from Algorithm 2.\nAlgorithm 2 Stochastic optimization of CCA.\nInput: Data matrices X \u2208 Rdx\u00d7N , Y \u2208 Rdy\u00d7N , regularization parameters (\u03b3x, \u03b3y). Initialize u\u03030 \u2208 Rdx , v\u03030 \u2208 Rdy . u0 \u2190 u\u03030/ \u221a u\u0303\u22a40 \u03a3xxu\u03030, v0 \u2190 v\u03030/ \u221a v\u0303\u22a40 \u03a3yyv\u03030\nfor t = 1, 2, . . . , T do Solve the least squares problem\nmin u\nft(u) := 1\n2N\n\u2225 \u2225u\u22a4X\u2212 v\u22a4t\u22121Y \u2225 \u2225 2 + \u03b3x 2 \u2016u\u20162\nwith SVRG and output approximate solution u\u0303t satisfying ft(u\u0303t) \u2264 minu ft(u) + \u01eb. Solve the least squares problem\nmin v\ngt(v) := 1\n2N\n\u2225 \u2225v\u22a4Y \u2212 u\u22a4t\u22121X \u2225 \u2225 2 + \u03b3y 2 \u2016v\u20162\nwith SVRG and output approximate solution v\u0303t satisfying gt(v\u0303t) \u2264 minv gt(v) + \u01eb. ut \u2190 u\u0303t/ \u221a u\u0303\u22a4t \u03a3xxu\u0303t vt \u2190 v\u0303t/ \u221a v\u0303\u22a4t \u03a3yyv\u0303t\nend for Output: (uT ,vT ) is the approximate solution to CCA.\n\u2022 Connection to previous work At step t, if we optimize ft(u) and gt(v) crudely by a single batch gradient descent step from the initialization (u\u0303t\u22121, v\u0303t\u22121), we obtain the following update rule (assuming \u03b3x = \u03b3y = 0):\nu\u0303t \u2190 u\u0303t\u22121 \u2212 2sX(X\u22a4u\u0303t\u22121 \u2212Y\u22a4vt\u22121)/N, ut \u2190 u\u0303t/ \u221a u\u0303\u22a4t \u03a3xxu\u0303t\nv\u0303t \u2190 v\u0303t\u22121 \u2212 2sY(Y\u22a4v\u0303t\u22121 \u2212X\u22a4ut\u22121)/N, vt \u2190 v\u0303t/ \u221a v\u0303\u22a4t \u03a3yyv\u0303t\nwhere s > 0 is the stepsize. This coincides with the AppGrad algorithm of [4, Algorithm 3], for which only local convergence is shown. Since the objectives ft(u) and gt(v) decouple over training samples, it is very efficient to apply stochastic gradient methods to them. This observation motivated the stochastic CCA algorithms of [4, 5]. We note however, no convergence guarantee was shown for these stochastic CCA algorithms.\n\u2022 Normalization Notice that at the end of each outer loop, Algorithm 2 implements exact normalization of the form ut \u2190 u\u0303t/ \u221a\nu\u0303\u22a4t \u03a3xxu\u0303t to ensure the constraints, where the computation of u\u0303\u22a4t \u03a3xxu\u0303t = 1 N (u\u0303\u22a4t X)(u\u0303 \u22a4 t X)\n\u22a4 + \u03b3x \u2016u\u0303t\u20162 requires projecting the entire training set. However, this does not introduce extra computation because we already compute the projection of entire training set anyway to obtain the batch gradient for SVRG (at the beginning of time step t + 1). In contrast, the stochastic algorithms of [4, 5] use (possibly adaptive) estimate of covariance from a minibatch for normalization, in order to reduce the cost of (frequent) normalization after each stochastic gradient descent step, which further introduces noise to the updates.\n\u2022 Input sparsity For high dimensional sparse data (e.g., such as those used in natural language processing [11, 12]), an advantage of gradient based methods over the closedform solution is that it can take into account the input sparsity.\n\u2022 Warm start Observe that for different t, the least squares problems ft(u) only differ in their targets as vt changes over time. Since vt\u22121 is close to vt (especially near convergence), we may use u\u0303t as initialization for minimizing ft+1(u) with an iterative algorithm.\n\u2022 Canonical ridge When (\u03b3x, \u03b3y) > 0, ft(u) and gt(v) are guaranteed to be strongly convex due to the \u21132 regularizations, in which case SVRG converges linearly. It is therefore beneficial to use small nonzero regularization for improved computational efficiency, especially for high dimensional datasets where the inputs X and Y are approximately low-rank."}, {"heading": "3.1 Analysis of inexact power iterations", "text": "The key to showing the convergence of Algorithm 2 is the convergence of inexact power iterations, where the the least squares problems are solved only to necessary accuracy.\nFrom now on, we distinguish the iterates of our stochastic algorithm (Algorithm 2) from the iterates of the exact power iterations (Algorithm 1) and denote the latter with asterisks, i.e., u\u0303\u2217t and v\u0303 \u2217 t for the unnormalized iterates and u\u2217t and v \u2217 t for the normalized iterates. We denote the exact optimum of ft(u) and gt(v) by u\u0304t and v\u0304t respectively.\nThe follow lemma bounds the distance between the iterates of inexact and exact power iterations. Lemma 3.1. Assume that Algorithm 1 and Algorithm 2 start with the same initialization, i.e., u\u03030 = u\u0303\u22170 and v\u03030 = v\u0303 \u2217 0.\nThen, for t \u2265 0, the unnormalized iterates of Algorithm 2 satisfy max ( \u2225 \u2225 \u2225 \u03a3 1 2 xxu\u0303t \u2212\u03a3 1 2 xxu\u0303 \u2217 t \u2225 \u2225 \u2225 , \u2225 \u2225 \u2225 \u03a3 1 2 yyv\u0303t \u2212\u03a3 1 2 yyv\u0303 \u2217 t \u2225 \u2225 \u2225 )\n\u2264 St, where\nSt := \u221a 2\u01eb\n{\nt if \u03c11 = 1 1\u2212\u03c1t1 1\u2212\u03c11 if \u03c11 < 1 .\nFurthermore, for t \u2265 1, the normalized iterates of Algorithm 2 satisfy max (\u2225 \u2225 \u2225 \u03a3 1 2 xxut \u2212\u03a3 1 2 xxu \u2217 t \u2225 \u2225 \u2225 , \u2225 \u2225 \u2225 \u03a3 1 2 yyvt \u2212\u03a3 1 2 yyv \u2217 t \u2225 \u2225 \u2225 )\n\u2264 2St \u03bdt ,\nwhere \u03bdt := max ( \u03c1r, \u03c11 min ( u\u22a4t\u22121\u03a3xxu \u2217, v\u22a4t\u22121\u03a3yyv \u2217 )) .\nBased on Lemma 3.1, we show the following convergence property of Algorithm 2.\nTheorem 3.2 (Convergence of Algorithm 2). Let T = \u2308 \u03c1 2 1\n\u03c12 1 \u2212\u03c12 2\nlog (\n2 \u00b5\u03b7\n)\n\u2309, and set\n\u01eb =\n{\n\u03b72\u03bd2T 128T 2 if \u03c11 = 1 \u03b72(1\u2212\u03c11)\n2\u03bd2T 128(1\u2212\u03c1T\n1 )2\nif \u03c11 < 1 .\nin Algorithm 2 where \u03bd2T \u2265 max ( \u03c12r, \u03c1 2 1(1\u2212 \u03b7/2) ) . Then, for t > T , we have\nmin ( (u\u22a4t \u03a3xxu \u2217)2, (v\u22a4t \u03a3yyv \u2217)2 ) \u2265 1\u2212 \u03b7, and u\u22a4t \u03a3xyvt \u2265 \u03c11(1 \u2212 2\u03b7).\nProof. We prove the theorem by relating the iterates of inexact power iterations to those of exact power iterations.\nAssume the same initialization as in Lemma 3.1. First observe that\n(u\u22a4t \u03a3xxu \u2217)2 =\n(\n(u\u2217t ) \u22a4 \u03a3xxu \u2217 + (ut \u2212 u\u2217t )\u22a4 \u03a3xxu\u2217 )2\n\u2265 (\n(u\u2217t ) \u22a4 \u03a3xxu\n\u2217 )2 + 2 (\n(u\u2217t ) \u22a4 \u03a3xxu\n\u2217 )(\n(ut \u2212 u\u2217t ) \u22a4 \u03a3xxu\n\u2217 )\n\u2265 (\n(u\u2217t ) \u22a4 \u03a3xxu\n\u2217 )2 \u2212 2 \u2223 \u2223 \u2223\n\u2223\n(\n\u03a3 1 2 xx (ut \u2212 u\u2217t )\n)\u22a4 (\n\u03a3 1 2 xxu \u2217 )\n\u2223 \u2223 \u2223 \u2223\n\u2265 (\n(u\u2217t ) \u22a4 \u03a3xxu\n\u2217 )2 \u2212 2 \u2225 \u2225 \u2225 \u03a3 1 2 xxut \u2212\u03a3 1 2 xxu \u2217 t \u2225 \u2225 \u2225 (5)\nwhere we have used the fact that \u2225 \u2225 \u2225 \u03a3 1 2 xxut \u2225 \u2225 \u2225 = \u2225 \u2225 \u2225 \u03a3 1 2 xxu \u2217 t \u2225 \u2225 \u2225 = \u2225 \u2225 \u2225 \u03a3 1 2 xxu \u2217 \u2225 \u2225 \u2225 = 1 and the Cauchy-Schwartz\ninequality in the last two steps.\nApplying Theorem 2.1 with T = \u2308 \u03c1 2 1\n\u03c12 1 \u2212\u03c12 2\nlog (\n2 \u00b5\u03b7\n) \u2309, we have that (\n(u\u2217t ) \u22a4 \u03a3xxu\n\u2217 )2\n\u2265 1\u2212 \u03b7/2 for t \u2265 T . On the other hand, in view of Lemma 3.1, we have for the specified \u01eb value in Algorithm 2 that \u2225 \u2225 \u2225 \u03a3 1 2 xxut \u2212\u03a3 1 2 xxu \u2217 t \u2225 \u2225 \u2225 \u2264 \u03b7/4. Plugging these two bounds into (5) gives the desired bound.\nThe proof for vt is completely analogous.\nAlgorithm 3 SVRG for minu f(u) := 1N \u2211N i=1\n(\n1 2\n\u2223 \u2223u\u22a4xi \u2212 v\u22a4yi \u2223 \u2223 2 + \u03b3x2 \u2016u\u2016\n2 )\n.\nInput: Stepsize s. Initialize u(0) \u2208 Rdx . for j = 1, 2, . . . ,M do w0 \u2190 u(j\u22121) Evaluate the batch gradient \u2207f(w0) = X(X\u22a4w0 \u2212Y\u22a4v)/N + \u03b3xw0 for t = 1, 2, . . . ,m do\nRandomly pick it from {1, . . . , N} wt \u2190 wt\u22121 \u2212 s ( (xitx \u22a4 it + \u03b3xI)(wt\u22121 \u2212w0) +\u2207f(w0) )\nend for u(j) \u2190 wt for randomly chosen t \u2208 {1, . . . ,m}.\nend for Output: u(M) is the approximate solution."}, {"heading": "3.2 Stochastic optimization of regularized least squares", "text": "We now discuss the inner loop of our algorithm, which approximately solves subproblems of the form (4). As mentioned earlier, the subproblems have the finite sum structure for which several stochastic optimization methods, such as SAG/SAGA [13, 14], SDCA [15], SVRG [10], provide linear convergence rates. All of these algorithms can be readily applied to the regularized least squares problem (4), and we choose SVRG because it is memory efficient and easy to implement.\nWe give the sketch of SVRG for (4) in Algorithm 3. Note that f(u) = \u2211N i=1 f i(u) where each component f i(u) = 12 \u2223 \u2223u\u22a4xi \u2212 v\u22a4yi \u2223 \u2223 2 + \u03b3x2 \u2016u\u2016\n2 is \u2016xi\u20162-smooth, and f(u) is \u03c3min(\u03a3xx)-strongly convex where \u03c3min(\u03a3xx) \u2265 \u03b3x is the minimum eigenvalue of \u03a3xx. We quote the convergence rate of SVRG below from [10].\nLemma 3.3. Fix \u01eb > 0. The SVRG algorithm detailed in Algorithm 3 finds a vector u\u0303 satisfying E[f(u\u0303)]\u2212minu f(u) \u2264 \u01eb2 in total time\nO (\ndx (N + \u03bax) log\n(\n1\n\u01eb\n))\nwhere \u03bax = maxi\u2016xi\u2016\n2\n\u03c3min(\u03a3xx) .\nRemarks We can also apply the recently developed accelerations techniques for first order optimization methods [16, 17]. These techniques can help replace the \u03bax term in the time complexity with \u221a N\u03bax, which is advantageous when \u03bax \u226b N .\nWe obtain the total time complexity of our algorithm, O\u0303 ( d (N + \u03ba) \u03c121\n\u03c12 1 \u2212\u03c12 2\n)\nas stated in Theo-\nrem 1.1, by combining the time complexity of inexact power iterations (Theorem 3.2) and the time complexity of SVRG (Lemma 3.3). For comparison, the local convergence of batch AppGrad ([4, Theorem 2.1]) can be translated into a total time complexity of O\u0303 ( dN\u03ba \u03c121\n\u03c12 1 \u2212\u03c12 2\n)\n, where N and the\ncondition number3 are multiplied together as AppGrad essentially applies batch gradient descent to the least squares problems. Within our framework, we can use accelerated gradient descent instead and reduce the dependence on condition number to \u221a \u03ba [18].\n2The expectation is taken over random sampling of component functions. 3The condition number of least squares for batch gradient descent is defined differently as \u03ba =\nmax\n(\n\u03c3max(\u03a3xx) \u03c3min(\u03a3xx) , \u03c3max(\u03a3yy) \u03c3min(\u03a3yy)\n)\n."}, {"heading": "3.3 Extension to multi-dimensional projections", "text": "In practice, we may be interested in projections of more than one dimensions. The CCA objective for extracting L-dimensional projections is\nmax U\u2208Rdx\u00d7L,V\u2208Rdy\u00d7L\ntr ( U\u22a4\u03a3xyV )\ns.t. U\u22a4\u03a3xxU = V\u22a4\u03a3yyV = I.\nTo adapt Algorithm 2 to this problem, one option is to extract the dimensions sequentially and remove the explained correlation from \u03a3xy each time we extract a new dimension [19]. A simpler approach is to extract the L dimensions simultaneously using (approximate) orthogonal iterations (an extension of power iterations to multiple dimensions, [9]). In this case, our subproblems become multi-dimensional regressions and our normalization steps are of the form Ut \u2190 U\u0303t(U\u0303\u22a4t \u03a3xxU\u0303t)\u2212 1 2 ; the same normalization is used by [4, 5]. Such normalization involves the eigenvalue decomposition of a L \u00d7 L matrix and can be solved exactly as we typically look for low dimensional projections. Our analysis for L = 1 can be extended to this scenario and the convergence rate of the algorithm now depends on the singular value gap between \u03c1L and \u03c1L+1."}, {"heading": "4 Related work", "text": "Recent years have witnessed continuous efforts to scale up fundamental methods such as principal component analysis (PCA) and partial least squares with stochastic/online updates [20, 21, 22, 23, 24, 25, 26, 27, 6, 28, 29, 30]. But as pointed out by [23], the CCA objective is more challenging due to the whitening constraints. Our algorithm is inspired by the stochastic PCA algorithm of [28] which transforms the nonconvex PCA objective into a small number of well-conditioned regularized least squares problems (solved by SVRG) through shifting and inverting the covariance matrix and running power iterations on the transformed matrix. For CCA, the alternating least squares formulation (Algorithm 1) already reduces to solving convex regularized least squares problems.\n[31] propose an adaptive CCA algorithm with efficient online updates based on matrix manifolds defined by the constraints. However, the goal of their algorithm is anomaly detection for streaming data with a varying distribution, rather than to optimize the CCA objective on a given dataset. Similar to our algorithm, the stochastic CCA algorithms of [4, 5] are motivated by the alternating least squares formulation. [6] proposed a stochastic algorithm based on the Lagrangian formulation of the CCA objective. It is important to note that none of these online/stochastic algorithms have rigorous global convergence guarantee."}, {"heading": "5 Experiments", "text": "In this section we demonstrate our algorithm, denoted by ALS-VR (Alternating Least Squares with Variance Reduction), on three real-world datasets: Mediamill [32], JW11 (a subset of the XRMB corpus [33]), and MNIST [34]. These datasets have also been used by [4, 5] for demonstrating their stochastic CCA algorithms. We give the description and size of these datasets in Table 1. We extract L = 5 dimensional projections, and for the approximate solution obtained after each pass over the data, we evaluate the canonical correlation between the projections and compare it with that of the exact solution by SVD.\nWe compare ALS-VR with batch AppGrad, and its stochastic version s-AppGrad [4] with both gradient and normalization steps estimated with minibatchs of 200 samples.4 We do not compare the algorithm of [5] because the main difference between it and AppGrad is that it uses adaptive\n4The authors of [4] suggest that the minibatch size shall be at least the same magnitude as L.\n\u03b3x = \u03b3y = 10 \u22125 \u03b3x = \u03b3y = 10 \u22124 \u03b3x = \u03b3y = 10 \u22123 \u03b3x = \u03b3y = 10 \u22122\n\u03c12 k \u2212\u03c12 k+1\n.\nestimates of the covariance matrices for projections, which become unnecessary with large enough minibatches. For ALS-VR, at every outer loop we apply SVRG to solve the least squares problems ft(u) and gt(v) with M = 4 epochs, and each epoch includes a batch gradient evaluation and m = 2N stochastic gradient steps. For each dataset, we also vary the regularization parameters rx = ry over {10\u22125, 10\u22124, 10\u22123, 10\u22122} and larger regularization leads to better conditioned least squares problems. We set the SGD step size according to the smoothness for each method, i.e., 1/\u03c3max(\u03a3xx) for AppGrad/s-AppGrad, and 1/maxi \u2016xi\u20162 for SVRG in ALS-VR. The results for each dataset and different regularizations are shown in Figure 1. ALS-VR achieves accurate solution more quickly than AppGrad and s-AppGrad in all settings."}, {"heading": "6 Conclusions", "text": "We have proposed a globally convergent stochastic algorithm for CCA, whose objective is nonconvex and does not decouple over training samples. Our algorithm makes use of the alternating least squares/power iterations formulation of CCA, and solves the least squares problems approximately with state-of-the-art stochastic gradient descent methods. The overall time complexity of our algorithm significantly improve upon previous work both theoretically and empirically.\nOne straightforward application of our algorithm is low-rank approximate kernel CCA [35] using random Fourier features [36, 37]. The algorithm of [35] first map original inputs into high dimensional random feature space and then perform linear CCA on top. We can solve the resulting high dimensional CCA problem with our stochastic algorithm, as done by [6, 38]. Another future direction is to extend the general inexact power iterations approach to other spectral dimension reduction algorithms (e.g., linear discriminant analysis is equivalent to CCA with proper representation for the class labels [39])."}, {"heading": "A Proof of Theorem 2.1", "text": "Proof. It is easy to see that by the end of the first iteration of Algorithm 1, \u03c8\u03031 and\u03c81 lie in the span of {bi}ri=1, while \u03c6\u03031 and \u03c61 lie in the span of {ai}ri=1. And therefore they remain in these spaces for all t \u2265 1. Let us first focus on \u03c6t. For t \u2265 2, we observe that\n\u03c6t = T\u03c8t\u22121 /\n\u2225 \u2225 \u2225 \u03c6\u0303t \u2225 \u2225 \u2225 = TT\u22a4\u03c6t\u22122 / (\u2225 \u2225 \u2225 \u03c6\u0303t \u2225 \u2225 \u2225 \u00b7 \u2225 \u2225 \u2225 \u03c8\u0303t\u22121 \u2225 \u2225 \u2225 ) . (6)\nSince \u2225 \u2225\u03c6t\u22122 \u2225 \u2225 = \u2016\u03c6t\u2016 = 1, it is equivalent to using the following updates:\n\u03c6t \u2190 TT\u22a4\u03c6t\u22122, \u03c6t \u2190 \u03c6t/ \u2016\u03c6t\u2016 . (7)\nThis indicates that, Algorithm 1 runs the standard power iterations on TT\u22a4 to generate the {\u03c6t}t\u22651 sequence for every two steps.\n(i) For t = 2, 4, . . . , we have \u03c6t = (TT\u22a4)\nt 2\u03c60\n\u2225 \u2225 \u2225 (TT\u22a4) t 2\u03c60 \u2225 \u2225 \u2225\n. Let M = TT\u22a4, whose nonzero eigenvalues are\n\u03c121 \u2265 \u03c122 \u2265 \u00b7 \u00b7 \u00b7 \u2265 \u03c12r > 0, with corresponding eigenvectors a1, . . . , ar. Then, for i = 1, . . . , r,\n(a\u22a4i \u03c6t) 2 =\n(\na\u22a4i M t 2\u03c60\n)2\n\u2225 \u2225 \u2225 M t 2\u03c60 \u2225 \u2225 \u2225\n2 =\n(\na\u22a4i M t 2\u03c60\n)2\n\u03c6\u22a40 M t\u03c60\n=\n(\n\u03c1tia \u22a4 i \u03c60\n)2\n\u2211r j=1 \u03c1 2t j (a \u22a4 j \u03c60)\n2 =\n( a\u22a4i \u03c60 )2\n\u2211r j=1\n(\n\u03c12 j \u03c12 i\n)t\n(a\u22a4j \u03c60) 2\n\u2264 ( a\u22a4i \u03c60 )2\n(\n\u03c12 1 \u03c12 i\n)t\n(a\u22a41 \u03c60) 2\n=\n( a\u22a4i \u03c60 )2 (a\u22a41 \u03c60) 2 ( \u03c12i \u03c121 )t = ( a\u22a4i \u03c60 )2 (a\u22a41 \u03c60) 2 ( 1\u2212 \u03c1 2 1 \u2212 \u03c12i \u03c121 )t\n\u2264 ( a\u22a4i \u03c60 )2\n(a\u22a41 \u03c60) 2 exp\n(\n\u2212\u03c1 2 1 \u2212 \u03c12i \u03c121 t\n)\n. (8)\n(ii) For t = 1, 3, . . . , we have \u03c6t = (TT\u22a4)\nt\u22121 2 T\u03c80\n\u2225 \u2225 \u2225 \u2225 (TT\u22a4) t\u22121 2 T\u03c80 \u2225 \u2225 \u2225 \u2225\n. Let N = T\u22a4T, whose nonzero eigenvalues\nare \u03c121 \u2265 \u03c122 \u2265 \u00b7 \u00b7 \u00b7 \u2265 \u03c12r > 0, with corresponding eigenvectors b1, . . . ,br. Then, for i = 1, . . . , r,\n(a\u22a4i \u03c6t) 2 =\n(\na\u22a4i (TT \u22a4) t\u22121 2 T\u03c80\n)2\n\u2225 \u2225 \u2225 (TT\u22a4) t\u22121 2 T\u03c80 \u2225 \u2225 \u2225\n2 =\n(\n(T\u22a4ai) \u22a4N t\u22121 2 \u03c80\n)2\n\u03c8\u22a40 N t\u03c80\n=\n(\n\u03c1tib \u22a4 i \u03c80\n)2\n\u2211r j=1 \u03c1 2t j (b \u22a4 j \u03c80) 2\n\u2264 ( b\u22a4i \u03c80 )2\n(b\u22a41 \u03c80) 2 exp\n(\n\u2212\u03c1 2 1 \u2212 \u03c12i \u03c121 t\n)\n.\nGiven \u03b4 \u2208 (0, 1), define S(\u03b4) = {i : \u03c12i > (1\u2212 \u03b4)\u03c121}. For \u03b41, \u03b42 \u2208 (0, 1), define\nT (\u03b41, \u03b42) := \u2308 1\n\u03b41 log\n(\n1\n\u00b5\u03b42\n)\n\u2309. (9)\nFor all i 6\u2208 S(\u03b41), when t > T (\u03b41, \u03b42), it holds that (a\u22a4i \u03c6t)2 \u2264 \u03b42(a\u22a4i \u03c60)2 if t is even, and (a\u22a4i \u03c6t) 2 \u2264 \u03b42(b\u22a4i \u03c80)2 if t is odd. In both cases, we have \u2211 i\u2208S(\u03b41) (a\u22a4i \u03c6t) 2 \u2265 1\u2212 \u03b42.\nWhen there exists a postive singular value gap, i.e., \u03c11 \u2212 \u03c12 > 0, set \u03b41 = (\u03c121 \u2212 \u03c122)/\u03c121 and thus S(\u03b41) = 1. Futhermore, set \u03b42 = \u03b7 and we obtain (a\u22a41 \u03c6t) 2 \u2265 1\u2212 \u03b7.\nThe proof for \u03c8t is completely analogous. To obtain the bound on the objective, we have\nu\u22a4t \u03a3xyvt = \u03c6 \u22a4 t T\u03c8t = \u03c11(\u03c6 \u22a4 t a1)(\u03c8 \u22a4 t b1) +\nr \u2211\ni=2\n\u03c1i(\u03c6 \u22a4 t ai)(\u03c8 \u22a4 t bi)\n\u2265 \u03c11(\u03c6\u22a4t a1)(\u03c8\u22a4t b1)\u2212 \u03c11 r \u2211\ni=2\n\u2223 \u2223 \u2223 \u03c6\u22a4t ai \u2223 \u2223 \u2223 \u2223 \u2223 \u2223 \u03c8\u22a4t bi \u2223 \u2223 \u2223\n\u2265 \u03c11(1\u2212 \u03b7)\u2212 \u03c11\n\u221a \u221a \u221a \u221a r \u2211\ni=2\n(\n\u03c6\u22a4t ai\n)2\n\u221a \u221a \u221a \u221a r \u2211\ni=2\n(\n\u03c8\u22a4t bi\n)2\n\u2265 \u03c11(1\u2212 \u03b7)\u2212 \u03c11\u03b7 = \u03c11(1\u2212 2\u03b7),\nwhere we have used the Cauchy-Schwartz inequality in the second inequality."}, {"heading": "B Proof of Lemma 3.1", "text": "Proof. We focus on the {u\u0303t}t\u22650 and {ut}t\u22650 sequences below; the proof for {v\u0303t}t\u22650 and {vt}t\u22650 is completely analogous.\n(i) We prove the bound for unnormalized iterates recursively. First, the case for t = 0 holds trivially. For t \u2265 1, we can bound the error of the unnormalized iterates using the exact solution to ft(u):\n\u2225 \u2225 \u2225 \u03a3 1 2 xxu\u0303t \u2212\u03a3 1 2 xxu\u0303 \u2217 t \u2225 \u2225 \u2225 \u2264 \u2225 \u2225 \u2225 \u03a3 1 2 xxu\u0303t \u2212\u03a3 1 2 xxu\u0304t \u2225 \u2225 \u2225 + \u2225 \u2225 \u2225 \u03a3 1 2 xxu\u0304t \u2212\u03a3 1 2 xxu\u0303 \u2217 t \u2225 \u2225 \u2225 . (10)\nFor the first term of (10), notice ft(u) is a quadratic function with minimum achieved at u\u0304t = \u03a3\u22121xx\u03a3xyvt\u22121. For the approximate solution u\u0303t, we have\nft(u\u0303t)\u2212 ft(u\u0304t) = 1\n2 (u\u0303t \u2212 u\u0304t)\u22a4\u03a3xx(u\u0303t \u2212 u\u0304t) =\n1\n2\n\u2225 \u2225 \u2225 \u03a3 1 2 xxu\u0303t \u2212\u03a3 1 2 xxu\u0304t \u2225 \u2225 \u2225\n2\n\u2264 \u01eb.\nIt then follows that \u2225 \u2225 \u2225 \u03a3 1 2 xxu\u0303t \u2212\u03a3 1 2 xxu\u0304t \u2225 \u2225 \u2225 \u2264 \u221a 2\u01eb.\nThe second term of (10) is concerned with the error due to inexact target in the least squares problem ft(u) as vt\u22121 is different from v\u2217t\u22121. We can bound it as\n\u2225 \u2225 \u2225 \u03a3 1 2 xxu\u0304t \u2212\u03a3 1 2 xxu\u0303 \u2217 t \u2225 \u2225 \u2225 = \u2225 \u2225 \u2225 \u03a3 1 2 xx\u03a3 \u22121 xx\u03a3xyvt\u22121 \u2212\u03a3 1 2 xx\u03a3 \u22121 xx\u03a3xyv \u2217 t\u22121 \u2225 \u2225 \u2225\n= \u2225 \u2225\n\u2225\n(\n\u03a3 \u2212 1\n2 xx \u03a3xy\u03a3\n\u2212 1 2 yy\n)(\n\u03a3 1 2 yy(vt\u22121 \u2212 v\u2217t\u22121)\n)\u2225\n\u2225 \u2225\n\u2264 \u2016T\u2016 \u2225 \u2225 \u2225 \u03a3 1 2 yy(vt\u22121 \u2212 v\u2217t\u22121) \u2225 \u2225 \u2225\n\u2264 \u03c11St\u22121.\nNow combine the two terms depending on the value of \u03c11. If \u03c11 = 1, we have\n\u2225 \u2225 \u2225 \u03a3 1 2 xxu\u0303t \u2212\u03a3 1 2 xxu\u0303 \u2217 t \u2225 \u2225 \u2225 \u2264 \u221a 2\u01eb+ \u221a 2\u01eb(t\u2212 1) = \u221a 2\u01ebt.\nOtherwise, we have \u03c11 < 1 and\n\u2225 \u2225 \u2225 \u03a3 1 2 xxu\u0303t \u2212\u03a3 1 2 xxu\u0303 \u2217 t \u2225 \u2225 \u2225 \u2264 \u221a 2\u01eb+ \u221a 2\u01eb\u03c11 1\u2212 \u03c1t\u221211 1\u2212 \u03c11 = \u221a 2\u01eb 1\u2212 \u03c1t1 1\u2212 \u03c11 .\n(ii) We now prove the bound for normalized iterates. In view of the update rule of our algorithm and the triangle inequality, we have\n\u2225 \u2225 \u2225 \u03a3 1 2 xxut \u2212\u03a3 1 2 xxu \u2217 t \u2225 \u2225 \u2225 \u2264\n\u2225 \u2225 \u2225 \u2225 \u2225 \u2225 \u03a3 1 2 xxu\u0303t \u2225 \u2225\n\u2225 \u03a3 1 2 xxu\u0303t\n\u2225 \u2225 \u2225\n\u2212 \u03a3 1 2 xxu\u0303t \u2225\n\u2225 \u2225 \u03a3 1 2 xxu\u0303 \u2217 t\n\u2225 \u2225 \u2225\n\u2225 \u2225 \u2225 \u2225 \u2225 \u2225 + \u2225 \u2225 \u2225 \u2225 \u2225 \u2225 \u03a3 1 2 xxu\u0303t \u2225 \u2225\n\u2225 \u03a3 1 2 xxu\u0303 \u2217 t\n\u2225 \u2225 \u2225\n\u2212 \u03a3 1 2 xxu\u0303 \u2217 t \u2225\n\u2225 \u2225 \u03a3 1 2 xxu\u0303 \u2217 t\n\u2225 \u2225 \u2225\n\u2225 \u2225 \u2225 \u2225 \u2225 \u2225\n= \u2225 \u2225 \u2225 \u03a3 1 2 xxu\u0303t \u2225 \u2225 \u2225\n\u2223 \u2223 \u2223 \u2223 \u2223 \u2223 1 \u2225 \u2225\n\u2225 \u03a3 1 2 xxu\u0303t\n\u2225 \u2225 \u2225\n\u2212 1\u2225 \u2225\n\u2225 \u03a3 1 2 xxu\u0303 \u2217 t\n\u2225 \u2225 \u2225\n\u2223 \u2223 \u2223 \u2223 \u2223 \u2223 + 1 \u2225 \u2225\n\u2225 \u03a3 1 2 xxu\u0303 \u2217 t\n\u2225 \u2225 \u2225\n\u2225 \u2225 \u2225 \u03a3 1 2 xxu\u0303t \u2212\u03a3 1 2 xxu\u0303 \u2217 t \u2225 \u2225 \u2225\n= 1 \u2225\n\u2225 \u2225 \u03a3 1 2 xxu\u0303 \u2217 t\n\u2225 \u2225 \u2225\n\u2223 \u2223 \u2223 \u2225 \u2225 \u2225 \u03a3 1 2 xxu\u0303 \u2217 t \u2225 \u2225 \u2225 \u2212 \u2225 \u2225 \u2225 \u03a3 1 2 xxu\u0303t \u2225 \u2225 \u2225 \u2223 \u2223 \u2223 + 1 \u2225\n\u2225 \u2225 \u03a3 1 2 xxu\u0303 \u2217 t\n\u2225 \u2225 \u2225\n\u2225 \u2225 \u2225 \u03a3 1 2 xxu\u0303t \u2212\u03a3 1 2 xxu\u0303 \u2217 t \u2225 \u2225 \u2225\n\u2264 2\u2225 \u2225\n\u2225 \u03a3 1 2 xxu\u0303 \u2217 t\n\u2225 \u2225 \u2225\n\u2225 \u2225 \u2225 \u03a3 1 2 xxu\u0303t \u2212\u03a3 1 2 xxu\u0303 \u2217 t \u2225 \u2225 \u2225 = 2St \u2225\n\u2225 \u2225 \u03a3 1 2 xxu\u0303 \u2217 t\n\u2225 \u2225 \u2225\n.\nIt remains to bound \u2225 \u2225 \u2225 \u03a3 1 2 xxu\u0303 \u2217 t \u2225 \u2225 \u2225 from below. Note that\n\u03a3 1 2 xxu\u0303 \u2217 t = \u03a3 1 2 xx\u03a3 \u22121 xx\u03a3xyu\u0303 \u2217 t =\n(\n\u03a3 \u2212 1\n2 xx \u03a3xy\u03a3\n\u2212 1 2 yy\n)(\n\u03a3 1 2 yyv \u2217 t\u22121\n) = T ( \u03a3 1 2 yyv \u2217 t\u22121 ) .\nAlso, \u03a3 1 2 yyv \u2217 t\u22121 corresponds to \u03c8t\u22121 of Algorithm 1 which has unit length and lies in the span of {b1, . . . ,br}, i.e., \u03c8t\u22121 = (\u03c8\u22a4t\u22121b1) \u00b7 b1 + \u00b7 \u00b7 \u00b7+ (\u03c8\u22a4t\u22121br) \u00b7 br. As a result,\nT\u03c8t\u22121 = \u03c11(\u03c8 \u22a4 t\u22121b1) \u00b7 a1 + \u00b7 \u00b7 \u00b7+ \u03c1r(\u03c8\u22a4t\u22121br) \u00b7 ar.\nSince {a1, . . . , ar} are orthonormal, we have \u2225\n\u2225 \u2225 \u03a3 1 2 xxu\u0303 \u2217 t\n\u2225 \u2225 \u2225 = \u2225 \u2225T\u03c8t\u22121 \u2225 \u2225 \u2265 \u03c1r.\nNote this bound can be pessimistic for large t. For large enough t, we expect the \u03c8t\u22121 to be close to \u03c8, and have\n\u2225 \u2225 \u2225 \u03a3 1 2 xxu\u0303 \u2217 t \u2225 \u2225 \u2225 = \u2225 \u2225T\u03c8t\u22121 \u2225 \u2225 \u2265 \u03c11 \u2223 \u2223 \u2223 \u03c8\u22a4t\u22121b1 \u2223 \u2223 \u2223 \u2248 \u03c11.\nThen the lemma follows."}], "references": [{"title": "Relations between two sets of variates", "author": ["Harold Hotelling"], "venue": "Biometrika, 28(3/4):321\u2013377,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 1936}, {"title": "Canonical ridge and econometrics of joint production", "author": ["H.D. Vinod"], "venue": "Journal of Econometrics,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 1976}, {"title": "On the regularization of canonical correlation analysis", "author": ["Tijl De Bie", "Bart De Moor"], "venue": "www.esat.kuleuven.ac.be/sista-cosic-docarch/,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2003}, {"title": "Finding linear structure in large datasets with scalable canonical correlation analysis", "author": ["Zhuang Ma", "Yichao Lu", "Dean Foster"], "venue": "In Proc. of the 32st Int. Conf. Machine Learning (ICML", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2015}, {"title": "Stochastic optimization for deep CCA via nonlinear orthogonal iterations", "author": ["Weiran Wang", "Raman Arora", "Nati Srebro", "Karen Livescu"], "venue": "In 53nd Annual Allerton Conference on Communication, Control and Computing,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2015}, {"title": "Scale up nonlinear component analysis with doubly stochastic gradients", "author": ["Bo Xie", "Yingyu Liang", "Le Song"], "venue": "In Advances in Neural Information Processing Systems (NIPS),", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2015}, {"title": "Linear Algebra for Signal Processing, volume 69 of The IMA Volumes in Mathematics and its Applications, chapter The Canonical Correlations of Matrix Pairs and their Numerical Computation, pages 27\u201349", "author": ["Gene H. Golub", "Hongyuan Zha"], "venue": null, "citeRegEx": "7", "shortCiteRegEx": "7", "year": 1995}, {"title": "Large scale canonical correlation analysis with iterative least squares", "author": ["Yichao Lu", "Dean P. Foster"], "venue": "In Advances in Neural Information Processing Systems (NIPS),", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2014}, {"title": "Accelerating stochastic gradient descent using predictive variance reduction", "author": ["Rie Johnson", "Tong Zhang"], "venue": "In Advances in Neural Information Processing Systems (NIPS),", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2013}, {"title": "Multi-view learning of word embeddings via CCA", "author": ["Paramveer Dhillon", "Dean Foster", "Lyle Ungar"], "venue": "In Advances in Neural Information Processing Systems (NIPS),", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2011}, {"title": "Using CCA to improve CCA: A new spectral method for estimating vector models of words", "author": ["Paramveer Dhillon", "Jordan Rodu", "Dean Foster", "Lyle Ungar"], "venue": "In Proc. of the 29th Int. Conf. Machine Learning (ICML", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2012}, {"title": "Minimizing finite sums with the stochastic average gradient", "author": ["Mark Schmidt", "Nicolas Le Roux", "Francis Bach"], "venue": "Technical Report HAL 00860051,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2013}, {"title": "SAGA: A fast incremental gradient method with support for non-strongly convex composite objectives", "author": ["Aaron Defazio", "Francis Bach", "Simon Lacoste-Julien"], "venue": "In Advances in Neural Information Processing Systems (NIPS),", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2014}, {"title": "Stochastic dual coordinate ascent methods for regularized loss minimization", "author": ["Shai Shalev-Shwartz", "Tong Zhang"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2013}, {"title": "Un-regularizing: Approximate proximal point and faster stochastic algorithms for empirical risk minimization", "author": ["Roy Frostig", "Rong Ge", "Sham Kakade", "Aaron Sidford"], "venue": "In Proc. of the 32st Int. Conf. Machine Learning (ICML", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2015}, {"title": "A universal catalyst for first-order optimization", "author": ["Hongzhou Lin", "Julien Mairal", "Zaid Harchaoui"], "venue": "In Advances in Neural Information Processing Systems (NIPS),", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2015}, {"title": "Introductory Lectures on Convex Optimization", "author": ["Y. Nesterov"], "venue": "A Basic Course. Number 87 in Applied Optimization. Springer-Verlag,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2004}, {"title": "A penalized matrix decomposition, with applications to sparse principal components and canonical correlation analysis", "author": ["Daniela M. Witten", "Robert Tibshirani", "Trevor Hastie"], "venue": null, "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2009}, {"title": "Krasulina. A method of stochastic approximation for the determination of the least eigenvalue of a symmetric matrix", "author": ["P. T"], "venue": "USSR Computational Mathematics and Mathematical Physics,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 1969}, {"title": "On stochastic approximation of the eigenvectors and eigenvalues of the expectation of a random matrix", "author": ["Erkki Oja", "Juha Karhunen"], "venue": "J. Math. Anal. Appl.,", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 1985}, {"title": "Randomized online PCA algorithms with regret bounds that are logarithmic in the dimension", "author": ["Manfred K. Warmuth", "Dima Kuzmin"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2008}, {"title": "Stochastic optimization for PCA and PLS", "author": ["Raman Arora", "Andy Cotter", "Karen Livescu", "Nati Srebro"], "venue": "In 50th Annual Allerton Conference on Communication,", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2012}, {"title": "Stochastic optimization of PCA with capped MSG", "author": ["Raman Arora", "Andy Cotter", "Nati Srebro"], "venue": "In Advances in Neural Information Processing Systems (NIPS),", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2013}, {"title": "Memory limited, streaming PCA", "author": ["Ioannis Mitliagkas", "Constantine Caramanis", "Prateek Jain"], "venue": "In Advances in Neural Information Processing Systems (NIPS),", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2013}, {"title": "The fast convergence of incremental PCA", "author": ["Akshay Balsubramani", "Sanjoy Dasgupta", "Yoav Freund"], "venue": "In Advances in Neural Information Processing Systems (NIPS),", "citeRegEx": "26", "shortCiteRegEx": "26", "year": 2013}, {"title": "A stochastic PCA and SVD algorithm with an exponential convergence rate", "author": ["Ohad Shamir"], "venue": "In Proc. of the 32st Int. Conf. Machine Learning (ICML", "citeRegEx": "27", "shortCiteRegEx": "27", "year": 2015}, {"title": "Fast and simple pca via convex optimization", "author": ["Dan Garber", "Elad Hazan"], "venue": "[math.OC],", "citeRegEx": "28", "shortCiteRegEx": "28", "year": 2015}, {"title": "Robust shift-and-invert preconditioning: Faster and more sample efficient algorithms for eigenvector computation", "author": ["Chi Jin", "Sham M. Kakade", "Cameron Musco", "Praneeth Netrapalli", "Aaron Sidford"], "venue": "[cs.DS],", "citeRegEx": "29", "shortCiteRegEx": "29", "year": 2015}, {"title": "Principal component projection without principal component analysis", "author": ["Roy Frostig", "Cameron Musco", "Christopher Musco", "Aaron Sidford"], "venue": "[cs.DS],", "citeRegEx": "30", "shortCiteRegEx": "30", "year": 2016}, {"title": "Adaptive canonical correlation analysis based on matrix manifolds", "author": ["Florian Yger", "Maxime Berar", "Gilles Gasso", "Alain Rakotomamonjy"], "venue": "In Proc. of the 29th Int. Conf. Machine Learning (ICML", "citeRegEx": "31", "shortCiteRegEx": "31", "year": 2012}, {"title": "The challenge problem for automated detection of 101 semantic concepts in multimedia", "author": ["Cees G.M. Snoek", "Marcel Worring", "Jan C. van Gemert", "Jan-Mark Geusebroek", "Arnold W.M. Smeulders"], "venue": "In Proceedings of the 14th ACM international conference on Multimedia,", "citeRegEx": "32", "shortCiteRegEx": "32", "year": 2006}, {"title": "X-Ray Microbeam Speech Production", "author": ["John R. Westbury"], "venue": "Database User\u2019s Handbook Version", "citeRegEx": "33", "shortCiteRegEx": "33", "year": 1994}, {"title": "Gradient-based learning applied to document recognition", "author": ["Yann LeCun", "L\u00e9on Bottou", "Yoshua Bengio", "Patrick Haffner"], "venue": "Proc. IEEE,", "citeRegEx": "34", "shortCiteRegEx": "34", "year": 1998}, {"title": "Randomized nonlinear component analysis", "author": ["David Lopez-Paz", "Suvrit Sra", "Alex Smola", "Zoubin Ghahramani", "Bernhard Schoelkopf"], "venue": "In Proc. of the 31st Int. Conf. Machine Learning (ICML", "citeRegEx": "35", "shortCiteRegEx": "35", "year": 2014}, {"title": "Random features for large-scale kernel machines", "author": ["Ali Rahimi", "Ben Recht"], "venue": "In Advances in Neural Information Processing Systems (NIPS),", "citeRegEx": "36", "shortCiteRegEx": "36", "year": 2008}, {"title": "Weighted sums of random kitchen sinks: Replacing minimization with randomization in learning", "author": ["Ali Rahimi", "Benjamin Recht"], "venue": "In Advances in Neural Information Processing Systems (NIPS),", "citeRegEx": "37", "shortCiteRegEx": "37", "year": 2009}, {"title": "Large-scale approximate kernel canonical correlation analysis", "author": ["Weiran Wang", "Karen Livescu"], "venue": "In Proc. of the 4nd Int. Conf. Learning Representations (ICLR 2016),", "citeRegEx": "38", "shortCiteRegEx": "38", "year": 2016}], "referenceMentions": [{"referenceID": 0, "context": "1 Introduction Canonical correlation analysis (CCA, [1]) and its extensions are ubiquitous techniques in scientific research areas for revealing the common sources of variability in multiple views of the same phenomenon.", "startOffset": 52, "endOffset": 55}, {"referenceID": 1, "context": "u\u03a3xxu = v\u03a3yyv = 1 where \u03a3xy = 1 NXY \u22a4 is the cross-covariance matrix, \u03a3xx = 1 NXX +\u03b3xI and \u03a3yy = 1 NYY + \u03b3yI are the auto-covariance matrices, and (\u03b3x, \u03b3y) \u2265 0 are regularization parameters [2, 3].", "startOffset": 190, "endOffset": 196}, {"referenceID": 2, "context": "u\u03a3xxu = v\u03a3yyv = 1 where \u03a3xy = 1 NXY \u22a4 is the cross-covariance matrix, \u03a3xx = 1 NXX +\u03b3xI and \u03a3yy = 1 NYY + \u03b3yI are the auto-covariance matrices, and (\u03b3x, \u03b3y) \u2265 0 are regularization parameters [2, 3].", "startOffset": 190, "endOffset": 196}, {"referenceID": 3, "context": "There have been recent attempts to solve (1) based on stochastic gradient descent (SGD) methods [4, 5, 6], but none of these work provides rigorous convergence analysis for their stochastic CCA algorithms.", "startOffset": 96, "endOffset": 105}, {"referenceID": 4, "context": "There have been recent attempts to solve (1) based on stochastic gradient descent (SGD) methods [4, 5, 6], but none of these work provides rigorous convergence analysis for their stochastic CCA algorithms.", "startOffset": 96, "endOffset": 105}, {"referenceID": 5, "context": "There have been recent attempts to solve (1) based on stochastic gradient descent (SGD) methods [4, 5, 6], but none of these work provides rigorous convergence analysis for their stochastic CCA algorithms.", "startOffset": 96, "endOffset": 105}, {"referenceID": 7, "context": "A similar algorithm is recently used by [8] for large scale linear CCA with high dimensional sparse inputs, and by [4, 5] for motivating stochastic CCA algorithms.", "startOffset": 40, "endOffset": 43}, {"referenceID": 3, "context": "A similar algorithm is recently used by [8] for large scale linear CCA with high dimensional sparse inputs, and by [4, 5] for motivating stochastic CCA algorithms.", "startOffset": 115, "endOffset": 121}, {"referenceID": 4, "context": "A similar algorithm is recently used by [8] for large scale linear CCA with high dimensional sparse inputs, and by [4, 5] for motivating stochastic CCA algorithms.", "startOffset": 115, "endOffset": 121}, {"referenceID": 8, "context": ", stochastic variance reduced gradient (SVRG, [10]) to obtain approximate matrix/vector product for power iterations.", "startOffset": 46, "endOffset": 50}, {"referenceID": 3, "context": "This observation motivated the stochastic CCA algorithms of [4, 5].", "startOffset": 60, "endOffset": 66}, {"referenceID": 4, "context": "This observation motivated the stochastic CCA algorithms of [4, 5].", "startOffset": 60, "endOffset": 66}, {"referenceID": 3, "context": "In contrast, the stochastic algorithms of [4, 5] use (possibly adaptive) estimate of covariance from a minibatch for normalization, in order to reduce the cost of (frequent) normalization after each stochastic gradient descent step, which further introduces noise to the updates.", "startOffset": 42, "endOffset": 48}, {"referenceID": 4, "context": "In contrast, the stochastic algorithms of [4, 5] use (possibly adaptive) estimate of covariance from a minibatch for normalization, in order to reduce the cost of (frequent) normalization after each stochastic gradient descent step, which further introduces noise to the updates.", "startOffset": 42, "endOffset": 48}, {"referenceID": 9, "context": ", such as those used in natural language processing [11, 12]), an advantage of gradient based methods over the closedform solution is that it can take into account the input sparsity.", "startOffset": 52, "endOffset": 60}, {"referenceID": 10, "context": ", such as those used in natural language processing [11, 12]), an advantage of gradient based methods over the closedform solution is that it can take into account the input sparsity.", "startOffset": 52, "endOffset": 60}, {"referenceID": 11, "context": "As mentioned earlier, the subproblems have the finite sum structure for which several stochastic optimization methods, such as SAG/SAGA [13, 14], SDCA [15], SVRG [10], provide linear convergence rates.", "startOffset": 136, "endOffset": 144}, {"referenceID": 12, "context": "As mentioned earlier, the subproblems have the finite sum structure for which several stochastic optimization methods, such as SAG/SAGA [13, 14], SDCA [15], SVRG [10], provide linear convergence rates.", "startOffset": 136, "endOffset": 144}, {"referenceID": 13, "context": "As mentioned earlier, the subproblems have the finite sum structure for which several stochastic optimization methods, such as SAG/SAGA [13, 14], SDCA [15], SVRG [10], provide linear convergence rates.", "startOffset": 151, "endOffset": 155}, {"referenceID": 8, "context": "As mentioned earlier, the subproblems have the finite sum structure for which several stochastic optimization methods, such as SAG/SAGA [13, 14], SDCA [15], SVRG [10], provide linear convergence rates.", "startOffset": 162, "endOffset": 166}, {"referenceID": 8, "context": "We quote the convergence rate of SVRG below from [10].", "startOffset": 49, "endOffset": 53}, {"referenceID": 14, "context": "Remarks We can also apply the recently developed accelerations techniques for first order optimization methods [16, 17].", "startOffset": 111, "endOffset": 119}, {"referenceID": 15, "context": "Remarks We can also apply the recently developed accelerations techniques for first order optimization methods [16, 17].", "startOffset": 111, "endOffset": 119}, {"referenceID": 16, "context": "Within our framework, we can use accelerated gradient descent instead and reduce the dependence on condition number to \u221a \u03ba [18].", "startOffset": 123, "endOffset": 127}, {"referenceID": 17, "context": "To adapt Algorithm 2 to this problem, one option is to extract the dimensions sequentially and remove the explained correlation from \u03a3xy each time we extract a new dimension [19].", "startOffset": 174, "endOffset": 178}, {"referenceID": 3, "context": "In this case, our subproblems become multi-dimensional regressions and our normalization steps are of the form Ut \u2190 \u0168t(\u0168t \u03a3xx\u0168t) 1 2 ; the same normalization is used by [4, 5].", "startOffset": 169, "endOffset": 175}, {"referenceID": 4, "context": "In this case, our subproblems become multi-dimensional regressions and our normalization steps are of the form Ut \u2190 \u0168t(\u0168t \u03a3xx\u0168t) 1 2 ; the same normalization is used by [4, 5].", "startOffset": 169, "endOffset": 175}, {"referenceID": 18, "context": "4 Related work Recent years have witnessed continuous efforts to scale up fundamental methods such as principal component analysis (PCA) and partial least squares with stochastic/online updates [20, 21, 22, 23, 24, 25, 26, 27, 6, 28, 29, 30].", "startOffset": 194, "endOffset": 241}, {"referenceID": 19, "context": "4 Related work Recent years have witnessed continuous efforts to scale up fundamental methods such as principal component analysis (PCA) and partial least squares with stochastic/online updates [20, 21, 22, 23, 24, 25, 26, 27, 6, 28, 29, 30].", "startOffset": 194, "endOffset": 241}, {"referenceID": 20, "context": "4 Related work Recent years have witnessed continuous efforts to scale up fundamental methods such as principal component analysis (PCA) and partial least squares with stochastic/online updates [20, 21, 22, 23, 24, 25, 26, 27, 6, 28, 29, 30].", "startOffset": 194, "endOffset": 241}, {"referenceID": 21, "context": "4 Related work Recent years have witnessed continuous efforts to scale up fundamental methods such as principal component analysis (PCA) and partial least squares with stochastic/online updates [20, 21, 22, 23, 24, 25, 26, 27, 6, 28, 29, 30].", "startOffset": 194, "endOffset": 241}, {"referenceID": 22, "context": "4 Related work Recent years have witnessed continuous efforts to scale up fundamental methods such as principal component analysis (PCA) and partial least squares with stochastic/online updates [20, 21, 22, 23, 24, 25, 26, 27, 6, 28, 29, 30].", "startOffset": 194, "endOffset": 241}, {"referenceID": 23, "context": "4 Related work Recent years have witnessed continuous efforts to scale up fundamental methods such as principal component analysis (PCA) and partial least squares with stochastic/online updates [20, 21, 22, 23, 24, 25, 26, 27, 6, 28, 29, 30].", "startOffset": 194, "endOffset": 241}, {"referenceID": 24, "context": "4 Related work Recent years have witnessed continuous efforts to scale up fundamental methods such as principal component analysis (PCA) and partial least squares with stochastic/online updates [20, 21, 22, 23, 24, 25, 26, 27, 6, 28, 29, 30].", "startOffset": 194, "endOffset": 241}, {"referenceID": 25, "context": "4 Related work Recent years have witnessed continuous efforts to scale up fundamental methods such as principal component analysis (PCA) and partial least squares with stochastic/online updates [20, 21, 22, 23, 24, 25, 26, 27, 6, 28, 29, 30].", "startOffset": 194, "endOffset": 241}, {"referenceID": 5, "context": "4 Related work Recent years have witnessed continuous efforts to scale up fundamental methods such as principal component analysis (PCA) and partial least squares with stochastic/online updates [20, 21, 22, 23, 24, 25, 26, 27, 6, 28, 29, 30].", "startOffset": 194, "endOffset": 241}, {"referenceID": 26, "context": "4 Related work Recent years have witnessed continuous efforts to scale up fundamental methods such as principal component analysis (PCA) and partial least squares with stochastic/online updates [20, 21, 22, 23, 24, 25, 26, 27, 6, 28, 29, 30].", "startOffset": 194, "endOffset": 241}, {"referenceID": 27, "context": "4 Related work Recent years have witnessed continuous efforts to scale up fundamental methods such as principal component analysis (PCA) and partial least squares with stochastic/online updates [20, 21, 22, 23, 24, 25, 26, 27, 6, 28, 29, 30].", "startOffset": 194, "endOffset": 241}, {"referenceID": 28, "context": "4 Related work Recent years have witnessed continuous efforts to scale up fundamental methods such as principal component analysis (PCA) and partial least squares with stochastic/online updates [20, 21, 22, 23, 24, 25, 26, 27, 6, 28, 29, 30].", "startOffset": 194, "endOffset": 241}, {"referenceID": 21, "context": "But as pointed out by [23], the CCA objective is more challenging due to the whitening constraints.", "startOffset": 22, "endOffset": 26}, {"referenceID": 26, "context": "Our algorithm is inspired by the stochastic PCA algorithm of [28] which transforms the nonconvex PCA objective into a small number of well-conditioned regularized least squares problems (solved by SVRG) through shifting and inverting the covariance matrix and running power iterations on the transformed matrix.", "startOffset": 61, "endOffset": 65}, {"referenceID": 29, "context": "[31] propose an adaptive CCA algorithm with efficient online updates based on matrix manifolds defined by the constraints.", "startOffset": 0, "endOffset": 4}, {"referenceID": 3, "context": "Similar to our algorithm, the stochastic CCA algorithms of [4, 5] are motivated by the alternating least squares formulation.", "startOffset": 59, "endOffset": 65}, {"referenceID": 4, "context": "Similar to our algorithm, the stochastic CCA algorithms of [4, 5] are motivated by the alternating least squares formulation.", "startOffset": 59, "endOffset": 65}, {"referenceID": 5, "context": "[6] proposed a stochastic algorithm based on the Lagrangian formulation of the CCA objective.", "startOffset": 0, "endOffset": 3}, {"referenceID": 30, "context": "5 Experiments In this section we demonstrate our algorithm, denoted by ALS-VR (Alternating Least Squares with Variance Reduction), on three real-world datasets: Mediamill [32], JW11 (a subset of the XRMB corpus [33]), and MNIST [34].", "startOffset": 171, "endOffset": 175}, {"referenceID": 31, "context": "5 Experiments In this section we demonstrate our algorithm, denoted by ALS-VR (Alternating Least Squares with Variance Reduction), on three real-world datasets: Mediamill [32], JW11 (a subset of the XRMB corpus [33]), and MNIST [34].", "startOffset": 211, "endOffset": 215}, {"referenceID": 32, "context": "5 Experiments In this section we demonstrate our algorithm, denoted by ALS-VR (Alternating Least Squares with Variance Reduction), on three real-world datasets: Mediamill [32], JW11 (a subset of the XRMB corpus [33]), and MNIST [34].", "startOffset": 228, "endOffset": 232}, {"referenceID": 3, "context": "These datasets have also been used by [4, 5] for demonstrating their stochastic CCA algorithms.", "startOffset": 38, "endOffset": 44}, {"referenceID": 4, "context": "These datasets have also been used by [4, 5] for demonstrating their stochastic CCA algorithms.", "startOffset": 38, "endOffset": 44}, {"referenceID": 3, "context": "We compare ALS-VR with batch AppGrad, and its stochastic version s-AppGrad [4] with both gradient and normalization steps estimated with minibatchs of 200 samples.", "startOffset": 75, "endOffset": 78}, {"referenceID": 4, "context": "4 We do not compare the algorithm of [5] because the main difference between it and AppGrad is that it uses adaptive The authors of [4] suggest that the minibatch size shall be at least the same magnitude as L.", "startOffset": 37, "endOffset": 40}, {"referenceID": 3, "context": "4 We do not compare the algorithm of [5] because the main difference between it and AppGrad is that it uses adaptive The authors of [4] suggest that the minibatch size shall be at least the same magnitude as L.", "startOffset": 132, "endOffset": 135}], "year": 2017, "abstractText": "We study the stochastic optimization of canonical correlation analysis (CCA), whose objective is nonconvex and does not decouple over training samples. Although several stochastic optimization algorithms have been recently proposed to solve this problem, no global convergence guarantee was provided by any of them. Based on the alternating least squares formulation of CCA, we propose a globally convergent stochastic algorithm, which solves the resulting least squares problems approximately to sufficient accuracy with state-of-the-art stochastic gradient methods for convex optimization. We provide the overall time complexity of our algorithm which significantly improves upon that of previous work. Experimental results demonstrate the superior performance of our algorithm.", "creator": "dvips(k) 5.991 Copyright 2011 Radical Eye Software"}}}