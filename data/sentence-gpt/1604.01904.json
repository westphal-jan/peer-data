{"id": "1604.01904", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "7-Apr-2016", "title": "Neural Headline Generation with Sentence-wise Optimization", "abstract": "Automatic headline generation is an important research area within text summarization and sentence compression. Recently, neural headline generation models have been proposed to take advantage of well-trained neural networks in learning sentence representations and mapping sequence to sequence. Nevertheless, traditional neural network encoder utilizes maximum likelihood estimation for parameter optimization, which essentially constraints the expected training objective within word level instead of sentence level. In particular, we are interested in the ability to predict and model prediction and prediction of words in a given sentence (Figure 1).\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n", "histories": [["v1", "Thu, 7 Apr 2016 07:47:11 GMT  (237kb,D)", "http://arxiv.org/abs/1604.01904v1", null], ["v2", "Sun, 9 Oct 2016 07:16:24 GMT  (182kb,D)", "http://arxiv.org/abs/1604.01904v2", null]], "reviews": [], "SUBJECTS": "cs.CL", "authors": ["ayana", "shiqi shen", "yu zhao", "zhiyuan liu", "maosong sun"], "accepted": false, "id": "1604.01904"}, "pdf": {"name": "1604.01904.pdf", "metadata": {"source": "CRF", "title": "Neural Headline Generation with Minimum Risk Training", "authors": ["Ayana", "Shiqi Shen", "Zhiyuan Liu", "Maosong Sun"], "emails": [], "sections": [{"heading": "1 Introduction", "text": "Automatic text summarization is the process of creating a coherent, informative and short summary for a document. Text summarization approaches can be divided into two typical categories: extractive and generative. Extractive summarization simply selects a subset of existing sentences from original documents as summary. Despite of its simplicity, extractive summarization suffers from some intrinsic drawbacks, e.g., unable to generate coherent and compact summary in arbitrary length, especially when shorter than one sentence.\nIn contrast, generative summarization builds semantic representations of a document and creates a summary with sentences not explicitly presented in the original document. When the generated summary is required to be a single compact sentence, we name the summarization task as headline generation [4].\nGenerative summarization needs to accurately understand and represent the semantics of original document, and then generate informative summary according to document representations. Most previous works heavily rely on modeling la-\ntent linguistic structures of input document, such as syntactic parsing and semantic parsing, which always bring inevitable errors and degrade summarization quality.\nRecent years have witnessed great success of deep neural models for various natural language processing tasks [3; 19; 1; 14] including text summarization. Take neural headline generation (NHG) for example, it learns to build a large neural network, which takes a document as input and directly outputs a compact sentence as headline of the document. NHG exhibits the following advantages: (1) NHG is fully datadriven, requiring no human annotation and other linguistic information. (2) NHG is completely end-to-end, which does not explicitly model latent linguistic structures, and thus prevents error propagation. Moreover, the attention mechanism [1] is introduced in NHG, which learns a soft alignment over input document to generate more accurate headline [15].\nNHG has achieved great advantages as compared to conventional generative summarization methods. Nevertheless, NHG still confronts an important drawback: current NHG models are mostly optimized with maximum likelihood estimation (MLE) over training data, without taking evaluation metrics of text summarization into consideration. It constraints the expected training objective within word level instead of sentence level and prevents NHG from capturing various aspects of summarization quality.\nTo address this issue, we argue that it is desirable to build NHG models in which parameters are tuned with respect to specific evaluation criteria such as ROUGE. We apply the minimum Bayes risk technique and perform minimum risk training (MRT) to develop NHG models. MRT aims at minimizing the expected loss over training data by taking automatic evaluation metric into consideration. Although MRT has been widely used in many NLP tasks such as statistical machine translation [12; 18; 6; 17], it has not been well considered in generative summarization, especially in NHG models.\nWe conduct experiments on two real-world datasets in English and Chinese respectively. Experiment results show that, NHG with MRT can significantly and consistently improve the summarization performance as compared to conventional NHG models and other baselines. Moreover, we explore the influence of employing different evaluation metrics and find the superiority of our model stable in MRT.\nar X\niv :1\n60 4.\n01 90\n4v 1\n[ cs\n.C L\n] 7\nA pr\n2 01\n6"}, {"heading": "2 Neural Headline Generation Model", "text": "We first introduce Neural Headline Generation (NHG) in this section, then introduce minimum risk training (MRT) for NHG in the next section.\nWe formalize the task of headline generation as follows. Denote the input document x as a sequence of words x = {x1, \u00b7 \u00b7 \u00b7 ,xM} with M words, in which each word xi comes from a fixed word vocabulary V of size |V |. Headline generator aims to take x as input, and generates a short headline y = {y1, \u00b7 \u00b7 \u00b7 ,yN} with length N < M , so as to maximize the conditional probability of y given x, i.e., arg max Pr(y|x).\nWe follow the Markov assumption to generate headline words yj in sequence, and the log conditional probability can be further formalized as:\nlog Pr(y|x; \u03b8) = N\u2211 j=1 log Pr(yj |x,y<j ; \u03b8), (1)\nwhere y<j = {y1, . . . ,yj\u22121}, \u03b8 indicates model parameters. That is, the j-th word yj in headline is generated according to all y<j generated in past and the input document x. In NHG, we design an encoder-decoder framework to parameterize Pr(yj |x,y<j ; \u03b8). The framework is shown in Fig. 1.\nThe encoder of NHG takes the document x as input and encodes it into a low-dimensional vector c. More specifically, we apply recurrent neural networks (RNN) to get hidden states\nhi = f(xi,hi\u22121), (2)\nfor each input word xi, and then we obtain the vector c = g({h1, . . . ,hM}), where g is a function that combines hidden states.\nIn the decoder of NHG, we also employ RNN to obtain hidden states sj , and generate headline word by word based on c, sj and y<j , i.e.,\nPr(yj |x,y<j) = Pr(yj |c, sj ,y<j). (3)\nWe introduce the encoder and decoder in detail as follows."}, {"heading": "2.1 Encoder", "text": "in this progress, we apply gated recurrent unit (GRU) to build a bidirectional RNN as the encoder of NHG."}, {"heading": "Gated Recurrent Unit", "text": "The RNN in NHG is implemented using GRU, which is originally proposed for NMT [3]. As shown in Fig. 2, GRU can adaptively capture dependencies of input sequence by introducing the update gate ui and reset gate ri. The reset gate determines whether to ignore previous hidden states. If the reset gate is close to 1, the update gate controls how much of previous hidden states will pass on. In GRU, hi and h\u0303i are generated hidden state and candidate activation. GRU calculates the i-th hidden state as follows:\nri = \u03c3 ( WrExi + Urhi\u22121 ) (4)\nui = \u03c3 ( WuExi + Uuhi\u22121 ) (5)\nh\u0303i = tanh ( WhExi + Uh(ri hi\u22121) ) (6)\nhi = ui hi\u22121 + (1\u2212 ui) h\u0303i (7)\nHere \u03c3() is the sigmoid function, indicates element-wise multiplication, E \u2208 RD\u00d7V is word embedding matrix, Wr,Wu,Wh \u2208 RH\u00d7D and Ur,Uu,Uh \u2208 RH\u00d7H are weighting matrices, with D and H denoting dimensions of word embeddings and hidden states respectively. Note that h0 is set to 0 vector."}, {"heading": "Bidirectional RNN", "text": "Conventional RNNs typically deal with text sequence from start to end, and build the hidden state of each word only considering its preceding words. It has been verified that, the hidden state should also consider its following words as well. Hence, we apply bidirectional RNN (BRNN) [16] to learn hidden states using both preceding and following words.\nAs shown in Figure 1, BRNN processes the input document in both forward direction and backward direction with two separate hidden layers calculated with GRUs, obtains the forward hidden states ( \u2212\u2192 h 1, . . . , \u2212\u2192 hM ) and the backward hidden states ( \u2190\u2212 h 1, . . . , \u2190\u2212 hM ). For each position i, we simply concatenate its both forward and backward states into the final hidden state:\nhi = \u2212\u2192 h i \u2295 \u2190\u2212 h i, (8)\nin which \u2212\u2192 h i is calculated following Eq. (2) and \u2190\u2212 h i is calculated following \u2190\u2212 hi = f(xi, \u2190\u2212 h i+1), operator \u2295 indicates concatenation."}, {"heading": "2.2 Decoder", "text": "We also use GRU to implement the decoder, which is expected to generate headline words using Eq. (3). The decoder calculates the j-th headline word as follows:\nrj = \u03c3 ( WrEyj\u22121 + Ursj\u22121 + Crcj ) (9)\nuj = \u03c3 ( WuEyj\u22121 + Uusj\u22121 + Cucj ) (10)\ns\u0303j = tanh ( WhEyj\u22121 + Uh(rj sj\u22121) + Chcj ) (11)\nsj = uj sj\u22121 + (1\u2212 uj) s\u0303j (12)\nThe notations are identical to those of GRUs in the encoder and Cr,Cu,Ch \u2208 RH\u00d72H are weighting matrices as well. We simply set s0 = tanh(Wf \u2190\u2212 h 1) with Wf \u2208 RH\u00d7H .\nAlso note that, the context vector cj conceives attention information of the input document given preterit generated headline words.\ncj = M\u2211 i=1 \u03b1jihi, (13)\nwhere hi is hidden state from the encoder, \u03b1ji indicates how much the i-th word xi from the original document contributes to generating the j-th word in headline. \u03b1ji is computed as follows:\n\u03b1ji = softmax ( z> tanh(W\u03b1sj\u22121 + U\u03b1hi) ) , (14)\nwhere z is the weighting vector, and W\u03b1 and U\u03b1 are weighting matrices.\nAfter introducing NHG in details, we describe minimum risk training for NHG in the next section."}, {"heading": "3 Minimum Risk Training for NHG", "text": "The model parameters of NHG can be estimated with large-scale document-headline pairs. We denote the training set as D = {(x(1),y(1)), . . . , (x(T ),y(T ))}. Before introducing minimum risk training, we begin with the conventional optimization technique, maximum likelihood estimation, for NHG."}, {"heading": "3.1 Maximum Likelihood Estimation", "text": "Maximum likelihood estimation (MLE) finds optimized parameters which can maximize the log likelihood of generating headlines over training set D:\nLMLE(\u03b8) = \u2211\n(x,y)\u2208D\nlog Pr(y|x; \u03b8), (15)\nwhere Pr(y|x; \u03b8) is defined in Eq. (1). Therefor, fundamentally the model is trained to maximize Pr(yj |x,y<j ; \u03b8) step by step. By doing so, the training procedure will inevitably lose global information. On the other hand, during training, y<j are authentic words from standard headline. However during testing, y<j are predicted by model and may not be correct, which lead to error propagation and inaccurate headline generation. Minimum risk training on the other hand, can improve these problems."}, {"heading": "3.2 Minimum Risk Training", "text": "Minimum risk training (MRT) aims to minimize the expected loss, i.e. risk, over the training data. Given a document x, we define Y(x; \u03b8) as the set of all possible headlines generated by NHG with parameters \u03b8. Regarding y as the gold-standard headline of x, we define the loss of a generated headline y\u2032 as semantic distance between y\u2032 and the standard y, denoted as \u2206(y\u2032,y). MRT defines the objective function as follows:\nLMRT(\u03b8) = \u2211\n(x,y)\u2208D\nEY(x;\u03b8)\u2206(y\u2032,y). (16)\nHere EY(x;\u03b8) indicates the expectation over the set Y(x; \u03b8). Thus the objective function of MRT can be further formalized as:\nLMRT(\u03b8) = \u2211\n(x,y)\u2208D \u2211 y\u2032\u2208Y(x;\u03b8) Pr(y\u2032|x; \u03b8)\u2206(y\u2032,y). (17)\nMoreover, it is usually time-consuming and inefficient to enumerate all possible instances in Y(x; \u03b8). For simplicity, we only sample a significant subset S(x; \u03b8) \u2282 Y(x; \u03b8) to approximate the probability distribution, and formalize the objective function as: LMRT(\u03b8) = \u2211\n(x,y)\u2208D \u2211 y\u2032\u2208S(x;\u03b8) Pr(y\u2032|x; \u03b8) \u2211 y\u2217\u2208S(x;\u03b8) Pr(y \u2217|x; \u03b8) \u2206(y \u2032,y),\n(18) in which is a hyper-parameter [12] that controls the smoothness of the objective function. Selection of this parameter directly affects the optimization results.\nMRT exploits \u2206(y\u2032,y) to measure the loss, which enables us to optimize NHG models with respect to the specific evaluation metrics of the task. As we know, the most widely adopted evaluation metric for document summarization is ROUGE [11] (Recall-Oriented Understudy of Gisting Evaluation). Hence, we simply measure the semantic distance \u2206(y\u2032,y) with ROUGE."}, {"heading": "3.3 ROUGE", "text": "ROUGE automatically measures summary quality by comparing computer-generated summaries to standard summaries created by human. ROUGE is the common evaluation metric in Document Understanding Conference (DUC), a largescale summarization evaluation sponsored by NIST [11]. The basic idea of ROUGE is to count the number of overlapping units such as overlapped n-grams, word sequences, and word pairs between computer-generated summaries and the standard summaries.\nIn this project, we consider two types of ROUGE: ROUGE-N and ROUGE-L. ROUGE-N counts n-grams, and ROUGE-L counts longest common sub-sequences.\nSuppose y\u2032 is the generated summary, and y is the standard summary. ROUGE-N is defined as follows:\nROUGE-N =\n\u2211 gramN\u2208y\nM(gramN)\u2211 gramN\u2208y C(gramN) , (19)\nwhere N indicates the type of N-gram (e.g., uni-gram and bi-gram, corresponding to ROUGE-1 and ROUGE-2),\nM(gramN) is the number of n-grams matched between y \u2032 and y, and C(gramN) is the total number of n-grams in y. ROUGE-L is formalized as:\nROUGE-L = (1 + \u03b22)RLPL RL + \u03b22PL . (20)\nHere \u03b2 is the harmonic factor between recallRL and precision PL, which are defined as:\nRL = Lcs(y\u2032,y)\nLen(y) , PL = Lcs(y\u2032,y) Len(y\u2032) , (21)\nwhere Lcs(y\u2032,y) is the length of longest common subsequence between y\u2032 and y, and Len(y) is the length of y.\nFundamentally ROUGE is a similarity metric. Hence, we simply define \u2206(y\u2032,y) = \u2212ROUGE-{1,2,L} in MRT."}, {"heading": "4 Experiments", "text": "We conduct experiments on both English and Chinese datasets and compare the performance of our model with several baseline systems. In this section we introduce datasets, baseline systems and experiment results in details."}, {"heading": "4.1 Datasets", "text": ""}, {"heading": "English Dataset", "text": "For English, we utilize the English Gigaword Fifth Edition [13] for training models. It is one of the largest static corpus of English news, consisting of nearly 10 million news articles from 7 news outlets, with a total of more than 4 billion words. In order to compare our work with [15], we take the same preprocessing techniques as theirs.\nWe opt the first sentence of each news article and pair it with its corresponding headline as an article-headline pair. To avoid noises in articles and headlines that may influence the performance, we filter out bylines, extraneous editing marks and question marks. The training set contains approximately 4 million article-headline pairs after the filtering step. We take DUC-2003 evaluation dataset as our validation set to determine the hyper-parameters resulting the best performance in minimum risk training. DUC-2003 contains 624 short articles, each of which corresponds to four human-generated reference headlines.\nWe use the dataset from DUC-2004 Task-1 as our test set. The dataset consists of 500 news articles from Associated Press Wire services and New York Times. Each article is paired with four human-generated reference headlines, with maximum length of 75 characters. 1"}, {"heading": "Chinese Dataset", "text": "We also implement experiments on a Chinese dataset LCSTS [8]. This dataset consists of article-headline pairs extracted from Sina Weibo 2, a Chinese social media that allows users disseminate and share information with their friends. A typical news article posted in Weibo is limited to 140 Chinese characters, and the corresponding headline is usually set in a pair of square brackets at the beginning of the news article.\n1The dataset can be downloaded from http://duc.nist. gov/ with agreements.\n2The website of Sina Weibo is http://weibo.com/\nThe LCSTS consists of three parts. Part-I incorporates about 2.4 million article-headline pairs. Part-II and Part-III contain article-headline pairs with human-labeled scores, indicating the relatedness between the article and its headline. The higher score, the more related they are to each other. PartII consists of 10, 666 and Part-III only 1, 106 article-headline pairs. Each pair in Part-II is labeled by only one annotator, and in Part-III is by three annotators.\nWe utilize Part-I of LCSTS for model training, and we opt the article-title pairs in Part-III with scores greater than or equal to 3 for testing. It is worth mentioning that, we take Chinese characters as inputs of NHG instead of words, in order to prevent the errors in Chinese word segmentation."}, {"heading": "4.2 Baseline Systems", "text": "In this progress, we compare our model against the following baselines on English headline generation task:\n\u2022 TOPIARY [21] is the winner system of DUC2004 Task1. This system utilizes linguistic-based sentence compression method and unsupervised topic detection at the same time, and achieves good performance. \u2022 MOSES+ [15] generate headlines based on a widely-\nused phrase-based machine translation system MOSES [10]. The MOSES+ system further takes two steps to improve the quality of generated headlines. One step is to enlarge phrase table and the other is to use MERT to tune models. \u2022 ABS and ABS+ [15] are attention-based neural models\nto generate short summaries given a sentence. The difference between the two systems is that, ABS+ takes an extractive tuning procedure to revise model parameters based on ABS. They both utilize DUC-2003 data as validation set to take a MERT tuning step.\nAmong them, MOSES+, ABS and ABS+ take the same dataset for learning."}, {"heading": "4.3 Implementation Details", "text": "In MRT, we initialize model parameters using the optimized parameters learned from NHG with MLE. In particular, the size of S(x; \u03b8) for each x is important: when the size is too small, the sampling will not be sufficient and hurt the performance; when the size is too large, the learning time will grow correspondingly. In this progeress, we set the size to 100 to achieve a trade-off between effectiveness and efficiency. We select top-ranked summaries, i.e., those with the largest generation probabilities, generated by the up-to-date NHG model into S(x; \u03b8). The hyper-parameter in Eq.(18) is set to 5\u00d7 10\u22123.\nTable 1 shows most of the hyper-parameters that we use in our systems and they remain the same in both MLE training\nand MRT training and we have not tuned them. We keep a fix-sized vocabulary for training and evaluation.\nThose words that are not included in this vocabulary are mapped to a special token [UNK]. When generating headlines, we utilize a [UNK] replacement technique following the idea in [9]. For English dataset, we preprocess with tokenization, lower-casing. For both English and Chinese, we also replace all digits with #.\nAll models are trained on Tesla K20 GPU. For NHG+MLE on the English dataset, it takes about 3.75 hours to conduct 10, 000 of iterations, and for NHG+MRT, it is about 5.5 hours. The training time of MRT is a bit longer than MLE because MRT has to learn on 100 sampled headlines for each article."}, {"heading": "4.4 Experiment Results and Analysis", "text": "We utilize ROUGE [11] that introduced in Section 3.3 to measure the performance of various models. For each model, we report ROUGE-1, ROUGE-2 and ROUGE-L scores in the form of percentage."}, {"heading": "Evaluation Results on English and Chinese", "text": "Table 2 shows the evaluation results of English headline generation on DUC-2004. TOPIARY, MOSES+, ABS and ABS+ are baselines introduced in Section 4.2. Since we carry out training and evaluation on the same dataset, and hence we simply use the evaluation results reported in [15]. NHG+MLE and NHG+MRT are neural headline generation models we described in Section 2 learned with MLE and MRT respectively.\nFrom Table 2 we can observe that: (1) NHG learned with MLE achieves competitive performance to the best baseline systems, especially on ROUGE-2 and ROUGE-L, and is slightly worse than ABS+ on ROUGE-1. This indicates that NHG is effective for headline generation. (2) NHG learned with MRT significantly and consistently outperforms NHG with MLE under three evaluation metrics.\nThe similar results are observed from the Chinese dataset as well, as shown in Table 3. NHG with MRT improves ROUGE scores up to nearly 3 points to NHG with MLE.\nThe significant superiority on both English and Chinese suggests that, MRT is more effective for learning NHG models as compared to MLE."}, {"heading": "Effectiveness of Evaluation Metrics", "text": "We are interested in the effectiveness of utilizing various evaluation metrics in MRT, i.e., ROUGE-1, ROUGE-2 and ROUGE-L, for headline generation performance.\nTable 4 and 5 show the results of MRT with various evaluation metrics on English validation and test set respectively.\nFrom Table 4 and 5, we observe that: (1) All NHG+MRT models with three evaluation metrics consistently outperform NHG+MLE. This indicates that the MRT technique is robust when evaluation metric varies. (2) It is straightforward that, NHG+MRT models with ROUGE-1 and ROUGE-2 perform slightly better when evaluated using the corresponding metrics. But when evaluated using ROUGE-L, NHG+MRT with ROUGE-L does not perform the best. The reason may be that, MRT with ROUGE-1 and ROUGE-2 is more compatible with the RNN encoder-decoder architecture. In contrast, ROUGE-L measures similarity based on longest common subsequences, which cannot be well considered in RNN."}, {"heading": "Case Study", "text": "To demonstrate the effectivenesses of MRT, we present several example outputs for comparison as shown in Table 6. From these examples we can observe that: (1) NHG with MRT generally is able to capture the important part of a document. For example in Article 1, the main subject is Honduras prepared for hurricane. NHG+MRT can successfully find the theme and generate a headline about Honduras, but NHG+MLE failed. There is a similar situation in Article 2. (2) When both systems capture the correct subject, NHG+MRT can grasp and generate more informative headline. As shown in Article 3, NHG+MLE missed the information who reported the fact, but NHG+MRT provided concisely with \u201cU.N.:\u201d. (3) NHG+MLE usually suffers from generating repeated words or phrases as we present in Article 4. NHG+MLE repeats the phrase \u201cAsian games\u201d twice and leads to a semantically incomplete headline. NHG+MRT seems to be able to overcome this issue, benefitting from directly optimizing sentence-level ROUGE.\nIn summary, all these examples indicate the effectiveness\nof neural headline generation with minimum risk training and its superiority over maximum likelihood estimation."}, {"heading": "5 Related Work", "text": "Headline generation is a well-defined task standardized in DUC-2003 and DUC-2004. Various approaches have been proposed for headline generation: rule-based, statisticalbased and neural-based, introduced in detail as follows.\nThe rule-based models create a headline for a news article using handcrafted and linguistically motivated rules to guide the choice of a potential headline. Hedge Trimmer [4] is a representative example of this approach which creates a headline by removing constituents from the parse tree of the first sentence until it reaches a specific length limit. Systems in this approach are in accordance with human intuition and easy to understand. However, it is unrealistic and impossible to induce every single rule due to the complexity of human languages.\nThe statistical-based methods make use of large scale training data to learn correlations between words in headlines and those in articles. For example, [2] applies statistical models\nfor content selection and surface realization to produce headlines. The best system on DUC-2004, TOPIARY [21] combines both linguistic and statistical information to generate headlines. There are also methods make use of knowledge bases to generate better headlines. For example, [20] utilizes Wikipedia to extract features like word inlinks, outlinks and categories to select keywords from an article and constitute a headline.\nRecently, with the advances of deep neural networks, there are growing works that design neural networks for headline generation. [15] proposes an attention-based model to generate headlines. This model introduces attention mechanism into sentence compression without recurrent unit. [5] proposes a recurrent neural network with long short term memory (LSTM) [7] for headline generation. This model regards headline generation as sentence compression, with words annotated as preserved or not to form the compressed sentence. This model is essentially an extractive model, unable to generate headlines with those words out of original document, which to some extent restricts its applicability.\nIn this work, we propose the NHG model realized by a bidirectional recurrent neural network with gated recurrent units. We also propose to apply minimum risk training (MRT) to optimize parameters of NHG model. MRT has been widely used in machine translation [12; 18; 6; 17], but less been explored in document summarization. To the best of our knowledge, this work is the first attempt to utilize MRT in neural headline generation."}, {"heading": "6 Conclusion and Future Work", "text": "In this progress, we build an end-to-end neural headline generation model, which does not require heavy linguistic analysis and is fully data-driven. We apply minimum risk training for model learning, which is able to take specific evaluation metrics into consideration. Evaluation results show significant and consistent improvements of NHG with MRT over both English and Chinese datasets, as compared to other baselines including NHG with MLE.\nThere are still many open problems to be explored as future work: (1) Besides article-headline pairs, there are also rich plain text data not considered in NHG training. We will investigate the probability of integrating these plain text to enhance NHG for semi-supervised learning. (2) We will investigate the hybrid approach of incorporating NHG with other successful headline generation approaches like sentence compression models. (3) Both input and output of NHG are typically in sentence level. We will investigate the effectiveness of neural models and MRT on more complicated summarization tasks like single document summarization and multiple document summarization."}], "references": [{"title": "Neural machine translation by jointly learning to align and translate", "author": ["Dzmitry Bahdanau", "Kyunghyun Cho", "Yoshua Bengio"], "venue": "In Proceedings of ICLR,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2015}, {"title": "Headline generation based on statistical translation", "author": ["Michele Banko", "Vibhu O Mittal", "Michael J Witbrock"], "venue": "In Proceedings of ACL,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2000}, {"title": "Learning phrase representations using rnn encoder\u2013decoder for statistical machine translation", "author": ["Kyunghyun Cho", "Bart van Merrienboer", "Caglar Gulcehre", "Dzmitry Bahdanau", "Fethi Bougares", "Holger Schwenk", "Yoshua Bengio"], "venue": null, "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2014}, {"title": "Hedge trimmer: A parse-and-trim approach to headline generation", "author": ["Bonnie Dorr", "David Zajic", "Richard Schwartz"], "venue": "In Proceedings of HLT-NAACL,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2003}, {"title": "Sentence compression by deletion with lstms", "author": ["Katja Filippova", "Enrique Alfonseca", "Carlos A Colmenares", "Lukasz Kaiser", "Oriol Vinyals"], "venue": "In Proceedings of EMNLP,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2015}, {"title": "Learning continuous phrase representations for translation modeling", "author": ["Jianfeng Gao", "Xiaodong He", "Wen-tau Yih", "Li Deng"], "venue": "In Proceedings of ACL,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2014}, {"title": "Long shortterm memory", "author": ["Sepp Hochreiter", "J\u00fcrgen Schmidhuber"], "venue": "Neural computation,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 1997}, {"title": "Lcsts: A large scale chinese short text summarization dataset", "author": ["Baotian Hu", "Qingcai Chen", "Fangze Zhu"], "venue": "In Proceedings of EMNLP,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2015}, {"title": "On using very large target vocabulary for neural machine translation", "author": ["S\u00e9bastien Jean", "Kyunghyun Cho", "Roland Memisevic", "Yoshua Bengio"], "venue": "In Proceedings of ACL-IJCNLP,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2015}, {"title": "Moses: Open source toolkit for statistical machine translation", "author": ["Philipp Koehn", "Hieu Hoang", "Alexandra Birch", "Chris Callison-Burch", "Marcello Federico", "Nicola Bertoldi", "Brooke Cowan", "Wade Shen", "Christine Moran", "Richard Zens"], "venue": "In Proceedings of ACL,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2007}, {"title": "Rouge: A package for automatic evaluation of summaries", "author": ["Chin-Yew Lin"], "venue": "In Proceedings of ACL,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2004}, {"title": "Minimum error rate training in statistical machine translation", "author": ["Franz Josef Och"], "venue": "In Proceedings of ACL,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2003}, {"title": "English gigaword fifth edition, june", "author": ["Robert Parker", "David Graff", "Junbo Kong", "Ke Chen", "Kazuaki Maeda"], "venue": null, "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2011}, {"title": "Sequence level training with recurrent neural networks", "author": ["Marc\u2019Aurelio Ranzato", "Sumit Chopra", "Michael Auli", "Wojciech Zaremba"], "venue": "arXiv preprint arXiv:1511.06732,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2015}, {"title": "A neural attention model for abstractive sentence summarization", "author": ["Alexander M Rush", "Sumit Chopra", "Jason Weston"], "venue": "In Proceedings of EMNLP,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2015}, {"title": "Bidirectional recurrent neural networks", "author": ["Mike Schuster", "Kuldip K Paliwal"], "venue": "IEEE Transactions on Signal Processing,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 1997}, {"title": "Minimum risk training for neural machine translation", "author": ["Shiqi Shen", "Yong Cheng", "Zhongjun He", "Wei He", "Hua Wu", "Maosong Sun", "Yang Liu"], "venue": "arXiv preprint arXiv:1512.02433,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2015}, {"title": "Minimum risk annealing for training log-linear models", "author": ["David A Smith", "Jason Eisner"], "venue": "In Proceedings of COLING/ACL,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2006}, {"title": "Sequence to sequence learning with neural networks", "author": ["Ilya Sutskever", "Oriol Vinyals", "Quoc VV Le"], "venue": "In Proceedings of NIPS,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2014}, {"title": "Keyword extraction and headline generation using novel word features", "author": ["Songhua Xu", "Shaohui Yang", "Francis Chi-Moon Lau"], "venue": "In Proceedings of AAAI,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2010}, {"title": "Bbn/umd at duc-2004: Topiary", "author": ["David Zajic", "Bonnie Dorr", "Richard Schwartz"], "venue": "In Proceedings of HLT- NAACL,", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2004}], "referenceMentions": [{"referenceID": 3, "context": "When the generated summary is required to be a single compact sentence, we name the summarization task as headline generation [4].", "startOffset": 126, "endOffset": 129}, {"referenceID": 0, "context": "Moreover, the attention mechanism [1] is introduced in NHG, which learns a soft alignment over input document to generate more accurate headline [15].", "startOffset": 34, "endOffset": 37}, {"referenceID": 14, "context": "Moreover, the attention mechanism [1] is introduced in NHG, which learns a soft alignment over input document to generate more accurate headline [15].", "startOffset": 145, "endOffset": 149}, {"referenceID": 2, "context": "The RNN in NHG is implemented using GRU, which is originally proposed for NMT [3].", "startOffset": 78, "endOffset": 81}, {"referenceID": 15, "context": "Hence, we apply bidirectional RNN (BRNN) [16] to learn hidden states using both preceding and following words.", "startOffset": 41, "endOffset": 45}, {"referenceID": 11, "context": "(18) in which is a hyper-parameter [12] that controls the smoothness of the objective function.", "startOffset": 35, "endOffset": 39}, {"referenceID": 10, "context": "As we know, the most widely adopted evaluation metric for document summarization is ROUGE [11] (Recall-Oriented Understudy of Gisting Evaluation).", "startOffset": 90, "endOffset": 94}, {"referenceID": 10, "context": "ROUGE is the common evaluation metric in Document Understanding Conference (DUC), a largescale summarization evaluation sponsored by NIST [11].", "startOffset": 138, "endOffset": 142}, {"referenceID": 12, "context": "For English, we utilize the English Gigaword Fifth Edition [13] for training models.", "startOffset": 59, "endOffset": 63}, {"referenceID": 14, "context": "In order to compare our work with [15], we take the same preprocessing techniques as theirs.", "startOffset": 34, "endOffset": 38}, {"referenceID": 7, "context": "We also implement experiments on a Chinese dataset LCSTS [8].", "startOffset": 57, "endOffset": 60}, {"referenceID": 20, "context": "\u2022 TOPIARY [21] is the winner system of DUC2004 Task1.", "startOffset": 10, "endOffset": 14}, {"referenceID": 14, "context": "\u2022 MOSES+ [15] generate headlines based on a widelyused phrase-based machine translation system MOSES [10].", "startOffset": 9, "endOffset": 13}, {"referenceID": 9, "context": "\u2022 MOSES+ [15] generate headlines based on a widelyused phrase-based machine translation system MOSES [10].", "startOffset": 101, "endOffset": 105}, {"referenceID": 14, "context": "\u2022 ABS and ABS+ [15] are attention-based neural models to generate short summaries given a sentence.", "startOffset": 15, "endOffset": 19}, {"referenceID": 8, "context": "When generating headlines, we utilize a [UNK] replacement technique following the idea in [9].", "startOffset": 90, "endOffset": 93}, {"referenceID": 10, "context": "We utilize ROUGE [11] that introduced in Section 3.", "startOffset": 17, "endOffset": 21}, {"referenceID": 14, "context": "Since we carry out training and evaluation on the same dataset, and hence we simply use the evaluation results reported in [15].", "startOffset": 123, "endOffset": 127}, {"referenceID": 3, "context": "Hedge Trimmer [4] is a representative example of this approach which creates a headline by removing constituents from the parse tree of the first sentence until it reaches a specific length limit.", "startOffset": 14, "endOffset": 17}, {"referenceID": 1, "context": "For example, [2] applies statistical models for content selection and surface realization to produce headlines.", "startOffset": 13, "endOffset": 16}, {"referenceID": 20, "context": "The best system on DUC-2004, TOPIARY [21] combines both linguistic and statistical information to generate headlines.", "startOffset": 37, "endOffset": 41}, {"referenceID": 19, "context": "For example, [20] utilizes Wikipedia to extract features like word inlinks, outlinks and categories to select keywords from an article and constitute a headline.", "startOffset": 13, "endOffset": 17}, {"referenceID": 14, "context": "[15] proposes an attention-based model to generate headlines.", "startOffset": 0, "endOffset": 4}, {"referenceID": 4, "context": "[5] proposes a recurrent neural network with long short term memory (LSTM) [7] for headline generation.", "startOffset": 0, "endOffset": 3}, {"referenceID": 6, "context": "[5] proposes a recurrent neural network with long short term memory (LSTM) [7] for headline generation.", "startOffset": 75, "endOffset": 78}], "year": 2017, "abstractText": "Automatic headline generation is an important research area within text summarization and sentence compression. Recently, neural headline generation models have been proposed to take advantage of well-trained neural networks in learning sentence representations and mapping sequence to sequence. Nevertheless, traditional neural network encoder utilizes maximum likelihood estimation for parameter optimization, which essentially constraints the expected training objective within word level instead of sentence level. Moreover, the performance of model prediction significantly relies on training data distribution. To overcome these drawbacks, we employ minimum risk training strategy in this paper, which directly optimizes model parameters with respect to evaluation metrics and statistically leads to significant improvements for headline generation. Experiment results show that our approach outperforms state-of-the-art systems on both English and Chinese headline generation tasks.", "creator": "LaTeX with hyperref package"}}}