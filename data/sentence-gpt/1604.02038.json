{"id": "1604.02038", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "7-Apr-2016", "title": "Sentence Level Recurrent Topic Model: Letting Topics Speak for Themselves", "abstract": "We propose Sentence Level Recurrent Topic Model (SLRTM), a new topic model that assumes the generation of each word within a sentence to depend on both the topic of the sentence and the whole history of its preceding words in the sentence. Different from conventional topic models that largely ignore the sequential order of words or their topic coherence, SLRTM gives full characterization to them by using a Recurrent Neural Networks (RNN) based framework. Experimental results have shown that SLRTM outperforms several strong baselines on various tasks. Furthermore, SLRTM can automatically generate sentences given a topic (i.e., topics to sentences), which is a key technology for real world applications such as personalized short text conversation. Using the model, we provide a new way for researchers to use SLRTM to estimate which sentences should be in the sentence for a specific topic and to infer its value by estimating their probability from multiple studies. Moreover, using a Recurrent Neural Networks (RNN), we demonstrate that it can predict the likelihood of sentences that contain a topic in a sentence in a sentence in a sentence in a sentence in a sentence in a sentence in a sentence in a sentence in a sentence in a sentence in a sentence in a sentence in a sentence in a sentence in a sentence in a sentence in a sentence in a sentence in a sentence in a sentence in a sentence in a sentence in a sentence in a sentence in a sentence in a sentence in a sentence in a sentence in a sentence in a sentence in a sentence in a sentence in a sentence in a sentence in a sentence in a sentence in a sentence in a sentence in a sentence in a sentence in a sentence in a sentence in a sentence in a sentence in a sentence in a sentence in a sentence in a sentence in a sentence in a sentence in a sentence in a sentence in a sentence in a sentence in a sentence in a sentence in a sentence in a sentence in a sentence in a sentence in a sentence in a sentence in a sentence in a sentence in a sentence in a sentence in a sentence in a sentence in a sentence in a sentence in a sentence in a sentence in a sentence in a sentence in a sentence in a sentence in a sentence in a sentence in a sentence in a sentence in a sentence in a sentence in a sentence in a sentence in a sentence in a sentence in a sentence in a sentence in a sentence in a sentence in a sentence in a sentence in a sentence in a sentence in a sentence in a sentence in a sentence in a sentence in a sentence in a", "histories": [["v1", "Thu, 7 Apr 2016 15:29:45 GMT  (54kb,D)", "http://arxiv.org/abs/1604.02038v1", null], ["v2", "Fri, 8 Apr 2016 05:45:44 GMT  (54kb,D)", "http://arxiv.org/abs/1604.02038v2", "The submitted version was done in Feb.2016. Still in improvement"]], "reviews": [], "SUBJECTS": "cs.LG cs.CL cs.IR", "authors": ["fei tian", "bin gao", "di he", "tie-yan liu"], "accepted": false, "id": "1604.02038"}, "pdf": {"name": "1604.02038.pdf", "metadata": {"source": "CRF", "title": "Sentence Level Recurrent Topic Model: Letting Topics Speak for Themselves", "authors": ["Fei Tian", "Bin Gao", "Tie-Yan Liu"], "emails": ["tianfei@mail.ustc.edu.cn", "bingao@microsoft.com", "tyliu@microsoft.com"], "sections": [{"heading": "1 Introduction", "text": "Statistic topic models such as Latent Dirichlet Allocation (LDA) and its variants [Landauer et al., 1998; Hofmann, 1999; Blei et al., 2003; Mcauliffe and Blei, 2008; Hoffman et al., 2010] have been proven to be effective in modeling textual documents. In these models, a word token in a document is assumed to be generated by a hidden mixture model, where the hidden variables are the topic indexes for each word and the topic assignments for words are related to document level topic weights. Due to the effectiveness and efficiency in modeling the document generation process, topic models are widely adopted in quite a lot of real world tasks such as sentiment classification [Mei et al., 2007], social network analysis [Ramage et al., 2010; Mei et al., 2007], and recommendation systems [Godin et al., 2013].\nMost topic models take the bag-of-words assumption, in which every document is treated as an unordered set of words and the word tokens in such a document are sampled independently with each other. The bag-of-words assumption brings computational convenience, however, it sacrifices the characterization of sequential properties of words in a document \u2217This work was done when the two authors were visiting Microsoft Research Asia.\nand the topic coherence between words belonging to the same language segment (e.g., sentence). As a result, people have observed many negative examples. Just list one for illustration [Wallach, 2006]: the department chair couches offers and the chair department offers couches have very different topics, although they have exactly the same bag of words.\nThere have been some works trying to solve the aforementioned problems, although still insufficiently. For example, several sentence level topic models [Gruber et al., 2007; Du et al., 2010; Wang et al., 2011] tackle the topic coherence problem by assuming all the words in a sentence to share the same topic (i.e., every sentence has only one topic). In addition, they model the sequential information by assuming the transition between sentence topics to be Markovian. However, words within the same sentence are still exchangeable in these models, and thus the bag-of-words assumption still holds within a sentence. For another example, in [Yang et al., 2015], the embedding based neural language model [Bengio et al., 2003; Mikolov et al., 2013; Le and Mikolov, 2014] and topic model are integrated. They assume the generation of a given word in a sentence to depend on its local context (including its preceding words within a fixed window) as well as the topics of the sentence and document it lies in. However, using a fixed window of preceding words, instead of the whole word stream within a sentence, could only introduce limited sequential dependency. Furthermore, there is no explicit coherence constraints on the word topics and sentence topics, since every word can have its own topics in their model.\nWe propose Sentence Level Recurrent Topic Model (SLRTM) to tackle the limitations of the aforementioned works. In the new model, we assume the words in the same sentence to share the same topic in order to guarantee topic coherence, and we assume the generation of a word to rely on the whole history in the same sentence in order to fully characterize the sequential dependency. Specifically, for a particular word w within a sentence s, we assume its generation depends on two factors: the first is the whole set of its historical words in the sentence and the second is the sentence topic, which we regard as a pseudo word and has its own distributed representations. We use Recurrent Neural Network (RNN) [Mikolov et al., 2010], such as Long Short Term Memory (LSTM) [Hochreiter and Schmidhuber, 1997] or Gated Recurrent Unit (GRU) network [Cho et al., 2014],\nar X\niv :1\n60 4.\n02 03\n8v 1\n[ cs\n.L G\n] 7\nA pr\n2 01\n6\nto model such a long term dependency. With the proposed SLRTM, we can not only model the document generation process more accurately, but also construct new natural sentences that are coherent with a given topic (we call it topic2sentence, similar to image2sentece[Vinyals et al., 2015]). Topic2sentence has its huge potential for many real world tasks. For example, it can serve as the basis of personalized short text conversation system [Shang et al., 2015; Serban et al., 2015], in which once we detect that the user is interested in certain topics, we can let these topics speak for themselves using SLRTM to improve the user satisfactory.\nWe have conducted experiments to compare SLRTM with several strong topic model baselines on two tasks: generative model evaluation (i.e. test set perplexity) and document classification. The results on several benchmark datasets quantitatively demonstrate SLRTM\u2019s advantages in modeling documents. We further provide some qualitative results on topic2sentence, the generated sentences for different topics clearly demonstrate the power of SLRTM in topic-sensitive short text conversations."}, {"heading": "2 Related Work", "text": "One of the most representative topic models is Latent Dirichlet Allocation [Blei et al., 2003], in which every word in a document has its topic drawn from document level topic weights. Several variants of LDA have been developed such as hierarchical topic models [Blei et al., 2004] and supervised topic models [Mcauliffe and Blei, 2008]. With the recent development of deep learning, there are also neural network based topic models such as [Hinton and Salakhutdinov, 2009; Larochelle and Lauly, 2012; Cao et al., 2015; Das et al., 2015], which use distributed representations of words to improve topic semantics.\nMost of the aforementioned works take the bag-of-words assumption, which might be too simple according to our discussions in the introduction. That is, it ignores both sequential dependency of words and topic coherence of words.\nThere are some efforts trying to address the limitations of the bag-of-words assumption. For example, in [Griffiths et al., 2004], both semantic (i.e., related with topics) and syntactic properties of words were modeled. After that, a hidden Markov transition model for topics was proposed [Gruber et al., 2007], in which all the words in a sentence were regarded as having the same topic. Such a one sentence, one topic assumption was also used by some other works, including [Du et al., 2010; Wang et al., 2011]. Although these works have made some meaningful attempts on topic coherence and sequential dependency across sentences, they have not sufficiently model the sequential dependency of words within a sentence. To address this problem, the authors of [Yang et al., 2015] adopted the neural language model technology [Bengio et al., 2003] to enhance topic model. In particular, they assume that every document, sentence, and word have their own topics and the topical information is conveyed by their embedding vectors through a Gaussian Mixture Model (GMM) as a prior. In the GMM distribution, each topic corresponds to a mixture parameterized by the mean vector and covariance matrix of the Gaussian distribution. The embed-\nding vectors sampled from the GMM are further used to generate words in a sentence according to a feedforward neural network. To be specific, the preceding words in a fixed sized window, together with the sentence and document, act as the context to generate the next word by a softmax conditional distribution, in which the context is represented by embedding vectors. While this work has explicitly modeled the sequential dependency of words, it ignores the topic coherence among adjacent words.\nAnother line of research related to our model is Recurrent Neural Network (RNN), especially some recently developed effective RNN models such as Long Short Term Memory [Hochreiter and Schmidhuber, 1997] and Gated Recurrent Unit [Cho et al., 2014]. These new RNN models characterize long range dependencies for a sequence, and has been widely adopted in sequence modeling tasks such as machine translation [Cho et al., 2014] and short text conversation [Shang et al., 2015]. In particular, for language modeling tasks, it has been shown that RNN (and its variants such as LSTM) is much more effective than simple feedforward neural networks with fixed window size [Mikolov et al., 2010] given that it can model dependencies with nearly arbitrary length."}, {"heading": "3 Sentence Level Recurrent Topic Model", "text": "In this section, we describe the proposed Sentence Level Recurrent Topic Model (SLRTM). First of all, we list three important design factors in SLRTM as below.\n\u2022 SLRTM takes the one sentence, one topic assumption as in [Gruber et al., 2007; Du et al., 2010; Wang et al., 2011]: all words within the same sentence share the same topic. This assumption guarantees the topic coherence within a sentence, and makes topic2sentence possible.\n\u2022 To model long range dependencies between words, SLRTM uses RNN (specifically LSTM) with word embedding vectors as input. The purpose is to leverage word embeddings to enhance the semantics of words, as indicated by the previous neural network based topic models [Hinton and Salakhutdinov, 2009; Larochelle and Lauly, 2012; Cao et al., 2015; Das et al., 2015].\n\u2022 Each topic in SLRTM has its own distributed representation, which is fine-tuned through the training process and is used to generate the whole sentence. Topic representation vector plays a similar role to the source sentence representation in LSTM based machine translation [Cho et al., 2014] and the image vector output by Convolutional Neural Network in image captioning [Vinyals et al., 2015].\nWith the three points in mind, let us introduce the detailed generative process of SLRTM, as well as the stochastic variational inference and learning algorithm for SLRTM in the following subsections."}, {"heading": "3.1 The generative process", "text": "Suppose we have K topics, |W| words contained in dictionary W , and M documents D = {d1, \u00b7 \u00b7 \u00b7 , dM}. For any\ndocument di, i \u2208 {1, 2, \u00b7 \u00b7 \u00b7 ,M}, it is composed of Ni sentences and its jth sentence sij consists of Tij words. Similar to LDA, we assume there is a K-dimensional Dirichlet prior distribution Dir(\u03b1) for topic mixture weights of each document. With these notations, the generative process for document di can be written as below:\n1. Sample the multinomial parameter \u03b8i from Dir(\u03b1);\n2. For the jth sentence of document di sij = (y1, \u00b7 \u00b7 \u00b7 , yTij ), j \u2208 {1, \u00b7 \u00b7 \u00b7 , Ni}, where yt \u2208 W is the tth word for sij :\n(a) Draw the topic index kij of this sentence from \u03b8i; (b) For t = 1, \u00b7 \u00b7 \u00b7 , Tij :\ni. Compute LSTM hidden state ht = f(ht\u22121;yt\u22121;kij);\nii. \u2200w \u2208 W , draw yt from\nP (w|yt\u22121, \u00b7 \u00b7 \u00b7 , y1; kij) \u221d g(w\u2032;ht;yt\u22121;kij) (1)\nHere we use bold characters to denote the distributed representations for the corresponding items. For example, yt and kij denote the embeddings for word yt and topic kij , respectively. h0 is a zero vector and y0 is a fake starting word. Function f is the LSTM unit to generate hidden states, for which we omit the details due to space restrictions. Function g typically takes the following form:\ng(w\u2032;ht;yt\u22121;kij) = \u03c3(w \u2032 \u00b7 (W1ht +W2yt\u22121 +W3kij + b)), (2) where \u03c3(x) = 1/(1 + exp(\u2212x)), w\u2032 denotes the output embedding for word w. W1,W2,W3 are feedforward weight matrices and b is the bias vector.\nThen the probability of observing document di can be written as: P (di|\u03b1,\u0398) = \u222b \u03b8\u223cDir(\u03b1) Ni\u220f j=1 K\u2211 k=1 \u03b8ikP (sij |k,\u0398)d\u03b8\n= \u222b \u03b8\u223cDir(\u03b1) Ni\u220f j=1 K\u2211 k=1 \u03b8ik Tij\u220f t=1 P (yt|yt\u22121, \u00b7 \u00b7 \u00b7 , y1; k)d\u03b8\n(3) where P (sij |k,\u0398) is the probability of generating sentence sij under topic k, and it is decomposed through the probability chain rule; P (yt|yt\u22121, \u00b7 \u00b7 \u00b7 , y1; k) is specified in equation (1) and (2); \u0398 represents all the model parameters, including the distributed representations for all the words and topics, as well as the weight parameters for LSTM.\nTo sum up, we use Figure 1 to illustrate the generative process of SLRTM, from which we can see that in SLRTM, the historical words and topic of the sentence jointly affect the LSTM hidden state and the next word."}, {"heading": "3.2 Stochastic Variational Inference and Learning", "text": "As the computation of the true posterior of hidden variables in equation (3) is untractable, we adopt mean field variational inference to approximate it. Particularly, we use multinomial distribution q\u03c6ij (kij) and Dirichlet distribution q\u03b3i(\u03b8i) as the variational distribution for the hidden variables kij and \u03b8i, and we denote the variational parameters for document di as \u03a6i = {\u03c6j , \u03b3},\u2200i, j, with the subscript i omitted. Then the variational lower bound of the data likelihood [Blei et al., 2003] can be written as:\nL(D; \u03a6,\u0398, \u03b1) = M\u2211 i=1 Ni\u2211 j=1 {Eq[logP (sij |kij ,\u0398)] + Eq[log p(kij |\u03b8i) q(kij) ]}\n+ M\u2211 i=1 Eq[log p(\u03b8i)\u2212 log q(\u03b8i)]\n(4) where p(\u00b7) is the true distribution for corresponding variables.\nThe introduction of LSTM-RNN makes the optimization of (4) computationally expensive, since we need to update both the model parameters \u0398 = {\u03981, \u00b7 \u00b7 \u00b7 ,\u0398D} and variational parameters \u03a6 after scanning the whole corpus. Considering that mini-batch (containing several sentences) inference and training are necessary to optimize the neural network, we leverage the stochastic variational inference algorithm developed in [Hoffman et al., 2010; Hoffman et al., 2013] to conduct inference and learning in a variational ExpectationMaximization framework.1 The detailed algorithm is given in Algorithm 1. The execution of the whole inference and learning process includes several epochs of iteration over all documents di, i = {1, 2, \u00b7 \u00b7 \u00b7 ,M} with Algorithm 1 (starting with t = 0).\nIn Algorithm 1, \u03a8(x) is the digamma function. Equation (5) guarantees the estimate of \u03b3k is unbiased. In equation (6), \u03c1t is set as \u03c1t = (\u03c40 + t)\u2212\u03ba, where \u03c40 \u2265 0, \u03ba \u2208 (0.5, 1], to make sure \u03b3(t) will converge [Hoffman et al., 2010]. Due to space limit, we omit the derivation details for the updating equations in Algorithm 1, as well as the forward/backward pass details for LSTM [Hochreiter and Schmidhuber, 1997].\n1We did not use any recently developed algorithms for inference and learning under deep neural networks such as variational autoencoder [Kingma and Welling, 2013] because they are designed for continuous hidden states while our model includes discrete variables.\nAlgorithm 1 Stochastic Variational EM for SLRTM\nInput: document di, variation parameters \u03a6 (t) i , and model weights \u0398(t). for every sentence minibatch S = (s1, \u00b7 \u00b7 \u00b7 , sL) in di do\nt = t+ 1 E-Step: repeat\nfor l\u2190 1, L do \u2200k \u2208 {1, \u00b7 \u00b7 \u00b7 ,K}, i.e., every topic index: Obtain \u03b2lk = logP (sl|k,\u0398(t\u22121)) by LSTM\nforward pass. \u03c6\n(t) lk \u221d \u03a8(\u03b3 (t\u22121) k ) + \u03b2lk\n\u03b3\u0303k = \u03b1+ \u2211M m=1Nm L \u2211 l \u03c6 (t) lk (5)\n\u03b3 (t) k = (1\u2212 \u03c1t)\u03b3 (t\u22121) k + (1\u2212 \u03c1t)\u03b3\u0303k (6)\nend for until convergence Collect variational parameters \u03a6(t)i =\n{\u03c6(t)lk , \u03b3(t)},\u2200l, k. M-Step: Compute the gradient grad(t) = \u2202L(S;\u03a6\n(t),\u0398(t\u22121),\u03b1) \u2202\u0398(t\u22121) =\u2211L l=1 \u2211K k=1 \u03c6 (t) lk \u2202 logP (sl|k,\u0398(t\u22121)) \u2202\u0398(t\u22121)\nby LSTM backward pass.\nUse grad(t) to obtain \u0398(t) by stochastic gradient descent methods such as Adagrad [Duchi et al., 2011]. end for"}, {"heading": "4 Experiments", "text": "We report our experimental results in this section. Our experiments include two parts: (1) quantitative experiments, including a generative document evaluation task and a document classification task, on two datasets; (2) qualitative inspection, including the examination of the sentences generated under each topic, in order to test whether SLRTM performs well in the topic2sentence task."}, {"heading": "4.1 Quantitative Results", "text": "We compare SLRTM with several state-of-the-art topic models on two tasks: generative document evaluation and document classification. The former task is to investigate the generation capability of the models, while the latter is to show the representation ability of the models.\nWe base our experiments on two benchmark datasets: \u2022 20Newsgroup, which contains 18,845 emails catego-\nrized into 20 different topical groups such as religion, politics, and sports. The dataset is originally partitioned into 11,314 training documents and 7,531 test documents2. \u2022 Wiki10+ [Zubiaga, 2012]3, which contains Web docu2http://qwone.com/\u02dcjason/20Newsgroups/\n20news-bydate.tar.gz 3http://www.zubiaga.org/datasets/Wiki10+/\nments from Wikipedia, each of which is associated with several tags such as philosophy, software, and music. Following [Cao et al., 2015], we kept the most frequent 25 tags and removed those documents without any of these tags, forming a training set and a test set with 11,164 and 6,161 documents, respectively. The social tags associated with each document are regarded as supervised labels in classification. Wiki10+ contains much more words per document (i.e., 1,704) than 20Newsgroup (i.e., 135).\nWe followed the practice in many previous works and removed infrequent words. After that, the dictionary contains about 32k unique words for 20Newsgroup and 41k for Wiki10+. We adopted the NLTK sentence tokenizer4 to split the datasets into sentences if sentence boundaries are needed.\nThe following baselines were used in our experiments:\n\u2022 LDA [Blei et al., 2003]. LDA is the classic topic model, and we used GibbsLDA++5 for its implementation.\n\u2022 Doc-NADE [Larochelle and Lauly, 2012]. Doc-NADE is a representative neural network based topic model. We used the open-source code provided by the authors.\n\u2022 HTMM [Gruber et al., 2007]. HTMM models consider the sentence level Markov transitions. Similar to DocNADE, the implementation was provided by the authors.\n\u2022 GMNTM [Yang et al., 2015]. GMNTM considers models the order of words within a sentence by a feedforward neural network. We implemented GMNTM according the descriptions in their papers by our own."}, {"heading": "Experimental Setting", "text": "For SLRTM, we implemented it in C++ using Eigen6 and Intel MKL. For the sake of fairness, similar to [Yang et al., 2015], we set the word embedding size, topic embedding size, and LSTM hidden layer size to be 128, 128, and 600 respectively. In the experiment, we tested the performances of SLRTM and the baselines with respect to different number of topics K, i.e., K = 128, 256. In initialization (values of \u0398(0) and \u03a6(0)), the LSTM weight matrices were initialized as orthogonal matrices, the word/topic embeddings were randomly sampled from the uniform distribution (\u22120.015, 0.015) and are fined-tuned through the training process, \u03b3(0) and \u03b1 were both set to 0.5. The mini-batch size in Algorithm 1 was set as L = 5, and we ran the E-Step of the algorithm for only one iteration for efficiently consideration, which leads to the final convergence after about 6 epochs for both datasets. Gradient clipping with a clip value of 20 was used during the optimization of LSTM weights. Asynchronous stochastic gradient descent [Dean et al., 2012] with Adagrad was used to perform multi-thread parallel training."}, {"heading": "Generative Document Evaluation", "text": "We measure the performances of different topic models according to the perplexity per word on the test set, defined as\n4http://www.nltk.org/api/nltk.tokenize.html 5http://gibbslda.sourceforge.net/ 6http://eigen.tuxfamily.org/\nperp(D) = exp{\u2212 \u2211M\ni=1 logP (di)\u2211M i=1N \u2032 i\n}, where N \u2032i is the number of words in document di. The experimental results are summarized in Table 1. Based on the table, we have the following discussions:\n\u2022 Our proposed SLRTM consistently outperforms the baseline models by significant margins, showing its outstanding ability in modelling the generative process of documents. In fact, as tested in our further verifications, the perplexity of SLRTM is close to that of standard LSTM language model, with a small gap of about 100 (higher perplexity) on both datasets which we conjecture is due to the margin between the lower bound in equation (4) and true data likelihood for SLRTM.\n\u2022 Models that consider sequential property within sentences (i.e., GMNTM and SLRTM) are generally better than other models, which verifies the importance of words\u2019 sequential information. Furthermore, LSTMRNN is much better in modelling such a sequential dependency than standard feed-forward networks with fixed words window as input, as verified by the lower perplexity of SLRTM compared with GMNTM."}, {"heading": "Document Classification", "text": "In this experiment, we fed the document vectors (e.g., the \u03b3 values in SLRTM) learnt by different topic models to supervised classifiers, to compare their representation power. For 20Newsgroup, we used the multi-class logistic regression classifier and used accuracy as the evaluation criterion. For Wiki10+, since multiple labels (tags) might be associated with each document, we used logistic regression for each label and the classification result is measured by Micro-F1 score [Lewis et al., 2004]. For both datasets, we use 10% of the original training set for validation, and the remaining for training.\nAll the classification results are shown in Table 2. From the table, we can see that SLRTM is the best model under each setting on both datasets. We can further find that the embedding based methods (Doc-NADE, GMNTM and SLRTM) generate better document representations than other models, demonstrating the representative power of neural networks based on distributed representations. In addition, when the training data is larger (i.e., with more sentences per document as Wiki10+), GMNTM generates worse topical information than Doc-NADE while our SLRTM outperforms Doc-NADE, showing that with sufficient data, SLRTM is more effective in topic modeling since topic coherence is further constrained for each sentence."}, {"heading": "4.2 Qualitative Results", "text": "In this subsection, we demonstrate the capability of SLRTM in generating reasonable and understandable sentences given particular topics. In the experiment, we trained a larger SLRTM with 128 topics on a randomly sampled 100k Wikipedia documents in the year of 20107 with average 275 words per document. The dictionary is composed of roughly 50k most frequent words including common punctuation marks, with uppercase letters transformed into lowercases. The size of word embedding, topic embedding and RNN hidden layer are set to 512, 1024 and 1024, respectively.\nWe used two different mechanisms in sentence generating. The first mechanism is random sampling new word yt at every time step t from the probability distribution defined in equation (3). The second is dynamic programming based beam search [Vinyals et al., 2015], which seeks to generate sentences by globally maximized likelihood. We set the beam size as 30. The generating process terminates until a predefined maximum sentence length is reached (set as 25) or an EOS token is met. Such an EOS is also appended after every training sentence.\nThe generating results are shown in Table 3. In the table, the sentences generated by random sampling and beam search are shown in the second and the third columns respectively. In the fourth column, we show the most representative words for each topics generated by SLRTM. For this purpose, we constrained the maximum sentence length to 1 in beam search, and removed stop words that are frequently used to start a sentence such as the, he, and there.\nFrom the table we have the following observations:\n\u2022 Most of the sentences generated by both mechanisms are natural and semantically correlated with particular topics that are summarized in the first column of the table.\n\u2022 The random sampling mechanism usually produces diverse sentences, whereas some grammar errors may happen (e.g., the last sampled sentence for Topic 4; reranking the randomly sampled words by a standalone language model might further improve the correctness of the sentence). In contrast, sentences outputted by beam search are safer in matching grammar rules, but are not diverse enough. This is consistent with the observations in [Serban et al., 2015].\n7http://www.psych.ualberta.ca/\u02dcwestburylab/ downloads/westburylab.wikicorp.download.html\n\u2022 In addition to topic2sentece, SLRTM maintains the capability of generating words for topics (shown in the last column of the table), similar to conventional topic models."}, {"heading": "Conclusion", "text": "In this paper, we proposed a novel topic model called Sentence Level Recurrent Topic Model (SLRTM), which models the sequential dependency of words and topic coherence within a sentence using Recurrent Neural Networks, and shows superior performance in both predictive document modeling and document classification. In addition, it makes topic2sentence possible, which can benefit many real world tasks such as personalized short text conversation (STC).\nIn the future, we plan to integrate SLRTM into RNN-based STC systems [Shang et al., 2015] to make the dialogue more topic sensitive. We would also like to conduct large scale SLRTM training on bigger corpus with more topics by specially designed scalable algorithms and computational platforms."}], "references": [{"title": "The Journal of Machine Learning Research", "author": ["Yoshua Bengio", "R\u00e9jean Ducharme", "Pascal Vincent", "Christian Janvin. A neural probabilistic language model"], "venue": "3:1137\u20131155,", "citeRegEx": "Bengio et al.. 2003", "shortCiteRegEx": null, "year": 2003}, {"title": "the Journal of machine Learning research", "author": ["David M Blei", "Andrew Y Ng", "Michael I Jordan. Latent dirichlet allocation"], "venue": "3:993\u20131022,", "citeRegEx": "Blei et al.. 2003", "shortCiteRegEx": null, "year": 2003}, {"title": "Hierarchical topic models and the nested chinese restaurant process", "author": ["David M Blei", "Thomas L Griffiths", "Michael I Jordan", "Joshua B Tenenbaum"], "venue": "Advances in Neural Information Processing Systems,", "citeRegEx": "Blei et al.. 2004", "shortCiteRegEx": null, "year": 2004}, {"title": "A novel neural topic model and its supervised extension", "author": ["Ziqiang Cao", "Sujian Li", "Yang Liu", "Wenjie Li", "Heng Ji"], "venue": "Twenty-Ninth AAAI Conference on Artificial Intelligence,", "citeRegEx": "Cao et al.. 2015", "shortCiteRegEx": null, "year": 2015}, {"title": "Learning phrase representations using rnn encoder\u2013decoder for statistical machine translation", "author": ["Cho et al", "2014] Kyunghyun Cho", "Bart V Merrienboer", "Caglar Gulcehre", "Dzmitry Bahdanau", "Fethi Bougares", "Holger Schwenk", "Yoshua Bengio"], "venue": "In Proceedings of the Conference", "citeRegEx": "al. et al\\.,? \\Q2014\\E", "shortCiteRegEx": "al. et al\\.", "year": 2014}, {"title": "In Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics", "author": ["Rajarshi Das", "Manzil Zaheer", "Chris Dyer. Gaussian lda for topic models with word embeddings"], "venue": "pages 795\u2013 804, July", "citeRegEx": "Das et al.. 2015", "shortCiteRegEx": null, "year": 2015}, {"title": "et al", "author": ["Jeffrey Dean", "Greg Corrado", "Rajat Monga", "Kai Chen", "Matthieu Devin", "Mark Mao", "Andrew Senior", "Paul Tucker", "Ke Yang", "Quoc V Le"], "venue": "Large scale distributed deep networks. In Advances in Neural Information Processing Systems, pages 1223\u20131231,", "citeRegEx": "Dean et al.. 2012", "shortCiteRegEx": null, "year": 2012}, {"title": "Sequential latent dirichlet allocation: Discover underlying topic structures within a document", "author": ["Du et al", "2010] Lan Du", "Wray Buntine", "Huidong Jin"], "venue": null, "citeRegEx": "al. et al\\.,? \\Q2010\\E", "shortCiteRegEx": "al. et al\\.", "year": 2010}, {"title": "The Journal of Machine Learning Research", "author": ["John Duchi", "Elad Hazan", "Yoram Singer. Adaptive subgradient methods for online learning", "stochastic optimization"], "venue": "12:2121\u20132159,", "citeRegEx": "Duchi et al.. 2011", "shortCiteRegEx": null, "year": 2011}, {"title": "In Proceedings of the 22nd international conference on World Wide Web", "author": ["Fr\u00e9deric Godin", "Viktor Slavkovikj", "Wesley De Neve", "Benjamin Schrauwen", "Rik Van de Walle. Using topic models for twitter hashtag recommendation"], "venue": "pages 593\u2013596,", "citeRegEx": "Godin et al.. 2013", "shortCiteRegEx": null, "year": 2013}, {"title": "In Advances in neural information processing systems", "author": ["Thomas L Griffiths", "Mark Steyvers", "David M Blei", "Joshua B Tenenbaum. Integrating topics", "syntax"], "venue": "pages 537\u2013544,", "citeRegEx": "Griffiths et al.. 2004", "shortCiteRegEx": null, "year": 2004}, {"title": "In International Conference on Artificial Intelligence and Statistics", "author": ["Amit Gruber", "Yair Weiss", "Michal Rosen-Zvi. Hidden topic markov models"], "venue": "pages 163\u2013170,", "citeRegEx": "Gruber et al.. 2007", "shortCiteRegEx": null, "year": 2007}, {"title": "Replicated softmax: an undirected topic model", "author": ["Geoffrey E Hinton", "Ruslan R Salakhutdinov"], "venue": "Advances in neural information processing systems, pages 1607\u20131614,", "citeRegEx": "Hinton and Salakhutdinov. 2009", "shortCiteRegEx": null, "year": 2009}, {"title": "Neural computation", "author": ["Sepp Hochreiter", "J\u00fcrgen Schmidhuber. Long short-term memory"], "venue": "9(8):1735\u20131780,", "citeRegEx": "Hochreiter and Schmidhuber. 1997", "shortCiteRegEx": null, "year": 1997}, {"title": "In advances in neural information processing systems", "author": ["Matthew Hoffman", "Francis R Bach", "David M Blei. Online learning for latent dirichlet allocation"], "venue": "pages 856\u2013864,", "citeRegEx": "Hoffman et al.. 2010", "shortCiteRegEx": null, "year": 2010}, {"title": "The Journal of Machine Learning Research", "author": ["Matthew D Hoffman", "David M Blei", "Chong Wang", "John Paisley. Stochastic variational inference"], "venue": "14(1):1303\u20131347,", "citeRegEx": "Hoffman et al.. 2013", "shortCiteRegEx": null, "year": 2013}, {"title": "In Proceedings of the 22nd annual international ACM SIGIR conference on Research and development in information retrieval", "author": ["Thomas Hofmann. Probabilistic latent semantic indexing"], "venue": "pages 50\u201357. ACM,", "citeRegEx": "Hofmann. 1999", "shortCiteRegEx": null, "year": 1999}, {"title": "Auto-encoding variational bayes", "author": ["Diederik P Kingma", "Max Welling"], "venue": "arXiv preprint arXiv:1312.6114,", "citeRegEx": "Kingma and Welling. 2013", "shortCiteRegEx": null, "year": 2013}, {"title": "Discourse processes", "author": ["Thomas K Landauer", "Peter W Foltz", "Darrell Laham. An introduction to latent semantic analysis"], "venue": "25(2-3):259\u2013284,", "citeRegEx": "Landauer et al.. 1998", "shortCiteRegEx": null, "year": 1998}, {"title": "In Advances in Neural Information Processing Systems", "author": ["Hugo Larochelle", "Stanislas Lauly. A neural autoregressive topic model"], "venue": "pages 2708\u2013 2716,", "citeRegEx": "Larochelle and Lauly. 2012", "shortCiteRegEx": null, "year": 2012}, {"title": "In Proceedings of the 31st International Conference on Machine Learning (ICML-14)", "author": ["Quoc Le", "Tomas Mikolov. Distributed representations of sentences", "documents"], "venue": "pages 1188\u20131196,", "citeRegEx": "Le and Mikolov. 2014", "shortCiteRegEx": null, "year": 2014}, {"title": "Rcv1: A new benchmark collection", "author": ["Lewis et al", "2004] David D Lewis", "Yiming Yang", "Tony G Rose", "Fan Li"], "venue": null, "citeRegEx": "al. et al\\.,? \\Q2004\\E", "shortCiteRegEx": "al. et al\\.", "year": 2004}, {"title": "In Advances in neural information processing systems", "author": ["Jon D Mcauliffe", "David M Blei. Supervised topic models"], "venue": "pages 121\u2013128,", "citeRegEx": "Mcauliffe and Blei. 2008", "shortCiteRegEx": null, "year": 2008}, {"title": "Topic sentiment mixture: modeling facets and opinions in weblogs", "author": ["Qiaozhu Mei", "Xu Ling", "Matthew Wondra", "Hang Su", "ChengXiang Zhai"], "venue": "Proceedings of the 16th international conference on World Wide Web, pages 171\u2013180. ACM,", "citeRegEx": "Mei et al.. 2007", "shortCiteRegEx": null, "year": 2007}, {"title": "In 11th Annual Conference of the International Speech Communication Association", "author": ["Tomas Mikolov", "Martin Karafi\u00e1t", "Lukas Burget", "Jan Cernock\u1ef3", "Sanjeev Khudanpur. Recurrent neural network based language model"], "venue": "2010, pages 1045\u20131048,", "citeRegEx": "Mikolov et al.. 2010", "shortCiteRegEx": null, "year": 2010}, {"title": "Efficient estimation of word representations in vector space", "author": ["Tomas Mikolov", "Kai Chen", "Greg Corrado", "Jeffrey Dean"], "venue": "arXiv preprint arXiv:1301.3781,", "citeRegEx": "Mikolov et al.. 2013", "shortCiteRegEx": null, "year": 2013}, {"title": "ICWSM 2010", "author": ["Daniel Ramage", "Susan Dumais", "Dan Liebling. Characterizing microblogs with topic models. In Proc"], "venue": "American Association for Artificial Intelligence, May", "citeRegEx": "Ramage et al.. 2010", "shortCiteRegEx": null, "year": 2010}, {"title": "Building end-to-end dialogue systems using generative hierarchical neural network models", "author": ["Iulian V Serban", "Alessandro Sordoni", "Yoshua Bengio", "Aaron Courville", "Joelle Pineau"], "venue": "arXiv preprint arXiv:1507.04808,", "citeRegEx": "Serban et al.. 2015", "shortCiteRegEx": null, "year": 2015}, {"title": "In Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics", "author": ["Lifeng Shang", "Zhengdong Lu", "Hang Li. Neural responding machine for short-text conversation"], "venue": "pages 1577\u20131586, July", "citeRegEx": "Shang et al.. 2015", "shortCiteRegEx": null, "year": 2015}, {"title": "Show and tell: A neural image caption generator", "author": ["Oriol Vinyals", "Alexander Toshev", "Samy Bengio", "Dumitru Erhan"], "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 3156\u20133164,", "citeRegEx": "Vinyals et al.. 2015", "shortCiteRegEx": null, "year": 2015}, {"title": "Topic modeling: beyond bag-of-words", "author": ["Hanna M Wallach"], "venue": "Proceedings of the 23rd international conference on Machine learning, pages 977\u2013984. ACM,", "citeRegEx": "Wallach. 2006", "shortCiteRegEx": null, "year": 2006}, {"title": "In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics", "author": ["Hongning Wang", "Duo Zhang", "ChengXiang Zhai. Structural topic model for latent topical structure analysis"], "venue": "pages 1526\u20131535,", "citeRegEx": "Wang et al.. 2011", "shortCiteRegEx": null, "year": 2011}, {"title": "Ordering-sensitive and semantic-aware topic modeling", "author": ["Min Yang", "Tianyi Cui", "Wenting Tu"], "venue": "Twenty-Ninth AAAI Conference on Artificial Intelligence,", "citeRegEx": "Yang et al.. 2015", "shortCiteRegEx": null, "year": 2015}, {"title": "Enhancing navigation on wikipedia with social tags", "author": ["Arkaitz Zubiaga"], "venue": "arXiv preprint arXiv:1202.5469,", "citeRegEx": "Zubiaga. 2012", "shortCiteRegEx": null, "year": 2012}], "referenceMentions": [{"referenceID": 18, "context": "Statistic topic models such as Latent Dirichlet Allocation (LDA) and its variants [Landauer et al., 1998; Hofmann, 1999; Blei et al., 2003; Mcauliffe and Blei, 2008; Hoffman et al., 2010] have been proven to be effective in modeling textual documents.", "startOffset": 82, "endOffset": 187}, {"referenceID": 16, "context": "Statistic topic models such as Latent Dirichlet Allocation (LDA) and its variants [Landauer et al., 1998; Hofmann, 1999; Blei et al., 2003; Mcauliffe and Blei, 2008; Hoffman et al., 2010] have been proven to be effective in modeling textual documents.", "startOffset": 82, "endOffset": 187}, {"referenceID": 1, "context": "Statistic topic models such as Latent Dirichlet Allocation (LDA) and its variants [Landauer et al., 1998; Hofmann, 1999; Blei et al., 2003; Mcauliffe and Blei, 2008; Hoffman et al., 2010] have been proven to be effective in modeling textual documents.", "startOffset": 82, "endOffset": 187}, {"referenceID": 22, "context": "Statistic topic models such as Latent Dirichlet Allocation (LDA) and its variants [Landauer et al., 1998; Hofmann, 1999; Blei et al., 2003; Mcauliffe and Blei, 2008; Hoffman et al., 2010] have been proven to be effective in modeling textual documents.", "startOffset": 82, "endOffset": 187}, {"referenceID": 14, "context": "Statistic topic models such as Latent Dirichlet Allocation (LDA) and its variants [Landauer et al., 1998; Hofmann, 1999; Blei et al., 2003; Mcauliffe and Blei, 2008; Hoffman et al., 2010] have been proven to be effective in modeling textual documents.", "startOffset": 82, "endOffset": 187}, {"referenceID": 23, "context": "Due to the effectiveness and efficiency in modeling the document generation process, topic models are widely adopted in quite a lot of real world tasks such as sentiment classification [Mei et al., 2007], social network analysis [Ramage et al.", "startOffset": 185, "endOffset": 203}, {"referenceID": 26, "context": ", 2007], social network analysis [Ramage et al., 2010; Mei et al., 2007], and recommendation systems [Godin et al.", "startOffset": 33, "endOffset": 72}, {"referenceID": 23, "context": ", 2007], social network analysis [Ramage et al., 2010; Mei et al., 2007], and recommendation systems [Godin et al.", "startOffset": 33, "endOffset": 72}, {"referenceID": 9, "context": ", 2007], and recommendation systems [Godin et al., 2013].", "startOffset": 36, "endOffset": 56}, {"referenceID": 30, "context": "Just list one for illustration [Wallach, 2006]: the department chair couches offers and the chair department offers couches have very different topics, although they have exactly the same bag of words.", "startOffset": 31, "endOffset": 46}, {"referenceID": 11, "context": "For example, several sentence level topic models [Gruber et al., 2007; Du et al., 2010; Wang et al., 2011] tackle the topic coherence problem by assuming all the words in a sentence to share the same topic (i.", "startOffset": 49, "endOffset": 106}, {"referenceID": 31, "context": "For example, several sentence level topic models [Gruber et al., 2007; Du et al., 2010; Wang et al., 2011] tackle the topic coherence problem by assuming all the words in a sentence to share the same topic (i.", "startOffset": 49, "endOffset": 106}, {"referenceID": 32, "context": "For another example, in [Yang et al., 2015], the embedding based neural language model [Bengio et al.", "startOffset": 24, "endOffset": 43}, {"referenceID": 0, "context": ", 2015], the embedding based neural language model [Bengio et al., 2003; Mikolov et al., 2013; Le and Mikolov, 2014] and topic model are integrated.", "startOffset": 51, "endOffset": 116}, {"referenceID": 25, "context": ", 2015], the embedding based neural language model [Bengio et al., 2003; Mikolov et al., 2013; Le and Mikolov, 2014] and topic model are integrated.", "startOffset": 51, "endOffset": 116}, {"referenceID": 20, "context": ", 2015], the embedding based neural language model [Bengio et al., 2003; Mikolov et al., 2013; Le and Mikolov, 2014] and topic model are integrated.", "startOffset": 51, "endOffset": 116}, {"referenceID": 24, "context": "We use Recurrent Neural Network (RNN) [Mikolov et al., 2010], such as Long Short Term Memory (LSTM) [Hochreiter and Schmidhuber, 1997] or Gated Recurrent Unit (GRU) network [Cho et al.", "startOffset": 38, "endOffset": 60}, {"referenceID": 13, "context": ", 2010], such as Long Short Term Memory (LSTM) [Hochreiter and Schmidhuber, 1997] or Gated Recurrent Unit (GRU) network [Cho et al.", "startOffset": 47, "endOffset": 81}, {"referenceID": 29, "context": "With the proposed SLRTM, we can not only model the document generation process more accurately, but also construct new natural sentences that are coherent with a given topic (we call it topic2sentence, similar to image2sentece[Vinyals et al., 2015]).", "startOffset": 226, "endOffset": 248}, {"referenceID": 28, "context": "For example, it can serve as the basis of personalized short text conversation system [Shang et al., 2015; Serban et al., 2015], in which once we detect that the user is interested in certain topics, we can let these topics speak for themselves using SLRTM to improve the user satisfactory.", "startOffset": 86, "endOffset": 127}, {"referenceID": 27, "context": "For example, it can serve as the basis of personalized short text conversation system [Shang et al., 2015; Serban et al., 2015], in which once we detect that the user is interested in certain topics, we can let these topics speak for themselves using SLRTM to improve the user satisfactory.", "startOffset": 86, "endOffset": 127}, {"referenceID": 1, "context": "One of the most representative topic models is Latent Dirichlet Allocation [Blei et al., 2003], in which every word in a document has its topic drawn from document level topic weights.", "startOffset": 75, "endOffset": 94}, {"referenceID": 2, "context": "Several variants of LDA have been developed such as hierarchical topic models [Blei et al., 2004] and supervised topic models [Mcauliffe and Blei, 2008].", "startOffset": 78, "endOffset": 97}, {"referenceID": 22, "context": ", 2004] and supervised topic models [Mcauliffe and Blei, 2008].", "startOffset": 36, "endOffset": 62}, {"referenceID": 12, "context": "With the recent development of deep learning, there are also neural network based topic models such as [Hinton and Salakhutdinov, 2009; Larochelle and Lauly, 2012; Cao et al., 2015; Das et al., 2015], which use distributed representations of words to improve topic semantics.", "startOffset": 103, "endOffset": 199}, {"referenceID": 19, "context": "With the recent development of deep learning, there are also neural network based topic models such as [Hinton and Salakhutdinov, 2009; Larochelle and Lauly, 2012; Cao et al., 2015; Das et al., 2015], which use distributed representations of words to improve topic semantics.", "startOffset": 103, "endOffset": 199}, {"referenceID": 3, "context": "With the recent development of deep learning, there are also neural network based topic models such as [Hinton and Salakhutdinov, 2009; Larochelle and Lauly, 2012; Cao et al., 2015; Das et al., 2015], which use distributed representations of words to improve topic semantics.", "startOffset": 103, "endOffset": 199}, {"referenceID": 5, "context": "With the recent development of deep learning, there are also neural network based topic models such as [Hinton and Salakhutdinov, 2009; Larochelle and Lauly, 2012; Cao et al., 2015; Das et al., 2015], which use distributed representations of words to improve topic semantics.", "startOffset": 103, "endOffset": 199}, {"referenceID": 10, "context": "For example, in [Griffiths et al., 2004], both semantic (i.", "startOffset": 16, "endOffset": 40}, {"referenceID": 11, "context": "After that, a hidden Markov transition model for topics was proposed [Gruber et al., 2007], in which all the words in a sentence were regarded as having the same topic.", "startOffset": 69, "endOffset": 90}, {"referenceID": 31, "context": "Such a one sentence, one topic assumption was also used by some other works, including [Du et al., 2010; Wang et al., 2011].", "startOffset": 87, "endOffset": 123}, {"referenceID": 32, "context": "To address this problem, the authors of [Yang et al., 2015] adopted the neural language model technology [Bengio et al.", "startOffset": 40, "endOffset": 59}, {"referenceID": 0, "context": ", 2015] adopted the neural language model technology [Bengio et al., 2003] to enhance topic model.", "startOffset": 53, "endOffset": 74}, {"referenceID": 13, "context": "Another line of research related to our model is Recurrent Neural Network (RNN), especially some recently developed effective RNN models such as Long Short Term Memory [Hochreiter and Schmidhuber, 1997] and Gated Recurrent Unit [Cho et al.", "startOffset": 168, "endOffset": 202}, {"referenceID": 28, "context": ", 2014] and short text conversation [Shang et al., 2015].", "startOffset": 36, "endOffset": 56}, {"referenceID": 24, "context": "In particular, for language modeling tasks, it has been shown that RNN (and its variants such as LSTM) is much more effective than simple feedforward neural networks with fixed window size [Mikolov et al., 2010] given that it can model dependencies with nearly arbitrary length.", "startOffset": 189, "endOffset": 211}, {"referenceID": 11, "context": "\u2022 SLRTM takes the one sentence, one topic assumption as in [Gruber et al., 2007; Du et al., 2010; Wang et al., 2011]: all words within the same sentence share the same topic.", "startOffset": 59, "endOffset": 116}, {"referenceID": 31, "context": "\u2022 SLRTM takes the one sentence, one topic assumption as in [Gruber et al., 2007; Du et al., 2010; Wang et al., 2011]: all words within the same sentence share the same topic.", "startOffset": 59, "endOffset": 116}, {"referenceID": 12, "context": "The purpose is to leverage word embeddings to enhance the semantics of words, as indicated by the previous neural network based topic models [Hinton and Salakhutdinov, 2009; Larochelle and Lauly, 2012; Cao et al., 2015; Das et al., 2015].", "startOffset": 141, "endOffset": 237}, {"referenceID": 19, "context": "The purpose is to leverage word embeddings to enhance the semantics of words, as indicated by the previous neural network based topic models [Hinton and Salakhutdinov, 2009; Larochelle and Lauly, 2012; Cao et al., 2015; Das et al., 2015].", "startOffset": 141, "endOffset": 237}, {"referenceID": 3, "context": "The purpose is to leverage word embeddings to enhance the semantics of words, as indicated by the previous neural network based topic models [Hinton and Salakhutdinov, 2009; Larochelle and Lauly, 2012; Cao et al., 2015; Das et al., 2015].", "startOffset": 141, "endOffset": 237}, {"referenceID": 5, "context": "The purpose is to leverage word embeddings to enhance the semantics of words, as indicated by the previous neural network based topic models [Hinton and Salakhutdinov, 2009; Larochelle and Lauly, 2012; Cao et al., 2015; Das et al., 2015].", "startOffset": 141, "endOffset": 237}, {"referenceID": 29, "context": ", 2014] and the image vector output by Convolutional Neural Network in image captioning [Vinyals et al., 2015].", "startOffset": 88, "endOffset": 110}, {"referenceID": 1, "context": "Then the variational lower bound of the data likelihood [Blei et al., 2003] can be written as: Figure 1: The illustration of the SLRTM generative process.", "startOffset": 56, "endOffset": 75}, {"referenceID": 14, "context": "Considering that mini-batch (containing several sentences) inference and training are necessary to optimize the neural network, we leverage the stochastic variational inference algorithm developed in [Hoffman et al., 2010; Hoffman et al., 2013] to conduct inference and learning in a variational ExpectationMaximization framework.", "startOffset": 200, "endOffset": 244}, {"referenceID": 15, "context": "Considering that mini-batch (containing several sentences) inference and training are necessary to optimize the neural network, we leverage the stochastic variational inference algorithm developed in [Hoffman et al., 2010; Hoffman et al., 2013] to conduct inference and learning in a variational ExpectationMaximization framework.", "startOffset": 200, "endOffset": 244}, {"referenceID": 14, "context": "5, 1], to make sure \u03b3 will converge [Hoffman et al., 2010].", "startOffset": 36, "endOffset": 58}, {"referenceID": 13, "context": "Due to space limit, we omit the derivation details for the updating equations in Algorithm 1, as well as the forward/backward pass details for LSTM [Hochreiter and Schmidhuber, 1997].", "startOffset": 148, "endOffset": 182}, {"referenceID": 17, "context": "We did not use any recently developed algorithms for inference and learning under deep neural networks such as variational autoencoder [Kingma and Welling, 2013] because they are designed for continuous hidden states while our model includes discrete variables.", "startOffset": 135, "endOffset": 161}, {"referenceID": 8, "context": "Use grad to obtain \u0398 by stochastic gradient descent methods such as Adagrad [Duchi et al., 2011].", "startOffset": 76, "endOffset": 96}, {"referenceID": 33, "context": "\u2022 Wiki10+ [Zubiaga, 2012]3, which contains Web docu-", "startOffset": 10, "endOffset": 25}, {"referenceID": 3, "context": "Following [Cao et al., 2015], we kept the most frequent 25 tags and removed those documents without any of these tags, forming a training set and a test set with 11,164 and 6,161 documents, respectively.", "startOffset": 10, "endOffset": 28}, {"referenceID": 1, "context": "\u2022 LDA [Blei et al., 2003].", "startOffset": 6, "endOffset": 25}, {"referenceID": 19, "context": "\u2022 Doc-NADE [Larochelle and Lauly, 2012].", "startOffset": 11, "endOffset": 39}, {"referenceID": 11, "context": "\u2022 HTMM [Gruber et al., 2007].", "startOffset": 7, "endOffset": 28}, {"referenceID": 32, "context": "\u2022 GMNTM [Yang et al., 2015].", "startOffset": 8, "endOffset": 27}, {"referenceID": 32, "context": "For the sake of fairness, similar to [Yang et al., 2015], we set the word embedding size, topic embedding size, and LSTM hidden layer size to be 128, 128, and 600 respectively.", "startOffset": 37, "endOffset": 56}, {"referenceID": 6, "context": "Asynchronous stochastic gradient descent [Dean et al., 2012] with Adagrad was used to perform multi-thread parallel training.", "startOffset": 41, "endOffset": 60}, {"referenceID": 29, "context": "The second is dynamic programming based beam search [Vinyals et al., 2015], which seeks to generate sentences by globally maximized likelihood.", "startOffset": 52, "endOffset": 74}, {"referenceID": 27, "context": "This is consistent with the observations in [Serban et al., 2015].", "startOffset": 44, "endOffset": 65}, {"referenceID": 28, "context": "In the future, we plan to integrate SLRTM into RNN-based STC systems [Shang et al., 2015] to make the dialogue more topic sensitive.", "startOffset": 69, "endOffset": 89}], "year": 2017, "abstractText": "We propose Sentence Level Recurrent Topic Model (SLRTM), a new topic model that assumes the generation of each word within a sentence to depend on both the topic of the sentence and the whole history of its preceding words in the sentence. Different from conventional topic models that largely ignore the sequential order of words or their topic coherence, SLRTM gives full characterization to them by using a Recurrent Neural Networks (RNN) based framework. Experimental results have shown that SLRTM outperforms several strong baselines on various tasks. Furthermore, SLRTM can automatically generate sentences given a topic (i.e., topics to sentences), which is a key technology for real world applications such as personalized short text conversation.", "creator": "LaTeX with hyperref package"}}}