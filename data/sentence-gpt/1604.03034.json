{"id": "1604.03034", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "11-Apr-2016", "title": "M3: Scaling Up Machine Learning via Memory Mapping", "abstract": "To process data that do not fit in RAM, conventional wisdom would suggest using distributed approaches. However, recent research has demonstrated virtual memory's strong potential in scaling up graph mining algorithms on a single machine. We propose to use a similar approach for general machine learning (HVM), in which distributed approaches offer both high-performance and low-cost applications. We hypothesize that using distributed approach will provide large, high-performance applications for distributed data mining. A recent study demonstrated the use of distributed methods to address this problem in several of the largest data centers in the world, including the National Science Foundation (NSF). This study demonstrated that distributed approaches could reduce computational complexity by reducing the computational power of distributed computing.\n\n\n\n\n\n\n\nWe hypothesized to have an integrated distributed and distributed learning algorithm, which would be effective across all systems. However, this approach might be too simplistic to implement as well as to support both low- and high-cost applications. One option is to use the distributed approach in a system that requires multiple processors in order to do so. A network-based distributed approach would eliminate the need to use multiple processors to achieve a single-processor throughput. We also proposed that this approach can achieve distributed computing efficiently without creating additional latency. We propose that such a solution is also feasible using distributed and distributed learning approaches.\n\n\nWe propose that the underlying algorithm would be distributed by a single-state network, and to maximize the use of distributed approaches.\n\nThe network is not a single-state network. In addition, it can be distributed as a whole. The system needs to receive input via either single-state networks or multiple-state networks. If a large number of users do not have the right combination of two systems, the system would not become connected. Instead, the network will be divided into clusters, with each user being the one who can participate and then the user who can participate. As a result, the system would be distributed in a single-state network in parallel.\nIn addition, we propose that the network's initial state can be divided into clusters by two different configurations, and in a single-state network. For example, a single state system has the ability to operate over multiple systems (such as a single-state network). This could be achieved by combining network processing on an interface that accepts multiple inputs, such as the user interface, which then can be represented as a single-state network.\nTo address the problem of distributed and distributed learning, we propose to reduce the amount of", "histories": [["v1", "Mon, 11 Apr 2016 17:12:14 GMT  (1793kb,D)", "http://arxiv.org/abs/1604.03034v1", "2 pages, 1 figure, 1 table"]], "COMMENTS": "2 pages, 1 figure, 1 table", "reviews": [], "SUBJECTS": "cs.LG cs.DC", "authors": ["dezhi fang", "duen horng chau"], "accepted": false, "id": "1604.03034"}, "pdf": {"name": "1604.03034.pdf", "metadata": {"source": "CRF", "title": "M3: Scaling Up Machine Learning via Memory Mapping", "authors": ["Dezhi Fang", "Duen Horng Chau"], "emails": ["dezhifang@gatech.edu", "polo@gatech.edu"], "sections": [{"heading": null, "text": "CCS Concepts \u2022Software and its engineering \u2192 Virtual memory; \u2022Computing methodologies \u2192 Machine learning;"}, {"heading": "1. INTRODUCTION", "text": "Leveraging virtual memory to extend algorithms for outof-core data has received increasing attention in data analytics communities. Recent research demonstrated virtual memory\u2019s strong potential to scale up graph algorithms on a single PC [4, 3]. Available on almost all modern platforms, virtual memory based approaches are straight forward to implement and to use, and can handle graphs with as many as 6 billion edges [3]. Some single-thread implementations on a PC can even outperform popular distributed systems like Spark (128 cores) [4]. Memory mapping a dataset into a machine\u2019s virtual memory space allows the dataset to be treated identically as an in-memory dataset. The algorithm developer no longer needs to explicitly determine how to partition the (large) dataset, nor manage which partitions should be loaded into RAM, or unloaded from it. The OS performs similar actions on the developer\u2019s behalf, through\nPermission to make digital or hard copies of part or all of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for third-party components of this work must be honored. For all other uses, contact the owner/author(s). SIGMOD/PODS\u201916 June 26 - July 01, 2016, San Francisco, CA, USA c\u00a9 2016 Copyright held by the owner/author(s).\nACM ISBN 978-1-4503-3531-7/16/06.\nDOI: http://dx.doi.org/10.1145/2882903.2914830\npaging the dataset in and out of RAM, via highly optimized OS-level operations."}, {"heading": "2. SCALING UP USING M3", "text": "As existing works focused on graph algorithms such as PageRank and finding connected components, we are investigating whether a similar virtual memory based approach can generalize to machine learning algorithms at large.\nInspired by prior works on graph computation, our M31\napproach uses memory mapping to amplify a single machine\u2019s capability to process large amounts of data for machine learning algorithms. As memory mapping a dataset allows it to be treated identically as an in-memory dataset, M3 is a transparent scale-up strategy that developers can easily apply, requiring minimal modifications to existing code. For example, Table 1 shows that with only minimal code changes and a trivial helper function, existing algorithm implementation can easily handle much larger, memory-mapped datasets.\nModern 64bit machines have address spaces large enough to fit large datasets into (up to 1024PB). Because the operating system has access to a variety of internal statistics on how the mapped data is being used, the access to such data can be further optimized by the operating system via\n1M3 stands for Machine Learning via Memory Mapping.\nar X\niv :1\n60 4.\n03 03\n4v 1\n[ cs\n.L G\n] 1\n1 A\npr 2\n01 6\nmethods including least recent used caching and read-ahead to achieve efficiency [1].\nTo test the feasibility of M3, we minimally modified mlpack, an efficient machine learning library written in C++ [2]. Memory mapping can be easily applied to other languages and algorithm libraries."}, {"heading": "3. EXPERIMENTS", "text": "Our current evaluation focuses on: (1) understanding of how M3 scales with increasing data sizes; and (2) how M3 compares with distributed systems such as Spark, as prior work suggested the possibility that a single machine can outperform a computer cluster [4].\nExperiment Setup. All tests with M3 are conducted on a desktop computer with Intel i7-4770K quad-core CPU at 3.50GHz (8 hyperthreads), 4\u00d78GB RAM, 1TB SSD of OCZ RevoDrive 350. We used Amazon EC2 m3.2xlarge instances for Spark experiments. Each instance has 8 vCPUs (hyperthreads of an Intel Xeon core) with 30GB memory and 2\u00d780GB SSDs. The Spark clusters are created by Amazon Elastic MapReduce and the datasets are stored on the cluster\u2019s HDFS.\nDataset. We used the Infimnist2 dataset, an infinite supply of digit images (0\u20139) derived from the well-known MNIST dataset using pseudo-random deformations and translations. Each image is 28\u00d728 pixel grayscale image (784 features; each image is 6272 bytes). We generated up to 32M images, whose dense data matrix representation contains 23.5 billion entries, amounting to 190GB. Smaller datasets are subsets of the full 32M images.\nAlgorithms Evaluated. We selected logistic regression (L-BFGS for optimization) and k-means, since they are commonly used classification and clustering algorithms.3"}, {"heading": "3.1 Key Findings & Implications", "text": "1. M3 scales linearly when data fits in RAM and when out-of-core, for logistic regression (Figure 1a). The dotted vertical line in the figure indicates RAM size (32GB). M3\u2019s runtime scales linearly both when the dataset fits in RAM (yellow region in Fig. 1a), and when it exceeds RAM (green dotted line), at a higher scaling constant, as expected.\nLooking at M3\u2019s resource utilization, we saw that M3 is I/O bound: disk I/O was 100% utilized while CPU was only utilized at around 13%. This suggests strong potential for M3 reaching even higher speed if we use faster disks, or configurations such as RAID 0. 2. M3\u2019s speed (one PC) is comparable to 8-instance Spark and significantly faster than 4-instance Spark for logistic regression (L-BFGS) and k-means (Figure 1b). This result echoes prior works focusing on graph computa-\n2http://leon.bottou.org/projects/infimnist 3We are primarily interested in runtimes, so we did not perform image pre-processing.\ntion that suggests cluster may not be necessary for moderatelysized datasets [4, 3]. Our result extends those findings to two general machine learning methods.\nFor logistic regression (with 10 iterations of L-BFGS), M3 is about 30% faster than 8-instance Spark. 4-instance Spark\u2019s runtime was 4.2X that of M3. For k-means (10 iterations, 5 clusters), M3 ran at a speed comparable to 8- instance Spark (1.37X), and more than twice as fast as 4- instance Spark.\nCertainly, using more Spark instances will increase speed, but that may also incur additional overhead (e.g., communication between nodes). Here, we showed that for moderatelysized datasets, single-machine approaches like M3 can be attractive alternatives to distributed approaches."}, {"heading": "4. CONCLUSIONS & ONGOING WORK", "text": "We are taking a first major step in assessing the feasibility of using virtual memory as a fundamental, alternative way to scale up machine learning algorithms. M3 adds an interesting perspective to existing solutions primarily based on distributed systems.\nWe contribute: (1) our latest finding that memory mapping could become a feasible technique for scaling up general machine learning algorithms when the dataset exceeds RAM; (2) M3, an easy-to-apply approach that enables existing machine learning implementations to work with outof-core datasets; (3) our observations that M3 on a PC can achieve a speed that is significantly faster than a 4-instance Spark cluster, and comparable to an 8-instance cluster.\nWe will extend our M3 approach to a wide range of machine learning (including online learning) and data mining algorithms. We plan to extensively study the memory access patterns and locality of algorithms (e.g., sequential scans vs random access) to better understand how they they affect performance. We plan to develop mathematical models and systematic approaches to profile and predict algorithm performance and energy usage based on extensive evaluations across platforms, datasets, and languages."}, {"heading": "5. ACKNOWLEDGMENTS", "text": "We thank Dr. Ryan Curtin and Minsuk (Brian) Kahng for their feedback. This work was supported by NSF grants IIS-1217559, TWC-1526254, IIS-1563816, and by gifts from Google, Symantec, Yahoo, eBay, Intel, Amazon, LogicBlox."}, {"heading": "6. REFERENCES", "text": "[1] S. Boyd-Wickizer, A. T. Clements, Y. Mao,\nA. Pesterev, M. F. Kaashoek, R. Morris, N. Zeldovich, et al. An analysis of linux scalability to many cores. In OSDI, volume 10, pages 86\u201393, 2010.\n[2] R. R. Curtin, J. R. Cline, N. P. Slagle, W. B. March, P. Ram, N. A. Mehta, and A. G. Gray. mlpack: A scalable C++ machine learning library. Journal of Machine Learning Research, 14:801\u2013805, 2013.\n[3] Z. Lin, M. Kahng, K. M. Sabrin, D. H. P. Chau, H. Lee, and U. Kang. Mmap: Fast billion-scale graph computation on a pc via memory mapping. In Big Data (Big Data), 2014 IEEE International Conference on, pages 159\u2013164. IEEE, 2014.\n[4] F. McSherry, M. Isard, and D. G. Murray. Scalability! but at what cost? In 15th Workshop on Hot Topics in Operating Systems (HotOS XV), 2015."}], "references": [{"title": "An analysis of linux scalability to many cores", "author": ["S. Boyd-Wickizer", "A.T. Clements", "Y. Mao", "A. Pesterev", "M.F. Kaashoek", "R. Morris", "N. Zeldovich"], "venue": "In OSDI,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2010}, {"title": "mlpack: A scalable C++ machine learning library", "author": ["R.R. Curtin", "J.R. Cline", "N.P. Slagle", "W.B. March", "P. Ram", "N.A. Mehta", "A.G. Gray"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2013}, {"title": "Mmap: Fast billion-scale graph computation on a pc via memory mapping", "author": ["Z. Lin", "M. Kahng", "K.M. Sabrin", "D.H.P. Chau", "H. Lee", "U. Kang"], "venue": "In Big Data (Big Data),", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2014}, {"title": "Scalability! but at what cost", "author": ["F. McSherry", "M. Isard", "D.G. Murray"], "venue": "In 15th Workshop on Hot Topics in Operating Systems (HotOS XV),", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2015}], "referenceMentions": [{"referenceID": 3, "context": "Recent research demonstrated virtual memory\u2019s strong potential to scale up graph algorithms on a single PC [4, 3].", "startOffset": 107, "endOffset": 113}, {"referenceID": 2, "context": "Recent research demonstrated virtual memory\u2019s strong potential to scale up graph algorithms on a single PC [4, 3].", "startOffset": 107, "endOffset": 113}, {"referenceID": 2, "context": "Available on almost all modern platforms, virtual memory based approaches are straight forward to implement and to use, and can handle graphs with as many as 6 billion edges [3].", "startOffset": 174, "endOffset": 177}, {"referenceID": 3, "context": "Some single-thread implementations on a PC can even outperform popular distributed systems like Spark (128 cores) [4].", "startOffset": 114, "endOffset": 117}, {"referenceID": 0, "context": "methods including least recent used caching and read-ahead to achieve efficiency [1].", "startOffset": 81, "endOffset": 84}, {"referenceID": 1, "context": "To test the feasibility of M3, we minimally modified mlpack, an efficient machine learning library written in C++ [2].", "startOffset": 114, "endOffset": 117}, {"referenceID": 3, "context": "Our current evaluation focuses on: (1) understanding of how M3 scales with increasing data sizes; and (2) how M3 compares with distributed systems such as Spark, as prior work suggested the possibility that a single machine can outperform a computer cluster [4].", "startOffset": 258, "endOffset": 261}, {"referenceID": 3, "context": "tion that suggests cluster may not be necessary for moderatelysized datasets [4, 3].", "startOffset": 77, "endOffset": 83}, {"referenceID": 2, "context": "tion that suggests cluster may not be necessary for moderatelysized datasets [4, 3].", "startOffset": 77, "endOffset": 83}], "year": 2016, "abstractText": "To process data that do not fit in RAM, conventional wisdom would suggest using distributed approaches. However, recent research has demonstrated virtual memory\u2019s strong potential in scaling up graph mining algorithms on a single machine. We propose to use a similar approach for general machine learning. We contribute: (1) our latest finding that memory mapping is also a feasible technique for scaling up general machine learning algorithms like logistic regression and k-means, when data fits in or exceeds RAM (we tested datasets up to 190GB); (2) an approach, called M3, that enables existing machine learning algorithms to work with out-of-core datasets through memory mapping, achieving a speed that is significantly faster than a 4-instance Spark cluster, and comparable to an 8-instance cluster.", "creator": "LaTeX with hyperref package"}}}