{"id": "1604.03247", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "12-Apr-2016", "title": "Thesis: Multiple Kernel Learning for Object Categorization", "abstract": "Object Categorization is a challenging problem, especially when the images have clutter background, occlusions or different lighting conditions. In the past, many descriptors have been proposed which aid object categorization even in such adverse conditions. Each descriptor has its own merits and de-merits (or, more accurately, those that are not relevant or that are not relevant to the subject).\n\n\n\nI have been studying classification for a few years, but I am currently learning a few of the most well known. However, as I am increasingly able to look at the data on this subject I am not able to know a lot.\nI am still trying to work out a way to describe the many features I can do with object categorization that are lacking, like the fact that I cannot tell the source of the information. What I know is that the most common and often important feature that is a type of object categorization is an arbitrary color. If a color is colored then a color is called a 'color', which means the color of an object (red, green, blue, red, blue, white) is not as red as red as blue.\nI am able to do an exact survey of all the classification options for every object. In this study I will discuss how many distinct categories are used in a given color. These categories are used in my study, and I will give more details on why each class is used.\nI will then present the results of this study and how many different classes are used for different types of objects. This will be followed by a brief discussion of my most recent research. It is very important that you understand why each class of classification has different characteristics than another class. You will have a great chance to find some examples of classification. I hope you will enjoy this discussion and learn from the results.\nWhat I am trying to achieve in my study is that I will explore a few different criteria that will be used to categorize object categorization.\nDefinition of the most commonly known category\nDefinition of the most commonly known category\nDefinition of the most commonly known category\nDefinition of the most commonly known category\nExample of the most commonly known category\nExample of the most commonly known category\nIn addition to the categories defined in the following categories, I will look at the definitions for the most commonly known categories that have been categorized.\nDefinition of the most commonly known category\nIf you would like to make a quick survey of all the categories, you should follow this guide. You should not", "histories": [["v1", "Tue, 12 Apr 2016 04:56:24 GMT  (2980kb,D)", "http://arxiv.org/abs/1604.03247v1", null]], "reviews": [], "SUBJECTS": "cs.CV cs.LG", "authors": ["dinesh govindaraj"], "accepted": false, "id": "1604.03247"}, "pdf": {"name": "1604.03247.pdf", "metadata": {"source": "CRF", "title": "Thesis: Multiple Kernel Learning for Object Categorization", "authors": ["Dinesh Govindaraj"], "emails": [], "sections": [{"heading": null, "text": "Object Categorization is a challenging problem, especially when the images have clutter background, occlusions or different lighting conditions. In the past, many descriptors have been proposed which aid object categorization even in such adverse conditions. Each descriptor has its own merits and de-merits. Some descriptors are invariant to transformations while the others are more discriminative [1, 2]. Past research has shown that, employing multiple descriptors rather than any single descriptor leads to better recognition [3, 4]. The problem of learning the optimal combination of the available descriptors for a particular classification task is studied. Multiple Kernel Learning (MKL) framework has been developed for learning an optimal combination of descriptors for object categorization. Existing MKL formulations often employ block l-1 norm regularization which is equivalent to selecting a single kernel from a library of kernels [5, 6, 7, 8, 9]. Since essentially a single descriptor is selected, the existing formulations maybe suboptimal for object categorization. A MKL formulation based on block l-\u221e norm regularization has been developed, which chooses an optimal combination of kernels as opposed to selecting a single kernel. A Composite Multiple Kernel Learning(CKL) formulation based on mixed l-\u221e and l-1 norm regularization has been developed. These formulations end in Second Order Cone Programs(SOCP). Other efficient alternative algorithms for these formulation have been implemented. Empirical results on benchmark datasets show significant improvement using these new MKL formulations."}, {"heading": "1 INTRODUCTION", "text": "Object Categorization is a challenging problem, especially when the images have clutter background, occlusions or different lighting conditions. In the past, many descriptors have been proposed which aid object\ncategorization even in such adverse conditions. Each descriptor has its own merits and de-merits. Some descriptors are invariant to transformations while the others are more discriminative. For example, Scale Invariant Feature Transformation (SIFT [1]) is invariant to affine transformations, geometric blur descriptor [2] is robust to shape deformation and pyramid histogram of gradient [10] is invariant to geometric and photometric transformations. Past research has shown that, employing multiple descriptors rather than any single descriptor leads to better recognition [3, 4]. The project focuses on the problem of learning the optimal combination of the available descriptors for a particular classification task.\nAdaBoost for combining the descriptors has been developed which is inspired by the MKL work where each kernel is formed with different descriptors. Difference between AdaBoost SVM with different descriptors and MKL is AdaBoost gives weight on the SVM classifier, each SVM with different descriptors in kernel, where MKL gives weights on each kernel.\nIn [3, 11, 12, 4], the authors employ the Multiple Kernel Learning (MKL) framework [5] to find the optimal combination of descriptors (kernels). The goal of MKL is to simultaneously optimize the combination of kernels and the usual classification objective. Most of the existing MKL formulations perform a l-1 regularization [8, 3] over the kernels. This is equivalent to selecting the best kernel from the given set of kernels; which, as discussed earlier, might be suboptimal for object categorization tasks. One way to circumvent this problem of optimal weights being zero for many of the kernels, was introduced in [3], where an additional constraint to employ prior information is included.\nA new formulation for the MKL problem based on block l-\u221e and mixed norm(l-\u221e and l-1 norm)regularization has been developed. It is well known that such a regularization would induce \u201cequal weightage\u201d to all the kernels rather than sparsity is developed. Hence would be ideal for applications such\nar X\niv :1\n60 4.\n03 24\n7v 1\n[ cs\n.C V\n] 1\n2 A\npr 2\n01 6\nas object categorization, in which a combination of the descriptors is known to perform better than any single descriptor.\nThese new MKL formulations are Second Order Cone Programs(SOCP) which can be solved using solvers like Mosek, SeDuMi, etc. Other efficient alternative algorithms are also developed which alternates between SVM optimization parameter and kernel weights. Empirical results on Caltech-4, Caltech101 and Oxford flower datasets show significant improvement using these new MKL formulations.\nThe outline of the report is as follows: section 2 briefly reviews the work on object recognition. Existing MKL formulations is given section 3 and the new MKL formulations, is presented in section 4 5. In the subsequent section, efficient algorithms for solving the proposed MKL formulation are discussed. Section 6 presents experimental results on synthetic and real-world datasets which illustrate the merits of the new MKL formulation. The results show that the new formulations achieves better recognition compared to state-of-the-art, which is an l-1 regularization based formulation. Video change detection problem is presented in section ??. The report concludes in section 7 by summarizing the work."}, {"heading": "2 Related Work", "text": "This section provides some of work done in area of Machine learning involved in Object categorization. SVM-KNN [13] gets motivation from Local learning which uses K-Nearest neighbor to select local training point and uses SVM algorithm in those local training points for classification of object. Main problem here is time taken for classification.\nMultiple kernel learning considers the scenario where several descriptors (kernels) for a particular classification task are available. It aims to simultaneously learn the optimal combination of the given kernels and the optimal classifier parameters that maximize the generalization ability. Most of the work on MKL, since it was first introduced in [5], concentrates on the employment of a block l-1 regularization. The main features of it being: a) l-1 regularization leads to sparse combination of the kernels, and hence automatically performs feature selection b) very efficient algorithms to solve the formulation exist [6, 7, 8, 9].\nThere has been lot of work on combining descriptors for the object categorization task [3, 11, 10, 4, 12, 14, 15, 16, 17, 18, 19, 20]. In [10], the authors introduce spatial pyramid kernel and combine shape (pyramid histogram of gradient),\nappearance descriptors for object classification. In [4], the Support Kernel Machine [6], which is again based on l-1 regularization, is employed for combining descriptors for object categorization. In [12], a sample dependent local ensemble kernel machine is learned for object categorization. In [3], the authors use six descriptors for object categorization and employ a MKL formulation for learning the optimal combination of descriptors. However, as observed by the authors, most of the (important) kernels get eliminated in the optimal combination. This, as discussed above, is a consequence of employing the l-1 regularization. In order to circumvent the problem of optimal weights being zero for most of the kernels, the authors introduce additional constraints and parameters to utilize additional prior information regarding the kernels. This MKL formulation [3] is known to achieve state-of-the-art performance for many object recognition tasks. In [11], four descriptors for flower classification task were combined using the multiple kernel learning formulation in [3] and this is shown to achieve state-of-the-art performance on such tasks.\nIn summary, most of the existing methodologies for object categorization employ the l-1 regularization based MKL formulation and its variants. As discussed earlier, such a regularization leads to kernel selection rather than kernel combination and hence suboptimal for object categorization tasks. MKL formulation with block l-\u221e regularization and CKL, which is more suited for combining kernels as opposed to selecting kernels is presented in this report."}, {"heading": "3 Multiple kernel learning", "text": "This section gives brief introduction about MKL. Let xki denote the feature vector of the i\nth training datapoint in the kth feature space (kth kernel). Suppose yi denotes its label. Let Xk represent the matrix whose columns are the training datapoints in the kth feature space. Also, let Kk \u2261 X>kXk, be the gram matrix of the training datapoints in the kth feature space. Note that Xk may not be explicitly known; the gram matrices, Kk, are assumed to be known. Let y,Y represent the column vector, diagonal matrix with entries as labels of the training datapoints respectively. Let the discriminating hyperplane be \u2211l k=1 w > k x\nk \u2212 b = 0 (here, xk denotes the kth feature space representation of the datapoint, x. l is the number of kernels given). The usual soft-margin Support Vector Machine (SVM) [21, 17] formulation with this notation\nis:\nmin wk,b,\u03bei\n1 2 [\u2211l k=1 \u2016wk\u201622 ] + C \u2211 i \u03bei\ns.t. yi (\u2211l k=1 w > k x k i \u2212 b ) \u2265 1\u2212 \u03bei,\n\u03bei \u2265 0, \u2200 i = 1, . . . ,m (L2-MKL)\nThis is evident by considering w \u2261 [ w>1 . . .w > l ]> and\nxi \u2261 [( x1i )> . . . ( xli )>]> in the usual SVM formulation. It is easy to see that in this case the gram-matrix of the datapoints xi, i = 1, . . . ,m (m is the number of\ntraining datapoints) is nothing but \u2211l k=1 Kk. Hence, in the context of MKL, the optimal kernel with this formulation is nothing but a simple sum of the given kernels. Another alternative which has been extensively explored in the past [5, 8] was to employ a block l-1 regularization in order to perform kernel selection. This formulation can be written as:\nmin wk,b,\u03bei\n1 2 [\u2211l k=1 \u2016wk\u20162 ]2 + C \u2211 i \u03bei\ns.t. yi (\u2211l k=1 w > k x k i \u2212 b ) \u2265 1\u2212 \u03bei,\n\u03bei \u2265 0, \u2200 i = 1, . . . ,m (L1-MKL)\nNote that this formulation performs a l-1 regularization over \u2016wk\u20162, k = 1, . . . , l. Hence it automatically performs kernel selection and is equivalent to selecting one (the best) of the given kernels. Since the formulation promotes sparsity in the usage of the given kernels, it is best suited for feature selection applications rather than for applications like object categorization where each kernel is believed to provide important information regarding the classification problem at hand."}, {"heading": "4 L-\u221e regularization MKL For-", "text": "mulation\nThe alternative to that of l-1 regularization is to perform block l-\u221e regularization. Such a regularization promotes the use of all the kernels while assuming they are equally preferable. The proposed MKL formulation can be written as:\nmin wk,b,\u03bei\n[ maxlk=1 \u2016wk\u201622 ] + C \u2211 i \u03bei\ns.t. yi (\u2211l k=1 w > k x k i \u2212 b ) \u2265 1\u2212 \u03bei,\n\u03bei \u2265 0, \u2200 i = 1, . . . ,m (Li-MKL)\nIn the remainder of this section, the ranges of the indices i, k are omitted for convenience. The (Li-MKL) formulation is same as:\nmin t,wk,b,\u03bei\nt+ C \u2211 i \u03bei\ns.t. yi( \u2211 kw > k x k i \u2212 b) \u2265 1\u2212 \u03bei, \u03bei \u2265 0, \u2016wk\u201622 \u2264 t\nIn the following text the dual of the proposed MKL formulation is derived. The Lagrangian turns out to be: L = t+C \u2211 i \u03bei+ \u2211 i \u03b1i ( 1\u2212 \u03bei \u2212 yi (\u2211 k w>k x k i \u2212 b )) \u2212\n\u2211 i \u03b2i\u03bei + \u2211 k \u03bbk ( \u2016wk\u201622 \u2212 t ) where \u03b1i, \u03b2i, \u03bbk \u2265 0 are the Lagrange multipliers. From the KKT conditions:\n\u2207wkL = 0\u21d2 \u03bbkwk = 1\n2 \u2211 i \u03b1iyix k i (1)\n\u2202L \u2202t = 0\u21d2 \u2211 k \u03bbk = 1 (2) \u2202L \u2202b = 0\u21d2 \u2211 i \u03b1iyi = 0 (3) \u2202L \u2202\u03bei = 0\u21d2 C = \u03b1i + \u03b2i (4)\nNow, suppose that all the gram-matrices Kk are positive-definite (add a small ridge if singular, see also [8]). Then, (1) implies that if \u03bbk = 0 for some k, then \u03b1i = 0, \u2200 i. Clearly, in this case rest of the \u03bbk must also be zero \u2014 which is not possible since\u2211 k \u03bbk = 1. Hence \u03bbk > 0 \u2200 k. Eliminating the primal variables, the dual can be written as:\nmin \u03b1,\u03bb\n1 2\u03b1 >Q(\u03bb)\u03b1\u2212 1>\u03b1\ns.t. 0 \u2264 \u03b1 \u2264 C1,y>\u03b1 = 0, \u03bb \u2265 0,1>\u03bb = 1 (5)\nwhere Q(\u03bb) \u2261 12 \u2211 k YKkY \u03bbk\nand \u03bb, \u03b1 denote the column vectors with entries as \u03bbk, \u03b1i respectively.\nThough the dual in (12) has more variables, it gives more insight into the structure of the solution. Consider re-writing the dual (12) in the following way:\nmin \u03bb\nJ(\u03bb)\ns.t. \u03bb \u2265 0,1>\u03bb = 1 (6)\nwhere J(\u03bb) is the optimal value of the following convex QP:\nJ(\u03bb) \u2261min \u03b1\n1 2 \u03b1>Q(\u03bb)\u03b1\u2212 1>\u03b1\ns.t. 0 \u2264 \u03b1 \u2264 C1,y>\u03b1 = 0 (7)\nNote that (7) is nothing but a usual SVM problem and hence the optimal \u03b1 is very sparse. Infact, algorithms which exploit this sparsity in solution and outperform standard QP solvers exist [22]."}, {"heading": "4.1 Algorithms for solving the Li-MKL Formulation", "text": "The Li-MKL formulation, can be solved using standard Second Order Cone Program (SOCP) solvers(e.g., SeDuMi1, Mosek2). However the optimization problem would involve l conic quadratic constraints (l, the number of kernels, can be large). Also, the size of the optimization problem (m, the number of training datapoints), can be large. Hence generic cone solvers fail to solve for large l or m. Interestingly, there are more efficient ways to solve the dual formulations (12). The following sections explain in brief the possible methodologies."}, {"heading": "4.2 Alternating Minimization Algorithm", "text": "The dual (12) can be solved efficiently using an alternating minimization algorithm in the variables \u03b1 and \u03bb. Note that for a fixed value of \u03bb, (12) is nothing but the SVM dual (which has very efficient scalable solvers). Also, for fixed value of \u03b1, the minimization wrt. \u03bb is the following simple problem:\nmin \u03bbk\n\u2211 k Dk \u03bbk\ns.t. \u03bbk \u2265 0, \u2211 k \u03bbk = 1 (8)\nwhere Dk \u2261 \u03b1>Qk\u03b1. It is easy to show that the optimal values of \u03bbk for the problem (8) are (\u03bbk > 0 \u2200 k):\n\u03bbk =\n\u221a Dk\u2211\nk\n\u221a Dk\n(9)\nHence the following iterative algorithm can be employed to solve (12) efficiently:\n1. Initialize with \u03bb (0) k = 1 l where l is the number of\nkernels.\n1http://sedumi.ie.lehigh.edu/ 2www.mosek.com\n2. At iteration i \u2265 1, solve a standard SVM problem with Hessian as Q(\u03bb(i\u22121)) for \u03b1(i).\n3. Using \u03b1(i) compute D (i) k . Update \u03bb (i) using (9).\n4. repeat until, say, change in objective value of (12) is negligible."}, {"heading": "5 Composite MKL Formulation", "text": "This section explains Composite MKL. Suppose n descriptors are available. Further, for each of these descriptors Kernels (linear, polynomial, Gaussian) are defined. Let number of Kernels of the jth descriptor be denoted by nj . Also, let \u03c6jk denote the mapping induced by the kth Kernel of the jth descriptor. The hyperplane classifier to be learnt has the form\u2211n j=1 \u2211nj k=1 w > jk\u03c6jk(x) \u2212 b = 0. The objective is to choose the \u201cbest\u201d combination of these Kernels in order to maximize the generalization. The idea is to combine the Kernels in such a way that: a) all descriptors are given equal priority (weightage) b) best of the Kernels in each descriptor are selected. In other words, perform an l-\u221e regularization over the parameters (wjk) such that each descriptor is given equal priority. Further, perform an l-1 regularization such that sparsity in selection of Kernels belonging to each descriptor is encouraged. Mathematically, the formulation can be written as:\nmin wjk,b,\u03bei\n[ maxj (\u2211nj k=1 \u2016wjk\u20162 )2] + C \u2211 i \u03bei\ns.t. yi (\u2211n j=1 \u2211nj k=1 w > jk\u03c6jk(xi)\u2212 b ) (10)\n\u2265 1\u2212 \u03bei \u03bei \u2265 0 (CKL) (11)\nwhere {(xi, yi), i = 1, . . . ,m} is the training dataset. C is the regularization parameter.\nLet y denote the vector with entries as the labels. Let Sm be the set {\u03b1 \u2208 Rm | 0 \u2264 \u03b1 \u2264 C, y>\u03b1 = 0}. Denote the set {\u03b3 \u2208 Rn | \u03b3 \u2265 0, 1>\u03b3 = 1} by \u2206n. The dual of the above formulation can be written as:\nmin \u03bbj\u2208\u2206nj max \u03b1\u2208Sm, \u03b3\u2208\u2206n 1>\u03b1\u22121 4 \u03b1>  n\u2211 j=1 (\u2211nj k=1 \u03bbjkQjk \u03b3j )\u03b1 (12) where Qjk \u2261 YKjkY. Here, Kjk is the gram-matrix of the training datapoints with the kth Kernel of the jth descriptor and Y is the diagonal matrix with entries as the labels. The gram-matrices are assumed to be positive definite.\nOne can solve the above dual (12) using a simple alternating algorithm described below. Due to com-\npactness of feasibility sets and convexity of the objective, the order of min., max. can be rearranged. Also since the variables \u03bbj1 are not inter-linked with the variables \u03bbj2 , for j1 6= j2, instead of minimizing sum over j index one can sum the minima:\n\u2261 max \u03b1\u2208Sm max \u03b3\u2208\u2206n min \u03bbj\u2208\u2206nj 1>\u03b1\u22121 4 \u03b1>  n\u2211 j=1 (\u2211nj k=1 \u03bbjkQjk \u03b3j )\u03b1\n\u2261 max \u03b1\u2208Sm 1>\u03b1\u22121 4 min \u03b3\u2208\u2206n n\u2211 j=1\nmax\u03bbj\u2208\u2206nj \u2211nj k=1 \u03b1 >\u03bbjkQjk\u03b1\n\u03b3j\nNow it is easy to see that for fixed values of \u03b3, \u03bbj \u2200 j, the problem wrt. \u03b1 is same as the SVM problem. Also, for fixed values of \u03b1, the problem wrt. \u03b3, \u03bbj \u2200 j is the following simple problem:\nmin \u03b3\u2208\u2206n n\u2211 j=1\nmax\u03bbj\u2208\u2206nj \u2211nj k=1 \u03b1 >\u03bbjkQjk\u03b1\n\u03b3j (13)\nwhich has a closed form solution described below. Consider solving\nmax \u03bbj\u2208\u2206nj nj\u2211 k=1 \u03b1>\u03bbjkQjk\u03b1\nfor a particular j. This amounts to just picking the maximum among \u03b1>Qjk\u03b1 for k = 1, . . . , nj . Let these maxima be denoted by Dj(\u2265 0). Hence (13) is equivalent to the following problem:\nmin \u03b3\u2208\u2206n n\u2211 j=1 Dj \u03b3j\nThe optimal solution for this problem is: \u03b3j =\u221a Dj\u2211\nj\n\u221a Dj .\nThe overall algorithm is as follows:\n\u2022 Initialize with \u03b3(0) = 1n and \u03bb (0) j = 1 nj .\n\u2022 At iteration i \u2265 1, solve an SVM taking kernel as\u2211n j=1 \u2211nj k=1 \u03bb (i\u22121) jk Qjk\n\u03b3 (i\u22121) j\n. Update \u03b1(i) as the solution\nof this SVM.\n\u2022 Using updated values of \u03b1(i), compute the closed form solution of (13) using the methodology described above.\n\u2022 Repeat until convergence."}, {"heading": "6 Numerical Experiments", "text": "This section presents the experimental results on standard object categorization. Various experiments also conducted using the Adaboost for combining descriptors on standard Object categorization dataset. The key idea is to show that the proposed l-\u221e regularization and Composite regularization based MKL formulation leads to better generalization than the l-1 regularization based MKL formulations, which represent state-of-the-art methodologies for object recognition. The results on synthetic and real-world data are summarized in sections 6.2.1 and 6.2.3 respectively. In all cases, the parameters for the respective methods were tuned on a validation set. Also, the accuracies reported are on unseen testsets and hence represent a true estimate of the generalization performance of the respective classifiers. All multi-class problems were handled using the one-vs-one scheme."}, {"heading": "6.1 Results using Adaboost", "text": "Adaboost mentioned in the result is performed with following setup:\n1. Set of classifier for Adaboost is SVM.\n2. Each SVM in that set is build on different base kernel Ki mentioned in previous section.\n3. Here each Ki are build with descriptors like pyramid histograms of gradient, scale-invariant feature descriptors.\n4. Adaboost provides weight on the SVM classifier which is build on each such kernel mentioned above.\nThis AdaBoost is inspired by the Multiple kernel learning work where each kernel is formed with different descriptors. Difference between AdaBoost with SVM(with different descriptors) and Multiple kernel learning is AdaBoost gives weight on the SVM classifier(each SVM with different descriptors in kernel) where in multiple kernel learning gives weights on each kernel in a SVM problem. Following table shows result."}, {"heading": "6.2 Results of L-\u221e MKL Experiments", "text": "This section provides experimental results for the LiMKL."}, {"heading": "6.2.1 Synthetic Data", "text": "In this section, results on synthetic datasets showing the benefit of the proposed methodology is presented. The key result to establish is that the Li-MKL formulation achieves better generalization, especially in cases where the redundancy in the given kernels is less, such as in applications like object categorization. For this, the experimental strategy given by [23]. We repeat the description of the experimental set-up here for the sake of completeness.\nWe wish to create l kernels whose degree of redundancy is controlled by a single parameter \u03c1. First, m datapoints are sampled from two independent Normal distributions with covariance as the identity matrix (dimensionality of data is n). Here, datapoints sampled from different Normals are assumed to belong to different classes. Now the features are grouped into p disjoint sets (p varies from 1 to l): X1, . . . ,Xp where Xk \u2208 R n p\u00d7m. We then sample l \u2212 p copies from these disjoint sets, by randomly picking one by one from X1, . . . ,Xp with replacement. For each of these l sets randomly generate a linear transformation matrix Ai \u2208 R\u03c4 n p\u00d7 n p (\u03c4 is a parameter). The gram-matrices are computed as Kk = X > kA > kAkXk. Clearly, by varying \u03c1 = pl , the redundancy in the kernels can be varied. More specifically, \u03c1 = 1 represents the extreme case where the redundancy in kernels is zero, and hence represents the best-suited scenario for the proposed methodology. The other extreme case is \u03c1 = 0, where the redundancy is maximum, and hence an ideal scenario for employing the l-1 regularization based MKL.\nFigure 1 shows the plot of ratio of testset accuracies achieved by Li-MKL and L1-MKL vs. redundancy in the given kernels (vertical bars represent variance in accuracy). As a baseline for comparison, plot a similar graph for the ratio of testset accuracy achieved by Li-MKL and L2-MKL. Note that as the degree of redundancy decreases the ratio in case of both graphs increases; proving that Li-MKL is well-suited for ap-\nplications like object categorization. In fact, observed a huge improvement in generalization over the L1MKL when \u03c1 is near 1 (as high as 8% and 2% in case of L2-MKL). Also, in cases where the redundancy is high (\u03c1 is near 0), Li-MKL achieves generalization comparable to the other MKL formulations."}, {"heading": "6.2.2 Results on Caltech4 dataset", "text": "This section presents results on Caltech-43. Caltech4 dataset contains images of airplanes, cars, faces and bikes. We have taken 80 images for each class, of which 40 are randomly taken as the training/validation data and the remaining as test data. We have used Pyramid Histogram Of Gradient (PHOG) features generated4 at various levels (1,2,3) and angles (180,360). We have generated kernels on these six PHOG features using different parameters for the polynomial and Gaussian kernel (9 for each feature, totally 54 kernels). This experimental procedure was repeated for 20 times with different training-test data splits. The mean testset accuracies obtained with L1-MKL and Li-MKL were 92.00\u00b12.44% and 93.50\u00b12.14% respectively. This shows that the Li-MKL achieves better generalization. Following figures 3 5 6 7 8 9 10 shows ratio of accuracies of Li-MKL to L1-MKL as function of number of kernels on Caltech-4 dataset. Figure 4 shows confusion for Caltech-4 dataset."}, {"heading": "6.2.3 Results on Oxford dataset", "text": "The task in the Oxford flower dataset is to categorize images of 17 varieties of flowers. This dataset\n3http://www.robots.ox.ac.uk/~vgg/data/data-cats.html 4Code available at http://www.robots.ox.ac.uk/~vgg/\nresearch/caltech/phog.html\ncontains 80 examples for each class. In [11], the authors introduced four different features color, SIFT for foreground region, SIFT for foreground boundary, Histogram of Gradients for flowers. We have used the \u03c72 distances given in [11, 24]5 for our experimentation on this dataset. We have used same training, validation and test splits as used in [11]. The mean testset accuracy achieved by L1-MKL and Li-MKL are 85.88\u00b11.83% and 87.35\u00b11.72% respectively. Again, the results confirm that the proposed methodology achieves better generalization than state-of-the-art. The accuracy achieved by the proposed formulation is comparable to the best accuracy reported in [11], which is 88.33\u00b10.3%. Note that this state-of-the-art accuracy was achieved after tuning the parameters for the various descriptors [11] and incorporating prior information following the strategy of [3]. As mentioned earlier, incorporating such prior information may further improve testset accuracies of the proposed formulation. Following figures 11 13 14 15 shows ratio of accuracies of Li-MKL to L1-MKL as function of number of kernels on Oxford flower dataset. Figure 12 shows confusion for Oxford flower dataset.\nIn the subsequent set of experiments the generalization performance of the Li-MKL and L1-MKL as a function of the number of base kernels is compared. The plots are shown in figures 11 3 for the two benchmark datasets. Figures show that in most of the cases Li-MKL achieves better generalization than L1-MKL. Also, in some cases the improvement is as high as 7.5%. Note that the base kernels were derived from the fixed sets of descriptors and hence have some degree of redundancy. These results show that the\n5http://www.robots.ox.ac.uk/~vgg/data/flowers/17/\nindex.html\nproposed formulation does achieve good improvement generalization even in these cases. The next set of experiments compare the performance of the methodologies at various values of the regularization parameter C (see figure ??). Note that at performance of L1-MKL drastically decreases for low values of C. In some cases the difference in accuracy between LiMKL and L1-MKL is as high as 9%. Hence, the proposed formulation is less sensitive to the variation in the regularization parameter."}, {"heading": "6.2.4 Results on Caltech-101 dataset", "text": "This section presents results on Caltech-1016. Caltech-101 dataset contains 101 object categories. We have taken 30 images for each class, of which 15 are randomly taken as the training/validation data and the remaining as test data. We have used Pyramid Histogram Of Gradient (PHOG) features generated7 at various levels (1,2,3) and angles (180,360). We have generated kernels on these six PHOG features using different parameters for the polynomial and Gaussian kernel (9 for each feature, totally 54 kernels). This experimental procedure was repeated for 3 times with different training-test data splits. The mean testset accuracies obtained with L1-MKL and Li-MKL were 31.45% and 27.12% respectively. This shows that the Li-MKL achieves better generalization.\n6http://www.vision.caltech.edu/Image_Datasets/\nCaltech101/ 7Code available at http://www.robots.ox.ac.uk/~vgg/ research/caltech/phog.html"}, {"heading": "6.3 Results of CKL Experiments", "text": "All the experiments in this section is carried out using descriptors available from ColorDescriptor software8. The general procedure for the experiment is given in the figrue ??. All the experiments in this section follows:\n\u2022 Generate for all training images.\n\u2022 Generate all the 14 descriptors provided by the software(RGB histogram, Opponent histogram, Hue histogram, rg histogram, Transformed Color histogram, Color moments, Color moment invariants, SIFT, HueSIFT, HSV-SIFT, OpponentSIFT, rgSIFT, C-SIFT, Transformed Color SIFT).\n\u2022 Use no spatial pyramids.\n\u2022 Clusters all points from all training images to form a codebook for each descriptors.\n\u2022 Generate histogram of codebook of both training and testing images.\n\u2022 Train the classifier using these histograms as features.\n8http://staff.science.uva.nl/~ksande/research/\ncolordescriptors/\nResults obtained using above procedure is given in following sections."}, {"heading": "6.4 Results on Caltech-5 dataset", "text": "This section presents results on Caltech-59 using new MKL formulation and descriptors(csift, opponentsift, rgsift, sift, transformedcolorsift) provided from ColorDescriptor software. Caltech-5 dataset contains images of airplanes, cars, faces, leopards and bikes. This section follows same procedure discussed in previous section. We have used cluster size of 100 to form codebook. Clusters are found using k-means algorithm. Note here we have not run k-means multiple times to find the best cluster or codebook. We have taken 100 images for each class, of which 15 are randomly taken to form a codebook. We have generated kernels on 5 descriptors provided using different parameters for Gaussian kernel (10 for each descriptor, totally 50 kernels). This experimental procedure was repeated for 5 times with different training-test data splits. Figure ?? reports mean accuracy as number of training size increases. Note here we have used same codebook generated on 15 training points for all training sizes.\n9http://www.robots.ox.ac.uk/~vgg/data/data-cats.html"}, {"heading": "6.5 Results on Oxford dataset", "text": "The task in the Oxford flower dataset is to categorize images of 17 varieties of flowers. This dataset contains 80 examples for each class. In [11], the authors introduced four different features color, SIFT for foreground region, SIFT for foreground boundary, Histogram of Gradients for flowers. We have used the \u03c72 distances given in [11, 24]10 for our experimentation on this dataset. We have used same training, validation and test splits as used in [11]. The mean testset accuracy achieved by L1-MKL, Li-MKL and CKL are 85.3922%, 86.6667% and 86.6667% respectively."}, {"heading": "6.6 Results on Caltech-101 dataset", "text": "This section presents results on Caltech-10111 using new MKL formulations and all 14 descriptors provided from ColorDescriptor software. We have taken 30 images for each class, of which 15 are randomly taken as the training/validation data and the remaining as test data. We have generated kernels on 14 descriptors provided using different parameters for Gaussian kernel (2 for each descriptor, totally 28 kernels). With cluster size 600 accuracy obtained is around 24.1% and with cluster size 300, accuracy obtained is 23.21%. Main problem in Caltech-101, is problem of clustering for\n10http://www.robots.ox.ac.uk/~vgg/data/flowers/17/\nindex.html 11http://www.vision.caltech.edu/Image_Datasets/ Caltech101/\nforming codebook. Because of huge size and dimension in data, we followed 2-level kmeans. Our guess is that codebook formed is not good because clustering is not good."}, {"heading": "7 Conclusions and Future Work", "text": "The project addressed the issue of combining various descriptors for a given object categorization problem in order to achieve better generalization. The project also briefly addressed problem of video change detection.\nAdaboost has been designed for combining descriptors. State-of-the-art methodologies for object categorization employ a l-1 regularization based MKL formulation, which is more suitable for selecting descriptors rather than combining them. The key idea is to employ a l-\u221e regularization and mixed l-\u221e and l-1 regularization for combining the descriptors in an MKL framework. The new MKL formulation is better suited for object categorization and highly efficient algorithms which solve the corresponding convex optimization problem were derived.\nEmpirical results performed on synthetic and realworld benchmark datasets clearly establish the efficacy of the proposed MKL formulation. In some cases, the increase in accuracy when compared to the standard l-1 regularization was as high as 9%. The results also show that there is a consistant improvement in accuracy in almost all the cases, however, the improvement is maximized when the redundancy in the base kernels is low. Another advantage with the proposed formulation is that it is less sensitive to variation in the regularization parameter, C.\nWork is going on to experiment the new MKL formulations for Caltech-101 dataset using codebook models described. Experiments is going on to form codebooks with different size as size of codebook largely affect classification accuracy. Novality of new MKL formulations will be known once experimentation on the bigger dataset has been done namely Pascal and Caltech-256. Future work also includes to experiment the new MKL formulations for these bigger datasets."}], "references": [{"title": "Distinctive image features from scaleinvariant keypoints", "author": ["David G. Lowe"], "venue": "International Journal of Computer Vision,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2004}, {"title": "Geometric blur for template matching", "author": ["A. Berg", "J. Malik"], "venue": "In CVPR,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2001}, {"title": "Learning the discriminative power-invariance trade-off", "author": ["M. Varma", "D. Ray"], "venue": "In ICCV, pages", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2007}, {"title": "Support kernel machines for object recognition", "author": ["A. Kumar", "C. Sminchisescu"], "venue": "In ICCV,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2007}, {"title": "Learning the kernel matrix with semidefinite programming", "author": ["Gert Lanckriet", "Nello Cristianini", "Peter Bartlett", "Laurent El Ghaoui"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2004}, {"title": "Multiple kernel learning, conic duality, and the smo algorithm", "author": ["F.R. Bach", "G.R.G. Lanckriet", "M.I. Jordan"], "venue": "In ICML,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2004}, {"title": "Large scale multiple kernel learning", "author": ["S\u00f6ren Sonnenburg", "Gunnar R\u00e4tsch", "Christin Sch\u00e4fer", "Bernhard Sch\u00f6lkopf"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2006}, {"title": "Simple MKL", "author": ["Alain Rakotomamonjy", "Francis R. Bach", "Stephane Canu", "Yves Grandvalet"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2008}, {"title": "An Extended Level Method for Efficient Multiple Kernel Learning", "author": ["Z. Xu", "R. Jin", "I. King", "M. Lyu"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2008}, {"title": "Representing shape with a spatial pyramid kernel", "author": ["A. Zisserman A. Bosch", "X. Munoz"], "venue": "In CIVR,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2007}, {"title": "Automated flower classification over a large number of classes", "author": ["M-E. Nilsback", "A Zisserman"], "venue": "In Proceedings of the Indian Conference on Computer Vision, Graphics and Image Processing,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2008}, {"title": "Local ensemble kernel learning for object category recognition", "author": ["Y.Y. Lin", "T.Y. Liu", "C.S. Fuh"], "venue": "In CVPR,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2007}, {"title": "Svm-knn: Discriminative nearest neighbor classification for visual category recognition", "author": ["Hao Zhang", "Alexander C. Berg", "Michael Maire", "Jitendra Malik"], "venue": "Proceedings of the 2006 IEEE Computer Society Conference on Computer Vision and Pattern Recognition,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2006}, {"title": "Local features and kernels for classification of texture and object categories: a comprehensive study", "author": ["S. Lazebnik J. Zhang", "M. Marszalek", "C. Schmid"], "venue": "In IJCV,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2007}, {"title": "On the algorithmics and applications of a mixed-norm based kernel learning formulation", "author": ["Saketha N. Jagarlapudi", "Dinesh Govindaraj", "Raman S", "Chiranjib Bhattacharyya", "Aharon Ben-tal", "Ramakrishnan K.r"], "venue": null, "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2009}, {"title": "Controlled sparsity kernel learning", "author": ["Dinesh Govindaraj", "Sankaran Raman", "Sreedal Menon", "Chiranjib Bhattacharyya"], "venue": "CoRR, abs/1401.0116,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2014}, {"title": "Sparse classifier design based on the shapley value", "author": ["Prashanth Ravipally", "Dinesh Govindaraj"], "venue": "In Proceedings of the World Congress on Engineering,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2010}, {"title": "Moneybee: Towards enabling a ubiquitous, efficient, and easyto-use mobile crowdsourcing service in the emerging market", "author": ["Dinesh Govindaraj", "Naidu K.V.M", "Animesh Nandi", "Girija Narlikar", "Viswanath Poosala"], "venue": "Bell Labs Technical Journal,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2011}, {"title": "Modeling attractiveness and multiple clicks in sponsored search results", "author": ["Dinesh Govindaraj", "Tao Wang", "S.V.N. Vishwanathan"], "venue": "CoRR, abs/1401.0255,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2014}, {"title": "Application of active appearance model to automatic face replacement", "author": ["Dinesh Govindaraj"], "venue": "Journal of Applied Statistics,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2011}, {"title": "Fast Training of Support Vector Machines using Sequential Minimal Optimization", "author": ["J. Platt"], "venue": "In Advances in Kernel Methods\u2014Support Vector Learning,", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 1999}, {"title": "Non-sparse multiple kernel learning", "author": ["Marius Kloft", "Ulf Brefeld", "Pavel Laskov", "Soren Sonnenburg"], "venue": "In Workshop on Kernel Learning: Automatic Selection of Optimal Kernels,", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2008}, {"title": "A visual vocabulary for flower classification", "author": ["M-E. Nilsback", "A. Zisserman"], "venue": "In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition,", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2006}], "referenceMentions": [{"referenceID": 0, "context": "Some descriptors are invariant to transformations while the others are more discriminative [1, 2].", "startOffset": 91, "endOffset": 97}, {"referenceID": 1, "context": "Some descriptors are invariant to transformations while the others are more discriminative [1, 2].", "startOffset": 91, "endOffset": 97}, {"referenceID": 2, "context": "Past research has shown that, employing multiple descriptors rather than any single descriptor leads to better recognition [3, 4].", "startOffset": 123, "endOffset": 129}, {"referenceID": 3, "context": "Past research has shown that, employing multiple descriptors rather than any single descriptor leads to better recognition [3, 4].", "startOffset": 123, "endOffset": 129}, {"referenceID": 4, "context": "Existing MKL formulations often employ block l-1 norm regularization which is equivalent to selecting a single kernel from a library of kernels [5, 6, 7, 8, 9].", "startOffset": 144, "endOffset": 159}, {"referenceID": 5, "context": "Existing MKL formulations often employ block l-1 norm regularization which is equivalent to selecting a single kernel from a library of kernels [5, 6, 7, 8, 9].", "startOffset": 144, "endOffset": 159}, {"referenceID": 6, "context": "Existing MKL formulations often employ block l-1 norm regularization which is equivalent to selecting a single kernel from a library of kernels [5, 6, 7, 8, 9].", "startOffset": 144, "endOffset": 159}, {"referenceID": 7, "context": "Existing MKL formulations often employ block l-1 norm regularization which is equivalent to selecting a single kernel from a library of kernels [5, 6, 7, 8, 9].", "startOffset": 144, "endOffset": 159}, {"referenceID": 8, "context": "Existing MKL formulations often employ block l-1 norm regularization which is equivalent to selecting a single kernel from a library of kernels [5, 6, 7, 8, 9].", "startOffset": 144, "endOffset": 159}, {"referenceID": 0, "context": "For example, Scale Invariant Feature Transformation (SIFT [1]) is invariant to affine transformations, geometric blur descriptor [2] is robust to shape deformation and pyramid histogram of gradient [10] is invariant to geometric and photometric transformations.", "startOffset": 58, "endOffset": 61}, {"referenceID": 1, "context": "For example, Scale Invariant Feature Transformation (SIFT [1]) is invariant to affine transformations, geometric blur descriptor [2] is robust to shape deformation and pyramid histogram of gradient [10] is invariant to geometric and photometric transformations.", "startOffset": 129, "endOffset": 132}, {"referenceID": 9, "context": "For example, Scale Invariant Feature Transformation (SIFT [1]) is invariant to affine transformations, geometric blur descriptor [2] is robust to shape deformation and pyramid histogram of gradient [10] is invariant to geometric and photometric transformations.", "startOffset": 198, "endOffset": 202}, {"referenceID": 2, "context": "Past research has shown that, employing multiple descriptors rather than any single descriptor leads to better recognition [3, 4].", "startOffset": 123, "endOffset": 129}, {"referenceID": 3, "context": "Past research has shown that, employing multiple descriptors rather than any single descriptor leads to better recognition [3, 4].", "startOffset": 123, "endOffset": 129}, {"referenceID": 2, "context": "In [3, 11, 12, 4], the authors employ the Multiple Kernel Learning (MKL) framework [5] to find the optimal combination of descriptors (kernels).", "startOffset": 3, "endOffset": 17}, {"referenceID": 10, "context": "In [3, 11, 12, 4], the authors employ the Multiple Kernel Learning (MKL) framework [5] to find the optimal combination of descriptors (kernels).", "startOffset": 3, "endOffset": 17}, {"referenceID": 11, "context": "In [3, 11, 12, 4], the authors employ the Multiple Kernel Learning (MKL) framework [5] to find the optimal combination of descriptors (kernels).", "startOffset": 3, "endOffset": 17}, {"referenceID": 3, "context": "In [3, 11, 12, 4], the authors employ the Multiple Kernel Learning (MKL) framework [5] to find the optimal combination of descriptors (kernels).", "startOffset": 3, "endOffset": 17}, {"referenceID": 4, "context": "In [3, 11, 12, 4], the authors employ the Multiple Kernel Learning (MKL) framework [5] to find the optimal combination of descriptors (kernels).", "startOffset": 83, "endOffset": 86}, {"referenceID": 7, "context": "Most of the existing MKL formulations perform a l-1 regularization [8, 3] over the kernels.", "startOffset": 67, "endOffset": 73}, {"referenceID": 2, "context": "Most of the existing MKL formulations perform a l-1 regularization [8, 3] over the kernels.", "startOffset": 67, "endOffset": 73}, {"referenceID": 2, "context": "One way to circumvent this problem of optimal weights being zero for many of the kernels, was introduced in [3], where an additional constraint to employ prior information is included.", "startOffset": 108, "endOffset": 111}, {"referenceID": 12, "context": "SVM-KNN [13] gets motivation from Local learning which uses K-Nearest neighbor to select local training point and uses SVM algorithm in those local training points for classification of object.", "startOffset": 8, "endOffset": 12}, {"referenceID": 4, "context": "Most of the work on MKL, since it was first introduced in [5], concentrates on the employment of a block l-1 regularization.", "startOffset": 58, "endOffset": 61}, {"referenceID": 5, "context": "The main features of it being: a) l-1 regularization leads to sparse combination of the kernels, and hence automatically performs feature selection b) very efficient algorithms to solve the formulation exist [6, 7, 8, 9].", "startOffset": 208, "endOffset": 220}, {"referenceID": 6, "context": "The main features of it being: a) l-1 regularization leads to sparse combination of the kernels, and hence automatically performs feature selection b) very efficient algorithms to solve the formulation exist [6, 7, 8, 9].", "startOffset": 208, "endOffset": 220}, {"referenceID": 7, "context": "The main features of it being: a) l-1 regularization leads to sparse combination of the kernels, and hence automatically performs feature selection b) very efficient algorithms to solve the formulation exist [6, 7, 8, 9].", "startOffset": 208, "endOffset": 220}, {"referenceID": 8, "context": "The main features of it being: a) l-1 regularization leads to sparse combination of the kernels, and hence automatically performs feature selection b) very efficient algorithms to solve the formulation exist [6, 7, 8, 9].", "startOffset": 208, "endOffset": 220}, {"referenceID": 2, "context": "There has been lot of work on combining descriptors for the object categorization task [3, 11, 10, 4, 12, 14, 15, 16, 17, 18, 19, 20].", "startOffset": 87, "endOffset": 133}, {"referenceID": 10, "context": "There has been lot of work on combining descriptors for the object categorization task [3, 11, 10, 4, 12, 14, 15, 16, 17, 18, 19, 20].", "startOffset": 87, "endOffset": 133}, {"referenceID": 9, "context": "There has been lot of work on combining descriptors for the object categorization task [3, 11, 10, 4, 12, 14, 15, 16, 17, 18, 19, 20].", "startOffset": 87, "endOffset": 133}, {"referenceID": 3, "context": "There has been lot of work on combining descriptors for the object categorization task [3, 11, 10, 4, 12, 14, 15, 16, 17, 18, 19, 20].", "startOffset": 87, "endOffset": 133}, {"referenceID": 11, "context": "There has been lot of work on combining descriptors for the object categorization task [3, 11, 10, 4, 12, 14, 15, 16, 17, 18, 19, 20].", "startOffset": 87, "endOffset": 133}, {"referenceID": 13, "context": "There has been lot of work on combining descriptors for the object categorization task [3, 11, 10, 4, 12, 14, 15, 16, 17, 18, 19, 20].", "startOffset": 87, "endOffset": 133}, {"referenceID": 14, "context": "There has been lot of work on combining descriptors for the object categorization task [3, 11, 10, 4, 12, 14, 15, 16, 17, 18, 19, 20].", "startOffset": 87, "endOffset": 133}, {"referenceID": 15, "context": "There has been lot of work on combining descriptors for the object categorization task [3, 11, 10, 4, 12, 14, 15, 16, 17, 18, 19, 20].", "startOffset": 87, "endOffset": 133}, {"referenceID": 16, "context": "There has been lot of work on combining descriptors for the object categorization task [3, 11, 10, 4, 12, 14, 15, 16, 17, 18, 19, 20].", "startOffset": 87, "endOffset": 133}, {"referenceID": 17, "context": "There has been lot of work on combining descriptors for the object categorization task [3, 11, 10, 4, 12, 14, 15, 16, 17, 18, 19, 20].", "startOffset": 87, "endOffset": 133}, {"referenceID": 18, "context": "There has been lot of work on combining descriptors for the object categorization task [3, 11, 10, 4, 12, 14, 15, 16, 17, 18, 19, 20].", "startOffset": 87, "endOffset": 133}, {"referenceID": 19, "context": "There has been lot of work on combining descriptors for the object categorization task [3, 11, 10, 4, 12, 14, 15, 16, 17, 18, 19, 20].", "startOffset": 87, "endOffset": 133}, {"referenceID": 9, "context": "In [10], the authors introduce spatial pyramid kernel and combine shape (pyramid histogram of gradient), appearance descriptors for object classification.", "startOffset": 3, "endOffset": 7}, {"referenceID": 3, "context": "In [4], the Support Kernel Machine [6], which is again based on l-1 regularization, is employed for combining descriptors for object categorization.", "startOffset": 3, "endOffset": 6}, {"referenceID": 5, "context": "In [4], the Support Kernel Machine [6], which is again based on l-1 regularization, is employed for combining descriptors for object categorization.", "startOffset": 35, "endOffset": 38}, {"referenceID": 11, "context": "In [12], a sample dependent local ensemble kernel machine is learned for object categorization.", "startOffset": 3, "endOffset": 7}, {"referenceID": 2, "context": "In [3], the authors use six descriptors for object categorization and employ a MKL formulation for learning the optimal combination of descriptors.", "startOffset": 3, "endOffset": 6}, {"referenceID": 2, "context": "This MKL formulation [3] is known to achieve state-of-the-art performance for many object recognition tasks.", "startOffset": 21, "endOffset": 24}, {"referenceID": 10, "context": "In [11], four descriptors for flower classification task were combined using the multiple kernel learning formulation in [3] and this is shown to achieve state-of-the-art performance on such tasks.", "startOffset": 3, "endOffset": 7}, {"referenceID": 2, "context": "In [11], four descriptors for flower classification task were combined using the multiple kernel learning formulation in [3] and this is shown to achieve state-of-the-art performance on such tasks.", "startOffset": 121, "endOffset": 124}, {"referenceID": 16, "context": "The usual soft-margin Support Vector Machine (SVM) [21, 17] formulation with this notation", "startOffset": 51, "endOffset": 59}, {"referenceID": 4, "context": "Another alternative which has been extensively explored in the past [5, 8] was to employ a block l-1 regularization in order to perform kernel selection.", "startOffset": 68, "endOffset": 74}, {"referenceID": 7, "context": "Another alternative which has been extensively explored in the past [5, 8] was to employ a block l-1 regularization in order to perform kernel selection.", "startOffset": 68, "endOffset": 74}, {"referenceID": 7, "context": "Now, suppose that all the gram-matrices Kk are positive-definite (add a small ridge if singular, see also [8]).", "startOffset": 106, "endOffset": 109}, {"referenceID": 20, "context": "Infact, algorithms which exploit this sparsity in solution and outperform standard QP solvers exist [22].", "startOffset": 100, "endOffset": 104}, {"referenceID": 21, "context": "For this, the experimental strategy given by [23].", "startOffset": 45, "endOffset": 49}, {"referenceID": 10, "context": "In [11], the authors introduced four different features color, SIFT for foreground region, SIFT for foreground boundary, Histogram of Gradients for flowers.", "startOffset": 3, "endOffset": 7}, {"referenceID": 10, "context": "We have used the \u03c7 distances given in [11, 24] for our experimentation on this dataset.", "startOffset": 38, "endOffset": 46}, {"referenceID": 22, "context": "We have used the \u03c7 distances given in [11, 24] for our experimentation on this dataset.", "startOffset": 38, "endOffset": 46}, {"referenceID": 10, "context": "We have used same training, validation and test splits as used in [11].", "startOffset": 66, "endOffset": 70}, {"referenceID": 10, "context": "The accuracy achieved by the proposed formulation is comparable to the best accuracy reported in [11], which is 88.", "startOffset": 97, "endOffset": 101}, {"referenceID": 10, "context": "Note that this state-of-the-art accuracy was achieved after tuning the parameters for the various descriptors [11] and incorporating prior information following the strategy of [3].", "startOffset": 110, "endOffset": 114}, {"referenceID": 2, "context": "Note that this state-of-the-art accuracy was achieved after tuning the parameters for the various descriptors [11] and incorporating prior information following the strategy of [3].", "startOffset": 177, "endOffset": 180}, {"referenceID": 10, "context": "In [11], the authors introduced four different features color, SIFT for foreground region, SIFT for foreground boundary, Histogram of Gradients for flowers.", "startOffset": 3, "endOffset": 7}, {"referenceID": 10, "context": "We have used the \u03c7 distances given in [11, 24] for our experimentation on this dataset.", "startOffset": 38, "endOffset": 46}, {"referenceID": 22, "context": "We have used the \u03c7 distances given in [11, 24] for our experimentation on this dataset.", "startOffset": 38, "endOffset": 46}, {"referenceID": 10, "context": "We have used same training, validation and test splits as used in [11].", "startOffset": 66, "endOffset": 70}], "year": 2016, "abstractText": "Object Categorization is a challenging problem, especially when the images have clutter background, occlusions or different lighting conditions. In the past, many descriptors have been proposed which aid object categorization even in such adverse conditions. Each descriptor has its own merits and de-merits. Some descriptors are invariant to transformations while the others are more discriminative [1, 2]. Past research has shown that, employing multiple descriptors rather than any single descriptor leads to better recognition [3, 4]. The problem of learning the optimal combination of the available descriptors for a particular classification task is studied. Multiple Kernel Learning (MKL) framework has been developed for learning an optimal combination of descriptors for object categorization. Existing MKL formulations often employ block l-1 norm regularization which is equivalent to selecting a single kernel from a library of kernels [5, 6, 7, 8, 9]. Since essentially a single descriptor is selected, the existing formulations maybe suboptimal for object categorization. A MKL formulation based on block l-\u221e norm regularization has been developed, which chooses an optimal combination of kernels as opposed to selecting a single kernel. A Composite Multiple Kernel Learning(CKL) formulation based on mixed l-\u221e and l-1 norm regularization has been developed. These formulations end in Second Order Cone Programs(SOCP). Other efficient alternative algorithms for these formulation have been implemented. Empirical results on benchmark datasets show significant improvement using these new MKL formulations.", "creator": "LaTeX with hyperref package"}}}