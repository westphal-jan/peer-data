{"id": "1604.03346", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "12-Apr-2016", "title": "An incremental linear-time learning algorithm for the Optimum-Path Forest classifier", "abstract": "We present a classification method with linear-time incremental capabilities based on the Optimum-Path Forest (OPF) classifier. The OPF considers instances as nodes of a fully-connected training graph, where the edges' weights are the distances between two nodes' feature vectors. Upon this graph, a minimum spanning tree is built, and every edge connecting instances of different classes is removed, with those nodes becoming prototypes or roots of a tree.\n\n\n\n\nFigure 1: Optimum-Path Tree\n(This is the following graph from the Optimum-Path Tree (Figure 1).\n(This is the following graph from the Optimum-Path Tree (Figure 1).\nFigure 2: Optimum-Path Tree\n(This is the following graph from the Optimum-Path Tree (Figure 1).\n(This is the following graph from the Optimum-Path Tree (Figure 1).\n(This is the following graph from the Optimum-Path Tree (Figure 1).\n(This is the following graph from the Optimum-Path Tree (Figure 1).\nFigure 3: Optimum-Path Tree\n(This is the following graph from the Optimum-Path Tree (Figure 1).\n(This is the following graph from the Optimum-Path Tree (Figure 1).\n(This is the following graph from the Optimum-Path Tree (Figure 1).\n(This is the following graph from the Optimum-Path Tree (Figure 1).\n(This is the following graph from the Optimum-Path Tree (Figure 1).\n(This is the following graph from the Optimum-Path Tree (Figure 1).\n(This is the following graph from the Optimum-Path Tree (Figure 1).\n(This is the following graph from the Optimum-Path Tree (Figure 1).\n(This is the following graph from the Optimum-Path Tree (Figure 1).\n(This is the following graph from the Optimum-Path Tree (Figure 1).\n(This is the following graph from the Optimum-Path Tree (Figure 1).\n(This is the following graph from the Optimum-Path Tree (Figure 1).\n(This is the following graph from the Optimum-Path Tree (Figure 1).\n(This is the following graph from the Optimum-Path Tree (Figure 1).\n(This is the following graph from the Optimum-Path Tree (Figure 1).\n(This is the following graph from the Optimum-Path Tree", "histories": [["v1", "Tue, 12 Apr 2016 11:31:23 GMT  (242kb,D)", "https://arxiv.org/abs/1604.03346v1", null], ["v2", "Wed, 22 Jun 2016 15:47:21 GMT  (483kb,D)", "http://arxiv.org/abs/1604.03346v2", null], ["v3", "Fri, 29 Jul 2016 15:54:04 GMT  (243kb,D)", "http://arxiv.org/abs/1604.03346v3", "submitted to Journal"], ["v4", "Fri, 12 Aug 2016 13:14:10 GMT  (243kb,D)", "http://arxiv.org/abs/1604.03346v4", "submitted to Journal"], ["v5", "Wed, 23 Nov 2016 12:08:23 GMT  (253kb,D)", "http://arxiv.org/abs/1604.03346v5", "submitted to IPL Journal for consideration in Nov/2016"]], "reviews": [], "SUBJECTS": "cs.LG cs.CV", "authors": ["moacir ponti", "mateus riva"], "accepted": false, "id": "1604.03346"}, "pdf": {"name": "1604.03346.pdf", "metadata": {"source": "CRF", "title": "An incremental linear-time learning algorithm for the Optimum-Path Forest classifier", "authors": ["Moacir Ponti", "Mateus Riva"], "emails": [], "sections": [{"heading": null, "text": "We present a classification method with incremental capabilities based on the Optimum-Path Forest classifier (OPF). The OPF considers instances as nodes of a fully-connected training graph, arc weights represent distances between two feature vectors. Our algorithm includes new instances in an OPF in linear-time, while keeping similar accuracies when compared with the original quadratic-time model."}, {"heading": "1. Introduction", "text": "The optimum-path forest (OPF) classifier [1] a classification method that can be used to build simple, multiclass and parameter independent classifiers. One possible drawback of using the OPF classifier in learning scenarios in which there is need to constantly update the model, is its quadratic training time. Let a training set be composed of n examples, the OPF training algorithm runs in O(n2). Some efforts were made to mitigate such running time by using several OPF classifiers trained with ensembles of reduced training sets [2] and fusion using split sets using multi-threading [3]. Also, recent work developed strategies to speed-up the training algorithm by taking advantage of data structures such as [1, 4]. However, an OPF-based method with incremental capabilities is still to be investigated, since sub-quadratic algorithms are important in many scenarios [5].\nIncremental learning is a machine learning paradigm in which the classifier changes and adapts itself to include new examples that emerged after the initial construction of the classifier [6]. As such, an incremental-capable classifier has to start with an incomplete a priori dataset and include successive new data without the need to rebuild itself. In [4] the authors propose\nPreprint submitted to IPL November 24, 2016\nar X\niv :1\n60 4.\n03 34\n6v 5\n[ cs\n.L G\n] 2\n3 N\nan alternative OPF algorithm which is more efficient to retrain the model, but their algorithm is not incremental. Also, the empirical evidence shows that the running time is still quadratic, although with a significantly smaller constant. In this paper we describe an algorithm that can include new examples individually (or in small batches) in an already build model, which is a different objective when compared to [4] and [1]. In fact, we already used the improvements proposed by [1]. Therefore our new algorithm does not compete, but rather can be used as a complement for those variants.\nBecause OPF is based on the Image Foresting Transform for which there is a differential algorithm available (DIFT) [7], it would be a natural algorithm to try. However, DIFT is an image processing algorithm and includes all new pixels/nodes as prototypes, which would progressively convert the model into a 1-Nearest Neighbour classifier. Therefore we propose an alternative solution that maintains the connectivity properties of optimum-path trees.\nOur OPF-Incremental (OPFI) is inspired in graph theory methods to update minimum spanning trees [8] and minimal length paths [9] in order to maintain the graph structure and thus the learning model. We assume there is an initial model trained with the original OPF training, and then perform several inclusions of new examples appearing over time. This is an important feature since models should be updated in an efficient way in order to comply with realistic scenarios. Our method will be useful everywhere the original OPF is useful, along with fulfilling incremental learning requirements."}, {"heading": "2. OPF Incremental (OPFI)", "text": "The optimum-path forest (OPF) classifier [1] interprets the instances (examples) as the nodes (vertices) of a graph. The edges connecting the vertices are defined by some adjacency relation between the examples, weighted by a distance function. It is expected that training examples from a given class will be connected by a path of nearby examples. Therefore the model that is learned by the algorithm is composed by several trees, each tree is a minimum spanning tree (MST) and the root of each tree is called prototype.\nOur OPF incremental updates an initial model obtained by the original OPF training by using the minimum-spanning tree properties in the existing optimum-path forest. Provided this initial model, our algorithm is able to include a new instance in linear-time. Note that in incremental learning scenarios it is typical to start with an incomplete training set, often presenting a poor accuracy due to the lack of a sufficient sample.\nOur solution works by first classifying the new example using the current model. Because the label of the classified example is known, it is possible to infer if it has been conquered by a tree of the same class (i.e. it was correctly classified) or not. We also know which node was responsible for the conquest, i.e. its predecessor. Using this knowledge, we the have three possible cases:\n1. Predecessor belongs to the same class and is not a prototype: the new example is inserted in the predecessor\u2019s tree, maintaining the properties of a minimum spanning tree.\n2. Predecessor belongs to the same class and is a prototype: we must discover if the new example will take over as prototype. If so, the new prototype must reconquer the tree; otherwise, it is inserted in the tree as in the first case.\n3. Predecessor belongs to another class: the new example and its predecessor become prototypes of a new tree. The new example will be root of an new tree; while the predecessor will begin a reconquest of its own tree, splitting it in two.\nThe Figure 1 illustrates the three cases when an element of the \u2019triangle\u2019 class is inserted in the OPF.\nThe classification and insertion of new elements is described on Algorithm 1 and shows the high-level solution described above.\nAlgorithm 1 OPF-Incremental insertion Require: a previously trained OPF model T with n vertices; new instances to be\nincluded Z[1...b]. 1: OPF Classify(Z, T) // as in [1] 2: for i\u2190 1 to b (each new example) do 3: if Z[i].label = Z[i].truelabel then 4: if Z[i].pred is prototype then 5: recheckPrototype(Z[i],Z[i].pred,T ) // Algorithm 3 6: else 7: insertIntoMST(Z[i],Z[i].pred,T ) // Algorithm 2 8: end if 9: else\n10: Z[i] becomes a prototype 11: Z[i].pred becomes a prototype 12: reconquest(Z[i].pred Z[i].pred, T ) // Algorithm 4 13: end if 14: end for 15: return T\nAlgorithm 2 OPF-Incremental MST insertion Require: T is the graph; z is the new example; r is any vertex in the tree; t is a\nglobal variable and is the largest edge in the path between w to z, whereas m is the largest edge between r and z.\n1: mark r \u201dold\u201d 2: m\u2190 (r, z) 3: for each vertex w adjacent to r do 4: if w is marked \u201dnew\u201d then 5: insertIntoMST(w, z, T ) // recursive call 6: k \u2190 the larger of the edges t and (w, r) 7: h\u2190 the smaller of the edges t and (w, r) 8: T gets the edge h 9: if cost of k < cost of m then\n10: m\u2190 k 11: end if 12: end if 13: end for 14: t\u2190 m 15: return T\nThe minimum spanning tree insertion function, described on Algorithm 2 is an adapted version of the minimum spanning tree updating algorithm proposed by [8]. The function for rechecking a prototype, described on Algorithm 3 takes the distance between the prototype and its pair (the corresponding prototype in the other tree, which edge was cut during the initial phase of classification), and between the new example and said pair. If the new example is closer to the pair, it takes over as prototype and reconquers the tree. Otherwise, it is inserted in the tree. The reconquest function was defined by [1], and described also here on Algorithm 4 for clarity.\nAlgorithm 3 OPF-Incremental recheck prototype Require: an input node Z and its predecessor pred; a previously trained OPF\nmodel T ; some distance function dist(\u00b7). 1: if dist(Z, pred.pair) < dist(pred, pred.pair) then 2: Z becomes a prototype 3: reconquest(Z, Z, T ) // Algorithm 4 4: else 5: insertIntoMST(Z,pred,T ) // Algorithm 2 6: end if 7: return T\nAlgorithm 4 OPF-Incremental reconquest Require: an root node Z and its predecessor pred; a previously trained OPF\nmodel T ; some distance function dist(\u00b7). 1: Z is marked \u201dold\u201d {In the first call the root is its own predecessor} 2: newPathval\u2190 dist(Z, pred) 3: if newPathval < Z.pathval then 4: Z.pred\u2190 pred 5: Z.pathval\u2190 newPathval 6: for each adjacent w of Z do 7: if w is not \u201dold\u201d then 8: reconquest(w,Z,T ) // recursive call 9: end if\n10: end for 11: end if 12: return T\nAfter the insertion is performed, an ordered list of nodes is updated as\nin [1]. The new example is inserted in its proper position in linear time, thus allowing for the optimisation of the classification step.\nWhen a new instance is inserted, we ensure that its classified label is equal to its true label, which is not always the case as in the original OPF algorithm because a given node can be conquered by a prototype with label different from the true label. Our method differs from the original OPF in this point, but we believe it is important to ensure the label of the new instance because the model is updated upon it and the label plays an important role for instance when a new class appears. Therefore, although our algorithm does not produces a model that is equal to the original OPF, it increments the model by maintaining the optimum path trees properties, rechecking prototypes and including new trees. Those are shown to be enough to achieve classification results that are similar to the original OPF."}, {"heading": "3. Complexity Analysis", "text": "The complexity of inserting a new example into a model containing a total of n nodes is O(n), as we demonstrate for each case below.\n(i) Predecessor is of another class. splitting a tree is O(1) since it only requires a given edge to be removed. The reconquest is O(n), since it goes through each node at most once as described by [1];\n(ii) Predecessor is of same class and is a prototype. again, the complexity of the reconquest is O(n). Otherwise, it is an insertion, with complexity O(n) as described in the case (iii) below.\n(iii) Predecessor is of same class and is not a prototype. the complexity of the operation is related to the inclusion of a new example on an existing tree. The complexity is O(n), or linear in terms of the number of examples, as proof below for Algorithm 2, showing that the function insertIntoMST() is able to update the MST in linear time.\nProof. Let z be the new example conquered by a vertex r on some tree. After executing line 5 of Algorithm 2, m and t are the largest edges in the paths from r to z, and from w (first vertex adjacent to r) to z respectively. Because the vertices are numbered in the order that they complete their calls to Algorithm 2 the linearity can be proven by induction.\nBase step. The first node to complete its call, say w\u2032, must be a leaf node of the graph. Thus, lines 5 to 10 are skipped and t is assigned as (w\u2032, z) which is the only edge joining w\u2032 and z. If r\u2032 is a vertex incident to w\u2032, it is easy to see that m = (r\u2032, z) both before and after the call insertIntoMST(w\u2032).\nInduction steps. When executing line 5, i.e. insertIntoMST(w), if w is a leaf, again lines 5 and 9 are skipped and t = (w, z). Otherwise, let x be the vertex which is incident to w and which is considered last in the call insertIntoMST(w). By induction hypothesis, m and t are the largest edges in the paths joining w and x to z, respectively, after executing insertIntoMST(x). It can be shown that in all cases t will be the largest edge in the path joining w to z. Similarly, m is the largest edge in the path joining r to z.\nAlso, in lines 6 to 9, the largest edge among m, (w, r) and t is deleted, and thus m and T (the MST) are updated. Since at most n \u2212 1 edges are deleted, each of which was the largest in a cycle, T will still remain a MST.\nBecause insertIntoMST(r) has (n \u2212 1) recursive calls at line 5, the lines 1, 2, 6\u201310 and 14 are executed n times. Lines 3 and 4 counts each tree edge twice (at most), and as this is proportional to the adjacency list, those are executed at most 2(n\u2212 1) times. Therefore, Algorithm 2 runs in O(n)."}, {"heading": "4. Experiments", "text": ""}, {"heading": "4.1. Data sets", "text": "The code and all datasets that are not publicly available can be found at http://www.icmc.usp.br/~moacir/paper/16opfi.html. A variety of synthetic and real datasets are used in the experiments.\nSynthetic datasets: Base1/Base2/Base3: with size 10000 and data distributed in 4 regions. Base1 has 2 classes, Base2 has 4 classes and Base3 has 3 classes; Lithuanian, Circle vs Gaussian (C-vs-G), Cone-Torus and Saturn: are 2-d classes with different distributions.\nReal datasets: CTG cardiotocography dataset, 2126 examples, 21 features, 3 classes; NTL (non-technical losses) energy profile dataset, 4952 examples, 8 features, 2 classes; Parkinsons dataset, 193 examples, 22 features, 2 classes; Produce image dataset, 1400 examples, 64 features, 14 classes; Skin segmentation dataset, 245057 examples, 3 features, 2 classes; SpamBase email dataset, 4601 examples, 56 features, 2 classes; MPEG7-B shape dataset, 70 classes and 20 examples."}, {"heading": "4.2. Experimental setup", "text": "Experiments were conduced to test the OPFI algorithm and comparing with the original OPF and DIFT. We aim to get similar accuracies with respect to them in linear time. Each experiment was conducted in 10-repeated hold-out sampling:\n1. Split data 50-50: S for supervised training and T for testing, keeping the class distribution of the original dataset;\n2. Split S: maintaining class proportions: BaseN, C-vs-G, Lithu, CTG, NTL, Parkinsons, Produce, SpamBase, Skin: into 100 subsets Si, with i = 0..99. Cone-Torus, Saturn, MPEG7: into 10 subsets Si, with i = 0..9 (fewer subsets because they have few examples/class).\n3. Initial training on S0 using original OPF as base to be incremented;\n4. Update model including each Si in sequence, starting with i = 1."}, {"heading": "5. Results and Discussion", "text": "The balanced accuracy (takes into account the proportions of examples in each class) results are shown in Table 1. A plot with accuracy and running time results for 3 datasets are shown in Figure 2. By inspecting the average and standard deviations, it is possible to see that OPFI is able to keep accuracies similar to original OPF and DIFT. However, it runs faster then the OPF and does not degrade the graph structure as happens with DIFT. The optimum-path trees are preserved and therefore can be explored in scenarios when incremental learning is needed.\nThe running time curves shows the linear versus quadratic behaviour on all experiments: for n examples in the previous model, each new inclusion with original OPF would take O((n+1)2), while with OPFI it is performed in O(n+1). When including examples in batches, our algorithm runs in O(n\u00b7b), where b is the batch size, while OPF runs in O((n + b)2). Therefore our method suits better several small inclusions than large batches. Neverthelles, OPFI\u2019s running time (n \u00b7 b) is still O(n) and o((n + b)2).\nWe believe our contribution will allow the OPF method to be used more efficiently in future studies, such as data stream mining and active learning applications. Previous algorithms for decreasing the running time of the OPF training step can also be used within each batch of examples to be added to the OPFI algorithm to further speed-up the process."}, {"heading": "Acknowledgment", "text": "We would like to thank FAPESP (#11/22749-8 and #11/16411-4)."}], "references": [{"title": "Efficient supervised optimum-path forest classification for large datasets", "author": ["J.P. Papa", "A. Falc\u00e3o", "V. De Albuquerque", "J. Tavares"], "venue": "Pattern Recognition 45 (1) ", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2012}, {"title": "Ensembles of optimum-path forest classifiers using input data manipulation and undersampling", "author": ["M. Ponti-Jr.", "I. Rossi"], "venue": "in: MCS 2013,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2013}, {"title": "Improving accuracy and speed of optimum-path forest classifier using combination of disjoint training subsets", "author": ["M. Ponti-Jr", "J. Papa"], "venue": "in: 10th Int. Work. on Multiple Classifier Systems ", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2011}, {"title": "A path-and label-cost propagation approach to speedup the training of the optimum-path forest classifier", "author": ["A.S. Iwashita", "J.P. Papa", "A. Souza", "A.X. Falc\u00e3o", "R. Lotufo", "V. Oliveira", "V.H.C. De Albuquerque", "J.M.R. Tavares"], "venue": "Pattern Recognition Letters 40 ", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2014}, {"title": "Estimating the number of connected components in sublinear time", "author": ["P. Berenbrink", "B. Krayenhoff", "F. Mallmann-Trenn"], "venue": "Information Processing Letters 114 (11) ", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2014}, {"title": "Incremental learning", "author": ["X. Geng", "K. Smith-Miles"], "venue": "Encyclopedia of Biometrics ", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2015}, {"title": "Interactive volume segmentation with differential image foresting transforms", "author": ["A.X. Falc\u00e3o", "F.P. Bergo"], "venue": "Medical Imaging, IEEE Transactions on 23 (9) ", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2004}, {"title": "Algorithms for updating minimal spanning trees", "author": ["F. Chin", "D. Houck"], "venue": "Journal of Computer and System Sciences 16 (3) ", "citeRegEx": "8", "shortCiteRegEx": null, "year": 1978}, {"title": "Incremental algorithms for minimal length paths", "author": ["G. Ausiello", "G.F. Italiano", "A.M. Spaccamela", "U. Nanni"], "venue": "Journal of Algorithms 12 (4) ", "citeRegEx": "9", "shortCiteRegEx": null, "year": 1991}], "referenceMentions": [{"referenceID": 0, "context": "The optimum-path forest (OPF) classifier [1] a classification method that can be used to build simple, multiclass and parameter independent classifiers.", "startOffset": 41, "endOffset": 44}, {"referenceID": 1, "context": "Some efforts were made to mitigate such running time by using several OPF classifiers trained with ensembles of reduced training sets [2] and fusion using split sets using multi-threading [3].", "startOffset": 134, "endOffset": 137}, {"referenceID": 2, "context": "Some efforts were made to mitigate such running time by using several OPF classifiers trained with ensembles of reduced training sets [2] and fusion using split sets using multi-threading [3].", "startOffset": 188, "endOffset": 191}, {"referenceID": 0, "context": "Also, recent work developed strategies to speed-up the training algorithm by taking advantage of data structures such as [1, 4].", "startOffset": 121, "endOffset": 127}, {"referenceID": 3, "context": "Also, recent work developed strategies to speed-up the training algorithm by taking advantage of data structures such as [1, 4].", "startOffset": 121, "endOffset": 127}, {"referenceID": 4, "context": "However, an OPF-based method with incremental capabilities is still to be investigated, since sub-quadratic algorithms are important in many scenarios [5].", "startOffset": 151, "endOffset": 154}, {"referenceID": 5, "context": "Incremental learning is a machine learning paradigm in which the classifier changes and adapts itself to include new examples that emerged after the initial construction of the classifier [6].", "startOffset": 188, "endOffset": 191}, {"referenceID": 3, "context": "In [4] the authors propose", "startOffset": 3, "endOffset": 6}, {"referenceID": 3, "context": "In this paper we describe an algorithm that can include new examples individually (or in small batches) in an already build model, which is a different objective when compared to [4] and [1].", "startOffset": 179, "endOffset": 182}, {"referenceID": 0, "context": "In this paper we describe an algorithm that can include new examples individually (or in small batches) in an already build model, which is a different objective when compared to [4] and [1].", "startOffset": 187, "endOffset": 190}, {"referenceID": 0, "context": "In fact, we already used the improvements proposed by [1].", "startOffset": 54, "endOffset": 57}, {"referenceID": 6, "context": "Because OPF is based on the Image Foresting Transform for which there is a differential algorithm available (DIFT) [7], it would be a natural algorithm to try.", "startOffset": 115, "endOffset": 118}, {"referenceID": 7, "context": "Our OPF-Incremental (OPFI) is inspired in graph theory methods to update minimum spanning trees [8] and minimal length paths [9] in order to maintain the graph structure and thus the learning model.", "startOffset": 96, "endOffset": 99}, {"referenceID": 8, "context": "Our OPF-Incremental (OPFI) is inspired in graph theory methods to update minimum spanning trees [8] and minimal length paths [9] in order to maintain the graph structure and thus the learning model.", "startOffset": 125, "endOffset": 128}, {"referenceID": 0, "context": "The optimum-path forest (OPF) classifier [1] interprets the instances (examples) as the nodes (vertices) of a graph.", "startOffset": 41, "endOffset": 44}, {"referenceID": 0, "context": "1: OPF Classify(Z, T) // as in [1] 2: for i\u2190 1 to b (each new example) do", "startOffset": 31, "endOffset": 34}, {"referenceID": 7, "context": "The minimum spanning tree insertion function, described on Algorithm 2 is an adapted version of the minimum spanning tree updating algorithm proposed by [8].", "startOffset": 153, "endOffset": 156}, {"referenceID": 0, "context": "The reconquest function was defined by [1], and described also here on Algorithm 4 for clarity.", "startOffset": 39, "endOffset": 42}, {"referenceID": 0, "context": "in [1].", "startOffset": 3, "endOffset": 6}, {"referenceID": 0, "context": "The reconquest is O(n), since it goes through each node at most once as described by [1];", "startOffset": 85, "endOffset": 88}], "year": 2016, "abstractText": "We present a classification method with incremental capabilities based on the Optimum-Path Forest classifier (OPF). The OPF considers instances as nodes of a fully-connected training graph, arc weights represent distances between two feature vectors. Our algorithm includes new instances in an OPF in linear-time, while keeping similar accuracies when compared with the original quadratic-time model.", "creator": "LaTeX with hyperref package"}}}