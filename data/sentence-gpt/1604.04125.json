{"id": "1604.04125", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "14-Apr-2016", "title": "Filling in the details: Perceiving from low fidelity images", "abstract": "Humans perceive their surroundings in great detail even though most of our visual field is reduced to low-fidelity color-deprived (e.g. dichromatic) input by the retina's ability to perceive the light from the peripheral region. This makes the perception of the body much less likely to detect the body's surroundings. In addition, our focus of our perception has been to learn more about the eyes, and more about how we perceive the rest of the world.\n\n\n\nFor example, in the study of humans, we observed the sight-and-sound interaction of the three types of eyes, eye-light- and eye-focus (see Figure 2 of the \"Eye Focused Eye: The Hidden Eye\" on the Web). In other words, as a result of these changes, we saw both eyes' vision in many different directions.\nOne reason for these changes in vision is that our brains are used to perceive the world around us. This is a reason that our eyes need different perspectives and different types of light. However, the visual-to-eye relationship between our brains is much different, because the two systems also have a common set of characteristics that can lead to different vision outcomes. As an example, when we had our brain trained in the eye, we were able to see both the eye and the sight, and our eye-focus was also able to see both the eye and the sight.\nAnother reason that our brain evolved for these different ways of perceiving the world is that our brain has a different processing model for recognizing the world around us. For example, in the eyes of the human eye, our brain is the focal point of our visual field. These parts of the brain have the ability to see both the light, and the sight to be viewed simultaneously. In a study conducted with a blind blind eye, participants were given an identical visual system and were able to see both the eye and the sight. A second system was used to interpret this interaction to measure both the eye and the sight. For example, when we were able to see the sight of the human eye, our brain was able to see the sight of the human eye. As a result, when we were able to see the sight of the human eye, our brain is the focal point of our visual field. The visual system is also capable of discriminating the light emitted by the eye and the sight.\nAnother possible mechanism that we use to understand the world around us, as a way to understand the world around us is", "histories": [["v1", "Thu, 14 Apr 2016 12:10:23 GMT  (3517kb,D)", "http://arxiv.org/abs/1604.04125v1", null]], "reviews": [], "SUBJECTS": "cs.CV cs.LG cs.NE", "authors": ["farahnaz ahmed wick", "michael l wick", "marc pomplun"], "accepted": false, "id": "1604.04125"}, "pdf": {"name": "1604.04125.pdf", "metadata": {"source": "CRF", "title": "Filling in the details: Perceiving from low fidelity images", "authors": ["Farahnaz Ahmed Wick", "Michael L. Wick"], "emails": ["fwick@cs.umb.edu", "mwick@cs.umass.edu", "mpomplun@cs.umb.edu", "fwick@cs.umb.edu"], "sections": [{"heading": "1 Introduction", "text": "The success of machine learning algorithms depends heavily upon the representation of the input data. A major appeal of deep learning, on which the current dominant approaches for machine vision tasks are based [1], is that they can automatically learn useful feature representations from the data. A criticism of most deep architectures is that they wastefully process every input component when performing a task; for example, the input layer considers all pixels in every region of the input when learning an image classifier and making classification decisions.\nIn contrast, the human visual system has only a small fovea of high resolution chromatic input allowing it to more judiciously budget computational resources [2]. In order to receive additional information in the field of view, we make either covert or overt shifts of attention. Overt shifts of attention or eye-movements allow us to bring the fovea over particular locations in the environment that are relevant to current behavior. To avoid the serial nature of processing as demanded from overt shifts of attention, our visual system can also engage in covert shifts of attention in which the eyes remain fixated on one location but attention is deployed to a different location.\n\u2217Email: fwick@cs.umb.edu\nar X\niv :1\n60 4.\n04 12\n5v 1\n[ cs\n.C V\n] 1\nThe human retina receives 10 million bits per second which exceeds the computational resources available to our visual system to assimilate at any given time [3]. Even though we perceive the environment around us in great detail, only a small fraction of the information registered by the visual system is processed. This paper asks a simple question: If high detail input were not available, would artificial neural networks still be able to capture aspects of the underlying distribution?\nTo further put this question in perspective, our own fovea takes up only 4% of the entire retina [4] and is solely responsible for sharp central full color vision with maximum acuity. The relative visual acuity diminishes rapidly with eccentricity from the fovea [5]. As a result, visual performance is best at the fovea and progressively worse towards the periphery [6]. Indeed, our visual cortex is receiving distorted color-deprived visual input except for the central two degrees of the visual field [7] as seen in Figure 1. Despite receiving such a distorted signal, we perceive the world in color and high resolution and are mostly unaware of this distortion. Even when confronted with actual blurry or distorted visual input, our visual system is good at extracting the scene contents and context. For instance, our system can recognize faces and emotions expressed by those faces in resolutions as low as 16 x 16 pixels [8]. We can reliably extract contents of a scene from the gist of an image [9] even at low resolutions [10, 11].\nRecently, Ullman et al. [12] has shown that our visual system is capable of recognizing contents of images from critical feature configurations (called minimal recognizable images or MIRCs) that current deep learning systems cannot utilize for similar tasks. These MIRCS resemble foveations on an image and their results reveal that the human visual system employs features and processes that are not used by current deep networks. Similarly, little attention has been given by the deep learning community to how these networks deal with distorted or noisy inputs. We draw inspiration from the abilities of the human visual system and ask whether an artificial neural network can learn to perceive an image from low fidelity input. If this is the case, we can design state of the art architectures in image super resolution, automatic image coloring, image compression and at the same time, reduce computational costs of processing entire images associated with deep networks.\nThere has been a revival in applying the idea of attention to deep learning architectures [13, 14, 15, 16]. Such work is exciting and has lead to improvements in tasks ranging from machine translation [15] to image captioning [16]. However, in many approaches\u2014especially those that employ a soft attention mechanism\u2014the computational cost is increased. For example, when generating a target sentence, a network must compute a softmax over every word of a source sentence or location of a source image. Unlike these systems, humans perceive by sequentially directing attention to relevant portions of the data and in turn enables our visual system to reduce computational costs [17, 3].\nIn this paper, we want to understand what kind of information can be gleaned from low-fidelity inputs. What can be gleaned from a single foveal glimpse? What is the most predictive region of an image? We present a framework for studying such questions based on a generative model known as an autoencoder. In contrast to traditional or de-noising autoencoders [18], which attempt to reconstruct the original image (or respectively, a salt and pepper corrupted version), our autoencoders\nattempt to reconstruct original high-detail inputs from lower-detail foveated versions of those images (that is, images that are entirely low detail except perhaps a small \u201cfovea\u201d of high detail). Thus, we have taken to calling them defoveating autoencoders (DFAE).\nWe find that even relatively simple DFAE architectures are able to perceive color, shape and contrast information, but fail to recover high-frequency information (e.g., textures) when confronted with extremely impoverished input. Interestingly, as the amount of detail present in the input diminishes, the structure of the learnt features becomes increasingly global."}, {"heading": "2 Background: Autoencoders", "text": "Autoencoders are a class of unsupervised algorithms which pairs a bottom-up recognition network (encoder) with a top-down generative network (decoder). The encoder, denoted as the function f\u03b8, forms a compressed representation y = f\u03b8(x) of the input x. y is the feature vector representation or code computed from x. In the context of our work, we were interested in whether or not we can learn a rich representation y of a low fidelity input image x.\nThe output, denoted as the function g\u03b8\u2032 , maps the feature vector back into the input space producing a reconstruction z = g\u03b8\u2032(y) through the minimization of a reconstruction error function. Good generalization means reconstruction error of test examples should be close to the reconstruction error for training examples. To capture the structure of the underlying data distribution and prevent the autoencoder from learning the identity function, we can either require the hidden layer to have lower dimensionality than the input or regularize the weights [19]. The lower dimension constraint is what the classical autoencoder or PCA does while the higher dimension is used by the sparse autoencoders [20]. Recently, denoising autoencoders have been shown to regularize the network by adding noise such as salt-and-pepper (SP) noise to the input, thus forcing the model to learn to predict the true image from its noisy representation [19].\nIn summary, the basic autoencoder training consists of optimizing parameter vector \u03b8 to minimize reconstruction error as measured by the loss, L:\u2211\ni\nL(xi, g\u03b8\u2032(f\u03b8(x i))) (1)\nwhere xi is a training example. The minimization is carried out by standard gradient descent algorithms like backpropagation. The commonly used forms for the encoder is an affine mapping followed by non linearity:\nf\u03b8(x) = s(Wx+ b) (2)\nwhere s is the encoder activation function, \u03b8 = {W,b}, W is the weight matrix and b is the bias vector. Similarly the decoder mapping is:\ng\u03b8\u2032(y) = s(W \u2032y + b\u2032) (3)\nwith the appropriately sized parameters \u03b8\u2032 = {W\u2032,b\u2032} As mentioned above, it has been shown that the features learnt by the encoder without any nonlinearity are a subspace of the principal components of the input space [21]. However, when a nonlinear activation such as a sigmoid is used in the encoder, an AE can learn more powerful feature detectors than a simple PCA [22]. The architecture of a simple one hidden layer AE is very similar to that of a multilayer perceptron (MLP). The difference between AEs and MLPs lies in the output layer: the MLP predicts the class C of the input X whereas an AE reconstructs Z from X .\nWe will start by reviewing related work on using distorted inputs to train deep networks and then move on to describe the architecture of AE that was used to test feature extraction from downsampled images."}, {"heading": "3 Related work", "text": "Using noisy or jittered inputs to understand feature learning in the framework of AEs or MLPs has been explored before [23, 19]. Vincent et al. [19] first proposed training autoencoders with corrupted\nimage as input. Therefore their denoising autoencoder (DAE) learnt to reconstruct the clean input from a corrupted version. They have shown that introducing noise to the input lowers classification error on benchmark classification problems. The filters produced by denoising AEs tend to capture more distinctive blob-like features and with higher level of corruption in the input image, they learn less localized filters. In fact, Bishop [24] has argued that in a linear system training with noise has a similar effect as training with a regularizer, such as an L2 weight decay. Another proposal to make autoencoders noise invariant is by Rifal et al. [25]. They improved on DAEs by adding a penalty term, called the contraction ratio, to the learnt mapping which makes the features learnt more robust and invariant to change of raw input. In the spirit of denoising AEs, we incorporate a form of noise in our input image. However, unlike the SP noise, our noise is generated from using a foveation function (described below) on the image. We investigated whether foveations acted as a strong regularizer for the AE like the SP noise, thus allowing us to use it in deep architectures.\nDenoising images has been investigated using architectures other than autoencoders. Xie et al. [26] presented an approach to remove noise from corrupted inputs using sparse coding and deep networks pre-trained with DAEs. Their end to end system could automatically remove complex patterns like text from an image in addition to simple patterns like pixels missing at random. The type of noise additions they investigated were white Gaussian noise, SP noise (flipping pixels randomly), and image background changes. Along the same lines, post deblurring denoising [27] and using convolutional neural networks for natural image denoising of patterns such as specks, dirt and rain has been investigated [28].\nAs mentioned above, low resolution images can be considered as a type of noisy input. In the domain of image super resolution, Cui et al. [29] used low resolution images interpolated to the size of the output image and AEs in their pipeline to restore resolution of these images. Their cascade model is not trained end-to-end and requires optimization of each layer individually. A similar approach by Dong et al. [30] improves on Cui et al.[29] by using convolutional neural networks and with an end-to-end system. Behnke et al. [31] demonstrated that difficult non-linear image reconstruction from low resolution inputs can be learnt by hiearchical recurrent networks. From a given 28 x 28 handwritten digit image as input, their system can iteratively increase it\u2019s resolution to 64 x 64 output.\nOur work can be viewed as an image super resolution problem, in that our network learns mapping between low resolution and high resolution images. In contrast to existing approaches, our network is end-to-end differentiable and thus learns features automatically via backpropagation. Current approaches require manual engineering of features and image pre-processing on top of interpolations. Finally, we emphasize here that our goal is not achieving state of the art results in image super resolution. Rather, we want to study a deep architecture\u2019s ability to extract useful representation from low-detail images and showing the range in which mapping between low resolution and high resolution images is possible. The usefulness of the representation is then measured using mean squared error between input and reconstructed output."}, {"heading": "4 Framework: Defoveating Autoencoders (DFAE)", "text": "We now present a framework for studying the extent to which neural networks can \u201cperceive\u201d an image given various types of low-detail (or foveated) inputs. We begin by specifying a space of neural network architectures and by precisely defining a notion of perceives that we can measure. It is important that the framework is general and not dependent on a specific task such as image classification in which, for example, the ability to learn domain-specific discriminating features might make it easy to solve the classification problem without fully modeling the structure of the input. This is undesirable because then we are unable to trust classification accuracy as a reliable surrogate for perceiving.\nWith this in mind, we focus instead on generative models of the raw input data itself, specifically autoencoders (AE). The AE\u2019s hidden units h are analogous to the intermediate neurons in our visual system that capture features and structure of the visual input. Similarly, the AE\u2019s weights W forge visual memories of the training set and are thus analogous to long-term memory. When these weights are properly trained, the activations of the hidden units reflect how the network is perceiving a novel input. However, since these units are not directly interpretable, we indirectly measure how well\nthe network perceives by evaluating the similarity between the original and generated (high-detail) images: the more similar the images are, the better the network is able to perceive.\nMore formally, let x be the original input image and x\u0302 = \u03c6(x) be a lower-detail foveated version of that image. That is, a version of the image which is mostly low-detail (e.g., downsampled, black-and-white, or both) except for possibly a small portion which is high-detail (mimicking our own fovea). For example, if we encode images as vectors of floats between 0 and 1 (reflecting pixel intensities in RGB or grayscale) then we might define a class of foveation functions as \u03c6 : [0, 1]n \u2192 [0, 1]m s.t. m n and the foveation function might downsample the original image according to the eccentricity from the image center while also removing most of the vector components corresponding to color. We then employ the autoencoder to defoveate x\u0302 by generating a high-quality output image y = f(x\u0302;W ) in which, for example, y \u2208 [0, 1]n. Finally, we can then measure the similarity between y and x as:\n1. a surrogate for how well the network perceives from the foveated input and 2. part of a loss function to train the network.\nIn summary, DFAEs simply comprise:\n1. A foveation function that filters an original image by removing detail (color, downsampling, blurring, etc). We will later make this the independent variables in our experiments so we can study the effect of different types of input distortion.\n2. An autoencoder network that inputs the low-detail foveated input, but is trained to output the high-detail original image.\n3. A loss function for measuring the quality of the reconstruction against the original image and for training the network.\nGiven this framework we can now study how well different architectures are able to cope with different types of foveated inputs. Note that much like denoising autoencoders, these autoencoders reconstruct an original image from a corrupted version. However, the form of corruption is a systematic foveation instead of random SP noise. Thus, as an homage to denoising autoencoders [18], we have termed these models defoveating autoencoders or DFAEs. We describe our exact model in the next section."}, {"heading": "4.1 DFAE Architecture and Loss Function", "text": "In our experiments, we study DFAEs with fully connected layers. That is, DFAEs of the form:\nx\u0302 = \u03c6(x) (no learnable parameters) (4) h(0) = tanh ( W (0)x\u0302 ) (5)\nh(i) = tanh ( W (i)h(i\u22121) ) for i = 1, \u00b7 \u00b7 \u00b7 , k \u2212 1 (6)\ny = \u03c3 ( W (k)h(k\u22121) ) (7)\nwhere \u03c3 is the logistic function. The sigmoid in the final layer conveniently allows us to compare the pixel intensities between the generated image y and the original image x directly, without having to post-process the output values. We experiment with the number of hidden units per-layer as well as the number of layers. For training, we could employ the traditional mean-squared (MSE) error or cross-entropy loss, but we found that the domain-specific loss function of peak signal-to-noise ratio (PSNR) yielded much better training behavior. The PSNR between generated image a y = f(\u03c6(x)) and its original input x is defined as follows:\nLH(x, y) = log10\n( 1\u221a\nMSE(x, y)\n) where MSE(x, y) = n\u22121xT y (8)\nNetwork parameters were initialized at random in the range [-0.1,0.1] and loss was minimized by stochastic gradient descent with adagrad updates [32]. Adaptive gradient descent, or adagrad, is a\nform of stochastic gradient descent that determines the per-feature learning rate dynamically during training. Adagrad calculates a different learning rate for each feature, allowing it to efficiently learn the weights even for features that rarely occur in the training data. The learning rate was initialized at 1.0 and was adjusted by adagrad during training. We performed 1000 epochs of training in all experiments."}, {"heading": "4.2 Recurrent DFAEs for Sequences of Foveations", "text": "The above architecture is useful for studying single foveations, which is the primary focus of this work. However, we remark that it is straightforward to augment DFAEs with recurrent connections to handle a sequence of foveations similar to what has has been done for solving classification tasks with attention [14]. First, augment the foveation function \u03c6 to include a locus ` on which the fovea is centered. Second, a saccade function s(ht;Ws) predicts such a locus from the DFAE\u2019s current hidden states ht, and finally we make the hidden state recurrent via a function g(ht\u22121,Wg). Putting this all together yields the following architecture:\nx\u0302t = \u03c6(xt, `t) foveate the image at location ` ht = fe(g(ht\u22121;Wg), x\u0302t;W ) encode: compute new hidden states `t = s(ht;Ws) compute new locus for next foveation yt = fd(ht) decode: reconstruct high detail image\nNow the DFAE can handle a sequence of foveations for a given input image, further allowing us to train the model in a more realistic fashion. That is, the human visual system does not have access to all the high detail information at once and must must instead forge visual memories from a sequence of foveations. Thus, to mimick this, rather than trying to reconstruct the original image, we can instead try to reconstruct the foveation at time t from the information available at time t\u2212 1. This is similar to how a language model is trained.\nFor now, we focus on studying the effects of single foveations using the non-recurrent form of the DFAE."}, {"heading": "5 Experiments", "text": "Recall that we are interested in the question of whether an artificial neural network can perceive an image from foveated input images. In the context of autoencoders, the hidden layers h are responsible for representing the foveated inputs x. If the network learns a reasonable representation, then it should be able to produce a higher resolution output y. We can then measure how similar\nthe output of the network is to the original image to evaluate how well the network can perceive. In these experiments, we fix the architecture of our network to the family described in the previous section and vary the type of foveation, the number of hidden units and the number of layers and study the learnt features and reconstruction accuracy. We address the following questions:\n1. Can the network perceive aspects of the image that are not present in the input? What can it perceive and under what conditions?\n2. Can the network perceive color that is not present in the input? Does it need a small fovea of color to do so?\n3. How much capacity is required to perceive missing details? 4. How does the network compensate for foveated inputs? Does the foveation affect the learnt\nfeatures?\nTo answer these questions, we define several foveation functions as described in the following section."}, {"heading": "5.1 Foveation Functions", "text": "In our experiments, we study several different foveation functions (described in more detail in the appropriate sections). In many cases, downampling is employed as part of the foveation function for which we employ the nearest neighbor interpolation algorithm. Nearest neighbor interpolation is a simple sampling algorithm which selects the value of the nearest point and does not consider the values of the neighboring points at all. As an interpolation algorithm, it generates poor quality or blocky images as there is no smoothing function. We picked nearest neighbor as our downsampling algorithm to test the worst case possible downsampled inputs on our system. Foveation functions include:\n\u2022 downsampled factor d (no fovea): no fovea is present, the entire image is uniformly downsampled by a factor of d using the nearest neighbor interopolation method. For example, a factor of 4 transforms a 28x28 image ito a 7x7 image and approximatley 94% of the pixels are removed. Note, in the case of color images, each channel (RGB) is separately downsampled resulting in color distortion. The downsampling factors tested for MNIST were 2, 4 and 7, and for CIFAR100 dataset were 2, 4 and 8. See 2b for examples. \u2022 scotoma r (SCT-R): entire regions (r = 25%, 50% and 75%) of the image are removed (by\nsetting the intensities to 0) to create a blind spot/region, but the rest of the image remains at the original resolution. We experiment with the location of the scotoma (centered or not). \u2022 fovea r (FOV-R): only a small fovea r of high resolution (r = 25% or 6%); the rest of the\nimage is downsampled by a factor of 4. Note that the special case of r = 0% is equivalent to downsampling the entire image uniformly. \u2022 achromatic r (ACH-R): only a region of size r has color; color is removed from the\nperiphery by averaging the RGB channels into a single grayscale channel. \u2022 fovea-achromatic r (FOVA-R): combines the fovea r with the achromatic region: only\nthe foveated region is in color, the rest of the image is in grayscale and downsampled by a factor of 4."}, {"heading": "5.2 Datasets and pre-processing", "text": "We used two datasets in our experiments: MNIST and CIFAR100. The MNIST database consists of 28 x 28 handwritten digits and has a training set of 60,000 examples and a test set of 10,000 examples. Therefore each class has 6000 examples. The CIFAR100 dataset consists of 32 x 32 color images of 100 classes. Some examples of classes are: flowers, large natural outdoor scenes, insects, people, vehicles etc. Each class has 600 examples. The training set consists of 50,000 images and the test set consists of 10,000 images.\nWe trained DFAEs on the MNIST and CIFAR100 dataset (in grayscale and color). We normalized the datasets so that the pixel values are between 0 and 1 and additionally, zero-centered them. This step corresponds to local brightness and contrast normalization. Aside from this step, no other preprocessing such as patch extraction or whitening was applied."}, {"heading": "5.3 Baseline: Comparison with standard interpolation functions", "text": "First, to establish baselines and context for our results, we compare a 1-layer DFAE to common upsampling algorithms found in image editing software. We report mean squared error (MSE) between the reconstructed image and original image to measure the quality of reconstructed images by the interpolation algorithm and the DFAE. An MSE of zero means the algorithm or DFAE is able to reconstruct the input with perfect accuracy. Figure 3 shows the MSE of the interpolation algorithms and a DFAE. Not surprisingly, the nearest neighbor performed the worst reconstruction overall. The bilinear interpolation performed the best in comparison to other upsampling algorithms tested. The interpolation algorithms performed poorly when they reconstructed an image that was downsampled beyond a factor of 2. The error rates produced by these interpolation algorithms on the MNIST dataset is higher than the natual image dataset. Figure 3 show that a single layer DFAE outperforms these standard algorithms for the datasets tested."}, {"heading": "5.4 Feature detectors learnt without any foveations", "text": "Here we experiment with foveation functions in which the size of the fovea is 0; that is, these foveation functions uniformly downsample the original by various factors (the factors are experimentally controlled). The purpose of this experiment is to study how well the network can reconstruct the image when no high-detail input is available. The variables to consider are the number of hidden units per layer and the number of layers. Pilot experiments showed when the number of hidden units was less than the downsampled input size, DFAEs performed very poorly. This is not surprising because autoencoders cannot learn features in an under complete state and the downsampled input contains impoverished features.\nFigure 4 show examples of the reconstructed images by a single layer DFAE. The images produced by the DFAE is compared to upsampled reconstructions by the bilinear algorithm. When compared\nto the bilinear algorithm, DFAEs can correctly extract the contents of a downsampled input even when 94% of pixels are removed. A compelling example is that even when faced with a blank input as seen in Figure 4a the DFAE can correctly predict the digit 1. However the performance of DFAEs suffered when the input was downsampled beyond factor 4. Even though the DFAE made predictions based on the input, most of the reconstructions were incorrect. The reconstructed natural images as seen in Figure 4b show that the DFAE learnt a smoothing and centering function even though it was unable to reconstruct the high frequencies in the images. The DFAEs could predict the shape of objects in the natural images but not the high frequency details.\nNext, we looked at filters or features learnt by the single layer DFAEs as shown in Figure 5. Feature detectors that correspond to the final hidden layer of the network were visualized. Each hidden neuron yj has an associated vector of weights Wj that it uses to compute a dot product with an input example. These weight vectors or filters have the same dimensionality as the input allowing us to visualize them as images, highlighting the aspects of the input to which a hidden unit is sensitive. The goal of visualizing feature detectors was to examine qualitatively the kind of feature detectors learnt from the downsampled images and compare them to those learnt from full-resolution input.\nFor MNIST images, a single layer DFAE learns neuron like features when given the original input. When the input was downsampled, it was forced to learn stroke like features. A curiously similar result was observed by Vincent et al. [19], where their denoising autoencoder learnt global structures when it was trained on corrupted inputs. Our DFAE also learnt increasingly global features when the input is downsampled correspondingly. However the ability to learn useful features deteriorated when the input was downsampled beyond a factor 4. For instance, when given an input downsampled by a factor 7, a majority of the features learnt were superimpositions of two digits and this was reflected in the images reconstructed as shown in Figure 4a. On the other hand, the filters learnt on CIFAR100 images does not look meaningful. In some cases the network learnt a specific color gradients or locally circular blobs which probably enabled it to be better at reconstructing low frquency shape information and landscapes particularly well. Since we did not whiten the images, nor used image patches during training, the noise modeled by the DFAE for natural images was not surprising.\nTo understand how the number of hidden units and layers affect performance of DFAEs, we increased the breadth and/or depth of the DFAE. Figure 6 show that the performance of DFAE does not improve drastically if the network is given additional capacity both in breadth (number of hidden units) and in depth (number of layers). The DFAE error rates stabilized when the number of hidden units was increased beyond 100. Note that the number of hidden units was varied according to the original input size. Therefore for 28 x 28 MNIST images the range of hidden units varied from 800 hidden units (rounded from 784 input size). Similarly, for CIFAR100 images increasing the number\nof hidden units of the DFAE did not improve performance of either CIFAR100 color or grayscale images. A pilot tests with networks upto 4-layers showed that performance on MNIST or CIFAR100 images did not improve significantly with increasing number of layers."}, {"heading": "5.5 Reconstructing foveated inputs", "text": "Until now, we evaluated DFAEs on uniformly downsampled images but this kind of input is unrealistic from those received by the retina. In this section, we evaluate DFAEs on foveated inputs, SCT-R and FOV-R, as described in section 5.1. As discussed in the introduction, the human visual system makes effective use of these kinds of foveated inputs. From a machine learning perspective, it is desirable to recognize or classify images from degraded configurations, which in turn will reduce the need for carefully pruned and preprocessed datasets during training.\nThe rationale for having scotoma-like regions in the input was to test whether the available input contained enough information to reconstruct the rest of the image. The dataset used was grayscale CIFAR100 images. Variable sized areas of region (r = 25%, 50%, 75%, 75% centered) were removed from the original input. The location of removal was chosen randomly from the four quadrants of the input image, except for the condition where 75% of the image around the center was removed. Since a majority of the input images have a subject of interest, we tested if the central region contained enough information to reconstruct the rest of the image.\nThe reconstructions in Figure 7a show the DFAE does not perform well when r > 25%. When r = 50% of the input is removed, the DFAE can reconstruct landscapes and reconstruct shape information and symmetry, demonstrating it\u2019s ability to extract low frequency information. When r = 75% and 75% centered, the reconstruction process breaks down and DFAE cannot predict the input beyond the given region of information. The filters learnt under these conditions look similar to the grayscale version of Figure 5b with bigger smooth blobs over blacked out regions of the input.\nIn FOV-R inputs, r is the same as SCT-R inputs and we chose to use downsampling factor 4 for regions outside the fovea since previous experiments revealed that DFAEs cannot reconstruct inputs downsampled beyond this factor. Figure 7b shows the reconstructed images from FOV-R inputs and Figure 7c show the error rate of reconstruction. The cluster of red lines with lower error rates show that the DFAE performed considerably well with FOV-R than SCT-R inputs The performance was\nbetter ( 1% error for r = 75% centered) than an DFAE trained with uniformly downsampled inputs (1.5% error). This result is not surprising, given that FOV-R contains additional information from regions outside the fovea. These results suggests that a small number of foveations containing rich details might be all these neural networks need to extract contents of the input in higher detail."}, {"heading": "5.6 Reconstructing color from foveated inputs", "text": "It is well known that the human visual system loses chromatic sensitivity towards the periphery of the retina. Recently, there has been interest in how deep networks, specifically convolutional neural networks (CNNs), can learn to color grayscale images [33] and learn artistic style [34]. Specifically in Dahl\u2019s [33] reconstructions from grayscale images, numerous cases of the colorized images produced were muted or sepia colored. The problem of colorization which is inherently ill-posed was treated as a classification task in these studies. Can DFAEs perceive color if it is absent in the input?\nWe investigated this question using ACH-R and FOVA-R inputs described in section 5.1. The regions of color tested were r = 0% or no color, 6%, 25% and 100% or full color. Figure 8a and 8b show examples of color reconstructions of the these input types. When the DFAE is trained with full color ACH-R inputs, it can make mistakes in reconstructing the right color as seen in Figure 8a. For example: it colors the yellow flower as pink and the purplish-red landscape as blue. When the input is grayscale (no color, r = 0%), the colorizations are gray, muted, sepia toned or simply incorrect in the case of landscapes. But if there is a \u201cfovea\u201d of color, the single layer DFAE can reconstruct the colorizations correctly. Ofcourse, if the \u201cfovea\u201d of color is reduced, i.e. 6%, the color reconstruction accuracy falls off but not too drastically. For example, it predicts a yellowish tone for the sunflower among a bed of brown leaves. The critical result is that the performance difference between 100%\nor full colored inputs and \u201cfoveated\u201d color inputs is small as seen in Figure 8c and 8d. These results suggest that color reconstructions can be just as accurate if these networks can figure out the color of one region of the image accurately as opposed to every region in the image. Similar to the human visual system, these networks are capable of determining accurate colors in the periphery if color information is available at foveation."}, {"heading": "6 Conclusions", "text": "The key finding in this paper is that current deep architectures are capable of learning useful features from low fidelity inputs. As discussed in the introduction, the human visual system uses sequential foveations to gather information about their surroundings. In each foveation, only a fraction of the input is in high resolution. We studied the capability of deep networks to learn in the face of minimal information, specifically foveated inputs. Our results indicate a single layer DFAE can reconstruct low fidelity inputs better than existing upsampling algorithms and remarkably, color reconstructions with foveated inputs are just as good with full colored inputs.\nIn general, our model achieves these results using a shallow network with only a small number of hidden units. We investigated how the capacity of the DFAE, in terms of layers and number of hidden units, interacts with foveated inputs. We found that small shallow networks were capable of learning good representation, especially low frequencies in the input. As noted, the performance of the the DFAE was qualitatively better on the MNIST digit images than the natural images. Firstly,\nthe MNIST dataset contains 6000 training examples for each class compared to the CIFAR100 dataset which contains 500 training examples for each class. Secondly, the shape of the digits (a low frequency feature) is prominent in the MNIST dataset but not in the natural images which contains texture, multiple objects, contrast variation adding to the high frequency noise. The noise to signal ratio is lower in MNIST dataset in general which helped DFAE learn better representations.\nColor information is obviously important to the human visual system but our results show that the performance of the DFAE does not improve significantly with color images as seen in Figures 6c and 6d. But color information was important in improving accuracy when the DFAE colorized images from foveated inputs.\nDoes an image (of scene or object) consist of a single or multiple image regions that are predictive of the contents of the image? In this paper, we focused on a single foveated region to test how that specific region was predictive of the rest of the image. Future studies can investigate which regions of an image are most predictive. How many of such regions exist within an image? Do these regions generalize across a class of images? How can we combine these regions to reconstruct the image?\nIn general, foveated inputs enabled the DFAE to learn the best representations overall in terms of image contents and color. This gives us hope that we can learn useful feature representations when full resolution input is not available and with a small computational budget. In future work, we want to study models that can make shifts of attention to improve the representation on demand as needed for the associated task."}], "references": [{"title": "Imagenet classification with deep convolutional neural networks. In Advances in neural information processing", "author": ["Alex Krizhevsky", "Ilya Sutskever", "Geoffrey E Hinton"], "venue": null, "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2012}, {"title": "The cost of cortical computation", "author": ["Peter Lennie"], "venue": "Current biology,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2003}, {"title": "How much the eye tells the brain", "author": ["Kristin Koch", "Judith McLean", "Ronen Segev", "Michael A Freed", "Michael J Berry", "Vijay Balasubramanian", "Peter Sterling"], "venue": "Current Biology,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2006}, {"title": "Retinal detachment", "author": ["Ronald G Michels", "TA CP Rice"], "venue": "St. Louis: CV Mosby,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 1990}, {"title": "Human cortical magnification factor and its relation to visual acuity", "author": ["A Cowey", "ET Rolls"], "venue": "Experimental Brain Research,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 1974}, {"title": "Some characteristics of peripheral visual performance", "author": ["Frank N Low"], "venue": "American Journal of Physiology\u2013 Legacy Content,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 1946}, {"title": "Color perception in the intermediate periphery of the visual field", "author": ["Thorsten Hansen", "Lars Pracejus", "Karl R Gegenfurtner"], "venue": "Journal of Vision,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2009}, {"title": "Face recognition by humans: Nineteen results all computer vision researchers should know about", "author": ["Pawan Sinha", "Benjamin Balas", "Yuri Ostrovsky", "Richard Russell"], "venue": "Proceedings of the IEEE,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 1962}, {"title": "Recognition memory for a rapid sequence of pictures", "author": ["Mary C Potter", "Ellen I Levy"], "venue": "Journal of experimental psychology,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 1969}, {"title": "Atoms of recognition in human and computer vision", "author": ["Shimon Ullman", "Liav Assif", "Ethan Fetaya", "Daniel Harari"], "venue": "Proceedings of National Academy of Sciences, page in press,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2016}, {"title": "Learning to combine foveal glimpses with a third-order boltzmann machine", "author": ["Hugo Larochelle", "Geoffrey E Hinton"], "venue": "In Advances in neural information processing systems,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2010}, {"title": "Recurrent models of visual attention", "author": ["Volodymyr Mnih", "Nicolas Heess", "Alex Graves", "koray kavukcuoglu"], "venue": "Advances in Neural Information Processing Systems", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2014}, {"title": "Neural machine translation by jointly learning to align and translate", "author": ["Dzmitry Bahdanau", "Kyunghyun Cho", "Yoshua Bengio"], "venue": "CoRR, abs/1409.0473,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2014}, {"title": "Show, attend and tell: Neural image caption generation with visual attention", "author": ["Kelvin Xu", "Jimmy Ba", "Ryan Kiros", "Kyunghyun Cho", "Aaron C. Courville", "Ruslan Salakhutdinov", "Richard S. Zemel", "Yoshua Bengio"], "venue": null, "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2015}, {"title": "Where to look next? eye movements reduce local uncertainty", "author": ["Laura Walker Renninger", "Preeti Verghese", "James Coughlan"], "venue": "Journal of Vision,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2007}, {"title": "Extracting and composing robust features with denoising autoencoders", "author": ["Pascal Vincent", "Hugo Larochelle", "Yoshua Bengio", "Pierre-Antoine Manzagol"], "venue": "In Proceedings of the 25th International Conference on Machine Learning,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2008}, {"title": "Stacked denoising autoencoders: Learning useful representations in a deep network with a local denoising criterion", "author": ["Pascal Vincent", "Hugo Larochelle", "Isabelle Lajoie", "Yoshua Bengio", "Pierre-Antoine Manzagol"], "venue": "The Journal of Machine Learning Research,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2010}, {"title": "Efficient learning of sparse representations with an energy-based model", "author": ["Christopher Poultney", "Sumit Chopra", "Yann L Cun"], "venue": "In Advances in neural information processing systems,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2006}, {"title": "Neural networks and principal component analysis: Learning from examples without local minima", "author": ["Pierre Baldi", "Kurt Hornik"], "venue": "Neural networks,", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 1989}, {"title": "Nonlinear autoassociation is not equivalent to pca", "author": ["Nathalie Japkowicz", "Stephen Jose Hanson", "Mark Gluck"], "venue": "Neural computation,", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2000}, {"title": "Backpropagation applied to handwritten zip code recognition", "author": ["Yann LeCun", "Bernhard Boser", "John S Denker", "Donnie Henderson", "Richard E Howard", "Wayne Hubbard", "Lawrence D Jackel"], "venue": "Neural computation,", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 1989}, {"title": "Training with noise is equivalent to tikhonov regularization", "author": ["Chris M Bishop"], "venue": "Neural computation,", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 1995}, {"title": "Contractive autoencoders: Explicit invariance during feature extraction", "author": ["Salah Rifai", "Pascal Vincent", "Xavier Muller", "Xavier Glorot", "Yoshua Bengio"], "venue": "In Proceedings of the 28th International Conference on Machine Learning", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2011}, {"title": "Image denoising and inpainting with deep neural networks", "author": ["Junyuan Xie", "Linli Xu", "Enhong Chen"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "26", "shortCiteRegEx": "26", "year": 2012}, {"title": "Scholkopf. A machine learning approach for non-blind image deconvolution", "author": ["Christian J Schuler", "Harold Christopher Burger", "Stefan Harmeling", "Bernhard"], "venue": "In Computer Vision and Pattern Recognition (CVPR),", "citeRegEx": "27", "shortCiteRegEx": "27", "year": 2013}, {"title": "Natural image denoising with convolutional networks", "author": ["Viren Jain", "Sebastian Seung"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "28", "shortCiteRegEx": "28", "year": 2009}, {"title": "Deep network cascade for image super-resolution", "author": ["Zhen Cui", "Hong Chang", "Shiguang Shan", "Bineng Zhong", "Xilin Chen"], "venue": "In Computer Vision\u2013ECCV", "citeRegEx": "29", "shortCiteRegEx": "29", "year": 2014}, {"title": "Learning a deep convolutional network for image super-resolution", "author": ["Chao Dong", "Chen Change Loy", "Kaiming He", "Xiaoou Tang"], "venue": "In Computer Vision\u2013ECCV", "citeRegEx": "30", "shortCiteRegEx": "30", "year": 2014}, {"title": "Learning iterative image reconstruction in the neural abstraction pyramid", "author": ["Sven Behnke"], "venue": "International Journal of Computational Intelligence and Applications,", "citeRegEx": "31", "shortCiteRegEx": "31", "year": 2001}, {"title": "Adaptive subgradient methods for online learning and stochastic optimization", "author": ["John Duchi", "Elad Hazan", "Yoram Singer"], "venue": "The Journal of Machine Learning Research,", "citeRegEx": "32", "shortCiteRegEx": "32", "year": 2011}, {"title": "Automatic colorization. http://tinyclouds.org/colorize", "author": ["Ryan Dahl"], "venue": null, "citeRegEx": "33", "shortCiteRegEx": "33", "year": 2016}, {"title": "A neural algorithm of artistic style", "author": ["Leon A Gatys", "Alexander S Ecker", "Matthias Bethge"], "venue": "arXiv preprint arXiv:1508.06576,", "citeRegEx": "34", "shortCiteRegEx": "34", "year": 2015}], "referenceMentions": [{"referenceID": 0, "context": "A major appeal of deep learning, on which the current dominant approaches for machine vision tasks are based [1], is that they can automatically learn useful feature representations from the data.", "startOffset": 109, "endOffset": 112}, {"referenceID": 1, "context": "In contrast, the human visual system has only a small fovea of high resolution chromatic input allowing it to more judiciously budget computational resources [2].", "startOffset": 158, "endOffset": 161}, {"referenceID": 2, "context": "The human retina receives 10 million bits per second which exceeds the computational resources available to our visual system to assimilate at any given time [3].", "startOffset": 158, "endOffset": 161}, {"referenceID": 3, "context": "To further put this question in perspective, our own fovea takes up only 4% of the entire retina [4] and is solely responsible for sharp central full color vision with maximum acuity.", "startOffset": 97, "endOffset": 100}, {"referenceID": 4, "context": "The relative visual acuity diminishes rapidly with eccentricity from the fovea [5].", "startOffset": 79, "endOffset": 82}, {"referenceID": 5, "context": "As a result, visual performance is best at the fovea and progressively worse towards the periphery [6].", "startOffset": 99, "endOffset": 102}, {"referenceID": 6, "context": "Indeed, our visual cortex is receiving distorted color-deprived visual input except for the central two degrees of the visual field [7] as seen in Figure 1.", "startOffset": 132, "endOffset": 135}, {"referenceID": 7, "context": "For instance, our system can recognize faces and emotions expressed by those faces in resolutions as low as 16 x 16 pixels [8].", "startOffset": 123, "endOffset": 126}, {"referenceID": 8, "context": "We can reliably extract contents of a scene from the gist of an image [9] even at low resolutions [10, 11].", "startOffset": 98, "endOffset": 106}, {"referenceID": 9, "context": "[12] has shown that our visual system is capable of recognizing contents of images from critical feature configurations (called minimal recognizable images or MIRCs) that current deep learning systems cannot utilize for similar tasks.", "startOffset": 0, "endOffset": 4}, {"referenceID": 10, "context": "There has been a revival in applying the idea of attention to deep learning architectures [13, 14, 15, 16].", "startOffset": 90, "endOffset": 106}, {"referenceID": 11, "context": "There has been a revival in applying the idea of attention to deep learning architectures [13, 14, 15, 16].", "startOffset": 90, "endOffset": 106}, {"referenceID": 12, "context": "There has been a revival in applying the idea of attention to deep learning architectures [13, 14, 15, 16].", "startOffset": 90, "endOffset": 106}, {"referenceID": 13, "context": "There has been a revival in applying the idea of attention to deep learning architectures [13, 14, 15, 16].", "startOffset": 90, "endOffset": 106}, {"referenceID": 12, "context": "Such work is exciting and has lead to improvements in tasks ranging from machine translation [15] to image captioning [16].", "startOffset": 93, "endOffset": 97}, {"referenceID": 13, "context": "Such work is exciting and has lead to improvements in tasks ranging from machine translation [15] to image captioning [16].", "startOffset": 118, "endOffset": 122}, {"referenceID": 14, "context": "Unlike these systems, humans perceive by sequentially directing attention to relevant portions of the data and in turn enables our visual system to reduce computational costs [17, 3].", "startOffset": 175, "endOffset": 182}, {"referenceID": 2, "context": "Unlike these systems, humans perceive by sequentially directing attention to relevant portions of the data and in turn enables our visual system to reduce computational costs [17, 3].", "startOffset": 175, "endOffset": 182}, {"referenceID": 15, "context": "In contrast to traditional or de-noising autoencoders [18], which attempt to reconstruct the original image (or respectively, a salt and pepper corrupted version), our autoencoders", "startOffset": 54, "endOffset": 58}, {"referenceID": 16, "context": "To capture the structure of the underlying data distribution and prevent the autoencoder from learning the identity function, we can either require the hidden layer to have lower dimensionality than the input or regularize the weights [19].", "startOffset": 235, "endOffset": 239}, {"referenceID": 17, "context": "The lower dimension constraint is what the classical autoencoder or PCA does while the higher dimension is used by the sparse autoencoders [20].", "startOffset": 139, "endOffset": 143}, {"referenceID": 16, "context": "Recently, denoising autoencoders have been shown to regularize the network by adding noise such as salt-and-pepper (SP) noise to the input, thus forcing the model to learn to predict the true image from its noisy representation [19].", "startOffset": 228, "endOffset": 232}, {"referenceID": 18, "context": "As mentioned above, it has been shown that the features learnt by the encoder without any nonlinearity are a subspace of the principal components of the input space [21].", "startOffset": 165, "endOffset": 169}, {"referenceID": 19, "context": "However, when a nonlinear activation such as a sigmoid is used in the encoder, an AE can learn more powerful feature detectors than a simple PCA [22].", "startOffset": 145, "endOffset": 149}, {"referenceID": 20, "context": "Using noisy or jittered inputs to understand feature learning in the framework of AEs or MLPs has been explored before [23, 19].", "startOffset": 119, "endOffset": 127}, {"referenceID": 16, "context": "Using noisy or jittered inputs to understand feature learning in the framework of AEs or MLPs has been explored before [23, 19].", "startOffset": 119, "endOffset": 127}, {"referenceID": 16, "context": "[19] first proposed training autoencoders with corrupted", "startOffset": 0, "endOffset": 4}, {"referenceID": 21, "context": "In fact, Bishop [24] has argued that in a linear system training with noise has a similar effect as training with a regularizer, such as an L2 weight decay.", "startOffset": 16, "endOffset": 20}, {"referenceID": 22, "context": "[25].", "startOffset": 0, "endOffset": 4}, {"referenceID": 23, "context": "[26] presented an approach to remove noise from corrupted inputs using sparse coding and deep networks pre-trained with DAEs.", "startOffset": 0, "endOffset": 4}, {"referenceID": 24, "context": "Along the same lines, post deblurring denoising [27] and using convolutional neural networks for natural image denoising of patterns such as specks, dirt and rain has been investigated [28].", "startOffset": 48, "endOffset": 52}, {"referenceID": 25, "context": "Along the same lines, post deblurring denoising [27] and using convolutional neural networks for natural image denoising of patterns such as specks, dirt and rain has been investigated [28].", "startOffset": 185, "endOffset": 189}, {"referenceID": 26, "context": "[29] used low resolution images interpolated to the size of the output image and AEs in their pipeline to restore resolution of these images.", "startOffset": 0, "endOffset": 4}, {"referenceID": 27, "context": "[30] improves on Cui et al.", "startOffset": 0, "endOffset": 4}, {"referenceID": 26, "context": "[29] by using convolutional neural networks and with an end-to-end system.", "startOffset": 0, "endOffset": 4}, {"referenceID": 28, "context": "[31] demonstrated that difficult non-linear image reconstruction from low resolution inputs can be learnt by hiearchical recurrent networks.", "startOffset": 0, "endOffset": 4}, {"referenceID": 0, "context": "For example, if we encode images as vectors of floats between 0 and 1 (reflecting pixel intensities in RGB or grayscale) then we might define a class of foveation functions as \u03c6 : [0, 1] \u2192 [0, 1] s.", "startOffset": 180, "endOffset": 186}, {"referenceID": 0, "context": "For example, if we encode images as vectors of floats between 0 and 1 (reflecting pixel intensities in RGB or grayscale) then we might define a class of foveation functions as \u03c6 : [0, 1] \u2192 [0, 1] s.", "startOffset": 189, "endOffset": 195}, {"referenceID": 0, "context": "We then employ the autoencoder to defoveate x\u0302 by generating a high-quality output image y = f(x\u0302;W ) in which, for example, y \u2208 [0, 1].", "startOffset": 129, "endOffset": 135}, {"referenceID": 15, "context": "Thus, as an homage to denoising autoencoders [18], we have termed these models defoveating autoencoders or DFAEs.", "startOffset": 45, "endOffset": 49}, {"referenceID": 29, "context": "1] and loss was minimized by stochastic gradient descent with adagrad updates [32].", "startOffset": 78, "endOffset": 82}, {"referenceID": 11, "context": "However, we remark that it is straightforward to augment DFAEs with recurrent connections to handle a sequence of foveations similar to what has has been done for solving classification tasks with attention [14].", "startOffset": 207, "endOffset": 211}, {"referenceID": 16, "context": "[19], where their denoising autoencoder learnt global structures when it was trained on corrupted inputs.", "startOffset": 0, "endOffset": 4}, {"referenceID": 30, "context": "Recently, there has been interest in how deep networks, specifically convolutional neural networks (CNNs), can learn to color grayscale images [33] and learn artistic style [34].", "startOffset": 143, "endOffset": 147}, {"referenceID": 31, "context": "Recently, there has been interest in how deep networks, specifically convolutional neural networks (CNNs), can learn to color grayscale images [33] and learn artistic style [34].", "startOffset": 173, "endOffset": 177}, {"referenceID": 30, "context": "Specifically in Dahl\u2019s [33] reconstructions from grayscale images, numerous cases of the colorized images produced were muted or sepia colored.", "startOffset": 23, "endOffset": 27}], "year": 2016, "abstractText": "Humans perceive their surroundings in great detail even though most of our visual field is reduced to low-fidelity color-deprived (e.g. dichromatic) input by the retina. In contrast, most deep learning architectures are computationally wasteful in that they consider every part of the input when performing an image processing task. Yet, the human visual system is able to perform visual reasoning despite having only a small fovea of high visual acuity. With this in mind, we wish to understand the extent to which connectionist architectures are able to learn from and reason with low acuity, distorted inputs. Specifically, we train autoencoders to generate full-detail images from low-detail \u201cfoveations\u201d of those images and then measure their ability to reconstruct the full-detail images from the foveated versions. By varying the type of foveation, we can study how well the architectures can cope with various types of distortion. We find that the autoencoder compensates for lower detail by learning increasingly global feature functions. In many cases, the learnt features are suitable for reconstructing the original fulldetail image. For example, we find that the networks accurately perceive color in the periphery, even when 75% of the input is achromatic.", "creator": "LaTeX with hyperref package"}}}