{"id": "1604.04835", "review": {"conference": "AAAI", "VERSION": "v1", "DATE_OF_SUBMISSION": "17-Apr-2016", "title": "SSP: Semantic Space Projection for Knowledge Graph Embedding with Text Descriptions", "abstract": "Knowledge graph embedding represents the entities and relations as numerical vectors, and then knowledge analysis could be promoted as a numerical method. So far, most methods merely concentrate on the fact triples that are composed by the symbolic entities and relations, while the textual information which is supposed to be most critical in NLP could hardly play a reasonable role. For this end, this paper proposes the method SSP which jointly learns from the symbolic triples and textual descriptions. Our model could interact both two information sources by characterizing the correlations, by which means, the textual descriptions could make effects to discover semantic relevance and offer precise semantic embedding. Extensive experiments show our method achieves the substantial improvements against the state-of-the-art baselines on the tasks of knowledge graph completion and entity classification. We also show how the structure of the model can be improved, and thus provides a clear and explicit understanding of the potential limitations of our approach. Moreover, if we are not already in agreement with the limitations of this model, we can easily find better ways of creating an explicit and explicit representation for the information to be generated and generated and applied.\n\n\nThe work was first presented by JK and C.T. G. Arieux, from the Department of Philosophy of Philosophy of Philosophy and Applied Philosophy, and of C.T. G. Arieux, from the Department of Philosophy and Applied Philosophy, and of C.T. G. Arieux, from the Department of Philosophy and Applied Philosophy, and of C.T. G. Arieux, from the Department of Philosophy and Applied Philosophy, and of C.T. G. Arieux, from the Department of Philosophy and Applied Philosophy, and of C.T. G. Arieux, from the Department of Philosophy and Applied Philosophy, and of C.T. G. Arieux, from the Department of Philosophy and Applied Philosophy, and of C.T. G. Arieux, from the Department of Philosophy and Applied Philosophy, and of C.T. G. Arieux, from the Department of Philosophy and Applied Philosophy, and of C.T. G. Arieux, from the Department of Philosophy and Applied Philosophy, and of C.T. G. Arieux, from the Department of Philosophy and Applied Philosophy, and of C.T. G. Arieux, from the Department of Philosophy and Applied Philosophy, and of C.T. G. Arieux, from the Department of Philosophy and", "histories": [["v1", "Sun, 17 Apr 2016 07:15:33 GMT  (79kb,D)", "https://arxiv.org/abs/1604.04835v1", "Submitted to ACL.2016"], ["v2", "Tue, 25 Oct 2016 02:39:15 GMT  (82kb,D)", "http://arxiv.org/abs/1604.04835v2", "Submitted to AAAI.2017"], ["v3", "Sat, 17 Jun 2017 04:33:41 GMT  (84kb,D)", "http://arxiv.org/abs/1604.04835v3", "Submitted to AAAI.2017"]], "COMMENTS": "Submitted to ACL.2016", "reviews": [], "SUBJECTS": "cs.CL cs.LG", "authors": ["han xiao 0005", "minlie huang", "lian meng", "xiaoyan zhu"], "accepted": true, "id": "1604.04835"}, "pdf": {"name": "1604.04835.pdf", "metadata": {"source": "META", "title": "SSP: Semantic Space Projection for Knowledge Graph Embedding with Text Descriptions", "authors": ["Han Xiao", "Minlie Huang", "Lian Meng", "Xiaoyan Zhu"], "emails": ["bookman@vip.163.com;", "aihuang@tsinghua.edu.cn;", "zxy-dcs@tsinghua.edu.cn;", "mengl15@foxmail.com;"], "sections": [{"heading": "Introduction", "text": "Knowledge graph provides an effective basis for NLP in many tasks such as question answering, web search and semantic analysis. In order to provide a numerical computation framework for knowledge graph, knowledge graph embedding projects the entities and relations to a continuous low-dimensional vector space. More specifically, a fact in knowledge graph is usually represented as a symbolic triple (h, r, t), while knowledge graph embedding attempts to represent the symbols with vectors, say h, r, t. To this end, a number of embedding methods have been proposed, such as TransE (Bordes et al. 2013), PTransE (Lin, Liu, and Sun 2015), KG2E (He et al. 2015), etc.\nAs a key branch of embedding models, the translationbased methods adopt the principle of translating the head entity to the tail one by a relation-specific vector, or formally as h + r = t. As Fig.1 shows, in the knowledge graph, the entities such as h, t have textual descriptions, which contain much supplementary semantic information about knowledge triples.\nCopyright c\u00a9 2017, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved.\nDespite the success of conventional knowledge graph embedding models, there are still two reasons why textual descriptions would be necessary in this task: discovering semantic relevance and offering precise semantic expression.\nFirstly, the semantic relevance between entities is capable to recognize the true triples, which are difficult to be inferred only with fact triples. For example, the triple (Anna Roosevelt, Parents, Franklin Roosevelt), indicates \u201cFranklin Roosevelt\u201d is the parent of \u201cAnna Roosevelt\u201d. However, it\u2019s quite difficult to infer this fact merely from other symbolic triples. In contrast, in the textual description of the head entity, there are many keywords such as \u201cRoosevelt\u201d and \u201cDaughter of the President\u201d, which may infer the fact triple easily. Specifically, we measure the possibility of a triple by projecting the loss onto a hyperplane that represents the semantic relevance between entities. Thus, it is always possible to accept a fact triple so long as the l2-norm of the projected loss vector onto the semantic hyperplane is sufficiently small.\nSecondly, precise semantic expression could promote the discriminative ability between two triples. For instance, when we query about the profession of \u201cDaniel Sturgeon\u201d, there are two possible candidates: \u201cpolitician\u201d and \u201clawyer\u201d. It\u2019s hard to distinguish if only focusing on the symbolic triples. However, the textual description of \u201cDaniel Sturgeon\u201d is full of politics-related keywords such as \u201cDemocratic Party\u201d, \u201cState Legislature\u201d etc. and even \u201cPolitician\u201d. The textual descriptions help to refine the topic of \u201cDaniel Sturgeon\u201d in a more precise way from the social celebrities to the government officers, which makes the true answer \u201cpolitician\u201d more preferable. Formally, even though the loss vectors of the two facts are almost of equal norm, after respectively projected onto the \u201cpolitician\u201d and \u201clawyer\u201d related semantic hyperplanes, the losses\nar X\niv :1\n60 4.\n04 83\n5v 3\n[ cs\n.C L\n] 1\n7 Ju\nn 20\n17\nare distinguished reasonably. In this way, precise semantic expression refines the embedding.\nThe existing embedding methods with textual semantics such as DKRL (Xie et al. 2016) and \u201cJointly\u201d (Zhong et al. 2015), have achieved much success. But there is still an issue to be addressed, the weak-correlation modeling issue that current models could hardly characterize the strong correlations between texts and triples. In DKRL, for a triple, the embedding vector of the head entity is translated to that of the tail one as well as possible, and then the vectors of texts and entities are concatenated as the final embedding vectors. While in the \u201cJointly\u201d model the authors attempt to generate coherent embeddings of the corresponding entity and text. Both DKRL and \u201cJointly\u201d apply first-order constraints which are weak in capturing the correlation of texts and triples. It\u2019s noteworthy that triple embedding is always the main procedure and textual descriptions must interact with triples for better embedding. Only in this way, the semantic effects could make more senses. For the above example of \u201cDaniel Sturgeon\u201d, the textual descriptions imply two candidate answers \u201cBanker\u201d and \u201cPolitician\u201d. Thus, by considering both triples and texts, we can obtain the true answer. Therefore, we focus on the stronger semantic interaction by projecting triple embedding onto a semantic subspace such as hyperplane, as shown in Fig.2. Mathematically, the quadratic constraint is adopted to model the strong correlation, thus the embedding topologies are sufficiently semantics-specific.\nWe evaluate the effectiveness of our model Semantic Subspace Projection (SSP) with two tasks on three benchmark datasets that are the subsets of Wordnet (Miller 1995) and Freebase (Bollacker et al. 2008). Experimental results on the datasets show that our model consistently outperforms the other baselines with remarkable improvement.\nContributions. We propose a knowledge graph embedding method SSP which models the strong correlations between the symbolic triples and the textual descriptions by performing the embedding process in a semantic subspace. Besides, our method outperforms all the baselines on the tasks of knowledge graph completion and entity classification, which justifies the effectiveness of our proposal."}, {"heading": "Related Work", "text": "We have surveyed the previous studies and categorized the embedding methods into two branches: Triple-only Embedding models that use only symbolic triples and \u201cTextAware\u201d Embedding models that employ textual descriptions."}, {"heading": "Triple-only Embedding Models", "text": "TransE (Bordes et al. 2013) is a pioneering work for this branch, which translates the head entity to the tail one by the relation vector, or h + r = t. Naturally, the L2 norm of the loss vector is the score function, which measures the plausibility of a triple and a smaller score is better.\nThe following variants transform entities into different subspaces. ManifoldE (Xiao, Huang, and Zhu 2016a) opens a classic branch, where a manifold is applied for the translation. TransH (Wang et al. 2014b) utilizes the relationspecific hyperplane to lay the entities. TransR (Lin et al. 2015) applies the relation-related matrix to rotate the embedding space. Similar researches include TransG (Xiao, Huang, and Zhu 2016b), TransD (Ji et al. ), (Feng et al. 2016) and TransM (Fan et al. 2014).\nFurther researches encode additional structural information into embedding. PTransE (Lin, Liu, and Sun 2015) is a path-based model, simultaneously considering the information and confidence level of a path in the knowledge graph. (Wang, Wang, and Guo 2015) incorporate the rules to restrict the embeddings for the complex relation types such as 1-N, N-1 and N-N. SSE (Guo et al. 2015) aims at discovering the geometric structure of embedding topologies and then based on these assumptions, designs a semantically smoothing score function. Also, KG2E (He et al. 2015) involves probabilistic analysis to characterize the uncertainty concepts of knowledge graph. There are also some other works such as SE (Bordes et al. 2011), LFM (Jenatton et al. 2012), NTN (Socher et al. 2013) and RESCAL (Nickel, Tresp, and Kriegel 2011), TransA (Xiao et al. 2015), etc."}, {"heading": "Text-aware Embedding Models", "text": "\u201cText-Aware\u201d Embedding, which attempts to representing knowledge graph with textual information, generally dates back to NTN (Socher et al. 2013). NTN makes use of entity name and embeds an entity as the average word embedding vectors of the name. (Wang et al. 2014a) attempts to aligning the knowledge graph with the corpus then jointly conducting knowledge embedding and word embedding. However, the necessity of the alignment information limits this method both in performance and practical applicability. Thus, (Zhong et al. 2015) proposes the \u201cJointly\u201d method that only aligns the freebase entity to the corresponding wiki-page. DKRL (Xie et al. 2016) extends the translationbased embedding methods from the triple-specific one to the \u201cText-Aware\u201d model. More importantly, DKRL adopts a CNN-structure to represent words, which promotes the expressive ability of word semantics. Generally speaking, by jointly modeling knowledge and texts, text-aware embedding models obtains the state-of-the-art performance."}, {"heading": "Methodology", "text": "In this section, we first introduce our model and then provide two different perspective to address the advantages of our model. First of all, let us introduce some notations: all the symbols h, t indicate the head and tail entity, respectively. h (or t) is the embedding of the entity from the triples, sh (or st) is the semantic vector generated from the texts, and d is the dimension of embedding.\nThe data involved in our model are the knowledge triples and the textual descriptions of entities. In experiments, we adopt the \u201centity descriptions\u201d of Freebase and the textual definitions of Wordnet as textual information."}, {"heading": "Model Description", "text": "Previous analysis in the introduction suggests to characterize the strong correlation between triples and texts. For the purpose of interacting between the symbolic triples and textual descriptions, this paper attempts to restrict the embedding procedure of a specific triple in the semantic subspace. Specifically, we leverage a hyperplane with normal vector s . = S(sh, st) as the subspace, where S : R2d 7\u2192 Rd is\nthe semantic composition function which will be discussed in the next section, and sh, st is the head-specific and tailspecific semantic vectors, respectively.\nThe score function in the translation-based methods is ||h + r\u2212 t||22, which means the triple embedding focuses on the loss vector e .= h + r\u2212 t . According to our motivation, assuming e is length-fixed, the target is to maximize the component inside the hyperplane, which is ||e\u2212 s>es||22. In detail, the component of the loss in the normal vector direction is (s>es), then the other orthogonal one, that is inside the hyperplane, is (e\u2212 s>es).\nIt\u2019s natural that the norm of the loss vector should also be constrained. To this end, we introduce a factor \u03bb to balance the two parts, formally as:\nfr(h, t) = \u2212\u03bb||e\u2212 s>es||22 + ||e||22 where \u03bb is a suitable hyper-parameter. Moreover, a smaller score means the triple is more plausible. For clarity, the definitions of the symbols are boxed. Notably, the projection part in our score function is negative, so smaller value means less loss.\nSemantic Vector Generation There are at least two methods that could be leveraged to generate the semantic vectors: topic model (Blei 2012) such as LSA, LDA, NMF (Stevens et al. 2012) and word embedding such as CBOW (Mikolov, Yih, and Zweig 2013), Skip-Gram (Mikolov et al. 2013). More concretely, this paper adopts the topic model, treating each entity description as a document and then obtains the topic distribution of document as the semantic vector of entity. The entities are usually organized by the topic in knowledge base, for example, \u201centity type\u201d is used in Freebase to categorize entities. Therefore, we conjecture that the topic model could be more suitable. Notably, the word embedding would also work well though maybe not better.\nGiven the pre-trained semantic vectors, our model fixes them and then optimizes the other parameters. We call this setting Standard (short as Std.). The reason why we could not adapt all the parameters, is that the training procedure would refill the semantic vectors and flush the semantics out. For the purpose of jointly learning the semantics and the embeddings, we conduct the topic model and the embedding model, simultaneously. In this way, the symbolic triples also impose a positive effect on the textual semantics and we call this setting Joint.\nAs each component of a semantic vector indicates the relevant level to a topic, we suggest the semantic composition should take the addition form:\nS(sh, st) = sh + st ||sh + st||22\nwhere the normalization is applied to make a normal vector. Since the largest components represent the topics, the addition operator corresponds to the union of topics, making the composition indicate the entire semantics. For example, when sh = (0.1, 0.9, 0.0) and st = (0.8, 0.0, 0.2), the topic of the head entity is #2 and that of the tail is #1, while the composition is s = (0.45, 0.45, 0.10), corresponding to the topic of #1,#2, which is accordant to our intuition."}, {"heading": "Correlation Perspective", "text": "Specifically, our model attempts to lay the loss h\u2032 \u2212 t onto the hyperplane, where h\u2032 .= h + r is the translated head entity. Mathematically, if a line lies on a hyperplane, so do all the points of this line. Correspondingly, the loss lays on the hyperplane, implying the head and tail entity also lay on it, as the beginning and ending point. Thus, there exists the important restriction, that the entities co-occur in a triple should be embedded in the semantic space composed by the associated textual semantics. This restriction is implemented as a quadric form to characterize the strong correlation between texts and triples, in other words, to interact with both the information sources. A strong interaction between the textual descriptions and symbolic triples complements each other in a more semantics-specific form, which guarantees the semantic effects. More concretely, the embeddings are decided in the training procedure not only by triples but also by textual semantics, based on which, our embedding topologies are semantically different from the other methods."}, {"heading": "Semantic Perspective", "text": "There are two semantic effects for textual descriptions: discovering semantic relevance and offering precise semantic expression. Our model characterizes the strong correlations with a semantic hyperplane, which is capable of taking the advantages of both two semantic effects.\nFirstly, according to the correlation perspective, the entities which are semantically relevant, approximately lay on a consistent hyperplane. Therefore, the loss vector between them (h\u2032 \u2212 t) is also around the hyperplane. Based on this geometric insight, when a head entity matches a negative tail, the triple is far from the hyperplane, making a large loss to be classified. Conversely, even if a correct triple makes\nmuch loss, the score function after projected onto the hyperplane could be relatively smaller (or better). By this mean, the semantic relevance achieved from the texts, promotes embedding. For instance, the fact triple (Portsmouth Football Club, Locate, Portsmouth) could hardly be inferred only within the triple embedding. It ranks 11,549 out of 14,951 by TransE in link prediction, which means a totally impossible fact. But the keywords \u201cPortsmouth\u201d, \u201cEngland\u201d, and \u201cFootball\u201d occur many times in both the textual descriptions, making the two entities semantically relevant. Unsurprisingly, after the semantic projection, the case ranks 65 out of 14,951 in our model, which is much more plausible.\nSecondly, all the equal-length loss vectors 1 in TransE are equivalent in term of discrimination since the score function of TransE is ||h + r\u2212 t||22, which leads to the weak distinction. However, with textual semantics, the discriminative ability could be strengthened in our model. Specifically, the equal-length loss vectors are measured with the projection onto the corresponding semantic hyperplanes, which makes a reasonable division of the losses. For an instance of the query about which film \u201cJohn Powell\u201d contributes to, there are two candidate entities, that the true answer \u201cKung Fu Panda\u201d and the negative one \u201cTerminator Salvation\u201d. Without textual semantics, it\u2019s difficult to discriminate, thus the losses calculated by TransE are 8.1 and 8.0, respectively, leading to a hard decision. Diving into the textural semantics, we discover, \u201cJohn Powell\u201d is much relevant to the topic of \u201cAnimated Films\u201d, which matches that of \u201cKung Fu Panda\u201d and does not for the other. Based on this fact, both the query and the true answer lie in the \u201cAnimated Films\u201ddirected hyperplane, whereas the query and the negative one do not co-occur in the corresponding associated semantic hyperplane. Thus, the projected loss of the true answer could be much less than that of the false one. Concretely, the losses in our model are 8.5 and 10.8, respectively, which are sufficient for discrimination."}, {"heading": "Objectives & Training", "text": "There are two parts in the objective function, which are respectively embedding-specific and topic-specific. To balance the two parts, a hyper-parameter \u00b5 is introduced. Overall, the total loss is:\nL = Lembed + \u00b5Ltopic (1)\nNotably, there is only the first part in the Standard setting where \u00b5 = 0 in fact.\nIn term of the embedding-related objective, the rankbased hinge loss is applied, which means to maximize the discriminative margin between the golden triples and the negative ones:\nL embed\n= \u2211\n(h, r, t) \u2208 \u2206 (h\u2032, r\u2032, t\u2032) \u2208 \u2206\u2032\n[fr\u2032(h \u2032, t\u2032)\u2212 fr(h, t) + \u03b3]+\nwhere \u2206 is the set of golden triples and \u2206\u2032 is that of the negative ones. \u03b3 is the margin, and [ \u00b7 ]+ = max( \u00b7 , 0) is the hinge loss. The false triples are sampled with \u201cBernoulli\n1Two vectors have equal-length iff they have the same l2 norm.\nSampling Method\u201d as introduced in (Wang et al. 2014b) and the method selects the negative samples from the set of\n{(h\u2032, r, t)|h\u2032 \u2208 E} \u222a {(h, r, t\u2032)|t\u2032 \u2208 E} \u222a {(h, r\u2032, t)|r\u2032 \u2208 R}\nWe initialize the embedding vectors by the similar methods used in the deep neural network (Glorot and Bengio 2010) and pre-train the topic model with Non-negative Matrix Factorization (NMF) (Stevens et al. 2012). The stochastic gradient descent algorithm (SGD) is adopted in the optimization.\nFor the topic-related objective, we take the advantage of the NMF Topic Model (Stevens et al. 2012), which is both simple and effective. Then we re-write the target as an innerproduct form with the L2-loss, stated as:\nLtopic = \u2211\ne\u2208E, w\u2208De\n(Ce,w \u2212 s>e w)2 (2)\nse \u2265 0,w \u2265 0 (3)\nwhere E is the set of entities, and De is the set of words in the description of entity e. Ce,w is the times of the word w occurring in the description e. se is the semantic vector of entity e and w is the topic distribution of word w. Similarly, SGD is applied in the optimization.\nTheoretically, our computation complexity is comparable to TransE, asO(\u03bd\u00d7O(TransE)), and the small constant \u03bd is caused by the projection operation and topic calculation. In practice, TransE costs 0.28s for one round in Link Prediction and our model costs 0.36s in the same setting. In general, TransE is most efficient among all the translation-based methods, while our method could be comparable to TransE in running time, justifying the efficiency of our model."}, {"heading": "Experiments", "text": ""}, {"heading": "Datasets & General Settings", "text": "Our experiments are conducted on three public benchmark datasets that are the subsets of Wordnet and Freebase. About the statistics of these datasets, we strongly suggest the readers to refer to (Xie et al. 2016) and (Lin et al. 2015). The entity descriptions of FB15K and FB20K are the same as DKRL (Xie et al. 2016), each of which is a small part of the corresponding wiki-page. The textual information of WN18 is the definitions that we extract from the Wordnet. Notably, for the zero-shot learning, FB20K is involved, which is also built by the authors of DKRL."}, {"heading": "Knowledge Graph Completion", "text": "Evaluation Protocol. The same protocol used in previous studies, is adopted. First, for each testing triple (h, r, t), we replace the tail t (or the head h) with every entity e in the knowledge graph. Then, a probabilistic score of this corrupted triple is calculated with the score function fr(h, t). By ranking these scores in ascending order, we then get the rank of the original triple. The evaluation metrics are the average of the ranks as Mean Rank and the proportion of testing triple whose rank is not larger than 10 (as HITS@10). This is called \u201cRaw\u201d setting. When we filter out the corrupted triples that exist in the training, validation, or test\ndatasets, this is the\u201cFilter\u201d setting. If a corrupted triple exists in the knowledge graph, ranking it ahead the original triple is also correct. To eliminate this effect, the \u201cFilter\u201d setting is more preferred. In both settings, a higher HITS@10 and a lower Mean Rank mean better performance.\nImplementation. As the datasets are the same, we directly reprint the experimental results of several baselines from the literature. We have attempted several settings on the validation dataset to get the best configuration. Under the \u201cbern.\u201d sampling strategy, the optimal configurations of our model SSP are as follows. For WN18, embedding dimension d = 100, learning rate \u03b1 = 0.001, margin \u03b3 = 6.0, balance factor \u03bb = 0.2 and for SSP(Joint) \u00b5 = 0.1. For FB15K, embedding dimension d = 100, learning rate \u03b1 = 0.001, margin \u03b3 = 1.8, balance factor \u03bb = 0.2 and for SSP(Joint) \u00b5 = 0.1. We train the model until convergence. Usually, it would converge until 10,000 rounds, but in current version, instead, we report the results of standard setting with 2,000 rounds. Regarding the joint setting, which is more difficult to converge, we train the model until 5,000 rounds.\nResults Evaluation results are reported in Tab.1 and Tab.2. Note that \u201cJointly\u201d refers to (Zhong et al. 2015). We observe that:\n1This method involves much more extra text corpus, thus it\u2019s unfair to directly compare with others.\n1. SSP outperforms all the baselines in all the tasks, demonstrating the effectiveness of our models and the correctness of our theoretical analysis. Specifically, SSP(Joint) improves much more than SSP(Std.) for jointly learning the textual semantics and symbolic triples.\n2. DKRL and \u201cJointly\u201d model only consider the first-order constraints, which interact between the textual and symbolic information, unsatisfactorily. By focusing on the strong correlation, SSP outperforms them. Notice that the \u201cJointly\u201d model involves much more additional data to produce the result, but SSP also has an remarkable advantage against it. Though TransH is also a hyperplanebased method, SSP adopts the hyperplane in a semanticsspecific way rather than a simple relation-specific form.\n3. TransE could be treated as missing textual descriptions, and DKRL(BOW) could be viewed as missing symbolic triples. SSP (Joint) improves 12.4% against TransE while 20.9% against DKRL(BOW), illustrating the triple embedding is always the key point and the interactions between the two information sources is a key factor."}, {"heading": "Entity Classification", "text": "This task is essentially a multi-label classification, focusing on predicting entity types, which is crucial and widely used in many NLP & IR tasks (Neelakantan and Chang 2015). The entity in Freebase always has types, for instance, the entity types of \u201cScots\u201d are Human Language, Rosetta Languoid.\nWe adopt the same datasets as DKRL, for the details of which, we refer readers to (Xie et al. 2016). Overall, this is a multi-label classification task with 50 classes, which means for each entity, the method should provide a set of types rather than a single type.\nEvaluation Protocol In the training, we use the concatenation of semantic vector and embedding vector (se, e) as entity representation, which is the feature for the front-end classifier. For a fair comparison, our front-end classifier is also the Logistic Regression as DKRL in a one-versus-rest setting for multi-label classification. The evaluation is following (Neelakantan and Chang 2015), which applies the mean average precision (MAP) that is commonly used in multi-label classification. Specifically, for an entity, if the methods predict a rank list of types and there are three correct types that lie in #1, #2, #4, the MAP is calculated as 1/1+2/2+3/4 3 . For FB20K, the methods could only make use of the descriptions. Obviously, FB20K is a zero-shot scenario which is really hard.\nImplementation. As the datasets are the same, we directly report the experimental results of several baselines from the literature. We have attempted several settings on the validation dataset to get the best configuration. Under the \u201cbern.\u201d sampling strategy, the optimal configurations of our model SSP are as follows. For FB15K,embedding dimension d = 100, learning rate \u03b1 = 0.001, margin \u03b3 = 1.8, balance factor \u03bb = 0.2 and for SSP(Joint) \u00b5 = 0.1. For FB20K, embedding dimension d = 100, learning rate \u03b1 = 0.001, margin \u03b3 = 1.8, balance factor \u03bb = 0.2 and for SSP(Joint) \u00b5 = 0.1. We train the model until convergence.\nResults The evaluation results are listed in Tab. 3. Notably, TransE is only triple-specific and SSP (Std.) performs as well as the NMF in the zero-shot learning. Thus they are trivial for FB20K. The observations are as follows:\n1. Overall, our method SSP yields the best accuracy, justifying the effectiveness of our method.\n2. Compared to NMF and TransE, the results show that the interactions between triples and texts make a key difference and only one of them could hardly produce an effective performance. The promotion of SSP (Joint) in FB20K demonstrates the triple embedding also has a positive effect on textual semantics.\n3. Compared to TransE, the improvements illustrate the effectiveness of semantic integration. Compared to DKRL, the promotion demonstrates the important role of modeling strong correlation."}, {"heading": "Semantic Relevance Analysis", "text": "One benefit of modeling semantic relevance is the ability to correctly classify the triples that could not be discriminated by just using the information from symbolic triples. Hence, we make statistic analysis of the results in Link Prediction, as reported in the Tab.4. The number in each cell means\nthe number of triples whose rank is larger than m in TransE and less than n in our models. For instance, the number 601 means there are 601 triples whose ranks are less than 100 in SSP(S.) while their ranks are more than 500 in TransE. Note that SSP(S.) indicates the standard setting, and SSP(J.) means the joint setting. The statistic results indicate that many triples benefit from the semantic relevance offered by textual descriptions. The experiments also justify the theoretical analysis about the semantic relevance and demonstrate the effectiveness of our models."}, {"heading": "Precise Semantic Expression Analysis", "text": "As discussed previously, precise semantic expression leads to better discrimination. To justify this claim, we have collected the negative triples by link prediction, which are scored sightly better than the golden ones by TransE (that means, these are hard examples for TransE), and then plotted the SSP score difference between each corresponding pair of the negative and golden triples as Fig.3 shows. All these triples are predicted wrongly by TransE, but with precise semantic expression, our model correctly distinguishes 82.0% (Std.) and 83.2% (Joint) of them. In the histogram, the right bars indicate that SSP makes correct decision while TransE fails and the left bars mean both SSP and TransE fail. The experiments justify the theoretical analysis about the precise semantic expression and demonstrate the effectiveness of our models."}, {"heading": "Conclusion", "text": "In this paper, we propose the knowledge graph embedding model SSP, which jointly learns from the symbolic triples and textual descriptions. SSP could interact between the triples and texts by characterizing the strong correlations between fact triples and textual descriptions. The textual descriptions have much effect on discovering semantic relevance and then offering precise semantic expression. Extensive experiments show our method achieves the substantial improvements against the state-of-the-art baselines.\nAcknowledgement. This work was partly supported by the National Basic Research Program (973 Program) under grant No. 2013CB329403, and the National Science Foundation of China under grant No.61272227/61332007."}], "references": [{"title": "Probabilistic topic models", "author": ["D.M. Blei"], "venue": "Communications of the ACM 55(4):77\u201384.", "citeRegEx": "Blei,? 2012", "shortCiteRegEx": "Blei", "year": 2012}, {"title": "Freebase: a collaboratively created graph database for structuring human knowledge", "author": ["K. Bollacker", "C. Evans", "P. Paritosh", "T. Sturge", "J. Taylor"], "venue": "Proceedings of the 2008 ACM SIGMOD international conference on Management of data, 1247\u20131250. ACM.", "citeRegEx": "Bollacker et al\\.,? 2008", "shortCiteRegEx": "Bollacker et al\\.", "year": 2008}, {"title": "Learning structured embeddings of knowledge bases", "author": ["A. Bordes", "J. Weston", "R. Collobert", "Y Bengio"], "venue": "In Proceedings of the Twenty-fifth AAAI Conference on Artificial Intelligence", "citeRegEx": "Bordes et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Bordes et al\\.", "year": 2011}, {"title": "Translating embeddings for modeling multi-relational data", "author": ["A. Bordes", "N. Usunier", "A. Garcia-Duran", "J. Weston", "O. Yakhnenko"], "venue": "Advances in Neural Information Processing Systems, 2787\u20132795.", "citeRegEx": "Bordes et al\\.,? 2013", "shortCiteRegEx": "Bordes et al\\.", "year": 2013}, {"title": "Transition-based knowledge graph embedding with relational mapping properties", "author": ["M. Fan", "Q. Zhou", "E. Chang", "T.F. Zheng"], "venue": "Proceedings of the 28th Pacific Asia Conference on Language, Information, and Computation, 328\u2013337.", "citeRegEx": "Fan et al\\.,? 2014", "shortCiteRegEx": "Fan et al\\.", "year": 2014}, {"title": "GAKE: Graph aware knowledge embedding", "author": ["J. Feng", "M. Huang", "Y. Yang", "X. Zhu"], "venue": "COLING 2016, 26th International Conference on Computational Linguistics.", "citeRegEx": "Feng et al\\.,? 2016", "shortCiteRegEx": "Feng et al\\.", "year": 2016}, {"title": "Understanding the difficulty of training deep feedforward neural networks", "author": ["X. Glorot", "Y. Bengio"], "venue": "International conference on artificial intelligence and statistics, 249\u2013256.", "citeRegEx": "Glorot and Bengio,? 2010", "shortCiteRegEx": "Glorot and Bengio", "year": 2010}, {"title": "Semantically smooth knowledge graph embedding", "author": ["S. Guo", "Q. Wang", "B. Wang", "L. Wang", "L. Guo"], "venue": "Proceedings of ACL.", "citeRegEx": "Guo et al\\.,? 2015", "shortCiteRegEx": "Guo et al\\.", "year": 2015}, {"title": "Learning to represent knowledge graphs with gaussian embedding", "author": ["S. He", "K. Liu", "G. Ji", "J. Zhao"], "venue": "Proceedings of the 24th ACM International on Conference on Information and Knowledge Management, 623\u2013632. ACM.", "citeRegEx": "He et al\\.,? 2015", "shortCiteRegEx": "He et al\\.", "year": 2015}, {"title": "A latent factor model for highly multi-relational data", "author": ["R. Jenatton", "N.L. Roux", "A. Bordes", "G.R. Obozinski"], "venue": "Advances in Neural Information Processing Systems, 3167\u20133175.", "citeRegEx": "Jenatton et al\\.,? 2012", "shortCiteRegEx": "Jenatton et al\\.", "year": 2012}, {"title": "Learning entity and relation embeddings for knowledge graph completion", "author": ["Y. Lin", "Z. Liu", "M. Sun", "Y. Liu", "X. Zhu"], "venue": "Proceedings of the Twenty-Ninth AAAI Conference on Artificial Intelligence.", "citeRegEx": "Lin et al\\.,? 2015", "shortCiteRegEx": "Lin et al\\.", "year": 2015}, {"title": "Modeling relation paths for representation learning of knowledge bases", "author": ["Y. Lin", "Z. Liu", "M. Sun"], "venue": "Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing (EMNLP). Association for Computational Linguistics.", "citeRegEx": "Lin et al\\.,? 2015", "shortCiteRegEx": "Lin et al\\.", "year": 2015}, {"title": "Distributed representations of words and phrases and their compositionality", "author": ["T. Mikolov", "I. Sutskever", "K. Chen", "G.S. Corrado", "J. Dean"], "venue": "Advances in neural information processing systems, 3111\u20133119.", "citeRegEx": "Mikolov et al\\.,? 2013", "shortCiteRegEx": "Mikolov et al\\.", "year": 2013}, {"title": "Linguistic regularities in continuous space word representations", "author": ["T. Mikolov", "W.-t. Yih", "G. Zweig"], "venue": "HLTNAACL, 746\u2013751.", "citeRegEx": "Mikolov et al\\.,? 2013", "shortCiteRegEx": "Mikolov et al\\.", "year": 2013}, {"title": "Wordnet: a lexical database for english", "author": ["G.A. Miller"], "venue": "Communications of the ACM 38(11):39\u201341.", "citeRegEx": "Miller,? 1995", "shortCiteRegEx": "Miller", "year": 1995}, {"title": "Inferring missing entity type instances for knowledge base completion: New dataset and methods", "author": ["A. Neelakantan", "Chang", "M.-W."], "venue": "arXiv preprint arXiv:1504.06658.", "citeRegEx": "Neelakantan et al\\.,? 2015", "shortCiteRegEx": "Neelakantan et al\\.", "year": 2015}, {"title": "A threeway model for collective learning on multi-relational data", "author": ["M. Nickel", "V. Tresp", "H.-P. Kriegel"], "venue": "Proceedings of the 28th international conference on machine learning (ICML-11), 809\u2013816.", "citeRegEx": "Nickel et al\\.,? 2011", "shortCiteRegEx": "Nickel et al\\.", "year": 2011}, {"title": "Reasoning with neural tensor networks for knowledge base completion", "author": ["R. Socher", "D. Chen", "C.D. Manning", "A. Ng"], "venue": "Advances in Neural Information Processing Systems, 926\u2013934.", "citeRegEx": "Socher et al\\.,? 2013", "shortCiteRegEx": "Socher et al\\.", "year": 2013}, {"title": "Exploring topic coherence over many models and many topics", "author": ["K. Stevens", "P. Kegelmeyer", "D. Andrzejewski", "D. Buttler"], "venue": "Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning, 952\u2013961. As-", "citeRegEx": "Stevens et al\\.,? 2012", "shortCiteRegEx": "Stevens et al\\.", "year": 2012}, {"title": "Knowledge graph and text jointly embedding", "author": ["Z. Wang", "J. Zhang", "J. Feng", "Z. Chen"], "venue": "EMNLP, 1591\u2013 1601. Citeseer.", "citeRegEx": "Wang et al\\.,? 2014a", "shortCiteRegEx": "Wang et al\\.", "year": 2014}, {"title": "Knowledge graph embedding by translating on hyperplanes", "author": ["Z. Wang", "J. Zhang", "J. Feng", "Z. Chen"], "venue": "Proceedings of the Twenty-Eighth AAAI Conference on Artificial Intelligence, 1112\u20131119.", "citeRegEx": "Wang et al\\.,? 2014b", "shortCiteRegEx": "Wang et al\\.", "year": 2014}, {"title": "Knowledge base completion using embeddings and rules", "author": ["Q. Wang", "B. Wang", "L. Guo"], "venue": "Proceedings of the 24th International Joint Conference on Artificial Intelligence.", "citeRegEx": "Wang et al\\.,? 2015", "shortCiteRegEx": "Wang et al\\.", "year": 2015}, {"title": "TransA: An adaptive approach for knowledge graph embedding", "author": ["H. Xiao", "M. Huang", "Y. Hao", "X. Zhu"], "venue": "arXiv preprint arXiv:1509.05490.", "citeRegEx": "Xiao et al\\.,? 2015", "shortCiteRegEx": "Xiao et al\\.", "year": 2015}, {"title": "From one point to a manifold: Knowledge graph embedding for precise link prediction", "author": ["H. Xiao", "M. Huang", "X. Zhu"], "venue": "Proceedings of the 25th International Joint Conference on Artificial Intelligence.", "citeRegEx": "Xiao et al\\.,? 2016a", "shortCiteRegEx": "Xiao et al\\.", "year": 2016}, {"title": "TransG : A generative model for knowledge graph embedding", "author": ["H. Xiao", "M. Huang", "X. Zhu"], "venue": "Proceedings of the 29th international conference on computational linguistics. Association for Computational Linguistics.", "citeRegEx": "Xiao et al\\.,? 2016b", "shortCiteRegEx": "Xiao et al\\.", "year": 2016}, {"title": "Representation learning of knowledge graphs with entity descriptions", "author": ["R. Xie", "Z. Liu", "J. Jia", "H. Luan", "M. Sun"], "venue": null, "citeRegEx": "Xie et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Xie et al\\.", "year": 2016}, {"title": "Aligning knowledge and text embeddings by entity descriptions", "author": ["H. Zhong", "J. Zhang", "Z. Wang", "H. Wan", "Z. Chen"], "venue": "Proceedings of EMNLP, 267\u2013272.", "citeRegEx": "Zhong et al\\.,? 2015", "shortCiteRegEx": "Zhong et al\\.", "year": 2015}], "referenceMentions": [{"referenceID": 3, "context": "To this end, a number of embedding methods have been proposed, such as TransE (Bordes et al. 2013), PTransE (Lin, Liu, and Sun 2015), KG2E (He et al.", "startOffset": 78, "endOffset": 98}, {"referenceID": 8, "context": "2013), PTransE (Lin, Liu, and Sun 2015), KG2E (He et al. 2015), etc.", "startOffset": 46, "endOffset": 62}, {"referenceID": 25, "context": "The existing embedding methods with textual semantics such as DKRL (Xie et al. 2016) and \u201cJointly\u201d (Zhong et al.", "startOffset": 67, "endOffset": 84}, {"referenceID": 26, "context": "2016) and \u201cJointly\u201d (Zhong et al. 2015), have achieved much success.", "startOffset": 20, "endOffset": 39}, {"referenceID": 14, "context": "We evaluate the effectiveness of our model Semantic Subspace Projection (SSP) with two tasks on three benchmark datasets that are the subsets of Wordnet (Miller 1995) and Freebase (Bollacker et al.", "startOffset": 153, "endOffset": 166}, {"referenceID": 1, "context": "We evaluate the effectiveness of our model Semantic Subspace Projection (SSP) with two tasks on three benchmark datasets that are the subsets of Wordnet (Miller 1995) and Freebase (Bollacker et al. 2008).", "startOffset": 180, "endOffset": 203}, {"referenceID": 3, "context": "TransE (Bordes et al. 2013) is a pioneering work for this branch, which translates the head entity to the tail one by the relation vector, or h + r = t.", "startOffset": 7, "endOffset": 27}, {"referenceID": 20, "context": "TransH (Wang et al. 2014b) utilizes the relationspecific hyperplane to lay the entities.", "startOffset": 7, "endOffset": 26}, {"referenceID": 10, "context": "TransR (Lin et al. 2015) applies the relation-related matrix to rotate the embedding space.", "startOffset": 7, "endOffset": 24}, {"referenceID": 5, "context": "), (Feng et al. 2016) and TransM (Fan et al.", "startOffset": 3, "endOffset": 21}, {"referenceID": 4, "context": "2016) and TransM (Fan et al. 2014).", "startOffset": 17, "endOffset": 34}, {"referenceID": 7, "context": "SSE (Guo et al. 2015) aims at discovering the geometric structure of embedding topologies and then based on these assumptions, designs a semantically smoothing score function.", "startOffset": 4, "endOffset": 21}, {"referenceID": 8, "context": "Also, KG2E (He et al. 2015) involves probabilistic analysis to characterize the uncertainty concepts of knowledge graph.", "startOffset": 11, "endOffset": 27}, {"referenceID": 2, "context": "There are also some other works such as SE (Bordes et al. 2011), LFM (Jenatton et al.", "startOffset": 43, "endOffset": 63}, {"referenceID": 9, "context": "2011), LFM (Jenatton et al. 2012), NTN (Socher et al.", "startOffset": 11, "endOffset": 33}, {"referenceID": 17, "context": "2012), NTN (Socher et al. 2013) and RESCAL (Nickel, Tresp, and Kriegel 2011), TransA (Xiao et al.", "startOffset": 11, "endOffset": 31}, {"referenceID": 22, "context": "2013) and RESCAL (Nickel, Tresp, and Kriegel 2011), TransA (Xiao et al. 2015), etc.", "startOffset": 59, "endOffset": 77}, {"referenceID": 17, "context": "\u201cText-Aware\u201d Embedding, which attempts to representing knowledge graph with textual information, generally dates back to NTN (Socher et al. 2013).", "startOffset": 125, "endOffset": 145}, {"referenceID": 19, "context": "(Wang et al. 2014a) attempts to aligning the knowledge graph with the corpus then jointly conducting knowledge embedding and word embedding.", "startOffset": 0, "endOffset": 19}, {"referenceID": 26, "context": "Thus, (Zhong et al. 2015) proposes the \u201cJointly\u201d method that only aligns the freebase entity to the corresponding wiki-page.", "startOffset": 6, "endOffset": 25}, {"referenceID": 25, "context": "DKRL (Xie et al. 2016) extends the translationbased embedding methods from the triple-specific one to the \u201cText-Aware\u201d model.", "startOffset": 5, "endOffset": 22}, {"referenceID": 0, "context": "There are at least two methods that could be leveraged to generate the semantic vectors: topic model (Blei 2012) such as LSA, LDA, NMF (Stevens et al.", "startOffset": 101, "endOffset": 112}, {"referenceID": 18, "context": "There are at least two methods that could be leveraged to generate the semantic vectors: topic model (Blei 2012) such as LSA, LDA, NMF (Stevens et al. 2012) and word embedding such as CBOW (Mikolov, Yih, and Zweig 2013), Skip-Gram (Mikolov et al.", "startOffset": 135, "endOffset": 156}, {"referenceID": 12, "context": "2012) and word embedding such as CBOW (Mikolov, Yih, and Zweig 2013), Skip-Gram (Mikolov et al. 2013).", "startOffset": 80, "endOffset": 101}, {"referenceID": 20, "context": "Sampling Method\u201d as introduced in (Wang et al. 2014b) and the method selects the negative samples from the set of", "startOffset": 34, "endOffset": 53}, {"referenceID": 6, "context": "We initialize the embedding vectors by the similar methods used in the deep neural network (Glorot and Bengio 2010) and pre-train the topic model with Non-negative Matrix Factorization (NMF) (Stevens et al.", "startOffset": 91, "endOffset": 115}, {"referenceID": 18, "context": "We initialize the embedding vectors by the similar methods used in the deep neural network (Glorot and Bengio 2010) and pre-train the topic model with Non-negative Matrix Factorization (NMF) (Stevens et al. 2012).", "startOffset": 191, "endOffset": 212}, {"referenceID": 18, "context": "For the topic-related objective, we take the advantage of the NMF Topic Model (Stevens et al. 2012), which is both simple and effective.", "startOffset": 78, "endOffset": 99}, {"referenceID": 25, "context": "About the statistics of these datasets, we strongly suggest the readers to refer to (Xie et al. 2016) and (Lin et al.", "startOffset": 84, "endOffset": 101}, {"referenceID": 10, "context": "2016) and (Lin et al. 2015).", "startOffset": 10, "endOffset": 27}, {"referenceID": 25, "context": "The entity descriptions of FB15K and FB20K are the same as DKRL (Xie et al. 2016), each of which is a small part of the corresponding wiki-page.", "startOffset": 64, "endOffset": 81}, {"referenceID": 26, "context": "Note that \u201cJointly\u201d refers to (Zhong et al. 2015).", "startOffset": 30, "endOffset": 49}, {"referenceID": 25, "context": "We adopt the same datasets as DKRL, for the details of which, we refer readers to (Xie et al. 2016).", "startOffset": 82, "endOffset": 99}], "year": 2017, "abstractText": "Knowledge graph embedding represents entities and relations in knowledge graph as low-dimensional, continuous vectors, and thus enables knowledge graph compatible with machine learning models. Though there have been a variety of models for knowledge graph embedding, most methods merely concentrate on the fact triples, while supplementary textual descriptions of entities and relations have not been fully employed. To this end, this paper proposes the semantic space projection (SSP) model which jointly learns from the symbolic triples and textual descriptions. Our model builds interaction between the two information sources, and employs textual descriptions to discover semantic relevance and offer precise semantic embedding. Extensive experiments show that our method achieves substantial improvements against baselines on the tasks of knowledge graph completion and entity classification. Papers, Posters, Slides, Datasets and Codes: http://www.ibookman.net/conference.html", "creator": "TeX"}}}