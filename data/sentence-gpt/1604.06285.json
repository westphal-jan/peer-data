{"id": "1604.06285", "review": {"conference": "HLT-NAACL", "VERSION": "v1", "DATE_OF_SUBMISSION": "21-Apr-2016", "title": "A Novel Approach to Dropped Pronoun Translation", "abstract": "Dropped Pronouns (DP) in which pronouns are frequently dropped in the source language but should be retained in the target language are challenge in machine translation. In response to this problem, we propose a semi-supervised approach to recall possibly missing pronouns in the translation. Firstly, we build training data for DP generation in which the DPs are automatically labelled according to the alignment information from a parallel corpus. Then we generate a set of pre-processing tasks to generate an order of magnitude (or equivalence) of 0.18 on the Pronouns, one that corresponds to a set of 100 points of each set. Finally, we create a dataset of pre-processing tasks to extract the total number of repetitions in the dataset, each with a corresponding sequence (a) to the DPs. In this dataset, the Pronouns are selected from a list of 20 sub-titles (the number of sub-titles was the same as the corresponding set of 16 subtitles) to extract all the information and the resulting order of magnitude. Finally, we define a DPs as having the same order of magnitude as the corresponding set of 16 subtitles (with a set of 16 subtitles). The dataset then includes all the necessary information (i.e., the original order of magnitude). Once the model is complete, it will be able to output an image representing a list of 20 subtitles (the number of sub-titles was the same as the corresponding set of 16 subtitles). When it is completed, it will be able to analyze the entire dataset with a few queries and to identify the missing subtitles using a set of 30 subtitles (the number of subtitles was the same as the corresponding set of 16 subtitles). Finally, we define a set of 24 subtitles (the number of subtitles was the same as the corresponding set of 16 subtitles). This will allow us to further refine our model for the sake of generating data for DP generation in which the DPs are automatically labeled according to the alignment information from a parallel corpus.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n", "histories": [["v1", "Thu, 21 Apr 2016 12:55:29 GMT  (328kb,D)", "http://arxiv.org/abs/1604.06285v1", "To appear in NAACL2016"]], "COMMENTS": "To appear in NAACL2016", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["longyue wang", "zhaopeng tu", "xiaojun zhang", "hang li", "andy way", "qun liu"], "accepted": true, "id": "1604.06285"}, "pdf": {"name": "1604.06285.pdf", "metadata": {"source": "CRF", "title": "A Novel Approach to Dropped Pronoun Translation", "authors": ["Longyue Wang", "Zhaopeng Tu", "Xiaojun Zhang", "Hang Li", "Andy Way", "Qun Liu"], "emails": ["qliu}@computing.dcu.ie", "hangli.hl}@huawei.com"], "sections": [{"heading": "1 Introduction", "text": "In pro-drop languages, certain classes of pronouns can be omitted to make the sentence compact yet comprehensible when the identity of the pronouns can be inferred from the context (Yang et al., 2015). Figure 1 shows an example, in which Chinese is a pro-drop language (Huang, 1984), while English is\nnot (Haspelmath, 2001). On the Chinese side, the subject pronouns {\u4f60 (you), \u6211 (I)} and the object pronouns {\u5b83 (it), \u4f60 (you)} are omitted in the dialogue between Speakers A and B. These omissions may not be problems for humans since people can easily recall the missing pronouns from the context. However, this poses difficulties for Statistical Machine Translation (SMT) from pro-drop languages (e.g. Chinese) to non-pro-drop languages (e.g. English), since translation of such missing pronouns cannot be normally reproduced. Generally, this phenomenon is more common in informal genres such as dialogues and conversations than others (Yang et al., 2015). We also validated this finding by analysing a large Chinese\u2013English dialogue corpus which consists of 1M sentence pairs extracted from movie and TV episode subtitles. We found that there are 6.5M Chinese pronouns and 9.4M English pronouns, which shows that more than 2.9 million Chinese pronouns are missing.\nIn response to this problem, we propose to find a general and replicable way to improve translation quality. The main challenge of this research is that training data for DP generation are scarce. Most\nar X\niv :1\n60 4.\n06 28\n5v 1\n[ cs\n.C L\n] 2\n1 A\npr 2\nworks either apply manual annotation (Yang et al., 2015) or use existing but small-scale resources such as the Penn Treebank (Chung and Gildea, 2010; Xiang et al., 2013). In contrast, we employ an unsupervised approach to automatically build a largescale training corpus for DP generation using alignment information from parallel corpora. The idea is that parallel corpora available in SMT can be used to project the missing pronouns from the target side (i.e. non-pro-drop language) to the source side (i.e. pro-drop language). To this end, we propose a simple but effective method: a bi-directional search algorithm with Language Model (LM) scoring.\nAfter building the training data for DP generation, we apply a supervised approach to build our DP generator. We divide the DP generation task into two phases: DP detection (from which position a pronoun is dropped), and DP prediction (which pronoun is dropped). Due to the powerful capacity of feature learning and representation learning, we model the DP detection problem as sequential labelling with Recurrent Neural Networks (RNNs) and model the prediction problem as classification with Multi-Layer Perceptron (MLP) using features at various levels: from lexical, through contextual, to syntax.\nFinally, we try to improve the translation of missing pronouns by explicitly recalling DPs for both parallel data and monolingual input sentences. More specifically, we extract an additional rule table from the DP-inserted parallel corpus to produce a \u201cpronoun-complete\u201d translation model. In addition, we pre-process the input sentences by inserting possible DPs via the DP generation model. This makes the input sentences more consistent with the additional pronoun-complete rule table. To alleviate the propagation of DP prediction errors, we feed the translation system N -best prediction results via confusion network decoding (Rosti et al., 2007).\nTo validate the effect of the proposed approach, we carried out experiments on a Chinese\u2013English translation task. Experimental results on a largescale subtitle corpus show that our approach improves translation performance by 0.61 BLEU points (Papineni et al., 2002) using the additional translation model trained on the DP-inserted corpus. Working together with DP-generated input sentences achieves a further improvement of nearly 1.0\nBLEU point. Furthermore, translation performance with N -best integration is much better than its 1-best counterpart (i.e. +0.84 BLEU points).\nGenerally, the contributions of this paper include the following: \u2022 We propose an automatic method to build a\nlarge-scale DP training corpus. Given that the DPs are annotated in the parallel corpus, models trained on this data are more appropriate to the translation task;\n\u2022 Benefiting from representation learning, our deep learning-based generation models are able to avoid ignore the complex featureengineering work while still yielding encouraging results;\n\u2022 To decrease the negative effects on translation caused by inserting incorrect DPs, we force the SMT system to arbitrate between multiple ambiguous hypotheses from the DP predictions.\nThe rest of the paper is organized as follows. In Section 2, we describe our approaches to building the DP corpus, DP generator and SMT integration. Related work is described in Section 3. The experimental results for both the DP generator and translation are reported in Section 4. Section 5 analyses some real examples which is followed by our conclusion in Section 6."}, {"heading": "2 Methodology", "text": "The architecture of our proposed method is shown in Figure 2, which can be divided into three phases: DP corpus annotation, DP generation, and SMT integration."}, {"heading": "2.1 DP Training Corpus Annotation", "text": "We propose an approach to automatically annotate DPs by utilizing alignment information. Given a parallel corpus, we first use an unsupervised word alignment method (Och and Ney, 2003; Tu et al., 2012) to produce a word alignment. From observing of the alignment matrix, we found it is possible to detect DPs by projecting misaligned pronouns from the non-pro-drop target side (English) to the pro-drop source side (Chinese). In this work, we focus on nominative and accusative pronouns including personal, possessive and reflexive instances, as listed in Table 1.\nWe use an example to illustrate our idea. Figure 3 features a dropped pronoun \u201c\u6211\u201d (not shown) on the source side, which is aligned to the second \u201cI\u201d (in red) on the target side. For each pronoun on the target side (e.g. \u201cI\u201d, \u201cyou\u201d), we first check whether it has an aligned pronoun on the source side. We find that the second \u201cI\u201d is not aligned to any source word and possibly corresponds to a DPI (e.g. \u201c\u6211\u201d). To determine the possible positions of DPI on the source side, we employ a diagonal heuristic based on the observation that there exists a diagonal rule in the local area of the alignment matrix. For example, the alignment blocks in Figure 3 generally\nfollow a diagonal line. Therefore, the pronoun \u201dI\u201d on the target side can be projected to the purple area (i.e. \u201c\u4f60\u8bf4\u8fc7\u60f3\u201d) on the source side, according to the preceding and following alignment blocks (i.e. \u201cyou-\u4f60\u201d and \u201cwant-\u60f3\u201d).\nHowever, there are still three possible positions to insert DPI (i.e. the three gaps in the purple area). To further determine the exact position of DPI , we generate possible sentences by inserting the corresponding Chinese DPs1 into every possible position. Then we employ an n-gram language model (LM) to score these candidates and select the one with the lowest perplexity as final result. This LM-based projection is based on the observation that the amount and type of DPs are very different in different gen-\n1The Chinese DP can be determined by using its English pronouns according to Table 1. Note that some English pronouns may correspond to different Chinese pronouns, such as \u201cthey -\u4ed6\u4eec /\u5979\u4eec /\u5b83\u4eec\u201d. In such cases, we use all the corresponding Chinese pronouns as the candidates.\nres. We hypothesize that the DP position can be determined by utilizing the inconsistency of DPs in different domains. Therefore, the LM is trained on a large amount of webpage data (detailed in Section 3.1). Considering the problem of incorrect DP insertion caused by incorrect alignment, we add the original sentence into the LM scoring to reduce impossible insertions (noise)."}, {"heading": "2.2 DP Generation", "text": "In light of the recent success of applying deep neural network technologies in natural language processing (Raymond and Riccardi, 2007; Mesnil et al., 2013), we propose a neural network-based DP generator via the DP-inserted corpus (Section 2.1). We first employ an RNN to predict the DP position, and then train a classifier using multilayer perceptrons to generate our N -best DP results."}, {"heading": "2.2.1 DP detection", "text": "The task of DP position detection is to label words if there are pronouns missing before the words, which can intuitively be regarded as a sequence labelling problem. We expect the output to be a sequence of labels y(1:n) = (y(1), y(2), \u00b7 \u00b7 \u00b7 , y(t), \u00b7 \u00b7 \u00b7 , y(n)) given a sentence consisting of words w(1:n) = (w(1), w(2), \u00b7 \u00b7 \u00b7 , w(t), \u00b7 \u00b7 \u00b7 , w(n)), where y(t) is the label of word w(t). In our task, there are two labels L = {NA,DP} (corresponding to non-pro-drop or pro-drop pronouns), thus y(t) \u2208 L.\nWord embeddings (Mikolov et al., 2013) are used for our generation models: given a word w(t), we try to produce an embedding representation v(t) \u2208 Rd where d is the dimension of the representation vectors. In order to capture short-term temporal dependencies, we feed the RNN unit a window of context, as in Equation (1):\nxd (t) = v(t\u2212k) \u2295 \u00b7 \u00b7 \u00b7 \u2295 v(t) \u2295 \u00b7 \u00b7 \u00b7 \u2295 v(t+k) (1)\nwhere k is the window size. We employ an RNN (Mesnil et al., 2013) to learn the dependency of sentences, which can be formulated as Equation (2):\nh(t) = f(Uxd (t) +Vh(t\u22121)) (2)\nwhere f(x) is a sigmoid function at the hidden layer. U is the weight matrix between the raw input and\nthe hidden nodes, and V is the weight matrix between the context nodes and the hidden nodes. At the output layer, a softmax function is adopted for labelling, as in Equation (3):\ny(t) = g(Wdh (t)) (3)\nwhere g(zm) = e zm\u2211 k e zk , and Wd is the output weight matrix."}, {"heading": "2.2.2 DP prediction", "text": "Once the DP position is detected, the next step is to determine which pronoun should be inserted based on this result. Accordingly, we train a 22- class classifier, where each class refers to a distinct Chinese pronoun in Table 1. We select a number of features based on previous work (Xiang et al., 2013; Yang et al., 2015), including lexical, contextual, and syntax features (as shown in Table 2). We set p as the DP position, S as the window size surrounding p, and X,Y as the window size surrounding current sentence (the one contains p). For Features 1\u2013 4, we extract words, POS tags and pronouns around p. For Features 5\u20138, we also consider the pronouns and nouns between X/Y surrounding sentences. For Features 9 and 10, in order to model the syntactic relation, we use a path feature, which is the combined tags of the sub-tree nodes from p/(p \u2212 1) to the root. Note that Features 3\u20136 consider all pronouns that were not dropped. Each unique feature is treated as a word, and assigned a \u201cword embedding\u201d. The embeddings of the features are then fed to the\nneural network. We fix the number of features for the variable-length features, where missing ones are tagged as None. Accordingly, all training instances share the same feature length. For the training data, we sample all DP instances from the corpus (annotated by the method in Section 2.1). During decoding, p can be given by our DP detection model.\nWe employ a feed-forward neural network with four layers. The input xp comprises the embeddings of the set of all possible feature indicator names. The middle two layers a(1), a(2) use Rectified Linear function R as the activation function, as in Equation (4)\u2013(5):\na(1) = R(b(1) +Wp (1)xp) (4)\na(2) = R(b(2) +Wp (2)a(1)) (5)\nwhere Wp(1) and b(1) are the weights and bias connecting the first hidden layer to second hidden layer; and so on. The last layer yp adopts the softmax function g, as in Equation (6):\nyp = g(Wp (3)a(2)) (6)"}, {"heading": "2.3 Integration into Translation", "text": "The baseline SMT system uses the parallel corpus and input sentences without inserting/generating DPs. As shown in Figure 2, the integration into SMT system is two fold: DP-inserted translation model (DP-ins. TM) and DP-generated input (DP-gen. Input)."}, {"heading": "2.3.1 DP-inserted TM", "text": "We train an additional translation model on the new parallel corpus, whose source side is inserted with DPs derived from the target side via the alignment matrix (Section 2.1). We hypothesize that DP insertion can help to obtain a better alignment, which can benefit translation. Then the whole translation process is based on the boosted translation model, i.e. with DPs inserted. As far as TM combination is concerned, we directly feed Moses the multiple phrase tables. The gain from the additional TM is mainly from complementary information about the recalled DPs from the annotated data."}, {"heading": "2.3.2 DP-generated input", "text": "Another option is to pre-process the input sentence by inserting possible DPs with the DP generation model (Section 2.2) so that the DP-inserted\ninput (Input ZH+DPs) is translated. The predicted DPs would be explicitly translated into the target language, so that the possibly missing pronouns in the translation might be recalled. This makes the input sentences and DP-inserted TM more consistent in terms of recalling DPs."}, {"heading": "2.3.3 N-best inputs", "text": "However, the above method suffers from a major drawback: it only uses the 1-best prediction result for decoding, which potentially introduces translation mistakes due to the propagation of prediction errors. To alleviate this problem, an obvious solution is to offer more alternatives. Recent studies have shown that SMT systems can benefit from widening the annotation pipeline (Liu et al., 2009; Tu et al., 2010; Tu et al., 2011; Liu et al., 2013). In the same direction, we propose to feed the decoder N -best prediction results, which allows the system to arbitrate between multiple ambiguous hypotheses from upstream processing so that the best translation can be produced. The general method is to make the input with N -best DPs into a confusion network. In our experiment, each prediction result in the N-best list is assigned a weight of 1/N ."}, {"heading": "3 Related Work", "text": "There is some work related to DP generation. One is zero pronoun resolution (ZP), which is a subdirection of co-reference resolution (CR). The difference to our task is that ZP contains three steps (namely ZP detection, anaphoricity determination and co-reference link) whereas DP generation only contains the first two steps. Some researchers (Zhao and Ng, 2007; Kong and Zhou, 2010; Chen and Ng, 2013) propose rich features based on different machine-learning methods. For example, Chen and Ng (2013) propose an SVM classifier using 32 features including lexical, syntax and grammatical roles etc., which are very useful in the ZP task. However, most of their experiments are conducted on a small-scale corpus (i.e. OntoNotes)2 and performance drops correspondingly when using a systemparse tree compared to the gold standard one. Novak and Zabokrtsky (2014) explore cross-language\n2It contains 144K coreference instances, but only 15% of them are dropped subjects.\ndifferences in pronoun behavior to affect the CR results. The experiment shows that bilingual feature sets are helpful to CR. Another line related to DP generation is using a wider range of empty categories (EC) (Yang and Xue, 2010; Cai et al., 2011; Xue and Yang, 2013), which aims to recover longdistance dependencies, discontinuous constituents and certain dropped elements3 in phrase structure treebanks (Xue et al., 2005). This work mainly focus on sentence-internal characteristics as opposed to contextual information at the discourse level. More recently, Yang et al. (2015) explore DP recovery for Chinese text messages based on both lines of work.\nThese methods can also be used for DP translation using SMT (Chung and Gildea, 2010; Le Nagard and Koehn, 2010; Taira et al., 2012; Xiang et al., 2013). Taira et al. (2012) propose both simple rule-based and manual methods to add zero pronouns in the source side for Japanese\u2013English translation. However, the BLEU scores of both systems are nearly identical, which indicates that only considering the source side and forcing the insertion of pronouns may be less principled than tackling the problem head on by integrating them into the SMT system itself. Le Nagard and Koehn (2010) present a method to aid English pronoun translation into French for SMT by integrating CR. Unfortunately, their results are not convincing due to the poor performance of the CR method (Pradhan et al., 2012). Chung and Gildea (2010) systematically examine the effects of EC on MT with three methods: pattern, CRF (which achieves best results) and parsing. The results show that this work can really improve the end translation even though the automatic prediction of EC is not highly accurate."}, {"heading": "4 Experiments", "text": ""}, {"heading": "4.1 Setup", "text": "For dialogue domain training data, we extract around 1M sentence pairs (movie or TV episode subtitles) from two subtitle websites.4 We manually create both development and test data with DP annotation. Note that all sentences maintain their con-\n3EC includes trace markers, dropped pronoun, big PRO etc, while we focus only on dropped pronoun.\n4Avaliable at http://www.opensubtitles.org and http://weisheshou.com.\ntextual information at the discourse level, which can be used for feature extraction in Section 2.1. The detailed statistics are listed in Table 3. As far as the DP training corpus is concerned, we annotate the Chinese side of the parallel data using the approach described in Section 2.1. There are two different language models for the DP annotation (Section 2.1) and translation tasks, respectively: one is trained on the 2.13TB Chinese Web Page Collection Corpus5 while the other one is trained on all extracted 7M English subtitle data (Wang et al., 2016).\nWe carry out our experiments using the phrasebased SMT model in Moses (Koehn et al., 2007) on a Chinese\u2013English dialogue translation task. Furthermore, we train 5-gram language models using the SRI Language Toolkit (Stolcke, 2002). To obtain a good word alignment, we run GIZA++ (Och and Ney, 2003) on the training data together with another larger parallel subtitle corpus that contains 6M sentence pairs.6 We use minimum error rate training (Och, 2003) to optimize the feature weights.\nThe RNN models are implemented using the common Theano neural network toolkit (Bergstra et al., 2010). We use a pre-trained word embedding via a lookup table. We use the following settings: windows = 5, the size of the single hidden layer = 200, iterations = 10, embeddings = 200. The MLP classifier use random initialized embeddings, with the following settings: the size of the single hidden layer = 200, embeddings = 100, iterations = 200.\nFor end-to-end evaluation, case-insensitive BLEU (Papineni et al., 2002) is used to measure\n5Available at http://www.sogou.com/labs/dl/ t-e.html.\n6Dual Subtitles \u2013 Mandarin-English Subtitles Parallel Corpus, extracted by Zhang et al. (2014) without contextual information at the discourse level.\ntranslation performance and micro-averaged F-score is used to measure DP generation quality."}, {"heading": "4.2 Evaluation of DP Generation", "text": "We first check whether our DP annotation strategy is reasonable. To this end, we follow the strategy to automatically and manually label the source sides of the development and test data with their target sides. The agreement between automatic labels and manual labels on DP prediction are 94% and 95% on development and test data and on DP generation are 92% and 92%, respectively. This indicates that the automatic annotation strategy is relatively trustworthy.\nWe then measure the accuracy (in terms of words) of our generation models in two phases. \u201cDP Detection\u201d shows the performance of our sequencelabelling model based on RNN. We only consider the tag for each word (pro-drop or not pro-drop before the current word), without considering the exact pronoun for DPs. \u201cDP Prediction\u201d shows the performance of the MLP classifier in determining the exact DP based on detection. Thus we consider both the detected and predicted pronouns. Table 4 lists the results of the above DP generation approaches. The F1 score of \u201cDP Detection\u201d achieves 88% and 86% on the Dev and Test set, respectively. However, it has lower F1 scores of 66% and 65% for the final pronoun generation (\u201cDP Prediction\u201d) on the development and test data, respectively. This indicates that predicting the exact DP in Chinese is a really difficult task. Even though the DP prediction is not highly accurate, we still hypothesize that the DP generation models are reliable enough to be used for end-to-end machine translation. Note that we only show the results of 1-best DP generation here, but in the translation task, we use N -best generation candidates to recall more DPs."}, {"heading": "4.3 Evaluation of DP Translation", "text": "In this section, we evaluate the end-to-end translation quality by integrating the DP generation results (Section 3.3). Table 5 summaries the results of translation performance with different sources of DP information. \u201cBaseline\u201d uses the original input to feed the SMT system. \u201c+DP-ins. TM\u201d denotes using an additional translation model trained on the DPinserted training corpus, while \u201c+DP-gen. Input N\u201d denotes further completing the input sentences with the N -best pronouns generated from the DP generation model. \u201cOracle\u201d uses the input with manual (\u201cManual\u201d) or automatic (\u201cAuto\u201d) insertion of DPs by considering the target set. Taking \u201cAuto Oracle\u201d for example, we annotate the DPs via alignment information (supposing the reference is available) using the technique described in Section 2.1.\nThe baseline system uses the parallel corpus and input sentences without inserting/generating DPs. It achieves 20.06 and 18.76 in BLEU score on the development and test data, respectively. The BLEU scores are relatively low because 1) we have only one reference, and 2) dialogue machine translation is still a challenge for the current SMT approaches.\nBy using an additional translation model trained on the DP-inserted parallel corpus as described in Section 2.1, we improve the performance consistently on both development (+0.26) and test data (+0.61). This indicates that the inserted DPs are helpful for SMT. Thus, the gain in the \u201c+DP-ins TM\u201d is mainly from the improved alignment quality.\nWe can further improve translation performance by completing the input sentences with our DP gen-\neration model as described in Section 2.2. We test N -best DP insertion to examine the performance, where N ={1, 2, 4, 6, 8}. Working together with \u201cDP-ins. TM\u201d, 1-best generated input already achieves +0.43 and + 0.74 BLEU score improvements on development and test set, respectively. The consistency between the input sentences and the DPinserted parallel corpus contributes most to these further improvements. As N increases, the BLEU score grows, peaking at 21.61 and 20.34 BLEU points when N=6. Thus we achieve a final improvement of 1.55 and 1.58 BLEU points on the development and test data, respectively. However, when adding more DP candidates, the BLEU score decreases by 0.97 and 0.51. The reason for this may be that more DP candidates add more noise, which harms the translation quality.\nThe oracle system uses the input sentences with manually annotated DPs rather than \u201cDP-gen. Input\u201d. The performance gap between \u201cOracle\u201d and \u201c+DP-gen. Input\u201d shows that there is still a large space (+4.22 or +3.17) for further improvement for the DP generation model."}, {"heading": "5 Case Study", "text": "We select sample sentences from the test set to further analyse the effects of DP generation on translation.\nIn Figure 4, we show an improved case (Case A), an unchanged case (Case B), and a worse case (Case C) of translation no-/using DP insertion (i.e. \u201c+DP-gen. Input 1-best\u201d). In each case, we give (a) the original Chinese sentence and its translation, (b) the DP-inserted Chinese sentence and its translation, and (c) the reference English sentence. In Case A, \u201cDo you\u201d in the translation output is compensated by adding DP \u2329 \u4f60 \u232a\n(you) in (b), which gives a better translation than in (a). In contrast, in case C, our DP generator regards the simple sentence as a compound sentence and insert a wrong pronoun\u2329 \u6211 \u232a (I) in (b), which causes an incorrect translation output (worse than (a)). This indicates that we need a highly accurate parse tree of the source sentences for more correct completion of the antecedent of the DPs. In Case B, the translation results are the same in (a) and (b). This kind of unchanged case always occurs in \u201cfixed\u201d linguistic chunks such as prepo-\nsition phrases (\u201con my way\u201d), greetings (\u201csee you later\u201d , \u201cthank you\u201d) and interjections (\u201cMy God\u201d). However, the alignment of (b) is better than that of (a) in this case.\nFigure 5 shows an example of \u201c+DP-gen. Input N-best\u201d translation. Here, (a) is the original Chinese sentence and its translation; (b) is the 1-best DP-generated Chinese sentence and its MT output; (c) stands for 2-best, 4-best and 6-best DP-generated Chinese sentences and their MT outputs (which are all the same); (d) is the 8-best DP-generated Chinese sentence and its MT output; (e) is the reference. The N -best DP candidate list is \u2329 \u6211 \u232a (I), \u2329 \u4f60 \u232a\n(You),\u2329 \u4ed6 \u232a (He), \u2329 \u6211\u4eec \u232a (We), \u2329 \u4ed6\u4eec \u232a (They), \u2329 \u4f60\u4eec \u232a (You), \u2329 \u5b83 \u232a (It) and \u2329 \u5979 \u232a\n(She). In (b), when integrating an incorrect 1-best DP into MT, we obtain the wrong translation. However, in (c), when considering more DPs (2-/4-/6-best), the SMT system\ngenerates a perfect translation by weighting the DP candidates during decoding. When further increasing N (8-best), (d) shows a wrong translation again due to increased noise."}, {"heading": "6 Conclusion and Future Work", "text": "We have presented a novel approach to recall missing pronouns for machine translation from a prodrop language to a non-pro-drop language. Experiments show that it is crucial to identify the DP to improve the overall translation performance. Our analysis shows that insertion of DPs affects the translation in a large extent.\nOur main findings in this paper are threefold:\n\u2022 Bilingual information can help to build monolingual models without any manually annotated training data;\n\u2022 Benefiting from representation learning, neural network-based models work well without complex feature engineering work;\n\u2022 N -best DP integration works better than 1-best insertion.\nIn future work, we plan to extend our work to different genres, languages and other kinds of dropped words to validate the robustness of our approach."}, {"heading": "Acknowledgments", "text": "This work is supported by the Science Foundation of Ireland (SFI) ADAPT project (Grant No.:13/RC/2106), and partly supported by the DCU-Huawei Joint Project (Grant No.:201504032A (DCU), YB2015090061 (Huawei)). It is partly supported by the Open Projects Program of National Laboratory of Pattern Recognition (Grant 201407353) and the Open Projects Program of Centre of Translation of GDUFS (Grant CTS201501)."}], "references": [{"title": "Theano: A cpu and gpu math expression compiler in python", "author": ["Olivier Breuleux", "Frederic Bastien", "Pascal Lamblin", "Razvan Pascanu", "Guillaume Desjardins", "Joseph Turian", "David WardeFarley", "Yoshua Bengio"], "venue": null, "citeRegEx": "Bergstra et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Bergstra et al\\.", "year": 2010}, {"title": "Language-independent parsing with empty elements. In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies: Short Papers ", "author": ["Cai et al.2011] Shu Cai", "David Chiang", "Yoav Goldberg"], "venue": null, "citeRegEx": "Cai et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Cai et al\\.", "year": 2011}, {"title": "Chinese zero pronoun resolution: Some recent advances", "author": ["Chen", "Ng2013] Chen Chen", "Vincent Ng"], "venue": "In Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing,", "citeRegEx": "Chen et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Chen et al\\.", "year": 2013}, {"title": "Effects of empty categories on machine translation", "author": ["Chung", "Gildea2010] Tagyoung Chung", "Daniel Gildea"], "venue": "In Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing,", "citeRegEx": "Chung et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Chung et al\\.", "year": 2010}, {"title": "The European linguistic area: standard average European", "author": ["Martin Haspelmath"], "venue": "In Language typology and language universals. (Handbu\u0308cher zur Sprach-und Kommunikationswissenschaft),", "citeRegEx": "Haspelmath.,? \\Q2001\\E", "shortCiteRegEx": "Haspelmath.", "year": 2001}, {"title": "On the distribution and reference of empty pronouns", "author": ["C.-T. James Huang"], "venue": "Linguistic Inquiry,", "citeRegEx": "Huang.,? \\Q1984\\E", "shortCiteRegEx": "Huang.", "year": 1984}, {"title": "A tree kernel-based unified framework for chinese zero anaphora resolution", "author": ["Kong", "Zhou2010] Fang Kong", "Guodong Zhou"], "venue": "In Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing,", "citeRegEx": "Kong et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Kong et al\\.", "year": 2010}, {"title": "Aiding pronoun translation with co-reference resolution", "author": ["Le Nagard", "Koehn2010] Ronan Le Nagard", "Philipp Koehn"], "venue": "In Proceedings of the Joint 5th Workshop on Statistical Machine Translation and MetricsMATR,", "citeRegEx": "Nagard et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Nagard et al\\.", "year": 2010}, {"title": "Weighted alignment matrices for statistical machine translation", "author": ["Liu et al.2009] Yang Liu", "Tian Xia", "Xinyan Xiao", "Qun Liu"], "venue": "In Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing: Volume", "citeRegEx": "Liu et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Liu et al\\.", "year": 2009}, {"title": "A novel graph-based compact representation of word alignment", "author": ["Liu et al.2013] Qun Liu", "Zhaopeng Tu", "Shouxun Lin"], "venue": "In Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers),", "citeRegEx": "Liu et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Liu et al\\.", "year": 2013}, {"title": "Investigation of recurrent-neural-network architectures and learning methods for spoken language understanding", "author": ["Xiaodong He", "Li Deng", "Yoshua Bengio"], "venue": "In Proceedings of the 14th Annual Conference", "citeRegEx": "Mesnil et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Mesnil et al\\.", "year": 2013}, {"title": "Distributed representations of words and phrases and their compositionality", "author": ["Ilya Sutskever", "Kai Chen", "Greg S Corrado", "Jeff Dean"], "venue": "In Proceedings of the 27th Annual Conference on Neural Information Processing Sys-", "citeRegEx": "Mikolov et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Mikolov et al\\.", "year": 2013}, {"title": "Cross-lingual coreference resolution of pronouns", "author": ["Novak", "Zabokrtsky2014] Michal Novak", "Zdenek Zabokrtsky"], "venue": "In Proceedings of the 25th International Conference on Computational Linguistics,", "citeRegEx": "Novak et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Novak et al\\.", "year": 2014}, {"title": "A systematic comparison of various statistical alignment models", "author": ["Och", "Ney2003] Franz Josef Och", "Hermann Ney"], "venue": "Computational Linguistics,", "citeRegEx": "Och et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Och et al\\.", "year": 2003}, {"title": "Minimum error rate training in statistical machine translation", "author": ["Franz Josef Och"], "venue": "In Proceedings of the 41st Annual Meeting on Association", "citeRegEx": "Och.,? \\Q2003\\E", "shortCiteRegEx": "Och.", "year": 2003}, {"title": "Bleu: A method for automatic evaluation of machine translation", "author": ["Salim Roukos", "Todd Ward", "Wei-Jing Zhu"], "venue": "In Proceedings of the 40th Annual Meeting on Association for Computational Linguistics,", "citeRegEx": "Papineni et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Papineni et al\\.", "year": 2002}, {"title": "CoNLL-2012 shared task: Modeling multilingual unrestricted coreference in ontonotes", "author": ["Alessandro Moschitti", "Nianwen Xue", "Olga Uryupina", "Yuchen Zhang"], "venue": "In Proceedings of the 15th Conference on Computational", "citeRegEx": "Pradhan et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Pradhan et al\\.", "year": 2012}, {"title": "Generative and discriminative algorithms for spoken language understanding", "author": ["Raymond", "Riccardi2007] Christian Raymond", "Giuseppe Riccardi"], "venue": "In Proceedings of 8th Annual Conference of the International Speech Communication Association,", "citeRegEx": "Raymond et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Raymond et al\\.", "year": 2007}, {"title": "Combining outputs from multiple machine translation systems", "author": ["Necip Fazil Ayan", "Bing Xiang", "Spyridon Matsoukas", "Richard M Schwartz", "Bonnie J Dorr"], "venue": "In Proceedings of the Human Language Technology", "citeRegEx": "Rosti et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Rosti et al\\.", "year": 2007}, {"title": "Srilm - an extensible language modeling toolkit", "author": ["Andreas Stolcke"], "venue": "In Proceedings of the 7th International Conference on Spoken Language Processing,", "citeRegEx": "Stolcke.,? \\Q2002\\E", "shortCiteRegEx": "Stolcke.", "year": 2002}, {"title": "Zero pronoun resolution can improve the quality of j-e translation", "author": ["Katsuhito Sudoh", "Masaaki Nagata"], "venue": "In Proceedings of the 6th Workshop on Syntax, Semantics and Structure in Statistical Translation,", "citeRegEx": "Taira et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Taira et al\\.", "year": 2012}, {"title": "Dependency forest for statistical machine translation", "author": ["Tu et al.2010] Zhaopeng Tu", "Yang Liu", "Young-Sook Hwang", "Qun Liu", "Shouxun Lin"], "venue": "In Proceedings of the 23rd International Conference on Computational Linguistics,", "citeRegEx": "Tu et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Tu et al\\.", "year": 2010}, {"title": "Extracting Hierarchical Rules from a Weighted Alignment Matrix", "author": ["Tu et al.2011] Zhaopeng Tu", "Yang Liu", "Qun Liu", "Shouxun Lin"], "venue": "In Proceedings of the 5th International Joint Conference on Natural Language Processing,", "citeRegEx": "Tu et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Tu et al\\.", "year": 2011}, {"title": "The automatic construction of discourse corpus for dialogue translation", "author": ["Wang et al.2016] Longyue Wang", "Xiaojun Zhang", "Zhaopeng Tu", "Andy Way", "Qun Liu"], "venue": "In Proceedings of the 10th Language Resources and Evaluation Conference,", "citeRegEx": "Wang et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Wang et al\\.", "year": 2016}, {"title": "Enlisting the ghost: Modeling empty categories for machine translation", "author": ["Xiang et al.2013] Bing Xiang", "Xiaoqiang Luo", "Bowen Zhou"], "venue": "In Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics", "citeRegEx": "Xiang et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Xiang et al\\.", "year": 2013}, {"title": "Dependency-based empty category detection via phrase structure trees", "author": ["Xue", "Yang2013] Nianwen Xue", "Yaqin Yang"], "venue": "In Proceedings of the 2013 Conference of the North American Chapter of the Association for Computational Linguistics: Human Lan-", "citeRegEx": "Xue et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Xue et al\\.", "year": 2013}, {"title": "The Penn Chinese Treebank: Phrase structure annotation of a large corpus", "author": ["Xue et al.2005] Naiwen Xue", "Fei Xia", "Fu-Dong Chiou", "Marta Palmer"], "venue": "Natural language engineering,", "citeRegEx": "Xue et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Xue et al\\.", "year": 2005}, {"title": "Chasing the ghost: recovering empty categories in the Chinese treebank", "author": ["Yang", "Xue2010] Yaqin Yang", "Nianwen Xue"], "venue": "In Proceedings of the 23rd International Conference on Computational Linguistics: Posters,", "citeRegEx": "Yang et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Yang et al\\.", "year": 2010}, {"title": "Recovering dropped pronouns from Chinese text messages", "author": ["Yang et al.2015] Yaqin Yang", "Yalin Liu", "Nianwen Xu"], "venue": "In Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Nat-", "citeRegEx": "Yang et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Yang et al\\.", "year": 2015}, {"title": "Dual subtitles as parallel corpora", "author": ["Zhang et al.2014] Shikun Zhang", "Wang Ling", "Chris Dyer"], "venue": "In Proceedings of the 10th International Conference on Language Resources and Evaluation,", "citeRegEx": "Zhang et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Zhang et al\\.", "year": 2014}, {"title": "Identification and resolution of Chinese zero pronouns: A machine learning approach", "author": ["Zhao", "Ng2007] Shanheng Zhao", "Hwee Tou Ng"], "venue": "In Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Compu-", "citeRegEx": "Zhao et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Zhao et al\\.", "year": 2007}], "referenceMentions": [{"referenceID": 28, "context": "In pro-drop languages, certain classes of pronouns can be omitted to make the sentence compact yet comprehensible when the identity of the pronouns can be inferred from the context (Yang et al., 2015).", "startOffset": 181, "endOffset": 200}, {"referenceID": 5, "context": "Figure 1 shows an example, in which Chinese is a pro-drop language (Huang, 1984), while English is Figure 1: Examples of dropped pronouns in a parallel dialogue", "startOffset": 67, "endOffset": 80}, {"referenceID": 4, "context": "not (Haspelmath, 2001).", "startOffset": 4, "endOffset": 22}, {"referenceID": 28, "context": "Generally, this phenomenon is more common in informal genres such as dialogues and conversations than others (Yang et al., 2015).", "startOffset": 109, "endOffset": 128}, {"referenceID": 28, "context": "works either apply manual annotation (Yang et al., 2015) or use existing but small-scale resources such as the Penn Treebank (Chung and Gildea, 2010; Xiang et al.", "startOffset": 37, "endOffset": 56}, {"referenceID": 24, "context": ", 2015) or use existing but small-scale resources such as the Penn Treebank (Chung and Gildea, 2010; Xiang et al., 2013).", "startOffset": 76, "endOffset": 120}, {"referenceID": 18, "context": "To alleviate the propagation of DP prediction errors, we feed the translation system N -best prediction results via confusion network decoding (Rosti et al., 2007).", "startOffset": 143, "endOffset": 163}, {"referenceID": 15, "context": "61 BLEU points (Papineni et al., 2002) using the additional translation model trained on the DP-inserted cor-", "startOffset": 15, "endOffset": 38}, {"referenceID": 10, "context": "In light of the recent success of applying deep neural network technologies in natural language processing (Raymond and Riccardi, 2007; Mesnil et al., 2013), we propose a neural network-based DP generator via the DP-inserted corpus (Section 2.", "startOffset": 107, "endOffset": 156}, {"referenceID": 11, "context": "Word embeddings (Mikolov et al., 2013) are used for our generation models: given a word w(t), we try to produce an embedding representation v(t) \u2208 Rd where d is the dimension of the representation vectors.", "startOffset": 16, "endOffset": 38}, {"referenceID": 10, "context": "We employ an RNN (Mesnil et al., 2013) to learn the dependency of sentences, which can be formulated as Equation (2):", "startOffset": 17, "endOffset": 38}, {"referenceID": 24, "context": "We select a number of features based on previous work (Xiang et al., 2013; Yang et al., 2015), including lexical, contextual, and syntax features (as shown in Table 2).", "startOffset": 54, "endOffset": 93}, {"referenceID": 28, "context": "We select a number of features based on previous work (Xiang et al., 2013; Yang et al., 2015), including lexical, contextual, and syntax features (as shown in Table 2).", "startOffset": 54, "endOffset": 93}, {"referenceID": 8, "context": "Recent studies have shown that SMT systems can benefit from widening the annotation pipeline (Liu et al., 2009; Tu et al., 2010; Tu et al., 2011; Liu et al., 2013).", "startOffset": 93, "endOffset": 163}, {"referenceID": 21, "context": "Recent studies have shown that SMT systems can benefit from widening the annotation pipeline (Liu et al., 2009; Tu et al., 2010; Tu et al., 2011; Liu et al., 2013).", "startOffset": 93, "endOffset": 163}, {"referenceID": 22, "context": "Recent studies have shown that SMT systems can benefit from widening the annotation pipeline (Liu et al., 2009; Tu et al., 2010; Tu et al., 2011; Liu et al., 2013).", "startOffset": 93, "endOffset": 163}, {"referenceID": 9, "context": "Recent studies have shown that SMT systems can benefit from widening the annotation pipeline (Liu et al., 2009; Tu et al., 2010; Tu et al., 2011; Liu et al., 2013).", "startOffset": 93, "endOffset": 163}, {"referenceID": 1, "context": "Another line related to DP generation is using a wider range of empty categories (EC) (Yang and Xue, 2010; Cai et al., 2011; Xue and Yang, 2013), which aims to recover longdistance dependencies, discontinuous constituents and certain dropped elements3 in phrase structure treebanks (Xue et al.", "startOffset": 86, "endOffset": 144}, {"referenceID": 26, "context": ", 2011; Xue and Yang, 2013), which aims to recover longdistance dependencies, discontinuous constituents and certain dropped elements3 in phrase structure treebanks (Xue et al., 2005).", "startOffset": 165, "endOffset": 183}, {"referenceID": 1, "context": "Another line related to DP generation is using a wider range of empty categories (EC) (Yang and Xue, 2010; Cai et al., 2011; Xue and Yang, 2013), which aims to recover longdistance dependencies, discontinuous constituents and certain dropped elements3 in phrase structure treebanks (Xue et al., 2005). This work mainly focus on sentence-internal characteristics as opposed to contextual information at the discourse level. More recently, Yang et al. (2015) explore DP recovery for Chinese text messages based on both lines of work.", "startOffset": 107, "endOffset": 457}, {"referenceID": 20, "context": "These methods can also be used for DP translation using SMT (Chung and Gildea, 2010; Le Nagard and Koehn, 2010; Taira et al., 2012; Xiang et al., 2013).", "startOffset": 60, "endOffset": 151}, {"referenceID": 24, "context": "These methods can also be used for DP translation using SMT (Chung and Gildea, 2010; Le Nagard and Koehn, 2010; Taira et al., 2012; Xiang et al., 2013).", "startOffset": 60, "endOffset": 151}, {"referenceID": 20, "context": "These methods can also be used for DP translation using SMT (Chung and Gildea, 2010; Le Nagard and Koehn, 2010; Taira et al., 2012; Xiang et al., 2013). Taira et al. (2012) propose both sim-", "startOffset": 112, "endOffset": 173}, {"referenceID": 16, "context": "their results are not convincing due to the poor performance of the CR method (Pradhan et al., 2012).", "startOffset": 78, "endOffset": 100}, {"referenceID": 16, "context": "their results are not convincing due to the poor performance of the CR method (Pradhan et al., 2012). Chung and Gildea (2010) systematically examine the effects of EC on MT with three methods: pattern, CRF (which achieves best results) and parsing.", "startOffset": 79, "endOffset": 126}, {"referenceID": 23, "context": "13TB Chinese Web Page Collection Corpus5 while the other one is trained on all extracted 7M English subtitle data (Wang et al., 2016).", "startOffset": 114, "endOffset": 133}, {"referenceID": 19, "context": "thermore, we train 5-gram language models using the SRI Language Toolkit (Stolcke, 2002).", "startOffset": 73, "endOffset": 88}, {"referenceID": 14, "context": "6 We use minimum error rate training (Och, 2003) to optimize the feature weights.", "startOffset": 37, "endOffset": 48}, {"referenceID": 0, "context": "The RNN models are implemented using the common Theano neural network toolkit (Bergstra et al., 2010).", "startOffset": 78, "endOffset": 101}, {"referenceID": 15, "context": "For end-to-end evaluation, case-insensitive BLEU (Papineni et al., 2002) is used to measure", "startOffset": 49, "endOffset": 72}, {"referenceID": 29, "context": "Dual Subtitles \u2013 Mandarin-English Subtitles Parallel Corpus, extracted by Zhang et al. (2014) without contextual information at the discourse level.", "startOffset": 74, "endOffset": 94}], "year": 2016, "abstractText": "Dropped Pronouns (DP) in which pronouns are frequently dropped in the source language but should be retained in the target language are challenge in machine translation. In response to this problem, we propose a semisupervised approach to recall possibly missing pronouns in the translation. Firstly, we build training data for DP generation in which the DPs are automatically labelled according to the alignment information from a parallel corpus. Secondly, we build a deep learning-based DP generator for input sentences in decoding when no corresponding references exist. More specifically, the generation is two-phase: (1) DP position detection, which is modeled as a sequential labelling task with recurrent neural networks; and (2) DP prediction, which employs a multilayer perceptron with rich features. Finally, we integrate the above outputs into our translation system to recall missing pronouns by both extracting rules from the DP-labelled training data and translating the DP-generated input sentences. Experimental results show that our approach achieves a significant improvement of 1.58 BLEU points in translation performance with 66% F-score for DP generation accuracy.", "creator": "LaTeX with hyperref package"}}}