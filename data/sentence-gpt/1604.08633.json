{"id": "1604.08633", "review": {"conference": "EMNLP", "VERSION": "v1", "DATE_OF_SUBMISSION": "28-Apr-2016", "title": "Word Ordering Without Syntax", "abstract": "Recent work on word ordering has argued that syntactic structure is important, or even required, for effectively recovering the order of a sentence. We find that, in fact, an n-gram language model with a simple heuristic gives strong results on this task. Furthermore, we show that a long short-term memory (LSTM) language model is comparatively effective at recovering order, with our basic model outperforming a state-of-the-art syntactic model by 11.5 BLEU points. Additional data and larger beams yield further gains, at the expense of training and search time.\n\n\n\n\n\n\n\n\nThe second paper, published in The Journal of Computer Science, argues that LSTM can be used to perform a task that produces the necessary content and order for a given sentence. Specifically, we show that we can perform the task of solving a list of simple sentences for a sentence that can be retrieved by a lstm task with the same precision of LSTM. LSTM supports multiple forms of LSTM in the application. We show that LSTM is useful for training LSTM for learning and training problems in learning and training problems in LSTM.\nThe third paper, published in The Journal of Computer Science, argues that LSTM can be used to perform a task that produces the necessary content and order for a given sentence. Finally, we show that LSTM is helpful in training LSTM in the application. We show that LSTM can be used to perform a task that produces the necessary content and order for a given sentence.\nThe Fourth paper, published in Computer Science, argues that LSTM can be used to perform a task that produces the necessary content and order for a given sentence. Additionally, we show that LSTM can be used to perform a task that produces the necessary content and order for a given sentence. Finally, we show that LSTM can be used to perform a task that produces the necessary content and order for a given sentence.\nThese results suggest that LSTM can be used to perform a task that produces the necessary content and order for a given sentence. Therefore, the final version of this paper provides a clearer approach to the LSTM concept of LSTM, as in the present paper, which is a relatively small group of work.", "histories": [["v1", "Thu, 28 Apr 2016 22:09:49 GMT  (182kb,D)", "http://arxiv.org/abs/1604.08633v1", null], ["v2", "Sat, 24 Sep 2016 03:41:45 GMT  (137kb)", "http://arxiv.org/abs/1604.08633v2", "EMNLP 2016"]], "reviews": [], "SUBJECTS": "cs.CL", "authors": ["allen schmaltz", "alexander m rush", "stuart m shieber"], "accepted": true, "id": "1604.08633"}, "pdf": {"name": "1604.08633.pdf", "metadata": {"source": "CRF", "title": "Word Ordering Without Syntax", "authors": ["Allen Schmaltz", "Stuart M. Shieber"], "emails": [], "sections": [{"heading": "1 Introduction", "text": "The task of word ordering, or linearization, is to recover the original order of a shuffled sentence. The task has been standardized in a recent line of research as a method useful for isolating the performance of text-to-text generation models (Zhang and Clark, 2011; Liu et al., 2015; Liu and Zhang, 2015; Zhang and Clark, 2015). The predominant argument of these works is that jointly recovering explicit syntactic structure is crucial for determining the correct word order of the original sentence. As such, these methods either generate or rely on given parse structure to reproduce the order.\nIndependently, Elman (1990) also explored linearization in his seminal work on recurrent neural networks. Elman judged the capacity of early recurrent neural networks via, in part, the network\u2019s ability to predict word order in simple sentences. He notes that,\nThe order of words in sentences reflects a number of constraints. . . Syntactic structure, selective restrictions, subcategorization, and discourse considerations are among the many factors which join together to fix the order in which words occur. . . [T]here is an abstract structure which underlies the surface strings and it is this structure which provides a more insightful basis for understanding the constraints on word order. . . . It is, therefore, an interesting question to ask whether a network can learn any aspects of that underlying abstract structure (Elman, 1990).\nRecently, recurrent neural networks have reemerged as a powerful tool for learning the latent structure of language. In particular, work on long short-term memory (LSTM) networks for language modeling has shown improvements in perplexity.\nWe revisit Elman\u2019s question by applying LSTMs to the word-ordering task, without any explicit syntactic modeling. We find that language models are in general effective for linearization relative to existing syntactic approaches, with LSTMs in particular outperforming the state-of-the-art by 11.5 BLEU points, with further gains observed when training with additional text and decoding with larger beams."}, {"heading": "2 Background: Linearization", "text": "The task of linearization is to recover the original order of a shuffled sentence. We assume a vocabulary V , and are given a sequence of out-of-order phrases x1, . . . , xN , with xn \u2208 V+ for 1 \u2264 n \u2264 N . Define M as the total number of tokens (i.e., the sum of the lengths of the phrases). We consider two varieties of the task: (1) WORDS, where each xn consists of a single word and M = N , and (2) WORDS+BNPS,\nar X\niv :1\n60 4.\n08 63\n3v 1\n[ cs\n.C L\n] 2\n8 A\npr 2\nwhere base noun phrases (noun phrases not containing inner noun phrases) are also provided and M \u2265 N . While the first is closer to Elman\u2019s formulation, the second has become more established in recent literature.\nGiven input x, we define the output set Y to be all possible permutations over the N elements of x, where y\u0302 \u2208 Y is the permutation generating the true order. We aim to find y\u0302, or a permutation close to it. We produce a linearization by (approximately) optimizing a learned scoring function f over the set of permutations, y\u2217 = arg maxy\u2208Y f(x, y)."}, {"heading": "3 Related Work: Syntactic Linearization", "text": "Recent approaches to linearization have been based on reconstructing the syntactic structure to produce the word order. Let Z represent all projective dependency parse trees over M words. The objective is to find y\u2217, z\u2217 = arg maxy\u2208Y,z\u2208Z f(x, y, z) where f is now over both the syntactic structure and the linearization. The current state of the art on the Penn Treebank (PTB) (Marcus et al., 1993), without external data, of Liu et al. (2015) uses a transitionbased parser with beam search to construct a sentence and a parse tree. The scoring function is a linear model f(x, y) = \u03b8>\u03a6(x, y, z), and is trained with an early update structured perceptron to match both a given order and syntactic tree. The feature function \u03a6 includes features on the syntactic tree. This work improves upon past work which used best-first search over a similar objective (Zhang and Clark, 2011).\nIn follow-up work, Liu and Zhang (2015) argue that syntactic models yield improvements over pure surface n-gram models for the WORDS+BNPS case. This result holds particularly on longer sentences and even when the syntactic trees used in training are of low quality. The n-gram decoder of this work utilizes a single beam, discarding the probabilities of internal, non-boundary words in the BNPs when comparing hypotheses. Our work revisits this comparison between syntactic models and surfacelevel models, utilizing a surface-level decoder with heuristic future costs and an alternative approach for scoring partial hypotheses for the WORDS+BNPS case, which we discuss below.\nAlgorithm 1 LM beam-search word ordering 1: procedure ORDER(x1 . . . xN , K, g) 2: B0 \u2190 \u3008(\u3008\u3009, {1, . . . , N}, 0,h0)\u3009 3: for m = 0, . . . ,M \u2212 1 do 4: for k = 1, . . . , |Bm| do 5: (y,R, s,h)\u2190 B(k)m 6: for i \u2208 R do 7: (s\u2032,h\u2032)\u2190 (s,h) 8: for word w in phrase xi do 9: s\u2032 \u2190 s\u2032 + log q(w,h\u2032) 10: h\u2032 \u2190 \u03b4(w,h\u2032) 11: j \u2190 m+ |xi| 12: Bj \u2190 Bj + (y + xi,R\u2212 i, s\u2032,h\u2032) 13: keep top-K of Bj by f(x, y) + g(R) 14: return BM"}, {"heading": "4 LM-Based Linearization", "text": "In contrast to most past work, we use an LM directly for word ordering. We consider two types of language models: an n-gram model and a long shortterm memory network (Hochreiter and Schmidhuber, 1997). For the purpose of this work, we define a common abstraction for both models. Let h \u2208 H be the current state of the model, with h0 as the initial state. Upon seeing a word wi \u2208 V , the LM advances to a new state hi = \u03b4(wi,hi\u22121). At any time, the LM can be queried to produce an estimate of the probability of the next word q(wi,hi\u22121) \u2248 p(wi | w1, . . . , wi\u22121). For n-gram language models, H, \u03b4, and q can naturally be defined respectively as the state space, transition model, and edge costs of a finite-state machine.\nLSTMs are a type of recurrent neural network (RNN) that are conducive to learning long-distance dependencies through the use of an internal memory cell. Existing work with LSTMs has generated stateof-the-art results in language modeling (Zaremba et al., 2014), along with a variety of other NLP tasks. On the Penn Treebank the basic LSTM architecture we use has been shown to have a perplexity of 82 compared to 141 for a 5-gram Kneser-Ney language model (Kneser and Ney, 1995).\nIn our notation, we define H as the hidden states and cell states of a multi-layer LSTM, \u03b4 as the LSTM update function, and q as a final affine transformation and softmax given as q(\u2217,hi\u22121; \u03b8q) = softmax(Wh\n(L) i\u22121 + b) where h (L) i\u22121 is the top hid-\nden layer and \u03b8q = (W , b) are parameters. We direct readers to the work of Graves (2013) for a full description of the LSTM update.\nUnder both models, we simply define the scoring function as\nf(x, y) = N\u2211\nn=1\nlog p(xy(n) | xy(1), . . . xy(n\u22121))\nwhere the phrase probabilities are calculated wordby-word by our language model.\nSearching over all permutations Y is intractable, so we instead follow past work on linearization (Liu et al., 2015) and LSTM generation (Sutskever et al., 2014) in adapting beam search for our generation step. Our work differs from the beam search approach for the WORDS+BNPS case of previous work in that we maintain multiple beams, as in stack decoding for phrase-based machine translation (Koehn, 2010), allowing us to incorporate the probabilities of internal, non-boundary words in the BNPs. Additionally, for both WORDS and WORDS+BNPS, we also include a future estimate in order to improve search accuracy.\nBeam search maintains M + 1 beams, B0, . . . , BM , each containing at most the topK partial hypotheses of that length. A partial hypothesis is a 4-tuple (y,R, s,h), where y is a partial ordering,R is the set of remaining indices to be ordered, s is the score of the partial linearization f(x, y), and h is the current LM state. Each step consists of expanding all next possible phrases and adding the next hypothesis to a later beam. The full beam search is given in Algorithm 1.\nAs part of the beam search scoring function we also include a future cost g, an estimate of the score contribution of the remaining elements in R. Together, f(x, y) + g(R) gives a noisy estimate of the total score, which is used to determine the K best elements in the beam. In our experiments we use a very simple unigram future cost estimate, g(R) =\u2211\ni\u2208R \u2211 w\u2208xi log p(w)."}, {"heading": "5 Experiments", "text": "Setup Experiments are on PTB, with sections 2- 21 as training, 22 as validation, and 23 as test1. Note\n1In practice, the results in Liu et al. (2015) and Liu and Zhang (2015) use section 0 instead of 22 for validation (author\nthat we train with true-cases and punctuation, resulting in a vocabulary containing 16,159 types from around 1M training tokens. We utilize two UNK types, one for initial upper-case tokens and one for all other low-frequency tokens, end sentence tokens, and start/end tokens, which are treated as words, to mark BNPs for the WORDS+BNPS task.\nFor experiments marked GW we augment the PTB with a subset of the Annotated Gigaword corpus (Napoles et al., 2012). We follow Liu and Zhang (2015) and train on a sample of 900k sentences from AFP, augmented with the PTB training set. The GW models benefit from both additional data and a larger vocabulary of 24,998 types, which reduces\ncorrespondence).\nunknowns in the validation and test sets. We compare the models of Liu et al. (2015) (known as ZGEN), a 5-gram LM using Kneser-Ney smoothing (NGRAM), and an LSTM. We experiment on the WORDS and WORDS+BNPS tasks, and also experiment with including future costs (g), the Gigaword data (GW), and varying beam size. We retrain ZGEN using publicly available code2 to replicate published results.\nThe LSTM is based on the LSTM-Medium setup of Zaremba et al. (2014). The LSTM beam search is implemented on a GPU with batched forward operations for efficiency. Our implementation uses the Torch3 framework and will be publicly available.\nWe compare the performance of the models using the BLEU metric (Papineni et al., 2002). In generation if there are multiple tokens of identical UNK type, we randomly replace each with possible unused tokens in the original source before calculating BLEU.\nResults Our main results are shown in Table 1. On the WORDS+BNPS task the NGRAM-64 model scores nearly 5 points higher than the syntax-based model ZGEN-64. The LSTM-64 then surpasses\n2https://github.com/SUTDNLP/ZGen 3http://torch.ch\nNGRAM-64 by more than 5 BLEU points. Differences on the WORDS task are smaller, but show a similar pattern. Incorporating Gigaword further increases the result another 2 points. Notably, the NGRAM model outperforms the combined result of ZGEN-64+LM+GW+POS from Liu and Zhang (2015), which uses a 4-gram model trained on Gigaword. We believe this is because the combined ZGEN model incorporates the n-gram scores as discretized indicator features instead of using the probability directly4. We also include results with beam 512 which yields a further improvement at the cost of search time.\nTo further explore the impact of search accuracy, Table 2 shows the results of various models with beam widths ranging from 1 (greedy search) to 512, and also with and without future costs g. We see that for the better models there is a steady increase in accuracy even with large beams, indicating that search errors are made even with relatively large beams.\nOne proposed advantage of syntax in linearization models is that it can better capture long-distance relationships. We explore this in Figure 1 which shows results by sentence length and distortion. Distortion is defined as the absolute difference between a token\u2019s index position in y\u2217 and y\u0302, normalized by M . We find that the LSTM model exhibits consistently better performance than existing syntax models across sentence lengths and generates fewer long-range distortions.\n4In Liu and Zhang (2015), with the given decoder, N-grams only yielded a small further improvement over the syntactic models when discretized versions of the LM probabilities were incorporated as indicator features in the syntactic models.\nFinally, Table 3 compares the syntactic fluency of the output. As a lightweight test, we parse the output with the Yara Parser (Rasooli and Tetreault, 2015) and compare the unlabeled attachment scores (UAS) to the trees produced by the syntactic system. We first align the gold head to each output token. (In cases where the alignment is not one-to-one, we randomly sample among the possibilities.) The models with no knowledge of syntax are able to recover a higher proportion of gold arcs."}, {"heading": "6 Conclusion", "text": "We have demonstrated that strong surface-level language models recover word order more accurately than previous models trained with explicit syntactic annotations. We have also provided strong baselines on the task by which the community can compare future text-generation models."}, {"heading": "Acknowledgments", "text": "We thank Yue Zhang and Jiangming Liu for assistance in using ZGen, as well as verification of the task setup for a valid comparison."}], "references": [{"title": "Finding structure in time", "author": ["Jeffrey L. Elman."], "venue": "Cog-", "citeRegEx": "Elman.,? 1990", "shortCiteRegEx": "Elman.", "year": 1990}, {"title": "Transition-based syntactic linearization", "author": ["Yijia Liu", "Yue Zhang", "Wanxiang Che", "Bing Qin."], "venue": "Proceedings of the 2015 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies,", "citeRegEx": "Liu et al\\.,? 2015", "shortCiteRegEx": "Liu et al\\.", "year": 2015}, {"title": "Building a large annotated corpus of english: The penn treebank", "author": ["Mitchell P. Marcus", "Beatrice Santorini", "Mary Ann Marcinkiewicz."], "venue": "Computational Linguistics, 19(2):313\u2013330.", "citeRegEx": "Marcus et al\\.,? 1993", "shortCiteRegEx": "Marcus et al\\.", "year": 1993}, {"title": "Annotated gigaword", "author": ["Courtney Napoles", "Matthew Gormley", "Benjamin Van Durme."], "venue": "Proceedings of the Joint Workshop on Automatic Knowledge Base Construction and Web-scale Knowledge Extraction, AKBC-WEKEX \u201912, pages 95\u2013100, Strouds-", "citeRegEx": "Napoles et al\\.,? 2012", "shortCiteRegEx": "Napoles et al\\.", "year": 2012}, {"title": "Bleu: A method for automatic evaluation of machine translation", "author": ["Kishore Papineni", "Salim Roukos", "Todd Ward", "WeiJing Zhu."], "venue": "Proceedings of the 40th Annual Meeting on Association for Computational Linguistics, ACL \u201902, pages 311\u2013318, Strouds-", "citeRegEx": "Papineni et al\\.,? 2002", "shortCiteRegEx": "Papineni et al\\.", "year": 2002}, {"title": "Yara parser: A fast and accurate dependency parser", "author": ["Mohammad Sadegh Rasooli", "Joel R. Tetreault."], "venue": "CoRR, abs/1503.06733.", "citeRegEx": "Rasooli and Tetreault.,? 2015", "shortCiteRegEx": "Rasooli and Tetreault.", "year": 2015}, {"title": "Sequence to sequence learning with neural networks", "author": ["Ilya Sutskever", "Oriol Vinyals", "Quoc VV Le."], "venue": "Advances in Neural Information Processing Systems, pages 3104\u20133112.", "citeRegEx": "Sutskever et al\\.,? 2014", "shortCiteRegEx": "Sutskever et al\\.", "year": 2014}, {"title": "Recurrent neural network regularization", "author": ["Wojciech Zaremba", "Ilya Sutskever", "Oriol Vinyals."], "venue": "CoRR, abs/1409.2329.", "citeRegEx": "Zaremba et al\\.,? 2014", "shortCiteRegEx": "Zaremba et al\\.", "year": 2014}, {"title": "Syntax-based grammaticality improvement using ccg and guided search", "author": ["Yue Zhang", "Stephen Clark."], "venue": "Proceedings of the Conference on Empirical Methods in Natural Language Processing, EMNLP \u201911, pages 1147\u20131157, Stroudsburg, PA,", "citeRegEx": "Zhang and Clark.,? 2011", "shortCiteRegEx": "Zhang and Clark.", "year": 2011}, {"title": "Discriminative syntax-based word ordering for text generation", "author": ["Yue Zhang", "Stephen Clark."], "venue": "Comput. Linguist., 41(3):503\u2013538, September.", "citeRegEx": "Zhang and Clark.,? 2015", "shortCiteRegEx": "Zhang and Clark.", "year": 2015}], "referenceMentions": [{"referenceID": 8, "context": "The task has been standardized in a recent line of research as a method useful for isolating the performance of text-to-text generation models (Zhang and Clark, 2011; Liu et al., 2015; Liu and Zhang, 2015; Zhang and Clark, 2015).", "startOffset": 143, "endOffset": 228}, {"referenceID": 1, "context": "The task has been standardized in a recent line of research as a method useful for isolating the performance of text-to-text generation models (Zhang and Clark, 2011; Liu et al., 2015; Liu and Zhang, 2015; Zhang and Clark, 2015).", "startOffset": 143, "endOffset": 228}, {"referenceID": 9, "context": "The task has been standardized in a recent line of research as a method useful for isolating the performance of text-to-text generation models (Zhang and Clark, 2011; Liu et al., 2015; Liu and Zhang, 2015; Zhang and Clark, 2015).", "startOffset": 143, "endOffset": 228}, {"referenceID": 0, "context": "It is, therefore, an interesting question to ask whether a network can learn any aspects of that underlying abstract structure (Elman, 1990).", "startOffset": 127, "endOffset": 140}, {"referenceID": 0, "context": "Independently, Elman (1990) also explored linearization in his seminal work on recurrent neural networks.", "startOffset": 15, "endOffset": 28}, {"referenceID": 2, "context": "The current state of the art on the Penn Treebank (PTB) (Marcus et al., 1993), without external data, of Liu et al.", "startOffset": 56, "endOffset": 77}, {"referenceID": 8, "context": "This work improves upon past work which used best-first search over a similar objective (Zhang and Clark, 2011).", "startOffset": 88, "endOffset": 111}, {"referenceID": 1, "context": ", 1993), without external data, of Liu et al. (2015) uses a transitionbased parser with beam search to construct a sentence and a parse tree.", "startOffset": 35, "endOffset": 53}, {"referenceID": 7, "context": "Existing work with LSTMs has generated stateof-the-art results in language modeling (Zaremba et al., 2014), along with a variety of other NLP tasks.", "startOffset": 84, "endOffset": 106}, {"referenceID": 1, "context": "Searching over all permutations Y is intractable, so we instead follow past work on linearization (Liu et al., 2015) and LSTM generation (Sutskever et al.", "startOffset": 98, "endOffset": 116}, {"referenceID": 6, "context": ", 2015) and LSTM generation (Sutskever et al., 2014) in adapting beam search for our generation step.", "startOffset": 28, "endOffset": 52}, {"referenceID": 1, "context": "In practice, the results in Liu et al. (2015) and Liu and Zhang (2015) use section 0 instead of 22 for validation (author Model WORDS WORDS+BNPS", "startOffset": 28, "endOffset": 46}, {"referenceID": 1, "context": "In practice, the results in Liu et al. (2015) and Liu and Zhang (2015) use section 0 instead of 22 for validation (author Model WORDS WORDS+BNPS", "startOffset": 28, "endOffset": 71}, {"referenceID": 3, "context": "For experiments marked GW we augment the PTB with a subset of the Annotated Gigaword corpus (Napoles et al., 2012).", "startOffset": 92, "endOffset": 114}, {"referenceID": 3, "context": "For experiments marked GW we augment the PTB with a subset of the Annotated Gigaword corpus (Napoles et al., 2012). We follow Liu and Zhang (2015) and train on a sample of 900k sentences from AFP, augmented with the PTB training set.", "startOffset": 93, "endOffset": 147}, {"referenceID": 1, "context": "We compare the models of Liu et al. (2015) (known as ZGEN), a 5-gram LM using Kneser-Ney smoothing (NGRAM), and an LSTM.", "startOffset": 25, "endOffset": 43}, {"referenceID": 7, "context": "The LSTM is based on the LSTM-Medium setup of Zaremba et al. (2014). The LSTM beam search is implemented on a GPU with batched forward operations for efficiency.", "startOffset": 46, "endOffset": 68}, {"referenceID": 4, "context": "We compare the performance of the models using the BLEU metric (Papineni et al., 2002).", "startOffset": 63, "endOffset": 86}, {"referenceID": 5, "context": "As a lightweight test, we parse the output with the Yara Parser (Rasooli and Tetreault, 2015) and compare the unlabeled attachment scores (UAS) to the trees produced by the syntactic system.", "startOffset": 64, "endOffset": 93}], "year": 2017, "abstractText": "Recent work on word ordering has argued that syntactic structure is important, or even required, for effectively recovering the order of a sentence. We find that, in fact, an n-gram language model with a simple heuristic gives strong results on this task. Furthermore, we show that a long short-term memory (LSTM) language model is comparatively effective at recovering order, with our basic model outperforming a state-of-the-art syntactic model by 11.5 BLEU points. Additional data and larger beams yield further gains, at the expense of training and search time.", "creator": "LaTeX with hyperref package"}}}