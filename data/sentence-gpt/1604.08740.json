{"id": "1604.08740", "review": {"conference": "NIPS", "VERSION": "v1", "DATE_OF_SUBMISSION": "29-Apr-2016", "title": "MetaGrad: Multiple Learning Rates in Online Learning", "abstract": "In online convex optimization it is well known that objective functions with curvature are much easier than arbitrary convex functions. Here we show that the regret can be significantly reduced even without curvature, in cases where there is a stable optimum to converge to. More precisely, the regret of existing methods is determined by the norms of the encountered gradients, and matching worst-case performance lower bounds tell us that this cannot be improved uniformly by doing the same procedure at all (see also).\n\n\nThe regret is a general condition in a problem (that can be solved by applying only a small number of parameters to the task). A particular problem is a particular type of convex function, which is quite common, and can be solved by a different set of functions. For instance, a type of convex function with a maximum range of tens of tens of tens of tens of tens of tens of tens of tens of tens of tens of tens of tens of tens of tens of tens of tens of tens of tens of tens of tens of tens of tens of tens of tens of tens of tens of tens of tens of tens of tens of tens of tens of tens of tens of tens of tens of tens of tens of tens of tens of tens of tens of tens of tens of tens of tens of tens of tens of tens of tens of tens of tens of tens of tens of tens of tens of tens of tens of tens of tens of tens of tens of tens of tens of tens of tens of tens of tens of tens of tens of tens of tens of tens of tens of tens of tens of tens of tens of tens of tens of tens of tens of tens of tens of tens of tens of tens of tens of tens of tens of tens of tens of tens of tens of tens of tens of tens of tens of tens of tens of tens of tens of tens of tens of tens of tens of tens of tens of tens of tens of tens of tens of tens of tens of tens of tens of tens of tens of tens of tens of tens of tens of tens of tens of tens of tens of tens of tens of tens of tens of tens of tens of tens of tens of tens of tens of tens of tens of tens of tens of tens of tens of tens of tens of tens of tens of tens of tens of tens of tens of tens of tens of tens of tens of tens of tens of tens of tens of tens of tens of tens of tens of tens of tens of tens of tens of tens of tens of tens of tens of tens of tens of", "histories": [["v1", "Fri, 29 Apr 2016 09:05:46 GMT  (43kb)", "http://arxiv.org/abs/1604.08740v1", null], ["v2", "Thu, 23 Jun 2016 22:25:38 GMT  (36kb)", "http://arxiv.org/abs/1604.08740v2", null], ["v3", "Tue, 1 Nov 2016 17:57:06 GMT  (37kb)", "http://arxiv.org/abs/1604.08740v3", null]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["tim van erven", "wouter m koolen"], "accepted": true, "id": "1604.08740"}, "pdf": {"name": "1604.08740.pdf", "metadata": {"source": "CRF", "title": "MetaGrad: Faster Convergence Without Curvature in Online Convex Optimization", "authors": ["Wouter M. Koolen", "Tim van Erven"], "emails": ["WMKOOLEN@CWI.NL", "TIM@TIMVANERVEN.NL"], "sections": [{"heading": null, "text": "ar X\niv :1\n60 4.\n08 74\n0v 1\n[ cs\n.L G\n] 2\n9 A\nMetaGrad does not require any manual tuning, but instead tunes a learning rate parameter automatically for the data. Unlike all previous methods with provable guarantees, its learning rates are not monotonically decreasing over time, but instead are based on a novel aggregation technique. We provide two versions of MetaGrad. The first maintains a full covariance matrix to guarantee the sharpest bounds for problems where we can afford update time quadratic in the dimension. The second version maintains only the diagonal. Its linear cost in the dimension makes it suitable for large-scale problems. Keywords: Online convex optimization, online learning, learning rate tuning, adaptive algorithms"}, {"heading": "1. Introduction", "text": "Methods for online convex optimization (OCO) (Shalev-Shwartz, 2012; Hazan, 2015) make it possible to optimize parameters sequentially, by processing convex functions in a streaming fashion. This is important in time series prediction where the data are inherently online; but it may also be convenient to process offline data sets sequentially, for instance if the data do not all fit into memory at the same time or if parameters need to be updated quickly when extra data become available.\nThe simplest and most commonly used strategy for OCO is online gradient descent (GD) (Zinkevich, 2003), which updates parameters wt+1 = wt \u2212 \u03b7\u2207ft(wt) by taking a step in the direction of the negative gradient. For carefully tuned step size \u03b7, this guarantees that the regret over T rounds, which measures the difference in performance between the GD iterates wt and the best offline parameters u, is bounded by (Shalev-Shwartz, 2012)\nRuT :=\nT\u2211\nt=1\nft(wt)\u2212 T\u2211\nt=1\nft(u) \u2264 O\n\n\n\u221a \u221a \u221a \u221a T\u2211\nt=1\n\u2016gt\u20162   , (1)\nc\u00a9 W.M. Koolen & T. van Erven.\nwhere gt := \u2207ft(wt) is the gradient of the function processed in the t-th step of the algorithm. The difficulty of the optimization problem depends on the sequence of functions f1, f2, . . . If the functions are nicely smooth and all have their minimum at the same point u, we can expect GD to converge to wt \u2192 u, such that the encountered gradients gt \u2192 0 vanish over time and hence by (1) the regret of GD plateaus. In most applications in machine learning, however, every function ft measures the loss on a different training example (Xt, Yt) and even though the average of all functions may have a clear minimum u, the noise in the data will prevent the individual gradients from going to zero even when the algorithm approaches u, and the guarantee (1) becomes of the worst-case order \u221a T .\nIf it is known a priori that the functions ft have sufficient curvature, the situation is dramatically better, and it is possible to achieve regret that scales as O(ln T ). For strongly convex functions this is achieved by GD with step size proportional to 1/t (Bartlett et al., 2007), while exp-concave functions require more sophisticated algorithms like Online Newton Step (Hazan et al., 2007).\nIn this paper we take a different approach to reducing regret, which does not depend on curvature of the functions. We introduce MetaGrad, a parameter-free algorithm that, up to a logarithmic factor, improves the regret bound (1) by replacing each gradient norm \u2016gt\u2016 by the quantity\n|(u\u2212wt)\u22bagt| = \u2016u\u2212wt\u2016 \ufe38 \ufe37\ufe37 \ufe38\ndistance\n\u2223 \u2223 \u2223 (u\u2212wt)\u22ba \u2016u\u2212wt\u2016 gt\n\ufe38 \ufe37\ufe37 \ufe38\ndirectional derivative\n\u2223 \u2223 \u2223, (2)\nwhich we interpret as a scale factor that measures the distance of the current parameters wt to the (unknown) offline best parameters u, times the directional derivative of the objective function ft in the direction of u. In this introduction, we assume for simplicity that u, wt and gt are all from the unit Euclidean ball. To compare the two bounds, let us first observe that the norm of the gradient \u2016gt\u2016 is equal to the directional derivative in the most difficult direction:\n\u2016gt\u2016 = max u (u\u2212wt)\u22ba \u2016u\u2212wt\u2016 gt. (3)\nThus, MetaGrad can be seen to perform better for two reasons: first, if gt points us in a wrong direction that is very different from u \u2212 wt, then the directional derivative in (2) will be smaller than \u2016gt\u2016. And second, most importantly, we gain a factor \u2016u\u2212wt\u2016 equal to the distance from wt to the optimum u. This means that the closer we get to u, the more the gradients are discounted, and consequently we can get an order of magnitude smaller regret even in cases where the gradients themselves do not vanish around u. We will give two examples where this happens in Section 1.1 below. We emphasize that MetaGrad needs no knowledge of the eventual global optimum u and will, in fact, guarantee a bound in terms of (2) compared to every point u of the domain.\nIn order to guarantee bounds with this refined dependence (2) on the data, MetaGrad maintains a covariance matrix of size d\u00d7 d where d is the parameter dimension. In the remainder of the paper we call this version full MetaGrad. We also design and analyze a faster approximation that only maintains the d diagonal elements called diagonal MetaGrad. We show in Theorem 8 below that\nTheorem 1 The regret of both full and diagonal MetaGrad is bounded by\nRuT \u2264 O (\u221a V uT d lnT )\nfor each u with \u2016u\u2016 \u2264 1, (4)\nwhere V uT denotes either\nT\u2211\nt=1\n((u\u2212wt)\u22bagt)2 (full MetaGrad) or T\u2211\nt=1\nd\u2211\ni=1\n(ui \u2212wt,i)2g2t,i (diagonal MetaGrad).\nThe crucial improvement over the GD bound (1) is that (4) replaces \u2211T\nt=1\u2016gs\u20162 by V uT (for both the full and diagonal version the latter is bounded in terms of the former by the Cauchy-Schwarz inequality). We see that this new adaptivity to the data may come at a cost of a factor \u221a d ln T . The presence of this factor now leaves open a regime where (1) is better than (4), namely when V uT is close to its bound\n\u2211T t=1\u2016gt\u20162. However, MetaGrad is always guaranteed to get the best of both,\nbecause in Theorem 8 we prove separately that its regret stays within a factor of the standard GD bound (1) as well. We now discuss two examples where our new type of adaptivity is key."}, {"heading": "1.1. Examples: Logarithmic regret without curvature", "text": "We illustrate the benefits of our new algorithm with two simple (yet representative) examples where our adaptive bound (4) implies fundamentally faster rates, namely O(d ln T ) instead of O( \u221a T ) regret. In the first example, we consider offline convex optimization of a fixed function. In the second case we look at stochastic optimization with convex functions that measure the loss on stochastic outcomes that are independently, identically distributed (IID). Figure 1 graphs the results. As our baseline we have included GD with three choices of learning rates. We also include AdaGrad (Duchi et al., 2011), which is a state-of-the-art adaptive OCO method that is commonly used in practice (Mikolov et al., 2013; Schmidhuber, 2015).\nBoth our examples are one-dimensional (so the full and diagonal algorithms coincide), and have a stable optimum (that good algorithms will converge to); yet the functions are based on absolute values, which are neither strongly convex nor smooth so the gradient norms do not vanish near the optimum. We see that in both cases the performance of AdaGrad is close to its O( \u221a T ) bound, while our new algorithm MetaGrad achieves a O(d ln T ) rate. We emphasize that MetaGrad has no parameters that we tuned specifically to achieve this.\nFixed function For our first example we consider a fixed convex function ft \u2261 f . We chose the 1-dimensional absolute value function f(x) = |x\u2212 14 |, offset by 14 instead of 0 so that the algorithms cannot exploit starting at the optimum. Now the gradients gt \u2208 {\u00b11} square to unity (none of the methods queries the non-differentiable point at 1/4), and hence sum to\n\u2211T t=1 g 2 t = T . So AdaGrad\n(which due to its adaptive tuning guarantees the GD bound (1)) will give us a \u221a T regret bound. We\nsee in Figure 1 that the actual regret is also of order \u221a T .\nNow for MetaGrad the situation is starkly different. We proceed to prove this, not just for the absolute value, but for any fixed convex function f .\nTheorem 2 A method with regret bound (4, full version) incurs regret O(d ln T ) on fixed ft = f with bounded domain and gradient.\nProof Let u\u2217 = argminu f(u) be the offline minimiser. Then (wt \u2212 u\u2217)\u22bagt \u2208 [0, 4]. The upper bound holds for all wt, u\u2217 and gt in the Euclidean ball. But the lower bound holds, by convexity of f , only for the optimizer u\u2217. It implies (here we use the full version of V u \u2217\nT ) that\nV u \u2217\nT =\nT\u2211\nt=1\n((u\u2217 \u2212wt)\u22bagt)2 \u2264 4 T\u2211\nt=1\n(wt \u2212 u\u2217)\u22bagt.\nNow using the fact that our regret bound (4) also bounds the right-hand side above (which we recognize as the standard \u201cgradient trick\u201d regret upper bound), we find V u \u2217\nT \u2264 O (\u221a V uT d ln T )\nso that \u221a\nV u \u2217 T \u2264 O (\u221a d ln T ) and hence Ru \u2217 T \u2264 O (d ln T ) .\nPreviously, such fast O(d lnT ) rates have only been obtained for functions with significant curvature, which are either strongly convex or at least exp-concave (Hazan et al., 2007). Here we obtain O(d ln T ) regret for any fixed convex function, even if it has no curvature at all.\nAt first sight this may appear to contradict the lower bound of order 1/ \u221a T for convergence of the iterates by Nesterov (2004) (see also Tibshirani (2014)), which implies a lower bound of order\u221a T on the regret. Yet there is no contradiction, as Nesterov\u2019s example requires large dimension\nd \u2265 T , in which case O(d ln T ) is vacuous. MetaGrad still gets the \u221a T rate, however, as can be seen because the GD bound (1) applies to MetaGrad as well.\nStochastic function In the second case we consider the popular extension to stochastic optimization. Staying with the absolute value function, we take ft(u) = |u \u2212 xt| where the outcomes xt = \u00b112 are now chosen IID with probabilities 0.4 and 0.6. (These values are by no means essential, it is important however to avoid the worst case where the probabilities are equal.) It can be shown (by a variation of the fixed function argument that holds in expectation, proof omitted)\nAlgorithm 1: Protocol of Online Convex Optimization from First-order Information Input: Convex set U\n1: for t = 1, 2, . . . do 2: Learner plays wt \u2208 U 3: Environment reveals convex loss function ft : U \u2192 R 4: Learner incurs loss ft(wt) and observes (sub)gradient gt = \u2207ft(wt) 5: end for\nthat in a broad class of stochastic optimization problems the expected regret is O(d ln T ) like in the fixed function case."}, {"heading": "1.2. Related Work", "text": "Other methods that adapt in various ways to the data have been developed in the literature. Among these, AdaGrad (Duchi et al., 2011) is an important extension of gradient descent that improves practical performance in training large neural networks (Mikolov et al., 2013; Schmidhuber, 2015). The crux of AdaGrad (and similar methods developed independently by McMahan and Streeter (2010)) is their adaptive online tuning of the learning rate. As briefly mentioned in the examples above, in one dimension AdaGrad guarantees the optimally tuned GD bound (1). In higher dimensions gradient energy may vary wildly between dimensions (e.g. due to rarely occurring but informative features). AdaGrad capitalizes on this by learning a separate learning rate per dimension (diagonal version) or even a positive definite learning rate matrix (full version) to find the optimal basis.\nAnother notion of adaptivity is explored by Hazan and Kale (2010) and Chiang et al. (2012), who obtain tighter bounds for (linear) functions ft that vary little between rounds (as measured by either deviation from the mean or by successive differences). Such bounds imply fast rates for optimizing a fixed function (first example above), but give slow O( \u221a T ) rates for stochastic optimization (second example above). A downside of AdaGrad and the other methods discussed above, is that they use monotonically decreasing learning rates. Methods that do this may suffer when the initial gradients are much larger than subsequent ones, because they can never recover from decreasing the learning rate too much in the first few rounds (Senior et al., 2013; Zeiler, 2012). Such monotonic tunings are avoided by Squint and iProd, which are adaptive methods developed for the expert setting (Koolen and Van Erven, 2015) that provide bounds in terms of the squared excess loss. If we disregard computational efficiency, then applying Squint to the continuously many experts in the unit ball shows that regret guarantees like (4) are possible in principle. In fact, our main contribution can be seen as an efficient implementation of the continuous extension of Squint."}, {"heading": "2. Setup", "text": "Let U \u2286 Rd be a closed convex set, which we assume contains the origin 0 (if not, it can always be translated). We consider algorithms for Online Convex Optimization over U , which operate according to the protocol displayed in Algorithm 1. Let wt \u2208 U be the iterate produced by the algorithm in round t, let ft : U \u2192 R be the convex loss function produced by the environment and let gt = \u2207ft(wt) be the gradient, which is the feedback given to the algorithm. (If ft is\nnot differentiable at wt, we do not make any assumptions about the choice of subgradient gt \u2208 \u2202ft(wt).) A function f is called exp-concave (with exp-concavity parameter 1) if e\u2212f is concave or, equivalently, if \u2207f\u2207f\u22ba \u22072f . We impose the following standard boundedness assumptions. For all u,v \u2208 U , all dimensions i and all times t (we give two sets of assumptions and definitions, one for the full and one for the diagonal treatment):\nfull diag\n\u2016u\u2212 v\u2016 \u2264 Dfull |ui \u2212 vi| \u2264 Ddiag (5) \u2016gt\u2016 \u2264 Gfull |gt,i| \u2264 Gdiag\nIn particular (u\u2212 v)\u22bagt \u2264 DfullGfull. In addition, we will make use of the following definitions:\nfull diag\nM fullt := gtg \u22ba t M diag t := diag(g 2 t,1, . . . , g 2 t,d) (6)\n\u03b1full := 1 \u03b1diag := 1/d.\nDepending on context, wt \u2208 U will refer to the full or diagonal MetaGrad prediction in round t. In the remainder we will drop the superscript from the letters above whenever a claim or construction works for both versions.\nMetaGrad will be defined by means of the following surrogate loss \u2113\u03b7t (u), which depends on a parameter \u03b7 > 0 that trades off regret compared to u with the square of the scaled directional derivative towards u (full case) or its approximation (diag case):\n\u2113\u03b7t (u) := \u2212 \u03b7(wt \u2212 u)\u22bagt + \u03b72(u\u2212wt)\u22baMt(u\u2212wt). (7) Our surrogate loss consists of a linear and a quadratic part. In the language of Steinhardt and Liang (2014), the quadratic part causes an \u201cadaptive regularizer\u201d and Duchi et al. (2011) call it \u201ctemporal adaptation of the proximal function\u201d, but in our approach it is more convenient to treat the two parts of the surrogate loss together, without moving the quadratic part into the regularizer. The sum of quadratic terms in our surrogate is what appears in the regret bound of Theorem 1."}, {"heading": "3. MetaGrad Algorithm", "text": "The MetaGrad algorithm is a two-level hierarchical construction, displayed as Algorithms 2 (main algorithm that learns the learning rate) and 3 (sub-module for individual learning rates). We call it MetaGrad because the key idea is the aggregation of \u201cmultiple \u03b7\u201d, i.e. the predictions of gradientbased learners parametrised by the learning rate \u03b7.\nMaster The task of the Master Algorithm 2 is to learn the best learning rate \u03b7 (parameter of the surrogate loss \u2113\u03b7t ). The (approximately) best learning rate is notoriously difficult to track online because the regret is non-monotonic over rounds and may have several local minima as a function of \u03b7 (see Koolen et al. (2014) for a study in the expert setting). Our approach, inspired by the approach for combinatorial games in (Koolen and Van Erven, 2015, Section 4), is to have our master aggregate the predictions of a discrete grid of learning rates. The master is in fact a variant of the exponential weights method (line 5) with the standard gradient trick (line 4), yet with a modified aggregation rule (line 3) that tilts the weights of the slaves by their learning rates, having the effect of giving a larger weight to larger \u03b7.\nAlgorithm 2: MetaGrad Master\nInput: Grid of learning rates 15DG \u2265 \u03b71 \u2265 \u03b72 \u2265 . . . with prior weights \u03c0 \u03b71 1 , \u03c0 \u03b72 1 , . . . \u22b2 As in (9)\n1: for t = 1, 2, . . . do 2: Get prediction w\u03b7t \u2208 U of slave (Algorithm 3) for each \u03b7 3: Play wt = \u2211 \u03b7 \u03c0 \u03b7 t \u03b7w \u03b7 t\u2211\n\u03b7 \u03c0 \u03b7 t \u03b7 \u2208 U \u22b2 Tilted Exponentially Weighted Average 4: Observe gradient gt = \u2207ft(wt) 5: Update \u03c0\u03b7t+1 = \u03c0 \u03b7 t e \u2212\u03b1\u2113 \u03b7 t (w \u03b7 t )\n\u2211 \u03b7 \u03c0 \u03b7 t e \u2212\u03b1\u2113 \u03b7 t (w \u03b7 t )\nfor all \u03b7 \u22b2 Exponential Weights with surrogate loss (7)\n6: end for\nAlgorithm 3: MetaGrad Slave\nInput: Learning rate 0 < \u03b7 \u2264 15DG , domain size D > 0. 1: w\n\u03b7 1 = 0 and \u03a3 \u03b7 1 = D 2I\n2: for t = 1, 2, . . . do 3: Issue w\u03b7t to master (Algorithm 2) 4: Observe gradient gt = \u2207ft(wt) \u22b2 Gradient at master point wt 5: Update \u03a3\u03b7t+1 = ( 1 D2 I + 2\u03b72 \u2211t s=1Ms )\u22121\n\u2022 For Mt = M fullt use rank-one update \u03a3\u03b7t+1 = \u03a3\u03b7t \u2212 2\u03b72 \u03a3\n\u03b7 t gtg \u22ba t \u03a3 \u03b7 t 1+g\u22bat \u03a3 \u03b7 t gt\n\u2022 For Mt = M diagt only maintain diagonal\n6: Update w\u03b7t+1 = \u03a0 \u03a3\n\u03b7 t+1 U ( w \u03b7 t \u2212 \u03b7\u03a3\u03b7t+1gt )\n\u2022 For Mt = M fullt use \u03a3\u03b7t+1gt = \u03a3\n\u03b7 t gt\n1+2\u03b72g\u22bat \u03a3 \u03b7 t gt\n7: end for\nGrid Our analysis below indicates that we may exponentially space the learning rates from 1 down to 1\u221a\nT , where T is the number of prediction rounds. So for T = 109 rounds we would\nneed 15 slaves. This seems computationally quite affordable, especially since the slaves operate completely independently and can hence be executed in parallel.\nSlaves The role of the Slave Algorithm 3 is to guarantee small surrogate regret for a fixed learning rate \u03b7. One way to obtain the Slave algorithm, as we will explain and use in Theorem 6 below, is as an instance of exponential weights with Gaussian prior, quadratic losses and Kullback-Leibler projections. The Slave algorithm is very similar to what would result from applying Online Newton Step (ONS) by Hazan et al. (2007) to our surrogate loss \u2113\u03b7t (u), which we will prove to be expconcave. The only difference is that the ONS algorithm (and analysis) make use of a quadratic lower bound on the loss that follows from the exp-concavity assumption. Our surrogate losses are quadratic by construction, and MetaGrad slaves uses them exactly. Intuitively, even though our bounds do not quantify the improvement, employing the actual Hessian will give a little epsilon boost in performance in practice.\nWe also remark that the combined algorithm is a bona fide first-order algorithm that only accesses ft through \u2207ft(wt). In particular, the Slaves do not see their \u201cown\u201d gradients (i.e. \u2207ft(w\u03b7t )), but all receive the gradient at the master point.\nWe consider two versions of the Slave algorithm, corresponding to whether we take rank-one or diagonal matrices Mt (see (6)) in the surrogate (7). The first version maintains a full d \u00d7 d covariance matrix and has the best regret bound. The second version uses only diagonal matrices (with d non-zero entries), thus trading off a mildly weaker bound with a better run-time in high dimensions. We expect the second version to be especially relevant in practice.\nWe finally note that we have chosen the Mirror Descent version that iteratively updates and projects (see line 6). One might alternatively consider the Lazy Projection version (as in Zinkevich (2004); Nesterov (2009); Xiao (2010)) that forgets past projections when updating on new data. Since projections are typically computationally expensive, we have opted for the Mirror Descent version, which we expect to project less often.\nRun time The run-time is dominated by the O(lnT ) slaves. Ignoring the projection, a slave with full covariance matrix takes O(d2) time to update, while slaves with diagonal covariance matrix take O(d) time. This makes the overall computational effort as follows:\ntime per round:\n{\nO(d2 lnT ) full version O(d ln T ) diagonal version memory:\n{\nO(d2 lnT ) full version O(d ln T ) diagonal version .\nOn top of that each slave may incur the cost of a projection, which depends on the shape of the domain U . To get a sense for the projection cost we consider a typical example. For the Euclidean ball a diagonal projection can be performed using a few iterations of Newton\u2019s method to get the desired precision. Each such iteration costs O(d) time. This is generally considered affordable. For full projections the story is starkly different. We typically reduce to the diagonal case by a basis transformation, which takes O(d3) to compute using SVD. Hence here the projection dwarfs the other run time by an order of magnitude. We refer to Duchi et al. (2011) for examples of how to compute projections for various domains U . Finally, we remark that a potential speed-up is possible by running the slaves in parallel."}, {"heading": "4. Analysis", "text": "We conduct the analysis in three parts. We first discuss the master, then the slaves and finally their composition. The idea is the following. The master guarantees for all \u03b7 simultaneously that\n0 =\nT\u2211\nt=1\n\u2113\u03b7t (wt) \u2264 T\u2211\nt=1\n\u2113\u03b7t (w \u03b7 t ) + master regret compared to \u03b7-slave. (8a)\nThen each \u03b7-slave takes care of learning u:\nT\u2211\nt=1\n\u2113\u03b7t (w \u03b7 t ) \u2264\nT\u2211\nt=1\n\u2113\u03b7t (u) + \u03b7-slave regret compared to u. (8b)\nThese two statements combine to\n\u03b7\nT\u2211\nt=1\n(wt \u2212 u)\u22bagt \u2212 \u03b72V uT = \u2212 T\u2211\nt=1\n\u2113\u03b7t (u) \u2264 sum of regrets above (8c)\nand the overall result follows by the \u201cgradient trick\u201d upper bound RuT \u2264 \u2211T\nt=1(wt \u2212 u)\u22bagt and optimizing \u03b7."}, {"heading": "4.1. Master", "text": "Our master algorithm is a variation of the Combinatorial iProd master algorithm that Koolen and Van Erven (2015, Section 4) develop for learning the learning rate in combinatorial domains. To show that we can aggregate the slave predictions, we start by showing that the surrogate loss exhibits useful curvature. Proofs can be found in Appendix A.\nLemma 3 (Full exp-concavity) Fix 0 < \u03b7 \u2264 1 5DfullGfull . For Mt = M fullt , the surrogate loss function \u2113\u03b7t (u) from (7) is exp-concave in u.\nFor the full case, where \u03b1 = \u03b1full = 1, exp-concavity immediately implies the following useful analog of the \u201cprod bound\u201d (Cesa-Bianchi et al., 2007; Gaillard et al., 2014; Koolen and Van Erven, 2015) by taking the tangent of e\u2212\u2113 \u03b7 t (w\n\u03b7 t ) at w\u03b7t = wt. In the diagonal case, the surrogate losses\nare only exp-concave separately for each dimension, but the same result can be established for \u03b1 = \u03b1diag = 1/d.\nLemma 4 (Tangent bound) Fix 0 < \u03b7 \u2264 15DG . We have\ne\u2212\u03b1\u2113 \u03b7 t (w \u03b7 t ) \u2264 1 + \u03b1\u03b7 (wt \u2212w\u03b7t ) \u22ba gt.\nThis tangent bound is the main ingredient in showing that the master keeps the surrogate loss approximately above zero for all slaves in the following formal sense.\nLemma 5 (Master combines slaves) Consider the potential \u03a6T := \u2211 \u03b7 \u03c0 \u03b7 1e \u2212\u03b1\u2211Tt=1 \u2113 \u03b7 t (w \u03b7 t ). The master algorithm guarantees 1 = \u03a60 \u2265 \u03a61 \u2265 . . . , so in particular \u03a6T \u2264 1.\nAs 0 \u2264 \u2212 1 \u03b1 ln \u03a6T \u2264 \u2211T t=1 \u2113 \u03b7 t (w \u03b7 t ) + \u22121 \u03b1 ln\u03c0\u03b71 , this implements step (8a) of our overall proof strategy. We remark that one may view our potential function \u03a6T as a game-theoretic supermartingale in the sense of Chernov et al. (2010), and this lemma as establishing that the MetaGrad Master is the corresponding defensive forecasting strategy.\nProof By induction on T . We have equality in the base case T = 0. Moreover\n\u03a6T+1 \u2212 \u03a6T = \u2211\n\u03b7\n\u03c0\u03b71e \u2212\u03b1\u2211Tt=1 \u2113 \u03b7 t (w \u03b7 t ) ( e\u2212\u03b1\u2113 \u03b7 t (w \u03b7 T+1) \u2212 1 )\n\u2264 \u2211\n\u03b7\n\u03c0\u03b71e \u2212\u03b1\u2211Tt=1 \u2113 \u03b7 t (w \u03b7 t )\u03b1\u03b7 ( wT+1 \u2212w\u03b7T+1 ) \u22ba gT+1 = 0,\nwhere the inequality is the tangent bound Lemma 4. The final equality is by definition of the master prediction (in fact it can be taken as the motivation for the choice of aggregation)\nwT+1 =\n\u2211\n\u03b7 \u03c0 \u03b7 T+1\u03b7w \u03b7 T+1\n\u2211\n\u03b7 \u03c0 \u03b7 T+1\u03b7\n=\n\u2211\n\u03b7 \u03c0 \u03b7 1e\n\u2212\u03b1\u2211Tt=1 \u2113 \u03b7 t (w\n\u03b7 t )\u03b7w\u03b7T+1\n\u2211\n\u03b7 \u03c0 \u03b7 1e\n\u2212\u03b1\u2211Tt=1 \u2113 \u03b7 t (w \u03b7 t )\u03b7\n."}, {"heading": "4.2. Slaves", "text": "Next we implement step (8b), which requires proving a regret bound for each MetaGrad slave. In the full case one may use a general method for exp-concave functions, like Online Newton Step, to get O(d ln T ) regret (Hazan et al., 2007). We employ a very similar but marginally tighter method that avoids the lower bounding step based on exp-concavity, and instead evaluates our quadratic surrogate loss exactly. A general method is not available for the diagonal case, which lacks joint exp-concavity. We use exp-concavity in each direction separately, and verify that the projections that tie the dimensions together do not cause any trouble. In Appendix B we analyze both cases simultaneously, and obtain the following bound on the regret:\nLemma 6 (Surrogate regret bound) Let 0 < \u03b7 \u2264 15DG . Let \u2113 \u03b7 t (u) be the surrogate losses as defined in (7) for either Mt = M fullt or Mt = M diag t . Then the regret of Algorithm 3 is bounded by\nT\u2211\nt=1\n\u2113\u03b7t (w \u03b7 t ) \u2264\nT\u2211\nt=1\n\u2113\u03b7t (u) + 1 2D2 \u2016u\u20162 + 1 2 ln det\n(\nI + 2\u03b72D2 T\u2211\nt=1\nMt\n)\nfor all u \u2208 U ."}, {"heading": "4.3. Composition", "text": "To complete the analysis of MetaGrad, we first put the regret bounds for the master and slaves together as in (8c). We then discuss how to choose the grid of \u03b7s, and optimize \u03b7 over this grid to get our main result. Proofs are postponed to Appendix C.\nTheorem 7 (Grid point regret) Algorithm 2 guarantees, for any grid point \u03b7 with prior weight \u03c0\u03b71 that\nRuT \u2264 \u03b7V uT + 1 2D2 \u2016u\u2016 2 \u2212 1 \u03b1 ln\u03c0\u03b71 + 1 2 ln det\n(\nI + 2\u03b72D2 \u2211T\nt=1 Mt\n)\n\u03b7 for all u \u2208 U .\nGrid We now specify the grid points and corresponding prior. Theorem 7 above implies that any two \u03b7 that are within a constant factor of each other will guarantee the same bound up to another constant factor. We therefore choose an exponentially spaced grid with a heavy tailed prior\n\u03b7i := 2\u2212i\n5DG and \u03c0\u03b7i1 :=\nC\ni(i+ 1) for i = 1, 2, . . . , \u230812 log2 T \u2309, (9)\nwith normalization C = 1+ 1 / \u230812 log2 T \u2309 . The net effect of (9) is that for each \u03b7 \u2208 [ 15DG\u221aT , 1 5DG ] there is an \u03b7i \u2208 [12\u03b7, \u03b7], for which \u2212 ln\u03c0 \u03b7i 1 \u2264 2 ln(i + 1) = O(ln ln(1/\u03b7i)) = O(ln ln(1/\u03b7)). We emphasize that, for other (say finer) discretizations, the simple prior masses \u03c0\u03b7i1 above are not appropriate. See Appendix D for further discussion.\nThe final step is to apply Theorem 7 to the grid described above, and to properly select the learning rate \u03b7i in the bound. This leads to our main result:\nTheorem 8 (Small regret without curvature) LetST = \u2211T t=1 Mt so that tr(ST ) = \u2211T t=1\u2016gt\u20162. The regret of both the full and the diagonal versions of Algorithm 2 with corresponding definitions\nfrom (5) and (6), and with grid and prior (9), is bounded by\nRuT \u2264 \u221a 8V uT ( 1\nD2 \u2016u\u20162 + \u039eT +\n1 \u03b1 CT\n)\n+10DG\n( 1\nD2 \u2016u\u20162 + \u039eT +\n1 \u03b1 CT\n)\nfor all u \u2208 U ,\nwhere \u039eT \u2264 min { ln det (\nI + D 2 rk(ST ) V u T ST\n) , rk(ST ) ln ( D2 tr(ST )\nV u T\n)}\n= O(d ln(D2G2T )) and\nCT = 4 ln ( 2 + 12 log2 T ) = O(ln lnT ). Moreover, the regret is simultaneously bounded by\nRuT \u2264 \u221a 8D2 tr (ST ) ( 1\nD2 \u2016u\u20162 + 1 \u03b1 CT\n)\n+ 10DG\n( 1\nD2 \u2016u\u20162 + 1 \u03b1 CT\n)\nfor all u \u2208 U .\nThese two bounds together show that MetaGrad achieves the new adaptive guarantee of Theorem 1 and at the same time guarantees the optimally tuned Gradient Descent guarantee (1). We emphasize that MetaGrad does not need to be tuned by hand; by design it performs all the necessary learning rate optimization internally."}, {"heading": "5. Discussion", "text": "One of the most time-consuming and laborious aspects of online learning is the tuning of the learning rate, which often leaves the sneaking suspicion that perhaps performance could have been substantially improved with a better tuning. Even though the learning rate is \u201cjust a single parameter\u201d, it governs a fundamental trade off between exploiting signal and stabilizing parameter estimates through regularization.\nTaking stock of online convex optimization, we find that all techniques for tuning the learning rate with provable regret bounds essentially amount to the following approach: monotonically decrease the learning rate over time to minimize some upper bound on the regret. This approach has two fundamental limitations. First, the monotonicity constraint prevents the algorithm from ever recovering if some hard initial data cause the learning rate to go down (Senior et al., 2013). And secondly, any slack in the bound (which often is significant since bounds must cover the hard instances along with the easy ones) will be hard-coded in the algorithm, making it overly conservative.\nWith MetaGrad, we introduce a fundamentally different technique for tuning the learning rate. By keeping track of multiple learning rates with tilted exponential weights, we have the data inform us about the best learning rate without invoking any bounds, and avoid monotonicity constraints altogether.\nOur methods lead to a new form of adaptive bound (Theorem 8), which cannot be achieved with any prior methods for tuning the learning rate. This bound expresses that in stable problem instances the MetaGrad algorithm will \u201caccelerate\u201d due to a virtuous feedback cycle (see e.g. Theorem 2). Namely, if the algorithm works well in the sense that its parameter estimates converge to the optimum u sufficiently fast, then the regret bound expresses that the algorithm converges even faster."}, {"heading": "5.1. Using MetaGrad for Stochastic Optimization", "text": "Given T IID samples from a distribution P , the standard technique of online-to-batch conversion can be used to turn MetaGrad into a method with small excess risk of at most E[RuT ]/T relative to\nthe best parameters u for P by averaging the parameters: w\u0304T = 1T \u2211T\nt=1wt (Cesa-Bianchi et al., 2004). However, when the regret RuT is of order O(d lnT ), the achievable excess risk is typically of order O(d/T ), so standard online-to-batch conversion will be suboptimal. This happens because parameter estimates in the first rounds are much worse than those in the later rounds, and can be fixed by averaging only the later parameters (Rakhlin et al., 2012; Shamir and Zhang, 2013) or by dividing the data into exponentially increasing epochs and using the average of the parameters over the previous epoch as a starting point for the next (Hazan and Kale, 2014)."}, {"heading": "5.2. Future Work", "text": "One may consider extending MetaGrad in various directions. In particular it would be interesting to integrate composite losses (Duchi et al., 2010; Xiao, 2009) and optimistic gradient estimates (Chiang et al., 2012; Rakhlin and Sridharan, 2013). We would further like to know whether it is possible to learn a learning rate vector (one rate per dimension), or a positive definite learning rate matrix (one rate per direction) like Duchi et al. (2011). We have tried this ourselves, but are stuck at the construction of the master. In contrast with a single learning rate, there is no obvious way to guarantee that the aggregate of slave predictions (which are all in U ) is itself in U , because the tilted average in the master leads to a non-convex constraint. A second problem would be computational efficiency, which gets out of hand when a grid of discretized learning rate vectors needs to be considered every round.\nOne way to address this efficiency problem would be to run an independent one-dimensional MetaGrad learner for each dimension separately. One would need to take care of the projections, which do tie the dimensions together. However, if we are running on the box U = [\u22121, 1]d, the problem factorizes, and d independent MetaGrads, one per dimension, would be able to tune a separate learning rate independently per dimension. If we are not too picky about the shape of the parameter domain U , this may actually be a very promising way to use MetaGrad."}, {"heading": "Acknowledgments", "text": "Koolen acknowledges support by the Netherlands Organization for Scientific Research (NWO, Veni grant 639.021.439)."}, {"heading": "Appendix A. Exp-concavity Proofs", "text": "A.1. Proof of Lemma 3\nProof We need to check \u2207\u2113\u03b7t (u)\u2207\u2113\u03b7t (u)\u22ba \u22072\u2113\u03b7t (u). That is, ( \u03b7gt + 2\u03b7 2Mt(u\u2212wt) ) ( \u03b7gt + 2\u03b7 2Mt(u\u2212wt) )\u22ba 2\u03b72Mt.\nUsing that Mt = M fullt = gtg \u22ba t , we need to show\n( \u03b7 + 2\u03b72g\u22bat (u\u2212wt) )2 gtg \u22ba t 2\u03b72gtg\u22bat\nwhich reduces to the scalar inequality\n( \u03b7 + 2\u03b72g\u22bat (u\u2212wt) )2 \u2264 2\u03b72.\nUsing the upper bound DfullGfull on the inner product, it is sufficient to have\n( 1 + 2\u03b7DfullGfull )2 \u2264 2,\nwhich holds with equality when we plug in the assumed upper bound on \u03b7.\nA.2. Proof of Lemma 4\nProof We have two cases. First, for Mt = M fullt , the left-hand side is exp-concave in w \u03b7 t by Lemma 3, and hence bounded by its tangent at w\u03b7t = wt, which is the right-hand side. Then for Mt = M diag t = diag(g 2 t,1, . . . , g 2 t,d) (recalling that now \u03b1 = 1/d), we have\ne\u2212\u03b1\u2113 \u03b7 t (w \u03b7 t ) = e \u2211 i 1 d(\u03b7(wt,i\u2212w \u03b7 t,i)gt,i\u2212\u03b72(wt,i\u2212w \u03b7 t,i) 2g2t,i)\nJensen \u2264 \u2211\ni\n1 d e(\u03b7(wt,i\u2212w \u03b7 t,i)gt,i\u2212\u03b72(wt,i\u2212w \u03b7 t,i) 2g2t,i)\nexp-concavity \u2264 \u2211\ni\n1\nd\n( 1 + \u03b7(wt,i \u2212 w\u03b7t,i)gt,i )\n= 1 + \u03b1\u03b7(w\u03b7t \u2212wt)\u22bagt,\nwhere the exp-concavity inequality is the result for the full case applied in 1 dimension.\nA.3. Gaussian Exp-Concavity\nLemma 3 establishes exp-concavity of the surrogate losses \u2113\u03b7t (u) for u \u2208 U . We now argue that this property somewhat extends outside of U for Gaussian distributions with mean in U . This does not follow from exp-concavity alone (which holds for u \u2208 U ) since Gaussian distributions are supported on all of Rd.\nLemma 9 (Gaussian exp-concavity) Let 0 < \u03b7 \u2264 15DG . Consider a Gaussian distribution with mean \u00b5 \u2208 U and arbitrary covariance \u03a3 \u227b 0 in the full case or diagonal \u03a3 \u227b 0 in the diagonal case. Then\nE u\u223cN (\u00b5,\u03a3)\n[\ne\u2212\u2113 \u03b7 t (u)\n]\n\u2264 e\u2212\u2113\u03b7t (\u00b5)\nProof We first consider the full case. Abbreviating r := (wt \u2212 \u00b5)\u22bagt and s := (\u00b5 \u2212 u)\u22bagt, from the definition (7) of \u2113\u03b7t we get\n\u2113\u03b7t (\u00b5) \u2212 \u2113\u03b7t (u) = \u03b7(\u00b5\u2212 u)\u22bagt \u2212 \u03b72 (2(\u00b5\u2212wt)\u22bagtg\u22bat (\u00b5\u2212 u) + (\u00b5\u2212 u)\u22bagtg\u22bat (\u00b5\u2212 u)) = \u03b7s\u2212 \u03b72 ( 2rs+ s2 ) .\nSince u \u223c N (\u00b5,\u03a3) implies s \u223c N (0, v) with v = g\u22bat\u03a3gt, the claim collapses to\n1 \u2265 E u\u223cN (\u00b5,\u03a3)\n[\ne\u2113 \u03b7 t (\u00b5)\u2212\u2113 \u03b7 t (u)\n]\n= E s\u223cN (0,v)\n[\ne\u03b7s\u2212\u03b7 2(2rs+s2)\n]\n= e\n\u03b72v(1\u22122\u03b7r)2\n2(1+2\u03b72v)\n\u221a 1 + 2\u03b72v ,\nwhich is equivalent to\n(1\u2212 2\u03b7r)2\u03b72v \u2264 ( 1 + 2\u03b72v ) ln ( 1 + 2\u03b72v ) .\nThe left-hand side is maximized over r \u2208 [\u2212DfullGfull,DfullGfull] at r = \u2212DfullGfull. So it suffices to establish\nv(1 + 2\u03b7DfullGfull)2\u03b72 \u2264 ( 1 + 2\u03b72v ) ln ( 1 + 2\u03b72v ) .\nNow the right-hand is convex in v and hence bounded below by its tangent at v = 0, which is 2\u03b72v. The proof is completed by observing that (1 + 2\u03b7DfullGfull)2 \u2264 2 by the assumed bound on \u03b7.\nIt remains to consider the diagonal case. There the surrogate loss (7) is a sum over dimensions, say \u2113\u03b7t (u) = \u2211d i=1 \u2113 \u03b7 t,i(ui). For a Gaussian with diagonal covariance matrix \u03a3 the coordinates of u are independent, and hence\nE u\u223cN (\u00b5,\u03a3)\n[\ne\u2212\u2113 \u03b7 t (u)\n] = d\u220f\ni=1\nE ui\u223cN (\u00b5i,\u03a3i,i)\n[\ne\u2212\u2113 \u03b7 t,i(ui)\n] \u2264 d\u220f\ni=1\ne\u2212\u2113 \u03b7 t,i(\u00b5i) = e\u2212\u2113 \u03b7 t (\u00b5),\nwhere the inequality is the result for the full case applied to each dimension separately."}, {"heading": "Appendix B. Slave Regret Bound (Proof of Lemma 6)", "text": "Proof For any distributions P and Q on Rd, let KL(P\u2016Q) = EP [ln dPdQ ] denote the KullbackLeibler divergence of P from Q, and let \u00b5P = EP [u] denote the mean of P . In addition, let N (\u00b5,\u03a3) denote a normal distribution with mean \u00b5 and covariance matrix \u03a3.\nIn round t, we play according to the mean of a multivariate Gaussian distribution Pt. In the first round, this is a normal distribution, which plays the role of a prior:\nP1 = N (0,D2I).\nThen we update using the exponential weights update, followed by a projection onto P = {P : \u00b5P \u2208 U}, such that the mean stays in the allowed domain U :\ndP\u0303t+1(u) = e\u2212\u2113 \u03b7 t (u) dPt(u) \u222b\nRd e\u2212\u2113 \u03b7 t (u \u2032) dPt(u\u2032) , Pt+1 = \u03a0P(P\u0303t).\nTo see that Algorithm 3 implements this algorithm, we prove by induction that\nPt = N (w\u03b7t ,\u03a3\u03b7t ).\nFor t = 1 this is clear, and if it holds for any t then it can be verified by comparing densities that P\u0303t+1 = N (w\u03b7t \u2212\u03b7\u03a3\u03b7t+1gt,\u03a3\u03b7t+1). Since it is well-known that the projection of a Gaussian N (\u00b5,\u03a3) onto P is another Gaussian N (\u03bd,\u03a3) with the same covariance matrix and mean \u03bd \u2208 U that minimizes 12(\u03bd \u2212 \u00b5)\u22ba\u03a3\u22121(\u03bd \u2212 \u00b5), it then follows that Pt+1 = N (w \u03b7 t+1,\u03a3 \u03b7 t+1). For completeness we provide a proof of this last result in Lemma 10 of Appendix E. It now remains to bound the regret. Since P is convex, the Pythagorean inequality for KullbackLeibler divergence implies that\nKL(Q\u2016P\u0303t+1) \u2265 KL(Q\u2016Pt+1) + KL(Pt+1\u2016P\u0303t+1) \u2265 KL(Q\u2016Pt+1)\nfor all Q \u2208 P. The following telescoping sum therefore gives us that\nKL(Q\u2016P1) \u2265 T\u2211\nt=1\nKL(Q\u2016Pt)\u2212KL(Q\u2016Pt+1) \u2265 T\u2211\nt=1\nKL(Q\u2016Pt)\u2212KL(Q\u2016P\u0303t+1)\n=\nT\u2211\nt=1\n\u2212 ln E Pt [e\u2212\u2113 \u03b7 t (u)]\u2212 E Q [\u2113\u03b7t (u)]. (10)\nThis may be interpreted as a regret bound in the space of distributions, which we will now relate to our regret of interest. If Mt = M fullt , then Lemma 9 implies that\n\u2212 ln E Pt [e\u2212\u2113 \u03b7 t (u)] \u2265 \u2113\u03b7t (w\u03b7t )\nbecause w\u03b7t is the mean of Pt. Alternatively, if Mt = M diag t then Pt has diagonal covariance \u03a3 \u03b7 t , and we can use Lemma 9 again to draw the same conclusion. To control EQ[\u2113 \u03b7 t (u)], we may restrict attention (without loss of generality by a standard maximum entropy argument) to normal distributions Q = N (\u00b5,D2\u03a3) with mean \u00b5 \u2208 U and covariance \u03a3 \u227b 0 (expressed relative to the prior variance D2). Then, using the cyclic property and linearity of the trace,\nE Q [\u2113\u03b7t (u)] = \u2212\u03b7(wt \u2212 \u00b5)\u22bagt + \u03b72(w\u22batMtwt \u2212 2\u00b5\u22baMtwt + E Q [tr(u\u22baMtu)])\n= \u2212\u03b7(wt \u2212 \u00b5)\u22bagt + \u03b72(w\u22batMtwt \u2212 2\u00b5\u22baMtwt + tr(E Q [uu\u22ba]Mt)) = \u2212\u03b7(wt \u2212 \u00b5)\u22bagt + \u03b72(w\u22batMtwt \u2212 2\u00b5\u22baMtwt + tr((D2\u03a3+ \u00b5\u00b5\u22ba)Mt)) = \u2212\u03b7(wt \u2212 \u00b5)\u22bagt + \u03b72 ( (\u00b5\u2212wt)\u22baMt(\u00b5\u2212wt) +D2 tr(\u03a3Mt) ) = \u2113\u03b7t (\u00b5) + \u03b7 2D2 tr(\u03a3Mt).\nFinally, it remains to work out\nKL(Q\u2016P1) = 1 2D2 \u2016\u00b5\u20162 + 1 2 (\u2212 ln det\u03a3+ tr(\u03a3)\u2212 d) .\nWe have now bounded all the pieces in (10). Putting them all together with the choice \u00b5 = u and optimizing the bound in \u03a3 gives:\nT\u2211\nt=1\n\u2113\u03b7t (w \u03b7 t )\u2212\nT\u2211\nt=1\n\u2113\u03b7t (u) \u2264 1 2D2 \u2016u\u20162 + 1 2 inf \u03a3\u227b0\n{ \u2212 ln det\u03a3+ tr ( \u03a3 ( I + 2\u03b72D2 T\u2211\nt=1\nMt\n)) \u2212 d }\n= 1 2D2 \u2016u\u20162 + 1 2 ln det\n(\nI + 2\u03b72D2 T\u2211\nt=1\nMt\n)\n, (11)\nwhere the minimum is attained at \u03a3 = ( I + 2\u03b72D2 \u2211T\nt=1Mt\n)\u22121 ."}, {"heading": "Appendix C. Composition Proofs", "text": "Throughout this section we abbreviate ST = \u2211T t=1 Mt.\nC.1. Proof of Theorem 7\nProof We start with\n0 Lemma 5 \u2265 1 \u03b1 ln\u03a6T \u2265 1 \u03b1 ln\u03c0\u03b71 \u2212\nT\u2211\nt=1\n\u2113\u03b7t (w \u03b7 t )\nLemma 6 \u2265 1 \u03b1 ln\u03c0\u03b71 \u2212\nT\u2211\nt=1\n\u2113\u03b7t (u)\u2212 1 2D2 \u2016u\u20162 \u2212 1 2 ln det ( I + 2\u03b72D2ST ) .\nNow expanding the definition (7) of the surrogate losses we find\n\u03b7 T\u2211\nt=1\n(wt \u2212 u)\u22bagt \u2264 1 2D2 \u2016u\u20162 \u2212 1 \u03b1 ln\u03c0\u03b71 + \u03b7 2V uT + 1 2 ln det ( I + 2\u03b72D2ST ) ,\nin which we may divide by \u03b7 and use convexity of ft to get RuT \u2264 \u2211T\nt=1(wt \u2212 u)\u22bagt to obtain the claim.\nC.2. Proof of Theorem 8\nProof In principle we would like to select the \u03b7 from the grid that optimizes the regret bound Theorem 7. But unfortunately we cannot tractably minimize that bound in \u03b7 as it occurs in the ln det. To bring the \u03b7 out, we apply the variational form from (11) to Theorem 7 to obtain\nRuT \u2264 inf \u03a3\u227b0\n\u03b7i ( V uT +D 2 tr (\u03a3ST ) ) +\n1 D2 \u2016u\u20162 \u2212 2 \u03b1 ln\u03c0\u03b7i1 \u2212 ln det\u03a3+ tr (\u03a3)\u2212 d\n2\u03b7i (12)\nfor all grid points \u03b7i.\nWe start by observing that c = rk(ST ) ( D2\nV u T \u2212 1tr(ST ) )\n\u2265 0, because V uT \u2264 D2 tr(ST ) by Cauchy-Schwarz. We now elect to plug in the convenient (and near-optimal) choice\n\u03a3 = (I + cST ) \u22121,\nand bound all terms involving \u03a3 above. Since ST and \u03a3 share the same eigenbasis, we may work in that basis. As \u03a3ST has rk(ST ) non-zero eigenvalues, we may pull out a factor rk(ST ) and replace the trace by a uniform average of the eigenvalues. Then Jensen\u2019s inequality for the concave function x 7\u2192 x1+cx for x \u2265 0 gives\nD2 tr(\u03a3ST ) Jensen \u2264 D 2 tr(ST ) (\n1 + crk(ST ) tr(ST ) ) = V uT .\nSecondly, using that \u03a3 \u227a I, we find tr(\u03a3) \u2264 tr(I) = d. And finally, by construction of the grid, for any \u03b7 \u2208 [ 1\n5DG \u221a T , 15DG ] there exists a grid point \u03b7i \u2208 [ \u03b7 2 , \u03b7], and the prior costs of this grid point\nsatisfy\n\u2212 ln\u03c0\u03b7i1 \u2264 2 ln(1 + i) = 2 ln ( 1 + log2 ( 1\n5DG\u03b7i\n))\n\u2264 2 ln ( 2 + 12 log2 T ) .\nPlugging these bounds into (12) and abbreviating\n\u039e := \u2212 ln det\u03a3 = ln det (I + cST ) \u2265 0\nA := 1 D2 \u2016u\u20162 + 4 \u03b1 ln ( 2 + 12 log2 T ) + \u039e \u2265 4 ln 2,\nwe obtain\nRuT \u2264 2\u03b7V uT + A\n\u03b7 .\nSubsequently tuning \u03b7 optimally as\n\u03b7\u0302 =\n\u221a\nA\n2V uT \u2265\n\u221a 2 ln 2 DG \u221a T \u2265 1 5DG \u221a T\nis allowed when \u03b7\u0302 \u2264 15DG , and gives RuT \u2264 \u221a\n8V uT A. Alternatively, if \u03b7\u0302 \u2265 15DG , then we plug in \u03b7 = 15DG and obtain R u T \u2264 2 15DGV uT + 5DGA \u2264 10DGA, where the second inequality follows from the constraint on \u03b7\u0302. In both cases, we find that\nRuT \u2264 \u221a 8V uT A+ 10DGA,\nwhich results in the first claim of the theorem upon observing that \u039e \u2264 rk(ST ) ln ( D2 tr(ST )\nV u T\n)\nby\nJensen\u2019s inequality and \u039e \u2264 ln det (\nI + D 2 rk(ST ) V u T ST\n)\nby monotonicity of ln det.\nTo prove the second claim, we instead take the comparator covariance \u03a3 = I equal to the prior covariance and again use V uT \u2264 D2 tr(ST ) to find\nRuT \u2264 \u03b7i ( V uT +D 2 tr (ST ) ) +\n1 D2 \u2016u\u20162 \u2212 2 \u03b1 ln\u03c0\u03b7i1\n2\u03b7i \u2264 2\u03b7iD2 tr (ST ) +\n1 D2 \u2016u\u20162 \u2212 2 \u03b1 ln\u03c0\u03b7i1\n2\u03b7i\n\u2264 2\u03b7D2 tr (ST ) + 1 D2 \u2016u\u20162 + 4 \u03b1 ln\n( 2 + 12 log2 T )\n\u03b7\nfor all \u03b7 \u2208 [ 1 5DG \u221a T , 15DG ]. Tuning \u03b7 as\n\u03b7\u0302 =\n\u221a 1 D2 \u2016u\u20162 + 4 \u03b1 ln ( 2 + 12 log2 T )\n2D2 tr (ST ) \u2265\n\u221a\n4 ln 2 2D2G2T \u2265 1 5DG \u221a T\nis allowed when \u03b7\u0302 \u2264 15DG , and gives\nRuT \u2264 \u221a 8D2 tr (ST ) ( 1\nD2 \u2016u\u20162 + 4 \u03b1 ln ( 2 + 12 log2 T\n) )\n.\nAlternatively, if \u03b7\u0302 \u2265 15DG , then we plug in \u03b7 = 15DG and obtain\nRuT \u2264 2\n5DG D2 tr (ST ) + 5DG\n( 1\nD2 \u2016u\u20162 + 4 \u03b1 ln ( 2 + 12 log2 T\n) )\n\u2264 10DG ( 1\nD2 \u2016u\u20162 + 4 \u03b1 ln ( 2 + 12 log2 T\n) )\n,\nwhere the second inequality follows from the constraint on \u03b7\u0302. In both cases, the second claim of the theorem follows."}, {"heading": "Appendix D. Discussion of the Choice of Grid Points and Prior Weights", "text": "We now think about the choice of the grid and corresponding prior. Theorem 7 above implies that any two \u03b7 that are within a constant factor of each other will guarantee the same bound up to another constant factor. Since \u03b7 is a continuous parameter, this suggests choosing a prior that is approximately uniform for ln \u03b7, which means it should have a density that looks like 1/\u03b7. Although Theorem 7 does not show it, there is never any harm in taking too many grid points, because grid points that are very close together will behave as a single point with combined prior mass. If we disregard computation, we would therefore like to use the prior discussed by Chernov and Vovk (2010), which is very close to uniform on ln \u03b7 and has density\n\u03c0(\u03b7) = C\n\u03b7 log22(5DG\u03b7) ,\nwhere we include the factor 5DG to make the prior invariant under rescalings of the problem, and C is a normalizing constant that makes the prior integrate to 1. To adapt this prior to a discrete grid,\nwe need to integrate this density between grid points and assign prior masses:\n\u03c0\u03b7i1 :=\n\u222b \u03b7i\n\u03b7i+1\n\u03c0(\u03b7) d\u03b7 = C\n\u2212 log2(5DG\u03b7 )\n\u2223 \u2223 \u2223 \u2223 \u2223 \u03b7i\n\u03b7i+1\n.\nFor the exponentially spaced grid in (9), this evaluates to the prior weights \u03c0\u03b7i1 specified there."}, {"heading": "Appendix E. Projection of Gaussians", "text": "It is well-known that the projection of a Gaussian onto the set of distributions with mean in the convex set U is also a Gaussian with the same covariance matrix. This result follows easily from, for instance, Theorems 1.8.5 and 1.8.2 of Ihara (1993), but we include a short proof for completeness:\nLemma 10 Let P\u0303t = N (\u00b5,\u03a3) be Gaussian and let Pt = argminP : \u00b5P\u2208U KL(P\u2016P\u0303t) be its projection onto the set of distributions with mean in U . Then Pt is also Gaussian with the same covariance matrix: Pt = N (\u03bd,\u03a3) for \u03bd \u2208 U that minimizes 12(\u03bd \u2212 \u00b5)\u22ba\u03a3\u22121(\u03bd \u2212 \u00b5).\nProof Let P be an arbitrary distribution with mean \u03bd \u2208 U , and let R = N (\u03bd,\u03a3). Then by straight-forward algebra and nonnegativity of Kullback-Leibler divergence it can be verified that\nKL(P\u2016P\u0303t) = KL(P\u2016R) + KL(R\u2016P\u0303t) \u2265 KL(R\u2016P\u0303t).\nThus the minimum over all P is achieved by a Gaussian with the same covariance matrix as P\u0303t. It remains to find the mean of the projection, which is the \u03bd \u2208 U that minimizes\nKL(R\u2016P\u0303t) = 1 2 (\u03bd \u2212 \u00b5)\u22ba\u03a3\u22121(\u03bd \u2212 \u00b5),\nas required."}], "references": [{"title": "Adaptive online gradient descent", "author": ["Peter L. Bartlett", "Elad Hazan", "Alexander Rakhlin"], "venue": "In Advances in Neural Information Processing Systems 20, Proceedings of the Twenty-First Annual Conference on Neural Information Processing Systems,", "citeRegEx": "Bartlett et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Bartlett et al\\.", "year": 2007}, {"title": "On the generalization ability of on-line learning algorithms", "author": ["Nicol\u00f2 Cesa-Bianchi", "Alex Conconi", "Claudio Gentile"], "venue": "IEEE Transactions on Information Theory,", "citeRegEx": "Cesa.Bianchi et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Cesa.Bianchi et al\\.", "year": 2004}, {"title": "Improved second-order bounds for prediction with expert advice", "author": ["Nicol\u00f2 Cesa-Bianchi", "Yishay Mansour", "Gilles Stoltz"], "venue": "Machine Learning,", "citeRegEx": "Cesa.Bianchi et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Cesa.Bianchi et al\\.", "year": 2007}, {"title": "Prediction with advice of unknown number of experts. In UAI, pages 117\u2013125", "author": ["Alexey V. Chernov", "Vladimir Vovk"], "venue": null, "citeRegEx": "Chernov and Vovk.,? \\Q2010\\E", "shortCiteRegEx": "Chernov and Vovk.", "year": 2010}, {"title": "Supermartingales in prediction with expert advice", "author": ["Alexey V. Chernov", "Yuri Kalnishkan", "Fedor Zhdanov", "Vladimir Vovk"], "venue": "Theor. Comput. Sci.,", "citeRegEx": "Chernov et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Chernov et al\\.", "year": 2010}, {"title": "Online optimization with gradual variations", "author": ["Chao-Kai Chiang", "Tianbao Yang", "Chia-Jung Le", "Mehrdad Mahdavi", "Chi-Jen Lu", "Rong Jin", "Shenghuo Zhu"], "venue": "In Proceedings of the 25th Annual Conference on Learning Theory, volume JMLR Workshop and Conference Proceedings Volume", "citeRegEx": "Chiang et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Chiang et al\\.", "year": 2012}, {"title": "Adaptive subgradient methods for online learning and stochastic optimization", "author": ["John Duchi", "Elad Hazan", "Yoram Singer"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Duchi et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Duchi et al\\.", "year": 2011}, {"title": "Composite objective mirror descent", "author": ["John C. Duchi", "Shai Shalev-Shwartz", "Yoram Singer", "Ambuj Tewari"], "venue": "In COLT 2010 - The 23rd Conference on Learning", "citeRegEx": "Duchi et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Duchi et al\\.", "year": 2010}, {"title": "A second-order bound with excess losses", "author": ["Pierre Gaillard", "Gilles Stoltz", "Tim van Erven"], "venue": "In JMLR Workshop and Conference Proceedings,", "citeRegEx": "Gaillard et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Gaillard et al\\.", "year": 2014}, {"title": "Introduction to online optimization", "author": ["Elad Hazan"], "venue": "DRAFT,", "citeRegEx": "Hazan.,? \\Q2015\\E", "shortCiteRegEx": "Hazan.", "year": 2015}, {"title": "Extracting certainty from uncertainty: Regret bounded by variation in costs", "author": ["Elad Hazan", "Satyen Kale"], "venue": "Machine learning,", "citeRegEx": "Hazan and Kale.,? \\Q2010\\E", "shortCiteRegEx": "Hazan and Kale.", "year": 2010}, {"title": "Beyond the regret minimization barrier: optimal algorithms for stochastic strongly-convex optimization", "author": ["Elad Hazan", "Satyen Kale"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Hazan and Kale.,? \\Q2014\\E", "shortCiteRegEx": "Hazan and Kale.", "year": 2014}, {"title": "Logarithmic regret algorithms for online convex optimization", "author": ["Elad Hazan", "Amit Agarwal", "Satyen Kale"], "venue": "Machine Learning,", "citeRegEx": "Hazan et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Hazan et al\\.", "year": 2007}, {"title": "Information Theory for Continuous Systems. Series on probability and statistics", "author": ["Shunsuke Ihara"], "venue": "World Scientific,", "citeRegEx": "Ihara.,? \\Q1993\\E", "shortCiteRegEx": "Ihara.", "year": 1993}, {"title": "Second-order quantile methods for experts and combinatorial games", "author": ["Wouter M. Koolen", "Tim van Erven"], "venue": "In Proceedings of The 28th Annual Conference on Learning Theory (COLT),", "citeRegEx": "Koolen and Erven.,? \\Q2015\\E", "shortCiteRegEx": "Koolen and Erven.", "year": 2015}, {"title": "Learning the learning rate for prediction with expert advice", "author": ["Wouter M. Koolen", "Tim van Erven", "Peter D. Gr\u00fcnwald"], "venue": "In Advances in Neural Information Processing Systems (NIPS)", "citeRegEx": "Koolen et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Koolen et al\\.", "year": 2014}, {"title": "Adaptive bound optimization for online convex optimization", "author": ["H. Brendan McMahan", "Matthew J. Streeter"], "venue": "In COLT 2010 - The 23rd Conference on Learning", "citeRegEx": "McMahan and Streeter.,? \\Q2010\\E", "shortCiteRegEx": "McMahan and Streeter.", "year": 2010}, {"title": "Efficient estimation of word representations in vector space", "author": ["Tomas Mikolov", "Kai Chen", "Greg S. Corrado", "Jeffrey Dean"], "venue": "International Conference on Learning Representations,", "citeRegEx": "Mikolov et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Mikolov et al\\.", "year": 2013}, {"title": "Introductory lectures on convex optimization : a basic course", "author": ["Yurii Nesterov"], "venue": null, "citeRegEx": "Nesterov.,? \\Q2004\\E", "shortCiteRegEx": "Nesterov.", "year": 2004}, {"title": "Primal-dual subgradient methods for convex problems", "author": ["Yurii Nesterov"], "venue": "Math. Program.,", "citeRegEx": "Nesterov.,? \\Q2009\\E", "shortCiteRegEx": "Nesterov.", "year": 2009}, {"title": "Online learning with predictable sequences", "author": ["Alexander Rakhlin", "Karthik Sridharan"], "venue": "In COLT 2013 - The 26th Annual Conference on Learning Theory, June 12-14,", "citeRegEx": "Rakhlin and Sridharan.,? \\Q2013\\E", "shortCiteRegEx": "Rakhlin and Sridharan.", "year": 2013}, {"title": "Making gradient descent optimal for strongly convex stochastic optimization", "author": ["Alexander Rakhlin", "Ohad Shamir", "Karthik Sridharan"], "venue": "In Proceedings of the 29th International Conference on Machine Learning,", "citeRegEx": "Rakhlin et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Rakhlin et al\\.", "year": 2012}, {"title": "Deep learning in neural networks: An overview", "author": ["J\u00fcrgen Schmidhuber"], "venue": "Neural Networks,", "citeRegEx": "Schmidhuber.,? \\Q2015\\E", "shortCiteRegEx": "Schmidhuber.", "year": 2015}, {"title": "An empirical study of learning rates in deep neural networks for speech recognition", "author": ["Andrew Senior", "Georg Heigold", "Marc\u2019Aurelio Ranzato", "Ke Yang"], "venue": "In IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP),", "citeRegEx": "Senior et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Senior et al\\.", "year": 2013}, {"title": "Online learning and online convex optimization", "author": ["Shai Shalev-Shwartz"], "venue": "Foundations and Trends in Machine Learning,", "citeRegEx": "Shalev.Shwartz.,? \\Q2012\\E", "shortCiteRegEx": "Shalev.Shwartz.", "year": 2012}, {"title": "Stochastic gradient descent for non-smooth optimization: Convergence results and optimal averaging schemes", "author": ["Ohad Shamir", "Tong Zhang"], "venue": "In Proceedings of the 30th International Conference on Machine Learning,", "citeRegEx": "Shamir and Zhang.,? \\Q2013\\E", "shortCiteRegEx": "Shamir and Zhang.", "year": 2013}, {"title": "Adaptivity and optimism: An improved exponentiated gradient algorithm", "author": ["Jacob Steinhardt", "Percy Liang"], "venue": "In Proceedings of the 31th International Conference on Machine Learning,", "citeRegEx": "Steinhardt and Liang.,? \\Q2014\\E", "shortCiteRegEx": "Steinhardt and Liang.", "year": 2014}, {"title": "Optimal rates in convex optimization", "author": ["Ryan Tibshirani"], "venue": "With Larry Wasserman,", "citeRegEx": "Tibshirani.,? \\Q2014\\E", "shortCiteRegEx": "Tibshirani.", "year": 2014}, {"title": "Dual averaging method for regularized stochastic learning and online optimization", "author": ["Lin Xiao"], "venue": "In Advances in Neural Information Processing Systems", "citeRegEx": "Xiao.,? \\Q2009\\E", "shortCiteRegEx": "Xiao.", "year": 2009}, {"title": "Dual averaging methods for regularized stochastic learning and online optimization", "author": ["Lin Xiao"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Xiao.,? \\Q2010\\E", "shortCiteRegEx": "Xiao.", "year": 2010}, {"title": "ADADELTA: an adaptive learning rate method", "author": ["Matthew D. Zeiler"], "venue": null, "citeRegEx": "Zeiler.,? \\Q2012\\E", "shortCiteRegEx": "Zeiler.", "year": 2012}, {"title": "Online convex programming and generalized infinitesimal gradient ascent", "author": ["Martin Zinkevich"], "venue": "In ICML, pages 928\u2013936,", "citeRegEx": "Zinkevich.,? \\Q2003\\E", "shortCiteRegEx": "Zinkevich.", "year": 2003}, {"title": "Theoretical Guarantees for Algorithms in Multi-Agent Settings", "author": ["Martin Zinkevich"], "venue": "PhD thesis,", "citeRegEx": "Zinkevich.,? \\Q2004\\E", "shortCiteRegEx": "Zinkevich.", "year": 2004}, {"title": "Projection of Gaussians It is well-known that the projection of a Gaussian onto the set of distributions with mean in the convex set U is also a Gaussian with the same covariance matrix. This result follows easily from, for instance, Theorems", "author": ["E. Appendix"], "venue": null, "citeRegEx": "Appendix,? \\Q1993\\E", "shortCiteRegEx": "Appendix", "year": 1993}], "referenceMentions": [{"referenceID": 24, "context": "Introduction Methods for online convex optimization (OCO) (Shalev-Shwartz, 2012; Hazan, 2015) make it possible to optimize parameters sequentially, by processing convex functions in a streaming fashion.", "startOffset": 58, "endOffset": 93}, {"referenceID": 9, "context": "Introduction Methods for online convex optimization (OCO) (Shalev-Shwartz, 2012; Hazan, 2015) make it possible to optimize parameters sequentially, by processing convex functions in a streaming fashion.", "startOffset": 58, "endOffset": 93}, {"referenceID": 31, "context": "The simplest and most commonly used strategy for OCO is online gradient descent (GD) (Zinkevich, 2003), which updates parameters wt+1 = wt \u2212 \u03b7\u2207ft(wt) by taking a step in the direction of the negative gradient.", "startOffset": 85, "endOffset": 102}, {"referenceID": 24, "context": "For carefully tuned step size \u03b7, this guarantees that the regret over T rounds, which measures the difference in performance between the GD iterates wt and the best offline parameters u, is bounded by (Shalev-Shwartz, 2012)", "startOffset": 201, "endOffset": 223}, {"referenceID": 0, "context": "For strongly convex functions this is achieved by GD with step size proportional to 1/t (Bartlett et al., 2007), while exp-concave functions require more sophisticated algorithms like Online Newton Step (Hazan et al.", "startOffset": 88, "endOffset": 111}, {"referenceID": 12, "context": ", 2007), while exp-concave functions require more sophisticated algorithms like Online Newton Step (Hazan et al., 2007).", "startOffset": 99, "endOffset": 119}, {"referenceID": 6, "context": "We also include AdaGrad (Duchi et al., 2011), which is a state-of-the-art adaptive OCO method that is commonly used in practice (Mikolov et al.", "startOffset": 24, "endOffset": 44}, {"referenceID": 17, "context": ", 2011), which is a state-of-the-art adaptive OCO method that is commonly used in practice (Mikolov et al., 2013; Schmidhuber, 2015).", "startOffset": 91, "endOffset": 132}, {"referenceID": 22, "context": ", 2011), which is a state-of-the-art adaptive OCO method that is commonly used in practice (Mikolov et al., 2013; Schmidhuber, 2015).", "startOffset": 91, "endOffset": 132}, {"referenceID": 12, "context": "Previously, such fast O(d lnT ) rates have only been obtained for functions with significant curvature, which are either strongly convex or at least exp-concave (Hazan et al., 2007).", "startOffset": 161, "endOffset": 181}, {"referenceID": 9, "context": "Previously, such fast O(d lnT ) rates have only been obtained for functions with significant curvature, which are either strongly convex or at least exp-concave (Hazan et al., 2007). Here we obtain O(d ln T ) regret for any fixed convex function, even if it has no curvature at all. At first sight this may appear to contradict the lower bound of order 1/ \u221a T for convergence of the iterates by Nesterov (2004) (see also Tibshirani (2014)), which implies a lower bound of order \u221a T on the regret.", "startOffset": 162, "endOffset": 411}, {"referenceID": 9, "context": "Previously, such fast O(d lnT ) rates have only been obtained for functions with significant curvature, which are either strongly convex or at least exp-concave (Hazan et al., 2007). Here we obtain O(d ln T ) regret for any fixed convex function, even if it has no curvature at all. At first sight this may appear to contradict the lower bound of order 1/ \u221a T for convergence of the iterates by Nesterov (2004) (see also Tibshirani (2014)), which implies a lower bound of order \u221a T on the regret.", "startOffset": 162, "endOffset": 439}, {"referenceID": 6, "context": "Among these, AdaGrad (Duchi et al., 2011) is an important extension of gradient descent that improves practical performance in training large neural networks (Mikolov et al.", "startOffset": 21, "endOffset": 41}, {"referenceID": 17, "context": ", 2011) is an important extension of gradient descent that improves practical performance in training large neural networks (Mikolov et al., 2013; Schmidhuber, 2015).", "startOffset": 124, "endOffset": 165}, {"referenceID": 22, "context": ", 2011) is an important extension of gradient descent that improves practical performance in training large neural networks (Mikolov et al., 2013; Schmidhuber, 2015).", "startOffset": 124, "endOffset": 165}, {"referenceID": 23, "context": "Methods that do this may suffer when the initial gradients are much larger than subsequent ones, because they can never recover from decreasing the learning rate too much in the first few rounds (Senior et al., 2013; Zeiler, 2012).", "startOffset": 195, "endOffset": 230}, {"referenceID": 30, "context": "Methods that do this may suffer when the initial gradients are much larger than subsequent ones, because they can never recover from decreasing the learning rate too much in the first few rounds (Senior et al., 2013; Zeiler, 2012).", "startOffset": 195, "endOffset": 230}, {"referenceID": 5, "context": "Among these, AdaGrad (Duchi et al., 2011) is an important extension of gradient descent that improves practical performance in training large neural networks (Mikolov et al., 2013; Schmidhuber, 2015). The crux of AdaGrad (and similar methods developed independently by McMahan and Streeter (2010)) is their adaptive online tuning of the learning rate.", "startOffset": 22, "endOffset": 297}, {"referenceID": 5, "context": "Among these, AdaGrad (Duchi et al., 2011) is an important extension of gradient descent that improves practical performance in training large neural networks (Mikolov et al., 2013; Schmidhuber, 2015). The crux of AdaGrad (and similar methods developed independently by McMahan and Streeter (2010)) is their adaptive online tuning of the learning rate. As briefly mentioned in the examples above, in one dimension AdaGrad guarantees the optimally tuned GD bound (1). In higher dimensions gradient energy may vary wildly between dimensions (e.g. due to rarely occurring but informative features). AdaGrad capitalizes on this by learning a separate learning rate per dimension (diagonal version) or even a positive definite learning rate matrix (full version) to find the optimal basis. Another notion of adaptivity is explored by Hazan and Kale (2010) and Chiang et al.", "startOffset": 22, "endOffset": 850}, {"referenceID": 5, "context": "Another notion of adaptivity is explored by Hazan and Kale (2010) and Chiang et al. (2012), who obtain tighter bounds for (linear) functions ft that vary little between rounds (as measured by either deviation from the mean or by successive differences).", "startOffset": 70, "endOffset": 91}, {"referenceID": 24, "context": "In the language of Steinhardt and Liang (2014), the quadratic part causes an \u201cadaptive regularizer\u201d and Duchi et al.", "startOffset": 19, "endOffset": 47}, {"referenceID": 6, "context": "In the language of Steinhardt and Liang (2014), the quadratic part causes an \u201cadaptive regularizer\u201d and Duchi et al. (2011) call it \u201ctemporal adaptation of the proximal function\u201d, but in our approach it is more convenient to treat the two parts of the surrogate loss together, without moving the quadratic part into the regularizer.", "startOffset": 104, "endOffset": 124}, {"referenceID": 15, "context": "The (approximately) best learning rate is notoriously difficult to track online because the regret is non-monotonic over rounds and may have several local minima as a function of \u03b7 (see Koolen et al. (2014) for a study in the expert setting).", "startOffset": 186, "endOffset": 207}, {"referenceID": 9, "context": "The Slave algorithm is very similar to what would result from applying Online Newton Step (ONS) by Hazan et al. (2007) to our surrogate loss l\u03b7t (u), which we will prove to be expconcave.", "startOffset": 99, "endOffset": 119}, {"referenceID": 27, "context": "One might alternatively consider the Lazy Projection version (as in Zinkevich (2004); Nesterov (2009); Xiao (2010)) that forgets past projections when updating on new data.", "startOffset": 68, "endOffset": 85}, {"referenceID": 18, "context": "One might alternatively consider the Lazy Projection version (as in Zinkevich (2004); Nesterov (2009); Xiao (2010)) that forgets past projections when updating on new data.", "startOffset": 86, "endOffset": 102}, {"referenceID": 18, "context": "One might alternatively consider the Lazy Projection version (as in Zinkevich (2004); Nesterov (2009); Xiao (2010)) that forgets past projections when updating on new data.", "startOffset": 86, "endOffset": 115}, {"referenceID": 6, "context": "We refer to Duchi et al. (2011) for examples of how to compute projections for various domains U .", "startOffset": 12, "endOffset": 32}, {"referenceID": 2, "context": "For the full case, where \u03b1 = \u03b1full = 1, exp-concavity immediately implies the following useful analog of the \u201cprod bound\u201d (Cesa-Bianchi et al., 2007; Gaillard et al., 2014; Koolen and Van Erven, 2015) by taking the tangent of e\u2212l \u03b7 t (w \u03b7 t ) at w t = wt.", "startOffset": 122, "endOffset": 200}, {"referenceID": 8, "context": "For the full case, where \u03b1 = \u03b1full = 1, exp-concavity immediately implies the following useful analog of the \u201cprod bound\u201d (Cesa-Bianchi et al., 2007; Gaillard et al., 2014; Koolen and Van Erven, 2015) by taking the tangent of e\u2212l \u03b7 t (w \u03b7 t ) at w t = wt.", "startOffset": 122, "endOffset": 200}, {"referenceID": 4, "context": "We remark that one may view our potential function \u03a6T as a game-theoretic supermartingale in the sense of Chernov et al. (2010), and this lemma as establishing that the MetaGrad Master is the corresponding defensive forecasting strategy.", "startOffset": 106, "endOffset": 128}, {"referenceID": 12, "context": "In the full case one may use a general method for exp-concave functions, like Online Newton Step, to get O(d ln T ) regret (Hazan et al., 2007).", "startOffset": 123, "endOffset": 143}, {"referenceID": 23, "context": "First, the monotonicity constraint prevents the algorithm from ever recovering if some hard initial data cause the learning rate to go down (Senior et al., 2013).", "startOffset": 140, "endOffset": 161}, {"referenceID": 1, "context": "the best parameters u for P by averaging the parameters: w\u0304T = 1 T \u2211T t=1wt (Cesa-Bianchi et al., 2004).", "startOffset": 76, "endOffset": 103}, {"referenceID": 21, "context": "This happens because parameter estimates in the first rounds are much worse than those in the later rounds, and can be fixed by averaging only the later parameters (Rakhlin et al., 2012; Shamir and Zhang, 2013) or by dividing the data into exponentially increasing epochs and using the average of the parameters over the previous epoch as a starting point for the next (Hazan and Kale, 2014).", "startOffset": 164, "endOffset": 210}, {"referenceID": 25, "context": "This happens because parameter estimates in the first rounds are much worse than those in the later rounds, and can be fixed by averaging only the later parameters (Rakhlin et al., 2012; Shamir and Zhang, 2013) or by dividing the data into exponentially increasing epochs and using the average of the parameters over the previous epoch as a starting point for the next (Hazan and Kale, 2014).", "startOffset": 164, "endOffset": 210}, {"referenceID": 11, "context": ", 2012; Shamir and Zhang, 2013) or by dividing the data into exponentially increasing epochs and using the average of the parameters over the previous epoch as a starting point for the next (Hazan and Kale, 2014).", "startOffset": 190, "endOffset": 212}, {"referenceID": 7, "context": "In particular it would be interesting to integrate composite losses (Duchi et al., 2010; Xiao, 2009) and optimistic gradient estimates (Chiang et al.", "startOffset": 68, "endOffset": 100}, {"referenceID": 28, "context": "In particular it would be interesting to integrate composite losses (Duchi et al., 2010; Xiao, 2009) and optimistic gradient estimates (Chiang et al.", "startOffset": 68, "endOffset": 100}, {"referenceID": 5, "context": ", 2010; Xiao, 2009) and optimistic gradient estimates (Chiang et al., 2012; Rakhlin and Sridharan, 2013).", "startOffset": 54, "endOffset": 104}, {"referenceID": 20, "context": ", 2010; Xiao, 2009) and optimistic gradient estimates (Chiang et al., 2012; Rakhlin and Sridharan, 2013).", "startOffset": 54, "endOffset": 104}, {"referenceID": 5, "context": ", 2010; Xiao, 2009) and optimistic gradient estimates (Chiang et al., 2012; Rakhlin and Sridharan, 2013). We would further like to know whether it is possible to learn a learning rate vector (one rate per dimension), or a positive definite learning rate matrix (one rate per direction) like Duchi et al. (2011). We have tried this ourselves, but are stuck at the construction of the master.", "startOffset": 55, "endOffset": 311}], "year": 2017, "abstractText": "In online convex optimization it is well known that objective functions with curvature are much easier than arbitrary convex functions. Here we show that the regret can be significantly reduced even without curvature, in cases where there is a stable optimum to converge to. More precisely, the regret of existing methods is determined by the norms of the encountered gradients, and matching worst-case performance lower bounds tell us that this cannot be improved uniformly. Yet we argue that this is a rather pessimistic assessment of the complexity of the problem. We introduce a new parameter-free algorithm, called MetaGrad, for which the gradient norms in the regret are scaled down by the distance to the (unknown) optimum. So when the optimum is reasonably stable over time, making the algorithm converge, this new scaling leads to orders of magnitude smaller regret even when the gradients themselves do not vanish. MetaGrad does not require any manual tuning, but instead tunes a learning rate parameter automatically for the data. Unlike all previous methods with provable guarantees, its learning rates are not monotonically decreasing over time, but instead are based on a novel aggregation technique. We provide two versions of MetaGrad. The first maintains a full covariance matrix to guarantee the sharpest bounds for problems where we can afford update time quadratic in the dimension. The second version maintains only the diagonal. Its linear cost in the dimension makes it suitable for large-scale problems.", "creator": "LaTeX with hyperref package"}}}