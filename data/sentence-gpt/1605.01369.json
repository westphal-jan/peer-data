{"id": "1605.01369", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "4-May-2016", "title": "Accelerating Deep Learning with Shrinkage and Recall", "abstract": "Deep Learning is a very powerful machine learning model. Deep Learning trains a large number of parameters for multiple layers and is very slow when data is in large scale and the architecture size is large. Inspired from the shrinking technique used in accelerating computation of Support Vector Machines (SVM) algorithm and screening technique used in LASSO, we propose a shrinking Deep Learning with recall (sDLr) approach to speed up deep learning computation. We experiment shrinking Deep Learning with recall (sDLr) using Deep Neural Network (DNN), Deep Belief Network (DBN) and Convolution Neural Network (CNN) on 4 data sets. Results show that the speedup using shrinking Deep Learning with recall (sDLr) can reach more than 2.0 while still giving competitive classification performance.\n\n\n\nDeep Learning\nThe following is a small summary of data sets which are based on the deep learning technique.\nIn order to learn Deep Learning, you need to perform a number of tasks. The tasks performed by deep learning are:\n1) Search the data from the Deep learning tool set. Then, use the Deep Learning Machine Learning dataset to search for additional information. If the search was complete, you can find all data in the dataset, and then search for more information using a Deep Learning Machine Learning dataset. The dataset consists of the Deep Learning Machine Learning dataset (DNN) for all datasets. It also contains a deep learning model for all datasets.\nIn order to learn Deep Learning, you need to perform a number of tasks. The tasks performed by deep learning are:\n1) Search the data from the Deep learning tool set. Then, use the Deep Learning Machine Learning dataset to search for additional information. If the search was complete, you can find all data in the dataset, and then search for more information using a Deep Learning Machine Learning dataset. The dataset consists of the Deep Learning Machine Learning dataset (DNN) for all datasets. It also contains a deep learning model for all datasets.\nIn order to learn Deep Learning, you need to perform a number of tasks. The tasks performed by deep learning are:\n1) Search the data from the Deep learning tool set. Then, use the Deep Learning Machine Learning dataset to search for additional information. If the search was complete, you can find all data in the dataset, and then search for more information using a Deep Learning Machine Learning dataset. The dataset consists of the Deep Learning Machine Learning dataset (DNN) for all datasets. It also contains a deep learning model for", "histories": [["v1", "Wed, 4 May 2016 18:17:37 GMT  (276kb,D)", "https://arxiv.org/abs/1605.01369v1", null], ["v2", "Mon, 19 Sep 2016 19:27:39 GMT  (4119kb)", "http://arxiv.org/abs/1605.01369v2", "The 22nd IEEE International Conference on Parallel and Distributed Systems (ICPADS 2016)"]], "reviews": [], "SUBJECTS": "cs.LG cs.CV cs.NE", "authors": ["shuai zheng", "abhinav vishnu", "chris ding"], "accepted": false, "id": "1605.01369"}, "pdf": {"name": "1605.01369.pdf", "metadata": {"source": "CRF", "title": "Accelerating Deep Learning with Shrinkage and Recall", "authors": ["Shuai Zheng", "Abhinav Vishnu", "Chris Ding"], "emails": ["abhinav.vishnu@pnnl.gov", "zhengs123@gmail.com,", "chqding@uta.edu"], "sections": [{"heading": null, "text": "ar X\niv :1\n60 5.\n01 36\n9v 2\n[ cs\n.L G\n] 1\n9 Se\np 20\nKeywords-Deep Learning; Deep Neural Network (DNN); Deep Belief Network (DBN); Convolution Neural Network (CNN)\nI. INTRODUCTION\nDeep Learning [1] has become a powerful machine learning model. It differs from traditional machine learning approaches in the following aspects: Firstly, Deep Learning contains multiple non-linear hidden layers and can learn very complicated relationships between inputs and outputs. Deep architectures using multiple layers outperform shadow models [2]. Secondly, there is no need to extract human design features [3], which can reduce the dependence of the quality of human extracted features. We mainly study three Deep Learning models in this work: Deep Neural Networks (DNN), Deep Belief Network (DBN) and Convolution Neural Network (CNN).\nDeep Neural Network (DNN) is the very basic deep learning model. It contains multiple layers with many hidden neurons with non-linear activation function in each layer. Figure 1 shows one simple example of Deep Neural Network (DNN) model. This Deep neural network has one input layer, two hidden layers and one output layer. Training process of Deep Neural Network (DNN) includes forward propagation and back propagation. Forward propagation uses the current connection weight to give a prediction based on current state\nThis work was conducted while the first author was doing internship at Pacific Northwest National Laboratory, Richland, WA, USA.\nof model. Back propagation computes the amount of weight should be changed based on the difference of ground truth label and forward propagation prediction. Back propagation in Deep Neural Network (DNN) is a non-convex problem. Different initialization affects classification accuracy and convergence speed of models.\nSeveral unsupervised pretraining methods for neural network have been proposed to improve the performance of random initialized DNN, such as using stacks of RBMs (Restricted Boltzmann Machines) [1], autoencoders [4], or DBM (Deep Boltzmann Machines) [5]. Compared to random initialization, pretraining followed with finetuning backpropagation will improve the performance significantly. Deep Belief Network (DBN) is a generative unsupervised pretraining network which uses stacked RBMs [6] during pretraining. A DNN with a corresponding configured DBN often produces much better results. DBN has undirected connections between its first two layers and directed connections between all its lower layers[7] [5].\nConvolution Neural Network (CNN) [8] [3] [9] has been proposed to deal with images, speech and time-series. This is because standard DNN has some limitations. Firstly, images, speeches are usually large. A simple Neural Network to process an image size of 100 \u00d7 100 with 1 layer of 100 hidden neurons will require 1,000,000 (100 \u00d7 100 \u00d7 100) weight parameters. With so many variables, it will lead to overfitting easily. Computation of standard DNN model requires expensive memory too. Secondly, standard DNN does not consider the local structure and topology of the input. For example, images have strong 2D local structure. Many areas in the image are similar. Speeches have a strong 1D structure, where variables temporally nearby are highly correlated. CNN forces the extraction of local features by restricting the receptive fields of hidden neurons to be local [9].\nHowever, the training process for deep learning algorithms, including DNN, DBN, CNN, is computationally expensive. This is due to the large number of training data and a large number of parameters for multiple layers. Inspired from the shrinking technique [10] [11] used in accelerating computation of Support Vector Machines (SVM) algorithm\nand screening [12] [13] technique used in LASSO, we propose an accelerating algorithm shrinking Deep Learning with Recall (sDLr). The main contribution of sDLr is that it can reduce the running time significantly. Though there is a trade-off between classification improvement and speedup on training time, for some data sets, sDLr approach can even improve classification accuracy. It should be noted that the approach sDLr is a general model and a new way of thinking, which can be applied to both large data, large network and small data small network, both sequential and parallel implementations. We will study the impact of proposed accelerating approaches on DNN, DBN and CNN using 4 data sets from computer vision and high energy physics, biology science."}, {"heading": "II. MOTIVATION", "text": "The amount of data in our world has been exploding. Analyzing large data sets, so-called big data, will become a key basis of competition, underpinning new waves of productivity growth, innovation, and consumer interest [14]. A lot of big data technologies, including cloud computing, dimensionality reduction have been proposed [15], [16], [17], [18], [19]. Analyzing big data with machine learning algorithms requires special hardware implementations and large amount of running time.\nSVM [20] solves the following optimization problem:\nmin w,\u03be,b\n1 2 wTw + C\nn\u2211\ni=1\n\u03bei, (1)\nsubject to yi(wT\u03a6(xi)\u2212 b) > 1\u2212 \u03bei,\n\u03bei > 0, i = 1, ..., n,\nwhere xi is a training sample, yi is the corresponding label, \u03bei is positive slack variable, \u03a6(xi) is mapping function, w gives the solution and is known as weight vector, C controls the relative importance of maximizing the margin\nand minimizing the amount of the slack. Since SVM learning problem has much less support vectors than training examples, shrinking [10] [11] was proposed to eliminate training samples for large learning tasks where the fraction of support vectors is small compared to the training sample size or when many support vectors are at the upper bound of Lagrange multipliers.\nLASSO [21] is an optimization problem to find sparse representation of some signals with respect to a predefined dictionary. It solves the following problem:\nmin x\n1 2 \u2016Dx\u2212 y\u201622 + \u03bb\u2016x\u20161, (2)\nwhere y is a testing point, D \u2208 \u211cp\u00d7n is a dictionary with dimension p and size n, \u03bb is a parameter controls the sparsity of representation x. When both p and n are large, which is usually the case in practical applications, such as denoising or classification, it is difficult and time-intensive to compute. Screening [12] [13] is a technique used to reduce the size of dictionary using some rules in order to accelerate the computation of LASSO.\nEither in shrinking of SVM or in screening of LASSO, these approaches are trying to reduce the size of computation data. Inspired from these two techniques, we propose a faster and reliable approach for deep learning, shrinking Deep Learning."}, {"heading": "III. SHRINKING DEEP LEARNING", "text": "Given testing point xi \u2208 \u211cp\u00d71, i = 1, 2, ..., n, let class indicator vector be y(0)i \u2208 \u211c\n1\u00d7c, where n is number of testing samples, c is number of classes, yi has all 0s except one 1 to indicate the class of this test point. Let the output of a neural network for testing point xi be yi \u2208 \u211c1\u00d7c. yi contains continuous values and is the ith row of Y."}, {"heading": "A. Standard Deep Learning", "text": "Algorithm 1 gives the framework of standard deep learning. During each epoch (iteration), standard deep learning first runs a forward-propagation on all training data, then computes the output Y(w), where output Y is a function of weight parameters w. Deep learning tries to find an optimal w to minimize error loss e = [e1, ..., en], which can be sum squared error loss (DNN, DBN in our experiment) or softmax loss (CNN in our experiment). In backpropagation process, deep learning updates weight parameter vector using gradient descent. For an training data xi, gradient descent can be denoted as:\nw(epoch+1) = w(epoch) \u2212 \u03b7\u2207ei(w (epoch)), (3)\nwhere \u03b7 is step size. Before we present shrinking Deep Leaning algorithm, we first give Lemma 1. Lemma 1: Magnitude of gradient \u2207ei(w(epoch)) in Eq.(3) is positive correlated with the error ei, i = 1, ..., n.\nAlgorithm 1 Deep Learning (DL)\nInput: Data matrix X \u2208 \u211cp\u00d7n, class matrix Y(0) \u2208 \u211cn\u00d7c Output: Classification error 1: Preprocessing training data 2: Active training data index A = {1, 2, ..., n} 3: for epoch = 1, 2, ... do 4: Run forward-propagation on A 5: Compute forward-propagation output Y \u2208 \u211cn\u00d7c\n6: Run back-propagation 7: Update weight w using Eq.(3) 8: end for 9: Compute classification error using Y and Y(0)\nProof: In the case of sum squared error, error loss of sample xi is given as:\nei = 1\n2\nc\u2211\nk=1\n(yik(w)\u2212 y (0) ik ) 2, i = 1, ..., n. (4)\nUsing Eq.(4), gradient \u2207ei(w) is:\n\u2207ei(w) =\nc\u2211\nk=1\n(yik(w)\u2212 y (0) ik )\u2207yik(w)\n\u2207ei(w) = \u2207yi(w)(yi \u2212 y (0) i ) T . (5)\nAs we can see from Eq.(5), \u2207ei(w) is linear related to (yi \u2212 y (0) i ). Data points with larger error will have larger gradient, thus will have a stronger and larger correction signal when updating w. Data points with smaller error will have smaller gradient, thus will have a weaker and smaller correction signal when updating w.\nIn the case of softmax loss function, ei is denoted as:\nei = \u2212 c\u2211\nk=1\ny (0) ik logpik, i = 1, ..., n (6)\npik = exp(yik(w))\u2211c j=1 exp(yij(w)) . (7)\nUsing Eq.(6), gradient \u2207ei(w) is:\n\u2207ei(w) =\nc\u2211\nk=1\n\u2202ei(yik)\n\u2202yik \u2207yik(w),\n\u2207ei(w) = c\u2211\nk=1\n(pik \u2212 y (0) ik )\u2207yik(w). (8)\nNow let\u2019s see the relation between softmax loss function (Eq.(6)) and its gradient with respect to weight parameter w (Eq.(8)). For example, given point i is in class 1, so y (0) i1 = 1 and y (0) ij = 0, j 6= 1. When pi1 is large, pi1 \u2192 1, softmax loss function (Eq.(6)) is very small. For gradient of softmax loss function (Eq.(8)), when k = 1, (pi1 \u2212 1) is close to 0; when k 6= 1, (pi1 \u2212 0) is also close to 0. In summary, when softmax loss function (Eq.(6)) is very small, its gradient (Eq.(8)) is also very small.\nAlgorithm 2 Shrinking Deep Learning (sDL)\nInput: Data matrix X \u2208 \u211cp\u00d7n, class matrix Y(0) \u2208 \u211cn\u00d7c, elimination rate s (s is a percentage), stop threshold t Output: Classification error 1: Preprocessing training data 2: Active training data index A = {1, 2, ..., n} 3: for epoch = 1, 2, ... do 4: Run forward-propagation on A 5: Compute forward-propagation output Y \u2208 \u211cn\u00d7c\n6: Run back-propagation 7: Update weight w using Eq.(3) 8: if nepoch >= t then 9: Compute error using Eq.(4)\n10: Compute set S, which contains indexes of nepochs smallest ei values (nepoch is size of A in current epoch) 11: Eliminate all samples in S and update A, A = A\u2212 S 12: end if 13: end for 14: Compute classification error using Y and Y(0)\nAlgorithm 3 Shrinking Deep Learning with Recall (sDLr)\nInput: Data matrix X \u2208 \u211cp\u00d7n, class matrix Y(0) \u2208 \u211cn\u00d7c, elimination rate s (s is a percentage), stop threshold t Output: Classification error 1: Preprocessing training data 2: Active training data index A = {1, 2, ..., n}, A0 = A 3: for epoch = 1, 2, ... do 4: Run forward-propagation on A 5: Compute forward-propagation output Y \u2208 \u211cn\u00d7c\n6: Run back-propagation 7: Update weight w using Eq.(3) 8: if nepoch >= t then 9: Compute error using Eq.(4)\n10: Compute set S, which contains indexes of nepochs smallest ei values (nepoch is size of A in current epoch) 11: Eliminate all samples in S and update A, A = A\u2212 S 12: else 13: Use all data for training, A = A0 14: end if 15: end for 16: Compute classification error using Y and Y(0)"}, {"heading": "B. Shrinking Deep Learning", "text": "In order to accelerate computation and inspired from techniques of shrinking in SVM and screening of LASSO, we propose shrinking Deep Learning in Algorithm 2 by eliminating samples with small error (Eq.(4)) from training data and use less data for training.\nAlgorithm 2 gives the outline of shrinking Deep Learning (sDL). Compared to standard deep learning in Algorithm 1, sDL requires two more inputs, elimination rate s and stop threshold t. s is a percentage indicating the amount of training data to be eliminated during one epoch, t is a number indication to stop eliminating training data when nepoch < t, where nepoch is current number of training data. We maintain an index vector A. In Algorithm 1, both forward and backward propagation apply on all training\ndata. In Algorithm 2, the training process is applied on a subset of all training data. In the first epoch, we set A = {1, 2, ..., n} to include all training indexes. After forward and backward propagation in each epoch, we select the nepochs indexes of training data with smallest error ei, where nepoch is size of current number of training data A. Then we eliminate indexes in S from A, and update A, A = A\u2212 S. When nepoch < t, we stop eliminating training data anymore. Lemma 1 gives theoretical foundation that samples with small error will smaller impact on the gradient. Thus eliminating those samples will not impact the gradient significantly. Figure 2 shows that the errors using sDL is smaller than errors using DL, which proves that sDL gives a stronger correction signal and reduce the errors faster.\nWhen eliminating samples, elimination rate s denotes the percentage of samples to be removed. We select the nepochs indexes of training data with smallest error ei. For the same epoch, in different batches, the threshold used to eliminate samples is different. Assume there are nbatch batches one epoch, in every batch, we need to drop nepochs/nbatch samples on average. In batch i, let the threshold to drop nepochs/nbatch smallest error be ti; in batch i + 1, let the threshold be ti+1. ti and ti+1 will differ a lot. We use exponential smoothing [22] to adjust the threshold used in batch i+1: instead of using ti+1 as the threshold to eliminate samples, we use the following t\u2032i+1:\nt\u2032i+1 = \u03b1t \u2032 i + (1\u2212 \u03b1)ti+1, (9)\nwhere \u03b1 \u2208 [0, 1) is a weight parameter which controls the importance of past threshold values, t\u20321 = t1. The intuition using exponential smoothing is that we want the threshold used in each epoch to be consistent. Samples with errors less than t\u2032i+1 in batch i+1 will be eliminated. If \u03b1 is close to 0, the smoothing effect on threshold is not obvious; if \u03b1 is close to 1, the threshold t\u2032i+1 will deviate a lot from ti+1. In practical, we find \u03b1 between 0.5 and 0.6 is a good setting in terms of smoothing threshold. We will show this in experiment part."}, {"heading": "IV. SHRINKING WITH RECALL", "text": "As the training data in sDL becomes less and less, the weight parameter w trained is based on the subset of training data. It is not optimized for the entire training dataset. We now introduce shrinking Deep Learning with recall (Algorithm 3) to deal with this situation. In order to utilize all the training data, when the number of active training samples nepoch < t, we start to use all training samples, as shown in Algorithm 3, A = A0. Algorithm 3 ensures that the model trained is optimized for the entire training data. Shrinking with recall of Algorithm 3 will produce competitive classification performance with standard Deep Learning of Algorithm 1. In experiment, we will also investigate the impact the threshold t on the classification results (see Figure 7)."}, {"heading": "V. EXPERIMENTS", "text": "In experiment, we test our algorithms on data sets of different domains using 5 different random initialization. The data sets we used are listed in Table I. MNIST is a standard toy data set of handwritten digits; CIFAR-10\nTable II: MNIST classification error improvement (IMP) and training time Speedup.\nMethod DNN sDNNr IMP/Speedup DBN sDBNr IMP/Speedup CNN sCNNr IMP/Speedup Testing error 0.0387 0.0324 16.3% 0.0192 0.0182 5.21% 0.0072 0.0073 \u22121.39%\nTraining time (s) 1653 805 2.05 1627 700 2.32 3042 1431 2.13\n0 50 100\n0.04\n0.045\n0.05\n0.055\n0.06\nIteration\nC la\nss ifi\nca tio\nn te\nst in\ng er\nro r\n784\u2212100\u221210 784\u2212100\u2212100\u221210 784\u2212200\u221210 784\u22121000\u221210\n(a) Testing error.\n0 50 100 0\n0.005\n0.01\n0.015\n0.02\nIteration\nC la\nss ifi\nca tio\nn tr\nai ni\nng e\nrr or\n784\u2212100\u221210 784\u2212100\u2212100\u221210 784\u2212200\u221210 784\u22121000\u221210\n(b) Training error.\nFigure 4: MNIST DNN testing and training error on different network (100 iterations/epochs).\ncontains tiny natural images; Higgs Boson is a dataset from high energy physics. Alternative Splicing is RNA features used for predicting alternative gene splicing. We use DNN and DBN implementation from [23] and CNN implementation from [24]. All experiments were conducted on a laptop with Intel Core i5-3210M CPU 2.50GHz, 4GB RAM, Windows 7 64-bit OS."}, {"heading": "A. Results on MNIST", "text": "MNIST is a standard toy data set of handwritten digits containing 10 classes. It contains 60K training samples and 10K testing samples. The image size is 784 (grayscale 28\u00d7 28). Figure 3a shows some examples of MNIST dataset.\n1) Deep Neural Network: In experiment, we first test on some network architecture and find a better one for our further investigations. Figure 4a and Figure 4b show the\n0 50 100 0.03\n0.04\n0.05\n0.06\n0.07\n0.08\nIteration\nC la\nss ifi\nca tio\nn te\nst in\ng er\nro r\nDNN sDNN sDNNr\n(a) Testing error.\n0 50 100 0\n0.02\n0.04\n0.06\n0.08\n0.1\nIteration\nC la\nss ifi\nca tio\nn tr\nai ni\nng e\nrr or\nDNN sDNN sDNNr\n(b) Training error.\nFigure 5: MNIST testing and training error (100 iterations/epochs).\ntesting and training classification error for different network settings. Results show that \u201c784-1000-10\u201d is a better setting with lower testing error and converges faster in training. We will use network \u201c784-1000-10\u201d for DNN and DBN on MNIST. Learning rate is set to be 1; activation function is tangent function and output unit is sigmoid function.\nFigure 5 shows the testing error and training error of using standard DNN, sDNN (Shrinking DNN) and sDNNr (shrinking DNN with recall). Results show that sDNNr improves the accuracy of standard DNN. While for training error, both DNN and sDNNr give almost 0 training error.\nFigure 6 shows training time and number of active samples in each iteration (epoch). In our experiments, for sDNN and sDNNr, we set eliminate rate s = 20%. sDNNr has a recall process to use the the entire training samples, as shown in Figure 6. When the number of active samples is less than t = 20%\u00d7 60K of total training samples, we stop eliminating samples. The speedup using sDNNr compared to DNN is\nSpeedup = tDNN tsDNNr = 2.05. (10)\nRecall is a technique when the number of training samples is decreased to a threshold t, we start to use all training samples. There is a trade-off between speedup and classification error: setting a lower t could reduce computation time more, but could increase classification error. Figure 7 shows the effect of using different recall threshold t sDNNr on MNIST data. When we bring all training samples back at t = 20%\u00d7 60K , we get the best testing error. It is worth noting that the classification error of sDNNr is improved compared to standard DNN, which could imply that there is less overfitting for this data set.\nFigure 8 shows an example of exponential smoothing on the elimination threshold (Eq.(9)) during one epoch. The threshold using \u03b1 = 0.5 smooths the curve a lot.\n2) Deep Belief Network: Figure 9 shows the classification testing error and training time of using Deep Belief Network (DBN) and shrinking DBN with recall (sDBNr) on MNIST. Network setting is same as it is in DNN experiment. sDBNr further reduces the classification error of DBN to 0.0182 by using sDBNr.\n3) Convolution Neural Networks (CNN): The network architecture used in MNIST is 4 convolutional layers with each of the first 2 convolutional layers followed by a maxpooling layer, then 1 layer followed by a ReLU layer, 1 layer followed by a Softmax layer. The first 2 convolutional layers have 5 \u00d7 5 receptive field applied with a stride of 1 pixel. The 3rd convolutional layer has 4\u00d7 4 receptive field\nand the 4th layer has 1\u00d7 1 receptive field with a stride of 1 pixel. The max pooling layers pool 2\u00d7 2 regions at strides of 2 pixels. Figure 10 shows the classification testing error and training time of CNN on MNIST data.\nTable II summarizes the classification error improvement (IMP) and training time speedup of DNN, DBN and CNN on MNIST data, where improvement is IMP = (errDL \u2212 errsDLr)/errDL."}, {"heading": "B. Results on CIFAR-10", "text": "CIFAR-10 [25] data contains 60,000 32\u00d7 32 color image in 10 classes, with 6,000 images per class. There are 50,000 training and 10,000 testing images. CIFAR-10 is an object dataset, which includes airplane, car, bird, cat and so on and classes are completely mutually exclusive. In our experiment, we use CNN network to evaluate the performance in terms of classification error. Network architecture uses 5 convolutional layers: for the first three layers, each convolutional layer is followed by a max pooling layer; 4th convolutional layer is followed by a ReLU layer; the\n5th layer is followed by a softmax loss output layer. Table III shows the classification error and training time. Top-1 classification testing error in Table III means that the predict label is determined by considering the class with maximum probability only."}, {"heading": "C. Results on Higgs Boson", "text": "Higgs Boson is a subset of data from [26] with 50, 000 training and 20, 000 testing. Each sample is a signal process which either produces Higgs bosons particle or not. We use 7 high-level features derived by physicists to help discriminate particles between the two classes. Both activation function and output function were sigmoid function. The DNN batchsize is 100 and recall threshold t = 20%\u00d7 50, 000. We test on different network settings and choose the best. Table IV shows the experiment results using different network."}, {"heading": "D. Results on Alternative Splicing", "text": "Alternative Splicing [27] is a set of RNA sequences used in bioinfomatics. It contains 3446 cassette-type mouse exons with 1389 features per exon. We randomly select 2500 exons for training and use the rest for testing. For each exon, the dataset contains three real-valued positive prediction targets yi = [q\ninc qexc qnc], corresponding to probabilities that the exon is more likely to be included in the given tissue, more likely to be excluded, or more likely to exhibit no change relative to other tissues. To demonstrate the effective of proposed shrinking Deep Learning with recall approach, we use a simple DNN network of different number of layers and neurons with optimal tangent activation function and sigmoid output function. We use the following average sum squared error criteria to evaluate the model performance error = \u2211n i=1 \u2016yi \u2212 y (0) i \u2016\n2/n, where yi is the predict vector label and y(0)i is the ground-truth label vector, n is number of samples. The DNN batchsize is 100 and recall threshold t = 20% \u00d7 2500. We test on different network settings and choose the best. Table V shows the experiment result."}, {"heading": "VI. CONCLUSION", "text": "In conclusion, we proposed a shrinking Deep Learning with recall (sDLr) approach and the main contribution of sDLr is that it can reduce the running time significantly. Extensive experiments on 4 datasets show that shrinking Deep Learning with recall can reduce training time significantly while still gives competitive classification performance."}], "references": [{"title": "Reducing the dimensionality of data with neural networks", "author": ["G.E. Hinton", "R.R. Salakhutdinov"], "venue": "Science, vol. 313, no. 5786, pp. 504\u2013507, 2006.", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2006}, {"title": "Learning deep architectures for ai", "author": ["Y. Bengio"], "venue": "Foundations and trends R  \u00a9 in Machine Learning, vol. 2, no. 1, pp. 1\u2013127, 2009.", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2009}, {"title": "Imagenet classification with deep convolutional neural networks", "author": ["A. Krizhevsky", "I. Sutskever", "G.E. Hinton"], "venue": "Advances in neural information processing systems, 2012, pp. 1097\u20131105.", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2012}, {"title": "Stacked denoising autoencoders: Learning useful representations in a deep network with a local denoising criterion", "author": ["P. Vincent", "H. Larochelle", "I. Lajoie", "Y. Bengio", "P.-A. Manzagol"], "venue": "The Journal of Machine Learning Research, vol. 11, pp. 3371\u20133408, 2010.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2010}, {"title": "Deep boltzmann machines", "author": ["R. Salakhutdinov", "G.E. Hinton"], "venue": "International Conference on Artificial Intelligence and Statistics, 2009, pp. 448\u2013455.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2009}, {"title": "Training products of experts by minimizing contrastive divergence", "author": ["G.E. Hinton"], "venue": "Neural computation, vol. 14, no. 8, pp. 1771\u20131800, 2002.", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2002}, {"title": "Three classes of deep learning architectures and their applications: a tutorial survey", "author": ["L. Deng"], "venue": "APSIPA transactions on signal and information processing, 2012.", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2012}, {"title": "Gradientbased learning applied to document recognition", "author": ["Y. LeCun", "L. Bottou", "Y. Bengio", "P. Haffner"], "venue": "Proceedings of the IEEE, vol. 86, no. 11, pp. 2278\u20132324, 1998.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 1998}, {"title": "Convolutional networks for images, speech, and time series", "author": ["Y. LeCun", "Y. Bengio"], "venue": "The handbook of brain theory and neural networks, vol. 3361, no. 10, 1995.", "citeRegEx": "9", "shortCiteRegEx": null, "year": 1995}, {"title": "Making large scale svm learning practical", "author": ["T. Joachims"], "venue": "Universit\u00e4t Dortmund, Tech. Rep., 1999.", "citeRegEx": "10", "shortCiteRegEx": null, "year": 1999}, {"title": "Fast support vector machines using parallel adaptive shrinking on distributed systems", "author": ["J. Narasimhan", "A. Vishnu", "L. Holder", "A. Hoisie"], "venue": "arXiv preprint arXiv:1406.5161, 2014.", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2014}, {"title": "Lasso screening rules via dual polytope projection", "author": ["J. Wang", "J. Zhou", "P. Wonka", "J. Ye"], "venue": "Advances in Neural Information Processing Systems, 2013, pp. 1070\u20131078.", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2013}, {"title": "A dynamic screening principle for the lasso", "author": ["A. Bonnefoy", "V. Emiya", "L. Ralaivola", "R. Gribonval"], "venue": "Signal Processing Conference (EUSIPCO), 2014 Proceedings of the 22nd European. IEEE, 2014, pp. 6\u201310.", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2014}, {"title": "Big data: The next frontier for innovation, competition, and productivity", "author": ["J. Manyika", "M. Chui", "B. Brown", "J. Bughin", "R. Dobbs", "C. Roxburgh", "A.H. Byers"], "venue": "2011.", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2011}, {"title": "A closed form solution to multi-view low-rank regression.", "author": ["S. Zheng", "X. Cai", "C.H. Ding", "F. Nie", "H. Huang"], "venue": "in AAAI,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2015}, {"title": "Kernel alignment inspired linear discriminant analysis", "author": ["S. Zheng", "C. Ding"], "venue": "Joint European Conference on Machine Learning and Knowledge Discovery in Databases. Springer Berlin Heidelberg, 2014, pp. 401\u2013416.", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2014}, {"title": "Tidewatch: Fingerprinting the cyclicality of big data workloads", "author": ["D. Williams", "S. Zheng", "X. Zhang", "H. Jamjoom"], "venue": "IEEE INFOCOM 2014-IEEE Conference on Computer Communications. IEEE, 2014, pp. 2031\u20132039.", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2014}, {"title": "Virtual machine migration in an over-committed cloud", "author": ["X. Zhang", "Z.-Y. Shae", "S. Zheng", "H. Jamjoom"], "venue": "2012 IEEE Network Operations and Management Symposium. IEEE, 2012, pp. 196\u2013203.", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2012}, {"title": "Analysis and modeling of social influence in high performance computing workloads", "author": ["S. Zheng", "Z.-Y. Shae", "X. Zhang", "H. Jamjoom", "L. Fong"], "venue": "European Conference on Parallel Processing. Springer Berlin Heidelberg, 2011, pp. 193\u2013204.", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2011}, {"title": "Least squares support vector machine classifiers", "author": ["J.A. Suykens", "J. Vandewalle"], "venue": "Neural processing letters, vol. 9, no. 3, pp. 293\u2013300, 1999.", "citeRegEx": "20", "shortCiteRegEx": null, "year": 1999}, {"title": "Regression shrinkage and selection via the lasso", "author": ["R. Tibshirani"], "venue": "Journal of the Royal Statistical Society. Series B (Methodological), pp. 267\u2013288, 1996.", "citeRegEx": "21", "shortCiteRegEx": null, "year": 1996}, {"title": "Exponential smoothing: The state of the art", "author": ["E.S. Gardner"], "venue": "Journal of forecasting, vol. 4, no. 1, pp. 1\u201328, 1985.", "citeRegEx": "22", "shortCiteRegEx": null, "year": 1985}, {"title": "Prediction as a candidate for learning deep hierarchical models of data", "author": ["R.B. Palm"], "venue": "Technical University of Denmark, 2012.", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2012}, {"title": "Matconvnet-convolutional neural networks for matlab", "author": ["A. Vedaldi", "K. Lenc"], "venue": "arXiv preprint arXiv:1412.4564, 2014.", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2014}, {"title": "Learning multiple layers of features from tiny images", "author": ["A. Krizhevsky", "G. Hinton"], "venue": "2009.", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2009}, {"title": "Searching for exotic particles in high-energy physics with deep learning", "author": ["P. Baldi", "P. Sadowski", "D. Whiteson"], "venue": "Nature communications, vol. 5, 2014.", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2014}, {"title": "Bayesian prediction of tissue-regulated splicing using rna sequence and cellular context", "author": ["H.Y. Xiong", "Y. Barash", "B.J. Frey"], "venue": "Bioinformatics, vol. 27, no. 18, pp. 2554\u20132562, 2011.", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2011}], "referenceMentions": [{"referenceID": 0, "context": "Deep Learning [1] has become a powerful machine learning model.", "startOffset": 14, "endOffset": 17}, {"referenceID": 1, "context": "Deep architectures using multiple layers outperform shadow models [2].", "startOffset": 66, "endOffset": 69}, {"referenceID": 2, "context": "Secondly, there is no need to extract human design features [3], which can reduce the dependence of the quality of human extracted features.", "startOffset": 60, "endOffset": 63}, {"referenceID": 0, "context": "Several unsupervised pretraining methods for neural network have been proposed to improve the performance of random initialized DNN, such as using stacks of RBMs (Restricted Boltzmann Machines) [1], autoencoders [4], or DBM (Deep Boltzmann Machines) [5].", "startOffset": 194, "endOffset": 197}, {"referenceID": 3, "context": "Several unsupervised pretraining methods for neural network have been proposed to improve the performance of random initialized DNN, such as using stacks of RBMs (Restricted Boltzmann Machines) [1], autoencoders [4], or DBM (Deep Boltzmann Machines) [5].", "startOffset": 212, "endOffset": 215}, {"referenceID": 4, "context": "Several unsupervised pretraining methods for neural network have been proposed to improve the performance of random initialized DNN, such as using stacks of RBMs (Restricted Boltzmann Machines) [1], autoencoders [4], or DBM (Deep Boltzmann Machines) [5].", "startOffset": 250, "endOffset": 253}, {"referenceID": 5, "context": "Deep Belief Network (DBN) is a generative unsupervised pretraining network which uses stacked RBMs [6] during pretraining.", "startOffset": 99, "endOffset": 102}, {"referenceID": 6, "context": "DBN has undirected connections between its first two layers and directed connections between all its lower layers[7] [5].", "startOffset": 113, "endOffset": 116}, {"referenceID": 4, "context": "DBN has undirected connections between its first two layers and directed connections between all its lower layers[7] [5].", "startOffset": 117, "endOffset": 120}, {"referenceID": 7, "context": "Convolution Neural Network (CNN) [8] [3] [9] has been proposed to deal with images, speech and time-series.", "startOffset": 33, "endOffset": 36}, {"referenceID": 2, "context": "Convolution Neural Network (CNN) [8] [3] [9] has been proposed to deal with images, speech and time-series.", "startOffset": 37, "endOffset": 40}, {"referenceID": 8, "context": "Convolution Neural Network (CNN) [8] [3] [9] has been proposed to deal with images, speech and time-series.", "startOffset": 41, "endOffset": 44}, {"referenceID": 8, "context": "CNN forces the extraction of local features by restricting the receptive fields of hidden neurons to be local [9].", "startOffset": 110, "endOffset": 113}, {"referenceID": 9, "context": "Inspired from the shrinking technique [10] [11] used in accelerating computation of Support Vector Machines (SVM) algorithm", "startOffset": 38, "endOffset": 42}, {"referenceID": 10, "context": "Inspired from the shrinking technique [10] [11] used in accelerating computation of Support Vector Machines (SVM) algorithm", "startOffset": 43, "endOffset": 47}, {"referenceID": 11, "context": "and screening [12] [13] technique used in LASSO, we propose an accelerating algorithm shrinking Deep Learning with Recall (sDLr).", "startOffset": 14, "endOffset": 18}, {"referenceID": 12, "context": "and screening [12] [13] technique used in LASSO, we propose an accelerating algorithm shrinking Deep Learning with Recall (sDLr).", "startOffset": 19, "endOffset": 23}, {"referenceID": 13, "context": "Analyzing large data sets, so-called big data, will become a key basis of competition, underpinning new waves of productivity growth, innovation, and consumer interest [14].", "startOffset": 168, "endOffset": 172}, {"referenceID": 14, "context": "A lot of big data technologies, including cloud computing, dimensionality reduction have been proposed [15], [16], [17], [18], [19].", "startOffset": 103, "endOffset": 107}, {"referenceID": 15, "context": "A lot of big data technologies, including cloud computing, dimensionality reduction have been proposed [15], [16], [17], [18], [19].", "startOffset": 109, "endOffset": 113}, {"referenceID": 16, "context": "A lot of big data technologies, including cloud computing, dimensionality reduction have been proposed [15], [16], [17], [18], [19].", "startOffset": 115, "endOffset": 119}, {"referenceID": 17, "context": "A lot of big data technologies, including cloud computing, dimensionality reduction have been proposed [15], [16], [17], [18], [19].", "startOffset": 121, "endOffset": 125}, {"referenceID": 18, "context": "A lot of big data technologies, including cloud computing, dimensionality reduction have been proposed [15], [16], [17], [18], [19].", "startOffset": 127, "endOffset": 131}, {"referenceID": 19, "context": "SVM [20] solves the following optimization problem:", "startOffset": 4, "endOffset": 8}, {"referenceID": 9, "context": "Since SVM learning problem has much less support vectors than training examples, shrinking [10] [11] was proposed to eliminate training samples for large learning tasks where the fraction of support vectors is small compared to the training sample size or when many support vectors are at the upper bound of Lagrange multipliers.", "startOffset": 91, "endOffset": 95}, {"referenceID": 10, "context": "Since SVM learning problem has much less support vectors than training examples, shrinking [10] [11] was proposed to eliminate training samples for large learning tasks where the fraction of support vectors is small compared to the training sample size or when many support vectors are at the upper bound of Lagrange multipliers.", "startOffset": 96, "endOffset": 100}, {"referenceID": 20, "context": "LASSO [21] is an optimization problem to find sparse representation of some signals with respect to a predefined dictionary.", "startOffset": 6, "endOffset": 10}, {"referenceID": 11, "context": "Screening [12] [13] is a technique used to reduce the size of dictionary using some rules in order to accelerate the computation of LASSO.", "startOffset": 10, "endOffset": 14}, {"referenceID": 12, "context": "Screening [12] [13] is a technique used to reduce the size of dictionary using some rules in order to accelerate the computation of LASSO.", "startOffset": 15, "endOffset": 19}, {"referenceID": 21, "context": "We use exponential smoothing [22] to adjust the threshold used in batch i+1: instead of using ti+1 as the threshold to eliminate samples, we use the following t\u2032i+1: t\u2032i+1 = \u03b1t \u2032 i + (1\u2212 \u03b1)ti+1, (9)", "startOffset": 29, "endOffset": 33}, {"referenceID": 22, "context": "We use DNN and DBN implementation from [23] and CNN implementation from [24].", "startOffset": 39, "endOffset": 43}, {"referenceID": 23, "context": "We use DNN and DBN implementation from [23] and CNN implementation from [24].", "startOffset": 72, "endOffset": 76}, {"referenceID": 24, "context": "CIFAR-10 [25] data contains 60,000 32\u00d7 32 color image in 10 classes, with 6,000 images per class.", "startOffset": 9, "endOffset": 13}, {"referenceID": 25, "context": "Higgs Boson is a subset of data from [26] with 50, 000 training and 20, 000 testing.", "startOffset": 37, "endOffset": 41}, {"referenceID": 26, "context": "Alternative Splicing [27] is a set of RNA sequences used in bioinfomatics.", "startOffset": 21, "endOffset": 25}], "year": 2016, "abstractText": "Deep Learning is a very powerful machine learning model. Deep Learning trains a large number of parameters for multiple layers and is very slow when data is in large scale and the architecture size is large. Inspired from the shrinking technique used in accelerating computation of Support Vector Machines (SVM) algorithm and screening technique used in LASSO, we propose a shrinking Deep Learning with recall (sDLr) approach to speed up deep learning computation. We experiment shrinking Deep Learning with recall (sDLr) using Deep Neural Network (DNN), Deep Belief Network (DBN) and Convolution Neural Network (CNN) on 4 data sets. Results show that the speedup using shrinking Deep Learning with recall (sDLr) can reach more than 2.0 while still giving competitive classification performance. Keywords-Deep Learning; Deep Neural Network (DNN); Deep Belief Network (DBN); Convolution Neural Network (CNN)", "creator": "LaTeX with hyperref package"}}}