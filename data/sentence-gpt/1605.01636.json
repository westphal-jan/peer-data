{"id": "1605.01636", "review": {"conference": "NIPS", "VERSION": "v1", "DATE_OF_SUBMISSION": "5-May-2016", "title": "Maximal Sparsity with Deep Networks?", "abstract": "The iterations of many sparse estimation algorithms are comprised of a fixed linear filter cascaded with a thresholding nonlinearity, which collectively resemble a typical neural network layer. Consequently, a lengthy sequence of algorithm iterations can be viewed as a deep network with shared, hand-crafted layer weights. It is therefore quite natural to examine the degree to which a learned network model might act as a viable surrogate for traditional sparse estimation in domains where ample training data is available. While the possibility of a reduced computational budget is readily apparent when a ceiling is imposed on the number of layers, our work primarily focuses on estimation accuracy. In particular, it is well-known that when a signal dictionary has coherent columns, as quantified by a large RIP constant, then most tractable iterative algorithms are unable to find maximally sparse representations. In contrast, we demonstrate both theoretically and empirically the potential for a trained deep network to recover minimal $\\ell_0$-norm representations in regimes where existing methods fail. The resulting system is deployed on a practical photometric stereo estimation problem, where the goal is to remove sparse outliers that can disrupt the estimation of surface normals from a 3D scene. For example, the model used for large-scale image-processing is implemented to achieve the approximate density of a light bulb, by a single technique used to obtain a high resolution image at a very low cost, by applying a simple method called the smoothed gradient descent algorithm. The method of smoothed gradient descent is called a gradient gradient gradient gradient gradient gradient gradient gradient gradient gradient gradient gradient gradient gradient gradient gradient gradient gradient gradient gradient gradient gradient gradient gradient gradient gradient gradient gradient gradient gradient gradient gradient gradient gradient gradient gradient gradient gradient gradient gradient gradient gradient gradient gradient gradient gradient gradient gradient gradient gradient gradient gradient gradient gradient gradient gradient gradient gradient gradient gradient gradient gradient gradient gradient gradient gradient gradient gradient gradient gradient gradient gradient gradient gradient gradient gradient gradient gradient gradient gradient gradient gradient gradient gradient gradient gradient gradient gradient gradient gradient gradient gradient gradient gradient gradient gradient gradient gradient gradient gradient gradient gradient gradient gradient gradient gradient gradient gradient gradient gradient gradient gradient gradient gradient gradient gradient gradient gradient gradient gradient gradient gradient gradient gradient gradient gradient gradient gradient gradient gradient gradient gradient gradient gradient gradient gradient gradient gradient gradient gradient gradient gradient gradient gradient gradient gradient gradient gradient gradient gradient gradient gradient gradient gradient gradient gradient gradient gradient gradient gradient gradient gradient gradient gradient gradient gradient gradient gradient gradient gradient gradient gradient gradient gradient gradient gradient gradient gradient gradient gradient gradient gradient gradient gradient gradient gradient gradient gradient gradient gradient gradient gradient gradient gradient gradient gradient gradient gradient gradient gradient gradient gradient gradient gradient gradient gradient gradient gradient gradient gradient gradient", "histories": [["v1", "Thu, 5 May 2016 15:58:55 GMT  (1206kb,D)", "http://arxiv.org/abs/1605.01636v1", null], ["v2", "Tue, 10 May 2016 05:38:46 GMT  (1296kb,D)", "http://arxiv.org/abs/1605.01636v2", null]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["bo xin", "yizhou wang 0001", "wen gao 0001", "david p wipf", "baoyuan wang"], "accepted": true, "id": "1605.01636"}, "pdf": {"name": "1605.01636.pdf", "metadata": {"source": "CRF", "title": "Maximal Sparsity with Deep Networks?", "authors": ["Bo Xin", "Yizhou Wang", "Wen Gao", "David Wipf"], "emails": ["boxin@pku.edu.cn", "yizhou.wang@pku.edu.cn", "wgao@pku.edu.cn", "davidwipf@gmail.com"], "sections": [{"heading": null, "text": "Keywords: Sparse estimation, compressive sensing, deep unfolding, deep networks, restricted isometry property (RIP)"}, {"heading": "1. Introduction", "text": "Our launching point is the optimization problem\nmin x \u2016x\u20160 s.t. y = \u03a6x, (1)\nwhere y \u2208 Rn is an observed vector, \u03a6 \u2208 Rn\u00d7m is some known, overcomplete dictionary of feature/basis vectors with m > n, and \u2016 \u00b7 \u20160 denotes the `0 norm of a vector, or a count of the number of nonzero elements. Consequently, (1) can be viewed as the search for a maximally sparse vector x\u2217 such that y can be represented using the fewest number\nar X\niv :1\n60 5.\n01 63\n6v 1\n[ cs\n.L G\n] 5\nof features in the feasible region.1 Unfortunately however, direct assault on (1) involves an intractable, combinatorial optimization process, and therefore efficient alternatives that return a maximally sparse x\u2217 with high probability in restricted regimes are sought. Popular examples with varying degrees of computational overhead include convex relaxations such as `1-norm minimization (Donoho and Elad, 2003; Tibshirani, 1996), greedy approaches like orthogonal matching pursuit (OMP) (Pati et al., 1993; Tropp, 2004), and many flavors of iterative thresholding (Beck and Teboulle, 2009; Blumensath and Davies, 2008).\nVariants of these algorithms find practical relevance in numerous disparate application domains, including feature selection (Cotter and Rao, 2002; Figueiredo, 2002), outlier removal (Cande\u0300s and Tao, 2005; Ikehata et al., 2012), compressive sensing (Donoho, 2006), and source localization (Baillet et al., 2001; Malioutov et al., 2005) among many others. However, a fundamental weakness underlies them all: If the Gram matrix \u03a6>\u03a6 has significant off-diagonal energy, indicative of strong coherence between columns of \u03a6, then estimation of x\u2217 may be extremely poor. Indeed both the cardinality of the solution, and often more importantly, the locations of nonzero elements, can be completely suboptimal. Loosely speaking this occurs because, as higher correlation levels are present, the null-space of \u03a6 is more likely to include large numbers of approximately sparse vectors that tend to distract existing algorithms in the feasible region. The degree to which this risk is present can be quantified by a so-called restricted isometry constant to be described in detail later. Compounding the problem is that, in all but the most ideal settings where we are free to choose \u03a6 randomly from certain favorable distributions, there is no way of knowing in advance the true degree in which this correlation structure will be disruptive (e.g., restricted isometry constants are actually not feasible to compute in practice).\nIn this paper we consider recent developments in the field of deep learning as an entry point for improving the performance of sparse recovery algorithms. Although seemingly unrelated at first glance, the layers of a deep neural network (DNN) can be viewed as iterations of some algorithm that have been unfolded into a network structure (Gregor and LeCun, 2010; Hershey et al., 2014). In particular, iterative thresholding approaches such as those mentioned above typically involve an update rule comprised of a fixed, linear filter followed by a non-linear activation function that promotes sparsity. Consequently, algorithm execution can be interpreted as passing an input through an extremely deep network with constant layer weights (dependent on \u03a6) at every layer.\nThis \u2018unfolding\u2019 viewpoint immediately suggests that we consider substituting discriminatively learned weights in place of those inspired by the original sparse recovery algorithm. For example, it has been argued that, given access to a sufficient number of {x\u2217,y} pairs, a trained network may be capable of producing quality sparse estimates with a modest number of layers. This in turn can lead to a dramatically reduced computational burden relative to purely optimization-based approaches, which can require hundreds or even thousands of iterations to sufficiently converge (Gregor and LeCun, 2010; Sprechmann et al., 2015).\nExisting work on sparse estimation through deep network training borrows basic network components directly from the underlying iterative algorithm. Different networks are primarily differentiated by the types of activation functions employed, which performed as sparsity-promoting non-linearities during their former life in service to iterative optimiza-\n1. In practice, it is common to relax the feasible region to \u2016y\u2212\u03a6x\u20162 \u2264 , or replace the constraint altogether with a sensible data fit term balanced with a trade-off parameter.\ntion. For example, (Gregor and LeCun, 2010) promotes a soft-threshold function inspired by and iterative shrinkage-thresholding algorithm (ISTA) for minimizing the `1-norm, a well-known convex approximation to the canonical `0 norm sparsity penalty from (1). In contrast, (Sprechmann et al., 2015) advocates a wider class of functions derived from proximal operators (Parikh and Boyd, 2014). Finally, it has also been suggested that replacing typically continuous activation functions with hard-threshold operators may lead to sparser representations (Wang et al., 2015). At a high level though, one common ingredient of all of these approaches is the adoption of shared weights across layers.\nWhile existing empirical results are promising, especially in terms of the reduction in computational footprint, there is as of yet no empirical demonstration of a learned deep network that can unequivocally recover maximally sparse vectors x\u2217 with greater accuracy than conventional, state-of-the-art optimization-based algorithms. Nor is there supporting theoretical evidence elucidating the exact mechanism whereby learning may be expected to improve the estimation accuracy, especially in the presence of coherent dictionaries \u03a6. Additionally, minimal insights exist that might be transferrable to assessing the behavior of broader learning objectives and systems."}, {"heading": "1.1 Paper Overview", "text": "This paper attempts to fill in these gaps described above, at least to the extent possible, via the following organizational structure. In Section 2 we begin by reviewing the iterative hardthresholding (IHT) algorithm for estimating a sparse vector. IHT was chosen because it can be directly unfolded for learning purposes, is representative of many sparse estimation paradigms, and is amenable to theoretical analysis. Next we discuss the limitations of IHT, including its high sensitivity to correlated designs, and motivate a DNN-like, unfolded alternative. Later, Section 3 considers this unfolded IHT network with shared layer-wise weights and activations, which is the standard template for existing methods. We explicitly quantify the degree to which such networks can compensate for correlations in \u03a6, but also expose the breaking point whereby any possible shared-weight construction is likely to fail.\nThis naturally segues to richer deep networks with layer-wise independent weights and activations (meaning different layers need not share the same, fixed weights and activations), which we scrutinize in Section 4. Here we describe a multi-resolution dictionary, with highly correlated clusters of columns, that explicitly requires the richer class of layer parameterizations to guarantee successful sparse recovery. Section 5 then further elucidates the essential multi-resolution nature of the sparse estimation problem, and how we may deviate from strict adherence to any particular unfolded algorithmic script in designing a practical DNN. In particular, we motivate a multi-label classification network to focus on learning correct support patterns. In Section 6 we describe what we believe to be an essential ingredient for constructing an effective training set. Corroborating simulation results and a real-world computer vision application example are presented in Sections 7 and 8 respectively, followed by exploration of alternative recurrent long-short-term-memory (LSTM) structures in Section 9. Final discussions are in Section 10, while all proofs are deferred to the Appendix."}, {"heading": "1.2 Summary of Contributions", "text": "Our technical and empirical contributions can be distilled to the following points:\n\u2022 We rigorously dissect the benefits of unfolding conventional sparse estimation algorithms to produce trainable deep networks. This includes a precise characterization of exactly how different architecture choices can affect the ability to improve effective restrictive isometry constants, which measure of the degree of disruptive correlation present in a dictionary. This helps to quantify the limits of shared layer weights and motivates more flexible network constructions that account for multi-resolution structure in \u03a6 in a previously unexplored fashion. Importantly, we envision that our analyses are emblematic of important factors present in other DNN-related domains.\n\u2022 Based on these theoretical insights, and a better understanding of the essential factors governing performance, we establish the degree to which it is favorable to diverge from strict conformity to any particular unfolded algorithmic script. In particular, we argue that the equivalent of layer-wise independent weights and/or activations are essential, while retainment of original hard-thresholding non-linearities and squared-error loss implicit to IHT and related algorithms is not. We also recast the the core problem as deep multi-label classification given that optimal support pattern is our primary concern. This allows us to adopt a novel training paradigm that is less sensitive to the specific distribution encountered during testing. Ultimately, we development the first, ultra-fast sparse estimation algorithm that can effectively deal with coherent dictionaries and adversarial restricted isometry constants.\n\u2022 We apply the proposed system to a practical photometric stereo computer vision problem, where the goal is to estimate the 3D geometry of an object using only 2D photos taken from a single camera under different lighting conditions. In this context, shadows and specularities represent sparse outliers that must be simultaneously removed from \u223c 104 \u2212 106 surface points. We achieve state-of-the-art performance despite a minuscule computational budget appropriate for real-time mobile environments.\n\u2022 Finally, we explore the connection between unfolded sparse estimation algorithms and unfolded recurrent LSTM networks, revealing that the gating functions intrinsic to the latter can improve performance in the former by allowing coarse-resolution sparsity patterns to prorogate to deeper layers."}, {"heading": "2. From Iterative Hard Thesholding (IHT) to Deep Neural Networks", "text": "This section first introduces IHT before detailing its unfolded DNN analogue."}, {"heading": "2.1 Introduction to IHT", "text": "With knowledge of an upper bound on the true cardinality, solving (1) can be replaced by the equivalent problem\nmin x\n1 2\u2016y \u2212\u03a6x\u2016 2 2 s.t. \u2016x\u20160 \u2264 k. (2)\nIterative hard-thresholding (IHT) attempts to minimize (2) using what can be viewed as computationally-efficient projected gradient iterations (Blumensath and Davies, 2009). Let\nx(t) denote the estimate of some maximally sparse x\u2217 after t iterations. IHT first computes the gradient of the quadratic objective evaluated at x(t) given by\n\u2207x|x=x(t) = \u03a6 >\u03a6x(t) \u2212\u03a6>y. (3)\nWe then take the unconstrained gradient step\nx(t+1) = x(t) \u2212 \u00b5 \u2207x|x=x(t) , (4)\nwhere \u00b5 is a step-size parameter. Finally, we project onto the constraint set by zeroing out all but the k largest values (in magnitude) of x(t+1) using a hard-thresholding operator denoted Hk[\u00b7]. The combined iteration becomes\nx(t+1) = Hk [ x(t) \u2212 \u00b5\u03a6> ( y \u2212\u03a6x(t) )] , (5)\nwhich only requires matrix-vector multiples and is computationally cheap to implement. For the vanilla version of IHT, the step-size \u00b5 = 1 leads to a number of recovery guarantees whereby iterating (5), starting from x(0) = 0 is guaranteed to reduce (2) at each step before eventually converging to the globally optimal solution.2 These results hinge on properties of \u03a6 which relate to the coherence structure of dictionary columns as encapsulated by the following definition.\nDefinition 1 (Restricted Isometry Property) A dictionary \u03a6 satisfies the Restricted Isometry Property (RIP) with constant \u03b4k[\u03a6] < 1 if\n(1\u2212 \u03b4k[\u03a6])\u2016x\u201622 \u2264 \u2016\u03a6x\u201622 \u2264 (1 + \u03b4k[\u03a6])\u2016x\u201622 (6)\nholds for all {x : \u2016x\u20160 \u2264 k}.\nIn brief, the smaller the value of the restricted isometry constant \u03b4k[\u03a6], the closer any submatrix of \u03a6 with k columns is to being orthogonal (i.e., it has less correlation structure).\nIt is now well-established that dictionaries with smaller values of \u03b4k[\u03a6] lead to sparse recovery problems that are inherently easier to solve. In the context of IHT, it has been shown (Blumensath and Davies, 2009) that if y = \u03a6x\u2217, with \u2016x\u2217\u20160 \u2264 k and \u03b43k[\u03a6] < 1/ \u221a 32, then at iteration t of (5)\n\u2016x(t) \u2212 x\u2217\u20162 \u2264 2\u2212t\u2016x\u2217\u20162. (7)\nIt follows that as t \u2192 \u221e, x(t) \u2192 x\u2217, meaning that we recovery the true, generating x\u2217. Moreover, it can be shown that this x\u2217 is also the unique, optimal solution to (1) (Cande\u0300s et al., 2006).\n2. Other values of \u00b5 or even a positive definite matrix, adaptively chosen, can lead to a faster convergence rate (Blumensath and Davies, 2010)."}, {"heading": "2.2 Unfolding IHT Iterations", "text": "The success of IHT in recovering maximally sparse solutions crucially depends on the RIPbased condition that \u03b43k[\u03a6] < 1/ \u221a 32, which heavily constrains the degree of correlation structure in \u03a6 that can be tolerated. While dictionaries with columns drawn independently and uniformly from the surface of a unit hypersphere3 will satisfy this condition with high probability provided k is small enough (Cande\u0300s and Tao, 2005), for many/most practical problems of interest we cannot rely on this type of IHT recovery guarantee. In fact, except for randomized dictionaries in high dimensions where tight bounds exist, we cannot even compute the value of \u03b43k[\u03a6], which requires calculating the spectral norm of ( m 3k ) subsets of dictionary columns. There are many ways nature might structure a dictionary such that IHT (or most any other existing sparse estimation algorithm) will fail. Here we consider one of the most straightforward forms of dictionary coherence that can easily disrupt performance. Consider the situation where \u03a6 = [ A+ uv> ] N , where columns ofA \u2208 Rn\u00d7m and u \u2208 Rn are drawn iid from the surface of a unit hypersphere, while v \u2208 Rm is arbitrary. Additionally, > 0 is a scalar and N is a diagonal normalization matrix that scales each column of \u03a6 to have unit `2 norm. It then follows that if is sufficiently small, the rank-one component begins to dominate, and there is no value of 3k such that \u03b43k[\u03a6] < 1/ \u221a 32.\nIt is here we hypothesize that DNNs provide a potential avenue for improvement to the extent that they might be able to compensate for disruptive correlation structure in \u03a6. To see this, note that from a qualitative standpoint it is quite clear that the iterations of sparsity-promoting algorithms like IHT resemble the layers of neural networks (Gregor and LeCun, 2010). Therefore we can view a long sequence of such iterations as a DNN with fixed, parameterized weights at every layer. However, what if we are able to learn alternative weights that somehow overcome the limitations of a poor RIP constant?\nFor example, at the most basic level we might consider general networks with the layer t+ 1 defined by x(t+1) = f [ \u03a8x(t) + \u0393y ] , (8)\nwhere f : Rm \u2192 Rm is a non-linear activation function, and \u03a8 \u2208 Rm\u00d7m and \u0393 \u2208 Rm\u00d7n are arbitrary. Moreover, given access to training pairs {x\u2217,y}, where x\u2217 is a sparse vector such that y = \u03a6x\u2217, we can optimize \u03a8 and \u0393 using traditional stochastic gradient descent just like any other DNN structure. In the next section we will precisely characterize the extent to which this modification affords any benefit over IHT using f(\u00b7) = Hk[\u00b7]. Later in Section 4 we will consider adaptive non-linearities f (t) and layer-specific parameters {\u03a8(t),\u0393(t)}."}, {"heading": "3. Analysis using Shared Layer-Wise Weights and Activations", "text": "For simplicity in this section we restrict ourselves to the fixed hard-threshold operator Hk[\u00b7] across all layers; however, many of the conclusions borne out of our analysis nonetheless carry over to a much wider range of activation functions f . In general it is difficult to analyze how arbitrary \u03a8 and \u0393 may improve upon the fixed parameterization from (5) where\n3. If elements of \u03a6 are drawn iid from N (0, 1/ \u221a n) and rescaled to have unit `2 norm, then the resulting\ncolumns will be iid distributed uniformly on the unit sphere. Moreover, as n\u2192\u221e, each `2 column norm converges to one such that normalization is not even necessary.\n\u03a8 = I \u2212 \u03a6>\u03a6 and \u0393 = \u03a6> (assuming \u00b5 = 1). Fortunately though, we can significantly collapse the space of potential weight matrices by including the natural requirement that if x\u2217 represents the true, maximally sparse solution, then it must be a fixed-point of (8). Indeed, without this stipulation the iterations could diverge away from the globally optimal value of x, something IHT itself will never do. These considerations lead to the following:\nProposition 2 Consider a generalized IHT-based network layer given by\nx(t+1) = Hk [ \u03a8x(t) + \u0393y ] (9)\nand let x\u2217 denote any unique, maximally sparse feasible solution to y = \u03a6x with \u2016x\u20160 \u2264 k. Then to ensure that any such x\u2217 is a fixed point of (9) it must be that \u03a8 = I \u2212 \u0393\u03a6.\nAlthough \u0393 remains unconstrained, this result has restricted \u03a8 to be a rank-n factor, parameterized by \u0393, subtracted from an identity matrix. Certainly this represents a significant contraction of the space of \u2018reasonable\u2019 parameterizations for a general IHT layer. In light of Proposition 2, we may then further consider whether the added generality of \u0393 (as opposed to the original fixed assignment \u0393 = \u03a6>) affords any further benefit to the revised IHT update\nx(t+1) = Hk [ (I \u2212 \u0393\u03a6)x(t) + \u0393y ] . (10)\nFor this purpose we note that (10) can be interpreted as a projected gradient descent step for solving\nmin x\n1 2x >\u0393\u03a6x\u2212 x>\u0393y s.t. \u2016x\u20160 \u2264 k. (11)\nHowever, if \u0393\u03a6 is not positive semi-definite, then this objective is no longer even convex, and combined with the non-convex constraint is likely to produce an even wider constellation of troublesome local minima with no clear affiliation with the global optimum of our original problem from (2). Consequently it does not immediately appear that \u0393 6= \u03a6> is likely to provide any tangible benefit. However, there do exist important exceptions.\nThe first indication of how learning a general \u0393 might help comes from the following result:\nProposition 3 Suppose that \u0393 = D\u03a6>WW>, where W is an arbitrary matrix of appropriate dimension and D is a full-rank diagonal that jointly solve\n\u03b4\u22173k [\u03a6] , inf W ,D \u03b43k [W\u03a6D] . (12)\nMoreover, assume that \u03a6 is substituted with \u03a6D in (10), meaning we have simply replaced \u03a6 with a new dictionary that has scaled columns. Given these qualifications, if y = \u03a6x\u2217, with \u2016x\u2217\u20160 \u2264 k and \u03b4\u22173k [\u03a6] < 1/ \u221a 32, then at iteration t of (10)\n\u2016D\u22121x(t) \u2212D\u22121x\u2217\u20162 \u2264 2\u2212t\u2016D\u22121x\u2217\u20162. (13)\nAs before, it follows that as t \u2192 \u221e, x(t) \u2192 x\u2217, meaning that we recovery the true, generating x\u2217. Additionally, it can be guaranteed that after a finite number of iterations, the correct support pattern will be discovered. And it should be emphasized that rescaling \u03a6\nby some known diagonal D is a common prescription for sparse estimation (e.g., column normalization) that does not alter the optimal `0-norm support pattern. 4\nBut the real advantage over regular IHT comes from the fact that \u03b4\u22173k [\u03a6] \u2264 \u03b4k [\u03a6], and in many practical cases, \u03b4\u22173k [\u03a6] \u03b43k [\u03a6], which implies success can be guaranteed across a much wider range of RIP conditions. For example, if we revisit the dictionary \u03a6 =[ A+ uv> ] N , an immediate benefit can be observed. More concretely, for sufficiently small we argued that \u03b43k [\u03a6] > 1/ \u221a\n32 for all k, and consequently convergence to the optimal solution may fail. In contrast, it can be shown that \u03b4\u22173k [\u03a6] will remain quite small, satisfying \u03b4\u22173k [\u03a6] \u2248 \u03b43k [A], implying that performance will nearly match that of an equivalent recovery problem using A (and as we discussed above, \u03b43k [A] is likely to be relatively small per its unique, randomized design). The following result generalizes a sufficient regime whereby this is possible: Corollary 4 Suppose \u03a6 = [ A+ \u2206r]N , where elements of A are drawn iid from N (0, 1/ \u221a n), \u2206r is any arbitrary matrix with rank[\u2206r] = r < n, and N is a diagonal matrix that enforces unit `2 column norms. Then\nE (\u03b4\u22173k [\u03a6]) \u2264 E ( \u03b43k [ A\u0303 ]) , (14)\nwhere A\u0303 denotes the matrix A with any r rows removed.\nAdditionally, as the size of \u03a6 grows proportionally larger, it can be shown that with overwhelming probability \u03b4\u22173k [\u03a6] \u2264 \u03b43k [ A\u0303 ] . Overall, these results suggest that we can essentially annihilate any potentially disruptive rank-r component \u2206r at the cost of implicitly losing r measurements (linearly independent rows of A, and implicitly the corresponding elements of y). Therefore, at least provided that r is sufficiently small such that\n\u03b43k [ A\u0303 ] \u2248 \u03b43k [A], we can indeed be confident that a modified form of IHT can perform much like a system with an ideal RIP constant.5 And of course in practice we may not ever be aware exactly how the dictionary decomposes as some \u03a6 \u2248 [ A+ \u2206r]N ; however, to the extent that this approximation can possibly hold, the effective RIP constant can be improved nonetheless.\nIt should be noted that globally solving (12) is non-differentiable and intractable, but this is the whole point of incorporating a DNN network to begin with. If we have access to a large number of training pairs {x\u2217,y} generated using the true \u03a6, then during the course of the learning process a useful W and D can be implicitly learned such that a maximal number of sparse vectors can be successfully recovered.\nMoving forward beyond RIP-related issues, there exists one additional way that learning \u0393 could afford some value. Suppose now that \u0393 = BB>D\u03a6>WW>, where W and D\n4. Inclusion of this diagonal factor D can be equivalently viewed as relaxing Proposition 2 to hold under some fixed rescaling of \u03a6, i.e., the optimal support pattern is preserved. 5. Of course at some point we will experience diminishing marginal returns using this prescription. For example, in the extreme case if r = n\u2212 1, then columns of A\u0303 will be reduced to a n\u2212 r = 1 dimensional subspace, and no RIP conditions can possibly hold (see (Bah and Tanner, 2010) for details of how the RIP constant scales with the dimensions of Gaussian iid matrices). Regardless, we can still choose some alternative W and D such that (12) is optimal, but the optimal solution will no longer involve complete eradication of \u2206r.\nare as before (preserving their attendant benefits) and B is an arbitrary invertible matrix. Given that multiplying a gradient by a positive-definite, symmetric matrix guarantees a descent direction is preserved, the inclusion of BB> could be viewed as as a form of natural gradient direction to be learned during training (Amari, 1998). However, given that such a direction must be universal across all layers and possible sparsity patterns, unlike the universal benefit of a lowered RIP constant, it is unclear the extent to which this BB> improves performance. It would be interesting to isolate this effect at least empirically, but we do not pursue this issue further herein.\nTo summarize then, learning layer-wise fixed weights \u03a8 and \u0393 can indeed provide an important benefit by implicitly reducing the RIP constant of \u03a6. We believe this to be a practically-realizable way of affecting what is otherwise an NP-hard constant to even compute, let alone optimize. Learning layer-wise fixed weights can also produce an alternative \u2018natural gradient\u2019 direction; however, given that this direction must be the same for all layers and for all sparse vectors x\u2217, it remains unclear whether or not this latter capability provides any tangible welfare."}, {"heading": "4. Analysis using Layer-Wise Independent Weights and Activations", "text": "In the previous section we observed how jointly adjusting W and D could implicitly remove the effects of low-rank components that inflate dictionary coherence and RIP constant values. However, we also qualified the advantages of this strategy, with diminishing marginal returns as more non-ideal components enter the picture. In fact, it is not difficult to describe a slightly more sophisticated scenario such that use of layer-wise constant weights and activations are no longer capable of lowering \u03b43k[\u03a6] at all, portending failure when it comes to accurate sparse recovery. In contrast, this section will reveal that independent weights and adaptive activations can nonetheless still succeed.\nTo illustrate this effect, we will now analyze dictionaries with columns that are tightly grouped into clusters such that the within-group correlation is high while the betweengroup correlation is modest. As further technical results require a bit more precision, we first present the following formal definition.\nDefinition 5 (Clustered Dictionary Model) Let A = [A1, . . . ,Ac] \u2208 Rn\u00d7m denote a partitioned matrix, with each partition Aj \u2208 Rn\u00d7mj sized such that m = \u2211 imi. Also define U = [u1, . . . ,uc] \u2208 Rn\u00d7c and vj \u2208 Rmj for all j = 1, . . . , c. Moreover, assume that both U and A are constructed with columns of unit `2 norm. Then a dictionary matrix \u03a6 is said to arise from the clustered dictionary model if \u03a6 = [\u03a61, . . . ,\u03a6c]N , with \u03a6j = ujv > j + Aj,\n> 0 as a scalar weighting factor, and N a diagonal matrix that applies final `2 column normalization. We also define the cluster support Sc(x) \u2282 {1, . . . , c} as the set of cluster indices whereby some x \u2208 Rm has at least one nonzero corresponding element.\nTherefore it becomes readily apparent that, provided is chosen sufficiently small, each partition \u03a6j will be a tight cluster of basis vectors centered around an axis formed by the corresponding uj . In some sense this model represents the simplest partitioning of correlation structure into two scales: the inter- and intra-cluster structures. It thus represents an accessible model for evaluating further manual or learned modifications of IHT. In particular, we note that assuming c is large, possibly even larger than n, we can no longer rely\non W and D to reduce \u03b43k[\u03a6] as we did in Section 3, as annihilating every rank-one ujv > j term is clearly impossible. We now turn to an adaptation of IHT that includes two important modifications that are reflective of many generic DNN structures:\n1. The hard-thresholding operator is generalized to account for prior information about learned support patterns from previous iterations, and\n2. We allow the dictionary or weight matrix to change from iteration to iteration sequencing through a fixed set akin to layers of a DNN.\nRegarding the former, we define an IHT iteration with partially known support as x(t+1) = Hk(t) [ \u03a8x(t) + \u0393y; \u2126(t)on ,\u2126 (t) off ] , (15)\nwhere \u2126(t)on denotes a support set of \u03c7 (t+1) , \u03a8x(t) + \u0393y that is immune from hardthresholding, and \u2126 (t) off denotes a second support set that is automatically forced to zero.\nThe remaining elements of \u03c7(t+1) not in \u2126(t)on \u22c3 \u2126 (t) off then face the standard hard-thresholding operator, with all but the largest k(t) values set to zero. In spirit, (15) can be viewed as something like a highway network element (Srivastava et al., 2015) or a LSTM cell (Hochreiter and Schmidhuber, 1997), where elements can be turned on and off via a gating mechanism separate from the activation function.\nWhen combined with layer-wise weights that change from iteration to iteration via a prescribed sequence (just like a DNN), we arrive at what we term adaptive IHT:\nDefinition 6 (Adaptive Iterative Hard Thresholding (A-IHT)) Let x(0) = 0 and assume we have access to some predefined sequence of weights {\u03a8(t),\u0393(t)} as well as a predefined schedule for computing \u2126(t)on , \u2126 (t) off , and k (t). Then we refer to the iterations\nx(t+1) = Hk(t) [ \u03a8(t)x(t) + \u0393(t)y; \u2126(t)on ,\u2126 (t) off ] (16)\nas adaptive iterative hard-thresholding.\nWe will now examine how A-IHT can directly handle the recovery of maximally sparse signals arising from the clustered dictionary model. We first introduce some additional notation. Let J denote any subset of {1, . . . , c} such that A(J ) , [Aj : j \u2208 J , ]; in other words A(J ) represents the matrix formed by concatenating all partitions of A from the set J .\nProposition 7 Suppose \u03a6 is generated from the clustered dictionary model and that the concatenated matrix [U ,A(J )] has RIP constant \u03b4(3kx+kc) ([U ,A(J )]) < 1/ \u221a 32 for all possible J with |J | \u2264 c. Then there exists an A-IHT algorithm that is guaranteed to produce the correct support pattern of any x\u2217 in a finite number of iterations provided that y = \u03a6x\u2217, \u2016x\u2217\u20160 \u2264 kx, |Sc(x\u2217)| \u2264 kc, and \u2208 (0, \u2032] where \u2032 is suitably small.\nThe technical nature of Proposition 7 belies the simplicity of the actual underlying core idea. We unpack this result via a few important intuitions:\n\u2022 x\u2217 itself is also trivially obtained once the support is correctly estimated. So practically speaking this result guarantees we can recover x\u2217 in a finite number of iterations.\n\u2022 The integrated dictionary \u03a6 can have an arbitrarily large RIP constant as grows small such that IHT (or `1 minimization, etc.) will likely fail to ever find the correct support. In fact, it can be proven that IHT will fail with minimal assumptions.6\n\u2022 In contrast, the sufficient condition for A-IHT to work only depends on coherence involving U and A, the components of the clustered dictionary model, not the integrated dictionary \u03a6. In particular, we require that at both the intra-cluster and between-cluster scales, groups of dictionary columns must be reasonably incoherent.\n\u2022 We can simplify the stated conditions by noting that RIP constants can only go up whenever we increase the number of nonzeros or pad a dictionary with extra columns. Therefore, since kx \u2265 kc and [U ,A] is a superset of the columns from any [U ,A(J )], Proposition 7 will also hold under the more stringent but easier to digest constraint \u03b44kx ([U ,A]) < 1/ \u221a 32. So we pay a small price in the sparsity level multiplier (from\n3k for regular IHT to 4k for A-IHT), but this is offset by the huge gain in working with [U ,A] as opposed to \u03a6 as the argument. And of course in reality we only need this to hold for all of the much smaller dictionary subsets as stipulated in the proposition, a significantly lower bar.\n\u2022 See the proof for details of how the layer weights \u03a8(t) and \u0393(t), and support sets \u2126(t)on and \u2126 (t) off can be constructed. But the core principle is that earlier layers must be tasked\nwith exposing the correct support at the cluster level, without concern for accuracy within each cluster. Once the correct cluster support has been obtained, later layers can then be charged with estimating the fine-grain details of within-cluster support. We believe this type of multi-resolution sparse estimation is essential when dealing with highly coherent dictionaries (more on this in the next section).\n\u2022 The support sets \u2126(t)on and \u2126 (t) off allow the network to \u2018remember\u2019 previously learned\ncluster-level sparsity patterns, in much the same that LSTM gates allow long term dependencies to propagate (Hochreiter and Schmidhuber, 1997) or highway networks (Srivastava et al., 2015) facilitate information flow unfettered to deeper layers. Moreover, practically speaking these sets can be computed by passing the prior layer\u2019s activations x(t) through linear filters followed by indicator functions, again reminiscent of how DNN gating functions are typically implemented.\n\u2022 Even if we exclude the column normalization multiplier N from the clustered dictionary model, IHT will still fail since the top kx elements obtained via hard-thresholding will be dominated by columns of \u03a6 with large norms at the mercy of vj scale factors.\nWe next turn to the more practically relevant situation where the dictionary cannot be so neatly partitioned into two levels of detail, such that manual construction of a priori layer-dependent weights is much more difficult.\n6. Technically speaking, the RIP conditions are sufficient but not necessary conditions for success. Therefore, just because the constant is too high alone does not always guarantee failure."}, {"heading": "5. Discriminative Multi-Resolution Sparse Estimation", "text": "As implied previously, guaranteed success for most existing sparse estimation strategies hinges on the dictionary \u03a6 having columns drawn (approximately) from a uniform distribution on the surface of a unit hypersphere, or some similar condition to ensure that subsets of columns behave approximately like an orthogonal basis. Essentially this confines the structure of the dictionary to operate on a single universal scale. The clustered dictionary model described in the previous section considers a dictionary built on two different scales, with a cluster-level distribution (coarse) and tightly-packed within-cluster details (fine). But in reality practical dictionaries may display structure operating across a variety of scales that interleave with one another, forming a continuum among multiple levels.\nWhen the scales are clearly demarcated, we have seen that it is possible to manually define a multi-resolution A-IHT algorithm that guarantees success in recovering the optimal support pattern; and indeed, A-IHT could be extended to handle a clustered dictionary model with nested structures across more than two scales. However, without clearly partitioned scales it is much less obvious how one would devise an optimal IHT modification. It is in this context that learning with DNNs is likely to be most advantageous. In fact, the situation is not at all unlike many computer vision scenarios whereby handcrafted features such as SIFT may work optimally in confined, idealized domains, while learned CNN-based features are often more effective otherwise.\nGiven a sufficient corpus of {x\u2217,y} pairs linked via some fixed \u03a6, we can replace manual filter construction with a learning-based approach. On this point, although we view our results from Section 4 as a convincing proof of concept, it is unlikely that there is anything intrinsically special about the specific hard-threshold operator and layer-wise construction we employed per se, as long as we alow for deep, adaptable layers that can account for structure at multiple scales. In practice, we expect that it is more important to establish a robust training pipeline that avoids stalling at the hand of vanishing gradients with deep network structure. It is here that we propose three significant deviations from the original IHT template.\n1. We exploit the fact that in producing a maximally sparse vector x\u2217, the main challenge is estimating supp[x\u2217]. Once the support is obtained, computing the actual nonzero coefficients just boils down to solving something like a least squares problem. But any learning system will be unaware of this and could easily expend undue effort in attempting to match coefficient magnitudes at the expense of support recovery. Certainly the use of a data fit penalty of the form h (\u2016y \u2212\u03a6x\u20162), as is adopted by nearly all sparse recovery algorithms, will expose us to this issue. Therefore we instead formulate sparse recovery as a multi-label classification problem. More specifically, instead of directly estimating x\u2217, we attempt to learn s\u2217 = [s\u22171, . . . , s \u2217 m] >, where\ns\u2217i = 1 if x \u2217 i 6= 0, and s\u2217i = 0 otherwise. (17)\nFor this purpose we may then incorporate a traditional multi-label classification loss function via a final softmax output layer, which forces the network to only concern itself with learning support patterns. This substitution is further justified by the fact that even with traditional IHT, the support pattern will be accurately recovered\nbefore the iterations converge exactly to x\u2217. Therefore we may expect that fewer layers (as well as training data) are required if all we seek is a support estimate.\n2. Given that IHT can take many iterations to converge on challenging problems, we may expect that a relatively deep network structure will be needed to obtain exact support recovery. We must therefore take care to avoid premature convergence to areas with vanishing gradient by incorporating several recent countermeasures proposed in the DNN community. For example, as mentioned previously, the adaptive threshold operator A-IHT employs is reminiscent of highway networks or LSTM cells, which have been proposed to allow longer range flow of gradient information to improve convergence through the use of gating functions. An even simpler version of this concept involves direct, un-gated connections that allow much deeper \u2018residual\u2019 networks to be trained (He et al., 2015a) (which is also reminiscent of the residual factor embedded in the original IHT iterations). We deploy this tool, along with batch-normalization (Ioffe and Szegedy, 2015) to aid convergence, for our basic feedforward pipeline. Later we also consider an alternative structure based on recurrent LSTM cells. Note that unfolded LSTM networks frequently receive a novel input for every time step, whereas here y is applied unaltered at every layer (more on this in Section 9).\n3. We replace the non-integrable hard-threshold operator with simple rectilinear (ReLu) units (Nair and Hinton, 2010), which are functionally equivalent to one-sided softthresholding.\nTaken together, these changes deviate from the original IHT script; however, we believe they nonetheless preserve the foundational principles of learning-based sparse estimation. Certainly our empirical evidence below supports this claim."}, {"heading": "6. Construction of Training Sets", "text": "If the ultimate goal is to learn an accurate model for computing the minimum `0-norm, maximally sparse solution, then care must be taken in how we construct the training data. This issue is especially acute in the operational regime considered herein, namely sparse linear inverse problems where dictionary coherence is high.\nAs an illustrative example of this point, suppose we have a dictionary \u03a6 that is known to facilitate highly compact representations of some signal class of interest Y. Even if we have access to a large set of observations y \u2208 Y, our work is still ahead of us to construct a viable training set. This is because in general, computing the maximally sparse x\u2217 that corresponds with each y represents an NP-hard problem, and if the dictionary is coherent fast approximate schemes like OMP and IHT will fail; similarly `1-`0 norm equivalency breaks down corrupting convex solutions. Consequently, if we use any of these methods to generate training values for x\u2217, we will merely end up learning a network that approximates a suboptimal strategy, not the true `0 norm solution that represents our goal to begin with. To the best of our knowledge though, this is the route that all previous learning-based sparse estimation pipelines proceed, e.g., see (Gregor and LeCun, 2010; Sprechmann et al., 2015; Wang et al., 2015). Hence these systems do not actually produce maximally sparse estimates as is our focus here, although they do represent quite useful methods for reducing the computational burden of approximate schemes like `1 minimization.\nIn this paper we advocate an entirely different strategy. We first generate sparse training vectors x\u2217 with random support patterns. We then compute synthetic observations using y = \u03a6x\u2217 (possibly with additional additive noise for robustness). Provided the dictionary satisfies minimal assumptions related to a quantity called matrix spark (Donoho and Elad, 2003), x\u2217 will provably represent the maximally sparse feasible solution. In this way we have an inexpensive means of producing whatever volume of training data we desire. Moreover, we have observed modest sensitivity at test time to the actual magnitude distribution of nonzero coefficients in x\u2217 used during training. In other words, even if we test using a different magnitude distribution than was used to generate training data, performance is relatively stable (see Section 7). This is a likely consequence of using a softmax final multi-label classification layer as opposed to trying to directly estimate x\u2217. In practice this stability is paramount since we may not have a good estimate of this distribution anyway."}, {"heading": "7. Feedforward Network Experiments", "text": "With existing sparse optimization algorithms, the goal is to estimate x\u2217 when presented with y and \u03a6. In contrast, with a large corpus of training pairs {x\u2217,y} we intend to learn a mapping from y to x\u2217 using a deep architecture. To isolate the various factors affecting the performance of feedforward networks in particular, this section describes experiments using a variety of different data generation procedures. Later, Section 9 will consider a competing recurrent LSTM architecture."}, {"heading": "7.1 Network Design", "text": "We confine our design here to the feedforward structure motivated in Section 5. In brief, we build a 20-layer network with residual connections (He et al., 2015a) and batch normalization (Ioffe and Szegedy, 2015). Moreover, because our sparse data will ultimately have no indication of local smoothness, we use fully connected layers rather than convolutions. For the nonlinearities we apply rectilinear units (ReLU). We also include a final softmax layer which outputs a vector p \u2208 Rm, with pj \u2208 [0, 1] providing an estimate of the probability that xj 6= 0. The detailed network structure, which was implemented using MXNet (Chen et al., 2015), can be found in Figure 1."}, {"heading": "7.2 Basic Sparse Estimation Experimental Setup", "text": "We generate a dictionary matrix \u03a6 \u2208 Rn\u00d7m using\n\u03a6 = n\u2211 i=1 1 i2 uv>, (18)\nwhere u \u2208 Rn and v \u2208 Rm have iid elements drawn from N (0, 1). We also rescale each column of \u03a6 to have unit `2 norm. \u03a6 generated in this way has super-linear decaying singular values (indicating correlation between the columns) but is not constrained to any specific structure. Many dictionaries in real applications have such a property. As a basic experiment, we generate N ground truth samples x\u2217 \u2208 Rm by randomly selecting d nonzero entries, with nonzero amplitudes drawn iid from the uniform distribution U [\u22120.5, 0.5], excluding the interval [\u22120.1, 0.1] to avoid small, relatively inconsequential contributions to the support pattern. We then create y \u2208 Rn via y = \u03a6x\u2217. As we increase d, the sparse estimation problem becomes intrinsically more difficult. We set n = 20 and m = 100, while d \u2264 10, noting that if d = 10 we have only twice as many measurements as nonzeros in x\u2217, which is a challenging regime, especially when \u03a6 has strong correlations.\nWe generated N = 700000 total samples and used the first N1 = 600000 for training and the remaining N2 = 100000 for testing. Network optimization is achieved via stochastic gradient descent (SGD) with a momentum of 0.9 and weight decay of 0.0001. The initialization follows (He et al., 2015b) and we did not apply any drop-out. The batch size was set to 250. The initial learning rate was 0.01 and was reduced by 90% every 50 epoches. We stopped training after 150 epoches, at which point empirical convergence was always observed. Unless otherwise specified, throughout this paper we convert x\u2217 to the binary label vector s\u2217 from (17) for training purposes using the stated softmax output layer.\nTo evaluate the performance, we introduce two metrics referred to as strict accuracy (sacc) and loose accuracy (l-acc), respectively. Both depend on two sets for each sample/trial, the ground truth labels and the predicted top-d labels given by\nSgt = {j : xj 6= 0} Spred(d) = {j : pj is one of the d largest values.} (19)\nStrict accuracy evaluates whether the d ground truth nonzeros are exactly aligned with the predicted top-d values produced by our network, and when averaged across test trails, can be computed via\ns-acc = 1\nN2 N2\u2211 i=1 I [ S(i)gt = S (i) pred(d) ] (20)\nwhere I[\u00b7] is an indicator function and here the superscript (i) denotes the sample number. This all-or-nothing metric is commonly adopted in the compressive literature, and effectively measures the percentage of trials where we can perfectly recover x\u2217.\nIn contrast, loose accuracy considers the degree to which the correct support is included in the largest values of p. Note that given our experimental design, it can be shown that with probability one there exists only a single feasible solution to y = \u03a6x such that \u2016x\u20160 < n = 20 (this will define the optimal support set by design). Moreover, any support\npattern with exactly n nonzeros is sufficient to produce a unique feasible solution (referred to as a basic feasible solution in the linear programming literature (Luenberger, 1984)). Hence we define loose accuracy as the degree to which the true support indeces are contained within the top 20 largest values of p, or\nl-acc = 1\nN2 N2\u2211 i=1 \u2223\u2223\u2223S(i)gt \u222a S(i)pred(n)\u2223\u2223\u2223 d . (21)\nBoth s-acc and l-acc metrics are computed for all N2 test points and compared against a battery of existing algorithms, both learning- and optimization-based.7 These include standard `1 minimization via ISTA iterations (Cande\u0300s et al., 2006), IHT (Blumensath and Davies, 2009), an ISTA-based network (Gregor and LeCun, 2010), and an IHT-inspired network (Wang et al., 2015). For `1 minimization we used publicly-available ISTA code, 8 while for IHT we applied our own implementation (it only requires a few lines in Matlab) and supplied the hard-thresholding operator with the ground truth number of nonzeros, meaning k = d for all experiments. For the learning-based methods, we estimated parameters for both the ISTA- and IHT-based networks using MXNet and the exact same training data described above."}, {"heading": "7.2.1 Accuracy Results", "text": "Figure 2 illustrates how different methods perform under both evaluation metrics. Given the correlated \u03a6 matrix, the recovery performance of IHT, and to a lesser degree `l minimization\n7. For competing algorithms, we compute Spred(d) using the largest (in magnitude) n elements of any estimate x\u0302. 8. http://www.eecs.berkeley.edu/ yang/software/l1benchmark/index.html\nusing ISTA, is rather modest as expected given that the associated RIP constant will be quite large by construction. In contrast our method achieves uniformly higher accuracy under both metrics, including over existing learning-based methods trained with the same data. This improvement is likely the result of three significant factors: (i) Existing learning methods initialize using weights derived from the original sparse estimation algorithms, but such an initialization will be associated with locally optimal solutions in most cases with correlated dictionaries. (ii) As described in Section 3, constant weights across layers have limited capacity to unravel multi-resolution dictionary structure, especially one that is not confined to only possess some low rank correlating component. (iii) The quadratic loss function used by existing methods does not adequately focus resources on the crux of the problem, which is accurate support recovery. In contrast our approach adopts an initialization motivated directly by DNN-based training considerations, unique layer weights to handle a multi-resolution dictionary, and a multi-label classification output layer to focus learning on support recovery."}, {"heading": "7.2.2 Computational Efficiency", "text": "Table 1 displays the average per-sample runtime required to produce each sparse estimate. Not surprisingly, the learning-based methods display a dramatic advantage over ISTA-based `1-minimization and IHT, both of which require a high number of iterations to converge. In contrast, ISTA-Net, IHT-Net, and our method only involve passing activations through a handful of layers. Although these learning-based approaches all require a potentially expensive training phase, for any task of interest with a fixed \u03a6 matrix, we need only fit the network model once up front and then subsequent testing/deployment will always be much more efficient."}, {"heading": "7.3 Variants of the Basic Experiment", "text": "Here we vary a number of different factors from the basic experiment, in each case holding all others fixed."}, {"heading": "7.3.1 Varying training set size", "text": "In Figure 3, we see that adding more training data to our method can further boost accuracy. This is not a surprise given that it is fundamentally based on learning, and therefore, as long as the capacity of the network allows and optimization ends up in a good basin, we may expect some improvement with additional data."}, {"heading": "7.3.2 Alternative network structures", "text": "As discussed in Section 5, our DNN design choices were largely motivated by practical issues related to the information and gradient flows to which DNN training can be highly sensitive. In this section we examine different network architectures to quantify essential factors affecting performance. In particular, we consider the following changes:\n1. We remove the residual-net connections.\n2. We replace ReLU with hard-threshold activations. In particular, we utilize the socalled HELU\u03c3 function introduced in (Wang et al., 2015), which is a continuous and piecewise linear approximation of the scalar hard-threshold operator given by\nHELU\u03c3(x) =  0 if |x| \u2264 1\u2212 \u03c3 1 \u03c3 (x\u2212 1 + \u03c3) if 1\u2212 \u03c3 < x < 1 1 \u03c3 (x+ 1\u2212 \u03c3) if \u2212 1 < x < \u03c3 \u2212 1 ui if |x| \u2265 1.\n3. We use a quadratic penalty layer instead of a multi-label classification loss layer, i.e., the loss function is changed to \u2211N1 i=1 \u2016a(i) \u2212 y(i)\u201622 (where a is the output of the last\nfully-connected layer) during training.\nFigure 4 displays the associated recovery percentages, where we observe that in each case performance degrades. Without the residual design, and also with the inclusion of a rigid, non-convex hard-threshold operator, local minima during training appear to be a likely culprit, consistent with observations from (He et al., 2015a). Likewise, use of a leastsquares loss function is likely to emphasize the estimation of coefficient amplitudes rather than focusing on support recovery."}, {"heading": "7.3.3 Different distributions for x\u2217", "text": "From a practical standpoint, we would like to estimate the support pattern of all the significant elements of x\u2217; however, these elements need not all have the same amplitudes.\nMoreover, in practice we may expect that the true amplitude distribution may deviate at times from the original training set. To explore robustness to such mismatch, as well as different amplitude distributions, we consider two sets of candidate data: the original from Section 7.2, and similarly-generated data but with the uniform distribution of nonzero elements replaced with the Gaussians N (\u00b10.3, 0.1), where the mean is selected with equal probability as either \u22120.3 or 0.3, thus avoiding tiny magnitudes with high probability.\nFigure 5 reports accuracies under different distributions for both training and testing, including mismatched cases. The label \u2018U2U\u2019 refers to training and testing with the uniformly distributed amplitudes described in Section 7.2, while \u2018U2N\u2019 uses uniform training set and a Gaussian test set. Analogous definitions apply for \u2018N2N\u2019 and \u2018N2U\u2019. In all cases we note that the performance is quite stable across training and testing conditions. We would argue that our recasting of the problem as multi-label classification contributes, at least in part, to this robustness. The application example described next demonstrates further tolerance of training-testing set mismatches."}, {"heading": "8. Practical Application: Photometric Stereo", "text": "Photometric stereo represents a powerful technique for recovering high-resolution surface normals from a 3D scene using appearance variations in 2D images under different lightings. For example, when images of an ideal Lambertian surface are obtained under illumination from three known directions, the surface orientation can be uniquely determined using a simple least-squares fit (Woodham, 1980).\nIn practice however, the estimation process is often disrupted by non-Lambertian effects such as specular highlights, shadows, or image noise. To account for such outlying factors, robust estimation methods have been proposed that decompose an observation matrix of stacked images under different lighting conditions into an ideal Lambertian component and\na sparse error term (Wu et al., 2010; Ikehata et al., 2012). While principled in theory, this approach requires solving on the order of 104\u2212 106 distinct sparse regression problems, one for each point for which we would like to obtain a surface normal estimate. We will now map this application domain into our sparse DNN framework, which can readily handle the required outlier removal problem potentially orders-of-magnitude faster than existing practical systems, facilitating real-time deployment in mobile environments."}, {"heading": "8.1 Problem Details", "text": "Suppose we have q observations of a given surface point from a Lambertian scene under different lighting directions. Then the resulting measurements, denoted o \u2208 Rq, can be expressed as\no = \u03c1Ln, (22)\nwhere n \u2208 R3 denotes the true surface normal, each row of L \u2208 Rq\u00d73 defines a lighting direction, and \u03c1 is the diffuse albedo, acting here as a scalar multiplier (Woodham, 1980). If specular highlights, shadows, or other gross outliers are present, then the observations are more realistically modeled as\no = \u03c1Ln+ e, (23)\nwhere e is an an unknown sparse vector (Wu et al., 2010; Ikehata et al., 2012). In this revised scenario, we might consider estimating n using\nmin n\u0303,e \u2016e\u20160 s.t. o = Ln\u0303+ e, (24)\nwhere n\u0303 is merely the surface normal rescaled with \u03c1. From this expression, it is apparent that, since n\u0303 is unconstrained, e need not compensate for any component of o in the range of L. Given that null[L>] is the orthogonal complement to range[L], we may transform (24) to the equivalent problem\nmin e \u2016e\u20160 s.t. Projnull[L>](o) = Projnull[L>](e). (25)\nThe constraint is of course equivalent to y = \u03a6e with y = Projnull[L>](o) and \u03a6e = Projnull[L>](e), and so (24) ultimately collapses to our canonical sparse estimation problem from (1). Additionally, given that rows of \u03a6 will form a basis for null[L>] which is lighting-hardware dependent, there are likely to be unavoidable correlations in the dictionary columns.\nWhile existing sparse estimation algorithms can be adopted to solve (25), this is impractical for many real-world applications since the number of surface points can be extremely large (possibly even greater than 106 for high-resolution reconstructions). Fortunately though, given that \u03a6 is fixed across all possible scenes and surface points for a given lighting geometry, to apply our method we only need learn a single DNN model, and once trained, testing on novel scenes will be extremely efficient. This allows fast computation of outlier positions via the support of e, after which the remaining inlier points can be used to compute surface normals using a traditional least squares fit."}, {"heading": "8.2 Results", "text": "Following (Ikehata et al., 2012), we use 32-bit HDR gray-scale images of the object Bunny (256\u00d7256) with foreground masks under different lighting conditions whose directions, or rows of L, are randomly selected from a hemisphere with the object placed at the center. To apply our method, we first compute \u03a6 using the appropriate projection operator derived from the lighting matrix L. As real-world training data is expensive to acquire, we instead synthetically generate a training set as follows. First, we draw a support pattern for e uniformly at random with cardinality d sampled uniformly from the range [d1, d2]. The values of d1 and d2 can be tuned in practice. Nonzero values of e are assigned iid random values from a Gaussian distribution whose mean and variance are also tunable. Beyond this, no attempt was made to match the true outlier distributions encountered in applications of photometric stereo. Finally, for each e we can naturally compute observations y = \u03a6e, which serve as candidate network inputs.\nGiven synthetic training data acquired in this way, we learn a network with the exact same structure and optimization parameters as in Section 7; no application-specific tuning\nwas introduced. We then deploy the resulting network on the gray-scale Bunny images.9 For each surface point, we use our DNN model to approximate (25). Since the network output will be a probability map for the outlier support set instead of the actual values of e, we choose the 4 indices with the least probability as inliers and use them to compute n via least squares.\nWe compare our method against the baseline least squares estimate from (Woodham, 1980), `1 norm minimization, and a sparse Bayesian learning (SBL) approach specifically developed in (Ikehata et al., 2012) for surface normal estimation. We also consider a second baseline estimator, denoted \u2018Rnd4\u2019, which computes a surface normal estimate using 4 randomly selected indices as putative inliers. As the number of images q is varied, we compute the angular error between the recovered normal map by each algorithm and the ground truth.\nResults are reported in Table 2 for q \u2208 {10, 20, 40}, which also includes runtime comparisons. In the hardest case, where only q = 10 images are present, our method significantly outperforms the others. For q \u2208 {20, 40} images our method is still quite competitive, with only SBL offering superior results. However, it must be noted that SBL represents a computationally-expensive Bayesian approach especially designed for this problem, with\n9. Note that for each different number of images we must train a separate model, since the lighting geometry effectively changes. However, in practice this is not an issue since we only ever need train a single model per hardware configuration.\nruntimes nearly two orders of magnitude higher than our DNN.10 Additionally, a key reason that our approach does not improve substantially as q increases is that we fixed the assumed number of inliers to be 4 in all cases; however, allowing a flexible number that grows with the number of images (as implicitly permitted by SBL) will likely improve performance.\nAs a complementary perspective, recovered surface normal error maps are displayed in Figure 6 when q = 10. Here we observe that our DNN estimates lead to far fewer regions of significant error. Overall though, this application example illustrates that mismatched synthetic training data can, at least for some problem domains, be sufficient to learn a quite useful sparse estimation DNN."}, {"heading": "9. Alternative LSTM Networks", "text": "Thus far our experimentation has focused on feedforward networks with a residual design. In this section we turn to a recurrent LSTM structure and execute some preliminary evaluations. As high-level motivation, there are many similarities between unfolded sparse estimation algorithms like the adaptive IHT discussed in Section 4 and an unfolded LSTM network. Both supply an input y to every unfolded layer, and both implicitly utilize gating functions to switch activations on or off as learning proceeds, allowing partial support patterns or other information to be remembered in deeper layers. Although we will defer a much more detailed, self-contained exploration to a future paper, we nonetheless here present an initial empirical proof-of-concept using a vanilla form of LSTM network that has not been explicitly tailored for sparse estimation problems beyond the final multi-label classification layer described previously.\nUsing a cell design from (Hochreiter and Schmidhuber, 1997), we adopt a two-layer LSTM network with a fixed size of 11 steps. Figure 7 presents the specific structure. We compare this network against both our original residual net implementation and SBL. The later was chosen because it represents an algorithm explicitly designed to handle dictionary correlations (Wipf, 2012). For training and testing we use the protocol from Section 7.2, but with nonzero elements of x\u2217 having unit magnitudes. This modification was introduced because it represents a challenging scenario for many traditional sparse optimization algorithms for technical reasons related to local minima detailed in (Wipf et al., 2011). Results are shown in Figure 8, where we observe that the LSTM net is even able to outperform SBL. Further experiments and connecting analyses will be presented in future work for space considerations."}, {"heading": "10. Conclusions", "text": "There is a clear relationship between iterative optimization rules promoting sparsity (such as those from IHT) and the layers of deep networks. Building from this perspective, in this paper we have shown that deep networks with hand-crafted, multi-resolution structure can provably solve certain specific classes of sparse recovery problems where existing algorithms fail. However, much like CNN-based features can often outperform SIFT on many computer vision tasks, we argue that a discriminative approach can outperform manual structuring of\n10. In fact, the runtime of our method is only twice that of Rnd4, which uses just a single, low-dimensional least squares fit.\nlayers/iterations and compensate for dictionary coherence under more general conditions. We also believe that many of the underlying principles explored herein also transfer to other applications operating at the boundary between optimization- and learning-based systems.\nThere is of course one important caveat with the pursuit of maximal sparsity using a deep network. Sparse estimation problems can be partitioned into two different categories, centered around whether or not the dictionary \u03a6 is reusable. In brief, learning-based methods are only feasible when a fixed (or similar) dictionary can be repeatedly used to represent a signal class of interest. Examples include outlier removal (Cande\u0300s and Tao, 2005; Ikehata et al., 2012), compressive sensing (Donoho, 2006), and source localization (Baillet et al., 2001; Malioutov et al., 2005) applications where, once learned, a deep model can produce maximally sparse representations as new input signals arrive. In contrast, other influential domains such as subspace clustering (Elhamifar and Vidal, 2013) effectively require solving sparse recovery problems with a novel dictionary at each instance such that any attempt to construct a viable training set would be infeasible. Hence optimizationbased sparse estimation nonetheless remains an important tool regardless of how effective learned models can sometimes be.\nAppendix\nHere we include technical proofs of our main results. In places we rely on three standard asymptotic notations describing the order of an arbitrary function f(x):\nf(x) = O(g(x)) \u21d0\u21d2 \u2203c > 0, |f(x)| \u2264 c|g(x)|, f(x) = \u2126(g(x)) \u21d0\u21d2 \u2203c > 0, |f(x)| \u2265 c|g(x)|, (26) f(x) = \u0398(g(x)) \u21d0\u21d2 \u2203c1, c2 > 0, c1|g(x)| \u2264 |f(x)| \u2264 c2|g(x)|."}, {"heading": "10.1 Proof of Proposition 2", "text": "Consider some x\u2217 where x\u22171 = 1 and x \u2217 i = for i \u2208 \u2126, with |\u2126| = k \u2212 1. In this restricted setting, it follows that\n\u03a8x\u2217 = \u03c81 +O( ) and \u0393y = \u0393[\u03c61 +O( )] = \u0393\u03c61 +O( ) (27)\nand therefore \u03a8x\u2217 + \u0393y = \u03c81 + \u0393\u03c61 +O( ). (28) To ensure that x\u2217 = Hk (\u03a8x \u2217 + \u0393y), the largest (in magnitude) k elements of z , \u03c81 + \u0393\u03c61 +O( ) must align with {1,\u2126}, and zi = x\u2217i for all i \u2208 {1,\u2126}. Together these conditions are necessary to ensure that the Hk operator will produce x\n\u2217; however, they also imply that z = e1 +O( ), where e1 is a zero vector with a \u20181\u2019 in the first position, since no element in the complement of {1,\u2126} can be larger than O( ). Therefore we require that\n\u03c81 + \u0393\u03c61 = e1 +O( ). (29)\nOf course since this must hold for any > 0, it must be that \u03c81 +\u0393\u03c61 = e1. Repeating this procedure with x\u2217i = 1 for all i \u2208 {1, . . . ,m} then leads to the requirement that \u03a8+\u0393\u03a6 = I."}, {"heading": "10.2 Proof of Proposition 3", "text": "With \u0393 = D\u03a6>WW> and the stipulated rescaled dictionary, (10) becomes\nx(t+1) = Hk [( I \u2212D\u03a6>WW>\u03a6D ) x(t) + \u0393y ] . (30)\nFor the moment, assume that W is invertible. Then the iteration (30) is consistent with the modified objective\nmin x\n1 2\u2016Wy \u2212W\u03a6Dx\u2016 2 2 s.t. \u2016x\u20160 \u2264 k. (31)\nSince \u2016x\u2217\u20160 \u2264 k with y = \u03a6x\u2217, it follows that \u2016x\u0303\u2217\u20160 \u2264 k and y\u0303 = \u03a6\u0303x\u0303\u2217, where x\u0303\u2217 ,D\u22121x\u2217, y\u0303 ,Wy, and \u03a6\u0303 ,W\u03a6D. We may then apply Theorem 5 from (Blumensath and Davies, 2009) using this revised system and conclude that\n\u2016x\u0303(t) \u2212 x\u0303\u2217\u20162 \u2264 2\u2212t\u2016x\u0303\u2217\u20162, (32)\nwhich leads to the stated result."}, {"heading": "10.3 Proof of Corollary 4", "text": "Consider some projection operator P = W>W onto null[\u2206>r ], using W formed with r orthonormal rows, which equivalently projects onto the orthogonal complement of range[\u2206r]. Therefore\nW\u03a6D = W [ A+ \u2206r]ND = W A (33)\nwhen D = ( N)\u22121. Given that elements of A are drawn iid from N (0, 1/ \u221a n) and W has orthonormal rows, elements of WA \u2208 R(n\u2212r)\u00d7m will also have iid elements from the same distribution. Hence the stated selections for W and D allow us to obtain the worst-case upper bound (in expectation) found in the corollary."}, {"heading": "10.4 Proof of Proposition 7", "text": "Let Ij denote the set of column indeces of \u03a6 associated with \u03a6j and v the vectorized concatenation of all vj , i.e., v , [v>1 , . . . ,v > c ] >. We then introduce the following intermediate result.\nLemma 8 By construction of the clustered dictionary model\ny = Uz\u2217 + \u03bd, (34)\nwhere z\u2217 \u2208 Rc is a sparse vector such that z\u2217j = \u2211 i\u2208Ij vix \u2217 i (which implies that \u2016z\u2217\u20160 \u2264 kc), and \u03bd = O( ).\nProof: Without loss of generality, we note that assuming i \u2208 Ij , then nii = v\u22121i + O( ). To see this, we note that\nn\u22121ii \u2016\u03c6i\u20162 = \u2016viuj + ai\u20162 \u2264 vi + = vi +O( ) (35)\nvia the triangle inequality and the fact that \u2016uj\u20162 = \u2016ai\u20162 = 1 by assumption. Therefore the normalization constant satisfies\nnii = 1\nvi +O( ) =\n1 vi + O( ) v2i + viO( ) = 1 vi +O( ). (36)\nWe then have\n\u03c6i = ni(viuj + ai) = uj +O( ) (37)\nand therefore \u03c6ix \u2217 i = ujx \u2217 i +O( ) which naturally leads to\n\u03a6x\u2217 = \u2211 j \u2211 i\u2208Ij ujx \u2217 i +O( ) = \u2211 j uj \u2211 i\u2208Ij x\u2217i +O( ), (38)\nproducing the stated result.\nThis design motivates developing initial iterations based solely around detecting the correct cluster support. To accomplish this we use \u03a8(t) = I \u2212U>U , \u0393(t) = U>, k(t) = kc, and \u2126(t)on = \u2126 (t) off = \u2205 for the initial A-IHT iterations, which is equivalent to the standard IHT updates applied using a sparsity level of kc and a dictionary formed only with U . Here we are effectively not requiring that x(t) = z(t) be the same dimension at each step, or equivalently, we do not require that each \u03a8(t) be a square matrix (this simplifies the exposition although we could nonetheless provide an alternative argument with a fixed dimension). Note that since kx \u2265 kc, then\n\u03b43kc (U) \u2264 \u03b4(3kx+kc) (U) \u2264 \u03b4(3kx+kc) ([U ,A(J )]) < 1/ \u221a 32 (39)\nfor any J \u2282 {1, . . . , c}. This allows us to apply Theorem 5 from (Blumensath and Davies, 2009), from which we can infer that after at most\n\u03c4 = log2 ( \u2016z\u2217\u20162 \u2016\u03bd\u20162 ) (40)\niterations, z\u2217 will be estimated with accuracy\n\u2016z(\u03c4) \u2212 z\u2217\u20162 \u2264 6\u2016\u03bd\u20162 = O( ). (41)\nDefine z\u2217min as the nonzero element of z \u2217 with smallest magnitude and suppose that supp\n[ z(\u03c4) ] 6=\nsupp [z\u2217]. Then in order to satisfy (41), it must be that z\u2217min = O( ). However, z\u2217min is independent of , the latter of which can be made arbitrarily small when the upper bound \u2032\nis small leading to a contradiction. Consequently, it must be that supp [ z(t) ]\n= supp [z\u2217]. Therefore we may conclude that after a finite number of iterations, we have extracted the correct cluster support, or the correct low-resolution approximation. Of course the correct support will likely be converged to long before we reach the iteration number from (40), but this is a worst case bound.\nOf course from a practical standpoint we will not have access to z\u2217 such that \u2016z(\u03c4)\u2212z\u2217\u20162 is computable. However, it can be shown that \u2016y\u2212Uz\u20162 = \u2126(z\u2217min) for any feasible z with \u2016z\u20160 \u2264 kc and supp[z] 6= supp[z\u2217]. Therefore we can monitor this observable error metric\nas a proxy, and when it reaches some O( ) < \u0398(z\u2217min) \u2264 \u2126(z\u2217min) for sufficiently small, we will be guaranteed that the correct support has been found.\nWe now turn our attention to extracting a final estimate of supp[x\u2217]. First we pad z(\u03c4) with zeros to form the vector z(\u03c4+1) = [z(\u03c4); 0] \u2208 Rm+c. This is trivially accomplished using a single layer with \u0393(\u03c4) = O, \u03a8(\u03c4) = [I; 0], and \u2126 = {1, . . . , c}. We next adopt the filters\n\u03a8(\u03c4+t) = I \u2212 [U ,A]>[U ,A] \u0393(\u03c4+t) = [U ,A]>, (42)\nwhich conform with IHT updates using the dictionary [U ,A] and the revised observation model y = [U ,A]z, where we know by assumption there exists an exact solution z\u2032 such that \u2016z\u2032\u20160 \u2264 kx + kc, with kc nonzero coefficients corresponding with columns of U , and kx nonzero coefficients corresponding with columns of A.\nThe support templates \u2126(t)on and \u2126 (t) off are constructed as follows. Define Jc , supp\n[ z(\u03c4) ] .\nWe then use\n\u2126(\u03c4+t)on = Jc (43) \u2126 (t) off = c+ supp [A({1, . . . , c}\\Jc)] .\nConceptually these selections are quite straightforward, any notational obfuscation notwithstanding. These support patterns should be viewed in light of the new, implicit dictionary [U ,A]. With \u2126(\u03c4+t)on , we are simply selecting the basis vectors associated with U that conform with true cluster centers. Likewise for \u2126 (t) off we are effectively pruning all A({1, . . . , c}\\Jc), meaning cluster details associated with clusters that have already been pruned (and the additive factor of c in (43) is merely added to account for the pre-padding with U in [U ,A]).\nGiven these selections, we are effectively running IHT with the collapsed dictionary [U ,A(Jc)], where we already know the true sparsity profile associated with columns of U . This corresponds with the problem of partial support recovery for nonzeros outside of \u2126(\u03c4+t)on . Additionally, with respect to [U ,A(Jc)] (and all zeros elsewhere), z\u2032 represents the maximally sparse feasible solution. This is because\n\u03b4(2[kx+kc]) ([U ,A(Jc)]) \u2264 \u03b4(3kx+kc) ([U ,A(Jc)]) < 1/ \u221a 32, (44)\nand all that is required for a unique, maximally sparsity with kx + kc nonzeros is the much weaker inequality \u03b4(2[kx+kc]) ([U ,A(Jc)]) < 1 (Cande\u0300s et al., 2006).\nKnown partial support allows us to loosen the requirement for guaranteed support recovery. In particular, using modifications of Theorem 1 from (Carrillo et al., 2011) and the fact that \u03b4(3kx+kc) ([U ,A(Jc)]) < 1/ \u221a 32, it follows that after \u03c4 + t iterations, the A-IHT reconstruction error is bounded by\n\u2016z\u2032 \u2212 z(\u03c4+t)\u20162 \u2264 2\u2212t\u2016z\u2032 \u2212 z(\u03c4+1)\u20162. (45)\nConsequently, after a finite number of iterations z(\u03c4+t) must have matching support as z\u2032, and moreover, as t\u2192\u221e, z(\u03c4+t) \u2192 z\u2032. These can be mapped directly to the optimal support of x\u2217, from which x\u2217 itself is also trivially obtained."}], "references": [{"title": "Natural gradient works efficiently in learning", "author": ["S. Amari"], "venue": "Neural Computation,", "citeRegEx": "Amari.,? \\Q1998\\E", "shortCiteRegEx": "Amari.", "year": 1998}, {"title": "Improved bounds on the restricted isometry constants for gaussian matrices", "author": ["B. Bah", "J. Tanner"], "venue": "SIAM J. Matrix Analysis Applications,", "citeRegEx": "Bah and Tanner.,? \\Q2010\\E", "shortCiteRegEx": "Bah and Tanner.", "year": 2010}, {"title": "Electromagnetic brain mapping", "author": ["S. Baillet", "J.C. Mosher", "R.M. Leahy"], "venue": "IEEE Signal Processing Magazine,", "citeRegEx": "Baillet et al\\.,? \\Q2001\\E", "shortCiteRegEx": "Baillet et al\\.", "year": 2001}, {"title": "A fast iterative shrinkage-thresholding algorithm for linear inverse problems", "author": ["A. Beck", "M. Teboulle"], "venue": "SIAM J. Imaging Sciences,", "citeRegEx": "Beck and Teboulle.,? \\Q2009\\E", "shortCiteRegEx": "Beck and Teboulle.", "year": 2009}, {"title": "Iterative thresholding for sparse approximations", "author": ["T. Blumensath", "M.E. Davies"], "venue": "J. Fourier Analysis and Applications,", "citeRegEx": "Blumensath and Davies.,? \\Q2008\\E", "shortCiteRegEx": "Blumensath and Davies.", "year": 2008}, {"title": "Iterative hard thresholding for compressed sensing", "author": ["T. Blumensath", "M.E. Davies"], "venue": "Applied and Computational Harmonic Analysis,", "citeRegEx": "Blumensath and Davies.,? \\Q2009\\E", "shortCiteRegEx": "Blumensath and Davies.", "year": 2009}, {"title": "Normalized iterative hard thresholding: Guaranteed stability and performance", "author": ["T. Blumensath", "M.E. Davies"], "venue": "IEEE J. Selected Topics Signal Processing,", "citeRegEx": "Blumensath and Davies.,? \\Q2010\\E", "shortCiteRegEx": "Blumensath and Davies.", "year": 2010}, {"title": "Decoding by linear programming", "author": ["E. Cand\u00e8s", "T. Tao"], "venue": "IEEE Trans. Information Theory,", "citeRegEx": "Cand\u00e8s and Tao.,? \\Q2005\\E", "shortCiteRegEx": "Cand\u00e8s and Tao.", "year": 2005}, {"title": "Robust uncertainty principles: Exact signal reconstruction from highly incomplete frequency information", "author": ["E. Cand\u00e8s", "J. Romberg", "T. Tao"], "venue": "IEEE Trans. Information Theory,", "citeRegEx": "Cand\u00e8s et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Cand\u00e8s et al\\.", "year": 2006}, {"title": "Iterative hard thresholding for compressed sensing with partially known support", "author": ["R.E. Carrillo", "L.F. Polania", "K.E. Barner"], "venue": "International Conference on Accoustics, Speech, and Signal Processing,", "citeRegEx": "Carrillo et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Carrillo et al\\.", "year": 2011}, {"title": "MXNet: A flexible and efficient machine learning library for heterogeneous distributed systems", "author": ["T. Chen", "M. Li", "Y. Li", "M. Lin", "N. Wang", "M. Wang", "T. Xiao", "B. Xu", "C. Zhang", "Z. Zhang"], "venue": "arXiv preprint arXiv:1512.01274,", "citeRegEx": "Chen et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Chen et al\\.", "year": 2015}, {"title": "Sparse channel estimation via matching pursuit with application to equalization", "author": ["S.F. Cotter", "B.D. Rao"], "venue": "IEEE Trans. on Communications,", "citeRegEx": "Cotter and Rao.,? \\Q2002\\E", "shortCiteRegEx": "Cotter and Rao.", "year": 2002}, {"title": "Compressed sensing", "author": ["D.L. Donoho"], "venue": "IEEE Trans. Information Theory,", "citeRegEx": "Donoho.,? \\Q2006\\E", "shortCiteRegEx": "Donoho.", "year": 2006}, {"title": "Optimally sparse representation in general (nonorthogonal) dictionaries via `1 minimization", "author": ["D.L. Donoho", "M. Elad"], "venue": "Proc. National Academy of Sciences,", "citeRegEx": "Donoho and Elad.,? \\Q2003\\E", "shortCiteRegEx": "Donoho and Elad.", "year": 2003}, {"title": "Sparse subspace clustering: Algorithm, theory, and applications", "author": ["E. Elhamifar", "R. Vidal"], "venue": "IEEE Trans. Pattern Analysis and Machine Intelligence,", "citeRegEx": "Elhamifar and Vidal.,? \\Q2013\\E", "shortCiteRegEx": "Elhamifar and Vidal.", "year": 2013}, {"title": "Adaptive sparseness using jeffreys prior", "author": ["M.A.T. Figueiredo"], "venue": "Advances in Neural Information Processing Systems", "citeRegEx": "Figueiredo.,? \\Q2002\\E", "shortCiteRegEx": "Figueiredo.", "year": 2002}, {"title": "Learning fast approximations of sparse coding", "author": ["K. Gregor", "Y. LeCun"], "venue": "In Proceedings of the 27th International Conference on Machine Learning,", "citeRegEx": "Gregor and LeCun.,? \\Q2010\\E", "shortCiteRegEx": "Gregor and LeCun.", "year": 2010}, {"title": "Deep residual learning for image recognition", "author": ["K. He", "X. Zhang", "S. Ren", "J. Sun"], "venue": "arXiv preprint arXiv:1512.03385,", "citeRegEx": "He et al\\.,? \\Q2015\\E", "shortCiteRegEx": "He et al\\.", "year": 2015}, {"title": "Delving deep into rectifiers: Surpassing human-level performance on imagenet classification", "author": ["K. He", "X. Zhang", "S. Ren", "J. Sun"], "venue": "In IEEE International Conference on Computer Vision,", "citeRegEx": "He et al\\.,? \\Q2015\\E", "shortCiteRegEx": "He et al\\.", "year": 2015}, {"title": "Deep unfolding: Model-based inspiration of novel deep architectures", "author": ["J.R. Hershey", "J. Le Roux", "F. Weninger"], "venue": "arXiv preprint arXiv:1409.2574v4,", "citeRegEx": "Hershey et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Hershey et al\\.", "year": 2014}, {"title": "Long short-term memory", "author": ["S. Hochreiter", "J. Schmidhuber"], "venue": "Neural computation,", "citeRegEx": "Hochreiter and Schmidhuber.,? \\Q1997\\E", "shortCiteRegEx": "Hochreiter and Schmidhuber.", "year": 1997}, {"title": "Robust photometric stereo using sparse regression", "author": ["S. Ikehata", "D.P. Wipf", "Y. Matsushita", "K. Aizawa"], "venue": "In Computer Vision and Pattern Recognition,", "citeRegEx": "Ikehata et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Ikehata et al\\.", "year": 2012}, {"title": "Batch normalization: Accelerating deep network training by reducing internal covariate shift", "author": ["S. Ioffe", "C. Szegedy"], "venue": "arXiv preprint arXiv:1502.03167,", "citeRegEx": "Ioffe and Szegedy.,? \\Q2015\\E", "shortCiteRegEx": "Ioffe and Szegedy.", "year": 2015}, {"title": "Linear and Nonlinear Programming", "author": ["D.G. Luenberger"], "venue": null, "citeRegEx": "Luenberger.,? \\Q1984\\E", "shortCiteRegEx": "Luenberger.", "year": 1984}, {"title": "Sparse signal reconstruction perspective for source localization with sensor arrays", "author": ["D.M. Malioutov", "M. \u00c7etin", "A.S. Willsky"], "venue": "IEEE Trans. Signal Processing,", "citeRegEx": "Malioutov et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Malioutov et al\\.", "year": 2005}, {"title": "Rectified linear units improve restricted boltzmann machines", "author": ["V. Nair", "G. Hinton"], "venue": "Proc. 27th International Conference on Machine Learning,", "citeRegEx": "Nair and Hinton.,? \\Q2010\\E", "shortCiteRegEx": "Nair and Hinton.", "year": 2010}, {"title": "Orthogonal matching pursuit: Recursive function approximation with applications to wavelet decomposition", "author": ["Y.C. Pati", "R. Rezaiifar", "P.S. Krishnaprasad"], "venue": "In Twenty-Seventh Asilomar Conference on Signals, Systems and Computers,", "citeRegEx": "Pati et al\\.,? \\Q1993\\E", "shortCiteRegEx": "Pati et al\\.", "year": 1993}, {"title": "Learning efficient sparse and low rank models", "author": ["P. Sprechmann", "A.M. Bronstein", "G. Sapiro"], "venue": "IEEE Trans. Pattern Analysis and Machine Intelligence,", "citeRegEx": "Sprechmann et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Sprechmann et al\\.", "year": 2015}, {"title": "Training very deep networks", "author": ["R.K. Srivastava", "K. Greff", "J. Schmidhuber"], "venue": "Advances in Neural Information Processing Systems", "citeRegEx": "Srivastava et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Srivastava et al\\.", "year": 2015}, {"title": "Regression shrinkage and selection via the lasso", "author": ["R. Tibshirani"], "venue": "Journal of the Royal Statistical Society,", "citeRegEx": "Tibshirani.,? \\Q1996\\E", "shortCiteRegEx": "Tibshirani.", "year": 1996}, {"title": "Greed is good: Algorithmic results for sparse approximation", "author": ["J.A. Tropp"], "venue": "IEEE Transactions on Information Theory,", "citeRegEx": "Tropp.,? \\Q2004\\E", "shortCiteRegEx": "Tropp.", "year": 2004}, {"title": "A deterministic analysis of noisy sparse subspace clustering for dimensionality-reduced data", "author": ["Y. Wang", "Y.X. Wang", "A. Singh"], "venue": "International Conference on Machine Learning,", "citeRegEx": "Wang et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Wang et al\\.", "year": 2015}, {"title": "Sparse estimation with structured dictionaries", "author": ["D.P. Wipf"], "venue": "Advances in Nerual Information Processing", "citeRegEx": "Wipf.,? \\Q2012\\E", "shortCiteRegEx": "Wipf.", "year": 2012}, {"title": "Latent variable Bayesian models for promoting sparsity", "author": ["D.P. Wipf", "B.D. Rao", "S. Nagarajan"], "venue": "IEEE Trans. Information Theory,", "citeRegEx": "Wipf et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Wipf et al\\.", "year": 2011}, {"title": "Photometric method for determining surface orientation from multiple images", "author": ["R.J. Woodham"], "venue": "Optical Engineering,", "citeRegEx": "Woodham.,? \\Q1980\\E", "shortCiteRegEx": "Woodham.", "year": 1980}, {"title": "Robust photometric stereo via low-rank matrix completion and recovery", "author": ["L. Wu", "A. Ganesh", "B. Shi", "Y. Matsushita", "Y. Wang", "Y. Ma"], "venue": "Asian Conference on Computer Vision,", "citeRegEx": "Wu et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Wu et al\\.", "year": 2010}], "referenceMentions": [{"referenceID": 13, "context": "Popular examples with varying degrees of computational overhead include convex relaxations such as `1-norm minimization (Donoho and Elad, 2003; Tibshirani, 1996), greedy approaches like orthogonal matching pursuit (OMP) (Pati et al.", "startOffset": 120, "endOffset": 161}, {"referenceID": 29, "context": "Popular examples with varying degrees of computational overhead include convex relaxations such as `1-norm minimization (Donoho and Elad, 2003; Tibshirani, 1996), greedy approaches like orthogonal matching pursuit (OMP) (Pati et al.", "startOffset": 120, "endOffset": 161}, {"referenceID": 26, "context": "Popular examples with varying degrees of computational overhead include convex relaxations such as `1-norm minimization (Donoho and Elad, 2003; Tibshirani, 1996), greedy approaches like orthogonal matching pursuit (OMP) (Pati et al., 1993; Tropp, 2004), and many flavors of iterative thresholding (Beck and Teboulle, 2009; Blumensath and Davies, 2008).", "startOffset": 220, "endOffset": 252}, {"referenceID": 30, "context": "Popular examples with varying degrees of computational overhead include convex relaxations such as `1-norm minimization (Donoho and Elad, 2003; Tibshirani, 1996), greedy approaches like orthogonal matching pursuit (OMP) (Pati et al., 1993; Tropp, 2004), and many flavors of iterative thresholding (Beck and Teboulle, 2009; Blumensath and Davies, 2008).", "startOffset": 220, "endOffset": 252}, {"referenceID": 3, "context": ", 1993; Tropp, 2004), and many flavors of iterative thresholding (Beck and Teboulle, 2009; Blumensath and Davies, 2008).", "startOffset": 65, "endOffset": 119}, {"referenceID": 4, "context": ", 1993; Tropp, 2004), and many flavors of iterative thresholding (Beck and Teboulle, 2009; Blumensath and Davies, 2008).", "startOffset": 65, "endOffset": 119}, {"referenceID": 11, "context": "Variants of these algorithms find practical relevance in numerous disparate application domains, including feature selection (Cotter and Rao, 2002; Figueiredo, 2002), outlier removal (Cand\u00e8s and Tao, 2005; Ikehata et al.", "startOffset": 125, "endOffset": 165}, {"referenceID": 15, "context": "Variants of these algorithms find practical relevance in numerous disparate application domains, including feature selection (Cotter and Rao, 2002; Figueiredo, 2002), outlier removal (Cand\u00e8s and Tao, 2005; Ikehata et al.", "startOffset": 125, "endOffset": 165}, {"referenceID": 7, "context": "Variants of these algorithms find practical relevance in numerous disparate application domains, including feature selection (Cotter and Rao, 2002; Figueiredo, 2002), outlier removal (Cand\u00e8s and Tao, 2005; Ikehata et al., 2012), compressive sensing (Donoho, 2006), and source localization (Baillet et al.", "startOffset": 183, "endOffset": 227}, {"referenceID": 21, "context": "Variants of these algorithms find practical relevance in numerous disparate application domains, including feature selection (Cotter and Rao, 2002; Figueiredo, 2002), outlier removal (Cand\u00e8s and Tao, 2005; Ikehata et al., 2012), compressive sensing (Donoho, 2006), and source localization (Baillet et al.", "startOffset": 183, "endOffset": 227}, {"referenceID": 12, "context": ", 2012), compressive sensing (Donoho, 2006), and source localization (Baillet et al.", "startOffset": 29, "endOffset": 43}, {"referenceID": 2, "context": ", 2012), compressive sensing (Donoho, 2006), and source localization (Baillet et al., 2001; Malioutov et al., 2005) among many others.", "startOffset": 69, "endOffset": 115}, {"referenceID": 24, "context": ", 2012), compressive sensing (Donoho, 2006), and source localization (Baillet et al., 2001; Malioutov et al., 2005) among many others.", "startOffset": 69, "endOffset": 115}, {"referenceID": 16, "context": "Although seemingly unrelated at first glance, the layers of a deep neural network (DNN) can be viewed as iterations of some algorithm that have been unfolded into a network structure (Gregor and LeCun, 2010; Hershey et al., 2014).", "startOffset": 183, "endOffset": 229}, {"referenceID": 19, "context": "Although seemingly unrelated at first glance, the layers of a deep neural network (DNN) can be viewed as iterations of some algorithm that have been unfolded into a network structure (Gregor and LeCun, 2010; Hershey et al., 2014).", "startOffset": 183, "endOffset": 229}, {"referenceID": 16, "context": "This in turn can lead to a dramatically reduced computational burden relative to purely optimization-based approaches, which can require hundreds or even thousands of iterations to sufficiently converge (Gregor and LeCun, 2010; Sprechmann et al., 2015).", "startOffset": 203, "endOffset": 252}, {"referenceID": 27, "context": "This in turn can lead to a dramatically reduced computational burden relative to purely optimization-based approaches, which can require hundreds or even thousands of iterations to sufficiently converge (Gregor and LeCun, 2010; Sprechmann et al., 2015).", "startOffset": 203, "endOffset": 252}, {"referenceID": 16, "context": "For example, (Gregor and LeCun, 2010) promotes a soft-threshold function inspired by and iterative shrinkage-thresholding algorithm (ISTA) for minimizing the `1-norm, a well-known convex approximation to the canonical `0 norm sparsity penalty from (1).", "startOffset": 13, "endOffset": 37}, {"referenceID": 27, "context": "In contrast, (Sprechmann et al., 2015) advocates a wider class of functions derived from proximal operators (Parikh and Boyd, 2014).", "startOffset": 13, "endOffset": 38}, {"referenceID": 31, "context": "Finally, it has also been suggested that replacing typically continuous activation functions with hard-threshold operators may lead to sparser representations (Wang et al., 2015).", "startOffset": 159, "endOffset": 178}, {"referenceID": 5, "context": "Iterative hard-thresholding (IHT) attempts to minimize (2) using what can be viewed as computationally-efficient projected gradient iterations (Blumensath and Davies, 2009).", "startOffset": 143, "endOffset": 172}, {"referenceID": 5, "context": "In the context of IHT, it has been shown (Blumensath and Davies, 2009) that if y = \u03a6x\u2217, with \u2016x\u20160 \u2264 k and \u03b43k[\u03a6] < 1/ \u221a 32, then at iteration t of (5)", "startOffset": 41, "endOffset": 70}, {"referenceID": 8, "context": "Moreover, it can be shown that this x\u2217 is also the unique, optimal solution to (1) (Cand\u00e8s et al., 2006).", "startOffset": 83, "endOffset": 104}, {"referenceID": 6, "context": "Other values of \u03bc or even a positive definite matrix, adaptively chosen, can lead to a faster convergence rate (Blumensath and Davies, 2010).", "startOffset": 111, "endOffset": 140}, {"referenceID": 7, "context": "While dictionaries with columns drawn independently and uniformly from the surface of a unit hypersphere3 will satisfy this condition with high probability provided k is small enough (Cand\u00e8s and Tao, 2005), for many/most practical problems of interest we cannot rely on this type of IHT recovery guarantee.", "startOffset": 183, "endOffset": 205}, {"referenceID": 16, "context": "To see this, note that from a qualitative standpoint it is quite clear that the iterations of sparsity-promoting algorithms like IHT resemble the layers of neural networks (Gregor and LeCun, 2010).", "startOffset": 172, "endOffset": 196}, {"referenceID": 1, "context": "For example, in the extreme case if r = n\u2212 1, then columns of \u00c3 will be reduced to a n\u2212 r = 1 dimensional subspace, and no RIP conditions can possibly hold (see (Bah and Tanner, 2010) for details of how the RIP constant scales with the dimensions of Gaussian iid matrices).", "startOffset": 161, "endOffset": 183}, {"referenceID": 0, "context": "Given that multiplying a gradient by a positive-definite, symmetric matrix guarantees a descent direction is preserved, the inclusion of BB> could be viewed as as a form of natural gradient direction to be learned during training (Amari, 1998).", "startOffset": 230, "endOffset": 243}, {"referenceID": 28, "context": "In spirit, (15) can be viewed as something like a highway network element (Srivastava et al., 2015) or a LSTM cell (Hochreiter and Schmidhuber, 1997), where elements can be turned on and off via a gating mechanism separate from the activation function.", "startOffset": 74, "endOffset": 99}, {"referenceID": 20, "context": ", 2015) or a LSTM cell (Hochreiter and Schmidhuber, 1997), where elements can be turned on and off via a gating mechanism separate from the activation function.", "startOffset": 23, "endOffset": 57}, {"referenceID": 20, "context": "\u2022 The support sets \u03a9(t) on and \u03a9 (t) off allow the network to \u2018remember\u2019 previously learned cluster-level sparsity patterns, in much the same that LSTM gates allow long term dependencies to propagate (Hochreiter and Schmidhuber, 1997) or highway networks (Srivastava et al.", "startOffset": 200, "endOffset": 234}, {"referenceID": 28, "context": "\u2022 The support sets \u03a9(t) on and \u03a9 (t) off allow the network to \u2018remember\u2019 previously learned cluster-level sparsity patterns, in much the same that LSTM gates allow long term dependencies to propagate (Hochreiter and Schmidhuber, 1997) or highway networks (Srivastava et al., 2015) facilitate information flow unfettered to deeper layers.", "startOffset": 255, "endOffset": 280}, {"referenceID": 22, "context": "We deploy this tool, along with batch-normalization (Ioffe and Szegedy, 2015) to aid convergence, for our basic feedforward pipeline.", "startOffset": 52, "endOffset": 77}, {"referenceID": 25, "context": "We replace the non-integrable hard-threshold operator with simple rectilinear (ReLu) units (Nair and Hinton, 2010), which are functionally equivalent to one-sided softthresholding.", "startOffset": 91, "endOffset": 114}, {"referenceID": 16, "context": ", see (Gregor and LeCun, 2010; Sprechmann et al., 2015; Wang et al., 2015).", "startOffset": 6, "endOffset": 74}, {"referenceID": 27, "context": ", see (Gregor and LeCun, 2010; Sprechmann et al., 2015; Wang et al., 2015).", "startOffset": 6, "endOffset": 74}, {"referenceID": 31, "context": ", see (Gregor and LeCun, 2010; Sprechmann et al., 2015; Wang et al., 2015).", "startOffset": 6, "endOffset": 74}, {"referenceID": 13, "context": "Provided the dictionary satisfies minimal assumptions related to a quantity called matrix spark (Donoho and Elad, 2003), x\u2217 will provably represent the maximally sparse feasible solution.", "startOffset": 96, "endOffset": 119}, {"referenceID": 22, "context": ", 2015a) and batch normalization (Ioffe and Szegedy, 2015).", "startOffset": 33, "endOffset": 58}, {"referenceID": 10, "context": "The detailed network structure, which was implemented using MXNet (Chen et al., 2015), can be found in Figure 1.", "startOffset": 66, "endOffset": 85}, {"referenceID": 23, "context": "pattern with exactly n nonzeros is sufficient to produce a unique feasible solution (referred to as a basic feasible solution in the linear programming literature (Luenberger, 1984)).", "startOffset": 163, "endOffset": 181}, {"referenceID": 8, "context": "7 These include standard `1 minimization via ISTA iterations (Cand\u00e8s et al., 2006), IHT (Blumensath and Davies, 2009), an ISTA-based network (Gregor and LeCun, 2010), and an IHT-inspired network (Wang et al.", "startOffset": 61, "endOffset": 82}, {"referenceID": 5, "context": ", 2006), IHT (Blumensath and Davies, 2009), an ISTA-based network (Gregor and LeCun, 2010), and an IHT-inspired network (Wang et al.", "startOffset": 13, "endOffset": 42}, {"referenceID": 16, "context": ", 2006), IHT (Blumensath and Davies, 2009), an ISTA-based network (Gregor and LeCun, 2010), and an IHT-inspired network (Wang et al.", "startOffset": 66, "endOffset": 90}, {"referenceID": 31, "context": ", 2006), IHT (Blumensath and Davies, 2009), an ISTA-based network (Gregor and LeCun, 2010), and an IHT-inspired network (Wang et al., 2015).", "startOffset": 120, "endOffset": 139}, {"referenceID": 31, "context": "In particular, we utilize the socalled HELU\u03c3 function introduced in (Wang et al., 2015), which is a continuous and piecewise linear approximation of the scalar hard-threshold operator given by", "startOffset": 68, "endOffset": 87}, {"referenceID": 34, "context": "For example, when images of an ideal Lambertian surface are obtained under illumination from three known directions, the surface orientation can be uniquely determined using a simple least-squares fit (Woodham, 1980).", "startOffset": 201, "endOffset": 216}, {"referenceID": 35, "context": "a sparse error term (Wu et al., 2010; Ikehata et al., 2012).", "startOffset": 20, "endOffset": 59}, {"referenceID": 21, "context": "a sparse error term (Wu et al., 2010; Ikehata et al., 2012).", "startOffset": 20, "endOffset": 59}, {"referenceID": 34, "context": "Then the resulting measurements, denoted o \u2208 Rq, can be expressed as o = \u03c1Ln, (22) where n \u2208 R3 denotes the true surface normal, each row of L \u2208 Rq\u00d73 defines a lighting direction, and \u03c1 is the diffuse albedo, acting here as a scalar multiplier (Woodham, 1980).", "startOffset": 244, "endOffset": 259}, {"referenceID": 35, "context": "where e is an an unknown sparse vector (Wu et al., 2010; Ikehata et al., 2012).", "startOffset": 39, "endOffset": 78}, {"referenceID": 21, "context": "where e is an an unknown sparse vector (Wu et al., 2010; Ikehata et al., 2012).", "startOffset": 39, "endOffset": 78}, {"referenceID": 21, "context": "Following (Ikehata et al., 2012), we use 32-bit HDR gray-scale images of the object Bunny (256\u00d7256) with foreground masks under different lighting conditions whose directions, or rows of L, are randomly selected from a hemisphere with the object placed at the center.", "startOffset": 10, "endOffset": 32}, {"referenceID": 34, "context": "We compare our method against the baseline least squares estimate from (Woodham, 1980), `1 norm minimization, and a sparse Bayesian learning (SBL) approach specifically developed in (Ikehata et al.", "startOffset": 71, "endOffset": 86}, {"referenceID": 21, "context": "We compare our method against the baseline least squares estimate from (Woodham, 1980), `1 norm minimization, and a sparse Bayesian learning (SBL) approach specifically developed in (Ikehata et al., 2012) for surface normal estimation.", "startOffset": 182, "endOffset": 204}, {"referenceID": 20, "context": "Using a cell design from (Hochreiter and Schmidhuber, 1997), we adopt a two-layer LSTM network with a fixed size of 11 steps.", "startOffset": 25, "endOffset": 59}, {"referenceID": 32, "context": "The later was chosen because it represents an algorithm explicitly designed to handle dictionary correlations (Wipf, 2012).", "startOffset": 110, "endOffset": 122}, {"referenceID": 33, "context": "This modification was introduced because it represents a challenging scenario for many traditional sparse optimization algorithms for technical reasons related to local minima detailed in (Wipf et al., 2011).", "startOffset": 188, "endOffset": 207}, {"referenceID": 7, "context": "Examples include outlier removal (Cand\u00e8s and Tao, 2005; Ikehata et al., 2012), compressive sensing (Donoho, 2006), and source localization (Baillet et al.", "startOffset": 33, "endOffset": 77}, {"referenceID": 21, "context": "Examples include outlier removal (Cand\u00e8s and Tao, 2005; Ikehata et al., 2012), compressive sensing (Donoho, 2006), and source localization (Baillet et al.", "startOffset": 33, "endOffset": 77}, {"referenceID": 12, "context": ", 2012), compressive sensing (Donoho, 2006), and source localization (Baillet et al.", "startOffset": 29, "endOffset": 43}, {"referenceID": 2, "context": ", 2012), compressive sensing (Donoho, 2006), and source localization (Baillet et al., 2001; Malioutov et al., 2005) applications where, once learned, a deep model can produce maximally sparse representations as new input signals arrive.", "startOffset": 69, "endOffset": 115}, {"referenceID": 24, "context": ", 2012), compressive sensing (Donoho, 2006), and source localization (Baillet et al., 2001; Malioutov et al., 2005) applications where, once learned, a deep model can produce maximally sparse representations as new input signals arrive.", "startOffset": 69, "endOffset": 115}, {"referenceID": 14, "context": "In contrast, other influential domains such as subspace clustering (Elhamifar and Vidal, 2013) effectively require solving sparse recovery problems with a novel dictionary at each instance such that any attempt to construct a viable training set would be infeasible.", "startOffset": 67, "endOffset": 94}, {"referenceID": 5, "context": "We may then apply Theorem 5 from (Blumensath and Davies, 2009) using this revised system and conclude that", "startOffset": 33, "endOffset": 62}, {"referenceID": 5, "context": "This allows us to apply Theorem 5 from (Blumensath and Davies, 2009), from which we can infer that after at most", "startOffset": 39, "endOffset": 68}, {"referenceID": 8, "context": "and all that is required for a unique, maximally sparsity with kx + kc nonzeros is the much weaker inequality \u03b4(2[kx+kc]) ([U ,A(Jc)]) < 1 (Cand\u00e8s et al., 2006).", "startOffset": 139, "endOffset": 160}, {"referenceID": 9, "context": "In particular, using modifications of Theorem 1 from (Carrillo et al., 2011) and the fact that \u03b4(3kx+kc) ([U ,A(Jc)]) < 1/ \u221a 32, it follows that after \u03c4 + t iterations, the A-IHT reconstruction error is bounded by", "startOffset": 53, "endOffset": 76}], "year": 2017, "abstractText": "The iterations of many sparse estimation algorithms are comprised of a fixed linear filter cascaded with a thresholding nonlinearity, which collectively resemble a typical neural network layer. Consequently, a lengthy sequence of algorithm iterations can be viewed as a deep network with shared, hand-crafted layer weights. It is therefore quite natural to examine the degree to which a learned network model might act as a viable surrogate for traditional sparse estimation in domains where ample training data is available. While the possibility of a reduced computational budget is readily apparent when a ceiling is imposed on the number of layers, our work primarily focuses on estimation accuracy. In particular, it is well-known that when a signal dictionary has coherent columns, as quantified by a large RIP constant, then most tractable iterative algorithms are unable to find maximally sparse representations. In contrast, we demonstrate both theoretically and empirically the potential for a trained deep network to recover minimal `0-norm representations in regimes where existing methods fail. The resulting system is deployed on a practical photometric stereo estimation problem, where the goal is to remove sparse outliers that can disrupt the estimation of surface normals from a 3D scene.", "creator": "LaTeX with hyperref package"}}}