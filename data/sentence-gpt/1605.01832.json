{"id": "1605.01832", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "6-May-2016", "title": "Cross-Graph Learning of Multi-Relational Associations", "abstract": "Cross-graph Relational Learning (CGRL) refers to the problem of predicting the strengths or labels of multi-relational tuples of heterogeneous object types, through the joint inference over multiple graphs which specify the internal connections among each type of objects. CGRL is an open challenge in machine learning due to the daunting number of all possible tuples to deal with when the numbers of nodes in multiple graphs are large, and because the labeled training instances are extremely sparse as typical. Existing methods such as tensor factorization or tensor-kernel machines do not work well because of the lack of convex formulation for the optimization of CGRL models, the poor scalability of the algorithms in handling combinatorial numbers of tuples, and/or the non-transductive nature of the learning methods which limits their ability to leverage unlabeled data in training. This paper proposes a novel framework which formulates CGRL as a convex optimization problem, enables transductive learning using both labeled and unlabeled tuples, and offers a scalable algorithm that guarantees the optimal solution and enjoys a linear time complexity with respect to the sizes of input graphs. In our experiments with a subset of DBLP publication records and an Enzyme multi-source dataset, the proposed method successfully scaled to the large cross-graph inference problem, and outperformed other representative approaches significantly. The results of the studies show that a complex computation can be done to train multiple discrete-layer data sets, which are similar to the distributed data sets used to train complex computing systems, for instance by performing the same tasks over multiple nodes. This is the first paper demonstrating the power of computational neural networks to work efficiently with CgRL, and is considered a promising future paper.", "histories": [["v1", "Fri, 6 May 2016 06:15:20 GMT  (862kb,D)", "http://arxiv.org/abs/1605.01832v1", null]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["hanxiao liu", "yiming yang"], "accepted": true, "id": "1605.01832"}, "pdf": {"name": "1605.01832.pdf", "metadata": {"source": "META", "title": "Cross-Graph Learning of Multi-Relational Associations", "authors": ["Hanxiao Liu", "Yiming Yang"], "emails": ["HANXIAOL@CS.CMU.EDU", "YIMING@CS.CMU.EDU"], "sections": [{"heading": "1. Introduction", "text": "Many important problems in multi-source relational learning could be cast as joint learning over multiple graphs about how heterogeneous types of objects interact with\neach other. In literature data analysis, for example, publication records provide rich information about how authors collaborate with each other in a co-authoring graph, how papers are linked in citation networks, how keywords are related via ontology, and so on. The challenging question is about how to combine such heterogeneous information in individual graphs for the labeling or scoring of the multi-relational associations in tuples like (author,paper,keyword), given some observed instances of such tuples as the labeled training set. Automated labeling or scoring of unobserved tuples allows us to discover who have been active in the literature on what areas of research, and to predict who would become influential in which areas in the future. In protein data analysis, as another example, a graph of proteins with pairwise sequence similarities is often jointly studied with a graph of chemical compounds with their structural similarities for the discovery of interesting patterns in (compound,protein) pairs. We call the prediction problem in both examples cross-graph learning of multirelational associations, or simply cross-graph relational learning (CGRL), where the multi-relational associations are defined by the tuples of heterogeneous types of objects, and each object type has its own graph with type-specific relational structure as a part of the provided data. The task is to predict the labels or the scores of unobserved multirelational tuples, conditioned on a relatively small set of labeled instances.\nCGRL is an open challenge in machine learning for several reasons. Firstly, the number of multi-relational tuples grows combinatorially in the numbers of individual graphs and the number of nodes in each graph. How to make crossgraph inference computationally tractable for large graphs is a tough challenge. Secondly, how to combine the internal structures or relations in individual graphs for joint inference in a principled manner is an open question. Thirdly, supervised information (labeled instances) is typically extremely sparse in CGRL due to the very large number of all possible combinations of heterogeneous objects in individual graphs. Consequently, the success of cross-graph learning crucially depends on effectively leveraging the massively available unlabeled tuples (and the latent relations\nar X\niv :1\n60 5.\n01 83\n2v 1\n[ cs\n.L G\n] 6\nM ay\n2 01\n6\namong them) in addition to the labeled training data. In other words, how to make the learning transductive is crucial for the true success of CGRL. Research on transdcutive CGRL has been quite limited, to our knowledge.\nExisting approaches in CGRL or CGRL-related areas can be outlined as those using tensors or graph-regularized tensors, and kernel machines that combine multiple kernels.\nTensor methods have been commonly used for combining multi-source evidence of the interactions among multiple types of objects (Nickel et al., 2011; Rendle et al., 2009; Kolda & Bader, 2009) as the combined evidence can be naturally represented as tuples. However, most of the tensor methods do not explicitly model the internal graph structure for each type of objects, although some of those methods implicitly leverage such information via graph-based regularization terms in their objective function that encourage similar objects within each graph to share similar latent factors (Narita et al., 2012; Cai et al., 2011). A major weakness in such tensor methods is the lack of convexity in their models, which leads to ill-posed optimization problems particularly in high-order scenarios. It has also been observed that tensor factorization models suffer from labelsparsity issue, which is typically severe in CGRL.\nKernel machines have been widely studied for supervised classifiers, where a kernel matrix corresponds to a similarity graph among a single type of objects. Multiple kernels can be combined, for example, by taking the tensor product of each individual kernel matrix, which results in a desired kernel matrix among cross-graph multi-relational tuples. The idea has been explored in relational learning combined with SVMs (Ben-Hur & Noble, 2005), perceptions (Basilico & Hofmann, 2004) or Gaussian process (Yu & Chu, 2008) for two types of objects and is generalizable to the multi-type scenario of CGRL. Although being generic, the complexity of such kernel-based methods grows exponentially in the number of individual kernels (graphs) and the size of each individual graph. As a result, kernel machines suffer from poor scalability in general. In addition, kernel machines are purely supervised (not for transductive learning), i.e., they cannot leverage the massive number of available non-observed tuples induced from individual graphs and the latent connections among them. Those limitations make existing kernel methods less powerful for solving the CGRL problem in large scale and under severely datasparse conditions.\nIn this paper, we propose a novel framework for CGRL which can be characterized as follows: (i) It uses graph products to map heterogeneous sources of information and the link structures in individual graphs onto a single homogeneous graph; (ii) It provides a convex formulation and approximation of the CGRL problem that ensure robust optimization and efficient computation; and (iii) It en-\nables transductive learning in the form of label propagation over the induced homogeneous graph so that the massively available non-observed tuples and the latent connections among them can play an important role in effectively addressing the label-sparsity issue.\nThe proposed framework is most related to (Liu & Yang, 2015), where the authors formulated graph products for learning the edges of a bipartite graph. Our new framework is fundamentally different in two aspects. First, our new formulation and algorithms allow the number of individual graphs to be greater than two, while method in (Liu & Yang, 2015) is only applicable to two graphs. Secondly, the algorithms in (Liu & Yang, 2015) suffer from cubic complexity over the graphs sizes (quadratic by using a non-convex approximation), while our new algorithm enjoys both the convexity of the formulation and the low time complexity which is linear over the graph sizes.\nThe paper is organized as follows: Section 2 shows how cross-graph multi-relations can be embedded into the vertex space of a homogeneous graph. Section 3 describes how efficient label propagation among multi-relations can be carried out in such space with approximation. We discuss our optimization algorithm in Section 4 and provide empirical evaluations over real-world datasets in Section 5."}, {"heading": "2. The Proposed Method", "text": "We introduce our notation in 2.1 and the notion of graph product (GP) in 2.2. We then narrow down to a specific GP family with desirable computational properties in 2.2, and finally propose our GP-based optimization objective in 2.4."}, {"heading": "2.1. Notations", "text": "We are given J heterogeneous graphs where the j-th graph contains nj vertices and is associated with an adjacency matrix G(j) \u2208 Rnj\u00d7nj . We use ij to index the ij-th vertex of graph j, and use a tuple (i1, . . . , iJ) to index each multi-relation across the J graphs. The system predictions over all possible \u220fJ j=1 nj multi-relations is summarized in an order-J tensor f \u2208 Rn1\u00d7\u00b7\u00b7\u00b7\u00d7nJ , where fi1,i2,...,iJ corresponds to the prediction about tuple (i1, . . . , iJ). Denote by \u2297 the Kronecker (Tensor) product. We use\u2297J j=1 xj (or simply \u2297 j xj) as the shorthand for x1\u2297\u00b7 \u00b7 \u00b7\u2297 xJ . Denote by\u00d7j the j-mode product between tensors. We refer the readers to (Kolda & Bader, 2009) for a thorough introduction about tensor mode product.\nP ( \ufe38 \ufe37\ufe37 \ufe38\nG(1)\n, \ufe38 \ufe37\ufe37 \ufe38 G(2) , \ufe38 \ufe37\ufe37 \ufe38 G(3)\n) =\nFigure 1. Graph product ofG(1),G(2) andG(3). Each vertex in the resulting graph P\n( G(1),G(2),G(3) ) corresponds to a multi-relation\nacross the original graphs. E.g., vertex 3.II.B in P corresponds to multi-relation (3,II,B) across G(1), G(2) and G(3)."}, {"heading": "2.2. Graph Product", "text": "In a nutshell, graph product (GP) 1 is a mapping from each cross-graph multi-relation to each vertex in a new graph P , whose edges encode similarities among the multi-relations (illustrated in Fig. 1). A desirable property of GP is it provides a natural reduction from the original multi-relational learning problem over heterogeneous information sources (Task 1) to an equivalent graph-based learning problem over a homogeneous graph (Task 2).\nTask 1. Given J graphs G(1), . . . , G(J) with a small set of labeled multi-relations O = {(i1, . . . , iJ)}, predict labels of the unlabeled multi-relations.\nTask 2. Given the product graph P ( G(1), . . . , G(J) ) with a small set of labeled vertices O = {(i1, . . . , iJ)}, predict labels of its unlabeled vertices."}, {"heading": "2.3. Spectral Graph Product", "text": "We define a parametric family of GP operators named the spectral graph product (SGP), which is of particular interest as it subsumes the well-known Tensor GP and Cartesian GP (Table 1), is well behaved (Theorem 1) and allows efficient optimization routines (Section 3).\nLet \u03bb(j)ij and v (j) ij\nbe the ij-th eigenvalue and eigenvector for the graph j, respectively. We construct SGP by defining the eigensystem of its adjacency matrix based on the provided J heterogeneous eigensystems of G(1), . . . , G(J).\nDefinition 1. The SGP of G(1), . . . , G(J) is a graph consisting of \u220f j nj vertices, with its adjacency matrix P\u03ba :=\nP\u03ba ( G(1), . . . , G(J) ) defined by the following eigensystem{\n\u03ba ( \u03bb (1) i1 , . . . , \u03bb (J) iJ ) , \u2297 j v (j) ij } i1,...,iJ\n(1)\nwhere \u03ba is a pre-specified nonnegative nondecreasing function over \u03bb(j)ij ,\u2200j = 1, 2, . . . , J .\n1 While traditional GP only applies to two graphs, we generalize it to the case of multiple graphs (Section 2.3).\nIn other words, the (i1, . . . , iJ)-th eigenvalue of P\u03ba is defined by coupling the \u03bb(1)i1 , . . . , \u03bb (J) iJ\nwith function \u03ba, and the (i1, . . . , iJ)-th eigenvector of P\u03ba is defined by coupling v(1)i1 , . . . , v (J) iJ via tensor (outer) product.\nRemark 1. If each individual { v (j) ij }nj ij=1 forms an orthog-\nonal basis in Rnj , \u2200j \u2208 1, . . . , J , then {\u2297\nj v (j) ij } i1,...,iJ\nforms an orthogonal basis in R \u220fJ j=1 nj .\nIn the following example we introduce two special kinds of SGPs, assuming J = 2 for brevity. Higher-order cases are later summarized in Table 1.\nExample 1. Tensor GP defines \u03ba(\u03bbi1 , \u03bbi2) = \u03bbi1\u03bbi2 , and is equivalent to Kronecker product: PTensor ( G(1), G(2) ) =\u2211\ni1,i2 (\u03bbi1\u03bbi2) ( v (1) i1 \u2297 v(2)i2 )( v (1) i1 \u2297 v(2)i2 )> \u2261 G(1)\u2297G(2). Cartesian GP defines \u03ba(\u03bbi1 , \u03bbi2) = \u03bbi1 + \u03bbi2 , and is equivalent to the Kronecker sum: PCartesian ( G(1), G(2) ) =\u2211\ni1,i2 (\u03bbi1+\u03bbi2) ( v (1) i1 \u2297v(2)i2 )( v (1) i1 \u2297v(2)i2 )> \u2261 G(1)\u2295G(2). SGP Type \u03ba ( \u03bb\n(1) i1 , \u00b7 \u00b7 \u00b7 , \u03bb(J)iJ ) [P\u03ba](i1,\u00b7\u00b7\u00b7iJ ),(i\u20321,\u00b7\u00b7\u00b7i\u2032J )\nTensor \u220f j \u03bb (j) ij\n\u220f j G (j)\nij ,i \u2032 j Cartesian \u2211 j \u03bb (j) ij \u2211 j G (j)\nij ,i \u2032 j \u220f j\u2032 6=j \u03b4ij\u2032=i\u2032j\u2032\nTable 1. Tensor GP and Cartesian GP in higher-orders.\nWhile Tensor GP and Cartesian GP provide mechanisms to associate multiple graphs in a multiplicative/additive manner, more complex cross-graph association patterns can be modeled by specifying \u03ba. E.g., \u03ba (\u03bbi1 , \u03bbi2 , \u03bbi3) = \u03bbi1\u03bbi2 + \u03bbi2\u03bbi3 + \u03bbi3\u03bbi1 indicates pairwise associations are allowed among three graphs, but no triple-wise association is allowed as term \u03bbi1\u03bbi2\u03bbi3 is not involved. Including higher order polynomials in \u03ba amounts to incorporating higher-order associations among the graphs, which can be achieved by simply exponentiating \u03ba.\nSince what the product graph P offers is essentially a similarity measure among multi-relations, shuffling the order of input graphs G(1), . . . , G(J) should not affect P\u2019s topological structure. For SGP, this property is guaranteed by the following theorem: Theorem 1 (The Commutative Property). SGP is commutative (up to graph isomorphism) if \u03ba is commutative.\nWe omit the proof. The theorem suggests the SGP family is well-behaved as long as \u03ba is commutative, which is true for both Tensor and Cartesian GPs as both multiplication and addition operations are order-insensitive."}, {"heading": "2.4. Optimization Objective", "text": "It is often more convenient to equivalently write tensor f as a multi-linear map. E.g., when J = 2, tensor (matrix) f \u2208 Rn1\u00d7n2 defines a bilinear map from Rn1 \u00d7 Rn2 to R via f(x1, x2) := x>1 fx2 and we have fi1,i2 = f(ei1 , ei2). Such equivalence is analogous to high-order cases where f defines a multi-linear map from Rn1 \u00d7 \u00b7 \u00b7 \u00b7 \u00d7 RnJ to R.\nTo carry out transductive learning over P\u03ba (Task 2), we inject the structure of the product graph into f via a Gaussian random fields prior (Zhu et al., 2003). The negative loglikelihood of the prior \u2212 log p (f |P\u03ba) is the same (up to constant) as the following squared semi-norm\n\u2016f\u20162P\u03ba = vec(f) >P\u22121\u03ba vec(f) (2)\n= \u2211\ni1,i2,...,iJ\nf ( v (1) i1 , . . . , v (J) iJ )2 \u03ba ( \u03bb (1) i1 , . . . , \u03bb (J) iJ\n) (3) Our optimization objective is therefore defined as\nmin f\u2208Rn1\u00d7\u00b7\u00b7\u00b7\u00d7nJ\n`O (f) + \u03b3\n2 \u2016f\u20162P\u03ba (4)\nwhere `O(\u00b7) is a loss function to be defined later (Section 4), O is the set of training tuples, and \u03b3 is a tuning parameter controlling the strength of graph regularization."}, {"heading": "3. Convex Approximation", "text": "The computational bottleneck for optimization (4) lies in evaluating \u2016f\u20162P\u03ba and its first-order derivative, due to the extremely large size of P\u03ba. In section 3.1, we first identify the computation bottleneck of using the exact formulation, based on which we propose our convex approximation scheme in 3.2 that reduces the time complexity of evaluating the semi-norm \u2016f\u20162P\u03ba from O ((\u2211 j nj )(\u220f j nj )) to\nO (\u220f j dj ) , where dj nj for j = 1, . . . , J ."}, {"heading": "3.1. Complexity of the Exact Formulation", "text": "The brute-force evaluation of \u2016f\u20162P\u03ba according to (3) costs O ((\u220f j nj )2) , as one has to evaluate O (\u220f j nj ) terms\ninside the summation where each term costs O (\u220f j nj ) . However, redundancies exist and the minimum complexity for the exact evaluation is given as follows Proposition 1. The exact evaluation of semi-norm \u2016f\u2016P\u03ba takes O ((\u2211 j nj )(\u220f j nj )) flops.\nProof. Notice that the collection of all numerators in (3), namely [ f ( v (1) i1 , . . . , v (J) iJ )] i1,\u00b7\u00b7\u00b7 ,iJ\n, is a tensor in Rn1\u00d7\u00b7\u00b7\u00b7\u00d7nJ that can be precomputed via((\nf \u00d71 V (1) ) \u00d72 V (2) ) \u00b7 \u00b7 \u00b7 \u00d7J V (J) (5)\nwhere \u00d7j stands for the j-mode product between a tensor in Rn1\u00d7\u00b7\u00b7\u00b7\u00d7nj\u00d7\u00b7\u00b7\u00b7\u00d7nJ and V (j) \u2208 Rnj\u00d7nj . The conclusion follows as the j-th mode product in (5) takes O ( nj \u220f j nj ) flops, and one has to do this for each j = 1, . . . , J . When J = 2, (5) reduces to the multiplication of three matrices V (1) > fV (2) at the complexity of O ((n1 + n2)n1n2)."}, {"heading": "3.2. Approximation via Tucker Form", "text": "Equation (5) implies the key for complexity reduction is to reduce the cost of the j-mode multiplications \u00b7 \u00d7j V (j). Such multiplication costs O ( nj \u220f j nj ) in general, but can be carried out more efficiently if f is structured.\nOur solution is twofold: First, we include only the top-dj eigenvectors in V (j) for each graph G(i), where dj nj . Hence each V (j) becomes a thin matrix in Rnj\u00d7dj . Second, we restrict tensor f to be within the linear span of the top\u220fJ j=1 dj eigenvectors of the product graph P\u03ba\nf = d1,\u00b7\u00b7\u00b7 ,dJ\u2211 k1,\u00b7\u00b7\u00b7 ,kJ=1 \u03b1k1,\u00b7\u00b7\u00b7 ,kJ \u2297 j v (j) kj\n(6)\n= \u03b1\u00d71 V (1) \u00d72 V (2) \u00d73 \u00b7 \u00b7 \u00b7 \u00d7J V (J) (7)\nThe combination coefficients \u03b1 \u2208 Rd1\u00d7\u00b7\u00b7\u00b7\u00d7dJ is known as the core tensor of Tucker decomposition. In the case where J = 2, the above is equivalent to saying f \u2208 Rn1\u00d7n2 is a low-rank matrix parametrized by \u03b1 \u2208 Rd1\u00d7d2 such that f = \u2211 k1,k2 \u03b1k1,k2v (1) k1 v (2) k2 > = V (1)\u03b1V (2) > .\nCombining (6) with the orthogonality property of eigenvectors leads to the fact that f ( v (1) k1 , . . . , v (J) kJ ) = \u03b1k1,\u00b7\u00b7\u00b7 ,kJ . To\nsee this for J = 2, notice f ( v (1) k1 , v (2) k2 ) = v (1) k1 > fv (2) k1 = v (1) k1 > V (1)\u03b1V (2) > v (2) k1\n= e>k1\u03b1ek2 = \u03b1k1,k2 . Therefore the semi-norm in (2) can be simplified as\n\u2016f\u20162P\u03ba = \u2016\u03b1\u2016 2 P\u03ba = d1,\u00b7\u00b7\u00b7 ,dJ\u2211 k1,...,kJ=1\n\u03b12k1,\u00b7\u00b7\u00b7 ,kJ \u03ba ( \u03bb (1) k1 , . . . , \u03bb (J) kJ ) (8) Comparing (8) with (3), the number of inside-summation terms is reduced from O (\u220f j nj ) to O (\u220f j dj ) where\nFigure 2. An illustration of the eigenvectors of G(1), G(2) and P\n( G(1), G(2) ) . We plot leading nontrivial eigenvectors of G(1) and\nG(2) in blue and red curves, respectively, and plot the induced leading nontrivial eigenvectors of P\n( G(1), G(2) ) in 3D. IfG(1) andG(2)\nare symmetrically normalized, their eigenvectors (corresponding to eigenvectors of the graph Laplacian) will be ordered by smoothness w.r.t. the graph structures. As a result, eigenvectors of P ( G(1), G(2) ) will also be ordered by smoothness.\ndj nj . In addition, the cost for evaluating each term inside summation is reduced from O (\u220f j nj ) to O(1).\nDenote by V (j)ij \u2208 R dj the ij-th row of V (j), we obtain the following optimization by replacing f with \u03b1 in (4)\nmin \u03b1\u2208Rd1\u00d7\u00b7\u00b7\u00b7\u00d7dJ\n`O (f) + \u03b3\n2 \u2016\u03b1\u20162P\u03ba\ns.t. f = \u03b1\u00d71 V (1) \u00d72 \u00b7 \u00b7 \u00b7 \u00d7J V (J) (9)\nOptimization above has intuitive interpretations. In principle, it is natural to emphasis bases in f that are \u201csmooth\u201d w.r.t. the manifold structure of P\u03ba, and de-emphasis those that are \u201cnonsmooth\u201d in order to obtain a parsimonious hypothesis with strong generalization ability. We claim this is exactly the role of regularizer (8). To see this, note any nonsmooth basis \u2297 j v (j) kj of P\u03ba is likely to be associated with\nsmall a eigenvalue \u03ba ( \u03bb (1) k1 , . . . , \u03bb (J) kJ ) (illustrated in Fig. 2). The conclusion follows by noticing that \u03b1k1,...,kJ is essentially the activation strength of \u2297 j v (j) kj\nin f (implied by (6)), and that (8) is going to give any \u03b1k1,...,kJ associated with a small \u03ba ( \u03bb (1) k1 , . . . , \u03bb (J) kJ ) a stronger penalty.\n(9) is a convex optimization problem over \u03b1 with any convex `O(\u00b7). Spectral approximation techniques for graphbased learning has been found successful in standard classification tasks (Fergus et al., 2009), which are special cases under our framework when J = 1. We introduce this technique for multi-relational learning, which is particularly desirable as the complexity reduction will be much more significant for high-order cases (J >= 2).\nWhile f in (6) is assumed to be in the Tucker form, other low-rank tensor representation schemes are potentially applicable. E.g., the Candecomp/Parafac (CP) form that fur-\nther restricts \u03b1 to be diagonal, which is more aggressive but substantially less expressive. The Tensor-Train decomposition (Oseledets, 2011) offers an alternative representation scheme in the middle of Tucker and CP, but the resulting optimization problem will suffer from non-convexity."}, {"heading": "4. Optimization", "text": "Let (x)+ = max (0, 1\u2212 x) be the shorthand for hinge loss. We define `O(f) to be the ranking `2-hinge loss\n`O(f) =\n\u2211 (i1, . . . , iJ ) \u2208 O (i\u20321, . . . , i \u2032 J ) \u2208 O\u0304 ( fi1...iJ \u2212 fi\u20321...i\u2032J )2 +\n|O \u00d7 O\u0304| (10)\nwhere O\u0304 is the complement of O w.r.t. all possible multirelations. Eq. (10) encourages the valid tuples in our training set O to be ranked higher than those corrupted ones in O\u0304, and is known to be a surrogate of AUC.\nWe use stochastic gradient descent for optimization as |O| is usually large. In each iteration, a random valid multirelation (i1, . . . , iJ) is uniformly drawn fromO, a random corrupted multirelation (i\u20321, . . . , i \u2032 J) is uniformly drawn from O\u0304. The associated noisy gradient is computed as\n\u2207\u03b1 = \u2202`O \u2202f ( \u2202fi1,...,iJ \u2202\u03b1 \u2212 \u2202fi\u20321,...,i\u2032J \u2202\u03b1 ) + \u03b3\u03b1 \u03ba (11)\nwhere we abuse the notation by defining \u03ba \u2208 Rd1\u00d7\u00b7\u00b7\u00b7\u00d7dJ , \u03bak1,...,kJ := \u03ba ( \u03bb (1) k1 , . . . , \u03bb (J) kJ ) ; is the element-wise di-\nAlgorithm 1: Transductive Learning over Product Graph (TOP) foreach j \u2208 1, . . . , J do{\nv (j) k , \u03bb (j) k }dj k=1 \u2190 APPROX EIGEN(G(j));\nforeach (k1, . . . , kJ) \u2208 [d1]\u00d7 . . . [dJ ] do \u03bak1,...,kJ \u2190 \u03ba(\u03bb (1) k1 , . . . , \u03bb (J) kJ ); \u03b1\u2190 0, Z \u2190 0; while not converge do\n(i1, . . . , iJ) uni\u223c O, (i\u20321, . . . , i\u2032J) uni\u223c O\u0304; fi1,...,iJ \u2190 \u03b1\u00d71 V (1) i1 \u00d72 \u00b7 \u00b7 \u00b7 \u00d7J V (J)iJ ; fi\u20321,...,i\u2032J \u2190 \u03b1\u00d71 V (1) i\u20321 \u00d72 \u00b7 \u00b7 \u00b7 \u00d7J V (J)i\u2032J ; \u03b4 = fi1,...,iJ \u2212 fi\u20321,...,i\u2032J ; if \u03b4 < 1 then \u2207\u03b1 \u2190 2(\u03b4\u2212 1) (\u2297 j V (j) ij \u2212 \u2297 j V (j) i\u2032j ) + \u03b3\u03b1 \u03ba;\nelse \u2207\u03b1 \u2190 \u03b3\u03b1 \u03ba;\nZ \u2190 Z +\u2207 2\u03b1 ; \u03b1\u2190 \u03b1\u2212 \u03b70Z \u2212 1 2 \u2207\u03b1;\nreturn \u03b1\nvision between tensors. The gradient w.r.t. \u03b1 in (11) is\n\u2202fi1,...,iJ \u2202\u03b1\n= \u2202 ( \u03b1\u00d71 V (1)i1 \u00d72 \u00b7 \u00b7 \u00b7 \u00d7J V (J) iJ ) \u2202\u03b1\n(12) = \u2297 j V (j) ij \u2208 Rd1\u00d7...dJ (13)\nEach SGD iteration costs O (\u220f j dj )\nflops, which is independent from n1, n2, . . . , nJ . After obtaining the solution \u03b1\u0302(\u03ba) of optimization (9) for any given SGP P\u03ba, our final predictions in f\u0302(\u03ba) can be recovered via (6).\nFollowing (Duchi et al., 2011), we allow adaptive step sizes for each element in \u03b1. That is, in the t-th iteration we use\n\u03b7 (t) k1,...,kJ = \u03b70 /[\u2211t \u03c4=1\u2207\u03b1 (\u03c4) k1,...,kJ 2] 12 as the step size for\n\u03b1k1,...,kJ , where { \u2207\u03b1(\u03c4)k1,...,kJ }t \u03c4=0\nare historical gradients associated with \u03b1k1,...,kJ and \u03b70 is the initial step size (set to be 1). The strategy is particularly efficient with highly redundant gradients, which is our case where the gradient is a regularized rank-2 tensor, according to (11) and (13).\nIn practice (especially for large J), the computation cost of tensor operations involving \u2297J j=1 V (j) ij \u2208 Rd1,...,dJ is not ignorable even if d1, d2, . . . , dJ are small. Fortunately, such medium-sized tensor operations in our algorithm are highly parallelable over GPU. The pseudocode for our optimization algorithm is summarized in Alg. 1."}, {"heading": "5. Experiments", "text": ""}, {"heading": "5.1. Datasets", "text": "We evaluate our method on real-world data in two different domains: the Enzyme dataset (Yamanishi et al., 2008) for compound-protein interaction and the DBLP dataset of scientific publication records. Fig. 3 illustrates their heterogeneous objects and relational structures.\nThe Enzyme dataset has been used for modeling and predicting drug-target interactions, which contains a graph of 445 chemical compounds (drugs) and a graph of 664 proteins (targets). The prediction task is to label the unknown compound-protein interactions based on both the graph structures and a small set of 2,926 known interactions. The graph of compounds is constructed based on the SIMCOMP score (Hattori et al., 2003), and the graph of proteins is constructed based on the normalized SmithWaterman score (Smith & Waterman, 1981). While both graphs are provided in the dense form, we converted them into sparse kNN graphs where each vertex is connected with its top 1% neighbors.\nAs for the DBLP dataset, we use a subset of 34,340 DBLP publication records in the domain of Artificial Intelligence (Tang et al., 2008), from which 3 graphs are constructed as:\n\u2022 For the author graph (G(1)) we draw an edge between two authors if they have coauthored an overlapping set of papers, and remove the isolated authors using a DFS algorithm. We then obtain a symmetric kNN graph by connecting each author with her top 0.5% nearest neighbors using the count of co-authored papers as the proximity measure. The resulting graph has 5,517 vertices with 17 links per vertex on average.\n\u2022 For the paper graph (G(2)) we connect two papers if both of them cite another paper, or are cited by another paper. Like G(1), we remove isolated papers using DFS and construct a symmetric 0.5%-NN graph. To measure the similarity of any given pair of papers, we represent each paper as a bag-of-citations and compute their cosine similarity. The resulted graph has 11,879 vertices and has an average degree of 50.\n\u2022 For the venue graph (G(3)) we connect two venues if they share similar research focus. The venue-venue similarity is measured by the total number of crosscitations in between, normalized by the size of the two venues involved. The symmetric venue graph has 22 vertices and an average degree of 7.\nTuples in the form of (Author,Paper,Venue) are extracted from the publication records, and there are 15,514 tuples (cross-graph interactions) after preprocessing."}, {"heading": "5.2. Methods for Comparison", "text": "\u2022 Transductive Learning over Product Graph (TOP). The proposed method. We explore the following \u03ba\u2019s for parametrizing the spectral graph product.\nName \u03ba(x, y) (J = 2) \u03ba(x, y, z) (J = 3)\nTensor xy xyz Cartesian x+ y x+ y + z\nExponential ex+y exy+yz+xz\nFlat 1 1\n\u2022 Tensor Factorization (TF) and Graph-regularized TF (GRTF). In TF we factorize f \u2208 Rn1\u00d7\u00b7\u00b7\u00b7\u00d7nJ as a set of dimensionality-reduced latent factors Cd1,\u00d7\u00b7\u00b7\u00b7\u00d7dJ , Un1\u00d7d11 , . . . , UJ \u2208 RnJ\u00d7dJ . In GRTF, we further enhanced the traditional TF by adding graph regularizations to the objective function, which enforce the model to be aware of the context information inG(j)\u2019s (Narita et al., 2012; Cai et al., 2011);\n\u2022 One-class Nearest Neighbor (NN). We score each tuple (i1, . . . , iJ) in the test set with f\u0302(i1, . . . , iJ) = max(i\u20321,...,i\u2032J)\u2208O \u220fJ j=1Giji\u2032j . That is, we assume the\ntuple-tuple similarity can be factorized as the product of vertex-level similarities across different graphs. We experimented with several other similarity measures and empirically found the multiplicative similarity leads to the best overall performance. Note it does\nnot rely on the presence of any negative examples.\n\u2022 Ranking Support Vector Machines (Joachims, 2002) (RSVM). For the task of completing the missing paper in (Author,?,Venue), we use a Learning-toRank strategy by treating (Author,Venue) as the query and Paper as the document to be retrieved. The query feature is constructed by concatenating the eigen-features of Author and Venue, where we define the eigen-feature of vertex ij in graph j as V (j) ij \u2208\nRdj . The feature for each query-document pair is obtained by taking the tensor product of the query feature and document eigen-feature.\n\u2022 Low-rank Tensor Kernel Machines (LTKM). While traditional tensor-based kernel construction methods for tuples suffer from poor scalability. We propose to speedup by replacing each individual kernel with its low-rank approximation before tensor product, leading to a low-rank kernel of tuples which allows more efficient optimization routines.\nFor fair comparison, loss functions for TF, GRTF, RSVM and LTKM are set to be exactly the same as that for TOP, i.e. E.q. (10). All algorithms are trained using a minibatched stochastic gradient descent.\nWe use the same eigensystems (eigenvectors and eigenvalues) of the G(j)\u2019s as the input for TOP, RSVM and LTKM. The number of top-eigenvalues/eigenvectors dj for graph j is chosen such that \u03bb(j)1 , . . . , \u03bb (j) dj\napproximately cover 80% of the total spectral energy ofG(j). With respect to this criterion, we choose d1 = 1, 281, d2 = 2, 170, d3 = 6 for DBLP, and d1 = 150, d2 = 159 for Enzyme."}, {"heading": "5.3. Experiment Setups", "text": "For both datasets, we randomly sample one third of known interactions for training (denoted by O), one third for validation and use the remaining ones for testing. Known interactions in the test set, denoted by T , are treated as positive examples. All tuples not in T , denoted by T\u0304 , are treated as\nnegative. Tuples that are already in O are removed from T\u0304 to avoid misleading results (Bordes et al., 2013).\nWe measure algorithm performance on Enzyme based on the quality of inferred target proteins given each compound, namely by the ability of completing (Compound,?). For DBLP, the performance is measured by the quality of inferred papers given author and venue, namely by the ability of completing (Author,?,Venue). We use Mean Average Prevision (MAP), Area Under the Curve (AUC) and Hits at Top 5 (Hits@5) as our evaluation metrics."}, {"heading": "5.4. Results", "text": "Fig. 4 compares the results of TOP with various parameterizations of the spectral graph product (SGP). Among those, Exponential \u03ba works better on average.\nFigs. 5 and 6 show the main results, comparing TOP (with Exponential \u03ba) with other representative baselines. Clearly, TOP outperforms all the other methods on both datasets in all the evaluation metrics of MAP 2, AUC and Hit@5.\nFig. 7 shows the performance curves of TOP on Enzyme over different model sizes (by varying the dj\u2019s). With a relatively small model size compared with using the full spectrum, TOP\u2019s performance converges to the optimal point.\n2MAP scores for random guessing are 0.014 on Enzyme and 0.00072 on DBLP, respectively."}, {"heading": "6. Concluding Remarks", "text": "The paper presents a novel convex optimization framework for transductive CGRL and a scalable algorithmic solution with guaranteed global optimum and a time complexity that does not depend on the sizes of input graphs. Our experiments on multi-graph data sets provide strong evidence for the superior power of the proposed approach in modeling cross-graph inference and large-scale optimization."}, {"heading": "Acknowledgements", "text": "We thank the reviewers for their helpful comments. This work is supported in part by the National Science Foundation (NSF) under grants IIS-1216282, 1350364, 1546329."}], "references": [{"title": "Unifying collaborative and content-based filtering", "author": ["Basilico", "Justin", "Hofmann", "Thomas"], "venue": "In Proceedings of the twenty-first international conference on Machine learning,", "citeRegEx": "Basilico et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Basilico et al\\.", "year": 2004}, {"title": "Kernel methods for predicting protein\u2013protein interactions", "author": ["Ben-Hur", "Asa", "Noble", "William Stafford"], "venue": "Bioinformatics, 21(suppl 1):i38\u2013i46,", "citeRegEx": "Ben.Hur et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Ben.Hur et al\\.", "year": 2005}, {"title": "Translating embeddings for modeling multi-relational data", "author": ["Bordes", "Antoine", "Usunier", "Nicolas", "Garcia-Duran", "Alberto", "Weston", "Jason", "Yakhnenko", "Oksana"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Bordes et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Bordes et al\\.", "year": 2013}, {"title": "Graph regularized nonnegative matrix factorization for data representation", "author": ["Cai", "Deng", "He", "Xiaofei", "Han", "Jiawei", "Huang", "Thomas S"], "venue": "Pattern Analysis and Machine Intelligence, IEEE Transactions on,", "citeRegEx": "Cai et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Cai et al\\.", "year": 2011}, {"title": "Adaptive subgradient methods for online learning and stochastic optimization", "author": ["Duchi", "John", "Hazan", "Elad", "Singer", "Yoram"], "venue": "The Journal of Machine Learning Research,", "citeRegEx": "Duchi et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Duchi et al\\.", "year": 2011}, {"title": "Semisupervised learning in gigantic image collections", "author": ["Fergus", "Rob", "Weiss", "Yair", "Torralba", "Antonio"], "venue": "In Advances in neural information processing systems,", "citeRegEx": "Fergus et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Fergus et al\\.", "year": 2009}, {"title": "Heuristics for chemical compound matching", "author": ["Hattori", "Masahiro", "Okuno", "Yasushi", "Goto", "Susumu", "Kanehisa", "Minoru"], "venue": "Genome Informatics,", "citeRegEx": "Hattori et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Hattori et al\\.", "year": 2003}, {"title": "Optimizing search engines using clickthrough data", "author": ["Joachims", "Thorsten"], "venue": "In Proceedings of the eighth ACM SIGKDD international conference on Knowledge discovery and data mining,", "citeRegEx": "Joachims and Thorsten.,? \\Q2002\\E", "shortCiteRegEx": "Joachims and Thorsten.", "year": 2002}, {"title": "Tensor decompositions and applications", "author": ["Kolda", "Tamara G", "Bader", "Brett W"], "venue": "SIAM review,", "citeRegEx": "Kolda et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Kolda et al\\.", "year": 2009}, {"title": "Bipartite edge prediction via transductive learning over product graphs", "author": ["Liu", "Hanxiao", "Yang", "Yiming"], "venue": "In Proceedings of The 32nd International Conference on Machine Learning,", "citeRegEx": "Liu et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Liu et al\\.", "year": 2015}, {"title": "Tensor factorization using auxiliary information", "author": ["Narita", "Atsuhiro", "Hayashi", "Kohei", "Tomioka", "Ryota", "Kashima", "Hisashi"], "venue": "Data Mining and Knowledge Discovery,", "citeRegEx": "Narita et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Narita et al\\.", "year": 2012}, {"title": "Identification of common molecular subsequences", "author": ["Smith", "Temple F", "Waterman", "Michael S"], "venue": "Journal of molecular biology,", "citeRegEx": "Smith et al\\.,? \\Q1981\\E", "shortCiteRegEx": "Smith et al\\.", "year": 1981}, {"title": "Arnetminer: extraction and mining of academic social networks", "author": ["Tang", "Jie", "Zhang", "Jing", "Yao", "Limin", "Li", "Juanzi", "Su", "Zhong"], "venue": "In Proceedings of the 14th ACM SIGKDD international conference on Knowledge discovery and data mining,", "citeRegEx": "Tang et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Tang et al\\.", "year": 2008}, {"title": "Prediction of drug\u2013target interaction networks from the integration of chemical and genomic spaces", "author": ["Yamanishi", "Yoshihiro", "Araki", "Michihiro", "Gutteridge", "Alex", "Honda", "Wataru", "Kanehisa", "Minoru"], "venue": "i232\u2013i240,", "citeRegEx": "Yamanishi et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Yamanishi et al\\.", "year": 2008}, {"title": "Gaussian process models for link analysis and transfer learning", "author": ["Yu", "Kai", "Chu", "Wei"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Yu et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Yu et al\\.", "year": 2008}, {"title": "Semi-supervised learning using gaussian fields and harmonic functions", "author": ["Zhu", "Xiaojin", "Ghahramani", "Zoubin", "Lafferty", "John"], "venue": "In ICML,", "citeRegEx": "Zhu et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Zhu et al\\.", "year": 2003}], "referenceMentions": [{"referenceID": 10, "context": "However, most of the tensor methods do not explicitly model the internal graph structure for each type of objects, although some of those methods implicitly leverage such information via graph-based regularization terms in their objective function that encourage similar objects within each graph to share similar latent factors (Narita et al., 2012; Cai et al., 2011).", "startOffset": 329, "endOffset": 368}, {"referenceID": 3, "context": "However, most of the tensor methods do not explicitly model the internal graph structure for each type of objects, although some of those methods implicitly leverage such information via graph-based regularization terms in their objective function that encourage similar objects within each graph to share similar latent factors (Narita et al., 2012; Cai et al., 2011).", "startOffset": 329, "endOffset": 368}, {"referenceID": 15, "context": "To carry out transductive learning over P\u03ba (Task 2), we inject the structure of the product graph into f via a Gaussian random fields prior (Zhu et al., 2003).", "startOffset": 140, "endOffset": 158}, {"referenceID": 5, "context": "Spectral approximation techniques for graphbased learning has been found successful in standard classification tasks (Fergus et al., 2009), which are special cases under our framework when J = 1.", "startOffset": 117, "endOffset": 138}, {"referenceID": 4, "context": "Following (Duchi et al., 2011), we allow adaptive step sizes for each element in \u03b1.", "startOffset": 10, "endOffset": 30}, {"referenceID": 13, "context": "We evaluate our method on real-world data in two different domains: the Enzyme dataset (Yamanishi et al., 2008) for compound-protein interaction and the DBLP dataset of scientific publication records.", "startOffset": 87, "endOffset": 111}, {"referenceID": 6, "context": "The graph of compounds is constructed based on the SIMCOMP score (Hattori et al., 2003), and the graph of proteins is constructed based on the normalized SmithWaterman score (Smith & Waterman, 1981).", "startOffset": 65, "endOffset": 87}, {"referenceID": 12, "context": "As for the DBLP dataset, we use a subset of 34,340 DBLP publication records in the domain of Artificial Intelligence (Tang et al., 2008), from which 3 graphs are constructed as:", "startOffset": 117, "endOffset": 136}, {"referenceID": 10, "context": "In GRTF, we further enhanced the traditional TF by adding graph regularizations to the objective function, which enforce the model to be aware of the context information inG\u2019s (Narita et al., 2012; Cai et al., 2011);", "startOffset": 176, "endOffset": 215}, {"referenceID": 3, "context": "In GRTF, we further enhanced the traditional TF by adding graph regularizations to the objective function, which enforce the model to be aware of the context information inG\u2019s (Narita et al., 2012; Cai et al., 2011);", "startOffset": 176, "endOffset": 215}, {"referenceID": 2, "context": "Tuples that are already in O are removed from T\u0304 to avoid misleading results (Bordes et al., 2013).", "startOffset": 77, "endOffset": 98}], "year": 2016, "abstractText": "Cross-graph Relational Learning (CGRL) refers to the problem of predicting the strengths or labels of multi-relational tuples of heterogeneous object types, through the joint inference over multiple graphs which specify the internal connections among each type of objects. CGRL is an open challenge in machine learning due to the daunting number of all possible tuples to deal with when the numbers of nodes in multiple graphs are large, and because the labeled training instances are extremely sparse as typical. Existing methods such as tensor factorization or tensor-kernel machines do not work well because of the lack of convex formulation for the optimization of CGRL models, the poor scalability of the algorithms in handling combinatorial numbers of tuples, and/or the non-transductive nature of the learning methods which limits their ability to leverage unlabeled data in training. This paper proposes a novel framework which formulates CGRL as a convex optimization problem, enables transductive learning using both labeled and unlabeled tuples, and offers a scalable algorithm that guarantees the optimal solution and enjoys a linear time complexity with respect to the sizes of input graphs. In our experiments with a subset of DBLP publication records and an Enzyme multi-source dataset, the proposed method successfully scaled to the large cross-graph inference problem, and outperformed other representative approaches significantly.", "creator": "LaTeX with hyperref package"}}}