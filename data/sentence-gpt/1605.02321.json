{"id": "1605.02321", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "8-May-2016", "title": "Asymmetric Move Selection Strategies in Monte-Carlo Tree Search: Minimizing the Simple Regret at Max Nodes", "abstract": "The combination of multi-armed bandit (MAB) algorithms with Monte-Carlo tree search (MCTS) has made a significant impact in various research fields. The UCT algorithm, which combines the UCB bandit algorithm with MCTS, is a good example of the success of this combination. The recent breakthrough made by AlphaGo, which incorporates convolutional neural networks with bandit algorithms in MCTS, also highlights the necessity of bandit algorithms in MCTS, and is an important step towards identifying the fundamental causes of neural networks.\n\n\n\n\n\nWhile the MCTS algorithm performs a high-frequency bandit and shows a low-frequency bandit, the performance of the algorithm by the MCTS algorithm is relatively slow. A significant number of participants are currently in the process of obtaining an MCTS trained and trained version of MCTS, and some participants, such as Graphene Project, will need to perform an MCTS training to get a good level of training for their tasks.\nThere are also significant problems with the performance of the MCTS algorithm, because the performance of the MCTS algorithm is often slow and that the MCTS algorithm is not capable of efficiently finding the correct answer, thus providing a better opportunity to identify the correct answer for a given task.\nThe MCTS algorithm offers a solution, namely to generate a highly-efficient bandit, which is highly efficient.\nThere are several factors related to the performance of the MCTS algorithm:\nIt enables it to perform a full-on MCTS training which is highly efficient.\nThe MCTS algorithm is very fast (the algorithm is not able to learn exactly which MCTS algorithm is capable of), because its performance can vary from one to another.\nIn order to perform a MCTS training, the user needs to be familiar with the MCTS algorithm, which provides high-performance (no training time required, in order to improve performance of the MCTS algorithm).\nThe MCTS algorithm, which utilizes MCTS, is very fast, because its performance can vary from one to another. It can perform a full-on MCTS training which is highly efficient. The MCTS algorithm can also perform a full-on MCTS training which is highly efficient. In the case of MCTS, it is difficult to detect the correct answer for the correct answer.\nThe MCTS algorithm also uses a", "histories": [["v1", "Sun, 8 May 2016 13:52:41 GMT  (100kb)", "http://arxiv.org/abs/1605.02321v1", "submitted to the 2016 IEEE Computational Intelligence and Games Conference"]], "COMMENTS": "submitted to the 2016 IEEE Computational Intelligence and Games Conference", "reviews": [], "SUBJECTS": "cs.AI", "authors": ["yun-ching liu", "yoshimasa tsuruoka"], "accepted": false, "id": "1605.02321"}, "pdf": {"name": "1605.02321.pdf", "metadata": {"source": "CRF", "title": "Asymmetric Move Selection Strategies in Monte-Carlo Tree Search: Minimizing the Simple Regret at Max Nodes", "authors": ["Yun-Ching Liu", "Yoshimasa Tsuruoka"], "emails": ["cipherman@logos.t.u-tokyo.ac.jp", "tsuruoka@logos.t.u-tokyo.ac.jp"], "sections": [{"heading": null, "text": "ar X\niv :1\n60 5.\n02 32\n1v 1\n[ cs\n.A I]\n8 M\nay 2\nI. INTRODUCTION\nMonte-Carlo Tree Search (MCTS) has made a significant impact on various fields in AI, especially on the field of computer Go [3]. The key factor to the success of MCTS lies in its combination with bandit algorithms, which solves the multiarmed bandit problem (MAB) [4]. The MAB problem is a problem where the agent needs to decide whether it should act optimally based on current available information (exploitation), or gather more information at the risk of suffering losses incurred by performing suboptimal actions (exploration) [1]. One of the most widely used MCTS variants is the UCT algorithm, which simply applies the UCB algorithm to every node in the tree [5]. The development of MCTS in recent years can be broadly classified into two main directions: one is the integration of knowledge learnt offline, and the other is increasing the effectiveness of the knowledge accumulated online.\nThe integration of offline knowledge was mainly focused on using logistic models to improve the quality of the simulations [13][14]. Recently, a lot of effort has been put into the training of convolutional neural networks and combining them with MCTS in computer Go [17][18]. A breakthrough was made by the program AlphaGo, which essentially combines\nconvolutional neural networks with the PUCB bandit algorithm [16], and has beaten a top human professional player Lee Sedol in a five-game challenge match [18].\nOn the other hand, various investigations in increasing the effectiveness of online knowledge have also been carried out. One of them is using various bandit algorithms with MCTS, especially the bandit algorithms that solve the pure exploration MAB problem [6]. The pure exploration MAB problem is a variant of the MAB problem. Unlike the standard MAB problem, its objective is to identify the optimal action after a number of trials, rather than accumulate as much reward as possible during those trials. The goal of the pure exploration MAB problem can be equivalently formulated as the minimization of simple regret, which is defined as the difference of the expected reward between the true optimal action and the action that has been identified as the optimal action. It has been argued that simple regret bandit algorithms might be better suited to the task of game tree search, since the ultimate goal of game tree search is to find the best possible action [8]. Therefore, various MCTS variants have been proposed based on simple regret bandit algorithms. The SR+CR scheme [8] is an MCTS algorithm that applies a simple regret bandit algorithm on the root node, and the UCB algorithm on all other nodes. The sequential halving on trees (SHOT) algorithm combines the sequential halving algorithm [15], which is a near optimal simple regret bandit algorithm, with MCTS. The Hybrid MCTS (H-MCTS) algorithm [11] first applies the UCB algorithm on each node, and then switches to the sequential halving algorithm if the number of times a node has been visited has exceeded a predetermined threshold. The CCB-MCTS algorithm [12] uses the improved UCB algorithm to regulate the amount of exploration performed by simple regret bandit algorithms.\nHowever, the paradigm of applying bandit algorithms in all MCTS variants is still essentially the same: viewing every node in the game tree as an independent instance of the MAB problem, and applying the same bandit algorithm and heuristics on every node. Although this approach allows MCTS to be applied in general domains other than game-play, it leaves certain properties of the game tree unexploited.\nThe adversarial game tree consists of two types of nodes: min nodes and max nodes. Max nodes and min nodes generally represent the decision of different players in the game tree, and\nit is conventional knowledge in various games that the decision of which strategy to adopt, should be based on which player he or she is. For example, in the game of Go, a komi of 6.5 is given to the Black player, that is the Black player needs to obtain at least 6.5 points more than the White player to win the game. Therefore, the Black player needs to adopt a more aggressive strategy, while the White player can play more conservatively or defensively. The same can also be observed in the game of Chess, where White is generally considered to have the initiative from the start, and hence needs to play more actively, while Black needs to solve its passivity first. Therefore, max nodes and min nodes are intrinsically different from this high-level point of view, and it would be natural to treat them differently, rather than symmetrically.\nSome methods have been proposed to reflect the min-max property of game trees in MCTS, but still essentially treat max nodes and min nodes symmetrically, and apply the same heuristic on every node [9]. The SR+CR scheme differs only the root node from other nodes, rather than between max nodes and min nodes [8].\nIn this paper, we propose that max nodes and min nodes should be treated differently, and one should apply different bandit algorithms for each node type in MCTS. We will develop the Asymmetric-MCTS algorithm, which applies the UCB\u221a\u00b7 algorithm on max nodes and the UCB algorithm on min nodes. We will demonstrate its performance on the game of 9\u00d7 9 Go, 9\u00d7 9 NoGo, and Othello."}, {"heading": "II. PRELIMINARIES", "text": "A key ingredient in the success of MCTS is the application of bandit algorithms. Bandit algorithms are algorithms that solve the MAB problem [1].\nIn the MAB problem, an agent faces K slot machines, or \u201cone-armed bandits\u201d, and the agent can choose to pull one of the slot machines at each play. The chosen slot machine will then produce a reward r \u2208 [0, 1]. The distribution of the reward of each slot machine is unknown to the agent.\nThere are two possible objectives in the MAB problem, and different types of bandit algorithms are required for achieving each objective."}, {"heading": "A. Cumulative Regret Minimization", "text": "The goal of the conventional MAB problem is to accumulate as much reward as possible over a total of T plays. The objective can be equivalently formulated as the minimization of the cumulative regret, which is defined as\nCRT = \u2211T t=1(r \u2217 \u2212 rIt),\nwhere r\u2217 is the expected reward of the optimal arm, and rIt is the reward that the agent received by pulling arm It on play t. A bandit algorithm is considered optimal if it can restrict the increase of cumulative regret to O(log T ) [1].\nThe UCB algorithm [5], which is applied in the UCT algorithm [4], is an optimal bandit algorithm which restricts the growth of cumulative regret to O(K log T\n\u2206 ), where \u2206 is the\ndifference of expected reward between a suboptimal arm and the optimal arm.\nAlgorithm 1 The UCB algorithm [5]\nInitialization: Play each machine once. for t = 1, 2, 3, \u00b7 \u00b7 \u00b7 do\nplay arm ai = arg max i\u2208K\nwi + c \u221a\nlog t ti ,\nwhere wi is the current average reward, ti is the number of times arm ai has been sampled. end for\nThe UCB algorithm, shown in Algorithm 1, maintains an estimated confidence bound of the expected reward of each arm, and the algorithm simply chooses the arm that has the highest upper bound to pull at each play. The UCB algorithm estimates the confidence bound of arm ai at play t as\nUCBi = wi \u00b1 cr \u221a\nlog t ti ,\nwhere wi is the average reward received from ai so far, and ti is the number of times ai has been played up to play t, and cr is a constant. It can be observed that the confidence bound consists of the exploitation term wi, and the exploration term c \u221a\nlog t ti . The width of the confidence bound is determined by the exploration term, and it gradually decreases as the number of times arm ai is played increases, e.g. as ti increases, the bound becomes tighter."}, {"heading": "B. Simple Regret Minimization", "text": "The objective of the pure exploration MAB problem is to identify the arm that has the highest expected reward after a given total amount of T plays [6]. This task can be formally stated as minimizing the simple regret, which is defined as\nSRT = r \u2217 \u2212 rT ,\nwhere r\u2217 is the expected reward of the optimal arm, and rT is the mean reward of the arm that is identified by the agent to be optimal after the T plays.\nSince the goal is to identify which arm is the optimal arm, it is more critical to gather as much information as possible about each arm, and therefore the amount of accumulated reward during these T plays is irrelevant. It has been shown that the minimization of cumulative regret CRT and the minimization of simple regret SRT are two contradicting objectives, i.e., as CRT decreases, SRT will increase at the same time, and vice versa [6]. Therefore, in order to solve the pure exploration MAB problem, a different type of bandit algorithms is needed.\nAlgorithm 2 The UCB\u221a\u00b7 algorithm [8]\nInitialization: Play each machine once. for t = 1, 2, 3, \u00b7 \u00b7 \u00b7 do\nplay arm ai = arg max i\u2208K\nwi + c \u221a\u221a\nt ti ,\nwhere wi is the current average reward, ti is the number of times arm ai has been sampled. end for\nThe UCB\u221a\u00b7 algorithm, shown in Algorithm 2, is a bandit algorithm that restricts the growth of simple regret to\nO((\u2206 exp(\u2212 \u221a T ))K) [8].\nThe algorithmic aspect of the UCB\u221a\u00b7 algorithm is basically the same as the UCB algorithm, as it also maintains an estimated confidence bound for each arm, and chooses the arm with the highest upper bound at each play. The UCB\u221a\u00b7 algorithm defines the confidence bound for arm ai as\nUCBi = wi \u00b1 cs \u221a\u221a\nt ti ,\nwhere wi is the average reward received so far from arm ai, ti is the number of times that ai has been played up to play t, and cs is a constant. As with the UCB algorithm, the confidence bound of the UCB\u221a\u00b7 algorithm also consists of the exploitation term wi and the exploration term c \u221a\u221a\nt ti .\nThe difference between the UCB and the UCB\u221a\u00b7 algorithm lies in the definition of the exploration term. The exploration term for the UCB\u221a\u00b7 algorithm decreases more slowly than that of the UCB algorithm, and hence tends to sample more arms over time than focusing on the arm that currently seems to be optimal."}, {"heading": "III. ASYMMETRIC MONTE-CARLO TREE SEARCH", "text": "MCTS consists of four major steps: selection, expansion, simulation, and backpropagation. Bandit algorithms are mainly applied in the selection phase by viewing each node as an independent instance of the MAB problem, where each child node is a single candidate arm. Currently, the most popular variant of MCTS is the UCT algorithm, in which the UCB algorithm is the applied bandit algorithm.\nAlthough this general MCTS paradigm allows it to be applied in a wide range of domains, it leaves a number of properties of the game tree unexploited."}, {"heading": "A. Concerns on Value Estimation of Different Node Types", "text": "The role of the bandit algorithm on every node of MCTS is to estimate the value of the node and perform selection\naccording to the estimated value. As the search progresses, the estimation value of the nodes also converges. Although the general goal is to obtain a good estimation as fast as possible, it can be observed that different node types in the game tree have different requirements to their estimated values:\n\u2022 Max node: since the max nodes represent the view point of the current decision maker, we need to be more certain about the estimated value of each possible decision. Estimations should also be more cautious, and not overly optimistic.\n\u2022 Min node: since the min nodes represent the reaction of the opponent, it is not necessary to determine the best possible reaction of the opponent. Just a good enough reaction that is sufficient to refute a decision made by the decision maker will do.\nDue to the selection and expansion performed in MCTS, the reward of the MAB problem at each node is non-stationary [7]. For example, consider the binary tree used for constructing a lower bound for the UCT algorithm [7], shown in Fig. 1. The binary tree has the depth of D, and the rightmost path, which is from the root node to the rightmost leaf node, is the optimal path. For a node N at depth d < D on the optimal path, if the left action is chosen, then a reward of D\u2212d\nD is received.\nIn other words, all the leaf nodes of the subtree rooted at N have the value of D\u2212d\nD . If the right action is chosen, the agent\ncan proceed to expand further down the optimal path. At depth D\u22121 of the optimal path, the left action will give the reward 0, and the right action will give the reward 1. Therefore, MCTS will most likely spend the majority of its time expanding the subtrees of the left action along the optimal path, as it seems to be better. Consider the MAB problem at the root node, which has two arms node Na and node Nb. Since the leaf nodes of the subtree rooted at Na all have the value of D\u22121D , the reward produced by Na will most likely be fixed around D\u22121D . However, as the search gradually expand down the optimal path, the reward produced by Nb will most likely be along the sequence\n{D\u22122 D , \u00b7 \u00b7 \u00b7 , D\u22122 D , D\u22123 D , \u00b7 \u00b7 \u00b7 , D\u22123 D , \u00b7 \u00b7 \u00b7 , 1 D , \u00b7 \u00b7 \u00b7 , 1 D , 1},\nAlgorithm 3 Asymmetric-MCTS Algorithm\nfunction ASYMMETRIC-MCTS(Node N ) bestucb \u2190 \u2212\u221e for all child nodes ni of N do\nif ni.t = 0 then ni.ucb\u2190\u221e else if N.type is MAX then\nni.ucb\u2190 n.w + cs \u00b7 \u221a\u221a\nN.t ni.t\nelse ni.ucb\u2190 n.w + cr \u00b7 \u221a\nlogN.t ni.t\nend if end if if bestucb \u2264 ni.ucb then\nbestucb \u2190 ni.ucb nbest \u2190 ni\nend if end for\nif nbest.t = 0 then result\u2190RANDOMSIMULATION(nbest) else if nbest is not expanded then EXPAND(nbest) result\u2190 ASYMMETRIC-MCTS(nbest) end if\nnbest.w\u2190 (nbest.w \u00d7 nbest.t+ result)/(nbest.t+ 1) nbest.t\u2190 nbest.t+ 1 N.t\u2190 N.t+ 1 return result\nend function\ninstead of being more evened out. Therefore, although the distribution of the reward of the MAB problem on each node is fixed and determined by the values of the leaf node, due to the selection and expansion performed in MCTS, the reward of the MAB problem on each node is biased, and hence affect the estimation made by the bandit algorithms.\nConsider the case where a sequence of rewards (r1, r2, r3, \u00b7 \u00b7 \u00b7 , rn) are drawn from distribution N , but due to some sampling bias, the sequence is ordered in a nondecreasing order, that is ri \u2264 rj if i < j. Therefore, the estimated mean reward will be higher than the true mean reward in the early period of the sequence, and hence causing the agent to be too optimistic. Similarly, if the sequence is in a non-increasing order, that is ri \u2265 rj if i > j, then the agent tends to be underestimate the mean in the early stages.\nTherefore, one should choose a bandit algorithm that is most likely to resist over optimistic estimations caused by biased reward to deploy on max nodes, and a bandit algorithm that can adapt itself rapidly to provide a \u201cgood enough\u201d estimation on min nodes."}, {"heading": "B. Different Bandit Algorithms for Different Node Types", "text": "As simple regret and cumulative regret bandit algorithms have different properties, they can be deployed to different node types accordingly to fulfill the requirements on the estimation value of each node type:\nThe Asymmetric-MCTS algorithm, which is shown in Algorithm 3, still retains the four steps in conventional MCTS, namely selection, expansion, simulation, and backpropagation. The main characteristic of the Asymmetric-MCTS is that it applies the UCB\u221a\u00b7 algorithm, which is a simple regret bandit algorithm, on max nodes, and the UCB algorithm, which is a cumulative regret bandit algorithm, on min nodes, as shown in Figure 2."}, {"heading": "IV. EXPERIMENTAL RESULTS", "text": "In this section, we will first demonstrate the effect of biased reward on the UCB and UCB\u221a\u00b7 algorithm. We will then\nproceed to demonstrate the performance of the AsymmetricMCTS algorithm on the game of 9 \u00d7 9 Go, 9 \u00d7 9 Nogo, and Othello. The baseline for all experiments is the plain UCT algorithm. For a direct comparison of the effect of the bandit algorithms, all MCTS algorithms used pure random simulations, and no performance enhancement heuristics were applied. Every experimental result is the average of 2300 games, and each algorithm took turns in playing with Black and White."}, {"heading": "A. Effect of Biased Reward in the MAB problem", "text": "We will first demonstrate how bias in the reward affects the performance of the UCB and UCB\u221a\u00b7 algorithm. In order to enhance the effect of the biased reward, we will examine two extreme cases: the rewards are biased to be produced in ascending order, and descending order.\nThe MAB problem testbed mainly follows the settings specified in Sutton et al. [2]. The results are the average of 2000 randomly generated K-armed bandit problems, with K = 20. A total of 5000 plays were given to each problem. The rewards of each bandit are first generated from a normal (Gaussian) distribution with the mean wi, i \u2208 K , and variance of 1. The mean wi of the bandits were randomly selected from a normal distribution with mean 0 and variance 1. To simulate biased rewards, the rewards are then sorted in ascending order and descending order.\nIt can be observed from Figure 3a and Figure 3b that\nregardless of the order in which the rewards are biased, the UCB algorithm has a higher percentage of pulling the optimal arm than the UCB\u221a\u00b7 algorithm, and hence suggesting the UCB\u221a\u00b7 algorithm tends to distribute its plays more evenly across the candidates. As a result, The UCB algorithm also has lower cumulative regret than the UCB\u221a\u00b7 algorithm in both cases, as shown in Figure 4a and Figure 4b.\nHowever, the UCB\u221a\u00b7 algorithm has a lower simple regret than the UCB algorithm when the rewards are produced in descending order, as shown in Figure 5b. As the UCB\u221a\u00b7 algorithm performs more exploration, it is able to obtain a better estimation of the mean reward of each candidate, and thus can make more informed recommendations, achieving lower simple regret. On the other hand, the extensive explorations performed by the UCB\u221a\u00b7 algorithm cause its estimations to be too conservative and pessimistic, and hence lower the quality of the recommendations, as shown in Figure 5a.\nTherefore, it can be observed that the UCB\u221a\u00b7 algorithm is more conservative in its estimations, and more resistant to situations where it is more likely to make overly optimistic estimations. One the other hand, the UCB algorithm follows closely the change in the reward with high efficiency."}, {"heading": "B. Performance of the Asymmetric-MCTS on 9\u00d7 9 Go", "text": "We will first investigate the performance of the Asymmetric-MCTS on the game of Go played on the 9 \u00d7 9 board, with the komi of 6.5.\n1) Performance of SR+CR scheme: For comparison, we demonstrate the performance of SR+CR scheme on the game of 9 \u00d7 9 Go. The SR+CR scheme applies the UCB\u221a\u00b7 bandit algorithm only on the root node, and the UCB bandit algorithm on all other nodes [8]. Table I shows the win rate of various settings for the constant cs in the UCB\u221a\u00b7 algorithm in the SR+CR scheme algorithms. The best constant setting for the UCB algorithm is cr = 0.4 in the SR+CR scheme and the plain UCT algorithm is c = 0.4. A total of 5000 playouts are given to both algorithms for each move.\nIt can be observed that the SR+CR scheme achieves around 54% with its best constant setting, which is slightly better than the plain UCT algorithm.\n2) Tuning the C constants: We now proceed to find the best settings for the constant cr in the UCB algorithm applied on min nodes, and the constant cs in the the UCB\u221a\u00b7 algorithm applied on the max nodes, in the Asymmetric-MCTS algorithm. We have found the optimal setting as cr = 0.5 and cs = 0.4, and Table II shows the win rate of the Asymmetric-MCTS against various constant settings for the plain UCT algorithm. A total of 5000 playouts are given to both algorithms for each move.\nIt can be observed that even against the best setting c = 0.3 of the plain UCT algorithm, the Asymmetric-MCTS still manages to achieve a win rate of around 57.70%. In comparison to the performance of SR+CR scheme, this result suggests that applying the UCB\u221a\u00b7 algorithm on the max nodes throughout the game tree, instead of only on the root node, can make a difference.\n3) Scalability of Asymmetric-MCTS: We now investigate the scalability of the Asymmetric-MCTS as the total number of playouts increases. The result is shown in Table III. The settings for Asymmetric-MCTS is cr = 0.5 and cs = 0.4, and that for the plain UCT algorithm is set to c = 0.3.\nWe can observe that the Asymmetric-MCTS achieves a very good win rate of around 65% over the plain UCT algorithm when 1000 playouts are given, and keeps the win rate to around 60% as more playouts are given to both algorithms. The results suggest that the Asymmetric-MCTS algorithm has very steady performance on the game of 9\u00d7 9 Go."}, {"heading": "C. Performance of the Asymmetric-MCTS on 9\u00d7 9 NoGo", "text": "We now demonstrate the performance of the AsymmetricMCTS on the game of Nogo. Nogo is a misere variation of the game of Go, in which the first player who has no legal moves other than capturing the stones of the opponent loses.\n1) Performance of SR+CR scheme: As in 9 \u00d7 9 Go, we first demonstrate the performance of the SR+CR scheme on the game of 9\u00d7 9 NoGo for comparison. Table IV shows the win rate of various settings for the constant cs in the SR+CR scheme. The constant setting for the plain UCT algorithm is c = 0.3. A total of 5000 playouts are given to both algorithms for each move.\nWe can observe that SR+CR scheme did extremely well against the plain UCT algorithm, achieving a near 68% win rate against the plain UCT algorithm.\n2) Tuning the C constants: We now proceed to find the best settings for the constants cr and cs the UCB\u221a\u00b7 in the Asymmetric-MCTS algorithm. The optimal setting for the Asymmetric-MCTS algorithm is cr = 0.5 and cs = 0.4. Table V shows the win rate of the Asymmetric-MCTS against various\nconstant settings for the plain UCT algorithm. A total of 5000 playouts are given to both algorithms for each move.\nIt can be observed that the Asymmetric-MCTS algorithm achieves at least a win rate of 62.43% against the plain UCT algorithm. This result suggests that differentiating max nodes and min nodes also produces very good performance, although the SR+CR scheme might be a better choice on the game of 9\u00d7 9 NoGo.\n3) Scalability of Asymmetric-MCTS: We now investigate the scalability of the Asymmetric-MCTS as the total number of playouts increases when applied on 9\u00d79 Nogo. The results are shown in Table VI. The settings for Asymmetric-MCTS is cr = 0.5 and cs = 0.4, and the constant for the plain UCT algorithm is set to c = 0.4.\nIt can be observed that the Asymmetric-MCTS algorithm dominates the plain UCT algorithm from a total of 1000 playouts to 11000 playouts, and the win rate gradually increases to near 66% when 11000 playouts are given to both algorithms. This result suggests that the effect of differentiating max nodes and min nodes will gradually increase with the number of total playouts."}, {"heading": "D. Performance of the Asymmetric-MCTS on Othello", "text": "Finally, we proceed to demonstrate the performance of the Asymmetric-MCTS algorithm on the game of Othello.\n1) Performance of SR+CR scheme: We will first investigate the performance of the SR+CR scheme on Othello for comparison. Table VII shows the win rate of various settings for the constant cs in the UCB\u221a\u00b7 algorithm of the SR+CR scheme. The constant setting for the UCB algorithm in the SR+CR scheme is cr = 0.6 and the plain UCT algorithm is\nc = 0.6. A total of 5000 playouts are given to both algorithms for each move.\nIt can be observed that the SR+CR scheme can produce a best win rate of around 53%, which is slightly better but still around the same level of the plain UCT algorithm.\n2) Tuning the C constants: We will now proceed to find the best settings for the constants cr and cs the UCB\u221a\u00b7 in the Asymmetric-MCTS algorithm. The optimal setting for the Asymmetric-MCTS algorithm is cr = 0.7 and cs = 0.4. Table V shows the win rate of Asymmetric-MCTS against various constant settings for the plain UCT algorithm. A total of 5000 playouts are given to both algorithms for each move.\nIt can be observed that the Asymmetric-MCTS algorithm can only achieve a win rate of around 50% against the plain UCT algorithm. This result suggests that differentiating max nodes and min nodes is not effective on the game of Othello, and is around the same level of performance as the plain UCT algorithm.\n3) Scalability of Asymmetric-MCTS: We will now investigate the scalability of the Asymmetric-MCTS as the total number of playouts increases when applied on Othello. The results are shown in Table IX. The settings for AsymmetricMCTS is cr = 0.7 and cs = 0.4, and the plain UCT algorithm is set to c = 0.4.\nIt can be observed that the performance of the AsymmetricMCTS algorithm does not change with the increase of the number of playouts. The win rate of Asymmetric-MCTS\nalgorithm holds steady around 50%, which is around the same performance level as the plain UCT algorithm."}, {"heading": "V. CONCLUSION", "text": "MCTS has made quite an impact on various fields, and the key to its success lies in the application of bandit algorithms, which solve the MAB problem. In most MCTS variants, the same bandit algorithm and heuristics are applied to every node in the game tree by viewing each node as an independent instance of the MAB problem. The current most dominate variant of MCTS is the UCT algorithm, which applies the UCB bandit algorithm on every node. Although this paradigm has the advantage of allowing MCTS to be applied in a wide spectrum of fields, it leaves a number of properties of the game tree unexploited.\nIn this work, we have proposed that max nodes and min nodes should be treated differently by applying different bandit algorithms according to its intrinsic nature, rather than using the same bandit algorithm throughout the whole tree. We have observed that different node types have different concerns in their estimation value, and the simple regret bandit algorithms seem to fit the requirements of max nodes, and cumulative regret bandit algorithms seem to fulfill the requirement of min nodes.\nThe Asymmetric-MCTS algorithm, which applies the UCB\u221a\u00b7 algorithm on max nodes, and the UCB algorithm on min nodes is proposed based on this observation. The experimental results show that the Asymmetric-MCTS algorithm has a really good performance and scalability on the games of 9\u00d7 9 Go. The Asymmetric-MCTS also did well on the game of 9 \u00d7 9 NoGo, but the SR+CR scheme seems to be a better choice. However, the Asymmetric-MCTS performed only on par with the UCT algorithm on the game of Othello.\nAs the main difference between the Asymmetric-MCTS algorithm and the UCT algorithm lies in the application of the UCB\u221a\u00b7 algorithm on max nodes, and hence the effectiveness of the Asymmetric-MCTS algorithm seems to depend on whether the UCB algorithm is more likely to be too optimistic in its estimations on max nodes. Therefore, it can be suggested from the experimental results that the UCB algorithm may make too optimistic estimations on max nodes in the game of 9\u00d7 9 Go, and on the root node in the game of 9\u00d7 9 Nogo. On the other hand, situations where the UCB algorithm is likely to be too optimistic rarely occurs in Othello.\nApplying bandit algorithms other than the UCB and the UCB\u221a\u00b7 algorithm would be a natural direction for further investigation. Apart from bandit algorithms, most performance enhancement methods and heuristics in MCTS, also treats each\nnode in the game tree as equal [13][14][18]. Therefore, it would be interesting to further investigate the possibility of developing enhancement heuristics according to node types as well."}], "references": [{"title": "Asymptotically efficient adaptive allocation rules", "author": ["T.L. Lai", "H. Robbins"], "venue": "Advances in applied mathematics, vol. 6, issue 1, pp. 4-22, 1985.", "citeRegEx": "1", "shortCiteRegEx": null, "year": 1985}, {"title": "Reinforcement learning: an introduction", "author": ["R.S. Sutton", "A.G. Barto"], "venue": "MIT Press, Cambridge, MA, 1998.", "citeRegEx": "2", "shortCiteRegEx": null, "year": 1998}, {"title": "A survey of monte-carlo tree search methods", "author": ["C.B. Browne", "E. Powley", "D. Whitehouse", "S.M. Lucas", "P.I. Cowling", "P. Rohlfshagen", "S. Tavener", "D. Perez", "S. Samothrakis", "S. Colton"], "venue": "IEEE Transactions on Computational Intelligence and Al in Games, vol. 4, issue 1, pp.1-43, 2012.", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2012}, {"title": "Bandit based monte-carlo planning", "author": ["L. Kocsis", "C. Szepesv\u00e1ri"], "venue": "Proceedings of the 17th European Conference on Machine Learning (ECML\u201906), pp. 282-293, 2006.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2006}, {"title": "Finite-time analysis of the multiarmed bandit problem", "author": ["P. Auer", "N. Cesa-Bianchi", "P. Fischer"], "venue": "Machine Learning, vol. 47, issue 2, pp. 235 - 256, 2002.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2002}, {"title": "Pure exploration in multi-armed bandits problems", "author": ["S. Bubeck", "R. Munos", "G. Stoltz"], "venue": "Proceedings of the 20th International Conference on Algorithmic Learning Theory (ALT", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2009}, {"title": "MCTS based on simple regret", "author": ["D. Tolpin", "S.E. Shimony"], "venue": "Proceedings of the 26th AAAI Conference on Artificial Intelligence (AAAI), pp. 570-576, 2012.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2012}, {"title": "Monte Carlo Tree Search with Heuristic Evaluations using Implicit Minimax Backups", "author": ["M. Lanctot", "M.H.M. Winands", "T. Pepels", "N.R. Sturtevant"], "venue": "Proceedings of the 2014 IEEE Conference on Computational Intelligence and Games (CIG", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2014}, {"title": "Sequential halving applied to trees", "author": ["T. Cazenave"], "venue": "IEEE Transactions on Computational Intelligence and Al in Games, vol. 7, issue 1, pp. 102-105, 2015.", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2015}, {"title": "Minimizing simple and cumulative regret in monte-carlo tree Search", "author": ["T. Pepels", "T. Cazenave", "M.H.M.M.H.M. Winands", "M. Lanctot"], "venue": "Proceedings of Computer Games Workshop at the 21st European Conference on Artificial Intelligence, pp. 1-15, 2014.", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2014}, {"title": "Tusruoka ,\u201cRegulation of Exploration for Simple Regret Minimization in Monte-Carlo Tree Search", "author": ["Y.Y.-C. Liu"], "venue": "Proceedings of the 2015 IEEE Conference on Computational Intelligence and Games (CIG 2015),", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2015}, {"title": "Monte-Carlo Simulation Balancing", "author": ["D. Silver", "G. Tesauro"], "venue": "Proceedings of the 26th Annual International Conference on Machine Learning (ICML\u201909), pp. 945-952, 2009.", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2009}, {"title": "Computing \u201celo ratings\u201d of move patterns in the game of go", "author": ["R. Coulom"], "venue": "ICGA Journal, vol. 30, issue 4, pp. 198-208, 2007.", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2007}, {"title": "Almost optimal exploration in multiarmed bandits", "author": ["Z. Karnin", "T.Koren", "S. Oren"], "venue": "Proceedings of the 30th International Conference on Machine Learning (ICML\u201913), pp. 1238-1246, 2013.", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2013}, {"title": "Multi-armed bandits with episode context", "author": ["C. Rosin"], "venue": "Annals of Mathematics and Artificial Intelligence, vol. 61, issue 3, pp. 203-230, 2011.", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2011}, {"title": "Better Computer Go Player with Neural Network and Long-term Prediction", "author": ["Y. Tian", "Y. Zhu"], "venue": "Proceedings of International Conference on Learning Representations (ICLR),", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2016}, {"title": "Mastering the game of Go with deep neural networks and tree search", "author": ["D. Silver", "A. Huang", "C.J. Maddison", "A. Guez", "L. Sifre", "G. van den Driessche", "J. Schrittwieser", "I. Antonoglou", "V. Panneershelvam", "M. Lanctot", "S. Dieleman", "D. Grewe", "J. Nham", "N. Kalchbrenner", "I. Sutskever", "T. Lillicrap", "M. Leach", "K. Kavukcuoglu", "T. Graepel", "D. Hassabis"], "venue": "Nature, 529, pp. 484-489, 2016.", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2016}], "referenceMentions": [{"referenceID": 2, "context": "Monte-Carlo Tree Search (MCTS) has made a significant impact on various fields in AI, especially on the field of computer Go [3].", "startOffset": 125, "endOffset": 128}, {"referenceID": 3, "context": "The key factor to the success of MCTS lies in its combination with bandit algorithms, which solves the multiarmed bandit problem (MAB) [4].", "startOffset": 135, "endOffset": 138}, {"referenceID": 0, "context": "The MAB problem is a problem where the agent needs to decide whether it should act optimally based on current available information (exploitation), or gather more information at the risk of suffering losses incurred by performing suboptimal actions (exploration) [1].", "startOffset": 263, "endOffset": 266}, {"referenceID": 4, "context": "One of the most widely used MCTS variants is the UCT algorithm, which simply applies the UCB algorithm to every node in the tree [5].", "startOffset": 129, "endOffset": 132}, {"referenceID": 11, "context": "The integration of offline knowledge was mainly focused on using logistic models to improve the quality of the simulations [13][14].", "startOffset": 123, "endOffset": 127}, {"referenceID": 12, "context": "The integration of offline knowledge was mainly focused on using logistic models to improve the quality of the simulations [13][14].", "startOffset": 127, "endOffset": 131}, {"referenceID": 15, "context": "Recently, a lot of effort has been put into the training of convolutional neural networks and combining them with MCTS in computer Go [17][18].", "startOffset": 134, "endOffset": 138}, {"referenceID": 16, "context": "Recently, a lot of effort has been put into the training of convolutional neural networks and combining them with MCTS in computer Go [17][18].", "startOffset": 138, "endOffset": 142}, {"referenceID": 14, "context": "A breakthrough was made by the program AlphaGo, which essentially combines convolutional neural networks with the PUCB bandit algorithm [16], and has beaten a top human professional player Lee Sedol in a five-game challenge match [18].", "startOffset": 136, "endOffset": 140}, {"referenceID": 16, "context": "A breakthrough was made by the program AlphaGo, which essentially combines convolutional neural networks with the PUCB bandit algorithm [16], and has beaten a top human professional player Lee Sedol in a five-game challenge match [18].", "startOffset": 230, "endOffset": 234}, {"referenceID": 5, "context": "One of them is using various bandit algorithms with MCTS, especially the bandit algorithms that solve the pure exploration MAB problem [6].", "startOffset": 135, "endOffset": 138}, {"referenceID": 6, "context": "It has been argued that simple regret bandit algorithms might be better suited to the task of game tree search, since the ultimate goal of game tree search is to find the best possible action [8].", "startOffset": 192, "endOffset": 195}, {"referenceID": 6, "context": "The SR+CR scheme [8] is an MCTS algorithm that applies a simple regret bandit algorithm on the root node, and the UCB algorithm on all other nodes.", "startOffset": 17, "endOffset": 20}, {"referenceID": 13, "context": "The sequential halving on trees (SHOT) algorithm combines the sequential halving algorithm [15], which is a near optimal simple regret bandit algorithm, with MCTS.", "startOffset": 91, "endOffset": 95}, {"referenceID": 9, "context": "The Hybrid MCTS (H-MCTS) algorithm [11] first applies the UCB algorithm on each node, and then switches to the sequential halving algorithm if the number of times a node has been visited has exceeded a predetermined threshold.", "startOffset": 35, "endOffset": 39}, {"referenceID": 10, "context": "The CCB-MCTS algorithm [12] uses the improved UCB algorithm to regulate the amount of exploration performed by simple regret bandit algorithms.", "startOffset": 23, "endOffset": 27}, {"referenceID": 7, "context": "Some methods have been proposed to reflect the min-max property of game trees in MCTS, but still essentially treat max nodes and min nodes symmetrically, and apply the same heuristic on every node [9].", "startOffset": 197, "endOffset": 200}, {"referenceID": 6, "context": "The SR+CR scheme differs only the root node from other nodes, rather than between max nodes and min nodes [8].", "startOffset": 106, "endOffset": 109}, {"referenceID": 0, "context": "Bandit algorithms are algorithms that solve the MAB problem [1].", "startOffset": 60, "endOffset": 63}, {"referenceID": 0, "context": "The chosen slot machine will then produce a reward r \u2208 [0, 1].", "startOffset": 55, "endOffset": 61}, {"referenceID": 0, "context": "A bandit algorithm is considered optimal if it can restrict the increase of cumulative regret to O(log T ) [1].", "startOffset": 107, "endOffset": 110}, {"referenceID": 4, "context": "The UCB algorithm [5], which is applied in the UCT algorithm [4], is an optimal bandit algorithm which restricts the growth of cumulative regret to O( log T \u2206 ), where \u2206 is the difference of expected reward between a suboptimal arm and the optimal arm.", "startOffset": 18, "endOffset": 21}, {"referenceID": 3, "context": "The UCB algorithm [5], which is applied in the UCT algorithm [4], is an optimal bandit algorithm which restricts the growth of cumulative regret to O( log T \u2206 ), where \u2206 is the difference of expected reward between a suboptimal arm and the optimal arm.", "startOffset": 61, "endOffset": 64}, {"referenceID": 4, "context": "Algorithm 1 The UCB algorithm [5]", "startOffset": 30, "endOffset": 33}, {"referenceID": 5, "context": "The objective of the pure exploration MAB problem is to identify the arm that has the highest expected reward after a given total amount of T plays [6].", "startOffset": 148, "endOffset": 151}, {"referenceID": 5, "context": ", as CRT decreases, SRT will increase at the same time, and vice versa [6].", "startOffset": 71, "endOffset": 74}, {"referenceID": 6, "context": "Algorithm 2 The UCB\u00b7 algorithm [8]", "startOffset": 31, "endOffset": 34}, {"referenceID": 6, "context": "O((\u2206 exp(\u2212 \u221a T ))) [8].", "startOffset": 19, "endOffset": 22}, {"referenceID": 1, "context": "[2].", "startOffset": 0, "endOffset": 3}, {"referenceID": 6, "context": "TABLE I: Win Rate of SR+CR scheme [8] against plain UCT algorithm in 9\u00d7 9 Go.", "startOffset": 34, "endOffset": 37}, {"referenceID": 6, "context": "The SR+CR scheme applies the UCB\u00b7 bandit algorithm only on the root node, and the UCB bandit algorithm on all other nodes [8].", "startOffset": 122, "endOffset": 125}, {"referenceID": 6, "context": "TABLE IV: Win Rate of UCB\u00b7 MCTS and SR+CR scheme [8] against plain UCT algorithm in 9\u00d7 9 NoGo.", "startOffset": 49, "endOffset": 52}, {"referenceID": 6, "context": "6 and the plain UCT algorithm is TABLE VII: Win Rate of the SR+CR scheme [8] against plain UCT algorithm on Othello.", "startOffset": 73, "endOffset": 76}, {"referenceID": 11, "context": "Apart from bandit algorithms, most performance enhancement methods and heuristics in MCTS, also treats each node in the game tree as equal [13][14][18].", "startOffset": 139, "endOffset": 143}, {"referenceID": 12, "context": "Apart from bandit algorithms, most performance enhancement methods and heuristics in MCTS, also treats each node in the game tree as equal [13][14][18].", "startOffset": 143, "endOffset": 147}, {"referenceID": 16, "context": "Apart from bandit algorithms, most performance enhancement methods and heuristics in MCTS, also treats each node in the game tree as equal [13][14][18].", "startOffset": 147, "endOffset": 151}], "year": 2016, "abstractText": "The combination of multi-armed bandit (MAB) algorithms with Monte-Carlo tree search (MCTS) has made a significant impact in various research fields. The UCT algorithm, which combines the UCB bandit algorithm with MCTS, is a good example of the success of this combination. The recent breakthrough made by AlphaGo, which incorporates convolutional neural networks with bandit algorithms in MCTS, also highlights the necessity of bandit algorithms in MCTS. However, despite the various investigations carried out on MCTS, nearly all of them still follow the paradigm of treating every node as an independent instance of the MAB problem, and applying the same bandit algorithm and heuristics on every node. As a result, this paradigm may leave some properties of the game tree unexploited. In this work, we propose that max nodes and min nodes have different concerns regarding their value estimation, and different bandit algorithms should be applied accordingly. We develop the Asymmetric-MCTS algorithm, which is an MCTS variant that applies a simple regret algorithm on max nodes, and the UCB algorithm on min nodes. We will demonstrate the performance of the Asymmetric-MCTS algorithm on the game of 9 \u00d7 9 Go, 9\u00d7 9 NoGo, and Othello.", "creator": "gnuplot 4.6 patchlevel 4"}}}