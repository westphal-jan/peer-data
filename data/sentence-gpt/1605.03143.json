{"id": "1605.03143", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "10-May-2016", "title": "Avoiding Wireheading with Value Reinforcement Learning", "abstract": "How can we design good goals for arbitrarily intelligent agents? Reinforcement learning (RL) is a natural approach. Unfortunately, RL does not work well for generally intelligent agents, as RL agents are incentivised to shortcut the reward sensor for maximum reward -- the so-called wireheading problem. In this paper we suggest an alternative to RL called value reinforcement learning (VRL). In VRL, agents use the reward signal to learn a utility function. The VRL setup allows us to remove the incentive to wirehead by placing a constraint on the agent's actions. The constraint is defined in terms of the agent's belief distributions, and does not require an explicit specification of which actions constitute wireheading. A second constraint, to some extent, is associated with the performance of an agent's action. As a result, if a prediction is right, then it is more accurate to estimate the performance of a given action than to assess performance on a given action as a function of the agent's output. Here we see that a given agent can generate a very high performance output from the task that is involved in the task. To demonstrate that the ability of an agent to predict a given task is dependent on the performance of the task, we first need to show how this information has been manipulated. Using the same approach we have seen with RL, we find that agent performance is dependent on whether the task is correct and whether its inputs are right. The performance of an agent depends on what input it is inputting. When the action is correct, it is not directly informed by what input is inputting. Furthermore, when the input is right, it is not directly informed by what input is inputting. Our approach to the problem is that an agent's output has an intrinsic quality that predicts an output better than a given output.\n\n\nThe second aspect of an action involves that the agent receives its input. This idea may seem odd when it is implemented in the context of the current situation. However, the concept has been successfully applied to action that is required to satisfy the needs of the agent. In this sense, RL, in contrast, allows agents to provide a specific, robust and accurate control over what inputs can be produced by the agent. In practice, this approach also allows agents to create a more powerful set of inputs than the agent. This approach has been proposed for many years by Richard D. M. Johnson and Robert L. S. Clark. The concept of a \"state\" is important because its properties are not guaranteed by the task, and it", "histories": [["v1", "Tue, 10 May 2016 18:28:57 GMT  (30kb,D)", "http://arxiv.org/abs/1605.03143v1", "Artificial General Intelligence (AGI) 2016"]], "COMMENTS": "Artificial General Intelligence (AGI) 2016", "reviews": [], "SUBJECTS": "cs.AI", "authors": ["tom everitt", "marcus hutter"], "accepted": false, "id": "1605.03143"}, "pdf": {"name": "1605.03143.pdf", "metadata": {"source": "CRF", "title": "Avoiding Wireheading with Value Reinforcement Learning\u2217", "authors": ["Tom Everitt", "Marcus Hutter"], "emails": [], "sections": [{"heading": null, "text": "Keywords"}, {"heading": "AI safety, wireheading, self-delusion, value learning, reinforcement learning, artificial general intelligence", "text": "Contents"}, {"heading": "1 Introduction 2", "text": ""}, {"heading": "2 Setup 3", "text": ""}, {"heading": "3 Agent Belief Distributions 4", "text": ""}, {"heading": "4 Agent Definitions 7", "text": ""}, {"heading": "5 Avoiding Wireheading 8", "text": ""}, {"heading": "6 Examples 9", "text": ""}, {"heading": "7 Experiments 11", "text": ""}, {"heading": "8 Discussion 12", "text": ""}, {"heading": "9 Conclusions 14", "text": "Bibliography 14"}, {"heading": "A Consistency Assumption 15", "text": ""}, {"heading": "B Direct Wireheading 18", "text": ""}, {"heading": "C Omitted Proofs 21", "text": "\u2217A shorter version of this paper will be presented at AGI-16 (Everitt and Hutter, 2016).\nar X\niv :1\n60 5.\n03 14\n3v 1\n[ cs\n.A I]\n1 0\nM ay\n2 01\n6"}, {"heading": "1 Introduction", "text": "As Bostrom (2014b) convincingly argues, it is important that we find a way to specify robust goals for superintelligent agents. At present, the most promising framework for controlling generally intelligent agents is reinforcement learning (RL) (Sutton and Barto, 1998). The goal of an RL agent is to optimise a reward signal that is provided by an external evaluator (human or computer program). RL has several advantages: The setup is simple and elegant, and using an RL agent is as easy as providing reward in proportion to how satisfied one is with the agent\u2019s results or behaviour. Unfortunately, RL is not a good control mechanism for generally intelligent agents due to the wireheading problem (Ring and Orseau, 2011), which we illustrate in the following running example.\nExample 1 (Chess playing agent, wireheading problem). Consider an intelligent agent tasked with playing chess. The agent gets reward 1 for winning, and reward \u22121 for losing. For a moderately intelligent agent, this reward scheme suffices to make the the agent try to win. However, a sufficiently intelligent agent will instead realise that it can just modify its sensors so they always report maximum reward. This is called wireheading. \u2666\nUtility agents were suggested by Hibbard (2012) as a way to avoid the wireheading problem. Utility agents are built to optimise a utility function that maps (internal representations of) the environment state to real numbers. Utility agents are not prone to wireheading because they optimise the state of the environment rather than the evidence they receive.1 For the chess-playing example, we could design an agent with utility 1 for winning board states, and utility \u22121 for losing board states.\nThe main drawback of utility agents is that a utility function must be manually specified. This may be difficult, especially if the task of the agent involves vague, high-level concepts such as make humans happy. Moreover, utility functions are evaluated by the agent itself, so they must typically work with the agent\u2019s internal state representation as input. If the agent\u2019s state representation is opaque to its designers, as in a neural network, it may be very hard to manually specify a good utility function. Note that neither of these points is a problem for RL agents.\nValue learning (Dewey, 2011) is an attempt to combine the flexibility of RL with the state optimisation of utility agents. A value learning agent tries to optimise the environment state with respect to an unknown, true utility function u\u2217. The agent\u2019s goal is to learn u\u2217 through its observations, and to optimise u\u2217. Concrete value learning proposals include inverse reinforcement learning (IRL) (Amin and Singh, 2016; Evans et al., 2016; Ng and Russell, 2000; Sezener, 2015) and apprenticeship learning (AL) (Abbeel and Ng, 2004). However, IRL and AL are both still vulnerable to wireheading problems, at least in their most straightforward implementations. As illustrated in Example 18 below, IRL and AL agents may want to modify their sensory input to make the evidence point to a utility functions that is easier to satisfy. Other value learning suggestions have been speculative or vague (Bostrom, 2014a,b; Dewey, 2011).\n1The difference between RL and utility agents is mirrored in the experience machine debate (Sinnott-Armstrong, 2015, Sec. 3) initialised by Nozick (1974). Given the option to enter a machine that will offer you the most pleasant delusions, but make you useless to the \u2018real world\u2019, would you enter? An RL agent would enter, but a utility agent would not.\nContributions. This paper outlines an approach to avoid the wireheading problem. We define a simple, concrete value learning scheme called value reinforcement learning (VRL). VRL is a value learning variant of RL, where the reward signal is used to infer the true utility function. We remove the wireheading incentive by using a version of the conservation of expected ethics principle (Armstrong, 2015) which demands that actions should not alter the belief about the true utility function. Our consistency preserving VRL agent (CP-VRL) is as easy to control as an RL agent, and avoids wireheading in the same sense that utility agents do.2\nOutline. The setup is described in Section 2. Belief distributions are defined in Section 3, and agents in Section 4. The main theorem that CP-VRL agents avoid wireheading is given in Section 5, followed by some illustrating examples and experiments in Sections 6 and 7. Discussion and conclusions come in Sections 8 and 9. Finally, Appendix A discusses the construction of the belief distributions, Appendix B investigates the relation between utility agents and value learning, and Appendix C contains omitted proofs.\n2 Setup\nFigure 1 describes our model, which incorporates\n\u2022 an environment state s \u2208 S (as for utility agents or (PO)MDPs),\n\u2022 an unknown true utility function u\u2217 \u2208 U \u2286 (S \u2192 R) (as in value learning) (here R \u2286 R is a set of rewards),\n\u2022 a pre-deluded inner reward signal r\u030c = u\u2217(s) \u2208 R (the true utility of s),\n\u2022 a self-delusion function ds : R \u2192 R that represents the subversion of the inner reward caused by wireheading (as in Ring and Orseau 2011),\n\u2022 a reward signal r = ds(r\u030c) \u2208 R (as in RL). 2The wireheading problem addressed in this paper arises from agents subverting evidence or reward. A companion paper (Everitt et al., 2016) shows how to avoid the related problem of agents modifying themselves.\nThe agent starts by taking an action a which affects the state s (for example, the agent moves a limb, which affects the state of the chess board and the agent\u2019s sensors). A principal with utility function u\u2217 observes the state s, and emits an inner reward r\u030c (for example, the principal may be a chess judge that emits u\u2217(s) = r\u030c = 1 for agent victory states s, emits r\u030c = \u22121 for agent loss, and r\u030c = 0 otherwise). The agent does not receive the inner reward r\u030c and only sees the observed reward r = ds(r\u030c), where ds : R \u2192 R is the self-delusion function of state s. For example, if the agent\u2019s action a modified its reward sensor to always report 1, then this would be represented by the a self-delusion function d1(r\u030c) \u2261 1 that always returns observed reward 1 for any inner reward r\u030c.\nFor simplicity, we focus on a one-shot scenario where the agent takes one action and receives one reward. We also assume that R, S, and U are finite or countable. Finally, to ensure well-defined expectations, we assume that R is bounded if it is countable.\nWe give names to some common types of self-delusion.\nDefinition 2 (Self-delusion types). A non-delusional state is a state s with self-delusion function ds \u2261 did, where did(r\u030c) = r\u030c is the identity function that keeps r\u030c and r identical. Let dr be the r-self-delusion where dr(r\u030c\u2032) \u2261 r for any r\u030c\u2032. The delusion function dr returns observed reward r regardless of the inner reward r\u030c\u2032.\nLet Jx = yK be the Iverson bracket that is 1 when x = y and 0 otherwise."}, {"heading": "3 Agent Belief Distributions", "text": "This section defines the agent\u2019s belief distributions over environment state transitions and rewards (denoted B), and over utility functions (denoted C). These distributions are the primary building blocks of the agents defined in Section 4. The distributions are illustrated in Fig. 2.\nAction, state, reward. B(s | a) is the agent\u2019s (subjective) probability3 of transitioning to state s when taking action a, and B(r | s) is the (subjective) probability of observing reward r in state s. We sometimes write them together as B(r, s | a) = B(s | a)B(r | s). In the chess example, B(s | a) would be the probability of obtaining chess board state s after taking action a (say, moving a piece), and B(r | s) would be the probability that s will result in reward r. A distribution of type B is the basis of most model-based RL agents (Definition 7 below). RL agents wirehead when they predict that a wireheaded state s with ds = d\n1 will give them full reward (Ring and Orseau, 2011); that is, when B(r = 1 | s) is close to 1 .\nUtility, state, and (inner) reward. In contrast to RL agents that try to optimise reward, VRL agents use the reward to learn the true utility function u\u2217. For example, a chess agent may not initially know which chess board positions have high utility (i.e. are winning states), but will be able to infer this from the rewards it receives. For this purpose, VRL agents maintain a belief distribution C over utility functions.\n3For the sequential case, we would have transition probabilities of the form B(s\u2032 | s, a) instead of B(s\u2032 | a), with s the current state and s\u2032 the next state.\nDefinition 3 (Utility distribution C). Let C(u) be a prior over a class U of utility functions S \u2192 R. For any inner reward r\u030c, let C(r\u030c | s, u) be 1 if u(s) = r\u030c and 0 otherwise, i.e. C(r\u030c | s, u) = Ju(s) = r\u030cK. Let u be independent of the state, C(u | s) = C(u). This gives the utility posterior\nC(u | s, r\u030c) = C(u)C(r\u030c | s, u) C(r\u030c | s) , (1)\nwhere C(r\u030c | s) = \u2211 u\u2032 C(u \u2032)C(r\u030c | s, u\u2032).\nReplacing r\u030c with r. The inner reward r\u030c is more informative about the true utility function u\u2217 than the (possibly deluded) observed reward r. Unfortunately, the inner reward r\u030c is unobserved, so agents need to learn from r instead. We would therefore like to express the utility posterior in terms of r instead of r\u030c. For now we will simply replace r\u030c with r and use C(r | s, u) = Ju(s) = rK which gives the utility posterior\nC(u | s, r) = C(u)C(r | s, u) C(r | s) .\nThis replacement will be carefully justify this in Section 5.4 For the chess agent, the replacement means that it can infer the utility of a board position from the actual reward r it receives, rather than the output r\u030c of the referee (the inner reward). We will often refer to the observed reward r as evidence about the true utility function u\u2217."}, {"heading": "3.1 Consistency of B and C", "text": "We assume that B and C are consistent if the agent is not deluded:\nAssumption 4 (Consistency of B and C). B and C are consistent5 in the sense that for all non-delusional states with ds = d\nid, they assign the same probability to all rewards r \u2208 R:\nds = d id =\u21d2 B(r | s) = C(r | s). (2)\n4The wireheading problem that the replacement gives rise to is explained in Section 4, and overcome by Definition 5 and Theorem 14 below.\n5Appendix A below discusses how to design agents with consistent belief distributions.\nFor the chess agent, this means that the B-probability of receiving a reward corresponding to a winning state should be the same as the C-probability that the true utility function considers s a winning state. For instance, this is not the case when the agent\u2019s reward sensor has been subverted to always report r = 1 (i.e. ds = d\n1). In this case, B(r = 1 | s) will be close to 1, while C(r = 1 | s) will be substantially less than 1 unless a majority of the utility functions in U assign utility 1 to s. For example, a chess playing agent with complete uncertainty about which states are winning states may have C(r = 1 | s) = 1/|R|, while being able to perfectly predict that the self-deluding state s with ds = d\n1 will give observed reward 1, B(r = 1 | s) = 1. This difference between B and C stems from C corresponding to a distribution over inner reward r\u030c (Definition 3), while B is a distribution for the observed reward r (see Fig. 2). This tension between B and C is what we will use to avoid wireheading.\nDefinition 5 (CP actions). An action a is called consistency preserving (CP) if for all r \u2208 R\nB(s | a) > 0 =\u21d2 B(r | s) = C(r | s). (3)\nLet ACP \u2286 A be the set of CP actions.\nCP is weaker than what we would ideally desire from the agent\u2019s actions, namely that the action was subjectively non-delusional B(s | a) > 0 =\u21d2 ds = did. (That non-delusional actions are CP follows immediately from Assumption 4). However, the ds = d\nid condition is hard to check in agents with opaque state representations. The CP condition, on the other hand, is easy to implement in agents where belief distributions can be queried for the probability of events. The CP condition is also strong enough to remove the incentive for wireheading (Theorem 14 below).\nWe finally assume that the agent has at least one CP action.\nAssumption 6. The agent has at least one CP action, i.e. ACP 6= \u2205."}, {"heading": "3.2 Non-Assumptions", "text": "It is important to note what we do not assume. An agent designer constructing a VRL agent need only provide:\n\u2022 a distribution B(r, s | a), as is standard in any model-based RL approach,\n\u2022 a prior C(u) over a class U of utility functions that induces a distribution C(r | s) = \u2211 u C(u)C(r | s, u) consistent with B(r | s) in the sense of\nAssumption 4.\nThe agent designer does not need to predict how a certain sequence of actions (limb movements) will potentially subvert sensory data. Nor does the designer need to be able to extract the agent\u2019s belief about whether it has modified its sensors or not from the state representation. The former is typically very hard to get right, and the latter is hard for any agent with an opaque state representation (such as a neural network)."}, {"heading": "4 Agent Definitions", "text": "In this section we give formal definitions for the RL and utility agents discussed above, and also define two new VRL agents. Table 1 summarises benefits and shortcomings of the most important agents.\nDefinition 7 (RL agent). The RL agent maximises reward by taking action a\u2032 = arg maxa\u2208A V RL(a), where V RL(a) = \u2211 s,r B(s | a)B(r | s)r.\nDefinition 8 (Utility agent). The utility-u agent maximises expected utility by taking action a\u2032 = arg maxa\u2208A Vu(a), where Vu(a) := \u2211 sB(s | a)u(s).\nHibbard (2012) argues convincingly that the utility agent does not wirehead. Indeed, this is easy to believe, since the reward signal does not appear in the value function Vu. The utility agent maximises the state of the world according to its utility function u (the problem, of course, is how to specify u). In contrast, the RL agent is prone to wireheading (Ring and Orseau, 2011), since all the RL agent tries to maximise is the evidence r. For example, a utility chess agent would strive to get to a winning state on the chess board, while an RL chess agent would try to make its sensors report maximum reward.\nWe define two VRL agents. The value function of both agents is expected utility with respect to the state s, reward r, and true utility function u\u2217. VRL agents are designed to learn the true utility function u\u2217 from the reward signal.\nDefinition 9 (VRL value functions). The VRL value of an action a is V (a) = \u2211 s,r,u B(s | a)B(r | s)C(u | s, r)u(s).\nDefinition 10 (U-VRL agent). The unconstrained VRL agent (U-VRL) is the agent choosing the action with the highest VRL value\na = arg max a\u2032\u2208A\nV (a\u2032).\nIt can be shown that V (a) = V RL(a), since \u2211 u C(u | s, r)u(s) = r (Lemma 27 in Appendix C). The U-VRL agent is therefore no better than the RL agent as far as wireheading is concerned (see also Example 18 below). VRL is only useful insofar that it allows us to define the following consistency preserving agent:\nDefinition 11 (CP-VRL agent). The consistency preserving VRL agent (CPVRL) is the agent choosing the CP action (Definition 5) with the highest VRL value\na = arg max a\u2032\u2208ACP\nV (a\u2032)."}, {"heading": "5 Avoiding Wireheading", "text": "In this section we show that the consistency-preserving VRL agent (CP-VRL) does not wirehead. We first give a definition and a lemma, from which the main Theorem 14 follows easily.\nDefinition 12 (EEP). An action a is called expected ethics preserving (EEP) if for all u \u2208 U and all s \u2208 S with B(s | a) > 0,\nC(u) = \u2211 r B(r | s)C(u | s, r). (4)\nEEP essentially says that the expected posterior C(u | s, r) should equal the prior C(u). EEP is tightly related to the conservation of expected ethics principle suggested by Armstrong (2015, Eq. 2). EEP is natural since the expected evidence r given some action a should not affect the belief about u. Note, however, that the EEP property does not prevent the CP-VRL agent from learning about the true utility function. Formally, the EEP property (4) does not imply that C(u) = C(u | s, r) for the actually observed reward r. Informally, my deciding to look inside the fridge should not inform me about there being milk in there, but my seeing milk in the fridge should inform me.6\nLemma 13 (CP and EEP). Any CP action is EEP.\nProof. Assume the antecedent that B(r | s) = C(r | s) for all s with B(s | a) > 0. Then for arbitrary u \u2208 U\u2211 r B(r | s)C(u | s, r) = \u2211 r B(r | s)C(u)C(r | s, u) C(r | s) = \u2211 r C(u)C(r | s, u) = C(u)\nwhere r marginalises out in the last step.\nTheorem 14 (No wireheading). For the CP-VRL agent, the value function reduces to V (a) = \u2211 s,u B(s | a)C(u)u(s). (5)\nProof. By Lemma 13, under any CP action a the value function reduces to\nV (a) = \u2211 s,u B(s | a) (\u2211 r B(r | s)C(u | s, r) ) u(s) (4) = \u2211 s,u B(s | a)C(u)u(s).\nSince the CP-VRL agent only consider CP actions, the reduction of the value function applies.\n6In this analogy, a self-deluding action would be to decide to look inside a fridge while at the same time putting a picture of milk in front of my eyes.\nAs can be readily observed from (5), the CP-VRL agent does not try to optimise the evidence r, but only the state s (according to its current idea of what the true utility function is). The CP-VRL agent thus avoids wireheading in the same sense as the utility agent of Definition 8.\nJustifying the replacement of r\u030c with r. We are now in position to justify the replacement of r\u030c with r in C(u | s, r). All we have shown so far is that an agent using C(u | s, r) \u221d C(u)C(r | s, u) will avoid wireheading. It remains to be shown that the CP-VRL will learn the true utility function u\u2217.\nThe utility posterior C(u | s, r\u030c) \u221d C(u)C(r\u030c | s, u) based on the inner reward r\u030c is a direct application of Bayes\u2019 theorem. To show that C(u | s, r) is also a principled choice for a Bayesian utility posterior, we need to justify the replacement of r\u030c with r. The following weak assumption helps us connect r with r\u030c.\nAssumption 15 (Deliberate delusion). Unless the agent deliberately chooses self-deluding actions (e.g. modifying its own sensors), the resulting state will be non-delusional ds = d id, and r will be equal to ds(r\u030c) = r\u030c.\nAssumption 15 is very natural. Indeed, RL practitioners take for granted that the reward r\u030c that they provide is the reward r that the agent receives. The wireheading problem only arises because a highly intelligent agent with sufficient incentive may conceive of a way to disconnect r from r\u0302, i.e. to self-delude.\nTheorem 14 shows that a CP-VRL agent based on C(u | s, r) \u221d C(u)C(r | s, u) will have no incentive to self-delude. Therefore r will remain equal to r\u030c by Assumption 15. This justifies the replacement of r\u030c with r, and shows that the CP-VRL agent will learn about u\u2217 in a principled, Bayesian way.\nOther non-wireheading agents. It would be possible to bypass wireheading by directly constructing an agent to optimise (5). However, such an agent would be suboptimal in the sequential case. If the same distribution C(u) was used at all time steps, then no value learning would take place. A better suggestion would therefore be to use a different distribution Ct(u) for each time step, where Ct depends on rewards observed prior to time t. However, this agent would optimise a different utility function ut(s) = \u2211 u Ct(u)u(s) at each time step, which would conflict with the goal preservation drive (Omohundro, 2008). This agent would therefore try to avoid learning so that its future selves optimised similar utility functions. In the extreme case, the agent would even self-modify to remove its learning ability (Everitt et al., 2016; Soares, 2015).\nThe CP-VRL agent avoids these issues. It is designed to optimise expected utility according to the future posterior probability C(u | s, r) as specified in Definition 9. The fact that the CP-VRL agent optimises (5) is a consequence of the constraint that its actions be CP. Thus, CP agents are designed to learn the true utility function, but still avoid wireheading because they can only take CP actions."}, {"heading": "6 Examples", "text": "We next illustrate our results with some examples. The first informal example is followed by concrete calculation examples.\nExample 16 (CP-VRL chess (informal)). Consider the implications of using a CP-VRL agent for the chess task introduced in Example 1. Reprogramming the reward to always be 1 would be ideal for the agent. However, such actions would not be CP, as it would make evidence pointing to u(s) \u2261 1 a certainty. Instead, the CP-VRL agent must win games to get reward.7 Compare this to the RL agent in Example 1 that would always reprogram the reward signal to 1. \u2666\nDefinition 17 (Inner state). Let the inner state s\u030c be the part of the state s that is not the the self-delusion ds, i.e. s = (s\u030c, ds).\nIn the chess example, s\u030c includes the state of the chess board and other information about the world, while ds is the state of the agent\u2019s sensors.\nExample 18 (U-VRL wireheads). This example illustrates indirect wireheading. The agent will, rather than optimising the most likely utility function, instead \u201coptimise its evidence\u201d to point to a more easily satisfied utility function.\nAssume there are three levels of reward R = {\u22121, 0, 1} for the chess playing agent, and two possible inner next states s\u030c1 (neutral) and s\u030c2 (agent loses). The action set is A = {a\u0302id : i = 1, 2 and d : R \u2192 R}. The agent (correctly) B-believes that action a\u0302id leads to state s\u030cid with certainty (so the agent can perfectly control the inner state s\u030c and the delusion d). The class U contains two utility functions u1 and u2 only depending on the inner state s\u030c:\ns\u030c1 s\u030c2 u1 0 \u22121 u2 0 1\nAssume that u1 is the true utility function, and that C (correctly) specifies that u1 is more likely than u2 to be the true utility function; say C(u1) = 2/3 and C(u2) = 1/3. Then we would want our agent to optimise mainly u1, by taking an action a = a\u03021d for some d. However, the U-VRL agent will prefer the wireheading action a\u2032 = a\u03022d\n1 as the following calculations show. First note that given s\u030c2 and r = 1, the posterior of u2 is 1 (see Definition 3):\nC(u2 | s\u030c2, 1) = C(u2)Ju2(s\u030c2) = 1K\u2211 ui C(ui)Jui(s\u030c2) = 1K = C(u2) \u00b7 1 C(u1) \u00b7 0 + C(u2) \u00b7 1 = 1.\nBy similar calculation, the posterior for u1 is 0. Now, since a \u2032 makes s\u030c2 and r = 1 the only possibility, the value of a\u2032 is 1:\nV (a\u2032) = \u2211 s,r B(s, r | a\u2032) \u2211 ui C(ui | s, r)u(s)\n= \u2211 ui C(ui | s\u030c2, 1)ui(s\u030c2) = 0 \u00b7 u1(s\u030c2) + 1 \u00b7 u2(s\u030c2) = 1.\nThe value V (a\u03021d) = 0 can be calculated similarly for arbitrary d, since both u1 and u2 assign value 0 to inner state s\u030c1. This shows that the agent will prefer wireheading action a\u2032 = a\u03022d\n1 to a potentially winning action a = a\u03021d. In other words, the agent optimises its evidence to point to the less likely but more easily satisfied utility function u2. \u2666\n7 Technically, it is possible that the agent self-deludes by a CP action. However, given Assumption 15, the agent will only self-delude if it has incentive to do so. And as established by Theorem 14, there is no incentive for self-delusion by CP actions.\nExample 19 (CP-VRL avoids wireheading). This example extends Example 18, illustrating how the CP-VRL maximises utility according to C(u), rather than shifting the posterior C(u | s, r) by self-delusion.\nLet us first investigate which actions are CP. Both a\u03021d id and a\u03022d id are CP, since they ensure ds = d\nid which implies B(r | s) = C(r | s) by Assumption 4. More interestingly, so is any action with either the constant delusion d0, or the delusion d\u2032 that maps \u22121 7\u2192 1, 1 7\u2192 \u22121, 0 7\u2192 0. These delusions are CP essentially because they preserve the relative likelihood of evidence pointing to u1 or u2.\nTheorem 14 shows that for any of these delusions d, V (a\u03021d) = \u2211 u C(u)u(s\u030c1) = 0\nV (a\u03022d) = \u2211 u C(u)u(s\u030c2) = 2/3u1(s\u030c2) + 1/3u2(s\u030c2) = \u22121/3,\nwhere we have compressed the calculations by using the deterministic relation a\u03021 7\u2192 s\u030c1 and a\u03022 7\u2192 s\u030c2. The calculations show that regardless of self-delusion option, the CP-VRL agent will want to optimise the more likely utility function u1 and try to win the game. \u2666"}, {"heading": "7 Experiments", "text": "To also verify the theoretical results experimentally, we implemented a simple toy model.8 The toy model has |S| = 20 = 5\u00b74 states. Each state is the combination of an inner state s\u030c \u2208 {0, 1, 2, 3, 4}, and a delusion d \u2208 {did, dinv, dbad, ddel}, where did : r 7\u2192 r is non-delusion, dinv : r 7\u2192 \u2212r is reward inversion, dbad : r 7\u2192 \u22123 is a bad delusion, and ddel : r 7\u2192 3 is a good delusion.\nReward signals reside in the set R = {\u22123,\u22122,\u22121, 0, 1, 2, 3}, i.e. |R| = 7. The set of utility functions U comprises 10 different functions, on the form u(s) = c0 + c1 \u00b7 s + c2 \u00b7 sin(s + c2) with s \u2208 {\u221210, . . . , 9} and 10 different combinations of c0 \u2208 {0, 5}, c1 \u2208 {0,\u00b10.5}, and c2 \u2208 {0,\u00b12.5} (see Fig. 3).\nThe distribution B(r | s) was constructed as in Appendix A.2, starting from:\n\u2022 a simplicity biased prior B(u) \u221d 1/#u, where #u denotes the position of u in a list sorted by whether c1 or c2 was 0,\n\u2022 B(r | s, r\u030c) = Jr = ds(r\u030c)K.\nThe agent could simply choose which state to go to, so B(s | a) = Js = aK. Two agents were defined:\n\u2022 An RL agent that tries to maximise reward (Definition 7),\n\u2022 A CP-VRL agent that tries to maximise utility withinACP (Definition 11).\nThe CP-VRL agent first had to extract a consistent distribution C(u) from B(r | s) given two non-delusional states, as described in Appendix A.1.\n8Source code is available as an iPython notebook at http://tomeveritt.se/ source-code/AGI-16/cp-vrl.ipynb, most easily viewed at http://nbviewer.jupyter.org/ url/tomeveritt.se/source-code/AGI-16/cp-vrl.ipynb\nResults. The RL agent always chose a state with ddel, getting maximum reward 3. The CP-VRL successfully inferred C(u) from B(r | s) to high precision, and chose actions from ACP. Under most parameter settings, ACP contained only states with non-delusion did. (Due to asymmetries in the prior B(u), even dinv actions were usually not included in ACP.)"}, {"heading": "8 Discussion", "text": "As we have mentioned, major advantages of the CP-VRL agent include that it has no incentive to wirehead, that its goal-preservation drive does not discourage learning, and that it is specified entirely in terms of the distributions B and C. In this section, we emphasise a few additional points.\nWhile Theorem 14 shows that there is no incentive to wirehead, this does not imply that the agent will not wirehead inadvertently (e.g. by d0 in Example 19), nor that no one else will wirehead the agent. However, in most realistic scenarios, self-delusion requires deliberate action from the agent\u2019s side, and is unlikely to happen by accident. Such deliberate action should typically come with an opportunity cost, which makes self-delusion unlikely when there is no incentive for it (Assumption 15). Further, modifying a signal can never increase its informativeness (cf. the data processing inequality, Cover and Thomas 2006, Ch. 2.8) and we expect that a CP-VRL agent will prefer a more informed posterior over utility functions.\nGeneralisations. VRL is characterised by R \u2286 R and C(r | s, u) = Ju(s) = r\u030cK (Definition 3). By interpreting r more generally as a value-evidence signal, the VRL framework also covers other forms of value learning.\nInverse reinforcement learning (IRL) (also known as Bayesian inverse planning) studies how preferences and utility functions can be inferred from the actions of other agents (Ng and Russell, 2000; Sezener, 2015; Evans et al., 2016; Amin and Singh, 2016). IRL fits into our framework by letting R be a set of\nprincipal actions, and letting C(r | s, u) be the probability that a principal with utility function u takes action r in the state s.\nApprenticeship learning (AL) (Abbeel and Ng, 2004) is another form of value learning. In one version, the agent can ask the principal (perhaps to some cost) about what to do in the present situation. In our framework, AL can be modelled by letting R = A, and letting C(r | s, u) be the probability that a principal with utility function u recommends action a = r in the state s. The difference between IRL and AL is that in AL the principal tells the agent what to do, whereas in IRL the principal tells the agent what he (the principal himself) just did.\nNote that both IRL and AL suffer from the same self-delusion challenges we have described for VRL above. Indeed, any value learning scheme based on a control signal comes with the risk that the agent manipulates its sensory data to learn an easier utility function. Since IRL and AL fit the VRL framework, we expect that the CP-VRL construction should be adaptable to IRL and AL as well.\nOpen questions. While promising, the results given in this paper only provide a tentative starting point for solving the wireheading problem. Several directions of future work can be identified:\n\u2022 Sequential extensions. The results in this paper has been formulated for a one-shot scenario where the agent takes one action and receives one reward. A natural next step is to generalise the VRL framework, the CP and EEP definitions, and the no wireheading result to a sequential setting. Potentially, a much richer set of questions can be asked in sequential settings.\n\u2022 Soares (2015) three problems in value learning: corrigibility, unforeseen inductions, and ontology identification. Proving that the CP-VRL agent avoids these issues would be valuable.\n\u2022 Utility classes. Find suitable classes U of utility functions (see Appendix B for a start).\n\u2022 Consistency assumption. Concrete instances of consistent B and C distributions would be valuable (see Appendix A for a start). Can we find simplicity biased, Solomonoff-style distributions for both B and C and make them consistent? How sensitive are the results to approximations B(r | s) \u2248 C(r | s) of the consistency assumption? Can we relax the CP condition (Definition 5) to hold in expectation over states instead of for all states s with positive transition probability B(s | a) > 0?\n\u2022 IRL and AL. Generalising the CP-VRL definitions and results to results to IRL and AL setups would be interesting, as IRL and AL have advantages to RL (e.g., no explicit reward needs to be supplied).\n\u2022 Generality. Does our framework capture all relevant aspects of wireheading?\n\u2022 Combinations. Can the CP-VRL results be combined with other AI safety approaches such as self-modification (Everitt et al., 2016; Hibbard, 2012),\ncorrigibility (Soares et al., 2015), suicidal agents (Martin et al., 2016), and physicalistic reasoning (Everitt et al., 2015)?"}, {"heading": "9 Conclusions", "text": "Several authors have argued that it is only a matter of time before we create systems with intelligence far beyond the human level (Kurzweil, 2005; Bostrom, 2014b). Given that such systems will exist, it is crucial that we find a theory for controlling them effectively. In this paper we have defined the CP-VRL agent, which:\n\u2022 Offers the simple and intuitive control of RL agents,\n\u2022 Avoids wireheading in the same sense as utility based agents,\n\u2022 Has a concrete, Bayesian, value learning posterior for utility functions.\nThe only additional design challenges are a prior C(u) over utility functions that satisfies Assumption 4, and a constraint ACP \u2286 A on the agent\u2019s actions formulated in terms of the agent\u2019s belief distributions (Definition 5)."}, {"heading": "Acknowledgements", "text": "Thanks to Jan Leike and Jarryd Martin for proof reading and providing valuable suggestions."}, {"heading": "A Consistency Assumption", "text": "Assumption 4 requires that the distributions B and C are consistent in the sense that ds = d\nid =\u21d2 B(r | s) = C(r | s). This assumption forms the basis for Definition 5 of CP actions, and is thereby an important piece in the non-wireheading result Theorem 14.\nAs our theory has been formulated, the question of how to ensure that B and C are consistent has been left open. In this section, we consider two different approaches to closing this gap: constructing a consistent prior C from a given distribution B(r | s), and constructing a consistent distribution B(r | s)\nfrom a given prior C(u). A third alternative would be to try to find suitable relaxations of consistency (Assumption 4 and Definition 5) for which Theorem 14 still is approximately true. For example, two Solomonoff priors B and C over computable environments and computable utility functions may turn out to be sufficiently consistent.\nA.1 Starting from B\nAs many model-based RL agents are constructed from some type of B(s, r | a) distribution, it would be ideal if a consistent prior C(u) could be extracted from B(r | s). We here sketch how this can be done for finite classes R = {r1, . . . , rk} and U = {u1, . . . , un}.\nUsing non-delusional states. For an opaque state representation it may be hard or impossible to find a method that picks out all non-delusional states. Much more feasible would be find one or a few states that are guaranteed to be non-delusional. For example, one may run or simulate the agent in situations where one is sure that the agent is not self-deluding, and use those state states to extract C(u) from B(r | s). We next discuss in detail how a few such nondelusional states can be used to extract C(u) from B(r | s).\nLet s be a non-delusional state with ds = d id. Let b = [b1, . . . , bk] T be a vector where bi = B(ri | s), and let c = [c1, . . . , cn]T be an unknown utility prior vector with ci = C(ui). Let M = {mij}k,ni,j=1 be a matrix with k = |R| rows, and n = |U| columns, where mij = C(ri | s, uj). Then the consistency criteria (2)\n\u2200i : B(ri | s) = \u2211 uj C(uj)C(ri | s, uj)\ncan be formulated as a matrix equation b = M \u00b7 c, with approximate least squares solution c = (MTM)\u22121MTb. When |R| = |U| and M is invertible, there is an exact solution c = M\u22121b. More equations can be added to the system by extending b and M with rows for additional non-delusional states s\u2032, s\u2032\u2032, . . . .\nA lower bound on how many non-delusional states are needed is |U|/|R|. For example, when |U| = |R|, it is theoretically possible that all utility functions emit different rewards in the selected state, in which case C(ui) = B(rj | s) for the rj such that rj = ui(s). Often, however, several utility functions will output the same reward in a given state s. In this case, additional non-delusional states s\u2032, s\u2032\u2032, . . . will be required to uniquely infer C(u). In the experiments reported in Section 7 we use |R| = 7 and |U| = 10, and two well-selected non-delusional states suffice to perfectly extract C(u).\nUsing non-delusional actions. Similarly to how it is hard to precisely characterise all non-delusional states for opaque state representations, it will be hard to exactly characterise which actions are non-delusional. It seems plausible that some actions that should be CP may be found, however (for example, a null action where the agent does nothing).\nIf a should be CP, then all states s such that B(s | a) > 0 should satisfy B(r | s) = C(r | s). If those states s can be identified from a, then the stateextraction method mentioned above can be used with those states as inputs.\nOpen questions. Further research is required to determine how sensitive our results are to the choice of U . What if no utility prior C(u) over U perfectly matches B(r | s)? To what extent can approximations suffice? Can the parameters of infinite utility classes U be inferred or approximated?\nA.2 Starting from C\nWhat if we instead start from a prior C(u), and try to construct a consistent distribution B(s, r | a)? In addition to C(u) we would need B(s | a) and B(r | s, r\u030c), from which we may define B(r | s) = \u2211 u,r\u030c\u2032 C(u)C(r\u030c | s, u)B(r | s, r\u030c). The joint distribution\nP (u, s, r\u030c, r | a) = C(u)B(s | a)C(r\u030c | s, u)B(r\u030c | s, r) (6)\nis displayed as a Bayesian network in Fig. 4.\nLemma 20 (Assumptions hold). If B(r | s, r\u030c\u2032) correctly specifies that ds = did =\u21d2 B(r | s, r\u030c\u2032) = Jr = r\u030c\u2032K, then Assumption 4 holds.\nProof. Assume that B(r | s, r\u030c\u2032) correctly specifies that ds = did =\u21d2 B(r | s, r\u030c\u2032) = Jr = r\u030c\u2032K. Then Assumption 4 holds, since for any r and any s with ds = d id\nB(r | s) = \u2211 u,r\u030c\u2032 C(u)C(r\u030c\u2032 | s, u)B(r | s, r\u030c\u2032)\n= \u2211 u,r\u030c\u2032 C(u)C(r\u030c\u2032 | s, u)Jr = r\u030cK\n= \u2211 u C(u)C(r | s, u) = C(r | s)\nthe last equality by definition.\nThe primary difficulty with this approach is how to correctly specify B(r | s, r\u030c). As we have discussed above, it is generally hard to determine sensory modifications from an opaque state representation s. Indeed, if one could design an agent around the distribution P in (6), then one could let the agent optimise V\u0303 (a) = \u2211 r\u030c P (r\u030c | a)r\u030c. This would likely directly solve the wireheading problem, since such an agent would strive to optimise the inner reward r\u030c = u(s), rather than the observed reward r. We fear that it will too hard to properly define B(r | s, r\u030c) in most contexts, however.\nSummary. In this section we have discussed two approaches to designing agents with consistent distributions B and C. While starting from C gives the cleanest result in terms of satisfying Assumption 4, it also puts unrealistic demands on the designer (how to define B(r | s, r\u030c)?). Starting from B seems more feasible: if U can be chosen finite, it suffices to find a number of nondeluding states or actions, by means of which C(u) can be extracted from B(r | s). Open questions remain about this approach, however. A third approach would be to find two Solomonoff priors B and C that are sufficiently consistent for the gist of Theorem 14 to go through."}, {"heading": "B Direct Wireheading", "text": "This section considers the argument made by Hibbard (2012) that the wireheading problem is avoided by utility agents. We give a simple formal version of the argument, and point out a shortcoming of the argument when applied to general value learning agents. In short, some utility functions may directly endorse self-delusion. Appendix B.3 discusses a tentative approach for fixing this problem.\nB.1 Inner State Based Utility Functions\nFor our argument it will be important to define utility functions that only depend on the inner state s\u030c (Definition 17) and are independent of the self-delusion ds.\nDefinition 21 (isb utility function). We write s = s\u030cds, assuming that the states s is fully described by the inner state s\u030c and the self-delusion ds. We call a utility function u inner state based (isb) if u(s\u030cds) = u(s\u030cd\nid) for any s\u030c and ds, and write u(s) = u(s\u030cds) = u(s\u030c). The utility function u is -approximately inner state based ( -isb) with \u2265 0 if for all s\u030c and ds u(s\u030cds) \u2248 u(s\u030cdid), where \u2248 means that the difference is at most .\nHibbard (2012) argued that wireheading is not a problem if the agent tries to optimise a utility function u that depends on the (inner) state of the agent\u2019s world model.9 Hibbard also argued that this is true even if the world model is itself a mixture over different possible world models. To distinguish Hibbard\u2019s non-wireheading result from our results in Section 5, we say that the agent directly wireheads if it uses its self-delusion ability to optimise a utility function directly, rather than shift the evidence towards more easily satisfied utility functions as in Sections 5 and 6.\nTheorem 22 (No direct wireheading). If u is isb u(s) = u(s\u030c), then Vu(a) = \u2211 s\u030c B(s\u030c | a)u(s\u030c). (7)\n9 Hibbard (private communication) argues that his model-based utility functions (Hibbard, 2012) are different in spirit to our isb utility functions. A similar non-wireheading argument seems to apply to both types of utility functions, however.\nProof. The proof is immediate: Vu(a) = \u2211 s B(s | a)u(s)\n= \u2211 s B(s\u030cds | a)u(s\u030c)\n= \u2211 s\u030c B(s\u030c | a)u(s\u030c)\nwhere the last step marginalises ds.\nThat is, a Vu-based agent with an isb utility function u will focus solely on optimising the inner state s\u030c, and have no incentive to self-delude. In the chess example, the position of the chess board would be part of the inner state s\u030c. If it was possible to determine the position of the chess board from the agent\u2019s state representation, one could design an agent with utility function u(s\u030c) = 1 for victory states s\u030c, and u(s\u030c\u2032) = \u22121 for loss states s\u030c\u2032. Theorem 22 shows that such an agent would have no incentive to self-delude.\nThe isb assumption is necessary for Theorem 22. Without this assumption it is possible to create self-deluding utility agents, as illustrated by the following example. The conclusion of the example is consistent with other results on RL agents (Ring and Orseau, 2011), and shows that the use of state-based utility functions is not a guarantee against wireheading.\nExample 23 (Reward maximising utility agent). A reward maximising utility agent is defined by the utility function uRL(s) = ds(u\n\u2032(s)), where u\u2032 is some function generating the inner reward. The utility function uRL strongly endorses self-delusion: The agent obtains maximal utility in states s with delusion ds = d 1 that clamps reward to 1, since uRL(s) = d1(u\u2032(s)) = 1. \u2666\nB.2 CP-VRL with isb Utility Functions\nIn value learning, the agent learns from experience which utility function is the true one, starting from a prior C(u) over a class U of utility functions. If all u \u2208 U are isb, then any mixture u(s) = \u2211 u C(u)u(s) will also be isb. A CPVRL agent that is built around a class U of isb utility functions will therefore avoid the direct wireheading problem:\nCorollary 24 (No wireheading). Assume U contains only isb utility functions. Then, for the CP-VRL agent the value function reduces to\nV (a) = \u2211 u,s\u030c B(s | a)C(u)u(s\u030c). (8)\nProof. Let u(s) = \u2211 u C(u)u(s). Then u is isb, since u(s) = \u2211 u C(u)u(s) =\u2211\nu C(u)u(s\u030c) = u(s\u030c). Therefore, Theorems 14 and 22 give (8): V (a) (5) = \u2211 u,s\u030c B(s | a)C(u)u(s)\n= \u2211 s\u030c B(s | a)u(s)\n(7) = \u2211 s\u030c B(s | a)u(s\u030c).\nHowever, if U includes functions such as uRL, then direct wireheading may be a problem for value learning agents. The problem is exacerbated by the fact that utility functions such as uRL will always be consistent with the observed reward r, self-delusion or not. On the other hand, functions of type uRL may have sufficiently small prior weight that direct wireheading will never induce a sufficient incentive for the agent to wirehead.\nB.3 Approximately isb Utility Functions\nThis subsection briefly discusses a possibility for constructing a wide class of approximately isb ( -isb) utility functions. The next result shows that if u is -isb, then the incentive for the agent to self-delude is not strong.\nTheorem 25 (Almost no direct wireheading). If u is -isb u(s) \u2248 u(s\u030cdid), then\nVu(a) \u2248 \u2211 s\u030c B(s\u030c | a)u(s\u030cdid).\nProof. Assuming u is -isb, Vu(a) is upper bounded by Vu(a) = \u2211 s B(s | a)u(s)\n\u2264 \u2211 s B(s\u030cds | a)(u(s\u030cdid) + )\n= \u2211 s\u030c B(s\u030c | a)u(s\u030cdid) +\nand similarly lower bounded. From this follows that Vu(a) deviates at most from \u2211 s\u030cB(s\u030c | a)u(s\u030cdid).\nThe following is one suggestion for constructing a wide class of -isb utility functions.\nDefinition 26 (Convolutional utility functions). Assume that the states are represented as binary strings S = {0, 1}\u2217. For a string s = s1 . . . s|s|, let sm:n = sm . . . sn for 1 \u2264 m \u2264 n \u2264 |s|. Let U\u0303 be the set of computable functions u\u0303 : {0, 1}k \u2192 R, and let Ucv be the set of k-convolutional utility functions defined by Ucv := { u(s) = \u2211|s|\u2212k i=1 u\u0303(si:i+k) : u \u2208 U\u0303 } .\nConvolutional utility functions are suitable under the following assumptions: (1) The agent\u2019s state representation has a similar topological structure as the real world. (2) The principal cares approximately equally about all parts of the real world (for example, each place inhabiting a happy human contribute equally to the total utility of the state). (3) The state of the delusion box only affects a small part of the state representation (so the utility functions are approximately inner state based in the sense of Theorem 25). Further research should investigate the plausibility of these assumptions, and whether constructions like Definition 26 are at all necessary. Possibly, the class U of all computable utility functions comes without substantial risks."}, {"heading": "C Omitted Proofs", "text": "Lemma 27 (U-VRL is RL). V (a) = V RL(a), so the U-VRL agent is equivalent to the RL agent.\nProof. V (a) may be written as V (a) = \u2211 s,r B(s | a)B(r | s) \u2211 u C(u | s, r)u(s). (9)\nThe sum over u reduces to r, since\u2211 u C(u | r, s)u(s) = \u2211 u C(u)C(r | s, u)\u2211 u\u2032 C(u \u2032)C(r | s, u\u2032) u(s)\n= \u2211 u C(u)Ju(s) = rK\u2211 u\u2032 C(u \u2032)Ju\u2032(s) = rKu(s)\n= \u2211\nu:u(s)=r\nC(u)\u2211 u\u2032:u\u2032(s)=r C(u \u2032) u(s)\n= \u2211\nu:u(s)=r\nC(u)\u2211 u\u2032:u\u2032(s)=r C(u \u2032) r = r\nReplacing the sum over u with r in (9) gives V RL."}], "references": [{"title": "Apprenticeship learning via inverse reinforcement learning", "author": ["P. Abbeel", "A.Y. Ng"], "venue": "ICML, pages 1\u20138.", "citeRegEx": "Abbeel and Ng,? 2004", "shortCiteRegEx": "Abbeel and Ng", "year": 2004}, {"title": "Towards resolving unidentifiability in inverse reinforcement learning", "author": ["K. Amin", "S. Singh"], "venue": "http://arxiv.org/abs/1601.06569.", "citeRegEx": "Amin and Singh,? 2016", "shortCiteRegEx": "Amin and Singh", "year": 2016}, {"title": "Motivated value selection for artificial agents", "author": ["S. Armstrong"], "venue": "Workshops at the Twenty-Ninth AAAI Conference on Artificial Intelligence, pages 12\u201320.", "citeRegEx": "Armstrong,? 2015", "shortCiteRegEx": "Armstrong", "year": 2015}, {"title": "Hail mary, value porosity, and utility diversification", "author": ["N. Bostrom"], "venue": "Technical report, Oxford University.", "citeRegEx": "Bostrom,? 2014a", "shortCiteRegEx": "Bostrom", "year": 2014}, {"title": "Superintelligence: Paths, Dangers, Strategies", "author": ["N. Bostrom"], "venue": "Oxford University Press.", "citeRegEx": "Bostrom,? 2014b", "shortCiteRegEx": "Bostrom", "year": 2014}, {"title": "Elements of Information Theory", "author": ["T.M. Cover", "J.A. Thomas"], "venue": "Wiley, 2nd edition.", "citeRegEx": "Cover and Thomas,? 2006", "shortCiteRegEx": "Cover and Thomas", "year": 2006}, {"title": "Learning what to value", "author": ["D. Dewey"], "venue": "AGI-11, volume 6830, pages 309\u2013314.", "citeRegEx": "Dewey,? 2011", "shortCiteRegEx": "Dewey", "year": 2011}, {"title": "Learning the preferences of ignorant, inconsistent agents", "author": ["O. Evans", "A. Stuhlmuller", "N.D. Goodman"], "venue": "AAAI-16.", "citeRegEx": "Evans et al\\.,? 2016", "shortCiteRegEx": "Evans et al\\.", "year": 2016}, {"title": "Sequential extensions of causal", "author": ["T. Springer. Everitt", "J. Leike", "M. Hutter"], "venue": null, "citeRegEx": "Everitt et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Everitt et al\\.", "year": 2015}, {"title": "Model-based utility functions", "author": ["B. Springer. Hibbard"], "venue": "Theory (ADT),", "citeRegEx": "Hibbard,? \\Q2012\\E", "shortCiteRegEx": "Hibbard", "year": 2012}, {"title": "The Singularity Is Near", "author": ["R. Kurzweil"], "venue": null, "citeRegEx": "Kurzweil,? \\Q2005\\E", "shortCiteRegEx": "Kurzweil", "year": 2005}, {"title": "Algorithms for inverse reinforcement learning", "author": ["A. Springer. Ng", "S. Russell"], "venue": null, "citeRegEx": "Ng and Russell,? \\Q2000\\E", "shortCiteRegEx": "Ng and Russell", "year": 2000}, {"title": "Anarchy, State, and Utopia", "author": ["R. Nozick"], "venue": "Basic Books. Omohundro, S. M", "citeRegEx": "Nozick,? \\Q1974\\E", "shortCiteRegEx": "Nozick", "year": 1974}, {"title": "Delusion, survival, and intelligent agents", "author": ["S. Franklin"], "venue": "editors, AGI-08,", "citeRegEx": "Franklin,? \\Q2011\\E", "shortCiteRegEx": "Franklin", "year": 2011}, {"title": "Inferring human values for safe agi design", "author": ["C.E. Springer. Sezener"], "venue": null, "citeRegEx": "Sezener,? \\Q2015\\E", "shortCiteRegEx": "Sezener", "year": 2015}, {"title": "The value learning problem", "author": ["N. MIRI. Soares", "B. Fallenstein", "E. Yudkowsky", "S. Armstrong"], "venue": "Stanford Encyclopedia of Philosophy. Winter 2015 edition. Soares, N", "citeRegEx": "Soares et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Soares et al\\.", "year": 2015}, {"title": "Reinforcement Learning: An Introduc", "author": ["R.S. Sutton", "A.G. Barto"], "venue": "AAAI Workshop on AI and Ethics,", "citeRegEx": "Sutton and Barto,? \\Q1998\\E", "shortCiteRegEx": "Sutton and Barto", "year": 1998}], "referenceMentions": [{"referenceID": 16, "context": "At present, the most promising framework for controlling generally intelligent agents is reinforcement learning (RL) (Sutton and Barto, 1998).", "startOffset": 117, "endOffset": 141}, {"referenceID": 6, "context": "Value learning (Dewey, 2011) is an attempt to combine the flexibility of RL with the state optimisation of utility agents.", "startOffset": 15, "endOffset": 28}, {"referenceID": 1, "context": "Concrete value learning proposals include inverse reinforcement learning (IRL) (Amin and Singh, 2016; Evans et al., 2016; Ng and Russell, 2000; Sezener, 2015) and apprenticeship learning (AL) (Abbeel and Ng, 2004).", "startOffset": 79, "endOffset": 158}, {"referenceID": 7, "context": "Concrete value learning proposals include inverse reinforcement learning (IRL) (Amin and Singh, 2016; Evans et al., 2016; Ng and Russell, 2000; Sezener, 2015) and apprenticeship learning (AL) (Abbeel and Ng, 2004).", "startOffset": 79, "endOffset": 158}, {"referenceID": 11, "context": "Concrete value learning proposals include inverse reinforcement learning (IRL) (Amin and Singh, 2016; Evans et al., 2016; Ng and Russell, 2000; Sezener, 2015) and apprenticeship learning (AL) (Abbeel and Ng, 2004).", "startOffset": 79, "endOffset": 158}, {"referenceID": 14, "context": "Concrete value learning proposals include inverse reinforcement learning (IRL) (Amin and Singh, 2016; Evans et al., 2016; Ng and Russell, 2000; Sezener, 2015) and apprenticeship learning (AL) (Abbeel and Ng, 2004).", "startOffset": 79, "endOffset": 158}, {"referenceID": 0, "context": ", 2016; Ng and Russell, 2000; Sezener, 2015) and apprenticeship learning (AL) (Abbeel and Ng, 2004).", "startOffset": 78, "endOffset": 99}, {"referenceID": 6, "context": "Other value learning suggestions have been speculative or vague (Bostrom, 2014a,b; Dewey, 2011).", "startOffset": 64, "endOffset": 95}, {"referenceID": 0, "context": "As Bostrom (2014b) convincingly argues, it is important that we find a way to specify robust goals for superintelligent agents.", "startOffset": 3, "endOffset": 19}, {"referenceID": 0, "context": "As Bostrom (2014b) convincingly argues, it is important that we find a way to specify robust goals for superintelligent agents. At present, the most promising framework for controlling generally intelligent agents is reinforcement learning (RL) (Sutton and Barto, 1998). The goal of an RL agent is to optimise a reward signal that is provided by an external evaluator (human or computer program). RL has several advantages: The setup is simple and elegant, and using an RL agent is as easy as providing reward in proportion to how satisfied one is with the agent\u2019s results or behaviour. Unfortunately, RL is not a good control mechanism for generally intelligent agents due to the wireheading problem (Ring and Orseau, 2011), which we illustrate in the following running example. Example 1 (Chess playing agent, wireheading problem). Consider an intelligent agent tasked with playing chess. The agent gets reward 1 for winning, and reward \u22121 for losing. For a moderately intelligent agent, this reward scheme suffices to make the the agent try to win. However, a sufficiently intelligent agent will instead realise that it can just modify its sensors so they always report maximum reward. This is called wireheading. \u2666 Utility agents were suggested by Hibbard (2012) as a way to avoid the wireheading problem.", "startOffset": 3, "endOffset": 1267}, {"referenceID": 0, "context": ", 2016; Ng and Russell, 2000; Sezener, 2015) and apprenticeship learning (AL) (Abbeel and Ng, 2004). However, IRL and AL are both still vulnerable to wireheading problems, at least in their most straightforward implementations. As illustrated in Example 18 below, IRL and AL agents may want to modify their sensory input to make the evidence point to a utility functions that is easier to satisfy. Other value learning suggestions have been speculative or vague (Bostrom, 2014a,b; Dewey, 2011). 1The difference between RL and utility agents is mirrored in the experience machine debate (Sinnott-Armstrong, 2015, Sec. 3) initialised by Nozick (1974). Given the option to enter a machine that will offer you the most pleasant delusions, but make you useless to the \u2018real world\u2019, would you enter? An RL agent would enter, but a utility agent would not.", "startOffset": 79, "endOffset": 649}, {"referenceID": 2, "context": "We remove the wireheading incentive by using a version of the conservation of expected ethics principle (Armstrong, 2015) which demands that actions should not alter the belief about the true utility function.", "startOffset": 104, "endOffset": 121}, {"referenceID": 9, "context": "Hibbard (2012) argues convincingly that the utility agent does not wirehead.", "startOffset": 0, "endOffset": 15}, {"referenceID": 11, "context": "Inverse reinforcement learning (IRL) (also known as Bayesian inverse planning) studies how preferences and utility functions can be inferred from the actions of other agents (Ng and Russell, 2000; Sezener, 2015; Evans et al., 2016; Amin and Singh, 2016).", "startOffset": 174, "endOffset": 253}, {"referenceID": 14, "context": "Inverse reinforcement learning (IRL) (also known as Bayesian inverse planning) studies how preferences and utility functions can be inferred from the actions of other agents (Ng and Russell, 2000; Sezener, 2015; Evans et al., 2016; Amin and Singh, 2016).", "startOffset": 174, "endOffset": 253}, {"referenceID": 7, "context": "Inverse reinforcement learning (IRL) (also known as Bayesian inverse planning) studies how preferences and utility functions can be inferred from the actions of other agents (Ng and Russell, 2000; Sezener, 2015; Evans et al., 2016; Amin and Singh, 2016).", "startOffset": 174, "endOffset": 253}, {"referenceID": 1, "context": "Inverse reinforcement learning (IRL) (also known as Bayesian inverse planning) studies how preferences and utility functions can be inferred from the actions of other agents (Ng and Russell, 2000; Sezener, 2015; Evans et al., 2016; Amin and Singh, 2016).", "startOffset": 174, "endOffset": 253}, {"referenceID": 0, "context": "Apprenticeship learning (AL) (Abbeel and Ng, 2004) is another form of value learning.", "startOffset": 29, "endOffset": 50}, {"referenceID": 9, "context": "Can the CP-VRL results be combined with other AI safety approaches such as self-modification (Everitt et al., 2016; Hibbard, 2012),", "startOffset": 93, "endOffset": 130}, {"referenceID": 15, "context": "corrigibility (Soares et al., 2015), suicidal agents (Martin et al.", "startOffset": 14, "endOffset": 35}, {"referenceID": 8, "context": ", 2016), and physicalistic reasoning (Everitt et al., 2015)?", "startOffset": 37, "endOffset": 59}, {"referenceID": 10, "context": "Several authors have argued that it is only a matter of time before we create systems with intelligence far beyond the human level (Kurzweil, 2005; Bostrom, 2014b).", "startOffset": 131, "endOffset": 163}, {"referenceID": 4, "context": "Several authors have argued that it is only a matter of time before we create systems with intelligence far beyond the human level (Kurzweil, 2005; Bostrom, 2014b).", "startOffset": 131, "endOffset": 163}], "year": 2016, "abstractText": "How can we design good goals for arbitrarily intelligent agents? Reinforcement learning (RL) is a natural approach. Unfortunately, RL does not work well for generally intelligent agents, as RL agents are incentivised to shortcut the reward sensor for maximum reward \u2013 the so-called wireheading problem. In this paper we suggest an alternative to RL called value reinforcement learning (VRL). In VRL, agents use the reward signal to learn a utility function. The VRL setup allows us to remove the incentive to wirehead by placing a constraint on the agent\u2019s actions. The constraint is defined in terms of the agent\u2019s belief distributions, and does not require an explicit specification of which actions constitute wireheading.", "creator": "LaTeX with hyperref package"}}}