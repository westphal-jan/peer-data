{"id": "1605.03392", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "11-May-2016", "title": "Learning Bounded Treewidth Bayesian Networks with Thousands of Variables", "abstract": "We present a method for learning treewidth-bounded Bayesian networks from data sets containing thousands of variables. Bounding the treewidth of a Bayesian greatly reduces the complexity of inferences. Yet, being a global property of the graph, it considerably increases the difficulty of the learning process in determining its extent.\n\n\nIn an experiment in which a user learns from Bayesian information, we measured a dataset using the data obtained by the Bayesian data sets from data sets of the Bayesian network. The results of the experiment were analyzed using the model, with data from the Bayesian network. The results of the experiment were described in the following comments.\nFigure 5: Bayesian inference and Bayesian inference.\nFigure 5: Bayesian inference and Bayesian inference.\nTo demonstrate a Bayesian inference using Bayesian data sets, we performed the following steps:\nIn the model, we used the Bayesian data sets of the Bayesian network (Fig. 5).\nThe Bayesian network is a single point and the group has four possible points. The group has two points. The group has three points. The group has three points. In a Bayesian network, each point corresponds to two points.\nThe second point is the same as the group.\nThe third point is the same as the group.\nThe third point is the same as the group.\nTo make the Bayesian inference, we used the Bayesian network for Bayesian inference (Fig. 5). This function is implemented with the following parameters:\nThe Bayesian network is a single point and the group has four points.\nThe Bayesian network is a single point and the group has four points.\nThe Bayesian network is a single point and the group has four points.\nThe Bayesian network is a single point and the group has four points.\nThe Bayesian network is a single point and the group has four points.\nThe Bayesian network is a single point and the group has four points.\nThe Bayesian network is a single point and the group has four points.\nThe Bayesian network is a single point and the group has four points.\nThe Bayesian network is a single point and the group has four points.\nThe Bayesian network is a single point and the group has four points.\nThe Bayesian network is a single point and the group has four points.\nThe Bayesian network is a single point and the group has four points.\nThe Bayesian network is a", "histories": [["v1", "Wed, 11 May 2016 11:54:26 GMT  (110kb,D)", "http://arxiv.org/abs/1605.03392v1", null]], "reviews": [], "SUBJECTS": "cs.AI", "authors": ["mauro scanagatta", "giorgio corani", "cassio p de campos", "marco zaffalon"], "accepted": false, "id": "1605.03392"}, "pdf": {"name": "1605.03392.pdf", "metadata": {"source": "CRF", "title": "Learning Bounded Treewidth Bayesian Networks with Thousands of Variables", "authors": ["Mauro Scanagatta", "Giorgio Corani", "Marco Zaffalon"], "emails": ["mauro@idsia.ch", "giorgio@idsia.ch", "c.decampos@qub.ac.uk", "zaffalon@idsia.ch"], "sections": [{"heading": null, "text": "We present a method for learning treewidthbounded Bayesian networks from data sets containing thousands of variables. Bounding the treewidth of a Bayesian greatly reduces the complexity of inferences. Yet, being a global property of the graph, it considerably increases the difficulty of the learning process. We propose a novel algorithm for this task, able to scale to large domains and large treewidths. Our novel approach consistently outperforms the state of the art on data sets with up to ten thousand variables."}, {"heading": "1 Introduction", "text": "We consider the problem of structural learning of Bayesian networks with bounded treewidth, adopting a score-based approach. Learning the structure of a bounded treewidth Bayesian network is a NP-hard problem (Korhonen and Parviainen, 2013). It is therefore unlikely the existence of an exact algorithm with complexity polynomial in the number of variables n. Yet learning Bayesian networks with bounded treewidth is deemed necessary to allow exact tractable inference, since the worst-case inference complexity of known algorithms is exponential in the treewidth k.\nThe topic has been thoroughly studied in the last years. A pioneering approach, polynomial in both the number of variables and the treewidth bound, has been proposed in (Elidan and Gould, 2009). It provides an upper-bound on the treewidth of the learned structure at each arc addition. The limit of this approach is that, as the number of variables\n\u2217Istituto Dalle Molle di studi sull\u2019Intelligenza Artificiale (IDSIA)\n\u2020Scuola universitaria professionale della Svizzera italiana (SUPSI)\n\u2021Universita\u0300 della Svizzera italiana (USI)\nincreases, the bound becomes too large leading to sparse networks.\nAn exact method has been proposed in (Korhonen and Parviainen, 2013), which finds the highest-scoring network with the desired treewidth. However, its complexity increases exponentially with the number of variables n. Thus it has been applied in experiments with up to only 15 variables.\nParviainen et al. (2014) adopted an anytime integer linear programming (ILP). If the algorithm is given enough time, it finds the highest-scoring network with bounded treewidth. Otherwise it returns a sub-optimal DAG with bounded treewidth. The ILP problem has an exponential number of constraints in the number of variables, which limits its scalability.\nNie et al. (2014) proposed a more efficient anytime ILP approach with a polynomial number of constraints in the number of variables. Yet they report that the quality of the solutions quickly degrades as the number of variables exceeds a few dozens and that no satisfactory solutions are found with data sets containing more than 50 variables.\nApproximate approaches are therefore needed to scale to larger domains. Nie et al. (2015) proposed the approximated method S2. It exploits the notion of k-tree, which is an undirected maximal graph with treewidth k. A Bayesian network whose moral graph is a subgraph of a k-tree has thus treewidth bounded by k. S2 is an iterative algorithm. Each iteration consists of two steps: a) sampling uniformly a k-tree from the space of k-trees and b) recovering via sampling a high-scoring DAG whose moral graph is a subgraph of the sampled k-tree. The goodness of the k-tree is approximated by using a heuristic evaluation, called Informative Score. Nie et al. (2016) further refines this idea, proposing an exploration guided via A* for finding the optimal k-tree with respect to the Informative Score. This algorithm is called S2+.\nRecent structural learning algorithms with unbounded treewidth (Scanagatta et al., 2015) can cope with thousands of variables. Yet the unbounded treewidth provides\nar X\niv :1\n60 5.\n03 39\n2v 1\n[ cs\n.A I]\n1 1\nM ay\n2 01\nno guarantee about the complexity of the inferences of the inferred models. We aim at filling this gap, learning treewidth-bounded Bayesian network models in domains with thousands of variables.\nStructural learning is usually accomplished in two steps: parent set identification and structure optimization. Parent set identification produces a list of suitable candidate parent sets for each variable. Structure optimization assigns a parent set to each node, maximizing the score of the resulting structure without introducing cycles.\nOur first contribution regards parent set identification. We provide a bound for pruning the sub-optimal parent sets when dealing with the BIC score; the bound is often tighter than the currently published ones (de Campos and Ji, 2011).\nAs a second contribution, we propose two approaches for learning Bayesian networks with bounded treewidth. They are based on an iterative procedure which is able to add new variables to the current structure, maximizing the resulting score and respecting the treewidth bound.\nWe compare experimentally our novel algorithms against S2 and S2+, which represent the state of the art on datasets with dozens of variables. Moreover, we present results for domains involving up to ten thousand variables, providing an increase of two order of magnitudes with respect to the results published to date. Our novel algorithms consistently outperform the competitors."}, {"heading": "2 Structural learning", "text": "Consider the problem of learning the structure of a Bayesian Network from a complete data set of N instances D = {D1, ..., DN}. The set of n categorical random variables is X = {X1, ..., Xn}. The goal is to find the best DAG G = (V,E), where V is the collection of nodes and E is the collection of arcs. E can be represented by the set of parents \u03a01, ...,\u03a0n of each variable.\nDifferent scores can be used to assess the fit of a DAG. We adopt the Bayesian Information Criterion (or simply BIC), which asymptotically approximates the posterior probability of the DAG under common assumptions. The BIC score is decomposable, being constituted by the sum of the scores of the individual variables:\nBIC(G) =\n= n\u2211 i=1 BIC(Xi,\u03a0i) = n\u2211 i=1 (LL(Xi|\u03a0i) + Pen(Xi,\u03a0i)) ,\nLL(Xi|\u03a0i) = \u2211\n\u03c0\u2208|\u03a0i|, x\u2208|Xi| Nx,\u03c0 log \u03b8\u0302x|\u03c0 ,\nPen(Xi,\u03a0i) = \u2212 logN\n2 (|Xi| \u2212 1)(|\u03a0i|) ,\nwhere \u03b8\u0302x|\u03c0 is the maximum likelihood estimate of the conditional probability P (Xi = x|\u03a0i = \u03c0), and Nx,\u03c0 repre-\nsents the number of times (X = x \u2227 \u03a0i = \u03c0) appears in the data set, and | \u00b7 | indicates the size of the Cartesian product space of the variables given as argument. Thus |Xi| is the number of states of Xi and |\u03a0i| is the product of the number of states of the parents of Xi.\nExploiting decomposability, we first identify independently for each variable a list of candidate parent sets (the parent set identification task). Later, we select for each node the parent set that yields the highest-scoring treewidthbounded DAG, which we call structure optimization."}, {"heading": "2.1 Parent sets identification", "text": "When learning with limited treewidth it should be noted that the number of parents is a lower bound for the treewidth, since a node and its parents form a clique in the moralized graph. Thus, before running the structure optimization task, the list of candidate parent sets of each node has to include parent sets with size up to k, if the treewidth has to be bounded by k (the precise definition of treewidth will be given later on). In spite of that, for values of k greater than 3 or 4, we cannot compute all candidate parent sets, since it already has time complexity \u0398(N \u00b7 nk+1). In this section we present the first contribution of this work: a bound for BIC scores that can be used to prune their evaluations while processing all parent set candidates. We first need a couple of auxiliary results.\nLemma 1. Let X be a node of X , and \u03a0 = \u03a01 \u222a \u03a02 be a parent set of X such that \u03a01 \u2229 \u03a02 = \u2205 and \u03a01,\u03a02 6= \u2205. Then LL(X|\u03a0) =\n= LL(X|\u03a01) + LL(X|\u03a02)\u2212LL(X) +N \u00b7 ii(X; \u03a01; \u03a02),\nwhere ii is the Interaction Information estimated from data.\nProof. It follows trivially from Theorem 1 in (Scanagatta et al., 2015).\nIt is known that LL(\u03a01) \u2264 N \u00b7 ii(\u03a01; \u03a02;X) \u2264 \u2212LL(\u03a01), and that the order of arguments is irrelevant (that is, ii(\u03a01; \u03a02;X) = ii(\u03a02; \u03a01;X) = ii(X; \u03a01; \u03a02)). These inequalities provide bounds for the log-likelihood in line with the result presented in Corollary 1 of (Scanagatta et al., 2015). We can manipulate that result to obtain new tigher bounds.\nLemma 2. Let X,Y1, . . . , Yt be nodes of X , and \u03a0 6= \u2205 be a parent set for X with \u03a0 \u2229 Y = \u2205, where Y = {Y1, . . . , Yt}. Then LL(X|\u03a0 \u222a Y) \u2264 LL(X|\u03a0) +\u2211t i=1 w(X,Yi), where w(X,Yi) = MI(X,Yi) \u2212 max{LL(X); LL(Yi)}, where MI(X,Yi) = LL(X|Yi) \u2212 LL(X) is the empirical mutual information.\nProof. It follows from the bounds of ii(\u00b7) and the successive application of Lemma 1 to LL(X|\u03a0 \u222a Y), taking out one node of Y a time.\nThe advantage of Lemma 2 is that MI(X,Yi) and LL(X) and LL(Yi) (and hence w(X,Yi)) can be all precomputed efficiently in total time O(N \u00b7 n) for a given X , and since BIC is composed of log-likelihood plus penalization (the latter is efficient to compute), we obtain a new means of bounding BIC scores as follows.\nTheorem 1. Let X \u2208 X , and \u03a0 6= \u2205 be a parent set for X , \u03a00 = \u03a0 \u222a {Y0} for some Y0 \u2208 X \\ \u03a0, and Y \u2032 = maxY \u2208X\\\u03a00 (w(X,Y ) + Pen(X,\u03a0 \u222a {Y })). If w(X,Y0) + Pen(X,\u03a00) \u2264 Pen(X,\u03a0) and w(X,Y \u2032) + Pen(X,\u03a0 \u222a {Y \u2032}) \u2264 0, with w(\u00b7) as defined in Lemma 2, then \u03a00 and any of its supersets are not optimal.\nProof. Suppose \u03a0\u2032 = \u03a00 \u222a Y , with Y = {Y1, . . . , Yt} and Y \u2229\u03a00 = \u2205 (Y may be empty). We have that\nBIC(X,\u03a0\u2032) = LL(X|\u03a0\u2032) + Pen(X,\u03a0\u2032)\n\u2264LL(X|\u03a0\u2032) + Pen(X,\u03a00) + t\u2211 i=1 Pen(X,\u03a0 \u222a {Yi})\n\u2264LL(X|\u03a0) + Pen(X,\u03a00) + w(X,Y0)+ t\u2211 i=1 (w(X,Yi) + Pen(X,\u03a0 \u222a {Yi})) \u2264BIC(X,\u03a0) + t (w(X,Y \u2032) + Pen(X,\u03a0 \u222a {Y \u2032})) \u2264BIC(X,\u03a0).\nFirst step is the definition of BIC, second step uses the fact that the penalty function is exponentially fast with the increase in number of parents, third step uses Lemma 2, fourth step uses the assumptions of the theorem and the fact that Y \u2032 is maximal. Therefore we would choose \u03a0 in place of \u03a00 or any of its supersets.\nTheorem 1 can be used to discard parent sets during already their evaluation and without the need to wait for precomputing all possible candidates. We point out that these bounds are new and not trivially achievable by current existing bounds for BIC. As a byproduct, we obtain bounds for the number of parents of any given node.\nCorollary 1. Using BIC score, each node has at most O(logN \u2212 log logN) parents in the optimal structure.\nProof. Let X be a node of X and \u03a0 a possible parent set. Let Y \u2208 X \\ \u03a0. From the fact that MI(X,Y ) \u2264 log |X|, and max{LL(X);LL(Y )} \u2265 \u2212N \u00b7 log |X|, we have that w(X,Y ) \u2264 (N + 1) log |X|, with w(\u00b7) as defined in Lemma 2. Now\nlog |\u03a0| \u2265 log (\n2 log |X| |X| \u2212 1\n) + log ( N + 1\nlogN\n) \u21d0\u21d2\n(N + 1) log |X| \u2264 logN 2 \u00b7 |\u03a0|(|X| \u2212 1) =\u21d2 w(X,Y ) \u2264 \u2212Pen(X,\u03a0 \u222a {Y }) + Pen(X,\u03a0)\nfor any Y , and so by Theorem 1 no super set of \u03a0 is optimal. Note that log |\u03a0| is greater than or equal to the number of parents in \u03a0, so we have proven that any node in the optimal structure has at most O(logN \u2212 log logN), which is similar to previous known results (see e.g. (de Campos and Ji, 2011)).\n2.2 Treewidth and k-trees\nWe use this section to provide the necessary definitions and notation.\nTreewidth We illustrate the concept of treewidth following the notation of (Elidan and Gould, 2009). We denote an undirected graph as H = (V,E) where V is the vertex set and E is the edge set. A tree decomposition of H is a pair (C, T ) where C = {C1, C2, ..., Cm} is a collection of subsets of V and T is a tree on C, so that:\n\u2022 \u222ami=1 Ci = V ;\n\u2022 for every edge which connects the vertices v1 and v2, there is a subset Ci which contains both v1 and v2;\n\u2022 for all i, j, k in {1, 2, ..m} ifCj is on the path between Ci and Ck in T then Ci \u2229 Ck \u2286 Cj .\nThe width of a tree decomposition is max(|Ci|)\u2212 1 where |Ci| is the number of vertices in Ci. The treewidth of H is the minimum width among all possible tree decompositions of G.\nThe treewidth can be equivalently defined in terms of triangulation of H. A triangulated graph is an undirected graph in which every cycle of length greater than three contains a chord. The treewidth of a triangulated graph is the size of the maximal clique of the graph minus one. The treewidth of H is the minimum treewidth over all the possible triangulations ofH.\nThe treewidth of a Bayesian network is characterized with respect to all possible triangulations of its moral graph. The moral graph M of a DAG is an undirected graph that includes an edge (i \u2192 j) for every edge (i \u2192 j) in the DAG and an edge (p \u2192 q) for every pair of edges (p \u2192 i), (q \u2192 i) in the DAG. The treewidth of a DAG is the minimum treewidth over all the possible triangulations of its moral graph M. Thus the maximal clique of any moralized triangulation of G is an upper bound on the treewidth of the model.\nk-trees An undirected graph Tk = (V,E) is a k-tree if it is a maximal graph of tree-width k: any edge added to Tk = (V,E) increases its treewidth.\nA k-tree is inductively defined as follows (Patil, 1986). Consider a (k + 1)-clique, namely a complete graph with\nk + 1 nodes. A (k + 1)-clique is a k-tree.\nA (k + 1)-clique can be decomposed into multiple kcliques. Let us denote by z a node not yet included in the list of vertices V . Then the graph obtained by connecting z to every node of a k-clique of Tk is also a k-tree.\nThe treewidth of any subgraph of a k-tree (partial k-tree) is bounded by k. Thus a DAG whose triangulated moral graph is subgraph of a k-tree has treewidth bounded by k."}, {"heading": "3 Incremental treewidth-bounded structure learning", "text": "We now turn our attention to the structure optimization task. Our approach proceeds by repeatedly sampling an order\u227a over the variables and then identifying the highestscoring DAG with bounded-treewidth consistent with the order. The size search space of the possible orders is n!, thus smaller than the search space of the possible ktrees. Once the order is sampled, we incrementally learn the DAG; it is guaranteed that at each step the moralization of the DAG is a subgraph of a k-tree. The treewidth of the DAG eventually obtained is thus bounded by k. The algorithm proceeds as follows.\nInitialization The initial k-tree Kk+1 is constituted by the complete clique over the first k + 1 variables in the order. The initial DAG Gk+1 is learned over the same k+ 1 variables. Since (k + 1) is a small number of variables, we can exactly learn Gk+1. In particular we adopt the method of Cussens (2011). The moral graph of Gk+1 is a subgraph of Kk+1 and thus Gk+1 has bounded treewidth.\nNode\u2019s addition We then iteratively add each remaining variable. Consider the next variable in the order, X\u227ai, where i \u2208 {k + 2, ..., n}. Let us denote by Gi\u22121 and Ki\u22121 the DAG and the k-tree which have to be updated by adding X\u227ai. We add X\u227ai to Gi\u22121, under the constraint that its parent set \u03a0\u227ai is a subset of a complete k-clique in Ki\u22121. This yields the updated DAG Gi. We then update the k-tree, connecting X\u227ai to such k-clique. This yields the updated k-tree Ki; it contains an additional k + 1-clique compared to Ki\u22121. By construction, Ki is also a k-tree. The moral graph of Gi cannot add arc outside this (k+ 1)-clique; thus it is a subgraph of Ki.\nPruning orders Notice thatKk+1 and Gk+1 depend only on which are the first k+1 variables and not on their relative positions. Thus all the orders which differ only as for the relative position of the first k + 1 elements are equivalent for our algorithm. Thus once we have sampled an order and identified the corresponding DAG, we can prune the remaining (k + 1)!\u2212 1 equivalent orders.\nIn order to choose the parent set to be assigned to each\nvariable added to the graph we propose two algorithms: kA* and k-G."}, {"heading": "3.1 k-A*", "text": "We formulate the problem as a shortest path finding problem. We define each state as a step towards the completion of the structure, where a new variable is added to the DAG G. Given X\u227ai the variable assigned in the state S, we define a successor state of S for each k-clique we can choose for adding the variable X\u227ai+1. The approach to solve the problem is based on a path-finding A* search, with cost function for state S defined as f(S) = g(S) + h(S). The goal is the state minimizing f(S) where all the variable have been assigned.\ng(S) is the cost from the initial state to S, and we define it as the sum of scores of already assigned parent sets:\ng(S) = i\u2211 j=0 score(X\u227aj ,\u03a0\u227aj) .\nh(S) is the estimated cost from S to the goal. It is the sum of best assignable parent sets for the remaining variables. Note that we know that Xa can have Xb as parent only if Xb \u227a Xa:\ng(S) = n\u2211 j=i+1 best(X\u227aj) .\nThe algorithm uses an open list to store the search frontier. At each step it recovers the state with the smallest f cost, generate the successors state and insert them into open, until the optimal is found.\nThe A* approach requires the h function to be admissible. The function h is admissible if the estimated cost is never greater than the true cost to the goal state. Our approach guarantees this property since the true cost of each step (score of chosen parent set for X\u227ai+1) is always equal or greater than the estimated (score of best selectable parent set for X\u227ai+1).\nWe also have that h is consistent, meaning that for any state S and its successor T , h(S) \u2264 h(T ) + c(S, T ), where c(S, T ) is the cost of the edges added in T . This follows from the previous argument. Now we have that f is monotonically non-decreasing on any path, and the algorithm is guaranteed to find the optimal path as long as the goal state is reachable."}, {"heading": "3.2 k-G", "text": "In some cases a high number of variables or a high treewidth prevent the use of k-A*. We thus propose\na greedy alternative approach, K-G. Following the pathfinding problem defined previously, it takes a greedy approach: at each step chooses for the variable Xi the highest-scoring parent set that is subset of an existing kclique in K."}, {"heading": "3.3 Space of learnable DAGs", "text": "A reverse topological order is an order {v1, ...vn} over the vertexes V of a DAG in which each vi appears before its parents \u03a0i. The search space of our algorithms is restricted to the DAGs whose reverse topological order, when used as variable elimination order, has treewidth k. This prevents recovering DAGs which have bounded treewidth but lack this property.\nWe start by proving by induction that the reverse topological order has treewidth k in the DAGs recovered by our algorithms. Consider the incremental construction of the DAG previously discussed.\nThe initial DAG Gk+1 is induced over k+ 1 variables; thus every elimination ordering has treewidth bounded by k.\nFor the inductive case, assume that Gi\u22121 satisfy the property. Consider the next variable in the order, X\u227ai , where i \u2208 {k + 2, ..., n}. Its parent set \u03a0\u227ai is a subset of a kclique in Ki\u22121. The only neighbors of X\u227ai in the updated DAG Gi are its parents \u03a0\u227ai . Consider performing variable elimination on the the moral graph of Gi, using a reverse topological order. Then X\u227ai will be eliminated before \u03a0\u227ai , without introducing fill-in edges. Thus the treewidth associated to any reverse topological order is bounded by k. This property inductively applies to the addition also of the following nodes up to X\u227an .\nInverted trees An example of DAG non recoverable by our algorithms is the specific class of polytrees that we call inverted trees, that is, DAGs with indegree equal to one. An inverted tree with m levels and treewidth k can be built as follows. Take the root node (level one) and connect it to k child nodes (level two). Connect each node of level two to k child nodes (level three). Proceed in this way up to the m-th level and then invert the direction of all the arcs.\nFigure 1 shows an inverted tree with k=2 and m=3. It has treewidth two, since its moral graph is constituted by the cliques {A,B,E}, {C,D,F}, {E,F,G}. The treewidth associated to the reverse topological order is instead three, using the order G, F, D, C, E, A, B.\nIf we run our algorithms with bounded treewidth k=2, it will be unable to recover the actual inverted tree. It will instead identify a high-scoring DAG whose reverse topological order has treewidth 2."}, {"heading": "3.4 Our implementation of S2 and S2+", "text": "Here we provide the details of our implementation of S2 and S2+. They both use the notion of Informative Score (Nie et al., 2015), an approximate measure of the fitness of a k-tree. The I-score of a k-tree Tk is defined as\nIS(Tk) = Smi(Tk)\n|Sl(Tk)| ,\nwhere Smi(Tk) measures the expected loss of representing the data with the k-tree. Let Iij denote the mutual information of node i and j:\nSmi(Tk) = \u2211 i,j Iij \u2212 \u2211 i,j /\u2208Tk Iij .\nSl(Tk) instead is defined as the score of the best pseudo subgraph of the k-tree by dropping the acyclic constraint:\nSl(Tk) = max m(G)\u2208Tk \u2211 i\u2208N score(Xi,\u03a0i) ,\nwhere m(G) is the moral graph of DAG G, and score(Xi,\u03a0i) is the local score function of variable Xi for the parent set \u03a0i.\nThe first phase of both S2 and S2+ consists in a k-tree sampling. In particular, S2 obtains k-trees by using the Dandelion sampling discussed in (Nie et al., 2014). The proposed k-trees are then accepted with probability:\n\u03b1 = min ( 1, IS(Tk)\nIS(T \u2217k )\n) ,\nwhere T \u2217k is the current k-tree with the largest I-score (Nie et al., 2015).\nInstead S2+ selects the k + 1 variables with the largest Iscore and finds the k-tree maximizing the I-score from this clique, as discussed in (Nie et al., 2016). Additional k-trees are obtained choosing a random initial clique.\nThe second phase of the algorithms looks for a DAG whose moralization is subgraph of the chosen k-tree. For this task, the authors proposed an approximate approach based on partial order sampling (Algorithm 2 of (Nie et al., 2014)). In our experiments, we found that using Gobnilp for this task yields slightly higher scores, therefore we adopt this\napproach in our implementation. We believe that it is due to the fact that constraining the structure optimization to a subjacent graph of a k-tree results in a small number of allowed arcs for the DAG. Thus our implementation finds the highest-scoring DAG whose moral graph is a subgraph of the provided k-tree."}, {"heading": "3.4.1 Discussion", "text": "The problem with k-tree sampling is that each k-tree enforces a random constraint over the arcs that may appear in the final structure. The chance that we randomly sample a k-tree that allows good scoring arcs becomes significantly smaller as the number of variables increases, and the space of possible k-tree increases as well. The criterion for probabilistic acceptance, presented in the past section, has been proposed for tackling this issue, but it does not resolve the situation completely.\nOur approach instead focus immediately on selecting the best arcs, in a way that guarantees the treewidth bound. Experimentally we observed that k-tree sampling is quicker, producing an higher number of candidate DAGs, whose scores are unfortunately low. Our approach instead generates less but higher-scoring DAGs.\n(Nie et al., 2016) improves on the notion of k-tree, searching for the optimal one with respect to the Informative Score (IS). IS considers only the mutual information between pair of variables, and it may exaggerate the importance of assigning some arcs. The IS criterion may suggest parents for a node with separately have high mutual information but are bad together as a parent set."}, {"heading": "4 Experiments", "text": "We compare k-A*, k-G, S2 and S2+ in various experiments. We compare them through an indicator which we call W-score: the percentage of worsening of the BIC score of the selected treewidth-bounded method compared to the score of the Gobnilp solver (Cussens, 2011). Gobnilp achieves higher score than the treewidth-bounded methods since it has no limits on the treewidth. Let us denote by G the BIC score achieved by Gobnilp and by T the BIC score obtained by the given treewidth-bounded method. Notice that bothG and T are negative. The W-score isW = G\u2212TG . W stands for worsening and thus lower values ofW are better. The lowest value of W is zero, while there is no upper bound on the value of W."}, {"heading": "4.1 Learning inverted trees", "text": "As already discussed our approach cannot learn an inverted tree with k parents per node if given bounded treewidth k. In this section we study their performance in this worstcase scenario.\nWe start with treewidth k = 2. We consider the number of variables n \u2208 {21, 41, 61, 81, 101}. For each value of n we generate 5 different inverted trees. An inverted tree is generated by randomly selecting a root variable X from the existing graph and adding k new variables as \u03a0X , until the graph contains n variables. All variables are binary and we sample their conditional probability tables from a Beta(1,1). We sample 10,000 instances from each generated inverted tree.\nWe then perform structural learning with k-A*, k-G, S2 and S2+, setting k = 2 as limit on the treewidth. We allow each method to run for ten minutes. Both S2 and S2+ could in principle recover the true structure, which is prevented to our algorithms. The results are shown in Fig.2. Qualitatively similar results are obtained repeating the experiments with k = 4.\nDespite the unfavorable setting, both k-G and k-A* yield DAGs with higher score than S2 and S2+, consistently for each value of n. Thus the limitation of the space of learnable DAGs does not hurt much the performance of k-G and k-A*. In fact S2 could theoretically recover the actual DAG, but this would require too many samples from the space of the k-trees, which is prohibitive.\nWe further investigate the differences between methods by providing in Table 2 some statistics about the candidate solutions they generate. Iterations is the number of proposed solutions; for S2 and S2+ it is the number of explored k-\ntrees, while for k-G and k-A* it is number of explored orders.\nDuring the execution, S2 samples almost one million ktrees. Yet it yields the lowest-scoring DAGs among the different methods. This can be explained considering that a randomly sampled k-tree has a low chance to cover a highscoring DAG. S2+ recovers only a few k-trees, but their scores are higher than those of S2. This confirms the effectiveness of driving the search for good k-trees through the Informative Score. As we will see later, however, this idea does not scale on very large data sets.\nAs for our methods, k-G samples a larger number of orders than k-A* does and this allows it to achieve higher scores, even if it sub-optimally deals with each single order."}, {"heading": "4.2 Small data sets", "text": "We now present experiments on the data sets already considered by (Nie et al., 2016). They involve up to 100 variables. We set the bounded treewidth to k = 4. We provide each structural learning method with the same precomputed scores of parent sets. We allow each method to run for ten minutes. We perform 10 experiments on each data set and we report the median scores in Table 1. Our results are not comparable with those reported by (Nie et al., 2016) since we use the BIC while they use BDeu.\nRemarkably both k-A* and k-G achieve higher scores than both S2 and S2+ do on almost all data sets. Only on the smallest data sets all methods achieve the same score. Between our two novel algorithms, k-A* has a slight advantage over k-G.\nWe provide statistics about the candidate solutions generated by each method in Table 3. The results of the table refer in particular to the community data set (n=100). The conclusions are similar to those of previous analyses. S2\nperforms almost one million iterations, but they are characterized by low scores. S2+ performs a drastically smaller number of iterations, but is able anyway to outperform S2. Similarly k-A* is more effective than k-G, despite generating a lower number of candidate solution. The reduced number of candidate solutions generated by both S2+ and k-A* suggest that they cannot scale on data sets much larger than those of this experiment."}, {"heading": "4.3 Large data sets", "text": "We now consider 10 large data sets (100 \u2264 n \u2264 400) listed in Table 4.\nWe consider the following treewidths: k \u2208 {2, 5, 8}. We split each data set randomly into three subsets. Thus for each treewidth we run 10\u00b73=30 structural learning experiments.\nWe provide all structural learning methods with the same pre-computed scores of parent sets and we let each method run for one hour. For S2+, we adopt a more favorable approach, allowing it to run for one hour; if after one hour the first k-tree was not yet solved, we allow it to run until it has solved the first k-tree.\nIn Table 5 we report how many times each method wins against another for each treewidth. The entries are boldfaced when the number of victories of an algorithm over another is statistically significant according to the sign-test (p-value <0.05). Consistently for any chosen treewidth, k-G is significantly better than any competitor, including k-A*; moreover, k-A* is significantly better than both S2 and S2+.\nThis can be explained by considering that k-G explores more orders than k-A*, as for a given order it only finds an approximate solution. The results suggest that it is more important to explore many orders instead of obtaining the optimal DAG given an order."}, {"heading": "4.4 Very large data sets", "text": "As final experiment, we consider 14 very large data sets, containing more than 400 variables. We include in these experiments three randomly-generated synthetic data sets containing 2000, 4000 and 10000 variables respectively. These networks have been generated using the software BNGenerator 1. Each variable has a number of states randomly drawn from 2 to 4 and a number of parents randomly drawn from 0 to 6. In this case, we perform 14\u00b73=42 structural learning experiments with each algorithm. The only two algorithms able to cope with these data sets are k-G and S2. Among them, k-G wins 42 times out of 42; this dominance is clearly significant. This result is consistently found under each choice of treewidth (k =2, 5, 8). On average, the improvement of k-G over S2 fills about 60% of the gap which separates S2 from the unbounded solver.\nThe W-scores of such 42 structural learning experiments are summarized in Figure 3. For both S2 and k-G, a larger treewidth allows to recover a higher-scoring graph. In turn\n1http://sites.poli.usp.br/pmr/ltd/ Software/BNGenerator/\nthis decreases the W-score. However k-G scales better than S2 with respect to the treewidth; its W-score decreases more sharply with the treewidth.\nIt is interesting to analyze the statistics of the solutions generated by the two methods. They are given in Table 7 for the data set Munin. K-G generates a number of solutions which is a few orders of magnitude smaller than that of S2. Yet, the scores of the obtained solutions are much higher."}, {"heading": "5 Conclusion", "text": "Our novel approaches for treewidth-bounded structural learning of Bayesian Networks perform significantly better\nthan state-of-the-art methods. The greedy approach scales up to thousands of nodes and suggests that it is more important to find good k-trees than to solve the internal structure optimization task for each one of them. The methods consistently outperform the competitors on a variety of experiments. All these methods and others for unbounded learning of Bayesian networks can make use of our new bounds for BIC scores in order to reduce the number of parent set evaluations during the precomputation of scores. Further analyses of the bounds are left for future work."}], "references": [{"title": "Bayesian network learning with cutting planes", "author": ["J. Cussens"], "venue": "Proceedings of the 27th Conference Annual Conference on Uncertainty in Artificial Intelligence,", "citeRegEx": "Cussens,? \\Q2011\\E", "shortCiteRegEx": "Cussens", "year": 2011}, {"title": "Efficient structure learning of Bayesian networks using constraints", "author": ["P. de Campos C", "Q. Ji"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "C. and Ji,? \\Q2011\\E", "shortCiteRegEx": "C. and Ji", "year": 2011}, {"title": "Learning bounded treewidth Bayesian networks", "author": ["G. Elidan", "S. Gould"], "venue": "In Advances in Neural Information Processing Systems", "citeRegEx": "Elidan and Gould,? \\Q2009\\E", "shortCiteRegEx": "Elidan and Gould", "year": 2009}, {"title": "Exact learning of bounded tree-width Bayesian networks", "author": ["H. Korhonen J", "P. Parviainen"], "venue": "In Proc. 16th Int. Conf. on AI and Stat.,", "citeRegEx": "J. and Parviainen,? \\Q2013\\E", "shortCiteRegEx": "J. and Parviainen", "year": 2013}, {"title": "Advances in learning Bayesian networks of bounded treewidth", "author": ["S. Nie", "D. Mau\u00e1 D", "P. de Campos C", "Q. Ji"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Nie et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Nie et al\\.", "year": 2014}, {"title": "Learning Bounded TreeWidth Bayesian Networks via Sampling", "author": ["S. Nie", "P. de Campos C", "Q. Ji"], "venue": "Proceedings of the 13th European Conference on Symbol and Quantitative Approaches to Reasoning with Uncertainty,", "citeRegEx": "Nie et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Nie et al\\.", "year": 2015}, {"title": "Learning Bayesian networks with bounded treewidth via guided search", "author": ["S. Nie", "P. de Campos C", "Q. Ji"], "venue": "Proceedings of the 30th AAAI Conference on Artificial Intelligence,", "citeRegEx": "Nie et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Nie et al\\.", "year": 2016}, {"title": "Learning bounded tree-width Bayesian networks using integer linear programming", "author": ["P. Parviainen", "S. Farahani H", "J. Lagergren"], "venue": "In Proceedings of the 17th International Conference on Artificial Intelligence and Statistics,", "citeRegEx": "Parviainen et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Parviainen et al\\.", "year": 2014}, {"title": "On the structure of k-trees", "author": ["P. Patil H"], "venue": "Journal of Combinatorics, Information and System Sciences,", "citeRegEx": "H.,? \\Q1986\\E", "shortCiteRegEx": "H.", "year": 1986}, {"title": "Learning Bayesian Networks with Thousands of Variables", "author": ["M. Scanagatta", "P. de Campos C", "G. Corani", "M. Zaffalon"], "venue": "Advances in Neural Information Processing Systems", "citeRegEx": "Scanagatta et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Scanagatta et al\\.", "year": 2015}], "referenceMentions": [{"referenceID": 2, "context": "A pioneering approach, polynomial in both the number of variables and the treewidth bound, has been proposed in (Elidan and Gould, 2009).", "startOffset": 112, "endOffset": 136}, {"referenceID": 4, "context": "Nie et al. (2015) proposed the approximated method S2.", "startOffset": 0, "endOffset": 18}, {"referenceID": 4, "context": "Nie et al. (2015) proposed the approximated method S2. It exploits the notion of k-tree, which is an undirected maximal graph with treewidth k. A Bayesian network whose moral graph is a subgraph of a k-tree has thus treewidth bounded by k. S2 is an iterative algorithm. Each iteration consists of two steps: a) sampling uniformly a k-tree from the space of k-trees and b) recovering via sampling a high-scoring DAG whose moral graph is a subgraph of the sampled k-tree. The goodness of the k-tree is approximated by using a heuristic evaluation, called Informative Score. Nie et al. (2016) further refines this idea, proposing an exploration guided via A* for finding the optimal k-tree with respect to the Informative Score.", "startOffset": 0, "endOffset": 590}, {"referenceID": 9, "context": "Recent structural learning algorithms with unbounded treewidth (Scanagatta et al., 2015) can cope with thousands of variables.", "startOffset": 63, "endOffset": 88}, {"referenceID": 9, "context": "It follows trivially from Theorem 1 in (Scanagatta et al., 2015).", "startOffset": 39, "endOffset": 64}, {"referenceID": 9, "context": "These inequalities provide bounds for the log-likelihood in line with the result presented in Corollary 1 of (Scanagatta et al., 2015).", "startOffset": 109, "endOffset": 134}, {"referenceID": 2, "context": "Treewidth We illustrate the concept of treewidth following the notation of (Elidan and Gould, 2009).", "startOffset": 75, "endOffset": 99}, {"referenceID": 0, "context": "In particular we adopt the method of Cussens (2011). The moral graph of Gk+1 is a subgraph of Kk+1 and thus Gk+1 has bounded treewidth.", "startOffset": 37, "endOffset": 52}, {"referenceID": 5, "context": "They both use the notion of Informative Score (Nie et al., 2015), an approximate measure of the fitness of a k-tree.", "startOffset": 46, "endOffset": 64}, {"referenceID": 4, "context": "In particular, S2 obtains k-trees by using the Dandelion sampling discussed in (Nie et al., 2014).", "startOffset": 79, "endOffset": 97}, {"referenceID": 5, "context": "where T \u2217 k is the current k-tree with the largest I-score (Nie et al., 2015).", "startOffset": 59, "endOffset": 77}, {"referenceID": 6, "context": "Instead S2+ selects the k + 1 variables with the largest Iscore and finds the k-tree maximizing the I-score from this clique, as discussed in (Nie et al., 2016).", "startOffset": 142, "endOffset": 160}, {"referenceID": 4, "context": "For this task, the authors proposed an approximate approach based on partial order sampling (Algorithm 2 of (Nie et al., 2014)).", "startOffset": 108, "endOffset": 126}, {"referenceID": 6, "context": "(Nie et al., 2016) improves on the notion of k-tree, searching for the optimal one with respect to the Informative Score (IS).", "startOffset": 0, "endOffset": 18}, {"referenceID": 0, "context": "We compare them through an indicator which we call W-score: the percentage of worsening of the BIC score of the selected treewidth-bounded method compared to the score of the Gobnilp solver (Cussens, 2011).", "startOffset": 190, "endOffset": 205}, {"referenceID": 6, "context": "Table 1: Comparison between bounded-treewidth structural learning algorithms on the data sets already analyzed by (Nie et al., 2016).", "startOffset": 114, "endOffset": 132}, {"referenceID": 6, "context": "We now present experiments on the data sets already considered by (Nie et al., 2016).", "startOffset": 66, "endOffset": 84}, {"referenceID": 6, "context": "Our results are not comparable with those reported by (Nie et al., 2016) since we use the BIC while they use BDeu.", "startOffset": 54, "endOffset": 72}], "year": 2016, "abstractText": "We present a method for learning treewidthbounded Bayesian networks from data sets containing thousands of variables. Bounding the treewidth of a Bayesian greatly reduces the complexity of inferences. Yet, being a global property of the graph, it considerably increases the difficulty of the learning process. We propose a novel algorithm for this task, able to scale to large domains and large treewidths. Our novel approach consistently outperforms the state of the art on data sets with up to ten thousand variables.", "creator": "LaTeX with hyperref package"}}}