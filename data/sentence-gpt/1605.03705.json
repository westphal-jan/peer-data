{"id": "1605.03705", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "12-May-2016", "title": "Movie Description", "abstract": "Audio Description (AD) provides linguistic descriptions of movies and allows visually impaired people to follow a movie along with their peers. Such descriptions are by design mainly visual and thus naturally form an interesting data source for computer vision and computational linguistics. In this work we propose a novel dataset which contains transcribed ADs, which are temporally aligned to full length movies and which are more accurate than any previous image source. It allows users to infer the exact spatial location of the film as they are scanned through a digital camera, which enables information about the location of the film, and thus further data visualization. This dataset is also designed to detect the number of times and number of films which have been scanned during the entire runtime of the film for this purpose. The data should be downloaded from the computer or via an e-mail to the general computer at http://sensor2.com. We hope to introduce a new data visualization approach in the future as it may not be complete in the future. The dataset will allow user to make better choices about the information they need to make sense of a movie or screen with the help of an interpreter. The information generated will be used to develop the data analysis method and the data will be processed in machine learning.", "histories": [["v1", "Thu, 12 May 2016 07:34:08 GMT  (7087kb,D)", "http://arxiv.org/abs/1605.03705v1", null]], "reviews": [], "SUBJECTS": "cs.CV cs.CL", "authors": ["anna rohrbach", "atousa torabi", "marcus rohrbach", "niket tandon", "christopher pal", "hugo larochelle", "aaron courville", "bernt schiele"], "accepted": false, "id": "1605.03705"}, "pdf": {"name": "1605.03705.pdf", "metadata": {"source": "CRF", "title": "Movie Description", "authors": ["Anna Rohrbach", "Atousa Torabi", "Marcus Rohrbach", "Niket Tandon", "Hugo Larochelle", "Aaron Courville", "Bernt Schiele"], "emails": [], "sections": [{"heading": null, "text": "descriptions of movies and allows visually impaired people to follow a movie along with their peers. Such descriptions are by design mainly visual and thus naturally form an interesting data source for computer vision and computational linguistics. In this work we propose a novel dataset which contains transcribed ADs, which are temporally aligned to full length movies. In addition we also collected and aligned movie scripts used in prior work and compare the two sources of descriptions. In total the Large Scale Movie Description Challenge (LSMDC) contains a parallel corpus of 118,114 sentences and video clips from 202 movies. First we characterize the dataset by benchmarking different approaches for generating video descriptions. Comparing ADs to scripts, we find that ADs are indeed more visual and describe precisely what is shown rather than what should happen according to the scripts created prior to movie production. Furthermore, we present and compare the results of several teams who participated in a challenge organized in the context of the workshop \u201cDescribing and Understanding Video & The Large Scale Movie Description Challenge (LSMDC)\u201d, at ICCV 2015.\nAnna Rohrbach 1 \u00b7 Atousa Torabi3 \u00b7 Marcus Rohrbach2 \u00b7 Niket Tandon 1 \u00b7 Christopher Pal4 \u00b7 Hugo Larochelle5,6 \u00b7 Aaron Courville7 \u00b7 Bernt Schiele1 1 Max Planck Institute for Informatics, Saarbru\u0308cken, Germany 2 ICSI and EECS, UC Berkeley, United States 3 Disney Research, Pittsburgh, United States 4 E\u0301cole Polytechnique de Montre\u0301al, Montre\u0301al, Canada 5 Universite\u0301 de Sherbrooke, Sherbrooke, Canada 6 Twitter, Cambridge, United States 7 Universite\u0301 de Montre\u0301al, Montre\u0301al, Canada\nAD: Abby gets in the basket. Mike leans over and sees how high they are. Abby clasps her hands around his face and kisses him passionately. Script: After a moment a frazzled Abby pops up in his place. Mike looks down to see \u2013 they are now fifteen feet above the ground.\nFor the first time in her life, she stops thinking and grabs Mike and kisses the hell out of him.\nFigure 1 Audio description (AD) and movie script samples from the movie \u201cUgly Truth\u201d."}, {"heading": "1 Introduction", "text": "Audio descriptions (ADs) make movies accessible to millions of blind or visually impaired people1. AD \u2014 sometimes also referred to as Descriptive Video Service (DVS) \u2014 provides an audio narrative of the \u201cmost important aspects of the visual information\u201d (Salway 2007), namely actions, gestures, scenes, and character appearance as can be seen in Figures 1 and 2. AD is prepared by trained describers and read by professional narrators. While more and more movies are audio transcribed, it may take up to 60 person-hours to describe a 2-hour movie (Lakritz and Salway 2006), resulting in the fact that today only a small subset of movies and TV programs are available for the blind. Consequently, automating this process has the potential to greatly increase accessibility to this media.\n1 In this work we refer for simplicity to \u201cthe blind\u201d to account for all blind and visually impaired people which benefit from AD, knowing of the variety of visually impaired and that AD is not accessible to all.\nar X\niv :1\n60 5.\n03 70\n5v 1\n[ cs\n.C V\n] 1\n2 M\nay 2\nIn addition to the benefits for the blind, generating descriptions for video is an interesting task in itself, requiring the combination of core techniques from computer vision and computational linguistics. To understand the visual input one has to reliably recognize scenes, human activities, and participating objects. To generate a good description one has to decide what part of the visual information to verbalize, i.e. recognize what is salient.\nLarge datasets of objects (Deng et al. 2009) and scenes (Xiao et al. 2010; Zhou et al. 2014) have had an important impact in computer vision and have significantly improved our ability to recognize objects and scenes. The combination of large datasets and convolutional neural networks (CNNs) has been particularly potent (Krizhevsky et al. 2012). To be able to learn how to generate descriptions of visual content, parallel datasets of visual content paired with descriptions are indispensable (Rohrbach et al. 2013). While recently several large datasets have been released which provide images with descriptions (Hodosh et al. 2014; Lin et al. 2014; Ordonez et al. 2011), video description datasets\nfocus on short video clips with single sentence descriptions and have a limited number of video clips (Xu et al. 2016; Chen and Dolan 2011) or are not publicly available (Over et al. 2012). TACoS Multi-Level (Rohrbach et al. 2014) and YouCook (Das et al. 2013) are exceptions as they provide multiple sentence descriptions and longer videos. While these corpora pose challenges in terms of fine-grained recognition, they are restricted to the cooking scenario. In contrast, movies are open domain and realistic, even though, as any other video source (e.g. YouTube or surveillance videos), they have their specific characteristics. ADs and scripts associated with movies provide rich multiple sentence descriptions. They even go beyond this by telling a story which means they facilitate the study of how to extract plots, the understanding of long term semantic dependencies and human interactions from both visual and textual data.\nFigures 1 and 2 show examples of ADs and compare them to movie scripts. Scripts have been used for various tasks (Cour et al. 2008; Duchenne et al. 2009; Laptev et al. 2008; Liang et al. 2011; Marszalek et al.\nMovie Description 3\nVisual Features\nSOMEONE ENTER DOOR ROOM Subject Location Verb Object CRF Classifier scores SMT Someone enters the room.SM Tba se d\nPLACES-CNN LSDA\nIDT\nPlaces class. scores Object class. scores\nVerb class. scores\nLS TM\n-b as ed select robust classifiers\nLSTMconcat\nconcat\nconcat\nSomeone\nenters\nthe\nLSTM\nLSTM\nconcat roomLSTM\nwave\ntake off\npunch\ndance\ngive\nopen\ngrab\nclimb\ndrink\nenter\nfall\nget out\ndrive off\njump\neat\nFigure 3 Some of the diverse verbs / actions present in our Large Scale Movie Description Challenge (LSMDC).\n2009), but so far not for video description. The main reason for this is that automatic alignment frequently fails due to the discrepancy between the movie and the script. As scripts are typically produced prior to the shooting of the movie they are frequently not as precise as the AD (Figure 2 shows such mistakes marked in red). A typical case is that part of the sentence is correct, while another part contains irrelevant information. As can be seen in the examples, AD narrations describe key visual elements of the video such as changes in the scene, people\u2019s appearance, gestures, actions, and their interaction with each other and the scene\u2019s objects in concise and precise language. Figure 3 shows the variability of AD data w.r.t. to verbs (actions) and corresponding scenes from the movies.\nIn this work we present a dataset which provides\ntranscribed ADs, aligned to full length movies. AD soundtracks are carefully positioned within movies to fit in natural pauses in the dialogue and are mixed with the original movie soundtrack by professional post-production. For this we retrieve audio streams from DVDs and Bluray disks, segment out the sections of the AD audio and transcribe them via a crowd-sourced transcription service. The ADs provide an initial temporal alignment, which however does not always cover the full activity in the video. We discuss a way to fully automate both audio-segmentation and temporal alignment, but also manually align each sentence to the movie for the majority of the data. Therefore, in contrast to Salway (2007) and Salway et al. (2007), our dataset provides alignment to the actions in the video, rather than just to the audio track of the description. In addition we also mine existing movie scripts, pre-align them automatically, similar to Cour et al. (2008) and Laptev et al. (2008), and then manually align the sentences to the movie.\nAs a first study on our dataset we benchmark several approaches for movie description. We first examine\nnearest neighbour retrieval using diverse visual features which do not require any additional labels, but retrieve sentences from the training data. Second, we adapt the approach of Rohrbach et al. (2013) by automatically extracting the semantic representation from the sentences using semantic parsing. Third, based on the success of Long Short-Term Memory networks (LSTMs) (Hochreiter and Schmidhuber 1997) for the image captioning problem (Donahue et al. 2015; Karpathy and Fei-Fei 2015; Kiros et al. 2015; Vinyals et al. 2015) we propose our approachVisual-Labels. It first builds robust visual classifiers which distinguish verbs, objects, and places extracted from weak sentence annotations. Then the visual classifiers form the input to an LSTM for generating movie descriptions.\nThe main contribution of this work is the Large Scale Movie Description Challenge (LSMDC)2 which provides transcribed and aligned AD and script data sentences. The LSMDC 2015 has been presented at the Workshop \u201cDescribing and Understanding Video & The Large Scale Movie Description Challenge (LSMDC)\u201d, collocated with ICCV 2015. We are in progress of setting up the LSMDC 2016 which will be presented at the \u201cJoint Workshop on Storytelling with Images and Videos and Large Scale Movie Description and Understanding Challenge\u201d, collocated with ECCV 2016. The challenge includes a public and blind test set and evaluation server3 for automatic evaluation. LSMDC is based on the MPII Movie Description dataset (MPIIMD) and the Montreal Video Annotation Dataset (MVAD) which were initially collected independently but are presented jointly in this work. We detail the data collection and dataset properties in Section 3, which includes our approach to automatically collect and align AD data. In Section 4 we present several benchmark\n2 https://sites.google.com/site/describingmovies/ 3 https://competitions.codalab.org/competitions/6121\napproaches for movie description, including our VisualLabels approach which learns robust visual classifiers and generates description using an LSTM. In Section 5 we present an evaluation of the benchmark approaches on the M-VAD and MPII-MD datasets, analyzing the influence of the different design choices. Using automatic and human evaluation, we also show that our Visual-Labels approach outperforms prior work on both datasets. In Section 5.5 we perform an analysis of prior work and our approach to understand the challenges of the movie description task. In Section 6 we present and discuss the results of the Large Scale Movie Description Challenge 2015, which we held in conjunction with ICCV 2015.\nThis work is partially based on the original publications from Rohrbach et al. (2015c,b) and the technical report from Torabi et al. (2015). Torabi et al. (2015) collected M-VAD, Rohrbach et al. (2015c) collected the MPII-MD dataset and presented the SMT-based description approach. Rohrbach et al. (2015b) proposed the Visual-Labels approach."}, {"heading": "2 Related work", "text": "We discuss recent approaches to image and video description including existing work using movie scripts and ADs. We also discuss works which build on our dataset. We compare our proposed dataset to related video description datasets in Table 3 (Section 3.5).\n2.1 Image description\nPrior work on image description includes Farhadi et al. (2010); Kulkarni et al. (2011); Kuznetsova et al. (2012); Li et al. (2011); Kuznetsova et al. (2014); Mitchell et al. (2012); Socher et al. (2014). Recently image description has gained increased attention with work such as that of Chen and Zitnick (2015); Donahue et al. (2015); Fang et al. (2015); Karpathy and Fei-Fei (2015); Kiros et al. (2014, 2015); Mao et al. (2015); Vinyals et al. (2015); Xu et al. (2015a). Much of the recent work has relied on Recurrent Neural Networks (RNNs) and in particular on Long Short-Term Memory networks (LSTMs). New datasets have been released, such as the Flickr30k (Young et al. 2014) and MS COCO Captions (Chen et al. 2015), where Chen et al. (2015) also presents a standardized protocol for image captioning evaluation. Other work has analyzed the performance of recent methods, e.g. Devlin et al. (2015) compare them with respect to the novelty of generated descriptions, while also exploring a nearest neighbor baseline that improves over recent methods.\n2.2 Video description\nIn the past video description has been addressed in controlled settings (Barbu et al. 2012; Kojima et al. 2002), on a small scale (Das et al. 2013; Guadarrama et al. 2013; Thomason et al. 2014) or in single domains like cooking (Rohrbach et al. 2014, 2013; Donahue et al. 2015). Donahue et al. (2015) first proposed to describe videos using an LSTM, relying on precomputed CRF scores from Rohrbach et al. (2014). Later Venugopalan et al. (2015c) extended this work to extract CNN features from frames which are max-pooled over time. Pan et al. (2016b) propose a framework that consists of a 2-/3-D CNN and LSTM trained jointly with a visual-semantic embedding to ensure better coherence between video and text. Xu et al. (2015b) jointly address the language generation and video/language retrieval tasks by learning a joint embedding for a deep video model and a compositional semantic language model. Li et al. (2015) study the problem of summarizing a long video to a single concise description by using ranking based summarization of multiple generated candidate sentences.\nConcurrent and consequent work. To handle the challenging scenario of movie description, Yao et al. (2015) propose a soft-attention based model which selects the most relevant temporal segments in a video, incorporates 3-D CNN and generates a sentence using an LSTM. Venugopalan et al. (2015b) propose S2VT, an encoderdecoder framework, where a single LSTM encodes the input video frame by frame and decodes it into a sentence. Pan et al. (2016a) extend the video encoding idea by introducing a second LSTM layer which receives input of the first layer, but skips several frames, reducing its temporal depth. Venugopalan et al. (2016) explore the benefit of pre-trained word embeddings and language models for generation on large external text corpora. Shetty and Laaksonen (2015) evaluate different visual features as input for an LSTM generation framework. Specifically they use dense trajectory features Wang et al. (2013) extracted for the clips and CNN features extracted at center frames of the clip. They find that training concept classifiers on MS COCO with the CNN features, combined with dense trajectories provides the best input for the LSTM. Ballas et al. (2016) leverages multiple convolutional maps from different CNN layers to improve the visual representation for activity and video description. To model multi-sentence description, Yu et al. (2016) propose to use two stacked RNNs where the first one models words within a sentence and the second one, sentences within a paragraph. Yao et al. (2016) has conducted an interesting study on\nperformance upper bounds for both image and video description tasks on available datasets, including the LSMDC dataset.\n2.3 Movie scripts and audio descriptions\nMovie scripts have been used for automatic discovery and annotation of scenes and human actions in videos (Duchenne et al. 2009; Laptev et al. 2008; Marszalek et al. 2009), as well as a resource to construct activity knowledge base (Tandon et al. 2015; de Melo and Tandon 2016). We rely on the approach presented by Laptev et al. (2008) to align movie scripts using subtitles.\nBojanowski et al. (2013) approach the problem of learning a joint model of actors and actions in movies using weak supervision provided by scripts. They rely on the semantic parser SEMAFOR (Das et al. 2012) trained on the FrameNet database (Baker et al. 1998), however, they limit the recognition only to two frames. Bojanowski et al. (2014) aim to localize individual short actions in longer clips by exploiting the ordering constraints as weak supervision. Bojanowski et al. (2013, 2014); Duchenne et al. (2009); Laptev et al. (2008); Marszalek et al. (2009) proposed datasets focused on extracting several activities from movies. Most of them are part of the \u201cHollywood2\u201d dataset (Marszalek et al. 2009) which contains 69 movies and 3669 clips. Another line of work (Cour et al. 2009; Everingham et al. 2006; Ramanathan et al. 2014; Sivic et al. 2009; Tapaswi et al. 2012) proposed datasets for character identification targeting TV shows. All the mentioned datasets rely on alignments to movie/TV scripts and none uses ADs.\nADs have also been used to understand which characters interact with each other (Salway et al. 2007). Other prior work has looked at supporting AD production using scripts as an information source (Lakritz and Salway 2006) and automatically finding scene boundaries (Gagnon et al. 2010). Salway (2007) analyses the linguistic properties on a non-public corpus of ADs from 91 movies. Their corpus is based on the original sources to create the ADs and contains different kinds of artifacts not present in actual description, such as dialogs and production notes. In contrast, our text corpus is much cleaner as it consists only of the actual ADs.\n2.4 Works building on our dataset\nInterestingly, other works, datasets, and challenges are already building upon our data. Zhu et al. (2015b) learn a visual-semantic embedding from our clips and ADs to relate movies to books. Tapaswi et al. (2016) used\nour AD transcripts for building their MovieQA dataset, which asks natural language questions about movies, requiring an understanding of visual and textual information, such as Dialogue and AD, to answer the question. Zhu et al. (2015a) present a fill-in-the-blank challenge for audio description of the current, previous, and next sentence description for a given clip, requiring to understand the temporal context of the clips."}, {"heading": "3 Datasets for movie description", "text": "In the following, we present how we collected our data for movie description and discuss its properties. The Large Scale Movie Description Challenge (LSMDC) is based on two datasets which were originally collected independently. The MPII Movie Description Dataset (MPII-MD), initially presented by Rohrbach et al. (2015c), was collected from Blu-ray movie data. It consists of AD and script data and uses sentence-level manual alignment of transcribed audio to the actions in the video (Section 3.1). In Section 3.2 we discuss how to fully automate AD audio segmentation and alignment for the Montreal Video Annotation Dataset (M-VAD), initially presented by Torabi et al. (2015). M-VAD was collected with DVD data quality and only relies on AD. Section 3.3 details the Large Scale Movie Description Challenge (LSMDC) which is based on M-VAD and MPIIMD, but also contains additional movies, and was set up as a challenge and includes a submission server using a public and blind test sets. In Section 3.4 we present the detailed statistics of our datasets, also see Table 1. In Section 3.5 we compare our movie description data to other video description datasets.\n3.1 The MPII Movie Description (MPII-MD) dataset\nIn the following we describe the approach behind the collection of ADs (Section 3.1.1) and script data (Section 3.1.2). Then we discuss how to manually align them to the video (Section 3.1.3) and which visual features we extracted from the video (Section 3.1.4)."}, {"heading": "3.1.1 Collection of ADs", "text": "We search for Blu-ray movies with ADs in the \u201cAudio Description\u201d section of the British Amazon4 and select a set of movies of diverse genres. As ADs are only available in audio format, we first retrieve the audio stream\n4 www.amazon.co.uk\nfrom the Blu-ray HD disks. We use MakeMKV5 to extract a Blu-ray in the .mkv file format, then XMediaRecode6 to select and extract the audio streams from it. Then we semi-automatically segment out the sections of the AD audio (which is mixed with the original audio stream) with the approach described below. The audio segments are then transcribed by a crowd-sourced transcription service7 that also provides us the time-stamps for each spoken sentence.\nSemi-automatic segmentation of ADs. We are given two audio streams: the original audio and the one mixed with the AD. We first estimate the temporal alignment between the two as there might be a few time frames difference. The precise alignment is important to compute the similarity of both streams. Both steps (alignment and similarity) are estimated using the spectograms of the audio stream, which is computed using a Fast Fourier Transform (FFT). If the difference between the two audio streams is larger than a given threshold we assume the mixed stream contains AD at that point in time. We smooth this decision over time using a minimum segment length of 1 second. The threshold was picked on a few sample movies, but had to be adjusted for each movie due to different mixing of the AD stream, different narrator voice level, and movie sound. While we found this semi-automatic approach sufficient when using a further manual alignment, we describe a fully automatic procedure in Section 3.2."}, {"heading": "3.1.2 Collection of script data", "text": "In addition to the ADs we mine script web resources8 and select 39 movie scripts. As starting point we use the movie scripts from \u201cHollywood2\u201d (Marszalek et al. 2009) that have highest alignment scores to their movie. We are also interested in comparing the two sources (movie scripts and ADs), so we are looking for the scripts labeled as \u201cFinal\u201d, \u201cShooting\u201d, or \u201cProduction Draft\u201d where ADs are also available. We found that the \u201coverlap\u201d is quite narrow, so we analyze 11 such movies in our dataset. This way we end up with 50 movie scripts in total. We follow existing approaches (Cour et al. 2008; Laptev et al. 2008) to automatically align scripts to movies. First we parse the scripts, extending the method of (Laptev et al. 2008) to handle scripts which deviate from the default format. Second,\n5 www.makemkv.com/ 6 www.xmedia-recode.de/ 7 CastingWords transcription service,\nhttp://castingwords.com/ 8 http://www.weeklyscript.com, http://www.simplyscripts.com, http://www.dailyscript.com, http://www.imsdb.com\nwe extract the subtitles from the Blu-ray disks with SubtitleEdit9. It also allows for subtitle alignment and spellchecking. Then we use the dynamic programming method of (Laptev et al. 2008) to align scripts to subtitles and infer the time-stamps for the description sentences. We select the sentences with a reliable alignment score (the ratio of matched words in the near-by monologues) of at least 0.5. The obtained sentences are then manually aligned to video in-house."}, {"heading": "3.1.3 Manual sentence-video alignment", "text": "As the AD is added to the original audio stream between the dialogs, there might be a small misalignment between the time of speech and the corresponding visual content. Therefore, we manually align each sentence from ADs and scripts to the movie in-house. During the manual alignment we also filter out: a) sentences describing movie introduction/ending (production logo, cast, etc); b) texts read from the screen; c) irrelevant sentences describing something not present in the video; d) sentences related to audio/sounds/music. For the movie scripts, the reduction in number of words is about 19%, while for ADs it is under 4%. In the case of ADs, filtering mainly happens due to initial/ending movie intervals and transcribed dialogs (when shown as text). For the scripts, it is mainly attributed to irrelevant sentences. Note that we retain the sentences that are \u201calignable\u201d but contain minor mistakes."}, {"heading": "3.1.4 Visual features", "text": "We extract video clips from the full movie based on the aligned sentence intervals. We also uniformly extract 10 frames from each video clip. As discussed earlier, ADs and scripts describe activities, objects and scenes (as well as emotions which we do not explicitly handle with these features, but they might still be captured, e.g. by the context or activities). In the following we briefly introduce the visual features computed on our data which are publicly available10.\nIDT We extract the improved dense trajectories compensated for camera motion (Wang and Schmid 2013). For each feature (Trajectory, HOG, HOF, MBH) we create a codebook with 4,000 clusters and compute the corresponding histograms. We apply L1 normalization to the obtained histograms and use them as features.\nLSDA We use the recent large scale object detection CNN (Hoffman et al. 2014) which distinguishes 7,604 ImageNet (Deng et al. 2009) classes. We run the\n9 www.nikse.dk/SubtitleEdit/ 10 mpii.de/movie-description\ndetector on every second extracted frame (due to computational constraints). Within each frame we maxpool the network responses for all classes, then do meanpooling over the frames within a video clip and use the result as a feature.\nPLACES and HYBRID Finally, we use the recent scene classification CNNs (Zhou et al. 2014) featuring 205 scene classes. We use both available networks, Places-CNN and Hybrid-CNN, where the first is trained on the Places dataset (Zhou et al. 2014) only, while the second is additionally trained on the 1.2 million images of ImageNet (ILSVRC 2012) (Russakovsky et al. 2014). We run the classifiers on all the extracted frames of our dataset. We mean-pool over the frames of each video clip, using the result as a feature.\n3.2 The Montreal Video Annotation Dataset (M-VAD)\nOne of the main challenges in automating the construction of a video annotation dataset derived from AD audio is accurately segmenting the AD output, which is mixed with the original movie soundtrack. In this section we describe our methods for AD narration isolation and video alignment. AD narrations are typically carefully placed within key locations of a movie and edited by a post-production supervisor for continuity. For example, when a scene changes rapidly, the narrator will speak multiple sentences without pauses. Such content should be kept together when describing that part of the movie. If a scene changes slowly, the narrator will instead describe the scene in one sentence, then pause for a moment, and later continue the description. By detecting those short pauses, we are able to align a movie with video descriptions automatically.\nIn the following we detail our automatic approach to AD segmentation (Section 3.2.1). In Section 3.2.2 we discuss how to align AD to the video and obtain high quality AD transcripts."}, {"heading": "3.2.1 AD narrations segmentation using vocal isolation", "text": "Despite the advantages offered by AD, creating a completely automated approach for extracting the relevant narration or annotation from the audio track and refining the alignment of the annotation with the scene still poses some challenges. In the following, we discuss our automatic solution for AD narrations segmentation. We use two audio tracks included in DVDs: 1) the standard movie audio signal and 2) the standard movie audio mixed with AD narrations signal.\nVocal isolation techniques boost vocals, including dialogues and AD narrations while suppressing background movie sound in stereo signals. This technique\nis used widely in karaoke machines for stereo signals to remove the vocal track by reversing the phase of one channel to cancel out any signal perceived to come from the center while leaving the signals that are perceived as coming from the left or the right. The main reason for using vocal isolation for AD segmentation is based on the fact that AD narration is mixed in natural pauses in the dialogue. Hence, AD narration can only be present when there is no dialogue. In vocal isolated signals, whenever the narrator speaks, the movie signal is almost a flat line relative to the AD signal, allowing us to cleanly separate the narration from other dialogue by comparing the two signals. Figure 4 illustrates an example from the movie \u201cLife of Pi\u201d, where in the original movie soundtrack there are sounds of ocean waves in the background.\nOur approach has three main steps. First we isolate vocals, including dialogues and AD narrations. Second, we separate the AD narrations from dialogues. Finally, we apply a simple thresholding method to extract AD segment audio tracks.\nWe isolate vocals using Adobe Audition\u2019s center chan-\nnel extractor11 implementation to boost AD narrations and movie dialogues while suppressing movie background sounds on both AD and movie audio signals. We align the movie and AD audio signals by taking an FFT of the two audio signals, compute the cross-correlation, measure similarity for different offsets and select the offset which corresponds to peak cross-correlation. After alignment, we apply Least Mean Square (LMS) noise cancellation and subtract the AD mono squared signal from the movie mono squared signal in order to suppress dialogue in the AD signal. For the majority of movies on the market (among the 104 movies that we purchased, 12 movies have been mixed to the center of the audio signal, therefore we were not able to automatically align them), applying LMS results in cleaned AD narrations for the AD audio signal. Even in cases where the shapes of the standard movie audio signal and standard movie audio mixed with AD signal are very different - due to the AD mixing process - our procedure is sufficient for the automatic segmentation of AD narration.\nFinally, we extract the AD audio tracks by detecting the beginning and end of AD narration segments in the AD audio signal (i.e. where the narrator starts and stops speaking) using a simple thresholding method that we applied to all DVDs without changing the threshold value.\n11 creative.adobe.com/products/audition"}, {"heading": "3.2.2 Movie/AD alignment and professional transcription", "text": "AD audio narration segments are time-stamped based on our automatic AD narration segmentation. In order to compensate for the potential 1-2 seconds misalignment between the AD narrator speaking and the corresponding scene in the movie, we automatically added two seconds to the end of each video clip. Also we have discard all the transcriptions related to movie introduction/ending which are located at the beginning and the end of movies. Unlike movie scripts, in AD less than 0.5% might be related to non-visual descriptions such as a text message on a cellphone, or street name in the movie, which we keep in our corpus.\nIn order to obtain high quality text descriptions, the AD audio segments were transcribed with more than 98% transcription accuracy, using a professional transcription service12. These services use a combination of automatic speech recognition techniques and human transcription to produce a high quality transcription.\n12 TranscribeMe professional transcription, http://transcribeme.com\nOur audio narration isolation technique allows us to process the audio into small, well defined time segments and reduce the overall transcription effort and cost.\n3.3 The Large Scale Movie Description Challenge (LSMDC)\nFor our Large Scale Movie Description Challenge (LSMDC), we combined the M-VAD and MPII-MD datasets. We first identified the overlap between the two, so that the same movie does not appear in the training and test set of the joined dataset. We also excluded script-based movie alignments from the validation and test sets of MPII-MD. The datasets were then joined by combining the corresponding training, validation and test sets, see Table 1 for detailed statistics. The combined test set is used as a public test set of the challenge. We additionally added 20 more movies where we only released the video clips, but not the aligned sentences. They form the blind test set of the challenge and are only used for evaluation. We rely on the respective best aspects of M-VAD and MPII-MD for the public and blind test\nsets: we provide Blu-ray quality for them, use the automatic alignment and transcription described in Section 3.2 and clean them using a manual alignment as in Section 3.1.3. We are also in the process to manually align the M-VAD validation and training sets and will release them with Blu-ray quality for LSMDC 2016. We set up the evaluation server3 for the challenge using the Codalab13 platform. The challenge data is available online2. We provide more information about the challenge setup and results in Section 6.\n3.4 Movie description dataset statistics\nTable 1 presents statistics for the number of words, sentences and clips in our movie description corpora. We also report the average/total length of the annotated time intervals. If the manually aligned video clip was shorter than 2 seconds, we symmetrically expanded it (from beginning and end) to be exactly 2 seconds long. In the table we include both, the final as well as the original (precise) clip length (in brackets). In total MPIIMD contains 68,337 clips and 68,375 sentences (sometimes multiple sentences migh refer to the same video clip), while M-VAD includes 46,589 clips and 55,904 sentences. Our combined LSMDC dataset contains over 118K sentence-clips pairs and 158 hours of video.\nFor LSMDC the training/validation/public-/blindtest split consist of 91,908, 6,542, 10,053 and 9,578\n13 https://codalab.org/\nvideo clips respectively. This split balances movie genres within each set, which is motivated by the fact that the vocabulary used to describe, say, an action movie could be very different from the vocabulary used in a comedy movie.\nTable 2 illustrates the vocabulary size, number of nouns, verbs, adjectives, and adverbs in each respective dataset. To compute the part of speech statistics for our corpora we tag and stem all words in the datasets with the Standford Part-Of-Speech (POS) tagger and stemmer toolbox (Toutanova et al. 2003), then we compute the frequency of stemmed words in the corpora. It is important to notice that in our computation each word and its variations in corpora is counted once since we applied stemmer. Interesting observation on statistics is that e.g. the number of adjectives is larger than the number of verbs, which shows that the AD is describing the characteristics of visual elements in the movie in high detail.\n3.5 Comparison to other video description datasets\nWe compare our corpus to other existing parallel video corpora in Table 3. The main limitations of prior datasets include the coverage of a single domain (Das et al. 2013; Regneri et al. 2013; Rohrbach et al. 2014) and having a limited number of video clips (Chen and Dolan 2011). Recently, two video description datasets have been proposed, namely MSR-VTT (Xu et al. 2016) and TGIF (Li et al. 2016). Similar to MSVD dataset (Chen and Dolan 2011), MSR-VTT is based on YouTube clips. While it has a large number of sentence descriptions (200K) it is still rather small in terms of the number of video clips (10K). TGIF is a large dataset of 100k image sequences (GIFs) with associated descriptions. Both datasets are similar in that they represent webvideos, while our proposed datasets focus on movies."}, {"heading": "4 Approaches for movie description", "text": "Given a training corpus of aligned videos and sentences we want to describe a new unseen test video. In this section we discuss two approaches to the video description task that we benchmark on our proposed datasets. Our first approach in Section 4.1 is based on the statistical machine translation (SMT) approach of (Rohrbach et al. 2013). Our second approach (Section 4.2) learns to generate descriptions using Long Short-Term Memory network (LSTM). For the first step both approaches rely on visual classifiers learned on annotations (labels) extracted from natural language descriptions using our semantic parser (Section 4.1.1). While the first approach does not differentiate which features to use for different labels, our second approach defines different semantic groups of labels and uses most relevant visual features for each group. For this reason we refer to this approach as Visual-Labels. In the second step, the SMT-based approach uses a CRF to predict single most likely tuple (Subject, Verb, Object, Location) and translates it into a sentence. On the other hand, the LSTM-based approach takes the complete score vectors from the classifiers as input and generates a sentence based on them. Figure 5 provides an overview of the two discussed approaches.\n4.1 Semantic parsing + SMT\nAs our first approach we adapt the two-step translation approach of (Rohrbach et al. 2013) which uses an intermediate semantic representation (SR), modeled as a tuple, e.g. \u3008cut, knife, tomato\u3009 and Statistical Machine Translation (SMT) (Koehn et al. 2007) (see Figure 5(a)). While we cannot rely on an annotated SR as in (Rohrbach et al. 2013), we automatically mine the SR from sentences using semantic parsing which we introduce in this section."}, {"heading": "4.1.1 Semantic parsing", "text": "Learning from a parallel corpus of videos and natural language sentences is challenging when no annotated intermediate representation is available. In this section we introduce our approach to exploit the sentences using semantic parsing. The proposed method extracts intermediate semantic representations (SRs) from the natural sentences.\nApproach. We lift the words in a sentence to a semantic space of roles and WordNet (Fellbaum 1998; Pedersen et al. 2004) senses by performing SRL (Semantic Role Labeling) and WSD (Word Sense Disambiguation). For an example, refer to Table 4, the expected outcome of semantic parsing on the input sentence \u201cHe shot a video in the moving bus\u201d is \u201cAgent: man, Action: shoot, Patient: video, Location: bus\u201d. Additionally, the role fillers are disambiguated.\nWe use the ClausIE tool (Del Corro and Gemulla\n2013) to decompose sentences into their respective clauses. For example, \u201che shot and modified the video\u201d is split into two phrases \u201che shot the video\u201d and \u201che modified the video\u201d). We then use the OpenNLP tool suite14 for chunking the text of each clause. In order to provide the linking of words in the sentence to their WordNet sense\n14 http://opennlp.sourceforge.net/\nwave\ntake off\npunch\ndance\ngive\nopen\ngrab\nclimb\ndrink\nenter\nfall\nget out\ndrive off\njump\neat\nmappings, we rely on a state-of-the-art WSD system, IMS (Zhong and Ng 2010). The WSD system, however, works at a word level. We enable it to work at a phrase level. For every noun phrase, we identify and disambiguate its head word (e.g. the moving bus to \u201cbus#1 \u201d, where \u201cbus#1 \u201d refers to the first sense of the word bus). We link verb phrases to the proper sense of its head word in WordNet (e.g. begin to shoot to \u201cshoot#4 \u201d). The phrasal verbs such as e.g. \u201cpick up\u201d or \u201cturn off\u201d are preserved as long as they exist in WordNet.\nIn order to obtain word role labels, we link verbs to VerbNet (Kipper et al. 2006; Schuler et al. 2009), a manually curated high-quality linguistic resource for English verbs. VerbNet is already mapped to WordNet, thus we map to VerbNet via WordNet. We perform two levels of matches in order to obtain role labels. First is the syntactic match. Every VerbNet verb sense comes with a syntactic frame e.g. for shoot, the syntactic frame is NP V NP. We first match the sentence\u2019s verb against the VerbNet frames. These become candidates for the next step. Second we perform the semantic match: VerbNet also provides a role restriction on the arguments of the roles e.g. for shoot (sense killing), the role restriction is Agent.animate V Patient.animate PP Instrument.solid. For the other sense for shoot (sense snap), the semantic restriction is Agent.animate V Patient.solid. We only accept candidates from the syntactic match that satisfy the semantic restriction.\nSemantic representation. VerbNet contains over 20 roles and not all of them are general or can be recognized reliably. Therefore, we group them to get the SUBJECT, VERB, OBJECT and LOCATION roles. We explore two approaches to obtain the labels based on the output of the semantic parser. First is to use the extracted text chunks directly as labels. Second is to use the corresponding senses as labels (and therefore group multiple text labels). In the following we refer\nto these as text- and sense-labels. Thus from each sentence we extract a semantic representation in a form of (SUBJECT, VERB, OBJECT, LOCATION)."}, {"heading": "4.1.2 SMT", "text": "For the sentence generation we build on the two-step translation approach of (Rohrbach et al. 2013). As the first step it learns a mapping from the visual input to the semantic representation (SR), modeling pairwise dependencies in a CRF using visual classifiers as unaries. The unaries are trained using an SVM on dense trajectories (Wang and Schmid 2013). In the second step it translates the SR to a sentence using Statistical Machine Translation (SMT) (Koehn et al. 2007). For this the approach uses a concatenated SR as input language, e.g. cut knife tomato, and natural sentence as output language, e.g. The person slices the tomato. We obtain the SR automatically from the semantic parser, as described above, Section 4.1.1. In addition to dense trajectories we use the features described in Section 3.1.4.\n4.2 Visual labels + LSTM\nNext we present our two-step LSTM-based approach. The first step performs visual recognition using the visual classifiers which we train according to labels\u2019 semantics and \u201cvisuality\u201d. The second step generates textual descriptions using an LSTM network (see Figure 5(b)). We explore various design choices for building and training the LSTM."}, {"heading": "4.2.1 Robust visual classifiers", "text": "For training we rely on a parallel corpus of videos and weak sentence annotations. As before (see Section 4.1) we parse the sentences to obtain a set of labels (single\nwords or short phrases, e.g. look up) to train visual classifiers. However, this time we aim to select the most visual labels which can be robustly recognized. In order to do that we take three steps.\nAvoiding parser failure. Not all sentences can be parsed successfully, as e.g. some sentences are incomplete or grammatically incorrect. To avoid loosing the potential labels in these sentences, we match our set of initial labels to the sentences which the parser failed to process.\nSemantic groups. Our labels correspond to different semantic groups. In this work we consider three most important groups: verbs, objects and places. We propose to treat each label group independently. First, we rely on a different representation for each semantic group, which is targeted to the specific group. Namely we use the activity recognition features Improved Dense Trajectories (DT) for verbs, LSDA scores for objects and PLACES-CNN scores for places. Second, we train onevs-all SVM classifiers for each group separately. The intuition behind this is to avoid \u201cwrong negatives\u201d (e.g. using object \u201cbed\u201d as negative for place \u201cbedroom\u201d).\nVisual labels. Now, how do we select visual labels for our semantic groups? In order to find the verbs among the labels we rely on our semantic parser (Section 4.1.1). Next, we look up the list of \u201cplaces\u201d used in (Zhou et al. 2014) and search for corresponding words among our labels. We look up the object classes used in (Hoffman et al. 2014) and search for these \u201cobjects\u201d, as well as their base forms (e.g. \u201cdomestic cat\u201d and \u201ccat\u201d). We discard all the labels that do not belong to any of our three groups of interest as we assume that they are likely not visual and thus are difficult to recognize. Finally, we discard labels which the classifiers could not learn reliably, as these are likely noisy or not visual. For this we require the classifiers to have certain minimum area under the ROC-curve (Receiver Operating Characteristic)."}, {"heading": "4.2.2 LSTM for sentence generation", "text": "We rely on the basic LSTM architecture proposed in (Donahue et al. 2015) for video description. At each time step an LSTM generates a word and receives the visual classifiers (input-vis) as well as as the previous generated word (input-lang) as input (see Figure 6(a)). We encode each word with a one-hot-vector according to its index in a dictionary and project it in a lower dimensional embedding. The embedding is jointly learned during training of the LSTM. We feed in the classifier scores as input to the LSTM which is equivalent to the best variant proposed in (Donahue et al. 2015). We analyze the following aspects for this architecture:\nLayer structure. We compare a 1-layer architecture with a 2-layer architecture. In the 2-layer architecture, the output of the first layer is used as input for the second layer (Figure 6b) and was used by (Donahue et al. 2015) for video description. Additionally we also compare to a 2-layer factored architecture of (Donahue et al. 2015), where the first layer only gets the language as input and the second layer gets the output of the first as well as the visual input.\nDropout placement. To learn a more robust network which is less likely to overfit we rely on a dropout (Hinton et al. 2012), i.e. a ratio r of randomly selected units is set to 0 during training (while all others are multiplied with 1/r). We explore different ways to place dropout in the network, i.e. either for language input (lang-drop) or visual (vis-drop) input only, for both inputs (concat-drop) or for the LSTM output (lstm-drop), see Figure 6(d)."}, {"heading": "5 Evaluation", "text": "In this section we provide more insights about our movie description datasets. First we compare ADs to movie scripts and then benchmark the approaches to video\nCorrectness Relevance\nMovie scripts 33.9 (11.2) 33.4 (16.8 ) ADs 66.1 (35.7) 66.6 (44.9 )\nTable 5 Human evaluation of movie scripts and ADs: which sentence is more correct/relevant with respect to the video (forced choice). Majority vote of 5 judges in %. In brackets: at least 4 of 5 judges agree. See also Section 5.1.\ndescription introduced in Section 4 as well as other related work.\n5.1 Comparison of AD vs script data\nWe compare the AD and script data using 11 movies from the MPII-MD dataset where both are available (see Section 3.1.2). For these movies we select the overlapping time intervals with an intersection over union overlap of at least 75%, which results in 279 sentence pairs, we remove 2 pairs which have idendical sentences. We ask humans via Amazon Mechanical Turk (AMT) to compare the sentences with respect to their correctness and relevance to the video, using both video intervals as a reference (one at a time). Each task was completed by 5 different human subjects, covering 2770 tasks done in total. Table 5 presents the results of this evaluation. AD is ranked as more correct and relevant in about 2/3 of the cases, which supports our intuition that scrips contain mistakes and irrelevant content even after being cleaned up and manually aligned.\n5.2 Semantic parser evaluation\nTable 6 reports the accuracy of individual components of the semantic parsing pipeline. The components are clause splitting (Clause), POS tagging and chunking (NLP), semantic role labeling (Roles) and word sense disambiguation (WSD). We manually evaluate the correctness on a randomly sampled set of sentences using human judges. It is evident that the poorest performing parts are the NLP and the WSD components. Some of the NLP mistakes arise due to incorrect POS tagging. WSD is considered a hard problem and when the dataset contains less frequent words, the performance is severely affected.\n5.3 Evaluation metrics for description\nIn this section we describe how we evaluate the generated descriptions using automatic and human evaluation.\nCorpus Clause NLP Roles WSD\nMPII-MD 0.89 0.62 0.86 0.7\nTable 6 Semantic parser accuracy on MPII-MD. Discussion in Section 5.2."}, {"heading": "5.3.1 Automatic metrics", "text": "For automatic evaluation we rely on the MS COCO Caption Evaluation API15. The automatic evaluation measures include BLEU-1,-2,-3,-4 (Papineni et al. 2002), METEOR (Denkowski and Lavie 2014), ROUGE-L (Lin 2004), and CIDEr (Vedantam et al. 2015). While we report all measures for the final evaluation in the LSMDC (Section 6), we focus our discussion on METEOR score in the preliminary evaluations in this section. According to (Elliott and Keller 2013; Vedantam et al. 2015), METEOR supersedes previously used measures such as BLEU or ROUGE in terms of agreement with human judgments. METEOR also outperforms CIDEr when the number of references is small and in the case of the movie description data we have only a single reference."}, {"heading": "5.3.2 Human evaluation", "text": "For the human evaluation we rely on a ranking approach, i.e. human judges are given multiple descriptions from different systems, and are asked to rank them with respect to the following criteria: correctness, relevance, and grammar, motivated by prior work Rohrbach et al. (2013) and on the other hand we asked human judges to rank sentences for \u201chow helpful they would be for a blind person to understand what is happening in the movie\u201d. The AMT workers are given randomized sentences, and, in addition to some general instruction, the following definitions:\nGrammar. \u201cRank grammatical correctness of sentences: Judge the fluency and readability of the sentence (independently of the correctness with respect to the video).\u201d\nCorrectness. \u201cRank correctness of sentences: For which sentence is the content more correct with respect to the video (independent if it is complete, i.e. describes everything), independent of the grammatical correctness.\u201d\nRelevance. \u201cRank relevance of sentences: Which sentence contains the more salient (i.e. relevant, important) events/objects of the video?\u201d\n15 https://github.com/tylin/coco-caption\nMETEOR\nSMT with our sense-labels IDT 30 4.93 IDT 100 5.12 Combi 100 5.19\nSMT with our text-labels IDT 30 5.59 IDT 100 5.51 Combi 100 5.42\nTable 7 Video description performance of different SMT versions on MPII-MD. Discussion in Section 5.4.1.\nHelpful for the blind. In the LSMDC evaluation we introduced a new measure, which should capture how useful a description would be for blind people:\n\u201cRank the sentences according to how useful they would be for a blind person which would like to understand/follow the movie without seeing it.\u201d\n5.4 Movie description evaluation\nAs the collected text data comes from the movie context, it contains a lot of information specific to the plot, such as names of the characters. We pre-process each sentence in the corpus, transforming the names to \u201cSomeone\u201d or \u201cpeople\u201d (in case of plural).\nWe first analyze the performance of the proposed approaches on the MPII-MD dataset, and then evaluate the best version on the M-VAD dataset. For MPIIMD we split the 11 movies with associated scripts and ADs (in total 22 alignments, see Section 3.1.2) into validation set (8) and test set (14). The other 83 movies are used for training. On M-VAD we use 10 movies for testing, 10 for validation and 72 for training."}, {"heading": "5.4.1 Semantic parsing + SMT", "text": "Table 7 summarizes results of multiple variants of the SMT approach when using the SR from our semantic parser. \u201cCombi\u201d refers to combining IDT, HYBRID, and PLACES as unaries in the CRF. We did not add LSDA as we found that it reduces the performance of the CRF. After extracting the labels we select the ones which appear at least 30 or 100 times as our visual attributes. Overall, we observe similar performance in all cases, with slightly better results for text-labels than sense-labels. This can be attributed to sense disambiguation errors of the semantic parser. In the following we use the \u201cIDT 30\u201d model, which achieves the highest score of 5.59, and denote it as \u201cSMT-Best\u201d16.\n16 We also evaluated the \u201cSemantic parsing+SMT\u201d approach on a corpus where annotated SRs are available,"}, {"heading": "5.4.2 Visual labels + LSTM", "text": "We start with exploring different design choices of our approach. We build on the labels discovered by the semantic parser. To learn classifiers we select the labels that appear at least 30 times, resulting in 1,263 labels. The parser additionally tells us whether the label is a verb. The LSTM output/hidden unit as well as memory cell have each 500 dimensions.\nRobust visual classifiers. We first analyze our proposal to consider groups of labels to learn different classifiers and also to use different visual representations for these groups (see Section 4.2). In Table 8 we evaluate our generated sentences using different input features to the LSTM on the validation set of MPII-MD. In our baseline, in the top part of Table 8, we use the same visual descriptors for all labels. The PLACES feature is best with 7.10 METEOR. Combination by stacking all features (IDT + LSDA + PLACES) improves further to 7.24 METEOR. The second part of the table demonstrates the effect of introducing different semantic label groups. We first split the labels into \u201cVerbs\u201d and all others. Given that some labels appear in both roles, the total number of labels increases to 1328 (line 5). We compare two settings of training the classifiers: \u201cRetrieved\u201d (we retrieve the classifier scores from the classifiers trained in the previous step), \u201cTrained\u201d (we train the SVMs specifically for each label type, e.g. \u201cVerbs\u201d). Next, we further divide the non-\u201dVerb\u201d labels into \u201cPlaces\u201d and \u201cOthers\u201d(line 6), and finally into \u201cPlaces\u201d and \u201cObjects\u201d(line 7). We discard the unused labels and end up with 913 labels. Out of these labels, we select the labels where the classifier obtains a ROC higher or equal to 0.7 (threshold selected on the validation set). After this we obtain 263 labels and the best performance in the \u201cTrained\u201d setting (line 8). To support our intuition about the importance of the label discrimination (i.e. using different features for different semantic groups of labels), we propose another baseline (line 9). Here we use the same set of 263 labels but provide the same feature for all of them, namely the best performing combination IDT + LSDA + PLACES. As we see, this results in an inferior performance.\nWe make several observations from Table 8 which lead to robust visual classifiers from the weak sentence annotations. a) It is beneficial to select features based on the label semantics. b) Training one-vs-all SVMs for specific label groups consistently improves the performance as it avoids \u201cwrong\u201d negatives. c) Focusing on\nnamely TACoS Multi-Level (Rohrbach et al. 2014), and showed the comparable performance to manually annotated SRs, see (Rohrbach et al. 2015c).\nmore \u201cvisual\u201d labels helps: we reduce the LSTM input dimensionality to 263 while improving the performance.\nLSTM architectures. Now, as described in Section 4.2.2, we look at different LSTM architectures and training configurations. In the following we use the best performing \u201cVisual Labels\u201d approach, Table 8, line (8).\nWe start with examining the architecture, where we explore different configurations of LSTM and dropout layers. Table 9(a) shows the performance of three different networks: \u201c1 layer\u201d, \u201c2 layers unfactored\u201d and \u201c2 layers factored\u201d introduced in Section 4.2.2. As we see, the \u201c1 layer\u201d and \u201c2 layers unfactored\u201d perform equally well, while \u201c2 layers factored\u201d is inferior to them. In the following experiments we use the simpler \u201c1 layer\u201d network. We then compare different dropout placements as illustrated in (Table 9(b)). We obtain the best result when applying dropout after the LSTM layer (\u201clstmdrop\u201d), while having no dropout or applying it only to language leads to stronger over-fitting to the visual features. Putting dropout after the LSTM (and prior to a final prediction layer) makes the entire system more robust. As for the best dropout ratio, we find that 0.5 works best with lstm-dropout (Table 9(c)).\nIn most of the experiments we trained our networks for 25,000 iterations. After looking at the METEOR\nscores for intermediate iterations we found that at iteration 15,000 we achieve best performance overall. Additionally we train multiple LSTMs with different random orderings of the training data. In our experiments we combine three in an ensemble, averaging the resulting word predictions.\nTo summarize, the most important aspects that decrease over-fitting and lead to better sentence generation are: (a) a correct learning rate and step size, (b) dropout after the LSTM layer, (c) choosing the training iteration based on METEOR score as opposed to only looking at the LSTM accuracy/loss which can be misleading, and (d) building ensembles of multiple networks with different random initializations17."}, {"heading": "5.4.3 Comparison to related work", "text": "Experimental setup. In this section we evaluate on the test set of the MPII-MD dataset (6,578 clips) and MVAD dataset (4,951 clips). We use METEOR for automatic evaluation and we perform a human evaluation on a random subset of 1,300 video clips, see Section 5.3 for details. We train our method on M-VAD and use the same LSTM architecture and parameters as for MPII-\n17 More details can be found in our corresponding arXiv version Rohrbach et al. (2015a)\nMD, but select the number of iterations on the M-VAD validation set.\nResults on MPII-MD. Table 10(a) summarizes the results on the test set of MPII-MD. Here we additionally include the results from a nearest neighbor baseline, i.e. we retrieve the closest sentence from the training corpus using L1-normalized visual features and the intersection distance. When comparing the different features (introduced in Section 3.1.4), we see that the pretrained features (LSDA, PLACES, HYBRID) perform better than IDT, with HYBRID performing best. Our SMT-Best approach clearly improves over the nearest neighbor baselines. With our Visual-Labels approach we again significantly improve the performance, specifically by 1.44 METEOR points. Moreover, we improve over the recent approach of (Venugopalan et al. 2015a),\nwhich also uses an LSTM to generate video descriptions. Exploring different strategies to label selection and classifier training, as well as various LSTM configurations allows to obtain better result than prior on the MPII-MD dataset. Human evaluation mainly agrees with the automatic measure. Visual-Labels outperforms both other methods in terms of Correctness and Relevance, however it loses to S2VT in terms of Grammar. This is due to the fact that S2VT produces overall shorter (7.4 versus 8.7 words per sentence) and simpler sentences, while our system generates longer sentences and therefore has higher chances to make mistakes. We also propose a retrieval upperbound. For every test sentence we retrieve the closest training sentence according to the METEOR score. The rather low METEOR score of 19.43 reflects the difficulty of the dataset. We show some qualitative results in Figure 7.\nResults on M-VAD. Table 10(b) shows the results on the test set of M-VAD dataset. Our Visual-Labels method outperforms S2VT (Venugopalan et al. 2015a) and Temporal attention (Yao et al. 2015) in METEOR score. As we see, the results agree with Table 10(a), but are consistently lower, suggesting that M-VAD is more challenging than MPII-MD. We attribute this to more precise manual alignments of the MPII-MD dataset.\n5.5 Movie description analysis\nDespite the recent advances in the video description task, the performance on the movie description datasets (MPII-MD and M-VAD) remains rather low. In this section we want to look closer at three methods, SMTBest, S2VT and Visual-Labels, in order to understand where these methods succeed and where they fail. In the following we evaluate all three methods on the MPIIMD test set."}, {"heading": "5.5.1 Difficulty versus performance", "text": "As the first study we suggest to sort the test reference sentences by difficulty, where difficulty is defined in multiple ways17.\nSentence length and Word frequency. Some of the intuitive sentence difficulty measures are its length and average frequency of its words. When sorting the data by difficulty (increasing sentence length or decreasing average word frequency), we find that all three methods have the same tendency to obtain lower METEOR score as the difficulty increases. Figure 9(a) shows the performance of compared methods w.r.t. the sentence length. For the word frequency the correlation is even stronger, see Figure 9(b). Visual-Labels consistently outperforms the other two methods, most notable as the difficulty increases."}, {"heading": "5.5.2 Semantic analysis", "text": "WordNet Verb Topics. Next we analyze the test reference sentences w.r.t. verb semantics. We rely on WordNet Topics (high level entries in the WordNet ontology), e.g. \u201cmotion\u201d, \u201cperception\u201d, defined for most synsets in WordNet (Fellbaum 1998). Sense information comes from our automatic semantic parser, thus it might be noisy. We showcase the 3 most frequent verbs for each Topic in Table 11. We select sentences with a single verb, group them according to the verb Topic and compute an average METEOR score for each Topic, see Figure 8. We find that Visual-Labels is best for all Topics except \u201ccommunication\u201d, where SMT-Best wins. The most frequent verbs there are \u201clook up\u201d and \u201cnod\u201d,\nwhich are also frequent in the dataset and in the sentences produced by SMT-Best. The best performing Topic, \u201ccognition\u201d, is highly biased to \u201clook at\u201d verb. The most frequent Topics, \u201cmotion\u201d and \u201ccontact\u201d, which are also visual (e.g. \u201cturn\u201d, \u201cwalk\u201d, \u201csit\u201d), are nevertheless quite challenging, which we attribute to their high diversity (see their entropy w.r.t. different verbs and their frequencies in Table 11). Topics with more abstract verbs (e.g. \u201cbe\u201d, \u201chave\u201d, \u201cstart\u201d) get lower scores.\nTop 100 best and worst sentences. We look at 100 test reference sentences, where Visual-Labels obtains highest and lowest METEOR scores. Out of 100 best sentences 44 contain the verb \u201clook\u201d (including phrases such as \u201clook at\u201d). The other frequent verbs are \u201cwalk\u201d, \u201cturn\u201d, \u201csmile\u201d, \u201cnod\u201d, \u201cshake\u201d, i.e. mainly visual verbs. Overall the sentences are simple. Among the worst 100 sentences we observe more diversity: 12 contain no verb,\n10 mention unusual words (specific to the movie), 24 have no subject, 29 have a non-human subject. This leads to a lower performance, in particular, as most training sentences contain \u201cSomeone\u201d as subject and generated sentences are biased towards it.\nSummary. a) The test reference sentences that mention verbs like \u201clook\u201d get higher scores due to their high frequency in the dataset. b) The sentences with more \u201cvisual\u201d verbs tend to get higher scores. c) The sentences without verbs (e.g. describing a scene), without subjects or with non-human subjects get lower scores, which can be explained by dataset biases.\n6 The Large Scale Movie Description Challenge 2015\nThe Large Scale Movie Description Challenge (LSMDC) was held in conjunction with ICCV 2015. For the automatic evaluation we set up an evaluation server3. During the first phase of the challenge the participants could evaluate the outputs of their system on the public test set of LSMDC dataset. In the second phase of the challenge the participants were provided with the videos from the blind test set (without textual descriptions). These were used for the final evaluation. To measure performance of the competing approaches we performed both automatic and human evaluation as described in Section 5.3. The submission format was similar to the MS COCO Challenge (Chen et al. 2015) and we also used the identical automatic evaluation protocol. The challenge winner was determined based on the human evaluation. The human evaluation was\nperformed over 1,200 randomly selected clips from the blind test set of LSMDC.\n6.1 LSMDC quantitative results\nWe received 4 submissions to the challenge, including our Visual-Labels approach. The other submissions are S2VT (Venugopalan et al. 2015b), Temporal Attention (Yao et al. 2015) and Frame-Video-Concept Fusion (Shetty and Laaksonen 2015). We first look at the results of the automatic evaluation on the blind test set of LSMDC in Table 12. As we can see the Visual-Labels approach obtains highest scores in all evaluation measures except BLEU-1,-2 where S2VT gets highest score. While Visual-Labels get 7.1 METEOR and S2VT 7.0 METEOR, Frame-Video-Concept Fusion drops to 6.1 METEOR and Temporal Attention to 5.2 METEOR. One\nreason for this drop in the automatic measures is the sentence length, which is much smaller for Frame-VideoConcept Fusion and Temporal Attention compared to the reference sentences, as we discuss in more details below (see also Table 14).\nNext, we look at the results of human evaluation in Table 13. As known from literature (Chen et al. 2015; Elliott and Keller 2013; Vedantam et al. 2015), automatic evaluation measures do not always agree with the human evaluation. Here we see that human judges prefer the descriptions from Frame-Video-Concept Fusion approach in terms of correctness, grammar and relevance. In our alternative evaluation, in terms of being helpful for the blind, Visual-Labels wins. Possible explanation for it is that in this evaluation criteria human judges penalized less the errors in the descriptions but rather looked at their overall informativeness. In\ngeneral, the gap between different approaches is not large. Based on the human evaluation the winner of the challenge is Frame-Video-Concept Fusion approach of Shetty and Laaksonen (2015).\nWe closer analyze the outputs of the compared approaches in Table 14, providing detailed statistics over the generated descriptions. With respect to the sentence length, Visual-Labels and S2VT demonstrate similar properties to the reference descriptions, while the approaches Frame-Video-Concept Fusion and Temporal Attention generate much shorter sentences (5.16 and 3.63 words on average vs. 8.74 of the references). In terms of vocabulary size all approaches fall far below the reference descriptions. This large gap indicates a problem in that all the compared approaches focus on a rather small set of visual and language concepts, ignoring a long tail in the distribution. The number of unique sentences confirms the previous finding, showing slightly higher numbers for Visual-Labels and S2VT, while the other two tend to frequently generate the same description for different clips. Finally, the percentage of novel sentences (not present among the training descriptions) highlights another aspect, namely the amount of novel vs. retrieved descriptions. As we see, all the methods \u201cretrieve\u201d some amount of descriptions from training data, while the approach Temporal Attention produces only 7.36% novel sentences.\n6.2 LSMDC qualitative results\nFigure 10 shows qualitative results from the competing approaches. The first two examples are success cases, where most of the approaches are able to describe the video correctly. The third example is an interesting case where visually relevant descriptions, provided by most approaches, do not match the reference description, which focuses on an action happening in the background of the scene (\u201cSomeone sets down his young daughter then moves to a small wooden table.\u201d). The last two rows contain partial and complete failures. In one all approaches fail to recognize the person running away, only capturing the \u201cturning\u201d action which indeed happened before running. In the other one, all approaches fail to recognize that the woman interacts with the small object (phone).\nFigure 11 further compares all approaches on a sequence of 5 consecutive clips. We can make the following observations from these examples. Indeed VisualLabels and S2VT produce longer and noisier descriptions. Sometimes, though, Visual-Labels is able to capture important details, such as \u201cSomeone sips his drink\u201d, which the other methods fail to recognize. Descriptions produced by Frame-Video-Concept Fusion and Temporal Attention are short and cleaner, also Temporal Attention tends to produce generally applicable sentences, e.g. \u201cSomeone looks at someone\u201d.\n6.3 LSMDC summary\nWe make the following conclusions from the presented results. (1) Visual-Labels and S2VT tend to generate longer, more diverse and novel descriptions, which leads to lower human rankings, due to higher error chances. (2) Temporal Attention and Frame-Video-Concept Fusion produce shorter and simpler descriptions and retrieve more from training data, and thus obtain better human rankings. (3) Frame-Video-Concept Fusion lands in the \u201csweet spot\u201d, in terms of sentence correctness and complexity. (4) Asking humans a different question, how helpful the descriptions would be for the blind, we see that the ranking changes, indicating that human judgments are sensitive to the question formulation. In the future we plan to experiment more with different evaluation criteria."}, {"heading": "7 Conclusion", "text": "In this work we present the Large Scale Movie Description Challenge (LSMDC), a novel dataset of movies with aligned descriptions sourced from movie scripts and ADs (audio descriptions for the blind, also referred to as DVS). Altogether the dataset is based on 202 movies and has 118,114 sentences with aligned clips. We compare AD with previously used script data and find that AD tends to be more correct and relevant to the movie than script sentences.\nOur approach, Visual-Labels, to automatic movie description trains visual classifiers and uses their scores as input to an LSTM. To handle the weak sentence annotations we rely on three ingredients. (1) We distinguish three semantic groups of labels (verbs, objects and places). (2) We train them separately, removing the noisy negatives. (3) We select only the most reliable classifiers. For sentence generation we show the benefits of exploring different LSTM architectures and learning configurations.\nTo evaluate different approaches for movie description, we organized a challenge at ICCV 2015 where we evaluated submissions using automatic and human evaluation criteria. We found that the approaches S2VT and our Visual-Labels generate longer and more diverse description than the other submissions but are also more susceptible to content or grammatical errors. This consequently leads to worse human rankings with respect to correctness and grammar. In contrast, FrameVideo-Concept Fusion wins the challenge by predicting medium length sentences with intermediate diversity, which gets rated best in human evaluation for correctness, grammar, and relevance. When ranking sentences with respect to the criteria \u201chelpful for the blind\u201d, our\nVisual-Labels is well received by human judges, likely because it includes important aspects provided by the strong visual labels. Overall all approaches have problems with the challenging long-tail distributions of our data. Additional training data cannot fully ameliorate this problem because a new movie might always contain novel parts. We expect new techniques, including relying on different modalities, see e.g. (Hendricks et al. 2016), to overcome this challenge.\nOur evaluation server will continue to be available for automatic evaluation and we will hold a second version of our challenge with human evaluation at ECCV 2016 as part of the Joint Workshop on Storytelling with Images and Videos and Large Scale Movie Description and Understanding Challenge. Our dataset has already been used beyond description, e.g. for learning videosentence embeddings or for movie question answering. Beyond our current challenge on single sentences, the dataset opens new possibilities to understand stories and plots across multiple sentences in an open domain scenario on large scale.\nAcknowledgements. Marcus Rohrbach was supported by a fellowship within the FITweltweit-Program of the German Academic Exchange Service (DAAD)."}], "references": [{"title": "The berkeley framenet project", "author": ["Collin F. Baker", "Charles J. Fillmore", "John B. Lowe"], "venue": "In Proceedings of the Annual Meeting of the Association for Computational Linguistics (ACL),", "citeRegEx": "Baker et al\\.,? \\Q1998\\E", "shortCiteRegEx": "Baker et al\\.", "year": 1998}, {"title": "Finding actors and actions in movies", "author": ["Piotr Bojanowski", "Francis Bach", "Ivan Laptev", "Jean Ponce", "Cordelia Schmid", "Josef Sivic"], "venue": "Intelligence", "citeRegEx": "Bojanowski et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Bojanowski et al\\.", "year": 2012}, {"title": "Weakly supervised action labeling in videos under ordering constraints", "author": ["Piotr Bojanowski", "R\u00e9mi Lajugie", "Francis Bach", "Ivan Laptev", "Jean Ponce", "Cordelia Schmid", "Josef Sivic"], "venue": "In European Conference on Computer Vision (ECCV),", "citeRegEx": "Bojanowski et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Bojanowski et al\\.", "year": 2014}, {"title": "Collecting highly parallel data for paraphrase evaluation", "author": ["David Chen", "William Dolan"], "venue": "In Proceedings of the Annual Meeting of the Association for Computational Linguistics (ACL),", "citeRegEx": "Chen and Dolan.,? \\Q2011\\E", "shortCiteRegEx": "Chen and Dolan.", "year": 2011}, {"title": "Microsoft coco captions: Data collection and evaluation", "author": ["Xinlei Chen", "Hao Fang", "Tsung-Yi Lin", "Ramakrishna Vedantam", "Saurabh Gupta", "Piotr Doll\u00e1r", "C Lawrence Zitnick"], "venue": "server. arXiv:1504.00325,", "citeRegEx": "Chen et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Chen et al\\.", "year": 2015}, {"title": "Movie/script: Alignment and parsing of video and text transcription", "author": ["Timoth\u00e9e Cour", "Chris Jordan", "Eleni Miltsakaki", "Ben Taskar"], "venue": "In European Conference on Computer Vision (ECCV),", "citeRegEx": "Cour et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Cour et al\\.", "year": 2008}, {"title": "Learning from ambiguously labeled images", "author": ["Timothee Cour", "Benjamin Sapp", "Chris Jordan", "Ben Taskar"], "venue": "In Conference on Computer Vision and Pattern Recognition (CVPR),", "citeRegEx": "Cour et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Cour et al\\.", "year": 2009}, {"title": "An exact dual decomposition algorithm for shallow semantic parsing with constraints", "author": ["Dipanjan Das", "Andr\u00e9 F.T. Martins", "Noah A. Smith"], "venue": "In Proceedings of the Annual Meeting of the Association for Computational Linguistics (ACL),", "citeRegEx": "Das et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Das et al\\.", "year": 2012}, {"title": "Thousand frames in just a few words: Lingual description of videos through latent topics and sparse object stitching", "author": ["Pradipto Das", "Chenliang Xu", "Richard Doell", "Jason Corso"], "venue": "In Conference on Computer Vision and Pattern Recognition (CVPR),", "citeRegEx": "Das et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Das et al\\.", "year": 2013}, {"title": "Seeing is believing: The quest for multimodal knowledge", "author": ["Gerard de Melo", "Niket Tandon"], "venue": "SIGWEB Newsl.,", "citeRegEx": "Melo and Tandon.,? \\Q2016\\E", "shortCiteRegEx": "Melo and Tandon.", "year": 2016}, {"title": "Clausie: Clausebased open information extraction", "author": ["Luciano Del Corro", "Rainer Gemulla"], "venue": "In Proceedings of the International World Wide Web Conference (WWW),", "citeRegEx": "Corro and Gemulla.,? \\Q2013\\E", "shortCiteRegEx": "Corro and Gemulla.", "year": 2013}, {"title": "Imagenet: A large-scale hierarchical image database", "author": ["Jia Deng", "Wei Dong", "Richard Socher", "Li-Jia Li", "Kai Li", "Li Fei-Fei"], "venue": "In Conference on Computer Vision and Pattern Recognition (CVPR),", "citeRegEx": "Deng et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Deng et al\\.", "year": 2009}, {"title": "Meteor universal: Language specific translation evaluation for any target language", "author": ["Michael Denkowski", "Alon Lavie"], "venue": "In Proceedings of the Ninth Workshop on Statistical Machine Translation,", "citeRegEx": "Denkowski and Lavie.,? \\Q2014\\E", "shortCiteRegEx": "Denkowski and Lavie.", "year": 2014}, {"title": "Automatic annotation of human actions in video", "author": ["Olivier Duchenne", "Ivan Laptev", "Josef Sivic", "Francis Bach", "Jean Ponce"], "venue": "In International Conference on Computer Vision (ICCV),", "citeRegEx": "Duchenne et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Duchenne et al\\.", "year": 2009}, {"title": "Image description using visual dependency representations", "author": ["Desmond Elliott", "Frank Keller"], "venue": "In Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP),", "citeRegEx": "Elliott and Keller.,? \\Q2013\\E", "shortCiteRegEx": "Elliott and Keller.", "year": 2013}, {"title": "hello! my name is... buffy\u201d - automatic naming of characters in tv video", "author": ["Mark Everingham", "Josef Sivic", "Andrew Zisserman"], "venue": "In Proceedings of the British Machine Vision Conference (BMVC),", "citeRegEx": "Everingham et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Everingham et al\\.", "year": 2006}, {"title": "From captions to visual concepts and back", "author": ["nick", "Geoffrey Zweig"], "venue": "In Conference on Computer Vision and Pattern Recognition (CVPR),", "citeRegEx": "nick and Zweig.,? \\Q2015\\E", "shortCiteRegEx": "nick and Zweig.", "year": 2015}, {"title": "Every picture tells a story: Generating sentences from images", "author": ["Ali Farhadi", "Mohsen Hejrati", "M.A. Sadeghi", "Peter Young", "C. Rashtchian", "Julia Hockenmaier", "D.A. Forsyth"], "venue": "In European Conference on Computer Vision (ECCV),", "citeRegEx": "Farhadi et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Farhadi et al\\.", "year": 2010}, {"title": "WordNet: An Electronic Lexical Database", "author": ["Christiane Fellbaum"], "venue": null, "citeRegEx": "Fellbaum.,? \\Q1998\\E", "shortCiteRegEx": "Fellbaum.", "year": 1998}, {"title": "Improving neural networks by preventing co-adaptation of feature detectors", "author": ["Geoffrey E Hinton", "Nitish Srivastava", "Alex Krizhevsky", "Ilya Sutskever", "Ruslan R Salakhutdinov"], "venue": null, "citeRegEx": "Hinton et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Hinton et al\\.", "year": 2012}, {"title": "Long short-term memory", "author": ["Sepp Hochreiter", "J\u00fcrgen Schmidhuber"], "venue": "Neural Computation,", "citeRegEx": "Hochreiter and Schmidhuber.,? \\Q1997\\E", "shortCiteRegEx": "Hochreiter and Schmidhuber.", "year": 1997}, {"title": "From image descriptions to visual denotations: New similarity metrics for semantic inference over event descriptions", "author": ["Peter Hodosh", "Alice Young", "Micah Lai", "Julia Hockenmaier"], "venue": "In Transactions of the Association for Computational Linguistics (TACL),", "citeRegEx": "Hodosh et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Hodosh et al\\.", "year": 2014}, {"title": "LSDA: Large scale detection through adaptation", "author": ["Judy Hoffman", "Sergio Guadarrama", "Eric Tzeng", "Jeff Donahue", "Ross Girshick", "Trevor Darrell", "Kate Saenko"], "venue": "In Conference on Neural Information Processing Systems (NIPS),", "citeRegEx": "Hoffman et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Hoffman et al\\.", "year": 2014}, {"title": "Deep visual-semantic alignments for generating image descriptions", "author": ["Andrej Karpathy", "Li Fei-Fei"], "venue": "In Conference on Computer Vision and Pattern Recognition (CVPR),", "citeRegEx": "Karpathy and Fei.Fei.,? \\Q2015\\E", "shortCiteRegEx": "Karpathy and Fei.Fei.", "year": 2015}, {"title": "Extending verbnet with novel verb classes", "author": ["Karen Kipper", "Anna Korhonen", "Neville Ryant", "Martha Palmer"], "venue": "In Proceedings of the International Conference on Language Resources and Evaluation (LREC),", "citeRegEx": "Kipper et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Kipper et al\\.", "year": 2006}, {"title": "Multimodal neural language models", "author": ["Ryan Kiros", "Ruslan Salakhutdinov", "Richard Zemel"], "venue": "In International Conference on Machine Learning (ICML),", "citeRegEx": "Kiros et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Kiros et al\\.", "year": 2014}, {"title": "Unifying visual-semantic embeddings with multimodal neural language models", "author": ["Ryan Kiros", "Ruslan Salakhutdinov", "Richard S. Zemel"], "venue": "Transactions of the Association for Computational Linguistics (TACL),", "citeRegEx": "Kiros et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Kiros et al\\.", "year": 2015}, {"title": "Natural language description of human activities from video images based on concept hierarchy of actions", "author": ["Atsuhiro Kojima", "Takeshi Tamura", "Kunio Fukunaga"], "venue": "International Journal of Computer Vision (IJCV),", "citeRegEx": "Kojima et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Kojima et al\\.", "year": 2002}, {"title": "Imagenet classification with deep convolutional neural networks", "author": ["Alex Krizhevsky", "Ilya Sutskever", "Geoffrey E. Hinton"], "venue": "In Conference on Neural Information Processing Systems (NIPS),", "citeRegEx": "Krizhevsky et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Krizhevsky et al\\.", "year": 2012}, {"title": "Baby talk: Understanding and generating simple image descriptions", "author": ["Girish Kulkarni", "Visruth Premraj", "Sagnik Dhar", "Siming Li", "Yejin Choi", "Alexander C. Berg", "Tamara L. Berg"], "venue": "In Conference on Computer Vision and Pattern Recognition (CVPR),", "citeRegEx": "Kulkarni et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Kulkarni et al\\.", "year": 2011}, {"title": "Collective generation of natural image descriptions", "author": ["Polina Kuznetsova", "Vicente Ordonez", "Alexander C Berg", "Tamara L Berg", "Yejin Choi"], "venue": "In Proceedings of the Annual Meeting of the Association for Computational Linguistics (ACL),", "citeRegEx": "Kuznetsova et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Kuznetsova et al\\.", "year": 2012}, {"title": "Treetalk: Composition and compression of trees for image descriptions", "author": ["Polina Kuznetsova", "Vicente Ordonez", "Tamara L Berg", "UNC Chapel Hill", "Yejin Choi"], "venue": "In Transactions of the Association for Computational Linguistics (TACL),", "citeRegEx": "Kuznetsova et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Kuznetsova et al\\.", "year": 2014}, {"title": "The semi-automatic generation of audio description from screenplays", "author": ["Lakritz", "Salway"], "venue": "Technical report, Dept. of Computing Technical Report, University of Surrey,", "citeRegEx": "Lakritz and Salway.,? \\Q2006\\E", "shortCiteRegEx": "Lakritz and Salway.", "year": 2006}, {"title": "Learning realistic human actions from movies", "author": ["Ivan Laptev", "Marcin Marszalek", "Cordelia Schmid", "Benjamin Rozenfeld"], "venue": "In Conference on Computer Vision and Pattern Recognition (CVPR),", "citeRegEx": "Laptev et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Laptev et al\\.", "year": 2008}, {"title": "Summarization-based video caption via deep neural networks", "author": ["Guang Li", "Shubo Ma", "Yahong Han"], "venue": "In Proceedings of the 23rd Annual ACM Conference on Multimedia Conference,", "citeRegEx": "Li et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Li et al\\.", "year": 2015}, {"title": "Tgif: A new dataset and benchmark on animated gif description", "author": ["Yuncheng Li", "Yale Song", "Liangliang Cao", "Joel Tetreault", "Larry Goldberg", "Alejandro Jaimes", "Jiebo Luo"], "venue": "In Conference on Computer Vision and Pattern Recognition (CVPR),", "citeRegEx": "Li et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Li et al\\.", "year": 2016}, {"title": "Tvparser: An automatic tv video parsing method", "author": ["Chao Liang", "Changsheng Xu", "Jian Cheng", "Hanqing Lu"], "venue": "In Conference on Computer Vision and Pattern Recognition (CVPR),", "citeRegEx": "Liang et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Liang et al\\.", "year": 2011}, {"title": "Rouge: A package for automatic evaluation of summaries", "author": ["Chin-Yew Lin"], "venue": "In Text Summarization Branches Out: Proceedings of the ACL-04 Workshop,", "citeRegEx": "Lin.,? \\Q2004\\E", "shortCiteRegEx": "Lin.", "year": 2004}, {"title": "Microsoft coco: Common objects in context", "author": ["Tsung-Yi Lin", "Michael Maire", "Serge Belongie", "James Hays", "Pietro Perona", "Deva Ramanan", "Piotr Doll\u00e1r", "C Lawrence Zitnick"], "venue": "In European Conference on Computer Vision (ECCV),", "citeRegEx": "Lin et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Lin et al\\.", "year": 2014}, {"title": "Deep captioning with multimodal recurrent neural networks (m-rnn)", "author": ["Junhua Mao", "Wei Xu", "Yi Yang", "Jiang Wang", "Zhiheng Huang", "Alan Yuille"], "venue": "In International Conference on Learning Representations (ICLR),", "citeRegEx": "Mao et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Mao et al\\.", "year": 2015}, {"title": "Actions in context", "author": ["Marcin Marszalek", "Ivan Laptev", "Cordelia Schmid"], "venue": "In Conference on Computer Vision and Pattern Recognition (CVPR),", "citeRegEx": "Marszalek et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Marszalek et al\\.", "year": 2009}, {"title": "Midge: Generating image descriptions from computer vision detections", "author": ["Alexander C. Berg", "Tamara L. Berg", "Hal Daum\u00e9 III"], "venue": "In Proceedings of the Conference of the European Chapter of the Association for Computational Linguistics (EACL),", "citeRegEx": "Berg et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Berg et al\\.", "year": 2012}, {"title": "Im2text: Describing images using 1 million captioned photographs", "author": ["Vicente Ordonez", "Girish Kulkarni", "Tamara L. Berg"], "venue": "In Conference on Neural Information Processing Systems (NIPS),", "citeRegEx": "Ordonez et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Ordonez et al\\.", "year": 2011}, {"title": "Trecvid 2012 \u2013 an overview of the goals, tasks, data, evaluation mechanisms and metrics", "author": ["Paul Over", "George Awad", "Martial Michel", "Jonathan Fiscus", "Greg Sanders", "B Shaw", "Alan F. Smeaton", "Georges Qu\u00e9enot"], "venue": "In Proceedings of TRECVID", "citeRegEx": "Over et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Over et al\\.", "year": 2012}, {"title": "Hierarchical recurrent neural encoder for video representation with application to captioning", "author": ["Pingbo Pan", "Zhongwen Xu", "Yi Yang", "Fei Wu", "Yueting Zhuang"], "venue": "In Conference on Computer Vision and Pattern Recognition (CVPR),", "citeRegEx": "Pan et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Pan et al\\.", "year": 2016}, {"title": "Jointly modeling embedding and translation to bridge video and language", "author": ["Yingwei Pan", "Tao Mei", "Ting Yao", "Houqiang Li", "Yong Rui"], "venue": "In Conference on Computer Vision and Pattern Recognition (CVPR),", "citeRegEx": "Pan et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Pan et al\\.", "year": 2016}, {"title": "BLEU: a method for automatic evaluation of machine translation", "author": ["Kishore Papineni", "Salim Roukos", "Todd Ward", "Wei jing Zhu"], "venue": "In Proceedings of the Annual Meeting of the Association for Computational Linguistics (ACL),", "citeRegEx": "Papineni et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Papineni et al\\.", "year": 2002}, {"title": "Wordnet:: Similarity: measuring the relatedness of concepts", "author": ["Ted Pedersen", "Siddharth Patwardhan", "Jason Michelizzi"], "venue": "In Demonstration Papers at HLT-NAACL", "citeRegEx": "Pedersen et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Pedersen et al\\.", "year": 2004}, {"title": "Linking people in videos with \u201dtheir\u201d names using coreference resolution", "author": ["Vignesh Ramanathan", "Armand Joulin", "Percy Liang", "Li Fei-Fei"], "venue": "In European Conference on Computer Vision (ECCV),", "citeRegEx": "Ramanathan et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Ramanathan et al\\.", "year": 2014}, {"title": "Grounding Action Descriptions in Videos", "author": ["Michaela Regneri", "Marcus Rohrbach", "Dominikus Wetzel", "Stefan Thater", "Bernt Schiele", "Manfred Pinkal"], "venue": "Transactions of the Association for Computational Linguistics (TACL),", "citeRegEx": "Regneri et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Regneri et al\\.", "year": 2013}, {"title": "Coherent multi-sentence video description with variable level of detail", "author": ["Anna Rohrbach", "Marcus Rohrbach", "Wei Qiu", "Annemarie Friedrich", "Manfred Pinkal", "Bernt Schiele"], "venue": "In Proceedings of the German Conference on Pattern Recognition (GCPR),", "citeRegEx": "Rohrbach et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Rohrbach et al\\.", "year": 2014}, {"title": "The long-short story of movie description", "author": ["Anna Rohrbach", "Marcus Rohrbach", "Bernt Schiele"], "venue": null, "citeRegEx": "Rohrbach et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Rohrbach et al\\.", "year": 2015}, {"title": "The long-short story of movie description", "author": ["Anna Rohrbach", "Marcus Rohrbach", "Bernt Schiele"], "venue": "Proceedings of the German Conference on Pattern Recognition (GCPR),", "citeRegEx": "Rohrbach et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Rohrbach et al\\.", "year": 2015}, {"title": "A dataset for movie description", "author": ["Anna Rohrbach", "Marcus Rohrbach", "Niket Tandon", "Bernt Schiele"], "venue": "In Conference on Computer Vision and Pattern Recognition (CVPR),", "citeRegEx": "Rohrbach et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Rohrbach et al\\.", "year": 2015}, {"title": "Translating video content to natural language descriptions", "author": ["Marcus Rohrbach", "Wei Qiu", "Ivan Titov", "Stefan Thater", "Manfred Pinkal", "Bernt Schiele"], "venue": "In International Conference on Computer Vision (ICCV),", "citeRegEx": "Rohrbach et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Rohrbach et al\\.", "year": 2013}, {"title": "A corpus-based analysis of audio description. Media for all: Subtitling for the deaf, audio description and sign", "author": ["Andrew Salway"], "venue": null, "citeRegEx": "Salway.,? \\Q2007\\E", "shortCiteRegEx": "Salway.", "year": 2007}, {"title": "Associating characters with events in films", "author": ["Andrew Salway", "Bart Lehane", "Noel E. O\u2019Connor"], "venue": "In Proceedings of the ACM international conference on Image and video retrieval (CIVR),", "citeRegEx": "Salway et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Salway et al\\.", "year": 2007}, {"title": "Verbnet overview, extensions, mappings and applications", "author": ["Karin Kipper Schuler", "Anna Korhonen", "Susan Windisch Brown"], "venue": "In Proceedings of the Conference of the North American Chapter of the Association for Computational Linguistics (NAACL),", "citeRegEx": "Schuler et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Schuler et al\\.", "year": 2009}, {"title": "Video captioning with recurrent networks based on frame-and video-level features and visual content classification", "author": ["Rakshith Shetty", "Jorma Laaksonen"], "venue": null, "citeRegEx": "Shetty and Laaksonen.,? \\Q2015\\E", "shortCiteRegEx": "Shetty and Laaksonen.", "year": 2015}, {"title": "who are you?\u201d-learning person specific classifiers from video", "author": ["Josef Sivic", "Mark Everingham", "Andrew Zisserman"], "venue": "In Conference on Computer Vision and Pattern Recognition (CVPR),", "citeRegEx": "Sivic et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Sivic et al\\.", "year": 2009}, {"title": "Grounded compositional semantics for finding and describing images with sentences", "author": ["Richard Socher", "Andrej Karpathy", "Quoc V. Le", "Christopher D. Manning", "Andrew Y. Ng"], "venue": "Transactions of the Association for Computational Linguistics (TACL),", "citeRegEx": "Socher et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Socher et al\\.", "year": 2014}, {"title": "Knowlywood: Mining activity knowledge from hollywood narratives", "author": ["Niket Tandon", "Gerard de Melo", "Abir De", "Gerhard Weikum"], "venue": "In Proc. CIKM,", "citeRegEx": "Tandon et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Tandon et al\\.", "year": 2015}, {"title": "knock! knock! who is it?\u201d probabilistic person identification in tv-series", "author": ["Makarand Tapaswi", "Martin Baeuml", "Rainer Stiefelhagen"], "venue": "In Conference on Computer Vision and Pattern Recognition (CVPR),", "citeRegEx": "Tapaswi et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Tapaswi et al\\.", "year": 2012}, {"title": "Movieqa: Understanding stories in movies through questionanswering", "author": ["Makarand Tapaswi", "Yukun Zhu", "Rainer Stiefelhagen", "Antonio Torralba", "Raquel Urtasun", "Sanja Fidler"], "venue": "In Conference on Computer Vision and Pattern Recognition", "citeRegEx": "Tapaswi et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Tapaswi et al\\.", "year": 2016}, {"title": "Using descriptive video services to create a large data source for video annotation research", "author": ["Atousa Torabi", "Christopher Pal", "Hugo Larochelle", "Aaron Courville"], "venue": null, "citeRegEx": "Torabi et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Torabi et al\\.", "year": 2015}, {"title": "Feature-rich part-of-speech tagging with a cyclic dependency network", "author": ["Kristina Toutanova", "Dan Klein", "Christopher D. Manning", "Yoram Singer"], "venue": "Proceedings of the 2003 Conference of the North American Chapter of the Association", "citeRegEx": "Toutanova et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Toutanova et al\\.", "year": 2003}, {"title": "Cider: Consensus-based image description evaluation", "author": ["Ramakrishna Vedantam", "C Lawrence Zitnick", "Devi Parikh"], "venue": "In Conference on Computer Vision and Pattern Recognition (CVPR),", "citeRegEx": "Vedantam et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Vedantam et al\\.", "year": 2015}, {"title": "Sequence to sequence \u2013 video to text. arXiv:1505.00487v2, 2015a", "author": ["Subhashini Venugopalan", "Marcus Rohrbach", "Jeff Donahue", "Raymond Mooney", "Trevor Darrell", "Kate Saenko"], "venue": null, "citeRegEx": "Venugopalan et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Venugopalan et al\\.", "year": 2015}, {"title": "Sequence to sequence \u2013 video to text", "author": ["Subhashini Venugopalan", "Marcus Rohrbach", "Jeff Donahue", "Raymond Mooney", "Trevor Darrell", "Kate Saenko"], "venue": "In International Conference on Computer Vision (ICCV),", "citeRegEx": "Venugopalan et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Venugopalan et al\\.", "year": 2015}, {"title": "Improving lstm-based video description with linguistic knowledge mined from text", "author": ["Subhashini Venugopalan", "Lisa Anne Hendricks", "Raymond Mooney", "Kate Saenko"], "venue": null, "citeRegEx": "Venugopalan et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Venugopalan et al\\.", "year": 2016}, {"title": "Show and tell: A neural image caption generator", "author": ["Oriol Vinyals", "Alexander Toshev", "Samy Bengio", "Dumitru Erhan"], "venue": "In Conference on Computer Vision and Pattern Recognition (CVPR),", "citeRegEx": "Vinyals et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Vinyals et al\\.", "year": 2015}, {"title": "Action recognition with improved trajectories", "author": ["Heng Wang", "Cordelia Schmid"], "venue": "In International Conference on Computer Vision (ICCV),", "citeRegEx": "Wang and Schmid.,? \\Q2013\\E", "shortCiteRegEx": "Wang and Schmid.", "year": 2013}, {"title": "Dense trajectories and motion boundary descriptors for action recognition", "author": ["Heng Wang", "Alexander Kl\u00e4ser", "Cordelia Schmid", "C.L. Liu"], "venue": "International Journal of Computer Vision (IJCV),", "citeRegEx": "Wang et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Wang et al\\.", "year": 2013}, {"title": "Sun database: Large-scale scene recognition from abbey to zoo", "author": ["Jianxiong Xiao", "James Hays", "Krista A. Ehinger", "Aude Oliva", "Antonio Torralba"], "venue": "Conference on Computer Vision and Pattern Recognition (CVPR),", "citeRegEx": "Xiao et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Xiao et al\\.", "year": 2010}, {"title": "Msr-vtt: A large video description dataset for bridging video and language", "author": ["Jun Xu", "Tao Mei", "Ting Yao", "Yong Rui"], "venue": "In Conference on Computer Vision and Pattern Recognition", "citeRegEx": "Xu et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Xu et al\\.", "year": 2016}, {"title": "Show, attend and tell: Neural image caption generation with visual attention", "author": ["Kelvin Xu", "Jimmy Ba", "Ryan Kiros", "Aaron Courville", "Ruslan Salakhutdinov", "Richard Zemel", "Yoshua Bengio"], "venue": "In International Conference on Machine Learning (ICML),", "citeRegEx": "Xu et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Xu et al\\.", "year": 2015}, {"title": "Jointly modeling deep video and compositional text to bridge vision and language in a unified framework", "author": ["Ran Xu", "Caiming Xiong", "Wei Chen", "Jason J. Corso"], "venue": "In Conference on Artificial Intelligence (AAAI),", "citeRegEx": "Xu et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Xu et al\\.", "year": 2015}, {"title": "Describing videos by exploiting temporal structure", "author": ["Li Yao", "Atousa Torabi", "Kyunghyun Cho", "Nicolas Ballas", "Christopher Pal", "Hugo Larochelle", "Aaron Courville"], "venue": "In International Conference on Computer Vision (ICCV),", "citeRegEx": "Yao et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Yao et al\\.", "year": 2015}, {"title": "Empirical performance upper bounds for image and video captioning", "author": ["Li Yao", "Nicolas Ballas", "Kyunghyun Cho", "John R Smith", "Yoshua Bengio"], "venue": "In International Conference on Learning Representations (ICLR),", "citeRegEx": "Yao et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Yao et al\\.", "year": 2016}, {"title": "From image descriptions to visual denotations: New similarity metrics for semantic inference over event descriptions", "author": ["Peter Young", "Alice Lai", "Micah Hodosh", "Julia Hockenmaier"], "venue": "Transactions of the Association for Computational Linguistics (TACL),", "citeRegEx": "Young et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Young et al\\.", "year": 2014}, {"title": "Video paragraph captioning using hierarchical recurrent neural networks", "author": ["Haonan Yu", "Jiang Wang", "Zhiheng Huang", "Yi Yang", "Wei Xu"], "venue": "In Conference on Computer Vision and Pattern Recognition", "citeRegEx": "Yu et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Yu et al\\.", "year": 2016}, {"title": "It makes sense: A wide-coverage word sense disambiguation system for free text", "author": ["Zhi Zhong", "Hwee Tou Ng"], "venue": "In Proceedings of the ACL 2010 System Demonstrations,", "citeRegEx": "Zhong and Ng.,? \\Q2010\\E", "shortCiteRegEx": "Zhong and Ng.", "year": 2010}, {"title": "Learning Deep Features for Scene Recognition using Places Database", "author": ["B. Zhou", "A. Lapedriza", "J. Xiao", "A. Torralba", "A. Oliva"], "venue": "In Conference on Neural Information Processing Systems (NIPS),", "citeRegEx": "Zhou et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Zhou et al\\.", "year": 2014}, {"title": "Uncovering temporal context for video question and answering", "author": ["Linchao Zhu", "Zhongwen Xu", "Yi Yang", "Alexander G Hauptmann"], "venue": null, "citeRegEx": "Zhu et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Zhu et al\\.", "year": 2015}], "referenceMentions": [{"referenceID": 11, "context": "Large datasets of objects (Deng et al. 2009) and scenes (Xiao et al.", "startOffset": 26, "endOffset": 44}, {"referenceID": 73, "context": "2009) and scenes (Xiao et al. 2010; Zhou et al. 2014) have had an important impact in computer vision and have significantly improved our ability to recognize objects and scenes.", "startOffset": 17, "endOffset": 53}, {"referenceID": 82, "context": "2009) and scenes (Xiao et al. 2010; Zhou et al. 2014) have had an important impact in computer vision and have significantly improved our ability to recognize objects and scenes.", "startOffset": 17, "endOffset": 53}, {"referenceID": 28, "context": "The combination of large datasets and convolutional neural networks (CNNs) has been particularly potent (Krizhevsky et al. 2012).", "startOffset": 104, "endOffset": 128}, {"referenceID": 54, "context": "To be able to learn how to generate descriptions of visual content, parallel datasets of visual content paired with descriptions are indispensable (Rohrbach et al. 2013).", "startOffset": 147, "endOffset": 169}, {"referenceID": 21, "context": "While recently several large datasets have been released which provide images with descriptions (Hodosh et al. 2014; Lin et al. 2014; Ordonez et al. 2011), video description datasets focus on short video clips with single sentence descriptions and have a limited number of video clips (Xu et al.", "startOffset": 96, "endOffset": 154}, {"referenceID": 38, "context": "While recently several large datasets have been released which provide images with descriptions (Hodosh et al. 2014; Lin et al. 2014; Ordonez et al. 2011), video description datasets focus on short video clips with single sentence descriptions and have a limited number of video clips (Xu et al.", "startOffset": 96, "endOffset": 154}, {"referenceID": 42, "context": "While recently several large datasets have been released which provide images with descriptions (Hodosh et al. 2014; Lin et al. 2014; Ordonez et al. 2011), video description datasets focus on short video clips with single sentence descriptions and have a limited number of video clips (Xu et al.", "startOffset": 96, "endOffset": 154}, {"referenceID": 74, "context": "2011), video description datasets focus on short video clips with single sentence descriptions and have a limited number of video clips (Xu et al. 2016; Chen and Dolan 2011) or are not publicly available (Over et al.", "startOffset": 136, "endOffset": 173}, {"referenceID": 43, "context": "2016; Chen and Dolan 2011) or are not publicly available (Over et al. 2012).", "startOffset": 57, "endOffset": 75}, {"referenceID": 50, "context": "TACoS Multi-Level (Rohrbach et al. 2014) and YouCook (Das et al.", "startOffset": 18, "endOffset": 40}, {"referenceID": 8, "context": "2014) and YouCook (Das et al. 2013) are exceptions as they provide multiple sentence descriptions and longer videos.", "startOffset": 18, "endOffset": 35}, {"referenceID": 52, "context": "Therefore, in contrast to Salway (2007) and Salway et al.", "startOffset": 26, "endOffset": 40}, {"referenceID": 52, "context": "Therefore, in contrast to Salway (2007) and Salway et al. (2007), our dataset provides alignment to the actions in the video, rather than just to the audio track of the description.", "startOffset": 26, "endOffset": 65}, {"referenceID": 5, "context": "In addition we also mine existing movie scripts, pre-align them automatically, similar to Cour et al. (2008) and Laptev et al.", "startOffset": 90, "endOffset": 109}, {"referenceID": 5, "context": "In addition we also mine existing movie scripts, pre-align them automatically, similar to Cour et al. (2008) and Laptev et al. (2008), and then manually align the sentences to the movie.", "startOffset": 90, "endOffset": 134}, {"referenceID": 50, "context": "Second, we adapt the approach of Rohrbach et al. (2013) by automatically ex-", "startOffset": 33, "endOffset": 56}, {"referenceID": 26, "context": "iter and Schmidhuber 1997) for the image captioning problem (Donahue et al. 2015; Karpathy and Fei-Fei 2015; Kiros et al. 2015; Vinyals et al. 2015) we propose our approachVisual-Labels.", "startOffset": 60, "endOffset": 148}, {"referenceID": 70, "context": "iter and Schmidhuber 1997) for the image captioning problem (Donahue et al. 2015; Karpathy and Fei-Fei 2015; Kiros et al. 2015; Vinyals et al. 2015) we propose our approachVisual-Labels.", "startOffset": 60, "endOffset": 148}, {"referenceID": 50, "context": "This work is partially based on the original publications from Rohrbach et al. (2015c,b) and the technical report from Torabi et al. (2015). Torabi et al.", "startOffset": 63, "endOffset": 140}, {"referenceID": 50, "context": "This work is partially based on the original publications from Rohrbach et al. (2015c,b) and the technical report from Torabi et al. (2015). Torabi et al. (2015) collected M-VAD, Rohrbach et al.", "startOffset": 63, "endOffset": 162}, {"referenceID": 50, "context": "This work is partially based on the original publications from Rohrbach et al. (2015c,b) and the technical report from Torabi et al. (2015). Torabi et al. (2015) collected M-VAD, Rohrbach et al. (2015c) collected the MPII-MD dataset and presented the SMT-based description approach.", "startOffset": 63, "endOffset": 203}, {"referenceID": 50, "context": "This work is partially based on the original publications from Rohrbach et al. (2015c,b) and the technical report from Torabi et al. (2015). Torabi et al. (2015) collected M-VAD, Rohrbach et al. (2015c) collected the MPII-MD dataset and presented the SMT-based description approach. Rohrbach et al. (2015b) proposed the Visual-Labels approach.", "startOffset": 63, "endOffset": 307}, {"referenceID": 79, "context": "New datasets have been released, such as the Flickr30k (Young et al. 2014) and MS COCO Captions (Chen et al.", "startOffset": 55, "endOffset": 74}, {"referenceID": 4, "context": "2014) and MS COCO Captions (Chen et al. 2015), where Chen et al.", "startOffset": 27, "endOffset": 45}, {"referenceID": 16, "context": "Prior work on image description includes Farhadi et al. (2010); Kulkarni et al.", "startOffset": 41, "endOffset": 63}, {"referenceID": 16, "context": "Prior work on image description includes Farhadi et al. (2010); Kulkarni et al. (2011); Kuznetsova et al.", "startOffset": 41, "endOffset": 87}, {"referenceID": 16, "context": "Prior work on image description includes Farhadi et al. (2010); Kulkarni et al. (2011); Kuznetsova et al. (2012); Li et al.", "startOffset": 41, "endOffset": 113}, {"referenceID": 16, "context": "Prior work on image description includes Farhadi et al. (2010); Kulkarni et al. (2011); Kuznetsova et al. (2012); Li et al. (2011); Kuznetsova et al.", "startOffset": 41, "endOffset": 131}, {"referenceID": 16, "context": "Prior work on image description includes Farhadi et al. (2010); Kulkarni et al. (2011); Kuznetsova et al. (2012); Li et al. (2011); Kuznetsova et al. (2014); Mitchell et al.", "startOffset": 41, "endOffset": 157}, {"referenceID": 16, "context": "Prior work on image description includes Farhadi et al. (2010); Kulkarni et al. (2011); Kuznetsova et al. (2012); Li et al. (2011); Kuznetsova et al. (2014); Mitchell et al. (2012); Socher et al.", "startOffset": 41, "endOffset": 181}, {"referenceID": 16, "context": "Prior work on image description includes Farhadi et al. (2010); Kulkarni et al. (2011); Kuznetsova et al. (2012); Li et al. (2011); Kuznetsova et al. (2014); Mitchell et al. (2012); Socher et al. (2014). Recently image description has gained increased attention with work such as that of Chen and Zitnick (2015); Donahue et al.", "startOffset": 41, "endOffset": 203}, {"referenceID": 16, "context": "Prior work on image description includes Farhadi et al. (2010); Kulkarni et al. (2011); Kuznetsova et al. (2012); Li et al. (2011); Kuznetsova et al. (2014); Mitchell et al. (2012); Socher et al. (2014). Recently image description has gained increased attention with work such as that of Chen and Zitnick (2015); Donahue et al.", "startOffset": 41, "endOffset": 312}, {"referenceID": 16, "context": "Prior work on image description includes Farhadi et al. (2010); Kulkarni et al. (2011); Kuznetsova et al. (2012); Li et al. (2011); Kuznetsova et al. (2014); Mitchell et al. (2012); Socher et al. (2014). Recently image description has gained increased attention with work such as that of Chen and Zitnick (2015); Donahue et al. (2015); Fang et al.", "startOffset": 41, "endOffset": 335}, {"referenceID": 16, "context": "Prior work on image description includes Farhadi et al. (2010); Kulkarni et al. (2011); Kuznetsova et al. (2012); Li et al. (2011); Kuznetsova et al. (2014); Mitchell et al. (2012); Socher et al. (2014). Recently image description has gained increased attention with work such as that of Chen and Zitnick (2015); Donahue et al. (2015); Fang et al. (2015); Karpathy and Fei-Fei (2015); Kiros et al.", "startOffset": 41, "endOffset": 355}, {"referenceID": 16, "context": "Prior work on image description includes Farhadi et al. (2010); Kulkarni et al. (2011); Kuznetsova et al. (2012); Li et al. (2011); Kuznetsova et al. (2014); Mitchell et al. (2012); Socher et al. (2014). Recently image description has gained increased attention with work such as that of Chen and Zitnick (2015); Donahue et al. (2015); Fang et al. (2015); Karpathy and Fei-Fei (2015); Kiros et al.", "startOffset": 41, "endOffset": 384}, {"referenceID": 16, "context": "Prior work on image description includes Farhadi et al. (2010); Kulkarni et al. (2011); Kuznetsova et al. (2012); Li et al. (2011); Kuznetsova et al. (2014); Mitchell et al. (2012); Socher et al. (2014). Recently image description has gained increased attention with work such as that of Chen and Zitnick (2015); Donahue et al. (2015); Fang et al. (2015); Karpathy and Fei-Fei (2015); Kiros et al. (2014, 2015); Mao et al. (2015); Vinyals et al.", "startOffset": 41, "endOffset": 430}, {"referenceID": 16, "context": "Prior work on image description includes Farhadi et al. (2010); Kulkarni et al. (2011); Kuznetsova et al. (2012); Li et al. (2011); Kuznetsova et al. (2014); Mitchell et al. (2012); Socher et al. (2014). Recently image description has gained increased attention with work such as that of Chen and Zitnick (2015); Donahue et al. (2015); Fang et al. (2015); Karpathy and Fei-Fei (2015); Kiros et al. (2014, 2015); Mao et al. (2015); Vinyals et al. (2015); Xu et al.", "startOffset": 41, "endOffset": 453}, {"referenceID": 16, "context": "Prior work on image description includes Farhadi et al. (2010); Kulkarni et al. (2011); Kuznetsova et al. (2012); Li et al. (2011); Kuznetsova et al. (2014); Mitchell et al. (2012); Socher et al. (2014). Recently image description has gained increased attention with work such as that of Chen and Zitnick (2015); Donahue et al. (2015); Fang et al. (2015); Karpathy and Fei-Fei (2015); Kiros et al. (2014, 2015); Mao et al. (2015); Vinyals et al. (2015); Xu et al. (2015a). Much of the recent work has relied on Recurrent Neural Networks (RNNs) and in particular on Long Short-Term Memory networks (LSTMs).", "startOffset": 41, "endOffset": 472}, {"referenceID": 4, "context": "2014) and MS COCO Captions (Chen et al. 2015), where Chen et al. (2015) also presents a standardized protocol for image captioning evaluation.", "startOffset": 28, "endOffset": 72}, {"referenceID": 4, "context": "2014) and MS COCO Captions (Chen et al. 2015), where Chen et al. (2015) also presents a standardized protocol for image captioning evaluation. Other work has analyzed the performance of recent methods, e.g. Devlin et al. (2015) compare them with respect to the novelty of generated descriptions, while also exploring a nearest neighbor baseline that improves over recent methods.", "startOffset": 28, "endOffset": 228}, {"referenceID": 27, "context": "In the past video description has been addressed in controlled settings (Barbu et al. 2012; Kojima et al. 2002), on a small scale (Das et al.", "startOffset": 72, "endOffset": 111}, {"referenceID": 8, "context": "2002), on a small scale (Das et al. 2013; Guadarrama et al. 2013; Thomason et al. 2014) or in single domains like cooking (Rohrbach et al.", "startOffset": 24, "endOffset": 87}, {"referenceID": 7, "context": "2002), on a small scale (Das et al. 2013; Guadarrama et al. 2013; Thomason et al. 2014) or in single domains like cooking (Rohrbach et al. 2014, 2013; Donahue et al. 2015). Donahue et al. (2015) first proposed to describe videos using an LSTM, relying on precomputed CRF scores from Rohrbach et al.", "startOffset": 25, "endOffset": 195}, {"referenceID": 7, "context": "2002), on a small scale (Das et al. 2013; Guadarrama et al. 2013; Thomason et al. 2014) or in single domains like cooking (Rohrbach et al. 2014, 2013; Donahue et al. 2015). Donahue et al. (2015) first proposed to describe videos using an LSTM, relying on precomputed CRF scores from Rohrbach et al. (2014). Later Venugopalan et al.", "startOffset": 25, "endOffset": 306}, {"referenceID": 7, "context": "2002), on a small scale (Das et al. 2013; Guadarrama et al. 2013; Thomason et al. 2014) or in single domains like cooking (Rohrbach et al. 2014, 2013; Donahue et al. 2015). Donahue et al. (2015) first proposed to describe videos using an LSTM, relying on precomputed CRF scores from Rohrbach et al. (2014). Later Venugopalan et al. (2015c) extended this work to extract CNN features from frames which are max-pooled over time.", "startOffset": 25, "endOffset": 340}, {"referenceID": 7, "context": "2002), on a small scale (Das et al. 2013; Guadarrama et al. 2013; Thomason et al. 2014) or in single domains like cooking (Rohrbach et al. 2014, 2013; Donahue et al. 2015). Donahue et al. (2015) first proposed to describe videos using an LSTM, relying on precomputed CRF scores from Rohrbach et al. (2014). Later Venugopalan et al. (2015c) extended this work to extract CNN features from frames which are max-pooled over time. Pan et al. (2016b) propose a framework that consists of a 2-/3-D CNN and LSTM trained jointly with a visual-semantic embedding to ensure better coherence between video and text.", "startOffset": 25, "endOffset": 446}, {"referenceID": 7, "context": "2002), on a small scale (Das et al. 2013; Guadarrama et al. 2013; Thomason et al. 2014) or in single domains like cooking (Rohrbach et al. 2014, 2013; Donahue et al. 2015). Donahue et al. (2015) first proposed to describe videos using an LSTM, relying on precomputed CRF scores from Rohrbach et al. (2014). Later Venugopalan et al. (2015c) extended this work to extract CNN features from frames which are max-pooled over time. Pan et al. (2016b) propose a framework that consists of a 2-/3-D CNN and LSTM trained jointly with a visual-semantic embedding to ensure better coherence between video and text. Xu et al. (2015b) jointly address the language generation and video/language retrieval tasks by learning a joint embedding for a deep video model and a compositional semantic language model.", "startOffset": 25, "endOffset": 623}, {"referenceID": 7, "context": "2002), on a small scale (Das et al. 2013; Guadarrama et al. 2013; Thomason et al. 2014) or in single domains like cooking (Rohrbach et al. 2014, 2013; Donahue et al. 2015). Donahue et al. (2015) first proposed to describe videos using an LSTM, relying on precomputed CRF scores from Rohrbach et al. (2014). Later Venugopalan et al. (2015c) extended this work to extract CNN features from frames which are max-pooled over time. Pan et al. (2016b) propose a framework that consists of a 2-/3-D CNN and LSTM trained jointly with a visual-semantic embedding to ensure better coherence between video and text. Xu et al. (2015b) jointly address the language generation and video/language retrieval tasks by learning a joint embedding for a deep video model and a compositional semantic language model. Li et al. (2015) study the problem of summa-", "startOffset": 25, "endOffset": 813}, {"referenceID": 77, "context": "lenging scenario of movie description, Yao et al. (2015) propose a soft-attention based model which selects the most relevant temporal segments in a video, incorporates 3-D CNN and generates a sentence using an LSTM.", "startOffset": 39, "endOffset": 57}, {"referenceID": 44, "context": "Pan et al. (2016a) extend the video encoding idea by introducing a second LSTM layer which receives input of the first layer, but skips several frames, reducing its temporal depth.", "startOffset": 0, "endOffset": 19}, {"referenceID": 44, "context": "Pan et al. (2016a) extend the video encoding idea by introducing a second LSTM layer which receives input of the first layer, but skips several frames, reducing its temporal depth. Venugopalan et al. (2016) explore the benefit of pre-trained word embeddings and language models for generation on large external text corpora.", "startOffset": 0, "endOffset": 207}, {"referenceID": 44, "context": "Pan et al. (2016a) extend the video encoding idea by introducing a second LSTM layer which receives input of the first layer, but skips several frames, reducing its temporal depth. Venugopalan et al. (2016) explore the benefit of pre-trained word embeddings and language models for generation on large external text corpora. Shetty and Laaksonen (2015) evaluate different visual features as input for an LSTM generation framework.", "startOffset": 0, "endOffset": 353}, {"referenceID": 44, "context": "Pan et al. (2016a) extend the video encoding idea by introducing a second LSTM layer which receives input of the first layer, but skips several frames, reducing its temporal depth. Venugopalan et al. (2016) explore the benefit of pre-trained word embeddings and language models for generation on large external text corpora. Shetty and Laaksonen (2015) evaluate different visual features as input for an LSTM generation framework. Specifically they use dense trajectory features Wang et al. (2013) extracted for the clips and CNN features extracted at center frames of the clip.", "startOffset": 0, "endOffset": 498}, {"referenceID": 44, "context": "Pan et al. (2016a) extend the video encoding idea by introducing a second LSTM layer which receives input of the first layer, but skips several frames, reducing its temporal depth. Venugopalan et al. (2016) explore the benefit of pre-trained word embeddings and language models for generation on large external text corpora. Shetty and Laaksonen (2015) evaluate different visual features as input for an LSTM generation framework. Specifically they use dense trajectory features Wang et al. (2013) extracted for the clips and CNN features extracted at center frames of the clip. They find that training concept classifiers on MS COCO with the CNN features, combined with dense trajectories provides the best input for the LSTM. Ballas et al. (2016) leverages multiple convolutional maps from different CNN layers to improve the visual representation for activity and video description.", "startOffset": 0, "endOffset": 749}, {"referenceID": 44, "context": "Pan et al. (2016a) extend the video encoding idea by introducing a second LSTM layer which receives input of the first layer, but skips several frames, reducing its temporal depth. Venugopalan et al. (2016) explore the benefit of pre-trained word embeddings and language models for generation on large external text corpora. Shetty and Laaksonen (2015) evaluate different visual features as input for an LSTM generation framework. Specifically they use dense trajectory features Wang et al. (2013) extracted for the clips and CNN features extracted at center frames of the clip. They find that training concept classifiers on MS COCO with the CNN features, combined with dense trajectories provides the best input for the LSTM. Ballas et al. (2016) leverages multiple convolutional maps from different CNN layers to improve the visual representation for activity and video description. To model multi-sentence description, Yu et al. (2016) propose to use two stacked RNNs where the first one models words within a sentence and the second one, sentences within a paragraph.", "startOffset": 0, "endOffset": 940}, {"referenceID": 44, "context": "Pan et al. (2016a) extend the video encoding idea by introducing a second LSTM layer which receives input of the first layer, but skips several frames, reducing its temporal depth. Venugopalan et al. (2016) explore the benefit of pre-trained word embeddings and language models for generation on large external text corpora. Shetty and Laaksonen (2015) evaluate different visual features as input for an LSTM generation framework. Specifically they use dense trajectory features Wang et al. (2013) extracted for the clips and CNN features extracted at center frames of the clip. They find that training concept classifiers on MS COCO with the CNN features, combined with dense trajectories provides the best input for the LSTM. Ballas et al. (2016) leverages multiple convolutional maps from different CNN layers to improve the visual representation for activity and video description. To model multi-sentence description, Yu et al. (2016) propose to use two stacked RNNs where the first one models words within a sentence and the second one, sentences within a paragraph. Yao et al. (2016) has conducted an interesting study on", "startOffset": 0, "endOffset": 1091}, {"referenceID": 13, "context": "Movie scripts have been used for automatic discovery and annotation of scenes and human actions in videos (Duchenne et al. 2009; Laptev et al. 2008; Marszalek et al. 2009), as well as a resource to construct activity knowledge base (Tandon et al.", "startOffset": 106, "endOffset": 171}, {"referenceID": 33, "context": "Movie scripts have been used for automatic discovery and annotation of scenes and human actions in videos (Duchenne et al. 2009; Laptev et al. 2008; Marszalek et al. 2009), as well as a resource to construct activity knowledge base (Tandon et al.", "startOffset": 106, "endOffset": 171}, {"referenceID": 40, "context": "Movie scripts have been used for automatic discovery and annotation of scenes and human actions in videos (Duchenne et al. 2009; Laptev et al. 2008; Marszalek et al. 2009), as well as a resource to construct activity knowledge base (Tandon et al.", "startOffset": 106, "endOffset": 171}, {"referenceID": 61, "context": "2009), as well as a resource to construct activity knowledge base (Tandon et al. 2015; de Melo and Tandon 2016).", "startOffset": 66, "endOffset": 111}, {"referenceID": 9, "context": "2015; de Melo and Tandon 2016). We rely on the approach presented by Laptev et al. (2008) to align movie scripts using subtitles.", "startOffset": 9, "endOffset": 90}, {"referenceID": 7, "context": "They rely on the semantic parser SEMAFOR (Das et al. 2012) trained on the FrameNet database (Baker et al.", "startOffset": 41, "endOffset": 58}, {"referenceID": 0, "context": "2012) trained on the FrameNet database (Baker et al. 1998), however, they limit the recognition only to two frames.", "startOffset": 39, "endOffset": 58}, {"referenceID": 56, "context": "ADs have also been used to understand which characters interact with each other (Salway et al. 2007).", "startOffset": 80, "endOffset": 100}, {"referenceID": 32, "context": "Other prior work has looked at supporting AD production using scripts as an information source (Lakritz and Salway 2006) and automatically finding scene boundaries (Gagnon et al. 2010). Salway (2007) analyses the linguistic properties on a non-public corpus of ADs from 91 movies.", "startOffset": 96, "endOffset": 200}, {"referenceID": 81, "context": "Zhu et al. (2015b) learn a visual-semantic embedding from our clips and ADs to relate movies to books.", "startOffset": 0, "endOffset": 19}, {"referenceID": 62, "context": "Tapaswi et al. (2016) used our AD transcripts for building their MovieQA dataset, which asks natural language questions about movies, requiring an understanding of visual and textual information, such as Dialogue and AD, to answer the question.", "startOffset": 0, "endOffset": 22}, {"referenceID": 62, "context": "Tapaswi et al. (2016) used our AD transcripts for building their MovieQA dataset, which asks natural language questions about movies, requiring an understanding of visual and textual information, such as Dialogue and AD, to answer the question. Zhu et al. (2015a) present a fill-in-the-blank challenge for audio description of the current, previous, and next sentence description for a given clip, requiring to understand the temporal context of the clips.", "startOffset": 0, "endOffset": 264}, {"referenceID": 50, "context": "The MPII Movie Description Dataset (MPII-MD), initially presented by Rohrbach et al. (2015c), was collected from Blu-ray movie data.", "startOffset": 69, "endOffset": 93}, {"referenceID": 64, "context": "tomate AD audio segmentation and alignment for the Montreal Video Annotation Dataset (M-VAD), initially presented by Torabi et al. (2015). M-VAD was collected", "startOffset": 117, "endOffset": 138}, {"referenceID": 40, "context": "As starting point we use the movie scripts from \u201cHollywood2\u201d (Marszalek et al. 2009) that have highest alignment scores to their movie.", "startOffset": 61, "endOffset": 84}, {"referenceID": 5, "context": "We follow existing approaches (Cour et al. 2008; Laptev et al. 2008) to automatically align scripts to movies.", "startOffset": 30, "endOffset": 68}, {"referenceID": 33, "context": "We follow existing approaches (Cour et al. 2008; Laptev et al. 2008) to automatically align scripts to movies.", "startOffset": 30, "endOffset": 68}, {"referenceID": 33, "context": "First we parse the scripts, extending the method of (Laptev et al. 2008) to handle scripts which deviate from the default format.", "startOffset": 52, "endOffset": 72}, {"referenceID": 33, "context": "Then we use the dynamic programming method of (Laptev et al. 2008) to align scripts to subtitles and infer the time-stamps for the description sentences.", "startOffset": 46, "endOffset": 66}, {"referenceID": 22, "context": "LSDA We use the recent large scale object detection CNN (Hoffman et al. 2014) which distinguishes 7,604 ImageNet (Deng et al.", "startOffset": 56, "endOffset": 77}, {"referenceID": 11, "context": "2014) which distinguishes 7,604 ImageNet (Deng et al. 2009) classes.", "startOffset": 41, "endOffset": 59}, {"referenceID": 82, "context": "PLACES and HYBRID Finally, we use the recent scene classification CNNs (Zhou et al. 2014) featuring 205 scene classes.", "startOffset": 71, "endOffset": 89}, {"referenceID": 82, "context": "We use both available networks, Places-CNN and Hybrid-CNN, where the first is trained on the Places dataset (Zhou et al. 2014) only, while the second is additionally trained on the 1.", "startOffset": 108, "endOffset": 126}, {"referenceID": 65, "context": "mer toolbox (Toutanova et al. 2003), then we compute the frequency of stemmed words in the corpora.", "startOffset": 12, "endOffset": 35}, {"referenceID": 8, "context": "The main limitations of prior datasets include the coverage of a single domain (Das et al. 2013; Regneri et al. 2013; Rohrbach et al. 2014) and having a limited number of video clips (Chen and Dolan 2011).", "startOffset": 79, "endOffset": 139}, {"referenceID": 49, "context": "The main limitations of prior datasets include the coverage of a single domain (Das et al. 2013; Regneri et al. 2013; Rohrbach et al. 2014) and having a limited number of video clips (Chen and Dolan 2011).", "startOffset": 79, "endOffset": 139}, {"referenceID": 50, "context": "The main limitations of prior datasets include the coverage of a single domain (Das et al. 2013; Regneri et al. 2013; Rohrbach et al. 2014) and having a limited number of video clips (Chen and Dolan 2011).", "startOffset": 79, "endOffset": 139}, {"referenceID": 74, "context": "Recently, two video description datasets have been proposed, namely MSR-VTT (Xu et al. 2016) and TGIF (Li et al.", "startOffset": 76, "endOffset": 92}, {"referenceID": 35, "context": "2016) and TGIF (Li et al. 2016).", "startOffset": 15, "endOffset": 31}, {"referenceID": 8, "context": "YouCook (Das et al. 2013) x cooking crowd 88 - 2,668 TACoS (Regneri et al.", "startOffset": 8, "endOffset": 25}, {"referenceID": 49, "context": "2013) x cooking crowd 88 - 2,668 TACoS (Regneri et al. 2013) x cooking crowd 127 7,206 18,227 TACoS Multi-Level (Rohrbach et al.", "startOffset": 39, "endOffset": 60}, {"referenceID": 50, "context": "2013) x cooking crowd 127 7,206 18,227 TACoS Multi-Level (Rohrbach et al. 2014) x cooking crowd 185 14,105 52,593 MSVD (Chen and Dolan 2011) open crowd - 1,970 70,028 TGIF (Li et al.", "startOffset": 57, "endOffset": 79}, {"referenceID": 35, "context": "2014) x cooking crowd 185 14,105 52,593 MSVD (Chen and Dolan 2011) open crowd - 1,970 70,028 TGIF (Li et al. 2016) open crowd - 100,000 125,781 MSR-VTT (Xu et al.", "startOffset": 98, "endOffset": 114}, {"referenceID": 74, "context": "2016) open crowd - 100,000 125,781 MSR-VTT (Xu et al. 2016) open crowd 7,180 10,000 200,000", "startOffset": 43, "endOffset": 59}, {"referenceID": 54, "context": "As our first approach we adapt the two-step translation approach of (Rohrbach et al. 2013) which uses an intermediate semantic representation (SR), modeled as a tuple, e.", "startOffset": 68, "endOffset": 90}, {"referenceID": 54, "context": "While we cannot rely on an annotated SR as in (Rohrbach et al. 2013), we automatically mine the SR from sentences using semantic parsing which we introduce in this section.", "startOffset": 46, "endOffset": 68}, {"referenceID": 47, "context": "We lift the words in a sentence to a semantic space of roles and WordNet (Fellbaum 1998; Pedersen et al. 2004) senses by performing SRL (Semantic Role Labeling) and WSD (Word Sense Disambiguation).", "startOffset": 73, "endOffset": 110}, {"referenceID": 54, "context": "Figure 5 Overview of our movie description approaches: (a) SMT-based approach, adapted from (Rohrbach et al. 2013); (b) our proposed LSTM-based approach.", "startOffset": 92, "endOffset": 114}, {"referenceID": 24, "context": "to VerbNet (Kipper et al. 2006; Schuler et al. 2009), a manually curated high-quality linguistic resource for English verbs.", "startOffset": 11, "endOffset": 52}, {"referenceID": 57, "context": "to VerbNet (Kipper et al. 2006; Schuler et al. 2009), a manually curated high-quality linguistic resource for English verbs.", "startOffset": 11, "endOffset": 52}, {"referenceID": 54, "context": "translation approach of (Rohrbach et al. 2013).", "startOffset": 24, "endOffset": 46}, {"referenceID": 82, "context": "Next, we look up the list of \u201cplaces\u201d used in (Zhou et al. 2014) and search for corresponding words among our labels.", "startOffset": 46, "endOffset": 64}, {"referenceID": 22, "context": "We look up the object classes used in (Hoffman et al. 2014) and search for these \u201cobjects\u201d, as well as their base forms (e.", "startOffset": 38, "endOffset": 59}, {"referenceID": 19, "context": "To learn a more robust network which is less likely to overfit we rely on a dropout (Hinton et al. 2012), i.", "startOffset": 84, "endOffset": 104}, {"referenceID": 46, "context": "The automatic evaluation measures include BLEU-1,-2,-3,-4 (Papineni et al. 2002), METEOR (Denkowski and Lavie 2014), ROUGE-L (Lin 2004), and CIDEr (Vedantam et al.", "startOffset": 58, "endOffset": 80}, {"referenceID": 66, "context": "2002), METEOR (Denkowski and Lavie 2014), ROUGE-L (Lin 2004), and CIDEr (Vedantam et al. 2015).", "startOffset": 72, "endOffset": 94}, {"referenceID": 66, "context": "According to (Elliott and Keller 2013; Vedantam et al. 2015), METEOR supersedes previously used measures such as BLEU or ROUGE in terms of agreement with human judgments.", "startOffset": 13, "endOffset": 60}, {"referenceID": 50, "context": "vance, and grammar, motivated by prior work Rohrbach et al. (2013) and on the other hand we asked human judges to rank sentences for \u201chow helpful they would be for a blind person to understand what is hap-", "startOffset": 44, "endOffset": 67}, {"referenceID": 50, "context": "namely TACoS Multi-Level (Rohrbach et al. 2014), and showed the comparable performance to manually annotated SRs, see (Rohrbach et al.", "startOffset": 25, "endOffset": 47}, {"referenceID": 50, "context": "17 More details can be found in our corresponding arXiv version Rohrbach et al. (2015a)", "startOffset": 64, "endOffset": 88}, {"referenceID": 77, "context": "2015a), Temporal attention (Yao et al. 2015).", "startOffset": 27, "endOffset": 44}, {"referenceID": 77, "context": "2015a) and Temporal attention (Yao et al. 2015) in METEOR score.", "startOffset": 30, "endOffset": 47}, {"referenceID": 77, "context": "0 Temporal Attention (Yao et al. 2015) 5.", "startOffset": 21, "endOffset": 38}, {"referenceID": 77, "context": "29 Temporal Attention (Yao et al. 2015) 3.", "startOffset": 22, "endOffset": 39}, {"referenceID": 4, "context": "The submission format was similar to the MS COCO Challenge (Chen et al. 2015) and we also used the identical automatic evaluation protocol.", "startOffset": 59, "endOffset": 77}, {"referenceID": 77, "context": "62 Temporal Attention (Yao et al. 2015) 3.", "startOffset": 22, "endOffset": 39}, {"referenceID": 77, "context": "2015b), Frame-Video-Concept Fusion (Shetty and Laaksonen 2015) and Temporal Attention (Yao et al. 2015) on the blind test set of the LSMDC.", "startOffset": 86, "endOffset": 103}, {"referenceID": 77, "context": "2015b), Temporal Attention (Yao et al. 2015) and Frame-Video-Concept Fusion (Shetty and Laaksonen 2015).", "startOffset": 27, "endOffset": 44}, {"referenceID": 4, "context": "As known from literature (Chen et al. 2015; Elliott and Keller 2013; Vedantam et al. 2015), automatic evaluation measures do not always agree with the human evaluation.", "startOffset": 25, "endOffset": 90}, {"referenceID": 66, "context": "As known from literature (Chen et al. 2015; Elliott and Keller 2013; Vedantam et al. 2015), automatic evaluation measures do not always agree with the human evaluation.", "startOffset": 25, "endOffset": 90}, {"referenceID": 77, "context": "2015b), Frame-Video-Concept Fusion (Shetty and Laaksonen 2015) and Temporal Attention (Yao et al. 2015) on 5 consecutive clips from the blind test set of the LSMDC.", "startOffset": 86, "endOffset": 103}, {"referenceID": 58, "context": "Based on the human evaluation the winner of the challenge is Frame-Video-Concept Fusion approach of Shetty and Laaksonen (2015).", "startOffset": 100, "endOffset": 128}], "year": 2016, "abstractText": "Audio Description (AD) provides linguistic descriptions of movies and allows visually impaired people to follow a movie along with their peers. Such descriptions are by design mainly visual and thus naturally form an interesting data source for computer vision and computational linguistics. In this work we propose a novel dataset which contains transcribed ADs, which are temporally aligned to full length movies. In addition we also collected and aligned movie scripts used in prior work and compare the two sources of descriptions. In total the Large Scale Movie Description Challenge (LSMDC) contains a parallel corpus of 118,114 sentences and video clips from 202 movies. First we characterize the dataset by benchmarking different approaches for generating video descriptions. Comparing ADs to scripts, we find that ADs are indeed more visual and describe precisely what is shown rather than what should happen according to the scripts created prior to movie production. Furthermore, we present and compare the results of several teams who participated in a challenge organized in the context of the workshop \u201cDescribing and Understanding Video & The Large Scale Movie Description Challenge (LSMDC)\u201d, at ICCV 2015. Anna Rohrbach 1 \u00b7 Atousa Torabi \u00b7 Marcus Rohrbach \u00b7 Niket Tandon 1 \u00b7 Christopher Pal \u00b7 Hugo Larochelle \u00b7 Aaron Courville \u00b7 Bernt Schiele 1 Max Planck Institute for Informatics, Saarbr\u00fccken, Germany 2 ICSI and EECS, UC Berkeley, United States 3 Disney Research, Pittsburgh, United States 4 \u00c9cole Polytechnique de Montr\u00e9al, Montr\u00e9al, Canada 5 Universit\u00e9 de Sherbrooke, Sherbrooke, Canada 6 Twitter, Cambridge, United States 7 Universit\u00e9 de Montr\u00e9al, Montr\u00e9al, Canada AD: Abby gets in the basket. Mike leans over and sees how high they are. Abby clasps her hands around his face and kisses him passionately. Script: After a moment a frazzled Abby pops up in his place. Mike looks down to see \u2013 they are now fifteen feet above the ground. For the first time in her life, she stops thinking and grabs Mike and kisses the hell out of him. Figure 1 Audio description (AD) and movie script samples from the movie \u201cUgly Truth\u201d.", "creator": "LaTeX with hyperref package"}}}