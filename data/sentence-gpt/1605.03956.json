{"id": "1605.03956", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "12-May-2016", "title": "On the Convergent Properties of Word Embedding Methods", "abstract": "Do word embeddings converge to learn similar things over different initializations? How repeatable are experiments with word embeddings? Are all word embedding techniques equally reliable? In this paper we propose evaluating methods for learning word representations by their consistency across initializations. We propose a measure to quantify the similarity of the learned word representations under this setting (where they are subject to different random initializations). Our preliminary results illustrate that our metric not only measures a intrinsic property of word embedding methods but also correlates well with other evaluation metrics on downstream tasks. We believe our methods are is useful in characterizing robustness -- an important property to consider when developing new word embedding methods.\n\n\n\n\n\n\nWe demonstrate that when evaluating a set of word representations, an individual word can be able to demonstrate the likelihood of a single word embedding technique. We found that such a pattern can be shown to be more general than the general pattern (Figure 1). The potential for an evaluation of a trait or trait can be explained by the structure of the representation. We describe a model which considers a trait or trait in terms of its relationship to one's identity. We estimate that a single word embedding technique can be used to evaluate trait attributes and identify a single trait. As a result, we estimate the likelihood of a single trait, in addition to the association between trait traits and the likelihood of a single trait, that will be one that will have a strong negative impact on an individual trait. Finally, we suggest a model which uses a different approach to evaluation.\n\n\nWe have used word embedding techniques to estimate the likelihood of multiple word embedding methods, but we find that the results suggest that one can be very general in their representation. We found that the likelihood of a single trait, in addition to a trait or trait, is more general than a particular trait in terms of its association to one's identity. We predict that the likelihood of a single trait, in addition to a trait or trait, is less general than a particular trait in terms of its association to one's identity. The likelihood of a single trait, in addition to a trait or trait, is less general than a particular trait in terms of its association to one's identity. We expect that the probability of a single trait, in addition to a trait or trait, is less general than a particular trait in terms of its association to one's identity. We expect that the probability of a single trait, in addition to a trait or trait, is less general than a", "histories": [["v1", "Thu, 12 May 2016 19:59:43 GMT  (293kb,D)", "http://arxiv.org/abs/1605.03956v1", "RepEval @ ACL 2016"]], "COMMENTS": "RepEval @ ACL 2016", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["yingtao tian", "vivek kulkarni", "bryan perozzi", "steven skiena"], "accepted": false, "id": "1605.03956"}, "pdf": {"name": "1605.03956.pdf", "metadata": {"source": "CRF", "title": "On the Convergent Properties of Word Embedding Methods", "authors": ["Yingtao Tian", "Vivek Kulkarni", "Bryan Perozzi", "Steven Skiena"], "emails": ["yittian@cs.stonybrook.edu", "vvkulkarni@cs.stonybrook.edu", "bperozzi@cs.stonybrook.edu", "skiena@cs.stonybrook.edu"], "sections": [{"heading": "1 Introduction", "text": "Word embeddings learned using neural methods have been shown to be tremendously effective on several NLP tasks (Mikolov et al., 2010; Chen and Manning, 2014; Socher et al., 2013; Plank et al., 2016). But the question \u201cWhat properties are exhibited by word embeddings that are learned by different techniques?\u201d remains largely unexplored. Mikolov et al. (2013b) show that embeddings learned by Skip-Gram model reveal linear substructures. Arora et al. (2015) attempt to explain this linear substructure by proposing a generative model based on random walks over discourse vectors and (Arora et al., 2016) show that\n1Copyright \u00a92016 This is the authors draft of the work. It is posted here for your personal use. Not for redistribution.\nthe different senses of a word can be recovered by a sparse coding in such embedding spaces.\nIn this work, we carry forward this line of research to understand the different properties of word embedding methods. First we motivate our research direction by noting the following: (a) The optimization to learn word embeddings is usually non-convex and hence depends on the random initialization and (b) Word embeddings (like skip-gram or GloVe) learned by neural models under different random initializations mostly converge to yield very similar performances on NLP tasks. This observation begs the following question: Do word embeddings obtained under two different random initializations actually learn equivalent features? If not, is their similar performance on downstream tasks merely a coincidence? To illustrate with an example, consider the performance of GloVe embeddings obtained on two independent runs (differing only in the random initialization) on the analogy task (Mikolov et al., 2013a) in Table 1. Although both embeddings show a high accuracy on this task (see Table 2), a closer look at the questions where they disagree reveals differences in the relationships they learn. An improved understanding of such properties of word embedding methods will enable the development of improved models and learning algorithms (which will will in-turn boost performance on downstream NLP tasks).\nSpecifically our contributions are as follows:\n\u2022 Mapping-based Similarity Measure between Dimensions: We propose a similarity measure between dimensions of two sets of embeddings, based on correlations using oneto-one or many-to-one linear transformation between these dimensions.\n\u2022 Intrinsic Way to Evaluate Embeddings: Based on this similarity measure, we propose\nar X\niv :1\n60 5.\n03 95\n6v 1\n[ cs\n.C L\n] 1\n2 M\nay 2\n01 6\na new, intrinsic way to quantify the similarity between two such embedding spaces. Our preliminary results show that our measure correlates with a measure of model agreement on the well known analogy task."}, {"heading": "2 Background and Setup", "text": "Here we describe the datasets and word embeddings we use in our experiments. We analyze two word embeddings GloVe (Pennington et al., 2014) and skip-gram (SG) (Mikolov et al., 2013a) where we set the number of dimensions D to be 300. Our corpora consists of a combination of English Wikipedia dump with 1.6 billions tokens and the English section of the News Crawl2 with 4.3 billion tokens totaling to 6 billion tokens. We use similar settings as Pennington et al. (2014). We set the vocabulary to the 400, 000 most frequent words, use a context window of size 10, and make 50 iterations through the whole dataset. We use News Crawl for two reasons: It is a public available corpus of the same genre as Gigawords 5 (news in formal English), and News Crawl is also available in Czech, German, Finish and Russia which allows our analysis to extend to multilingual settings.\nWe train both Glove and skip-gram embeddings twice using different random initializations and order of corpus, and refer to them\n2Available at http://www.statmt.org/wmt16/ translation-task.html. We use articles from 2007 to 2015.\nas GloVe-1, GloVe-2, SG-1, SG-2 respectively. The accuracies of these embeddings on the word analogy task is shown in Table 2. Note that these accuracies are comparable to scores obtained on the same task in Pennington et al. (2014). We denote these 4 embeddings as E(m) (a matrix of dimensions |V | \u00d7 D) where m \u2208 {GloVe-1,GloVe-2,SG-1,SG-2}.\nLet E(m)i denote feature dimension i of E (m) (column i of E(m)). We now seek to quantify the similarity between embeddings E(m) and E(n), the methods for which we discuss in forthcoming sections."}, {"heading": "3 Similarities of Word Embeddings", "text": "How similar are the embeddings learned from independent runs over the same corpus? More formally, given a pair of D dimensional embeddings E(m) and E(n) differing only in their initial random initialization:\n\u2022 Can we define a measure of similarity between the dimensions (E(m)i , E (n) j )? \u2022 Given such a measure, can we align dimensions in E(m) to dimensions in E(n) to quan-\ntify the similarity between E(m) and E(n)?"}, {"heading": "3.1 Measure of Similarity between Feature Dimensions", "text": "Given E(m) and E(n) we define the similarity between them as \u03ba(i, j) = \u03c1(E(m)i ,E (n) j ) where \u03c1(x, y) is defined as the Pearson Correlation Coefficient between the column vectors x and y. We note that this similarity measure is well-suited to capture linear relationships between feature dimensions. We leave exploring metrics that capture non-linear relationships (like mutual information) between feature dimensions to future work.\nIn Figure 1 we show in blue, the histogram of values in correlation matrices \u03ba(i, j) on E(m) and E(n) obtained on the Glove and skip-gram embeddings."}, {"heading": "3.2 One-to-one Alignment", "text": "Since the optimization problems involved in GloVe and skip-gram are inherently non-convex, the embeddings learned could potentially correspond to different local optima. This implies the features learned on different runs could be equivalent under some manifold transformation. Therefore as a first step, we ask: Is there a one-to-one alignment between features in two embeddings? (similar to Li et al. (2016))\nTherefore, given E(m) and E(n), we seek to find an one-to-one matching represented as a = (a1, a2, . . . aD) meaning that for each d \u2208 {1, 2, . . . , D}, E(n)d is matched with E (m) ad , where\nthe aggregated correlation is given by\n\u03b61to1 = 1\nD D\u2211 d=1 \u03c1(E (n) d ,E (m) ad ). (1)\nBy building a complete bipartite graph, this can be converted into an instance of maximum weighted bipartite matching (West, 1996), which can be solved effectively using Hopcroft-Karp Algorithm (Hopcroft and Karp, 1973) in polynomial time.\nThis perfect bipartite matching allows us to permute features (columns) of E(m) to correspondingly match features in E(n). In Figure 1, we show in green the histogram of matched correlations for both GloVe (Figure 1a) and skip-gram (Figure 1b).\nFinally, we show the correlations between matched dimensions in Figure 2. Observe that using the one-to-one alignment between the dimensions of the embeddings, the Glove embeddings display a stronger correlation (21.6%) than skip-gram (19.5%). Note that random embeddings show correlations of 0. We believe this indicates that Glove embeddings are more stable and consistent than skip-gram under random initialization."}, {"heading": "3.3 Many-to-one Mapping", "text": "In previous section we sought a one-to-one alignment between the features in two embeddings, One drawback of which is its restrictiveness since it assumes both models learn exactly the same set of features equivalent under permutations. In this section, we hypothesize that a feature in E(m) could correspond to multiple features in E(n). Hence we relax the restrictions to a many-to-one\nmatching, by assuming a single dimension in E(n) can be obtained to some degree from a linear transformation of multiple dimensions in E(m) . We capture this by measuring linear relationship between E(n) and E(m) transformed using CCA (Canonical Correlation Analysis).\nGiven two matrices X \u2208 R|T |\u00d7d1 and Y \u2208 R|T |\u00d7d2 , CCA finds two projections P \u2208 R|T |\u00d7d and Q \u2208 R|T |\u00d7d (d \u2264 min(d1, d2)) that maximizes the correlation \u03c1(Pxi, Qyj) between corresponding columns\narg max P,Q d\u2211 i=1 \u03c1(Pxi, Qyi). (2)\nUsing the projection matrices P and Q, one can now project the inputs X and Y to yield U = PX and V = QY such that \u03c1(Pxi, Qyi) is maximized. Therefore we propose to use CCA on two embeddings E(n) and E(m) of dimension D and define the similarity between E(n) and E(m) as\n\u03b6CCA = 1\nD D\u2211 i=1 \u03c1(Pxi, Qyi). (3)\nIn Figure 1, we show in red, the histogram of sample canonical correlations obtained from CCA for GloVe and skip-gram embedding methods. These values stand out from one-to-one matching of features, and show different distributions for GloVe and skip-gram. Also in Figure 3 we show the sample canonical correlations obtained in descending order. Both skip-gram and GloVe demonstrate strong correlations on several dimensions as compared to random embeddings. It is also clear that GloVe is better than skip-gram as measured by \u03b6CCA. This is consistent with previous observation in Figure 2.\nFinally, we show that our proposed metric, \u03b6CCA, correlates well with the agreement on the analogy task, as shown in Table 3. We emphasize that our metric is completely intrinsic, does not require any external data for evaluation, and yet captures useful convergent properties of embedding models."}, {"heading": "4 Conclusion", "text": "In this paper we proposed an intrinsic metric for evaluating word embeddings \u2013 their consistency across random initializations. Our preliminary results showed that while total performance remained consistent across embeddings, there was a sizeable disagreement between each instance of a particular embedding. To examine this difference in more depth, we investigated the similarity between the dimensions of each embedding space. Furthermore we propose methods to align dimensions of word embeddings and an intrinsic metric \u03b6cca to measure the similarity of the learned word embeddings, which our preliminary results show may correlate with downstream tasks in terms of model agreement, with the popular word analogy task in Mikolov et al. (2013a) as an example. We believe the methods and metrics proposed in our work will enable a deeper investigation into the convergent properties of embedding models, and lead to improved optimization algorithms and performance on downstream NLP tasks."}], "references": [{"title": "Random walks on context spaces: Towards an explanation of the mysteries of semantic word embeddings", "author": ["Sanjeev Arora", "Yuanzhi Li", "Yingyu Liang", "Tengyu Ma", "Andrej Risteski."], "venue": "arXiv preprint arXiv:1502.03520.", "citeRegEx": "Arora et al\\.,? 2015", "shortCiteRegEx": "Arora et al\\.", "year": 2015}, {"title": "Linear algebraic structure of word senses, with applications to polysemy", "author": ["Sanjeev Arora", "Yuanzhi Li", "Yingyu Liang", "Tengyu Ma", "Andrej Risteski."], "venue": "arXiv preprint arXiv:1601.03764.", "citeRegEx": "Arora et al\\.,? 2016", "shortCiteRegEx": "Arora et al\\.", "year": 2016}, {"title": "A fast and accurate dependency parser using neural networks", "author": ["Danqi Chen", "Christopher D Manning."], "venue": "Empirical Methods in Natural Language Processing (EMNLP).", "citeRegEx": "Chen and Manning.,? 2014", "shortCiteRegEx": "Chen and Manning.", "year": 2014}, {"title": "An n\u02c65/2 algorithm for maximum matchings in bipartite graphs", "author": ["John E Hopcroft", "Richard M Karp."], "venue": "SIAM Journal on computing, 2(4):225\u2013231.", "citeRegEx": "Hopcroft and Karp.,? 1973", "shortCiteRegEx": "Hopcroft and Karp.", "year": 1973}, {"title": "Convergent learning: Do different neural networks learn the same representations", "author": ["Yixuan Li", "Jason Yosinski", "Jeff Clune", "Hod Lipson", "John Hopcroft"], "venue": "In International Conference on Learning Representation", "citeRegEx": "Li et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Li et al\\.", "year": 2016}, {"title": "Recurrent neural network based language model", "author": ["Tomas Mikolov", "Martin Karafi\u00e1t", "Lukas Burget", "Jan Cernock\u1ef3", "Sanjeev Khudanpur."], "venue": "INTERSPEECH, volume 2, page 3.", "citeRegEx": "Mikolov et al\\.,? 2010", "shortCiteRegEx": "Mikolov et al\\.", "year": 2010}, {"title": "Exploiting similarities among languages for machine translation", "author": ["Tomas Mikolov", "Quoc V Le", "Ilya Sutskever."], "venue": "nternational Conference on Learning Representations (ICLR).", "citeRegEx": "Mikolov et al\\.,? 2013a", "shortCiteRegEx": "Mikolov et al\\.", "year": 2013}, {"title": "Linguistic regularities in continuous space word representations", "author": ["Tomas Mikolov", "Wen-tau Yih", "Geoffrey Zweig."], "venue": "HLT-NAACL, pages 746\u2013 751.", "citeRegEx": "Mikolov et al\\.,? 2013b", "shortCiteRegEx": "Mikolov et al\\.", "year": 2013}, {"title": "Glove: Global vectors for word representation", "author": ["Jeffrey Pennington", "Richard Socher", "Christopher D. Manning."], "venue": "Empirical Methods in Natural Language Processing (EMNLP), pages 1532\u2013 1543.", "citeRegEx": "Pennington et al\\.,? 2014", "shortCiteRegEx": "Pennington et al\\.", "year": 2014}, {"title": "Multilingual Part-of-Speech Tagging with Bidirectional Long Short-Term Memory Models and Auxiliary Loss", "author": ["B. Plank", "A. S\u00f8gaard", "Y. Goldberg."], "venue": "ArXiv e-prints, April.", "citeRegEx": "Plank et al\\.,? 2016", "shortCiteRegEx": "Plank et al\\.", "year": 2016}, {"title": "Recursive deep models for semantic compositionality over a sentiment treebank", "author": ["Richard Socher", "Alex Perelygin", "Jean Y Wu", "Jason Chuang", "Christopher D Manning", "Andrew Y Ng", "Christopher Potts."], "venue": "Proceedings of the conference on", "citeRegEx": "Socher et al\\.,? 2013", "shortCiteRegEx": "Socher et al\\.", "year": 2013}, {"title": "Introduction to graph theory", "author": ["Douglas B West"], "venue": null, "citeRegEx": "West.,? \\Q1996\\E", "shortCiteRegEx": "West.", "year": 1996}], "referenceMentions": [{"referenceID": 5, "context": "Word embeddings learned using neural methods have been shown to be tremendously effective on several NLP tasks (Mikolov et al., 2010; Chen and Manning, 2014; Socher et al., 2013; Plank et al., 2016).", "startOffset": 111, "endOffset": 198}, {"referenceID": 2, "context": "Word embeddings learned using neural methods have been shown to be tremendously effective on several NLP tasks (Mikolov et al., 2010; Chen and Manning, 2014; Socher et al., 2013; Plank et al., 2016).", "startOffset": 111, "endOffset": 198}, {"referenceID": 10, "context": "Word embeddings learned using neural methods have been shown to be tremendously effective on several NLP tasks (Mikolov et al., 2010; Chen and Manning, 2014; Socher et al., 2013; Plank et al., 2016).", "startOffset": 111, "endOffset": 198}, {"referenceID": 9, "context": "Word embeddings learned using neural methods have been shown to be tremendously effective on several NLP tasks (Mikolov et al., 2010; Chen and Manning, 2014; Socher et al., 2013; Plank et al., 2016).", "startOffset": 111, "endOffset": 198}, {"referenceID": 1, "context": "(2015) attempt to explain this linear substructure by proposing a generative model based on random walks over discourse vectors and (Arora et al., 2016) show that", "startOffset": 132, "endOffset": 152}, {"referenceID": 3, "context": "Mikolov et al. (2013b) show that embeddings learned by Skip-Gram model reveal linear substructures.", "startOffset": 0, "endOffset": 23}, {"referenceID": 0, "context": "Arora et al. (2015) attempt to explain this linear substructure by proposing a generative model based on random walks over discourse vectors and (Arora et al.", "startOffset": 0, "endOffset": 20}, {"referenceID": 6, "context": "on downstream tasks merely a coincidence? To illustrate with an example, consider the performance of GloVe embeddings obtained on two independent runs (differing only in the random initialization) on the analogy task (Mikolov et al., 2013a) in Table 1.", "startOffset": 217, "endOffset": 240}, {"referenceID": 6, "context": "Table 1: A set of examples of questions in word analogy task (Mikolov et al., 2013a) where two independent runs of GloVe embeddings differ in the answers.", "startOffset": 61, "endOffset": 84}, {"referenceID": 6, "context": "Table 2: Performance of two independent runs of GloVe and skip-gram on word analogy task (Mikolov et al., 2013a).", "startOffset": 89, "endOffset": 112}, {"referenceID": 8, "context": "We analyze two word embeddings GloVe (Pennington et al., 2014) and skip-gram (SG) (Mikolov et al.", "startOffset": 37, "endOffset": 62}, {"referenceID": 6, "context": ", 2014) and skip-gram (SG) (Mikolov et al., 2013a) where we set the number of dimensions D to be 300.", "startOffset": 27, "endOffset": 50}, {"referenceID": 5, "context": ", 2014) and skip-gram (SG) (Mikolov et al., 2013a) where we set the number of dimensions D to be 300. Our corpora consists of a combination of English Wikipedia dump with 1.6 billions tokens and the English section of the News Crawl2 with 4.3 billion tokens totaling to 6 billion tokens. We use similar settings as Pennington et al. (2014). We set the vocabulary to the 400, 000 most frequent words, use a context window of size 10, and make 50 iterations through the whole dataset.", "startOffset": 28, "endOffset": 340}, {"referenceID": 8, "context": "Note that these accuracies are comparable to scores obtained on the same task in Pennington et al. (2014). We denote these 4 embeddings as E(m) (a matrix of dimensions |V | \u00d7 D) where m \u2208 {GloVe-1,GloVe-2,SG-1,SG-2}.", "startOffset": 81, "endOffset": 106}, {"referenceID": 4, "context": "Therefore as a first step, we ask: Is there a one-to-one alignment between features in two embeddings? (similar to Li et al. (2016))", "startOffset": 115, "endOffset": 132}, {"referenceID": 11, "context": "By building a complete bipartite graph, this can be converted into an instance of maximum weighted bipartite matching (West, 1996), which can be solved effectively using Hopcroft-Karp Algorithm (Hopcroft and Karp, 1973) in polynomial time.", "startOffset": 118, "endOffset": 130}, {"referenceID": 3, "context": "By building a complete bipartite graph, this can be converted into an instance of maximum weighted bipartite matching (West, 1996), which can be solved effectively using Hopcroft-Karp Algorithm (Hopcroft and Karp, 1973) in polynomial time.", "startOffset": 194, "endOffset": 219}, {"referenceID": 5, "context": "Furthermore we propose methods to align dimensions of word embeddings and an intrinsic metric \u03b6cca to measure the similarity of the learned word embeddings, which our preliminary results show may correlate with downstream tasks in terms of model agreement, with the popular word analogy task in Mikolov et al. (2013a) as an example.", "startOffset": 295, "endOffset": 318}], "year": 2016, "abstractText": "Do word embeddings converge to learn similar things over different initializations? How repeatable are experiments with word embeddings? Are all word embedding techniques equally reliable? In this paper we propose evaluating methods for learning word representations by their consistency across initializations. We propose a measure to quantify the similarity of the learned word representations under this setting (where they are subject to different random initializations). Our preliminary results illustrate that our metric not only measures a intrinsic property of word embedding methods but also correlates well with other evaluation metrics on downstream tasks. We believe our methods are is useful in characterizing robustness \u2013 an important property to consider when developing new word embedding methods. 1", "creator": "TeX"}}}