{"id": "1605.04238", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "13-May-2016", "title": "Semantic Spaces", "abstract": "Any natural language can be considered as a tool for producing large databases (consisting of texts, written, or discursive). This tool for its description in turn requires other large databases (dictionaries, grammars etc.). Nowadays, the notion of database is associated with computer processing and computer memory. However, a natural language resides also in human brains and functions in human communication, from interpersonal to intergenerational one. We discuss in this survey/research paper mathematical, in particular geometric, constructions, which help to bridge these two worlds. In particular, in this paper we consider the Vector Space Model of semantics based on frequency matrices, as used in Natural Language Processing. We investigate underlying geometries, formulated in terms of Grassmannians, projective spaces, and flag varieties. We formulate the relation between vector space models and semantic spaces based on semic axes in terms of projectability of subvarieties in Grassmannians and projective spaces. We interpret Latent Semantics as a geometric flow on Grassmannians. We also discuss how to formulate G\\\"ardenfors' notion of \"meeting of minds\" in our geometric setting. In general, we consider G\\\"ardenfors' concept of \"being in the same room\" as described in the above text. For instance, we define G\\\"ardenfors' concept of \"meeting of minds\" in G\ufffdardenfors' concept of \"seeing\", and refer to it to it as a \"mind\" which represents \"the whole room\", rather than a \"mental room\". This implies that G\ufffdardenfors' concept of \"meeting of minds\" is not necessarily a mathematical concept, but an analytical theory of the language. It is rather a mathematical concept, based on a mathematical concept. In particular, in this paper we examine how to formulate the term \"meeting of mind\" in G\ufffdardenfors' concept of \"meeting of minds\" in G\ufffdardenfors' concept of \"seeing\", and refer to it to it as a \"mind\" which represents \"the whole room\", rather than a \"mental room\". This implies that G\ufffdardenfors' concept of \"meeting of minds\" is not necessarily a mathematical concept, but an analytical theory of the language. It is rather a mathematical concept, based on a mathematical concept. In particular, in this paper we examine how to formulate the term \"meeting of minds\" in G\ufffdardenfors' concept of \"seeing\", and refer to", "histories": [["v1", "Fri, 13 May 2016 16:25:38 GMT  (112kb)", "http://arxiv.org/abs/1605.04238v1", "32 pages, TeX, 1 eps figure"]], "COMMENTS": "32 pages, TeX, 1 eps figure", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["yuri manin", "matilde marcolli"], "accepted": false, "id": "1605.04238"}, "pdf": {"name": "1605.04238.pdf", "metadata": {"source": "CRF", "title": null, "authors": ["Yuri I. Manin", "Matilde Marcolli"], "emails": [], "sections": [{"heading": null, "text": "ar X\niv :1\n60 5.\n04 23\n8v 1\n[ cs\n.C L\n] 1\nO INTERIOR DO EXTERIOR DO INTERIOR\nPascal Mercier \u201cNachtzug nach Lissabon\u201d\n1. Introduction:\nlinguistics, semiotics, and topology\nOne of the basic \u201cmeta\u2013physical\u201d principles of classical physics consisted in the subdivision of informational content of any physical model into two parts:\n\u2013 a description of the configuration and phase spaces of the studied system; \u2013 a description of the time evolution law (usually a vector field in the phase\nspace). 1\n2 Some of the recent approaches to semantics of natural languages describe various versions of \u201cspaces of meanings\u201d which we consider as a metaphorical analog of configuration spaces: cf. comprehensive accounts [Ga\u030800], [Ga\u030814]. For Ga\u0308rdenfors, semantics is (in particular) meeting of minds, and the space of meanings is the space where minds meet.\nOur initial motivation for undertaking this survey and the research summarised in this paper was our desire to introduce \u201ca time dimension\u201d in this discussion, to see a discourse or reception of a text as a path in the appropriate space of meanings.\nIn particular, we wanted to use mathematical models in order to bridge the approaches to semantics reviewed in [Ga\u030814], neurolinguistic studies reviewed in [JeLe94], [InLe04], and neurobiological studies of neural mechanism involved in coping with tasks related to orientation in physical space (see [CuIt08], [CuItVCYo13] an brief survey for mathematicians [Ma15].)\nIn the remaining part of the introduction we will give a very short list of several approaches to description of \u201cmeaning\u201d using geometric/topological representations and/or metaphors.\n1.1. Semic axes. In the following it is essential to keep in mind that core \u201cmeanings\u201d are generally assigned not to \u201cwords\u201d but to \u201clexemes\u201d. According to [Me16], p. 240, lexeme is \u201ca word taken in one well defined sense \u2013 more precisely, a set of all word forms and analytical form phrases that differ only by inflectional significations.\u201d\nExample ( [Me16], p. 135): lexeme TAKE(V ) includes the following lexical items: take, takes, took, taking, . . . , have taken, has taken, . . . , have been taken, . . .\nThe tag (V) here means that our lexeme refers to the word \u201dtake\u201d understood as a verb rather than a noun.\nWhen one extracts a vocabulary of lexemes from a dictionary of words, one should do \u201cstemming\u201d (extracting roots of words), \u201ctagging\u201d etc., cf. a more detailed description in Sec. 3 of [TuPa10].\nWe will allow ourselves the use of the term \u201cword\u201d in place of \u201clexeme\u201d when it cannot lead to a confusion.\nThe approach to encoding of meaning, or \u201csense\u201d of lexemes, briefly surveyed in [Gui08], starts with postulating a list of \u201csemes\u201d such as animate, inanimate, actor, process etc.\nThe meaning is specified by listing a subset of semes.\n3 In the respective geometric picture, N semes are represented by basis vectors ei, i = 1, . . . , N , of RN , and meanings are represented by (a subset of) vertices of the unit cube [0, 1]N . P. Guiraud actually prefers the \u201cbisemic\u201d description, in which meanings are represented by a subset of vertices of [\u22121, 1]N . Sign changes of basic coordinates represent the complementarity relations such as in animate/inanimate.\nA qualitative weakening of the bisemic model allows meanings to be represented by points in RN that are localised near the boundary of the unit cube, but not necessarily coincide with its vertices. A nice illustration is given on p. 59 of [Ga\u030814]. It represents bisemes in a two\u2013dimensional \u201cemotional space\u201d R2 whose bisemic axes represent dichotomies pleasure/displeasure and high/low whereas, say, the quadrant \u201clow pleasure\u201d accommodates lexemes content, serene, calm, relaxed, sleepy.\nSome of the largest subsets of the space of meanings that can accommodate, say, path of a narrative, might encode notions related to\n\u2013 senses: vision, hearing, feelings, time, space . . . \u2013 some subregions like \u201cfar away \u2013 near\u201d , \u201cquiet \u2013 loud\u201d, \u201cpast \u2013 future\u201d \u2013 regions related to \u201cme\u201d, to \u201cother people\u201d , \u201cunrelated to humans\u201d, etc.\nWhat is important is that we should construct this semantic space at first in a way maximally independent of the \u201cnatural language\u201d we choose, and that it will widen at each stage of construction in order to accommodate new words, sentences, languages etc.\n1.2. Semic axes and neural encoding of place field recognition. We want to derive from semantics of a natural language a structure encoding it that would be a space covered by subsets, say, Ui. (Some) non-empty finite intersections should correspond to words or short sentences, paths through this space should correspond to texts.\nA nice example of this is provided in [Li], together with a picture representing symbolically two different subsets of semantic space in two possible mutual relationships: (i) inclusion of one in another, and (ii) non\u2013empty intersection without inclusion.\nThis picture illustrates the difference between usages of words which and that in the following two sentences:\nCorrect use of that: \u201cTiffany likes shoes that are expensive\u201d.\n\u201cThe set of things called shoes includes both expensive and inexpensive shoes, so when we say \u2018that are expensive,\u2019 we are talking only about a subset of the set of all things called shoes.\u201d\n4 Correct use of which: \u201cTiffany likes emeralds, which are expensive\u201d.\n\u201cThe set of things called emeralds are all expensive, so the clause \u2018which are expensive\u2019 talks about the whole set of emeralds. There is no inexpensive subset of emeralds. \u2018Which are expensive\u2019 simply gives you additional information about this whole set\u201d ([Li]).\nThis basic picture representing meanings by domains in the space of meanings and the relationship of intersection/inclusion between the respective domains fits very well the studies aimed to the understanding how brain copes with multiple tasks of orienting and navigating in the world, cf. [CuIt08], [CuItVCYo13], [Yo14], and references therein.\nThe brain of an animal must be able to reconstruct, say, a map of its environment and its current position in it, using only the action potentials (spikes) of the relevant cell groups. In laboratory experiments it is found that stimuli related to the positions are naturally divided into groups, and with each group a certain type of neural activity is associated. In [CuIt08] and [Yo14], it is postulated that a given domain of stimuli can be modelled via a topological, or metric stimuli space X. Furthermore, brain reaction to a point in X is modelled by spiking activity of certain finite set of neurons NX . The list of subsets of NX consisting of subsets whose neurons can be activated simultaneously, corresponds to a certain covering of X . Thus this covering can be described by a binary code, and relations of intersection/inclusion between domains coincide with the relations of intersection/inclusion between the respective code words. For more details, see [Ma15].\n1.3. Meaning\u2013Text model. In the model of semic axes, there is one intrinsic source of incompleteness: as P. Guiraud says ([Gui68], p. 157), the lexical units (corresponding to vertices of [\u22121, 1]N) \u201cmust in addition be associated in syntagms, each one of them constitutes a \u2018sense\u2019. But there again we must setup rules for combinations, for the sense supposes that certain syntagms are permitted, other excluded.\u201d The difference between which and that discussed above is precisely an example of such syntagms.\nThis problem is very systematically addressed in the model \u201cMeaning\u2013Text\u201d which I. Mel\u2019c\u030cuk and his collaborators have been developing for several decades: see [Me16] for its most recent summary and further references.\nIn this model, meaning of a text of language L \u201cis exclusively [ . . . ] linguistic meaning\u201d that can be extracted \u201conly on the basis of the mastery of L, without the\n5 participation of common sense, encyclopaedic knowledge, logic etc. ([Me16], Sec. 3.2.2).\nOn the other hand, geometry/topology figures in this model mainly as a tool for producing graphs of various levels of linguistic representation. Each such graph consists of several vertices, certain pairs of which are connected by edges. Moreover, both vertices and edges are additionally marked. For example, on the level of the surface\u2013syntactic structure, a sentence is represented by the graph, whose vertices are marked by lexemes corresponding to the words in this sentence, and by additional information encoding the passage from the lexeme to the word. Edges of this graph are marked by technical terms expressing syntactic relations between the respective pair of words.\nBelow we illustrate this principle by presenting the surface\u2013syntactic graph of the first line of a sonnet by Michelangelo. We are very grateful to I. Mel\u2019c\u030cuk who produced for us this graph and allowed us to reproduce it here.\n1.4. Neurolinguistic data. There exists a large body of neuroimaging studies of production and perception of spoken language. In the survey [InLe04], the reader will find descriptions of methodology used and results obtained in \u201cthe enterprise of relating the function components of word production, such as lexical selection,\n6 phonological code retrieval, and syllabification, to regions in a cerebral network\u201d ([InLe04], p. 102.)\nAn illustration of segment of lexical production network ([JeLe94], p. 826) shows fascinating parallels with the Meaning\u2013Text model.\nDue to the vastness of semantic space needed to accommodate all meanings expressible in a natural language, direct comparison with the neural encoding of place field recognition as in [CuIt08] is not yet feasible. However, the development of new methods of studying and collecting databases of results allows us to hope that such comparison will become possible. In this paper, we try to contribute some mathematical tools that may be useful for this endeavor.\n1.5. This report. Most of the approaches discussed above directly appeal to the linguistic intuition and communicative experience of scientists, experimenters, and participants in experiments. Information obtained by the respective methods should be considered as local data about semantic space, and/or about short paths in it.\nOn the other hand, if we want to obtain mathematical models of topology of semantic spaces and of longer routes in such a space, expressed by texts of the size, say, of a chapter in \u201cWar and Peace\u201d, we may turn to the statistical natural language processing.\nThen, in the first approximation, a text becomes a point in the space of paths in the semantic space, and we discuss here approaches to studying the topology of such spaces appealing mostly to the data about frequencies of lexemes and other text fragments taking in account their linear ordering in the text. Semantics of such fragments as it is represented by dictionaries and experiments is thus put aside to a certain degree, although not fully.\nIn the main body of this article, we will describe some mathematical tools that can be used for the introduction of \u201ctime dimension\u201d in the study of texts. They will refer to the geometry of real projective spaces and real Grassmannians. Passage from texts to the relevant geometry is based here on the Vector Space Models of semantics (VSM) surveyed in [TuPa10], and we will briefly explain this model for further use.\nIn Section 2 we discuss how the frequency matrix of the VSM approach, that counts occurrences of lexemes in contexts in a given corpus of texts, determines a point in a Grassmannian. We show that, in the case of a large vocabulary of\n7 lexemes and a smaller number of contexts, the condition that the resulting point lies in the positive Grassmannian provides a geometric test for the property that a choice of lexemes gives a good semantic disambiguation of the contexts. A similar condition holes in the case of a small number of lexemes, where one wants to test if a set contexts would disambiguate the words semantically. This geometric viewpoint takes into account the fact that contexts come with a specific ordering by occurrence in a text.\nIn Section 3 we discuss other geometric models associated to the frequency matrices of the VSM approach, which also takes into account the specific ordering of contexts in a text. We assign to a text a piecewise geodesic path of points in a projective space. Instead of measuring semantic relatedness in terms of angle distances between the semantic vectors of the frequency matrices, as it is customary to do in Natural Language Processing, we compare the paths in an ambient projective space through a geometric distance function between (geodesic) polygonal curves, which is known to be computable in polynomial time. In a variant of this construction, we also consider assigning to the frequency matrix a point in a flag variety, where the flag corresponds to the span of successive semantic vectors for the successive contexts ordered by occurrence in a text. Again semantic similarity can be measured in terms of the geodesic distance in the flag variety, with respect to its natural metric as a quotient of Lie groups.\nIn Section 4 we consider the case where lexemes are grouped together according to some semantic axes, either by explicit semantic tagging (supervised learning) or just by grouping together lexemes with similar occurrences in contexts (unsupervised learning). In both cases, we describe the process of passing from frequency matrices for a given corpus of text, computed with respect to a dictionary of lexemes, to density matrices with respect to a semantic dictionary, where identification of lexemes by semantic criteria has already occurred. When we view the frequencies as determining points in Grassmannians, we can view geometrically this operation as a projection between two Grassmannian. The question of whether one can avoid loss of semantic information in this process, when applied to a given collection of texts, is then interpreted in terms of whether the points corresponding to these texts lie on a subvariety of the Grassmannian that can be isomorphically projected to the other Grassmannians. A similar condition arises when we assign to a given text a piecewise geodesic path in a projective space as discussed in Section 3.\nIn Section 5 we connect the geometric setting described in the previous section with the point of view of persistent topology. According to our previous construc-\n8 tion, a large corpus of texts determines a corresponding set of points in an ambient Grassmannian, where we assume that the same fixed dictionary of lexemes (or semes) is used to analyze all texts in the corpus. We then show that one can identify more refined forms of semantic relatedness between these points. These are topological in nature and arise from constructing Vietoris\u2013Rips simplicial complexes at varying scales, associated to the set of points in the ambient variety and computing their persistent homology. We discuss possible relations to the use of persistent topology in the theory of neural codes.\nIn Section 6 we show that the Latent Semantics technique for dealing with very sparse frequency matrices in the VSM approach, which identifies lower dimensional subspaces (latent meanings) through singular value decomposition, can be interpreted in terms of the geometry of Grassmannians described in Section 2, as a Riccati flow on the ambient Grassmannian.\nIn Section 7 we discuss how to implement, in our geometric setting, a model analogous to Ga\u0308rdenfors\u2019 \u201cmeeting of minds\u201d, where common meaning between different users communicating with one another is achieved as via a fixed point problem in a convex semantic space. We suggest that a similar idea can be implemented in our setting if different users come to somewhat different semantic interpretations of a given texts, on the bases of semantic interpretations based on other texts available to them, under the assumption that users have access to different (partially overlapping) corpora of texts. We then describe the procedure of \u201cmeeting of minds\u201d as the construction of a geodesic barycenter in the ambient geometric space of the distribution of points obtained by the users, possibly weighted according to some measure of \u201creliability\u201d of the different corpora used for semantic interpretation.\nIn Section 8 we discuss how to compensate for the fact that the frequency distribution for words in a dictionary is skewed towards the more frequent and less semantically significant words according to Zipf\u2019s law."}, {"heading": "2. Vector Space Models of semantics", "text": "2.1. Texts and their processing. A concrete VSM starts with a large corpus of natural language texts and produces from it a matrix of numbers (frequences). The intermediate steps of this production are subdivided into two groups: (i) linguistic processing, and (ii) statistic processing.\nFor us, linguistic processing results in the creation of the relevant vocabulary of lexemes where we understand \u201clexeme\u201d as in 1.3 above. Each text is also represented\n9 as a sequence of the relevant lexemes, although from the description of [TuPa10] it becomes clear that at least some fragments of it are modelled by their surface\u2013 syntactic structures in the sense of Meaning\u2013Text model.\nWe accept this as a reasonable approximation to the procedures described in Sec. 3 of [TuPa10].\nStatistic processing, as we mentioned, produces a (normalised) matrix of frequencies, see [TuPa10], Sec. 4.\nIn the typical case called \u201cthe term\u2013document matrix\u201d in [TuPa10], rows of the matrix are labelled by lexemes (\u201cterms\u201d), whereas columns are labelled by texts in our collection.\nIn another typical case called \u201cthe word\u2013context matrix\u201d ([TuPa10], Sec. 2.5), the texts, already at the stage of linguistic processing, are represented as a union of \u201ccontexts\u201d. Here again the rows of matrix are labelled by lexemes, whereas columns are labelled by contexts.\nFinally, matrix entries as we mentioned characterise correlations between the lexemes and text/contexts. We will treat in more detail some cases below, and address the question of \u201csmoothing\u201d.\n2.2. \u201cTime dimension\u201d and other linear orderings. Any vocabulary of lexemes, or contexts, must be in the final count also presented as linearly ordered dictionary. This ordering might be totally irrelevant to the situation under study (as e.g. alphabetic ordering). It can take into account the order of first appearance of the respective lexeme in the text. Finally, it can be a Zipf\u2019s\u2013like ordering according to diminishing frequency rate.\nA considerable part of statistical characteristics of a VSM does not depend of the chosen orderings (although the mode of their usage might depend on it). However, for the purpose of our paper this might become essential, and we will pay due attention to it.\n2.3. Notation and assumptions. We will consider word\u2013contexts matrices described above in one of two possible extreme subcases.\n(A). Large vocabulary case. In this setting, we assume that our vocabulary of lexemes is sufficiently large and includes at least all the lexemes that appear in the texts (excluding words with large occurrences in all contexts such as \u201cand\u201d or \u201cthe\u201d in an English text that are semantically less informative). Moreover, we assume that the size of the vocabulary is large compared to the number of contexts in the\n10\ntexts. In this case, one aims at selecting from the large dictionary choices of words that best represent the given contexts semantically.\n(B). Information retrieval case. In this case we consider a vocabulary that is small compared to the number of contexts, as would be the case with a choice of words used in a query. In this case one aims at selecting among the various contexts in a given corpus those that best match semantically the chosen words in the query.\nLet D be our vocabulary, and M = #D be the number of lexemes in it.\nA given text T is then endowed with a set of subtexts called contexts: C(T ) = {c1, . . . , cN}. Typical examples of contexts are: sentences, paragraphs, or else windows of certain length around each word/lexeme.\n2.4. Matrix of frequencies. Following [TuPa10], one produces from these data an N \u00d7M matrix of frequencies P = P (T ) with entries pij . Here pij is the estimated probability (frequency) of occurrence of the word wi \u2208 D in the context cj \u2208 C(T ). In the VSM model, one usually considers also the matrix X = X(T ) with entries X = (xij),\nxij = max{0, log\n(\npij pi\u22c6p\u22c6j\n)\n},\nwhere pi\u22c6 = \u2211\nj pij is the estimated probability of the word wi \u2208 D and p\u22c6j = \u2211\ni pij is the estimated probability of the context cj \u2208 C(T ). The condition that pij = pi\u22c6p\u22c6j corresponds to statistical independence of word wi and context cj , while pij > pi\u22c6p\u22c6j signals the presence of a semantic relation between them.\nMore precisely, the formula pi\u2217 = \u2211\nj pij gives the frequency of appearance in the text in the case where contexts do not overlap whereas their union is the whole text.\nIn the more general case where contexts may overlap one still uses the same matrix but now its entries are the frequencies of appearance across all contexts (or, equivalently, the frequencies of appearance in the text, weighted by some multiplicities that keep track of when a word appears in the intersection of more than one context).\nIf a word is in the intersection of two adjacent contexts j and j+1, then it affects the counting in both pij and pi,j+1, so \u2211 j pij is still the normalization factor.\nThe typical example of this \u201coverlapping contexts\u201d method is the original Shannon 3-gram model: here one has probabilities (based on frequencies) for occurrences\n11\nof 3 words in a row. For example, one can have a word sequence a-b-c-d where the three words a-b-c have a very high probability of occurring together, while the probability of the triple b-c-d is very low. This suggests that it is a-b-c rather than b-c-d that clarifies better the semantic meaning of the words b and c, and that the semantic meaning of d will more likely be clarified by the trigrams that follow like c-d-e and d-e-f, with the following words e,f, rather than by b-c-d.\n2.5. Large Vocabulary case. Here we have N \u2264 M . The dictionary D includes (at least) all the (relevant) lexemes that occur in the text T , and the number of contexts in which the text T is subdivided is smaller than the number of words in the dictionary.\nThe Statistical Semantics Hypothesis states that statistical patterns of word usage in texts determine their semantical meaning, and in particular that (parts of) text that have similar vectors in the above frequency matrices also have similar meanings.\nLet r = rank(P ) be the rank of the matrix P (T ). In the case of large dictionary, we have r \u2264 N . Under the Statistical Semantics Hypothesis, the rank r measures the largest number of words and contexts that the text T disambiguates semantically. Namely, the linear dependence of frequency vectors is interpreted as revealing the presence of underlying semantic relations. When r = N , all the contexts in T have a choice of corresponding words that they semantically disambiguate.\nIn the case where r = N , the matrix P (T ) of a text T determines a point p(T ) in the real Grassmannian Gr(N,M) of N -planes in real Euclidean space RM . Similarly, if rank(X(T )) = N , the matrix X(T ) determines a point x(T ) \u2208 Gr(N,M). For simplicity, we argue about the matrix P (T ). When not otherwise stated, the same will apply toX(T ). LetM = M(T ) be the set of subsets of {1, . . . ,M} of cardinality N , such that the determinant of the corresponding minor is \u2206I(P (T )) 6= 0. The set M determines a matroid stratum SM \u2282 Gr(N,M), with P (T ) \u2208 M.\nIn the case N \u2264 M , instead of working with a fixed (large) dictionary D for all texts, it is convenient, given a text T , to discard all the words in D that do not appear anywhere in T , as the text does not have any relevance for those words. Thus, we can assume that D = D(T ), with #D(T ) = M(T ) is the list of words that appear in T (with a suitable stop list). A text T has a linear ordering, which induces an ordering on the set D(T ) that lists words in order of apparition in T . We identify D(T ) with the set {1, . . . ,M(T )} using this ordering. Similarly, the set C(T ) of contexts is also ordered by how they are ordered in the text T , and we\n12\nidentify C(T ) with {1, . . . , N(T )} using this ordering. The order of apparition of words in the text T is relevant to the semantic interpretation of the text, as the first occurrence of a word is the first instance where a semantic interpretation for that word is required.\nConsider then the set of subsets I = {i1, . . . , iN} of [M ] := {1, . . . ,M} with i1 < i2 < . . . < iN . These correspond to choices of words wi1 , . . . , wiN in D(T ), such that the order of apparition of these words in the text T is respected, and we consider the frequency vectors Pik := (pik,j)j for the occurrence of the word wik in the context cj . We consider the Gale ordering on these subsets I. Namely, two such subsets I = {i1, . . . , iN} and J = {j1, . . . , jN}, with i1 < i2 < \u00b7 \u00b7 \u00b7 < iN and j1 < j2 < \u00b7 \u00b7 \u00b7 < jN , we have I \u2264G J iff i1 \u2264 j1, i2 \u2264 j2, . . . , iN \u2264 jN . The Gale ordering corresponds therefore to the relative position of words wik and wjk in the dictionary D(T ) according to first apparition in T .\nThe original dictionary D also has an ordering, and therefore the smaller dictionary D(T ) also has an induced ordering, which is different than the order of apparition in the text T . One then has some permutation \u03c3 \u2208 SM , such that the Gale ordering described above corresponds to the ordering I \u2264\u03c3 J , namely \u03c3\u22121I \u2264G \u03c3\u22121J .\nThe condition that, for one of these subsets I, the corresponding minor of the matrix P (T ) has vanishing determinant \u2206I(P (T )) = 0 means that there is a linear dependence between the vectors Pik , hence under the Statistical Semantics Hypothesis a semantic relation between the wik . Thus, the matroid stratum SM \u2282 Gr(N,M) containing the point p(T ) \u2208 SM determined by the text T describes, for the given contexts ci of the text T , all the choices of words wik , k = 1, . . . , N in the dictionary for which the semantic vectors Pik are independent. This can be seen as the maximal amount of semantic information that can be extracted from the text and its contexts.\nRecall that the positive (or totally non-negative) Grassmannian Gr\u22650(N,M) is the subset Gr\u22650(N,M) \u2282 Gr(N,M) of matrices A such that for all \u2206I(A) \u2265 0, for I as above. The intersections of the matroid strata with the positive Grassmannians S\u22650M = SM \u2229Gr\u22650(N,M) are cells, the positroid cells of [Pos06].\nIn particular, the condition that the point p(T ) lies in the positroid cell S\u22650M , that is, that all \u2206I(A) > 0, for all I \u2208 M, is equivalent to the existence of continuous paths \u03b3I , for each I \u2208 M, where \u03b3I(0) = P (T ) and \u03b3I(1) is a matrix where the I-minor is the identity, and for all t \u2208 [0, 1] one has \u03b3I(t) \u2208 S \u22650 M . This condition\n13\ncan be regarded as expressing the fact that the choice of words wi1 , . . . , wiN for the contexts c1, . . . , cN of the text T contains a maximal amount of semantic information. Indeed, the case where the corresponding minor would be the identity, would correspond to a case where the word wik is entirely specified semantically by the context ck and by none of the other cj with j 6= k.\n2.6. Information Retrieval case. We now focus on the other case mentioned above, the \u201cinformation retrieval\u201d setting, where we have N \u2265 M , that is, where the list of words is, for example, the list of words in a query, and one wants to locate texts, or contexts within a text, that are semantically most relevant for that query. In this case, we can assume that the number of words searched is no greater than the number of contexts.\nThe setting is similar to what we described before, except that we now consider the case where the matrix P (T ) determines a point in the Grassmannian Gr(M,N). The minors I = {i1, . . . , iM} correspond to choices of contexts cik in the text T in response to a query given by the words wk. As before the condition \u2206I(P (T )) > 0 corresponds to those assignments of a context to each word of the query that best matches it semantically.\n2.7. Literary texts and their statistical processing. D. Yu. Manin in his article [Man12] suggests that literary texts (prose/poetry) require qualitatively different methods of statistical processing in order to make explicit what puts them apart from texts produced in ordinary speech.\nHere we only mention a different kind of contexts used there ([Man12], p. 286).\nNamely, a context in his sense is a fragment of text with a blank, a hole where different words might occur, like \u201ca\u2013*\u2013b\u201d. This would allow one to extract statistical data allowing one to say that \u201cwords x and y often occur in the same contexts\u201d. Presumably, this fact would then reflect semantic relationships between x and y.\nIn the limiting case where x can occur in all the same contexts as y, and with the same frequencies, that would mean that x and y are exact synonyms. Or, if x can share contexts with u and v, but u and v do not share contexts, then they probably represent two very distinct meanings of x.\nIn this paper, we do not try to study semantic spaces and paths in them relevant to this approach. We only mention that it might be a very interesting project.\n14"}, {"heading": "3. Projective Spaces and Flag Varieties", "text": "We describe here two variants of the construction above, aimed at encoding more explicitly the fact that a linguistic text has an ordered linear structure that is crucial to its semantic interpretation. We propose two modifications of the geometry described above that better encode this fact. One is based on regarding a text subdivided into contexts, as a collection of points determining a path in a projective space, rather than as a single point in a Grassmannian. The second is in terms of points in a flag variety.\n3.1. Texts as Paths in Projective Spaces. Here we again consider the case where we have some fixed large vocabulary D of lexemes of size M = #D, which contains at least all the words in the given text T . We also subdivide the text into contexts ck, as before, but we do not necessarily assume that the total number N of contexts is smaller than M . Indeed, in this setting we could be dealing with a large corpus of texts and a large number of contexts. We again consider the semantic vectors Pk(T ) = (pik)i\u2208D that collect the probabilities (frequencies) of occurrence of words wi \u2208 D in the context ck of T . We regard each Pk as determining a point pk in the projective space P\nM\u22121 \u2243 Gr(1,M). Thus, a text T here corresponds to an ordered N -tuple of points in PM\u22121, where N is the number of contexts. We can think of this collection of points as an oriented path by drawing geodesic arcs between consecutive points. We denote by \u0393(T ) the resulting path associated to a text T .\nGiven different texts T and T \u2032, the comparison at the level of semantic vectors can be performed, in this setting, by computing the distance between the corresponding paths in the same ambient PM\u22121. This can be computed as the Fre\u0301chet distance between the two polygonal curves. The latter is defined as the infimum over reparameterizations by [0, 1] of the maximum over t \u2208 [0, 1] of the distance between corresponding points\n\u03b4(\u0393(T ),\u0393(T \u2032)) = inf \u03b3,\u03b3\u2032 max t\u2208[0,1] dFS(\u03b3(t), \u03b3 \u2032(t)),\nwhere \u03b3 : [0, 1] \u2192 \u0393(T ) and \u03b3\u2032 : [0, 1] \u2192 \u0393(T \u2032) are parameterizations of the two curves by [0, 1], and dFS(x, y) is the Fubini-Study metric on P\nM\u22121. The Fre\u0301chet distance for polygonal curves is computable in polynomial time ([AlGo95]).\n3.2. Texts as points in flag varieties. Another way to keep track of the linear ordering of contexts in a given text is by building larger subspaces, as more and\n15\nmore contexts in the given texts are encountered in a linear reading of the text. Thus, if Pk(T ) = (pik)i\u2208D are the semantic vectors as above, one considers the vector spaces Vk = span{Pj : j = 1, . . . , k}. The spaces V1 \u2282 V2 \u2282 \u00b7 \u00b7 \u00b7 \u2282 VN form a flag in RM . We denote by F (d1, . . . , d\u2113) the flag varieties of flags W1 \u2282 \u00b7 \u00b7 \u00b7W\u2113 with dim(Wk/Wk\u22121) = dk. We associate to a text T the point of the corresponding flag variety F (1, . . . , 1,M \u2212 N) determines by the flag V1 \u2282 V2 \u2282 \u00b7 \u00b7 \u00b7 \u2282 VN with Vk = span{Pj : j = 1, . . . , k}.\nThe natural Fubini\u2013Study metric on projective spaces has an analog for Grassmannians and flag varieties. It is obtained from the curvature form of the first Chern class of the determinant line bundle of a hermitian vector bundles ([Dem88]), or else by considering these varieties as quotients of SU(n) by subgroups, with the metric induced from the bivariant metric of SU(n) ([Gri74]). Thus, one can compare texts viewed as points in Grassmannians or in flag varieties, by measuring their distance with respect to this metric."}, {"heading": "4. From Lexemes to Semantic Dictionaries", "text": "We now consider a setting where, instead of a \u201clexemes dictionary\u201d D of words, one passes to a \u201csemantic dictionary\u201d S where lexemes are grouped together according to some semantic description. This can happen in two different ways, based on supervised or unsupervised learning.\n(a) Supervised Learning. In this case, also referred to as \u201csense tagging\u201d (see [MaSch99]), lexemes are grouped together into semantic categories by assigning appropriate tagging. In this setting, the type of question we look at is to what extent the information contained in the semantic vectors computed for the initial lexical vocabulary still retains the correct information when passing to a quotient that corresponds to the identification by semantic categories.\n(b) Unsupervised Learning. In this case, sense tags are not assigned, so that one cannot identify directly the corresponding semantic categories, but one can still obtain a \u201csense discrimination\u201d by grouping together words into unlabelled groups using the information contained in the semantic vectors. In this setting, we will show that the resulting grouping can be studied in terms of persistent topology ([Ca09]).\n4.1. Supervised Learning. We consider the case where we associate to texts T points p(T ) in a Grassmannian (either Gr(N,M) or Gr(M,N) depending on\n16\nrelative size of vocabulary and contexts). We consider the case N < M . The other possibility can be treated similarly.\nWe want to consider also the case where we deal not with a single text T but with a corpus consisting of several texts. In this case, we need to assume that the vocabulary D, with M = #D, is large enough to include all words that occur in all the texts of the corpus. Moreover, if we choose an ordering of the dictionary, as discussed previously, by order of apparition in a text, we can extend the order to the whole corpus, by choosing an order in which the different texts in the corpus are looked at. For the model with points in Grassmannians, or in flag varieties, we consider the case where the number N of contexts is fixed across all texts in the corpus. In the more general case where the number N = N(T ) varies across texts, we will be working with the model in which texts determine a sequence of points and an oriented polygonal path in a fixed projective space. In both cases, the question will be the behavior of the locus (in the Grassmannian, flag variety, or projective space) determined by the semantic vectors of all texts in the corpus, under a projection map that corresponds to passing from the lexical to the semantic dictionary.\n4.2. Points in Grassmannians and Flag Varieties. At the level of the matrix P (T ) and the corresponding point p(T ) in the Grassmannian Gr(N,M), one can view the operation of passing from the lexemes in D to the semantic categories in S as the effect of a projection \u03c0M,M \u2032 : Gr(N,M) \u0589 Gr(N,M\n\u2032), where M \u2032 \u2264 M is the size of the set of semantic categories considered, M \u2032 = #S.\nWe regard a corpus C = {T} of texts T as a discrete sampling of a subvariety of the Grassmannian Gr(N,M), under the hypothesis that the number of contexts is fixed and the size of the dictionary D is also fixed for all T \u2208 C. We denote by \u03a0C = {p(T )}T\u2208C the finite set of points on Gr(N,M) corresponding to the texts in the corpus. Given the finite set \u03a0C , we consider possible algebraic subvarietiesXC \u2282 Gr(N,M) that interpolate the points p(T ) \u2208 \u03a0C , namely algebraic subvarieties XC of Gr(N,M) with \u03a0C \u2282 XC.\nWe recall some results about projectability of subvarieties of Grassmannians, see [ArRa05]. A subvariety X \u2282 Gr(N,M) is k\u2013projectable, for some 0 \u2264 k \u2264 N \u2212 1, under \u03c0M,M \u2032 : Gr(N,M) \u0589 Gr(N,M\n\u2032) if any two N\u2013planes in the image of X only meet along linear spaces of dimension less than k. The case k = N corresponds to X being isomorphically projectable to Gr(N,M \u2032). Note that k\u2013projectability also implies that no two N\u2013planes in X can intersect in dimension greater than or equal\n17\nto k.\nIf the variety XC associated to a corpus C of texts is k\u2013projectable to Gr(N,M \u2032), this means that the N -planes given by the images \u03c0M,M \u2032(p(T )) and \u03c0M,M \u2032(p(T\n\u2032)) of any two points p(T ), p(T \u2032), with T, T \u2032 \u2208 C, will intersect in at most a (k \u2212 1)- dimensional space.\nThe size of the intersection between the N -planes of T and T \u2032 is a measure of dependence between the respective semantic vectors, hence of the semantic relatedness of the two texts. If in the variety XC every two N\u2013planes intersect in dimension less than k, but the variety is not k-projectable under \u03c0M,M \u2032 : Gr(N,M) \u0589 Gr(N,M\n\u2032), this means that there is loss of semantic information in the matching of words (and their semantic categories) to contexts in the texts of the corpus.\nThere are strong algebro\u2013geometric restrictions on k-projectable varieties. For example, it is shown in [ArRa05] that the Veronese embedding of Pn is the only variety in Gr(d\u2212 1, dn + d\u2212 1) that can be projected to Gr(d\u2212 1, n + 2d\u2212 3) so that any two (d\u2212 1)-planes meet in at most one point.\nWe have only discussed here the case where we associate texts to points in Grassmannians. The case of points in flag varieties is similar, with similar questions about k-projectable subvarieties.\n4.3. Paths in Projective Spaces. We then consider the case where the size N = N(T ) of contexts in a text is varying with T \u2208 C. In this case, instead of working with texts defining points in a Grassmannian, it is more convenient to adopt the viewpoint where texts determine polygonal paths in a projective space PM\u22121 with M = #D the size of the dictionary. In this case, we are looking at a similar question about k\u2013projectable subvarieties in projective spaces.\nMore precisely, we consider again algebraic subvarieties XC of P M\u22121 that contain all the paths \u0393(T ) for T \u2208 C. As a weaker condition, we can just assume that the variety XC contains the set of points \u03a0C = {pk(T ) : T \u2208 C, k = 1, . . . , N(T )}. If XC is also geodesically complete, then it contains also the paths \u0393(T ).\nWe then consider a projection \u03c0M,M \u2032 : P M\u22121 \u0589 PM \u2032\u22121 that corresponds to performing some identification of the vocabulary by grouping lexemes according to a choice of semantic categories, with M \u2032 = #S.\nWe are then looking at the problem of whether it is possible to project isomorphically a subvariety XC of P M\u22121 that contains the points \u03a0C (and possibly the\n18\ncollection of paths \u0393(T )) to the quotient PM \u2032\u22121. Again, there are strong restrictions on the existence of such isomorphically projectable subvarieties. For example, it is shown in [Ar01] that the only n-dimensional variety that can be isomorphically projected from Gr(1, 2n+ 1) to Gr(1, n) is the Veronese variety, that is, the embedding of Pn in Gr(1, 2n+ 1) via OPn(1) \u2295d.\nWhen the variety XC is not isomorphically projectable from P M\u22121 to PM \u2032\u22121, there is some loss of information in the semantic vectors, when the identification of words according to semantic tags is performed. In such cases, which will be typical in view of the very restrictive condition of isomorphic projectability, one can describe the effect of the identification on semantic vectors by analyzing the change of topology in the polygonal path \u0393C = \u222aT\u2208C\u0393(T ). We describe ways of approaching computationally such topology changes.\n4.4. Persistent Topology. It was understood in recent years that clusters of data points can exhibit interesting topological structure that can be useful in analyzing large data set, see [Ca09] for a general introduction and overview of the field of persistent topology. Applications of persistent topology to Linguistics were recently discussed in [PorGhGuCLDMar15].\nGiven a set \u03a0 of points in a metric space, one considers a family of simplicial complexes, parameterized by a real number \u01eb > 0, the so called Vietoris\u2013Rips complexes R(\u03a0, \u01eb). Here the n\u2013th term Rn(\u03a0, \u01eb) is the vector space spanned by all the unordered (n+1)-tuples of points in \u03a0 where all pairs have distance at most \u01eb. There are inclusion maps R(\u03a0, \u01eb1) \u2192\u0592 R(\u03a0, \u01eb2) when \u01eb1 < \u01eb2. These induce maps in homology Hn(R(\u03a0, \u01eb1)) \u2192 Hn(R(\u03a0, \u01eb2)). In analyzing the dependence on \u01eb of the ranks of these homology groups one discards as \u201cnoise\u201d those generators that arise and disappear within a small range of values of \u01eb, while one regards those generators that persist for sufficiently long intervals of values of \u01eb, the \u201cpersistent generators\u201d, as signaling the presence of actual structure in the data.\nPersistent topology of the set \u03a0C in the Grassmannian. Persistent topology can also be used to enrich the semantic comparison of different texts, when we assign to each text in a corpus a point in a Grassmannian or in a flag variety, as discussed above. The simplest level of comparison would be to cluster together the points corresponding to the various texts by separating them into groups according to the relative distances in the ambient metric. The resulting groups are dependent upon the scale of the neighborhoods of points, and the number of different groups of semantic similarity correspond to the rank of the zeroth order persistent homology\n19\nof the Vietoris\u2013Rips complex. Thus, more refined information about how texts cluster together by semantic similarity is obtained by additionally considering also the first and higher dimensional persistent homology.\nWe consider, as above, a projection \u03c0M,M \u2032 : Gr(N,M) \u0589 Gr(N,M \u2032) and the image \u03c0M,M \u2032(\u03a0C). In the case where the set of points \u03a0C does not fit on an interpolating variety that is isomorphically projectable, we can analyze the change in the semantic proximity of texts by analyzing the differences between the persistent topology of \u03a0C and of \u03c0M,M \u2032(\u03a0C). This can be seen by computing the number of persistent generators, in various degrees, of the homology of the respective Vietoris\u2013 Rips complexes. The case of points in flag varieties can be treated analogously to the case of points in Grassmannians.\nPersistent topology and paths in PM\u22121. In a similar manner, one can use persistent topology to analyze syntactic proximity of texts in the point of view where we assign to each text in a corpus a path in projective space PM\u22121. In this case, we again associate to a corpus C a simplicial complex, where the zero-cells are all the points pk(T ) \u2208 PM\u22121, for all texts T \u2208 C, and all the one-cells are the geodesic arcs connecting consecutive pairs of points pk(T ) and pk+1(T ). The higher dimensional skeleta are then constructed as in the Vietoris\u2013Rips complex, by adding an n-dimensional simplex whenever an n + 1-tuple of points {pk0(T0), . . . , pkn(Tn)} where the geodesic distances between all pairs of these points are less than a fixed scale \u01eb. This may require introducing additional one-cells in the complex.\nAs in the case of points in Grassmannians and flag varieties, when we consider a projection \u03c0M,M \u2032 : P M\u22121 \u0589 PM\n\u2032\u22121, we can study the effect of the projection on the persistent topology of the set of paths \u0393C = \u222aT\u2208C\u0393(T ) and its image \u03c0M,M \u2032(\u0393C), by associating complexes as indicated above to \u0393C and \u03c0M,M \u2032(\u0393C) and comparing generators of the respective persistent homologies.\n4.5. Unsupervised Learning. In the case of unsupervised learning, a grouping corresponding to \u201csense discrimination\u201d is obtained solely on the basis of the semantic vectors and the position of the corresponding points in the ambient variety, without any external tagging of words by semantic categories. In the setting of unsupervised learning, the grouping together of subsets of the M lexical dimensions into putative semantic categories is itself performed solely on the basis of the semantic vectors. A simple way to search for semantic relatedness in an unsupervised context is to identify frequent co-occurrences within the same contexts (see Section 2.4 of [TuPa10]). Many co-occurrences arise for purely syntactic reasons,\n20\nbut those tend to be between words that belong to different parts of speech, while co-occurrences that carry semantic significance are more often found between words in the same part of speech, see [BuHi06], [ChiaBRP90], [SchPe93], [TuPa10].\n4.6. Syntactic dependence of semantic vectors. Clearly, the vectors Pk(T ) = (pik)i\u2208D, associated to the contexts ck in a text, depend on both syntactic and semantic information and there is a priori no obvious way to distinguish between the dependence on syntax and on semantics. However, a possible way to make these semantic vectors more syntax independent would be to consider a training corpus of different language translations of the same texts, with marked matching paragraphs and matching word dictionaries, and average the semantic vectors Pk(T, L) over the set of languages L. This can be done either by simply averaging the vectors, or else by considering the corresponding points pk(T, L), for all languages L, in the fixed ambient PM\u22121, and replace them by the barycenter p\u0304k(T ) computed with respect to the Fubini-Study metric on P\nM\u22121. This has the effect of reducing the purely syntactic contribution, especially if the set of languages chosen contains languages with sufficiently different set of syntactic parameters. Of course, it is not possible to entirely decouple semantics from syntax, as the syntactic-semantic interface is very rich (see for example [Ha13], [Va05]), but this averaging method can at least partially reduce the influence of those effects that are due to syntax alone."}, {"heading": "5. Geodesically convex neighborhoods and semantic spaces", "text": "In the setting above, we have associated to texts in a corpus a collection of points (or of paths) in an ambient geometric space (a Grassmannian, or a flag variety, or a projective space). We have also seen that, when we group together words in the lexicon by semantic categories, geometrically we look at how the set of points and paths behaves under a projection map of the ambient variety. In this section we use the same general geometric picture, and we consider coverings by convex open sets. These local neighborhoods correspond to grouping together texts by semantic similarity. The convexity property corresponds to the possibility of interpolation and will be compared in Section 7 with the approach of Ga\u0308rdenfors on conceptual spaces as \u201cmeeting of minds\u201d, cf. [Ga\u030800], [Ga\u030814], [WaGa\u030813].\n5.1. Geodesic convexity and good coverings. Recall that a subset U \u2282 X in a Riemannian manifold X is said to be geodesically convex if for arbitrary points p 6= p\u2032 \u2208 U there is a distance minimizing geodesic arc connecting them that is\n21\nentirely contained in U . In particular, a geodesically convex U is topologically a contractible set. Moreover, a non\u2013empty finite intersection of geodesically convex open sets Ui is also a geodesically convex open set. If X is compact, we can assume the number of open sets in such a covering to be finite. Their size (measured as the diameter) in such a covering is bounded. We then say that U\u01eb = {Ui(\u01eb)}ni=1 is a good \u01eb\u2013covering of the compact Riemannian manifold X , if the Ui are geodesically convex with \u01eb = maxi{diam(Ui(\u01eb))}.\nIn particular, we consider such coverings for the Grassmannians Gr(N,M \u2032), flag varieties F (1, . . . , 1,M \u2032 \u2212 N), and projective spaces PM \u2032\u22121, with the respective metrics discussed above, and where M \u2032 is the size of the semantic vocabulary, after semantic identifications have been performed on the initial lexical vocabulary of size M \u2265 M \u2032, as discussed in the previous section. We view points pk(T ) that lie within the same convex neighborhood Ui(\u01eb) of a good \u01eb-open covering by geodesically convex sets as being semantically related. In particular, we are interested in considering good \u01eb-open coverings that are generated by starting with a collection Uk(\u01eb, T ) of geodesic balls of radius \u01eb/2 centered at the points pk(T ) associated to a text T in a corpus. Consider the case where pk(T ) \u2208 PM \u2032\u22121. The cases of points in Grassmannians and flag varieties are analogous. We construct an \u01eb-open covering of the ambient variety by starting with the collection {Uk(\u01eb, T ) : k = 1, . . . , N(T ); T \u2208 C} and we complete it to an \u01eb-open covering of PM \u2032\u22121 by adding enough additional sets Ui(\u01eb) covering the complement of YC,\u01eb := \u222ak,TUk(\u01eb, T ). We let U\u01eb denote the resulting covering of PM \u2032\u22121 and we write U\u01eb(C) \u2282 U\u01eb for the covering of YC,\u01eb \u2282 PM \u2032\u22121 by the {Uk(\u01eb, T ) : k = 1, . . . , N(T ); T \u2208 C}.\nAs it is customary in topology, we can associate to a given good \u01eb-covering U\u01eb(C) the simplicial complex given by its C\u030cech complex N\u22c6(C, \u01eb) := N\u22c6(U\u01eb(C)), with geometric realization N (C, \u01eb) := |N\u22c6(C, \u01eb)|. Note that, while the geometric realization of the C\u030cech complex of the full covering U\u01eb of PM \u2032\u22121 is just homotopy equivalent to PM \u2032\u22121 (see [Se68] and also [DuI] for a generalization), the geometric realizationsN (C, \u01eb) of the subcomplexes U\u01eb(C) of U\u01eb in general depend on the corpus C and will in general not be homotopy equivalent to the ambient space.\nOne can then study, for a given corpus of texts C, how the homotopy type, and invariants such as homology, of the simplicial space N (C, \u01eb) vary with the scale \u01eb. According to the usual approach of persistent topology, those features that change rapidly with the scale are attributed to random fluctuation, while persistent features can be identified with actual structures.\n22"}, {"heading": "5.2. Geodesically convex neighborhoods, C\u030cech complexes, and neural", "text": "codes. As in the previous subsection, we consider simplicial complexes N\u22c6(C, \u01eb) obtained as the C\u030cech complex of the collection U\u01eb(C) of the geodesically convex balls Uk(\u01eb, T ) of diameter \u01eb around the points pk(T ) \u2208 PM \u2032\u22121, for k = 1, . . . , N(T ), the number of contexts in the text T and for T varying in a given corpus C. Their geometric realizations are denoted, as above, by N (C, \u01eb) = |N\u22c6(C, \u01eb)|.\nFollowing the approach of [CuItVCYo13], we associate a code C = C(C, \u01eb) to the collection U\u01eb(C) of geodesically convex balls. This is a code C \u2282 {0, 1}m, where m = \u2211\nT\u2208C N(T ). Here we assume chosen an ordering of the texts T \u2208 C, with n = #C, so that we identify the set of contexts\n{c1(T1), . . . , cN(T1)(T1), . . . , c1(Tn), . . . , cN(Tn)(Tn)}\nof the entire corpus C with the set {1, . . . , m}. The code words w \u2208 C are those elements w \u2208 {0, 1}m such that\n\n\n\u22c2\ni\u2208supp(w)\nUki(\u01eb, Ti)\n\n \\\n\n\n\u22c3\nj /\u2208supp(w)\nUkj (\u01eb, Tj)\n\n 6= \u2205,\nwhere supp(w) = {i \u2208 {1, . . . , m} : wi = 1}.\nAccording to the \u201cnerve theorem\u201d ([Ha02], Corollary 4G.3), as discussed in [CuItVCYo13] and [Ma15], the homotopy type of the space YC,\u01eb = \u222ak,TUk(\u01eb, T ) is equal to the homotopy type of the nerve N (C, \u01eb) of the complex N\u22c6(C, \u01eb). In particular, the persistent homology of YC,\u01eb is the same as the persistent homology of N (C, \u01eb).\nThis is the setting used in [CuItVCYo13] to reconstruct information about the topology of the stimulus space from knowledge of the associated neural code. Neural codes and the problem of how they encode the structure of the stimulus space have been studied extensively in neuroscience, especially in relation to vision (see [CuItVCYo13] and references therein). The study of neural codes in the linguistic setting is presently less extensive: neural codes for syntax, based on data of neurosurgical procedures, have been studied (see [BikSza09]). A detailed criticism of a possible linguistic approach to neurosemantics is given for instance in [Eli05], while a proposal for semantic representation of linguistic data via shared neural codes (for auditory, visual or somatosensory inputs) is analyzed in [Poe06].\n23\nWe argue for a proposal of the simplicial complexes N\u22c6(C, \u01eb) and their persistent homotopy type as possible computational models of neural codes for neurosemantics, at least up to homotopy. Namely, instead of the usual approach to measuring semantic relatedness of texts on the basis of angular distances of semantic vectors, one can consider topological notions of relatedness and proximity, in terms of deformability and homotopy equivalence of the complexes N\u22c6(C, \u01eb)."}, {"heading": "6. Spectral decompositions and Riccati flows", "text": "6.1. Singular Value Decomposition. Typically, the word\u2013document semantic matrices discussed above are very sparse, with often only a small percentage of entries being non\u2013zero. It is known that this creates problems in measuring semantic similarity with the usual cosine method (see [TuPa10]), as the method easily assigns zero to non co\u2013occurring words even though they are semantically related.\nIn order to circumvent this problem, one can perform a dimensional reduction based on a singular value decomposition (SVD). This represents the semantic matrix P as a product U\u03a3V \u03c4 , where U and V are, respectively, and M \u00d7M and an N\u00d7N unitary matrix and \u03a3 is an N \u00d7M matrix with the singular values on the diagonal, of rank r equal to the rank of the original semantic matrix.\n6.2. Latent Semantics. The technique known as \u201clatent semantics\u201d (see Section 4.3 of [TuPa10]) then considers truncations of the matrix U\u03a3V \u03c4 to a rank k < r approximation Uk\u03a3kV \u03c4 k obtained by considering only the k largest singular values. This has the effect of creating a low-dimensional linear mapping between words and contexts, which reduces noise and improves the estimates of semantic similarity, or \u201cdiscover latent meaning\u201d in the terminology used in vector space semantics.\nThus, according to this procedure, the process of analyzing semantic relatedness based on the given word-context semantic matrix, involves a singular value decomposition and a truncation according to the largest singular values. We will see in the rest of this section that these operations also have a very natural geometric interpretation in terms of the geometry of projective spaces and Grassmannians.\n6.3. Term co-occurrence matrix. In order to obtain the singular value decomposition and restrict to the largest k singular values, one considers the symmetric matrix A = P \u03c4P , the term co-occurrence matrix, and its spectral decomposition. The truncations discussed above can then be obtained by applying power\n24\nmethods to separate out the span of the eigenvectors of the largest k eigenvalues of A = P \u03c4P from the complementary space. For further discussions of \u201csemantic spectrum\u201d and \u201ceigenword\u201d decomposition see for instance [DhFU15], [WiDa11].\n6.4. Perron\u2013Frobenius and Riccati equation. If there is only one top eigenvalue one can apply the usual Perron\u2013Frobenius theory. Let Sp(A) = {\u03bb1, . . . , \u03bbN} with |\u03bb1| > |\u03bb2| \u2265 \u00b7 \u00b7 \u00b7 \u2265 |\u03bbN |. In the case we considered in the previous sections where N \u2264 M and the rank is N , the matrix A determines an action on PN\u22121, and the sequence of points xm = A\nmx0, for an assigned initial point x0 \u2208 PN\u22121, converges to the point in PN\u22121 corresponding to the line spanned by the Perron\u2013 Frobenius eigenvector of A. Moreover, as discussed in [AmMa86], [MaAm92], in a local chart corresponding to vectors with first component equal to one, we have\nA : xm =\n(\n1 ym\n)\n7\u2192 xm+1 = Axm =\n(\n1 ym+1\n)\n,\nwith\nym+1 = A3 +A4ym A1 +A2ym\nwhere\nA =\n(\nA1 A2 A3 A4\n)\n,\nwhere A4 is an (N \u2212 1)\u00d7 (N \u2212 1)-matrix and A1 a number. The recursion relation of the sequence ym is then given by\nym+1 \u2212 ym = (A3 + A4ym \u2212 ymA1 \u2212 ymA2ym)(A1 +A2ym) \u22121.\nThe above can be viewed as a discretization of the matrix Riccati equation\nd\ndt y(t) = A3 + A4y(t)\u2212 y(t)A1 \u2212 y(t)A2y(t),\nin particular, both equations have the same stationary solutions given by solutions to\nA3 +A4y \u2212 yA1 \u2212 yA2y = 0.\nThus, in order to find the limit x = limm xm, or equivalently the stationary solution ym+1 = ym of the difference equation above, one can consider the Riccati flow to the\n25\nsame fixed point. For this reformulation of the Perron\u2013Frobenius theory in terms of a matrix Riccati equation in a projective space, see [AmMa86], [MaAm92].\n6.5. Latent Semantics and flows on Grassmannians. In a similar way, it is shown in [AmMa86], [MaAm92] that the selection of the span of the eigenvectors of the k largest eigenvalues of the matrix A = P \u03c4P can be performed dynamically in terms of a Riccati flow on the Grassmannian G(k,N). More precisely, for a given k-dimensional vector space V \u2208 G(k,N) and a matrix A \u2208 GLN , we have AV \u2208 G(k,N) given by AV = {Av : v \u2208 V }. Thus, given an initial point V0 \u2208 G(k,N) one can consider the power sequence Vm+1 = AVm. If Spec(A) = {\u03bb1, . . . , \u03bbN} with\n|\u03bb1| \u2265 |\u03bb2| \u2265 |\u03bbk| > |\u03bbk=1| \u2265 \u00b7 \u00b7 \u00b7 |\u03bbN |,\nand U is the span of the eigenvectors corresponding to \u03bbi with i = 1, . . . , k, then the sequence of points Vm in G(k,N) converge to the point corresponding to the space U , for every choice of initial V0 with V0 \u2229W = {0}, where W is the span of the eigenvectors with eigenvalues \u03bbi with i = k + 1, . . . , N .\nFor a choice of complementary subspaces U \u2208 G(k,N) and W \u2208 G(N \u2212 k,N), and a morphism L \u2208 Hom(U,W ), consider the element UL \u2208 G(k,N) given by the subspace\nUL = {\n(\nu Lu\n)\n| u \u2208 U} \u2282 U \u2295W.\nIf the matrix A in the decomposition U \u2295W has the form\nA =\n(\nA1 A2 A3 A4\n)\nthen\nAUL = U(A3+A4L)(A1+A2L)\u22121\nin this local chart on the Grassmannian G(k,N). Thus, one obtains a corresponding sequence\nLm+1 = (A3 + A4Lm)(A1 + A2Lm) \u22121\nwhich can be written as a difference equation\nLm+1 \u2212 Lm = (A3 +A4Lm \u2212 LmA1 \u2212 LmA2Lm)(A1 + A2Lm) \u22121.\n26\nAs before, the stationary solutions can be equivalently obtained as the stationary points of the matrix Riccati flow\nd\ndt L(t) = A3 + A4L(t)\u2212 L(t)A1 \u2212 L(t)A2L(t).\nThis shows that the latent semantics method based on singular value decomposition and truncation to the top k singular values for P can be reformulated in terms of a geometric flow on a Grassmannian."}, {"heading": "7. Relation to Ga\u0308rdenfors\u2019 \u201cmeeting of minds\u201d", "text": "7.1. Where the minds meet. In [Ga\u030814], Ga\u0308rdenfors developed an approach to semantic spaces based on the metaphor \u201cmeeting of minds\u201d (see [WaGa\u030813]) and on models of \u201cconceptual spaces\u201d developed in [Ga\u030800]. The main idea is that meaning is emergent in communication (see Section 5.1 of [Ga\u030814]). Typically, coming to a common understanding of meaning in communication is seen as a fixed point problem taking place in a convex space which describes some configuration domain, such as colors, some kind of actions, etc. Communication is modeled in terms of a partitioning of this domain determined by the transmitter and a sample set of points in the domain obtained by the received, and the common understanding is achieved by the construction of a Voronoi partition common to both sets of points, see Section 5.4.1 of [Ga\u030814].\nIn our setting, the geometry of semantic spaces is not dictated by conceptual spaces determined by preassigned external semantic categories as in [Ga\u030814], but rather the geometry of an ambient space (a Grassmannian, or a set of paths in a projective space) built out of corpora of texts and the frequencies of occurrences of lexemes in contexts of these texts. However, we can still develop an approach to communication as a fixed point problem leading to a common semantic interpretation between different users, which resembles, in a different geometric setting, the \u201cmeeting of minds\u201d approach of Ga\u0308rdenfors.\nConsider a set A of different users. All users have access to the same dictionary D of lexemes, while each user \u03b1 \u2208 A has access to a certain corpus of texts C\u03b1, and derives semantic information from the analysis of occurrences of the words of D in the contexts of the texts T \u2208 C\u03b1. Thus, each user \u03b1 \u2208 A obtains a matrix P\u03b1(T ) of semantic vectors for each text T \u2208 C\u03b1. Assuming each user has analyzed the entire corpus C\u03b1, and used the information available in all texts T \u2208 C\u03b1 to obtain semantic\n27\ninformation, we obtain, for each lexeme wk \u2208 D and for each user \u03b1 \u2208 A, a semantic vector P\u03b1,k = (p\u03b1,ki), where the index i ranges over all the contexts ci(T ) of all the texts T , listed in a given order in the corpus C\u03b1. We can view all these semantic vectors P\u03b1,k inside a larger vector space that corresponds to the union C = \u222a\u03b1C\u03b1, where we add zero entries to the vector P\u03b1,k whenever a certain text T in some corpus C\u03b2 is not also contained in C\u03b1. In this way, for a given lexeme wk \u2208 D, the different users arrive at somewhat different semantic interpretations, depending on the different texts they had access to. This difference is measured by the different position of the vectors P\u03b1,k in this ambient space. In a similar way, if we consider the entire dictionary, or just some subset of lexemes, we obtain for each user a different semantic matrix P\u03b1, computed as above over all texts T in C = \u222a\u03b1C\u03b1. As before, we regard these matrices as points p\u03b1 in a Grassmannian Gr(M,N), where M is the number of lexemes considered and N is the overall number of contexts in all the texts in the entire union C of corpora. Here we typically are in the situation were we are seeking a common semantic understanding of a small number of lexemes using a large number of context and corpora, hence M < N .\nGiven this finite collection {p\u03b1}\u03b1\u2208A of points in a Grassmannian Gr(M,N), which represents the different positions in semantic space the different users arrived at by analyzing the occurrence of the same list of lexemes in the corpora available to them, we need a simple geometric procedure that arrives to a common position in semantic space and that can be implemented interactively as a sequence of approximations. A simple such procedure consists of taking the geodesic barycenter of the set {p\u03b1}\u03b1\u2208A. In fact, more generally one can considered a weighted distribution of the points p\u03b1, where each p\u03b1 is assigned a weight \u03bb\u03b1 \u2265 0 with \u2211\n\u03b1 \u03bb\u03b1 = 1. The additional information contained in the weights \u03bb\u03b1 can be some a priori knowledge of the higher reliability or relevance of some corpora C\u03b1 with respect to others, which would make the semantic matrix P\u03b1 obtained by some user more reliable than that obtained by some other user. Given the set {p\u03b1} in Gr(M,N) and the respective weights \u03bb\u03b1, the barycenter pB is determined by the condition\n\u2211\n\u03b1\n\u03bb\u03b1\u03b4 2(p\u03b1, pB) = min p\u2208Gr(M,N) { \u2211\n\u03b1\n\u03bb\u03b1\u03b4 2(p\u03b1, p)},\nwhere \u03b4(x, y) is the geodesic distance. Assuming that all the points p\u03b1 lie sufficiently close to each other (as would be the case if there is enough overlap between the corpora available to different users) so that they are contained in a single geodesically\n28\nconvex neighborhood U \u2282 Gr(M,N), the potential function\nV (p) = \u2211\n\u03b1\n\u03bb\u03b1\u03b4 2(p\u03b1, p)\nis a strictly convex function on the neighborhood U and has therefore a unique minimum. The barycenter is then the point pB where V (p) achieves its minimum.\nIt can be also described as the unique fixed point of the map p 7\u2192 p\u2212 h\u2207V (p), where \u2207V = g(dV, \u00b7), g being the Riemannian metric tensor, and h is a finite increment in a discretized gradient descent. Recursively, pB is then approximated by pk+1 = pk \u2212 h\u2207V (pk).\nIn a similar way, one can consider simplicial Vietoris\u2013Rips complexes N\u2217(C\u03b1, \u01eb) obtained by different users based on different corpora C\u03b1. After again considering them inside a larger common projective space, one can construct a new complex which is their common barycentric subdivision. The homotopy type and persistent homology of the resulting complex can then be treated as a model of the \u201cmeeting of minds\u201d in our setting."}, {"heading": "8. Semantic vectors, Zipf\u2019s law, and Kolmogorov complexity", "text": "8.1. Zipf\u2019s law. As observed in [Lowe01], constructions of semantic spaces based on semantic vectors should take into consideration the fact that the distribution of linguistic data is skewed towards high count data, according to the empirical Zipf\u2019s law.\nGiven a corpus of texts C and a word w (in the sense of a lexeme from a dictionary of words), let FC(w) denote the number of tokens of the given word that appear in the corpus, and FC(w)/N the relative frequencies, where N = #C is the size of the corpus. Let {wk} be an enumeration of the dictionary words by decreasing frequencies. Then Zipf\u2019s law states that log(FC(wk)) = \u03ba(N) \u2212 B log(k), for a constant \u03ba(N) depending on the corpus size and with the power law B satisfying B \u223c 1. It was shown in [Ma13] that if one postulates that, in Zipf\u2019s original explanation as\u201cminimization of effort\u201d, the word \u201ceffort\u201d means Kolmogorov\u2019s complexity, then Zipf\u2019s law with exponent 1 becomes a consequence of properties of the related universal Levin probability distribution.\nIn the construction of semantic spaces, when one counts co\u2013occurrences of words in given contexts with certain given vocabulary lexemes, one encounters a situation\n29\nwhere low frequency words may be more significant for semantic association, but produce very sparse semantic matrices, while high frequency words provide more reliable statistics, but are less significant in determining semantic association, as they tend to appear in almost every context. Semantic vectors based on low frequency words will have high variance, and Zipf\u2019s law predicts that the amount of additional data required in order to reduce the variability is expressed by a power law relation.\n8.2. Latent semantic analysis. In latent semantic analysis this phenomenon is accounted for by introducing weights assigned to the vocabulary entries, so that the estimated probability (frequency) pc(w, \u2113) of co-occurrence of a given word w with a given lexeme \u2113 in a context c is weighted by S(\u2113)\u22121 log(1 + pc(w, \u2113)), where the denominator is given by the entropy S(\u2113) = \u2212 \u2211\nc\u2208C(T ) pc(\u2113) log(pc(\u2113)), with\npc(\u2113) the probability of occurrence of \u2113 in the context c. In this way, if \u2113 is equally distributed in all contexts, as one expects for the most frequent words, the entropy is maximal and the weighted co-occurrence is less significant, while if \u2113 is likely to occur only in a smaller number of contexts the co-occurrence is weighted more, as more semantically significant. Other methods that can be used for taking into account the effects due to Zipf\u2019s law and the different semantic significance of words with different frequencies are surveyed in [Lowe01].\nThis type of considerations based on Zipf\u2019s law apply to any suitable construction of semantic spaces, including the geometric construction we discussed in the previous sections. In particular, one can similarly consider introducing appropriate weights in the construction of the semantic matrix P (T ) of a text, that we discussed before, so that, in addition to counting occurrences in contexts c \u2208 C(T ), one also keeps into account how uniform or non\u2013uniform the distribution over contexts is, measured in terms of the Shannon entropy of the resulting probability distribution."}], "references": [], "referenceMentions": [], "year": 2016, "abstractText": "Any natural language can be considered as a tool for producing<lb>large databases (consisting of texts, written, or discursive). This tool for its descrip-<lb>tion in turn requires other large databases (dictionaries, grammars etc.). Nowadays,<lb>the notion of database is associated with computer processing and computer mem-<lb>ory. However, a natural language resides also in human brains and functions in<lb>human communication, from interpersonal to intergenerational one. We discuss in<lb>this survey/research paper mathematical, in particular geometric, constructions,<lb>which help to bridge these two worlds. In particular, in this paper we consider the<lb>Vector Space Model of semantics based on frequency matrices, as used in Natural<lb>Language Processing. We investigate underlying geometries, formulated in terms<lb>of Grassmannians, projective spaces, and flag varieties. We formulate the relation<lb>between vector space models and semantic spaces based on semic axes in terms of<lb>projectability of subvarieties in Grassmannians and projective spaces. We interpret<lb>Latent Semantics as a geometric flow on Grassmannians. We also discuss how to<lb>formulate G\u00e4rdenfors\u2019 notion of \u201cmeeting of minds\u201d in our geometric setting. O INTERIOR DO EXTERIOR DO INTERIOR Pascal Mercier<lb>\u201cNachtzug nach Lissabon\u201d", "creator": "dvips(k) 5.991 Copyright 2011 Radical Eye Software"}}}