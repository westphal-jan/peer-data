{"id": "1605.04569", "review": {"conference": "ACL", "VERSION": "v1", "DATE_OF_SUBMISSION": "15-May-2016", "title": "Syntactically Guided Neural Machine Translation", "abstract": "We investigate the use of hierarchical phrase-based SMT lattices in end-to-end neural machine translation (NMT). Weight pushing transforms the Hiero scores for complete translation hypotheses, with the full translation grammar score and full n-gram language model score, into posteriors compatible with NMT predictive probabilities. With a slightly modified NMT beam-search decoder we find gains over both Hiero and NMT decoding alone, with practical advantages in extending NMT to very large input and output vocabularies.\n\n\n\nThe above results have been obtained from the OpenJSCG.", "histories": [["v1", "Sun, 15 May 2016 15:53:02 GMT  (152kb,D)", "https://arxiv.org/abs/1605.04569v1", "ACL 2016"], ["v2", "Thu, 19 May 2016 08:52:37 GMT  (152kb,D)", "http://arxiv.org/abs/1605.04569v2", "ACL 2016"]], "COMMENTS": "ACL 2016", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["felix stahlberg", "eva hasler", "aurelien waite", "bill byrne"], "accepted": true, "id": "1605.04569"}, "pdf": {"name": "1605.04569.pdf", "metadata": {"source": "CRF", "title": "Syntactically Guided Neural Machine Translation", "authors": ["Felix Stahlberg", "Aurelien Waite", "Bill Byrne"], "emails": [], "sections": [{"heading": "1 Introduction", "text": "We report on investigations motivated by the idea that the structured search spaces defined by syntactic machine translation approaches such as Hiero (Chiang, 2007) can be used to guide Neural Machine Translation (NMT) (Kalchbrenner and Blunsom, 2013; Sutskever et al., 2014; Cho et al., 2014; Bahdanau et al., 2015). NMT and Hiero have complementary strengths and weaknesses and differ markedly in how they define probability distributions over translations and what search procedures they use.\nThe NMT encoder-decoder formalism provides a probability distribution over translations y = yT1 of a source sentence x as (Bahdanau et al., 2015) P (yT1 |x) = T\u220f t=1 P (yt|yt\u221211 ,x) = T\u220f t=1 g(yt\u22121, st, ct) (1) where st = f(st\u22121, yt\u22121, ct) is a decoder state variable and ct is a context vector depending on the source sentence and the attention mechanism.\nThis posterior distribution is potentially very powerful, however it does not easily lend itself\nto sophisticated search procedures. Decoding is done by \u2018beam search to find a translation that approximately maximizes the conditional probability\u2019 (Bahdanau et al., 2015). Search looks only one word ahead and no deeper than the beam.\nHiero defines a synchronous context-free grammar (SCFG) with rules: X \u2192 \u3008\u03b1, \u03b3\u3009, where \u03b1 and \u03b3 are strings of terminals and non-terminals in the source and target languages. A target language sentence y can be a translation of a source language sentence x if there is a derivation D in the grammar which yields both y and x: y = y(D), x = x(D). This defines a regular language Y over strings in the target language via a projection of the sentence to be translated: Y = {y(D) : x(D) = x} (Iglesias et al., 2011; Allauzen et al., 2014). Scores are defined over derivations via a log-linear model with features {\u03c6i} and weights \u03bb. The decoder searches for the translation y(D) in Y with the highest derivation score S(D) (Chiang, 2007, Eq. 24) :\ny\u0302 = y  argmax D:x(D)=x PG(D)PLM (y(D)) \u03bbLM\ufe38 \ufe37\ufe37 \ufe38\nS(D)  (2) where PLM is an n-gram language model and PG(D) \u221d \u220f (X\u2192\u3008\u03b3,\u03b1\u3009)\u2208D \u220f i \u03c6i(X \u2192 \u3008\u03b3, \u03b1\u3009)\u03bbi .\nHiero decoders attempt to avoid search errors when combining the translation and language model for the translation hypotheses (Chiang, 2007; Iglesias et al., 2009). These procedures search over a vast space of translations, much larger than is considered by the NMT beam search. However the Hiero context-free grammars that make efficient search possible are weak models of translation. The basic Hiero formalism can be extended through \u2018soft syntactic constraints\u2019 (Venugopal et al., 2009; Marton and Resnik, 2008) or by\nar X\niv :1\n60 5.\n04 56\n9v 2\n[ cs\n.C L\n] 1\n9 M\nay 2\n01 6\nadding very high dimensional features (Chiang et al., 2009), however the translation score assigned by the grammar is still only the product of probabilities of individual rules. From the modelling perspective, this is an overly strong conditional independence assumption. NMT clearly has the potential advantage in incorporating long-term context into translation scores.\nNMT and Hiero differ in how they \u2018consume\u2019 source words. Hiero applies the translation rules to the source sentence via the CYK algorithm, with each derivation yielding a complete and unambiguous translation of the source words. The NMT beam decoder does not have an explicit mechanism for tracking source coverage, and there is evidence that may lead to both \u2018over-translation\u2019 and \u2018under-translation\u2019 (Tu et al., 2016).\nNMT and Hiero also differ in their internal representations. The NMT continuous representation captures morphological, syntactic and semantic similarity (Collobert and Weston, 2008) across words and phrases. However, extending these representations to the large vocabularies needed for open-domain MT is an open area of research (Jean et al., 2015a; Luong et al., 2015; Sennrich et al., 2015; Chitnis and DeNero, 2015). By contrast, Hiero (and other symbolic systems) can easily use translation grammars and language models with very large vocabularies (Heafield et al., 2013; Lin and Dyer, 2010). Moreover, words and phrases can be easily added to a fully-trained symbolic MT system. This is an important consideration for commercial MT, as customers often wish to customise and personalise SMT systems for their own application domain. Adding new words and phrases to an NMT system is not as straightforward, and it is not clear that the advantages of the continuous representation can be extended to the new additions to the vocabularies.\nNMT has the advantage of including long-range context in modelling individual translation hypotheses. Hiero considers a much bigger search space, and can incorporate n-gram language models, but a much weaker translation model. In this paper we try to exploit the strengths of each approach. We propose to guide NMT decoding using Hiero. We show that restricting the search space of the NMT decoder to a subset of Y spanned by Hiero effectively counteracts NMT modelling errors. This can be implemented by generating translation lattices with Hiero, which are then rescored by the\nNMT decoder. Our approach addresses the limited vocabulary issue in NMT as we replace NMT OOVs with lattice words from the much larger Hiero vocabulary. We also find good gains from neural and Kneser-Ney n-gram language models."}, {"heading": "2 Syntactically Guided NMT (SGNMT)", "text": ""}, {"heading": "2.1 Hiero Predictive Posteriors", "text": "The Hiero decoder generates translation hypotheses as weighted finite state acceptors (WFSAs), or lattices, with weights in the tropical semiring. For a translation hypothesis y(D) arising from the Hiero derivation D, the path weight in the WFSA is \u2212 logS(D), after Eq. 2. While this representation is correct with respect to the Hiero translation grammar and language model scores, having Hiero scores at the path level is not convenient for working with the NMT system. What we need are predictive probabilities in the form of Eq. 1.\nThe Hiero WFSAs are determinised and minimised with epsilon removal under the tropical semiring, and weights are pushed towards the initial state under the log semiring (Mohri and Riley, 2001). The resulting transducer is stochastic in the log semiring, i.e. the log sum of the arc log probabilities leaving a state is 0 (= log 1). In addition, because the WFSA is deterministic, there is a unique path leading to every state, which corresponds to a unique Hiero translation prefix. Suppose a path to a state accepts the translation prefix yt\u221211 . An outgoing arc from that state with symbol y has a weight that corresponds to the (negative log of the) conditional probability\nPHiero(yt = y|yt\u221211 ,x). (3)\nThis conditional probability is such that for a Hiero translation yT1 = y(D) accepted by the WFSA\nPHiero(y T 1 ) = T\u220f t=1 PHiero(yt|yt\u221211 ,x) \u221d S(D).\n(4) The Hiero WFSAs have been transformed so that their arc weights have the negative log of the conditional probabilities defined in Eq. 3. All the probability mass of this distribution is concentrated on the Hiero translation hypotheses. The complete translation and language model scores computed over the entire Hiero translations are pushed as far forward in the WFSAs as possible. This is commonly done for left-to-right decoding in speech recognition (Mohri et al., 2002)."}, {"heading": "2.2 NMT\u2013Hiero Decoding", "text": "As above, suppose a path to a state in the WFSA accepts a Hiero translation prefix yt\u221211 , and let yt be a symbol on an outgoing arc from that state. We define the joint NMT+Hiero score as\nlogP (yt|yt\u221211 ,x) = \u03bbHiero logPHiero(yt|yt\u221211 ,x) +\n\u03bbNMT { logPNMT (yt|yt\u221211 ,x) yt \u2208 \u03a3NMT logPNMT (unk|yt\u221211 ,x) yt 6\u2208 \u03a3NMT\n(5)\nNote that the NMT-HIERO decoder only considers hypotheses in the Hiero lattice. As discussed earlier, the Hiero vocabulary can be much larger than the NMT output vocabulary \u03a3NMT . If a Hiero translation contains a word not in the NMT vocabulary, the NMT model provides a score and updates its decoder state as for an unknown word.\nOur decoding algorithm is a natural extension of beam search decoding for NMT. Due to the form of Eq. 5 we can build up hypotheses from left-toright on the target side. Thus, we can represent a partial hypothesis h = (yt1, hs) by a translation prefix yt1 and an accumulated score hs. At each iteration we extend the current hypotheses by one target token, until the best scoring hypothesis reaches a final state of the Hiero lattice. We refer to this step as node expansion, and in Sec. 3.1 we report the number of node expansions per sentence, as an indication of computational cost.\nWe can think of the decoding algorithm as breath-first search through the translation lattices with a limited number of active hypotheses (a beam). Rescoring is done on-the-fly: as the decoder traverses an edge in the WFSA, we update its weight by Eq. 5. The output-synchronous char-\nacteristic of beam search enables us to compute the NMT posteriors only once for each history based on previous calculations.\nAlternatively, we can think of the algorithm as NMT decoding with revised posterior probabilities: instead of selecting the most likely symbol yt according the NMT model, we adjust the NMT posterior with the Hiero posterior scores and delete NMT entries that are not allowed by the lattice. This may result in NMT choosing a different symbol, which is then fed back to the neural network for the next decoding step."}, {"heading": "3 Experimental Evaluation", "text": "We evaluate SGNMT on the WMT news-test2014 test sets (the filtered version) for English-German (En-De) and English-French (En-Fr). We also report results on WMT news-test2015 En-De.\nThe En-De training set includes Europarl v7, Common Crawl, and News Commentary v10. Sentence pairs with sentences longer than 80 words or length ratios exceeding 2.4:1 were deleted, as were Common Crawl sentences from other languages (Shuyo, 2010). The En-Fr NMT system was trained on preprocessed data (Schwenk, 2014) used by previous work (Sutskever et al., 2014; Bahdanau et al., 2015; Jean et al., 2015a), but with truecasing like our Hiero baseline. Following (Jean et al., 2015a), we use news-test2012 and news-test2013 as a development set. The NMT vocabulary size is 50k for En-De and 30k for En-Fr, taken as the most frequent words in training (Jean et al., 2015a). Tab. 1 provides statistics and shows the severity of the OOV problem for NMT.\nThe BASIC NMT system is built using the Blocks framework (van Merrie\u0308nboer et al., 2015) based on the Theano library (Bastien et al., 2012) with standard hyper-parameters (Bahdanau et al., 2015): the encoder and decoder networks consist of 1000 gated recurrent units (Cho et al., 2014). The decoder uses a single maxout (Goodfellow et al., 2013) output layer with the feed-forward attention model (Bahdanau et al., 2015).\nThe En-De Hiero system uses rules which encourage verb movement (de Gispert et al., 2010). The rules for En-Fr were extracted from the full data set available at the WMT\u201915 website using a shallow-1 grammar (de Gispert et al., 2010). 5- gram Kneser-Ney language models (KN-LM) for the Hiero systems were trained on WMT\u201915 parallel and monolingual data (Heafield et al., 2013).\nOur SGNMT system1 is built with the Pyfst interface 2 to OpenFst (Allauzen et al., 2007)."}, {"heading": "3.1 SGNMT Performance", "text": "Tab. 2 compares our combined NMT+Hiero decoding with NMT results in the literature. We use a beam size of 12. In En-De and in En-Fr, we find that our BASIC NMT system performs similarly (within 0.5 BLEU) to previously published results (16.31 vs. 16.46 and 30.42 vs. 29.97).\nIn NMT-HIERO, decoding is as described in Sec. 2.2, but with \u03bbHiero = 0. The decoder searches through the Hiero lattice, ignoring the Hiero scores, but using Hiero word hypotheses in place of any UNKs that might have been produced by NMT. The results show that NMT-HIERO is much more effective in fixing NMT OOVs than the \u2018UNK Replace\u2019 technique (Luong et al., 2015); this holds in both En-De and En-Fr.\nFor the NMT-HIERO+TUNING systems, lattice MERT (Macherey et al., 2008) is used to optimise \u03bbHiero and \u03bbNMT on the tuning sets. This yields further gains in both En-Fr and En-De, suggesting\n1http://ucam-smt.github.io/sgnmt/html/ 2https://pyfst.github.io/\nthat in addition to fixing UNKs, the Hiero predictive posteriors can be used to improve the NMT translation model scores.\nTab. 3 reports results of our En-De system with reshuffling and tuning on news-test2015. BLEU scores are directly comparable to WMT\u201915 results 3. By comparing row 3 to row 10, we see that constraining NMT to the search space defined by the Hiero lattices yields an improvement of +0.8 BLEU for single NMT. If we allow Hiero to fix NMT UNKs, we see a further +2.7 BLEU gain (row 11). The majority of gains come from fixing UNKs, but there is still improvement from the constrained search space for single NMT.\nWe next investigate the contribution of the Hiero system scores. We see that, once lattices are generated, the KN-LM contributes more to rescoring than the Hiero grammar scores (rows 12- 14). Further gains can be achieved by adding a feed-forward neural language model with NPLM (Vaswani et al., 2013) (row 15). We observe that n-best list rescoring with NMT (Neubig et al., 2015) also outperforms both the Hiero and NMT\n3http://matrix.statmt.org/matrix/systems list/1774\nbaselines, although lattice rescoring gives the best results (row 9 vs. row 15). Lattice rescoring with SGNMT also uses far fewer node expansions per sentence. We report n-best rescoring speeds for rescoring each hypothesis separately, and a depthfirst (DFS) scheme that efficiently traverses the nbest lists. Both these techniques are very slow compared to lattice rescoring. Fig. 1 shows that we can reduce the beam size from 12 to 5 with only a minor drop in BLEU. This is nearly 100 times faster than DFS over the 1000-best list.\nCost of Lattice Preprocessing As described in Sec. 2.1, we applied determinisation, minimisation, and weight pushing to the Hiero lattices in order to work with probabilities. Tab. 4 shows that those operations are generally fast4.\nLattice Size For previous experiments we set the Hiero pruning parameters such that lattices had 8,510 nodes on average. Fig. 2 plots the BLEU score over the lattice size. We find that SGNMT works well on lattices of moderate or large size, but pruning lattices too heavily has a negative effect as they are then too similar to Hiero first best hypotheses. We note that lattice rescoring involves nearly as many node expansions as unconstrained NMT decoding. This confirms that the lattices at 8,510 nodes are already large enough for SGNMT.\n4Testing environment: Ubuntu 14.04, Linux 3.13.0, single Intel R\u00a9 Xeon R\u00a9 X5650 CPU at 2.67 GHz\nLocal Softmax In SGNMT decoding we have the option of normalising the NMT translation probabilities over the words on outgoing words from each state rather than over the full 50,000 words translation vocabulary. There are \u223c4.5 arcs per state in our En-De\u201914 lattices, and so avoiding the full softmax could cause significant computational savings. We find this leads to only a modest 0.5 BLEU degradation: 21.45 BLEU in En-De\u201914, compared to 21.87 BLEU using NMT probabilities computed over the full vocabulary.\nModelling Errors vs. Search Errors In our EnDe\u201914 experiments with \u03bbHiero = 0 we find that constraining the NMT decoder to the Hiero lattices yields translation hypotheses with much lower NMT probabilities than unconstrained BASIC NMT decoding: under the NMT model, NMT hypotheses are 8,300 times more likely (median) than NMT-HIERO hypotheses. We conclude (tentatively) that BASIC NMT is not suffering only from search errors, but rather that NMT-HIERO discards some hypotheses ranked highly by the NMT model but lower in the evaluation metric."}, {"heading": "4 Conclusion", "text": "We have demonstrated a viable approach to Syntactically Guided Neural Machine Translation formulated to exploit the rich, structured search space generated by Hiero and the long-context translation scores of NMT. SGNMT does not suffer from the severe limitation in vocabulary size of basic NMT and avoids any difficulty of extending distributed word representations to new vocabulary items not seen in training data."}, {"heading": "Acknowledgements", "text": "This work was supported in part by the U.K. Engineering and Physical Sciences Research Council (EPSRC grant EP/L027623/1)."}], "references": [{"title": "OpenFst: A general and efficient weighted finite-state transducer library", "author": ["Cyril Allauzen", "Michael Riley", "Johan Schalkwyk", "Wojciech Skut", "Mehryar Mohri."], "venue": "Implementation and Application of Automata, pages 11\u201323. Springer.", "citeRegEx": "Allauzen et al\\.,? 2007", "shortCiteRegEx": "Allauzen et al\\.", "year": 2007}, {"title": "Pushdown automata in statistical machine translation. Volume 40, Issue 3 ", "author": ["Cyril Allauzen", "Bill Byrne", "de Adri\u00e0 Gispert", "Gonzalo Iglesias", "Michael Riley"], "venue": null, "citeRegEx": "Allauzen et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Allauzen et al\\.", "year": 2014}, {"title": "Neural machine translation by jointly learning to align and translate", "author": ["Dzmitry Bahdanau", "Kyunghyun Cho", "Yoshua Bengio."], "venue": "ICLR.", "citeRegEx": "Bahdanau et al\\.,? 2015", "shortCiteRegEx": "Bahdanau et al\\.", "year": 2015}, {"title": "Theano: new features and speed improvements", "author": ["Fr\u00e9d\u00e9ric Bastien", "Pascal Lamblin", "Razvan Pascanu", "James Bergstra", "Ian Goodfellow", "Arnaud Bergeron", "Nicolas Bouchard", "David Warde-Farley", "Yoshua Bengio."], "venue": "Deep Learning and Unsupervised", "citeRegEx": "Bastien et al\\.,? 2012", "shortCiteRegEx": "Bastien et al\\.", "year": 2012}, {"title": "11,001 new features for statistical machine translation", "author": ["David Chiang", "Kevin Knight", "Wei Wang."], "venue": "ACL, pages 218\u2013226.", "citeRegEx": "Chiang et al\\.,? 2009", "shortCiteRegEx": "Chiang et al\\.", "year": 2009}, {"title": "Hierarchical phrase-based translation", "author": ["David Chiang."], "venue": "Computational Linguistics, 33(2):201\u2013228.", "citeRegEx": "Chiang.,? 2007", "shortCiteRegEx": "Chiang.", "year": 2007}, {"title": "Variablelength word encodings for neural translation models", "author": ["Rohan Chitnis", "John DeNero."], "venue": "EMNLP, pages 2088\u20132093.", "citeRegEx": "Chitnis and DeNero.,? 2015", "shortCiteRegEx": "Chitnis and DeNero.", "year": 2015}, {"title": "Learning phrase representations using RNN encoder-decoder for statistical machine translation", "author": ["Kyunghyun Cho", "Bart van Merrienboer", "Caglar Gulcehre", "Fethi Bougares", "Holger Schwenk", "Yoshua Bengio."], "venue": "EMNLP.", "citeRegEx": "Cho et al\\.,? 2014", "shortCiteRegEx": "Cho et al\\.", "year": 2014}, {"title": "A unified architecture for natural language processing: Deep neural networks with multitask learning", "author": ["Ronan Collobert", "Jason Weston."], "venue": "Proceedings of the 25th International Conference on Machine Learning, pages 160\u2013167. ACM.", "citeRegEx": "Collobert and Weston.,? 2008", "shortCiteRegEx": "Collobert and Weston.", "year": 2008}, {"title": "Hierarchical phrase-based translation with weighted finite-state transducers and shallow-n grammars", "author": ["Adri\u00e0 de Gispert", "Gonzalo Iglesias", "Graeme Blackwood", "Eduardo R Banga", "William Byrne."], "venue": "Computational Linguistics, 36(3):505\u2013533.", "citeRegEx": "Gispert et al\\.,? 2010", "shortCiteRegEx": "Gispert et al\\.", "year": 2010}, {"title": "Maxout networks", "author": ["Ian Goodfellow", "David Warde-farley", "Mehdi Mirza", "Aaron Courville", "Yoshua Bengio."], "venue": "ICML, pages 1319\u20131327.", "citeRegEx": "Goodfellow et al\\.,? 2013", "shortCiteRegEx": "Goodfellow et al\\.", "year": 2013}, {"title": "Scalable modified Kneser-Ney language model estimation", "author": ["Kenneth Heafield", "Ivan Pouzyrevsky", "Jonathan H. Clark", "Philipp Koehn."], "venue": "ACL, pages 690\u2013696.", "citeRegEx": "Heafield et al\\.,? 2013", "shortCiteRegEx": "Heafield et al\\.", "year": 2013}, {"title": "Hierarchical phrasebased translation with weighted finite state transducers", "author": ["Gonzalo Iglesias", "Adri\u00e0 de Gispert", "Eduardo R Banga", "William Byrne."], "venue": "NAACL-HLT, pages 433\u2013441.", "citeRegEx": "Iglesias et al\\.,? 2009", "shortCiteRegEx": "Iglesias et al\\.", "year": 2009}, {"title": "Hierarchical phrase-based translation representations", "author": ["Gonzalo Iglesias", "Cyril Allauzen", "William Byrne", "Adri\u00e0 de Gispert", "Michael Riley."], "venue": "EMNLP, pages 1373\u20131383.", "citeRegEx": "Iglesias et al\\.,? 2011", "shortCiteRegEx": "Iglesias et al\\.", "year": 2011}, {"title": "On using very large target vocabulary for neural machine translation", "author": ["S\u00e9bastien Jean", "Kyunghyun Cho", "Roland Memisevic", "Yoshua Bengio."], "venue": "ACL, pages 1\u201310.", "citeRegEx": "Jean et al\\.,? 2015a", "shortCiteRegEx": "Jean et al\\.", "year": 2015}, {"title": "Montreal neural machine translation systems for WMT15", "author": ["S\u00e9bastien Jean", "Orhan Firat", "Kyunghyun Cho", "Roland Memisevic", "Yoshua Bengio."], "venue": "Proceedings of the Tenth Workshop on Statistical Machine Translation, pages 134\u2013140.", "citeRegEx": "Jean et al\\.,? 2015b", "shortCiteRegEx": "Jean et al\\.", "year": 2015}, {"title": "Recurrent continuous translation models", "author": ["Nal Kalchbrenner", "Phil Blunsom."], "venue": "EMNLP, page 413.", "citeRegEx": "Kalchbrenner and Blunsom.,? 2013", "shortCiteRegEx": "Kalchbrenner and Blunsom.", "year": 2013}, {"title": "Data-intensive text processing with MapReduce", "author": ["Jimmy Lin", "Chris Dyer."], "venue": "Morgan &Claypool.", "citeRegEx": "Lin and Dyer.,? 2010", "shortCiteRegEx": "Lin and Dyer.", "year": 2010}, {"title": "Addressing the rare word problem in neural machine translation", "author": ["Minh-Thang Luong", "Ilya Sutskever", "Quoc V Le", "Oriol Vinyals", "Wojciech Zaremba."], "venue": "ACL.", "citeRegEx": "Luong et al\\.,? 2015", "shortCiteRegEx": "Luong et al\\.", "year": 2015}, {"title": "Lattice-based minimum error rate training for statistical machine translation", "author": ["Wolfgang Macherey", "Franz Josef Och", "Ignacio Thayer", "Jakob Uszkoreit."], "venue": "EMNLP, pages 725\u2013734.", "citeRegEx": "Macherey et al\\.,? 2008", "shortCiteRegEx": "Macherey et al\\.", "year": 2008}, {"title": "Soft syntactic constraints for hierarchical phrased-based translation", "author": ["Yuval Marton", "Philip Resnik."], "venue": "ACL, pages 1003\u20131011.", "citeRegEx": "Marton and Resnik.,? 2008", "shortCiteRegEx": "Marton and Resnik.", "year": 2008}, {"title": "A weight pushing algorithm for large vocabulary speech recognition", "author": ["Mehryar Mohri", "Michael Riley."], "venue": "Interspeech, pages 1603\u20131606.", "citeRegEx": "Mohri and Riley.,? 2001", "shortCiteRegEx": "Mohri and Riley.", "year": 2001}, {"title": "Weighted finite-state transducers in speech recognition", "author": ["Mehryar Mohri", "Fernando Pereira", "Michael Riley."], "venue": "Computer Speech and Language, 16(1).", "citeRegEx": "Mohri et al\\.,? 2002", "shortCiteRegEx": "Mohri et al\\.", "year": 2002}, {"title": "Neural reranking improves subjective quality of machine translation: NAIST at WAT2015", "author": ["Graham Neubig", "Makoto Morishita", "Satoshi Nakamura."], "venue": "Workshop on Asian Translation, pages 35\u201341.", "citeRegEx": "Neubig et al\\.,? 2015", "shortCiteRegEx": "Neubig et al\\.", "year": 2015}, {"title": "Universit du Maine", "author": ["Holger Schwenk."], "venue": "http://www-lium.univ-lemans.fr/ \u0303schwenk/nnmt-shared-task/. [Online; accessed 1-March-2016].", "citeRegEx": "Schwenk.,? 2014", "shortCiteRegEx": "Schwenk.", "year": 2014}, {"title": "Neural machine translation of rare words with subword units", "author": ["Rico Sennrich", "Barry Haddow", "Alexandra Birch."], "venue": "arXiv preprint arXiv:1508.07909.", "citeRegEx": "Sennrich et al\\.,? 2015", "shortCiteRegEx": "Sennrich et al\\.", "year": 2015}, {"title": "Language detection library for Java", "author": ["Nakatani Shuyo."], "venue": "http://code.google.com/ p/language-detection/. [Online; accessed 1-March-2016].", "citeRegEx": "Shuyo.,? 2010", "shortCiteRegEx": "Shuyo.", "year": 2010}, {"title": "Sequence to sequence learning with neural networks", "author": ["Ilya Sutskever", "Oriol Vinyals", "Quoc V Le."], "venue": "Advances in Neural Information Processing Systems, pages 3104\u20133112.", "citeRegEx": "Sutskever et al\\.,? 2014", "shortCiteRegEx": "Sutskever et al\\.", "year": 2014}, {"title": "Coverage-based neural machine translation", "author": ["Zhaopeng Tu", "Zhengdong Lu", "Yang Liu", "Xiaohua Liu", "Hang Li."], "venue": "arXiv preprint arXiv:1601.04811.", "citeRegEx": "Tu et al\\.,? 2016", "shortCiteRegEx": "Tu et al\\.", "year": 2016}, {"title": "Blocks and fuel: Frameworks for deep learning", "author": ["Bart van Merri\u00ebnboer", "Dzmitry Bahdanau", "Vincent Dumoulin", "Dmitriy Serdyuk", "David Warde-Farley", "Jan Chorowski", "Yoshua Bengio."], "venue": "arXiv preprint arXiv:1506.00619.", "citeRegEx": "Merri\u00ebnboer et al\\.,? 2015", "shortCiteRegEx": "Merri\u00ebnboer et al\\.", "year": 2015}, {"title": "Decoding with large-scale neural language models improves translation", "author": ["Ashish Vaswani", "Yinggong Zhao", "Victoria Fossum", "David Chiang."], "venue": "EMNLP, pages 1387\u20131392.", "citeRegEx": "Vaswani et al\\.,? 2013", "shortCiteRegEx": "Vaswani et al\\.", "year": 2013}, {"title": "Preference grammars: Softening syntactic constraints to improve statistical machine translation", "author": ["Ashish Venugopal", "Andreas Zollmann", "Noah A. Smith", "Stephan Vogel."], "venue": "NAACL-HLT, pages 236\u2013 244.", "citeRegEx": "Venugopal et al\\.,? 2009", "shortCiteRegEx": "Venugopal et al\\.", "year": 2009}], "referenceMentions": [{"referenceID": 5, "context": "We report on investigations motivated by the idea that the structured search spaces defined by syntactic machine translation approaches such as Hiero (Chiang, 2007) can be used to guide Neural Machine Translation (NMT) (Kalchbrenner and", "startOffset": 150, "endOffset": 164}, {"referenceID": 2, "context": "The NMT encoder-decoder formalism provides a probability distribution over translations y = yT 1 of a source sentence x as (Bahdanau et al., 2015)", "startOffset": 123, "endOffset": 146}, {"referenceID": 2, "context": "Decoding is done by \u2018beam search to find a translation that approximately maximizes the conditional probability\u2019 (Bahdanau et al., 2015).", "startOffset": 113, "endOffset": 136}, {"referenceID": 13, "context": "This defines a regular language Y over strings in the target language via a projection of the sentence to be translated: Y = {y(D) : x(D) = x} (Iglesias et al., 2011; Allauzen et al., 2014).", "startOffset": 143, "endOffset": 189}, {"referenceID": 1, "context": "This defines a regular language Y over strings in the target language via a projection of the sentence to be translated: Y = {y(D) : x(D) = x} (Iglesias et al., 2011; Allauzen et al., 2014).", "startOffset": 143, "endOffset": 189}, {"referenceID": 5, "context": "Hiero decoders attempt to avoid search errors when combining the translation and language model for the translation hypotheses (Chiang, 2007; Iglesias et al., 2009).", "startOffset": 127, "endOffset": 164}, {"referenceID": 12, "context": "Hiero decoders attempt to avoid search errors when combining the translation and language model for the translation hypotheses (Chiang, 2007; Iglesias et al., 2009).", "startOffset": 127, "endOffset": 164}, {"referenceID": 31, "context": "The basic Hiero formalism can be extended through \u2018soft syntactic constraints\u2019 (Venugopal et al., 2009; Marton and Resnik, 2008) or by ar X iv :1 60 5.", "startOffset": 79, "endOffset": 128}, {"referenceID": 20, "context": "The basic Hiero formalism can be extended through \u2018soft syntactic constraints\u2019 (Venugopal et al., 2009; Marton and Resnik, 2008) or by ar X iv :1 60 5.", "startOffset": 79, "endOffset": 128}, {"referenceID": 4, "context": "adding very high dimensional features (Chiang et al., 2009), however the translation score assigned by the grammar is still only the product of probabilities of individual rules.", "startOffset": 38, "endOffset": 59}, {"referenceID": 28, "context": "The NMT beam decoder does not have an explicit mechanism for tracking source coverage, and there is evidence that may lead to both \u2018over-translation\u2019 and \u2018under-translation\u2019 (Tu et al., 2016).", "startOffset": 174, "endOffset": 191}, {"referenceID": 8, "context": "The NMT continuous representation captures morphological, syntactic and semantic similarity (Collobert and Weston, 2008) across words and phrases.", "startOffset": 92, "endOffset": 120}, {"referenceID": 14, "context": "open-domain MT is an open area of research (Jean et al., 2015a; Luong et al., 2015; Sennrich et al., 2015; Chitnis and DeNero, 2015).", "startOffset": 43, "endOffset": 132}, {"referenceID": 18, "context": "open-domain MT is an open area of research (Jean et al., 2015a; Luong et al., 2015; Sennrich et al., 2015; Chitnis and DeNero, 2015).", "startOffset": 43, "endOffset": 132}, {"referenceID": 25, "context": "open-domain MT is an open area of research (Jean et al., 2015a; Luong et al., 2015; Sennrich et al., 2015; Chitnis and DeNero, 2015).", "startOffset": 43, "endOffset": 132}, {"referenceID": 6, "context": "open-domain MT is an open area of research (Jean et al., 2015a; Luong et al., 2015; Sennrich et al., 2015; Chitnis and DeNero, 2015).", "startOffset": 43, "endOffset": 132}, {"referenceID": 11, "context": "very large vocabularies (Heafield et al., 2013; Lin and Dyer, 2010).", "startOffset": 24, "endOffset": 67}, {"referenceID": 17, "context": "very large vocabularies (Heafield et al., 2013; Lin and Dyer, 2010).", "startOffset": 24, "endOffset": 67}, {"referenceID": 21, "context": "tial state under the log semiring (Mohri and Riley, 2001).", "startOffset": 34, "endOffset": 57}, {"referenceID": 22, "context": "This is commonly done for left-to-right decoding in speech recognition (Mohri et al., 2002).", "startOffset": 71, "endOffset": 91}, {"referenceID": 26, "context": "4:1 were deleted, as were Common Crawl sentences from other languages (Shuyo, 2010).", "startOffset": 70, "endOffset": 83}, {"referenceID": 24, "context": "The En-Fr NMT system was trained on preprocessed data (Schwenk, 2014) used by previous work (Sutskever et al.", "startOffset": 54, "endOffset": 69}, {"referenceID": 14, "context": "Following (Jean et al., 2015a), we use news-test2012 and news-test2013 as a development set.", "startOffset": 10, "endOffset": 30}, {"referenceID": 14, "context": "taken as the most frequent words in training (Jean et al., 2015a).", "startOffset": 45, "endOffset": 65}, {"referenceID": 3, "context": ", 2015) based on the Theano library (Bastien et al., 2012) with standard hyper-parameters (Bahdanau et al.", "startOffset": 36, "endOffset": 58}, {"referenceID": 2, "context": ", 2012) with standard hyper-parameters (Bahdanau et al., 2015): the encoder and decoder networks consist of 1000 gated recurrent units (Cho et al.", "startOffset": 39, "endOffset": 62}, {"referenceID": 7, "context": ", 2015): the encoder and decoder networks consist of 1000 gated recurrent units (Cho et al., 2014).", "startOffset": 80, "endOffset": 98}, {"referenceID": 10, "context": "The decoder uses a single maxout (Goodfellow et al., 2013) output layer with the feed-forward attention model (Bahdanau et al.", "startOffset": 33, "endOffset": 58}, {"referenceID": 2, "context": ", 2013) output layer with the feed-forward attention model (Bahdanau et al., 2015).", "startOffset": 59, "endOffset": 82}, {"referenceID": 11, "context": "5gram Kneser-Ney language models (KN-LM) for the Hiero systems were trained on WMT\u201915 parallel and monolingual data (Heafield et al., 2013).", "startOffset": 116, "endOffset": 139}, {"referenceID": 14, "context": "NMT-LV refers to the RNNSEARCH-LV model from (Jean et al., 2015a) for large output vocabularies.", "startOffset": 45, "endOffset": 65}, {"referenceID": 15, "context": "4 16 Neural MT \u2013 UMontreal-MILA (Jean et al., 2015b) 22.", "startOffset": 32, "endOffset": 52}, {"referenceID": 0, "context": "Our SGNMT system1 is built with the Pyfst interface 2 to OpenFst (Allauzen et al., 2007).", "startOffset": 65, "endOffset": 88}, {"referenceID": 18, "context": "The results show that NMT-HIERO is much more effective in fixing NMT OOVs than the \u2018UNK Replace\u2019 technique (Luong et al., 2015); this holds in both En-De and En-Fr.", "startOffset": 107, "endOffset": 127}, {"referenceID": 19, "context": "For the NMT-HIERO+TUNING systems, lattice MERT (Macherey et al., 2008) is used to optimise \u03bbHiero and \u03bbNMT on the tuning sets.", "startOffset": 47, "endOffset": 70}, {"referenceID": 30, "context": "Further gains can be achieved by adding a feed-forward neural language model with NPLM (Vaswani et al., 2013) (row 15).", "startOffset": 87, "endOffset": 109}, {"referenceID": 23, "context": "We observe that n-best list rescoring with NMT (Neubig et al., 2015) also outperforms both the Hiero and NMT", "startOffset": 47, "endOffset": 68}], "year": 2016, "abstractText": "We investigate the use of hierarchical phrase-based SMT lattices in end-to-end neural machine translation (NMT). Weight pushing transforms the Hiero scores for complete translation hypotheses, with the full translation grammar score and full ngram language model score, into posteriors compatible with NMT predictive probabilities. With a slightly modified NMT beam-search decoder we find gains over both Hiero and NMT decoding alone, with practical advantages in extending NMT to very large input and output vocabularies.", "creator": "TeX"}}}