{"id": "1605.04934", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "16-May-2016", "title": "Self-Reflective Risk-Aware Artificial Cognitive Modeling for Robot Response to Human Behaviors", "abstract": "In order for cooperative robots (\"co-robots\") to respond to human behaviors accurately and efficiently in human-robot collaboration, interpretation of human actions, awareness of new situations, and appropriate decision making are all crucial abilities for co-robots. For this purpose, the human behaviors should be interpreted by co-robots in the same manner as human peers. To address this issue, a novel interpretability indicator is introduced so that robot actions are appropriate to the current human behaviors. In addition, the complete consideration of all potential situations of a robot's environment is nearly impossible in real-world applications, making it difficult for the co-robot to act appropriately and safely in new scenarios. This is true even when the pretrained model is highly accurate in a known situation. For effective and safe teaming with humans, we introduce a new generalizability indicator that allows a co-robot to self-reflect and reason about when an observation falls outside the co-robot's learned model. Based on topic modeling and two novel indicators, we propose a new Self-reflective Risk-aware Artificial Cognitive (SRAC) model. The co-robots are able to consider action risks and identify new situations so that better decisions can be made. Experiments both using real-world datasets and on physical robots suggest that our SRAC model significantly outperforms the traditional methodology and enables better decision making in response to human activities. We propose that self-reflective risk-aware AI models, such as HRAS (reactive task-critical tasks), would be capable of taking action actions as part of an effective control task, so that autonomous decisions and actions are more accurately and effectively represented when self-reflective risk-aware AI models are used to monitor activity. In addition, we propose that self-reflective risk-aware AI models, such as HRAS, would be able to use self-reflective risk-aware AI models, such as HRAS (reactive task-critical tasks), and would be capable of taking action as part of an effective control task, so that autonomous decisions and actions are more accurately and effectively represented when self-reflective risk-aware AI models are used to monitor activity.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n", "histories": [["v1", "Mon, 16 May 2016 20:22:30 GMT  (2629kb,D)", "http://arxiv.org/abs/1605.04934v1", "40 pages"]], "COMMENTS": "40 pages", "reviews": [], "SUBJECTS": "cs.RO cs.AI", "authors": ["fei han", "christopher reardon", "lynne e parker", "hao zhang"], "accepted": false, "id": "1605.04934"}, "pdf": {"name": "1605.04934.pdf", "metadata": {"source": "CRF", "title": "Self-Reflective Risk-Aware Artificial Cognitive Modeling for Robot Response to Human Behaviors", "authors": ["Fei Hana", "Christopher Reardonb", "Lynne E. Parkerb", "Hao Zhanga"], "emails": ["fhan@mines.edu", "creardon@utk.edu", "leparker@utk.edu", "hzhang@mines.edu"], "sections": [{"heading": null, "text": "In order for cooperative robots (\u201cco-robots\u201d) to respond to human behaviors accurately and efficiently in human-robot collaboration, interpretation of human actions, awareness of new situations, and appropriate decision making are all crucial abilities for corobots. For this purpose, the human behaviors should be interpreted by co-robots in the same manner as human peers. To address this issue, a novel interpretability indicator is introduced so that robot actions are appropriate to the current human behaviors. In addition, the complete consideration of all potential situations of a robot\u2019s environment is nearly impossible in real-world applications, making it difficult for the co-robot to act appropriately and safely in new scenarios. This is true even when the pretrained model is highly accurate in a known situation. For effective and safe teaming with humans, we introduce a new generalizability indicator that allows a co-robot to self-reflect and reason about when an observation falls outside the co-robot\u2019s learned model. Based on topic modeling and two novel indicators, we propose a new Self-reflective Risk-aware Artificial Cognitive (SRAC) model. The co-robots are able to consider action risks and identify new situations so that better decisions can be made. Experiments both using real-world datasets and on physical robots suggest that our SRAC model significantly outperforms the traditional methodology and enables better decision making in response to human activities.\nEmail addresses: fhan@mines.edu (Fei Han), creardon@utk.edu (Christopher Reardon), leparker@utk.edu (Lynne E. Parker), hzhang@mines.edu (Hao Zhang)\nMay 18, 2016\nar X\niv :1\n60 5.\n04 93\n4v 1\n[ cs\n.R O\n] 1\n6 M\nKeywords: Reflective cognitive models, risk-aware decision making, human behavior interpretation, human-robot collaboration"}, {"heading": "1. Introduction", "text": "In human-robot collaboration, it is crucial for a cooperative robot (\u201cco-robot\u201d) to have the abilities of perception of human activities and corresponding appropriate decision-making to understand and interact with human peers. In order to provide these important capabilities, an artificial cognitive model integrating perception, reasoning, and decision making modules is required by intelligent co-robots to respond to humans effectively. Artificial cognition has its origin in cybernetics; its intention is to create a science of mind based on logic [1]. Among other mechanisms, cognitivism is a most widely used cognitive paradigm [2]. Several cognitive architectures were developed within this paradigm, including ACT-R [3] (and its extensions ACT-R/E [4], ACT-R\u03a6 [5], etc), Soar [6], C4 [7], and architectures for robotics [8]. Because an architecture represents the connection and interaction of different cognitive components, it cannot accomplish a specific task on its own without specifying each component that can provide knowledge to the cognitive architecture. The combination of the cognitive architecture and components is usually referred to as a cognitive model [2].\nImplementing such an artificial cognitive system is challenging, since the highlevel processes (e.g., reasoning and decision making) must be able to seamlessly work with the low-level components, e.g., perception, under significant uncertainty in a complex environment [9]. In the context of human-robot collaboration, perceiving human behaviors is a necessary component, where uncertainty arises due to human behavior complexity, including variations in human motions and appearances, and challenges of machine vision, such as lighting changes and occlusion. This perception uncertainty is addressed in this work using the bag-of-visual-words (BoW) representation based on local spatio-temporal features, which has previously shown promising performance [10, 11, 12].\nTo further process the perceptual data, a high-level reasoning component is nec-\nessary for a co-robot to make decisions. In recent years, topic modeling has attracted increasing attention in human behavior discovery and recognition due to its ability to generate a distribution over activities of interest, and its promising performance using BoW representations in robotics applications [12, 13]. However, previous work only aimed at human behavior understanding; the essential task of incorporating topic modeling into cognitive decision making (e.g., selecting a response action) is not well analyzed.\nTraditional activity recognition systems typically use accuracy as a performance metric [14]. Because the accuracy metric ignores the distribution of activity categories, which is richer and more informative than a single label, it is not appropriate for decision making. For example, in a task of behavior understanding with two categories, assume that two recognition systems obtain two distributions [0.8, 0.2] and [0.55, 0.45] on a given observation, and the ground truth indicates the first category is correct. Although both systems are accurate, in the sense that the most probable category matches the ground truth, the first model obviously performs better, since it better separates the\ncorrect from the incorrect assignment. Previous studies did not consider this important phenomenon.\nIn real-world applications, artificial cognitive models must be applied in an online fashion. If a co-robot is unable to determine whether its knowledge is accurate, then if it observes a new human behavior that was not presented during the training phase, it cannot be correctly recognized, because the learned behavior recognition model no longer applies. Decision making based on incorrect recognition in situations like these can result in inappropriate or even unsafe robot action response. Thus, an artificial cognitive model requires the capability to self-reflect whether the learned activity recognition system becomes less applicable, analogous to human self-reflection on learned knowledge, when applied in a new unstructured environment. This problem was not well investigated in previous works.\nIn this paper, we develop a novel artificial cognitive model, based on topic models, for robot decision making in response to human behaviors. Our model is able to incorporate human behavior distributions and take into account robot action risks to make more appropriate decisions (we label this \u201crisk-aware\u201d). Also, our model is able to identify new scenarios when the learned recognition subsystem is less applicable (we label this \u201cself-reflective\u201d). Accordingly, we call our model the self-reflective, risk-aware artificial cognitive (SRAC) model.\nOur primary contributions are twofold:\n\u2022 Two novel indicators are proposed. The interpretability indicator (II ) enables a\nco-robot to interpret category distributions in a similar manner to humans. The online generalizability indicator (IG) measures the human behavior recognition model\u2019s generalization capacity (i.e., how well unseen observations can be represented by the learned model).\n\u2022 A novel artificial cognitive model (i.e., SRAC) is introduced based on topic mod-\nels and the indicators, which is able to consider robot action risks and perform self-reflection to improve robot decision making in response to human activities in new situations.\nThe rest of the paper is organized as follows. We first overview the related work\nin Section 2. Then, the artificial cognitive architecture and its functional modules are described in Section 3. Section 4 introduces the new indicators. Section 5 presents selfreflective risk-aware decision making. Experimental results are discussed in Section 6. Finally, we conclude our paper in Section 7."}, {"heading": "2. Related Work", "text": "In this section, we provide an overview of a variety of methods related to our proposed SRAC cognitive model for human-robot teaming, including human activity recognition, topic models, and artificial cognitive modeling."}, {"heading": "2.1. Human Activity Recognition", "text": "We focus our review on the commonly used sequential and space-time volume methods [15]. A comprehensive review of different aspects of human activity recognition (HAR) is presented in [15] and [16].\nA popular sequential method is to use centroid trajectories to identify human activities in visual data, in which a human is represented as a single point indicating the human\u2019s location. Chen and Yang represented a human with just a point to derive the gait features for the pedestrian detection [17]. Ge et al. extracted pedestrian trajectories from video to automatically detect small groups of people traveling together [18]. These methods can avoid the influence of human appearances such as dresses and carrying, but are not able to recognize activities involving various relative bodypart movements. Another sequential method relies on human shapes, including human contours and silhouettes. Singh et al. extracted directional vectors from the silhouette contours and utilize the distinct data distribution of these vectors in a vector space for activity recognition [19]. Junejo et al. transformed silhouettes of a human from every frame into time-series, then each of these time series is converted into the symbolic vector to represent actions [20]. A third method is based on body-part models. Zhang and Parker implemented a bio-inspired predictive orientation decomposition (BIPOD) to construct representations of people from skeleton trajectories for the activity recognition and prediction, where the human body is decomposed into five body parts [21].\nHowever, techniques based on shapes and/or body-part models rely on human and body-part detection, which are hard-to-solve problems due to occlusions and dynamic backgrounds, among others.\nSpace-time volume methods use local features to represent local texture and motion variations regardless of global human appearance and activity. A large number of HAR methods are based on SIFT features [22] and its extensions [23]. For example, Behera et al. proposed a random forest that unifies randomization, discriminative relationships mining and a Markov temporal structure for real-time activity recognition with SIFT features [24]. However, SIFT features only encode appearance information and are not able to represent temporal information. STIP features were introduced in [25] and SVMs were applied to classify human activities. Dollar et al. used separable filters in spatial and temporal dimensions to extract features for HAR [26]. Four-dimensional features were also introduced in [12] to combine depth information to classify human activities.\nPrevious work focused only on recognizing human activities but did not discuss the consequent issue: how a co-robot can make decisions based on recognition results, especially when risks are associated with different human activities."}, {"heading": "2.2. Topic Models and Evaluation", "text": "Among other machine learning techniques, topic modeling has been widely applied to HAR. Zhao et al. incorporated Bayesian learning into an undirected topic model and proposed a \u201drelevance topic model\u201d for the unstructured social group activity recognition [27]. A semi-latent topic model trained in a supervised fashion was introduced in [28] and used to classify activities in videos. Zhang and Parker adopted topic models to classify activities in 3D point clouds from color-depth cameras on mobile robots [12]. Topic models were also widely used to discover human activities in streaming data. The use of topic models was explored in [29] to discover daily activity patterns in wearable sensor data. An unsupervised topic model was proposed in [30] to detect daily routines from streaming location and proximity data. Taking temporal and/or object relational information into account, Freedman et al. explored a new method using topic models for both plan recognition and activity recognition objective [31].\nAlthough there is a significant body of work introducing and developing sophisticated topic models, few efforts have been undertaken to evaluate them. Existing methods are dominated by either intrinsic methods, (e.g., computing the probability of held-out documents to evaluate generalization ability [32]) or extrinsic methods that rely on external tasks, (e.g., information retrieval [33]). Some work also focused on evaluation of topic modeling\u2019s interpretability as semantically coherent concepts. For example, Chang et al. demonstrated that the probability of held-out documents is not always a good indicator of human judgment [34]. Newman et al. showed that metrics based on word co-occurrence statistics are able to predict human evaluations of topic quality [35].\nAs recently pointed out by Blei [36], topic model evaluation is an essential research topic. Despite this, previous works use only the accuracy metric to evaluate topic modeling results in HAR tasks; issues such as the model\u2019s interpretability and generalizability have not been studied. In this paper, we analyze these two aspects of topic model evaluation in HAR tasks, explore their relationship, and show how they can be used to improve robot decision making."}, {"heading": "2.3. Artificial Cognitive Modeling", "text": "Artificial cognition has its origin in cybernetics with the intention to create a science of mind based on logic [1]. Among other cognitive paradigms, cognitivism has undoubtedly been predominant to date [2]. Within the cognitivism paradigm, several cognitive architectures were developed, including Soar [6], ACT-R [3] (and its extensions ACT-R/E [4], ACT-R\u03a6 [5], etc), C4 [7], and architectures for robotics [8], which are relatively independent of applications [37]. Because architectures represent the mechanism for cognition but lack the relevant information for using that mechanism, they cannot accomplish anything in their own right and need to be provided with knowledge to conduct a specific task. The combination of a cognitive architecture and a particular knowledge set is generally referred to as a cognitive model [2]. The knowledge incorporated in cognitive models is typically determined by human designers [2]. The knowledge can be also learned and adapted using machine learning techniques.\nCognitive models have been widely used in human-machine interaction and robotic\nvision applications. For example, cognitive modeling was adopted in [38, 39, 40] to construct intelligent human-machine interaction systems. Cognitive perception systems were also used to recognize traffic signs [41, 42], interpret traffic behaviors [43, 44], and recognize human activities [45, 46]. Over the last decade, probabilistic models of cognition, as an alternative of deterministic cognitive models, have attracted more attention in cognitive development [47]. For example, an adaptive remote data mirroring system was proposed applying dynamic decision networks in [48]. Another cognitive model was introduced in [49] to apply dynamic Bayesian networks for vehicle classification. Probabilistic models have also been widely used for learning and reasoning in cognitive modeling [50].\nWe believe we are the first to adopt topic models for the construction of reliable artificial cognitive models and show that they are particularly suited for this task. We demonstrate topic modeling\u2019s ability to combine risks in decision making. In addition, we develop two evaluation metrics and show their effectiveness in model selection and decision making. These aspects were not addressed in previous artificial cognitive modeling research."}, {"heading": "3. Topic Modeling for Artificial Cognition", "text": ""}, {"heading": "3.1. Cognitive Architecture Overview", "text": "The proposed SRAC model is inspired by the C4 cognitive architecture [7]. As\nshown in Fig. 1, our model is organized into four modules by their functionality:\n\u2022 Sensory and perception: Visual cameras observe the environment. Then, the\nperception system builds a BoW representation from raw data, which can be processed by topic models.\n\u2022 Probabilistic reasoning: Topic models are applied to reason about human activ-\nities, which are trained off-line and used online. The training set is provided as a prior that encodes a history of sensory information. This module uses the proposed indicators to select topic models that better match human\u2019s perspective, and to discover new activities in an online fashion.\n\u2022 Decision making: Robot action risk based on topic models and the evaluation re-\nsults is estimated and a response robot action that minimizes this risk is selected. The risk is provided as a prior to the module.\n\u2022 Navigation and motor system: The selected robot action is executed in response\nto human activities."}, {"heading": "3.2. Topic Modeling", "text": "Latent Dirichlet Allocation (LDA) [51], which showed promising activity recogni-\ntion performance in our prior work [12], is applied in the SRAC model.\nGiven a set of observations {w}, LDA models each of K activities as a multinomial distribution of all possible visual words in the dictionary D. This distribution is parameterized by \u03d5 = {\u03d5w1 , . . . , \u03d5w|D|}, where \u03d5w is the probability that the word w is generated by the activity. LDA also represents each w as a collection of visual words, and assumes that each word w \u2208 w is associated with a latent activity assignment z. By applying the visual words to connect observations and activities, LDA models w as a multinomial distribution over the activities, which is parameterized by \u03b8= {\u03b8z1 , . . . , \u03b8zK}, where \u03b8z is the probability that w is generated by the activity z. LDA is a Bayesian model, which places Dirichlet priors on the multinomial parameters: \u03d5\u223cDir(\u03b2) and \u03b8 \u223cDir(\u03b1), where \u03b2 = {\u03b2w1 , . . . , \u03b2w|D|} and \u03b1= {\u03b1z1 , . . . , \u03b1zK} are the concentration hyperparameters.\nOne of the major objectives in HAR tasks to is to estimate the parameter \u03b8, i.e., the per-observation activity proportion. However, exact parameter estimation is intractable in general [51]. Our model applies Gibbs sampling [52] to compute the per-observation activity distribution \u03b8, based on two considerations: 1) This sampling-based method is generally accurate, since it asymptotically approaches the correct distribution [53], and 2) This method can be used to intrinsically evaluate topic model\u2019s performance [32], thereby providing a consistent method to infer, learn, and evaluate topic models. At convergence, the element \u03b8zk \u2208\u03b8, k=1, . . . ,K, is estimated by:\n\u03b8\u0302zk = nzk + \u03b1zk\u2211 z (nz + \u03b1z) , (1)\nwhere nz is the number of times that a visual word is assigned to activity zk in the observation.\nThe incorporation of topic models into cognitive modeling has several important advantages. First, as a probabilistic reasoning approach, it serves as a bridge to allow information to flow from the perception module to the decision making module. Second, the ability to model per-observation activity distribution allows topic models to take into account the risks of all robot actions in a probabilistic way and make an appropriate decision. Third, by introducing an extrinsic evaluation metric for topic model selection, the constructed cognitive system is able to accurately interpret human activities. Fourth, the unsupervised nature of topic modeling, which is explored using our new intrinsic metric, facilitates online discovery of new knowledge (e.g., human activities). All these advantages allow us to apply topic models to construct an artificial cognitive system that is able to better interpret human activities, discover new knowledge and react more appropriately and safely to humans, which is highly desirable for real-world online human-robot interaction scenarios."}, {"heading": "4. Interpretability and Generalizability", "text": "To improve artificial cognitive modeling, we introduce two novel indicators and discuss their relationship in this section, which are the core of the Self-Reflection module in Fig. 1."}, {"heading": "4.1. Interpretability Indicator", "text": "We observe that accuracy is not an appropriate assessment metric for robot decision making, since it only considers the most probable human activity category and ignores the others. To utilize the category distribution, which contains much richer information, the interpretability indicator, denoted by II , is introduced. II is able to encode how well topic modeling matches human common sense. Like the accuracy metric, II is an extrinsic metric, meaning that it requires a ground truth to compute. Formally, II is defined as follows:\nDefinition 1 (Interpretability indicator). Given the observation w with the ground truth g and the distribution \u03b8 over K \u2265 2 categories, let \u03b8s = (\u03b81, . . . , \u03b8k\u22121, \u03b8k, \u03b8k+1, . . . , \u03b8K) denote the sorted proportion satisfying \u03b81 \u2265 \u00b7 \u00b7 \u00b7 \u2265 \u03b8k\u22121 \u2265 \u03b8k \u2265\n\u03b8k+1 \u2265 \u00b7 \u00b7 \u00b7 \u2265 \u03b8K \u2265 0 and \u2211K i=1 \u03b8i = 1, and let k \u2208 {1, \u00b7 \u00b7 \u00b7 ,K} represent the index of the assignment in \u03b8s that matches g. The interpretability indicator II(\u03b8, g) = II(\u03b8s,k) is defined as:\nII(\u03b8s,k) , 1\na ( K\u2212k K\u22121 + 1(k=K) )( \u03b8k \u03b81 \u2212 \u03b8k+1(k 6=K) \u03b8k +b ) (2)\nwhere 1(\u00b7) is the indicator function, and a = 2, b = 1 are normalizing constants.\nThe indicator II is defined over the per-observation category proportion \u03b8, which takes values in the (K\u22121)-simplex [51]. The sorted proportion \u03b8s is computed through sorting \u03b8, which is inferred by topic models. In the definition, the ground truth is represented by its location in \u03b8s, i.e., the k-th most probable assignment in \u03b8s matches the ground truth label. The indicator function 1(\u00b7) in Eq. (2) is adopted to deal with the special case when k = K.\nFor an observation in an activity recognition task with K categories, given its ground truth index k and sorted category proportion \u03b8s, we summarize II \u2019s properties as follows:\nProposition 1 (II \u2019s properties). The interpretability indicator II(\u03b8, g) = II(\u03b8s, k) satisfies the following properties:\n1. If k = 1, \u2200\u03b8s, II(\u03b8s, k) \u2265 0.5. 2. If k = K, \u2200\u03b8s, II(\u03b8s, k) \u2264 0.5. 3. \u2200\u03b8s, II(\u03b8s, k) \u2208 [0, 1]. 4. \u2200k \u2208 {1, . . . ,K} and \u03b8s, \u03b8\u2032s such that \u03b81 \u2265 \u03b8\u20321, \u03b8k = \u03b8\u2032k and \u03b8k+1(k 6=K) =\n\u03b8\u2032k+1(k 6=K), II(\u03b8s, k) \u2264 II(\u03b8 \u2032 s, k) holds.\n5. \u2200k \u2208 {1, . . . ,K} and \u03b8s, \u03b8\u2032s such that \u03b8k+1(k 6=K) \u2265 \u03b8\u2032k+1(k 6=K), \u03b81 = \u03b8 \u2032 1 and\n\u03b8k=\u03b8 \u2032 k, II(\u03b8s, k) \u2264 II(\u03b8 \u2032 s, k) holds.\n6. \u2200k \u2208 {1, . . . ,K} and \u03b8s, \u03b8\u2032s such that \u03b8k \u2265 \u03b8\u2032k, \u03b81 = \u03b8\u20321 and \u03b8k+1(k 6=K) =\n\u03b8\u2032k+1(k 6=K), II(\u03b8s, k) \u2265 II(\u03b8 \u2032 s, k) holds.\n7. \u2200k, k\u2032 \u2208 {1, . . . ,K} such that k \u2264 k\u2032 < K and \u2200\u03b8s, \u03b8\u2032s such that \u03b8k = \u03b8\u2032k,\n\u03b81 = \u03b8 \u2032 1 and \u03b8k+1(k 6=K) = \u03b8 \u2032 k+1(k 6=K), II(\u03b8s, k) \u2265 II(\u03b8 \u2032 s, k \u2032) holds.\nPROOF. See Appendix A.\nThe indicator II is able to quantitatively measure how well topic modeling can match human common sense, because it captures three essential considerations to simulate the process of how humans evaluate the category proportion \u03b8:\n\u2022 A topic model performs better, in general, if it obtains a larger \u03b8k (Property 6).\nIn addition, a larger \u03b8k generally indicates \u03b8k is closer to the beginning in \u03b8s and further away from the end (Property 7).\nExample: A topic model obtaining the sorted proportion [0.4, 0.35 , 0.15, 0.10] performs better than a model obtaining [0.4, 0.30 , 0.15, 0.15], where the ground truth is marked with a box, i.e., k = 2 in the example.\n\u2022 A smaller difference between \u03b8k and \u03b81 indicates better modeling performance\n(Properties 4 and 5), in general. Since the resulting category proportion is sorted, a small difference between \u03b8k and \u03b81 guarantees \u03b8k has an even smaller difference from \u03b82 to \u03b8k\u22121.\nExample: A topic model obtaining the sorted proportion [0.4, 0.3 , 0.2, 0.1] performs better than the model with the proportion [0.5, 0.3 , 0.2, 0].\n\u2022 A larger distinction between \u03b8k and \u03b8k+1 generally indicates better modeling\nperformance (Properties 5 and 6), since it better separates the correct assignment from the incorrect assignments with lower probabilities.\nExample: A topic model obtaining the sorted proportion [0.4, 0.4 , 0.1, 0.1] performs better than the topic model obtaining the proportion [0.4, 0.4 , 0.2, 0].\nThe indicator II extends the accuracy metric IA (i.e., rate of correctly recognized\ndata), as described in Proposition 2:\nProposition 2 (Relationship of II and IA). The accuracy measure IA is a special case of II(\u03b8s, k), when \u03b81 = 1.0, \u03b82 = . . .= \u03b8K = 0, and k = 1 or k = K.\nPROOF. See Appendix B."}, {"heading": "4.2. Generalizability Indicator", "text": "An artificial cognitive model requires the crucial capability of detecting new situations and being aware that the learned knowledge becomes less applicable in an online fashion. To this end, we propose the generalizability indicator (IG), an intrinsic metric that does not require ground truth to compute and consequently can be used online.\nThe introduction of IG is inspired by the perplexity metric (also referred to as heldout likelihood), which evaluates a topic model\u2019s generalization ability on a fraction of held-out instances using cross-validation [54] or unseen observations [55]. The perplexity is defined as the log-likelihood of words in an observation [32]. Because different observations may contain a different number of visual words, we compute the Per-Visual-Word Perplexity (Pvwp). Mathematically, given the trained topic modelM and an observation w, Pvwp is defined as follows:\nPvwp(w|M)= 1 N logP (w|M)= 1 N log N\u220f n=1 P (wn|w<n,M) (3)\nwhere N = |w| is the number of visual words in w, and the subscript < n denotes positions before n. Because P (w|M) is a probability that satisfies P (w|M)\u22641, it is guaranteed Pvwp(w|M)\u2264 0. The left-to-right algorithm, presented in Algorithm 1, is used to estimate Pvwp, which is an accurate and efficient Gibbs sampling method to estimate perplexity [32]. The algorithm decomposes P (w|M) in an incremental, left-to-right fashion, where the subscript \u00acn is a quantity that excludes data from the nth position. Given observationsW={w1, . . . ,wM}, Pvwp(W|M) is defined as the average of each observation\u2019s perplexity:\nPvwp(W|M) = 1 M M\u2211 m=1 Pvwp(wm|M) (4)\nBased on Pvwp, the generalizability indicator IG, on previously unseen observations in the testing phase or using the held-out instances in cross-validation, is defined as follows:\nDefinition 2 (Generalizability indicator). LetM denote a trained topic model,Wvalid denote the validation dataset that is used in the training phase, andw be an previously\nAlgorithm 1: Left-to-right Pvwp estimation Input : w (observation),M (trained topic model), and R (number of particles)\nOutput : Pvwp(w|M)\n1: Initialize l = 0 and N = |w|; 2: for each position n = 1 to N inw do 3: Initialize pn = 0; 4: for each particle r = 1 to R do 5: for n\u2032 < n do 6: Sample z(r)n\u2032 \u223c P (z (r) n\u2032 |wn\u2032 , {z (r) <n}\u00acn\u2032 ,M); 7: end\n8: Compute pn = pn + \u2211 t P (wn, z (r) n = t|z(r)<n,M); 9: Sample z(r)n \u223c P (z(r)n |wn, z(r)<n,M);\n10: end 11: Update pn = pnR and l = l + log pn; 12: end 13: return Pvwp(w|M) ' l N .\nunseen observation. We define the generalizability indicator:\nIG(w),  exp(Pvwp(w|M)) c \u00b7 exp(Pvwp(Wvalid|M)) if exp(Pvwp(w|M))<c\u00b7exp(Pvwp(Wvalid|M))\n1 if exp(Pvwp(w|M))\u2265c\u00b7exp(Pvwp(Wvalid|M))\n(5)\nwhere c \u2208 [1,\u221e) is a constant encoding novelty levels.\nBesides considering the topic model\u2019s generalization ability, IG also evaluates whether\npreviously unseen observations are well-represented by the training set, i.e., whether the training set used to train the topic model is exhaustive. The training set is defined as exhaustive when it contains instances from all categories that can possibly be observed in the testing phase [56]. When some categories are missing and not represented by the training set, it is defined as non-exhaustive; in this case, novel categories emerge in the testing phase. Since it is impractical, often impossible, to define an exhaustive training set, mainly because some of the categories may not exist at the time of training, the ability to discover novelty and be aware that the learned model is less applicable\nis essential for safe, adaptive decision making. The indicator IG provides this ability through evaluating how well new observations are represented by the validation set in the training phase.\nWe constrain IG\u2019s value in the range (0, 1], with a greater value indicating less novelty, which means an observation can be better encoded by the training set and the topic model generalizes better on this observation. The constant c in Eq. (5) provides the flexibility to encode the degree to which we consider an observation to be novel."}, {"heading": "4.3. Indicator Relationship", "text": "While the interpretability indicator interprets human activity distributions in a way that is similar to human reasoning, the generalizability indicator endows a co-robot with the self-reflection capability. We summarize their relationship in the cases when a training set is exhaustive (i.e., training contains all possible categories) and nonexhaustive (i.e., new human behavior occurs during testing), as follows:\nObservation (Relationship of IG and II ): LetWtrain be the training dataset used to train a topic model, and II and IG be the model\u2019s interpretability and generalizability indicators.\n\u2022 IfWtrain is exhaustive, then IG \u2192 1 and II is generally independent of IG.\n\u2022 IfWtrain is non-exhaustive, then IG takes values that are much smaller than 1;\nII also takes small values and is moderately to strongly correlated with IG.\nmodel can lead to better recognition performance. Intuitively, ifWtrain is non-exhaustive\nand a previously unseen observationw belongs to a novel category, which is indicated by a small IG value, a topic model trained onWtrain cannot accurately classifyw. On the other hand, if w belongs to a category that is known in Wtrain, then IG\u2192 1 and the recognition performance over w only depends on the model\u2019s performance on the validation set used in the training phase. The meaning and relationship of the indicators II and IG are summarized in Table 1, where the gray area denotes that it is generally impossible for a topic model to obtain a low generalizability but a high interpretability, as a model is never correct when presented with a novel activity."}, {"heading": "5. Self-Reflective Risk-Aware Decision Making", "text": "Another contribution of this research is a decision making framework that is capable of incorporating activity category distribution, robot self-reflection (enabled by the indicators), and co-robot action risk, which is realized in the module of Decision Making in Fig. 1. Our new self-reflective risk-aware decision making algorithm is presented in Algorithm 2.\nGiven the robot action set a = {a1, . . . , aS} and the human activity set z = {z1, . . . , zK}, an action-activity risk rij is defined as the amount of discomfort, interference, or harm that can be expected to occur during the time period if the robot takes a specific action ai,\u2200i \u2208 {1, . . . , S} in response to an observed human activity zj ,\u2200j \u2208 {1, . . . ,K}. While \u03b8 and IG are computed online, the risks r = {rij}S\u00d7K , with each element rij \u2208 [0, 100], are manually estimated off-line by domain experts and used as a prior in the decision making module. In practice, the amount of risk is\nAlgorithm 2: Self-reflective risk-aware decision making Input : w (observation),M (trained topic model), and N (decision making\nbipartite network)\nOutput: a? (Selected robot action with minimum risk)\n1: Estimate per-observation activity proportion \u03b8 of w; 2: Compute generalizability indicator IG(w); 3: for each robot action i = 1 to S do 4: Estimate activity-independent risk: rini = 1 K \u2211K j=1rij ;\n5: Calculate activity-dependent risk: rdei = \u2211K j=1(\u03b8j \u00b7 rij); 6: Combine activity-independent and dependent risks, and assign to per-observation action risk vector: ra(i) = (1\u2212 IG(w)) \u00b7 rini + IG(w) \u00b7 rdei ; 7: end 8: Select optimal robot action a? with minimum risk in ra; 9: return a?.\ncategorized into a small number of risk levels for simplicity\u2019s sake. To assign a value to rij , a risk level is first selected. Then, a risk value is determined within that risk level. As listed in Table 2, we define four risk levels with different risk value ranges in our application. We intentionally leave a five-point gap between critical risk and high risk to increase the separation of critical risk from high risk actions.\nA bipartite network N = {a, z, r} is proposed to graphically illustrate the risk matrix r of robot actions a associated with human activities z. In this network, vertices are divided into two disjoint sets a and z, such that every edge with a weight rij connects a vertex ai \u2208 a to a vertex zj \u2208 z. An example of such a bipartite network is depicted in Fig. 2 for assistive robotics applications. The bipartite network also has a tabular representation (for example, in Table 4). Given the bipartite network, for a new observationw, after \u03b8 and IG(w) are computed in the probabilistic reasoning module, the robot action a? \u2208 a is selected according to:\na?= argmin ai:i=1,...,S\n( 1\u2212IG(w)\nK \u00b7 K\u2211 j=1 rij + IG(w)\u00b7 K\u2211 j=1 (\u03b8j \u00b7rij)\n) (6)\nThe risk of taking a specific robot action is determined by two separate components: activity-independent and activity-dependent action risks. The activity-independent risk\n(that is 1K \u2211K j=1 rij) measures the inherent risk of an action, which is independent of the human activity context information, i.e., computing this risk does not require the category distribution. For example, the robot action \u201cstanding-by\u201d generally has a smaller risk than \u201cmoving backward\u201d in most situations. The activity-dependent risk\n(that is \u2211K j=1 (\u03b8j \u00b7rij)) is the average risk weighted by context-specific information (i.e., the human activity distribution). The combination of these two risks is controlled by IG, which automatically encodes preference over robot actions. When the learned model generalizes well over w, i.e., IG(w)\u2192 1, the decision making process prefers co-robot actions that are more appropriate to the recognized human activity. Otherwise, if the model generalizes poorly, indicating new human activities occur and the learned model is less applicable, our decision making module would ignore the recognition results and select co-robot actions with lower activity-independent risk."}, {"heading": "6. Experiments", "text": ""}, {"heading": "6.1. Datasets and Visual Features", "text": "We employ three real-world benchmark datasets to evaluate our cognitive model on HAR tasks, which are widely used in the machine vision community: the Weizmann\nactivity dataset [57], the KTH activity dataset [58], and the UTK 3D activity dataset [12]. Illustrative examples from each activity category in the datasets are depicted in Fig. 3.\nIn our experiments, we apply different types of local visual features to encode these datasets. For 2D datasets that contain only color videos (i.e., the Weizmann and KTH datasets), we use two different features: scale-invariant feature transform (SIFT) features [22] and space-time interest points (STIP) features [58]. For 3D datasets that contain both color and depth videos (i.e., the UTK dataset), we use the 4-dimensional local spatio-temporal features (4D-LSTF) [12].\nSIFT features are the most commonly applied local visual features and have de-\nsirable characteristics including invariance to transformation, rotation and scale, and robustness to partial occlusion [22]. We employ the algorithm and implementation in [22] to detect and describe SIFT features. A disadvantage of SIFT features in HAR tasks is that these features are extracted in a frame-by-frame fashion, i.e., SIFT features do not capture any temporal information. To encode time information, we also apply STIP along with the histogram of oriented gradients (HOG) and histogram of optical flow (HOF) descriptors [58]. These two types of features are extracted using only color or intensity information. Previous work has demonstrated that local features incorporating both depth and color information can greatly improve recognition accuracy [12]. Therefore, for the 3D UTK dataset we use 4D-LSTF [12] features, which are highly robust and distinct and are generated using both color and depth videos. It is also noteworthy that SIFT and STIP features can be directly extracted from color or depth videos in the 3D dataset.\nThese feature extraction algorithms generate a collection of feature vectors for each visual observation. Then, the feature vectors are clustered into discrete visual words using the k-means algorithm, and the number of clusters is set equal to the dictionary size. Lastly, each feature vector is indexed by a discrete word that represents cluster assignment. At this point, each observation is encoded by a BoW representation, which can be perceived by topic modeling. Although we only test the most widely used features, one should note that our artificial cognitive model is capable of incorporating different types of local visual features, since our reasoning and decision making process is independent of the features given their BoW representation."}, {"heading": "6.2. Activity Recognition", "text": "We first evaluate the SRAC model\u2019s capability to recognize human activities using the interpretability indicator II , when the training set is exhaustive. In this experiment, each dataset is split into disjoint training and testing sets. We randomly select 75% of data instances in each category as the training set, and employ the rest of the instances for testing. During training, fourfold cross-validation is used to estimate model parameters. Then, the interpretability of the topic model is computed using the testing set, which is fully represented by the training set and does not contain novel human\nactivities. This training-testing process is repeated five times to obtain reliable results.\nExperimental results of the interpretability and its standard deviation versus the dictionary size are illustrated in Fig. 4. Our SRAC model obtains promising recognition performance in terms of interpretability: 0.989 is obtained using the STIP feature and a dictionary size 1800 on the Weizmann dataset, 0.952 using the STIP feature and a dictionary size 2000 on the KTH dataset, and 0.936 using the 4D-LSTF feature and a dictionary size 1600 on the UTK3D dataset. In general, STIP features perform better than SIFT features for color data, and 4D-LSTF features perform the best for RGB-D visual data. The dictionary size in the range [1500, 2000] can generally result in satis-\nfactory human activity recognition performance. The results are also very consistent, as illustrated by the small error bars in Fig. 4, which demonstrates our interpretability indicator\u2019s consistency.\nThe model\u2019s interpretability is also evaluated over different activity categories using the UTK3D dataset, which includes more complex activities (i.e., sequential activities) and contains more information (i.e., depth). It is observed that topic modeling\u2019s interpretability varies for different activities. This performance variation is affected by three main factors: the topic model\u2019s modeling ability, feature and BoW\u2019s representability, and human activity complexity and similarity. For example, since the LDA topic model and SIFT features are not capable of modeling time, the reversal human activities including \u201clifting a box\u201d and \u201cremoving a box\u201d in the UTK3D dataset cannot be well distinguished, as illustrated in Fig. 5. Since sequential activities (e.g., \u201cremoving a box\u201d) are more complex than repetitive activities (e.g., \u201cwaving\u201d), they generally result in low interpretability. Since \u201cpushing\u201d and \u201cwalking\u201d are similar, which share motions such as moving forward, they can also reduce interpretability. This observation provides general guidance for designing future recognition systems with the SRAC model."}, {"heading": "6.3. Knowledge Discovery", "text": "We evaluate the SRAC model\u2019s capability to discover new situations using the generalizability indicator IG, when the training dataset is non-exhaustive (i.e., new human activities occur during testing). A non-exhausted setup is created by dividing the used benchmark datasets as follows. We place all data instances of one activity in the unknown testing set, and randomly select 25% of the instances from the remaining activities in the known testing set. The remaining instances are placed in the training set for learning, based on fourfold cross-validation. To evaluate the model\u2019s ability to discover each individual human activity, given a dataset that contains K activity categories, the experiments are repeatedK times, each using one category as the novel activity. Visual features that achieve the best model interpretability over each dataset are used in this set of experiments i.e., STIP features for the Weizmann and KTH datasets and 4D-LSTF features for the UTK3D dataset.\nVariations of Pvwp values versus the dictionary size over the validation set (in cross-validation), known testing set, and unknown testing set are shown in Fig. 6. Several important phenomena are observed. First, there exists a large Pvwp gap between the known and unknown testing sets, as shown by the gray area in the figure, indicating that topic models generalize differently over data instances from known and unknown activities. A better generalization result indicates a less novel instance, which can be better represented by the training set. Since data instances from the known testing and validation sets are well represented by the training set, the Pvwp gap between them is small. As shown in Fig. 6(a), it is possible that the known testing set\u2019s Pvwp value is greater than the Pvwp value of the validation set, if its data instances can be better represented by the training set. Second, Fig. 6 shows that the gap\u2019s width varies over different datasets: the Weizmann dataset generally has the largest Pvwp gap, followed by the KTH dataset, and then the UTK3D dataset. The gap\u2019s width mainly depends on the observation\u2019s novelty, in terms of the novel activity\u2019s similarity to the activities in the training dataset. This similarity is encoded by the portion of overlapping features. A more novel activity is generally represented by a set of more distinct visual features with less overlapping with the features existing during training, which generally results in a larger gap. For example, activities in the Weizamann dataset share fewer motions\nand thus contain a less number of overlapping features, which leads to a larger gap. Third, when the dictionary size increases, the model\u2019s Pvwp values decrease at a similar rate. This is because in this case, the probability of a specific codeword appearing in an instance decreases, resulting in a decreasing Pvwp value.\nThe generalizability indicator IG\u2019s characteristics are also empirically validated on the known and unknown testing sets, as illustrated in Fig. 7. An important characteristic of IG is its invariance to dictionary size. Because Pvwp over testing and validation sets has similar decreasing rate, the division operation in Eq. (5) removes the variance to dictionary size. In addition, a more novel activity generally leads to a smaller IG value. For example, the Weizmann dataset has a smaller IG value over the unknown testing set, because its activities are more novel in the sense that they share less overlapping motions. In general, we observe IG is smaller than 0.5 for unknown activities\nand greater than 0.7 for activities that are included in training sets. As indicated by the gray area in Fig. 7, similar to Pvwp, there exists a large gap between the IG values over the unknown and known testing datasets. The average IG gap across different dictionary sizes is 0.69 for the Weizmann dataset, 0.48 for the KTH dataset, and 0.36 for the UTK3D dataset. This reasoning process, based on IG, provides a co-robot with the critical self-reflection capability, and allows a robot to reason about when new situations occur as well as when the learned model becomes less applicable.\nWe have pointed out that the indicator IG is heavily affected by the novelty of an activity in terms of its proportion of overlapping features. To validate this conclusion, we generate a synthetic dataset by manually controlling the proportion of overlapping visual words in the testing instances. In order to make the characteristics of the synthetic dataset as close as possible to real-world datasets, features used in the simulation are\nborrowed from the KTH dataset. Instances of two activities (i.e., \u201cbending\u201d and \u201cwaving2\u201d) are used to train a topic model, which is then applied as a classifier to perform recognition in this experiment. This topic model is also applied to generate overlapping visual words for a testing instance. Another topic model, whose parameters are learned using activities \u201csiding\u201d and \u201cjacking\u201d, is used to generate non-overlapping words in the testing instance. A dictionary of size 1800 is adopted, which is created using the visual words of the KTH dataset. The used four activities contain 906 unique visual words, with each pair of activities sharing less than 1% overlapping words. We generate 50 instances for each testing set, with the number of words in each instance set to 112, which is the average number of visual words in real-world instances. We present the results of five simulations in Fig. 8, which clearly shows that, in general, IG\u2019s value over testing instances increases linearly with the percentage of features that overlap with the features of known activities in the training set."}, {"heading": "6.4. Relationship of IG and II", "text": "Here, we empirically analyze the relationship between the interpretability and generalizability indicators. We first validate the correlation of II and IG. In addition, we investigate additional relationships of II and IG, such as the probability that II \u2264 IG.\nWhile we are able to employ the exhaustive experimental setup from Section 6.2\nto analyze IG and II \u2019s relationship when testing instances are fully represented by the training set, unfortunately, we cannot use the non-exhaustive setup in Section 6.3 to validate this relationship in cases where IG takes small values. This is because ground truth cannot be assigned to instances belonging to novel activities to compute II , since these activities only exist in the testing set and are not presented to our model during the training phase. Inspired by the method used to generate synthetic data in Section 6.3, we adopt a semi-exhaustive experimental setup by replacing certain portions of words in each testing instance with visual words from novel activities. This experimental setup is used to validate the indicators\u2019 relationship when the training set cannot fully represent testing instances.\nEach experiment is performed using F folds, where F is the number of activities in a dataset. In each fold, we take all instances of one activity out from the dataset, which is treated as a novel activity that is not presented to the topic model in the learning phase. Then, we randomly select 75% of the instances of the remaining activities as training set, which is further divided into training and validation sets to perform fourfold cross-validation. The rest of the instances are used as an \u201cinitial\u201d testing set. During the testing phase, the novel activity\u2019s word distribution is used to generate new visual words to replace a proportion of the words in each instance in the initial testing set. This testing is performed six times within each of the F folds using different replacement rates (i.e., 0.25, 0.35, . . . , 0.75). Testing results from all F folds are used to investigate II and IG\u2019s relationship. In this experimental setup, we use features that achieve the best interpretability over each dataset. In addition, we set the dictionary size to 1600, which achieves the best interpretability over all datasets in general.\nThis experimental setup is semi-exhaustive in the sense that, although training data cannot fully represent testing instances due to the replaced features that are generated from unknown activities, the remaining non-replaced features are presented to the model during the learning phase, and the ground truth assigned to each testing instance remains the same, which is also known to the model. It is noteworthy that we do not use very high or very low replacement rates. A very low replacement rate makes the experimental setup equivalent to the exhaustive setup. When using a very high replacement rate, testing instances can be viewed as being drawn from the novel activity; in\nthis case the ground truth associated with a testing instance would be meaningless or incorrect.\nWe empirically analyze the correlation between II and IG, using both exhaustive and semi-exhaustive datasets, in order to determine whether better generalizability indicates better interpretability. The Pearson correlation coefficient is used to measure the strength and direction of the linear relationship between these two indicators. Given a datasetW = {w1, . . . ,w|W|} and its ground truth g = {g1, . . . , g|W|}, this correlation is mathematically defined as follows:\n\u03c1I,G = E[(II \u2212 \u00b5II )(IG \u2212 \u00b5IG)]\n\u03c3II\u03c3IG , (7)\nwhere IG = {IG(w1), . . . , IG(w|W|)} and II = {II(w1, g1), . . . , II(w|W|, g|W|)} are vectors of interpretability and generalizability indicators for all of the instances in the dataset, \u00b5 is the mean and \u03c3 is the standard deviation of the indicators in the vector. Our experimental results are listed in Table 3. For the exhaustive dataset, topic models which perform better on generalizability are not necessarily better interpreted, which is indicated by the weak linear correlation between the indicators. This is because, when testing on exhaustive datasets, IG takes values closer to 1. But the model\u2019s interpretability takes a wide range of values, depending on the model\u2019s modeling capacity, feature representability and dataset complexity, as explained in Section 6.2. For semiexhaustive datasets, II and IG are moderately to strongly correlated, which indicates that a poor generalizability usually leads to a poor interpretability. Since IG reflects the novelty of an instance as discussed in Section 6.3, a low IG\u2019s value means the instance is badly represented by the training set. Therefore, the trained model cannot obtain a\ngood interpretability over the instance of an activity that is not well represented during the training phase.\nWe also check an additional relationship, i.e., the probability that II is smaller than or equal to IG. Given a labeled dataset W = {w1, . . . ,wM} and its ground truth g = {g1, . . . , gM}, this probability is defined as follows:\nPII\u2264IG = 1\nM M\u2211 m=1 1(II(wm, gm) \u2264 IG(wm)). (8)\nThe experimental results are presented in Table 3. One of the most important observations is that, for a majority of testing instances (more than 85%) in the semi-exhaustive experiment, IG\u2019s value is greater than II \u2019s value. This again shows that a poor generalizability usually indicates a poor interpretability. Using the exhaustive experimental setup, it is more probable that IG takes smaller values than II . This is because when the training set is exhaustive, the topic model is well trained and can well recognize testing instances, which leads to II\u2192 1 for most of testing instances. On the other hand, although IG also takes a large value in general, it is usually slightly smaller than one, because features in testing instances usually do not completely overlap with features in training instances."}, {"heading": "6.5. Decision Making", "text": "We assess our SRAC model\u2019s decision making capability using a Turtlebot 2 robot in a human following task, which is important in many human-robot teaming applications. In this task, a robotic follower needs to decide at what distance to follow the human teammate. We are interested in three human behaviors: \u201cwalking\u201d in a straight line, \u201cturning\u201d, and \u201cfalling\u201d, shown in Fig. 9. With perfect perception and reasoning, i.e., a robot always perfectly interprets human activities, we assume the ideal robot actions are to \u201cstay far from the human\u201d when he or she is walking in a straight line (to not interrupt the human), \u201cmove close to the human\u201d when the subject is turning (to avoid losing the target), and \u201cstop beside the human\u201d when he or she is falling (to provide assistance).\nIn order to qualitatively assess the performance, we collect 20 color-depth instances from each human behaviors to train the SRAC model, using a BoW representation\nbased on 4D-LSTF features. The risk matrix used in this task is presented in Table 4. We evaluate our model in two circumstances. Case1: exhaustive training (i.e., no unseen human behaviors occur in testing). In this case, the subjects only perform the three activities during testing with small variations in motion speed and style. Case2: non-exhaustive training (i.e., novel movements occur during testing). In this case, the subjects not only perform the activities with large variations, but also add additional movements (such as jumping and squatting) which are not observed in the training phase. During testing, each activity is performed 40 times. The model performance is measured using failure rate, i.e., the percentage with which the robot fails to stop besides to help the human or loses the target.\nExperimental results are presented in Table 5, where the traditional methodology, which selects the co-robot actions only based on the most probable human activity, is used as a baseline for comparison. We observe that the proposed SRAC model significantly decreases the failure rate in both exhaustive and non-exhaustive setups. When\nthe training set is exhaustive and no new activities occur during testing (Case1), the results demonstrate that incorporating human activity distributions and robot action risks improves decision making performance. When the training set is non-exhaustive and new activities occur during testing (Case2), the SRAC model significantly outperforms the baseline model. In this situation, if IG has a very small value, according to Eq. 6, our model tends to select safer robot actions, i.e., \u201cstay beside humans,\u201d since its average risk is the lowest, which is similar to the human common practice \u201cplaying it safe in uncertain times.\u201d The results show the importance of self-reflection for decision making especially under uncertainty."}, {"heading": "7. Conclusion", "text": "In this paper, we construct an artificial cognitive model that provides co-robots with both accurate perception and new information discovery capabilities, which enables safe, reliable robot decision making for HAR tasks in human-robot interaction applications.\nThe proposed SRAC model exploits topic modeling, which is unsupervised and allows for the discovery of new knowledge without presence in training. In addition, topic modeling is also able to treat activity estimation as a distribution and incorporate risks for each action response, which is beneficial for the system\u2019s ability to make decisions. In order to provide the capability of accurate human activity interpretation, we define a new interpretability indicator (II ) and demonstrate its ability to enable a robot to interpret category distribution in a similar fashion to humans. The indicator II is applied to map detected clusters to known activity categories and select the best\ninterpreted model. In addition, to provide the ability of knowledge discovery, we introduce a novel generalizability indicator (IG). It measures how well an observation can be represented by the learned knowledge, which allows for self-reflection that can enable the SRAC model to identify new scenarios.\nWe applied the proposed SRAC model in extensive experiments to demonstrate the effectiveness using both synthetic and real-world datasets. We show that our model performs extremely well in terms of interpretability; that is, our model\u2019s recognition results closely and consistently match human common sense. We demonstrate that, using IG, our cognitive model is capable of discovering new knowledge, i.e., observations from new activity categories that are not considered in the training phase can be automatically detected. We also examine the relationship between II and IG and show, both analytically and experimentally, that IG can also be used as an indicator for II . The results reveal that scenarios with a low IG score for an observation will equate to a low II score with high confidence, i.e., a badly generalized model is likely to be inaccurate. We further demonstrate the advantages of using distributions over activity categories, as well as the importance of the evaluation metrics in order to create a system capable of safe, reliable decision making."}, {"heading": "Appendix A. Proof of II \u2019s Properties (Proposition 1)", "text": "If denominator in Definition 1 is 0, then limit is used. Given the normalizing con-\nstants a = 2 and b = 1: 1. If k = 1, II(\u03b8s, k) = 1a ( 1 + b\u2212 \u03b82\u03b81 ) = 1 \u2212 \u03b822\u03b81 . Since \u03b8s is decreasingly sorted, satisfying \u03b81 \u2265 \u03b82 \u2265 0, then \u2212 \u03b822\u03b81 \u2265 \u22120.5. Thus, II(\u03b8s, k) = 1\u2212 \u03b82 2\u03b81 \u2265 0.5\n2. If k = K, II(\u03b8s, k) = 1a ( \u03b8K \u03b81 \u2212 1 + b ) = \u03b8K2\u03b81 Since \u03b8s is decreasingly sorted,\nsatisfying \u03b81 \u2265 \u03b8K \u2265 0, then \u03b8K\u03b81 \u2264 1. Thus, II(\u03b8s, k) = \u03b8K 2\u03b81 \u2264 0.5.\n3. First, we prove II(\u03b8s, k) \u2265 0. Since K \u2265 k > 0 and K \u2265 2, the second multiplier F2 = K\u2212kK\u22121 + 1(k = K) > 0. Given b = 1, the third multiplier satisfies F3 = \u03b8k \u03b81 \u2212 \u03b8k+1(k 6=K)\u03b8k + b = \u03b82k+\u03b81(\u03b8k\u2212\u03b8k+1(k 6=K)) \u03b81\u03b8k . Since \u03b8s is decreasingly sorted, then \u03b81 \u2265 \u03b8k \u2265 \u03b8k+1(k=K) \u2265 0. Thus, F3 \u2265 0. Equality is obtained when \u03b8k = \u03b8k+1(k=K) = 0. Since a > 0, F2 > 0 and F3 \u2265 0, then II(\u03b8s, k) = 1a \u00b7 F2 \u00b7 F3 \u2265 0.\nNow, we prove II(\u03b8s, k) \u2264 1. When k = K, by property 2, II(\u03b8s, k) \u2264 1 directly holds. If K > k \u2265 1, then F2 = K\u2212kK\u22121 \u2264 1. Equality holds when k = 1. Since \u03b8s is decreasingly sorted, satisfying \u03b81 \u2265 \u03b8k \u2265 \u03b8k+1 \u2265 0, then \u03b8k\u03b81 \u2264 1 and \u03b8k+1 \u03b8k \u2265 0. Given b = 1, we have F3 = \u03b8k\u03b81 \u2212 \u03b8k+1 \u03b8k\n+b \u2264 \u03b8k\u03b81 +1 \u2264 2. Equality holds when \u03b8k = \u03b81 and \u03b8k+1 = 0. Thus, given a = 2, we obtain II(\u03b8s, k) = 1a \u00b7 F2 \u00b7 F3 \u2264 1. Thus, \u2200\u03b8, II(\u03b8s, k) \u2208 [0, 1] holds.\n4. Since \u2200k \u2208 {1, \u00b7 \u00b7 \u00b7 ,K}, \u03b8s, \u03b8\u2032s satisfy \u03b8k = \u03b8\u2032k and \u03b8k+l(k=K) = \u03b8\u2032k+l(k=K), we obtain II(\u03b8s, k) \u2212 II(\u03b8\u2032s, k) = 1a ( K\u2212k K\u22121 + l(k = K) )( \u03b8k \u03b81\u03b8\u20321 (\u03b8\u20321 \u2212 \u03b81) )\n. Since K\u2212k K\u22121 + l(k = K) > 0 and \u03b8 \u2032 1 \u2265 \u03b81, Then, II(\u03b8s, k) \u2212 II(\u03b8 \u2032 s, k) \u2264 0. Equality holds if \u03b8\u20321 = \u03b81 or \u03b8k = 0. Thus, II(\u03b8s, k) \u2264 II(\u03b8 \u2032 s, k).\n5. Since \u2200k \u2208 {1, \u00b7 \u00b7 \u00b7 ,K}, \u03b8s, \u03b8\u2032s satisfy \u03b81 = \u03b8\u20321 and \u03b8k = \u03b8\u2032k, we obtain II(\u03b8s, k)\u2212II(\u03b8\u2032s, k) = 1a ( K\u2212k K\u22121 + l(k = K) )( 1 \u03b8k (\u03b8\u2032k+l(k=K) \u2212 \u03b8k+l(k=K)) )\n. Since K\u2212k K\u22121 + l(k = K) > 0 and \u03b8k+l(k=K) \u2265 \u03b8 \u2032 k+l(k=K), Then, II(\u03b8s, k)\u2212 II(\u03b8 \u2032 s, k) \u2264 0. Equality holds if \u03b8k+l(k=K) = \u03b8\u2032k+l(k=K). Thus, II(\u03b8s, k) \u2264 II(\u03b8 \u2032 s, k) holds.\n6. Since \u2200k \u2208 {1, \u00b7 \u00b7 \u00b7 ,K}, \u03b8s, \u03b8\u2032s satisfy \u03b8k = \u03b8\u2032k and \u03b8k+l(k=K) = \u03b8\u2032k+l(k=K), we obtain II(\u03b8s, k)\u2212II(\u03b8\u2032s, k) = 1a ( K\u2212k K\u22121 + l(k = K) )( 1 \u03b81 + \u03b8k+l(k=K) \u03b8k\u03b8\u2032k ) (\u03b8k\u2212\u03b8\u2032k). Since K\u2212kK\u22121 + l(k = K) > 0 and 1 \u03b81 + \u03b8k+l(k=K) \u03b8k\u03b8\u2032k \u2265 0, and \u03b8k > \u03b8\u2032k, Then, II(\u03b8s, k)\u2212 II(\u03b8 \u2032 s, k) \u2265 0. Equality holds if \u03b8k = \u03b8\u2032k. Thus, II(\u03b8s, k) \u2265 II(\u03b8 \u2032 s, k) holds.\n7. \u2200k, k\u2032 \u2208 {1, . . . ,K} satisfying k \u2264 k\u2032 < K, and \u2200\u03b8s, \u03b8\u2032s satisfying \u03b8k+1 =\n\u03b8\u2032k\u2032+1, \u03b81 = \u03b8 \u2032 1 and \u03b8k = \u03b8 \u2032 k\u2032 , we obtain II(\u03b8s, k)\u2212II(\u03b8 \u2032 s, k \u2032) = 1a(K\u22121) ( \u03b8k \u03b81 \u2212 \u03b8k+1\u03b8k + b ) (k\u2032 \u2212 k). Since K > 1, \u03b8k\u03b81 \u2212 \u03b8k+1 \u03b8k + b \u2265 0, and k\u2032 \u2265 k, II(\u03b8s, k) \u2212 II(\u03b8\u2032s, k\u2032) \u2265 0, with equality holding when \u03b8k = \u03b8k+1 = 0 or k = k\u2032. Thus, II(\u03b8s, k) \u2265 II(\u03b8\u2032s, k\u2032) holds."}, {"heading": "Appendix B. Proof of the Relationship between II and IA (Proposition 2)", "text": "Given an observation w, the accuracy metric IA indicates whether the recognition\nresult y(w) matches the ground truth g. Formally, IA is defined as follows:\nIA(y(w), g) = l(y(w) = g). (B.1)\nWith this definition, we prove that IA is a special case of our II indicator in Defi-\nnition 1 when \u03b81 = 1.0, \u03b82 = . . .= \u03b8K = 0, and k = 1 or k = K.\nGiven the normalizing constants a = 2 and b = 1, when \u03b81 = 1.0, \u03b82 = . . . = \u03b8K = 0, and k = 1 (i.e., the recognition result y(w) matches the ground truth g), we obtain:\nIs(\u03b8s, 1) = 1\na ( K \u2212 1 K \u2212 1 + 0 )( \u03b81 \u03b81 \u2212 \u03b82 \u03b81 + b ) = b+ 1 a = 1.\nWhen k = K (i.e., y(w) 6= g), we obtain:\nIs(\u03b8s,K)= 1\na ( K\u22121 K\u22121 +1 )( \u03b8K \u03b81 \u2212 \u03b8K \u03b8K +b ) = 2(b\u22121) a = 0.\nCombining both cases, we obtain:\nIs(\u03b8s, k) = 1 if k = 1 (i.e., y(w) = g)0 if k = K (i.e., y(w) 6= g) = l(y(w) = g). (B.2)\nWe observe Eq. (B.1) is equivalent to Eq. (B.2), and thereby prove that IA is a special case of the II indicator in the cases when \u03b81 = 1.0, \u03b82 = . . .= \u03b8K = 0, and k = 1 or k = K."}], "references": [{"title": "Understanding Origins", "author": ["F.J. Varela", "J. Dupuy"], "venue": "Kluwer Academic Publishers", "citeRegEx": "1", "shortCiteRegEx": null, "year": 1992}, {"title": "A survey of artificial cognitive systems: Implications for the autonomous development of mental capabilities in computational agents", "author": ["D. Vernon", "G. Metta", "G. Sandini"], "venue": "IEEE Transactions on Evolutionary Computation 11 (2) ", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2007}, {"title": "ACT: A simple theory of complex cognition", "author": ["J.R. Anderson"], "venue": "American Psychologist 51 ", "citeRegEx": "3", "shortCiteRegEx": null, "year": 1996}, {"title": "ACT- R/E: An embodied cognitive architecture for human-robot interaction", "author": ["G. Trafton", "L. Hiatt", "A. Harrison", "F. Tamborello", "S. Khemlani", "A. Schultz"], "venue": "Journal of Human-Robot Interaction 2 (1) ", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2013}, {"title": "ACT-R\u03a6: A cognitive architecture with physiology and affect", "author": ["C.L. Dancy"], "venue": "Biologically Inspired Cognitive Architectures 6 ", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2013}, {"title": "The Soar cognitive architecture", "author": ["J. Laird"], "venue": "MIT Press", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2012}, {"title": "A layered brain architecture for synthetic creatures", "author": ["D. Isla", "R. Burke", "M. Downie", "B. Blumberg"], "venue": "in: International Joint Conferences on Artificial Intelligence", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2001}, {"title": "A cognitive architecture for a humanoid robot: a first approach", "author": ["C. Burghart", "R. Mikut", "R. Stiefelhagen", "T. Asfour", "H. Holzapfel", "P. Steinhaus", "R. Dillmann"], "venue": "in: IEEE-RAS International Conference on Humanoid Robots", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2005}, {"title": "The challenge of complexity for cognitive systems", "author": ["U. Schmid", "M. Ragni", "C. Gonzalez", "J. Funke"], "venue": "Cognitive Systems Research 12 (3-4) ", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2011}, {"title": "Learning hierarchical invariant spatiotemporal features for action recognition with independent subspace analysis", "author": ["Q.V. Le", "W.Y. Zou", "S.Y. Yeung", "A.Y. Ng"], "venue": "in: IEEE Conference on Computer Vision and Pattern Recognition", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2011}, {"title": "FREAK: Fast retina keypoint", "author": ["A. Alahi", "R. Ortiz", "P. Vandergheynst"], "venue": "in: IEEE Conference on Computer Vision and Pattern Recognition", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2012}, {"title": "L", "author": ["H. Zhang"], "venue": "E. Parker, 4-dimensional local spatio-temporal features for human activity recognition., in: IEEE/RSJ International Conference on Intelligent Robots and Systems", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2011}, {"title": "Autonomous adaptive exploration using realtime online spatiotemporal topic modeling", "author": ["Y. Girdhar", "P. Giguere", "G. Dudek"], "venue": "International Journal of Robotics Research 33 (4) ", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2013}, {"title": "Unsupervised learning of human action categories using spatial-temporal words", "author": ["J.C. Niebles", "H. Wang", "L. Fei-Fei"], "venue": "International Journal of Computer Vision 79 (3) ", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2008}, {"title": "Human activity analysis: A review", "author": ["J. Aggarwal", "M. Ryoo"], "venue": "ACM Computing Surveys 43 (3) ", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2011}, {"title": "Video-based human behavior understanding: a survey", "author": ["P.V.K. Borges", "N. Conci", "A. Cavallaro"], "venue": "IEEE Transactions on Circuits and Systems for Video Technology 23 (11) ", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2013}, {"title": "Extraction method of gait feature based on human centroid trajectory", "author": ["X. Chen", "T. Yang"], "venue": "in: Computer Engineering and Networking", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2014}, {"title": "Vision-based analysis of small groups in pedestrian crowds", "author": ["W. Ge", "R.T. Collins", "R.B. Ruback"], "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence 34 (5) ", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2012}, {"title": "Human activity recognition based on silhouette directionality", "author": ["M. Singh", "A. Basu", "M. Mandal"], "venue": "IEEE Transactions on Circuits and Systems for Video Technology 18 (9) ", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2008}, {"title": "Silhouette-based human action recognition using SAX-Shapes", "author": ["I.N. Junejo", "K.N. Junejo", "Z. Al Aghbari"], "venue": "The Visual Computer 30 (3) ", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2014}, {"title": "Bio-inspired predictive orientation decomposition of skeleton trajectories for real-time human activity prediction", "author": ["H. Zhang", "L.E. Parker"], "venue": "in: IEEE International Conference on Robotics and Automation", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2015}, {"title": "Distinctive image features from scale-invariant keypoints", "author": ["D.G. Lowe"], "venue": "International Journal of Computer Vision 60 (2) ", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2004}, {"title": "Action recognition via local descriptors and holistic features", "author": ["X. Sun", "M.Y. Chen", "A. Hauptmann"], "venue": "in: IEEE Conference on Computer Vision and Pattern Recognition", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2009}, {"title": "Real-time activity recognition by discerning qualitative relationships between randomly chosen visual features", "author": ["A. Behera", "A.G. Cohn", "D.C. Hogg"], "venue": "in: British Machine Vision Conference", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2014}, {"title": "Recognizing human actions: A local SVM approach", "author": ["C. Schuldt", "I. Laptev", "B. Caputo"], "venue": "in: International Conference on Pattern Recognition", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2004}, {"title": "Behavior recognition via sparse spatio-temporal features", "author": ["P. Doll\u00e1r", "V. Rabaud", "G. Cottrell", "S. Belongie"], "venue": "in: IEEE International Workshop on Visual Surveillance and Performance Evaluation of Tracking and Surveillance", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2005}, {"title": "Relevance topic model for unstructured social group activity recognition", "author": ["F. Zhao", "Y. Huang", "L. Wang", "T. Tan"], "venue": "in: Advances in Neural Information Processing Systems", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2013}, {"title": "Human action recognition by semilatent topic models", "author": ["Y. Wang", "G. Mori"], "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence 31 (10) ", "citeRegEx": "28", "shortCiteRegEx": null, "year": 2009}, {"title": "Discovery of activity patterns using topic models", "author": ["T. Huynh", "M. Fritz", "B. Schiele"], "venue": "in: International Conference on Ubiquitous Computing", "citeRegEx": "29", "shortCiteRegEx": null, "year": 2008}, {"title": "Discovering routines from large-scale human locations using probabilistic topic models", "author": ["K. Farrahi", "D. Gatica-Perez"], "venue": "ACM Transactions on Intelligent Systems and Technology 2 (1) ", "citeRegEx": "30", "shortCiteRegEx": null, "year": 2011}, {"title": "Temporal and object relations in plan and activity recognition for robots using topic models", "author": ["R.G. Freedman", "H.-T. Jung", "S. Zilberstein"], "venue": "in: AAAI Fall Symposium Series", "citeRegEx": "31", "shortCiteRegEx": null, "year": 2014}, {"title": "Evaluation methods for topic models", "author": ["H. Wallach", "I. Murray", "R. Salakhutdinov", "D. Mimno"], "venue": "in: International Conference on Machine Learning", "citeRegEx": "32", "shortCiteRegEx": null, "year": 2009}, {"title": "LDA-based document models for ad-hoc retrieval", "author": ["X. Wei", "W.B. Croft"], "venue": "in: Interational Conference on Research and Development in Information Retrieval", "citeRegEx": "33", "shortCiteRegEx": null, "year": 2006}, {"title": "Reading tea leaves: How humans interpret topic models", "author": ["J. Chang", "J. Boyd-Graber", "S. Gerrish", "C. Wang", "D. Blei"], "venue": "in: Neural Information Processing Systems", "citeRegEx": "34", "shortCiteRegEx": null, "year": 2009}, {"title": "Automatic evaluation of topic coherence", "author": ["D. Newman", "J.H. Lau", "K. Grieser", "T. Baldwin"], "venue": "in: Human Language Technologies: The Annual Conference of the North American Chapter of the Association for Computational Linguistics", "citeRegEx": "35", "shortCiteRegEx": null, "year": 2010}, {"title": "Probabilistic topic models", "author": ["D.M. Blei"], "venue": "Communications of the ACM 55 (4) ", "citeRegEx": "36", "shortCiteRegEx": null, "year": 2012}, {"title": "Introduction to this special issue on cognitive architectures and human-computer interaction", "author": ["W.D. Gray", "R.M. Young", "S.S. Kirschenbaum"], "venue": "Human-Computer Interaction 12 (4) ", "citeRegEx": "37", "shortCiteRegEx": null, "year": 1997}, {"title": "Integrating perceptual and cognitive modeling for adaptive and intelligent human-computer interaction", "author": ["Z. Duric", "W. Gray", "R. Heishman", "F. Li", "A. Rosenfeld", "M. Schoelles", "C. Schunn", "H. Wechsler"], "venue": "Proceedings of the IEEE 90 (7) ", "citeRegEx": "38", "shortCiteRegEx": null, "year": 2002}, {"title": "EFAA: a companion emerges from integrating a layered cognitive architecture", "author": ["S. Lall\u00e9e", "V. Vouloutsi", "S. Wierenga", "U. Pattacini", "P. Verschure"], "venue": "in: ACM/IEEE International Conference on Human-robot Interaction", "citeRegEx": "39", "shortCiteRegEx": null, "year": 2014}, {"title": "J", "author": ["P.E. Baxter"], "venue": "de Greeff, T. Belpaeme, Cognitive architecture for human\u2013robot interaction: towards behavioural alignment, Biologically Inspired Cognitive Architectures 6 ", "citeRegEx": "40", "shortCiteRegEx": null, "year": 2013}, {"title": "C-HMAX: Artificial cognitive model inspired by the color vision mechanism of the human brain", "author": ["B. Yang", "L. Zhou", "Z. Deng"], "venue": "Tsinghua Science and Technology 18 (1) ", "citeRegEx": "41", "shortCiteRegEx": null, "year": 2013}, {"title": "ACT-CV: Bridging the gap between cognitive models and the outer world", "author": ["M. Halbr\u00fcgge"], "venue": "Grundlagen und anwendungen der mensch-maschine-interaktion ", "citeRegEx": "42", "shortCiteRegEx": null, "year": 2013}, {"title": "Steps toward a cognitive vision system", "author": ["H.-H. Nagel"], "venue": "AI Magine 25 (2) ", "citeRegEx": "43", "shortCiteRegEx": null, "year": 2004}, {"title": "Interactive scene prediction for automotive applications", "author": ["A. Lawitzky", "D. Althoff", "C.F. Passenberg", "G. Tanzmeister", "D. Wollherr", "M. Buss"], "venue": "in: Intelligent Vehicles Symposium", "citeRegEx": "44", "shortCiteRegEx": null, "year": 2013}, {"title": "Things that see: Context-aware multi-modal interaction", "author": ["J.L. Crowley"], "venue": "in: Cognitive Vision Systems,", "citeRegEx": "45", "shortCiteRegEx": "45", "year": 2006}, {"title": "Learning situation models in a smart home", "author": ["O. Brdiczka", "J.L. Crowley", "P. Reignier"], "venue": "IEEE Transactions on Systems, Man, and Cybernetics, Part B: Cybernetics 39 (1) ", "citeRegEx": "46", "shortCiteRegEx": null, "year": 2009}, {"title": "Probabilistic models of cognitive development: Towards a rational constructivist approach to the study of learning and development", "author": ["F. Xu", "T.L. Griffiths"], "venue": "Cognition 120 ", "citeRegEx": "47", "shortCiteRegEx": null, "year": 2011}, {"title": "Dynamic decision networks for decisionmaking in self-adaptive systems: A case study", "author": ["N. Bencomo", "A. Belaggoun", "V. Issarny"], "venue": "in: International Symposium on Software Engineering for Adaptive and Self-Managing Systems", "citeRegEx": "48", "shortCiteRegEx": null, "year": 2013}, {"title": "Dynamic Bayesian networks for vehicle classification in video", "author": ["M. Kafai", "B. Bhanu"], "venue": "IEEE Transactions on Industrial Informatics 8 (1) ", "citeRegEx": "49", "shortCiteRegEx": null, "year": 2012}, {"title": "Probabilistic models of cognition: where next", "author": ["N. Chater", "J.B. Tenenbaum", "A. Yuille"], "venue": "Trends in Cognitive Sciences 10 (7) ", "citeRegEx": "50", "shortCiteRegEx": null, "year": 2006}, {"title": "Latent dirichlet allocation", "author": ["D.M. Blei", "A.Y. Ng", "M.I. Jordan"], "venue": "Journal of Machine Learning Research 3 ", "citeRegEx": "51", "shortCiteRegEx": null, "year": 2003}, {"title": "Finding scientific topics", "author": ["T.L. Griffiths", "M. Steyvers"], "venue": "in: National Academy of Sciences", "citeRegEx": "52", "shortCiteRegEx": null, "year": 2004}, {"title": "Fast collapsed Gibbs sampling for latent dirichlet allocation", "author": ["I. Porteous", "D. Newman", "A. Ihler", "A. Asuncion", "P. Smyth", "M. Welling"], "venue": "in: ACM SIGKDD International Conference on Knowledge Discovery and Data Mining", "citeRegEx": "53", "shortCiteRegEx": null, "year": 2008}, {"title": "Improving topic evaluation using conceptual knowledge", "author": ["C.C. Musat", "J. Velcin", "S. Trausan-Matu", "M.-A. Rizoiu"], "venue": "in: International Joint Conference on Artificial Intelligence", "citeRegEx": "54", "shortCiteRegEx": null, "year": 2011}, {"title": "Correlated topic models", "author": ["D. Blei", "J. Lafferty"], "venue": "in: Neural Information Processing Systems", "citeRegEx": "55", "shortCiteRegEx": null, "year": 2006}, {"title": "Bayesian nonexhaustive learning for online discovery and modeling of emerging classes", "author": ["M. Dundar", "F. Akova", "A. Qi", "B. Rajwa"], "venue": "in: International Conference on Machine Learning", "citeRegEx": "56", "shortCiteRegEx": null, "year": 2012}, {"title": "Actions as space-time shapes", "author": ["L. Gorelick", "M. Blank", "E. Shechtman", "M. Irani", "R. Basri"], "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence 29 (12) ", "citeRegEx": "57", "shortCiteRegEx": null, "year": 2007}, {"title": "On space-time interest points", "author": ["I. Laptev"], "venue": "International Journal of Computer Vision 64 (2-3) ", "citeRegEx": "58", "shortCiteRegEx": null, "year": 2005}], "referenceMentions": [{"referenceID": 0, "context": "Artificial cognition has its origin in cybernetics; its intention is to create a science of mind based on logic [1].", "startOffset": 112, "endOffset": 115}, {"referenceID": 1, "context": "Among other mechanisms, cognitivism is a most widely used cognitive paradigm [2].", "startOffset": 77, "endOffset": 80}, {"referenceID": 2, "context": "Several cognitive architectures were developed within this paradigm, including ACT-R [3] (and its extensions ACT-R/E [4], ACT-R\u03a6 [5], etc), Soar [6], C4 [7], and architectures for robotics [8].", "startOffset": 85, "endOffset": 88}, {"referenceID": 3, "context": "Several cognitive architectures were developed within this paradigm, including ACT-R [3] (and its extensions ACT-R/E [4], ACT-R\u03a6 [5], etc), Soar [6], C4 [7], and architectures for robotics [8].", "startOffset": 117, "endOffset": 120}, {"referenceID": 4, "context": "Several cognitive architectures were developed within this paradigm, including ACT-R [3] (and its extensions ACT-R/E [4], ACT-R\u03a6 [5], etc), Soar [6], C4 [7], and architectures for robotics [8].", "startOffset": 129, "endOffset": 132}, {"referenceID": 5, "context": "Several cognitive architectures were developed within this paradigm, including ACT-R [3] (and its extensions ACT-R/E [4], ACT-R\u03a6 [5], etc), Soar [6], C4 [7], and architectures for robotics [8].", "startOffset": 145, "endOffset": 148}, {"referenceID": 6, "context": "Several cognitive architectures were developed within this paradigm, including ACT-R [3] (and its extensions ACT-R/E [4], ACT-R\u03a6 [5], etc), Soar [6], C4 [7], and architectures for robotics [8].", "startOffset": 153, "endOffset": 156}, {"referenceID": 7, "context": "Several cognitive architectures were developed within this paradigm, including ACT-R [3] (and its extensions ACT-R/E [4], ACT-R\u03a6 [5], etc), Soar [6], C4 [7], and architectures for robotics [8].", "startOffset": 189, "endOffset": 192}, {"referenceID": 1, "context": "The combination of the cognitive architecture and components is usually referred to as a cognitive model [2].", "startOffset": 105, "endOffset": 108}, {"referenceID": 8, "context": ", perception, under significant uncertainty in a complex environment [9].", "startOffset": 69, "endOffset": 72}, {"referenceID": 9, "context": "This perception uncertainty is addressed in this work using the bag-of-visual-words (BoW) representation based on local spatio-temporal features, which has previously shown promising performance [10, 11, 12].", "startOffset": 195, "endOffset": 207}, {"referenceID": 10, "context": "This perception uncertainty is addressed in this work using the bag-of-visual-words (BoW) representation based on local spatio-temporal features, which has previously shown promising performance [10, 11, 12].", "startOffset": 195, "endOffset": 207}, {"referenceID": 11, "context": "This perception uncertainty is addressed in this work using the bag-of-visual-words (BoW) representation based on local spatio-temporal features, which has previously shown promising performance [10, 11, 12].", "startOffset": 195, "endOffset": 207}, {"referenceID": 11, "context": "In recent years, topic modeling has attracted increasing attention in human behavior discovery and recognition due to its ability to generate a distribution over activities of interest, and its promising performance using BoW representations in robotics applications [12, 13].", "startOffset": 267, "endOffset": 275}, {"referenceID": 12, "context": "In recent years, topic modeling has attracted increasing attention in human behavior discovery and recognition due to its ability to generate a distribution over activities of interest, and its promising performance using BoW representations in robotics applications [12, 13].", "startOffset": 267, "endOffset": 275}, {"referenceID": 13, "context": "Traditional activity recognition systems typically use accuracy as a performance metric [14].", "startOffset": 88, "endOffset": 92}, {"referenceID": 14, "context": "We focus our review on the commonly used sequential and space-time volume methods [15].", "startOffset": 82, "endOffset": 86}, {"referenceID": 14, "context": "A comprehensive review of different aspects of human activity recognition (HAR) is presented in [15] and [16].", "startOffset": 96, "endOffset": 100}, {"referenceID": 15, "context": "A comprehensive review of different aspects of human activity recognition (HAR) is presented in [15] and [16].", "startOffset": 105, "endOffset": 109}, {"referenceID": 16, "context": "Chen and Yang represented a human with just a point to derive the gait features for the pedestrian detection [17].", "startOffset": 109, "endOffset": 113}, {"referenceID": 17, "context": "extracted pedestrian trajectories from video to automatically detect small groups of people traveling together [18].", "startOffset": 111, "endOffset": 115}, {"referenceID": 18, "context": "extracted directional vectors from the silhouette contours and utilize the distinct data distribution of these vectors in a vector space for activity recognition [19].", "startOffset": 162, "endOffset": 166}, {"referenceID": 19, "context": "transformed silhouettes of a human from every frame into time-series, then each of these time series is converted into the symbolic vector to represent actions [20].", "startOffset": 160, "endOffset": 164}, {"referenceID": 20, "context": "Zhang and Parker implemented a bio-inspired predictive orientation decomposition (BIPOD) to construct representations of people from skeleton trajectories for the activity recognition and prediction, where the human body is decomposed into five body parts [21].", "startOffset": 256, "endOffset": 260}, {"referenceID": 21, "context": "A large number of HAR methods are based on SIFT features [22] and its extensions [23].", "startOffset": 57, "endOffset": 61}, {"referenceID": 22, "context": "A large number of HAR methods are based on SIFT features [22] and its extensions [23].", "startOffset": 81, "endOffset": 85}, {"referenceID": 23, "context": "proposed a random forest that unifies randomization, discriminative relationships mining and a Markov temporal structure for real-time activity recognition with SIFT features [24].", "startOffset": 175, "endOffset": 179}, {"referenceID": 24, "context": "STIP features were introduced in [25] and SVMs were applied to classify human activities.", "startOffset": 33, "endOffset": 37}, {"referenceID": 25, "context": "used separable filters in spatial and temporal dimensions to extract features for HAR [26].", "startOffset": 86, "endOffset": 90}, {"referenceID": 11, "context": "Four-dimensional features were also introduced in [12] to combine depth information to classify human activities.", "startOffset": 50, "endOffset": 54}, {"referenceID": 26, "context": "incorporated Bayesian learning into an undirected topic model and proposed a \u201drelevance topic model\u201d for the unstructured social group activity recognition [27].", "startOffset": 156, "endOffset": 160}, {"referenceID": 27, "context": "A semi-latent topic model trained in a supervised fashion was introduced in [28] and used to classify activities in videos.", "startOffset": 76, "endOffset": 80}, {"referenceID": 11, "context": "Zhang and Parker adopted topic models to classify activities in 3D point clouds from color-depth cameras on mobile robots [12].", "startOffset": 122, "endOffset": 126}, {"referenceID": 28, "context": "The use of topic models was explored in [29] to discover daily activity patterns in wearable sensor data.", "startOffset": 40, "endOffset": 44}, {"referenceID": 29, "context": "An unsupervised topic model was proposed in [30] to detect daily routines from streaming location and proximity data.", "startOffset": 44, "endOffset": 48}, {"referenceID": 30, "context": "explored a new method using topic models for both plan recognition and activity recognition objective [31].", "startOffset": 102, "endOffset": 106}, {"referenceID": 31, "context": ", computing the probability of held-out documents to evaluate generalization ability [32]) or extrinsic methods that rely on external tasks, (e.", "startOffset": 85, "endOffset": 89}, {"referenceID": 32, "context": ", information retrieval [33]).", "startOffset": 24, "endOffset": 28}, {"referenceID": 33, "context": "demonstrated that the probability of held-out documents is not always a good indicator of human judgment [34].", "startOffset": 105, "endOffset": 109}, {"referenceID": 34, "context": "showed that metrics based on word co-occurrence statistics are able to predict human evaluations of topic quality [35].", "startOffset": 114, "endOffset": 118}, {"referenceID": 35, "context": "As recently pointed out by Blei [36], topic model evaluation is an essential research topic.", "startOffset": 32, "endOffset": 36}, {"referenceID": 0, "context": "Artificial cognition has its origin in cybernetics with the intention to create a science of mind based on logic [1].", "startOffset": 113, "endOffset": 116}, {"referenceID": 1, "context": "Among other cognitive paradigms, cognitivism has undoubtedly been predominant to date [2].", "startOffset": 86, "endOffset": 89}, {"referenceID": 5, "context": "Within the cognitivism paradigm, several cognitive architectures were developed, including Soar [6], ACT-R [3] (and its extensions ACT-R/E [4], ACT-R\u03a6 [5], etc), C4 [7], and architectures for robotics [8], which are relatively independent of applications [37].", "startOffset": 96, "endOffset": 99}, {"referenceID": 2, "context": "Within the cognitivism paradigm, several cognitive architectures were developed, including Soar [6], ACT-R [3] (and its extensions ACT-R/E [4], ACT-R\u03a6 [5], etc), C4 [7], and architectures for robotics [8], which are relatively independent of applications [37].", "startOffset": 107, "endOffset": 110}, {"referenceID": 3, "context": "Within the cognitivism paradigm, several cognitive architectures were developed, including Soar [6], ACT-R [3] (and its extensions ACT-R/E [4], ACT-R\u03a6 [5], etc), C4 [7], and architectures for robotics [8], which are relatively independent of applications [37].", "startOffset": 139, "endOffset": 142}, {"referenceID": 4, "context": "Within the cognitivism paradigm, several cognitive architectures were developed, including Soar [6], ACT-R [3] (and its extensions ACT-R/E [4], ACT-R\u03a6 [5], etc), C4 [7], and architectures for robotics [8], which are relatively independent of applications [37].", "startOffset": 151, "endOffset": 154}, {"referenceID": 6, "context": "Within the cognitivism paradigm, several cognitive architectures were developed, including Soar [6], ACT-R [3] (and its extensions ACT-R/E [4], ACT-R\u03a6 [5], etc), C4 [7], and architectures for robotics [8], which are relatively independent of applications [37].", "startOffset": 165, "endOffset": 168}, {"referenceID": 7, "context": "Within the cognitivism paradigm, several cognitive architectures were developed, including Soar [6], ACT-R [3] (and its extensions ACT-R/E [4], ACT-R\u03a6 [5], etc), C4 [7], and architectures for robotics [8], which are relatively independent of applications [37].", "startOffset": 201, "endOffset": 204}, {"referenceID": 36, "context": "Within the cognitivism paradigm, several cognitive architectures were developed, including Soar [6], ACT-R [3] (and its extensions ACT-R/E [4], ACT-R\u03a6 [5], etc), C4 [7], and architectures for robotics [8], which are relatively independent of applications [37].", "startOffset": 255, "endOffset": 259}, {"referenceID": 1, "context": "The combination of a cognitive architecture and a particular knowledge set is generally referred to as a cognitive model [2].", "startOffset": 121, "endOffset": 124}, {"referenceID": 1, "context": "The knowledge incorporated in cognitive models is typically determined by human designers [2].", "startOffset": 90, "endOffset": 93}, {"referenceID": 37, "context": "For example, cognitive modeling was adopted in [38, 39, 40] to construct intelligent human-machine interaction systems.", "startOffset": 47, "endOffset": 59}, {"referenceID": 38, "context": "For example, cognitive modeling was adopted in [38, 39, 40] to construct intelligent human-machine interaction systems.", "startOffset": 47, "endOffset": 59}, {"referenceID": 39, "context": "For example, cognitive modeling was adopted in [38, 39, 40] to construct intelligent human-machine interaction systems.", "startOffset": 47, "endOffset": 59}, {"referenceID": 40, "context": "Cognitive perception systems were also used to recognize traffic signs [41, 42], interpret traffic behaviors [43, 44], and recognize human activities [45, 46].", "startOffset": 71, "endOffset": 79}, {"referenceID": 41, "context": "Cognitive perception systems were also used to recognize traffic signs [41, 42], interpret traffic behaviors [43, 44], and recognize human activities [45, 46].", "startOffset": 71, "endOffset": 79}, {"referenceID": 42, "context": "Cognitive perception systems were also used to recognize traffic signs [41, 42], interpret traffic behaviors [43, 44], and recognize human activities [45, 46].", "startOffset": 109, "endOffset": 117}, {"referenceID": 43, "context": "Cognitive perception systems were also used to recognize traffic signs [41, 42], interpret traffic behaviors [43, 44], and recognize human activities [45, 46].", "startOffset": 109, "endOffset": 117}, {"referenceID": 44, "context": "Cognitive perception systems were also used to recognize traffic signs [41, 42], interpret traffic behaviors [43, 44], and recognize human activities [45, 46].", "startOffset": 150, "endOffset": 158}, {"referenceID": 45, "context": "Cognitive perception systems were also used to recognize traffic signs [41, 42], interpret traffic behaviors [43, 44], and recognize human activities [45, 46].", "startOffset": 150, "endOffset": 158}, {"referenceID": 46, "context": "Over the last decade, probabilistic models of cognition, as an alternative of deterministic cognitive models, have attracted more attention in cognitive development [47].", "startOffset": 165, "endOffset": 169}, {"referenceID": 47, "context": "For example, an adaptive remote data mirroring system was proposed applying dynamic decision networks in [48].", "startOffset": 105, "endOffset": 109}, {"referenceID": 48, "context": "Another cognitive model was introduced in [49] to apply dynamic Bayesian networks for vehicle classification.", "startOffset": 42, "endOffset": 46}, {"referenceID": 49, "context": "Probabilistic models have also been widely used for learning and reasoning in cognitive modeling [50].", "startOffset": 97, "endOffset": 101}, {"referenceID": 6, "context": "The proposed SRAC model is inspired by the C4 cognitive architecture [7].", "startOffset": 69, "endOffset": 72}, {"referenceID": 50, "context": "Latent Dirichlet Allocation (LDA) [51], which showed promising activity recognition performance in our prior work [12], is applied in the SRAC model.", "startOffset": 34, "endOffset": 38}, {"referenceID": 11, "context": "Latent Dirichlet Allocation (LDA) [51], which showed promising activity recognition performance in our prior work [12], is applied in the SRAC model.", "startOffset": 114, "endOffset": 118}, {"referenceID": 50, "context": "However, exact parameter estimation is intractable in general [51].", "startOffset": 62, "endOffset": 66}, {"referenceID": 51, "context": "Our model applies Gibbs sampling [52] to compute the per-observation activity distribution \u03b8, based on two considerations: 1) This sampling-based method is generally accurate, since it asymptotically approaches the correct distribution [53], and 2) This method can be used to intrinsically evaluate topic model\u2019s performance [32], thereby providing a consistent method to infer, learn, and evaluate topic models.", "startOffset": 33, "endOffset": 37}, {"referenceID": 52, "context": "Our model applies Gibbs sampling [52] to compute the per-observation activity distribution \u03b8, based on two considerations: 1) This sampling-based method is generally accurate, since it asymptotically approaches the correct distribution [53], and 2) This method can be used to intrinsically evaluate topic model\u2019s performance [32], thereby providing a consistent method to infer, learn, and evaluate topic models.", "startOffset": 236, "endOffset": 240}, {"referenceID": 31, "context": "Our model applies Gibbs sampling [52] to compute the per-observation activity distribution \u03b8, based on two considerations: 1) This sampling-based method is generally accurate, since it asymptotically approaches the correct distribution [53], and 2) This method can be used to intrinsically evaluate topic model\u2019s performance [32], thereby providing a consistent method to infer, learn, and evaluate topic models.", "startOffset": 325, "endOffset": 329}, {"referenceID": 50, "context": "The indicator II is defined over the per-observation category proportion \u03b8, which takes values in the (K\u22121)-simplex [51].", "startOffset": 116, "endOffset": 120}, {"referenceID": 0, "context": "\u2200\u03b8s, II(\u03b8s, k) \u2208 [0, 1].", "startOffset": 17, "endOffset": 23}, {"referenceID": 53, "context": "The introduction of IG is inspired by the perplexity metric (also referred to as heldout likelihood), which evaluates a topic model\u2019s generalization ability on a fraction of held-out instances using cross-validation [54] or unseen observations [55].", "startOffset": 216, "endOffset": 220}, {"referenceID": 54, "context": "The introduction of IG is inspired by the perplexity metric (also referred to as heldout likelihood), which evaluates a topic model\u2019s generalization ability on a fraction of held-out instances using cross-validation [54] or unseen observations [55].", "startOffset": 244, "endOffset": 248}, {"referenceID": 31, "context": "The perplexity is defined as the log-likelihood of words in an observation [32].", "startOffset": 75, "endOffset": 79}, {"referenceID": 31, "context": "The left-to-right algorithm, presented in Algorithm 1, is used to estimate Pvwp, which is an accurate and efficient Gibbs sampling method to estimate perplexity [32].", "startOffset": 161, "endOffset": 165}, {"referenceID": 55, "context": "The training set is defined as exhaustive when it contains instances from all categories that can possibly be observed in the testing phase [56].", "startOffset": 140, "endOffset": 144}, {"referenceID": 0, "context": "Low risk [1,30] Unsatisfied with the robot\u2019s performance.", "startOffset": 9, "endOffset": 15}, {"referenceID": 29, "context": "Low risk [1,30] Unsatisfied with the robot\u2019s performance.", "startOffset": 9, "endOffset": 15}, {"referenceID": 30, "context": "Medium risk [31,60] Annoyed or upset by the robot\u2019s actions.", "startOffset": 12, "endOffset": 19}, {"referenceID": 56, "context": "activity dataset [57], the KTH activity dataset [58], and the UTK 3D activity dataset [12].", "startOffset": 17, "endOffset": 21}, {"referenceID": 57, "context": "activity dataset [57], the KTH activity dataset [58], and the UTK 3D activity dataset [12].", "startOffset": 48, "endOffset": 52}, {"referenceID": 11, "context": "activity dataset [57], the KTH activity dataset [58], and the UTK 3D activity dataset [12].", "startOffset": 86, "endOffset": 90}, {"referenceID": 21, "context": ", the Weizmann and KTH datasets), we use two different features: scale-invariant feature transform (SIFT) features [22] and space-time interest points (STIP) features [58].", "startOffset": 115, "endOffset": 119}, {"referenceID": 57, "context": ", the Weizmann and KTH datasets), we use two different features: scale-invariant feature transform (SIFT) features [22] and space-time interest points (STIP) features [58].", "startOffset": 167, "endOffset": 171}, {"referenceID": 11, "context": ", the UTK dataset), we use the 4-dimensional local spatio-temporal features (4D-LSTF) [12].", "startOffset": 86, "endOffset": 90}, {"referenceID": 21, "context": "sirable characteristics including invariance to transformation, rotation and scale, and robustness to partial occlusion [22].", "startOffset": 120, "endOffset": 124}, {"referenceID": 21, "context": "We employ the algorithm and implementation in [22] to detect and describe SIFT features.", "startOffset": 46, "endOffset": 50}, {"referenceID": 57, "context": "To encode time information, we also apply STIP along with the histogram of oriented gradients (HOG) and histogram of optical flow (HOF) descriptors [58].", "startOffset": 148, "endOffset": 152}, {"referenceID": 11, "context": "Previous work has demonstrated that local features incorporating both depth and color information can greatly improve recognition accuracy [12].", "startOffset": 139, "endOffset": 143}, {"referenceID": 11, "context": "Therefore, for the 3D UTK dataset we use 4D-LSTF [12] features, which are highly robust and distinct and are generated using both color and depth videos.", "startOffset": 49, "endOffset": 53}, {"referenceID": 0, "context": "Thus, \u2200\u03b8, II(\u03b8s, k) \u2208 [0, 1] holds.", "startOffset": 22, "endOffset": 28}], "year": 2016, "abstractText": "In order for cooperative robots (\u201cco-robots\u201d) to respond to human behaviors accurately and efficiently in human-robot collaboration, interpretation of human actions, awareness of new situations, and appropriate decision making are all crucial abilities for corobots. For this purpose, the human behaviors should be interpreted by co-robots in the same manner as human peers. To address this issue, a novel interpretability indicator is introduced so that robot actions are appropriate to the current human behaviors. In addition, the complete consideration of all potential situations of a robot\u2019s environment is nearly impossible in real-world applications, making it difficult for the co-robot to act appropriately and safely in new scenarios. This is true even when the pretrained model is highly accurate in a known situation. For effective and safe teaming with humans, we introduce a new generalizability indicator that allows a co-robot to self-reflect and reason about when an observation falls outside the co-robot\u2019s learned model. Based on topic modeling and two novel indicators, we propose a new Self-reflective Risk-aware Artificial Cognitive (SRAC) model. The co-robots are able to consider action risks and identify new situations so that better decisions can be made. Experiments both using real-world datasets and on physical robots suggest that our SRAC model significantly outperforms the traditional methodology and enables better decision making in response to human activities. Email addresses: fhan@mines.edu (Fei Han), creardon@utk.edu (Christopher Reardon), leparker@utk.edu (Lynne E. Parker), hzhang@mines.edu (Hao Zhang) May 18, 2016 ar X iv :1 60 5. 04 93 4v 1 [ cs .R O ] 1 6 M ay 2 01 6", "creator": "LaTeX with hyperref package"}}}