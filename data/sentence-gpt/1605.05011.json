{"id": "1605.05011", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "17-May-2016", "title": "Locally Weighted Ensemble Clustering", "abstract": "Due to its ability to combine multiple base clusterings into a probably better and more robust clustering, the ensemble clustering technique has been attracting increasing attention in recent years. Despite the significant success, one limitation to most of the existing ensemble clustering methods is that they generally treat all base clusterings equally regardless of their reliability, which makes them vulnerable to low-quality base clusterings. Although some efforts have been made to (globally) evaluate and weight the base clusterings, yet these methods tend to view each base clustering as an individual and neglect the local diversity of clusters inside the same base clustering. It remains an open problem how to evaluate the reliability of clusters and exploit the local diversity in the ensemble to enhance the consensus performance, without access to data features or specific assumptions on data distribution. To address this, in this paper, we propose a novel ensemble clustering approach based on ensemble-driven cluster uncertainty estimation and local weighting strategy. In particular, the uncertainty of each cluster is estimated by considering the cluster labels in the entire ensemble via an entropic criterion. A novel ensemble-driven cluster validity measure is introduced, and a locally weighted co-association matrix is presented to serve as a summary for the ensemble of diverse clusters. With the local diversity in ensembles exploited, two novel consensus functions are further proposed. Extensive experiments on a variety of real-world datasets demonstrate the superiority of the proposed approach over the state-of-the-art. Using the ensemble approach, we present a novel model that uses cluster weights and a model that employs clustering. Specifically, we provide a set of experimental test models to illustrate how to use ensemble-based clustering to assess the quality and reliability of ensemble clustering. The model provides a series of models which are used to assess and evaluate the accuracy and performance of the ensemble and the strength of a model with the best-to-fail model, with the best-to-fail model, the best-to-fail model, and the best-to-fail model. The ensemble model can also be used to determine the quality and reliability of a model with the best-to-fail model for each single data set and the best-to-fail model for each individual data set. The model contains the model's most commonly used and most frequently used components such as an array of three data sets. The ensemble model consists of three data sets: one for each subtype of data set, and one for each subtype of data set. These three components comprise an", "histories": [["v1", "Tue, 17 May 2016 03:52:38 GMT  (3767kb,D)", "http://arxiv.org/abs/1605.05011v1", "Under peer review"], ["v2", "Fri, 5 May 2017 16:35:41 GMT  (3380kb,D)", "http://arxiv.org/abs/1605.05011v2", "To appear in IEEE Transactions on Cybernetics. The MATLAB source code and experimental data of this work are available at:this https URL"]], "COMMENTS": "Under peer review", "reviews": [], "SUBJECTS": "cs.LG", "authors": ["dong huang", "chang-dong wang", "jian-huang lai"], "accepted": false, "id": "1605.05011"}, "pdf": {"name": "1605.05011.pdf", "metadata": {"source": "CRF", "title": "Locally Weighted Ensemble Clustering", "authors": ["Dong Huang", "Chang-Dong Wang", "Jian-Huang Lai"], "emails": ["huangdonghere@gmail.com."], "sections": [{"heading": null, "text": "Index Terms\u2014Data clustering, Ensemble clustering, Consensus clustering, Cluster uncertainty estimation, Local weighting.\nI. INTRODUCTION\nDATA clustering is a fundamental yet very challengingproblem in the field of data mining and machine learning [1]. The purpose of it is to discover the inherent structures of a given dataset and partition the dataset into a certain number of homogeneous groups, i.e., clusters. During the past few decades, a large number of clustering algorithms have been developed by exploiting various techniques [2], [3], [4], [5], [6], [7], [8], [9], [10], [11], [12], [13]. Each clustering algorithm has its advantages as well as its drawbacks, and may perform well for some specific applications. There is no single clustering algorithm that is capable of dealing with all types of data structures and cluster shapes. Given a data set, different clustering algorithms, or even the same algorithm with different initializations or parameters, may\nThis project was supported by NSFC (61173084 & 61502543), National Science & Technology Pillar Program (No. 2012BAK16B06), Guangdong Natural Science Funds for Distinguished Young Scholar, the GuangZhou Program (No. 201508010032), and the PhD Start-up Fund of Natural Science Foundation of Guangdong Province, China (No. 2014A030310180).\nDong Huang is with the College of Mathematics and Informatics, South China Agricultural University, Guangzhou, China. E-mail: huangdonghere@gmail.com.\nChang-Dong Wang and Jian-Huang Lai are with the School of Data and Computer Science, Sun Yat-sen University, Guangzhou, China and with Guangdong Key Laboratory of Information Security Technology, Guangzhou, China. E-mail: changdongwang@hotmail.com, stsljh@mail.sysu.edu.cn.\nlead to different clustering results. However, without prior knowledge, it is extremely difficult to decide which algorithm would be the appropriate one for a given clustering task. Even with the clustering algorithm given, it may still be difficult to find proper parameters for it.\nDifferent clusterings produced by different algorithms (or the same algorithm with different initializations and parameters) may reflect different perspectives of the data. To exploit the complementary and rich information in multiple clusterings, the ensemble clustering technique has emerged as a powerful tool for data clustering and has been attracting increasing attention in recent years [14], [15], [16], [17], [18], [19], [20], [21], [22], [23], [24], [25]. Ensemble clustering aims to combine multiple clusterings to obtain a probably better and more robust clustering result, which has shown advantages in finding bizarre clusters, dealing with noise, and integrating clustering solutions from multiple distributed sources [26]. In ensemble clustering, each input clustering is referred to as a base clustering, while the final clustering result is referred to as the consensus clustering.\nIn ensemble clustering, the quality of the base clusterings plays a crucial role in the consensus process. The consensus results may be badly affected by low-quality (or even ill) base clusterings. To deal with low-quality base clusterings, some efforts have been made to evaluate and weight the base clusterings to enhance the consensus performance [27], [28], [29]. However, these approaches [27], [28], [29] are developed based on an implicit assumption that all of the clusters in the same base clustering have the same reliability. They typically treat each base clustering as an individual and assign a weight to each base clustering regardless of the diversity of the clusters inside it [27], [28], [29]. However, due to the noise and inherent complexity of real-world datasets, the different clusters in the same clustering may have different reliability. There is a need to respect the local diversity of ensembles and deal with the different reliability of clusters. More recently, Zhong et al. [30] proposed to evaluate the reliability of clusters by considering the Euclidean distances between data objects in clusters. The method in [30] requires access to the original data features, and its efficacy heavily relies on the data distribution of the dataset. However, in the general formulation of ensemble clustering (see Section III-B), there is no access to the original data features. Without needing access to the data features or relying on specific assumptions about data distribution, the key problem here is how to evaluate the reliability of clusters and weight them accordingly to enhance the accuracy and robustness of the consensus clusterings.\nAiming to address the aforementioned problem, in this paper, we propose a novel ensemble clustering approach based on ensemble-driven cluster uncertainty estimation and local weighting strategy. The overall process of our approach is\nar X\niv :1\n60 5.\n05 01\n1v 1\n[ cs\n.L G\n] 1\n7 M\nay 2\n01 6\n2 illustrated in Fig. 1. We take advantage of the ensemble diversity at the cluster-level and integrate the cluster uncertainty and validity into a locally weighted scheme to enhance the consensus performance. A cluster can be viewed as a local region in the corresponding base clustering. Without needing access to the data features, in our work, the uncertainty of each cluster is estimated with regard to the cluster labels in the entire ensemble based on an entropic criterion. In particular, given a cluster, we investigate its uncertainty by considering how the objects inside this cluster are grouped in the multiple base clusterings. Based on cluster uncertainty estimation, an ensemble-driven cluster index (ECI) is then presented to measure the reliability of clusters. In this paper, we argue that the crowd of diverse clusters in the ensemble can provide an effective indication for evaluating each individual cluster. By evaluating and weighting the clusters in the ensemble via the ECI measure, we further present the concept of locally weighted co-association (LWCA) matrix, which incorporates local adaptivity into the conventional coassociation (CA) matrix and serves as a summary for the\nensemble of diverse clusters. Finally, to achieve the final clustering result, we propose two novel consensus functions, termed locally weighted evidence accumulation (LWEA) and locally weighted graph partitioning (LWGP), respectively, with the diversity of clusters exploited and the local weighting strategy incorporated.\nFor clarity, we summarize the main contributions of this paper as follows: \u2022 We propose to estimate the uncertainty of clusters by\nconsidering the distribution of all cluster labels in the ensemble using an entropic criterion, which requires no access to the original data features and makes no assumptions on the data distribution. \u2022 We present an ensemble-driven cluster validity index to evaluate and weight the clusters in the ensemble, which provides an indication of reliability at the cluster-level and plays a crucial role in the local weighting scheme. \u2022 We propose two novel consensus functions to construct the final clusterings based on ensemble-driven cluster uncertainty estimation and local weighting strategy. \u2022 Extensive experiments have been conducted on a variety of real-world datasets, which demonstrate that the proposed ensemble clustering approach outperforms the state-of-the-art approaches in terms of both clustering accuracy and efficiency.\nThe rest of the paper is organized as follows. The related work is reviewed in Section II. The background knowledge about entropy and ensemble clustering is introduced in Section III. The proposed ensemble clustering approach based on cluster uncertainty estimation and local weighting strategy is described in Section IV. The experimental results are reported in Section V. Finally, we conclude the paper in Section VI."}, {"heading": "II. RELATED WORK", "text": "Ensemble clustering aims to combine multiple base clustering clusterings to obtain a probably better and more robust consensus clustering. In the past decade, many ensemble\nInput: the clustering ensemble \u03a0\nCluster uncertainty estimation using entropy criterion\nEnsemble-driven cluster validity\nRefining co-association matrix by local weighting\nConsensus functions\nLWEA\nOutput: the consensus clustering \ud835\udf0b\u2217\nLWGP\nFig. 1. Flow diagram of the proposed approach.\nclustering approaches have been developed, which can be mainly classified into three categories, i.e., the pair-wise cooccurrence based approaches [17], [19], [20], [21], the graph partitioning based approaches [14], [16], and the median partition based approaches [15], [18], [22].\nThe pair-wise co-occurrence based approaches [17], [19], [20], [21] typically construct a co-association (CA) matrix by considering how many times two objects occur in the same cluster among the multiple base clusterings. By exploiting the CA matrix as the similarity matrix, the conventional clustering techniques, such as the agglomerative clustering methods [1], can be exploited to build the final clustering result. Fred and Jain [17] for the first time presented the concept of CA matrix and proposed the evidence accumulation clustering (EAC) method. Wang et al. [19] extended the EAC method by taking the sizes of clusters into consideration, and proposed the probability accumulation method. Iam-On et al. [20] refined the CA matrix by considering the shared neighbors between clusters to improve the consensus results. Wang [21] introduced a dendrogram-like hierarchical data structure termed CA-tree to facilitate the co-association based ensemble clustering process.\nThe graph partitioning based approaches [14], [16] address the ensemble clustering problem by constructing a graph model to reflect the ensemble information. The consensus clustering is then obtained by partitioning the graph into a certain number of segments. Strehl and Ghosh [14] proposed three graph partitioning based ensemble clustering algorithms, i.e., cluster-based similarity partitioning algorithm (CSPA), hypergraph partitioning algorithm (HGPA), and meta-clustering algorithm (MCLA). Fern and Brodley [16] constructed a bipartite graph for the clustering ensemble by treating both clusters and objects as graph nodes, and obtain the consensus clustering by partitioning the bipartite graph.\nThe median partition based approaches [15], [18], [22] formulate the ensemble clustering problem into an optimization\n3 problem, which aims to find a median partition (or clustering) by maximizing the similarity between this clustering and the multiple base clusterings. The median partition problem is NPhard [18]. Finding the globally optimal solution in the huge space of all possible clusterings is computationally infeasible for large datasets. Cristofor and Simovici [15] proposed to obtain an approximate solution using the genetic algorithm, where clusterings are treated as chromosomes. Topchy et al. [18] cast the median partition problem into a maximum likelihood problem and approximately solve it by the EM algorithm. Franek and Jiang [22] cast the median partition problem into an Euclidean median problem by clustering embedding in vector spaces. The median vector is found by the Weiszfeld algorithm [31] and then transformed into a clustering again, which is treated as the consensus clustering.\nThese algorithms attempt to solve the ensemble clustering problem in various ways [14], [15], [16], [17], [18], [19], [20], [21], [22]. However, one common limitation to most of the existing methods is that they generally treat all clusters and all base clusterings in the ensemble equally and may suffer from low-quality clusters or low-quality base clusterings. To partially address this limitation, recently some weighted ensemble clustering approaches have been presented [27], [28], [29]. Li and Ding [27] cast the ensemble clustering problem into a nonnegative matrix factorization problem and proposed a weighted consensus clustering approach, where each base clustering is assigned a weight in order to improve the consensus result. Yu et al. [28] exploited the feature selection techniques to weight and select the base clusterings. In fact, clustering selection [28] can be viewed as a 0-1 weighting scheme, where 1 indicates selecting a clustering and 0 indicates removing a clustering. Huang et al. [29] proposed to evaluate and weight the base clusterings based on the normalized crowd agreement index (NCAI). Although these approaches [27], [28], [29] are able to estimate the reliability of base clusterings and weight them accordingly, yet they generally treat a base clustering as a whole and neglect the local diversity of clusters inside the same base clustering. To explore the reliability of clusters, Alizadeh et al. [32] proposed to evaluate clusters in the ensemble by averaging normalized mutual information (NMI) [14] between clusterings, which results in a very expensive computational cost and is not feasible for large datasets. Zhong et al. [30] exploited the Euclidean distances between objects to estimate the cluster reliability, which needs access to the original data features and is only applicable to numerical data. However, in the more general formulation of ensemble clustering [14], [15], [16], [17], [18], [19], [20], [21], [22], [33], the original data features are not available in the consensus process. Moreover, by measuring the within-cluster similarity based on Euclidean distances, the efficacy of the method in [30] heavily relies on some implicit assumptions about data distribution, which places an unstable factor in the consensus process. Different from [30], in this paper, our ensemble clustering approach requires no access to the original data features. We propose to estimate the uncertainty of clusters by considering the cluster labels in the entire ensemble based on an entropic criterion, and then present an ensemble-driven cluster index (ECI) to evaluate cluster reliability without making any assumptions\non the data distribution. Further, to obtain the consensus clustering results, two novel consensus functions are developed based on cluster uncertainty estimation and local weighting strategy. Extensive experiments on a variety of real-world datasets have shown that our approach exhibits significant advantages in clustering accuracy and efficiency over the stateof-the-art approaches."}, {"heading": "III. PRELIMINARIES", "text": ""}, {"heading": "A. Entropy", "text": "In this section, we introduce the concept of entropy. In information theory [34], the entropy is a measure of the uncertainty associated with a random variable. The formal definition of entropy is provided in Definition 1.\nDefinition 1. For a discrete random variable X , the entropy H(X) is defined as\nH(X) = \u2212 \u2211 x\u2208X p(x) log2 p(x), (1)\nwhere X is the set of values that X can take, and p(x) is the probability mass function of X .\nThe joint entropy is a measure of the uncertainty associated with a set of random variables. The formal definition of joint entropy is provided in Definition 2.\nDefinition 2. For a pair of discrete random variables (X,Y ), the joint entropy H(X,Y ) is defined as\nH(X,Y ) = \u2212 \u2211 x\u2208X \u2211 y\u2208Y p(x, y) log2 p(x, y), (2)\nwhere p(x, y) is the joint probability of (X,Y ).\nIf and only if the two random variables X and Y are independent of each other, it holds that H(X,Y ) = H(X)+H(Y ). Hence, given n independent random variables X1, \u00b7 \u00b7 \u00b7 , Xn, we have [34]\nH(X1, \u00b7 \u00b7 \u00b7 , Xn) = H(X1) + \u00b7 \u00b7 \u00b7+H(Xn). (3)"}, {"heading": "B. Ensemble Clustering", "text": "In this section, we introduce the general formulation of the ensemble clustering problem. Let O = {o1, \u00b7 \u00b7 \u00b7 , oN} be a dataset, where oi is the i-th data object and N is the number of objects in O. Consider M partitions (or clusterings) for the dataset O, each of which is treated as a base clustering and consists of a certain number of clusters. Formally, we denote the ensemble of M base clusterings as follows:\n\u03a0 = {\u03c01, \u00b7 \u00b7 \u00b7 , \u03c0M}, (4)\nwhere \u03c0m = {Cm1 , \u00b7 \u00b7 \u00b7 , Cmnm} (5)\ndenotes the m-th base clustering in \u03a0, Cmi denotes the i-th cluster in \u03c0m, and nm denotes the number of clusters in \u03c0m. Each cluster is a set of data objects. Obviously, the union of all clusters in the same base clustering covers the entire dataset, i.e., \u2200\u03c0m \u2208 \u03a0, \u22c3nm i=1 C m i = O. Different clusters in\n4 the same base clustering do not intersect with each other, i.e., \u2200Cmi , Cmj \u2208 \u03c0m s.t. i 6= j, Cmi \u22c2 Cmj = \u2205. Let Clsm(oi) denote the cluster in \u03c0m \u2208 \u03a0 that object oi belongs to. That is, if oi belongs to the k-th cluster in \u03c0m, i.e., oi \u2208 Cmk , then we have Clsm(oi) = Cmk .\nFor convenience, we represent the set of all clusters in the ensemble \u03a0 as\nC = {C1, \u00b7 \u00b7 \u00b7 , Cnc}, (6)\nwhere Ci denotes the i-th cluster and nc denotes the total number of clusters in \u03a0. It is obvious that nc = n1+\u00b7 \u00b7 \u00b7+nM .\nThe purpose of ensemble clustering is to combine the multiple base clusterings in the ensemble \u03a0 to obtain a probably better and more robust clustering. With regard to the difference in the input information, there are two different formulations of the ensemble clustering problem. In the first formulation, the ensemble clustering system only takes the multiple base clusterings as input and has no access to the original data features [14], [15], [16], [17], [18], [19], [20], [21], [22], [33], [35], [36]. In the other formulation, the ensemble clustering system takes both the multiple base clusterings and the original data features as inputs [30], [37], [38]. In this paper, we comply with the first formulation of the ensemble clustering problem, which is also the common practice for most of the existing ensemble clustering approaches [39]. Hence, in our formulation, the input is the clustering ensemble \u03a0, and the output is the consensus clustering \u03c0\u2217."}, {"heading": "IV. LOCALLY WEIGHTED ENSEMBLE CLUSTERING", "text": "In this paper, we propose a novel ensemble clustering approach based on ensemble-driven cluster uncertainty estimation and local weighting strategy. Without needing access to the original data features or making some assumptions about data distribution, we exploit the ensemble information to estimate the uncertainty (or unreliability) of clusters based on an entropic criterion. With the cluster uncertainty obtained, an ensemble-driven cluster validity index termed ECI is presented to evaluate the reliability of each cluster with the help of the cluster labels in the clustering ensemble. In this paper, we argue that the diverse clusters in the ensemble can provide an effective indication for evaluating the reliability of each individual cluster. Then, we refine the conventional CA matrix using a local weighting strategy based on the ECI measure, and introduce the concept of locally weighted co-association (LWCA) matrix , which serves as a summary for the ensemble with diverse clusters. To obtain the final clustering results, in this paper, two novel consensus functions are further presented, that is, LWEA and LWGP. In the following of this section, we will describe each step of our approach in detail."}, {"heading": "A. Measuring Cluster Uncertainty in Ensembles", "text": "In the general formulation of ensemble clustering, there is no access to the original data features. To evaluate the reliability of each cluster, we appeal to the concept of entropy with the help of the cluster labels in the entire ensemble.\nAs introduced in Section III-A, entropy is a measure of uncertainty associated with a random variable. Each cluster\nis a set of data objects. Given a cluster Ci \u2208 C and a base clustering \u03c0m \u2208 \u03a0, if cluster Ci does not belong to \u03c0m, then it is possible that the objects in Ci belong to more than one cluster in \u03c0m. In fact, the objects in Ci may belong to at most nm different clusters in \u03c0m, where nm is the total number of clusters in \u03c0m. The uncertainty (or entropy) of Ci w.r.t. \u03c0m can be computed by considering how the objects in Ci are clustered in \u03c0m.\nDefinition 3. Given the ensemble \u03a0, the uncertainty of cluster Ci w.r.t. the base clustering \u03c0m \u2208 \u03a0 is computed as\nHm(Ci) = \u2212 nm\u2211 j=1 p(Ci, C m j ) log2 p(Ci, C m j ) (7)\nwith\np(Ci, C m j ) =\n|Ci \u22c2 Cmj |\n|Ci| , (8)\nwhere nm is the number of clusters in \u03c0m, Cmj is the j-th cluster in \u03c0m, \u22c2 computes the intersection of two sets (or clusters), and |Ci| outputs the number of objects in Ci.\nThe formal definition of the cluster uncertainty w.r.t. a base clustering is given in Definition 3. Because p(Ci, Cmj ) \u2208 [0, 1] for any i, j, m, it is obvious that Hm(Ci) \u2208 [0,+\u221e). When all of the objects in Ci belong to the same cluster in \u03c0m, the uncertainty of Ci w.r.t. \u03c0m reaches its minimum, i.e., zero. When the objects in Ci belong to more different clusters in \u03c0m, the uncertainty of Ci w.r.t. \u03c0m typically gets greater, which indicates that the objects in Ci are less likely to be in the same cluster with regard to \u03c0m.\nWithout loss of generality, based on the assumption that the base clusterings in the ensemble are independent [39], the uncertainty (or entropy) of Ci w.r.t. the ensemble \u03a0 can be computed by summing up the uncertainty of Ci w.r.t. the M base clusterings in \u03a0 according to Eq. (3). Its formal definition is given in Definition 4.\nDefinition 4. Given the ensemble \u03a0, the uncertainty of cluster Ci w.r.t. the entire ensemble \u03a0 is computed as\nH\u03a0(Ci) = M\u2211 m=1 Hm(Ci), (9)\nwhere M is the number of base clusterings in \u03a0.\nIntuitively, the uncertainty of Ci w.r.t. \u03a0 reflects how the objects in Ci are clustered in the ensemble of multiple base clusterings. If the objects in Ci belong to the same cluster in each of the base clusterings, which can be viewed as that all base clusterings agree that the objects in Ci should be assigned to the same cluster, then the uncertainty of Ci w.r.t. \u03a0 reaches its minimum, i.e., zero. When the uncertainty of Ci w.r.t. \u03a0 gets larger, it is indicated that the objects in Ci are less likely to be in the same cluster with consideration to the ensemble of multiple base clusterings.\nWe provide an example in Fig. 2 and Table I to show the computation of cluster uncertainty w.r.t. an ensemble of three base clusterings. For the dataset O = {o1, \u00b7 \u00b7 \u00b7 , o16} with 16 data objects, three base clusterings (\u03c01, \u03c02, and \u03c03) are generated, each of which consists of three clusters (as illustrated in\n5 1 1 1 1 2 2 2 2 1 1 1 2 2 2 3 3 Cluster labels RSO labels SO labels \u03c01 \u03c02 \u03c03 o1 1 1 1 \ud835\udc5f1 s1 o2 1 1 1 \ud835\udc5f1 s1 o3 2 1 1 \ud835\udc5f2 s2 o4 2 1 1 \ud835\udc5f2 s2 o5 1 1 2 \ud835\udc5f3 s3 o6 1 1 2 \ud835\udc5f3 s3 o7 2 1 1 \ud835\udc5f2 s2 o8 1 2 2 \ud835\udc5f4 s3 o9 1 2 3 \ud835\udc5f5 s4 o10 1 2 3 \ud835\udc5f5 s4 o11 3 2 3 \ud835\udc5f6 s4 o12 1 2 3 \ud835\udc5f5 s4 o13 3 3 3 \ud835\udc5f7 s5 o14 3 3 3 \ud835\udc5f7 s5 o15 3 3 3 \ud835\udc5f8 s6 o16 3 3 3 \ud835\udc5f8 s6\no1 o2\no6 o5\no10\no9 o12\no11\no 3\no 7\no 4\no 14\no 13 o 15 o 16\no 8\nC1 1\nC2 1\nC3 1\no1 o2\no6 o5\no10\no9 o12\no11\no 3\no 7\no 4\no 14\no\n13\no\n15\no\n16\no 8\nC1 3\nC2 3\nC3 3\n(a) \u03c01\no1 o2\no6 o5\no10\no9 o12\no11\no 3 o\n7\no 4\no 14\no 13 o 15 o 16\no 8\nC1 2\nC2 2\nC3 2\no1\no2\no6 o5\no10\no9 o12\no11\no 3\no 7\no 4\no 14\no 13 o 15 o 16\no 8\no1 o2\no6 o5\no10\no9 o12\no11\no 3\no 7\no 4\no 14\no 13 o 15 o 16\no 8\n\ud835\udc451\n\ud835\udc452\n\ud835\udc453\n\ud835\udc454\n\ud835\udc455\n\ud835\udc456 \ud835\udc457\n\ud835\udc458\no1 o2\no6 o5\no10\no9 o12\no11\no 3\no 7\no 4\no 14\no\n13\no\n15\no 16\no 8\n\ud835\udc461\n\ud835\udc462\n\ud835\udc463 \ud835\udc464 \ud835\udc465\n\ud835\udc466\n(b) \u03c02\n1 1 1 1 2 2 2 2 1 1 1 2 2 2 3 3 Cluster labels RSO labels SO labels \u03c01 \u03c02 \u03c03 o1 1 1 1 \ud835\udc5f1 s1 o2 1 1 1 \ud835\udc5f1 s1 o3 2 1 1 \ud835\udc5f2 s2 o4 2 1 1 \ud835\udc5f2 s2 o5 1 1 2 \ud835\udc5f3 s3 o6 1 1 2 \ud835\udc5f3 s3 o7 2 1 1 \ud835\udc5f2 s2 o8 1 2 2 \ud835\udc5f4 s3 o9 1 2 3 \ud835\udc5f5 s4 o10 1 2 3 \ud835\udc5f5 s4 o11 3 2 3 \ud835\udc5f6 s4 o12 1 2 3 \ud835\udc5f5 s4 o13 3 3 3 \ud835\udc5f7 s5 o14 3 3 3 \ud835\udc5f7 s5 o15 3 3 3 \ud835\udc5f8 s6 o16 3 3 3 \ud835\udc5f8 s6 o1 o2 o6 o5\no10\no9 o12\no11\no 3 o 7 o 4\no 14\no 13 o 15 o 16\no 8\nC1 1\nC2 1\nC3 1\no1 o2\no6 o5\no10\no9 o12\no11\no 3\no 7\no 4\no 14\no 13 o 15 o 16\no 8\nC1 3\nC2 3\nC3 3\n(c) \u03c03\nFig. 2. Illustration of an ensemble of three base clusterings, namely, \u03c01, \u03c02, and \u03c03.\nTABLE I COMPUTATION OF CLUSTER UNCERTAINTY AND ECI (WITH \u03b8 = 0.5) FOR THE CLUSTERS IN THE ENSEMBLE SHOWN IN FIG. 2.\nBase Clustering Cluster\nCluster Uncertainty w.r.t. the Ensemble ECI\n\u03c01 C11 H \u03a0(C11 ) = 2.56 ECI(C 1 1 ) = 0.60\nC12 H \u03a0(C12 ) = 0.00 ECI(C 1 2 ) = 1.00 C13 H \u03a0(C13 ) = 0.72 ECI(C 1 3 ) = 0.87\n\u03c02 C21 H \u03a0(C21 ) = 0.97 ECI(C 2 1 ) = 0.82\nC22 H \u03a0(C22 ) = 0.92 ECI(C 2 2 ) = 0.83 C23 H \u03a0(C23 ) = 1.95 ECI(C 2 3 ) = 0.68\n\u03c03 C31 H \u03a0(C31 ) = 1.85 ECI(C 3 1 ) = 0.69\nC32 H \u03a0(C32 ) = 1.44 ECI(C 3 2 ) = 0.75 C33 H \u03a0(C33 ) = 0.00 ECI(C 3 3 ) = 1.00\nFig. 2). Of the three clusters in \u03c01, C11 contains eight objects, C12 contains three objects, and C 1 3 contains five objects. Then, we proceed to compute the uncertainty of the three clusters in \u03c01 w.r.t. the ensemble. The eight objects in cluster C11 belong to three different clusters in \u03c02. According to Definition 3, with p(C11 , C 2 1 ) = 2 8 , p(C 1 1 , C 2 2 ) = 3 8 , and p(C 1 1 , C 2 3 ) = 3 8 , the uncertainty of C11 w.r.t. base clustering \u03c0 2 is computed as H2(C11 ) = \u2212 28 \u00b7 log2 2 8 \u2212 3 8 \u00b7 log2 3 8 \u2212 3 8 \u00b7 log2 3 8 \u2248 1.56. Similarly, we can obtain H3(C11 ) = 1. It is obvious that the uncertainty of cluster C11 w.r.t. the base clustering that contains it equals zero, i.e, H1(C11 ) = 0. Therefore, the uncertainty of cluster C11 w.r.t. the entire ensemble \u03a0 can be computed as H\u03a0(C11 ) = 0 + 1.56 + 1 = 2.56. In a similar way, the uncertainty of the other clusters in \u03a0 can be obtained (see Table I). It is noteworthy that the three objects in C12 belong to the same cluster in each of the three base clusterings in \u03a0, i.e., all base clusterings in \u03a0 agree that the objects in C12 should be in the same cluster. Thereby the uncertainty of C12 w.r.t. \u03a0 reaches the minimum value, that is, H\u03a0(C12 ) = 0. As shown in Table I, of the nine clusters in \u03a0, C11 is the cluster with the greatest uncertainty, while C12 and C 3 3 are the two most stable clusters. For clarity, in the following, when we refer to cluster uncertainty without mentioning whether it is with respect to a base clustering or with respect to the ensemble, we mean cluster uncertainty w.r.t. the ensemble."}, {"heading": "B. Ensemble-Driven Cluster Validity", "text": "Having obtained the uncertainty (or entropy) of each cluster in the clustering ensemble, we further propose an ensembledriven cluster index (ECI), which measures the reliability of clusters by considering their uncertainty w.r.t. the ensemble.\nDefinition 5. Given an ensemble \u03a0 with M base clusterings, the ensemble-driven cluster index (ECI) for a cluster Ci is\ndefined as\nECI(Ci) = e \u2212H\n\u03a0(Ci) \u03b8\u00b7M , (10)\nwhere \u03b8 > 0 is a parameter to adjust the influence of the cluster uncertainty over the index.\nThe formal definition of ECI is given in Definition 5. According to the definition, because H\u03a0(Ci) \u2208 [0,+\u221e), it holds that ECI(Ci) \u2208 (0, 1] for any Ci \u2208 C. Obviously, smaller uncertainty of a cluster leads to a greater ECI value. As an example, Table I shows the ECI values for the clusters in the ensemble illustrated in Fig. 2.\nWhen the uncertainty of a cluster Ci reaches its minimum, i.e., H\u03a0(Ci) = 0, its ECI will thereby reaches its maximum, i.e., ECI(Ci) = 1. The ECI of a cluster approaches zero when its cluster uncertainty approaches infinity. A parameter \u03b8 is adopted in the computation of ECI to adjust the influence of the cluster uncertainty over the index (see Eq. (10)). As shown in Fig. 3, when setting \u03b8 to small values, e.g., setting \u03b8 < 0.1, the ECI decreases dramatically as the cluster uncertainty increases. When setting \u03b8 to large values, the difference between the ECI values of high-uncertainty clusters and low-uncertainty ones will be narrowed down. Empirically, it is suggested that the parameter \u03b8 be set in the interval of [0.2, 1]. The consensus performance of our approach with different parameters \u03b8 is evaluated by extensive experiments. Please see Section V-B for more details."}, {"heading": "C. Refining Co-association Matrix by Local Weighting", "text": "The co-association (CA) matrix is first proposed by Fred and Jain [17], which reflects how many times two data objects are grouped into the same cluster among the multiple base clusterings in the ensemble.\nDefinition 6. Given an ensemble \u03a0, the co-association (CA) matrix is computed as\nA = {aij}N\u00d7N (11)\n6 with\naij = 1\nM \u00b7 M\u2211 m=1 \u03b4mij , (12)\n\u03b4mij = { 1, if Clsm(oi) = Clsm(oj), 0, otherwise,\n(13)\nwhere Clsm(oi) denotes the cluster in \u03c0m \u2208 \u03a0 that object oi belongs to.\nThe CA matrix is a classical and widely used tool for dealing with the ensemble clustering problem [17], [21], [25], [40]. Despite the significant success, one limitation of the CA matrix is that it treats all clusters and all base clusterings in the ensemble equally and lack the ability to evaluate and weight the ensemble members w.r.t. their reliability. Huang et al. [29] exploited the NCAI index to weight the base clusterings and thereby construct a weighted co-association (WCA) matrix, which, however, only considers the reliability of base clusterings, but still neglects the cluster-wise diversity inside the same base clustering.\nDifferent from the (globally) weighting strategy [29] that treats each base clustering as a whole, in this section, we refine the CA matrix by a local weighting strategy based on the ensemble-driven cluster validity and propose the concept of locally weighted co-association (LWCA) matrix.\nDefinition 7. Given an ensemble \u03a0, the locally weighted coassociation (LWCA) matrix is computed as\nA\u0303 = {a\u0303ij}N\u00d7N (14)\nwith\na\u0303ij = 1\nM \u00b7 M\u2211 m=1 wmi \u00b7 \u03b4mij , (15)\nwmi =ECI (Cls m(oi)) , (16)\n\u03b4mij = { 1, if Clsm(oi) = Clsm(oj), 0, otherwise,\n(17)\nwhere Clsm(oi) denotes the cluster in \u03c0m \u2208 \u03a0 that object oi belongs to.\nA cluster can be viewed as a local region in a base clustering. To take into consideration the different reliability of clusters in the ensemble, the weighting term wmi is incorporated to assign weights to clusters via the ECI measure (see Definition 7). The intuition is that the objects that co-occur in more reliable clusters (with higher ECI values) are more likely to belong to the same cluster in the true clustering. With the local weighting strategy, the LWCA matrix not just considers how many times two objects occur in the same cluster among the multiple base clusterings, but also reflects how reliable the clusters in the ensemble are."}, {"heading": "D. Consensus Functions", "text": "In this paper, based on ensemble-driven cluster uncertainty estimation and local weighting strategy, we further propose two novel consensus functions, i.e., locally weighted evidence\naccumulation (LWEA) and locally weighted graph partitioning (LWGP), which will be described in Section IV-D1 and Section IV-D2, respectively.\n1) Locally Weighted Evidence Accumulation (LWEA): In this section, we introduce the consensus function termed LWEA, which is based on hierarchical agglomerative clustering.\nHierarchical agglomerative clustering is a widely used clustering technique [1], which typically takes a similarity matrix as input and performs region merging iteratively to achieve a dendrogram, i.e., a hierarchical representation of clusterings. Here, we exploit the LWCA matrix (see Definition 7) as the initial similarity matrix, denoted as\nS(0) = {S(0)ij }N\u00d7N , (18)\nwith S\n(0) ij = a\u0303ij , (19)\nwhere a\u0303ij is the (i, j)-th entry in the LWCA matrix. The N original data objects are treated as the N initial regions. Formally, we denote the set of initial regions as follows:\nR(0) = {R(0)1 , \u00b7 \u00b7 \u00b7 , R (0) N }, (20)\nwhere\nR (0) i = {oi}, for i = 1, \u00b7 \u00b7 \u00b7 , N. (21)\ndenotes the i-th region in R(0). Note that each initial region contains exactly one object.\nWith the initial similarity matrix and the initial regions constructed, the region merging process is then performed iteratively. In each step of region merging, the two regions with the highest similarity will be merged into a new and larger region and thereby the set of regions will be updated. The set of the obtained regions in the t-th step is denoted as follows:\nR(t) = {R(t)1 , \u00b7 \u00b7 \u00b7 , R (t) |R(t)|}, (22)\nwhere R(t)i denotes the i-th region and |R(t)| denotes the number of regions in R(t). After each step of region merging, to get prepared for the next iteration, the similarity matrix will be updated according to the new set of regions. Typically, we adopt the average-link (AL), which is a classical agglomerative clustering method [1], to update the similarity matrix for the t-step. That is\nS(t) = {S(t)ij }|R(t)|\u00d7|R(t)| (23)\nwith S\n(t) ij =\n1\n|R(t)i | \u00b7 |R (t) j | \u2211 ok\u2208R(t)i ,ol\u2208R (t) j a\u0303kl, (24)\nwhere |R(t)i | denotes the number of objects in the region R (t) i .\nBy iterative region merging, a dendrogram is constructed. The root of the dendrogram is the entire dataset, while the leaves of it are the original data objects. Each level of the dendrogram represents a clustering with a certain number of clusters. Therefore, the final clustering result can be obtained by specifying a number of clusters for the dendrogram.\nFor clarity, the overall algorithm of LWEA is summarized in Algorithm 1.\n7 Algorithm 1 (Locally Weighted Evidence Accumulation) Input: \u03a0, k.\n1: Compute the uncertainty of the clusters in \u03a0 w.r.t. Definition 4. 2: Compute the ECI measures of the clusters in \u03a0 w.r.t. Definition 5. 3: Construct the LWCA matrix w.r.t. Definition 7. 4: Initialize the set of regions R(0) and the similarity matrix S(0). 5: Construct the dendrogram iteratively:\nfor t = 1, 2, \u00b7 \u00b7 \u00b7 , N\u0303 \u2212 1 Merge the two most similar regions in R(t\u22121) w.r.t. S(t\u22121). Obtain the new set of regions R(t). Obtain the new similarity matrix S(t).\nend for 6: Obtain the clustering with k clusters in the dendrogram.\nOutput: the consensus clustering \u03c0\u2217.\n2) Locally Weighted Graph Partitioning (LWGP): In this section, we introduce the consensus function termed LWGP, which is based on bipartite graph formulating and partitioning.\nTo construct the bipartite graph, we treat both clusters and objects as graph nodes. A link between two nodes exists if and only if one node is a data object and the other node is the cluster that contains it (see Fig. 4). Given an object oi \u2208 O and a cluster Cj \u2208 C such that oi \u2208 Cj , the link weight between them is decided by the ECI value of Cj , i.e., the weight of a link is correlated to the reliability of the cluster that it connects to. Hence, with the ECI measure incorporated, the bipartite graph not only considers the belong-to relationship between objects and clusters, but also reflects the local reliability, i.e., the reliability of clusters, in the ensemble. Formally, the locally weighted bipartite graph (LWBG) is defined in Definition 8.\nDefinition 8. The locally weighted bipartite graph (LWBP) is defined as\nG = (V,L), (25) where V = O \u22c3 C is the node set and L is the link set. The link weight between two nodes vi and vj is defined as\nlij =  ECI(vj), if vi \u2208 O, vj \u2208 C, and vi \u2208 vj , ECI(vi), if vj \u2208 O, vi \u2208 C, and vj \u2208 vi, 0, otherwise. (26)\nHaving constructed the LWBG according to Definition 8, we proceed to partition the graph using the Tcut algorithm [41], which is able to take advantage of the bipartite graph structure to greatly facilitate the computation of the graph partitioning process. The graph is partitioned into a certain number of disjoint node sets. The object nodes in the same segment are treated as a cluster, and hence the final clustering result can be obtained.\nFor clarity, we summarize the LWGP algorithm in Algorithm 2."}, {"heading": "V. EXPERIMENTS", "text": "In this section, we evaluate the proposed LWEA and LWGP methods against the state-of-the-art ensemble clustering methods on a variety of real-world datasets. All experiments are conducted in Matlab R2014a 64-bit on a workstation (Windows Server 2008 R2 64-bit, 8 Intel 2.40 GHz processors, 96 GB of RAM).\nAlgorithm 2 (Locally Weighted Graph Partitioning) Input: \u03a0, k.\n1: Compute the uncertainty of the clusters in \u03a0 w.r.t. Definition 4. 2: Compute the ECI measures of the clusters in \u03a0 w.r.t. Definition 5. 3: Build the LWBG graph w.r.t. Definition 8. 4: Partition the LWBG into a certain number of segments using the\nTcut algorithm [41]. 5: Treat objects in the same segment as a cluster and form clusters\nfor the entire dataset. 6: Obtain the consensus clustering by the obtained clusters .\nOutput: the consensus clustering \u03c0\u2217.\n\ud835\udc5c1 \ud835\udc5c2 \ud835\udc5c\ud835\udc56 \ud835\udc5c\ud835\udc41\n\ud835\udc361 \ud835\udc36\ud835\udc57 \ud835\udc36\ud835\udc5b\ud835\udc50\n\u2026 \u2026\n\u2026 \u2026\n\ud835\udc52\ud835\udc56\ud835\udc57\nD ta Objects\nClusters"}, {"heading": "A. Datasets and Evaluation Metric", "text": "In our experiments, ten real-world datasets are used, namely, Semeion, Multiple Features (MF), Image Segmentation (IS), Forest Covertype (FCT), MNIST, Optical Digit Recognition (ODR), Landsat Satellite (LS), ISOLET, USPS, and Letter Recognition (LR). The MNIST dataset and the USPS dataset are respectively from [42] and [43], whereas the other eight datasets are from the UCI machine learning repository [44]. The details of the ten datasets are given in Table II.\nWe use the normalized mutual information (NMI) [14] to evaluate the quality of clusterings. The NMI measure provides a sound indication of the shared information between clusterings and has been one of the most widely used evaluation metric for clustering analysis. A larger value of NMI indicates a better clustering result.\nTo evaluate the consensus performance over various ensembles, we construct a pool of a large number of candidate base clusterings. Each of the candidate clusterings is produced by the k-means algorithm with the number of clusters k randomly selected in the interval of [2, \u221a N ], where N is the number of\n8\nobjects in the dataset. In this work, a pool of 100 candidate clusterings are randomly generated for each dataset.\nWith the base clustering pool generated, to rule out the factor of getting lucky occasionally and provide a fair comparison, the proposed methods and the baseline methods are evaluated by their average performances over a large number of runs, where the clustering ensemble for each run is constructed by randomly choosing M base clusterings from the pool. Typically, the ensemble size M = 10 is used. The consensus performances of different methods with varying ensemble sizes are also evaluated in the following of this paper (see Section V-E)."}, {"heading": "B. Choices of Parameter \u03b8", "text": "The parameter \u03b8 controls the influence of the cluster uncertainty over the consensus process of LWEA and LWGP. A smaller \u03b8 leads to a stronger influence of cluster uncertain over the consensus process via the ECI measure (see Fig. 3).\nWe evaluate the clustering performances of LWEA and LWGP with varying parameters \u03b8. For each value of parameter \u03b8, we run the proposed LWEA and LWGP methods 20 times, respectively, with the ensemble of base clusterings randomly drawn from the base clustering pool at each time, and report their average NMI scores with varying parameters \u03b8 in Table III and Table IV. As can be seen in Table III and Table IV, the proposed LWEA and LWGP methods yield consistent clustering performances in terms of NMI with different values of \u03b8 on the benchmark datasets. Empirically, it is suggested that the parameter \u03b8 be set to moderate values, e.g., in the interval of [0.2, 1]. In the following of this paper, for both\nLWEA and LWGP, we will use \u03b8 = 0.4 in all experiments on the benchmark datasets."}, {"heading": "C. Comparison against Base Clusterings", "text": "The purpose of ensemble clustering is to combine multiple base clusterings to obtain a probably better and more robust consensus clustering. In this section, we compare the consensus clusterings of the proposed LWEA and LWGP methods against the base clusterings. For each benchmark dataset, we\n9\nrun the proposed LWEA and LWGP methods 100 times, respectively, with the ensemble of base clusterings randomly drawn from the pool at each time. The average NMI scores and variances of LWEA, LWGP, as well as the base clusterings are illustrated in Fig. 5. The proposed methods exhibit significant improvements over the base clusterings on all of the ten benchmark datasets (see Fig. 5). Especially, for the Semeion, MF, IS, MNIST, ODR, LS, and USPS datasets, the advantage of the proposed methods over the base clusterings is even greater."}, {"heading": "D. Comparison against Other Ensemble Clustering Methods", "text": "In this section, we compare the proposed LWEA and LWGP methods against ten ensemble clustering methods, namely, CSPA [14], HGPA [14], MCLA [14], hybrid bipartite graph formulation (HBGF) [16], EAC [17], SimRank similarity based method (SRS) [45], weighted connected triple based method (WCT) [20], weighted evidence accumulation clustering (WEAC) [29], graph partitioning with multi-granularity link analysis (GP-MGLA) [29], and Two-level-refined cOassociation Matrix Ensemble (TOME) [30]. For each of the\nproposed methods and the baseline methods, we use two criteria to specify the number of clusters for the consensus clustering, that is, best-k and true-k. For best-k, the number of clusters that leads to the best performance is adopted for each test method. For true-k, the actual number of classes in the dataset is adopted for each method.\nTo achieve a fair comparison, we run each of the proposed methods and the baseline methods 100 times with the ensembles randomly constructed from the base clustering pool (see Section V-A). The average performances of different methods over 100 runs are reported in Table V. As shown in Table V, the proposed LWEA and LWGP methods achieve the best NMI scores on the Semeion, IS, MNIST, ODR, USPS, and LR datasets in terms of both best-k and true-k, and nearly the best scores on the FCT, LS, and ISOLET datasets. Although the TOME method outperforms the proposed methods on the MF dataset, yet on all of the other nine datasets it shows a lower or significantly lower NMI scores than our methods. That is probably due to the fact that the TOME method exploits Euclidian distances between objects to improve the\n10\nEnsemble size 10 20 30 40 50\nN M\nI\n0.58\n0.6\n0.62\n0.64\n0.66\n0.68\n0.7 semeion\nLWEA LWGP MCLA HBGF EAC SRS WCT WEAC GP-MGLA TOME\nEnsemble size 10 20 30 40 50\nN M\nI\n0.6\n0.62\n0.64\n0.66\n0.68\n0.7\n0.72\n0.74 MF\nEnsemble size 10 20 30 40 50\nN M\nI\n0.55\n0.6\n0.65\n0.7 IS\nEnsemble size 10 20 30 40 50\nN M\nI\n0.22\n0.225\n0.23\n0.235\n0.24\n0.245\n0.25 FCT\nEnsemble size 10 20 30 40 50\nN M\nI\n0.56\n0.58\n0.6\n0.62\n0.64\n0.66\n0.68 MNIST\nN M\nI\n0.79\n0.8\n0.81\n0.82\n0.83\n0.84\n0.85 OD\nN M\nI\n0.52\n0.54\n0.56\n0.58\n0.6\n0.62\n0.64\n0.66\nLS\nN M\nI\n0.71\n0.72\n0.73\n0.74\n0.75\n0.76\n0.77 ISOLET\nN M\nI\n0.56\n0.58\n0.6\n0.62\n0.64\n0.66\n0.68\n0.7 USPS\nN M\nI\n0.41\n0.42\n0.43\n0.44\n0.45\n0.46 LR\n(a) Semeion\nEnsemble size 10 20 30 40 50\nN M\nI\n0.58\n0.6\n0.62\n0.64\n0.6\n0.68\n0.7 semeion\nLWEA LWGP MCLA HBGF EAC SRS WCT WEAC GP-MGLA TOME\nEnsemble size 10 20 30 40 50\nN M\nI\n0.6\n0.62\n0.64\n0.6\n0.68\n0.7\n0.72\n0.74 MF\nEnsemble size 10 20 30 40 50\nN M\nI\n0.5\n0.6\n0.65\n0.7 IS\nEnsemble size 10 20 30 40 50\nN M\nI\n0.2\n0.2 5\n0.23\n0.235\n0.24\n0.245\n0.25 FCT\nEnsemble size 10 20 30 40 50\nN M\nI\n0.56\n0.58\n0.6\n0.62\n0.64\n0.6\n0.68 MNIST\nN M\nI\n0.79\n0.8\n0.81\n0.82\n0.83\n0.84\n0.85 OD\nN M\nI\n0.52\n0.54\n0.56\n0.58\n0.6\n0.62\n0.64\n0.6\nLS\nN M\nI\n0.71\n0.72\n0.73\n0.74\n0.75\n0.76\n0.7 ISOLET\nN M\nI\n0.56\n0.58\n0.6\n0.62\n0.64\n0.6\n0.68\n0.7 USPS\nN M\nI\n0.41\n0.42\n0.43\n0.4\n0.45\n0.46 LR\n(b) MF\nl i\nN M\nI\n.\n.\n.\n.\n.\n.\n. i\n-\nl i\nN M\nI\n.\n.\n.\n.\n.\n.\n.\n.\nl i\nN M\nI\n.\n.\n.\n. I\nl i\nN M\nI\n.\n.\n.\n.\n.\n.\n.\nl i\nN M\nI\n.\n.\n.\n.\n.\n.\n. I\nN M\nI\n.\n.\n.\n.\n.\n.\n.\nN M\nI\n.\n.\n.\n.\n.\n.\n.\n.\nN M\nI\n.\n.\n.\n.\n.\n.\n. I\nN M\nI\n.\n.\n.\n.\n.\n.\n.\n.\nN M\nI\n.\n.\n.\n.\n.\n.\n(c) IS\nEnsemble size 10 20 30 40 50\nN M\nI\n0.58\n0.6\n0.62\n0.64\n0.6\n0.68\n0.7 semeion\nLWEA LWGP MCLA HBGF EAC SRS WCT WEAC GP-MGLA TOME\nEnsemble size 10 20 30 40 50\nN M\nI\n0.6\n0.62\n0.64\n0.6\n0.68\n0.7\n0.72\n0.74 MF\nEnsemble size 10 20 30 40 50\nN M\nI\n0.5\n0.6\n0.65\n0.7 IS\nEnsemble size 10 20 30 40 50\nN M\nI\n0.2\n0.2 5\n0.23\n0.235\n0.24\n0.245\n0.25 FCT\nEnsemble size 10 20 30 40 50\nN M\nI\n0.56\n0.58\n0.6\n0.62\n0.64\n0.6\n0.68 MNIST\nN M\nI\n0.79\n0.8\n0.81\n0.82\n0.83\n0.84\n0.85 OD\nN M\nI\n0.52\n0.54\n0.56\n0.58\n0.6\n0.62\n0.64\n0.6\nLS\nN M\nI\n0.71\n0.72\n0.73\n0.74\n0.75\n0.76\n0.7 ISOLET\nN M\nI\n0.56\n0.58\n0.6\n0.62\n0.64\n0.6\n0.68\n0.7 USPS\nN M\nI\n0.41\n0.42\n0.43\n0.4\n0.45\n0.46 LR\n(d) FCT\nEnsemble size 10 20 30 40 50\nN M\nI\n0.58\n0.6\n0.62\n0.64\n0.6\n0.68\n0.7 semeion\nLWEA LWGP MCLA HBGF EAC SR WCT WEAC GP-MGLA TOME\nEnsemble size 10 20 30 40 50\nN M\nI\n0.6\n0.62\n0.64\n0.6\n0.68\n0.7\n0.72\n0.74 MF\nEnsemble size 10 20 30 40 50\nN M\nI\n0.5\n0.6\n0.65\n0.7 IS\nEnsemble size 10 20 30 40 50\nN M\nI\n0.2\n0.2 5\n0.23\n0.235\n0.24\n0.245\n0.25 FCT\nEnsemble size 10 20 30 40 50\nN M\nI\n0.56\n0.58\n0.6\n0.62\n0.64\n0.6\n0.68 MNIST\nN M\nI\n0.79\n0.8\n0.81\n0.82\n0.83\n0.84\n0.85 OD\nN M\nI\n0.52\n0.54\n0.56\n0.58\n0.6\n0.62\n0.64\n0.6\nLS\nN M\nI\n0.71\n0.72\n0.73\n0.74\n0.75\n0.76\n0.7 ISOLET\nN M\nI\n0.56\n0.58\n0.6\n0.62\n0.64\n0.6\n0.68\n0.7 USP\nN M\nI\n0.41\n0.42\n0.43\n0.4\n0.45\n0.46 LR\n(e) MNIST\nEnsemble size 10 20 30 40 50\nN M\nI\n0.58\n0.6\n0.62\n0.64\n0.66\n0.68\n0.7 semeion\nLWEA LWGP MCLA HBGF EAC SRS WCT WEAC GP-MGLA TOME\nEnsemble size 10 20 30 40 50\nN M\nI\n0.6\n0.62\n0.64\n0.66\n0.68\n0.7\n0.72\n0.74 MF\nEnsemble size 10 20 30 40 50\nN M\nI\n0.55\n0.6\n0.65\n0.7 IS\nEnsemble size 10 20 30 40 50\nN M\nI\n0.22\n0.225\n0.23\n0.235\n0.24\n0.245\n0.25 FCT\nEnsemble size 10 20 30 40 50\nN M\nI\n0.56\n0.58\n0.6\n0.62\n0.64\n0.66\n0.68 MNIST\nEnsemble size 10 20 30 40 50\nN M\nI\n0.79\n0.8\n0.81\n0.82\n0.83\n0.84\n0.85 OD\nEnsemble size 10 20 30 40 50\nN M\nI\n0.5\n0.52\n0.54\n0.56\n0.58\n0.6\n0.62\n0.64\n0.66\nLS\nEnsemble size 10 20 30 40 50\nN M\nI\n0.7\n0.71\n0.72\n0.73\n0.74\n0.75\n0.76\n0.77 ISOLET\nEnsemble size 10 20 30 40 50\nN M\nI\n54\n56\n0.58\n0.6\n0.62\n0.64\n0.66\n0.68\n0.7 USPS\nEnsemble size 10 20 30 40 50\nN M\nI\n0.41\n0.42\n0.43\n0.44\n0.45\n0.46 LR\n(f) ODR\nEnsemble size 10 20 30 40 50\nN M\nI\n0.58\n0.6\n0.62\n0.64\n0.6\n0.68\n0.7 semeion\nLWEA LWGP MCLA HBGF EAC SRS WCT WEAC GP-MGLA TOME\nEnsemble size 10 20 30 40 50\nN M\nI\n0.6\n0.62\n0.64\n0.6\n0.68\n0.7\n0.72\n0.74 MF\nEnsemble size 10 20 30 40 50\nN M\nI\n0.5\n0.6\n0.65\n0.7 IS\nEnsemble size 10 20 30 40 50\nN M\nI\n0.2\n0.2 5\n0.23\n0.235\n0.24\n0.245\n0.25 FCT\nEnsemble size 10 20 30 40 50\nN M\nI\n0.56\n0.58\n0.6\n0.62\n0.64\n0.6\n0.68 MNIST\nEnsemble size 10 20 30 40 50\nN M\nI\n0.79\n0.8\n0.81\n0.82\n0.83\n0.84\n0.85 OD\nEnsemble size 10 20 30 40 50\nN M\nI\n0.5\n0.52\n0.54\n0.56\n0.58\n0.6\n0.62\n0.64\n0.6\nLS\nEnsemble size 10 20 30 40 50\nN M\nI\n0.7\n0.71\n0.72\n0.73\n0.74\n0.75\n0.76\n0.7 ISOLET\nEnsemble size 10 20 30 40 50\nN M\nI\n54\n56\n0.58\n0.6\n0.62\n0.64\n0.6\n0.68\n0.7 USPS\nEnsemble size 10 20 30 40 50\nN M\nI\n0.41\n0.42\n0.43\n0.4\n0.45\n0.46 LR\n(g) LS\nl i\nN M\nI\n.\n.\n.\n.\n.\n.\n. i\n-\nl i\nN M\nI\n.\n.\n.\n.\n.\n.\n.\n.\nl i\nN M\nI\n.\n.\n.\n. I\nl i\nN M\nI\n.\n.\n.\n.\n.\n.\n.\nl i\nN M\nI\n.\n.\n.\n.\n.\n.\n. I\nl i\nN M\nI\n.\n.\n.\n.\n.\n.\n.\nl i\nN M\nI\n.\n.\n.\n.\n.\n.\n.\n.\n.\nl i N\nM I\n.\n.\n.\n.\n.\n.\n.\n. I\nl i\nN M\nI\n.\n.\n.\n.\n.\n.\n.\n.\nl i\nN M\nI\n.\n.\n.\n.\n.\n.\n(h) ISOLET\nEnsemble size 10 20 30 40 50\nN M\nI\n0.58\n0.6\n0.62\n0.64\n0.6\n0.68\n0.7 semeion\nLWEA LWGP MCLA HBGF EAC SRS WCT WEAC GP-MGLA TOME\nEnsemble size 10 20 30 40 50\nN M\nI\n0.6\n0.62\n0.64\n0.6\n0.68\n0.7\n0.72\n0.74 MF\nEnsemble size 10 20 30 40 50\nN M\nI\n0.5\n0.6\n0.65\n0.7 IS\nEnsemble size 10 20 30 40 50\nN M\nI\n0.2\n0.2 5\n0.23\n0.235\n0.24\n0.245\n0.25 FCT\nEnsemble size 10 20 30 40 50\nN M\nI\n0.56\n0.58\n0.6\n0.62\n0.64\n0.6\n0.68 MNIST\nEnsemble size 10 20 30 40 50\nN M\nI\n0.79\n0.8\n0.81\n0.82\n0.83\n0.84\n0.85 OD\nEnsemble size 10 20 30 40 50\nN M\nI\n0.5\n0.52\n0.54\n0.56\n0.58\n0.6\n0.62\n0.64\n0.6\nLS\nEnsemble size 10 20 30 40 50\nN M\nI\n0.7\n0.71\n0.72\n0.73\n0.74\n0.75\n0.76\n0.7 ISOLET\nEnsemble size 10 20 30 40 50\nN M\nI\n54\n.56\n0.58\n0.6\n0.62\n0.64\n0.6\n0.68\n0.7 USPS\nEnsemble size 10 20 30 40 50\nN M\nI\n0.41\n0.42\n0.43\n0.4\n0.45\n0.46 LR\n(i) USPS\nEnsemble size 10 20 30 40 50\nN M\nI\n0.58\n0.6\n0.62\n0.64\n0.6\n0.68\n0.7 semeion\nLWEA LWGP MCLA HBGF EAC SR WCT WEAC GP-MGLA TOME\nEnsemble size 10 20 30 40 50\nN M\nI\n0.6\n0.62\n0.64\n0.6\n0.68\n0.7\n0.72\n0.74 MF\nEnsemble size 10 20 30 40 50\nN M\nI\n0.5\n0.6\n0.65\n0.7 IS\nEnsemble size 10 20 30 40 50\nN M\nI\n0.2\n0.2 5\n0.23\n0.235\n0.24\n0.245\n0.25 FCT\nEnsemble size 10 20 30 40 50\nN M\nI\n0.56\n0.58\n0.6\n0.62\n0.64\n0.6\n0.68 MNIST\nEnsemble size 10 20 30 40 50\nN M\nI\n0.79\n0.8\n0.81\n0.82\n0.83\n0.84\n0.85 OD\nEnsemble size 10 20 30 40 50\nN M\nI\n0.5\n0.52\n0.54\n0.56\n0.58\n0.6\n0.62\n0.64\n0.6\nLS\nEnsemble size 10 20 30 40 50\nN M\nI\n0.7\n0.71\n0.72\n0.73\n0.74\n0.75\n0.76\n0.7 ISOLET\nEnsemble size 10 20 30 40 50\nN M\nI\n54\n0.56\n0.58\n0.6\n0.62\n0.64\n0.6\n0.68\n0.7 USP\nEnsemble size 10 20 30 40 50\nN M\nI\n0.41\n0.42\n0.43\n0.4\n0.45\n0.46 LR\n(j) LR\nEnsemble size 10 20 30 40 50\nN M\nI\n0.58\n0.6\n0.62\n0.64\n0.66\n0.68\n0.7 semeion\nLWEA LWGP MCLA HBGF EAC SRS WCT WEAC GP-MGLA TOME\nEnsemble size 10 20 30 40 50\nN M\nI\n0.6\n0.62\n0.64\n0.66\n0.68\n0.7\n0.72\n0.74 MF\nEnsemble size 10 20 30 40 50\nN M\nI\n0.55\n0.6\n0.65\n0.7 IS\nEnsemble size 10 20 30 40 50\nN M\nI\n0 22\n0.225\n0.23\n0.235\n0.24\n0.245\n0.25 FCT\nEnsemble size 10 20 30 40 50\nN M\nI\n0.56\n0.58\n0.6\n0.62\n0.64\n0.66\n0.68 MNIST\nEnsemble size 10 20 30 40 50\nN M\nI\n0.79\n0.8\n0.81\n0.82\n0.83\n0.84\n0.85 OD\nEnsemble size 10 20 30 40 50\nN M\nI\n0.5\n0.52\n0.54\n0.56\n0.58\n0.6\n0.62\n0.64\n0.66\nLS\nEnsemble size 10 20 30 40 50\nN M\nI\n0.7\n0.71\n0.72\n0.73\n0.74\n0.75\n0.76\n0.77 ISOLET\nEnsemble size 10 20 30 40 50\nN M\nI\n0.54\n0.56\n0.58\n0.6\n0.62\n0.64\n0.66\n0.68\n0.7 USPS\nEnsemble size 10 20 30 40 50\nN M\nI\n0.41\n0.42\n0.43\n0.44\n0.45\n0.46 LR\nconsensus process and its efficacy heavily relies on some implicit assumptions on the data distribution, which places an unstable factor for the consensus performance of TOME. As shown in Table V, in comparison with the ten baseline methods, the proposed LWEA and LWGP methods yield overall the best performance on the benchmark datasets.\nIn Table VI, we provide a summary for the experimental results in Table V. With two comparisons (in terms of best-k and true-k respectively) on each benchmark dataset, there are totally 20 comparisons on the 10 benchmark datasets, each corresponding to a row in Table V. As shown in Table VI, out of the 20 comparisons, the LWEA method and the LWGP method are ranked in the top two 17 times and 18 times, respectively, while the best baseline approach is ranked in the top two only 2 times, which shows the significant advantage of our methods in consensus robustness across datasets over the baseline methods.\nTable VI also shows the average performances (in terms of NMI) across ten datasets for each method. It is noteworthy that each test method is run 100 times for each dataset (see Table V). Thereby the average NMI scores in Table VI are in fact obtained over totally 1, 000 runs across 10 benchmark datasets, which provides a good view about the robustness of different ensemble clustering methods. As shown in Table VI, in terms of best-k, the proposed LWEA and LWGP methods achieve the best two NMI scores of 0.622 and 0.623 respectively out of the twelve test methods, while the third best score is only 0.605, which is achieved by GP-MGLA and significantly lower than the proposed methods. In terms of true-k, the proposed methods still exhibit a significant advantage in the average NMI scores across the ten datasets. The summary statistics in Table VI demonstrate the superiority of our methods with regard to the clustering accuracy and robustness over a large number of runs across various datasets.\nEnsemble size 10 15 20 25 30 35 40 45 50\nN M\nI\n0.57\n0.58\n0.59\n0.6\n0.61\n0.62\n0.63\nLWEA LWGP MCLA HBGF EAC WCT WEAC GP-MGLA TOME\nFig. 7. Average performances (of different methods) over ten datasets with varying ensemble sizes M . The illustration is obtained by averaging the ten sub-figures in Fig. 6, i.e., the sub-figures from Fig. 6(a) to Fig. 6(j)."}, {"heading": "E. Robustness to Ensemble Sizes M", "text": "Further, we evaluate the performances of our methods and the baseline methods with varying ensemble sizes M . For each ensemble size M , we run the proposed methods and the baseline methods 20 times on each benchmark dataset, with the ensemble of M base clusterings randomly selected at each time. Then we illustrate the average performances (over 20 runs) of different methods with varying ensemble sizes in Fig. 6. In the MF dataset, the TOME method yields the best performance, whereas LWGP and LWEA yield the second and third best scores, respectively. But in all of the other nine datasets, the proposed methods significantly outperform the TOME method. As shown in Fig. 6, compared with the baseline methods, the proposed methods achieve overall the most consistent and robust performances with different\n11\nensemble sizes on the benchmark datasets. Additionally, we illustrate the average performances (of different methods) over ten datasets in Fig. 7. In fact, Fig. 7 is the average of the ten sub-figures in Fig. 6, ranging from Fig. 6(a) to Fig. 6(j). As can be seen in Fig. 7, in terms of the average NMI scores over ten datasets with varying ensemble sizes, the proposed LWGP and LWEA methods achieve the best consensus performances and exhibit significant advantages over the baseline methods."}, {"heading": "F. Execution Time", "text": "In this section, we compare the execution time of different ensemble clustering methods with varying data sizes. The experiments are performed on different subsets of the LR dataset. The LR dataset consists of totally 20, 000 data objects. When testing the data size of N \u2032, we randomly select a subset of N \u2032 objects from the LR dataset and run different methods on this subset to evaluate their execution time. As illustrated in Fig. 8, the proposed LWEA method requires 75.20 seconds to process the entire LR dataset, which is comparable to GPMGLA but much faster than CSPA, WCT, SRS, and TOME. Out of the totally twelve test methods, the MCLA method is the fastest method, while the proposed LWGP method is the second fastest method. The MCLA method and the proposed LWGP method consume 5.31 seconds and 8.74 seconds respectively to process the entire LR dataset. Note that, although the proposed LWGP method is slightly slower than MCLA (but faster than all of the other ten test methods), yet it significantly outperforms MCLA in clustering accuracy and robustness on the benchmark datasets (see Tables V and VI and Figs. 6 and 7).\nTo summarize, as shown in the experimental results on various datasets (see Tables V and VI and Figs. 6, 7, and 8), the proposed LWEA and LWGP methods yield significantly better consensus performances than the baseline methods while exhibiting competitive efficiency."}, {"heading": "VI. CONCLUSION", "text": "In this paper, we have proposed a novel ensemble clustering approach based on ensemble-driven cluster uncertainty estimation and local weighting strategy. We propose to estimate the\nuncertainty of clusters by considering the cluster labels in the entire ensemble based on an entropic criterion, and devise a new ensemble-driven cluster validity index termed ECI. The ECI measure requires no access to the original data features and makes no assumptions on the data distribution. Then, a local weighting scheme is presented to extend the conventional CA matrix into the LWCA matrix via the ECI measure. With the reliability of clusters investigated and the local diversity in ensembles exploited, we further propose two novel consensus functions, termed LWEA and LWGP, respectively. We have conducted extensive experiments on a variety of real-world datasets. The experimental results have shown the superiority of the proposed approach in clustering quality and efficiency when compared to the state-of-the-art approaches."}], "references": [{"title": "Data clustering: 50 years beyond k-means", "author": ["A.K. Jain"], "venue": "Pattern Recognition Letters, vol. 31, no. 8, pp. 651\u2013666, 2010.", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2010}, {"title": "Rival penalized competitive learning for clustering analysis, RBF net, and curve detection", "author": ["L. Xu", "A. Krzyzak", "E. Oja"], "venue": "IEEE Transactions on Neural Networks, vol. 4, no. 4, pp. 636\u2013649, 1993.", "citeRegEx": "2", "shortCiteRegEx": null, "year": 1993}, {"title": "On spectral clustering: Analysis and an algorithm", "author": ["A.Y. Ng", "M.I. Jordan", "Y. Weiss"], "venue": "Advances in Neural Information Processing Systems (NIPS), 2002, pp. 849\u2013856.", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2002}, {"title": "Clustering by passing messages between data points", "author": ["B.J. Frey", "D. Dueck"], "venue": "Science, vol. 315, pp. 972\u2013976, 2007.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2007}, {"title": "Approximate pairwise clustering for large data sets via sampling plus extension", "author": ["L. Wang", "C. Leckie", "R. Kotagiri", "J. Bezdek"], "venue": "Pattern Recognition, vol. 44, no. 2, pp. 222\u2013235, 2011.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2011}, {"title": "Graph-based multiprototype competitive learning and its applications", "author": ["C.-D. Wang", "J.-H. Lai", "J.-Y. Zhu"], "venue": "IEEE Transactions on Systems, Man, and Cybernetics, Part C: Applications and Reviews, vol. 42, no. 6, pp. 934\u2013946, 2012.", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2012}, {"title": "SVStream: A support vector based algorithm for clustering data streams", "author": ["C.-D. Wang", "J.-H. Lai", "D. Huang", "W.-S. Zheng"], "venue": "IEEE Transactions on Knowledge and Data Engineering, vol. 25, no. 6, pp. 1410\u20131424, 2013.", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2013}, {"title": "Multi-exemplar affinity propagation", "author": ["C.-D. Wang", "J.-H. Lai", "C.Y. Suen", "J.-Y. Zhu"], "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence, vol. 35, no. 9, pp. 2223\u20132237, 2013.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2013}, {"title": "Constraint neighborhood projections for semi-supervised clustering", "author": ["H. Wang", "T. Li", "T. Li", "Y. Yang"], "venue": "IEEE Transactions on Cybernetics, vol. 44, no. 5, pp. 636\u2013643, 2014.", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2014}, {"title": "Multitask spectral clustering by exploring intertask correlation", "author": ["Y. Yang", "Z. Ma", "Y. Yang", "F. Nie", "H.T. Shen"], "venue": "IEEE Transactions on Cybernetics, vol. 45, no. 5, pp. 1083\u20131094, 2015.", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2015}, {"title": "Joint local and global consistency on interdocument and interword relationships for co-clustering", "author": ["B.-K. Bao", "W. Min", "T. Li", "C. Xu"], "venue": "IEEE Transactions on Cybernetics, vol. 45, no. 1, pp. 15\u201328, 2015.", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2015}, {"title": "Multi-view clustering based on belief propagation", "author": ["C.-D. Wang", "J.-H. Lai", "P.S. Yu"], "venue": "IEEE Transactions on Knowledge and Data Engineering, in press, 2016.", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2016}, {"title": "A hybrid approach to clustering in big data", "author": ["D. Kumar", "J. Bezdek", "M. Palaniswami", "S. Rajasegarar", "C. Leckie", "T. Havens"], "venue": "IEEE Transactions on Cybernetics, in press, 2016.", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2016}, {"title": "Cluster ensembles: A knowledge reuse framework for combining multiple partitions", "author": ["A. Strehl", "J. Ghosh"], "venue": "Journal of Machine Learning Research, vol. 3, pp. 583\u2013617, 2003.", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2003}, {"title": "Finding median partitions using information-theoretical-based genetic algorithms", "author": ["D. Cristofor", "D. Simovici"], "venue": "Journal of Universal Computer Science, vol. 8, no. 2, pp. 153\u2013172, 2002.", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2002}, {"title": "Solving cluster ensemble problems by bipartite graph partitioning", "author": ["X.Z. Fern", "C.E. Brodley"], "venue": "Proc. of International Conference on Machine Learning (ICML), 2004.", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2004}, {"title": "Combining multiple clusterings using evidence accumulation", "author": ["A.L.N. Fred", "A.K. Jain"], "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence, vol. 27, no. 6, pp. 835\u2013850, 2005.", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2005}, {"title": "Clustering ensembles: models of consensus and weak partitions", "author": ["A. Topchy", "A.K. Jain", "W. Punch"], "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence, vol. 27, no. 12, pp. 1866\u20131881, 2005.  12", "citeRegEx": "18", "shortCiteRegEx": null, "year": 1866}, {"title": "Clustering aggregation by probability accumulation", "author": ["X. Wang", "C. Yang", "J. Zhou"], "venue": "Pattern Recognition, vol. 42, no. 5, pp. 668\u2013675, 2009.", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2009}, {"title": "A link-based approach to the cluster ensemble problem", "author": ["N. Iam-On", "T. Boongoen", "S. Garrett", "C. Price"], "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence, vol. 33, no. 12, pp. 2396\u2013 2409, 2011.", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2011}, {"title": "CA-Tree: A hierarchical structure for efficient and scalable coassociation-based cluster ensembles", "author": ["T. Wang"], "venue": "IEEE Transactions on Systems, Man, and Cybernetics, Part B: Cybernetics, vol. 41, no. 3, pp. 686\u2013698, 2011.", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2011}, {"title": "Ensemble clustering by means of clustering embedding in vector spaces", "author": ["L. Franek", "X. Jiang"], "venue": "Pattern Recognition, vol. 47, no. 2, pp. 833\u2013842, 2014.", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2014}, {"title": "K-means-based consensus clustering: A unified view", "author": ["J. Wu", "H. Liu", "H. Xiong", "J. Cao", "J. Chen"], "venue": "IEEE Transactions on Knowledge and Data Engineering, vol. 27, no. 1, pp. 155\u2013169, 2015.", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2015}, {"title": "Efficient clustering aggregation based on data fragments", "author": ["O. Wu", "W. Hu", "S. Maybank", "M. Zhu", "B. Li"], "venue": "IEEE Transactions on Systems, Man, and Cybernetics, Part B: Cybernetics, vol. 42, no. 3, pp. 913\u2013926, 2012.", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2012}, {"title": "Adaptive noise immune cluster ensemble using affinity propagation", "author": ["Z. Yu", "L. Li", "J. Liu", "J. Zhang", "G. Han"], "venue": "IEEE Transactions on Knowledge and Data Engineering, vol. 27, no. 12, pp. 3176\u20133189, 2015.", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2015}, {"title": "A theoretic framework of k-meansbased consensus clustering", "author": ["J. Wu", "H. Liu", "H. Xiong", "J. Cao"], "venue": "Proc. of International Joint Conference on Artificial Intelligence, 2013.", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2013}, {"title": "Weighted consensus clustering", "author": ["T. Li", "C. Ding"], "venue": "Proc. of SIAM International Conference on Data Mining (SDM), 2008.", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2008}, {"title": "Hybrid clustering solution selection strategy", "author": ["Z. Yu", "L. Li", "Y. Gao", "J. You", "J. Liu", "H.-S. Wong", "G. Han"], "venue": "Pattern Recognition, vol. 47, no. 10, pp. 3362\u20133375, 2014.", "citeRegEx": "28", "shortCiteRegEx": null, "year": 2014}, {"title": "Combining multiple clusterings via crowd agreement estimation and multi-granularity link analysis", "author": ["D. Huang", "J.-H. Lai", "C.-D. Wang"], "venue": "Neurocomputing, vol. 170, pp. 240\u2013250, 2015.", "citeRegEx": "29", "shortCiteRegEx": null, "year": 2015}, {"title": "A clustering ensemble: Two-level-refined co-association matrix with path-based transformation", "author": ["C. Zhong", "X. Yue", "Z. Zhang", "J. Lei"], "venue": "Pattern Recognition, vol. 48, no. 8, pp. 2699\u2013 2709, 2015.", "citeRegEx": "30", "shortCiteRegEx": null, "year": 2015}, {"title": "On the point for which the sum of the distances to n given points is minimum", "author": ["E. Weiszfeld", "F. Plastria"], "venue": "Annals of Operations Research, vol. 167, no. 1, pp. 7\u201341, 2009.", "citeRegEx": "31", "shortCiteRegEx": null, "year": 2009}, {"title": "To improve the quality of cluster ensembles by selecting a subset of base clusters", "author": ["H. Alizadeh", "B. Minaei-Bidgoli", "H. Parvin"], "venue": "Journal of Experimental & Theoretical Artificial Intelligence, vol. 26, no. 1, pp. 127\u2013150, 2014.", "citeRegEx": "32", "shortCiteRegEx": null, "year": 2014}, {"title": "Ensemble clustering using factor graph", "author": ["D. Huang", "J.-H. Lai", "C.-D. Wang"], "venue": "Pattern Recognition, vol. 50, pp. 131\u2013142, 2016.", "citeRegEx": "33", "shortCiteRegEx": null, "year": 2016}, {"title": "Elements of Information Theory, 2nd ed", "author": ["T.M. Cover", "J.A. Thomas"], "venue": null, "citeRegEx": "34", "shortCiteRegEx": "34", "year": 2006}, {"title": "Robust ensemble clustering by matrix completion", "author": ["J. Yi", "T. Yang", "R. Jin", "A.K. Jain"], "venue": "Proc. of IEEE International Conference on Data Mining (ICDM), 2012.", "citeRegEx": "35", "shortCiteRegEx": null, "year": 2012}, {"title": "Exploiting the wisdom of crowd: A multi-granularity approach to clustering ensemble", "author": ["D. Huang", "J.-H. Lai", "C.-D. Wang"], "venue": "Proc. of International Conference on Intelligence Science and Big Data Engineering (IScIDE), 2013, pp. 112\u2013119.", "citeRegEx": "36", "shortCiteRegEx": null, "year": 2013}, {"title": "Weighted partition consensus via kernels", "author": ["S. Vega-Pons", "J. Correa-Morris", "J. Ruiz-Shulcloper"], "venue": "Pattern Recognition, vol. 43, no. 8, pp. 2712\u20132724, 2010.", "citeRegEx": "37", "shortCiteRegEx": null, "year": 2010}, {"title": "Weighted association based methods for the combination of heterogeneous partitions", "author": ["S. Vega-Pons", "J. Ruiz-Shulcloper", "A. Guerra-Gand\u00f3n"], "venue": "Pattern Recognition Letters, vol. 32, no. 16, pp. 2163\u20132170, 2011.", "citeRegEx": "38", "shortCiteRegEx": null, "year": 2011}, {"title": "A survey of clustering ensemble algorithms", "author": ["S. Vega-Pons", "J. Ruiz-Shulcloper"], "venue": "International Journal of Pattern Recognition and Artificial Intelligence, vol. 25, no. 3, pp. 337\u2013372, 2011.", "citeRegEx": "39", "shortCiteRegEx": null, "year": 2011}, {"title": "Combining multiple clusterings using similarity graph", "author": ["S. Mimaroglu", "E. Erdil"], "venue": "Pattern Recognition, vol. 44, no. 3, pp. 694\u2013703, 2011.", "citeRegEx": "40", "shortCiteRegEx": null, "year": 2011}, {"title": "Segmentation using superpixels: A bipartite graph partitioning approach", "author": ["Z. Li", "X.-M. Wu", "S.-F. Chang"], "venue": "Proc. of IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2012.", "citeRegEx": "41", "shortCiteRegEx": null, "year": 2012}, {"title": "Gradient-based learning applied to document recognition", "author": ["Y. LeCun", "L. Bottou", "Y. Bengio", "P. Haffner"], "venue": "Proceedings of the IEEE, vol. 86, no. 11, pp. 2278\u20132324, 1998.", "citeRegEx": "42", "shortCiteRegEx": null, "year": 1998}, {"title": "Affinity propagation: Clustering data by passing messages", "author": ["D. Dueck"], "venue": "Ph.D. dissertation, University of Toronto, 2009.", "citeRegEx": "43", "shortCiteRegEx": null, "year": 2009}, {"title": "UCI machine learning repository", "author": ["K. Bache", "M. Lichman"], "venue": "2013. [Online]. Available: http://archive.ics.uci.edu/ml", "citeRegEx": "44", "shortCiteRegEx": null, "year": 2013}], "referenceMentions": [{"referenceID": 0, "context": "INTRODUCTION DATA clustering is a fundamental yet very challenging problem in the field of data mining and machine learning [1].", "startOffset": 124, "endOffset": 127}, {"referenceID": 1, "context": "During the past few decades, a large number of clustering algorithms have been developed by exploiting various techniques [2], [3], [4], [5], [6], [7], [8], [9], [10], [11], [12], [13].", "startOffset": 122, "endOffset": 125}, {"referenceID": 2, "context": "During the past few decades, a large number of clustering algorithms have been developed by exploiting various techniques [2], [3], [4], [5], [6], [7], [8], [9], [10], [11], [12], [13].", "startOffset": 127, "endOffset": 130}, {"referenceID": 3, "context": "During the past few decades, a large number of clustering algorithms have been developed by exploiting various techniques [2], [3], [4], [5], [6], [7], [8], [9], [10], [11], [12], [13].", "startOffset": 132, "endOffset": 135}, {"referenceID": 4, "context": "During the past few decades, a large number of clustering algorithms have been developed by exploiting various techniques [2], [3], [4], [5], [6], [7], [8], [9], [10], [11], [12], [13].", "startOffset": 137, "endOffset": 140}, {"referenceID": 5, "context": "During the past few decades, a large number of clustering algorithms have been developed by exploiting various techniques [2], [3], [4], [5], [6], [7], [8], [9], [10], [11], [12], [13].", "startOffset": 142, "endOffset": 145}, {"referenceID": 6, "context": "During the past few decades, a large number of clustering algorithms have been developed by exploiting various techniques [2], [3], [4], [5], [6], [7], [8], [9], [10], [11], [12], [13].", "startOffset": 147, "endOffset": 150}, {"referenceID": 7, "context": "During the past few decades, a large number of clustering algorithms have been developed by exploiting various techniques [2], [3], [4], [5], [6], [7], [8], [9], [10], [11], [12], [13].", "startOffset": 152, "endOffset": 155}, {"referenceID": 8, "context": "During the past few decades, a large number of clustering algorithms have been developed by exploiting various techniques [2], [3], [4], [5], [6], [7], [8], [9], [10], [11], [12], [13].", "startOffset": 157, "endOffset": 160}, {"referenceID": 9, "context": "During the past few decades, a large number of clustering algorithms have been developed by exploiting various techniques [2], [3], [4], [5], [6], [7], [8], [9], [10], [11], [12], [13].", "startOffset": 162, "endOffset": 166}, {"referenceID": 10, "context": "During the past few decades, a large number of clustering algorithms have been developed by exploiting various techniques [2], [3], [4], [5], [6], [7], [8], [9], [10], [11], [12], [13].", "startOffset": 168, "endOffset": 172}, {"referenceID": 11, "context": "During the past few decades, a large number of clustering algorithms have been developed by exploiting various techniques [2], [3], [4], [5], [6], [7], [8], [9], [10], [11], [12], [13].", "startOffset": 174, "endOffset": 178}, {"referenceID": 12, "context": "During the past few decades, a large number of clustering algorithms have been developed by exploiting various techniques [2], [3], [4], [5], [6], [7], [8], [9], [10], [11], [12], [13].", "startOffset": 180, "endOffset": 184}, {"referenceID": 13, "context": "To exploit the complementary and rich information in multiple clusterings, the ensemble clustering technique has emerged as a powerful tool for data clustering and has been attracting increasing attention in recent years [14], [15], [16], [17], [18], [19], [20], [21], [22], [23], [24], [25].", "startOffset": 221, "endOffset": 225}, {"referenceID": 14, "context": "To exploit the complementary and rich information in multiple clusterings, the ensemble clustering technique has emerged as a powerful tool for data clustering and has been attracting increasing attention in recent years [14], [15], [16], [17], [18], [19], [20], [21], [22], [23], [24], [25].", "startOffset": 227, "endOffset": 231}, {"referenceID": 15, "context": "To exploit the complementary and rich information in multiple clusterings, the ensemble clustering technique has emerged as a powerful tool for data clustering and has been attracting increasing attention in recent years [14], [15], [16], [17], [18], [19], [20], [21], [22], [23], [24], [25].", "startOffset": 233, "endOffset": 237}, {"referenceID": 16, "context": "To exploit the complementary and rich information in multiple clusterings, the ensemble clustering technique has emerged as a powerful tool for data clustering and has been attracting increasing attention in recent years [14], [15], [16], [17], [18], [19], [20], [21], [22], [23], [24], [25].", "startOffset": 239, "endOffset": 243}, {"referenceID": 17, "context": "To exploit the complementary and rich information in multiple clusterings, the ensemble clustering technique has emerged as a powerful tool for data clustering and has been attracting increasing attention in recent years [14], [15], [16], [17], [18], [19], [20], [21], [22], [23], [24], [25].", "startOffset": 245, "endOffset": 249}, {"referenceID": 18, "context": "To exploit the complementary and rich information in multiple clusterings, the ensemble clustering technique has emerged as a powerful tool for data clustering and has been attracting increasing attention in recent years [14], [15], [16], [17], [18], [19], [20], [21], [22], [23], [24], [25].", "startOffset": 251, "endOffset": 255}, {"referenceID": 19, "context": "To exploit the complementary and rich information in multiple clusterings, the ensemble clustering technique has emerged as a powerful tool for data clustering and has been attracting increasing attention in recent years [14], [15], [16], [17], [18], [19], [20], [21], [22], [23], [24], [25].", "startOffset": 257, "endOffset": 261}, {"referenceID": 20, "context": "To exploit the complementary and rich information in multiple clusterings, the ensemble clustering technique has emerged as a powerful tool for data clustering and has been attracting increasing attention in recent years [14], [15], [16], [17], [18], [19], [20], [21], [22], [23], [24], [25].", "startOffset": 263, "endOffset": 267}, {"referenceID": 21, "context": "To exploit the complementary and rich information in multiple clusterings, the ensemble clustering technique has emerged as a powerful tool for data clustering and has been attracting increasing attention in recent years [14], [15], [16], [17], [18], [19], [20], [21], [22], [23], [24], [25].", "startOffset": 269, "endOffset": 273}, {"referenceID": 22, "context": "To exploit the complementary and rich information in multiple clusterings, the ensemble clustering technique has emerged as a powerful tool for data clustering and has been attracting increasing attention in recent years [14], [15], [16], [17], [18], [19], [20], [21], [22], [23], [24], [25].", "startOffset": 275, "endOffset": 279}, {"referenceID": 23, "context": "To exploit the complementary and rich information in multiple clusterings, the ensemble clustering technique has emerged as a powerful tool for data clustering and has been attracting increasing attention in recent years [14], [15], [16], [17], [18], [19], [20], [21], [22], [23], [24], [25].", "startOffset": 281, "endOffset": 285}, {"referenceID": 24, "context": "To exploit the complementary and rich information in multiple clusterings, the ensemble clustering technique has emerged as a powerful tool for data clustering and has been attracting increasing attention in recent years [14], [15], [16], [17], [18], [19], [20], [21], [22], [23], [24], [25].", "startOffset": 287, "endOffset": 291}, {"referenceID": 25, "context": "Ensemble clustering aims to combine multiple clusterings to obtain a probably better and more robust clustering result, which has shown advantages in finding bizarre clusters, dealing with noise, and integrating clustering solutions from multiple distributed sources [26].", "startOffset": 267, "endOffset": 271}, {"referenceID": 26, "context": "To deal with low-quality base clusterings, some efforts have been made to evaluate and weight the base clusterings to enhance the consensus performance [27], [28], [29].", "startOffset": 152, "endOffset": 156}, {"referenceID": 27, "context": "To deal with low-quality base clusterings, some efforts have been made to evaluate and weight the base clusterings to enhance the consensus performance [27], [28], [29].", "startOffset": 158, "endOffset": 162}, {"referenceID": 28, "context": "To deal with low-quality base clusterings, some efforts have been made to evaluate and weight the base clusterings to enhance the consensus performance [27], [28], [29].", "startOffset": 164, "endOffset": 168}, {"referenceID": 26, "context": "However, these approaches [27], [28], [29] are developed based on an implicit assumption that all of the clusters in the same base clustering have the same reliability.", "startOffset": 26, "endOffset": 30}, {"referenceID": 27, "context": "However, these approaches [27], [28], [29] are developed based on an implicit assumption that all of the clusters in the same base clustering have the same reliability.", "startOffset": 32, "endOffset": 36}, {"referenceID": 28, "context": "However, these approaches [27], [28], [29] are developed based on an implicit assumption that all of the clusters in the same base clustering have the same reliability.", "startOffset": 38, "endOffset": 42}, {"referenceID": 26, "context": "They typically treat each base clustering as an individual and assign a weight to each base clustering regardless of the diversity of the clusters inside it [27], [28], [29].", "startOffset": 157, "endOffset": 161}, {"referenceID": 27, "context": "They typically treat each base clustering as an individual and assign a weight to each base clustering regardless of the diversity of the clusters inside it [27], [28], [29].", "startOffset": 163, "endOffset": 167}, {"referenceID": 28, "context": "They typically treat each base clustering as an individual and assign a weight to each base clustering regardless of the diversity of the clusters inside it [27], [28], [29].", "startOffset": 169, "endOffset": 173}, {"referenceID": 29, "context": "[30] proposed to evaluate the reliability of clusters by considering the Euclidean distances between data objects in clusters.", "startOffset": 0, "endOffset": 4}, {"referenceID": 29, "context": "The method in [30] requires access to the original data features, and its efficacy heavily relies on the data distribution of the dataset.", "startOffset": 14, "endOffset": 18}, {"referenceID": 16, "context": ", the pair-wise cooccurrence based approaches [17], [19], [20], [21], the graph partitioning based approaches [14], [16], and the median partition based approaches [15], [18], [22].", "startOffset": 46, "endOffset": 50}, {"referenceID": 18, "context": ", the pair-wise cooccurrence based approaches [17], [19], [20], [21], the graph partitioning based approaches [14], [16], and the median partition based approaches [15], [18], [22].", "startOffset": 52, "endOffset": 56}, {"referenceID": 19, "context": ", the pair-wise cooccurrence based approaches [17], [19], [20], [21], the graph partitioning based approaches [14], [16], and the median partition based approaches [15], [18], [22].", "startOffset": 58, "endOffset": 62}, {"referenceID": 20, "context": ", the pair-wise cooccurrence based approaches [17], [19], [20], [21], the graph partitioning based approaches [14], [16], and the median partition based approaches [15], [18], [22].", "startOffset": 64, "endOffset": 68}, {"referenceID": 13, "context": ", the pair-wise cooccurrence based approaches [17], [19], [20], [21], the graph partitioning based approaches [14], [16], and the median partition based approaches [15], [18], [22].", "startOffset": 110, "endOffset": 114}, {"referenceID": 15, "context": ", the pair-wise cooccurrence based approaches [17], [19], [20], [21], the graph partitioning based approaches [14], [16], and the median partition based approaches [15], [18], [22].", "startOffset": 116, "endOffset": 120}, {"referenceID": 14, "context": ", the pair-wise cooccurrence based approaches [17], [19], [20], [21], the graph partitioning based approaches [14], [16], and the median partition based approaches [15], [18], [22].", "startOffset": 164, "endOffset": 168}, {"referenceID": 17, "context": ", the pair-wise cooccurrence based approaches [17], [19], [20], [21], the graph partitioning based approaches [14], [16], and the median partition based approaches [15], [18], [22].", "startOffset": 170, "endOffset": 174}, {"referenceID": 21, "context": ", the pair-wise cooccurrence based approaches [17], [19], [20], [21], the graph partitioning based approaches [14], [16], and the median partition based approaches [15], [18], [22].", "startOffset": 176, "endOffset": 180}, {"referenceID": 16, "context": "The pair-wise co-occurrence based approaches [17], [19], [20], [21] typically construct a co-association (CA) matrix by considering how many times two objects occur in the same cluster among the multiple base clusterings.", "startOffset": 45, "endOffset": 49}, {"referenceID": 18, "context": "The pair-wise co-occurrence based approaches [17], [19], [20], [21] typically construct a co-association (CA) matrix by considering how many times two objects occur in the same cluster among the multiple base clusterings.", "startOffset": 51, "endOffset": 55}, {"referenceID": 19, "context": "The pair-wise co-occurrence based approaches [17], [19], [20], [21] typically construct a co-association (CA) matrix by considering how many times two objects occur in the same cluster among the multiple base clusterings.", "startOffset": 57, "endOffset": 61}, {"referenceID": 20, "context": "The pair-wise co-occurrence based approaches [17], [19], [20], [21] typically construct a co-association (CA) matrix by considering how many times two objects occur in the same cluster among the multiple base clusterings.", "startOffset": 63, "endOffset": 67}, {"referenceID": 0, "context": "By exploiting the CA matrix as the similarity matrix, the conventional clustering techniques, such as the agglomerative clustering methods [1], can be exploited to build the final clustering result.", "startOffset": 139, "endOffset": 142}, {"referenceID": 16, "context": "Fred and Jain [17] for the first time presented the concept of CA matrix and proposed the evidence accumulation clustering (EAC) method.", "startOffset": 14, "endOffset": 18}, {"referenceID": 18, "context": "[19] extended the EAC method by taking the sizes of clusters into consideration, and proposed the probability accumulation method.", "startOffset": 0, "endOffset": 4}, {"referenceID": 19, "context": "[20] refined the CA matrix by considering the shared neighbors between clusters to improve the consensus results.", "startOffset": 0, "endOffset": 4}, {"referenceID": 20, "context": "Wang [21] introduced a dendrogram-like hierarchical data structure termed CA-tree to facilitate the co-association based ensemble clustering process.", "startOffset": 5, "endOffset": 9}, {"referenceID": 13, "context": "The graph partitioning based approaches [14], [16] address the ensemble clustering problem by constructing a graph model to reflect the ensemble information.", "startOffset": 40, "endOffset": 44}, {"referenceID": 15, "context": "The graph partitioning based approaches [14], [16] address the ensemble clustering problem by constructing a graph model to reflect the ensemble information.", "startOffset": 46, "endOffset": 50}, {"referenceID": 13, "context": "Strehl and Ghosh [14] proposed three graph partitioning based ensemble clustering algorithms, i.", "startOffset": 17, "endOffset": 21}, {"referenceID": 15, "context": "Fern and Brodley [16] constructed a bipartite graph for the clustering ensemble by treating both clusters and objects as graph nodes, and obtain the consensus clustering by partitioning the bipartite graph.", "startOffset": 17, "endOffset": 21}, {"referenceID": 14, "context": "The median partition based approaches [15], [18], [22] formulate the ensemble clustering problem into an optimization", "startOffset": 38, "endOffset": 42}, {"referenceID": 17, "context": "The median partition based approaches [15], [18], [22] formulate the ensemble clustering problem into an optimization", "startOffset": 44, "endOffset": 48}, {"referenceID": 21, "context": "The median partition based approaches [15], [18], [22] formulate the ensemble clustering problem into an optimization", "startOffset": 50, "endOffset": 54}, {"referenceID": 17, "context": "The median partition problem is NPhard [18].", "startOffset": 39, "endOffset": 43}, {"referenceID": 14, "context": "Cristofor and Simovici [15] proposed to obtain an approximate solution using the genetic algorithm, where clusterings are treated as chromosomes.", "startOffset": 23, "endOffset": 27}, {"referenceID": 17, "context": "[18] cast the median partition problem into a maximum likelihood problem and approximately solve it by the EM algorithm.", "startOffset": 0, "endOffset": 4}, {"referenceID": 21, "context": "Franek and Jiang [22] cast the median partition problem into an Euclidean median problem by clustering embedding in vector spaces.", "startOffset": 17, "endOffset": 21}, {"referenceID": 30, "context": "The median vector is found by the Weiszfeld algorithm [31] and then transformed into a clustering again, which is treated as the consensus clustering.", "startOffset": 54, "endOffset": 58}, {"referenceID": 13, "context": "These algorithms attempt to solve the ensemble clustering problem in various ways [14], [15], [16], [17], [18], [19], [20], [21], [22].", "startOffset": 82, "endOffset": 86}, {"referenceID": 14, "context": "These algorithms attempt to solve the ensemble clustering problem in various ways [14], [15], [16], [17], [18], [19], [20], [21], [22].", "startOffset": 88, "endOffset": 92}, {"referenceID": 15, "context": "These algorithms attempt to solve the ensemble clustering problem in various ways [14], [15], [16], [17], [18], [19], [20], [21], [22].", "startOffset": 94, "endOffset": 98}, {"referenceID": 16, "context": "These algorithms attempt to solve the ensemble clustering problem in various ways [14], [15], [16], [17], [18], [19], [20], [21], [22].", "startOffset": 100, "endOffset": 104}, {"referenceID": 17, "context": "These algorithms attempt to solve the ensemble clustering problem in various ways [14], [15], [16], [17], [18], [19], [20], [21], [22].", "startOffset": 106, "endOffset": 110}, {"referenceID": 18, "context": "These algorithms attempt to solve the ensemble clustering problem in various ways [14], [15], [16], [17], [18], [19], [20], [21], [22].", "startOffset": 112, "endOffset": 116}, {"referenceID": 19, "context": "These algorithms attempt to solve the ensemble clustering problem in various ways [14], [15], [16], [17], [18], [19], [20], [21], [22].", "startOffset": 118, "endOffset": 122}, {"referenceID": 20, "context": "These algorithms attempt to solve the ensemble clustering problem in various ways [14], [15], [16], [17], [18], [19], [20], [21], [22].", "startOffset": 124, "endOffset": 128}, {"referenceID": 21, "context": "These algorithms attempt to solve the ensemble clustering problem in various ways [14], [15], [16], [17], [18], [19], [20], [21], [22].", "startOffset": 130, "endOffset": 134}, {"referenceID": 26, "context": "To partially address this limitation, recently some weighted ensemble clustering approaches have been presented [27], [28], [29].", "startOffset": 112, "endOffset": 116}, {"referenceID": 27, "context": "To partially address this limitation, recently some weighted ensemble clustering approaches have been presented [27], [28], [29].", "startOffset": 118, "endOffset": 122}, {"referenceID": 28, "context": "To partially address this limitation, recently some weighted ensemble clustering approaches have been presented [27], [28], [29].", "startOffset": 124, "endOffset": 128}, {"referenceID": 26, "context": "Li and Ding [27] cast the ensemble clustering problem into a nonnegative matrix factorization problem and proposed a weighted consensus clustering approach, where each base clustering is assigned a weight in order to improve the consensus result.", "startOffset": 12, "endOffset": 16}, {"referenceID": 27, "context": "[28] exploited the feature selection techniques to weight and select the base clusterings.", "startOffset": 0, "endOffset": 4}, {"referenceID": 27, "context": "In fact, clustering selection [28] can be viewed as a 0-1 weighting scheme, where 1 indicates selecting a clustering and 0 indicates removing a clustering.", "startOffset": 30, "endOffset": 34}, {"referenceID": 28, "context": "[29] proposed to evaluate and weight the base clusterings based on the normalized crowd agreement index (NCAI).", "startOffset": 0, "endOffset": 4}, {"referenceID": 26, "context": "Although these approaches [27], [28], [29] are able to estimate the reliability of base clusterings and weight them accordingly, yet they generally treat a base clustering as a whole and neglect the local diversity of clusters inside the same base clustering.", "startOffset": 26, "endOffset": 30}, {"referenceID": 27, "context": "Although these approaches [27], [28], [29] are able to estimate the reliability of base clusterings and weight them accordingly, yet they generally treat a base clustering as a whole and neglect the local diversity of clusters inside the same base clustering.", "startOffset": 32, "endOffset": 36}, {"referenceID": 28, "context": "Although these approaches [27], [28], [29] are able to estimate the reliability of base clusterings and weight them accordingly, yet they generally treat a base clustering as a whole and neglect the local diversity of clusters inside the same base clustering.", "startOffset": 38, "endOffset": 42}, {"referenceID": 31, "context": "[32] proposed to evaluate clusters in the ensemble by averaging normalized mutual information (NMI) [14] between clusterings, which results in a very expensive computational cost and is not feasible for large datasets.", "startOffset": 0, "endOffset": 4}, {"referenceID": 13, "context": "[32] proposed to evaluate clusters in the ensemble by averaging normalized mutual information (NMI) [14] between clusterings, which results in a very expensive computational cost and is not feasible for large datasets.", "startOffset": 100, "endOffset": 104}, {"referenceID": 29, "context": "[30] exploited the Euclidean distances between objects to estimate the cluster reliability, which needs access to the original data features and is only applicable to numerical data.", "startOffset": 0, "endOffset": 4}, {"referenceID": 13, "context": "However, in the more general formulation of ensemble clustering [14], [15], [16], [17], [18], [19], [20], [21], [22], [33], the original data features are not available in the consensus process.", "startOffset": 64, "endOffset": 68}, {"referenceID": 14, "context": "However, in the more general formulation of ensemble clustering [14], [15], [16], [17], [18], [19], [20], [21], [22], [33], the original data features are not available in the consensus process.", "startOffset": 70, "endOffset": 74}, {"referenceID": 15, "context": "However, in the more general formulation of ensemble clustering [14], [15], [16], [17], [18], [19], [20], [21], [22], [33], the original data features are not available in the consensus process.", "startOffset": 76, "endOffset": 80}, {"referenceID": 16, "context": "However, in the more general formulation of ensemble clustering [14], [15], [16], [17], [18], [19], [20], [21], [22], [33], the original data features are not available in the consensus process.", "startOffset": 82, "endOffset": 86}, {"referenceID": 17, "context": "However, in the more general formulation of ensemble clustering [14], [15], [16], [17], [18], [19], [20], [21], [22], [33], the original data features are not available in the consensus process.", "startOffset": 88, "endOffset": 92}, {"referenceID": 18, "context": "However, in the more general formulation of ensemble clustering [14], [15], [16], [17], [18], [19], [20], [21], [22], [33], the original data features are not available in the consensus process.", "startOffset": 94, "endOffset": 98}, {"referenceID": 19, "context": "However, in the more general formulation of ensemble clustering [14], [15], [16], [17], [18], [19], [20], [21], [22], [33], the original data features are not available in the consensus process.", "startOffset": 100, "endOffset": 104}, {"referenceID": 20, "context": "However, in the more general formulation of ensemble clustering [14], [15], [16], [17], [18], [19], [20], [21], [22], [33], the original data features are not available in the consensus process.", "startOffset": 106, "endOffset": 110}, {"referenceID": 21, "context": "However, in the more general formulation of ensemble clustering [14], [15], [16], [17], [18], [19], [20], [21], [22], [33], the original data features are not available in the consensus process.", "startOffset": 112, "endOffset": 116}, {"referenceID": 32, "context": "However, in the more general formulation of ensemble clustering [14], [15], [16], [17], [18], [19], [20], [21], [22], [33], the original data features are not available in the consensus process.", "startOffset": 118, "endOffset": 122}, {"referenceID": 29, "context": "Moreover, by measuring the within-cluster similarity based on Euclidean distances, the efficacy of the method in [30] heavily relies on some implicit assumptions about data distribution, which places an unstable factor in the consensus process.", "startOffset": 113, "endOffset": 117}, {"referenceID": 29, "context": "Different from [30], in this paper, our ensemble clustering approach requires no access to the original data features.", "startOffset": 15, "endOffset": 19}, {"referenceID": 33, "context": "In information theory [34], the entropy is a measure of the uncertainty associated with a random variable.", "startOffset": 22, "endOffset": 26}, {"referenceID": 33, "context": "Hence, given n independent random variables X1, \u00b7 \u00b7 \u00b7 , Xn, we have [34] H(X1, \u00b7 \u00b7 \u00b7 , Xn) = H(X1) + \u00b7 \u00b7 \u00b7+H(Xn).", "startOffset": 68, "endOffset": 72}, {"referenceID": 13, "context": "In the first formulation, the ensemble clustering system only takes the multiple base clusterings as input and has no access to the original data features [14], [15], [16], [17], [18], [19], [20], [21], [22], [33], [35], [36].", "startOffset": 155, "endOffset": 159}, {"referenceID": 14, "context": "In the first formulation, the ensemble clustering system only takes the multiple base clusterings as input and has no access to the original data features [14], [15], [16], [17], [18], [19], [20], [21], [22], [33], [35], [36].", "startOffset": 161, "endOffset": 165}, {"referenceID": 15, "context": "In the first formulation, the ensemble clustering system only takes the multiple base clusterings as input and has no access to the original data features [14], [15], [16], [17], [18], [19], [20], [21], [22], [33], [35], [36].", "startOffset": 167, "endOffset": 171}, {"referenceID": 16, "context": "In the first formulation, the ensemble clustering system only takes the multiple base clusterings as input and has no access to the original data features [14], [15], [16], [17], [18], [19], [20], [21], [22], [33], [35], [36].", "startOffset": 173, "endOffset": 177}, {"referenceID": 17, "context": "In the first formulation, the ensemble clustering system only takes the multiple base clusterings as input and has no access to the original data features [14], [15], [16], [17], [18], [19], [20], [21], [22], [33], [35], [36].", "startOffset": 179, "endOffset": 183}, {"referenceID": 18, "context": "In the first formulation, the ensemble clustering system only takes the multiple base clusterings as input and has no access to the original data features [14], [15], [16], [17], [18], [19], [20], [21], [22], [33], [35], [36].", "startOffset": 185, "endOffset": 189}, {"referenceID": 19, "context": "In the first formulation, the ensemble clustering system only takes the multiple base clusterings as input and has no access to the original data features [14], [15], [16], [17], [18], [19], [20], [21], [22], [33], [35], [36].", "startOffset": 191, "endOffset": 195}, {"referenceID": 20, "context": "In the first formulation, the ensemble clustering system only takes the multiple base clusterings as input and has no access to the original data features [14], [15], [16], [17], [18], [19], [20], [21], [22], [33], [35], [36].", "startOffset": 197, "endOffset": 201}, {"referenceID": 21, "context": "In the first formulation, the ensemble clustering system only takes the multiple base clusterings as input and has no access to the original data features [14], [15], [16], [17], [18], [19], [20], [21], [22], [33], [35], [36].", "startOffset": 203, "endOffset": 207}, {"referenceID": 32, "context": "In the first formulation, the ensemble clustering system only takes the multiple base clusterings as input and has no access to the original data features [14], [15], [16], [17], [18], [19], [20], [21], [22], [33], [35], [36].", "startOffset": 209, "endOffset": 213}, {"referenceID": 34, "context": "In the first formulation, the ensemble clustering system only takes the multiple base clusterings as input and has no access to the original data features [14], [15], [16], [17], [18], [19], [20], [21], [22], [33], [35], [36].", "startOffset": 215, "endOffset": 219}, {"referenceID": 35, "context": "In the first formulation, the ensemble clustering system only takes the multiple base clusterings as input and has no access to the original data features [14], [15], [16], [17], [18], [19], [20], [21], [22], [33], [35], [36].", "startOffset": 221, "endOffset": 225}, {"referenceID": 29, "context": "In the other formulation, the ensemble clustering system takes both the multiple base clusterings and the original data features as inputs [30], [37], [38].", "startOffset": 139, "endOffset": 143}, {"referenceID": 36, "context": "In the other formulation, the ensemble clustering system takes both the multiple base clusterings and the original data features as inputs [30], [37], [38].", "startOffset": 145, "endOffset": 149}, {"referenceID": 37, "context": "In the other formulation, the ensemble clustering system takes both the multiple base clusterings and the original data features as inputs [30], [37], [38].", "startOffset": 151, "endOffset": 155}, {"referenceID": 38, "context": "In this paper, we comply with the first formulation of the ensemble clustering problem, which is also the common practice for most of the existing ensemble clustering approaches [39].", "startOffset": 178, "endOffset": 182}, {"referenceID": 0, "context": "Because p(Ci, C j ) \u2208 [0, 1] for any i, j, m, it is obvious that H(Ci) \u2208 [0,+\u221e).", "startOffset": 22, "endOffset": 28}, {"referenceID": 38, "context": "Without loss of generality, based on the assumption that the base clusterings in the ensemble are independent [39], the uncertainty (or entropy) of Ci w.", "startOffset": 110, "endOffset": 114}, {"referenceID": 16, "context": "Refining Co-association Matrix by Local Weighting The co-association (CA) matrix is first proposed by Fred and Jain [17], which reflects how many times two data objects are grouped into the same cluster among the multiple base clusterings in the ensemble.", "startOffset": 116, "endOffset": 120}, {"referenceID": 16, "context": "The CA matrix is a classical and widely used tool for dealing with the ensemble clustering problem [17], [21], [25], [40].", "startOffset": 99, "endOffset": 103}, {"referenceID": 20, "context": "The CA matrix is a classical and widely used tool for dealing with the ensemble clustering problem [17], [21], [25], [40].", "startOffset": 105, "endOffset": 109}, {"referenceID": 24, "context": "The CA matrix is a classical and widely used tool for dealing with the ensemble clustering problem [17], [21], [25], [40].", "startOffset": 111, "endOffset": 115}, {"referenceID": 39, "context": "The CA matrix is a classical and widely used tool for dealing with the ensemble clustering problem [17], [21], [25], [40].", "startOffset": 117, "endOffset": 121}, {"referenceID": 28, "context": "[29] exploited the NCAI index to weight the base clusterings and thereby construct a weighted co-association (WCA) matrix, which, however, only considers the reliability of base clusterings, but still neglects the cluster-wise diversity inside the same base clustering.", "startOffset": 0, "endOffset": 4}, {"referenceID": 28, "context": "Different from the (globally) weighting strategy [29] that treats each base clustering as a whole, in this section, we refine the CA matrix by a local weighting strategy based on the ensemble-driven cluster validity and propose the concept of locally weighted co-association (LWCA) matrix.", "startOffset": 49, "endOffset": 53}, {"referenceID": 0, "context": "Hierarchical agglomerative clustering is a widely used clustering technique [1], which typically takes a similarity matrix as input and performs region merging iteratively to achieve a dendrogram, i.", "startOffset": 76, "endOffset": 79}, {"referenceID": 0, "context": "Typically, we adopt the average-link (AL), which is a classical agglomerative clustering method [1], to update the similarity matrix for the t-step.", "startOffset": 96, "endOffset": 99}, {"referenceID": 40, "context": "Having constructed the LWBG according to Definition 8, we proceed to partition the graph using the Tcut algorithm [41], which is able to take advantage of the bipartite graph structure to greatly facilitate the computation of the graph partitioning process.", "startOffset": 114, "endOffset": 118}, {"referenceID": 40, "context": "4: Partition the LWBG into a certain number of segments using the Tcut algorithm [41].", "startOffset": 81, "endOffset": 85}, {"referenceID": 41, "context": "The MNIST dataset and the USPS dataset are respectively from [42] and [43], whereas the other eight datasets are from the UCI machine learning repository [44].", "startOffset": 61, "endOffset": 65}, {"referenceID": 42, "context": "The MNIST dataset and the USPS dataset are respectively from [42] and [43], whereas the other eight datasets are from the UCI machine learning repository [44].", "startOffset": 70, "endOffset": 74}, {"referenceID": 43, "context": "The MNIST dataset and the USPS dataset are respectively from [42] and [43], whereas the other eight datasets are from the UCI machine learning repository [44].", "startOffset": 154, "endOffset": 158}, {"referenceID": 13, "context": "We use the normalized mutual information (NMI) [14] to evaluate the quality of clusterings.", "startOffset": 47, "endOffset": 51}, {"referenceID": 13, "context": "Comparison against Other Ensemble Clustering Methods In this section, we compare the proposed LWEA and LWGP methods against ten ensemble clustering methods, namely, CSPA [14], HGPA [14], MCLA [14], hybrid bipartite graph formulation (HBGF) [16], EAC [17], SimRank similarity based method (SRS) [45], weighted connected triple based method (WCT) [20], weighted evidence accumulation clustering (WEAC) [29], graph partitioning with multi-granularity link analysis (GP-MGLA) [29], and Two-level-refined cOassociation Matrix Ensemble (TOME) [30].", "startOffset": 170, "endOffset": 174}, {"referenceID": 13, "context": "Comparison against Other Ensemble Clustering Methods In this section, we compare the proposed LWEA and LWGP methods against ten ensemble clustering methods, namely, CSPA [14], HGPA [14], MCLA [14], hybrid bipartite graph formulation (HBGF) [16], EAC [17], SimRank similarity based method (SRS) [45], weighted connected triple based method (WCT) [20], weighted evidence accumulation clustering (WEAC) [29], graph partitioning with multi-granularity link analysis (GP-MGLA) [29], and Two-level-refined cOassociation Matrix Ensemble (TOME) [30].", "startOffset": 181, "endOffset": 185}, {"referenceID": 13, "context": "Comparison against Other Ensemble Clustering Methods In this section, we compare the proposed LWEA and LWGP methods against ten ensemble clustering methods, namely, CSPA [14], HGPA [14], MCLA [14], hybrid bipartite graph formulation (HBGF) [16], EAC [17], SimRank similarity based method (SRS) [45], weighted connected triple based method (WCT) [20], weighted evidence accumulation clustering (WEAC) [29], graph partitioning with multi-granularity link analysis (GP-MGLA) [29], and Two-level-refined cOassociation Matrix Ensemble (TOME) [30].", "startOffset": 192, "endOffset": 196}, {"referenceID": 15, "context": "Comparison against Other Ensemble Clustering Methods In this section, we compare the proposed LWEA and LWGP methods against ten ensemble clustering methods, namely, CSPA [14], HGPA [14], MCLA [14], hybrid bipartite graph formulation (HBGF) [16], EAC [17], SimRank similarity based method (SRS) [45], weighted connected triple based method (WCT) [20], weighted evidence accumulation clustering (WEAC) [29], graph partitioning with multi-granularity link analysis (GP-MGLA) [29], and Two-level-refined cOassociation Matrix Ensemble (TOME) [30].", "startOffset": 240, "endOffset": 244}, {"referenceID": 16, "context": "Comparison against Other Ensemble Clustering Methods In this section, we compare the proposed LWEA and LWGP methods against ten ensemble clustering methods, namely, CSPA [14], HGPA [14], MCLA [14], hybrid bipartite graph formulation (HBGF) [16], EAC [17], SimRank similarity based method (SRS) [45], weighted connected triple based method (WCT) [20], weighted evidence accumulation clustering (WEAC) [29], graph partitioning with multi-granularity link analysis (GP-MGLA) [29], and Two-level-refined cOassociation Matrix Ensemble (TOME) [30].", "startOffset": 250, "endOffset": 254}, {"referenceID": 19, "context": "Comparison against Other Ensemble Clustering Methods In this section, we compare the proposed LWEA and LWGP methods against ten ensemble clustering methods, namely, CSPA [14], HGPA [14], MCLA [14], hybrid bipartite graph formulation (HBGF) [16], EAC [17], SimRank similarity based method (SRS) [45], weighted connected triple based method (WCT) [20], weighted evidence accumulation clustering (WEAC) [29], graph partitioning with multi-granularity link analysis (GP-MGLA) [29], and Two-level-refined cOassociation Matrix Ensemble (TOME) [30].", "startOffset": 345, "endOffset": 349}, {"referenceID": 28, "context": "Comparison against Other Ensemble Clustering Methods In this section, we compare the proposed LWEA and LWGP methods against ten ensemble clustering methods, namely, CSPA [14], HGPA [14], MCLA [14], hybrid bipartite graph formulation (HBGF) [16], EAC [17], SimRank similarity based method (SRS) [45], weighted connected triple based method (WCT) [20], weighted evidence accumulation clustering (WEAC) [29], graph partitioning with multi-granularity link analysis (GP-MGLA) [29], and Two-level-refined cOassociation Matrix Ensemble (TOME) [30].", "startOffset": 400, "endOffset": 404}, {"referenceID": 28, "context": "Comparison against Other Ensemble Clustering Methods In this section, we compare the proposed LWEA and LWGP methods against ten ensemble clustering methods, namely, CSPA [14], HGPA [14], MCLA [14], hybrid bipartite graph formulation (HBGF) [16], EAC [17], SimRank similarity based method (SRS) [45], weighted connected triple based method (WCT) [20], weighted evidence accumulation clustering (WEAC) [29], graph partitioning with multi-granularity link analysis (GP-MGLA) [29], and Two-level-refined cOassociation Matrix Ensemble (TOME) [30].", "startOffset": 472, "endOffset": 476}, {"referenceID": 29, "context": "Comparison against Other Ensemble Clustering Methods In this section, we compare the proposed LWEA and LWGP methods against ten ensemble clustering methods, namely, CSPA [14], HGPA [14], MCLA [14], hybrid bipartite graph formulation (HBGF) [16], EAC [17], SimRank similarity based method (SRS) [45], weighted connected triple based method (WCT) [20], weighted evidence accumulation clustering (WEAC) [29], graph partitioning with multi-granularity link analysis (GP-MGLA) [29], and Two-level-refined cOassociation Matrix Ensemble (TOME) [30].", "startOffset": 537, "endOffset": 541}], "year": 2017, "abstractText": "Due to its ability to combine multiple base clusterings into a probably better and more robust clustering, the ensemble clustering technique has been attracting increasing attention in recent years. Despite the significant success, one limitation to most of the existing ensemble clustering methods is that they generally treat all base clusterings equally regardless of their reliability, which makes them vulnerable to low-quality base clusterings. Although some efforts have been made to (globally) evaluate and weight the base clusterings, yet these methods tend to view each base clustering as an individual and neglect the local diversity of clusters inside the same base clustering. It remains an open problem how to evaluate the reliability of clusters and exploit the local diversity in the ensemble to enhance the consensus performance, without access to data features or specific assumptions on data distribution. To address this, in this paper, we propose a novel ensemble clustering approach based on ensemble-driven cluster uncertainty estimation and local weighting strategy. In particular, the uncertainty of each cluster is estimated by considering the cluster labels in the entire ensemble via an entropic criterion. A novel ensemble-driven cluster validity measure is introduced, and a locally weighted co-association matrix is presented to serve as a summary for the ensemble of diverse clusters. With the local diversity in ensembles exploited, two novel consensus functions are further proposed. Extensive experiments on a variety of real-world datasets demonstrate the superiority of the proposed approach over the state-of-the-art.", "creator": "TeX"}}}