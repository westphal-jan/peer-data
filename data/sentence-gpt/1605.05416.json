{"id": "1605.05416", "review": {"conference": "ACL", "VERSION": "v1", "DATE_OF_SUBMISSION": "18-May-2016", "title": "Leveraging Lexical Resources for Learning Entity Embeddings in Multi-Relational Data", "abstract": "Recent work in learning vector-space embeddings for multi-relational data has focused on combining relational information derived from knowledge bases with distributional information derived from large text corpora. We propose a simple approach that leverages the descriptions of entities or phrases available in lexical resources, in conjunction with distributional semantics, in order to derive a better initialization for training relational models. Applying this initialization to the TransE model results in significant new state-of-the-art performances on the WordNet dataset, decreasing the mean rank from the previous best of 212 to 51. It also results in faster convergence of the entity representations. We find that there is a trade-off between improving the mean rank and the hits@10 with this approach. This illustrates that much remains to be understood regarding performance improvements in relational models.", "histories": [["v1", "Wed, 18 May 2016 01:45:32 GMT  (432kb,D)", "http://arxiv.org/abs/1605.05416v1", "6 pages. Accepted to ACL 2016 (short paper)"]], "COMMENTS": "6 pages. Accepted to ACL 2016 (short paper)", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["teng long", "ryan lowe", "jackie chi kit cheung", "doina precup"], "accepted": true, "id": "1605.05416"}, "pdf": {"name": "1605.05416.pdf", "metadata": {"source": "CRF", "title": "Leveraging Lexical Resources for Learning Entity Embeddings in Multi-Relational Data", "authors": ["Teng Long", "Ryan Lowe", "Jackie Chi", "Kit Cheung", "Doina Precup"], "emails": ["teng.long@mail.mcgill.ca", "ryan.lowe@cs.mcgill.ca", "jcheung@cs.mcgill.ca", "dprecup@cs.mcgill.ca", "hits@10"], "sections": [{"heading": "1 Introduction", "text": "A surprising result of work on vector-space word embeddings is that word representations that are learned from a large training corpus display semantic regularities in the form of linear vector translations. For example, Mikolov et al. (2013b) show that using their induced word vector representations, king\u2212man+woman \u2248 queen. Such a structure is appealing because it provides an interpretation to the distributional vector space through lexical-semantic analogical inferences.\nConcurrent to that work, Bordes et al. (2013) proposed translating embeddings (TransE), which takes a pre-existing semantic hierarchy as in-\nput and embeds its structure into a vector space. In their model, the linear relationship between two entities that are in some semantic relation to each other is an explicit part of the model\u2019s objective function. For example, given a relation such as won(Germany,FIFA Worldcup), the TransE model learns vector representations for won, Germany, and FIFA Worldcup such that Germany + won \u2248 FIFA Worldcup.\nA natural next step is to attempt to integrate the two approaches in order to develop a representation that is informed by both unstructured text and a structured knowledge base (Faruqui et al., 2015; Xu et al., 2014; Fried and Duh, 2015; Yang et al., 2015). However, existing work makes a crucial assumption\u2014that reliable distributional vectors are available for all of the entities in the hierarchy being modeled. Unfortunately, this assumption does not hold in practice; when moving to a new domain with a new knowledge base, for example, there will likely be many entities or phrases for which there is no distributional information in\n2This means that word2vec was trained in the usual way on a large textual corpus, but the vocabulary was truncated to include as many entities from Freebase as possible. Indeed, this is the reason for the small overlap between W2V, GloVe, and the relational databases: after training the word embeddings, the vocabulary must be truncated to a reasonable size, which leaves out many entities from these datasets.\nar X\niv :1\n60 5.\n05 41\n6v 1\n[ cs\n.C L\n] 1\n8 M\nay 2\n01 6\nthe training corpus. This important problem is illustrated in Table 1, where most of the entities from WordNet and Freebase are seen to be missing from the distributional vectors derived using Word2Vec and GloVe trained on the Google News corpus. Even when the entities are found, they may not have occurred enough times in the training corpus for their vector representation to be reliable. What is needed is a method to derive entity representations that works well for both common and rare entities.\nFortunately, knowledge bases typically contain a short description or definition for each of the entities or phrases they contain. For example, in a medical dataset with many technical words, the Wikipedia pages, dictionary definitions, or medical descriptions via a site such as medilexicon.com could be leveraged as lexical resources. Similarly, when building language models for social media, resources such as urbandicionary.com could be used for information about slang words. For the WordNet and Freebase datasets, we use entity descriptions which are readily available (see Table 2).\nIn this paper, we propose a simple and efficient procedure to convert these short descriptions into a vector space representation, with the help of existing word embedding models. These vectors are then used as the input to further training with the TransE model, in order to incorporate structural information. Our method provides a better initialization for the TransE model, not just for the entities that do not appear in the data, but in fact for all entities. This is demonstrated by achieving stateof-the-art mean rank on an entity ranking task on two very different data sets: WordNet synsets with lexical semantic relations (Miller, 1995), and Freebase named entities with general semantic relations (Bollacker et al., 2008)."}, {"heading": "2 Related Work", "text": "Dictionary definitions were the core component of early methods in word sense disambiguation (WSD), such as the Lesk algorithm (1986). Chen et al. (2014) build on the use of synset glosses for WSD by leveraging lexical resources. Our work goes further to tie these glosses together with relational semantics, a connection that has not been drawn in the literature before. The integration of lexical resources into distributional semantics has been studied in other lexical semantic tasks,\nsuch as synonym expansion (Sinha and Mihalcea, 2009), relation extraction (Kambhatla, 2004), and calculating the semantic distance between concepts (Mohammad, 2008; Marton et al., 2009). We aim to combine lexical resources and other semantic knowledge, but we do so in the context of neural network-based word embeddings, rather than in specific lexical semantic tasks.\nBordes et al. (2011) propose the Structured Embeddings (SE) model, which embeds entities into vectors and relations into matrices. The relation connection between two entities is modeled by the projection of their embeddings into a different vector space. Rothe and Schu\u0308tze (2015) use Wordnet as a lexical resource to learn embeddings for synsets and lexemes. Perhaps most related to our work are previous relational models that initialize their embeddings via distributional semantics calculated from a larger corpus. Socher et al. (2013) propose the Neural Tensor Network (NTN), and Yang et al. (2015) the Bilinear model using this technique. Other approaches modify the objective function or change the structure of the model in order to integrate distributional and relational information (Xu et al., 2014; Fried and Duh, 2015; Toutanova and Chen, 2015). Faruqui et al. (2015) retrofit word vectors after they are trained according to distributional criteria. We propose a method that does not necessitate post-processing of the embeddings, and can be applied orthogonally to the previously mentioned improvements."}, {"heading": "3 Architecture of the Approach", "text": ""}, {"heading": "3.1 The TransE Model", "text": "The Translating Embedding (TransE) model (Bordes et al., 2013) has become one of the most popu-\nlar multi-relational models due to its relative simplicity, scalability to large datasets, and (until recently) state-of-the-art results. It assumes a simple additive interaction between vector representations of entities and relations. More precisely, assume a given relationship triplet (h, l, t) is valid; then, the embedding of the object t should be very close to the embedding of the subject h plus some vector in Rk that depends on the relation l3.\nFor each positive triplet (h, l, t) \u2208 S, a negative triplet (h\u2032, l, t\u2032) \u2208 S\u2032 is constructed by randomly sampling an entity from E to replace either the subject h or the object t of the relationship. The training objective of TransE is to minimize the dissimilarity measure d(h + l, t) of a positive triplet while ensuring that d(h\u2032 + l, t\u2032) for the corrupted triplet remains large. This is accomplished by minimizing the hinge loss over the training set:\nL = \u2211\n(h,l,t)\u2208S \u2211 (h\u2032,l,t\u2032)\u2208S\u2032 [\u03b3+d(h+l, t)\u2212d(h\u2032+l, t\u2032)]+\nwhere \u03b3 is the hinge loss margin and [x]+ represents the positive portion of x. There is an additional constraint that the L2-norm of entity embeddings (but not relation embeddings) must be 1, which prevents the training process to trivially minimize L by artificially increasing the norms of entity embeddings."}, {"heading": "3.2 Initializing Representations with Entity Descriptions", "text": "We propose to leverage some external lexical resource to improve the quality of the entity vector representations. In general, this could consist of product descriptions in a product database, or information from a web resource. For the WordNet and Freebase datasets, we use entity descriptions which are readily available.\nAlthough there are many ways to incorporate this, we propose a simple method whereby the entity descriptions are used to initialize the entity representations of the model, which we show to have empirical benefits. In particular, we first decompose the description of a given entity into a sequence of word vectors, and combine them into a single embedding by averaging. We then reduce the dimensionality using principle component analysis (PCA), which we found\n3Note that we use h, l, t \u2208 Rk to denote both the entities and relations, in addition to the vector representations of the entities and relations\nexperimentally to reduce overfitting. We obtain these word vectors using distributed representations computed using word2vec (Mikolov et al., 2013a) and GloVe (Pennington et al., 2014). Approximating compositionality by averaging vector representations is simple, yet has some theoretical justification (Tian et al., 2015) and can work well in practice (Wieting et al., 2015).\nAdditional decisions need to be made concerning which parts of the entity description to include. In particular, if an entity description or word definition is longer than several sentences, using the entire description could cause a \u2018dilution\u2019 of the desired embedding, as not all sentences will be equally pertinent. We solve this by only considering the first sentence of any entity description, which is often the most relevant one. This is necessary for Freebase, where the description length can be several paragraphs."}, {"heading": "4 Experiments", "text": ""}, {"heading": "4.1 Training and Testing Setup", "text": "We perform experiments on the WordNet (WN) (Miller, 1995) and Freebase (FB15k) (Bollacker et al., 2008) datasets used by the original TransE model. TransE hyperparameters include the learning rate \u03bb for stochastic gradient descent, the margin \u03b3 for the hinge loss, the dimension of the embeddings k, and the dissimilarity metric d. For the TransE model with random initialization, we use the optimal hyperparameters from (Bordes et al., 2013): for WN, \u03bb = 0.01, \u03b3 = 2, k = 20, and d = L1-norm; for FB15k, \u03bb = 0.01, \u03b3 = 0.5, k = 50, and d = L2-norm. The values of k were further tested to ensure that k = 20 and k = 50 were optimal. For the TransE model with strategic initialization, we used different embedding dimensions. The distributional vectors used in the entity descriptions are of dimension 1000 for the word2vec vectors with Freebase vocabulary, and dimension 300 in all other cases. Dimensionality reduction with PCA was then applied to reduce this to k = 30 for WN, and k = 55 for FB15k, which were empirically found to be optimal. PCA was necessary in this case as pre-trained vectors from word2vec and GloVe are not available for all dimension values.\nWe use the same train/test/validation split and evaluation procedure as (Bordes et al., 2013): for each test triplet (h, l, t), we remove entity h and t in turn, and rank each entity in the dictionary\nby similarity according to the model. We evaluate using the original and most common metrics for relational models: i) the mean of the predicted ranks, and ii) hits@10, which represents the percentage of correct entities found in the top 10 list; however, other metrics are possible, such as mean reciprocal rank (MRR). We evaluate in both the filtered setting, where other correct responses are removed from the lists ranked by the model, and the raw setting, where no changes are made.\nWe compare against the TransE model with random initialization, and the SE model (Bordes et al., 2011). We also compare against the state-ofthe-art TransD model (Ji et al., 2015). This model uses two vectors to represent each entity and relation; one to represent the meaning of the entity, and one to construct a mapping matrix dynamically. This allows for the representation of more diverse entities."}, {"heading": "4.2 Results and Analysis", "text": "Table 3 summarizes the experimental results, compared to baseline and state-of-the-art relational models. We see that the mean rank is greatly im-\nproved for the TransE model with strategic initialization over random initialization. More surprisingly, all of our models achieve state-of-the-art performance for both raw and filtered data, compared to the recently developed TransD model. These results are highly significant with p < 10\u22123 according to the Mann-Whitney U test. Thus, even though our method is simple and straightforward to apply, it can still beat all attempts at more complicated structural modifications to the TransE model on this dataset. Further, the fact that our optimal embedding dimensions are larger (30 and 55 vs. 20 and 50) suggests that our initialization approach helps avoid overfitting.\nFor Freebase, our models slightly outperform the TransE model with random initialization, with p-values of 0.173 and 0.410 for initialization with descriptions (including stopwords) using GloVe and word2vec, respectively. We also see improvements over the case of direct initialization with word2vec. Further, we set a new state-of-the-art for mean rank on the raw data, though the improvement is marginal.\nFinally, we see in Figure 1 that the TransE model converges more quickly during training when initialized with our approach, compared to random initialization. This is particularly true on WordNet.\nMean rank and hits@10 discrepancy It is interesting to note the relationship between the mean rank and hits@10. By changing our model, we are able to increase one at the expense of the other. For example, using word2vec without stopwords gives similar hits@10 to TransD with better mean rank, while using GloVe further improves the mean rank at a cost to hits@10. The exact nature of this tradeoff isn\u2019t clear, and is an interesting avenue for future work.\nHowever, there are potential reasons for the results discrepancy betweeen mean rank and hits@10. We conjecture that our model helps avoid \u2018disasters\u2019 where some correct entities are ranked very low. For TransE with random initialization, these disasters cause a large decrease in mean rank, which is significantly improved by our model. On the other hand, reducing the number of correct entities that are poorly ranked may not significantly affect the hits@10, since this only considers entities near the top of the ranking.\nNote also that using hits@10 to evaluate relational models is not ideal; a model can rank reasonable alternative entities highly, but be penalized because the target entity is not in the top 10. For example, given \u201crabbit IS-A\u201d, both \u201canimal\u201d and \u201cmammal\u201d fit as target entities. This is alleviated by filtering, but is not completely eliminated due to the sparsity of relations in the dataset (which is the reason we require the link prediction task). Thus, we believe the mean rank is a more accurate measure of the performance of a model, particularly on raw data.\nDataset differences It is also interesting to note the discrepancy between the results on the WordNet and Freebase datasets. Although using the entity descriptions leads to a significantly lower mean rank for the WordNet dataset, it only results in a faster convergence rate for Freebase. However, the relations presented in these two datasets are significantly different: WordNet relations are quite general and are meant to provide links between concepts, while the Freebase relations are very specific, and denote relationships between named entities. This is shown in Table 4. It seems that incorporating the definition of these named entities does not improve the ability of the algorithm to answer very specific relation questions. This would be the case if the optimization landscape for the TransE model had fewer local minima for Freebase than for WordNet, thus rendering it less sensitive to the initial condition. It is also possible that the TransE model is simply not powerful enough to achieve a filtered mean rank lower than 90, no matter the initialization strategy."}, {"heading": "5 Conclusion and Future Work", "text": "We have shown that leveraging external lexical resources, along with distributional semantics, can lead to both a significantly improved optimum and a faster rate of convergence when applied with the TransE model for relational data. We established new state-of-the-art results on WordNet, and obtain small improvements to the state-of-the-art on raw relational data for Freebase. Our method is quite simple and could be applied in a straightforward manner to other models that take entity vector representations as input. Further research is needed to investigate whether performance on other NLP tasks can be improved by leveraging available lexical resources in a similar manner.\nMore complex methods initialization methods could easily be devised, e.g. by using inverse document frequency (idf) weighted averaging, or by applying the work of Le et al. (2014) on paragraph vectors. Alternatively, distributional semantics could be used as a regularizer, similar to (Labutov and Lipson, 2013), with learned embeddings being penalized for how far they stray from the pre-trained GloVe embeddings. However, even with intuitive and straightforward methodology, leveraging lexical resources can have a significant impact on the results of models for multi-relational data."}], "references": [{"title": "Freebase: a collaboratively created graph database for structuring human knowledge", "author": ["Kurt Bollacker", "Colin Evans", "Praveen Paritosh", "Tim Sturge", "Jamie Taylor."], "venue": "Proceedings of SIGMOD.", "citeRegEx": "Bollacker et al\\.,? 2008", "shortCiteRegEx": "Bollacker et al\\.", "year": 2008}, {"title": "Learning structured embeddings of knowledge bases", "author": ["Antoine Bordes", "Jason Weston", "Ronan Collobert", "Yoshua Bengio."], "venue": "Proceedings of AAAI.", "citeRegEx": "Bordes et al\\.,? 2011", "shortCiteRegEx": "Bordes et al\\.", "year": 2011}, {"title": "Translating embeddings for modeling multirelational data", "author": ["Antoine Bordes", "Nicolas Usunier", "Alberto GarciaDuran", "Jason Weston", "Oksana Yakhnenko."], "venue": "Proceedings of NIPS.", "citeRegEx": "Bordes et al\\.,? 2013", "shortCiteRegEx": "Bordes et al\\.", "year": 2013}, {"title": "A unified model for word sense representation and disambiguation", "author": ["Xinxiong Chen", "Zhiyuan Liu", "Maosong Sun."], "venue": "EMNLP, pages 1025\u20131035. Citeseer.", "citeRegEx": "Chen et al\\.,? 2014", "shortCiteRegEx": "Chen et al\\.", "year": 2014}, {"title": "Retrofitting word vectors to semantic lexicons", "author": ["Manaal Faruqui", "Jesse Dodge", "Sujay K Jauhar", "Chris Dyer", "Eduard Hovy", "Noah A Smith."], "venue": "Proceedings of NAACL.", "citeRegEx": "Faruqui et al\\.,? 2015", "shortCiteRegEx": "Faruqui et al\\.", "year": 2015}, {"title": "Incorporating both distributional and relational semantics in word representations", "author": ["Daniel Fried", "Kevin Duh."], "venue": "In Proceedings of ICLR.", "citeRegEx": "Fried and Duh.,? 2015", "shortCiteRegEx": "Fried and Duh.", "year": 2015}, {"title": "Knowledge graph embedding via dynamic mapping matrix", "author": ["Guoliang Ji", "Shizhu He", "Liheng Xu", "Kang Liu", "Jun Zhao."], "venue": "Proceedings of ACL.", "citeRegEx": "Ji et al\\.,? 2015", "shortCiteRegEx": "Ji et al\\.", "year": 2015}, {"title": "Combining lexical, syntactic, and semantic features with maximum entropy models for extracting relations", "author": ["Nanda Kambhatla."], "venue": "Proceedings of ACL on Interactive Poster and Demonstration Sessions.", "citeRegEx": "Kambhatla.,? 2004", "shortCiteRegEx": "Kambhatla.", "year": 2004}, {"title": "Re-embedding words", "author": ["Igor Labutov", "Hod Lipson."], "venue": "Proceedings of ACL.", "citeRegEx": "Labutov and Lipson.,? 2013", "shortCiteRegEx": "Labutov and Lipson.", "year": 2013}, {"title": "Distributed representations of sentences and documents", "author": ["Quoc V Le", "Tomas Mikolov."], "venue": "Proceedings of ICML.", "citeRegEx": "Le and Mikolov.,? 2014", "shortCiteRegEx": "Le and Mikolov.", "year": 2014}, {"title": "Automatic sense disambiguation using machine readable dictionaries: How to tell a pine cone from an ice cream cone", "author": ["Michael Lesk."], "venue": "Proceedings of SIGDOC.", "citeRegEx": "Lesk.,? 1986", "shortCiteRegEx": "Lesk.", "year": 1986}, {"title": "Estimating semantic distance using soft semantic constraints in knowledge-source-corpus hybrid models", "author": ["Yuval Marton", "Saif Mohammad", "Philip Resnik."], "venue": "Proceedings of EMNLP.", "citeRegEx": "Marton et al\\.,? 2009", "shortCiteRegEx": "Marton et al\\.", "year": 2009}, {"title": "Efficient estimation of word representations in vector space", "author": ["Tomas Mikolov", "Kai Chen", "Greg Corrado", "Jeffrey Dean."], "venue": "Proceedings of ICLR.", "citeRegEx": "Mikolov et al\\.,? 2013a", "shortCiteRegEx": "Mikolov et al\\.", "year": 2013}, {"title": "Linguistic regularities in continuous space word representations", "author": ["Tomas Mikolov", "Wen-tau Yih", "Geoffrey Zweig."], "venue": "Proceedings of NAACLHLT.", "citeRegEx": "Mikolov et al\\.,? 2013b", "shortCiteRegEx": "Mikolov et al\\.", "year": 2013}, {"title": "Wordnet: a lexical database for english", "author": ["George A Miller."], "venue": "Communications of the ACM, 38(11):39\u201341.", "citeRegEx": "Miller.,? 1995", "shortCiteRegEx": "Miller.", "year": 1995}, {"title": "Measuring semantic distance using distributional profiles of concepts", "author": ["Saif Mohammad."], "venue": "Ph.D. thesis, University of Toronto.", "citeRegEx": "Mohammad.,? 2008", "shortCiteRegEx": "Mohammad.", "year": 2008}, {"title": "Glove: Global vectors for word representation", "author": ["Jeffrey Pennington", "Richard Socher", "Christopher D Manning."], "venue": "Proceedings of EMNLP.", "citeRegEx": "Pennington et al\\.,? 2014", "shortCiteRegEx": "Pennington et al\\.", "year": 2014}, {"title": "Autoextend: Extending word embeddings to embeddings for synsets and lexemes", "author": ["Sascha Rothe", "Hinrich Sch\u00fctze."], "venue": "Proceedings of ACL.", "citeRegEx": "Rothe and Sch\u00fctze.,? 2015", "shortCiteRegEx": "Rothe and Sch\u00fctze.", "year": 2015}, {"title": "Combining lexical resources for contextual synonym expansion", "author": ["Ravi Sinha", "Rada Mihalcea."], "venue": "Proceedings of RANLP.", "citeRegEx": "Sinha and Mihalcea.,? 2009", "shortCiteRegEx": "Sinha and Mihalcea.", "year": 2009}, {"title": "Reasoning with neural tensor networks for knowledge base completion", "author": ["Richard Socher", "Danqi Chen", "Christopher D Manning", "Andrew Ng."], "venue": "Proceedings of NIPS.", "citeRegEx": "Socher et al\\.,? 2013", "shortCiteRegEx": "Socher et al\\.", "year": 2013}, {"title": "The mechanism of additive composition", "author": ["Ran Tian", "Naoaki Okazaki", "Kentaro Inui."], "venue": "arXiv preprint arXiv:1511.08407.", "citeRegEx": "Tian et al\\.,? 2015", "shortCiteRegEx": "Tian et al\\.", "year": 2015}, {"title": "Observed versus latent features for knowledge base and text inference", "author": ["Kristina Toutanova", "Danqi Chen."], "venue": "Proceedings of the 3rd Workshop on Continuous Vector Space Models and their Compositionality.", "citeRegEx": "Toutanova and Chen.,? 2015", "shortCiteRegEx": "Toutanova and Chen.", "year": 2015}, {"title": "Towards universal paraphrastic sentence embeddings", "author": ["John Wieting", "Mohit Bansal", "Kevin Gimpel", "Karen Livescu."], "venue": "arXiv preprint arXiv:1511.08198.", "citeRegEx": "Wieting et al\\.,? 2015", "shortCiteRegEx": "Wieting et al\\.", "year": 2015}, {"title": "Rcnet: A general framework for incorporating knowledge into word representations", "author": ["Chang Xu", "Yalong Bai", "Jiang Bian", "Bin Gao", "Gang Wang", "Xiaoguang Liu", "Tie-Yan Liu."], "venue": "Proceedings of CIKM.", "citeRegEx": "Xu et al\\.,? 2014", "shortCiteRegEx": "Xu et al\\.", "year": 2014}, {"title": "Embedding entities and relations for learning and inference in knowledge bases", "author": ["Bishan Yang", "Wen-tau Yih", "Xiaodong He", "Jianfeng Gao", "Li Deng."], "venue": "Proceedings of ICLR.", "citeRegEx": "Yang et al\\.,? 2015", "shortCiteRegEx": "Yang et al\\.", "year": 2015}], "referenceMentions": [{"referenceID": 12, "context": "For example, Mikolov et al. (2013b)", "startOffset": 13, "endOffset": 36}, {"referenceID": 1, "context": "Concurrent to that work, Bordes et al. (2013) proposed translating embeddings (TransE), which takes a pre-existing semantic hierarchy as inW2V GloVe Dataset Total found% found%", "startOffset": 25, "endOffset": 46}, {"referenceID": 4, "context": "A natural next step is to attempt to integrate the two approaches in order to develop a representation that is informed by both unstructured text and a structured knowledge base (Faruqui et al., 2015; Xu et al., 2014; Fried and Duh, 2015; Yang et al., 2015).", "startOffset": 178, "endOffset": 257}, {"referenceID": 23, "context": "A natural next step is to attempt to integrate the two approaches in order to develop a representation that is informed by both unstructured text and a structured knowledge base (Faruqui et al., 2015; Xu et al., 2014; Fried and Duh, 2015; Yang et al., 2015).", "startOffset": 178, "endOffset": 257}, {"referenceID": 5, "context": "A natural next step is to attempt to integrate the two approaches in order to develop a representation that is informed by both unstructured text and a structured knowledge base (Faruqui et al., 2015; Xu et al., 2014; Fried and Duh, 2015; Yang et al., 2015).", "startOffset": 178, "endOffset": 257}, {"referenceID": 24, "context": "A natural next step is to attempt to integrate the two approaches in order to develop a representation that is informed by both unstructured text and a structured knowledge base (Faruqui et al., 2015; Xu et al., 2014; Fried and Duh, 2015; Yang et al., 2015).", "startOffset": 178, "endOffset": 257}, {"referenceID": 14, "context": "This is demonstrated by achieving stateof-the-art mean rank on an entity ranking task on two very different data sets: WordNet synsets with lexical semantic relations (Miller, 1995), and Freebase named entities with general semantic relations (Bollacker et al.", "startOffset": 167, "endOffset": 181}, {"referenceID": 0, "context": "This is demonstrated by achieving stateof-the-art mean rank on an entity ranking task on two very different data sets: WordNet synsets with lexical semantic relations (Miller, 1995), and Freebase named entities with general semantic relations (Bollacker et al., 2008).", "startOffset": 243, "endOffset": 267}, {"referenceID": 9, "context": "Dictionary definitions were the core component of early methods in word sense disambiguation (WSD), such as the Lesk algorithm (1986). Chen et al.", "startOffset": 112, "endOffset": 134}, {"referenceID": 3, "context": "Chen et al. (2014) build on the use of synset glosses for WSD by leveraging lexical resources.", "startOffset": 0, "endOffset": 19}, {"referenceID": 18, "context": "such as synonym expansion (Sinha and Mihalcea, 2009), relation extraction (Kambhatla, 2004), and calculating the semantic distance between con-", "startOffset": 26, "endOffset": 52}, {"referenceID": 7, "context": "such as synonym expansion (Sinha and Mihalcea, 2009), relation extraction (Kambhatla, 2004), and calculating the semantic distance between con-", "startOffset": 74, "endOffset": 91}, {"referenceID": 15, "context": "cepts (Mohammad, 2008; Marton et al., 2009).", "startOffset": 6, "endOffset": 43}, {"referenceID": 11, "context": "cepts (Mohammad, 2008; Marton et al., 2009).", "startOffset": 6, "endOffset": 43}, {"referenceID": 23, "context": "Other approaches modify the objective function or change the structure of the model in order to integrate distributional and relational information (Xu et al., 2014; Fried and Duh, 2015; Toutanova and Chen, 2015).", "startOffset": 148, "endOffset": 212}, {"referenceID": 5, "context": "Other approaches modify the objective function or change the structure of the model in order to integrate distributional and relational information (Xu et al., 2014; Fried and Duh, 2015; Toutanova and Chen, 2015).", "startOffset": 148, "endOffset": 212}, {"referenceID": 21, "context": "Other approaches modify the objective function or change the structure of the model in order to integrate distributional and relational information (Xu et al., 2014; Fried and Duh, 2015; Toutanova and Chen, 2015).", "startOffset": 148, "endOffset": 212}, {"referenceID": 15, "context": "Rothe and Sch\u00fctze (2015) use Wordnet as a lexical resource to learn embeddings for synsets and lexemes.", "startOffset": 0, "endOffset": 25}, {"referenceID": 15, "context": "Rothe and Sch\u00fctze (2015) use Wordnet as a lexical resource to learn embeddings for synsets and lexemes. Perhaps most related to our work are previous relational models that initialize their embeddings via distributional semantics calculated from a larger corpus. Socher et al. (2013) propose the Neural Tensor Network (NTN), and Yang et al.", "startOffset": 0, "endOffset": 284}, {"referenceID": 15, "context": "Rothe and Sch\u00fctze (2015) use Wordnet as a lexical resource to learn embeddings for synsets and lexemes. Perhaps most related to our work are previous relational models that initialize their embeddings via distributional semantics calculated from a larger corpus. Socher et al. (2013) propose the Neural Tensor Network (NTN), and Yang et al. (2015) the Bilinear model using this technique.", "startOffset": 0, "endOffset": 348}, {"referenceID": 4, "context": "Faruqui et al. (2015) retrofit word vectors after they are trained according to distributional criteria.", "startOffset": 0, "endOffset": 22}, {"referenceID": 2, "context": "The Translating Embedding (TransE) model (Bordes et al., 2013) has become one of the most popu-", "startOffset": 41, "endOffset": 62}, {"referenceID": 12, "context": "these word vectors using distributed representations computed using word2vec (Mikolov et al., 2013a) and GloVe (Pennington et al.", "startOffset": 77, "endOffset": 100}, {"referenceID": 16, "context": ", 2013a) and GloVe (Pennington et al., 2014).", "startOffset": 19, "endOffset": 44}, {"referenceID": 20, "context": "Approximating compositionality by averaging vector representations is simple, yet has some theoretical justification (Tian et al., 2015) and can work well in practice (Wieting et al.", "startOffset": 117, "endOffset": 136}, {"referenceID": 22, "context": ", 2015) and can work well in practice (Wieting et al., 2015).", "startOffset": 38, "endOffset": 60}, {"referenceID": 14, "context": "We perform experiments on the WordNet (WN) (Miller, 1995) and Freebase (FB15k) (Bollacker et al.", "startOffset": 43, "endOffset": 57}, {"referenceID": 0, "context": "We perform experiments on the WordNet (WN) (Miller, 1995) and Freebase (FB15k) (Bollacker et al., 2008) datasets used by the original TransE model.", "startOffset": 79, "endOffset": 103}, {"referenceID": 2, "context": "For the TransE model with random initialization, we use the optimal hyperparameters from (Bordes et al., 2013): for WN, \u03bb = 0.", "startOffset": 89, "endOffset": 110}, {"referenceID": 2, "context": "We use the same train/test/validation split and evaluation procedure as (Bordes et al., 2013): for each test triplet (h, l, t), we remove entity h and t in turn, and rank each entity in the dictionary", "startOffset": 72, "endOffset": 93}, {"referenceID": 1, "context": "m od el s SE (Bordes et al., 2011) \u2014 1,011 985 68.", "startOffset": 13, "endOffset": 34}, {"referenceID": 6, "context": "8% TransD (unif) (Ji et al., 2015) \u2014 242 229 79.", "startOffset": 17, "endOffset": 34}, {"referenceID": 6, "context": "2% TransD (bern) (Ji et al., 2015) \u2014 224 212 79.", "startOffset": 17, "endOffset": 34}, {"referenceID": 1, "context": "We compare against the TransE model with random initialization, and the SE model (Bordes et al., 2011).", "startOffset": 81, "endOffset": 102}, {"referenceID": 6, "context": "We also compare against the state-ofthe-art TransD model (Ji et al., 2015).", "startOffset": 57, "endOffset": 74}, {"referenceID": 8, "context": "Alternatively, distributional semantics could be used as a regularizer, similar to (Labutov and Lipson, 2013), with learned embeddings being penalized for how far they stray from the pre-trained GloVe embeddings.", "startOffset": 83, "endOffset": 109}], "year": 2016, "abstractText": "Recent work in learning vector-space embeddings for multi-relational data has focused on combining relational information derived from knowledge bases with distributional information derived from large text corpora. We propose a simple approach that leverages the descriptions of entities or phrases available in lexical resources, in conjunction with distributional semantics, in order to derive a better initialization for training relational models. Applying this initialization to the TransE model results in significant new stateof-the-art performances on the WordNet dataset, decreasing the mean rank from the previous best of 212 to 51. It also results in faster convergence of the entity representations. We find that there is a tradeoff between improving the mean rank and the hits@10 with this approach. This illustrates that much remains to be understood regarding performance improvements in relational models.", "creator": "TeX"}}}