{"id": "1605.05826", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "19-May-2016", "title": "Declarative Machine Learning - A Classification of Basic Properties and Types", "abstract": "Declarative machine learning (ML) aims at the high-level specification of ML tasks or algorithms, and automatic generation of optimized execution plans from these specifications. The fundamental goal is to simplify the usage and/or development of ML algorithms, which is especially important in the context of large-scale computations. However, ML systems at different abstraction levels have emerged over time and accordingly there has been a controversy about the meaning of this general definition of declarative ML. Specification alternatives range from ML algorithms expressed in domain-specific languages (DSLs) with optimization for performance, to ML task (learning problem) specifications with optimization for performance and accuracy. We argue that these different types of declarative ML complement each other as they address different users (data scientists and end users). This paper makes an attempt to create a taxonomy for declarative ML, including a definition of essential basic properties and types of declarative ML. Along the way, we provide insights into implications of these properties. We also use this taxonomy to classify existing systems. Finally, we draw conclusions on defining appropriate benchmarks and specification languages for declarative ML.\n\n\n\n\nTable 2. Example: A general definition of declarative ML.\nThe general definition of declarative ML is defined by a common definition of what is \"a particular set of functions.\" This definition of functions is in the form of: (a) a function defined by a type and type (b) a function defined by a type and type (c) a function defined by a type and type (d) a function defined by a type and type (e) a function defined by a type and type (f) a function defined by a type and type (g) a function defined by a type and type (h) a function defined by a type and type (i) a function defined by a type and type (ii) a function defined by a type and type (iii) a function defined by a type and type (iv) a function defined by a type and type (v) a function defined by a type and type (vi) a function defined by a type and type (vii) a function defined by a type and type (viii) a function defined by a type and type (v) a function defined by a type and type (viv) a function defined by a type and type (vvvvvvvvvvvvvvvvvvvvvvvvv", "histories": [["v1", "Thu, 19 May 2016 06:39:28 GMT  (32kb,D)", "http://arxiv.org/abs/1605.05826v1", null]], "reviews": [], "SUBJECTS": "cs.DB cs.DC cs.LG cs.PL", "authors": ["matthias boehm", "alexandre v evfimievski", "niketan pansare", "berthold reinwald"], "accepted": false, "id": "1605.05826"}, "pdf": {"name": "1605.05826.pdf", "metadata": {"source": "META", "title": "Declarative Machine Learning \u2013 A Classification of Basic Properties and Types", "authors": ["Matthias Boehm", "Alexandre V. Evfimievski", "Niketan Pansare", "Berthold Reinwald"], "emails": [], "sections": [{"heading": "1. INTRODUCTION", "text": "Large-scale machine learning (ML) leverages large data collections for advanced analytics in order to find interesting patterns and train robust predictive models. Traditional frameworks and tools like R, Matlab, Weka, SPSS, or SAS provide rich functionality but\u2014except for dedicated packages\u2014struggle to provide scalable analytics. Due to the data-intensive characteristics, increasingly often dataparallel frameworks like MapReduce [14], Spark [41], or Flink [3] are used for cost-effective parallelization on commodity hardware. However, large-scale computation inherently increases the complexity of specifying ML algorithms, especially with regard to efficient and scalable execution.\nLarge-Scale ML Libraries: Large-scale ML libraries like MLlib (aka SparkML) [30], Mahout [37], and MADlib [12, 23] are currently the predominant tools for largescale ML. These libraries provide algorithms with fixed distributed runtime plans and often expose the underlying physical data representation. Although such libraries are very valuable tools for end-users, it takes substantial effort to write new or customize existing algorithms because it requires knowledge of ML algorithms, their distributed implementation, and the underlying data-parallel framework. Similarly, improvements often require a modification of all individual algorithms to exploit these improvements.\nDeclarative ML: Declarative ML aims at a high-level specification of ML tasks or algorithms to simplify the usage and/or development of ML algorithms by separating application or algorithm semantics from the underlying data representations and execution plans. Table 1 categorizes types of declarative ML and delineates them from ML libraries. Overall, the major benefits of declarative ML are:\n\u2022 Simple, Analysis-Centric Specification, \u2022 Physical Data Independence, \u2022 Automatic Execution Plan Generation (optimization,\nplatform independence, data-size independence),\n\u2022 Ease of Deployment (platform independence, adaptivity of \u201cpackaged\u201d applications), and\n\u2022 Separation of Concerns (skill sets of users/devs).\nOver time, systems at different levels of abstraction have been proposed by industry and academia. Example systems range from UDF-centric ML extensions of data-parallel frameworks, to domain-specific languages (DSLs) for ML tasks or ML algorithms. This broad spectrum of systems aiming for declarative ML naturally led to a controversy regarding the scope of declarative ML. Not surprisingly\u2014as many ML algorithms are iterative\u2014the discussion centers around the syntax for specifying loops and control flow in general. Various projects adopt an R/Python-like syntax [1, 15, 22, 40, 42, 43] inheriting the full flexibility of loops, branches, and functions. Others support loops with (1) more restrictive iteration constructs [13, 17], (2) model updates with implicit convergence checks [8], or encapsulating entire algorithm classes as ML tasks [27, 33, 42]. We argue that the specific language-level syntax is actually irrelevant if the ML task or algorithm specification conforms to a set of basic properties required for declarative ML.\nContributions and Structure: The primary contribution of this paper is a systematic analysis and classification of declarative machine learning. We first define\u2014in an syntax-independent manner\u2014a set of basic properties in Section 2, that any system for declarative ML should satisfy. Subsequently, we describe the types of declarative ML in Section 3. Finally, we use this taxonomy to classify existing systems in Section 4, and draw conclusions for defining appropriate benchmarks and languages in Section 5.\nar X\niv :1\n60 5.\n05 82\n6v 1\n[ cs\n.D B\n] 1\n9 M\nay 2\n01 6"}, {"heading": "2. BASIC PROPERTIES", "text": "As a foundation for discussing types of declarative ML, we define essential, basic properties in the three categories of data, operations, and result correctness. We also discuss the implications of the individual properties and provide examples of how Apache SystemML\u2014as a representative system for declarative ML\u2014realizes these properties."}, {"heading": "2.1 Physical Data Independence", "text": "The most significant goal of declarative ML is data independence because it decouples the high-level specification of ML tasks or algorithms from the underlying data representations and related runtime plans and operations.\nProperty 1. Independence of Data Structures: The data types of inputs, intermediate results, and outputs like matrices or scalars are exposed as abstract data types without access to the underlying physical data representations.\nIn the context of declarative ML, independence of data structures serves two major purposes. First, abstract data types like matrix hide the decision on distributed vs local data representations. Accordingly, specified tasks or algorithms become independent of data size and deployment context (e.g., distributed computation vs streaming, different runtime backends). Second, abstract data types also hide the physical data representation (e.g., dense/sparse matrices, lossless compression), which allow internal improvements of storage and operation efficiency.\nProperty 2. Independence of Data Flow Properties: Data flow properties are not exposed, i.e., the user has, at specification level, no explicit control over properties like partitioning, caching, and blocking configurations.\nThe property of independence of data flow properties further restricts the notion of abstract data types, disallowing the explicit specification of interesting data flow properties. Examples are (1) caching and checkpointing (e.g., local and distributed caching, with certain storage levels), (2) logical and physical partitioning (e.g., row/column/block partitioning and range/hash partitioning of distributed data sets), (3) blocking configurations (row/column block sizes, fixed or variable logical/physical block sizes), as well as (4) data formats (text or binary cell/block formats). Note, however, that it is valid to allow the specification of ordering because it is both a logical and physical data flow property.\nExample SystemML: SystemML satisfies both properties by exposing only the abstract data types frame, matrix, and scalar without their physical data structures or interesting data flow properties as shown in Figure 1(a), as an example of a valid specification. The decisions on physical data flow properties are, however, crucial for performance. Hence, the system automatically injects, for example, caching and partitioning directives via rewrites. Overall, data independence allowed us to evolve and rebase SystemML without changing a single ML algorithm. Examples are extensions such as the support for different sparse representations, compression, and additional backends like Spark or GPUs. In contrast, for example, Mahout Samsara [15] does not satisfy the properties of data independence because decisions on dense/sparse and distributed/local matrices (e.g., drmFromHDFS, collect) as well as data flow properties like partitioning (e.g., par) and caching (e.g., checkpoint) are exposed to the user as shown in Figure 1(b)."}, {"heading": "2.2 Operation Semantics", "text": "The second major goal of declarative ML is to specify ML tasks or algorithms using domain-specific, high-level operations with well-defined semantics to simplify algorithm usage or development, and enable efficient evaluation plans.\nProperty 3. Analysis-Centric Operation Primitives: Basic operation primitives, common in the target analytics domain, are supported. For ML algorithms, this includes linear algebra and statistical functions, whereas for ML tasks, this includes task-specific primitives and models.\nIn order to allow declarative ML with simple specification, there is a need for operation primitives that closely resemble a natural description of ML tasks or algorithms at a conceptual level. For ML algorithms this includes linear algebra, aggregations, and statistical functions but specific domains like deep learning might require additional domain-specific operations like convolution. Similarly, for ML tasks this includes task-specific abstractions, operations, and models. For example, for specifying a task like classify, common classification algorithms, loss functions, and parameters should be supported to describe candidates and the optimization objective. The same applies for the task of general-purpose optimization optimize, where one would expect a way to specify gradient/loss functions, and termination conditions. Note that this property excludes systems that are declarative but unrelated to ML.\nProperty 4. Known Semantics of Operation Primitives: The semantics of operation primitives used to specify ML tasks or algorithms are known to the system in terms of knowledge of operation characteristics and equivalences.\nWe require operational semantics, where there exists at least one na\u0308\u0131ve evaluation plan or straightforward mapping. Knowledge of operation semantics is essential for generating efficient evaluation plans\u2014for example, via rewrites and operator selection\u2014from high-level specifications. In the context of ML, operation semantics also cover characteristics like commutativity and associativity, sparse-safeness (correctness of processing only non-zero cells), value repositioning (e.g., reorg operations like transpose, or order), symmetry properties, as well as an understanding of composite operations (e.g., sum-product for matrix multiplication). This meta information might be built into the system or annotated in case of extensible systems. Overall operation semantics allow to reason about equivalences, alternative execution strategies, and costs of these alternatives.\nProperty 5. Implementation-Agnostic Operations: The specification of ML tasks or algorithms is independent of the underlying runtime operations. This property prohibits user-defined execution strategies and parameterization.\nSpecifying implementation-agnostic operations\u2014i.e., independent of runtime backends, distributed vs local operations, and execution strategies\u2014is related to the properties for data independence (see Subsection 2.1) but with a focus on operations to ensure the flexibility of alternative or hybrid runtime backends, alternative deployments, and optimizations like rewrites and operator selection. Furthermore, avoiding low level parameterization like degree of parallelism or cache blocking ensures independence of the ML tasks or algorithms from workload characteristics (e.g., data size) and underlying hardware infrastructure.\nProperty 6. Well-Defined Plan Optimization Objective: ML tasks or algorithms specify their expected results unambiguously, using a well-defined (potentially multicriteria) objective for execution plan optimization.\nTo specify ML tasks or algorithms in an unambiguous manner, the specification must exhibit (implicitly or explicitly) a well-defined plan optimization objective. This property differentiates major types of declarative ML. If the specification relies on unambiguous operations, the implicit optimization objective is efficiency (runtime or resource requirements). In case of multi-objective optimizations, we further need to define a primary dimension and constraints for the other dimensions. For example, if we aim to optimize for both efficiency and accuracy, we might want to optimize accuracy in terms of a quality measure (e.g., L2 loss on a holdout dataset) as the primary dimension along with constraints on efficiency (e.g., time budget, number of models).\nExample SystemML: SystemML satisfies these properties by minimizing execution time (under memory budget constraints per execution context) of specified ML algorithms, composed of linear algebra and statistical operations with well-defined semantics. The known operation semantics are used to propagate dimension and sparsity information through the entire algorithm, compute memory estimates, apply static (size-independent) and dynamic (size-dependent) rewrites, and eventually decide upon alternative physical operators. Implementation-agnostic operations allow fine-grained optimization decisions per operation. Note that sequences of operations like t(X) %*% X %*% p from Figure 1(a) or specifications like independent foreach loops (parfor) [6] are assertions on semantics and properties of the algorithm rather than imperative execution strategies."}, {"heading": "2.3 Result Correctness", "text": "The properties of data independence and operation semantics are necessary but not sufficient for declarative ML. We further need to define the notion of result correctness. With regard to practicability for distributed computing, we define operation results as equivalent if they are essentially the same, i.e., they are algebraically (logically) equivalent, which ignores round-off errors (e.g., due to partial aggregation or alternative evaluation orders of operations).\nProperty 7. Implementation-Agnostic Results: The results of ML tasks or algorithms as well as individual operations are equivalent (essentially the same), independent of type and location of underlying runtime operations.\nThis property further qualifies implementation-agnostic operations (P5). In order to produce correct results, independent of optimization decisions, alternative execution strategies need to produce equivalent results no matter if\nthey are executed locally or as distributed operations. To accomplish that for an operation like rand with fixed seed, both local and distributed operations need to consistently generate seeds for fixed-sized blocks from the initially given input seed. Furthermore, this property prohibits, for example, lossy compression to reduce communication overhead.\nProperty 8. Deterministic Results: A given ML task or algorithm yields equivalent (essentially the same) results for multiple executions over the same input data and configuration. Randomized tasks or algorithms achieve this using pseudorandom number generators.\nDeterministic results of operations and ML tasks or algorithms is an important property, especially with regard to fault tolerance, where the same operation might be executed multiple times. Furthermore, it is also the basis for benchmarking ML systems in a systematic manner.\nExample SystemML: In SystemML, the properties of result correctness are satisfied via consistent local and distributed as well as deterministic operations. ML algorithms are composed of these operations, lifting the properties of result correctness to algorithm level too. Furthermore, we bound the round-off errors via numerically stable operations (based on Kahan+) [38] for descriptive statistics and aggregations. SystemML also provides configuration knobs to disable rewrites and operator selection to force strict computation. However, other than for debugging, we have not seen data scientists or end-users making use of that."}, {"heading": "3. TYPES OF DECLARATIVE ML", "text": "So far we discussed general properties of declarative machine learning, which apply to all types of declarative ML. We now create a taxonomy of types of declarative ML, namely declarative ML algorithms and declarative ML tasks. These types refer to fundamentally different concepts and thus, also differ in their scope of specification."}, {"heading": "3.1 Declarative ML Algorithms (Type 1)", "text": "Declarative ML algorithms allow data scientists to write and customize ML algorithms in a declarative manner. This scope requires fine-grained semantics including control flow and data flow, where the core operation primitives are often based on linear algebra or statistical functions and the common optimization objective is to minimize execution time but other objectives such as resource consumption are possible. The algorithm-centric specification defines precise semantics but leaves substantial freedom regarding data representations and execution plan optimization. This abstraction level allows data scientists to encode algorithms as they are most naturally expressed and thus, to quickly exploit the latest algorithmic advances. End users also benefit from simply calling these algorithms in terms of automatic optimization, adaptivity, and portability. Example system categories are DSL-centric, SQL-centric, and UDF-centric systems."}, {"heading": "3.2 Declarative ML Tasks (Type 2)", "text": "In contrast to declarative ML algorithms, declarative ML tasks allow end users (without ML background), to specify ML tasks like classify, factorize, optimize independent of ML algorithm specifics. This coarse-grained scope includes automatic feature and model selection and allows for the optimization of both model accuracy and runtime. Core operation primitives are task-specific, i.e., depending\non the task at hand, alternative candidate algorithms, loss functions (measure for goodness of fit), and hyper parameters are supported. Operation semantics are either built-in or annotated at the level of used algorithms or loss functions. The properties of declarative ML need to apply to the core optimization problem of the given ML task not the entire stack of used ML algorithms, as long as they are annotated with relevant properties that allow reasoning about alternative plans and costs. However, relying on declarative ML algorithms provides additional flexibility. Example system categories for declarative ML tasks are general-purpose optimization, model selection, and feature selection."}, {"heading": "4. SYSTEMS CLASSIFICATION", "text": "Given the taxonomy of basic properties and types of declarative ML, we now classify existing systems. There has been some related work on similar classifications, most notably, Kumar et al. defined a notion of a model selection management system [28], along with categories of ML systems, but primarily focused on coverage of industrial systems rather than specifics of declarative machine learning. Tables 2 and 3 classify\u2014in the scope of declarative ML algorithms and tasks\u2014existing systems with regard to the defined basic properties and types of declarative machine learning. This classification also indicates distributed vs local operations and the used optimization objective."}, {"heading": "4.1 Declarative ML Algorithms", "text": "DSL-Centric Systems: The class of DSL-centric systems focuses on domain-specific languages (DSLs) for ML to simplify the writing of ML algorithms. Early examples of declarative systems are RIOT [43] and OptiML [35], which provide R and Scala DSLs, respectively. Both focus on single-node computation only which makes it easier to adhere to basic properties of declarative machine learning. SystemML [7, 22] covers both single-node and distributed computation (on MapReduce and Spark) and satisfies all eight properties of declarative ML as described throughout this paper. More recent systems like Cumulon [24, 25], Mahout Samsara [15], DMac [40], and TensorFlow [1] similarly aim for declarative, large-scale ML but struggle to satisfy all properties of declarative ML. Cumulon [24, 25] can be seen as a declarative system. However, in a strict sense, the optimization objective is ill-defined (P6) as the hard runtime constraint cannot be satisfied without knowing the number\nof iterations until convergence. Cumulon still allows users to explore per-iteration trade-offs of monetary cost and runtime in cloud environments. Mahout Samsara [15], Distributed R [39], and DMac [40] are not declarative because they expose physical data structures and distributed operations to the user (P1, P5). Mahout Samsara and Distributed R require the user to decide between local and distributed matrices and expose data flow properties like caching and partitioning (P2), while DMac exposes dense/sparse data structures. Additionally, Distributed R executes arbitrary user-defined R functions\u2014i.e., with unknown operation semantics (P4)\u2014per partition. TensorFlow [1] is a compelling system but focuses more on extensibility than declarative specification. Accordingly, operations are handled as blackbox kernels, i.e., with unknown operation semantics (P4). Also, deep-learning-centric optimizations like lossy compression for communication work very well for noisy data but do not satisfy the property of implementation-agnostic results (P7) required for general-purpose declarative ML.\nSQL-Centric Systems: The class of SQL-centric systems complements the class of DSL-centric systems as both aim for custom analysis algorithms. Common SQL-centric systems are array databases and SQL-like ML (e.g., for Bayesian ML). These systems often follow the compelling argument of integrating advanced analytics with traditional query processing to simplify data pre-processing and leverage well-understood abstractions for data and operations. At the same time, linear algebra operations are either directly supported or emulated. A prime example of array databases is SciDB [9, 34] which indeed satisfies all basic properties for declarative ML. Furthermore, also SimSQL [11]\u2014as an example of Bayesian ML or more broadly stochastic analysis\u2014 satisfies all basic properties of declarative ML.\nUDF-Centric Systems: There is also a class of UDFcentric systems that evolved bottom up from existing dataparallel frameworks like MapReduce, Spark, or Flink as well as compiler frameworks to simplify large-scale ML. Examples are ScalOps [8] (which compiles Scala UDF workflows to datalog programs), Tupleware [13] (which compiles workflows of UDFs in various frontend languages to custom distributed programs of native code via LLVM), and Emma [4] (which compiles Scala UDF workflows to Spark and Flink programs). These systems cover a great variety of use cases, but they systematically fail to satisfy several properties of declarative ML. First, data independence (P1)\nis not satisfied as the UDFs are implemented against custom data structures which makes it hard to efficiently support dense/sparse or compressed datasets. Second, the operation semantics of UDFs are by definition unknown (P4), miss support of analysis-centric operations (P3), and the focus on large-scale computation requires the UDF workflows to realize distributed algorithm implementations (P5)."}, {"heading": "4.2 Declarative ML Tasks", "text": "As discussed before, systems for declarative ML tasks mostly target end users (not algorithm developers) and automatically optimize for runtime and accuracy of generalpurpose optimization or model and feature selection tasks.\nGeneral-Purpose Optimization: Bismarck [18] provides in-database general-purpose optimization via incremental gradient decent, where users provide UDFs for initialization, transition, and termination. This abstraction covers many algorithms over existing relational data. Similar to UDF-centric systems, however, it does not satisfy the independence of data structures (P1) and known operation semantics (P4). Furthermore, effective parallelization requires either violations of implementation-agnostic and deterministic results (P7, P8) for the pure UDA approach or modifications of the UDFs (P5) for the shared-memory UDA approach. The latter also does not satisfy P7/P8 due to Hogwild!-style [31] model updates. TensorFlow [1] also provides primitives for general-purpose optimization via different optimization algorithms. Similar to Bismarck, users provide UDFs for loss and gradient computation. However, as TensorFlow provides sufficient data abstraction via so-called placeholders, it satisfies the properties of data independence. Furthermore, the operation semantics of inference, loss, and training are known and UDFs can leverage the existing built-in functions which also provide abstractions for gradient computation. Thus, although TensorFlow did not qualify for declarative ML algorithms, it satisfies the properties at the level of declarative ML tasks.\nModel and Feature Selection: MLbase [27] with its TUPAQ [33] component for automatic model search allows users to specify candidate model configurations, a quality measure, and runtime constraints (e.g., models considered, number of scans, etc) and returns the best model along with tuned hyperparameters. Disregarding the underlying library of ML algorithms, MLbase can be classified as a system for declarative ML tasks, as the model search problem is welldefined, independent of the underlying data and operations, with annotated algorithm characteristics, and deterministic results (unless the runtime constraint is a runtime budget). Columbus [42] allows users to specify feature engineering workflows in R including data preparations and model building with existing R packages. The optimization objective is to minimize runtime, but an error tolerance allows for more\naggressive reuse by leveraging runtime-accuracy trade-offs. Columbus also satisfies the properties for declarative ML tasks. DeepDive [32] enables knowledge base construction via statistical inference. In a first step, users specify SQL queries and UDFs for feature extraction to populate a factor graph. In a second step, candidate mappings are specified via SQL queries to express rules for entities and relations. Finally, marginal probabilities are learned via statistical inference over this factor graph. As data and operations are abstracted via SQL and the actual inference algorithms and details are not exposed, DeepDive can be classified as a declarative system for feature selection.\nOther System Categories: Finally, there are also declarative systems for more specific ML tasks like time series forecasting (e.g., Fa [16], a skip-list approach [20], or F2DB [19]), which also consider the trade-off between accuracy and runtime (model selection) but are not subject to this classification for general-purpose declarative ML."}, {"heading": "5. BENCHMARKING ML SYSTEMS", "text": "Having discussed the individual types of declarative ML, it is clear that there cannot exist a single benchmark to cover them all. Existing benchmarks for large-scale computation like BigBench [5, 21], SparkBench [2, 29], or HiBench [26] do cover machine learning but often simply refer to reference implementations of large-scale ML libraries. This is fine to evaluate underlying Hadoop or Spark implementations but simply cannot serve as a benchmark for declarative ML.\nA Case for Type-Specific Benchmarks: We argue that industry and academia is best served with benchmarks specific to types of declarative machine learning (see Section 3). In order to properly reflect common workload characteristics, the benchmarks should be further tailored to major subcategories of systems. For example, systems for declarative ML algorithms cover the major sub-categories of DSL-centric, UDF-centric, and SQL-centric systems, which\u2014despite some overlap of operation primitives\u2014all target different primary usage scenarios. This calls for specific benchmarks that allow fair comparisons and foster system advancements via challenging workloads. Interestingly, exactly that already happened for SQL-centric systems as both, the SciDB [9, 34] and SimSQL [11] projects published benchmarks covering the main characteristics of array databases [36] and Bayesian ML [10]. Defining simple yet challenging ML benchmarks that also cover common workload characteristics is not easy, so we\u2014as a community\u2014 should highly appreciate contributions in this area.\nNote on Specification Languages: The same reasoning as with system-type-specific benchmarks also applies to specification languages. In order to satisfy the property of analysis-centric operation support (P4), systems need to support the operations of their primary analysis use case,\nwhich motivates tailor-made specification languages. However, as the properties for declarative ML are syntax independent, we could establish a common syntax for a specification language to cover multiple types of declarative ML."}, {"heading": "6. CONCLUSION", "text": "To summarize, we introduced a taxonomy of declarative ML in terms of basic properties of data, operations, and result correctness as well as types of systems for declarative ML. The classification of existing systems has shown that this taxonomy is indeed a useful tool for qualifying systems characteristics in a systematic manner. Fundamentally, this makes a case for a syntax-independent classification of declarative ML, which disqualifies the philosophical argument against loops and control flow in general. We are at the beginning of an exciting era of declarative ML, with a good understanding of various aspects but also lots of open research challenges. As advanced analytics become ubiquitous and technology environments are changing at an increasing rate, a declarative specification of ML tasks or algorithms becomes increasingly important. Accordingly, we encourage the research community to participate in this discussion on basic properties of declarative ML in order to eventually converge to a common understanding."}, {"heading": "7. REFERENCES", "text": "[1] M. Abadi et al. TensorFlow: Large-Scale Machine Learning\non Heterogeneous Distributed Systems. CoRR, abs/1603.04467, 2016.\n[2] D. Agrawal et al. SparkBench - A Spark Performance Testing Suite. In TPCTC, 2015. [3] A. Alexandrov et al. The Stratosphere Platform for Big Data Analytics. VLDB J., 23(6), 2014. [4] A. Alexandrov et al. Implicit Parallelism through Deep Language Embedding. In SIGMOD, 2015. [5] C. K. Baru et al. Discussion of BigBench: A Proposed Industry Standard Performance Benchmark for Big Data. In TPCTC, 2014. [6] M. Boehm et al. Hybrid Parallelization Strategies for Large-Scale Machine Learning in SystemML. PVLDB, 7(7), 2014. [7] M. Boehm et al. SystemML\u2019s Optimizer: Plan Generation for Large-Scale Machine Learning Programs. IEEE Data Eng. Bull., 37(3), 2014. [8] V. R. Borkar et al. Declarative Systems for Large-Scale Machine Learning. IEEE Data Eng. Bull., 35(2), 2012. [9] P. G. Brown. Overview of SciDB: Large Scale Array Storage, Processing and Analysis. In SIGMOD, 2010.\n[10] Z. Cai, Z. J. Gao, S. Luo, L. L. Perez, Z. Vagena, and C. M. Jermaine. A Comparison of Platforms for Implementing and Running Very Large Scale Machine Learning Algorithms. In SIGMOD, 2014. [11] Z. Cai, Z. Vagena, L. L. Perez, S. Arumugam, P. J. Haas, and C. M. Jermaine. Simulation of Database-Valued Markov Chains Using SimSQL. In SIGMOD, 2013. [12] J. Cohen, B. Dolan, M. Dunlap, J. M. Hellerstein, and C. Welton. MAD Skills: New Analysis Practices for Big Data. PVLDB, 2(2), 2009. [13] A. Crotty et al. An Architecture for Compiling UDF-centric Workflows. PVLDB, 8(12), 2015. [14] J. Dean and S. Ghemawat. MapReduce: Simplified Data Processing on Large Clusters. In OSDI, 2004. [15] Dmitriy Lyubimov. Mahout Scala Bindings and Mahout Spark Bindings for Linear Algebra Subroutines. Apache, 2016. mahout.apache.org/users/sparkbindings. [16] S. Duan and S. Babu. Processing Forecasting Queries. In VLDB, 2007.\n[17] S. Ewen, K. Tzoumas, M. Kaufmann, and V. Markl. Spinning Fast Iterative Data Flows. PVLDB, 5(11), 2012. [18] X. Feng, A. Kumar, B. Recht, and C. Re\u0301. Towards a Unified Architecture for In-RDBMS Analytics. In SIGMOD, 2012. [19] U. Fischer, F. Rosenthal, and W. Lehner. F2DB: The Flash-Forward Database System. In ICDE, 2012. [20] T. Ge and S. B. Zdonik. A Skip-List Approach for Efficiently Processing Forecasting Queries. PVLDB, 1(1), 2008. [21] A. Ghazal et al. BigBench: Towards an Industry Standard Benchmark for Big Data Analytics. In SIGMOD, 2013. [22] A. Ghoting et al. SystemML: Declarative Machine Learning on MapReduce. In ICDE, 2011. [23] J. M. Hellerstein et al. The MADlib Analytics Library or MAD Skills, the SQL. PVLDB, 5(12), 2012. [24] B. Huang, S. Babu, and J. Yang. Cumulon: Optimizing Statistical Data Analysis in the Cloud. In SIGMOD, 2013. [25] B. Huang, N. W. D. Jarrett, S. Babu, S. Mukherjee, and J. Yang. Cumulon: Matrix-Based Data Analytics in the Cloud with Spot Instances. PVLDB, 9(3), 2015. [26] S. Huang, J. Huang, J. Dai, T. Xie, and B. Huang. The HiBench Benchmark Suite: Characterization of the MapReduce-based data analysis. In ICDE Workshops, 2010. [27] T. Kraska, A. Talwalkar, J. C. Duchi, R. Griffith, M. J. Franklin, and M. I. Jordan. MLbase: A Distributed Machine-learning System. In CIDR, 2013. [28] A. Kumar, R. McCann, J. Naughton, and J. M. Patel. Model Selection Management Systems: The Next Frontier of Advanced Analytics. SIGMOD Record, 2015. [29] M. Li, J. Tan, Y. Wang, L. Zhang, and V. Salapura. SparkBench: A Comprehensive Benchmarking Suite For In Memory Data Analytic Platform Spark. In CF, 2015. [30] X. Meng et al. MLlib: Machine Learning in Apache Spark. CoRR, abs/1505.06807, 2015. [31] B. Recht, C. Re, S. J. Wright, and F. Niu. Hogwild: A Lock-Free Approach to Parallelizing Stochastic Gradient Descent. In NIPS, 2011. [32] J. Shin, S. Wu, F. Wang, C. D. Sa, C. Zhang, and C. Re\u0301. Incremental Knowledge Base Construction Using DeepDive. PVLDB, 8(11), 2015. [33] E. R. Sparks, A. Talwalkar, D. Haas, M. J. Franklin, M. I. Jordan, and T. Kraska. Automating Model Search for Large Scale Machine Learning. In SOCC, 2015. [34] M. Stonebraker, P. Brown, A. Poliakov, and S. Raman. The Architecture of SciDB. In SSDBM, 2011. [35] A. K. Sujeeth et al. OptiML: An Implicitly Parallel Domain-Specific Language for Machine Learning. In ICML, 2011. [36] R. Taft, M. Vartak, N. R. Satish, N. Sundaram, S. Madden, and M. Stonebraker. GenBase: A Complex Analytics Genomics Benchmark. In SIGMOD, 2014. [37] The Apache Software Foundation. Mahout. [38] Y. Tian, S. Tatikonda, and B. Reinwald. Scalable and\nNumerically Stable Descriptive Statistics in SystemML. In ICDE, 2012. [39] S. Venkataraman, E. Bodzsar, I. Roy, A. AuYoung, and R. S. Schreiber. Presto: Distributed Machine Learning and Graph Processing with Sparse Matrices. In EuroSys, 2013. [40] L. Yu, Y. Shao, and B. Cui. Exploiting Matrix Dependency for Efficient Distributed Matrix Computation. In SIGMOD, 2015. [41] M. Zaharia et al. Resilient Distributed Datasets: A Fault-Tolerant Abstraction for In-Memory Cluster Computing. In NSDI, 2012. [42] C. Zhang, A. Kumar, and C. Re\u0301. Materialization Optimizations for Feature Selection Workloads. In SIGMOD, 2014. [43] Y. Zhang, H. Herodotou, and J. Yang. RIOT: I/O-Efficient Numerical Computing without SQL. In CIDR, 2009."}], "references": [{"title": "TensorFlow: Large-Scale Machine", "author": ["M. Abadi"], "venue": "Learning on Heterogeneous Distributed Systems. CoRR,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2016}, {"title": "SparkBench - A Spark Performance Testing Suite", "author": ["D. Agrawal"], "venue": "TPCTC,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2015}, {"title": "The Stratosphere Platform for Big Data Analytics", "author": ["A. Alexandrov"], "venue": "VLDB J.,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2014}, {"title": "Implicit Parallelism through Deep Language Embedding", "author": ["A. Alexandrov"], "venue": "In SIGMOD,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2015}, {"title": "Discussion of BigBench: A Proposed Industry Standard Performance Benchmark for Big Data", "author": ["C.K. Baru"], "venue": "In TPCTC,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2014}, {"title": "Hybrid Parallelization Strategies for Large-Scale Machine Learning in SystemML", "author": ["M. Boehm"], "venue": null, "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2014}, {"title": "SystemML\u2019s Optimizer: Plan Generation for Large-Scale Machine Learning Programs", "author": ["M. Boehm"], "venue": "IEEE Data Eng. Bull.,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2014}, {"title": "Declarative Systems for Large-Scale Machine Learning", "author": ["V.R. Borkar"], "venue": "IEEE Data Eng. Bull.,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2012}, {"title": "Overview of SciDB: Large Scale Array Storage, Processing and Analysis", "author": ["P.G. Brown"], "venue": "In SIGMOD,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2010}, {"title": "A Comparison of Platforms for Implementing and Running Very Large Scale Machine Learning Algorithms", "author": ["Z. Cai", "Z.J. Gao", "S. Luo", "L.L. Perez", "Z. Vagena", "C.M. Jermaine"], "venue": "In SIGMOD,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2014}, {"title": "Simulation of Database-Valued Markov Chains Using SimSQL", "author": ["Z. Cai", "Z. Vagena", "L.L. Perez", "S. Arumugam", "P.J. Haas", "C.M. Jermaine"], "venue": "In SIGMOD,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2013}, {"title": "MAD Skills: New Analysis Practices for Big Data", "author": ["J. Cohen", "B. Dolan", "M. Dunlap", "J.M. Hellerstein", "C. Welton"], "venue": null, "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2009}, {"title": "An Architecture for Compiling UDF-centric", "author": ["A. Crotty"], "venue": "Workflows. PVLDB,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2015}, {"title": "MapReduce: Simplified Data Processing on Large Clusters", "author": ["J. Dean", "S. Ghemawat"], "venue": "In OSDI,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2004}, {"title": "Processing Forecasting Queries", "author": ["S. Duan", "S. Babu"], "venue": "In VLDB,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2007}, {"title": "Spinning Fast Iterative Data Flows", "author": ["S. Ewen", "K. Tzoumas", "M. Kaufmann", "V. Markl"], "venue": null, "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2012}, {"title": "Towards a Unified Architecture for In-RDBMS Analytics", "author": ["X. Feng", "A. Kumar", "B. Recht", "C. R\u00e9"], "venue": "In SIGMOD,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2012}, {"title": "F2DB: The Flash-Forward Database System", "author": ["U. Fischer", "F. Rosenthal", "W. Lehner"], "venue": "In ICDE,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2012}, {"title": "A Skip-List Approach for Efficiently Processing", "author": ["T. Ge", "S.B. Zdonik"], "venue": "Forecasting Queries. PVLDB,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2008}, {"title": "BigBench: Towards an Industry Standard Benchmark for Big Data Analytics", "author": ["A. Ghazal"], "venue": "In SIGMOD,", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2013}, {"title": "SystemML: Declarative Machine Learning on MapReduce", "author": ["A. Ghoting"], "venue": "In ICDE,", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2011}, {"title": "The MADlib Analytics Library or MAD Skills, the SQL", "author": ["J.M. Hellerstein"], "venue": null, "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2012}, {"title": "Cumulon: Optimizing Statistical Data Analysis in the Cloud", "author": ["B. Huang", "S. Babu", "J. Yang"], "venue": "In SIGMOD,", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2013}, {"title": "Cumulon: Matrix-Based Data Analytics in the Cloud with Spot Instances", "author": ["B. Huang", "N.W.D. Jarrett", "S. Babu", "S. Mukherjee", "J. Yang"], "venue": null, "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2015}, {"title": "The HiBench Benchmark Suite: Characterization of the MapReduce-based data analysis", "author": ["S. Huang", "J. Huang", "J. Dai", "T. Xie", "B. Huang"], "venue": "In ICDE Workshops,", "citeRegEx": "26", "shortCiteRegEx": "26", "year": 2010}, {"title": "MLbase: A Distributed Machine-learning System", "author": ["T. Kraska", "A. Talwalkar", "J.C. Duchi", "R. Griffith", "M.J. Franklin", "M.I. Jordan"], "venue": "In CIDR,", "citeRegEx": "27", "shortCiteRegEx": "27", "year": 2013}, {"title": "Model Selection Management Systems: The Next Frontier of Advanced Analytics", "author": ["A. Kumar", "R. McCann", "J. Naughton", "J.M. Patel"], "venue": "SIGMOD Record,", "citeRegEx": "28", "shortCiteRegEx": "28", "year": 2015}, {"title": "SparkBench: A Comprehensive Benchmarking Suite For In Memory Data Analytic Platform Spark", "author": ["M. Li", "J. Tan", "Y. Wang", "L. Zhang", "V. Salapura"], "venue": "In CF,", "citeRegEx": "29", "shortCiteRegEx": "29", "year": 2015}, {"title": "Hogwild: A Lock-Free Approach to Parallelizing Stochastic Gradient Descent", "author": ["B. Recht", "C. Re", "S.J. Wright", "F. Niu"], "venue": "In NIPS,", "citeRegEx": "31", "shortCiteRegEx": "31", "year": 2011}, {"title": "Incremental Knowledge Base Construction", "author": ["J. Shin", "S. Wu", "F. Wang", "C.D. Sa", "C. Zhang", "C. R\u00e9"], "venue": "Using DeepDive. PVLDB,", "citeRegEx": "32", "shortCiteRegEx": "32", "year": 2015}, {"title": "Automating Model Search for Large Scale Machine Learning", "author": ["E.R. Sparks", "A. Talwalkar", "D. Haas", "M.J. Franklin", "M.I. Jordan", "T. Kraska"], "venue": "SOCC,", "citeRegEx": "33", "shortCiteRegEx": "33", "year": 2015}, {"title": "The Architecture of SciDB", "author": ["M. Stonebraker", "P. Brown", "A. Poliakov", "S. Raman"], "venue": "In SSDBM,", "citeRegEx": "34", "shortCiteRegEx": "34", "year": 2011}, {"title": "OptiML: An Implicitly Parallel Domain-Specific Language for Machine Learning", "author": ["A.K. Sujeeth"], "venue": "In ICML,", "citeRegEx": "35", "shortCiteRegEx": "35", "year": 2011}, {"title": "GenBase: A Complex Analytics Genomics Benchmark", "author": ["R. Taft", "M. Vartak", "N.R. Satish", "N. Sundaram", "S. Madden", "M. Stonebraker"], "venue": "In SIGMOD,", "citeRegEx": "36", "shortCiteRegEx": "36", "year": 2014}, {"title": "Scalable and Numerically Stable Descriptive Statistics in SystemML", "author": ["Y. Tian", "S. Tatikonda", "B. Reinwald"], "venue": "In ICDE,", "citeRegEx": "38", "shortCiteRegEx": "38", "year": 2012}, {"title": "Presto: Distributed Machine Learning and Graph Processing with Sparse Matrices", "author": ["S. Venkataraman", "E. Bodzsar", "I. Roy", "A. AuYoung", "R.S. Schreiber"], "venue": "In EuroSys,", "citeRegEx": "39", "shortCiteRegEx": "39", "year": 2013}, {"title": "Exploiting Matrix Dependency for Efficient Distributed Matrix Computation", "author": ["L. Yu", "Y. Shao", "B. Cui"], "venue": "In SIGMOD,", "citeRegEx": "40", "shortCiteRegEx": "40", "year": 2015}, {"title": "Resilient Distributed Datasets: A Fault-Tolerant Abstraction for In-Memory Cluster Computing", "author": ["M. Zaharia"], "venue": "In NSDI,", "citeRegEx": "41", "shortCiteRegEx": "41", "year": 2012}, {"title": "Materialization Optimizations for Feature Selection Workloads", "author": ["C. Zhang", "A. Kumar", "C. R\u00e9"], "venue": "In SIGMOD,", "citeRegEx": "42", "shortCiteRegEx": "42", "year": 2014}, {"title": "RIOT: I/O-Efficient Numerical Computing without SQL", "author": ["Y. Zhang", "H. Herodotou", "J. Yang"], "venue": "In CIDR,", "citeRegEx": "43", "shortCiteRegEx": "43", "year": 2009}], "referenceMentions": [{"referenceID": 13, "context": "Due to the data-intensive characteristics, increasingly often dataparallel frameworks like MapReduce [14], Spark [41], or Flink [3] are used for cost-effective parallelization on commodity hardware.", "startOffset": 101, "endOffset": 105}, {"referenceID": 37, "context": "Due to the data-intensive characteristics, increasingly often dataparallel frameworks like MapReduce [14], Spark [41], or Flink [3] are used for cost-effective parallelization on commodity hardware.", "startOffset": 113, "endOffset": 117}, {"referenceID": 2, "context": "Due to the data-intensive characteristics, increasingly often dataparallel frameworks like MapReduce [14], Spark [41], or Flink [3] are used for cost-effective parallelization on commodity hardware.", "startOffset": 128, "endOffset": 131}, {"referenceID": 11, "context": "Large-Scale ML Libraries: Large-scale ML libraries like MLlib (aka SparkML) [30], Mahout [37], and MADlib [12, 23] are currently the predominant tools for largescale ML.", "startOffset": 106, "endOffset": 114}, {"referenceID": 21, "context": "Large-Scale ML Libraries: Large-scale ML libraries like MLlib (aka SparkML) [30], Mahout [37], and MADlib [12, 23] are currently the predominant tools for largescale ML.", "startOffset": 106, "endOffset": 114}, {"referenceID": 25, "context": ", MLbase [27, 33], (fixed task) Columbus [42], DeepDive [32]", "startOffset": 9, "endOffset": 17}, {"referenceID": 30, "context": ", MLbase [27, 33], (fixed task) Columbus [42], DeepDive [32]", "startOffset": 9, "endOffset": 17}, {"referenceID": 38, "context": ", MLbase [27, 33], (fixed task) Columbus [42], DeepDive [32]", "startOffset": 41, "endOffset": 45}, {"referenceID": 29, "context": ", MLbase [27, 33], (fixed task) Columbus [42], DeepDive [32]", "startOffset": 56, "endOffset": 60}, {"referenceID": 32, "context": ", OptiML [35], SciDB [9, 34] (fixed algorithm) SystemML [7, 22], SimSQL [11]", "startOffset": 9, "endOffset": 13}, {"referenceID": 8, "context": ", OptiML [35], SciDB [9, 34] (fixed algorithm) SystemML [7, 22], SimSQL [11]", "startOffset": 21, "endOffset": 28}, {"referenceID": 31, "context": ", OptiML [35], SciDB [9, 34] (fixed algorithm) SystemML [7, 22], SimSQL [11]", "startOffset": 21, "endOffset": 28}, {"referenceID": 6, "context": ", OptiML [35], SciDB [9, 34] (fixed algorithm) SystemML [7, 22], SimSQL [11]", "startOffset": 56, "endOffset": 63}, {"referenceID": 20, "context": ", OptiML [35], SciDB [9, 34] (fixed algorithm) SystemML [7, 22], SimSQL [11]", "startOffset": 56, "endOffset": 63}, {"referenceID": 10, "context": ", OptiML [35], SciDB [9, 34] (fixed algorithm) SystemML [7, 22], SimSQL [11]", "startOffset": 72, "endOffset": 76}, {"referenceID": 11, "context": ", MLlib [30], Mahout [37], (fixed plan) MADlib [12, 23], ORE, Rev R", "startOffset": 47, "endOffset": 55}, {"referenceID": 21, "context": ", MLlib [30], Mahout [37], (fixed plan) MADlib [12, 23], ORE, Rev R", "startOffset": 47, "endOffset": 55}, {"referenceID": 0, "context": "Various projects adopt an R/Python-like syntax [1, 15, 22, 40, 42, 43] inheriting the full flexibility of loops, branches, and functions.", "startOffset": 47, "endOffset": 70}, {"referenceID": 20, "context": "Various projects adopt an R/Python-like syntax [1, 15, 22, 40, 42, 43] inheriting the full flexibility of loops, branches, and functions.", "startOffset": 47, "endOffset": 70}, {"referenceID": 36, "context": "Various projects adopt an R/Python-like syntax [1, 15, 22, 40, 42, 43] inheriting the full flexibility of loops, branches, and functions.", "startOffset": 47, "endOffset": 70}, {"referenceID": 38, "context": "Various projects adopt an R/Python-like syntax [1, 15, 22, 40, 42, 43] inheriting the full flexibility of loops, branches, and functions.", "startOffset": 47, "endOffset": 70}, {"referenceID": 39, "context": "Various projects adopt an R/Python-like syntax [1, 15, 22, 40, 42, 43] inheriting the full flexibility of loops, branches, and functions.", "startOffset": 47, "endOffset": 70}, {"referenceID": 12, "context": "Others support loops with (1) more restrictive iteration constructs [13, 17], (2) model updates with implicit convergence checks [8], or encapsulating entire algorithm classes as ML tasks [27, 33, 42].", "startOffset": 68, "endOffset": 76}, {"referenceID": 15, "context": "Others support loops with (1) more restrictive iteration constructs [13, 17], (2) model updates with implicit convergence checks [8], or encapsulating entire algorithm classes as ML tasks [27, 33, 42].", "startOffset": 68, "endOffset": 76}, {"referenceID": 7, "context": "Others support loops with (1) more restrictive iteration constructs [13, 17], (2) model updates with implicit convergence checks [8], or encapsulating entire algorithm classes as ML tasks [27, 33, 42].", "startOffset": 129, "endOffset": 132}, {"referenceID": 25, "context": "Others support loops with (1) more restrictive iteration constructs [13, 17], (2) model updates with implicit convergence checks [8], or encapsulating entire algorithm classes as ML tasks [27, 33, 42].", "startOffset": 188, "endOffset": 200}, {"referenceID": 30, "context": "Others support loops with (1) more restrictive iteration constructs [13, 17], (2) model updates with implicit convergence checks [8], or encapsulating entire algorithm classes as ML tasks [27, 33, 42].", "startOffset": 188, "endOffset": 200}, {"referenceID": 38, "context": "Others support loops with (1) more restrictive iteration constructs [13, 17], (2) model updates with implicit convergence checks [8], or encapsulating entire algorithm classes as ML tasks [27, 33, 42].", "startOffset": 188, "endOffset": 200}, {"referenceID": 5, "context": "Note that sequences of operations like t(X) %*% X %*% p from Figure 1(a) or specifications like independent foreach loops (parfor) [6] are assertions on semantics and properties of the algorithm rather than imperative execution strategies.", "startOffset": 131, "endOffset": 134}, {"referenceID": 34, "context": "Furthermore, we bound the round-off errors via numerically stable operations (based on Kahan+) [38] for descriptive statistics and aggregations.", "startOffset": 95, "endOffset": 99}, {"referenceID": 39, "context": "RIOT [43] X X X X X X X X 1 min runtime OptiML [35] X X X X X X X X 1 min runtime SystemML [7, 22] X X X X X X X X X 1 min runtime s.", "startOffset": 5, "endOffset": 9}, {"referenceID": 32, "context": "RIOT [43] X X X X X X X X 1 min runtime OptiML [35] X X X X X X X X 1 min runtime SystemML [7, 22] X X X X X X X X X 1 min runtime s.", "startOffset": 47, "endOffset": 51}, {"referenceID": 6, "context": "RIOT [43] X X X X X X X X 1 min runtime OptiML [35] X X X X X X X X 1 min runtime SystemML [7, 22] X X X X X X X X X 1 min runtime s.", "startOffset": 91, "endOffset": 98}, {"referenceID": 20, "context": "RIOT [43] X X X X X X X X 1 min runtime OptiML [35] X X X X X X X X 1 min runtime SystemML [7, 22] X X X X X X X X X 1 min runtime s.", "startOffset": 91, "endOffset": 98}, {"referenceID": 35, "context": "memory constraints Mahout Samsara [15] X X X X X X N/A min runtime Distributed R [39] X X X X X N/A min runtime Cumulon [24, 25] X X X X X X X X N/A min costs s.", "startOffset": 81, "endOffset": 85}, {"referenceID": 22, "context": "memory constraints Mahout Samsara [15] X X X X X X N/A min runtime Distributed R [39] X X X X X N/A min runtime Cumulon [24, 25] X X X X X X X X N/A min costs s.", "startOffset": 120, "endOffset": 128}, {"referenceID": 23, "context": "memory constraints Mahout Samsara [15] X X X X X X N/A min runtime Distributed R [39] X X X X X N/A min runtime Cumulon [24, 25] X X X X X X X X N/A min costs s.", "startOffset": 120, "endOffset": 128}, {"referenceID": 36, "context": "runtime constraints DMac [40] X X X X X X X N/A min runtime s.", "startOffset": 25, "endOffset": 29}, {"referenceID": 0, "context": "memory constraints TensorFlow [1] X X X X X X X N/A min runtime s.", "startOffset": 30, "endOffset": 33}, {"referenceID": 8, "context": "SciDB [9, 34] X X X X X X X X X 1 min runtime SimSQL [11] X X X X X X X X X 1 min runtime", "startOffset": 6, "endOffset": 13}, {"referenceID": 31, "context": "SciDB [9, 34] X X X X X X X X X 1 min runtime SimSQL [11] X X X X X X X X X 1 min runtime", "startOffset": 6, "endOffset": 13}, {"referenceID": 10, "context": "SciDB [9, 34] X X X X X X X X X 1 min runtime SimSQL [11] X X X X X X X X X 1 min runtime", "startOffset": 53, "endOffset": 57}, {"referenceID": 7, "context": "ScalOps [8] X X X X X N/A min runtime Tupleware [13] X X X X X N/A min runtime Emma [4] X X X X X N/A min runtime", "startOffset": 8, "endOffset": 11}, {"referenceID": 12, "context": "ScalOps [8] X X X X X N/A min runtime Tupleware [13] X X X X X N/A min runtime Emma [4] X X X X X N/A min runtime", "startOffset": 48, "endOffset": 52}, {"referenceID": 3, "context": "ScalOps [8] X X X X X N/A min runtime Tupleware [13] X X X X X N/A min runtime Emma [4] X X X X X N/A min runtime", "startOffset": 84, "endOffset": 87}, {"referenceID": 26, "context": "defined a notion of a model selection management system [28], along with categories of ML systems, but primarily focused on coverage of industrial systems rather than specifics of declarative machine learning.", "startOffset": 56, "endOffset": 60}, {"referenceID": 39, "context": "Early examples of declarative systems are RIOT [43] and OptiML [35], which provide R and Scala DSLs, respectively.", "startOffset": 47, "endOffset": 51}, {"referenceID": 32, "context": "Early examples of declarative systems are RIOT [43] and OptiML [35], which provide R and Scala DSLs, respectively.", "startOffset": 63, "endOffset": 67}, {"referenceID": 6, "context": "SystemML [7, 22] covers both single-node and distributed computation (on MapReduce and Spark) and satisfies all eight properties of declarative ML as described throughout this paper.", "startOffset": 9, "endOffset": 16}, {"referenceID": 20, "context": "SystemML [7, 22] covers both single-node and distributed computation (on MapReduce and Spark) and satisfies all eight properties of declarative ML as described throughout this paper.", "startOffset": 9, "endOffset": 16}, {"referenceID": 22, "context": "More recent systems like Cumulon [24, 25], Mahout Samsara [15], DMac [40], and TensorFlow [1] similarly aim for declarative, large-scale ML but struggle to satisfy all properties of declarative ML.", "startOffset": 33, "endOffset": 41}, {"referenceID": 23, "context": "More recent systems like Cumulon [24, 25], Mahout Samsara [15], DMac [40], and TensorFlow [1] similarly aim for declarative, large-scale ML but struggle to satisfy all properties of declarative ML.", "startOffset": 33, "endOffset": 41}, {"referenceID": 36, "context": "More recent systems like Cumulon [24, 25], Mahout Samsara [15], DMac [40], and TensorFlow [1] similarly aim for declarative, large-scale ML but struggle to satisfy all properties of declarative ML.", "startOffset": 69, "endOffset": 73}, {"referenceID": 0, "context": "More recent systems like Cumulon [24, 25], Mahout Samsara [15], DMac [40], and TensorFlow [1] similarly aim for declarative, large-scale ML but struggle to satisfy all properties of declarative ML.", "startOffset": 90, "endOffset": 93}, {"referenceID": 22, "context": "Cumulon [24, 25] can be seen as a declarative system.", "startOffset": 8, "endOffset": 16}, {"referenceID": 23, "context": "Cumulon [24, 25] can be seen as a declarative system.", "startOffset": 8, "endOffset": 16}, {"referenceID": 35, "context": "Mahout Samsara [15], Distributed R [39], and DMac [40] are not declarative because they expose physical data structures and distributed operations to the user (P1, P5).", "startOffset": 35, "endOffset": 39}, {"referenceID": 36, "context": "Mahout Samsara [15], Distributed R [39], and DMac [40] are not declarative because they expose physical data structures and distributed operations to the user (P1, P5).", "startOffset": 50, "endOffset": 54}, {"referenceID": 0, "context": "TensorFlow [1] is a compelling system but focuses more on extensibility than declarative specification.", "startOffset": 11, "endOffset": 14}, {"referenceID": 8, "context": "A prime example of array databases is SciDB [9, 34] which indeed satisfies all basic properties for declarative ML.", "startOffset": 44, "endOffset": 51}, {"referenceID": 31, "context": "A prime example of array databases is SciDB [9, 34] which indeed satisfies all basic properties for declarative ML.", "startOffset": 44, "endOffset": 51}, {"referenceID": 10, "context": "Furthermore, also SimSQL [11]\u2014as an example of Bayesian ML or more broadly stochastic analysis\u2014 satisfies all basic properties of declarative ML.", "startOffset": 25, "endOffset": 29}, {"referenceID": 7, "context": "Examples are ScalOps [8] (which compiles Scala UDF workflows to datalog programs), Tupleware [13] (which compiles workflows of UDFs in various frontend languages to custom distributed programs of native code via LLVM), and Emma [4] (which compiles Scala UDF workflows to Spark and Flink programs).", "startOffset": 21, "endOffset": 24}, {"referenceID": 12, "context": "Examples are ScalOps [8] (which compiles Scala UDF workflows to datalog programs), Tupleware [13] (which compiles workflows of UDFs in various frontend languages to custom distributed programs of native code via LLVM), and Emma [4] (which compiles Scala UDF workflows to Spark and Flink programs).", "startOffset": 93, "endOffset": 97}, {"referenceID": 3, "context": "Examples are ScalOps [8] (which compiles Scala UDF workflows to datalog programs), Tupleware [13] (which compiles workflows of UDFs in various frontend languages to custom distributed programs of native code via LLVM), and Emma [4] (which compiles Scala UDF workflows to Spark and Flink programs).", "startOffset": 228, "endOffset": 231}, {"referenceID": 16, "context": "Bismarck [18] X X X N/A min runtime s.", "startOffset": 9, "endOffset": 13}, {"referenceID": 0, "context": "accuracy constraints TensorFlow [1] X X X X X X X X X 2 min runtime s.", "startOffset": 32, "endOffset": 35}, {"referenceID": 25, "context": "accuracy constraints MLbase [27, 33] X X X X X X X X X 2 max accuracy s.", "startOffset": 28, "endOffset": 36}, {"referenceID": 30, "context": "accuracy constraints MLbase [27, 33] X X X X X X X X X 2 max accuracy s.", "startOffset": 28, "endOffset": 36}, {"referenceID": 38, "context": "Columbus [42] X X X X X X X X 2 min runtime s.", "startOffset": 9, "endOffset": 13}, {"referenceID": 29, "context": "accuracy constraints DeepDive [32] X X X X X X X X 2 max accuracy s.", "startOffset": 30, "endOffset": 34}, {"referenceID": 16, "context": "General-Purpose Optimization: Bismarck [18] provides in-database general-purpose optimization via incremental gradient decent, where users provide UDFs for initialization, transition, and termination.", "startOffset": 39, "endOffset": 43}, {"referenceID": 28, "context": "The latter also does not satisfy P7/P8 due to Hogwild!-style [31] model updates.", "startOffset": 61, "endOffset": 65}, {"referenceID": 0, "context": "TensorFlow [1] also provides primitives for general-purpose optimization via different optimization algorithms.", "startOffset": 11, "endOffset": 14}, {"referenceID": 25, "context": "Model and Feature Selection: MLbase [27] with its TUPAQ [33] component for automatic model search allows users to specify candidate model configurations, a quality measure, and runtime constraints (e.", "startOffset": 36, "endOffset": 40}, {"referenceID": 30, "context": "Model and Feature Selection: MLbase [27] with its TUPAQ [33] component for automatic model search allows users to specify candidate model configurations, a quality measure, and runtime constraints (e.", "startOffset": 56, "endOffset": 60}, {"referenceID": 38, "context": "Columbus [42] allows users to specify feature engineering workflows in R including data preparations and model building with existing R packages.", "startOffset": 9, "endOffset": 13}, {"referenceID": 29, "context": "DeepDive [32] enables knowledge base construction via statistical inference.", "startOffset": 9, "endOffset": 13}, {"referenceID": 14, "context": ", Fa [16], a skip-list approach [20], or F2DB [19]), which also consider the trade-off between accuracy and runtime (model selection) but are not subject to this classification for general-purpose declarative ML.", "startOffset": 5, "endOffset": 9}, {"referenceID": 18, "context": ", Fa [16], a skip-list approach [20], or F2DB [19]), which also consider the trade-off between accuracy and runtime (model selection) but are not subject to this classification for general-purpose declarative ML.", "startOffset": 32, "endOffset": 36}, {"referenceID": 17, "context": ", Fa [16], a skip-list approach [20], or F2DB [19]), which also consider the trade-off between accuracy and runtime (model selection) but are not subject to this classification for general-purpose declarative ML.", "startOffset": 46, "endOffset": 50}, {"referenceID": 4, "context": "Existing benchmarks for large-scale computation like BigBench [5, 21], SparkBench [2, 29], or HiBench [26] do cover machine learning but often simply refer to reference implementations of large-scale ML libraries.", "startOffset": 62, "endOffset": 69}, {"referenceID": 19, "context": "Existing benchmarks for large-scale computation like BigBench [5, 21], SparkBench [2, 29], or HiBench [26] do cover machine learning but often simply refer to reference implementations of large-scale ML libraries.", "startOffset": 62, "endOffset": 69}, {"referenceID": 1, "context": "Existing benchmarks for large-scale computation like BigBench [5, 21], SparkBench [2, 29], or HiBench [26] do cover machine learning but often simply refer to reference implementations of large-scale ML libraries.", "startOffset": 82, "endOffset": 89}, {"referenceID": 27, "context": "Existing benchmarks for large-scale computation like BigBench [5, 21], SparkBench [2, 29], or HiBench [26] do cover machine learning but often simply refer to reference implementations of large-scale ML libraries.", "startOffset": 82, "endOffset": 89}, {"referenceID": 24, "context": "Existing benchmarks for large-scale computation like BigBench [5, 21], SparkBench [2, 29], or HiBench [26] do cover machine learning but often simply refer to reference implementations of large-scale ML libraries.", "startOffset": 102, "endOffset": 106}, {"referenceID": 8, "context": "Interestingly, exactly that already happened for SQL-centric systems as both, the SciDB [9, 34] and SimSQL [11] projects published benchmarks covering the main characteristics of array databases [36] and Bayesian ML [10].", "startOffset": 88, "endOffset": 95}, {"referenceID": 31, "context": "Interestingly, exactly that already happened for SQL-centric systems as both, the SciDB [9, 34] and SimSQL [11] projects published benchmarks covering the main characteristics of array databases [36] and Bayesian ML [10].", "startOffset": 88, "endOffset": 95}, {"referenceID": 10, "context": "Interestingly, exactly that already happened for SQL-centric systems as both, the SciDB [9, 34] and SimSQL [11] projects published benchmarks covering the main characteristics of array databases [36] and Bayesian ML [10].", "startOffset": 107, "endOffset": 111}, {"referenceID": 33, "context": "Interestingly, exactly that already happened for SQL-centric systems as both, the SciDB [9, 34] and SimSQL [11] projects published benchmarks covering the main characteristics of array databases [36] and Bayesian ML [10].", "startOffset": 195, "endOffset": 199}, {"referenceID": 9, "context": "Interestingly, exactly that already happened for SQL-centric systems as both, the SciDB [9, 34] and SimSQL [11] projects published benchmarks covering the main characteristics of array databases [36] and Bayesian ML [10].", "startOffset": 216, "endOffset": 220}], "year": 2016, "abstractText": "Declarative machine learning (ML) aims at the high-level specification of ML tasks or algorithms, and automatic generation of optimized execution plans from these specifications. The fundamental goal is to simplify the usage and/or development of ML algorithms, which is especially important in the context of large-scale computations. However, ML systems at different abstraction levels have emerged over time and accordingly there has been a controversy about the meaning of this general definition of declarative ML. Specification alternatives range from ML algorithms expressed in domain-specific languages (DSLs) with optimization for performance, to ML task (learning problem) specifications with optimization for performance and accuracy. We argue that these different types of declarative ML complement each other as they address different users (data scientists and end users). This paper makes an attempt to create a taxonomy for declarative ML, including a definition of essential basic properties and types of declarative ML. Along the way, we provide insights into implications of these properties. We also use this taxonomy to classify existing systems. Finally, we draw conclusions on defining appropriate benchmarks and specification languages for declarative ML.", "creator": ""}}}