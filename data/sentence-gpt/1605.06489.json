{"id": "1605.06489", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "20-May-2016", "title": "Deep Roots: Improving CNN Efficiency with Hierarchical Filter Groups", "abstract": "We propose a new method for training computationally efficient and compact convolutional neural networks (CNNs) using a novel sparse connection structure that resembles a tree root. Our sparse connection structure facilitates a significant reduction in computational cost and number of parameters of state-of-the-art deep CNNs without compromising accuracy. We validate our approach by using it to train more efficient variants of state-of-the-art CNN architectures, evaluated on the CIFAR10 and ILSVRC datasets. Our results show similar or higher accuracy than the baseline architectures with much less compute, as measured by CPU and GPU timings. For example, for ResNet 50, our model has 40% fewer parameters, 45% fewer floating point operations, and is 31% (12%) faster on a CPU (GPU). For the deeper ResNet 200 our model has 25% fewer floating point operations and 44% fewer parameters, while maintaining state-of-the-art accuracy. For GoogLeNet, our model has 7% fewer parameters and is 21% (16%) faster on a CPU (GPU).\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n", "histories": [["v1", "Fri, 20 May 2016 19:51:37 GMT  (1968kb,D)", "http://arxiv.org/abs/1605.06489v1", null], ["v2", "Tue, 29 Nov 2016 17:29:01 GMT  (1661kb,D)", "http://arxiv.org/abs/1605.06489v2", "Short workshop paper for EMDNN 2016"], ["v3", "Wed, 30 Nov 2016 15:32:03 GMT  (4435kb,D)", "http://arxiv.org/abs/1605.06489v3", "Updated full version of paper, in full letter paper two-column paper. Includes many textual changes, updated CIFAR10 results, and new analysis of inter/intra-layer correlation"]], "reviews": [], "SUBJECTS": "cs.NE cs.CV cs.LG", "authors": ["yani ioannou", "duncan robertson", "roberto cipolla", "antonio criminisi"], "accepted": false, "id": "1605.06489"}, "pdf": {"name": "1605.06489.pdf", "metadata": {"source": "CRF", "title": "Deep Roots: Improving CNN Efficiency with Hierarchical Filter Groups", "authors": ["Yani Ioannou", "Duncan Robertson", "Roberto Cipolla", "Antonio Criminisi"], "emails": [], "sections": [{"heading": "1 Introduction", "text": "This paper describes a new method for creating computationally efficient and compact convolutional neural networks (CNNs) using a novel sparse connection structure that resembles a tree root. This allows a significant reduction in computational cost and number of parameters of state-of-the-art deep CNNs without compromising accuracy.\nIt is well understood that the structure of a neural network is critical to its ability to learn from labelled training data and to generalize. Whilst it has been shown that an infinitely wide hidden layer is a universal approximator [14], in practice wide shallow networks do not learn as well as thinner deeper ones \u2013 as shown by recent research [24, 35, 33, 10]. This does not appear to be a limitation of finite capacity, since deep networks have been shown to be representable by shallow networks [1]. Rather it seems to be a consequence of limitations in our methods of learning the weights of deep networks.\nIt has been shown that a large proportion of the learned weights in deep networks are redundant [4] (a property that many have attempted to exploit to make neural networks smaller and more computationally efficient [35, 5]). It is unsurprising then that regularization is a critical part of training such\nar X\niv :1\n60 5.\n06 48\n9v 1\n[ cs\n.N E\nnetworks using large training datasets [24]. Without regularization (e.g. weight decay, dropout [13]) deep networks are susceptible to severe over-fitting (see \u00a72). Aside from different training methods, a carefully designed network connection structure can have a regularizing effect in and of itself. Convolutional Neural Networks (CNNs) [6, 26] embody this idea, exploiting prior knowledge of the locality of natural image structure to design neural architectures with more limited, but salient, connectivity than a fully-connected neural network, that is in turn easier to learn. More recently, learning CNNs with low rank filters was found to have a regularizing effect, improving generalization compared to a CNN with only full rank filters [16].\nWith few exceptions, state-of-the-art CNNs for image recognition are largely monolithic \u2013 convolutional neural networks with each filter operating on the feature maps of all filters on a previous layer, presenting no significant per-layer heterogeneity. Interestingly, this is in stark contrast to what we understand of biological neural networks, where we see \u201chighly evolved arrangements of smaller, specialized networks which are interconnected in very specific ways\u201d [30].\nIn this paper we show that simple alterations to the architecture of stateof-the-art CNNs for image recognition can drastically reduce computational cost and model size while maintaining (or even increasing) accuracy, through structure-induced regularization, by reducing the connectivity in monolithic networks to reflect more closely the sparse, localized filter co-dependencies within."}, {"heading": "2 Related Work", "text": "Reducing Co-dependence in Deep Networks. Hinton et al. [13] introduced dropout for regularization of deep networks. When training a network layer with dropout, a random subset of neurons is excluded from both the forward and backward pass for each mini-batch. Effectively a different (random) network topology is trained at each iteration. As the authors observe, this approach has some similarities with that of using model ensembles, another effective way to increase generalization. However, one limitation of dropout is that it increases the number of training iterations required for convergence, typically by a factor of two. Recently, Szegedy et al. [35] have suggested that dropout provides little incremental accuracy improvement compared to simply training using batch normalization.\nCogswell et al. [3] observe a correlation between the cross-covariance of hidden unit activations and overfitting. To explicitly reduce the cross-covariance of hidden activations, they train networks with a loss function, based on the covariance matrix of the activations in a hidden layer. Cogswell et al. [3] use this loss on the fully connected layers at the end of deep networks such as AlexNet and demonstrate an increase in generalization accuracy comparable with that obtained by using dropout.\nReducing Model Size and Computation. Most previous work on reducing computation in CNNs has focused on the spatial extents of the convolutional\nfilters in the form of low-rank spatial filters [28, 18, 34, 25, 16], using frequencybased convolution [29, 31]. More general methods for speeding up CNNs have used reduced precision numerics [9] or pre-trained model compression [2, 21]. Here we explore methods that reduce the computational impact of the large number of filter channels within state-of-the art networks. In particular, methods to decrease the number of filters and thus output feature map channels while maintaining accuracy (i.e. the depth of filters, c in Fig. 1a rather than the spatial extents (h,w) ).\nAlexNet Filter Groups. Amongst the seminal contributions made by Krizhevsky et al. [24] is the use of \u2018filter groups\u2019 in the convolutional layers of a CNN (see Fig. 1). While the use of filter groups was necessitated by the practical consideration of sub-dividing the work of training a large network across multiple GPUs, the side effects are somewhat surprising. Specifically, the authors observe that independent filter groups learn a separation of responsibility (colour features vs. texture features) that is consistent over different random initializations. Also surprising, and not explicitly stated in [24], is the fact that the AlexNet network has approximately 57% fewer connection weights than the corresponding network without filter groups (see Fig. 2). This is due to the reduction in the input channel dimension of the grouped convolution filters. Surprisingly, despite\nthe large difference in the number of parameters between the models, both architectures achieve comparable error on ILSVRC \u2013 in fact the smaller grouped network gets \u2248 1% lower top-5 validation error. This paper builds upon and extends these findings to state-of-the-art networks.\nLow-dimensional Embeddings. Lin et al. [27] proposed a method to reduce the dimensionality of convolutional feature maps. By using relatively cheap \u20181\u00d71\u2019 convolutional layers (i.e. layers comprising d filters of size 1\u00d71\u00d7c, where d < c), they learn to map feature maps into lower-dimensional spaces, i.e. to new feature maps with fewer channels. Subsequent spatial filters operating on this lower dimensional input space require significantly less computation. This method is used in most state of the art networks for image classification to reduce computation [35, 10]. Our method is complementary to low-dimensional embeddings.\nConditional Networks. Ioannou et al. [15] describe conditional networks, i.e. deep networks trained from random initialization with a tree-like structure of convolutional filter groups. This tree-like structure reduces computation, but can also reduce accuracy. Our results show that with a different structure of sparsity, not only can we reduce computation further, but we can maintain if not increase accuracy.\nGoogLeNet. In contrast to much other work, Szegedy et al. [35] propose a CNN architecture that is highly optimized for computational efficiency. GoogLeNet uses, as a basic building block, a mixture of low-dimensional embeddings [27] and heterogeneously sized spatial filters \u2013 collectively an \u2018inception\u2019 module.\nThere are two distinct forms of convolutional layers in the inception module, low-dimensional embeddings (1\u00d71) and spatial (3\u00d73, 5\u00d75). GoogLeNet keeps large, expensive spatial convolutions (i.e. 5\u00d75) to a minimum by using few of these filters, using more 3\u00d73 convolutions, and even more 1\u00d71 filters again. The motivation is that most of the convolutional filters respond to localized patterns in a small receptive field, with few requiring a larger receptive field. The number\nof filters in each successive inception unit increase slowly with decreasing feature map size, in order to maintain computational performance.\nGoogLeNet is by far the most efficient state-of-the-art network for ILSVRC, achieving near state-of-the-art accuracy with the lowest computation/model size. However, we will show that even such an efficient and optimized network architecture benefits from our method.\nLow-Rank Approximations. Various authors have suggested approximating learned convolutional filters using tensor decomposition [18, 25, 21]. For example, Jaderberg et al. [18] propose approximating the convolutional filters in a pre-trained network with representations that are low-rank both in the spatial and the channel domains. This approach significantly decreases computational complexity, albeit at the expense of a small amount of accuracy. By contrast, here we will show that by training our network from scratch with a sparse structure of filter connectivity, we can similarly decrease computation without having to train a larger network beforehand, or to perform an expensive post-processing step. Because we are not approximating existing weights, but allowing standard stochastic gradient descent (SGD) to optimize our network, we can even increase accuracy through increased regularization."}, {"heading": "3 Hierarchical Filter Groups", "text": "In this section we present the main contribution of our work \u2013 an exploration of the use of hierarchical filter groups to decrease computational complexity and model size compared to state-of-the-art deep networks for image recognition."}, {"heading": "3.1 Reduced Co-adaption and Filter Groups", "text": "It has been shown that reducing the co-adaption of filters is beneficial to generalization in deep networks [13, 8, 3]. Co-adaption occurs when hidden layer filters (or neurons) rely on one-another to fit training data, becoming highly correlated. However, instead of using a modified loss, regularization penalty, or randomized network connectivity during training to prevent co-adaption of features, we take a much more direct approach. We use hierarchical filter groups (see \u00a72) to allow the network itself to learn independent filters. By restricting the connectivity between filters on subsequent layers the network is forced to learn filters of limited interdependence.\nThis reduced connectivity also reduces computational complexity and model size. Unlike methods for increasing the efficiency of deep networks by approximating pre-trained existing networks (see \u00a72), our models are trained from random initialization using stochastic gradient descent. This means that our method can also speed up training and, since we are not merely approximating an existing models\u2019 weights, the accuracy of an existing model is not an upper bound on accuracy of the modified model."}, {"heading": "3.2 Network Topology", "text": "An exhaustive exploration of the possible topologies for filter grouping within state-of-the-art deep networks is prohibitively expensive. Instead we based our exploration on a restricted, but interesting, subset of hierarchical topologies, each of which is illustrated in Fig. 3. Each of these topologies makes different fundamental assumptions on the co-dependence of filters within each layer and overall network architecture.\nColumns. Columnar topologies assume that filters are consistently co-dependent with depth, and that this co-dependence is relatively small throughout the whole network. AlexNet uses a 2-column architecture on most layers.\nTrees. Tree-like topologies, as used by Ioannou et al. [16], in conditional networks, assume that either filters become less co-dependent with depth, or the number of filters grows with depth enough that there are many more filters in deep layers than required. Notably, Ioannou et al. [16] evaluated their architecture on VGG networks, which are very large, and over-parametrized even compared to state-of-the-art networks such as residual networks.\nRoots. Root-like topologies (i.e. inverted trees) assume that filters become more co-dependent with depth. This aligns with the intuition of deep networks for image recognition subsuming the deformable parts model (DPM). If we assume that filter responses identify parts (or more elemental features), then there should be more filter co-dependence with depth, as more complex relationships emerge [7].\nIn general, state-of-the-art CNN architectures have few filters in early convolutional layers, increasing throughout the network, and so it might seem that trees would have the best computational savings. Even so, the first few layers of a CNN are always the most computationally expensive, simply because the input feature map\u2019s spatial size is largest before any sub-sampling, in the layers close to the input image. As such, root topologies in general have the largest computational savings."}, {"heading": "4 Results", "text": "Here we present results of training from scratch re-structured (as described in \u00a73) state-of-the-art network architectures for image classification on the CIFAR10 [23] and ILSVRC [32] datasets."}, {"heading": "4.1 Improving Network in Network on CIFAR-10", "text": "Network in Network (NiN) [27] is a near state-of-the-art network for CIFAR10 [23]. It is composed of 3 spatial (5\u00d75, 3\u00d73) convolutional layers with a large number of filters (192), interspersed with low-dimensional embedding (1\u00d71) layers. We replicated the standard NiN network architecture as described by Lin et al. [27], but with state-of-the-art training methods. We trained using random 32\u00d732 cropped and mirrored images from 4-pixel zero-padded mean-subtracted images, as in [8, 10]. We also used the initialization of He et al. [11] and batch normalization [17]. With this configuration, ZCA whitening was not required to reproduce validation accuracies obtained in [27]. We also did not use dropout, having found it to have little effect, presumably due to our use of batch normalization.\nWe restructured the network with hierarchical groups, while preserving the original number of filters per layer. To determine which of the network topologies\nin Fig. 3 gave the best computational savings with the minimal loss in accuracy, we implemented each for various numbers of initial filter groups.\nWe only group filters within the the spatial (i.e. non 1\u00d71) layers. Using filter groups within the low-dimensional embedding layers (i.e. 1\u00d71 convolutions) leads to learning even lower dimensional embeddings of the input space, which is likely to compromise their representational ability. Indeed, in such configurations we observed a large decrease in accuracy. We also do not group filters in the first convolutional layer \u2013 since it operates on the three-channel image space, it is of limited computational impact compared to other layers, and we believe learning color filters to be critical to learning.\nTable 1 lists the best results for each of the topologies. Only the root-like topology maintains accuracy while decreasing computation and model size significantly \u2013 the best, root-4, has only 55% of the floating point operations (FLOPS), 47% of the model parameters of the original network, and approximately 22% faster CPU and GPU timings."}, {"heading": "4.2 Improving Residual Networks on ILSVRC", "text": "Residual networks (ResNets) [10] are the state-of-the art network for ILSVRC. ResNets are more computationally efficient than the VGG architecture [33] they are based on, due to the use of so called \u2018bottleneck\u2019 layers (low-dimensional embeddings [27]), but are also more accurate and quicker to converge, due to the use of identity mappings (shortcuts). These shortcut connections have been shown to help the training of deep networks.\nResNet 50 We chose to apply root filter hierarchies within the \u2018ResNet 50\u2019 model [10], the largest residual network model to fit onto 8 GPUs. ResNet 50 has 50 convolutional layers, of which one-third are spatial convolutions (non1\u00d71). We did not use any training augmentation aside from random cropping\nand mirroring. To aid training, we used the initialization of [11] but modified for compound layers [16], and batch normalization [17].\nWhile preserving the original number of filters per layer, networks with various numbers of filter groups were trained in a root topology, as described in Table 3. We only grouped filters within each of the \u2018spatial\u2019 convolutions (3\u00d73), denoted (s) in Table 2. For all layers between sub-sampling layers, we split the filters into the same number of groups, for example a ResNet 50 model with a root-8 topology has eight groups of filters on layers res2{a,b,c}, four on layers res3{a,..., d}, two on layers res4{a,...,f} and a single group on all other convolutional layers.\nAs shown in Fig. 5, our method yields a significant reduction in computational complexity \u2013 as measured in FLOPS (multiply-adds), CPU and GPU timings \u2013 and model size, as measured in the number of floating point parameters. All timings were taken on the network\u2019s forward pass, using Caffe compiled with the CuDNN and MKL BLAS libraries, on a machine with an Nvidia Titan Z GPU and 2 10-core Intel Xeon E5-2680 v2 CPUs. We do not factor the batch normalization layers into our FLOPS or parameter counts since, as argued by Szegedy et al. [35], at test time the batch normalization layers/parameters may effectively be removed.\nFor many of the configurations the accuracy is 0.2% higher than that of the baseline network. Of these, the best result (root-16) exceeds the baseline accuracy by 0.2% while reducing the model size by 27% and floating-point operations (multiply-add) by 37%. CPU timings were 23% faster, while GPU timings were\n13% faster1. With a drop in accuracy of only 0.1% however, the root-64 model reduces the model size by 40%, and reduces the floating point operations by 45%. CPU timings were 31% faster, while GPU timings were 12% faster.\nResNet 200 To show that the method applies to deeper architectures, we also applied our method to ResNet 200, the deepest network for ILSVRC 2012. For this network we used code implementing full training augmentation to achieve state-of-the-art results2, re-implementing more recent models [12]. Table 4 shows the results of these experiments, top-1 and top-5 error are for center cropped images. The models trained with hierarchical filter groups have comparable error to the baseline network, with fewer parameters and less computation. The root32 model has 25% fewer FLOPS and 44% fewer parameters than ResNet 200."}, {"heading": "4.3 Improving GoogLeNet on ILSVRC", "text": "We replicated the network as described by Szegedy et al. [35], with the exception of not using any training augmentation aside from random crops and mirroring (as supported by Caffe [20]). To train we used the initialization of [11] modified for compound layers [16] and batch normalization without the scale and bias [17]. At test time we only evaluate the center crop image.\nWhile preserving the original number of filters per layer, we trained networks with various degrees of filter grouping, described in Table 6. While the inception\n1 See \u00a75 for an explanation of the GPU timing disparity. 2 https://github.com/facebook/fb.resnet.torch\narchitecture is relatively complex, for simplicity, we always use the same number of groups within each of the groups of different filter sizes, despite them having different cardinality. For some of the networks, we only grouped filters within each of the \u2018spatial\u2019 convolutions (3\u00d73, 5\u00d75), denoted (s) in Table 5. For other networks, we grouped all filters, including the 1\u00d71.\nAs shown in Table 5, and plotted in Fig. 6, our method shows significant reduction in computational complexity \u2013 as measured in FLOPS (multiply-adds), CPU and GPU timings \u2013 and model size, as measured in the number of floating point parameters. For many of the configurations the top-5 accuracy remains within 0.5% of the baseline model. The highest accuracy result, is 0.1% off the top-5 accuracy of the baseline model, but has a 0.1% higher top-1 accuracy \u2013 within the error bounds resulting from training with different random initializations. While maintaining the same accuracy, this network has 9% faster CPU and GPU timings. However, a model with only 0.3% lower top-5 accuracy than the baseline has much higher gains in computational efficiency \u2013 44% fewer floating point operations (multiply-add), 7% fewer model parameters, 21% faster CPU and 16% faster GPU timings.\nWhile these results may seem modest compared to the results for ResNet 50, GoogLeNet is by far the smallest and fastest near state-of-the-art model ILSVRC model. We believe that more experimentation in using different cardinalities of filter grouping in the heterogeneously-sized filter groups within each inception module will improve results further."}, {"heading": "5 GPU Implementation", "text": "Our experiments show that our method can achieve a significant reduction in CPU and GPU runtimes for state-of-the-art CNNs without compromising accuracy. However, the reductions in GPU runtime were smaller than might have been expected based on theoretical predictions of computational complexity (FLOPs). We believe this is largely a consequence of the optimization of Caffe for existing network architectures (particularly AlexNet and GoogLeNet) that do not use a high degree of filter grouping.\nCaffe presently parallelizes over filter groups by using multiple CUDA streams to run multiple CuBLAS matrix multiplications simultaneously. However, with a large degree of filter grouping, and hence more, smaller matrix multiplications, the overhead associated with calling CuBLAS from the host can take approximately as long as the matrix computation itself. To avoid this overhead, CuBLAS provides batched methods (e.g. cublasXgemmBatched), where many small matrix multiplications can be batched together in one call. Jhurani and Mullowney [19] explore in depth the problem of using GPUs to accelerate the multiplication of very small matrices (smaller than 16\u00d716), and show it is possible to achieve high throughput with large batches, by implementing a more efficient interface than that used in the CuBLAS batched calls. We have modified Caffe to use CuBLAS batched calls, and achieved significant speedups for our root-like network architectures compared to vanilla Caffe without CuDNN, e.g. a 25% speed up on our root-16 modified version of the GoogleNet architecture. However, our optimized implementation still is not as fast as Caffe with CuDNN (which was used to generate the results in this paper), presumably because of other unrelated optimizations in the (proprietary) CuDNN library. Therefore we suggest that direct integration of CuBLAS-style batching into CuDNN could improve the performance of filter groups significantly."}, {"heading": "6 Future Work", "text": "In this paper we focused on using homogeneous filter groups (with a uniform division of filters in each group), however this may not be optimal. Heterogeneous filter groups may reflect better the filter co-dependencies found in deep networks."}, {"heading": "7 Conclusion", "text": "We explored the effect of using complex hierarchical arrangements of filter groups in CNNs and show that imposing a structured decrease in the degree of filter grouping with depth \u2013 a \u2018root\u2019 (inverse tree) topology \u2013 can allow us to obtain more efficient variants of state-of-the-art networks without compromising accuracy. Our method appears to be complementary to existing methods, such as low-dimensional embeddings, and can be used more efficiently to train deep networks than methods that only approximate a pre-trained model\u2019s weights.\nWe validated our method by using it to create more efficient variants of stateof-the-art Network-in-network, Googlenet, and ResNet architectures, which were evaluated on the CIFAR10 and ILSVRC datasets. Our results show comparable accuracy with the baseline architecture with fewer parameters and much less compute (as measured by CPU and GPU timings). For Network-in-Network on CIFAR10, our model has 47% of the parameters of the original network, and approximately 22% faster CPU and GPU timings. For ResNet 50, our model has 27% fewer parameters, and was 24% (11%) faster on a CPU (GPU). Even for the most efficient of the near state-of-the-art ILSVRC network, GoogLeNet, our model uses 7% fewer parameters and is 21% (16%) faster on a CPU (GPU)."}], "references": [{"title": "Do Deep Nets Really Need to be Deep", "author": ["L.J. Ba", "R. Caruana"], "venue": null, "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2013}, {"title": "Compressing Neural Networks with the Hashing Trick", "author": ["W. Chen", "J.T. Wilson", "S. Tyree", "K.Q. Weinberger", "Y. Chen"], "venue": "JMLR Proceedings,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2015}, {"title": "Reducing Overfitting in Deep Networks by Decorrelating Representations", "author": ["M. Cogswell", "F. Ahmed", "R.B. Girshick", "L. Zitnick", "D. Batra"], "venue": "In: International Conference on Learning Representations", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2016}, {"title": "Predicting Parameters in Deep Learning", "author": ["M. Denil", "B. Shakibi", "L. Dinh", "M.A. Ranzato", "N. deFreitas"], "venue": null, "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2013}, {"title": "R.Fergus: Exploiting Linear Structure Within Convolutional Networks for Efficient Evaluation", "author": ["E. Denton", "W. Zaremba", "J. Bruna", "Y. LeCun"], "venue": null, "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2014}, {"title": "Neocognitron: A self-organizing neural network model for a mechanish of pattern recognition unaffected by shifts in position", "author": ["K. Fukushima"], "venue": "Biological Cybernetics", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 1980}, {"title": "Deformable part models are convolutional neural networks", "author": ["R. Girshick", "F. Iandola", "T. Darrell", "J. Malik"], "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. pp", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2015}, {"title": "Maxout Networks", "author": ["I.J. Goodfellow", "D. Warde-Farley", "M. Mirza", "A.C. Courville", "Y. Bengio"], "venue": "JMLR Proceedings,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2013}, {"title": "Deep Learning with Limited Numerical Precision", "author": ["S. Gupta", "A. Agrawal", "K. Gopalakrishnan", "P. Narayanan"], "venue": "Proceedings of the 32nd International Conference on Machine Learning", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2015}, {"title": "Deep Residual Learning for Image Recognition", "author": ["K. He", "X. Zhang", "S. Ren", "J. Sun"], "venue": "arXiv preprint arXiv:1512.03385", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2015}, {"title": "Delving Deep into Rectifiers: Surpassing Human-Level Performance on ImageNet Classification", "author": ["K. He", "X. Zhang", "S. Ren", "J. Sun"], "venue": null, "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2015}, {"title": "Identity mappings in deep residual networks", "author": ["K. He", "X. Zhang", "S. Ren", "J. Sun"], "venue": null, "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2016}, {"title": "Improving neural networks by preventing co-adaptation of feature detectors", "author": ["G.E. Hinton", "N. Srivastava", "A. Krizhevsky", "I. Sutskever", "R.R. Salakhutdinov"], "venue": null, "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2012}, {"title": "Multilayer feedforward networks are universal approximators", "author": ["K. Hornik", "M. Stinchcombe", "H. White"], "venue": "Neural Networks", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 1989}, {"title": "Decision Forests, Convolutional Networks and the Models in- Between", "author": ["Y. Ioannou", "D. Robertson", "D. Zikic", "P. Kontschieder", "J. Shotton", "M. Brown", "A. Criminisi"], "venue": "Tech. Rep. MSR-TR-2015-58, Microsoft Research (Apr", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2015}, {"title": "Training CNNs with Low-Rank Filters for Efficient Image Classification", "author": ["Y. Ioannou", "D.P. Robertson", "J. Shotton", "R. Cipolla", "A. Criminisi"], "venue": "In: International Conference on Learning Representations", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2016}, {"title": "Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift", "author": ["S. Ioffe", "C. Szegedy"], "venue": "Proceedings of the 32 nd International Conference on Machine Learning,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2015}, {"title": "Speeding up Convolutional Neural Networks with Low Rank Expansions", "author": ["M. Jaderberg", "A. Vedaldi", "A. Zisserman"], "venue": "British Machine Vision Conference", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2014}, {"title": "A GEMM interface and implementation on NVIDIA GPUs for multiple small matrices", "author": ["C. Jhurani", "P. Mullowney"], "venue": "Journal of Parallel and Distributed Computing", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2015}, {"title": "Caffe: Convolutional Architecture for Fast Feature Embedding", "author": ["Y. Jia", "E. Shelhamer", "J. Donahue", "S. Karayev", "J. Long", "R. Girshick", "S. Guadarrama", "T. Darrell"], "venue": null, "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2014}, {"title": "Compression of Deep Convolutional Neural Networks for Fast and Low Power Mobile Applications", "author": ["Y. Kim", "E. Park", "S. Yoo", "T. Choi", "L. Yang", "D. Shin"], "venue": "In: International Conference on Learning Representations", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2016}, {"title": "One weird trick for parallelizing convolutional neural networks", "author": ["A. Krizhevsky"], "venue": "arXiv preprint arXiv:1404.5997", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2014}, {"title": "Learning multiple layers of features from tiny images", "author": ["A. Krizhevsky", "G. Hinton"], "venue": "Tech. rep., University of Toronto (Apr", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2009}, {"title": "ImageNet Classification with Deep Convolutional Neural Networks", "author": ["A. Krizhevsky", "I. Sutskever", "G.E. Hinton"], "venue": "K.Q. (eds.) NIPS. pp", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2012}, {"title": "Speeding-up Convolutional Neural Networks Using Fine-tuned CP-Decomposition", "author": ["V. Lebedev", "Y. Ganin", "M. Rakhuba", "I. Oseledets", "V. Lempitsky"], "venue": "In: International Conference on Learning Representations", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2015}, {"title": "Gradient-based learning applied to document recognition", "author": ["Y. Lecun", "L. Bottou", "Y. Bengio", "P. Haffner"], "venue": "Proceedings of the IEEE 86(11),", "citeRegEx": "26", "shortCiteRegEx": "26", "year": 1998}, {"title": "Simplifying convnets for fast learning", "author": ["F. Mamalet", "C. Garcia"], "venue": "Artificial Neural Networks and Machine Learning\u2013ICANN", "citeRegEx": "28", "shortCiteRegEx": "28", "year": 2012}, {"title": "Fast training of convolutional networks through FFTs", "author": ["M. Mathieu", "M. Henaff", "Y. LeCun"], "venue": "In: International Conference on Learning Representations", "citeRegEx": "29", "shortCiteRegEx": "29", "year": 2014}, {"title": "Perceptrons. MIT press", "author": ["M. Minsky", "S. Papert"], "venue": null, "citeRegEx": "30", "shortCiteRegEx": "30", "year": 1988}, {"title": "Spectral Representations for Convolutional Neural Networks", "author": ["O. Rippel", "J. Snoek", "R.P. Adams"], "venue": "Advances in Neural Information Processing Systems. pp", "citeRegEx": "31", "shortCiteRegEx": "31", "year": 2015}, {"title": "Very Deep Convolutional networks for Large-Scale Image Recognition", "author": ["K. Simonyan", "A. Zisserman"], "venue": null, "citeRegEx": "33", "shortCiteRegEx": "33", "year": 2014}, {"title": "Learning Separable Filters", "author": ["A. Sironi", "B. Tekin", "R. Rigamonti", "V. Lepetit", "P. Fua"], "venue": "IEEE Trans. Pattern Anal. Mach. Intell", "citeRegEx": "34", "shortCiteRegEx": "34", "year": 2015}, {"title": "Going Deeper with Convolutions", "author": ["C. Szegedy", "W. Liu", "Y. Jia", "P. Sermanet", "S. Reed", "D. Anguelov", "D. Erhan", "V. Vanhoucke", "A. Rabinovich"], "venue": null, "citeRegEx": "35", "shortCiteRegEx": "35", "year": 2014}], "referenceMentions": [{"referenceID": 13, "context": "Whilst it has been shown that an infinitely wide hidden layer is a universal approximator [14], in practice wide shallow networks do not learn as well as thinner deeper ones \u2013 as shown by recent research [24, 35, 33, 10].", "startOffset": 90, "endOffset": 94}, {"referenceID": 23, "context": "Whilst it has been shown that an infinitely wide hidden layer is a universal approximator [14], in practice wide shallow networks do not learn as well as thinner deeper ones \u2013 as shown by recent research [24, 35, 33, 10].", "startOffset": 204, "endOffset": 220}, {"referenceID": 32, "context": "Whilst it has been shown that an infinitely wide hidden layer is a universal approximator [14], in practice wide shallow networks do not learn as well as thinner deeper ones \u2013 as shown by recent research [24, 35, 33, 10].", "startOffset": 204, "endOffset": 220}, {"referenceID": 30, "context": "Whilst it has been shown that an infinitely wide hidden layer is a universal approximator [14], in practice wide shallow networks do not learn as well as thinner deeper ones \u2013 as shown by recent research [24, 35, 33, 10].", "startOffset": 204, "endOffset": 220}, {"referenceID": 9, "context": "Whilst it has been shown that an infinitely wide hidden layer is a universal approximator [14], in practice wide shallow networks do not learn as well as thinner deeper ones \u2013 as shown by recent research [24, 35, 33, 10].", "startOffset": 204, "endOffset": 220}, {"referenceID": 0, "context": "This does not appear to be a limitation of finite capacity, since deep networks have been shown to be representable by shallow networks [1].", "startOffset": 136, "endOffset": 139}, {"referenceID": 3, "context": "It has been shown that a large proportion of the learned weights in deep networks are redundant [4] (a property that many have attempted to exploit to make neural networks smaller and more computationally efficient [35, 5]).", "startOffset": 96, "endOffset": 99}, {"referenceID": 32, "context": "It has been shown that a large proportion of the learned weights in deep networks are redundant [4] (a property that many have attempted to exploit to make neural networks smaller and more computationally efficient [35, 5]).", "startOffset": 215, "endOffset": 222}, {"referenceID": 4, "context": "It has been shown that a large proportion of the learned weights in deep networks are redundant [4] (a property that many have attempted to exploit to make neural networks smaller and more computationally efficient [35, 5]).", "startOffset": 215, "endOffset": 222}, {"referenceID": 23, "context": "networks using large training datasets [24].", "startOffset": 39, "endOffset": 43}, {"referenceID": 12, "context": "weight decay, dropout [13]) deep networks are susceptible to severe over-fitting (see \u00a72).", "startOffset": 22, "endOffset": 26}, {"referenceID": 5, "context": "Convolutional Neural Networks (CNNs) [6, 26] embody this idea, exploiting prior knowledge of the locality of natural image structure to design neural architectures with more limited, but salient, connectivity than a fully-connected neural network, that is in turn easier to learn.", "startOffset": 37, "endOffset": 44}, {"referenceID": 25, "context": "Convolutional Neural Networks (CNNs) [6, 26] embody this idea, exploiting prior knowledge of the locality of natural image structure to design neural architectures with more limited, but salient, connectivity than a fully-connected neural network, that is in turn easier to learn.", "startOffset": 37, "endOffset": 44}, {"referenceID": 15, "context": "More recently, learning CNNs with low rank filters was found to have a regularizing effect, improving generalization compared to a CNN with only full rank filters [16].", "startOffset": 163, "endOffset": 167}, {"referenceID": 28, "context": "Interestingly, this is in stark contrast to what we understand of biological neural networks, where we see \u201chighly evolved arrangements of smaller, specialized networks which are interconnected in very specific ways\u201d [30].", "startOffset": 217, "endOffset": 221}, {"referenceID": 12, "context": "[13] introduced dropout for regularization of deep networks.", "startOffset": 0, "endOffset": 4}, {"referenceID": 32, "context": "[35] have suggested that dropout provides little incremental accuracy improvement compared to simply training using batch normalization.", "startOffset": 0, "endOffset": 4}, {"referenceID": 2, "context": "[3] observe a correlation between the cross-covariance of hidden unit activations and overfitting.", "startOffset": 0, "endOffset": 3}, {"referenceID": 2, "context": "[3] use this loss on the fully connected layers at the end of deep networks such as AlexNet and demonstrate an increase in generalization accuracy comparable with that obtained by using dropout.", "startOffset": 0, "endOffset": 3}, {"referenceID": 26, "context": "filters in the form of low-rank spatial filters [28, 18, 34, 25, 16], using frequencybased convolution [29, 31].", "startOffset": 48, "endOffset": 68}, {"referenceID": 17, "context": "filters in the form of low-rank spatial filters [28, 18, 34, 25, 16], using frequencybased convolution [29, 31].", "startOffset": 48, "endOffset": 68}, {"referenceID": 31, "context": "filters in the form of low-rank spatial filters [28, 18, 34, 25, 16], using frequencybased convolution [29, 31].", "startOffset": 48, "endOffset": 68}, {"referenceID": 24, "context": "filters in the form of low-rank spatial filters [28, 18, 34, 25, 16], using frequencybased convolution [29, 31].", "startOffset": 48, "endOffset": 68}, {"referenceID": 15, "context": "filters in the form of low-rank spatial filters [28, 18, 34, 25, 16], using frequencybased convolution [29, 31].", "startOffset": 48, "endOffset": 68}, {"referenceID": 27, "context": "filters in the form of low-rank spatial filters [28, 18, 34, 25, 16], using frequencybased convolution [29, 31].", "startOffset": 103, "endOffset": 111}, {"referenceID": 29, "context": "filters in the form of low-rank spatial filters [28, 18, 34, 25, 16], using frequencybased convolution [29, 31].", "startOffset": 103, "endOffset": 111}, {"referenceID": 8, "context": "More general methods for speeding up CNNs have used reduced precision numerics [9] or pre-trained model compression [2, 21].", "startOffset": 79, "endOffset": 82}, {"referenceID": 1, "context": "More general methods for speeding up CNNs have used reduced precision numerics [9] or pre-trained model compression [2, 21].", "startOffset": 116, "endOffset": 123}, {"referenceID": 20, "context": "More general methods for speeding up CNNs have used reduced precision numerics [9] or pre-trained model compression [2, 21].", "startOffset": 116, "endOffset": 123}, {"referenceID": 23, "context": "[24] is the use of \u2018filter groups\u2019 in the convolutional layers of a CNN (see Fig.", "startOffset": 0, "endOffset": 4}, {"referenceID": 23, "context": "Also surprising, and not explicitly stated in [24], is the fact that the AlexNet network has approximately 57% fewer connection weights than the corresponding network without filter groups (see Fig.", "startOffset": 46, "endOffset": 50}, {"referenceID": 32, "context": "This method is used in most state of the art networks for image classification to reduce computation [35, 10].", "startOffset": 101, "endOffset": 109}, {"referenceID": 9, "context": "This method is used in most state of the art networks for image classification to reduce computation [35, 10].", "startOffset": 101, "endOffset": 109}, {"referenceID": 14, "context": "[15] describe conditional networks, i.", "startOffset": 0, "endOffset": 4}, {"referenceID": 32, "context": "[35] propose a CNN architecture that is highly optimized for computational efficiency.", "startOffset": 0, "endOffset": 4}, {"referenceID": 17, "context": "Various authors have suggested approximating learned convolutional filters using tensor decomposition [18, 25, 21].", "startOffset": 102, "endOffset": 114}, {"referenceID": 24, "context": "Various authors have suggested approximating learned convolutional filters using tensor decomposition [18, 25, 21].", "startOffset": 102, "endOffset": 114}, {"referenceID": 20, "context": "Various authors have suggested approximating learned convolutional filters using tensor decomposition [18, 25, 21].", "startOffset": 102, "endOffset": 114}, {"referenceID": 17, "context": "[18] propose approximating the convolutional filters in a pre-trained network with representations that are low-rank both in the spatial and the channel domains.", "startOffset": 0, "endOffset": 4}, {"referenceID": 12, "context": "It has been shown that reducing the co-adaption of filters is beneficial to generalization in deep networks [13, 8, 3].", "startOffset": 108, "endOffset": 118}, {"referenceID": 7, "context": "It has been shown that reducing the co-adaption of filters is beneficial to generalization in deep networks [13, 8, 3].", "startOffset": 108, "endOffset": 118}, {"referenceID": 2, "context": "It has been shown that reducing the co-adaption of filters is beneficial to generalization in deep networks [13, 8, 3].", "startOffset": 108, "endOffset": 118}, {"referenceID": 15, "context": "[16], in conditional networks, assume that either filters become less co-dependent with depth, or the number of filters grows with depth enough that there are many more filters in deep layers than required.", "startOffset": 0, "endOffset": 4}, {"referenceID": 15, "context": "[16] evaluated their architecture on VGG networks, which are very large, and over-parametrized even compared to state-of-the-art networks such as residual networks.", "startOffset": 0, "endOffset": 4}, {"referenceID": 6, "context": "If we assume that filter responses identify parts (or more elemental features), then there should be more filter co-dependence with depth, as more complex relationships emerge [7].", "startOffset": 176, "endOffset": 179}, {"referenceID": 22, "context": "Here we present results of training from scratch re-structured (as described in \u00a73) state-of-the-art network architectures for image classification on the CIFAR10 [23] and ILSVRC [32] datasets.", "startOffset": 163, "endOffset": 167}, {"referenceID": 22, "context": "Network in Network (NiN) [27] is a near state-of-the-art network for CIFAR10 [23].", "startOffset": 77, "endOffset": 81}, {"referenceID": 7, "context": "We trained using random 32\u00d732 cropped and mirrored images from 4-pixel zero-padded mean-subtracted images, as in [8, 10].", "startOffset": 113, "endOffset": 120}, {"referenceID": 9, "context": "We trained using random 32\u00d732 cropped and mirrored images from 4-pixel zero-padded mean-subtracted images, as in [8, 10].", "startOffset": 113, "endOffset": 120}, {"referenceID": 10, "context": "[11] and batch normalization [17].", "startOffset": 0, "endOffset": 4}, {"referenceID": 16, "context": "[11] and batch normalization [17].", "startOffset": 29, "endOffset": 33}, {"referenceID": 9, "context": "Residual networks (ResNets) [10] are the state-of-the art network for ILSVRC.", "startOffset": 28, "endOffset": 32}, {"referenceID": 30, "context": "ResNets are more computationally efficient than the VGG architecture [33] they are based on, due to the use of so called \u2018bottleneck\u2019 layers (low-dimensional embeddings [27]), but are also more accurate and quicker to converge, due to the use of identity mappings (shortcuts).", "startOffset": 69, "endOffset": 73}, {"referenceID": 9, "context": "ResNet 50 We chose to apply root filter hierarchies within the \u2018ResNet 50\u2019 model [10], the largest residual network model to fit onto 8 GPUs.", "startOffset": 81, "endOffset": 85}, {"referenceID": 10, "context": "To aid training, we used the initialization of [11] but modified for compound layers [16], and batch normalization [17].", "startOffset": 47, "endOffset": 51}, {"referenceID": 15, "context": "To aid training, we used the initialization of [11] but modified for compound layers [16], and batch normalization [17].", "startOffset": 85, "endOffset": 89}, {"referenceID": 16, "context": "To aid training, we used the initialization of [11] but modified for compound layers [16], and batch normalization [17].", "startOffset": 115, "endOffset": 119}, {"referenceID": 32, "context": "[35], at test time the batch normalization layers/parameters may effectively be removed.", "startOffset": 0, "endOffset": 4}, {"referenceID": 11, "context": "For this network we used code implementing full training augmentation to achieve state-of-the-art results, re-implementing more recent models [12].", "startOffset": 142, "endOffset": 146}, {"referenceID": 32, "context": "[35], with the exception of not using any training augmentation aside from random crops and mirroring (as supported by Caffe [20]).", "startOffset": 0, "endOffset": 4}, {"referenceID": 19, "context": "[35], with the exception of not using any training augmentation aside from random crops and mirroring (as supported by Caffe [20]).", "startOffset": 125, "endOffset": 129}, {"referenceID": 10, "context": "To train we used the initialization of [11] modified for compound layers [16] and batch normalization without the scale and bias [17].", "startOffset": 39, "endOffset": 43}, {"referenceID": 15, "context": "To train we used the initialization of [11] modified for compound layers [16] and batch normalization without the scale and bias [17].", "startOffset": 73, "endOffset": 77}, {"referenceID": 16, "context": "To train we used the initialization of [11] modified for compound layers [16] and batch normalization without the scale and bias [17].", "startOffset": 129, "endOffset": 133}, {"referenceID": 18, "context": "Jhurani and Mullowney [19] explore in depth the problem of using GPUs to accelerate the multiplication of very small matrices (smaller than 16\u00d716), and show it is possible to achieve high throughput with large batches, by implementing a more efficient interface than that used in the CuBLAS batched calls.", "startOffset": 22, "endOffset": 26}], "year": 2016, "abstractText": "We propose a new method for training computationally efficient and compact convolutional neural networks (CNNs) using a novel sparse connection structure that resembles a tree root. Our sparse connection structure facilitates a significant reduction in computational cost and number of parameters of state-of-the-art deep CNNs without compromising accuracy. We validate our approach by using it to train more efficient variants of state-of-the-art CNN architectures, evaluated on the CIFAR10 and ILSVRC datasets. Our results show similar or higher accuracy than the baseline architectures with much less compute, as measured by CPU and GPU timings. For example, for ResNet 50, our model has 40% fewer parameters, 45% fewer floating point operations, and is 31% (12%) faster on a CPU (GPU). For the deeper ResNet 200 our model has 25% fewer floating point operations and 44% fewer parameters, while maintaining state-of-the-art accuracy. For GoogLeNet, our model has 7% fewer parameters and is 21% (16%) faster on a CPU (GPU).", "creator": "TeX"}}}