{"id": "1605.06652", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "21-May-2016", "title": "Bending the Curve: Improving the ROC Curve Through Error Redistribution", "abstract": "Classification performance is often not uniform over the data. Some areas in the input space are easier to classify than others. Features that hold information about the \"difficulty\" of the data may be non-discriminative and are therefore disregarded in the classification process. It is therefore important to consider the differences between the two types of data. The first is the performance characteristics. This is the reason that differences in the inputs are very common among data types. The second is the performance characteristics. The third is the performance characteristics. The Fourth, the information and the data are related equally to those of the inputs and are not in the data. We will take this as a general guideline to understand the differences.\n\n\nThe first rule for understanding the differences between the data and the inputs.\nThe first rule for understanding the differences.\nThe second rule for understanding the differences between the data and the inputs.\nThe third rule for understanding the differences between the data and the inputs.\nThe fourth rule for understanding the differences between the data and the inputs.\nThe fourth rule for understanding the differences between the data and the inputs.\nThe fourth rule for understanding the differences between the data and the inputs.\nThe fourth rule for understanding the differences between the data and the inputs.\nThe fourth rule for understanding the differences between the data and the inputs.\nThe fourth rule for understanding the differences between the data and the inputs.\nThe fourth rule for understanding the differences between the data and the inputs.\nThe fourth rule for understanding the differences between the data and the inputs.\nThe fourth rule for understanding the differences between the data and the inputs.\nThe fourth rule for understanding the differences between the data and the inputs.\nThe fourth rule for understanding the differences between the data and the inputs.\nThe fourth rule for understanding the differences between the data and the inputs.\nThe fourth rule for understanding the differences between the data and the inputs.\nThe fourth rule for understanding the differences between the data and the inputs.\nThe fourth rule for understanding the differences between the data and the inputs.\nThe fourth rule for understanding the differences between the data and the inputs.\nThe fourth rule for understanding the differences between the data and the inputs.\nThe fourth rule for understanding the differences between the data and the inputs.\nThe fourth rule for understanding the differences between the data and the inputs.\nThe fourth rule for understanding the differences between the data and the inputs.\nThe fourth rule for understanding the differences between the data and the", "histories": [["v1", "Sat, 21 May 2016 14:46:23 GMT  (504kb)", "http://arxiv.org/abs/1605.06652v1", null]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["oran richman", "shie mannor"], "accepted": false, "id": "1605.06652"}, "pdf": {"name": "1605.06652.pdf", "metadata": {"source": "CRF", "title": "Bending the Curve: Improving the ROC Curve Through Error Redistribution", "authors": ["Oran Richman"], "emails": ["roran@tx.technion.ac.il", "shie@ee.technion.ac.il"], "sections": [{"heading": null, "text": "ar X\niv :1\n60 5.\n06 65\n2v 1\n[ cs\nClassification performance is often not uniform over the data. Some areas in the input space are easier to classify than others. Features that hold information about the \u201ddifficulty\u201d of the data may be nondiscriminative and are therefore disregarded in the classification process. We propose a meta-learning approach where performance may be improved by post-processing. This improvement is done by establishing a dynamic threshold on the base-classifier results. Since the base-classifier is treated as a \u201cblack box\u201d the method presented can be used on any state of the art classifier in order to try an improve its performance. We focus our attention on how to better control the true-positive/false-positive tradeoff known as the ROC curve. We propose an algorithm for the derivation of optimal thresholds by redistributing the error depending on features that hold information about difficulty. We demonstrate the resulting benefit on both synthetic and real-life data."}, {"heading": "1 Introduction", "text": "Binary classification is perhaps the most widely studied in machine learning and many methods are used to obtain binary classifiers from data. For most appli-\ncations two performance measures are of special interest. The first is the True Positive Rate (TPR)\u2013the portion of true positives that are classified as such by the classifier. The second is the False Positive Rate (FPR)\u2013the portion of true negatives that are classified as positive by the classifier.\nThere is a fundamental trade-off between those two measures. This trade-off is often controlled through thresholding: the classifier produces a continuous score for each sample, and a threshold is used to determine if the sample is classified as positive (above the threshold) or negative (below the threshold). The pair (FPR,TPR) is the operating point of the resulting classifier.\nThe typical approach is to vary the threshold and obtain the complete curve of operating points called the Receiver operating characteristic (ROC) curve [12]. The performance of the classifier is then evaluated based on the whole curve using a specific operating point (i.e., a desired FPR level) or by considering the area under the curve (AUC). The area under the curve is an interesting measure since it as a probabilistic interpretation. The area under the curve of a classifier h(x),Rn \u2192 R is the probability that for a random positive sample x+ and a random negative sample x\u2212 the classifier will produce h(x+) > h(x\u2212). In this paper we show that the thresholding approach can be refined such that performance can be im-\nproved without retraining the classifier. Our approach is based on two observations. The first is that even after conditioning on the true class of the sample, the score is often correlated with some features (we will refer to them as auxiliary features). Moreover, those features may hold no or little discriminative information and are therefore disregarded during the learning process. For example, picture resolution may affect performance of object recognition greatly [18]. It is however often uncorrelated to the picture content. The Discriminatingly Trained Deformable Part Model classifier [10] is a popular state of the art object detector. It can be seen that in this classifier high resolution pictures receive higher scores compared with low resolution pictures [20]. The second observation is that the correlation with the score of positive examples and the correlation with the score of negative examples may be statistically different and even significantly so. We are mainly concerned with features that are correlated with the \u201cdifficulty\u201d of the problem. The reference to \u201cdifficulty\u201d implies some different effect of those features on the positive and negative examples score. For example, the scores of the positive and negative examples get more or less concentrated. Revisiting the image resolution example, the effect or reducing resolution on a real-object\u2019s score differs from the effect on a random background image. This difference can be exploited to improve performance for a specific operating point. For every desired operating point, we propose to use a threshold that depends on auxiliary features instead of being fixed for the entire input-space: the threshold is a function instead of a constant as in the standard approach. The threshold \u201ccurve\u201d can be designed so that performance is improved (i.e., higher TPR for a given FPR or a lower FPR for a given TPR). Our approach effectively rebalances the performance in different areas of the input space and redistributes the error. A simple heuristic for determining the threshold (as a function of the features) is to eliminate the correlation between the adjusted score (original score difference from the threshold) and the features. However, in the case where the positive and negative samples are affected differentially this is not trivial and requires estimating the conditional distribution of each class given the features. The score can be adjusted either according to the positive examples or according to the negative examples. In the first case we use a threshold which follows the mean of the score of negative examples. We refer to this approach as \u201cconstant false positive rate\u201d. Another approach is to use a threshold that follows the mean of the score of positive examples. We refer to this approach as \u201cconstant true positive rate\u201d. An illustration of these approaches on a simple example can be seen on Figure 2. Both approaches, however, suffer from the same structural deficiency, some threshold \u201ccurve\u201d is derived and then the entire ROC curve is created by adding a fixed offset to it. We present the Optimal Error Redistribution (OER) framework that \u201cbends\u201d the curve differently for different operating points. Our method is general and does not require any knowledge concerning the learning process used to train the classifier. The classifier is treated as a \u201cblack box\u201d allowing to \u201cbend the curve\u201d for a wide variety of classifiers. Our method is based on an alternative view of the ROC curve. Instead of viewing the operating point as a consequence of a varying threshold, we can consider the following optimization: Given some desired FPR, find the threshold curve (threshold as a function of the auxiliary features) that brings the TPR to a maximum. This essentially treats the FPR as a resource which need to be distributed between samples. Easy examples will contribute (in expectation) lower FPR than that contributed by the harder examples. This view allows introducing methods from the field or resource allocation (for example, methods from sensor management; a good review can be found at [15]) Example 1 Consider the following case. Some random variable X1 is drawn uniformly from the set [1 , 5]. Some random variable Y \u2208 {\u22121, 1} is drawn such that Y = 1 with probability 0.5. The random variable X2 is then drawn according to the following distribution:\nX2|y = 1 \u223c N(X1, 1), X2|y = \u22121 \u223c N(0, 1)\nSince X1 contains no discriminative information, a reasonable classifier for Y is h(X1, X2) = X2 (using a linear classifier do not change the results significantly, however it makes the visual understanding of the following figures more difficult). Figure 2 shows dynamic thresholds (with respect toX1) derived from the different approaches described above. The upper figure shows the curve matching the constant false positive approach. In this example it coincides with the original fixed (with respect toX1) threshold. The middle figure shows the curve matching the constant true positive approach. This corresponds to a linear classifier which uses also the data in X1. Both threshold curves are not optimal. The lower figure show the optimal curves. It can be seen that for different operating points the curve \u201cbends\u201d. When the example is \u201chard\u201d to classify, the optimal threshold varies much more than when the example is \u201ceasy\u201d. Using a more complex classifier may produce different curves than those presented in those figures but will not be able to produce the \u201cbending\u201d effect.\nExample 2 The optimal threshold may vary even when the mean and standard-deviations do not de-\npend on the features. This may happen when the prior changes. Meaning, the ratio between the quantity of positive and negative examples is related to the auxiliary features. As an example, consider the following. Some random variable Y \u2208 {\u22121, 1} is drawn such that Y = 1 with probability 0.5. Some random vector x = (X1, X2) is then drawn according to the following distribution:\nx|y = 1 \u223c N((0, 1), 2I), x|y = \u22121 \u223c N((0, 0), I),\nwhere I is the 2x2 unit matrix. It is easy to see that a reasonable classifier for Y is h(X1, X2) = X2 Observe that in this example the constant true positive and constant false positive coincide and derive a constant threshold. Figure 3 shows the data distribution of this example along with some optimal dynamic thresholds (with respect to X1). It can be seen that our method had essentially created a nonlinear classifier for each desired operating-point. As before, the different curves are not with fixed offset from one another. Interestingly, for large enough |X1| the prior is so significant that the optimal threshold\nis at \u2212\u221e. This characterizes situations in which the standard deviation of the positive examples score is larger than that of the negative examples score.\nRelated work Meta learning [25] is concerned with the enhancement of classifiers. A meta classifier takes a set of classifiers (base classifiers) and merges them in various ways to produce a unified classification result. The base classifiers are often trained using some variations of the same training set. This includes bagging [2], boosting [9] and many others (for example [3, 17]). Some works in this field target specifically the improvement of the ROC curve. In [22] the authors proposed the ROC Convex Hull (ROCCH) method. The ROCCH is based on the observation that given two classifiers with different ROC curves any point on the line segment between two operation points can be achieved. This is by randomly using one or the other classifier with appropriate probabilities. This allows to combine several classifiers to achieve an ROC curve which is better than each classifier. The method we are presenting in\nthis paper shares the basic approach with the field of meta-learning. In our case, the set of base-classifiers is the base classifier with different thresholds. We differ, however, from existing work in this field in two important aspects. First, we use as input only a single classifier. Second, the auxiliary feature space can be completely different from the base classifier\u2019s feature space. We do not require any \u201cre-training\u201d and no access to the classifier innerworkings is needed. As a result our method is much less sensitive to the way the original classifier was derived. This allows, in our view, much greater flexibility in applying this method. Note that we do require some training set to determine the dynamic thresholds. This set however can be different from the one used to train the classifier. A different approach that targets specifically the improvement of the ROC curve is trying to build classifiers that optimize the area under the curve (AUC) directly [4, 16, 26]. Using various surrogates the area under the curve can be optimized to derive some hopt(x). The optimization is done with respect to some hypothesis class. Our method does not optimize the AUC but rather optimizes the ROC curve point by point. The resulting classifier however is in a different hypothesis class than the base classifier. The relation between hopt(x) and the result of using our method on some h(x) is unclear. This is since the optimization of both methods is done for different hypothesis classes. However, our method can even improve hopt after it is derived using one of the AUC optimization methods. It is important to note also that while optimizing the AUC is possible for some limited set of hypothesis classes our method is general and can accommodate complex learning schemes. Recent work has also explored different threshold choice methods [5, 7, 14]. A threshold choice method adjusts the threshold to accommodate changes in the cost functions or class distributions. Those methods share a similarity with the ideas presented in this paper. However, the setting which we explore in this paper is substantially different. In our setting the threshold may vary between different regions of the input-space with the goal of achieving maximal average performance. The above mentioned work explores the case where the threshold is used to adapt\nthe base classifier in order to maximize current performance. It is important to note that simply appending the auxiliary features to the features vector will not produce the same result. First, similarly to meta learning, the resulting hypothesis class is significantly larger than that of the base classifier. Moreover, in many cases it is far from trivial to parametrize the resulting hypothesis class in such a way that will allow learning a \u201cstandard\u201d classifier. As can be seen by examples 1 and 2, using the method presented allows creating complex classifiers using simple (linear) base classifier. This also implies that simply treating the auxiliary features as features will often provide much smaller benefit. Also, it is far from trivial to directly learn such complex hypothesis classes. It is possible, obviously, to incorporate the ideas presented in this paper in the learning process of the base classifier. While such tight-coupling may produce better results such adoption is far from being trivial for most learning schemes. The method presented is treating the classifier as a \u201dblack box\u201d. Therefore, it can be easily incorporated on top of any existing classifier. As mentioned before, in our method the threshold is a function of the auxiliary features. If the base-classifier was \u201csmart enough\u201d to use the full information contained in those features then the method will produce no benefit. As we will see in the following this is often not the case, especially when the features have low correlation with the real class. Our contributions are threefold: First, we introduce a novel framework in which the threshold may vary over the input-space. Second, we introduce the Optimal Error Redistribution (OER) method. This method allows the creation of a meta classifier with improved ROC curve comparing with the base classifier. In addition, we derive a closed form solution of the optimal threshold for the special case of Gaussian distributions. Simulations which demonstrate the benefit which may arise are presented. Finally, we present a feature selection technique (for OER). This allows the selection of the auxiliary features without the explicit calculation of the ROC curve. We believe that the method presented in this paper should become a standard tool in ROC analysis. It is always beneficial to try and improve the ROC curve some more and our method proposes a generic way to do so. This paper is structured as follows: Section 2 defines the problem formally and provides the general OER method. Section 3 details a simple implementation and provides a closed-form solution for a special case. Section 4 outlines a feature selection technique which allows to select features for the method without the explicit calculation of the ROC curve. Section 5 demonstrates the feasibility of the problem on reallife data while Section 6 concludes the paper with some final thoughts and some still open questions."}, {"heading": "2 Optimal Error Redistribu-", "text": "tion Consider binary classification of objects represented by some vector x \u2208 Rn. The base classifier is based on some function h(x),Rn \u2192 R. In the original classification scheme a threshold is used to transform the output of the function to a binary classification. A sample is classified as positive if h(x) \u2265 k and negative otherwise. We allow the threshold to depend on some auxiliary feature vector x\u0303. Notice that x\u0303 should not be confused with the vector x that represents the data. The feature vector x\u0303 can be some subset of x or measured separately from the raw data (as in the example of picture resolution). We would like to find some function k(x\u0303) which assigns a threshold for each example. We approximate this function by partitioning the feature space into N bins. Each bin can be assigned a different threshold. The determination of a continuous function k(x\u0303) is possible in a special case which is outlined in Section 3.1. Formally, The data distribution is modelled as a superposition of N populations {Ai i = 1, . . . , N}. The auxiliary feature vector x\u0303 deterministically determine the population from which the example was taken. In the derived meta-classifier the original scalar threshold k is replaced with a vector (k1, . . . , kN ). Sample x \u2208 Ai is classified as positive if h(x) \u2265 ki and negative otherwise. In each population the score distribution obeys the\nfollowing:\nh(x)|x \u2208 Ai, y = 1 \u223c fi, h(x)|x \u2208 Ai, y = \u22121 \u223c gi.\n(1)\nWhere fi and gi are probability density functions. Denote the corresponding cumulative distribution functions as Fi and Gi. Further, p + i = P(x \u2208 Ai|y = 1) and p\u2212i = P(x \u2208 Ai|y = \u22121). The optimal threshold curve is given by solving an optimization problem in which the average TPR is maximised while some constraint is imposed on the average FPR. Namely, the optimization problem:\nmax(k1,...,kN ) N\u2211\ni=1\np+i (1\u2212 Fi(ki))\ns.t N\u2211\ni=1\np\u2212i (1 \u2212Gi(ki)) = C.\n(2)\nThis problem can be non-concave and finding the global maximum may be hard [23]. We can however, use an equivalent form of problem (2) to construct a gradient ascent algorithm that will lead us to a local maximum. Instead of solving Problem (2) we will solve the following problem for some \u03bb > 0:\nmax(k1,...,kN ) N\u2211\ni=1\np+i (1 \u2212 Fi(ki))\u2212 \u03bb N\u2211\ni=1\np\u2212i (1 \u2212Gi(ki)).\n(3) It is known that for both problems a necessary condition for a vector (k1, . . . , kN ) to be a solution is given by\np+i fi(ki) = \u03bbp \u2212 i gi(ki). (4)\nWe will denote as the benefit-cost ratio the expression:\np+i fi(ki) p\u2212i gi(ki) . (5)\nFor a thresholds vector (k1, . . . , kN ) to be optimal the benefit-cost ratio should be constant between populations. The OER algorithm is given by Algorithm 1. As we will see in the following, for the special case where fi and gi are Gaussian with the same variance, it is possible to derive a closed-form solution for the\nAlgorithm 1 OER\nParamters \u03b6 - learning rate, \u01eb - stopping threshold. Input: f , g, p+ ,p\u2212, \u03bb all vector operations are done point-wise. \u2206 = 1 k = (0, 0, . . . , 0) while \u2206 > \u01eb do k = k \u2212 \u03b6[p+f(k)\u2212 \u03bbp\u2212g(k)] \u2206 = ||p+f(k)\u2212 \u03bbp\u2212g(k)||2 end while return k\nglobal maxima. The necessary condition (4) implies that for the optimal threshold the benefit-cost ratio is constant between populations. Notice that since we would like to derive the complete ROC curve there is no need to solve the problem for different values of C. We can use the common benefit-cost ratio \u03bb as a parameter and derive the ROC curve by varying \u03bb. A specific operating point can then be chosen for implementation. The method presented can be also interpreted from a calibration perspective. Calibration is used to transform classifier outputs into posterior probabilities [13,21]. One popular calibration method, known as Platt calibration, fits a sigmoid model to the data [21]. The method finds two parameters a and b such that the posterior probability fits as good as possible to P (y = 1|h(x)) = 11+exp(ah(x)+b) . Earlier work as used a Gaussian fit as the base distribution [13]. Our method (with a slight modification, since we also use Gaussian as our base distribution) can be viewed as an extension to Platt calibration where the two scalars a an b are replaced with two functions of the auxiliary features. This results in:\nP (y = 1|h(x)) = 1\n1 + exp(a(x\u0303)h(x) + b(x\u0303)) .\nThe posterior probabilities can then be compared to a threshold such that the resulting classifier is equivalent to that received by our method. while this interpretation of our method is valid we believe that the interpretation detailed in this paper is clearer and\neasier to implement. Some previous work by Vapnik [24] considered a calibration method which is not uniform over the sample space. However, this method is limited to Support Vector Machines (SVM) and uses the original feature space with no auxiliary features. Our method is much more general."}, {"heading": "3 Implementation", "text": "The OER method presented earlier is general and flexible. There are two main design choices. First choosing the auxiliary features and corresponding Ai. Second, choosing a model for fi(y) and gi(y) and a corresponding method for fitting the data. Section 4 provides a heuristic method for choosing auxiliary features. However, this question is still open and a topic for future research. One simple model for fi(y) and gi(y) can be the use of a Gaussian model for the conditional behaviour of the score. Formally, the Gaussian model is stated as:\nh(x)|x \u2208 Ai, y = 1 \u223c N(\u00b5 + i , \u03c3 + i ) h(x)|x \u2208 Ai, y = \u22121 \u223c N(\u00b5 \u2212 i , \u03c3 \u2212 i ).\nOne of the main benefits of using such a model is that it requires only the estimation of the first and second moments. Both can be easily estimated for each bin. The necessary condition for an extremum now takes the form of:\np+i \u03c3+i e \u2212\n(ki\u2212\u00b5 + i ) 2\n2\u03c3 + i\n2 = p\u2212i \u03c3\u2212i \u03bbe \u2212\n(ki\u2212\u00b5 \u2212 i ) 2\n2\u03c3 \u2212\ni\n2\n. (6)\nWhere ki is the threshold for the desired classifier. The benefit-cost ratio is\np+i \u03c3 \u2212 i p\u2212i \u03c3 + i e \u2212\n((ki\u2212\u00b5 + i ) 2\n2\u03c3 + i\n2 + ((ki\u2212\u00b5\n\u2212 i ) 2\n2\u03c3 \u2212\ni\n2\n. (7)\nAn illustration of the benefit-cost ratio can be seen in Figure 4 for different relations between \u03c3+i and \u03c3 \u2212 i . Notice that when \u03c3+i = \u03c3 \u2212\ni the ratio (7) is strictly monotone in ki. Therefore, if \u2200i \u03c3 + i = \u03c3 \u2212\ni then (6) admits a single solution for every \u03bb. In that case, a closed form solution to the optimization problem can be derived. This however is not the case in general.\nIn the general case multiple extremum points may exist and therefore local optimization methods need to be used. Notice also that if \u03c3+i > \u03c3 \u2212\ni then there is a minimum to the benefit-cost ratio. Therefore, for large enough FPR the optimal threshold is \u2212\u221e. Similarly if \u03c3+i < \u03c3 \u2212\ni then there is a maximum to the benefit-cost ratio and for small enough FPR the optimal threshold is \u221e. A solution for the optimization problem can be found by using OER (Algorithm 1). The gradient is given by:\n\u2207i = p+i \u03c3+i exp(\u2212 (ki \u2212 \u00b5 + i ) 2 2\u03c3+i 2 )\u2212 p\u2212i \u03bb \u03c3\u2212i exp(\u2212 (ki \u2212 \u00b5 \u2212 i ) 2 2\u03c3\u2212i 2 ).\n(8) Notice that if \u03c3+i > \u03c3 \u2212\ni then the optimal threshold may be \u2212\u221e and if \u03c3+i < \u03c3 \u2212\ni then the optimal threshold may be \u221e. It is advised at each step to project the threshold into some fixed interval [\u2212K K] such that the gradient-ascent method will converge.\nExample 2 revisited We have used the described method on example 2. We divided X1 values into 120\nbins, where x \u2208 Ai if \u22126 + 0.1i < X1 < \u22125.9 + 0.1i. Two additional bins were used for the intervals X1 < \u22126 and X1 > 6. We have generated a data-set of 20000 data-points and tested the method. The results can be seen in Figure 5. It can be seen that the method presented a significant benefit over the two other approaches. As mentioned before, for sufficiently large |X1| the threshold is \u2212\u221e. This is since \u03c3+i > \u03c3 \u2212\ni and the benefit-cost ratio admits a minimum. When the desired benefit-cost ratio is below the minimum possible value it is always desirable to trade more TPR for more FPR. Notice that in those bins the calculation of h(x) is useless and can be avoided, therefore reducing computation resources needed. Direct comparison to AUC optimization methods (like [16]) is inappropriate. This is since it is highly sensitive to the hypothesis set for which the AUC is optimized. It is clear from Figure 3 that in spite the fact that our base classifier is linear, no linear classifier can achieve decent performance. Optimizing the AUC over a different hypothesis class may produce better results than ours. However, using this classifier as our base classifier and employing OER may improve it even further or at least will not reduce its performance."}, {"heading": "3.1 Similar effect on the score variance", "text": "In some cases, we can assume that the auxiliary features affect only the expectation of the score and do not affect the variance of positive and negative samples. Formally, \u2200i, \u03c3\u2212i = \u03c3 + i . In this case problem (2) can be solved directly. The solution to problem (2) is given by:\nki = \u03c3+i 2 [log p \u2212 i p + i + \u03bb]\n\u00b5+i \u2212 \u00b5 \u2212 i\n+ \u00b5+i + \u00b5 \u2212 i\n2 , (9)\nwhere \u2212\u221e < \u03bb < \u221e The ROC curve can then be derived by calculating the optimal threshold for different values of \u03bb ranging from \u2212\u221e to \u221e.\nRemark 1. For this special case the extension to an infinite number of bins is straightforward. Instead\nof fitting a Gaussian model to each bin it is possible to estimate some functions \u00b5+(x\u0303) and \u00b5\u2212(x\u0303) that represent the mean score as a function of the features for the positive and negative examples, respectively. Similarly, the functions \u03c3+(x\u0303),\u03c3\u2212(x\u0303),p+(x\u0303) and p\u2212(x\u0303) should be estimated. All of these functions can be estimated using conventional parametric estimation methods (For example, maximizing the log likelihood). The optimal threshold for each example can then be calculated using (9) by substituting \u00b5+i by \u00b5+(x\u0303), \u00b5\u2212i by \u00b5 \u2212(x\u0303) and so on.\nExample 1 revisited We used the described method on Example 1. We divided X1 values into 8 bins, where x \u2208 Ai if 0.5+ 0.5i < X1 < 1 + 0.5i. It follows that \u00b5i = 0.75+0.5i. Notice that we neglected the fact that \u03c3+i 6= \u03c3 \u2212\ni . We have generated a data-set of 20000 data-points and tested the method. The results are presented in Figure 6. It can be seen that the method presented a significant benefit over the two other approaches. Notice also that the derived ROC curve outperforms the convex hull of the two other methods, therefore outperform the ROCCH method."}, {"heading": "4 Finding good features to ap-", "text": "ply error redistribution on\nOne important question that arises in the context of OER is how to choose auxiliary features that provide the most benefit. Using features that do not contain relevant information may degrade performance due to over-fitting. The simplest approach is probably to use knowledge about the domain of the problem and consider features that may impact the problem difficulty. In image classification this can be for example picture\u2019s size, lightning conditions etc. In speaker verification difficulty is often related to the type of recording device. As the quality of the recording gets better it is easier to classify. Using the type of recording device as an auxiliary feature seems natural for this setting. Other examples include document length in spam filtering, channel characteristics in communication, distance from target in remote sensing and many more. Another obvious approach is to enumerate over potential options. For each feature apply OER, then calculate the derived ROC and choose the features that gives the most benefit. Sufficient estimation of\nthe ROC however requires large amount of labelled data. In certain cases, labelled data are scarce and therefore the estimation of the ROC is prone to errors. An alternative approach is to use the modelling process to uncover potential auxiliary features. Looking at the benefit-cost ratio provides us with the necessary insight about elements of the model that impact performance. One measure that can be proposed is the difference in separation difficulty. The separation difficulty (SD) is defined by the number of standard deviations between the mean of positive and negative examples. Namely the quantity\nSDi = (\u00b5 + i \u2212 \u00b5 \u2212 i )/(\u03c3 + i \u03c3 \u2212 i ).\nThe difference in separation difficulty can then be defined as var(SDi). The variance is taken with respect to the data\u2019s distribution. A large difference causes significant bending of the curve for different operating points. While this does not guarantee significant benefit on the ROC it implies a potential for such benefit. Example 1 demonstrates the feasibility of this measure. Another measure is the difference in the prior. The prior of bin i ( denoted by Pi) can be defined as\nPi = log(p + i \u03c3 \u2212 i /(p \u2212 i \u03c3 + i )).\nAs before, the difference in prior can be taken to be var(Pi) where the variance is taken with respect to the data distribution. A large difference indicates that there might be a potential for significant benefits. Example 2 demonstrates the feasibility of this measure. Those measures allow to establish a feature selection mechanism. First, enumerate over possible features, for each feature, partition the space into bins and measure the difference in separation difficulty and difference in prior. Only features for which those measures exceed some threshold should be used for OER. In the spirit of supervised PCA [1], further reduction in the feature space\u2019s dimension can be achieved by using only the few main principal components of the remaining features. The resulting feature space can then be divided into bins and OER can be applied.\nIt is important to calibrate the number of bins to the amount of training data available. Using too few bins leads to a mismatch between the data and model and therefore sub-optimal performance (which may even be worse than the original classifier). Using too many bins may lead to over-fitting. We advise using cross-validation in order to optimize the number of bins."}, {"heading": "5 Simulation Results", "text": "In addition to the results described earlier on synthetic examples we demonstrate our method\u2019s potential benefit on real-life data . First we tested the method using the UCI \u201cAdult\u201d dataset [19]. In this dataset the goal is to predict whether a person\u2019s income exceeds 50K/yr based on census data. We have used SVM as the base classifier. As ab auxiliary feature we selected number of years of education. This selection was made by reviewing the difference in separation difficulty and the difference in prior of all available features as explained in section 4. It is possible that choosing more then one auxiliary feature will improve the results. Figure 7 shows the derived ROC curves. Figure 8 shows a zoom-in of the ROC. It is clearly visible that the derived ROC curve is always better then the original. The AUC improves from 0.878 for the baseline SVM to 0.9028 for the derived classifier, 20.33% improvement. It should be noted that since some of the input features are categorical the ROC curve is highly sensitive. The results shown are averaged over ten-fold cross-validation. Note that on all conducted experiments OER outperforms the original classifier (0.001 p-value with the sign test).\nTaking a closer look at the data distribution and the derived thresholds shows that the improvement is made by keeping the threshold in the \u201ceasy\u201d bins low and increasing it on the more \u201cdifficult\u201d bins. It can be seen that the benefit arise even though the data distribution isn\u2019t Gaussian. It is possible that using a different distribution for modelling will produce better results. The data distribution as well as three possible thresholds can be seen on figure 9.\nSecond, we used OER for the task of object recognition. The task at hand is finding a certain object\n(person, car, dog, etc.) inside a picture. For that purpose, multiple bounding boxes (BB) are extracted from the picture. A classifier assign a score for each of the BB. Detection is made using some threshold on this score. For simplicity we have tested only the \u201cclassification\u201d stage of this problem.\nFrom the PASCAL ( [6]) data-base, positive examples of several classes of objects were extracted (only the bounding box which contains the object). From the same data-base, 100000 background examples were taken (from 10 different pictures). Each example was scored using the state of the art Discriminatingly Trained Deformable Part Model classifier [8, 10]. This classifier models the object as composed out of a set of parts (for example a person is composed out of head, hands, body, etc.). The classifier then matches the content of the bounding box with all possible orientations of the modelled object and its parts. It is known that the size of the bounding box affects the performance of this classifier significantly [18].\nThe size of the bounding box was used to divide the data into 4 bins. For each bin the expectation and standard deviation of the positive and negative examples were estimated as well as p+ and p\u2212. The scores for the class \u201cperson\u201d as a function of size can be seen in Figure 12.\nTwo effects are notable. First, the bigger the BB\n(higher resolution) the higher the score. It can be seen that the effects on positive and negative examples are roughly the same in expectation but for larger BB the variance of the positives decrease while the variance of the negatives remain roughly the same. Second, Since the data-base is constructed from partitioning of pictures, it contains a high number of small BBs and a low number of large BBs. The positive examples however are distributed roughly uniform over size. This causes the change of prior to be rather large.\nOptimal thresholds were calculated using OER and the results were compared to using a fixed threshold. The area under the curve (AUC) was used as a performance measure. The results are summarised in Table 1. As can be seen, substantial benefit (around 20% improvement) arises from using OER. Further examination of the benefit shows that for a very low FPRmodelling errors start to affect and benefit is minor. For a very high FPR there is not much room for improvement. In between, there is a substantial area in which benefit arise. Figures 10 shows the derived ROC curves for the class \u201cperson\u201d. Figure 11 shows a zoom-in of the ROC of the area in which the benefit is maximal. This improvement is done using only the picture size as a feature. This feature boost the base classifier\u2019s performance although it holds little to non discriminative information. Ten-fold cross-validation was performed. Recently the validity of AUC for model comparison was questioned [11]. While for simplicity we do use AUC as a performance measure our method improve the entire ROC curve. Since the improved ROC curve dominates the original one, other measures will also likely to show improvement.\nNote that on all conducted experiments OER outperforms the original classifier (0.001 p-value with the sign test). Notice also that we have used only a few bins (four) and a simplistic modelling as Gaussian. We believe that by using more complex features and more complex models this results can be even further improved.\n0 0.01 0.02 0.03 0.04 0.05 0.06 0.07 0.08 0.09 0.1 0.85\n0.9\n0.95\n1\nFalse Positive Rate\nT ru\ne P\no s it iv\ne R\na te\nROC curves of object recognition ,class \"person\"\noptimal threshold fixed threshold\nFigure 11: Zoom in on the ROC curve of object recognition, class \u201cperson\u201d\n50 100 150 200 250 300 350 400 450 500 \u22124\n\u22123\n\u22122\n\u22121\n0\n1\n2\nPicture size\nS c o re\nScore as a function of picture size\npositive negetive\nFigure 12: Score as a function of the bounding box size\nTable 1: Simulation results for several object classes\nclass person dog car chair Number of positive examples 2358 253 625 400 Fixed thresholds AUC 0.98663 0.97827 0.99292 0.99540 OER AUC 0.99043 0.98329 0.99411 0.99648 Improvement in 1\u2212 AUC 28.42% 23.1% 16.81% 23.48%"}, {"heading": "6 Conclusion", "text": "In this work we present a novel approach for improving the ROC curve of existing classifiers. We believe that this method should become a standard tool in ROC analysis and can enhance essentially any classifier. The method presented is general and may provide substantial benefit for any application: as long as there is sufficient data to mitigate overfitting, anyone who considers ROC optimization should try to \u201cbend the curve\u201d since there is nothing much to lose from it, and potentially much to gain.\nWe suggest three natural directions for further research: First, the method presented takes a two step approach. Start with modelling the data and then find optimal threshold curve according to this model. The model is used to derive the benefit-cost ratio. An alternative approach is to use empirical estimates of the benefit-cost ratio directly. The effect of such an approach is unclear. On the one hand, it may improve performance whenever a parametric model is in-adequate to describe the data. On the other hand, it may increase over-fitting.\nSecond, accurate estimation of the model\u2019s parameters requires a large amount of labelled data. This is especially true when the number of prospective features is large. Partitioning the space into too many bins may lead to a faulty model. An interesting open question is how to optimally partition the feature space.\nThird, in some scenarios it may be preferable to use a different optimization problems than (2). For example, in multi-view problems several classifiers, each with a different feature-space, are fused into a single classification output. It may be interesting to jointly optimize the threshold curves of those classifiers."}], "references": [{"title": "Prediction by supervised principal components", "author": ["Eric Bair", "Trevor Hastie", "Debashis Paul", "Robert Tibshirani"], "venue": "Journal of the American Statistical Association,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2006}, {"title": "Stacked regressions", "author": ["Leo Breiman"], "venue": "Machine learning,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 1996}, {"title": "Auc optimization vs. error rate minimization", "author": ["Corinna Cortes", "Mehryar Mohri"], "venue": "Advances in neural information processing systems,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2004}, {"title": "Cost curves: An improved method for visualizing classifier", "author": ["Chris Drummond", "Robert C Holte"], "venue": null, "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2006}, {"title": "The pascal visual object classes (voc) challenge", "author": ["M. Everingham", "L. Van Gool", "C.K.I. Williams", "J. Winn", "A. Zisserman"], "venue": "International Journal of Computer Vision,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2010}, {"title": "Adaptive fraud detection", "author": ["Tom Fawcett", "Foster Provost"], "venue": "Data mining and knowledge discovery,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 1997}, {"title": "Object detection with discriminatively trained part based models", "author": ["P.F. Felzenszwalb", "R.B. Girshick", "D. McAllester", "D. Ramanan"], "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2010}, {"title": "Experiments with a new boosting algorithm", "author": ["Yoav Freund", "Robert E Schapire"], "venue": "In ICML,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 1996}, {"title": "Measuring classifier performance: a coherent alternative to the area under the roc curve", "author": ["David J Hand"], "venue": "Machine learning,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2009}, {"title": "The meaning and use of the area under a receiver operating characteristic (roc", "author": ["James A Hanley", "Barbara J McNeil"], "venue": "curve. Radiology,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 1982}, {"title": "Classification by pairwise coupling", "author": ["Trevor Hastie", "Robert Tibshirani"], "venue": null, "citeRegEx": "13", "shortCiteRegEx": "13", "year": 1998}, {"title": "A unified view of performance metrics: Translating threshold choice into expected classification loss", "author": ["Jos\u00e9 Hern\u00e1ndez-Orallo", "Peter Flach", "Cesar Ferri"], "venue": "The Journal of Machine Learning Research,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2012}, {"title": "Sensor management: Past, present, and future", "author": ["Alfred O Hero", "Douglas Cochran"], "venue": "Sensors Journal, IEEE,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2011}, {"title": "Optimising area under the roc curve using gradient descent", "author": ["Alan Herschtal", "Bhavani Raskutti"], "venue": "In Proceedings of the twenty-first international conference on Machine learning,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2004}, {"title": "The random subspace method for constructing decision forests", "author": ["Tin Kam Ho"], "venue": "Pattern Analysis and Machine Intelligence, IEEE Transactions on,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 1998}, {"title": "Diagnosing error in object detectors", "author": ["Derek Hoiem", "Yodsawalai Chodpathumwan", "Qieyun Dai"], "venue": "In Computer Vision\u2013ECCV", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2012}, {"title": "Toward real-time pedestrian detection based on a deformable template model", "author": ["Marco Pedersoli", "Jordi Gonz\u00e0lez", "Xu Hu", "Xavier Roca"], "venue": "Intelligent Transportation Systems, IEEE Transactions on,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2014}, {"title": "Probabilistic outputs for support vector machines and comparisons to regularized likelihood methods", "author": ["John Platt"], "venue": "Advances in large margin classifiers,", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 1999}, {"title": "Robust classification for imprecise environments", "author": ["Foster Provost", "Tom Fawcett"], "venue": "Machine learning,", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2001}, {"title": "Engineering optimization: theory and practice", "author": ["Singiresu S Rao", "SS Rao"], "venue": null, "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2009}, {"title": "Statistical learning theory, volume 1", "author": ["Vladimir Naumovich Vapnik"], "venue": "Wiley New York,", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 1998}, {"title": "A perspective view and survey of meta-learning", "author": ["Ricardo Vilalta", "Youssef Drissi"], "venue": "Artificial Intelligence Review,", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2002}, {"title": "Optimizing classifier performance via an approximation to the wilcoxon-mann-whitney statistic", "author": ["Lian Yan", "Robert H Dodier", "Michael Mozer", "Richard H Wolniewicz"], "venue": "In Proceedings of the 20th International Conference on Machine Learning", "citeRegEx": "26", "shortCiteRegEx": "26", "year": 2003}], "referenceMentions": [{"referenceID": 9, "context": "The typical approach is to vary the threshold and obtain the complete curve of operating points called the Receiver operating characteristic (ROC) curve [12].", "startOffset": 153, "endOffset": 157}, {"referenceID": 15, "context": "For example, picture resolution may affect performance of object recognition greatly [18].", "startOffset": 85, "endOffset": 89}, {"referenceID": 16, "context": "It can be seen that in this classifier high resolution pictures receive higher scores compared with low resolution pictures [20].", "startOffset": 124, "endOffset": 128}, {"referenceID": 12, "context": "This view allows introducing methods from the field or resource allocation (for example, methods from sensor management; a good review can be found at [15])", "startOffset": 151, "endOffset": 155}, {"referenceID": 0, "context": "Some random variable X1 is drawn uniformly from the set [1 , 5].", "startOffset": 56, "endOffset": 63}, {"referenceID": 3, "context": "Some random variable X1 is drawn uniformly from the set [1 , 5].", "startOffset": 56, "endOffset": 63}, {"referenceID": 21, "context": "Related work Meta learning [25] is concerned with the enhancement of classifiers.", "startOffset": 27, "endOffset": 31}, {"referenceID": 7, "context": "This includes bagging [2], boosting [9] and many others (for example [3, 17]).", "startOffset": 36, "endOffset": 39}, {"referenceID": 1, "context": "This includes bagging [2], boosting [9] and many others (for example [3, 17]).", "startOffset": 69, "endOffset": 76}, {"referenceID": 14, "context": "This includes bagging [2], boosting [9] and many others (for example [3, 17]).", "startOffset": 69, "endOffset": 76}, {"referenceID": 18, "context": "In [22] the authors proposed the ROC Convex Hull (ROCCH) method.", "startOffset": 3, "endOffset": 7}, {"referenceID": 2, "context": "A different approach that targets specifically the improvement of the ROC curve is trying to build classifiers that optimize the area under the curve (AUC) directly [4, 16, 26].", "startOffset": 165, "endOffset": 176}, {"referenceID": 13, "context": "A different approach that targets specifically the improvement of the ROC curve is trying to build classifiers that optimize the area under the curve (AUC) directly [4, 16, 26].", "startOffset": 165, "endOffset": 176}, {"referenceID": 22, "context": "A different approach that targets specifically the improvement of the ROC curve is trying to build classifiers that optimize the area under the curve (AUC) directly [4, 16, 26].", "startOffset": 165, "endOffset": 176}, {"referenceID": 3, "context": "Recent work has also explored different threshold choice methods [5, 7, 14].", "startOffset": 65, "endOffset": 75}, {"referenceID": 5, "context": "Recent work has also explored different threshold choice methods [5, 7, 14].", "startOffset": 65, "endOffset": 75}, {"referenceID": 11, "context": "Recent work has also explored different threshold choice methods [5, 7, 14].", "startOffset": 65, "endOffset": 75}, {"referenceID": 19, "context": "This problem can be non-concave and finding the global maximum may be hard [23].", "startOffset": 75, "endOffset": 79}, {"referenceID": 10, "context": "Calibration is used to transform classifier outputs into posterior probabilities [13,21].", "startOffset": 81, "endOffset": 88}, {"referenceID": 17, "context": "Calibration is used to transform classifier outputs into posterior probabilities [13,21].", "startOffset": 81, "endOffset": 88}, {"referenceID": 17, "context": "One popular calibration method, known as Platt calibration, fits a sigmoid model to the data [21].", "startOffset": 93, "endOffset": 97}, {"referenceID": 10, "context": "Earlier work as used a Gaussian fit as the base distribution [13].", "startOffset": 61, "endOffset": 65}, {"referenceID": 20, "context": "Some previous work by Vapnik [24] considered a calibration method which is not uniform over the sample space.", "startOffset": 29, "endOffset": 33}, {"referenceID": 13, "context": "Direct comparison to AUC optimization methods (like [16]) is inappropriate.", "startOffset": 52, "endOffset": 56}, {"referenceID": 0, "context": "In the spirit of supervised PCA [1], further reduction in the feature space\u2019s dimension can be achieved by using only the few main principal components of the remaining features.", "startOffset": 32, "endOffset": 35}, {"referenceID": 4, "context": "From the PASCAL ( [6]) data-base, positive examples of several classes of objects were extracted (only the bounding box which contains the object).", "startOffset": 18, "endOffset": 21}, {"referenceID": 6, "context": "Each example was scored using the state of the art Discriminatingly Trained Deformable Part Model classifier [8, 10].", "startOffset": 109, "endOffset": 116}, {"referenceID": 15, "context": "It is known that the size of the bounding box affects the performance of this classifier significantly [18].", "startOffset": 103, "endOffset": 107}, {"referenceID": 8, "context": "Recently the validity of AUC for model comparison was questioned [11].", "startOffset": 65, "endOffset": 69}], "year": 2016, "abstractText": "Classification performance is often not uniform over the data. Some areas in the input space are easier to classify than others. Features that hold information about the \u201ddifficulty\u201d of the data may be nondiscriminative and are therefore disregarded in the classification process. We propose a meta-learning approach where performance may be improved by post-processing. This improvement is done by establishing a dynamic threshold on the base-classifier results. Since the base-classifier is treated as a \u201cblack box\u201d the method presented can be used on any state of the art classifier in order to try an improve its performance. We focus our attention on how to better control the true-positive/false-positive tradeoff known as the ROC curve. We propose an algorithm for the derivation of optimal thresholds by redistributing the error depending on features that hold information about difficulty. We demonstrate the resulting benefit on both synthetic and real-life data.", "creator": "LaTeX with hyperref package"}}}