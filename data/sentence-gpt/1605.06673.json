{"id": "1605.06673", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "21-May-2016", "title": "Cross Domain Adaptation by Learning Partially Shared Classifiers and Weighting Source Data Points in the Shared Subspaces", "abstract": "Transfer learning is a problem defined over two domains. These two domains share the same feature space and class label space, but have significantly different distributions. One domain has sufficient labels, named as source domain, and the other domain has few labels, named as target do- main domain. This type of information can be divided into two domains. Here we show how the labels between the source domain and target domain have differences.\n\nFigure 1: The labels are separated by class labels.\nFigure 2: The label is separated by class labels.\nFigure 3: The labels are separated by class labels.\nFigure 4: The label is separated by class labels.\nFigure 5: The label is separated by class labels.\nFigure 6: The labels are separated by class labels.\nFigure 7: The labels are separated by class labels.\nFigure 8: The labels are separated by class labels.\nFigure 9: The labels are separated by class labels.\nFigure 10: The labels are separated by class labels.\nFigure 11: The labels are separated by class labels.\nFigure 12: The labels are separated by class labels.\nFigure 13: The labels are separated by class labels.\nFigure 14: The labels are separated by class labels.\nFigure 15: The labels are separated by class labels.\nFigure 16: The labels are separated by class labels.\nFigure 17: The labels are separated by class labels.\nFigure 18: The labels are separated by class labels.\nFigure 19: The labels are separated by class labels.\nFigure 20: The labels are separated by class labels.\nFigure 21: The labels are separated by class labels.\nFigure 22: The labels are separated by class labels.\nFigure 23: The labels are separated by class labels.\nFigure 24: The labels are separated by class labels.\nFigure 25: The labels are separated by class labels.\nFigure 26: The labels are separated by class labels.\nFigure 27: The labels are separated by class labels.\nFigure 28: The labels are separated by class labels.\nFigure 29: The labels are separated by class labels.\nFigure 30: The labels are separated by class labels.\nFigure 31: The labels are separated by class labels.\nFigure 32: The labels are separated by class labels.\nFigure 33: The labels are separated by class labels.\nFigure 34: The labels are separated by class labels.\nFigure 35: The labels are separated by class labels.\nFigure 36: The labels are separated", "histories": [["v1", "Sat, 21 May 2016 16:57:37 GMT  (35kb)", "http://arxiv.org/abs/1605.06673v1", null]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["hongqi wang", "anfeng xu", "shanshan wang", "sunny chughtai"], "accepted": false, "id": "1605.06673"}, "pdf": {"name": "1605.06673.pdf", "metadata": {"source": "CRF", "title": "Cross Domain Adaptation by Learning Partially Shared Classifiers and Weighting Source Data Points in the Shared Subspaces", "authors": ["Hongqi Wang", "Sunny Chughtai", "Anfeng Xu", "Shanshan Wang"], "emails": ["wanghongqi@yahoo.com,", "anfengxu150@yahoo.com", "sunnychughtai313@yahoo.com"], "sections": [{"heading": null, "text": "ar X\niv :1\n60 5.\n06 67\n3v 1\n[ cs\n.L G\n] 2\n1 M\nay 2\nHongqi Wang, Anfeng Xu, and Shanshan Wang School of Management of Harbin University of Science and Technology, Harbin 150000, China E-mail: wanghongqi@yahoo.com, anfengxu150@yahoo.com Anfeng Xu is the corresponding author.\nSunny Chughtai Punjab University College of Information Technology (PUCIT), University of the Punjab, Lahore, Pakistan E-mail: sunnychughtai313@yahoo.com\nKeywords Transfer Learning \u00b7 Subspace Learning \u00b7 Distribution Matching \u00b7 Weighted Mean \u00b7 Travel Destination Review"}, {"heading": "1 Introduction", "text": "In this paper, we study the problem of transfer learning. This problem is defined over two domains, which is a source domain and a target domain. In the source domain, there are sufficient labeled data points. In the target domain, only a small number of data points are labeled. To learn a effective classifier for the target domain, we need the help of the source domain. However, the source domain and target domain have significantly different distributions, although they share the same feature and label space. Transfer learning, or named as cross domain adaptation, aims to fill the gap between the two domains to provide sufficient data for the target domain learning problem [7,19,18,27,28, 20].\nExample. For example, in the problem of face expression, we want to train a classifiers for images of face to recognize the expression of this face. To this end, we can label the images of a training face image set of one individual, and when the images of anther individual appears, we can only label a few of his/her images, and training the classifier using the data of both individuals. The first individual is considered as a source domain, and the new individual is considered as target domain. The target domain has only a few labeled images while all the images of the source domain are labeled. Moreover, it is obvious that the distributions of these two individuals are different. Thus it is necessary to fill this gap for the learning from the two domains.\n1.1 Existing Works and Their Shortages\nThere are plenty of works proposed to solve this problem, but they have some limitations. We discuss them as follows.\n\u2013 Chen et al. [2] proposed a transfer learning framework for the adaptation of text mining models which is based on low-rank shared concept space. It minimize the gap between the distributions of the two domains, and meanwhile minimize the classification errors over the source domain. This method performs the domain adaptation over both the linear and kernel space of the features. \u2013 Chu et al. [3] proposed a transductive learning model, which is called selective transfer machine, for the problem of face expression recognition. This method tries to attenuate the person-specific biases of face expressions by personalizing the face expression classifiers. It solve this problem by jointly learning the source domain classifier and selecting the source domain data points for the learning problem. The selected source domain data points are assumed to be the most relevant data points to the target domain.\n\u2013 Ma et al. [16] proposed a transfer learning which can adapt knowledge from a source domain to a target domain, and can handle the features of the source and target domains which are partially different. The proposed framework minimizes the classification errors of the partially different features over both the source and target domains, while also hopes the classification results of the complete features of the target domain and the that of the features shared by the source domain can be consistent. Meanwhile the \u21132,p norm of the matrix of parameters of source and target domains are minimize so that the shared features of both domain can be jointly selected. \u2013 Xiao and Guo [25] proposed a transfer learning method to handle the domain adaptation problem with completed feature spaces for the source and target domains. This method is based on the learning of classifiers of the source domain and the mapping of the target domain data points to the source domain. The mapping is conducted by the kernel matching of the kernel matrix of the two domains. The mapping of the kernel matrices is based on the Hilbert Schmidt Independence Criterion. \u2013 Li et al. [8] proposed method for the heterogeneous domain adaptation problem. In this transfer learning problem, the features from the source and target domains can also be different. To handle different feature spaces, two different projection matrices are applied to them, so that they can be mapped to a shared subspace. Moreover, a new features mapping method is proposed for each domain, and the data points are augmented with the transformed features and the original features. Furthermore, the support vector machine framework is employed to learn the transformation matrices and the classifier parameters.\nThe shortages of these works paper are of three folds.\n\u2013 Some methods ignore the label information of the target domain, which is critical for the learning of the target domain. Works of Chen et al. [2] and Chu et al. [3] do not use the labels of the target domain data points. Without using the target domain label information, only minimizing the gap of distributions between the two domains cannot guarantee the learned classifier not over-fitting to the source domain. \u2013 Some works also ignore the local connection of the data points of both source and target domain. The learning is performed over the data points independently. However, researches of manifold learning showed that it is important to explore the local connections between the data points when the classifier is learned. Among all the methods mentioned above, only the work of Xiao and Guo [25] uses the local connection information to regularize the learning of the classifiers. \u2013 Some works ignore the differences of the source domain data points for the learning problem of target domain. It treats all the source domain data points evenly. However, we observed that actually different source domain data points paly different roles in the learning of target domain classifier. Only the work of Chu et al. [3] select the source domain data points for the\nlearning of the target domain classifier, while all the other methods treat them source domain data points equally.\n1.2 The Contributions\nIn this paper, we propose a novel transfer learning problem. This method is aimed to solve the problems of existing transfer learning methods. The learning framework and model is summarized as follows,\n\u2013 To solve the problem of ignoring the labels of the target domain, we proposed to minimize the classification errors of the data points of both the source and target domains. The classification errors are measured by the loss functions such as logistic loss and hinge loss. \u2013 To solve the problem of ignoring the differences of importance of different source domain data points, we propose to weight them with different weighting factors. The weight factors are used in two different scenes. The first scene is the matching of the distributions of the two domains. We propose to use the weighted mean of the source domain to match the original mean of the target domain. In this way, the source domain data points which match better to the target domain can receive larger weighting factors. In the second scene, the loss terms of these data points with larger weighting factors are also assigned with larger weights in the loss function. \u2013 To solve the problem of ignoring the local connection information, we propose to use local reconstruction information to regularize the learning of the weights of the source domain data points, and the classifier of the target domain. The local reconstruction coefficients are learned from the original features. \u2013 Moreover, an inessential contribution of this work is a novel cross domain classification framework to implement the above three thoughts. To gap the fill of the two domains, we propose to map the data of two domains to some shared subspaces, and then design a shared classifier in this space. In these shared subspaces, we match the distributions by using the weighting factors of the source domain data points. Moreover, we propose to adapt this shared classifier to source and target domains by addling adaptation functions of the original spaces. The final classifiers are actually partially shared classifiers. The loss functions are defined based on these partially shared classifiers, and the local reconstruction regularization is also performed over the partially shared classifier.\nThe modeling of the learning problem is based on the three thoughts mentioned above, and each thought is corresponding to a term of the objective function. The learning problem is constructed by minimizing the objective function with regard to the parameters of the mapping matrix, the shared classifier parameter and the adaptation parameters. Moreover, the weighting factors should also be considered as variables to learn. We design an iterative learning algorithm to solve this problem.\n1.3 Organization of the Rest Parts\nThe paper is organized as follows. In section 2, we introduce the model of the proposed data representation and classification method, and the learning method of the parameters of this model is introduced in section 3. In section 4, the proposed algorithm is evaluated over some benchmark data sets. In section 5, the conclusion is given, and in section 6, the future work is summarized."}, {"heading": "2 Transfer Representation and Classification Model", "text": "2.1 Shared Subspace Representation\nSuppose we have a source domain and a target domain, and each domain has a training set. The source domain training set is give as a set of n1 labeled data points, S = {(xs1, y s 1), \u00b7 \u00b7 \u00b7 , (x s n1 , ysn1)}, where x s i \u2208 R\nm is the feature vector of m dimensions of the i-th data point, and ysi \u2208 {+1,\u22121} is the binary label of the i-th data point. The target domain training set is given as a set of n2 partially labeled data points, T = {(xt1, y t 1), \u00b7 \u00b7 \u00b7 , (x t n3 , ytn3),x t n3+1\n, \u00b7 \u00b7 \u00b7 ,xtn2}, where xtj \u2208 R\nm is the feature vector of the j-th data point, and ytj \u2208 {+1,\u22121} is the binary label of the j-th data point. In the training set of the target domain, the first n3 data points are labeled, while the remaining n2 \u2212n3 data points are not labeled. To represent the data points from both the source and target domains, we proposed to map them to r shared subspaces by a transformation matrix \u0398 \u2208 Rr\u00d7m, where r < m is the number of subspaces. Given a data point x, we use the transformation matrix to project it to a lower dimensional space, and the feature vector in this space y is obtained as,\ny = \u0398x. (1)\nPlease note that this subspace is shared by both the source and target domain, thus it makes it possible to learn a shared classifier for the two domains. To this end, we designed a linear classifier and apply it to a feature vector, y, in the subspace which is obtained from (1),\ng(x) = w\u22a4y = w\u22a4\u0398x, (2)\nwhere w \u2208 Rr is the parameter vector of the shared classifier g.\n2.2 Filling Gap Between Source and Target Domains\nAlthough the shared subspace and classifier seems to work well for both domains, there is a gap between these two domains. Usually the two domains have significant different distributions in the original feature space, and simply mapping the data of two domains to shared subspaces will not automatically fill this gap. To solve this problem, we propose two possible solutions."}, {"heading": "2.2.1 Partially Shared Classifiers", "text": "The first solution is to design the classifiers of source and target domain by adapting the shared classifier to the two domains respectively. The adaptation is implemented by adding a linear function over the original feature vectors. For a source domain data point, xs, the classifier is given as follows,\nf(xs) = g(xs) +\u2206s(x s) = w\u22a4\u0398xs + u\u22a4xs, (3)\nwhere \u2206s(x s) = u\u22a4xs is the source domain adaption function and u \u2208 Rm is the parameter vector of the adaptation function. For a target domain data point, xt, the classifier is obtained as the combination of g and a target domain function,\nh(xt) = g(xt) +\u2206t(x t) = w\u22a4\u0398xt + v\u22a4xt, (4)\nwhere \u2206t(x t) = v\u22a4xt is the target domain adaptation function, and v \u2208 Rm is its parameter vector. In this way, the source and target domain has a shared classifier, g, and two independent adaptation functions, \u2206s and \u2206t. Using the subspace projection shared by both two domains and its corresponding classifier g, the gap between the two domains are somehow filled. Meanwhile, we also respect the difference between the two domains by adapting the shared classifier to two different domains respectively. In this classification framework, we have several parameters to learn, including \u0398, w, u, and v. To estimate these parameters, we propose a novel learning framework with an objective function and an optimization method."}, {"heading": "2.2.2 Distribution Matching in the Subspaces by Weighting the Source Domain Data Points", "text": "The second solution to fill the gap is to match the distributions of the two domains in the subspaces. Since the subspaces are shared by both the domains, we hope that in these subspaces, the gap between distributions of source and target domains can be filled. To this end, we proposed to represent the distributions of subspaces of source and target domains as the mean of the vectors of their data points in the subspaces,\n\u00b5s = 1\nn1\nn1 \u2211\ni=1\n\u0398xsi , and\n\u00b5t = 1\nn2\nn2 \u2211\nj=1\n\u0398xtj\n(5)\nwhere \u00b5s and \u00b5t are the mean vectors of source and target domains respectively. Moreover, we argue that actually different source domain data points plays different roles in the matching of distributions of two domains. It is not suitable to treat all the source domain data points evenly when they are\nmatched to the target domain. Some source domain data points are more important to the learning of target domain classifier, and they matches to the target domain better than the other source domain data points. Thus we propose to weight the source domain data points with different weights, so that the important data points can obtain larger weights than other data points. The nonnegative weight of the i-th source domain data points is defined as \u03c0i, and the vector of weights is defined as \u03c0 = [\u03c01, \u00b7 \u00b7 \u00b7 , \u03c0n1 ]. With these weights, we refine the mean vector of the source domain as follows,\n\u00b5\u03c0s = 1\nn1\nn1 \u2211\ni=1\n\u0398xsi\u03c0i. (6)\nIn this refined mean, each data point in the subspaces are weighted by a factor \u03c0i. To match the refined distribution mean of the source domain and the target domain mean, we propose to minimize the squared \u21132 norm distance between them with regard to both \u0398 and \u03c0,\nmin \u0398,\u03c0\n1 2 \u2016\u00b5\u03c0s \u2212 \u00b5t\u2016 2 2\n= 1\n2\n\u2225 \u2225 \u2225 \u2225 \u2225 \u2225 1 n1 n1 \u2211\ni=1\n\u0398xsi\u03c0i \u2212 1\nn2\nn2 \u2211\nj=1\n\u0398xtj\n\u2225 \u2225 \u2225 \u2225 \u2225 \u2225 2\n2\n.\n(7)\nBy solving this problem, we argue that a good combination of \u0398 and \u03c0 should make the data distributions of source domain and target domain match each other well, i.e., make the squared \u21132 norm distance in (7) minimum. In this problem, we need to approximate the weighting factors in \u03c0."}, {"heading": "3 Parameter Estimation Method", "text": "In this section, we build a joint learning framework to learn the parameters of our classification methods. We will first discuss the objective function of the learning problem, and then minimize the objective function with regard to the variables.\n3.1 Objective function\nThe objective function of the learning framework is given as follows,\nO(\u0398,w,u,v,\u03c0) = n1 \u2211\ni=1\nL(ysi , f(x s i ))\u03c0i +\nn3 \u2211\nj=1\nL(yti , h(x t j))\n+ C1\n2\n(\n\u2016u\u201622 + \u2016v\u2016 2 2\n)\n+ C2\n\n \nn1 \u2211\ni=1\n\u2225 \u2225 \u2225 \u2225 \u2225 \u2225 \u03c0i \u2212 \u2211\nk\u2208N s i\n\u03c9sik\u03c0k\n\u2225 \u2225 \u2225 \u2225 \u2225 \u2225 2\n2\n+\nn2 \u2211\nj=1\n\u2225 \u2225 \u2225 \u2225 \u2225 \u2225 h(xtj)\u2212 \u2211\nk\u2032\u2208N t j\n\u03c9tjk\u2032h(x t k\u2032 )\n\u2225 \u2225 \u2225 \u2225 \u2225 \u2225 2\n2\n\n \n+ C3\n2\n\u2225 \u2225 \u2225 \u2225 \u2225 \u2225 1 n1 n1 \u2211\ni=1\n\u0398xsi\u03c0i \u2212 1\nn2\nn2 \u2211\nj=1\n\u0398xtj\n\u2225 \u2225 \u2225 \u2225 \u2225 \u2225 2\n2\n.\n(8)\nIn this objective function, we have four types of terms. We discuss them as follows in details.\n\u2013 Losses of classification errors. The first two terms of the objective function are the classification losses of the data points of source and target domains. In both terms, L(y, f(x)) is the loss function of a data point, where x is the input, f is the classifier, and y is the true class label. We discuss the following three types of loss functions,\n1. Hinge loss: L(y, f(x)) = max(0, 1\u2212 yf(x)), 2. Logistic loss: L(y, f(x)) = log (1 + exp(\u2212yf(x))), and 3. Exponential loss: L(y, f(x)) = exp(\u2212yf(x)).\nThe first term, \u2211n1 i=1 L(y s i , f(x s i ))\u03c0i is the weighted summation of the losses of classification errors the source domain data points. Each loss is weighted by its corresponding weighting factor of the distribution matching. The motive is that we observed that if a source domain data point is important for the matching of source and target domain matching in the subspaces, it is also important for the learning of classifier of target domain. Using the same weighting factor to regularize the learning of the source domain classifier also regularize the learning of the subspaces and shared classifier, which is critical for the target domain. The second term,\n\u2211n3 j=1 L(y t i , h(x t j)), is the losses of classification of the\nlabeled target domain data points. Only the first n3 labeled data points are considered in this loss function because only their labels are available. \u2013 Adaptation function parameter regularization. We further propose to regularize the parameter vectors of both the source and target domain adaptation functions to avoid the over-fitting to different domains. The motive for this is that since the classifiers are adapted from a shared subspace classifier, we do not want to make the adaptation too complex and over-fitted to the training sets of two different domains, so that the gap\nwill not be enlarged but filled. To this end, we argue that the adaption parameters should be as simple as possible. To measure the complexities, we use the squared \u21132 norms of the parameters of both \u2206s and \u2206t, u and v. Thus the third term of (8), C1\n2\n(\n\u2016u\u201622 + \u2016v\u2016 2 2\n)\n, is minimized. C1 is the weight of this term in the objective function. \u2013 Neighborhood reconstruction regularization of \u03c0 and target do-\nmain classifier. We argue the if the neighbors of a source domain data points are receiving large weights, itself should also receive large weights. We denote the neighbor set of a source data point xsi as N s i . To impose our argument, we propose to reconstruct each data point xsi for its neighbors in N si , and further use the reconstruction coefficients to regularize the learning of \u03c0. The reconstruction coefficients are solved as the following minimization problem,\nmin \u03c9ik,k\u2208N s i\n\u2225 \u2225 \u2225 \u2225 \u2225 \u2225 xsi \u2212 \u2211\nk\u2208N s i\n\u03c9sikx s k\n\u2225 \u2225 \u2225 \u2225 \u2225 \u2225 2\n2\ns.t. \u2211\nk\u2208N si\n\u03c9sik = 1, \u03c9 s ik \u2265 0, \u2200 k \u2208 N s i\n(9)\nwhere \u03c9sik, k \u2208 N s i are the coefficients for reconstruction of x s i from the neighbors in N si , and \u2225 \u2225 \u2225xsi \u2212 \u2211\nk\u2208N s i \u03c9ikx\ns k\n\u2225 \u2225 \u2225 2\n2\nis the reconstruction error\nmeasure. Using the reconstruction coefficients, we regularize the learning of the weighting factors, by minimizing the reconstruction error in the space of \u03c0, by minimizing \u2225 \u2225 \u2225\u03c0i \u2212 \u2211\nk\u2208N s i \u03c9sik\u03c0k\n\u2225 \u2225 \u2225 2\n2\n. In this case, if \u03c9sik is\nlarge, it means xsk contribute significantly to the reconstruction of x s i , and the similarity between xsi and x s k is large. Thus we also hope \u03c0i and \u03c0k can be similar to each other. Similarly, we also regularize the learning of the target domain classifier by the neighborhood reconstruction coefficients. The motive is to use the unlabeled data points of the target domain. These data points are not used in the classification error part of the objective. They are used to match the distributions of the two domains, however, the distribution matching does not consider the label information. Thus we propose to propagate the label information by the neighborhood reconstruction regularization. In the third term of the objective of (8), the reconstruction error of the classification\nresponses are also minimized as \u2211n2\nj=1\n\u2225 \u2225 \u2225h(xtj)\u2212 \u2211\nk\u2032\u2208N t j \u03c9tjk\u2032h(x t k\u2032)\n\u2225 \u2225 \u2225 2\n2\n. By\nminimizing this term, we hope that the neighboring target domain data points also have neighboring classification results. In this term, N tj is the set of neighboring target domain data points of xtj , and \u03c9 t jk\u2032 , k \u2032 \u2208 N tj are the reconstruction coefficient of xtj from N t j . \u2013 The last term is the target domain distribution matching term. We have discussed this term in (7). C3 is the weight for this term.\nThe learning problem is to minimize this objective function with regard to the parameters of \u0398, w, u, u and \u03c0,\nmin \u0398,w,u,v,\u03c0 O(\u0398,w,u,v,\u03c0),\ns.t. \u0398\u0398\u22a4 = Ir,\n0 \u2264 \u03c0 \u2264 \u03b41, and \u03c0\u22a41 = n1.\n(10)\nIn this minimization problem, we impose the constraint \u0398\u0398\u22a4 = Ir, where Ir is a r \u00d7 r identity matrix, so that the subspace transformation matrix \u0398 is orthogonal. Moreover, we impose a lower bound for \u03c0, 0 which is a vector of all zeros, and a upper bound of \u03c0, \u03b41, where 1 is a vector of all ones. We also propose an additional constraint to \u03c0, so that the summation of all the elements of \u03c0 is n1. The motive of this constraint is that for the original calculation of \u00b5s of (5), actually we set the weight of each data point to one, and the summation of the weights is n1. This makes it comparable to the scale of \u00b5t. To maintain this property when we use \u00b5 \u03c0 s to replace \u00b5s, we still requires that the summation of the weights in \u03c0 to be one.\n3.2 Solution\nIn this section, we discuss how to solve the minimization problem in (10), and design an iterative algorithm based on the solutions. To make the problem easier to solve, we rewrite the source domain and target domain classifiers as a linear function of the input feature vectors,\nf(xs) = w\u22a4xs + u\u22a4xs\n= \u03c6\u22a4xs, where\n\u03c6 = \u0398\u22a4w+ u, and\nh(xt) = w\u22a4\u0398xt + v\u22a4xt,\n= \u03d5\u22a4xt, where\n\u03d5 = \u0398\u22a4w+ v.\n(11)\nIn this way, we present the adaptation parameter vectors u and v as functions of \u0398, w, and \u03c6 or \u03d5,\nu = \u03c6\u2212\u0398\u22a4w, and v = \u03d5\u2212\u0398\u22a4w. (12)\nSubstituting (11) and (12) back to (8) and (10), we have the following minimization problem,\nmin \u0398,w,\u03c6,\u03d5,\u03c0\nn1 \u2211\ni=1\nL(ysi ,\u03c6 \u22a4xs)\u03c0i +\nn3 \u2211\nj=1\nL(yti ,\u03d5 \u22a4xt)\n+ C1\n2\n(\n\u2016\u03c6\u2212\u0398\u22a4w\u201622 + \u2016\u03d5\u2212\u0398 \u22a4w\u201622\n)\n+ C2\n\n \nn1 \u2211\ni=1\n\u2225 \u2225 \u2225 \u2225 \u2225 \u2225 \u03c0i \u2212 \u2211\nk\u2208N s i\n\u03c9sik\u03c0k\n\u2225 \u2225 \u2225 \u2225 \u2225 \u2225 2\n2\n+\nn2 \u2211\nj=1\n\u2225 \u2225 \u2225 \u2225 \u2225 \u2225 \u03d5\u22a4xtj \u2212 \u2211\nk\u2032\u2208N t j\n\u03c9tjk\u2032\u03d5 \u22a4xtk\u2032\n\u2225 \u2225 \u2225 \u2225 \u2225 \u2225 2\n2\n\n \n+ C3\n2\n\u2225 \u2225 \u2225 \u2225 \u2225 \u2225 1 n1 n1 \u2211\ni=1\n\u0398xsi\u03c0i \u2212 1\nn2\nn2 \u2211\nj=1\n\u0398xtj\n\u2225 \u2225 \u2225 \u2225 \u2225 \u2225 2\n2\n,\ns.t. \u0398\u0398\u22a4 = Ir ,\n0 \u2264 \u03c0 \u2264 \u03b41, and \u03c0\u22a41 = n1. (13)\nTo solve this problem, we use the iterative optimization method. We propose to update the parameters one by one in an iteration of an iterative algorithm. When on parameter is updated, the other parameters are fixed. In the following subsections, we will discuss how to update the parameters one by one."}, {"heading": "3.2.1 Updating \u0398 and w", "text": "When we update \u0398 andw, we fix the other parameters and obtain the following minimization problem,\nmin \u0398,w\nC1\n2\n(\n\u2016\u03c6\u2212\u0398\u22a4w\u201622 + \u2016\u03d5\u2212\u0398 \u22a4w\u201622\n)\n+ C3\n2\n\u2225 \u2225 \u2225 \u2225 \u2225 \u2225 1 n1 n1 \u2211\ni=1\n\u0398xsi\u03c0i \u2212 1\nn2\nn2 \u2211\nj=1\n\u0398xtj\n\u2225 \u2225 \u2225 \u2225 \u2225 \u2225 2\n2\n,\ns.t. \u0398\u0398\u22a4 = Ir.\n(14)\nPlease note that in the objective of this minimization problem, the terms which do not contains \u0398 and w have been removed. To minimize this objective with regard to w, we set is derivative with regard to w to zero, and obtain the optimal solution of w as follows,\nC2\u0398 [ (\u03c6\u2212\u0398\u22a4w) + (\u03d5\u2212\u0398\u22a4w) ] = 0\n\u21d2 C2\u0398 [ (\u03c6+\u03d5)\u2212 2\u0398\u22a4w ] = 0\n\u21d2 C2\u0398(\u03c6 +\u03d5) = C22w \u21d2 w = 1\n2 \u0398(\u03c6 +\u03d5).\n(15)\nSubstituting w of (15) back to (16), we have\nmin \u0398\nC1\n2\n(\n\u2225 \u2225 \u2225 \u2225 \u03c6\u2212 1\n2 \u0398\u22a4\u0398(\u03c6+\u03d5)\n\u2225 \u2225 \u2225 \u2225 2\n2\n+\n\u2225 \u2225 \u2225 \u2225 \u03d5\u2212 1\n2 \u0398\u22a4\u0398(\u03c6 +\u03d5)\n\u2225 \u2225 \u2225 \u2225 2\n2\n)\n+ C3\n2\n\u2225 \u2225 \u2225 \u2225 \u2225 \u2225 1 n1 n1 \u2211\ni=1\n\u0398xsi\u03c0i \u2212 1\nn2\nn2 \u2211\nj=1\n\u0398xtj\n\u2225 \u2225 \u2225 \u2225 \u2225 \u2225 2\n2\n,\n= C1\n2\n(\n\u2225 \u2225 \u2225 \u2225 \u03c6\u2212 1\n2 \u0398\u22a4\u0398(\u03c6 +\u03d5)\n\u2225 \u2225 \u2225 \u2225 2\n2\n+\n\u2225 \u2225 \u2225 \u2225 \u03d5\u2212 1\n2 \u0398\u22a4\u0398(\u03c6+\u03d5)\n\u2225 \u2225 \u2225 \u2225 2\n2\n)\n+ C3\n2\n\u2225 \u2225 \u2225 \u2225 \u2225 \u2225 1 n1 n1 \u2211\ni=1\n\u0398xsi\u03c0i \u2212 1\nn2\nn2 \u2211\nj=1\n\u0398xtj\n\u2225 \u2225 \u2225 \u2225 \u2225 \u2225 2\n2\n,\n= C1\n2\n[\n\u03c6\u22a4\u03c6+\u03d5\u22a4\u03d5\u2212 1\n2 Tr\n( \u0398(\u03c6 +\u03d5)(\u03c6+\u03d5)\u22a4\u0398\u22a4 )\n]\n+ C3\n2 Tr\n\n  \u0398\n\n\n1\nn1\nn1 \u2211\ni=1\nxsi\u03c0i \u2212 1\nn2\nn2 \u2211\nj=1\nxtj\n\n\n\n\n1\nn1\nn1 \u2211\ni=1\nxsi\u03c0i \u2212 1\nn2\nn2 \u2211\nj=1\nxtj\n\n\n\u22a4\n\u0398\u22a4\n\n  ,\n= C1\n2\n( \u03c6\u22a4\u03c6+\u03d5\u22a4\u03d5 ) + Tr(\u0398\u03a6\u0398\u22a4),\ns.t. \u0398\u0398\u22a4 = Ir, (16)\nwhere Tr(X) is the trace of a matrix X , and\n\u03a6 =\u2212 C1\n4 (\u03c6+\u03d5)(\u03c6+\u03d5)\u22a4\n+ C3\n2\n\n\n1\nn1\nn1 \u2211\ni=1\nxsi\u03c0i \u2212 1\nn2\nn2 \u2211\nj=1\nxtj\n\n\n\n\n1\nn1\nn1 \u2211\ni=1\nxsi\u03c0i \u2212 1\nn2\nn2 \u2211\nj=1\nxtj\n\n\n\u22a4\n.\n(17)\nThe first term of (16) is irrelevant to \u0398, thus we can remove it from the objective. The problem in (16) is further turned to\nmin \u0398\nTr(\u0398\u03a6\u0398\u22a4)\ns.t. \u0398\u0398\u22a4 = Ir . (18)\nTo solve this problem, we should decompose \u03a6 by the eigen-decomposition,\n\u03a6 = \u03a8\u039b\u03a8\u22a4 (19)\nwhere \u039b is the diagonal matrix of eigenvalues, and the rows of \u03a8 contain the rows of eigenvectors corresponding to the eigenvalues. We pick up the r rows corresponding to the largest r eigenvalues from \u03a8 , and the matrix of these r rows are the optimal solution of \u03a6."}, {"heading": "3.2.2 Updating \u03c6 and \u03d5", "text": "In this step, we fixe the other parameters and consider only \u03c6 and \u03d5. When other parameters are fixed and the terms which do not contain \u03c6 and \u03d5 are removed, the minimization problem in (13) is reduced to the following problem,\nmin \u03c6,\u03d5\nQ(\u03c6,\u03d5) = n1 \u2211\ni=1\nL(ysi ,\u03c6 \u22a4xs)\u03c0i +\nn3 \u2211\nj=1\nL(yti ,\u03d5 \u22a4xt)\n+ C1\n2\n(\n\u2016\u03c6\u2212\u0398\u22a4w\u201622 + \u2016\u03d5\u2212\u0398 \u22a4w\u201622\n)\n+ C2\nn2 \u2211\nj=1\n\u2225 \u2225 \u2225 \u2225 \u2225 \u2225 \u03d5\u22a4xtj \u2212 \u2211\nk\u2032\u2208N t j\n\u03c9tjk\u2032\u03d5 \u22a4xtk\u2032\n\u2225 \u2225 \u2225 \u2225 \u2225 \u2225 2\n2\n.\n(20)\nTo solve this problem, we use the coordinate descent algorithm. This algorithm can minimize a function of multiple variables. When one variable is updated, we update the variable toward the direction of the sub-gradient, while other variable are fixed. The sub-gradient functions of Q with regard to \u03c6 and \u03d5 are as follows,\n\u2207Q\u03c6 = n1 \u2211\ni=1\n\u2207L\u03c6(y s i ,\u03c6 \u22a4xs)\u03c0i + C1(\u03c6\u2212\u0398 \u22a4w), and\n\u2207Q\u03d5 = n3 \u2211\nj=1\n\u2207L\u03d5(y t i ,\u03d5 \u22a4xt) + C1(\u03d5\u2212\u0398 \u22a4w)\n+ 2C2\nn2 \u2211\nj=1\n\nxtj \u2212 \u2211\nk\u2032\u2208N t j\n\u03c9tjk\u2032x t k\u2032\n\n\n\nxtj \u2212 \u2211\nk\u2032\u2208N t j\n\u03c9tjk\u2032x t k\u2032\n\n\n\u22a4\n\u03d5.\n(21)\nThe updating rules are given as follows,\n\u03c6 \u2190 \u03c6\u2212 \u03c1\u2207Q\u03c6, and \u03d5 \u2190 \u03d5\u2212 \u03c1\u2207Q\u03d5, (22)\nwhere \u03c1 is the step of descent."}, {"heading": "3.2.3 Updating \u03c0", "text": "In this step, we update \u03c0 and fix other parameters. To this end, we have the following minimization problem,\nmin \u03c0\nn1 \u2211\ni=1\nL(ysi ,\u03c6 \u22a4 xs)\u03c0i + C2\nn1 \u2211\ni=1\n\u2225 \u2225 \u2225 \u2225 \u2225 \u2225 \u03c0i \u2212 \u2211\nk\u2208N s i\n\u03c9sik\u03c0k\n\u2225 \u2225 \u2225 \u2225 \u2225 \u2225 2\n2\n+ C3\n2\n\u2225 \u2225 \u2225 \u2225 \u2225 \u2225 1 n1 n1 \u2211\ni=1\n\u0398xsi\u03c0i \u2212 1\nn2\nn2 \u2211\nj=1\n\u0398xtj\n\u2225 \u2225 \u2225 \u2225 \u2225 \u2225 2\n2\n,\ns.t. 0 \u2264 \u03c0 \u2264 \u03b41, and \u03c0\u22a41 = n1.\n(23)\nThe first term of the objective can be rewritten as\nn1 \u2211\ni=1\nL(ysi ,\u03c6 \u22a4 xs)\u03c0i = \u03c4 \u22a4\u03c0,\nwhere \u03c4 = [\u03c41, \u00b7 \u00b7 \u00b7 , \u03c4n1 ] \u22a4, and \u03c4i = L(y s i ,\u03c6 \u22a4 xs).\n(24)\nThe second term of the objective can be rewritten as follows,\nC2\nn1 \u2211\ni=1\n\u2225 \u2225 \u2225 \u2225 \u2225 \u2225 \u03c0i \u2212 \u2211\nk\u2208N s i\n\u03c9sik\u03c0k\n\u2225 \u2225 \u2225 \u2225 \u2225 \u2225 2\n2\n= C2\nn1 \u2211\ni=1\n\u2225 \u2225a\u22a4i \u03c0 \u2212 \u03c9 \u22a4 i \u03c0 \u2225 \u2225 2\n2\n= C2\nn1 \u2211\ni=1\n\u2225 \u2225 \u2225 (ai \u2212 \u03c9i) \u22a4 \u03c0 \u2225 \u2225 \u2225 2\n2\n= C2\nn1 \u2211\ni=1\n\u03c0\u22a4 (ai \u2212 \u03c9i) (ai \u2212 \u03c9i) \u22a4 \u03c0\n= \u03c0\u22a4\n(\nC2\nn1 \u2211\ni=1\n(ai \u2212 \u03c9i) (ai \u2212 \u03c9i) \u22a4\n)\n\u03c0,\n(25)\nwhere ai \u2208 {1, 0}n1 and its i-th element is one, while the other elements are zeros. \u03c9i \u2208 Rn1 and its k-th element is defined as follows,\n\u03c9ik =\n{\n\u03c9ik, if k \u2208 N si 0, otherwise.\n(26)\nThe third term of the objective function is rewritten as follows,\nC3\n2\n\u2225 \u2225 \u2225 \u2225 \u2225 \u2225 1 n1 n1 \u2211\ni=1\n\u0398xsi\u03c0i \u2212 1\nn2\nn2 \u2211\nj=1\n\u0398xtj\n\u2225 \u2225 \u2225 \u2225 \u2225 \u2225 2\n2\n= C3\n2 \u2016\u0393\u03c0 \u2212 \u03d1\u20162 2\n= C3\n2 Tr\n( (\u0393\u03c0 \u2212 \u03d1)\u22a4(\u0393\u03c0 \u2212 \u03d1) )\n= C3\n2\n( \u03c0\u22a4\u0393\u22a4\u0393\u03c0 \u2212 2\u03d1\u22a4\u0393\u03c0 + \u03d1\u22a4\u03d1 ) ,\nwhere \u0393 =\n[\n1\nn1 \u0398xs1, \u00b7 \u00b7 \u00b7 ,\n1\nn1 \u0398xsn1\n]\n\u2208 Rr\u00d7n1 ,\nand \u03d1 = 1\nn2\nn2 \u2211\nj=1\n\u0398xtj \u2208 R n1 .\n(27)\nSubstituting (24), (25), and (27) back to (23), we have\nmin \u03c0\n\u03c4\u22a4\u03c0 + \u03c0\u22a4\n(\nC2\nn1 \u2211\ni=1\n(ai \u2212 \u03c9i) (ai \u2212 \u03c9i) \u22a4\n)\n\u03c0\n+ C3\n2\n( \u03c0\u22a4\u0393\u22a4\u0393\u03c0 \u2212 2\u03d1\u22a4\u0393\u03c0 + \u03d1\u22a4\u03d1 )\ns.t. 0 \u2264 \u03c0 \u2264 \u03b41, and \u03c0\u22a41 = n1.\n(28)\nThis is a quadratic programming problem with linear constraints. We can solve this problem by active set algorithm.\n3.3 Iterative algorithm\nThe iterative learning algorithm of our proposed method is summarized in Algorithm 1.\nAlgorithm 1 Iterative learning algorithm of shared subspace and partially shared classifiers (SSPSC).\nInput: Source and target domain training sets, {(xs1, y s 1), \u00b7 \u00b7 \u00b7 , (x s n1 , ys n1 )} and {(xt 1 , yt 1 ), \u00b7 \u00b7 \u00b7 , (xt n3 , yt n3 ),xt n3+1 , \u00b7 \u00b7 \u00b7 ,xt n2 }; Input: Weight factors of different terms C1, C2 and C3; Initialize \u03c6, \u03d5, and \u03c0; repeat\nUpdate \u0398 by solving (18); Update w by (15); Update \u03c6 and \u03d5 by (22); Update \u03c0 by solving (28);\nuntil Converge"}, {"heading": "4 Experimental Results", "text": "In this section, we conduct some experiments over five benchmark domain transfer learning data sets to evaluate the performances of the proposed method. We firstly test how the proposed method works with different values of term weights, C1, C2, and C3. Then we test the classification performance of the proposed method by comparing it to different transfer learning methods, both in term of classification accuracy and running time.\n4.1 Benchmark Data Sets\nIn the experiments, we use five benchmark data sets. They are listed as follows.\n\u2013 Travel destination review data set is a data set of reviews to several destinations of touring. In this data set, we have four Europe destinations of traveling, which are London, Rome, Paris, and Venice. For each destination, we collected 200 positive reviews and 200 negative reviews. We treat each travel destination as a domain, and each review as a data point. In the experiment, we randomly select one destination as a source domain, and select another destination as a target domain. To extract features from a review, we use the bag-of-words features. \u2013 20-Newsgroup corpus data set is a data set of newspaper documents. It contains documents of 20 classes. The classes are organized in a hierarchical structure. For a class, it usually have two or more sub-classes. For example, in the class of car, there are two sub-classes, which aremotorcycle and auto. To split this data set to source domain and target domain, for one class, we keep one sub-class in the source domain, while put the other sub-class to the target domain. We follow the splitting of source and target domain of NG14 data set of [2]. In this data set, there are 6 classes, and for each class, one sub-class is in the source domain, and another sub-class is in the target domain. For each domain, the number of data points is 2,400. The bag-of-word features of each document are used as original features. \u2013 Amazon review data set is a data set of reviews of products. It contains reviews of three types of products, which are books, DVD and Music. The reviews belongs to two classes, which are positive and negative. We treat the review of books as source domain, and that of DVD as target domain. For each domain, we have 2,000 positive reviews and 2,000 reviews. Again, we use the bag-of-words features as the features of reviews. \u2013 GEMEP-FERA face expression data set is a data of videos of faces. In this data set, we have the 87 face videos of 7 individuals. We treat the each individual as a domain, and each frame of the videos as a data point. We randomly select one individual as a source domain, and another individual as a target domain. The problem of classification is to classify a given frame to one of the 7 face expression classes. \u2013 Spam email data set is a set of emails of different individuals. In this data set, there are emails of three different individuals\u2019 inboxes, and we\ntreat each individual as a domain. In each individual\u2019s inbox, there are 2,500 emails, and the emails are classified to two different classes, which are normal email and spam email. we also randomly choose one individual as a source domain, and another one as a target domain.\n4.2 Experiment Process\nIn the experiments, we use the 10-fold cross validation. We set all the source domain data points labeled data points, and use all of them in the training process. Moreover, we split the target domain set to ten folds. Each fold is used as a test set, and the other folds are combined and used as a training set. For the training set, we randomly choose a half of the data points and set them as labeled data points, and leave the remaining half as unlabeled. We use the source domain training set and the target domain training set to train the parameters of our model using the proposed algorithm, and then apply the trained model to the test set and evaluate the classification performances. For the multi-class classification problem, we extend the proposed binary classification model to multi-class classification by the one-vs-all strategy. For the data set with more than two domains, we use each domain as a target domain in turns, and randomly choose anther domain as a source domain. The accuracies of over all the target domains are averaged and reported as the final results.\n4.3 Sensitivity to Term Weights\nWe study the sensitivity to the term weights of the objective function, C1, C2, and C3. As an example, we use the data set of travel destination reviews. The accuracies of the proposed algorithm with different values of C1, C2, and C3 are reported in Figure 1. From the figure, we can see that the proposed method is stable to weight C1. The highest accuracy is obtained when C1 is set to 10. For C2, the accuracies are also stable to the changes of the values. The accuracies are around 0.75. However, for C3, we have a clear trend that the accuracies are increasing with larger values of C3.\n4.4 Comparison to Other Transfer Learning Methods\nWe compare the proposed method to the methods listed in the section 1.1. The comparison is reported in terms of classification accuracy and running time."}, {"heading": "4.4.1 Classification Accuracy", "text": "The classification accuracies of the compared methods over five benchmark data sets are reported in Table 1. The proposed method outperforms all the\ncompared methods over four benchmark data sets. The only exception is the case of the face expression classification problem over GEMEP data set, where the method of Chu et al. [3] obtains slightly better performance than the proposed method. However, even in the experiments over GEMEP, the proposed method still has the second best performance. In the experiments over both Travel destination review data set and the 20-Newsgroup data set, the proposed method outperforms the other methods significantly."}, {"heading": "4.4.2 Running Time", "text": "The running time of the training process of the compared algorithms over different benchmark data sets are reported in Table 2. From the reported results, we can see that the proposed method has the least running time. Moreover, we can also see that the running time is also relevant to the size of the data set. For example, in the two smallest data set, Travel and GEMEP data sets, the running time is also shorter than the running time over other data sets."}, {"heading": "5 Conclusions", "text": "In this paper, we proposed a novel transfer learning method. The features of this work is listed as follows,\n\u2013 Instead of learning a shared representation and classifier directly for both source and target domains, we proposed to learn shared subspaces and classifier, and then adapt it to source and target domains.\n\u2013 Instead of using the source domain data points equally to estimate the distribution of the source domain, we proposed to weight the source domain data points in the subspaces to match the distributions of the two domains. \u2013 We also proposed to regularize the weighting factors of the source domain data points and the classification responses of the target domain data points by the local reconstruction coefficients.\nThe minimization problem of our method is based on these features, and we solve it by an iterative algorithm. Experiments show its advantages over some other methods."}, {"heading": "6 Future Works", "text": "In the future, we will study extending the proposed method to extremely large data sets, i.e., big data. We have two strategies to change the proposed algorithm to scale to big data sets. The first strategy is to parallelize the algorithm. The big data set can be split to many small sub-sets and the algorithm can be parallelized to process these sub-sets simultaneously. The second strategy is to use the stochastic optimization method by using the data points one by one, not using all of them sententiously. We also will extend the proposed algorithm to various applications, such as computational mechanic [24,29,15,17,26,30], multimedia[21,9,14,22,23], nanotechnology [10,11,13,12,1], etc. We will also consider use some other models to represent and construction the classifier, such as Bayesian network [6,4,5]."}, {"heading": "Acknowledgements", "text": "This research was supported by National Natural Science Foundation (71173062, 71203047), Key Program for Science and Technology Research of Heilongjiang Province (GB14D201) and University Academic Innovation Team Construction Plan of Philosophy and Social Sciences in Heilongjiang Province (TD201203)."}], "references": [{"title": "Dual-color fluorescence cross-correlation spectroscopy on a planar optofluidic chip", "author": ["A. Chen", "M. Eberle", "E. Lunt", "S. Liu", "K. Leake", "M. Rudenko", "A. Hawkins", "H. Schmidt"], "venue": "Lab on a Chip 11(8), 1502\u20131506", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2011}, {"title": "Discovering low-rank shared concept space for adapting text mining models", "author": ["B. Chen", "W. Lam", "I. Tsang", "T.L. Wong"], "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence 35(6), 1284\u20131297", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2013}, {"title": "Selective transfer machine for personalized facial action unit detection", "author": ["W.S. Chu", "F.D.L. Torre", "J.F. Cohn"], "venue": "Computer Vision and Pattern Recognition (CVPR), 2013 IEEE Conference on, pp. 3515\u20133522", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2013}, {"title": "Finding optimal bayesian network structures with constraints learned from data", "author": ["X. Fan", "B. Malone", "C. Yuan"], "venue": "Proceed. of the 30th Conf. on Uncertainty in Artificial Intelligence (UAI-2014)", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2014}, {"title": "An improved lower bound for bayesian network structure learning", "author": ["X. Fan", "C. Yuan"], "venue": "Proceedings of the 29th AAAI Conference on Artificial Intelligence (AAAI-2015)", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2015}, {"title": "Tightening bounds for bayesian network structure learning", "author": ["X. Fan", "C. Yuan", "B. Malone"], "venue": "Proceedings of the 28th AAAI Conference on Artificial Intelligence (AAAI-2014)", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2014}, {"title": "Transfer learning with reasonable boosting strategy", "author": ["L. La", "Q. Guo", "Q. Cao", "Y. Wang"], "venue": "Neural Computing and Applications 24(3-4), 807\u2013816", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2014}, {"title": "Learning with augmented features for supervised and semi-supervised heterogeneous domain adaptation", "author": ["W. Li", "L. Duan", "D. Xu", "I. Tsang"], "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence 36(6), 1134\u20131148", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2014}, {"title": "Multi-kernel learning for multivariate performance measures optimization", "author": ["F. Lin", "J. Wang", "N. Zhang", "J. Xiahou", "N. McDonald"], "venue": "Neural Computing and Applications pp. 1\u201313", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2016}, {"title": "Optofluidic devices with integrated solid-state nanopores", "author": ["S. Liu", "A.R. Hawkins", "H. Schmidt"], "venue": "Microchimica Acta 183(4), 1275\u20131287", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2016}, {"title": "Electro-optical detection of single \u03bb-dna", "author": ["S. Liu", "T.A. Wall", "D. Ozcelik", "J.W. Parks", "A.R. Hawkins", "H. Schmidt"], "venue": "Chemical Communications 51(11), 2084\u20132087", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2015}, {"title": "Effect of fabrication-dependent shape and composition of solid-state nanopores on single nanoparticle detection", "author": ["S. Liu", "T.D. Yuzvinsky", "H. Schmidt"], "venue": "ACS nano 7(6), 5621\u20135627", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2013}, {"title": "Correlated electrical and optical analysis of single nanoparticles and biomolecules on a nanoporegated optofluidic chip", "author": ["S. Liu", "Y. Zhao", "J.W. Parks", "D.W. Deamer", "A.R. Hawkins", "H. Schmidt"], "venue": "Nano letters 14(8), 4816\u20134820", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2014}, {"title": "Supervised learning of sparse context reconstruction coefficients for data representation and classification", "author": ["X. Liu", "J. Wang", "M. Yin", "B. Edwards", "P. Xu"], "venue": "Neural Computing and Applications pp. 1\u20139", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2015}, {"title": "Structure design of vascular stents", "author": ["Y. Liu", "J. Yang", "Y. Zhou", "J. Hu"], "venue": "Multiscale simulations and mechanics of biological materials pp. 301\u2013317", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2013}, {"title": "Knowledge adaptation with partiallyshared features for event detectionusing few exemplars", "author": ["Z. Ma", "Y. Yang", "N. Sebe", "A.G. Hauptmann"], "venue": "Pattern Analysis and Machine Intelligence, IEEE Transactions on 36(9), 1789\u20131802", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2014}, {"title": "Modeling nanoparticle targeting to a vascular surface in shear flow through diffusive particle dynamics", "author": ["B. Peng", "Y. Liu", "Y. Zhou", "L. Yang", "G. Zhang", "Y. Liu"], "venue": "Nanoscale research letters 10(1), 1\u20139", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2015}, {"title": "Transfer learning using the online fuzzy min-max neural network", "author": ["M. Seera", "C. Lim"], "venue": "Neural Computing and Applications pp. 1\u201312", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2013}, {"title": "Transfer learning using the online fuzzy minmax neural network", "author": ["M. Seera", "C. Lim"], "venue": "Neural Computing and Applications 25(2), 469\u2013480", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2014}, {"title": "Visual domain adaptation via transfer feature learning", "author": ["J. Tahmoresnezhad", "S. Hashemi"], "venue": "Knowledge and Information Systems pp. 1\u201321", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2016}, {"title": "An effective image representation method using kernel classification", "author": ["H. Wang", "J. Wang"], "venue": "Tools with Artificial Intelligence (ICTAI), 2014 IEEE 26th International Conference on, pp. 853\u2013858. IEEE", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2014}, {"title": "Multiple kernel multivariate performance learning using cutting plane algorithm", "author": ["J. Wang", "H. Wang", "Y. Zhou", "N. McDonald"], "venue": "Systems, Man, and Cybernetics (SMC), 2015 IEEE International Conference on, pp. 1870\u20131875. IEEE", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2015}, {"title": "Supervised cross-modal factor analysis for multiple modal data classification", "author": ["J. Wang", "Y. Zhou", "K. Duan", "J.J.Y. Wang", "H. Bensmail"], "venue": "Systems, Man, and Cybernetics (SMC), 2015 IEEE International Conference on, pp. 1882\u20131888. IEEE", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2015}, {"title": "Computational modeling of magnetic nanoparticle targeting to stent surface under high gradient field", "author": ["S. Wang", "Y. Zhou", "J. Tan", "J. Xu", "J. Yang", "Y. Liu"], "venue": "Computational mechanics 53(3), 403\u2013412", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2014}, {"title": "Feature space independent semi-supervised domain adaptation via kernel matching", "author": ["M. Xiao", "Y. Guo"], "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence 37(1), 54\u201366", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2015}, {"title": "Mechanical response of cardiovascular stents under vascular dynamic bending", "author": ["J. Xu", "J. Yang", "N. Huang", "C. Uhl", "Y. Zhou", "Y. Liu"], "venue": "Biomedical engineering online 15(1), 1", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2016}, {"title": "Robust non-negative matrix factorization via joint sparse and graph regularization for transfer learning", "author": ["S. Yang", "C. Hou", "C. Zhang", "Y. Wu"], "venue": "Neural Computing and Applications 23(2), 541\u2013559", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2013}, {"title": "A general framework for transfer sparse subspace learning", "author": ["S. Yang", "M. Lin", "C. Hou", "C. Zhang", "Y. Wu"], "venue": "Neural Computing and Applications 21(7), 1801\u20131817", "citeRegEx": "28", "shortCiteRegEx": null, "year": 2012}, {"title": "Biomarker binding on an antibody-functionalized biosensor surface: the influence of surface properties, electric field, and coating density", "author": ["Y. Zhou", "W. Hu", "B. Peng", "Y. Liu"], "venue": "The Journal of Physical Chemistry C 118(26), 14,586\u201314,594", "citeRegEx": "29", "shortCiteRegEx": null, "year": 2014}, {"title": "Mechanical properties of nanoworm assembled by dna and nanoparticle conjugates", "author": ["Y. Zhou", "S. Sohrabi", "J. Tan", "Y. Liu"], "venue": "Journal of Nanoscience and Nanotechnology 16(6), 5447\u20135456", "citeRegEx": "30", "shortCiteRegEx": null, "year": 2016}], "referenceMentions": [{"referenceID": 6, "context": "Transfer learning, or named as cross domain adaptation, aims to fill the gap between the two domains to provide sufficient data for the target domain learning problem [7,19,18,27,28, 20].", "startOffset": 167, "endOffset": 186}, {"referenceID": 18, "context": "Transfer learning, or named as cross domain adaptation, aims to fill the gap between the two domains to provide sufficient data for the target domain learning problem [7,19,18,27,28, 20].", "startOffset": 167, "endOffset": 186}, {"referenceID": 17, "context": "Transfer learning, or named as cross domain adaptation, aims to fill the gap between the two domains to provide sufficient data for the target domain learning problem [7,19,18,27,28, 20].", "startOffset": 167, "endOffset": 186}, {"referenceID": 26, "context": "Transfer learning, or named as cross domain adaptation, aims to fill the gap between the two domains to provide sufficient data for the target domain learning problem [7,19,18,27,28, 20].", "startOffset": 167, "endOffset": 186}, {"referenceID": 27, "context": "Transfer learning, or named as cross domain adaptation, aims to fill the gap between the two domains to provide sufficient data for the target domain learning problem [7,19,18,27,28, 20].", "startOffset": 167, "endOffset": 186}, {"referenceID": 19, "context": "Transfer learning, or named as cross domain adaptation, aims to fill the gap between the two domains to provide sufficient data for the target domain learning problem [7,19,18,27,28, 20].", "startOffset": 167, "endOffset": 186}, {"referenceID": 1, "context": "[2] proposed a transfer learning framework for the adaptation of text mining models which is based on low-rank shared concept space.", "startOffset": 0, "endOffset": 3}, {"referenceID": 2, "context": "[3] proposed a transductive learning model, which is called selective transfer machine, for the problem of face expression recognition.", "startOffset": 0, "endOffset": 3}, {"referenceID": 15, "context": "[16] proposed a transfer learning which can adapt knowledge from a source domain to a target domain, and can handle the features of the source and target domains which are partially different.", "startOffset": 0, "endOffset": 4}, {"referenceID": 24, "context": "\u2013 Xiao and Guo [25] proposed a transfer learning method to handle the domain adaptation problem with completed feature spaces for the source and target domains.", "startOffset": 15, "endOffset": 19}, {"referenceID": 7, "context": "[8] proposed method for the heterogeneous domain adaptation problem.", "startOffset": 0, "endOffset": 3}, {"referenceID": 1, "context": "[2] and Chu et al.", "startOffset": 0, "endOffset": 3}, {"referenceID": 2, "context": "[3] do not use the labels of the target domain data points.", "startOffset": 0, "endOffset": 3}, {"referenceID": 24, "context": "Among all the methods mentioned above, only the work of Xiao and Guo [25] uses the local connection information to regularize the learning of the classifiers.", "startOffset": 69, "endOffset": 73}, {"referenceID": 2, "context": "[3] select the source domain data points for the", "startOffset": 0, "endOffset": 3}, {"referenceID": 1, "context": "We follow the splitting of source and target domain of NG14 data set of [2].", "startOffset": 72, "endOffset": 75}, {"referenceID": 1, "context": "[2] 0.", "startOffset": 0, "endOffset": 3}, {"referenceID": 2, "context": "[3] 0.", "startOffset": 0, "endOffset": 3}, {"referenceID": 15, "context": "[16] 0.", "startOffset": 0, "endOffset": 4}, {"referenceID": 24, "context": "8012 Xiao and Guo [25] 0.", "startOffset": 18, "endOffset": 22}, {"referenceID": 7, "context": "[8] 0.", "startOffset": 0, "endOffset": 3}, {"referenceID": 1, "context": "[2] 28.", "startOffset": 0, "endOffset": 3}, {"referenceID": 2, "context": "[3] 18.", "startOffset": 0, "endOffset": 3}, {"referenceID": 15, "context": "[16] 21.", "startOffset": 0, "endOffset": 4}, {"referenceID": 24, "context": "18 Xiao and Guo [25] 19.", "startOffset": 16, "endOffset": 20}, {"referenceID": 7, "context": "[8] 35.", "startOffset": 0, "endOffset": 3}], "year": 2016, "abstractText": "Abstract Transfer learning is a problem defined over two domains. These two domains share the same feature space and class label space, but have significantly different distributions. One domain has sufficient labels, named as source domain, and the other domain has few labels, named as target domain. The problem is to learn a effective classifier for the target domain. In this paper, we propose a novel transfer learning method for this problem by learning a partially shared classifier for the target domain, and weighting the source domain data points. We learn some shared subspaces for both the data points of the two domains, and a shared classifier in the shared subspaces. We hope that in the shared subspaces, the distributions of two domain can match each other well, and to match the distributions, we weight the source domain data points with different weighting factors. Moreover, we adapt the shared classifier to each domain by learning different adaptation functions. To learn the subspace transformation matrices, the classifier parameters, and the adaptation parameters, we build a objective function with weighted classification errors, parameter regularization, local reconstruction regularization, and distribution matching. This objective function is minimized by an iterative algorithm. Experiments show its effectiveness over benchmark data sets, including travel destination review data set, face expression data set, spam email data set, etc.", "creator": "LaTeX with hyperref package"}}}