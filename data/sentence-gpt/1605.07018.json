{"id": "1605.07018", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "23-May-2016", "title": "Online Learning with Feedback Graphs Without the Graphs", "abstract": "We study an online learning framework introduced by Mannor and Shamir (2011) in which the feedback is specified by a graph, in a setting where the graph may vary from round to round and is \\emph{never fully revealed} to the learner. We show a large gap between the adversarial and the stochastic cases. In the adversarial case, we prove that even for dense feedback graphs, the learner cannot improve upon a trivial regret bound obtained by ignoring any additional feedback besides her own loss of interest.\n\n\n\nOur model illustrates how the decision-making processes are structured and used to predict our behaviour:", "histories": [["v1", "Mon, 23 May 2016 14:07:43 GMT  (42kb)", "http://arxiv.org/abs/1605.07018v1", null]], "reviews": [], "SUBJECTS": "cs.LG stat.ML", "authors": ["alon cohen", "tamir hazan", "tomer koren"], "accepted": true, "id": "1605.07018"}, "pdf": {"name": "1605.07018.pdf", "metadata": {"source": "CRF", "title": "Online Learning with Feedback Graphs Without the Graphs", "authors": ["Alon Cohen", "Tamir Hazan", "Tomer Koren"], "emails": ["alon.cohen@technion.ac.il", "tamir.hazan@technion.ac.il", "tomerk@technion.ac.il"], "sections": [{"heading": null, "text": "ar X\niv :1\n60 5.\n07 01\n8v 1\n[ cs\n.L G\n] 2\n3 M\nay 2\n01 6\n? \u03b1T q regret over T rounds, provided that the\nindependence numbers of the hidden feedback graphs are at most \u03b1. We also extend our results to a more general feedback model, in which the learner does not necessarily observe her own loss, and show that, even in simple cases, concealing the feedback graphs might render a learnable problem unlearnable."}, {"heading": "1 Introduction", "text": "Online learning is a general framework for sequential decision-making under uncertainty. In its most basic form, it can be described as follows. A learner has to iteratively choose an action from a set of K available actions, and suffer a loss associated with that action. The losses of the actions on each round are assigned in advance by an arbitrary, possibly adversarial, environment. The learner\u2019s goal is to minimize her regret over T rounds of the game, which is the difference between her cumulative loss and that of the best fixed action in hindsight.\nAfter making each decision, the learner receives some form of feedback about the losses. Traditionally, the literature considers two types of feedback: full feedback (Littlestone and Warmuth, 1994; Vovk, 1990; Cesa-Bianchi et al., 1997), where the learner observes the losses associated with all of her possible actions, and bandit feedback Auer et al. (2002b), where the learner only observes the loss of the action she has actually taken.\nFull feedback and bandit feedback are special cases of a general framework introduced by Mannor and Shamir (2011), in which the feedback model is specified by a sequence G1, . . . , GT of feedback graphs, one for each round t of the game. Each feedback graph Gt is a directed graph whose nodes correspond to the learner\u2019s K possible actions; a directed edge u \u00d1 v in this graph indicates that whenever the learner chooses action u on round t, in addition to observing the loss of action u, she also gets to observe the loss associated with the action v on that round.\nOnline learning with feedback graphs was further studied by several authors. Alon et al. (2013), and subsequently Koca\u0301k et al. (2014); Alon et al. (2015), gave regret-minimization algorithms that achieve rOp ? \u03b1T q regret, where \u03b1 is a bound on the independence numbers of the graphs G1, . . . , GT . Up to logarithmic factors, their results recover and interpolate between the classic bounds of Op ? T logKq with full feedback Freund and Schapire (1997) and Op ? KT q with bandit feedback Auer et al. (2002b); Audibert and Bubeck (2009). The rOp ? \u03b1T q bound turns out to be tight for any feedback graph (when it is fixed throughout the game and known in advance), in light of a matching lower bound due to Mannor and Shamir (2011).\nHowever, all of the optimal algorithms mentioned above require the full structure of the feedback graph in order to operate. While some require the entire graph Gt for performing their updates only at the end of round t (e.g., Alon et al., 2013; Koca\u0301k et al., 2014; Alon et al., 2015),1 others actually need the description of Gt at the beginning of the round before making their decision (e.g., Alon et al., 2014). In fact, none of the algorithms previously proposed in the literature is able to provide non-trivial regret guarantees without the feedback graphs being disclosed.\nThe assumption that the entire observation system is revealed to the learner on each round, even if only after making her prediction, is rather unnatural. In principle, the learner need not be even aware of the fact that there is a graph underlying the feedback model; the feedback graph is merely a technical notion for us to specify a set of observations for each of the possible actions. Ideally, the only signal we would like the learner to receive following each round is the set of observations that corresponds to the action she has taken on that round (in addition to her own loss).\nAs a motivating example for situations where receiving the entire observation system is unrealistic, consider the following online pricing problem that faces any vendor selling goods over the internet. On each round, the seller has to announce a price for his product. Then, a buyer arrives and decides whether or not to purchase the product at this price based on his private value; the only feedback the seller receives is whether or not the buyer purchased the product at the announced price. However, when a purchase takes place, the seller also knows that the buyer would have bought the product at any price lower than the price that she announced. While this feedback structure can be thought of as a directed graph over the seller\u2019s actions (i.e., prices), the graph itself is never fully revealed to the seller as its structure discloses the buyer\u2019s private value."}, {"heading": "1.1 Our contributions", "text": "In this paper, we study online learning with feedback graphs in a setting where the feedback graphs are never revealed to the learner in their entirety. That is, in this setting the only feedback available to the learner at the end of round t is the out-neighborhood of her chosen action in the graph Gt, along with the loss associated with each of the actions in this neighborhood and the loss of the action that she chose. We address the following questions: how this lack of full disclosure affects the learner\u2019s regret? Is it possible to achieve any non-trivial regret guarantee in this setting, i.e., one that improves on the trivial Op ? KT q bound? In particular, can we obtain bounds that scale with the independence numbers of the feedback graphs? Our main results show that not knowing the entire feedback graphs can have a significant impact on the learner\u2019s achievable regret. First, we show that in a standard adversarial online learning setting, where we assume nothing about the process generating the losses and the feedback graphs (i.e., both are possibly chosen by an adversary), any strategy of the learner must suffer \u2126p ? KT q regret in the worst case, even if the independence numbers of G1, . . . , GT are all bounded by some small absolute constant. Namely, by hiding the feedback graphs from the learner, the problem surprisingly becomes as hard as the K-armed bandit problem, even when the feedback available to the learner is \u201calmost full\u201d: each of the feedback graphs is \u201calmost a clique.\u201d In other words, the side observations received by the learner are effectively useless; she may as well ignore them and use a standard bandit algorithm such as Exp3 Auer et al. (2002b) to perform optimally.\nSecond, and in contrast to the adversarial setting, we show that in a stochastic setting where the losses of each action are known to be drawn i.i.d. from some unknown probability distribution, side observations can still be very useful. We show that the learner is able to\n1More precisely, these algorithms do not need the entire graph but rather the incoming neighborhood of each\nof the actions for which the associated loss has been observed.\nachieve an optimal regret bound of the form rOp ? \u03b1T q, even if the graphs G1, . . . , GT are chosen adversarially and are never fully revealed to the learner, as long as their independence numbers are all bounded by \u03b1. We give an efficient elimination-based algorithm achieving this bound, that does not require knowing the value of \u03b1 in advance. This result is optimal up to logarithmic factors, even when the feedback graph is fixed throughout the game and known in advance, due to a lower bound of Mannor and Shamir (2011).\nFor our algorithm in the stochastic case, we also prove a distribution-dependent regret bound that scales logarithmically with T . The bound we prove is of the form Op\u0159vPV 1p1{\u2206vq log T q, where \u2206v is the gap of action v, and V\n1 is the subset of rOp\u03b1q actions with smallest gaps. This bound is a substantial improvement over standard regret bounds of stochastic multi-armed bandit algorithms such as UCB Auer et al. (2002a): whereas the regret of the latter algorithms is typically bounded by a sum \u0159 vPV p1{\u2206vq taken over all K actions, the sum in our bound is taken only over the subset of rOp\u03b1q actions with the smallest gaps. Again, this result cannot be improved even when the feedback graph is fixed throughout the game, and has an optimal dependence on \u03b1 as well as on the gaps \u2206v, thus resolving an open question of Alon et al. (2014).\nFinally, we extend our results to a more general feedback model recently studied by Alon et al. (2015), in which the learner does not necessarily observe her own loss after making predictions (namely, each action may or may not have a self-loop in each feedback graph). Alon et al. (2015) gave a necessary and sufficient condition for attaining \u0398p ? T q regret in this more general model\u2014a graph-theoretic condition they call strong observability. The extension of our results to their model bears some surprising consequences: for example, even in the strongly observable case with only two actions, not revealing the entire feedback graphs to the learner might make the problem unlearnable! Nevertheless, in the case of stochastic losses, our positive results do extend to the more general feedback model."}, {"heading": "1.2 Additional related work", "text": "Online learning with feedback graphs was previously considered in the stochastic setting by Caron et al. (2012), who gave results depending on the graph clique structure. Their analysis, however, only applies when the feedback graph is fixed throughout the game, and can only bound the regret in terms of a quantity akin to the clique-partition number of this graph, which is always larger than its independence number (the gap between the two can be very large; see Alon et al., 2014).\nMore recently, Wu et al. (2015) and Koca\u0301k et al. (2016) have investigated a noisy version of the feedback graph model, where feedback is specified by a weighted directed graph with edge weights indicating the quality (e.g., the noise level or variance) of the feedback received on adjacent vertices. Wu et al. (2015) provided finite-time problem-dependent lower bounds for this setting; Koca\u0301k et al. (2016) generalized the notion of independence number to the noisy case and gave new efficient algorithms in this setting."}, {"heading": "2 Setup and Main Results", "text": "We consider a general online learning model with graph-structured feedback, which can be described as a game between a learner and an environment that proceeds for T rounds. Before the game begins, the environment privately determines a sequence of loss functions \u21131, ..., \u2113T : V \u00de\u00d1 r0, 1s defined over a set V \u201c t1, ...,Ku of K actions, which we view as a sequence of loss vectors \u21131, ..., \u2113T P r0, 1sK . In addition, the environment fixes a sequence of directed graphs G1, . . . , GT over V as vertices.\nWe will consider two different cases, that we refer to as the adversarial setting and the stochastic setting:\n\u2022 In the adversarial setting, the loss vectors \u21131, ..., \u2113T and the feedback graphs G1, . . . , GT are chosen by the environment in an arbitrary way.\n\u2022 In the stochastic setting, the environment privately selects a loss distribution D over r0, 1sK and an arbitrary sequence G1, . . . , GT ; thereafter, the loss vectors \u21131, . . . , \u2113T are sampled i.i.d. from D. An important property of this setting is that the loss vectors are statistically independent from the feedback graphs.\nIteratively on rounds t \u201c 1, 2, ..., T , the learner randomly chooses an action vt P V and incurs the loss \u2113tpvtq. At the end of each round t, the learner receives a feedback comprised of tpv, \u2113tpvqq : pvt \u00d1 vq P Gtqu, that includes the loss \u2113tpvtq incurred by the learner (i.e., we assume that pv \u00d1 vq P Gt for all t and v P V ). In words, the learner observes the losses associated with vt and the actions in the out-neighborhood of vt in the feedback graph Gt. The feedback graph Gt itself is never revealed in its entirety to the learner.\nThe goal of the learner throughout the T rounds of the game is to minimize her expected regret, which is defined as\nRT \u201c E \u00ab T\u00ff\nt\u201c1\n\u2113tpvtq \u00b4 T\u00ff\nt\u201c1\n\u2113tpv\u2039q ff , (1)\nwhere v\u2039 \u201c minvPV Er \u0159T\nt\u201c1 \u2113tpvqs is the best action in hindsight. Here, the expectations are taken over the random choices of the learner and, in the stochastic setting, also over the randomness of the losses.\nFor the stochastic setting we require additional notation. For each v P V , we denote by \u00b5pvq the mean of the loss of action v under D. We denote \u00b5\u2039 \u201c \u00b5pv\u2039q, and let \u2206v \u201c \u00b5pvq \u00b4 \u00b5\u2039 for all v P V . We refer to \u2206v as the gap of action v, and assume for simplicity that v\u2039 is unique so that \u2206v \u0105 0 for all v \u2030 v\u2039.\nFor stating our results, we need a standard graph-theoretic definition. An independent set in a graph G \u201c pV,Eq (either directed or undirected) is a set of vertices that are not connected by any edges. Namely, S \u010e V is independent if for any u, v P S, u \u2030 v, it holds that pu \u00d1 vq R E. The independence number \u03b1pGq of G is the size of the largest independent set in G."}, {"heading": "2.1 Main results", "text": "We now state the main results of this paper. Our first result deals with the adversarial case and shows that when the feedback graphs are not revealed to the learner at the end of each round, her regret might be very large even when the independence numbers of the graphs are small\u2014they are all bounded by a constant. Theorem 1. In the adversarial setting, any online learning algorithm must suffer at least \u2126p ? KT q regret in the worst case, even when all feedback graphs G1, . . . , GT have independence numbers \u010f Op1q.\nThe lower bound in the theorem is tight: it can be matched by simply running a standard bandit algorithm (e.g., Exp3, Auer et al., 2002b), ignoring all observed feedback besides the loss of the action played.\nOur next result shows that in the stochastic case, the learner is still able to attain non-trivial regret despite the fact that the feedback graphs are never fully revealed to her.\nTheorem 2. In the stochastic setting, Algorithm 1 described in Section 4 attains an expected regret of at most rOp ? \u03b1T q, provided that the independence numbers of the graphs G1, . . . , GT are all bounded by \u03b1.\nThis regret bound is optimal up to logarithmic factors, since the lower bound of \u2126p ? \u03b1T q found in Mannor and Shamir (2011) applies in our stochastic setting. In the stochastic setting we also give a distribution-dependent analysis of Algorithm 1 which depends on the gaps of the actions under the distribution D.\nTheorem 3. In the stochastic setting, Algorithm 1 described in Section 4 attains an expected regret of\nO\n\u02dc \u00ff\nvPV 1\n1\n\u2206v log T\n\u00b8 ,\nwhere V 1 is the set of rOp\u03b1q actions with the smallest gaps (excluding v\u2039), provided that the the independence numbers of the graphs G1, . . . , GT are all bounded by \u03b1.\nWe also extend our results to a more general class of feedback graphs, in which each vertex may or may not have a self-loop. For the statements of these additional results, see Section 5."}, {"heading": "2.2 Discussion of the results", "text": "Our results show that there is a large gap between the achievable regret rates in the adversarial and stochastic settings, in terms of the dependence on the properties of the feedback graphs.\nIn the adversarial case, the environment is free to simultaneously choose the sequences of loss values and feedback graphs in conjunction with each other; for example, they can be drawn from a joint distribution over sequences of loss values and sequences of directed graphs. The environment may use this freedom to manipulate the feedback observed by the learner and bias her observations in a malicious way. In the stochastic setting, on the other hand, the loss values are drawn from the underlying distribution only after the environment commits to some arbitrary sequence of graphs, so that the feedback graphs are probabilistically independent of the realizations of the losses.\nIn fact, as our arguments in Section 3 reveal, there exists a randomized construction of loss vectors and feedback graphs that inflicts \u2126p ? KT q on any learner, in which the loss vectors are i.i.d. However, the stochastic process that generates the feedback graphs in that construction is correlated with the actual realizations of the i.i.d. losses. This is a crucial aspect of our construction, as implied by our upper bound in the stochastic case."}, {"heading": "3 Lower Bound for Adversarial Losses", "text": "In this section we deal with the adversarial setting and prove Theorem 1: we show an \u2126p ? KT q lower bound on the performance of any online learning algorithm, where both the losses of the actions and the feedback graphs can be chosen arbitrarily.\nLet us sketch the idea behind the lower bound, and defer the formal details to Section 6. By Yao\u2019s minimax principle, in order to prove a lower bound on the learner\u2019s regret it is enough to demonstrate a randomized strategy for the environment that forces any deterministic learner to incur \u2126p ? KT q regret. We construct our environment\u2019s strategy as follows.\nFirst, before the game begins, the environment chooses an action v\u2039 uniformly at random from V . At each round, the loss of all actions v \u2030 v\u2039 is distributed Bernoulli(1{2), while the loss of action v\u2039 is distributed Bernoulli(1{2\u00b4 \u01eb) with \u01eb \u201c p1{8q a K{T . All of the loss values in the construction are drawn independently of each other. The feedback graphs G1, . . . , GT are chosen i.i.d. from the following distribution. Any edge u \u00d1 v for v \u2030 v\u2039 appears with probability 1 \u00b4 2\u01eb independently from all other edges and the losses of the actions. Edges of the form u \u00d1 v\u2039 appear mutually independently given the loss of action v\u2039: if the loss of v\u2039 is 1, each edge appears with probability 1; if the loss of v\u2039 is 0, each edge appears with probability p1 \u00b4 2\u01ebq{p1 ` 2\u01ebq. See Figure 1 for a summary of the edge probabilities in this construction.\nThe idea behind the construction is as following. Suppose that the learner plays some action u \u2030 v\u2039, the distributions of the observed losses of every other actions are identical, including that of v\u2039. In other words, her only option of finding v\u2039 is by sampling it directly and observing its loss. Hence, the construction is capable of simulating a K-armed bandit problem whose minimax regret is \u2126p ? KT q.\nFor the construction above, we prove the following theorem. The proof itself is deferred to Section 6.4.\nTheorem 4. Assume that K \u011b 2 and T \u011b K2. Any deterministic learner must suffer an expected regret of at least p1{32q ? KT against the environment constructed above.\nTo show that Theorem 1 holds, we need to show that the learner suffers a large regret against an environment that selects feedback graphs with constant independence numbers. While the independence numbers of the graphs that we have constructed might, in principle, be large, we can show that with very high probability they are uniformly bounded by a constant.\nLemma 5. Suppose that |V | \u201c K \u011b 2 and T \u011b K2. Let G1, ..., GT be a sequence of graphs as constructed above. With probability at least 1\u00b4 \u01eb{8, the independence numbers of all graphs are at most 9.\nTheorem 1 now follows by combining Theorem 4 and Lemma 5; for the technical details, see Section 6.5."}, {"heading": "4 Algorithms for Stochastic Losses", "text": "In this section we present and analyze our algorithm for the stochastic setting. The algorithm, given in Algorithm 1, is reminiscent of elimination-based algorithms for the stochastic multiarmed bandit problem (e.g., Even-Dar et al., 2002; Karnin et al., 2013). For this algorithm, we prove the following guarantee on the expected regret, which implies Theorem 2.\nTheorem 6. Assume that K \u011b 2. Suppose that Algorithm 1 is run on a sequence of feedback graphs with independence numbers \u010f \u03b1. Then the expected regret of the algorithm is at most rOp ? \u03b1T q.\nAlgorithm 1 works in phases r \u201c 1, 2, . . .. It maintains a subset of actions Vr, where initially V1 \u201c V . At each phase r, the algorithm estimates the mean losses of all actions in Vr to within \u01ebr accuracy, by invoking a procedure called AlphaSample nr times. It then filters out from Vr the actions that are known to be 2\u01ebr-suboptimal with sufficient confidence, and repeats this process, decreasing the accuracy parameter \u01ebr after each phase.\nThe key for achieving optimal regret lies in the the procedure AlphaSample, that appears as Algorithm 2. Each call to this procedure allows us to observe the losses of all actions in Vr\nAlgorithm 1\ninput Set V of K actions, number of rounds T initialize r \u00d0 1, V1 \u201c V , \u01eb1 \u201c 1{4 while |Vr| \u0105 1 and T rounds have not elapsed do\nSet nr \u201c r2 logp2KT q{\u01eb2rs Invoke AlphaSample(Vr) for nr times, and\ncompute empirical mean mrpvq of each action v P Vr using collected samples\nCompute m\u2039r \u201c minvPVr mrpvq Eliminate actions:\nVr`1 \u201c tv P Vr : mrpvq \u010f m\u2039r ` 2\u01ebru Set \u01ebr`1 \u201c \u01ebr{2, r \u00d0 r ` 1\nend while Play the action left in Vr until T rounds have passed\nonce, while spending only rOp\u03b1q rounds in expectation. The exact details of AlphaSample are discussed in Section 4.2 below, and here we just state its guarantee.\nLemma 7. AlphaSample returns one sample of the loss of each action in Vr and terminates after at most 10\u03b1 logK rounds of the game in expectation, provided that the independence numbers of all feedback graphs G1, . . . , GT are at most \u03b1.\nTo prove Theorem 6 we need one additional lemma. It shows that, at each phase, the elimination procedure of the algorithm succeeds with high probability. Namely, after phase r, the algorithm is left with actions that are at most 4\u01ebr-suboptimal.\nLemma 8. For all r, with probability at least 1\u00b4 1{T we have \u00b5pvq \u010f \u00b5\u2039` 4\u01ebr for all v P Vr`1.\nWe can now proceed with the proof of the theorem.\nProof of Theorem 6. Let us start by bounding the number of phases R the algorithm makes. Let the random variable Tr denote the number of game rounds elapsed during phase r. Since the algorithm runs for T rounds we must have that\nR\u00ff\nr\u201c1\nTr \u010f T . (2)\nIn particular, since AlphaSample takes at least one round to complete, we have that Tr \u011b nr \u011b 2 logp2KT q4r`1 and we get the crude bound of\nR \u010f r\u0304 \u201c 1 2 log2\n\u02c6 3T 32 logp2KT q ` 1 \u02d9 . (3)\nWe turn to bound the expected regret of the algorithm. By Lemma 8 and the union bound, the total probability of failure of the mean estimations is at most r\u0304{T . Then the expected regret of the algorithm is at most the expected regret conditioned on the success of the estimation of the means plus pr\u0304{T q \u00a8 T \u201c r\u0304 \u201c Oplog T q by Eq. (3), and since the regret is bounded by T with probability 1. Thus it remains to bound the regret conditioned on the success of the mean estimations.\nFor convenience, define \u01eb0 \u201c 1{2. On phase r, by Lemma 8 we have an instantaneous regret of at most 4\u01ebr\u00b41 \u201c 8\u01ebr per round. If only one action is left in Vr then it must be v\u2039 and therefore after the final phase the algorithm suffers zero instantaneous expected regret. Overall,\nthe expected regret is at most,\nE\n\u00ab R\u00ff\nr\u201c1\nTr \u00a8 8\u01ebr ff \u010f 8 gffeE \u00ab R\u00ff\nr\u201c1\nTr\nff \u00a8 gffeE \u00ab R\u00ff\nr\u201c1\nTr\u01eb2r\nff\nby the Cauchy-Schwartz inequality. Note that \u0159R\nr\u201c1 Tr \u010f T by Eq. (2). Additionally, by Lemma 7 each call to AlphaSample spends at most m \u201c 10\u03b1 logK rounds in expectation and thus ErTrs \u010f mnr. Hence,\nE\n\u00ab R\u00ff\nr\u201c1\nTr\u01eb 2 r\nff \u010f r\u0304\u00ff\nr\u201c1\nmnr\u01eb 2 r \u010f mr\u0304p2 logp2KT q ` 1q .\nThe first inequality holds since the number of phases is at most r\u0304. The right-hand side is Op\u03b1 logpKq log2pKT qq by Eq. (3) and the definition of m."}, {"heading": "4.1 Gap-based analysis", "text": "We can also provide a distribution-dependent analysis of Algorithm 1 that yields a logarithmic regret bound, albeit with an explicit dependence on the gaps \u2206v.\nDenote by V pnq the set of n actions with smallest gaps, excluding v\u2039 and breaking ties arbitrarily.2 Our main result in this section is the following theorem, which gives Theorem 3. Recall that we assume v\u2039 is the unique optimal action, and so the gaps of all other actions are positive.\nTheorem 9. Suppose that K \u011b 2 and T \u011b K, and let \u03c4 \u201c r10\u03b1 logKs. Suppose that Algorithm 1 is run on a sequence of feedback graphs with independence numbers \u010f \u03b1. Then the expected regret of Algorithm 1 is at most\nO\n\u02dc \u00ff\nvPV p\u03c4q\n1\n\u2206v log T\n\u00b8 .\nWe can explain the intuition behind the bound as follows. Each call to AlphaSample spends at most rOp\u03b1q rounds while producing samples of all K actions. Thus, in the worst case, after a quick pruning phase the algorithm is left with the \u201chardest\u201d \u03c4 \u201c rOp\u03b1q actions and has to tell them apart; in this last phase, the additional observations provided by the feedback graphs might not help the algorithm at all (e.g., the remaining \u03c4 actions might form an independent set in all graphs). Let us turn to the proof of the theorem.\nProof of Theorem 9. As in the proof of Theorem 6, we have that the expected regret of the algorithm is at most the expected regret conditioned on the success of the mean estimations plus Oplog T q, and thus it remains to bound the regret conditioned on the success of the mean estimations.\nConditioned on the success of the algorithm, the regret of the algorithm is at most the regret of an algorithm that has finished running with Vr \u201c tv\u2039u. Thus we can assume that T is large enough for that to happen.\nIf \u03c4 \u010f K \u00b4 1, we begin by bounding the regret until the algorithm eliminates all actions besides the ones in V p\u03c4q. Let \u2206\u0304 be the largest gap of an action from V p\u03c4q. Let r\u0304 \u201c tlog2p2{\u2206\u0304qu. Thus, it takes r\u0304 ` 1 phases in order for \u01ebr to be less than \u2206\u0304{4. The regret up to round r\u0304 is bounded using the following lemma.\n2If n \u0105 K \u00b4 1, we simply take V pnq to be the set of all actions besides v\u2039.\nLemma 10. Let m \u201c 10\u03b1 logK. The expected regret of Algorithm 1 up to round r\u0304 is at most 128m\n\u2206\u0304 logp2KT q .\nWe proceed with the analysis of the expected regret after phase r\u0304. This is given by this next lemma.\nLemma 11. The expected regret of Algorithm 1 from round r\u0304 ` 1 until the end of the game is at most \u00ff\nvPV p\u03c4q\n128 \u2206v logp2KT q .\nIf \u03c4 \u0105 K\u00b41 then the regret of the algorithm is given by Lemma 11. Otherwise, the proof of the theorem is completed by noticing that the regret of the algorithm up to round r\u0304 is at most the regret from round r\u0304 ` 1 thereafter. Since \u2206\u0304 \u011b \u2206v for all v P V p\u03c4q we get that\nm \u2206\u0304 \u010f m|V p\u03c4q|\n\u00ff\nvPV p\u03c4q\n1\n\u2206v \u010f\n\u00ff\nvPV p\u03c4q\n1\n\u2206v ,\nby definition of m and V p\u03c4q. This in total gives a regret bound of Op\u0159vPV p\u03c4qp1{\u2206vq logpKT qq. Finally, we use our assumption that T \u011b K to simplify the bound.\nProof of Lemma 10. By Lemma 7, each call to AlphaSample spends at most m rounds in expectation. By Lemma 8, the instantaneous regret for each round on phase r is at most 4\u01ebr\u00b41 \u201c 8\u01ebr. Then the expected regret up to round r\u0304 is at most\nr\u0304\u00ff\nr\u201c1\nm \u00a8 nr \u00a8 8\u01ebr \u010f 32m logp2KT q r\u0304\u00ff\nr\u201c1\n1 \u01ebr ,\nand we have r\u0304\u00ff\nr\u201c1\n1 \u01ebr \u201c\nr\u0304\u00ff\nr\u201c1\n2r`1 \u010f 2r\u0304`2 \u010f 4 \u2206\u0304 .\nProof of Lemma 11. Let us denote r\u0304v \u201c tlog2p2{\u2206vqu, the number of phases until v is removed from Vr. Let w be the action with the minimum nonzero gap. We shall assume that the game is finished after r\u0304w phases.\nNote that after we have eliminated all actions not in V p\u03c4q, each call to AlphaSample is finished after at most |Vr| steps. Thus, the expected regret for the remaining phases is at most\nr\u0304w\u00ff\nr\u201cr\u0304`1\n32 logp2KT q \u01ebr |Vr| \u201c 32 logp2KT q \u00ff\nvPV p\u03c4q\nr\u0304v\u00ff\nr\u201cr\u0304`1\n1 \u01ebr ,\nand, for all v P V p\u03c4q, r\u0304v\u00ff\nr\u201cr\u0304`1\n1 \u01ebr \u010f\nr\u0304v\u00ff\nr\u201c0\n2r`1 \u201c 2r\u0304v`2 \u010f 4 \u2206v ."}, {"heading": "4.2 Efficient sampling scheme", "text": "In this section, we discuss the AlphaSample randomized sampling procedure. This procedure allows us to collect one sample of the loss for each action while spending only rOp\u03b1q rounds in expectation. AlphaSample is described in Algorithm 2.\nLet us now explain the intuition behind the procedure. At each round, the procedure samples the loss of an action uniformly at random from a subset of actions U . As each sample is uniform over U , the procedure observes the losses of \u2126p|U |{\u03b1q actions in expectation. The actions that\nAlgorithm 2 AlphaSample\ninput Set of actions U \u010e V initialize S \u00d0 H while |U | \u0105 0 do\nPlay an action u P U uniformly at random, and let W puq be the set of actions observed Collect samples of losses of each w P W puq into S Update U \u00d0 UzW puq\nend while return S\nhave been observed are then removed from U and the process continues recursively until U is empty. This phase is complete after an expected rOp\u03b1q rounds.\nThe main result regarding AlphaSample is the following theorem, from which Lemma 7 would follow immediately (see Section 6.2).\nTheorem 12. Algorithm 2 returns one sample of the loss of each action in U and terminates after at most 4\u03b1 logpK{\u03b4q rounds with probability at least 1\u00b4\u03b4, provided that all feedback graphs G1, . . . , GT have independence numbers \u010f \u03b1.\nTo analyze the number of rounds that the algorithm spends, we shall define the following random process. Consider an infinite sequence U1, U2, ... such that U1 \u201c U . For every r \u0105 0, if Ur is not empty we sample an action uniformly at random from Ur, and we let Ur`1 be Ur after removing the actions whose losses were observed. Otherwise, we let Ur`1 be the empty set.\nThe following lemma lower bounds the expected number of actions whose losses are observed at each iteration of the process.\nLemma 13. Let r \u0105 0. Let N be the number of actions seen when sampling uniformly at random from Ur. Then, ErN |Urs \u011b |Ur|{p2\u03b1q.\nThe main tool used in the proof of the lemma is the following version of Tura\u0301n\u2019s theorem (see, e.g., Alon and Spencer, 2008).\nTheorem 14 (Tura\u0301n). Let G \u201c pV,Eq be an undirected graph and \u03b1 be the independence number of G. Then,\n\u03b1 \u011b |V | 1` 2|E|{|V | .\nProof of Lemma 13. Fix some feedback graph G \u201c pV,Eq with independence number \u010f \u03b1, and let doutpvq be the out-degree of vertex v. Note that the independence number of the subgraph over U can only decrease, namely it is also at most \u03b1. As such, we shall think of doutpvq as the out-degree of v in the subgraph.\nWe would like to apply Tura\u0301n\u2019s theorem to the subgraph, which is a directed graph. We do so by constructing an undirected version of the subgraph, namely one in which we ignore the orientation of the edges. Note that the number of edges in the undirected version can only decrease. Therefore,\nErN |Urs \u201c 1` 1 |Ur| \u00ff\nvPUr\ndoutpvq \u201c 1` |E| |Ur| \u011b |Ur| 2\u03b1 ,\nwhere the inequality follows from Tura\u0301n\u2019s theorem (Theorem 14).\nProof of Theorem 12. By the construction of the random process, the probability that Algorithm 2 spends more than t rounds of the game is exactly the probability that Ut`1 is not empty. To bound this probability we claim that for any r \u0105 0,\nEr|Ur`1|s \u010f K expp\u00b4r{p2\u03b1qq . (4)\nIndeed, fix some i \u0105 0. By Lemma 13 we have that Er|Ui`1||Uis \u201c |Ui| \u00b4 ErN |Uis \u010f |Ui|p1 \u00b4 1{p2\u03b1qq. Taking expectation with respect to Ui and then applying this argument recursively, we get that Er|Ur`1|s \u010f |U1|p1\u00b4 1{p2\u03b1qqr \u010f K expp\u00b4r{p2\u03b1qq.\nNow, let t1 \u201c t2\u03b1 logpK{\u03b4qu ` 1. We will show that the probability that Ut1`1 is not empty is at most \u03b4. By Markov\u2019s inequality and Eq. (4),\nPr|Ut1`1| \u0105 0s \u010f Er|Ut1`1|s \u010f K expp\u00b4t1{p2\u03b1qq \u0103 \u03b4 .\nTo conclude, with probability at least 1\u00b4 \u03b4, the number of rounds that the algorithm spends is at most t1 \u010f 4\u03b1 logpK{\u03b4q, since K \u011b 2 by assumption."}, {"heading": "5 Beyond Bandit Feedback", "text": "In this section we extend our results to a more general class of feedback graphs. In particular, we no longer assume that the learner automatically gets to observe the loss of the action that she chose. Instead, we allow the graphs to have self-loops, namely edges of the form v \u00d1 v. The absence of self-loops at individual actions allows for feedback models that are not necessarily more informative than the bandit model.\nRecently, Alon et al. (2015) have studied this more general feedback model and divided feedback graphs into three categories: unobservable graphs, for which the induced problem is not learnable; weakly observable graphs, for which r\u0398pT 2{3q regret is achievable; and strongly observable graphs, for which it is possible to attain r\u0398p ? T q regret. Their results assume that the feedback graphs are available to the learner, at least after making each prediction. Here, we revisit their results assuming that the graphs are never fully revealed to the learner.\nWe begin by recalling the definitions of observability of Alon et al. (2015). A vertex in a directed graph is observable if it has at least one incoming edge. A vertex is strongly observable if it has either a self-loop or edges incoming from all other vertices. A vertex is weakly observable if it is observable but not strongly observable. A graph is observable if all of its vertices are observable, and it is strongly observable if all of its vertices are strongly observable. A graph is weakly observable if it is observable but not strongly observable. Note that a graph with self-loops at all vertices is necessarily strongly observable.\nLet us now discuss our results; below, we only give the main ideas and sketch the proofs, deferring details to the full version of the paper."}, {"heading": "5.1 Strongly observable graphs", "text": "In the adversarial setting, we show that the problem might be unlearnbable even with strongly observable graphs; formally, we prove (see Section 6.6):\nTheorem 15. In the adversarial setting, any algorithm must suffer at least T {16 regret in the worst case, even when G1, . . . , GT are all strongly observable.\nProof sketch. Consider a problem over two actions, u and v. The environment chooses one of two distributions over the choice of the loss of action v, the edge u \u00d1 v and the self-loop v \u00d1 v, that are summarized in Figure 2. Each cell in the table is split into two, where the left half is for the first distribution, and the right half is for the second distribution. The two rightmost columns indicate the marginal distributions between the loss of action v and either the edge\nu \u00d1 v or the self-loop at v. Additionally, the action u always has a self-loop and its loss is constantly 1{2.\nThe key implication of the construction is that under both distributions, whether the learner plays action v or u, she does not observe the loss of v with probability 1{4, she observes a loss of 0 for action v with probability 3{8 and she observes a loss of 1 with probability 3{8. This is although in the first distribution the loss of v is distributed Bernoulli(3{8), and in the second distribution it is distributed Bernoulli(5{8). Therefore, the learner can never tell if u or v is the action with the smaller loss.\nThe result above is in contrast to the stochastic setting; not only that the problem is learnable but there is an algorithm that attains rOp ? \u03b1T q regret\u2014the same regret bound that is obtained in the setting where the learner gets to observe the feedback graph fully at the end of each round. In particular, we have:\nTheorem 16. In the stochastic setting, there exists an online learning algorithm that attains rOp ? \u03b1T q regret, provided that G1, . . . , GT are all strongly observable.\nProof sketch. The algorithm is the same as Algorithm 1, where the only difference is in the implementation of Algorithm 2. Even if the graph is strongly observable, a subgraph of exactly one vertex might not be observable since this vertex might not have a self-loop. Therefore, we stop the while loop in Algorithm 2 when Vr has one action or less. If it is left with exactly one action, say v, we add a different arbitrary action from V and we sample uniformly at random from both actions until the loss of v is observed. The probability of observing v is at least 1{2, and therefore the modification only adds a constant number of rounds in expectation to the total runtime of Algorithm 2."}, {"heading": "5.2 Observable graphs", "text": "For the adversarial setting, the problem is unlearnable since it is already unlearnable for strongly observable graphs by Theorem 15. On the other hand, in the stochastic setting we have the following result.\nTheorem 17. In the stochastic setting, there exists an online learning algorithm that attains rOpK1{3T 2{3q regret, provided that G1, . . . , GT are all observable.\nThis regret bound is tight up to logarithmic factors for weakly observable G1, ..., GT , since the r\u2126pK1{3T 2{3q lower bound proved by Alon et al. (2015), in the easier setting where the graphs are revealed following each decision, applies in our stochastic setting.\nProof sketch. The algorithm is done in two phases. In the exploration phase, the learner estimates the means of the losses of all actions to \u01eb accuracy. The learner simply plays actions uniformly at random. By an argument similar to that of the coupon collector problem, it suffices for the exploration phase to complete after rOpK{\u01eb2q rounds. In the exploitation phase, the learner plays the best action found during the exploration phase, and suffers an expected instantaneous regret of at most \u01eb per round. In total, the expected regret of the learner is rOpK{\u01eb2 ` \u01ebT q. Setting \u01eb \u201c r\u0398ppK{T q1{3q gives an expected regret bound of rOpK1{3T 2{3q."}, {"heading": "6 Additional proofs", "text": ""}, {"heading": "6.1 Proof of Lemma 8", "text": "Proof. AlphaSample observed the loss of each action in Vr for nr times. Note that by assumption the losses of the actions are distributed independently from the feedback graphs. Therefore by Hoeffding\u2019s inequality and the union bound we have w.p. at least 1\u00b4 T\u00b41,\n@ v P Vr , |\u00b5pvq \u00b4mrpvq| \u010f \u01ebr .\nDenote by v\u0303 an action such that mrpv\u0303q \u201c m\u2039r. Note that by induction v\u2039 P Vr`1 since if v\u2039 P Vr then mrpv\u2039q \u010f \u00b5\u2039 ` \u01ebr \u010f \u00b5pv\u0303q ` \u01ebr \u010f m\u2039r ` 2\u01ebr . Therefore, for all v P Vr`1 we have\n\u00b5pvq \u010f mrpvq ` \u01ebr \u010f m\u2039r ` 3\u01ebr \u010f mrpv\u2039q ` 3\u01ebr \u010f \u00b5\u2039 ` 4\u01ebr ."}, {"heading": "6.2 Proof of Lemma 7", "text": "Proof. Notice that Algorithm 2 takes at most K rounds with probability 1. Using Theorem 12 with \u03b4 \u201c \u03b1{K, we get that the expected number of rounds for the algorithm to complete is at most 4\u03b1 logpK2{\u03b1q ` p\u03b1{Kq \u00a8K \u010f 10\u03b1 logK, since K \u011b 2 by assumption and \u03b1 \u011b 1."}, {"heading": "6.3 Proof of Lemma 5", "text": "To prove Lemma 5 we will need the following lemma.\nLemma 18. Let G \u201c pV,Eq be an undirected Erdo\u0308s-Re\u0301nyi graph, such that each edge appears independently with probability p. For any 0 \u010f \u03b4 \u010f ?1\u00b4 p, the independence number of G is at most 2 log1{p1\u00b4pqpK{\u03b4q ` 1 with probability at least 1\u00b4 \u03b4. Proof. Denote \u03c4 \u201c 1{p1 \u00b4 pq and set \u03b1 \u201c r2 log\u03c4 K ` a 2 log\u03c4 p1{\u03b4qs. First, consider the concave quadratic x log\u03c4 K \u00b4 xpx \u00b4 1q{2 \u00b4 log\u03c4 \u03b4. It is negative for any x \u0105 log\u03c4 K ` 1{2 `b log2\u03c4 K \u00b4 2 log\u03c4 \u03b4 and in particular it is negative for x \u201c \u03b1` 1. Thus we have\np\u03b1` 1q log\u03c4 K \u00b4 \u03b1p\u03b1 ` 1q{2 \u0103 log\u03c4 \u03b4 (5)\nNext, suppose that the independence number of the graph is more than \u03b1, and therefore there is an independent set of \u03b1 ` 1 vertices. The probability that a subset S \u010e V is an independent set is p1\u00b4 pqp |S| 2 q \u201c \u03c4\u00b4p |S| 2 q. By a union bound,\nPr\u03b1pGq \u0105 \u03b1s \u010f K\u03b1`1\u03c4\u00b4p \u03b1`1 2 q \u201c \u03c4 p\u03b1`1q log\u03c4 K\u00b4\u03b1p\u03b1`1q{2 \u0103 \u03c4 log\u03c4 \u03b4 \u201c \u03b4 .\nwhere the second inequality is by Eq. (5).\nTo finish the proof of the lemma, we have that with probability at least 1\u00b4 \u03b4, the independence number of the graph is at most\n\u03b1 \u201c R 2 log\u03c4 K ` b 2 log\u03c4 1 \u03b4 V \u010f 2 log\u03c4 K ` 2 log\u03c4 1\u03b4 ` 1 \u201c 2 log\u03c4 K\u03b4 ` 1\nsince \u03b4 \u010f \u03c4\u00b41{2 by assumption.\nWe now turn to prove Lemma 5.\nProof of Lemma 5. Let G be one of the graphs in the sequence, and consider an undirected version of G3. If we remove v\u2039 from the graph, we get an Erdo\u0308s-Re\u0301nyi subgraph with each edge appearing with probability 1\u00b44\u01eb2 and independently of other edges. The independence number of the original graph is at most that of the subgraph plus one. Thus, it remains to bound the independence number of the subgraph.\nDenote \u03c4 \u201c 1{p4\u01eb2q \u201c 16T {K. We apply Lemma 18 with \u03b4 \u201c \u01eb{p8T q and get that with probability at least 1\u00b4\u03b4, the independence number of the subgraph is at most \u03b1 \u201c 2 log\u03c4 p8KT {\u01ebq`1. Then by the choice of \u01eb and since T \u011b K2,\n\u03b1 \u201c 2 log\u03c4 8KT\n\u01eb ` 1 \u201c 2 log\u03c4 p64K1{2T 3{2q ` 1\n\u010f 2 log\u03c4 \u02c6 16T\nK\n\u02d97{2 ` 1\n\u201c 2 \u00a8 7 2 ` 1 \u201c 8 ,\nAppying this argument to each of the graphs G1, ..., GT , the claim holds by the union bound."}, {"heading": "6.4 Proof of Theorem 4", "text": "To prove the theorem, we shall need a few definitions. Let P,Q be a couple of distributions over the same space and sigma-algebra F . We define the total variation distance between P and Q as\nDTVpP , Qq\u201c sup EPF |PrEs \u00b4QrEs|\nIf P and Q are discrete distributions, we define the KL divergence between P and Q as\nDKLpP }Qq\u201c \u00ff\nx\nlog \u02c6 Prxs Qrxs \u02d9 Prxs\nassuming the support of P is contained in that of Q, and where the sum is taken over the support of P.\nWe can now turn to the proof of the theorem.\nProof of Theorem 4. Let us introduce the random variables Tv whose value is the number of times the learner plays action v. We also introduce the notations Pv and Ev indicating probability and expectation with respect to the marginal distributions under which v\u2039 \u201c v. Then,\n3An undirected graph in which there is an edge between u and v if pu, vq P E or pv, uq P E.\nwe have\nRT \u201c E \u00ab T\u00ff\nt\u201c1\n\u2113tpvtq \u00b4 T\u00ff\nt\u201c1\n\u2113tpv\u2039q ff\n\u201c 1 K\n\u00ff\nvPV\nEv\n\u00ab T\u00ff\nt\u201c1\n\u2113tpvtq \u00b4 T\u00ff\nt\u201c1\n\u2113tpvq ff\n\u201c 1 K\n\u00ff\nvPV\n\u01eb \u00a8 EvrT \u00b4 Tvs\n\u201c \u01eb \u02dc T \u00b4 1\nK\n\u00ff\nvPV\nEvrTvs \u00b8 , (6)\nand in order to proceed we shall upper bound EvrTvs. Introduce a new distribution, in which the losses of the actions are independent Bernoulli(1{2) variables, and the feedback graphs are such that each directed edge appears with probability 1 \u00b4 2\u01eb independently of the other edges and the losses of the actions. We will refer to this new law using P0 and E0. Let \u03bbt be the losses and edges observed at time t, and similarly \u03bbptq \u201c p\u03bb1, ..., \u03bbtq are the losses and edges observed up until time t (inclusive). Then, since the sequence \u03bbpT q determines the actions of the learner over the entire game, and by Pinsker\u2019s inequality,\nEvrTvs \u00b4 E0rTvs \u010f T \u00a8DTV \u00b4 Pvr\u03bbpT qs , P0r\u03bbpT qs \u00af\n\u010f T c 1\n2 DKL\n` P0r\u03bbpT qs \u203a\u203aPvr\u03bbpT qs \u02d8 . (7)\nMoreover, by the chain rule of KL-divergence, DKL ` P0r\u03bbpT qs \u203a\u203aPvr\u03bbpT qs \u02d8 equals\nT\u00ff\nt\u201c1\n\u00ff\n\u03bbpt\u00b41q\nP0r\u03bbpt\u00b41qsDKL \u00b4 P0r\u03bbt|\u03bbpt\u00b41qs \u203a\u203a\u203aPvr\u03bbt|\u03bbpt\u00b41qs \u00af . (8)\nConsider a single term in the sum. Recall that \u03bbpt\u00b41q determines the action vt chosen by the learner on round t. If vt \u2030 v then, by our construction, the losses and edges of the graph observed by the learner are distributed exactly the same under Pv and P0, and the KL divergence is 0. If vt \u201c v then the losses of all other actions are distributed Bernoulli(1{2), and independently of the loss of action v and the observed edges. The latter is so under both Pv and P0. Moreover, the observed edges are distributed Bernoulli(1 \u00b4 2\u01eb) independently of the loss of action v under both Pv and P0. Namely, the only element that is distributed differently under Pv and P0 is the loss of action v, and the latter is distributed independently from all other observed variables. Recall that the loss of action v is distributed as Bernoulli(1{2) under P0 and as Bernoulli(1{2 \u00b4 \u01eb) under Pv. Therefore, DKL ` P0r\u03bbt|\u03bbpt\u00b41qs \u203a\u203aPvr\u03bbt|\u03bbpt\u00b41qs \u02d8 is upper-bounded by\nDKL\n\u02c6 1\n2\n\u203a\u203a\u203a\u203a 1\n2 \u00b4 \u01eb\n\u02d9 \u201c \u00b41\n2 logp1\u00b4 4\u01eb2q \u010f 4\u01eb2 ,\nwhere the last inequality holds since \u01eb \u0103 1{4 by assumption. Plugging the above back into Eq. (8),\nDKL \u00b4 P0r\u03bbpT qs \u203a\u203a\u203aPvr\u03bbpT qs \u00af \u010f T\u00ff\nt\u201c1\nP0rvt \u201c vs4\u01eb2 \u201c 4\u01eb2E0rTvs ,\nand the latter into Eq. (7), we get that EvrTvs \u010f E0rTvs ` T\u01eb a 2E0rTvs.\nNow, K \u011b 2 by assumption, and therefore 1\nK\n\u00ff\nvPV\nEvrTvs \u010f 1\nK\n\u00ff\nvPV\nE0rTvs ` 1\nK\n\u00ff\nvPV\nT\u01eb a 2E0rTvs\n\u010f 1 K\n\u00ff\nvPV\nE0rTvs ` T\u01eb d 1\nK\n\u00ff\nvPV\n2E0rTvs\n\u201c T K\n` T\u01eb c 2T\nK \u010f T 2 ` T\u01eb\nc 2T\nK .\nLet us now return to Eq. (6). We can lower bound the regret as\nRT \u011b \u01eb \u02dc T \u00b4 T\n2 \u00b4 T\u01eb\nc 2T\nK\n\u00b8 \u201c \u01ebT \u02dc 1\n2 \u00b4 \u01eb\nc 2T\nK\n\u00b8 .\nBy our choice of \u01eb, we have that \u01eb a p2T q{K is at most 1{4, and so\nRT \u011b T\n8\nc K\nT\n\u02c6 1\n2 \u00b4 1 4\n\u02d9 \u201c 1\n32\n? KT ,\nas claimed."}, {"heading": "6.5 Proof of Theorem 1", "text": "We shall construct an environment whose distribution over graphs is the same as the environment described in Section 3, conditioned on the event that the independence numbers of all graphs are bounded by 9.\nWe now claim that the regret against this environment is not too far off the regret against the original environment. This is because the expected regret against the original environment is at most the expected regret against this environment plus p\u01eb{8qT , by Lemma 5 and since the regret is at most T (with probability 1).\nBy Theorem 4, the regret against this environment is at least\nRT \u00b4 \u01eb\n8 T \u011b\n? KT\n32 \u00b4 p1{8q\na K{T\n8 T \u201c\n? KT\n64\nand thus the lower bound holds."}, {"heading": "6.6 Proof of Theorem 15", "text": "Proof. By Yao\u2019s minimax principle, in order to prove a lower bound on the learner\u2019s regret it is enough to demonstrate a randomized strategy for the environment that forces any deterministic learner to incur \u2126pT q regret. We will construct our environment\u2019s strategy as follows.\nConsider a learning problem over two actions, u and v. Before the game starts, the environment samples an index \u03c7 P t1, 2u uniformly at random. If \u03c7 \u201c 1 then the environment plays one distribution; if \u03c7 \u201c 2 then she plays another distribution. Under the first distribution, the loss of v is distributed Bernoulli(3{8); under the second distribution, it is distributed as Bernoulli(5{8). In both cases the action u always has a self-loop and its loss is constantly 1{2.\nThe feedback graphs G1, . . . , GT are chosen i.i.d. and are dependent on the loss of action v. Under the first distribution, if the loss of v is 1, both an edge u \u00d1 v and the self-loop v \u00d1 v appear with probability 1; if the loss of v is 0, with probability 2{5 only the edge u \u00d1 v appears, with probability 2{5 only the self-loop v \u00d1 v appears and with probability 1{5 both the edge and the self-loop appear. Under the second distribution, if the loss of v is 1, with probability\n2{5 only the edge u \u00d1 v appears, with probability 2{5 only the self-loop v \u00d1 v appears and with probability 1{5 both the edge and the self-loop appear; if the loss of v is 0, both an edge u \u00d1 v and the self-loop v \u00d1 v appear with probability 1. See Figure 2 for a summary of the edge probabilities in this construction. Note that in every case, the action v either have a self-loop or and incoming edge from u (or both). Therefore, the graphs G1, ...GT are all strongly observable.\nThe key implication of the construction above is as following. Under both distributions, if the learner plays action v, she does not observe the loss of v with probability 1{4, she observes a loss of 0 with probability 3{8 and she observes a loss of 1 with probability 3{8. If the learner plays action u, she does not observe the loss of v with probability 1{4, she observes a loss of 0 with probability 3{8 and she observes a loss of 1 with probability 3{8. Moreover, if the learner plays action v, she does not observe whether there is an incoming edge u \u00d1 v; if she plays action u she does not observe whether there is a self-loop v \u00d1 v.\nLet M denote the number of times the learner chooses to play action v. By the argument above she must play the exact same strategy under both distributions, and as a consequence the random variable M is independent of the choice of distribution \u03c7. Hence, the expected regret of the learner against the environment constructed above is at least\n1 2 E\n\u201e 1\n8 pT \u00b4Mq\n\u02c7\u030c \u02c7\u030c\u03c7 \u201c 1  ` 1\n2 E\n\u201e 1\n8 M\n\u02c7\u030c \u02c7\u030c\u03c7 \u201c 2  \u201c 1\n2 E\n\u201e 1\n8 pT \u00b4Mq\n ` 1\n2 E\n\u201e 1\n8 M\n \u201c 1\n16 T\nas claimed."}, {"heading": "Acknowledgements", "text": "TK would like to thank Nicolo\u0300 Cesa-Bianchi and Ofer Dekel for stimulating discussions in the early stages of this research."}], "references": [{"title": "The Probabilistic Method", "author": ["N. Alon", "J.H. Spencer"], "venue": null, "citeRegEx": "Alon and Spencer.,? \\Q2008\\E", "shortCiteRegEx": "Alon and Spencer.", "year": 2008}, {"title": "From bandits to experts: A tale of domination and independence", "author": ["N. Alon", "N. Cesa-Bianchi", "C. Gentile", "Y. Mansour"], "venue": "In Advances in Neural Information Processing Systems", "citeRegEx": "Alon et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Alon et al\\.", "year": 2013}, {"title": "Nonstochastic multi-armed bandits with graph-structured feedback", "author": ["N. Alon", "N. Cesa-Bianchi", "C. Gentile", "S. Mannor", "Y. Mansour", "O. Shamir"], "venue": "CoRR, abs/1409.8428,", "citeRegEx": "Alon et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Alon et al\\.", "year": 2014}, {"title": "Online learning with feedback graphs: Beyond bandits", "author": ["N. Alon", "N. Cesa-Bianchi", "O. Dekel", "T. Koren"], "venue": "In Proceedings of The 28th Conference on Learning", "citeRegEx": "Alon et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Alon et al\\.", "year": 2015}, {"title": "Minimax policies for adversarial and stochastic bandits", "author": ["J. Audibert", "S. Bubeck"], "venue": "In Proceedings of the 22nd Conference on Learning Theory (COLT),", "citeRegEx": "Audibert and Bubeck.,? \\Q2009\\E", "shortCiteRegEx": "Audibert and Bubeck.", "year": 2009}, {"title": "Finite-time analysis of the multiarmed bandit problem", "author": ["P. Auer", "N. Cesa-Bianchi", "P. Fischer"], "venue": "Machine learning,", "citeRegEx": "Auer et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Auer et al\\.", "year": 2002}, {"title": "The nonstochastic multiarmed bandit problem", "author": ["P. Auer", "N. Cesa-Bianchi", "Y. Freund", "R.E. Schapire"], "venue": "SIAM Journal on Computing,", "citeRegEx": "Auer et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Auer et al\\.", "year": 2002}, {"title": "Leveraging side observations in stochastic bandits", "author": ["S. Caron", "B. Kveton", "M. Lelarge", "S. Bhagat"], "venue": "In Proceedings of the Twenty-Eighth Conference on Uncertainty in Artificial Intelligence, Catalina Island, CA,", "citeRegEx": "Caron et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Caron et al\\.", "year": 2012}, {"title": "How to use expert advice", "author": ["N. Cesa-Bianchi", "Y. Freund", "D. Haussler", "D.P. Helmbold", "R.E. Schapire", "M.K. Warmuth"], "venue": "Journal of the ACM (JACM),", "citeRegEx": "Cesa.Bianchi et al\\.,? \\Q1997\\E", "shortCiteRegEx": "Cesa.Bianchi et al\\.", "year": 1997}, {"title": "Pac bounds for multi-armed bandit and markov decision processes", "author": ["E. Even-Dar", "S. Mannor", "Y. Mansour"], "venue": "In Computational Learning Theory,", "citeRegEx": "Even.Dar et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Even.Dar et al\\.", "year": 2002}, {"title": "A decision-theoretic generalization of on-line learning and an application to boosting", "author": ["Y. Freund", "R.E. Schapire"], "venue": "Journal of computer and system sciences,", "citeRegEx": "Freund and Schapire.,? \\Q1997\\E", "shortCiteRegEx": "Freund and Schapire.", "year": 1997}, {"title": "Almost optimal exploration in multi-armed bandits", "author": ["Z. Karnin", "T. Koren", "O. Somekh"], "venue": "In Proceedings of the 30th International Conference on Machine Learning", "citeRegEx": "Karnin et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Karnin et al\\.", "year": 2013}, {"title": "Efficient learning by implicit exploration in bandit problems with side observations", "author": ["T. Koc\u00e1k", "G. Neu", "M. Valko", "R. Munos"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Koc\u00e1k et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Koc\u00e1k et al\\.", "year": 2014}, {"title": "Online learning with noisy side observations", "author": ["T. Koc\u00e1k", "G. Neu", "M. Valko"], "venue": "In Proceedings of the Nineteenth International Conference on Artificial Intelligence and Statistics (AISTATS),", "citeRegEx": "Koc\u00e1k et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Koc\u00e1k et al\\.", "year": 2016}, {"title": "The weighted majority algorithm", "author": ["N. Littlestone", "M.K. Warmuth"], "venue": "Information and computation,", "citeRegEx": "Littlestone and Warmuth.,? \\Q1994\\E", "shortCiteRegEx": "Littlestone and Warmuth.", "year": 1994}, {"title": "From bandits to experts: On the value of side-observations", "author": ["S. Mannor", "O. Shamir"], "venue": "In Advances in Neural Information Processing Systems", "citeRegEx": "Mannor and Shamir.,? \\Q2011\\E", "shortCiteRegEx": "Mannor and Shamir.", "year": 2011}, {"title": "Aggregating strategies", "author": ["V.G. Vovk"], "venue": "In Proc. Third Workshop on Computational Learning Theory,", "citeRegEx": "Vovk.,? \\Q1990\\E", "shortCiteRegEx": "Vovk.", "year": 1990}, {"title": "Online learning with gaussian payoffs and side observations", "author": ["Y. Wu", "A. Gy\u00f6rgy", "C. Szepesv\u00e1ri"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Wu et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Wu et al\\.", "year": 2015}], "referenceMentions": [{"referenceID": 15, "context": "We study an online learning framework introduced by Mannor and Shamir (2011) in which the feedback is specified by a graph, in a setting where the graph may vary from round to round and is never fully revealed to the learner.", "startOffset": 52, "endOffset": 77}, {"referenceID": 14, "context": "Traditionally, the literature considers two types of feedback: full feedback (Littlestone and Warmuth, 1994; Vovk, 1990; Cesa-Bianchi et al., 1997), where the learner observes the losses associated with all of her possible actions, and bandit feedback Auer et al.", "startOffset": 77, "endOffset": 147}, {"referenceID": 16, "context": "Traditionally, the literature considers two types of feedback: full feedback (Littlestone and Warmuth, 1994; Vovk, 1990; Cesa-Bianchi et al., 1997), where the learner observes the losses associated with all of her possible actions, and bandit feedback Auer et al.", "startOffset": 77, "endOffset": 147}, {"referenceID": 8, "context": "Traditionally, the literature considers two types of feedback: full feedback (Littlestone and Warmuth, 1994; Vovk, 1990; Cesa-Bianchi et al., 1997), where the learner observes the losses associated with all of her possible actions, and bandit feedback Auer et al.", "startOffset": 77, "endOffset": 147}, {"referenceID": 1, "context": ", 1997), where the learner observes the losses associated with all of her possible actions, and bandit feedback Auer et al. (2002b), where the learner only observes the loss of the action she has actually taken.", "startOffset": 112, "endOffset": 132}, {"referenceID": 1, "context": ", 1997), where the learner observes the losses associated with all of her possible actions, and bandit feedback Auer et al. (2002b), where the learner only observes the loss of the action she has actually taken. Full feedback and bandit feedback are special cases of a general framework introduced by Mannor and Shamir (2011), in which the feedback model is specified by a sequence G1, .", "startOffset": 112, "endOffset": 326}, {"referenceID": 1, "context": "Alon et al. (2013), and subsequently Koc\u00e1k et al.", "startOffset": 0, "endOffset": 19}, {"referenceID": 1, "context": "Alon et al. (2013), and subsequently Koc\u00e1k et al. (2014); Alon et al.", "startOffset": 0, "endOffset": 57}, {"referenceID": 1, "context": "Alon et al. (2013), and subsequently Koc\u00e1k et al. (2014); Alon et al. (2015), gave regret-minimization algorithms that achieve r Op ? \u03b1T q regret, where \u03b1 is a bound on the independence numbers of the graphs G1, .", "startOffset": 0, "endOffset": 77}, {"referenceID": 1, "context": "Alon et al. (2013), and subsequently Koc\u00e1k et al. (2014); Alon et al. (2015), gave regret-minimization algorithms that achieve r Op ? \u03b1T q regret, where \u03b1 is a bound on the independence numbers of the graphs G1, . . . , GT . Up to logarithmic factors, their results recover and interpolate between the classic bounds of Op ? T logKq with full feedback Freund and Schapire (1997) and Op ? KT q with bandit feedback Auer et al.", "startOffset": 0, "endOffset": 379}, {"referenceID": 1, "context": "Alon et al. (2013), and subsequently Koc\u00e1k et al. (2014); Alon et al. (2015), gave regret-minimization algorithms that achieve r Op ? \u03b1T q regret, where \u03b1 is a bound on the independence numbers of the graphs G1, . . . , GT . Up to logarithmic factors, their results recover and interpolate between the classic bounds of Op ? T logKq with full feedback Freund and Schapire (1997) and Op ? KT q with bandit feedback Auer et al. (2002b); Audibert and Bubeck (2009).", "startOffset": 0, "endOffset": 434}, {"referenceID": 1, "context": "Alon et al. (2013), and subsequently Koc\u00e1k et al. (2014); Alon et al. (2015), gave regret-minimization algorithms that achieve r Op ? \u03b1T q regret, where \u03b1 is a bound on the independence numbers of the graphs G1, . . . , GT . Up to logarithmic factors, their results recover and interpolate between the classic bounds of Op ? T logKq with full feedback Freund and Schapire (1997) and Op ? KT q with bandit feedback Auer et al. (2002b); Audibert and Bubeck (2009). The r Op ? \u03b1T q bound turns out to be tight for any feedback graph (when it is fixed throughout the game and known in advance), in light of a matching lower bound due to Mannor and Shamir (2011).", "startOffset": 0, "endOffset": 462}, {"referenceID": 1, "context": "Alon et al. (2013), and subsequently Koc\u00e1k et al. (2014); Alon et al. (2015), gave regret-minimization algorithms that achieve r Op ? \u03b1T q regret, where \u03b1 is a bound on the independence numbers of the graphs G1, . . . , GT . Up to logarithmic factors, their results recover and interpolate between the classic bounds of Op ? T logKq with full feedback Freund and Schapire (1997) and Op ? KT q with bandit feedback Auer et al. (2002b); Audibert and Bubeck (2009). The r Op ? \u03b1T q bound turns out to be tight for any feedback graph (when it is fixed throughout the game and known in advance), in light of a matching lower bound due to Mannor and Shamir (2011).", "startOffset": 0, "endOffset": 658}, {"referenceID": 12, "context": "While some require the entire graph Gt for performing their updates only at the end of round t (e.g., Alon et al., 2013; Koc\u00e1k et al., 2014; Alon et al., 2015), others actually need the description of Gt at the beginning of the round before making their decision (e.", "startOffset": 95, "endOffset": 159}, {"referenceID": 3, "context": "While some require the entire graph Gt for performing their updates only at the end of round t (e.g., Alon et al., 2013; Koc\u00e1k et al., 2014; Alon et al., 2015), others actually need the description of Gt at the beginning of the round before making their decision (e.", "startOffset": 95, "endOffset": 159}, {"referenceID": 5, "context": "\u201d In other words, the side observations received by the learner are effectively useless; she may as well ignore them and use a standard bandit algorithm such as Exp3 Auer et al. (2002b) to perform optimally.", "startOffset": 166, "endOffset": 186}, {"referenceID": 10, "context": "This result is optimal up to logarithmic factors, even when the feedback graph is fixed throughout the game and known in advance, due to a lower bound of Mannor and Shamir (2011). For our algorithm in the stochastic case, we also prove a distribution-dependent regret bound that scales logarithmically with T .", "startOffset": 154, "endOffset": 179}, {"referenceID": 2, "context": "This bound is a substantial improvement over standard regret bounds of stochastic multi-armed bandit algorithms such as UCB Auer et al. (2002a): whereas the regret of the latter algorithms is typically bounded by a sum \u0159 vPV p1{\u2206vq taken over all K actions, the sum in our bound is taken only over the subset of r Op\u03b1q actions with the smallest gaps.", "startOffset": 124, "endOffset": 144}, {"referenceID": 1, "context": "Again, this result cannot be improved even when the feedback graph is fixed throughout the game, and has an optimal dependence on \u03b1 as well as on the gaps \u2206v, thus resolving an open question of Alon et al. (2014). Finally, we extend our results to a more general feedback model recently studied by Alon et al.", "startOffset": 194, "endOffset": 213}, {"referenceID": 1, "context": "Again, this result cannot be improved even when the feedback graph is fixed throughout the game, and has an optimal dependence on \u03b1 as well as on the gaps \u2206v, thus resolving an open question of Alon et al. (2014). Finally, we extend our results to a more general feedback model recently studied by Alon et al. (2015), in which the learner does not necessarily observe her own loss after making predictions (namely, each action may or may not have a self-loop in each feedback graph).", "startOffset": 194, "endOffset": 317}, {"referenceID": 1, "context": "Again, this result cannot be improved even when the feedback graph is fixed throughout the game, and has an optimal dependence on \u03b1 as well as on the gaps \u2206v, thus resolving an open question of Alon et al. (2014). Finally, we extend our results to a more general feedback model recently studied by Alon et al. (2015), in which the learner does not necessarily observe her own loss after making predictions (namely, each action may or may not have a self-loop in each feedback graph). Alon et al. (2015) gave a necessary and sufficient condition for attaining \u0398p ? T q regret in this more general model\u2014a graph-theoretic condition they call strong observability.", "startOffset": 194, "endOffset": 503}, {"referenceID": 4, "context": "Online learning with feedback graphs was previously considered in the stochastic setting by Caron et al. (2012), who gave results depending on the graph clique structure.", "startOffset": 92, "endOffset": 112}, {"referenceID": 1, "context": "Their analysis, however, only applies when the feedback graph is fixed throughout the game, and can only bound the regret in terms of a quantity akin to the clique-partition number of this graph, which is always larger than its independence number (the gap between the two can be very large; see Alon et al., 2014). More recently, Wu et al. (2015) and Koc\u00e1k et al.", "startOffset": 296, "endOffset": 348}, {"referenceID": 1, "context": "Their analysis, however, only applies when the feedback graph is fixed throughout the game, and can only bound the regret in terms of a quantity akin to the clique-partition number of this graph, which is always larger than its independence number (the gap between the two can be very large; see Alon et al., 2014). More recently, Wu et al. (2015) and Koc\u00e1k et al. (2016) have investigated a noisy version of the feedback graph model, where feedback is specified by a weighted directed graph with edge weights indicating the quality (e.", "startOffset": 296, "endOffset": 372}, {"referenceID": 1, "context": "Their analysis, however, only applies when the feedback graph is fixed throughout the game, and can only bound the regret in terms of a quantity akin to the clique-partition number of this graph, which is always larger than its independence number (the gap between the two can be very large; see Alon et al., 2014). More recently, Wu et al. (2015) and Koc\u00e1k et al. (2016) have investigated a noisy version of the feedback graph model, where feedback is specified by a weighted directed graph with edge weights indicating the quality (e.g., the noise level or variance) of the feedback received on adjacent vertices. Wu et al. (2015) provided finite-time problem-dependent lower bounds for this setting; Koc\u00e1k et al.", "startOffset": 296, "endOffset": 633}, {"referenceID": 1, "context": "Their analysis, however, only applies when the feedback graph is fixed throughout the game, and can only bound the regret in terms of a quantity akin to the clique-partition number of this graph, which is always larger than its independence number (the gap between the two can be very large; see Alon et al., 2014). More recently, Wu et al. (2015) and Koc\u00e1k et al. (2016) have investigated a noisy version of the feedback graph model, where feedback is specified by a weighted directed graph with edge weights indicating the quality (e.g., the noise level or variance) of the feedback received on adjacent vertices. Wu et al. (2015) provided finite-time problem-dependent lower bounds for this setting; Koc\u00e1k et al. (2016) generalized the notion of independence number to the noisy case and gave new efficient algorithms in this setting.", "startOffset": 296, "endOffset": 723}, {"referenceID": 15, "context": "This regret bound is optimal up to logarithmic factors, since the lower bound of \u03a9p ? \u03b1T q found in Mannor and Shamir (2011) applies in our stochastic setting.", "startOffset": 100, "endOffset": 125}, {"referenceID": 11, "context": "The algorithm, given in Algorithm 1, is reminiscent of elimination-based algorithms for the stochastic multiarmed bandit problem (e.g., Even-Dar et al., 2002; Karnin et al., 2013).", "startOffset": 129, "endOffset": 179}, {"referenceID": 1, "context": "Recently, Alon et al. (2015) have studied this more general feedback model and divided feedback graphs into three categories: unobservable graphs, for which the induced problem is not learnable; weakly observable graphs, for which r \u0398pT 2{3q regret is achievable; and strongly observable graphs, for which it is possible to attain r \u0398p ? T q regret.", "startOffset": 10, "endOffset": 29}, {"referenceID": 1, "context": "Recently, Alon et al. (2015) have studied this more general feedback model and divided feedback graphs into three categories: unobservable graphs, for which the induced problem is not learnable; weakly observable graphs, for which r \u0398pT 2{3q regret is achievable; and strongly observable graphs, for which it is possible to attain r \u0398p ? T q regret. Their results assume that the feedback graphs are available to the learner, at least after making each prediction. Here, we revisit their results assuming that the graphs are never fully revealed to the learner. We begin by recalling the definitions of observability of Alon et al. (2015). A vertex in a directed graph is observable if it has at least one incoming edge.", "startOffset": 10, "endOffset": 639}, {"referenceID": 1, "context": ", GT , since the r \u03a9pK1{3T 2{3q lower bound proved by Alon et al. (2015), in the easier setting where the graphs are revealed following each decision, applies in our stochastic setting.", "startOffset": 54, "endOffset": 73}], "year": 2016, "abstractText": "We study an online learning framework introduced by Mannor and Shamir (2011) in which the feedback is specified by a graph, in a setting where the graph may vary from round to round and is never fully revealed to the learner. We show a large gap between the adversarial and the stochastic cases. In the adversarial case, we prove that even for dense feedback graphs, the learner cannot improve upon a trivial regret bound obtained by ignoring any additional feedback besides her own loss. In contrast, in the stochastic case we give an algorithm that achieves r \u0398p ? \u03b1T q regret over T rounds, provided that the independence numbers of the hidden feedback graphs are at most \u03b1. We also extend our results to a more general feedback model, in which the learner does not necessarily observe her own loss, and show that, even in simple cases, concealing the feedback graphs might render a learnable problem unlearnable.", "creator": "LaTeX with hyperref package"}}}