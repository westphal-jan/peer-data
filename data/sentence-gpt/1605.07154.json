{"id": "1605.07154", "review": {"conference": "NIPS", "VERSION": "v1", "DATE_OF_SUBMISSION": "23-May-2016", "title": "Path-Normalized Optimization of Recurrent Neural Networks with ReLU Activations", "abstract": "We investigate the parameter-space geometry of recurrent neural networks (RNNs), and develop an adaptation of path-SGD optimization method, attuned to this geometry, that can learn plain RNNs with ReLU activations. On several datasets that require capturing long-term dependency structure, we show that path-SGD can significantly improve trainability of ReLU RNNs compared to RNNs trained with SGD, even with various recently suggested initialization schemes. We show that the RNNs trained with ReLU activations may outperform RNNs with reLU activations of ReLU activations of ReLU activations of ReLU activations of ReLU activations of ReLU activations of ReLU activations of ReLU activations of ReLU activations of ReLU activations of ReLU activations of ReLU activations of ReLU activations of ReLU activations of ReLU activations of ReLU activations of ReLU activations of ReLU activations of ReLU activations of ReLU activations of ReLU activations of ReLU activations of ReLU activations of ReLU activations of ReLU activations of ReLU activations of ReLU activations of ReLU activations of ReLU activations of ReLU activations of ReLU activations of ReLU activations of ReLU activations of ReLU activations of ReLU activations of ReLU activations of ReLU activations of ReLU activations of ReLU activations of ReLU activations of ReLU activations of ReLU activations of ReLU activations of ReLU activations of ReLU activations of ReLU activations of ReLU activations of ReLU activations of ReLU activations of ReLU activations of ReLU activations of ReLU activations of ReLU activations of ReLU activations of ReLU activations of ReLU activations of ReLU activations of ReLU activations of ReLU activations of ReLU activations of ReLU activations of ReLU activations of ReLU activations of ReLU activations of ReLU activations of ReLU activations of ReLU activations of ReLU activations of ReLU activations of ReLU activations of ReLU activations of ReLU activations of ReLU activations of ReLU activations of ReLU activations of ReLU activations of ReLU activations of ReLU activations of ReLU activations of ReLU", "histories": [["v1", "Mon, 23 May 2016 19:40:50 GMT  (119kb,D)", "http://arxiv.org/abs/1605.07154v1", "15 pages"]], "COMMENTS": "15 pages", "reviews": [], "SUBJECTS": "cs.LG cs.NE", "authors": ["behnam neyshabur", "yuhuai wu", "ruslan salakhutdinov", "nati srebro"], "accepted": true, "id": "1605.07154"}, "pdf": {"name": "1605.07154.pdf", "metadata": {"source": "CRF", "title": "Path-Normalized Optimization of Recurrent Neural Networks with ReLU Activations", "authors": ["Behnam Neyshabur", "Yuhuai Wu", "Ruslan Salakhutdinov", "Nathan Srebro"], "emails": [], "sections": [{"heading": "1 Introduction", "text": "Recurrent Neural Networks (RNNs) have been found to be successful in a variety of sequence learning problems [4, 3, 9], including those involving long term dependencies (e.g., [1, 23]). However, most of the empirical success has not been with \u201cplain\u201d RNNs but rather with alternate, more complex structures, such as Long Short-Term Memory (LSTM) networks [7] or Gated Recurrent Units (GRUs) [3]. Much of the motivation for these more complex models is not so much because of their modeling richness, but perhaps more because they seem to be easier to optimize. As we discuss in Section 3, training plain RNNs using gradient-descent variants seems problematic, and the choice of the activation function could cause a problem of vanishing gradients or of exploding gradients.\nIn this paper our goal is to better understand the geometry of plain RNNs, and develop better optimization methods, adapted to this geometry, that directly learn plain RNNs with ReLU activations. One motivation for insisting on plain RNNs, as opposed to LSTMs or GRUs, is because they are simpler and might be more appropriate for applications that require low-complexity design such as in mobile computing platforms [22, 5]. In other applications, it might be better to solve optimization issues by better optimization methods rather than reverting to more complex models. Better understanding optimization of plain RNNs can also assist us in designing, optimizing and intelligently using more complex RNN extensions.\nImproving training RNNs with ReLU activations has been the subject of some recent attention, with most research focusing on different initialization strategies [12, 22]. While initialization can certainly have a strong effect on the success of the method, it generally can at most delay the problem of gradient explosion during optimization. In this paper we take a different approach that can be combined with any initialization choice, and focus on the dynamics of the optimization itself.\nAny local search method is inherently tied to some notion of geometry over the search space (e.g. the space of RNNs). For example, gradient descent (including stochastic gradient descent) is tied to the Euclidean \u2217Contributed equally.\nar X\niv :1\n60 5.\n07 15\n4v 1\n[ cs\n.L G\n] 2\n3 M\ngeometry and can be viewed as steepest descent with respect to the Euclidean norm. Changing the norm (even to a different quadratic norm, e.g. by representing the weights with respect to a different basis in parameter space) results in different optimization dynamics. We build on prior work on the geometry and optimization in feed-forward networks, which uses the path-norm [16] (defined in Section 4) to determine a geometry leading to the path-SGD optimization method. To do so, we investigate the geometry of RNNs as feedforward networks with shared weights (Section 2) and extend a line of work on Path-Normalized optimization to include networks with shared weights. We show that the resulting algorithm (Section 4) has similar invariance properties on RNNs as those of standard path-SGD on feedforward networks, and can result in better optimization with less sensitivity to the scale of the weights."}, {"heading": "2 Recurrent Neural Nets as Feedforward Nets with Shared Weights", "text": "We view Recurrent Neural Networks (RNNs) as feedforward networks with shared weights. We denote a general feedforward network with ReLU activations and shared weights is indicated by N (G, \u03c0,p) where G(V,E) is a directed acyclic graph over the set of nodes V that corresponds to units v \u2208 V in the network, including special subsets of input and output nodes Vin, Vout \u2282 V , p \u2208 Rm is a parameter vector and \u03c0 : E \u2192 {1, . . . ,m} is a mapping from edges in G to parameters indices. For any edge e \u2208 E, the weight of the edge e is indicated by we = p\u03c0(e). We refer to the set of edges that share the ith parameter pi by Ei = {e \u2208 E|\u03c0(e) = i}. That is, for any e1, e2 \u2208 Ei, \u03c0(e1) = \u03c0(e2) and hence we1 = we2 = p\u03c0(e1).\nSuch a feedforward network represents a function fN (G,\u03c0,p) : R|Vin| \u2192 R|Vout| as follows: For any input node v \u2208 Vin, its output hv is the corresponding coordinate of the input vector x \u2208 R|Vin|. For each internal node v, the output is defined recursively as hv = [\u2211 (u\u2192v)\u2208E wu\u2192v \u00b7 hu ] + where [z]+ = max(z, 0) is the ReLU activation function1. For output nodes v \u2208 Vout, no non-linearity is applied and their output hv = \u2211 (u\u2192v)\u2208E wu\u2192v \u00b7hu determines the corresponding coordinate of the computed function fN (G,\u03c0,p)(x). Since we will fix the graph G and the mapping \u03c0 and learn the parameters p, we use the shorthand fp = fN (G,\u03c0,p) to refer to the function implemented by parameters p. The goal of training is to find parameters p that minimize some error functional L(fp) that depends on p only through the function fp. E.g. in supervised learning L(f) = E [loss(f(x), y)] and this is typically done by minimizing an empirical estimate of this expectation.\nIf the mapping \u03c0 is a one-to-one mapping, then there is no weight sharing and it corresponds to standard feedforward networks. On the other hand, weight sharing exists if \u03c0 is a many-to-one mapping. Two well-known examples of feedforward networks with shared weights are convolutional and recurrent networks. We mostly use the general notation of feedforward networks with shared weights throughout the paper as this will be more general and simplifies the development and notation. However, when focusing on RNNs, it is helpful to discuss them using a more familiar notation which we briefly introduce next.\nRecurrent Neural Networks Time-unfolded RNNs are feedforward networks with shared weights that map an input sequence to an output sequence. Each input node corresponds to either a coordinate of the input vector at a particular time step or a hidden unit at time 0. Each output node also corresponds to a coordinate of the output at a specific time step. Finally, each internal node refers to some hidden unit at time t \u2265 1. When discussing RNNs, it is useful to refer to different layers and the values calculated at different time-steps. We use a notation for RNN structures in which the nodes are partitioned into layers and hit denotes the output of nodes in layer i at time step t. Let x = (x1, . . . ,xT ) be the input at different time steps where T is the\n1The bias terms can be modeled by having an additional special node vbias that is connected to all internal and output nodes, where hvbias = 1.\nmaximum number of propagations through time and we refer to it as the length of the RNN. For 0 \u2264 i < d, let Wiin and W i rec be the input and recurrent parameter matrices of layer i and Wout be the output parameter matrix. Table 1 shows forward computations for RNNs.The output of the function implemented by RNN can then be calculated as fW,t(x) = hdt . Note that in this notations, weight matrices Win, Wrec and Wout correspond to \u201cfree\u201d parameters of the model that are shared in different time steps."}, {"heading": "3 Non-Saturating Activation Functions", "text": "The choice of activation function for neural networks can have a large impact on optimization. We are particularly concerned with the distinction between \u201csaturating\u201d and \u201cnon-starting\u201d activation functions. We consider only monotone activation functions and say that a function is \u201csaturating\u201d if it is bounded\u2014this includes, e.g. sigmoid, hyperbolic tangent and the piecewise-linear ramp activation functions. Boundedness necessarily implies that the function values converge to finite values at negative and positive infinity, and hence asymptote to horizontal lines on both sides. That is, the derivative of the activation converges to zero as the input goes to both \u2212\u221e and +\u221e. Networks with saturating activations therefore have a major shortcoming: the vanishing gradient problem [6]. The problem here is that the gradient disappears when the magnitude of the input to an activation is large (whether the unit is very \u201cactive\u201d or very \u201cinactive\u201d) which makes the optimization very challenging.\nWhile sigmoid and hyperbolic tangent have historically been popular choices for fully connected feedforward and convolutional neural networks, more recent works have shown undeniable advantages of non-saturating activations such as ReLU, which is now the standard choice for fully connected and Convolutional networks [15, 10]. Non-saturating activations, including the ReLU, are typically still bounded from below and asymptote to a horizontal line, with a vanishing derivative, at \u2212\u221e. But they are unbounded from above, enabling their derivative to remain bounded away from zero as the input goes to +\u221e. Using ReLUs enables gradients to not vanish along activated paths and thus can provide a stronger signal for training.\nHowever, for recurrent neural networks, using ReLU activations is challenging in a different way, as even a small change in the direction of the leading eigenvector of the recurrent weights could get amplified and potentially lead to the explosion in forward or backward propagation [1].\nTo understand this, consider a long path from an input in the first element of the sequence to an output of the last element, which passes through the same RNN edge at each step (i.e. through many edges in some Ei in the shared-parameter representation). The length of this path, and the number of times it passes through edges associated with a single parameter, is proportional to the sequence length, which could easily be a few hundred or more. The effect of this parameter on the path is therefore exponentiated by the sequence length, as are gradient updates for this parameter, which could lead to parameter explosion unless an extremely small step size is used.\nUnderstanding the geometry of RNNs with ReLUs could helps us deal with the above issues more effectively. We next investigate some properties of geometry of RNNs with ReLU activations.\nInvariances in Feedforward Nets with Shared Weights\nFeedforward networks (with or without shared weights) are highly over-parameterized, i.e. there are many parameter settings p that represent the same function fp. Since our true object of interest is the function\nf , and not the identity p of the parameters, it would be beneficial if optimization would depend only on fp and not get \u201cdistracted\u201d by difference in p that does not affect fp. It is therefore helpful to study the transformations on the parameters that will not change the function presented by the network and come up with methods that their performance is not affected by such transformations.\nDefinition 1. We say a network N is invariant to a transformation T if for any parameter setting p, fp = fT (p). Similarly, we say an update rule A is invariant to T if for any p, fA(p) = fA(T (p)).\nInvariances have also been studied as different mappings from the parameter space to the same function space [19] while we define the transformation as a mapping inside a fixed parameter space. A very important invariance in feedforward networks is node-wise rescaling [17]. For any internal node v and any scalar \u03b1 > 0, we can multiply all incoming weights into v (i.e. wu\u2192v for any (u \u2192 v) \u2208 E) by \u03b1 and all the outgoing weights (i.e. wv\u2192u for any (v \u2192 u) \u2208 E) by 1/\u03b1 without changing the function computed by the network. Not all node-wise rescaling transformations can be applied in feedforward nets with shared weights. This is due to the fact that some weights are forced to be equal and therefore, we are only allowed to change them by the same scaling factor.\nDefinition 2. Given a network N , we say an invariant transformation T\u0303 that is defined over edge weights (rather than parameters) is feasible for parameter mapping \u03c0 if the shared weights remain equal after the transformation, i.e. for any i and for any e, e\u2032 \u2208 Ei, T\u0303 (w)e = T\u0303 (w)e\u2032 .\nTherefore, it is helpful to understand what are the feasible node-wise rescalings for RNNs. In the following theorem, we characterize all feasible node-wise invariances in RNNs.\nTheorem 1. For any \u03b1 such that \u03b1ij > 0, any Recurrent Neural Network with ReLU activation is invariant to the transformation T\u03b1 ([Win,Wrec,Wout]) = [Tin,\u03b1 (Win) , Trec,\u03b1 (Wrec) , Tout,\u03b1 (Wout)] where for any i, j, k:\nTin,\u03b1(Win)i[j, k] = { \u03b1ijW i in[j, k] i = 1,(\n\u03b1ij/\u03b1 i\u22121 k ) Wiin[j, k] 1 < i < d,\n(1)\nTrec,\u03b1(Wrec)i[j, k] = ( \u03b1ij/\u03b1 i k ) Wirec[j, k], Tout,\u03b1(Wout)[j, k] = ( 1/\u03b1d\u22121k ) Wout[j, k].\nFurthermore, any feasible node-wise rescaling transformation can be presented in the above form.\nThe proofs of all theorems and lemmas are given in Appendix A. The above theorem shows that there are many transformations under which RNNs represent the same function. An example of such invariances is shown in Fig. 1. Therefore, we would like to have optimization algorithms that are invariant to these transformations and in order to do so, we need to look at measures that are invariant to such mappings."}, {"heading": "4 Path-SGD for Networks with Shared Weights", "text": "As we discussed, optimization is inherently tied to a choice of geometry, here represented by a choice of complexity measure or \u201cnorm\u201d2. Furthermore, we prefer using an invariant measure which could then lead to an invariant optimization method. In Section 4.1 we introduce the path-regularizer and in Section 4.2, the derived Path-SGD optimization algorithm for standard feed-forward networks. Then in Section 4.3 we extend these notions also to networks with shared weights, including RNNs, and present two invariant optimization algorithms based on it. In Section 4.4 we show how these can be implemented efficiently using forward and backward propagations."}, {"heading": "4.1 Path-regularizer", "text": "The path-regularizer is the sum over all paths from input nodes to output nodes of the product of squared weights along the path. To define it formally, let P be the set of directed paths from input to output units so that for any path \u03b6 = ( \u03b60, . . . , \u03b6len(\u03b6) ) \u2208 P of length len(\u03b6), we have that \u03b60 \u2208 Vin, \u03b6len(\u03b6) \u2208 Vout and for any 0 \u2264 i \u2264 len(\u03b6) \u2212 1, (\u03b6i \u2192 \u03b6i+1) \u2208 E. We also abuse the notation and denote e \u2208 \u03b6 if for some i, e = (\u03b6i, \u03b6i+1). Then the path regularizer can be written as:\n\u03b32net(w) = \u2211 \u03b6\u2208P len(\u03b6)\u22121\u220f i=0 w2\u03b6i\u2192\u03b6i+1 (2)\nEquivalently, the path-regularizer can be defined recursively on the nodes of the network as:\n\u03b32v(w) = \u2211\n(u\u2192v)\u2208E\n\u03b32u(w)w 2 u\u2192v , \u03b3 2 net(w) = \u2211 u\u2208Vout \u03b32u(w) (3)"}, {"heading": "4.2 Path-SGD for Feedforward Networks", "text": "Path-SGD is an approximate steepest descent step with respect to the path-norm. More formally, for a network without shared weights, where the parameters are the weights themselves, consider the diagonal quadratic approximation of the path-regularizer about the current iterate w(t):\n\u03b3\u03022net(w (t) + \u2206w) = \u03b32net(w (t)) + \u2329 \u2207\u03b32net(w(t)),\u2206w \u232a + 1\n2 \u2206w> diag\n( \u22072\u03b32net(w(t)) ) \u2206w (4)\nUsing the corresponding quadratic norm \u2016w \u2212w\u2032\u20162\u03b3\u03022net(w(t)+\u2206w) = 1 2 \u2211 e\u2208E \u22022\u03b32net \u2202w2e\n(we \u2212 w\u2032e)2, we can define an approximate steepest descent step as:\nw(t+1) = min w\n\u03b7 \u2329 \u2207L(w),w \u2212w(t) \u232a + \u2225\u2225\u2225w \u2212w(t)\u2225\u2225\u22252\n\u03b3\u03022net(w (t)+\u2206w)\n. (5)\nSolving (5) yields the update:\nw(t+1)e = w (t) e \u2212\n\u03b7\n\u03bae(w(t))\n\u2202L\n\u2202we (w(t)) where: \u03bae(w) =\n1\n2\n\u22022\u03b32net(w)\n\u2202w2e . (6)\nThe stochastic version that uses a subset of training examples to estimate \u2202L\u2202wu\u2192v (w (t)) is called Path-SGD [16]. We now show how Path-SGD can be extended to networks with shared weights. 2The path-norm which we define is a norm on functions, not on weights, but as we prefer not getting into this technical discussion here, we use the term \u201cnorm\u201d very loosely to indicate some measure of magnitude [18]."}, {"heading": "4.3 Extending to Networks with Shared Weights", "text": "When the networks has shared weights, the path-regularizer is a function of parameters p and therefore the quadratic approximation should also be with respect to the iterate p(t) instead of w(t) which results in the following update rule:\np(t+1) = min p \u03b7 \u2329 \u2207L(p),p\u2212 p(t) \u232a + \u2225\u2225\u2225p\u2212 p(t)\u2225\u2225\u2225\n\u03b3\u03022net(p (t)+\u2206p)\n. (7)\nwhere \u2016p\u2212 p\u2032\u20162\u03b3\u03022net(p(t)+\u2206p) = 1 2 \u2211m i=1 \u22022\u03b32net \u2202p2i (pi \u2212 p\u2032i)2. Solving (7) gives the following update:\np (t+1) i = p (t) i \u2212\n\u03b7\n\u03bai(p(t))\n\u2202L \u2202pi (p(t)) where: \u03bai(p) = 1 2\n\u22022\u03b32net(p)\n\u2202p2i . (8)\nThe second derivative terms \u03bai are specified in terms of their path structure as follows:\nLemma 1. \u03bai(p) = \u03ba (1) i (p) + \u03ba (2) i (p) where\n\u03ba (1) i (p) = \u2211 e\u2208Ei \u2211 \u03b6\u2208P 1e\u2208\u03b6 len(\u03b6)\u22121\u220f j=0\ne6=(\u03b6j\u2192\u03b6j+1)\np2\u03c0(\u03b6j\u2192\u03b6j+1) = \u2211 e\u2208Ei \u03bae(w), (9)\n\u03ba (2) i (p) = p 2 i \u2211 e1,e2\u2208Ei e1 6=e2 \u2211 \u03b6\u2208P 1e1,e2\u2208\u03b6 len(\u03b6)\u22121\u220f j=0\ne1 6=(\u03b6j\u2192\u03b6j+1) e2 6=(\u03b6j\u2192\u03b6j+1)\np2\u03c0(\u03b6j\u2192\u03b6j+1), (10)\nand \u03bae(w) is defined in (6).\nThe second term \u03ba(2)i (p) measures the effect of interactions between edges corresponding to the same parameter (edges from the same Ei) on the same path from input to output. In particular, if for any path from an input unit to an output unit, no two edges along the path share the same parameter, then \u03ba(2)(p) = 0. For example, for any feedforward or Convolutional neural network, \u03ba(2)(p) = 0. But for RNNs, there certainly are multiple edges sharing a single parameter on the same path, and so we could have \u03ba(2)(p) 6= 0.\nThe above lemma gives us a precise update rule for the approximate steepest descent with respect to the path-regularizer. The following theorem confirms that the steepest descent with respect to this regularizer is also invariant to all feasible node-wise rescaling for networks with shared weights.\nTheorem 2. For any feedforward networks with shared weights, the update (8) is invariant to all feasible node-wise rescalings. Moreover, a simpler update rule that only uses \u03ba(1)i (p) in place of \u03bai(p) is also invariant to all feasible node-wise rescalings.\nEquations (9) and (10) involve a sum over all paths in the network which is exponential in depth of the network. However, we next show that both of these equations can be calculated efficiently."}, {"heading": "4.4 Simple and Efficient Computations for RNNs", "text": "We show how to calculate \u03ba(1)i (p) and \u03ba (2) i (p) by considering a network with the same architecture but with squared weights:\nTheorem 3. For any networkN (G, \u03c0, p), considerN (G, \u03c0, p\u0303) where for any i, p\u0303i = p2i . Define the function g : R|Vin| \u2192 R to be the sum of outputs of this network: g(x) = \u2211|Vout|i=1 fp\u0303(x)[i]. Then \u03ba(1) and \u03ba(2) can be calculated as follows where 1 is the all-ones input vector:\n\u03ba(1)(p) = \u2207p\u0303g(1), \u03ba(2)i (p) = \u2211\n(u\u2192v),(u\u2032\u2192v\u2032)\u2208Ei (u\u2192v)6=(u\u2032\u2192v\u2032)\np\u0303i \u2202g(1)\n\u2202hv\u2032(p\u0303)\n\u2202hu\u2032(p\u0303) \u2202hv(p\u0303) hu(p\u0303). (11)\nIn the process of calculating the gradient \u2207p\u0303g(1), we need to calculate hu(p\u0303) and \u2202g(1)/\u2202hv(p\u0303) for any u, v. Therefore, the only remaining term to calculate (besides\u2207p\u0303g(1)) is \u2202hu\u2032(p\u0303)/\u2202hv(p\u0303).\nRecall that T is the length (maximum number of propagations through time) and d is the number of layers in an RNN. Let H be the number of hidden units in each layer and B be the size of the mini-batch. Then calculating the gradient of the loss at all points in the minibatch (the standard work required for any mini-batch gradient approach) requires time O(BdTH2). In order to calculate \u03ba(1)i (p), we need to calculate the gradient \u2207p\u0303g(1) of a similar network at a single input\u2014so the time complexity is just an additional O(dTH2). The second term \u03ba(2)(p) can also be calculated for RNNs in O(dTH2(T +H)) 3. Therefore, the ratio of time complexity of calculating the first term and second term with respect to the gradient over mini-batch isO(1/B) andO((T+H)/B) respectively. Calculating only \u03ba(1)i (p) is therefore very cheap with minimal per-minibatch cost, while calculating \u03ba(2)i (p) might be expensive for large networks. Beyond the low computational cost, calculating \u03ba(1)i (p) is also very easy to implement as it requires only taking the gradient with respect to a standard feed-forward calculation in a network with slightly modified weights\u2014with most deep learning libraries it can be implemented very easily with only a few lines of code."}, {"heading": "5 Experiments", "text": ""}, {"heading": "5.1 The Contribution of the Second Term", "text": "As we discussed in section 4.4, the second term \u03ba(2) in the update rule can be computationally expensive for large networks. In this section we investigate the significance of the second term and show that at least in our experiments, the contribution of the second term is negligible. To compare the two terms \u03ba(1) and \u03ba(2), we train a single layer RNN with H = 200 hidden units for the task of word-level language modeling on Penn Treebank (PTB) Corpus [13]. Fig. 2 compares the performance of SGD vs. Path-SGD with/without \u03ba(2). We clearly see that both version of Path-SGD are performing very similarly and both of them outperform SGD significantly. This results in Fig. 2 suggest that the first term is more significant and therefore we can ignore the second term.\nTo better understand the importance of the two terms, we compared the ratio of the norms \u2225\u2225\u03ba(2)\u2225\u2225 2 / \u2225\u2225\u03ba(1)\u2225\u2225\n2 for different RNN lengths T and number of hidden units H . The table in Fig. 2 shows that the contribution of the second term is bigger when the network has fewer number of hidden units and the length of the RNN is larger (H is small and T is large). However, in many cases, it appears that the first term has a much bigger contribution in the update step and hence the second term can be safely ignored. Therefore, in the rest of our experiments, we calculate the Path-SGD updates only using the first term \u03ba(1).\n3 For an RNN, \u03ba(2)(Win) = 0 and \u03ba(2)(Wout) = 0 because only recurrent weights are can be shared multiple times along an input-output path. \u03ba(2)(Wrec) can be written and calculated in the matrix form: \u03ba(2)(Wirec) = W\u2032irec \u2211T\u22123\nt1=0\n[(( W\u2032irec )t1)> \u2211T\u2212t1\u22121t2=2 \u2202g(1)\u2202hi t1+t2+1 (p\u0303) ( hit2 (p\u0303) )>] where for any i, j, k we have W\u2032irec[j, k] = (Wirec[j, k])2. The only\nterms that require extra computation are powers of Wrec which can be done in O(dTH3) and the rest of the matrix computations need O(dT 2H2)."}, {"heading": "5.2 Synthetic Problems with Long-term Dependencies", "text": "Training Recurrent Neural Networks is known to be hard for modeling long-term dependencies due to the gradient vanishing/exploding problem [6, 2]. In this section, we consider synthetic problems that are specifically designed to test the ability of a model to capture the long-term dependency structure. Specifically, we consider the addition problem and the sequential MNIST problem.\nAddition problem: The addition problem was introduced in [7]. Here, each input consists of two sequences of length T , one of which includes numbers sampled from the uniform distribution with range [0, 1] and the other sequence serves as a mask which is filled with zeros except for two entries. These two entries indicate which of the two numbers in the first sequence we need to add and the task is to output the result of this addition. Sequential MNIST: In sequential MNIST, each digit image is reshaped into a sequence of length 784, turning the digit classification task into sequence classification with long-term dependencies [12, 1].\nFor both tasks, we closely follow the experimental protocol in [12]. We train a single-layer RNN consisting of 100 hidden units with path-SGD, referred to as RNN-Path. We also train an RNN of the same size with identity initialization, as was proposed in [12], using SGD as our baseline model, referred to as IRNN. We performed grid search for the learning rates over {10\u22122, 10\u22123, 10\u22124} for both our model and the baseline. Non-recurrent weights were initialized from the uniform distribution with range [\u22120.01, 0.01]. Similar to [1], we found the IRNN to be fairly unstable (with SGD optimization typically diverging). Therefore for IRNN, we ran 10 different initializations and picked the one that did not explode to show its performance.\nIn our first experiment, we evaluate Path-SGD on the addition problem. The results are shown in Fig. 3 with increasing the length T of the sequence: {100, 400, 750}. We note that this problem becomes much harder as T increases because the dependency between the output (the sum of two numbers) and the corresponding inputs becomes more distant. We also compare RNN-Path with the previously published results, including identity initialized RNN [12] (IRNN), unitary RNN [1] (uRNN), and np-RNN4 introduced by [22]. Table 2 shows the effectiveness of using Path-SGD. Perhaps more surprisingly, with the help of path-normalization, a simple RNN with the identity initialization is able to achieve a 0% error on the sequences of length 750, whereas all the other methods, including LSTMs, fail. This shows that Path-SGD may help stabilize the training and alleviate the gradient problem, so as to perform well on longer sequence. We next tried to model the sequences length of 1000, but we found that for such very long sequences RNNs, even with Path-SGD, fail to learn.\n4The original paper does not include any result for 750, so we implemented np-RNN for comparison. However, in our implementation the np-RNN is not able to even learn sequences of length of 200. Thus we put \u201c>2\u201d for length of 750.\nAdding Adding Adding 100 400 750 sMNIST\nIRNN [12] 0 16.7 16.7 5.0 uRNN [1] 0 3 16.7 4.9 LSTM [1] 0 2 16.7 1.8 np-RNN[22] 0 2 >2 3.1\nIRNN 0 0 16.7 7.1 RNN-Path 0 0 0 3.1\nTable 2: Test error (MSE) for the adding problem with different input sequence lengths and test classification error for the sequential MNIST.\nPTB text8\nRNN+smoothReLU [20] - 1.55 HF-MRNN [14] 1.42 1.54 RNN-ReLU[11] 1.65 - RNN-tanh[11] 1.55 - TRec,\u03b2 = 500[11] 1.48 -\nRNN-ReLU 1.55 1.65 RNN-tanh 1.58 1.70 RNN-Path 1.47 1.58 LSTM 1.41 1.52\nTable 3: Test BPC for PTB and text8.\nNext, we evaluate Path-SGD on the Sequential MNIST problem. Table 2, right column, reports test error rates achieved by RNN-Path compared to the previously published results. Clearly, using Path-SGD helps RNNs achieve better generalization. In many cases, RNN-Path outperforms other RNN methods (except for LSTMs), even for such a long-term dependency problem."}, {"heading": "5.3 Language Modeling Tasks", "text": "In this section we evaluate Path-SGD on a language modeling task. We consider two datasets, Penn Treebank (PTB-c) and text8 5. PTB-c: We performed experiments on a tokenized Penn Treebank Corpus, following the experimental protocol of [11]. The training, validations and test data contain 5017k, 393k and 442k characters respectively. The alphabet size is 50, and each training sequence is of length 50. text8: The text8 dataset contains 100M characters from Wikipedia with an alphabet size of 27. We follow the data partition of [14], where each training sequence has a length of 180. Performance is evaluated using bits-per-character (BPC) metric, which is log2 of perplexity.\nSimilar to the experiments on the synthetic datasets, for both tasks, we train a single-layer RNN consisting of 2048 hidden units with path-SGD (RNN-Path). Due to the large dimension of hidden space, SGD can take a fairly long time to converge. Instead, we use Adam optimizer [8] to help speed up the training, where we simply use the path-SGD gradient as input to the Adam optimizer.\nWe also train three additional baseline models: a ReLU RNN with 2048 hidden units, a tanh RNN with 2048 hidden units, and an LSTM with 1024 hidden units, all trained using Adam. We performed grid search for learning rate over {10\u22123, 5 \u00b7 10\u22124, 10\u22124} for all of our models. For ReLU RNNs, we initialize the recurrent matrices from uniform[\u22120.01, 0.01], and uniform[\u22120.2, 0.2] for non-recurrent weights. For LSTMs, we use orthogonal initialization [21] for the recurrent matrices and uniform[\u22120.01, 0.01] for non-recurrent weights. The results are summarized in Table 3.\n5http://mattmahoney.net/dc/textdata\nWe also compare our results to an RNN that uses hidden activation regularizer [11] (TRec,\u03b2 = 500), Multiplicative RNNs trained by Hessian Free methods [14] (HF-MRNN), and an RNN with smooth version of ReLU [20]. Table 3 shows that path-normalization is able to outperform RNN-ReLU and RNN-tanh, while at the same time shortening the performance gap between plain RNN and other more complicated models (e.g. LSTM by 57% on PTB and 54% on text8 datasets). This demonstrates the efficacy of path-normalized optimization for training RNNs with ReLU activation."}, {"heading": "6 Conclusion", "text": "We investigated the geometry of RNNs in a broader class of feedforward networks with shared weights and showed how understanding the geometry can lead to significant improvements on different learning tasks. Designing an optimization algorithm with a geometry that is well-suited for RNNs, we closed over half of the performance gap between vanilla RNNs and LSTMs. This is particularly useful for applications in which we seek compressed models with fast prediction time that requires minimum storage; and also a step toward bridging the gap between LSTMs and RNNs."}, {"heading": "Acknowledgments", "text": "This research was supported in part by an NSF RI-AF award and by Intel ICRI-CI. We thank Saizheng Zhang for sharing a base code for RNNs."}, {"heading": "A Proofs", "text": "A.1 Proof of Theorem 1\nWe first show that any RNN is invariant to T\u03b1 by induction on layers and time-steps. More specifically, we prove that for any 0 \u2264 t \u2264 T and 1 \u2264 i < d, hit (T\u03b1(W)) [j] = \u03b1ijhit(W)[j]. The statement is clearly true for t = 0; because for any i, j, hi0 (T\u03b1(W)) [j] = \u03b1ijhi0(W)[j] = 0.\nNext, we show that for i = 1, if we assume that the statement is true for t = t\u2032, then it is also true for t = t\u2032 + 1:\nh1t\u2032+1 (T\u03b1(W)) [j] = \u2211 j\u2032 Tin,\u03b1(Win)1[j, j\u2032]xt\u2032+1[j\u2032] + Trec,\u03b1(Wrec)1[j, j\u2032]h1t\u2032 (T\u03b1(W)) [j\u2032]  +\n= \u2211 j\u2032 \u03b11jW 1 in[j, j \u2032]xt\u2032+1[j \u2032] + ( \u03b11j/\u03b1 1 j\u2032 ) W1rec[j, j \u2032]\u03b11j\u2032h 1 t\u2032(W))[j \u2032]  + = \u03b11jh i t(W)[j]\nWe now need to prove the statement for 1 < i < d. Assuming that the statement is true for t \u2264 t\u2032 and the layers before i, we have:\nhit\u2032+1 (T\u03b1(W)) [j] = \u2211 j\u2032 Tin,\u03b1(Win)i[j, j\u2032]hi\u22121t\u2032+1 (T\u03b1(W)) [j\u2032] + Trec,\u03b1(Wrec)i[j, j\u2032]hit\u2032 (T\u03b1(W)) [j\u2032]  +\n= \u2211 j\u2032 \u03b1ij \u03b1i\u22121j\u2032 Wiin[j, j \u2032]\u03b1i\u22121j\u2032 h i\u22121 t\u2032+1(W))[j \u2032] + \u03b1ij \u03b1ij\u2032 Wirec[j, j \u2032]\u03b1ij\u2032h i t\u2032(W))[j \u2032]  + = \u03b1ijh i t(W)[j]\nFinally, we can show that the output is invariant for any j at any time step t:\nfT (W),t(xt)[j] = \u2211 j\u2032 Tout,\u03b1(Wout)[j, j\u2032]hd\u22121t (T\u03b1(W)[j\u2032] = \u2211 j\u2032 (1/\u03b1d\u22121j\u2032 )Wout[j, j \u2032]\u03b1d\u22121j\u2032 h d\u22121 t (W)[j \u2032]\n= \u2211 j\u2032 Wout[j, j \u2032]hd\u22121t (W)[j \u2032] = fW,t(xt)[j]\nWe now show that any feasible node-wise rescaling can be presented as T\u03b1. Recall that node-wise rescaling invariances for a general feedforward network can be written as T\u0303\u03b2(w)u\u2192v = (\u03b2v/\u03b2u)wu\u2192v for some \u03b2 where \u03b2v > 0 for internal nodes and \u03b2v = 1 for any input/output nodes. An RNN with T = 0 has no weight sharing and for each node v with index j in layer i, we have \u03b2v = \u03b1ij . For any T > 0 however, we there is no invariance that is not already counted. The reason is that by fixing the values of \u03b2v for the nodes in time step 0, due to the feasibility, the values of \u03b2 for nodes in other time-steps should be tied to the corresponding value in time step 0. Therefore, all invariances are included and can be presented in form of T\u03b1.\nA.2 Proof of Lemma 1\nWe prove the statement simply by calculating the second derivative of the path-regularizer with respect to each parameter:\n\u03bai(p) = 1\n2 \u22022\u03b32net \u2202p2i = 1 2 \u2202 \u2202pi  \u2202 \u2202pi \u2211 \u03b6\u2208P len(\u03b6)\u22121\u220f j=0 w2\u03b6j\u2192\u03b6j+1  = 1\n2\n\u2202\n\u2202pi  \u2202 \u2202pi \u2211 \u03b6\u2208P len(\u03b6)\u22121\u220f j=0 p2\u03c0(\u03b6j\u2192\u03b6j+1)  = 1 2 \u2211 \u03b6\u2208P \u2202 \u2202pi  \u2202 \u2202pi len(\u03b6)\u22121\u220f j=0 p2\u03c0(\u03b6j\u2192\u03b6j+1) \nTaking the second derivative then gives us both terms after a few calculations:\n\u03bai(p) = 1\n2 \u2211 \u03b6\u2208P \u2202 \u2202pi  \u2202 \u2202pi len(\u03b6)\u22121\u220f j=0 p2\u03c0(\u03b6j\u2192\u03b6j+1)  = \u2211 \u03b6\u2208P \u2202 \u2202pi pi \u2211 e\u2208Ei 1e\u2208\u03b6 len(\u03b6)\u22121\u220f j=0\ne 6=(\u03b6j\u2192\u03b6j+1\np2\u03c0(\u03b6j\u2192\u03b6j+1) \n= \u2211 \u03b6\u2208P pi \u2202\u2202pi \u2211 e\u2208Ei 1e\u2208\u03b6 len(\u03b6)\u22121\u220f j=0\ne 6=(\u03b6j\u2192\u03b6j+1\np2\u03c0(\u03b6j\u2192\u03b6j+1) + \u2211 e\u2208Ei 1e\u2208\u03b6 len(\u03b6)\u22121\u220f j=0\ne 6=(\u03b6j\u2192\u03b6j+1\np2\u03c0(\u03b6j\u2192\u03b6j+1) \n= p2i \u2211\ne1,e2\u2208Ei e1 6=e2\n \u2211 \u03b6\u2208P 1e1,e2\u2208\u03b6 len(\u03b6)\u22121\u220f j=0\ne1 6=(\u03b6j\u2192\u03b6j+1) e2 6=(\u03b6j\u2192\u03b6j+1)\np2\u03c0(\u03b6j\u2192\u03b6j+1) + \u2211 e\u2208Ei \u2211 \u03b6\u2208P 1e\u2208\u03b6 len(\u03b6)\u22121\u220f j=0\ne 6=(\u03b6j\u2192\u03b6j+1)\np2\u03c0(\u03b6j\u2192\u03b6j+1) \nA.3 Proof of Theorem 2\nNode-wise rescaling invariances for a feedforward network can be written as T\u03b2(w)u\u2192v = (\u03b2v/\u03b2u)wu\u2192v for some \u03b2 where \u03b2v > 0 for internal nodes and \u03b2v = 1 for any input/output nodes. Any feasible invariance for a network with shared weights can also be written in the same form. The only difference is that some of \u03b2vs are now tied to each other in a way that shared weights have the same value after transformation. First, note that since the network is invariant to the transformation, the following statement holds by an induction similar to Theorem 1 but in the backward direction:\n\u2202L \u2202hv (T\u03b2(p)) = 1 \u03b2v \u2202L \u2202hu (p) (12)\nfor any (u \u2192 v) \u2208 E. Furthermore, by the proof of the Theorem 1 we have that for any (u \u2192 v) \u2208 E, hu(T\u03b2(p)) = \u03b2uhu(p). Therefore,\n\u2202L\n\u2202T\u03b2(p)i (T\u03b2(p)) = \u2211 (u\u2192v)\u2208Ei \u2202L \u2202hv (T\u03b2(p))hu(T\u03b2(p)) = \u03b2u\u2032 \u03b2v\u2032 \u2202L \u2202pi (p) (13)\nwhere (u\u2032 \u2192 v\u2032) \u2208 Ei. In order to prove the theorem statement, it is enough to show that for any edge (u\u2192 v) \u2208 Ei, \u03bai(T\u03b2(p)) = (\u03b2u/\u03b2v)2\u03bai(p) because this property gives us the following update:\nT\u03b2(p)i \u2212 \u03b7 \u03bai(T\u03b2(p)) \u2202L(T\u03b2(p)) \u2202T\u03b2(p)i = \u03b2v \u03b2u pi \u2212\n\u03b7\n(\u03b2u/\u03b2v)2\u03bai(p) \u03b2u \u03b2v \u2202L \u2202pi (p) = T\u03b2(p+)i\nTherefore, it is remained to show that for any edge (u\u2192 v) \u2208 Ei v, \u03bai(T\u03b2(p)) = (\u03b2u/\u03b2v)2\u03bai(p). We show that this is indeed true for both terms \u03ba(1) and \u03ba(2) separately.\nWe first prove the statement for \u03ba(1). Consider each path \u03b6 \u2208 P . By an inductive argument along the path, it is easy to see that multiplying squared weights along this path is invariant to the transformation:\nlen(\u03b6)\u22121\u220f j=0 T\u03b2(p)2\u03c0(\u03b6j\u2192\u03b6j+1) = len(\u03b6)\u22121\u220f j=0 p2\u03c0(\u03b6j\u2192\u03b6j+1)\nTherefore, we have that for any edge e \u2208 E and any \u03b6 \u2208 P ,\nlen(\u03b6)\u22121\u220f j=0\ne 6=(\u03b6j\u2192\u03b6j+1)\nT\u03b2(p)2\u03c0(\u03b6j\u2192\u03b6j+1) = ( \u03b2u \u03b2v )2 len(\u03b6)\u22121\u220f j=0\ne 6=(\u03b6j\u2192\u03b6j+1)\np2\u03c0(\u03b6j\u2192\u03b6j+1)\nTaking sum over all paths \u03b6 \u2208 P and all edges e = (u\u2192 v) \u2208 E completes the proof for \u03ba(1). Similarly for \u03ba(2), considering any two edges e1 6= e2 and any path \u03b6P , we have that:\nT\u03b2(p)2i len(\u03b6)\u22121\u220f j=0\ne1 6=(\u03b6j\u2192\u03b6j+1) e2 6=(\u03b6j\u2192\u03b6j+1)\nT\u03b2(p)2\u03c0(\u03b6j\u2192\u03b6j+1) = ( \u03b2v \u03b2u )2 p2i ( \u03b2u \u03b2v )4 len(\u03b6)\u22121\u220f j=0\ne1 6=(\u03b6j\u2192\u03b6j+1) e2 6=(\u03b6j\u2192\u03b6j+1)\np2\u03c0(\u03b6j\u2192\u03b6j+1)\nwhere (u\u2192 v) \u2208 Ei. Again, taking sum over all paths \u03b6 and all edges e1 6= e2 proves the statement for \u03ba(2) and consequently for \u03ba(1) + \u03ba(2).\nA.4 Proof of Theorem 3\nFirst, note that based on the definitions in the theorem statement, for any node v, hv(p\u0303) = \u03b32v(p) and therefore g(1) = \u03b32net(p). Using Lemma 1, main observation here is that for each edge e \u2208 Ei and each path \u03b6 \u2208 P , the corresponding term in \u03ba(1) is nothing but product of the squared weights along the path except the weights that correspond to the edge e:\n1e\u2208\u03b6 len(\u03b6)\u22121\u220f j=0\ne6=(\u03b6j\u2192\u03b6j+1)\np2\u03c0(\u03b6j\u2192\u03b6j+1)\nThis path can therefore be decomposed into a path from input to edge e and a path from edge e to the output. Therefore, for any edge e, we can factor out the number corresponding to the paths that go through e and rewrite \u03ba(1) as follows:\n\u03ba(1)(p) = \u2211\n(u\u2192v)\u2208Ei  \u2211 \u03b6\u2208Pin\u2192u len(\u03b6)\u22121\u220f j=0 p2\u03c0(\u03b6j\u2192\u03b6j+1)  \u2211 \u03b6\u2208Pv\u2192out len(\u03b6)\u22121\u220f j=0 p2\u03c0(\u03b6j\u2192\u03b6j+1)  (14)\nwhere Pin\u2192u is the set of paths from input nodes to node v and Pv\u2192out is defined similarly for the output nodes.\nBy induction on layers of N (G, \u03c0, p\u0303), we get the following:\n\u2211 \u03b6\u2208Pin\u2192u len(\u03b6)\u22121\u220f j=0 p2\u03c0(\u03b6j\u2192\u03b6j+1) = hu(p\u0303) (15)\n\u2211 \u03b6\u2208Pv\u2192out len(\u03b6)\u22121\u220f j=0 p2\u03c0(\u03b6j\u2192\u03b6j+1) = \u2202g(1) \u2202hv(p\u0303) (16)\nTherefore, \u03ba(1) can be written as:\n\u03ba(1)(p) = \u2211\n(u\u2192v)\u2208Ei\n\u2202g(1)\n\u2202hv(p\u0303) hu(p\u0303) = \u2211 (u\u2192v)\u2208Ei \u2202g(1) \u2202w\u2032u\u2192v = \u2202g(1) \u2202p\u0303i (17)\nNext, we show how to calculate the second term, i.e. \u03ba(2). Each term in \u03ba(2) corresponds to a path that goes through two edges. We can decompose such paths and rewrite \u03ba(2) similar to the first term:\n\u03ba(2)(p) = p2i \u2211\n(u\u2192v)\u2208Ei (u\u2032\u2192v\u2032)\u2208Ei\n(u\u2192v)6=(u\u2032\u2192v\u2032)\n \u2211 \u03b6\u2208Pin\u2192u len(\u03b6)\u220f j=0 p2\u03c0(\u03b6j\u2192\u03b6j+1)   \u2211 \u03b6\u2208Pv\u2192u\u2032 len(\u03b6)\u22121\u220f j=0 p2\u03c0(\u03b6j\u2192\u03b6j+1)  \u2211 \u03b6\u2208Pv\u2032\u2192out len(\u03b6)\u22121\u220f j=0 p2\u03c0(\u03b6j\u2192\u03b6j+1)\n =\n\u2211 (u\u2192v)\u2208Ei (u\u2032\u2192v\u2032)\u2208Ei\n(u\u2192v)6=(u\u2032\u2192v\u2032)\np\u0303i \u2202g(1)\n\u2202hv\u2032(p\u0303)\n\u2202hu\u2032(p\u0303) \u2202hv(p\u0303) hu(p\u0303)\nwhere Pu\u2192v is the set of all directed paths from node u to node v."}], "references": [{"title": "Unitary evolution recurrent neural networks", "author": ["Martin Arjovsky", "Amar Shah", "Yoshua Bengio"], "venue": "arXiv preprint arXiv:1511.06464,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2015}, {"title": "Learning long-term dependencies with gradient descent is difficult", "author": ["Yoshua Bengio", "Patrice Simard", "Paolo Frasconi"], "venue": "Neural Networks, IEEE Transactions on,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 1994}, {"title": "Learning phrase representations using RNN encoder\u2013decoder for statistical machine translation", "author": ["Kyunghyun Cho", "Bart Van Merri\u00ebnboer", "Caglar Gulcehre", "Dzmitry Bahdanau", "Fethi Bougares", "Holger Schwenk", "Yoshua Bengio"], "venue": "In Proceeding of the 2015 Conference on Empirical Methods in Natural Language Processing (EMNLP),", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2014}, {"title": "Towards end-to-end speech recognition with recurrent neural networks", "author": ["Alex Graves", "Navdeep Jaitly"], "venue": "In Proceeding of the International Conference on Machine Learning (ICML),", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2014}, {"title": "Deep compression: Compressing deep neural networks with pruning, trained quantization and huffman coding", "author": ["Song Han", "Huizi Mao", "William J Dally"], "venue": "In Proceeding of the International Conference on Learning Representations,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2016}, {"title": "The vanishing gradient problem during learning recurrent neural nets and problem solutions", "author": ["Sepp Hochreiter"], "venue": "International Journal of Uncertainty, Fuzziness and Knowledge-Based Systems,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 1998}, {"title": "Long short-term memory", "author": ["Sepp Hochreiter", "J\u00fcrgen Schmidhuber"], "venue": "Neural computation,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 1997}, {"title": "Adam: A method for stochastic optimization", "author": ["Diederik Kingma", "Jimmy Ba"], "venue": "In Proceeding of the International Conference on Learning Representations,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2015}, {"title": "Unifying visual-semantic embeddings with multimodal neural language models", "author": ["Ryan Kiros", "Ruslan Salakhutdinov", "Richard S Zemel"], "venue": "Transactions of the Association for Computational Linguistics,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2015}, {"title": "ImageNet classification with deep convolutional neural networks. In Advances in neural information processing systems (NIPS), pages", "author": ["Alex Krizhevsky", "Ilya Sutskever", "Geoffrey E Hinton"], "venue": null, "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2012}, {"title": "Regularizing RNNs by stabilizing activations", "author": ["David Krueger", "Roland Memisevic"], "venue": "In Proceeding of the International Conference on Learning Representations,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2016}, {"title": "A simple way to initialize recurrent networks of rectified linear units", "author": ["Quoc V Le", "Navdeep Jaitly", "Geoffrey E Hinton"], "venue": "arXiv preprint arXiv:1504.00941,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2015}, {"title": "Building a large annotated corpus of english: The penn treebank", "author": ["Mitchell P Marcus", "Mary Ann Marcinkiewicz", "Beatrice Santorini"], "venue": "Computational linguistics,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 1993}, {"title": "Subword language modeling with neural networks", "author": ["Tom\u00e1\u0161 Mikolov", "Ilya Sutskever", "Anoop Deoras", "Hai-Son Le", "Stefan Kombrink", "J Cernocky"], "venue": "(http://www.fit.vutbr.cz/ imikolov/rnnlm/char.pdf),", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2012}, {"title": "Rectified linear units improve restricted boltzmann machines", "author": ["Vinod Nair", "Geoffrey E Hinton"], "venue": "In Proceedings of the International Conference on Machine Learning (ICML),", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2010}, {"title": "Path-SGD: Path-normalized optimization in deep neural networks", "author": ["Behnam Neyshabur", "Ruslan Salakhutdinov", "Nathan Srebro"], "venue": "In Advanced in Neural Information Processsing Systems (NIPS),", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2015}, {"title": "Data-dependent path normalization in neural networks", "author": ["Behnam Neyshabur", "Ryota Tomioka", "Ruslan Salakhutdinov", "Nathan Srebro"], "venue": "In the International Conference on Learning Representations,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2016}, {"title": "Norm-based capacity control in neural networks", "author": ["Behnam Neyshabur", "Ryota Tomioka", "Nathan Srebro"], "venue": "In Proceeding of the 28th Conference on Learning Theory (COLT),", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2015}, {"title": "Riemannian metrics for neural networks ii: recurrent networks and learning symbolic data sequences", "author": ["Yann Ollivier"], "venue": "Information and Inference, page iav007,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2015}, {"title": "Regularization and nonlinearities for neural language models: when are they needed", "author": ["Marius Pachitariu", "Maneesh Sahani"], "venue": "arXiv preprint arXiv:1301.5650,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2013}, {"title": "Exact solutions to the nonlinear dynamics of learning in deep linear neural networks", "author": ["Andrew M Saxe", "James L McClelland", "Surya Ganguli"], "venue": "In International Conference on Learning Representations,", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2014}, {"title": "Improving performance of recurrent neural network with relu nonlinearity", "author": ["Sachin S. Talathi", "Aniket Vartak"], "venue": "In the International Conference on Learning Representations workshop track,", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2014}], "referenceMentions": [{"referenceID": 3, "context": "Recurrent Neural Networks (RNNs) have been found to be successful in a variety of sequence learning problems [4, 3, 9], including those involving long term dependencies (e.", "startOffset": 109, "endOffset": 118}, {"referenceID": 2, "context": "Recurrent Neural Networks (RNNs) have been found to be successful in a variety of sequence learning problems [4, 3, 9], including those involving long term dependencies (e.", "startOffset": 109, "endOffset": 118}, {"referenceID": 8, "context": "Recurrent Neural Networks (RNNs) have been found to be successful in a variety of sequence learning problems [4, 3, 9], including those involving long term dependencies (e.", "startOffset": 109, "endOffset": 118}, {"referenceID": 0, "context": ", [1, 23]).", "startOffset": 2, "endOffset": 9}, {"referenceID": 6, "context": "However, most of the empirical success has not been with \u201cplain\u201d RNNs but rather with alternate, more complex structures, such as Long Short-Term Memory (LSTM) networks [7] or Gated Recurrent Units (GRUs) [3].", "startOffset": 169, "endOffset": 172}, {"referenceID": 2, "context": "However, most of the empirical success has not been with \u201cplain\u201d RNNs but rather with alternate, more complex structures, such as Long Short-Term Memory (LSTM) networks [7] or Gated Recurrent Units (GRUs) [3].", "startOffset": 205, "endOffset": 208}, {"referenceID": 21, "context": "One motivation for insisting on plain RNNs, as opposed to LSTMs or GRUs, is because they are simpler and might be more appropriate for applications that require low-complexity design such as in mobile computing platforms [22, 5].", "startOffset": 221, "endOffset": 228}, {"referenceID": 4, "context": "One motivation for insisting on plain RNNs, as opposed to LSTMs or GRUs, is because they are simpler and might be more appropriate for applications that require low-complexity design such as in mobile computing platforms [22, 5].", "startOffset": 221, "endOffset": 228}, {"referenceID": 11, "context": "Improving training RNNs with ReLU activations has been the subject of some recent attention, with most research focusing on different initialization strategies [12, 22].", "startOffset": 160, "endOffset": 168}, {"referenceID": 21, "context": "Improving training RNNs with ReLU activations has been the subject of some recent attention, with most research focusing on different initialization strategies [12, 22].", "startOffset": 160, "endOffset": 168}, {"referenceID": 15, "context": "We build on prior work on the geometry and optimization in feed-forward networks, which uses the path-norm [16] (defined in Section 4) to determine a geometry leading to the path-SGD optimization method.", "startOffset": 107, "endOffset": 111}, {"referenceID": 5, "context": "Networks with saturating activations therefore have a major shortcoming: the vanishing gradient problem [6].", "startOffset": 104, "endOffset": 107}, {"referenceID": 14, "context": "While sigmoid and hyperbolic tangent have historically been popular choices for fully connected feedforward and convolutional neural networks, more recent works have shown undeniable advantages of non-saturating activations such as ReLU, which is now the standard choice for fully connected and Convolutional networks [15, 10].", "startOffset": 318, "endOffset": 326}, {"referenceID": 9, "context": "While sigmoid and hyperbolic tangent have historically been popular choices for fully connected feedforward and convolutional neural networks, more recent works have shown undeniable advantages of non-saturating activations such as ReLU, which is now the standard choice for fully connected and Convolutional networks [15, 10].", "startOffset": 318, "endOffset": 326}, {"referenceID": 0, "context": "However, for recurrent neural networks, using ReLU activations is challenging in a different way, as even a small change in the direction of the leading eigenvector of the recurrent weights could get amplified and potentially lead to the explosion in forward or backward propagation [1].", "startOffset": 283, "endOffset": 286}, {"referenceID": 18, "context": "Invariances have also been studied as different mappings from the parameter space to the same function space [19] while we define the transformation as a mapping inside a fixed parameter space.", "startOffset": 109, "endOffset": 113}, {"referenceID": 16, "context": "A very important invariance in feedforward networks is node-wise rescaling [17].", "startOffset": 75, "endOffset": 79}, {"referenceID": 15, "context": "The stochastic version that uses a subset of training examples to estimate \u2202L \u2202wu\u2192v (w (t)) is called Path-SGD [16].", "startOffset": 111, "endOffset": 115}, {"referenceID": 17, "context": "The path-norm which we define is a norm on functions, not on weights, but as we prefer not getting into this technical discussion here, we use the term \u201cnorm\u201d very loosely to indicate some measure of magnitude [18].", "startOffset": 210, "endOffset": 214}, {"referenceID": 12, "context": "To compare the two terms \u03ba(1) and \u03ba(2), we train a single layer RNN with H = 200 hidden units for the task of word-level language modeling on Penn Treebank (PTB) Corpus [13].", "startOffset": 169, "endOffset": 173}, {"referenceID": 5, "context": "2 Synthetic Problems with Long-term Dependencies Training Recurrent Neural Networks is known to be hard for modeling long-term dependencies due to the gradient vanishing/exploding problem [6, 2].", "startOffset": 188, "endOffset": 194}, {"referenceID": 1, "context": "2 Synthetic Problems with Long-term Dependencies Training Recurrent Neural Networks is known to be hard for modeling long-term dependencies due to the gradient vanishing/exploding problem [6, 2].", "startOffset": 188, "endOffset": 194}, {"referenceID": 6, "context": "Addition problem: The addition problem was introduced in [7].", "startOffset": 57, "endOffset": 60}, {"referenceID": 0, "context": "Here, each input consists of two sequences of length T , one of which includes numbers sampled from the uniform distribution with range [0, 1] and the other sequence serves as a mask which is filled with zeros except for two entries.", "startOffset": 136, "endOffset": 142}, {"referenceID": 11, "context": "Sequential MNIST: In sequential MNIST, each digit image is reshaped into a sequence of length 784, turning the digit classification task into sequence classification with long-term dependencies [12, 1].", "startOffset": 194, "endOffset": 201}, {"referenceID": 0, "context": "Sequential MNIST: In sequential MNIST, each digit image is reshaped into a sequence of length 784, turning the digit classification task into sequence classification with long-term dependencies [12, 1].", "startOffset": 194, "endOffset": 201}, {"referenceID": 11, "context": "For both tasks, we closely follow the experimental protocol in [12].", "startOffset": 63, "endOffset": 67}, {"referenceID": 11, "context": "We also train an RNN of the same size with identity initialization, as was proposed in [12], using SGD as our baseline model, referred to as IRNN.", "startOffset": 87, "endOffset": 91}, {"referenceID": 0, "context": "Similar to [1], we found the IRNN to be fairly unstable (with SGD optimization typically diverging).", "startOffset": 11, "endOffset": 14}, {"referenceID": 11, "context": "We also compare RNN-Path with the previously published results, including identity initialized RNN [12] (IRNN), unitary RNN [1] (uRNN), and np-RNN4 introduced by [22].", "startOffset": 99, "endOffset": 103}, {"referenceID": 0, "context": "We also compare RNN-Path with the previously published results, including identity initialized RNN [12] (IRNN), unitary RNN [1] (uRNN), and np-RNN4 introduced by [22].", "startOffset": 124, "endOffset": 127}, {"referenceID": 21, "context": "We also compare RNN-Path with the previously published results, including identity initialized RNN [12] (IRNN), unitary RNN [1] (uRNN), and np-RNN4 introduced by [22].", "startOffset": 162, "endOffset": 166}, {"referenceID": 11, "context": "Adding Adding Adding 100 400 750 sMNIST IRNN [12] 0 16.", "startOffset": 45, "endOffset": 49}, {"referenceID": 0, "context": "0 uRNN [1] 0 3 16.", "startOffset": 7, "endOffset": 10}, {"referenceID": 0, "context": "9 LSTM [1] 0 2 16.", "startOffset": 7, "endOffset": 10}, {"referenceID": 21, "context": "8 np-RNN[22] 0 2 >2 3.", "startOffset": 8, "endOffset": 12}, {"referenceID": 19, "context": "PTB text8 RNN+smoothReLU [20] - 1.", "startOffset": 25, "endOffset": 29}, {"referenceID": 13, "context": "55 HF-MRNN [14] 1.", "startOffset": 11, "endOffset": 15}, {"referenceID": 10, "context": "54 RNN-ReLU[11] 1.", "startOffset": 11, "endOffset": 15}, {"referenceID": 10, "context": "65 RNN-tanh[11] 1.", "startOffset": 11, "endOffset": 15}, {"referenceID": 10, "context": "55 TRec,\u03b2 = 500[11] 1.", "startOffset": 15, "endOffset": 19}, {"referenceID": 10, "context": "PTB-c: We performed experiments on a tokenized Penn Treebank Corpus, following the experimental protocol of [11].", "startOffset": 108, "endOffset": 112}, {"referenceID": 13, "context": "We follow the data partition of [14], where each training sequence has a length of 180.", "startOffset": 32, "endOffset": 36}, {"referenceID": 7, "context": "Instead, we use Adam optimizer [8] to help speed up the training, where we simply use the path-SGD gradient as input to the Adam optimizer.", "startOffset": 31, "endOffset": 34}, {"referenceID": 20, "context": "For LSTMs, we use orthogonal initialization [21] for the recurrent matrices and uniform[\u22120.", "startOffset": 44, "endOffset": 48}, {"referenceID": 10, "context": "We also compare our results to an RNN that uses hidden activation regularizer [11] (TRec,\u03b2 = 500), Multiplicative RNNs trained by Hessian Free methods [14] (HF-MRNN), and an RNN with smooth version of ReLU [20].", "startOffset": 78, "endOffset": 82}, {"referenceID": 13, "context": "We also compare our results to an RNN that uses hidden activation regularizer [11] (TRec,\u03b2 = 500), Multiplicative RNNs trained by Hessian Free methods [14] (HF-MRNN), and an RNN with smooth version of ReLU [20].", "startOffset": 151, "endOffset": 155}, {"referenceID": 19, "context": "We also compare our results to an RNN that uses hidden activation regularizer [11] (TRec,\u03b2 = 500), Multiplicative RNNs trained by Hessian Free methods [14] (HF-MRNN), and an RNN with smooth version of ReLU [20].", "startOffset": 206, "endOffset": 210}], "year": 2016, "abstractText": "We investigate the parameter-space geometry of recurrent neural networks (RNNs), and develop an adaptation of path-SGD optimization method, attuned to this geometry, that can learn plain RNNs with ReLU activations. On several datasets that require capturing long-term dependency structure, we show that path-SGD can significantly improve trainability of ReLU RNNs compared to RNNs trained with SGD, even with various recently suggested initialization schemes.", "creator": "LaTeX with hyperref package"}}}