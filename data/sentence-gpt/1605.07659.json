{"id": "1605.07659", "review": {"conference": "NIPS", "VERSION": "v1", "DATE_OF_SUBMISSION": "24-May-2016", "title": "Adaptive Newton Method for Empirical Risk Minimization to Statistical Accuracy", "abstract": "We consider empirical risk minimization for large-scale datasets. We introduce Ada Newton as an adaptive algorithm that uses Newton's method with adaptive sample sizes. The main idea of Ada Newton is to increase the size of the training set by a factor larger than one in a way that the minimization variable for the current training set is in the local neighborhood of the optimal argument of the next training set. As the algorithm uses Newton's algorithm, we have a way to do so by making a local neighborhood of the optimal argument of the next training set and the probability of obtaining the maximum number of parameters. The parameter is the value of the minimization variable in the local neighborhood, and is the value of the optimizer.\n\n\n\n\nThe algorithm also requires all the available parameters to avoid a known and likely unknown source of errors. The algorithm then uses a linear algorithm to approximate the values obtained. The resulting result is an algorithm with the minimum probability for the optimal argument of the next training set.\n\nIn practice, the algorithm achieves the maximum number of parameters, using randomization of the parameters and a random number generator. The algorithm then relies upon an algorithm of the same size, by which the algorithm would be most likely to produce a given optimal representation of each of the parameters in the local neighborhood of the optimum argument of the next training set. The algorithm's goal is to maximize the number of parameters that would be most likely to produce a given optimal representation of each of the parameters in the local neighborhood of the optimal argument of the next training set. The algorithm would also use a linear algorithm to approximate the values obtained. The algorithm then uses a linear algorithm to approximate the values obtained. The result is an algorithm with the minimum probability for the optimal argument of the next training set.\nThe algorithm then uses a linear algorithm to approximate the values obtained. The result is an algorithm with the minimum probability for the optimal argument of the next training set. The algorithm then uses a linear algorithm to approximate the values obtained. The result is an algorithm with the minimum probability for the optimal argument of the next training set. The algorithm then uses a linear algorithm to approximate the values obtained. The result is an algorithm with the minimum probability for the optimal argument of the next training set. The algorithm then uses a linear algorithm to approximate the values obtained. The result is an algorithm with the minimum probability for the optimal argument of the next training set. The algorithm then uses a linear algorithm to approximate the values obtained. The result is an algorithm with the minimum probability for the optimal", "histories": [["v1", "Tue, 24 May 2016 21:02:50 GMT  (50kb,D)", "http://arxiv.org/abs/1605.07659v1", null]], "reviews": [], "SUBJECTS": "cs.LG math.OC", "authors": ["aryan mokhtari", "hadi daneshmand", "aur\u00e9lien lucchi", "thomas hofmann", "alejandro ribeiro"], "accepted": true, "id": "1605.07659"}, "pdf": {"name": "1605.07659.pdf", "metadata": {"source": "CRF", "title": "Adaptive Newton Method for Empirical Risk Minimization to Statistical Accuracy", "authors": ["Aryan Mokhtari"], "emails": ["aryanm@seas.upenn.edu", "aribeiro@seas.upenn.edu"], "sections": [{"heading": "1 Introduction", "text": "A hallmark of empirical risk minimization (ERM) on large datasets is that evaluating descent directions requires a complete pass over the dataset. Since this is undesirable due to the large number of training samples, stochastic optimization algorithms with descent directions estimated from a subset of samples are the method of choice. First order stochastic optimization has a long history [20, 19] but the last decade has seen fundamental progress in developing alternatives with faster convergence. A partial list of this consequential literature includes Nesterov acceleration [18, 2], stochastic averaging gradient [21, 7], variance reduction [12, 27], dual coordinate methods [24, 25], and hybrid algorithms [28, 13].\nWhen it comes to stochastic second order methods the first challenge is that while evaluation of Hessians is as costly as evaluation of gradients, the stochastic estimation of Hessians has proven more challenging. This difficulty is addressed by incremental computations in [11] and subsampling in [8] or circumvented altogether in stochastic quasi-Newton methods [22, 3, 14, 15, 16, 10]. Despite this incipient progress it is nonetheless fair to say that the striking success in developing stochastic first order methods is not matched by equal success in the development of stochastic second order methods. This is because even if the problem of estimating a Hessian is solved there are still four challenges left in the implementation of Newton-like methods in ERM:\n(i) Global convergence of Newton\u2019s method requires implementation of a line search subroutine and line searches in ERM require a complete pass over the dataset.\n(ii) The quadratic convergence advantage of Newton\u2019s method manifests close to the optimal solution but there is no point in solving ERM problems beyond their statistical accuracy.\n(iii) Newton\u2019s method works for strongly convex functions but loss functions are not strongly convex for many ERM problems of practical importance.\n(iv) Newton\u2019s method requires inversion of Hessians which is costly in large dimensional ERM.\nar X\niv :1\n60 5.\n07 65\n9v 1\n[ cs\n.L G\n] 2\n4 M\nay 2\nBecause they can\u2019t use line searches [cf. (i)], must work on problems that may be not strongly convex [cf. (iii)], and never operate very close to the optimal solution [cf (ii)], stochastic Newtonlike methods never experience quadratic convergence. They do improve convergence constants in ill-conditioned problems but they still converge at linear rates.\nIn this paper we attempt to overcome (i)-(iv) with the Ada Newton algorithm that combines the use of Newton iterations with adaptive sample sizes [6]. Say the total number of available samples is N , consider subsets of n \u2264 N samples, and suppose the statistical accuracy of the ERM associated with n samples is Vn (Section 2). In Ada Newton we add a quadratic regularization term of order Vn to the empirical risk \u2013 so that the regularized risk also has statistical accuracy Vn \u2013 and assume that for a certain initial sample size m0, the problem has been solved to its statistical accuracy Vm0 . The sample size is then increased by a factor \u03b1 > 1 to n = \u03b1m0. We proceed to perform a single Newton iteration with unit stepsize and prove that the result of this update solves this extended ERM problem to its statistical accuracy (Section 3). This permits a second increase of the sample size by a factor \u03b1 and a second Newton iteration that is likewise guaranteed to solve the problem to its statistical accuracy. Overall, this permits minimizing the empirical risk in \u03b1/(\u03b1\u2212 1) passes over the dataset and inverting log\u03b1N Hessians. Our theoretical and numerical analyses indicate that we can make \u03b1 = 2. In that cases we can optimize to within statistical accuracy in about 2 passes over the dataset and after inversion of about 3.32 log10N Hessians."}, {"heading": "2 Empirical risk minimization", "text": "We want to solve ERM problems to their statistical accuracy. To state this problem formally consider an argument w \u2208 Rp, a random variable Z with realizations z and a convex loss function f(w; z). We want to find an argument w\u2217 that minimizes the statistical average loss L(w) := EZ [f(w, Z)],\nw\u2217 := argmin w L(w) = argmin w EZ [f(w, Z)]. (1)\nThe loss in (1) can\u2019t be evaluated because the distribution of Z is unknown. We have, however, access to a training set T = {z1, . . . , zN} containing N independent samples z1, . . . , zN that we can use to estimate L(w). We therefore consider a subset Sn \u2286 T and settle for minimization of the empirical risk Ln(w) := (1/n) \u2211n k=1 f(w, zk),\nw\u2020n := argmin w Ln(w) = argmin w\n1\nn n\u2211 k=1 f(w, zk), (2)\nwhere, without loss of generality, we have assumed Sn = {z1, . . . , zn} contains the first n elements of T . The difference between the empirical risk in (2) and the statistical loss in (1) is a fundamental quantities in statistical learning. We assume here that there exists a constant Vn, which depends on the number of samples n, that upper bounds their difference for all w with high probability (w.h.p),\nsup w |L(w)\u2212 Ln(w)| \u2264 Vn, w.h.p. (3)\nThat the statement in (3) holds with w.h.p means that there exists a constant \u03b4 such that the inequality holds with probability at least 1 \u2212 \u03b4. The constant Vn depends on \u03b4 but we keep that dependency implicit to simplify notation. For subsequent discussions, observe that bounds Vn of order Vn = O(1/ \u221a n) date back to the seminal work of Vapnik \u2013 see e.g., [26, Section 3.4]. Bounds of order Vn = O(1/n) have been derived more recently under stronger regularity conditions that are not uncommon in practice, [1, 9, 4]\nAn important consequence of (1) is that there is no point in solving (2) to an accuracy higher than Vn. Indeed, if we find a variable w for which Ln(wn)\u2212Ln(w\u2020) \u2264 Vn finding a better approximation of w\u2020 is moot because (3) implies that this is not necessarily a better approximation of the minimizer w\u2217 of the statistical loss. We say the variable wn solves the ERM problem in (2) to within its statistical accuracy. In particular, this implies that adding a regularization of order Vn to (2) yields a problem that is essentially equivalent. We can then consider a quadratic regularizer of the form cVn/2\u2016w\u20162 to define the regularized empirical risk Rn(w) := Ln(w) + (cVn/2)\u2016w\u20162 and the corresponding optimal argument\nw\u2217n := argmin w Rn(w) = argmin w Ln(w) + cVn 2 \u2016w\u20162. (4)\nSince the regularization in (4) is of order Vn and (3) holds, the difference between Rn(w\u2217n) and L(w\u2217) is also of order Vn \u2013 this may be not as immediate as it seems; see [23]. Thus, we can say that a variable wn satisfying Rn(wn) \u2212 Rn(w\u2217n) \u2264 Vn solves the ERM problem to within its statistical accuracy. We accomplish this in this paper with the Ada Newton algorithm."}, {"heading": "3 Ada Newton", "text": "To solve (4) suppose the problem has been solved to within its statistical accuracy for a set Sm \u2282 Sn withm = n/\u03b1 samples. Therefore, we have found a variable wm for whichRm(wm)\u2212Rm(w\u2217m) \u2264 Vm. We want to update wm to obtain a variable wn that estimates w\u2217n with accuracy Vn. To do so compute the gradient of the risk Rn evaluated at wm\n\u2207Rn(wm) = 1\nn n\u2211 k=1 \u2207f(wm, zk) + cVnwm, (5)\nas well as the Hessian Hn of Rn evaluated at wm\nHn := \u22072Rn(wm) = 1\nn n\u2211 k=1 \u22072f(wm, zk) + cVnI, (6)\nand update wm with the Newton step of the regularized risk Rn to compute\nwn = wm \u2212H\u22121n \u2207Rn(wm). (7)\nThe main contribution of this paper is to derive a condition that guarantees that wn solves Rn to within its statistical accuracy Vn.\nTheorem 1. Consider the variable wm as a Vm-optimal solution of the risk Rm, i.e., a solution such that Rm(wm) \u2212 Rm(w\u2217m) \u2264 Vm. Let n = \u03b1m > m, consider the risk Rn associated with sample set Sn \u2283 Sm, and suppose assumptions 1 - 3 hold. If the sample size n is chosen such that(\n2(M + cVm)Vm cVn\n)1/2 +\n2(n\u2212m) nc1/2 +\n( (2 + \u221a 2)c1/2 + c\u2016w\u2217\u2016 ) (Vm \u2212 Vn)\n(cVn)1/2 \u2264 1 4 (8)\nand\n144 ( Vm +\n2(n\u2212m) n (Vn\u2212m + Vm) + 2 (Vm \u2212 Vn) + c(Vm \u2212 Vn) 2 \u2016w\u2217\u20162\n)2 \u2264 Vn (9)\nare satisfied, then the variable wn, which is the outcome of applying one Newton step on the variable wm as in (7), has sub-optimality error Vn with high probability, i.e.,\nRn(wn)\u2212Rn(w\u2217n) \u2264 Vn, w.h.p. (10)\nProof. See section 4.\nTheorem 1 states conditions under which we can iteratively increase the sample size while applying single Newton iterations without line search and staying within the statistical accuracy of the regularized empirical risk. The constants in (8) and (9) are not easy to parse but we can understand them qualitatively if we focus on large m. This results in a simpler condition that we state next.\nProposition 2. Consider a learning problem in which the statistical accuracy satisfies Vm \u2264 \u03b1Vn for n = \u03b1m and limn\u2192\u221e Vn = 0. If the regularization constant c is chosen so that(\n2\u03b1M\nc\n)1/2 +\n2\u03b1\n(\u03b1\u2212 1)c1/2 <\n1 4 , (11)\nthen, there exists a sample size m\u0303 such that (8) and (9) are satisfied for all m > m\u0303 and n = \u03b1m. In particular, if \u03b1 = 2 we can satisfy (8) and (9) with c > 64( \u221a M + 2)2.\nAlgorithm 1 Ada Newton 1: Parameters: Sample size increase constants \u03b10 > 1 and 0 < \u03b2 < 1. 2: Input: Initial sample size n = m0 and argument wn = wm0 with \u2016\u2207Rn(wn)\u2016 < ( \u221a 2c)Vn\n3: while n \u2264 N do {main loop} 4: Update argument and index: wm = wn and m = n. Reset factor \u03b1 = \u03b10 . 5: repeat {sample size backtracking loop} 6: Increase sample size: n = max{\u03b1m,N}. 7: Compute gradient [cf. (5)]: \u2207Rn(wm) = (1/n) \u2211n k=1\u2207f(wm, zk) + cVnwm\n8: Compute Hessian [cf. (6)]: Hn = (1/n) \u2211n k=1\u22072f(wm, zk) + cVnI\n9: Newton Update [cf. (7)]: wn = wm \u2212H\u22121n \u2207Rn(wm) 10: Compute gradient [cf. (5)]: \u2207Rn(wn) = (1/n) \u2211n k=1\u2207f(wn, zk) + cVnwn 11: Backtrack sample size increase \u03b1 = \u03b2\u03b1. 12: until \u2016\u2207Rn(wn)\u2016 > ( \u221a 2c)Vn\n13: end while\nProof. That the condition in (9) is satisfied for all m > m\u0303 follows simply because the left hand side is of order V 2m and the right hand side is of order Vn. To show that the condition in (8) is satisfied for sufficiently large m observe that the third summand in (8) is of order O((Vm \u2212 Vn)/V 1/2n ) and vanishes for large m. In the second summand of (8) we make n = \u03b1m to obtain the second summand in (11) and in the first summand replace the ratio Vm/Vn by its bound \u03b1 to obtain the first summand of (11). To conclude the proof just observe that the inequality in (11) is strict.\nThe condition Vm \u2264 \u03b1Vn is satisfied if Vn = 1/n and is also satisfied if Vn = 1/ \u221a n because\u221a\n\u03b1 < \u03b1. This means that for most ERM problems we can progress geometrically over the sample size and arrive at a solution wN that solves the ERM problem RN to its statistical accuracy VN as long as (11) is satisfied .\nThe result in Theorem 1 motivates definition of the Ada Newton algorithm that we summarize in Algorithm 1. The core of the algorithm is in steps 6-9. Step 6 implements an increase in the sample size by a factor \u03b1 and steps 7-9 implement the Newton iteration in (5)-(7). The required input to the algorithm is an initial sample size m0 and a variable wm0 that is known to solve the ERM problem with accuracy Vm0 . Observe that this initial iterate doesn\u2019t have to be computed with Newton iterations. The initial problem to be solved contains a moderate number of samples m0, a mild condition number because it is regularized with constant cVm0 , and is to be solved to a moderate accuracy Vm0 \u2013 recall that Vm0 is of order Vm0 = O(1/m0) or order Vm0 = O(1/ \u221a m0) depending on regularity assumptions. Stochastic first order methods excel at solving problems with moderate number of samples m0 and moderate condition to moderate accuracy.\nWe remark that the conditions in Theorem 1 and Proposition 2 are conceptual but that the constants involved are unknown in practice. In particular, this means that the allowed values of the factor \u03b1 that controls the growth of the sample size are unknown a priori. We solve this problem in Algorithm 1 by backtracking the increase in the sample size until we guarantee that wn minimizes the empirical risk Rn(wn) to within its statistical accuracy. This backtracking of the sample size is implemented in Step 11 and the optimality condition of wn is checked in Step 12. The condition in Step 12 is on the gradient norm that, because Rn is strongly convex, can be used to bound the suboptimality Rn(wn)\u2212Rn(w\u2217n) as\nRn(wn)\u2212Rn(w\u2217n) \u2264 1\n2cVn \u2016\u2207Rn(wn)\u20162. (12)\nObserve that checking this condition requires an extra gradient computation undertaken in Step 10. That computation can be reused in the computation of the gradient in Step 5 once we exit the backtracking loop. We emphasize that when the condition in (11) is satisfied, there exist a sufficiently large m for which the conditions in Theorem 1 are satisfied for n = \u03b1m. This means that the backtracking condition in Step 12 is satisfied after one iteration and that, eventually, Ada Newton progresses by increasing the sample size by a factor \u03b1. This means that Algorithm 1 can\nbe thought of as having a damped phase where the sample size increases by a factor smaller than \u03c1 and a geometric phase where the sample size grows by a factor \u03c1 in all subsequent iterations. The computational cost of this geometric phase is of not more than \u03b1/(\u03b1\u2212 1) passes over the dataset and requires inverting not more than log\u03b1N Hessians. If c > 64( \u221a M + 2)2, we make \u03b1 = 2 for optimizing to within statistical accuracy in about 2 passes over the dataset and after inversion of about 3.32 log10N Hessians."}, {"heading": "4 Convergence Analysis", "text": "In this section we study the proof of Theorem 1. In proving this result we first assume the following conditions are satisfied. Assumption 1. The loss functions f(w, z) are convex with respect to w for all values of z. Moreover, their gradients\u2207f(w, z) are Lipschitz continuous with constant M\n\u2016\u2207f(w, z)\u2212\u2207f(w\u2032, z)\u2016 \u2264M\u2016w \u2212w\u2032\u2016, for all z. (13) Assumption 2. The loss functions f(w, z) are self-concordant with respect to w for all z. Assumption 3. The difference between the gradients of the empirical loss Ln and the statistical average loss L is bounded by V 1/2n for all w with high probability,\nsup w \u2016\u2207L(w)\u2212\u2207Ln(w)\u2016 \u2264 V 1/2n , w.h.p. (14)\nThe conditions in Assumption 1 imply that the average loss L(w) and the empirical loss Ln(w) are convex and their gradients are Lipschitz continuous with constant M . Thus, the empirical risk Rn(w) is strongly convex with constant cVn and its gradients \u2207Rn(w) are Lipschitz continuous with parameter M + cVn. Likewise, the condition in Assumption 2 implies that the average loss L(w), the empirical loss Ln(w), and the empirical risk Rn(w) are also self-concordant. The condition in Assumption 3 says that the gradients of the empirical risk converge to their statistical average at a rate of order V 1/2n . If the constant Vn in condition (3) is of order not faster than O(1/n) the condition in Assumption 3 holds if the gradients converge to their statistical average at a rate of order V 1/2n = O(1/ \u221a n). This is a conservative rate for the law of large numbers.\nThe main idea of the Ada Newton algorithm is introducing a policy for increasing the size of training set from m to n in a way that the current variable wm is in the Newton quadratic convergence phase for the next regularized empirical riskRn. In the following proposition, we characterize the required condition to guarantee staying in the local neighborhood of Newton\u2019s method. Proposition 3. Consider the sets Sm and Sn as subsets of the training set T such that Sm \u2282 Sn \u2282 T . We assume that the number of samples in the sets Sm and Sn are m and n, respectively. Further, define wm as an Vm optimal solution of the riskRm, i.e., Rm(wm)\u2212Rm(w\u2217m) \u2264 Vm. In addition, define \u03bbn(w) := ( \u2207Rn(w)T\u22072Rn(w)\u22121\u2207Rn(w) )1/2 as the Newton decrement of variable w associated with the risk Rn. If Assumption 1-3 hold, then Newton\u2019s method at point wm is in the quadratic convergence phase for the objective function Rn, i.e., \u03bbn(wm) < 1/4, if we have(\n2(M + cVm)Vm cVn\n)1/2 + (2(n\u2212m)/n)V 1/2n + ( \u221a 2c+ 2 \u221a c+ c\u2016w\u2217\u2016)(Vm \u2212 Vn)\n(cVn)1/2 \u2264 1 4 w.h.p.\n(15)\nProof. See Section 7.1 in the Appendix.\nFrom the analysis of Newton\u2019s method we know that if the Newton decrement \u03bbn(w) is smaller than 1/4, the variable w is in the local neighborhood of Newton\u2019s method; see e.g., Chapter 9 of [5]. From the result in Proposition 3, we obtain a sufficient condition to guarantee that \u03bbn(wm) < 1/4 which implies that wm, which is a Vm optimal solution for the regularized empirical loss Rm, i.e., Rm(wm) \u2212 Rm(w\u2217m) \u2264 Vm, is in the local neighborhood of the optimal argument of Rn that Newton\u2019s method converges quadratically.\nUnfortunately, the quadratic convergence of Newton\u2019s method for self-concordant functions is in terms of the Newton decrement \u03bbn(w) and it does not necessary guarantee quadratic convergence in terms of objective function error. To be more precise, we can show that \u03bbn(wn) \u2264\n\u03b3\u03bbn(wm) 2; however, we can not conclude that the quadratic convergence of Newton\u2019s method implies Rn(wn) \u2212 Rn(w\u2217n) \u2264 \u03b3\u2032(Rn(wm) \u2212 Rn(w\u2217n))2. In the following proposition we try to characterize an upper bound for the error Rn(wn) \u2212 Rn(w\u2217n) in terms of the squared error (Rn(wm)\u2212Rn(w\u2217n))2 using the quadratic convergence property of Newton decrement. Proposition 4. Consider wm as a variable that is in the local neighborhood of the optimal argument of the risk Rn where Newton\u2019s method has a quadratic convergence rate, i.e., \u03bbn(wm) \u2264 1/4. Recall the definition of the variable wn in (7) as the updated variable using Newton step. If Assumption 1 and 2 hold, then the difference Rn(wn)\u2212Rn(w\u2217n) is upper bounded by\nRn(wn)\u2212Rn(w\u2217n) \u2264 144(Rn(wm)\u2212Rn(w\u2217n))2. (16)\nProof. To prove the result in (16) first we need to find upper and lower bounds for the difference Rn(w)\u2212Rn(w\u2217n) in terms of the Newton decrement parameter \u03bbn(w). To do so, we use the result in Theorem 4.1.11 of [17] which shows that\n\u03bbn(w)\u2212 ln (1 + \u03bbn(w)) \u2264 Rn(w)\u2212Rn(w\u2217n) \u2264 \u2212\u03bbn(w)\u2212 ln (1\u2212 \u03bbn(w)) . (17)\nNote that we assume that 0 < \u03bbn(w) < 1/4. Thus, we can use the Taylor\u2019s expansion of ln(1 + a) for a = \u03bbn(w) to show that \u03bbn(w) \u2212 ln (1 + \u03bbn(w)) is bounded below by (1/2)\u03bbn(w)\n2 \u2212 (1/3)\u03bbn(w)3. Since 0 < \u03bbn(w) < 1/4 we can show that (1/6)\u03bbn(w)2 \u2264 (1/2)\u03bbn(w)\n2 \u2212 (1/3)\u03bbn(w)3. Thus, the term \u03bbn(w) \u2212 ln (1 + \u03bbn(w)) is bounded below by (1/6)\u03bb2. Likewise, we use Taylor\u2019s expansion of ln(1 \u2212 a) for a = \u03bbn(w) to show that \u2212\u03bbn(w) \u2212 ln (1\u2212 \u03bbn(w)) is bounded above by \u03bbn(w)2 for \u03bbn(w) < 1/4; see e.g., Chapter 9 of [5]. Considering these bounds and the inequalities in (17) we can write\n1 6 \u03bbn(w) 2 \u2264 Rn(w)\u2212Rn(w\u2217n) \u2264 \u03bbn(w)2. (18)\nRecall that the variable wm satisfies the condition \u03bbn(wm) \u2264 1/4. Thus, according to the quadratic convergence rate of Newton\u2019s method for self-concordant functions [5], we know that the Newton decrement has a quadratic convergence and we can write\n\u03bbn(wn) \u2264 2\u03bbn(wm)2. (19)\nWe use the result in (18) and (19) to show that the optimality error Rn(wn) \u2212 Rn(w\u2217n) has an upper bound which is proportional to (Rn(wm)\u2212Rn(w\u2217n))2. In particular, we can writeRn(wn)\u2212 Rn(w \u2217 n) \u2264 \u03bbn(wn)2 based on the second inequality in (18). This observation in conjunction with the result in (19) implies that\nRn(wn)\u2212Rn(w\u2217n) \u2264 4\u03bbn(wm)4. (20)\nThe first inequality in (18) implies that \u03bbn(wm)4 \u2264 36(Rn(wm) \u2212 Rn(w\u2217n))2. Thus, we can substitute \u03bbn(wm)4 in (20) by 36(Rn(wm)\u2212Rn(w\u2217n))2 to obtain the result in (16).\nThe result in Proposition 4 provides an upper bound for the sub-optimality Rn(wn) \u2212 Rn(w\u2217n) in terms of the sub-optimality of variable wm for the riskRn, i.e., Rn(wm)\u2212Rn(w\u2217n). Recall that we know that wm is in the statistical accuracy of Rm, i.e., Rm(wm)\u2212Rm(w\u2217m) \u2264 Vm, and we aim to show that the updated variable wn stays in the statistical accuracy ofRn, i.e.,Rn(wn)\u2212Rn(w\u2217n) \u2264 Vn. This can be done by showing that the upper bound for Rn(wn) \u2212 Rn(w\u2217n) in (16) is smaller than Vn. We proceed to derive an upper bound for the sub-optimality Rn(wm) \u2212 Rn(w\u2217n) in the following proposition. Proposition 5. Consider the sets Sm and Sn as subsets of the training set T such that Sm \u2282 Sn \u2282 T . We assume that the number of samples in the sets Sm and Sn are m and n, respectively. Further, define wm as an Vm optimal solution of the risk Rm, i.e., Rm(wm)\u2212R\u2217m \u2264 Vm. If Assumption 1-3 hold, then the empirical risk error Rn(wm)\u2212Rn(w\u2217n) of the variable wm corresponding to the set Sn is bounded above by\nRn(wm)\u2212Rn(w\u2217n) \u2264 Vm+ 2(n\u2212m)\nn (Vn\u2212m + Vm)+2 (Vm \u2212 Vn)+ c(Vm \u2212 Vn) 2\n\u2016w\u2217\u20162 w.h.p. (21)\nProof. See Section 7.2 in the Appendix.\nThe result in Proposition 5 characterizes the sub-optimality of the variable wm, which is an Vm sub-optimal solution for the risk Rm, with respect to the empirical risk Rn associated with the set Sn. The results in Proposition 3, 4, and 5 lead to the result in Theorem 1. To be more precise, from the result in Proposition 3 we obtain that the condition in (8) implies that wm is in the local neighborhood of the optimal argument of Rn and \u03bbn(wm) \u2264 1/4. Hence, the hypothesis of Proposition 4 is satisfied and we have Rn(wn) \u2212 Rn(w\u2217n) \u2264 144(Rn(wm) \u2212 Rn(w\u2217n))2. This result paired with the result in Proposition 5 shows that if the condition in (9) is satisfied we can conclude that Rn(wn)\u2212Rn(w\u2217n) \u2264 Vn which completes the proof of Theorem 1."}, {"heading": "5 Experiments", "text": "In this section, we study the performance of the proposed Ada Newton method and compare it with state-of-the-art in solving a large-scale classification problem. We use the protein homology dataset provided on KDD cup 2004 website. The dataset contains N = 145751 samples and the dimension of each sample is p = 74. We consider three algorithms to compare with the proposed Ada Newton method. One of them is the classic Newton\u2019s method with backtracking line search. The second algorithm is Stochastic Gradient Descent (SGD) and the last one is the SAGA algorithm introduced in [7]. In our experiments, we use logistic loss and set the regularization parameters as c = 200 and Vn = 1/n.\nThe stepsize of SGD in our experiments is 2\u00d710\u22122. Note that picking larger stepsize leads to faster but less accurate convergence and choosing smaller stepsize improves the accuracy convergence with the price of slower convergence rate. The stepsize for SAGA is hand-optimized and the best performance has been observed for \u03b1 = 0.2 which is the one that we use in the experiments. For Newton\u2019s method, the backtracking line search parameters are \u03b1 = 0.4 and \u03b2 = 0.5. In the implementation of Ada Newton we increase the size of the training set by factor 2 at each iteration, i.e., \u03b1 = 2 and we observe that the condition \u2016\u2207Rn(wn)\u2016 > ( \u221a 2c)Vn is always satisfied and there is no need for reducing the factor \u03b1. Moreover, the size of initial training set is m0 = 124. For the warmup step that we need to get into to the quadratic neighborhood of Newton\u2019s method we use the gradient descent method. In particular, we run gradient descent with stepsize 10\u22123 for 100 iterations. Note that since the number of samples is very small at the beginning, m0 = 124, and the regularizer is very large, the condition number of problem is very small. Thus, gradient descent is able to converge to a good neighborhood of the optimal solution in a reasonable time. Notice that the computation of this warm up process is very low and is equal to 12400 gradient evaluations. This number of samples is less than 10% of the full training set. In other words, the cost is less than 10% of one pass over the dataset. Although, this cost is negligible, we consider it in comparison\nwith SGD, SAGA, and Newton\u2019s method. We would like to mention that other algorithms such as Newton\u2019s method and stochastic algorithms can also be used for the warm up process; however, the gradient descent method sounds the best option since the gradient evaluation is not costly and the problem is well-conditioned for a small training set .\nFigure 1 illustrates the convergence path of SGD, SAGA, Newton, and Ada Newton for the protein homology dataset. Note that the x axis is the total number of samples used divided by the size of the training set N = 145751 which we call number of passes over the dataset. As we observe, The best performance among the four algorithms belongs to Ada Newton. In particular, Ada Newton is able to achieve the accuracy of RN (w)\u2212 R\u2217n < 1/N by 2.4 passes over the dataset which is very close to theoretical result in Theorem 1 that guarantees accuracy of order O(1/N) after \u03b1/(\u03b1 \u2212 1) = 2 passes over the dataset. To achieve the same accuracy of 1/N Newton\u2019s method requires 7.5 passes over the dataset, while SAGA needs 10 passes. The SGD algorithm can not achieve the statistical accuracy of order O(1/N) even after 25 passes over the dataset.\nAlthough, Ada Newton and Newton outperform SAGA and SGD, their computational complexity are different. We address this concern by comparing the algorithms in terms of runtime. Figure 2 demonstrates the convergence paths of the considered methods in terms of runtime. As we observe, Newton\u2019s method requires more time to achieve the statistical accuracy of 1/N relative to SAGA. This observation justifies the belief that Newton\u2019s method is not practical for large-scale optimization problems, since by enlarging p or making the initial solution worse the performance of Newton\u2019s method will be even worse than the ones in Figure 1. Ada Newton resolves this issue by starting from small sample size which is computationally less costly. Ada Newton also requires Hessian inverse evaluations, but the number of inversions is proportional to log\u03b1N . Moreover, the performance of Ada Newton doesn\u2019t depend on the initial point and the warm up process is not costly as we described before. We observe that Ada Newton outperforms SAGA significantly. In particular it achieves the statistical accuracy of 1/N in less than 25 seconds, while SAGA achieves the same accuracy in 62 seconds. Note that since the variable wN is in the quadratic neighborhood of Newton\u2019s method for RN the convergence path of Ada Newton becomes quadratic eventually when the size of the training set becomes equal to the size of the full dataset. It follows that the advantage of Ada Newton with respect to SAGA is more significant if we look for a sub-optimality less than Vn. We have observed similar performances for other datasets such as A9A, COVTYPE, and SUSY."}, {"heading": "6 Discussions", "text": "As explained in Section 4, Theorem 1 holds because condition (8) makes wm part of the quadratic convergence region of Rn. From this fact, it follows that the Newton iteration makes the subopti-\nmality gapRn(wn)\u2212Rn(w\u2217n) the square of the suboptimality gapRn(wm)\u2212Rn(w\u2217n). This yields condition (9) and is the fact that makes Newton steps valuable in increasing the sample size. If we replace Newton iterations by any method with linear convergence rate, the orders of both sides on condition (9) are the same. This would make aggressive increase of the sample size unlikely.\nIn Section 1 we pointed out four reasons that challenge the development of stochastic Newton methods. It would not be entirely accurate to call Ada Newton a stochastic method because it doesn\u2019t rely on stochastic descent directions. It is, nonetheless, a method for ERM that makes pithy use of the dataset. The challenges listed in Section 1 are overcome by Ada Newton because:\n(i) Ada Newton does not use line searches. Optimality improvement is guaranteed by increasing the sample size.\n(ii) The advantages of Newton\u2019s method are exploited by increasing the sample size at a rate that keeps the solution for sample size m in the quadratic convergence region of the risk associated with sample size n = \u03b1m. This allows aggressive growth of the sample size.\n(iii) The ERM problem is not necessarily strongly convex. A regularization of order Vn is added to construct the empirical risk Rn\n(iv) Ada Newton inverts only log\u03b1N Hessians.\nIt is fair to point out that items (ii) and (iv) are true only to the extent that the damped phase in Algorithm 1 is not significant. Our numerical experiments indicate that this is true but the conclusion is not warranted by out theoretical bounds except when the dataset is very large. This suggests the bounds are loose and that further research is warranted to develop tighter bounds."}, {"heading": "Acknowledgement", "text": "We thank Hadi Daneshmand, Aurelien Lucci, and Thomas Hofmann for useful discussions on the use of adaptive sample sizes for solving large-scale ERM problems and on the importance of using adaptive regularization coefficients."}, {"heading": "7 Appendix", "text": "In this section we study the proofs of Propositions 3 and 5. To do so, first we prove Lemmata 6 and 7 which are intermediate results that we use in proving the mentioned propositions.\nWe start the analysis by providing an upper bound for the difference between the loss functions Ln and Lm. The upper bound is studied in the following lemma which uses the condition in (3).\nLemma 6. Consider Ln and Lm as the empirical losses of the sets Sn and Sm, respectively, where they are chosen such that Sm \u2282 Sn. If we define n and m as the number of samples in the training sets Sn and Sm, respectively, then the absolute value of the difference between the empirical losses is bounded above by\n|Ln(w)\u2212 Lm(w)| \u2264 n\u2212m n (Vn\u2212m + Vm) , w.h.p. (22)\nfor any w.\nProof. First we characterize the difference between the difference of the loss functions associated with the sets Sm and Sn. To do so, consider the difference\nLn(w)\u2212 Lm(w) = 1\nn \u2211 i\u2208Sn fi(w)\u2212 1 m \u2211 i\u2208Sm fi(w). (23)\nNotice that the set Sm is a subset of the set Sn and we can write Sn = Sm \u222a Sn\u2212m. Thus, we can rewrite the right hand side of (23) as\nLn(w)\u2212 Lm(w) = 1\nn \u2211 i\u2208Sm fi(w) + \u2211 i\u2208Sn\u2212m fi(w) \u2212 1 m \u2211 i\u2208Sm fi(w)\n= 1\nn \u2211 i\u2208Sn\u2212m fi(w)\u2212 n\u2212m mn \u2211 i\u2208Sm fi(w). (24)\nFactoring (n\u2212m)/n from the terms in the right hand side of (24) follows\nLn(w)\u2212 Lm(w) = n\u2212m n  1 n\u2212m \u2211 i\u2208Sn\u2212m fi(w)\u2212 1 m \u2211 i\u2208Sm fi(w)  (25) Now add and subtract the statistical loss L(w) to obtain\n|Ln(w)\u2212 Lm(w)| = n\u2212m n \u2223\u2223\u2223\u2223\u2223\u2223 1n\u2212m \u2211\ni\u2208Sn\u2212m\nfi(w)\u2212 L(w) + L(w)\u2212 1\nm \u2211 i\u2208Sm fi(w) \u2223\u2223\u2223\u2223\u2223\u2223 \u2264 n\u2212m\nn (Vn\u2212m + Vm) . (26)\nwhere the last inequality follows by using the triangle inequality and the upper bound in (3).\nThe result in Lemma 6 shows that the upper bound for the difference between the loss functions associated with the sets Sm and Sn where Sm \u2282 Sn is proportional to the difference between the size of these two sets n\u2212m. This result will help us later to understand how much we can increase the size of the training set at each iteration. In other words, how large the difference n \u2212m could be, while we have the statistical accuracy.\nIn the following lemma, we characterize an upper bound for the norm of the optimal argument w\u2217n of the empirical risk Rn(w) in terms of the norm of statistical average loss L(w) optimal argument w\u2217. Lemma 7. Consider Ln as the empirical loss of the set Sn and L as the statistical average loss. Moreover, recall w\u2217 as the optimal argument of the statistical average loss L, i.e., w\u2217 = argminw L(w). If Assumption 1 holds, then the norm of the optimal argument w \u2217 n of the regularized empirical risk Rn(w) := Ln(w) + cVn\u2016w\u20162 is bounded above by\n\u2016w\u2217n\u20162 \u2264 4\nc + \u2016w\u2217\u20162, w.h.p. (27)\nProof. The optimality condition of w\u2217n for the the regularized empirical risk Rn(w) = Ln(w) + (cVn)/2\u2016w\u20162 implies that\nLn(w \u2217 n) + cVn 2 \u2016w\u2217n\u20162 \u2264 Ln(w\u2217) + cVn 2 \u2016w\u2217\u20162. (28)\nBy regrouping the terms we obtain that the squared norm \u2016w\u2217n\u20162 is bonded above by\n\u2016w\u2217n\u20162 \u2264 2\ncVn (Ln(w\n\u2217)\u2212 Ln(w\u2217n)) + \u2016w\u2217\u20162. (29)\nWe proceed to bound the difference Ln(w\u2217)\u2212Ln(w\u2217n). By adding and subtracting the terms L(w\u2217) and L(w\u2217n) we obtain that\nLn(w \u2217)\u2212 Ln(w\u2217n) = [ Ln(w \u2217)\u2212 L(w\u2217) ] + [ L(w\u2217)\u2212 L(w\u2217n) ] + [ L(w\u2217n)\u2212 Ln(w\u2217n) ] . (30)\nNotice that the second bracket in (30) is non-positive since L(w\u2217) \u2264 L(w\u2217n). Therefore, it is bounded by 0. According to (3), the first and third brackets in (30) are with high probability bounded above by Vn. Replacing these upper bounds by the brackets in (30) yields\nLn(w \u2217)\u2212 Ln(w\u2217n) \u2264 2Vn. (31)\nSubstituting the upper bound in (31) into (29) follows the claim in (27)."}, {"heading": "7.1 Proof of Proposition 3", "text": "From the self-concordance analysis of Newton\u2019s method we know that the variable wm is in the neighborhood that Newton\u2019s method has a quadratic convergence rate if \u03bbn(wm) \u2264 1/4; see e.g., Chapter 9 of [5]. We proceed to come up with a condition for the quadratic convergence phase which guarantees that \u03bbn(wm) < 1/4 and wm is in the local neighborhood of the optimal argument of Rn. Recall that we have a wm which has sub-optimality Vm for Rm. We then proceed to enlarge the sample size to n and start from the observation that we can bound \u03bbn(wm) as\n\u03bbn(wm) = \u2016\u2207Rn(wm)\u2016H\u22121n \u2264 \u2016\u2207Rm(wm)\u2016H\u22121n + \u2016\u2207Rn(wm)\u2212\u2207Rm(wm)\u2016H\u22121n , (32)\nwhere we have used the definition Hn = \u22072Rn(wm). Note that the weighted norm \u2016a\u2016A for vector a and matrix A is equal to \u2016a\u2016A = (aTAa)1/2. First, we bound the norm \u2016\u2207Rn(wm)\u2016H\u22121n in (32). Notice that the Hessian \u22072Rn(wm) can be written as \u22072Ln(wm) + cVnI. Thus, the eigenvalues of the Hessian Hn = \u22072Rn(wm) are bounded below by cVn and consequently the eigenvalues of the Hessian inverse H\u22121n = \u22072Rn(wm)\u22121 are upper bounded by 1/(cVn). This bound implies that \u2016H\u22121n \u2016 \u2264 1/(cVn). Moreover, from Theorem 2.1.5 of [17], we know that the Lipschitz continuity of the gradients \u2207Rm(w) with constant M + cVm implies that\n\u2016\u2207Rm(wm)\u20162 \u2264 2(M + cVm)(Rm(wm)\u2212Rm(w\u2217m)) \u2264 2(M + cVm)Vm, (33)\nwhere the last inequality holds comes from the condition that Rm(wm)\u2212Rm(w\u2217m) \u2264 Vm. Considering the upper bound for \u2016\u2207Rm(wm)\u20162 in (33) and the inequality \u2016\u22072Rn(wm)\u22121\u2016 \u2264 1/(cVn) we can write\n\u2016\u2207Rm(wm)\u2016H\u22121n = [ \u2207Rm(wm)TH\u22121n \u2207Rm(wm) ]1/2 \u2264 ( 2(M + cVm)Vm\ncVn\n)1/2 . (34)\nNow we proceed to bound the second the term in (32). The definition of the risk function the gradient can be written as \u2207Rn(w) = \u2207Ln(w) + (cVn)w. Thus, we can derive an upper bound for the difference \u2016\u2207Rn(wm)\u2212\u2207Rm(wm)\u2016 as\n\u2016\u2207Rn(wm)\u2212\u2207Rm(wm)\u2016 \u2264 \u2016\u2207Ln(wm)\u2212\u2207Lm(wm)\u2016+ c(Vm \u2212 Vn)\u2016wm\u2016 \u2264 \u2016\u2207Ln(wm)\u2212\u2207Lm(wm)\u2016+ c(Vm \u2212 Vn)\u2016wm \u2212w\u2217m\u2016+ c(Vm \u2212 Vn)\u2016w\u2217m\u2016, (35)\nwhere in the second inequality we have used the triangle inequality and replaced \u2016wm\u2016 by its upper bound \u2016wm \u2212w\u2217m\u2016 + \u2016w\u2217m\u2016. By following the steps in (23)-(26) we can show that the difference \u2016\u2207Ln(wm)\u2212\u2207Lm(wm)\u2016 is bounded above by\n\u2016\u2207Ln(w)\u2212\u2207Lm(w)\u2016 \u2264 n\u2212m n \u2016\u2207Ln\u2212m(w)\u2212\u2207L(w)\u2016+ n\u2212m n \u2016\u2207Lm(w)\u2212\u2207L(w)\u2016\n\u2264 2(n\u2212m) n V 1/2n , (36)\nwhere the second inequality uses the condition that \u2016\u2207Lm(w)\u2212\u2207L(w)\u2016 \u2264 V 1/2m . Note that the strong convexity of the risk Rm with parameter cVm yields\n\u2016wm \u2212w\u2217m\u20162 \u2264 2\ncVm (Rm(wm)\u2212Rm(w\u2217m)) \u2264\n2 c . (37)\nThus, by considering the inequalities in (36) and (37) we can show that upper bound in (35) can be replaced by\n\u2016\u2207Rn(wm)\u2212\u2207Rm(wm)\u2016 \u2264 2(n\u2212m)\nn V 1/2n + (\n\u221a 2c+ c\u2016w\u2217m\u2016)(Vm \u2212 Vn). (38)\nSubstituting the upper bounds in (34) and (38) for the first and second summands in (32), respectively, follows the inequality\n\u03bbn(wm) \u2264 ( 2(M + cVm)Vm\ncVn\n)1/2 + (2(n\u2212m)/n)V 1/2n + ( \u221a 2c+ c\u2016w\u2217m\u2016)(Vm \u2212 Vn)\n(cVn)1/2 . (39)\nNote that the result in (27) shows that \u2016w\u2217m\u20162 \u2264 (4/c) + \u2016w\u2217\u20162 with high probability. This observation follows that \u2016w\u2217m\u2016 is bounded above by (2/ \u221a c) + \u2016w\u2217\u2016. Replacing the norm \u2016w\u2217m\u2016 in (39)\nby the upper bound (2/ \u221a c) + \u2016w\u2217\u2016 follows\n\u03bbn(wm) \u2264 ( 2(M + cVm)Vm\ncVn\n)1/2 + (2(n\u2212m)/n)V 1/2n + ( \u221a 2c+ 2 \u221a c+ c\u2016w\u2217\u2016)(Vm \u2212 Vn)\n(cVn)1/2 .\n(40)\nAs we mentioned previously, the variable wm is in the neighborhood that Newton\u2019s method has a quadratic convergence rate for the function Rn if the condition \u03bbn(wm) \u2264 1/4 holds. Hence, if the right hand side of (40) is bounded above by 1/4 we can conclude that wm is in the local neighborhood and the proof is complete."}, {"heading": "7.2 Proof of Proposition 5", "text": "Note that the difference Rn(wm)\u2212Rn(w\u2217n) can be written as\nRn(wm)\u2212Rn(w\u2217n) = Rn(wm)\u2212Rm(wm) +Rm(wm)\u2212Rm(w\u2217m) +Rm(w \u2217 m)\u2212Rm(w\u2217n) +Rm(w\u2217n)\u2212Rn(w\u2217n). (41)\nWe proceed to bound the differences in (41). To do so, note that the differenceRn(wm)\u2212Rm(wm) can be simplified as\nRn(wm)\u2212Rm(wm) = Ln(wm)\u2212 Lm(wm) + c(Vn \u2212 Vm)\n2 \u2016wm\u20162\n\u2264 Ln(w)\u2212 Lm(w), (42)\nwhere the inequality follows from the fact that Vn < Vm and Vn \u2212 Vm is negative. It follows from the result in Lemma 6 that the right hand side of (42) is bounded by (n\u2212m)/n (Vn\u2212m + Vm). Therefore,\nRn(wm)\u2212Rm(wm) \u2264 n\u2212m n (Vn\u2212m + Vm) . (43)\nAccording to the fact that wm as an Vm optimal solution for the sub-optimalityRm(wm)\u2212Rm(w\u2217m) we know that Rm(wm)\u2212Rm(w\u2217m) \u2264 Vm. (44) Based on the definition of w\u2217m which is the optimal solution of the risk Rm, the third difference in (41) which is Rm(w\u2217m)\u2212Rm(w\u2217n) is always negative. I.e.,\nRm(w \u2217 m)\u2212Rm(w\u2217n) \u2264 0. (45)\nMoreover, we can use the triangle inequality to bound the difference Rm(w\u2217n)\u2212Rn(w\u2217n) in (41) as\nRm(w \u2217 n)\u2212Rn(w\u2217n) = Lm(w\u2217n)\u2212 Ln(w\u2217n) + c(Vm \u2212 Vn) 2 \u2016w\u2217n\u20162\n\u2264 n\u2212m n (Vn\u2212m + Vm) + c(Vm \u2212 Vn) 2 \u2016w\u2217n\u20162. (46)\nReplacing the differences in (41) by the upper bounds in (43)-(46) follows\nRn(wm)\u2212Rn(w\u2217n) \u2264 Vm + 2(n\u2212m)\nn (Vn\u2212m + Vm) + c(Vm \u2212 Vn) 2 \u2016w\u2217n\u20162 w.h.p. (47)\nSubstitute \u2016w\u2217n\u20162 in (47) by the upper bound in (27) to obtain the result in (21)."}], "references": [{"title": "Convexity, classification, and risk bounds", "author": ["Peter L Bartlett", "Michael I Jordan", "Jon D McAuliffe"], "venue": "Journal of the American Statistical Association,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2006}, {"title": "A fast iterative shrinkage-thresholding algorithm for linear inverse problems", "author": ["Amir Beck", "Marc Teboulle"], "venue": "SIAM journal on imaging sciences,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2009}, {"title": "Sgd-qn: Careful quasi-newton stochastic gradient descent", "author": ["Antoine Bordes", "L\u00e9on Bottou", "Patrick Gallinari"], "venue": "The Journal of Machine Learning Research,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2009}, {"title": "The tradeoffs of large scale learning", "author": ["Olivier Bousquet", "L\u00e9on Bottou"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2008}, {"title": "Convex Optimization", "author": ["Stephen Boyd", "Lieven Vandenberghe"], "venue": null, "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2004}, {"title": "Starting small\u2013learning with adaptive sample sizes", "author": ["Hadi Daneshmand", "Aurelien Lucchi", "Thomas Hofmann"], "venue": "arXiv preprint arXiv:1603.02839,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2016}, {"title": "Saga: A fast incremental gradient method with support for non-strongly convex composite objectives", "author": ["Aaron Defazio", "Francis Bach", "Simon Lacoste-Julien"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2014}, {"title": "Convergence rates of sub-sampled newton methods", "author": ["Murat A Erdogdu", "Andrea Montanari"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2015}, {"title": "Competing with the empirical risk minimizer in a single pass", "author": ["Roy Frostig", "Rong Ge", "Sham M Kakade", "Aaron Sidford"], "venue": "arXiv preprint arXiv:1412.6606,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2014}, {"title": "Stochastic block bfgs: Squeezing more curvature out of data", "author": ["Robert M Gower", "Donald Goldfarb", "Peter Richt\u00e1rik"], "venue": "arXiv preprint arXiv:1603.09649,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2016}, {"title": "A globally convergent incremental newton method", "author": ["Mert G\u00fcrb\u00fczbalaban", "Asuman Ozdaglar", "Pablo Parrilo"], "venue": "Mathematical Programming,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2015}, {"title": "Accelerating stochastic gradient descent using predictive variance reduction", "author": ["Rie Johnson", "Tong Zhang"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2013}, {"title": "Semi-stochastic gradient descent methods", "author": ["Jakub Kone\u010dn\u1ef3", "Peter Richt\u00e1rik"], "venue": "arXiv preprint arXiv:1312.1666,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2013}, {"title": "Res: Regularized stochastic bfgs algorithm", "author": ["Aryan Mokhtari", "Alejandro Ribeiro"], "venue": "Signal Processing, IEEE Transactions on,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2014}, {"title": "Global convergence of online limited memory bfgs", "author": ["Aryan Mokhtari", "Alejandro Ribeiro"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2015}, {"title": "A linearly-convergent stochastic l-bfgs algorithm", "author": ["Philipp Moritz", "Robert Nishihara", "Michael I Jordan"], "venue": "arXiv preprint arXiv:1508.02087,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2015}, {"title": "Introductory lectures on convex programming volume", "author": ["Yu Nesterov"], "venue": "i: Basic course", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 1998}, {"title": "Gradient methods for minimizing composite objective function", "author": ["Yurii Nesterov"], "venue": "Technical report,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2007}, {"title": "Acceleration of stochastic approximation by averaging", "author": ["Boris T Polyak", "Anatoli B Juditsky"], "venue": "SIAM Journal on Control and Optimization,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 1992}, {"title": "A stochastic approximation method", "author": ["Herbert Robbins", "Sutton Monro"], "venue": "The Annals of Mathematical Statistics,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 1951}, {"title": "A stochastic gradient method with an exponential convergence rate for finite training sets", "author": ["Nicolas L Roux", "Mark Schmidt", "Francis R Bach"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2012}, {"title": "A stochastic quasi-newton method for online convex optimization", "author": ["Nicol N Schraudolph", "Jin Yu", "Simon G\u00fcnter"], "venue": "In International Conference on Artificial Intelligence and Statistics,", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2007}, {"title": "Learnability, stability and uniform convergence", "author": ["Shai Shalev-Shwartz", "Ohad Shamir", "Nathan Srebro", "Karthik Sridharan"], "venue": "The Journal of Machine Learning Research,", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2010}, {"title": "Stochastic dual coordinate ascent methods for regularized loss", "author": ["Shai Shalev-Shwartz", "Tong Zhang"], "venue": "The Journal of Machine Learning Research,", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2013}, {"title": "Accelerated proximal stochastic dual coordinate ascent for regularized loss minimization", "author": ["Shai Shalev-Shwartz", "Tong Zhang"], "venue": "Mathematical Programming,", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2016}, {"title": "The nature of statistical learning theory", "author": ["Vladimir Vapnik"], "venue": "Springer Science & Business Media,", "citeRegEx": "26", "shortCiteRegEx": "26", "year": 2013}, {"title": "A proximal stochastic gradient method with progressive variance reduction", "author": ["Lin Xiao", "Tong Zhang"], "venue": "SIAM Journal on Optimization,", "citeRegEx": "27", "shortCiteRegEx": "27", "year": 2014}, {"title": "Linear convergence with condition number independent access of full gradients", "author": ["Lijun Zhang", "Mehrdad Mahdavi", "Rong Jin"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "28", "shortCiteRegEx": "28", "year": 2013}], "referenceMentions": [{"referenceID": 19, "context": "First order stochastic optimization has a long history [20, 19] but the last decade has seen fundamental progress in developing alternatives with faster convergence.", "startOffset": 55, "endOffset": 63}, {"referenceID": 18, "context": "First order stochastic optimization has a long history [20, 19] but the last decade has seen fundamental progress in developing alternatives with faster convergence.", "startOffset": 55, "endOffset": 63}, {"referenceID": 17, "context": "A partial list of this consequential literature includes Nesterov acceleration [18, 2], stochastic averaging gradient [21, 7], variance reduction [12, 27], dual coordinate methods [24, 25], and hybrid algorithms [28, 13].", "startOffset": 79, "endOffset": 86}, {"referenceID": 1, "context": "A partial list of this consequential literature includes Nesterov acceleration [18, 2], stochastic averaging gradient [21, 7], variance reduction [12, 27], dual coordinate methods [24, 25], and hybrid algorithms [28, 13].", "startOffset": 79, "endOffset": 86}, {"referenceID": 20, "context": "A partial list of this consequential literature includes Nesterov acceleration [18, 2], stochastic averaging gradient [21, 7], variance reduction [12, 27], dual coordinate methods [24, 25], and hybrid algorithms [28, 13].", "startOffset": 118, "endOffset": 125}, {"referenceID": 6, "context": "A partial list of this consequential literature includes Nesterov acceleration [18, 2], stochastic averaging gradient [21, 7], variance reduction [12, 27], dual coordinate methods [24, 25], and hybrid algorithms [28, 13].", "startOffset": 118, "endOffset": 125}, {"referenceID": 11, "context": "A partial list of this consequential literature includes Nesterov acceleration [18, 2], stochastic averaging gradient [21, 7], variance reduction [12, 27], dual coordinate methods [24, 25], and hybrid algorithms [28, 13].", "startOffset": 146, "endOffset": 154}, {"referenceID": 26, "context": "A partial list of this consequential literature includes Nesterov acceleration [18, 2], stochastic averaging gradient [21, 7], variance reduction [12, 27], dual coordinate methods [24, 25], and hybrid algorithms [28, 13].", "startOffset": 146, "endOffset": 154}, {"referenceID": 23, "context": "A partial list of this consequential literature includes Nesterov acceleration [18, 2], stochastic averaging gradient [21, 7], variance reduction [12, 27], dual coordinate methods [24, 25], and hybrid algorithms [28, 13].", "startOffset": 180, "endOffset": 188}, {"referenceID": 24, "context": "A partial list of this consequential literature includes Nesterov acceleration [18, 2], stochastic averaging gradient [21, 7], variance reduction [12, 27], dual coordinate methods [24, 25], and hybrid algorithms [28, 13].", "startOffset": 180, "endOffset": 188}, {"referenceID": 27, "context": "A partial list of this consequential literature includes Nesterov acceleration [18, 2], stochastic averaging gradient [21, 7], variance reduction [12, 27], dual coordinate methods [24, 25], and hybrid algorithms [28, 13].", "startOffset": 212, "endOffset": 220}, {"referenceID": 12, "context": "A partial list of this consequential literature includes Nesterov acceleration [18, 2], stochastic averaging gradient [21, 7], variance reduction [12, 27], dual coordinate methods [24, 25], and hybrid algorithms [28, 13].", "startOffset": 212, "endOffset": 220}, {"referenceID": 10, "context": "This difficulty is addressed by incremental computations in [11] and subsampling in [8] or circumvented altogether in stochastic quasi-Newton methods [22, 3, 14, 15, 16, 10].", "startOffset": 60, "endOffset": 64}, {"referenceID": 7, "context": "This difficulty is addressed by incremental computations in [11] and subsampling in [8] or circumvented altogether in stochastic quasi-Newton methods [22, 3, 14, 15, 16, 10].", "startOffset": 84, "endOffset": 87}, {"referenceID": 21, "context": "This difficulty is addressed by incremental computations in [11] and subsampling in [8] or circumvented altogether in stochastic quasi-Newton methods [22, 3, 14, 15, 16, 10].", "startOffset": 150, "endOffset": 173}, {"referenceID": 2, "context": "This difficulty is addressed by incremental computations in [11] and subsampling in [8] or circumvented altogether in stochastic quasi-Newton methods [22, 3, 14, 15, 16, 10].", "startOffset": 150, "endOffset": 173}, {"referenceID": 13, "context": "This difficulty is addressed by incremental computations in [11] and subsampling in [8] or circumvented altogether in stochastic quasi-Newton methods [22, 3, 14, 15, 16, 10].", "startOffset": 150, "endOffset": 173}, {"referenceID": 14, "context": "This difficulty is addressed by incremental computations in [11] and subsampling in [8] or circumvented altogether in stochastic quasi-Newton methods [22, 3, 14, 15, 16, 10].", "startOffset": 150, "endOffset": 173}, {"referenceID": 15, "context": "This difficulty is addressed by incremental computations in [11] and subsampling in [8] or circumvented altogether in stochastic quasi-Newton methods [22, 3, 14, 15, 16, 10].", "startOffset": 150, "endOffset": 173}, {"referenceID": 9, "context": "This difficulty is addressed by incremental computations in [11] and subsampling in [8] or circumvented altogether in stochastic quasi-Newton methods [22, 3, 14, 15, 16, 10].", "startOffset": 150, "endOffset": 173}, {"referenceID": 5, "context": "In this paper we attempt to overcome (i)-(iv) with the Ada Newton algorithm that combines the use of Newton iterations with adaptive sample sizes [6].", "startOffset": 146, "endOffset": 149}, {"referenceID": 0, "context": "Bounds of order Vn = O(1/n) have been derived more recently under stronger regularity conditions that are not uncommon in practice, [1, 9, 4]", "startOffset": 132, "endOffset": 141}, {"referenceID": 8, "context": "Bounds of order Vn = O(1/n) have been derived more recently under stronger regularity conditions that are not uncommon in practice, [1, 9, 4]", "startOffset": 132, "endOffset": 141}, {"referenceID": 3, "context": "Bounds of order Vn = O(1/n) have been derived more recently under stronger regularity conditions that are not uncommon in practice, [1, 9, 4]", "startOffset": 132, "endOffset": 141}, {"referenceID": 22, "context": "Since the regularization in (4) is of order Vn and (3) holds, the difference between Rn(w n) and L(w\u2217) is also of order Vn \u2013 this may be not as immediate as it seems; see [23].", "startOffset": 171, "endOffset": 175}, {"referenceID": 4, "context": ", Chapter 9 of [5].", "startOffset": 15, "endOffset": 18}, {"referenceID": 16, "context": "11 of [17] which shows that", "startOffset": 6, "endOffset": 10}, {"referenceID": 4, "context": ", Chapter 9 of [5].", "startOffset": 15, "endOffset": 18}, {"referenceID": 4, "context": "Thus, according to the quadratic convergence rate of Newton\u2019s method for self-concordant functions [5], we know that the Newton decrement has a quadratic convergence and we can write", "startOffset": 99, "endOffset": 102}, {"referenceID": 6, "context": "The second algorithm is Stochastic Gradient Descent (SGD) and the last one is the SAGA algorithm introduced in [7].", "startOffset": 111, "endOffset": 114}], "year": 2016, "abstractText": "We consider empirical risk minimization for large-scale datasets. We introduce Ada Newton as an adaptive algorithm that uses Newton\u2019s method with adaptive sample sizes. The main idea of Ada Newton is to increase the size of the training set by a factor larger than one in a way that the minimization variable for the current training set is in the local neighborhood of the optimal argument of the next training set. This allows to exploit the quadratic convergence property of Newton\u2019s method and reach the statistical accuracy of each training set with only one iteration of Newton\u2019s method. We show theoretically and empirically that Ada Newton can double the size of the training set in each iteration to achieve the statistical accuracy of the full training set with about two passes over the dataset.", "creator": "LaTeX with hyperref package"}}}