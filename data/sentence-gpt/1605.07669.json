{"id": "1605.07669", "review": {"conference": "ACL", "VERSION": "v1", "DATE_OF_SUBMISSION": "24-May-2016", "title": "On-line Active Reward Learning for Policy Optimisation in Spoken Dialogue Systems", "abstract": "The ability to compute an accurate reward function is essential for optimising a dialogue policy via reinforcement learning. In real-world applications, using explicit user feedback as the reward signal is often unreliable and costly to collect. This problem can be mitigated if the user's intent is known in advance or data is available to pre-train a task success predictor off-line, in a system that includes the following variables:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n", "histories": [["v1", "Tue, 24 May 2016 21:56:08 GMT  (703kb,D)", "https://arxiv.org/abs/1605.07669v1", "Accepted as a long paper in ACL 2016"], ["v2", "Thu, 2 Jun 2016 14:01:07 GMT  (857kb,D)", "http://arxiv.org/abs/1605.07669v2", "Accepted as a long paper in ACL 2016"]], "COMMENTS": "Accepted as a long paper in ACL 2016", "reviews": [], "SUBJECTS": "cs.CL cs.LG", "authors": ["pei-hao su", "milica gasic", "nikola mrksic", "lina maria rojas-barahona", "stefan ultes", "david vandyke", "tsung-hsien wen", "steve j young"], "accepted": true, "id": "1605.07669"}, "pdf": {"name": "1605.07669.pdf", "metadata": {"source": "CRF", "title": "On-line Active Reward Learning for Policy Optimisation in Spoken Dialogue Systems", "authors": ["Pei-Hao Su", "Milica Ga\u0161i\u0107", "Nikola Mrk\u0161i\u0107", "Lina Rojas-Barahona", "Stefan Ultes", "David Vandyke", "Tsung-Hsien Wen"], "emails": ["sjy}@cam.ac.uk"], "sections": [{"heading": "1 Introduction", "text": "Spoken Dialogue Systems (SDS) allow humancomputer interaction using natural speech. They can be broadly divided into two categories: chatoriented systems which aim to converse with users and provide reasonable contextually relevant responses (Vinyals and Le, 2015; Serban et al., 2015), and task-oriented systems designed to assist users to achieve specific goals (e.g. find hotels, movies or bus schedules) (Daubigney et al., 2014; Young et al., 2013). The latter are typically designed according to a structured ontology (or a database schema), which defines the domain\nthat the system can talk about. Teaching a system how to respond appropriately in a task-oriented SDS is non-trivial. This dialogue management task is often formulated as a manually defined dialogue flow that directly determines the quality of interaction. More recently, dialogue management has been formulated as a reinforcement learning (RL) problem which can be automatically optimised (Levin and Pieraccini, 1997; Roy et al., 2000; Williams and Young, 2007; Young et al., 2013). In this framework, the system learns by a trial and error process governed by a potentially delayed learning objective defined by a reward function.\nA typical approach to defining the reward function in a task-oriented dialogue system is to apply a small per-turn penalty to encourage short dialogues and to give a large positive reward at the end of each successful interaction. Figure 1 is an example of a dialogue task which is typically set for users who are being paid to converse with the system. When users are primed with a specific task to complete, dialogue success can be determined from subjective user ratings (Subj), or\nar X\niv :1\n60 5.\n07 66\n9v 2\n[ cs\n.C L\n] 2\nJ un\n2 01\n6\nan objective measure (Obj) based on whether or not the pre-specified task was completed (Walker et al., 1997; Gas\u030cic\u0301 et al., 2013). However, prior knowledge of the user\u2019s goal is not normally available in real situations, making the objective reward estimation approach impractical.\nFurthermore, objective ratings are inflexible and often fail as can be seen from Figure 1, if the user does not strictly follow the task. This results in a mismatch between the Obj and Subj ratings. However, relying on subjective ratings alone is also problematic since crowd-sourced subjects frequently give inaccurate responses and real users are often unwilling to extend the interaction in order to give feedback, resulting in unstable learning (Zhao et al., 2011; Gas\u030cic\u0301 et al., 2011). In order to filter out incorrect user feedback, Gas\u030cic\u0301 et al. (2013) used only dialogues for which Obj = Subj. Nonetheless, this is inefficient and not feasible anyway in most real-world tasks where the user\u2019s goal is generally unknown and difficult to infer.\nIn light of the above, Su et al. (2015a) proposed learning a neural network-based Obj estimator from off-line simulated dialogue data. This removes the need for the Obj check during online policy learning and the resulting policy is as effective as one trained with dialogues using the Obj = Subj check. However, a user simulator will only provide a rough approximation of real user statistics and developing a user simulator is a costly process (Schatzmann et al., 2006).\nTo deal with the above issues, this paper describes an on-line active learning method in which users are asked to provide feedback on whether the dialogue was successful or not. However, active learning is used to limit requests for feedback to only those cases where the feedback would be useful, and also a noise model is introduced to compensate for cases where the user feedback is inaccurate. A Gaussian process classification (GPC) model is utilised to robustly model the uncertainty presented by the noisy user feedback. Since GPC operates on a fixed-length observation space and dialogues are of variable-length, a recurrent neural network (RNN)-based embedding function is used to provide fixed-length dialogue representations. In essence, the proposed method learns a dialogue policy and a reward estimator on-line from scratch, and is directly applicable to real-world applications.\nThe rest of the paper is organised as follows.\nThe next section gives an overview of related work. The proposed framework is then described in \u00a73. This consists of the policy learning algorithm, the creation of the dialogue embedding function and the active reward model trained from real user ratings. In \u00a74, the proposed approach is evaluated in the context of an application providing restaurant information in Cambridge, UK. We first give an in-depth analysis of the dialogue embedding space. The results of the active reward model when it is trained together with a dialogue policy on-line with real users are then presented. Finally, our conclusions are presented in \u00a75."}, {"heading": "2 Related Work", "text": "Dialogue evaluation has been an active research area since late 90s. Walker et al. (1997) proposed the PARADISE framework, where a linear function of task completion and various dialogue features such as dialogue duration were used to infer user satisfaction. This measure was later used as a reward function for learning a dialogue policy (Rieser and Lemon, 2011). However, as noted, task completion is rarely available when the system is interacting with real users and also concerns have been raised regarding the theoretical validity of the model (Larsen, 2003).\nSeveral approaches have been adopted for learning a dialogue reward model given a corpus of annotated dialogues. Yang et al. (2012) used collaborative filtering to infer user preferences. The use of reward shaping has also been investigated in (El Asri et al., 2014; Su et al., 2015b) to enrich the reward function in order to speed up dialogue policy learning. Also, Ultes and Minker (2015) demonstrated that there is a strong correlation between expert\u2019s user satisfaction ratings and dialogue success. However, all these methods assume the availability of reliable dialogue annotations such as expert ratings, which in practice are hard to obtain.\nOne effective way to mitigate the effects of annotator error is to obtain multiple ratings for the same data and several methods have been developed to guide the annotation process with uncertainty models (Dai et al., 2013; Lin et al., 2014). Active learning is particularly useful for determining when an annotation is needed (Settles, 2010; Zhang and Chaudhuri, 2015). It is often utilised using Bayesian optimisation approaches (Brochu et al., 2010). Based on this, Daniel et al. (2014)\nexploited a pool-based active learning method for a robotics application. They queried the user for feedback on the most informative sample collected so far and showed the effectiveness of this method.\nRather than explicitly defining a reward function, inverse RL (IRL) aims to recover the underlying reward from demonstrations of good behaviour and then learn a policy which maximises the recovered reward (Russell, 1998). IRL was first introduced to SDS in (Paek and Pieraccini, 2008), where the reward was inferred from human-human dialogues to mimic the behaviour observed in a corpus. IRL has also been studied in a Wizard-of-Oz (WoZ) setting (Boularias et al., 2010; Rojas Barahona and Cerisara, 2014), where typically a human expert served as the dialogue manager to select each system reply based on the speech understanding output at different noise levels. However, this approach is costly and there is no reason to suppose that a human wizard is acting optimally, especially at high noise levels.\nSince humans are better at giving relative judgements than absolute scores, another related line of research has focused on preference-based approaches to RL (Cheng et al., 2011). In (Sugiyama et al., 2012), users were asked to provide rankings between pairs of dialogues. However, this is also costly and does not scale well in real applications."}, {"heading": "3 Proposed Framework", "text": "The proposed system framework is depicted in Figure 2. It is divided into three main parts: a dialogue policy, a dialogue embedding function, and an active reward model of user feedback. When each dialogue ends, a set of turn-level features ft is extracted and fed into an embedding function \u03c3 to obtain a fixed-dimension dialogue representation d that serves as the input space of the reward model R. This reward is modelled as a Gaussian process which for every input point provides an estimate of task success along with a measure of the estimate uncertainty. Based on this uncertainty, R decides whether to query the user for feedback or not. It then returns a reinforcement signal to update the dialogue policy \u03c0, which is trained using the GP-SARSA algorithm (Gas\u030cic\u0301 and Young, 2014). GP-SARSA also deploys Gaussian process estimation to provide an on-line sample-efficient reinforcement learning algorithm capable of bootstrapping estimates of sparse value functions from minimal numbers of samples (dialogues). The\nquality of each dialogue is defined by its cumulative reward, where each dialogue turn incurs a small negative reward (-1) and the final reward of either 0 or 20 depending on the estimate of task success are provided by the reward model.\nNote that the key contribution here is to learn the noise robust reward model and the dialogue policy simultaneously on-line, using the user as a \u2018supervisor\u2019. Active learning is not an essential component of the framework but highly desirable in practice to minimise the impact of the supervision burden on users. The use of a pre-trained embedding function is a sub-component of the proposed approach and is trained off-line on corpus data rather than manually designed here."}, {"heading": "3.1 Unsupervised Dialogue Embeddings", "text": "In order to model user feedback over dialogues of varying length, an embedding function is used to map each dialogue into a fixed-dimensional continuous-space. The use of embedding functions has recently gained attention especially for word representations, and has boosted performance on several natural language processing tasks (Mikolov et al., 2013; Turian et al., 2010; Levy and Goldberg, 2014). Embedding has also been successfully applied to machine translation (MT) where it enables varying-length phrases to be mapped to fixed-length vectors using an RNN Encoder-Decoder (Cho et al., 2014). Similar to MT, dialogue embedding enables variable length sequences of utterances to be mapped into an appropriate fixed-length vector. Although embedding is used here to create a fixed-dimension input space for the GPC-based task success classifier, it should be noted that it potentially facilitates a variety of other downstream tasks which depend on classification or clustering.\nThe model structure of the embedding function is described on the left of Figure 2, where the episodic turn-level features ft are extracted from a dialogue and serve as the input features to the encoder. In our proposed model, the encoder is a Bi-directional Long Short-Term Memory network (BLSTM) (Hochreiter and Schmidhuber, 1997; Graves et al., 2013). The LSTM is a Recurrent Neural Network (RNN) with gated recurrent units introduced to alleviate the vanishing gradient problem. The BLSTM encoder takes into account the sequential information from both directions of the input data, computing the forward\nhidden sequences \u2212\u2192 h 1:T and the backward hidden sequences \u2190\u2212 h T :1 while iterating over all input features ft, t = 1, ..., T : \u2212\u2192 ht = LSTM(ft, \u2212\u2192 h t\u22121)\n\u2190\u2212 ht = LSTM(ft, \u2190\u2212 h t+1)\nwhere LSTM denotes the activation function. The dialogue representation d is then calculated as the average over all hidden sequences:\nd = 1\nT T\u2211 t=1 ht (1)\nwhere ht = [ \u2212\u2192 ht; \u2190\u2212 ht] is the concatenation of the two directional hidden sequences. Given the dialogue representation d output by the encoder, the decoder is a forward LSTM that takes d as its input for each turn t to produce the sequence of features f \u20321:T .\nThe training objective of the encoder-decoder minimises the mean-square-error (MSE) between the prediction f \u20321:T and the output f1:T (which is also the input):\nMSE = 1\nN N\u2211 i=1 T\u2211 t=1 ||ft \u2212 f \u2032t ||2 (2)\nwhere N is the number of training dialogues and || \u00b7 ||2 denotes the l2-norm. Since all the functions used in the encoder and decoder are differentiable, stochastic gradient decent (SGD) can be used to train the model.\nThe dialogue representations generated by this LSTM-based unsupervised embedding function are then used as the observations for the reward model described in the next section 3.2."}, {"heading": "3.2 Active Reward Learning", "text": "A Gaussian process is a Bayesian non-parametric model that can be used for regression or classification (Rasmussen and Williams, 2006). It is particularly appealing since it can learn from a small number of observations by exploiting the correlations defined by a kernel function and it provides a measure of uncertainty of its estimates. In the context of spoken dialogue systems it has been successfully used for RL policy optimisation (Gas\u030cic\u0301 and Young, 2014; Casanueva et al., 2015) and IRL reward function regression (Kim et al., 2014).\nHere we propose modelling dialogue success as a Gaussian process (GP). This involves estimating the probability p(y|d,D) that the task was successful given the current dialogue representation d and the pool D containing previously classified dialogues. We pose this as a classification problem where the rating is a binary observation y \u2208 {\u22121, 1} that defines failure or success. The observations y are considered to be drawn from a Bernoulli distribution with a success probability p(y = 1|d,D). The probability is related to a latent function f(d|D) : Rdim(d) \u2192 R that is mapped to a unit interval by a probit function p(y = 1|d,D) = \u03c6(f(d|D)), where \u03c6 denotes the cumulative density function of the standard Gaus-\nsian distribution. The latent function is given a GP prior: f(d) \u223c GP(m(d), k(d,d\u2032)), where m(\u00b7) is the mean function and k(\u00b7, \u00b7) the covariance function (kernel). Here the stationary squared exponential kernel kSE is used. It is also combined with a white noise kernel kWN in order to account for the \u201cnoise\u201d in users\u2019 ratings:\nk(d,d\u2032) = p2 exp(\u2212||d\u2212 d \u2032||2\n2l2 ) + \u03c32n (3)\nwhere the first term denotes kSE and the second term kWN .\nThe hyper-parameters p, l, \u03c3n can be adequately optimised by maximising the marginal likelihood using a gradient-based method (Chen et al., 2015). Since \u03c6(\u00b7) is not Gaussian, the resulting posterior probability p(y = 1|d,D) is analytically intractable. So instead an approximation method, expectation propagation (EP), was used (Nickisch and Rasmussen, 2008).\nQuerying the user for feedback is costly and may impact negatively on the user experience. This impact can be reduced by using active learning informed by the uncertainty estimate of the GP model (Kapoor et al., 2007). This ensures that user feedback is only sought when the model is uncertain about its current prediction. For the current application, an on-line (stream-based) version of active learning is required.\nAn illustration of a 1-dimensional example is shown in Figure 3. Given the labelled data D, the predictive posterior mean \u00b5\u2217 and posterior variance \u03c32\u2217 of the latent value f(d\u2217) for the current dialogue representation d\u2217 can be calculated. Then a threshold interval [1 \u2212 \u03bb, \u03bb] is set on the predictive success probability p(y\u2217 = 1|d\u2217,D) = \u03c6(\u00b5\u2217/ \u221a 1 + \u03c32\u2217) to decide whether this dialogue should be labelled or not. The decision boundary implicitly considers both the posterior mean as well as the variance.\nWhen deploying this reward model in the proposed framework, a GP with a zero-mean prior for f is initialised and D = {}. After the dialogue policy \u03c0 completes each episode with the user, the generated dialogue turns are transformed into the dialogue representation d = \u03c3(f1:T ) using the dialogue embedding function \u03c3. Given d, the predictive mean and variance of f(d|D) are determined, and the reward model decides whether or not it should seek user feedback based on the threshold \u03bb on \u03c6(f(d|D)). If the model is uncertain, the\nuser\u2019s feedback on the current episode d is used to update the GP model and to generate the reinforcement signal for training the policy \u03c0; otherwise the predictive success rating from the reward model is used directly to update the policy. This process takes place after each dialogue."}, {"heading": "4 Experimental results", "text": "The target application is a live telephone-based spoken dialogue system providing restaurant information for the Cambridge (UK) area. The domain consists of approximately 150 venues each having 6 slots (attributes) of which 3 can be used by the system to constrain the search (food-type, area and price-range) and the remaining 3 are informable properties (phone-number, address and postcode) available once a required database entity has been found.\nThe shared core components of the SDS common to all experiments comprise a HMM-based recogniser, a confusion network (CNet) semantic input decoder (Henderson et al., 2012), the BUDS belief state tracker (Thomson and Young, 2010) that factorises the dialogue state using a dynamic Bayesian network, and a template based natural language generator to map system semantic actions into natural language responses to the user. All policies were trained using the GP-SARSA algorithm and the summary action space of the RL policy contains 20 actions.\nThe reward given to each dialogue was set to 20 \u00d7 1success \u2212 N , where N is the dialogue turn\nnumber and 1 is the indicator function for dialogue success, which is determined by different methods as described in the following section. These rewards constitute the reinforcement signal used for policy learning."}, {"heading": "4.1 Dialogue representations", "text": "The LSTM Encoder-Decoder model described in \u00a73.1 was used to generate an embedding d for each dialogue. For each dialogue turn that contains a user\u2019s utterance and a system\u2019s response, a feature vector f of size 74 was extracted (Vandyke et al., 2015). This vector consists of the concatenation of the most likely user intention determined by the semantic decoder, the distribution over each concept of interest defined in the ontology, a onehot encoding of the system\u2019s reply action, and the turn number normalised by the maximum number of turns (here 30). This feature vector was used as the input and the target for the LSTM EncoderDecoder model, where the training objective was to minimise the MSE of the reconstruction loss.\nThe model was implemented using the Theano library (Bergstra et al., 2010; Bastien et al., 2012). A corpus consisting of 8565, 1199 and 650 real user dialogues in the Cambridge restaurant domain was used for training, validation and testing respectively. This corpus was collected via the Amazon Mechanical Turk (AMT) service, where paid subjects interacted with the dialogue system. The sizes of \u2212\u2192 ht and \u2190\u2212 ht in the encoder and the hidden layer in the decoder were all 32, resulting in dim(ht) = dim(d) = 64. SGD per dialogue was used during backpropagation to train each model. In order to prevent over-fitting, early stopping was applied based on the held-out validation set.\nIn order to visualise the impact of the embeddings, the dialogue representations of all the 650 test dialogues were transformed by the embedding function in Figure 4 and reduced to two dimensions using t-SNE (Van der Maaten and Hinton, 2008). For each dialogue sample, the shape indicates whether or not the dialogue was successful, and the colour indicates the length of the dialogue (maximum 30 turns).\nFrom the figure we can clearly see the colour gradient from the top left (shorter dialogues) to the bottom right (longer dialogues) for the positive Subj labels. This shows that dialogue length was one of the prominent features in the dialogue representation d. It can also be seen that the longer\nfailed dialogues (more than 15 turns) are located close to each other, mostly at the bottom right. On the other hand, there are other failed dialogues which are spread throughout the cluster. We can also see that the successful dialogues were on average shorter than 10 turns, which is consistent with the claim that users do not engage in longer dialogues with well-trained task-oriented systems.\nThis visualisation shows the potential of the unsupervised dialogue embedding since the transformed dialogue representations appear to be correlated with dialogue success in the majority of cases. For the purpose of GP reward modelling, this LSTM Encoder-Decoder embedding function appears therefore to be suitable for extracting an adequate fixed-dimension dialogue representation."}, {"heading": "4.2 Dialogue Policy Learning", "text": "Given the well-trained dialogue embedding function, the proposed GP reward model operates on this input space. The system was implemented using the GPy library (Hensman et al., 2012). Given the predictive success probability of each newly seen dialogue, the threshold \u03bb for the uncertainty region was initially set to 1 to encourage label querying and annealed to 0.85 for the first 50 collected dialogues and then set to 0.85 thereafter.\nInitially, as each new dialogue was added to the training set, the hyper-parameters that defined the structure of the kernels mentioned in Eqn. 3 were optimised to minimise the negative log marginal likelihood using conjugate gradient as-\nFigure 5: Learning curves showing subjective success as a function of the number of training dialogues used during on-line policy optimisation. The on-line GP, Subj, off-line RNN and Obj=Subj systems are shown as black, yellow, blue, and red lines. The light-coloured areas are one standard error intervals.\ncent (Rasmussen and Williams, 2006). To prevent overfitting, after the first 40 dialogues, these hyper-parameters were only re-optimised after every batch of 20 dialogues.\nTo investigate the performance of the proposed on-line GP policy learning, three other contrasting systems were also tested. Note that the handcrafted system is not compared since it does not scale to larger domains and is sensitive to speech recognition errors. In each case, the only difference was the method used to compute the reward:\n\u2022 the Obj=Subj system which uses prior knowledge of the task to only use training dialogues for which the user\u2019s subjective assessment of success is consistent with the objective assessment of success as in (Gas\u030cic\u0301 et al., 2013).\n\u2022 the Subj system which directly optimises the policy using only the user assessment of success whether accurate or not.\n\u2022 the off-line RNN system that uses 1K simulated data and the corresponding Obj labels to train an RNN success estimator as in (Su et al., 2015a).\nFor the Subj system rating, in order to focus solely on the performance of the policy rather than other aspects of the system such as the fluency of the reply sentence, users were asked to rate dialogue success by answering the following question: Did you find all the information you were looking for?\nFigure 6: The number of times each system queries the user for feedback during on-line policy optimisation as a function of the number of training dialogues. The orange line represents both the Obj=Subj and Subj systems, and the black line represents the on-line GP system.\nAll four of the above systems were trained with a total of 500 dialogues on-line by users recruited via the AMT service. Figure 5 shows the online learning curve of the subjective success rating when during training. For each system, the moving average was calculated using a window of 150 dialogues. In each case, three distinct policies were trained and the results were averaged to reduce noise.\nAs can be seen, all four systems perform better than 80 % subjective success rate after approximately 500 training dialogues. The Obj=Subj system is relatively poor compared to the others. This might be because users often report success even though the objective evaluation indicates failure. In such cases, the dialogue is discarded and not used for training. As a consequence, the Obj=Subj system required approximately 700 dialogues in order to obtain 500 which were useful, whereas all other systems made use of every dialogue.\nTo investigate learning behaviour over longer spans, training for the on-line GP and the Subj systems was extended to 850 dialogues. As can be seen, performance in both cases is broadly flat.\nSimilar to the conclusions drawn in (Gas\u030cic\u0301 et al., 2011), the Subj system suffers from unreliable user feedback. Firstly, as in the Obj=Subj system, users forget the full requirements of the task and in particular, forget to ask for all required information. Secondly, users give inconsistent feedback due to a lack of proper care and attention. From Figure 5 it can be clearly seen that the online GP system consistently performed better than\nSubj system, presumably, because its noise model mitigates the effect of inconsistency in user feedback. Of course, unlike crowd-sourced subjects, real users might provide more consistent feedback, but nevertheless, some inconsistency is inevitable and the noise model offers the needed robustness.\nThe advantage of the on-line GP system in reducing the number of times that the system requests user feedback (i.e. the label cost) can be seen in Figure 6. The black curve shows the number of active learning queries triggered in the online GP system averaged across the three policies. This system required only 150 user feedback requests to train a robust reward model. On the other hand, the Obj=Subj and Subj systems require user feedback for every training dialogue as shown by the dashed orange line.\nOf course, the off-line RNN system required no user feedback at all when training the system online since it had the benefit of prior access to a user simulator. Its performance during training after the first 300 dialogues was, however, inferior to the on-line GP system."}, {"heading": "4.3 Dialogue Policy Evaluation", "text": "In order to compare performance, the averaged results obtained between 400-500 training dialogues are shown in the first section of Table 1 along with one standard error. For the 400-500 interval, the Subj, off-line RNN and on-line GP systems achieved comparable results without statistical differences. The results of continuing training on the Subj and on-line GP systems from 500 to 850 training dialogues are also shown. As can be seen, the on-line GP system was significantly better presumably because it is more robust to erroneous user feedback compared to the Subj system."}, {"heading": "4.4 Reward Model Evaluation", "text": "The above results verify the effectiveness of the proposed reward model for policy learning. Here we investigate further the accuracy of the model in predicting the subjective success rate. An evaluation of the on-line GP reward model between 1 and 850 training dialogues is presented in Table 2.\nSince three reward models were learnt each with 850 dialogues, there were a total of 2550 training dialogues. Of these, the models queried the user for feedback a total of 454 times, leaving 2096 dialogues for which learning relied on the reward model\u2019s prediction. The results shown in the table are thus the average over 2096 dialogues.\nAs can be seen, there was a significant imbalance between success and fail labels since the policy was improving along with the training dialogues. This lowered the recall on failed dialogue prediction as the model was biased to data with positive labels. Nevertheless, its precision scores well. On the other hand, the successful dialogues were accurately predicted by the proposed model."}, {"heading": "4.5 Example Dialogues", "text": "The key benefits of the on-line GP reward model compared to other models are its robustness to noise and efficient use of user supervision. Since the four systems compared above differ only in the design of reward model (learning objective), their on-line behaviours were broadly similar.\nTwo example dialogues between users and the on-line GP system are listed in Table 3 to illustrate how the system operates under different noise conditions. The user\u2019s subjective rating and the rating determined by the on-line GP reward model are also shown. The labels \u2018n-th ASR\u2019 and \u2018nth SEM\u2019 indicate the n-th most likely hypotheses from speech recogniser and semantic decoder respectively."}, {"heading": "5 Conclusion", "text": "In this paper we have proposed an active reward learning model using Gaussian process classification and an unsupervised neural network-based dialogue embedding to enable truly on-line policy learning in spoken dialogue systems. The system enables stable policy optimisation by robustly modelling the inherent noise in real user feedback and uses active learning to minimise the number of feedback requests to the user. We found that the proposed model achieved efficient policy learning and better performance compared to other stateof-the-art methods in the Cambridge restaurant domain. A key advantage of this Bayesian model is that its uncertainty estimate allows active learning and noise handling in a natural way. The unsupervised dialogue embedding function required no labelled data to train whilst providing a compact and useful input to the reward predictor. Overall, the techniques developed in this paper enable for the first time a viable approach to on-line learning\nin deployed real-world dialogue systems which does not need a large corpus of manually annotated data or the construction of a user simulator.\nConsistent with all of our previous work, the reward function studied here is focused primarily on task success. This may be too simplistic for many commercial applications and further work will be needed in conjunction with human interaction experts to identify and incorporate the extra dimensions of dialogue quality that will be needed to achieve the highest levels of user satisfaction."}, {"heading": "Acknowledgments", "text": "Pei-Hao Su is supported by Cambridge Trust and the Ministry of Education, Taiwan. This research was partly funded by the EPSRC grant EP/M018946/1 Open Domain Statistical Spoken Dialogue Systems. The data used in the experiments is available at www.repository.cam.ac.uk/handle/1810/256020."}], "references": [{"title": "Theano: new features and speed improvements", "author": ["Pascal Lamblin", "Razvan Pascanu", "James Bergstra", "Ian J. Goodfellow", "Arnaud Bergeron", "Nicolas Bouchard", "Yoshua Bengio"], "venue": null, "citeRegEx": "Bastien et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Bastien et al\\.", "year": 2012}, {"title": "Theano: a CPU and GPU math expression compiler", "author": ["Olivier Breuleux", "Fr\u00e9d\u00e9ric Bastien", "Pascal Lamblin", "Razvan Pascanu", "Guillaume Desjardins", "Joseph Turian", "David WardeFarley", "Yoshua Bengio"], "venue": null, "citeRegEx": "Bergstra et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Bergstra et al\\.", "year": 2010}, {"title": "Learning the reward model of dialogue pomdps from data", "author": ["Hamid R Chinaei", "Brahim Chaib-draa"], "venue": "In NIPS Workshop on Machine Learning for Assistive Techniques", "citeRegEx": "Boularias et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Boularias et al\\.", "year": 2010}, {"title": "A tutorial on bayesian optimization of expensive cost functions, with application to active user modeling and hierarchical reinforcement learning", "author": ["Brochu et al.2010] Eric Brochu", "Vlad M Cora", "Nando De Freitas"], "venue": null, "citeRegEx": "Brochu et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Brochu et al\\.", "year": 2010}, {"title": "Knowledge transfer between speakers for personalised dialogue management", "author": ["Thomas Hain", "Heidi Christensen", "Ricard Marxer", "Phil Green"], "venue": "In Proc of SigDial", "citeRegEx": "Casanueva et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Casanueva et al\\.", "year": 2015}, {"title": "Hyper-parameter optimisation of gaussian process reinforcement learning for statistical dialogue management", "author": ["Chen et al.2015] Lu Chen", "Pei-Hao Su", "Milica"], "venue": "Gas\u030cic", "citeRegEx": "Chen et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Chen et al\\.", "year": 2015}, {"title": "Preference-based policy iteration: Leveraging preference learning for reinforcement learning. In Machine learning and knowledge", "author": ["Cheng et al.2011] Weiwei Cheng", "Johannes F\u00fcrnkranz", "Eyke H\u00fcllermeier", "Sang-Hyeun Park"], "venue": null, "citeRegEx": "Cheng et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Cheng et al\\.", "year": 2011}, {"title": "Learning phrase representations using rnn encoder\u2013decoder for statistical machine", "author": ["Cho et al.2014] Kyunghyun Cho", "Dzmitry Bahdanau", "Fethi Bougares Holger Schwenk", "Yoshua Bengio"], "venue": null, "citeRegEx": "Cho et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Cho et al\\.", "year": 2014}, {"title": "Pomdp-based control of workflows for crowdsourcing", "author": ["Dai et al.2013] Peng Dai", "Christopher H Lin", "Daniel S Weld"], "venue": "Artificial Intelligence,", "citeRegEx": "Dai et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Dai et al\\.", "year": 2013}, {"title": "Active reward learning", "author": ["Malte Viering", "Jan Metz", "Oliver Kroemer", "Jan Peters"], "venue": "In Proc of RSS", "citeRegEx": "Daniel et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Daniel et al\\.", "year": 2014}, {"title": "A comprehensive reinforcement learning framework for dialogue management", "author": ["Matthieu Geist", "Senthilkumar Chandramohan", "Olivier Pietquin"], "venue": null, "citeRegEx": "Daubigney et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Daubigney et al\\.", "year": 2014}, {"title": "Task completion transfer learning for reward inference", "author": ["Romain Laroche", "Olivier Pietquin"], "venue": "In Proc of MLIS", "citeRegEx": "Asri et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Asri et al\\.", "year": 2014}, {"title": "Gaussian processes for pomdp-based dialogue manager optimization", "author": ["Ga\u0161i\u0107", "Young2014] Milica Ga\u0161i\u0107", "Steve Young"], "venue": null, "citeRegEx": "Ga\u0161i\u0107 et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Ga\u0161i\u0107 et al\\.", "year": 2014}, {"title": "Online policy optimisation of spoken dialogue systems via live interaction with human subjects", "author": ["Ga\u0161i\u0107 et al.2011] Milica Ga\u0161i\u0107", "Filip Jurcicek", "Blaise. Thomson", "Kai Yu", "Steve Young"], "venue": null, "citeRegEx": "Ga\u0161i\u0107 et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Ga\u0161i\u0107 et al\\.", "year": 2011}, {"title": "On-line policy optimisation of bayesian spoken dialogue systems via human", "author": ["Ga\u0161i\u0107 et al.2013] Milica Ga\u0161i\u0107", "Catherine Breslin", "Matthew Henderson", "Dongho Kim", "Martin Szummer", "Blaise Thomson", "Pirros Tsiakoulis", "Steve J. Young"], "venue": null, "citeRegEx": "Ga\u0161i\u0107 et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Ga\u0161i\u0107 et al\\.", "year": 2013}, {"title": "Hybrid speech recognition with deep bidirectional lstm", "author": ["Graves et al.2013] Alax Graves", "Navdeep Jaitly", "Abdel-rahman Mohamed"], "venue": "In IEEE ASRU", "citeRegEx": "Graves et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Graves et al\\.", "year": 2013}, {"title": "Discriminative spoken language understanding using word confusion networks", "author": ["Milica Ga\u0161i\u0107", "Blaise Thomson", "Pirros Tsiakoulis", "Kai Yu", "Steve Young"], "venue": null, "citeRegEx": "Henderson et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Henderson et al\\.", "year": 2012}, {"title": "GPy: A gaussian process framework in python. http: //github.com/SheffieldML/GPy", "author": ["Nicolo Fusi", "Ricardo Andrade", "Nicolas Durrande", "Alan Saul", "Max Zwiessele", "Neil D. Lawrence"], "venue": null, "citeRegEx": "Hensman et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Hensman et al\\.", "year": 2012}, {"title": "Long short-term memory", "author": ["Hochreiter", "Schmidhuber1997] Sepp Hochreiter", "J\u00fcrgen Schmidhuber"], "venue": "Neural computation,", "citeRegEx": "Hochreiter et al\\.,? \\Q1997\\E", "shortCiteRegEx": "Hochreiter et al\\.", "year": 1997}, {"title": "Active learning with gaussian processes for object categorization", "author": ["Kapoor et al.2007] Ashish Kapoor", "Kristen Grauman", "Raquel Urtasun", "Trevor Darrell"], "venue": "In Proc of ICCV", "citeRegEx": "Kapoor et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Kapoor et al\\.", "year": 2007}, {"title": "Inverse reinforcement learning for micro-turn management", "author": ["Kim et al.2014] Dongho Kim", "Catherine Breslin", "Pirros Tsiakoulis", "Matthew Henderson", "Steve J Young"], "venue": null, "citeRegEx": "Kim et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Kim et al\\.", "year": 2014}, {"title": "Issues in the evaluation of spoken dialogue systems using objective and subjective measures", "author": ["L.B. Larsen"], "venue": "In IEEE ASRU", "citeRegEx": "Larsen.,? \\Q2003\\E", "shortCiteRegEx": "Larsen.", "year": 2003}, {"title": "A stochastic model of computerhuman interaction for learning dialogue strategies. Eurospeech", "author": ["Levin", "Pieraccini1997] Esther Levin", "Roberto Pieraccini"], "venue": null, "citeRegEx": "Levin et al\\.,? \\Q1997\\E", "shortCiteRegEx": "Levin et al\\.", "year": 1997}, {"title": "Neural word embedding as implicit matrix factorization", "author": ["Levy", "Goldberg2014] Omer Levy", "Yoav Goldberg"], "venue": null, "citeRegEx": "Levy et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Levy et al\\.", "year": 2014}, {"title": "To re (label), or not to re (label)", "author": ["Daniel S Weld"], "venue": "In Second AAAI Conference on Human Computation and Crowdsourcing", "citeRegEx": "Lin and Weld,? \\Q2014\\E", "shortCiteRegEx": "Lin and Weld", "year": 2014}, {"title": "Distributed representations of words and phrases and their compositionality", "author": ["Ilya Sutskever", "Kai Chen", "Greg S Corrado", "Jeff Dean"], "venue": null, "citeRegEx": "Mikolov et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Mikolov et al\\.", "year": 2013}, {"title": "Approximations for binary gaussian process classification", "author": ["Nickisch", "Rasmussen2008] Hannes Nickisch", "Carl Edward Rasmussen"], "venue": null, "citeRegEx": "Nickisch et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Nickisch et al\\.", "year": 2008}, {"title": "Automating spoken dialogue management design using machine learning: An industry perspective", "author": ["Paek", "Pieraccini2008] Tim Paek", "Roberto Pieraccini"], "venue": "Speech communication,", "citeRegEx": "Paek et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Paek et al\\.", "year": 2008}, {"title": "Gaussian processes for machine learning", "author": ["Rasmussen", "Chris Williams"], "venue": null, "citeRegEx": "Rasmussen et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Rasmussen et al\\.", "year": 2006}, {"title": "Learning and evaluation of dialogue strategies for new applications: Empirical methods for optimization from small data sets", "author": ["Rieser", "Lemon2011] Verena Rieser", "Oliver Lemon"], "venue": null, "citeRegEx": "Rieser et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Rieser et al\\.", "year": 2011}, {"title": "Bayesian Inverse Reinforcement Learning for Modeling Conversational Agents in a Virtual Environment", "author": ["Rojas Barahona", "Christophe Cerisara"], "venue": "In Conference on Intelligent Text", "citeRegEx": "Barahona et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Barahona et al\\.", "year": 2014}, {"title": "Spoken dialogue management using probabilistic reasoning", "author": ["Roy et al.2000] Nicholas Roy", "Joelle Pineau", "Sebastian Thrun"], "venue": "In Proc of SigDial", "citeRegEx": "Roy et al\\.,? \\Q2000\\E", "shortCiteRegEx": "Roy et al\\.", "year": 2000}, {"title": "Learning agents for uncertain environments", "author": ["Stuart Russell"], "venue": "In Proc of COLT", "citeRegEx": "Russell.,? \\Q1998\\E", "shortCiteRegEx": "Russell.", "year": 1998}, {"title": "A survey of statistical user simulation techniques for reinforcement-learning of dialogue management strategies", "author": ["Karl Weilhammer", "Matt Stuttle", "Steve Young"], "venue": "The knowledge engineering review,", "citeRegEx": "Schatzmann et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Schatzmann et al\\.", "year": 2006}, {"title": "Hierarchical neural network generative models for movie dialogues", "author": ["Alessandro Sordoni", "Yoshua Bengio", "Aaron Courville", "Joelle Pineau"], "venue": "arXiv preprint arXiv:1507.04808", "citeRegEx": "Serban et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Serban et al\\.", "year": 2015}, {"title": "Active learning literature survey", "author": ["Burr Settles"], "venue": "Computer Sciences Technical Report", "citeRegEx": "Settles.,? \\Q2010\\E", "shortCiteRegEx": "Settles.", "year": 2010}, {"title": "Learning from real users: Rating dialogue success with neural networks for reinforcement learning in spoken dialogue", "author": ["Su et al.2015a] Pei-Hao Su", "David Vandyke", "Milica Ga\u0161i\u0107", "Dongho Kim", "Nikola Mrk\u0161i\u0107", "Tsung-Hsien Wen", "Steve Young"], "venue": null, "citeRegEx": "Su et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Su et al\\.", "year": 2015}, {"title": "Reward shaping with recurrent neural networks for speeding up on-line policy learning in spoken dialogue systems", "author": ["Su et al.2015b] Pei-Hao Su", "David Vandyke", "Milica Ga\u0161i\u0107", "Nikola Mrk\u0161i\u0107", "Tsung-Hsien Wen", "Steve Young"], "venue": "Proc of SigDial", "citeRegEx": "Su et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Su et al\\.", "year": 2015}, {"title": "Preferencelearning based inverse reinforcement learning for dialog control", "author": ["Toyomi Meguro", "Yasuhiro Minami"], "venue": "In Proc of Interspeech", "citeRegEx": "Sugiyama et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Sugiyama et al\\.", "year": 2012}, {"title": "Bayesian update of dialogue state: A pomdp framework for spoken dialogue systems", "author": ["Thomson", "Young2010] Blaise Thomson", "Steve Young"], "venue": "Computer Speech and Language,", "citeRegEx": "Thomson et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Thomson et al\\.", "year": 2010}, {"title": "Word representations: a simple and general method for semi-supervised learning", "author": ["Turian et al.2010] Joseph Turian", "Lev Ratinov", "Yoshua Bengio"], "venue": "In Proc of ACL", "citeRegEx": "Turian et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Turian et al\\.", "year": 2010}, {"title": "Quality-adaptive spoken dialogue initiative selection and implications on reward modelling", "author": ["Ultes", "Minker2015] Stefan Ultes", "Wolfgang Minker"], "venue": "In Proc of SigDial", "citeRegEx": "Ultes et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Ultes et al\\.", "year": 2015}, {"title": "Visualizing data using t-sne", "author": ["Van der Maaten", "Geoffrey Hinton"], "venue": null, "citeRegEx": "Maaten et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Maaten et al\\.", "year": 2008}, {"title": "Multi-domain dialogue success classifiers for policy training", "author": ["Pei-Hao Su", "Milica Ga\u0161i\u0107", "Nikola Mrk\u0161i\u0107", "Tsung-Hsien Wen", "Steve Young"], "venue": null, "citeRegEx": "Vandyke et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Vandyke et al\\.", "year": 2015}, {"title": "A neural conversational model. arXiv preprint arXiv:1506.05869", "author": ["Vinyals", "Le2015] Oriol Vinyals", "Quoc Le"], "venue": null, "citeRegEx": "Vinyals et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Vinyals et al\\.", "year": 2015}, {"title": "PARADISE: A framework for evaluating spoken dialogue agents", "author": ["Diane J. Litman", "Candace A. Kamm", "Alicia Abella"], "venue": "Proc of EACL", "citeRegEx": "Walker et al\\.,? \\Q1997\\E", "shortCiteRegEx": "Walker et al\\.", "year": 1997}, {"title": "Partially observable Markov decision processes for spoken dialog systems", "author": ["Williams", "Young2007] Jason D. Williams", "Steve Young"], "venue": "Computer Speech and Language,", "citeRegEx": "Williams et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Williams et al\\.", "year": 2007}, {"title": "Predicting user satisfaction in spoken dialog system evaluation with collaborative filtering", "author": ["Yang et al.2012] Zhaojun Yang", "G Levow", "Helen Meng"], "venue": "IEEE Journal of Selected Topics in Signal Processing,", "citeRegEx": "Yang et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Yang et al\\.", "year": 2012}, {"title": "Pomdp-based statistical spoken dialogue systems: a review", "author": ["Young et al.2013] Steve Young", "Milica Ga\u0161ic", "Blaise Thomson", "Jason Williams"], "venue": "In Proc of IEEE,", "citeRegEx": "Young et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Young et al\\.", "year": 2013}, {"title": "Active learning from weak and strong labelers", "author": ["Zhang", "Chaudhuri2015] Chicheng Zhang", "Kamalika Chaudhuri"], "venue": null, "citeRegEx": "Zhang et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Zhang et al\\.", "year": 2015}, {"title": "Incremental relabeling for active learning with noisy crowdsourced annotations", "author": ["Zhao et al.2011] Liyue Zhao", "Gita Sukthankar", "Rahul Sukthankar"], "venue": "In Proc of PASSAT and Proc of SocialCom", "citeRegEx": "Zhao et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Zhao et al\\.", "year": 2011}], "referenceMentions": [{"referenceID": 34, "context": "They can be broadly divided into two categories: chatoriented systems which aim to converse with users and provide reasonable contextually relevant responses (Vinyals and Le, 2015; Serban et al., 2015), and task-oriented systems designed to assist users to achieve specific goals (e.", "startOffset": 158, "endOffset": 201}, {"referenceID": 10, "context": "find hotels, movies or bus schedules) (Daubigney et al., 2014; Young et al., 2013).", "startOffset": 38, "endOffset": 82}, {"referenceID": 48, "context": "find hotels, movies or bus schedules) (Daubigney et al., 2014; Young et al., 2013).", "startOffset": 38, "endOffset": 82}, {"referenceID": 31, "context": "More recently, dialogue management has been formulated as a reinforcement learning (RL) problem which can be automatically optimised (Levin and Pieraccini, 1997; Roy et al., 2000; Williams and Young, 2007; Young et al., 2013).", "startOffset": 133, "endOffset": 225}, {"referenceID": 48, "context": "More recently, dialogue management has been formulated as a reinforcement learning (RL) problem which can be automatically optimised (Levin and Pieraccini, 1997; Roy et al., 2000; Williams and Young, 2007; Young et al., 2013).", "startOffset": 133, "endOffset": 225}, {"referenceID": 45, "context": "not the pre-specified task was completed (Walker et al., 1997; Ga\u0161i\u0107 et al., 2013).", "startOffset": 41, "endOffset": 82}, {"referenceID": 14, "context": "not the pre-specified task was completed (Walker et al., 1997; Ga\u0161i\u0107 et al., 2013).", "startOffset": 41, "endOffset": 82}, {"referenceID": 50, "context": "der to give feedback, resulting in unstable learning (Zhao et al., 2011; Ga\u0161i\u0107 et al., 2011).", "startOffset": 53, "endOffset": 92}, {"referenceID": 13, "context": "der to give feedback, resulting in unstable learning (Zhao et al., 2011; Ga\u0161i\u0107 et al., 2011).", "startOffset": 53, "endOffset": 92}, {"referenceID": 12, "context": ", 2011; Ga\u0161i\u0107 et al., 2011). In order to filter out incorrect user feedback, Ga\u0161i\u0107 et al. (2013) used only dialogues for which Obj = Subj.", "startOffset": 8, "endOffset": 97}, {"referenceID": 36, "context": "In light of the above, Su et al. (2015a) proposed learning a neural network-based Obj estimator from off-line simulated dialogue data.", "startOffset": 23, "endOffset": 41}, {"referenceID": 33, "context": "However, a user simulator will only provide a rough approximation of real user statistics and developing a user simulator is a costly process (Schatzmann et al., 2006).", "startOffset": 142, "endOffset": 167}, {"referenceID": 45, "context": "Walker et al. (1997) proposed the PARADISE framework, where a linear function of task completion and various dialogue features such as dialogue duration were used to in-", "startOffset": 0, "endOffset": 21}, {"referenceID": 21, "context": "have been raised regarding the theoretical validity of the model (Larsen, 2003).", "startOffset": 65, "endOffset": 79}, {"referenceID": 44, "context": "Yang et al. (2012) used collaborative filtering to infer user preferences.", "startOffset": 0, "endOffset": 19}, {"referenceID": 11, "context": "The use of reward shaping has also been investigated in (El Asri et al., 2014; Su et al., 2015b) to enrich the reward function in order to speed up dialogue policy learning. Also, Ultes and Minker (2015) demonstrated that there is a strong correlation between expert\u2019s user satisfaction ratings and dialogue success.", "startOffset": 60, "endOffset": 204}, {"referenceID": 8, "context": "One effective way to mitigate the effects of annotator error is to obtain multiple ratings for the same data and several methods have been developed to guide the annotation process with uncertainty models (Dai et al., 2013; Lin et al., 2014).", "startOffset": 205, "endOffset": 241}, {"referenceID": 35, "context": "Active learning is particularly useful for determining when an annotation is needed (Settles, 2010; Zhang and Chaudhuri, 2015).", "startOffset": 84, "endOffset": 126}, {"referenceID": 3, "context": "It is often utilised using Bayesian optimisation approaches (Brochu et al., 2010).", "startOffset": 60, "endOffset": 81}, {"referenceID": 3, "context": "It is often utilised using Bayesian optimisation approaches (Brochu et al., 2010). Based on this, Daniel et al. (2014)", "startOffset": 61, "endOffset": 119}, {"referenceID": 32, "context": "Rather than explicitly defining a reward function, inverse RL (IRL) aims to recover the underlying reward from demonstrations of good behaviour and then learn a policy which maximises the recovered reward (Russell, 1998).", "startOffset": 205, "endOffset": 220}, {"referenceID": 6, "context": "ments than absolute scores, another related line of research has focused on preference-based approaches to RL (Cheng et al., 2011).", "startOffset": 110, "endOffset": 130}, {"referenceID": 38, "context": "In (Sugiyama et al., 2012), users were asked to provide rankings between pairs of dialogues.", "startOffset": 3, "endOffset": 26}, {"referenceID": 25, "context": "tions has recently gained attention especially for word representations, and has boosted performance on several natural language processing tasks (Mikolov et al., 2013; Turian et al., 2010; Levy and Goldberg, 2014).", "startOffset": 146, "endOffset": 214}, {"referenceID": 40, "context": "tions has recently gained attention especially for word representations, and has boosted performance on several natural language processing tasks (Mikolov et al., 2013; Turian et al., 2010; Levy and Goldberg, 2014).", "startOffset": 146, "endOffset": 214}, {"referenceID": 7, "context": "been successfully applied to machine translation (MT) where it enables varying-length phrases to be mapped to fixed-length vectors using an RNN Encoder-Decoder (Cho et al., 2014).", "startOffset": 160, "endOffset": 178}, {"referenceID": 15, "context": "In our proposed model, the encoder is a Bi-directional Long Short-Term Memory network (BLSTM) (Hochreiter and Schmidhuber, 1997; Graves et al., 2013).", "startOffset": 94, "endOffset": 149}, {"referenceID": 4, "context": "In the context of spoken dialogue systems it has been successfully used for RL policy optimisation (Ga\u0161i\u0107 and Young, 2014; Casanueva et al., 2015) and IRL reward function regression (Kim et al.", "startOffset": 99, "endOffset": 146}, {"referenceID": 20, "context": ", 2015) and IRL reward function regression (Kim et al., 2014).", "startOffset": 43, "endOffset": 61}, {"referenceID": 5, "context": "The hyper-parameters p, l, \u03c3n can be adequately optimised by maximising the marginal likelihood using a gradient-based method (Chen et al., 2015).", "startOffset": 126, "endOffset": 145}, {"referenceID": 19, "context": "model (Kapoor et al., 2007).", "startOffset": 6, "endOffset": 27}, {"referenceID": 16, "context": "The shared core components of the SDS common to all experiments comprise a HMM-based recogniser, a confusion network (CNet) semantic input decoder (Henderson et al., 2012), the BUDS belief state tracker (Thomson and Young, 2010) that factorises the dialogue state using a dynamic Bayesian network, and a template based natural language generator to map system semantic actions into natural language responses to the user.", "startOffset": 147, "endOffset": 171}, {"referenceID": 43, "context": "tains a user\u2019s utterance and a system\u2019s response, a feature vector f of size 74 was extracted (Vandyke et al., 2015).", "startOffset": 94, "endOffset": 116}, {"referenceID": 1, "context": "The model was implemented using the Theano library (Bergstra et al., 2010; Bastien et al., 2012).", "startOffset": 51, "endOffset": 96}, {"referenceID": 0, "context": "The model was implemented using the Theano library (Bergstra et al., 2010; Bastien et al., 2012).", "startOffset": 51, "endOffset": 96}, {"referenceID": 17, "context": "The system was implemented using the GPy library (Hensman et al., 2012).", "startOffset": 49, "endOffset": 71}, {"referenceID": 14, "context": "\u2022 the Obj=Subj system which uses prior knowledge of the task to only use training dialogues for which the user\u2019s subjective assessment of success is consistent with the objective assessment of success as in (Ga\u0161i\u0107 et al., 2013).", "startOffset": 207, "endOffset": 227}, {"referenceID": 13, "context": "Similar to the conclusions drawn in (Ga\u0161i\u0107 et al., 2011), the Subj system suffers from unreliable user feedback.", "startOffset": 36, "endOffset": 56}], "year": 2016, "abstractText": "The ability to compute an accurate reward function is essential for optimising a dialogue policy via reinforcement learning. In real-world applications, using explicit user feedback as the reward signal is often unreliable and costly to collect. This problem can be mitigated if the user\u2019s intent is known in advance or data is available to pre-train a task success predictor off-line. In practice neither of these apply for most real world applications. Here we propose an on-line learning framework whereby the dialogue policy is jointly trained alongside the reward model via active learning with a Gaussian process model. This Gaussian process operates on a continuous space dialogue representation generated in an unsupervised fashion using a recurrent neural network encoder-decoder. The experimental results demonstrate that the proposed framework is able to significantly reduce data annotation costs and mitigate noisy user feedback in dialogue policy learning.", "creator": "LaTeX with hyperref package"}}}