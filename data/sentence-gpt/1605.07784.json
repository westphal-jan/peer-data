{"id": "1605.07784", "review": {"conference": "NIPS", "VERSION": "v1", "DATE_OF_SUBMISSION": "25-May-2016", "title": "Fast Algorithms for Robust PCA via Gradient Descent", "abstract": "We consider the problem of Robust PCA in the the fully and partially observed settings. Without corruptions, this is the well-known matrix completion problem. From a statistical standpoint this problem has been recently well-studied, and conditions on when recovery is possible (how many observations do we need, how many corruptions can we tolerate) via polynomial-time algorithms is by now understood. This paper presents and analyzes a non-convex optimization approach that greatly reduces the computational complexity of the above problems, compared to the best available algorithms. In particular, in the fully observed case, with $r$ denoting rank and $d$ dimension, we reduce the complexity from $\\mathcal{O}(r^2d^2\\log(1/\\varepsilon))$ to $\\mathcal{O}(rd^2\\log(1/\\varepsilon))$ -- a big savings when the rank is big. For the partially observed case, we show the complexity of our algorithm is no more than $\\mathcal{O}(r^4d \\log d \\log(1/\\varepsilon))$. Not only is this the best-known run-time for a provable algorithm under partial observation, but in the setting where $r$ is small compared to $d$, it also allows for near-linear-in-$d$ run-time that can be exploited in the fully-observed case as well, by simply running our algorithm on a subset of the observations.\n\n\nOur results are a step further by step analysis and we demonstrate the first time the algorithm with less than $r$ is completely unsupervised (this technique is a first and probably least relevant approach, as well as the second time a set of observations would have to be used to validate the first) as a method of unsupervised, even if it is the most sophisticated one. The best known system for unsupervised computing, for example, is the Go Go program, a program that generates and performs all the tasks that require all data in a Go program. The program consists of a number of functions that are based on a standard Go program, each of which is essentially the same for the most common and most common algorithms. There are a few programs that have the same problem:\n\nI.D.s\nD.S.R.s\nO.D.s\nD.S.R.s\n\nI.D", "histories": [["v1", "Wed, 25 May 2016 09:10:07 GMT  (3615kb,D)", "http://arxiv.org/abs/1605.07784v1", null], ["v2", "Mon, 19 Sep 2016 17:28:25 GMT  (3616kb,D)", "http://arxiv.org/abs/1605.07784v2", null]], "reviews": [], "SUBJECTS": "cs.IT cs.LG math.IT math.ST stat.ML stat.TH", "authors": ["xinyang yi", "dohyung park", "yudong chen", "constantine caramanis"], "accepted": true, "id": "1605.07784"}, "pdf": {"name": "1605.07784.pdf", "metadata": {"source": "CRF", "title": "Fast Algorithms for Robust PCA via Gradient Descent", "authors": ["Xinyang Yi", "Dohyung Park", "Yudong Chen", "Constantine Caramanis"], "emails": ["yixy@utexas.edu", "dhpark@utexas.edu", "constantine@utexas.edu", "yudong.chen@cornell.edu"], "sections": [{"heading": "1 Introduction", "text": "Principal component analysis (PCA) aims to find a low rank subspace that best-approximates a data matrix Y \u2208 Rd1\u00d7d2 . The simple and standard method of PCA by singular value decomposition (SVD) fails in many modern data problems due to missing and corrupted entries, as well as sheer scale of the problem. Indeed, SVD is highly sensitive to outliers by virtue of the squared-error criterion it minimizes. Moreover, its running time scales as O(rd2) to recover a rank r approximation of a d-by-d matrix.\nWhile there have been recent results developing provably robust algorithms for PCA (e.g., [8, 27]), the running times range from O(r2d2) to O(d3)1 and hence are significantly worse than SVD. Meanwhile, the literature developing sub-quadratic algorithms for PCA (e.g., [15, 14, 3]) seems unable to guarantee robustness to outliers or missing data.\nOur contribution lies precisely in this area: provably robust algorithms for PCA with improved run-time. Specifically, we provide an efficient algorithm with running time that matches SVD while nearly matching the best-known robustness guarantees. In the case where rank is small compared to dimension, we develop an algorithm with running time that is nearly linear in the dimension. This last algorithm works by subsampling the data, and therefore we also show that our algorithm solves the Robust PCA problem with partial observations (a generalization of matrix completion and Robust PCA).\n1For precise dependence on error and other factors, please see details below.\nar X\niv :1\n60 5.\n07 78\n4v 1\n[ cs\n.I T\n] 2\n5 M\nay 2\n01 6"}, {"heading": "1.1 The Model and Prior Work", "text": "We consider the following setting for robust PCA. Suppose we are given a matrix Y \u2208 Rd1\u00d7d2 that has decomposition Y = M\u2217+S\u2217, where M\u2217 is a rank r matrix and S\u2217 is a sparse corruption matrix containing entries with arbitrary magnitude. The goal is to recover M\u2217 and S\u2217 from Y . To ease notation, we let d1 = d2 = d in the remainder of this section.\nProvable solutions for this model are first provided in the works of [9] and [8]. They propose to solve this problem by convex relaxation:\nmin M,S |||M |||nuc + \u03bb\u2016S\u20161, s.t. Y = M + S, (1)\nwhere |||M |||nuc denotes the nuclear norm of M . Despite analyzing the same method, the corruption models in [8] and [9] differ. In [8], the authors consider the setting where the entries of M\u2217 are corrupted at random with probability \u03b1. They show their method succeeds in exact recovery with \u03b1 as large as 0.1, which indicates they can tolerate a constant fraction of corruptions. Work in [9] considers a deterministic corruption model, where nonzero entries of S\u2217 can have arbitrary position, but the sparsity of each row and column does not exceed \u03b1d. They prove that for exact recovery, it can allow \u03b1 = O(1/(\u00b5r \u221a d)). This was subsequently further improved to \u03b1 = O(1/(\u00b5r)), which is in fact optimal [12, 18]. Here, \u00b5 represents the incoherence of M\u2217 (see Section 2 for details). In this paper, we follow this latter line and focus on the deterministic corruption model.\nThe state-of-the-art solver [20] for (1) has time complexity O(d3/\u03b5) to achieve error \u03b5, and is thus much slower than SVD, and prohibitive for even modest values of d. Work in [22] considers the deterministic corruption model, and improves this running time without sacrificing the robustness guarantee on \u03b1. They propose an alternating projection (AltProj) method to estimate the low rank and sparse structures iteratively and simultaneously, and show their algorithm has complexity O(r2d2 log(1/\u03b5)), which is faster than the convex approach but still slower than SVD.\nNon-convex approaches have recently seen numerous developments for applications in low-rank estimation, including alternating minimization (see e.g. [19, 17, 16]) and gradient descent (see e.g. [4, 11, 24, 25, 30, 31]). These works have fast running times, yet do not provide robustness guarantees. One exception is [11], where the authors analyze a row-wise `1 projection method for recovering S\u2217. Their analysis hinges on positive semidefinite M\u2217, and the algorithm requires prior knowledge of the `1 norm of every row of S\u2217 and is thus prohibitive in practice. Another exception is work [16], which analyzes alternating minimization plus an overall sparse projection. Their algorithm is shown to tolerate at most a fraction of \u03b1 = O(1/(\u00b52/3r2/3d)) corruptions. As we discuss below, we can allow S\u2217 to have much higher sparsity \u03b1 = O(1/(\u00b5r1.5)), which is close to optimal.\nIt is worth mentioning other works that obtain provable guarantees of non-convex algorithms or problems including phase retrieval [5, 13, 29], EM algorithms [2, 26, 28], tensor decompositions [1] and second order method [23]. It might be interesting to bring robust considerations to these works."}, {"heading": "1.2 Our Contributions", "text": "In this paper, we develop efficient non-convex algorithms for robust PCA. We propose a novel algorithm based on the projected gradient method on the factorized space. We also extend it to solve robust PCA in the setting with partial observations, i.e., in addition to gross corruptions, the data matrix has a large number of missing values. Our main contributions are summarized as\nfollows.2\n1. We propose a novel sparse estimator for the setting of deterministic corruptions. For the lowrank structure to be identifiable, it is natural to assume that deterministic corruptions are \u201cspread out\u201d (no more than some number in each row/column). We leverage this information in a simple but critical algorithmic idea, that is tied to the ultimate complexity advantages our algorithm delivers.\n2. Based on the proposed sparse estimator, we propose a projected gradient method on the matrix factorized space. While non-convex, the algorithm is shown to enjoy linear convergence under proper initialization. Along with a new initialization method, we show that robust PCA can be solved within complexity O(rd2 log(1/\u03b5)) while ensuring robustness \u03b1 = O(1/(\u00b5r1.5)). Our algorithm is thus faster than the best previous known algorithm by a factor of r, and enjoys superior empirical performance as well.\n3. Algorithms for Robust PCA with partial observations still rely on a computationally expensive convex approach, as apparently this problem has evaded treatment by non-convex methods. We consider precisely this problem. In a nutshell, we show that our gradient method succeeds (it is guaranteed to produce the subspace of M\u2217) even when run on no more than O(\u00b52r2d log d) random entries of Y . The computational cost is O(\u00b53r4d log d log(1/\u03b5)). When rank r is small compared to the dimension d, in fact this dramatically improves on our bound above, as our cost becomes nearly linear in d. We show, moreover, that this savings and robustness to erasures comes at no cost in the robustness guarantee for the deterministic (gross) corruptions. While this demonstrates our algorithm is robust to both outliers and erasures, it also provides a way to reduce computational costs even in the fully observed setting, when r is small.\n4. An immediate corollary of the above result provides a guarantee for exact matrix completion, with general rectangular matrices, usingO(\u00b52r2d log d) observed entries andO(\u00b53r4d log d log(1/\u03b5)) time, thereby improving on existing results in [11, 24]."}, {"heading": "1.3 Organization and Notation", "text": "The remainder of this paper is organized as follows. In Section 2, we formally describe our problem and assumptions. In Section 3, we present and describe our algorithms for fully (Algorithm 1) and partially (Algorithm 2) observed settings. In Section 4.1, we establish theoretical guarantees of Algorithm 1. The theory for partially observed setting are presented in Section 4.2. Numerical results are collected in Section 5. Sections 6, 7 and Appendix A contain all the proofs and technical lemmas.\nFor any index set \u2126 \u2286 [d1]\u00d7[d2], we let \u2126(i,\u00b7) := { (i, j) \u2208 \u2126 \u2223\u2223 j \u2208 [d2]}, \u2126(\u00b7,j) := {(i, j) \u2208 \u2126 \u2223\u2223 i \u2208 [d1]}.\nFor any matrix A \u2208 Rd1\u00d7d2 , we denote its projector onto support \u2126 by \u03a0\u2126 (A), i.e., the (i, j)-th entry of \u03a0\u2126 (A) is equal to A if (i, j) \u2208 \u2126 and zero otherwise. The i-th row and j-th column of A are denoted by A(i,\u00b7) and A(\u00b7,j). The (i, j)-th entry is denoted as A(i,j). Operator norm of A is |||A|||op. Frobenius norm of A is |||A|||F. The `a/`b norm of A is denoted by |||A|||b,a, i.e., the `a norm of the vector formed by the `b norm of every row. For instance, |||A|||2,\u221e stands for maxi\u2208[d1] \u2016A(i,\u00b7)\u20162.\n2To ease presentation, the discussion here assumes M\u2217 has constant condition number, whereas our results below show the dependence on condition number explicitly."}, {"heading": "2 Problem Setup", "text": "We consider the problem where we observe a matrix Y \u2208 Rd1\u00d7d2 that satisfies Y = M\u2217+S\u2217, where M\u2217 has rank r, and S\u2217 is corruption matrix with sparse support. Our goal is to recover M\u2217 and S\u2217. In the partially observed setting, in addition to sparse corruptions, we have erasures. We assume that each entry of M\u2217 + S\u2217 is revealed independently with probability p \u2208 (0, 1). In particular, for any (i, j) \u2208 [d1]\u00d7 [d2], we consider the Bernoulli model where\nY(i,j) = { (M\u2217 + S\u2217)(i,j), with probability p; \u2217, otherwise.\n(2)\nWe denote the support of Y by \u03a6 = {(i, j) | Y(i,j) 6= \u2217}. Note that we assume S\u2217 is not adaptive to \u03a6. As is well-understood thanks to work in matrix completion, this task is impossible in general \u2013 we need to guarantee that M\u2217 is not both low-rank and sparse. To avoid such identifiability issues, we make the following standard assumptions on M\u2217 and S\u2217: (i) M\u2217 is not near-sparse or \u201cspiky.\u201d We impose this by requiring M\u2217 to be \u00b5-incoherent \u2013 given a singular value decomposition (SVD)3 M\u2217 = L\u2217\u03a3\u2217R\u2217>, we assume that\n|||L\u2217|||2,\u221e \u2264 \u221a \u00b5r\nd1 , |||R\u2217|||2,\u221e \u2264\n\u221a \u00b5r\nd2 .\n(ii) The entries of S\u2217 are \u201cspread out\u201d \u2013 for \u03b1 \u2208 [0, 1), we assume S\u2217 \u2208 S\u03b1, where\nS\u03b1 := { A \u2208 Rd1\u00d7d2 \u2223\u2223 \u2016A(i,\u00b7)\u20160 \u2264 \u03b1d2 for all i \u2208 [d1] ; \u2016A(\u00b7,j)\u20160 \u2264 \u03b1d1 for all j \u2208 [d2]} . (3) In other words, S\u2217 contains at most \u03b1-fraction nonzero entries per row and column."}, {"heading": "3 Algorithms", "text": "For both the full and partial observation settings, our method proceeds in two phases. In the first phase, we use a new sorting-based sparse estimator to produce a rough estimate Sinit for S\u2217 based on the observed matrix Y , and then find a rank r matrix factorized as U0V >0 that is a rough estimate of M\u2217 by performing SVD on (Y \u2212 Sinit). In the second phase, given (U0, V0), we perform an iterative method to produce series {(Ut, Vt)}\u221et=0. In each step t, we first apply our sparse estimator to produce a sparse matrix St based on (Ut, Vt), and then perform a projected gradient descent step on the low-rank factorized space to produce (Ut+1, Vt+1). This flow is the same for full and partial observations, though a few details differ. Algorithm 1 gives the full observation algorithm, and Algorithm 2 gives the partial observation algorithm. We now describe the key details of each algorithm.\nSparse Estimation. A natural idea is to keep those entries of residual matrix Y \u2212M that have large magnitude. At the same time, we need to make use of the dispersed property of S\u03b1 that every column and row contain at most \u03b1-fraction of nonzero entries. Motivated by these two principles, we\n3Throughout this paper, we refer to SVD of rank r matrix by form L\u03a3R> where \u03a3 \u2208 Rr\u00d7r.\nintroduce the following sparsification operator: For any matrix A \u2208 Rd1\u00d7d2 : for all (i, j) \u2208 [d1]\u00d7[d2], we let\nT\u03b1 [A] :=\n{ A(i,j), if |A(i,j)| \u2265 |A (\u03b1d2) (i,\u00b7) | and |A(i,j)| \u2265 |A (\u03b1d1) (\u00b7,j) |\n0, otherwise , (4)\nwhere A(k)(i,\u00b7) and A (k) (\u00b7,j) denote the elements of A(i,\u00b7) and A(\u00b7,j) that have the k-th largest magnitude respectively. In other words, we choose to keep those elements that are simultaneously among the largest \u03b1-fraction entries in the corresponding row and column. In the case of entries having identical magnitude, we break ties arbitrarily. It is thus guaranteed that T\u03b1 [A] \u2208 S\u03b1.\nAlgorithm 1 Fast RPCA INPUT: Observed matrix Y with rank r and corruption fraction \u03b1; parameters \u03b3, \u03b7; number of\niterations T . // Phase I: Initialization. 1: Sinit \u2190 T\u03b1 [Y ] // see (4) for the definition of T\u03b1 [\u00b7]. 2: [L,\u03a3, R]\u2190 SVDr[Y \u2212 Sinit] 4 3: U0 \u2190 L\u03a31/2, V0 \u2190 R\u03a31/2. Let U ,V be defined according to (7).\n// Phase II: Gradient based iterations. 4: U0 \u2190 \u03a0U (U0), V0 \u2190 \u03a0V (V0) 5: for t = 0, 1, . . . , T \u2212 1 do 6: St \u2190 T\u03b3\u03b1 [ Y \u2212 UtV >t\n] 7: Ut+1 \u2190 \u03a0U ( Ut \u2212 \u03b7\u2207UL(Ut, Vt;St)\u2212 12\u03b7Ut(U > t Ut \u2212 V >t Vt)\n) 8: Vt+1 \u2190 \u03a0V ( Vt \u2212 \u03b7\u2207V L(Ut, Vt;St)\u2212 12\u03b7Vt(V > t Vt \u2212 U>t Ut)\n) 9: end for\nOUTPUT: (UT , VT )\nInitialization. In the fully observed setting, we compute Sinit based on Y as Sinit = T\u03b1 [Y ]. In the partially observed setting with sampling rate p, we let Sinit = T2p\u03b1 [Y ]. In both cases, we then set U0 = L\u03a3 1/2 and V0 = R\u03a31/2, where L\u03a3R> is an SVD of the best rank r approximation of Y \u2212Sinit.\nGradient Method on Factorized Space. After initialization, we proceed by projected gradient descent. To do this, we define loss functions explicitly in the factored space, i.e., in terms of U, V and S:\nL(U, V ;S) := 1 2 |||UV > + S \u2212 Y |||2F, (fully observed) (5) L\u0303(U, V ;S) := 1 2p |||\u03a0\u03a6 ( UV > + S \u2212 Y ) |||2F. (partially observed) (6)\nRecall that our goal is to recover M\u2217 that satisfies the \u00b5-incoherent condition. Given an SVD M\u2217 = L\u2217\u03a3R\u2217>, we expect that the solution (U, V ) is close to (L\u2217\u03a31/2, R\u2217\u03a31/2) up to some rotation. In order to serve such \u00b5-incoherent structure, it is natural to put constraints on the row norms of\n4SVDr[A] stands for computing a SVD of the matrix that is the best rank r approximation of A.\nU, V based on |||M\u2217|||op. As |||M\u2217|||op is unavailable, given U0, V0 computed in the first phase, we rely on the sets U , V defined as\nU := { A \u2208 Rd1\u00d7r \u2223\u2223 |||A|||2,\u221e \u2264\u221a2\u00b5r d1 |||U0|||op } , V := { A \u2208 Rd2\u00d7r \u2223\u2223 |||A|||2,\u221e \u2264\u221a2\u00b5r d2 |||V0|||op } . (7)\nNow we consider the following optimization problems with constraints:\nmin U\u2208U ,V \u2208V,S\u2208S\u03b1\nL(U, V ;S) + 1 8 |||U>U \u2212 V >V |||2F, (fully observed) (8)\nmin U\u2208U ,V \u2208V,S\u2208Sp\u03b1\nL\u0303(U, V ;S) + 1 64 |||U>U \u2212 V >V |||2F. (partially observed) (9)\nThe regularization term in the objectives above is used to encourage that U and V have the same scale. Given (U0, V0), we propose the following iterative method to produce series {(Ut, Vt)}\u221et=0 and {St}\u221et=0. We give the details for the fully observed case \u2013 the partially observed case is similar. For t = 0, 1, . . ., we update St using the sparse estimator St = T\u03b3\u03b1 [ Y \u2212 UtV >t ] , followed by a projected gradient update on Ut and Vt\nUt+1 = \u03a0U ( Ut \u2212 \u03b7\u2207UL(Ut, Vt;St)\u2212 1\n2 \u03b7Ut(U\n> t Ut \u2212 V >t Vt)\n) ,\nVt+1 = \u03a0V ( Vt \u2212 \u03b7\u2207V L(Ut, Vt;St)\u2212 1\n2 \u03b7Vt(V\n> t Vt \u2212 U>t Ut)\n) .\nHere \u03b1 is the model parameter that characterizes the corruption fraction, \u03b3 and \u03b7 are algorithmic tunning parameters, which we specify in our analysis. Essentially, the above algorithm corresponds to applying projected gradient method to optimize (8), where S is replaced by the aforementioned sparse estimator in each step.\nAlgorithm 2 Fast RPCA with partial observations INPUT: Observed matrix Y with support \u03a6; parameters \u03c4, \u03b3, \u03b7; number of iterations T . // Phase I: Initialization. 1: Sinit \u2190 T2p\u03b1 [\u03a0\u03a6(Y )] 2: [L,\u03a3, R]\u2190 SVDr[1p(Y \u2212 Sinit)] 3: U0 \u2190 L\u03a31/2, V0 \u2190 R\u03a31/2. Let U ,V be defined according to (7).\n// Phase II: Gradient based iterations. 4: U0 \u2190 \u03a0U (U0), V0 \u2190 \u03a0V (V0) 5: for t = 0, 1, . . . , T \u2212 1 do 6: St \u2190 T\u03b3p\u03b1 [ \u03a0\u03a6 ( Y \u2212 UtV >t\n)] 7: Ut+1 \u2190 \u03a0U ( Ut \u2212 \u03b7\u2207U L\u0303(Ut, Vt;St)\u2212 116\u03b7Ut(U > t Ut \u2212 V >t Vt)\n) 8: Vt+1 \u2190 \u03a0V ( Vt \u2212 \u03b7\u2207V L\u0303(Ut, Vt;St)\u2212 116\u03b7Vt(V > t Vt \u2212 U>t Ut)\n) 9: end for\nOUTPUT: (UT , VT )"}, {"heading": "4 Main Results", "text": "In this section, we establish theoretical guarantees for Algorithm 1 in the fully observed setting and for Algorithm 2 in the partially observed setting."}, {"heading": "4.1 Analysis of Algorithm 1", "text": "We begin with some definitions and notation. It is important to define a proper error metric because the optimal solution corresponds to a manifold and there are many distinguished pairs (U, V ) that minimize (8). Given the SVD of the true low-rank matrixM\u2217 = L\u2217\u03a3\u2217R\u2217>, we let U\u2217 := L\u2217\u03a3\u22171/2 and V \u2217 := R\u2217\u03a3\u22171/2. We also let \u03c3\u22171 \u2265 \u03c3\u22172 \u2265 . . . \u2265 \u03c3\u2217r be sorted nonzero singular values ofM\u2217, and denote the condition number of M\u2217 by \u03ba, i.e., \u03ba := \u03c3\u22171/\u03c3\u2217r . We define estimation error d(U, V ;U\u2217, V \u2217) as the minimal Frobenius norm between (U, V ) and (U\u2217, V \u2217) with respect to the optimal rotation, namely\nd(U, V ;U\u2217, V \u2217) := min Q\u2208Qr\n\u221a |||U \u2212 U\u2217Q|||2F + |||V \u2212 V \u2217Q|||2F, (10)\nfor Qr the set of r-by-r orthonormal matrices. This metric controls reconstruction error, as\n|||UV > \u2212M\u2217|||F . \u221a \u03c3\u22171d(U, V ;U \u2217, V \u2217), (11)\nwhen d(U, V ;U\u2217, V \u2217) \u2264 \u221a \u03c3\u22171. We denote the local region around the optimum (U\n\u2217, V \u2217) with radius \u03c9 as B2 (\u03c9) := { (U, V ) \u2208 Rd1\u00d7r \u00d7 Rd2\u00d7r \u2223\u2223 d(U, V ;U\u2217, V \u2217) \u2264 \u03c9} .\nThe next two theorems provide guarantees for the initialization phase and gradient iterations, respectively, of Algorithm 1. The proofs are given in Sections 6.1 and 6.2.\nTheorem 1 (Initialization). Consider the paired (U0, V0) produced in the first phase of Algorithm 1. If \u03b1 \u2264 1/(16\u03ba\u00b5r), we have\nd(U0, V0;U \u2217, V \u2217) \u2264 28 \u221a \u03ba\u03b1\u00b5r \u221a r \u221a \u03c3\u22171.\nTheorem 2 (Convergence). Consider the second phase of Algorithm 1. Suppose we choose \u03b3 = 2 and \u03b7 = c/\u03c3\u22171 for any c \u2264 1/36. There exist constants c1, c2 such that when \u03b1 \u2264 c1/(\u03ba2\u00b5r), given any (U0, V0) \u2208 B2 ( c2 \u221a \u03c3\u2217r/\u03ba ) , the iterates {(Ut, Vt)}\u221et=0 satisfy\nd2(Ut, Vt;U \u2217, V \u2217) \u2264 ( 1\u2212 c\n8\u03ba\n)t d2(U0, V0;U \u2217, V \u2217).\nTherefore, using proper initialization and step size, the gradient iteration converges at a linear rate with a constant contraction factor 1 \u2212 O(1/\u03ba). To obtain relative precision \u03b5 compared to the initial error, it suffices to perform O(\u03ba log(1/\u03b5)) iterations. Note that the step size is chosen according to 1/\u03c3\u22171. When \u03b1 . 1/(\u00b5 \u221a \u03bar3), Theorem 1 and the inequality (11) together imply that |||U0V >0 \u2212M\u2217|||op \u2264 12\u03c3 \u2217 1. Hence we can set the step size as \u03b7 = O(1/\u03c31(U0V >0 )) using being the top singular value \u03c31(U0V >0 ) of the matrix U0V >0 Combining Theorems 1 and 2 implies the following result, proved in Section 6.3, that provides an overall guarantee for Algorithm 1.\nCorollary 1. Suppose that\n\u03b1 \u2264 cmin\n{ 1\n\u00b5 \u221a \u03bar\n3 , 1\n\u00b5\u03ba2r } for some constant c. Then for any \u03b5 \u2208 (0, 1), Algorithm 1 T = O(\u03ba log(1/\u03b5)) outputs a pair (UT , VT ) that satisfies\n|||UTV >T \u2212M\u2217|||F \u2264 \u03b5 \u00b7 \u03c3\u2217r . (12)\nRemark 1 (Time Complexity). For simplicity we assume d1 = d2 = d. Our sparse estimator (4) can be implemented by finding the top \u03b1d elements of each row and column via partial quick sort, which has running time O(d2 log(\u03b1d)). Performing rank-r SVD in the first phase and computing the gradient in each iteration both have complexity O(rd2).5 Algorithm 1 thus has total running time O(\u03bard2 log(1/\u03b5)) for achieving an accuracy as in (12). We note that when \u03ba = O(1), our algorithm is orderwise faster than the AltProj algorithm in [22], which has running time O(r2d2 log(1/\u03b5)). Moreover, our algorithm only requires computing one singular value decomposition.\nRemark 2 (Robustness). Assuming \u03ba = O(1), our algorithm can tolerate corruption at a sparsity level up to \u03b1 = O(1/(\u00b5r \u221a r)). This is worse by a factor \u221a r compared to the optimal statistical guarantee 1/(\u00b5r) obtained in [12, 18, 22]. This looseness is a consequence of the condition for (U0, V0) in Theorem 2. Nevertheless, when \u00b5r = O(1), our algorithm can tolerate a constant \u03b1 fraction of corruptions."}, {"heading": "4.2 Analysis of Algorithm 2", "text": "We now move to the guarantees of Algorithm 2. We show here that not only can we handle partial observations, but in fact subsampling the data in the fully observed case can significantly reduce the time complexity from the guarantees given in the previous section without sacrificing robustness. In particular, for smaller values of r, the complexity of Algorithm 2 has near linear dependence on the dimension d, instead of quadratic.\nIn the following discussion, we let d := max{d1, d2}. The next two results, proved in Sections 6.4 and 6.5, control the quality of the initialization step, and then the gradient iterations.\nTheorem 3 (Initialization, partial observations). Suppose the observed indices \u03a6 follow the Bernoulli model given in (2). Consider the pair (U0, V0) produced in the first phase of Algorithm 2. There exist constants {ci}3i=1 such that for any \u2208 (0, \u221a r/(8c1\u03ba)), if\n\u03b1 \u2264 1 64\u03ba\u00b5r\n, p \u2265 c2 ( \u00b5r2\n2 +\n1\n\u03b1\n) log d\nd1 \u2227 d2 , (13)\nthen we have d(U0, V0;U \u2217, V \u2217) \u2264 51 \u221a \u03ba\u03b1\u00b5r \u221a r \u221a \u03c3\u22171 + 7c1 \u221a \u03ba\u03c3\u22171,\nwith probability at least 1\u2212 c3d\u22121. 5In fact, it suffices to compute the best rank-r approximation with running time independent of the eigen gap.\nTheorem 4 (Convergence, partial observations). Suppose the observed indices \u03a6 follow the Bernoulli model given in (2). Consider the second phase of Algorithm 2. Suppose we choose \u03b3 = 3, and \u03b7 = c/(\u00b5r\u03c3\u22171) for a sufficiently small constant c. There exist constants {ci}4i=1 such that if\n\u03b1 \u2264 c1 \u03ba2\u00b5r and p \u2265 c2 \u03ba4\u00b52r2 log d d1 \u2227 d2 , (14)\nthen with probability at least 1\u2212 c3d\u22121, the iterates {(Ut, Vt)}\u221et=0 satisfy\nd2(Ut, Vt;U \u2217, V \u2217) \u2264 ( 1\u2212 c\n64\u00b5r\u03ba\n)t d2(U0, V0;U \u2217, V \u2217)\nfor all (U0, V0) \u2208 B2 ( c4 \u221a \u03c3\u2217r/\u03ba ) .\nThe above result ensures linear convergence to (U\u2217, V \u2217) (up to rotation) even when the gradient iterations are computed using partial observations. Note that setting p = 1 recovers Theorem 2 up to an additional factor \u00b5r in the contraction factor. For achieving \u03b5 relative accuracy, now we need O(\u00b5r\u03ba log(1/\u03b5)) iterations.\nPutting Theorems 3 and 4 together, we have the following overall guarantee, proved in Section 6.6, for Algorithm 2.\nCorollary 2. Suppose that\n\u03b1 \u2264 cmin\n{ 1\n\u00b5 \u221a \u03bar\n3 , 1\n\u00b5\u03ba2r\n} , p \u2265 c\u2032\u03ba 4\u00b52r2 log d\nd1 \u2227 d2 ,\nfor some constants c, c\u2032. With probability at least 1 \u2212 O(d\u22121), for any \u03b5 \u2208 (0, 1), Algorithm 2 with T = O(\u00b5r\u03ba log(1/\u03b5)) outputs a pair (UT , VT ) that satisfies\n|||UTV >T \u2212M\u2217|||F \u2264 \u03b5 \u00b7 \u03c3\u2217r . (15)\nThis result shows that partial observations do not compromise robustness to sparse corruptions: as long as the observation probability p satisfies the condition in Corollary 2, Algorithm 2 enjoys the same robustness guarantees as the method using all entries. Below we provide two remarks on the sample and time complexity. For simplicity, we assume d1 = d2 = d, \u03ba = O(1). Remark 3 (Sample complexity and matrix completion). Using the lower bound on p, it is sufficient to have O(\u00b52r2d log d) observed entries. In the special case S\u2217 = 0, our partial observation model is equivalent to the model of exact matrix completion (see, e.g., [7]). We note that our sample complexity (i.e., observations needed) matches that of completing a positive semidefinite (PSD) matrix by gradient descent as shown in [11], and is better than the non-convex matrix completion algorithms in [19] and [24]. Accordingly, our result reveals the important fact that we can obtain robustness in matrix completion without deterioration of our statistical guarantees. It is known that that any algorithm for solving exact matrix completion must have sample size \u2126(\u00b5rd log d) [7], and a nearly tight upper bound O(\u00b5rd log2 d) is obtained in [10] by convex relaxation. While sub-optimal by a factor \u00b5r, our algorithm is much faster than convex relaxation as shown below.\nRemark 4 (Time complexity). Our sparse estimator on the sparse matrix with support \u03a6 can be implemented via partial quick sort with running time O(pd2 log(\u03b1pd)). Computing the gradient in each step involves the two terms in the objective function (9). Computing the gradient of the first term L\u0303 takes time O(r|\u03a6|), whereas the second term takes time O(r2d). In the initialization phase, performing rank-r SVD on a sparse matrix with support \u03a6 can be done in time O(r|\u03a6|). We conclude that when |\u03a6| = O(\u00b52r2d log d), Algorithm 2 achieves the error bound (15) with running time O(\u00b53r4d log d log(1/\u03b5)). Therefore, in the small rank setting with r d1/3, even when full observations are given, it is better to use Algorithm 2 by subsampling the entries of Y ."}, {"heading": "5 Numerical Results", "text": "In this section, we provide numerical results and compare the proposed algorithms with existing methods, including the inexact augmented lagrange multiplier (IALM) approach [20] for solving the convex relaxation (1) and the alternating projection (AltProj) algorithm proposed in [21]. All algorithms are implemented in MATLAB 6, and the codes for existing algorithms are obtained from their authors. SVD computation in all algorithms uses the PROPACK library.7 We ran all simulations on a machine with Intel 32-core Xeon (E5-2699) 2.3GHz with 240GB RAM."}, {"heading": "5.1 Synthetic Datasets.", "text": "We generate a squared data matrix Y = M\u2217 + S\u2217 \u2208 Rd\u00d7d as follows. The low-rank part M\u2217 is given by M\u2217 = AB>, where A,B \u2208 Rd\u00d7r have entries drawn independently from a zero mean Gaussian distribution with variance 1/d. For a given sparsity parameter \u03b1, each entry of S\u2217 is set to be nonzero with probability \u03b1, and the values of the nonzero entries are sampled uniformly from [\u22125r/d, 5r/d].\nThe results are summarized in Figure 1. Figure 1a shows the convergence of our algorithms for different random instances with different sub-sampling rate p (note that p = 1 corresponds to\n6Our code is available at https://people.orie.cornell.edu/yudong.chen/rpca_gd/RPCA_GD.zip. 7http://sun.stanford.edu/~rmunk/PROPACK/\nthe fully observed setting). As predicted by Theorems 2 and 4, our gradient method converges geometrically with a contraction factor nearly independent of p, Figure 1b shows the running time of our algorithm with partially observed data. We see that the running time scales linearly with d, again consistent with the theory. We note that our algorithm is memory-efficient: in the large scale setting with d = 2\u00d7105, using approximately 0.1% entries is sufficient for the successful recovery In contrast, AltProj and IALM are designed to manipulate the entire matrix with d2 = 4\u00d71010 entries, which is prohibitive on a single machine. Figure 1c compares our algorithms with AltProj and IALM by showing reconstruction error versus real running time. Our algorithm requires significantly less computation to achieve the same accuracy level, and using only a subset of the entries provides additional speed-up."}, {"heading": "5.2 Foreground-background Separation", "text": "We apply our method to the task of foreground-background (FB) separation in a video. We use two public benchmarks, the Restaurant and ShoppingMall datasets.8, Each dataset contains a video with static background. By vectorizing and stacking the frames as columns of a matrix Y , the FB separation problem can be cast as RPCA, where the static background corresponds to a low rank matrix M\u2217 with identical columns, and the moving objects in the video can be modeled as sparse corruptions S\u2217. Figure 2 shows the output of different algorithms on two frames from the dataset. Our algorithms require significantly less running time than both AltProj and IALM. Moreover, even with 20% sub-sampling, our methods still appear to achieve better separation quality (note that in each of the frames our algorithms remove a person that is not identified by the other algorithms).\nFigure 3 shows recovery results for several more frames. Again, our algorithms enjoy better running time and outperform AltProj and IALM in separating persons from the background images. In Appendix B, we describe the detailed parameter settings for our algorithm.\n8http://perception.i2r.a-star.edu.sg/bk_model/bk_index.html"}, {"heading": "6 Proofs", "text": "In this section we provide the proofs for our main theoretical results in Theorems 1\u20134 and Corollaries 1\u20132."}, {"heading": "6.1 Proof of Theorem 1", "text": "Let Y := Y \u2212 Sinit. As Y = M\u2217 + S\u2217, we have Y \u2212M\u2217 = S\u2217 \u2212 Sinit. We obtain Y \u2212M\u2217 \u2208 S2\u03b1 because S\u2217, Sinit \u2208 S\u03b1.\nWe claim that \u2016Y \u2212M\u2217\u2016\u221e \u2264 2\u2016M\u2217\u2016\u221e. Denote the support of S\u2217, Sinit by \u2126\u2217 and \u2126 respectively. Since Y \u2212M\u2217 is supported on \u2126 \u222a \u2126\u2217, to prove the claim it suffices to consider the following three cases.\n\u2022 For (i, j) \u2208 \u2126\u2217 \u2229 \u2126, due to rule of sparse estimation, we have (S\u2217 \u2212 Sinit)(i,j) = 0.\n\u2022 For (i, j) \u2208 \u2126\u2217 \\ \u2126, we must have |S\u2217(i,j)| \u2264 2\u2016M \u2217\u2016\u221e. Otherwise, we have |Y(i,j)| = |(S\u2217 +\nM\u2217)(i,j)| > \u2016M\u2217\u2016\u221e. So |Y(i,j)| is larger than any uncorrupted entries in its row and column. Since there are at most \u03b1 fraction corruptions per row and column, we have Y(i,j) \u2208 \u2126, which violates the prior condition (i, j) \u2208 \u2126\u2217 \\ \u2126.\n\u2022 For the last case (i, j) \u2208 \u2126 \\ \u2126\u2217, since (Sinit)(i,j) = M\u2217(i,j), trivially we have |(Sinit)(i,j)| \u2264 \u2016M\u2217\u2016\u221e.\nThe following result, proved in Section 7.1, relates the operator norm of Y \u2212M\u2217 to its infinite norm.\nLemma 1. For any matrix A \u2208 Rd1\u00d7d2 that belongs to S\u03b1 given in (3), we have |||A|||op \u2264 \u03b1 \u221a d1d2\u2016A\u2016\u221e.\nWe thus obtain |||Y \u2212M\u2217|||op \u2264 2\u03b1 \u221a d1d2\u2016Y \u2212M\u2217\u2016\u221e \u2264 4\u03b1 \u221a d1d2\u2016M\u2217\u2016\u221e = 4\u03b1\u00b5r\u03c3\u22171. (16)\nIn the last step, we use the fact that M\u2217 satisfies the \u00b5-incoherent condition, which leads to\n\u2016M\u2217\u2016\u221e \u2264 |||M\u2217|||op|||L\u2217|||2,\u221e|||R\u2217|||2,\u221e \u2264 \u00b5r\u221a d1d2 |||M\u2217|||op. (17)\nWe denote the i-th largest singular value of Y by \u03c3i. By Weyl\u2019s theorem, we have |\u03c3\u2217i \u2212 \u03c3i| \u2264 |||Y \u2212M\u2217|||op for all i \u2208 [d1 \u2227 d2]. Since \u03c3\u2217r+1 = 0, we have \u03c3r+1 \u2264 |||Y \u2212M\u2217|||op. Recall that U0V >0 is the best rank r approximation of Y . Accordingly, we have\n|||U0V >0 \u2212M\u2217|||op \u2264 |||U0V >0 \u2212 Y |||op + |||Y \u2212M\u2217|||op = \u03c3r+1 + |||Y \u2212M\u2217|||op \u2264 2|||Y \u2212M\u2217|||op \u2264 8\u03b1\u00b5r\u03c3\u22171.\nUnder condition \u03b1\u00b5r \u2264 116\u03ba , we obtain |||U0V > 0 \u2212M\u2217|||op \u2264 12\u03c3 \u2217 r . Applying Lemma 5.14 in [25]\n(we provide it as Lemma 15 for the sake of completeness), we obtain\nd2(U0, V0;U \u2217, V \u2217) \u2264 2\u221a 2\u2212 1 |||U0V >0 \u2212M\u2217|||2F \u03c3\u2217r \u2264 10r|||U0V >0 \u2212M\u2217|||2op \u03c3\u2217r .\nPlugging the upper bound of |||U0V >0 \u2212M\u2217|||op into the above inequality completes the proof."}, {"heading": "6.2 Proof of Theorem 2", "text": "We essentially follow the general framework developed in [11] for analyzing the behaviors of gradient descent in factorized low-rank optimization. But it is worth to note that [11] only studies the symmetric and positive semidefinite setting, while we avoid such constraint on M\u2217. The techniques for analyzing general asymmetric matrix in factorized space is inspired by the recent work [25] on solving low-rank matrix equations. In our setting, the technical challenge is to verify the local descent condition of the loss function (8), which not only has a bilinear dependence on U and V , but also involves our sparse estimator (4).\nWe begin with some notations. Define the equivalent set of optimal solution as E(M\u2217) := { (A,B) \u2208 Rd1\u00d7r \u00d7 Rd2\u00d7r \u2223\u2223 A = L\u2217\u03a3\u22171/2Q,B = R\u2217\u03a3\u22171/2Q, where Q \u2208 Qr} . (18) Given (U0, V0) \u2208 B2 ( c2 \u221a \u03c3\u2217r/\u03ba ) , by (11), we have |||U0V >0 \u2212M\u2217|||op \u2264 12\u03c3 \u2217 r when c2 is sufficiently\nsmall. By Weyl\u2019s theorem We thus have\u221a \u03c3\u22171/2 \u2264 |||U0|||op \u2264 \u221a 3\u03c3\u22171/2, and \u221a \u03c3\u22171/2 \u2264 |||V0|||op \u2264 \u221a 3\u03c3\u22171/2.\nAs a result, for U ,V constructed according to (7), we have\nE(M\u2217) \u2286 U \u00d7 V, and U \u2286 U ,V \u2286 V, (19)\nwhere\nU := { A \u2208 Rd1\u00d7r \u2223\u2223 |||A|||2,\u221e \u2264\u221a3\u00b5r\u03c3\u22171 d1 } , V := { A \u2208 Rd2\u00d7r \u2223\u2223 |||A|||2,\u221e \u2264\u221a3\u00b5r\u03c3\u22171 d2 } .\nWe let G(U, V ) := 1\n8 |||U>U \u2212 V >V |||2F. (20)\nFor L(U, V ;S), we denote the gradient with respect to M by \u2207ML(U, V ;S), i.e. \u2207ML(U, V ;S) = UV > + S \u2212 Y .\nThe local descent property is implied by combining the following two results, which are proved in Section 6.7 and 6.8 respectively. Lemma 2 (Local descent property of L). Suppose U ,V satisfy (19). For any (U, V ) \u2208 (U \u00d7 V) \u2229 B2( \u221a \u03c3\u22171), we let S = T\u03b3\u03b1 [ Y \u2212 UV > ] , where we choose \u03b3 = 2. Then we have that for (U\u03c0\u2217 , V\u03c0\u2217) \u2208 argmin(A,B)\u2208E(M\u2217) |||U \u2212A|||2F + |||V \u2212B|||2F and \u03b2 > 0,\n\u3008\u3008\u2207ML(U, V ;S), UV > \u2212 U\u03c0\u2217V >\u03c0\u2217 + \u2206U\u2206>V \u3009\u3009 \u2265 |||UV > \u2212 U\u03c0\u2217V >\u03c0\u2217 |||2F \u2212 \u03bd\u03c3\u22171\u03b4 \u2212 3 \u221a \u03c3\u22171\u03b4 3. (21)\nHere \u2206U := U \u2212 U\u03c0\u2217, \u2206V := V \u2212 V\u03c0\u2217, \u03b4 := |||\u2206U |||2F + |||\u2206V |||2F, and \u03bd := 9(\u03b2 + 6)\u03b1\u00b5r + 5\u03b2\u22121.\nLemma 3 (Local descent property of G). For any (U, V ) \u2208 B2( \u221a \u03c3\u2217r ) and\n(U\u03c0\u2217 , V\u03c0\u2217) \u2208 arg min (A,B)\u2208E(M\u2217) |||U \u2212A|||2F + |||V \u2212B|||2F,\nwe have\n\u3008\u3008\u2207UG(U, V ), U \u2212 U\u03c0\u2217\u3009\u3009+ \u3008\u3008\u2207V G(U, V ), V \u2212 V\u03c0\u2217\u3009\u3009\n\u2265 1 8 |||U>U \u2212 V >V |||2F + 1 8 \u03c3\u2217r\u03b4 \u2212\n\u221a \u03c3\u22171\u03b4 3\n2 \u2212 1 2 |||UV > \u2212 U\u03c0\u2217V >\u03c0\u2217 |||2F,\nwhere \u03b4 is defined according to Lemma 2.\nAs another key ingredient, we establish the following smoothness condition, proved in Section 6.9, which indicates that the Frobenius norm of gradient decreases as (U, V ) approaches the optimal manifold.\nLemma 4 (Smoothness). For any (U, V ) \u2208 B2( \u221a \u03c3\u22171), we let S = T\u03b3\u03b1 [ Y \u2212 UV > ] , where we choose \u03b3 = 2. We have that |||\u2207ML(U, V ;S)|||2F \u2264 6|||UV > \u2212M\u2217|||2F, (22)\nand |||\u2207UG(U, V )|||2F + |||\u2207V G(U, V )|||2F \u2264 2\u03c3\u22171|||U>U \u2212 V >V |||2F. (23)\nWith the above results in hand, we are ready to prove Theorem 2.\nProof of Theorem 2. We use shorthands\n\u03b4t := d 2(Ut, Vt;U \u2217, V \u2217), Lt := L(Ut, Vt;St), and Gt := G(Ut, Vt).\nFor (Ut, Vt), let (U t\u03c0\u2217 , V t\u03c0\u2217) := argmin(A,B)\u2208E(M\u2217) |||Ut \u2212 A|||2F + |||Vt \u2212 B|||2F. Define \u2206tU := Ut \u2212 U t\u03c0\u2217 , \u2206tV := Vt \u2212 V t\u03c0\u2217 .\nWe prove Theorem 2 by induction. It\u2019s sufficient to consider one step of iteration. For any t \u2265 0, under the induction hypothesis (Ut, Vt) \u2208 B2 ( c2 \u221a \u03c3\u2217r/\u03ba ) . We find that\n\u03b4t+1 \u2264 |||Ut+1 \u2212 U t\u03c0\u2217 |||2F + |||Vt+1 \u2212 V t\u03c0\u2217 |||2F \u2264 |||Ut \u2212 \u03b7\u2207ULt \u2212 \u03b7\u2207UGt \u2212 U t\u03c0\u2217 |||2F + |||Vt \u2212 \u03b7\u2207V Lt \u2212 \u03b7\u2207V Gt \u2212 V t\u03c0\u2217 |||2F \u2264 \u03b4t \u2212 2\u03b7 \u3008\u3008\u2207ULt +\u2207UGt, Ut \u2212 U t\u03c0\u2217\u3009\u3009\ufe38 \ufe37\ufe37 \ufe38\nW1\n\u22122\u03b7 \u3008\u3008\u2207V Lt +\u2207V Gt, Vt \u2212 V t\u03c0\u2217\u3009\u3009\ufe38 \ufe37\ufe37 \ufe38 W2\n+ \u03b72 |||\u2207ULt +\u2207UGt|||2F\ufe38 \ufe37\ufe37 \ufe38 W3 +\u03b72 |||\u2207V Lt +\u2207V Gt|||2F\ufe38 \ufe37\ufe37 \ufe38 W4 , (24)\nwhere the second step follows from the non-expansion property of projection onto U ,V, which is implied by E(M\u2217) \u2286 U \u00d7 V shown in (19). Since \u2207ULt = [\u2207MLt]V and \u2207V Lt = [\u2207MLt]> U , we have\n\u3008\u3008\u2207ULt, Ut \u2212 U t\u03c0\u2217\u3009\u3009+ \u3008\u3008\u2207V Lt, Vt \u2212 V t\u03c0\u2217\u3009\u3009 = \u3008\u3008\u2207MLt, UtV >t \u2212 U t\u03c0\u2217V t>\u03c0\u2217 + \u2206tU\u2206t>V \u3009\u3009.\nCombining Lemma 2 and 3, under condition \u03b4t < \u03c3\u2217r , we have that\nW1 +W2 \u2265 1 2 |||UtV >t \u2212M\u2217|||2F + 1 8 |||U>t Ut \u2212 V >t Vt|||2F + 1 8 \u03c3\u2217r\u03b4t \u2212 \u03bd\u03c3\u22171\u03b4t \u2212 4\n\u221a \u03c3\u22171\u03b4 3 t .\nOn the other hand, we have\nW3 +W4 \u2264 2|||\u2207ULt|||2F + 2|||\u2207UGt|||2F + 2|||\u2207V Lt|||2F + 2|||\u2207V Gt|||2F \u2264 2(|||Ut|||2op + |||Vt|||2op)|||\u2207MLt|||2F + 2|||\u2207UGt|||2F + 2|||\u2207V Gt|||2F \u2264 36\u03c3\u22171|||UtV >t \u2212M\u2217|||2F + 4\u03c3\u22171|||U>t Ut \u2212 V >t Vt|||2F,\nwhere the last step is implied by Lemma 4 and the assumption (Ut, Vt) \u2208 B2 ( c2 \u221a \u03c3\u2217r/\u03ba ) that leads\nto |||Ut|||op \u2264 \u221a 3\u03c3\u22171/2, |||Vt|||op \u2264 \u221a\n3\u03c3\u22171/2. By the assumption \u03b7 = c/\u03c3\u22171 for any constant c \u2264 1/36, we thus have\n\u22122\u03b7(W1 +W2) + \u03b72(W3 +W4) \u2264 \u2212 1 4 \u03b7\u03c3\u2217r\u03b4t + 2\u03b7\u03bd\u03c3 \u2217 1\u03b4t + 8\u03b7\n\u221a \u03c3\u22171\u03b4 3 t .\nIn Lemma 2, choosing \u03b2 = 320\u03ba and assuming \u03b1 . 1/(\u03ba2\u00b5r), we can have \u03bd \u2264 1/(32\u03ba). By assuming \u03b4t . \u03c3\u2217r/\u03ba leads to 14 \u221a \u03c3\u22171\u03b4 3 t \u2264 116\u03c3 \u2217 r\u03b4t. We thus obtain\n\u03b4t+1 \u2264 ( 1\u2212 \u03b7\u03c3 \u2217 r\n8\n) \u03b4t. (25)\nUnder initial condition \u03b40 . \u03c3\u2217r/\u03ba, since estimation error decays geometrically after each iteration, then such condition holds for all t. Then applying (25) for all iterations, we conclude that for all t = 0, 1, . . .,\n\u03b4t \u2264 ( 1\u2212 \u03b7\u03c3 \u2217 r\n8\n)t \u03b40."}, {"heading": "6.3 Proof of Corollary 1", "text": "We need \u03b1 . 1 \u03ba2\u00b5r due to the condition of Theorem 2. In order to ensure the linear convergence happens, it suffices to let the initial error shown in Theorem 1 be less than the corresponding condition in Theorem 2. Accordingly, we need\n28 \u221a \u03ba\u03b1\u00b5r \u221a r \u221a \u03c3\u22171 . \u221a \u03c3\u2217r/\u03ba,\nwhich leads to \u03b1 . 1 \u00b5 \u221a r\u03ba 3 . Using the conclusion that gradient descent has linear convergence, choosing T = O(\u03ba log(1/\u03b5)), we have d2(UT , VT ;U\n\u2217, V \u2217) \u2264 \u03b52d2(U0, V0;U\u2217, V \u2217) . \u03b52 \u03c3\u2217r \u03ba .\nFinally, applying the relationship between d(UT , VT ;U\u2217, V \u2217) and |||UTV >T \u2212M\u2217|||F shown in (11), we complete the proof."}, {"heading": "6.4 Proof of Theorem 3", "text": "Let Y := 1p(Y \u2212 Sinit). Similar to the proof of Theorem 1, we first establish an upper bound on |||Y \u2212M\u2217|||op. We have that\n|||Y \u2212M\u2217|||op \u2264 |||Y \u2212 1\np \u03a0\u03a6M\n\u2217|||op + ||| 1\np \u03a0\u03a6(M\n\u2217)\u2212M\u2217|||op. (26)\nFor the first term, we have Y \u2212 1p\u03a0\u03a6M \u2217 = 1p(\u03a0\u03a6(S \u2217)\u2212Sinit) because Y = \u03a0\u03a6(M\u2217+S\u2217). Lemma 10 shows that under condition p & log d\u03b1(d1\u2227d2) , there are at most 3 2p\u03b1-fraction nonzero entries in each row and column of \u03a0\u03a6(S\u2217) with high probability. Since Sinit \u2208 S2p\u03b1, we have\n\u03a0\u03a6(S \u2217)\u2212 Sinit \u2208 S4p\u03b1. (27)\nIn addition, we prove below that\n\u2016\u03a0\u03a6(S\u2217)\u2212 Sinit\u2016\u221e \u2264 2\u2016M\u2217\u2016\u221e. (28)\nDenote the support of \u03a0\u03a6(S\u2217) and Sinit by \u2126\u2217o and \u2126. For (i, j) \u2208 \u2126\u2217o \u2229 \u2126 and (i, j) \u2208 \u2126 \\ \u2126\u2217o, we have (\u03a0\u03a6(S\u2217)\u2212 Sinit)(i,j) = 0 and (\u03a0\u03a6(S\u2217)\u2212 Sinit)(i,j) = \u2212M\u2217(i,j), respectively. To prove the claim, it remains to show that for (i, j) \u2208 \u2126\u2217o \\\u2126, |S\u2217(i,j)| < 2\u2016M\n\u2217\u2016\u221e. If this is not true, then we must have |Y(i,j)| > \u2016M\u2217\u2016\u221e. Accordingly, |Y(i,j)| is larger than the magnitude of any uncorrupted entries in its row and column. Note that on the support \u03a6, there are at most 32p\u03b1 corruptions per row and column, we have (i, j) \u2208 \u2126, which violates our prior condition (i, j) \u2208 \u2126\u2217o \\ \u2126.\nUsing these two properties (27), (28) and applying Lemma 1, we have\n|||Y \u2212 1 p \u03a0\u03a6M \u2217|||op \u2264 4\u03b1\n\u221a d1d2\u2016\u03a0\u03a6(S\u2217)\u2212 Sinit\u2016\u221e \u2264 8\u03b1 \u221a d1d2\u2016M\u2217\u2016\u221e \u2264 8\u03b1\u00b5r\u03c3\u22171, (29)\nwhere the last step follow from (17). For the second term in (26), we use the following lemma proved in [10].\nLemma 5 (Lemma 2 in [10]). Suppose A \u2208 Rd1\u00d7d2 is a fixed matrix. We let d := max{d1, d2}. There exists a constant c such that with probability at least 1\u2212O(d\u22121),\n|||1 p \u03a0\u03a6(A)\u2212A|||op \u2264 c\n( log d\np \u2016A\u2016\u221e +\n\u221a log d\np max\n{ |||A|||2,\u221e, |||A>|||2,\u221e }) .\nGiven the SVD M\u2217 = L\u2217\u03a3R\u2217>, for any i \u2208 [d1], we have\n\u2016M\u2217(i,\u00b7)\u20162 = \u2016L \u2217 (i,\u00b7)\u03a3R \u2217>\u20162 \u2264 \u03c3\u22171\u2016L\u2217(i,\u00b7)\u20162 \u2264 \u03c3 \u2217 1\n\u221a \u00b5r\nd1 .\nWe can bound |||M\u2217>|||2,\u221e similarly. Lemma 5 leads to\n|||1 p \u03a0\u03a6(M \u2217)\u2212M\u2217|||op \u2264 c\u2032 log d p \u00b5r\u221a d1d2 \u03c3\u22171 + c \u2032\n\u221a log d\np\n\u221a \u00b5r\nd1 \u2227 d2 \u03c3\u22171 \u2264 c\u2032 \u03c3\u22171/\n\u221a r (30)\nunder condition p \u2265 4\u00b5r 2 log d\n2(d1\u2227d2) . Putting (29) and (30) together, we obtain\n|||Y \u2212M\u2217|||op \u2264 8\u03b1\u00b5r\u03c3\u22171 + c\u2032 \u03c3\u22171/ \u221a r.\nThen using the fact that U0V >0 is the best rank r approximation of Y and applying Wely\u2019s theorem (see the proof of Theorem 1 for a detailed argument), we have\n|||U0V >0 \u2212M\u2217|||op \u2264 |||U0V >0 \u2212 Y |||op + |||Y \u2212M\u2217|||op \u2264 2|||Y \u2212M\u2217|||op \u2264 16\u03b1\u00b5r\u03c3\u22171 + 2c\u2032 \u03c3\u22171/ \u221a r\nUnder our assumptions, we have 16\u03b1\u00b5r\u03c3\u22171 + 2c\u2032 \u03c3\u22171/ \u221a r \u2264 12\u03c3 \u2217 r . Accordingly, Lemma 15 gives\nd2(U0, V0;U \u2217, V \u2217) \u2264 2\u221a 2\u2212 1 |||U0V >0 \u2212M\u2217|||2F \u03c3\u2217r \u2264 10r|||U0V >0 \u2212M\u2217|||2op \u03c3\u2217r .\nWe complete the proof by combining the above two inequalities."}, {"heading": "6.5 Proof of Theorem 4", "text": "In this section, we turn to prove Theorem 4. Similar to the proof of Theorem 2, we rely on establishing the local descent and smoothness conditions. Compared to the full observation setting, we replace L by L\u0303 given in (6), while the regularization term G\u0303(U, V ) := 164 |||U\n>U \u2212 V >V |||2F merely differs from G(U, V ) given in (20) by a constant factor. It is thus sufficient to analyze the properties of L\u0303.\nDefine E(M\u2217) according to (18). Under the initial condition, we still have\nE(M\u2217) \u2286 U \u00d7 V, and U \u2286 U ,V \u2286 V. (31)\nWe prove the next two lemmas in Section 6.10 and 6.11 respectively. In both lemmas, for any (U, V ) \u2208 U \u00d7 V, we use shorthands\n(U\u03c0\u2217 , V\u03c0\u2217) = arg min (A,B)\u2208E(M\u2217)\n|||U \u2212A|||2F + |||V \u2212B|||2F,\n\u2206U := U \u2212 U\u03c0\u2217 , \u2206V := V \u2212 V\u03c0\u2217 , and \u03b4 := |||\u2206U |||2F + |||\u2206V |||2F. Recall that d := max{d1, d2}.\nLemma 6 (Local descent property of L\u0303). Suppose U ,V satisfy (31). Suppose we let\nS = T\u03b3p\u03b1 [ \u03a0\u03a6 ( Y \u2212 UV > )] ,\nwhere we choose \u03b3 = 3. For any \u03b2 > 0 and \u2208 (0, 14), we define \u03bd := (14\u03b2+81)\u03b1\u00b5r+26 \u221a +18\u03b2\u22121. There exist constants {ci}2i=1 such that if\np \u2265 c1 ( \u00b52r2\n2 +\n1 \u03b1 + 1\n) log d\nd1 \u2227 d2 , (32)\nthen with probability at least 1\u2212 c2d\u22121,\n\u3008\u3008\u2207M L\u0303(U, V ;S), UV > \u2212 U\u03c0\u2217V >\u03c0\u2217 + \u2206U\u2206>V \u3009\u3009 \u2265 3 16 |||UV >\u2212U\u03c0\u2217V >\u03c0\u2217 |||2F\u2212 \u03bd\u03c3\u22171\u03b4\u2212 10\n\u221a \u03c3\u22171\u03b4 3\u2212 2\u03b42 (33)\nfor all (U, V ) \u2208 (U \u00d7 V) \u2229 B2 (\u221a \u03c3\u22171 ) .\nLemma 7 (Smoothness of L\u0303). Suppose U ,V satisfy (31). Suppose we let S = T\u03b3\u03b1p [ \u03a0\u03a6 ( Y \u2212 UV > )] for \u03b3 = 3. There exist constants {ci}3i=1 such that for any \u2208 (0, 14), when p satisfies condition (32), with probability at least 1\u2212 c2d\u22121, we have that for all (U, V ) \u2208 (U \u00d7 V) \u2229 B2( \u221a \u03c3\u22171),\n|||\u2207U L\u0303(U, V ;S)|||2F + |||\u2207V L\u0303(U, V ;S)|||2F \u2264 c3 [ \u00b5r\u03c3\u22171|||UV > \u2212 U\u03c0\u2217V >\u03c0\u2217 |||2F + \u00b5r\u03c3\u22171\u03b4(\u03b4 + \u03c3\u22171) ] . (34)\nIn the remainder of this section, we condition on the events in Lemma 6 and 7. Now we are ready to prove Theorem 4.\nProof of Theorem 4. We essentially follow the process for proving Theorem 2. Let the following shorthands be defined in the same fashion: \u03b4t, (U t\u03c0\u2217 , V t\u03c0\u2217), (\u2206tU ,\u2206 t V ), L\u0303t, G\u0303t.\nHere we show error decays in one step of iteration. The induction process is the same as the proof of Theorem 2, and is thus omitted. For any t \u2265 0, similar to (24) we have that\n\u03b4t+1 \u2264 \u03b4t \u2212 2\u03b7 \u3008\u3008\u2207U L\u0303t +\u2207U G\u0303t, Ut \u2212 U t\u03c0\u2217\u3009\u3009\ufe38 \ufe37\ufe37 \ufe38 W1 \u22122\u03b7 \u3008\u3008\u2207V L\u0303t +\u2207V G\u0303t, Vt \u2212 V t\u03c0\u2217\u3009\u3009\ufe38 \ufe37\ufe37 \ufe38 W2\n+ \u03b72 |||\u2207U L\u0303t +\u2207U G\u0303t|||2F\ufe38 \ufe37\ufe37 \ufe38 W3 +\u03b72 |||\u2207V L\u0303t +\u2207V G\u0303t|||2F\ufe38 \ufe37\ufe37 \ufe38 W4 .\nWe also have\n\u3008\u3008\u2207U L\u0303t, Ut \u2212 U t\u03c0\u2217\u3009\u3009+ \u3008\u3008\u2207V L\u0303t, Vt \u2212 V t\u03c0\u2217\u3009\u3009 = \u3008\u3008\u2207M L\u0303t, UtV >t \u2212 U t\u03c0\u2217V t>\u03c0\u2217 + \u2206tU\u2206t>V \u3009\u3009,\nwhich can be lower bounded by Lemma 6. Note that G\u0303 differs from G by a constant, we can still leverage Lemma 3. Hence, we obtain that\nW1 +W2 \u2265 1 8 |||UtV >t \u2212M\u2217|||2F + 1 64 |||U>t Ut \u2212 V >t Vt|||2F + 1 64 \u03c3\u2217r\u03b4t \u2212 \u03bd\u03c3\u22171\u03b4t \u2212 11\n\u221a \u03c3\u22171\u03b4 3 t \u2212 2\u03b42t .\nOn the other hand, we have\nW3 +W4 \u2264 2|||\u2207U L\u0303t|||2F + 2|||\u2207U G\u0303t|||2F + 2|||\u2207V L\u0303t|||2F + 2|||\u2207V G\u0303t|||2F \u2264 c [ \u00b5r\u03c3\u22171|||UtV >t \u2212M\u2217|||2F + \u00b5r\u03c3\u22171\u03b4t(\u03b4t + \u03c3\u22171) + \u03c3\u22171|||U>t Ut \u2212 V >t Vt|||2F ] ,\nwhere c is a constant, and the last step is implied by Lemma 4 and Lemma 7. By the assumption \u03b7 = c\u2032/[\u00b5r\u03c3\u22171] for sufficiently small constant c\u2032, we thus have\n\u22122\u03b7(W1 +W2) + \u03b72(W3 +W4) \u2264 \u2212 1 32 \u03b7\u03c3\u2217r\u03b4t + 2\u03b7\u03bd\u03c3 \u2217 1\u03b4t + 22\u03b7\n\u221a \u03c3\u22171\u03b4 3 t + 4\u03b7\u03b4 2 t .\nRecall that \u03bd := (14\u03b2 + 81)\u03b1\u00b5r + 26 \u221a + 18\u03b2\u22121. By letting \u03b2 = c1\u03ba, = c2/\u03ba2 and assuming \u03b1 \u2264 c3/(\u00b5r\u03ba2) and \u03b4t \u2264 c4\u03c3\u2217r/\u03ba for some sufficiently small constants {ci}4i=1, we can have \u22122\u03b7(W1 + W2) + \u03b7 2(W3 +W4) \u2264 \u2212 164\u03b7\u03c3 \u2217 r\u03b4t, which implies that\n\u03b4t+1 \u2264 ( 1\u2212 \u03b7\u03c3 \u2217 r\n64\n) \u03b4t,\nand thus completes the proof."}, {"heading": "6.6 Proof of Corollary 2", "text": "We need \u03b1 . 1 \u00b5\u03ba2r due to the condition of Theorem 4. Letting the initial error provided in Theorem 3 be less than the corresponding condition in Theorem 4, we have\n51 \u221a \u03ba\u03b1\u00b5r \u221a r \u221a \u03c3\u22171 + 7c1 \u221a \u03ba\u03c3\u22171 . \u221a \u03c3\u2217r/\u03ba,\nwhich leads to \u03b1 .\n1\n\u00b5 \u221a r3\u03ba3 , . 1\u221a \u03ba3 .\nPlugging the above two upper bounds into the second term in (13), it suffices to have\np & \u03ba3\u00b5r2 log d\nd1 \u2227 d2 .\nComparing the above bound with the second term in (14) completes the proof."}, {"heading": "6.7 Proof of Lemma 2", "text": "Let M := UV >. We observe that\n\u2207ML(U, V ;S) = M + S \u2212M\u2217 \u2212 S\u2217.\nPlugging it back into the left hand side of (21), we obtain\n\u3008\u3008\u2207ML(U, V ;S), UV > \u2212 U\u03c0\u2217V >\u03c0\u2217 + \u2206U\u2206>V \u3009\u3009 = \u3008\u3008M + S \u2212M\u2217 \u2212 S\u2217, M \u2212M\u2217 + \u2206U\u2206>V \u3009\u3009 \u2265 |||M \u2212M\u2217|||2F \u2212 |\u3008\u3008S \u2212 S\u2217, M \u2212M\u2217\u3009\u3009|\ufe38 \ufe37\ufe37 \ufe38\nT1\n\u2212 |\u3008\u3008M + S \u2212M\u2217 \u2212 S\u2217, \u2206U\u2206>V \u3009\u3009|\ufe38 \ufe37\ufe37 \ufe38 T2 . (35)\nNext we derive upper bounds of T1 and T2 respectively.\nUpper bound of T1. We denote the support of S, S\u2217 by \u2126 and \u2126\u2217 respectively. Since S \u2212 S\u2217 is supported on \u2126\u2217 \u222a \u2126, we have\nT1 \u2264 |\u3008\u3008\u03a0\u2126(S \u2212 S\u2217), M \u2212M\u2217\u3009\u3009|\ufe38 \ufe37\ufe37 \ufe38 W1 + |\u3008\u3008\u03a0\u2126\u2217\\\u2126(S \u2212 S\u2217), M \u2212M\u2217\u3009\u3009|\ufe38 \ufe37\ufe37 \ufe38 W2 .\nRecall that for any (i, j) \u2208 \u2126, we have S(i,j) = (M\u2217 + S\u2217 \u2212M)(i,j). Accordingly, we have\nW1 = |||\u03a0\u2126(M \u2212M\u2217)|||2F. (36)\nNow we turn to bound W2. Since S(i,j) = 0 for any (i, j) \u2208 \u2126\u2217 \\ \u2126, we have\nW2 = |\u3008\u3008\u03a0\u2126\u2217\\\u2126S\u2217, M \u2212M\u2217\u3009\u3009|.\nLet ui be the i-th row of M \u2212M\u2217, and vj be the j-th column of M \u2212M\u2217. For any k \u2208 [d2], we let u\n(k) i denote the element of ui that has the k-th largest magnitude. Similarly, for any k \u2208 [d1], we let v(k)j denote the element of vj that has the k-th largest magnitude. From the design of sparse estimator (4), we have that for any (i, j) \u2208 \u2126\u2217 \\\u2126, |(M\u2217+S\u2217\u2212M)(i,j)| is either smaller than the \u03b3\u03b1d2-th largest entry of the i-th row of M\u2217 + S\u2217 \u2212M or smaller than the \u03b3\u03b1d1-th largest entry of the j-th column of M\u2217 + S\u2217 \u2212M . Note that S\u2217 only contains at most \u03b1-fraction nonzero entries per row and column. As a result, |(M\u2217+S\u2217\u2212M)(i,j)| has to be less than the magnitude of u(\u03b3\u03b1d2\u2212\u03b1d2)i or v (\u03b3\u03b1d1\u2212\u03b1d1) j . Formally, we have for (i, j) \u2208 \u2126\u2217 \\ \u2126,\n|(M\u2217 + S\u2217 \u2212M)(i,j)| \u2264 max { |u(\u03b3\u03b1d2\u2212\u03b1d2)i |, |v (\u03b3\u03b1d1\u2212\u03b1d1) j | } \ufe38 \ufe37\ufe37 \ufe38\nbij\n. (37)\nFurthermore, we obtain\nb2ij \u2264 |u (\u03b3\u03b1d2\u2212\u03b1d2) i | 2 + |v(\u03b3\u03b1d1\u2212\u03b1d1)j | 2 \u2264 \u2016ui\u2016 2 2\n(\u03b3 \u2212 1)\u03b1d2 + \u2016vj\u201622 (\u03b3 \u2212 1)\u03b1d1 . (38)\nMeanwhile, for any (i, j) \u2208 \u2126\u2217 \\ \u2126, we have\n|S\u2217(i,j) \u00b7 (M \u2212M \u2217)(i,j)| = |(M\u2217 + S\u2217 \u2212M \u2212M\u2217 +M)(i,j) \u00b7 (M \u2212M\u2217)(i,j)|\n\u2264 |(M \u2212M\u2217)(i,j)|2 + |(M\u2217 + S\u2217 \u2212M)(i,j)| \u00b7 |(M \u2212M\u2217)(i,j)| \u2264 |(M \u2212M\u2217)(i,j)|2 + bij \u00b7 |(M \u2212M\u2217)(i,j)|\n\u2264 ( 1 + \u03b2\n2\n) |(M \u2212M\u2217)(i,j)|2 +\nb2ij 2\u03b2 , (39)\nwhere \u03b2 in the last step can be any positive number. Combining (38) and (39) leads to W2 \u2264 \u2211\n(i,j)\u2208\u2126\u2217\\\u2126\n|S\u2217(i,j) \u00b7 (M \u2212M \u2217)(i,j)|\n\u2264 ( 1 + \u03b2\n2\n) |||\u03a0\u2126\u2217\\\u2126(M \u2212M\u2217)|||2F + \u2211 (i,j)\u2208\u2126\u2217\\\u2126 b2ij 2\u03b2\n\u2264 ( 1 + \u03b2\n2\n) |||\u03a0\u2126\u2217\\\u2126(M \u2212M\u2217)|||2F + 1\n2\u03b2 \u2211 (i,j)\u2208\u2126\u2217\\\u2126 ( \u2016ui\u201622 (\u03b3 \u2212 1)\u03b1d2 + \u2016vj\u201622 (\u03b3 \u2212 1)\u03b1d1 )\n\u2264 ( 1 + \u03b2\n2\n) |||\u03a0\u2126\u2217\\\u2126(M \u2212M\u2217)|||2F +\n1\n\u03b2(\u03b3 \u2212 1) |||M \u2212M\u2217|||2F. (40)\nIn the last step, we use\u2211 (i,j)\u2208\u2126\u2217\\\u2126 ( 1 d2 \u2016ui\u201622 + 1 d1 \u2016vj\u201622 ) \u2264 \u2211 (i,j)\u2208\u2126\u2217 ( 1 d2 \u2016ui\u201622 + 1 d1 \u2016vj\u201622 ) \u2264 \u2211 i\u2208[d] \u2211 j\u2208\u2126\u2217\n(i,\u00b7)\n1\nd2 \u2016ui\u201622 + \u2211 j\u2208[d] \u2211 i\u2208\u2126\u2217\n(\u00b7,j)\n1\nd1 \u2016vj\u201622\n\u2264 \u03b1 \u2211 i\u2208[d] \u2016ui\u201622 + \u03b1 \u2211 j\u2208[d] \u2016vj\u201622 = 2\u03b1|||M \u2212M\u2217|||2F. (41)\nWe introduce shorthand \u03b4 := |||\u2206U |||2F + |||\u2206V |||2F. We prove the following inequality in the end of this section.\n|||M \u2212M\u2217|||F \u2264 \u221a 5\u03c3\u22171\u03b4. (42)\nCombining (36), (40) and (42) leads to T1 \u2264 |||\u03a0\u2126(M \u2212M\u2217)|||2F + ( 1 + \u03b2\n2\n) |||\u03a0\u2126\u2217\\\u2126(M \u2212M\u2217)|||2F + 5\u03c3\u22171\u03b4\n\u03b2(\u03b3 \u2212 1)\n\u2264 9(2\u03b3 + \u03b2 + 2)\u03b1\u00b5r\u03c3\u22171\u03b4 + 5\u03c3\u22171\u03b4\n\u03b2(\u03b3 \u2212 1) , (43)\nwhere the last step follows from Lemma 14 by noticing that \u03a0\u2126(M \u2212M\u2217) has at most \u03b3\u03b1-fraction nonzero entries per row and column.\nUpper bound of T2. To ease notation, we let C := M + S \u2212M\u2217 \u2212 S\u2217. We observe that C is supported on \u2126c, we have\nT2 \u2264 |\u3008\u3008\u03a0\u2126\u2217c\u2229\u2126c(M \u2212M\u2217), \u2206U\u2206>V \u3009\u3009|\ufe38 \ufe37\ufe37 \ufe38 W3 + |\u3008\u3008\u03a0\u2126\u2217\u2229\u2126cC, \u2206U\u2206>V \u3009\u3009|\ufe38 \ufe37\ufe37 \ufe38 W4 .\nBy Cauchy-Swartz inequality, we have\nW3 \u2264 |||\u03a0\u2126\u2217c\u2229\u2126c(M \u2212M\u2217)|||F|||\u2206U\u2206>V |||F \u2264 |||M \u2212M\u2217|||F|||\u2206U |||F|||\u2206V |||F \u2264 \u221a 5\u03c3\u22171\u03b4 3/2,\nwhere the last step follows from (42) and |||\u2206U |||F|||\u2206V |||F \u2264 \u03b4/2. It remains to bound W4. By Cauchy-Swartz inequality, we have\nW4 \u2264 |||\u03a0\u2126\u2217\u2229\u2126cC|||F|||\u2206U\u2206>V |||F \u2264 |||\u03a0\u2126\u2217\u2229\u2126c(M\u2217 + S\u2217 \u2212M)|||F|||\u2206U\u2206>V |||F\n(a) \u2264 \u221a \u2211\n(i,j)\u2208\u2126\u2217\\\u2126\nb2ij |||\u2206U |||F|||\u2206V |||F (b) \u2264  \u2211 (i,j)\u2208\u2126\u2217\\\u2126 \u2016ui\u201622 (\u03b3 \u2212 1)\u03b1d2 + \u2016vj\u201622 (\u03b3 \u2212 1)\u03b1d1 1/2 |||\u2206U |||F|||\u2206V |||F. (c)\n\u2264 \u221a 2\n\u03b3 \u2212 1 |||M \u2212M\u2217|||F|||\u2206U |||F|||\u2206V |||F \u2264\n\u221a 5\u03c3\u22171\u03b4 3\n2(\u03b3 \u2212 1) ,\nwhere step (a) is from (37), step (b) follows from (38), and step (c) follows from (41). Combining the upper bounds of W3 and W4, we obtain\nT2 \u2264 \u221a 5\u03c3\u22171\u03b4 3/2 + \u221a 5\u03c3\u22171\u03b4 3\n2(\u03b3 \u2212 1) . (44)\nCombining pieces. Now we choose \u03b3 = 2. Then inequality (43) implies that\nT1 \u2264 [9(\u03b2 + 6)\u03b1\u00b5r + 5\u03b2\u22121]\u03c3\u22171\u03b4.\nInequality (44) then implies that\nT2 \u2264 3 \u221a \u03c3\u22171\u03b4 3.\nPlugging the above two inequalities into (35) completes the proof.\nProof of inequality (42). We find that\n|||M \u2212M\u2217|||2F \u2264 [\u221a \u03c3\u22171(|||\u2206V |||F + |||\u2206U |||F) + |||\u2206U |||F|||\u2206V |||F ]2\n\u2264 [\u221a \u03c3\u22171(|||\u2206V |||F + |||\u2206U |||F) + 1\n2\n\u221a \u03c3\u22171|||\u2206U |||F + 1\n2\n\u221a \u03c3\u22171|||\u2206V |||F ]2 \u2264 5\u03c3\u22171(|||\u2206U |||2F + |||\u2206V |||2F),\nwhere the first step follows from the upper bound of |||M \u2212M\u2217|||F shown in Lemma 12, and the second step follows from the assumption |||\u2206U |||F, |||\u2206V |||F \u2264 \u221a \u03c3\u22171."}, {"heading": "6.8 Proof of Lemma 3", "text": "We first observe that\n\u2207UG(U, V ) = 1 2 U(U>U \u2212 V >V ), \u2207V G(U, V ) = 1 2 V (V >V \u2212 U>U),\nTherefore, we obtain\n\u3008\u3008\u2207UG(U, V ), U \u2212 U\u03c0\u2217\u3009\u3009+ \u3008\u3008\u2207V G(U, V ), V \u2212 V\u03c0\u2217\u3009\u3009\n= 1\n2 \u3008\u3008U>U \u2212 V >V , U>U \u2212 V >V \u2212 U>U\u03c0\u2217 + V >V\u03c0\u2217\u3009\u3009\n= 1\n4 |||U>U \u2212 V >V |||2F +\n1 4 \u3008\u3008U>U \u2212 V >V , U>U \u2212 V >V \u2212 2U>U\u03c0\u2217 + 2V >V\u03c0\u2217\u3009\u3009. (45)\nNote that\nU>U \u2212 V >V = (U\u03c0\u2217 + \u2206U )>(U\u03c0\u2217 + \u2206U )\u2212 (V\u03c0\u2217 + \u2206V )>(V\u03c0\u2217 + \u2206V ) = U>\u03c0\u2217\u2206U + \u2206 > UU\u03c0\u2217 + \u2206 > U\u2206U \u2212 V >\u03c0\u2217\u2206V \u2212\u2206>V V\u03c0\u2217 \u2212\u2206>V \u2206V ,\nwhere we use U>\u03c0\u2217U\u03c0\u2217 = V >\u03c0\u2217V\u03c0\u2217 in the last step. Furthermore, since U>U \u2212 V >V is symmetric, we have\n\u3008\u3008U>U \u2212 V >V , U>\u03c0\u2217\u2206U + \u2206>UU\u03c0\u2217 \u2212 V >\u03c0\u2217\u2206V \u2212\u2206>V V\u03c0\u2217\u3009\u3009 = \u3008\u3008U>U \u2212 V >V , 2\u2206>UU\u03c0\u2217 \u2212 2\u2206>V V\u03c0\u2217\u3009\u3009.\nUsing these arguments, for the second term in (45), denoted by T2, we have\nT2 = 1 4 \u3008\u3008U>U \u2212 V >V , \u2206>U\u2206U + \u2206>V \u2206V \u3009\u3009.\nMoreover, we have 4T2 \u2264 |\u3008\u3008U>U \u2212 V >V , \u2206>U\u2206U + \u2206>V \u2206V \u3009\u3009| \u2264 |||U>U \u2212 V >V |||F ( |||\u2206U |||2F + |||\u2206V |||2F ) \u2264 ( |||U>U \u2212 U>\u03c0\u2217U\u03c0\u2217 |||F + |||V >V \u2212 V >\u03c0\u2217V\u03c0\u2217 |||F ) \u03b4\n\u2264 2 (|||U\u03c0\u2217 |||op|||\u2206U |||F + |||V\u03c0\u2217 |||op|||\u2206V |||F) \u03b4 \u2264 2 \u221a 2\u03c3\u22171\u03b4 3. (46)\nWe still need to find a lower bound of |||U>U \u2212V >V |||F. The following inequality,which we prove later, is true:\n|||U>U \u2212 V >V |||2F \u2265 |||UU> \u2212 U\u03c0\u2217U>\u03c0\u2217 |||2F + |||V V > \u2212 V\u03c0\u2217V >\u03c0\u2217 |||2F \u2212 2|||UV > \u2212 U\u03c0\u2217V >\u03c0\u2217 |||2F. (47)\nProceeding with the first term in (45) by using (47), we get\n1 4 |||U>U \u2212 V >V |||2F = 1 8 |||U>U \u2212 V >V |||2F + 1 8 |||U>U \u2212 V >V |||2F \u2265 1 8 |||U>U \u2212 V >V |||2F + 1 8 |||UU> \u2212 U\u03c0\u2217U>\u03c0\u2217 |||2F + 1 8 |||V V > \u2212 V\u03c0\u2217V >\u03c0\u2217 |||2F \u2212 1 4 |||UV > \u2212 U\u03c0\u2217V >\u03c0\u2217 |||2F = 1\n8 |||U>U \u2212 V >V |||2F +\n1 8 |||FF> \u2212 F\u03c0\u2217F>\u03c0\u2217 |||2F \u2212 1 2 |||UV > \u2212 U\u03c0\u2217V >\u03c0\u2217 |||2F, (48)\nwhere we let\nF := [ U V ] , F\u03c0\u2217 := [ U\u03c0\u2217\nV\u03c0\u2217\n] .\nIntroduce \u2206F := F \u2212F\u03c0\u2217 . Recall that \u03b4 := |||\u2206U |||2F + |||\u2206V |||2F. Equivalently \u03b4 = |||\u2206F |||2F. We have\n|||FF> \u2212 F\u03c0\u2217F>\u03c0\u2217 |||F = |||\u2206FF>\u03c0\u2217 + F\u03c0\u2217\u2206>F + \u2206F\u2206>F |||F \u2265 |||\u2206FF>\u03c0\u2217 + F\u03c0\u2217\u2206>F |||F \u2212 |||\u2206F |||2F = |||\u2206FF>\u03c0\u2217 + F\u03c0\u2217\u2206>F |||F \u2212 \u03b4.\nFor the first term, we have\n|||\u2206FF>\u03c0\u2217 + F\u03c0\u2217\u2206>F |||2F = 2|||\u2206FF>\u03c0\u2217|||2F + \u3008\u3008\u2206FF>\u03c0\u2217, F\u03c0\u2217\u2206>F \u3009\u3009 \u2265 2\u03c3r(F\u03c0\u2217)2|||\u2206F |||2F + \u3008\u3008\u2206FF>\u03c0\u2217, F\u03c0\u2217\u2206>F \u3009\u3009 = 4\u03c3\u2217r |||\u2206F |||2F + \u3008\u3008\u2206FF>\u03c0\u2217, F\u03c0\u2217\u2206>F \u3009\u3009.\nFor the cross term, by the following result, proved in [11] (we also provide a proof in Section 7.5 for the sake of completeness), we have \u3008\u3008\u2206FF>\u03c0\u2217, F\u03c0\u2217\u2206>F \u3009\u3009 \u2265 0. Lemma 8. When |||F \u2212 F\u03c0\u2217 |||op < \u221a 2\u03c3\u2217r , we have that \u2206>FF\u03c0\u2217 is symmetric.\nAccordingly, we have |||FF>\u2212F\u03c0\u2217F>\u03c0\u2217 |||F \u2265 2 \u221a \u03c3\u2217r\u03b4\u2212\u03b4 \u2265 \u221a \u03c3\u2217r\u03b4 under condition \u03b4 \u2264 \u03c3\u2217r . Plugging\nthis lower bound into (48), we obtain\n1 4 |||U>U \u2212 V >V |||2F \u2265 1 8 |||U>U \u2212 V >V |||2F + 1 8 \u03c3\u2217r\u03b4 \u2212 1 2 |||UV > \u2212 U\u03c0\u2217V >\u03c0\u2217 |||2F.\nPutting (45), (46) and the above inequality together completes the proof.\nProof of inequality (47). For the term on the left hand side of (47), it is easy to check that\n|||U>U \u2212 V >V |||2F = |||UU>|||2F + |||V V >|||2F \u2212 2|||UV >|||2F. (49)\nThe property U>\u03c0\u2217U\u03c0\u2217 = V >\u03c0\u2217V\u03c0\u2217 implies that |||U\u03c0\u2217U>\u03c0\u2217 |||F = |||V\u03c0\u2217V >\u03c0\u2217 |||F = |||U\u03c0\u2217V >\u03c0\u2217 |||F. Therefore, expanding those quadratic terms on the right hand side of (47), one can show that it is equal to\n|||UU>|||2F + |||V V >|||2F \u2212 2|||U>\u03c0\u2217U |||2F \u2212 2|||V >\u03c0\u2217V |||2F + 4\u3008\u3008U>\u03c0\u2217U, V >\u03c0\u2217V \u3009\u3009 \u2212 2|||UV >|||2F. (50)\nComparing inequalities (49) and (50), it thus remains to show that\n\u22122|||U>\u03c0\u2217U |||2F \u2212 2|||V >\u03c0\u2217V |||2F + 4\u3008\u3008U>\u03c0\u2217U, V >\u03c0\u2217V \u3009\u3009 \u2264 0.\nEquivalently, we always have |||U>\u03c0\u2217U \u2212 V >\u03c0\u2217V |||2F \u2265 0, and thus prove (47)."}, {"heading": "6.9 Proof of Lemma 4", "text": "First, we turn to prove (23). As\n\u2207UG(U, V ) = 1 2 U(U>U \u2212 V >V ), \u2207V G(U, V ) = 1 2 V (V >V \u2212 U>U),\nwe have |||\u2207UG(U, V )|||2F + |||\u2207V G(U, V )|||2F \u2264 1\n4\n( |||U |||2op + |||V |||2op ) |||U>U \u2212 V >V |||2F.\nAs (U, V ) \u2208 B2( \u221a \u03c3\u22171), we thus have |||U |||op \u2264 |||U\u03c0\u2217 |||op + |||U\u03c0\u2217 \u2212 U |||op \u2264 2 \u221a \u03c3\u22171, and similarly\n|||V |||op \u2264 2 \u221a \u03c3\u22171. We obtain\n|||\u2207UG(U, V )|||2F + |||\u2207V G(U, V )|||2F \u2264 2\u03c3\u22171|||U>U \u2212 V >V |||2F.\nNow we turn to prove (22). We observe that\n\u2207ML(U, V ;S) = M + S \u2212M\u2217 \u2212 S\u2217,\nwhere we let M := UV >. We denote the support of S, S\u2217 by \u2126 and \u2126\u2217 respectively. Based on the sparse estimator (4) for computing S, \u2207ML(U, V ;S) is only supported on \u2126c. We thus have\n|||\u2207ML(U, V ;S)|||F \u2264 |||\u03a0\u2126c\\\u2126\u2217(M \u2212M\u2217)|||F + |||\u03a0\u2126c\u2229\u2126\u2217(M \u2212M\u2217 \u2212 S\u2217)|||F \u2264 |||M \u2212M\u2217|||F + |||\u03a0\u2126c\u2229\u2126\u2217(M \u2212M\u2217 \u2212 S\u2217)|||F.\nIt remains to upper bound the second term on the right hand side. Following (37) and (38), we have\n|||\u03a0\u2126c\u2229\u2126\u2217(M \u2212M\u2217 \u2212 S\u2217)|||2F \u2264 \u2211\n(i,j)\u2208\u2126c\u2229\u2126\u2217\n\u2016ui\u201622 (\u03b3 \u2212 1)\u03b1d2 + \u2016vj\u201622 (\u03b3 \u2212 1)\u03b1d1 \u2264 2 \u03b3 \u2212 1 |||M \u2212M\u2217|||2F,\nwhere the last step is proved in (41). By choosing \u03b3 = 2, we thus conclude that\n|||\u2207ML(U, V ;S)|||F \u2264 (1 + \u221a 2)|||M \u2212M\u2217|||F."}, {"heading": "6.10 Proof of Lemma 6", "text": "We denote the support of \u03a0\u03a6(S\u2217), S by \u2126\u2217o and \u2126. We always have \u2126\u2217o \u2286 \u03a6 and \u2126 \u2286 \u03a6. In the sequel, we establish several results that characterize the properties of \u03a6. The first result, proved in Section 7.2, shows that the Frobenius norm of any incoherent matrix whose row (or column) space are equal to L\u2217 (or R\u2217) is well preserved under partial observations supported on \u03a6.\nLemma 9. Suppose M\u2217 \u2208 Rd1\u00d7d2 is a rank r and \u00b5-incoherent matrix that has SVD M\u2217 = L\u2217\u03a3\u2217R\u2217>. Then there exists an absolute constant c such that for any \u2208 (0, 1), if p \u2265 c \u00b5r log d\n2(d1\u2227d2) , then with probability at least 1\u2212 2d\u22123, we have that for all A \u2208 Rd2\u00d7r, B \u2208 Rd1\u00d7r,\n(1\u2212 )|||L\u2217A> +BR\u2217>|||2F \u2264 p\u22121|||\u03a0\u03a6 ( L\u2217A> +BR\u2217> ) |||2F \u2264 (1 + )|||L\u2217A> +BR\u2217>|||2F.\nWe need the next result, proved in Section 7.3, to control the number of nonzero entries per row and column in \u2126\u2217o and \u03a6.\nLemma 10. If p \u2265 563 log d \u03b1(d1\u2227d2) , then with probability at least 1\u2212 6d \u22121, we have \u2223\u2223|\u03a6(i,\u00b7)| \u2212 pd2\u2223\u2223 \u2264 12pd2, \u2223\u2223|\u03a6(\u00b7,j)| \u2212 pd1\u2223\u2223 \u2264 12pd1, |\u2126\u2217o(i,\u00b7)| \u2264 32\u03b1pd2, |\u2126\u2217o(\u00b7,j)| \u2264 32\u03b1pd1, for all i \u2208 [d1] and j \u2208 [d2].\nThe next lemma, proved in Section 7.4, can be used to control the projection of small matrices to \u03a6.\nLemma 11. There exists constant c such that for any \u2208 (0, 1), if p \u2265 c \u00b5 2r2 log d\n2(d1\u2227d2) , then with probability at least 1\u2212O(d\u22121), for all matrices Z \u2208 Rd1\u00d7d2, U \u2208 Rd1\u00d7r and V \u2208 Rd2\u00d7r that satisfy |||U |||2,\u221e \u2264 \u221a \u00b5r/d1,|||V |||2,\u221e \u2264 \u221a \u00b5r/d2, we have\np\u22121|||\u03a0\u03a6(UV >)|||2F \u2264 |||U |||2F|||V |||2F + |||U |||F|||V |||F; (51)\np\u22121|||\u03a0\u03a6(Z)V |||2F \u2264 2\u00b5r|||\u03a0\u03a6(Z)|||2F; (52) p\u22121|||U>\u03a0\u03a6(Z)|||2F \u2264 2\u00b5r|||\u03a0\u03a6(Z)|||2F. (53)\nIn the remainder of this section, we condition on the events in Lemmas 9, 10 and 11. Now we are ready to prove Lemma 6.\nProof of Lemma 6. Using shorthand M := UV >, we have\n\u2207M L\u0303(U, V ;S) = p\u22121\u03a0\u03a6 (M + S \u2212M\u2217 \u2212 S\u2217) .\nPlugging it back into the left hand side of (21), we obtain\n\u3008\u3008\u2207M L\u0303(U, V ;S), UV > \u2212 U\u03c0\u2217V >\u03c0\u2217 + \u2206U\u2206>V \u3009\u3009\n= 1\np \u3008\u3008\u03a0\u03a6 (M + S \u2212M\u2217 \u2212 S\u2217), M \u2212M\u2217 + \u2206U\u2206>V \u3009\u3009\n\u2265 1 p |||\u03a0\u03a6 (M \u2212M\u2217) |||2F\ufe38 \ufe37\ufe37 \ufe38\nT1\n\u2212 1 p |\u3008\u3008\u03a0\u03a6 (S \u2212 S\u2217), M \u2212M\u2217\u3009\u3009|\ufe38 \ufe37\ufe37 \ufe38\nT2\n\u2212 1 p |\u3008\u3008\u03a0\u03a6 (M + S \u2212M\u2217 \u2212 S\u2217), \u2206U\u2206>V \u3009\u3009|\ufe38 \ufe37\ufe37 \ufe38\nT3\n.\n(54)\nNext we derive lower bounds of T1, upper bounds of T2 and T3 respectively.\nLower bound of T1. We observe thatM\u2212M\u2217 = U\u2217\u03c0\u2217\u2206>V +\u2206UV >\u03c0\u2217+\u2206U\u2206>V . By triangle inequality, we have |||\u03a0\u03a6(M \u2212M\u2217)|||F \u2265 |||\u03a0\u03a6(U\u03c0\u2217\u2206>V + \u2206UV >\u03c0\u2217)|||F \u2212 |||\u03a0\u03a6(\u2206U\u2206>V )|||F. Note that when c \u2265 a\u2212 b for a, b \u2265 0, we always have c2 \u2265 12a 2 \u2212 b2. We thus have\nT1 \u2265 1 2p |||\u03a0\u03a6(U\u03c0\u2217\u2206>V + \u2206UV >\u03c0\u2217)|||2F \u2212 1 p |||\u03a0\u03a6(\u2206U\u2206>V )|||2F\n\u2265 1 2 (1\u2212 )|||U\u03c0\u2217\u2206>V + \u2206UV >\u03c0\u2217 |||2F \u2212 1 p |||\u03a0\u03a6(\u2206U\u2206>V )|||2F\n\u2265 1 2 (1\u2212 )|||M \u2212M\u2217 \u2212\u2206U\u2206>V |||2F \u2212 |||\u2206U |||2F|||\u2206V |||2F \u2212 9 \u03c3\u22171|||\u2206U |||F|||\u2206V |||F \u2265 1 4 (1\u2212 )|||M \u2212M\u2217|||2F \u2212 1 2 (1\u2212 )|||\u2206U\u2206V |||2F \u2212 |||\u2206U |||2F|||\u2206V |||2F \u2212 9 \u03c3\u22171|||\u2206U |||F|||\u2206V |||F \u2265 1 4 (1\u2212 )|||M \u2212M\u2217|||2F \u2212 2\u03b42 \u2212 5 \u03c3\u22171\u03b4.\nwhere the second step is implied by Lemma 9, the third step follows from (51) in Lemma 11 by noticing that |||\u2206U |||2,\u221e \u2264 3 \u221a \u00b5r\u03c3\u22171/d1 and |||\u2206V |||2,\u221e \u2264 3 \u221a \u00b5r\u03c3\u22171/d1, which is further implied by (31).\nUpper bound of T2. Since S \u2212 S\u2217 is supported on \u2126\u22170 \u222a \u2126, we have\npT2 \u2264 |\u3008\u3008\u03a0\u2126\u2217o\\\u2126(S \u2217), \u03a0\u2126\u2217o\\\u2126(M \u2212M \u2217)\u3009\u3009|+ |\u3008\u3008\u03a0\u2126(S \u2212 S\u2217), \u03a0\u2126(M \u2212M\u2217)\u3009\u3009|. (55)\nFor any (i, j) \u2208 \u2126, we have (S \u2212 S\u2217)(i,j) = (M\u2217 \u2212M)(i,j). Therefore, for the second term on the right hand side, we have\n|\u3008\u3008\u03a0\u2126(S \u2212 S\u2217), \u03a0\u2126(M \u2212M\u2217)\u3009\u3009| \u2264 |||\u03a0\u2126(M \u2212M\u2217)|||2F \u2264 18\u03b3p\u03b1\u00b5r\u03c3\u22171\u03b4, (56)\nwhere the last inequality follows from Lemma 14 and the fact that |\u2126(i,\u00b7)| \u2264 \u03b3p\u03b1d2, |\u2126(\u00b7,j)| \u2264 \u03b3p\u03b1d1 for all i \u2208 [d1], j \u2208 [d2].\nWe denote the i-th row of \u03a0\u03a6(M \u2212M\u2217) by ui, and we denote the j-th column of \u03a0\u03a6(M \u2212M\u2217) by vj . We let u (k) i denote the element of ui that has the k-th largest magnitude. We let v (k) j denote the element of vj that has the k-th largest magnitude. For the first term on the right hand side of (55), we first observe that for (i, j) \u2208 \u2126\u2217o \\\u2126, |(M\u2217+ S\u2217 \u2212M)(i,j)| is either less than the \u03b3p\u03b1d2-th largest element in the i-th row of \u03a0\u03a6(M\u2217 + S\u2217 \u2212M), or less than \u03b3p\u03b1d1-th largest element in the j-th row of \u03a0\u03a6(M\u2217 + S\u2217 \u2212M). Based on Lemma 10, \u03a0\u03a6(S\u2217) has at most 3p\u03b1d2/2 nonzero entries per row and at most 3p\u03b1d1/2 nonzero entries per column. Therefore, we have\n|(M\u2217 + S\u2217 \u2212M)(i,j)| \u2264 max { |u((\u03b3\u22121.5)p\u03b1d2)i |, |v ((\u03b3\u22121.5)p\u03b1d1) j | } . (57)\nIn addition, we observe that\n|\u3008\u3008\u03a0\u2126\u2217o\\\u2126(S \u2217), \u03a0\u2126\u2217o\\\u2126(M \u2212M \u2217)\u3009\u3009| \u2264 \u2211\n(i,j)\u2208\u2126\u2217o\\\u2126\n|(M\u2217 + S\u2217 \u2212M)(i,j)||(M\u2217 \u2212M)(i,j)|+ |(M\u2217 \u2212M)(i,j)|2\n\u2264 ( 1 + \u03b2\n2\n) |||\u03a0\u2126\u2217o(M \u2217 \u2212M)|||2F + 1\n2\u03b2 \u2211 (i,j)\u2208\u2126\u2217o\\\u2126 |(M\u2217 + S\u2217 \u2212M)(i,j)|2,\n\u2264 (27 + 14\u03b2)p\u03b1\u00b5r\u03c3\u22171\u03b4 + 1\n2\u03b2 \u2211 (i,j)\u2208\u2126\u2217o\\\u2126 |(M\u2217 + S\u2217 \u2212M)(i,j)|2, (58)\nwhere the second step holds for any \u03b2 > 0 and the last step follows from Lemma 14 under the size constraints of \u2126\u2217o shown in Lemma 10. For the second term in (58), using (57), we have\u2211\n(i,j)\u2208\u2126\u2217o\\\u2126\n|(M\u2217 + S\u2217 \u2212M)(i,j)|2 \u2264 \u2211\n(i,j)\u2208\u2126\u2217o\n|u((\u03b3\u22121.5)p\u03b1d2)i | 2 + |v((\u03b3\u22121.5)p\u03b1d1)j | 2\n= \u2211 i\u2208[d1] \u2211 j\u2208\u2126\u2217\no(i,\u00b7)\n|u((\u03b3\u22121.5)p\u03b1d2)i | 2 + \u2211 j\u2208[d2] \u2211 i\u2208\u2126\u2217\no(\u00b7,j)\n|v((\u03b3\u22121.5)p\u03b1d1)j | 2\n\u2264 \u2211 i\u2208[d1] 1.5 \u03b3 \u2212 1.5 \u2016ui\u201622 + \u2211 j\u2208[d2] 1.5 \u03b3 \u2212 1.5 \u2016vj\u201622 \u2264\n3\n\u03b3 \u2212 1.5 |||\u03a0\u03a6(M \u2212M\u2217)|||2F. (59)\nMoreover, we have\n|||\u03a0\u03a6(M \u2212M\u2217)|||2F \u2264 2|||\u03a0\u03a6(U\u03c0\u2217\u2206>V + \u2206UV >\u03c0\u2217)|||2F + 2|||\u03a0\u03a6(\u2206U\u2206>V )|||2F \u2264 2(1 + )p|||U\u03c0\u2217\u2206>V + \u2206UV >\u03c0\u2217 |||2F + 2p|||\u2206U |||2F|||\u2206V |||2F + 18p \u03c3\u22171|||\u2206U |||F|||\u2206V |||F \u2264 4(1 + )p ( |||U\u03c0\u2217 |||2op|||\u2206V |||2F + |||V\u03c0\u2217 |||2op|||\u2206U |||2F ) + 2p|||\u2206U |||2F|||\u2206V |||2F + 18p \u03c3\u22171|||\u2206U |||F|||\u2206V |||F\n\u2264 (4 + 13 )p\u03c3\u22171\u03b4 + 2p\u03b42, (60)\nwhere the second step follows from Lemma 9 and inequality (51) in Lemma 11. Putting (55)-(60) together, we obtain\nT2 \u2264 (18\u03b3 + 14\u03b2 + 27)\u03b1\u00b5r\u03c3\u22171\u03b4 + 3[(2 + 7 )\u03c3\u22171\u03b4 + \u03b4 2]\n\u03b2(\u03b3 \u2212 1.5) .\nUpper bound of T3. By Cauchy-Schwarz inequality, we have\npT3 \u2264 |||\u03a0\u03a6(M \u2212M\u2217 + S \u2212 S\u2217)|||F|||\u03a0\u03a6(\u2206U\u2206>V )|||F \u2264 |||\u03a0\u03a6(M \u2212M\u2217 + S \u2212 S\u2217)|||F \u221a p|||\u2206U |||2F|||\u2206V |||2F + 9p \u03c3\u22171|||\u2206U |||F|||\u2206V |||F\n\u2264 |||\u03a0\u03a6(M \u2212M\u2217 + S \u2212 S\u2217)|||F \u221a p\u03b42 + 5p \u03c3\u22171\u03b4.\nwhere we use (51) in Lemma 11 in the second step. We observe that \u03a0\u03a6(M \u2212M\u2217 + S \u2212 S\u2217) is supported on \u03a6 \\ \u2126. Therefore, we have\n|||\u03a0\u03a6(M \u2212M\u2217 + S \u2212 S\u2217)|||F \u2264 |||\u03a0\u03a6\u2229\u2126c\u2229\u03a6\u2217c(M \u2212M\u2217)|||F + |||\u03a0\u03a6\u2229\u2126c\u2229\u03a6\u2217(M \u2212M\u2217 \u2212 S\u2217)|||F \u2264 |||\u03a0\u03a6(M \u2212M\u2217)|||F + |||\u03a0\u2126c\u2229\u03a6\u2217(M \u2212M\u2217 \u2212 S\u2217)|||F\n\u2264 |||\u03a0\u03a6(M \u2212M\u2217)|||F + \u221a\n3\n\u03b3 \u2212 1.5 |||\u03a0\u03a6(M \u2212M\u2217)|||F\n\u2264 ( 1 + \u221a 3\n\u03b3 \u2212 1.5\n)\u221a (4 + 13 )p\u03c3\u22171\u03b4 + 2p\u03b4 2,\nwhere the third step follows from (59), and the last step is from (60). Under assumptions \u03b3 = 3, \u2264 1/4 and \u03b4 \u2264 \u03c3\u22171, we have\nT3 \u2264 3 \u221a 9\u03c3\u22171\u03b4 + 2\u03b4 2 \u221a \u03b42 + 5 \u03c3\u22171\u03b4 \u2264 10 \u221a \u03c3\u22171\u03b4 3 + 23 \u221a \u03c3\u22171\u03b4.\nCombining pieces. Under the aforementioned assumptions, putting all pieces together leads to\n\u3008\u3008\u2207M L\u0303(U, V ;S), UV > \u2212 U\u03c0\u2217V >\u03c0\u2217 + \u2206U\u2206>V \u3009\u3009\n\u2265 3 16 |||M \u2212M\u2217|||2F \u2212 (14\u03b2 + 81)\u03b1\u00b5r\u03c3\u22171\u03b4 \u2212\n( 26 \u221a + 18\n\u03b2\n) \u03c3\u22171\u03b4 \u2212 10 \u221a \u03c3\u22171\u03b4 3 \u2212 2\u03b42."}, {"heading": "6.11 Proof of Lemma 7", "text": "Let M := UV >. We find that\n\u2207U L\u0303(U, V ;S) = p\u22121\u03a0\u03a6 (M + S \u2212M\u2217 \u2212 S\u2217)V, \u2207V L\u0303(U, V ;S) = p\u22121\u03a0\u03a6 (M + S \u2212M\u2217 \u2212 S\u2217)> U.\nConditioning on the event in Lemma 11, since (U, V ) \u2208 U \u00d7V, inequalities (52) and (53) imply that\n|||\u2207U L\u0303(U, V ;S)|||2F + |||\u2207V L\u0303(U, V ;S)|||2F \u2264 12 p \u00b5r\u03c3\u22171|||\u03a0\u03a6 (M + S \u2212M\u2217 \u2212 S\u2217) |||2F.\nIt remains to bound the term |||\u03a0\u03a6 (M + S \u2212M\u2217 \u2212 S\u2217) |||2F. Let \u2126\u2217o and \u2126 be the support of \u03a0\u03a6(S\u2217) and S respectively. We observe that\n|||\u03a0\u03a6 (M + S \u2212M\u2217 \u2212 S\u2217) |||2F = |||\u03a0\u2126\u2217o\\\u2126 (M \u2212M \u2217 \u2212 S\u2217) |||2F + |||\u03a0\u03a6\u2217c\u2229\u2126c\u2229\u03a6 (M \u2212M\u2217) |||2F\n\u2264 |||\u03a0\u2126\u2217o\\\u2126 (M \u2212M \u2217 \u2212 S\u2217) |||2F + |||\u03a0\u03a6 (M \u2212M\u2217) |||2F.\nIn the proof of Lemma 6, it is shown in (59) that\n|||\u03a0\u2126\u2217o\\\u2126 (M \u2212M \u2217 \u2212 S\u2217) |||2F \u2264\n3\n\u03b3 \u2212 1.5 |||\u03a0\u03a6(M \u2212M\u2217)|||2F.\nMoreover, following (60), we have that\n|||\u03a0\u03a6(M \u2212M\u2217)|||2F \u2264 2(1 + )p|||U\u03c0\u2217\u2206>V + \u2206UV >\u03c0\u2217 |||2F + 2p|||\u2206U |||2F|||\u2206V |||2F + 18p \u03c3\u22171|||\u2206U |||F|||\u2206V |||F \u2264 4(1 + )p|||M \u2212M\u2217|||2F + (6 + 4 )p|||\u2206U |||2F|||\u2206V |||2F + 18p \u03c3\u22171|||\u2206U |||F|||\u2206V |||F \u2264 4(1 + )p|||M \u2212M\u2217|||2F + (6 + 4 )p\u03b42 + 9p \u03c3\u22171\u03b4.\nWe thus finish proving our conclusion by combining all pieces and noticing that \u03b3 = 3 and \u2264 1/4."}, {"heading": "7 Proofs for Technical Lemmas", "text": "In this section, we prove several technical lemmas that are used in the proofs of our main theorems."}, {"heading": "7.1 Proof of Lemma 1", "text": "We observe that |||A|||op = sup\nx\u2208Sd1\u22121 sup y\u2208Sd2\u22121 x>Ay.\nWe denote the support of A by \u2126. For any x \u2208 Rd1 , y \u2208 Rd2 and \u03b2 > 0, we have\nx>Ay = \u2211\n(i,j)\u2208\u2126\nxiA(i,j)yj \u2264 \u2211\n(i,j)\u2208\u2126\n1 2 \u2016A\u2016\u221e(\u03b2\u22121x2i + \u03b2y2j )\n= 1\n2 \u2016A\u2016\u221e \u2211 i \u2211 j\u2208\u2126(i,\u00b7) \u03b2\u22121x2i + \u2211 j \u2211 i\u2208\u2126(\u00b7,j) \u03b2y2j  \u2264 1\n2 \u2016A\u2016\u221e\n( \u03b1d2\u03b2 \u22121\u2016x\u201622 + \u03b1d1\u03b2\u2016y\u201622 ) .\nIt is thus implied that |||A|||op \u2264 12\u03b1(\u03b2 \u22121d2 +\u03b2d1)\u2016A\u2016\u221e. Choosing \u03b2 = \u221a d2/d1 completes the proof."}, {"heading": "7.2 Proof of Lemma 9", "text": "We define a subspace K \u2286 Rd1\u00d7d2 as\nK := { X \u2223\u2223 X = L\u2217A> +BR\u2217> for some A \u2208 Rd2\u00d7r, B \u2208 Rd1\u00d7r } .\nLet \u03a0K be Euclidean projection onto K. Then according to Theorem 4.1 in [6], under our assumptions, for all matrices X \u2208 Rd1\u00d7d2 , inequality\np\u22121||| (\u03a0K\u03a0\u03a6\u03a0K \u2212 p\u03a0K)X|||F \u2264 |||X|||F (61)\nholds with probability at least 1\u2212 2d\u22123. In our setting, by restricting X = L\u2217A> + BR\u2217>, we have \u03a0KX = X. Therefore, (61) implies that |||\u03a0K\u03a0\u03a6X \u2212 pX|||F \u2264 p |||X|||F.\nFor |||\u03a0\u03a6X|||2F, we have\n|||\u03a0\u03a6X|||2F = \u3008\u3008\u03a0\u03a6X, \u03a0\u03a6X\u3009\u3009 = \u3008\u3008\u03a0\u03a6X, X\u3009\u3009 = \u3008\u3008\u03a0K\u03a0\u03a6X, X\u3009\u3009 \u2264 |||\u03a0K\u03a0\u03a6X|||F|||X|||F \u2264 p(1 + )|||X|||2F.\nOn the other hand, we have\n|||\u03a0\u03a6X|||2F = \u3008\u3008\u03a0K\u03a0\u03a6X, X\u3009\u3009 = \u3008\u3008\u03a0K\u03a0\u03a6X \u2212 pX + pX, X\u3009\u3009 = p|||X|||2F \u2212 \u3008\u3008X, \u2212\u03a0K\u03a0\u03a6X + pX\u3009\u3009 \u2265 p|||X|||2F \u2212 |||X|||F|||\u03a0K\u03a0\u03a6X \u2212 pX|||F \u2265 p(1\u2212 )|||X|||2F.\nCombining the above two inequalities, we complete the proof."}, {"heading": "7.3 Proof of Lemma 10", "text": "We observe that |\u03a6(i,\u00b7)| is a summation of d2 i.i.d. binary random variables with mean p and variance p(1\u2212 p). By Bernstein\u2019s inequality, for any i \u2208 [d1],\nPr [\u2223\u2223|\u03a6(i,\u00b7)| \u2212 pd2\u2223\u2223 \u2265 12pd2 ] \u2264 2 exp ( \u2212\n\u221212(pd2/2) 2\nd2p(1\u2212 p) + 13(pd2/2)\n) \u2264 2 exp ( \u2212 3\n28 pd2\n) .\nBy probabilistic union bound, we have\nPr [ sup i\u2208[d1] \u2223\u2223|\u03a6(i,\u00b7)| \u2212 pd2\u2223\u2223 \u2265 12pd2 ] \u2264 2d1 exp ( \u2212 3 28 pd2 ) \u2264 2d\u22121,\nwhere the last inequality holds by assuming p \u2265 563 log d d2 . The term |\u2126\u2217o(i,\u00b7)| is a summation of at most \u03b1d2 i.i.d. binary random variables with mean p and variance p(1\u2212 p). Again, applying Bernstein\u2019s inequality leads to\nPr [ |\u2126\u2217o(i,\u00b7)| \u2212 E [ |\u2126\u2217o(i,\u00b7)| ] \u2265 1\n2 p\u03b1d2\n] \u2264 exp ( \u2212 3\n28 p\u03b1d2\n) .\nAccordingly, by the assumption p \u2265 563 log d \u03b1d2 , we obtain\nPr [ sup i\u2208[d1] |\u2126\u2217o(i,\u00b7)| \u2212 pk \u2265 1 2 pk ] \u2264 d1 exp ( \u2212 3 28 p\u03b1d2 ) \u2264 d\u22121.\nThe proofs for |\u03a6(\u00b7,j)| and |\u2126\u2217o(\u00b7,j)| follow the same idea."}, {"heading": "7.4 Proof of Lemma 11", "text": "According to Lemma 3.2 in [8], under condition p \u2265 c1 \u00b5 log dd1\u2227d2 , for any fixed matrix A \u2208 R d1\u00d7d2 , we have\n|||A\u2212 p\u22121\u03a0\u03a6A|||op \u2264 c2\n\u221a d log d\np \u2016A\u2016\u221e,\nholds with probability at least 1\u2212O(d\u22123). Letting A be all-ones matrix, then we have that for all u \u2208 Rd1 , v \u2208 Rd2 , \u2211\n(i,j)\u2208\u03a6\nuivj \u2264 p\u2016u\u20161\u2016v\u20161 + c2 \u221a pd log d\u2016u\u20162\u2016v\u20162.\nWe find that |||\u03a0\u03a6(UV >)|||2F \u2264 \u2211\n(i,j)\u2208\u03a6\n\u2016U(i,\u00b7)\u201622\u2016V(j,\u00b7)\u201622\n\u2264 p|||U |||2F|||V |||2F + c2 \u221a pd log d \u221a\u2211 i\u2208[d1] \u2016U(i,\u00b7)\u201642 \u221a\u2211 j\u2208[d2] \u2016V(j,\u00b7)\u201642\n\u2264 p|||U |||2F|||V |||2F + c2 \u221a pd log d|||U |||F|||V |||F|||U |||2,\u221e|||V |||2,\u221e\n\u2264 p|||U |||2F|||V |||2F + c2\n\u221a p\u00b52r2d log d\nd1d2 |||U |||F|||V |||F.\nBy the assumption p & \u00b5 2r2 log d\n2(d1\u2227d2) , we finish proving (51).\nAccording to the proof of Lemma 10, if p \u2265 c log dd1\u2227d2 , with probability at least 1 \u2212 O(d \u22121), we have |\u03a6(i,\u00b7)| \u2264 32pd2 and |\u03a6(\u00b7,j)| \u2264 3 2pd1 for all i \u2208 [d1] and j \u2208 [d2]. Conditioning on this event, we have\n|||\u03a0\u03a6(Z)V |||2F = \u2211 i\u2208[d1] \u2211 k\u2208[r] \u3008(\u03a0\u03a6(Z))(i,\u00b7), H(\u00b7,k)\u30092\n\u2264 \u2211 i\u2208[d1] \u2211 k\u2208[r] \u2016(\u03a0\u03a6(Z))(i,\u00b7)\u201622 \u2211 j\u2208\u2126(i,\u00b7) V 2(j,k)\n= |||\u03a0\u03a6Z|||2F \u2211\nj\u2208\u2126(i,\u00b7)\n\u2016V(i,\u00b7)\u201622\n\u2264 |||\u03a0\u03a6Z|||2F 3\n2 pd2 \u00b7 |||V |||22,\u221e \u2264 2\u00b5rp|||\u03a0\u03a6Z|||2F.\nWe thus finish proving (52). Inequality (53) can be proved in the same way."}, {"heading": "7.5 Proof of Lemma 8", "text": "Recall that we let F := [U ;V ] and F\u03c0\u2217 := [U\u2217;V \u2217]Q for some matrix Q \u2208 Qr, which minimizes the following function\n|||F \u2212 [U\u2217;V \u2217]Q|||2F. (62)\nLet F \u2217 := [U\u2217;V \u2217]. Expanding the above term, we find that Q is the maximizer of \u3008\u3008F, F \u2217Q\u3009\u3009 = Tr(F>F \u2217Q). Suppose F>F \u2217 has SVD with form Q1\u039bQ>2 for Q1, Q2 \u2208 Qr. When the minimum\ndiagonal term of \u039b is positive, we conclude that the minimizer of (62) is unique and Q = Q2Q>1 . To prove this argument, we note that\nTr(F>F \u2217Q) = \u2211 i\u2208[r] \u039b(i,i)\u3008pi, qi\u3009,\nwhere pi is the i-th column of Q1 and qi is the i-th column of Q>Q2. Hence, Tr(F>F \u2217Q) \u2264\u2211 i\u2208[r] \u039b(i,i) and the equality holds if and only if pi = qi for all i \u2208 [r] since every \u039b(i,i) > 0. We have Q1 = Q>Q2 and thus finish proving the argument. Under our assumption |||F \u2212 F\u03c0\u2217 |||op < \u221a 2\u03c3\u2217r , for any nonzero vector u \u2208 Rr, we have\n\u2016F>F\u03c0\u2217u\u20162 \u2265 \u2016F>\u03c0\u2217F\u03c0\u2217u\u20162 \u2212 \u2016(F\u03c0\u2217 \u2212 F )>F\u03c0\u2217u\u20162 \u2265 ( \u221a 2\u03c3\u2217r \u2212 |||F\u03c0\u2217 \u2212 F |||op)|||F\u03c0\u2217u|||F > 0.\nIn the second step, we use the fact that the singular values of F\u03c0\u2217 are equal to the diagonal terms of \u221a\n2\u03a3\u22171/2. Hence, F>F\u03c0\u2217 has full rank. Furthermore, it implies that F>F \u2217 has full rank and only contains positive singular values.\nProceeding with the proved argument, we have\nF>F\u03c0\u2217 = Q1\u039bQ > 2 Q2Q > 1 = Q1\u039bQ > 1 ,\nwhich implies that F>F\u03c0\u2217 is symmetric. Accordingly, we have (F \u2212 F\u03c0\u2217)>F\u03c0\u2217 is also symmetric."}, {"heading": "Acknowledgment", "text": "Y. Chen acknowledges support from the School of Operations Research and Information Engineering, Cornell University."}, {"heading": "A Supporting Lemmas", "text": "In this section, we provide several technical lemmas used for proving our main results.\nLemma 12. For any (U\u2217, V \u2217) \u2208 E(M\u2217), U \u2208 Rd1\u00d7r and V \u2208 Rd2\u00d7r, we have\n|||UV > \u2212 U\u2217V \u2217>|||F \u2264 \u221a \u03c3\u22171(|||\u2206V |||F + |||\u2206U |||F) + |||\u2206U |||F|||\u2206V |||F,\nwhere \u2206U := U \u2212 U\u2217, \u2206V := V \u2212 V \u2217.\nProof. We observe that UV > \u2212 U\u2217V \u2217> = U\u2217\u2206>V + \u2206UV \u2217> + \u2206U\u2206>V . Hence,\n|||UV > \u2212 U\u2217V \u2217>|||F \u2264 |||U\u2217\u2206>V |||F + |||\u2206UV \u2217>|||F + |||\u2206U\u2206>V |||F \u2264 |||U\u2217|||op|||\u2206V |||F + |||V \u2217|||op|||\u2206U |||F + |||\u2206U |||F|||\u2206V |||F.\nFurthermore, assuming (U, V ) \u2208 U \u00d7 V, where U and V satisfy the conditions in (19), we have the next result.\nLemma 13. For any (i, j) \u2208 [d1]\u00d7 [d2], we have\n|(UV > \u2212 U\u2217V \u2217>)(i,j)| \u2264 3 \u221a \u00b5r\u03c3\u22171 d1 \u2016\u2206V (j,\u00b7)\u20162 + 3 \u221a \u00b5r\u03c3\u22171 d2 \u2016\u2206U(i,\u00b7)\u20162 (63)\nProof. We observe that\n|(UV > \u2212M\u2217)(i,j)| \u2264 |\u3008U\u2217(i,\u00b7), \u2206V (j,\u00b7)\u3009|+ |\u3008V \u2217 (j,\u00b7), \u2206U(i,\u00b7)\u3009|+ |\u3008\u2206U(i,\u00b7), \u2206V (j,\u00b7)\u3009| \u2264 \u221a \u00b5r\u03c3\u22171 d1 \u2016\u2206V (j,\u00b7)\u20162 + \u221a \u00b5r\u03c3\u22171 d2 \u2016\u2206U(i,\u00b7)\u20162 + 1 2 |||\u2206U |||2,\u221e\u2016\u2206V (j,\u00b7)\u20162 + 1 2 |||\u2206V |||2,\u221e\u2016\u2206U(i,\u00b7)\u20162.\nBy noticing that\n|||\u2206U |||2,\u221e \u2264 |||U\u2217|||2,\u221e + |||U |||2,\u221e \u2264 3 \u221a \u00b5r\u03c3\u22171 d1 , |||\u2206V |||2,\u221e \u2264 |||V \u2217|||2,\u221e + |||V |||2,\u221e \u2264 3 \u221a \u00b5r\u03c3\u22171 d2 ,\nwe complete the proof.\nLemma 13 can be used to prove the following result.\nLemma 14. For any \u03b1 \u2208 [0, 1], suppose \u2126 \u2286 [d1] \u00d7 [d2] satisfies |\u2126(i,\u00b7)| \u2264 \u03b1d2 for all i \u2208 [d1] and |\u2126(\u00b7,j)| \u2264 \u03b1d1 for all j \u2208 [d2]. Then we have\n|||\u03a0\u2126(UV > \u2212 U\u2217V \u2217>)|||2F \u2264 18\u03b1\u00b5r\u03c3\u22171(|||\u2206V |||2F + |||\u2206U |||2F).\nProof. Using Lemma 13 for bounding each entry of UV > \u2212 U\u2217V \u2217>, we have that\n|||\u03a0\u2126(UV > \u2212 U\u2217V \u2217>)|||2F \u2264 \u2211\n(i,j)\u2208\u2126\n|(UV > \u2212 U\u2217V \u2217>)(i,j)|2\n\u2264 \u2211\n(i,j)\u2208\u2126\n18\u00b5r\u03c3\u22171 d1 \u2016\u2206V (j,\u00b7)\u201622 + 18\u00b5r\u03c3\u22171 d2 \u2016\u2206U(i,\u00b7)\u201622\n\u2264 \u2211 j \u2211 i\u2208\u2126(\u00b7,j) 18\u00b5r\u03c3\u22171 d1 \u2016\u2206V (j,\u00b7)\u201622 + \u2211 i \u2211 j\u2208\u2126(i,\u00b7) 18\u00b5r\u03c3\u22171 d2 \u2016\u2206U(i,\u00b7)\u201622 \u2264 18\u03b1\u00b5r\u03c3\u22171(|||\u2206V |||2F + |||\u2206U |||2F).\nDenote the i-th largest singular value of matrix M by \u03c3i(M).\nLemma 15 (Lemma 5.14 in [25]). Let M1,M2 \u2208 Rd1\u00d7d2 be two rank r matrices. Suppose they have SVDs M1 = L1\u03a31R>1 and M2 = L2\u03a32R > 2 . Suppose |||M1 \u2212M2|||op \u2264 12\u03c3r(M1). Then we have\nd2(L2\u03a3 1/2 2 , R2\u03a3 1/2 2 ;L1\u03a3 1/2 1 , R1\u03a3 1/2 1 ) \u2264 2\u221a 2\u2212 1 |||M2 \u2212M1|||2F \u03c3r(M1) ."}, {"heading": "B Parameter Settings for FB Separation Experiments", "text": "We approximate the FB separation problem by the RPCA framework with r = 10, \u03b1 = 0.2, \u00b5 = 10. Our algorithmic parameters are set as \u03b3 = 1, \u03b7 = 1/(2\u03c3\u0302\u22171), where \u03c3\u0302\u22171 is an estimate of \u03c3\u22171 obtained from the initial SVD. The parameters of AltProj are kept as provided in the default setting. For IALM, we use the tradeoff paramter \u03bb = 1/ \u221a d1, where d1 is the number of pixels in each frame (the number of rows in Y ). Note that both IALM and AltProj use the stopping criterion\n\u2016Y \u2212Mt \u2212 St\u2016F /\u2016Y \u2016F \u2264 10\u22123.\nOur algorithm never explicitly forms the d1-by-d2 matrixMt = UtV >t , which is favored in large scale problems, but also renders the above criterion inapplicable. Instead, we use the following stopping criterion\n|||Ut+1 \u2212 Ut|||2F + |||Vt+1 \u2212 Vt|||2F |||Ut|||2F + |||Vt|||2F \u2264 4\u00d7 10\u22124.\nThis rule checks whether the iterates corresponding to low-rank factors becomes stable. In fact, our stopping criterion seems more natural and practical because in most real applications, matrix Y cannot be strictly decomposed into low-rank M and sparse S that satisfy Y = M + S. Instead of forcing M + S to be close to Y , our rule relies on seeking a robust subspace that captures the most variance of Y ."}], "references": [{"title": "Tensor decompositions for learning latent variable models", "author": ["Animashree Anandkumar", "Rong Ge", "Daniel Hsu", "Sham M. Kakade", "Matus Telgarsky"], "venue": "In The Journal of Machine Learning Research", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2014}, {"title": "Statistical guarantees for the EM algorithm: From population to sample-based analysis", "author": ["Sivaraman Balakrishnan", "Martin J. Wainwright", "Bin Yu"], "venue": "In arXiv preprint arXiv:1408.2156", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2014}, {"title": "Tighter low-rank approximation via sampling the leveraged element", "author": ["Srinadh Bhojanapalli", "Prateek Jain", "Sujay Sanghavi"], "venue": "In Proceedings of the Twenty-Sixth Annual ACM-SIAM Symposium on Discrete Algorithms", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2015}, {"title": "Dropping convexity for faster semi-definite optimization", "author": ["Srinadh Bhojanapalli", "Anastasios Kyrillidis", "Sujay Sanghavi"], "venue": "In arXiv preprint arXiv:1509.03917", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2015}, {"title": "Phase retrieval via Wirtinger flow: Theory and algorithms", "author": ["Emmanuel J. Cand\u00e8s", "Xiaodong Li", "Mahdi Soltanolkotabi"], "venue": "In IEEE Transactions on Information Theory", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2015}, {"title": "Exact matrix completion via convex optimization", "author": ["Emmanuel J. Cand\u00e8s", "Benjamin Recht"], "venue": "In Foundations of Computational mathematics", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2009}, {"title": "The power of convex relaxation: Near-optimal matrix completion", "author": ["Emmanuel J. Cand\u00e8s", "Terence Tao"], "venue": "In IEEE Transactions on Information Theory", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2010}, {"title": "Robust principal component analysis?", "author": ["Emmanuel J. Cand\u00e8s", "Xiaodong Li", "Yi Ma", "John Wright"], "venue": "In Journal of the ACM (JACM)", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2011}, {"title": "Ranksparsity incoherence for matrix decomposition", "author": ["Venkat Chandrasekaran", "Sujay Sanghavi", "Pablo A. Parrilo", "Alan S. Willsky"], "venue": "In SIAM Journal on Optimization", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2011}, {"title": "Incoherence-Optimal Matrix Completion", "author": ["Yudong Chen"], "venue": "In IEEE Transactions on Information Theory", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2015}, {"title": "Fast low-rank estimation by projected gradient descent: General statistical and algorithmic guarantees", "author": ["Yudong Chen", "Martin J. Wainwright"], "venue": "In arXiv preprint arXiv:1509.03025", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2015}, {"title": "Low-rank Matrix Recovery from Errors and Erasures", "author": ["Yudong Chen", "Ali Jalali", "Sujay Sanghavi", "Constantine Caramanis"], "venue": "In IEEE Transactions on Information Theory", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2013}, {"title": "Solving random quadratic systems of equations is nearly as easy as solving linear systems", "author": ["Yuxin Chen", "Emmanuel J. Cand\u00e8s"], "venue": "In Advances in Neural Information Processing Systems", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2015}, {"title": "Low rank approximation and regression in input sparsity time", "author": ["Kenneth L. Clarkson", "David P. Woodruff"], "venue": "In Proceedings of the forty-fifth annual ACM symposium on Theory of computing", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2013}, {"title": "Fast Monte-Carlo algorithms for finding low-rank approximations", "author": ["Alan Frieze", "Ravi Kannan", "Santosh Vempala"], "venue": "In Journal of the ACM (JACM)", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2004}, {"title": "Low-Rank and Sparse Structure Pursuit via Alternating Minimization", "author": ["Quanquan Gu", "Zhaoran Wang", "Han Liu"], "venue": "In Proceedings of the 19th International Conference on Artificial Intelligence and Statistics", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2016}, {"title": "Understanding alternating minimization for matrix completion", "author": ["Moritz Hardt"], "venue": "IEEE 55th Annual Symposium on Foundations of Computer Science (FOCS). IEEE", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2014}, {"title": "Robust matrix decomposition with sparse corruptions", "author": ["Daniel Hsu", "Sham M. Kakade", "Tong Zhang"], "venue": "In IEEE Transactions on Information Theory", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2011}, {"title": "Low-rank matrix completion using alternating minimization", "author": ["Prateek Jain", "Praneeth Netrapalli", "Sujay Sanghavi"], "venue": "In Proceedings of the forty-fifth annual ACM symposium on Theory of computing", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2013}, {"title": "The Augmented Lagrange Multiplier Method for Exact Recovery of Corrupted Low-Rank Matrices", "author": ["Zhouchen Lin", "Minming Chen", "Yi Ma"], "venue": "In Arxiv preprint arxiv:1009.5055v3", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2013}, {"title": "Phase Retrieval using Alternating Minimization", "author": ["Praneeth Netrapalli", "Prateek Jain", "Sujay Sanghavi"], "venue": "In Arxiv preprint arxiv:arXiv:1306.0160", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2013}, {"title": "Non-convex robust PCA", "author": ["Praneeth Netrapalli", "UN Niranjan", "Sujay Sanghavi", "Animashree Anandkumar", "Prateek Jain"], "venue": "In Advances in Neural Information Processing Systems", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2014}, {"title": "When Are Nonconvex Problems Not Scary?", "author": ["Ju Sun", "Qing Qu", "John Wright"], "venue": "In arXiv preprint arXiv:1510.06096", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2015}, {"title": "Guaranteed matrix completion via nonconvex factorization", "author": ["Ruoyu Sun", "Zhi-Quan Luo"], "venue": "IEEE 56th Annual Symposium on Foundations of Computer Science (FOCS). IEEE", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2015}, {"title": "Low-rank solutions of linear matrix equations via procrustes flow", "author": ["Stephen Tu", "Ross Boczar", "Mahdi Soltanolkotabi", "Benjamin Recht"], "venue": "In arXiv preprint arXiv:1507.03566", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2015}, {"title": "High Dimensional EM Algorithm: Statistical Optimization and Asymptotic Normality", "author": ["Zhaoran Wang", "Quanquan Gu", "Yang Ning", "Han Liu"], "venue": "In Advances in Neural Information Processing Systems", "citeRegEx": "26", "shortCiteRegEx": "26", "year": 2015}, {"title": "Robust PCA via Outlier Pursuit", "author": ["Huan Xu", "Constantine Caramanis", "Sujay Sanghavi"], "venue": "In IEEE Transactions on Information Theory", "citeRegEx": "27", "shortCiteRegEx": "27", "year": 2012}, {"title": "Regularized EM Algorithms: A Unified Framework and Statistical Guarantees", "author": ["Xinyang Yi", "Constantine Caramanis"], "venue": "In Advances in Neural Information Processing Systems", "citeRegEx": "28", "shortCiteRegEx": "28", "year": 2015}, {"title": "Provable Non-convex Phase Retrieval with Outliers: Median Truncated Wirtinger Flow", "author": ["Huishuai Zhang", "Yuejie Chi", "Yingbin Liang"], "venue": "In arXiv preprint arXiv:1603.03805", "citeRegEx": "29", "shortCiteRegEx": "29", "year": 2016}, {"title": "A Nonconvex Optimization Framework for Low Rank Matrix Estimation", "author": ["Tuo Zhao", "Zhaoran Wang", "Han Liu"], "venue": "In Advances in Neural Information Processing Systems", "citeRegEx": "30", "shortCiteRegEx": "30", "year": 2015}], "referenceMentions": [{"referenceID": 7, "context": ", [8, 27]), the running times range from O(r2d2) to O(d3)1 and hence are significantly worse than SVD.", "startOffset": 2, "endOffset": 9}, {"referenceID": 26, "context": ", [8, 27]), the running times range from O(r2d2) to O(d3)1 and hence are significantly worse than SVD.", "startOffset": 2, "endOffset": 9}, {"referenceID": 14, "context": ", [15, 14, 3]) seems unable to guarantee robustness to outliers or missing data.", "startOffset": 2, "endOffset": 13}, {"referenceID": 13, "context": ", [15, 14, 3]) seems unable to guarantee robustness to outliers or missing data.", "startOffset": 2, "endOffset": 13}, {"referenceID": 2, "context": ", [15, 14, 3]) seems unable to guarantee robustness to outliers or missing data.", "startOffset": 2, "endOffset": 13}, {"referenceID": 8, "context": "Provable solutions for this model are first provided in the works of [9] and [8].", "startOffset": 69, "endOffset": 72}, {"referenceID": 7, "context": "Provable solutions for this model are first provided in the works of [9] and [8].", "startOffset": 77, "endOffset": 80}, {"referenceID": 7, "context": "Despite analyzing the same method, the corruption models in [8] and [9] differ.", "startOffset": 60, "endOffset": 63}, {"referenceID": 8, "context": "Despite analyzing the same method, the corruption models in [8] and [9] differ.", "startOffset": 68, "endOffset": 71}, {"referenceID": 7, "context": "In [8], the authors consider the setting where the entries of M\u2217 are corrupted at random with probability \u03b1.", "startOffset": 3, "endOffset": 6}, {"referenceID": 8, "context": "Work in [9] considers a deterministic corruption model, where nonzero entries of S\u2217 can have arbitrary position, but the sparsity of each row and column does not exceed \u03b1d.", "startOffset": 8, "endOffset": 11}, {"referenceID": 11, "context": "This was subsequently further improved to \u03b1 = O(1/(\u03bcr)), which is in fact optimal [12, 18].", "startOffset": 82, "endOffset": 90}, {"referenceID": 17, "context": "This was subsequently further improved to \u03b1 = O(1/(\u03bcr)), which is in fact optimal [12, 18].", "startOffset": 82, "endOffset": 90}, {"referenceID": 19, "context": "The state-of-the-art solver [20] for (1) has time complexity O(d3/\u03b5) to achieve error \u03b5, and is thus much slower than SVD, and prohibitive for even modest values of d.", "startOffset": 28, "endOffset": 32}, {"referenceID": 21, "context": "Work in [22] considers the deterministic corruption model, and improves this running time without sacrificing the robustness guarantee on \u03b1.", "startOffset": 8, "endOffset": 12}, {"referenceID": 18, "context": "[19, 17, 16]) and gradient descent (see e.", "startOffset": 0, "endOffset": 12}, {"referenceID": 16, "context": "[19, 17, 16]) and gradient descent (see e.", "startOffset": 0, "endOffset": 12}, {"referenceID": 15, "context": "[19, 17, 16]) and gradient descent (see e.", "startOffset": 0, "endOffset": 12}, {"referenceID": 3, "context": "[4, 11, 24, 25, 30, 31]).", "startOffset": 0, "endOffset": 23}, {"referenceID": 10, "context": "[4, 11, 24, 25, 30, 31]).", "startOffset": 0, "endOffset": 23}, {"referenceID": 23, "context": "[4, 11, 24, 25, 30, 31]).", "startOffset": 0, "endOffset": 23}, {"referenceID": 24, "context": "[4, 11, 24, 25, 30, 31]).", "startOffset": 0, "endOffset": 23}, {"referenceID": 29, "context": "[4, 11, 24, 25, 30, 31]).", "startOffset": 0, "endOffset": 23}, {"referenceID": 10, "context": "One exception is [11], where the authors analyze a row-wise `1 projection method for recovering S\u2217.", "startOffset": 17, "endOffset": 21}, {"referenceID": 15, "context": "Another exception is work [16], which analyzes alternating minimization plus an overall sparse projection.", "startOffset": 26, "endOffset": 30}, {"referenceID": 4, "context": "It is worth mentioning other works that obtain provable guarantees of non-convex algorithms or problems including phase retrieval [5, 13, 29], EM algorithms [2, 26, 28], tensor decompositions [1] and second order method [23].", "startOffset": 130, "endOffset": 141}, {"referenceID": 12, "context": "It is worth mentioning other works that obtain provable guarantees of non-convex algorithms or problems including phase retrieval [5, 13, 29], EM algorithms [2, 26, 28], tensor decompositions [1] and second order method [23].", "startOffset": 130, "endOffset": 141}, {"referenceID": 28, "context": "It is worth mentioning other works that obtain provable guarantees of non-convex algorithms or problems including phase retrieval [5, 13, 29], EM algorithms [2, 26, 28], tensor decompositions [1] and second order method [23].", "startOffset": 130, "endOffset": 141}, {"referenceID": 1, "context": "It is worth mentioning other works that obtain provable guarantees of non-convex algorithms or problems including phase retrieval [5, 13, 29], EM algorithms [2, 26, 28], tensor decompositions [1] and second order method [23].", "startOffset": 157, "endOffset": 168}, {"referenceID": 25, "context": "It is worth mentioning other works that obtain provable guarantees of non-convex algorithms or problems including phase retrieval [5, 13, 29], EM algorithms [2, 26, 28], tensor decompositions [1] and second order method [23].", "startOffset": 157, "endOffset": 168}, {"referenceID": 27, "context": "It is worth mentioning other works that obtain provable guarantees of non-convex algorithms or problems including phase retrieval [5, 13, 29], EM algorithms [2, 26, 28], tensor decompositions [1] and second order method [23].", "startOffset": 157, "endOffset": 168}, {"referenceID": 0, "context": "It is worth mentioning other works that obtain provable guarantees of non-convex algorithms or problems including phase retrieval [5, 13, 29], EM algorithms [2, 26, 28], tensor decompositions [1] and second order method [23].", "startOffset": 192, "endOffset": 195}, {"referenceID": 22, "context": "It is worth mentioning other works that obtain provable guarantees of non-convex algorithms or problems including phase retrieval [5, 13, 29], EM algorithms [2, 26, 28], tensor decompositions [1] and second order method [23].", "startOffset": 220, "endOffset": 224}, {"referenceID": 10, "context": "An immediate corollary of the above result provides a guarantee for exact matrix completion, with general rectangular matrices, usingO(\u03bc2r2d log d) observed entries andO(\u03bc3r4d log d log(1/\u03b5)) time, thereby improving on existing results in [11, 24].", "startOffset": 239, "endOffset": 247}, {"referenceID": 23, "context": "An immediate corollary of the above result provides a guarantee for exact matrix completion, with general rectangular matrices, usingO(\u03bc2r2d log d) observed entries andO(\u03bc3r4d log d log(1/\u03b5)) time, thereby improving on existing results in [11, 24].", "startOffset": 239, "endOffset": 247}, {"referenceID": 21, "context": "We note that when \u03ba = O(1), our algorithm is orderwise faster than the AltProj algorithm in [22], which has running time O(r2d2 log(1/\u03b5)).", "startOffset": 92, "endOffset": 96}, {"referenceID": 11, "context": "This is worse by a factor \u221a r compared to the optimal statistical guarantee 1/(\u03bcr) obtained in [12, 18, 22].", "startOffset": 95, "endOffset": 107}, {"referenceID": 17, "context": "This is worse by a factor \u221a r compared to the optimal statistical guarantee 1/(\u03bcr) obtained in [12, 18, 22].", "startOffset": 95, "endOffset": 107}, {"referenceID": 21, "context": "This is worse by a factor \u221a r compared to the optimal statistical guarantee 1/(\u03bcr) obtained in [12, 18, 22].", "startOffset": 95, "endOffset": 107}, {"referenceID": 6, "context": ", [7]).", "startOffset": 2, "endOffset": 5}, {"referenceID": 10, "context": ", observations needed) matches that of completing a positive semidefinite (PSD) matrix by gradient descent as shown in [11], and is better than the non-convex matrix completion algorithms in [19] and [24].", "startOffset": 119, "endOffset": 123}, {"referenceID": 18, "context": ", observations needed) matches that of completing a positive semidefinite (PSD) matrix by gradient descent as shown in [11], and is better than the non-convex matrix completion algorithms in [19] and [24].", "startOffset": 191, "endOffset": 195}, {"referenceID": 23, "context": ", observations needed) matches that of completing a positive semidefinite (PSD) matrix by gradient descent as shown in [11], and is better than the non-convex matrix completion algorithms in [19] and [24].", "startOffset": 200, "endOffset": 204}, {"referenceID": 6, "context": "It is known that that any algorithm for solving exact matrix completion must have sample size \u03a9(\u03bcrd log d) [7], and a nearly tight upper bound O(\u03bcrd log d) is obtained in [10] by convex relaxation.", "startOffset": 107, "endOffset": 110}, {"referenceID": 9, "context": "It is known that that any algorithm for solving exact matrix completion must have sample size \u03a9(\u03bcrd log d) [7], and a nearly tight upper bound O(\u03bcrd log d) is obtained in [10] by convex relaxation.", "startOffset": 171, "endOffset": 175}, {"referenceID": 19, "context": "In this section, we provide numerical results and compare the proposed algorithms with existing methods, including the inexact augmented lagrange multiplier (IALM) approach [20] for solving the convex relaxation (1) and the alternating projection (AltProj) algorithm proposed in [21].", "startOffset": 173, "endOffset": 177}, {"referenceID": 20, "context": "In this section, we provide numerical results and compare the proposed algorithms with existing methods, including the inexact augmented lagrange multiplier (IALM) approach [20] for solving the convex relaxation (1) and the alternating projection (AltProj) algorithm proposed in [21].", "startOffset": 279, "endOffset": 283}, {"referenceID": 21, "context": "2, AltProj [22], and IALM [20].", "startOffset": 11, "endOffset": 15}, {"referenceID": 19, "context": "2, AltProj [22], and IALM [20].", "startOffset": 26, "endOffset": 30}, {"referenceID": 24, "context": "14 in [25] (we provide it as Lemma 15 for the sake of completeness), we obtain", "startOffset": 6, "endOffset": 10}, {"referenceID": 10, "context": "2 Proof of Theorem 2 We essentially follow the general framework developed in [11] for analyzing the behaviors of gradient descent in factorized low-rank optimization.", "startOffset": 78, "endOffset": 82}, {"referenceID": 10, "context": "But it is worth to note that [11] only studies the symmetric and positive semidefinite setting, while we avoid such constraint on M\u2217.", "startOffset": 29, "endOffset": 33}, {"referenceID": 24, "context": "The techniques for analyzing general asymmetric matrix in factorized space is inspired by the recent work [25] on solving low-rank matrix equations.", "startOffset": 106, "endOffset": 110}, {"referenceID": 9, "context": "For the second term in (26), we use the following lemma proved in [10].", "startOffset": 66, "endOffset": 70}, {"referenceID": 9, "context": "Lemma 5 (Lemma 2 in [10]).", "startOffset": 20, "endOffset": 24}, {"referenceID": 10, "context": "For the cross term, by the following result, proved in [11] (we also provide a proof in Section 7.", "startOffset": 55, "endOffset": 59}, {"referenceID": 5, "context": "1 in [6], under our assumptions, for all matrices X \u2208 Rd1\u00d7d2 , inequality p\u22121||| (\u03a0K\u03a0\u03a6\u03a0K \u2212 p\u03a0K)X|||F \u2264 |||X|||F (61) holds with probability at least 1\u2212 2d\u22123.", "startOffset": 5, "endOffset": 8}, {"referenceID": 7, "context": "2 in [8], under condition p \u2265 c1 \u03bc log d d1\u2227d2 , for any fixed matrix A \u2208 R d1\u00d7d2 , we have |||A\u2212 p\u03a0\u03a6A|||op \u2264 c2 \u221a d log d p \u2016A\u2016\u221e, holds with probability at least 1\u2212O(d\u22123).", "startOffset": 5, "endOffset": 8}], "year": 2016, "abstractText": "We consider the problem of Robust PCA in the the fully and partially observed settings. Without corruptions, this is the well-known matrix completion problem. From a statistical standpoint this problem has been recently well-studied, and conditions on when recovery is possible (how many observations do we need, how many corruptions can we tolerate) via polynomialtime algorithms is by now understood. This paper presents and analyzes a non-convex optimization approach that greatly reduces the computational complexity of the above problems, compared to the best available algorithms. In particular, in the fully observed case, with r denoting rank and d dimension, we reduce the complexity from O(rd log(1/\u03b5)) to O(rd log(1/\u03b5)) \u2013 a big savings when the rank is big. For the partially observed case, we show the complexity of our algorithm is no more than O(rd log d log(1/\u03b5)). Not only is this the best-known run-time for a provable algorithm under partial observation, but in the setting where r is small compared to d, it also allows for near-linear-in-d run-time that can be exploited in the fully-observed case as well, by simply running our algorithm on a subset of the observations.", "creator": "LaTeX with hyperref package"}}}