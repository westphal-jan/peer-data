{"id": "1605.07852", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "25-May-2016", "title": "SS4MCT: A Statistical Stemmer for Morphologically Complex Texts", "abstract": "There have been multiple attempts to resolve the various formation matching problem in information retrieval. Stemming is a common strategy to this end. Among many approaches for stemming, statistical stemming has shown to be effective in a few number of languages, particularly those highly inflected ones. Common statistical approaches heavily relies on string similarity in terms of prefix and suffix matching. In this paper we propose a method that is able to find any popular affix in any position of a word; specifically finding infixes is a required task in Persian, Arabic, and Malay particularly in irregular formations and is a future challenge in informal texts. The proposed method aims at finding a number of transformation rules for expanding a word with its inflectional/derivation formations. Our experimental results on CLEF 2008 and CLEF 2009 English-Persian CLIR tasks indicate that the proposed method significantly outperforms all the baselines in terms of MAP. These results suggest that such a approach is not applicable to English.\n\n\n\nTo achieve optimal alignment of the text, our paper provides some useful results:\nIt is expected that each letter in the paper was not followed with other words in the paper except in a few cases of ambiguous syntax. Thus the number of words in the paper in this paper is extremely small, which could result in missing words in several ways:\nThe number of words in the paper was about 2,000 words. The authors note that these values can vary from case to case, depending on the location and the type of sentence that is used in the document.\nThe number of words in the paper was about 2,000 words. The authors note that these values can vary from case to case, depending on the location and the type of sentence that is used in the document. However, the authors note that these values can vary from case to case, depending on the location and the type of sentence that is used in the document. We use the following syntax:\nThere is no syntactic difference between a translation to English or any other language in the paper. The difference is in the way that the word is parsed and determined, and a given number of times, depending on the location.\nA particular type of letter is used in the paper.\nThe letter is parsed and determined, and a given number of times, depending on the location. There is no syntactic difference between a translation to English or any other language in the paper.\nThe first character in the document is a letter that is followed by a number of words in the paper.", "histories": [["v1", "Wed, 25 May 2016 12:25:26 GMT  (1026kb,D)", "https://arxiv.org/abs/1605.07852v1", null], ["v2", "Mon, 20 Jun 2016 21:37:19 GMT  (1019kb,D)", "http://arxiv.org/abs/1605.07852v2", null]], "reviews": [], "SUBJECTS": "cs.IR cs.CL", "authors": ["javid dadashkarimi", "hossein nasr esfahani", "heshaam faili", "azadeh shakery"], "accepted": false, "id": "1605.07852"}, "pdf": {"name": "1605.07852.pdf", "metadata": {"source": "CRF", "title": "SS4MCT: A Statistical Stemmer for Morphologically Complex Texts", "authors": ["Javid Dadashkarimi", "Hossein Nasr Esfahani", "Heshaam Faili"], "emails": ["dadashkarimi@ut.ac.ir", "h_nasr@ut.ac.ir", "shakery@ut.ac.ir", "hfaili@ut.ac.ir"], "sections": [{"heading": null, "text": "Keywords: Stemming, infix recognition, inflectional/derivation formation matching, dictionary-based cross-language information retrieval."}, {"heading": "1 Introduction", "text": "Uniforming different inflections of words is a required task in a wide range of text mining algorithms, including, but not limited to text classification, text clustering, document retrieval, and language modeling [1]. Stemming has been considered as a common approach for this goal in several studies [9,1]. Stemmers usually remove affixes from the words to present them in the form of their morphological roots. Conventional rule-based stemmers tailor the linguistic knowledge of experts. On the other hand, statistical stemmers provide languageindependent approaches which generally group related words based on various string-similarity measures. Such approaches often involve n-grams; equivalence classes can be formed from words that share the same properties: word-initial letter n-grams, common n-grams throughout the word, or by refining these classes ar X iv :1\n60 5.\n07 85\n2v 2\n[ cs\n.I R\n] 2\n0 Ju\nn 20\n16\nwith clustering techniques. This kind of statistical stemming has been shown to be effective for many languages, including English, Turkish, and Malay. For example, Bhat introduced a method for Kannada where the similarity of two words is determined by three distance measures based on prefix and suffix matching and the first mismatch point in the words [2].\nDefining precise rules for morphologically complex texts, especially for the purpose of infix removal is sometimes impossible [5]. Informal/irregular forms\nusually do not obey the conventional rules in the languages. For instance, \u2018k\u0302hunh\u2019 (home) is a frequent form for \u2018k\u0302hanh\u2019 in Persian conversations or \u2018goood \u2019 and \u2018good \u2019 are used interchangeably in English tweets.\nIn this paper, we propose a statistical technique for finding inflectional and derivation formations of words. To this end, we introduce an unsupervised method to cluster all morphological variants of a word. The proposed algorithm learns linguistic patterns to match a word and its morphological variants based on a given large collection of documents, which is readily available on the Web. A linguistic pattern captures a transformation rule between a word and its morphological variant. The extracted rules indicate which letters in which positions of a word should be modified. Affix characters, positions of the characters, operations on the characters based on the minimum edit distance (MED) algorithm (i.e., insertion or deletion) [10], and part-of-speech (POS) tag of the input word are the attributes of a rule. Our algorithm assigns a score to each rule, indicating its confidence. The higher the frequency of a rule in the input collection, the higher the confidence value of that rule. Finally, a small subset of the obtained rules are selected based on their scores and a learned threshold as valid rules. We demonstrate that using this subset for query expansion can significantly improve English-Persian CLIR performance compared to comprehensive baselines.\nIn Section 2 we elaborate on the subject, in Section 3 we assess its quality in an IR task, and in Section 4 we conclude the paper."}, {"heading": "2 SS4MCT: A Statistical Stemmer for Morphologically Complex Texts", "text": "In this section we propose an unsupervised method for finding inflections in a language. To this end, we first introduce a transformation rule which is an edit path transforming the word w into w\u2032 based on a number of actions. Our goal is to estimate the probability of each transformation rule P (R) and compute the likelihood of generating inflections for a given term (i.e., p(w\u2032|w)). In Section 2.1 and Section 2.2 we introduce the proposed method in more details and in Section 2.3 we propose an evaluation framework for the method."}, {"heading": "2.1 Transformation Rules", "text": "Each transformation rule contains a number of actions transforming a word into an inflection. If two terms are k points distant from each other, the rule that transforms the input term to the output term contains k actions and the maximum likelihood POS tag of the input word. Each action consists of the following attributes: c, the character in difference, p, the position of that character (begin,\nmiddle, and end), and o, the corresponding operation on the character in the MED algorithm (deletion or insertion). Intuitively we define a few general positions for affixes to prevent sparsity of the rules. A substitution operation can be replaced by a couple of insertion/deletion operations; therefore we ignore the substitution operation. Table 1 shows a number of examples for the rules."}, {"heading": "2.2 Probability of the Rules", "text": "To compute the probability of generating an inflection for a given term (i.e., p(w\u2032|w)) we can compute the transformation rule (R) between w and w\u2032 and estimate p(w\u2032|w) by p(R). To compute the probability of each rule we use a large collection of words extracted from a document collection. For each pair of words in the collection (w and w\u2032), we compute the rule for transforming w into w\u2032 and count the number of times this rule has happened in the collection. The higher the occurrences of a rule, the more likely it is to be a valid one. Finally we estimate the rules probability with maximum likelihood estimator."}, {"heading": "2.3 How to Evaluate the Algorithm", "text": "In this section we provide a framework for the stemming algorithm to evaluate its effectiveness. We use dictionary-based cross-lingual information retrieval (CLIR) to this end. In highly inflected languages, bilingual dictionaries contain only original forms of the words. Therefore, in dictionary-based CLIR, retrieval systems are obliged either to stem documents and queries, or to leave them intact [8,4,12], or expand the query with inflections. We opted the query expansion approach which is a widely used approach to compensate the shortage of inflections [9,3,5]. We used the following probabilistic framework to this end [5]:\np(ci,j |qi) = \u2211 i\u2032 6=i ( |ci\u2032 |\u2211 j\u2032=1 p(ci,j , ci\u2032,j\u2032) + |c\u2032i\u2032 |\u2211 j\u2032=1 p(ci,j , c \u2032 i\u2032,j\u2032) ) , p(c\u2032i,j |qi) = \u2211 i\u20326=i |ci\u2032|\u2211 j\u2032=1 p(c\u2032i,j , ci\u2032,j\u2032). (1)\nwhere qi is a query term and ci is the set of translation candidates provided in a bilingual dictionary for qi. c \u2032 i is the set of the most probable inflections of the words appeared in ci selected by a tuned threshold. Then, we compute\nthe translation probability of ci,j or c \u2032 i,j for the given qi. To avoid adding noisy terms, we only compute the joint probabilities between either a pair of translation candidates from the dictionary (ci,j and ci\u2032,j\u2032) or a pair of a candidate from the dictionary and an inflection from the collection (ci,j and c \u2032 i\u2032,j\u2032) [5].\nOur goal is to find c\u0304i using the proposed SS4MCT (i.e. set of top-ranked c\u0304i\u2032,j\u2032 according to pt(c\u0304i\u2032,j\u2032 |ci,j)) and then evaluate its impact on the performance of the CLIR task. Figure 1 shows the whole process of extracting rules (off-line part) and the evaluation framework (on-line part)."}, {"heading": "3 Experiments", "text": ""}, {"heading": "3.1 Experimental Setup", "text": "The statistics of the collection used for both rule extraction and evaluation is provided in Table 2. We employed the statistical language modeling framework with Kullback-Leibler similarity measure of Lemur toolkit for our retrieval task. Dirichlet Prior is selected as our document smoothing strategy. Top 30 documents are used for the mixture pseudo-relevance feedback algorithm. Queries are expanded by the top 50 terms generated by the feedback model [14,6]. We removed Persian stop words from the queries and documents [4,5]. We used STeP1 [13] in our stemming process in Persian. We also stem the source English queries in all experiments with the Porter stemmer. We use Google EnglishPersian dictionary 1 as the translation resource. Dadashkarimi et al., demonstrated that Google has better coverage compared to other English-Persian dictionaries [5]. We have exploited 40 Persian POS tags in our experiments.2 The\n1 https://translate.google.com/#en/fa/ 2 http://ece.ut.ac.ir/dbrg/bijankhan/\nretrieval results are mainly evaluated by Mean Average Precision (MAP) over top 1000 retrieved documents. Significance tests are computed using two-tailed paired t-test with 95% confidence. Precision at top 5 documents (P@5) and top 10 documents (P@10) are also reported."}, {"heading": "3.2 Comparing Different Morphological Processing Methods", "text": "In this section we aim at evaluating the proposed SS4MCT method. To this end we compare the proposed SS4MCT with a number of dictionary-based CLIR methods; the 5-gram truncation method (SPLIT) proposed in [11], rule-based query expansion (RBQE) based on inflectional/derivation rules from Farazzin machine translator3, and the STeP1 stemmer [13] are the morphological processing approaches for the retrieval system. On the other hand, we run another set of experiments without applying any morphological processing method similar to the Persian state-of-the-art CLIR methods. Iterative translation disambiguation (ITD) [11], joint cross-lingual topical relevance model (JCLTRLM) [7], top-ranked translation (TOP-1), and the bi-gram coherence translation method (BiCTM), introduced in [5] (assume |c\u0304i| = 0), are the baselines without any morphological processing units. As shown in Table 3 BiCTM outperforms all the baselines when there is no morphological processing unit. Although the improvement compared to JCLTRLM is not statistically significant, for simplicity we assume this model as a base of comparisons in the next set of experiments. In other words, we study the effect of the morphological processing units on the performance of BiCTM. As shown in Table 3 the performance of the CLIR task degraded when we use the SPLIT approach. It is due to expanding the query with irrelevant tokens (e.g., normal/abnormal). RBQE suffers from a similar problem to some extent; for example jat is a valid suffix for sabzi (=vegetable) in Persian whereas it is an invalid suffix for ketab (=book). The results demonstrate that SS4MCT outperforms all the baselines in terms of MAP. This is due to a couple of reasons; first the ability of SS4MCT at finding infixes along with other affixes, particularly in irregular inflections and second its ability at deriving the likelihood/relevance of the rules in the collection/query."}, {"heading": "4 Conclusion and Future Works", "text": "In this paper we proposed a new method for statistical stemming in morphologically complex texts. SS4MCT extracts a number of morphological rules based on edit distances of a large number of word pairs from a collection. Evaluating\n3 http://www.faraazin.ir\nSS4MCT on a dictionary-based English-Persian CLIR task demonstrates its effectiveness. Considering adjacency of the characters in the rules and evaluating the method on informal text mining remained as future works."}, {"heading": "5 Acknowledgement", "text": "The author would like to thank Razieh Rahimi and the anonymous reviewers for their helpful comments and feedback."}], "references": [{"title": "Mining text data", "author": ["C.C. Aggarwal", "C. Zhai"], "venue": "Springer Science & Business Media", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2012}, {"title": "Statistical stemming for kannada", "author": ["S. Bhat"], "venue": null, "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2013}, {"title": "Selecting query term alternations for web search by exploiting query contexts", "author": ["G. Cao", "S. Robertson", "J.Y. Nie"], "venue": "Proceedings of ACL-08: HLT. pp. 148\u2013155. Association for Computational Linguistics, Columbus, Ohio", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2008}, {"title": "Dimension projection among languages based on pseudo-relevant documents for query translation", "author": ["J. Dadashkarimi", "M.S. Shahshahani", "A. Tebbifakhr", "H. Faili", "A. Shakery"], "venue": "arXiv preprint arXiv:1605.07844", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2016}, {"title": "A Probabilistic Translation Method for Dictionary-based Cross-lingual Information Retrieval in Agglutinative Languages", "author": ["J. Dadashkarimi", "A. Shakery", "H. Faili"], "venue": "CCL \u201914. Tehran, Iran", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2014}, {"title": "Profile-based translation in multilingual expertise retrieval", "author": ["H.N. Esfahani", "J. Dadashkarimi", "A. Shakery"], "venue": "Proceedings of MultilingMine@ECIR", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2016}, {"title": "Cross-Lingual Topical Relevance Models", "author": ["D. Ganguly", "J. Leveling", "G. Jones"], "venue": "COLING \u201912. pp. 927\u2013942. Mumbai, India", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2012}, {"title": "Mining a Persian-English Comparable Corpus for Cross-language Information Retrieval", "author": ["H.B. Hashemi", "A. Shakery"], "venue": "Inf. Process. Manage. 50(2), 384\u2013398", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2014}, {"title": "Viewing morphology as an inference process", "author": ["R. Krovetz"], "venue": "Proceedings SIGIR \u201993. pp. 191\u2013202. SIGIR", "citeRegEx": "9", "shortCiteRegEx": null, "year": 1993}, {"title": "Binary Codes Capable of Correcting Deletions, Insertions and Reversals", "author": ["V.I. Levenshtein"], "venue": "Soviet Physics Doklady 10", "citeRegEx": "10", "shortCiteRegEx": null, "year": 1966}, {"title": "Iterative Translation Disambiguation for Cross-language Information Retrieval", "author": ["C. Monz", "B.J. Dorr"], "venue": "SIGIR. pp. 520\u2013527. Salvador, Brazil", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2005}, {"title": "Building a multi-domain comparable corpus using a learning to rank method", "author": ["R. Rahimi", "A. Shakery", "J. Dadashkarimi", "M. Aryannejad", "M. Dehghani", "H.N. Esfahani"], "venue": "Natural Language Engineering 22(Special Issue 04), 627\u2013653", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2016}, {"title": "Step-1: A set of fundamental tools for persian text processing", "author": ["M. Shamsfard", "H.S. Jafari", "M. Ilbeygi"], "venue": "LREC", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2010}, {"title": "Model-based Feedback in the Language Modeling Approach to Information Retrieval", "author": ["C. Zhai", "J. Lafferty"], "venue": "CIKM. pp. 403\u2013410. Atlanta, Georgia, USA", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2001}], "referenceMentions": [{"referenceID": 0, "context": "Uniforming different inflections of words is a required task in a wide range of text mining algorithms, including, but not limited to text classification, text clustering, document retrieval, and language modeling [1].", "startOffset": 214, "endOffset": 217}, {"referenceID": 8, "context": "Stemming has been considered as a common approach for this goal in several studies [9,1].", "startOffset": 83, "endOffset": 88}, {"referenceID": 0, "context": "Stemming has been considered as a common approach for this goal in several studies [9,1].", "startOffset": 83, "endOffset": 88}, {"referenceID": 1, "context": "For example, Bhat introduced a method for Kannada where the similarity of two words is determined by three distance measures based on prefix and suffix matching and the first mismatch point in the words [2].", "startOffset": 203, "endOffset": 206}, {"referenceID": 4, "context": "Defining precise rules for morphologically complex texts, especially for the purpose of infix removal is sometimes impossible [5].", "startOffset": 126, "endOffset": 129}, {"referenceID": 9, "context": ", insertion or deletion) [10], and part-of-speech (POS) tag of the input word are the attributes of a rule.", "startOffset": 25, "endOffset": 29}, {"referenceID": 7, "context": "Therefore, in dictionary-based CLIR, retrieval systems are obliged either to stem documents and queries, or to leave them intact [8,4,12], or expand the query with inflections.", "startOffset": 129, "endOffset": 137}, {"referenceID": 3, "context": "Therefore, in dictionary-based CLIR, retrieval systems are obliged either to stem documents and queries, or to leave them intact [8,4,12], or expand the query with inflections.", "startOffset": 129, "endOffset": 137}, {"referenceID": 11, "context": "Therefore, in dictionary-based CLIR, retrieval systems are obliged either to stem documents and queries, or to leave them intact [8,4,12], or expand the query with inflections.", "startOffset": 129, "endOffset": 137}, {"referenceID": 8, "context": "We opted the query expansion approach which is a widely used approach to compensate the shortage of inflections [9,3,5].", "startOffset": 112, "endOffset": 119}, {"referenceID": 2, "context": "We opted the query expansion approach which is a widely used approach to compensate the shortage of inflections [9,3,5].", "startOffset": 112, "endOffset": 119}, {"referenceID": 4, "context": "We opted the query expansion approach which is a widely used approach to compensate the shortage of inflections [9,3,5].", "startOffset": 112, "endOffset": 119}, {"referenceID": 4, "context": "We used the following probabilistic framework to this end [5]:", "startOffset": 58, "endOffset": 61}, {"referenceID": 4, "context": "To avoid adding noisy terms, we only compute the joint probabilities between either a pair of translation candidates from the dictionary (ci,j and ci\u2032,j\u2032) or a pair of a candidate from the dictionary and an inflection from the collection (ci,j and c \u2032 i\u2032,j\u2032) [5].", "startOffset": 259, "endOffset": 262}, {"referenceID": 13, "context": "Queries are expanded by the top 50 terms generated by the feedback model [14,6].", "startOffset": 73, "endOffset": 79}, {"referenceID": 5, "context": "Queries are expanded by the top 50 terms generated by the feedback model [14,6].", "startOffset": 73, "endOffset": 79}, {"referenceID": 3, "context": "We removed Persian stop words from the queries and documents [4,5].", "startOffset": 61, "endOffset": 66}, {"referenceID": 4, "context": "We removed Persian stop words from the queries and documents [4,5].", "startOffset": 61, "endOffset": 66}, {"referenceID": 12, "context": "We used STeP1 [13] in our stemming process in Persian.", "startOffset": 14, "endOffset": 18}, {"referenceID": 4, "context": ", demonstrated that Google has better coverage compared to other English-Persian dictionaries [5].", "startOffset": 94, "endOffset": 97}, {"referenceID": 10, "context": "To this end we compare the proposed SS4MCT with a number of dictionary-based CLIR methods; the 5-gram truncation method (SPLIT) proposed in [11], rule-based query expansion (RBQE) based on inflectional/derivation rules from Farazzin machine translator, and the STeP1 stemmer [13] are the morphological processing approaches for the retrieval system.", "startOffset": 140, "endOffset": 144}, {"referenceID": 12, "context": "To this end we compare the proposed SS4MCT with a number of dictionary-based CLIR methods; the 5-gram truncation method (SPLIT) proposed in [11], rule-based query expansion (RBQE) based on inflectional/derivation rules from Farazzin machine translator, and the STeP1 stemmer [13] are the morphological processing approaches for the retrieval system.", "startOffset": 275, "endOffset": 279}, {"referenceID": 10, "context": "Iterative translation disambiguation (ITD) [11], joint cross-lingual topical relevance model (JCLTRLM) [7], top-ranked translation (TOP-1), and the bi-gram coherence translation method (BiCTM), introduced in [5] (assume |c\u0304i| = 0), are the baselines without any morphological processing units.", "startOffset": 43, "endOffset": 47}, {"referenceID": 6, "context": "Iterative translation disambiguation (ITD) [11], joint cross-lingual topical relevance model (JCLTRLM) [7], top-ranked translation (TOP-1), and the bi-gram coherence translation method (BiCTM), introduced in [5] (assume |c\u0304i| = 0), are the baselines without any morphological processing units.", "startOffset": 103, "endOffset": 106}, {"referenceID": 4, "context": "Iterative translation disambiguation (ITD) [11], joint cross-lingual topical relevance model (JCLTRLM) [7], top-ranked translation (TOP-1), and the bi-gram coherence translation method (BiCTM), introduced in [5] (assume |c\u0304i| = 0), are the baselines without any morphological processing units.", "startOffset": 208, "endOffset": 211}], "year": 2016, "abstractText": "There have been multiple attempts to resolve various inflection matching problems in information retrieval. Stemming is a common approach to this end. Among many techniques for stemming, statistical stemming has been shown to be effective in a number of languages, particularly highly inflected languages. In this paper we propose a method for finding affixes in different positions of a word. Common statistical techniques heavily rely on string similarity in terms of prefix and suffix matching. Since infixes are common in irregular/informal inflections in morphologically complex texts, it is required to find infixes for stemming. In this paper we propose a method whose aim is to find statistical inflectional rules based on minimum edit distance table of word pairs and the likelihoods of the rules in a language. These rules are used to statistically stem words and can be used in different text mining tasks. Experimental results on CLEF 2008 and CLEF 2009 English-Persian CLIR tasks indicate that the proposed method significantly outperforms all the baselines in terms of MAP.", "creator": "LaTeX with hyperref package"}}}