{"id": "1605.08003", "review": {"conference": "NIPS", "VERSION": "v1", "DATE_OF_SUBMISSION": "25-May-2016", "title": "Tight Complexity Bounds for Optimizing Composite Objectives", "abstract": "We provide tight upper and lower bounds on the complexity of minimizing the average of m convex functions using gradient and prox oracles of the component functions. We show a significant gap between the complexity of deterministic vs randomized optimization. For smooth functions, we show that accelerated gradient descent (AGD) and Katyusha are optimal in the deterministic and randomized settings respectively, and that a gradient oracle is sufficient for the optimal rate. For non-smooth functions, having access to prox oracles reduces the complexity and we present optimal methods based on smoothing AGD that improve over methods using just gradient accesses.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n", "histories": [["v1", "Wed, 25 May 2016 18:44:54 GMT  (33kb)", "https://arxiv.org/abs/1605.08003v1", null], ["v2", "Thu, 27 Oct 2016 18:32:55 GMT  (35kb)", "http://arxiv.org/abs/1605.08003v2", null]], "reviews": [], "SUBJECTS": "math.OC cs.LG stat.ML", "authors": ["blake e woodworth", "nati srebro"], "accepted": true, "id": "1605.08003"}, "pdf": {"name": "1605.08003.pdf", "metadata": {"source": "CRF", "title": "Tight Complexity Bounds for Optimizing Composite Objectives", "authors": ["Blake Woodworth"], "emails": ["blake@ttic.edu", "nati@ttic.edu"], "sections": [{"heading": null, "text": "ar X\niv :1\n60 5.\n08 00\n3v 2\n[ m\nat h.\nO C\n] 2\n7 O\nct 2"}, {"heading": "1 Introduction", "text": "We consider minimizing the average of m \u2265 2 convex functions:\nmin x\u2208X\n{\nF (x) := 1\nm\nm \u2211\ni=1\nfi(x)\n}\n(1)\nwhere X \u2286 Rd is a closed, convex set, and where the algorithm is given access to the following gradient (or subgradient in the case of non-smooth functions) and prox oracle for the components:\nhF (x, i, \u03b2) = [ fi(x), \u2207fi(x), proxfi(x, \u03b2) ]\n(2)\nwhere\nproxfi(x, \u03b2) = argmin u\u2208X\n{\nfi(u) + \u03b2\n2 \u2016x\u2212 u\u20162\n}\n(3)\nA natural question is how to leverage the prox oracle, and how much benefit it provides over gradient access alone. The prox oracle is potentially much more powerful, as it provides global, rather then local, information about the function. For example, for a single function (m = 1), one prox oracle call (with \u03b2 = 0) is sufficient for exact optimization. Several methods have recently been suggested for optimizing a sum or average of several functions using prox accesses to each component, both in the distributed setting where each components might be handled on a different machine (e.g. ADMM [7], DANE [18], DISCO [20]) or for functions that can be decomposed into several \u201ceasy\u201d parts (e.g. PRISMA [13]). But as far as we are aware, no meaningful lower bound was previously known on the number of prox oracle accesses required even for the average of two functions (m = 2).\nThe optimization of composite objectives of the form (1) has also been extensively studied in the context of minimizing empirical risk over m samples. Recently, stochastic methods such as SDCA [16], SAG [14], SVRG [8], and other variants, have been presented which leverage the finite nature of the problem to reduce the variance in stochastic gradient estimates and obtain guarantees that dominate both batch and stochastic gradient descent. As methods with improved complexity, such as accelerated SDCA [17], accelerated SVRG, and Katyusha [3], have been presented, researchers have also tried to obtain lower bounds on the best possible complexity in this settings\u2014but as we survey below, these have not been satisfactory so far.\nIn this paper, after briefly surveying methods for smooth, composite optimization, we present methods for optimizing non-smooth composite objectives, which show that prox oracle access can indeed be leveraged to improve over methods using merely subgradient access (see Section 3). We then turn to studying lower bounds. We consider algorithms that access the objective F only through the oracle hF and provide lower bounds on the number of such oracle accesses (and thus the runtime) required to find \u01eb-suboptimal solutions. We consider optimizing both Lipschitz (non-smooth) functions and smooth functions, and guarantees that do and do not depend on strong convexity, distinguishing between deterministic optimization algorithms and randomized algorithms. Our upper and lower bounds are summarized in Table 1.\nAs shown in the table, we provide matching upper and lower bounds (up to a log factor) for all function and algorithm classes. In particular, our bounds establish the optimality (up to log factors) of accelerated SDCA, SVRG, and SAG for randomized finite-sum optimization, and also the optimality of our deterministic smoothing algorithms for non-smooth composite optimization.\nOn the power of gradient vs prox oracles For non-smooth functions, we show that having access to prox oracles for the components can reduce the polynomial dependence on \u01eb from 1/\u01eb2 to 1/\u01eb, or from 1/(\u03bb\u01eb) to 1/ \u221a \u03bb\u01eb for \u03bb-strongly convex functions. However, all of the optimal complexities for smooth functions can be attained with only component gradient access using accelerated gradient descent (AGD) or accelerated SVRG. Thus the worst-case complexity cannot be improved (at least not significantly) by using the more powerful prox oracle.\nOn the power of randomization We establish a significant gap between deterministic and randomized algorithms for finite-sum problems. Namely, the dependence on the number of components must be linear in m for any deterministic algorithm, but can be reduced to \u221a m (in the typically significant term) using randomization. We emphasize that the randomization here is only in the algorithm\u2014not in the oracle. We always assume the oracle returns an exact answer (for the requested component) and is not a stochastic oracle. The distinction is that the algorithm is allowed to flip coins in deciding what operations and queries to perform but the oracle must return an exact answer to that query (of course, the algorithm could simulate a stochastic oracle).\nPrior Lower Bounds Several authors recently presented lower bounds for optimizing (1) in the smooth and strongly convex setting using component gradients. Agarwal and Bottou [1] presented a lower bound of \u2126 ( m+ \u221a\nm\u03b3 \u03bb log 1 \u01eb\n)\n. However, their bound is valid only for deterministic algorithms (thus not including SDCA, SVRG, SAG, etc.)\u2014we not only consider randomized algorithms, but also show a much higher lower\nbound for deterministic algorithms (i.e. the bound of Agarwal and Bottou is loose). Improving upon this, Lan [9] shows a similar lower bound for a restricted class of randomized algorithms: the algorithm must select which component to query for a gradient by drawing an index from a fixed distribution, but the algorithm must otherwise be deterministic in how it uses the gradients, and its iterates must lie in the span of the gradients it has received. This restricted class includes SAG, but not SVRG nor perhaps other realistic attempts at improving over these. Furthermore, both bounds allow only gradient accesses, not prox computations. Thus SDCA, which requires prox accesses, and potential variants are not covered by such lower bounds. We prove as similar lower bound to Lan\u2019s, but our analysis is much more general and applies to any randomized algorithm, making any sequence of queries to a gradient and prox oracle, and without assuming that iterates lie in the span of previous responses. In addition to smooth functions, we also provide lower bounds for non-smooth problems which were not considered by these previous attempts. Another recent observation [15] was that with access only to random component subgradients without knowing the component\u2019s identity, an algorithm must make \u2126(m2) queries to optimize well. This shows how relatively subtle changes in the oracle can have a dramatic effect on the complexity of the problem. Since the oracle we consider is quite powerful, our lower bounds cover a very broad family of algorithms, including SAG, SVRG, and SDCA.\nOur deterministic lower bounds are inspired by a lower bound on the number of rounds of communication required for optimization when each fi is held by a different machine and when iterates lie in the span of certain permitted calculations [5]. Our construction for m = 2 is similar to theirs (though in a different setting), but their analysis considers neither scaling with m (which has a different role in their setting) nor randomization.\nNotation and Definitions We use \u2016\u00b7\u2016 to denote the standard Euclidean norm on Rd. We say that a function f is L-Lipschitz continuous on X if \u2200x, y \u2208 X |f(x)\u2212 f(x)| \u2264 L \u2016x\u2212 y\u2016; \u03b3-smooth on X if it is differentiable and its gradient is \u03b3-Lipschitz on X ; and \u03bb-strongly convex on X if \u2200x, y \u2208 X fi(y) \u2265 fi(x) + \u3008\u2207fi(x), y \u2212 x\u3009 + \u03bb2 \u2016x\u2212 y\u2016\n2. We consider optimizing (1) under four combinations of assumptions: each component fi is either L-Lipschitz or \u03b3-smooth, and either F (x) is \u03bb-strongly convex or its domain is bounded, X \u2286 {x : \u2016x\u2016 \u2264 B}."}, {"heading": "2 Optimizing Smooth Sums", "text": "We briefly review the best known methods for optimizing (1) when the components are \u03b3-smooth, yielding the upper bounds on the right half of Table 1. These upper bounds can be obtained using only component gradient access, without need for the prox oracle.\nWe can obtain exact gradients of F (x) by computing allm component gradients\u2207fi(x). Running accelerated gradient descent (AGD) [12] on F (x) using these exact gradients achieves the upper complexity bounds for deterministic algorithms and smooth problems (see Table 1).\nSAG [14], SVRG [8] and related methods use randomization to sample components, but also leverage the finite nature of the objective to control the variance of the gradient estimator used. Accelerating these methods using the Catalyst framework [10] ensures that for \u03bb-strongly convex objectives we have E [ F (x(k))\u2212 F (x\u2217) ] < \u01eb after k = O (( m+ \u221a\nm\u03b3 \u03bb\n) log2 \u01eb0\u01eb ) iterations, where F (0)\u2212 F (x\u2217) = \u01eb0. Katyusha [3] is a more direct approach to accelerating SVRG which avoids extraneous log-factors, yielding the complexity k = O (( m+ \u221a\nm\u03b3 \u03bb\n) log \u01eb0\u01eb ) indicated in Table 1.\nWhen F is not strongly convex, adding a regularizer to the objective and instead optimizing F\u03bb(x) =\nF (x) + \u03bb2 \u2016x\u2016 2 with \u03bb = \u01eb/B2 results in an oracle complexity of O\n((\nm+\n\u221a\nm\u03b3B2\n\u01eb\n)\nlog \u01eb0\u01eb\n)\n. The log-factor\nin the second term can be removed using the more delicate reduction of Allen-Zhu and Hazan [4], which involves optimizing F\u03bb(x) for progressively smaller values of \u03bb, yielding the upper bound in the table.\nKatyusha and Catalyst-accelerated SAG or SVRG use only gradients of the components. Accelerated SDCA [17] achieves a similar complexity using gradient and prox oracle access."}, {"heading": "3 Leveraging Prox Oracles for Lipschitz Sums", "text": "In this section, we present algorithms for leveraging the prox oracle to minimize (1) when each component is L-Lipschitz. This will be done by using the prox oracle to \u201csmooth\u201d each component, and optimizing the new, smooth sum which approximates the original problem. This idea was used in order to apply Katyusha [3] and accelerated SDCA [17] to non-smooth objectives. We are not aware of a previous explicit presentation of the AGD-based deterministic algorithm, which achieves the deterministic upper complexity indicated in Table 1.\nThe key is using a prox oracle to obtain gradients of the \u03b2-Moreau envelope of a non-smooth function, f , defined as:\nf (\u03b2)(x) = inf u\u2208X\nf(u) + \u03b2\n2 \u2016x\u2212 u\u20162 (4)\nLemma 1 ([13, Lemma 2.2], [6, Proposition 12.29], following [11]). Let f be convex and L-Lipschitz continuous. For any \u03b2 > 0,\n1. f (\u03b2) is \u03b2-smooth\n2. \u2207(f (\u03b2))(x) = \u03b2(x \u2212 proxf (x, \u03b2))\n3. f (\u03b2)(x) \u2264 f(x) \u2264 f (\u03b2)(x) + L22\u03b2\nConsequently, we can consider the smoothed problem\nmin x\u2208X\n{\nF\u0303 (\u03b2)(x) := 1\nm\nm \u2211\ni=1\nf (\u03b2) i (x)\n}\n. (5)\nWhile F\u0303 (\u03b2) is not, in general, the \u03b2-Moreau envelope of F , it is \u03b2-smooth, we can calculate the gradient of its components using the oracle hF , and F\u0303 (\u03b2)(x) \u2264 F (x) \u2264 F\u0303 (\u03b2)(x) + L22\u03b2 . Thus, to obtain an \u01eb-suboptimal solution to (1) using hF , we set \u03b2 = L 2/\u01eb and apply any algorithm which can optimize (5) using gradients of the L2/\u01eb-smooth components, to within \u01eb/2 accuracy. With the rates presented in Section 2, using AGD on (5) yields a complexity of O (\nmLB \u01eb\n)\nin the deterministic setting. When the functions are \u03bb-strongly convex, smoothing with a fixed \u03b2 results in a spurious log-factor. To avoid this, we again apply the reduction of Allen-Zhu and Hazan [4], this time optimizing F\u0303 (\u03b2) for increasingly large values of \u03b2. This leads to the upper bound of O (\nmL\u221a \u03bb\u01eb\n)\nwhen used with AGD (see Appendix A for details).\nSimilarly, we can apply an accelerated randomized algorithm (such as Katyusha) to the smooth problem F\u0303 (\u03b2) to obtain complexities of O ( m log \u01eb0\u01eb + \u221a mLB \u01eb ) and O ( m log \u01eb0\u01eb + \u221a mL\u221a \u03bb\u01eb ) \u2014this matches the presentation of Allen-Zhu [3] and is similar to that of Shalev-Shwartz and Zhang [17].\nFinally, if m > L2B2/\u01eb2 or m > L2/(\u03bb\u01eb), stochastic gradient descent is a better randomized alternative, yielding complexities of O(L2B2/\u01eb2) or O(L2/(\u03bb\u01eb))."}, {"heading": "4 Lower Bounds for Deterministic Algorithms", "text": "We now turn to establishing lower bounds on the oracle complexity of optimizing (1). We first consider only deterministic optimization algorithms. What we would like to show is that for any deterministic optimization algorithm we can construct a \u201chard\u201d function for which the algorithm cannot find an \u01eb-suboptimal solution until it has made many oracle accesses. Since the algorithm is deterministic, we can construct such a function by simulating the (deterministic) behavior of the algorithm. This can be viewed as a game, where an adversary controls the oracle being used by the algorithm. At each iteration the algorithm queries the oracle with some triplet (x, i, \u03b2) and the adversary responds with an answer. This answer must be consistent with all previous answers, but the adversary ensures it is also consistent with a composite function F that\nthe algorithm is far from optimizing. The \u201chard\u201d function is then gradually defined in terms of the behavior of the optimization algorithm.\nTo help us formulate our constructions, we define a \u201cround\u201d of queries as a series of queries in which \u2308m2 \u2309 distinct functions fi are queried. The first round begins with the first query and continues until exactly \u2308m2 \u2309 unique functions have been queried. The second round begins with the next query, and continues until exactly \u2308m2 \u2309 more distinct components have been queried in the second round, and so on until the algorithm terminates. This definition is useful for analysis but requires no assumptions about the algorithm\u2019s querying strategy."}, {"heading": "4.1 Non-Smooth Components", "text": "We begin by presenting a lower bound for deterministic optimization of (1) when each component fi is convex and L-Lipschitz continuous, but is not necessarily strongly convex, on the domain X = {x : \u2016x\u2016 \u2264 B}. Without loss of generality, we can consider L = B = 1. We will construct functions of the following form:\nfi(x) = 1\u221a 2 |b \u2212 \u3008x, v0\u3009|+ 1 2 \u221a k\nk \u2211\nr=1\n\u03b4i,r |\u3008x, vr\u22121\u3009 \u2212 \u3008x, vr\u3009| . (6)\nwhere k = \u230a 112\u01eb\u230b, b = 1\u221ak+1 , and {vr} is an orthonormal set of vectors in R d chosen according to the behavior of the algorithm such that vr is orthogonal to all points at which the algorithm queries hF before round r, and where \u03b4i,r are indicators chosen so that \u03b4i,r = 1 if the algorithm does not query component i in round r (and zero otherwise). To see how this is possible, consider the following truncations of (6):\nf ti (x) = 1\u221a 2 |b\u2212 \u3008x, v0\u3009|+ 1 2 \u221a k\nt\u22121 \u2211\nr=1\n\u03b4i,r |\u3008x, vr\u22121\u3009 \u2212 \u3008x, vr\u3009| (7)\nDuring each round t, the adversary answers queries according to f ti , which depends only on vr, \u03b4i,r for r < t, i.e. from previous rounds. When the round is completed, \u03b4i,t is determined and vt is chosen to be orthogonal to the vectors {v0, ..., vt\u22121} as well as every point queried by the algorithm so far, thus defining f t+1i for the next round. In Appendix B.1 we prove that these responses based on f ti are consistent with fi.\nThe algorithm can only learn vr after it completes round r\u2014until then every iterate is orthogonal to it by construction. The average of these functions reaches its minimum of F (x\u2217) = 0 at x\u2217 = b\n\u2211k r=0 vr, so we\ncan view optimizing these functions as the task of discovering the vectors vr\u2014even if only vk is missing, a suboptimality better than b/(6 \u221a k) > \u01eb cannot be achieved. Therefore, the deterministic algorithm must complete at least k rounds of optimization, each comprising at least \u2308\nm 2\n\u2309\nqueries to hF in order to optimize F . The key to this construction is that even though each term |\u3008x, vr\u22121\u3009 \u2212 \u3008x, vr\u3009| appears in m/2 components, and hence has a strong effect on the average F (x), we can force a deterministic algorithm to make \u2126(m) queries during each round before it finds the next relevant term. We obtain (for complete proof see Appendix B.1):\nTheorem 1. For any L,B > 0, any 0 < \u01eb < LB12 , any m \u2265 2, and any deterministic algorithm A with access to hF , there exists a dimension d = O ( mLB \u01eb ) , and m functions fi defined over X = { x \u2208 Rd : \u2016x\u2016 \u2264 B } , which are convex and L-Lipschitz continuous, such that in order to find a point x\u0302 for which F (x\u0302)\u2212F (x\u2217) < \u01eb, A must make \u2126 (\nmLB \u01eb\n)\nqueries to hF .\nFurthermore, we can always reduce optimizing a function over \u2016x\u2016 \u2264 B to optimizing a strongly convex function by adding the regularizer \u01eb \u2016x\u20162 /(2B2) to each component, implying (see complete proof in Appendix B.2):\nTheorem 2. For any L, \u03bb > 0, any 0 < \u01eb < L 2\n288\u03bb , any m \u2265 2, and any deterministic algorithm A with access to hF , there exists a dimension d = O (\nmL\u221a \u03bb\u01eb\n)\n, and m functions fi defined over X \u2286 Rd, which are LLipschitz continuous and \u03bb-strongly convex, such that in order to find a point x\u0302 for which F (x\u0302)\u2212F (x\u2217) < \u01eb, A must make \u2126 (\nmL\u221a \u03bb\u01eb\n)\nqueries to hF ."}, {"heading": "4.2 Smooth Components", "text": "When the components fi are required to be smooth, the lower bound construction is similar to (6), except it is based on squared differences instead of absolute differences. We consider the functions:\nfi(x) = 1\n8\n(\n\u03b4i,1\n( \u3008x, v0\u30092 \u2212 2a \u3008x, v0\u3009 ) + \u03b4i,k \u3008x, vk\u30092 + k \u2211\nr=1\n\u03b4i,r (\u3008x, vr\u22121\u3009 \u2212 \u3008x, vr\u3009)2 )\n(8)\nwhere \u03b4i,r and vr are as before. Again, we can answer queries at round t based only on \u03b4i,r, vr for r < t. This construction yields the following lower bounds (full details in Appendix B.3):\nTheorem 3. For any \u03b3,B, \u01eb > 0, any m \u2265 2, and any deterministic algorithm A with access to hF , there exists a sufficiently large dimension d = O ( m \u221a \u03b3B2/\u01eb )\n, and m functions fi defined over X = {\nx \u2208 Rd : \u2016x\u2016 \u2264 B } , which are convex and \u03b3-smooth, such that in order to find a point x\u0302 \u2208 Rd for which F (x\u0302)\u2212 F (x\u2217) < \u01eb, A must make \u2126 ( m \u221a \u03b3B2/\u01eb )\nqueries to hF .\nIn the strongly convex case, we use a very similar construction, adding the term \u03bb \u2016x\u20162 /2, which gives the following bound (see Appendix B.4):\nTheorem 4. For any \u03b3, \u03bb > 0 such that \u03b3\u03bb > 73, any \u01eb > 0, any \u01eb0 > 3\u03b3\u01eb \u03bb , any m \u2265 2, and any deterministic algorithm A with access to hF , there exists a sufficiently large dimension d = O ( m \u221a\u03b3 \u03bb log ( \u03bb\u01eb0 \u03b3\u01eb )) , and m functions fi defined over X \u2286 Rd, which are \u03b3-smooth and \u03bb-strongly convex and where F (0)\u2212 F (x\u2217) = \u01eb0, such that in order to find a point x\u0302 for which F (x\u0302)\u2212 F (x\u2217) < \u01eb, A must make \u2126 ( m \u221a\n\u03b3 \u03bb log\n(\n\u03bb\u01eb0 \u03b3\u01eb\n))\nqueries\nto hF ."}, {"heading": "5 Lower Bounds for Randomized Algorithms", "text": "We now turn to randomized algorithms for (1). In the deterministic constructions, we relied on being able to set vr and \u03b4i,r based on the predictable behavior of the algorithm. This is impossible for randomized algorithms, we must choose the \u201chard\u201d function before we know the random choices the algorithm will make\u2014so the function must be \u201chard\u201d more generally than before.\nPreviously, we chose vectors vr orthogonal to all previous queries made by the algorithm. For randomized algorithms this cannot be ensured. However, if we choose orthonormal vectors vr randomly in a high dimensional space, they will be nearly orthogonal to queries with high probability. Slightly modifying the absolute or squared difference from before makes near orthogonality sufficient. This issue increases the required dimension but does not otherwise affect the lower bounds.\nMore problematic is our inability to anticipate the order in which the algorithm will query the components, precluding the use of \u03b4i,r. In the deterministic setting, if a term revealing a new vr appeared in half of the components, we could ensure that the algorithm must make m/2 queries to find it. However, a randomized algorithm could find it in two queries in expectation, which would eliminate the linear dependence on m in the lower bound! Alternatively, if only one component included the term, a randomized algorithm would indeed need \u2126(m) queries to find it, but that term\u2019s effect on suboptimality of F would be scaled down by m, again eliminating the dependence on m. To establish a \u2126( \u221a m) lower bound for randomized algorithms we must take a new approach. We define \u230a\nm 2\n\u230b\npairs of functions which operate on \u230a m 2 \u230b orthogonal subspaces of Rd. Each pair of functions resembles the constructions from the previous section, but since there are many of them, the algorithm must solve \u2126(m) separate optimization problems in order to optimize F ."}, {"heading": "5.1 Lipschitz Continuous Components", "text": "First consider the non-smooth, non-strongly-convex setting and assume for simplicity m is even (otherwise we simply let the last function be zero). We define the helper function \u03c8c, which replaces the absolute\nvalue operation and makes our construction resistant to small inner products between iterates and not-yetdiscovered components: \u03c8c(z) = max (0, |z| \u2212 c) (9) Next, we define m/2 pairs of functions, indexed by i = 1..m/2:\nfi,1(x) = 1\u221a 2 |b\u2212 \u3008x, vi,0\u3009|+ 1 2 \u221a k\nk \u2211\nr even\n\u03c8c (\u3008x, vi,r\u22121\u3009 \u2212 \u3008x, vi,r\u3009) (10)\nfi,2(x) = 1\n2 \u221a k\nk \u2211\nr odd\n\u03c8c (\u3008x, vi,r\u22121\u3009 \u2212 \u3008x, vi,r\u3009)\nwhere {vi,r}r=0..k,i=1..m/2 are random orthonormal vectors and k = \u0398( 1\u01eb\u221am ). With c sufficiently small and the dimensionality sufficiently high, with high probability the algorithm only learns the identity of new vectors vi,r by alternately querying fi,1 and fi,2; so revealing all k + 1 vectors requires at least k + 1 total queries. Until vi,k is revealed, an iterate is \u2126(\u01eb)-suboptimal on (fi,1 + fi,2)/2. From here, we show that an \u01eb-suboptimal solution to F (x) can be found only after at least k + 1 queries are made to at least m/4 pairs, for a total of \u2126(mk) queries. This time, since the optimum x\u2217 will need to have inner product b with \u0398(mk) vectors vi,r , we need to have b = \u0398( 1\u221a mk ) = \u0398( \u221a \u01eb/ \u221a m), and the total number of queries is \u2126(mk) = \u2126( \u221a m \u01eb ). The \u2126(m) term of the lower bound follows trivially since we require \u01eb = O(1/ \u221a m), (proofs in Appendix C.1):\nTheorem 5. For any L,B > 0, any 0 < \u01eb < LB 10 \u221a m , any m \u2265 2, and any randomized algorithm A with access to hF , there exists a dimension d = O ( L4B6 \u01eb4 log ( LB \u01eb ) )\n, and m functions fi defined over X = {\nx \u2208 Rd : \u2016x\u2016 \u2264 B } , which are convex and L-Lipschitz continuous, such that to find a point x\u0302 for which\nE [F (x\u0302)\u2212 F (x\u2217)] < \u01eb, A must make \u2126 ( m+ \u221a mLB \u01eb ) queries to hF .\nAn added regularizer gives the result for strongly convex functions (see Appendix C.2):\nTheorem 6. For any L, \u03bb > 0, any 0 < \u01eb < L 2\n200\u03bbm , any m \u2265 2, and any randomized algorithm A with access to hF , there exists a dimension d = O ( L4\n\u03bb3\u01eb log L\u221a \u03bb\u01eb\n)\n, and m functions fi defined over X \u2286 Rd, which are L-Lipschitz continuous and \u03bb-strongly convex, such that in order to find a point x\u0302 for which E [F (x\u0302)\u2212 F (x\u2217)] < \u01eb, A must make \u2126 (\nm+ \u221a mL\u221a \u03bb\u01eb ) queries to hF .\nThe large dimension required by these lower bounds is the cost of omitting the assumption that the algorithm\u2019s queries lie in the span of previous oracle responses. If we do assume that the queries lie in that span, the necessary dimension is only on the order of the number of oracle queries needed. When \u01eb = \u2126(LB/ \u221a m) in the non-strongly convex case or \u01eb = \u2126 ( L2/(\u03bbm) )\nin the strongly convex case, the lower bounds for randomized algorithms presented above do not apply. Instead, we can obtain a lower bound based on an information theoretic argument. We first uniformly randomly choose a parameter p, which is either (1/2 \u2212 2\u01eb) or (1/2 + 2\u01eb). Then for i = 1, ...,m, in the non-strongly convex case we make fi(x) = x with probability p and fi(x) = \u2212x with probability 1 \u2212 p. Optimizing F (x) to within \u01eb accuracy then implies recovering the bias of the Bernoulli random variable, which requires \u2126(1/\u01eb2) queries based on a standard information theoretic result [2, 19]. Setting fi(x) = \u00b1x+ \u03bb2 \u2016x\u2016 2 gives a \u2126(1/(\u03bb\u01eb)) lower bound in the \u03bb-strongly convex setting. This is formalized in Appendix C.5."}, {"heading": "5.2 Smooth Components", "text": "When the functions fi are smooth and not strongly convex, we define another helper function \u03c6c:\n\u03c6c(z) =\n\n \n  0 |z| \u2264 c 2(|z| \u2212 c)2 c < |z| \u2264 2c z2 \u2212 2c2 |z| > 2c\n(11)\nand the following pairs of functions for i = 1, ...,m/2:\nfi,1(x) = 1\n16\n( \u3008x, vi,0\u30092 \u2212 2a \u3008x, vi,0\u3009+ k \u2211\nr even\n\u03c6c (\u3008x, vi,r\u22121\u3009 \u2212 \u3008x, vi,r\u3009) )\n(12)\nfi,2(x) = 1\n16\n( \u03c6c (\u3008x, vi,k\u3009) + k \u2211\nr odd\n\u03c6c (\u3008x, vi,r\u22121\u3009 \u2212 \u3008x, vi,r\u3009) )\nwith vi,r as before. The same arguments apply, after replacing the absolute difference with squared difference. A separate argument is required in this case for the \u2126(m) term in the bound, which we show using a construction involving m simple linear functions (see Appendix C.3).\nTheorem 7. For any \u03b3,B, \u01eb > 0, any m \u2265 2, and any randomized algorithm A with access to hF , there exists a sufficiently large dimension d = O ( \u03b32B6\n\u01eb2 log ( \u03b3B2 \u01eb ) +B2m logm ) and m functions fi defined over\nX = { x \u2208 Rd : \u2016x\u2016 \u2264 B } , which are convex and \u03b3-smooth, such that to find a point x\u0302 \u2208 Rd for which E [F (x\u0302)\u2212 F (x\u2217)] < \u01eb, A must make \u2126 ( m+ \u221a m\u03b3B2\n\u01eb\n)\nqueries to hF .\nIn the strongly convex case, we add the term \u03bb \u2016x\u20162 /2 to fi,1 and fi,2 (see Appendix C.4) to obtain:\nTheorem 8. For any m \u2265 2, any \u03b3, \u03bb > 0 such that \u03b3\u03bb > 161m, any \u01eb > 0, any \u01eb0 > 60\u01eb \u221a \u03b3 \u03bbm , and any randomized algorithm A, there exists a dimension d = O (\n\u03b32.5\u01eb0 \u03bb2.5\u01eb log\n3 (\n\u03bb\u01eb0 \u03b3\u01eb\n) + m\u03b3\u01eb0\u03bb\u01eb logm )\n, domain X \u2286 R\nd, x0 \u2208 X , and m functions fi defined on X which are \u03b3-smooth and \u03bb-strongly convex, and such that F (x0) \u2212 F (x\u2217) = \u01eb0 and such that in order to find a point x\u0302 \u2208 X such that E [F (x\u0302)\u2212 F (x\u2217)] < \u01eb, A must make \u2126 ( m+ \u221a\nm\u03b3 \u03bb log\n(\n\u01eb0 \u01eb\n\u221a\nm\u03bb \u03b3\n))\nqueries to hF .\nRemark: We consider (1) as a constrained optimization problem, thus the minimizer of F could be achieved on the boundary of X , meaning that the gradient need not vanish. If we make the additional assumption that the minimizer of F lies on the interior of X (and is thus the unconstrained global minimum), Theorems 1-8 all still apply, with a slight modification to Theorems 3 and 7. Since the gradient now needs to vanish on X , 0 is always O(\u03b3B2)-suboptimal, and only values of \u01eb in the range 0 < \u01eb < \u03b3B2128 and 0 < \u01eb < 9\u03b3B2\n128 result in a non-trivial lower bound (see Remarks at the end of Appendices B.3 and C.3)."}, {"heading": "6 Conclusion", "text": "We provide a tight (up to a log factor) understanding of optimizing finite sum problems of the form (1) using a component prox oracle.\nRandomized optimization of (1) has been the subject of much research in the past several years, starting with the presentation of SDCA and SAG, and continuing with accelerated variants. Obtaining lower bounds can be very useful for better understanding the problem, for knowing where it might or might not be possible to improve or where different assumptions would be needed to improve, and for establishing optimality of optimization methods. Indeed, several attempts have been made at lower bounds for the finite sum setting [1, 9]. But as we explain in the introduction, these were unsatisfactory and covered only limited classes of methods. Here we show that in a fairly general sense, accelerated SDCA, SVRG, SAG, and Katyusha are optimal up to a log factor. Improving on their runtime would require additional assumptions, or perhaps a stronger oracle. However, even if given \u201cfull\u201d access to the component functions, all algorithms that we can think of utilize this information to calculate a prox vector. Thus, it is unclear what realistic oracle would be more powerful than the prox oracle we consider.\nOur results highlight the power of randomization, showing that no deterministic algorithm can beat the linear dependence on m and reduce it to the \u221a m dependence of the randomized algorithms.\nThe deterministic algorithm for non-smooth problems that we present in Section 3 is also of interest in its own right. It avoids randomization, which is not usually problematic, but makes it fully parallelizable unlike the optimal stochastic methods. Consider, for example, a supervised learning problem where fi(x) = \u2113(\u3008\u03c6i, x\u3009, yi) is the (non-smooth) loss on a single training example (\u03c6i, yi), and the data is distributed across machines. Calculating a prox oracle involves applying the Fenchel conjugate of the loss function \u2113, but even if a closed form is not available, this is often easy to compute numerically, and is used in algorithms such as SDCA. But unlike SDCA, which is inherently sequential, we can calculate all m prox operations in parallel on the different machines, average the resulting gradients of the smoothed function, and take an accelerated gradient step to implement our optimal deterministic algorithm. This method attains a recent lower bound for distributed optimization, resolving a question raised by Arjevani and Shamir [5], and when the number of machines is very large improves over all other known distributed optimization methods for the problem.\nIn studying finite sum problems, we were forced to explicitly study lower bounds for randomized optimization as opposed to stochastic optimization (where the source of randomness is the oracle, not the algorithm). Even for the classic problem of minimizing a smooth function using a first order oracle, we could not locate a published proof that applies to randomized algorithms. We provide a simple construction using \u01eb-insensitive differences that allows us to easily obtain such lower bounds without reverting to assuming the iterates are spanned by previous responses (as was done, e.g., in [9]), and could potentially be useful for establishing randomized lower bounds also in other settings.\nAcknowledgements: We thank Ohad Shamir for his helpful discussions and for pointing out [4]."}, {"heading": "A Upper bounds for non-smooth sums", "text": "Consider the case where the components are not strongly convex. As shown in lemma 1, we can use a single call to a prox oracle to obtain the gradient of\nf (\u03b2)(x) = inf u\u2208X\nf(u) + \u03b2\n2 \u2016x\u2212 u\u20162\nwhich is a \u03b2-smooth approximation to f . We then consider the new optimization problem:\nmin x\u2208X\n{\nF\u0303 (\u03b2)(x) := 1\nm\nm \u2211\ni=1\nf (\u03b2) i (x)\n}\n. (13)\nAlso by lemma 1, setting \u03b2 = L 2\n\u01eb ensures that F\u0303 (\u03b2)(x) \u2264 F (x) \u2264 F\u0303 (\u03b2)(x) + \u01eb2 for all x. Consequently, any\npoint which is \u01eb2 -suboptimal for F\u0303 (\u03b2) will be \u01eb-suboptimal for F . This technique therefore reduces the task of optimizing an instance of an L-Lipschitz finite sum to that of optimizing an L 2\n\u01eb -smooth finite sum.\nSolving (13) to \u01eb2 -suboptimality using AGD requires O ( mLB \u01eb ) gradients for F\u0303 (\u03b2) which requires that same number of prox oracles from hF . Formally:\nTheorem 9. For any L,B > 0, any \u01eb < LB, and any m \u2265 1 functions fi which are convex and L-Lipschitz continuous over the domain X \u2286 { x \u2208 Rd : \u2016x\u2016 \u2264 B } , applying AGD to (13) for \u03b2 = L 2\n\u01eb , will result in a\npoint x\u0302 such that F (x\u0302)\u2212 F (x\u2217) < \u01eb after O ( mLB \u01eb ) queries to hF .\nWhen the component functions are \u03bb-strongly convex, a more sophisticated strategy is required to avoid an extra log factor. The solution is the AdaptSmooth algorithm [4]. This involves solving O(log 1\u01eb ) smooth and strongly convex subproblems, where the tth subproblem is reducing the suboptimality of the \u03b2t-smooth and \u03bb-strongly convex function F (\u03b2t)(x) by a factor of four, where \u03b2t = L2\n\u01eb0 2t and where \u01eb0 \u2264 L\n2\n\u03bb upper bounds the\ninitial suboptimality. Using this method results in an \u01eb-suboptimal solution for F after \u2211log\n\u01eb0 \u01eb\nt=0 Time(\u03b2t, \u03bb) queries to hF . In the case of AGD, Time(\u03b3, \u03bb) = O ( m \u221a\n\u03b3 \u03bb\n)\nand\nlog \u01eb0 \u01eb\n\u2211\nt=0\nTime\n(\nL2 \u01eb0 2t, \u03bb\n) = O ( mL\u221a \u03bb\u01eb )\nTheorem 10. For any L, \u03bb, \u01eb > 0, and any m \u2265 1 functions fi, which are L-Lipschitz continuous and \u03bb-strongly convex on the domain X \u2286 Rd, applying AdaptSmooth with AGD will find a point x\u0302 \u2208 X such that F (x\u0302)\u2212 F (x\u2217) < \u01eb after O (\nmL\u221a \u03bb\u01eb\n)\nqueries to hF .\nTo conclude our presentation of upper bounds, we emphasize that the smoothing methods described in this section will only improve oracle complexity when used with accelerated methods. For example, using non-accelerated gradient descent on F\u0303 (\u03b2) in the not strongly convex case leads to an oracle complexity of O ( mL2B2\n\u01eb2\n)\n, which is no better than the convergence rate of gradient descent applied directly to F ."}, {"heading": "B Lower bounds for deterministic algorithms", "text": "B.1 Non-smooth and not strongly convex components\nTheorem 1. For any L,B > 0, any 0 < \u01eb < LB12 , any m \u2265 2, and any deterministic algorithm A with access to hF , there exists a dimension d = O ( mLB \u01eb ) , and m functions fi defined over X = { x \u2208 Rd : \u2016x\u2016 \u2264 B } , which are convex and L-Lipschitz continuous, such that in order to find a point x\u0302 for which F (x\u0302)\u2212F (x\u2217) < \u01eb, A must make \u2126 (\nmLB \u01eb\n)\nqueries to hF .\nProof. Without loss of generality, we can assume L = B = 1. For particular values b and k to be decided upon later, we use the functions (6):\nfi(x) = 1\u221a 2 |b\u2212 \u3008x, v0\u3009|+ 1 2 \u221a k\nk \u2211\nr=1\n\u03b4i,r |\u3008x, vr\u22121\u3009 \u2212 \u3008x, vr\u3009|\nIt is straightforward to confirm that fi is both 1-Lipschitz and convex (for orthonormal vectors vr and indicators \u03b4i,r \u2208 {0, 1}). As explained in the main text, the orthonormal vectors vr \u2208 Rd and indicators \u03b4i,r \u2208 {0, 1} are chosen according to the behavior of the algorithm A. At the end of each round t, we set \u03b4i,t = 1 iff the algorithm did not query function i during round t (and zero otherwise), and we set vt to be orthogonal to the vectors {v0, ..., vt\u22121} as well as every query made by the algorithm so far. Orthogonalizing the vectors in this way is possible as long as the dimension is at least as large as the number of oracle queries A has made so far plus t. We are allowed to construct vt and \u03b4i,t in this way as long as the algorithm\u2019s execution up until round t, and thus our choice of vt and \u03b4i,t, depends only on vr and \u03b4i,r for r < t. We can enforce this condition by answering the queries during round t according to\nf ti (x) = 1\u221a 2 |b\u2212 \u3008x, v0\u3009|+ 1 2 \u221a k\nt\u22121 \u2211\nr=1\n\u03b4i,r |\u3008x, vr\u22121\u3009 \u2212 \u3008x, vr\u3009|\nFor non-smooth functions, the subgradient oracle is not uniquely defined\u2014-many different subgradients might be a valid response. However, in order to say that an algorithm successfully optimizes a function, it must be able to do so no matter which subgradient is receives. Conversely, to show a lower bound, it is sufficient to show that for some valid subgradient the algorithm fails. And so, in constructing a \u201chard\u201d instance to optimize we are actually constructing both a function and a subgradient oracle for it, with specific subgradient responses. Therefore, answering the algorithm\u2019s queries during round t according to f ti is valid so long as the subgradient we return is a valid subgradient for fi (the converse need not be true) and the prox returned is exactly the prox of fi. For now, assume that this query-answering strategy is consistent (we will prove this last).\nThen if d = \u2308m\u01eb \u2309+k+1 and if x is an iterate generated both before A completes round k and before it makes \u2308m\u01eb \u2309 queries to hF (so that the dimension is large enough to orthogonalize each vt as described above), then \u3008x, vk\u3009 = 0 by construction. This allows us to bound the suboptimality of F (x) (since \u2308m2 \u2309 functions are queried during each round,\n\u2211m i=1 \u03b4i,r = \u230am2 \u230b):\nF (x) = 1\nm\nm \u2211\ni=1\nfi(x)\n= 1\u221a 2 |b\u2212 \u3008x, v0\u3009|+ \u230am2 \u230b 2m \u221a k k \u2211\nr=1\n|\u3008x, vr\u22121\u3009 \u2212 \u3008x, vr\u3009|\nF is non-negative and F (xb) = 0 where xb = b \u2211k r=0 vr. Choosing b = 1\u221a k+1 makes \u2016xb\u2016 = 1 so that xb \u2208 X . Therefore, F achieves its minimum on X and\nF (x) \u2212 F (x\u2217) = 1\u221a 2 |b\u2212 \u3008x, v0\u3009|+ \u230am2 \u230b 2m \u221a k k \u2211\nr=1\n|\u3008x, vr\u22121\u3009 \u2212 \u3008x, vr\u3009| \u2212 0\n\u2265 1\u221a 2 |b\u2212 \u3008x, v0\u3009|+ 1 6 \u221a k |\u3008x, v0\u3009 \u2212 \u3008x, vk\u3009| = 1\u221a 2 |b\u2212 \u3008x, v0\u3009|+ 1 6 \u221a k |\u3008x, v0\u3009|\n\u2265 min z\u2208R 1\u221a 2 |b\u2212 z|+ 1 6 \u221a k |z| = b\n6 \u221a k\n\u2265 1 12k\nWhere the final inequality holds when k \u2265 1. Setting k = \u230a 112\u01eb\u230b implies F (x)\u2212F (x\u2217) \u2265 \u01eb. Therefore, A must either query hF more than \u2308m\u01eb \u2309 times or complete k rounds to reach an \u01eb-suboptimal solution. Completing each round requires at least \u2308m2 \u2309 queries to hF , so when \u01eb \u2264 112 , this implies a lower bound of\nmin\n(\nm\n\u01eb ,\n\u230a\n1\n12\u01eb\n\u230b\nm\n2\n)\n\u2265 m 48\u01eb\nTo complete the proof, it remains to show that the subgradients and proxs of f ti are consistent with those of fi at every time t. Since every function operates on the (k+1)-dimensional subspace of R\nd spanned by {vr}, it will be convenient to decompose vectors into two components: x = xv + x\u22a5 where xv =\n\u2211k r=0 \u3008x, vr\u3009 vr\nand x\u22a5 = x\u2212 xv. Note that f ti (x) = f ti (xv). Lemma 2. For any t \u2264 k and any x such that xv \u2208 span {v0, v1, ..., vt\u22121}, if function i is queried during round t, then \u2202f ti (x) \u2286 \u2202fi(x).\nProof. All subgradients of fi have the form\nsign ( b\u2212 \u3008x, v0\u3009 )\n\u221a 2\nv0 + 1\n2 \u221a k\nk \u2211\nr=1\n\u03b4i,rsign ( \u3008x, vr\u22121\u3009 \u2212 \u3008x, vr\u3009 ) (vr\u22121 \u2212 vr)\nwhere we define sign(0) = 0. Since function i is queried during round t, \u03b4i,t = 0, and since \u3008x, vr\u22121\u3009 = 0 = \u3008x, vr\u3009 for all r > t, \u2202fi(x) contains all subgradients of the form\nsign ( b\u2212 \u3008x, v0\u3009 )\n\u221a 2\nv0 + 1\n2 \u221a k\nt\u22121 \u2211\nr=1\n\u03b4i,rsign ( \u3008x, vr\u22121\u3009 \u2212 \u3008x, vr\u3009 ) (vr\u22121 \u2212 vr)\nwhich is exactly \u2202f ti (x).\nLemma 3. For any t \u2264 k and any x such that xv \u2208 span {v0, v1, ..., vt\u22121}, if function i is queried during round t then \u2200\u03b2 > 0, proxft\ni (x, \u03b2) = proxfi(x, \u03b2).\nProof. Consider the definition of the prox oracle from equation 3\nproxfi(x, \u03b2) = argmin u\nfi(u) + \u03b2\n2 \u2016x\u2212 u\u20162\n= argmin uv ,u\u22a5\nfi(u v) +\n\u03b2\n2\n\u2225 \u2225xv + x\u22a5 \u2212 uv \u2212 u\u22a5 \u2225 \u2225 2\n= argmin uv\nfi(u v) +\n\u03b2 2 \u2016xv \u2212 uv\u20162 + argmin\nu\u22a5\n\u03b2\n2\n\u2225 \u2225x\u22a5 \u2212 u\u22a5 \u2225 \u2225 2\n= x\u22a5 + proxfi(x v, \u03b2)\nNext, we further decompose xv = x\u2212 + x+ where\nx\u2212 = t\u22121 \u2211\nr=0\n\u3008xv, vr\u3009 vr and x+ = k \u2211\nr=t\n\u3008xv, vr\u3009 vr\nNote that x+ = 0 and since function i is queried during round t, \u03b4i,t = 0. Therefore,\nproxfi(x v , \u03b2) = argmin\nu\u2212,u+ fi(u\n\u2212 + u+) + \u03b2\n2\n\u2225 \u2225x\u2212 \u2212 u\u2212 \u2212 u+ \u2225 \u2225 2 (14)\n= argmin u\u2212,u+ 1\u221a 2 \u2223 \u2223b\u2212 \u2329 u\u2212, v0 \u232a\u2223 \u2223+ 1 2 \u221a k\nt\u22121 \u2211\nr=1\n\u03b4i,r \u2223 \u2223 \u2329 u\u2212, vr\u22121 \u232a \u2212 \u2329 u\u2212, vr \u232a\u2223 \u2223\n+ 1\n2 \u221a k\nk \u2211\nr=t+1\n\u03b4i,r \u2223 \u2223 \u2329 u+, vr\u22121 \u232a \u2212 \u2329 u+, vr \u232a\u2223 \u2223+ \u03b2\n2\n(\n\u2225 \u2225x\u2212 \u2212 u\u2212 \u2225 \u2225 2 + \u2225 \u2225u+ \u2225 \u2225\n2 )\n= proxft i (xv, \u03b2)\nThe last equality follows from the fact that that the minimization is completely separable between u\u2212 and u+, allowing us to minimized over each variable separately. The terms containing u+ are non-negative and can be simultaneously equal to 0 when u+ = 0. Therefore, proxft\ni (x, \u03b2) = proxfi(x, \u03b2).\nThese lemmas show that the subgradients and proxs of f ti at vectors which are queried during round t are consistent with the subgradients and proxs of fi. This confirms that our construction is sound, and completes the proof.\nB.2 Non-smooth and strongly convex components\nTheorem 2. For any L, \u03bb > 0, any 0 < \u01eb < L 2 288\u03bb , any m \u2265 2, and any deterministic algorithm A with access to hF , there exists a dimension d = O (\nmL\u221a \u03bb\u01eb\n)\n, and m functions fi defined over X \u2286 Rd, which are LLipschitz continuous and \u03bb-strongly convex, such that in order to find a point x\u0302 for which F (x\u0302)\u2212F (x\u2217) < \u01eb, A must make \u2126 (\nmL\u221a \u03bb\u01eb\n)\nqueries to hF .\nProof. Suppose towards contradiction that the contrary were true, and there is an A which can find a point x\u0302 for which F (x\u0302) \u2212 F (x\u2217) < \u01eb after at most o (\nmL\u221a \u03bb\u01eb\n)\nqueries to hF . Then A could be used to minimize the\nsum F\u0303 of m functions f\u0303i, which are convex and L-Lipschitz continuous over a domain of {x : \u2016x\u2016 \u2264 B} by adding a regularizer. Let\nF (x) = 1\nm\nm \u2211\ni=1\nfi(x) := 1\nm\nm \u2211\ni=1\nf\u0303i(x) + \u03bb\n2 \u2016x\u20162\nNote that fi is \u03bb-strongly convex and since f\u0303i is L-Lipschitz on the B-bounded domain, fi is (L + \u03bbB)Lipschitz continuous on the same domain. Furthermore, by setting \u03bb = \u01ebB2 ,\nF\u0303 (x) \u2264 F (x) \u2264 F\u0303 (x) + \u01eb 2B2 \u2016x\u20162 \u2264 F\u0303 (x) + \u01eb 2\nBy assumption, A can find an x\u0302 such that F (x\u0302)\u2212 F (x\u2217) < \u01eb2 using o ( m(L+\u03bbB)\u221a \u03bb\u01eb ) = o ( mLB \u01eb ) queries to hF , and \u01eb\n2 > F (x\u0302)\u2212 F (x\u2217) \u2265 F\u0303 (x\u0302)\u2212 F\u0303 (x\u0303\u2217)\u2212 \u01eb 2\nThus x\u0302 is \u01eb-suboptimal for F\u0303 . However, this contradicts the conclusion of theorem 1 when the parameters of the strongly convex problem correspond to parameters of a non-strongly convex problem to which theorem 1 applies. In particular, for any values L > 0, \u03bb > 0, 0 < \u01eb < L 2\n288\u03bb , and dimension d = O ( mL\u221a \u03bb\u01eb ) there is a\ncontradiction.\nB.3 Smooth and not strongly convex components\nTheorem 3. For any \u03b3,B, \u01eb > 0, any m \u2265 2, and any deterministic algorithm A with access to hF , there exists a sufficiently large dimension d = O ( m \u221a \u03b3B2/\u01eb )\n, and m functions fi defined over X = {\nx \u2208 Rd : \u2016x\u2016 \u2264 B } , which are convex and \u03b3-smooth, such that in order to find a point x\u0302 \u2208 Rd for which F (x\u0302)\u2212 F (x\u2217) < \u01eb, A must make \u2126 ( m \u221a \u03b3B2/\u01eb )\nqueries to hF .\nProof. This proof will be very similar to the proof of theorem 1. Without loss of generality, we can assume that \u03b3 = B = 1. For a values a and k to be fixed later, we define:\nfi(x) = 1\n8\n(\n\u03b4i,1\n( \u3008x, v0\u30092 \u2212 2a \u3008x, v0\u3009 ) +\nk \u2211\nr=1\n\u03b4i,r (\u3008x, vr\u22121\u3009 \u2212 \u3008x, vr\u3009)2 + \u03b4i,k \u3008x, vk\u30092 )\nWe define the orthonormal vectors vr \u2208 Rd and indicators \u03b4i,t \u2208 {0, 1} as in the proof of theorem 1. That is, at the end of round t, we set \u03b4i,t = 1 if the algorithm A does not query function i during round t (and zero otherwise) and we construct vt to be orthogonal to {v0, ..., vt\u22121} as well as every point queried by the algorithm so far. Orthogonalizing the vectors is possible as long as the dimension is at least as large as the number of oracle queries A has made so far plus t. As before, we are allowed to construct vt and \u03b4i,t in this way as long as the algorithm\u2019s execution up until round t, and thus our choice of vt and \u03b4i,t, depends only on vr and \u03b4i,r for r < t. We enforce this condition by answering the queries during round t < k according to\nf ti (x) = 1\n8\n(\n\u03b4i,1\n( \u3008x, v0\u30092 \u2212 2a \u3008x, v0\u3009 ) +\nt\u22121 \u2211\nr=1\n\u03b4i,r (\u3008x, vr\u22121\u3009 \u2212 \u3008x, vr\u3009)2 )\nWe will assume for now that this query-answering strategy is self-consistent, and prove it later. This allows us to bound the suboptimality of F (x). Note that since exactly \u2308m2 \u2309 functions are queried each round, \u2211m\ni=1 \u03b4i,r = \u230am2 \u230b, so let\nF t(x) = 1\nm\nm \u2211\ni=1\nf ti (x) + \u03b4i,t \u3008x, vt\u22121\u3009 2\n= \u230am2 \u230b 8m\n( \u3008x, v0\u30092 \u2212 2a \u3008x, v0\u3009+ t\u22121 \u2211\nr=1\n(\u3008x, vr\u22121\u3009 \u2212 \u3008x, vr\u3009)2 + \u3008x, vt\u22121\u30092 )\nThen if d = \u2308 m\u221a \u01eb \u2309 + k + 1, and if x is an iterate generated both before A completes round q := \u230ak2\u230b and before it makes \u2308 m\u221a \u01eb \u2309 queries to hF , then \u3008x, vr\u3009 = 0 for all r \u2265 q by construction. Then, for this x, F q(x) = F k+1(x) = F (x). By first order optimality conditions for F t, its optimum x\u2217t must satisfy that:\n2 \u3008x\u2217t , v0\u3009 \u2212 \u3008x\u2217t , v1\u3009 = a \u3008x\u2217t , vr\u22121\u3009 \u2212 2 \u3008x\u2217t , vr\u3009+ \u3008x\u2217t , vr+1\u3009 = 0 for 1 \u2264 r \u2264 t\u2212 2\n\u3008x\u2217t , vt\u22122\u3009 \u2212 2 \u3008x\u2217t , vt\u22121\u3009 = 0\nIt is straightforward to confirm that the solution to this system of equations is\nx\u2217t = a t\u22121 \u2211\nr=0\n(\n1\u2212 r + 1 t+ 1\n)\nvr\nthat\nF t(x\u2217t ) = \u2212 a2\u230am2 \u230b 8m ( 1\u2212 1 t+ 1 )\nand that\n\u2016x\u2217t \u20162 = a2 t\u22121 \u2211\nr=0\n(\n1\u2212 r + 1 t+ 1\n)2\n= a2\n(\nt\u2212 2 t+ 1\nt\u22121 \u2211\nr=0\n(r + 1) + 1\n(t+ 1)2\nt\u22121 \u2211\nr=0\n(r + 1)2\n)\n= a2 ( t\u2212 2 t+ 1 t(t+ 1) 2 +\n1 (t+ 1)2 t(t+ 1)(2t+ 1) 6\n)\n\u2264 a 2t\n3\nThus, we set a = \u221a\n3 k+1 , ensuring\n\u2225 \u2225x\u2217k+1 \u2225 \u2225 = 1 so that x\u2217k+1 = x \u2217 \u2208 X . Furthermore, for the iterate x made\nbefore q rounds of queries,\nF (x) \u2212 F (x\u2217) = F q(x) \u2212 F k+1(x\u2217k+1) \u2265 F q(x\u2217q)\u2212 F k+1(x\u2217k+1)\n= \u2212 3\u230a m 2 \u230b\n8m(k + 1)\n(\n1\u2212 1 \u230ak2 \u230b+ 1\n)\n+ 3\u230am2 \u230b\n8m(k + 1)\n(\n1\u2212 1 k + 2\n)\n\u2265 1 32k2\nwhere the last inequality holds as long as k \u2265 2. So, when \u01eb < 1128 and we let k = \u230a 1\u221a32\u01eb\u230b, this ensures that\nF (x)\u2212 F (x\u2217) = F q(x)\u2212 F k+1(x\u2217k+1) \u2265 \u01eb\nand therefore, A must complete at least q rounds or make more than \u2308 m\u221a \u01eb \u2309 queries to hF in order to reach an \u01eb-suboptimal point. This implies a lower bound of\nmin\n(\u2308\nm\u221a \u01eb\n\u2309\n, q \u2308m\n2\n\u2309\n)\n\u2265 m 16 \u221a 6\u01eb\nTo complete the proof, it remains to show that the gradient and prox of f ti is consistent with those of fi at every time t. Since every function operates on the (k + 1)-dimensional subspace of Rd spanned by {vr}, it will be convenient to decompose vectors into two components: x = xv + x\u22a5 where xv =\n\u2211k r=0 \u3008x, vr\u3009 vr and\nx\u22a5 = x\u2212 xv. Note that f ti (x) = f ti (xv). Lemma 4. For any t \u2264 k and any x such that xv \u2208 span {v0, v1, ..., vt\u22121}, if function i is queried during round t, then \u2207f ti (x) = \u2207fi(x).\nProof. Since function i is queried during round t, \u03b4i,t = 0 so\n\u2207fi(x) = 1\n4\n( \u03b4i,1 (\u3008x, v1\u3009 v0 \u2212 2v0) + t\u22121 \u2211\nr=1\n\u03b4i,r (\u3008x, vr\u22121\u3009 \u2212 \u3008x, vr\u3009) (vr\u22121 \u2212 vr) ) = \u2207f ti (x)\nLemma 5. For any t \u2264 k and any x such that xv \u2208 span {v0, v1, ..., vt\u22121}, if function i is queried during round t then \u2200\u03b2 > 0, proxft\ni (x, \u03b2) = proxfi(x, \u03b2).\nProof. Up until the last step, this proof is identical to the proof of lemma 3, thus we pick up at (14):\nproxfi(x v, \u03b2) = argmin\nu\u2212,u+ fi(u\n\u2212 + u+) + \u03b2\n2\n\u2225 \u2225x\u2212 \u2212 u\u2212 \u2212 u+ \u2225 \u2225 2\n= argmin u\u2212,u+\n1\n8\n(\n\u03b4i,1\n(\n\u2329 u\u2212, v0 \u232a2 \u2212 2a \u2329 u\u2212, v0 \u232a\n)\n+\nt\u22121 \u2211\nr=1\n\u03b4i,r (\u2329 u\u2212, vr\u22121 \u2212 vr \u232a)2\n+\nk \u2211\nr=t+1\n\u03b4i,r (\u2329 u+, vr\u22121 \u2212 vr \u232a)2 + \u03b4i,k \u2329 u+, vk \u232a2\n)\n+ \u03b2\n2\n(\n\u2225 \u2225x\u2212 \u2212 u\u2212 \u2225 \u2225 2 + \u2225 \u2225u+ \u2225 \u2225\n2 )\n= proxfti (x v, \u03b2)\nThe final step comes from the fact that the argmin is separable over u\u2212 and u+, meaning we can minimize the two terms individually. The terms which contain u+ are non-negative and equal to zero when u+ = 0.\nThese lemmas show that the gradient and prox of f ti at vectors which are queried during round t are consistent with the gradient and prox of fi. This confirms that our construction is sound. This proves the lower bound for \u01eb < \u03b3B 2\n128 , we can extend the same lower bound to \u01eb \u2265 \u03b3B2\n128 using the following, very simple construction. Let\nfi(x) =\n{\n0 if function i is queried in the first m\u2212 1 queries 2m\u01eb \u3008x, v\u3009\nwhere v is a unit vector that is orthogonal to all of the first m\u22121 queries. This function is trivially 1-smooth. By construction, the algorithm must make at least m queries to learn the identity of v. Until it has done so, any iterate will have objective value zero, while the optimum F (x\u2217) = F (v) = \u22122\u01eb. Therefore, the algorithm must make at least m queries to reach an \u01eb-suboptimal solution. For \u01eb \u2265 \u03b3B 2\n128\nm \u2265 m \u221a \u03b3B2\n128\u01eb\nTherefore a lower bound of \u2126\n(\nm \u221a \u03b3B2\n\u01eb\n)\napplies for any \u01eb > 0.\nRemark: If make the additional assumption that F is minimized on the interior of X , since 0 is O(\u03b3B2)suboptimal, only 0 < \u01eb < \u03b3B 2\n128 gives a non-trivial lower bound. This lower bound is shown by the first construction presented in the previous proof.\nB.4 Smooth and strongly convex components\nTheorem 4. For any \u03b3, \u03bb > 0 such that \u03b3\u03bb > 73, any \u01eb > 0, any \u01eb0 > 3\u03b3\u01eb \u03bb , any m \u2265 2, and any deterministic algorithm A with access to hF , there exists a sufficiently large dimension d = O ( m \u221a \u03b3 \u03bb log ( \u03bb\u01eb0 \u03b3\u01eb )) , and m functions fi defined over X \u2286 Rd, which are \u03b3-smooth and \u03bb-strongly convex and where F (0)\u2212 F (x\u2217) = \u01eb0, such that in order to find a point x\u0302 for which F (x\u0302)\u2212 F (x\u2217) < \u01eb, A must make \u2126 ( m \u221a\n\u03b3 \u03bb log\n(\n\u03bb\u01eb0 \u03b3\u01eb\n))\nqueries\nto hF .\nProof. We will prove the theorem for 1-smooth and \u03bb-strongly convex components for any \u03bb < 173 . This can be extended to arbitrary constants \u03b3 and \u03bb\u2032 by taking \u03bb = \u03bb \u2032\n\u03b3 .\nFor any k and for \u03b6 and C to be defined later, let\nfi(x) = 1\u2212 \u03bb 8\n(\n\u03b4i,1\n( \u3008x, v0\u30092 \u2212 2C \u3008x, v0\u3009 ) + \u03b4i,k\u03b6 \u3008x, vk\u30092 + k \u2211\nr=1\n\u03b4i,r \u3008x, vr\u22121 \u2212 vr\u30092 ) + \u03bb\n2 \u2016x\u20162\nwhere the vectors vr and indicators \u03b4i,r are defined in the same way as in the previous proof. This function is just a multiple of the construction in the proof of theorem 3 plus the \u03bb \u2016x\u20162 /2 term. It is clear that the norm term is uninformative for learning the identity of vectors vr, as the component of the gradients and proxs which is due to that term is simply a scaling of the query point. Thus, for any iterate x generated by A before completing t rounds of optimization, \u3008x, vr\u3009 = 0 for all r \u2265 t so long as the dimension is greater than the total number of queries made to hF so far plus k+1; a fact which follows directly from the previous proof. Since exactly \u2308m2 \u2309 functions are queried per round, \u2211m i=1 \u03b4i,r = \u230am2 \u230b and thus\nF (x) = \u03bb(Q \u2212 1)\n8\n(\n\u3008x, v0\u30092 \u2212 2C \u3008x, v0\u3009+ \u03b6 \u3008x, vk\u30092 + k \u2211\nr=1\n\u3008x, vr\u22121 \u2212 vr\u30092 ) + \u03bb\n2 \u2016x\u20162\nwhere\nQ = \u230am2 \u230b m ( 1 \u03bb \u2212 1) + 1\nBy the first order optimality conditions for F (x), its optimum x\u2217 must satisfy that:\n2 Q+ 1 Q\u2212 1 \u3008x \u2217, v0\u3009 \u2212 \u3008x\u2217, v1\u3009 = C\n\u3008x\u2217, vr\u22121\u3009 \u2212 2 Q+ 1 Q\u2212 1 \u3008x \u2217, vr\u3009+ \u3008x\u2217, vr+1\u3009 = 0\n(\n1 + \u03b6 + 4\nQ\u2212 1\n)\n\u3008x\u2217, vk\u3009 \u2212 \u3008x\u2217, vk\u22121\u3009 = 0\nDefining q := \u221a Q\u22121\u221a Q+1 < 1 and setting \u03b6 = 1\u2212 q, it is straightforward to confirm that\nx\u2217 = C k \u2211\nr=0\nqr+1vr\nand also that\nF (x\u2217) = \u2212\u03bbC 2\n8\n(\n\u221a Q\u2212 1 )2\nThus, F (0)\u2212F (x\u2217) = \u03bbC28 (\u221a Q\u2212 1 )2 , so by choosing C appropriately, we can make the initial suboptimality of our construction take any value \u01eb0. Since F is \u03bb-strongly convex, for any xt, F (xt)\u2212F (x\u2217) \u2265 \u03bb2 \u2016xt \u2212 x\u2217\u2016 2 . Let xt be an iterate which is generated before t rounds of optimization have been completed, implying that \u3008xt, vr\u3009 = 0 for all r \u2265 t. So\nF (xt)\u2212 F (x\u2217) F (0)\u2212 F (x\u2217) \u2265 \u03bb 2 \u2016xt \u2212 x\u2217\u2016 2 \u03bbC2\n8\n(\u221a Q\u2212 1 )2\n\u2265 4 C2\nC2 \u2211k r=t q 2r+2\n(\u221a Q\u2212 1 )2\n= 4(q2t+2 \u2212 q2k+4)\n(1\u2212 q2) (\u221a Q\u2212 1 )2\n= (q2t \u2212 q2k+2)\u221a\nQ\nIf we set k + 1 = \u2308 t\u2212 12 log q \u2309 then\nF (xt)\u2212 F (x\u2217) F (0)\u2212 F (x\u2217) \u2265 (q2t \u2212 q2k+2)\u221a Q\n\u2265 q 2t\n2 \u221a Q\n= 1\n2 \u221a Q exp\n(\n\u22122t log 1 q\n)\n= 1\n2 \u221a Q exp\n( \u22122t log ( 1 + 2\u221a\nQ\u2212 1\n))\n\u2265 1 2 \u221a Q exp ( \u22124t\u221a Q\u2212 1 )\nand when t = \u230a\u221a\nQ\u22121 4 log \u01eb0 2 \u221a Q\u01eb\n\u230b\nF (xt)\u2212 F (x\u2217) F (0)\u2212 F (x\u2217) \u2265 \u01eb\n\u01eb0\nTherefore, the algorithm must complete at least t rounds of queries before it can reach an \u01eb-suboptimal point. If \u01eb0 > 3\u01eb \u03bb and \u03bb < 1 73 , since each round includes at least m 2 oracle queries, this implies a lower bound of m\n2 \u230a\u221a Q\u2212 1 4 log \u01eb0 2 \u221a Q\u01eb \u230b \u2265 m 40 \u221a \u03bb log \u01eb0 2 \u221a Q\u01eb \u2265 m 40 \u221a \u03bb log \u221a \u03bb\u01eb0 2\u01eb\nqueries to hF in order to reach an \u01eb-suboptimal point."}, {"heading": "C Lower bounds for randomized algorithms", "text": "To prove the deterministic lower bounds, we constructed vectors vr adversarially, orthogonalizing them to queries made by the algorithm. In the randomized setting, this is impossible, as we cannot anticipate query points. Our solution was to instead draw the important directions vi,r randomly in high dimensions. The intuition is that a given vector, in this case the query made by the algorithm, will have a very small inner product with a random unit vector with high probability if the dimension is large enough.\nUsing this fact, we construct helper functions \u03c8c and \u03c6c to replace the absolute and squared difference functions used in the deterministic lower bounds. These functions are both flat at 0 on the interval [\u2212c, c], meaning that the algorithm\u2019s query needs to have a significant inner product with vi,r before the oracle needs to give that vector away as a gradient or prox. We will show that each one of our constructions satisfies the following property:\nProperty 1. For all i, all t \u2264 k and x such that \u2200r \u2265 t |\u3008x, vi,r\u3009| < c2 , if t is odd, then\n\u2202fi,1(x) \u2286 span {x, vi,0, ..., vi,t\u22121} and \u2202fi,2(x) \u2286 span {x, vi,0, ..., vi,t}\nproxfi,1 (x, \u03b2) \u2208 span {x, vi,0, ..., vi,t\u22121} and proxfi,2(x, \u03b2) \u2208 span {x, vi,0, ..., vi,t} and if t is even, then\n\u2202fi,1(x) \u2286 span {x, vi,0, ..., vi,t} and \u2202fi,2(x) \u2286 span {x, vi,0, ..., vi,t\u22121}\nproxfi,1 (x, \u03b2) \u2208 span {x, vi,0, ..., vi,t} and proxfi,2(x, \u03b2) \u2208 span {x, vi,0, ..., vi,t\u22121}\nIn other words, when x has a small inner product with vi,r for all r \u2265 t, then querying either fi,1 or fi,2 at x will reveal at most vi,t. Our bounds on the complexity of optimizing our functions are based on the principle that the algorithm can only learn one vi,r per query, so we need to control the probability that the hypotheses of these lemmas hold for every query made by the algorithm. In this section, we will bound how large the dimensionality of the problem needs to be to ensure that with high probability, only one vector is revealed to the algorithm by each oracle response.\nWe consider the following setup:\n\u2022 For i = 1, 2, ...,m/2, fi,1 and fi,2 are pairs of component functions that satisfy Property 1 to be optimized by the randomized algorithm, we can assume that m is even by letting the last component function be 0 if m is odd, at the cost of a factor of (m\u2212 1)/m to the complexity.\n\u2022 For i = 1, 2, ...,m/2 and r = 1, ..., k, {vi,r} is a uniformly random set of orthonormal vectors in Rd. \u2022 We denote the nth query made by the algorithm q(n) = ( i(n), j(n), x(n), \u03b2(n) ) , which is a query to\nfunction fi(n),j(n) at the point x (n) with the prox parameter \u03b2(n). We require that \u2203B s.t.\n\u2225 \u2225x(n) \u2225 \u2225 \u2264 B for all n; this will be justified in the individual lower bound proofs. The nth query is allowed to depend on the previous n \u2212 1 queries, the oracle\u2019s responses to those queries, and the randomness in the algorithm.\n\u2022 For n = 1, ..., N , let Sn = span { x(t) : t < n, i(t) = i(n) } and let \u22a5 (Sin) be its orthogonal complement. \u2022 Let PSv be the projection of the vector v onto the subspace S, and P\u22a5S v be its projection onto \u22a5 (S).\n\u2022 Let t(n) = \u2211n\u22121\nn\u2032=1 1\n(\ni(n \u2032) = i(n)\n)\n. The counter t(n) keeps track of the number of times that function\nfi(n),1 or fi(n),2 has been queried by the algorithm before the n th query.\n\u2022 Let Un = { vi(n),r : r \u2265 t(n) } . This is the set of vectors vi,r which are supposed to be \u201cunknown\u201d to\nthe algorithm before the nth query.\nUltimately, we want to prove the following statement:\nP\n( \u2200n \u2200v \u2208 Un \u2223 \u2223 \u2223 \u2329 x(n), v \u232a\u2223 \u2223 \u2223 < c\n2\n)\n> 1\u2212 \u03b4 (15)\nwhen the dimension is adequately large. The main difficulty here is that queries made by the algorithm are allowed to depend on the oracle\u2019s responses to previous queries, which in turn depends on the vectors v. Therefore, there is a complicated statistical dependence between x(n) and v \u2208 Un for each n > 1, which makes analyzing the distribution of the inner product hard. We will get around this by proving a slightly different statement, and then show that it implies (15).\nDefine the following \u201cgood\u201d event:\nGn =\n[\n\u2200v \u2208 Un\n\u2223 \u2223 \u2223 \u2223 \u2223 \u2329 x(n) \u2225 \u2225x(n) \u2225 \u2225 , P\u22a5Snv \u232a\u2223 \u2223 \u2223 \u2223 \u2223 < \u03b1 ]\nwhere \u03b1 = c 2B( \u221a N+1) . The following lemma shows why Gn is a useful thing to look at: Lemma 6. For any c > 0 and N , [\n\u22c2N n=1 Gn\n]\n=\u21d2 [ \u2200n \u2264 N \u2200v \u2208 Un \u2329 x(n), v \u232a < c2 ] .\nProof. Because \u2225 \u2225x(n) \u2225 \u2225 \u2264 B and \u22c2nn\u2032=1 Gn\u2032 \u2223\n\u2223 \u2223\n\u2329 x(n), v \u232a\u2223 \u2223 \u2223 = \u2225 \u2225 \u2225x(n) \u2225 \u2225 \u2225\n\u2223 \u2223 \u2223 \u2223 \u2223 \u2329 x(n) \u2225 \u2225x(n) \u2225 \u2225 , P\u22a5Snv \u232a + \u2329 x(n) \u2225 \u2225x(n) \u2225 \u2225 , PSnv \u232a\u2223 \u2223 \u2223 \u2223 \u2223\n\u2264 B ( \u03b1+ \u221a n\u2212 1\u03b1 ) \u2264 B\u03b1+B\u03b1 \u221a N \u2264 c 2\nNow that we know that [\u2200n Gn] implies (15), we prove the following:\nLemma 7. For any set of functions satisfying Property 1, any c > 0, any 0 < \u03b4 < 1, any k,N , and any dimension d \u2265 32B2Nc2 log ( kN \u03b4 ) , P ( \u2200n \u2264 N \u2200v \u2208 Un \u2223 \u2223 \u2329 x(n), v \u232a\u2223 \u2223 < c2 ) > 1\u2212 \u03b4\nProof.\nP\n(\nN \u22c2\nn=1\nGn\n)\n= N \u220f\nn=1\nP (Gn | G<n)\nwhere G<n is shorthand for \u22c2n\u22121\nn\u2032=1 Gn\u2032 . We lower bound each term of the product by showing that for any sequence of n queries q(1), ..., q(n)\nP\n\n  \u2200v \u2208 Un\n\u2223 \u2223 \u2223 \u2223 \u2223 \u2329 x(n) \u2225 \u2225x(n) \u2225 \u2225 , P\u22a5Snv \u232a\u2223 \u2223 \u2223 \u2223 \u2223 < \u03b1 G<n, q (1), ..., q(n)\n\n \n= P\n\n  \u2200v \u2208 Un\n\u2223 \u2223 \u2223 \u2223 \u2223 \u2329 P\u22a5Snx (n) \u2225 \u2225x(n) \u2225 \u2225 , P\u22a5Snv \u232a\u2223 \u2223 \u2223 \u2223 \u2223 < \u03b1 G<n, q (1), ..., q(n)\n\n \n> 1\u2212 \u03b4\u2032\nMarginally, each v is uniformly random on the unit sphere. Consequently, v projected onto any fixed subspace is independent of the projection onto the orthogonal complement of that subspace when conditioned on the norm of the projection, so P\u22a5Snv \u22a5 PSnv \u2223\n\u2223 \u2016PSnv\u2016. Conditioned on G<n, 6 and Property 1 ensures that the oracle\u2019s responses to the first n \u2212 1 queries were independent of v. So, v is independent of the queries conditioned on G<n and \u2016PStv\u2016, and those events depend only on v\u2019s projection onto the subspace St. Therefore, P \u22a5 Sn v remains uniformly distributed on the sphere of radius \u221a\n1\u2212 \u2016PSnv\u20162 in the subspace \u22a5 (Sn). Furthermore, since the projection operator is non-expansive:\nP\n\n \n\u2223 \u2223 \u2223 \u2223 \u2223 \u2329 P\u22a5Snx (n) \u2225 \u2225x(n) \u2225 \u2225 , P\u22a5Snv \u232a\u2223 \u2223 \u2223 \u2223 \u2223 \u2265 \u03b1 v \u2208 Un, G<n, q(1), ..., q(n), \u2016PSnv\u2016\n\n \n< P\n\n \n\u2223 \u2223 \u2223 \u2223 \u2223 \u2329 P\u22a5Snx (n) \u2225\n\u2225P\u22a5Snx (n)\n\u2225 \u2225\n, P\u22a5Snv \u2225\n\u2225P\u22a5Snv \u2225 \u2225\n\u232a\u2223\n\u2223 \u2223 \u2223 \u2223 \u2265 \u03b1 v \u2208 Un, G<n, q(1), ..., q(n), \u2016PSnv\u2016\n\n \nLet d\u2032 = dim(\u22a5 (Sn)), then this is the inner product between two unit vectors, one fixed, and one uniformly random on the unit sphere in Rd \u2032 . The set of vectors for which the absolute value of the inner product is greater than \u03b1 are two \u201cends\u201d of the sphere which lie above and below circles of radius \u221a 1\u2212 \u03b12. The total surface area of the two portions of the sphere is strictly less than the surface area of the sphere of radius\u221a 1\u2212 \u03b12. Therefore,\nP\n\n \n\u2223 \u2223 \u2223 \u2223 \u2223 \u2329 P\u22a5Snx (n) \u2225\n\u2225P\u22a5Snx (n)\n\u2225 \u2225\n, P\u22a5Snv \u2225\n\u2225P\u22a5Snv \u2225 \u2225\n\u232a\u2223\n\u2223 \u2223 \u2223 \u2223 \u2265 \u03b1 v \u2208 Un, G<n, q(1), ..., q(n), \u2016PSnv\u2016\n\n \n<\n\u221a 1\u2212 \u03b12d \u2032\u22121\n1d\u2032\u22121 =\n( 1\u2212 \u03b12 )\nd\u2032\u22121 2\nand since d\u2032 \u2265 d\u2212 n and 1\u2212 x \u2264 e\u2212x\n( 1\u2212 \u03b12 ) d\u2032\u22121 2 \u2264 e\u2212 \u03b12(d\u2212n\u22121) 2 \u2264 e\u2212 \u03b12(d\u2212N\u22121) 2\nUsing this result and a union bound over the set Un which has size at most k:\nP\n\n  \u2200v \u2208 Un\n\u2223 \u2223 \u2223 \u2223 \u2223 \u2329 x(n) \u2225 \u2225x(n) \u2225 \u2225 , P\u22a5Snv \u232a\u2223 \u2223 \u2223 \u2223 \u2223 < \u03b1 G<n, q (1), ..., q(n), \u2016PSnv\u2016\n\n  > 1\u2212 ke\u2212\n\u03b12(d\u2212N\u22121) 2\nSince this argument applied for any n, any sequence of queries q(1), ..., q(n), and any value of \u2016PSnv\u2016 that is consistent with G<n, this implies that\n\u2200n P (Gn | G<n) > 1\u2212 ke\u2212 \u03b12(d\u2212N\u22121) 2\n=\u21d2 P ( N \u22c2\nn=1\nGn\n)\n>\n(\n1\u2212 ke\u2212 \u03b12(d\u2212N\u22121) 2\n)N\n\u2265 1\u2212 kNe\u2212 \u03b12(d\u2212N\u22121) 2\nSo, when\nd \u2265 2 \u03b12 log\n(\nkN\n\u03b4\n)\n+N + 1\nthen\nP\n(\nN \u22c2\nn=1\nGn\n)\n> 1\u2212 kNe\u2212 \u03b12(d\u2212N\u22121) 2\n\u2265 1\u2212 kNe\u2212 log kN\u03b4 = 1\u2212 \u03b4\nApplying lemma 6 completes the proof.\nTogether, Lemma 7 and Property 1 allow us to ensure that any algorithm can only learn one important vector per query with high probability as long as the dimension is large enough. What is left is to show that Property 1 holds for each of our constructions and to bound the suboptimality of any iterate that has small inner product with the vectors in Un.\nC.1 Non-smooth and not strongly convex components\nWe first consider the Lipschitz and non-strongly convex setting and prove theorem 5:\nTheorem 5. For any L,B > 0, any 0 < \u01eb < LB 10 \u221a m , any m \u2265 2, and any randomized algorithm A with access to hF , there exists a dimension d = O ( L4B6 \u01eb4 log ( LB \u01eb ) )\n, and m functions fi defined over X = {\nx \u2208 Rd : \u2016x\u2016 \u2264 B } , which are convex and L-Lipschitz continuous, such that to find a point x\u0302 for which\nE [F (x\u0302)\u2212 F (x\u2217)] < \u01eb, A must make \u2126 ( m+ \u221a mLB \u01eb ) queries to hF .\nAs shown in Equations (9) and (10), we define\n\u03c8c(z) = max (0, |z| \u2212 c)\nand for values b, c, and k to be fixed later we define m/2 pairs of functions, indexed by i = 1..m/2:\nfi,1(x) = 1\u221a 2 |b\u2212 \u3008x, vi,0\u3009|+ 1 2 \u221a k\nk \u2211\nr even\n\u03c8c (\u3008x, vi,r\u22121\u3009 \u2212 \u3008x, vi,r\u3009)\nfi,2(x) = 1\n2 \u221a k\nk \u2211\nr odd\n\u03c8c (\u3008x, vi,r\u22121\u3009 \u2212 \u3008x, vi,r\u3009)\nAssume for now that m is even. If m is odd, then we simply set one of the functions to 0 and the oracle complexity is reduced by a factor proportional to m\u22121m . At the end of this proof, we will show that the functions fi,\u00b7 satisfy Property 1. Since the domain, and therefore the queries made to the oracle are bounded by B, Property 1 and Lemma 7 ensure that when the dimension is at least d = 32B 2N\nc2 log(10kN), for iterate x generated after N oracle queries, \u3008x, vi,r\u3009 \u2265 c2 for no more than N vectors vi,r with probability 9 10 . We now bound the suboptimality of (fi,1 + fi,2)/2 for any x where \u3008x, vi,k\u3009 < c2 .\n1 2 (fi,1(x) + fi,2(x)) = 1 2 \u221a 2 |b\u2212 \u3008x, vi,0\u3009|+ 1 4 \u221a k\nk \u2211\nr=1\n\u03c8c (\u3008x, vi,r\u22121\u3009 \u2212 \u3008x, vi,r\u3009)\nIt is straightforward to confirm that this function is minimized when \u3008x, vi,r\u3009 = b for all r. Since this is also true for every i, F is minimized at xb = b \u2211 m 2\ni=1 \u2211k r=0 vi,r. In order that \u2016xb\u2016 = 1 so that xb \u2208 X , we set\nb = \u221a\n2 m(k+1) . Thus,\n1 2 (fi,1(x) + fi,2(x)) \u2212 1 2 (fi,1(x \u2217) + fi,2(x \u2217)) \u2265 1 2 (fi,1(x) + fi,2(x)) \u2212 0\n\u2265 1 2 \u221a 2 |b\u2212 \u3008x, vi,0\u3009|+ 1 4 \u221a k\nk \u2211\nr=1\n|\u3008x, vi,r\u22121\u3009 \u2212 \u3008x, vi,r\u3009| \u2212 c\n\u2265 \u2212 k 4 \u221a k c+ 1 2 \u221a 2 |b\u2212 \u3008x, vi,0\u3009|+ 1 4 \u221a k |\u3008x, vi,0\u3009 \u2212 \u3008x, vi,k\u3009| \u2265 \u2212 k 4 \u221a k c+ 1 2 \u221a 2 |b\u2212 \u3008x, vi,0\u3009|+ 1 4 \u221a k |\u3008x, vi,0\u3009| \u2212 1 4 \u221a k c 2 \u2265 \u22122k + 1 8 \u221a k c+min z\u2208R 1 2 \u221a 2 |b\u2212 z|+ 1 4 \u221a k |z| = \u22122k + 1 8 \u221a k c+ b 4 \u221a k \u2265 \u22122k + 1 8 \u221a k c+ 1 4k \u221a m\nTherefore, we set c = \u01eb\u221a k and k = \u230a 1 10\u01eb \u221a m \u230b so that\n1 2 (fi,1(x) + fi,2(x)) \u2212 1 2 (fi,1(x \u2217) + fi,2(x \u2217)) \u2265 \u2212 \u01eb 2 + 5 2 \u01eb = 2\u01eb\nThis ensures that if \u3008x, vi,k\u3009 < c2 for at least m/4 i\u2019s, then x cannot be \u01eb-suboptimal for F . Therefore, after N = m(k+1)4 queries in dimension d = 32B2N c2 log(10kN) = 32B2 c2 ( m(k+1) 4 ) log ( 10mk(k+1) 4 )\nthen F (x)\u2212 F (x\u2217) \u2265 \u01eb with probability 910 . When \u01eb < 110\u221am , A must make at least\nm(k + 1) 4 \u2265 m 4 +\n\u221a m\n80\u01eb\nqueries with probability 910 Finally, we prove that Property 1 holds for our construction:\nProof of Propetry 1 for Lipschitz, non-strongly convex construction. First we prove the properties about the gradients:\nConsider the case when t is odd. From (9), it is clear that d\u03c8cdz (z) = 0 when |z| < c. Furthermore, for r > t,\n|\u3008x, vi,r\u22121\u3009 \u2212 \u3008x, vi,r\u3009| < c. Therefore, any subgradient g1 \u2208 \u2202fi,1(x) and g2 \u2208 \u2202fi,2(x) can be expressed as\ng1 = sign(b\u2212 \u3008x, vi,0\u3009)\u221a\n2 vi,0 +\n1\n2 \u221a k\nt\u22121 \u2211\nr even\n\u03c8\u2032c(\u3008x, vi,r\u22121\u3009 \u2212 \u3008x, vi,r\u3009)(vi,r\u22121 \u2212 vi,r)\ng2 = 1\n2 \u221a k\nt \u2211\nr odd\n\u03c8\u2032c(\u3008x, vi,r\u22121\u3009 \u2212 \u3008x, vi,r\u3009)(vi,r\u22121 \u2212 vi,r)\nwhere sign(0) can take any value in the range [\u22121, 1] and where \u03c8\u2032c is a subderivative of \u03c8c. It is clear from these expressions that \u2202fi,1(x) \u2286 span {vi,0, ..., vi,t\u22121} and \u2202fi,2(x) \u2286 span {vi,1, ..., vi,t}. The proof for the case when t is even follows the same line of reasoning.\nWe now prove the properties about the proxs:\nSince each pair of functions fi,\u00b7 operates on a separate (k + 1)-dimensional subspace, it will be useful to decompose vectors into x = xvi + x \u22a5 i where x v i = \u2211k r=0 \u3008x, vi,r\u3009 vi,r and x\u22a5i = x\u2212 xvi . First, note that\nproxfi,1(x, \u03b2) = argmin u\nfi,1(u) + \u03b2\n2 \u2016x\u2212 u\u20162\n= argmin uv i ,u\u22a5 i\nfi,1(u v i ) +\n\u03b2\n2\n\u2225 \u2225xvi + x \u22a5 i \u2212 uvi \u2212 u\u22a5i \u2225 \u2225 2\n= argmin uv i ,u\u22a5 i\nfi,1(u v i ) +\n\u03b2 2 \u2016xvi \u2212 uvi \u20162 + \u2225 \u2225x\u22a5i \u2212 u\u22a5i \u2225 \u2225 2\n= argmin uv i\nfi,1(u v i ) +\n\u03b2 2 \u2016xvi \u2212 uvi \u20162 + argmin\nu\u22a5 i\n\u03b2\n2\n\u2225 \u2225x\u22a5i \u2212 u\u22a5i \u2225 \u2225 2\n= x\u22a5i + argmin uv i fi,1(u v i ) +\n\u03b2 2 \u2016xvi \u2212 uvi \u2016 2\n= x\u22a5i + proxfi,1(x v i , \u03b2)\n(and similarly for fi,2). From there, the proof is similar to the proof of lemma 3. First, consider the function fi,2 and let t\n\u2032 \u2265 t be the smallest even number which is not smaller than t. It will be convenient to further decompose vectors into xvi = x \u2212 + x+ where x\u2212 = \u2211t\u2032\u22121 r=0 \u3008xvi , vi,r\u3009 vi,r and x+ = \u2211k r=t\u2032 \u3008xvi , vi,r\u3009 vi,r . So\nfi,2(x \u2212 + x+) =\n1\n2 \u221a k\n\u2211\nr\u2208{1,3,...,t\u2032\u22121} \u03c8c\n(\u2329 x\u2212, vi,r\u22121 \u232a \u2212 \u2329 x\u2212, vi,r \u232a)\n+ 1\n2 \u221a k\n\u2211\nr\u2208{t\u2032+1,t\u2032+3,...k} \u03c8c\n(\u2329 x+, vi,r\u22121 \u232a \u2212 \u2329 x+, vi,r \u232a)\n= fi,2(x \u2212) + fi,2(x +)\nTherefore,\nproxfi,2(x v i , \u03b2) = argmin\nu\u2212,u+ fi,2(u\n\u2212 + u+) + \u03b2\n2\n\u2225 \u2225x\u2212 + x+ \u2212 u\u2212 \u2212 u+ \u2225 \u2225 2\n= argmin u\u2212,u+\nfi,2(u \u2212 + u+) +\n\u03b2\n2\n\u2225 \u2225x\u2212 \u2212 u\u2212 \u2225 \u2225 2 +\n\u03b2\n2\n\u2225 \u2225x+ \u2212 u+ \u2225 \u2225 2\n= argmin u\u2212,u+\nfi,2(u \u2212) + fi,2(u +) + \u03b2\n2\n\u2225 \u2225x\u2212 \u2212 u\u2212 \u2225 \u2225 2 +\n\u03b2\n2\n\u2225 \u2225x+ \u2212 u+ \u2225 \u2225 2\n= argmin u\u2212\nfi,2(u \u2212) +\n\u03b2\n2\n\u2225 \u2225x\u2212 \u2212 u\u2212 \u2225 \u2225 2 + argmin\nu+ fi,2(u\n+) + \u03b2\n2\n\u2225 \u2225x+ \u2212 u+ \u2225 \u2225 2\nSince |\u3008x+, vi,r\u3009| < c2 for all r \u2265 t, |\u3008x+, vi,r\u22121\u3009 \u2212 \u3008x+, vi,r\u3009| < c for r > t, which implies that fi,2(x+) = 0.\nTherefore, the objective of the second argmin is non-negative and is equal to zero when u+ = x+ so\nproxfi,2(x v i , \u03b2) = x + + argmin u\u2212 fi,2(u \u2212) +\n\u03b2\n2\n\u2225 \u2225x\u2212 \u2212 u\u2212 \u2225 \u2225 2\nTherefore, when t is even, t\u2032 = t and proxfi,2(x, \u03b2) \u2208 span {x, vi,0, ..., vi,t\u22121}, and when t is odd, t\u2032 = t + 1 and proxfi,2 (x, \u03b2) \u2208 span {x, vi,0, ..., vi,t}. A very similar line of reasoning can be used to show the statement for fi,1.\nRemark: As was mentioned before, Lemma 7 applies when the norm of every query point is bounded by B. Since all points in the domain of the optimization problem have norm bounded by B, this is not problematic. However, we can slightly modify our construction to make optimizing F hard even for algorithms that are allowed to query outside of the domain.\nWe could redefine our functions as follows:\nf \u2032i,j(x) =\n{\nfi,j(x) \u2016x\u2016 \u2264 B fi,j ( B x\u2016x\u2016 ) + L (\u2016x\u2016 \u2212B) \u2016x\u2016 > B\nf \u2032i,j is still continuous, and L-Lipschitz, and it also has the property that it behaves exactly like fi,j on B-ball. However, querying the oracle of f \u2032i,j outside of the B-ball gives no more information about the function than querying at B x\u2016x\u2016 . In fact, an algorithm that was only allowed to query within the B-ball would be able to simulate the oracle of F \u2032. Therefore, since the algorithm that is not allowed to query at large vectors cannot optimize F \u2032 quickly, and it could simulate queries with unbounded norm, it follows that querying with unbounded norm cannot improve the rate of convergence. This fact is needed in the proof of Theorem 6 below.\nC.2 Non-smooth and strongly convex components\nWe now prove Theorem 6 using a reduction from the Lipschitz and non-strongly convex setting:\nTheorem 6. For any L, \u03bb > 0, any 0 < \u01eb < L 2\n200\u03bbm , any m \u2265 2, and any randomized algorithm A with access to hF , there exists a dimension d = O ( L4\n\u03bb3\u01eb log L\u221a \u03bb\u01eb\n)\n, and m functions fi defined over X \u2286 Rd, which are L-Lipschitz continuous and \u03bb-strongly convex, such that in order to find a point x\u0302 for which E [F (x\u0302)\u2212 F (x\u2217)] < \u01eb, A must make \u2126 (\nm+ \u221a mL\u221a \u03bb\u01eb ) queries to hF .\nProof. Just as in the proof of Theorem 2, we assume towards contradiction that there is an algorithm A which can optimize F using o (\nm+ \u221a mL\u221a \u03bb\u01eb ) queries to hF in expectation. Then A could be used to minimize\nthe sum F\u0303 of m functions f\u0303i, which are convex and L-Lipschitz continuous over the domain {x : \u2016x\u2016 \u2264 B} by adding a regularizer. Let\nF (x) = 1\nm\nm \u2211\ni=1\nfi(x) := 1\nm\nm \u2211\ni=1\nf\u0303i(x) + \u03bb\n2 \u2016x\u20162\nNote that fi is \u03bb-strongly convex and since f\u0303i is L-Lipschitz, fi is (L+\u03bbB)-Lipschitz continuous on the same domain. Furthermore, by setting \u03bb = \u01ebB2 ,\nF\u0303 (x) \u2264 F (x) \u2264 F\u0303 (x) + \u01eb 2B2 \u2016x\u20162 \u2264 F\u0303 (x) + \u01eb 2\nBy assumption, A can find an x\u0302 such that F (x\u0302) \u2212 F (x\u2217) < \u01eb2 using o (\nm+ \u221a m(L+\u03bbB)\u221a\n\u03bb\u01eb\n) = o ( m+ \u221a mLB \u01eb )\nqueries to hF , and \u01eb\n2 > F (x\u0302)\u2212 F (x\u2217) \u2265 F\u0303 (x\u0302)\u2212 F\u0303 (x\u0303\u2217)\u2212 \u01eb 2\nThus x\u0302 is \u01eb-suboptimal for F\u0303 . However, this contradicts the conclusion of theorem 5 when L > 0, \u03bb > 0, 0 < \u01eb < L 2\n200\u03bbm , and d = \u2126 ( L4 \u03bb3\u01eb log L\u221a \u03bb\u01eb ) leads to contradiction.\nC.3 Smooth and not strongly convex components\nTheorem 7. For any \u03b3,B, \u01eb > 0, any m \u2265 2, and any randomized algorithm A with access to hF , there exists a sufficiently large dimension d = O ( \u03b32B6\n\u01eb2 log ( \u03b3B2 \u01eb ) +B2m logm ) and m functions fi defined over\nX = { x \u2208 Rd : \u2016x\u2016 \u2264 B } , which are convex and \u03b3-smooth, such that to find a point x\u0302 \u2208 Rd for which E [F (x\u0302)\u2212 F (x\u2217)] < \u01eb, A must make \u2126 ( m+ \u221a m\u03b3B2\n\u01eb\n)\nqueries to hF .\nWithout loss of generality, we can assume that \u03b3 = B = 1. We will first consider the case where \u01eb = O ( 1 m ) and prove that A must make \u2126 (\u221a\nm \u01eb\n)\nqueries to hF . Afterwards, we will show a lower bound of \u2126(m) in the large-\u01eb regime where that term dominates.\nThe function construction in this case is very similar to the non-smooth randomized construction. As in Equation (11)\n\u03c6c(z) =\n\n \n  0 |z| \u2264 c 2(|z| \u2212 c)2 c < |z| \u2264 2c z2 \u2212 2c2 |z| > 2c\nThe key properties of this function for this proof are that it is convex, everywhere differentiable and 4-smooth, and when |z| \u2264 c, the function is constant at 0. It is also useful to note that\n0 \u2264 z2 \u2212 \u03c6c(z) \u2264 2c2 (16)\nAs in Equation (12), for values a and k to be fixed later, we define the pairs of functions for i = 1, ...,m/2:\nfi,1(x) = 1\n16\n\n\u3008x, vi,0\u30092 \u2212 2a \u3008x, vi,0\u3009+ \u2211\nr\u2208{2,4,...}\u2264k \u03c6c (\u3008x, vi,r\u22121\u3009 \u2212 \u3008x, vi,r\u3009)\n\n\nfi,2(x) = 1\n16\n\n\n\u2211\nr\u2208{1,3,...}\u2264k \u03c6c (\u3008x, vi,r\u22121\u3009 \u2212 \u3008x, vi,r\u3009) + \u03c6c (\u3008x, vi,k\u3009)\n\n\nwith orthonormal vectors vi,r chosen randomly on the unit sphere in R d as for Theorem 5.\nAt the end of this proof, we will show that the functions fi,\u00b7 satisfy Property 1. Since the domain, and therefore the queries made to the oracle are bounded by B, Property 1 and Lemma 7 ensure that when the dimension is at least d = 32B 2N\nc2 log(10kN) then after N oracle queries, \u3008x, vi,r\u3009 \u2265 c2 for no more than N vectors vi,r with probability 9 10 . Now, we will bound the suboptimality of Fi(x) := (fi,1(x)+fi,2(x))/2 at an iterate x such that |\u3008x, vi,r\u3009| < c2 for all r \u2265 t. From the definition of \u03c6c:\nFi(x) = 1\n32\n(\n\u3008x, vi,0\u30092 \u2212 2a \u3008x, vi,0\u3009+ k \u2211\nr=1\n\u03c6c (\u3008x, vi,r\u22121\u3009 \u2212 \u3008x, vi,r\u3009) + \u03c6c (\u3008x, vi,k\u3009) )\n= 1\n32\n(\n\u3008x, vi,0\u30092 \u2212 2a \u3008x, vi,0\u3009+ t \u2211\nr=1\n\u03c6c (\u3008x, vi,r\u22121\u3009 \u2212 \u3008x, vi,r\u3009) )\nFi(x) \u2264 1\n32\n(\n\u3008x, vi,0\u30092 \u2212 2a \u3008x, vi,0\u3009+ t \u2211\nr=1\n(\u3008x, vi,r\u22121\u3009 \u2212 \u3008x, vi,r\u3009)2 + \u3008x, vi,t\u30092 )\nFi(x) \u2265 1\n32\n(\n\u3008x, vi,0\u30092 \u2212 2a \u3008x, vi,0\u3009+ t \u2211\nr=1\n(\u3008x, vi,r\u22121\u3009 \u2212 \u3008x, vi,r\u3009)2 + \u3008x, vi,t\u30092 )\n\u2212 t+ 1 16 c2\nDefine\nF t+1i (x) := 1\n32\n(\n\u3008x, vi,0\u30092 \u2212 2a \u3008x, vi,0\u3009+ t \u2211\nr=1\n(\u3008x, vi,r\u22121\u3009 \u2212 \u3008x, vi,r\u3009)2 + \u3008x, vi,t\u30092 )\nand note that in the proof of Theorem 3 we already showed that that the optimum of F ti is achieved at\nx\u2217i,t = a t\u22121 \u2211\nr=0\n(\n1\u2212 r + 1 t+ 1\n)\nvi,r\nand\nF ti ( x\u2217i,t )\n= \u2212a 2\n32\n(\n1\u2212 1 t+ 1\n)\nand \u2225\n\u2225x\u2217i,t \u2225 \u2225\n2 \u2264 a 2t\n3\nTherefore, setting a = \u221a\n6 m(k+1) ensures that\n\u2225 \u2225 \u2225 \u2211 m 2 i=1 x \u2217 i,k+1 \u2225 \u2225\n\u2225 \u2264 1. It is not necessarily true that x\u2217 = \u2211\nm 2 i=1 x \u2217 i,k+1, but it serves as an upper bound on the optimum.\nLet q := \u230ak2\u230b and consider an iterate x generated by A before it makes q\u2212 1 queries to the functions fi,1 and fi,2. When \u3008x, vi,r\u3009 < c2 for all r \u2265 q,\nFi(x)\u2212 Fi(x\u2217) \u2265 F qi (x)\u2212 qc2\n16 \u2212 Fi(x\u2217i,k+1)\n\u2265 F qi (x\u2217i,q)\u2212 F k+1i (x\u2217i,k+1)\u2212 qc2\n16\n= \u2212a 2\n32\n(\n1\u2212 1 q + 1\n)\n+ a2\n32\n(\n1\u2212 1 k + 2\n)\n\u2212 qc 2\n16\n\u2265 1 32k2m \u2212 kc 2 32\nwhere the last inequality holds as long as k \u2265 2. When \u01eb < 1320m , setting c = \u221a 16\u01eb k and k = \u230a 1\u221a80\u01ebm\u230b \u2265 2, ensures that\nFi(x)\u2212 Fi(x\u2217) \u2265 5 2 \u01eb\u2212 \u01eb 2 = 2\u01eb\nTherefore, if \u3008x, vi,r\u3009 < c2 for all r \u2265 q is true for at least m4 of the i\u2019s, then x cannot be \u01eb-suboptimal for F . So, for N = mq4 in dimension d = 32B2N c2 log(10kN) = 32B2 c2 ( mq 4 )\nlog(10kN), with probability 910 , the algorithm must make at least mq4 queries in order to reach an \u01eb-suboptimal point. This gives a lower bound of\nm 4 q \u2265\n\u221a m\n48 \u221a 10\u01eb\nwhich holds with probability 910 . To complete the first half of the proof, we prove that Property 1 holds for this construction:\nProof of Property 1 for smooth and non-strongly convex construction. First we prove the properties about gradients:\nConsider the case when t is odd. From equation 16, we can see that d\u03c6cdz (z) = 0 when |z| < c. Furthermore, for r > t, |\u3008x, vi,r\u22121\u3009 \u2212 \u3008x, vi,r\u3009| < c. We can therefore express the gradients:\n\u2207fi,1(x) = 1\n16\n( 2 \u3008x, vi,0\u3009 \u2212 2avi,0 + t\u22121 \u2211\nr even\n\u03c6\u2032c (\u3008x, vi,r\u22121\u3009 \u2212 \u3008x, vi,r\u3009) (vi,r\u22121 \u2212 vi,r) )\n\u2207fi,2(x) = 1\n16\n(\nt \u2211\nr odd\n\u03c6\u2032c (\u3008x, vi,r\u22121\u3009 \u2212 \u3008x, vi,r\u3009) (vi,r\u22121 \u2212 vi,r) + \u03c6\u2032c (\u3008x, vi,k\u3009) vi,k )\nIt is clear from these expressions that \u2207fi,1(x) \u2208 span {vi,0, ..., vi,t\u22121} and \u2207fi,2(x) \u2208 span {vi,0, ..., vi,t}. The proof for the case when t is even follows the same line of reasoning.\nNow, we prove the properties about proxs:\nWe follow the same line of reasoning as in the Lipschitz and non-strongly convex case. The only necessary addition is to show, that when t\u2032 \u2265 t is the smallest even number which is not smaller than t and u\u2212 = \u2211t\u2032\u22121\nr=0 \u3008uvi , vi,r\u3009 vi,r and u+ = \u2211k r=t\u2032 \u3008uvi , vi,r\u3009 vi,r, then fi,2(u\u2212 + u+) = fi,2(u\u2212) + fi,2(u+):\nfi,2(u \u2212 + u+) =\n1\n16\n(\n\u2211\nr\u2208{1,3,...,t\u2032\u22121} \u03c6c\n(\u2329 u\u2212, vi,r\u22121 \u232a \u2212 \u2329 u\u2212, vi,r \u232a)\n+ \u2211\nr\u2208{t\u2032+1,t\u2032+3,...}<k \u03c6c\n(\u2329 u+, vi,r\u22121 \u232a \u2212 \u2329 u+, vi,r \u232a) + \u03c6c (\u2329 u+, vi,k \u232a)\n)\n= fi,2(u \u2212) + fi,2(u +)\nThis same reasoning applies for fi,1 or odd t \u2032.\nSo far, we have shown a lower bound of \u2126 (\u221a m \u01eb ) when \u01eb = O ( 1 m ) . We now show a lower bound of \u2126(m) for all \u01eb > 0, which accounts for the first term in the lower bound \u2126 ( m+ \u221a\nm \u01eb\n)\nwhich dominates when\n\u01eb = \u2126 ( 1 m ) . Consider the 0-smooth functions\nfi(x) = C \u3008x, vi\u3009\nfor any constant C > 0, and where the orthonormal vectors vi are randomly chosen as before. F reaches its minimum on the unit ball at\nargmin x:\u2016x\u2016\u22641 F (x) = \u22121\u221a m\nm \u2211\ni=1\nvi\nand F (x\u2217) = \u2212 C\u221a m . Using similar analysis as inside the proof of Lemma 7, if d = 2B 2 (\n1 4 \u221a m\n)2 log 2m \u2264\n32B2m log 2m then P ( \u2203i which has not been queried s.t. |\u3008x, vi\u3009| \u2265 14\u221am ) < 12 . So if fewer than m 2 functions have been queried, then with probability at least 12 :\nF (x) \u2212 F (x\u2217) \u2265 (\n\u2212C \u221a 31\n8 \u221a m + \u2212C 8 \u221a m\n)\n\u2212 \u2212C\u221a m \u2265 0.16C\u221a m\nso\nE [F (x)\u2212 F (x\u2217)] \u2265 0.08C\u221a m\nTherefore, by simply choosing C = \u01eb \u221a m\n0.08 , we ensure that such a point x is at least \u01eb-suboptimal, completing the proof for all \u01eb > 0.\nAs noted above, the queries made to the oracle must be bounded for Lemma 7. Since the domain of F is the B-ball, this is easy to satisfy. If we want to ensure that our construction is still hard to optimize, even if the algorithm is allowed to query arbitrarily large vectors, then we can modify our construction in the following way;\nf \u2032i,j(x) =\n{\nfi,j(x) \u2016x\u2016 \u2264 B fi,j ( B x\u2016x\u2016 ) + \u2329 \u2207fi,j ( B x\u2016x\u2016 ) , x\u2212B x\u2016x\u2016 \u232a \u2016x\u2016 > B\nThis function is continuous and smooth, and also has the property that querying the oracle at a point x outside of the B-ball is cannot be more informative than querying at B x\u2016x\u2016 . That is, an algorithm that is not allowed to query outside the B-ball can simulate such queries using its restricted oracle. Since this restricted algorithm cannot optimize quickly, but can still calculate the oracle outputs that it would have recieved by querying large vectors, it follows that an unrestricted algorithm could not optimize this function quickly either.\nRemark: Another variant of (1) that one might consider is an unconstrained optimization problem, where we assume that the minimizer of F lies on the interior of that ball. In other words, we could consider a version of (1) where the gradient of F must vanish on the interior of X . In this case, there is little reason to consider any \u01eb larger than \u03b3B 2\n2 , since F (0) \u2212 F (x\u2217) \u2264 \u03b3B2 2 always (by\nsmoothness F (0) \u2212 F (x\u2217) \u2264 \u3008\u2207F (x\u2217), x0 \u2212 x\u2217\u3009 + \u03b32 \u2016x\u2217\u2016 2 \u2264 \u03b3B22 ). Consequently, when \u01eb \u2265 \u03b3B2\n2 there is a trivial upper bound of zero oracle queries, as just returning the zero vector guarantees \u01eb-suboptimality. We can construct functions so that Theorem 7 still applies for 0 < \u01eb < 9\u03b3B 2\n128 . In the previous proof, the first construction is still valid in the unconstrained case since the minimizer lies within the unit ball. For the \u2126(m) term, consider the 1-smooth functions (assume w.l.o.g. that \u03b3 = B = 1)\nfi(x) = \u221a m \u3008x, vi\u3009+ \u2016x\u20162 2\nwhere the m orthonormal vectors vi are drawn randomly from the unit sphere in R d as in the previous construction. The gradient of F vanishes at x\u2217 = \u2212 1\u221a m \u2211m i=1 vi, (note \u2016x\u2217\u2016 = 1) and F (x\u2217) = \u2212 12 . Using similar techniques as inside the proof of Lemma 7 if d = 2B 2 (\n1 4 \u221a m\n)2 log 10m \u2264 32B2m log 10m, then for any iterate\nx generated by A before fi has been queried, P ( \u2203i which has not been queried s.t. |\u3008x, vi\u3009| \u2265 14\u221am ) < 910 . Furthermore, if |\u3008x, vi\u3009| < 14\u221am for more than m 2 of the functions, then\nF (x) = 1\nm\nm \u2211\ni=1\n\u221a m \u3008x, vi\u3009+ \u2016x\u20162 2\n\u2265 1 m\n(\nm 2 \u00b7 \u221a m \u22121\u221a m + m 2 \u00b7 \u221a m \u22121 4 \u221a m ) + m 2 \u00b7 1m + m2 \u00b7 116m 2\n= \u221223 64\nTherefore, if fewer than m2 functions have been queried, then with probability at least 9 10 :\nF (x)\u2212 F (x\u2217) \u2265 \u221223 64 \u2212 \u22121 2 = 9 64\nso\nE [F (x) \u2212 F (x\u2217)] \u2265 9 128\nThis proves a lower bound of \u2126(m) for 0 < \u01eb < 9\u03b3B 2\n128 .\nC.4 Smooth and strongly convex components\nIn the smooth and strongly convex case, we cannot use the same simple reduction that was used to prove Theorem 6. Using that construction, we would be able to show a lower bound of m, but would not be able to show any dependence on \u01eb, so the lower bound would be loose. Instead, we will use an explicit construction similar to the one used in Theorem 7.\nTheorem 8. For any m \u2265 2, any \u03b3, \u03bb > 0 such that \u03b3\u03bb > 161m, any \u01eb > 0, any \u01eb0 > 60\u01eb \u221a \u03b3 \u03bbm , and any randomized algorithm A, there exists a dimension d = O (\n\u03b32.5\u01eb0 \u03bb2.5\u01eb log\n3 (\n\u03bb\u01eb0 \u03b3\u01eb\n) + m\u03b3\u01eb0\u03bb\u01eb logm )\n, domain X \u2286 R\nd, x0 \u2208 X , and m functions fi defined on X which are \u03b3-smooth and \u03bb-strongly convex, and such that F (x0) \u2212 F (x\u2217) = \u01eb0 and such that in order to find a point x\u0302 \u2208 X such that E [F (x\u0302)\u2212 F (x\u2217)] < \u01eb, A must make \u2126 ( m+ \u221a\nm\u03b3 \u03bb log\n(\n\u01eb0 \u01eb\n\u221a\nm\u03bb \u03b3\n))\nqueries to hF .\nProof. We will prove the theorem for a 1-smooth, \u03bb-strongly convex problem, for \u03bb < 173m , which can be generalized by scaling.\nAs in the proof for the non-strongly convex case, we introduce the 4-smooth helper function\n\u03c6c(z) =\n\n \n  0 |z| \u2264 c 2(|z| \u2212 c)2 c < |z| \u2264 2c z2 \u2212 2c2 |z| > 2c\nusing which we will construct m/2 pairs of functions, which will each be based on the following. As in previous proofs, we randomly select orthonormal vectors vi,r from R d. Then, for constants k, C, and \u03b6 to be decided upon later; with \u03bb\u0303 := m \u00b7 \u03bb; and for i = 1, ..., \u230am/2\u230b define the following pairs of functions (if m is odd, let fm(x) = \u03bb\u0303 2m \u2016x\u2016 2 ):\nfi,1(x) = 1\u2212 \u03bb\u0303 16\n(\n\u3008x, vi,0\u30092 \u2212 2C \u3008x, vi,0\u3009 k \u2211\nr even\n\u03c6c (\u3008x, vi,r\u22121\u3009 \u2212 \u3008x, vi,r\u3009) ) + \u03bb\u0303\n2m \u2016x\u20162\nfi,2(x) = 1\u2212 \u03bb\u0303 16\n(\n\u03b6\u03c6c(\u3008x, vi,k\u3009) + k \u2211\nr odd\n\u03c6c (\u3008x, vi,r\u22121\u3009 \u2212 \u3008x, vi,r\u3009) ) + \u03bb\u0303\n2m \u2016x\u20162\nWhen \u03bb\u0303 \u2208 [0, 1] these function are 1-smooth and \u03bb-strongly convex. These functions also have Property 1, but we will omit the proof, as it follows directly from the proof in Appendix C.3. Intuitively, the squared norm reveals no new information about the vectors vi,r besides what is already included in the query point x.\nWhen all of the queries are bounded by B, Property 1 along with Lemma 7 ensures that when d = 32B2N\nc2 log(10kN), after the algorithm make N queries \u3008x, vi,r\u3009 \u2265 c2 for at most N of the vectors vi,r with probability 910 . For this probability bound to apply, we need that all of the queries made by the algorithm are within a B-ball around the origin. We know that F (0) \u2212 F (x\u2217) = \u01eb0, and by strong-convexity F (0) \u2265 F (x\u2217) + \u03bb2 \u2016x\u2217\u2016 2 , therefore, \u2016x\u2217\u2016 \u2264 \u221a 2\u01eb0 \u03bb =: B. Since the optimum point must lie in the B-ball around the origin, we will restrict the algorithm to query only at points within the B-ball. At the end of the proof, we will show that with a small modification to the functions outside of the B-ball, querying at vectors of large norm cannot help the algorithm.\nNow it remains to lower bound the suboptimality of the pair fi,1 and fi,2 at an iterate which is orthogonal to all vectors vi,r for r > t:\nIn order to bound the suboptimality of a pair of functions i, it will be convenient to bundle up all of the terms which affect the value of \u3008x\u2217, vi,r\u3009 from all m of the component functions. Most of those terms are contained in fi,1 and fi,2, however, \u2016x\u20162 terms in each of the other components also affect the value of \u3008x\u2217, vi,r\u3009. For each i, consider the projection operator Pi which projects a vector x onto the subspace spanned by {vi,r}kr=0, and P\u22a5 projecting onto the space orthogonal to vi,r for all i, r. Now decompose\n\u03bb\u0303 2m \u2016x\u20162 = \u03bb\u0303 2m\n\n\n\u230am2 \u230b \u2211\ni=1\n\u2016Pix\u20162 + \u2016P\u22a5x\u20162  \nGather all m of the \u03bb\u03032m \u2016Pix\u2016 2 terms and split them amongst fi,1 and fi,2 to make the following modified functions:\nf\u0303i,1(x) = fi,1 \u2212 \u03bb\u0303 2m \u2016x\u20162 + \u03bb\u0303 4 \u2016Pix\u20162\nf\u0303i,2(x) = fi,2 \u2212 \u03bb\u0303 2m \u2016x\u20162 + \u03bb\u0303 4 \u2016Pix\u20162\nAfter this shuffle, all of the terms affecting \u3008x\u2217, vi,r\u3009 are contained in these two functions which will help the analysis. Note that there is also a remaining \u03bb\u03032 \u2016P\u22a5x\u2016 2 term, however, this term is not very important to\ntrack since we are bounding the suboptimality of F , which can only increase by considering that non-negative term and P\u22a5x\u2217 = ~0. Now, consider\n1\n2\n( f\u0303i,1(x) + f\u0303i,2(x) ) = 1\u2212 \u03bb\u0303 32\n(\n\u3008x, vi,0\u30092 \u22122C \u3008x, vi,0\u3009+ \u03b6\u03c6c(\u3008x, vi,k\u3009)\n+\nk \u2211\nr=1\n\u03c6c (\u3008x, vi,r\u22121\u3009 \u2212 \u3008x, vi,r\u3009) ) + \u03bb\u0303\n4 \u2016Pix\u20162\nIf we define\nF ti (x) := 1\u2212 \u03bb\u0303 32\n( \u3008x, vi,0\u30092 \u22122C \u3008x, vi,0\u3009+ \u3008x, vi,t\u30092 + t \u2211\nr=1\n(\u3008x, vi,r\u22121\u3009 \u2212 \u3008x, vi,r\u3009)2 ) + \u03bb\u0303\n4 \u2016Pix\u20162\nand\nFi(x) := 1\u2212 \u03bb\u0303 32\n( \u3008x, vi,0\u30092 \u22122C \u3008x, vi,0\u3009+ \u03b6 \u3008x, vi,k\u30092 + k \u2211\nr=1\n(\u3008x, vi,r\u22121\u3009 \u2212 \u3008x, vi,r\u3009)2 ) + \u03bb\u0303\n4 \u2016Pix\u20162\nthen when |\u3008x, vi,r\u3009| < c2 F ti (x) \u2264 1\n2\n( f\u0303i,1(x) + f\u0303i,2(x) ) + (1\u2212 \u03bb\u0303)(t+ 1)\n16 c2\nand for any y 1\n2\n( f\u0303i,1(y) + f\u0303i,2(y) ) \u2264 Fi(y)\nand, conveniently, Fi is very similar to the construction from Appendix B.4. In particular, let Q\u0303 := 1 2 ( 1 \u03bb\u0303 \u2212 1) + 1, then\nF ti (x) = 1\n2\n(\n\u03bb\u0303(Q\u0303 \u2212 1) 8\n( \u3008x, vi,0\u30092 \u22122C \u3008x, vi,0\u3009+ \u3008x, vi,t\u30092 + t \u2211\nr=1\n(\u3008x, vi,r\u22121\u3009 \u2212 \u3008x, vi,r\u3009)2 ) + \u03bb\u0303\n2 \u2016Pix\u20162\n)\nFi(x) = 1\n2\n(\n\u03bb\u0303(Q\u0303\u2212 1) 8\n( \u3008x, vi,0\u30092 \u22122C \u3008x, vi,0\u3009+ \u03b6 \u3008x, vi,k\u30092 + k \u2211\nr=1\n(\u3008x, vi,r\u22121\u3009 \u2212 \u3008x, vi,r\u3009)2 ) + \u03bb\u0303\n2 \u2016Pix\u20162\n)\nWe have already showed in Appendix B.4 that if x\u0302 := argminx Fi(x), and if\nC > 12\n\u221a \u01eb\n\u03bb\u0303(\n\u221a Q\u0303\u2212 1) =\u21d2 2\u01ebi0 := 2 (Fi(0)\u2212 Fi(x\u0302)) >\n30\u01eb\n\u03bb\u0303\n\u03b6 = 2 \u221a\nQ\u0303+ 1\n\u03bb\u0303 < 1\n73\nt =\n   \n\u221a\nQ\u0303\u2212 1 4 log \u01ebi0\n20\n\u221a\nQ\u0303\u01eb\n   \n|\u3008x, vi,r\u3009| \u2264 c\n2 \u2200r > t\nthen, 2 (\nF ti (x)\u2212 Fi(x\u0302) ) \u2265 10\u01eb\nTherefore,\n10\u01eb \u2264 2 ( F ti (x)\u2212 Fi(x\u0302) )\n\u2264 2 ( 1\n2\n( f\u0303i,1(x) + f\u0303i,2(x) ) + (1\u2212 \u03bb\u0303)(k + \u03b6) 16 c2 \u2212 1 2 ( f\u0303i,1(x\u0302) + f\u0303i,2(x\u0302) )\n)\n\u2264 ( f\u0303i,1(x) + f\u0303i,2(x) ) + (1 \u2212 \u03bb\u0303)(k + \u03b6)\n8 c2 \u2212\n(\nf\u0303i,1(x \u2217) + f\u0303i,2(x \u2217) )\nSo (\nf\u0303i,1(x) + f\u0303i,2(x) ) \u2212 ( f\u0303i,1(x \u2217)\u2212 f\u0303i,2(x\u2217) ) \u2265 10\u01eb\u2212 (1 \u2212 \u03bb\u0303)(k + \u03b6) 8 c2\nSetting\nc =\n\u221a\n16\u01eb\n(1\u2212 \u03bb\u0303)(k + \u03b6) then\n( f\u0303i,1(x) + f\u0303i,2(x) ) \u2212 ( f\u0303i,1(x \u2217)\u2212 f\u0303i,2(x\u2217) ) \u2265 10\u01eb\u2212 (1\u2212 \u03bb\u0303)(k + \u03b6) 8 c2 = 8\u01eb\nTherefore, if at least m/4 of the pairs i it holds that |\u3008x, vi,r\u3009| < c2 for r > t, then\nF (x)\u2212 F (x\u2217) \u2265 1 m\n\u230am2 \u230b \u2211\ni=1\n( f\u0303i,1(x) + f\u0303i,2(x) ) \u2212 ( f\u0303i,1(x \u2217)\u2212 f\u0303i,2(x\u2217) )\n\u2265 m 4 \u00b7 1 m \u00b7 8\u01eb = 2\u01eb\nAs a consequence of this, when the dimension is d = 32c2 ( 2\u01eb0 \u03bb ) ( mt 4 ) log ( 5kmt 2 ) then with probability 910 the optimization algorithm must make at least t queries to each of at least m4 pairs of functions in order to reach an \u01eb-suboptimal solution in expectation. So, when\n\u03bb \u2264 1 161m\n\u01eb0 \u01eb \u2265 60\u221a m\u03bb\nthis gives a lower bound of\n\u2308m\n4\n\u2309 \u00b7 t \u2265 m 4\n   \n\u221a\nQ\u0303\u2212 1 4 log \u01ebi0\n20\n\u221a\nQ\u0303\u01eb\n   \n\u2265 m 4\n   \n\u221a\nQ\u0303\u2212 1 4 log \u01eb0\n40\n\u221a\nQ\u0303\u01eb\n   \n\u2265 m 4\n\u221a\nQ\u0303\u2212 1 8 log \u01eb0\n40\n\u221a\nQ\u0303\u01eb\n\u2265 m 4\n3\n40 \u221a m\u03bb\nlog \u01eb0 \u221a m\u03bb\n30\u01eb\n= 3\n160\n\u221a\nm \u03bb log\n\u01eb0 \u221a m\u03bb\n30\u01eb\n= \u2126\n(\n\u221a\nm \u03bb log\n\u01eb0 \u221a m\u03bb\n\u01eb\n)\nThe same argument as was used in the discussion after theorem 7 to show the \u2126(m) term of the lower bound can be used here, as the function in that construction was both smooth and strongly convex.\nAs mentioned above, Lemma 7 requires that the norm of all query points be bounded by B. We argued above that the optimum of F must lie within the B-ball around the origin. Even so, we can slightly modify our construction to show that even if the algorithm were allowed to query arbitrarily large points, it still would not be able to optimize F quickly. Define:\nf \u2032i,j(x) =\n\n\n fi,j(x) \u2016x\u2016 \u2264 B fi,j ( B x\u2016x\u2016 ) + \u2329 \u2207fi,j ( B x\u2016x\u2016 ) , x\u2212B x\u2016x\u2016 \u232a + \u03bb2 \u2225 \u2225 \u2225x\u2212B x\u2016x\u2016 \u2225 \u2225 \u2225 2 \u2016x\u2016 > B\nThis new function is continuous, \u03b3-smooth, and \u03bb-strongly convex, and it also has the property that querying the function at a point x outside the B-ball, it is no more informative than querying at B x\u2016x\u2016 . That is, an algorithm that was not allowed to query outside the B-ball could simulate the result of such queries. Since that restricted algorithm can\u2019t optimize F \u2032 well, as proven above, another algorithm which could query at arbitrary points, therefore could not either.\nC.5 Non-smooth components when \u01eb is large\nTheorem 11. For any L,B > 0, any 10\u221a m < \u01eb < 14 , and any m \u2265 161, there exists m functions fi which are convex and L-Lipschitz continuous defined on X = {x \u2208 R : |x| \u2264 B} such that for any randomized algorithm A for solving problem (1) using access to hF , A must make at least \u2126 ( L2B2\n\u01eb2\n)\nqueries to hF in order to find\na point x\u0302 such that E[F (x\u0302)\u2212 F (x\u2217)] < \u01eb.\nProof. Without loss of generality, we can assume that L = B = 1. We construct m functions fi on R 1 in the following manner: first, sample p from the following distribution\np =\n{\n1 2 \u2212 2\u01eb w.p. 12 1 2 + 2\u01eb w.p. 1 2\nThen for i = 1, ...,m, we define\nfi(x) =\n{\nx w.p. p \u2212x w.p. 1\u2212 p\nConsider now the task of optimizing F (x) = 1m \u2211m i=1 fi(x) = Y x m . Clearly, F is optimized at \u2212sign(Y ), and as long as |Y | > 2m\u01eb, then any x which is \u01eb-suboptimal given sign(Y ) = +1 must be at least 3\u01eb-suboptimal given sign(Y ) = \u22121. Using Chernoff bounds, P(|Y | \u2264 2m\u01eb) \u2264 exp(\u2212 \u01eb2m2 ) < exp(\u22125). Therefore, since the expected suboptimality of an iterate x is at least\nE [F (x)\u2212 F (x\u2217)] \u2265 3P (sign(x) 6= \u2212sign(Y )||Y | \u2265 2m\u01eb)P (|Y | \u2265 2m\u01eb) \u00b7 \u01eb > 3(1\u2212 exp(\u22125))P (sign(x) 6= \u2212sign(Y )||Y | \u2265 2m\u01eb) \u00b7 \u01eb\nTherefore, until the algorithm has made enough queries so that\nP (sign(x) 6= \u2212sign(Y )||Y | \u2265 2m\u01eb) < 1 3\u2212 3 exp(\u22125)\nthe expected suboptimality is greater than \u01eb. By a standard information theoretic result [2, 19], achieving that probability of success at predicting the sign of Y implies a comparable level of accuracy at distinguishing between p = 0.5 + 2\u01eb and p = 0.5\u2212 2\u01eb, and that requires at least 1128\u01eb2 queries to hF .\nIt is straightforward to show a lower bound of \u2126 ( L2\n\u03bb\u01eb\n)\nfor strongly convex functions using the same reduction\nby regularization as in the proofs of theorems 2 and 6. We also note that this lower bound implies a lower bound of \u2126(m) for smooth functions, whether strongly convex or not. Each function fi in this construction is linear, and therefore is trivially 0-smooth. We make the gradient of each function arbitrarily large by multiplying each fi by a large number. As the multiplier grows, the algorithm need be more and more certain of the sign of Y in order to achieve a expected suboptimality of less than \u01eb. Thus for a sufficiently large multiplier, the algorithm must query \u2126(m) functions. We cannot force it to query more than that, of course, since it only needs to query m functions to know the sign of Y with probability 1."}], "references": [{"title": "A lower bound for the optimization of finite sums", "author": ["Alekh Agarwal", "Leon Bottou"], "venue": "arXiv preprint arXiv:1410.0723,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2014}, {"title": "Information-theoretic lower bounds on the oracle complexity of convex optimization", "author": ["Alekh Agarwal", "Martin J Wainwright", "Peter L Bartlett", "Pradeep K Ravikumar"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2009}, {"title": "Katyusha: The first truly accelerated stochastic gradient descent", "author": ["Zeyuan Allen-Zhu"], "venue": "arXiv preprint arXiv:1603.05953,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2016}, {"title": "Optimal black-box reductions between optimization objectives", "author": ["Zeyuan Allen-Zhu", "Elad Hazan"], "venue": "arXiv preprint arXiv:1603.05642,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2016}, {"title": "Communication complexity of distributed convex learning and optimization", "author": ["Yossi Arjevani", "Ohad Shamir"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2015}, {"title": "Convex analysis and monotone operator theory in Hilbert spaces", "author": ["Heinz H Bauschke", "Patrick L Combettes"], "venue": "Springer Science & Business Media,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2011}, {"title": "Distributed optimization and statistical learning via the alternating direction method of multipliers", "author": ["Stephen Boyd", "Neal Parikh", "Eric Chu", "Borja Peleato", "Jonathan Eckstein"], "venue": "Foundations and Trends R  \u00a9 in Machine Learning,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2011}, {"title": "Accelerating stochastic gradient descent using predictive variance reduction", "author": ["Rie Johnson", "Tong Zhang"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2013}, {"title": "An optimal randomized incremental gradient method", "author": ["Guanghui Lan"], "venue": "arXiv preprint arXiv:1507.02000,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2015}, {"title": "A universal catalyst for first-order optimization", "author": ["Hongzhou Lin", "Julien Mairal", "Zaid Harchaoui"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2015}, {"title": "Smooth minimization of non-smooth functions", "author": ["Yu Nesterov"], "venue": "Mathematical programming,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2005}, {"title": "A method of solving a convex programming problem with convergence rate o (1/k2)", "author": ["Yurii Nesterov"], "venue": "Soviet Mathematics Doklady,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 1983}, {"title": "Prisma: Proximal iterative smoothing algorithm", "author": ["Francesco Orabona", "Andreas Argyriou", "Nathan Srebro"], "venue": "arXiv preprint arXiv:1206.2372,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2012}, {"title": "Minimizing finite sums with the stochastic average gradient", "author": ["Mark Schmidt", "Nicolas Le Roux", "Francis Bach"], "venue": "arXiv preprint arXiv:1309.2388,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2013}, {"title": "Stochastic optimization for machine learning. Slides of presentation at \u201cOptimization Without Borders 2016", "author": ["Shai Shalev-Shwartz"], "venue": null, "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2016}, {"title": "Stochastic dual coordinate ascent methods for regularized loss", "author": ["Shai Shalev-Shwartz", "Tong Zhang"], "venue": "The Journal of Machine Learning Research,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2013}, {"title": "Accelerated proximal stochastic dual coordinate ascent for regularized loss minimization", "author": ["Shai Shalev-Shwartz", "Tong Zhang"], "venue": "Mathematical Programming,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2016}, {"title": "Communication efficient distributed optimization using an approximate newton-type method", "author": ["Ohad Shamir", "Nathan Srebro", "Tong Zhang"], "venue": "arXiv preprint arXiv:1312.7853,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2013}, {"title": "Assouad, fano, and le cam", "author": ["Bin Yu"], "venue": "In Festschrift for Lucien Le Cam,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 1997}], "referenceMentions": [{"referenceID": 6, "context": "ADMM [7], DANE [18], DISCO [20]) or for functions that can be decomposed into several \u201ceasy\u201d parts (e.", "startOffset": 5, "endOffset": 8}, {"referenceID": 17, "context": "ADMM [7], DANE [18], DISCO [20]) or for functions that can be decomposed into several \u201ceasy\u201d parts (e.", "startOffset": 15, "endOffset": 19}, {"referenceID": 12, "context": "PRISMA [13]).", "startOffset": 7, "endOffset": 11}, {"referenceID": 15, "context": "Recently, stochastic methods such as SDCA [16], SAG [14], SVRG [8], and other variants, have been presented which leverage the finite nature of the problem to reduce the variance in stochastic gradient estimates and obtain guarantees that dominate both batch and stochastic gradient descent.", "startOffset": 42, "endOffset": 46}, {"referenceID": 13, "context": "Recently, stochastic methods such as SDCA [16], SAG [14], SVRG [8], and other variants, have been presented which leverage the finite nature of the problem to reduce the variance in stochastic gradient estimates and obtain guarantees that dominate both batch and stochastic gradient descent.", "startOffset": 52, "endOffset": 56}, {"referenceID": 7, "context": "Recently, stochastic methods such as SDCA [16], SAG [14], SVRG [8], and other variants, have been presented which leverage the finite nature of the problem to reduce the variance in stochastic gradient estimates and obtain guarantees that dominate both batch and stochastic gradient descent.", "startOffset": 63, "endOffset": 66}, {"referenceID": 16, "context": "As methods with improved complexity, such as accelerated SDCA [17], accelerated SVRG, and Katyusha [3], have been presented, researchers have also tried to obtain lower bounds on the best possible complexity in this settings\u2014but as we survey below, these have not been satisfactory so far.", "startOffset": 62, "endOffset": 66}, {"referenceID": 2, "context": "As methods with improved complexity, such as accelerated SDCA [17], accelerated SVRG, and Katyusha [3], have been presented, researchers have also tried to obtain lower bounds on the best possible complexity in this settings\u2014but as we survey below, these have not been satisfactory so far.", "startOffset": 99, "endOffset": 102}, {"referenceID": 0, "context": "Agarwal and Bottou [1] presented a lower bound of \u03a9 ( m+ \u221a m\u03b3 \u03bb log 1 \u01eb ) .", "startOffset": 19, "endOffset": 22}, {"referenceID": 8, "context": "Improving upon this, Lan [9] shows a similar lower bound for a restricted class of randomized algorithms: the algorithm must select which component to query for a gradient by drawing an index from a fixed distribution, but the algorithm must otherwise be deterministic in how it uses the gradients, and its iterates must lie in the span of the gradients it has received.", "startOffset": 25, "endOffset": 28}, {"referenceID": 14, "context": "Another recent observation [15] was that with access only to random component subgradients without knowing the component\u2019s identity, an algorithm must make \u03a9(m) queries to optimize well.", "startOffset": 27, "endOffset": 31}, {"referenceID": 4, "context": "Our deterministic lower bounds are inspired by a lower bound on the number of rounds of communication required for optimization when each fi is held by a different machine and when iterates lie in the span of certain permitted calculations [5].", "startOffset": 240, "endOffset": 243}, {"referenceID": 11, "context": "Running accelerated gradient descent (AGD) [12] on F (x) using these exact gradients achieves the upper complexity bounds for deterministic algorithms and smooth problems (see Table 1).", "startOffset": 43, "endOffset": 47}, {"referenceID": 13, "context": "SAG [14], SVRG [8] and related methods use randomization to sample components, but also leverage the finite nature of the objective to control the variance of the gradient estimator used.", "startOffset": 4, "endOffset": 8}, {"referenceID": 7, "context": "SAG [14], SVRG [8] and related methods use randomization to sample components, but also leverage the finite nature of the objective to control the variance of the gradient estimator used.", "startOffset": 15, "endOffset": 18}, {"referenceID": 9, "context": "Accelerating these methods using the Catalyst framework [10] ensures that for \u03bb-strongly convex objectives we have E [ F (x(k))\u2212 F (x\u2217) ] < \u01eb after k = O (( m+ \u221a m\u03b3 \u03bb ) log \u01eb0 \u01eb ) iterations, where F (0)\u2212 F (x\u2217) = \u01eb0.", "startOffset": 56, "endOffset": 60}, {"referenceID": 2, "context": "Katyusha [3] is a more direct approach to accelerating SVRG which avoids extraneous log-factors, yielding the complexity k = O (( m+ \u221a m\u03b3 \u03bb ) log \u01eb0 \u01eb ) indicated in Table 1.", "startOffset": 9, "endOffset": 12}, {"referenceID": 3, "context": "The log-factor in the second term can be removed using the more delicate reduction of Allen-Zhu and Hazan [4], which involves optimizing F\u03bb(x) for progressively smaller values of \u03bb, yielding the upper bound in the table.", "startOffset": 106, "endOffset": 109}, {"referenceID": 16, "context": "Accelerated SDCA [17] achieves a similar complexity using gradient and prox oracle access.", "startOffset": 17, "endOffset": 21}, {"referenceID": 2, "context": "This idea was used in order to apply Katyusha [3] and accelerated SDCA [17] to non-smooth objectives.", "startOffset": 46, "endOffset": 49}, {"referenceID": 16, "context": "This idea was used in order to apply Katyusha [3] and accelerated SDCA [17] to non-smooth objectives.", "startOffset": 71, "endOffset": 75}, {"referenceID": 10, "context": "29], following [11]).", "startOffset": 15, "endOffset": 19}, {"referenceID": 3, "context": "To avoid this, we again apply the reduction of Allen-Zhu and Hazan [4], this time optimizing F\u0303 (\u03b2) for increasingly large values of \u03b2.", "startOffset": 67, "endOffset": 70}, {"referenceID": 2, "context": "\u2014this matches the presentation of Allen-Zhu [3] and is similar to that of Shalev-Shwartz and Zhang [17].", "startOffset": 44, "endOffset": 47}, {"referenceID": 16, "context": "\u2014this matches the presentation of Allen-Zhu [3] and is similar to that of Shalev-Shwartz and Zhang [17].", "startOffset": 99, "endOffset": 103}, {"referenceID": 1, "context": "Optimizing F (x) to within \u01eb accuracy then implies recovering the bias of the Bernoulli random variable, which requires \u03a9(1/\u01eb) queries based on a standard information theoretic result [2, 19].", "startOffset": 184, "endOffset": 191}, {"referenceID": 18, "context": "Optimizing F (x) to within \u01eb accuracy then implies recovering the bias of the Bernoulli random variable, which requires \u03a9(1/\u01eb) queries based on a standard information theoretic result [2, 19].", "startOffset": 184, "endOffset": 191}, {"referenceID": 0, "context": "Indeed, several attempts have been made at lower bounds for the finite sum setting [1, 9].", "startOffset": 83, "endOffset": 89}, {"referenceID": 8, "context": "Indeed, several attempts have been made at lower bounds for the finite sum setting [1, 9].", "startOffset": 83, "endOffset": 89}, {"referenceID": 4, "context": "This method attains a recent lower bound for distributed optimization, resolving a question raised by Arjevani and Shamir [5], and when the number of machines is very large improves over all other known distributed optimization methods for the problem.", "startOffset": 122, "endOffset": 125}, {"referenceID": 8, "context": ", in [9]), and could potentially be useful for establishing randomized lower bounds also in other settings.", "startOffset": 5, "endOffset": 8}, {"referenceID": 3, "context": "Acknowledgements: We thank Ohad Shamir for his helpful discussions and for pointing out [4].", "startOffset": 88, "endOffset": 91}, {"referenceID": 0, "context": "References [1] Alekh Agarwal and Leon Bottou.", "startOffset": 11, "endOffset": 14}, {"referenceID": 1, "context": "[2] Alekh Agarwal, Martin J Wainwright, Peter L Bartlett, and Pradeep K Ravikumar.", "startOffset": 0, "endOffset": 3}, {"referenceID": 2, "context": "[3] Zeyuan Allen-Zhu.", "startOffset": 0, "endOffset": 3}, {"referenceID": 3, "context": "[4] Zeyuan Allen-Zhu and Elad Hazan.", "startOffset": 0, "endOffset": 3}, {"referenceID": 4, "context": "[5] Yossi Arjevani and Ohad Shamir.", "startOffset": 0, "endOffset": 3}, {"referenceID": 5, "context": "[6] Heinz H Bauschke and Patrick L Combettes.", "startOffset": 0, "endOffset": 3}, {"referenceID": 6, "context": "[7] Stephen Boyd, Neal Parikh, Eric Chu, Borja Peleato, and Jonathan Eckstein.", "startOffset": 0, "endOffset": 3}, {"referenceID": 7, "context": "[8] Rie Johnson and Tong Zhang.", "startOffset": 0, "endOffset": 3}, {"referenceID": 8, "context": "[9] Guanghui Lan.", "startOffset": 0, "endOffset": 3}, {"referenceID": 9, "context": "[10] Hongzhou Lin, Julien Mairal, and Zaid Harchaoui.", "startOffset": 0, "endOffset": 4}, {"referenceID": 10, "context": "[11] Yu Nesterov.", "startOffset": 0, "endOffset": 4}, {"referenceID": 11, "context": "[12] Yurii Nesterov.", "startOffset": 0, "endOffset": 4}, {"referenceID": 12, "context": "[13] Francesco Orabona, Andreas Argyriou, and Nathan Srebro.", "startOffset": 0, "endOffset": 4}, {"referenceID": 13, "context": "[14] Mark Schmidt, Nicolas Le Roux, and Francis Bach.", "startOffset": 0, "endOffset": 4}, {"referenceID": 14, "context": "[15] Shai Shalev-Shwartz.", "startOffset": 0, "endOffset": 4}, {"referenceID": 15, "context": "[16] Shai Shalev-Shwartz and Tong Zhang.", "startOffset": 0, "endOffset": 4}, {"referenceID": 16, "context": "[17] Shai Shalev-Shwartz and Tong Zhang.", "startOffset": 0, "endOffset": 4}, {"referenceID": 17, "context": "[18] Ohad Shamir, Nathan Srebro, and Tong Zhang.", "startOffset": 0, "endOffset": 4}, {"referenceID": 18, "context": "[19] Bin Yu.", "startOffset": 0, "endOffset": 4}, {"referenceID": 3, "context": "The solution is the AdaptSmooth algorithm [4].", "startOffset": 42, "endOffset": 45}, {"referenceID": 0, "context": "+ \u03bb\u0303 2m \u2016x\u2016 When \u03bb\u0303 \u2208 [0, 1] these function are 1-smooth and \u03bb-strongly convex.", "startOffset": 22, "endOffset": 28}, {"referenceID": 1, "context": "By a standard information theoretic result [2, 19], achieving that probability of success at predicting the sign of Y implies a comparable level of accuracy at distinguishing between p = 0.", "startOffset": 43, "endOffset": 50}, {"referenceID": 18, "context": "By a standard information theoretic result [2, 19], achieving that probability of success at predicting the sign of Y implies a comparable level of accuracy at distinguishing between p = 0.", "startOffset": 43, "endOffset": 50}], "year": 2016, "abstractText": "We provide tight upper and lower bounds on the complexity of minimizing the average of m convex functions using gradient and prox oracles of the component functions. We show a significant gap between the complexity of deterministic vs randomized optimization. For smooth functions, we show that accelerated gradient descent (AGD) and an accelerated variant of SVRG are optimal in the deterministic and randomized settings respectively, and that a gradient oracle is sufficient for the optimal rate. For non-smooth functions, having access to prox oracles reduces the complexity and we present optimal methods based on smoothing that improve over methods using just gradient accesses.", "creator": "LaTeX with hyperref package"}}}