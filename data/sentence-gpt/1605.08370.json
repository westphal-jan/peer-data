{"id": "1605.08370", "review": {"conference": "NIPS", "VERSION": "v1", "DATE_OF_SUBMISSION": "26-May-2016", "title": "Provable Efficient Online Matrix Completion via Non-convex Stochastic Gradient Descent", "abstract": "Matrix completion, where we wish to recover a low rank matrix by observing a few entries from it, is a widely studied problem in both theory and practice with wide applications. Most of the provable algorithms so far on this problem have been restricted to the offline setting where they provide an estimate of the unknown matrix using all observations simultaneously. However, in many applications, the online version, where we observe one entry at a time and dynamically update our estimate, is more appealing. While existing algorithms are efficient for the offline setting, they could be highly inefficient for the online setting.\n\n\nThe problem with online computation, as it turns out, has always been that computers in the near future need to run programs without the hardware. This is due to the fact that most computers will still be operating on the cloud. There are a number of different solutions available to computer programmers in their field. There is also the possibility of cloud computing where it can be used to test computer code or to check its contents and make sure it does not run on the cloud. We have been researching the idea of cloud computing for many years, but I have decided to try it. To help solve this problem, we will need to make several solutions available on the cloud. To improve the solution, we will have to solve some problems in order to do so on the cloud.\nThe solution can be found on the cloud page in the link below.\nhttp://cloud.google.com/d/search/c.php?source=microsoft-qld.saa.io\nNote that many of the problems we have found can be solved in the source code, but we are not satisfied with this solution. We want to make the solution available to the next generation of computer programmers. We have published the solution which will be compatible with the current architecture for the next generation of computer programmers. We also want to try it at any number of levels, which could be in addition to the existing algorithms which can be used by the current architectures for the next generation of computer programmers. We have started to make the solution available to the next generation of computer programmers.\nOur solution to these problems can be found on the online website at the link below.\nhttp://cloud.google.com/d/search/c.php?source=microsoft-qld.saa.io\nI first found out about a group of computers who had a hard drive that they had no way to change. When I first started, I had to use a 32-bit USB", "histories": [["v1", "Thu, 26 May 2016 17:26:18 GMT  (35kb)", "http://arxiv.org/abs/1605.08370v1", null]], "reviews": [], "SUBJECTS": "cs.LG math.OC stat.ML", "authors": ["chi jin", "sham m kakade", "praneeth netrapalli"], "accepted": true, "id": "1605.08370"}, "pdf": {"name": "1605.08370.pdf", "metadata": {"source": "CRF", "title": "Provable Efficient Online Matrix Completion via Non-convex Stochastic Gradient Descent", "authors": ["Chi Jin", "Sham M. Kakade", "Praneeth Netrapalli"], "emails": ["chijin@cs.berkeley.edu", "sham@cs.washington.edu", "praneeth@microsoft.com"], "sections": [{"heading": null, "text": "ar X\niv :1\n60 5.\n08 37\n0v 1\n[ cs\n.L G\n] 2\n6 M\nIn this paper, we propose the first provable, efficient online algorithm for matrix completion. Our algorithm starts from an initial estimate of the matrix and then performs non-convex stochastic gradient descent (SGD). After every observation, it performs a fast update involving only one row of two tall matrices, giving near linear total runtime. Our algorithm can be naturally used in the offline setting as well, where it gives competitive sample complexity and runtime to state of the art algorithms. Our proofs introduce a general framework to show that SGD updates tend to stay away from saddle surfaces and could be of broader interests for other non-convex problems to prove tight rates.\n1 Introduction\nLow rank matrix completion refers to the problem of recovering a low rank matrix by observing the values of only a tiny fraction of its entries. This problem arises in several applications such as video denoising [14], phase retrieval [3] and most famously in movie recommendation engines [16]. In the context of recommendation engines for instance, the matrix we wish to recover would be user-item rating matrix where each row corresponds to a user and each column corresponds to an item. Each entry of the matrix is the rating given by a user to an item. Low rank assumption on the matrix is inspired by the intuition that rating of an item by a user depends on only a few hidden factors, which are much fewer than the number of users or items. The goal is to estimate the ratings of all items by users given only partial ratings of items by users, which would then be helpful in recommending new items to users.\nThe seminal works of Cande\u0300s and Recht [4] first identified regularity conditions under which low rank matrix completion can be solved in polynomial time using convex relaxation \u2013 low rank matrix\n\u2217UC Berkeley. Email: chijin@cs.berkeley.edu \u2020University of Washington. Email: sham@cs.washington.edu \u2021Microsoft Research New England. Email: praneeth@microsoft.com\ncompletion could be ill-posed and NP-hard in general without such regularity assumptions [10]. Since then, a number of works have studied various algorithms under different settings for matrix completion: weighted and noisy matrix completion, fast convex solvers, fast iterative non-convex solvers, parallel and distributed algorithms and so on.\nMost of this work however deals only with the offline setting where all the observed entries are revealed at once and the recovery procedure does computation using all these observations simultaneously. However in several applications [5, 19], we encounter the online setting where observations are only revealed sequentially and at each step the recovery algorithm is required to maintain an estimate of the low rank matrix based on the observations so far. Consider for instance recommendation engines, where the low rank matrix we are interested in is the user-item rating matrix. While we make an observation only when a user rates an item, at any point of time, we should have an estimate of the user-item rating matrix based on all prior observations so as to be able to continuously recommend items to users. Moreover, this estimate should get better as we observe more ratings.\nAlgorithms for offline matrix completion can be used to solve the online version by rerunning the algorithm after every additional observation. However, performing so much computation for every observation seems wasteful and is also impractical. For instance, using alternating minimization, which is among the fastest known algorithms for the offline problem, would mean that we take several passes of the entire data for every additional observation. This is simply not feasible in most settings. Another natural approach is to group observations into batches and do an update only once for each batch. This however induces a lag between observations and estimates which is undesirable. To the best of our knowledge, there is no known provable, efficient, online algorithm for matrix completion.\nOn the other hand, in order to deal with the online matrix completion scenario in practical applications, several heuristics (with no convergence guarantees) have been proposed in literature [2, 20]. Most of these approaches are based on starting with an estimate of the matrix and doing fast updates of this estimate whenever a new observation is presented. One of the update procedures used in this context is that of stochastic gradient descent (SGD) applied to the following non-convex optimization problem\nmin U,V\n\u2016M\u2212UV\u22a4\u20162F s.t. U \u2208 Rd1\u00d7k,V \u2208 Rd2\u00d7k, (1)\nwhere M is the unknown matrix of size d1 \u00d7 d2, k is the rank of M and UV\u22a4 is a low rank factorization of M we wish to obtain. The algorithm starts with some U0 and V0, and given a new observation (M)ij , SGD updates the i\nth-row and the jth-row of the current iterates Ut and Vt respectively by\nU (i) t+1 = U (i) t \u2212 2\u03b7d1d2 ( UtV \u22a4 t \u2212M ) ij V (j) t , and, V (j) t+1 = V (j) t \u2212 2\u03b7d1d2 ( UtV \u22a4 t \u2212M ) ij U (i) t , (2)\nwhere \u03b7 is an appropriately chosen stepsize, and U(i) denote the ith row of matrix U. Note that each update modifies only one row of the factor matrices U and V, and the computation only involves one row of U,V and the new observed entry (M)ij and hence are extremely fast. These fast updates make SGD extremely appealing in practice. Moreover, SGD, in the context of matrix completion, is also useful for parallelization and distributed implementation [24].\n1.1 Our Contributions\nIn this work we present the first provable efficient algorithm for online matrix completion by showing that SGD (2) with a good initialization converges to a true factorization of M at a geometric rate. Our main contributions are as follows.\n\u2022 We provide the first provable, efficient, online algorithm for matrix completion. Starting with a good initialization, after each observation, the algorithm makes quick updates each taking time O(k3) and requires O(\u00b5dk\u03ba4(k+log \u2016M\u2016F\n\u01eb ) log d) observations to reach \u01eb accuracy, where\n\u00b5 is the incoherence parameter, d = max(d1, d2), k is the rank and \u03ba is the condition number of M.\n\u2022 Moreover, our result features both sample complexity and total runtime linear in d, and is competitive to even the best existing offline results for matrix completion. (either improve over or is incomparable, i.e., better in some parameters and worse in others, to these results). See Table 1 for the comparison.\n\u2022 To obtain our results, we introduce a general framework to show SGD updates tend to stay away from saddle surfaces. In order to do so, we consider distances from saddle surfaces, show that they behave like sub-martingales under SGD updates and use martingale convergence techniques to conclude that the iterates stay away from saddle surfaces. While [25] shows that SGD updates stay away from saddle surfaces, the stepsizes they can handle are quite small (scaling as 1/poly(d1, d2)), leading to suboptimal computational complexity. Our framework makes it possible to establish the same statement for much larger step sizes, giving us nearoptimal runtime. We believe these techniques may be applicable in other non-convex settings as well.\n1.2 Related Work\nIn this section we will mention some more related work. Offline matrix completion: There has been a lot of work on designing offline algorithms for matrix completion, we provide the detailed comparison with our algorithm in Table 1. The nuclear norm relaxation algorithm [23] has near-optimal sample complexity for this problem but is computationally expensive. Motivated by the empirical success of non-convex heuristics, a long line of works, [15, 9, 13, 25] and so on, has obtained convergence guarantees for alternating minimization, gradient descent, projected gradient descent etc. Even the best of these are suboptimal in sample complexity by poly(k, \u03ba) factors. Our sample complexity is better than that of [15] and is incomparable to those of [9, 13]. To the best of our knowledge, the only provable online algorithm for this problem is that of Sun and Luo [25]. However the stepsizes they suggest are quite small, leading to suboptimal computational complexity by factors of poly(d1, d2). The runtime of our algorithm is linear in d, which makes poly(d) improvements over it.\nOther models for online matrix completion: Another variant of online matrix completion studied in the literature is where observations are made on a column by column basis e.g., [17, 27]. These models can give improved offline performance in terms of space and could potentially work under relaxed regularity conditions. However, they do not tackle the version where only entries (as opposed to columns) are observed.\nNon-convex optimization: Over the last few years, there has also been a significant amount of work in designing other efficient algorithms for solving non-convex problems. Examples include\neigenvector computation [6, 12], sparse coding [21, 1] etc. For general non-convex optimization, an interesting line of recent work is that of [7], which proves gradient descent with noise can also escape saddle point, but they only provide polynomial rate without explicit dependence. Later [18, 22] show that without noise, the space of points from where gradient descent converges to a saddle point is a measure zero set. However, they do not provide a rate of convergence. Another related piece of work to ours is [11], proves global convergence along with rates of convergence, for the special case of computing matrix squareroot. During the preparation of this draft, the recent work [8] was announced which proves the global convergence of SGD for matrix completion and can also be applied to the online setting. However, their result only deals with the case where M is positive semidefinite (PSD) and their rate is still suboptimal by factors of poly(d1, d2).\n1.3 Outline\nThe rest of the paper is organized as follows. In Section 2 we formally describe the problem and all relevant parameters. In Section 3, we present our algorithms, results and some of the key intuition behind our results. In Section 4 we give proof outline for our main results. We conclude in Section 5. All formal proofs are deferred to the Appendix.\n2 Preliminaries\nIn this section, we introduce our notation, formally define the matrix completion problem and regularity assumptions that make the problem tractable.\n1This result only applies to the case where M is symmetric PSD\n2.1 Notation\nWe use [d] to denote {1, 2, \u00b7 \u00b7 \u00b7 , d}. We use bold capital letters A,B to denote matrices and bold lowercase letters u,v to denote vectors. Aij means the (i, j)\nth entry of matrix A. \u2016w\u2016 denotes the \u21132-norm of vector w and \u2016A\u2016/\u2016A\u2016F/\u2016A\u2016\u221e denotes the spectral/Frobenius/infinity norm of matrix A. \u03c3i(A) denotes the i\nth largest singular value of A and \u03c3min(A) denotes the smallest singular value of A. We also let \u03ba(A) = \u2016A\u2016 /\u03c3min(A) denote the condition number of A (i.e., the ratio of largest to smallest singular value). Finally, for orthonormal bases of a subspace W, we also use PW = WW\u22a4 to denote the projection to the subspace spanned by W.\n2.2 Problem statement and assumptions\nConsider a general rank k matrix M \u2208 Rd1\u00d7d2 . Let \u2126 \u2282 [d1]\u00d7 [d2] be a subset of coordinates, which are sampled uniformly and independently from [d1]\u00d7 [d2]. We denote P\u2126(M) to be the projection of M on set \u2126 so that:\n[P\u2126(M)]ij = {\nMij , if (i, j) \u2208 \u2126 0, if (i, j) 6\u2208 \u2126\nLow rank matrix completion is the task of recovering M by only observing P\u2126(M). This task is ill-posed and NP-hard in general [10]. In order to make this tractable, we make by now standard assumptions about the structure of M.\nDefinition 2.1. Let W \u2208 Rd\u00d7k be an orthonormal basis of a subspace of Rd of dimension k. The coherence of W is defined to be\n\u00b5(W) def =\nd k max 1\u2264i\u2264d \u2016PWei\u20162 = d k max 1\u2264i\u2264d\n\u2225\u2225\u2225e\u22a4i W \u2225\u2225\u2225 2\nAssumption 2.2 (\u00b5-incoherence[4, 23]). We assume M is \u00b5-incoherent, i.e., max{\u00b5(X), \u00b5(Y)} \u2264 \u00b5, where X \u2208 Rd1\u00d7k,Y \u2208 Rd2\u00d7k are the left and right singular vectors of M.\n3 Main Results\nIn this section, we present our main result. We will first state result for a special case where M is a symmetric positive semi-definite (PSD) matrix, where the algorithm and analysis are much simpler. We will then discuss the general case.\n3.1 Symmetric PSD Case\nConsider the special case where M is symmetric PSD. We let d def = d1 = d2, and we can parametrize a rank k symmetric PSD matrix by UU\u22a4 where U \u2208 Rd\u00d7k. Our algorithm for this case is given in Algorithm 1. The following theorem provides guarantees on the performance of Algorithm 1. The algorithm starts by using an initial set of samples \u2126init to construct a crude approximation to the low rank of factorization of M. It then observes samples from M one at a time and updates its factorization after every observation. Note that each update step modifies two rows of Ut and hence takes time O(k).\nAlgorithm 1 Online Algorithm for PSD Matrix Completion. Input: Initial set of uniformly random samples \u2126init of a symmetric PSD matrix M \u2208 Rd\u00d7d, learning rate \u03b7, iterations T Output: U such that UU\u22a4 \u2248 M U0U \u22a4 0 \u2190 top k SVD of d 2\n|\u2126init|P\u2126init(M) for t = 0, \u00b7 \u00b7 \u00b7 , T \u2212 1 do\nObserve Mij where (i, j) \u223c Unif ([d]\u00d7 [d]) Ut+1 \u2190 Ut \u2212 2\u03b7d2(UtU\u22a4t \u2212M)ij(eie\u22a4j + eje\u22a4i )Ut\nend for\nReturn UT\nTheorem 3.1. Let M \u2208 Rd\u00d7d be a rank k, symmetric PSD matrix with \u00b5-incoherence. There exist some absolute constants c0 and c such that if |\u2126init| \u2265 c0\u00b5dk2\u03ba2(M) log d, learning rate \u03b7 \u2264 c \u00b5dk\u03ba3(M)\u2016M\u2016 log d , then for any fixed T \u2265 1, with probability at least 1 \u2212 T d10 , we will have for all t \u2264 T that: \u2016UtU\u22a4t \u2212M\u20162F \u2264 ( 1\u2212 1\n2 \u03b7 \u00b7 \u03c3min(M) )t( 1 10 \u03c3min(M) )2 .\nRemarks:\n\u2022 The algorithm uses an initial set of observations \u2126init to produce a warm start iterate U0, then enters the online stage, where it performs SGD.\n\u2022 The sample complexity of the warm start phase is O(\u00b5dk2\u03ba2(M) log d). The initialization consists of a top-k SVD on a sparse matrix, whose runtime is O(\u00b5dk3\u03ba2(M) log d).\n\u2022 For the online phase (SGD), if we choose \u03b7 = c \u00b5dk\u03ba3(M)\u2016M\u2016 log d , the number of observations T\nrequired for the error \u2016UTU\u22a4T \u2212M\u2016F to be smaller than \u01eb is O(\u00b5dk\u03ba(M)4 log d log \u03c3min(M) \u01eb ).\n\u2022 Since each SGD step modifies two rows of Ut, its runtime is O(k) with a total runtime for online phase of O(kT ).\nOur proof approach is to essentially show that the objective function is well-behaved (i.e., is smooth and strongly convex) in a local neighborhood of the warm start region, and then use standard techniques to show that SGD obtains geometric convergence in this setting. The most challenging and novel part of our analysis comprises of showing that the iterate does not leave this local neighborhood while performing SGD updates. Refer Section 4 for more details on the proof outline.\n3.2 General Case\nLet us now consider the general case where M \u2208 Rd1\u00d7d2 can be factorized as UV\u22a4 with U \u2208 Rd1\u00d7k and V \u2208 Rd2\u00d7k. In this scenario, we denote d = max{d1, d2}. We recall our remarks from the previous section that our analysis of the performance of SGD depends on the smoothness and strong convexity properties of the objective function in a local neighborhood of the iterates. Having U 6= V introduces additional challenges in this approach since for any nonsingular k-by-k matrix C, and U\u2032 def = UC\u22a4,V\u2032 def = VC\u22121, we have U\u2032V\u2032\u22a4 = UV\u22a4. Suppose for instance C is a very small scalar times the identity i.e., C = \u03b4I for some small \u03b4 > 0. In this case, U\u2032 will be large while V\u2032\nwill be small. This drastically deteriorates the smoothness and strong convexity properties of the objective function in a neighborhood of (U\u2032,V\u2032).\nAlgorithm 2 Online Algorithm for Matrix Completion (Theoretical) Input: Initial set of uniformly random samples \u2126init of M \u2208 Rd1\u00d7d2 , learning rate \u03b7, iterations T Output: U,V such that UV\u22a4 \u2248 M\nU0V \u22a4 0 \u2190 top k SVD of d1d2|\u2126init|P\u2126init(M) for t = 0, \u00b7 \u00b7 \u00b7 , T \u2212 1 do WUDW \u22a4 V \u2190 SVD(UtV\u22a4t )\nU\u0303t \u2190 WUD 1 2 , V\u0303t \u2190 WV D 1 2 Observe Mij where (i, j) \u223c Unif ([d]\u00d7 [d]) Ut+1 \u2190 U\u0303t \u2212 2\u03b7d1d2(U\u0303tV\u0303\u22a4t \u2212M)ijeie\u22a4j V\u0303t Vt+1 \u2190 V\u0303t \u2212 2\u03b7d1d2(U\u0303tV\u0303\u22a4t \u2212M)ijeje\u22a4i U\u0303t\nend for Return UT ,VT .\nTo preclude such a scenario, we would ideally like to renormalize after each step by doing U\u0303t \u2190 WUD 1 2 , V\u0303t \u2190 WV D 1 2 , where WUDW \u22a4 V is the SVD of matrix UtV \u22a4 t . This algorithm is described in Algorithm 2. However, a naive implementation of Algorithm 2, especially the SVD step, would incur O(min{d1, d2}) computation per iteration, resulting in a runtime overhead of O(d) over both the online PSD case (i.e., Algorithm 1) as well as the near linear time offline algorithms (see Table 1). It turns out that we can take advantage of the fact that in each iteration we only update a single row of Ut and a single row of Vt, and do efficient (but more complicated) update steps instead of doing an SVD on d1 \u00d7 d2 matrix. The resulting algorithm is given in Algorithm 3. The key idea is that in order to implement the updates, it suffices to do an SVD of U\u22a4t Ut and V\u22a4t Vt which are k \u00d7 k matrices. So the runtime of each iteration is at most O(k3). The following lemma shows the equivalence between Algorithms 2 and 3.\nAlgorithm 3 Online Algorithm for Matrix Completion (Practical) Input: Initial set of uniformly random samples \u2126init of M \u2208 Rd1\u00d7d2 , learning rate \u03b7, iterations T Output: U,V such that UV\u22a4 \u2248 M\nU0V \u22a4 0 \u2190 top k SVD of d1d2\u2126initP\u2126init(M) for t = 0, \u00b7 \u00b7 \u00b7 , T \u2212 1 do RUDUR \u22a4 U \u2190 SVD(U\u22a4t Ut)\nRV DV R \u22a4 V \u2190 SVD(V\u22a4t Vt) QUDQ \u22a4 V \u2190 SVD(D 1 2 UR \u22a4 URV (D 1 2 V ) \u22a4) Observe Mij where (i, j) \u223c Unif ([d]\u00d7 [d]) Ut+1 \u2190 Ut \u2212 2\u03b7d1d2(UtV\u22a4t \u2212M)ijeie\u22a4j VtRV D \u2212 1 2 V QVQ \u22a4 UD 1 2 UR \u22a4 U Vt+1 \u2190 Vt \u2212 2\u03b7d1d2(UtV\u22a4t \u2212M)ijeje\u22a4i UtRUD \u2212 1 2 U QUQ \u22a4 V D 1 2 V R \u22a4 V\nend for Return UT ,VT .\nLemma 3.2. Algorithm 2 and Algorithm 3 are equivalent in the sense that: given same observations\nfrom M and other inputs, the outputs of Algorithm 2, U,V and those of Algorithm 3, U\u2032,V\u2032 satisfy UV\u22a4 = U\u2032V\u2032\u22a4.\nSince the output of both algorithms is the same, we can analyze Algorithm 2 (which is easier than that of Algorithm 3), while implementing Algorithm 3 in practice. The following theorem is the main result of our paper which presents guarantees on the performance of Algorithm 2.\nTheorem 3.3. Let M \u2208 Rd1\u00d7d2 be a rank k matrix with \u00b5-incoherence and let d def= max(d1, d2). There exist some absolute constants c0 and c such that if |\u2126init| \u2265 c0\u00b5dk2\u03ba2(M) log d, learning rate \u03b7 \u2264 c\n\u00b5dk\u03ba3(M)\u2016M\u2016 log d , then for any fixed T \u2265 1, with probability at least 1\u2212 T d10 , we will have for all\nt \u2264 T that: \u2016UtV\u22a4t \u2212M\u20162F \u2264 ( 1\u2212 1\n2 \u03b7 \u00b7 \u03c3min(M) )t( 1 10 \u03c3min(M) )2 .\nRemarks:\n\u2022 Just as in the case of PSD matrix completion (Theorem 3.1), Algorithm 2 needs a an initial set of observations \u2126init to provide a warm start U0 and V0 after which it performs SGD.\n\u2022 The sample complexity and runtime of the warm start phase are the same as in symmetric PSD case. The stepsize \u03b7 and the number of observations T to achieve \u01eb error in online phase (SGD) are also the same as in symmetric PSD case.\n\u2022 However, runtime of each update step in online phase is O(k3) with total runtime for online phase O(k3T ).\nThe proof of this theorem again follows a similar line of reasoning as that of Theorem 3.1 by first showing that the local neighborhood of warm start iterate has good smoothness and strong convexity properties and then use them to show geometric convergence of SGD. Proof of the fact that iterates do not move away from this local neighborhood however is significantly more challenging due to renormalization steps in the algorithm. Please see Appendix C for the full proof.\n4 Proof Sketch\nIn this section we will provide the intuition and proof sketch for our main results. For simplicity and highlighting the most essential ideas, we will mostly focus on the symmetric PSD case (Theorem 3.1). For the asymmetric case, though the high-level ideas are still valid, a lot of additional effort is required to address the renormalization step in Algorithm 2. This makes the proof more involved.\nFirst, note that our algorithm for the PSD case consists of an initialization and then stochastic descent steps. The following lemma provides guarantees on the error achieved by the initial iterate U0.\nLemma 4.1. Let M \u2208 Rd\u00d7d be a rank-k PSD matrix with \u00b5-incoherence. There exists a constant c0 such that if |\u2126init| \u2265 c0\u00b5dk2\u03ba2(M) log d, then with probability at least 1\u2212 1d10 , the top-k SVD of d2 |init|P\u2126init(M) satisfies Then there exists universal constant c0, for any m \u2265, we have:\n\u2016M\u2212U0U\u22a40 \u2016F \u2264 1\n20 \u03c3min(M) and max j\n\u2225\u2225\u2225e\u22a4j U0 \u2225\u2225\u2225 2 \u2264 10\u00b5k\u03ba(M)\nd \u2016M\u2016 (3)\nBy Lemma 4.1, we know the initialization algorithm already gives U0 in the local region given by Eq.(3). Intuitively, stochastic descent steps should keep doing local search within this local region.\nTo establish linear convergence on \u2016UtU\u22a4t \u2212M\u20162F and obtain final result, we first establish several important lemmas describing the properties of this local regions. Throughout this section, we always denote SVD(M) = XSX\u22a4, whereX \u2208 Rd\u00d7k, and diagnal matrix S \u2208 Rk\u00d7k. We postpone all the formal proofs in Appendix.\nLemma 4.2. For function f(U) = \u2016M\u2212UU\u22a4\u20162F and any U1,U2 \u2208 {U| \u2016U\u2016 \u2264 \u0393}, we have:\n\u2016\u2207f(U1)\u2212\u2207f(U2)\u2016F \u2264 16max{\u03932, \u2016M\u2016} \u00b7 \u2016U1 \u2212U2\u2016F\nLemma 4.3. For function f(U) = \u2016M\u2212UU\u22a4\u20162F and any U \u2208 {U|\u03c3min(X\u22a4U) \u2265 \u03b3}, we have:\n\u2016\u2207f(U)\u20162F \u2265 4\u03b32f(U)\nLemma 4.2 tells function f is smooth if spectral norm of U is not very large. On the other hand, \u03c3min(X\n\u22a4U) not too small requires both \u03c3min(U\u22a4U) and \u03c3min(X\u22a4W) are not too small, where W is top-k eigenspace of UU\u22a4. That is, Lemma 4.3 tells function f has a property similar to strongly convex in standard optimization literature, if U is rank k in a robust sense (\u03c3k(U) is not too small), and the angle between the top k eigenspace of UU\u22a4 and the top k eigenspace M is not large. Lemma 4.4. Within the region D = {U| \u2225\u2225M\u2212UU\u22a4\n\u2225\u2225 F \u2264 110\u03c3k(M)}, we have:\n\u2016U\u2016 \u2264 \u221a 2 \u2016M\u2016, \u03c3min(X\u22a4U) \u2265 \u221a \u03c3k(M)/2\nLemma 4.4 tells inside region {U| \u2225\u2225M\u2212UU\u22a4 \u2225\u2225 F\n\u2264 110\u03c3k(M)}, matrix U always has a good spectral property which gives preconditions for both Lemma 4.2 and 4.3, where f(U) is both smooth and has a property very similar to strongly convex.\nWith above three lemmas, we already been able to see the intuition behind linear convergence in Theorem 3.1. Denote stochastic gradient\nSG(U) = 2d2(UU\u22a4 \u2212M)ij(eie\u22a4j + eje\u22a4i )U (4)\nwhere SG(U) is a random matrix depends on the randomness of sample (i, j) of matrix M. Then, the stochastic update step in Algorithm 1 can be rewritten as:\nUt+1 \u2190 Ut \u2212 \u03b7SG(Ut)\nLet f(U) = \u2016M\u2212UU\u22a4\u20162F, By easy caculation, we know ESG(U) = \u2207f(U), that is SG(U) is unbiased. Combine Lemma 4.4 with Lemma 4.2 and Lemma 4.3, we know within region D specified by Lemma 4.4, we have function f(U) is 32 \u2016M\u2016-smooth, and \u2016\u2207f(U)\u20162F \u2265 2\u03c3min(M)f(U).\nLet\u2019s suppose ideally, we always have U0, . . . ,Ut inside region D, this directly gives:\nEf(Ut+1) \u2264 Ef(Ut)\u2212 \u03b7E\u3008\u2207f(Ut), SG(Ut)\u3009+ 16\u03b72 \u2016M\u2016 \u00b7 E\u2016SG(Ut)\u20162F = Ef(Ut)\u2212 \u03b7E\u2016\u2207f(Ut)\u20162F + 16\u03b72 \u2016M\u2016 \u00b7 E\u2016SG(Ut)\u20162F \u2264 (1\u2212 2\u03b7\u03c3min(M))Ef(Ut) + 16\u03b72 \u2016M\u2016 \u00b7 E\u2016SG(Ut)\u20162F\nOne interesting aspect of our main result is that we actually show linear convergence under the presence of noise in gradient. This is true because for the second-order (\u03b72) term above, we can roughly see from Eq.(4) that \u2016SG(U)\u20162F \u2264 h(U) \u00b7 f(U), where h(U) is a factor depends on U and always bounded. That is, SG(U) enjoys self-bounded property \u2014 \u2016SG(U)\u20162F will goes to zero, as objective function f(U) goes to zero. Therefore, by choosing learning rate \u03b7 appropriately small, we can have the first-order term always dominate the second-order term, which establish the linear convergence.\nNow, the only remaining issue is to prove that \u201cU0, . . . ,Ut always stay inside local region D\u201d. In reality, we can only prove this statement with high probability due to the stochastic nature of the update. This is also the most challenging part in our proof, which makes our analysis different from standard convex analysis, and uniquely required due to non-convex setting.\nOur key theorem is presented as follows:\nTheorem 4.5. Let f(U) = \u2225\u2225UU\u22a4 \u2212M \u2225\u22252 F and gi(U) = \u2225\u2225e\u22a4i U \u2225\u22252. Suppose initial U0 satisfying:\nf(U0) \u2264 ( \u03c3min(M)\n20\n)2 , max\ni gi(U0) \u2264\n10\u00b5k\u03ba(M)2\nd \u2016M\u2016\nThen, there exist some absolute constant c such that for any learning rate \u03b7 < c \u00b5dk\u03ba3(M)\u2016M\u2016 log d , with at least 1\u2212 T d10 probability, we will have for all t \u2264 T that:\nf(Ut) \u2264 (1\u2212 1\n2 \u03b7\u03c3min(M))\nt\n( \u03c3min(M)\n10\n)2 , max\ni gi(Ut) \u2264\n20\u00b5k\u03ba(M)2\nd \u2016M\u2016 (5)\nNote function maxi gi(U) indicates the incoherence of matrix U. Theorem 4.5 guarantees if inital U0 is in the local region which is incoherent and U0U \u22a4 0 is close to M, then with high probability for all steps t \u2264 T , Ut, Ut will always stay in a slightly relaxed local region, and f(Ut) has linear convergence.\nIt is not hard to show that all saddle point of f(U) satisfies \u03c3k(U) = 0, and all local minima are global minima. Since U0, . . . ,Ut automatically stay in region f(U) \u2264 (\u03c3min(M)10 )2 with high probability, we know Ut also stay away from all saddle points. The claim that U0, . . . ,Ut stays incoherent is essential to better control the variance and probability 1 bound of SG(Ut), so that we can have large step size and tight convergence rate.\nThe major challenging in proving Theorem 4.5 is to both prove Ut stays in the local region, and achieve good sample complexity and running time (linear in d) in the same time. This also requires the learning rate \u03b7 in Algorithm 1 to be relatively large. Let the event Et denote the good event where U0, . . . ,Ut satisfies Eq.(5). Theorem 4.5 is claiming that P (ET ) is large. The essential steps in the proof is contructing two supermartingles related to f(Ut)1Et and gi(Ut)1Et (where 1(\u00b7) denote indicator function), and use Bernstein inequalty to show the concentration of supermartingales. The 1Etterm allow us the claim all previousU0, . . . ,Ut have all desired properties inside local region.\nFinally, we see Theorem 3.1 as a immediate corollary of Theorem 4.5.\n5 Conclusion\nIn this paper, we presented the first provable, efficient online algorithm for matrix completion, based on nonconvex SGD. In addition to the online setting, our results are also competitive with state of\nthe art results in the offline setting. We obtain our results by introducing a general framework that helps us show how SGD updates self-regulate to stay away from saddle points. We hope our paper and results help generate interest in online matrix completion, and our techniques and framework prompt tighter analysis for other nonconvex problems.\nReferences\n[1] Sanjeev Arora, Rong Ge, Tengyu Ma, and Ankur Moitra. Simple, efficient, and neural algorithms for sparse coding. arXiv preprint arXiv:1503.00778, 2015.\n[2] Matthew Brand. Fast online svd revisions for lightweight recommender systems. In SDM, pages 37\u201346. SIAM, 2003.\n[3] Emmanuel J Candes, Yonina C Eldar, Thomas Strohmer, and Vladislav Voroninski. Phase retrieval via matrix completion. SIAM Review, 57(2):225\u2013251, 2015.\n[4] Emmanuel J. Cande\u0300s and Benjamin Recht. Exact matrix completion via convex optimization. Foundations of Computational Mathematics, 9(6):717\u2013772, December 2009.\n[5] James Davidson, Benjamin Liebald, Junning Liu, Palash Nandy, Taylor Van Vleet, Ullas Gargi, Sujoy Gupta, Yu He, Mike Lambert, Blake Livingston, et al. The youtube video recommendation system. In Proceedings of the fourth ACM conference on Recommender systems, pages 293\u2013296. ACM, 2010.\n[6] Christopher De Sa, Kunle Olukotun, and Christopher Re\u0301. Global convergence of stochastic gradient descent for some non-convex matrix problems. arXiv preprint arXiv:1411.1134, 2014.\n[7] Rong Ge, Furong Huang, Chi Jin, and Yang Yuan. Escaping from saddle points\u2014online stochastic gradient for tensor decomposition. arXiv preprint arXiv:1503.02101, 2015.\n[8] Rong Ge, Jason D. Lee, and Tengyu Ma. Matrix completion has no spurious local minimum. arXiv preprint arXiv:1605.07272, 2016.\n[9] Marcus Hardt. Understanding alternating minimization for matrix completion. In Foundations of Computer Science (FOCS), 2014 IEEE 55th Annual Symposium on, pages 651\u2013660. IEEE, 2014.\n[10] Moritz Hardt, Raghu Meka, Prasad Raghavendra, and Benjamin Weitz. Computational limits for matrix completion. In COLT, pages 703\u2013725, 2014.\n[11] Prateek Jain, Chi Jin, Sham M Kakade, and Praneeth Netrapalli. Computing matrix squareroot via non convex local search. arXiv preprint arXiv:1507.05854, 2015.\n[12] Prateek Jain, Chi Jin, Sham M Kakade, Praneeth Netrapalli, and Aaron Sidford. Matching matrix bernstein with little memory: Near-optimal finite sample guarantees for oja\u2019s algorithm. arXiv preprint arXiv:1602.06929, 2016.\n[13] Prateek Jain and Praneeth Netrapalli. Fast exact matrix completion with finite samples. arXiv preprint arXiv:1411.1087, 2014.\n[14] Hui Ji, Chaoqiang Liu, Zuowei Shen, and Yuhong Xu. Robust video denoising using low rank matrix completion. 2010.\n[15] Raghunandan Hulikal Keshavan. Efficient algorithms for collaborative filtering. PhD thesis, STANFORD UNIVERSITY, 2012.\n[16] Yehuda Koren. The BellKor solution to the Netflix grand prize, 2009.\n[17] Akshay Krishnamurthy and Aarti Singh. Low-rank matrix and tensor completion via adaptive sampling. In Advances in Neural Information Processing Systems, pages 836\u2013844, 2013.\n[18] Jason D Lee, Max Simchowitz, Michael I Jordan, and Benjamin Recht. Gradient descent converges to minimizers. University of California, Berkeley, 1050:16, 2016.\n[19] G. Linden, B. Smith, and J. York. Amazon.com recommendations: item-to-item collaborative filtering. IEEE Internet Computing, 7(1):76\u201380, Jan 2003.\n[20] Xin Luo, Yunni Xia, and Qingsheng Zhu. Incremental collaborative filtering recommender based on regularized matrix factorization. Knowledge-Based Systems, 27:271\u2013280, 2012.\n[21] Julien Mairal, Francis Bach, Jean Ponce, and Guillermo Sapiro. Online learning for matrix factorization and sparse coding. The Journal of Machine Learning Research, 11:19\u201360, 2010.\n[22] Ioannis Panageas and Georgios Piliouras. Gradient descent converges to minimizers: The case of non-isolated critical points. arXiv preprint arXiv:1605.00405, 2016.\n[23] Benjamin Recht. A simple approach to matrix completion, 2009.\n[24] Benjamin Recht and Christopher Re\u0301. Parallel stochastic gradient algorithms for large-scale matrix completion. Mathematical Programming Computation, 5(2):201\u2013226, 2013.\n[25] Ruoyu Sun and Zhi-Quan Luo. Guaranteed matrix completion via nonconvex factorization. In Foundations of Computer Science (FOCS), 2015 IEEE 56th Annual Symposium on, pages 270\u2013289. IEEE, 2015.\n[26] Joel A Tropp. User-friendly tail bounds for sums of random matrices. Foundations of computational mathematics, 12(4):389\u2013434, 2012.\n[27] Se-Young Yun, Marc Lelarge, and Alexandre Proutiere. Streaming, memory limited matrix completion with noise. arXiv preprint arXiv:1504.03156, 2015.\nA Proof of Initialization\nIn this section, we will prove Lemma 4.1 and a corresponding lemma for asymmetric case as follows (which will be used to prove Theorem 3.3):\nLemma A.1. Assume M \u2208 Rd1\u00d7d2 is a rank k matrix with \u00b5-incoherence, and \u2126 is a subset unformly i.i.d sampled from all coordinate. Let U0V \u22a4 0 be the top-k SVD of d1d2 m\nP\u2126(M), where |\u2126| = m. Let d = max{d1, d2}. Then there exists universal constant c0, for any m \u2265 c0\u00b5dk2\u03ba2(M) log d, with probability at least 1\u2212 1\nd10 , we have:\n\u2016M\u2212U0V\u22a40 \u2016F \u2264 1\n20 \u03c3min(M),\nmax i\n\u2225\u2225\u2225e\u22a4i U0V\u22a40 \u2225\u2225\u2225 2 \u2264 10\u00b5k\nd1 \u2016M\u2016 , max j\n\u2225\u2225\u2225e\u22a4j V0U\u22a40 \u2225\u2225\u2225 2 \u2264 10\u00b5k\nd2 \u2016M\u2016 (6)\nWe will focus mostly on Lemma A.1, and prove Lemma 4.1 as a special case. Most of the argument of this section follows from [15]. We include here for completeness. The remaining of this section can be viewed as proving both the Frobenius norm claim and incoherence claim of Lemma A.1 seperately.\nIn this section, We always denote d = max{d1, d2}. For simplicity, WLOG, we also assume \u2016M\u2016 = 1 in all proof. Also, when it\u2019s clear from the context, we use \u03ba to specifically to represent \u03ba(M). Then \u03c3min(M) = 1 \u03ba . Also in the proof, we always denote SVD(M) = XSY\u22a4, and SVD(UV\u22a4) = WUDW\u22a4V, where S and D are k \u00d7 k diagonal matrix.\nA.1 Frobenius Norm of Initialization\nTheorem A.2 (Matrix Bernstein [26]). A finite sequence {Xt} of independent, random matrices with dimension d! \u00d7 d2. Assume that each matrix satisfies:\nEXt = 0, and \u2016Xt\u2016 \u2264 R almost surely\nDefine\n\u03c32 = max{ \u2225\u2225\u2225\u2225\u2225 \u2211\nt\nE(XtX \u22a4 t ) \u2225\u2225\u2225\u2225\u2225 , \u2225\u2225\u2225\u2225\u2225 \u2211\nt\nE(X\u22a4t Xt) \u2225\u2225\u2225\u2225\u2225}\nThen, for all s \u2265 0,\nPr( \u2225\u2225\u2225\u2225\u2225 \u2211\nt\nXt \u2225\u2225\u2225\u2225\u2225 \u2265 s) \u2264 (d1 + d2) \u00b7 exp( \u2212s2/2 \u03c32 +Rs/3 )\nLemma A.3. Let |\u2126| = m, then there exists universal constant C, c0, for any m \u2265 c0\u00b5dk log d, with probability at least 1\u2212 1\nd10 , we have:\n\u2225\u2225\u2225\u2225M\u2212 d1d2 m P\u2126(M) \u2225\u2225\u2225\u2225 \u2264 C \u221a \u00b5dk log d m\nProof. We know \u2225\u2225\u2225\u2225M\u2212 d1d2 m P\u2126(M) \u2225\u2225\u2225\u2225 = d1d2 m \u2225\u2225\u2225\u2225P\u2126(M)\u2212 m d1d2 M \u2225\u2225\u2225\u2225\nand note: P\u2126(M)\u2212 m\nd1d2 M =\n\u2211\nij\nMij(Zij \u2212 m\nd1d2 )eie\n\u22a4 j\nwhere Zij are independence Bernoulli(m/d1d2) random variables. Let matrix\n\u03c8ij = Mij(Zij \u2212 m\nd1d2 )eie\n\u22a4 j\nBy construction, we have: \u2225\u2225\u2225\u2225\u2225\u2225 \u2211\nij\n\u03c8ij \u2225\u2225\u2225\u2225\u2225\u2225 = \u2225\u2225\u2225\u2225P\u2126(M)\u2212 m d1d2 M \u2225\u2225\u2225\u2225\nClearly E\u03c8ij = 0. Let XSY \u22a4 = SVD(M), then by \u00b5-incoherence of M, with probability 1:\n\u2016M\u2016\u221e \u2264 max ij |e\u22a4i XSY\u22a4ej| \u2264 \u2016M\u2016 \u00b5k\u221a d1d2\nAlso: \u2225\u2225\u2225\u2225\u2225\u2225 \u2211\nij\nE(\u03c8ij\u03c8 \u22a4 ij) \u2225\u2225\u2225\u2225\u2225\u2225 = \u2225\u2225\u2225\u2225\u2225\u2225 \u2211\nij\nEM2ij(Zij \u2212 m\nd1d2 )2eie \u22a4 i \u2225\u2225\u2225\u2225\u2225\u2225 \u2264 m d1d2 (1\u2212 m d1d2 ) \u2225\u2225\u2225\u2225\u2225\u2225 \u2211\nij\nM2ijeie \u22a4 i \u2225\u2225\u2225\u2225\u2225\u2225\n= m d1d2 (1\u2212 m d1d2 )max i\n\u2211\nj\nM2ij \u2264 2m\nd1d2\n\u00b5k d1 \u2016M\u20162 = 2m\u00b5k d21d2 \u2016M\u20162\n\u2225\u2225\u2225\u2225\u2225\u2225 \u2211\nij\nE(\u03c8\u22a4ij\u03c8ij) \u2225\u2225\u2225\u2225\u2225\u2225 = \u2225\u2225\u2225\u2225\u2225\u2225 \u2211\nij\nEM2ij(Zij \u2212 m\nd1d2 )2eje \u22a4 j \u2225\u2225\u2225\u2225\u2225\u2225 \u2264 m d1d2 (1\u2212 m d1d2 ) \u2225\u2225\u2225\u2225\u2225\u2225 \u2211\nij\nM2ijeje \u22a4 j \u2225\u2225\u2225\u2225\u2225\u2225\n= m d1d2 (1\u2212 m d1d2 )max j\n\u2211\ni\nM2ij \u2264 2m\nd1d2\n\u00b5k d2 \u2016M\u20162 = 2m\u00b5k d1d 2 2 \u2016M\u20162\nThen, by matrix Bernstein (Theorem A.2), we have:\nPr( \u2225\u2225\u2225\u2225\u2225\u2225 \u2211\nij\n\u03c8ij \u2225\u2225\u2225\u2225\u2225\u2225 \u2265 s) \u2264 2(d1 + d2) \u00b7 exp( \u2212s2/2 2m\u00b5dk d2 1 d2 2 \u2016M\u20162 + \u2016M\u2016 \u00b5k 3 \u221a d1d2 s )\nThat is, with probability at least 1\u2212 1 d10 , for some universal constant C, we have:\n\u2225\u2225\u2225\u2225P\u2126(M)\u2212 m\nd1d2 M\n\u2225\u2225\u2225\u2225 \u2264 C \u2016M\u2016 \u00b7max{ \u221a m\u00b5dk log d\nd21d 2 2\n, \u00b5k log d\u221a\nd1d2 }\nFor m \u2265 \u00b5dk log d, we finishes the proof.\nTheorem A.4. Let U0V \u22a4 0 be the top-k SVD of d1d2 m P\u2126(M), where |\u2126| = m then there exists universal constant c0, for any m \u2265 c0\u00b5dk2\u03ba2 log d, with probability at least 1\u2212 1d10 , we have:\n\u2225\u2225\u2225M\u2212U0V\u22a40 \u2225\u2225\u2225 F \u2264 1 20\u03ba\nProof. Since M is a rank k matrix, we know \u03c3k+1(M) = 0, thus\n\u03c3k+1( d1d2 m P\u2126(M)) \u2264 \u03c3k+1(M) + \u2225\u2225\u2225\u2225 d1d2 m P\u2126(M)\u2212M \u2225\u2225\u2225\u2225 = \u2225\u2225\u2225\u2225 d1d2 m P\u2126(M)\u2212M \u2225\u2225\u2225\u2225\nTherefore:\n\u2225\u2225\u2225M\u2212U0V\u22a40 \u2225\u2225\u2225 \u2264 \u2225\u2225\u2225\u2225M\u2212 d1d2 m P\u2126(M) \u2225\u2225\u2225\u2225+ \u2225\u2225\u2225\u2225 d1d2 m P\u2126(M)\u2212U0V\u22a40 \u2225\u2225\u2225\u2225\n\u2264 \u2225\u2225\u2225\u2225M\u2212 d1d2 m P\u2126(M) \u2225\u2225\u2225\u2225+ \u03c3k+1( d1d2 m P\u2126(M)) \u2264 2 \u2225\u2225\u2225\u2225M\u2212 d1d2 m P\u2126(M) \u2225\u2225\u2225\u2225\nMeanwhile, since rank(M) = k, rank(U0V \u22a4 0 ) = k, we know: rank(M\u2212U0V\u22a40 ) \u2264 2k, and therefore:\n\u2225\u2225\u2225M\u2212U0V\u22a40 \u2225\u2225\u2225 F \u2264 \u221a 2k \u2225\u2225\u2225M\u2212U0V\u22a40 \u2225\u2225\u2225 \u2264 2 \u221a 2k \u2225\u2225\u2225\u2225M\u2212 d1d2 m P\u2126(M) \u2225\u2225\u2225\u2225\nby choosing m \u2265 c0\u00b5dk2 log d \u00b7 \u03ba2 for large enough constant c0 and apply Lemma A.3, we finishes the proof.\nA.2 Incoherence of Initialization\nLemma A.5. Let UV\u22a4 be the top-k SVD of d1d2 m P\u2126(M), where |\u2126| = m. then there exists universal constant c0, for any m \u2265 c0\u00b5dk\u03ba2 log d, with probability at least 1\u2212 1d10 , we have:\nmax j\n\u2225\u2225\u2225e\u22a4j (M\u22a4 \u2212VU\u22a4) \u2225\u2225\u2225 \u2264 2\n\u221a \u00b5k\nd2\nProof. Suppose SVD(M) = XSY\u22a4. Denote X\u0303 = XS 1 2 and Y\u0303 = YS 1 2 . Also let SVD(UV\u22a4) = WUDW \u22a4 V .\nThen, we have:\n\u2225\u2225\u2225e\u22a4j (M\u22a4 \u2212VU\u22a4) \u2225\u2225\u2225 = \u2225\u2225\u2225\u2225e \u22a4 j (M \u22a4 \u2212 d1d2 m P\u2126(M)\u22a4WUW\u22a4U) \u2225\u2225\u2225\u2225\n= \u2225\u2225\u2225\u2225e \u22a4 j (M \u22a4 \u2212M\u22a4WUW\u22a4U +M\u22a4WUW\u22a4U \u2212 d1d2 m P\u2126(M)\u22a4WUW\u22a4U) \u2225\u2225\u2225\u2225 \u2264 \u2225\u2225\u2225e\u22a4j M\u22a4(I\u2212WUW\u22a4U) \u2225\u2225\u2225+ \u2225\u2225\u2225\u2225e \u22a4 j (M \u22a4 \u2212 d1d2 m P\u2126(M)\u22a4)WUW\u22a4U \u2225\u2225\u2225\u2225\nFor the first term, since W\u22a4 U WU,\u22a5 = 0, we have:\n\u2225\u2225\u2225e\u22a4j M\u22a4(I\u2212WUW\u22a4U) \u2225\u2225\u2225 \u2264 \u2225\u2225\u2225e\u22a4j Y \u2225\u2225\u2225 \u2225\u2225\u2225SX\u22a4WU,\u22a5W\u22a4U,\u22a5 \u2225\u2225\u2225\n=\n\u221a \u00b5k\nd2\n\u2225\u2225\u2225Y\u22a4(M\u22a4 \u2212WVDW\u22a4U)WU,\u22a5W\u22a4U,\u22a5 \u2225\u2225\u2225\n\u2264 \u221a \u00b5k\nd2\n\u2225\u2225\u2225M\u22a4 \u2212WVDW\u22a4U \u2225\u2225\u2225 \u2264\n\u221a \u00b5k\nd2 \u00b7 1 \u03ba\nThe last step is due to sample m \u2265 \u00b5dk\u03ba2 log d, and theorem A.4. For the second term, we have:\n\u2225\u2225\u2225\u2225e \u22a4 j ( d1d2 m P\u2126(M)\u22a4 \u2212M\u22a4)WUW\u22a4U \u2225\u2225\u2225\u2225 = \u2225\u2225\u2225\u2225\u2225\u2225 Y\u0303\u22a4j ( d1d2 m \u2211 i:(i,j)\u2208\u2126 x\u0303iw \u22a4 U,i \u2212 \u2211 i x\u0303iw \u22a4 U,i)W \u22a4 U \u2225\u2225\u2225\u2225\u2225\u2225\n\u2264 \u221a \u00b5k\nd2 \u00b7 d1d2 m \u00b7 \u2225\u2225\u2225\u2225\u2225\u2225 \u2211 i:(i,j)\u2208\u2126 x\u0303iw \u22a4 U,i \u2212 m d1d2 \u2211 i x\u0303iw \u22a4 U,i \u2225\u2225\u2225\u2225\u2225\u2225 (7)\nWhere x\u0303i and wU,i are the i-th row of X\u0303 and WU respectively. Let \u03c6ij = x\u0303iw\n\u22a4 U,i(Zij\u2212 md1d2 ), where Zij is Bernoulli( m d1d2 ) random variable, Zij = 1 iff (i, j) \u2208 \u2126. Clearly, we have E\u03c6 = 0, and with probability 1:\n\u2016\u03c6ij\u2016 \u2264 2 \u2016x\u0303i\u2016 \u2016wU,i\u2016 \u2264 2 \u221a \u00b5k\nd1 max i\n\u2225\u2225\u2225e\u22a4i WU \u2225\u2225\u2225\nAlso, we have variance term: \u2225\u2225\u2225\u2225\u2225 \u2211\ni\nE\u03c6\u22a4ij\u03c6ij \u2225\u2225\u2225\u2225\u2225 = \u2225\u2225\u2225\u2225\u2225 \u2211\ni\nE(Zij \u2212 m d1d2 )2 \u2016x\u0303i\u20162wU,iw\u22a4U,i \u2225\u2225\u2225\u2225\u2225\n\u2264 m d1d2 (1\u2212 m d1d2 )max i \u2016x\u0303i\u20162 \u2225\u2225\u2225\u2225\u2225 \u2211\ni\nwU,iw \u22a4 U,i \u2225\u2225\u2225\u2225\u2225\n\u2264 m d1d2 \u00b5k d1\n\u2225\u2225\u2225W\u22a4UWU \u2225\u2225\u2225 \u2264 \u00b5km\nd21d2\u2225\u2225\u2225\u2225\u2225 \u2211\ni\nE\u03c6ij\u03c6 \u22a4 ij \u2225\u2225\u2225\u2225\u2225 = \u2225\u2225\u2225\u2225\u2225 \u2211\ni\nE(Zij \u2212 m d1d2 )2 \u2016wU,i\u20162 x\u0303ix\u0303\u22a4i \u2225\u2225\u2225\u2225\u2225\n\u2264 m d1d2 max i\n\u2225\u2225\u2225e\u22a4i WU \u2225\u2225\u2225 2\nTherefore, with m \u2265 \u00b5dk\u03ba2 log d, by matrix Bernstein, we have with probability at least 1\u2212 1 d10\n, we know that for all j \u2208 [d2], there exists some absolute constant C \u2032 so that:\n\u2225\u2225\u2225\u2225\u2225\u2225 \u2211 i:(i,j)\u2208\u2126 x\u0303iw \u22a4 U,i \u2212 m d1d2 \u2211 i x\u0303iw \u22a4 U,i \u2225\u2225\u2225\u2225\u2225\u2225 \u2264 C \u2032 \u221a m log d d1d2 ( \u221a \u00b5k d1 +max i \u2225\u2225\u2225e\u22a4i WU \u2225\u2225\u2225)\nSubstitue into Eq.(7), this gives:\n\u2225\u2225\u2225\u2225e \u22a4 j ( d1d2 m P\u2126(M)\u22a4 \u2212M\u22a4)WUW\u22a4U \u2225\u2225\u2225\u2225 \u2264 C \u2032 \u221a \u00b5kd1 log d m ( \u221a \u00b5k d1 +max i \u2225\u2225\u2225e\u22a4i WU \u2225\u2225\u2225)\nOn the other hand, we also have: \u2225\u2225\u2225e\u22a4i WU \u2225\u2225\u2225 \u2264 \u2225\u2225\u2225e\u22a4i WUS \u2225\u2225\u2225 \u2225\u2225S\u22121 \u2225\u2225 = 2\u03ba \u2225\u2225\u2225e\u22a4i UV\u22a4 \u2225\u2225\u2225 \u2264 2\u03ba( \u2225\u2225\u2225e\u22a4i (UV\u22a4 \u2212M) \u2225\u2225\u2225+ \u2225\u2225\u2225e\u22a4i M \u2225\u2225\u2225)\n\u22642\u03ba( \u221a \u00b5k\nd1 +\n\u2225\u2225\u2225e\u22a4i (UV\u22a4 \u2212M) \u2225\u2225\u2225)\nThis gives overall inequality:\nmax j\n\u2225\u2225\u2225e\u22a4j (VU\u22a4 \u2212M\u22a4) \u2225\u2225\u2225 \u2264\n\u221a \u00b5k\nd2 \u00b7 1 \u03ba + C \u2032\u2032\n\u221a \u00b5kd1 log d\nm \u03ba(\n\u221a \u00b5k\nd1 +max i\n\u2225\u2225\u2225e\u22a4i (UV\u22a4 \u2212M) \u2225\u2225\u2225)\nBy symmetry, we will also have:\nmax i\n\u2225\u2225\u2225e\u22a4i (UV\u22a4 \u2212M) \u2225\u2225\u2225 \u2264\n\u221a \u00b5k\nd1 \u00b7 1 \u03ba +C \u2032\u2032\n\u221a \u00b5kd2 log d\nm \u03ba(\n\u221a \u00b5k\nd2 +max j\n\u2225\u2225\u2225e\u22a4j (VU\u22a4 \u2212M\u22a4) \u2225\u2225\u2225)\nCombine above two equations and choose m \u2265 c0\u00b5dk\u03ba2 log d for some large enough c0. We have:\nmax j\n\u2225\u2225\u2225e\u22a4j (M\u22a4 \u2212VU\u22a4) \u2225\u2225\u2225 \u2264 2\n\u221a \u00b5k\nd2\nThis finishes the proof.\nTheorem A.6. Let U0V \u22a4 0 be the top-k SVD of d1d2 m P\u2126(M), where |\u2126| = m. then there exists universal constant c0, for any m \u2265 c0\u00b5dk\u03ba2 log d, with probability at least 1\u2212 1d10 , we have:\nmax i\n\u2225\u2225\u2225e\u22a4i U0V\u22a40 \u2225\u2225\u2225 2 \u2264 9\u00b5k\nd1 and max j\n\u2225\u2225\u2225e\u22a4j V0U\u22a40 \u2225\u2225\u2225 2 \u2264 9\u00b5k\nd2\nProof. By Theorem A.5, we know for any j \u2208 [d2]: \u2225\u2225\u2225e\u22a4j (M\u22a4 \u2212V0U\u22a40 ) \u2225\u2225\u2225 \u2264 2 \u221a \u00b5k\nd2\nTherefore, we have:\n\u2225\u2225\u2225e\u22a4j V0U\u22a40 \u2225\u2225\u2225 \u2264[ \u2225\u2225\u2225e\u22a4j M\u22a4 \u2225\u2225\u2225+ \u2225\u2225\u2225e\u22a4j (M\u22a4 \u2212V0U\u22a40 ) \u2225\u2225\u2225] \u2264 3\n\u221a \u00b5k\nd2\nBy symmetry, we also know for any i \u2208 [d1] \u2225\u2225\u2225e\u22a4i U0V\u22a40 \u2225\u2225\u2225 \u2264 3 \u221a \u00b5k\nd1\nWhich finishes the proof.\nFor the special case where M \u2208 Rd\u00d7d is symmetric and PSD, we can easily extends to have following:\nCorollary A.7. Let U0U \u22a4 0 be the top-k SVD of d2 m P\u2126(M), where |\u2126| = m. then there exists universal constant c0, for any m \u2265 c0\u00b5dk\u03ba2 log d, with probability at least 1\u2212 1d10 , we have:\nmax i\n\u2225\u2225\u2225e\u22a4i U0 \u2225\u2225\u2225 2 \u2264 10\u00b5k\u03ba\nd\nProof. By Corollary A.6, we have:\nmax i\n\u2225\u2225\u2225e\u22a4i U0U\u22a40 \u2225\u2225\u2225 2 \u2264 9\u00b5k\nd\nOn the other hand, by Theorem A.4, we have:\n\u03c3min(U \u22a4 0 U0) = \u03c3k(U0U \u22a4 0 ) \u2265 \u03c3k(M)\u2212 \u2225\u2225\u2225M\u2212U0U\u22a40 \u2225\u2225\u2225 \u2265 9\n10\u03ba\nTherefore, for any i \u2208 [d] we have:\n\u2225\u2225\u2225e\u22a4i U0 \u2225\u2225\u2225 2 \u2264\n\u2225\u2225e\u22a4i U0U\u22a40 \u2225\u22252\n\u03c3min(U\u22a40 U0) \u2264 10\u00b5k\u03ba d\nWhich finishes the proof.\nFinally, Lemma A.1 can be easily concluded from Theorem A.4 and Theorem A.6, while Lemma 4.1 is also directly proved by Theorem A.4 and Corollary A.7.\nB Proof of Symmetric PSD Case\nIn this section, we prove Theorem 3.1. WLOG, we continue to assume \u2016M\u2016 = 1 in all proof. Also, when it\u2019s clear from the context, we use \u03ba to specifically to represent \u03ba(M). Then \u03c3min(M) = 1 \u03ba . Also in this section, we always denote SVD(M) = XSX\u22a4, and SVD(UU\u22a4) = WDW\u22a4. The most essential part to prove Theorem 3.1 is proving following Theorem: Theorem B.1 (restatement of Theorem 4.5). Let f(U) = \u2225\u2225UU\u22a4 \u2212M\n\u2225\u22252 F and gi(U) = \u2225\u2225e\u22a4i U \u2225\u22252. Suppose after initialization, we have:\nf(U0) \u2264 ( 1\n20\u03ba\n)2 , max\ni gi(U0) \u2264\n10\u00b5k\u03ba2\nd\nThen, there exist some absolute constant c such that for any learning rate \u03b7 < c \u00b5dk\u03ba3 log d , with at least 1\u2212 T d10 probability, we will have for all t \u2264 T that:\nf(Ut) \u2264 (1\u2212 \u03b7 2\u03ba )t ( 1 10\u03ba )2 , max i gi(Ut) \u2264 20\u00b5k\u03ba2 d\nTheorem B.1 says once initialization algorithm provides U0 in good local region, with high probability Ut will always stay in this good region and f(Ut) is linear converging to 0. With this theorem, we can then immediately conclude Theorem 3.1 from Theorem B.1 and Lemma 4.1.\nThe rest of this section all focus on proving Theorem B.1. First, we prepare with a few lemmas about the property of objective function, and the spectral property of U in a local Frobenius ball around optimal. Then, we prove Theorem B.1 by constructing two supermartingales related to f(Ut), gi(Ut) each, and applying concentration argument.\nFor symmetric PSD case, we denote the stochastic gradient as:\nSG(U) = 2d2(UU\u22a4 \u2212M)ij(eie\u22a4j + eje\u22a4i )U\nThe update in Algorithm 1 can be now written as:\nUt+1 \u2190 Ut \u2212 \u03b7SG(Ut) (8)\nWe immediately have the property:\nESG(U) = \u2207f(U) = 4(UU\u22a4 \u2212M)U\nB.1 Geometric Properties in Local Region\nFirst, we prove two lemmas w.r.t the smoothness and property similar to strongly convex for objective function:\nLemma B.2. (restatement of Lemma 4.2) Within the region D = {U| \u2016U\u2016 \u2264 \u0393}, we have function f(U) = \u2016M\u2212UU\u22a4\u20162F satisfying for any U1,U2 \u2208 D:\n\u2016\u2207f(U1)\u2212\u2207f(U2)\u2016F \u2264 \u03b2\u2016U1 \u2212U2\u2016F\nwhere smoothness parameter \u03b2 = 16max{\u03932, \u2016M\u2016}.\nProof. Inside region D, we have:\n\u2016\u2207f(U1)\u2212\u2207f(U2)\u2016F =\u20164(U1U\u22a41 \u2212M)U1 \u2212 4(U2U\u22a42 \u2212M)U2\u2016F \u22644\u2016U1U\u22a41 U1 \u2212U2U\u22a42 U2\u2016F + 4\u2016M(U1 \u2212U2)\u2016F =4\u2016U1U\u22a41 (U1 \u2212U2) +U1(U1 \u2212U2)\u22a4U2 + (U1 \u2212U2)U\u22a42 U2\u2016F + 4\u2016M(U1 \u2212U2)\u2016F \u226412max{\u2016U1\u20162 , \u2016U2\u20162}\u2016U1 \u2212U2\u2016F + 4 \u2016M\u2016 \u2016U1 \u2212U2\u2016F \u226416max{\u03932, \u2016M\u2016}\u2016U1 \u2212U2\u2016F\nLemma B.3. (restatement of Lemma 4.3) Within the region D = {U|\u03c3min(X\u22a4U) \u2265 \u03b3}, then we have function f(U) = \u2016M\u2212UU\u22a4\u20162F satisfying:\n\u2016\u2207f(U)\u20162F \u2265 \u03b1f(U)\nwhere constant \u03b1 = 4\u03b32.\nProof. Inside region D, recall we denote WDW\u22a4 = SVD(UU\u22a4), thus we have:\n\u2016\u2207f(U)\u20162F = 16\u2016(UU\u22a4 \u2212M)U\u20162F =16[\u2016PW(UU\u22a4 \u2212M)U\u20162F + \u2016PW\u22a5(UU\u22a4 \u2212M)U\u20162F] \u226516[\u03c3min(D)\u2016PW(UU\u22a4 \u2212M)PW\u20162F + \u2016PW\u22a5MU\u20162F] \u226516[\u03c3min(D)\u2016UU\u22a4 \u2212 PWMPW\u20162F + \u2016PW\u22a5MU\u20162F]\nOn the other hand, we have:\n\u2016PW\u22a5MU\u20162F = \u2016PW\u22a5X\u03a3X\u22a4U\u20162F \u2265 \u03c32min(X\u22a4U)\u2016PW\u22a5X\u03a3\u20162F =\u03c32min(X \u22a4U)tr(PW\u22a5M2PW\u22a5) = \u03c32min(X\u22a4U)\u2016PW\u22a5M\u20162F\nand \u03c3min(D) = \u03bbmin(U\n\u22a4U) \u2265 \u03bbmin(U\u22a4PXU) = \u03c32min(X\u22a4U) Therefore, combine all above, we have:\n\u2016\u2207f(U)\u20162F \u2265 16\u03c32min(X\u22a4U)[\u2016UU\u22a4 \u2212 PWMPW\u20162F + \u2016PW\u22a5M\u20162F] \u22654\u03c32min(X\u22a4U)[\u2016UU\u22a4 \u2212 PWMPW\u20162F + \u2016PW\u22a5MPW\u20162F + \u2016PWMPW\u22a5\u20162F + \u2016PW\u22a5MPW\u22a5\u20162F] =4\u03c32min(X \u22a4U)\u2016UU\u22a4 \u2212M\u20162F\nNext, we show as long as we are in some Frobenious ball around optimum, then we have good spectral property over U which guarantees the preconditions for Lemma B.2 and Lemma B.3. Lemma B.4. (restatement of Lemma 4.4) Within the region D = {U| \u2225\u2225M\u2212UU\u22a4\n\u2225\u2225 F \u2264 110\u03c3k(M)},\nwe have: \u2016U\u2016 \u2264 \u221a 2 \u2016M\u2016, \u03c3min(X\u22a4U) \u2265 \u221a \u03c3k(M)/2\nProof. For spectral norm of U, we have:\n\u2016U\u20162 \u2264 \u2016M\u2016+ \u2225\u2225\u2225M\u2212UU\u22a4 \u2225\u2225\u2225 \u2264 \u2016M\u2016+ \u2225\u2225\u2225M\u2212UU\u22a4 \u2225\u2225\u2225 F \u2264 2 \u2016M\u2016\nFor the minimum singular value of U\u22a4U, we have:\n\u03c3min(U \u22a4U) =\u03c3k(UU \u22a4) \u2265 \u03c3k(M)\u2212 \u2225\u2225\u2225M\u2212UU\u22a4 \u2225\u2225\u2225\n\u2265\u03c3k(UU\u22a4) \u2265 \u03c3k(M)\u2212 \u2225\u2225\u2225M\u2212UU\u22a4 \u2225\u2225\u2225 F \u2265 9 10 \u03c3k(M)\nOn the other hand, we have:\n9\n10 \u03c3k(M) \u2016X\u22a5W\u20162 \u2264\u03c3min(D) \u2016X\u22a5W\u20162 \u2264\n\u2225\u2225\u2225X\u22a4\u22a5W\u03a3W\u22a4X\u22a5 \u2225\u2225\u2225\n\u2264 \u2225\u2225\u2225X\u22a4\u22a5UU\u22a4X\u22a5 \u2225\u2225\u2225 F = \u2225\u2225\u2225PX\u22a5(M\u2212UU\u22a4)PX\u22a5 \u2225\u2225\u2225 F \u2264 \u2225\u2225\u2225M\u2212UU\u22a4\n\u2225\u2225\u2225 F \u2264 1 10 \u03c3k(M)\nLet the principal angle between X and W to be \u03b8. This gives sin2 \u03b8 = \u2225\u2225X\u22a4\u22a5W \u2225\u22252 \u2264 19 . Thus cos2 \u03b8 = \u03c32min(X \u22a4W) \u2265 89 . Therefore:\n\u03c32min(X \u22a4U) \u2265 \u03c32min(X\u22a4W)\u03c3min(U\u22a4U) \u2265 \u03c3k(M)/2\nB.2 Proof of Theorem B.1\nNow, we are ready for our key theorem. By Lemma B.2, Lemma B.3, and Lemma B.4, we already know the function has good property locally in the region D = {U| \u2225\u2225M\u2212UU\u22a4 \u2225\u2225 F\n\u2264 110\u03c3k(M)} which alludes linear convergence. Then, the work remains and also the most challenging part is to prove that once we initialize inside this region, our algorithm will guarantee U never leave this region with high probability even with relatively large stepsize. The requirement for tight sample complexity and near optimal runtime makes it more challenging, and require us to further control the incoherence of Ut over all iterates in addition to the distance \u2225\u2225M\u2212UU\u22a4 \u2225\u2225 F .\nFollowing is our formal proof.\nProof of Theorem B.1. Define event Et = {\u2200\u03c4 \u2264 t, f(U\u03c4 ) \u2264 (1\u2212 \u03b72\u03ba)t( 110\u03ba )2,maxi gi(U\u03c4 ) \u2264 20\u00b5k\u03ba2 d }. Theorem B.1 is equivalent to prove event ET happens with high probability. The proof achieves this by contructing two supermartingales for f(Ut)1Et and gi(Ut)1Et (where 1(\u00b7) denote indicator function), applies concentration argument.\nThe proofs follow the structure of:\n1. The constructions of supermartingales\n2. Their probability 1 bound and variance bound in order to apply Azuma-Bernstein inequality\n3. Final combination of concentration results to conclude the proof\nFirst, let filtration Ft = \u03c3{SG(U0), \u00b7 \u00b7 \u00b7 , SG(Ut\u22121)} where \u03c3{\u00b7} denotes the sigma field. Note by definiton of Et, we have Et \u2282 Ft. Also Et+1 \u2282 Et, and thus 1Et+1 \u2264 1Et . Note Et denotes the event which up to time t, U\u03c4 always stay in a local region which both close to M and incoherent.\nBy Lemma B.4, we immediately know that conditioned on Et, we have \u2016Ut\u2016 \u2264 \u221a 2, \u03c3min(X\n\u22a4Ut) \u2265 1/ \u221a 2\u03ba and \u03c3min(U \u22a4 t Ut) \u2265 1/2\u03ba. We will use this fact throughout the proof.\nConstruction of supermartingale G: Since gi(U) = e \u22a4 i UU \u22a4ei is a quadratic function, we know for any change \u2206U, we have:\ngi(U+\u2206U) = gi(U) + 2e \u22a4 i (\u2206U)U \u22a4ei + \u2225\u2225\u2225e\u22a4i \u2206U \u2225\u2225\u2225 2\nWe know for any l \u2208 [d]:\nE\u2016e\u22a4l SG(U)\u201621Et \u2264 E16d4\u03b4il(u\u22a4i uj \u2212Mij)2 max i\n\u2225\u2225\u2225e\u22a4i U \u2225\u2225\u2225 2 1Et\n=16d2 \u2225\u2225\u2225e\u22a4l (UU\u22a4 \u2212M) \u2225\u2225\u2225 2 max\ni\n\u2225\u2225\u2225e\u22a4i U \u2225\u2225\u2225 2 1Et \u2264 O(\u00b52k2\u03ba4)1Et\nTherefore, by update Eq.(8), and ESG(U) = \u2207f(U) = 4(UU\u22a4 \u2212M)U, we know:\nE[gi(Ut+1)1Et |Ft]\n=[gi(Ut)\u2212 2\u03b7e\u22a4i [ESG(Ut)]U\u22a4t ei + \u03b72\n2 E\n\u2225\u2225\u2225e\u22a4i SG(Ut) \u2225\u2225\u2225 2 ]1Et\n=[tr(U\u22a4t eie \u22a4 i [I\u2212 8\u03b7(UtU\u22a4t \u2212M)]Ut) +\n\u03b72\n2 E\n\u2225\u2225\u2225e\u22a4i SG(Ut) \u2225\u2225\u2225 2 ]1Et\n=[tr(U\u22a4t eie \u22a4 i Ut(I\u2212 8\u03b7U\u22a4t Ut)) + 8\u03b7tr(U\u22a4t eie\u22a4i MUt) + \u03b72O(\u00b52k2\u03ba4)]1Et \u2264[(1\u2212 8\u03b7\u03c3min(U\u22a4t Ut))gi(Ut) + 8\u03b7tr(U\u22a4t eie\u22a4i MUt) + \u03b72O(\u00b52k2\u03ba4)]1Et \u2264[(1\u2212 4\u03b7\n\u03ba )gi(Ut) + 16\n\u221a 10 \u03b7\u00b5k\u03ba\nd + \u03b72O(\u00b52k2\u03ba4)]1Et\n\u2264[(1\u2212 4\u03b7 \u03ba )gi(Ut) + 60 \u03b7\u00b5k\u03ba d )]1Et\nThe last step is true by choosing constant c in learning rate \u03b7 to be small enough.\nLet Git = (1\u2212 4\u03b7\u03ba )\u2212t(gi(Ut)1Et\u22121 \u2212 15 \u00b5k\u03ba2 d ). This gives:\nEGi(t+1) \u2264 (1\u2212 4\u03b7 \u03ba )\u2212t(gi(Ut)1Et \u2212 15\n\u00b5k\u03ba2\nd ) \u2264 Git\nThat is Git is supermartingale.\nProbability 1 bound for G: We also know\nGit \u2212 E[Git|Ft\u22121] =(1\u2212 4\u03b7 \u03ba )\u2212t\n[ \u2212\u03b7e\u22a4i [SG(Ut)\u2212 ESG(Ut)]U\u22a4t ei\n+ \u03b72\n2 [\u2016e\u22a4i SG(Ut)\u20162 \u2212 E\u2016e\u22a4i SG(Ut)\u20162]\n] 1Et\u22121 (9)\nSince when sample (i, j) entry of matrix M, for any l \u2208 [d], we have:\ne\u22a4l [SG(Ut)]U \u22a4 t el \u00b7 1Et\u22121 = O(1)tr(U\u22a4ele\u22a4l SG(Ut))1Et\u22121\n=O(1)d2(UU\u22a4 \u2212M)ijtr[U\u22a4ele\u22a4l (eiu\u22a4j + eju\u22a4i )]1Et\u22121 \u2264O(1)d2\n\u2225\u2225\u2225UU\u22a4 \u2212M \u2225\u2225\u2225 \u221e max i \u2225\u2225\u2225e\u22a4i U \u2225\u2225\u2225 2 1Et\u22121 \u2264 O(\u00b52k2\u03ba4)1Et\u22121\nand\n\u2016e\u22a4l SG(Ut)\u201621Et\u22121 = O(1) \u2225\u2225\u2225e\u22a4l SG(Ut) \u2225\u2225\u2225 2 1Et\u22121\n=O(1)d4(UU\u22a4 \u2212M)2ij \u2225\u2225\u2225e\u22a4l (eiu\u22a4j + eju\u22a4i ) \u2225\u2225\u2225 2 1Et\u22121 \u2264O(1)d4 \u2225\u2225\u2225UU\u22a4 \u2212M \u2225\u2225\u2225 2\n\u221e max i\n\u2225\u2225\u2225e\u22a4i U \u2225\u2225\u2225 2 1Et\u22121 \u2264 O(\u00b53dk3\u03ba6)1Et\u22121\nTherefore, by Eq.(9), we have with probability 1:\n|Git \u2212 E[Git|Ft\u22121]| \u2264 (1\u2212 4\u03b7 \u03ba )\u2212t\u03b7O(\u00b52k2\u03ba4)1Et\u22121 (10)\nVariance bound for G: For any l \u2208 [d], we also know\nVar(e\u22a4l [SG(Ut)]U \u22a4 t el \u00b7 1Et\u22121 |Ft\u22121) \u2264 E[\u3008\u2207gl(Ut), SG(Ut)\u300921Et\u22121 |Ft\u22121]\n=O(1) 1\nd2\n\u2211\nij\nd4(UU\u22a4 \u2212M)2ijtr[U\u22a4ele\u22a4l (eiu\u22a4j + eju\u22a4i )]21Et\u22121\n\u2264O(1)d2 \u2211\nj\n(UU\u22a4 \u2212M)2ljtr[U\u22a4elu\u22a4j ]21Et\u22121\n\u2264O(1)d2 \u2225\u2225\u2225e\u22a4l (UU\u22a4 \u2212M) \u2225\u2225\u2225 2 max\ni\n\u2225\u2225\u2225e\u22a4i U \u2225\u2225\u2225 4 1Et\u22121 \u2264 O( \u00b53k3\u03ba6\nd )1Et\u22121\nand\nVar(\u2016e\u22a4l SG(U)\u201621Et\u22121 |Ft\u22121) \u2264 E[\u22072gk(SG(Ut), SG(Ut))21Et\u22121 |Ft\u22121]\n=O(1) 1\nd2\n\u2211\nij\nd8(UU\u22a4 \u2212M)4ij \u2225\u2225\u2225e\u22a4k (eiu\u22a4j + eju\u22a4i ) \u2225\u2225\u2225 4 1Et\u22121\n\u2264O(1)d6 \u2211\nj\n(UU\u22a4 \u2212M)4kj \u2016uj\u20164 1Et\u22121\n\u2264O(1)d6 \u2225\u2225\u2225UU\u22a4 \u2212M \u2225\u2225\u2225 2\n\u221e\n\u2225\u2225\u2225e\u22a4k (UU\u22a4 \u2212M) \u2225\u2225\u2225 2 max\ni\n\u2225\u2225\u2225e\u22a4i U \u2225\u2225\u2225 4 1Et\u22121 \u2264 O(\u00b55dk5\u03ba10)1Et\u22121\nTherefore, by Eq.(9), we have\nVar(Git|Ft\u22121) \u2264 (1\u2212 4\u03b7 \u03ba )\u22122t\u03b72O(\n\u00b53k3\u03ba6\nd )1Et\u22121 (11)\nBernstein\u2019s inequality for G: Let \u03c32 = \u2211t\n\u03c4=1Var(Gi\u03c4 |F\u03c4\u22121), and R satisfies, with probability 1 that |Gi\u03c4 \u2212E[Gi\u03c4 |F\u03c4\u22121]| \u2264 R, \u03c4 = 1, \u00b7 \u00b7 \u00b7 , t. Then By standard Bernstein concentration inequality, we know:\nP (Git \u2265 Gi0 + s) \u2264 exp( s2/2\n\u03c32 +Rs/3 )\nSince Gi0 = gi(U0)\u2212 15\u00b5k\u03ba 2 d , let s\u2032 = O(1)(1 \u2212 4\u03b7 \u03ba )t[ \u221a \u03c32 log d+R log d], we know\nP ( gi(Ut)1Et\u22121 \u2265 15 \u00b5k\u03ba2\nd + (1\u2212 4\u03b7 \u03ba )t(gi(U0)\u2212 15\n\u00b5k\u03ba2\nd ) + s\u2032\n) \u2264 1\n2d11\nBy Eq.(10), we know R = (1 \u2212 4\u03b7 \u03ba )\u2212t\u03b7O(\u00b52k2\u03ba4) satisfies that |Gi\u03c4 \u2212 E[Gi\u03c4 |F\u03c4\u22121]| \u2264 R, \u03c4 =\n1, \u00b7 \u00b7 \u00b7 , t. Also by Eq. (11), we have:\n(1\u2212 4\u03b7 \u03ba )t \u221a\n\u03c32 log d \u2264 \u03b7O( \u221a \u00b53k3\u03ba6 log d\nd )\n\u221a\u221a\u221a\u221a t\u2211\n\u03c4=1\n(1\u2212 4\u03b7 \u03ba )2t\u22122\u03c4 \u2264 \u221a\u03b7O(\n\u221a \u00b53k3\u03ba7 log d\nd )\nby \u03b7 < c \u00b5dk\u03ba3 log d and choosing c to be small enough, we have:\ns\u2032 = \u221a \u03b7O(\n\u221a \u00b53k3\u03ba7 log d\nd ) + \u03b7O(\u00b52k2\u03ba4 log d) \u2264 \u00b5k\u03ba\n2\nd\nSince initialization gives maxi gi(U0) \u2264 10\u00b5k\u03ba 2\nd , therefore:\nP (gi(Ut)1Et\u22121 \u2265 20 \u00b5k\u03ba2 d ) \u2264 1 2d11\nThat is equivalent to:\nP (Et\u22121 \u2229 {gi(Ut) \u2265 20 \u00b5k\u03ba2 d }) \u2264 1 2d11 (12)\nConstruction of supermartingale F : On the other hand, we also have\nE\u2016SG(Ut)\u20162F 1Et \u2264 E16d4(u\u22a4i uj \u2212Mij)2 max i\n\u2225\u2225\u2225e\u22a4i U \u2225\u2225\u2225 2 1Et\n\u226416d2\u2016UtU\u22a4t \u2212M\u20162F max i\n\u2225\u2225\u2225e\u22a4i Ut \u2225\u2225\u2225 2 1Et \u2264 O(\u00b5dk\u03ba2)f(Ut)1Et\nTherefore, by update function Eq.(8),\nE[f(Ut+1)1Et |Ft] \u2264[f(Ut)\u2212 E\u3008\u2207f(Ut), \u03b7SG(Ut)\u3009+ \u03b72E \u2016SG(Ut)\u20162F ]1Et =[f(Ut)\u2212 \u03b7 \u2016\u2207f(Ut)\u20162F + \u03b72E \u2016SG(Ut)\u2016 2 F ]1Et \u2264[(1 \u2212 2\u03b7 \u03ba )f(Ut) + \u03b7 2O(\u00b5dk\u03ba2)f(Ut)]1Et \u2264(1\u2212 \u03b7 \u03ba )f(Ut)1Et\nLet Ft = (1\u2212 \u03b7\u03ba)\u2212tf(Ut)1Et\u22121 , we know Ft is also a supermartingale.\nProbability 1 bound for F : With probabilty 1, we also have:\nFt \u2212 E[Ft|Ft\u22121] =(1\u2212 \u03b7 \u03ba )\u2212t[\u2212\u03b7\u3008\u2207f(Ut), SG(Ut)\u2212 ESG(Ut)\u3009\n+ \u03b72\n2 (\u22072f(\u03b6t)(SG(Ut), SG(Ut))\u2212 E\u22072f(\u03b6t)(SG(Ut), SG(Ut)))]1Et\u22121 (13)\nwhere \u03b6t depends on SG(Ut). First, recall we denote SVD(M) = XSX\u22a4, and SVD(UU\u22a4) = WDW\u22a4 , and observe that:\n\u2225\u2225\u2225UU\u22a4 \u2212M \u2225\u2225\u2225 \u221e 1Et\u22121 = max ij |tr(e\u22a4i (UU\u22a4 \u2212M)ej)|1Et\u22121\n=max ij\n|tr(e\u22a4i (PX + PX\u22a5)(UU\u22a4 \u2212M)ej)|1Et\u22121\n\u2264max ij |tr(e\u22a4i PX(UU\u22a4 \u2212M)ej)|1Et\u22121 +max ij |tr(e\u22a4i PX\u22a5UU\u22a4ej)|1Et\u22121\n\u2264max i\n\u2225\u2225\u2225e\u22a4i X \u2225\u2225\u2225 \u2225\u2225\u2225UU\u22a4 \u2212M \u2225\u2225\u2225 F 1Et\u22121 +max i \u2225\u2225\u2225e\u22a4i W \u2225\u2225\u2225 \u2225\u2225\u2225UU\u22a4 \u2212M \u2225\u2225\u2225 F 1Et\u22121\n\u2264O( \u221a \u00b5k\u03ba3\nd ) \u221a f(Ut)\nThen, when sample (i, j) entry of matrix M, we have:\n\u3008\u2207f(Ut), SG(Ut)\u30091Et\u22121 \u2264 O(1) \u2016\u2207f(Ut)\u2016F \u2016SG(Ut)\u2016F 1Et\u22121 \u2264O(1)d2 \u221a f(Ut)(UU \u22a4 \u2212M)ij \u2225\u2225\u2225eiu\u22a4j + eju\u22a4i \u2225\u2225\u2225 2\nF 1Et\u22121\n\u2264O(1)d2 \u221a f(Ut) \u2225\u2225\u2225UU\u22a4 \u2212M \u2225\u2225\u2225 \u221e max i \u2225\u2225\u2225e\u22a4i U \u2225\u2225\u2225 1Et\u22121 \u2264 O(\u00b5dk\u03ba2.5)f(Ut)1Et\u22121\nand\n\u22072f(\u03b6t)(SG(Ut), SG(Ut))1Et\u22121 \u2264 O(1) \u2016SG(Ut)\u20162F \u2264O(1)d4 \u2225\u2225\u2225UU\u22a4 \u2212M \u2225\u2225\u2225 2\n\u221e max i\n\u2225\u2225\u2225e\u22a4i U \u2225\u2225\u2225 2 1Et\u22121 \u2264 O(\u00b52d2k2\u03ba5)f(Ut)1Et\u22121\nTherefore, by decomposition Eq.(13), we have with probability 1:\n|Ft\u2212E[Ft|Ft\u22121]| \u2264 (1\u2212 \u03b7 \u03ba )\u2212t\u03b7O(\u00b5dk\u03ba2.5)f(Ut\u22121)1Et\u22121 \u2264 (1\u2212 \u03b7 \u03ba )\u2212t(1\u2212 \u03b7 2\u03ba )t\u03b7O(\u00b5dk\u03ba0.5)1Et\u22121 (14)\nVariance bound for F : We also know\nVar(\u3008\u2207f(Ut), SG(Ut)\u30091Et\u22121 |Ft\u22121) \u2264 E[\u3008\u2207f(Ut), SG(Ut)\u300921Et\u22121 |Ft\u22121]\n\u2264\u2016\u2207f(Ut)\u20162F E \u2016SG(Ut)\u2016 2 F 1Et\u22121 \u2264 O(1)d2 max\ni\n\u2225\u2225\u2225e\u22a4i U \u2225\u2225\u2225 2 f2(Ut\u22121)1Et\u22121\n\u2264O(\u00b5dk\u03ba2)f2(Ut\u22121)1Et\u22121\nand\nVar(\u22072f(\u03b6t)(SG(Ut), SG(Ut))1Et\u22121 |Ft\u22121) \u2264 E[\u22072f(\u03b6t)(SG(Ut), SG(Ut))21Et\u22121 |Ft\u22121]\n\u2264O(1)E \u2016SG(Ut)\u20164F = O(1)Ed8(UU\u22a4 \u2212M)4ij max i\n\u2225\u2225\u2225e\u22a4i U \u2225\u2225\u2225 4 1Et\u22121\n\u2264O(1)d6 \u2225\u2225\u2225UU\u22a4 \u2212M \u2225\u2225\u2225 2\n\u221e\n\u2225\u2225\u2225UU\u22a4 \u2212M \u2225\u2225\u2225 2\nF max i\n\u2225\u2225\u2225e\u22a4i U \u2225\u2225\u2225 4 1Et\u22121\n\u2264O(\u00b53d3k3\u03ba7)f2(Ut\u22121)1Et\u22121\nTherefore, by decomposition Eq.(13), we have:\nVar(Ft|Ft\u22121) \u2264 (1\u2212 \u03b7 \u03ba )\u22122t\u03b72O(\u00b5dk\u03ba2)f2(Ut\u22121)1Et\u22121 \u2264 (1\u2212 \u03b7 \u03ba )\u22122t(1\u2212 \u03b7 2\u03ba )2t\u03b72O( \u00b5dk \u03ba2 )1Et\u22121 (15)\nBernstein\u2019s inequality for F : Let \u03c32 = \u2211t\n\u03c4=1 Var(F\u03c4 |F\u03c4\u22121), and R satisfies, with probability 1 that |F\u03c4 \u2212E[F\u03c4 ||F\u03c4\u22121]| \u2264 R, \u03c4 = 1, \u00b7 \u00b7 \u00b7 , t. Then By standard Bernstein concentration inequality, we know:\nP (Ft \u2265 F0 + s) \u2264 exp( s2/2\n\u03c32 +Rs/3 )\nLet s\u2032 = O(1)(1 \u2212 \u03b7 \u03ba )t[ \u221a \u03c32 log d+R log d], this gives:\nP (f(Ut)1Et\u22121 \u2265 (1\u2212 \u03b7\n\u03ba )tf(U0) + s \u2032) \u2264 1 2d10\nBy Eq.(14), we know R = (1\u2212 \u03b7 \u03ba )\u2212t(1\u2212 \u03b72\u03ba)t\u03b7O(\u00b5dk\u03ba0.5) satisfies that |F\u03c4 \u2212E[F\u03c4 |F\u03c4\u22121]| \u2264 R, \u03c4 = 1, \u00b7 \u00b7 \u00b7 , t. Also by Eq. (15), we have:\n(1\u2212 \u03b7 \u03ba )t \u221a\n\u03c32 log d \u2264 \u03b7O( \u221a \u00b5dk log d\n\u03ba2 )\n\u221a\u221a\u221a\u221a t\u2211\n\u03c4=1\n(1\u2212 \u03b7 \u03ba )2t\u22122\u03c4 (1\u2212 \u03b7 2\u03ba )2\u03c4\n\u2264(1\u2212 \u03b7 2\u03ba )t\u03b7O(\n\u221a \u00b5dk log d\n\u03ba2 )\n\u221a\u221a\u221a\u221a t\u2211\n\u03c4=1\n(1\u2212 \u03b7 \u03ba )2t\u22122\u03c4 (1\u2212 \u03b7 2\u03ba )2\u03c4\u22122t \u2264 (1\u2212 \u03b7 2\u03ba )t \u221a \u03b7O(\n\u221a \u00b5dk log d\n\u03ba )\nby \u03b7 < c \u00b5dk\u03ba3 log d and choosing c to be small enough, we have:\ns\u2032 = (1\u2212 \u03b7 2\u03ba )t[ \u221a \u03b7O(\n\u221a \u00b5dk log d\n\u03ba ) + \u03b7O(\u00b5dk\u03ba0.5)] \u2264 (1\u2212 \u03b7 2\u03ba )t( 1 20\u03ba )2\nSince F0 = f(U0) \u2264 ( 120\u03ba)2, therefore:\nP (f(Ut)1Et\u22121 \u2265 (1\u2212 \u03b7\n2\u03ba )t(\n1 10\u03ba )2) \u2264 1 2d10\nThat is equivalent to:\nP (Et\u22121 \u2229 {f(Ut) \u2265 (1\u2212 \u03b7\n2\u03ba )t(\n1 10\u03ba )2}) \u2264 1 2d10 (16)\nProbability for event ET : Finally, combining the concentration result for martingaleG (Eq.(12)) and martingale F (Eq.(16)), we conclude:\nP (Et\u22121 \u2229 E\u0304t) = P [ Et\u22121 \u2229 ( \u222ai{gi(Ut) \u2265 20 \u00b5k\u03ba2\nd } \u222a {f(Ut) \u2265 (1\u2212\n\u03b7\n2\u03ba )t(\n1\n10\u03ba )2}\n)]\n\u2264 d\u2211\ni=1\nP (Et\u22121 \u2229 {gi(Ut) \u2265 20 \u00b5k\u03ba2\nd }) + P (Et\u22121 \u2229 {f(Ut) \u2265 (1\u2212\n\u03b7\n2\u03ba )t(\n1 10\u03ba )2}) \u2264 1 d10\nSince\nP (E\u0304T ) =\nT\u2211\nt=1\nP (Et\u22121 \u2229 E\u0304t) \u2264 T\nd10\nWe finishes the proof.\nC Proof of General Asymmetric Case\nIn this section, we first prove Lemma 3.2, set up the equivalence between Algorithm 2 and Algorithm 3. Then we prove the main theorem for general asymmetric matrix (Theorem 3.3). WLOG, we continue to assume \u2016M\u2016 = 1 in all proof. Also, when it\u2019s clear from the context, we use \u03ba to specifically to represent \u03ba(M). Then \u03c3min(M) = 1 \u03ba . Also in this section, we always use d = max{d1, d2} and denote SVD(M) = XSY\u22a4, and SVD(UV\u22a4) = WUDW\u22a4V .\nProof of Lemma 3.2. Let us always denote the iterates in Algorithm 2 by Ut, Vt, and denote the corresponding iterates in Algorithm 3 by U\u2032t, V \u2032 t using prime version. We use induction to prove the equivalence. Assume at time t we have UtV \u22a4 t = U \u2032 tV \u2032 t \u22a4. Recall in Algorithm 2, we renormalize Ut,Vt to U\u0303t, V\u0303t, this set up the correspondence:\nU\u0303t = U \u2032 tR \u2032 UD \u2032 U \u2212 1 2Q\u2032UD \u2032 1 2 V\u0303t = V \u2032 tR \u2032 V D \u2032 V \u2212 1 2Q\u2032V D \u2032 1 2\nDenote P\u2032U = R \u2032 UD \u2032 U \u2212 1 2Q\u2032UD \u2032 1 2 , and P\u2032V = V \u2032 tR \u2032 V D \u2032 V \u2212 1 2Q\u2032V D \u2032 1 2 . Clearly P\u2032UP \u2032 V \u22a4 = I. Then we have U\u0303t = U \u2032 tP \u2032 U , V\u0303t = P \u2032 V and thus:\nUt+1V \u22a4 t+1\n=(U\u0303t \u2212 2\u03b7d1d2(U\u0303tV\u0303\u22a4t \u2212M)ijeie\u22a4j V\u0303t)(V\u0303t \u2212 2\u03b7d1d2(U\u0303tV\u0303\u22a4t \u2212M)ijeje\u22a4i U\u0303t)\u22a4 =(U\u2032tP \u2032 U \u2212 2\u03b7d1d2(U\u2032tV\u2032t\u22a4 \u2212M)ijeie\u22a4j V\u2032tP\u2032V )(V\u2032tP\u2032V \u2212 2\u03b7d1d2(U\u2032tV\u2032t\u22a4 \u2212M)ijeje\u22a4i U\u2032tP\u2032U )\u22a4 =(U\u2032t \u2212 2\u03b7d1d2(U\u2032tV\u2032t\u22a4 \u2212M)ijeie\u22a4j V\u2032tP\u2032V P\u2032U\u22121) \u00b7 (V\u2032t \u2212 2\u03b7d1d2(U\u2032tV\u2032t\u22a4 \u2212M)ijeje\u22a4i U\u2032tP\u2032UP\u2032V \u22121)\u22a4 =U\u2032t+1V \u2032 t+1 \u22a4\nClearly with same initialization algorithm, we have U0V \u22a4 0 = U \u2032 0V \u2032 0 \u22a4, by induction, we finish the proof.\nNow we proceed to prove Theorem 3.3. Since Algorithm 2 and Algorithm 3 are equivalent, we will focus our analysis on Algorithm 2 which is more theoretical appealing. As for the symmetric PSD case, we first present the essential ingradient: Theorem C.1. Let f(U,V) = \u2225\u2225UV\u22a4 \u2212M\n\u2225\u22252 F , gi(U,V) = \u2225\u2225e\u22a4i UV\u22a4 \u2225\u22252, and hj(U,V) = \u2225\u2225\u2225e\u22a4j VU\u22a4 \u2225\u2225\u2225 2 ,\nfor i \u2208 [d1] and j \u2208 [d2]. Suppose after initialization, we have:\nf(U0,V0) \u2264 ( 1\n20\u03ba )2, max i gi(U0,V0) \u2264\n10\u00b5k\u03ba2\nd1 , max j hj(U0,V0) \u2264\n10\u00b5k\u03ba2\nd2\nThen, there exist some absolute constant c such that for any learning rate \u03b7 < c \u00b5dk\u03ba3 log d , with at least 1\u2212 T d10 probability, we will have for all t \u2264 T that:\nf(Ut,Vt) \u2264 (1\u2212 \u03b7\n2\u03ba )t(\n1\n10\u03ba )2, max i gi(Ut,Vt) \u2264\n100\u00b5k\u03ba2\nd1 , max j hj(Ut,Vt) \u2264\n100\u00b5k\u03ba2\nd2\nTheorem 3.3 can easily be concluded from Theorem C.1 and Lemma A.1. Theorem C.1 also provides similar guarantees as Theorem B.1 in symmetric case. However, due to the additional invariance between U and V, Theorem C.1 need to keep track of more complicated potential function gi(U,V) and hj(U,V) to control the incoherence, which makes the proof more involved.\nThe rest of this section all focus on proving Theorem C.1. Similar to symmetric PSD case, we also first prepare with a few lemmas about the property of objective function, and the spectral property of U,V in a local Frobenius ball around optimal. Then, we prove Theorem C.1 by constructing three supermartingales related to f(Ut,Vt), gi(Ut,Vt), hj(Ut,Vt) each, and applying concentration argument.\nTo make the notation clear, denote gradient \u2207f(U,V) \u2208 R(d1+d2)\u00d7k:\n\u2207f(U,V) = ( \u2202 \u2202U\nf(U,V) \u2202 \u2202V f(U,V)\n)\nAlso denote the stochastic gradient SG(U,V) by (if we sampled entry (i, j) of matrix M)\nSG(U,V) = 2d1d2(UV \u22a4 \u2212M)ij\n( eie \u22a4 j V\neje \u22a4 i U\n)\nESG(U,V) = \u2207f(U,V) = 2 (\n(UV\u22a4 \u2212M)V (UV\u22a4 \u2212M)\u22a4U\n)\nBy update function, we know:\n( Ut+1 Vt+1 ) = ( U\u0303t V\u0303t ) \u2212 \u03b7SG(U\u0303t, V\u0303t)\nand U\u0303tV\u0303 \u22a4 t = UtV \u22a4 t is the renormalized version of UtV \u22a4 t .\nC.1 Geometric Properties in Local Region\nSimilar to symmetric PSD case, we first prove two lemmas w.r.t the smoothness and property similar to strongly convex for objective function:\nLemma C.2. Within the region D = {(U,V)| \u2016U\u2016 \u2264 \u0393, \u2016V\u2016 \u2264 \u0393}, we have function f(U,V) = \u2016M\u2212UV\u22a4\u20162F satisfying:\n\u2016\u2207f(U1,V1)\u2212\u2207f(U2,V2)\u20162F \u2264 \u03b22(\u2016U1 \u2212U2\u20162F + \u2016V1 \u2212V2\u20162F)\nwhere smoothness parameter \u03b2 = 8max{\u03932, \u2016M\u2016}.\nProof. Inside region D, we have:\n\u2016\u2207f(U1,V1)\u2212\u2207f(U2,V2)\u20162F =\u2016 \u2202\n\u2202U f(U1,V1)\u2212\n\u2202\n\u2202U f(U2,V2)\u20162F + \u2016\n\u2202\n\u2202V f(U1,V1)\u2212\n\u2202\n\u2202V f(U2,V2)\u20162F\n=4(\u2016(U1V\u22a41 \u2212M)V1 \u2212 (U2V\u22a42 \u2212M)V2\u20162F + \u2016(U1V\u22a41 \u2212M)\u22a4U1 \u2212 (U2V\u22a42 \u2212M)\u22a4U2\u20162F) \u226464max{\u03934, \u2016M\u20162}(\u2016U1 \u2212U2\u20162F + \u2016V1 \u2212V2\u20162F)\nThe last step is by similar technics as in the proof of Lemma B.2, by expanding\nU1V \u22a4 1 V1 \u2212U2V\u22a42 V2 = (U1 \u2212U2)V\u22a41 V1 +U2(V1 \u2212V2)\u22a4V1 +U2V\u22a42 (V1 \u2212V2)\nLemma C.3. Within the region D = {(U,V)|\u03c3min(X\u22a4U) \u2265 \u03b3, \u03c3min(Y\u22a4V) \u2265 \u03b3}, then we have function f(U,V) = \u2016M\u2212UV\u22a4\u20162F satisfying:\n\u2016\u2207f(U,V)\u20162F \u2265 \u03b1f(U,V)\nwhere constant \u03b1 = 4\u03b32.\nProof. Let U\u0302, V\u0302 be the left singular vectors of U,V. Inside region D, we have:\n\u2016(UV\u22a4 \u2212M)V\u20162F =\u2016P\nU\u0302 (UV\u22a4 \u2212M)V\u20162F + \u2016PU\u0302\u22a5(UV \u22a4 \u2212M)V\u20162F \u2265\u03c3k(V)2\u2016PU\u0302(UV \u22a4 \u2212M)P V\u0302 \u20162F + \u2016PU\u0302\u22a5MV\u2016 2 F \u2265\u03c3k(V)2\u2016PU\u0302(UV \u22a4 \u2212M)P V\u0302 \u20162F + \u03c3min(Y\u22a4V)2\u2016PU\u0302\u22a5X\u03a3\u2016 2 F =\u03c3k(V) 2\u2016P U\u0302 (UV\u22a4 \u2212M)P V\u0302 \u20162F + \u03c3min(Y\u22a4V)2\u2016PU\u0302\u22a5M\u2016 2 F\nTherefore, by symmetry, we have:\n\u2016\u2207f(U,V)\u20162F =4(\u2016(UV\u22a4 \u2212M)V\u20162F + \u2016(UV\u22a4 \u2212M)\u22a4U\u20162F) \u22654\u03b32(2\u2016P\nU\u0302 (UV\u22a4 \u2212M)P V\u0302 \u20162F + \u2016PU\u0302\u22a5M\u2016 2 F + \u2016MPV\u0302\u22a5\u2016 2 F)\n\u22654\u03b32\u2016UV\u22a4 \u2212M\u20162F\nNext, we show as long as we are in some Frobenious ball around optimum, then we have good spectral property over U,V which guarantees the preconditions for Lemma C.2 and Lemma C.3. Lemma C.4. Within the region D = {(U,V)| \u2225\u2225M\u2212UV\u22a4\n\u2225\u2225 F \u2264 110\u03c3k(M)}, and for U = WUD 1 2 ,V =\nWV D 1 2 where WUDWV = SVD(UV \u22a4), we have:\n\u2016U\u2016 \u2264 \u221a 2 \u2016M\u2016, \u03c3min(X\u22a4U) \u2265 \u221a \u03c3k(M)/2 \u2016V\u2016 \u2264 \u221a 2 \u2016M\u2016, \u03c3min(Y\u22a4V) \u2265 \u221a \u03c3k(M)/2\nProof. For spectral norm of U, we have:\n\u2016U\u20162 = \u2016D\u2016 = \u2225\u2225\u2225UV\u22a4 \u2225\u2225\u2225 \u2264 \u2016M\u2016+ \u2225\u2225\u2225M\u2212UV\u22a4 \u2225\u2225\u2225 \u2264 \u2016M\u2016+ \u2225\u2225\u2225M\u2212UV\u22a4 \u2225\u2225\u2225 F \u2264 2 \u2016M\u2016\nFor the minimum singular value of U\u22a4U, we have:\n\u03c3min(U \u22a4U) =\u03c3k(D) = \u03c3k(UV \u22a4) \u2265 \u03c3k(M)\u2212 \u2225\u2225\u2225M\u2212UV\u22a4 \u2225\u2225\u2225\n\u2265\u03c3k(M)\u2212 \u2225\u2225\u2225M\u2212UU\u22a4 \u2225\u2225\u2225 F \u2265 9 10 \u03c3k(M)\nBy symmetry, the same holds for V. On the other hand, we have:\n1\n10 \u03c3k(M) \u2265 \u2225\u2225\u2225M\u2212UV\u22a4 \u2225\u2225\u2225 F \u2265 \u2225\u2225\u2225PX\u22a5(M\u2212UV\u22a4) \u2225\u2225\u2225 F = \u2225\u2225\u2225PX\u22a5UV\u22a4 \u2225\u2225\u2225 F = \u2016PX\u22a5WUD\u2016F\n\u2265\u2016PX\u22a5WUD\u2016 \u2265 9\n10 \u03c3k(M) \u2016X\u22a5WU\u2016\nLet the principal angle between X and WU to be \u03b8. This gives sin 2 \u03b8 = \u2225\u2225X\u22a4\u22a5WU \u2225\u22252 \u2264 19 . Thus cos2 \u03b8 = \u03c32min(X \u22a4WU ) \u2265 89 . Therefore:\n\u03c32min(X \u22a4U) \u2265 \u03c32min(X\u22a4WU )\u03c3min(U\u22a4U) \u2265 \u03c3k(M)/2\nC.2 Proof of Theorem C.1\nNow, we are ready for our key theorem. By Lemma C.2, Lemma C.3, and Lemma C.4, we already know the function has good property locally in the region D = {(U,V)| \u2225\u2225M\u2212UV\u22a4 \u2225\u2225 F \u2264 110\u03c3k(M)} which alludes linear convergence. Similar to the symmetric PSD case, the work remains is to prove that once we initialize inside this region, our algorithm will guarantee U,V never leave this region with high probability even with relatively large stepsize. Again, we also need to control the incoherence of Ut,Vt over all iterates additionally to achieve tight sample complexity and near optimal runtime.\nFollowing is our formal proof.\nProof of Theorem C.1. For simplicity of notation, we assume d = d1 = d2, and do not distinguish d1 and d2. However, it is easy to check our proof never use the property M is square matrix. The proof easily extends to d1 6= d2 case by replacing d in the proof with suitable d1, d2.\nDefine event Et = {\u2200\u03c4 \u2264 t, f(U\u03c4 ,V\u03c4 ) \u2264 (1\u2212 \u03b72\u03ba )t( 110\u03ba)2,maxi gi(U\u03c4 ,V\u03c4 ) \u2264 100\u00b5k\u03ba2 d ,maxj hj(U\u03c4 ,V\u03c4 ) \u2264\n100\u00b5k\u03ba2\nd }. Theorem C.1 is equivalent to prove event ET happens with high probability. The proof achieves this by contructing two supermartingales for f(Ut,Vt)1Et , gi(Ut,Vt)1Et and hi(Ut,Vt)1Et (where 1(\u00b7) denote indicator function), applies concentration argument.\nThe proofs also follow similar structure as symmetric PSD case:\n1. The constructions of supermartingales\n2. Their probability 1 bound and variance bound in order to apply Azuma-Bernstein inequality\n3. Final combination of concentration results to conclude the proof\nThen let filtration Ft = \u03c3{SG(U0,V0), \u00b7 \u00b7 \u00b7 , SG(Ut\u22121,Vt\u22121)} where \u03c3{\u00b7} denotes the sigma field. Also let event , note Et \u2282 Ft. Also Et+1 \u2282 Et, and thus 1Et+1 \u2264 1Et .\nBy Lemma C.4, we immediately know that conditioned on Et, we have \u2016Ut\u2016 \u2264 \u221a 2, \u2016Vt\u2016 \u2264 \u221a 2,\n\u03c3min(X \u22a4Ut) \u2265 1/ \u221a 2\u03ba, \u03c3min(Y \u22a4Vt) \u2265 1/ \u221a 2\u03ba. We will use this fact throughout the proof.\nFor simplicity, when it\u2019s clear from the context, we denote:\n( \u2206U \u2206V ) = \u2212\u03b7SG(U\u0303t, V\u0303t) = ( Ut+1 Vt+1 ) \u2212 ( U\u0303t V\u0303t )\nConstruction of supermartingale G: First, since potential function gl(U,V) is forth-order polynomial, we can expand:\ngl(U\u0303t+1, V\u0303t+1) = gl(Ut+1,Vt+1) = gl(U\u0303t +\u2206U, V\u0303t +\u2206V)\n= e\u22a4l (U\u0303t +\u2206U)(V\u0303t +\u2206V) \u22a4(V\u0303t +\u2206V)(U\u0303t +\u2206U) \u22a4el = gl(U\u0303t, V\u0303t) + 2e \u22a4 l \u2206UV\u0303 \u22a4 t V\u0303tU\u0303 \u22a4 t el + 2e \u22a4 l U\u0303tV\u0303 \u22a4 t \u2206VU\u0303tel +R2\n= gl(U\u0303t, V\u0303t) +R1\nWhere we denote R1 as the sum of first order terms and higher order terms (all second/third/forth order terms), and R2 as the sum of second order terms and higher order terms.\nWe now give a proposition about properties of R1 and R2 which involves a lot calculation, and postpone its proof in the end of this section.\nProposition C.5. With above notations, we have following inequalities hold true.\nE[R21Et |Ft] \u2264 \u03b72O(\u00b52k2\u03ba4)1Et |R1|1Et \u2264 \u03b7O(\u00b52k2\u03ba5)1Et w.p 1 E[R211Et |Ft] \u2264 \u03b72O( \u00b53k3\u03ba6\nd )1Et\nThen by taking conditional expectation, we have:\nE[gl(U\u0303t+1, V\u0303t+1)1Et |Ft] = E[gl(Ut+1,Vt+1)1Et |Ft] \u2264E[gl(U\u0303t, V\u0303t) + 2e\u22a4l \u2206UV\u0303\u22a4t V\u0303tU\u0303\u22a4t el + 2e\u22a4l U\u0303tV\u0303\u22a4t \u2206VU\u0303tel +R2]1Et\nThe first order term can be calculated as:\n[\u2212E2e\u22a4l \u2206UV\u0303\u22a4t V\u0303tU\u0303\u22a4t el + 2e\u22a4l U\u0303tV\u0303\u22a4t \u2206VU\u0303tel]1Et =[\u22124e\u22a4l (U\u0303tV\u0303\u22a4t \u2212M)V\u0303tV\u0303\u22a4t V\u0303tU\u0303\u22a4t el \u2212 4e\u22a4l U\u0303tU\u0303\u22a4t (U\u0303tV\u0303\u22a4t \u2212M\u22a4)V\u0303tU\u0303\u22a4t el]1Et =[\u22124e\u22a4l U\u0303tV\u0303\u22a4t V\u0303tV\u0303\u22a4t V\u0303tU\u0303\u22a4t el + 4e\u22a4l MV\u0303tV\u0303\u22a4t V\u0303tU\u0303\u22a4t el \u2212 4e\u22a4l U\u0303tU\u0303\u22a4t (U\u0303tV\u0303\u22a4t \u2212M\u22a4)V\u0303tU\u0303\u22a4t el]1Et \u2264[\u22124[\u03c3min(V\u0303\u22a4t V\u0303t) \u2225\u2225\u2225e\u22a4l U\u0303tV\u0303\u22a4t \u2225\u2225\u2225 2 + \u2225\u2225\u2225e\u22a4l U\u0303tV\u0303\u22a4t \u2225\u2225\u2225 \u2225\u2225\u2225V\u0303tV\u0303\u22a4t \u2225\u2225\u2225 \u2225\u2225\u2225e\u22a4l M \u2225\u2225\u2225\n+ \u2225\u2225\u2225e\u22a4l U\u0303tU\u0303\u22a4t \u2225\u2225\u2225 \u2225\u2225\u2225U\u0303tV\u0303\u22a4t \u2212M\u22a4 \u2225\u2225\u2225 F \u2225\u2225\u2225e\u22a4l U\u0303tV\u0303\u22a4t \u2225\u2225\u2225]]1Et\n\u2264[\u22122 \u03ba gl(U\u0303t, V\u0303t) + 80 \u00b5k\u03ba d + 4 10\u03ba gl(U\u0303t, V\u0303t)]1Et \u2264[\u22121 \u03ba gl(U\u0303t, V\u0303t) + 80 \u00b5k\u03ba d ]1Et\nIn second last inequality, we use key observation:\n\u2225\u2225\u2225e\u22a4k U\u0303tV\u0303\u22a4t \u2225\u2225\u2225 = \u2225\u2225\u2225e\u22a4k WUDW\u22a4V \u2225\u2225\u2225 = \u2225\u2225\u2225e\u22a4k WUDW\u22a4U \u2225\u2225\u2225 = \u2225\u2225\u2225e\u22a4k U\u0303tU\u0303\u22a4t \u2225\u2225\u2225\nBy Proposition C.5, we know E[R21Et |Ft] \u2264 \u03b72O(\u00b52k2\u03ba4)1Et . Combine both facts and recall \u03b7 < c \u00b5dk\u03ba3 log d , we have:\nE[gi(U\u0303t+1, V\u0303t+1)1Et |Ft] \u2264 [(1\u2212 \u03b7\n\u03ba )gi(U\u0303t, V\u0303t) +\n80\u03b7\u00b5k\u03ba\nd +O(\u03b72\u00b52k2\u03ba4)]1Et\n\u2264 [(1\u2212 \u03b7 \u03ba )gi(U\u0303t, V\u0303t) + 90\u03b7\u00b5k\u03ba d ]1Et\nThe last inequality is achieved by choosing c small enough.\nLet Git = (1\u2212 \u03b7\u03ba)\u2212t(gi(U\u0303t, V\u0303t)1Et\u22121 \u2212 90 \u00b5k\u03ba2 d ). This gives:\nE[Gi(t+1)|Ft] \u2264 (1\u2212 \u03b7 \u03ba )\u2212t(gi(U\u0303t, V\u0303t)1Et \u2212 90\n\u00b5k\u03ba2\nd ) \u2264 Git\nThe right inequality is true since 1Et \u2264 1Et\u22121 . This implies Git is supermartingale.\nProbability 1 bound for G: We also know:\nGi(t+1) \u2212 E[Gi(t+1)|Ft] =(1\u2212 \u03b7 \u03ba )\u2212(t+1)[R1 \u2212 ER1]1Et\nBy Proposition C.5, we know with probability 1 that |R1|1Et \u2264 \u03b7O(\u00b52k2\u03ba5)1Et . This gives with probability 1:\n|Git \u2212 E[Git|Ft\u22121]| \u2264 (1\u2212 \u03b7 \u03ba )\u2212t\u03b7O(\u00b52k2\u03ba5)1Et\u22121 (17)\nVariance bound for G: We also know\nVar(Gi(t+1)|Ft) = (1\u2212 \u03b7 \u03ba )\u22122(t+1)[ER211Et \u2212 (ER11Et)2] \u2264 E[R211Et |Ft]\nBy Proposition C.5, we know that E[R211Et |Ft] \u2264 \u03b72O(\u00b5 3k3\u03ba6 d )1Et . This gives:\nVar(Git|Ft\u22121) \u2264 (1\u2212 \u03b7 \u03ba )\u22122t\u03b72O(\n\u00b53k3\u03ba6\nd )1Et\u22121 (18)\nBernstein\u2019s inequality for G: Let \u03c32 = \u2211t\n\u03c4=1Var(Gi\u03c4 |F\u03c4\u22121), and R satisfies, with probability 1 that |Gi\u03c4 \u2212E[Gi\u03c4 |F\u03c4\u22121]| \u2264 R, \u03c4 = 1, \u00b7 \u00b7 \u00b7 , t. Then By standard Bernstein concentration inequality, we know:\nP (Git \u2265 Gi0 + s) \u2264 exp( s2/2\n\u03c32 +Rs/3 )\nSince Gi0 = gi(U\u03030, V\u03030)\u2212 90\u00b5k\u03ba 2 d , let s\u2032 = O(1)(1 \u2212 \u03b7 \u03ba )t[ \u221a \u03c32 log d+R log d], we know\nP ( gi(U\u0303t, V\u0303t)1Et\u22121 \u2265 90 \u00b5k\u03ba2\nd + (1\u2212 \u03b7 \u03ba )t(gi(U\u03030, V\u03030)\u2212 90\n\u00b5k\u03ba2\nd ) + s\u2032\n) \u2264 1\n3d11\nBy Eq.(17), we know R = (1 \u2212 \u03b7 \u03ba )\u2212t\u03b7O(\u00b52k2\u03ba5) satisfies that |Gi\u03c4 \u2212 E[Gi\u03c4 |F\u03c4\u22121]| \u2264 R, \u03c4 =\n1, \u00b7 \u00b7 \u00b7 , t. Also by Eq. (18), we have:\n(1\u2212 \u03b7 \u03ba )t \u221a\n\u03c32 log d \u2264 \u03b7O( \u221a \u00b53k3\u03ba6 log d\nd )\n\u221a\u221a\u221a\u221a t\u2211\n\u03c4=1\n(1\u2212 \u03b7 \u03ba )2t\u22122\u03c4 \u2264 \u221a\u03b7O(\n\u221a \u00b53k3\u03ba7 log d\nd )\nby \u03b7 < c \u00b5dk\u03ba3 log d and choosing c to be small enough, we have:\ns\u2032 = \u221a \u03b7O(\n\u221a \u00b53k3\u03ba7 log d\nd ) + \u03b7O(\u00b52k2\u03ba5 log d) \u2264 10\u00b5k\u03ba\n2\nd\nSince initialization gives maxi gi(U0,V0) \u2264 10\u00b5k\u03ba 2\nd , therefore:\nP (gi(U\u0303t, V\u0303t)1Et\u22121 \u2265 100 \u00b5k\u03ba2 d ) \u2264 1 3d11\nThat is equivalent to:\nP (Et\u22121 \u2229 {gi(U\u0303t, V\u0303t) \u2265 100 \u00b5k\u03ba2 d }) \u2264 1 3d11 (19)\nBy symmetry, we can also have corresponding result for hj(U\u0303t, V\u0303t).\nConstruction of supermartingale F: Similarly, we also need to construct a martingale for f(U\u0303t, V\u0303t). Again, we can write f as forth order polynomial:\nf(U\u0303t+1, V\u0303t+1) = f(Ut+1,Vt+1) = f(U\u0303t +\u2206U, V\u0303t +\u2206V)\n= tr ( [(U\u0303t +\u2206U)(V\u0303t +\u2206V)\u2212M][(U\u0303t +\u2206U)(V\u0303t +\u2206V)\u2212M]\u22a4 )\n= f(U\u0303t, V\u0303t) + 2tr(\u2206UV\u0303 \u22a4 t (U\u0303tV\u0303 \u22a4 t \u2212M)\u22a4) + 2tr(\u2206VU\u0303\u22a4t (U\u0303tV\u0303\u22a4t \u2212M)) +Q2\n= f(U\u0303t, V\u0303t) +Q1\nWhere we denoteQ1 as the sum of first order terms and higher order terms (all second/third/forth order terms), and Q2 as the sum of second order terms and higher order terms.\nWe also now give a proposition about properties of Q1 and Q2 which involves a lot calculation, and postpone its proof in the end of this section.\nProposition C.6. With above notations, we have following inequalities hold true.\nE[Q21Et |Ft] \u2264 \u03b72O(\u00b5dk\u03ba2)f(U\u0303t, V\u0303t)1Et |Q1|1Et \u2264 \u03b7O(\u00b5dk\u03ba3)f(U\u0303t, V\u0303t)1Et w.p 1 E[Q211Et |Ft] \u2264 \u03b72O(\u00b5dk\u03ba2)f2(U\u0303t, V\u0303t)1Et\nThen by Proposition C.6, we know E[Q21Et |Ft] \u2264 \u03b72O(\u00b5dk\u03ba2)f(U\u0303t, V\u0303t)1Et . By taking conditional expectation, we have:\nE[f(Ut+1)1Et |Ft] \u2264[f(U\u0303t, V\u0303t)\u2212 E\u3008\u2207f(U\u0303t, V\u0303t), \u03b7SG(Ut)\u3009+ EQ2]1Et =[f(U\u0303t, V\u0303t)\u2212 \u03b7 \u2225\u2225\u2225\u2207f(U\u0303t, V\u0303t) \u2225\u2225\u2225 2\nF + EQ2]1Et\n\u2264[(1\u2212 2\u03b7 \u03ba )f(U\u0303t, V\u0303t) + \u03b7 2O(\u00b5dk\u03ba2)f(U\u0303t, V\u0303t)]1Et \u2264(1\u2212 \u03b7 \u03ba )f(U\u0303t, V\u0303t)1Et\nLet Ft = (1\u2212 \u03b7\u03ba)\u2212tf(U\u0303t, V\u0303t)1Et\u22121 , we know Ft is also a supermartingale.\nProbability 1 bound: We also know\nFt+1 \u2212 E[Ft+1|Ft] = (1\u2212 \u03b7 \u03ba )\u2212(t+1)[Q1 \u2212 EQ1]1Et\nBy Proposition C.6, we know with probability 1 that |Q1|1Et \u2264 \u03b7O(\u00b5dk\u03ba3)f(Ut,Vt)1Et . This gives with probability 1:\n|Ft \u2212 EFt| \u2264 (1\u2212 \u03b7 \u03ba )\u2212t\u03b7O(\u00b5dk\u03ba3)f(Ut\u22121)1Et\u22121 \u2264 (1\u2212 \u03b7 \u03ba )\u2212t(1\u2212 \u03b7 2\u03ba )t\u03b7O(\u00b5dk\u03ba)1Et\u22121 (20)\nVariance bound: We also know\nVar(Ft+1|Ft) = (1\u2212 \u03b7 \u03ba )\u22122(t+1)[EQ211Et \u2212 (EQ11Et)2] \u2264 (1\u2212 \u03b7 \u03ba )\u22122(t+1)E[Q211Et |Ft]\nBy Proposition C.6, we know that E[Q211Et |Ft] \u2264 \u03b72O(\u00b5dk\u03ba2)f2(Ut,Vt)1Et . This gives:\nVar(Ft|Ft\u22121) \u2264 (1\u2212 \u03b7 \u03ba )\u22122t\u03b72O(\u00b5dk\u03ba2)f2(Ut\u22121)1Et\u22121 \u2264 (1\u2212 \u03b7 \u03ba )\u22122t(1\u2212 \u03b7 2\u03ba )2t\u03b72O( \u00b5dk \u03ba2 )1Et\u22121 (21)\nBernstein\u2019s inequality: Let \u03c32 = \u2211t\n\u03c4=1 Var(F\u03c4 |F\u03c4\u22121), and R satisfies, with probability 1 that |F\u03c4 \u2212 E[F\u03c4 ||F\u03c4\u22121]| \u2264 R, \u03c4 = 1, \u00b7 \u00b7 \u00b7 , t. Then By standard Bernstein concentration inequality, we know:\nP (Ft \u2265 F0 + s) \u2264 exp( s2/2\n\u03c32 +Rs/3 )\nLet s\u2032 = O(1)(1 \u2212 \u03b7 \u03ba )t[ \u221a \u03c32 log d+R log d], this gives:\nP (f(U\u0303t, V\u0303t)1Et\u22121 \u2265 (1\u2212 \u03b7\n\u03ba )tf(U0) + s \u2032) \u2264 1 3d10\nBy Eq.(20), we know R = (1 \u2212 \u03b7 \u03ba )\u2212t(1 \u2212 \u03b72\u03ba)t\u03b7O(\u00b5dk\u03ba) satisfies that |F\u03c4 \u2212 E[F\u03c4 |F\u03c4\u22121]| \u2264 R, \u03c4 = 1, \u00b7 \u00b7 \u00b7 , t. Also by Eq. (21), we have:\n(1\u2212 \u03b7 \u03ba )t \u221a\n\u03c32 log d \u2264 \u03b7O( \u221a \u00b5dk log d\n\u03ba2 )\n\u221a\u221a\u221a\u221a t\u2211\n\u03c4=1\n(1\u2212 \u03b7 \u03ba )2t\u22122\u03c4 (1\u2212 \u03b7 2\u03ba )2\u03c4\n\u2264(1\u2212 \u03b7 2\u03ba )t\u03b7O(\n\u221a \u00b5dk log d\n\u03ba2 )\n\u221a\u221a\u221a\u221a t\u2211\n\u03c4=1\n(1\u2212 \u03b7 \u03ba )2t\u22122\u03c4 (1\u2212 \u03b7 2\u03ba )2\u03c4\u22122t \u2264 (1\u2212 \u03b7 2\u03ba )t \u221a \u03b7O(\n\u221a \u00b5dk log d\n\u03ba )\nby \u03b7 < c \u00b5dk\u03ba3 log d and choosing c to be small enough, we have:\ns\u2032 = (1\u2212 \u03b7 2\u03ba )t[ \u221a \u03b7O(\n\u221a \u00b5dk\u03ba log d\n\u03ba ) + \u03b7O(\u00b5dk\u03ba)] \u2264 (1\u2212 \u03b7 2\u03ba )t( 1 20\u03ba )2\nSince F0 = f(U0) \u2264 ( 120\u03ba)2, therefore:\nP (f(U\u0303t, V\u0303t)1Et\u22121 \u2265 (1\u2212 \u03b7\n2\u03ba )t(\n1 10\u03ba )2) \u2264 1 3d10\nThat is equivalent to:\nP (Et\u22121 \u2229 {f(U\u0303t, V\u0303t) \u2265 (1\u2212 \u03b7\n2\u03ba )t(\n1 10\u03ba )2}) \u2264 1 3d10 (22)\nProbability for event ET : Finally, combining the concentration result for martingaleG (Eq.(19)) and martingale F (Eq.(22)), we conclude:\nP (Et\u22121 \u2229 E\u0304t)\n=P [ Et\u22121 \u2229 ( [\u222ai{gi(Ut,Vt) \u2265 100 \u00b5k\u03ba2\nd }]\n\u222a [\u222aj{hj(Ut,Vt) \u2265 100 \u00b5k\u03ba2\nd }] \u222a {f(Ut) \u2265 (1\u2212\n\u03b7\n2\u03ba )t(\n1\n10\u03ba )2}\n)]\n\u22642 d\u2211\ni=1\nP (Et\u22121 \u2229 {gi(Ut,Vt) \u2265 100 \u00b5k\u03ba2\nd }) + P (Et\u22121 \u2229 {f(Ut,Vt) \u2265 (1\u2212\n\u03b7\n2\u03ba )t(\n1\n10\u03ba )2})\n\u2264 1 d10\nSince\nP (E\u0304T ) =\nT\u2211\nt=1\nP (Et\u22121 \u2229 E\u0304t) \u2264 T\nd10\nWe finishes the proof.\nFinally we give proof for Proposition C.5 and Proposition C.6. The proof mostly consistsof expanding every term and careful calculations.\nProof of Proposition C.5. For simplicity of notation, we hide the term 1Et in all following equations. Reader should always think every term in this proof multiplied by 1Et . Recall that:\nSG(U,V) = 2d2(UV\u22a4 \u2212M)ij ( eie \u22a4 j V\neje \u22a4 i U\n)\n( \u2206U \u2206V ) =\u2212 \u03b7SG(U\u0303t, V\u0303t) = ( Ut+1 Vt+1 ) \u2212 ( U\u0303t V\u0303t )\nWe first prove first three inequality. Recall that:\ngl(U\u0303t+1, V\u0303t+1) = gl(Ut+1,Vt+1) = gl(U\u0303t +\u2206U, V\u0303t +\u2206V)\n= e\u22a4l (U\u0303t +\u2206U)(V\u0303t +\u2206V) \u22a4(V\u0303t +\u2206V)(U\u0303t +\u2206U) \u22a4el = gl(U\u0303t, V\u0303t) + 2e \u22a4 l \u2206UV\u0303 \u22a4 t V\u0303tU\u0303 \u22a4 t el + 2e \u22a4 l U\u0303tV\u0303 \u22a4 t \u2206VU\u0303tel +R2\n= gl(U\u0303t, V\u0303t) +R1\nBy expanding the polynomial, we can write out the first order term:\nR1 \u2212R2 =2e\u22a4l \u2206UV\u0303\u22a4t V\u0303tU\u0303\u22a4t el + 2e\u22a4l U\u0303tV\u0303\u22a4t \u2206VU\u0303tel =\u2212 4\u03b7d2(U\u0303tV\u0303\u22a4t \u2212M)ij ( \u03b4ile \u22a4 j V\u0303tV\u0303 \u22a4 t V\u0303tU\u0303 \u22a4 t el + (U\u0303tV\u0303 \u22a4 t )lj(U\u0303tU\u0303 \u22a4 t )il )\nSecond order term:\nR2 \u2212R3 =e\u22a4l \u2206UV\u0303 \u22a4 t V\u0303t\u2206 \u22a4 Uel + e \u22a4 l U\u0303t\u2206 \u22a4 V\u2206VU\u0303 \u22a4 t el + 2e \u22a4 l \u2206UV\u0303 \u22a4 t \u2206VU\u0303 \u22a4 t el + 2e \u22a4 l \u2206U\u2206 \u22a4 VV\u0303tU\u0303 \u22a4 t el =4\u03b72d4(U\u0303tV\u0303 \u22a4 t \u2212M)2ij\n\u00b7 ( \u03b4il \u2225\u2225\u2225e\u22a4j V\u0303tV\u0303\u22a4t \u2225\u2225\u2225 2 + (U\u0303tU\u0303 \u22a4 t ) 2 li + 2\u03b4il(V\u0303tV\u0303 \u22a4 t )jj(U\u0303tU\u0303 \u22a4 t )ii + 2\u03b4il(U\u0303tV\u0303 \u22a4 t ) 2 ij )\nThird order term:\nR4 \u2212R3 =2e\u22a4l \u2206UV\u0303\u22a4t \u2206V\u2206\u22a4Uel + 2e\u22a4l \u2206U\u2206\u22a4V\u2206VU\u0303\u22a4t el =\u2212 16\u03b73d6(U\u0303tV\u0303\u22a4t \u2212M)3ij\u03b4il ( (V\u0303tV\u0303t)jj(U\u0303tV\u0303t)ij + (U\u0303tV\u0303t)ij(U\u0303tU\u0303t)ii )\nFourth order term:\nR4 = e \u22a4 l \u2206U\u2206 \u22a4 V\u2206V\u2206 \u22a4 Uel = 16\u03b7 4d8(U\u0303tV\u0303 \u22a4 t \u2212M)4ij\u03b4il(U\u0303tV\u0303t)2ij\nFor the ease of proof, we denote \u03c7 = \u00b5k\u03ba 2\nd , then we know conditioned on event Et, we have:\nmaxi \u2225\u2225\u2225e\u22a4i U\u0303tV\u0303\u22a4t \u2225\u2225\u2225 2 \u2264 O(\u03c7), and maxj \u2225\u2225\u2225e\u22a4j V\u0303tU\u0303\u22a4t \u2225\u2225\u2225 2 \u2264 O(\u03c7). Some key inequality we need to use in the proof are listed here: \u2225\u2225\u2225e\u22a4l U\u0303tV\u0303\u22a4t \u2225\u2225\u2225 = \u2225\u2225\u2225e\u22a4l U\u0303tU\u0303\u22a4t \u2225\u2225\u2225 and \u2225\u2225\u2225e\u22a4l V\u0303tU\u0303\u22a4t \u2225\u2225\u2225 = \u2225\u2225\u2225e\u22a4l V\u0303tV\u0303\u22a4t \u2225\u2225\u2225 (23)\nand |(U\u0303tV\u0303t)ij | \u2264 \u2225\u2225\u2225e\u22a4i U\u0303tV\u0303\u22a4t \u2225\u2225\u2225 \u2264 O(\u221a\u03c7) (24)\nThe same also holds true for:\n|(U\u0303tU\u0303\u22a4t )ii| \u2264 O( \u221a \u03c7) and |(V\u0303tV\u0303\u22a4t )jj | \u2264 O( \u221a \u03c7) (25)\nAnother fact we frequently used is:\n1\n2\n\u2225\u2225\u2225e\u22a4i UV\u22a4 \u2225\u2225\u2225 2 \u2264 \u2225\u2225\u2225e\u22a4i U \u2225\u2225\u2225 2 \u2264 2\u03ba \u2225\u2225\u2225e\u22a4i UV\u22a4 \u2225\u2225\u2225 2\nThis gives: \u2225\u2225\u2225U\u0303tV\u0303\u22a4t \u2212M \u2225\u2225\u2225 \u221e \u2264 max k \u2225\u2225\u2225ekU\u0303t \u2225\u2225\u2225max k \u2225\u2225\u2225ekV\u0303t \u2225\u2225\u2225+max k \u2016ekX\u2016max k \u2016ekY\u2016 \u2016S\u2016 \u2264 O(\u03c7\u03ba)\nand recall we choose \u03b7 < c \u00b5dk\u03ba3 log d , where c is some universal constant, then we have:\n\u03b7d2 \u2225\u2225\u2225U\u0303tV\u0303\u22a4t \u2212M \u2225\u2225\u2225 \u221e = O(\u03b7d2\u03c7\u03ba) \u2264 O(1) (26)\nWith equation (23), (24), (25), (26), now we are ready to prove Lemma.\nFor the first inequality E[R21Et |Ft] \u2264 \u03b72O(\u00b52k2\u03ba4)1Et :\nE[R21Et |Ft] \u2264 E[|R2 \u2212R3|1Et |Ft] + E[|R3 \u2212R4|1Et |Ft] + E[|R4|1Et |Ft]\nFor each term, we can bound as:\nE[|R2 \u2212R3|1Et |Ft] \u2264 \u03b72O(d2) \u2211\nij\n(U\u0303tV\u0303 \u22a4 t \u2212M)2ij ( \u03b4ilO(\u03c7) + (U\u0303tU\u0303 \u22a4 t ) 2 li )\n\u2264\u03b72O(d2)max l\u2032\n\u2225\u2225\u2225e\u22a4l\u2032 (U\u0303tV\u0303\u22a4t \u2212M) \u2225\u2225\u2225 2 \u2211\ni\n( \u03b4ilO(\u03c7) + (U\u0303tU\u0303 \u22a4 t ) 2 li ) \u2264 \u03b72O(d2\u03c72)\nE[|R3 \u2212R4|1Et |Ft] \u2264 \u03b73O(d4) \u2211\nij\n|U\u0303tV\u0303\u22a4t \u2212M|3ij\u03b4ilO(\u03c7)\n\u2264\u03b73O(d4) \u2225\u2225\u2225U\u0303tV\u0303\u22a4t \u2212M \u2225\u2225\u2225 \u221e \u2225\u2225\u2225e\u22a4l (U\u0303tV\u0303\u22a4t \u2212M) \u2225\u2225\u2225 2 O(\u03c7) \u2264 \u03b72O(d2\u03c72)\nE[|R4|1Et |Ft] \u2264 \u03b74O(d6) \u2211\nij\n(U\u0303tV\u0303 \u22a4 t \u2212M)4ij\u03b4ilO(\u03c7)\n\u2264\u03b74O(d6) \u2225\u2225\u2225U\u0303tV\u0303\u22a4t \u2212M \u2225\u2225\u2225 2\n\u221e\n\u2225\u2225\u2225e\u22a4l (U\u0303tV\u0303\u22a4t \u2212M) \u2225\u2225\u2225 2 O(\u03c7) \u2264 \u03b72O(d2\u03c72)\nThis gives in sum that\nE[R21Et |Ft] \u2264 \u03b72O(d2\u03c72)1Et = \u03b72O(\u00b5dk\u03ba2)f2(Ut)1Et\nFor the second inequality |R1|1Et \u2264 \u03b7O(\u00b52k2\u03ba5)1Et w.p 1:\n|R1|1Et \u2264 |R1 \u2212R2|1Et + |R2 \u2212R3|1Et + |R3 \u2212R4|1Et + |R4|1Et\nFor each term, we can bound as:\n|R1 \u2212R2|1Et \u2264\u03b7O(d2) \u2225\u2225\u2225U\u0303tV\u0303\u22a4t \u2212M \u2225\u2225\u2225 \u221e O(\u03c7) \u2264 \u03b7O(d2\u03c72\u03ba) |R2 \u2212R3|1Et \u2264\u03b72O(d4) \u2225\u2225\u2225U\u0303tV\u0303\u22a4t \u2212M \u2225\u2225\u2225 2\n\u221e O(\u03c7) \u2264 \u03b7O(d2\u03c72\u03ba)\n|R3 \u2212R4|1Et \u2264\u03b73O(d6) \u2225\u2225\u2225U\u0303tV\u0303\u22a4t \u2212M \u2225\u2225\u2225 3\n\u221e O(\u03c7) \u2264 \u03b7O(d2\u03c72\u03ba)\n|R4|1Et \u2264\u03b74O(d8) \u2225\u2225\u2225U\u0303tV\u0303\u22a4t \u2212M \u2225\u2225\u2225 4\n\u221e O(\u03c7) \u2264 \u03b7O(d2\u03c72\u03ba)\nThis gives in sum that, with probability 1:\n|R1|1Et \u2264 \u03b7O(d2\u03c72\u03ba)1Et = \u03b7O(\u00b52k2\u03ba5)1Et\nFor the third inequality E[R211Et |Ft] \u2264 \u03b72O(\u00b5 3k3\u03ba6 d )1Et :\nER211Et \u2264 4 [ E(R1 \u2212R2)21Et + E(R2 \u2212R3)21Et + E(R3 \u2212R4)21Et + ER241Et ]\nFor each term, we can bound as:\nE(R1 \u2212R2)21Et \u2264 \u03b72O(d2) \u2211\nij\n(U\u0303tV\u0303 \u22a4 t \u2212M)2ij ( \u03b4ilO(\u03c7 2) + (U\u0303tV\u0303 \u22a4 t ) 2 lj(U\u0303tU\u0303 \u22a4 t ) 2 il )\n\u2264\u03b72O(d2)max l\u2032\n\u2225\u2225\u2225e\u22a4l\u2032 (U\u0303tV\u0303\u22a4t \u2212M) \u2225\u2225\u2225 2 \u2211\ni\n( \u03b4ilO(\u03c7 2) + (U\u0303tV\u0303 \u22a4 t ) 2 lj(U\u0303tU\u0303 \u22a4 t ) 2 il ) \u2264 \u03b72O(d2\u03c73)\nE(R2 \u2212R3)21Et \u2264 \u03b74O(d6) \u2211\nij\n(U\u0303tV\u0303 \u22a4 t \u2212M)4ij ( \u03b4ilO(\u03c7 2) + (U\u0303tU\u0303 \u22a4 t ) 4 li )\n\u2264\u03b74O(d6) \u2225\u2225\u2225U\u0303tV\u0303\u22a4t \u2212M \u2225\u2225\u2225 2\n\u221e max l\u2032\n\u2225\u2225\u2225e\u22a4l\u2032 (U\u0303tV\u0303\u22a4t \u2212M) \u2225\u2225\u2225 2\u2211\ni\n( \u03b4ilO(\u03c7 2) + (U\u0303tU\u0303 \u22a4 t ) 4 li ) \u2264 \u03b72O(d2\u03c73)\nE(R3 \u2212R4)21Et \u2264 \u03b76O(d10) \u2211\nij\n|U\u0303tV\u0303\u22a4t \u2212M|6ij\u03b4ilO(\u03c72)\n\u2264\u03b76O(d10) \u2225\u2225\u2225U\u0303tV\u0303\u22a4t \u2212M \u2225\u2225\u2225 4\n\u221e\n\u2225\u2225\u2225e\u22a4l (U\u0303tV\u0303\u22a4t \u2212M) \u2225\u2225\u2225 2 O(\u03c72) \u2264 \u03b72O(d2\u03c73)\nER241Et \u2264 \u03b78O(d14) \u2211\nij\n(U\u0303tV\u0303 \u22a4 t \u2212M)8ij\u03b4ilO(\u03c72)\n\u2264\u03b78O(d14) \u2225\u2225\u2225U\u0303tV\u0303\u22a4t \u2212M \u2225\u2225\u2225 6\n\u221e\n\u2225\u2225\u2225e\u22a4l (U\u0303tV\u0303\u22a4t \u2212M) \u2225\u2225\u2225 2 O(\u03c72) \u2264 \u03b72O(d2\u03c73)\nThis gives in sum that:\nER211Et \u2264 \u03b72O(d2\u03c73)1Et = \u03b72O( \u00b53k3\u03ba6\nd )1Et\nThis finishes the proof.\nProof of Proposition C.6. Similarly to the proof of Proposition C.5, we hide the term 1Et in all following equations. Reader should always think every term in this proof multiplied by 1Et . Recall that:\nf(U\u0303t+1, V\u0303t+1) = f(Ut+1,Vt+1) = f(U\u0303t +\u2206U, V\u0303t +\u2206V)\n= tr ( [(U\u0303t +\u2206U)(V\u0303t +\u2206V)\u2212M][(U\u0303t +\u2206U)(V\u0303t +\u2206V)\u2212M]\u22a4 )\n= f(U\u0303t, V\u0303t) + 2tr(\u2206UV\u0303 \u22a4 t (U\u0303tV\u0303 \u22a4 t \u2212M)\u22a4) + 2tr(\u2206VU\u0303\u22a4t (U\u0303tV\u0303\u22a4t \u2212M)) +Q2\n= f(U\u0303t, V\u0303t) +Q1\nBy expanding the polynomial, we can write out the first order term:\nQ1 \u2212Q2 =2tr(\u2206UV\u0303\u22a4t (U\u0303tV\u0303\u22a4t \u2212M)\u22a4) + 2tr(\u2206VU\u0303\u22a4t (U\u0303tV\u0303\u22a4t \u2212M))\n=\u2212 4\u03b7d2(UV\u22a4 \u2212M)ij ( e\u22a4j V\u0303tV\u0303 \u22a4 t (U\u0303tV\u0303 \u22a4 t \u2212M)\u22a4ei + e\u22a4i U\u0303tU\u0303\u22a4t (U\u0303tV\u0303\u22a4t \u2212M)ej )\nThe second order term:\nQ2 \u2212Q3 =tr(\u2206UV\u0303 \u22a4 t V\u0303t\u2206 \u22a4 U) + tr(U\u0303t\u2206 \u22a4 V\u2206VU\u0303 \u22a4 t ) + 2tr(\u2206UV\u0303 \u22a4 t \u2206VU\u0303 \u22a4 t ) + 2tr(\u2206U\u2206 \u22a4 V (U\u0303tV\u0303 \u22a4 t \u2212M)\u22a4) =4\u03b72d4(UV\u22a4 \u2212M)2ij\n\u00b7 (\u2225\u2225\u2225e\u22a4j V\u0303tV\u0303\u22a4t \u2225\u2225\u2225 2 + \u2225\u2225\u2225e\u22a4i U\u0303tU\u0303\u22a4t \u2225\u2225\u2225 2 + (V\u0303tV\u0303 \u22a4 t )jj(U\u0303tU\u0303 \u22a4 t )ii + (U\u0303tV\u0303 \u22a4 t )ij(U\u0303tV\u0303 \u22a4 t \u2212M)ij )\nThe third order term:\nQ3 \u2212Q4 =2tr(\u2206UV\u0303\u22a4t \u2206V\u2206\u22a4U) + 2tr(\u2206U\u2206\u22a4V\u2206VU\u0303\u22a4t )\n=\u2212 16\u03b73d6(UV\u22a4 \u2212M)3ij ( (V\u0303tV\u0303t)jj(U\u0303tV\u0303t)ij + (U\u0303tV\u0303t)ij(U\u0303tU\u0303t)ii )\nThe forth order term:\nQ4 = tr(\u2206U\u2206 \u22a4 V\u2206V\u2206 \u22a4 U) = 16\u03b7 4d8(UV\u22a4 \u2212M)4ij(U\u0303tV\u0303t)2ij\nAgain, in addition to equation (23), (23), (24), (25), we also need following inequality:\n\u2225\u2225\u2225U\u0303tV\u0303\u22a4t \u2212M \u2225\u2225\u2225 \u221e 1Et = max ij |tr(e\u22a4i (U\u0303tV\u0303\u22a4t \u2212M)ej)|1Et\n=max ij\n|tr(e\u22a4i (PX + PX\u22a5)(U\u0303tV\u0303\u22a4t \u2212M)ej)|1Et\n\u2264max ij |tr(e\u22a4i PX(U\u0303tV\u0303\u22a4t \u2212M)ej)|1Et +max ij |tr(e\u22a4i PX\u22a5U\u0303tV\u0303\u22a4t ej)|1Et\n\u2264max i\n\u2225\u2225\u2225e\u22a4i X \u2225\u2225\u2225 \u2225\u2225\u2225U\u0303tV\u0303\u22a4t \u2212M \u2225\u2225\u2225 F 1Et +max j \u2225\u2225\u2225e\u22a4j WV \u2225\u2225\u2225 \u2225\u2225\u2225U\u0303tV\u0303\u22a4t \u2212M \u2225\u2225\u2225 F 1Et\n\u2264O(\u03ba\u221a\u03c7) \u221a\nf(Ut) (27)\nNow we are ready to prove Lemma.\nFor the first inequality E[Q21Et |Ft] \u2264 \u03b72O(\u00b5dk\u03ba2)f(U\u0303t, V\u0303t)1Et:\nE[Q21Et |Ft] \u2264 E[|Q2 \u2212Q3|1Et |Ft] + E[|Q3 \u2212Q4|1Et |Ft] + E[|Q4|1Et |Ft]\nFor each term, we can bound as:\nE[|Q2 \u2212Q3|1Et |Ft] \u2264\u03b72O(d2) \u2211\nij\n(U\u0303tV\u0303 \u22a4 t \u2212M)2ijO(\u03c7) = \u03b72O(d2\u03c7)f(U\u0303t, V\u0303t)\nE[|Q3 \u2212Q4|1Et |Ft] \u2264\u03b73O(d4) \u2211\nij\n|U\u0303tV\u0303\u22a4t \u2212M|3ijO(\u03c7)\n\u2264\u03b73O(d4\u03c7) \u2225\u2225\u2225U\u0303tV\u0303\u22a4t \u2212M \u2225\u2225\u2225 \u221e f(U\u0303t, V\u0303t) \u2264 \u03b72O(d2\u03c7)f(U\u0303t, V\u0303t)\nE[|Q4|1Et |Ft] \u2264\u03b74O(d6) \u2211\nij\n(U\u0303tV\u0303 \u22a4 t \u2212M)4ijO(\u03c7)\n\u2264\u03b74O(d6\u03c7) \u2225\u2225\u2225U\u0303tV\u0303\u22a4t \u2212M \u2225\u2225\u2225 2\n\u221e f(U\u0303t, V\u0303t) \u2264 \u03b72O(d2\u03c7)f(U\u0303t, V\u0303t)\nThis gives in sum that\nE[Q21Et |Ft] \u2264 \u03b72O(d2\u03c7)f(U\u0303t, V\u0303t)1Et = \u03b72O(\u00b5dk\u03ba2)f(U\u0303t, V\u0303t)1Et\nFor the second inequality |Q1|1Et \u2264 \u03b7O(\u00b5dk\u03ba3)f(U\u0303t, V\u0303t)1Et w.p 1:\n|Q1|1Et \u2264 |Q1 \u2212Q2|1Et + |Q2 \u2212Q3|1Et + |Q3 \u2212Q4|1Et + |Q4|1Et\nFor each term, we can bound as:\n|Q1 \u2212Q2|1Et \u2264\u03b7O(d2) \u2225\u2225\u2225U\u0303tV\u0303\u22a4t \u2212M \u2225\u2225\u2225 \u221e \u2225\u2225\u2225U\u0303tV\u0303\u22a4t \u2212M \u2225\u2225\u2225 F O( \u221a \u03c7) \u2264 \u03b7O(d2\u03c7\u03ba)f(U\u0303t, V\u0303t) |Q2 \u2212Q3|1Et \u2264\u03b72O(d4) \u2225\u2225\u2225U\u0303tV\u0303\u22a4t \u2212M \u2225\u2225\u2225 2\n\u221e O(\u03c7) \u2264 \u03b72O(d4\u03c72\u03ba2)f(U\u0303t, V\u0303t) = \u03b7O(d2\u03c7\u03ba)f(U\u0303t, V\u0303t)\n|Q3 \u2212Q4|1Et \u2264\u03b73O(d6) \u2225\u2225\u2225U\u0303tV\u0303\u22a4t \u2212M \u2225\u2225\u2225 3\n\u221e O(\u03c7) \u2264 \u03b7O(d2\u03c7\u03ba)f(U\u0303t, V\u0303t)\n|Q4|1Et \u2264\u03b74O(d8) \u2225\u2225\u2225U\u0303tV\u0303\u22a4t \u2212M \u2225\u2225\u2225 4\n\u221e O(\u03c7) \u2264 \u03b7O(d2\u03c7\u03ba)f(U\u0303t, V\u0303t)\nThis gives in sum that, with probability 1:\n|Q1|1Et \u2264 \u03b7O(d2\u03c7\u03ba)f(U\u0303t, V\u0303t)1Et = \u03b7O(\u00b5dk\u03ba3)f(U\u0303t, V\u0303t)1Et\nFor the third inequality E[Q211Et |Ft] \u2264 \u03b72O(\u00b5dk\u03ba2)f2(U\u0303t, V\u0303t)1Et :\nEQ211Et \u2264 4 [ E(Q1 \u2212Q2)21Et + E(Q2 \u2212Q3)21Et + E(Q3 \u2212Q4)21Et + EQ241Et ]\nFor each term, we can bound as:\nE(Q1 \u2212Q2)21Et \u2264\u03b72O(d2) \u2211\nij\n(U\u0303tV\u0303 \u22a4 t \u2212M)2ij \u2225\u2225\u2225U\u0303tV\u0303\u22a4t \u2212M \u2225\u2225\u2225 2\nF O(\u03c7) \u2264 \u03b72O(d2\u03c7)f2(U\u0303t, V\u0303t)\nE(Q2 \u2212Q3)21Et \u2264\u03b74O(d6) \u2211\nij\n(U\u0303tV\u0303 \u22a4 t \u2212M)4ijO(\u03c72)\n\u2264\u03b74O(d6\u03c72) \u2225\u2225\u2225U\u0303tV\u0303\u22a4t \u2212M \u2225\u2225\u2225 2\n\u221e\n\u2225\u2225\u2225U\u0303tV\u0303\u22a4t \u2212M \u2225\u2225\u2225 2\nF \u2264 \u03b74O(d6\u03c73\u03ba2)f2(U\u0303t, V\u0303t)\n\u2264\u03b72O(d2\u03c7)f2(U\u0303t, V\u0303t) E(Q3 \u2212Q4)21Et \u2264\u03b76O(d10) \u2211\nij\n|U\u0303tV\u0303\u22a4t \u2212M|6ijO(\u03c72)\n\u2264\u03b76O(d10\u03c72) \u2225\u2225\u2225U\u0303tV\u0303\u22a4t \u2212M \u2225\u2225\u2225 4\n\u221e\n\u2225\u2225\u2225U\u0303tV\u0303\u22a4t \u2212M \u2225\u2225\u2225 2\nF \u2264 \u03b72O(d2\u03c7)f2(U\u0303t, V\u0303t)\nEQ241Et \u2264\u03b78O(d14) \u2211\nij\n(U\u0303tV\u0303 \u22a4 t \u2212M)8ij\u03b4ilO(\u03c72)\n\u2264\u03b78O(d14\u03c72) \u2225\u2225\u2225U\u0303tV\u0303\u22a4t \u2212M \u2225\u2225\u2225 6\n\u221e\n\u2225\u2225\u2225U\u0303tV\u0303\u22a4t \u2212M \u2225\u2225\u2225 2\nF \u2264 \u03b72O(d2\u03c7)f2(U\u0303t, V\u0303t)\nThis gives in sum that:\nEQ211Et \u2264 \u03b72O(d2\u03c7)f2(U\u0303t, V\u0303t)1Et = \u03b72O(\u00b5dk\u03ba2)f2(U\u0303t, V\u0303t)1Et\nThis finishes the proof."}], "references": [{"title": "Simple, efficient, and neural algorithms for sparse coding", "author": ["Sanjeev Arora", "Rong Ge", "Tengyu Ma", "Ankur Moitra"], "venue": "arXiv preprint arXiv:1503.00778,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2015}, {"title": "Fast online svd revisions for lightweight recommender systems", "author": ["Matthew Brand"], "venue": "In SDM,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2003}, {"title": "Phase retrieval via matrix completion", "author": ["Emmanuel J Candes", "Yonina C Eldar", "Thomas Strohmer", "Vladislav Voroninski"], "venue": "SIAM Review,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2015}, {"title": "Exact matrix completion via convex optimization", "author": ["Emmanuel J. Cand\u00e8s", "Benjamin Recht"], "venue": "Foundations of Computational Mathematics,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2009}, {"title": "The youtube video recommendation system", "author": ["James Davidson", "Benjamin Liebald", "Junning Liu", "Palash Nandy", "Taylor Van Vleet", "Ullas Gargi", "Sujoy Gupta", "Yu He", "Mike Lambert", "Blake Livingston"], "venue": "In Proceedings of the fourth ACM conference on Recommender systems,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2010}, {"title": "Global convergence of stochastic gradient descent for some non-convex matrix problems", "author": ["Christopher De Sa", "Kunle Olukotun", "Christopher R\u00e9"], "venue": "arXiv preprint arXiv:1411.1134,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2014}, {"title": "Escaping from saddle points\u2014online stochastic gradient for tensor decomposition", "author": ["Rong Ge", "Furong Huang", "Chi Jin", "Yang Yuan"], "venue": "arXiv preprint arXiv:1503.02101,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2015}, {"title": "Matrix completion has no spurious local minimum", "author": ["Rong Ge", "Jason D. Lee", "Tengyu Ma"], "venue": "arXiv preprint arXiv:1605.07272,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2016}, {"title": "Understanding alternating minimization for matrix completion", "author": ["Marcus Hardt"], "venue": "In Foundations of Computer Science (FOCS),", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2014}, {"title": "Computational limits for matrix completion", "author": ["Moritz Hardt", "Raghu Meka", "Prasad Raghavendra", "Benjamin Weitz"], "venue": "In COLT,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2014}, {"title": "Computing matrix squareroot via non convex local search", "author": ["Prateek Jain", "Chi Jin", "Sham M Kakade", "Praneeth Netrapalli"], "venue": "arXiv preprint arXiv:1507.05854,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2015}, {"title": "Matching matrix bernstein with little memory: Near-optimal finite sample guarantees for oja\u2019s algorithm", "author": ["Prateek Jain", "Chi Jin", "Sham M Kakade", "Praneeth Netrapalli", "Aaron Sidford"], "venue": "arXiv preprint arXiv:1602.06929,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2016}, {"title": "Fast exact matrix completion with finite samples", "author": ["Prateek Jain", "Praneeth Netrapalli"], "venue": "arXiv preprint arXiv:1411.1087,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2014}, {"title": "Robust video denoising using low rank matrix completion", "author": ["Hui Ji", "Chaoqiang Liu", "Zuowei Shen", "Yuhong Xu"], "venue": null, "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2010}, {"title": "Efficient algorithms for collaborative filtering", "author": ["Raghunandan Hulikal Keshavan"], "venue": "PhD thesis, STANFORD UNIVERSITY,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2012}, {"title": "The BellKor solution to the Netflix grand prize", "author": ["Yehuda Koren"], "venue": null, "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2009}, {"title": "Low-rank matrix and tensor completion via adaptive sampling", "author": ["Akshay Krishnamurthy", "Aarti Singh"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2013}, {"title": "Gradient descent converges to minimizers", "author": ["Jason D Lee", "Max Simchowitz", "Michael I Jordan", "Benjamin Recht"], "venue": "University of California, Berkeley,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2016}, {"title": "Amazon.com recommendations: item-to-item collaborative filtering", "author": ["G. Linden", "B. Smith", "J. York"], "venue": "IEEE Internet Computing,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2003}, {"title": "Incremental collaborative filtering recommender based on regularized matrix factorization", "author": ["Xin Luo", "Yunni Xia", "Qingsheng Zhu"], "venue": "Knowledge-Based Systems,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2012}, {"title": "Online learning for matrix factorization and sparse coding", "author": ["Julien Mairal", "Francis Bach", "Jean Ponce", "Guillermo Sapiro"], "venue": "The Journal of Machine Learning Research,", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2010}, {"title": "Gradient descent converges to minimizers: The case of non-isolated critical points", "author": ["Ioannis Panageas", "Georgios Piliouras"], "venue": "arXiv preprint arXiv:1605.00405,", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2016}, {"title": "A simple approach to matrix completion", "author": ["Benjamin Recht"], "venue": null, "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2009}, {"title": "Parallel stochastic gradient algorithms for large-scale matrix completion", "author": ["Benjamin Recht", "Christopher R\u00e9"], "venue": "Mathematical Programming Computation,", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2013}, {"title": "Guaranteed matrix completion via nonconvex factorization", "author": ["Ruoyu Sun", "Zhi-Quan Luo"], "venue": "In Foundations of Computer Science (FOCS),", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2015}, {"title": "User-friendly tail bounds for sums of random matrices", "author": ["Joel A Tropp"], "venue": "Foundations of computational mathematics,", "citeRegEx": "26", "shortCiteRegEx": "26", "year": 2012}], "referenceMentions": [{"referenceID": 13, "context": "This problem arises in several applications such as video denoising [14], phase retrieval [3] and most famously in movie recommendation engines [16].", "startOffset": 68, "endOffset": 72}, {"referenceID": 2, "context": "This problem arises in several applications such as video denoising [14], phase retrieval [3] and most famously in movie recommendation engines [16].", "startOffset": 90, "endOffset": 93}, {"referenceID": 15, "context": "This problem arises in several applications such as video denoising [14], phase retrieval [3] and most famously in movie recommendation engines [16].", "startOffset": 144, "endOffset": 148}, {"referenceID": 3, "context": "The seminal works of Cand\u00e8s and Recht [4] first identified regularity conditions under which low rank matrix completion can be solved in polynomial time using convex relaxation \u2013 low rank matrix UC Berkeley.", "startOffset": 38, "endOffset": 41}, {"referenceID": 9, "context": "completion could be ill-posed and NP-hard in general without such regularity assumptions [10].", "startOffset": 89, "endOffset": 93}, {"referenceID": 4, "context": "However in several applications [5, 19], we encounter the online setting where observations are only revealed sequentially and at each step the recovery algorithm is required to maintain an estimate of the low rank matrix based on the observations so far.", "startOffset": 32, "endOffset": 39}, {"referenceID": 18, "context": "However in several applications [5, 19], we encounter the online setting where observations are only revealed sequentially and at each step the recovery algorithm is required to maintain an estimate of the low rank matrix based on the observations so far.", "startOffset": 32, "endOffset": 39}, {"referenceID": 1, "context": "On the other hand, in order to deal with the online matrix completion scenario in practical applications, several heuristics (with no convergence guarantees) have been proposed in literature [2, 20].", "startOffset": 191, "endOffset": 198}, {"referenceID": 19, "context": "On the other hand, in order to deal with the online matrix completion scenario in practical applications, several heuristics (with no convergence guarantees) have been proposed in literature [2, 20].", "startOffset": 191, "endOffset": 198}, {"referenceID": 23, "context": "Moreover, SGD, in the context of matrix completion, is also useful for parallelization and distributed implementation [24].", "startOffset": 118, "endOffset": 122}, {"referenceID": 24, "context": "While [25] shows that SGD updates stay away from saddle surfaces, the stepsizes they can handle are quite small (scaling as 1/poly(d1, d2)), leading to suboptimal computational complexity.", "startOffset": 6, "endOffset": 10}, {"referenceID": 22, "context": "The nuclear norm relaxation algorithm [23] has near-optimal sample complexity for this problem but is computationally expensive.", "startOffset": 38, "endOffset": 42}, {"referenceID": 14, "context": "Motivated by the empirical success of non-convex heuristics, a long line of works, [15, 9, 13, 25] and so on, has obtained convergence guarantees for alternating minimization, gradient descent, projected gradient descent etc.", "startOffset": 83, "endOffset": 98}, {"referenceID": 8, "context": "Motivated by the empirical success of non-convex heuristics, a long line of works, [15, 9, 13, 25] and so on, has obtained convergence guarantees for alternating minimization, gradient descent, projected gradient descent etc.", "startOffset": 83, "endOffset": 98}, {"referenceID": 12, "context": "Motivated by the empirical success of non-convex heuristics, a long line of works, [15, 9, 13, 25] and so on, has obtained convergence guarantees for alternating minimization, gradient descent, projected gradient descent etc.", "startOffset": 83, "endOffset": 98}, {"referenceID": 24, "context": "Motivated by the empirical success of non-convex heuristics, a long line of works, [15, 9, 13, 25] and so on, has obtained convergence guarantees for alternating minimization, gradient descent, projected gradient descent etc.", "startOffset": 83, "endOffset": 98}, {"referenceID": 14, "context": "Our sample complexity is better than that of [15] and is incomparable to those of [9, 13].", "startOffset": 45, "endOffset": 49}, {"referenceID": 8, "context": "Our sample complexity is better than that of [15] and is incomparable to those of [9, 13].", "startOffset": 82, "endOffset": 89}, {"referenceID": 12, "context": "Our sample complexity is better than that of [15] and is incomparable to those of [9, 13].", "startOffset": 82, "endOffset": 89}, {"referenceID": 24, "context": "To the best of our knowledge, the only provable online algorithm for this problem is that of Sun and Luo [25].", "startOffset": 105, "endOffset": 109}, {"referenceID": 16, "context": ", [17, 27].", "startOffset": 2, "endOffset": 10}, {"referenceID": 22, "context": "Nuclear Norm [23] \u00d5(\u03bcdk) \u00d5(d3/ \u221a \u01eb) No Alternating minimization [15] \u00d5(\u03bcdk\u03ba8 log 1 \u01eb ) \u00d5(\u03bcdk2\u03ba8 log 1 \u01eb ) No", "startOffset": 13, "endOffset": 17}, {"referenceID": 14, "context": "Nuclear Norm [23] \u00d5(\u03bcdk) \u00d5(d3/ \u221a \u01eb) No Alternating minimization [15] \u00d5(\u03bcdk\u03ba8 log 1 \u01eb ) \u00d5(\u03bcdk2\u03ba8 log 1 \u01eb ) No", "startOffset": 64, "endOffset": 68}, {"referenceID": 8, "context": "Alternating minimization [9] \u00d5 ( \u03bcdk2\u03ba2 ( k + log 1 \u01eb )) \u00d5 ( \u03bcdk3\u03ba2 ( k + log 1 \u01eb )) No", "startOffset": 25, "endOffset": 28}, {"referenceID": 12, "context": "Projected gradient descent[13] \u00d5(\u03bcdk) \u00d5(\u03bcdk log 1 \u01eb ) No", "startOffset": 26, "endOffset": 30}, {"referenceID": 24, "context": "SGD [25] \u00d5(\u03bcdk\u03ba) poly(\u03bc, d, k, \u03ba) log 1 \u01eb Yes SGD [8]1 d \u00b7 poly(\u03bc, k, \u03ba) poly(\u03bc, d, k, \u03ba, 1 \u01eb ) Yes Our result \u00d5 ( \u03bcdk\u03ba4 ( k + log 1 \u01eb )) \u00d5 ( \u03bcdk4\u03ba4 log 1 \u01eb ) Yes", "startOffset": 4, "endOffset": 8}, {"referenceID": 7, "context": "SGD [25] \u00d5(\u03bcdk\u03ba) poly(\u03bc, d, k, \u03ba) log 1 \u01eb Yes SGD [8]1 d \u00b7 poly(\u03bc, k, \u03ba) poly(\u03bc, d, k, \u03ba, 1 \u01eb ) Yes Our result \u00d5 ( \u03bcdk\u03ba4 ( k + log 1 \u01eb )) \u00d5 ( \u03bcdk4\u03ba4 log 1 \u01eb ) Yes", "startOffset": 50, "endOffset": 53}, {"referenceID": 5, "context": "eigenvector computation [6, 12], sparse coding [21, 1] etc.", "startOffset": 24, "endOffset": 31}, {"referenceID": 11, "context": "eigenvector computation [6, 12], sparse coding [21, 1] etc.", "startOffset": 24, "endOffset": 31}, {"referenceID": 20, "context": "eigenvector computation [6, 12], sparse coding [21, 1] etc.", "startOffset": 47, "endOffset": 54}, {"referenceID": 0, "context": "eigenvector computation [6, 12], sparse coding [21, 1] etc.", "startOffset": 47, "endOffset": 54}, {"referenceID": 6, "context": "For general non-convex optimization, an interesting line of recent work is that of [7], which proves gradient descent with noise can also escape saddle point, but they only provide polynomial rate without explicit dependence.", "startOffset": 83, "endOffset": 86}, {"referenceID": 17, "context": "Later [18, 22] show that without noise, the space of points from where gradient descent converges to a saddle point is a measure zero set.", "startOffset": 6, "endOffset": 14}, {"referenceID": 21, "context": "Later [18, 22] show that without noise, the space of points from where gradient descent converges to a saddle point is a measure zero set.", "startOffset": 6, "endOffset": 14}, {"referenceID": 10, "context": "Another related piece of work to ours is [11], proves global convergence along with rates of convergence, for the special case of computing matrix squareroot.", "startOffset": 41, "endOffset": 45}, {"referenceID": 7, "context": "During the preparation of this draft, the recent work [8] was announced which proves the global convergence of SGD for matrix completion and can also be applied to the online setting.", "startOffset": 54, "endOffset": 57}, {"referenceID": 9, "context": "This task is ill-posed and NP-hard in general [10].", "startOffset": 46, "endOffset": 50}, {"referenceID": 3, "context": "2 (\u03bc-incoherence[4, 23]).", "startOffset": 16, "endOffset": 23}, {"referenceID": 22, "context": "2 (\u03bc-incoherence[4, 23]).", "startOffset": 16, "endOffset": 23}, {"referenceID": 0, "context": "[1] Sanjeev Arora, Rong Ge, Tengyu Ma, and Ankur Moitra.", "startOffset": 0, "endOffset": 3}, {"referenceID": 1, "context": "[2] Matthew Brand.", "startOffset": 0, "endOffset": 3}, {"referenceID": 2, "context": "[3] Emmanuel J Candes, Yonina C Eldar, Thomas Strohmer, and Vladislav Voroninski.", "startOffset": 0, "endOffset": 3}, {"referenceID": 3, "context": "[4] Emmanuel J.", "startOffset": 0, "endOffset": 3}, {"referenceID": 4, "context": "[5] James Davidson, Benjamin Liebald, Junning Liu, Palash Nandy, Taylor Van Vleet, Ullas Gargi, Sujoy Gupta, Yu He, Mike Lambert, Blake Livingston, et al.", "startOffset": 0, "endOffset": 3}, {"referenceID": 5, "context": "[6] Christopher De Sa, Kunle Olukotun, and Christopher R\u00e9.", "startOffset": 0, "endOffset": 3}, {"referenceID": 6, "context": "[7] Rong Ge, Furong Huang, Chi Jin, and Yang Yuan.", "startOffset": 0, "endOffset": 3}, {"referenceID": 7, "context": "[8] Rong Ge, Jason D.", "startOffset": 0, "endOffset": 3}, {"referenceID": 8, "context": "[9] Marcus Hardt.", "startOffset": 0, "endOffset": 3}, {"referenceID": 9, "context": "[10] Moritz Hardt, Raghu Meka, Prasad Raghavendra, and Benjamin Weitz.", "startOffset": 0, "endOffset": 4}, {"referenceID": 10, "context": "[11] Prateek Jain, Chi Jin, Sham M Kakade, and Praneeth Netrapalli.", "startOffset": 0, "endOffset": 4}, {"referenceID": 11, "context": "[12] Prateek Jain, Chi Jin, Sham M Kakade, Praneeth Netrapalli, and Aaron Sidford.", "startOffset": 0, "endOffset": 4}, {"referenceID": 12, "context": "[13] Prateek Jain and Praneeth Netrapalli.", "startOffset": 0, "endOffset": 4}, {"referenceID": 13, "context": "[14] Hui Ji, Chaoqiang Liu, Zuowei Shen, and Yuhong Xu.", "startOffset": 0, "endOffset": 4}, {"referenceID": 14, "context": "[15] Raghunandan Hulikal Keshavan.", "startOffset": 0, "endOffset": 4}, {"referenceID": 15, "context": "[16] Yehuda Koren.", "startOffset": 0, "endOffset": 4}, {"referenceID": 16, "context": "[17] Akshay Krishnamurthy and Aarti Singh.", "startOffset": 0, "endOffset": 4}, {"referenceID": 17, "context": "[18] Jason D Lee, Max Simchowitz, Michael I Jordan, and Benjamin Recht.", "startOffset": 0, "endOffset": 4}, {"referenceID": 18, "context": "[19] G.", "startOffset": 0, "endOffset": 4}, {"referenceID": 19, "context": "[20] Xin Luo, Yunni Xia, and Qingsheng Zhu.", "startOffset": 0, "endOffset": 4}, {"referenceID": 20, "context": "[21] Julien Mairal, Francis Bach, Jean Ponce, and Guillermo Sapiro.", "startOffset": 0, "endOffset": 4}, {"referenceID": 21, "context": "[22] Ioannis Panageas and Georgios Piliouras.", "startOffset": 0, "endOffset": 4}, {"referenceID": 22, "context": "[23] Benjamin Recht.", "startOffset": 0, "endOffset": 4}, {"referenceID": 23, "context": "[24] Benjamin Recht and Christopher R\u00e9.", "startOffset": 0, "endOffset": 4}, {"referenceID": 24, "context": "[25] Ruoyu Sun and Zhi-Quan Luo.", "startOffset": 0, "endOffset": 4}, {"referenceID": 25, "context": "[26] Joel A Tropp.", "startOffset": 0, "endOffset": 4}, {"referenceID": 14, "context": "Most of the argument of this section follows from [15].", "startOffset": 50, "endOffset": 54}, {"referenceID": 25, "context": "2 (Matrix Bernstein [26]).", "startOffset": 20, "endOffset": 24}], "year": 2016, "abstractText": "Matrix completion, where we wish to recover a low rank matrix by observing a few entries from it, is a widely studied problem in both theory and practice with wide applications. Most of the provable algorithms so far on this problem have been restricted to the offline setting where they provide an estimate of the unknown matrix using all observations simultaneously. However, in many applications, the online version, where we observe one entry at a time and dynamically update our estimate, is more appealing. While existing algorithms are efficient for the offline setting, they could be highly inefficient for the online setting. In this paper, we propose the first provable, efficient online algorithm for matrix completion. Our algorithm starts from an initial estimate of the matrix and then performs non-convex stochastic gradient descent (SGD). After every observation, it performs a fast update involving only one row of two tall matrices, giving near linear total runtime. Our algorithm can be naturally used in the offline setting as well, where it gives competitive sample complexity and runtime to state of the art algorithms. Our proofs introduce a general framework to show that SGD updates tend to stay away from saddle surfaces and could be of broader interests for other non-convex problems to prove tight rates.", "creator": "LaTeX with hyperref package"}}}