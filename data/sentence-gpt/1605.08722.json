{"id": "1605.08722", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "27-May-2016", "title": "An algorithm with nearly optimal pseudo-regret for both stochastic and adversarial bandits", "abstract": "We present an algorithm that achieves almost optimal pseudo-regret bounds against adversarial and stochastic bandits. Against adversarial bandits the pseudo-regret is $O(K\\sqrt{n \\log n})$ and against stochastic bandits the pseudo-regret is $O(\\sum_i (\\log n)/\\Delta_i)$. We also show that no algorithm with $O(\\log n)$ pseudo-regret against stochastic bandits can achieve $\\tilde{O}(\\sqrt{n})$ expected regret against adaptive adversarial bandits.", "histories": [["v1", "Fri, 27 May 2016 17:28:35 GMT  (24kb)", "http://arxiv.org/abs/1605.08722v1", null]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["peter auer", "chao-kai chiang"], "accepted": false, "id": "1605.08722"}, "pdf": {"name": "1605.08722.pdf", "metadata": {"source": "CRF", "title": "An algorithm with nearly optimal pseudo-regret for both stochastic and adversarial bandits", "authors": ["Peter Auer", "Chao-Kai Chiang", "C.-K. Chiang", "AUER CHIANG"], "emails": ["AUER@UNILEOBEN.AC.AT", "CHAOKAI@GMAIL.COM"], "sections": [{"heading": null, "text": "ar X\niv :1\n60 5.\n08 72\n2v 1\n[ cs\n.L G\n(\u221a Kn logn )\nand against stochastic bandits the regret is O ( \u2211\ni (logn)/\u2206i). We also show that no algorithm with O (logn)\npseudo-regret against stochastic bandits can achieve O\u0303 ( \u221a n) expected regret against adaptive adversarial bandits. This complements previous results of Bubeck and Slivkins (2012) that show O\u0303 ( \u221a n) expected adversarial regret with O ( (log n)2 ) stochastic pseudo-regret."}, {"heading": "1. Introduction", "text": "We consider the multi-armed bandit problem, which is the most basic example of a sequential decision problem with an exploration-exploitation trade-off. In each time step t = 1, 2, . . . , n, the player has to play an arm It \u2208 {1, . . . ,K} from this fixed finite set and receives reward xIt(t) \u2208 [0, 1] depending on its choice1. The player observes only the reward of the chosen arm, but not the rewards of the other arms xi(t), i 6= It. The player\u2019s goal is to maximize its total reward \u2211n\nt=1 xIt(t), and this total reward is compared to the best total reward of a single arm, \u2211n\nt=1 xi(t). To identify the best arm the player needs to explore all arms by playing them, but it also needs to limit this exploration to often play the best arm. The optimal amount of exploration constitutes the exploration-exploitation trade-off.\nDifferent assumptions on how the rewards xi(t) are generated have led to different approaches and algorithms for the multi-armed bandit problem. In the original formulation (Robbins, 1952) it is assumed that the rewards are generated independently at random, governed by fixed but unknown probability distributions with means \u00b5i for each arm i = 1, . . . ,K. This type of bandit problem is called stochastic. The other type of bandit problem that we consider in this paper is called non-stochastic or adversarial (Auer et al., 2002b). Here the rewards may be selected arbitrarily by\n\u2217 Accepted for presentation at the Conference on Learning Theory (COLT) 2016. 1. We assume that the player knows the total number of time steps n.\nc\u00a9 P. Auer & C.-K. Chiang.\nan adversary and the player should still perform well for any selection of rewards. An extensive overview of multi-armed bandit problems is given in (Bubeck and Cesa-Bianchi, 2012).\nA central notion for the analysis of stochastic and adversarial bandit problems is the regret R(n), the difference between the total reward of the best arm and the total reward of the player:\nR(n) = max 1\u2264i\u2264K\nn \u2211\nt=1\nxi(t)\u2212 n \u2211\nt=1\nxIt(t).\nSince the player does not know the best arm beforehand and needs to do exploration, we expect that the total reward of the player is less than the total reward of the best arm. Thus the regret is a measure for the cost of not knowing the best arm. In the analysis of bandit problems we are interested in high probability bounds on the regret or in bounds on the expected regret. Often it is more convenient, though, to analyze the pseudo-regret\nR(n) = max 1\u2264i\u2264K E\n[\nn \u2211\nt=1\nxi(t)\u2212 n \u2211\nt=1\nxIt(t)\n]\ninstead of the expected regret\nE [R(n)] = E\n[\nmax 1\u2264i\u2264K\nn \u2211\nt=1\nxi(t)\u2212 n \u2211\nt=1\nxIt(t)\n]\n.\nWhile the notion of pseudo-regret is weaker than the expected regret with R(n) \u2264 E [R(n)], bounds on the pseudo-regret imply bounds on the expected regret for adversarial bandit problems with oblivious rewards xi(t) selected independently from the player\u2019s choices. The pseudo-regret also allows for refined bounds in stochastic bandit problems."}, {"heading": "1.1. Previous results", "text": "For adversarial bandit problems, algorithms with high probability bounds on the regret are known (Bubeck and Cesa-Bianchi, 2012, Theorem 3.3): with probability 1\u2212 \u03b4,\nRadv(n) = O ( \u221a n log(1/\u03b4) ) .\nFor stochastic bandit problems, several algorithms achieve logarithmic bounds on the pseudo-regret, e.g. Auer et al. (2002a):\nRsto(n) = O (log n) .\nBoth of these bounds are known to be best possible. While the result for adversarial bandits is a worst-case \u2014 and thus possibly pessimistic \u2014 bound that holds for any sequence of rewards, the strong assumptions for stochastic bandits may sometimes be unjustified. Therefore an algorithm that can adapt to the actual difficulty of the problem is of great interest. The first such result was obtained by Bubeck and Slivkins (2012), who developed the SAO algorithm that with probability 1\u2212 \u03b4 achieves\nRadv(n) \u2264 O ( (log n) \u221a n log(n/\u03b4) )\nregret for adversarial bandits and\nRsto(n) = O ( (log n)2 )\npseudo-regret for stochastic bandits. It has remained as an open question if a stochastic pseudo-regret of order O ( (log n)2 )\nis necessary or if the optimal O (log n) pseudo-regret can be achieved while maintaining an adversarial regret of order \u221a n."}, {"heading": "1.2. Summary of new results", "text": "We give a twofold answer to this open question. We show that stochastic pseudo-regret of order O ( (log n)2 )\nis necessary for a player to achieve high probability adversarial regret of order \u221a n against an oblivious adversary, and to even achieve expected regret of order \u221a n against an adaptive adversary. But we also show that a player can achieve O (log n) stochastic pseudoregret and O\u0303 ( \u221a n) adversarial pseudo-regret at the same time. This gives, together with the results of (Bubeck and Slivkins, 2012), a quite complete characterization of algorithms that perform well both for stochastic and adversarial bandit problems.\nMore precisely, for any player with stochastic pseudo-regret bound of order O ( (log n)\u03b2 )\n, \u03b2 < 2, and any \u01eb > 0, \u03b1 < 1, there is an adversarial bandit problem for which the player suffers \u2126(n\u03b1) regret with probability \u2126(n\u2212\u01eb). Furthermore, there is an adaptive adversary against which the player suffers \u2126(n\u03b1) expected regret. Secondly, we construct an algorithm with\nRsto(n) = O (log n)\nand Radv(n) = O ( \u221a n log n ) .\nAt first glance these two results may appear contradictory for \u03b1\u2212 \u01eb > 1/2, as the lower bound seems to suggest a pseudo-regret of \u2126(n\u03b1\u2212\u01eb). This is not the case, though, since the regret may also be negative. Indeed, consider an adversarial multi-armed bandit that initially gives higher rewards for one arm, and from some time step on gives higher rewards for a second arm. A player that detects this change and initially plays the first arm and later the second arm, may outperform both arms and achieve negative regret. But if the player misses the change and keeps playing the first arm, it may suffer large regret against the second arm.\nIn our analysis we use both mechanisms. For the lower bound on the pseudo-regret we show that a player with little exploration (which is necessary for small stochastic pseudo-regret) will miss such a change with significant probability and then will suffer large regret. For the upper bound we explicitly compensate possible large regret that occurs with small probability by negative regret that occurs with sufficiently large probability. For the lower bound on the expected regret we construct an adaptive adversary that prevents such negative regret. Consequently, our results exhibit one of the rare cases where there is a significant gap between the achievable pseudo-regret and the achievable expected regret.\nThe explicit consideration of negative regret is one of the technical contributions of this work. Another, maybe even more significant contribution, is a weak testing scheme for non-stochastic arms. This weak testing scheme is necessary since O (log n) stochastic pseudo-regret allows only for very little exploration. Each individual weak test has a constant false positive rate (predicting\na non-stochastic arm although the arm is stochastic) and a constant false negative rate (missing a non-stochastic arm). To avoid classifying a stochastic arm as non-stochastic, an arm is classified as non-stochastic only after O (log n) positive tests. This reduces the false positive rate of a decision to acceptable O (1/n). Conversely, this delayed detection needs to be accounted for in the regret analysis when the arms are indeed non-stochastic."}, {"heading": "2. Definitions and statement of results", "text": "In a multi-armed bandit problem with arms i = 1, . . . ,K the interaction of a player with its environment is governed by the following protocol:\nFor time steps t = 1, . . . , n:\n1. The player chooses an arm It \u2208 {1, . . . ,K}, possibly using randomization. 2. The player receives and observes the reward xIt(t).\nIt does not observe the reward from any other arm i 6= It.\nThe player\u2019s choice It may depend only on information available at this time, namely I1, . . . , It\u22121 and xI1(1), . . . , xIt\u22121(t \u2212 1). If the bandit problem is stochastic, then the rewards xi(t) are generated independently at random. If the bandit problem is adversarial, then the rewards are generated arbitrarily by an adversary. We assume that all rewards xi(t) \u2208 [0, 1] and that the number of time steps n is known to the player."}, {"heading": "2.1. Stochastic multi-armed bandit problems", "text": "In a stochastic multi-armed bandit problem the rewards for each arm i are generated by a fixed but unknown probability distribution \u03bdi on [0, 1]. All rewards xi(t), 1 \u2264 i \u2264 K , 1 \u2264 t \u2264 n, are generated independently at random with xi(t) \u223c \u03bdi.\nImportant quantities are the average rewards of the arms, \u00b5i = E [xi(t)], the average reward of the best arm \u00b5\u2217 = maxi \u00b5i, and the resulting gaps \u2206i = \u00b5\u2217 \u2212 \u00b5i.\nThe goal of the player is to achieve low pseudo-regret which for a stochastic bandit problem can be written as\nRsto(n) = max 1\u2264i\u2264K E\n[\nn \u2211\nt=1\nxi(t)\u2212 n \u2211\nt=1\nxIt(t)\n]\n=\nK \u2211\ni=1\n\u2206iE [Ti(n)] ,\nwhere Ti(n) = #{1 \u2264 t \u2264 n : It = i} is the number of plays of arm i. It can be shown (Auer et al., 2002a) that \u2014 among others \u2014 upper confidence bound algorithms achieve\nE [Ti(n)] = O\n(\nlog n\n\u22062i\n)\nfor any arm i with \u2206i > 0 such that\nRsto(n) = O\n\n\n\u2211\ni:\u2206i>0\nlog n\n\u2206i\n\n .\nIt can be even shown that for arms i with \u2206i > 0,\nTi(n) = O\n(\nlog(n/\u03b4)\n\u22062i\n)\nwith probability 1\u2212 \u03b4 when n is known to the player."}, {"heading": "2.2. Adversarial multi-armed bandit problems", "text": "In adversarial bandit problems the rewards are selected by an adversary. If this is done beforehand (before the player interacts with the environment), then the adversary is called oblivious as the selection of rewards is independent from the arms It chosen by the player. In this case any upper bound on the pseudo-regret that holds for any selection of rewards is also an upper bound on the expected regret.\nIf the selection of rewards xi(t), 1 \u2264 i \u2264 K , depends on which arms I1, . . . , It\u22121 the player has chosen in the past, then the adversary is called adaptive. In this case a bound on the pseudo-regret does not necessarily translate into a bound on the expected regret. Nevertheless, strong bounds on the regret against an adaptive adversary are known for the EXP3.P algorithm (Auer et al., 2002b):\nTheorem 1 (Bubeck and Cesa-Bianchi, 2012, Theorem 3.3) When EXP3.P is run with appropriate parameters depending on n, K , and \u03b4, then with probability 1\u2212 \u03b4 its regret satisfies\nRada(n) = O ( \u221a nK log(K/\u03b4) ) ."}, {"heading": "2.3. Results", "text": "First, we state our lower bounds for oblivious and adaptive adversaries.\nTheorem 2 Let \u03b1 < 1, \u01eb > 0, \u03b2 < 2, and C > 0. Consider a player that achieves pseudo-regret\nRsto(n) \u2264 C(log n)\u03b2\nfor any stochastic bandit problem with two arms and gap \u2206 = 1/8. Then for large enough n there is an adversarial bandit problem with two arms and an oblivious adversary such that the player suffers regret\nRobl(n) \u2265 n\u03b1/8\u2212 4 \u221a n log n\nwith probability at least 1/(16n\u01eb) \u2212 2/n2. Furthermore, there is an adversarial bandit problem with two arms and an adaptive adversary such that the player suffers expected regret\nE [Rada(n)] \u2265 n\u03b1\u2212\u01eb\n128 \u2212 3\n\u221a\nn log n.\nIn Section 3 we present our SAPO algorithm (Stochastic and Adversarial Pseudo-Optimal) that achieves optimal pseudo-regret in stochastic bandit problems and nearly optimal pseudo-regret in adversarial bandit problems. Its performance is summarized in the following theorem.\nTheorem 3 For large enough n and any \u03b4 > 0, algorithm SAPO achieves the following bounds for suitable constants Csto, Cadv, and C1b:\n\u2022 For stochastic bandit problems with gaps \u2206i such that C1b \u2211 i:\u2206i>0 log(n/\u03b4) \u2206i \u2264 \u221a nK log(n/\u03b4),\nTi(n) \u2264 Csto log(n/\u03b4)\n\u22062i\nwith probability 1\u2212 \u03b4 for any arm i with \u2206i > 0, and thus\nRsto(n) \u2264 Csto \u2211\ni:\u2206i>0\nlog(n/\u03b4)\n\u2206i + \u03b4n.\n\u2022 For adversarial bandit problems\nRada(n) \u2264 CadvK \u221a n log(n/\u03b4) + \u03b4n.\nRemark 4 Our bound for adversarial bandit problems shows a worse dependency on K than Theorem 1. This is an artifact of our current analysis and can be improved to a bound Rada(n) = O ( \u221a nK log(n/\u03b4) ) ."}, {"heading": "2.4. Comparison with related work", "text": "Bubeck and Slivkins (2012) show for their SAO algorithm that with probability 1\u2212 \u03b4, K \u2211\ni=1\n\u2206iTi(n) \u2264 O ( K logK(log n/\u03b4)2\n\u2206\n)\nfor stochastic bandits where \u2206 = mini:\u2206i>0 \u2206i, and\nRada(n) \u2264 O ( (logK)(log n) \u221a nK log n/\u03b4 )\nfor adaptive adversarial bandits. While our bounds in Theorem 3 are somewhat tighter, in particular showing the optimal dependency on the gaps \u2206i for stochastic bandits, we have only a result on the pseudo-regret for adversarial bandits. We conjecture though, that our analysis can be used to construct an algorithm that with probability 1\u2212 \u03b4 achieves Ti(n) \u2264 O ( (log n/\u03b4)2/\u22062i ) for stochastic bandits and Rada(n) \u2264 O ( (logK)(log n) \u221a nK log n/\u03b4 ) for adaptive adversarial bandits.\nOur SAPO algorithm follows the general strategy of the SAO algorithm by essentially employing an algorithm for stochastic bandit problems that is equipped with additional tests to detect nonstochastic arms. A different approach is taken in (Seldin and Slivkins, 2014): here the starting point is an algorithm for adversarial bandit problems that is modified by adding an additional exploration parameter to achieve also low pseudo-regret in stochastic bandit problems. While this approach has not yet allowed for the tight O (log n) regret bound in stochastic bandit problems (they achieve a O ( log3 n )\nbound), the approach is quite flexible and more generally applicable than the SAO and SAPO algorithms."}, {"heading": "2.5. Proof sketch of the lower bound (Theorem 2)", "text": "We present here the main idea of the proof. The proof itself is given in Appendix B. We consider a stochastic bandit problem with constant reward x1(t) = 1/2 for arm 1 and Bernoulli rewards with \u00b52 = 1/2 \u2212\u2206 for arm 2, \u2206 = 1/8. We divide the time steps into phases of increasing length Lj = 3jn\u03b1, j = 0, . . . , J with J = \u2126(log n). Since the pseudo-regret of the player is O ( (log n)\u03b2 )\n, there is a phase j\u2217 where the expected number of plays of arm 2 in this phase is O ( (log n)\u03b2\u22121 )\n. We construct an oblivious adversarial bandit by modifying the Bernoulli distribution of arm 2 in phase j\u2217 and beyond by setting \u00b52 = 1/2 +\u2206. By this modification arm 2 gives larger total reward than arm 1.\nBecause of the limited number of plays in phase j\u2217, a standard argument shows that the player will not detect this modification during phase j\u2217 with probability exp{\u2212O(log\u03b2\u22121 n)} = \u2126(n\u2212\u01eb). When the modification is not detected during phase j\u2217, then in this phase the player suffers roughly regret \u2206Lj\u2217 against arm 2. This is not compensated by negative regret against arm 2 in previous phases since \u2206\n\u2211j\u2217\u22121 j=0 Lj \u2264 \u2206Lj\u2217/2. Thus in this case the overall regret of the player against arm 2\nis roughly \u2206Lj\u2217/2 = \u2126(n\u03b1). In a very similar way we can construct also an adaptive adversarial bandit: As for the oblivious bandit, we set \u00b52 = 1/2 + \u2206 in phase j\u2217. If the player chooses arm 2 only C(log n)\u03b2\u22121 times in phase j\u2217, then we keep \u00b52 = 1/2 + \u2206 also for the remaining phases. As for the oblivious bandit this happens with probability \u2126(n\u2212\u01eb) and gives regret \u2126(n\u03b1). To avoid negative regret, we switch back to \u00b52 = 1/2 \u2212 \u2206, as soon as there more than C(log n)\u03b2\u22121 plays of arm 2 in phase j\u2217. In this case the reward of the algorithm is roughly n/2+C\u2206(log n)\u03b2\u22121 such that in this case R(n) \u2265 \u2212C\u2206(log n)\u03b2\u22121. Hence the expected regret is E [R(n)] \u2265 \u2126(n\u03b1\u2212\u01eb)\u2212 C\u2206(log n)\u03b2\u22121 = \u2126(n\u03b1\u2212\u01eb)."}, {"heading": "3. The SAPO algorithm", "text": "In its core the algorithm is an elimination procedure for stochastic bandits that is augmented by tests safeguarding against non-stochastic arms. If there is sufficient evidence for non-stochastic arms, then the algorithm switches to the adversarial bandit algorithm EXP3.P, starting with the current time step.\nThe algorithm maintains a set of active arms A and a set of supposedly suboptimal \u201cbad\u201d arms B. For each arm i it maintains the sample mean \u00b5\u0302i(s),\n\u00b5\u0302i(s) = 1\nTi(s)\ns \u2211\nt=1\nxi(t)I [It = i] ,\nTi(s) = s \u2211\nt=1\nI [It = i] ,\nand also an unbiased estimate to deal with non-stochastic arms,\n\u00b5\u0304i(s) = 1\ns\ns \u2211\nt=1\nxi(t) I [It = i]\npi(t) ,\nAlgorithm 1 : SAPO Input: Number of arms K , number of rounds n \u2265 K , and confidence parameter \u03b4. Initialization: All arms are active, A(0) = {1, . . . ,K}, B(0) = \u2205. For t = 1, . . . , n:\n1. (a) If there is an arm i \u2208 A(t\u2212 1) with \u00b5\u0304i(t\u2212 1) 6\u2208 [lcbi(t\u2212 1),ucbi(t\u2212 1)], then switch to EXP3.P.\n(b) If \u2211t\u22121 s=1[lcb \u2217(s)\u2212 xIs(s)] > C1b\n\u221a\nKn log(n/\u03b4), then switch to Exp3.P.\n2. Evict arms from A:\n(a) Let B(t) = {i \u2208 A(t\u2212 1) : Ti(t\u2212 1) \u2265 Cinit \u00b7 log(n/\u03b4) \u2227 \u00b5\u0302i(t\u2212 1) +Cgap \u00b7 widthi(t\u2212 1) < lcb\u2217(t\u2212 1)},\nA(t) = A(t\u2212 1) \\B(t), B(t) = B(t\u2212 1) \u222aB(t). (b) For all i \u2208 B(t) set \u00b5\u0303i = \u00b5\u0302i(t\u2212 1), \u2206\u0303i = Cgap \u00b7 widthi(t\u2212 1),\nni(t) = t, Li(t) = L0i := \u2308CpK/\u2206\u03032i \u2309, and Ei(t) = 0.\n3. Choose It = i with probabilities\npi(t) =\n{\nL0i /(KLi(t)) for i \u2208 B(t) (\n1\u2212\u2211j\u2208B(t) pj(t) )/ |A(t)| for i \u2208 A(t)\n4. Test and update all arms i \u2208 B(t):\n(a) If \u2203s : ni(t) \u2264 s \u2264 t : D\u0302i(s, t) \u2265 C4a\u2206\u0303iLi(t)pi(t), (b) then ni(t+ 1) = t+ 1, Li(t+ 1) = max{Li(t)/2, L0i },\nand Ei(t+ 1) = Ei(t) + 1,\n(c) if Ei(t+ 1) = E0 := \u2308CE \u00b7 log(n/\u03b4)\u2309, then switch to EXP3.P; (d) else if t = ni(t) + Li(t)\u2212 1 then ni(t+ 1) = t+ 1, Li(t+ 1) = 2Li(t),\nand Ei(t+ 1) = Ei(t);\n(e) else ni(t+ 1) = ni(t), Li(t+ 1) = Li(t), and Ei(t+ 1) = Ei(t).\nwhere pi(t) is the probability of choosing arm i at time t. Confidence bounds2 around the estimated means are used to evict arms from the active set A,\nlcbi(s) = max{lcbi(s\u2212 1), \u00b5\u0302i(s)\u2212 widthi(s)}, lcbi(s) = max{lcbi(s\u2212 1), \u00b5\u0304i(s)\u2212 width(s)}, ucbi(s) = min{ucbi(s \u2212 1), \u00b5\u0304i(s) + width(s)}, lcb\u2217(s) = max\n1\u2264i\u2264K max{lcbi(s), lcbi(s)},\nwidthi(s) = \u221a Cw log(n/\u03b4)/Ti(s),\nwidth(s) = \u221a\nCwK log(n/\u03b4)/s.\n2. We start with lcbi(0) = lcbi(0) = 0 and ucbi(0) = 1.\nNote that lcbi(s), lcbi(s), and lcb \u2217(s) are non-decreasing and ucbi(s) are non-increasing. This reflects the intuition that confidence intervals should be shrinking and is used to safeguard against non-stochastic arms.\nAn arm i is evicted from A in Step 2.a, if it has a sufficient number of plays (Cinit \u00b7 log(n/\u03b4)) for reasonably accurate estimates, and if its sample mean \u00b5\u0302i(t\u2212 1) is significantly smaller than the optimal lower confidence bound lcb\u2217(t \u2212 1). The additional distance Cgap \u00b7 widthi(t \u2212 1) is used to estimate the gap \u2206i. For evicted arms, in Step 2.b an estimate for the gap \u2206\u0303i and the current estimated mean are frozen, \u00b5\u0303i = \u00b5\u0302i(t \u2212 1). For stochastic bandits the accuracy of this estimate is proportional to the estimated gap \u2206\u0303i. These quantities are used in the tests for detecting nonstochastic arms. Also the starting time ni(t) and the length Li(t) = L0i of the first testing phase (see below), as well as the number of detections Ei(t) = 0 are set.\nSince SAPO needs to perform well also against adversaries, all choices of arms are randomized. In Step 3 an active arm is chosen uniformly at random, or with some smaller probabilitya bad arm i is chosen where the probability depends on the length of its current testing phase Li(t). Choosing also bad arms is necessary to detect non-stochastic arms among the bad arms."}, {"heading": "3.1. Tests for detecting non-stochastic arms", "text": "The most important test is in Step 4.a for detecting that a bad arm receives larger rewards than it should if it were stochastic. Such an arm could be optimal if the bandit problem is adversarial. The best way to view this test is by dividing the time steps of an evicted arm i into testing phases\n\u03c4i,1, . . . , \u03c4i,2 \u2212 1; \u03c4i,2, . . . , \u03c4i,3 \u2212 1; \u03c4i,3, . . . , \u03c4i,4 \u2212 1; . . .\nThe first phase starts when arm i is evicted from A. A phase k ends at time \u03c4i,k+1 \u2212 1 if either the phase has exhausted its length (Step 4.d), or when the test in Step 4.a reports a detection.3 Thus the length parameter Li(t) is only the maximal length of a phase and the phase may end earlier. In the notation of the algorithm ni(t) denotes the start of the current phase. Within a phase the probability pi(t) for choosing arm i is constant since the length parameter Li(t) does not change (Step 4.e). For notational convenience we denote by pik the probability for choosing arm i in its k-th testing phase, and by Lik the corresponding length parameter,\npi(t) = pik for i \u2208 B(t) and \u03c4i,k \u2264 t < \u03c4i,k+1, Li(t) = Lik for i \u2208 B(t) and \u03c4i,k \u2264 t < \u03c4i,k+1, ni(t) = \u03c4i,k for i \u2208 B(t) and \u03c4i,k \u2264 t < \u03c4i,k+1.\nNow the test in Step 4.a checks if a bad arm i has received significantly more rewards in the current phase then expected, given the estimated mean \u00b5\u0303i, the maximal phase length Li(t) and the probability for choosing arm i, pi(t), where\nD\u0302i(s1, s2) =\ns2 \u2211\nt=s1\n[xi(t)\u2212 \u00b5\u0303i]I [It = i] .\nIf arm i is stochastic, then E [ D\u0302i(s1, s2) ] = O ( Li(t)\u2206\u0303ipi(t) ) such that a positive test suggests that the arm is non-stochastic. Since the expected number of plays of arm i is L0i /K in each phase,\n3. The last phase ends when the total number of time steps n is exhausted or when the algorithm switches to EXP3.P.\nthe test is weak, though, with constant false positive and false negative rates. To avoid incorrectly classifying a stochastic arm as non-stochastic, the test is repeated several times. To make the tests independent, a new phase is started in Step 4.b after a detection is reported. To avoid that too much regret accumulates in the case of a non-stochastic arm, the phase length is halved. If there have been E0 independent detections, then in Step 4.c there is sufficient evidence for a non-stochastic arm and the algorithm switches to EXP3.P.\nIn Step 4.d the phase ends because it has exhausted its length. Since the test in Step 4.a has given no detection, arm i has performed as expected and the algorithm has accumulated negative regret against this bad arm. This negative regret allows to start the next phase with a doubled phase length, even if the arm were non-stochastic. Doubling the phase length is necessary to avoid too many phases for a stochastic arm. (Remember that the expected number of plays of a bad arm is L0i /K in each phase.)\nIn Step 4.e none of the above condition is satisfied and the phase continues. Additional simpler tests for non-stochastic arms are performed in Step 1. Step 1.a checks whether for all active arms the unbiased estimates of the means obey the corresponding confidence intervals. Finally, Step 1.b checks if the algorithm receives significantly less reward than expected from the best lower confidence bound. This may happen if a non-stochastic arm first appears close to optimal but then receives less rewards."}, {"heading": "3.2. Choice of constants in the SAPO algorithm", "text": "In the algorithm we keep the constant names because we find them easier to read than actual values. Proper values for the constants are as follows: Cw = 16, C1b = 522, Cinit = 100/9, Cgap = 60, Cp = 1300, C4a = 1/10, and CE = 15."}, {"heading": "4. Preliminaries for the analysis of SAPO", "text": "An important tool for our analysis are concentration inequalities, in particular Bernstein\u2019s inequality for martingales and a variant of Hoeffding-Azuma\u2019s inequality for the maximum of partial sums, max1\u2264s\u2264t\u2264n \u2211t i=s Yi. These inequalities are given in Appendix A. We denote by Ht the past up to and including time t. The next lemma states some properties of algorithm SAPO. Let\nTi(s1, s2) = #{t : s1 \u2264 t \u2264 s2 : It = i}\ndenote the number of plays of arm i in time steps s1 to s2, let nB,i be the time when arm i is evicted from A, i \u2208 A(nB,i \u2212 1) and i \u2208 B(nB,i), and let nS be the time step when SAPO switches to EXP3.P. If SAPO never switches to EXP3.P, then nS = n.\nLemma 5 (a) If i \u2208 B(t) then \u00b5\u0303i + \u2206\u0303i < lcb\u2217(t). (b) For each arm the number of testing phases k, \u03c4i,k \u00b7 \u00b7 \u00b7 \u03c4i,k+1 \u2212 1 is\nat most M = \u2308log2 n\u2309+ 2E0. (c) With probability 1\u2212O (\u03b4), the number of plays of any bad arm i is bounded as\nTi(nB,i, nS) \u2264 101100L0iM/K = O ( M/\u2206\u03032i ) .\nProof (Sketch) Statement (a) follows immediately from Step 2 of the algorithm since \u00b5\u0303i = \u00b5\u0302i(nB,i\u22121), \u2206\u0303i = Cgap \u00b7widthi(nB,i\u22121), \u00b5\u0302i(nB,i\u22121)+Cgap \u00b7widthi(nB,i\u22121) < lcb\u2217(nB,i\u22121), and lcb\u2217(t) is non-decreasing.\nStatement (b) follows from the fact that Step 4.b (where the phase length is halved) is executed at most E0 times. In the other phases the phase length is doubled in Step 4.d. Since the phase length is at most n, the number of phases is at most log2 n+ 2E\n0. For statement (c) we observe that by the definition of pi(t) the expected number of plays in any testing phase of a bad arm i is L0i /K . Thus the expected number of plays in all phases is L 0 iM/K . Since the variance is bounded by the same quantity, an application of Bernstein\u2019s inequality gives the result.\nDetailed proofs are given in Appendix C."}, {"heading": "5. Analysis of SAPO for adversarial bandits", "text": "In this section we prove pseudo-regret bounds for SAPO against adversarial and possibly adaptive bandits. Since we know from Theorem 1 that EXP3.P suffers small regret, we only need to bound the pseudo-regret of SAPO before it switches to EXP3.P. For the remaining section we fix some arm i. We have\nnS \u2211\nt=1\nxi(t)\u2212 nS \u2211\nt=1\nxIt(t) =\nnS \u2211\nt=1\n[xi(t)\u2212 lcb\u2217(t)] + nS \u2211\nt=1\n[lcb\u2217(t)\u2212 xIt(t)]\n=\nnB,i\u22121 \u2211\nt=1\n[xi(t)\u2212 lcb\u2217(t)] + nS \u2211\nt=nB,i\n[xi(t)\u2212 lcb\u2217(t)] + nS \u2211\nt=1\n[lcb\u2217(t)\u2212 xIt(t)] (1)\nThe first sum in (1) bounds the regret for the time when i is an active arm. For stochastic arms, the best lower confidence bound lcb\u2217(t) would be not too far from the rewards of the arms that are still active. For non-stochastic arms, though, we need the tests in SAPO, in particular those in Step 1, to guarantee a similar behavior and achieve\nE\n\n\nnB,i\u22121 \u2211\nt=1\n[xi(t)\u2212 lcb\u2217(t)]\n\n = O ( \u221a Kn log(n/\u03b4) ) , (2)\nsee Appendix D.1. The crucial part of the analysis concerns the second sum in (1) which bounds the regret for the time when i is a bad arm. For its analysis we explicitly track negative regret to compensate for positive regret. In Section 5.1 below we sketch the main ideas for handling this sum (formal proofs are given in Appendix D.2), showing that\nE\n\n\nnS \u2211\nt=nB,i\n[xi(t)\u2212 lcb\u2217(t)]\n\n = O\n(\nK log(n/\u03b4)\n\u2206\u0303i\n)\n. (3)\nNote that 1/\u2206\u0303i = O (widthi(nB,i \u2212 1)) = O (\u221a Ti(nB,i)/ log(n/\u03b4) ) = O ( \u221a n/ log(n/\u03b4) ) such that O (\nK log(n/\u03b4)/\u2206\u0303i\n) = O ( K \u221a n log(n/\u03b4) ) .\nFinally, the third sum can be observed by the algorithm and is taken care of by the test in Step 1.b, such that\nnS \u2211\nt=1\n[lcb\u2217(t)\u2212 xIt(t)] = O ( \u221a Kn log(n/\u03b4) ) . (4)\nTogether, inequalities (1)\u2013(4) and the bound on EXP3.P in Theorem 1 give the bound on the pseudoregret in Theorem 3."}, {"heading": "5.1. Bounding the regret for bad arms", "text": "If a bad arm is non-stochastic, then it may first appear suboptimal but still be optimal after all. We need to show that the tests of our algorithm, in particular the test in Step 4.a, are sufficient to detect such a situation. Since the algorithm checks arms in B(t) only rarely, it will take some time for such detection. In our analysis we explicitly compensate the regret during this delayed detection by the negative regret accumulated while arm i was performing suboptimally.\nWe consider the testing phases k, \u03c4i,k . . . \u03c4i,k+1 \u2212 1, of arm i, and recall that Lik is the length parameter for phase k and pik = L0i /(KLik) is the probability for choosing arm i in phase k. Furthermore, let Eik the value of Ei(t) in phase k. Note that these quantities may change only when a new phase begins. We denote by Pik {\u00b7} = P { \u00b7|H\u03c4i,k\u22121 } and Eik [\u00b7] = E [ \u00b7|H\u03c4i,k\u22121 ]\nthe probabilities and expectations conditioned on the past before phase k.\nFor any phase we have\n\u03c4i,k+1\u22121 \u2211\nt=\u03c4i,k\n[xi(t)\u2212 lcb\u2217(t)] = \u03c4i,k+1\u22121 \u2211\nt=\u03c4i,k\n[xi(t)\u2212 \u00b5\u0303i + \u00b5\u0303i \u2212 lcb\u2217(t)]\n<\n\u03c4i,k+1\u22121 \u2211\nt=\u03c4i,k\n[xi(t)\u2212 \u00b5\u0303i]\u2212 \u2206\u0303i[\u03c4i,k+1 \u2212 \u03c4i,k] (5)\nby Lemma 5a. Thus we want to prevent that the rewards of arm i are significantly larger than the estimated mean \u00b5\u0303i. In particular, the test in Step 4.a is supposed to detect events Di(s1, s2) > 2C4a\u2206\u0303iLik with\nDi(s1, s2) :=\ns2 \u2211\nt=s1\n[xi(t)\u2212 \u00b5\u0303i].\nSince on average arm i is chosen only L0i /K times per phase, there is a constant false negative rate qadv for missing such events. For appropriate Cp, though, the false negative rate qadv is sufficiently small, qadv \u2264 1/25: Since E [ D\u0302i(s1, s2) ]\n= pikDi(s1, s2) for \u03c4i,k \u2264 s1 \u2264 s2 < \u03c4i,k+1, and Step 4.a tests for D\u0302i(s1, s2) > C4a\u2206\u0303iLikpik, we can bound qadv by Bernstein\u2019s inequality using that 1 \u2264 \u2206\u03032iL0i /(KCp) and a bound on the variance,\nV\n[ D\u0302i(s1, s2) ] \u2264 Likpik = L0i /K \u2264 (\u2206\u0303iL0i /K)2/Cp = (\u2206\u0303iLikpik)2/Cp.\nThe formal proof is given in Lemma 13. We use the false negative rate qadv to bound Eik [Di(\u03c4i.k, \u03c4i.k+1 \u2212 1)]. Each time an event Di(s, t) > 2C4a\u2206\u0303iLik is missed (we consider only non-overlapping such events), Di(\u03c4i.k, t)\nhas increased by at most 2C4a\u2206\u0303iLik + 1, and the probability for the m-th miss is at most qmadv. When such an event is detected, then the phase ends and Di(\u03c4i.k, t) again has increased by at most 2C4a\u2206\u0303iLik + 1. Thus (see Lemma 15 for the formal proof)\nEik [Di(\u03c4i.k, \u03c4i.k+1 \u2212 1)] \u2264 (2C4a\u2206\u0303iLik + 1) \u2211\nm\u22650\nqmadv = 2C4a\u2206\u0303iLik + 1\n1\u2212 qadv\nwhich by (5) gives\nEik\n\n\n\u03c4i,k+1\u22121 \u2211\nt=\u03c4i,k\n[xi(t)\u2212 lcb\u2217(t)]\n\n < 2C4a\u2206\u0303iLik + 1\n1\u2212 qadv \u2212 \u2206\u0303iEik [\u03c4i,k+1 \u2212 \u03c4i,k] . (6)\nSince the bound in (6) is large for large Lik, we show that such a large contribution to the regret can be compensated by negative regret in previous phases due to the term \u2212\u2206\u0303i[\u03c4i,k+1 \u2212 \u03c4i,k]. We show by backward induction over the phases that the expected regret starting from phase k can be bounded,\nEik\n\n\nnS \u2211\nt=\u03c4i,k\n[xi(t)\u2212 lcb\u2217(t)]\n\n \u2264 \u03a6i(k, Lik) := Lik\u2206\u0303i/2 + 3L0i \u2206\u0303i(M \u2212 k + 1)\nwhere M is the maximal number of phases from Lemma 5b.\nLemma 6 Let\nFik =\nnS \u2211\nt=\u03c4i,k\n[xi(t)\u2212 lcb\u2217(t)] .\nThen\nEik [Fik] \u2264 \u03a6i(k, Lik).\nProof Let kS be the last phase before the algorithm switches to EXP3.P with \u03c4kS+1 \u2212 1 = nS . By Lemma 5b we have kS \u2264 M . For k = kS + 1 the lemma holds trivially since Fi,kS+1 = 0.\nBy (6) we have\nEik [Fik] \u2264 2C4a\u2206\u0303iLik + 1\n1\u2212 qadv + Eik\n[ Fi,k+1 \u2212 \u2206\u0303i(\u03c4i,k+1 \u2212 \u03c4i,k) ] .\nFor the expectation on the right hand side we distinguish three cases, depending on the termination condition of phase k and the value of Lik.\nCase 1: Phase k is terminated by the condition in Step 4.d. Then Li,k+1 = 2Lik and\nEik\n[ Fi,k+1 \u2212 \u2206\u0303i(\u03c4i,k+1 \u2212 \u03c4i,k) \u2223 \u2223 \u2223 Case 1 ] \u2264 \u03a6i(k + 1, 2Lik)\u2212 \u2206\u0303iLik (7)\nusing the induction hypothesis. This is the case where negative regrets accumulate since 2C4a/(1\u2212 qadv) < 1.\nCase 2: Phase k is terminated by the condition in Step 4.a (4) and Lik > L0i . Then Li,k+1 = Lik/2 and\nEik\n[ Fi,k+1 \u2212 \u2206\u0303i(\u03c4i,k+1 \u2212 \u03c4i,k) \u2223 \u2223 \u2223 Case 2 ] \u2264 \u03a6i(k + 1, Lik/2). (8)\nCase 3: Phase k is terminated by the condition in Step 4.a and Lik = L0i . Then Li,k+1 = L0i and\nEik\n[ Fi,k+1 \u2212 \u2206\u0303i(\u03c4i,k+1 \u2212 \u03c4i,k) \u2223 \u2223 \u2223Case 3 ] \u2264 \u03a6i(k + 1, L0i ). (9)\nTo complete the induction proof, we need to show that for all three cases the right hand side of (7)\u2013 (9) is upper bounded by\n\u03a6i(k, Lik)\u2212 2C4a\u2206\u0303iLik + 1\n1\u2212 qadv .\nThis can be verified by straightforward calculation.\nNow (3) follows from Lemma 6 for k = 1:\nE\n\n\nnS \u2211\nt=nB,i\n[xi(t)\u2212 lcb\u2217(t)]\n\n \u2264 \u03a6i(1, L0i ) = O ( L0i \u2206\u0303iM ) = O\n(\nK log(n/\u03b4)\n\u2206\u0303i\n)\n."}, {"heading": "6. The stochastic analysis", "text": "In this section we assume that all arms i are indeed stochastic with means \u00b5i. Recall that \u2206i = \u00b5\u2217 \u2212 \u00b5i, \u00b5\u2217 = maxi \u00b5i. We show that with high probability the algorithm does not switch to EXP3.P and any suboptimal arm i is chosen at most O (\nlog(n/\u03b4)/\u22062i ) times.\nWe already have from Lemma 5c that with probability 1 \u2212 O (\u03b4), Ti(nB,i, nS) = O ( M/\u2206\u03032i ) for all arms. Thus we only need to bound the number of plays before an arm is evicted from A, Ti(1, nB,i \u2212 1). The next lemma summarizes some properties of SAPO against stochastic bandits.\nLemma 7 With probability 1\u2212O (\u03b4) the following holds for all time steps t and all arms i: (a) If i \u2208 A(t) then |\u00b5\u0304i(t)\u2212 \u00b5i| \u2264 width(t)/2. (b) If i \u2208 A(t) then |\u00b5\u0302i(t)\u2212 \u00b5i| \u2264 widthi(t)/2. (c) If i \u2208 A(t) then \u00b5\u0304i(t), \u00b5i \u2208 [lcbi(t),ucbi(t)] and \u00b5\u0302i(t), \u00b5i \u2265 lcbi(t). (d) If \u2206i\u2217 = 0 then i\u2217 \u2208 A(t). Furthermore, \u00b5\u2217 \u2265 lcb\u2217(t). (e) If i \u2208 B(t) then \u2206\u0303i \u2264 2\u2206i.\nProof (Sketch) Statements (a) and (b) follow from Hoeffding-Azuma\u2019s inequality. Details are given in Appendix E.1.\nFor statement (c) we observe that by construction there is a time s \u2264 t with \u00b5\u0304i(s)\u2212width(s) = lcbi(t). Thus (a) implies \u00b5\u0304i(t) \u2265 \u00b5i\u2212width(t)/2 \u2265 \u00b5\u0304i(s)\u2212width(s)/2\u2212width(t)/2 \u2265 \u00b5\u0304i(s)\u2212 width(s) = lcbi(t). The other inequalities follow analogously.\n4. If k is the last phase and the phase is terminated by a condition in Step 1, then the same analysis applies but the value of Lk+1,i is irrelevant, since Fi,k+1 = 0.\nStatement (d) is proven by induction on t. Let i\u2217 be an arm with \u00b5i\u2217 = \u00b5\u2217. If i\u2217 \u2208 A(t \u2212 1) then we have by (c) that \u00b5\u2217 \u2265 lcb\u2217(t\u22121). If any arm i is evicted at time t, then we have by Step 2.a and (b) that \u2206i = \u00b5\u2217\u2212\u00b5i \u2265 lcb\u2217(t\u22121)\u2212\u00b5\u0302i(t\u22121)\u2212widthi(t\u22121)/2 \u2265 (Cgap\u22121/2)widthi(t\u22121) > 0. Thus i 6= i\u2217 and i\u2217 \u2208 A(t).\nThis also shows that when arm i is evicted, \u2206\u0303i = Cgap \u00b7widthi(t\u22121) \u2264 Cgap/(Cgap\u22121/2)\u2206i, which is statement (e).\nTo get a bound on Ti(1, nB,i \u2212 1), we show that \u2206\u0303i = Cgap \u00b7widthi(nB,i \u2212 1) cannot be too small.\nLemma 8 With probability 1\u2212O (\u03b4) it holds for all times t and all arms i \u2208 A(t) with Ti(t\u22121) \u2265 Cinit log(n/\u03b4), that\nCgap \u00b7 widthi(t\u2212 1) \u2265 \u2206i/2.\nThe argument behind the lemma is that if i \u2208 A(t) then Cgap \u00b7widthi(t\u22121) \u2265 lcb\u2217(t\u22121)\u2212\u00b5\u0302i(t\u22121) where lcb\u2217(t \u2212 1) is sufficiently close to \u00b5\u2217 and \u00b5\u0302i(t \u2212 1) is sufficiently close to \u00b5i. The proof is given in Appendix E.2.\nSince i \u2208 A(nB,i \u2212 1), we get from Lemma 8 that with probability 1\u2212O (\u03b4),\nTi(nB,i \u2212 1) \u2264 Ti(nB,i \u2212 2) + 1 = Cw log(n/\u03b4)\n[widthi(nB,i \u2212 2)]2 + 1 \u2264\n4CwC 2 gap log(n/\u03b4)\n\u22062i + 1.\nTogether with Lemma 5c we have with probability 1\u2212O (\u03b4) that for all arms,\nTi(nS) \u2264 101\n100 L0iM/K +\n4CwC 2 gap log(n/\u03b4)\n\u22062i + 1 = O\n(\nlog(n/\u03b4)\n\u22062i\n)\n. (10)\nFinally, we need to bound the probability the SAPO switches to EXP3.P. Switching in Step 1.a is already handled by Lemma 7c. Switching in Step 1.b is also unlikely, since it would mean that the algorithm has accumulated large regret. This contradicts the upper bound (10). Lemma 17 shows that SAPO switches in Step 1.b only with probability 1\u2212O (\u03b4).\nThe difficult part, though, is to show that the condition in Step 4.a is not triggered too often such that Step 4.c switches to EXP3.P. We first calculate the false positive rate qsto, the probability that during a given phase the condition in Step 4.a is triggered. The false positive rate is again a constant but small, qsto \u2264 0.21, see Lemma 18.\nNow for a fixed arm the probability that in exactly E \u2265 E0 out of at most M phases the condition in Step 4.a is triggered, is at most\n(M E ) qsto E . We set p = qsto/(1 + qsto) and use a tail\nbound for the binomial distribution to sum over E = E0, . . . ,M :\nM \u2211\nE=E0\n(\nM\nE\n)\nqsto E = (1 + qsto)\nM M \u2211\nE=E0\n(\nM\nE\n)\npE(1\u2212 p)M\u2212E\n\u2264 (1 + qsto)M exp { \u2212M \u00b7D(E0/M ||p) }\nwhere D(a||p) = a log ap + (1 \u2212 a) log 1\u2212a1\u2212p is the relative entropy. Since E 0 M \u2265 CE2CE+1/ log 2 , this sum is O (\u03b4/n) and a union bound over the arms completes the proof."}, {"heading": "Acknowledgments", "text": "We thank the anonymous reviewers for their very valuable comments. The research leading to these results has received funding from the European Community\u2019s Seventh Framework Programme (FP7/2007-2013) under grant agreement n\u25e6 231495 (CompLACS) and from the Austrian Science Fund (FWF) under contract P 26219-N15."}, {"heading": "Appendix A. Concentration inequalities", "text": "Lemma 9 ((McDiarmid, 1998, Theorem 3.15)) Let Y1, . . . , YN be a martingale difference sequence with SN = Y1 + . . . + YN with the corresponding filtration F0 \u2286 F1 \u2286 . . . \u2286 FN . Let Yi \u2264 b and \u2211N i=1 E [ Y 2i |Fi\u22121 ]\n\u2264 V . Then for any z \u2265 0, P {SN \u2265 z} \u2264 exp ( \u2212z2/(2V + 2bz/3) ) .\nLemma 10 ((McDiarmid, 1998, Theorem 3.13)) Let Y1, . . . , YN be a martingale difference sequence with ak \u2264 Yk \u2264 bk for suitable constants ak, bk. Then for any z \u2265 0,\nP\n{\nmax 1\u2264m\u2264N\nm \u2211\nk=1\nYk \u2265 z } \u2264 exp ( \u22122z2 / N \u2211\nk=1\n(bk \u2212 ak)2 ) .\nCorollary 11 Let Y1, . . . , YN be a martingale difference sequence with ak \u2264 Yk \u2264 bk for suitable constants ak, bk. Then for any z \u2265 0,\nP\n{\nmax 1\u2264s\u2264t\u2264N\nt \u2211\nk=s\nYk \u2265 z } \u2264 2 exp ( \u2212z2 / 2 \u2211N\nk=1 (bk \u2212 ak)2\n)\n.\nProof\nP\n{\nmax 1\u2264s\u2264t\u2264N\nt \u2211\nk=s\nYk \u2265 z } \u2264 P {\nmax 1\u2264t\u2264N\nt \u2211\nk=1\nYk \u2265 z/2 } + P {\nmax 1\u2264s\u2264N\ns\u22121 \u2211\nk=1\n(\u2212Yk) \u2265 z/2 }\n\u2264 2 exp ( \u2212z2 / 2 \u2211N\nk=1 (bk \u2212 ak)2\n)\n."}, {"heading": "Appendix B. Proof of the lower bound (Theorem 2)", "text": "Let \u2206 = 1/8. We consider a stochastic bandit problem with constant reward x1(t) = 1/2 for arm 1 and Bernoulli rewards with \u00b52 = 1/2 \u2212 \u2206 for arm 2. We divide the time steps into phases of increasing length Lj = 3j\u230an\u03b1\u230b, j = 0, . . . , J \u2212 1 with J \u2265 1\u2212\u03b1log 3 log n and an incomplete last phase j = J . Since the pseudo-regret of the player is at most C(log n)\u03b2 , there is a phase j\u2217 < J where the expected number of plays of arm 2 in this phase is at most B with\nB = 8C log 3\n1\u2212 \u03b1 (log n) \u03b2\u22121.\nWe construct an adversarial bandit problem by modifying the Bernoulli distribution of arm 2. Before phase j\u2217 the distribution remains unchanged with \u00b52 = 1/2\u2212\u2206, but in phase j\u2217 and beyond we set \u00b52 = 1/2+\u2206. Since this bandit problem depends only on the player strategy (for identifying phase j\u2217) but not on the actual choices of the player, this adversary is oblivious.\nLet T j \u2217 2 be the number of plays of arm 2 in phase j \u2217, and let Padv {\u00b7} and Eadv [\u00b7] denote the probability and expectation in respect to this adversarial bandit problem. By Lemma 12 below we have\nPadv\n{\nT j \u2217 2 \u2264 4B } \u2265 1/(16n\u01eb).\nSince xIt(t) \u2212 Eadv [xIt(t)|Ht\u22121], t = 1, . . . , n, forms a martingale difference sequence, we can apply Azuma-Hoeffding\u2019s inequality (Lemma 10) and obtain\nPadv\n{\nn \u2211\nt=1\nxIt(t) \u2265 n \u2211\nt=1\nEadv [xIt|Ht\u22121] + \u221a 2n log n\n}\n\u2264 1/n2\nand\nPadv\n{\nT j \u2217 2 \u2264 4B \u2227 n \u2211\nt=1\nxIt(t) <\nn \u2211\nt=1\nEadv [xIt|Ht\u22121] + \u221a 2n log n\n}\n\u2265 1/(16n\u01eb)\u2212 1/n2. (11)\nBy the construction of the adversarial bandit problem, T j \u2217 2 \u2264 4B implies that n \u2211\nt=1\nEadv [xIt |Ht\u22121] \u2264 n/2 + 4B\u2206+ (n \u2212 tj\u2217)\u2206, (12)\nwhere tj\u2217 denotes the time step at the end of phase j\u2217. For arm 2 we have\nn \u2211\nt=1\nEadv [x2(t)] = n/2\u2212 j\u2217\u22121 \u2211\nj=0\nLj\u2206+ Lj\u2217\u2206+ (n \u2212 tj\u2217)\u2206\n= n/2 + \u230an\u03b1\u230b\u2206 ( 3j \u2217 \u2212 3 j\u2217 \u2212 1 2 ) + (n\u2212 tj\u2217)\u2206 \u2265 n/2 + \u230an\u03b1\u230b\u2206+ (n\u2212 tj\u2217)\u2206.\nApplying Azuma-Hoeffdings\u2019s inequality for arm 2 and combining with (11) and (12) we get\nPadv\n{\nn \u2211\nt=1\nx2(t)\u2212 n \u2211\nt=1\nxIt(t) \u2265 \u230an\u03b1\u230b\u2206 \u2212 4B\u2206\u2212 2 \u221a 2n log n\n}\n\u2265 1/(16n\u01eb)\u2212 2/n2.\nBy the condition on n, 4B\u2206 \u2264 (\u01eb log n)/(16\u2206) such that \u230an\u03b1\u230b\u2206\u2212 4B\u2206\u2212 2\u221a2n log n \u2265 n\u03b1/8\u2212 4 \u221a n log n, which completes the proof of the high probability lower bound. For the lower bound on the expected regret we construct an adaptive adversary by modifying the construction above: Let T j \u2217\n2 (t) be the number of plays of arm 2 in phase j \u2217 up to and including time\nstep t. If T j \u2217 2 = T j\u2217 2 (tj\u2217) \u2264 4B then the adversarial bandit problem above remains unmodified. If there is a time step t \u2264 tj\u2217 with T j \u2217\n2 (t) > 4B, then for all time steps > t we set again \u00b52 = 1/2\u2212\u2206. From the argument for the oblivious adversary we have\nE\n[\nn \u2211\nt=1\nx2(t)\u2212 n \u2211\nt=1\nxIt(t)\n\u2223 \u2223 \u2223 \u2223 \u2223 T j \u2217 2 \u2264 4B ] P { T j \u2217 2 \u2264 4B }\n\u2265 [ n\u03b1/8\u2212 4 \u221a n log n ] P\n{\nT j \u2217 2 \u2264 4B \u2227 n \u2211\nt=1\nx2(t)\u2212 n \u2211\nt=1\nxIt(t) \u2265 n\u03b1/8\u2212 4 \u221a n log n\n}\n\u2212n \u00b7 P { T j \u2217 2 \u2264 4B \u2227 n \u2211\nt=1\nx2(t)\u2212 n \u2211\nt=1\nxIt(t) < n \u03b1/8\u2212 4\n\u221a\nn log n\n}\n\u2265 [ n\u03b1/8\u2212 4 \u221a n log n ] [ 1/(16n\u01eb)\u2212 2/n2 ] \u2212 2/n\n\u2265 [ n\u03b1/8\u2212 4 \u221a n log n ] 1\n(16n\u01eb) \u2212 3.\nAnalogously we get\nE\n[\nn \u2211\nt=1\nx1(t)\u2212 n \u2211\nt=1\nxIt(t)\n\u2223 \u2223 \u2223 \u2223 \u2223 T j \u2217 2 > 4B ] P { T j \u2217 2 > 4B }\n\u2265 \u2212(4B + 1)\u2206 \u2212 \u221a 2n log n\n\u2212n \u00b7 P { T j \u2217 2 > 4B \u2227 n \u2211\nt=1\nx1(t)\u2212 n \u2211\nt=1\nxIt(t) < \u2212(4B + 1)\u2206 \u2212 \u221a 2n log n\n}\n\u2265 \u2212(4B + 1)\u2206 \u2212 \u221a 2n log n\u2212 1/n \u2265 \u22122 \u221a n log n.\nThus\nE\n[\nmax i\nn \u2211\nt=1\nxi(t)\u2212 n \u2211\nt=1\nxIt(t)\n]\n\u2265 [ n\u03b1/8\u2212 4 \u221a n log n ] 1\n(16n\u01eb) \u2212 3\u2212 2\n\u221a\nn log n\n\u2265 n \u03b1\u2212\u01eb\n128 \u2212 3\n\u221a\nn log n.\nLemma 12 For any n with (log n)2\u2212\u03b2 \u2265 64C log 3(1\u2212\u03b1)\u01eb ,\nPadv\n{\nT j \u2217 2 \u2264 4B } \u2265 1/(16n\u01eb).\nProof The proof follows a standard argument, e.g. (Mannor and Tsitsiklis, 2004). Let Psto {\u00b7} and Esto [\u00b7] denote the probability and expectation in respect to the stochastic bandit problem defined above. Since Esto [ T j \u2217\n2\n] \u2264 B we have Psto { T j \u2217 2 > 4B } < 1/4 and thus\nPsto\n{\nT j \u2217 2 \u2264 4B } > 3/4. (13)\nLet Gj \u2217 2 be the sum of rewards received when playing arm 2 in phase j \u2217. Conditioned on T j \u2217 2 , G j\u2217 2 is a binomial random variable with parameters T j \u2217\n2 and \u00b52. Hence by (Kaas and Buhrman, 1980),\nPsto\n{\nGj \u2217 2 \u2264 \u230aT j \u2217 2 (1/2 \u2212\u2206)\u230b } \u2264 1/2. (14)\nLet \u03c9 denote a particular realization of rewards xi(t), i \u2208 {1, 2}, 1 \u2264 t \u2264 tj\u2217 , and player choices I1, . . . , Itj\u2217 . For any realization \u03c9 the probabilities Psto {\u03c9} and Padv {\u03c9} are related by\nPadv {\u03c9} = Psto {\u03c9} (1/2 + \u2206)G\nj\u2217 2 (\u03c9)(1/2 \u2212\u2206)T j \u2217 2 (\u03c9)\u2212Gj \u2217 2 (\u03c9)\n(1/2 \u2212\u2206)Gj \u2217 2 (\u03c9)(1/2 + \u2206)T j\u2217 2 (\u03c9)\u2212Gj \u2217 2 (\u03c9)\n= Psto {\u03c9} ( 1\u2212 2\u2206 1 + 2\u2206\n)T j \u2217 2 (\u03c9)\u22122Gj \u2217 2 (\u03c9)\n.\nIf Gj \u2217 2 (\u03c9) \u2265 \u230a(1/2 \u2212\u2206)T j\u2217 2 (\u03c9)\u230b then\nPadv {\u03c9} \u2265 Psto {\u03c9} ( 1\u2212 2\u2206 1 + 2\u2206\n)T j \u2217 2 (\u03c9)\u22122((1/2\u2212\u2206)T j \u2217 2 (\u03c9)\u22121)\n= Psto {\u03c9} ( 1\u2212 2\u2206 1 + 2\u2206\n)2\u2206T j \u2217\n2 (\u03c9)+2\n.\nIf furthermore T j \u2217 2 (\u03c9) \u2264 4B, then\nPadv {\u03c9} \u2265 Psto {\u03c9} ( 1\u2212 2\u2206 1 + 2\u2206 )8\u2206B+2 .\nHence\nPadv\n{\nT j \u2217 2 \u2264 4B } \u2265 Padv { T j \u2217 2 \u2264 4B \u2227Gj \u2217 2 \u2265 \u230aT j \u2217 2 (1/2 \u2212\u2206)\u230b }\n\u2265 Psto { T j \u2217 2 \u2264 4B \u2227G j\u2217 2 \u2265 \u230aT j\u2217 2 (1/2 \u2212\u2206)\u230b }\n(\n1\u2212 2\u2206 1 + 2\u2206\n)8\u2206B+2\n[by (13) and (14)]\n\u2265 1 4\n(\n1\u2212 2\u2206 1 + 2\u2206\n)8\u2206B+2\n\u2265 1 4 (1\u2212 4\u2206)8\u2206B+2 [\u2206 = 1/8, 1\u2212 x \u2265 e\u22122x for 0 \u2264 x \u2264 1/2]\n\u2265 1 16 exp{\u221264\u22062B} \u2265 1 16n\u01eb\nfor (log n)2\u2212\u03b2 \u2265 64C log 3(1\u2212\u03b1)\u01eb ."}, {"heading": "Appendix C. Proof of Lemma 5", "text": "Proof of (b) We fix some arm i. By the condition in Step 4.c, Step 4.b can be executed for this arm at most E0 times. Let m be the number of executions of Step 4.d for arm i, such that the number of phases is at most m + E0 + 1 and the length of the longest phase is at least 2m\u2212E0\u22121 \u00b7 L0i . Then n \u2265 \u2211m+E0k=1 (\u03c4i,k+1\u2212\u03c4i,k) \u2265 2m\u2212E 0\u22121+2E0 and m \u2264 E0+\u230alog2(n\u22121)\u230b \u2264 E0+\u2308log2 n\u2309\u22121.\nProof of (c) We fix some arm i and use Bernstein\u2019s inequality (Lemma 9) with the martingale differences\nYt = I [It = i]\u2212 pi(t)\nfor nB,i \u2264 t \u2264 nS and Yt = 0 otherwise. Then Yj \u2264 1 and n \u2211\nt=1\nE[Y 2t |Ht\u22121] = n \u2211\nt=1\nE[Y 2t |pi(t)] \u2264 nS \u2211\nt=nB,i\npi(t).\nIn any testing phase k, pi(t) = L0i /(KLik) for \u03c4i,k \u2264 t < \u03c4i,k+1 \u2264 \u03c4i,k + Lik. Thus in each phase \u2211\u03c4i,k+1\u22121\nt=\u03c4i,k pi(t) \u2264 L0i /K and \u2211nS t=nB,i pi(t) \u2264 L0iM/K . Hence Bernstein\u2019s inequality gives\nP { Ti(nB,i, nS) \u2265 (1 + C)L0iM/K }\n\u2264 P { n \u2211\nt=1\nYt \u2265 CL0iM/K }\n\u2264 exp { \u2212C 2L0iM/K\n2 + 2C/3\n} \u2264 exp { \u22122C 2CpCE\n2 + 2C/3 log(n/\u03b4)\n}\n\u2264 \u03b4/n\nfor C \u2265 1/100. A union bound for i completes the proof."}, {"heading": "Appendix D. Proofs for SAPO against adversarial bandits", "text": "D.1. Proof of inequality (2)\nWe need to show that\nE\n\n\nnB,i\u22121 \u2211\nt=1\n[xi(t)\u2212 lcb\u2217(t)]\n\n = O ( \u221a Kn log(n/\u03b4) ) .\nBy the definition of \u00b5\u0304i(t) and by Step 1.a of SAPO we have by Wald\u2019s equation that\nE\n\n\nnB,i\u22121 \u2211\nt=1\nxi(t)\n\n = E [(nB,i \u2212 1) \u00b7 \u00b5\u0304i(nB,i \u2212 1)] \u2264 E [ (nB,i \u2212 1) \u00b7 ucbi(nB,i \u2212 1) ] .\nSince lcbi(t) \u2264 lcb\u2217(t) and ucbi(t) is non-increasing,\nE\n\n\nnB,i\u22121 \u2211\nt=1\n[xi(t)\u2212 lcb\u2217(t)]\n\n \u2264 E\n\n\nnB,i\u22121 \u2211\nt=1\n[ ucbi(t)\u2212 lcbi(t) ]\n\n\n\u2264 2E\n\n\nnB,i\u22121 \u2211\nt=1\nwidth(t)\n\n = 2E\n\n\nnB,i\u22121 \u2211\nt=1\n\u221a\n2CwK log(n/\u03b4)\nt\n\n \u2264 4 \u221a\n2CwKn log(n/\u03b4).\nD.2. Proof of inequality (3)\nLemma 13 We fix some phase k and s \u2265 \u03c4i,k. Let tC(s) = min{s \u2264 t < \u03c4i,k+1 : Di(s, t) > 2C4a\u2206\u0303iLik}. (15)\nIf no such t exists, we set tC(s) = \u03c4i,k+1 \u2212 1. Then\nP\n{ Di(s, tC(s)) > 2C4a\u2206\u0303iLik \u2227 D\u0302i(s, tC(s)) < C4a\u2206\u0303iLikpik \u2223 \u2223 \u2223 Hs\u22121 } \u2264 qadv := 1/25.\nProof We use Bernstein\u2019s inequality for martingales (Lemma 9) on the martingale differences\nYt = pik[xi(t)\u2212 \u00b5\u0303i]\u2212 I [It = i] [xi(t)\u2212 \u00b5\u0303i]\nfor s \u2264 t \u2264 tC(s) and Yj = 0 otherwise; with b = 1 and V = pikLik = L0i /K . We get\nP\n{ Di(s, tC(s)) > 2C4a\u2206\u0303iLik \u2227 D\u0302i(s, tC(s)) < C4a\u2206\u0303iLikpik \u2223 \u2223 \u2223Hs\u22121 }\n\u2264 P { pikDi(s, tC(s))\u2212 D\u0302i(s, tC(s)) > C4a\u2206\u0303iLikpik \u2223 \u2223 \u2223 Hs\u22121 }\n= P { pikDi(s, tC(s))\u2212 D\u0302i(s, tC(s)) > C4a\u2206\u0303iL0i /K \u2223 \u2223 \u2223 Hs\u22121 }\n\u2264 exp ( \u2212min{CpC24a/4, CpC4a/2} ) \u2264 1/25.\nLemma 14 Consider some phase k. Then\nPik\n{ Di(\u03c4i,k, \u03c4i,k+1 \u2212 1) \u2265 m(2C4a\u2206\u0303iLik + 1) } \u2264 qmadv.\nProof Since Di(s, t + 1) \u2212Di(s, t) \u2264 1, Di(\u03c4i,k, \u03c4i,k+1 \u2212 1) \u2265 m(2C4a\u2206\u0303iLik + 1) implies that there are time steps \u03c4i,k = s1 < s2 < \u00b7 \u00b7 \u00b7 < sm+1 \u2264 \u03c4i,k+1 with D(sj, sj+1 \u2212 2) \u2264 2C4a\u2206\u0303iLik and 2C4a\u2206\u0303iLik < D(sj, sj+1 \u2212 1) \u2264 2C4a\u2206\u0303iLik + 1. Furthermore, by the condition in Step 4.a, D\u0302i(sj , t) < C4a\u2206\u0303iLikpik for j = 1, . . . ,m and sj \u2264 t < \u03c4i,k+1 (otherwise the phase would have ended before \u03c4i,k+1). We define the event\nNDj = {sj+1 = tC(sj) + 1 \u2227 Di(sj , sj+1 \u2212 1) > 2C4a\u2206\u0303iLik \u2227 D\u0302i(sj , sj+1 \u2212 1) < C4a\u2206\u0303iLikpik}.\nThen\nPik\n{ Di(\u03c4i,k, \u03c4i,k+1 \u2212 1) \u2265 m(2C4a\u2206\u0303iLik + 1) } \u2264 Pik\n\n\n\nm \u2227\nj=1\nNDj\n\n\n\n=\nm \u220f\nj=1\nPik\n\n\n\nNDj\n\u2223 \u2223 \u2223 \u2223 \u2223 \u2223 j\u22121 \u2227\nj\u2032=1\nNDj\u2032\n\n\n\n\u2264 qmadv by Lemma 13.\nLemma 15 For any phase k,\nEik [Di(\u03c4i,k, \u03c4i,k+1 \u2212 1)] \u2264 2C4a\u2206\u0303iLik + 1\n1\u2212 qadv\nProof\nEik [Di(\u03c4i,k, \u03c4i,k+1 \u2212 1)] \u2264 (2C4a\u2206\u0303iLik + 1) \u2211\nm\u22650\nPik\n{ Di(\u03c4i,k, \u03c4i,k+1 \u2212 1) \u2265 m(2C4a\u2206\u0303iLik + 1) }\n\u2264 2C4a\u2206\u0303iLik + 1 1\u2212 qadv\nby Lemma 14."}, {"heading": "Appendix E. Proofs for SAPO against stochastic bandits", "text": "E.1. Proof of Lemma 7\nWe show that (a) and (b) hold with probability 1\u2212O (\u03b4). The other statements of the lemma follow from the events in (a) and (b).\nProof of (a) and (b) We fix some step t and some arm i, and condition on Ti(t) = T . Using Hoeffding-Azuma\u2019s inequality (Lemma 10) we find\nP {\u00b5\u0302i(t)\u2212 \u00b5i > widthi(t)/2|Ti(t) = T} \u2264 exp {\u2212Cw log(n/\u03b4)/2} \u2264 \u03b4/(16Kn2).\nAnalogously we bound \u00b5i \u2212 \u00b5\u0302i(t). A union bound over t, i, and T gives (b). Since i \u2208 A(t) implies pi(t) \u2265 1/K , Bernstein\u2019s inequality (Lemma 9) with b = K and V = Kt gives\nP { \u00b5\u0304i(t)\u2212 \u00b5i > width(t)/2 } \u2264 exp { \u2212Cw log(n/\u03b4) 4(2 + 2/3) } \u2264 \u03b4/(16Kn).\nUsing the same bound for \u00b5i \u2212 \u00b5\u0304i(t) and summing over t and i gives (a).\nE.2. Proof of Lemma 8\nLemma 16 With probability 1\u2212 O (\u03b4) the following holds for all time steps t and all arms i, i\u2032: If i\u2032 \u2208 A(t) and Ti(t) \u2265 Cinit log(n/\u03b4), then Ti\u2032(t) \u2265 Ti(t)/4.\nProof We fix t, i, and i\u2032. By the construction of SAPO we have P {It = i\u2032|Ht\u22121, i\u2032 \u2208 A(t)} \u2265 P {It = i|Ht\u22121, i\u2032 \u2208 A(t)}. From Is, . . . , It we select those with It\u2032\n1 , . . . , It\u2032 k \u2208 {i, i\u2032} and define a\nsuper-martingale with differences Yj = I [ It\u2032j = i ] \u2212 I [ It\u2032j = i \u2032 ] for t\u2032j \u2264 t and Yj = 0 for t\u2032j > t.\nThen\nP { Ti\u2032(t) < Ti(t)/4 \u2227 i\u2032 \u2208 A(t) \u2227 Ti(s, t) \u2265 Cinit log(n/\u03b4) }\n= P\n{\n3 8 [Ti\u2032(s, t) + Ti(s, t)] < 5 8 [Ti(s, t)\u2212 Ti\u2032(s, t)] \u2227 i\u2032 \u2208 A(t) \u2227 Ti(s, t) \u2265 Cinit log(n/\u03b4)\n}\n\u2264 \u2211\nk\u2265Cinit log(n/\u03b4)\nP\n{\n3 8 k < 5 8 [Ti(s, t)\u2212 Ti\u2032(s, t)] \u2227 i\u2032 \u2208 A(t) \u2227 Ti\u2032(s, t) + Ti(s, t) = k\n}\n\u2264 \u2211\nk\u2265Cinit log(n/\u03b4)\nP\n\n\n\n3k\n5 <\nk \u2211\nj=1\nYj\n\n\n\n\u2264 \u2211\nk\u2265Cinit log(n/\u03b4)\nexp\n{\n\u22129k 50\n} \u2264 exp { \u22129Cinit 50 log(n/\u03b4) }\n1\n1\u2212 exp{\u22129/50}\nby Hoeffding-Azuma\u2019s inequality (Lemma 10). A union bound for t, i, and i\u2032 completes the proof.\nProof of Lemma 8 Let arm i\u2217 be optimal, \u00b5i\u2217 = \u00b5\u2217, such that i\u2217 \u2208 A(t) by Lemma 7d. By Lemma 7b, with probability 1\u2212O (\u03b4) we have |\u00b5\u0302i(t\u22121)\u2212\u00b5i| \u2264 widthi(t\u22121)/2 for arms i and i\u2217. By construction, lcb\u2217(t\u22121) \u2265 lcbi\u2217(t \u2212 1) \u2265 \u00b5\u0302i\u2217(t \u2212 1) \u2212 widthi\u2217(t \u2212 1). By Lemma 16, with probability 1 \u2212 O (\u03b4) we have Ti\u2217(t\u2212 1) \u2265 Ti(t\u2212 1)/4. Then\n\u2206i = \u00b5 \u2217 \u2212 \u00b5i\n\u2264 \u00b5\u0302i\u2217(t\u2212 1) + widthi\u2217(t\u2212 1)/2 \u2212 \u00b5\u0302i(t\u2212 1) + widthi(t\u2212 1)/2 \u2264 lcb\u2217(t\u2212 1) + 3widthi\u2217(t\u2212 1)/2 \u2212 \u00b5\u0302i(t\u2212 1) + widthi(t\u2212 1)/2 \u2264 (Cgap + 3 + 1/2)widthi(t\u2212 1) \u2264 2Cgap \u00b7 widthi(t\u2212 1).\nE.3. Considering Step 1.b\nLemma 17 The probability that there is a time t with \u2211t\u22121 s=1[lcb \u2217(s) \u2212 xIs(s)] > C1b \u221a Kn log(n/\u03b4) is at most O (\u03b4).\nProof By Lemma 7d we have lcb\u2217(s) \u2264 \u00b5\u2217 for all s with probability 1\u2212O (\u03b4). Thus (10) implies that with probability 1\u2212O (\u03b4),\nt\u22121 \u2211\ns=1\n(lcb\u2217(s)\u2212 E [xIs(s)|Hs\u22121]) \u2264 t\u22121 \u2211\ns=1\n(\u00b5\u2217(s)\u2212 E [xIs(s)|Hs\u22121])\n=\nK \u2211\ni=1\n\u2206iTi(t\u2212 1) \u2264 K \u2211\ni=1\nC log(n/\u03b4) \u2206i \u2264 C C1b \u221a Kn log(n/\u03b4)\nfor C > 101100Cp(2CE+1)+4CwC 2 gap. By Hoeffding-Azuma\u2019s inequality (Lemma 10) we also have\nP\n{\nmax 1\u2264t\u2264n\nt\u22121 \u2211\ns=1\n(xIs(s)\u2212 E [xIs(s)|Hs\u22121]) \u2265 \u221a 2n log(n/\u03b4)\n}\n\u2264 (\u03b4/n) \u2264 3\u03b4/4.\nThus the lemma follows for C1b \u2265 522 which satisfies C1b \u2265 C/C1b + 1.\nE.4. Considering Step 4.a\nLemma 18 If the statements in Lemma 7 hold, then\nPik {The condition of Step 4.a is triggered for arm i in its phase k} \u2264 qsto := 0.21.\nProof The probability of triggering the condition in phase k is\nPik\n{\nmax \u03c4i,k\u2264s\u2264t<\u03c4i,k+1\nD\u0302(s, t) \u2265 C4a\u2206\u0303iLikpik } .\nWe first bound the number of plays in this round, Ti(\u03c4i,k, \u03c4i,k+1\u22121). Applying Bernstein\u2019s inequality (Lemma 9) with b = 1, V = Likpik, and z = Likpik we get\nPik {Ti(\u03c4i,k, \u03c4i,k+1 \u2212 1) \u2265 2Likpik} \u2264 exp ( \u2212 L 2 ikp 2 ik\n2Likpik + 2Likpik/3\n) \u2264 exp (\n\u22123Cp 8\u2206\u03032i\n)\n.\nBy Lemma 7b and Step 2.b, \u00b5i \u2212 \u00b5\u0303i \u2264 widthi(nB,i \u2212 1)/2 \u2264 \u2206\u0303i/(2Cgap). Conditioning on Ti(\u03c4i,k, \u03c4i,k+1 \u2212 1) < 2Likpik and applying Corollary 11 of Hoeffding-Azuma\u2019s inequality with\nz = C4a\u2206\u0303iLikpik \u2212 (\u00b5i \u2212 \u00b5\u0303i)2Likpik = (C4a \u2212 1/Cgap)\u2206\u0303iLikpik\nyields\nPik\n{\nmax \u03c4i,k\u2264s\u2264t<\u03c4i,k+1\nD\u0302(s, t) \u2265 C4a\u2206\u0303iLikpik \u2223 \u2223 \u2223\n\u2223\nTi(\u03c4i,k, \u03c4i,k+1 \u2212 1) < 2Likpik }\n\u2264 2 exp ( \u2212 ( C4a \u2212 1\nCgap\n)2 (\u2206\u0303iLikpik) 2\n4Likpik\n) \u2264 2 exp ( \u2212 ( C4a \u2212 1\nCgap\n)2 Cp 4\n)\n\u2264 0.21"}], "references": [{"title": "Finite-time analysis of the multiarmed bandit problem", "author": ["Peter Auer", "Nicol\u00f2 Cesa-Bianchi", "Paul Fischer"], "venue": "Machine Learning,", "citeRegEx": "Auer et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Auer et al\\.", "year": 2002}, {"title": "The nonstochastic multiarmed bandit problem", "author": ["Peter Auer", "Nicol\u00f2 Cesa-Bianchi", "Yoav Freund", "Robert E. Schapire"], "venue": "SIAM J. Comput.,", "citeRegEx": "Auer et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Auer et al\\.", "year": 2002}, {"title": "Regret analysis of stochastic and nonstochastic multiarmed bandit problems", "author": ["S\u00e9bastien Bubeck", "Nicol\u00f2 Cesa-Bianchi"], "venue": "Foundations and Trends in Machine Learning,", "citeRegEx": "Bubeck and Cesa.Bianchi.,? \\Q2012\\E", "shortCiteRegEx": "Bubeck and Cesa.Bianchi.", "year": 2012}, {"title": "The best of both worlds: Stochastic and adversarial bandits", "author": ["S\u00e9bastien Bubeck", "Aleksandrs Slivkins"], "venue": "In COLT - The 25th Annual Conference on Learning Theory,", "citeRegEx": "Bubeck and Slivkins.,? \\Q2012\\E", "shortCiteRegEx": "Bubeck and Slivkins.", "year": 2012}, {"title": "Mean, median and mode in binomial distributions", "author": ["R. Kaas", "J.M. Buhrman"], "venue": "Statistica Neerlandica,", "citeRegEx": "Kaas and Buhrman.,? \\Q1980\\E", "shortCiteRegEx": "Kaas and Buhrman.", "year": 1980}, {"title": "The sample complexity of exploration in the multi-armed bandit problem", "author": ["Shie Mannor", "John N. Tsitsiklis"], "venue": "JMLR, 5:623\u2013648,", "citeRegEx": "Mannor and Tsitsiklis.,? \\Q2004\\E", "shortCiteRegEx": "Mannor and Tsitsiklis.", "year": 2004}, {"title": "Concentration, volume 16 of Algorithms Combin., pages 195\u2013248", "author": ["Colin McDiarmid"], "venue": null, "citeRegEx": "McDiarmid.,? \\Q1998\\E", "shortCiteRegEx": "McDiarmid.", "year": 1998}, {"title": "Some aspects of the sequential design of experiments", "author": ["Herbert Robbins"], "venue": "Bull. Amer. Math. Soc.,", "citeRegEx": "Robbins.,? \\Q1952\\E", "shortCiteRegEx": "Robbins.", "year": 1952}, {"title": "One practical algorithm for both stochastic and adversarial bandits", "author": ["Yevgeny Seldin", "Aleksandrs Slivkins"], "venue": "In Proceedings of the 31th International Conference on Machine Learning,", "citeRegEx": "Seldin and Slivkins.,? \\Q2014\\E", "shortCiteRegEx": "Seldin and Slivkins.", "year": 2014}, {"title": "Concentration inequalities Lemma", "author": ["A. Appendix"], "venue": "((McDiarmid,", "citeRegEx": "Appendix,? \\Q1998\\E", "shortCiteRegEx": "Appendix", "year": 1998}], "referenceMentions": [{"referenceID": 7, "context": "In the original formulation (Robbins, 1952) it is assumed that the rewards are generated independently at random, governed by fixed but unknown probability distributions with means \u03bci for each arm i = 1, .", "startOffset": 28, "endOffset": 43}, {"referenceID": 1, "context": "This complements previous results of Bubeck and Slivkins (2012) that show \u00d5 ( \u221a n) expected adversarial regret with O ( (log n) ) stochastic pseudo-regret.", "startOffset": 37, "endOffset": 64}, {"referenceID": 2, "context": "An extensive overview of multi-armed bandit problems is given in (Bubeck and Cesa-Bianchi, 2012).", "startOffset": 65, "endOffset": 96}, {"referenceID": 0, "context": "Auer et al. (2002a): Rsto(n) = O (log n) .", "startOffset": 0, "endOffset": 20}, {"referenceID": 0, "context": "Auer et al. (2002a): Rsto(n) = O (log n) . Both of these bounds are known to be best possible. While the result for adversarial bandits is a worst-case \u2014 and thus possibly pessimistic \u2014 bound that holds for any sequence of rewards, the strong assumptions for stochastic bandits may sometimes be unjustified. Therefore an algorithm that can adapt to the actual difficulty of the problem is of great interest. The first such result was obtained by Bubeck and Slivkins (2012), who developed the SAO algorithm that with probability 1\u2212 \u03b4 achieves Radv(n) \u2264 O (", "startOffset": 0, "endOffset": 473}, {"referenceID": 3, "context": "This gives, together with the results of (Bubeck and Slivkins, 2012), a quite complete characterization of algorithms that perform well both for stochastic and adversarial bandit problems.", "startOffset": 41, "endOffset": 68}, {"referenceID": 3, "context": "Comparison with related work Bubeck and Slivkins (2012) show for their SAO algorithm that with probability 1\u2212 \u03b4, K \u2211", "startOffset": 29, "endOffset": 56}, {"referenceID": 8, "context": "A different approach is taken in (Seldin and Slivkins, 2014): here the starting point is an algorithm for adversarial bandit problems that is modified by adding an additional exploration parameter to achieve also low pseudo-regret in stochastic bandit problems.", "startOffset": 33, "endOffset": 60}], "year": 2016, "abstractText": "We present an algorithm that achieves almost optimal pseudo-regret bounds against adversarial and stochastic bandits. Against adversarial bandits the pseudo-regret is O (\u221a Kn logn ) and against stochastic bandits the regret is O ( \u2211 i (logn)/\u2206i). We also show that no algorithm with O (logn) pseudo-regret against stochastic bandits can achieve \u00d5 ( \u221a n) expected regret against adaptive adversarial bandits. This complements previous results of Bubeck and Slivkins (2012) that show \u00d5 ( \u221a n) expected adversarial regret with O ( (log n) ) stochastic pseudo-regret.", "creator": "LaTeX with hyperref package"}}}