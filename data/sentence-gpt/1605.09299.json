{"id": "1605.09299", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "30-May-2016", "title": "k2-means for fast and accurate large scale clustering", "abstract": "We propose k^2-means, a new clustering method which efficiently copes with large numbers of clusters and achieves low energy solutions. k^2-means builds upon the standard k-means (Lloyd's algorithm) and combines a new strategy to accelerate the convergence with a new low time complexity divisive initialization. The accelerated convergence is achieved through only looking at k_n nearest clusters and using triangle inequality bounds in the assignment step while the divisive initialization employs an optimal 2-clustering along a direction of 1.2 (Eckle's algorithm).\n\n\n\nThe Lloyd algorithm is well suited to the high energy density of clusters and provides the best support for low energy clusters, as well as low energy applications. It is also the best choice of clustering algorithms to solve the problem of long-run clustering. Lloyd's algorithm is based on three approaches:\nA cluster and a cluster:\nAn algorithm that can solve large-scale large-scale problems\nAn algorithm with a single key is easy to implement by creating a random set of points in the cluster. For instance, in an image, an image is the highest value, because it is stored in a small disk. If a larger image is stored in a small disk, the next high value will be the lowest value.\nThe Lloyd algorithm is supported by the following algorithms:\nA cluster and a cluster:\nAn algorithm with a single key is easy to implement by creating a random set of points in the cluster. For instance, in an image, an image is the highest value, because it is stored in a small disk. If a larger image is stored in a small disk, the next high value will be the lowest value. An algorithm with a single key is easy to implement by creating a random set of points in the cluster. For instance, in an image, an image is the highest value, because it is stored in a small disk. If a larger image is stored in a small disk, the next high value will be the lowest value. An algorithm with a single key is easy to implement by creating a random set of points in the cluster. For instance, in an image, an image is the highest value, because it is stored in a small disk. If a larger image is stored in a small disk, the next high value will be the lowest value. An algorithm with a single key is easy to implement by creating a random set of points in the cluster. For instance, in", "histories": [["v1", "Mon, 30 May 2016 16:17:45 GMT  (639kb,D)", "http://arxiv.org/abs/1605.09299v1", null]], "reviews": [], "SUBJECTS": "cs.LG cs.CV", "authors": ["eirikur agustsson", "radu timofte", "luc van gool"], "accepted": false, "id": "1605.09299"}, "pdf": {"name": "1605.09299.pdf", "metadata": {"source": "CRF", "title": "k-means for fast and accurate large scale clustering", "authors": ["Eirikur Agustsson", "Radu Timofte"], "emails": ["aeirikur@vision.ee.ethz.ch", "timofter@vision.ee.ethz.ch", "vangool@vision.ee.ethz.ch"], "sections": [{"heading": "1 Introduction", "text": "The k-means algorithm in its standard form (Lloyd\u2019s algorithm) employs two steps to cluster n data points of d dimensions and k initial cluster centers [18]. The expectation or assignment step assigns each point to its nearest cluster while the maximization or update step updates the k cluster centers with the mean of the points belonging to each cluster. The k-means algorithm repeats the two steps until convergence, that is the assignments no longer change in an iteration i.\nk-means is one of the most widely used clustering algorithms, being included in a list of top 10 data mining algorithms [25]. Its simplicity and general applicability vouch for its broad adoption. Unfortunately, its O(ndki) time complexity depends on the product between number of points n, number of dimensions d, number of clusters k, and number of iterations i. Thus, for large such values even a single iteration of the algorithm is very slow.\nThe simplest way to handle larger datasets is parallelization [27, 26], however this requires more computation power as well. Another way is to process the data online in batches as done by the MiniBatch algorithm of Sculley [21], a variant of the Lloyd algorithm that trades off quality (i.e. the converged energy) for speed.\nar X\niv :1\n60 5.\n09 29\n9v 1\n[ cs\n.L G\n] 3\nTo improve both the speed and the quality of the clustering results, Arthur and Vassilvitskii [1] proposed the k-means++ initialization method. The initialization typically results in a higher quality clustering and fewer iterations for k-means, than when using the default random initialization. Furthermore, the expected value of the clustering energy is within a 8(ln k + 2) factor of the optimal solution. However, the time complexity of the method is O(ndk), i.e. the same as a single iteration of the Lloyd algorithm - which can be too expensive in a large scale setting. Since k-means++ is sequential in nature, Bahman et al. [2] introduced a parallel version k-means|| of k-means++, but did not reduce the time complexity of the method.\nAnother direction is to speed up the actual k-means iterations. Elkan [7], Hamerly [10] and Drake and Hamerly [6] go in this direction and use the triangle inequality to avoid unnecessary distance computation between cluster centers and the data points. However, these methods still require a full Lloyd iteration in the beginning to then gradually reduce the computation of progressive iterations. The recent Yinyang k-means method of Ding et al. [5] is a similar method, that also leverages bounds to avoid redundant distance calculations. While typically performing 2-3\u00d7 faster than Elkan method, it also requires a full Lloyd iteration to start with.\nPhilbin et al. [20] introduce an approximate k-means (AKM) method based on kd-trees to speed up the assignment step, reducing the complexity of each k-means iteration from O(nkd) to O(nmd), where m < k. In this case m, the distance computations performed per each iteration, controls the trade-off between a fast and an accurate (i.e. low energy) clustering. Wang et al. [24] use cluster closures for further 2.5\u00d7 speedups. In this paper we propose k2-means, a method aiming at both fast and accurate clustering. Following the observation that usually the clusters change gradually and affect only local neighborhoods, in the assignment step we only consider the kn nearest neighbours of a center as the candidates for the clusters members. Furthermore we employ the triangle inequality bounds idea as introduced by Elkan [7] to reduce the number of operations per each iteration. For initializing k2-means, we propose a divisive initialization method, which we experimentally prove to be more efficient than k-means++.\nOur k2-means gives a significant algorithmic speedup, i.e. reducing the complexity to O(nknd) per iteration, while still maintaining a high accuracy comparable to methods such as k-means++ for a chosen kn < k. Similar to m in AKM, kn also controls a trade-off between speed and accuracy. However, our experiments show that we can use a significantly lower kn when aiming for a high accuracy.\nThe paper is structured as follows. In Table 1 we summarize the notations used in this paper. In Section 2 we introduce our proposed k2-means method and our divisive initialization. In Section 3 we describe the experimental benchmark and discuss the results obtained, while in Section 4 we draw conclusions.\nAlgorithm 1 k2-means 1: Given: k, data X , neighbourhood size kn 2: Initialize centers C 3: Initialize assignments a : {1 \u00b7 \u00b7 \u00b7n} \u2192 {1, \u00b7 \u00b7 \u00b7 , k} .\n4: while Not converged do 5: Build kn-NN graph of C: 6: Nkn : C \u2192 {1 \u00b7 \u00b7 \u00b7 k}kn 7: for x \u2208 X do 8: Get current center for x: 9: l\u2190 a(x)\n10: Assign x to nearest candidate center: 11: a(x)\u2190 argminl\u2032\u2208Nkn (cl) \u2016x\u2212 cl\u2032\u2016 12: end for 13: for j \u2208 {1 \u00b7 \u00b7 \u00b7 k} do 14: cj \u2190 \u00b5(Xj) {Update center} 15: end for 16: end while 17: return C, a\n2 Proposed k2-means\nIn this section we introduce our k2-means method and motivate the design decisions. The pseudocode of the method is given in Algorithm 1.\nGiven some data X = (xi)ni=1, xi \u2208 Rd, the k-means clustering objective is to find cluster centers C = (cj) k j=1, cj \u2208 Rd and cluster assignments a : {1, \u00b7 \u00b7 \u00b7 , n} \u2192 {1, \u00b7 \u00b7 \u00b7 , k}, such that the cluster energy k\u2211\nj=1 \u2211 x\u2208Xj \u2016x\u2212 cj\u20162 (1)\nis minimized, where Xj := (xi \u2208 X|a(i) = j) denotes the points assigned to a cluster j. For a data point xi, we sometimes write a(xi) instead of a(i) for the cluster assignment. Similarly, for a subset X \u2032 of the data, a(X \u2032) denotes the cluster assignments of the corresponding points.\nStandard Lloyd obtains an approximate solution by repeating the following until convergence: i) In the assignment step, each x is assigned to the nearest center in C. ii) For the update step, each center is recomputed as the mean of its members.\nThe assignment step requires O(nk) distance computations, i.e. O(nkd) operations, and dominates the time complexity of each iteration. The update step requires only O(nd) operations for mean computations.\nTo speed up the assignment step, an approximate nearest neighbour method can be used, such as kd-trees [20, 19] or locality sensitive hashing [12]. However, these methods ignore the fact that the cluster centers are moving across iterations and often this movement is slow, affecting a small neighborhood of points. With this observation, we obtain a very simple fast nearest neighbour scheme:\nSuppose at iteration i, a data point x was assigned to a nearby center, l = a(x). After updating the centers, we still expect cl to be close to x. Therefore, the centers nearby cl are likely candidates for the nearest center of x in iteration i + 1. To speed up the assignment step, we thus only consider the kn nearest neighbours of cl, Nkn(cl), as candidate centers for the points x \u2208 Xl. Since for each point we only consider kn centers in the assignment step (in line 11 of Algorithm 1), the complexity is reduced to O(nknd). In practice, we can set kn k. We also use inequalities as in [7] to avoid redundant distance computations in the assignment step (in line 11 of Algorithm 1). Note that we maintain only nkn lower bounds, for the neighbourhood of each point, instead of nk for the Elkan method. We refer to the original Elkan paper [7] for a detailed discussion on triangle inequalities and bounds.\nAs for standard Lloyd, the total energy can only decrease, both in the assignment step (since the points are moved to closer centers) and in the update step. Thus, the total energy is monotonically decreasing which guarantees convergence.\nAs shown by Arthur and Vassilvitskii [1], a good initialization, such as k-means++, often leads to a higher quality clustering compared to random sampling. Since the O(ndk) complexity of k-means++ would negate the benefits of the k2-means computation savings, we propose an alternative fast initialization scheme, which also leads to high quality clustering solutions."}, {"heading": "2.1 Greedy Divisive Initialization (GDI)", "text": "For the initialization of our k2-means, we propose a simple hierarchical clustering method named Greedy Divisive Initialization (GDI), detailed in Algorithm 2. Similarly to other divisive clustering methods, such as [4, 22], we start with a single cluster and repeatedly split the highest energy cluster until we reach k clusters.\nTo efficiently split each cluster, we use Projective Split (Algorithm 3), a variant of k-means with k = 2, that is motivated by the following observation: Suppose we have pointsX \u2032 and centers (c1, c2) in the k-means method. Let H be the hyperplane with normal vector c2 \u2212 c1, going through \u00b5(c1, c2) (see e.g. the top left corner of Figure 1). When we perform the standard k-means assignment step, we greedily assign each point to its closest centroid to get a solution with a lower energy, thus assigning the points on one side of H to c1, and the other side of H to c2.\nAlthough this is the best assignment choice for the current centers c1 and c2, this may not be a good split of the data. Therefore, we depart from the standard assignment step and consider instead all hyperplanes along the direction c2 \u2212 c1 (i.e. with normal vector c2 \u2212 c1). We project X \u2032 onto c2 \u2212 c1 and \u201cscan\u201d a hyperplane through the data to find the split that gives the lowest energy (lines 4-8 in Algorithm 3). To efficiently recompute the energy of the cluster splits as the hyperplane is scanned, we use the following Lemma: Lemma 1. [13][Lemma 2.1] Let S be a set of points with mean \u00b5(S). Then for any point z \u2208 Rd\u2211\nx\u2208S \u2016x\u2212 z\u20162 = \u2211 x\u2208S \u2016x\u2212 \u00b5(S)\u20162 + |S|\u2016z \u2212 \u00b5(S)\u20162 (2)\nWe can now compute \u03c6(S \u222a {y}) = \u2211\nx\u2208S\u222a{y}\n\u2016x\u2212 \u00b5(S \u222a {y})\u20162 (3)\n= \u2211 x\u2208S \u2016x\u2212 \u00b5(S \u222a {y})\u20162 + \u2016y \u2212 \u00b5(S \u222a {y})\u20162 (4)\n= \u03c6(S) + |S|\u2016\u00b5(S \u222a {y})\u2212 \u00b5(S)\u20162 + \u2016y \u2212 \u00b5(S \u222a {y})\u20162, (5) where we used Lemma 1 in (4). Equipped with (5) we can efficiently update energy terms in line 8 in Algorithm 3 as we scan the hyperplane through the data Xj , using in total only O(|Xj |) distance computations and mean updates. Note that \u00b5(S \u222a {y}) is easily computed with an add operation as (|S|\u00b5(S) + y)/(|S|+ 1).\nAlgorithm 2 Greedy Divisive Initialization (GDI)\n1: Given: k, data X 2: Assign all points to one cluster 3: C = {\u00b5(X)}, a(X) = 1 4: while |C| < k do 5: Pick highest energy cluster: 6: j \u2190 argmaxl \u03c6(Xl) 7: Split the cluster: 8: Xa, ca, Xb, cb \u2190 ProjectiveSplit(Xj) 9: cj \u2190 ca\n10: c|C|+1 \u2190 cb 11: a(Xb)\u2190 |C|+ 1 12: C \u2190 C \u222a {c|C|+1} 13: end while 14: return C, a\nAlgorithm 3 Projective Split 1: Given:data Xj = (xi) nj i=1\n2: Pick two random samples ca, cb from Xj 3: while Not Converged do 4: Sort Xj along ca \u2212 cb: 5: Pj \u2190 (xi \u00b7 (ca \u2212 cb)|xi \u2208 Xj) 6: X\u0303j \u2190 Xj sorted by Pj 7: Find minimum-energy split: 8: lmin = argminl \u03c6((x\u0303i) l i=1) + \u03c6((x\u0303i) nj i=l+1)\n9: Xa \u2190 (x\u0303i)lmini=1 10: Xb \u2190 (x\u0303i)\nnj i=lmin+1\n11: ca, cb \u2190 \u00b5(Xa), \u00b5(Xb) 12: end while 13: return Xa, ca, Xb, cb\nCompared to standard k-means with k = 2, our Projective Split takes the optimal split along the direction c2 \u2212 c1 but greedily considers only this direction. In Figure 1 we show how this can lead to a faster convergence."}, {"heading": "2.2 Time Complexity", "text": "Table 2 shows the time and memory complexity of Lloyd, Elkan, MiniBatch, AKM, and our k2-means.\nThe time complexity of each k2-means iteration is dominated by two factors: building the nearest neighbour graph of C (line 6), which costs O(k2) distance computations, as well as computing distances between points and candidate centers (line 11), which initially costs nkn distance computations. Elkan and k2-means use the triangle inequality to avoid redundant distance calculations and empirically we observe the O(nkd) and O(nknd) terms (respectively) gradually reduce down to O(nd) at convergence.\nIn MiniBatch k-means processes only b samples per iteration (with b n) but needs more iterations for convergence. AKM limits the number of distance computations to m per iteration, giving a complexity of O(nmd).\nTable 3 shows the time and memory complexity of random, k-means++ and our GDI initialization. For the GDI, the time complexity is dominated by calls to Projective Split. If we limit Projective Split to maximum O(1) iterations (2 in our experiments) then a call to ProjectiveSplit(Xj) costs O(|Xj |) distance computations and vector additions, O(|Xj |) inner products and O(|Xj | log |Xj |) comparisons (for the sort), giving in total O(|Xj |(log |Xj |+ d)) complexity. However, the resulting time complexity of GDI depends on the data.\nFor pathological datasets, it could happen for each call to ProjectiveSplit(X \u2032), that the minimum split is of the form {y}, X \u2032 \\ {y}, i.e. only one point y is split off. In this case, for |X| = n, the total complexity will be O(n(log n+ d) + (n\u2212 1)(log(n\u2212 1) + d) + \u00b7 \u00b7 \u00b7+ (n\u2212 k)(log(n\u2212 k) + d)) = O(nk(d+ log n)). 1\nA more reasonable case is when at each call ProjectiveSplit(X \u2032) splits each cluster into two similarly large clusters, i.e. the minimum split is of the form (X \u2032a, X \u2032 b) where |Xa| \u2248 |Xb|. In this case the worst case scenario is when in each split the highest energy cluster is the largest cluster (in no. of samples), resulting a total complexity of O(n log k(d+ log n)). 2 Therefore the time complexity of GDI is somewhere between O(n log k(d+ log n)) \u223c O(n(d+ log n)k). In our experiments we count vector operations for simplicity (i.e. dropping the O(d) factor), as detailed in the next section. To fairly account for the O(|Xj | log |Xj |) complexity of the sorting step in ProjectiveSplit, we artificially count it as |Xj | log2(|Xj |)/d vector operations."}, {"heading": "3 Experiments", "text": "For a fair comparison between methods implemented in various programming languages, we use the number of vector operations as a measure of complexity, i.e. distances, inner products and\n1 A simple example of such a pathological dataset is X = (xi)ni=1 \u2282 R where x1 = 0, x2 = 1, x3 = \u03c6(x1, x2), x4 = \u03c6(x1, x2, x3) and xn = \u03c6(x1, \u00b7 \u00b7 \u00b7 , xn). The size of xn grows extremely fast though, e.g. x10 \u2248 1581397605569 and x14 has 195 digits.\n2If we split all clusters of approximately equal size simultaneously, we need O(log k) passes and perform O(n(d+ logn)) computations in each pass.\nadditions. While the operations all share an O(d) complexity, the distance computations are most expensive accounting for the constant factor. However, since the runtime of all methods is dominated by distance computations (i.e. more than 95% of the runtime), for simplicity we count all vector operations equally and refer to them as \u201cdistance computations\u201d, using the terminology from [7]."}, {"heading": "3.1 Datasets", "text": "In our experiments we use datasets with 2414-150000 samples ranging from 50 to 32256 dimensions as listed in Table 5. The datasets are diverse in content and feature representation.\nTo create cnnvoc we extract 4096-dimensional CNN features [15] for 15662 bounding boxes, each belonging to 20 object categories, from PASCAL VOC 2007 [8] dataset. covtype uses the first 150000 entries of the Covertype dataset [3] of cartographic features. From the mnist database [16] of handwritten digits we also generate mnist50 by random projection of the raw pixels to a 50- dimensional subspace. For tinygist10k we use the first 10000 images with extracted gist features from the 80 million tiny images dataset [23]. cifar represents 50000 training images from the CIFAR [14] dataset. usps [11] has scans of handwritten digits (raw pixels) from envelopes. yale contains cropped face images from the Extended Yale B Database [9, 17]."}, {"heading": "3.2 Methods", "text": "We compare our k2-means with relevant clustering methods: Lloyd (standard k-means), Elkan [7] (accelerated Lloyd), MiniBatch [21] (web-scale online clustering), and AKM [20] (efficient search structure).\nAside from our GDI initialization, we also use random initialization and k-means++ [1] in our experiments. For k-means++ we use the provided Matlab implementation. We Matlab implement MiniBatch k-means according to Algorithm 1 in [21] and use the provided codes for Elkan and AKM. Lloyd++ and Elkan++ combine k-means++ initialization with Lloyd and Elkan, respectively.\nWe run all methods, except MiniBatch, for a maximum of 100 iterations. For MiniBatch k-means we use b = 100 samples per batch and t = n/2 iterations. For the Projective Split, Algorithm 3, we perform only 2 iterations."}, {"heading": "3.3 Initializations", "text": "We compare k-means++, random and our GDI initialization by running 20 trials of k-means (Lloyd) clustering with k \u2208 {100, 200, 500} on the datasets (excluding cifar and tiny10k due to the prohibitive cost of standard Lloyd with a high number of clusters). Table 4 reports minimum and average cluster energy as well as the average number of distance computations, relative to k-means++, averaged over the settings of k (i.e. 20\u00d7 3 experiments for each row). Our GDI gives a (slightly) better average and minimum convergence energy than the other initializations, while its runtime complexity is an order of magnitude smaller than in the case of k-means++ initialization.\nThe corresponding expanded Table 7 of the supplementary materials shows that speedup of GDI over k-means++ improves as k grows, and at k = 500 is typically more than an order of magnitude. This makes GDI a good choice for the initialization of k2-means."}, {"heading": "3.4 Performance", "text": "Our goal is fast accurate clustering, where the cluster energy differs only slightly from Lloyd with a good initialization (such as k-means++) at convergence. Therefore, we measure the runtime complexity needed to achieve a clustering energy that is within 1% of the energy obtained with Lloyd++ at convergence. In the supplementary material we report on performance for more reference levels (0%, 0.5% and 2%).\nFor a given budget i.e. the maximum number of iterations and parameters such as m for AKM and kn for k2 means, it is not known beforehand how well the algorithms approximate the targeted Lloyd++ energy. For a fair comparison we use an oracle to select the best parameters and the number of iterations for each method, i.e. the ones that give the highest speedup but still reach the reference error. In practice, one can use a rule of thumb or progressively increase k, m and the number of iterations until a desired energy has been reached.\nTo measure performance we run AKM, Elkan++, Elkan, Lloyd++, Lloyd, MiniBatch, and k2-means with k \u2208 {50, 200, 1000} on various datasets, with 3 different seeds and report average speedups over Lloyd++ when the energy reached is within 1% from Lloyd++ at convergence in Table 5.\nEach method is stopped once it reaches the reference energy and for AKM and k2-means, we use the parameters m and kn from {3, 5, 10, 20, 30, 50, 100, 200} that give the highest speedup. Table 5 shows that for most settings, our k2-means has the highest algorithmic speedup at 1% error. It benefits the most when both the number of clusters and the number of points are large, e.g. for k = 200 at least 19\u00d7 speedup for all datasets with n \u2265 7000 samples. We do not reach the target energy for usps and yale with k = 1000, because kn was limited to 200.\nFigure 3 show the convergence curves corresponding to cifar and mnist50 entries in Table 5. On cifar the benefit of k2-means is clear since it reaches the reference error significantly faster than the other methods. On mnist50 k2-means is considerably faster than AKM for k = 1000 but AKM reaches the 1% reference faster for k = 50. We show more convergence curves in the supplementary material.\nIn all settings of Table 5, Elkan++ gives a consistent up to 8.5\u00d7 speedup (since it is an exact acceleration of Lloyd++). For some settings Elkan is faster than Elkan++ in reaching the desired accuracy. This is due to the faster initialization. MiniBatch fails in all but one case (mnist, k = 50) to reach the reference error of 1% and is thus not shown.\nFor accurate clustering, when the reference energy is the Lloyd++ convergence energy (i.e. 0% error), Table 6 shows that the speedups of k2-means are even higher. For this setting, the second fastest method is Elkan++, which is designed for accelerating the exact Lloyd++.\nIn the supplementary material we report extra results at more energy reference levels and also more plots showing the convergence of the compared methods on different datasets."}, {"heading": "4 Conclusions", "text": "We proposed k2-means, a simple yet efficient method ideally suited for fast and accurate large scale clustering (n > 10000, k > 100, d > 50). k2-means combines an efficient divisive initialization with a new method to speed up the k-means iterations by using the kn nearest clusters as the new set of candidate centers for the cluster members as well as triangle inequalities. The algorithmic complexity of our k2-means is sublinear in k for n k and experimentally shown to give a high accuracy on diverse datasets. For accurate clustering, k2-means requires an order of magnitude fewer computations than alternative methods such as the fast approximate k-means (AKM) clustering. Moreover, our efficient divisive initialization leads to comparable clustering energies and significantly lower runtimes than the k-means++ initialization under the same conditions."}], "references": [{"title": "k-means++: The advantages of careful seeding", "author": ["D. Arthur", "S. Vassilvitskii"], "venue": "Proceedings of the eighteenth annual ACM-SIAM symposium on Discrete algorithms, pages 1027\u20131035. Society for Industrial and Applied Mathematics", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2007}, {"title": "Scalable k-means++", "author": ["B. Bahmani", "B. Moseley", "A. Vattani", "R. Kumar", "S. Vassilvitskii"], "venue": "Proceedings of the VLDB Endowment, 5(7):622\u2013633", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2012}, {"title": "and C", "author": ["C. Blake", "E. Keogh"], "venue": "Merz. Uci repository of machine learning databases.", "citeRegEx": "3", "shortCiteRegEx": null, "year": 1998}, {"title": "Principal direction divisive partitioning", "author": ["D. Boley"], "venue": "Data mining and knowledge discovery, 2(4):325\u2013344", "citeRegEx": "4", "shortCiteRegEx": null, "year": 1998}, {"title": "Yinyang k-means: A drop-in replacement of the classic k-means with consistent speedup", "author": ["Y. Ding", "Y. Zhao", "X. Shen", "M. Musuvathi", "T. Mytkowicz"], "venue": "Proceedings of the 32nd International Conference on Machine Learning (ICML-15), pages 579\u2013587", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2015}, {"title": "Accelerated k-means with adaptive distance bounds", "author": ["J. Drake", "G. Hamerly"], "venue": "5th NIPS workshop on optimization for machine learning", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2012}, {"title": "Using the triangle inequality to accelerate k-means", "author": ["C. Elkan"], "venue": "ICML, volume 3, pages 147\u2013153", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2003}, {"title": "The pascal visual object classes (voc) challenge", "author": ["M. Everingham", "L. Van Gool", "C.K. Williams", "J. Winn", "A. Zisserman"], "venue": "International journal of computer vision, 88(2):303\u2013338", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2010}, {"title": "From few to many: Illumination cone models for face recognition under variable lighting and pose", "author": ["A. Georghiades", "P. Belhumeur", "D. Kriegman"], "venue": "IEEE Trans. Pattern Anal. Mach. Intelligence, 23(6):643\u2013660", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2001}, {"title": "Making k-means even faster", "author": ["G. Hamerly"], "venue": "SDM, pages 130\u2013140. SIAM", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2010}, {"title": "A database for handwritten text recognition research", "author": ["J.J. Hull"], "venue": "Pattern Analysis and Machine Intelligence, IEEE Transactions on, 16(5):550\u2013554", "citeRegEx": "11", "shortCiteRegEx": null, "year": 1994}, {"title": "Approximate nearest neighbors: towards removing the curse of dimensionality", "author": ["P. Indyk", "R. Motwani"], "venue": "Proceedings of the thirtieth annual ACM symposium on Theory of computing, pages 604\u2013613. ACM", "citeRegEx": "12", "shortCiteRegEx": null, "year": 1998}, {"title": "A local search approximation algorithm for k-means clustering", "author": ["T. Kanungo", "D.M. Mount", "N.S. Netanyahu", "C.D. Piatko", "R. Silverman", "A.Y. Wu"], "venue": "Proceedings of the eighteenth annual symposium on Computational geometry, pages 10\u201318. ACM", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2002}, {"title": "Learning multiple layers of features from tiny", "author": ["A. Krizhevsky", "G. Hinton"], "venue": null, "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2009}, {"title": "Imagenet classification with deep convolutional neural networks", "author": ["A. Krizhevsky", "I. Sutskever", "G.E. Hinton"], "venue": "Advances in neural information processing systems, pages 1097\u20131105", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2012}, {"title": "and C", "author": ["Y. LeCun", "C. Cortes"], "venue": "J. Burges. The mnist database of handwritten digits", "citeRegEx": "16", "shortCiteRegEx": null, "year": 1998}, {"title": "Acquiring linear subspaces for face recognition under variable lighting", "author": ["K. Lee", "J. Ho", "D. Kriegman"], "venue": "IEEE Trans. Pattern Anal. Mach. Intelligence, 27(5):684\u2013698", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2005}, {"title": "Least squares quantization in pcm", "author": ["S.P. Lloyd"], "venue": "Information Theory, IEEE Transactions on, 28(2):129\u2013137", "citeRegEx": "18", "shortCiteRegEx": null, "year": 1982}, {"title": "Fast approximate nearest neighbors with automatic algorithm configuration", "author": ["M. Muja", "D.G. Lowe"], "venue": "VISAPP (1), 2", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2009}, {"title": "Object retrieval with large vocabularies and fast spatial matching", "author": ["J. Philbin", "O. Chum", "M. Isard", "J. Sivic", "A. Zisserman"], "venue": "Computer Vision and Pattern Recognition, 2007. CVPR\u201907. IEEE Conference on, pages 1\u20138. IEEE", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2007}, {"title": "Web-scale k-means clustering", "author": ["D. Sculley"], "venue": "Proceedings of the 19th international conference on World wide web, pages 1177\u20131178. ACM", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2010}, {"title": "A deterministic method for initializing k-means clustering", "author": ["T. Su", "J. Dy"], "venue": "Tools with Artificial Intelligence, 2004. ICTAI 2004. 16th IEEE International Conference on, pages 784\u2013786. IEEE", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2004}, {"title": "80 million tiny images: A large data set for nonparametric object and scene recognition", "author": ["A. Torralba", "R. Fergus", "W.T. Freeman"], "venue": "Pattern Analysis and Machine Intelligence, IEEE Transactions on, 30(11):1958\u2013 1970", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2008}, {"title": "Fast approximate k-means via cluster closures", "author": ["J. Wang", "J. Wang", "Q. Ke", "G. Zeng", "S. Li"], "venue": "2012 IEEE Conference on Computer Vision and Pattern Recognition", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2012}, {"title": "et al", "author": ["X. Wu", "V. Kumar", "J.R. Quinlan", "J. Ghosh", "Q. Yang", "H. Motoda", "G.J. McLachlan", "A. Ng", "B. Liu", "S.Y. Philip"], "venue": "Top 10 algorithms in data mining. Knowledge and Information Systems, 14(1):1\u201337", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2008}, {"title": "Efficient k -means++ approximation with mapreduce", "author": ["Y. Xu", "W. Qu", "Z. Li", "G. Min", "K. Li", "Z. Liu"], "venue": "Parallel and Distributed Systems, IEEE Transactions on,", "citeRegEx": "26", "shortCiteRegEx": "26", "year": 2014}], "referenceMentions": [{"referenceID": 17, "context": "The k-means algorithm in its standard form (Lloyd\u2019s algorithm) employs two steps to cluster n data points of d dimensions and k initial cluster centers [18].", "startOffset": 152, "endOffset": 156}, {"referenceID": 24, "context": "k-means is one of the most widely used clustering algorithms, being included in a list of top 10 data mining algorithms [25].", "startOffset": 120, "endOffset": 124}, {"referenceID": 25, "context": "The simplest way to handle larger datasets is parallelization [27, 26], however this requires more computation power as well.", "startOffset": 62, "endOffset": 70}, {"referenceID": 20, "context": "Another way is to process the data online in batches as done by the MiniBatch algorithm of Sculley [21], a variant of the Lloyd algorithm that trades off quality (i.", "startOffset": 99, "endOffset": 103}, {"referenceID": 0, "context": "To improve both the speed and the quality of the clustering results, Arthur and Vassilvitskii [1] proposed the k-means++ initialization method.", "startOffset": 94, "endOffset": 97}, {"referenceID": 1, "context": "[2] introduced a parallel version k-means|| of k-means++, but did not reduce the time complexity of the method.", "startOffset": 0, "endOffset": 3}, {"referenceID": 6, "context": "Elkan [7], Hamerly [10] and Drake and Hamerly [6] go in this direction and use the triangle inequality to avoid unnecessary distance computation between cluster centers and the data points.", "startOffset": 6, "endOffset": 9}, {"referenceID": 9, "context": "Elkan [7], Hamerly [10] and Drake and Hamerly [6] go in this direction and use the triangle inequality to avoid unnecessary distance computation between cluster centers and the data points.", "startOffset": 19, "endOffset": 23}, {"referenceID": 5, "context": "Elkan [7], Hamerly [10] and Drake and Hamerly [6] go in this direction and use the triangle inequality to avoid unnecessary distance computation between cluster centers and the data points.", "startOffset": 46, "endOffset": 49}, {"referenceID": 4, "context": "[5] is a similar method, that also leverages bounds to avoid redundant distance calculations.", "startOffset": 0, "endOffset": 3}, {"referenceID": 19, "context": "[20] introduce an approximate k-means (AKM) method based on kd-trees to speed up the assignment step, reducing the complexity of each k-means iteration from O(nkd) to O(nmd), where m < k.", "startOffset": 0, "endOffset": 4}, {"referenceID": 23, "context": "[24] use cluster closures for further 2.", "startOffset": 0, "endOffset": 4}, {"referenceID": 6, "context": "Furthermore we employ the triangle inequality bounds idea as introduced by Elkan [7] to reduce the number of operations per each iteration.", "startOffset": 81, "endOffset": 84}, {"referenceID": 19, "context": "To speed up the assignment step, an approximate nearest neighbour method can be used, such as kd-trees [20, 19] or locality sensitive hashing [12].", "startOffset": 103, "endOffset": 111}, {"referenceID": 18, "context": "To speed up the assignment step, an approximate nearest neighbour method can be used, such as kd-trees [20, 19] or locality sensitive hashing [12].", "startOffset": 103, "endOffset": 111}, {"referenceID": 11, "context": "To speed up the assignment step, an approximate nearest neighbour method can be used, such as kd-trees [20, 19] or locality sensitive hashing [12].", "startOffset": 142, "endOffset": 146}, {"referenceID": 6, "context": "We also use inequalities as in [7] to avoid redundant distance computations in the assignment step (in line 11 of Algorithm 1).", "startOffset": 31, "endOffset": 34}, {"referenceID": 6, "context": "We refer to the original Elkan paper [7] for a detailed discussion on triangle inequalities and bounds.", "startOffset": 37, "endOffset": 40}, {"referenceID": 0, "context": "As shown by Arthur and Vassilvitskii [1], a good initialization, such as k-means++, often leads to a higher quality clustering compared to random sampling.", "startOffset": 37, "endOffset": 40}, {"referenceID": 3, "context": "Similarly to other divisive clustering methods, such as [4, 22], we start with a single cluster and repeatedly split the highest energy cluster until we reach k clusters.", "startOffset": 56, "endOffset": 63}, {"referenceID": 21, "context": "Similarly to other divisive clustering methods, such as [4, 22], we start with a single cluster and repeatedly split the highest energy cluster until we reach k clusters.", "startOffset": 56, "endOffset": 63}, {"referenceID": 12, "context": "[13][Lemma 2.", "startOffset": 0, "endOffset": 4}, {"referenceID": 6, "context": "Complexity Lloyd Elkan [7] MiniBatch [21] AKM [20] k-means (ours) Time O(nkd) O(nkd+ kd) \u223c O(nd+ kd) O(bkd) O(nmd) O(nknd+ kd) \u223c O(nd+ kd) Memory O((n+ k)d) O((n+ k)d+ nk + k) O((b+ k)d) O((n+ k)d) O((n+ k)d+ nkn + k) Table 2: Time and memory complexity per iteration for Lloyd, Elkan, MiniBatch, AKM and our k-means.", "startOffset": 23, "endOffset": 26}, {"referenceID": 20, "context": "Complexity Lloyd Elkan [7] MiniBatch [21] AKM [20] k-means (ours) Time O(nkd) O(nkd+ kd) \u223c O(nd+ kd) O(bkd) O(nmd) O(nknd+ kd) \u223c O(nd+ kd) Memory O((n+ k)d) O((n+ k)d+ nk + k) O((b+ k)d) O((n+ k)d) O((n+ k)d+ nkn + k) Table 2: Time and memory complexity per iteration for Lloyd, Elkan, MiniBatch, AKM and our k-means.", "startOffset": 37, "endOffset": 41}, {"referenceID": 19, "context": "Complexity Lloyd Elkan [7] MiniBatch [21] AKM [20] k-means (ours) Time O(nkd) O(nkd+ kd) \u223c O(nd+ kd) O(bkd) O(nmd) O(nknd+ kd) \u223c O(nd+ kd) Memory O((n+ k)d) O((n+ k)d+ nk + k) O((b+ k)d) O((n+ k)d) O((n+ k)d+ nkn + k) Table 2: Time and memory complexity per iteration for Lloyd, Elkan, MiniBatch, AKM and our k-means.", "startOffset": 46, "endOffset": 50}, {"referenceID": 6, "context": "more than 95% of the runtime), for simplicity we count all vector operations equally and refer to them as \u201cdistance computations\u201d, using the terminology from [7].", "startOffset": 158, "endOffset": 161}, {"referenceID": 14, "context": "To create cnnvoc we extract 4096-dimensional CNN features [15] for 15662 bounding boxes, each belonging to 20 object categories, from PASCAL VOC 2007 [8] dataset.", "startOffset": 58, "endOffset": 62}, {"referenceID": 7, "context": "To create cnnvoc we extract 4096-dimensional CNN features [15] for 15662 bounding boxes, each belonging to 20 object categories, from PASCAL VOC 2007 [8] dataset.", "startOffset": 150, "endOffset": 153}, {"referenceID": 2, "context": "covtype uses the first 150000 entries of the Covertype dataset [3] of cartographic features.", "startOffset": 63, "endOffset": 66}, {"referenceID": 15, "context": "From the mnist database [16] of handwritten digits we also generate mnist50 by random projection of the raw pixels to a 50dimensional subspace.", "startOffset": 24, "endOffset": 28}, {"referenceID": 22, "context": "For tinygist10k we use the first 10000 images with extracted gist features from the 80 million tiny images dataset [23].", "startOffset": 115, "endOffset": 119}, {"referenceID": 13, "context": "cifar represents 50000 training images from the CIFAR [14] dataset.", "startOffset": 54, "endOffset": 58}, {"referenceID": 10, "context": "usps [11] has scans of handwritten digits (raw pixels) from envelopes.", "startOffset": 5, "endOffset": 9}, {"referenceID": 8, "context": "yale contains cropped face images from the Extended Yale B Database [9, 17].", "startOffset": 68, "endOffset": 75}, {"referenceID": 16, "context": "yale contains cropped face images from the Extended Yale B Database [9, 17].", "startOffset": 68, "endOffset": 75}, {"referenceID": 6, "context": "We compare our k-means with relevant clustering methods: Lloyd (standard k-means), Elkan [7] (accelerated Lloyd), MiniBatch [21] (web-scale online clustering), and AKM [20] (efficient search structure).", "startOffset": 89, "endOffset": 92}, {"referenceID": 20, "context": "We compare our k-means with relevant clustering methods: Lloyd (standard k-means), Elkan [7] (accelerated Lloyd), MiniBatch [21] (web-scale online clustering), and AKM [20] (efficient search structure).", "startOffset": 124, "endOffset": 128}, {"referenceID": 19, "context": "We compare our k-means with relevant clustering methods: Lloyd (standard k-means), Elkan [7] (accelerated Lloyd), MiniBatch [21] (web-scale online clustering), and AKM [20] (efficient search structure).", "startOffset": 168, "endOffset": 172}, {"referenceID": 0, "context": "Aside from our GDI initialization, we also use random initialization and k-means++ [1] in our experiments.", "startOffset": 83, "endOffset": 86}, {"referenceID": 20, "context": "We Matlab implement MiniBatch k-means according to Algorithm 1 in [21] and use the provided codes for Elkan and AKM.", "startOffset": 66, "endOffset": 70}, {"referenceID": 0, "context": "[1] D.", "startOffset": 0, "endOffset": 3}, {"referenceID": 1, "context": "[2] B.", "startOffset": 0, "endOffset": 3}, {"referenceID": 2, "context": "[3] C.", "startOffset": 0, "endOffset": 3}, {"referenceID": 3, "context": "[4] D.", "startOffset": 0, "endOffset": 3}, {"referenceID": 4, "context": "[5] Y.", "startOffset": 0, "endOffset": 3}, {"referenceID": 5, "context": "[6] J.", "startOffset": 0, "endOffset": 3}, {"referenceID": 6, "context": "[7] C.", "startOffset": 0, "endOffset": 3}, {"referenceID": 7, "context": "[8] M.", "startOffset": 0, "endOffset": 3}, {"referenceID": 8, "context": "[9] A.", "startOffset": 0, "endOffset": 3}, {"referenceID": 9, "context": "[10] G.", "startOffset": 0, "endOffset": 4}, {"referenceID": 10, "context": "[11] J.", "startOffset": 0, "endOffset": 4}, {"referenceID": 11, "context": "[12] P.", "startOffset": 0, "endOffset": 4}, {"referenceID": 12, "context": "[13] T.", "startOffset": 0, "endOffset": 4}, {"referenceID": 13, "context": "[14] A.", "startOffset": 0, "endOffset": 4}, {"referenceID": 14, "context": "[15] A.", "startOffset": 0, "endOffset": 4}, {"referenceID": 15, "context": "[16] Y.", "startOffset": 0, "endOffset": 4}, {"referenceID": 16, "context": "[17] K.", "startOffset": 0, "endOffset": 4}, {"referenceID": 17, "context": "[18] S.", "startOffset": 0, "endOffset": 4}, {"referenceID": 18, "context": "[19] M.", "startOffset": 0, "endOffset": 4}, {"referenceID": 19, "context": "[20] J.", "startOffset": 0, "endOffset": 4}, {"referenceID": 20, "context": "[21] D.", "startOffset": 0, "endOffset": 4}, {"referenceID": 21, "context": "[22] T.", "startOffset": 0, "endOffset": 4}, {"referenceID": 22, "context": "[23] A.", "startOffset": 0, "endOffset": 4}, {"referenceID": 23, "context": "[24] J.", "startOffset": 0, "endOffset": 4}, {"referenceID": 24, "context": "[25] X.", "startOffset": 0, "endOffset": 4}, {"referenceID": 25, "context": "[26] Y.", "startOffset": 0, "endOffset": 4}], "year": 2016, "abstractText": "We propose k-means, a new clustering method which efficiently copes with large numbers of clusters and achieves low energy solutions. k-means builds upon the standard k-means (Lloyd\u2019s algorithm) and combines a new strategy to accelerate the convergence with a new low time complexity divisive initialization. The accelerated convergence is achieved through only looking at kn nearest clusters and using triangle inequality bounds in the assignment step while the divisive initialization employs an optimal 2-clustering along a direction. The worst-case time complexity per iteration of our k-means is O(nknd+ kd), where d is the dimension of the n data points and k is the number of clusters and usually n k kn. Compared to k-means\u2019 O(nkd) complexity, our k-means complexity is significantly lower, at the expense of slightly increasing the memory complexity by O(nkn + k). In our extensive experiments k-means is order(s) of magnitude faster than standard methods in computing accurate clusterings on several standard datasets and settings with hundreds of clusters and high dimensional data. Moreover, the proposed divisive initialization generally leads to clustering energies comparable to those achieved with the standard k-means++ initialization, while being significantly faster.", "creator": "LaTeX with hyperref package"}}}