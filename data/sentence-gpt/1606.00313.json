{"id": "1606.00313", "review": {"conference": "NIPS", "VERSION": "v1", "DATE_OF_SUBMISSION": "1-Jun-2016", "title": "Improved Regret Bounds for Oracle-Based Adversarial Contextual Bandits", "abstract": "We give an oracle-based algorithm for the adversarial contextual bandit problem, where either contexts are drawn i.i.d. or the sequence of contexts is known a priori, but where the losses are picked adversarially. Our algorithm is computationally efficient, assuming access to an offline optimization oracle, and enjoys a regret of order $O((KT)^{\\frac{2}{3}}(\\log N)^{\\frac{1}{3}})$, where $K$ is the number of actions, $T$ is the number of iterations and $N$ is the number of baseline policies. Our result is the first to break the $O(T^{\\frac{3}{4}})$ barrier that is achieved by recently introduced algorithms. Breaking this barrier was left as a major open problem. Our analysis is based on the recent relaxation based approach of (Rakhlin and Sridharan, 2016). The first steps are the first of the two algorithms, namely the previous algorithm, which attempts to overcome the resistance to adversarial context . The second algorithm uses an algorithm named T-f. This algorithm also generates the first rule of the algorithm. We also compare the results of all the recent relaxation based algorithms in the previous algorithms. The first steps are the second of the two algorithms, namely the first rule of the algorithm. We also compare the results of all the recent relaxation based algorithms in the previous algorithms. We also compare the results of all the recent relaxation based algorithms in the previous algorithms. We also compare the results of all the recent relaxation based algorithms in the previous algorithms.\n\n\n\n\n\nThe most important of the problems in the first version of this paper was the problems that are addressed by the first algorithm. We have defined the first algorithm as the first algorithm with the first algorithm that could be solved and the second algorithm with the first algorithm that could be solved and the third algorithm with the first algorithm that could be solved and the fourth algorithm with the first algorithm that could be solved and the fourth algorithm with the first algorithm that could be solved and the third algorithm with the first algorithm that could be solved and the third algorithm with the first algorithm that could be solved and the fourth algorithm with the first algorithm that could be solved and the fourth algorithm with the first algorithm that could be solved and the third algorithm with the first algorithm that could be solved and the fourth algorithm with the first algorithm that could be solved and the fourth algorithm with the first algorithm that could be solved and the third algorithm", "histories": [["v1", "Wed, 1 Jun 2016 14:47:19 GMT  (23kb)", "http://arxiv.org/abs/1606.00313v1", null]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["vasilis syrgkanis", "haipeng luo", "akshay krishnamurthy", "robert e schapire"], "accepted": true, "id": "1606.00313"}, "pdf": {"name": "1606.00313.pdf", "metadata": {"source": "CRF", "title": "Improved Regret Bounds for Oracle-Based Adversarial Contextual Bandits", "authors": ["Vasilis Syrgkanis", "Haipeng Luo", "Akshay Krishnamurthy"], "emails": [], "sections": [{"heading": null, "text": "ar X\niv :1\n60 6.\n00 31\n3v 1\n[ cs\n.L G\n] 1\na regret of order O((KT ) 2 3 (logN) 1 3 ), where K is the number of actions, T is the number of iterations and N is the number of baseline policies. Our result is the first to break the O(T 3\n4 ) barrier that is achieved by recently introduced algorithms. Breaking this barrier was left as a major open problem. Our analysis is based on the recent relaxation based approach of Rakhlin and Sridharan [7]."}, {"heading": "1 Introduction", "text": "We study online decision making problems where a learner chooses an action based on some side information (context) and incurs some cost for that action with a goal of incurring minimal cost over a sequence of rounds. These contextual online learning settings form a powerful framework for modeling many important decision-making scenarios with applications ranging from personalized health care in the medical domain to content recommendation and targeted advertising in internet applications. Many of these applications also involve a partial feedback component wherein costs for alternative actions are unobserved, and are typically modeled as contextual bandits.\nThe contextual information present in these problems enables learning of a much richer policy for choosing actions based on context. In the literature, the typical goal for the learner is to have cumulative cost that is not much higher then the best policy \u03c0 in a large set \u03a0. This is formalized by the notion of regret, which is the learner\u2019s cumulative cost minus the cumulative cost of the best fixed policy \u03c0 in hindsight. Achieving this goal requires learning a rich policy, depending on \u03a0.\nNaively one can view the contextual problem as a standard online learning problem where the set of possible \u201cactions\u201d available at each iteration is the set of policies. This perspective is fruitful, as classical algorithms, such as Hedge [5, 3] and Exp4 [2], give information theoretically optimal regret bounds of O( \u221a T log(|\u03a0|)) in full-information and O( \u221a\nTK log(|\u03a0|) in the bandit setting, where T is the number of rounds, K is the number of actions, and \u03a0 is the policy set. However, naively lifting standard online learning algorithms to the contextual setting leads to a running time that is linear in the number of policies. Given that the optimal regret is only logarithmic in |\u03a0| and that our high-level goal is to learn a very rich policy, we want to capture policy classes that are exponentially large. When we use a large policy class, existing algorithms are no longer computationally tractable.\nTo study this computational question, a number of recent papers have developed oracle-based algorithms that only access the policy class through an optimization oracle for the offline full-information problem. Oracle-based approaches harness the research in supervised learning that focuses on designing efficient algorithms for full-information problems and uses it for online and partial-feedback problems. Optimization oracles have been used in designing contextual bandit algorithms [1, 6, 4] that achieve the optimal\nO( \u221a KT log(|\u03a0|)) regret while also being computationally efficient (i.e. requiring poly(K, log(\u03a0), T ) oracle calls and computation). However, these results only apply when the contexts and costs are drawn at random and identically and independently at each iteration, contrasting with the computationally inefficient approaches that can handle adversarial inputs.\nTwo very recent works provide the first oracle efficient algorithms for the contextual bandit problem in adversarial settings [7, 8]. Rakhlin and Sridharan [7] considers a setting where the contexts are drawn i.i.d. from a known distribution with adversarial costs and they provide an oracle efficient algorithm with O(T 3 4K 1\n2 (log(|\u03a0|)) 14 ) regret. Their algorithm also applies in the transductive setting where the sequence of contexts is known a priori. Srygkanis et. al [8] also obtain a T 3\n4 -style bound with a different oracle-efficient algorithm, but in a setting where the learner knows only the set of contexts that will arrive. Both of these results achieve very suboptimal regret bounds, as the dependence on the number of iterations is far from the optimal O( \u221a T )-bound. A major open question posed by both works is whether the O(T 3\n4 ) barrier can be broken.\nIn this paper, we provide an oracle-based contextual bandit algorithm that achieves regretO((KT ) 2 3 (log(|\u03a0|)) 13 ) in both the i.i.d. context and the transductive settings considered by Rakhlin and Sridharan [7]. This bound matches that of the epoch-greedy algorithm of Langford and Zhang [6] that only applies to the fully stochastic setting. As in Rakhlin and Sridharan [7], our algorithm only requires access to a value oracle, which is weaker than the standard argmax oracle, and it makes K + 1 oracle calls per iteration. To our knowledge, this is the best regret bound achievable by an oracle-efficient algorithm for any adversarial contextual bandit problem.\nOur algorithm and regret bound are based on a novel and intricate analysis of the minimax problem that arises in the relaxation-based framework of Rakhlin and Sridharan [7]. Our proof requires analyzing the value of a sequential game where the learner chooses a distribution over actions and then the adversary chooses a distribution over costs in some bounded finite domain, with - importantly - a bounded variance. This is unlike the simpler minimax problem analyzed in [7], where the adversary is only constrained by the range of the costs. We provide this tighter minimax analysis in Section 3.\nApart from showing that this more structured minimax problem has a small value, we also need to derive an oracle-based strategy for the learner that achieves the improved regret bound. The additional constraints on the game require a much more intricate argument to derive this strategy which is an algorithm for solving a structured two-player minimax game. We present this part in Section 4."}, {"heading": "2 Model and Preliminaries", "text": "Basic notation. Throughout the paper we denote with x1:t a sequence of quantities {x1, . . . , xt} and with (x, y, z)1:t a sequence of tuples {(x1, y1, z1), . . .}. \u2205 denotes an empty sequence. The vector of ones is denoted by 1 and the vector of zeroes is denoted by 0. Denote with [K] the set {1, . . . ,K} and \u2206U the set of distributions over a set U . We also use \u2206K as a shorthand for \u2206[K].\nContextual online learning. We consider the following version of the contextual online learning problem. On each round t = 1, . . . , T , the learner observes a context xt and then chooses a probability distribution qt over a set of K actions. The adversary then chooses a cost vector ct \u2208 [0, 1]K . The learner picks an action y\u0302t drawn from distribution qt, suffers a loss ct(y\u0302t) and observes only ct(y\u0302t) and not the loss of the other actions.\nThroughout the paper we will assume that the context xt at each iteration t is drawn i.i.d. from a distribution D. This is referred to as the hybrid i.i.d.-adversarial setting [7]. As in prior work [7], we assume that the learner can sample contexts from this distribution as needed. It is easy to adapt the arguments in the paper to apply for the transductive setting where the learner knows the sequence of contexts that will arrive. The cost vectors ct are chosen by a non-adaptive adversary.\nThe goal of the learner is to compete with a set of policies \u03a0, where each policy \u03c0 \u2208 \u03a0 is a function mapping from the set of contexts to the set of actions. The cumulative expected regret with respect to the best fixed policy in hindsight is\nReg =\nT \u2211\nt=1\nqt \u00b7 ct \u2212 inf \u03c0\u2208\u03a0\nT \u2211\nt=1\nct(\u03c0(xt)) .\nOptimization value oracle. We will assume that we are given access to an optimization oracle that when given as input a sequence of contexts and loss vectors (x, c)1:t, it outputs the value of the cumulative loss of the best fixed policy: i.e.\ninf \u03c0\u2208\u03a0\nt \u2211\n\u03c4=1\nct(\u03c0(xt)) . (1)\nThis can be viewed as an offline batch optimization or ERM oracle."}, {"heading": "2.1 Relaxation based algorithms", "text": "We briefly review the relaxation based framework proposed in [7]. The reader is directed to [7] for a more extensive exposition. We will also slightly augment the framework by adding some internal random state that the algorithm might keep and use subsequently and which does not affect the cost of the algorithm.\nA crucial concept in the relaxation based framework is the information obtained by the learner at the end of each round t \u2208 [T ], which is the following tuple:\nIt(xt, qt, y\u0302t, ct, St) = (xt, qt, y\u0302t, ct(y\u0302t), St)\nwhere y\u0302t is the realized chosen action drawn from the distribution qt and St is some random string drawn from some distribution that can depend on qt, y\u0302t and ct(y\u0302t) and which can be used by the algorithm in subsequent rounds.\nDefinition 1 A partial-information relaxation Rel(\u00b7) is a function that maps (I1, . . . , It) to a real value for any t \u2208 [T ]. A partial-information relaxation is admissible if for any t \u2208 [T ], and for all I1, . . . , It\u22121:\nExt\n[\ninf qt sup ct Ey\u0302t\u223cqt,St [ct(y\u0302t) + Rel(I1:t\u22121, It(xt, qt, y\u0302t, ct, St))]\n]\n\u2264 Rel(I1:t\u22121) (2)\nand for all x1:T , c1:T and q1:T :\nEy\u03021:T\u223cq1:T ,S1:T [Rel(I1:T )] \u2265 \u2212 inf \u03c0\u2208\u03a0\nT \u2211\nt=1\nct(\u03c0(xt)) . (3)\nDefinition 2 Any randomized strategy q1:T that certifies inequalities (2) and (3) is called an admissible strategy.\nA basic lemma proven in [7] is that if one constructs a relaxation and a corresponding admissible strategy, then the expected regret of the admissible strategy is upper bounded by the value of the relaxation at the beginning of time.\nLemma 1 ([7]) Let Rel be an admissible relaxation and q1:T be an admissible strategy. Then for any c1:T , we have\nE [Reg] \u2264 Rel(\u2205) .\nWe will utilize this framework and construct a novel relaxation with an admissible strategy. We will show that the value of the relaxation at the beginning of time is upper bounded by the desired improved regret bound and that the admissible strategy can be efficiently computed assuming access to an optimization value oracle."}, {"heading": "3 A Faster Contextual Bandit Algorithm", "text": "First we define an unbiased estimator for each loss vector ct. In addition to doing the usual importance weighting, we also discretize the estimated loss to either 0 or L for some constant L \u2265 K to be specified later.\nSpecifically, consider a random variable Xt which we construct at the end of each iteration conditioning on y\u0302t:\nXt =\n{\n1 with probability ct(y\u0302t)Lqt(y\u0302t) , 0 with the remainig probability . (4)\nThis is a valid random variable whenever mini qt(i) \u2265 1L , which will be ensured by the algorithm. This is the only random variable in the random string St that we used in the general formulation of the relaxation framework.\nNow the construction of an unbiased estimate for each ct based on the information It collected at the end of each round is: c\u0302t = LXtey\u0302t . Observe that for any i \u2208 [K]:\nEy\u0302t\u223cqt,Xt [c\u0302t(i)] = L \u00b7 Pr[y\u0302t = i] \u00b7 Pr[Xt = 1|y\u0302t = i] = L \u00b7 qt(i) \u00b7 ct(i)\nLqt(i) = ct(i) .\nHence, c\u0302t is an unbiased estimate of ct. We are now ready to define our relaxation. Let \u01ebt \u2208 {\u22121, 1}K be a Rademacher random vector (i.e. each coordinate is an independent Rademacher random variable, which is \u22121 or 1 with equal probability), and let Zt \u2208 {0, L} be a random variable which is L with probability K/L and 0 otherwise. With the notation \u03c1t = (x, \u01eb, Z)t+1:T , our relaxation is defined as follows:\nRel(I1:t) = E\u03c1t [R((x, c\u0302)1:t, \u03c1t)] , (5)\nwhere\nR((x, c\u0302)1:t, \u03c1t) = \u2212 inf \u03c0\u2208\u03a0\n(\nt \u2211\n\u03c4=1\nc\u0302\u03c4 (\u03c0(x\u03c4 )) +\nT \u2211\n\u03c4=t+1\n2\u01eb\u03c4(\u03c0(x\u03c4 ))Zt\n)\n+ (T \u2212 t)K/L .\nNote that Rel(\u2205) is the following quantity, whose first part resembles a Rademacher average:\nR\u03a0 = 2E(x,\u01eb,Z)1:T\n[\ninf \u03c0\u2208\u03a0\nT \u2211\n\u03c4=t+1\n\u01eb\u03c4 (\u03c0(x\u03c4 ))Zt\n]\n+ TK/L .\nUsing the following Lemma (whose proof is deferred to the supplementary material) and the fact E [ Z2t ] \u2264 KL, we can upper bound R\u03a0 by O( \u221a\nTKL log(N) + TK/L), which after tuning L will give the claimed O(T 2/3) bound.\nLemma 2 Let \u01ebt be Rademacher random vectors, and Zt be non-negative real-valued random variables, such that E [\nZ2t ] \u2264 M . Then:\nEZ1:T ,\u01eb1:T\n[\nsup \u03c0\u2208\u03a0\nT \u2211\nt=1\n\u01ebt(\u03c0(xt)) \u00b7 Zt ] \u2264 \u221a 2TM log(N)\nTo show an admissible strategy for our relaxation, we next introduce some more notation. Let\nD = {L \u00b7 ei : i \u2208 [K]} \u222a {0},\nwhere (e1, . . . , eK) are the orthonormal basis vectors (i.e. ei is 1 at coordinate i and zero otherwise), and 0 is the all zeros vector. We will denote with \u2206D the set of distributions over D. For a distribution p \u2208 \u2206(D), we will denote with p(i), for i \u2208 {0, . . . ,K}, the probability assigned to vector ei, with the convention that e0 = 0. Also let \u2206 \u2032 D = {p \u2208 \u2206D : p(i) \u2264 1/L, \u2200i \u2208 [K]}.\nBased on this notation our admissible strategy is defined as\nqt = E\u03c1t [qt(\u03c1t)] where qt(\u03c1t) =\n(\n1\u2212 K L\n)\nq\u2217t (\u03c1t) + 1\nL 1 (6)\nAlgorithm 1 A New Contextual Bandit Algorithm\nInput: parameter L \u2265 K for each time step t \u2208 [T ] do Observe xt. Draw \u03c1t = (x, \u01eb, Z)t+1:T where each x\u03c4 is drawn from the distribution of contexts, \u01eb\u03c4 is a Rademacher random vectors and Z\u03c4 \u2208 {0, L} is L with probability K/L and 0 otherwise. Compute qt(\u03c1t) based on Eq. (6) (using Algorithm 2). Predict y\u0302t \u223c qt(\u03c1t) and observe ct(y\u0302t). Create an estimate c\u0302t = LXtey\u0302t , where Xt is defined in Eq. (4) with qt in that equation instantiated with qt(\u03c1t). end for\nand q\u2217t (\u03c1t) = argmin\nq\u2208\u2206K sup pt\u2208\u2206\u2032D\nEc\u0302t\u223cpt [\u3008q, c\u0302t\u3009+R((x, c\u0302)1:t, \u03c1t)] . (7)\nAlgorithm 1 implements this admissible strategy. Note that it suffices to use qt(\u03c1t) instead of qt for a random draw \u03c1t in the algorithm to ensure the exact same guarantee in expectation. Moreover, in Section 4 we will show that qt(\u03c1t) can be computed efficiently using an optimization value oracle.\nWe now prove that our relaxation and strategy are indeed admissible.\nTheorem 3 The relaxation defined in Equation (5) is admissible. An admissible randomized strategy for this relaxation is given by (6). The expected regret of the Algorithm 1 is upper bounded by\n2 \u221a 2TKL log(N) + TK/L, (8)\nfor any L \u2265 K. Specifically, setting L = (KT/ log(N)) 13 when T \u2265 K2 log(N), the regret is of order O((KT ) 2 3 (log(N)) 1 3 ).\nProof: We verify the two conditions for admissibility.\nFinal condition. It is clear that inequality (3) is satisfied since c\u0302t are unbiased estimates of ct:\nEy\u03021:T ,X1:T [Rel(I1:T )] = Ey\u03021:T ,X1:T\n[\nsup \u03c0\u2208\u03a0\n\u2212 T \u2211\n\u03c4=1\nc\u0302\u03c4 (\u03c0(x\u03c4 ))\n]\n\u2265 sup \u03c0\u2208\u03a0 \u2212Ey\u03021:T ,X1:T\n[\nT \u2211\n\u03c4=1\nc\u0302\u03c4 (\u03c0(x\u03c4 ))\n]\n= sup \u03c0\u2208\u03a0\n\u2212 T \u2211\n\u03c4=1\nc\u03c4 (\u03c0(x\u03c4 ))\nt-th Step condition. We now check that inequality (2) is also satisfied at some time step t \u2208 [T ]. We reason conditionally on the observed context xt and show that qt defines an admissible strategy for the relaxation. Let q\u2217t = E\u03c1t [q \u2217 t (\u03c1t)]. First observe that:\nEy\u0302t,Xt [ct(y\u0302t)] = Ey\u0302t\u223cqt [ct(y\u0302t)] = \u3008qt, ct\u3009 \u2264 \u3008q\u2217t , ct\u3009+ 1\nL \u30081, ct\u3009 \u2264 Ey\u0302t,Xt [\u3008q\u2217t , c\u0302t\u3009] +\nK\nL\nWe remind that c\u0302t = LXtey\u0302t is the unbiased estimate, which is a deterministic function of y\u0302t and Xt. Hence:\nsup ct\u2208[0,1]K Ey\u0302t,Xt [ct(y\u0302t) + Rel(I1:t)] \u2264 sup ct\u2208[0,1]K\nEy\u0302t,Xt [\u3008q\u2217t , c\u0302t\u3009+ Rel(I1:t)] + K\nL\nWe now work with the first term of the right hand side:\nsup ct\u2208[0,1]K Ey\u0302t,Xt [\u3008q\u2217t , c\u0302t\u3009+ Rel(I1:t)] = sup ct\u2208[0,1]K Ey\u0302t,Xt [\u3008q\u2217t , c\u0302t\u3009+ E\u03c1t [R((x, c\u0302)1:t, \u03c1t]]\n= sup ct\u2208[0,1]K\nEy\u0302t,Xt [E\u03c1t [\u3008q\u2217t (\u03c1t), c\u0302t\u3009+R((x, c\u0302)1:t, \u03c1t)]]\nObserve that c\u0302t is a random variable taking values in D and such that the probability that it is equal to Lei can be upper bounded as:\nPr[c\u0302t = Lei] = E\u03c1t [Pr[c\u0302t = Lei|\u03c1t]] = E\u03c1t [ qt(\u03c1t)(i) ct(i)\nL \u00b7 qt(\u03c1t)(i)\n]\n\u2264 1/L.\nThus we can upper bound the latter quantity by the supremum over all distributions in \u2206\u2032D, i.e.:\nsup ct\u2208[0,1]K Ey\u0302t,Xt [\u3008q\u2217t , c\u0302t\u3009+ Rel(I1:t)] \u2264 sup pt\u2208\u2206\u2032D Ec\u0302t\u223cpt [E\u03c1t [\u3008q\u2217t (\u03c1), c\u0302t\u3009+R((x, c\u0302)1:t, \u03c1t)]]\nNow we can continue by pushing the expectation over \u03c1t outside of the supremum, i.e.\nsup ct\u2208[0,1]K\nEy\u0302t\u223cqt,Xt [\u3008q\u2217t , c\u0302t\u3009+ Rel(I1:t)] \u2264 E\u03c1t\n[\nsup pt\u2208\u2206\u2032D\nEc\u0302t\u223cpt [\u3008q\u2217t (\u03c1t), c\u0302t\u3009+R((x, c\u0302)1:t, \u03c1t)] ]\nand working conditionally on \u03c1t. Observe that by the definition of q \u2217 t (\u03c1t) the quantity inside the expectation is equal to: inf\nq\u2208\u2206K sup\npt\u2208\u2206\u2032D\nEc\u0302t\u223cpt [\u3008q, c\u0302t\u3009+R((x, c\u0302)1:t, \u03c1t)]\nWe can now apply the minimax theorem and upper bound the above by:\nsup pt\u2208\u2206\u2032D inf q\u2208\u2206K\nEc\u0302t\u223cpt [\u3008q, c\u0302t\u3009+R((x, c\u0302)1:t, \u03c1t)]\nSince the inner objective is linear in q, we continue with\nsup pt\u2208\u2206\u2032D min i Ec\u0302t\u223cpt [c\u0302t(i) +R((x, c\u0302)1:t, \u03c1t)]\nWe can now expand the definition of R(\u00b7):\nsup pt\u2208\u2206\u2032D min i Ec\u0302t\u223cpt\n[\nc\u0302t(i) + sup \u03c0\u2208\u03a0\n\u2212 ( t \u2211\n\u03c4=1\nc\u0302\u03c4 (\u03c0(x\u03c4 )) +\nT \u2211\n\u03c4=t+1\n2\u01eb\u03c4 (\u03c0(x\u03c4 ))Zt\n)]\n+ (T \u2212 t)K/L\nWith the notation\nA\u03c0 = \u2212 t\u22121 \u2211\n\u03c4=1\nc\u0302\u03c4 (\u03c0(x\u03c4 ))\u2212 T \u2211\n\u03c4=t+1\n2\u01eb\u03c4(\u03c0(x\u03c4 ))Zt\nwe re-write the above quantity as:\nsup pt\u2208\u2206\u2032D min i Ec\u0302t\u223cpt\n[\nc\u0302t(i) + sup \u03c0\u2208\u03a0\n(A\u03c0 \u2212 c\u0302t(\u03c0(xt))) ] + (T \u2212 t)K/L\nWe now upper bound the first term. The extra term (T \u2212 t)K/L will be combined with the extra K/L that we have abandoned to give the correct term (T \u2212 (t\u2212 1))K/L needed for Rel(I1:t\u22121).\nObserve that we can re-write the first term by using symmetrization as:\nsup pt\u2208\u2206\u2032D min i Ec\u0302t\u223cpt\n[\nc\u0302t(i) + sup \u03c0\u2208\u03a0\n(A\u03c0 \u2212 c\u0302t(\u03c0(xt))) ]\n= sup pt\u2208\u2206\u2032D Ec\u0302t\u223cpt\n[\nsup \u03c0\u2208\u03a0 (A\u03c0 +min i\nEc\u0302\u2032t\u223cpt [c\u0302 \u2032 t(i)]\u2212 c\u0302t(\u03c0(xt)))\n]\n\u2264 sup pt\u2208\u2206\u2032D Ec\u0302t\u223cpt\n[\nsup \u03c0\u2208\u03a0\n(A\u03c0 + Ec\u0302\u2032t\u223cpt [c\u0302 \u2032 t(\u03c0(xt))]\u2212 c\u0302t(\u03c0(xt)))\n]\n\u2264 sup pt\u2208\u2206\u2032D Ec\u0302t,c\u0302\u2032t\u223cpt\n[\nsup \u03c0\u2208\u03a0\n(A\u03c0 + c\u0302 \u2032 t(\u03c0(xt))\u2212 c\u0302t(\u03c0(xt)))\n]\n= sup pt\u2208\u2206\u2032D Ec\u0302t,c\u0302\u2032t\u223cpt,\u03b4\n[\nsup \u03c0\u2208\u03a0\n(A\u03c0 + \u03b4 (c\u0302 \u2032 t(\u03c0(xt))\u2212 c\u0302t(\u03c0(xt))))\n]\n\u2264 sup pt\u2208\u2206\u2032D Ec\u0302t\u223cpt,\u03b4\n[\nsup \u03c0\u2208\u03a0 (A\u03c0 + 2\u03b4c\u0302t(\u03c0(xt)))\n]\nwhere \u03b4 is a random variable which is \u22121 and 1 with equal probability. The last inequality follows by splitting the supremum into two equal parts.\nConditioning on c\u0302t, consider the random variableMt which is \u2212maxi c\u0302t(i) ormaxi c\u0302t(i) on the coordinates where c\u0302t is equal to zero and equal to c\u0302t on the coordinate that achieves the maximum. This is clearly an unbiased estimate of c\u0302t. Thus we can upper bound the last quantity by:\nsup pt\u2208\u2206\u2032D Ec\u0302t\u223cpt,\u03b4\n[\nsup \u03c0\u2208\u03a0\n(A\u03c0 + 2\u03b4E [Mt(\u03c0(xt))|c\u0302t]) ]\n\u2264 sup pt\u2208\u2206\u2032D Ec\u0302t\u223cpt,\u03b4,Mt\n[\nsup \u03c0\u2208\u03a0 (A\u03c0 + 2\u03b4Mt(\u03c0(xt)))\n]\nThe random vector \u03b4Mt, conditioning on c\u0302t, is equal to \u2212maxi c\u0302t(i) or maxi c\u0302t(i) with equal probability independently on each coordinate. Moreover, observe that for any distribution pt \u2208 \u2206\u2032D, the distribution of the maximum coordinate of c\u0302t has support on {0, L} and is equal to L with probability at most K/L. Since the objective only depends on the distribution of the maximum coordinate of c\u0302t, we can continue the upper bound with a supremum over any distribution of random vectors whose coordinates are 0 with probability at least 1 \u2212 K/L and otherwise are \u2212L or L with equal probability. Specifically, let \u01ebt be a Rademacher random vector, we continue with:\nsup Zt\u2208\u2206{0,L}:Pr[Zt=L]\u2264K/L E\u01ebt,Zt\n[\nsup \u03c0\u2208\u03a0 (A\u03c0 + 2\u01ebt(\u03c0(xt))Zt)\n]\nNow observe that if we denote with a = Pr[Zt = L], the above is equal to:\nsup a:0\u2264a\u2264K/L\n(\n(1\u2212 a) sup \u03c0\u2208\u03a0 (A\u03c0) + aE\u01ebt\n[\nsup \u03c0\u2208\u03a0 (A\u03c0 + 2\u01ebt(\u03c0(xt))L)\n])\nWe now argue that this supremum is achieved by setting a = K/L. For that it suffices to show that:\nsup \u03c0\u2208\u03a0\n(A\u03c0) \u2264 E\u01ebt [\nsup \u03c0\u2208\u03a0 (A\u03c0 + 2\u01ebt(\u03c0(xt))L)\n]\nwhich is true by observing that with \u03c0\u2217 = argsup\u03c0\u2208\u03a0(A\u03c0) one has:\nE\u01ebt\n[\nsup \u03c0\u2208\u03a0 (A\u03c0 + 2\u01ebt(\u03c0(xt))L)\n]\n\u2265 E\u01ebt [A\u03c0\u2217 + 2\u01ebt(\u03c0\u2217(xt))L)] = A\u03c0\u2217 + E\u01ebt [2\u01ebt(\u03c0\u2217(xt))L)] = A\u03c0\u2217 .\nThus we can upper bound the quantity we want by:\nE\u01ebt,Zt\n[\nsup \u03c0\u2208\u03a0 (A\u03c0 + 2\u01ebt(\u03c0(xt))Zt\n]\nAlgorithm 2 Computing q\u2217t (\u03c1t)\nInput: a value optimization oracle, (x, c\u0302)1:t\u22121, xt and \u03c1t. Output: q \u2208 \u2206K as a solution of Eq. (7).\nCompute \u03c8i as in Eq. (9) for all i = 0, . . . ,K using the optimization oracle. Compute \u03c6i = \u03c8i\u2212\u03c80 L for all i \u2208 [K]. Let m = 1 and q = 0. for each coordinate i \u2208 [K] do Set q(i) = min{(\u03c6i)+,m}. Update m \u2190 m\u2212 q(i). end for Distribute m arbitrarily on the coordinates of q if m > 0.\nwhere \u01ebt is a Rademacher random vector and Zt is now a random variable which is equal to L with probability K/L and is equal to 0 with the remaining probability.\nTaking expectation over \u03c1t and xt and adding the (T \u2212 (t\u2212 1))K/L term that we abandoned, we arrive at the desired upper bound of Rel(I1:t\u22121). This concludes the proof of admissibility.\nRegret bound. By applying Lemma 2 with E[Z2t ] = L 2Pr[Zt = L] = KL and invoking Lemma 1, we get the regret bound in Equation (8)."}, {"heading": "4 Computational Efficiency", "text": "In this section we will argue that if one is given access to a value optimization oracle (1), then one can run Algorithm 1 efficiently. Specifically, we will show that the minimizer of Equation (7) can be computed efficiently (see Algorithm 2).\nLemma 4 Computing the quantity defined in equation (7) for any given \u03c1t can be done in time O(K) and with only K + 1 accesses to a value optimization oracle.\nProof: For i \u2208 {0, . . . ,K}, let:\n\u03c8i = inf \u03c0\u2208\u03a0\n(\nt\u22121 \u2211\n\u03c4=1\nc\u0302\u03c4 (\u03c0(x\u03c4 )) + Lei(\u03c0(xt)) +\nT \u2211\n\u03c4=t+1\n2\u01eb\u03c4(\u03c0(x\u03c4 ))Zt\n)\n(9)\nwith the convention e0 = 0. Then observe that we can re-write the definition of q \u2217 t (\u03c1t) as:\nq\u2217t (\u03c1t) = arginf q\u2208\u2206K sup pt\u2208\u2206\u2032D\nK \u2211\ni=1\npt(i)(L \u00b7 q(i)\u2212 \u03c8i)\u2212 pt(0) \u00b7 \u03c80\nObserve that each \u03c8i can be computed with a single oracle access. Thus we can assume that all K + 1 \u03c8\u2019s are computed efficiently and are given. We now argue how to compute the minimizer.\nFor each given q, the supremum over pt can be characterized as follows. With the notation zi = L\u00b7q(i)\u2212\u03c8i and z0 = \u2212\u03c80 we re-write the minimax quantity as:\nq\u2217t (\u03c1t) = arginf q\u2208\u2206K sup pt\u2208\u2206\u2032D\nK \u2211\ni=1\npt(i) \u00b7 zi + pt(0) \u00b7 z0\nObserve that if we didn\u2019t have the constraint that pt(i) \u2264 1/L for i > 0, then we would have put all the probability mass on the maximum of the zi. However, now that we are constrained we will simply put as much probability mass as allowed on the maximum coordinate argmaxi\u2208{0,...,K} zi and continue to the next highest quantity. We repeat this until reaching the quantity z0. At that point, the probability mass that\nwe can put on coordinate 0 is unconstrained. Thus we can put all the remaining probability mass on this coordinate.\nLet z(1), z(2), . . . , z(K) denote the ordered zi quantities for i > 0 (from largest to smallest). Moreover, let \u00b5 \u2208 [K] be the largest index such that z(\u00b5) \u2265 z0. By the above reasoning we get that for a given q, the supremum over pt is equal to:\n\u00b5 \u2211\nt=1\nz(t) L + ( 1\u2212 \u00b5 L ) z0 =\n\u00b5 \u2211\nt=1\nz(t) \u2212 z0 L + z0 .\nNow since for any t > \u00b5, z(t) < z0, we can write the latter as:\n\u00b5 \u2211\nt=1\nz(t) L + ( 1\u2212 \u00b5 L )\nz0 = K \u2211\ni=1\n(zi \u2212 z0)+ L + z0\nwith the convention (x)+ = max{x, 0}. We thus further re-write the minimax expression as:\nq\u2217t (\u03c1t) = arginf q\u2208\u2206K\nK \u2211\ni=1\n(zi \u2212 z0)+ L + z0 = arginf q\u2208\u2206K\nK \u2211\ni=1\n(zi \u2212 z0)+ L = arginf q\u2208\u2206K\nK \u2211\ni=1\n( q(i)\u2212 \u03c8i \u2212 \u03c80 L\n)+\nLet \u03c6i = \u03c8i\u2212\u03c80 L . The expression becomes: q \u2217 t (\u03c1t) = arginfq\u2208\u2206K \u2211K t=1(q(i)\u2212 \u03c6i)+.\nThe latter is minimized as follows: consider any i \u2208 [K] such that \u03c6i \u2264 0. Then putting any positive mass on such a coordinate i is going to lead to a marginal increase of 1. On the other hand if we put some mass on an index \u03c6i > 0, then that will not increase the objective until we reach the point where q(i) = \u03c6i. Thus a minimizer will distribute probability mass of min{\u2211i:\u03c6i>0 \u03c6i, 1}, on the coordinates for which \u03c6i > 0. The remainder mass (if any) can be distributed arbitrarily. See Algorithm 2 for details."}, {"heading": "5 Discussion", "text": "In this paper, we present a new oracle-efficient algorithm for adversarial contextual bandits and we prove that it achieves O((KT )2/3 log(|\u03a0|)1/3) regret in the settings studied by Rakhlin and Sridharan [7]. This is the best regret bound that we are aware of among oracle-based algorithms.\nWhile our bound improves on the O(T 3/4) bounds in prior work [7, 8], achieving the optimalO( \u221a TK log(|\u03a0|)) regret bound with an oracle based approach still remains an important open question. Another interesting avenue for future work involves understanding the role of transductivity assumptions and developing an algorithm that can handle the non-transductive fully adversarial setting. We look forward to pursuing these directions."}, {"heading": "A Supplementary Lemma", "text": "Lemma 2. Let \u01ebt be Rademacher random vectors, and Zt be non-negative real-valued random variables, such that E [\nZ2t ] \u2264 M . Then:\nEZ1:T ,\u01eb1:T\n[\nsup \u03c0\u2208\u03a0\nT \u2211\nt=1\n\u01ebt(\u03c0(xt)) \u00b7 Zt ] \u2264 \u221a 2TM log(N)\nProof:\nEZ1:T ,\u01eb1:T\n[\nsup \u03c0\u2208\u03a0\nT \u2211\nt=1\n\u01ebt(\u03c0(xt)) \u00b7 Zt ] = EZ1:T [ 1\n\u03bb E\u01eb1:T\n[\nlog\n(\nsup \u03c0\u2208\u03a0\ne\u03bb \u2211 T t=1 \u01ebt(\u03c0(xt))\u00b7Zt\n)]]\n\u2264 EZ1:T [ 1\n\u03bb log\n(\nE\u01eb1:T\n[\nsup \u03c0\u2208\u03a0\ne\u03bb \u2211 T t=1 \u01ebt(\u03c0(xt))\u00b7Zt\n])]\n\u2264 EZ1:T\n[\n1 \u03bb log\n(\nE\u01eb1:T\n[\n\u2211\n\u03c0\u2208\u03a0\ne\u03bb \u2211 T t=1 \u01ebt(\u03c0(xt))\u00b7Zt\n])]\n= EZ1:T\n[\n1 \u03bb log\n(\n\u2211\n\u03c0\u2208\u03a0\nE\u01eb1:T\n[\nT \u220f\nt=1\ne\u03bb\u01ebt(\u03c0(xt))\u00b7Zt\n])]\n= EZ1:T\n[\n1 \u03bb log\n(\n\u2211\n\u03c0\u2208\u03a0\nT \u220f\nt=1\nE\u01ebt\n[ e\u03bb\u01ebt(\u03c0(xt))\u00b7Zt ]\n)]\nNow observe that E\u01ebt [ e\u03bb\u01ebt(\u03c0(xt))\u00b7Zt ] = e \u03bb\u00b7Zt+e\u2212\u03bb\u00b7Zt 2 \u2264 e\u03bb 2\u00b7Z2t /2. Thus:\nEZ1:T ,\u01eb1:T\n[\nsup \u03c0\u2208\u03a0\nT \u2211\nt=1\n\u01ebt(\u03c0(xt)) \u00b7 Zt ] \u2264 EZ1:T [ 1\n\u03bb log\n(\n\u2211\n\u03c0\u2208\u03a0\nT \u220f\nt=1\ne\u03bb 2\u00b7Z2t /2\n)]\n= EZ1:T\n[\n1 \u03bb log ( Ne\u03bb 2 \u2211 T t=1 Z2t /2 )\n]\n= 1\n\u03bb log(N) + \u03bbEZ1:T\n[\nT \u2211\nt=1\nZ2t /2\n]\n\u2264 1 \u03bb log(N) + \u03bbMT/2\nOptimizing over \u03bb yields the result."}], "references": [{"title": "Taming the monster: A fast and simple algorithm for contextual bandits", "author": ["Alekh Agarwal", "Daniel Hsu", "Satyen Kale", "John Langford", "Lihong Li", "Robert E. Schapire"], "venue": "In International Conference on Machine Learning (ICML),", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2014}, {"title": "Gambling in a rigged casino: The adversarial multi-armed bandit pproblem", "author": ["Peter Auer", "Nicolo Cesa-Bianchi", "Yoav Freund", "Robert E. Schapire"], "venue": "In Foundations of Computer Science (FOCS),", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 1995}, {"title": "How to use expert advice", "author": ["Nicolo Cesa-Bianchi", "Yoav Freund", "David Haussler", "David P Helmbold", "Robert E Schapire", "Manfred K Warmuth"], "venue": "Journal of the ACM (JACM),", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 1997}, {"title": "Efficient optimal learning for contextual bandits", "author": ["Miroslav Dud\u00edk", "Daniel Hsu", "Satyen Kale", "Nikos Karampatziakis", "John Langford", "Lev Reyzin", "Tong Zhang"], "venue": "In Uncertainty and Artificial Intelligence (UAI),", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2011}, {"title": "A decision-theoretic generalization of on-line learning and an application to boosting", "author": ["Yoav Freund", "Robert E Schapire"], "venue": "Journal of Computer and System Sciences,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 1997}, {"title": "The epoch-greedy algorithm for multi-armed bandits with side information", "author": ["John Langford", "Tong Zhang"], "venue": "In Advances in Neural Information Processing Systems (NIPS),", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2008}, {"title": "BISTRO: an efficient relaxation-based method for contextual bandits", "author": ["Alexander Rakhlin", "Karthik Sridharan"], "venue": "In Proceedings of the Twentieth International Conference on Machine Learning (ICML 2016),", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2016}, {"title": "Efficient algorithms for adversarial contextual learning", "author": ["Vasilis Syrgkanis", "Akshay Krishnamurthy", "Robert E. Schapire"], "venue": "In Proceedings of the Twentieth International Conference on Machine Learning (ICML 2016),", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2016}], "referenceMentions": [{"referenceID": 6, "context": "Our analysis is based on the recent relaxation based approach of Rakhlin and Sridharan [7].", "startOffset": 87, "endOffset": 90}, {"referenceID": 4, "context": "This perspective is fruitful, as classical algorithms, such as Hedge [5, 3] and Exp4 [2], give information theoretically optimal regret bounds of O( \u221a T log(|\u03a0|)) in full-information and O( \u221a TK log(|\u03a0|) in the bandit setting, where T is the number of rounds, K is the number of actions, and \u03a0 is the policy set.", "startOffset": 69, "endOffset": 75}, {"referenceID": 2, "context": "This perspective is fruitful, as classical algorithms, such as Hedge [5, 3] and Exp4 [2], give information theoretically optimal regret bounds of O( \u221a T log(|\u03a0|)) in full-information and O( \u221a TK log(|\u03a0|) in the bandit setting, where T is the number of rounds, K is the number of actions, and \u03a0 is the policy set.", "startOffset": 69, "endOffset": 75}, {"referenceID": 1, "context": "This perspective is fruitful, as classical algorithms, such as Hedge [5, 3] and Exp4 [2], give information theoretically optimal regret bounds of O( \u221a T log(|\u03a0|)) in full-information and O( \u221a TK log(|\u03a0|) in the bandit setting, where T is the number of rounds, K is the number of actions, and \u03a0 is the policy set.", "startOffset": 85, "endOffset": 88}, {"referenceID": 0, "context": "Optimization oracles have been used in designing contextual bandit algorithms [1, 6, 4] that achieve the optimal 1", "startOffset": 78, "endOffset": 87}, {"referenceID": 5, "context": "Optimization oracles have been used in designing contextual bandit algorithms [1, 6, 4] that achieve the optimal 1", "startOffset": 78, "endOffset": 87}, {"referenceID": 3, "context": "Optimization oracles have been used in designing contextual bandit algorithms [1, 6, 4] that achieve the optimal 1", "startOffset": 78, "endOffset": 87}, {"referenceID": 6, "context": "Two very recent works provide the first oracle efficient algorithms for the contextual bandit problem in adversarial settings [7, 8].", "startOffset": 126, "endOffset": 132}, {"referenceID": 7, "context": "Two very recent works provide the first oracle efficient algorithms for the contextual bandit problem in adversarial settings [7, 8].", "startOffset": 126, "endOffset": 132}, {"referenceID": 6, "context": "Rakhlin and Sridharan [7] considers a setting where the contexts are drawn i.", "startOffset": 22, "endOffset": 25}, {"referenceID": 7, "context": "al [8] also obtain a T 3 4 -style bound with a different oracle-efficient algorithm, but in a setting where the learner knows only the set of contexts that will arrive.", "startOffset": 3, "endOffset": 6}, {"referenceID": 6, "context": "context and the transductive settings considered by Rakhlin and Sridharan [7].", "startOffset": 74, "endOffset": 77}, {"referenceID": 5, "context": "This bound matches that of the epoch-greedy algorithm of Langford and Zhang [6] that only applies to the fully stochastic setting.", "startOffset": 76, "endOffset": 79}, {"referenceID": 6, "context": "As in Rakhlin and Sridharan [7], our algorithm only requires access to a value oracle, which is weaker than the standard argmax oracle, and it makes K + 1 oracle calls per iteration.", "startOffset": 28, "endOffset": 31}, {"referenceID": 6, "context": "Our algorithm and regret bound are based on a novel and intricate analysis of the minimax problem that arises in the relaxation-based framework of Rakhlin and Sridharan [7].", "startOffset": 169, "endOffset": 172}, {"referenceID": 6, "context": "This is unlike the simpler minimax problem analyzed in [7], where the adversary is only constrained by the range of the costs.", "startOffset": 55, "endOffset": 58}, {"referenceID": 0, "context": "The adversary then chooses a cost vector ct \u2208 [0, 1] .", "startOffset": 46, "endOffset": 52}, {"referenceID": 6, "context": "-adversarial setting [7].", "startOffset": 21, "endOffset": 24}, {"referenceID": 6, "context": "As in prior work [7], we assume that the learner can sample contexts from this distribution as needed.", "startOffset": 17, "endOffset": 20}, {"referenceID": 6, "context": "1 Relaxation based algorithms We briefly review the relaxation based framework proposed in [7].", "startOffset": 91, "endOffset": 94}, {"referenceID": 6, "context": "The reader is directed to [7] for a more extensive exposition.", "startOffset": 26, "endOffset": 29}, {"referenceID": 6, "context": "A basic lemma proven in [7] is that if one constructs a relaxation and a corresponding admissible strategy, then the expected regret of the admissible strategy is upper bounded by the value of the relaxation at the beginning of time.", "startOffset": 24, "endOffset": 27}, {"referenceID": 6, "context": "Lemma 1 ([7]) Let Rel be an admissible relaxation and q1:T be an admissible strategy.", "startOffset": 9, "endOffset": 12}, {"referenceID": 0, "context": "Hence: sup ct\u2208[0,1] E\u0177t,Xt [ct(\u0177t) + Rel(I1:t)] \u2264 sup ct\u2208[0,1] E\u0177t,Xt [\u3008q t , \u0109t\u3009+ Rel(I1:t)] + K L We now work with the first term of the right hand side: sup ct\u2208[0,1] E\u0177t,Xt [\u3008q t , \u0109t\u3009+ Rel(I1:t)] = sup ct\u2208[0,1] E\u0177t,Xt [\u3008q t , \u0109t\u3009+ E\u03c1t [R((x, \u0109)1:t, \u03c1t]] = sup ct\u2208[0,1] E\u0177t,Xt [E\u03c1t [\u3008q t (\u03c1t), \u0109t\u3009+R((x, \u0109)1:t, \u03c1t)]]", "startOffset": 14, "endOffset": 19}, {"referenceID": 0, "context": "Hence: sup ct\u2208[0,1] E\u0177t,Xt [ct(\u0177t) + Rel(I1:t)] \u2264 sup ct\u2208[0,1] E\u0177t,Xt [\u3008q t , \u0109t\u3009+ Rel(I1:t)] + K L We now work with the first term of the right hand side: sup ct\u2208[0,1] E\u0177t,Xt [\u3008q t , \u0109t\u3009+ Rel(I1:t)] = sup ct\u2208[0,1] E\u0177t,Xt [\u3008q t , \u0109t\u3009+ E\u03c1t [R((x, \u0109)1:t, \u03c1t]] = sup ct\u2208[0,1] E\u0177t,Xt [E\u03c1t [\u3008q t (\u03c1t), \u0109t\u3009+R((x, \u0109)1:t, \u03c1t)]]", "startOffset": 57, "endOffset": 62}, {"referenceID": 0, "context": "Hence: sup ct\u2208[0,1] E\u0177t,Xt [ct(\u0177t) + Rel(I1:t)] \u2264 sup ct\u2208[0,1] E\u0177t,Xt [\u3008q t , \u0109t\u3009+ Rel(I1:t)] + K L We now work with the first term of the right hand side: sup ct\u2208[0,1] E\u0177t,Xt [\u3008q t , \u0109t\u3009+ Rel(I1:t)] = sup ct\u2208[0,1] E\u0177t,Xt [\u3008q t , \u0109t\u3009+ E\u03c1t [R((x, \u0109)1:t, \u03c1t]] = sup ct\u2208[0,1] E\u0177t,Xt [E\u03c1t [\u3008q t (\u03c1t), \u0109t\u3009+R((x, \u0109)1:t, \u03c1t)]]", "startOffset": 163, "endOffset": 168}, {"referenceID": 0, "context": "Hence: sup ct\u2208[0,1] E\u0177t,Xt [ct(\u0177t) + Rel(I1:t)] \u2264 sup ct\u2208[0,1] E\u0177t,Xt [\u3008q t , \u0109t\u3009+ Rel(I1:t)] + K L We now work with the first term of the right hand side: sup ct\u2208[0,1] E\u0177t,Xt [\u3008q t , \u0109t\u3009+ Rel(I1:t)] = sup ct\u2208[0,1] E\u0177t,Xt [\u3008q t , \u0109t\u3009+ E\u03c1t [R((x, \u0109)1:t, \u03c1t]] = sup ct\u2208[0,1] E\u0177t,Xt [E\u03c1t [\u3008q t (\u03c1t), \u0109t\u3009+R((x, \u0109)1:t, \u03c1t)]]", "startOffset": 209, "endOffset": 214}, {"referenceID": 0, "context": "Hence: sup ct\u2208[0,1] E\u0177t,Xt [ct(\u0177t) + Rel(I1:t)] \u2264 sup ct\u2208[0,1] E\u0177t,Xt [\u3008q t , \u0109t\u3009+ Rel(I1:t)] + K L We now work with the first term of the right hand side: sup ct\u2208[0,1] E\u0177t,Xt [\u3008q t , \u0109t\u3009+ Rel(I1:t)] = sup ct\u2208[0,1] E\u0177t,Xt [\u3008q t , \u0109t\u3009+ E\u03c1t [R((x, \u0109)1:t, \u03c1t]] = sup ct\u2208[0,1] E\u0177t,Xt [E\u03c1t [\u3008q t (\u03c1t), \u0109t\u3009+R((x, \u0109)1:t, \u03c1t)]]", "startOffset": 267, "endOffset": 272}, {"referenceID": 0, "context": ": sup ct\u2208[0,1] E\u0177t,Xt [\u3008q t , \u0109t\u3009+ Rel(I1:t)] \u2264 sup pt\u2208\u2206D E\u0109t\u223cpt [E\u03c1t [\u3008q t (\u03c1), \u0109t\u3009+R((x, \u0109)1:t, \u03c1t)]] Now we can continue by pushing the expectation over \u03c1t outside of the supremum, i.", "startOffset": 9, "endOffset": 14}, {"referenceID": 0, "context": "sup ct\u2208[0,1] E\u0177t\u223cqt,Xt [\u3008q t , \u0109t\u3009+ Rel(I1:t)] \u2264 E\u03c1t [", "startOffset": 7, "endOffset": 12}], "year": 2016, "abstractText": "We give an oracle-based algorithm for the adversarial contextual bandit problem, where either contexts are drawn i.i.d. or the sequence of contexts is known a priori, but where the losses are picked adversarially. Our algorithm is computationally efficient, assuming access to an offline optimization oracle, and enjoys a regret of order O((KT ) 2 3 (logN) 1 3 ), where K is the number of actions, T is the number of iterations and N is the number of baseline policies. Our result is the first to break the O(T 3 4 ) barrier that is achieved by recently introduced algorithms. Breaking this barrier was left as a major open problem. Our analysis is based on the recent relaxation based approach of Rakhlin and Sridharan [7].", "creator": "LaTeX with hyperref package"}}}