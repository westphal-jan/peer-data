{"id": "1606.00372", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "1-Jun-2016", "title": "Conversational Contextual Cues: The Case of Personalization and History for Response Ranking", "abstract": "We investigate the task of modeling open-domain, multi-turn, unstructured, multi-participant, conversational dialogue. We specifically study the effect of incorporating different elements of the conversation. Unlike previous efforts, which focused on modeling messages and responses, we extend the modeling to long context and participant's history of speech to study new features. Our results demonstrate that the effects of changing language on participant behavior in the study are not linear. We further explore the role of cultural and social influences in shaping engagement on the speech-to-speech relationship. To find the relationship between language and social influence in the study, we use inter-individual (unstructured) questions and multivariate analyses using the Multivariate Analysis Toolkit. We present the data on participants' self-reported ability to learn a set of questions, as well as their self-reported response time (ST). We identify participants as non-unstructured and unstructured, and examine whether their ability to learn a set of questions is influenced by their social interactions with others (N = 8). We also address whether they are able to learn information about others (N = 5), as well as how they are perceived, as well as what they would like to hear from others. We explore whether social interactions influence language on responses to different topics, even though they can be limited to an individual\u2021 and therefore unstructured.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n", "histories": [["v1", "Wed, 1 Jun 2016 18:01:14 GMT  (886kb,D)", "http://arxiv.org/abs/1606.00372v1", "10 pages, 6 figures"]], "COMMENTS": "10 pages, 6 figures", "reviews": [], "SUBJECTS": "cs.CL cs.LG", "authors": ["rami al-rfou", "marc pickett", "javier snaider", "yun-hsuan sung", "brian strope", "ray kurzweil"], "accepted": false, "id": "1606.00372"}, "pdf": {"name": "1606.00372.pdf", "metadata": {"source": "CRF", "title": "Conversational Contextual Cues: The Case of Personalization and History for Response Ranking", "authors": ["Rami Al-Rfou"], "emails": ["raykurzweil}@google.com"], "sections": [{"heading": "1 Introduction", "text": "Designing conversational systems is a challenging task and one of the original goals of Artificial Intelligence (Turing, 1950). For decades, conversational agent design was dominated by systems that rely on knowledge bases and rule-based mechanisms to understand human inputs and generate reasonable responses (Weizenbaum, 1966; Parkinson et al., 1977; Wallace, 2009). Data-driven approaches emphasize learning directly from corpora of written or spoken conversations. Recently, this approach gained momentum because of data abundance (Serban et al., 2015b), increasing computational power, and better learning algorithms that automate the feature engineering process (Schmidhuber, 2015; LeCun et al.,\n2015). Here, we study how modeling dialogue is influenced by the history within a conversation, and participants\u2019 histories across their conversations. Recent work in data-driven models focuses on modeling the next response as a function of the preceding message (Vinyals and Le, 2015; Li et al., 2015). We extend previous models in two new directions. First, we model the history of what has been said before the last message, termed context. This allows the model to include medium-term signals, presumably references and entities, which disambiguate the most recent information. As the conversation continues and the context grows, we expect our model to make better predictions of the next message (See Table 1). Second, to capture longer-term contextual signals, we model each user\u2019s personal history across all the conversations in which he or she participated in. We refer to this information as personal history. The model can personalize its predictions depending on specific users\u2019 opinions, interests, experiences, and styles of writing or speaking. Both of these contextual signals give us the ability to make better predicar X iv :1 60 6. 00 37\n2v 1\n[ cs\n.C L\n] 1\nJ un\n2 01\ntions regarding future responses. Characterizing users, language, discourse coherence, and response diversity requires huge datasets and large models. To gather conversations at scale, we turn to web forums as a source of data. Specifically, we extract conversations from Reddit, a popular social news networking website. The website is divided into sub-forums (subreddits), each of which has its own theme of topics and interests. Registered users can submit URLs or questions, comment on a topic or on other users\u2019 comments, and vote on submissions or comments. Unlike previous efforts that used Twitter as a source of conversations (Ritter et al., 2010), Reddit does not have length constraints, allowing more natural text. We extracted 133 million posts from 326K different subforums, consisting of 2.1 billion comments. This dataset is several orders of magnitude larger than existing datasets (Serban et al., 2015b).\nInstead of modeling message generation directly, the current work focuses on the ranking task of \u201cresponse selection.\u201d At each point in the conversation, the task is to pick the correct next message from a pool of random candidates. Picking the correct next message is likely to be correlated with implicit understanding of the conversation. We use Precision@k to characterize the accuracy of the system. We train a deep neural network as a binary classifier to learn the difference between positive, real examples of input / response pairs, and negative, random examples of input / response pairs. The classifier\u2019s probabilities are used as scores to rank the candidates. This ranker will choose the response with the highest score.\nUnlike generative approaches, where the modeling focus can be dominated by within-sentence language modeling, our approach encourages the system to discriminate between the connections of an actual response to the current conversation, and the lack of connections from a random response. Our model ranks candidates given any subset of the features we discussed so far (i.e., user models, conversation history, or the previous message). We also jointly learn a shared word embedding space (Bengio et al., 2006) and a user embedding space. With this arrangement, the models share common dynamics across users, giving us better models of conversations, and avoiding the need to construct a different\nmodel for each user. To summarize, our contributions are: \u2022 We model users\u2019 responses over long histories\nthat consist of their contributions over the years in various subforums and discussions. Furthermore, we integrate this model to offer better predictions. \u2022 We model the conversation history beyond the\ncurrent message. We study the length of the modeled history on the performance of our model. \u2022 We outline a direct path to train and use a dis-\ncriminative classifier as a response ranker. \u2022 We demonstrate how to use Reddit\u2019s comment\nstructure to extract complex dialogues on a large scale. We use a scalable neural network architecture that is able to take advantage of the large data size. In Section 2, we discuss recent relevant work in data-driven modeling of dialogue systems. Section 3 discusses the Reddit dataset\u2019s diversity and scale and the steps we took to process the raw data. In Section 4, we discuss our choices in conversation modeling with deep neural networks. We discuss our experimental setup in Section 5, analyze our results in Section 6, and conclude our discussion in Section 7."}, {"heading": "2 Related Work", "text": "Ritter et al. (2010) proposed a data-driven approach for building dialogue systems, and they extracted 1.3 million conversations from Twitter with the aim of discovering dialogue acts. Building on the distributional similarities of the vector space model frame-\nwork, Banchs and Li (2012) built a search engine to retrieve the most suitable response for any input message. Other approaches focused on domain specific tasks such as games (Narasimhan et al., 2015) and restaurants (Wen et al., 2016; Cuaya\u0301huitl, 2016)\nPersonalizing dialogue systems requires sufficient information from each user and a sufficient user population to define the space. Writing styles quantified by word length, verb strength, polarity, and distribution of dialogue acts have been used to model users (Walker et al., 2012). Other efforts focused on building a user profile based on demographics, such as gender, income, age, and marital status (Bonin et al., 2014). Because Reddit users are pseudoanonymous, we differ from these approaches by learning the relevant features to model the users\u2019 dialogue behavior through embedding each user into a distributed representation.\nWith the introduction of the sequence-tosequence framework (Sutskever et al., 2014), many recent learning systems have used recurrent neural networks (RNNs) to generate novel responses given an input message or sentence. For example, Vinyals and Le (2015) proposed using IT desk chat logs as a dataset to train LSTM network to generate new sentences. Sordoni et al. (2015) constructed Twitter conversations limiting the history context to one message. With the help of pre-trained RNN language models, they encoded each message into a vector representation. To eliminate the need for a language model, Serban et al. (2015a) tried end-to-end training on an RNN encoder-decoder network. They also bootstrapped their system with pre-trained word embeddings.\nWhile these systems are able to produce novel responses, it is difficult to understand how much capacity is consumed by modeling natural language versus modeling discourse and the coherence of the conversation. Often responses gravitate to the most frequent sentences observed in the training corpus (Li et al., 2015).\nPerplexity, BLEU, and deltaBLEU, adapted from language modeling and machine translation communities, have been used for evaluating novel responses (Yao et al., 2015; Sordoni et al., 2015; Galley et al., 2015). These metrics only measure the response\u2019s lexical fluency and do not penalize for incoherent candidates with regard to the conversational\nContext length 2 Context length 3\ndiscourse. While the search for better metrics is still on going, automatic evaluation of response generation stays an open problem (Shang et al., 2015).\nRecall@k or Precision@k are commonly used for measuring a ranker\u2019s performance on the task of response selection. Typically, a positive response is mixed with random responses, and then the system is asked to score the right response higher than others (Hu et al., 2014; Kadlec et al., 2015). This task measures the model\u2019s ability to discriminate what goes together and what does not. As these metrics are better understood, we focus on the response selection task in our modeling effort."}, {"heading": "3 Reddit Dataset", "text": "As conversational data-driven models are growing in popularity, datasets are increasing in number and size. However, most are small and domain specific. Serban et al. (2015b) surveyed 56 datasets and found that only 9 have more than 100,000 conversations, only one having more than 5 million conversations. This limits the complexity and the capacity of the models we can train. To target open-domain conversations, we need larger datasets. So far, there has been some limited effort to exploit the rich structure of Reddit. For example, Schrading et al. (2015) extracted comments from a small number of subreddits to build a classifier that identifies domestic\nabuse content. Unlike the Ubuntu dataset, logs of technical chat rooms (Lowe et al., 2015), Reddit conversations tend to be more diverse in regard to topics and user backgrounds. There are more than 300 thousands sub-forums (subreddits) with different topics of discussion. Compared to Twitter, Reddit conversations tend to be more natural, as there is no limit on message size (See Table 2).\nFigure 1 shows how Reddit conversations are organized as a tree or \u201cReddit post.\u201d Users can comment on each other\u2019s comments indefinitely, leading to long conversations, which help us construct significant dialogue history. We construct a conversation by traversing the tree starting at any node, up through its parent and ancestors until we reach the first comment (i.e., the tree\u2019s root). Since users cannot comment unless they are registered with a unique user name, we use those names as our labels for learning our user embedding vectors to personalize the dialogue system. Note that users tend to be pseudo-anonymous. They do not use their real names and they participate on the website without sharing private identifying information.\nSpecifically, we use a public crawl of the reddit website1. Figure 2 shows that the dataset is hugely\n1https://bigquery.cloud.google.com/\ndiverse and complex. Figure 2a shows that the website has both irregular contributors and heavy users who have a large number of comments. Unlike the datasets surveyed in (Serban et al., 2015b), a comment can have several user generated responses. While these diverse responses by no means cover the space of reasonable responses, this property may help our models in generalization (See Figure 2c)."}, {"heading": "4 Models", "text": "We define a conversation C to be a sequence of k pairs of Messages and participants (Authors) C \u2261 ((M1, A1), (M2, A2), . . . , (Mk, Ak)). Here, a messageMi is a sequence of a variable number of words Mi = (wi1, wi2, ....wil). Ai andwj are random variables taking values in the user population Puser and the word vocabulary Vword, respectively. Puser is a fixed set of the most frequent authors in reddit, it is basically, a dictionary of their usernames that is used to index the author embedding matrix. Every vector is limited to only one user.\nTo represent messages we use bag of words technique over recurrent or convolutional networks for its speed and ability to scale to a dataset as large as Reddit. To improve bag of words capability of keeping track of words\u2019 order and sentence structure, we define Vngram to be a dictionary of a subset of ngrams defined over the word vocabulary.\nThe first step in our modeling is to map each user in our population Puser and each word in our vocabulary Vword to a vector of d dimensions. Specifically, we define \u03c6user : Ai 7\u2192 RdA to be the embedding of the user Ai and \u03c6ngram : (wi, wj , . . . ) 7\u2192 Rdn to be the embedding of the ngram (wi, wj , . . . ). For a sequence of k messages, we define the bag of ngrams embedding (\u03c8 \u2208 Rdn) to be the average of the embeddings of the ngrams extracted from all the messages:\n\u03c8(M1, . . . ,Mk) = 1\nL \u2211 1\u2264j\u2264k \u2211 g\u2208ngrams(Mj) \u03c6ngram(g)\n(1) where L is the total number of ngrams extracted from all the messages {M1, . . . ,Mk}. Next, for a sequence of message-participant pairs of length k, we define the following features:\ndataset/fh-bigquery:reddit_comments\n\u2022 Response : R = \u03c8(Mk) where Mk is the last message in the sequence. \u2022 Input Message: I = \u03c8(Mk\u22121) where Mk\u22121 is\nthe message that the response is addressing. \u2022 Context: C = \u03c8(M1,M2, . . .Mk\u22122) where (M1,M2, . . .Mk\u22122) is the subsequence of messages that preceded the input message. \u2022 Author: A = \u03c6user(Ak) where Ak is the user\nwho generated the response message. In Reddit, for each message in the post tree, we consider its parent to be the input message and its parent\u2019s ancestors to be the context. The content of the message is the response and the user that wrote the message is the author."}, {"heading": "4.1 Response Ranking", "text": "To measure the effect of our features on modeling conversations, our task is to select the best response out of a pool of random candidates. This selection process could be viewed as a ranking problem. There are several approaches to ranking: pointwise, pairwise, and list-wise (Liu, 2009). Kadlec et al. (2015) chose pointwise ranking for its simplicity, and we follow the same choice for its speed benefits, which are necessary for training on hundreds of millions of examples. In pointwise ranking, we consider the compatibility of only one candidate at a time. Specifically, we learn a model that estimates the probability of a candidate given a subset of the features {I,C,A}.\nTo construct the training dataset, we form pairs of features and responses. For each response appearing in the corpus, we form two pairs. The first is composed of the features with the observed re-\nsponse ({I,C,A},R). In the second pair, we replace the response with another random response sampled from our corpus, ({I,C,A},R\u2032). The first pair is used as a positive example and the second is a negative one."}, {"heading": "4.2 Single-loss Network", "text": "To estimate the probability of the response appearing given the features, we train a binary logistic regression classifier. Figure 3 shows a network that concatenates the previous features into one input vector input = [I;C;A;R]. Then, several hidden layers with Relu non-linearities follow to produce a hidden layer h. Given the hidden layer h, we estimate the probability of the response, as follows:\nPr(R|I,C,A) \u2248 \u03c3(Wh+ b) (2)\nWhere \u03c3 is the sigmoid function \u03c3(x) = 1/(1 + e\u2212x). We call this model a single-loss model because it makes one prediction using all the information available."}, {"heading": "4.3 Multi-loss Network", "text": "We can formalize the previous singleloss model network further by declaring Pr(R|x) \u2248 Network(x), where x is the input feature vector. The network uses a logistic regression layer on top of a feed-forward neural network. Figure 4 shows the multi-loss architecture that could be viewed as a network of networks. This architecture is achieved by replicating the single-loss architecture (Network) three times for each feature. Each of the networks makes\npredictions taking into account one feature at a time. Furthermore, each network produces a hidden layer (hi) that will be used in an aggregate network. The aggregate network concatenates the hidden layers from the previous networks, [h1;h2;h3], to produce a final hidden layer h4. This allows the final prediction to take advantage of all the features jointly. This network also allows us to measure the performance of each feature alone. This modular architecture facilitates diagnosis of any possible problems during training.\nMore specifically, the networks represent the following classification problems:\nPr(R|I) \u2248 \u03c3(W1h1 + b1) Pr(R|C) \u2248 \u03c3(W2h2 + b2) Pr(R|A) \u2248 \u03c3(W3h3 + b3)\nPr(R|I,C,A) \u2248 \u03c3(W4h4 + b4)\nWe use only the final prediction in the evaluation Pr(R|A,C, I), but we penalize the model with the sum of all predictions\u2019 losses."}, {"heading": "5 Experimental Setup", "text": "We extract 2.1 Billion comments that were posted on the Reddit website between 2007 and 2015. We group the comments by their post page, treating each Reddit post as a tree rooted at the title of the post. We generate a positive example from each comment in the post. The example features are calculated by looking at the message\u2019s attributes, its parent, and its ancestors in the tree. We exclude Reddit posts that have more than 1000 comments for computational reasons. Most of these large posts are \u201cMegathreads\u201d, each containing hundreds of thousands of comments. We do not generate examples for comments with empty or missing input message features. We also exclude examples where the author is not in our user population Puser, or the user profile was deleted. After this filtering, 550 million positive examples are yielded. For each positive example, we generate a negative example by replacing the response feature by a random comment from Reddit."}, {"heading": "5.1 Vocabulary", "text": "Reddit comments are written in markdown markup language. First, we remove the markdown and then,\ntokenize the textual content. We normalize URLs and then include in our vocabulary the most frequent 200K unigrams and 200K bigrams. The count of the least frequent unigram is 1229, and for the least frequent bigram is 27670. For the author embeddings, we construct a user population (Puser) of the most frequent contributing 400K users. The least contributing user created 922 comments. This population is, essentially, a dictionary of the user names."}, {"heading": "5.2 Training", "text": "We set the ngram embedding and the user embedding space to 300 dimensions. The single loss model consists of one network, while the multiloss network consists of four networks. Each network consists of three hidden layers of size [500, 300, 100]. The hidden layer parameters, the ngram embeddings, and the user embeddings are trained jointly, and we use stochastic gradient descent (SGD) to optimize the parameters of our models (Bottou, 1991). The derivatives are estimated using the backpropagation algorithm and the updates are applied asynchronously. The learning rate \u03b1 for SGD is set to 0.03. The models are implemented using TensorFlow (Abadi et al., 2015). Despite that each model is trained on 5 GPUs, the training time still takes several weeks due to data size.\nWe split our dataset into three partitions: train, dev, and test. The training dataset consists of 90% of the data and the rest is divided between dev and test. We train our classifier for one epoch, which is equal to 1 Billion examples. We stop training our models when we observe no significant improvement in the accuracy of our binary classifier on the dev dataset.\nAs our binary classifier will be evaluated on ranking candidate responses, we extract 10,000 examples from our test dataset. For each example, we sample N \u2212 1 random responses from the pool. Given N of candidates, the classifier is asked to give the highest probability to the positive candidate available in the pool. We report precision (P@1) as a metric of quality."}, {"heading": "6 Discussion & Results", "text": "In this section, we discuss the gains achieved by integrating the conversation history as well as the participants\u2019 history into our modeling. We con-\ntrast both approaches and contrast their qualities and show a final model that takes advantage of both. Then, we show the effect of increasing the training dataset size on our models performance."}, {"heading": "6.1 Length of the Context", "text": "How far back do we need to look to improve the quality of our ranking? To test that, we train both models discussed in Section 4 on several datasets with context features that vary in temporal scope. Table 3 shows the Precision @ 1 for both models using two different ranking tasks, the first involves 10 candidates and the second has 100 candidates. Context of length 0 corresponds to using only the input message as a feature. Each model was trained and tested on examples that included a conversation history (context length) up to m number of messages and not necessarily all the messages in the training or the test included the same history length.\nFirst, we observe clear gains when we integrate the context feature (C). P@1 increases by 4-6 points the moment we include the message that preceded the input message. However, we see a diminishing return as the context increases, particularly when the context is larger than 5 messages. In this case, there could be two factors at work. First, the more messages we use, the larger the number of vectors we average; this tends to blur the features and increase the information loss compared to the insight we gain. Second, we have less training and a smaller number of test examples that could take advantage of a long history. Figure 2d shows that more than 90% of the reddit comments haver a lower depth than 6\nmessages in the tree."}, {"heading": "6.2 Personalization", "text": "Table 4 shows larger gains in our rankers\u2019 precision when using the author feature compared to the conversational history (context) feature. The multi-loss model improves by 5 points in the task of ranking 100 response candidates. The author vector represents longer historical information than the current conversation history. Personal history could include interests, opinions, demographics, writing style, and personality traits. These could be essential in determining if a response is appropriate.\nFinally, if we use all the features available to us, we get further improvement in performance over any of the features used alone. This highlights that the information we recover from each feature is different."}, {"heading": "6.3 Multi-loss Vs Single-loss", "text": "The motivation behind the multi-loss model is to prevent adaptation between features (Hinton et al., 2012). In the single-loss model, the author feature could be subsumed for many cases with the input message and the context. Only subtle cases may require knowing the author identity to determine if the response is suitable. This slows the learning process of good author vectors. Therefore, the multi-loss network requires that the author vector should capture enough information to perform the prediction task, solely. This architecture extends the deep supervision idea where companion objective function is introduced to train intermediate layers in a deep network (Lee et al., 2015). Notice how the author feature outperforms the context feature in all tasks\nwith the introduction of the multi-loss model. The multi-loss model is also easier to debug and probe. By reporting every loss on its own, we can see the development of the network.\nFigure 5 shows the loss contributed by each feature. Notice, how the author vector takes more than 100 million examples to start influencing the prediction task. We conjecture that this behavior is the result of two factors. First, the author distribution does not follow Zipf\u2019s law, as language does. There is no small number of authors that could cover most of the examples. Second, author vectors depend. indirectly, on the content of the comments they posted. Unless the representation of the language, and consequently messages, are stable, we cannot learn a aggregate representation of the user\u2019s set of messages. This multi-stage learning is similar to what McClelland and Rogers (2003) observed in their work."}, {"heading": "6.4 New users", "text": "In our evaluation we did not consider the case of unknown users. However, if a new user is encountered by our model, we can add a randomly initialized vector as a temporary representation. As the conversation goes on, we can then refine user vector using backpropagation while the rest of the model parameters are fixed. This technique is similar to the paragraph vector\u2019s strategy of dealing with new paragraphs after training is finished (Le and Mikolov, 2014)."}, {"heading": "6.5 Learning Curves", "text": "Figure 6a shows the improvement of the classifier accuracy by increasing the training dataset size orders of magnitude at a time. The results we presented so far would not have been possible without the billion examples we extracted from Reddit. It is quite clear that our models would have performed poorly given the other previously used datasets given their small sizes.\nMoreover, the accuracy of the binary classifier is correlated highly with the P@1 of the rankers we evaluated. We found that the pearson correlation between accuracy observed on the dev dataset and P@1 of the ranker tested on the test dataset is both strong and positive, between +0.94 and +0.99. Therefore, we may infer the future gains of increasing the size of the dataset on the quality of the ranker (See Figure 6b)."}, {"heading": "7 Conclusion", "text": "Using Reddit, our model is trained on a significantly larger conversational dataset than previously published efforts. We train two scalable neural network models using bags of ngram embeddings and user embeddings. We measure significant improvement in the task of selecting the next response by integrating what has been said in the conversation so far. We study the effect of the length of the conversation history on performance. We also personalize the selection process by learning an identity feature for each user. This yields further improvement as it models the longer history of what a user has said in all conversations. Finally, our multi-loss model shows improvements over the baseline single-loss model using any subset of the features."}], "references": [{"title": "Iris: a chat-oriented dialogue system based on the vector space model", "author": ["Banchs", "Li2012] Rafael E. Banchs", "Haizhou Li"], "venue": "In Proceedings of the ACL 2012 System Demonstrations,", "citeRegEx": "Banchs et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Banchs et al\\.", "year": 2012}, {"title": "Neural probabilistic language models", "author": ["Bengio et al.2006] Yoshua Bengio", "Holger Schwenk", "Jean-S\u00e9bastien Sen\u00e9cal", "Fr\u00e9deric Morin", "Jean-Luc Gauvain"], "venue": "In Innovations in Machine Learning,", "citeRegEx": "Bengio et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Bengio et al\\.", "year": 2006}, {"title": "A context-aware nlp approach for noteworthiness detection in cellphone conversations", "author": ["Jose San Pedro", "Nuria Oliver"], "venue": "In COLING,", "citeRegEx": "Bonin et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Bonin et al\\.", "year": 2014}, {"title": "Stochastic gradient learning in neural networks", "author": ["L\u00e9on Bottou"], "venue": "In Proceedings of NeuroN\u0131\u0302mes", "citeRegEx": "Bottou.,? \\Q1991\\E", "shortCiteRegEx": "Bottou.", "year": 1991}, {"title": "Simpleds: A simple deep reinforcement learning dialogue system", "author": ["Heriberto Cuay\u00e1huitl"], "venue": "CoRR, abs/1601.04574", "citeRegEx": "Cuay\u00e1huitl.,? \\Q2016\\E", "shortCiteRegEx": "Cuay\u00e1huitl.", "year": 2016}, {"title": "deltableu: A discriminative metric for generation tasks with intrinsically diverse targets", "author": ["Galley et al.2015] Michel Galley", "Chris Brockett", "Alessandro Sordoni", "Yangfeng Ji", "Michael Auli", "Chris Quirk", "Margaret Mitchell", "Jianfeng Gao", "Bill Dolan"], "venue": null, "citeRegEx": "Galley et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Galley et al\\.", "year": 2015}, {"title": "Improving neural networks by preventing co-adaptation of feature detectors. arXiv preprint arXiv:1207.0580", "author": ["Nitish Srivastava", "Alex Krizhevsky", "Ilya Sutskever", "Ruslan R Salakhutdinov"], "venue": null, "citeRegEx": "Hinton et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Hinton et al\\.", "year": 2012}, {"title": "Convolutional neural network architectures for matching natural language sentences", "author": ["Baotian Hu", "Zhengdong Lu", "Hang Li", "Qingcai Chen"], "venue": null, "citeRegEx": "Hu et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Hu et al\\.", "year": 2014}, {"title": "Improved deep learning", "author": ["Kadlec et al.2015] Rudolf Kadlec", "Martin Schmid", "Jan Kleindienst"], "venue": null, "citeRegEx": "Kadlec et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Kadlec et al\\.", "year": 2015}, {"title": "Distributed representations of sentences and documents", "author": ["Le", "Mikolov2014] Quoc Le", "Tomas Mikolov"], "venue": "Proceedings of the 31st International Conference on Machine Learning", "citeRegEx": "Le et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Le et al\\.", "year": 2014}, {"title": "A diversitypromoting objective function for neural conversation models. arXiv preprint arXiv:1510.03055", "author": ["Li et al.2015] Jiwei Li", "Michel Galley", "Chris Brockett", "Jianfeng Gao", "Bill Dolan"], "venue": null, "citeRegEx": "Li et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Li et al\\.", "year": 2015}, {"title": "Learning to rank for information retrieval. Foundations and Trends in Information Retrieval, 3(3):225\u2013331", "author": ["Tie-Yan Liu"], "venue": null, "citeRegEx": "Liu.,? \\Q2009\\E", "shortCiteRegEx": "Liu.", "year": 2009}, {"title": "The ubuntu dialogue corpus: A large dataset for research in unstructured multi-turn dialogue systems. arXiv preprint arXiv:1506.08909", "author": ["Lowe et al.2015] Ryan Lowe", "Nissan Pow", "Iulian Serban", "Joelle Pineau"], "venue": null, "citeRegEx": "Lowe et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Lowe et al\\.", "year": 2015}, {"title": "The parallel distributed processing approach to semantic cognition", "author": ["McClelland", "Rogers2003] James L McClelland", "Timothy T Rogers"], "venue": "Nature Reviews Neuroscience,", "citeRegEx": "McClelland et al\\.,? \\Q2003\\E", "shortCiteRegEx": "McClelland et al\\.", "year": 2003}, {"title": "Language understanding for text-based games using deep reinforcement learning", "author": ["Tejas Kulkarni", "Regina Barzilay"], "venue": "In Proceedings of the 2015 Conference on Empirical Methods in Natural", "citeRegEx": "Narasimhan et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Narasimhan et al\\.", "year": 2015}, {"title": "Conversational language comprehension using integrated pattern-matching and parsing", "author": ["Kenneth Mark Colby", "William S Faught"], "venue": "Artificial Intelligence,", "citeRegEx": "Parkinson et al\\.,? \\Q1977\\E", "shortCiteRegEx": "Parkinson et al\\.", "year": 1977}, {"title": "Unsupervised modeling of twitter conversations", "author": ["Ritter et al.2010] Alan Ritter", "Colin Cherry", "Bill Dolan"], "venue": "In Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the Association for Computational Linguistics,", "citeRegEx": "Ritter et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Ritter et al\\.", "year": 2010}, {"title": "Deep learning in neural networks: An overview", "author": ["J\u00fcrgen Schmidhuber"], "venue": "Neural Networks,", "citeRegEx": "Schmidhuber.,? \\Q2015\\E", "shortCiteRegEx": "Schmidhuber.", "year": 2015}, {"title": "An analysis of domestic abuse discourse on reddit", "author": ["Cecilia Ovesdotter Alm", "Ray Ptucha", "Christopher Homan"], "venue": "In Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing,", "citeRegEx": "Schrading et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Schrading et al\\.", "year": 2015}, {"title": "Building end-to-end dialogue systems using generative hierarchical neural network models. arXiv preprint arXiv:1507.04808", "author": ["Alessandro Sordoni", "Yoshua Bengio", "Aaron Courville", "Joelle Pineau"], "venue": null, "citeRegEx": "Serban et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Serban et al\\.", "year": 2015}, {"title": "2015b. A survey of available corpora for building data-driven dialogue systems. arXiv preprint arXiv:1512.05742", "author": ["Ryan Lowe", "Laurent Charlin", "Joelle Pineau"], "venue": null, "citeRegEx": "Serban et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Serban et al\\.", "year": 2015}, {"title": "Neural responding machine for shorttext conversation", "author": ["Shang et al.2015] Lifeng Shang", "Zhengdong Lu", "Hang Li"], "venue": "arXiv preprint arXiv:1503.02364", "citeRegEx": "Shang et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Shang et al\\.", "year": 2015}, {"title": "A neural network approach to context-sensitive generation of conversational responses", "author": ["Michel Galley", "Michael Auli", "Chris Brockett", "Yangfeng Ji", "Margaret Mitchell", "Jian-Yun Nie", "Jianfeng Gao", "Bill Dolan"], "venue": null, "citeRegEx": "Sordoni et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Sordoni et al\\.", "year": 2015}, {"title": "Sequence to sequence learning with neural networks. In Advances in neural information processing systems, pages 3104\u20133112", "author": ["Oriol Vinyals", "Quoc V Le"], "venue": null, "citeRegEx": "Sutskever et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Sutskever et al\\.", "year": 2014}, {"title": "A neural conversational model. arXiv preprint arXiv:1506.05869", "author": ["Vinyals", "Le2015] Oriol Vinyals", "Quoc Le"], "venue": null, "citeRegEx": "Vinyals et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Vinyals et al\\.", "year": 2015}, {"title": "An annotated corpus of film dialogue for learning and characterizing character style", "author": ["Grace Lin", "Jennifer Sawyer"], "venue": null, "citeRegEx": "Walker et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Walker et al\\.", "year": 2012}, {"title": "The anatomy of ALICE", "author": ["Richard S Wallace"], "venue": null, "citeRegEx": "Wallace.,? \\Q2009\\E", "shortCiteRegEx": "Wallace.", "year": 2009}, {"title": "Elizaa computer program for the study of natural language communication between man and machine", "author": ["Joseph Weizenbaum"], "venue": "Communications of the ACM,", "citeRegEx": "Weizenbaum.,? \\Q1966\\E", "shortCiteRegEx": "Weizenbaum.", "year": 1966}, {"title": "A Network-based End-to-End Trainable Task-oriented Dialogue System. ArXiv eprints, April", "author": ["S. Young"], "venue": null, "citeRegEx": "Young.,? \\Q2016\\E", "shortCiteRegEx": "Young.", "year": 2016}, {"title": "Attention with intention for a neural network conversation model", "author": ["Yao et al.2015] Kaisheng Yao", "Geoffrey Zweig", "Baolin Peng"], "venue": "arXiv preprint arXiv:1510.08565", "citeRegEx": "Yao et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Yao et al\\.", "year": 2015}], "referenceMentions": [{"referenceID": 27, "context": "For decades, conversational agent design was dominated by systems that rely on knowledge bases and rule-based mechanisms to understand human inputs and generate reasonable responses (Weizenbaum, 1966; Parkinson et al., 1977; Wallace, 2009).", "startOffset": 182, "endOffset": 239}, {"referenceID": 15, "context": "For decades, conversational agent design was dominated by systems that rely on knowledge bases and rule-based mechanisms to understand human inputs and generate reasonable responses (Weizenbaum, 1966; Parkinson et al., 1977; Wallace, 2009).", "startOffset": 182, "endOffset": 239}, {"referenceID": 26, "context": "For decades, conversational agent design was dominated by systems that rely on knowledge bases and rule-based mechanisms to understand human inputs and generate reasonable responses (Weizenbaum, 1966; Parkinson et al., 1977; Wallace, 2009).", "startOffset": 182, "endOffset": 239}, {"referenceID": 10, "context": "Recent work in data-driven models focuses on modeling the next response as a function of the preceding message (Vinyals and Le, 2015; Li et al., 2015).", "startOffset": 111, "endOffset": 150}, {"referenceID": 16, "context": "Unlike previous efforts that used Twitter as a source of conversations (Ritter et al., 2010), Reddit does not have length constraints, allowing more natural text.", "startOffset": 71, "endOffset": 92}, {"referenceID": 1, "context": "We also jointly learn a shared word embedding space (Bengio et al., 2006) and a user embedding space.", "startOffset": 52, "endOffset": 73}, {"referenceID": 14, "context": "Other approaches focused on domain specific tasks such as games (Narasimhan et al., 2015) and restaurants (Wen et al.", "startOffset": 64, "endOffset": 89}, {"referenceID": 4, "context": ", 2015) and restaurants (Wen et al., 2016; Cuay\u00e1huitl, 2016)", "startOffset": 24, "endOffset": 60}, {"referenceID": 25, "context": "Writing styles quantified by word length, verb strength, polarity, and distribution of dialogue acts have been used to model users (Walker et al., 2012).", "startOffset": 131, "endOffset": 152}, {"referenceID": 2, "context": "Other efforts focused on building a user profile based on demographics, such as gender, income, age, and marital status (Bonin et al., 2014).", "startOffset": 120, "endOffset": 140}, {"referenceID": 23, "context": "With the introduction of the sequence-tosequence framework (Sutskever et al., 2014), many recent learning systems have used recurrent neural networks (RNNs) to generate novel responses given an input message or sentence.", "startOffset": 59, "endOffset": 83}, {"referenceID": 20, "context": "With the introduction of the sequence-tosequence framework (Sutskever et al., 2014), many recent learning systems have used recurrent neural networks (RNNs) to generate novel responses given an input message or sentence. For example, Vinyals and Le (2015) proposed using IT desk chat logs as a dataset to train LSTM network to generate new sentences.", "startOffset": 60, "endOffset": 256}, {"referenceID": 20, "context": "Sordoni et al. (2015) constructed Twitter conversations limiting the history context to one message.", "startOffset": 0, "endOffset": 22}, {"referenceID": 19, "context": "To eliminate the need for a language model, Serban et al. (2015a) tried end-to-end training on an RNN encoder-decoder network.", "startOffset": 44, "endOffset": 66}, {"referenceID": 10, "context": "Often responses gravitate to the most frequent sentences observed in the training corpus (Li et al., 2015).", "startOffset": 89, "endOffset": 106}, {"referenceID": 29, "context": "Perplexity, BLEU, and deltaBLEU, adapted from language modeling and machine translation communities, have been used for evaluating novel responses (Yao et al., 2015; Sordoni et al., 2015; Galley et al., 2015).", "startOffset": 147, "endOffset": 208}, {"referenceID": 22, "context": "Perplexity, BLEU, and deltaBLEU, adapted from language modeling and machine translation communities, have been used for evaluating novel responses (Yao et al., 2015; Sordoni et al., 2015; Galley et al., 2015).", "startOffset": 147, "endOffset": 208}, {"referenceID": 5, "context": "Perplexity, BLEU, and deltaBLEU, adapted from language modeling and machine translation communities, have been used for evaluating novel responses (Yao et al., 2015; Sordoni et al., 2015; Galley et al., 2015).", "startOffset": 147, "endOffset": 208}, {"referenceID": 21, "context": "While the search for better metrics is still on going, automatic evaluation of response generation stays an open problem (Shang et al., 2015).", "startOffset": 121, "endOffset": 141}, {"referenceID": 7, "context": "Typically, a positive response is mixed with random responses, and then the system is asked to score the right response higher than others (Hu et al., 2014; Kadlec et al., 2015).", "startOffset": 139, "endOffset": 177}, {"referenceID": 8, "context": "Typically, a positive response is mixed with random responses, and then the system is asked to score the right response higher than others (Hu et al., 2014; Kadlec et al., 2015).", "startOffset": 139, "endOffset": 177}, {"referenceID": 18, "context": "Serban et al. (2015b) surveyed 56 datasets and found that only 9 have more than 100,000 conversations, only one having more than 5 million conversations.", "startOffset": 0, "endOffset": 22}, {"referenceID": 18, "context": "For example, Schrading et al. (2015) extracted comments from a small number of subreddits to build a classifier that identifies domestic", "startOffset": 13, "endOffset": 37}, {"referenceID": 12, "context": "Unlike the Ubuntu dataset, logs of technical chat rooms (Lowe et al., 2015), Reddit conversations tend to be more diverse in regard to topics and user backgrounds.", "startOffset": 56, "endOffset": 75}, {"referenceID": 11, "context": "There are several approaches to ranking: pointwise, pairwise, and list-wise (Liu, 2009).", "startOffset": 76, "endOffset": 87}, {"referenceID": 8, "context": "Kadlec et al. (2015) chose pointwise ranking for its simplicity, and we follow the same choice for its speed benefits, which are necessary for training on hundreds of millions of examples.", "startOffset": 0, "endOffset": 21}, {"referenceID": 3, "context": "The hidden layer parameters, the ngram embeddings, and the user embeddings are trained jointly, and we use stochastic gradient descent (SGD) to optimize the parameters of our models (Bottou, 1991).", "startOffset": 182, "endOffset": 196}, {"referenceID": 6, "context": "The motivation behind the multi-loss model is to prevent adaptation between features (Hinton et al., 2012).", "startOffset": 85, "endOffset": 106}], "year": 2016, "abstractText": "We investigate the task of modeling opendomain, multi-turn, unstructured, multiparticipant, conversational dialogue. We specifically study the effect of incorporating different elements of the conversation. Unlike previous efforts, which focused on modeling messages and responses, we extend the modeling to long context and participant\u2019s history. Our system does not rely on handwritten rules or engineered features; instead, we train deep neural networks on a large conversational dataset. In particular, we exploit the structure of Reddit comments and posts to extract 2.1 billion messages and 133 million conversations. We evaluate our models on the task of predicting the next response in a conversation, and we find that modeling both context and participants improves prediction accuracy.", "creator": "LaTeX with hyperref package"}}}