{"id": "1606.00499", "review": {"conference": "EMNLP", "VERSION": "v1", "DATE_OF_SUBMISSION": "1-Jun-2016", "title": "Generalizing and Hybridizing Count-based and Neural Language Models", "abstract": "Language models (LMs) are statistical models that calculate probabilities over sequences of words or other discrete symbols. Currently two major paradigms for language modeling exist: count-based n-gram models, which have advantages of scalability and test-time speed, and neural LMs, which often achieve superior modeling performance. We demonstrate how both varieties of models can be unified in a single modeling framework that defines a set of probability distributions over the vocabulary of words, and then dynamically calculates mixture weights over these distributions over the sequence of words, in which different weights are given. This approach offers a variety of approaches to this task, but it also proposes different approaches to the computational models involved.\n\n\n\n\n\n\nThe distribution model of the words (and the distribution model of the word) is a multivariate regression model that can be constructed using statistical model, with only the parameter set. In a single model, each word is divided into two groups: the more significant and the more significant the likelihood that each group of word has different distribution weights. For the purpose of testing, each value of the distribution model can be divided into two groups: the more likely to have different distribution weights. For the purpose of testing, each value of the distribution model can be divided into two groups: the more likely to have different distribution weights. For the purpose of testing, each value of the distribution model can be divided into two groups: the more likely to have different distribution weights. For the purpose of testing, each value of the distribution model can be divided into two groups: the more likely to have different distribution weights. For the purpose of testing, each value of the distribution model can be divided into two groups: the more likely to have different distribution weights. For the purpose of testing, each value of the distribution model can be divided into two groups: the more likely to have different distribution weights. For the purpose of testing, each value of the distribution model can be divided into two groups: the more likely to have different distribution weights. For the purpose of testing, each value of the distribution model can be divided into two groups: the more likely to have different distribution weights. For the purpose of testing, each value of the distribution model can be divided into two groups: the more likely to have different distribution weights. For the purpose of testing, each value of the distribution model can be divided into two groups: the more likely to have different distribution weights. For the purpose of testing, each value of the distribution model can be divided into two groups: the more", "histories": [["v1", "Wed, 1 Jun 2016 23:26:20 GMT  (79kb,D)", "http://arxiv.org/abs/1606.00499v1", null], ["v2", "Mon, 26 Sep 2016 01:48:57 GMT  (89kb,D)", "http://arxiv.org/abs/1606.00499v2", "Presented at EMNLP2016"]], "reviews": [], "SUBJECTS": "cs.CL", "authors": ["graham neubig", "chris dyer"], "accepted": true, "id": "1606.00499"}, "pdf": {"name": "1606.00499.pdf", "metadata": {"source": "CRF", "title": "Generalizing and Hybridizing Count-based and Neural Language Models", "authors": ["Graham Neubig", "Chris Dyer"], "emails": [], "sections": [{"heading": "1 Introduction", "text": "Language models (LMs) are statistical models that, given a sentence wI1 := w1, . . . , wI , calculate its probability P (wI1). LMs are widely used in applications such as machine translation and speech recognition, and because of their broad applicability they have also been widely studied in the literature. The most traditional and broadly used language modeling paradigm is that of count-based LMs, usually smoothed n-grams (Witten and Bell, 1991; Chen and Goodman, 1996). Recently, there has been a focus on LMs based on neural networks (Nakamura et al., 1990; Bengio et al., 2006; Mikolov et al., 2010), which have shown impressive improvements in performance over count-based LMs. On the other\nhand, these neural LMs also come at the cost of increased computational complexity at both training and test time, and even the largest reported neural LMs (Chen et al., 2015; Williams et al., 2015) are trained on a fraction of the data of their count-based counterparts (Brants et al., 2007).\nIn this paper we focus on a class of LMs, which we will call mixture of distributions LMs (MODLMs; \u00a72). Specifically, we define MODLMs as all LMs that take the following form, calculating the probabilities of the next word in a sentence wi given preceding context c according to a mixture of several component probability distributions Pk(wi|c):\nP (wi|c) = K\u2211 k=1 \u03bbk(c)Pk(wi|c). (1)\nHere, \u03bbk(c) is a function that defines the mixture weights, with the constraint that \u2211K k=1 \u03bbk(c) = 1 for all c. This form is not new in itself, and widely used both in the calculation of smoothing coefficients for n-gram LMs (Chen and Goodman, 1996), and interpolation of LMs of various varieties (Jelinek and Mercer, 1980).\nThe main contribution of this paper is to demonstrate that depending on our definition of c, \u03bbk(c), and Pk(wi|c), Eq. 1 can be used to describe not only n-gram models, but also feed-forward (Nakamura et al., 1990; Bengio et al., 2006; Schwenk, 2007) and recurrent (Mikolov et al., 2010; Sundermeyer et al., 2012) neural network LMs (\u00a73). This observation is useful theoretically, as it provides a single mathematical framework that encompasses several widely used classes of LMs. It is also useful practically, in\nar X\niv :1\n60 6.\n00 49\n9v 1\n[ cs\n.C L\n] 1\nJ un\n2 01\nthat this new view of these traditional models allows us to create new models that combine the desirable features of n-gram and neural models, such as:\nneurally interpolated n-gram LMs (\u00a74.1), which learn the interpolation weights of n-gram models using neural networks, and\nneural/n-gram hybrid LMs (\u00a74.2), which add a count-based n-gram component to neural models, allowing for flexibility to add large-scale external data sources to neural LMs.\nWe discuss learning methods for these models (\u00a75) including a novel method of randomly dropping out more easy-to-learn distributions to prevent the parameters from falling into sub-optimal local minima.\nExperiments on language modeling benchmarks (\u00a76) find that these models outperform baselines in terms of performance and convergence speed."}, {"heading": "2 Mixture of Distributions LMs", "text": "As mentioned above, MODLMs are LMs that take the form of Eq. 1. This can be re-framed as the following matrix-vector multiplication:\np\u1d40c = Dc\u03bb \u1d40 c,\nwhere pc is a vector with length equal to vocabulary size, in which the jth element pc,j corresponds to P (wi = j|c), \u03bbc is a size K vector that contains the mixture weights for the distributions, and Dc is a Jby-K matrix, where element dc,j,k is equivalent to the probability Pk(wi = j|c).1 An example of this formulation is shown in Fig. 1.\nNote that all columns in D represent probability distributions, and thus must sum to one over the J words in the vocabulary, and that all \u03bb must sum to 1 over the K distributions. Under this condition, the vector pwill represent a well-formed probability distribution as well. This conveniently allows us to calculate the probability of a single word wi = j by calculating the product of the jth row of Dc and \u03bb \u1d40 c\nPk(wi = j|c) = dc,j\u03bb\u1d40c.\nIn the sequel we show how this formulation can be used to describe several existing LMs (\u00a73) as well as several novel model structures that are more powerful and general than these existing models (\u00a74).\n1We omit the subscript c when appropriate."}, {"heading": "3 Existing LMs as Linear Mixtures", "text": "3.1 n-gram LMs as Mixtures of Distributions First, we discuss how count-based interpolated ngram LMs fit within the MODLM framework.\nMaximum likelihood estimation: n-gram models predict the next word based on the previous N -1 words. In other words, we set c = wi\u22121i\u2212N+1 and calculate P (wi|wi\u22121i\u2212N+1). The maximum-likelihood (ML) estimate for this probability is\nPML(wi|wi\u22121i\u2212N+1) = c(w i i\u2212N+1)/c(w i\u22121 i\u2212N+1),\nwhere c(\u00b7) counts frequency in the training corpus. Interpolation: Because ML estimation assigns zero probability to word sequences where c(wii\u2212N+1) = 0, n-gram models often interpolate the ML distributions for sequences of length 1 to N . The simplest form is static interpolation\nP (wi|wi\u22121i\u2212n+1) = N\u2211\nn=1\n\u03bbS,nPML(wi|wi\u22121i\u2212n+1). (2)\n\u03bbS is a vector where \u03bbS,n represents the weight put on the distribution PML(wi|wi\u22121i\u2212n+1). This can be expressed as linear equations (Fig. 2a) by setting the nth column of D to the ML distribution PML(wi|wi\u22121i\u2212n+1), and \u03bb(c) equal to \u03bbS .\nStatic interpolation can be improved by calculating \u03bb(c) dynamically, using heuristics based on the frequency counts of the context (Good, 1953; Katz, 1987; Witten and Bell, 1991). These methods define a context-sensitive fallback probability \u03b1(wi\u22121i\u2212n+1) for order n models, and recursively calculate the probability of the higher order models from the lower order models:\nP (wi|wi\u22121i\u2212n+1) = \u03b1(w i\u22121 i\u2212n+1)P (wi|w i\u22121 i\u2212n+2)+\n(1\u2212 \u03b1(wi\u22121i\u2212n+1))PML(wi|w i\u22121 i\u2212n+1). (3)\nTo express this as a linear mixture, we convert \u03b1(wi\u22121i\u2212n+1) into the appropriate value for \u03bbn(w i\u22121 i\u2212N+1). Specifically, the probability assigned to each PML(wi|wi\u22121i\u2212n+1) is set to the product of the fallbacks \u03b1 for all higher orders and the probability of not falling back (1\u2212 \u03b1) at the current level:\n\u03bbn(w i\u22121 i\u2212N+1) = (1\u2212\u03b1(w i\u22121 i\u2212n+1)) N\u220f n\u0303=n+1 \u03b1(wi\u22121i\u2212n\u0303+1).\nDiscounting: The widely used technique of discounting (Ney et al., 1994) defines a fixed discount d and subtracts it from the count of each word before calculating probabilities:\nPD(wi|wi\u22121i\u2212n+1) = (c(w i i\u2212n+1)\u2212 d)/c(wi\u22121i\u2212n+1).\nDiscounted LMs then assign the remaining probability mass after discounting as the fallback probability\n\u03b2D(w i\u22121 i\u2212n+1) =1\u2212 J\u2211 j=1 PD(wi = j|wi\u22121i\u2212n+1),\nP (wi|wi\u22121i\u2212n+1) =\u03b2D(w i\u22121 i\u2212n+1)P (wi|w i\u22121 i\u2212n+2)+\nPD(wi|wi\u22121i\u2212n+1). (4)\nIn this case, PD(\u00b7) does not add to one, and thus violates the conditions for MODLMs stated in \u00a72, but it is easy to turn discounted LMs into interpolated LMs by normalizing the discounted distribution:\nPND(wi|wi\u22121i\u2212n+1) = PD(wi|wi\u22121i\u2212n+1)\u2211J\nj=1 PD(wi = j|w i\u22121 i\u2212n+1)\n,\nwhich allows us to replace \u03b2(\u00b7) for \u03b1(\u00b7) and PND(\u00b7) for PML(\u00b7) in Eq. 3, and proceed as normal.\nKneser\u2013Ney (KN; Kneser and Ney (1995)) and Modified KN (Chen and Goodman, 1996) smoothing further improve discounted LMs by adjusting the counts of lower-order distributions to more closely match their expectations as fallbacks for higher order distributions. Modified KN is currently the defacto standard in n-gram LMs despite occasional improvements (Teh, 2006; Durrett and Klein, 2011), and we will express it as PKN (\u00b7)."}, {"heading": "3.2 Neural LMs as Mixtures of Distributions", "text": "In this section we demonstrate how neural network LMs can also be viewed as an instantiation of the MODLM framework.\nFeed-forward neural network LMs: Feedforward LMs (Bengio et al., 2006; Schwenk, 2007) are LMs that, like n-grams, calculate the probability of the next word based on the previous words. Given context wi\u22121i\u2212N+1, these words are converted into real-valued word representation vectors ri\u22121i\u2212N+1, which are concatenated into an overall representation vector q = \u2295(ri\u22121i\u2212N+1), where \u2295(\u00b7) is the vector concatenation function. q is then run through a series of affine transforms and nonlinearities defined as function NN(q) to obtain a vector h. For example, for a one-layer neural network with a tanh non-linearity we can define\nNN(q) := tanh(qWq + bq), (5)\nwhere Wq and bq are weight matrix and bias vector parameters respectively. Finally, the probability vector p is calculated using the softmax function p = softmax(hWs + bs), similarly parameterized.\nAs these models are directly predicting p with no concept of mixture weights \u03bb, they cannot be interpreted as MODLMs as-is. However, we can per-\nform a trick shown in Fig. 2b, not calculating p directly, but instead calculating mixture weights \u03bb = softmax(hWs + bs), and defining the MODLM\u2019s distribution matrix D as a J-by-J identity matrix. This is equivalent to defining a linear mixture of J Kronecker \u03b4j distributions, the jth of which assigns a probability of 1 to word j and zero to everything else, and estimating the mixture weights with a neural network. While it may not be clear why it is useful to define neural LMs in this somewhat roundabout way, we describe in \u00a74 how this opens up possibilities for novel expansions to standard models.\nRecurrent neural network LMs: LMs using recurrent neural networks (RNNs) (Mikolov et al., 2010) consider not the previous few words, but also maintain a hidden state summarizing the sentence up until this point by re-defining the net in Eq. 5 as\nRNN(qi) := tanh(qiWq + hi\u22121Wh + bq),\nwhere qi is the current input vector and hi\u22121 is the hidden vector at the previous time step. This allows for consideration of long-distance dependencies beyond the scope of standard n-grams, and LMs using RNNs or long short-term memory (LSTM) networks (Sundermeyer et al., 2012) have posted large improvements over standard n-grams and feed-forward models. Like feed-forward LMs, LMs using RNNs can be expressed as MODLMs by predicting \u03bb instead of predicting p directly."}, {"heading": "4 Novel Applications of MODLMs", "text": "This section describes how we can use this framework of MODLMs to design new varieties of LMs that combine the advantages of both n-gram and neural network LMs.\n4.1 Neurally Interpolated n-gram Models\nThe first novel instantiation of MODLMs that we propose is neurally interpolated n-gram models, shown in Fig. 3a. In these models, we setD to be the same matrix used in n-gram LMs, but calculate\u03bb(c) using a neural network model. As \u03bb(c) is learned from data, this framework has the potential to allow us to learn more intelligent interpolation functions than the heuristics described in \u00a73.1. In addition, because the neural network only has to calculate a softmax over N distributions instead of J vocabulary words, training and test efficiency of these models can be expected to be much greater than that of standard neural network LMs.\nWithin this framework, there are several design decisions. First, how we decide D: do we use the maximum likelihood estimate PML or KN estimated distributions PKN? Second, what do we provide as input to the neural network to calculate the mixture weights? To provide the neural net with the same information used by interpolation heuristics used in traditional LMs, we first calculate three features for each of the N contexts wi\u22121i\u2212n+1: a binary feature indicating whether the context has been observed in the training corpus (c(wi\u22121i\u2212n+1) > 0), the log frequency of the context counts (log(c(wi\u22121i\u2212n+1)) or zero for unobserved contexts), and the log frequency of the number of unique words following the context (log(u(wi\u22121i\u2212n+1)) or likewise zero). When using discounted distributions, we also use the log of the sum of the discounted counts as a feature. We can also optionally use the word representation vector q used in neural LMs, allowing for richer representation of the input, but this may or may not be necessary in the face of the already informative count-based features.\n4.2 Neural/n-gram Hybrid Models Our second novel model enabled by MODLMs is neural/n-gram hybrid models, shown in Fig. 3b. These models are similar to neurally interpolated n-grams, but D is augmented with J additional columns representing the Kronecker \u03b4j distributions used in the standard neural LMs. In this construction, \u03bb is still a stochastic vector, but its contents are both the mixture coefficients for the count-based models and direct predictions of the probabilities of words. Thus, the learned LM can use count-based models when they are deemed accurate, and deviate from them when deemed necessary.\nThis model is attractive conceptually for several reasons. First, it has access to all information used by both neural and n-gram LMs, and should be able to perform as well or better than both models. Second, the efficiently calculated n-gram counts are likely sufficient to capture many phenomena necessary for language modeling, allowing the neural component to focus on learning only the phenomena that are not well modeled by n-grams, requiring fewer parameters and less training time. Third, it is possible to train n-grams from much larger amounts of data, and use these massive models to bootstrap learning of neural nets on smaller datasets."}, {"heading": "5 Learning Mixtures of Distributions", "text": "While the MODLM formulations of standard heuristic n-gram LMs do not require learning, the remaining models are parameterized. This section discusses the details of learning these parameters."}, {"heading": "5.1 Learning MODLMs", "text": "The first step in learning parameters is defining our training objective. Like most previous work on LMs (Bengio et al., 2006), we use a negative loglikelihood loss summed over words wi in every sentence w in corpusW\nL(W) = \u2212 \u2211 w\u2208W \u2211 wi\u2208w logP (wi|c),\nwhere c represents all words preceding wi inw that are used in the probability calculation. As noted in Eq. 2, P (wi = j|c) can be calculated efficiently from the distribution matrix Dc and mixture function output \u03bbc.\nGiven that we can calculate the log likelihood, the remaining parts of training are similar to training for standard neural network LMs. As usual, we perform forward propagation to calculate the probabilities of all the words in the sentence, back-propagate the gradients through the computation graph, and perform some variant of stochastic gradient descent (SGD) to update the parameters."}, {"heading": "5.2 Block Dropout for Hybrid Models", "text": "While the training method described in the previous section is similar to that of other neural network models, we make one important modification to the training process specifically tailored to the hybrid models of \u00a74.2.\nThis is motivated by our observation (detailed in \u00a76.3) that the hybrid models, despite being strictly more expressive than the corresponding neural network LMs, were falling into poor local minima with higher training error than neural network LMs. This is because at the very beginning of training, the count-based elements of the distribution matrix in Fig. 3b are already good approximations of the target distribution, while the weights of the single-word \u03b4j distributions are not yet able to provide accurate probabilities. Thus, the model learns to set the mixture proportions of the \u03b4 elements to near zero and rely mainly on the count-based n-gram distributions.\nTo encourage the model to use the \u03b4 mixture components, we adopt a method called block dropout (Ammar et al., 2016). In contrast to standard dropout (Srivastava et al., 2014), which drops out single nodes or connections, block dropout randomly drops out entire subsets of network nodes. In our case, we want to prevent the network from overusing the count-based n-gram distributions, so for a randomly selected portion of the training examples (here, 50%) we disable all n-gram distributions and force the model to rely on only the \u03b4 distributions. To do so, we zero out all elements in \u03bb(c) that correspond to n-gram distributions, and re-normalize over the rest of the elements so they sum to one."}, {"heading": "5.3 Network and Training Details", "text": "Finally, we note design details that were determined based on preliminary experiments.\nNetwork structures: We used both feed-forward networks with tanh non-linearities and LSTM\n(Hochreiter and Schmidhuber, 1997) networks. Most experiments used single-layer 200-node networks, and 400-node networks were used for experiments with larger training data. Word representations were the same size as the hidden layer. Larger and multi-layer networks did not yield improvements.\nTraining: We used ADAM (Kingma and Ba, 2015) with a learning rate of 0.001, and minibatch sizes of 512 words. This led to faster convergence than standard SGD, and more stable optimization than other update rules. Models were evaluated every 500k-3M words, and the model with the best development likelihood was used. In addition to the block dropout of \u00a75.2, we used standard dropout with a rate of 0.5 for both feed-forward (Srivastava et al., 2014) and LSTM (Pham et al., 2014) nets in the neural LMs and neural/n-gram hybrids, but not in the neurally interpolated n-grams, where it resulted in slightly worse perplexities.\nFeatures: If parameters are learned on the data used to train count-based models, they will heavily over-fit and learn to trust the count-based distributions too much. To prevent this, we performed 10-fold cross validation, calculating count-based elements of D for each fold with counts trained on the other 9/10. In addition, the count-based contextual features in \u00a74.1 were normalized by subtracting the training set mean, which improved performance."}, {"heading": "6 Experiments", "text": ""}, {"heading": "6.1 Experimental Setup", "text": "In this section, we perform experiments to evaluate the neurally interpolated n-grams (\u00a76.2) and neural/n-gram hybrids (\u00a76.3), the ability of our models to take advantage of information from large data sets (\u00a76.4), and the relative performance compared to post-facto static interpolation of already-trained models (\u00a76.5). For the main experiments, we evaluate on two corpora: the Penn Treebank (PTB) data set prepared by Mikolov et al. (2010),2 and the first 100k sentences in the English side of the ASPEC corpus (Nakazawa et al., 2015)3 (details in Tab. 1). The PTB corpus uses the standard vocabulary of 10k words, and for the ASPEC corpus we use a vocabu-\n2http://rnnlm.org/simple-examples.tgz 3http://lotus.kuee.kyoto-u.ac.jp/ASPEC/\nlary of the 20k most frequent words. Our implementation is included as supplementary material.\n6.2 Results for Neurally Interpolated n-grams First, we investigate the utility of neurally interpolated n-grams. In all cases, we use a history of N = 5 and test several different settings for the models:\nEstimation type: \u03bb(c) is calculated with heuristics (HEUR) or by the proposed method using feedforward (FF), or LSTM nets.\nDistributions: We compare PML(\u00b7) and PKN (\u00b7). For heuristics, we use Witten-Bell for ML and the appropriate discounted probabilities for KN.\nInput features: As input features for the neural network, we either use only the count-based features (C) or count-based features together with the word representation for the single previous word (CR).\nFrom the results shown in Tab. 2, we can first see that when comparing models using the same set of input distributions, the neurally interpolated model outperforms corresponding heuristic methods. We can also see that LSTMs have a slight advantage over FF nets, and models using word representations have a slight advantage over those that use only the count-based features. Overall, the best model achieves a relative perplexity reduction of 4- 5% over KN models. Interestingly, even when using simple ML distributions, the best neurally interpolated n-gram model nearly matches the heuristic KN method, demonstrating that the proposed model can\nautomatically learn interpolation functions that are nearly as effective as carefully designed heuristics.4\n6.3 Results for Neural/n-gram Hybrids\nIn experiments with hybrid models, we test a neural/n-gram hybrid LM using LSTM networks with both Kronecker \u03b4 and KN smoothed 5-gram distributions, trained either with or without block dropout. As our main baseline, we compare to LSTMs with only \u03b4 distributions, which have reported competitive numbers on the PTB data set (Zaremba et al., 2014).5 We also report results for heuristically smoothed KN 5-gram models, and the best neurally interpolated n-grams from the previous section for reference.\nThe results, shown in Tab. 3, demonstrate that similarly to previous research, LSTM LMs (2) achieve a large improvement in perplexity over ngram models, and that the proposed neural/n-gram hybrid method (5) further reduces perplexity by 10- 11% relative over this strong baseline.\nComparing models without (4) and with (5) the proposed block dropout, we can see that this method contributes significantly to these gains. To examine this more closely, we show the test perplexity for the\n4 Neurally interpolated n-grams are also more efficient than standard neural LMs, as mentioned in \u00a74.1. While a standard LSTM LM calculated 1.4kw/s on the PTB data, the neurally interpolated models using LSTMs and FF nets calculated 11kw/s and 58kw/s respectively, only slightly inferior to 140kw/s of heuristic KN.\n5 Note that unlike this work, we opt to condition only on insentence context, not inter-sentential dependencies, as training through gradient calculations over sentences is more straightforward and because examining the effect of cross-boundary information is not central to the proposed method. Thus our baseline numbers are not directly comparable (i.e. have higher perplexity) to previous reported results on this data, but we still feel that the comparison is appropriate.\nthree models using \u03b4 distributions in Tab. 4, and the amount of the probability mass in \u03bb(c) assigned to the non-\u03b4 distributions in the hybrid models. From this, we can see that the model with block dropout quickly converges to a better result than the LSTM LM, but the model without converges to a worse result, assigning too much probability mass to the dense count-based distributions, demonstrating the learning problems mentioned in \u00a75.2."}, {"heading": "6.4 Results for Larger Data Sets", "text": "To examine the ability of the hybrid models to use counts trained over larger amounts of data, we perform experiments using two larger data sets:\nWSJ: The PTB uses data from the 1989 Wall Street Journal, so we add the remaining years between 1987 and 1994 (1.81M sents., 38.6M words).\nGW: News data from the English Gigaword 5th Edition (LDC2011T07, 59M sents., 1.76G words).\nWe incorporate this data either by training net parameters over the whole large data, or by separately training count-based n-grams on each of PTB, WSJ, and GW, and learning net parameters on only PTB data. The former has the advantage of training the net on much larger data. The latter has two main advantages: 1) when the smaller data is of a particular domain the mixture weights can be learned to match this in-domain data; 2) distributions can be trained on data such as Google n-grams (LDC2006T13), which contain n-gram counts but not full sentences.\nIn the results of Fig. 5, we can first see that the neural/n-gram hybrids significantly outperform the traditional neural LMs in the scenario with larger data as well. Comparing the two methods for in-\ncorporating larger data, we can see that the results are mixed depending on the type and size of the data being used. For the WSJ data, training on all data slightly outperforms the method of adding distributions, but when the GW data is added this trend reverses. This can be explained by the fact that the GW data differs from the PTB test data, and thus the effect of choosing domain-specific interpolation coefficients was more prominent."}, {"heading": "6.5 Comparison with Static Interpolation", "text": "Finally, because the proposed neural/n-gram hybrid models combine the advantages of neural and ngram models, we compare with the more standard method of training models independently and combining them with static interpolation weights tuned on the validation set using the EM algorithm. Tab. 4 shows perplexities for combinations of a standard neural model (or \u03b4 distributions) trained on PTB, and count based distributions trained on PTB, WSJ, and GW are added one-by-one using the standard static and proposed LSTM interpolation methods. From the results, we can see that when only PTB data is used, the methods have similar results, but with the more diverse data sets the proposed method edges out its static counterpart.6\n6In addition to better perplexities, neural/n-gram hybrids are trained in a single pass instead of performing post-facto interpolation, which may give advantages when training for other objectives (Auli and Gao, 2014; Li et al., 2015)."}, {"heading": "7 Related Work", "text": "A number of alternative methods focus on interpolating LMs of multiple varieties such as in-domain and out-of-domain LMs (Bulyko et al., 2003; Bacchiani et al., 2006; Gu\u0308lc\u0327ehre et al., 2015). Perhaps most relevant is Hsu (2007)\u2019s work on learning to interpolate multiple LMs using log-linear models. This differs from our work in that it learns functions to estimate the fallback probabilities \u03b1n(c) in Eq. 3 instead of \u03bb(c), and does not cover interpolation of n-gram components, non-linearities, or the connection with neural network LMs. Also conceptually similar is work on adaptation of n-gram LMs, which start with n-gram probabilities (Della Pietra et al., 1992; Kneser and Steinbiss, 1993; Rosenfeld, 1996; Iyer and Ostendorf, 1999) and adapt them based on the distribution of the current document, albeit in a linear model. Finally, recent works have compared n-gram and neural models, finding that neural models often perform better in perplexity, but n-grams have their own advantages such as effectiveness in extrinsic tasks (Baltescu and Blunsom, 2015) and better modeling of rare words (Chen et al., 2015)."}, {"heading": "8 Conclusion and Future Work", "text": "In this paper, we proposed a framework for language modeling that generalizes both neural network and count-based n-gram LMs. This allowed us to learn more effective interpolation functions for count-based n-grams, and to create neural LMs that incorporate information from count-based models.\nAs the framework discussed here is general, it is also possible that they could be used in other tasks that perform sequential prediction of words such as neural machine translation (Sutskever et al., 2014) or dialog response generation (Sordoni et al., 2015). In addition, given the positive results using block dropout for hybrid models, we plan to develop more effective learning methods for mixtures of sparse and dense distributions."}, {"heading": "Acknowledgements", "text": "We thank Kevin Duh, Austin Matthews, Shinji Watanabe, and anonymous reviewers for valuable comments on earlier drafts. This work was supported in part by JSPS KAKENHI Grant Number 16H05873, and the Program for Advancing Strategic International Networks to Accelerate the Circulation of Talented Researchers."}], "references": [{"title": "Decoder integration and expected bleu training for recurrent neural network language models", "author": ["Auli", "Gao2014] Michael Auli", "Jianfeng Gao"], "venue": "In Proc. ACL,", "citeRegEx": "Auli et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Auli et al\\.", "year": 2014}, {"title": "Map adaptation of stochastic grammars", "author": ["Michael Riley", "Brian Roark", "Richard Sproat"], "venue": "Computer Speech and Language,", "citeRegEx": "Bacchiani et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Bacchiani et al\\.", "year": 2006}, {"title": "Pragmatic neural language modelling in machine translation", "author": ["Baltescu", "Blunsom2015] Paul Baltescu", "Phil Blunsom"], "venue": "In Proc. NAACL,", "citeRegEx": "Baltescu et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Baltescu et al\\.", "year": 2015}, {"title": "Neural probabilistic language models", "author": ["Bengio et al.2006] Yoshua Bengio", "Holger Schwenk", "Jean-S\u00e9bastien Sen\u00e9cal", "Fr\u00e9deric Morin", "Jean-Luc Gauvain"], "venue": "In Innovations in Machine Learning,", "citeRegEx": "Bengio et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Bengio et al\\.", "year": 2006}, {"title": "Large language models in machine translation", "author": ["Ashok C. Popat", "Peng Xu", "Franz J. Och", "Jeffrey Dean"], "venue": "In Proc. EMNLP,", "citeRegEx": "Brants et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Brants et al\\.", "year": 2007}, {"title": "Getting more mileage from web text sources for conversational speech language modeling using class-dependent mixtures", "author": ["Bulyko et al.2003] Ivan Bulyko", "Mari Ostendorf", "Andreas Stolcke"], "venue": "In Proc. HLT,", "citeRegEx": "Bulyko et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Bulyko et al\\.", "year": 2003}, {"title": "An empirical study of smoothing techniques for language modeling", "author": ["Chen", "Goodman1996] Stanley F. Chen", "Joshua Goodman"], "venue": "In Proc. ACL,", "citeRegEx": "Chen et al\\.,? \\Q1996\\E", "shortCiteRegEx": "Chen et al\\.", "year": 1996}, {"title": "Strategies for Training Large Vocabulary Neural Language Models", "author": ["W. Chen", "D. Grangier", "M. Auli"], "venue": null, "citeRegEx": "Chen et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Chen et al\\.", "year": 2015}, {"title": "Adaptive language modeling using minimum", "author": ["Vincent Della Pietra", "Robert L Mercer", "Salim Roukos"], "venue": null, "citeRegEx": "Pietra et al\\.,? \\Q1992\\E", "shortCiteRegEx": "Pietra et al\\.", "year": 1992}, {"title": "An empirical investigation of discounting in cross-domain language models", "author": ["Durrett", "Klein2011] Greg Durrett", "Dan Klein"], "venue": "In Proc. ACL", "citeRegEx": "Durrett et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Durrett et al\\.", "year": 2011}, {"title": "The population frequencies of species and the estimation of population parameters", "author": ["Irving J Good"], "venue": null, "citeRegEx": "Good.,? \\Q1953\\E", "shortCiteRegEx": "Good.", "year": 1953}, {"title": "On using monolingual corpora in neural machine translation. CoRR, abs/1503.03535", "author": ["Orhan Firat", "Kelvin Xu", "Kyunghyun Cho", "Lo\u0131\u0308c Barrault", "Huei-Chi Lin", "Fethi Bougares", "Holger Schwenk", "Yoshua Bengio"], "venue": null, "citeRegEx": "G\u00fcl\u00e7ehre et al\\.,? \\Q2015\\E", "shortCiteRegEx": "G\u00fcl\u00e7ehre et al\\.", "year": 2015}, {"title": "Long short-term memory", "author": ["Hochreiter", "Schmidhuber1997] Sepp Hochreiter", "J\u00fcrgen Schmidhuber"], "venue": "Neural computation,", "citeRegEx": "Hochreiter et al\\.,? \\Q1997\\E", "shortCiteRegEx": "Hochreiter et al\\.", "year": 1997}, {"title": "Generalized linear interpolation of language models", "author": ["Bo-June Hsu"], "venue": "In Proc. ASRU,", "citeRegEx": "Hsu.,? \\Q2007\\E", "shortCiteRegEx": "Hsu.", "year": 2007}, {"title": "Modeling long distance dependence in language: Topic mixtures versus dynamic cache models", "author": ["Iyer", "Ostendorf1999] Rukmini M Iyer", "Mari Ostendorf"], "venue": "Speech and Audio Processing, IEEE Transactions", "citeRegEx": "Iyer et al\\.,? \\Q1999\\E", "shortCiteRegEx": "Iyer et al\\.", "year": 1999}, {"title": "Interpolated estimation of markov source parameters from sparse data. In Workshop on pattern recognition in practice", "author": ["Jelinek", "Mercer1980] Frederick Jelinek", "Robert Mercer"], "venue": null, "citeRegEx": "Jelinek et al\\.,? \\Q1980\\E", "shortCiteRegEx": "Jelinek et al\\.", "year": 1980}, {"title": "Estimation of probabilities from sparse data for the language model component of a speech recognizer", "author": ["Slava M Katz"], "venue": "IEEE Transactions on Acoustics, Speech and Signal Processing,", "citeRegEx": "Katz.,? \\Q1987\\E", "shortCiteRegEx": "Katz.", "year": 1987}, {"title": "Adam: A method for stochastic optimization", "author": ["Kingma", "Ba2015] Diederik Kingma", "Jimmy Ba"], "venue": "Proc. ICLR", "citeRegEx": "Kingma et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Kingma et al\\.", "year": 2015}, {"title": "Improved backing-off for m-gram language modeling", "author": ["Kneser", "Ney1995] Reinhard Kneser", "Hermann Ney"], "venue": "In Proc. ICASSP,", "citeRegEx": "Kneser et al\\.,? \\Q1995\\E", "shortCiteRegEx": "Kneser et al\\.", "year": 1995}, {"title": "On the dynamic adaptation of stochastic language models", "author": ["Kneser", "Steinbiss1993] Reinhard Kneser", "Volker Steinbiss"], "venue": "In Proc. ICASSP,", "citeRegEx": "Kneser et al\\.,? \\Q1993\\E", "shortCiteRegEx": "Kneser et al\\.", "year": 1993}, {"title": "A diversitypromoting objective function for neural conversation models. CoRR, abs/1510.03055", "author": ["Li et al.2015] Jiwei Li", "Michel Galley", "Chris Brockett", "Jianfeng Gao", "Bill Dolan"], "venue": null, "citeRegEx": "Li et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Li et al\\.", "year": 2015}, {"title": "Recurrent neural network based language model", "author": ["Martin Karafi\u00e1t", "Lukas Burget", "Jan Cernock\u1ef3", "Sanjeev Khudanpur"], "venue": "In Proc. InterSpeech,", "citeRegEx": "Mikolov et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Mikolov et al\\.", "year": 2010}, {"title": "Neural network approach to word category prediction for English texts", "author": ["Katsuteru Maruyama", "Takeshi Kawabata", "Kiyohiro Shikano"], "venue": "In Proc. COLING", "citeRegEx": "Nakamura et al\\.,? \\Q1990\\E", "shortCiteRegEx": "Nakamura et al\\.", "year": 1990}, {"title": "On structuring probabilistic dependences in stochastic language modelling", "author": ["Ney et al.1994] Hermann Ney", "Ute Essen", "Reinhard Kneser"], "venue": "Computer Speech and Language,", "citeRegEx": "Ney et al\\.,? \\Q1994\\E", "shortCiteRegEx": "Ney et al\\.", "year": 1994}, {"title": "Dropout improves recurrent neural networks for handwriting recognition", "author": ["Pham et al.2014] Vu Pham", "Th\u00e9odore Bluche", "Christopher Kermorvant", "J\u00e9r\u00f4me Louradour"], "venue": "In Proc. ICFHR,", "citeRegEx": "Pham et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Pham et al\\.", "year": 2014}, {"title": "A maximum entropy approach to adaptive statistical language modelling", "author": ["Ronald Rosenfeld"], "venue": "Computer Speech and Language,", "citeRegEx": "Rosenfeld.,? \\Q1996\\E", "shortCiteRegEx": "Rosenfeld.", "year": 1996}, {"title": "Continuous space language models", "author": ["Holger Schwenk"], "venue": "Computer Speech and Language,", "citeRegEx": "Schwenk.,? \\Q2007\\E", "shortCiteRegEx": "Schwenk.", "year": 2007}, {"title": "A neural network approach to context-sensitive generation of conversational responses", "author": ["Michel Galley", "Michael Auli", "Chris Brockett", "Yangfeng Ji", "Margaret Mitchell", "Jian-Yun Nie", "Jianfeng Gao", "Bill Dolan"], "venue": null, "citeRegEx": "Sordoni et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Sordoni et al\\.", "year": 2015}, {"title": "Dropout: A simple way to prevent neural networks from overfitting", "author": ["Geoffrey Hinton", "Alex Krizhevsky", "Ilya Sutskever", "Ruslan Salakhutdinov"], "venue": "The Journal of Machine Learning Research,", "citeRegEx": "Srivastava et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Srivastava et al\\.", "year": 2014}, {"title": "LSTM neural networks for language modeling", "author": ["Ralf Schl\u00fcter", "Hermann Ney"], "venue": "In Proc. InterSpeech", "citeRegEx": "Sundermeyer et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Sundermeyer et al\\.", "year": 2012}, {"title": "Sequence to sequence learning with neural networks", "author": ["Oriol Vinyals", "Quoc VV Le"], "venue": "In Proc. NIPS,", "citeRegEx": "Sutskever et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Sutskever et al\\.", "year": 2014}, {"title": "A Bayesian interpretation of interpolated Kneser-Ney", "author": ["Yee Whye Teh"], "venue": "Technical report, School of Computing, National Univ. of Singapore", "citeRegEx": "Teh.,? \\Q2006\\E", "shortCiteRegEx": "Teh.", "year": 2006}, {"title": "Scaling recurrent neural network language models", "author": ["Niranjani Prasad", "David Mrva", "Tom Ash", "Tony Robinson"], "venue": "In Proc. ICASSP", "citeRegEx": "Williams et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Williams et al\\.", "year": 2015}, {"title": "The zero-frequency problem: Estimating the probabilities of novel events in adaptive text compression", "author": ["Witten", "Bell1991] Ian H. Witten", "Timothy C. Bell"], "venue": "IEEE Transactions on Information", "citeRegEx": "Witten et al\\.,? \\Q1991\\E", "shortCiteRegEx": "Witten et al\\.", "year": 1991}, {"title": "Recurrent neural network regularization. CoRR, abs/1409.2329", "author": ["Ilya Sutskever", "Oriol Vinyals"], "venue": null, "citeRegEx": "Zaremba et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Zaremba et al\\.", "year": 2014}], "referenceMentions": [{"referenceID": 22, "context": "Recently, there has been a focus on LMs based on neural networks (Nakamura et al., 1990; Bengio et al., 2006; Mikolov et al., 2010), which have shown impressive improvements in performance over count-based LMs.", "startOffset": 65, "endOffset": 131}, {"referenceID": 3, "context": "Recently, there has been a focus on LMs based on neural networks (Nakamura et al., 1990; Bengio et al., 2006; Mikolov et al., 2010), which have shown impressive improvements in performance over count-based LMs.", "startOffset": 65, "endOffset": 131}, {"referenceID": 21, "context": "Recently, there has been a focus on LMs based on neural networks (Nakamura et al., 1990; Bengio et al., 2006; Mikolov et al., 2010), which have shown impressive improvements in performance over count-based LMs.", "startOffset": 65, "endOffset": 131}, {"referenceID": 7, "context": "On the other hand, these neural LMs also come at the cost of increased computational complexity at both training and test time, and even the largest reported neural LMs (Chen et al., 2015; Williams et al., 2015) are trained on a fraction of the data of their count-based counterparts (Brants et al.", "startOffset": 169, "endOffset": 211}, {"referenceID": 32, "context": "On the other hand, these neural LMs also come at the cost of increased computational complexity at both training and test time, and even the largest reported neural LMs (Chen et al., 2015; Williams et al., 2015) are trained on a fraction of the data of their count-based counterparts (Brants et al.", "startOffset": 169, "endOffset": 211}, {"referenceID": 4, "context": ", 2015) are trained on a fraction of the data of their count-based counterparts (Brants et al., 2007).", "startOffset": 80, "endOffset": 101}, {"referenceID": 22, "context": "1 can be used to describe not only n-gram models, but also feed-forward (Nakamura et al., 1990; Bengio et al., 2006; Schwenk, 2007) and recurrent (Mikolov et al.", "startOffset": 72, "endOffset": 131}, {"referenceID": 3, "context": "1 can be used to describe not only n-gram models, but also feed-forward (Nakamura et al., 1990; Bengio et al., 2006; Schwenk, 2007) and recurrent (Mikolov et al.", "startOffset": 72, "endOffset": 131}, {"referenceID": 26, "context": "1 can be used to describe not only n-gram models, but also feed-forward (Nakamura et al., 1990; Bengio et al., 2006; Schwenk, 2007) and recurrent (Mikolov et al.", "startOffset": 72, "endOffset": 131}, {"referenceID": 21, "context": ", 2006; Schwenk, 2007) and recurrent (Mikolov et al., 2010; Sundermeyer et al., 2012) neural network LMs (\u00a73).", "startOffset": 37, "endOffset": 85}, {"referenceID": 29, "context": ", 2006; Schwenk, 2007) and recurrent (Mikolov et al., 2010; Sundermeyer et al., 2012) neural network LMs (\u00a73).", "startOffset": 37, "endOffset": 85}, {"referenceID": 10, "context": "Static interpolation can be improved by calculating \u03bb(c) dynamically, using heuristics based on the frequency counts of the context (Good, 1953; Katz, 1987; Witten and Bell, 1991).", "startOffset": 132, "endOffset": 179}, {"referenceID": 16, "context": "Static interpolation can be improved by calculating \u03bb(c) dynamically, using heuristics based on the frequency counts of the context (Good, 1953; Katz, 1987; Witten and Bell, 1991).", "startOffset": 132, "endOffset": 179}, {"referenceID": 23, "context": "Discounting: The widely used technique of discounting (Ney et al., 1994) defines a fixed discount d and subtracts it from the count of each word before calculating probabilities:", "startOffset": 54, "endOffset": 72}, {"referenceID": 31, "context": "Modified KN is currently the defacto standard in n-gram LMs despite occasional improvements (Teh, 2006; Durrett and Klein, 2011), and we will express it as PKN (\u00b7).", "startOffset": 92, "endOffset": 128}, {"referenceID": 3, "context": "Feed-forward neural network LMs: Feedforward LMs (Bengio et al., 2006; Schwenk, 2007) are LMs that, like n-grams, calculate the probability of the next word based on the previous words.", "startOffset": 49, "endOffset": 85}, {"referenceID": 26, "context": "Feed-forward neural network LMs: Feedforward LMs (Bengio et al., 2006; Schwenk, 2007) are LMs that, like n-grams, calculate the probability of the next word based on the previous words.", "startOffset": 49, "endOffset": 85}, {"referenceID": 21, "context": "Recurrent neural network LMs: LMs using recurrent neural networks (RNNs) (Mikolov et al., 2010) consider not the previous few words, but also maintain a hidden state summarizing the sentence up until this point by re-defining the net in Eq.", "startOffset": 73, "endOffset": 95}, {"referenceID": 29, "context": "This allows for consideration of long-distance dependencies beyond the scope of standard n-grams, and LMs using RNNs or long short-term memory (LSTM) networks (Sundermeyer et al., 2012) have posted large improvements over standard n-grams and feed-forward models.", "startOffset": 159, "endOffset": 185}, {"referenceID": 3, "context": "Like most previous work on LMs (Bengio et al., 2006), we use a negative loglikelihood loss summed over words wi in every sentence w in corpusW", "startOffset": 31, "endOffset": 52}, {"referenceID": 28, "context": "In contrast to standard dropout (Srivastava et al., 2014), which drops out single nodes or connections, block dropout randomly drops out entire subsets of network nodes.", "startOffset": 32, "endOffset": 57}, {"referenceID": 28, "context": "5 for both feed-forward (Srivastava et al., 2014) and LSTM (Pham et al.", "startOffset": 24, "endOffset": 49}, {"referenceID": 24, "context": ", 2014) and LSTM (Pham et al., 2014) nets in the neural LMs and neural/n-gram hybrids, but not in the neurally interpolated n-grams, where it resulted in slightly worse perplexities.", "startOffset": 17, "endOffset": 36}, {"referenceID": 21, "context": "For the main experiments, we evaluate on two corpora: the Penn Treebank (PTB) data set prepared by Mikolov et al. (2010),2 and the first 100k sentences in the English side of the ASPEC corpus (Nakazawa et al.", "startOffset": 99, "endOffset": 121}, {"referenceID": 34, "context": "As our main baseline, we compare to LSTMs with only \u03b4 distributions, which have reported competitive numbers on the PTB data set (Zaremba et al., 2014).", "startOffset": 129, "endOffset": 151}, {"referenceID": 20, "context": "In addition to better perplexities, neural/n-gram hybrids are trained in a single pass instead of performing post-facto interpolation, which may give advantages when training for other objectives (Auli and Gao, 2014; Li et al., 2015).", "startOffset": 196, "endOffset": 233}, {"referenceID": 5, "context": "A number of alternative methods focus on interpolating LMs of multiple varieties such as in-domain and out-of-domain LMs (Bulyko et al., 2003; Bacchiani et al., 2006; G\u00fcl\u00e7ehre et al., 2015).", "startOffset": 121, "endOffset": 189}, {"referenceID": 1, "context": "A number of alternative methods focus on interpolating LMs of multiple varieties such as in-domain and out-of-domain LMs (Bulyko et al., 2003; Bacchiani et al., 2006; G\u00fcl\u00e7ehre et al., 2015).", "startOffset": 121, "endOffset": 189}, {"referenceID": 11, "context": "A number of alternative methods focus on interpolating LMs of multiple varieties such as in-domain and out-of-domain LMs (Bulyko et al., 2003; Bacchiani et al., 2006; G\u00fcl\u00e7ehre et al., 2015).", "startOffset": 121, "endOffset": 189}, {"referenceID": 25, "context": "Also conceptually similar is work on adaptation of n-gram LMs, which start with n-gram probabilities (Della Pietra et al., 1992; Kneser and Steinbiss, 1993; Rosenfeld, 1996; Iyer and Ostendorf, 1999) and adapt them based on the distribution of the current document, albeit in a linear model.", "startOffset": 101, "endOffset": 199}, {"referenceID": 7, "context": "Finally, recent works have compared n-gram and neural models, finding that neural models often perform better in perplexity, but n-grams have their own advantages such as effectiveness in extrinsic tasks (Baltescu and Blunsom, 2015) and better modeling of rare words (Chen et al., 2015).", "startOffset": 267, "endOffset": 286}, {"referenceID": 1, "context": ", 2003; Bacchiani et al., 2006; G\u00fcl\u00e7ehre et al., 2015). Perhaps most relevant is Hsu (2007)\u2019s work on learning to interpolate multiple LMs using log-linear models.", "startOffset": 8, "endOffset": 92}, {"referenceID": 30, "context": "As the framework discussed here is general, it is also possible that they could be used in other tasks that perform sequential prediction of words such as neural machine translation (Sutskever et al., 2014) or dialog response generation (Sordoni et al.", "startOffset": 182, "endOffset": 206}, {"referenceID": 27, "context": ", 2014) or dialog response generation (Sordoni et al., 2015).", "startOffset": 38, "endOffset": 60}], "year": 2017, "abstractText": "Language models (LMs) are statistical models that calculate probabilities over sequences of words or other discrete symbols. Currently two major paradigms for language modeling exist: count-based n-gram models, which have advantages of scalability and test-time speed, and neural LMs, which often achieve superior modeling performance. We demonstrate how both varieties of models can be unified in a single modeling framework that defines a set of probability distributions over the vocabulary of words, and then dynamically calculates mixture weights over these distributions. This formulation allows us to create novel hybrid models that combine the desirable features of count-based and neural LMs, and experiments demonstrate the advantages of these approaches.", "creator": "LaTeX with hyperref package"}}}