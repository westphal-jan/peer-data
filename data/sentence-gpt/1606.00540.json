{"id": "1606.00540", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "2-Jun-2016", "title": "Multi-pretrained Deep Neural Network", "abstract": "Pretraining is widely used in deep neutral network and one of the most famous pretraining models is Deep Belief Network (DBN). The optimization formulas are different during the pretraining process for different pretraining models. In this paper, we pretrained deep neutral network by different pretraining models and hence investigated the difference between DBN and Stacked Denoising Autoencoder (SDA) when used as pretraining model. We used DBN (2.3 Mg) to compare two pretraining models as shown below. In a nutshell, we used DBN for the pretraining model, while Deep Belief Network was used in the pre-training model. DBN and Stacked Denoising Autoencoder in our POTM project, showed that Deep Belief Network was able to perform better on training for different pretraining models (which also did not perform better at learning without a Stacked Denoising Autoencoder. Moreover, Deep Belief Network did not perform better on training for a Stacked Denoising Autoencoder. These results imply that Deep Belief Network can perform better when using deep training for different pretraining models (which also did not perform better on training for different pretraining models) as shown below. The neural network has its own DBN network. Deep Belief Network was able to perform better on training for different pretraining models (which also did not perform better on training for different pretraining models). The neural network has its own DBN network. Deep Belief Network was able to perform better on training for different pretraining models (which also did not perform better on training for different pretraining models).\n\n\n\n\n\n\nThe neural network has its own DBN network. Deep Belief Network was able to perform better on training for different pretraining models (which also did not perform better on training for different pretraining models) as shown below.\nThe neural network has its own DBN network. Deep Belief Network was able to perform better on training for different pretraining models (which also did not perform better on training for different pretraining models). The neural network has its own DBN network. Deep Belief Network was able to perform better on training for different pretraining models (which also did not perform better on training for different pretraining models). The neural network has its own DBN network. Deep Belief Network was able to perform better on training for different pretraining models (which also did not perform better on training for different pretraining models).\n\nThis results suggest that Deep", "histories": [["v1", "Thu, 2 Jun 2016 05:39:54 GMT  (317kb)", "http://arxiv.org/abs/1606.00540v1", null]], "reviews": [], "SUBJECTS": "cs.NE cs.LG", "authors": ["zhen hu", "zhuyin xue", "tong cui", "shiqiang zong", "chenglong he"], "accepted": false, "id": "1606.00540"}, "pdf": {"name": "1606.00540.pdf", "metadata": {"source": "CRF", "title": "Multi-pretrained Deep Neural Network", "authors": ["Zhen Hu", "Zhuyin Xue", "Tong Cui", "Shiqiang Zong", "Chenglong He"], "emails": ["49859211@qq.com"], "sections": [{"heading": null, "text": "models is Deep Belief Network (DBN). The optimization formulas are different during the pretraining process for different pretraining models. In this paper, we pretrained deep neutral network by different pretraining models and hence investigated the difference between DBN and Stacked Denoising Autoencoder (SDA) when used as pretraining model. The experimental results show that DBN get a better initial model. However the model converges to a relatively worse model after the finetuning process. Yet after pretrained by SDA for the second time the model converges to a better model if finetuned."}, {"heading": "Introduction", "text": "Neural Network has long been a widely used model in machine learning. In 1982 Hopfield proposed the Hopfield Network[8] and proved that neural network can be used to simulate the XOR function. In 1986, Hinton et. al proposed the Backpropagation algorithm(BP algorithm) to train multiple-layer neural network[16] and henceforth neural network has become a widely used model in machine learning areas such as image processing[5], control[9] and optimization[14].\nHowever, the training process of neural network is a non-convex problem while the BP algorithm is intrinsically a gradient-descent algorithm which is guaranteed to converge to a global optima when solving convex problems[1]. This very property makes the BP algorithm converges to a local optima which is severely dependent to the Initial state of the network. In real cases, researchers usually randomly choose different Initial states before training the network and at last choose the best-performed model. This trick is very inefficient and even unbearable when training large-scale network. Some researchers proposed improved algorithms such as simulated annealing[4], Genetic algorithm[19] to reduce the training epoches. However the progress was not so expecting. What\u2019s more, as the powerful expressive ability of neural network, a network in the global optimal state maybe severely over-fitting[17], and even worse than a network in the local optimal state. As the fast development fast-learnable model such as Support Vector Machine[6], researchers were more and more considerable about the training data[3]. As the scale of training data rises, more complicated neural network were proposed and the harder the models were trained by BP algorithm.\nTo relief the training burden, Lecun et. al introduced the parameter bonding strategy into the training process of neural network[12]. In their model, they bonded the parameter to suit some kind of prior knowledge to restrict the expression power of neural network. And by doing so, the number of local optima was reduced which made the training of deep neural network possible. In the parameter bonding training process, the global optima is not important any more. In fact, by bonding the parameters, the global optima is un-reachable. The work of Lecun et.al inspired researchers to divide the training process into two stage. One for feature extracting which was named as feature learning\ninput vector by convolution of kernel and input vector. The prior was the shifting invariant of images. The proposed model solved the MNIST problem well.\nAnother strategy to solve the large-scale machine learning problem was focus on the initial state. In 2006, Hinton et. al proposed the pre-training strategy[7]. The pre-training process aims at seeking a well-performed initial state of the neural network which was fine-tuned by the BP algorithm. The fine-tuning process was conducted only once. Since the pre-training process was layer-wised, the computation complexity increased linearly an the number of layers raised. The pre-training process proposed by Hinton et. al took the conjoint layers as Restricted Boltzmann Machine (RBM) and pre-trained the layers by maximizing the likelihood function of RBM. Some researchers used models other than RBM to pre-train the neural network. Honglak Lee et. al proposed the convolutional RBM[13] while Larochelle et. al proposed the auto-encoder (AE) [11]. In convolutional RBM, the computation rule between layers is convolution other than production and in AE the optimization problem is to minimize the reconstruction error other than maximizing the likelihood function. Some researchers improved AE, such as De-noising Auto-encoder (DAE)[18], Contractive Autoencoder (CAE)[15] and so on.\nIn our work, we proposed a new pre-training process to get a better initial state. We want to combine the advantage of different models to pre-train the network as much as possible. We proposed the Multi Pre-trained Deep Neural Network (MPDNN). In our model, we pre-trained the network by RBM and DAE multi-times and the experiment results show that the network performed best when pre-trained by RBM and then pre-trained by DA.\nThe paper was arranged as follow. In section 2, we introduced our proposed model. we reported our experimental results in section 3 we also tested different pre-training strategies in this section. In section 4 we concluded our work."}, {"heading": "Model", "text": "In this section, we introduced our proposed model. Since our model was based on RBM and DAE, we introduced these two models first."}, {"heading": "Restricted Boltzmann Machine", "text": "In RBM, nodes were divided into visible nodes and hidden nodes. The visible nodes took the original data as input and the hidden nodes were not directly connected to the input. The value of visible and hidden nodes were denoted by v and h respectively.\nIn RBM, an energy function was defined as E(\ud835\udc63, \u210e) = hT\ud835\udc4a\ud835\udc63 + \ud835\udc4f\ud835\udc47\u210e + \ud835\udc50\ud835\udc47\ud835\udc63. (1) where W was the weight matrix, b and c were the visible and hidden biases. The likelihood\nfunction was defined as\nL(v) = log\u2211 \ud835\udc43(\ud835\udc63, \u210e)\u210e = log \u2211 \ud835\udc52\u2212\ud835\udc38(\ud835\udc63,\u210e)\u210e\n\u2211 \u2211 \ud835\udc52\u2212\ud835\udc38(\ud835\udc63,\u210e)\u210e\ud835\udc63 . (2)\nIt\u2019s easy to see that values of hidden nodes were conditionally independent on condition of visible nodes and values of visible nodes were conditionally independent on condition of hidden nodes[10]. So equation (2) can be optimized by gibbs sampling[2]. Hinton proposed that the gibbs sampling process can be only run for once to get a well enough initial state[7]."}, {"heading": "De-noising Auto-encoder", "text": "DAE was a variant of AE. In AE, values of hidden nodes were calculated by the following equation, \u210e = sigm(\ud835\udc4a\ud835\udc63 + \ud835\udc4f), (3) where sigm denoted the non-linear sigmoid function. The reconstruction process was fulfilled by \ud835\udc63 = sigm(\ud835\udc4a\ud835\udc47\u210e + c), (4) The optimization problem for AE was defined as\nmin\ud835\udc4a,\ud835\udc4f,\ud835\udc50||\ud835\udc63 \u2212 \ud835\udc63||2. (5)\nset some elements to 0 first to get the corrupted input \ud835\udc63. After encoded and decoded as AE, the reconstructed input was compared with the clean input \ud835\udc63. Their experiment showed that by corrupting the input, the network was more general."}, {"heading": "Multi-pretrained Deep Neural Network", "text": "Our proposed model, MPDNN, was based on RBM and DAE. We use these models to pre-train our network twice and the experimental results showed the difference.\nThe optimization problem for RBM was a likelihood maximization problem and that for DAE was an error minimization problem. So DAE was more loyal to the original input and RBM can be varied. These difference can not be erased by the fine-tuning process."}, {"heading": "Experiment", "text": "In this section, we report our experiment results."}, {"heading": "Experiment Setup", "text": "Our proposed model, MPDNN, was based on RBM and DAE. We use these models to pre-train our network twice and the experimental results showed the difference.\nThe optimization problem for RBM was a likelihood maximization problem and that for DAE was an error minimization problem. So DAE was more loyal to the original input and RBM can be varied. These difference can not be erased by the fine-tuning process.\nIn our paper, we considered a widely used benchmark problem, the MNIST handwritten digits recognition problem. In this problem, we picked 50,000 pictures as the training set, 10,000 pictures as the validation set and 10,000 as the test set.\nWe proposed a 4-layer network to recognize the digits. The number of hidden nodes in all 3 hidden\nlayers was 1,000. In our experiment we implemented four models,\n-MPDNN-SS, the network was pre-trained by DAE twice, -MPDNN-SD, the network was pre-trained by DAE first and then pre-trained by RBM, -MPDNN-DS, the network was pre-trained by RBM first and then DAE -MPDNN-DD, the network was pre-trained by RBM twice. All these models were implemented by Theano and run on NVIDIA C2075. We run every model\nfor 50 times and table 1 showed the result."}, {"heading": "Experiment Results", "text": "The results showed that, MPDNN-DD was dull to the initial state as the variance for all 50 runs was smallest. MPDNN-DS performed best on accuracy among all the models. Once pretrained by DAE, no matter followed by DAE or RBM, the network performed worse, with either larger error rate or larger variance.\nSince the pre-training process was layer-wise, MPDNN-DD was equivalent to RBM with more\npre-training epoches, and MPDNN-SS was equivalent to DAE with more pre-training epoches.\nFigure. 1 showed the curve of error rate along with fine-tuning iteration. We find that MPDNN-DX (X=D,S) performed better than MPDNN-SX (X=D,S) from the beginning of fine-tuning till the end. And as the fine-tuning iteration raised MPDNN-DS became better than MPDNN-SS, even though MPDNN-DS was worse than MPDNN-SS in the beginning. The\nDAE for the second time, the model performed better."}, {"heading": "Summary", "text": "In this work, we used different models to pre-train a deep neural network, and eventually got a better performed model. The experimental results showed that RBM was good at finding a better initial state for the neural network while this initial state may get stuck when fine-tuning the model. By pre-training the model using DAE for the second time, the model may performed worse than the model pre-trained by RBM at the beginning of fine-tuning, and eventually surpass the RBM pre-trained model as the fine-tuning process goes.\nIn our work, we only compared two models, that is RBM and DAE. We will test more models in the near future to find the advantage of different pre-training models. By doing this, we can choose the most suitable pre-training model when we train our network."}, {"heading": "Acknowledgement", "text": "The research work was supported by National Natural Science Foundation of China under Grant No. 61402426, Training Program of the Major Research Plan of the National Natural Science Foundation of China (No. 91546103) and partially supported by Collaborative Innovation Center of Social Safety Science and Technology."}], "references": [{"title": "Convex optimization. Cambridge university press", "author": ["S. Boyd", "L. Vandenberghe"], "venue": null, "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2004}, {"title": "On gibbs sampling for state space models", "author": ["C.K. Carter", "R. Kohn"], "venue": "Biometrika 81(3),", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 1994}, {"title": "A pendulum swung too far", "author": ["K. Church"], "venue": "Linguistic Issues in Language Technology 6(5),", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2011}, {"title": "Global optimization of statistical functions with simulated annealing", "author": ["W.L. Goffe", "G.D. Ferrier", "J. Rogers"], "venue": "Journal of econometrics 60(1-2),", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 1994}, {"title": "Support vector machines", "author": ["M.A. Hearst", "S.T. Dumais", "E. Osman", "J. Platt", "B. Scholkopf"], "venue": "Intelligent Systems and their Applications,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 1998}, {"title": "A fast learning algorithm for deep belief nets. Neural computation", "author": ["G.E. Hinton", "S. Osindero", "Y.W. Teh"], "venue": null, "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2006}, {"title": "Neural networks and physical systems with emergent collective computational abilities", "author": ["J.J. Hopfield"], "venue": "Proceedings of the national academy of sciences 79(8),", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 1982}, {"title": "Neural networks for control systemsa survey", "author": ["K.J. Hunt", "D. Sbarbaro", "R. Z \u0307 bikowski", "P.J. Gawthrop"], "venue": "Automatica 28(6),", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 1992}, {"title": "Probabilistic graphical models: principles and techniques", "author": ["D. Koller", "N. Friedman"], "venue": null, "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2009}, {"title": "An empirical evaluation of deep architectures on problems with many factors of variation", "author": ["H. Larochelle", "D. Erhan", "A. Courville", "J. Bergstra", "Y. Bengio"], "venue": "In Proceedings of the 24th international conference on Machine learning", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2007}, {"title": "Gradient-based learning applied to document recognition", "author": ["Y. LeCun", "L. Bottou", "Y. Bengio", "P. Haffner"], "venue": "Proceedings of the IEEE 86(11),", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 1998}, {"title": "Convolutional deep belief networks for scalable unsupervised learning of hierarchical representations", "author": ["H. Lee", "R. Grosse", "R. Ranganath", "A.Y. Ng"], "venue": "In Proceedings of the 26th Annual International Conference on Machine Learning", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2009}, {"title": "State-of-the-art surveythe traveling salesman problem: A neural network perspective", "author": ["J.Y. Potvin"], "venue": "ORSA Journal on Computing", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 1993}, {"title": "Contractive auto-encoders: Explicit invariance during feature extraction", "author": ["S. Rifai", "P. Vincent", "X. Muller", "X. Glorot", "Y. Bengio"], "venue": "In Proceedings of the 28th international conference on machine learning", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2011}, {"title": "Learning internal representation by back propagation. Parallel distributed processing: exploration in the microstructure of cognition", "author": ["D. Rumelhart", "G. Hinton", "R. Williams"], "venue": null, "citeRegEx": "16", "shortCiteRegEx": "16", "year": 1986}, {"title": "Adaptive probabilities of crossover and mutation in genetic algorithms", "author": ["M. Srinivas", "L.M. Patnaik"], "venue": "Systems, Man and Cybernetics, IEEE Transactions on 24(4),", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 1994}, {"title": "Stacked denoising autoencoders: Learning useful representations in a deep network with a local denoising criterion", "author": ["P. Vincent", "H. Larochelle", "I. Lajoie", "Y. Bengio", "P.A. Manzagol"], "venue": "The Journal of Machine Learning Research", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2010}, {"title": "Genetic algorithms and neural networks: Optimizing connections and connectivity. Parallel computing", "author": ["D. Whitley", "T. Starkweather", "C. Bogart"], "venue": null, "citeRegEx": "19", "shortCiteRegEx": "19", "year": 1990}], "referenceMentions": [{"referenceID": 6, "context": "In 1982 Hopfield proposed the Hopfield Network[8] and proved that neural network can be used to simulate the XOR function.", "startOffset": 46, "endOffset": 49}, {"referenceID": 14, "context": "al proposed the Backpropagation algorithm(BP algorithm) to train multiple-layer neural network[16] and henceforth neural network has become a widely used model in machine learning areas such as image processing[5], control[9] and optimization[14].", "startOffset": 94, "endOffset": 98}, {"referenceID": 7, "context": "al proposed the Backpropagation algorithm(BP algorithm) to train multiple-layer neural network[16] and henceforth neural network has become a widely used model in machine learning areas such as image processing[5], control[9] and optimization[14].", "startOffset": 222, "endOffset": 225}, {"referenceID": 12, "context": "al proposed the Backpropagation algorithm(BP algorithm) to train multiple-layer neural network[16] and henceforth neural network has become a widely used model in machine learning areas such as image processing[5], control[9] and optimization[14].", "startOffset": 242, "endOffset": 246}, {"referenceID": 0, "context": "However, the training process of neural network is a non-convex problem while the BP algorithm is intrinsically a gradient-descent algorithm which is guaranteed to converge to a global optima when solving convex problems[1].", "startOffset": 220, "endOffset": 223}, {"referenceID": 3, "context": "Some researchers proposed improved algorithms such as simulated annealing[4], Genetic algorithm[19] to reduce the training epoches.", "startOffset": 73, "endOffset": 76}, {"referenceID": 17, "context": "Some researchers proposed improved algorithms such as simulated annealing[4], Genetic algorithm[19] to reduce the training epoches.", "startOffset": 95, "endOffset": 99}, {"referenceID": 15, "context": "What\u2019s more, as the powerful expressive ability of neural network, a network in the global optimal state maybe severely over-fitting[17], and even worse than a network in the local optimal state.", "startOffset": 132, "endOffset": 136}, {"referenceID": 4, "context": "As the fast development fast-learnable model such as Support Vector Machine[6], researchers were more and more considerable about the training data[3].", "startOffset": 75, "endOffset": 78}, {"referenceID": 2, "context": "As the fast development fast-learnable model such as Support Vector Machine[6], researchers were more and more considerable about the training data[3].", "startOffset": 147, "endOffset": 150}, {"referenceID": 10, "context": "al introduced the parameter bonding strategy into the training process of neural network[12].", "startOffset": 88, "endOffset": 92}, {"referenceID": 10, "context": "al proposed the model named as LeNet5[12] which was a 8-layer neural network.", "startOffset": 37, "endOffset": 41}, {"referenceID": 5, "context": "al proposed the pre-training strategy[7].", "startOffset": 37, "endOffset": 40}, {"referenceID": 11, "context": "al proposed the convolutional RBM[13] while Larochelle et.", "startOffset": 33, "endOffset": 37}, {"referenceID": 9, "context": "al proposed the auto-encoder (AE) [11].", "startOffset": 34, "endOffset": 38}, {"referenceID": 16, "context": "Some researchers improved AE, such as De-noising Auto-encoder (DAE)[18], Contractive Autoencoder (CAE)[15] and so on.", "startOffset": 67, "endOffset": 71}, {"referenceID": 13, "context": "Some researchers improved AE, such as De-noising Auto-encoder (DAE)[18], Contractive Autoencoder (CAE)[15] and so on.", "startOffset": 102, "endOffset": 106}, {"referenceID": 8, "context": "It\u2019s easy to see that values of hidden nodes were conditionally independent on condition of visible nodes and values of visible nodes were conditionally independent on condition of hidden nodes[10].", "startOffset": 193, "endOffset": 197}, {"referenceID": 1, "context": "So equation (2) can be optimized by gibbs sampling[2].", "startOffset": 50, "endOffset": 53}, {"referenceID": 5, "context": "Hinton proposed that the gibbs sampling process can be only run for once to get a well enough initial state[7].", "startOffset": 107, "endOffset": 110}], "year": 2016, "abstractText": "Pretraining is widely used in deep neutral network and one of the most famous pretraining models is Deep Belief Network (DBN). The optimization formulas are different during the pretraining process for different pretraining models. In this paper, we pretrained deep neutral network by different pretraining models and hence investigated the difference between DBN and Stacked Denoising Autoencoder (SDA) when used as pretraining model. The experimental results show that DBN get a better initial model. However the model converges to a relatively worse model after the finetuning process. Yet after pretrained by SDA for the second time the model converges to a better model if finetuned.", "creator": "Microsoft\u00ae Word 2010"}}}