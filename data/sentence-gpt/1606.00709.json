{"id": "1606.00709", "review": {"conference": "NIPS", "VERSION": "v1", "DATE_OF_SUBMISSION": "2-Jun-2016", "title": "f-GAN: Training Generative Neural Samplers using Variational Divergence Minimization", "abstract": "Generative neural samplers are probabilistic models that implement sampling using feedforward neural networks: they take a random input vector and produce a sample from a probability distribution defined by the network weights. These models are expressive and allow efficient computation of samples and derivatives, but cannot be used for computing likelihoods or for marginalization. The generative-adversarial training method allows to train such models through the use of an auxiliary discriminative neural network (GFC) that learns a subset of each subset of the input vectors by selecting its weights and then selecting its samples in a specific order for the input vectors to be matched. This approach is especially useful in understanding neural learning and in generating models in order to detect potential confounding differences. An auxiliary discriminative neural network with no training network with at least 30% training training is also useful in generating models for sampling (e.g., this discriminative-ADV) or for the likelihood of inferring whether a given subset of the input vectors is different than the average distribution. The discriminative-ADV training approach was used in a number of examples of artificial intelligence that have been demonstrated.\n\n\n\n\n\nThe neural network has been studied for many years in both theoretical and functional domains of machine learning, which have developed and are now widely used in machine learning.\nThe neural network consists of three classes of representations, each with its own neural network. These representations are generated by the neural network training a model of its input vectors, for each sample to be generated in a set of sets of sets of vectors corresponding to the input vectors by the recurrent neural network.\nThe neural network can represent the average and non-linear distributions of input vectors at different time points in time, and they are shown in Figure 1 of Figure 1. The neural network is able to infer the random output from the inputs of its input vectors by choosing an approximation, and it is able to predict the rate at which inputs are generated and then learning the variance, to which each input vector in the output vector is fitted.\nFigure 1.\nThe neural network can represent the average and non-linear distributions of input vectors from each input vector (see Table 1).\nThe neural network also supports a number of computational approaches. These include:\nComplexity modeling for the output vector (e.g., Gaussian distribution),\nComplexity modeling for the output vector (e.g., Gaussian distribution),\nComplexity modeling for the output vector (e.g., Ga", "histories": [["v1", "Thu, 2 Jun 2016 14:53:33 GMT  (2657kb,D)", "http://arxiv.org/abs/1606.00709v1", "17 pages"]], "COMMENTS": "17 pages", "reviews": [], "SUBJECTS": "stat.ML cs.LG stat.ME", "authors": ["sebastian nowozin", "botond cseke", "ryota tomioka"], "accepted": true, "id": "1606.00709"}, "pdf": {"name": "1606.00709.pdf", "metadata": {"source": "CRF", "title": "f -GAN: Training Generative Neural Samplers using Variational Divergence Minimization", "authors": ["Sebastian Nowozin", "Botond Cseke"], "emails": ["Sebastian.Nowozin@microsoft.com", "botcse@microsoft.com", "ryoto@microsoft.com"], "sections": [{"heading": null, "text": "Generative neural samplers are probabilistic models that implement sampling using feedforward neural networks: they take a random input vector and produce a sample from a probability distribution defined by the network weights. These models are expressive and allow efficient computation of samples and derivatives, but cannot be used for computing likelihoods or for marginalization. The generativeadversarial training method allows to train such models through the use of an auxiliary discriminative neural network. We show that the generative-adversarial approach is a special case of an existing more general variational divergence estimation approach. We show that any f -divergence can be used for training generative neural samplers. We discuss the benefits of various choices of divergence functions on training complexity and the quality of the obtained generative models."}, {"heading": "1 Introduction", "text": "Probabilistic generative models describe a probability distribution over a given domainX , for example a distribution over natural language sentences, natural images, or recorded waveforms.\nGiven a generative model Q from a class Q of possible models we are generally interested in performing one or multiple of the following operations:\n\u2022 Sampling. Produce a sample from Q. By inspecting samples or calculating a function on a set of samples we can obtain important insight into the distribution or solve decision problems. \u2022 Estimation. Given a set of iid samples {x1, x2, . . . , xn} from an unknown true distribution P , find Q \u2208 Q that best describes the true distribution.\n\u2022 Point-wise likelihood evaluation. Given a sample x, evaluate the likelihood Q(x). Generative-adversarial networks (GAN) in the form proposed by [10] are an expressive class of generative models that allow exact sampling and approximate estimation. The model used in GAN is simply a feedforward neural network which receives as input a vector of random numbers, sampled, for example, from a uniform distribution. This random input is passed through each layer in the network and the final layer produces the desired output, for example, an image. Clearly, sampling from a GAN model is efficient because only one forward pass through the network is needed to produce one exact sample.\nar X\niv :1\n60 6.\n00 70\n9v 1\n[ st\nat .M\nSuch probabilistic feedforward neural network models were first considered in [22] and [3], here we call these models generative neural samplers. GAN is also of this type, as is the decoder model of a variational autoencoder [18].\nIn the original GAN paper the authors show that it is possible to estimate neural samplers by approximate minimization of the symmetric Jensen-Shannon divergence,\nDJS(P\u2016Q) = 12DKL(P\u2016 1 2 (P +Q)) + 1 2DKL(Q\u2016 1 2 (P +Q)), (1)\nwhere DKL denotes the Kullback-Leibler divergence. The key technique used in the GAN training is that of introducing a second \u201cdiscriminator\u201d neural networks which is optimized simultaneously. Because DJS(P\u2016Q) is a proper divergence measure between distributions this implies that the true distribution P can be approximated well in case there are sufficient training samples and the model class Q is rich enough to represent P . In this work we show that the principle of GANs is more general and we can extend the variational divergence estimation framework proposed by Nguyen et al. [25] to recover the GAN training objective and generalize it to arbitrary f -divergences.\nMore concretely, we make the following contributions over the state-of-the-art:\n\u2022 We derive the GAN training objectives for all f -divergences and provide as example additional divergence functions, including the Kullback-Leibler and Pearson divergences. \u2022 We simplify the saddle-point optimization procedure of Goodfellow et al. [10] and provide a theoretical justification. \u2022 We provide experimental insight into which divergence function is suitable for estimating generative neural samplers for natural images."}, {"heading": "2 Method", "text": "We first review the divergence estimation framework of Nguyen et al. [25] which is based on f -divergences. We then extend this framework from divergence estimation to model estimation."}, {"heading": "2.1 The f-divergence Family", "text": "Statistical divergences such as the well-known Kullback-Leibler divergence measure the difference between two given probability distributions. A large class of different divergences are the so called f -divergences [5, 21], also known as the Ali-Silvey distances [1]. Given two distributions P and Q that possess, respectively, an absolutely continuous density function p and q with respect to a base measure dx defined on the domain X , we define the f -divergence,\nDf (P\u2016Q) = \u222b X q(x)f ( p(x) q(x) ) dx, (2)\nwhere the generator function f : R+ \u2192 R is a convex, lower-semicontinuous function satisfying f(1) = 0. Different choices of f recover popular divergences as special cases in (2). We illustrate common choices in Table 5. See supplementary material for more divergences and plots."}, {"heading": "2.2 Variational Estimation of f -divergences", "text": "Nguyen et al. [25] derive a general variational method to estimate f -divergences given only samples from P and Q. We will extend their method from merely estimating a divergence for a fixed model to estimating model parameters. We call this new method variational divergence minimization (VDM) and show that the generative-adversarial training is a special case of this more general VDM framework.\nFor completeness, we first provide a self-contained derivation of Nguyen et al\u2019s divergence estimation procedure. Every convex, lower-semicontinuous function f has a convex conjugate function f\u2217, also known as Fenchel conjugate [14]. This function is defined as\nf\u2217(t) = sup u\u2208domf {ut\u2212 f(u)} . (3)\nThe function f\u2217 is again convex and lower-semicontinuous and the pair (f, f\u2217) is dual to another in the sense that f\u2217\u2217 = f . Therefore, we can also represent f as f(u) = supt\u2208domf\u2217 {tu\u2212 f \u2217(t)}.\nNguyen et al. leverage the above variational representation of f in the definition of the f -divergence to obtain a lower bound on the divergence, Df (P\u2016Q) = \u222b X q(x) sup\nt\u2208domf\u2217\n{ tp(x)q(x) \u2212 f \u2217(t) } dx\n\u2265 supT\u2208T (\u222b X p(x)T (x) dx\u2212 \u222b X q(x) f\u2217(T (x)) dx\n) = sup T\u2208T (Ex\u223cP [T (x)]\u2212 Ex\u223cQ [f\u2217(T (x))]) , (4)\nwhere T is an arbitrary class of functions T : X \u2192 R. The above derivation yields a lower bound for two reasons: first, because of Jensen\u2019s inequality when swapping the integration and supremum operations. Second, the class of functions T may contain only a subset of all possible functions. By taking the variation of the lower bound in (4) w.r.t. T , we find that under mild conditions on f [25], the bound is tight for\nT \u2217(x) = f \u2032 ( p(x)\nq(x)\n) , (5)\nwhere f \u2032 denotes the first order derivative of f . This condition can serve as a guiding principle for choosing f and designing the class of functions T . For example, the popular reverse Kullback-Leibler divergence corresponds to f(u) = \u2212 log(u) resulting in T \u2217(x) = \u2212q(x)/p(x), see Table 5. We list common f -divergences in Table 5 and provide their Fenchel conjugates f\u2217 and the domains domf\u2217 in Table 6. We provide plots of the generator functions and their conjugates in the supplementary materials."}, {"heading": "2.3 Variational Divergence Minimization (VDM)", "text": "We now use the variational lower bound (4) on the f -divergence Df (P\u2016Q) in order to estimate a generative model Q given a true distribution P .\nTo this end, we follow the generative-adversarial approach [10] and use two neural networks, Q and T . Q is our generative model, taking as input a random vector and outputting a sample of interest. We parametrize Q through a vector \u03b8 and write Q\u03b8. T is our variational function, taking as input a sample and returning a scalar. We parametrize T using a vector \u03c9 and write T\u03c9 .\nWe can learn a generative model Q\u03b8 by finding a saddle-point of the following f -GAN objective function, where we minimize with respect to \u03b8 and maximize with respect to \u03c9,\nF (\u03b8, \u03c9) = Ex\u223cP [T\u03c9(x)]\u2212 Ex\u223cQ\u03b8 [f\u2217(T\u03c9(x))] . (6)\nTo optimize (6) on a given finite training data set, we approximate the expectations using minibatch samples. To approximate Ex\u223cP [\u00b7] we sample B instances without replacement from the training set. To approximate Ex\u223cQ\u03b8 [\u00b7] we sample B instances from the current generative model Q\u03b8."}, {"heading": "2.4 Representation for the Variational Function", "text": "To apply the variational objective (6) for different f -divergences, we need to respect the domain domf\u2217 of the conjugate functions f\u2217. To this end, we assume that variational function T\u03c9 is represented in the form T\u03c9(x) = gf (V\u03c9(x)) and rewrite the saddle objective (6) as follows:\nF (\u03b8, \u03c9) = Ex\u223cP [gf (V\u03c9(x))] + Ex\u223cQ\u03b8 [\u2212f\u2217(gf (V\u03c9(x)))] , (7)\nwhere V\u03c9 : X \u2192 R without any range constraints on the output, and gf : R\u2192 domf\u2217 is an output activation function specific to the f -divergence used. In Table 6 we propose suitable output activation functions for the various conjugate functions f\u2217 and their domains.1 Although the choice of gf is somewhat arbitrary, we choose all of them to be monotone increasing functions so that a large output V\u03c9(x) corresponds to the belief of the variational function that the sample x comes from the data distribution P as in the GAN case; see Figure 1. It is also instructive to look at the second term \u2212f\u2217(gf (v)) in the saddle objective (7). This term is typically (except for the Pearson \u03c72 divergence) a decreasing function of the output V\u03c9(x) favoring variational functions that output negative numbers for samples from the generator.\nWe can see the GAN objective,\nF (\u03b8, \u03c9) = Ex\u223cP [logD\u03c9(x)] + Ex\u223cQ\u03b8 [log(1\u2212D\u03c9(x))] , (8)\nas a special instance of (7) by identifying each terms in the expectations of (7) and (8). In particular, choosing the last nonlinearity in the discriminator as the sigmoid D\u03c9(x) = 1/(1 + e\u2212V\u03c9(x)), corresponds to output activation function is gf (v) = \u2212 log(1 + e\u2212v); see Table 6."}, {"heading": "2.5 Example: Univariate Mixture of Gaussians", "text": "To demonstrate the properties of the different f -divergences and to validate the variational divergence estimation framework we perform an experiment similar to the one of [24].\nSetup. We approximate a mixture of Gaussians by learning a Gaussian distribution. We represent our model Q\u03b8 using a linear function which receives a random z \u223c N (0, 1) and outputs G\u03b8(z) = \u00b5+\u03c3z, where \u03b8 = (\u00b5, \u03c3) are the two scalar parameters to be learned. For the variational function T\u03c9\n1Note that for numerical implementation we recommend directly implementing the scalar function f\u2217(gf (\u00b7)) robustly instead of evaluating the two functions in sequence; see Figure 1.\nwe use a neural network with two hidden layers having 64 units each and tanh activations. We optimise the objective F (\u03c9, \u03b8) by using the single-step gradient method presented in Section 3. In each step we sample batches of size 1024 each for both p(x) and p(z) and we use a step-size of \u03b7 = 0.01 for updating both \u03c9 and \u03b8. We compare the results to the best fit provided by the exact optimization of Df (P\u2016Q\u03b8) w.r.t. \u03b8, which is feasible in this case by solving the required integrals in (2) numerically. We use (\u03c9\u0302, \u03b8\u0302) (learned) and \u03b8\u2217 (best fit) to distinguish the parameters sets used in these two approaches.\nResults. The left side of Table 3 shows the optimal divergence and objective values Df (P ||Q\u03b8\u2217) and F (\u03c9\u0302, \u03b8\u0302) as well as the resulting means and standard deviations. Note that the results are in line with the lower bound property, that is, we have Df (P ||Q\u03b8\u2217) \u2265 F (\u03c9\u0302, \u03b8\u0302). There is a good correspondence between the gap in objectives and the difference between the fitted means and standard deviations. The right side of Table 3 shows the results of the following experiment: (1) we train T\u03c9 and Q\u03b8 using a particular divergence, then (2) we estimate the divergence and re-train T\u03c9 while keeping Q\u03b8 fixed. As expected, Q\u03b8 performs best on the divergence it was trained with. Further details showing detailed plots of the fitted Gaussians and the optimal variational functions are presented in the supplementary materials.\nIn summary, the above results demonstrate that when the generative model is misspecified and does not contain the true distribution, the divergence function used for estimation has a strong influence on which model is learned."}, {"heading": "3 Algorithms for Variational Divergence Minimization (VDM)", "text": "We now discuss numerical methods to find saddle points of the objective (6). To this end, we distinguish two methods; first, the alternating method originally proposed by Goodfellow et al. [10], and second, a more direct single-step optimization procedure.\nIn our variational framework, the alternating gradient method can be described as a double-loop method; the internal loop tightens the lower bound on the divergence, whereas the outer loop improves the generator model. While the motivation for this method is plausible, in practice the choice taking a single step in the inner loop is popular. Goodfellow et al. [10] provide a local convergence guarantee."}, {"heading": "3.1 Single-Step Gradient Method", "text": "Motivated by the success of the alternating gradient method with a single inner step, we propose a simpler algorithm shown in Algorithm 1. The algorithm differs from the original one in that there is no inner loop and the gradients with respect to \u03c9 and \u03b8 are computed in a single back-propagation.\nAlgorithm 1 Single-Step Gradient Method 1: function SINGLESTEPGRADIENTITERATION(P, \u03b8t, \u03c9t, B, \u03b7) 2: Sample XP = {x1, . . . , xB} and XQ = {x\u20321, . . . , x\u2032B}, from P and Q\u03b8t , respectively. 3: Update: \u03c9t+1 = \u03c9t + \u03b7\u2207\u03c9F (\u03b8t, \u03c9t). 4: Update: \u03b8t+1 = \u03b8t \u2212 \u03b7\u2207\u03b8F (\u03b8t, \u03c9t). 5: end function\nAnalysis. Here we show that Algorithm 1 geometrically converges to a saddle point (\u03b8\u2217, \u03c9\u2217) if there is a neighborhood around the saddle point in which F is strongly convex in \u03b8 and strongly concave in \u03c9. These conditions are similar to the assumptions made in [10] and can be formalized as follows:\n\u2207\u03b8F (\u03b8\u2217, \u03c9\u2217) = 0, \u2207\u03c9F (\u03b8\u2217, \u03c9\u2217) = 0, \u22072\u03b8F (\u03b8, \u03c9) \u03b4I, \u22072\u03c9F (\u03b8, \u03c9) \u2212\u03b4I. (9)\nThese assumptions are necessary except for the \u201cstrong\u201d part in order to define the type of saddle points that are valid solutions of our variational framework. Note that although there could be many saddle points that arise from the structure of deep networks [6], they do not qualify as the solution of our variational framework under these assumptions.\nFor convenience, let\u2019s define \u03c0t = (\u03b8t, \u03c9t). Now the convergence of Algorithm 1 can be stated as follows (the proof is given in the supplementary material): Theorem 1. Suppose that there is a saddle point \u03c0\u2217 = (\u03b8\u2217, \u03c9\u2217) with a neighborhood that satisfies conditions (9). Moreover, we define J(\u03c0) = 12\u2016\u2207F (\u03c0)\u2016 2 2 and assume that in the above neighborhood, F is sufficiently smooth so that there is a constantL > 0 such that \u2016\u2207J(\u03c0\u2032)\u2212\u2207J(\u03c0)\u20162 \u2264 L\u2016\u03c0\u2032\u2212\u03c0\u20162 for any \u03c0, \u03c0\u2032 in the neighborhood of \u03c0\u2217. Then using the step-size \u03b7 = \u03b4/L in Algorithm 1, we have\nJ(\u03c0t) \u2264 ( 1\u2212 \u03b4 2\n2L\n)t J(\u03c00)\nThat is, the squared norm of the gradient\u2207F (\u03c0) decreases geometrically."}, {"heading": "3.2 Practical Considerations", "text": "Here we discuss principled extensions of the heuristic proposed in [10] and real/fake statistics discussed by Larsen and S\u00f8nderby2. Furthermore we discuss practical advice that slightly deviate from the principled viewpoint.\nGoodfellow et al. [10] noticed that training GAN can be significantly sped up by maximizing Ex\u223cQ\u03b8 [logD\u03c9(x)] instead of minimizing Ex\u223cQ\u03b8 [log (1\u2212D\u03c9(x))] for updating the generator. In the more general f -GAN Algorithm (1) this means that we replace line 4 with the update\n\u03b8t+1 = \u03b8t + \u03b7\u2207\u03b8Ex\u223cQ\u03b8t [gf (V\u03c9t(x))], (10)\nthereby maximizing the generator output. This is not only intuitively correct but we can show that the stationary point is preserved by this change using the same argument as in [10]; we found this useful also for other divergences.\nLarsen and S\u00f8nderby recommended monitoring real and fake statistics, which are defined as the true positive and true negative rates of the variational function viewing it as a binary classifier. Since our output activation gf are all monotone, we can derive similar statistics for any f -divergence by only changing the decision threshold. Due to the link between the density ratio and the variational function (5), the threshold lies at f \u2032(1) (see Table 6). That is, we can interpret the output of the variational function as classifying the input x as a true sample if the variational function T\u03c9(x) is larger than f \u2032(1), and classifying it as a sample from the generator otherwise.\nWe found Adam [17] and gradient clipping to be useful especially in the large scale experiment on the LSUN dataset."}, {"heading": "4 Experiments", "text": "We now train generative neural samplers based on VDM on the MNIST and LSUN datasets.\nMNIST Digits. We use the MNIST training data set (60,000 samples, 28-by-28 pixel images) to train the generator and variational function model proposed in [10] for various f -divergences. With z \u223c Uniform100(\u22121, 1) as input, the generator model has two linear layers each followed by batch normalization and ReLU activation and a final linear layer followed by the sigmoid function. The variational function V\u03c9(x) has three linear layers with exponential linear unit [4] in between. The\n2http://torch.ch/blog/2015/11/13/gan.html\nfinal activation is specific to each divergence and listed in Table 6. As in [27] we use Adam with a learning rate of \u03b1 = 0.0002 and update weight \u03b2 = 0.5. We use a batchsize of 4096, sampled from the training set without replacement, and train each model for one hour. We also compare against variational autoencoders [18] with 20 latent dimensions.\nResults and Discussion. We evaluate the performance using the kernel density estimation (Parzen window) approach used in [10]. To this end, we sample 16k images from the model and estimate a Parzen window estimator using an isotropic Gaussian kernel bandwidth using three fold cross validation. The final density model is used to evaluate the average log-likelihood on the MNIST test set (10k samples). We show the results in Table 4, and some samples from our models in Figure 2.\nThe use of the KDE approach to log-likelihood estimation has known deficiencies [31]. In particular, for the dimensionality used in MNIST (d = 784) the number of model samples required to obtain accurate log-likelihood estimates is infeasibly large. We found a large variability (up to 50 nats) between multiple repetitions. As such the results are not entirely conclusive. We also trained the same KDE estimator on the MNIST training set, achieving a significantly higher holdout likelihood. However, it is reassuring to see that the model trained for the Kullback-Leibler divergence indeed achieves a high holdout likelihood compared to the GAN model.\nLSUN Natural Images. Through the DCGAN work [27] the generative-adversarial approach has shown real promise in generating natural looking images. Here we use the same architecture as as in [27] and replace the GAN objective with our more general f -GAN objective.\nWe use the large scale LSUN database [34] of natural images of different categories. To illustrate the different behaviors of different divergences we train the same model on the classroom category of images, containing 168,103 images of classroom environments, rescaled and center-cropped to 96-by-96 pixels.\nSetup. We use the generator architecture and training settings proposed in DCGAN [27]. The model receives z \u2208 Uniformdrand(\u22121, 1) and feeds it through one linear layer and three deconvolution layers with batch normalization and ReLU activation in between. The variational function is the same as the discriminator architecture in [27] and follows the structure of a convolutional neural network with batch normalization, exponential linear units [4] and one final linear layer.\nResults. Figure 3 shows 16 random samples from neural samplers trained using GAN, KL, and squared Hellinger divergences. All three divergences produce equally realistic samples. Note that the difference in the learned distribution Q\u03b8 arise only when the generator model is not rich enough."}, {"heading": "5 Related Work", "text": "We now discuss how our approach relates to existing work. Building generative models of real world distributions is a fundamental goal of machine learning and much related work exists. We only discuss work that applies to neural network models.\nMixture density networks [2] are neural networks which directly regress the parameters of a finite parametric mixture model. When combined with a recurrent neural network this yields impressive generative models of handwritten text [11].\nNADE [19] and RNADE [33] perform a factorization of the output using a predefined and somewhat arbitrary ordering of output dimensions. The resulting model samples one variable at a time conditioning on the entire history of past variables. These models provide tractable likelihood evaluations and compelling results but it is unclear how to select the factorization order in many applications .\nDiffusion probabilistic models [29] define a target distribution as a result of a learned diffusion process which starts at a trivial known distribution. The learned model provides exact samples and approximate log-likelihood evaluations.\nNoise contrastive estimation (NCE) [13] is a method that estimates the parameters of unnormalized probabilistic models by performing non-linear logistic regression to discriminate the data from artificially generated noise. NCE can be viewed as a special case of GAN where the discriminator is constrained to a specific form that depends on the model (logistic regression classifier) and the generator (kept fixed) is providing the artificially generated noise (see supplementary material).\nThe generative neural sampler models of [22] and [3] did not provide satisfactory learning methods; [22] used importance sampling and [3] expectation maximization. The main difference to GAN and to our work really is in the learning objective, which is effective and computationally inexpensive.\nVariational auto-encoders (VAE) [18, 28] are pairs of probabilistic encoder and decoder models which map a sample to a latent representation and back, trained using a variational Bayesian learning objective. The advantage of VAEs is in the encoder model which allows efficient inference from observation to latent representation and overall they are a compelling alternative to f -GANs and recent work has studied combinations of the two approaches [23]\nAs an alternative to the GAN training objective the work [20] and independently [7] considered the use of the kernel maximum mean discrepancy (MMD) [12, 9] as a training objective for probabilistic models. This objective is simpler to train compared to GAN models because there is no explicitly represented variational function. However, it requires the choice of a kernel function and the reported results so far seem slightly inferior compared to GAN. MMD is a particular instance of a larger class of probability metrics [30] which all take the form D(P,Q) = supT\u2208T |Ex\u223cP [T (x)]\u2212 Ex\u223cQ[T (x)]|, where the function class T is chosen in a manner specific to the divergence. Beyond MMD other popular metrics of this form are the total variation metric (also an f -divergence), the Wasserstein distance, and the Kolmogorov distance.\nIn [16] a generalisation of the GAN objective is proposed by using an alternative Jensen-Shannon divergence that mimics an interpolation between the KL and the reverse KL divergence and has Jensen-Shannon as its mid-point. It can be shown that with \u03c0 close to 0 and 1 it leads to a behavior similar the objectives resulting from the KL and reverse KL divergences (see supplementary material)."}, {"heading": "6 Discussion", "text": "Generative neural samplers offer a powerful way to represent complex distributions without limiting factorizing assumptions. However, while the purely generative neural samplers as used in this paper are interesting their use is limited because after training they cannot be conditioned on observed data and thus are unable to provide inferences.\nWe believe that in the future the true benefits of neural samplers for representing uncertainty will be found in discriminative models and our presented methods extend readily to this case by providing additional inputs to both the generator and variational function as in the conditional GAN model [8].\nAcknowledgements. We thank Ferenc Husza\u0301r for discussions on the generative-adversarial approach."}, {"heading": "B f -divergences and Generator-Conjugate Pairs", "text": "In Table 5 we show an extended list of f-divergences Df (P\u2016Q) together with their generators f(u) and the corresponding optimal variational functions T \u2217(x). For all divergences we have f : domf \u2192 R \u222a {+\u221e}, where f is convex and lower-semicontinuous. Also we have f(1) = 0 which ensures that Df (P\u2016P ) = 0 for any distribution P . As shown by [10] GAN is related to the Jensen-Shannon divergence through DGAN = 2DJS \u2212 log(4). The GAN generator function f does not satisfy f(1) = 0 hence DGAN(P\u2016P ) 6= 0. Table 6 lists the convex conjugate functions f\u2217(t) of the generator functions f(u) in Table 5, their domains, as well as the activation functions gf we use in the last layers of the generator networks to obtain a correct mapping of the network outputs into the domains of the conjugate functions.\nThe panels of Figure 4 show the generator functions and the corresponding convex conjugate functions for a variety of f-divergences.\nName Df (P\u2016Q) Generator f(u) T \u2217(x) Total variation 12 \u222b |p(x)\u2212 q(x)| dx 12 |u\u2212 1| 1 2 sign( p(x) q(x) \u2212 1)\nKullback-Leibler \u222b p(x) log p(x)q(x) dx u log u 1 + log p(x) q(x)\nReverse Kullback-Leibler \u222b q(x) log q(x)p(x) dx \u2212 log u \u2212 q(x) p(x)\nPearson \u03c72 \u222b (q(x)\u2212p(x))2\np(x) dx (u\u2212 1) 2 2(p(x)q(x) \u2212 1) Neyman \u03c72 \u222b (p(x)\u2212q(x))2\nq(x) dx (1\u2212u)2 u 1\u2212 [ q(x) p(x) ]2 Squared Hellinger \u222b (\u221a p(x)\u2212 \u221a q(x) )2 dx ( \u221a u\u2212 1)2 ( \u221a p(x) q(x) \u2212 1) \u00b7 \u221a q(x) p(x)\nJeffrey \u222b (p(x)\u2212 q(x)) log ( p(x) q(x) ) dx (u\u2212 1) log u 1 + log p(x)q(x) \u2212 q(x) p(x)\nJensen-Shannon 12 \u222b p(x) log 2p(x)p(x)+q(x) + q(x) log 2q(x) p(x)+q(x) dx \u2212(u+ 1) log 1+u 2 + u log u log 2p(x) p(x)+q(x)\nJensen-Shannon-weighted \u222b p(x)\u03c0 log p(x)\n\u03c0p(x)+(1\u2212\u03c0)q(x) + (1\u2212 \u03c0)q(x) log q(x) \u03c0p(x)+(1\u2212\u03c0)q(x) dx \u03c0u log u\u2212 (1\u2212 \u03c0 + \u03c0u) log(1\u2212 \u03c0 + \u03c0u) \u03c0 log p(x) (1\u2212\u03c0)q(x)+\u03c0p(x) GAN \u222b p(x) log 2p(x)p(x)+q(x) + q(x) log 2q(x) p(x)+q(x) dx\u2212 log(4) u log u\u2212 (u+ 1) log(u+ 1) log p(x) p(x)+q(x)\n\u03b1-divergence (\u03b1 /\u2208 {0, 1}) 1\u03b1(\u03b1\u22121) \u222b ( p(x) [( q(x) p(x) )\u03b1 \u2212 1 ] \u2212 \u03b1(q(x)\u2212 p(x)) ) dx 1\u03b1(\u03b1\u22121) (u \u03b1 \u2212 1\u2212 \u03b1(u\u2212 1)) 1\u03b1\u22121 [[p(x) q(x) ]\u03b1\u22121 \u2212 1]\nTable 5: List of f -divergences Df (P\u2016Q) together with generator functions and the optimal variational functions."}, {"heading": "C Proof of Theorem 1", "text": "In this section we present the proof of Theorem 2 from Section 3 of the main text. For completeness, we reiterate the conditions and the theorem.\nWe assume that F is strongly convex in \u03b8 and strongly concave in \u03c9 such that\n\u2207\u03b8F (\u03b8\u2217, \u03c9\u2217) = 0, \u2207\u03c9F (\u03b8\u2217, \u03c9\u2217) = 0, (11) \u22072\u03b8F (\u03b8, \u03c9) \u03b4I, \u22072\u03c9F (\u03b8, \u03c9) \u2212\u03b4I. (12)\nThese assumptions are necessary except for the \u201cstrong\u201d part in order to define the type of saddle points that are valid solutions of our variational framework.\n10-1 100 101\nu= dP/d\u00b5\ndQ/d\u00b5\n0\n5\n10\n15\n20\nf( u )\nf-divergence Generators f(u)\nSquared Hellinger Kullback-Leibler Pearson Chi-Square Reverse Kullback-Leibler Total Variation\nJeffrey Neyman Chi-Square GAN Jensen-Shannon\n5 4 3 2 1 0 1 2 t\n3\n2\n1\n0\n1\n2\n3\n4\n5\nf \u2217 (t\n)\nf-divergence Conjugates f \u2217 (t)\nJeffreys Squared Hellinger Kullback-Leibler Pearson Chi-Square Reverse Kullback-Leibler Total Variation Neyman Chi-Square GAN Jensen-Shannon\nFigure 4: Generator-conjugate (f, f\u2217) pairs in the variational framework of Nguyen et al. [25]. Left:\ngenerator functions f used in the f -divergence Df (P\u2016Q) =\n\u222b\nX q(x)f\n(\np(x) q(x)\n)\ndx. Right: conjugate\nfunctions f\u2217 in the variational divergence lower bound Df (P\u2016Q) \u2265 supT\u2208T \u222b X p(x)T (x) \u2212 q(x)f\u2217(T (x)) dx.\nWe define \u03c0t = (\u03b8t, \u03c9t) and use the notation \u2207F (\u03c0) = ( \u2207\u03b8F (\u03b8, \u03c9) \u2207\u03c9F (\u03b8, \u03c9) ) , \u2207\u0303F (\u03c0) = ( \u2212\u2207\u03b8F (\u03b8, \u03c9) \u2207\u03c9F (\u03b8, \u03c9) ) .\nWith this notation, Algorithm 1 in the main text can be written as\n\u03c0t+1 = \u03c0t + \u03b7\u2207\u0303F (\u03c0t).\nGiven the above assumptions and notation, in Section 3 of the main text we formulate the following theorem. Theorem 2. Suppose that there is a saddle point \u03c0\u2217 = (\u03b8\u2217, \u03c9\u2217) with a neighborhood that satisfies conditions (11) and (12). Moreover we define J(\u03c0) = 12\u2016\u2207F (\u03c0)\u2016 2 2 and assume that in the above neighborhood, F is sufficiently smooth so that there is a constant L > 0 and\nJ(\u03c0\u2032) \u2264 J(\u03c0) + \u3008\u2207J(\u03c0), \u03c0\u2032 \u2212 \u03c0\u3009+ L 2 \u2016\u03c0\u2032 \u2212 \u03c0\u201622 (13)\nfor any \u03c0, \u03c0\u2032 in the neighborhood of \u03c0\u2217. Then using the step-size \u03b7 = \u03b4/L in Algorithm 1, we have\nJ(\u03c0t) \u2264 ( 1\u2212 \u03b4 2\n2L\n)t J(\u03c00)\nwhere L is the smoothness parameter of J . That is, the squared norm of the gradient \u2207F (\u03c0) decreases geometrically.\nProof. First, note that the gradient of J can be written as\n\u2207J(\u03c0) = \u22072F (\u03c0)\u2207F (\u03c0).\nTherefore we notice that,\u2329 \u2207\u0303F (\u03c0),\u2207J(\u03c0) \u232a = \u2329 \u2207\u0303F (\u03c0),\u22072F (\u03c0)\u2207F (\u03c0) \u232a = \u2329( \u2212\u2207\u03b8F (\u03b8, \u03c9) \u2207\u03c9F (\u03b8, \u03c9) ) , ( \u22072\u03b8F (\u03b8, \u03c9) \u2207\u03b8\u2207\u03c9F (\u03b8, \u03c9) \u2207\u03c9\u2207\u03b8F (\u03b8, \u03c9) \u22072\u03c9F (\u03b8, \u03c9) )( \u2207\u03b8F (\u03b8, \u03c9) \u2207\u03c9F (\u03b8, \u03c9)\n)\u232a = \u2212 \u2329 \u2207\u03b8F (\u03b8, \u03c9),\u22072\u03b8F (\u03b8, \u03c9)\u2207\u03b8F (\u03b8, \u03c9) \u232a + \u2329 \u2207\u03c9F (\u03b8, \u03c9),\u22072\u03c9F (\u03b8, \u03c9)\u2207\u03c9F (\u03b8, \u03c9)\n\u232a \u2264 \u2212\u03b4 ( \u2016\u2207\u03b8F (\u03b8, \u03c9)\u201622 + \u2016\u2207\u03c9F (\u03b8, \u03c9)\u201622 ) = \u2212\u03b4\u2016\u2207F (\u03c0)\u201622 (14)\nIn other words, Algorithm 1 decreases J by an amount proportional to the squared norm of\u2207F (\u03c0). Now combining the smoothness (13) with Algorithm 1, we get\nJ(\u03c0t+1) \u2264 J(\u03c0t) + \u03b7 \u2329 \u2207J(\u03c0t), \u2207\u0303F (\u03c0t) \u232a + L\u03b72\n2 \u2016\u2207\u0303F (\u03c0t)\u201622 \u2264 ( 1\u2212 \u03b4\u03b7 + L\u03b7 2\n2\n) J(\u03c0t)\n= ( 1\u2212 \u03b4 2\n2L\n) J(\u03c0t),\nwhere we used sufficient decrease (14) and J(\u03c0) = \u2016\u2207F (\u03c0)\u201622 = \u2016\u2207\u0303F (\u03c0)\u201622 in the second inequality, and the final equality follows by taking \u03b7 = \u03b4/L."}, {"heading": "D Related Algorithms", "text": "Due to recent interest in GAN type models, there have been attempts to derive other divergence measures and algorithms. In particular, an alternative Jensen-Shannon divergence has been derived in [16] and a heuristic algorithm that behaves similarly to the one resulting from this new divergence has been proposed in [15].\nIn this section we summarise (some of) the current algorithms and show how they are related. Note that some algorithms use heuristics that do not correspond to saddle point optimisation, that is, in the corresponding maximization and minimization steps they optimise alternative objectives that do not add up to a coherent joint objective. We include a short discussion of [13] because it can be viewed as a special case of GAN.\nTo illustrate how the discussed algorithms work, we define the objective function\nF (\u03b8, \u03c9;\u03b1, \u03b2) =Ex\u223cP [logD\u03c9(x)] + \u03b1Ex\u223cQ\u03b8 [log(1\u2212D\u03c9(x))]\u2212 \u03b2Ex\u223cQ\u03b8 [log(D\u03c9(x))], (15)\nwhere we introduce two scalar parameters, \u03b1 and \u03b2, to help us highlight the differences between the algorithms shown in Table 7.\nNoise-Contrastive Estimation (NCE)\nNCE [13] is a method that estimates the parameters of an unnormalised model p(x;\u03c9) by performing non-linear logistic regression to discriminate between the model and artificially generated noise. To achieve this NCE casts the estimation problem as a ML estimation in a binary classification model where the data is augmented with artificially generated data. The \u201ctrue\u201d data items are labeled as positives while the artificially generated data items are labeled as negatives. The discriminant function is defined as D\u03c9(x) = p(x;\u03c9)/(p(x;\u03c9) + q(x)) where q(x) denotes the distribution of the artificially generated data, typically a Gaussian parameterised by the empirical mean and covariance of the true data. ML estimation in this binary classification model results in an objective that has the form (15) with \u03b1 = 1 amd \u03b2 = 0, where the expectations are taken w.r.t. the empirical distribution of augmented data. As a result, NCE can be viewed as a special case of GAN where the generator is fixed and we only have maximise the objective w.r.t. the parameters of the discriminator. Another slight difference is that in this case the data distribution is learned through the discriminator not the generator, however, the method has many conceptual similarities to GAN.\nGAN-1 and GAN-2\nThe first algorithm (GAN-1) proposed in [10] performs a stochastic gradient ascent-descent on the objective with \u03b1 = 1 and \u03b2 = 0, however, the authors point out that in practice it is more advantageous to minimise \u2212Ex\u223cQ\u03b8 [logD\u03c9(x)] instead of Ex\u223cQ\u03b8 [log(1\u2212D\u03c9(x))], we denote this by GAN-2. This is motivated by the observation that in the early stages of training when Q\u03b8 is not sufficiently well fitted, D\u03c9 can saturate fast leading to weak gradients in Ex\u223cQ\u03b8 [log(1\u2212D\u03c9(x))]. The \u2212Ex\u223cQ\u03b8 [logD\u03c9(x)] term, however, can provide stronger gradients and leads to the same fixed point. This heuristic can be viewed as using \u03b1 = 1, \u03b2 = 0 in the maximisation step and \u03b1 = 0, \u03b2 = 1 in the minimisation step3.\nGAN-3\nIn [15] a further heuristic for the minimisation step is proposed. Formally, it can be viewed as a combination of the minimisation steps in GAN-1 and GAN-2. In the proposed algorithm, the maximisation step is performed similarly (\u03b1 = 1, \u03b2 = 0), but the minimisation is done using \u03b1 = 1 and \u03b2 = 1. This choice is motivated by KL optimality arguments. The author makes the observation that the optimal discriminator is given by\nD\u2217(x) = p(x)\nq\u03b8(x) + p(x) (16)\nand thus, close to optimality, the minimisation of Ex\u223cQ\u03b8 [log(1\u2212D\u03c9(x))]\u2212Ex\u223cQ\u03b8 [logD\u03c9(x)] corresponds to the minimisation of the reverse KL divergence Ex\u223cQ\u03b8 [log(q\u03b8(x)/p(x))]. This approach can be viewed as choosing \u03b1 = 1 and \u03b2 = 1 in the minimisation step.\nRemarks on the Weighted Jensen-Shannon Divergence in [16]\nThe GAN/variational objective corresponding to alternative Jensen-Shannon divergence measure proposed in [16] (see Jensen-Shannon-weighted in Table 1) is\nF (\u03b8, \u03c9;\u03c0) =Ex\u223cP [logD\u03c9(x)]\u2212 (1\u2212 \u03c0)Ex\u223cQ\u03b8 [ log\n1\u2212 \u03c0 1\u2212 \u03c0D\u03c9(x)1/\u03c0\n] . (17)\nNote that we have the T\u03c9(x) = logD\u03c9(x) correspondence. According to the definition of the variational objective, when T\u03c9 is close to optimal then in the minimisation step the objective function is close to the chosen divergence. In this case the optimal discriminator is\nD\u2217(x)1/\u03c0 = p(x)\n(1\u2212 \u03c0)q\u03b8(x) + \u03c0p(x) . (18)\nThe objective in (17) vanishes when \u03c0 \u2208 {0, 1}, however, when \u03c0 is only is close to 0 and 1, it can behave similarly to the KL and reverse KL objectives, respectively. Overall, the connection between\n3 A somewhat similar observation regarding the artificially generated data is made in [13]: in order to have meaningful training one should choose the artificially generated data to be close the the true data, hence the choice of an ML multivariate Gaussian.\nGAN-3 and the optimisation of (17) can only be considered as approximate. To obtain an exact KL or reverse KL behavior one can use the corresponding variational objectives. For a simple illustration of how these divergences behave see Section 2.5 and Section E below."}, {"heading": "E Details of the Univariate Example", "text": "We follow up on the example in Section 2.5 of the main text by presenting further details about the quality and behavior of the approximations resulting from using various divergence measures. For completeness, we reiterate the setup and then we present further results.\nSetup. We approximate a mixture of Gaussian 4 by learning a Gaussian distribution. The model Q\u03b8 is represented by a linear function which receives a random z \u223c N (0, 1) and outputs\nG\u03b8(z) = \u00b5+ \u03c3z, (19)\nwhere \u03b8 = (\u00b5, \u03c3) are the parameters to be learned. For the variational function T\u03c9 we use the neural network\nx \u2192 Linear(1,64)\u2192 Tanh\u2192 Linear(64,64)\u2192 Tanh\u2192 Linear(64,1). (20) We optimise the objective F (\u03c9, \u03b8) by using the single-step gradient method presented in Section 3.1 of the main text . In each step we sample batches of size 1024 each for both p(x) and p(z) and we use a step-size of 0.01 for updating both \u03c9 and \u03b8. We compare the results to the best fit provided by the exact optimisation of Df (P\u2016Q\u03b8) w.r.t. \u03b8, which is feasible in this case by solving the required integrals numerically. We use (\u03c9\u0302, \u03b8\u0302) (learned) and \u03b8\u2217 (best fit) to distinguish the parameters sets used in these two approaches.\nResults. The panels in Figure 5 shows the density function of the data distribution as well as the Gaussian approximations corresponding to a few f -divergences form Table 5. As expected, the KL approximation covers the data distribution by fitting its mean and variance while KL-rev has more of a mode-seeking behavior [24]. The fit corresponding to the Jensen-Shannon divergence is somewhere between KL and KL-rev. All Gaussian approximations resulting from neural network training are close to the ones obtained by direct optimisation of the divergence (learned vs. best fit).\nIn the right\u2013bottom panel of Figure 5 we compare the variational functions T\u03c9\u0302 and T \u2217. The latter is defined as T \u2217(x) = f \u2032(p(x)/q\u03b8\u2217(x)), see main text. The objective value corresponding to T \u2217 is the true divergence Df (P ||Q\u03b8\u2217). In the majority of the cases our T\u03c9\u0302 is close to T \u2217 in the area of interest. The discrepancies around the tails are due to the fact that (1) the class of functions resulting from the tanh activation function has limited capability representing the tails, and (2) in the Gaussian case there is a lack of data in the tails. These limitations, however, do not have a significant effect on the learned parameters."}, {"heading": "F Details of the Experiments", "text": "In this section we present the technical setup as well as the architectures we used in the experiments described in Section 4.\nF.1 Deep Learning Environment\nWe use the deep learning framework Chainer [32], version 1.8.1, running on CUDA 7.5 with CuDNN v5 on NVIDIA GTX TITAN X.\nF.2 MNIST Setup\nMNIST Generator\nz \u2192 Linear(100, 1200)\u2192 BN\u2192 ReLU\u2192 Linear(1200, 1200)\u2192 BN\u2192 ReLU \u2192 Linear(1200, 784)\u2192 Sigmoid (21)\nAll weights are initialized at a weight scale of 0.05, as in [10].\nMNIST Variational Function\nx \u2192 Linear(784,240)\u2192 ELU\u2192 Linear(240,240)\u2192 ELU\u2192 Linear(240,1), (22) where ELU is the exponential linear unit [4]. All weights are initialized at a weight scale of 0.005, one order of magnitude smaller than in [10].\nVariational Autoencoders For the variational autoencoders [18], we used the example implementation included with Chainer [32]. We trained for 100 epochs with 20 latent dimensions.\n4The plots on Figure 5 correspond to p(x) = (1\u2212w)N(x;m1, v1)+wN(x;m2, v2) withw = 0.67,m1 = \u22121, v1 = 0.0625,m2 = 2, v2 = 2.\nF.3 LSUN Natural Images\nz \u2192 Linear(100, 6 \u00b7 6 \u00b7 512)\u2192 BN\u2192 ReLU\u2192 Reshape(512,6,6) \u2192 Deconv(512,256)\u2192 BN\u2192 ReLU\u2192 Deconv(256,128)\u2192 BN\u2192 ReLU \u2192 Deconv(128,64)\u2192 BN\u2192 ReLU\u2192 Deconv(64,3), (23)\nwhere all Deconv operations use a kernel size of four and a stride of two."}], "references": [{"title": "A general class of coefficients of divergence of one distribution from another", "author": ["S.M. Ali", "S.D. Silvey"], "venue": "JRSS (B), pages 131\u2013142,", "citeRegEx": "1", "shortCiteRegEx": null, "year": 1966}, {"title": "Mixture density networks", "author": ["C.M. Bishop"], "venue": "Technical report, Aston University,", "citeRegEx": "2", "shortCiteRegEx": null, "year": 1994}, {"title": "GTM: The generative topographic mapping", "author": ["C.M. Bishop", "M. Svens\u00e9n", "C.K.I. Williams"], "venue": "Neural Computation, 10(1):215\u2013234,", "citeRegEx": "3", "shortCiteRegEx": null, "year": 1998}, {"title": "Fast and accurate deep network learning by exponential linear units (ELUs)", "author": ["D.A. Clevert", "T. Unterthiner", "S. Hochreiter"], "venue": "arXiv:1511.07289,", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2015}, {"title": "Information theory and statistics: A tutorial", "author": ["I. Csisz\u00e1r", "P.C. Shields"], "venue": "Foundations and Trends in Communications and Information Theory, 1:417\u2013528,", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2004}, {"title": "Identifying and attacking the saddle point problem in high-dimensional non-convex optimization", "author": ["Y.N. Dauphin", "R. Pascanu", "C. Gulcehre", "K. Cho", "S. Ganguli", "Y. Bengio"], "venue": "NIPS, pages 2933\u20132941,", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2014}, {"title": "Training generative neural networks via maximum mean discrepancy optimization", "author": ["G.K. Dziugaite", "D.M. Roy", "Z. Ghahramani"], "venue": "UAI, pages 258\u2013267,", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2015}, {"title": "Conditional generative adversarial nets for convolutional face generation", "author": ["J. Gauthier"], "venue": "Class Project for Stanford CS231N: Convolutional Neural Networks for Visual Recognition, Winter semester 2014,", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2014}, {"title": "Strictly proper scoring rules, prediction, and estimation", "author": ["T. Gneiting", "A.E. Raftery"], "venue": "JASA, 102(477): 359\u2013378,", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2007}, {"title": "Generative adversarial nets", "author": ["I. Goodfellow", "J. Pouget-Abadie", "M. Mirza", "B. Xu", "D. Warde-Farley", "S. Ozair", "A. Courville", "Y. Bengio"], "venue": "NIPS, pages 2672\u20132680,", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2014}, {"title": "Generating sequences with recurrent neural networks", "author": ["A. Graves"], "venue": "arXiv:1308.0850,", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2013}, {"title": "A kernel statistical test of independence", "author": ["A. Gretton", "K. Fukumizu", "C.H. Teo", "L. Song", "B. Sch\u00f6lkopf", "A.J. Smola"], "venue": "NIPS, pages 585\u2013592,", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2007}, {"title": "Noise-contrastive estimation: A new estimation principle for unnormalized statistical models", "author": ["M. Gutmann", "A. Hyv\u00e4rinen"], "venue": "AISTATS, pages 297\u2013304,", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2010}, {"title": "Fundamentals of convex analysis", "author": ["J.B. Hiriart-Urruty", "C. Lemar\u00e9chal"], "venue": "Springer,", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2012}, {"title": "How (not) to train your generative model: scheduled sampling, likelihood, adversary", "author": ["F. Husz\u00e1r"], "venue": null, "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2015}, {"title": "Adam: A method for stochastic optimization", "author": ["D. Kingma", "J. Ba"], "venue": "arXiv:1412.6980,", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2014}, {"title": "Auto-encoding variational Bayes", "author": ["D.P. Kingma", "M. Welling"], "venue": "arXiv:1402.0030,", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2013}, {"title": "The neural autoregressive distribution estimator", "author": ["H. Larochelle", "I. Murray"], "venue": "AISTATS,", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2011}, {"title": "Generative moment matching networks", "author": ["Y. Li", "K. Swersky", "R. Zemel"], "venue": "ICML,", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2015}, {"title": "On divergences and informations in statistics and information theory", "author": ["F. Liese", "I. Vajda"], "venue": "Information Theory, IEEE, 52(10):4394\u20134412,", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2006}, {"title": "Bayesian neural networks and density networks", "author": ["D.J.C. MacKay"], "venue": "Nucl. Instrum. Meth. A, 354(1):73\u201380,", "citeRegEx": "22", "shortCiteRegEx": null, "year": 1995}, {"title": "Adversarial autoencoders", "author": ["A. Makhzani", "J. Shlens", "N. Jaitly", "I. Goodfellow"], "venue": "arXiv:1511.05644,", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2015}, {"title": "Divergence measures and message passing", "author": ["T. Minka"], "venue": "Technical report, Microsoft Research,", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2005}, {"title": "Estimating divergence functionals and the likelihood ratio by convex risk minimization", "author": ["X. Nguyen", "M.J. Wainwright", "M.I. Jordan"], "venue": "Information Theory, IEEE, 56(11):5847\u20135861,", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2010}, {"title": "On the chi-square and higher-order chi distances for approximating f-divergences", "author": ["F. Nielsen", "R. Nock"], "venue": "Signal Processing Letters, IEEE, 21(1):10\u201313,", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2014}, {"title": "Unsupervised representation learning with deep convolutional generative adversarial networks", "author": ["A. Radford", "L. Metz", "S. Chintala"], "venue": "arXiv:1511.06434,", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2015}, {"title": "Stochastic backpropagation and approximate inference in deep generative models", "author": ["D.J. Rezende", "S. Mohamed", "D. Wierstra"], "venue": "ICML, pages 1278\u20131286,", "citeRegEx": "28", "shortCiteRegEx": null, "year": 2014}, {"title": "Deep unsupervised learning using non-equilibrium thermodynamics", "author": ["J. Sohl-Dickstein", "E.A. Weiss", "N. Maheswaranathan", "S. Ganguli"], "venue": "ICML, pages 2256\u20132265,", "citeRegEx": "29", "shortCiteRegEx": null, "year": 2015}, {"title": "Hilbert space embeddings and metrics on probability measures", "author": ["B.K. Sriperumbudur", "A. Gretton", "K. Fukumizu", "B. Sch\u00f6lkopf", "G. Lanckriet"], "venue": "JMLR, 11:1517\u20131561,", "citeRegEx": "30", "shortCiteRegEx": null, "year": 2010}, {"title": "A note on the evaluation of generative models", "author": ["L. Theis", "A. v.d. Oord", "M. Bethge"], "venue": null, "citeRegEx": "31", "shortCiteRegEx": "31", "year": 2015}, {"title": "Chainer: a next-generation open source framework for deep learning", "author": ["S. Tokui", "K. Oono", "S. Hido", "J. Clayton"], "venue": "NIPS,", "citeRegEx": "32", "shortCiteRegEx": null, "year": 2015}, {"title": "RNADE: The real-valued neural autoregressive density-estimator", "author": ["B. Uria", "I. Murray", "H. Larochelle"], "venue": "NIPS, pages 2175\u20132183,", "citeRegEx": "33", "shortCiteRegEx": null, "year": 2013}, {"title": "LSUN: Construction of a large-scale image dataset using deep learning with humans in the loop", "author": ["F. Yu", "Y. Zhang", "S. Song", "A. Seff", "J. Xiao"], "venue": "arXiv:1506.03365,", "citeRegEx": "34", "shortCiteRegEx": null, "year": 2015}], "referenceMentions": [{"referenceID": 9, "context": "Generative-adversarial networks (GAN) in the form proposed by [10] are an expressive class of generative models that allow exact sampling and approximate estimation.", "startOffset": 62, "endOffset": 66}, {"referenceID": 20, "context": "Such probabilistic feedforward neural network models were first considered in [22] and [3], here we call these models generative neural samplers.", "startOffset": 78, "endOffset": 82}, {"referenceID": 2, "context": "Such probabilistic feedforward neural network models were first considered in [22] and [3], here we call these models generative neural samplers.", "startOffset": 87, "endOffset": 90}, {"referenceID": 16, "context": "GAN is also of this type, as is the decoder model of a variational autoencoder [18].", "startOffset": 79, "endOffset": 83}, {"referenceID": 23, "context": "[25] to recover the GAN training objective and generalize it to arbitrary f -divergences.", "startOffset": 0, "endOffset": 4}, {"referenceID": 9, "context": "[10] and provide a theoretical justification.", "startOffset": 0, "endOffset": 4}, {"referenceID": 23, "context": "[25] which is based on f -divergences.", "startOffset": 0, "endOffset": 4}, {"referenceID": 4, "context": "A large class of different divergences are the so called f -divergences [5, 21], also known as the Ali-Silvey distances [1].", "startOffset": 72, "endOffset": 79}, {"referenceID": 19, "context": "A large class of different divergences are the so called f -divergences [5, 21], also known as the Ali-Silvey distances [1].", "startOffset": 72, "endOffset": 79}, {"referenceID": 0, "context": "A large class of different divergences are the so called f -divergences [5, 21], also known as the Ali-Silvey distances [1].", "startOffset": 120, "endOffset": 123}, {"referenceID": 23, "context": "[25] derive a general variational method to estimate f -divergences given only samples from P and Q.", "startOffset": 0, "endOffset": 4}, {"referenceID": 13, "context": "Every convex, lower-semicontinuous function f has a convex conjugate function f\u2217, also known as Fenchel conjugate [14].", "startOffset": 114, "endOffset": 118}, {"referenceID": 24, "context": "Part of the list of divergences and their generators is based on [26].", "startOffset": 65, "endOffset": 69}, {"referenceID": 9, "context": "As shown by [10] GAN is related to the Jensen-Shannon divergence through DGAN = 2DJS \u2212 log(4).", "startOffset": 12, "endOffset": 16}, {"referenceID": 23, "context": "T , we find that under mild conditions on f [25], the bound is tight for T \u2217(x) = f \u2032 ( p(x) q(x) ) , (5)", "startOffset": 44, "endOffset": 48}, {"referenceID": 9, "context": "To this end, we follow the generative-adversarial approach [10] and use two neural networks, Q and T .", "startOffset": 59, "endOffset": 63}, {"referenceID": 22, "context": "To demonstrate the properties of the different f -divergences and to validate the variational divergence estimation framework we perform an experiment similar to the one of [24].", "startOffset": 173, "endOffset": 177}, {"referenceID": 9, "context": "[10], and second, a more direct single-step optimization procedure.", "startOffset": 0, "endOffset": 4}, {"referenceID": 9, "context": "[10] provide a local convergence guarantee.", "startOffset": 0, "endOffset": 4}, {"referenceID": 9, "context": "These conditions are similar to the assumptions made in [10] and can be formalized as follows: \u2207\u03b8F (\u03b8\u2217, \u03c9\u2217) = 0, \u2207\u03c9F (\u03b8\u2217, \u03c9\u2217) = 0, \u2207\u03b8F (\u03b8, \u03c9) \u03b4I, \u2207\u03c9F (\u03b8, \u03c9) \u2212\u03b4I.", "startOffset": 56, "endOffset": 60}, {"referenceID": 5, "context": "Note that although there could be many saddle points that arise from the structure of deep networks [6], they do not qualify as the solution of our variational framework under these assumptions.", "startOffset": 100, "endOffset": 103}, {"referenceID": 9, "context": "Here we discuss principled extensions of the heuristic proposed in [10] and real/fake statistics discussed by Larsen and S\u00f8nderby2.", "startOffset": 67, "endOffset": 71}, {"referenceID": 9, "context": "[10] noticed that training GAN can be significantly sped up by maximizing Ex\u223cQ\u03b8 [logD\u03c9(x)] instead of minimizing Ex\u223cQ\u03b8 [log (1\u2212D\u03c9(x))] for updating the generator.", "startOffset": 0, "endOffset": 4}, {"referenceID": 9, "context": "This is not only intuitively correct but we can show that the stationary point is preserved by this change using the same argument as in [10]; we found this useful also for other divergences.", "startOffset": 137, "endOffset": 141}, {"referenceID": 15, "context": "We found Adam [17] and gradient clipping to be useful especially in the large scale experiment on the LSUN dataset.", "startOffset": 14, "endOffset": 18}, {"referenceID": 9, "context": "We use the MNIST training data set (60,000 samples, 28-by-28 pixel images) to train the generator and variational function model proposed in [10] for various f -divergences.", "startOffset": 141, "endOffset": 145}, {"referenceID": 3, "context": "The variational function V\u03c9(x) has three linear layers with exponential linear unit [4] in between.", "startOffset": 84, "endOffset": 87}, {"referenceID": 25, "context": "As in [27] we use Adam with a learning rate of \u03b1 = 0.", "startOffset": 6, "endOffset": 10}, {"referenceID": 16, "context": "We also compare against variational autoencoders [18] with 20 latent dimensions.", "startOffset": 49, "endOffset": 53}, {"referenceID": 9, "context": "We evaluate the performance using the kernel density estimation (Parzen window) approach used in [10].", "startOffset": 97, "endOffset": 101}, {"referenceID": 29, "context": "The use of the KDE approach to log-likelihood estimation has known deficiencies [31].", "startOffset": 80, "endOffset": 84}, {"referenceID": 16, "context": "97 Variational Autoencoder [18] 445 5.", "startOffset": 27, "endOffset": 31}, {"referenceID": 25, "context": "Through the DCGAN work [27] the generative-adversarial approach has shown real promise in generating natural looking images.", "startOffset": 23, "endOffset": 27}, {"referenceID": 25, "context": "Here we use the same architecture as as in [27] and replace the GAN objective with our more general f -GAN objective.", "startOffset": 43, "endOffset": 47}, {"referenceID": 32, "context": "We use the large scale LSUN database [34] of natural images of different categories.", "startOffset": 37, "endOffset": 41}, {"referenceID": 25, "context": "We use the generator architecture and training settings proposed in DCGAN [27].", "startOffset": 74, "endOffset": 78}, {"referenceID": 25, "context": "The variational function is the same as the discriminator architecture in [27] and follows the structure of a convolutional neural network with batch normalization, exponential linear units [4] and one final linear layer.", "startOffset": 74, "endOffset": 78}, {"referenceID": 3, "context": "The variational function is the same as the discriminator architecture in [27] and follows the structure of a convolutional neural network with batch normalization, exponential linear units [4] and one final linear layer.", "startOffset": 190, "endOffset": 193}, {"referenceID": 1, "context": "Mixture density networks [2] are neural networks which directly regress the parameters of a finite parametric mixture model.", "startOffset": 25, "endOffset": 28}, {"referenceID": 10, "context": "When combined with a recurrent neural network this yields impressive generative models of handwritten text [11].", "startOffset": 107, "endOffset": 111}, {"referenceID": 17, "context": "NADE [19] and RNADE [33] perform a factorization of the output using a predefined and somewhat arbitrary ordering of output dimensions.", "startOffset": 5, "endOffset": 9}, {"referenceID": 31, "context": "NADE [19] and RNADE [33] perform a factorization of the output using a predefined and somewhat arbitrary ordering of output dimensions.", "startOffset": 20, "endOffset": 24}, {"referenceID": 27, "context": "Diffusion probabilistic models [29] define a target distribution as a result of a learned diffusion process which starts at a trivial known distribution.", "startOffset": 31, "endOffset": 35}, {"referenceID": 12, "context": "Noise contrastive estimation (NCE) [13] is a method that estimates the parameters of unnormalized probabilistic models by performing non-linear logistic regression to discriminate the data from artificially generated noise.", "startOffset": 35, "endOffset": 39}, {"referenceID": 20, "context": "The generative neural sampler models of [22] and [3] did not provide satisfactory learning methods; [22] used importance sampling and [3] expectation maximization.", "startOffset": 40, "endOffset": 44}, {"referenceID": 2, "context": "The generative neural sampler models of [22] and [3] did not provide satisfactory learning methods; [22] used importance sampling and [3] expectation maximization.", "startOffset": 49, "endOffset": 52}, {"referenceID": 20, "context": "The generative neural sampler models of [22] and [3] did not provide satisfactory learning methods; [22] used importance sampling and [3] expectation maximization.", "startOffset": 100, "endOffset": 104}, {"referenceID": 2, "context": "The generative neural sampler models of [22] and [3] did not provide satisfactory learning methods; [22] used importance sampling and [3] expectation maximization.", "startOffset": 134, "endOffset": 137}, {"referenceID": 16, "context": "Variational auto-encoders (VAE) [18, 28] are pairs of probabilistic encoder and decoder models which map a sample to a latent representation and back, trained using a variational Bayesian learning objective.", "startOffset": 32, "endOffset": 40}, {"referenceID": 26, "context": "Variational auto-encoders (VAE) [18, 28] are pairs of probabilistic encoder and decoder models which map a sample to a latent representation and back, trained using a variational Bayesian learning objective.", "startOffset": 32, "endOffset": 40}, {"referenceID": 21, "context": "The advantage of VAEs is in the encoder model which allows efficient inference from observation to latent representation and overall they are a compelling alternative to f -GANs and recent work has studied combinations of the two approaches [23] As an alternative to the GAN training objective the work [20] and independently [7] considered the use of the kernel maximum mean discrepancy (MMD) [12, 9] as a training objective for probabilistic models.", "startOffset": 241, "endOffset": 245}, {"referenceID": 18, "context": "The advantage of VAEs is in the encoder model which allows efficient inference from observation to latent representation and overall they are a compelling alternative to f -GANs and recent work has studied combinations of the two approaches [23] As an alternative to the GAN training objective the work [20] and independently [7] considered the use of the kernel maximum mean discrepancy (MMD) [12, 9] as a training objective for probabilistic models.", "startOffset": 303, "endOffset": 307}, {"referenceID": 6, "context": "The advantage of VAEs is in the encoder model which allows efficient inference from observation to latent representation and overall they are a compelling alternative to f -GANs and recent work has studied combinations of the two approaches [23] As an alternative to the GAN training objective the work [20] and independently [7] considered the use of the kernel maximum mean discrepancy (MMD) [12, 9] as a training objective for probabilistic models.", "startOffset": 326, "endOffset": 329}, {"referenceID": 11, "context": "The advantage of VAEs is in the encoder model which allows efficient inference from observation to latent representation and overall they are a compelling alternative to f -GANs and recent work has studied combinations of the two approaches [23] As an alternative to the GAN training objective the work [20] and independently [7] considered the use of the kernel maximum mean discrepancy (MMD) [12, 9] as a training objective for probabilistic models.", "startOffset": 394, "endOffset": 401}, {"referenceID": 8, "context": "The advantage of VAEs is in the encoder model which allows efficient inference from observation to latent representation and overall they are a compelling alternative to f -GANs and recent work has studied combinations of the two approaches [23] As an alternative to the GAN training objective the work [20] and independently [7] considered the use of the kernel maximum mean discrepancy (MMD) [12, 9] as a training objective for probabilistic models.", "startOffset": 394, "endOffset": 401}, {"referenceID": 28, "context": "MMD is a particular instance of a larger class of probability metrics [30] which all take the form D(P,Q) = supT\u2208T |Ex\u223cP [T (x)]\u2212 Ex\u223cQ[T (x)]|, where the function class T is chosen in a manner specific to the divergence.", "startOffset": 70, "endOffset": 74}, {"referenceID": 14, "context": "In [16] a generalisation of the GAN objective is proposed by using an alternative Jensen-Shannon divergence that mimics an interpolation between the KL and the reverse KL divergence and has Jensen-Shannon as its mid-point.", "startOffset": 3, "endOffset": 7}], "year": 2016, "abstractText": "Generative neural samplers are probabilistic models that implement sampling using feedforward neural networks: they take a random input vector and produce a sample from a probability distribution defined by the network weights. These models are expressive and allow efficient computation of samples and derivatives, but cannot be used for computing likelihoods or for marginalization. The generativeadversarial training method allows to train such models through the use of an auxiliary discriminative neural network. We show that the generative-adversarial approach is a special case of an existing more general variational divergence estimation approach. We show that any f -divergence can be used for training generative neural samplers. We discuss the benefits of various choices of divergence functions on training complexity and the quality of the obtained generative models.", "creator": "LaTeX with hyperref package"}}}