{"id": "1606.01261", "review": {"conference": "nips", "VERSION": "v1", "DATE_OF_SUBMISSION": "3-Jun-2016", "title": "Minimizing Regret on Reflexive Banach Spaces and Learning Nash Equilibria in Continuous Zero-Sum Games", "abstract": "We study a general version of the adversarial online learning problem. We are given a decision set $\\mathcal{X}$ in a reflexive Banach space $X$ and a sequence of reward vectors in the dual space of $X$. At each iteration, we choose an action from $\\mathcal{X}$, based on the observed sequence of previous rewards. Following an input-related selection, we select the next reward vector with the same sequence and choose the next reward vector with the same sequence. This decision is described in a new paper and we are not interested in the effects of a choice on the random number of reward vectors.\n\n\n\n\n\n\n\nWe are looking for the most optimal selection in the first place. We use the same algorithm that we did in the second paper to solve the adversarial online learning problem.\n\nThis paper shows that we use the same algorithm that we did in the first paper to solve the adversarial online learning problem.\nWe use the same algorithm that we did in the first paper to solve the adversarial online learning problem.\nWe use the same algorithm that we did in the first paper to solve the adversarial online learning problem.\nThis paper shows that we use the same algorithm that we did in the first paper to solve the adversarial online learning problem.\nThis paper shows that we use the same algorithm that we did in the first paper to solve the adversarial online learning problem.\nIn the first paper, we use the same algorithm that we did in the first paper to solve the adversarial online learning problem.\nThis paper shows that we use the same algorithm that we did in the first paper to solve the adversarial online learning problem.\nThis paper shows that we use the same algorithm that we did in the first paper to solve the adversarial online learning problem.\nThe following two papers show that we use the same algorithm that we did in the first paper to solve the adversarial online learning problem.\nWe use the same algorithm that we did in the first paper to solve the adversarial online learning problem.\nThe following two papers show that we use the same algorithm that we did in the first paper to solve the adversarial online learning problem.\n\nIn the first paper, we use the same algorithm that we did in the first paper to solve the adversarial online learning problem.\nThe following two papers show that we use the same algorithm that we did in the first paper to solve the adversarial online learning problem.\nIn the first", "histories": [["v1", "Fri, 3 Jun 2016 20:07:41 GMT  (3840kb,D)", "http://arxiv.org/abs/1606.01261v1", null]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["maximilian balandat", "walid krichene", "claire tomlin", "alexandre bayen"], "accepted": true, "id": "1606.01261"}, "pdf": {"name": "1606.01261.pdf", "metadata": {"source": "CRF", "title": "Minimizing Regret on Reflexive Banach Spaces and Learning Nash Equilibria in Continuous Zero-Sum Games", "authors": ["Maximilian Balandat", "Walid Krichene", "Claire Tomlin", "Alexandre Bayen"], "emails": ["BALANDAT@EECS.BERKELEY.EDU", "WALID@EECS.BERKELEY.EDU", "TOMLIN@EECS.BERKELEY.EDU", "BAYEN@BERKELEY.EDU"], "sections": [{"heading": "1. Introduction", "text": "Regret analysis is a general technique for designing and analyzing algorithms for sequential decision problems in adversarial or stochastic settings (Audibert and Bubeck, 2009; Bubeck and CesaBianchi, 2012). Online learning algorithms have many applications, including in machine learning (Xiao, 2010), portfolio optimization (Cover, 1991), and online convex optimization (Hazan et al., 2007). A particularly interesting role that regret plays manifests in the study of repeated play of finite games (Hart and Mas-Colell, 2001). It is well known, for example, that in a two-player zero-sum finite game, if both players play according to a Hannan-consistent strategy (Hannan, 1957), their (marginal) empirical distributions of play almost surely converge to the set of Nash equilibria of the game (Cesa-Bianchi and Lugosi, 2006). Moreover, it can be shown that playing a strategy that achieves sublinear regret almost surely guarantees Hannan-consistency.\nar X\niv :1\n60 6.\n01 26\n1v 1\n[ cs\n.L G\n] 3\nJ un\nA natural question to ask is whether a similar result holds for games in which the action sets are infinite. In this paper we show that this is in fact true. In particular, we prove that in a continuous two-player zero sum game over compact (not necessarily convex) metric spaces, if both players follow a Hannan-consistent strategy, then with probability 1, their empirical distributions of play weakly converge to the set of Nash equilibria of the game. This in turn raises another important question: Do algorithms that ensure Hannan-consistency exist for such games? More generally, can one develop algorithms that guarantee sub-linear growth of the worst-case regret? We answer these questions affirmatively as well in this article. To this end, we develop a general framework to study the Dual Averaging (or Follow the Regularized Leader) method on reflexive Banach spaces. This framework subsumes a wide range of existing results in the literature, including algorithms for online learning on finite sets (e.g. the Hedge Algorithm (Arora et al., 2012)), and finite-dimensional online convex optimization (e.g. Exponentially Weighted Online Optimization (Hazan et al., 2007)). Our results are related to (Lehrer, 2003) and (Sridharan and Tewari, 2010). Lehrer (2003) conducts an abstract analysis, providing necessary geometric conditions for Blackwell approachability in infinite-dimensional spaces, but no implementable algorithm that guarantees Hannan-consistency. Sridharan and Tewari (2010) provide general regret bounds for Mirror Descent (MD) under the assumption that the strategy set is uniformly bounded in the norm of the Banach space. We make no such assumption. In fact, for our applications in Section 3 this is typically not the case.\nGiven a convex subset X of a reflexive Banach space X , the generalized Dual Averaging (DA) method maximizes, at each iteration, the cumulative past rewards (which are elements of X\u2217, the dual space ofX) plus a regularization term h. We show that under certain conditions, the maximizer in the DA update is the Fre\u0301chet gradientDh\u2217 of the regularizer\u2019s conjugate function. In doing so, we develop a novel characterization of the duality between essential strong convexity of h and essential Fre\u0301chet differentiability of h\u2217 in reflexive Banach spaces, which may be of independent interest.\nWe apply these general results to the problem of minimizing regret when the rewards are uniformly continuous functions over a compact metric space S. Importantly, we do not assume convexity of either S or the reward functions. Under the assumption that S admits a locally Q-regular Borel measure \u00b5, we lift this non-convex finite dimensional problem on S, to the convex infinitedimensional problem on the set of probability distributions on S that are absolutely continuous with respect to \u00b5, and whose Radon-Nikdoym derivatives are in X = Lp(S, \u00b5) for some p > 1. We provide explicit bounds for a class of regularizers, which guarantee sublinear growth of the worst-case regret. We also prove a general lower bound on the regret for any online algorithm.\nThe paper is organized as follows: In Section 2 we introduce and provide a general analysis of Dual Averaging in reflexive Banach spaces. In Section 3 we apply these results to obtain explicit regret bounds on compact metric spaces with uniformly continuous reward functions. We use these results in Section 4 in the context of learning Nash equilibria in continuous two-player zero sum games, for which we provide numerical examples in Section 5. All proofs are given in Appendix E."}, {"heading": "2. Regret Minimization on Reflexive Banach Spaces", "text": "Consider a sequential decision problem in which we are to choose a sequence (x1, x2, . . . ) of actions from some feasible subset X of a reflexive Banach space X , and seek to maximize a sequence (u1(x1), u2(x2), . . . ) of rewards, where the u\u03c4 : X \u2192 R are elements of a given subset U \u2282 X\u2217, with X\u2217 the dual space of X . We assume that xt, the action chosen at time t, may only depend on the sequence of previously observed reward vectors (u1, . . . , ut\u22121). We call any such algorithm\nan online algorithm. We consider the adversarial setting, i.e., we do not make any distributional assumptions on the rewards. In particular, they could be picked maliciously by some adversary.\nThe notion of regret is a standard measure of performance for such a sequential decision problem. For a sequence (u1, . . . , ut) of reward vectors, and a sequence of decisions (x1, . . . , xt) produced by an algorithm, the regret of the algorithm with respect to a (fixed) decision x \u2208 X is the gap between the realized reward and the reward under x. In other words,\nRt(x) := t\u2211\n\u03c4=1\nu\u03c4 (x)\u2212 t\u2211\n\u03c4=1\nu\u03c4 (x\u03c4 ) (1)\nThe worst-case regret is defined as\nRt := sup x\u2208X Rt(x) (2)\nAn algorithm is said to have sublinear regret if for any sequence (ut)t\u22651 in the set of admissible reward functions U , the worst-case regret grows sublinearly, i.e. lim suptRt/t \u2264 0.\nExample 1 (Finite Action Sets) Consider a finite action set S = {1, . . . , n}, let X = X\u2217 = Rn, and let X = \u2206n\u22121, the probability simplex in Rn. In this case, a reward function on S is simply a vector u \u2208 Rn, such that the i-th element ui is the reward of action i. A choice x \u2208 X corresponds to a randomization over the n discrete actions in S. This is the classic setting of many regretminimizing algorithms in the literature.\nExample 2 (Online Optimization on Compact Metric Spaces) Let S be a compact metric space, and let \u00b5 be a finite measure on S. Consider the Hilbert Space X = X\u2217 = L2(S, \u00b5) and let X = {x \u2208 X : x \u2265 0 a.e., \u2016x\u20161 = 1}. The set S is again the set of feasible actions. A reward function is an L2-integrable function on S, and each choice x \u2208 X corresponds to a probability distribution (absolutely continuous w.r.t. to \u00b5) over the set of actions. We will further explore a more general variant of this problem in Section 3.\nIn this Section, we prove a general bound on the worst-case regret for the Dual Averaging method. Dual Averaging was introduced by Nesterov (2009) for (finite dimensional) convex optimization, and was since applied to the online learning setting, for example by Xiao (2010). In the finite dimensional case, the method works by solving, at each iteration, the maximization problem\nxt+1 = arg max x\u2208X\n\u2329 \u03b7t \u2211t \u03c4=1 u\u03c4 , x \u232a \u2212 h(x)\nwhere h is a strongly convex regularizer defined on X \u2282 Rn and (\u03b7t)t\u22650 is a sequence of learning rates. The regret analysis of the method relies on the duality between strong convexity and smoothness for a convex function and its conjugate (Nesterov, 2009, Lemma 1), see also (Rockafellar, 1997, Theorem 26.3). To generalize the Dual Averaging method to our Banach space setting, we first need an analogous duality result. We develop such a result in Theorem 4. In particular, we show that the correct notion of strong convexity in our setting is (uniform) essential strong convexity. Equipped with this duality result, we can analyze the regret of the Dual Averaging method and derive a general bound in Theorem 6."}, {"heading": "2.1. Preliminaries", "text": "Let (X, \u2016 \u00b7 \u2016) be a reflexive Banach space, and denote by \u3008 \u00b7 , \u00b7 \u3009 : X \u00d7 X\u2217 \u2192 R the canonical pairing between X and its dual space X\u2217, so that \u3008x, \u03be\u3009 := \u03be(x) for all x \u2208 X, \u03be \u2208 X\u2217. By the effective domain of an extended real-valued function f : X \u2192 [\u2212\u221e,+\u221e] we mean the set dom f = {x \u2208 X : f(x) < +\u221e}. A function f is proper if f > \u2212\u221e and dom f is non-empty. The conjugate or Legendre-Fenchel transform of f is the function f\u2217 : X\u2217 \u2192 [\u2212\u221e,+\u221e] given by\nf\u2217(\u03be) = sup x\u2208X \u3008x, \u03be\u3009 \u2212 f(x) (3)\nfor all \u03be \u2208 X\u2217. If f is proper, lower semicontinuous and convex, its subdifferential \u2202f is the set-valued mapping \u2202f(x) = { \u03be \u2208 X\u2217 : f(y) \u2265 f(x) + \u3008y \u2212 x, \u03be\u3009 for all y \u2208 X } . We define dom \u2202f := {x \u2208 X : \u2202f(x) 6= \u2205}. Let \u0393 denote the set of all convex, lower semicontinuous functions \u03b3 : [0,\u221e)\u2192 [0,\u221e] such that \u03b3(0) = 0, and let\n\u0393U := { \u03b3 \u2208 \u0393 : \u03b3(r) > 0, for r > 0 } (4a)\n\u0393L := { \u03b3 \u2208 \u0393 : \u03b3(r)/r \u2192 0, as r \u2192 0 } (4b)\nWe now introduce the appropriate definitions of strong convexity, differentiability and smoothness for our setting. Some related results are reviewed in Appendix A.\nDefinition 1 (Essential strong convexity (Stro\u0308mberg, 2011)) A proper convex lower semicontinuous function f : X \u2192 (\u2212\u221e,\u221e] is essentially strongly convex if\n(i) f is strictly convex on every convex subset of dom \u2202f\n(ii) (\u2202f)\u22121 is locally bounded on its domain\n(iii) for every x0 \u2208 dom \u2202f there exists \u03be0 \u2208 X\u2217 and \u03b3 \u2208 \u0393U such that\nf(x) \u2265 f(x0) + \u3008x\u2212 x0, \u03be0\u3009+ \u03b3(\u2016x\u2212 x0\u2016), \u2200x \u2208 X (5)\nIf (5) holds with \u03b3 independent of x0, then f is said to be uniformly essentially strongly convex with modulus \u03b3.\nDefinition 2 (Essential Fre\u0301chet differentiability (Stro\u0308mberg, 2011)) A proper convex lower semicontinuous function f : X \u2192 (\u2212\u221e,\u221e] is essentially Fre\u0301chet differentiable if int dom f 6= \u2205, f is Fre\u0301chet differentiable on int dom f with Fre\u0301chet derivative D, and \u2016Df(xj)\u2016\u2217 \u2192 \u221e for any sequence (xj)j in int dom f converging to some boundary point of dom f .\nDefinition 3 (Essential strong smoothness) A proper Fre\u0301chet differentiable function f : X \u2192 (\u2212\u221e,\u221e] is essentially strongly smooth if \u2200x0 \u2208 dom \u2202f, \u2203 \u03be0 \u2208 X\u2217, \u03ba \u2208 \u0393L such that\nf(x) \u2264 f(x0) + \u3008\u03be0, x\u2212 x0\u3009+ \u03ba(\u2016x\u2212 x0\u2016), \u2200x \u2208 X (6)\nIf (6) holds with \u03ba independent of x0, then f is said to be uniformly essentially strongly smooth with modulus \u03ba.\nWe are now ready to give our main duality result:\nTheorem 4 Let f : X \u2192 (\u2212\u221e,+\u221e] be proper, lower semicontinuous and uniformly essentially strongly convex with modulus \u03b3 \u2208 \u0393U . Then\n(i) f\u2217 is proper and essentially Fre\u0301chet differentiable with Fre\u0301chet derivative\nDf\u2217(\u03be) = arg max x\u2208X \u3008x, \u03be\u3009 \u2212 f(x) (7)\nIf, in addition, \u03b3\u0303(r) := \u03b3(r)/r is strictly increasing, then \u2016Df\u2217(\u03be1)\u2212Df\u2217(\u03be2)\u2016 \u2264 \u03b3\u0303\u22121 ( \u2016\u03be1 \u2212 \u03be2\u2016\u2217/2 ) (8)\nIn other words, Df\u2217 is uniformly continuous with modulus of continuity \u03c7(r) = \u03b3\u0303\u22121(r/2).\n(ii) f\u2217 is uniformly essentially smooth with modulus \u03b3\u2217.\nCorollary 5 If \u03b3(r) \u2265 C r1+\u03ba, \u2200 r \u2265 0 then \u2016Df\u2217(\u03be1) \u2212 Df\u2217(\u03be2)\u2016 \u2264 (2C)\u22121/\u03ba\u2016\u03be1 \u2212 \u03be2\u20161/\u03ba\u2217 . In particular, with \u03b3(r) = K2 r\n2, Definition 1 becomes the classic definition of K-strong convexity, and (8) yields the result familiar from the finite-dimensional case that the gradient Df\u2217 is 1/K Lipschitz with respect to the dual norm (Nesterov, 2009, Lemma 1)."}, {"heading": "2.2. Dual Averaging in Reflexive Banach Spaces", "text": "We call a proper convex function h : X \u2192 (\u2212\u221e,+\u221e] a regularizer function on a set X \u2282 X if h is essentially strongly convex and domh = X . We emphasize that we do not assume h to be Fre\u0301chet-differentiable. Definition 1 in conjunction with Lemma 29 implies that for any regularizer function h, the supremum of any function of the form \u3008 \u00b7 , \u03be\u3009 \u2212 h( \u00b7 ) over X , where \u03be \u2208 X\u2217, will be attained at a unique element of X , namely Dh\u2217(\u03be), the Fre\u0301chet gradient of h\u2217 at \u03be.\nThe Dual Averaging method with regularizer h and a sequence of learning rates (\u03b7t)t\u22651 generates a sequence of decisions using the simple update rule:\nxt+1 = Dh \u2217(\u03b7tUt) where Ut = \u2211t \u03c4=1 u\u03c4 and U0 := 0. The following theorem provides a general regret bound.\nTheorem 6 (Dual Averaging Regret) Let h be a uniformly essentially strongly convex regularizer on X with modulus \u03b3. Let (\u03b7t)t\u22651 be a positive non-increasing sequence of learning rates. Then, for any sequence of payoff functions (ut)t\u22651 in X\u2217, the sequence of plays (xt)t\u22650 given by\nxt+1 = Dh \u2217(\u03b7t\u2211t\u03c4=1 u\u03c4) (9)\nensures that\nRt(x) := t\u2211\n\u03c4=1\n\u3008u\u03c4 , x\u3009 \u2212 t\u2211\n\u03c4=1\n\u3008u\u03c4 , x\u03c4 \u3009 \u2264 h(x)\u2212 h\n\u03b7t + t\u2211 \u03c4=1 \u2016u\u03c4\u2016\u2217 \u03b3\u0303\u22121 (\u03b7\u03c4\u22121 2 \u2016u\u03c4\u2016\u2217 ) (10)\nwhere h = infx\u2208X h(x), \u03b3\u0303(r) := \u03b3(r)/r and \u03b70 := \u03b71.\nNote that it is possible to obtain a regret bound similar to (10) also in a continuous-time setting. In fact, following Kwon and Mertikopoulos (2014), we derive the bound (10) by first proving a bound on a suitably defined notion of continuous-time regret, and then bounding the difference between the continuous-time and discrete-time regrets. This analysis is detailed in Appendix B.\nTheorem 6 provides a bound on the regret Rt(x) with respect to a particular choice x \u2208 X . Recall that the worst-case regret is defined asRt := supx\u2208X Rt(x). In the finite-dimensional seting of Example 1 the set X is compact, so any continuous regularizer h will be bounded, and hence taking the supremum over x in (10) poses no issue. However, this is not the case in our general setting, as the regularizer may be unbounded on X . For instance, consider Example 2 with the entropy regularizer h(x) = \u222b S x(s) log(x(s))ds, which is easily seen to be unbounded on X . As a consequence, obtaining a worst-case bound will in general require additional assumptions on the reward functions and the decision set X . This will be investigated in detail in Section 3.\nCorollary 7 Suppose that \u03b3(r) \u2265 C r1+\u03ba, \u2200 r \u2265 0 for some C > 0 and \u03ba > 0. Then\nRt(x) \u2264 h(x)\u2212 h\n\u03b7t + (2C)\u22121/\u03ba t\u2211 \u03c4=1 \u03b7 1/\u03ba \u03c4\u22121\u2016u\u03c4\u2016 1+1/\u03ba \u2217 (11)\nIn particular, if \u2016ut\u2016\u2217 \u2264M for all t and \u03b7t = \u03b7 t\u2212\u03b2 , then\nRt(x) \u2264 h(x)\u2212 h\n\u03b7 t\u03b2 +\n\u03ba\n\u03ba\u2212 \u03b2 ( \u03b7 2C )1/\u03ba M1+1/\u03ba t1\u2212\u03b2/\u03ba (12)\nAssuming h is bounded, optimizing over \u03b2 yields a rate of Rt(x) = O(t \u03ba\n1+\u03ba ). In particular, if \u03b3(r) = K2 r 2, which corresponds to the classic definition of strong convexity, then Rt(x) = O( \u221a t). For non-vanishing u\u03c4 we will need that \u03b7t \u2198 0 for the sum in (11) to converge. Thus we could get potentially tighter control over the rate of this term for \u03ba < 1, at the expense of larger constants."}, {"heading": "3. Online Optimization on Compact Metric Spaces with Uniformly Continuous Rewards", "text": "Motivated by Example 2, in this section we apply our results to the problem of regret minimization on compact metric spaces under the additional assumption of uniformly continuous reward functions. Importantly, we make no assumptions on convexity of either the feasible set or the reward functions. Essentially, this can be seen as lifting the non-convex problem of minimizing a sequence of functions over the (possibly non-convex) set S to the convex (albeit infinite-dimensional) problem of minimizing a sequence of linear functionals over the convex subset X of probability measures over the vector space of measures on S. This correspondance is illustrated in Figure 1."}, {"heading": "3.1. An Upper Bound on the Worst-Case Regret", "text": "Let (S, d) be a compact metric space, and let \u00b5 be a Borel measure on S. Suppose that the reward vectors u\u03c4 are given by elements in Lq(S, \u00b5), where q > 1. Let X = Lp(S, \u00b5), where p and q are Ho\u0308lder conjugates, i.e., 1p + 1 q = 1. Consider X = {x \u2208 X : x \u2265 0 a.e., \u2016x\u20161 = 1}, the set of probability measures on S that are absolutely continuous w.r.t. to \u00b5 with p-integrable RadonNikodym derivatives. Moreover, denote by Z the class of non-decreasing \u03c7 : [0,\u221e)\u2192 [0,\u221e] such that limr\u21920 \u03c7(r) = \u03c7(0) = 0. The following assumption will be made throughout this section:\nAssumption 1 The reward vectors ut have modulus of continuity \u03c7 on S, uniformly in t. That is, there exists \u03c7 \u2208 Z such that |ut(s)\u2212 ut(s\u2032)| \u2264 \u03c7(d(s, s\u2032)) for all t and for all s, s\u2032 \u2208 S.\nDenote by B(s, r) = {s\u2032 \u2208 S : d(s, s\u2032) < r} the open ball of radius r centered at s and by B(s, \u03b4) \u2282 X the set of elements of X with support contained in B(s, \u03b4). Furthermore, let DS := sups,s\u2032\u2208S d(s, s \u2032) the diameter of S. Then we have the following:\nTheorem 8 (Dual Averaging Regret on Metric Spaces with Uniformly Continuous Rewards) Let (S, d) be compact, and suppose that Assumption 1 holds. Let h be a uniformly essentially strongly convex regularizer on X with modulus \u03b3, and let (\u03b7t)t\u22651 be a positive non-increasing sequence of learning rates. Then, under (9), for any positive sequence (\u03d1t)t\u22651,\nRt \u2264 sups\u2208S infx\u2208B(s,\u03d1t) h(x)\u2212 h\n\u03b7t + t \u03c7(\u03d1t) + t\u2211 \u03c4=1 \u2016u\u03c4\u2016\u2217 \u03b3\u0303\u22121 (\u03b7\u03c4\u22121 2 \u2016u\u03c4\u2016\u2217 ) (13)\nRemark 9 The sequence (\u03d1t)t\u22651 that appears in Theorem 8 is not a parameter of the Dual Averaging algorithm, but rather a parameter in the regret bound. In particular, (13) holds true for any such positive sequence, and we will use this fact later on to obtain explicit bounds by instantiating (13) with a particular choice of (\u03d1t)t\u22651.\nIt is important to realize that the infimum over B(s, \u03d1t) in (13) may be infinite, in which case the bound is meaningless. This happens for example if s is an isolated point of a compact subset S \u2282 Rn and \u00b5 is the Lebesgue measure, in which case B(s, \u03d1t) = \u2205. However, under an additional regularity assumption on the measure \u00b5 we can avoid such degenerate situations.\nDefinition 10 (Q-regularity (Heinonen. et al., 2015)) A Borel measure \u00b5 on a metric space (S, d) is (Ahlfors) Q-regular if there exist 0 < c0 \u2264 C0 <\u221e such that for any open ball B(s, r)\nc0r Q \u2264 \u00b5(B(s, r)) \u2264 C0rQ (14)\nWe say that \u00b5 is r0-locally Q-regular if (14) holds for all 0 < r \u2264 r0.\nIntuitively, under an r0-locally Q-regular measure, the mass in the neighborhood of any point of S is uniformly bounded from above and below. This will allow, at each iteration t, to assign sufficient probability mass around the maximizer(s) of the cumulative reward function.\nExample 3 (Regularity of the Lebesgue measure \u03bb) The canonical example for aQ-regular measure is the Lebesgue measure \u03bb on Rn. If d is the metric induced by the standard Euclidean norm,\nthenQ = n and the bound (14) is tight with c0 = C0, a dimensional constant. However, for general sets S \u2282 Rn, \u03bb need not be locally Q-regular. This is for example the case if S includes isolated points. One sufficient condition for local regularity of the Lebesgue measure is that S is v-uniformly fat, as defined by Krichene et al. (2015), i.e., that for all s \u2208 S, there exists a convex set K \u2286 S that contains s with \u03bb(K) \u2265 v. In this case one can to prove that \u03bb|S is r0-locally n-regular, with r0, c0 and C0 depending on the geometry of S. Importantly, S need not be convex or even connected.\nAssumption 2 The measure \u00b5 is r0-locally Q-regular on (S, d).\nRecall that he worst-case regretRt is defined as the supremum of Rt(x) over all elements of X . In the setting of this section, we can in fact say more:\nProposition 11 Suppose Assumption 2 holds. Then\nRt = sup x\u2208X Rt(x) = sup x\u2208P Rt(x) = sup s\u2208S\nUt(s)\u2212 \u2211t \u03c4=1\u3008u\u03c4 , x\u03c4 \u3009 (15)\nUnder Assumption 2, B(s, \u03d1t) 6= \u2205 for all s \u2208 S and \u03d1t > 0, and hence there is hope for a bound on infx\u2208B(s,\u03d1t) h(x) that is uniform in s. To get explicit rates of convergence, we have to consider a more specific class of regularizers.\n3.2. Explicit Rates for f -Divergences on Lp(S)\nIn this section we consider a particular class of regularizers called f -divergences or Csisza\u0301r divergences (Csisza\u0301r, 1967) and provide explicit bounds on the worst-case regret. Following Audibert et al. (2014), we define \u03c9-potentials and the associated f -divergence.\nDefinition 12 (\u03c9-Potential) Let \u03c9 \u2264 0 and a \u2208 (\u2212\u221e,+\u221e]. A continuous increasing diffeomorphism \u03c6 : (\u2212\u221e, a) \u2192 (\u03c9,\u221e), is an \u03c9-potential if limz\u2192\u2212\u221e \u03c6(z) = \u03c9, limz\u2192a \u03c6(z) = +\u221e and \u03c6(0) \u2264 1. Associated to \u03c6 is the convex function f\u03c6 : [0,\u221e)\u2192 R defined by\nf\u03c6(x) = \u222b x 1 \u03c6\u22121(z) dz,\nand the f\u03c6-divergence, defined by\nh\u03c6(x) = \u222b S f\u03c6 ( x(s) ) d\u00b5(s) + \u03b9X (x).\nwhere \u03b9X is the indicator function of X (i.e. \u03b9X (x) = 0 if x \u2208 X and \u03b9X (x) = +\u221e if x /\u2208 X ).\nA remarkable fact is that for regularizers based on \u03c9 potentials, the Dual Averaging update (9) can be computed efficiently. More precisely, it can be shown (Krichene and Balandat, 2016) that the maximizer in this case has a simple expression in terms of the dual problem, and the problem of computing xt+1 = Dh\u2217(\u03b7t \u2211t \u03c4=1 u\u03c4 ) reduces to computing a scalar dual variable \u03bd \u2217 t . In Proposition 34 in Appendix C we provide a bound on \u03bd\u2217t+1 that depends on the value of \u03bd \u2217 t and other parameters of the problem. In practice, these bounds greatly speed up computation. Note that the measure \u00b5 plays the role of a design variable, and its choice will affect our bounds through the constantsQ,C0 and c0. The problem of finding a \u201cgood\u201d measure \u00b5 is a very interesting problem for future studies. For now we will assume that \u00b5(S) = 1, which due to compactness of S is without loss of generality if we want arbitrarily small balls to have finite measure.\nProposition 13 Suppose that \u00b5(S) = 1, and that Assumption 2 holds with constants r0 > 0 and 0 < c0 \u2264 C0 < \u221e. Under the Assumptions of Theorem 8, with h = h\u03c6 the regularizer associated to an \u03c9-potential \u03c6, we have that, for any positive sequence (\u03d1t)t\u22651 with \u03d1t \u2264 r0,\nRt t \u2264 min(C0\u03d1 Q t , \u00b5(S)) t \u03b7t f\u03c6 ( c\u221210 \u03d1 \u2212Q t ) + \u03c7(\u03d1t) + 1 t t\u2211 \u03c4=1 \u2016u\u03c4\u2016\u2217 \u03b3\u0303\u22121 (\u03b7\u03c4\u22121 2 \u2016u\u03c4\u2016\u2217 ) (16)\nFor particular choices of the learning rates (\u03b7t)t\u22651 and the sequence (\u03d1t)t\u22651, we can derive explicit regret rates. To intuit, suppose for simplicity that the reward functions are uniformly bounded in the dual norm. Then it is clear that in order for the last term in (16) to vanish asymptotically, \u03b7t must be vanishing. Similarly, for the second term to vanish, \u03d1t must be vanishing as well. Thus, if both (\u03b7t)t\u22651 and (\u03d1t)t\u22651 are decreasing sequences, their respective rates of decay must be carefully chosen so that the first term also vanishes. These tradeoffs will become clear in the statement of Corollary 14 and in the numerical examples presented in Appendix D."}, {"heading": "3.3. Analysis for Entropy Dual Averaging (The Generalized Hedge Algorithm)", "text": "Taking \u03c6(z) = ez\u22121, we have that f\u03c6(x) = \u222b x 1 \u03c6 \u22121(z)dz = x log x, and hence the regularizer is\nh\u03c6(x) = \u222b S x(s) log x(s)d\u00b5(s). Then Dh\n\u2217(\u03be)(s) = exp \u03be(s)\u2016 exp \u03be(s)\u20161 . This corresponds to a generalized version of the Hedge algorithm (Arora et al., 2012; Krichene et al., 2015). The regularizer h\u03c6 can be shown to be essentially strongly convex with modulus \u03b3(r) = 12r 2.\nCorollary 14 (Regret Bound for Entropy Dual Averaging) Suppose that \u00b5(S) = 1, that \u00b5 is r0-locally Q-regular with constants c0, C0, that \u2016ut\u2016\u2217 \u2264 M for all t, and that \u03c7(r) = C\u03b1r\u03b1 for 0 < \u03b1 \u2264 1 (that is, the rewards are \u03b1-Ho\u0308lder continuous). Then, under Entropy Dual Averaging, choosing \u03b7t = \u03b7 \u221a log t/t with \u03b7 = 1M (C0Q 2c0 log(c\u221210 \u03d1 \u2212Q/\u03b1) + Q2\u03b1\n)1/2 and \u03d1 > 0, we have that Rt t \u2264 ( 2M \u221a 2C0 c0 ( log(c\u221210 \u03d1 \u2212Q/\u03b1) + Q 2\u03b1 ) + C\u03b1\u03d1 )\u221a log t t (17)\nwhenever \u221a\nlog t/t < r\u03b10 \u03d1 \u22121.\nOne can further optimize over the choice of \u03d1 to obtain the best constant in the bound. Note also that the case \u03b1 = 1 corresponds to Lipschitz continuity."}, {"heading": "3.4. A General Lower Bound", "text": "We also prove the following general lower bound for any online algorithm:\nTheorem 15 (General Lower Bound) Let (S, d) be compact, suppose that Assumption 2 holds, and let \u03c7 \u2208 Z . Then for any online algorithm, there exist a sequence (u\u03c4 )t\u03c4=1 of reward vectors u\u03c4 \u2208 X\u2217 with \u2016u\u03c4\u2016\u2217 \u2264M and modulus of continuity \u03c7\u03c4 < \u03c7 such that\nRt \u2265 w(DS)\n2 \u221a 2\n\u221a t (18)\nwhere w : R \u2192 R is any function with modulus of continuity \u03c7 such that \u2016w(d( \u00b7 , s\u2032))\u2016q \u2264 M for some s\u2032 \u2208 S for which there exists s \u2208 S with d(s, s\u2032) = DS .\nMaximizing the constant in (18) is of interest in order to benchmark the bound against the upper bounds obtained in the previous sections. This problem is however quite challenging, and we will defer this analysis to future work. For Ho\u0308lder-continuous functions, we have the following result:\nProposition 16 (General Lower Bound for Ho\u0308lder-Continuous Functions) In the setting of Theorem 15, suppose that \u00b5(S) = 1 and that \u03c7(r) = C\u03b1r\u03b1 for some 0 < \u03b1 \u2264 1. Then\nRt \u2265 min\n( C\n1/\u03b1 \u03b1 D\u03b1S , M ) 2 \u221a 2 \u221a t (19)\nObserve that, up to a \u221a\nlog t factor, the asymptotic rate of this general lower bound for any online algorithm matches that of the upper bound (17) of Entropy Dual Averaging."}, {"heading": "3.5. Consistency of Dual Averaging", "text": "It is quite intuitive to see that Dual Averaging would recover the greedy algorithm as the regularizer h \u201capproaches a constant\u201d. In the following, we make this intuition precise.\nDefinition 17 (Consistency of a Sequence of Regularizers) A sequence (h1, h2, . . . ) of regularizers on X is consistent if there exists C \u2208 R such that hi(x)\u2192 C as i\u2192\u221e for all x \u2208 X .\nFor s \u2208 S, A \u2282 S, let d(s,A) = infs\u2032\u2208A d(s, s\u2032). For \u03b4 > 0, let B\u2217\u03b4 := {s \u2208 S : d(s, S\u2217) < \u03b4}. Moreover, let \u03bd|A denote the restriction of \u03bd \u2208 P(S) to A.\nProposition 18 Suppose Assumption 2 holds and that (hi)i\u22651 is a sequence of regularizers that is consistent. Fix t and let U\u2217 := maxs\u2208S Ut(s) and S\u2217 := {s \u2208 S : Ut(s) = U\u2217}. For i \u2265 1 let x\u2217t,i := Dh \u2217 i (Ut) Then, for any \u03b4 > 0, we have that x \u2217 t,i|(B\u2217\u03b4 )c \u2192 0|(B\u2217\u03b4 )c (strongly) as i \u2192 \u221e.\nEquivalently, \u222b S\u2217 x \u2217 i (s) ds\u2192 1 as i\u2192\u221e.\nProposition 18 shows that if the sequence of regularizers is consistent, the optimizers, in the limit, collapse to distributions supported on the set of maximizers of Ut (as illustrated numerically in Example 5 in Appendix D). If the maximizer of Ut is unique, we can say the following:\nCorollary 19 In the setting of Proposition 18, suppose that Ut admits a unique maximizer s\u2217t \u2208 S. Then x\u2217i weakly converges to the Dirac measure on s \u2217 t as i\u2192\u221e. We write x\u2217t,i \u21c0 \u03b4s\u2217t ."}, {"heading": "4. Learning in Continuous Two-Player Zero-Sum Games", "text": "In this section we apply our esults on Dual Averaging on Lp-spaces in the context of repeated play of continuous games. In particular, we focus on continuous two-player zero-sum games. In the finite case similar results exist for non-zero sum games, and we believe that they can be extended to our setting, however this is outside the scope of this article (see for example (Stoltz and Lugosi, 2007) for related work on learning correlated equilibria under additional convexity assumptions)."}, {"heading": "4.1. Static Two-Player Zero-Sum Games", "text": "Consider a two-player zero sum game G = (S1, S2, u), in which the strategy spaces S1 and S2 of player 1 and 2, respectively, are Hausdorff spaces, and u : S1 \u00d7 S2 \u2192 R is the payoff function of player 1. As the game is zero-sum, the payoff function of player 2 is \u2212u. For each i, denote by Pi := P(Si) the set of Borel probability measures on Si. Denote S := S1\u00d7S2 and P := P1\u00d7P2. For a (joint) mixed strategy x \u2208 P , we define the natural extension u\u0304 : P \u2192 R by u\u0304(x) := Ex[u] =\u222b S u(s\n1, s2) dx(s1, s2), which is the expected payoff of player 1 under x. A continuous zero-sum game G is said to have value V if\nsup x1\u2208P1 inf x2\u2208P2 u\u0304(x1, x2) = inf x2\u2208P2 sup x1\u2208P1 u\u0304(x1, x2) = V (20)\nThe elements x1\u00d7x2 \u2208 P at which (20) holds are the (mixed) Nash Equilibria of G. We denote the set of Nash equilibria of G by N (G). In the case of finite games, it is well known that every twoplayer zero-sum game has a value. This is not true in general for continuous games, and additional conditions on strategy sets and payoffs are required. A classic result is the following:\nTheorem 20 (Glicksberg, 1950) Let S1 and S2 be compact, and suppose that u : S1\u00d7S2 \u2192 R is semi-continuous (upper or lower). Then G has a value."}, {"heading": "4.2. Repeated Play", "text": "We consider repeated play of the continuous two-player zero-sum game. Given a game G and a sequence of plays (s1t )t\u22651 and (s 2 t )t\u22651, we say that player i has sublinear (realized) regret if\nlim sup t\u2192\u221e\n1\nt ( sup si\u2208Si t\u2211 \u03c4=1 ui(s i, s\u2212i\u03c4 )\u2212 t\u2211 \u03c4=1 ui(s i \u03c4 , s \u2212i \u03c4 ) ) = 0 (21)\nwhere we use \u2212i to denote the other player (e.g. \u22121 = 2). A strategy \u03c3i for player i is, loosely speaking, a (possibly random) mapping from past observations to its actions. Of primary interest to us are Hannan-consistent strategies (Hannan, 1957):\nDefinition 21 (Hannan Consistency) A strategy \u03c3i of player i is Hannan consistent if, for any sequence (st\u2212i)t\u22651, the sequence of plays (s t i)t\u22651 generated by \u03c3 i has sublinear regret almost surely.\nNote that the almost sure statement in Definition 21 is with respect to the randomness in the strategy \u03c3i. The following results are generalizations of their counterparts for discrete games:\nProposition 22 Suppose G has value V and consider a sequence of plays (s1t )t\u22651, (s2t )t\u22651 and suppose that player 1 has sublinear realized regret. Then\nlim inf t\u2192\u221e\n1\nt t\u2211 \u03c4=1 u(s1\u03c4 , s 2 \u03c4 ) \u2265 V (22)\nCorollary 23 Suppose G has value V and consider a sequence of plays (s1t )t\u22651, (s2t )t\u22651 and assume that both players have sublinear realized regret. Then\nlim t\u2192\u221e\n1\nt t\u2211 \u03c4=1 u(s1\u03c4 , s 2 \u03c4 ) = V (23)\nAs in the discrete case (Cesa-Bianchi and Lugosi, 2006), we can also say something about convergence of the empirical distributions of play to the set of Nash Equilibria. Since these distributions have finite support for every t, we can at best hope for convergence in the weak sense as follows:\nTheorem 24 (Weak Convergence of the Empirical Distributions of Play) Suppose that in a repeated two-player zero sum game G that has a value both players follow a Hannan-consistent strategy, and denote by x\u0302it = 1 t \u2211t \u03c4=1 \u03b4si\u03c4 the marginal empirical distribution of play of player i at iteration t. Let x\u0302t := (x\u03021t , x\u0302 2 t ). Then x\u0302t \u21c0 N (G) almost surely, that is, with probability 1 the sequence (x\u0302t)t\u22651 weakly converges to the set of Nash equilibria of G.\nCorollary 25 If G has a unique Nash equilibrium x\u2217, then with probability 1, x\u0302t \u21c0 x\u2217."}, {"heading": "4.3. Hannan-Consistent Strategies", "text": "By Theorem 24, if each player follows a Hannan-consistent strategy, then the empirical distributions of play weakly converge to the set of Nash equilibria of the game. But do such strategies exist? Regret minimizing strategies are intuitive candidates, and the intimate connection between regret minimization and learning in games is well studied for special cases such as for finite games (CesaBianchi and Lugosi, 2006) or potential games (Monderer and Shapley, 1996). Using our results from Section 3, we will show that, under an additional assumption on the underlying information structure, no-regret learning based on Dual Averaging leads to Hannan consistency in our setting.\nSuppose that after each iteration t, each player i observes a partial payoff function u\u0303it : Si \u2192 R describing their payoff as a function of only their own action, si, holding the action played by the other player fixed. That is,\nu\u03031t (s 1) := u(s1, s2t ) u\u0303 2 t (s 2) := \u2212u(s1t , s2) (24)\nRemark 26 Note that we do not assume that the players have knowledge of the the joint utility function u. However, we do assume that the player has full information feedback, in the sense that they observe partial reward functions u( \u00b7 , s\u2212i\u03c4 ) on their entire action set, as opposed to only observing the reward u(s1\u03c4 , s 2 \u03c4 ) of the action played (the latter corresponds to the bandit setting).\nWe denote by U\u0303 it = {u\u0303i\u03c4}t\u03c4=1 the sequence of partial payoff functions observed by player i. We use U it to denote the set of all possible such histories, and define U i0 := \u2205. A strategy \u03c3i of player i is a collection {\u03c3it}\u221et=1 of (possibly random) mappings \u03c3it : U it\u22121 \u2192 Si, such that at iteration t, player i plays sit = \u03c3 i t(U i t\u22121). We make the following assumption on the payoff function:\nAssumption 3 The payoff function u is uniformly continuous in si with modulus of continuity independent of s\u2212i for i = 1, 2. That is, for each i there exists \u03c7i \u2208 Z such that |u(s, s\u2212i) \u2212 u(s\u2032, s\u2212i)| \u2264 \u03c7i(di(s, s\u2032)) for all s\u2212i \u2208 S\u2212i.\nIt is easy to see that Assumption 3 implies that the game has a value (see e.g. the argument in the proof of Lemma 39). It also makes our setting compatible with that of our Dual Averaging algorithm from Section 3. Suppose therefore that each player randomizes their play according to the sequence of probability distributions on Si generated by Dual Averaging with regularizer hi. That is, suppose that for i \u2208 {1, 2}, \u03c3it is a random variable with the following distribution:\n\u03c3it \u223c Dh\u2217i ( \u03b7t\u22121 \u2211t\u22121 \u03c4=1 u\u0303 i \u03c4 ) . (25)\nTheorem 27 Suppose that player i uses strategy \u03c3i according to (25). If the Dual Averaging algorithm ensures sublinear regret (i.e. lim suptRt/t \u2264 0), then \u03c3i is Hannan-consistent.\nCorollary 28 If both players use strategies according to (25) with the respective Dual Averaging ensuring that lim suptRt/t \u2264 0, then with probability 1 the sequence (x\u0302t)t\u22651 of empirical distributions of play weakly converges to the set of Nash equilibria of G.\nInterestingly, even though Dual Averaging is performed on Lp(Si), a strict subset of P(Si), Corollary 28 still ensures weak convergence of the empirical distributions of play to N (G)."}, {"heading": "5. Examples", "text": ""}, {"heading": "5.1. A Game With Unique Mixed Strategy Equilibrium", "text": "Consider the zero-sum game G1 between two players playing on the unit interval Si = [0, 1] with payoff function given by\nu(s1, s2) = (1 + s1)(1 + s2)(1\u2212 s1s2)\n(1 + s1s2)2 (26)\nSince |Dsiu| \u2264 8 for any s\u2212i \u2208 [0, 1] the payoff function is Lipschitz. It can be shown that V = 4/\u03c0 and that this game has no pure and a unique mixed Nash equilibrium, with equilibrium density xi(s) = 2\n\u03c0 \u221a s(1+s) the same for both players (Glicksberg and Gross, 1953). Note that xi is un-\nbounded and that xi \u2208 Lp(Si, \u03bb) for any 1 \u2264 p < 2. This unboundedness is the reason for the slow convergence of the empirical distributions to xi near zero that we can observe in Figure 2."}, {"heading": "5.2. A Game With Explicit Dual Averaging Updates", "text": "Consider a zero-sum game G2 between two players on the unit interval with payoff function\nu(s1, s2) = s2s2 \u2212 a1s1 \u2212 a2s2\nwhere a1 = e\u22122e\u22121 and a 2 = 1e\u22121 . It is easy to verify that the pair (x 1, x2) given by x1(s) = exp(s)e\u22121 and x2(s) = exp(1\u2212s)e\u22121 is a mixed-strategy Nash equilibrium of G2. For sequences (s 1 \u03c4 ) t \u03c4=1 and (s 2 \u03c4 ) t \u03c4=1, the cumulative payoff functions for fixed action s \u2208 [0, 1] are given, respectively, by\nU1t (s 1) = ( \u03a3t\u03c4=1s 2 \u03c4 \u2212 a1t ) s1 \u2212 a2\u03a3t\u03c4=1s2\u03c4 U2t (s2) = ( a2t\u2212 \u03a3t\u03c4=1s1\u03c4 ) s2 \u2212 a1\u03a3t\u03c4=1s1\u03c4\nIf each player i uses the Generalized Hedge Algorithm with a sequence of learning rates (\u03b7\u03c4 )t\u03c4=1 to minimize their respective regret, then their strategy in period t is given by sampling from the distribution xit(s) \u221d exp(\u03b1its), where \u03b11t = \u03b7t ( \u03a3t\u03c4=1s 2 \u03c4 \u2212 a1t ) and \u03b12t = \u03b7t ( a1t \u2212 \u03a3t\u03c4=1s1\u03c4 ) . Interestingly, in this case the sum of the opponent\u2019s past plays is a sufficient statistic, in the sense that it completely determines the mixed strategy at time t.\nFigure 3 shows normalized histograms of the empirical distributions of play at different iterations t. As t grows the histograms approach the equilibrium densities x1 and x2, respectively. Note however, that this does not mean that the individual strategies xit converge. Indeed, Figure 4 shows that the parameters \u03b1it keep oscillating around the equilibrium parameters 1 and \u22121, respectively, even for very large t. We do, however, observe that the time-averaged parameters \u03b1\u0304it converge to the equilibrium values 1 and \u22121.\n100 101 102 103 104 105 106 1.5\n1.0\n0.5\n0.0\n0.5\n1.0\n1.5\n2.0\n\u03b11t \u03b1\u03041t\n\u03b12t \u03b1\u03042t\nFigure 4: Evolution of parameters \u03b1it and \u03b1\u0304 i t := 1 t\n\u2211t \u03c4=1 \u03b1 i \u03c4 in G2"}, {"heading": "5.3. A Game on a Non-Convex Domain", "text": "One of the most interesting features of the Dual Averaging algorithms discussed in Section 3 is that they are applicable also in case of non-convex domains. We may therefore utilize them as a tool to compute approximate Nash equilibria in continuous zero-sum games on non-convex domains. In particular, consider a game G3 in which each Si = [0, 2]2 \\ [0.4, 1]2 is an L-shaped subset of R2. It is easy to see that the Lebesgue measure on this set is Q-regular with Q = 2, c0 = \u03c04 and C0 = \u03c0. We define the metric d\u0303 on S1 between any two points a, b \u2208 Si as the length (in the Euclidean distance) of the shortest path between a and b that is entirely contained in Si. The payoff function u is given as u(s1, s2) = d\u0303(s1, s2)\u2212 110 d\u0303(s 1, 0), which can be interpreted as a \u201chide and seek\u201d game\nin which player 1 would like to get as far away from player 2 as possible, while at the same time having a preference for being near the origin. Player 2 instead wants to be as close to player 1 as possible. Intuitively, this game will not admit a pure Nash equilibrium. Given the geometry of the problem, computing a mixed Nash equilibrium (whose existence follows from Theorem 20) poses a challenge. Instead, having both players play Entropy Dual Averaging on Lp(Si, \u03bb), we observe in Figure 5 that they indeed incur sublinear regret, and that the empirical distributions of play do converge. Figure 6 shows Kernel Density Estimates (KDE) of x\u03021t and x\u0302 2 t after t = 7500 iterations."}, {"heading": "Appendix A. Review of Some Results From Convex Analysis", "text": "In this section we collect some results from infinite-dimensional convex analysis that will play an important role in our analysis of the Dual Averaging algorithm.\nLemma 29 (Asplund, 1968) Let f : X \u2192 (\u2212\u221e,+\u221e] be proper lower semicontinuous. For a pair (x0, \u03be0) \u2208 X \u00d7X\u2217 the following are equivalent:\n(i) f\u2217 is finite and Fre\u0301chet differentiable at \u03be0 with Fre\u0301chet derivative Df\u2217(\u03be0) = x0.\n(ii) For some \u03b3\u2217 \u2208 \u0393L,\nf\u2217(\u03be) \u2264 f\u2217(\u03be0) + \u3008x0, \u03be \u2212 \u03be0\u3009+ \u03b3\u2217(\u2016\u03be \u2212 \u03be0\u2016), \u2200 \u03be \u2208 X\u2217 (27)\nand f\u2217(\u03be0) \u2208 R.\n(iii) For some \u03b3 \u2208 \u0393U ,\nf(x) \u2265 f(x0) + \u3008x\u2212 x0, \u03be0\u3009+ \u03b3(\u2016x\u2212 x0\u2016), \u2200x \u2208 X (28)\nand f(x0) \u2208 R.\n(iv) f\u2217 is finite at \u03be0, dom f\u2217 is radial at \u03be0, and xj \u2192 x0 in norm whenever\nlim j\u2192\u221e \u3008xj , \u03be0\u3009 \u2212 f(xj) = f\u2217(\u03be0) (29)\nAny of the above conditions implies that \u3008x0, \u03be0\u3009 = f(x0) + f\u2217(\u03be0) (in other words: the Fenchel-Young inequality holds with equality) and that f(x0) = f\u2217\u2217(x0). The functions \u03b3 and \u03b3\u2217 in (ii) and (iii) form a pair of mutually dual functions.\nNote that the function f in Lemma 29 need not be convex. The following result will be essential to our analysis:\nTheorem 30 (Stro\u0308mberg, 2011) Let f : X \u2192 (\u2212\u221e,+\u221e] be lower semicontinuous. Then f\u2217 is proper and essentially Fre\u0301chet differentiable if and only if f is a convex proper function that is essentially strongly convex."}, {"heading": "Appendix B. Dual Averaging in Continuous Time", "text": "In this section we use ideas from Kwon and Mertikopoulos (2014) and introduce a continuous-time regret minimization problem related to the one in discrete-time discussed in Section 2.2. In fact, this analysis will be crucial in proving the discrete-time regret bound (9) in Theorem 6."}, {"heading": "B.1. Regret Minimization in Continuous Time on Reflexive Banach Spaces", "text": "Consider a reflexive Banach space X with dual X\u2217 and regularizer h on X . Furthermore, suppose that uc : [0,\u221e)\u2192 X\u2217 is a continuous-time reward process satisfying the following assumptions:\nAssumption 4 The reward process uc is locally integrable for any x \u2208 X . That is, for all x \u2208 X , rx : t 7\u2192 \u3008uct , x\u3009 is Lebesgue-integrable on any compact set K \u2282 [0,\u221e).\nAssumption 5 There exists M <\u221e such that supx\u2208X |\u3008uct , x\u3009| \u2264M for all t.\nLet \u03b7c : [0,\u221e) \u2192 (0,\u221e) be a non-increasing and piece-wise continuous learning rate process. Furthermore, let U ct = \u222b t 0 uc\u03c4 d\u03c4 be the cumulative reward function. We consider the continuous-time process xc : [0,\u221e)\u2192 X given by\nxct := Dh \u2217(\u03b7ct U c t ) (30)\nTheorem 31 (Continuous-Time Regret Bound) Let h be a regularizer function on X , let \u03b7c be nondecreasing and locally piecewise continuous. Suppose that the reward process uc satisfies Assumptions 4 and 5. Then under (30) we have, for any x \u2208 X , that\nRct(x) := \u222b t 0 \u3008uc\u03c4 , x\u3009 d\u03c4 \u2212 \u222b t 0 \u3008uc\u03c4 , xc\u03c4 \u3009 d\u03c4 \u2264 h(x)\u2212 h \u03b7ct (31)\nwhere h := infx\u2208X h(x).\nProof [Theorem 31] Let yct = \u03b7ct \u222b t 0 uc\u03c4 d\u03c4 . By linearity,\n\u03b7ct \u222b t 0 \u3008uc\u03c4 , x\u3009 d\u03c4 = \u03b7ct \u2329\u222b t 0 uc\u03c4 d\u03c4, x \u232a = \u3008yct , x\u3009\nAssume for now that \u03b7c \u2208 C1. If h is proper, then\u222b t 0 \u3008uc\u03c4 , x\u3009 d\u03c4 = \u3008yct , x\u3009 \u03b7ct \u2264 h \u2217(yct ) + h(x) \u03b7ct = h\u2217(yct ) \u03b7ct + h(x) \u03b7ct (32)\nby the Fenchel-Young inequality. By Theorem 4, h\u2217 is essentially Fre\u0301chet differentiable with Fre\u0301chet gradient Dh\u2217(y). Furthermore, yct is differentiable. Thus, applying the chain rule and using that x c t = Dh\n\u2217(yct ) = arg maxx\u2208X ( \u3008x, yct \u3009 \u2212 h(x) ) = arg maxx\u2208X ( \u3008x, yct \u3009 \u2212 h(x) ) we obtain\nd\ndt\nh\u2217(yct ) \u03b7ct = \u03b7ct \u3008Dh\u2217(yct ), ddty c t \u3009 \u2212 h\u2217(yct ) \u03b7\u0307c(t) (\u03b7ct ) 2\n=\n\u2329 xct , ( \u03b7ctu c t + \u03b7\u0307 c t \u222b t 0 uc(s, \u03c4) d\u03c4 )\u232a \u03b7ct \u2212 \u03b7\u0307 c t (\u03b7ct ) 2 h\u2217(yct )\n= \u3008xct , uct\u3009+ \u03b7\u0307ct\n(\u03b7ct ) 2\n( \u3008xct , yct \u3009 \u2212 h\u2217(yct ) ) = \u3008xct , uct\u3009+\n\u03b7\u0307ct (\u03b7ct ) 2 h(xct)\nNow \u03b7\u0307ct \u2264 0 by assumption, and hence\nd\ndt\nh\u2217(yct )\n\u03b7ct \u2264 \u3008xct , uct\u3009+ \u03b7\u0307ct (\u03b7ct ) 2 h\nIntegrating from t = 0 to t = t yields\nh\u2217(yct )\n\u03b7ct \u2212 h\n\u2217(yc0) \u03b7c(0) \u2264 \u222b t 0 \u3008xc\u03c4 , uc\u03c4 \u3009 d\u03c4 + h \u222b t 0 \u03b7\u0307c\u03c4 (\u03b7c\u03c4 ) 2 d\u03c4\n= \u222b t 0 \u3008xc\u03c4 , uc\u03c4 \u3009 d\u03c4 \u2212 h ( 1 \u03b7ct \u2212 1 \u03b7c0 ) Now yc0 = 0, and hence h \u2217(yc0) = supx\u2208X \u2212h(x) = \u2212h, and so\nh\u2217(yct ) \u03b7ct \u2264 \u222b t 0 \u3008xc\u03c4 , uc\u03c4 \u3009 d\u03c4 \u2212 h \u03b7ct\nPlugging this into (32), collecting terms and rearranging yields (31). Now suppose that \u03b7c is only piecewise continuous. Then there exists a sequence (\u03b7c,i)\u221ei=1 of positive nonincreasing C1 functions such that \u03b7c,i \u2192 \u03b7c pointwise a.e.. Let xc,it := Dh\u2217(\u03b7 c,i t U c t ). Note that Dh\n\u2217 is continuous by Theorem 4 and thus xc,it \u2192 xct pointwise. By Assumption 5 we have that |\u3008uc\u03c4 , xc,i\u03c4 \u3009| < M for all \u03c4, i and thus \u222b t 0 \u3008uc\u03c4 , xc,i\u03c4 \u3009 d\u03c4 \u2192 \u222b t 0 \u3008uc\u03c4 , xc\u03c4 \u3009 d\u03c4 by Dominated Convergence."}, {"heading": "B.2. Online Optimization in Continuous Time on Compact Metric Spaces", "text": "One can also obtain bounds on the regret in continuous time by using similar arguments as in Section 3. While we do not make use of them in the main part of this article, these bounds may be of independent interest.\nWe consider the setting of Section 3. Specifically, let (S, d) be a compact metric space, and let \u00b5 \u2208 P , the set of Borel measures on S. Denote by B(s, r) = {s\u2032 \u2208 S : d(s, s\u2032) < r} the open ball of radius r centered at s. For p > 1 consider X = Lp(S, \u00b5) and X = {x \u2208 X : x \u2265 0 a.e., \u2016x\u20161 = 1}, the set of probability measures on S that are absolutely continuous w.r.t. to \u00b5 and whose Radon-Nikodym densities are p-integrable. Denote by DS := sups,s\u2032\u2208S d(s, s\n\u2032) the diameter of S and by B(s, \u03d1ct) \u2282 X the set of elements of X with support contained in B(s, \u03d1ct). We need the following continuous-time variant of Assumption 1:\nAssumption 6 The reward process uc has modulus of continuity \u03c7 on S, uniformly in t. That is, there exists \u03c7 \u2208 Z such that |uct(s)\u2212 uct(s\u2032)| \u2264 \u03c7(d(s, s\u2032)) for all s, s\u2032 \u2208 S for all t.\nTheorem 32 (Continuous-Time Regret Bound on Metric Spaces) Let (S, d) be compact, and suppose that Assumption 6 holds. Let h be a regularizer function on X , and let \u03b7c be non-decreasing and locally piecewise continuous. Suppose further that \u03d1c : [0,\u221e) \u2192 (0,\u221e) is a non-negative function and that the reward process uc satisfies Assumptions 4 and 5. Then, under the process (30),\nRct \u2264 sups\u2208S infx\u2208B(s,\u03d1ct) h(x)\n\u03b7ct + t \u03c7(\u03d1ct)\u2212\nh \u03b7ct (33)\nProof [Theorem 32] Similar to the proof of Theorem 8.\nProposition 33 Suppose that Assumption 2 holds with constants c0 > 0 and C0 < \u221e. Under the Assumptions of Theorem 32, with essentially strongly convex regularizer h\u03c6 the f -divergence of an \u03c9-potential \u03c6, we have the following regret bound:\nRct t \u2264 min(C0 (\u03d1 c t) Q, \u00b5(S)) t \u03b7ct f\u03c6 ( c\u221210 (\u03d1 c t) \u2212Q)+ \u03c7(\u03d1ct) (34)\nProof [Proposition 33] Similar to the proof of Proposition 13."}, {"heading": "Appendix C. Computing the Dual Averaging Optimizer", "text": "In this section we discuss some aspects concerning the computation of the optimizer in the Dual Averaging update in the setting of online optimization on compact metric spaces with uniformly continuous rewards. The results of this section are used for generating the Hannan-consistent strategies in the repeated games in Section 5, and for performing the numerical benchmarks of the algorithms in Appendix D.\nAs pointed out in Section 3.2, it can be shown that for f -Divergences of \u03c9-potentials, the Fre\u0301chet differential Dh\u2217 in this case has a simple expression in terms of the dual problem, and the problem of computing xt+1 = Dh \u2217(\u03b7t \u2211t \u03c4=1 u\u03c4 ) reduces to computing a scalar dual variable \u03bd \u2217 t . In particular, one can show the following:\nProposition 34 (Krichene and Balandat, 2016) Let \u03c6 be an \u03c9-potential with associated f -Divergence h\u03c6 on X . Then\nDh\u2217\u03c6(\u03be) = \u03c6(\u03be + \u03bd ?)+ (35)\nwhere ( \u00b7 )+ denotes the positive part of ( \u00b7 ), and \u03bd? satisfies \u222b S \u03c6(\u03be + \u03bd?)+ d\u00b5(s) = 1.\nBy Proposition 34, the Fre\u0301chet derivativeDh\u2217\u03c6 at \u03be = \u03b7tUt is entirely determined by the dual variable \u03bd ?, the unique \u03bd such that f(\u03bd) = 1, where f(\u03bd) = \u222b S \u03c6(\u03b7t(Ut(s) + \u03bd\n?))+ d\u00b5(s). Since f is increasing by assumption on \u03c6, \u03bd? can be determined using a simple bisection method. To guide the search for \u03bd?t for t > 0 we can make use of the following result:\nProposition 35 Suppose \u03c6 is convex and let \u03bd?t the optimal dual variable determining Dh\u2217\u03c6(\u03b7tUt). Then\n\u03b7t \u03b7t+1 \u03bd?t \u2212M \u2264 \u03bd?t+1 \u2264 \u03b7t \u03b7t+1 \u03bd?t + \u03b7t \u2212 \u03b7t+1 \u03b7t+1 tM (36)\nwhere \u03bd?0 = \u03b7 \u22121 0 \u03c6 \u22121(1). Moreover, for \u03b7t = \u03b7 t\u2212\u03b2 this interval has length \u2248 (1 + \u03b2)M .\nProof [Proposition 35] Since Ut \u2261 0, we have \u03bd?0 = \u03b7\u221210 \u03c6\u22121(1). Moreover, by definition we have\u222b S \u03c6 ( \u03b7t ( Ut(s) + \u03bd ? t )) + d\u00b5(s) = \u222b S \u03c6 ( \u03b7t+1 ( Ut+1(s) + \u03bd ? t+1 )) + d\u00b5(s) = 1\nIf \u03c6 is convex, then so is \u03c6( \u00b7 )+ as z 7\u2192 z+ is convex and nondecreasing. Therefore\n1 = \u222b S \u03c6 ( \u03b7t ( Ut(s) + \u03bd ? t ) + (\u03b7t+1 \u2212 \u03b7t)Ut(s)\u2212 \u03b7t\u03bd?t + \u03b7t+1\u03bd?t+1 + \u03b7t+1ut+1(s) ) + d\u00b5(s)\n\u2264 \u222b S \u03c6 ( \u03b7t ( Ut(s) + \u03bd ? t )) + + \u03c6 ( \u03b7t ( Ut(s) + \u03bd ? t ))\u2032 + ( (\u03b7t+1 \u2212 \u03b7t)Ut(s)\n\u2212 \u03b7t\u03bd?t + \u03b7t+1\u03bd?t+1 + \u03b7t+1ut+1(s) ) d\u00b5(s)\n\u2264 1 + \u222b S \u03c6 ( \u03b7t ( Ut(s) + \u03bd ? t ))\u2032 + ( \u03b7t+1\u03bd ? t+1 \u2212 \u03b7t\u03bd?t + \u03b7t+1M ) d\u00b5(s)\nand hence, since \u03c6\u2032 \u2265 0, we must have that \u03b7t+1\u03bd?t+1 \u2212 \u03b7t\u03bd?t + \u03b7t+1M \u2265 0. Rearranging yields the lower bound on \u03bd?t+1. The other inequality is proven in a similar fashion by reversing the roles of t and t+1. Finally, to show that the interval has length \u2248 (1 + \u03b2)M independent of t, note that \u03b7t\u03b7t+1 = (1 + 1 t ) \u03b2 \u2248 1 + \u03b2t , and so \u03b7t\u2212\u03b7t+1\u03b7t+1 tM \u2248 \u03b2M .\nHaving determined \u03bd?t , we then have an explicit form of the distribution over S from which to sample st+1. For this, a variety of established methods can be used, from simple rejection sampling in low dimensions (employed in our simulations) to MCMC methods (e.g. slice sampling) in higher dimensions. In cetain special cases, sampling from xt may be done very efficiently. For example, if the losses are affine, the domain S is a hyperrectangle, and the potential is a generalized Exponential Potential, then st+1 can be obtained by sampling from n independent truncated exponential random variables. The main computational challenge is then to compute the integral in f . Off-the-shelf numerical integration schemes work well if n is small, but are typically not applicable in higher dimensions. Instead, one has to resort to other methods, such as Monte Carlo methods or sparse grids."}, {"heading": "Appendix D. Numerical Results and Comparison With Other Methods", "text": "In this section, we review some algorithms for online convex optimization over subsets of Rn that have been proposed in the literature, and compare them with our Dual Averaging method for online optimization on compact metric spaces with uniformly continuous rewards from Section 3. Such algorithms are often formulated in terms of loss functions `\u03c4 , but clearly these algorithms apply just as well by setting `\u03c4 = \u2212u\u03c4 , as long as the set S is convex and the rewards are concave and satisfy the additional assumptions made by the algorithms. Table 1 summarizes the regret bounds of each method, with the corresponding assumptions on the feasible set and the loss functions.\nThe bound on Dual Averaging in Table 1 is obtained by assuming the regularizer to be the f -divergence associated to an \u03c9-potential and making an assumption on the asymptotic growth rate of the function f\u03c6 as follows:\nCorollary 36 Suppose that f\u03c6(x) \u2264 C\u03c6 x1+\u03ba for some \u03ba > 0 and C\u03c6 < \u221e. Suppose further the rewards are \u03b1-Ho\u0308lder continuous, i.e. \u03c7(r) = C\u03b1 r\u03b1, and that h\u03c6 is uniformly essentially strongly convex with modulus \u03b3(r) = K2 r 2. Then the learning rate \u03b7t = \u03b7 t\u2212\u03b2 with \u03b7 = 1M ( 1+ \u03ba\u03b1Q 2+ \u03ba\u03b1Q C0C\u03c6 c1+\u03ba0 \u03d1 \u03baQ )1/2 and \u03b2 = 12+ \u03ba\u03b1Q yields the following bound:\nR t \u2264 ( 2MC\u0303\u03d1\u2212 \u03baQ 2 + C\u03b1\u03d1 \u03b1 ) t \u2212 1 2+ \u03ba \u03b1 Q (37)\nfor any \u03d1 < r0, where C\u0303 = \u221a 2+ \u03ba\u03b1Q\n1+ \u03ba\u03b1Q C0C\u03c6 c1+\u03ba0 ."}, {"heading": "D.1. Optimizing Sequences of Convex Functions over Convex Sets", "text": "Zinkevich (2003) formalized the online convex optimization problem, in which the feasible set S and the loss functions are assumed to be convex. He proposed a Greedy Projection method (GP), summarized in Algorithm 1, which we will also refer to as Online Gradient Descent (OGD). Theorem 1 in (Zinkevich, 2003) shows that when \u2016\u2207`t\u2016 is uniformly bounded, the regret of GP with learning rates \u03b7t = 1/ \u221a t grows as\nO( \u221a t). Hazan et al. (2007) show that it is possible to obtain logarithmic regret under additional assumptions on the loss functions. In particular, if the losses are H-strongly convex then GP with learning rates \u03b7t = 1Ht has regret Rt \u2264 M 2\n2H (1 + log t). They also propose methods for uniformly exp-concave losses, that is,\nwhen there exists \u03b1 > 0 such that exp(\u2212\u03b1`t) is concave for all t. These methods, Exponentially Weighted Online Optimization (EWOO) and Follow The Approximate Leader (FTAL), are summarized in Algorithm 2 and 3 (their Online Newton Step (ONS) algorithm is very similar to FTAL and and therefore omitted). The respective regret bounds are given in Theorems 4 and 7 in (Hazan et al., 2007) and are summarized in Table 1.\nAlgorithm 1 Greedy Projection method (GP) a.k.a. Online Gradient Descent (OGD), with input sequence (`t) and learning rates (\u03b7t)\n1: for t \u2208 N do 2: Let s\u0303t+1 = st \u2212 \u03b7t+1\u2207`t(st) 3: Update: xt+1 = \u03b4st+1 , where\nst+1 = arg min s\u2208S\n\u2016s\u2212 s\u0303t+1\u2016\nAlgorithm 2 Exponentially Weighted Online Optimization method (EWOO), with input sequence (`t) and learning rate \u03b1.\n1: for t \u2208 N do 2: Let Lt = \u2211t \u03c4=1 `\u03c4 3: Let x\u0303t+1(s) = e \u2212\u03b1Lt(s)\u222b\nS e \u2212\u03b1Lt(s)\u03bb(ds)\n4: Update: xt+1 = \u03b4st+1 , where\nst+1 = Es\u223cx\u0303t+1 [s]\nAlgorithm 3 Follow The Approximate Leader (FTAL) with input sequence (`t) and parameter \u03b2.\n1: for t \u2208 N do 2: Let g\u03c4 = \u2207`\u03c4 (s\u03c4 ) 3: Let At = \u2211t \u03c4=1 g\u03c4 (g\u03c4 )\nT and s\u0303t+1 = (At) \u2020(\u2211t \u03c4=1 g\u03c4 (g\u03c4 ) T s\u03c4 \u2212 1\u03b2 g\u03c4 ) ,\nand define \u2016s\u2016At = \u3008s,Ats\u3009. 4: Update: xt+1 = \u03b4st+1 , where\nst+1 = arg min s\u2208S \u2016s\u2212 s\u0303t+1\u2016At\nAlgorithm 4 Dual Averaging (DA) with input sequence (ut), learning rates (\u03b7t), and regularizer h.\n1: for t \u2208 N do 2: Let Ut = \u2211t \u03c4=1 u\u03c4 3: Update\nx(t+1) = Dh \u2217(\u03b7tUt)\n= arg max x\u2208X\n\u2329 \u03b7tUt, x \u232a \u2212 h(x)\nExample 4 (Convex Quadratics on a Hypercube) As a first example, we consider quadratic reward functions of the form ut(s) = \u2212 12 (s\u2212\u00b5t)\nTQt(s\u2212\u00b5t)\u2212 ct, where Qt is p.d. symmetric, and ct \u2265 0. The domain is S = {\u2016s\u2016\u221e \u2264 0.5} with DS = \u221a n, and the rewards are generated randomly, L-Lipschitz with L = 5 and uniformly bounded by \u2016ut\u2016\u221e \u2264 3.75 and \u2016ut\u20164 \u2264 1.6. Figure 7 shows the time-average regretsRt/t in dimensions n = 2 and n = 3 for time horizons of T = 104 and T = 4 \u00b7 103, respectively. Displayed are the empirical means over N = 2500 runs of the algorithm (solid), the associated theoretical bounds1 (dashed), and the regions between the associated 10% and 90% quantiles (shaded).\nNot surprisingly, those algorithms that exploit the strong convexity of the problem (OGD, FTAL, EWOO) achieve better asymptotic rates than GP (which requires only convexity) or DA (which makes no convexity assumptions at all). Still, the regret of DA is not significantly higher than that of GP and OGD, and is competitive with FTAL over the simulation horizon. We note that the theoretical regret bounds for both DA instances are much closer to the actual regret of the algorithm.\nTable 2 shows the decay rates (which correspond to the slopes in the log-log plots) of empirical means and theoretical bounds in Figure 7 at the end of the simulation horizon. There is a relatively good match between bounds and simulations. Except for FTAL and EWOO, all algorithms exhibit a decay that is faster than that of the associated bound2. When making this comparison, one must keep in mind that all these bounds are worst-case in nature, and that it is not entirely clear what characterizes a worst-case sequence of\n1. For easier readability we omitted the bound on FTAL, which in this example is much higher than the others. 2. For EWOO this discrepancy is likely due to numerical inaccuracies at the very small regrets for large t, while for\nFTAL the simulation may not have reached the asymptotic regime yet.\nreward functions (see Example 5 for a partial remedy).\nn = 2 n = 3 Algorithm simulation theory simulation theory GP -0.564 -0.497 -0.515 -0.495 OGD -0.920 -0.900 -0.892 -0.888 FTAL -0.780 -0.900 -0.705 -0.888 EWOO -0.809 -0.900 -0.676 -0.888 DA, Exp -0.519 -0.446 -0.481 -0.439 DA, 1.5-Norm -0.452 -0.333 -0.396 -0.286\nTable 2: Rates in Figure 7\nPotential simulation theory ExpPot -0.557 -0.446 1.01-Norm -0.546 -0.495 1.05-Norm -0.477 -0.476 1.5-Norm -0.307 -0.333 1.75-Norm -0.279 -0.286\nTable 3: Rates in Figure 8\nExample 5 (Alternating Affine Losses on a Hypercube) In this example we consider a situation in which the greedy algorithm mentioned in Section 3 fails3, and offer a simulation that illustrates the result of Proposition 18. We consider a sequence of affine reward functions on S = {\u2016s\u2016\u221e \u2264 0.5} in R2, alternating in such a way that any maximizer s?t of Ut is in fact a minimizer of Ut+1. Specifically, we choose ut(s) = \u2212\u3008at, s\u3009\u2212ct, where\na0 = [L/2, 0], c0 = L/4, at = [(\u22121)tL, 0], ct = L/2\nfor t \u2265 1. It is easy to see that in this case the greedy algorithm incurs time-average regretRt/t = L + o(1). Figure 8 shows regrets for the greedy algorithm and DA with Exponential and different \u03c1-Norm potentials. Besides the obvious failure of the greedy algorithm, we observe that for p-Norm potentials performance decreases as \u03c1 \u2198 1, which can be explained by Proposition 18. Nevertheless, DA guarantees sublinear regret for any \u03c1 > 1 (with theoretical asymptotic rate approaching t\u22121/2 as \u03c1 \u2192 1), though at the cost of much higher constants in the bound as \u03c1 \u2248 1. Table 3 shows that empirical and theoretical rates in this instance (which is intuitively hard) are very close, providing further support for the theoretical analysis of DA. Finally, Figure 9 for each potential shows the negative entropy DKL(xt||\u03bb) of xt. From this we observe that the minimizers x?[\u03c1] are indeed more and more concentrated around their mode as \u03c1\u2198 1.\n3. In fact, any deterministic policy will incur linear regret in a nontrivial adversarial setting."}, {"heading": "Appendix E. Proofs Omitted in the Main Part", "text": ""}, {"heading": "PROOF OF THEOREM 4", "text": "Proof [Theorem 4] Essential Fre\u0301chet differentiability, the characterization (7) of the Fre\u0301chet gradient in (i) and (ii) follow from Theorem 30, Lemma 29, and the definition of uniform essential strong convexity. To prove (8), let \u03be1, \u03be2 \u2208 X\u2217 and let xi = Df\u2217(\u03bei) = arg maxx\u2208X\u3008\u03bei, x\u3009\u2212f(x). Then, by first-order optimality, \u3008z \u2212 \u03bei, x\u2212 xi\u3009 \u2265 0, \u2200 z \u2208 \u2202f(xi),\u2200x \u2208 X . In particular,\n\u3008z1 \u2212 \u03be1, x2 \u2212 x1\u3009 \u2265 0 \u3008z2 \u2212 \u03be2, x1 \u2212 x2\u3009 \u2265 0\nfor all zi \u2208 \u2202f(xi), i = 1, 2. Summing these inequalities we find that\n\u3008\u03be1 \u2212 \u03be2, x1 \u2212 x2\u3009 \u2265 \u3008z1 \u2212 z2, x1 \u2212 x2\u3009\nBy uniform strong convexity, we further have that f(x) \u2265 f(xi) + \u3008x\u2212 xi, zi\u3009+ \u03b3(\u2016x\u2212 xi\u2016) for all x \u2208 X . In particular,\nf(x1) \u2265 f(x2) + \u3008x1 \u2212 x2, z2\u3009+ \u03b3(\u2016x1 \u2212 x2\u2016) f(x2) \u2265 f(x1) + \u3008x2 \u2212 x1, z1\u3009+ \u03b3(\u2016x2 \u2212 x1\u2016)\nand summing these inequalities yields\n\u3008z1 \u2212 z2, x1 \u2212 x2\u3009 \u2265 2\u03b3(\u2016x1 \u2212 x2\u2016)\nOn the other hand, \u3008\u03be1 \u2212 \u03be2, x1 \u2212 x2\u3009 \u2264 \u2016\u03be1 \u2212 \u03be2\u2016\u2217\u2016x1 \u2212 x2\u2016 by definition of the dual norm, so\n\u03b3\u0303(\u2016x1 \u2212 x2\u2016) \u2264 1\n2 \u2016\u03be1 \u2212 \u03be2\u2016\u2217\nusing the definition of \u03b3\u0303. If \u03b3\u0303 is strictly increasing it admits a (strictly increasing) inverse \u03b3\u0303\u22121. Applying \u03b3\u0303\u22121 to both sides then yields (8)."}, {"heading": "PROOF OF THEOREM 6", "text": "Proof [Theorem 6] We consider the continuous-time reward and learning rate processes uc and \u03b7c given by uct := udte and \u03b7\nc(t) := \u03b7btc\u22281, respectively, where dre := inf{n \u2208 Z : n \u2265 r} and brc = sup{n \u2208 Z : n \u2264 r} for all r \u2208 R and a \u2228 b = min(a, b). In doing so we follow the ideas of the analysis of Kwon\nand Mertikopoulos (2014) (our problem is, however, different as our reward vectors are infinite-dimensional). With this\nxk = Dh \u2217 ( \u03b7k\u22121 k\u22121\u2211 j=1 uj ) = Dh\u2217 ( \u03b7c(k \u2212 1) \u222b k\u22121 0 uc\u03c4 d\u03c4 ) = xck\u22121\nand thus, for j \u2265 1 and t \u2208 (j \u2212 1, j), we have\n|\u3008uct | xct\u3009 \u2212 \u3008uj , xj\u3009| = |\u3008uj , xct \u2212 xcj\u22121\u3009| \u2264 \u2016uj\u2016\u2217\u2016xct \u2212 xcj\u22121\u2016 (38)\nby definition of the dual norm. Therefore\n|\u3008uct , xct\u3009 \u2212 \u3008uj , xj\u3009| \u2264 \u2016uj\u2016\u2217\u2016Dh\u2217(yct )\u2212Dh\u2217(ycj\u22121)\u2016 \u2264 \u2016uj\u2016\u2217 \u03b3\u0303\u22121 ( \u2016yct \u2212 ycj\u22121\u2016\u2217/2 ) (39)\nwhere the second inequality follows from Theorem 4. From the definition of yct , we have\n\u2016yct \u2212 ycj\u22121\u2016\u2217 = \u2225\u2225\u2225\u2225\u03b7c(j \u2212 1)\u222b t\nj\u22121 uc\u03c4 d\u03c4 \u2225\u2225\u2225\u2225 \u2217 \u2264 \u03b7j\u22121\u2016uj\u2016\u2217(t\u2212 j + 1)\nand therefore\u2223\u2223\u2223\u2223\u222b k 0 \u3008uc\u03c4 , xc\u03c4 \u3009d\u03c4 \u2212 k\u2211 j=1 \u3008uj , xj\u3009 \u2223\u2223\u2223\u2223 \u2264 k\u2211 j=1 \u222b j j\u22121 |\u3008uc\u03c4 , xc\u03c4 \u3009 \u2212 \u3008uj , xj\u3009| d\u03c4\n\u2264 k\u2211 j=1 \u2016uj\u2016\u2217 \u222b j j\u22121 \u03b3\u0303\u22121 ( \u03b7j\u22121(t\u2212 j + 1) 2 \u2016uj\u2016\u2217 ) d\u03c4 \u2264 k\u2211 j=1 \u2016uj\u2016\u2217\u03b3\u0303\u22121 (\u03b7j\u22121 2 \u2016uj\u2016\u2217\n) where the last equality follows since \u03b3\u0303\u22121 is non-decreasing (a consequence of \u03b3 being sublinear). Finally, we note that\nk\u2211 j=1 \u3008uj , x\u3009 \u2212 k\u2211 j=1 \u3008uj , xj\u3009 = \u222b k 0 \u3008uc\u03c4 , x\u3009 d\u03c4 \u2212 k\u2211 j=1 \u3008uj , xj\u3009\n\u2264 \u2223\u2223\u2223\u2223\u222b k\n0 \u3008uc\u03c4 , x\u3009 d\u03c4 \u2212 \u222b k 0 \u3008uc\u03c4 , xc\u03c4 \u3009d\u03c4 \u2223\u2223\u2223\u2223+ \u2223\u2223\u2223\u2223\u222b k 0 \u3008uc\u03c4 , xc\u03c4 \u3009d\u03c4 \u2212 k\u2211 j=1 \u3008uj , xj\u3009 \u2223\u2223\u2223\u2223\nThe bound (10) then follows from Theorem 31 and the above."}, {"heading": "PROOF OF COROLLARY 7", "text": "Proof [Corollary 7] It is easy to show that \u03b3\u0303\u22121 (\u03b7j\u22121 2 \u2016uj\u2016\u2217 ) \u2264 (2C)\u22121/\u03ba \u03b71/\u03baj\u22121 \u2016uj\u2016 1/\u03ba \u2217 . If \u2016uj\u2016\u2217 \u2264 M for all j, then Rt(x) \u2264 h(x)\u2212h\u03b7ct + (2C) \u22121/\u03baM1+1/\u03ba \u2211t \u03c4=1 \u03b7 1/\u03ba \u03c4\u22121. In particular, if \u03b7t = \u03b7 t \u2212\u03b2 , then (12) follows\nfrom the bound \u2211t \u03c4=1 (j \u2212 1) \u2212\u03b2/\u03ba \u2264 \u222b t 0 v\u2212\u03b2/\u03badv = \u03ba\u03ba\u2212\u03b2 t 1\u2212 \u03b2\u03ba"}, {"heading": "PROOF OF THEOREM 8", "text": "Proof [Theorem 8] The spaceX = Lp(S) is uniformly convex (Clarkson, 1936), and thus reflexive (Milman, 1938). Its dual is X\u2217 = Lq(S, \u00b5) for q = pp\u22121 and \u3008x, \u03be\u3009 = \u222b S x(s)\u03be(s)\u00b5(ds) for x \u2208 X and \u03be \u2208 X\u2217. Fix t <\u221e. Then for any s \u2208 S and all x \u2208 B(s, \u03d1t)\n\u3008Ut, x\u3009 = \u222b B(s,\u03d1t) Ut(s \u2032)x(s\u2032) d\u00b5(s\u2032) = \u222b B(s,\u03d1t) t\u2211 \u03c4=1 u\u03c4 (s \u2032)x(s\u2032) d\u00b5(s\u2032) d\u03c4\n\u2265 t\u2211\n\u03c4=1\n\u222b B(s,\u03d1t) ( u\u03c4 (s)\u2212 \u03c7(\u03d1t) ) x(s\u2032) d\u00b5(s\u2032) d\u03c4 = Ut(s)\u2212 t \u03c7(\u03d1t)\nand therefore\nRt = sup s\u2208S\nUt(s)\u2212 t\u2211\n\u03c4=1\n\u3008u\u03c4 , x\u03c4 \u3009\n\u2264 sup s\u2208S inf x\u2208B(s,\u03d1t)\n\u3008Ut, x\u3009+ t \u03c7(\u03d1t)\u2212 t\u2211\n\u03c4=1\n\u3008u\u03c4 , x\u03c4 \u3009\n= sup s\u2208S inf x\u2208B(s,\u03d1t) Rt(x) + t \u03c7(\u03d1t)\nand thus (33) follows from (31) in Theorem 31."}, {"heading": "PROOF OF PROPOSITION 11", "text": "Denote by 1A the indicator function of the set A, i.e. 1A(s) = 1 if s \u2208 A and 1A(s) = 0 if s 6\u2208 A. In this proof we will make use of the following Lemma:\nLemma 37 Let (S, d) be a compact metric space and let \u00b5 be an r0-locally Q-regular measure on S. For p \u2265 1 let X p := {x \u2208 Lp(S, \u00b5) : x \u2265 0 a.s., \u2016x\u20161 = 1}. Suppose further that f : S \u2192 R is continuous. Then\nsup s\u2208S f(s) = sup x\u2208P \u222b S f(s) dx(s) = sup x\u2208Xp \u222b S f(s) dx(s), \u2200 p \u2208 [0,\u221e] (40)\nProof [Lemma 37] The first equality follows directly by observing that Borel measures measures include measures with finite support. Clearly supx\u2208P \u222b S f(s) dx(s) \u2265 supx\u2208Xp \u222b S f(s) dx(s) since X p \u2282 P for all p \u2208 [1,\u221e]. Since Lp \u2282 Lq for all q \u2265 p it suffices to show the reverse inequality holds for p =\u221e. Since S is compact and f is continuous, there exists a maximizer s? of f on S. Let > 0. By continuity, there exists \u03b4 > 0 such that |f(s)\u2212 f(s\u2032)| \u2264 whenever d(s, s\u2032) < \u03b4. Moreover, by local Q-regularity of \u00b5 we have that \u00b5(B(s?, \u03b4)) > 0. Now let x(s) = 1\u00b5(B(s?,\u03b4)) 1B(s?,\u03b4)(s). Clearly x \u2208 X\n\u221e, and\u222b Si f(s) dx(s) = 1 \u00b5(B(s?, \u03b4)) \u222b B(s?,\u03b4) f(s) d\u03bb(s) \u2265 1 \u00b5(B(s?, \u03b4)) \u222b B(s?,\u03b4) (f(s?)\u2212 ) d\u03bb(s) = f(s\u2217)\u2212\nNow let \u2198 0.\nProof [Proposition 11] Recall that\nRt(x) = t\u2211 \u03c4=1 \u3008u\u03c4 , x\u3009 \u2212 t\u2211 \u03c4=1 \u3008u\u03c4 , x\u03c4 \u3009 = \u222b S Ut(s) dx(s)\u2212 t\u2211 \u03c4=1 \u3008u\u03c4 , x\u03c4 \u3009\nClearly Ut is continuous (in fact, with modulus of continuity t \u03c7(r)) on S for any t < \u221e. The equivalence of the suprema then follows from a direct application of Lemma 37."}, {"heading": "PROOF OF PROPOSITION 13", "text": "Proof [Proposition 13] By convexity of f , we have that h(x) = h\u03c6(x) \u2265 f\u03c6 (\u222b S dx d\u00b5d\u00b5(s) ) = f\u03c6(1) = 0 for all x \u2208 X , and thus h = 0. Furthermore, choosing x as the uniform Radon-Nikodym density w.r.t. \u00b5 on B(s, \u03d1t), i.e.,\nx(s\u2032) = 1B(s,\u03d1t)(s\n\u2032)\n\u00b5(B(s, \u03d1t))\nwe have that\nh(x) = \u222b Si f\u03c6(x(s \u2032))\u00b5(ds\u2032) = \u222b B(s,\u03d1t) f\u03c6 ( 1 \u00b5(B(s, \u03d1t)) ) \u00b5(ds\u2032)\n\u2264 min ( C0(\u03d1t) Q, \u00b5(S) ) f\u03c6\n( 1\n\u00b5(B(s, \u03d1t)) ) where we used the assumption of r0-local Q-regularity and the fact that \u03d1t \u2264 r0. It is easy to see that f\u03c6 is increasing on [1,\u221e). Indeed, f \u2032\u03c6(x) = \u03c6\u22121(x), and \u03c6\u22121(x) is increasing by assumption with \u03c6\u22121(1) \u2265 0. Moreover, since \u00b5(S) = 1 by assumption, we have that \u00b5(B(s, \u03d1t)) \u2264 1 for any s, so\nh(x) \u2264 min ( C0(\u03d1t) Q, \u00b5(S) ) f\u03c6 ( c\u221210 (\u03d1t) \u2212Q) Plugging this into the general bound (13) of Theorem 8 yields (16)."}, {"heading": "PROOF OF COROLLARY 14", "text": "Proof [Corollary 14] Plugging \u03b3\u0303(r) = 2r, f\u03c6(x) = x log x and \u03c7(r) = C\u03b1r\u03b1 into (16) we find that\nRt t \u2264 C0 c0 t \u03b7t\nlog ( c\u221210 \u03d1 \u2212Q t ) + C\u03b1\u03d1 \u03b1 t + M2\nt t\u2211 \u03c4=1 \u03b7\u03c4\u22121\nLetting \u03b7t = \u03b7 \u221a log t t\u2212\u03b2 we have that\nt\u2211 \u03c4=1 \u03b7\u03c4\u22121 \u2264 \u03b7 \u221a log t t\u2211 \u03c4=1 t\u2212\u03b2 \u2264 \u03b7 \u221a log t t\u2211 \u03c4=1 \u222b \u03c4 \u03c4\u22121 z\u2212\u03b2dz = \u03b7 \u221a log t \u222b t 0 z\u2212\u03b2dz = \u03b7 \u221a log t 1\u2212 \u03b2 t1\u2212\u03b2\nand therefore\nRt t \u2264 C0 c0\u03b7 t\u03b2\u22121\u221a log t\nlog ( c\u221210 \u03d1 \u2212Q t ) + C\u03b1\u03d1 \u03b1 t + \u03b7M2 1\u2212 \u03b2 \u221a log t t\u2212\u03b2\nChoosing \u03b2 = 1/2 and \u03d1t = \u03d1 1 \u03b1 (log t) 1 2\u03b1 t\u2212 \u03b2 \u03b1 this becomes, after dropping a 1/ log t term,\nRt t \u2264 ( C0 c0 \u03b7 ( log(c\u221210 \u03d1 \u2212Q/\u03b1) + Q 2\u03b1 ) + C\u03b1\u03d1+ 2\u03b7M 2 )\u221a log t t\nas \u03d1t < r0 since \u221a log t/t < \u03d1\u22121r\u03b10 . Then choosing \u03b7 = 1 M \u221a C0Q 2c0 log(c\u221210 \u03d1 \u2212Q/\u03b1) + Q2\u03b1 gives\nRt t \u2264 ( 2M \u221a 2C0 c0 ( log(c\u221210 \u03d1 \u2212Q/\u03b1) + Q 2\u03b1 ) + C\u03b1\u03d1 )\u221a log t t"}, {"heading": "PROOF OF THEOREM 15", "text": "Proof [Theorem 15] Since S is compact there exist sa, sb \u2208 S such that d(sa, sb) = DS . Let xa = \u03b4sa and xb = \u03b4sb , where \u03b4s denotes the Dirac measure on S at s. Let w : R \u2192 R be any function with modulus of continuity \u03c7 such that \u2016w(d( \u00b7 , sb))\u2016q \u2264 M . Define v : S \u2192 R by v(s) = w(d(s, sb)). Using the triangle inequality it is easy to see that v also has modulus of continuity \u03c7. Now observe that\n\u3008v, xa \u2212 xb\u3009 = v(sa)\u2212 v(sb) = w(d(sa, sb)) = w(DS)\nLet V1, . . . , V2 a sequence of i.i.d. Rademacher random variables, i.e. P(Vi = +1) = P(Vi = \u22121) = 12 , and consider the (random) sequence of reward vectors (u\u03c4 )t\u03c4=1 with ut = Vtv. By Proposition 11 we have that Rt = supx\u2208P Rt(x), and thus\nE[Rt] = E [\nsup x\u2208P t\u2211 \u03c4=1 \u3008u\u03c4 , x\u3009 \u2212 t\u2211 \u03c4=1 \u3008u\u03c4 , x\u03c4 \u3009 ] \u2265 E [ max x\u2208{xa,xb} t\u2211 \u03c4=1 \u3008u\u03c4 , x\u3009 ] \u2212 E [ t\u2211 \u03c4=1 \u3008u\u03c4 , x\u03c4 \u3009 ]\n= E [\nmax x\u2208{xa,xb} t\u2211 \u03c4=1 V\u03c4 \u3008v, x\u3009 ] \u2212 E [ t\u2211 \u03c4=1 V\u03c4 \u3008v, x\u03c4 \u3009 ]\nObserve that the second expectation is zero for any sequence of (x\u03c4 )t\u03c4=1 with x\u03c4 measurable with respect to \u03c3(V1, . . . , V\u03c4\u22121), i.e. any online algorithm. Noting that max(a, b) = 12 (a+ b) + 1 2 |a\u2212 b| we thus have that\nE[Rt] \u2265 1 2 E [ t\u2211 \u03c4=1 V\u03c4 \u3008v, xa + xb\u3009 ] + 1 2 E [ \u2223\u2223\u2223\u2223 t\u2211 \u03c4=1 V\u03c4 \u3008v, xa \u2212 xb\u3009 \u2223\u2223\u2223\u2223]\n= w(DS) 2 E [ \u2223\u2223\u2223\u2223 t\u2211\n\u03c4=1\nV\u03c4 \u2223\u2223\u2223\u2223] \u2265 w(DS)2\u221a2 \u221at where the last step follows from an application of Khintchine\u2019s inequality (Haagerup, 1981)."}, {"heading": "PROOF OF PROPOSITION 16", "text": "Lemma 38 Let C \u2208 R and 0 < \u03b2 \u2264 1. The function v : [0,\u221e) given by v(r) = Cr\u03b2 is Ho\u0308lder continuous with modulus of continuity \u03c7(r) = |C|\u03b2r\u03b2 .\nProof [Lemma 38] Noting that |x + y|\u03b2 \u2264 |x|\u03b2 + |y|\u03b2 for any x, y \u2208 R we find with x = Cr1 \u2212 Cr2 and y = Cr2 for any r1, r2 \u2265 0 that |C|r\u03b21 \u2212 |C|r \u03b2 2 \u2264 |C|\u03b2 |r1 \u2212 r2|\u03b2 . Exchanging the roles of r1 and r2 then\nyields \u2223\u2223Cr\u03b21 \u2212 Cr\u03b22 \u2223\u2223 \u2264 |C|\u03b2 |r1 \u2212 r2|\u03b2 .\nProof [Proposition 16] With sa, sb as in the proof of Theorem 15, choose w(r) = min ( C1/\u03b1\u03b1 , M\u2016d( \u00b7 , sb)\u03b1\u2016\u22121q ) r\u03b1\nThen clearly \u2016w(d( \u00b7 , sb))\u2016q \u2264 M by construction. Moreover, w has modulus of continuity \u03c7\u0303(r) \u2264 C\u03b1r\u03b1 by Lemma 38. The result follows from observing that \u2016d( \u00b7 , sb)\u03b1\u2016q \u2264 \u2016D\u03b1S \u2016q = D\u03b1S ."}, {"heading": "PROOF OF PROPOSITION 18", "text": "Proof [Proposition 18] Fix t < \u221e and let \u03b4 > 0. Consider x \u2208 X with := \u222b (B\u2217\u03b4 )\nc x(s)\u00b5(ds) > 0. and define the function \u03ba : R+ \u2192 R+ as \u03ba(u) = sups\u2208(B\u2217u)c Ut(s). Clearly, \u03ba is decreasing, \u03ba(u) < U\n\u2217 for u > 0 by definition of S\u2217, and continuous (by continuity of Ut). We then have that Ut(s) < U\u2217\u2212\u03ba(d(s, S\u2217)) for all s \u2208 S. Let 0 < \u03b4\u2032 < \u03c7\u22121(\u03ba(\u03b4)2 t ) such that \u00b5(B \u2217 \u03b4\u2032) > 0. Such a \u03b4\n\u2032 always exists by Q-regularity of \u00b5. Consider\nx\u0303(s) = x(s)1B\u2217\u03b4 (s) + \u00b5(B\u2217\u03b4\u2032) 1B\u2217 \u03b4\u2032 (s)\nClearly, x\u0303 \u2208 X . Furthermore, (\u2217) := \u222b S \u03b7tUt(v)x\u0303(v)\u00b5(dv)\u2212 hi(x\u0303)\u2212 \u222b S \u03b7kUk(v)x(v)\u00b5(dv) + hi(x)\n=\n\u00b5(B\u2217\u03b4\u2032) \u222b B\u2217 \u03b4\u2032 \u03b7kUk(v)\u00b5(dv)\u2212 \u222b (B\u2217\u03b4 ) c \u03b7tUk(v)x(v)\u00b5(dv)\u2212 (hi(x\u0303)\u2212 hi(x))\n\u2265 \u03b7k(U\u2217 \u2212 t\u03c7(\u03b4\u2032))\u2212 \u03b7t(U\u2217 \u2212 \u03ba(\u03b4))\u2212 (hi(x\u0303)\u2212 hi(x)) \u2265 \u03b7t(\u03ba(\u03b4)\u2212 t\u03c7(\u03b4\u2032))\u2212 (hi(x\u0303)\u2212 hi(x))\n> \u03b7t \u03ba(\u03b4)\n2 t \u2212 (hi(x\u0303)\u2212 hi(x))\nNow hi(x\u0303) \u2212 hi(x) \u2192 0 as i \u2192 \u221e by consistency of (hi)i\u22650. Hence there exists j < \u221e such that (\u2217) > 0 and thus x 6= x\u2217j for all i \u2265 j. Since was arbitrary, this shows that \u222b (B\u2217\u03b4 ) c x \u2217 i (s)\u00b5(ds)\u2192 0 as i\u2192\u221e."}, {"heading": "PROOF OF COROLLARY 19", "text": "Proof [Corollary 19] Let f : S \u2192 R be continuous and bounded, say |f(s)| \u2264 M for all s \u2208 S. Let > 0. Since S is compact, f is uniformly continuous, i.e. \u2203 \u03b4 > 0 such that |f(s) \u2212 f(s\u2217)| < /2 for all s \u2208 B\u2217\u03b4 . By Corollary 18 there exists j <\u221e such that x\u2217i ((B\u2217\u03b4 )c) < 4M for all i > j. Hence\u222b\nS |f(s)\u2212 f(s\u2217)|x\u2217i (s)\u03bb(ds) < /2 \u222b B\u2217\u03b4 x\u2217i (s)\u00b5(ds) + 2M \u222b (B\u2217\u03b4 ) c x\u2217i (s)\u00b5(ds) <\nfor all i > j."}, {"heading": "PROOF OF PROPOSITION 22", "text": "Proof [Proposition 22] This proof uses similar arguments as Theorem 7.2 in Cesa-Bianchi and Lugosi (2006), with modifications to accommodate our more general setting of functions on metric spaces.\nSince player 1 has sublinear (realized) regret, by (21) it suffices to show that\nsup s1\u2208S1\n1\nt t\u2211 \u03c4=1 u(s1, s2\u03c4 ) \u2265 V.\nNow clearly sups1\u2208S1 f(s 1) = supx1\u2208P1 \u222b Si f(s) dx1(s) for any f measurable, thus we may equivalently show that supx1\u2208P1 1 t \u2211t \u03c4=1 \u222b S1 u(s1, s2\u03c4 ) dx 1(s1) \u2265 V . Observe that, for all x1 \u2208 P1,\n1\nt t\u2211 \u03c4=1 \u222b S1 u(s1, s2\u03c4 ) dx 1(s1) = \u222b S1 1 t t\u2211 \u03c4=1 u(s1, s2\u03c4 ) dx 1(s1)\n= \u222b S1 1 t t\u2211 \u03c4=1 (\u222b S2 u(s1, s) d\u03b4s2\u03c4 (s) ) dx1(s1) = u\u0304(x1, x\u03022t )\nwhere x\u03022t (B) := 1 t \u2211t \u03c4=1 1B(s 2 \u03c4 ) for any Borel set B \u2282 S2. Since x\u03022t \u2208 P2 we thus have that\nsup x1\u2208P1 u\u0304(x1, x\u03022t ) \u2265 inf x2\u2208P2 sup x1\u2208P1 u\u0304(x1, x2) = V"}, {"heading": "PROOF OF COROLLARY 23", "text": "Proof [Corollary 23] Using the fact that the payoff of player 2 is the negative of player 1, we have from Theorem 22 and the fact that the game has a value that\nlim inf t\u2192\u221e\n1\nt t\u2211 \u03c4=1 \u2212u(s1\u03c4 , s2\u03c4 ) \u2265 \u2212V\nand thus\nlim sup t\u2192\u221e\n1\nt t\u2211 \u03c4=1 u(s1\u03c4 , s 2 \u03c4 ) \u2264 V\nCombining this with (22) proves (23)."}, {"heading": "PROOF OF THEOREM 24", "text": "In the proof of the theorem we will use the following Lemma:\nLemma 39 The functions g1(x2) := supx1\u2208P1 u\u0304(x 1, x2) and g2(x1) := infx2\u2208P2 u\u0304(x 1, x2) are continuous with respect to the weak topology.\nProof [Lemma 39] It suffices to show that g\u221211 ((\u2212\u221e, a)) and g\u22121((b,\u221e)) are open, since the sets of the form (\u2212\u221e, a) and (b,\u221e) form a subbase for the topology of R. Observe first that u is continuous. Indeed, by Assumption 3, we have for any s, t \u2208 S1 \u00d7 S2 that\n|u(s1, s2)\u2212 u(t1, t2)| \u2264 |u(s1, s2)\u2212 u(s1, t2)|+ |u(s1, t2)\u2212 u(t1, t2)| \u2264 \u03c72(d2(s2, t2)) + \u03c71(d1(s1, t1))\nand so for any > 0 there exists \u03b4 > 0 such that |u(s1, s2) \u2212 u(t1, t2)| < whenever (d1 \u00d7 d2)(s, t) < \u03b4. Since u is continuous on the compact set S1 \u00d7 S2 it is bounded, i.e. there exists M < \u221e such that |u(s1, s2)| \u2264 M for all s \u2208 S. This implies that u\u0304(x1, x2) is 2M -Lipschitz w.r.t the Le\u0301vy-Prokhorov metric\nonP1\u00d7P2, hence in particular (jointly) continuous w.r.t. the weak (product) topology. Let \u03c02 : P1\u00d7P2 \u2192 P2 denote the canonical projection onto P2, which by definition of the product topology is continuous. Together with the continuity of u\u0304 this implies that g\u221211 ((b,\u221e)) = \u03c02 \u25e6 u\u0304\u22121((b,\u221e)) is open. Furthermore, note that u\u0304(x1, x2) < a, \u2200x1 \u2208 P1 whenever g1(x) < a, and hence for any x2 \u2208 P2, the set (x1, x2) \u2208 g\u221211 ((\u2212\u221e, a)) is open. That is, there exists an open cover of P1 \u00d7 {x2}. Now P1 is compact in the weak topology, which means we can find a finite subcover {U jx2} nx2 j=1 such that \u22c2nx2 j=1 U j x2 \u2283 P1 \u00d7 {x2}. Taking the union over all\nx2 \u2208 g\u22121((\u2212\u221e, a)) we have that g\u22121((\u2212\u221e, a)) = \u22c3 x2\u2208g\u22121((\u2212\u221e,a)) \u22c2nx2 j=1 U j x2 , which is an open set. This shows that g1 is continuous. The argument for showing continuity of g2 is essentially the same.\nProof [Theorem 24] Note that both Pi are metrizable and compact in the weak topology (as each Si is compact), and hence P1 \u00d7 P2 by Tychonoff\u2019s theorem. Therefore it suffices to show that with probability 1, the weak limit of any weakly converging subsequence of (x\u0302t)\u221et=0 is a Nash equilibrium. Let (x\u0302 1 \u03b8, x\u0302 2 \u03b8) \u221e \u03b8=1 be such weakly convergent subsequence, and (z1, z2) \u2208 P1 \u00d7 P2 its weak limit. We will show that whenever a given realization of plays (s1t ), (s 2 t ) has sublinear regret for both players, (z 1, z2) is a Nash Equilibrium, i.e.,\nsup x1\u2208P1 u\u0304(x1, z2) = V = inf x2\u2208P2 u\u0304(z1, x2). (41)\nLet g1(x2) := supx1\u2208P1 u\u0304(x 1, x2) and g2(x1) := infx2\u2208P2 u\u0304(x 1, x2), which by Lemma 39 are continuous w.r.t. the weak topology. Hence, using that x\u0302i\u03b8 \u21c0 z i for i = 1, 2, (41) is equivalent to\nlim \u03b8\u2192\u221e sup x1\u2208P1\nu\u0304(x1, x\u03022\u03b8) = V, (42a)\nlim \u03b8\u2192\u221e inf x2\u2208P2\nu\u0304(x\u03021\u03b8, x 2) = V. (42b)\nWe first show (42a). By assumption, the game has value V , i.e. it holds that infx2\u2208P2 supx1\u2208P1 u\u0304(x 1, x2) = V and thus, in particular, that\nlim inf \u03b8\u2192\u221e sup x1\u2208P1\nu\u0304(x1, x\u03022\u03b8) \u2265 V. (43)\nNow, suppose that for a realization (s1\u03c4 ), (s 2 \u03c4 ), the regret of the second player is sublinear, i.e.\nlim sup t\u2192\u221e\n1\nt ( sup x1\u2208P1 t\u2211 \u03c4=1 \u222b S1 u(s1, s2\u03c4 ) dx 1(s1)\u2212 n\u2211 \u03c4=1 u(s1\u03c4 , s 2 \u03c4 ) ) \u2264 0.\nThen by Corollary 23, limt\u2192\u221e 1t \u2211t \u03c4=1 u(s 1 \u03c4 , s 2 \u03c4 ) = V , and we have\nV \u2265 lim sup t\u2192\u221e sup x1\u2208P1\n1\nt t\u2211 \u03c4=1 \u222b S1 u(s1, s2\u03c4 ) dx 1(s1)\n= lim sup t\u2192\u221e sup x1\u2208P1 \u222b S1 1 t t\u2211 \u03c4=1 u(s1, s 2 \u03c4 ) dx 1(s1)\n= lim sup t\u2192\u221e sup x1\u2208P1\nu\u0304(x1, x\u03022t )\n\u2265 lim sup \u03b8\u2192\u221e sup x1\u2208P1 u\u0304(x1, x\u03022\u03b8).\nCombining the last inequality with (43) proves (42a). The argument for (42b) is essentially the same, modulo some sign changes.\nThis proves that for any realization with sublinear regret for both players, all weak limit points of the sequence (x\u03021t , x\u0302 2 t ) lie in the set of Nash equilibria. But by definition of Hannan consistency, this happens with probability 1."}, {"heading": "PROOF OF THEOREM 27", "text": "Proof [Theorem 27] To start, note that for any p > 1 the space X as a closed subset of Lp(S, \u00b5) is a complete metric space, hence Polish and thus there exists a Borel isomorphism between X and the Lebesgue measure on the unit interval. Consequently, to randomize its plays according to a sequence of probability measures in X , it suffices that player i has access to a sequence of i.i.d. random variables drawn from the uniform distribution on [0, 1]. Denote this sequence by Zi = (Zi1, Z i 2, . . . ).\nThe key observation is that if player \u2212i plays a non-oblivious strategy, then the partial rewards will not be some a priori fixed sequence of reward functions, but will depend on the history of play. Indeed, since u\u0303it( \u00b7 ) = \u2211t \u03c4=1 ui( \u00b7 , s\u2212i\u03c4 ) and since s\u2212i\u03c4 is itself some function of past plays si1, . . . , si\u03c4\u22121, the partial reward functions u\u0303it are measurable w.r.t. the \u03c3 field generated by (Z i 1, . . . , Z i t). Note that this implicitly assumes\nthat any randomization performed by player \u2212i is independent of that of player i. Let Eit[X] := E [ X |\nZi1, . . . , Z i t\u22121 ] denote the conditional expectation of X given the past plays of player i. Then\nt\u2211 \u03c4=1 ui(si, s\u2212i\u03c4 )\u2212 t\u2211 \u03c4=1 Ei\u03c4 [ ui(si\u03c4 , s \u2212i \u03c4 ) ] \u2264 t\u2211 \u03c4=1 sup s\u2212i\u03c4 Ei\u03c4 [ ui(s i, s\u2212i\u03c4 )\u2212 ui(si\u03c4 , s\u2212i\u03c4 ) ]\n= t\u2211 \u03c4=1 sup u\u0303i\u03c4 Ei\u03c4 [ u\u0303i\u03c4 (s i)\u2212 u\u0303i\u03c4 (si\u03c4 ) ]\n= sup u\u0303i1,...,u\u0303 i t t\u2211 \u03c4=1 Ei\u03c4 [ u\u0303i\u03c4 (s i)\u2212 u\u0303i\u03c4 (si\u03c4 ) ]\n(44)\nwhere the last step uses the fact that si\u03c4 \u223c xi\u03c4 := Dh\u2217i ( \u03b7\u03c4\u22121 \u2211t\u22121 \u03b8=1 u\u0303 i \u03b8 ) , which depends on the sequence {si\u03b8} \u03c4\u22121 \u03b8=1 only through the sequence {u\u0303i\u03b8} \u03c4\u22121 \u03b8=1 of observed partial loss functions.\nFrom Proposition 11 we have that\nRt = sup si\u2208Si sup u\u0303i1,...,u\u0303 i t t\u2211 \u03c4=1 u\u0303i\u03c4 (s i)\u2212 t\u2211 \u03c4=1 \u3008u\u0303\u03c4i , xi\u03c4 \u3009 = sup si\u2208Si sup u\u0303i1,...,u\u0303 i t t\u2211 \u03c4=1 Ei\u03c4 [ u\u0303i\u03c4 (s i)\u2212 u\u0303i\u03c4 (si\u03c4 ) ]\n(45)\nNow let W i\u03c4 = u\u0303 i \u03c4 (s i \u03c4 )\u2212 \u3008u\u0303i\u03c4 , xi\u03c4 \u3009 and observe that W i\u03c4 is a martingale. Indeed,\nE[W i\u03c4 |W i\u03c4 , . . . ,W \u03c4\u22121i ] = E[W i \u03c4 | Z\u03c4i , . . . , Z\u03c4\u22121i ] = 0 a.s.\nMoreover, since by assumption ui is continuous on the compact set S1 \u00d7 S2, we have that ui is bounded and therefore |W i\u03c4 \u2212W i\u03c4\u22121| \u2264M for some M <\u221e. Noting that W i\u03c4 = 0 it follows from the Azuma-Hoeffding inequality that, for every > 0, P(W i\u03c4 \u2264 ) \u2265 1\u2212 exp(\u2212 2 2\u03c4M2 ) and thus\nP (\u2211t\n\u03c4=1W i \u03c4 \u2264M\n\u221a 2t log(t/ ) ) \u2265 1\u2212 \u2200 > 0\nNow \u2211t \u03c4=1W i \u03c4 = \u2211t \u03c4=1 u\u0303 i \u03c4 (s i \u03c4 ) \u2212 \u2211t \u03c4=1\u3008u\u0303i\u03c4 , xi\u03c4 \u3009, and hence, using (45) and (44), we have for all t < \u221e that\nsup si\u2208Si\n1\nt ( t\u2211 \u03c4=1 ui(s i, s\u2212i\u03c4 )\u2212 t\u2211 \u03c4=1 ui(s i \u03c4 , s \u2212i \u03c4 ) ) \u2264 Rt t +M \u221a 2 log(t/ ) t\nNowRt/t\u2192 0 by assumption, and \u221a log(t/ ) t \u2192 0 for any > 0, which proves Hannan consistency."}], "references": [], "referenceMentions": [], "year": 2016, "abstractText": "<lb>We study a general version of the adversarial online learning problem. We are given a decision<lb>set X in a reflexive Banach space X and a sequence of reward vectors in the dual space of X . At<lb>each iteration, we choose an action from X , based on the observed sequence of previous rewards.<lb>Our goal is to minimize regret, defined as the gap between the realized reward and the reward<lb>of the best fixed action in hindsight. Using results from infinite dimensional convex analysis, we<lb>generalize the method of Dual Averaging (or Follow the Regularized Leader) to our setting and<lb>obtain general upper bounds on the worst-case regret that subsume a wide range of results from the<lb>literature. Under the assumption of uniformly continuous rewards, we obtain explicit anytime regret<lb>bounds in a setting where the decision set is the set of probability distributions on a compact metric<lb>space S whose Radon-Nikodym derivatives are elements of L(S) for some p > 1. Importantly,<lb>we make no convexity assumptions on either the set S or the reward functions. We also prove<lb>a general lower bound on the worst-case regret for any online algorithm. We then apply these<lb>results to the problem of learning in repeated continuous two-player zero-sum games, in which<lb>players\u2019 strategy sets are compact metric spaces. In doing so, we first prove that if both players<lb>play a Hannan-consistent strategy, then with probability 1 the empirical distributions of play weakly<lb>converge to the set of Nash equilibria of the game. We then show that, under mild assumptions,<lb>Dual Averaging on the (infinite-dimensional) space of probability distributions indeed achieves<lb>Hannan-consistency. Finally, we illustrate our results through numerical examples.<lb>", "creator": "LaTeX with hyperref package"}}}