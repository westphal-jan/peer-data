{"id": "1606.01341", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "4-Jun-2016", "title": "Neural Architectures for Fine-grained Entity Type Classification", "abstract": "In this work, we investigate several neural network architectures for fine-grained entity type classification. Particularly, we consider extensions to a recently proposed attentive neural architecture and make three key contributions. Previous work on attentive neural architectures do not consider hand-crafted features, we combine learnt and hand-crafted features and observe that they complement each other. Additionally, through quantitative analysis we establish that the attention mechanism is capable of learning to attend over syntactic heads and the phrase containing the mention, where both are known strong hand-crafted features for our task. We enable parameter sharing through a hierarchical label encoding method, that in low-dimensional projections show clear clusters for each type hierarchy. Lastly, despite using the same evaluation dataset, the literature frequently compare models trained using different data. We establish that the choice of training data has a drastic impact on performance, with decreases by as much as 9.85% loose micro F1 score for a previously proposed method. Despite this, our best model achieves state-of-the-art results with 75.36% loose micro F1 score on the well- established FIGER (GOLD) dataset.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n", "histories": [["v1", "Sat, 4 Jun 2016 07:52:22 GMT  (273kb,D)", "http://arxiv.org/abs/1606.01341v1", "10 pages, 3 figures"], ["v2", "Tue, 21 Feb 2017 06:49:42 GMT  (532kb,D)", "http://arxiv.org/abs/1606.01341v2", "10 pages, 3 figures, accepted at EACL2017 conference"]], "COMMENTS": "10 pages, 3 figures", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["sonse shimaoka", "pontus stenetorp", "kentaro inui", "sebastian riedel"], "accepted": false, "id": "1606.01341"}, "pdf": {"name": "1606.01341.pdf", "metadata": {"source": "CRF", "title": "Neural Architectures for Fine-grained Entity Type Classification", "authors": ["Sonse Shimaoka", "Pontus Stenetorp", "Kentaro Inui", "Sebastian Riedel"], "emails": ["simaokasonse@ecei.tohoku.ac.jp", "inui@ecei.tohoku.ac.jp", "p.stenetorp@cs.ucl.ac.uk", "s.riedel@cs.ucl.ac.uk"], "sections": [{"heading": "1 Introduction", "text": "Entity type classification aims to label entity mentions in their context with their respective semantic types. Information regarding entity type men-\n\u2217This work was conducted during a research visit to University College London.\ntions have proven to be valuable for several natural language processing tasks; such as question answering (Lee et al., 2006), knowledge base population (Carlson et al., 2010), and co-reference resolution (Recasens et al., 2013). A natural extension to traditional entity type classification has been to divide the set of types \u2013 which may be too coarsegrained for some applications (Sekine, 2008) \u2013 into a larger set of fine-grained entity types (Lee et al., 2006; Ling and Weld, 2012; Yosef et al., 2012; Gillick et al., 2014; Del Corro et al., 2015); for example person into actor, artist, etc.\nIn a recent workshop paper, Shimaoka et al. (2016) proposed a neural model with an attention mechanism that enabled the model to focus on informative words and phrases in the context\nar X\niv :1\n60 6.\n01 34\n1v 1\n[ cs\n.C L\n] 4\nJ un\n2 01\nof a type mention (Figure 1). They reported state-of-the-art performance for the well-established FIGER (GOLD) (Ling and Weld, 2012) dataset and that their attention mechanism captured relevant contextual linguistic expressions. Given these promising preliminary results, we build upon their work and address several short-comings with their and other previous work on fine-grained entity recognition; namely the empirical evaluation, limited model variations, and qualitative analysis of the attention mechanism.\nOur contributions are three-fold:\n1. Previous work on attentive neural architectures do not consider hand-crafted features. We combine learnt and hand-crafted features and observe that they complement each other. Additionally, we perform extensive analysis of the attention mechanism of our model and establish that the attention mechanism is capable of learning to attend over syntactic heads and the tokens prior to and after a mention, both which are highly relevant to successfully classifying a mention.\n2. We investigate the possibility of parameter sharing by using a hierarchical encoding of labels that improves performance on one of our datasets and that the low-dimensional projections of the embedded labels form clear clusters.\n3. While research on fine-grained entity type classification has settled on using two evaluation datasets, a wide variety of training datasets are used. We find that the choice of training data has a drastic impact on performance, observing performance decreases by as much as 9.85% Loose Micro F1 score for a previously proposed method. However, even when comparing to models trained using different datasets we report state-of-the-art results of 75.36 loose micro F1 score on the FIGER (GOLD) dataset."}, {"heading": "2 Related Work", "text": "Our work draws upon two primary strains of research, fine-grained entity classification and attention mechanisms for neural models. In this section we will introduce both of these research directions.\nBy expanding a set of coarse-grained types into a set of 147 fine-grained types, Lee et al. (2006) were the first to address the task of fine-grained entity classification. Their end goal was to use the resulting types in a question answering system and they developed a conditional random field model that they trained and evaluated on a manually annotated Korean dataset to detect and classify entity mentions. Other early work include Sekine (2008), that emphasized the need for having access to a large set of entity types for several NLP applications.\nThe first work to use the subsequently increasingly popular approach of using distant supervision (Mintz et al., 2009) to induce a large, but noisy, training set and manually label a significantly smaller dataset to evaluate their fine-grained entity classification system, was Ling and Weld (2012) who introduced both a training and evaluation dataset FIGER (GOLD). Arguing that finegrained sets of types must be organized in a very fine-grained hierarchical taxonomy, Yosef et al. (2012) introduced such a taxonomy covering 505 distinct types. This new set of types lead to improvements on FIGER (GOLD), and they also demonstrated that the fine-grained labels could be used as features to improve coarse-grained entity type classification performance. More recently, continuing this very fine-grained strategy, Del Corro et al. (2015) introduced the most fine-grained entity type classification system to date, covering the more than 16, 000 types contained in the WordNet hierarchy.\nWhile initial work largely assumed that mention assignments could be done independently of the mention context, Gillick et al. (2014) introduced the concept of context-dependent fine-grained entity type classification where the types of a mention is constrained to what can be deduced from its context and introduced a new OntoNotes-derived manually annotated evaluation dataset. In addition, they addressed the problem of label noise induced by distant supervision and proposed three label cleaning heuristics. Extending upon the noise reduction aspects of this work, Ren et al. (2016) introduced a method to reduce label noise even further, leading to significant performance gains on both the evaluation dataset of Ling and Weld (2012) and Gillick et al. (2014).\nYogatama et al. (2015) proposed to map hand-\ncrafted features and labels to embeddings in order to facilitate information sharing between both related types and features. A pure feature learning approach was proposed by Dong et al. (2015). They defined 22 types and used a two-part neural classifier that used a recurrent neural network to obtain a vector representation of each entity mention and in its second part used a fixed-size window to capture the context of a mention. Most recently, Shimaoka et al. (2016) introduced an attentive neural model that unlike previous work obtained vector representations of each mention context by composing it using a recurrent neural network and employed an attention mechanism to allow the model to focus on relevant expressions in the mention context. Although not pointed out by Shimaoka et al. (2016), their attention mechanism differs from the ones used in previous work since it does not condition the attention.\nTo the best of our knowledge, the first work that utilized an attention architecture within the context of NLP was Bahdanau et al. (2014), that allowed a machine translation decoder to attend over the source sentence. Doing so, they showed that adding the attention mechanism significantly improved their machine translation results as the model was capable of learning to align the source and target sentences. Moreover, in their qualitative analysis, they concluded that the model can correctly align mutually related words and phrases. For the set of neural models proposed by Hermann et al. (2015), attention mechanisms are used to focus on the aspects of a document that help the model answer a question, as well as providing a way to qualitatively analyze the inference process.\nRather, they used global weights optimised to provide attention for every fine-grained entity type classification decision. Our work differs from previous work on fine-grained entity classification in that we use the same publicly available training data when comparing models. We also believe that we are the first to consider the direct combination of handcrafted features and an attentive neural model."}, {"heading": "3 Models", "text": "In this section, we describe the models from Shimaoka et al. (2016), our extensions, as well as a strong feature-based baseline from the literature. We\npose fine-grained entity classification as a multiclass, multi-label classification problem. Given a mention in a sentence, the classifier predicts the types t \u2208 {1, 0}K where K is the size of the set of types. Across all the models, we compute a probability yk \u2208 R for each of the K types using logistic regression. Variations of the models stem from the ways of computing the input to the logistic regression.\nAt inference time, we enforce the assumption that at least one type is assigned to each mention by first assigning the type with the largest probability. We then assign any additional types based on the condition that their corresponding probabilities must be greater than a threshold of 0.5."}, {"heading": "3.1 Sparse Feature Model", "text": "For each entity mention m, we create a binary feature indicator vector f(m) \u2208 {0, 1}Df and feed it to the logistic regression layer. The features used are described in Table 1, which are similar to those used by Gillick et al. (2014) and Yogatama et al. (2015). It is worth noting that we aimed for this model to resemble the independent classifier model in Gillick et al. (2014) so that it constitutes as a meaningful well-established baseline; however, there are two noteworthy differences. Firstly, we use the more widely-established clustering method of Brown et al. (1992), as opposed to Uszkoreit and Brants (2008), as Gillick et al. (2014) did not make\nthe data used for their clusters publicly available. Secondly, we learned a set of 15 topics from the OntoNotes dataset using the LDA (Blei et al., 2003) implementation from the popular gensim1 software package, in contrast to Gillick et al. (2014) that used a supervised topic model trained on an unspecified dataset. Despite these differences, we argue that our set of features is comparable."}, {"heading": "3.2 Neural Models", "text": "The neural models proposed by Shimaoka et al. (2016) processes embeddings of the words of the mention and its context; and we adopt a similar formalism when introducing their models and our variants. First, the mention representation vm \u2208 RDm\u00d71 and context representation vc \u2208 RDc\u00d71 are computed separately. Then, the concatenation of these representations is used to compute the prediction:\ny = 1\n1 + exp ( \u2212Wy [ vm vc ]) (1) Where Wy \u2208 RK\u00d7(Dm+Dc) is the weight matrix. Let the words in the mention be m1,m2, ...,m|m|. Then the representation of the mention is computed as follows:\nvm = 1\n|m| |m|\u2211 i=1 u(mi) (2)\nWhere u is a mapping from a word to an embedding. As pointed out by Shimaoka et al. (2016), the reason for applying this relatively naive method to obtain the mention representation is that more complex models tend to suffer from severe overfitting on the training data.\nNext, we describe the three methods from Shimaoka et al. (2016) for computing the context representations; namely, Averaging, LSTM, and Attentive Encoder."}, {"heading": "3.2.1 Averaging Encoder", "text": "Similarly to the method of computing the mention representation, the Averaging encoder computes the averages of the words in the left and right context. Formally, let l1, ..., lC and r1, ..., rC be the words in\n1http://radimrehurek.com/gensim/\nthe left and right contexts respectively, where C is the window size. Then, for each sequence of words, we compute the average of the corresponding word embeddings. Those two vectors are then concatenated to form the representation of the context vc."}, {"heading": "3.2.2 LSTM Encoder", "text": "For the LSTM Encoder, the left and right contexts are encoded by an LSTM (Hochreiter and Schmidhuber, 1997). The high-level formulation of an LSTM is as follows:\nhi, si = lstm(ui, hi\u22121, si\u22121) (3)\nWhere ui \u2208 RDm\u00d71 is an input embedding, hi\u22121 \u2208 RDh\u00d71 is the previous output, and si\u22121 \u2208 RDh\u00d71 is the previous cell state.\nFor the left context, the LSTM is applied to the sequence l1, ..., lC from left to right and produces the outputs \u2212\u2192 hl1, ..., \u2212\u2192 hlC . For the right context, the sequence rC , ..., r1 is processed from right to left to produce the outputs \u2190\u2212 hr1, ..., \u2190\u2212 hrC . The concatenation of \u2212\u2192 hlC and \u2190\u2212 hr1 then serves as the context representation vc."}, {"heading": "3.2.3 Attentive Encoder", "text": "An attention mechanism aims to encourage the model to focus on salient local information that is relevant for the classification decision. The attention mechanism variant used in this work is defined as follows. First, bi-directional LSTMs (Graves, 2012) are applied for both the right and left context. We denote the output layers of the bi-directional LSTMs as \u2212\u2192 hl1, \u2190\u2212 hl1, ..., \u2212\u2192 hlC , \u2190\u2212 hlC and \u2212\u2192 hr1, \u2190\u2212 hr1, ..., \u2212\u2192 hrC , \u2190\u2212 hrC .\nFor each output layer, a scalar value a\u0303i \u2208 R is computed using a feed forward neural network with the hidden layer ei \u2208 RDa\u00d71 and weight matrices We \u2208 RDa\u00d72Dh and Wa \u2208 R1\u00d7Da :\neli = tanh ( We [ \u2212\u2192 hli\u2190\u2212 hli ]) (4) a\u0303li = exp(Wae l i) (5)\nNext, the scalar values are normalized such that they sum to 1:\nali = a\u0303li\u2211C\ni=1 a\u0303 l i + a\u0303 r i\n(6)\nThese normalized scalar values ai \u2208 R are referred to as attentions. Finally, we compute the sum of the output layers of the bidirectional LSTMs, weighted by the attentions ai as the representation of the context:\nvc = C\u2211 i=1 ali [ \u2212\u2192 hli\u2190\u2212 hli ] + ari [ \u2212\u2192 hri\u2190\u2212 hri ] (7)\nAn illustration of the attentive encoder model variant can be found in Figure 1."}, {"heading": "3.3 Hybrid Models", "text": "To allow model variants to use both human background knowledge through hand-crafted features as well as features learnt from data, we extended the neural models to create new hybrid model variants as follows. Let vf \u2208 RDl\u00d71 be a low-dimensional projection of the sparse feature f(m):\nvf = Wff(m) (8)\nWhere Wf \u2208 RDl\u00d7Df is a projection matrix. The hybrid model variants are then defined as follows:\ny = 1\n1 + exp \u2212Wy  vmvc\nvf\n (9)\nThese models can thus draw upon learnt features through vm and vc as well as hand-crafted features using vf when making classification decisions. While existing work on fine-grained entity type classification have used either sparse, manually designed features or dense, automatically learned embedding vectors, our work is the first to propose and evaluate a model using the combination of both features."}, {"heading": "3.4 Hierarchical Label Encoding", "text": "Since the fine-grained types tend to form a forest of type hierarchies (e.g. musician is a subtype of artist, which in turn is a subtype of person), we investigated whether the encoding of each label could utilise this structure to enable parameter sharing. Concretely, we compose the weight matrix Wy for the logistic regression layer as the product of a learnt weight matrix Vy and a constant sparse binary matrix S:\nW Ty = VyS (10)\nWe encode the type hierarchy formed by the set of types in the binary matrix S as follows. Each type is mapped to a unique column in S, where membership at each level of its type hierarchy is marked by a 1. For example, if we use the set of types defined by Gillick et al. (2014), the column for /person could be encoded as [1, 0, . . .], /person/artist as [1, 1, 0, . . .], and /person/artist/actor as [1, 1, 1, 0, . . .]. This encoding scheme is illustrated in Figure 2.\nThis enables us to share parameters between labels in the same hierarchy, potentially making learning easier for infrequent types that can now draw upon annotations of other types in the same hierarchy."}, {"heading": "4 Experiments", "text": ""}, {"heading": "4.1 Datasets", "text": "Despite the research community having largely settled on using the manually annotated datasets FIGER (GOLD) (Ling and Weld, 2012) and OntoNotes (Gillick et al., 2014) for evaluation, there\nis still a remarkable difference in the data used to train models (Table 2) that are then evaluated on the same manually annotated datasets. Also worth noting is that some data is not even publicly available, making a fair comparison between methods even more difficult. For evaluation, in our experiments we use the two well-established manually annotated datasets FIGER (GOLD) and OntoNotes, where like Gillick et al. (2014), we discarded pronominal mentions, resulting in a total of 8, 963 mentions. For training, we use the automatically induced publicly available datasets provided by Ren et al. (2016). Ren et al. (2016) aimed to eliminate label noise generated in the process of distant supervision and we use the \u201craw\u201d noisy data2 provided by them for training our models."}, {"heading": "4.2 Pre-trained Word Embeddings", "text": "We use pre-trained word embeddings that were not updated during training to help the model generalize for words not appearing in the training set (Rockta\u0308schel et al., 2015). For this purpose, we used the freely available 300-dimensional cased word embeddings trained on 840 billion tokens from the Common Crawl supplied by Pennington et al. (2014). For words not present in the pre-trained word embeddings, we use the embedding of the \u201cunk\u201d token."}, {"heading": "4.3 Evaluation Criteria", "text": "We adopt the same criteria as Ling and Weld (2012), that is, we evaluate the model performance by strict accuracy, loose macro, and loose micro scores."}, {"heading": "4.4 Hyperparameter Settings", "text": "Values for the hyperparameters were obtained from preliminary experiments by evaluating the model performance on the development sets. Concretely, all neural and hybrid models used the same Dm = 300-dimensional word embeddings, the hidden-size of the LSTM was set to Dh = 100, the hidden-layer size of the attention module was set to Da = 100,\n2 Although Ren et al. (2016) provided both \u201craw\u201d data and a pipeline to \u201cdenoise\u201d the data, we were unable to replicate the performance benefits reported in their work after running their pipeline. We have contacted them regarding this as we would be interested in comparing the benefit of their denoising algorithm for each model, but at the time of writing we have not yet received a response.\nand the size of the low-dimensional projection of the sparse features was set to Dl = 50. We used Adam (Kingma and Ba, 2014) as our optimization method with a learning rate of 0.001, a mini-batch size of 1, 000, and iterated over the training data for five epochs. As a regularizer we used dropout (Hinton et al., 2012) with probability 0.5 applied to the mention representation and sparse feature representation. The context window size was set to C = 10 and if the length of a context extends beyond the sentence length, we use a padding symbol in-place of a word. After training, we picked the best model on the development set as our final model and report their performance on the test sets."}, {"heading": "4.5 Results", "text": "When presenting our results, it should be noted that we aim to make a clear separation between results from models trained using different datasets."}, {"heading": "4.5.1 FIGER (GOLD)", "text": "We first analyze the results on FIGER (GOLD) (Tables 3 and 4). The performance of the baseline model that uses the sparse hand-crafted features is relatively close to that of FIGER system of Ling and Weld (2012). This is consistent with the fact that\nboth systems use linear classifiers, similar sets of features, and training data of the same size and domain.\nLooking at the results of neural models, we observe a consistent pattern that adding hand-crafted features boosts performance significantly, indicating that the learnt and hand-crafted features complement each other. The effects of adding the hierarchical label encoding were inconsistent, sometimes increasing, sometimes decreasing performance. We thus opted not to include them in the results table due to space constraints and hypothesize that given the size of the training data, parameter sharing may not yield any large performance benefits. Among the neural models, we see that averaging encoder perform considerably worse than the others. Both the LSTM and attentive encoder show strong results and the attentive encoder with hand-crafted features achieves the best performance among the all models we investigated.\nWhen comparing our best model to previously introduced models trained using different training data, we find that we achieve state-of-the-art results both in terms of loose macro and micro scores. The closest competitor, FIGER + PLE, achieves higher accuracy at the expense of lower F1 scores, we suspect that this is due to an accuracy focus in their label pruning strategy. It is also worth noting that we achieve our state-of-the-art results without the need for any noise reduction strategies.\nIn regards to the impact of the choice of training set, we observe that the model introduced by Shimaoka et al. (2016) drops as much as 3.36 points of loose micro score when using a smaller dataset. Thus casting doubts upon the interpretability of results of fine-grained entity classification models using different training data."}, {"heading": "4.5.2 OntoNotes", "text": "Secondly, we discuss the results on OntoNotes (Tables 5, and 6). Once again, we see consistent performance improvements when the sparse handcrafted features are added to the neural models. In the absence of hand-crafted features, the averaging encoder suffer relatively poor performance and the attentive encoder achieves the best performance. However, when the hand-crafted features are added, a significant improvement occurs for the averaging\nencoder, making the performance of the three neural models much alike. As a reason of these results, we speculate that some of the hand-crafted features such as the dependency role and parent word of the head noun, provide crucial information for the task that cannot be captured by the plain averaging model, but can be learnt if an attention mechanism is present. Another speculative reason is that because the training dataset is noisy compared to FIGER (GOLD) (since FIGER (GOLD) uses anchors to detect entities whereas OntoNotes uses an external tool to detect entities), and the size of the dataset is small, the robustness of the simpler averaging model becomes clearer when combined with the hand-crafted features.\nAnother interesting observation can be found in models with the hierarchical label encoding, where it is clear that consistent performance increases occur. This can be explained by the fact that the type ontology used in OntoNotes is more well-formed than its FIGER counterpart. While we do not attain state-of-the-art performance when considering models using different training data. But we do note that in terms of F1-scores we perform within 1 point of the state of the art, despite having trained our models on noisy data.\nOnce again we have an opportunity to study the impact of the choice of training data by comparing the results of the hand-crafted features of Gillick et\nal. (2014) to our own set of corresponding features. What we find is that the performance drop is very dramatic, 9.85 points of loose micro score. Given that the training data for the previously introduced model is not publicly available, we hesitate to speculate as to exactly why this drop is so dramatic. But it clearly underlines how essential it is to compare models on an equal footing using the same training data."}, {"heading": "4.6 PCA visualization of label embeddings", "text": "By visualizing the learnt label embeddings (Figure 3) and comparing the non-hierarchical and hierarchical label encodings, we can observe that the hierarchical encoding forms clear distinct clusters."}, {"heading": "4.7 Attention Analysis", "text": "While visualizing the attention weights for specific examples have become commonplace, it is still not\nalways clear exactly what syntactic and semantic patterns that are learnt by the attention mechanism. To better understand this, we first analyzed a large set of attention visualizations and observed that head words and the words contained in the phrase forming the mention tended to have the highest level of attention. In order to quantify this notion, we calculated how frequently the word strongest attended over for all mentions of a specific type was the syntactic head or the words before and after the mention in its phrase. What we found through our analysis (Table 7) was that our attentive model without handcrafted features does indeed learn that head words and the phrase surrounding the mention are highly indicative of the mention type, without any explicit supervision. Furthermore, we believe that this in part might explain why the performance benefit of adding hand-crafted features was smaller for the attentive model compared to our other two neural variants."}, {"heading": "5 Conclusions and Future Work", "text": "In this paper, we proposed a novel state-of-the-art neural network architecture with an attention mechanism for the task of fine-grained entity type classification. We also demonstrated that the model can successfully learn to attend over expressions that are important for the classification of fine-grained types. As future work, we see the re-implementation of more methods from the literature as a desirable target, so that they can be evaluated after utilizing the same training data. Upon acceptance we will publish our code and experimental pipeline to enable reproducability of our work."}, {"heading": "Acknowledgments", "text": "This work was supported by CREST-JST, JSPS KAKENHI Grant Number 15H01702, a Marie Curie\nCareer Integration Award, and an Allen Distinguished Investigator Award. We would like to thank Dan Gillick for answering several questions related to his 2014 paper."}], "references": [{"title": "Kyunghyun Cho", "author": ["Dzmitry Bahdanau"], "venue": "and Yoshua Bengio.", "citeRegEx": "Bahdanau et al.2014", "shortCiteRegEx": null, "year": 2014}, {"title": "2003", "author": ["David M Blei", "Andrew Y Ng", "Michael I Jordan"], "venue": "Latent dirichlet allocation. the Journal of machine Learning research, 3:993\u2013", "citeRegEx": "Blei et al.2003", "shortCiteRegEx": null, "year": 1022}, {"title": "Della Pietra", "author": ["P.F. Brown", "P.V. Desouza", "R.L. Mercer", "V.J"], "venue": "and J.C. Lai.", "citeRegEx": "Brown et al.1992", "shortCiteRegEx": null, "year": 1992}, {"title": "Estevam R Hruschka Jr", "author": ["Andrew Carlson", "Justin Betteridge", "Richard C Wang"], "venue": "and Tom M Mitchell.", "citeRegEx": "Carlson et al.2010", "shortCiteRegEx": null, "year": 2010}, {"title": "Rainer Gemulla", "author": ["Luciano Del Corro", "Abdalghani Abujabal"], "venue": "and Gerhard Weikum.", "citeRegEx": "Del Corro et al.2015", "shortCiteRegEx": null, "year": 2015}, {"title": "Ming Zhou", "author": ["Li Dong", "Furu Wei", "Hong Sun"], "venue": "and Ke Xu.", "citeRegEx": "Dong et al.2015", "shortCiteRegEx": null, "year": 2015}, {"title": "Jesse Kirchner", "author": ["Dan Gillick", "Nevena Lazic", "Kuzman Ganchev"], "venue": "and David Huynh.", "citeRegEx": "Gillick et al.2014", "shortCiteRegEx": null, "year": 2014}, {"title": "Supervised sequence labelling", "author": ["Alex Graves"], "venue": null, "citeRegEx": "Graves.,? \\Q2012\\E", "shortCiteRegEx": "Graves.", "year": 2012}, {"title": "Mustafa Suleyman", "author": ["Karl Moritz Hermann", "Tomas Kocisky", "Edward Grefenstette", "Lasse Espeholt", "Will Kay"], "venue": "and Phil Blunsom.", "citeRegEx": "Hermann et al.2015", "shortCiteRegEx": null, "year": 2015}, {"title": "Ilya Sutskever", "author": ["Geoffrey E Hinton", "Nitish Srivastava", "Alex Krizhevsky"], "venue": "and Ruslan R Salakhutdinov.", "citeRegEx": "Hinton et al.2012", "shortCiteRegEx": null, "year": 2012}, {"title": "Long short-term memory", "author": ["Hochreiter", "Schmidhuber1997] Sepp Hochreiter", "J\u00fcrgen Schmidhuber"], "venue": "Neural computation,", "citeRegEx": "Hochreiter et al\\.,? \\Q1997\\E", "shortCiteRegEx": "Hochreiter et al\\.", "year": 1997}, {"title": "Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980", "author": ["Kingma", "Ba2014] Diederik Kingma", "Jimmy Ba"], "venue": null, "citeRegEx": "Kingma et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Kingma et al\\.", "year": 2014}, {"title": "Ji-Hyun Wang", "author": ["Changki Lee", "Yi-Gyu Hwang", "Hyo-Jung Oh", "Soojong Lim", "Jeong Heo", "Chung-Hee Lee", "Hyeon-Jin Kim"], "venue": "and Myung-Gil Jang.", "citeRegEx": "Lee et al.2006", "shortCiteRegEx": null, "year": 2006}, {"title": "Fine-grained entity recognition", "author": ["Ling", "Weld2012] Xiao Ling", "Daniel S Weld"], "venue": "In In Proc. of the 26th AAAI Conference on Artificial Intelligence. Citeseer", "citeRegEx": "Ling et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Ling et al\\.", "year": 2012}, {"title": "Rion Snow", "author": ["Mike Mintz", "Steven Bills"], "venue": "and Dan Jurafsky.", "citeRegEx": "Mintz et al.2009", "shortCiteRegEx": null, "year": 2009}, {"title": "Richard Socher", "author": ["Jeffrey Pennington"], "venue": "and Christopher D Manning.", "citeRegEx": "Pennington et al.2014", "shortCiteRegEx": null, "year": 2014}, {"title": "Marie-Catherine de Marneffe", "author": ["Marta Recasens"], "venue": "and Christopher Potts.", "citeRegEx": "Recasens et al.2013", "shortCiteRegEx": null, "year": 2013}, {"title": "Heng Ji", "author": ["Xiang Ren", "Wenqi He", "Meng Qu", "Clare R Voss"], "venue": "and Jiawei Han.", "citeRegEx": "Ren et al.2016", "shortCiteRegEx": null, "year": 2016}, {"title": "Tom\u00e1\u0161 Ko\u010disk\u1ef3", "author": ["Tim Rockt\u00e4schel", "Edward Grefenstette", "Karl Moritz Hermann"], "venue": "and Phil Blunsom.", "citeRegEx": "Rockt\u00e4schel et al.2015", "shortCiteRegEx": null, "year": 2015}, {"title": "Extended named entity ontology with attribute information", "author": ["Satoshi Sekine"], "venue": "In LREC,", "citeRegEx": "Sekine.,? \\Q2008\\E", "shortCiteRegEx": "Sekine.", "year": 2008}, {"title": "Kentaro Inui", "author": ["Sonse Shimaoka", "Pontus Stenetorp"], "venue": "and Sebastian Riedel.", "citeRegEx": "Shimaoka et al.2016", "shortCiteRegEx": null, "year": 2016}, {"title": "Distributed word clustering for large scale class-based language modeling in machine translation", "author": ["Uszkoreit", "Brants2008] Jakob Uszkoreit", "Thorsten Brants"], "venue": "In Proceedings of ACL08: HLT,", "citeRegEx": "Uszkoreit et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Uszkoreit et al\\.", "year": 2008}, {"title": "Dan Gillick", "author": ["Dani Yogatama"], "venue": "and Nevena Lazic.", "citeRegEx": "Yogatama et al.2015", "shortCiteRegEx": null, "year": 2015}, {"title": "Marc Spaniol", "author": ["Mohamed Amir Yosef", "Sandro Bauer", "Johannes Hoffart"], "venue": "and Gerhard Weikum.", "citeRegEx": "Yosef et al.2012", "shortCiteRegEx": null, "year": 2012}], "referenceMentions": [], "year": 2016, "abstractText": "In this work, we investigate several neural network architectures for fine-grained entity type classification. Particularly, we consider extensions to a recently proposed attentive neural architecture and make three key contributions. Previous work on attentive neural architectures do not consider hand-crafted features, we combine learnt and hand-crafted features and observe that they complement each other. Additionally, through quantitative analysis we establish that the attention mechanism is capable of learning to attend over syntactic heads and the phrase containing the mention, where both are known strong hand-crafted features for our task. We enable parameter sharing through a hierarchical label encoding method, that in low-dimensional projections show clear clusters for each type hierarchy. Lastly, despite using the same evaluation dataset, the literature frequently compare models trained using different data. We establish that the choice of training data has a drastic impact on performance, with decreases by as much as 9.85% loose micro F1 score for a previously proposed method. Despite this, our best model achieves state-of-the-art results with 75.36% loose micro F1 score on the wellestablished FIGER (GOLD) dataset.", "creator": "LaTeX with hyperref package"}}}