{"id": "1606.01467", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "5-Jun-2016", "title": "Deep Q-Networks for Accelerating the Training of Deep Neural Networks", "abstract": "In this paper, we show that by feeding the weights of a deep neural network (DNN) during training into a deep Q-network (DQN) as its states, this DQN can learn policies to accelerate the training of that DNN. The actions of the DQN modify different hyperparameters during training. Empirically, this acceleration leads to better generalization performance of the DNN during training. A further investigation of these effects can be found by comparing different neural networks on the same training period from the same time frame, and using these two-dimensional datasets to investigate the effects of the effects of the DQN and the time-based learning time. We are confident that this is the first empirical study that shows that neural networks can be trained through continuous training with recurrent learning and learning in this environment.", "histories": [["v1", "Sun, 5 Jun 2016 06:42:56 GMT  (352kb,D)", "http://arxiv.org/abs/1606.01467v1", null], ["v2", "Wed, 8 Jun 2016 17:02:49 GMT  (380kb,D)", "http://arxiv.org/abs/1606.01467v2", null], ["v3", "Mon, 1 Aug 2016 14:17:18 GMT  (254kb,D)", "http://arxiv.org/abs/1606.01467v3", null], ["v4", "Sun, 16 Oct 2016 04:20:25 GMT  (42kb,D)", "http://arxiv.org/abs/1606.01467v4", null], ["v5", "Mon, 7 Nov 2016 05:27:02 GMT  (318kb,D)", "http://arxiv.org/abs/1606.01467v5", null], ["v6", "Fri, 11 Nov 2016 06:22:25 GMT  (313kb,D)", "http://arxiv.org/abs/1606.01467v6", null], ["v7", "Thu, 17 Nov 2016 06:16:24 GMT  (0kb,I)", "http://arxiv.org/abs/1606.01467v7", "This paper has been withdrawn by the author due to a crucial error in the source-code (the epsilon configuration), which makes the results invalid"], ["v8", "Tue, 20 Jun 2017 10:28:34 GMT  (0kb,I)", "http://arxiv.org/abs/1606.01467v8", "The DQN itself has too many hyperparameters"], ["v9", "Wed, 12 Jul 2017 12:29:29 GMT  (0kb,I)", "http://arxiv.org/abs/1606.01467v9", "We choose to withdraw this paper. The DQN itself has too many hyperparameters, which makes it almost impossible to be applied to reasonably large datasets. In the later versions with SGDR experiments, the policies seem to be random actions only"], ["v10", "Thu, 13 Jul 2017 08:49:36 GMT  (0kb,I)", "http://arxiv.org/abs/1606.01467v10", "We choose to withdraw this paper. The DQN itself has too many hyperparameters, which makes it almost impossible to be applied to reasonably large datasets. In the later versions (from v4) with SGDR experiments, it seems that the agent only performs random actions"]], "reviews": [], "SUBJECTS": "cs.LG cs.NE", "authors": ["jie fu"], "accepted": false, "id": "1606.01467"}, "pdf": {"name": "1606.01467.pdf", "metadata": {"source": "META", "title": "Deep Q-Networks for Accelerating the Training of Deep Neural Networks", "authors": ["Jie Fu", "Zichuan Lin", "Miao Liu", "Nicholas Leonard", "Jiashi Feng", "Tat-Seng Chua"], "emails": [], "sections": [{"heading": "1 Introduction", "text": "The motivation for this work is founded on the proof [4] that deep networks trained by a stochastic gradient method with fewer iterations have a better generalization ability. A natural question arises: how can we accelerate the training of deep networks in practice? In this paper, we achieve this by tuning two unusual hyperparameters at every iteration: mini-batch selecting and learning rate scheduling.\nThe contributions of this paper are as follows:\n\u2022 We propose a practical framework based on DQNs to improve modern deep neural networks\u2019 generalization performance by accelerating their training.\n\u2022 We demonstrate how to use a DQN to optimize learning rates of a DNN during training. We also propose to use a DQN to select mini-batches for a DNN during training. These two new methods only require a limited \u201cblack box\u201d interface to deep models, allowing us to tune very sophisticated, state-of-the-art deep networks without having to look under the hood.\n\u2217National University of Singapore, homepage: http://bigaidream.github.io/ \u2020Tsinghua University, Beijing \u2021Massachusetts Institute of Technology, homepage: http://www.mit.edu/\u223cmiaoliu/ \u00a7Element Inc., New York City \u00b6National University of Singapore, homepage: https://sites.google.com/site/jshfeng/ \u2016National University of Singapore, homepage: http://www.comp.nus.edu.sg/\u223cchuats/\nar X\niv :1\n60 6.\n01 46\n7v 1\n[ cs\n.L G\n] 5\n\u2022 A regressor is added to each layer of the deep nets whose hyperparameters are being optimized. This regressor can speed up the overall hyperparameter optimization process."}, {"heading": "2 Related Work", "text": "A pseudo-Bayesian neural network has been used to tune a few hyperparameters of another deep neural network in [13]. The authors add a Bayesian linear regressor to the last hidden layer of a deep net, marginalizing only the output weights of the net while using a point estimate for the remaining parameters. The hyperparameters of this pesudo-Bayesian net are tuned by another Gaussian process based Bayesian optimizer.\nFor tuning learning rates, the most similar work is [3], where a DQN has been proposed to control one hyperparameter. However, the states require careful hand-construction of features and can only be used to tune learning rates. Furthermore, their method cannot handle stochastic environment and is thus not practical.\nAs for mini-batch selection, the authors in [10] study how to choose samples for a mini-batch. Their rank-based approach relies on the assumption that ranks (defined by entropies of training samples during training) of losses over samples change slowly in time, which does not hold in large datasets. Also based on the entropy of training samples, self-paced learning methods[7] are, however, usually coupled with the original optimization problems and, as far as we know, there is no work successfully applying self-paced learning to deep nets training [1]."}, {"heading": "3 Deep Q-Networks (DQNs)", "text": "The goal of a reinforcement learning agent is to maximize its expected total reward by learning an optimal policy (mapping from states to actions). At each time step t, the agent observes a state st \u2208 S (in fact we set st = wt), selects an action at \u2208 A, and receives a reward rt+1, following the agent\u2019s decision it observes the next state st+1. The return at time t is given by Rt+1 = rt+1 +\u2211T\u2212t\u22121 t\u2032=t \u03b3\nt\u2032\u2212trt, where T is the termination time step. In this paper, one episode is defined as one entire training of the inner loop DNN from random weights till convergence. The action-value function Q\u03c0(s, a) : (S,A)\u2192 R measures the expected return after observing state st and taking an action under a policy \u03c0 : S \u2192 A, Q(s, a) = E[Rt|st = s, at = a, \u03c0]. The optimal action-value function Q\u2217(s, a) = max\u03c0Q\u03c0(s, a) obeys the well-known Bellman equation,\nQ\u2217(st, at) = E[rt + \u03b3maxa\u2032Q\u2217(st+1, a\u2032)|st = s, at = a]. (1) At each time step the estimate y\u0302t is defined as y\u0302t = Qt(st, at) and the target yt is given by yt = rt+1 + \u03b3maxa\u2032Qt(st+1, a\u2032). The update is based on the difference between y\u0302t and yt.\nThe DQNmethod [11] approximates the optimal Q-function with a DNN.We denote it as Q(s; \u03b8) : R|S| \u2192 R|A| to emphasize that this DQN is parameterized by the weights \u03b8. It tries to minimize the expected TD error of the optimal Bellman equation: Est,at,rt,st+1 \u2016Q\u03b8(st, at)\u2212 yt\u201622, where\nyt = { rt+1 + 1t 6=T\u22121(\u03b3maxa\u2032 [Q(st+1; \u03b8)]a\u2032 a = at [Q(st; \u03b8)]a a 6= at . (2)\nDQN represents a state by a sequence of history features."}, {"heading": "4 Generic DQN Design for Tuning Hyperparameters", "text": ""}, {"heading": "4.1 Weights as States", "text": "In this paper, the states fed into a DQN are defined by the learned weights of a DNN during training. This is different from [3], where state features are manually crafted by the author. This only only avoids the troublesome feature engineering procedure, but also enables us to tune more types of hyperparameters besides learning rates. For example, as we will demonstrate later, the states are also useful for selecting mini-batches.\nBecause currently the best performing DNN type for supervised tasks is convolutional neural networks (CNNs), we focus on tuning the hyperparameters of CNNs. More specifically, the states are defined by the weights of those convolutional layers during training. Also because of this change1, different from [11], we only employ a standard MLP in the DQN."}, {"heading": "4.2 Fixing Weights by Adding Regressors", "text": "Unfortunately, due to the global update mechanism of back-propagation, the positions of filters will be changed randomly after every episode. To force the filters to appear at the same positions at every episode, we add a regressor to every layer: Lregressor(Wcurrent,Wlast) = 12 \u2016Wcurrent \u2212Wlast\u2016\n2, where Wcurrent are the parameters at current iteration and Wpast are the parameters copied from last episode after convergence.\nBy fixing the weights of a CNN, we have two main added benefits. The first benefit is that the repeated training processes of the CNN can be accelerated. Obviously, the training trajectories of the training of the CNN should not be significant different from each episode, if we only gradually change some hyperparameters. Therefore, this regressor will provide more information to the training of the CNN from the previous episodes.\nThe second byproduct is that the number of states can be decreased dramatically, which is crucial for its scability. This is due to the locality assumed by a convolutional operation, which leads to a filter at a convolutional layer being\n1To some extent, we put the CNN outside the DQN.\nrelatively independent from each other. In other words, a small subset of filters is representative of all the filers. This is quite different from the states used by Atari games [11], where the global state representations are needed."}, {"heading": "4.3 Reward Function", "text": "The reward functions is defined to ensure that the DQN learns a policy to find the optimal objective value in the fewest number of time steps. We define the reward function, similar to [3], as:\nr(f, s\u2032t) = 1 f(s\u2032t)\u2212 flower , f(s\u2032) > flower \u2200s\u2032 (3)\nwhere flower is a predefined lower-bound constant of the training loss, s\u2032t is the whole weights of the CNN during training, and f is the training loss function."}, {"heading": "5 DQN Actions for Specific Tasks", "text": "We need to determine the suitable actions for tuning learning rates and minibatch selection."}, {"heading": "5.1 Tuning Learning Rates", "text": "The training of a DNN with n free parameters can be formulated as the problem of minimizing a function f : Rn \u2192 R. Following the tradition of [10, 5], we define a loss function \u03c8 : Rn \u2192 R for each training sample; the distribution of training samples then induces a distribution over function D, and the overall function f we aim to optimize is the expectation of this distribution:\nf(w) := E\u03c8\u223cD[\u03c8(w)]. (4)\nThe commonly used procedure to optimize f is to iteratively adjust wt (the parameter vector at time step t) using gradient information obtained on a minibatch of size b. More specifically, at each time step t and for a given wt \u2208 Rn, a mini-batch {\u03c8bi=1} \u223c Db is selected to compute \u2207ft(wt), where ft(wt) = 1 b\n\u2211b i=1 \u03c8i(wt). In this work, the stochastic setting is assumed. We also assume that we are preparing training samples for a classification task. The SGD is not robust in that its performance is heavily dependent on how learning rates are tuned over time [12].\nThe SGD procedure then becomes a natural extension of the Gradient Descent (GD) to stochastic optimization of f as follows:\nwt+1 = wt \u2212 \u03b7t\u2207ft(wt), (5)\nwhere \u03b7t is a learning rate. It has been shown in [10] that not only the selection of the update of wt given \u2207ft(wt) is crucial, but the selection of the mini-batch {\u03c8bi=1} \u223c Db used to compute \u2207ft(wt) also greatly contributes to the overall performance.\nTo tune the learning rates of the CNN, we design three actions: increasing the \u03b7t by 0.05, decreasing \u03b7t by 0.05 and remaining the same."}, {"heading": "5.2 Tuning Mini-Batch Selection", "text": "Training samples are not equally valuable [9]. The next natural question is how can we squeeze the training data efficiently and effectively? Self-paced learning [8] has a similar flavor to active learning, which chooses a sample to learn from at each iteration, based on the entropy during training. Active learning approaches differ in their sample selection criteria. However, entropy is a kind of low-level information in the sense that it changes rapidly over training.\nIn this paper, we consider the information associate with labels as the selecting criteria, which is more close to curriculum learning [2, 7]. More specifically, DQN will decide which categories of training samples should be included in one mini-batch. Different from [7], where they encourage diverse examples within a mini-batch, we do not hard-encode such prior knowledge. Take image classification task for example, maybe in the early stage of training, it might be better to put cats and dogs into the mini-batches more frequently; but later it would be better to put dogs, cars and trucks into mini-batches more often.\nThe actions of the DQN are associated with distinct categories. One problem still remains: DQN can only take one action at a time. To solve this problem, we use a First-In-First-Out Queue to act as a buffer for the actions. For example if the buffer size is 5, then at every iteration the DQN will take an action and put it into the buffer. Initially, DQN has to fill the buffer with 10 actions (categories), which implies in the first 5 iterations the DQN will have no control of the CNN. After this \u201cburn-in\u201d phase, DQN will update this buffer with a new action.\nSuppose that we are dealing with a classification task with 3 categories {c1, c2, c3}, and the actions in the buffer are [c1, c1, c3, c3, c1]. Then the minibatch will consist of 60% training samples from category 1 and 40% training samples from category 3."}, {"heading": "6 Experiments", "text": "In this section, we empirically demonstrate how a DQN can accelerate a CNN and thus improve the CNN\u2019s test accuracy. All the experiments are done on the MNIST dataset with 20,000 training samples and 10,000 testing samples. This CNN consists of 2 convolutional layers, with the first having 16 and the second having 256 filters. Above these, we add 2 fully-connected layers with 256 and 128 neurons respectively. All the non-linear functions are Tanh. The original learning rate is set to 0.05 in all experiments."}, {"heading": "6.1 Tuning Learning Rates", "text": "We only test if the DQN can learn useful learning rates scheduling at all, but not intend to compare it to other adaptive learning rate schedulers.\nThe mini-batch size is set to 10. After training the DQN for 20 episodes, the final testing accuracy obtained by a CNN tuned by that DQN is 98.02%, whereas the baseline CNN\u2019s testing accuracy is 97%. From Figure 1 we can also observe that the convergence rate of the CNN tuned by a DQN is much faster than the baseline CNN."}, {"heading": "6.2 Tuning Mini-Batch Selection", "text": "Here we test whether the selection of training samples makes the training algorithms converge faster.\nThe mini-batch size is set to 128, the buffer size is 10 and the number of epochs is 40. Actually we tried 10 and 32 as the mini-batch sizes, but the improvements were not significant. However, increasing the mini-batch size is more than a pedantic trick, as doing so could utilize the parallel computing resources more effectively [6].\nAs shown in Figure 2, after training the DQN for 64 episodes, the final testing accuracy of a CNN with adaptive mini-batch selected by a DQN is 90.6, whereas the baseline CNN\u2019s testing accuracy is 89.98. Figure 2 also shows that\na CNN with adaptive mini-batch selection performs consistantly better than the baseline CNN at every epoch."}, {"heading": "7 Conclusion and Discussion", "text": "In this paper, we proposed a practical framework based on DQNs to improve modern deep neural networks\u2019 generalization performance by accelerating their training. We demonstrated how to use a DQN to optimize learning rates of a DNN during training. We also showed to use a DQN to select mini-batches for a DNN during training.\nIt has been shown that [14] lower layers features are relatively easy to transfer. Thus we are interested to study the transferrablity of the learned policies of DQNs. We would also investigate the combined effects of tuning learning rates and mini-batch selection together. Releasing an easy-to-use toolbox based on this paper is considered as one of our future works."}, {"heading": "Acknowledgement", "text": "Jie Fu thanks Amazon Web Services for providing free GPUs. This work is also supported by NUS-Tsinghua Extreme Search (NExT) project through the National Research Foundation, Singapore."}], "references": [{"title": "Curriculum learning with deep convolutional neural networks", "author": ["Vanya Avramova"], "venue": null, "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2015}, {"title": "Curriculum learning", "author": ["Yoshua Bengio", "J\u00e9r\u00f4me Louradour", "Ronan Collobert", "Jason Weston"], "venue": "In Proceedings of the 26th annual international conference on machine learning,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2009}, {"title": "Using deep q-learning to control optimization hyperparameters", "author": ["Samantha Hansen"], "venue": "arXiv preprint arXiv:1602.04062,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2016}, {"title": "Train faster, generalize better: Stability of stochastic gradient descent", "author": ["Moritz Hardt", "Benjamin Recht", "Yoram Singer"], "venue": "arXiv preprint arXiv:1509.01240,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2015}, {"title": "Beyond convexity: Stochastic quasi-convex optimization", "author": ["Elad Hazan", "Kfir Levy", "Shai Shalev-Shwartz"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2015}, {"title": "Batch normalization: Accelerating deep network training by reducing internal covariate shift", "author": ["Sergey Ioffe", "Christian Szegedy"], "venue": "arXiv preprint arXiv:1502.03167,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2015}, {"title": "Self-paced curriculum learning", "author": ["Lu Jiang", "Deyu Meng", "Qian Zhao", "Shiguang Shan", "Alexander G Hauptmann"], "venue": "In Twenty-Ninth AAAI Conference on Artificial Intelligence,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2015}, {"title": "Self-paced learning for latent variable models", "author": ["M Pawan Kumar", "Benjamin Packer", "Daphne Koller"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2010}, {"title": "Are all training examples equally valuable", "author": ["Agata Lapedriza", "Hamed Pirsiavash", "Zoya Bylinskii", "Antonio Torralba"], "venue": "arXiv preprint arXiv:1311.6510,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2013}, {"title": "Online batch selection for faster training of neural networks", "author": ["Ilya Loshchilov", "Frank Hutter"], "venue": "arXiv preprint arXiv:1511.06343,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2015}, {"title": "Human-level control through deep reinforcement learning", "author": ["Volodymyr Mnih", "Koray Kavukcuoglu", "David Silver", "Andrei A Rusu", "Joel Veness", "Marc G Bellemare", "Alex Graves", "Martin Riedmiller", "Andreas K Fidjeland", "Georg Ostrovski"], "venue": "Nature, 518(7540):529\u2013533,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2015}, {"title": "No more pesky learning rates", "author": ["Tom Schaul", "Sixin Zhang", "Yann LeCun"], "venue": "arXiv preprint arXiv:1206.1106,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2012}, {"title": "Scalable bayesian optimization using deep neural networks", "author": ["Jasper Snoek", "Oren Rippel", "Kevin Swersky", "Ryan Kiros", "Nadathur Satish", "Narayanan Sundaram", "Md Patwary", "Mostofa Ali", "Ryan P Adams"], "venue": "arXiv preprint arXiv:1502.05700,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2015}, {"title": "How transferable are features in deep neural networks", "author": ["Jason Yosinski", "Jeff Clune", "Yoshua Bengio", "Hod Lipson"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2014}], "referenceMentions": [{"referenceID": 3, "context": "The motivation for this work is founded on the proof [4] that deep networks trained by a stochastic gradient method with fewer iterations have a better generalization ability.", "startOffset": 53, "endOffset": 56}, {"referenceID": 12, "context": "A pseudo-Bayesian neural network has been used to tune a few hyperparameters of another deep neural network in [13].", "startOffset": 111, "endOffset": 115}, {"referenceID": 2, "context": "For tuning learning rates, the most similar work is [3], where a DQN has been proposed to control one hyperparameter.", "startOffset": 52, "endOffset": 55}, {"referenceID": 9, "context": "As for mini-batch selection, the authors in [10] study how to choose samples for a mini-batch.", "startOffset": 44, "endOffset": 48}, {"referenceID": 6, "context": "Also based on the entropy of training samples, self-paced learning methods[7] are, however, usually coupled with the original optimization problems and, as far as we know, there is no work successfully applying self-paced learning to deep nets training [1].", "startOffset": 74, "endOffset": 77}, {"referenceID": 0, "context": "Also based on the entropy of training samples, self-paced learning methods[7] are, however, usually coupled with the original optimization problems and, as far as we know, there is no work successfully applying self-paced learning to deep nets training [1].", "startOffset": 253, "endOffset": 256}, {"referenceID": 10, "context": "The DQNmethod [11] approximates the optimal Q-function with a DNN.", "startOffset": 14, "endOffset": 18}, {"referenceID": 2, "context": "This is different from [3], where state features are manually crafted by the author.", "startOffset": 23, "endOffset": 26}, {"referenceID": 10, "context": "Also because of this change1, different from [11], we only employ a standard MLP in the DQN.", "startOffset": 45, "endOffset": 49}, {"referenceID": 10, "context": "This is quite different from the states used by Atari games [11], where the global state representations are needed.", "startOffset": 60, "endOffset": 64}, {"referenceID": 2, "context": "We define the reward function, similar to [3], as:", "startOffset": 42, "endOffset": 45}, {"referenceID": 9, "context": "Following the tradition of [10, 5], we define a loss function \u03c8 : R \u2192 R for each training sample; the distribution of training samples then induces a distribution over function D, and the overall function f we aim to optimize is the expectation of this distribution:", "startOffset": 27, "endOffset": 34}, {"referenceID": 4, "context": "Following the tradition of [10, 5], we define a loss function \u03c8 : R \u2192 R for each training sample; the distribution of training samples then induces a distribution over function D, and the overall function f we aim to optimize is the expectation of this distribution:", "startOffset": 27, "endOffset": 34}, {"referenceID": 11, "context": "The SGD is not robust in that its performance is heavily dependent on how learning rates are tuned over time [12].", "startOffset": 109, "endOffset": 113}, {"referenceID": 9, "context": "It has been shown in [10] that not only the selection of the update of wt given \u2207ft(wt) is crucial, but the selection of the mini-batch {\u03c8b i=1} \u223c Db used to compute \u2207ft(wt) also greatly contributes to the overall performance.", "startOffset": 21, "endOffset": 25}, {"referenceID": 8, "context": "Training samples are not equally valuable [9].", "startOffset": 42, "endOffset": 45}, {"referenceID": 7, "context": "The next natural question is how can we squeeze the training data efficiently and effectively? Self-paced learning [8] has a similar flavor to active learning, which chooses a sample to learn from at each iteration, based on the entropy during training.", "startOffset": 115, "endOffset": 118}, {"referenceID": 1, "context": "In this paper, we consider the information associate with labels as the selecting criteria, which is more close to curriculum learning [2, 7].", "startOffset": 135, "endOffset": 141}, {"referenceID": 6, "context": "In this paper, we consider the information associate with labels as the selecting criteria, which is more close to curriculum learning [2, 7].", "startOffset": 135, "endOffset": 141}, {"referenceID": 6, "context": "Different from [7], where they encourage diverse examples within a mini-batch, we do not hard-encode such prior knowledge.", "startOffset": 15, "endOffset": 18}, {"referenceID": 5, "context": "However, increasing the mini-batch size is more than a pedantic trick, as doing so could utilize the parallel computing resources more effectively [6].", "startOffset": 147, "endOffset": 150}, {"referenceID": 13, "context": "It has been shown that [14] lower layers features are relatively easy to transfer.", "startOffset": 23, "endOffset": 27}], "year": 2017, "abstractText": "In this paper, we show that by feeding the weights of a deep neural network (DNN) during training into a deep Q-network (DQN) as its states, this DQN can learn policies to accelerate the training of that DNN. The actions of the DQN modify different hyperparameters during training. Empirically, this acceleration leads to better generalization performance of the DNN.", "creator": "LaTeX with hyperref package"}}}