{"id": "1606.02003", "review": {"conference": "EMNLP", "VERSION": "v1", "DATE_OF_SUBMISSION": "7-Jun-2016", "title": "Memory-enhanced Decoder for Neural Machine Translation", "abstract": "We propose to enhance the RNN decoder in a neural machine translator (NMT) with external memory, as a natural but powerful extension to the state in the decoding RNN. This memory-enhanced RNN decoder is called \\textsc{MemDec}. At each time during decoding, \\textsc{MemDec} will read from this memory and write to this memory once, both with content-based addressing. Unlike the unbounded memory in previous work\\cite{RNNsearch} to store the representation of source sentence, the memory in \\textsc{MemDec} is a matrix with pre-determined size designed to better capture the information important for the decoding process at each time step. Our empirical study on Chinese-English translation shows that it can improve by $4.8$ BLEU upon Groundhog and $5.3$ BLEU upon on Moses, yielding the best performance achieved with the same training set. This study shows that it can increase by $5.3$ BLEU upon Groundhog, yielding the best performance achieved with the same training set. The above model gives the results of a real-world translation in the text-spacing program.\n\n\n\nThe study shows that it can increase by $4.8$ BLEU upon Groundhog and $5.3$ BLEU upon Groundhog and $5.3$ BLEU upon Groundhog and $5.3$ BLEU upon Groundhog and $5.3$ BLEU upon Groundhog and $5.3$ BLEU upon Groundhog and $5.3$ BLEU upon Groundhog and $5.3$ BLEU upon Groundhog and $5.3$ BLEU upon Groundhog and $5.3$ BLEU upon Groundhog and $5.3$ BLEU upon Groundhog and $5.3$ BLEU upon Groundhog and $5.3$ BLEU upon Groundhog and $5.3$ BLEU upon Groundhog and $5.3$ BLEU upon Groundhog and $5.3$ BLEU upon Groundhog and $5.3$ BLEU upon Groundhog and $5.3$ BLEU upon Groundhog and $5.3$ BLEU upon Groundhog and $5.3$ BLEU upon Groundhog and $5.3$ BLEU upon Groundhog and $5", "histories": [["v1", "Tue, 7 Jun 2016 02:28:19 GMT  (148kb,D)", "http://arxiv.org/abs/1606.02003v1", "11 pages"]], "COMMENTS": "11 pages", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["mingxuan wang", "zhengdong lu", "hang li", "qun liu"], "accepted": true, "id": "1606.02003"}, "pdf": {"name": "1606.02003.pdf", "metadata": {"source": "CRF", "title": "Memory-enhanced Decoder for Neural Machine Translation", "authors": ["Mingxuan Wang", "Zhengdong Lu", "Hang Li", "Qun Liu"], "emails": ["wangmingxuan@ict.ac.cn", "liuqun@ict.ac.cn", "Lu.Zhengdong@huawei.com", "HangLi.HL@huawei.com"], "sections": [{"heading": "1 Introduction", "text": "The introduction of external memory has greatly expanded the representational capability of neural network-based model on modeling sequences(Graves et al., 2014), by providing flexible ways of storing and accessing information. More specifically, in neural machine translation, one great improvement came from using an array of vectors to represent the source in a sentence-level memory and dynamically accessing relevant segments of them (alignment) (Bahdanau et al., 2014) through content-based addressing (Graves et al., 2014). The success of RNNsearch demonstrated the advantage of saving the entire sentence of arbitrary length in an unbounded memory for operations of next stage (e.g., decoding).\nIn this paper, we show that an external memory can be used to facilitate the decoding/generation process thorough a memory-enhanced RNN decoder, called MEMDEC. The memory in MEMDEC is a direct extension to the state in the decoding, therefore functionally closer to the memory cell in LSTM(Hochreiter and Schmidhuber, 1997). It takes the form of a matrix with pre-determined size, each column (\u201ca memory cell\u201d) can be accessed by the decoding RNN with content-based addressing for both reading and writing during the decoding process. This memory is designed to provide a more flexible way to select, represent and synthesize the information of source sentence and previously generated words of target relevant to the decoding. This is in contrast to the set of hidden states of the entire source sentence (which can viewed as another form of memory) in (Bahdanau et al., 2014) for attentive read, but can be combined with it to greatly improve the performance of neural machine translator. We apply our model on English-Chinese translation tasks, achieving performance superior to any published results, SMT or NMT, on the same training data (Xie et al., 2011; Meng et al., 2015; Tu et al., 2016; Hu et al., 2015)\nOur contributions are mainly two-folds\nar X\niv :1\n60 6.\n02 00\n3v 1\n[ cs\n.C L\n] 7\nJ un\n2 01\n6\n\u2022 we propose a memory-enhanced decoder for neural machine translator which naturally extends the RNN with vector state.\n\u2022 our empirical study on Chinese-English translation tasks show the efficacy of the proposed model.\nRoadmap In the remainder of this paper, we will first give a brief introduction to attention-based neural machine translation in Section 2, presented from the view of encoder-decoder, which treats the hidden states of source as an unbounded memory and the attention model as a content-based reading. In Section 3, we will elaborate on the memory-enhanced decoder MEMDEC. In Section 4, we will apply NMT with MEMDEC to a Chinese-English task. Then in Section 5 and 6, we will give related work and conclude the paper."}, {"heading": "2 Neural machine translation with attention", "text": "Our work is built on attention-based NMT(Bahdanau et al., 2014), which represents the source sentence as a sequence of vectors after being processed by RNN or bi-directional RNNs, and then conducts dynamic alignment and generation of the target sentence with another RNN simultaneously.\nAttention-based NMT, with RNNsearch as its most popular representative, generalizes the conventional notion of encoder-decoder in using a unbounded memory for the intermediate representation of source sentence and content-based addressing read in decoding, as illustrated in Figure 1. More specifically, at time step t, RNNsearch first get context vector ct after reading from the source representation MS, which is then used to update the state, and generate the word yt (along with the current hidden state st, and the previously generated word yi\u22121).\nFormally, given an input sequence x = [x1, x2, . . . , xTx ] and the previously generated sequence y<t = [y1, y2, . . . , yt\u22121], the probability of next word yt is\np(yt|y<t;x) = f(ct, yt\u22121, st), (1)\nwhere st is state of decoder RNN at time step t calculated as\nst = g(st\u22121, yt\u22121, ct). (2)\nwhere g(\u00b7) can be an be any activation function, here we adopt a more sophisticated dynamic operator as in Gated Recurrent Unit (GRU, (Cho et al., 2014)). In the remainder of the paper, we will also use\nGRU to stand for the operator. The reading ct is calculated as\nct = j=Tx\u2211 j=1 \u03b1t,jhj , (3)\nwhere hj is the jth cell in memory MS. More formally, hj = [ \u2190\u2212 hj >, \u2212\u2192 hj >]> is the annotations of xj and contains information about the whole input sequence with a strong focus on the parts surrounding xj , which is computed by a bidirectional RNN. The weight \u03b1t,j is computed by\n\u03b1t,j = exp(et,j)\u2211k=Tx\nk=1 exp(et,k) .\nwhere ei,j = vTa tanh(Wast\u22121 +Uahj) scores how well st\u22121 and the memory cell hj match. This is called automatic alignment (Bahdanau et al., 2014) or attention model (Luong et al., 2015), but it is essentially reading with content-based addressing defined in (Graves et al., 2014). With this addressing strategy the decoder can attend to the source representation that is most relevant to the stage of decoding."}, {"heading": "2.1 Improved Attention Model", "text": "The alignment model \u03b1t,j scores how well the output at position t matches the inputs around position j based on st\u22121 and hj . It is intuitively beneficial to exploit the information of yt\u22121 when reading from MS, which is missing from the implementation of attention-based NMT in (Bahdanau et al., 2014). In this work, we build a more effective alignment path by feeding both previous hidden state st\u22121 and the context word yt\u22121 to the attention model, inspired by the recent implementation of attention-based NMT1. Formally, the calculation of et,j becomes\net,j = v T a tanh(Was\u0303t\u22121 +Uahj),\nwhere 1github.com/nyu-dl/dl4mt-tutorial/tree/master/session2\n\u2022 s\u0303t\u22121 = H(st\u22121, eyt\u22121) is an intermediate state tailored for reading from MS with the information of yt\u22121 (its word embedding being eyt\u22121) added;\n\u2022 H is a nonlinear function, which can be as simple as tanh or as complex as GRU. In our preliminary experiments, we found GRU works slightly better than tanh function, but we chose the latter for simplicity."}, {"heading": "3 Decoder with External Memory", "text": "In this section we will elaborate on the proposed memory-enhanced decoder MEMDEC. In addition to the source memory MS, MEMDEC is equipped with a buffer memory MB as an extension to the conventional state vector. Figure 3 contrasts MEMDEC with the decoder in RNNsearch (Figure 1) on a high level.\nIn the remainder of the paper, we will refer to the conventional state as vector-state (denoted st) and its memory extension as memory-state (denoted as MBt ). Both states are updated at each time step in a interweaving fashion, while the output symbol yt is predicted based solely on vector-state st (along with ct and yt\u22121). The diagram of this memory-enhanced decoder is given in Figure 2.\nVector-State Update At time t, the vector-state st is first used to read MB\nrt\u22121 = readB(st\u22121,MBt\u22121) (4)\nwhich then meets the previous prediction yt\u22121 to form an \u201cintermediate\u201d state-vector\ns\u0303t = tanh(Wrrt\u22121 +Wyeyt\u22121). (5)\nwhere eyt\u22121 is the word-embedding associated with the previous prediction yt\u22121. This pre-state s\u0303t is used to read the source memory MS\nct = readS(s\u0303t,MS). (6)\nBoth readings in Eq. (4) & (6) follow content-based addressing(Graves et al., 2014) (details later in Section 3.1). After that, rt\u22121 is combined with output symbol yt\u22121 and ct to update the new vectorstate\nst = GRU(rt\u22121,yt\u22121, ct) (7)\nThe update of vector-state is illustrated in Figure 4.\nMemory-State Update As illustrated in Figure 5, the update for memory-state is simple after the update of vector-state: with the vector-state st+1 the updated memory-state will be\nMBt = write(st,M B t\u22121) (8)\nThe writing to the memory-state is also content-based, with same forgetting mechanism suggested in (Graves et al., 2014), which we will elaborate with more details later in this section.\nPrediction As illustrated in Figure 6, the prediction model is same as in (Bahdanau et al., 2014), where the score for word y is given by\nscore(y) = DNN([st, ct, eyt\u22121 ]) >\u03c9y (9)\nwhere \u03c9y is the parameters associated with the word y. The probability of generating word y at time t is then given by a softmax over the scores\np(y|st, ct, yt\u22121) = exp(score(y))\u2211 y\u2032 exp(score(y\u2032)) ."}, {"heading": "3.1 Reading Memory-State", "text": "Formally MBt\u2032 \u2208 Rn\u00d7m is the memory-state at time t\u2032 after the memory-state update, where n is the number of memory cells and m is the dimension of vector in each cell. Before the vector-state update at time t, the output of reading rt is given by\nrt = j=n\u2211 j=1 wRt (j)M B t\u22121(j)\nwhere wRt \u2208 Rn specifies the normalized weights assigned to the cells in MBt . Similar with the reading from MS ( a.k.a. attention model), we use content-based addressing in determining wRt . More specifically, wRt is also updated from the one from previous time w R t\u22121 as\nwRt = g R t w R t\u22121 + (1\u2212 gRt )w\u0303Rt , (10)\nwhere\n\u2022 gRt = \u03c3(wRgst) is the gate function, with parameters wRg \u2208 Rm;\n\u2022 w\u0303t gives the contribution based on the current vector-state st\nw\u0303Rt = softmax(a R t ) (11)\naRt (i) = v >(WRaM B t\u22121(i) +U R ast\u22121), (12)\nwith parameters WRa,U R a \u2208 Rm\u00d7m and v \u2208 Rm."}, {"heading": "3.2 Writing to Memory-State", "text": "There are two types of operation on writing to memory-state: ERASE and ADD. Erasion is similar to the forget gate in LSTM or GRU, which determines the content to be remove from memory cells. More specifically, the vector \u00b5ERSt \u2208 Rm specifies the values to be removed on each dimension in memory cells, which is than assigned to each cell through normalized weights wWt . Formally, the memory-state after ERASE is given by\nM\u0303Bt (i) = M B t\u22121(i)(1\u2212wWt (i) \u00b7 \u00b5ERSt ) (13)\ni = 1, \u00b7 \u00b7 \u00b7 , n where\n\u2022 \u00b5ERSt = \u03c3(WERSst) is parametrized with WERS \u2208 Rm\u00d7m;\n\u2022 wWt (i) specifies the weight associated with the ith cell in the same parametric form as in Eq. (10)- (12) with generally different parameters.\nADD operation is similar with the update gate in LSTM or GRU, deciding how much current information should be written to the memory.\nMBt (i) = M\u0303 B t (i) +w W t (i)\u00b5 ADD t\n\u00b5ADDt = \u03c3(W ADDst)\nwhere \u00b5ADDt \u2208 Rm and WADD \u2208 Rm\u00d7m. In our experiments, we have a peculiar but interesting observation: it is often beneficial to use the same weights for both reading (i.e., wRt in Section 3.1) and writing (i.e., w W t in Section 3.2 ) for the same vector-state st. We conjecture that this acts like a regularization mechanism to encourage the content of reading and writing to be similar to each other."}, {"heading": "3.3 Some Analysis", "text": "The writing operation in Eq. (13) at time t can be viewed as an nonlinear way to combine the previous memory-state MBt\u22121 and the newly updated vector-state st, where the nonlinearity comes from both the content-based addressing and the gating. This is in a way similar to the update of states in regular RNN, while we conjecture that the addressing strategy in MEMDEC makes it easier to selectively change some content updated (e.g., the relatively short-term content) while keeping other content less modified (e.g., the relatively long-term content).\nThe reading operation in Eq. (10) can \u201cextract\u201d the content from MBt relevant to the alignment (reading from MS) and prediction task at time t. This is in contrast with the regular RNN decoder including its gated variants, which takes the entire state vector to for this purpose. As one advantage, although only part of the information in MBt is used at t, the entire memory-state, which may store other information useful for later, will be carry over to time t+ 1 for memory-state update (writing)."}, {"heading": "4 Experiments on Chinese-English Translation", "text": "We test the memory-enhanced decoder to task of Chinese-to-English translation, where MEMDEC is put on the top of encoder same as in (Bahdanau et al., 2014)."}, {"heading": "4.1 Datasets and Evaluation metrics", "text": "Our training data for the translation task consists of 1.25M sentence pairs extracted from LDC corpora2, with 27.9M Chinese words and 34.5M English words respectively. We choose NIST 2002 (MT02) dataset as our development set, and the NIST 2003 (MT03), 2004 (MT04) 2005 (MT05) and 2006 (MT06) datasets as our test sets. We use the case-insensitive 4-gram NIST BLEU score as our evaluation metric as our evaluation metric (Papineni et al., 2002)."}, {"heading": "4.2 Experiment settings", "text": "Hyper parameters In training of the neural networks, we limit the source and target vocabularies to the most frequent 30K words in both Chinese and English, covering approximately 97.7% and 99.3% of the two corpora respectively. The dimensions of word embedding is 512 and the size of the hidden layer is 1024. The dimemsion of each cell in MB is set to 1024 and the number of cells n is set to 8.\nTraining details We initialize the recurrent weight matrices as random orthogonal matrices. All the bias vectors were initialize to zero. For other parameters, we initialize them by sampling each element from the Gaussian distribution of mean 0 and variance 0.012. Parameter optimization is performed using stochastic gradient descent. Adadelta (Zeiler, 2012) is used to automatically adapt the learning rate of each parameter ( = 10\u22126 and \u03c1 = 0.95). To avoid gradients explosion, the gradients of the cost function which had `2 norm larger than a predefined threshold 1.0 was normalized to the threshold (Pascanu et al., 2013). Each SGD is of a mini-batch of 80 sentences. We train our NMT model with the sentences of length up to 50 words in training data, while for moses system we use the full training data.\n2The corpora include LDC2002E18, LDC2003E07, LDC2003E14, Hansards portion of LDC2004T07, LDC2004T08 and LDC2005T06.\nMemory Initialization Each memory cell is initialized with the source sentence hidden state computed as\nMB(i) = m+ \u03bdi (14)\nm = \u03c3(WINI i=Tx\u2211 i=0 hi)/Tx (15)\nwhere WINI \u2208 Rm\u00d72\u00b7m; \u03c3 is tanh function. m makes a nonlinear transformation of the source sentence information. \u03bdi is a random vector sampled from N (0, 0.1).\nDropout we also use dropout for our NMT baseline model and MEMDEC to avoid over-fitting (Hinton et al., 2012). The key idea is to randomly drop units (along with their connections) from the neural network during training. This prevents units from co-adapting too much. In the simplest case, each unit is omitted with a fixed probability p, namely dropout rate. In our experiments, dropout was applied only on the output layer and the dropout rate is set to 0.5. We also try other strategy such as dropout at word embeddings or RNN hidden states but fail to get further improvements.\nPre-training For MEMDEC, the objective function is a highly non-convex function of the parameters with more complicated landscape than that for decoder without external memory, rendering direct optimization over all the parameters rather difficult. Inspired by the effort on easing the training of very deep architectures (Hinton and Salakhutdinov, 2006), we propose a simple pre-training strategyFirst we train a regular attention-based NMT model without external memory. Then we use the trained NMT model to initialize the parameters of encoder and parameters of MEMDEC, except those related to memory-state (i.e., {WRa,URa,v,wRg ,WERS,WADD}). After that, we fine-tune all the parameters of NMT with MEMDEC decoder, including the parameters initialized with pre-training and those associated with accessing memory-state."}, {"heading": "4.3 Comparison systems", "text": "We compare our method with three state-of-the-art systems:\n\u2022 Moses: an open source phrase-based translation system 3: with default configuration and a 4- gram language model trained on the target portion of training data.\n\u2022 RNNSearch: an attention-based NMT model with default settings. We use the open source system GroundHog as our NMT baseline4.\n\u2022 Coverage model: a state-of-the-art variant of attention-based NMT model (Tu et al., 2016) which improves the attention mechanism through modelling a soft coverage on the source representation."}, {"heading": "4.4 Results", "text": "The main results of different models are given in Table 1. Clearly MEMDEC leads to remarkable improvement over Moses (+5.28 BLEU) and Groundhog (+4.78 BLEU). The feedback attention gains +1.06 BLEU score on top of Groundhog on average, while together with dropout adds another +0.83 BLEU score, which constitute the 1.89 BLEU gain of RNNsearch? over Groundhog. Compared to RNNsearch? MEMDEC is +2.89 BLEU score higher, showing the modeling power gained from the external memory. Finally, we also compare MEMDEC with the state-of-the-art attention-based NMT\n3http://www.statmt.org/moses/ 4https://github.com/lisa-groundhog/GroundHog\nwith COVERAGE mechanism(Tu et al., 2016), which is about 2 BLEU over than the published result after adding fast attention and dropout. In this comparison MEMDEC wins with big margin (+1.46 BLEU score)."}, {"heading": "4.5 Model selection", "text": "Pre-training plays an important role in optimizing the memory model. As can be seen in Tab.2, pre-training improves upon our baseline +1.11 BLEU score on average, but even without pre-training our model still gains +1.04 BLEU score on average. Our model is rather robust to the memory size: with merely four cells, our model will be over 2 BLEU higher than RNNsearch?. This further verifies our conjecture the the external memory is mostly used to store part of the source and history of target sentence."}, {"heading": "4.6 Case study", "text": "We show in Table 5 sample translations from Chinese to English, comparing mainly MEMDEC and the RNNsearch model for its pre-training. It is appealing to observe that MEMDEC can produce more fluent translation results and better grasp the semantic information of the sentence."}, {"heading": "5 Related Work", "text": "There is a long thread of work aiming to improve the ability of RNN in remembering long sequences, with the long short-term memory RNN (LSTM) (Hochreiter and Schmidhuber, 1997) being the most salient examples and GRU (Cho et al., 2014) being the most recent one. Those works focus on\ndesigning the dynamics of the RNN through new dynamic operators and appropriate gating, while still keeping vector form RNN states. MEMDEC, on top of the gated RNN, explicitly adds matrixform memory equipped with content-based addressing to the system, hence greatly improving the power of the decoder RNN in representing the information important for the translation task.\nMEMDEC is obviously related to the recent effort on attaching an external memory to neural networks, with two most salient examples being Neural Turing Machine (NTM) (Graves et al., 2014) and Memory Network (Weston et al., 2014). In fact MEMDEC can be viewed as a special case of NTM, with specifically designed reading (from two different types of memory) and writing mechanism for the translation task. Quite remarkably MEMDEC is among the rare instances of NTM which significantly improves upon state-of-the-arts on a real-world NLP task with large training corpus.\nOur work is also related to the recent work on machine reading (Cheng et al., 2016), in which the machine reader is equipped with a memory tape, enabling the model to directly read all the previous hidden state with an attention mechanism. Different from their work, we use an external bounded memory and make an abstraction of previous information. In (Meng et al., 2015), Meng et. al. also proposed a deep architecture for sequence-to-sequence learning with stacked layers of memory to store the intermediate representations, while our external memory was applied within a sequence."}, {"heading": "6 Conclusion", "text": "We propose to enhance the RNN decoder in a neural machine translator (NMT) with external memory. Our empirical study on Chinese-English translation shows that it can significantly improve the performance of NMT."}], "references": [{"title": "Neural machine translation by jointly learning to align and translate", "author": ["Kyunghyun Cho", "Yoshua Bengio"], "venue": "arXiv preprint arXiv:1409.0473", "citeRegEx": "Bahdanau et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Bahdanau et al\\.", "year": 2014}, {"title": "Long short-term memory-networks for machine reading", "author": ["Cheng et al.2016] Jianpeng Cheng", "Li Dong", "Mirella Lapata"], "venue": "arXiv preprint arXiv:1601.06733", "citeRegEx": "Cheng et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Cheng et al\\.", "year": 2016}, {"title": "Learning phrase representations using rnn encoder-decoder for statistical machine translation", "author": ["Cho et al.2014] Kyunghyun Cho", "Bart Van Merri\u00ebnboer", "Caglar Gulcehre", "Dzmitry Bahdanau", "Fethi Bougares", "Holger Schwenk", "Yoshua Bengio"], "venue": "arXiv preprint arXiv:1406.1078", "citeRegEx": "Cho et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Cho et al\\.", "year": 2014}, {"title": "Reducing the dimensionality of data with neural networks", "author": ["Hinton", "Ruslan R Salakhutdinov"], "venue": null, "citeRegEx": "Hinton et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Hinton et al\\.", "year": 2006}, {"title": "Improving neural networks by preventing co-adaptation of feature detectors", "author": ["Nitish Srivastava", "Alex Krizhevsky", "Ilya Sutskever", "Ruslan Salakhutdinov"], "venue": null, "citeRegEx": "Hinton et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Hinton et al\\.", "year": 2012}, {"title": "Long short-term memory", "author": ["Hochreiter", "Schmidhuber1997] Sepp Hochreiter", "J\u00fcrgen Schmidhuber"], "venue": "Neural computation,", "citeRegEx": "Hochreiter et al\\.,? \\Q1997\\E", "shortCiteRegEx": "Hochreiter et al\\.", "year": 1997}, {"title": "Context-dependent translation selection using convolutional neural network", "author": ["Hu et al.2015] Baotian Hu", "Zhaopeng Tu", "Zhengdong Lu", "Hang Li"], "venue": null, "citeRegEx": "Hu et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Hu et al\\.", "year": 2015}, {"title": "Effective approaches to attention-based neural machine translation", "author": ["Hieu Pham", "Christopher D Manning"], "venue": "arXiv preprint arXiv:1508.04025", "citeRegEx": "Luong et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Luong et al\\.", "year": 2015}, {"title": "A deep memorybased architecture for sequence-to-sequence learning", "author": ["Meng et al.2015] Fandong Meng", "Zhengdong Lu", "Zhaopeng Tu", "Hang Li", "Qun Liu"], "venue": "arXiv preprint arXiv:1506.06442", "citeRegEx": "Meng et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Meng et al\\.", "year": 2015}, {"title": "Bleu: a method for automatic evaluation of machine translation", "author": ["Salim Roukos", "Todd Ward", "Wei-Jing Zhu"], "venue": "In Proceedings of the 40th annual meeting on association for computational linguistics,", "citeRegEx": "Papineni et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Papineni et al\\.", "year": 2002}, {"title": "How to construct deep recurrent neural networks. arXiv preprint arXiv:1312.6026", "author": ["Caglar Gulcehre", "Kyunghyun Cho", "Yoshua Bengio"], "venue": null, "citeRegEx": "Pascanu et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Pascanu et al\\.", "year": 2013}, {"title": "Modeling coverage for neural machine translation", "author": ["Tu et al.2016] Zhaopeng Tu", "Zhengdong Lu", "Yang Liu", "Xiaohua Liu", "Hang Li"], "venue": null, "citeRegEx": "Tu et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Tu et al\\.", "year": 2016}, {"title": "A novel dependency-to-string model for statistical machine translation", "author": ["Xie et al.2011] Jun Xie", "Haitao Mi", "Qun Liu"], "venue": null, "citeRegEx": "Xie et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Xie et al\\.", "year": 2011}, {"title": "Adadelta: an adaptive learning rate method", "author": ["Matthew D Zeiler"], "venue": "arXiv preprint arXiv:1212.5701", "citeRegEx": "Zeiler.,? \\Q2012\\E", "shortCiteRegEx": "Zeiler.", "year": 2012}], "referenceMentions": [{"referenceID": 0, "context": "Unlike the unbounded memory in previous work(Bahdanau et al., 2014) to store the representation of source sentence, the memory in MEMDEC is a matrix with pre-determined size designed to better capture the information important for the decoding process at each time step.", "startOffset": 44, "endOffset": 67}, {"referenceID": 0, "context": "More specifically, in neural machine translation, one great improvement came from using an array of vectors to represent the source in a sentence-level memory and dynamically accessing relevant segments of them (alignment) (Bahdanau et al., 2014) through content-based addressing (Graves et al.", "startOffset": 223, "endOffset": 246}, {"referenceID": 0, "context": "This is in contrast to the set of hidden states of the entire source sentence (which can viewed as another form of memory) in (Bahdanau et al., 2014) for attentive read, but can be combined with it to greatly improve the performance of neural machine translator.", "startOffset": 126, "endOffset": 149}, {"referenceID": 12, "context": "We apply our model on English-Chinese translation tasks, achieving performance superior to any published results, SMT or NMT, on the same training data (Xie et al., 2011; Meng et al., 2015; Tu et al., 2016; Hu et al., 2015) Our contributions are mainly two-folds ar X iv :1 60 6.", "startOffset": 152, "endOffset": 223}, {"referenceID": 8, "context": "We apply our model on English-Chinese translation tasks, achieving performance superior to any published results, SMT or NMT, on the same training data (Xie et al., 2011; Meng et al., 2015; Tu et al., 2016; Hu et al., 2015) Our contributions are mainly two-folds ar X iv :1 60 6.", "startOffset": 152, "endOffset": 223}, {"referenceID": 11, "context": "We apply our model on English-Chinese translation tasks, achieving performance superior to any published results, SMT or NMT, on the same training data (Xie et al., 2011; Meng et al., 2015; Tu et al., 2016; Hu et al., 2015) Our contributions are mainly two-folds ar X iv :1 60 6.", "startOffset": 152, "endOffset": 223}, {"referenceID": 6, "context": "We apply our model on English-Chinese translation tasks, achieving performance superior to any published results, SMT or NMT, on the same training data (Xie et al., 2011; Meng et al., 2015; Tu et al., 2016; Hu et al., 2015) Our contributions are mainly two-folds ar X iv :1 60 6.", "startOffset": 152, "endOffset": 223}, {"referenceID": 0, "context": "2 Neural machine translation with attention Our work is built on attention-based NMT(Bahdanau et al., 2014), which represents the source sentence as a sequence of vectors after being processed by RNN or bi-directional RNNs, and then conducts dynamic alignment and generation of the target sentence with another RNN simultaneously.", "startOffset": 84, "endOffset": 107}, {"referenceID": 2, "context": "(2) where g(\u00b7) can be an be any activation function, here we adopt a more sophisticated dynamic operator as in Gated Recurrent Unit (GRU, (Cho et al., 2014)).", "startOffset": 138, "endOffset": 156}, {"referenceID": 0, "context": "This is called automatic alignment (Bahdanau et al., 2014) or attention model (Luong et al.", "startOffset": 35, "endOffset": 58}, {"referenceID": 7, "context": ", 2014) or attention model (Luong et al., 2015), but it is essentially reading with content-based addressing defined in (Graves et al.", "startOffset": 27, "endOffset": 47}, {"referenceID": 0, "context": "It is intuitively beneficial to exploit the information of yt\u22121 when reading from M, which is missing from the implementation of attention-based NMT in (Bahdanau et al., 2014).", "startOffset": 152, "endOffset": 175}, {"referenceID": 0, "context": "Prediction As illustrated in Figure 6, the prediction model is same as in (Bahdanau et al., 2014), where the score for word y is given by", "startOffset": 74, "endOffset": 97}, {"referenceID": 0, "context": "We test the memory-enhanced decoder to task of Chinese-to-English translation, where MEMDEC is put on the top of encoder same as in (Bahdanau et al., 2014).", "startOffset": 132, "endOffset": 155}, {"referenceID": 9, "context": "We use the case-insensitive 4-gram NIST BLEU score as our evaluation metric as our evaluation metric (Papineni et al., 2002).", "startOffset": 101, "endOffset": 124}, {"referenceID": 13, "context": "Adadelta (Zeiler, 2012) is used to automatically adapt the learning rate of each parameter ( = 10\u22126 and \u03c1 = 0.", "startOffset": 9, "endOffset": 23}, {"referenceID": 10, "context": "0 was normalized to the threshold (Pascanu et al., 2013).", "startOffset": 34, "endOffset": 56}, {"referenceID": 4, "context": "Dropout we also use dropout for our NMT baseline model and MEMDEC to avoid over-fitting (Hinton et al., 2012).", "startOffset": 88, "endOffset": 109}, {"referenceID": 11, "context": "\u2022 Coverage model: a state-of-the-art variant of attention-based NMT model (Tu et al., 2016) which improves the attention mechanism through modelling a soft coverage on the source representation.", "startOffset": 74, "endOffset": 91}, {"referenceID": 11, "context": "The coverage model on top of RNNsearch? has significantly improved upon its published version (Tu et al., 2016), which achieves the best published result on this training set.", "startOffset": 94, "endOffset": 111}, {"referenceID": 11, "context": "with COVERAGE mechanism(Tu et al., 2016), which is about 2 BLEU over than the published result after adding fast attention and dropout.", "startOffset": 23, "endOffset": 40}, {"referenceID": 2, "context": "5 Related Work There is a long thread of work aiming to improve the ability of RNN in remembering long sequences, with the long short-term memory RNN (LSTM) (Hochreiter and Schmidhuber, 1997) being the most salient examples and GRU (Cho et al., 2014) being the most recent one.", "startOffset": 232, "endOffset": 250}, {"referenceID": 1, "context": "Our work is also related to the recent work on machine reading (Cheng et al., 2016), in which the machine reader is equipped with a memory tape, enabling the model to directly read all the previous hidden state with an attention mechanism.", "startOffset": 63, "endOffset": 83}, {"referenceID": 8, "context": "In (Meng et al., 2015), Meng et.", "startOffset": 3, "endOffset": 22}], "year": 2016, "abstractText": "We propose to enhance the RNN decoder in a neural machine translator (NMT) with external memory, as a natural but powerful extension to the state in the decoding RNN. This memoryenhanced RNN decoder is called MEMDEC. At each time during decoding, MEMDEC will read from this memory and write to this memory once, both with content-based addressing. Unlike the unbounded memory in previous work(Bahdanau et al., 2014) to store the representation of source sentence, the memory in MEMDEC is a matrix with pre-determined size designed to better capture the information important for the decoding process at each time step. Our empirical study on Chinese-English translation shows that it can improve by 4.8 BLEU upon Groundhog and 5.3 BLEU upon on Moses, yielding the best performance achieved with the same training set.", "creator": "LaTeX with hyperref package"}}}