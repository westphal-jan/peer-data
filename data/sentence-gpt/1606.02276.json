{"id": "1606.02276", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "7-Jun-2016", "title": "Multilingual Visual Sentiment Concept Matching", "abstract": "The impact of culture in visual emotion perception has recently captured the attention of multimedia research. In this study, we pro- vide powerful computational linguistics tools to explore, retrieve and browse a dataset of 16K multilingual affective visual concepts and 7.3M Flickr images. First, we design an effective crowdsourc- ing experiment to collect human judgements of sentiment connected to the visual concepts. We then use word embeddings to repre- sent these concepts in a low dimensional vector space, allowing us to expand the meaning around concepts, and thus enabling insight about commonalities and differences among different languages. We compare a variety of concept representations through a novel evaluation task based on the notion of visual semantic relatedness. Based on these representations, we design clustering schemes to group multilingual visual concepts, and evaluate them with novel metrics based on the crowdsourced sentiment annotations as well as visual semantic relatedness. The proposed clustering framework enables us to analyze the full multilingual dataset in-depth and also show an application on a facial data subset, exploring cultural in- sights of portrait-related affective visual concepts.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n", "histories": [["v1", "Tue, 7 Jun 2016 19:40:00 GMT  (7864kb,D)", "http://arxiv.org/abs/1606.02276v1", null]], "reviews": [], "SUBJECTS": "cs.CL cs.CV cs.IR cs.MM", "authors": ["nikolaos pappas", "miriam redi", "mercan topkara", "brendan jou", "hongyi liu", "tao chen", "shih-fu chang"], "accepted": false, "id": "1606.02276"}, "pdf": {"name": "1606.02276.pdf", "metadata": {"source": "CRF", "title": "Multilingual Visual Sentiment Concept Matching", "authors": ["Nikolaos Pappas", "Miriam Redi", "Mercan Topkara", "Brendan Jou", "Hongyi Liu", "Tao Chen", "Shih-Fu Chang"], "emails": ["npappas@idiap.ch", "redi@yahoo-inc.com", "mercan@jwplayer.com", "bjou@ee.columbia.edu", "hongyi.liu@columbia.edu", "taochen@ee.columbia.edu", "sfchang@ee.columbia.edu", "permissions@acm.org."], "sections": [{"heading": null, "text": "Keywords Multilingual; Language; Cultures; Cross-cultural; Emotion; Sentiment; Ontology; Concept Detection; Social Multimedia"}, {"heading": "1. INTRODUCTION", "text": "Everyday, billions of users from all around the world share their visual memories in online photo sharing platforms. Web users speak hundreds of different languages, come from different countries and backgrounds. Such multicultural diversity also results in users representing the visual world in very different ways. For instance, [1] showed that Flickr users with different cultural backgrounds use different concepts to describe visual emotions. But how can we build tools to analyze and retrieve the multimedia data generated by such a diverse population?\nPermission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than ACM must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org.\nICMR\u201916, June 06-09, 2016, New York, NY, USA c\u00a9 2016 ACM. ISBN 978-1-4503-4359-6/16/06. . . $15.00\nDOI: http://dx.doi.org/10.1145/2911996.2912016\nMultimedia retrieval in a multilingual and multicultural environment cannot be independent of the spoken language used by users to describe their visual content. For example, among the vast content of photo sharing platforms such as in Flickr, it is easy to find pictures of traditional costumes from all around the world. However, a basic keyword search, e.g. traditional costumes, would not return multiculturally rich results: the returned content would very likely come from Western countries, especially from countries where English is the primary language. The problem we tackle is to analyze, index and develop a deeper understanding of multicultural content in the context of a large social photo sharing platform. A purely image-based analysis would not provide a complete understanding as it clusters visually-similar images together, missing the differences between cultures, e.g. how an old house or good food look in each culture. We mitigate the problems of pure image-based analysis with the aid of analytical language tools.\nIn this paper, we focus on one dimension which characterizes users\u2019 cultural background, namely their spoken language, and we build the first complete framework for analyzing, exploring, and retrieving multilingual emotion-biased visual concepts. This allows to retrieve examples of concepts such as traditional costumes from visual collections of different languages (see Fig. 1). Our goal is to study the lexical differences of sentiment-biased visual concepts across multiple languages from the MVSO dataset [1]. Complementarily, we also seek to investigate the visual differences for images related to similar visual concepts across languages. To achieve this, it is essential to match lexical expressions of concepts from one language to another. One way to achieve this is through exact\nar X\niv :1\n60 6.\n02 27\n6v 1\n[ cs\n.C L\n] 7\nJ un\n2 01\n6\nmatching, arising from the translation of all languages to a single one, e.g. English. However, given that lexical choices for the same concepts vary across languages, the exact matching of multilingual concepts has a small coverage across languages. To overcome this sparsity issue, we propose an approximate matching approach. We represent multilingual concepts in a common semantic embedding based on pre-trained word embeddings. This allows us to compute the semantic proximity or distance between visual concepts and cluster concepts from multiple languages. Furthermore, the approximate matching enables a better connectivity between visual concepts of different languages. This allows discovery of multilingual clusters of visual concepts, whereas exact matching clusters are mostly dominated by a single language.\nThe contributions of our work can be summarized as follows: (1) We design a crowdsourcing experiment to annotate the sentiment score of visual concepts from 11 languages in MVSO, and make the resulting data publicly available. (2) We evaluate and compare a variety of unsupervised distributed word and concept representations. To do so, we define a novel evaluation metric called visual semantic relatedness, defined as the cosine distance of the multilingual concept co-occurrence on realworld images from MVSO. By analyzing sentiment values across languages and their translations, we prove the need of effective multilingual sentiment analysis tools. (3) We design new tools to evaluate sentiment and semantic consistency on various visual concept clustering techniques that allow us to organize MVSO in an hierarchical manner by obtaining rich multilingual clusters of affective concepts. (4) We demonstrate the applicability of the proposed approaches with qualitative analysis and a novel case study on analyzing portrait images in MVSO. We find that Eastern and Western languages tend to attach different visual concepts to portrait images, but that many languages similarly attach positive concepts to face pictures."}, {"heading": "2. RELATED WORK", "text": "Research on distributed word representations [2, 3, 4, 5] have recently extended to multiple languages either by using bilingual word alignments or parallel corpora to transfer linguistic information from multiple languages. For instance, [6] proposed to learn distributed representations of words across languages by using a multilingual corpus from Wikipedia. [7, 8] proposed to learn bilingual embeddings in the context of neural language models utilizing multilingual word alignments. [9] proposed to learn joint-space embeddings across multiple languages without relying on word alignments. Similarly, [10] proposed auto-encoder-based methods to learn multilingual word embeddings. A limitation when dealing with many languages is the scarcity of data for all pairs. In this study, we use a pivot language to align the multiple languages using machine translation.\nTo our knowledge, the visual context of sentiment concepts for evaluating multilingual embeddings have not been considered before. However, studies on multimodal distributional semantics have combined visual and textual features to learn more informed word embeddings and have used the notion of semantics [11, 12] and visual similarity to evaluate word embeddings [13, 14]. Furthermore, there are studies which have combined language and vision for image caption generation and retrieval [15, 16, 17, 18] based on multimodal neural language models. We argue that our evaluation metric is a rich resource for learning more informed multimodal embeddings which can benefit these systems. Perhaps the most related study to ours is in [19] which aimed to learn visually grounded word embeddings to capture visual notions of semantic relatedness using abstract visual scenes. Our differences are that we focus on\nsentiment concepts and we define visual semantic relatedness based on real-world images which are annotated by community users of Flickr instead of abstract scenes."}, {"heading": "3. VISUAL SENTIMENT OF CONCEPTS", "text": "In this work, we make use of the MVSO dataset [1]. One drawback of MVSO is that the sentiment scores assigned to affective visual concepts were automatically inferred through sentiment analysis tools. Although such tools have achieved impressive performances in the recent years, they are typically based on text modalities alone. To counter this, we designed a crowdsourcing experiment with CrowdFlower1 to annotate the sentiment of the multilingual adjective-noun pairs (ANPs) in MVSO. We considered 11 out of 12 languages in MVSO, leaving out Persian due to the limited number of ANPs. We constructed separate tasks for each language, using all ANPs in MVSO for that language. Task Interface. We asked crowdsourcing workers to evaluate the sentiment value of ANPs on a scale from 1 to 5. We provided annotators with intuitive instructions, along with examples ANPs with different sentiment values. Each task showed five ANPs from a given language along with Flickr images associated to those ANPs. Annotators rated the sentiment expressed by each ANP, choosing between \u201cvery negative,\u201d \u201cslightly negative,\u201d \u201cneutral,\u201d \u201cslightly positive\u201d or \u201cvery positive\u201d. Task Setup. The sentiment of each ANP was judged by five or more independent workers. Similar to the MVSO setup, we required that workers were both native speakers of the task\u2019s language and highly ranked on the platform. We also developed a subset of test questions with an expert-labeled gold standard: to access a crowdsourcing task, workers needed to correctly answer 7 of 10 test questions. Their performance was also monitored throughout the task by randomly inserting a test question in each task. Crowdsourcing Results. To assess the quality of the collected annotations, we computed the level of agreement between contributors. And although sentiment assessment is intrinsically a subjective task, we found an average agreement of \u223c68%. We found an average correlation of 0.54 between crowdsourced sentiment scores and the automatically assigned sentiment scores in [1]. Although this value is reasonably high, it still shows that the two sets of scores do not completely overlap. A high-level summary of the average sentiment collected per language is shown in Figure 2. Unlike the findings of previous work using automatic sentiment scores [1], we find that visual concepts from Chinese and Russian users carry more positive sentiment on the average when compared to concepts shared by speakers of other languages."}, {"heading": "4. EXACT CONCEPT MATCHING", "text": "To match visual sentiment concepts (ANPs) across languages we translated them from each language L = {lij | i = 1 . . .m, j = 1 . . . ni} to the concepts of a pivot languageC = {ci | i = 1 . . . N} using the Google Translate API2. We selected English as the pivot 1http://www.crowdflower.com 2https://cloud.google.com/translate\nlanguage because it has the most complete translation resources (parallel corpora) for each of the other languages due to its popularity in relevant studies. For instance, the concepts chien heureux (French), perro feliz (Spanish) and gl\u00fccklicher hund (German) are translated to the English concept happy dog. Rightly so, one would expect that the visual sentiment concepts in the pivot language might have shifted in terms of sentiment and meaning as a result of the translation process. And so, we examine and analyze the effects of translation to the sentiment and meaning of the multilingual concepts as well as the matching coverage across languages."}, {"heading": "4.1 Sentiment and Meaning Shift", "text": "To quantitatively examine the effect of translation to the sentiment of concepts, we used the crowdsourced sentiment values and count the number of concepts for which the sentiment is shifted after translation in English. The higher this number for a given language, the higher the specificitiy of the visual sentiment for that language. To avoid counting sentiment shifts caused by small sentiment values we define a sentiment threshold t below which we do not consider sign changes, as follows: |sent(ci)| > t. Table 1 displays the percentage of concepts with shifted sign due to translation. The percentages are on average about 33% for t = 0. The highest percentage of sign shift is 60% from Arabic and the lowest percentage is 18.6% for Dutch. Moreover, the percentage of concepts with shifted sign decreases for most languages as we increase the absolute sentiment value threshold t from 0 to 0.3. This result is particularly interesting since it suggests that visual sentiment understanding can be enriched by considering the language dimension.\nThe translation can affect also the meaning of the original concept in the pivot language. For instance, a concept in the original language which has intricate compound words (adjective and noun) could be translated to simpler compound words. This might be due to the lack of expressivity of the pivot language, or to compound words with shifted meaning, because of translation mistake, language idioms, or lack of large enough context. For example, \u6c11 \u4e3b\u6cd5\u6cbb (Chinese) is translated to democracy and the rule of law in English, while passo grande (Italian) is translated to plunge and marode sch\u00f6nheit (German) is translated in to ramshackle beauty. Examining the extent of this effect quantitatively is very costly because it requires language experts from all languages at hand. Sentiment and meaning shift effects are very interesting open prob-\nlems, however, in this study, we focus specifically on how to increase the matching coverage among languages rather than what is the optimal modeling of sentiment and meaning, hence we leave them as future work."}, {"heading": "4.2 Matching Coverage", "text": "The matching coverage is an essential property for multilingual concept matching and clustering. We examine this property by performing k-Means clustering of multilingual concepts, and counting the number of concepts between two languages that belong to the same cluster. This reveals the connectivity of language clusters based on exact matching, as shown in Figure 3 (a) for the top8 most popular languages in MVSO. From the connection stripes which represent the number of concepts between two languages, we can observe that, when using exact matching, concept clusters are dominated by single languages. For instance, in all the languages there is a connection stripe which connects back to the same language: this means that many clusters contain monolingual concepts. Another example which highlights the disadvantage of exact matching is that out of all the German translations (781), the ones matched with Dutch concepts (39) were more numerous than the ones matched with Chinese concepts (23). This was striking given that there were less (340) translations from Dutch than from Chinese (472). This demonstrates that the matching of concepts among languages is generally very sparse and does not depend necessarily on the number of translated concepts; this hinders our ability to compare concepts across languages in a unified manner.\nMoreover, we want be able to know the relation among concepts from original languages where we cannot have a direct translation. To address the limitations of exact matching coverage of concepts, we propose below to expand the exact match translation and approximate translations based on compound word semantics."}, {"heading": "5. APPROXIMATE CONCEPT MATCHING", "text": "We saw that matching sentiment-biased visual concepts in different languages with exact matching has several limitations, in terms of translation correctness and concept coverage. To overcome these limitations, we relax the exact condition for matching multilingual concepts, and instead we approximately match concepts based on their semantic meaning. Intuitively, in order to match concepts from different languages, we need a proximity (or distance) measure reflecting how \u2018close\u2019 or similar concepts are in the semantic distance space. This enables to achieve our main goal: comparing visual concepts cross-lingually, and cluster them in to multilingual groups. When using, approximate matching the clustering connectivity between languages is greatly enriched, as shown in Figure 3 (b): connection stripes are more evenly distributed for all languages. To learn such representations of meaning we make use of the recent advances in distributional lexical semantics [4, 5, 20, 21] utilizing the skip-gram model provided by word2vec toolkit3 trained on large text corpora."}, {"heading": "5.1 Distributed Word Representations", "text": "To represent words in a semantic space we use unsupervised word embeddings based on the skip-gram model via word2vec. Essentially, the skip-gram model aims to learn vector representations for words by predicting the context of a word in a large corpus. The context is defined as a window of w words before and w after the current word. We consider the following corpora in English on which the skip-gram model is trained:\n1. Google News: A corpus of news which contains 100 billion tokens and 3,000,000 unique words which have at least 5 occurrences from [22]. News are typically used to describe real-world events which contains accurate word meanings, however it has indirect relevance to visual content.\n2. Wikipedia: A corpus of Wikipedia articles which contains 1.74 billion tokens and 693,056 unique words which have at least 10 occurrences. The pre-processed text of this corpus was obtained from [23]. Wikipedia articles are more thorough descriptions of real-world events, entities, objects, and concepts. Similar to Google news, the visual content is indirectly connected to the word usage.\n3. Wikipedia + Reuters + Wall Street Journal: A mixture corpus of Wikipedia articles, Wall Street Journal (WSJ) and Reuters news which contains 1.96 billion tokens and 960,494 unique words which have at least 10 occurrences. The preprocessed text of this corpus was obtained from [23]. This combination of news articles and Wikipedia articles captures a balance between these two different types of word usage.\n4. Flickr 100M: A corpus of image metadata which contains 0.75 billion tokens and 693,056 unique words (with frequency higher than 10) available from Yahoo! 4. For the first time in this paper, this corpus is used to train skip-gram models. In contrast to the previous corpora, the description of real-world images contains spontaneous word usage which is directly related to visual content. Hence, we expect it to provide embeddings able to capture visual properties.\nFor the Google News corpus, we used pre-trained embeddings of 300 dimensions with a context window of 5 words provided by [22]. For the other corpora, we trained the skip-gram model with a 3https://code.google.com/p/word2vec 4http://webscope.sandbox.yahoo.com\ncontext window w of 5 and 10 words, fixing the dimensionality of the word embeddings to 300 dimensions. In addition to training the vanilla skip-gram model on word tokens we propose to train each of the corpora (except Google news due to lack of access to original documents used for training) by treating ANPs (concepts) as unique tokens. This pre-processing step allows the skip-gram model to directly learn concepts embeddings while taking advantage from the word contextual information over the above corpora."}, {"heading": "5.2 Distributed Concept Representations", "text": "To represent concepts in a semantic space we use the word embeddings in the pivot language (English), and compose the representation of a concept based on its compound words. Each sentimentbiased visual concept ci comprises zero or more adjective and one or more noun words (as translation does not necessarily preserve the adjective-noun pair structure of the original phrase). Given the word vector embeddings, ~xadj and ~xnoun, we compute the concept embedding ~ci using the sum operation for composition: ~ci = ~xadj + ~xnoun or the concept embedding ~ci which is directly learned from the skip-gram model. This enables the comparison of multilingual concepts using the pivot language (English). At this stage, we note that there are several other ways to define composition of short phrases, e.g. [24, 25, 22]; however, here, we focus on the type of corpora used for obtaining the word embeddings rather than on the composition function."}, {"heading": "6. EVALUATION", "text": ""}, {"heading": "6.1 Visual Semantic Relatedness", "text": "Evaluating word embeddings learned from text is typically performed on tasks such as semantic relatedness, syntactic relations and analogy relations [4]. These tasks are not able to capture concept properties related to visual content. For instance, while deserted beach and lonely person seem unrelated according to text, in the context of an image they share visual semantics. An individual person in a deserted beach gives to a remote observer the impression of loneliness. To obtain groundtruth for visual semantic distance we collected co-occurrence statistics of ANPs (concepts) translated in English from 12 languages by analyzing the MVSO image tags (1,000 samples per concept), as displayed in Table 2. We compute the visually anchored semantic distance using cosine distance between two co-occurrence vectors: d(~hi,~hj) = 1\u2212 cosine(~hi,~hj). We now compare the performance of the various concept embeddings of Section 5.1 on the visual semantic relatedness task. Figure 4 displays their performance over all languages in terms of Mean Squared Error (MSE), and Table 3 displays their performance per language accoring to the MSE score for all the pairs of concept embeddings ~ci and ~cj , as:\nMSE = 1\nT N\u2211 i |{i,...,N}|\u2211 j:j 6=i & Uij 6=0 (d(~ci,~cj)\u2212 d(~hi,~hj)2, (1)\nwhere Uij is the co-occurrence between concepts i and j, and T is the total number of comparisons, that is T = (N\n2\u2212N\u2212|{Uij=0}|) 2\n. Note that to avoid redundancy, we have excluded from the comparisons the diagonal and the lower triangular matrix of the concept co-occurrence matrix (N \u00d7 N ). This does not affect the average mean squared error calculation because the cosine distance is symmetric. From the comparisons T we further exclude ANP pairs with zero co-occurrence to penalize embeddings which tend to separate most of the concepts apart.\nThe best performance over all languages (Figure 4) is achieved by the flickr-anp-l (w=5) embeddings, followed by the wiki-anp-l (w=5) embeddings. The superior performance of flickr-anp-l (w=5) is attributed to its ability to learn directly the embedding of a given concept (ANP). The lowest performance is observed by wiki-reuwsj (w=10) and flickr (w=10). The larger context (w=10) performed worse than the smaller context (w=5); it appears that the semantic relatedness prediction over all languages does not benefit from large contexts. When the concept embeddings are evaluated per language in Table 3 we obtain slightly different ranking of the methods. In the languages with the most data, namely English (EN), Spanish (ES), Italian (IT), French (FR) and Chinese (ZH), the ranking is similar as before, with flickr-anp-l (w=5), flickr-anp (w=5) and wiki-anp (w=5), wiki-anp-l (w=5) embeddings having the lowest error in predicting semantic relatedness.\nGenerally, we observed that for well-resourced languages the quality of concept embeddings learned by a skip-gram model improves when the model is trained using ANPs as tokens (both when using directly learned concept embeddings or composition of word embeddings with sum operation). Furthermore, the usage of learned embeddings abbreviated with \u2212l on the top-5 languages outperforms on average all other embeddings in English, Spanish and Chinese languages and performs similar to the best embeddings on Italian and French. In the low resourced languages the results are the following: in German (DE) language the lowest error is from flickr-anp (w=10), in the Dutch (NL) and Russian (RU) is the flickr (w=10). Lastly, the lowest error in the Turkish (TR), Persian (FA)\nand Arabic (AR) languages is from wiki-reu-wsj (w=10). It appears that for the languages with small data the large context benefits the visual semantic relatedness task.\nMoreover, the performance of embeddings with a small context window (w = 5), are outperformed by the ones that use a larger one (w = 10) as the pool of examples of the languages narrows. This is likely due to the different properties which are captured by different context windows, namely more abstract semantic and syntactic relations with a larger context window and more specific relations with a smaller one. Note that the co-occurrence of concepts in MVSO images is computed on the English translations and hence some of the syntactic properties and specific meaning of words of low-resourced languages might have vanished due to errors in the translation process. Lastly, the superior performance of the embeddings learned from the Flickr 100M corpus in the top-5 most\nresourced languages, validates our hypothesis that word usage directly related to the visual content helps to learn concept embeddings with superior visual semantic properties."}, {"heading": "6.2 Multilingual Concept Clustering", "text": "Given a common way to represent multilingual concepts, we are now able to cluster them. We normalize the concept vectors to perform k-means clustering over Euclidean distances. We perform part-of-speech tagging on the translation to extract the representative noun or adjective with TreeTagger [26]. We then experimented with two types of clustering approaches: a one-stage and a twostage approach. The one-stage approach directly clusters all the concept vectors using k-means. The two-stage clustering operates first on the noun or adjective word vectors and then second on concept vectors. Here, we first split the translated concepts into different groups based on the clusters their nouns belong to, and then run another round of k-means clustering within the groups formed in the first stage. In the case when a translation phrase has more than one noun, we select the last noun as the representative and use it in the first stage of clustering. The second stage uses the sum of vectors for all the concepts that comprise that translation for clustering. We also experimented with first clustering by adjectives and then by nouns using the same process.\nWe experimented with different number of clusters k. For the two-stage clustering, we adjust the number of clusters k in the last stage based on the number of concepts in each first-stage cluster, e.g. concepts in each noun-cluster ranged from 3 to 253 in one setup, to bring the final number of clusters in two-stage clustering to our desired number for fair comparison of clustering with different setups. We ended up with clusters such as beautiful music, beautiful concert, beautiful singer that clusters concepts like musique magnifique (French), bella musica (Italian), bellissimo concerto. While noun-first clustering brings concepts that talk about similar objects, e.g. estate/unit/property/building, adjective-based clustering yields concepts about similar and closely related emotions, e.g. grateful/festive/joyous/floral/glowing/delightful (examples are from two-stage clustering with the Google News corpus).\nTo evaluate the clustering of affective visual concepts, we consider two dimensions: (1) Semantics: ANPs are concepts, so we seek a clustering method to group ANPs with similar semantic meaning, such as for example beautiful woman and beautiful lady, (2) Sentiment: Given that ANPs have an affective bias, we need a clustering method that groups ANPs with similar sentiment values, thus ensuring the integrity of ANPs sentiment information after clustering. To evaluate clustering over both dimensions, we score clustering methods using two evaluation metrics specifically designed for this task, one for semantics and one for sentiment, and then combine the resulting ratings into a single score. Semantic Consistency. Each clustering method produces k ANPs clusters, out of which C contains two or more ANPs. For each of these multi-ANP clusters, each withNc ANPs, we compute the average visually grounded semantic distance (Eq. 1) between all pairs of ANPs, and then average it over all C clusters, thus obtaining a Semantic Consistency semC metric for a given clustering method:\nsemC = 1\nC C\u2211 c=1\n\u2211|{i,...,Nc}| j:j 6=i & Uij 6=0 d(ANPc,i,ANPc,j)\nNc (2)\nSentiment Consistency. For each multi-ANP cluster c, we compute a sentiment quantization error, namely the average difference between the sentiment of each ANP in the cluster, and the average sentiment of the cluster. So, given senc = \u2211Nc i=1 sen(ANPi)/Nc as the average sentiment for a clusterc, we obtain a Sentiment Con-\nsistency senC metric for a given clustering method as follows:\nsenC = 1\nC C\u2211 c=1 \u2211Nc i=1(sen(ANPc,i)\u2212 senc) 2 Nc (3)\nClustering Evaluation Results. We evaluate all the clustering methods using these two scoring methods and an overall consistency metric (average of semantic and sentiment consistencies). The lower the value of the metrics, the higher the quality of the clustering method. We observe that semantic consistency and sentiment consistency are actually highly related. When we correlate the vector containing semantic consistency scores for all clustering methods with the vector containing sentiment consistency scores, we find that the Pearson\u2019s coefficient is around 0.7, thus suggesting that the higher the semantic relatedness of the clusters resulting from one method, the higher their respective sentiment coherence.\nWe observed that as the total cluster size k increases, the consistency within a cluster also increases, even though the images and ANPs come from different languages and though the source of training corpus for word vectors did not always come from social content. Word vectors trained on Flickr data provided clustering with top consistency in all evaluation measures. Based on the analysis of sentiment consistency, we observed that 9 out of top 10 two-stage clustering setups were based on adjective-first clustering (only 9th being noun-first Google News clustering with 1000 clusters). This confirms our intuition that similar sentiments are clustered together when we first cluster ANPs with similar adjectives. Lastly, the vectors trained on full ANP tokens lead to increased semantic consistency, similar to on the full corpus (Section 6.1)."}, {"heading": "7. PORTRAIT CONCEPT CLUSTERING", "text": "The proposed multilingual concept clustering framework can be a useful tool for exploring and analyzing any large, multilingual collections of visual concepts. As an example application, we applied this concept matching framework to study how affective concepts attach to human portraits, i.e. photos with faces, through the viewing lens of different languages."}, {"heading": "7.1 Portrait-based Sentiment Ontology", "text": "Portrait and face-centric photography has been a subject of research in multiple disciplines for years. Facial perception is among\nthe most developed human capabilities, where our brains even contain a dedicated sub-network of neurons for face processing [27]. Recently, computational understanding portrait modeling has attracted much attention from the multimedia community, e.g. in computational aesthetics [28], animated GIFs [29], and social dynamics [30]. Here, we seek to unpack what sentiment-biased visual concepts, specifically ANPs, languages attach to faces. Face Detection and ANP Filtering. To obtain a corpus of visual concepts relating to faces, we ran a frontal face detector [31] which projects images onto a normalized pixel difference feature space and performs quadtree-based face detection. A total of 3,858,869 faces were detected across the 7,368,364 images in the MVSO image dataset [1]. Over 53.67% of these detections came from the English image subset (2,071,078 detections), where the next leading language subset was Spanish at 23.68% (913,596 detections). We then computed a portrait score for each ANP which we define as the ratio of detected faces to all images in each ANP. We then selected the subset of ANPs whose portrait score was greater than 0.6. To ensure statistical significance, we only considered languages with 20 or more face-dominated ANPs: Turkish, Russian, German, Chinese, French, Spanish, Italian and English. Of the 11,832 concepts from the full MVSO dataset, we retained 2,345 face ANPs. We found that in general, detected faces from the French and German datasets we larger in size on average than other languages. In addition, images originating from the Italian subset typically contained more than one person while images in the Chinese and Turkish subset tended to contain mostly single-subject portraits. Concept Sentiment and Face Portraits. To explore the sentiment correlations of different languages to the presence of faces, we computed the Pearson\u2019s correlation coefficient \u03c1 for each language between ANP portrait scores and the ANP sentiment values, as shown in Table 5. The higher the correlation, the higher the tendency of a given language to associate positive sentiment with a face image. Here, for all languages except Turkish, the presence of portraits in an ANP image pool tended to be positively correlated with the ANP sentiment. In particular, the languages having the strongest tendency to attach positive sentiments to portraits are Russian and Chinese."}, {"heading": "7.2 Multilingual Portraits", "text": "We sought to investigate how similar/different languages are with regard to affective visual concepts (ANPs) and their face images. We clustered face ANPs from the subset of eight languages using our approximate match-based clustering techniques, and evaluated different clustering approaches to find that the single-step clustering over Flickr-trained word2vec vector with w = 5 and k = 1000 gave us the best results when combining semantic and sentiment consistency metrics. This method output 1,000 multilingual clusters of affective visual concepts related to portraits. This provides a powerful tool to analyze the visual concept preferences for different languages, i.e. if the ANPs of two languages fall often in similar clusters, such languages tend to attach similar concepts to face images.\nLanguages, Sentiments, Face Sizes. Among the clusters found by this method, around 60% are monolingual clusters: English and Spanish are the languages with the highest percentage of monolingual clusters (\u223c32% and 31% respectively), probably due to the large number of ANPs, implying a wider vocabulary compared to other languages. Around 22% of the face ANP clusters are bilingual, out of which 40% contains 2 of the 4 Western-most languages in the corpus (French, Italian, English and Spanish), while the others contain a mixture of other languages. The remaining 18% of clusters show three languages or more. But what is the relation between multilingual clusters and portrait sentiment? Do languages agree more on positive or negative sentiment for similar visual concepts? To answer these, we computed the correlation between the number of languages falling into each cluster and the average sentiment of the ANPs in that cluster. These two dimensions statistically significantly correlate with a coefficient of 0.13, showing that different languages tend to associate similar visual concepts to portraits when concepts carry positive sentiment.\nWe also analyze the relation between cluster multilingual-ness and face sizes. Do visual concepts shared by different languages refer to portraits with bigger or smaller faces? We compute here the correlation between the number of languages falling into each cluster and the average face sizes for the ANPs in that cluster. The correlation coefficient stands at 0.17, showing that, the bigger the average face sizes of portraits related to a visual concept, the higher the possibility that different languages share such concept. Clustering Analysis. Which languages are more similar when tagging portraits? To further understand language-specific concepts used when tagging face images, we perform a multivariate analysis of language distribution across visual concepts. To better understand which groups of languages tend to attach similar affective concepts to face images, we proceed as follows. We create eight kdimensional vectors, one for each language. Each element of such vectors corresponds to the number of ANPs falling into each cluster, normalized by the total number of ANPs for a given language. Finally, we cluster these vectors using k-means with cosine distance, progressively raising k from 2 to 6, thus separating languages into different groups, as shown in Figure 5. The binary subdivision of the languages in two clusters (k = 2) shows immediately a clear separation between more Eastern (Turkish, Chinese, Russian) versus Western (Italian, Spanish, French, English, German) languages. When we raise the number of clusters to 3, Turkish is the first one to detach from the Eastern clusters, suggesting that Turkish images tend to have more unique ways to assign concepts to portraits. Within the Western languages, when k = 4 we see a separation: Italian gets clustered with Spanish and English, while French gets clustered with German. Chinese and Russian become independent clusters for k = 5. Finally, when raising k to 6, Western languages split again: English becomes an independent cluster, leaving two bilingual clusters: Italian/Spanish and French/German. What Do Different Languages Say About Portraits? According to our clustering method, \u223c3% of the clusters contain five or more languages. The limited size of this data allows us to proceed with manual inspection, to understand the topics of the ANPs falling in highly multilingual clusters. The most highly multilingual cluster contains 8 languages and 20 ANPs: its main topic is about little guy or little girl (e.g. piccola bimba in Italian, or petit fille in French). One interesting observation is that not all languages agree on the sentiment value for this concept: while Chinese and Turkish give a score slightly below 3, Italian, Spanish and Russian languages consider it a very positive concept, having respectively an average sentiment value of 4.0, 4.0 and 4.6, respectively, for this cluster. The\nsecond biggest cluster, spanning seven languages (all apart from Chinese) with 24 ANPs, contained concepts like gorgeous girl in English or belle fille in French. Here, all languages agree on the highly positive value of this concept sentiment. Other highly multilingual noun clusters contain concepts related to happy children, young women, healthy food, beautiful women, etc, and also negative concepts such as sexual violence."}, {"heading": "8. CONCLUSIONS & FUTURE WORK", "text": "In this study, we showed that visual sentiment concepts from multiple languages can be effectively represented in a common semantic space using a pivot language (English) and existing advances in distributional semantics of words. The best results on the prediction of visually grounded semantic distance was achieved by a skip-gram model trained on real-world image metadata, namely titles and descriptions from the Flickr 100M corpus, with a summed combination of concept word embeddings or also by direct learning. This enabled multilingual clustering of visual sentiment concepts in 11 languages, and allowed us to better hierarchically organize an ontology in [1] as well as provide deep analysis into portrait imagery from a multilingual perspective.\nIn the future, we plan to learn visually grounded concept embeddings in multiple languages directly from the groundtruth cooccurrence data (Section 5.2), e.g. by modeling the proposed visual semantic distance. An essential element for achieving this goal is to semantically align multiple languages using parallel corpora, as it has been effectively done in previous studies in multilingual representation learning. Finally, we plan to evaluate such word and concept embeddings on visual sentiment concept prediction, concept recommendation and caption generation."}, {"heading": "9. REFERENCES", "text": "[1] B. Jou, T. Chen, N. Pappas, M. Redi, M. Topkara, and S.-F. Chang, \u201cVisual\naffect around the world: A large-scale multilingual visual sentiment ontology,\u201d in ACM International Conference on Multimedia, (Brisbane, Australia), pp. 159\u2013168, 2015.\n[2] J. Turian, L. Ratinov, and Y. Bengio, \u201cWord representations: A simple and general method for semi-supervised learning,\u201d in 48th Annual Meeting of the Association for Computational Linguistics, ACL \u201910, (Uppsala, Sweden), pp. 384\u2013394, 2010. [3] R. Collobert, J. Weston, L. Bottou, M. Karlen, K. Kavukcuoglu, and P. Kuksa, \u201cNatural language processing (almost) from scratch,\u201d Journal of Machine Learning Research, vol. 12, pp. 2493\u20132537, 2011. [4] T. Mikolov, K. Chen, G. Corrado, and J. Dean, \u201cEfficient estimation of word representations in vector space,\u201d CoRR, vol. abs/1301.3781, 2013. [5] J. Pennington, R. Socher, and C. D. Manning, \u201cGloVe: Global vectors for word representation,\u201d in Empirical Methods in Natural Language Processing, pp. 1532\u20131543, 2014. [6] R. Al-Rfou, B. Perozzi, and S. Skiena, \u201cPolyglot: Distributed word representations for multilingual NLP,\u201d CoRR, vol. abs/1307.1662, 2013. [7] A. Klementiev, I. Titov, and B. Bhattarai, \u201cInducing crosslingual distributed representations of words,\u201d in Proceedings of COLING 2012, (Mumbai, India), pp. 1459\u20131474, 2012.\n[8] W. Y. Zou, R. Socher, D. Cer, and C. D. Manning, \u201cBilingual word embeddings for phrase-based machine translation,\u201d in Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, (Seattle, WA, USA), pp. 1393\u20131398, 2013. [9] K. M. Hermann and P. Blunsom, \u201cMultilingual models for compositional distributed semantics,\u201d in Annual Meeting of the Association for Computational Linguistics, (Baltimore, Maryland), pp. 58\u201368, 2014.\n[10] A. P. S. Chandar, S. Lauly, H. Larochelle, M. M. Khapra, B. Ravindran, V. C. Raykar, and A. Saha, \u201cAn autoencoder approach to learning bilingual word representations,\u201d CoRR, vol. abs/1402.1454, 2014. [11] F. Hill, R. Reichart, and A. Korhonen, \u201cSimlex-999: Evaluating semantic models with (genuine) similarity estimation,\u201d CoRR, vol. abs/1408.3456, 2014. [12] E. Bruni, N. K. Tran, and M. Baroni, \u201cMultimodal distributional semantics,\u201d Journal of Artificial Intelligence Research, vol. 49, pp. 1\u201347, Jan. 2014. [13] C. Silberer and M. Lapata, \u201cLearning grounded meaning representations with autoencoders,\u201d in 52nd Annual Meeting of the Association for Computational Linguistics, (Baltimore, Maryland), pp. 721\u2013732, June 2014. [14] A. Lazaridou, N. T. Pham, and M. Baroni, \u201cCombining language and vision with a multimodal skip-gram model,\u201d in Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, (Denver, Colorado), pp. 153\u2013163, 2015. [15] A. Karpathy, A. Joulin, and F. Li, \u201cDeep fragment embeddings for bidirectional image sentence mapping,\u201d in Advances in Neural Information Processing Systems 27, pp. 1889\u20131897, Curran Associates, Inc., 2014. [16] R. Kiros, R. Salakhutdinov, and R. S. Zemel, \u201cUnifying visual-semantic embeddings with multimodal neural language models,\u201d CoRR, vol. abs/1411.2539, 2014. [17] R. Socher, A. Karpathy, Q. V. Le, C. D. Manning, and A. Y. Ng, \u201cGrounded compositional semantics for finding and describing images with sentences,\u201d TACL, vol. 2, pp. 207\u2013218, 2014. [18] J. Mao, W. Xu, Y. Yang, J. Wang, and A. L. Yuille, \u201cExplain images with multimodal recurrent neural networks,\u201d CoRR, vol. abs/1410.1090, 2014. [19] S. Kottur, R. Vedantam, J. M. F. Moura, and D. Parikh, \u201cVisual word2vec (vis-w2v): Learning visually grounded word embeddings using abstract scenes,\u201d CoRR, vol. abs/1511.07067, 2015. [20] T. Schnabel, I. Labutov, D. Mimno, and T. Joachims, \u201cEvaluation methods for unsupervised word embeddings,\u201d in Conference on Empirical Methods in Natural Language Processing, (Lisbon, Portugal), pp. 298\u2013307, 2015. [21] O. Levy, Y. Goldberg, and I. Dagan, \u201cImproving distributional similarity with lessons learned from word embeddings,\u201d Transactions of Association for Computational Linguistics, vol. 3, pp. 211\u2013225, 2015. [22] T. Mikolov, I. Sutskever, K. Chen, G. S. Corrado, and J. Dean, \u201cDistributed representations of words and phrases and their compositionality,\u201d in Advances in Neural Information Processing Systems 26, pp. 3111\u20133119, 2013. [23] R. Lebret and R. Collobert, \u201cWord embeddings through hellinger pca,\u201d in Conference of the European Chapter of the Association for Computational Linguistics, (Gothenburg, Sweden), pp. 482\u2013490, 2014. [24] M. Baroni and R. Zamparelli, \u201cNouns are vectors, adjectives are matrices: Representing adjective-noun constructions in semantic space,\u201d in Conference on Empirical Methods in Natural Language Processing, (Cambridge, MA, USA), pp. 1183\u20131193, 2010. [25] R. Socher, B. Huval, C. D. Manning, and A. Y. Ng, \u201cSemantic compositionality through recursive matrix-vector spaces,\u201d in Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning, (Jeju Island, Korea), pp. 1201\u20131211, 2012. [26] H. Schmid, \u201cProbabilistic part-of-speech tagging using decision trees,\u201d in International Conference on New Methods in Language Processing, (Manchester, UK), 1994. [27] W. A. Freiwald and D. Y. Tsao, \u201cNeurons that keep a straight face,\u201d National Academy of Sciences, vol. 111, no. 22, pp. 7894\u20137895, 2014. [28] M. Redi, N. Rasiwasia, G. Aggarwal, and A. Jaimes, \u201cThe beauty of capturing faces: Rating the quality of digital portraits,\u201d in IEEE International Conference and Workshops on Automatic Face and Gesture Recognition, (Ljubljana, Slovenia), pp. 1\u20138, 2015. [29] B. Jou, S. Bhattacharya, and S.-F. Chang, \u201cPredicting viewer perceived emotions in animated GIFs,\u201d in ACM International Conference on Multimedia, (Orlando, Florida, USA), pp. 213\u2013216, 2014. [30] S. Bakhshi, D. A. Shamma, and E. Gilbert, \u201cFaces engage us: Photos with faces attract more likes and comments on instagram,\u201d in ACM Conference on Human Factors in Computing Systems, (Toronto, ON, Canada), pp. 965\u2013974, 2014. [31] S. Liao, A. K. Jain, and S. Z. Li, \u201cA fast and accurate unconstrained face detector,\u201d IEEE Transactions on Pattern Analysis and Machine Intelligence, vol. 38, pp. 211\u2013223, Feb 2016."}], "references": [{"title": "Visual affect around the world: A large-scale multilingual visual sentiment ontology", "author": ["B. Jou", "T. Chen", "N. Pappas", "M. Redi", "M. Topkara", "S.-F. Chang"], "venue": "ACM International Conference on Multimedia, (Brisbane, Australia), pp. 159\u2013168, 2015.", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2015}, {"title": "Word representations: A simple and general method for semi-supervised learning", "author": ["J. Turian", "L. Ratinov", "Y. Bengio"], "venue": "48th Annual Meeting of the Association for Computational Linguistics, ACL \u201910, (Uppsala, Sweden), pp. 384\u2013394, 2010.", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2010}, {"title": "Natural language processing (almost) from scratch", "author": ["R. Collobert", "J. Weston", "L. Bottou", "M. Karlen", "K. Kavukcuoglu", "P. Kuksa"], "venue": "Journal of Machine Learning Research, vol. 12, pp. 2493\u20132537, 2011.", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2011}, {"title": "Efficient estimation of word representations in vector space", "author": ["T. Mikolov", "K. Chen", "G. Corrado", "J. Dean"], "venue": "CoRR, vol. abs/1301.3781, 2013.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2013}, {"title": "GloVe: Global vectors for word representation", "author": ["J. Pennington", "R. Socher", "C.D. Manning"], "venue": "Empirical Methods in Natural Language Processing, pp. 1532\u20131543, 2014.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2014}, {"title": "Polyglot: Distributed word representations for multilingual NLP", "author": ["R. Al-Rfou", "B. Perozzi", "S. Skiena"], "venue": "CoRR, vol. abs/1307.1662, 2013.", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2013}, {"title": "Inducing crosslingual distributed representations of words", "author": ["A. Klementiev", "I. Titov", "B. Bhattarai"], "venue": "Proceedings of COLING 2012, (Mumbai, India), pp. 1459\u20131474, 2012.", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2012}, {"title": "Bilingual word embeddings for phrase-based machine translation", "author": ["W.Y. Zou", "R. Socher", "D. Cer", "C.D. Manning"], "venue": "Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, (Seattle, WA, USA), pp. 1393\u20131398, 2013.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2013}, {"title": "Multilingual models for compositional distributed semantics", "author": ["K.M. Hermann", "P. Blunsom"], "venue": "Annual Meeting of the Association for Computational Linguistics, (Baltimore, Maryland), pp. 58\u201368, 2014.", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2014}, {"title": "An autoencoder approach to learning bilingual word representations", "author": ["A.P.S. Chandar", "S. Lauly", "H. Larochelle", "M.M. Khapra", "B. Ravindran", "V.C. Raykar", "A. Saha"], "venue": "CoRR, vol. abs/1402.1454, 2014.", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2014}, {"title": "Simlex-999: Evaluating semantic models with (genuine) similarity estimation", "author": ["F. Hill", "R. Reichart", "A. Korhonen"], "venue": "CoRR, vol. abs/1408.3456, 2014.", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2014}, {"title": "Multimodal distributional semantics", "author": ["E. Bruni", "N.K. Tran", "M. Baroni"], "venue": "Journal of Artificial Intelligence Research, vol. 49, pp. 1\u201347, Jan. 2014.", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2014}, {"title": "Learning grounded meaning representations with autoencoders", "author": ["C. Silberer", "M. Lapata"], "venue": "52nd Annual Meeting of the Association for Computational Linguistics, (Baltimore, Maryland), pp. 721\u2013732, June 2014.", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2014}, {"title": "Combining language and vision with a multimodal skip-gram model", "author": ["A. Lazaridou", "N.T. Pham", "M. Baroni"], "venue": "Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, (Denver, Colorado), pp. 153\u2013163, 2015.", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2015}, {"title": "Deep fragment embeddings for bidirectional image sentence mapping", "author": ["A. Karpathy", "A. Joulin", "F. Li"], "venue": "Advances in Neural Information Processing Systems 27, pp. 1889\u20131897, Curran Associates, Inc., 2014.", "citeRegEx": "15", "shortCiteRegEx": null, "year": 1889}, {"title": "Unifying visual-semantic embeddings with multimodal neural language models", "author": ["R. Kiros", "R. Salakhutdinov", "R.S. Zemel"], "venue": "CoRR, vol. abs/1411.2539, 2014.", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2014}, {"title": "Grounded compositional semantics for finding and describing images with sentences", "author": ["R. Socher", "A. Karpathy", "Q.V. Le", "C.D. Manning", "A.Y. Ng"], "venue": "TACL, vol. 2, pp. 207\u2013218, 2014.", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2014}, {"title": "Explain images with multimodal recurrent neural networks", "author": ["J. Mao", "W. Xu", "Y. Yang", "J. Wang", "A.L. Yuille"], "venue": "CoRR, vol. abs/1410.1090, 2014.", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2014}, {"title": "Visual word2vec (vis-w2v): Learning visually grounded word embeddings using abstract scenes", "author": ["S. Kottur", "R. Vedantam", "J.M.F. Moura", "D. Parikh"], "venue": "CoRR, vol. abs/1511.07067, 2015.", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2015}, {"title": "Evaluation methods for unsupervised word embeddings", "author": ["T. Schnabel", "I. Labutov", "D. Mimno", "T. Joachims"], "venue": "Conference on Empirical Methods in Natural Language Processing, (Lisbon, Portugal), pp. 298\u2013307, 2015.", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2015}, {"title": "Improving distributional similarity with lessons learned from word embeddings", "author": ["O. Levy", "Y. Goldberg", "I. Dagan"], "venue": "Transactions of Association for Computational Linguistics, vol. 3, pp. 211\u2013225, 2015.", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2015}, {"title": "Distributed representations of words and phrases and their compositionality", "author": ["T. Mikolov", "I. Sutskever", "K. Chen", "G.S. Corrado", "J. Dean"], "venue": "Advances in Neural Information Processing Systems 26, pp. 3111\u20133119, 2013.", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2013}, {"title": "Word embeddings through hellinger pca", "author": ["R. Lebret", "R. Collobert"], "venue": "Conference of the European Chapter of the Association for Computational Linguistics, (Gothenburg, Sweden), pp. 482\u2013490, 2014.", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2014}, {"title": "Nouns are vectors, adjectives are matrices: Representing adjective-noun constructions in semantic space", "author": ["M. Baroni", "R. Zamparelli"], "venue": "Conference on Empirical Methods in Natural Language Processing, (Cambridge, MA, USA), pp. 1183\u20131193, 2010.", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2010}, {"title": "Semantic compositionality through recursive matrix-vector spaces", "author": ["R. Socher", "B. Huval", "C.D. Manning", "A.Y. Ng"], "venue": "Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning, (Jeju Island, Korea), pp. 1201\u20131211, 2012.", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2012}, {"title": "Probabilistic part-of-speech tagging using decision trees", "author": ["H. Schmid"], "venue": "International Conference on New Methods in Language Processing, (Manchester, UK), 1994.", "citeRegEx": "26", "shortCiteRegEx": null, "year": 1994}, {"title": "Neurons that keep a straight face", "author": ["W.A. Freiwald", "D.Y. Tsao"], "venue": "National Academy of Sciences, vol. 111, no. 22, pp. 7894\u20137895, 2014.", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2014}, {"title": "The beauty of capturing faces: Rating the quality of digital portraits", "author": ["M. Redi", "N. Rasiwasia", "G. Aggarwal", "A. Jaimes"], "venue": "IEEE International Conference and Workshops on Automatic Face and Gesture Recognition, (Ljubljana, Slovenia), pp. 1\u20138, 2015.", "citeRegEx": "28", "shortCiteRegEx": null, "year": 2015}, {"title": "Predicting viewer perceived emotions in animated GIFs", "author": ["B. Jou", "S. Bhattacharya", "S.-F. Chang"], "venue": "ACM International Conference on Multimedia, (Orlando, Florida, USA), pp. 213\u2013216, 2014.", "citeRegEx": "29", "shortCiteRegEx": null, "year": 2014}, {"title": "Faces engage us: Photos with faces attract more likes and comments on instagram", "author": ["S. Bakhshi", "D.A. Shamma", "E. Gilbert"], "venue": "ACM Conference on Human Factors in Computing Systems, (Toronto, ON, Canada), pp. 965\u2013974, 2014.", "citeRegEx": "30", "shortCiteRegEx": null, "year": 2014}, {"title": "A fast and accurate unconstrained face detector", "author": ["S. Liao", "A.K. Jain", "S.Z. Li"], "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence, vol. 38, pp. 211\u2013223, Feb 2016.", "citeRegEx": "31", "shortCiteRegEx": null, "year": 2016}], "referenceMentions": [{"referenceID": 0, "context": "For instance, [1] showed that Flickr users with different cultural backgrounds use different concepts to describe visual emotions.", "startOffset": 14, "endOffset": 17}, {"referenceID": 0, "context": "Our goal is to study the lexical differences of sentiment-biased visual concepts across multiple languages from the MVSO dataset [1].", "startOffset": 129, "endOffset": 132}, {"referenceID": 1, "context": "Research on distributed word representations [2, 3, 4, 5] have recently extended to multiple languages either by using bilingual word alignments or parallel corpora to transfer linguistic information from multiple languages.", "startOffset": 45, "endOffset": 57}, {"referenceID": 2, "context": "Research on distributed word representations [2, 3, 4, 5] have recently extended to multiple languages either by using bilingual word alignments or parallel corpora to transfer linguistic information from multiple languages.", "startOffset": 45, "endOffset": 57}, {"referenceID": 3, "context": "Research on distributed word representations [2, 3, 4, 5] have recently extended to multiple languages either by using bilingual word alignments or parallel corpora to transfer linguistic information from multiple languages.", "startOffset": 45, "endOffset": 57}, {"referenceID": 4, "context": "Research on distributed word representations [2, 3, 4, 5] have recently extended to multiple languages either by using bilingual word alignments or parallel corpora to transfer linguistic information from multiple languages.", "startOffset": 45, "endOffset": 57}, {"referenceID": 5, "context": "For instance, [6] proposed to learn distributed representations of words across languages by using a multilingual corpus from Wikipedia.", "startOffset": 14, "endOffset": 17}, {"referenceID": 6, "context": "[7, 8] proposed to learn bilingual embeddings in the context of neural language models utilizing multilingual word alignments.", "startOffset": 0, "endOffset": 6}, {"referenceID": 7, "context": "[7, 8] proposed to learn bilingual embeddings in the context of neural language models utilizing multilingual word alignments.", "startOffset": 0, "endOffset": 6}, {"referenceID": 8, "context": "[9] proposed to learn joint-space embeddings across multiple languages without relying on word alignments.", "startOffset": 0, "endOffset": 3}, {"referenceID": 9, "context": "Similarly, [10] proposed auto-encoder-based methods to learn multilingual word embeddings.", "startOffset": 11, "endOffset": 15}, {"referenceID": 10, "context": "However, studies on multimodal distributional semantics have combined visual and textual features to learn more informed word embeddings and have used the notion of semantics [11, 12] and", "startOffset": 175, "endOffset": 183}, {"referenceID": 11, "context": "However, studies on multimodal distributional semantics have combined visual and textual features to learn more informed word embeddings and have used the notion of semantics [11, 12] and", "startOffset": 175, "endOffset": 183}, {"referenceID": 12, "context": "visual similarity to evaluate word embeddings [13, 14].", "startOffset": 46, "endOffset": 54}, {"referenceID": 13, "context": "visual similarity to evaluate word embeddings [13, 14].", "startOffset": 46, "endOffset": 54}, {"referenceID": 14, "context": "for image caption generation and retrieval [15, 16, 17, 18] based on multimodal neural language models.", "startOffset": 43, "endOffset": 59}, {"referenceID": 15, "context": "for image caption generation and retrieval [15, 16, 17, 18] based on multimodal neural language models.", "startOffset": 43, "endOffset": 59}, {"referenceID": 16, "context": "for image caption generation and retrieval [15, 16, 17, 18] based on multimodal neural language models.", "startOffset": 43, "endOffset": 59}, {"referenceID": 17, "context": "for image caption generation and retrieval [15, 16, 17, 18] based on multimodal neural language models.", "startOffset": 43, "endOffset": 59}, {"referenceID": 18, "context": "Perhaps the most related study to ours is in [19] which aimed to learn visually grounded word embeddings to capture visual notions of semantic relatedness using abstract visual scenes.", "startOffset": 45, "endOffset": 49}, {"referenceID": 0, "context": "In this work, we make use of the MVSO dataset [1].", "startOffset": 46, "endOffset": 49}, {"referenceID": 0, "context": "54 between crowdsourced sentiment scores and the automatically assigned sentiment scores in [1].", "startOffset": 92, "endOffset": 95}, {"referenceID": 0, "context": "Unlike the findings of previous work using automatic sentiment scores [1],", "startOffset": 70, "endOffset": 73}, {"referenceID": 3, "context": "To learn such representations of meaning we make use of the recent advances in distributional lexical semantics [4, 5, 20, 21] utilizing the skip-gram model provided by word2vec toolkit trained on large text corpora.", "startOffset": 112, "endOffset": 126}, {"referenceID": 4, "context": "To learn such representations of meaning we make use of the recent advances in distributional lexical semantics [4, 5, 20, 21] utilizing the skip-gram model provided by word2vec toolkit trained on large text corpora.", "startOffset": 112, "endOffset": 126}, {"referenceID": 19, "context": "To learn such representations of meaning we make use of the recent advances in distributional lexical semantics [4, 5, 20, 21] utilizing the skip-gram model provided by word2vec toolkit trained on large text corpora.", "startOffset": 112, "endOffset": 126}, {"referenceID": 20, "context": "To learn such representations of meaning we make use of the recent advances in distributional lexical semantics [4, 5, 20, 21] utilizing the skip-gram model provided by word2vec toolkit trained on large text corpora.", "startOffset": 112, "endOffset": 126}, {"referenceID": 21, "context": "Google News: A corpus of news which contains 100 billion tokens and 3,000,000 unique words which have at least 5 occurrences from [22].", "startOffset": 130, "endOffset": 134}, {"referenceID": 22, "context": "The pre-processed text of this corpus was obtained from [23].", "startOffset": 56, "endOffset": 60}, {"referenceID": 22, "context": "The preprocessed text of this corpus was obtained from [23].", "startOffset": 55, "endOffset": 59}, {"referenceID": 21, "context": "For the Google News corpus, we used pre-trained embeddings of 300 dimensions with a context window of 5 words provided by [22].", "startOffset": 122, "endOffset": 126}, {"referenceID": 23, "context": "[24, 25, 22]; however, here, we focus on the type of corpora used for obtaining the word embeddings rather than on the composition function.", "startOffset": 0, "endOffset": 12}, {"referenceID": 24, "context": "[24, 25, 22]; however, here, we focus on the type of corpora used for obtaining the word embeddings rather than on the composition function.", "startOffset": 0, "endOffset": 12}, {"referenceID": 21, "context": "[24, 25, 22]; however, here, we focus on the type of corpora used for obtaining the word embeddings rather than on the composition function.", "startOffset": 0, "endOffset": 12}, {"referenceID": 3, "context": "and analogy relations [4].", "startOffset": 22, "endOffset": 25}, {"referenceID": 25, "context": "We perform part-of-speech tagging on the translation to extract the representative noun or adjective with TreeTagger [26].", "startOffset": 117, "endOffset": 121}, {"referenceID": 26, "context": "the most developed human capabilities, where our brains even contain a dedicated sub-network of neurons for face processing [27].", "startOffset": 124, "endOffset": 128}, {"referenceID": 27, "context": "in computational aesthetics [28], animated GIFs [29], and social dynamics [30].", "startOffset": 28, "endOffset": 32}, {"referenceID": 28, "context": "in computational aesthetics [28], animated GIFs [29], and social dynamics [30].", "startOffset": 48, "endOffset": 52}, {"referenceID": 29, "context": "in computational aesthetics [28], animated GIFs [29], and social dynamics [30].", "startOffset": 74, "endOffset": 78}, {"referenceID": 30, "context": "To obtain a corpus of visual concepts relating to faces, we ran a frontal face detector [31] which projects images onto a normalized pixel difference feature space and performs quadtree-based face detection.", "startOffset": 88, "endOffset": 92}, {"referenceID": 0, "context": "A total of 3,858,869 faces were detected across the 7,368,364 images in the MVSO image dataset [1].", "startOffset": 95, "endOffset": 98}, {"referenceID": 0, "context": "This enabled multilingual clustering of visual sentiment concepts in 11 languages, and allowed us to better hierarchically organize an ontology in [1] as well as provide deep analysis into portrait imagery from a multilingual perspective.", "startOffset": 147, "endOffset": 150}], "year": 2016, "abstractText": "The impact of culture in visual emotion perception has recently captured the attention of multimedia research. In this study, we provide powerful computational linguistics tools to explore, retrieve and browse a dataset of 16K multilingual affective visual concepts and 7.3M Flickr images. First, we design an effective crowdsourcing experiment to collect human judgements of sentiment connected to the visual concepts. We then use word embeddings to represent these concepts in a low dimensional vector space, allowing us to expand the meaning around concepts, and thus enabling insight about commonalities and differences among different languages. We compare a variety of concept representations through a novel evaluation task based on the notion of visual semantic relatedness. Based on these representations, we design clustering schemes to group multilingual visual concepts, and evaluate them with novel metrics based on the crowdsourced sentiment annotations as well as visual semantic relatedness. The proposed clustering framework enables us to analyze the full multilingual dataset in-depth and also show an application on a facial data subset, exploring cultural insights of portrait-related affective visual concepts.", "creator": "LaTeX with hyperref package"}}}