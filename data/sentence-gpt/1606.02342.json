{"id": "1606.02342", "review": {"conference": "ACL", "VERSION": "v1", "DATE_OF_SUBMISSION": "7-Jun-2016", "title": "Optimizing Spectral Learning for Parsing", "abstract": "We describe a search algorithm for optimizing the number of latent states when estimating latent-variable PCFGs with spectral methods. Our results show that contrary to the common belief that the number of latent states for each nonterminal in an L-PCFG can be decided in isolation with spectral methods, parsing results significantly improve if the number of latent states for each nonterminal is globally optimized, while taking into account interactions between the different nonterminals. In addition, we contribute an empirical analysis of spectral algorithms on eight morphologically rich languages: Basque, French, German, Hebrew, Hungarian, Korean, Polish and Swedish. Our results show that our estimation consistently performs better or close to coarse-to-fine expectation-maximization techniques for these languages.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n", "histories": [["v1", "Tue, 7 Jun 2016 21:58:41 GMT  (51kb)", "https://arxiv.org/abs/1606.02342v1", "11 pages, ACL 2016"], ["v2", "Thu, 9 Jun 2016 08:34:12 GMT  (51kb)", "http://arxiv.org/abs/1606.02342v2", "11 pages, ACL 2016"], ["v3", "Tue, 14 Jun 2016 13:10:41 GMT  (51kb)", "http://arxiv.org/abs/1606.02342v3", "11 pages, ACL 2016"]], "COMMENTS": "11 pages, ACL 2016", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["shashi narayan", "shay b cohen"], "accepted": true, "id": "1606.02342"}, "pdf": {"name": "1606.02342.pdf", "metadata": {"source": "CRF", "title": "Optimizing Spectral Learning for Parsing", "authors": ["Shashi Narayan"], "emails": ["snaraya2@inf.ed.ac.uk", "scohen@inf.ed.ac.uk"], "sections": [{"heading": null, "text": "ar X\niv :1\n60 6.\n02 34\n2v 3\n[ cs\n.C L\n] 1\n4 Ju\nn 20"}, {"heading": "1 Introduction", "text": "Latent-variable probabilistic context-free grammars (L-PCFGs) have been used in the natural language processing community (NLP) for syntactic parsing for over a decade. They were introduced in the NLP community by Matsuzaki et al. (2005) and Prescher (2005), with Matsuzaki et al. using the expectation-maximization (EM) algorithm to estimate them. Their performance on syntactic parsing of English at that stage lagged behind state-of-the-art parsers.\nPetrov et al. (2006) showed that one of the reasons that the EM algorithm does not estimate state-of-the-art parsing models for English is that the EM algorithm does not control well for the model size used in the parser \u2013 the number of la-\ntent states associated with the various nonterminals in the grammar. As such, they introduced a coarse-to-fine technique to estimate the grammar. It splits and merges nonterminals (with latent state information) with the aim to optimize the likelihood of the training data. Together with other types of fine tuning of the parsing model, this led to state-of-the-art results for English parsing.\nIn more recent work, Cohen et al. (2012) described a different family of estimation algorithms for L-PCFGs. This so-called \u201cspectral\u201d family of learning algorithms is compelling because it offers a rigorous theoretical analysis of statistical convergence, and sidesteps local maxima issues that arise with the EM algorithm.\nWhile spectral algorithms for L-PCFGs are compelling from a theoretical perspective, they have been lagging behind in their empirical results on the problem of parsing. In this paper we show that one of the main reasons for that is that spectral algorithms require a more careful tuning procedure for the number of latent states than that which has been advocated for until now. In a sense, the relationship between our work and the work of Cohen et al. (2013) is analogous to the relationship between the work by Petrov et al. (2006) and the work by Matsuzaki et al. (2005): we suggest a technique for optimizing the number of latent states for spectral algorithms, and test it on eight languages.\nOur results show that when the number of latent states is optimized using our technique, the parsing models the spectral algorithms yield perform significantly better than the vanilla-estimated models, and for most of the languages \u2013 better than the Berkeley parser of Petrov et al. (2006).\nAs such, the contributions of this parser are twofold:\n\u2022 We describe a search algorithm for optimiz-\ning the number of latent states for spectral learning.\n\u2022 We describe an analysis of spectral algorithms on eight languages (until now the results of L-PCFG estimation with spectral algorithms for parsing were known only for English). Our parsing algorithm is rather language-generic, and does not require significant linguistically-oriented adjustments.\nIn addition, we dispel the common wisdom that more data is needed with spectral algorithms. Our models yield high performance on treebanks of varying sizes from 5,000 sentences (Hebrew and Swedish) to 40,472 sentences (German).\nThe rest of the paper is organized as follows. In \u00a72 we describe notation and background. \u00a73 further investigates the need for an optimization of the number of latent states in spectral learning and describes our optimization algorithm, a search algorithm akin to beam search. In \u00a74 we describe our experiments with natural language parsing for Basque, French, German, Hebrew, Hungarian, Korean, Polish and Swedish. We conclude in \u00a75."}, {"heading": "2 Background and Notation", "text": "We denote by [n] the set of integers {1, . . . , n}. An L-PCFG is a 5-tuple (N ,I,P, f, n) where:\n\u2022 N is the set of nonterminal symbols in the grammar. I \u2282 N is a finite set of interminals. P \u2282 N is a finite set of preterminals. We assume that N = I \u222a P, and I \u2229 P = \u2205. Hence we have partitioned the set of nonterminals into two subsets.\n\u2022 f :N \u2192 N is a function that maps each nonterminal a to the number of latent states it uses. The set [ma] includes the possible hidden states for nonterminal a.\n\u2022 [n] is the set of possible words.\n\u2022 For all a \u2208 I , b \u2208 N , c \u2208 N , h1 \u2208 [ma], h2 \u2208 [mb], h3 \u2208 [mc], we have a binary context-free rule a(h1) \u2192 b(h2) c(h3).\n\u2022 For all a \u2208 P, h \u2208 [ma], x \u2208 [n], we have a lexical context-free rule a(h) \u2192 x.\nThe estimation of an L-PCFG requires an assignment of probabilities (or weights) to each of\nthe rules a(h1) \u2192 b(h2) c(h3) and a(h) \u2192 x, and also an assignment of starting probabilities for each a(h), where a \u2208 I and h \u2208 [ma]. Estimation is usually assumed to be done from a set of parse trees (a treebank), where the latent states are not included in the data \u2013 only the \u201cskeletal\u201d trees which consist of nonterminals in N .\nL-PCFGs, in their symbolic form, are related to regular tree grammars, an old grammar formalism, but they were introduced as statistical models for parsing with latent heads more recently by Matsuzaki et al. (2005) and Prescher (2005). Earlier work about L-PCFGs by Matsuzaki et al. (2005) used the expectation-maximization (EM) algorithm to estimate the grammar probabilities. Indeed, given that the latent states are not observed, EM is a good fit for L-PCFG estimation, since it aims to do learning from incomplete data. This work has been further extended by Petrov et al. (2006) to use EM in a coarse-to-fine fashion: merging and splitting nonterminals using the latent states to optimize the number of latent states for each nonterminal.\nCohen et al. (2012) presented a so-called spectral algorithm to estimate L-PCFGs. This algorithm uses linear-algebraic procedures such as singular value decomposition (SVD) during learning. The spectral algorithm of Cohen et al. builds on an estimation algorithm for HMMs by Hsu et al. (2009).1 Cohen et al. (2013) experimented with this spectral algorithm for parsing English. A different variant of a spectral learning algorithm for L-PCFGs was developed by Cohen and Collins (2014). It breaks the problem of L-PCFG estimation into multiple convex optimization problems which are solved using EM.\nThe family of L-PCFG spectral learning algorithms was further extended by Narayan and Cohen (2015). They presented a simplified version of the algorithm of Cohen et al. (2012) that estimates sparse grammars and assigns probabilities (instead of weights) to the rules in the grammar, and as such does not suffer from the problem of negative probabilities that arise with the original spectral algorithm (see discussion in Cohen et al., 2013). In this paper, we use the algorithms by Narayan and Cohen (2015) and Cohen\n1A related algorithm for weighted tree automata (WTA) was developed by Bailly et al. (2010). However, the conversion from L-PCFGs to WTA is not straightforward, and information is lost in this conversion. See also (Rabusseau et al., 2016).\net al. (2012), and we compare them against stateof-the-art L-PCFG parsers such as the Berkeley parser (Petrov et al., 2006). We also compare our algorithms to other state-of-the-art parsers where elaborate linguistically-motivated feature specifications (Hall et al., 2014), annotations (Crabbe\u0301, 2015) and formalism conversions (Ferna\u0301ndezGonza\u0301lez and Martins, 2015) are used."}, {"heading": "3 Optimizing Spectral Estimation", "text": "In this section, we describe our optimization algorithm and its motivation."}, {"heading": "3.1 Spectral Learning of L-PCFGs and Model Size", "text": "The family of spectral algorithms for latentvariable PCFGs rely on feature functions that are defined for inside and outside trees. Given a tree, the inside tree for a node contains the entire subtree below that node; the outside tree contains everything in the tree excluding the inside tree. Figure 1 shows an example of inside and outside trees for the nonterminal VP in the parse tree of the sentence \u201cthe mouse chased the cat\u201d.\nWith L-PCFGs, the model dictates that an inside tree and an outside tree that are connected at a node are statistically conditionally independent of each other given the node label and the latent state that is associated with it. As such, one can identify the distribution over the latent states for a given nonterminal a by using the cross-covariance matrix of the inside and the outside trees, \u2126a. For more information on the definition of this crosscovariance matrix, see Cohen et al. (2012) and Narayan and Cohen (2015).\nThe L-PCFG spectral algorithms use singular value decomposition (SVD) on \u2126a to reduce the dimensionality of the feature functions. If \u2126a is computed from the true L-PCFG distribution then\nthe rank of \u2126a (the number of non-zero singular values) gives the number of latent states according to the model.\nIn the case of estimating \u2126a from data generated from an L-PCFG, the number of latent states for each nonterminal can be exposed by capping it when the singular values of \u2126a are smaller than some threshold value. This means that spectral algorithms give a natural way for the selection of the number of latent states for each nonterminal a in the grammar.\nHowever, when the data from which we estimate an L-PCFG model are not drawn from an LPCFG (the model is \u201cincorrect\u201d), the number of non-zero singular values (or the number of singular values which are large) is no longer sufficient to determine the number of latent states for each nonterminal. This is where our algorithm comes into play: it optimizes the number of latent search for each nonterminal by applying a search algorithm akin to beam search."}, {"heading": "3.2 Optimizing the Number of Latent States", "text": "As mentioned in the previous section, the number of non-zero singular values of \u2126a gives a criterion to determine the number of latent states ma for a given nonterminal a. In practice, we cap ma not to include small singular values which are close to 0, because of estimation errors of \u2126a.\nThis procedure does not take into account the interactions that exist between choices of latent state numbers for the various nonterminals. In principle, given the independence assumptions that L-PCFGs make, choosing the nonterminals based only on the singular values is \u201cstatistically correct.\u201d However, because in practice the modeling assumptions that we make (that natural language parse trees are drawn from an L-PCFG) do not hold, we can improve further the accuracy of the model by taking into account the nonterminal interaction. Another source of difficulty in choosing the number of latent states based the singular values of \u2126a is sampling error: in practice, we are using data to estimate \u2126a, and as such, even if the model is correct, the rank of the estimated matrix does not have to correspond to the rank of \u2126a according to the true distribution. As a matter of fact, in addition to neglecting small singular values, the spectral methods of Cohen et al. (2013) and Narayan and Cohen (2015) also cap the number of latent states for each nonterminal to an up-\nper bound to keep the grammar size small.\nPetrov et al. (2006) improves over the estimation described in Matsuzaki et al. (2005) by taking into account the interactions between the nonterminals and their latent state numbers in the training data. They use the EM algorithm to split and merge nonterminals using the latent states, and op-\ntimize the number of latent states for each nonterminal such that it maximizes the likelihood of a training treebank. Their refined grammar successfully splits nonterminals to various degrees to capture their complexity. We take the analogous step with spectral methods. We propose an algorithm where we first compute \u2126a on the training data and then we optimize the number of latent states for each nonterminal by optimizing the PARSEVAL metric (Black et al., 1991) on a development set.\nOur optimization algorithm appears in Figure 2. The input to the algorithm is training and development data in the form of parse trees, a basic spectral estimation algorithm S in its default setting, an upper bound m on the number of latent states that can be used for the different nonterminals and a beam size k which gives a maximal queue size for the beam. The algorithm aims to learn a function f that maps each nonterminal a to the number of latent states. It initializes f by estimating a default grammar GS : (N ,I,P, fS , n) using S and setting f = fS . It then iterates over a \u2208 N , improving f such that it optimizes the PARSEVAL metric on the development set.\nThe state of the algorithm includes a queue that consists of tuples of the form (s, j, f, t) where f is an assignment of latent state numbers to each nonterminal in the grammar, j is the index of a nonterminal to be explored in the input nonterminal list L, s is the F1 score on the development set for a grammar that is estimated with f and t is a tag that can either be coarse or refine.\nThe algorithm orders these tuples by s in the queue, and iteratively dequeues elements from the queue. Then, depending on the label t, it either makes a refined search for the number of latent states for aj , or a more coarse search. As such, the algorithm can be seen as a variant of a beam search algorithm.\nThe search algorithm can be used with any training algorithm for L-PCFGs, including the algorithms of Cohen et al. (2013) and Narayan and Cohen (2015). These methods, in their default setting, use a function fS which maps each nonterminal a to a fixed number of latent states ma it uses. In this case, S takes as input training data, in the form of a treebank, decomposes into inside and outside trees at each node in each tree in the training set; and reduces the dimensionality of the inside and outside feature functions by running\nSVD on the cross-covariance matrix \u2126a of the inside and the outside trees, for each nonterminal a. Cohen et al. (2013) estimate the parameters of the L-PCFG up to a linear transformation using f(a) non-zero singular values of \u2126a, whereas Narayan and Cohen (2015) use the feature representations induced from the SVD step to cluster instances of nonterminal a in the training data into f(a) clusters; these clusters are then treated as latent states that are \u201cobserved.\u201d Finally, Narayan and Cohen follow up with a simple frequency count maximum likelihood estimate to estimate the parameters in the L-PCFG with these latent states.\nAn important point to make is that the learning algorithms of Narayan and Cohen (2015) and Cohen et al. (2013) are relatively fast,2 in comparison to the EM algorithm. They require only one iteration over the data. In addition, the SVD step of S for these learning algorithms is computed just once for a large m. The SVD of a lower rank can then be easily computed from that SVD."}, {"heading": "4 Experiments", "text": "In this section, we describe our setup for parsing experiments on a range of languages."}, {"heading": "4.1 Experimental Setup", "text": "Datasets We experiment with nine treebanks consisting of eight different morphologically rich languages: Basque, French, German, Hebrew, Hungarian, Korean, Polish and Swedish. Table 1 shows the statistics of 9 different treebanks with their splits into training, development and test sets. Eight out of the nine datasets (Basque, French, German-T, Hebrew, Hungarian, Korean, Polish\n2It has been documented in several papers that the family of spectral estimation algorithms is faster than algorithms such as EM, not just for L-PCFGs. See, for example, Parikh et al. (2012).\nand Swedish) are taken from the workshop on Statistical Parsing of Morphologically Rich Languages (SPMRL; Seddah et al., 2013). The German corpus in the SPMRL workshop is taken from the TiGer corpus (German-T, Brants et al., 2004). We also experiment with another German corpus, the NEGRA corpus (German-N, Skut et al., 1997), in a standard evaluation split.3 Words in the SPMRL datasets are annotated with their morphological signatures, whereas the NEGRA corpus does not contain any morphological information.\nData preprocessing and treatment of rare words We convert all trees in the treebanks to a binary form, train and run the parser in that form, and then transform back the trees when doing evaluation using the PARSEVAL metric. In addition, we collapse unary rules into unary chains, so that our trees are fully binarized. The column \u201c#nts\u201d in Table 1 shows the number of nonterminals after binarization in the various treebanks. Before binarization, we also drop all functional information from the nonterminals. We use fine tags for all languages except Korean. This is in line with Bjo\u0308rkelund et al. (2013).4 For Korean, there are 2,825 binarized nonterminals making it impractical to use our optimization algorithm, so we use the coarse tags.\nBjo\u0308rkelund et al. (2013) have shown that the morphological signatures for rare words are useful to improve the performance of the Berkeley parser.\n3We use the first 18,602 sentences as a training set, the next 1,000 sentences as a development set and the last 1,000 sentences as a test set. This corresponds to an 80%-10%-10% split of the treebank.\n4In their experiments Bjo\u0308rkelund et al. (2013) found that fine tags were not useful for Basque also; they did not find a proper explanation for that. In our experiments, however, we found that fine tags were useful for Basque. To retrieve the fine tags, we concatenate coarse tags with their refinement feature (\u201cAZP\u201d) values.\nIn our preliminary experiments with na\u0131\u0308ve spectral estimation, we preprocess rare words in the training set in two ways: (i) we replace them with their corresponding POS tags, and (ii) we replace them with their corresponding POS+morphological signatures. We follow Bjo\u0308rkelund et al. (2013) and consider a word to be rare if it occurs less than 20 times in the training data. We experimented both with a version of the parser that does not ignore and does ignore letter cases, and discovered that the parser behaves better when case is not ignored.\nSpectral algorithms: subroutine choices The latent state optimization algorithm will work with either the clustering estimation algorithm of Narayan and Cohen (2015) or the spectral algorithm of Cohen et al. (2013). In our setup, we first run the latent state optimization algorithm with the clustering algorithm. We then run the spectral algorithm once with the optimized f from the clustering algorithm. We do that because the clustering algorithm is significantly faster to iteratively parse the development set, because it leads to sparse estimates.\nOur optimization algorithm is sensitive to the initialization of the number of latent states assigned to each nonterminals as it sequentially goes through the list of nonterminals and chooses latent state numbers for each nonterminal, keeping latent state numbers for other nonterminals fixed. In our setup, we start our search algorithm with the best model from the clustering algorithm, controlling for all hyperparameters; we tune f , the function which maps each nonterminal to a fixed number of latent states m, by running the vanilla version with different values of m for different languages. Based on our preliminary experiments, we set m to 4 for Basque, Hebrew, Polish and Swedish; 8 for German-N; 16 for German-T, Hungarian and Korean; and 24 for French.\nWe use the same features for the spectral methods as in Narayan and Cohen (2015) for GermanN. For the SPMRL datasets we do not use the head features. These require linguistic understanding of the datasets (because they require head rules for propagating leaf nodes in the tree), and we discovered that simple heuristics for constructing these rules did not yield an increase in performance.\nWe use the kmeans function in Matlab to do the clustering for the spectral algorithm of Narayan and Cohen (2015). We experimented with several versions of k-means, and discovered\nthat the version that works best in a set of preliminary experiments is hard k-means.5\nDecoding and evaluation For efficiency, we use a base PCFG without latent states to prune marginals which receive a value less than 0.00005 in the dynamic programming chart. This is just a bare-bones PCFG that is estimated using maximum likelihood estimation (with frequency count). The parser takes part-of-speech tagged sentences as input. We tag the German-N data using the Turbo Tagger (Martins et al., 2010). For the languages in the SPMRL data we use the MarMot tagger of Mu\u0308eller et al. (2013) to jointly predict the POS and morphological tags.6 The parser itself can assign different part-of-speech tags to words to avoid parse failure. This is also particularly important for constituency parsing with morphologically rich languages. It helps mitigate the problem of the taggers to assign correct tags when long-distance dependencies are present.\nFor all results, we report the F1 measure of the PARSEVAL metric (Black et al., 1991). We use the EVALB program7 with the parameter file COLLINS.prm (Collins, 1999) for the German-N data and the SPMRL parameter file, spmrl.prm, for the SPMRL data (Seddah et al., 2013).\nIn this setup, the latent state optimization algorithm terminates in few hours for all datasets except French and German-T. The German-T data has 762 nonterminals to tune over a large development set consisting of 5,000 sentences, whereas, the French data has a high average sentence length of 31.43 in the development set.8\nFollowing Narayan and Cohen (2015), we further improve our results by using multiple spectral models where noise is added to the underlying features in the training set before the estimation of\n5To be more precise, we use the Matlab function kmeans while passing it the parameter \u2018start\u2019=\u2018sample\u2019 to randomly sample the initial centroid positions. In our experiments, we found that default initialization of centroids differs in Matlab14 (random) and in Matlab15 (kmeans++). Our estimation performs better with random initialization.\n6See Bjo\u0308rkelund et al. (2013) for the performance of the MarMot tagger on the SPMRL datasets.\n7We use EVALB from http://nlp.cs.nyu. edu/evalb/ for the German-N data and its modified version from http://dokufarm.phil.hhu. de/spmrl2014/doku.php?id=shared_task_ description for the SPMRL datasets.\n8To speed up tuning on the French data, we drop sentences with length >46 from the development set, dropping its size from 12,35 to 1,006.\neach model.9 Using the optimized f , we estimate 80 models for each of noise induction mechanisms in Narayan and Cohen: Dropout, Gaussian (additive) and Gaussian (multiplicative). To decode with multiple noisy models, we train the MaxEnt reranker of Charniak and Johnson (2005).10 Hierarchical decoding with \u201cmaximal tree coverage\u201d over MaxEnt models, further improves our accuracy. See Narayan and Cohen (2015) for more de-\n9We only use the algorithm of Narayan and Cohen (2015) for the noisy model estimation. They have shown that decoding with noisy models performs better with their sparse estimates than the dense estimates of Cohen et al. (2013).\n10Implementation: https://github.com/BLLIP/ bllip-parser. More specifically, we used the programs extract-spfeatures, cvlm-lbfgs and best-indices. extract-spfeatures uses head features, we bypass this for the SPMRL datasets by creating a dummy heads.cc file. cvlm-lbfgs was used with the default hyperparameters from the Makefile.\ntails on the estimation of a diverse set of models, and on decoding with them."}, {"heading": "4.2 Results", "text": "Table 2 and Table 3 give the results for the various languages.11 Our main focus is on comparing the coarse-to-fine Berkeley parser (Petrov et al., 2006) to our method. However, for the sake of completeness, we also present results for other parsers, such as parsers of Hall et al. (2014), Ferna\u0301ndezGonza\u0301lez and Martins (2015) and Crabbe\u0301 (2015).\nIn line with Bjo\u0308rkelund et al. (2013), our preliminary experiments with the treatment of rare words suggest that morphological features are useful for all SPMRL languages except French.\n11See more in http://cohort.inf.ed.ac.uk/ lpcfg/.\nSpecifically, for Basque, Hungarian and Korean, improvements are significantly large.\nOur results show that the optimization of the number of latent states with the clustering and spectral algorithms indeed improves these algorithms performance, and these increases generalize to the test sets as well. This was a point of concern, since the optimization algorithm goes through many points in the hypothesis space of parsing models, and identifies one that behaves optimally on the development set \u2013 and as such it could overfit to the development set. However, this did not happen, and in some cases, the increase in accuracy of the test set after running our optimization algorithm is actually larger than the one for the development set.\nWhile the vanilla estimation algorithms (without latent state optimization) lag behind the Berkeley parser for many of the languages, once the number of latent states is optimized, our parsing models do better for Basque, Hebrew, Hungarian, Korean, Polish and Swedish. For GermanT we perform close to the Berkeley parser (78.2 vs. 78.3). It is also interesting to compare the clustering algorithm of Narayan and Cohen (2015) to the spectral algorithm of Cohen et al. (2013). In the vanilla version, the spectral algorithm does better in most cases. However, these differences are narrowed, and in some cases, overcome, when the number of latent states is optimized. Decoding with multiple models further improves our accuracy. Our \u201cCl multiple\u201d results lag behind \u201cBk multiple.\u201d We believe this is the result of the need of head features for the MaxEnt models.12\n12Bjo\u0308rkelund et al. (2013) also use the MaxEnt raranker with multiple models of the Berkeley parser, and in their case also the performance after the raranking step is not always significantly better. See footnote 10 on how we create dummy head-features for our MaxEnt models.\nOur results show that spectral learning is a viable alternative to the use of expectationmaximization coarse-to-fine techniques. As we discuss later, further improvements have been introduced to state-of-the-art parsers that are orthogonal to the use of a specific estimation algorithm. Some of them can be applied to our setup."}, {"heading": "4.3 Further Analysis", "text": "In addition to the basic set of parsing results, we also wanted to inspect the size of the parsing models when using the optimization algorithm in comparison to the vanilla models. Table 4 gives this analysis. In this table, we see that in most cases, on average, the optimization algorithm chooses to enlarge the number of latent states. However, for German-T and Korean, for example, the optimization algorithm actually chooses a smaller model than the original vanilla model.\nWe further inspected the behavior of the optimization algorithm for the preterminals in German-N, for which the optimal model chose (on average) a larger number of latent states. Table 5 describes this analysis. We see that in most cases, the optimization algorithm chose to decrease the number of latent states for the various preterminals, but in some cases significantly increases the number of latent states.13\nOur experiments dispel another \u201ccommon wisdom\u201d about spectral learning and training data size. It has been believed that spectral learning do not behave very well when small amounts of data are available (when compared to maximum likelihood estimation algorithms such as EM) \u2013 however we see that our results do better than the\n13Interestingly, most of the punctuation symbols, such as $\u2217LRB\u2217, $. and $,, drop their latent state number to a significantly lower value indicating that their interactions with other nonterminals in the tree are minimal.\nBerkeley parser for several languages with small training datasets, such as Basque, Hebrew, Polish and Hungarian. The source of this common wisdom is that ML estimators tend to be statistically \u201cefficient:\u201d they extract more information from the data than spectral learning algorithms do. Indeed, there is no reason to believe that spectral algorithms are statistically efficient. However, it is not clear that indeed for L-PCFGs with the EM algorithm, the ML estimator is statistically efficient either. MLE is statistically efficient under specific assumptions which are not clearly satisfied with L-PCFG estimation. In addition, when the model is \u201cincorrect,\u201d (i.e. when the data is not sampled from L-PCFG, as we would expect from natural language treebank data), spectral algorithms could yield better results because they can mimic a higher order model. This can be understood through HMMs. When estimating an HMM of a low order with data which was generated from a higher order model, EM does quite poorly. However, if the number of latent states (and feature functions) is properly controlled with spectral algorithms, a spectral algorithm would learn a \u201cproduct\u201d HMM, where the states in the lower order model are the product of states of a higher order.14\nState-of-the-art parsers for the SPMRL datasets improve the Berkeley parser in ways which are orthogonal to the use of the basic estimation algorithm and the method for optimizing the number of latent states. They include transformations of the treebanks such as with unary rules (Bjo\u0308rkelund et al., 2013), a more careful handling of unknown words and better use of morphological informa-\n14For example, a trigram HMM can be reduced to a bigram HMM where the states are products of the original trigram HMM.\ntion such as decorating preterminals with such information (Bjo\u0308rkelund et al., 2014; Sza\u0301nto\u0301 and Farkas, 2014), with careful feature specifications (Hall et al., 2014) and head-annotations (Crabbe\u0301, 2015), and other techniques. Some of these techniques can be applied to our case."}, {"heading": "5 Conclusion", "text": "We demonstrated that a careful selection of the number of latent states in a latent-variable PCFG with spectral estimation has a significant effect on the parsing accuracy of the L-PCFG. We described a search procedure to do this kind of optimization, and described parsing results for eight languages (with nine datasets). Our results demonstrate that when comparing the expectationmaximization with coarse-to-fine techniques to our spectral algorithm with latent state optimization, spectral learning performs better on six of the datasets. Our results are comparable to other stateof-the-art results for these languages. Using a diverse set of models to parse these datasets further improves the results."}, {"heading": "Acknowledgments", "text": "The authors would like to thank David McClosky for his help with running the BLLIP parser and his comments on the paper and also the three anonymous reviewers for their helpful comments. We also thank Eugene Charniak, DK Choe and Geoff Gordon for useful discussions. Finally, thanks to Djame\u0301 Seddah for providing us with the SPMRL datasets and to Thomas Mu\u0308ller and Anders Bjo\u0308rkelund for providing us the MarMot models. This research was supported by an EPSRC grant (EP/L02411X/1) and an EU H2020 grant (688139/H2020-ICT-2015; SUMMA)."}], "references": [{"title": "A spectral approach for probabilistic grammatical inference on trees", "author": ["Rapha\u00ebl Bailly", "Amaury Habrard", "Fran\u00e7ois Denis."], "venue": "Proceedings of International Conference on Algorithmic Learning Theory.", "citeRegEx": "Bailly et al\\.,? 2010", "shortCiteRegEx": "Bailly et al\\.", "year": 2010}, {"title": "Re)ranking meets morphosyntax: State-of-the-art results from the SPMRL 2013 shared task", "author": ["Anders Bj\u00f6rkelund", "\u00d6zlem \u00c7etino\u011flu", "Rich\u00e1rd Farkas", "Thomas M\u00fceller", "Wolfgang Seeker."], "venue": "Proceedings of the Fourth Workshop on Statistical Pars-", "citeRegEx": "Bj\u00f6rkelund et al\\.,? 2013", "shortCiteRegEx": "Bj\u00f6rkelund et al\\.", "year": 2013}, {"title": "Introducing the IMS-Wroc\u0142aw-Szeged-CIS entry at the SPMRL 2014 shared task: Reranking and morphosyntax", "author": ["Anders Bj\u00f6rkelund", "\u00d6zlem \u00c7etino\u011flu", "Agnieszka Fale\u0144ska", "Rich\u00e1rd Farkas", "Thomas M\u00fcller", "Wolfgang Seeker", "Zsolt Sz\u00e1nt\u00f3"], "venue": null, "citeRegEx": "Bj\u00f6rkelund et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Bj\u00f6rkelund et al\\.", "year": 2014}, {"title": "A procedure for quantitatively comparing the syntactic coverage of English grammars", "author": ["torini", "Tomek Strzalkowski"], "venue": "In Proceedings of DARPA Workshop on Speech and Natural Language", "citeRegEx": "torini and Strzalkowski.,? \\Q1991\\E", "shortCiteRegEx": "torini and Strzalkowski.", "year": 1991}, {"title": "TIGER: Linguistic interpretation of a German corpus", "author": ["Sabine Brants", "Stefanie Dipper", "Peter Eisenberg", "Silvia Hansen-Schirra", "Esther K\u00f6nig", "Wolfgang Lezius", "Christian Rohrer", "George Smith", "Hans Uszkoreit."], "venue": "Research on Language and Com-", "citeRegEx": "Brants et al\\.,? 2004", "shortCiteRegEx": "Brants et al\\.", "year": 2004}, {"title": "Coarseto-fine n-best parsing and maxent discriminative reranking", "author": ["Eugene Charniak", "Mark Johnson."], "venue": "Proceedings of ACL.", "citeRegEx": "Charniak and Johnson.,? 2005", "shortCiteRegEx": "Charniak and Johnson.", "year": 2005}, {"title": "A provably correct learning algorithm for latent-variable PCFGs", "author": ["Shay B. Cohen", "Michael Collins."], "venue": "Proceedings of ACL.", "citeRegEx": "Cohen and Collins.,? 2014", "shortCiteRegEx": "Cohen and Collins.", "year": 2014}, {"title": "Spectral learning of latent-variable PCFGs", "author": ["Shay B. Cohen", "Karl Stratos", "Michael Collins", "Dean F. Foster", "Lyle Ungar."], "venue": "Proceedings of ACL.", "citeRegEx": "Cohen et al\\.,? 2012", "shortCiteRegEx": "Cohen et al\\.", "year": 2012}, {"title": "Experiments with spectral learning of latent-variable PCFGs", "author": ["Shay B. Cohen", "Karl Stratos", "Michael Collins", "Dean P. Foster", "Lyle Ungar."], "venue": "Proceedings of NAACL.", "citeRegEx": "Cohen et al\\.,? 2013", "shortCiteRegEx": "Cohen et al\\.", "year": 2013}, {"title": "Head-Driven Statistical Models for Natural Language Parsing", "author": ["Michael Collins."], "venue": "Ph.D. thesis, University of Pennsylvania.", "citeRegEx": "Collins.,? 1999", "shortCiteRegEx": "Collins.", "year": 1999}, {"title": "Multilingual discriminative lexicalized phrase structure parsing", "author": ["Benoit Crabb\u00e9."], "venue": "Proceedings of EMNLP.", "citeRegEx": "Crabb\u00e9.,? 2015", "shortCiteRegEx": "Crabb\u00e9.", "year": 2015}, {"title": "Parsing as reduction", "author": ["Daniel Fern\u00e1ndez-Gonz\u00e1lez", "Andr\u00e9 F.T. Martins."], "venue": "Proceedings of ACLIJCNLP.", "citeRegEx": "Fern\u00e1ndez.Gonz\u00e1lez and Martins.,? 2015", "shortCiteRegEx": "Fern\u00e1ndez.Gonz\u00e1lez and Martins.", "year": 2015}, {"title": "Less grammar, more features", "author": ["David Hall", "Greg Durrett", "Dan Klein."], "venue": "Proceedings of ACL.", "citeRegEx": "Hall et al\\.,? 2014", "shortCiteRegEx": "Hall et al\\.", "year": 2014}, {"title": "A spectral algorithm for learning hidden Markov models", "author": ["Daniel Hsu", "Sham M. Kakade", "Tong Zhang."], "venue": "Proceedings of COLT.", "citeRegEx": "Hsu et al\\.,? 2009", "shortCiteRegEx": "Hsu et al\\.", "year": 2009}, {"title": "TurboParsers: Dependency parsing by approximate variational inference", "author": ["Andr\u00e9 F.T. Martins", "Noah A. Smith", "Eric P. Xing", "M\u00e1rio A.T. Figueiredo", "Pedro M.Q. Aguiar."], "venue": "Proceedings of EMNLP.", "citeRegEx": "Martins et al\\.,? 2010", "shortCiteRegEx": "Martins et al\\.", "year": 2010}, {"title": "Probabilistic CFG with latent annotations", "author": ["Takuya Matsuzaki", "Yusuke Miyao", "Junichi Tsujii."], "venue": "Proceedings of ACL.", "citeRegEx": "Matsuzaki et al\\.,? 2005", "shortCiteRegEx": "Matsuzaki et al\\.", "year": 2005}, {"title": "Efficient higher-order CRFs for morphological tagging", "author": ["Thomas M\u00fceller", "Helmut Schmid", "Hinrich Sch\u00fctze."], "venue": "Proceedings of EMNLP.", "citeRegEx": "M\u00fceller et al\\.,? 2013", "shortCiteRegEx": "M\u00fceller et al\\.", "year": 2013}, {"title": "Diversity in spectral learning for natural language parsing", "author": ["Shashi Narayan", "Shay B. Cohen."], "venue": "Proceedings of EMNLP.", "citeRegEx": "Narayan and Cohen.,? 2015", "shortCiteRegEx": "Narayan and Cohen.", "year": 2015}, {"title": "A spectral algorithm for latent junction trees", "author": ["Ankur P. Parikh", "Le Song", "Mariya Ishteva", "Gabi Teodoru", "Eric P. Xing."], "venue": "Proceedings of the Twenty-Eighth Conference on Uncertainty in Artificial Intelligence.", "citeRegEx": "Parikh et al\\.,? 2012", "shortCiteRegEx": "Parikh et al\\.", "year": 2012}, {"title": "Learning accurate, compact, and interpretable tree annotation", "author": ["Slav Petrov", "Leon Barrett", "Romain Thibaux", "Dan Klein."], "venue": "Proceedings of COLING-ACL.", "citeRegEx": "Petrov et al\\.,? 2006", "shortCiteRegEx": "Petrov et al\\.", "year": 2006}, {"title": "Products of random latent variable grammars", "author": ["Slav Petrov."], "venue": "Proceedings of HLT-NAACL.", "citeRegEx": "Petrov.,? 2010", "shortCiteRegEx": "Petrov.", "year": 2010}, {"title": "Head-driven PCFGs with latent-head statistics", "author": ["Detlef Prescher."], "venue": "Proceedings of IWPT.", "citeRegEx": "Prescher.,? 2005", "shortCiteRegEx": "Prescher.", "year": 2005}, {"title": "Low-rank approximation of weighted tree automata", "author": ["Guillaume Rabusseau", "Borja Balle", "Shay B. Cohen."], "venue": "Proceedings of The 19th International Conference on Artificial Intelligence and Statistics.", "citeRegEx": "Rabusseau et al\\.,? 2016", "shortCiteRegEx": "Rabusseau et al\\.", "year": 2016}, {"title": "An annotation scheme for free word order languages", "author": ["Wojciech Skut", "Brigitte Krenn", "Thorsten Brants", "Hans Uszkoreit."], "venue": "Proceedings of ANLP.", "citeRegEx": "Skut et al\\.,? 1997", "shortCiteRegEx": "Skut et al\\.", "year": 1997}, {"title": "Special techniques for constituent parsing of morphologically rich languages", "author": ["Zsolt Sz\u00e1nt\u00f3", "Rich\u00e1rd Farkas."], "venue": "Proceedings of EACL.", "citeRegEx": "Sz\u00e1nt\u00f3 and Farkas.,? 2014", "shortCiteRegEx": "Sz\u00e1nt\u00f3 and Farkas.", "year": 2014}], "referenceMentions": [{"referenceID": 15, "context": "They were introduced in the NLP community by Matsuzaki et al. (2005) and Prescher (2005), with Matsuzaki et al.", "startOffset": 45, "endOffset": 69}, {"referenceID": 15, "context": "They were introduced in the NLP community by Matsuzaki et al. (2005) and Prescher (2005), with Matsuzaki et al.", "startOffset": 45, "endOffset": 89}, {"referenceID": 15, "context": "They were introduced in the NLP community by Matsuzaki et al. (2005) and Prescher (2005), with Matsuzaki et al. using the expectation-maximization (EM) algorithm to estimate them. Their performance on syntactic parsing of English at that stage lagged behind state-of-the-art parsers. Petrov et al. (2006) showed that one of the reasons that the EM algorithm does not estimate state-of-the-art parsing models for English is that the EM algorithm does not control well for the model size used in the parser \u2013 the number of latent states associated with the various nonterminals in the grammar.", "startOffset": 45, "endOffset": 305}, {"referenceID": 7, "context": "In more recent work, Cohen et al. (2012) described a different family of estimation algorithms for L-PCFGs.", "startOffset": 21, "endOffset": 41}, {"referenceID": 7, "context": "In a sense, the relationship between our work and the work of Cohen et al. (2013) is analogous to the relationship between the work by Petrov et al.", "startOffset": 62, "endOffset": 82}, {"referenceID": 7, "context": "In a sense, the relationship between our work and the work of Cohen et al. (2013) is analogous to the relationship between the work by Petrov et al. (2006) and the work by Matsuzaki et al.", "startOffset": 62, "endOffset": 156}, {"referenceID": 7, "context": "In a sense, the relationship between our work and the work of Cohen et al. (2013) is analogous to the relationship between the work by Petrov et al. (2006) and the work by Matsuzaki et al. (2005): we suggest a technique for optimizing the number of latent states for spectral algorithms, and test it on eight", "startOffset": 62, "endOffset": 196}, {"referenceID": 19, "context": "parsing models the spectral algorithms yield perform significantly better than the vanilla-estimated models, and for most of the languages \u2013 better than the Berkeley parser of Petrov et al. (2006).", "startOffset": 176, "endOffset": 197}, {"referenceID": 15, "context": "L-PCFGs, in their symbolic form, are related to regular tree grammars, an old grammar formalism, but they were introduced as statistical models for parsing with latent heads more recently by Matsuzaki et al. (2005) and Prescher (2005).", "startOffset": 191, "endOffset": 215}, {"referenceID": 15, "context": "L-PCFGs, in their symbolic form, are related to regular tree grammars, an old grammar formalism, but they were introduced as statistical models for parsing with latent heads more recently by Matsuzaki et al. (2005) and Prescher (2005). Earlier work about L-PCFGs by Matsuzaki et al.", "startOffset": 191, "endOffset": 235}, {"referenceID": 15, "context": "L-PCFGs, in their symbolic form, are related to regular tree grammars, an old grammar formalism, but they were introduced as statistical models for parsing with latent heads more recently by Matsuzaki et al. (2005) and Prescher (2005). Earlier work about L-PCFGs by Matsuzaki et al. (2005) used the expectation-maximization (EM) algorithm to estimate the grammar probabilities.", "startOffset": 191, "endOffset": 290}, {"referenceID": 15, "context": "L-PCFGs, in their symbolic form, are related to regular tree grammars, an old grammar formalism, but they were introduced as statistical models for parsing with latent heads more recently by Matsuzaki et al. (2005) and Prescher (2005). Earlier work about L-PCFGs by Matsuzaki et al. (2005) used the expectation-maximization (EM) algorithm to estimate the grammar probabilities. Indeed, given that the latent states are not observed, EM is a good fit for L-PCFG estimation, since it aims to do learning from incomplete data. This work has been further extended by Petrov et al. (2006) to use EM in a coarse-to-fine fashion: merging and splitting nonterminals using the latent states to optimize the number of latent states for each nonterminal.", "startOffset": 191, "endOffset": 584}, {"referenceID": 17, "context": "The family of L-PCFG spectral learning algorithms was further extended by Narayan and Cohen (2015). They presented a simplified version", "startOffset": 74, "endOffset": 99}, {"referenceID": 7, "context": "of the algorithm of Cohen et al. (2012) that estimates sparse grammars and assigns probabilities (instead of weights) to the rules in the grammar, and as such does not suffer from the problem of negative probabilities that arise with the", "startOffset": 20, "endOffset": 40}, {"referenceID": 7, "context": "original spectral algorithm (see discussion in Cohen et al., 2013). In this paper, we use the algorithms by Narayan and Cohen (2015) and Cohen", "startOffset": 47, "endOffset": 133}, {"referenceID": 22, "context": "See also (Rabusseau et al., 2016).", "startOffset": 9, "endOffset": 33}, {"referenceID": 0, "context": "A related algorithm for weighted tree automata (WTA) was developed by Bailly et al. (2010). However, the conversion from L-PCFGs to WTA is not straightforward, and information is lost in this conversion.", "startOffset": 70, "endOffset": 91}, {"referenceID": 19, "context": "(2012), and we compare them against stateof-the-art L-PCFG parsers such as the Berkeley parser (Petrov et al., 2006).", "startOffset": 95, "endOffset": 116}, {"referenceID": 12, "context": "We also compare our algorithms to other state-of-the-art parsers where elaborate linguistically-motivated feature specifications (Hall et al., 2014), annotations (Crabb\u00e9, 2015) and formalism conversions (Fern\u00e1ndezGonz\u00e1lez and Martins, 2015) are used.", "startOffset": 129, "endOffset": 148}, {"referenceID": 10, "context": ", 2014), annotations (Crabb\u00e9, 2015) and formalism conversions (Fern\u00e1ndezGonz\u00e1lez and Martins, 2015) are used.", "startOffset": 21, "endOffset": 35}, {"referenceID": 7, "context": "For more information on the definition of this crosscovariance matrix, see Cohen et al. (2012) and", "startOffset": 75, "endOffset": 95}, {"referenceID": 7, "context": "As a matter of fact, in addition to neglecting small singular values, the spectral methods of Cohen et al. (2013)", "startOffset": 94, "endOffset": 114}, {"referenceID": 17, "context": "and Narayan and Cohen (2015) also cap the number of latent states for each nonterminal to an up-", "startOffset": 4, "endOffset": 29}, {"referenceID": 15, "context": "(2006) improves over the estimation described in Matsuzaki et al. (2005) by taking into account the interactions between the nonter-", "startOffset": 49, "endOffset": 73}, {"referenceID": 7, "context": "gorithms of Cohen et al. (2013) and Narayan and Cohen (2015).", "startOffset": 12, "endOffset": 32}, {"referenceID": 7, "context": "gorithms of Cohen et al. (2013) and Narayan and Cohen (2015). These methods, in their default setting, use a function fS which maps each nonterminal a to a fixed number of latent states ma it uses.", "startOffset": 12, "endOffset": 61}, {"referenceID": 7, "context": "Cohen et al. (2013) estimate the parameters of the L-PCFG up to a linear transformation using f(a) non-zero singular values of \u03a9a, whereas Narayan and Cohen (2015) use the feature representations induced from the SVD step to cluster instances of nonterminal a in the training data into f(a) clusters; these clusters are then treated as latent states that are \u201cobserved.", "startOffset": 0, "endOffset": 20}, {"referenceID": 7, "context": "Cohen et al. (2013) estimate the parameters of the L-PCFG up to a linear transformation using f(a) non-zero singular values of \u03a9a, whereas Narayan and Cohen (2015) use the feature representations induced from the SVD step to cluster instances of nonterminal a in the training data into f(a) clusters; these clusters are then treated as latent states that are \u201cobserved.", "startOffset": 0, "endOffset": 164}, {"referenceID": 15, "context": "An important point to make is that the learning algorithms of Narayan and Cohen (2015) and Cohen et al.", "startOffset": 62, "endOffset": 87}, {"referenceID": 7, "context": "An important point to make is that the learning algorithms of Narayan and Cohen (2015) and Cohen et al. (2013) are relatively fast,2 in comparison to the EM algorithm.", "startOffset": 91, "endOffset": 111}, {"referenceID": 17, "context": "See, for example, Parikh et al. (2012). and Swedish) are taken from the workshop on Statistical Parsing of Morphologically Rich Languages (SPMRL; Seddah et al.", "startOffset": 18, "endOffset": 39}, {"referenceID": 1, "context": "This is in line with Bj\u00f6rkelund et al. (2013).4 For Korean, there are", "startOffset": 21, "endOffset": 46}, {"referenceID": 1, "context": "Bj\u00f6rkelund et al. (2013) have shown that the morphological signatures for rare words are useful to improve the performance of the Berkeley parser.", "startOffset": 0, "endOffset": 25}, {"referenceID": 1, "context": "In their experiments Bj\u00f6rkelund et al. (2013) found that fine tags were not useful for Basque also; they did not find a proper explanation for that.", "startOffset": 21, "endOffset": 46}, {"referenceID": 1, "context": "We follow Bj\u00f6rkelund et al. (2013) and consider a word to be rare if it occurs less than 20 times in the training data.", "startOffset": 10, "endOffset": 35}, {"referenceID": 15, "context": "Spectral algorithms: subroutine choices The latent state optimization algorithm will work with either the clustering estimation algorithm of Narayan and Cohen (2015) or the spectral algorithm of Cohen et al.", "startOffset": 141, "endOffset": 166}, {"referenceID": 7, "context": "Spectral algorithms: subroutine choices The latent state optimization algorithm will work with either the clustering estimation algorithm of Narayan and Cohen (2015) or the spectral algorithm of Cohen et al. (2013). In our setup, we first run the latent state optimization algorithm with the clustering algorithm.", "startOffset": 195, "endOffset": 215}, {"referenceID": 17, "context": "We use the same features for the spectral methods as in Narayan and Cohen (2015) for GermanN.", "startOffset": 56, "endOffset": 81}, {"referenceID": 17, "context": "We use the kmeans function in Matlab to do the clustering for the spectral algorithm of Narayan and Cohen (2015). We experimented with several versions of k-means, and discovered that the version that works best in a set of preliminary experiments is hard k-means.", "startOffset": 88, "endOffset": 113}, {"referenceID": 14, "context": "We tag the German-N data using the Turbo Tagger (Martins et al., 2010).", "startOffset": 48, "endOffset": 70}, {"referenceID": 14, "context": "We tag the German-N data using the Turbo Tagger (Martins et al., 2010). For the languages in the SPMRL data we use the MarMot tagger of M\u00fceller et al. (2013) to jointly predict the POS and morphological tags.", "startOffset": 49, "endOffset": 158}, {"referenceID": 9, "context": "prm (Collins, 1999) for the German-N data and the SPMRL parameter file, spmrl.", "startOffset": 4, "endOffset": 19}, {"referenceID": 17, "context": "Following Narayan and Cohen (2015), we further improve our results by using multiple spec-", "startOffset": 10, "endOffset": 35}, {"referenceID": 1, "context": "See Bj\u00f6rkelund et al. (2013) for the performance of the MarMot tagger on the SPMRL datasets.", "startOffset": 4, "endOffset": 29}, {"referenceID": 19, "context": "\u201cBk\u201d makes use of the Berkeley parser with its coarse-to-fine mechanism to optimize the number of latent states (Petrov et al., 2006).", "startOffset": 112, "endOffset": 133}, {"referenceID": 20, "context": "\u201cBk multiple\u201d shows the best results with the multiple models using product-of-grammars procedure (Petrov, 2010) and discriminative reranking (Charniak and Johnson, 2005).", "startOffset": 98, "endOffset": 112}, {"referenceID": 5, "context": "\u201cBk multiple\u201d shows the best results with the multiple models using product-of-grammars procedure (Petrov, 2010) and discriminative reranking (Charniak and Johnson, 2005).", "startOffset": 142, "endOffset": 170}, {"referenceID": 17, "context": "\u201cCl multiple\u201d gives the results with multiple models generated using the noise induction and decoded using the hierarchical decoding (Narayan and Cohen, 2015).", "startOffset": 133, "endOffset": 158}, {"referenceID": 11, "context": "\u201cBk\u201d makes use of the Berkeley parser with its coarse-to-fine mechanism to optimize the number of latent states (Petrov et al., 2006). For Bk, \u201cvan\u201d uses the vanilla treatment of rare words using signatures defined by Petrov et al. (2006), whereas \u201crep.", "startOffset": 113, "endOffset": 239}, {"referenceID": 10, "context": "\u201cCl\u201d uses the algorithm of Narayan and Cohen (2015) and \u201cSp\u201d uses the algorithm of Cohen et al.", "startOffset": 27, "endOffset": 52}, {"referenceID": 4, "context": "\u201cCl\u201d uses the algorithm of Narayan and Cohen (2015) and \u201cSp\u201d uses the algorithm of Cohen et al. (2013). In Cl, \u201cvan (pos)\u201d and \u201cvan (rep)\u201d are vanilla estimations (i.", "startOffset": 83, "endOffset": 103}, {"referenceID": 1, "context": "For others, we report Bk results from Bj\u00f6rkelund et al. (2013). We also include results from Hall et al.", "startOffset": 38, "endOffset": 63}, {"referenceID": 1, "context": "For others, we report Bk results from Bj\u00f6rkelund et al. (2013). We also include results from Hall et al. (2014) and Crabb\u00e9 (2015).", "startOffset": 38, "endOffset": 112}, {"referenceID": 1, "context": "For others, we report Bk results from Bj\u00f6rkelund et al. (2013). We also include results from Hall et al. (2014) and Crabb\u00e9 (2015).", "startOffset": 38, "endOffset": 130}, {"referenceID": 17, "context": "For the German-N data, Bk results are taken from Petrov (2010). \u201cCl van\u201d shows the performance of the best vanilla models from Table 2 on the test set.", "startOffset": 49, "endOffset": 63}, {"referenceID": 10, "context": "We also include results from Hall et al. (2014), Crabb\u00e9 (2015) and Fern\u00e1ndez-Gonz\u00e1lez and Martins (2015).", "startOffset": 29, "endOffset": 48}, {"referenceID": 10, "context": "(2014), Crabb\u00e9 (2015) and Fern\u00e1ndez-Gonz\u00e1lez and Martins (2015).", "startOffset": 8, "endOffset": 22}, {"referenceID": 10, "context": "(2014), Crabb\u00e9 (2015) and Fern\u00e1ndez-Gonz\u00e1lez and Martins (2015).", "startOffset": 8, "endOffset": 64}, {"referenceID": 5, "context": "To decode with multiple noisy models, we train the MaxEnt reranker of Charniak and Johnson (2005).10 Hi-", "startOffset": 70, "endOffset": 98}, {"referenceID": 17, "context": "See Narayan and Cohen (2015) for more de-", "startOffset": 4, "endOffset": 29}, {"referenceID": 15, "context": "We only use the algorithm of Narayan and Cohen (2015) for the noisy model estimation.", "startOffset": 29, "endOffset": 54}, {"referenceID": 7, "context": "They have shown that decoding with noisy models performs better with their sparse estimates than the dense estimates of Cohen et al. (2013). Implementation: https://github.", "startOffset": 120, "endOffset": 140}, {"referenceID": 19, "context": "11 Our main focus is on comparing the coarse-to-fine Berkeley parser (Petrov et al., 2006)", "startOffset": 69, "endOffset": 90}, {"referenceID": 11, "context": "However, for the sake of completeness, we also present results for other parsers, such as parsers of Hall et al. (2014), Fern\u00e1ndezGonz\u00e1lez and Martins (2015) and Crabb\u00e9 (2015).", "startOffset": 101, "endOffset": 120}, {"referenceID": 11, "context": "However, for the sake of completeness, we also present results for other parsers, such as parsers of Hall et al. (2014), Fern\u00e1ndezGonz\u00e1lez and Martins (2015) and Crabb\u00e9 (2015).", "startOffset": 101, "endOffset": 158}, {"referenceID": 10, "context": "(2014), Fern\u00e1ndezGonz\u00e1lez and Martins (2015) and Crabb\u00e9 (2015).", "startOffset": 49, "endOffset": 63}, {"referenceID": 1, "context": "In line with Bj\u00f6rkelund et al. (2013), our preliminary experiments with the treatment of rare words suggest that morphological features are useful for all SPMRL languages except French.", "startOffset": 13, "endOffset": 38}, {"referenceID": 15, "context": "It is also interesting to compare the clustering algorithm of Narayan and Cohen (2015) to the spectral algorithm of Cohen et al.", "startOffset": 62, "endOffset": 87}, {"referenceID": 7, "context": "It is also interesting to compare the clustering algorithm of Narayan and Cohen (2015) to the spectral algorithm of Cohen et al. (2013). In the vanilla version, the spectral algorithm does better in most cases.", "startOffset": 116, "endOffset": 136}, {"referenceID": 1, "context": "They include transformations of the treebanks such as with unary rules (Bj\u00f6rkelund et al., 2013), a more careful handling of unknown words and better use of morphological informa-", "startOffset": 71, "endOffset": 96}, {"referenceID": 2, "context": "tion such as decorating preterminals with such information (Bj\u00f6rkelund et al., 2014; Sz\u00e1nt\u00f3 and Farkas, 2014), with careful feature specifications (Hall et al.", "startOffset": 59, "endOffset": 109}, {"referenceID": 24, "context": "tion such as decorating preterminals with such information (Bj\u00f6rkelund et al., 2014; Sz\u00e1nt\u00f3 and Farkas, 2014), with careful feature specifications (Hall et al.", "startOffset": 59, "endOffset": 109}, {"referenceID": 12, "context": ", 2014; Sz\u00e1nt\u00f3 and Farkas, 2014), with careful feature specifications (Hall et al., 2014) and head-annotations (Crabb\u00e9, 2015), and other techniques.", "startOffset": 70, "endOffset": 89}, {"referenceID": 10, "context": ", 2014) and head-annotations (Crabb\u00e9, 2015), and other techniques.", "startOffset": 29, "endOffset": 43}], "year": 2016, "abstractText": "We describe a search algorithm for optimizing the number of latent states when estimating latent-variable PCFGs with spectral methods. Our results show that contrary to the common belief that the number of latent states for each nonterminal in an L-PCFG can be decided in isolation with spectral methods, parsing results significantly improve if the number of latent states for each nonterminal is globally optimized, while taking into account interactions between the different nonterminals. In addition, we contribute an empirical analysis of spectral algorithms on eight morphologically rich languages: Basque, French, German, Hebrew, Hungarian, Korean, Polish and Swedish. Our results show that our estimation consistently performs better or close to coarse-to-fine expectation-maximization techniques for", "creator": "dvips(k) 5.991 Copyright 2011 Radical Eye Software"}}}