{"id": "1606.02514", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "8-Jun-2016", "title": "DefExt: A Semi Supervised Definition Extraction Tool", "abstract": "We present DefExt, an easy to use semi supervised Definition Extraction Tool. DefExt is designed to extract from a target corpus those textual fragments where a term is explicitly mentioned together with its core features, i.e. the source text. DefExt is used to extract the base document as well as the text with its core features. DefExt can also generate a source code (e.g. a file with an extension of its core features) to provide a more convenient way to extract the relevant text.\n\n\n\n\n\n\nDefExt is a flexible approach to extract data from a corpus with support for the following features:\n1) The corpus's structure can be used to construct or manipulate the text of a corpus that is the target corpus for any particular purpose. This is implemented by using a built-in form called the default, which is a new form of custom extraction methods.\n2) The default definition of a corpus is:\n1) The word \"defext\" is an optional attribute that is required in the language of the language that the word is intended to be extracted, and it is not required to be used in any way.\n2) The word \"defext\" is defined as the most common definition of the specified syntax, and can be used by all of the following:\n1) A language with a particular syntax.\n2) The word \"defext\" has the corresponding prefix.\n3) The word \"defext\" can be used as the base syntax for the entire corpus.\n4) The default definition of the default definition of the default definition of the default definition of the default definition of the default definition of the default definition of the default definition of the default definition of the default definition of the default definition of the default definition of the default definition of the default definition of the default definition of the default definition of the default definition of the default definition of the default definition of the default definition of the default definition of the default definition of the default definition of the default definition of the default definition of the default definition of the default definition of the default definition of the default definition of the default definition of the default definition of the default definition of the default definition of the default definition of the default definition of the default definition of the default definition of the default definition of the default definition of the default definition of the default definition of the default definition of the default definition of the default definition of the default definition of the default definition of the default definition of the default definition of the default definition of the", "histories": [["v1", "Wed, 8 Jun 2016 11:22:12 GMT  (890kb,D)", "http://arxiv.org/abs/1606.02514v1", "GLOBALEX 2016 Lexicographic Resources for Human Language Technology Workshop Programme (p. 24) (2016, May)"]], "COMMENTS": "GLOBALEX 2016 Lexicographic Resources for Human Language Technology Workshop Programme (p. 24) (2016, May)", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["luis espinosa-anke", "roberto carlini", "horacio saggion", "francesco ronzano"], "accepted": false, "id": "1606.02514"}, "pdf": {"name": "1606.02514.pdf", "metadata": {"source": "CRF", "title": "DEFEXT: A Semi Supervised Definition Extraction Tool", "authors": ["Luis Espinosa-Anke", "Roberto Carlini", "Horacio Saggion", "Francesco Ronzano"], "emails": ["luis.espinosa@upf.edu", "firstname.lastname@upf.edu"], "sections": [{"heading": null, "text": "Keywords: lexicography, definition extraction, bootstrapping"}, {"heading": "1. Introduction", "text": "Definitions are the source of knowledge to consult when the meaning of a term is sought, but manually constructing and updating glossaries is a costly task which requires the cooperative effort of domain experts (Navigli and Velardi, 2010). Exploiting lexicographic information in the form of definitions has proven useful not only for Glossary Building (Muresan and Klavans, 2002; Park et al., 2002) or Question Answering (Cui et al., 2005; Saggion and Gaizauskas, 2004), but also more recently in tasks like Hypernym Extraction (Espinosa-Anke et al., 2015b), Taxonomy Learning (Velardi et al., 2013; Espinosa-Anke et al., 2016) and Knowledge Base Generation (Delli Bovi et al., 2015). Definition Extraction (DE), i.e. the task to automatically extract definitions from naturally occurring text, can be approached by exploiting lexico-syntactic patterns (Rebeyrolle and Tanguy, 2000; Sarmento et al., 2006; Storrer and Wellinghoff, 2006), in a supervised machine learning setting (Navigli et al., 2010; Jin et al., 2013; Espinosa-Anke and Saggion, 2014; Espinosa-Anke et al., 2015a), or leveraging bootstrapping algorithms (Reiplinger et al., 2012; De Benedictis et al., 2013). In this paper, we extend our most recent contribution to DE by releasing DEFEXT1, a toolkit based on experiments described in (Espinosa-Anke et al., 2015c), consisting in machine learning sentence-level DE along with a bootstrapping approach. First, we provide a summary of the foundational components of DEFEXT (Section 2.). Next, we summarize the contribution from which it stems (EspinosaAnke et al., 2015c) as well as its main conclusion, namely that our approach effectively generates a model that gradually adapts to a target domain (Section 3.1.). Furthermore, we introduce one additional evaluation where, after bootstrapping a subset of the ACL Anthology, we present human experts in NLP with definitions and distractors (Section 3.2.), and ask them to judge whether the sentence includes definitional knowledge. Finally, we provide a brief description of the released toolkit along with accompanying enriched corpora to enable immediate use (Section 3.3.).\n1https://bitbucket.org/luisespinosa/defext"}, {"heading": "2. Data Modelling", "text": "DEFEXT is a weakly supervised DE system based on Conditional Random Fields (CRF) which, starting from a set of manually validated definitions and distractors, trains a seed model and iteratively enriches it with high confidence instances (i.e. highly likely definition sentences, and highly likely not definition sentences, such as a text fragments expressing a personal opinion). What differentiates DEFEXT from any supervised system is that thanks to its iterative architecture (see Figure 1), it gradually identifies more appropriate definitions with larger coverage on non-standard text than a system trained only on WordNet glosses or Wikipedia definitions (experimental results supporting this claim are briefly discussed in Section 3.). Twitch Data Gathering Preproc ssing Embeddings Training (word2vec) TwitchVec"}, {"heading": "2.1. Corpora", "text": "We release DEFEXT along with two automatically annotated datasets in column format, where each line represents a word and each column a feature value, the last column being reserved for the token\u2019s label, which in our setting is DEF/NODEF. Sentence splitting is encoded as a double line break. These corpora are enriched versions of (1) The Word Class Lattices corpus (Navigli et al., 2010) (WCLd); and (2) A subset of the ACL Anthology Reference Corpus (Bird et al., 2008) (ACL-ARCd). Statistics about their size in tokens and sentences, as well as definitional distribution (in the case of WCLd) are provided in Table 1.\nar X\niv :1\n60 6.\n02 51\n4v 1\n[ cs\n.C L\n] 8\nJ un\n2 01\n6"}, {"heading": "2.2. Feature Extraction", "text": "The task of DE is modelled as a sentence classification problem, i.e. a sentence may or may not contain a definition. The opting for CRF as the machine learning algorithm is twofold: First, its sequential nature allows us to encode fine-grained features at word level, also considering the context of each word. And second, it has proven useful in previous work in the task of DE (Jin et al., 2013). The features used for modelling the data are based on both linguistic, lexicographic and statistical information, such as:\n\u2022 Linguistic Features: Surface and lemmatized words, part-of-speech, chunking information (NPs) and syntactic dependencies.\n\u2022 Lexicographic Information: A feature that looks at noun phrases and whether they appear at potential definiendum (D) or definiens (d) position2, as illustrated in the following example: The\u3008o-D\u3009 Abwehr\u3008b-D\u3009 was\u3008o-d\u3009 a\u3008o-d\u3009 German\u3008bd\u3009 intelligence\u3008i-d\u3009 organization\u3008i-d\u3009 from\u3008o-d\u3009 1921\u3008o-d\u3009 to\u3008o-d\u3009 1944\u3008o-d\u3009.\n\u2022 Statistical Features: These are features designed to capture the degree of termhood of a word, its frequency in generic or domain-specific corpora, or evidence of their salience in definitional knowledge. These are:\n\u2013 termhood: This metric determines the importance of a candidate token to be a terminological unit by looking at its frequency in general and domain-specific corpora (Kit and Liu, 2008). It is obtained as follows:\nTermhood(w) = rD(w) |VD| \u2212 rB(w) |VB|\nWhere rD is the frequency-wise ranking of word w in a domain corpus (in our case, WCLd), and rB is the frequency-wise ranking of such word in a general corpus, namely the Brown corpus (Francis and Kucera, 1979). Denominators refer to the token-level size of each corpus. If word w only appears in the general corpus, we set the value of Termhood(w) to \u2212\u221e, and to \u221e in the opposite case.\n2The genus et differentia model of a definition, which traces back to Aristotelian times, distinguishes between the definiendum, the term that is being defined, and the definiens, i.e. the cluster of words that describe the core characteristics of the term.\n\u2013 tf-gen: Frequency of the current word in the general-domain corpus rB (Brown Corpus).\n\u2013 tf-dom: Frequency of the current word in the domain-specific corpus rD (WCLd).\n\u2013 tfidf: Tf-idf of the current word over the training set, where each sentence is considered a separate document.\n\u2013 def prom: The notion of Definitional Prominence describes the probability of a word w to appear in a definitional sentence (s = def ). For this, we consider its frequency in definitions and non-definitions in the WCLd as follows:\nDefProm(w) = DF |Defs| \u2212 NF |Nodefs|\nwhere DF = \u2211i=n\ni=0 (si = def \u2227 w \u2208 si) and NF = \u2211i=n i=0 (si = nodef \u2227 w \u2208 si). Similarly as with the termhood feature, in cases where a word w is only found in definitional sentences, we set the DefProm(w) value to\u221e, and to \u2212\u221e if it was only seen in non-definitional sentences.\n\u2013 D prom: Definiendum Prominence, on the other hand, models our intuition that a word appearing more often in position of potential definiendum might reveal its role as a definitional keyword. This feature is computed as follows:\nDP(w) = \u2211i=n\ni=0 wi \u2208 termD |DT |\nwhere termD is a noun phrase (i.e. a term candidate) appearing in potential definiendum position and |DT| refers to the size of the candidate term corpus in candidate definienda position.\n\u2013 d prom: Similarly computed as D prom, but considering position of potential definiens.\nThese features are used to train a CRF algorithm. DEFEXT operates on the back of the CRF toolkit CRF++3, which allows selecting features to be considered at each iteration, as well as the context window."}, {"heading": "2.3. Bootstrapping", "text": "We implemented on DEFEXT a bootstrapping approach inspired by the well-known Yarowsky algorithm for Word Sense Disambiguation (Yarowsky, 1995). It works as follows: Assuming a small set of seed labeled examples(in our case, WCLd), a large target dataset of cases to be classified (ACL-ARCd), and a learning algorithm (CRF), the initial training is performed on the initial seeds in order to classify the whole target data. Those instances classified with high confidence are appended to the training data until convergence or a number of maximum iterations is reached. We apply this methodology to the definition bootstrapping process, and for each iteration, extract the highest confidence definition and the highest confidence non-definition from the target corpus, retrain, and classify again. The\n3https://taku910.github.io/crfpp/\nnumber of maximum iterations may be introduced as an input parameter by the user. Only the latest versions of each corpus are kept in disk."}, {"heading": "3. Experiments and Evaluation", "text": "As the bootstrapping process advances, the trained models gradually become more aware of the linguistic particularities of the genre and register of a target corpus. This allows capturing definition fragments with a particular syntactic structure which may not exist in the original seeds. In this section, we summarize the main conclusions drawn from the experiments performed over two held out test datasets, namely the W00 corpus (Jin et al., 2013), and the MSRNLP corpus (Espinosa-Anke et al., 2015c)4. The former is a manually annotated subset of the ACL anthology, which shows high domain-specifity as well as considerable variability in terms of how a term is introduced and defined (e.g. by means of a comparison or by placing the defined term at the end of the sentence). The latter is compiled manually from a set of abstracts available at the Microsoft Research website5, where the first sentence of each abstract is tagged in the website as a definition. This corpus shows less linguistic variability and thus its definitions are in the vast majority of cases, highly canonical. We show one sample definition from each corpus in Table 26."}, {"heading": "3.1. Definition Extraction", "text": "Starting with the WCLd corpus as seed data, and the ACLARCd collection for bootstrapping, we performed 200 iterations and, at every iteration, we computed Precision, Recall and F-Score at sentence level for both the W00 and the MSR-NLP corpora. At iteration 100, we recalculated the statistical features over the bootstrapped training data (which included 200 more sentences, 100 of each label). The trend for both corpora, shown in Figure 2, indicates that the model improves at classifying definitional knowledge in corpora with greater variability, as performance on the W00 corpus suggests. Moreover, it shows decreasing performance on standard language. The best iterations with their corresponding scores for both datasets are shown in Table 3."}, {"heading": "3.2. Human Evaluation", "text": "In this additional experiment, we assessed the quality of extracted definitions during the bootstrapping process. To this end, we performed 100 iterations over the ACL-ARCd\n4Available at http://taln.upf.edu/MSR-NLP RANLP2015 5http://academic.research.microsoft.com 6Note that, for clarity, we have removed from the examples\nany metainformation present in the original datasets.\ncorpus and presented experts in NLP with 200 sentences (100 candidate definitions and 100 distractors), with shuffled order. Note that since the ACL-ARC corpus comes from parsing pdf papers, there is a considerable amount of noise derived from diverse formatting, presence of equations or tables, and so on. All sentences, however, were presented in their original form, noise included, as in many cases we found that even noise could give the reader an idea of the context in which the sentence was uttered (e.g. if it is followed by a formula, or if it points to a figure or table). The experiment was completed by two judges who had extensive familiarity with the NLP domain and its terminology. Evaluators were allowed to leave the answer field blank if the sentence was unreadable due to noise. In this experiment, DEFEXT reached an average (over the scores provided by both judges) of 0.50 Precision when computed over the whole dataset, and 0.65 if we only consider sentences which were not considered noise by the evaluators. Evaluators found an average of 23 sentences that they considered unreadable."}, {"heading": "3.3. Technical Details", "text": "As mentioned earlier, DEFEXT is a bootstrapping wrapper around the CRF toolkit CRF++, which requires input data to be preprocessed in column-based format. Specifically, each sentence is encoded as a matrix, where each word is a row and each feature is represented as a column, each of them tab-separated. Usually, the word\u2019s surface form or lemma will be at the first or second column, and then other features such as part-of-speech, syntactic dependency or corpus-based features follow. The last column in the dataset is the sentence label, which in DEFEXT is either DEF or NODEF, as it is designed as a sentence classification system. Once training and target data are preprocessed accordingly, one may simply invoke DEFEXT in any Unix machine. Further implementation details and command line arguments can be found in the toolkit\u2019s documentation, as well as in comments throughout the code."}, {"heading": "4. Discussion and Conclusion", "text": "We have presented DEFEXT, a system for weakly supervised DE at sentence level. We have summarized the most outstanding features of the algorithm by referring to experiments which took the NLP domain as a use case (EspinosaAnke et al., 2015c), and complemented them with one additional human evaluation. We have also covered the main requirements for it to function properly, such as data format and command line arguments. No external Python libraries are required, and the only prerequisite is to have CRF++ installed. We hope the research community in lexicography, computational lexicography or corpus linguistics find this tool useful for automating term and definition extraction, for example, as a support for glossary generation or hypernymic (is-a) relation extraction."}, {"heading": "5. Acknowledgements", "text": "This work is partially funded by the Spanish Ministry of Economy and Competitiveness under the Maria de Maeztu Units of Excellence Programme (MDM-2015-0502) and Dr. Inventor (FP7-ICT-2013.8.1611383)."}, {"heading": "6. References", "text": "Bird, S., Dale, R., Dorr, B., Gibson, B., Joseph, M., Kan,\nM.-Y., Lee, D., Powley, B., Radev, D., and Tan, Y. F. (2008). The acl anthology reference corpus: A reference dataset for bibliographic research in computational linguistics. In Proceedings of the Sixth International Conference on Language Resources and Evaluation (LREC08), Marrakech, Morocco, May. European Language Resources Association (ELRA). ACL Anthology Identifier: L08-1005.\nCui, H., Kan, M.-Y., and Chua, T.-S. (2005). Generic soft pattern models for definitional question answering. In Proceedings of the 28th annual international ACM SIGIR conference on Research and development in information retrieval, pages 384\u2013391. ACM.\nDe Benedictis, F., Faralli, S., Navigli, R., et al. (2013). Glossboot: Bootstrapping multilingual domain glossaries from the web. In ACL (1), pages 528\u2013538.\nDelli Bovi, C., Telesca, L., and Navigli, R. (2015). Largescale information extraction from textual definitions through deep syntactic and semantic analysis. Transac-\ntions of the Association for Computational Linguistics, 3:529\u2013543.\nEspinosa-Anke, L. and Saggion, H. (2014). Applying dependency relations to definition extraction. In Natural Language Processing and Information Systems, pages 63\u201374. Springer.\nEspinosa-Anke, L., Saggion, H., and Delli Bovi, C. (2015a). Definition extraction using sense-based embeddings. In International Workshop on Embeddings and Semantics, SEPLN.\nEspinosa-Anke, L., Saggion, H., and Ronzano, F. (2015b). Hypernym extraction: Combining machine learning and dependency grammar. In CICLING 2015, page To appear, Cairo, Egypt. Springer-Verlag.\nEspinosa-Anke, L., Saggion, H., and Ronzano, F. (2015c). Weakly supervised definition extraction. In Proceedings of RANLP 2015.\nEspinosa-Anke, L., Saggion, H., Ronzano, F., and Navigli, R. (2016). Extasem! extending, taxonomizing and semantifying domain terminologies. In AAAI.\nFrancis, W. N. and Kucera, H. (1979). Brown corpus manual. Brown University.\nJin, Y., Kan, M.-Y., Ng, J.-P., and He, X. (2013). Mining scientific terms and their definitions: A study of the ACL anthology. In Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 780\u2013790, Seattle, Washington, USA, October. Association for Computational Linguistics.\nKit, C. and Liu, X. (2008). Measuring mono-word termhood by rank difference via corpus comparison. Terminology, 14(2):204\u2013229.\nMuresan, A. and Klavans, J. (2002). A method for automatically building and evaluating dictionary resources. In Proceedings of the Language Resources and Evaluation Conference (LREC.\nNavigli, R. and Velardi, P. (2010). Learning word-class lattices for definition and hypernym extraction. In ACL, pages 1318\u20131327.\nNavigli, R., Velardi, P., and Ruiz-Mart\u0131\u0301nez, J. M. (2010). An annotated dataset for extracting definitions and hypernyms from the web. In Proceedings of LREC\u201910, Valletta, Malta, may.\nPark, Y., Byrd, R. J., and Boguraev, B. K. (2002). Automatic Glossary Extraction: Beyond Terminology Identi-\nfication. In Proceedings of the 19th International Conference on Computational Linguistics, pages 1\u20137, Morristown, NJ, USA. Association for Computational Linguistics. Rebeyrolle, J. and Tanguy, L. (2000). Repe\u0301rage automatique de structures linguistiques en corpus : le cas des e\u0301nonce\u0301s de\u0301finitoires. Cahiers de Grammaire, 25:153\u2013 174. Reiplinger, M., Scha\u0308fer, U., and Wolska, M. (2012). Extracting glossary sentences from scholarly articles: A comparative evaluation of pattern bootstrapping and deep analysis. In Proceedings of the ACL-2012 Special Workshop on Rediscovering 50 Years of Discoveries, pages 55\u201365, Jeju Island, Korea, July. Association for Computational Linguistics. Saggion, H. and Gaizauskas, R. (2004). Mining on-line sources for definition knowledge. In 17th FLAIRS, Miami Bearch, Florida. Sarmento, L., Maia, B., Santos, D., Pinto, A., and Cabral, L. (2006). Corpo\u0301grafo V3 From Terminological Aid to Semi-automatic Knowledge Engineering. In 5th International Conference on Language Resources and Evaluation (LREC\u201906), Geneva. Storrer, A. and Wellinghoff, S. (2006). Automated detection and annotation of term definitions in German text corpora. In Conference on Language Resources and Evaluation (LREC). Velardi, P., Faralli, S., and Navigli, R. (2013). Ontolearn reloaded: A graph-based algorithm for taxonomy induction. Computational Linguistics, 39(3):665\u2013707. Yarowsky, D. (1995). Unsupervised word sense disambiguation rivaling supervised methods. In Proceedings of the 33rd annual meeting on Association for Computational Linguistics, pages 189\u2013196. Association for Computational Linguistics."}], "references": [{"title": "The acl anthology reference corpus: A reference dataset for bibliographic research in computational linguistics", "author": ["S. Bird", "R. Dale", "B. Dorr", "B. Gibson", "M. Joseph", "Kan", "M.-Y.", "D. Lee", "B. Powley", "D. Radev", "Y.F. Tan"], "venue": "Proceedings of the Sixth International Con-", "citeRegEx": "Bird et al\\.,? 2008", "shortCiteRegEx": "Bird et al\\.", "year": 2008}, {"title": "Generic soft pattern models for definitional question answering", "author": ["H. Cui", "Kan", "M.-Y.", "Chua", "T.-S."], "venue": "In", "citeRegEx": "Cui et al\\.,? 2005", "shortCiteRegEx": "Cui et al\\.", "year": 2005}, {"title": "Glossboot: Bootstrapping multilingual domain glossaries from the web", "author": ["F. De Benedictis", "S. Faralli", "R Navigli"], "venue": "ACL", "citeRegEx": "Benedictis et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Benedictis et al\\.", "year": 2013}, {"title": "Largescale information extraction from textual definitions through deep syntactic and semantic analysis", "author": ["C. Delli Bovi", "L. Telesca", "R. Navigli"], "venue": "Transac-", "citeRegEx": "Bovi et al\\.,? 2015", "shortCiteRegEx": "Bovi et al\\.", "year": 2015}, {"title": "Applying dependency relations to definition extraction", "author": ["L. Espinosa-Anke", "H. Saggion"], "venue": "Natural Language Processing and Information Systems, pages 63\u201374. Springer.", "citeRegEx": "Espinosa.Anke and Saggion,? 2014", "shortCiteRegEx": "Espinosa.Anke and Saggion", "year": 2014}, {"title": "Definition extraction using sense-based embeddings", "author": ["L. Espinosa-Anke", "H. Saggion", "C. Delli Bovi"], "venue": "International Workshop on Embeddings and Semantics, SEPLN.", "citeRegEx": "Espinosa.Anke et al\\.,? 2015a", "shortCiteRegEx": "Espinosa.Anke et al\\.", "year": 2015}, {"title": "Hypernym extraction: Combining machine learning and dependency grammar", "author": ["L. Espinosa-Anke", "H. Saggion", "F. Ronzano"], "venue": "CICLING 2015, page To appear, Cairo, Egypt. Springer-Verlag.", "citeRegEx": "Espinosa.Anke et al\\.,? 2015b", "shortCiteRegEx": "Espinosa.Anke et al\\.", "year": 2015}, {"title": "Weakly supervised definition extraction", "author": ["L. Espinosa-Anke", "H. Saggion", "F. Ronzano"], "venue": "Proceedings of RANLP 2015.", "citeRegEx": "Espinosa.Anke et al\\.,? 2015c", "shortCiteRegEx": "Espinosa.Anke et al\\.", "year": 2015}, {"title": "Extasem! extending, taxonomizing and semantifying domain terminologies", "author": ["L. Espinosa-Anke", "H. Saggion", "F. Ronzano", "R. Navigli"], "venue": "AAAI.", "citeRegEx": "Espinosa.Anke et al\\.,? 2016", "shortCiteRegEx": "Espinosa.Anke et al\\.", "year": 2016}, {"title": "Brown corpus manual", "author": ["W.N. Francis", "H. Kucera"], "venue": "Brown University.", "citeRegEx": "Francis and Kucera,? 1979", "shortCiteRegEx": "Francis and Kucera", "year": 1979}, {"title": "Mining scientific terms and their definitions: A study of the ACL anthology", "author": ["Y. Jin", "Kan", "M.-Y.", "Ng", "J.-P.", "X. He"], "venue": "Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 780\u2013790, Seattle, Washington, USA, October. As-", "citeRegEx": "Jin et al\\.,? 2013", "shortCiteRegEx": "Jin et al\\.", "year": 2013}, {"title": "Measuring mono-word termhood by rank difference via corpus comparison", "author": ["C. Kit", "X. Liu"], "venue": "Terminology, 14(2):204\u2013229.", "citeRegEx": "Kit and Liu,? 2008", "shortCiteRegEx": "Kit and Liu", "year": 2008}, {"title": "A method for automatically building and evaluating dictionary resources", "author": ["A. Muresan", "J. Klavans"], "venue": "Proceedings of the Language Resources and Evaluation Conference (LREC.", "citeRegEx": "Muresan and Klavans,? 2002", "shortCiteRegEx": "Muresan and Klavans", "year": 2002}, {"title": "Learning word-class lattices for definition and hypernym extraction", "author": ["R. Navigli", "P. Velardi"], "venue": "ACL, pages 1318\u20131327.", "citeRegEx": "Navigli and Velardi,? 2010", "shortCiteRegEx": "Navigli and Velardi", "year": 2010}, {"title": "An annotated dataset for extracting definitions and hypernyms from the web", "author": ["R. Navigli", "P. Velardi", "J.M. Ruiz-Mart\u0131\u0301nez"], "venue": "In Proceedings of LREC\u201910,", "citeRegEx": "Navigli et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Navigli et al\\.", "year": 2010}, {"title": "Automatic Glossary Extraction: Beyond Terminology Identi", "author": ["Y. Park", "R.J. Byrd", "B.K. Boguraev"], "venue": null, "citeRegEx": "Park et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Park et al\\.", "year": 2002}, {"title": "Rep\u00e9rage automatique de structures linguistiques en corpus : le cas des \u00e9nonc\u00e9s d\u00e9finitoires", "author": ["J. Rebeyrolle", "L. Tanguy"], "venue": "Cahiers de Grammaire, 25:153\u2013 174.", "citeRegEx": "Rebeyrolle and Tanguy,? 2000", "shortCiteRegEx": "Rebeyrolle and Tanguy", "year": 2000}, {"title": "Extracting glossary sentences from scholarly articles: A comparative evaluation of pattern bootstrapping and deep analysis", "author": ["M. Reiplinger", "U. Sch\u00e4fer", "M. Wolska"], "venue": "Proceedings of the ACL-2012 Special Workshop on Rediscovering 50 Years of Discover-", "citeRegEx": "Reiplinger et al\\.,? 2012", "shortCiteRegEx": "Reiplinger et al\\.", "year": 2012}, {"title": "Mining on-line sources for definition knowledge", "author": ["H. Saggion", "R. Gaizauskas"], "venue": "17th FLAIRS, Miami Bearch, Florida.", "citeRegEx": "Saggion and Gaizauskas,? 2004", "shortCiteRegEx": "Saggion and Gaizauskas", "year": 2004}, {"title": "Corp\u00f3grafo V3 From Terminological Aid to Semi-automatic Knowledge Engineering", "author": ["L. Sarmento", "B. Maia", "D. Santos", "A. Pinto", "L. Cabral"], "venue": "5th International Conference on Language Resources and Evaluation (LREC\u201906), Geneva.", "citeRegEx": "Sarmento et al\\.,? 2006", "shortCiteRegEx": "Sarmento et al\\.", "year": 2006}, {"title": "Automated detection and annotation of term definitions in German text corpora", "author": ["A. Storrer", "S. Wellinghoff"], "venue": "Conference on Language Resources and Evaluation (LREC).", "citeRegEx": "Storrer and Wellinghoff,? 2006", "shortCiteRegEx": "Storrer and Wellinghoff", "year": 2006}, {"title": "Ontolearn reloaded: A graph-based algorithm for taxonomy induction", "author": ["P. Velardi", "S. Faralli", "R. Navigli"], "venue": "Computational Linguistics, 39(3):665\u2013707.", "citeRegEx": "Velardi et al\\.,? 2013", "shortCiteRegEx": "Velardi et al\\.", "year": 2013}, {"title": "Unsupervised word sense disambiguation rivaling supervised methods", "author": ["D. Yarowsky"], "venue": "Proceedings of the 33rd annual meeting on Association for Computational Linguistics, pages 189\u2013196. Association for Computational Linguistics.", "citeRegEx": "Yarowsky,? 1995", "shortCiteRegEx": "Yarowsky", "year": 1995}], "referenceMentions": [{"referenceID": 13, "context": "Definitions are the source of knowledge to consult when the meaning of a term is sought, but manually constructing and updating glossaries is a costly task which requires the cooperative effort of domain experts (Navigli and Velardi, 2010).", "startOffset": 212, "endOffset": 239}, {"referenceID": 12, "context": "Exploiting lexicographic information in the form of definitions has proven useful not only for Glossary Building (Muresan and Klavans, 2002; Park et al., 2002) or Question Answering (Cui et al.", "startOffset": 113, "endOffset": 159}, {"referenceID": 15, "context": "Exploiting lexicographic information in the form of definitions has proven useful not only for Glossary Building (Muresan and Klavans, 2002; Park et al., 2002) or Question Answering (Cui et al.", "startOffset": 113, "endOffset": 159}, {"referenceID": 1, "context": ", 2002) or Question Answering (Cui et al., 2005; Saggion and Gaizauskas, 2004), but also more recently in tasks like Hypernym Extraction (Espinosa-Anke et al.", "startOffset": 30, "endOffset": 78}, {"referenceID": 18, "context": ", 2002) or Question Answering (Cui et al., 2005; Saggion and Gaizauskas, 2004), but also more recently in tasks like Hypernym Extraction (Espinosa-Anke et al.", "startOffset": 30, "endOffset": 78}, {"referenceID": 6, "context": ", 2005; Saggion and Gaizauskas, 2004), but also more recently in tasks like Hypernym Extraction (Espinosa-Anke et al., 2015b), Taxonomy Learning (Velardi et al.", "startOffset": 96, "endOffset": 125}, {"referenceID": 21, "context": ", 2015b), Taxonomy Learning (Velardi et al., 2013; Espinosa-Anke et al., 2016) and Knowledge Base Generation (Delli Bovi et al.", "startOffset": 28, "endOffset": 78}, {"referenceID": 8, "context": ", 2015b), Taxonomy Learning (Velardi et al., 2013; Espinosa-Anke et al., 2016) and Knowledge Base Generation (Delli Bovi et al.", "startOffset": 28, "endOffset": 78}, {"referenceID": 16, "context": "the task to automatically extract definitions from naturally occurring text, can be approached by exploiting lexico-syntactic patterns (Rebeyrolle and Tanguy, 2000; Sarmento et al., 2006; Storrer and Wellinghoff, 2006), in a supervised machine learning setting (Navigli et al.", "startOffset": 135, "endOffset": 218}, {"referenceID": 19, "context": "the task to automatically extract definitions from naturally occurring text, can be approached by exploiting lexico-syntactic patterns (Rebeyrolle and Tanguy, 2000; Sarmento et al., 2006; Storrer and Wellinghoff, 2006), in a supervised machine learning setting (Navigli et al.", "startOffset": 135, "endOffset": 218}, {"referenceID": 20, "context": "the task to automatically extract definitions from naturally occurring text, can be approached by exploiting lexico-syntactic patterns (Rebeyrolle and Tanguy, 2000; Sarmento et al., 2006; Storrer and Wellinghoff, 2006), in a supervised machine learning setting (Navigli et al.", "startOffset": 135, "endOffset": 218}, {"referenceID": 14, "context": ", 2006; Storrer and Wellinghoff, 2006), in a supervised machine learning setting (Navigli et al., 2010; Jin et al., 2013; Espinosa-Anke and Saggion, 2014; Espinosa-Anke et al., 2015a), or leveraging bootstrapping algorithms (Reiplinger et al.", "startOffset": 81, "endOffset": 183}, {"referenceID": 10, "context": ", 2006; Storrer and Wellinghoff, 2006), in a supervised machine learning setting (Navigli et al., 2010; Jin et al., 2013; Espinosa-Anke and Saggion, 2014; Espinosa-Anke et al., 2015a), or leveraging bootstrapping algorithms (Reiplinger et al.", "startOffset": 81, "endOffset": 183}, {"referenceID": 4, "context": ", 2006; Storrer and Wellinghoff, 2006), in a supervised machine learning setting (Navigli et al., 2010; Jin et al., 2013; Espinosa-Anke and Saggion, 2014; Espinosa-Anke et al., 2015a), or leveraging bootstrapping algorithms (Reiplinger et al.", "startOffset": 81, "endOffset": 183}, {"referenceID": 5, "context": ", 2006; Storrer and Wellinghoff, 2006), in a supervised machine learning setting (Navigli et al., 2010; Jin et al., 2013; Espinosa-Anke and Saggion, 2014; Espinosa-Anke et al., 2015a), or leveraging bootstrapping algorithms (Reiplinger et al.", "startOffset": 81, "endOffset": 183}, {"referenceID": 17, "context": ", 2015a), or leveraging bootstrapping algorithms (Reiplinger et al., 2012; De Benedictis et al., 2013).", "startOffset": 49, "endOffset": 102}, {"referenceID": 7, "context": "In this paper, we extend our most recent contribution to DE by releasing DEFEXT1, a toolkit based on experiments described in (Espinosa-Anke et al., 2015c), consisting in machine learning sentence-level DE along with a bootstrapping approach.", "startOffset": 126, "endOffset": 155}, {"referenceID": 14, "context": "These corpora are enriched versions of (1) The Word Class Lattices corpus (Navigli et al., 2010) (WCLd); and (2) A subset of the ACL Anthology Reference Corpus (Bird et al.", "startOffset": 74, "endOffset": 96}, {"referenceID": 0, "context": ", 2010) (WCLd); and (2) A subset of the ACL Anthology Reference Corpus (Bird et al., 2008) (ACL-ARCd).", "startOffset": 71, "endOffset": 90}, {"referenceID": 10, "context": "And second, it has proven useful in previous work in the task of DE (Jin et al., 2013).", "startOffset": 68, "endOffset": 86}, {"referenceID": 11, "context": "\u2013 termhood: This metric determines the importance of a candidate token to be a terminological unit by looking at its frequency in general and domain-specific corpora (Kit and Liu, 2008).", "startOffset": 166, "endOffset": 185}, {"referenceID": 9, "context": "Where rD is the frequency-wise ranking of word w in a domain corpus (in our case, WCLd), and rB is the frequency-wise ranking of such word in a general corpus, namely the Brown corpus (Francis and Kucera, 1979).", "startOffset": 184, "endOffset": 210}, {"referenceID": 22, "context": "We implemented on DEFEXT a bootstrapping approach inspired by the well-known Yarowsky algorithm for Word Sense Disambiguation (Yarowsky, 1995).", "startOffset": 126, "endOffset": 142}, {"referenceID": 10, "context": "In this section, we summarize the main conclusions drawn from the experiments performed over two held out test datasets, namely the W00 corpus (Jin et al., 2013), and the MSRNLP corpus (Espinosa-Anke et al.", "startOffset": 143, "endOffset": 161}, {"referenceID": 7, "context": ", 2013), and the MSRNLP corpus (Espinosa-Anke et al., 2015c)4.", "startOffset": 31, "endOffset": 60}], "year": 2016, "abstractText": "We present DEFEXT, an easy to use semi supervised Definition Extraction Tool. DEFEXT is designed to extract from a target corpus those textual fragments where a term is explicitly mentioned together with its core features, i.e. its definition. It works on the back of a Conditional Random Fields based sequential labeling algorithm and a bootstrapping approach. Bootstrapping enables the model to gradually become more aware of the idiosyncrasies of the target corpus. In this paper we describe the main components of the toolkit as well as experimental results stemming from both automatic and manual evaluation. We release DEFEXT as open source along with the necessary files to run it in any Unix machine. We also provide access to training and test data for immediate use.", "creator": "LaTeX with hyperref package"}}}