{"id": "1606.02560", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "8-Jun-2016", "title": "Towards End-to-End Learning for Dialog State Tracking and Management using Deep Reinforcement Learning", "abstract": "This paper presents an end-to-end framework for task-oriented dialog systems using a variant of Deep Recurrent Q-Networks (DRQN). The model is able to interface with a relational database and jointly learn policies for both language understanding and dialog strategy. Moreover, we propose a hybrid algorithm that combines the strength of reinforcement learning and supervised learning to achieve faster learning speed. We evaluated the proposed model on a 20 Question Game conversational game simulator. Results show that the proposed method outperforms the modular-based baseline and learns a distributed representation of the latent dialog state. In addition, we show that reinforcement learning improves by providing more flexible control over the learning state.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n", "histories": [["v1", "Wed, 8 Jun 2016 14:03:25 GMT  (2922kb,D)", "http://arxiv.org/abs/1606.02560v1", "8 pages. Under peer review of to SigDial 2016"], ["v2", "Thu, 15 Sep 2016 21:50:30 GMT  (3036kb,D)", "http://arxiv.org/abs/1606.02560v2", "In proceeding of SIGDIAL 2016. Added changes based-on peer review, including: 1. Added references, 2. fixed typos in text and figures, 3. added minor change to introduction"]], "COMMENTS": "8 pages. Under peer review of to SigDial 2016", "reviews": [], "SUBJECTS": "cs.AI cs.CL cs.LG", "authors": ["tiancheng zhao", "maxine eskenazi"], "accepted": false, "id": "1606.02560"}, "pdf": {"name": "1606.02560.pdf", "metadata": {"source": "CRF", "title": "Towards End-to-End Learning for Dialog State Tracking and Management using Deep Reinforcement Learning", "authors": ["Tiancheng Zhao"], "emails": ["max+}@cs.cmu.edu"], "sections": [{"heading": "1 Introduction", "text": "Task-oriented dialog systems have been an important branch of spoken dialog system (SDS) research (Raux et al., 2005; Young, 2006; Bohus and Rudnicky, 2003). The SDS agent has to achieve some predefined targets (e.g. booking a flight) through natural language interaction with the users. The typical structure of a task-oriented dialog system is outlined in Figure 1 (Young, 2006). This pipeline consists of several independently-developed modules: natural language understanding (the NLU) maps the user utterances to some semantic representation. This information is further processed by the dialog state tracker (DST), which accumulates the input of the turn along with the dialog history. The DST outputs the current dialog state and the dialog policy selects the next system action based on the dialog state. Then natural language generation (NLG) maps the selected action to its surface form which\nis sent to the TTS (Text-to-Speech). This process repeats until the agent\u2019s goal is satisfied.\nThe conventional SDS pipeline has limitations. The first issue is the credit assignment problem. Developers usually only get feedback from the end users, who inform them about system performance quality. Determining the source of the error requires tedious error analysis in each module because errors from upstream modules can propagate to the rest of the pipeline. The second limitation is process interdependence, which makes online adaptation challenging. For example, when one module (e.g. NLU) is retrained with new data, all the others (e.g DM) that depend on it become sub-optimal due to the fact that they were trained on the output distributions of the older version of the module. Although the ideal solution is to retrain the entire pipeline to ensure global optimality, this requires significant human effort.\nDue to these limitations, the goal of this study is to develop a neural network-based end-to-end framework for task-oriented SDS that replaces 3 important modules: the NLU, the DST and the dialog policy with a single module that can be jointly optimized. Developing such a model for task-\nar X\niv :1\n60 6.\n02 56\n0v 1\n[ cs\n.A I]\n8 J\nun 2\n01 6\noriented dialog systems faces several challenges. The foremost challenge is that a task-oriented system must learn a strategic dialog policy that can achieve the goal of a given task which is beyond the ability of standard supervised learning (Li et al., 2014). The second challenge is that often a task-oriented agent needs to interface with structured external databases, which have symbolic query formats (e.g. SQL query). In order to find answers to the users\u2019 requests from the databases, the agent must formulate a valid database query. This is difficult for conventional neural network models which do not provide intermediate symbolic representations.\nThis paper describes a deep reinforcement learning based end-to-end framework for both dialog state tracking and dialog policy that addresses the above-mentioned issues. We evaluated the proposed approach on a conversational game simulator that requires both language understanding and strategic planning. Our studies yield promising results 1) in jointly learning policies for state tracking and dialog strategies that are superior to a modular-based baseline, 2) in efficiently incorporating various types of labelled data and 3) in learning dialog state representations.\nSection 2 of the paper discusses related work; Section 3 reviews the basics of deep reinforcement learning; Section 4 describes the proposed framework; Section 5 gives experimental results and model analysis; and Section 6 concludes."}, {"heading": "2 Related Work", "text": "Dialog State Tracking: The process of constantly representing the state of the dialog is called dialog state tracking (DST). Most industrial systems use rule-based heuristics to update the dialog state by selecting a high-confidence output from the NLU (Williams et al., 2013). Numerous advanced statistical methods have been proposed to exploit the correlation between turns to make the system more robust given the uncertainty of the automatic speech recognition (ASR) and the NLU (Bohus and Rudnicky, 2006; Thomson and Young, 2010). The Dialog State Tracking Challenge (DSTC) (Williams et al., 2013) formalizes the problem as a supervised sequential labelling task where the state tracker estimates the true slot values based on a sequence of NLU outputs. In practice the output of the state tracker is used by a different dialog policy, so that the distribution\nin the training data and in the live data are mismatched (Williams et al., 2013). Therefore one of the basic assumptions of DSTC is that the state tracker\u2019s performance will translate to better dialog policy performance. Lee (2014) showed positive results following this assumption by showing a positive correlation between end-to-end dialog performance and state tracking performance.\nReinforcement Learning (RL): RL has been a popular approach for learning the optimal dialog policy of a task-oriented dialog system (Singh et al., 2002; Williams and Young, 2007; Georgila and Traum, 2011; Lee and Eskenazi, 2012). A dialog policy is formulated as a Partially Observable Markov Decision Process (POMDP) which models the uncertainty existing in both the users\u2019 goals and the outputs of the ASR and the NLU. Williams (2007) showed that POMDP-based systems perform significantly better than rule-based systems especially when the ASR word error rate (WER) is high. Other work has explored methods that improve the amount of training data needed for a POMDP-based dialog manager. Gas\u030cic\u0301 (2010) utilized Gaussian Process RL algorithms and greatly reduced the data needed for training. Existing applications of RL to dialog management assume a given dialog state representation. Instead, our approach learns its own dialog state representation from the raw dialogs along with a dialog policy in an end-to-end fashion.\nEnd-to-End SDSs: There have been many attempts to develop end-to-end chat-oriented dialog systems that can directly map from the history of a conversation to the next system response (Vinyals and Le, 2015; Serban et al., 2015). These methods train sequence-to-sequence models (Sutskever et al., 2014) on large human-human conversation corpora. The resulting models are able to do basic chatting with users. The work in this paper differs from them by focusing on building a task-oriented system that can interface with structured databases and provide real information to users.\nRecently, Wen el al. (2016) introduced a network-based end-to-end trainable taskedoriented dialog system. Their approach treated a dialog system as a mapping problem between the dialog history and the system response. They learned this mapping via a novel variant of the encoder-decoder model. The main differences between our models and theirs are that ours has the advantage of learning a strategic plan using\nRL and jointly optimizing state tracking beyond standard supervised learning."}, {"heading": "3 Deep Reinforcement Learning", "text": "Before describing the proposed algorithms, we briefly review deep reinforcement learning (RL). RL models are based on the Markov Decision Process (MDP). An MDP is a tuple (S,A, P, \u03b3,R), where S is a set of states; A is a set of actions; P defines the transition probability P (s\u2032|s, a); R defines the expected immediate reward R(s, a); and \u03b3 \u2208 [0, 1) is the discounting factor. The goal of reinforcement learning is to find the optimal policy \u03c0\u2217, such that the expected cumulative return is maximized (Sutton and Barto, 1998). MDPs assume full observability of the internal states of the world, which is rarely true for real-world applications. The Partially Observable Markov Decision Process (POMDP) takes the uncertainty in the state variable into account. A POMDP is defined by a tuple (S,A, P, \u03b3,R,O,Z). O is a set of observations and Z defines an observation probability P (o|s, a). The other variables are the same as the ones in MDPs. Solving a POMDP usually requires computing the belief state b(s), which is the probability distribution of all possible states, such that \u2211 s b(s) = 1. It has been shown that the belief state is sufficient for optimal control (Monahan, 1982), so that the objective is to find \u03c0\u2217 : b \u2192 a that maximizes the expected future return."}, {"heading": "3.1 Deep Q-Network", "text": "The deep Q-Network (DQN) introduced by Mnih (2015) uses a deep neural network (DNN) to parametrize the Q-value function Q(s, a; \u03b8) and achieves human-level performance in playing many Atari games. DQN keeps two separate models: a target network \u03b8\u2212i and a behavior network \u03b8i. For every K new samples, DQN uses \u03b8\u2212i to compute the target values yDQN and updates the parameters in \u03b8i. Only after every C updates, the new weights of \u03b8i are copied over to \u03b8\u2212i . Furthermore, DQN utilizes experience replay to store all previous experience tuples (s, a, r, s\u2032). Before a new model update, the algorithm samples a minibatch of experiences of size M from the memory and computes the gradient of the following loss function:\nL(\u03b8i) = E(s,a,r,s\u2032)[(yDQN \u2212Q(s, a; \u03b8i))2] (1) yDQN = r + \u03b3max\na\u2032 Q(s\u2032, a\u2032; \u03b8\u2212i ) (2)\nRecently, Hasselt et al. (2015) leveraged the overestimation problem of standard Q-Learning by introducing double DQN and Schaul et al. (2015) improves the convergence speed of DQN via prioritized experience replay. We found both modifications useful and included them in our studies."}, {"heading": "3.2 Deep Recurrent Q-Network", "text": "An extension to DQN is a Deep Recurrent QNetwork (DRQN) which introduces a Long ShortTerm Memory (LSTM) layer (Hochreiter and Schmidhuber, 1997) on top of the convolutional layer of the original DQN model (Hausknecht and Stone, 2015) which allows DRQN to solve POMDPs. The recurrent neural network can thus be viewed as an approximation of the belief state that can aggregate information from a sequence of observations. Hausknecht (2015) shows that DRQN performs significantly better than DQN when an agent only observes partial states. A similar model was proposed by Narasimhan and Kulkarni (2015) and learns to play Multi-User Dungeon (MUD) games (Curtis, 1992) with game states hidden in natural language paragraphs."}, {"heading": "4 Proposed Model", "text": ""}, {"heading": "4.1 Overview", "text": "End-to-end learning refers to models that can back-propagate error signals from the end output to the raw inputs. Prior work in end-to-end state tracking (Henderson et al., 2014) learns a sequential classifier that estimates the dialog state based on ASR output without the need of an NLU. Instead of treating state tracking as a standard supervised learning task, we propose to unify dialog state tracking with the dialog policy so that both are treated as actions available to a reinforcement learning agent. Specifically, we learn an optimal policy that either generates a verbal response or modifies the current estimated dialog state based on the new observations. This formulation makes it possible to obtain a state tracker even without the labelled data required for DSTC, as long as the rewards from the users and the databases are available. Furthermore, in cases where dialog state tracking labels are available, the proposed model can incorporate them with minimum modification and greatly accelerate its learning speed. Thus, the following sections describe two models: RL and Hybrid-RL, corresponding to two labelling scenarios: 1) only dialog success labels and 2) dialog\nsuccess labels with state tracking labels."}, {"heading": "4.2 Learning from the Users and Databases", "text": "Figure 2 shows an overview of the framework. We consider a task-oriented dialog task, in which there are S slots, each with cardinality Ci, i \u2208 [0, S). The environment consists of a user, Eu and a database Edb. The agent can send verbal actions, av \u2208 Av to the user and the user will reply with natural language responses ou and rewards ru. In order to interface with the database environment Edb, the agent can apply special actions ah \u2208 Ah that can modify a query hypothesis h. The hypothesis is a slot-filling form that represents the most likely slot values given the observed evidence. Given this hypothesis, h, the database can perform a normal query and give the results as observations, odb and rewards rdb.\nAt each turn t, the agent applies its selected action at \u2208 {Av, Ah} and receives the observations from either the user or the database. We can then define the observation ot of turn t as,\not =  atout odbt  (3) We then use the LSTM network as the dialog state tracker that is capable of aggregating information over turns and generating a dialog state representation, bt = LSTM(ot, bt\u22121), where bt is an approximation of the belief state at turn t. Finally, the dialog state representation from the LSTM network is the input to S + 1 policy networks implemented as Multilayer Perceptrons (MLP). The first policy network approximates the Q-value function for all verbal actions Q(bt, av) while the rest estimate the Q-value function for each slot, Q(bt, av), as shown in Figure 3."}, {"heading": "4.3 Incorporating State Tracking Labels", "text": "The pure RL approach described in the previous section could suffer from slow convergence when the cardinality of slots is large. This is due to the nature of reinforcement learning: that it has to try different actions (possible values of a slot) in order to estimate the expected long-term payoff. On the other hand, a supervised classifier can learn much more efficiently. A typical multi-class classification loss function (e.g. categorical cross entropy) assumes that there is a single correct label such that it encourages the probability of the correct label and suppresses the probabilities of the all the wrong ones. Modeling dialog state tracking as a Q-value function has advantages over a local classifier. For instance, take the situation where a user wants to send an email and the state tracker needs to estimate the user\u2019s goal from among three possible values: send, edit and delete. In a classification task, all the incorrect labels (edit, delete) are treated as equally undesirable. However, the cost of mistakenly recognizing the user goal as delete is much larger than edit, which can only be learned from the future rewards. In order to train the slot-filling policy with both short-term and long-term supervision signals, we decompose the reward function for Ah into two parts:\nQ\u03c0(b, ah) = R\u0304(b, a) + \u03b3 \u2211 b\u2032 P (b\u2032|b, ah)V \u03c0(b\u2032)\n(4)\nR\u0304(b, a, b\u2032) = R(b, ah) + P (ah|b) (5)\nwhere P (ah|b) is the conditional probability that the correct label of the slots is ah given the cur-\nrent belief state. In practice, instead of training a separate model estimating P (ah|b), we can replace P (ah|b) by 1(y = ah) as the sample reward r, where y is the label. Furthermore, a key observation is that although it is expensive to collect data from the user Eu, one can easily sample trajectories of interaction with the database since P (b\u2032|b, ah) is known. Therefore, we can accelerate learning by generating synthetic experiences, i.e. tuple (b, ah, r, b\u2032)\u2200ah \u2208 Ah and add them to the experience replay buffer. This approach is closely related to the Dyna Q-Learning proposed in (Sutton, 1990). The difference is that Dyna Qlearning uses the estimated environment dynamics to generating experiences, while our method only uses the known transition function (i.e. the dynamics of the database) to generate synthetic samples."}, {"heading": "4.4 Implementation Details", "text": "We can optimize the network architecture in several ways to improve its efficiency:\nShared State Tracking Policies: it is more efficient to tie the weights of the policy networks for similar slots and use the index of slot as an input. This can reduce the number of parameters that needs to be learned and encourage shared structures. The studies in Section 5 illustrates one example.\nConstrained Action Mask: We can constrain the available actions at each turn to force the agent to alternate between verbal response and slot-filling. We define Amask as a function that takes state s and outputs a set of available actions for:\nAmask(s) = Ah if there are new inputs (6)\n= Av otherwise (7)\nReward Shaping based on the Database: the reward signals from the users are usually sparse (at the end of a dialog), the database, however, can provide frequent rewards to the agent. Reward shaping is a technique used to speed up learning. Ng et al. (1999) showed that potential-based reward shaping does not alter the optimal solution; it only impacts the learning speed. The pseudo reward function F (s, a, s\u2032) is defined as:\nR\u0304(s, a, s\u2032) = R(s, a, s\u2032) + F (s, a, s\u2032) (8) F (s, a, s\u2032) = \u03b3\u03c6(s\u2032)\u2212 \u03c6(s) (9)\nLet the total number of entities in the database be D and Pmax be the max potential, the potential \u03c6(s) is:\n\u03c6(st) = Pmax(1\u2212 dt D ) if dt > 0 (10) \u03c6(st) = 0 if dt = 0 (11)\nThe intuition of this potential function is to encourage the agent to narrow down the possible range of valid entities as quickly as possible. Meanwhile, if no entities are consistent with the current hypothesis, this implies that there are mistakes in previous slot filling, which gives a potential of 0."}, {"heading": "5 Experiments", "text": ""}, {"heading": "5.1 20Q Game as Task-oriented Dialog", "text": "In order to test the proposed framework, we chose the 20 Question Game (20Q). The game rules are as follows: at the beginning of each game, the user thinks of a famous person. Then the agent asks the user a series of Yes/No questions. The user honestly answers, using one of three answers: yes, no or I don\u2019t know. In order to have this resemble a dialog, our user can answer with any natural utterance representing one of the three intents. The agent can make guesses at any turn, but a wrong guess results in a negative reward. The goal is to guess the correct person within a maximum number of turns with the least number of wrong guesses. An example game conversation is as follows:\nSys: Is this person male? User: Yes I think so. Sys: Is this person an artist? User: He is not an artist. ... Sys: I guess this person is Bill Gates. User: Correct. We can formulate the game as a slot-filling dialog. Assume the system has |Q| available questions to select from at each turn. The answer to each question becomes a slot and each slot has three possible values: yes/no/unknown. Due to the length limit and wrong guess penalty, the optimal policy does not allow the agent to ask all of the questions regardless of the context or guess every person in the database one by one."}, {"heading": "5.2 Simulator Construction", "text": "We constructed a simulator for 20Q. The simulator has two parts: a database of 100 famous people and a user simulator.\nWe selected 100 people from Freebase (Bollacker et al., 2008), each of them has 6 attributes: birthplace, degree, gender, profession and birthday. We manually designed several Yes/No questions for each attribute that is available to the agent. Each question covers a different set of possible values for a given attribute and thus carries a different discriminative power to pinpoint the person that the user is thinking of. As a result, the agent needs to judiciously select the question, given the context of the game, in order to narrow down the range of valid people. There are 31 questions. Table 1 shows a summary.\nAt the beginning of each game, the simulator will first uniformly sample a person from the database as the person it is thinking of. Also there is a 5% chance that the simulator will consider unknown as an attribute and thus it will answer with unknown intent for any question related to it. After the game begins, when the agent asks a question, the simulator first determines the answer (yes, no or unknown) and replies using natural language. In order to generate realistic natural language with the yes/no/unknown intent, we collected utterances from the Switchboard Dialog Act (SWDA) Corpus (Jurafsky et al., 1997). Table 2 presents the mapping from the SWDA dialog acts to yes/no/unknown. We further post-processed results and removed irrelevant utterances, which led to 508, 445 and 251 unique utterances with intent respectively yes/no/unknown. We keep the frequency counts for each unique expression. Thus at run time, the simulator can sample a response according to the original distribution in the SWDA\nCorpus.\nyes/no/unknown\nA game is terminated when one of the four conditions is fulfilled: 1) the agent guesses the correct answer, 2) there are no people in the database consistent with the current hypothesis, 3) the max game length (100 steps) is reached and 4) the max number of guesses is reached (10 guesses). Only if the agent guesses the correct answer (condition 1) treated as a game victory. The win and loss rewards are 30 and \u221230 and a wrong guess leads to a \u22125 penalty."}, {"heading": "5.3 Training Details", "text": "The user environmentEu is the simulator that only accepts verbal actions, either a Yes/No question or a guess, and replies with a natural language utterance. Therefore Av contains |Q| + 1 actions, in which the first |Q| actions are questions and the last action makes a guess, given the results from the database.\nThe database environment reads in a query hypothesis h and returns a list of people that satisfy the constraints in the query. h has a size of |Q| and each dimension can be one of the three values: yes/no/unknown. Since the cardinality for all slots is the same, we only need 1 slot-filling policy network with 3 Q-value outputs for yes/no/unknown, to modify the value of the latest asked question, which is the shared policy approach mentioned in Section 4. Thus Ah = {yes, no, unknown}. For example, considering Q = 3 and the hypothesis h is: [unknown, unknown, unknown]. If the latest asked question is Q1 (1-based), then applying action ah = yes will result in the new hypothesis: [yes, unknown, unknown].\nTo represent the observation ot in vectorial form, we use a bag-of-bigrams feature vector to represent a user utterance; a one-hot vector to represent a system action and a single discrete number to represent the number of people satisfying the current hypothesis.\nThe hyper-parameters of the neural network model are as follows: the size of turn embedding is 30; the size of LSTMs is 256; each policy network has a hidden layer of 128 with tanh nonlinear activation. We also add a dropout rate of 0.3 for both LSTMs and tanh layer outputs. The network has a total of 470,005 parameters. The network was trained through RMSProp (Tieleman and Hinton, 2012). For hyper-parameters of DRQN, the behavior network was updated every 4 steps and the interval between each target network update C is 1000. The reward shaping constant Pmax is 2 and the discounting factor \u03b3 is 0.99. The resulting network was evaluated every 5000 steps and the model was trained up to 120,000 steps. Each evaluation records the agent\u2019s performance with a greedy policy for 200 independent episodes."}, {"heading": "5.4 Dialog Policy Analysis", "text": "We compare the performance of three models: a strong modular baseline, RL and Hybrid-RL. The baseline has an independently trained state tracker and dialog policy. The state tracker is also an LSTM-based classifier that inputs a dialog history and predicts the slot-value of the latest question. The dialog policy is a DRQN that assumes perfect slot-filling during training and simply controls the next verbal action. Thus the essential difference between the baseline and the proposed models is that the state tracker and dialog policy are not trained jointly. Also, since hybrid-RL effectively changes the reward function, the typical average cumulative reward metric is not applicable for performance comparison. Therefore, we directly compare the win rate and average game length in later discussions.\nTable 3 shows that both proposed models achieve significantly higher win rate than the baseline by asking more questions before making guesses. Figure 4 illustrates the learning process of the three models. The baseline model has the fastest learning speed but its performance saturated quickly because the dialog policy was not trained together with the state tracker. So the dia-\nlog policy is not aware of the uncertainty in slotfilling and the slot-filler does not distinguish between the consequences of different wrong labels (e.g classify yes to no versus to unknown). On the other hand, although RL reaches high performance at the end of the training, it struggles in the early stages and suffers from slow convergence. This is due to that fact that correct slot-filling is a prerequisite for winning 20Q, while the reward signal has a long delayed horizon in the RL approach. Finally, the hybrid-RL approach is able to converge to the optimal solution much faster than RL due to the fact that it efficiently exploits the information in the state tracking label."}, {"heading": "5.5 State Tracking Analysis", "text": "One of the hypotheses is that the RL approach can learn a good state tracker using only dialog success reward signals. We ran the best trained models using a greedy policy and collected 10,000 samples. Table 4 reports the precision and recall of slot filling in these trajectories. The results indi-\ncate that the RL model learns a completely different strategy compared to the baseline. The RL model aims for high precision so that it predicts unknown when the input is ambiguous, which is a\nsafer option than predicting yes/no, because confusing between yes and no may potentially lead to a contradiction and a game failure. This is very different from the baseline which does not distinguish between incorrect labels. Therefore, although the baseline achieves better classification metrics, it does not take into account the longterm payoff and performs sub-optimally in terms of overall performance."}, {"heading": "5.6 Dialog State Representation Analysis", "text": "Tracking the state over multiple turns is crucial because the agent\u2019s optimal action depends on the history, e.g. the question it has already asked, the number of guesses it has spent. Furthermore, one of the assumptions is that the output of the LSTM network is an approximation of the belief state in the POMDP. We conducted two studies to test these hypotheses. For both studies, we ran the Hybrid-RL models saved at 20K, 50K and 100K steps against the simulator with a greedy policy and recorded 10,000 samples for each model.\nThe first study checks whether we can reconstruct an important state feature: the number of guesses the agent has made from the dialog state embedding. We divide the collected 10,000 samples into 80% for training and 20% for testing. We used the LSTM output as input features to a linear regression model with l2 regularization. Table 5 shows the correlation of determination r2 increases for the model that was trained with more data.\nThe second study is a retrieval task. The latent state of the 20Q game is the true intent of the users\u2019 answers to all the questions that have been asked so far. Therefore, the true state vector, s has a size of 31 and each slot, s[k], k \u2208 [0, 31) is one of the four values: not yet asked, yes, no, unknown. Therefore, if the LSTM output b is in fact implicitly learning the distribution over this latent state s, they must be highly correlated for a well-trained model. Therefore, for each sample, bi, i \u2208 [0, 10, 000), we find its nearest 5 neighbors N(bi) : B \u2192 [S] based on cosine distance measuring, and compute the probability that si\u2019s true\nstates differ from the ones of the retrieved neighbors:\npdiff(s[k]) = Esi\n[\u22114 n=0 1(N(bi)[n][k] 6= si[k])\n5\n] (12)\nwhere 1 is the indicator function, k is the slot index and n is the neighbor index.\nFigure 5 shows that the retrieval error continues to decrease for the model that was trained better, which confirms our assumption that the LSTM output is an approximation of the belief state."}, {"heading": "6 Conclusion", "text": "This paper identifies the limitations of the conventional SDS pipeline and describes a novel end-toend framework for a task-oriented dialog system using deep reinforcement learning. We have assessed the model on the 20Q game. The proposed models show superior performance for both natural language understanding and dialog strategy. Furthermore, our analysis confirms our hypotheses that the proposed models implicitly capture essential information in the latent dialog states.\nFuture studies will include developing fullfledged task-orientated dialog systems using the proposed approach and exploring methods that allow easy integration of domain knowledge so that the system can be more easily debugged and corrected."}, {"heading": "7 Acknowledgements", "text": "This work was funded by NSF grant CNS1512973. The opinions expressed in this paper do not necessarily reflect those of NSF. We would also like to thank Alan W Black for discussions on this paper."}], "references": [{"title": "Ravenclaw: Dialog management using hierarchical task decomposition and an expectation agenda", "author": ["Dan Bohus", "Alexander I Rudnicky"], "venue": null, "citeRegEx": "Bohus and Rudnicky.,? \\Q2003\\E", "shortCiteRegEx": "Bohus and Rudnicky.", "year": 2003}, {"title": "A k hypotheses+ otherbelief updating model", "author": ["Dan Bohus", "Alex Rudnicky."], "venue": "Proc. of the AAAI Workshop on Statistical and Empirical Methods in Spoken Dialogue Systems.", "citeRegEx": "Bohus and Rudnicky.,? 2006", "shortCiteRegEx": "Bohus and Rudnicky.", "year": 2006}, {"title": "Freebase: a collaboratively created graph database for structuring human knowledge", "author": ["Kurt Bollacker", "Colin Evans", "Praveen Paritosh", "Tim Sturge", "Jamie Taylor."], "venue": "Proceedings of the 2008 ACM SIGMOD international conference on Management", "citeRegEx": "Bollacker et al\\.,? 2008", "shortCiteRegEx": "Bollacker et al\\.", "year": 2008}, {"title": "Mudding: Social phenomena in text-based virtual realities", "author": ["Pavel Curtis."], "venue": "High noon on the electronic frontier: Conceptual issues in cyberspace, pages 347\u2013374.", "citeRegEx": "Curtis.,? 1992", "shortCiteRegEx": "Curtis.", "year": 1992}, {"title": "Gaussian processes for fast policy optimisation of pomdp-based dialogue managers", "author": ["M Ga\u0161i\u0107", "F Jur\u010d\u0131\u0301\u010dek", "Simon Keizer", "Fran\u00e7ois Mairesse", "Blaise Thomson", "Kai Yu", "Steve Young"], "venue": "In Proceedings of the 11th Annual Meeting of the Special Interest", "citeRegEx": "Ga\u0161i\u0107 et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Ga\u0161i\u0107 et al\\.", "year": 2010}, {"title": "Reinforcement learning of argumentation dialogue policies in negotiation", "author": ["Kallirroi Georgila", "David R Traum."], "venue": "INTERSPEECH, pages 2073\u2013 2076.", "citeRegEx": "Georgila and Traum.,? 2011", "shortCiteRegEx": "Georgila and Traum.", "year": 2011}, {"title": "Deep recurrent q-learning for partially observable mdps", "author": ["Matthew Hausknecht", "Peter Stone."], "venue": "arXiv preprint arXiv:1507.06527.", "citeRegEx": "Hausknecht and Stone.,? 2015", "shortCiteRegEx": "Hausknecht and Stone.", "year": 2015}, {"title": "Word-based dialog state tracking with recurrent neural networks", "author": ["Matthew Henderson", "Blaise Thomson", "Steve Young."], "venue": "Proceedings of the 15th Annual Meeting of the Special Interest Group on Discourse and Dialogue (SIGDIAL), pages 292\u2013", "citeRegEx": "Henderson et al\\.,? 2014", "shortCiteRegEx": "Henderson et al\\.", "year": 2014}, {"title": "Long short-term memory", "author": ["Sepp Hochreiter", "J\u00fcrgen Schmidhuber."], "venue": "Neural computation, 9(8):1735\u20131780.", "citeRegEx": "Hochreiter and Schmidhuber.,? 1997", "shortCiteRegEx": "Hochreiter and Schmidhuber.", "year": 1997}, {"title": "Switchboard swbd-damsl shallow-discoursefunction annotation coders manual", "author": ["Dan Jurafsky", "Elizabeth Shriberg", "Debra Biasca."], "venue": "Institute of Cognitive Science Technical Report, pages 97\u2013102.", "citeRegEx": "Jurafsky et al\\.,? 1997", "shortCiteRegEx": "Jurafsky et al\\.", "year": 1997}, {"title": "Pomdpbased let\u2019s go system for spoken dialog challenge", "author": ["Sungjin Lee", "Maxine Eskenazi."], "venue": "Spoken Language Technology Workshop (SLT), 2012 IEEE, pages 61\u201366. IEEE.", "citeRegEx": "Lee and Eskenazi.,? 2012", "shortCiteRegEx": "Lee and Eskenazi.", "year": 2012}, {"title": "Extrinsic evaluation of dialog state tracking and predictive metrics for dialog policy optimization", "author": ["Sungjin Lee."], "venue": "15th Annual Meeting of the Special Interest Group on Discourse and Dialogue, page 310.", "citeRegEx": "Lee.,? 2014", "shortCiteRegEx": "Lee.", "year": 2014}, {"title": "Temporal supervised learning for inferring a dialog policy from example conversations", "author": ["Lihong Li", "He He", "Jason D Williams."], "venue": "Spoken Language Technology Workshop (SLT), 2014 IEEE, pages 312\u2013317. IEEE.", "citeRegEx": "Li et al\\.,? 2014", "shortCiteRegEx": "Li et al\\.", "year": 2014}, {"title": "Human-level control through deep reinforcement learning", "author": ["Volodymyr Mnih", "Koray Kavukcuoglu", "David Silver", "Andrei A Rusu", "Joel Veness", "Marc G Bellemare", "Alex Graves", "Martin Riedmiller", "Andreas K Fidjeland", "Georg Ostrovski"], "venue": null, "citeRegEx": "Mnih et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Mnih et al\\.", "year": 2015}, {"title": "State of the arta survey of partially observable markov decision processes: theory, models, and algorithms", "author": ["George E Monahan."], "venue": "Management Science, 28(1):1\u201316.", "citeRegEx": "Monahan.,? 1982", "shortCiteRegEx": "Monahan.", "year": 1982}, {"title": "Language understanding for textbased games using deep reinforcement learning", "author": ["Karthik Narasimhan", "Tejas Kulkarni", "Regina Barzilay."], "venue": "arXiv preprint arXiv:1506.08941.", "citeRegEx": "Narasimhan et al\\.,? 2015", "shortCiteRegEx": "Narasimhan et al\\.", "year": 2015}, {"title": "Policy invariance under reward transformations: Theory and application to reward shaping", "author": ["Andrew Y Ng", "Daishi Harada", "Stuart Russell."], "venue": "ICML, volume 99, pages 278\u2013287.", "citeRegEx": "Ng et al\\.,? 1999", "shortCiteRegEx": "Ng et al\\.", "year": 1999}, {"title": "Lets go public! taking a spoken dialog system to the real world", "author": ["Antoine Raux", "Brian Langner", "Dan Bohus", "Alan W Black", "Maxine Eskenazi."], "venue": "in Proc. of Interspeech 2005. Citeseer.", "citeRegEx": "Raux et al\\.,? 2005", "shortCiteRegEx": "Raux et al\\.", "year": 2005}, {"title": "Prioritized experience replay", "author": ["Tom Schaul", "John Quan", "Ioannis Antonoglou", "David Silver."], "venue": "arXiv preprint arXiv:1511.05952.", "citeRegEx": "Schaul et al\\.,? 2015", "shortCiteRegEx": "Schaul et al\\.", "year": 2015}, {"title": "Building end-to-end dialogue systems using generative hierarchical neural network models", "author": ["Iulian V Serban", "Alessandro Sordoni", "Yoshua Bengio", "Aaron Courville", "Joelle Pineau."], "venue": "arXiv preprint arXiv:1507.04808.", "citeRegEx": "Serban et al\\.,? 2015", "shortCiteRegEx": "Serban et al\\.", "year": 2015}, {"title": "Optimizing dialogue management with reinforcement learning: Experiments with the njfun system", "author": ["Satinder Singh", "Diane Litman", "Michael Kearns", "Marilyn Walker."], "venue": "Journal of Artificial Intelligence Research, pages 105\u2013133.", "citeRegEx": "Singh et al\\.,? 2002", "shortCiteRegEx": "Singh et al\\.", "year": 2002}, {"title": "Sequence to sequence learning with neural networks", "author": ["Ilya Sutskever", "Oriol Vinyals", "Quoc V Le."], "venue": "Advances in neural information processing systems, pages 3104\u20133112.", "citeRegEx": "Sutskever et al\\.,? 2014", "shortCiteRegEx": "Sutskever et al\\.", "year": 2014}, {"title": "Introduction to reinforcement learning", "author": ["Richard S Sutton", "Andrew G Barto."], "venue": "MIT Press.", "citeRegEx": "Sutton and Barto.,? 1998", "shortCiteRegEx": "Sutton and Barto.", "year": 1998}, {"title": "Integrated architectures for learning, planning, and reacting based on approximating dynamic programming", "author": ["Richard S Sutton."], "venue": "Proceedings of the seventh international conference on machine learning, pages 216\u2013224.", "citeRegEx": "Sutton.,? 1990", "shortCiteRegEx": "Sutton.", "year": 1990}, {"title": "Bayesian update of dialogue state: A pomdp framework for spoken dialogue systems", "author": ["Blaise Thomson", "Steve Young."], "venue": "Computer Speech & Language, 24(4):562\u2013588.", "citeRegEx": "Thomson and Young.,? 2010", "shortCiteRegEx": "Thomson and Young.", "year": 2010}, {"title": "Deep reinforcement learning with double qlearning", "author": ["Hado Van Hasselt", "Arthur Guez", "David Silver."], "venue": "arXiv preprint arXiv:1509.06461.", "citeRegEx": "Hasselt et al\\.,? 2015", "shortCiteRegEx": "Hasselt et al\\.", "year": 2015}, {"title": "A neural conversational model", "author": ["Oriol Vinyals", "Quoc Le."], "venue": "arXiv preprint arXiv:1506.05869.", "citeRegEx": "Vinyals and Le.,? 2015", "shortCiteRegEx": "Vinyals and Le.", "year": 2015}, {"title": "A network-based end-to-end trainable task-oriented dialogue system", "author": ["Tsung-Hsien Wen", "Milica Gasic", "Nikola Mrksic", "Lina M Rojas-Barahona", "Pei-Hao Su", "Stefan Ultes", "David Vandyke", "Steve Young."], "venue": "arXiv preprint arXiv:1604.04562.", "citeRegEx": "Wen et al\\.,? 2016", "shortCiteRegEx": "Wen et al\\.", "year": 2016}, {"title": "Partially observable markov decision processes for spoken dialog systems", "author": ["Jason D Williams", "Steve Young."], "venue": "Computer Speech & Language, 21(2):393\u2013422.", "citeRegEx": "Williams and Young.,? 2007", "shortCiteRegEx": "Williams and Young.", "year": 2007}, {"title": "The dialog state tracking challenge", "author": ["Jason Williams", "Antoine Raux", "Deepak Ramachandran", "Alan Black."], "venue": "Proceedings of the SIGDIAL 2013 Conference, pages 404\u2013413.", "citeRegEx": "Williams et al\\.,? 2013", "shortCiteRegEx": "Williams et al\\.", "year": 2013}, {"title": "Using pomdps for dialog management", "author": ["Steve J Young."], "venue": "SLT, pages 8\u201313.", "citeRegEx": "Young.,? 2006", "shortCiteRegEx": "Young.", "year": 2006}], "referenceMentions": [{"referenceID": 17, "context": "Task-oriented dialog systems have been an important branch of spoken dialog system (SDS) research (Raux et al., 2005; Young, 2006; Bohus and Rudnicky, 2003).", "startOffset": 98, "endOffset": 156}, {"referenceID": 30, "context": "Task-oriented dialog systems have been an important branch of spoken dialog system (SDS) research (Raux et al., 2005; Young, 2006; Bohus and Rudnicky, 2003).", "startOffset": 98, "endOffset": 156}, {"referenceID": 0, "context": "Task-oriented dialog systems have been an important branch of spoken dialog system (SDS) research (Raux et al., 2005; Young, 2006; Bohus and Rudnicky, 2003).", "startOffset": 98, "endOffset": 156}, {"referenceID": 30, "context": "The typical structure of a task-oriented dialog system is outlined in Figure 1 (Young, 2006).", "startOffset": 79, "endOffset": 92}, {"referenceID": 12, "context": "The foremost challenge is that a task-oriented system must learn a strategic dialog policy that can achieve the goal of a given task which is beyond the ability of standard supervised learning (Li et al., 2014).", "startOffset": 193, "endOffset": 210}, {"referenceID": 29, "context": "Most industrial systems use rule-based heuristics to update the dialog state by selecting a high-confidence output from the NLU (Williams et al., 2013).", "startOffset": 128, "endOffset": 151}, {"referenceID": 1, "context": "Numerous advanced statistical methods have been proposed to exploit the correlation between turns to make the system more robust given the uncertainty of the automatic speech recognition (ASR) and the NLU (Bohus and Rudnicky, 2006; Thomson and Young, 2010).", "startOffset": 205, "endOffset": 256}, {"referenceID": 24, "context": "Numerous advanced statistical methods have been proposed to exploit the correlation between turns to make the system more robust given the uncertainty of the automatic speech recognition (ASR) and the NLU (Bohus and Rudnicky, 2006; Thomson and Young, 2010).", "startOffset": 205, "endOffset": 256}, {"referenceID": 29, "context": "The Dialog State Tracking Challenge (DSTC) (Williams et al., 2013) formalizes the problem as a supervised sequential labelling task where the state tracker estimates the true slot values based on a sequence of NLU outputs.", "startOffset": 43, "endOffset": 66}, {"referenceID": 29, "context": "In practice the output of the state tracker is used by a different dialog policy, so that the distribution in the training data and in the live data are mismatched (Williams et al., 2013).", "startOffset": 164, "endOffset": 187}, {"referenceID": 0, "context": "Numerous advanced statistical methods have been proposed to exploit the correlation between turns to make the system more robust given the uncertainty of the automatic speech recognition (ASR) and the NLU (Bohus and Rudnicky, 2006; Thomson and Young, 2010). The Dialog State Tracking Challenge (DSTC) (Williams et al., 2013) formalizes the problem as a supervised sequential labelling task where the state tracker estimates the true slot values based on a sequence of NLU outputs. In practice the output of the state tracker is used by a different dialog policy, so that the distribution in the training data and in the live data are mismatched (Williams et al., 2013). Therefore one of the basic assumptions of DSTC is that the state tracker\u2019s performance will translate to better dialog policy performance. Lee (2014) showed positive results following this assumption by showing a positive correlation between end-to-end dialog performance and state tracking performance.", "startOffset": 206, "endOffset": 820}, {"referenceID": 20, "context": "Reinforcement Learning (RL): RL has been a popular approach for learning the optimal dialog policy of a task-oriented dialog system (Singh et al., 2002; Williams and Young, 2007; Georgila and Traum, 2011; Lee and Eskenazi, 2012).", "startOffset": 132, "endOffset": 228}, {"referenceID": 28, "context": "Reinforcement Learning (RL): RL has been a popular approach for learning the optimal dialog policy of a task-oriented dialog system (Singh et al., 2002; Williams and Young, 2007; Georgila and Traum, 2011; Lee and Eskenazi, 2012).", "startOffset": 132, "endOffset": 228}, {"referenceID": 5, "context": "Reinforcement Learning (RL): RL has been a popular approach for learning the optimal dialog policy of a task-oriented dialog system (Singh et al., 2002; Williams and Young, 2007; Georgila and Traum, 2011; Lee and Eskenazi, 2012).", "startOffset": 132, "endOffset": 228}, {"referenceID": 10, "context": "Reinforcement Learning (RL): RL has been a popular approach for learning the optimal dialog policy of a task-oriented dialog system (Singh et al., 2002; Williams and Young, 2007; Georgila and Traum, 2011; Lee and Eskenazi, 2012).", "startOffset": 132, "endOffset": 228}, {"referenceID": 26, "context": "End-to-End SDSs: There have been many attempts to develop end-to-end chat-oriented dialog systems that can directly map from the history of a conversation to the next system response (Vinyals and Le, 2015; Serban et al., 2015).", "startOffset": 183, "endOffset": 226}, {"referenceID": 19, "context": "End-to-End SDSs: There have been many attempts to develop end-to-end chat-oriented dialog systems that can directly map from the history of a conversation to the next system response (Vinyals and Le, 2015; Serban et al., 2015).", "startOffset": 183, "endOffset": 226}, {"referenceID": 21, "context": "These methods train sequence-to-sequence models (Sutskever et al., 2014) on large human-human conversation corpora.", "startOffset": 48, "endOffset": 72}, {"referenceID": 22, "context": "The goal of reinforcement learning is to find the optimal policy \u03c0\u2217, such that the expected cumulative return is maximized (Sutton and Barto, 1998).", "startOffset": 123, "endOffset": 147}, {"referenceID": 14, "context": "It has been shown that the belief state is sufficient for optimal control (Monahan, 1982), so that the objective is to find \u03c0\u2217 : b \u2192 a", "startOffset": 74, "endOffset": 89}, {"referenceID": 24, "context": "L(\u03b8i) = E(s,a,r,s\u2032)[(y \u2212Q(s, a; \u03b8i))] (1) y = r + \u03b3max a\u2032 Q(s\u2032, a\u2032; \u03b8\u2212 i ) (2) Recently, Hasselt et al. (2015) leveraged the overestimation problem of standard Q-Learning by introducing double DQN and Schaul et al.", "startOffset": 89, "endOffset": 111}, {"referenceID": 18, "context": "(2015) leveraged the overestimation problem of standard Q-Learning by introducing double DQN and Schaul et al. (2015) improves the convergence speed of DQN via prioritized experience replay.", "startOffset": 97, "endOffset": 118}, {"referenceID": 8, "context": "An extension to DQN is a Deep Recurrent QNetwork (DRQN) which introduces a Long ShortTerm Memory (LSTM) layer (Hochreiter and Schmidhuber, 1997) on top of the convolutional layer of the original DQN model (Hausknecht and Stone, 2015) which allows DRQN to solve POMDPs.", "startOffset": 110, "endOffset": 144}, {"referenceID": 6, "context": "An extension to DQN is a Deep Recurrent QNetwork (DRQN) which introduces a Long ShortTerm Memory (LSTM) layer (Hochreiter and Schmidhuber, 1997) on top of the convolutional layer of the original DQN model (Hausknecht and Stone, 2015) which allows DRQN to solve POMDPs.", "startOffset": 205, "endOffset": 233}, {"referenceID": 3, "context": "Kulkarni (2015) and learns to play Multi-User Dungeon (MUD) games (Curtis, 1992) with game states hidden in natural language paragraphs.", "startOffset": 66, "endOffset": 80}, {"referenceID": 7, "context": "tracking (Henderson et al., 2014) learns a sequential classifier that estimates the dialog state based on ASR output without the need of an NLU.", "startOffset": 9, "endOffset": 33}, {"referenceID": 23, "context": "This approach is closely related to the Dyna Q-Learning proposed in (Sutton, 1990).", "startOffset": 68, "endOffset": 82}, {"referenceID": 16, "context": "Ng et al. (1999) showed that potential-based re-", "startOffset": 0, "endOffset": 17}, {"referenceID": 2, "context": "We selected 100 people from Freebase (Bollacker et al., 2008), each of them has 6 attributes:", "startOffset": 37, "endOffset": 61}, {"referenceID": 9, "context": "In order to generate realistic natural language with the yes/no/unknown intent, we collected utterances from the Switchboard Dialog Act (SWDA) Corpus (Jurafsky et al., 1997).", "startOffset": 150, "endOffset": 173}], "year": 2016, "abstractText": "This paper presents an end-to-end framework for task-oriented dialog systems using a variant of Deep Recurrent QNetworks (DRQN). The model is able to interface with a relational database and jointly learn policies for both language understanding and dialog strategy. Moreover, we propose a hybrid algorithm that combines the strength of reinforcement learning and supervised learning to achieve faster learning speed. We evaluated the proposed model on a 20 Question Game conversational game simulator. Results show that the proposed method outperforms the modular-based baseline and learns a distributed representation of the latent dialog state.", "creator": "TeX"}}}