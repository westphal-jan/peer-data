{"id": "1606.02608", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "8-Jun-2016", "title": "Fast and Extensible Online Multivariate Kernel Density Estimation", "abstract": "We present xokde++, a state-of-the-art online kernel density estimation approach that maintains Gaussian mixture models input data streams. The approach follows state-of-the-art work on online density estimation, but was redesigned with computational efficiency, numerical robustness, and extensibility in mind. Our approach produces comparable or better results than the current state-of-the-art, while achieving significant computational performance gains and improved numerical stability, particularly in practice.\n\n\n\n\nThe xokde-compatible Xcode 3.0 release was a significant update, but was also significantly simplified: all new features are available from the new Xcode 3.0 release, and all new features are available from the new Xcode 3.0 release, and all new features are available from the new Xcode 3.0 release, and all new features are available from the new Xcode 3.0 release, and all new features are available from the new Xcode 3.0 release, and all new features are available from the new Xcode 3.0 release, and all new features are available from the new Xcode 3.0 release, and all new features are available from the new Xcode 3.0 release, and all new features are available from the new Xcode 3.0 release, and all new features are available from the new Xcode 3.0 release, and all new features are available from the new Xcode 3.0 release, and all new features are available from the new Xcode 3.0 release, and all new features are available from the new Xcode 3.0 release, and all new features are available from the new Xcode 3.0 release, and all new features are available from the new Xcode 3.0 release, and all new features are available from the new Xcode 3.0 release, and all new features are available from the new Xcode 3.0 release, and all new features are available from the new Xcode 3.0 release, and all new features are available from the new Xcode 3.0 release, and all new features are available from the new Xcode 3.0 release, and all new features are available from the new Xcode 3.0 release, and all new features are available from the new Xcode 3.0 release, and all new features are available from the new Xcode 3.0 release, and all new features are available from the new Xcode 3.0 release, and all new features", "histories": [["v1", "Wed, 8 Jun 2016 15:39:17 GMT  (92kb,D)", "http://arxiv.org/abs/1606.02608v1", "17 pages, 1 figure, 7 tables, submission to Pattern Recognition Letters, review version"]], "COMMENTS": "17 pages, 1 figure, 7 tables, submission to Pattern Recognition Letters, review version", "reviews": [], "SUBJECTS": "cs.LG cs.CV", "authors": ["jaime ferreira", "david martins de matos", "ricardo ribeiro"], "accepted": false, "id": "1606.02608"}, "pdf": {"name": "1606.02608.pdf", "metadata": {"source": "META", "title": "Fast and Extensible Online Multivariate Kernel Density Estimation", "authors": ["Jaime Ferreiraa", "David Martins de Matosa", "Ricardo Ribeiroa"], "emails": ["david.matos@inesc-id.pt"], "sections": [{"heading": null, "text": "In this paper we present xokde++, a state-of-the-art online kernel density estimation approach that maintains Gaussian mixture models input data streams. The approach follows state-of-the-art work on online density estimation, but was redesigned with computational efficiency, numerical robustness, and extensibility in mind. Our approach produces comparable or better results than the current state-of-the-art, while achieving significant computational performance gains and improved numerical stability. The use of diagonal covariance Gaussian kernels, which further improve performance and stability, at a small loss of modelling quality, is also explored. Our approach is up to 40 times faster, while requiring 90% less memory than the closest state-of-the-art counterpart.\nc\u00a9 2016 Elsevier Ltd. All rights reserved."}, {"heading": "1. Introduction", "text": "Online learning is needed when data is not known a priori, its distribution evolves, or when the data model must be updated without interrupting operation: the model must evolve as new data appears, allowing older data to become progressively less important. Our goal is to produce a fast, robust way of handling long-running modelling tasks. Since capturing, processing, and saving all past observed samples is computationally unfeasible, similar samples can be grouped and represented by more compact entities, such as Gaussian mixture models (GMMs).\nKernel density estimation (KDE) methods are nonparametric approaches to build GMMs that do not require prior definition of the number of components. In order to manage complexity, some approaches compress the model to a predefined number of components [7], or optimize some data-driven choice [3]. A different approach [20] is to view model compression as a clustering\n\u2217Corresponding author e-mail: david.matos@inesc-id.pt (David Martins de Matos)\nar X\niv :1\n60 6.\n02 60\n8v 1\n[ cs\n.L G\n] 8\nJ un\n2 01\n6\n2 problem. The main difficulty of adapting KDE methods to online operation lies in maintaining sufficient information to generalize to unobserved data and to adjust model complexity without access to all past samples. An online approach based on mean-shift mode finding adds each new sample to the model as a Gaussian component. However, it is sensitive to non-Gaussian areas due to skewed or heavy tailed data [9]. A two-level approach [4], based on the idea that each component of the (non-overfitting) mixture is in turn represented by an underlying mixture that represents data very precisely (possibly overfitting), allows the model to be refined without sacrificing accuracy, as a merge in the upper level can later be accurately split. It is also able handle the problem of non-Gaussian data by using a uniform distribution in those regions.\nThe online kernel density estimation (oKDE) approach [13, 14] builds a two-level model of the target distribution. It maintains a sample distribution that is used to compute the corresponding KDE. It supports single-sample updates and has a forgetting factor. This allows model adaptation when the target distribution evolves. Compressions approximate component clusters using single Gaussians. If the target distribution changes sufficiently, compressions can be reversed, using the underlying model kept by each component.\nOur approach follows oKDE, obtaining comparable or better results. It allows for flexibility, abstraction, efficient memory use, computational efficiency, and numerical robustness. Furthermore, our C++ implementation, based on state-of-the-art libraries [8], allows easy exploration of modelling alternatives.\nThe article is organized as follows: Section 2 describes oKDE. Section 3 describes our adaptations for numerical stability. Section 4 describes our approach to high dimensionality. Section 5 presents specific computational strategies to improve performance. Section 6 describes the evaluation setup and Section 7 presents our results. Section 8 concludes the article."}, {"heading": "2. Online Kernel Density Estimation", "text": "oKDE was not developed with high dimensionality in mind. Although it allows for any number of dimensions, computational complexity scales quadratically with the number of dimensions, making the approach computationally prohibitive. Furthermore, problems due to overflows, underflows, and precision issues also arise. These issues worsen as the number of dimensions grows, limiting the usefulness of the approach. Before describing our approach, we present oKDE in detail.\n3"}, {"heading": "2.1. Model Definition", "text": "Sample models are defined as d-dimensional, N -component GMMs (Eq. 1): x are d-dimensional observations; wi are mixture\nweights, such that \u03a3Ni=1wi=1; and \u03c6\u03a3i(x\u2212\u00b5i) are d-variate components (Eq. 2), with mean \u00b5i and covariance \u03a3i.\nps(x) = N\u2211 i=1 wi\u03c6\u03a3i(x\u2212 xi) (1)\n\u03c6\u03a3i(x\u2212 \u00b5i) = 1\n2\u03c0D/2|\u03a3i|1/2 exp\n{ 1\n2 (x\u2212 \u00b5i)T\u03a3\u22121i (x\u2212 \u00b5i)\n} (2)\nTo maintain low complexity, ps(x) is compressed when a threshold is met. To allow recovery from mis-compressions, a model of the observations is also kept for each component: Smodel={ps(x), {qi(x)}i=1:N}, where qi(x) is a mixture model (with at most two components) for the i-th component of ps(x).\nThe kernel density estimate is defined as a convolution of ps(x) by a kernel with a covariance (bandwidth) H:\np\u0302KDE(x) = \u03c6H(x) \u2217 ps(x) = M\u2211 i=1 wi\u03c6H+\u03a3i(x\u2212 xi) (3)"}, {"heading": "2.2. Optimal Bandwidth", "text": "KDE approaches determine a bandwidth H that minimizes the distance between p\u0302KDE(x) and (the unknown) p(x) generating the data. The bandwidth can be written as H=\u03b22F (\u03b2 is the scale and F is the structure). Finding H is equivalent to finding the optimal scaling \u03b2opt (Eq. 4), where R(p, F ) \u2248 R\u0302(p, F,G) (Eq. 5) and Aij=(\u03a3gj+\u03a3sj)\u22121, \u2206ij=\u00b5i\u2212\u00b5j , mij=\u2206TijAij\u2206ij , \u03a3gj=G+\u03a3sj , and \u03a3sj is the covariance of the j-th component of ps(x).\n\u03b2opt = [d(4\u03c0) d/2NR(p, F )]\u22121/(d+4) (4)\nR\u0302(p, F,G) = N\u2211 i=1 N\u2211 j=1 wiwj\u03c6A\u22121ij (\u2206ij)\n\u00d7 [ [2tr(F 2A2ij)][1\u2212 2mij ] + tr2(FAij)[1\u2212mij ]2 ] (5)\nG is the pilot bandwidth (Eq. 6). \u03a3\u0302smp is the covariance of a single Gaussian approximating the entire model. The structure of\nH can be approximated by the structure of the covariance of the observations [26, 5], i.e., F=\u03a3\u0302smp.\nG = \u03a3\u0302smp (\n4\n(d+ 2)N\n)2/(d+4) (6)\n4"}, {"heading": "2.3. Model Compression", "text": "Compression approximates the original N -component ps(x) by an M -component, M<N , p\u0302s(x) (Eq. 7), such that the com-\npressed KDE does not change significantly.\np\u0302s(x) = M\u2211 j=1 w\u0302j\u03c6\u03a3\u0302j (x\u2212 x\u0302j) (7)\nClustering approaches can be used to compress ps(x). The aim is to find clusters such that each cluster can be approximated by a single component in p\u0302s(x). Let \u039e(M)={\u03c0j}j=1:M be a set of assignments clustering ps(x) intoM sub-mixtures. The sub-mixture corresponding to indexes i\u2208\u03c0j is defined as\nps(x;\u03c0j) = \u2211 i\u2208\u03c0j wi\u03c6\u03a3i(x\u2212 xi) (8)\nThe parameters of each w\u0302j\u03c6\u03a3\u0302j (x\u2212\u00b5\u0302j) are defined by matching the first two moments (mean and covariance) of \u03c0j\nw\u0302j = \u2211 i\u2208\u03c0j wi, \u00b5\u0302j = w\u0302j \u22121 \u2211 i\u2208\u03c0j wi\u00b5i, (9)\n\u03a3\u0302j = ( w\u0302\u22121j \u2211 i\u2208\u03c0j wi(\u03a3i + \u00b5i\u00b5 T i ) ) \u2212 \u00b5\u0302j \u00b5\u0302jT\n\u039e(M) must be such that M\u0302= arg minM E(\u039e(M)) and E(\u039e(M\u0302))\u2264Dth (a threshold). E(\u039e(M)) is the largest local clustering\nerror E\u0302(ps(x;\u03c0j),Hopt), i.e., the error induced under the KDE if ps(x;\u03c0j) is approximated by a single Gaussian.\nE(\u039e(M)) = max \u03c0j\u2208 \u039e(M) E\u0302(ps(x;\u03c0j),Hopt) (10)"}, {"heading": "2.4. Local Clustering Error", "text": "Consider p1(x), a sub-mixture of ps(x), and p0(x), its single-Gaussian approximation (Eq. 9). The local clustering error is the\ndistance (Eq. 11) between the KDEs. It may be quantified by using the Hellinger distance [22] (Eq. 12).\nE\u0302(p1(x),Hopt) = D(p1KDE(x), p0KDE(x)) (11)\nD2(p1KDE(x), p0KDE(x)) , 1 2 \u222b ( p1KDE(x)1/2 \u2212 p0KDE(x)1/2 )2 dx\n(12)\n5"}, {"heading": "2.5. Distance between mixture models", "text": "The Hellinger distance cannot be calculated analytically for mixture models and is approximated by the unscented trans-\nform [12] (Eq. 13), where {(j)Xi,(j)Wi}j=0:2d+1 are weighted sets of sigma points of the i-th Gaussian \u03c6\u03a3i(x\u2212xi).\nD2(p1, p2) \u2248 1\n2 N\u2211 i=1 wi 2d+1\u2211 j=0 g((j)Xi)(j)Wi (13)\n(0)Xi = xi, (0)Wi = k\nd+ k , k= max([0,m\u2212 d])\n(j)Xi = xi + sj \u221a (d+ k)\u03bej , (j)Wi =\n1\n2(d+ k)\nsj = 1, j \u2264 d\u22121, otherwise Note that \u03bej is the j-th column of \u03be= \u221a \u03a3i (\u03be has d columns). The sigma points are simply xi (j=0) and xi\u00b1\u03bej . Thus, \u03bej must be counted separately for the sj=1 and sj=\u22121 sets, such that j=[1, d] for each set of sigma points. Let UDUT be a singular value decomposition of \u03a3, such that U={U1, . . . , Ud} and D=diag{\u03bb1, . . . , \u03bbd}, then \u03bek= \u221a \u03bbkUk. In line with [12], m was set to 3."}, {"heading": "2.6. Hierarchical Compression", "text": "A hierachical approach avoids evaluating all possible assignments \u039e(M) [7, 11]. ps(x) is first split into two sub-mixtures using Goldberger\u2019s K-means [7]. To avoid singularities associated with Dirac-delta components of ps(x), K-means is applied to p\u0302KDE(x). Next, each sub-mixture is approximated by a single Gaussian (section 2.3) and the sub-mixture yielding the largest local error is further split into two sub-mixtures. The process continues until E(\u039e(M\u0302))\u2264Dth, producing a binary tree where each of the M\u0302 leaves represents the clustering assignments \u039e(M\u0302)={\u03c0i}j=1:M . \u039e(M\u0302) can then be used to approximate the corresponding components in ps(x) by a single Gaussian, resulting in the compressed distribution p\u0302s(x).\nIf two components of ps(x) are merged, their detailed models must also be merged: the detailed model q\u0302j(x) of the j-th component in p\u0302s(x) is calculated by first defining a normalized extended mixture (Eq. 14). If this mixture has more than two components, then the two-component q\u0302j(x) is generated by splitting q\u0302jext(x) into two sub-mixtures using Golberger\u2019s K-means and approximating each sub-mixture by a single Gaussian.\nq\u0302jext(x) = (\u2211 i\u2208\u03c0j wi )\u22121 \u2211 i\u2208\u03c0j wi qi(x) (14)\n6"}, {"heading": "2.7. Revitalization", "text": "Over-compressions can be detected by checking whether the local clustering error E\u0302(qi(x),Hopt) of each ps(x) component, evaluated against its detailed model qi(x), exceedsDth. Those components are removed from ps(x) and replaced by the components of their detailed models. Each new component needs a detailed model: new ones are generated based on their covariances. Let wi\u03c6\u03a3i(x\u2212\u00b5i) be one of the new components. If the determinant of \u03a3i is zero, then the component is a single data-point and its detailed model is just the component itself. Otherwise, the component has been generated through clustering in previous compression steps: its detailed model is initialized by splitting \u03c6\u03a3i(x\u2212\u00b5i) along its principal axis [10] into a two-component mixture, whose first two moments match those of the original component. This has two advantages: first, since the component is symmetric around the mean, the splitting process minimizes the induced error; and second, it is moment preserving, i.e., the mean and covariance of the split Gaussian (and, thus, of the entire mixture), remains unchanged."}, {"heading": "3. Improving Numeric Stability", "text": "In this section, we detail strategies that allow working with skewed or degenerate sample distributions, as well as avoiding\nunderflows and overflows. 3.1. Degenerate Covariance Matrices\nSome data dimensions may seldom change or sufficient observations with different values may not have occurred, causing covariance values of 0 along those axes. One way to detect and correct singular or near-singular matrices is to compute their eigen decomposition \u03a3=Q\u039bQT, with \u039bii=\u03bbi, and check for eigenvalues smaller than 10\u22129. These eigenvalues are then corrected by 1% of the average of the eigenvalues (Eq. 15, 16, 17). The corrected covariance is then given by \u03a3\u2032=Q\u039b\u2032QT.\n\u039b\u0302ii = \u03bb\u0302i = \u03bbi\narg maxi \u03bbi (15)\n\u039b\u2032ii = \u03bb \u2032 i = \u03bbi if \u03bb\u0302i \u2265 10 \u22129 \u03b1 otherwise (16)\n\u03b1 = 1\n100 |{\u03bbj : \u03bb\u0302i > 10\u22129}|\u22121 \u2211 \u03bb\u0302i>10\u22129 \u03bbi, (17)\n7"}, {"heading": "3.2. Determinant Computation", "text": "The Gaussian probability density function (pdf) (Eq. 2) uses the determinant (|\u03a3|) and the inverse (\u03a3\u22121) of the covariance (\u03a3). A way to efficiently compute the inverse of a non-singular matrix is to first compute its LU decomposition and use it to compute the inverse. Since we guarantee that \u03a3 is positive definite, the determinant can be computed by multiplying the diagonal entries of U. However, when working in high dimensionality, overflows or underflows are bound to occur, even when using double precision floating point. To avoid this problem, we compute the logarithm of the determinant instead: this is simply the sum of the logarithm of each entry in the diagonal of matrix U. 3.3. Whitening Transformation\nA single Hopt means that all components are scaled equally in all dimensions. If the spread of data is much greater in one dimension than in others, extreme differences of spread in the various dimensions may occur. This can be avoided by first whitening the data, by transforming it to have unit covariance, smoothing it using a radially symmetric kernel, and, finally, transforming it back [6]. Whitening allows Hopt to better fit the global distribution of the data, leading to better models [24].\nIn our approach, both the computation of Hopt and model compression are performed on whitened data. Whitening starts by approximating the whole model by a single Gaussian. The parameters needed to transform the model covariance into the identity matrix are then computed and individually applied to all components in the mixture. The inverse operation can be used to recover the model.\nFormally, the whitening transformation of data x with covariance \u03a3 is defined as w=\u039b\u22121/2\u03a6Tx, where \u039b and \u03a6 are the\neigenvalues and eigenvectors from the eigendecomposition of the covariance matrix \u03a3.\nWhitening impacts the model in two ways. The first is computational performance: since the structure F is equal to I , it significantly saves computational costs in matrix multiplications (eq. 5). The second is model quality, as both the computation of Hopt and the compression phase, performed using Goldberger\u2019s K-means, benefit from working on spherized data.\n8"}, {"heading": "3.4. Optimal Bandwidth Stability", "text": "Sometimes, observed data may produce zero-valued Hopt. Recall that we assume that the structureF of Hopt is well-approximated\nby the covariance of the entire model and that finding Hopt amounts to finding \u03b2opt (section 2.2). If this scaling factor is zero, e.g. caused by precision issues or by data distribution in the model, Hopt will be zero. This means that no smoothing will be applied to the KDE. In this case, Dirac-deltas will remain with zero covariance, not allowing the use of the standard likelihood function. Since Hopt is computed on whitened data, one solution for this problem is to detect these occurrences and set the bandwidth as the identity matrix, and then computing the backwards whitening transformation."}, {"heading": "4. Using Diagonal Covariance Matrices", "text": "Since a KDE is a linear combination of components, full covariances may not be necessary, even when features are not statistically independent: a second linear combination of (a higher number of) components with diagonal covariances is capable of modelling the correlations between features.\nWith diagonal covariances, computational needs grow linearly with the number of dimensions, a very convenient aspect for high dimensionality applications. This is in contrast with quadratic growth for full covariances. Furthermore, for non-normalized data, diagonal covariances may improve stability, as certain relationships that could result in singular covariances are ignored. Our numerical stability strategies have a stronger impact with diagonal covariances: for data with flat or near-flat dimensions this is particularly relevant (section 7)."}, {"heading": "5. Lazy Operation and Result Buffering", "text": "In theory, each time a sample is added, Hopt should be updated. However, Hopt is only needed for the compression phase or to evaluate the likelihood of a given sample. In other cases, computation of Hopt may be postponed. This means that the cost of adding a sample is simply that of adding one Dirac-delta component to the mixture and updating the current weights. This can be done without impact on model quality.\nThe determinant and the inverse of covariance matrices are needed to compute the likelihood function (which is called very often). Thus, it makes sense to save them, once they have been computed, and keep them until the covariance changes. Then,\n9 previous values are marked as invalid, but are not recomputed immediately. As with Hopt, it is advantageous to postpone these computations until they are actually needed, making the process of adding components to the mixture faster."}, {"heading": "6. Evaluation Setup", "text": "To evaluate our approach and to critically compare our results, we follow the same evaluation strategies of the original oKDE paper [14]. For intrinsic evaluation, we assess model quality by using the average negative log-likelihood and model complexity. For extrinsic evaluation, we assess the accuracy of a classification task. For the latter, we also compare our results with a discriminative approach, since it is typically better suited for classification tasks. We used the online independent support vector machine (OISVM) [18, 19].\nThe datasets (Table 1 and Fig. 1) and evaluation setup from the original oKDE paper were used. Since some of the datasets have been updated, we ran both approaches on the most recent versions. We also evaluated our approach on the L2F Face Database, a high dimensional scenario (section 6.1). For the online classification task, we randomly shuffled the data in each dataset and used 75% of the data to train and the rest to test. For each dataset, to minimize impact of lucky/unlucky partitions, we generated 12 random shuffles. Since we wish to make minimal assumptions about data nature and distribution, we did not do any data preprocessing.\nFor the KDE approaches, each class is represented by the model built from its training samples: for oKDE, each model is initialized with 3 samples before adding the rest, one at a time; in our approach, due to lazy operation, there is no need for\n10\nDataset NS ND NC Iris 150 4 3 Yeast 1484 8 10 Pima 768 8 2 Red (wine quality, red) 1599 11 6 White (wine quality, white) 4898 11 7 Wine 178 13 3 Letter 20000 16 26 Seg (image segmentation) 2310 19 7 Steel 1941 27 7 Cancer (breast cancer) 569 30 2 Skin 245057 3 2 Covtype 581012 10 7\nTable 1: Properties of the considered UCI datasets [15], denoted by number of samples (NS), number of dimensions (ND), and number of classes (NC)\ninitialization, and each model is trained by adding one sample at a time from the very start. To evaluate classification performance we used y\u0302= arg maxk p(x|ck)p(ck).\nFor OISVM, we trained k binary classifiers, using training samples from a class as positive examples, and the other training samples as negative examples. Then, we followed a 1-vs-All approach, y\u0302= arg maxk fk(x), in which fk(x) is the confidence score of the k-th binary classifier for x. A second order polynomial kernel was used with gamma and coef0 parameters set to 1. The complexity parameter C was also set to 1. 6.1. L2F Face Database\nThe L2F Face Database consists of 30,000 indoor free pose face images from 10 subjects (3,000 images per subject). Capture was performed using a PlayStation Eye camera (640x480 pixel). A Haar cascade [25, 1] detected frontal face poses, but also some high inclination and head rotation poses. The square face regions were cropped and resized to 64x64 pixels. During capture, each subject was asked to behave normally and avoid being static, to avoid restrictions on facial pose or expression. Since faces tend to be centered in the cropped square, a fixed mask, roughly selecting parts of eyes, nose, and cheeks, was applied to the crops, producing 128-pixel vectors.\n11"}, {"heading": "7. Results", "text": "Tests were run on an Intel Xeon E5530@2.40GHz, 48GB RAM, Linux openSUSE 13.1. oKDE and OISVM were run on MATLAB R2013a with -nosplash -nodesktop -nodisplay -nojvm. Our approach (xokde++) was compiled enabling all vectorizing options available on our CPU architecture. In the tables, \u201c\u2014\u201d indicates that the model could not be built. 7.1. Model Quality\nAs a proxy for estimation quality, we use the average negative log-likelihood and the average complexity of the models. Tables 2\nand 3 present results after observing all samples.\nRelative to oKDE, xokde++ produces models with similar complexity but with lower average negative log-likelihood (better fits). This is clear in datasets where some dimensions have very little variance while others have very large variance. The \u201csteel\u201d, \u201csegmentation\u201d, and \u201ccancer\u201d datasets are examples of this kind of problem. The numerical stability methods in xokde++ allow recovery from these situations (section 3.1). 7.2. Classifier Accuracy\nTable 4 shows that xokde++ achieves better performance in 7 out of 12 datasets, and lower performance in only 3 datasets. Some of the results for oKDE are different than those reported in the original paper, perhaps due to the fact that no dataset was balanced or\n12\nxokde++ xokde++/d oKDE\nDataset K \u03c3 K \u03c3 K \u03c3 Iris 28 3 22 3 28 3 Yeast 31 15 30 19 31 15 Pima 62 9 42 19 64 7 Red 39 23 53 37 40 24 White 31 24 54 40 36 25 Wine 44 7 44 7 45 7 Letter 65 12 42 9 66 14 Seg 49 12 51 24 52 11 Steel \u2014 \u2014 20 7 \u2014 \u2014 Cancer 40 7 153 11 50 12 Skin 8 2 10 2 8 2 Covtype 18 5 17 5 20 5\nTable 3: Average number of components (K) (\u03c3 = one standard deviation).\nnormalized for unit variance. We intentionally used the datasets with no further processing to study a more realistic online operation scenario, where the sample distribution is not known. These results demonstrate the numerical robustness of xokde++ in handling non-normalized datasets. OISVM typically outperforms the generative approaches, but only slightly. However, it shares some of the oKDE numerical instabilities.\nThe high dimensionality scenario was tested using the L2F Face Database. oKDE, OISVM, and xokde++ (with full covariances), were unable to build models for this dataset. xokde++ (with diagonal covariances) obtained an accuracy of 94.1%, very close to the accuracy of 95.4% obtained by a batch SVM. 7.3. Time and Memory Performance\nxokde++ produces models with similar or better quality than oKDE. Moreover, it does so at lower computational cost. Tables 5 and 6 show the time and memory needed to train and test each dataset: xokde++ achieves speedups from 3 to 10; with diagonal covariances, speedups range from 11 to 40. Regarding memory, xokde++ uses at most 10% of the memory required by oKDE. This difference is more critical in large datasets such as the \u201cskin\u201d and \u201ccovtype\u201d. For these datasets, oKDE needed 913MB and 5064MB, while xokde++ needed only 83MB and 361MB, respectively.\nRegarding memory, OISVM performs poorly when compared with the other approaches. For the \u201cletter\u201d dataset, with 26\n13\nxokde++ xokde++/d oKDE OISVM\nDataset A \u03c3 A \u03c3 A \u03c3 A \u03c3 Iris 96.4 2.7 95.0 3.4 96.4 2.4 97.1 2.1 Yeast 49.7 2.3 48.1 1.6 50.6 3.3 59.2 2.2 Pima 67.8 3.4 70.1 3.9 69.7 2.9 76.9 2.5 Red 62.0 2.5 54.6 1.9 56.9 6.3 58.3 1.9 White 49.9 1.3 42.4 1.7 44.9 10.6 53.2 1.6 Wine 97.7 1.4 98.5 1.8 93.9 6.1 96.8 2.8 Letter 95.8 0.2 93.4 0.4 95.8 0.2 93.0 0.4 Seg 91.5 1.1 89.4 1.2 75.0 5.3 95.0 0.9 Steel \u2014 \u2014 56.9 9.0 \u2014 \u2014 \u2014 \u2014 Cancer 94.8 1.7 96.2 1.7 52.8 12.0 95.9 0.8 Skin 99.6 0.1 99.4 0.0 99.7 0.1 99.8 0.0 Covtype 52.0 1.2 51.6 0.6 68.0 0.9 \u2014 \u2014\nTable 4: Average classification accuracy (A) (\u03c3 = one standard deviation).\nclasses, the memory needed by OISVM was 4 times the required by oKDE and 43 and 78 times more than xokde++ with full and diagonal covariances, respectively. For large datasets, time performance also degrades: the \u201cskin\u201d dataset took nearly twice the time it took for oKDE. This is even clearer for the \u201ccovtype\u201d dataset, which has 500,000 samples: 7 days of computation were not enough to complete training a single shuffle, showing that it was unfeasible to complete the 12 shuffles of our evaluation setup in an acceptable time frame. 7.4. Performance of Full vs. Diagonal Covariances\nTable 7 presents a detailed comparison of the two xokde++ variants, regarding model quality and computational needs. Using diagonal covariances produces models with slightly higher complexity at a small loss in model quality, while needing less memory and greatly improving computational time performance. For the \u201csteel\u201d dataset, diagonal covariances also allowed the model to cope with feature dimensions with low variance, thus avoiding the severe numerical instability found when using full covariances, where the lack of feature variance frequently produced singular matrices.\nFor full covariances, the number of variables increases quadratically with the number of dimensions and datasets with higher dimensionality will benefit from using diagonal covariances. Even though the diagonal approach tends to need more components, partially offsetting memory savings, the average memory usage of the diagonal approach is just 78.5% of the needed for full\n14\nxokde++ xokde++/d oKDE OISVM\nDataset MB \u03c1% MB \u03c1% MB MB \u03c1% Iris 4.6 3.8 4.5 3.7 123.0 116.6 94.8 Yeast 6.8 3.9 5.8 3.4 173.1 148.9 86.0 Pima 6.2 3.9 5.2 3.3 157.5 124.8 79.3 Red 8.1 5.6 6.2 4.2 145.4 162.2 111.5 White 10.3 5.1 8.3 4.1 204.3 228.4 111.8 Wine 6.3 4.4 4.8 3.3 144.3 120.2 83.3 Letter 42.6 9.8 23.2 5.4 433.3 1823.9 420.9 Seg 15.7 7.7 7.7 3.8 203.9 192.3 94.3 Steel \u2014 \u2014 7.2 4.0 \u2014 \u2014 \u2014 Cancer 10.6 5.5 6.6 3.4 193.1 141.8 73.4 Skin 83.1 9.1 83.2 9.1 913.3 2255.1 246.9 Covtype 361.2 7.1 360.4 7.1 5064.8 \u2014 \u2014\nTable 5: Results for memory performance (MB). \u03c1 is the relative usage against the oKDE baseline.\ncovariances. We observed an improvement of this number when we considered more than 15 dimensions: the average memory usage falls to 73% with only small losses in accuracy, around 0.72% (absolute). Considering all datasets, we obtain an average value of 1.67% (absolute) accuracy loss against full covariance models."}, {"heading": "8. Conclusions", "text": "xokde++ is a state-of-the-art online KDE approach. It is efficient, numerically robust, and able to handle high dimensionality. Model quality comparable to that of oKDE, while needing significantly less memory and computation time. The numerical stability improvements allow xokde++ to cope with non-normalized data and achieve an average classifier accuracy of 78% (against oKDE\u2019s 73%). This is a 6.5% average improvement over the baseline. Furthermore, it continues to achieve good results where other approaches fail to even convege (section 7.2). From a software standpoint, xokde++ is extensible, due to its pure template library nature, as shown by the use of diagonal covariances, which were easily implemented and provide aditional time and memory efficiency while sacrificing little model quality.\nBesides speed and memory efficiency, one central contribution of our approach is its adaptability and extensibility. Since it was implemented in an OOP language (C++), the templated classes allow for easy extension, as shown by switching between full and\n15\nxokde++ xokde++/d oKDE OISVM\nDataset time \u03c3 \u03c1 time \u03c3 \u03c1 time \u03c3 time \u03c3 \u03c1 Iris 0.5 0.1 11.0 0.1 0.0 34.2 5.0 0.7 0.2 0.1 26.7 Yeast 39.4 3.2 4.5 6.5 0.7 27.1 177.2 8.1 7.7 0.3 23.1 Pima 16.9 2.1 7.9 3.3 0.3 40.1 133.9 2.3 1.5 0.4 90.1 Red 61.0 7.7 5.4 14.4 0.7 22.6 326.4 27.5 10.6 1.8 30.7 White 130.7 4.0 5.8 39.0 1.7 19.6 764.1 51.3 37.2 3.6 20.5 Wine 3.0 0.5 3.2 0.5 0.1 21.4 9.7 0.6 0.3 0.1 29.4 Letter 2119.0 66.7 3.0 191.7 3.5 33.7 6452.9 191.9 5225.4 1341.0 1.2 Seg 296.8 26.1 1.7 45.0 6.1 11.5 515.7 21.5 84.1 12.0 6.1 Steel \u2014 \u2014 \u2014 8.7 0.5 \u2014 \u2014 \u2014 \u2014 \u2014 \u2014 Cancer 52.6 6.9 2.6 11.7 1.1 11.6 135.6 11.2 0.6 0.1 221.8 Skin 572.9 204.5 11.5 192.2 40.9 34.2 6565.1 1471.0 10522.5 2144.0 0.6 Covtype 9803.8 1614.9 2.6 1156.6 124.5 22.2 25713.3 2081.3 \u2014 \u2014 \u2014\nTable 6: Results for time performance (seconds) (\u03c3 = one standard deviation). \u03c1 is the speedup over the oKDE baseline.\ndiagonal covariances. Other parts of the code are also easily extended, e.g., using triangular covariances, or replacing the Gaussian pdf with a different one. Similarly, changing the compression phase is also a simple process.\nFuture work to improve computational performance includes high-performance computing adaptations. Other improvements include changing the hierarchical compression approach, to another, more paralelizable approach such as pairwise merging of components [23]. Moreover, model quality can also be improved. Instead of using full or diagonal covariances, which are either too large for high dimensionality or too restrictive for certain non-linearly independent data, covariances could be approximated with low-rank perturbations [16], or using Toeplitz matrices [21, 2]. Finally, improvements on numerical stability and model quality may be obtained by avoiding correction of degenerate covariances and, instead, compute pseudo-inverses and pseudo-logdeterminants [17]."}, {"heading": "9. Acknowledgements", "text": "This work was supported by national funds through Fundac\u0327a\u0303o para a Cie\u0302ncia e a Tecnologia (FCT) with reference\nUID/CEC/50021/2013."}], "references": [{"title": "The OpenCV Library", "author": ["G. Bradski"], "venue": "Dr. Dobb\u2019s Journal of Software Tools", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2000}, {"title": "Estimating structured high-dimensional covariance and precision matrices: Optimal rates and adaptive estimation", "author": ["T.T. Cai", "Z. Ren", "H.H. Zhou"], "venue": "The Annals of Statistics", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2014}, {"title": "Probability density estimation with tunable kernels using orthogonal forward regression. Systems, Man, and Cybernetics, Part B: Cybernetics", "author": ["S. Chen", "X. Hong", "C.J. Harris"], "venue": "IEEE Trans. on", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2010}, {"title": "Online learning of Gaussian mixture models - a two-level approach", "author": ["A. Declercq", "J.H. Piater"], "venue": "VISAPP", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2008}, {"title": "Plug-in bandwidth matrices for bivariate kernel density estimation", "author": ["T. Duong", "M. Hazelton"], "venue": "Journal of Nonparametric Statistics", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2003}, {"title": "Introduction to statistical pattern recognition. 2 ed", "author": ["K. Fukunaga"], "venue": "Academic press", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 1990}, {"title": "Hierarchical clustering of a mixture model, in: Advances in Neural Information", "author": ["J. Goldberger", "S.T. Roweis"], "venue": "Processing Systems,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2004}, {"title": "Sequential kernel density approximation and its application to real-time visual tracking", "author": ["B. Han", "D. Comaniciu", "Y. Zhu", "L.S. Davis"], "venue": "Pattern Analysis and Machine Intelligence, IEEE Trans", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2008}, {"title": "Nonlinear Gaussian Filtering: Theory, Algorithms, and Applications", "author": ["M. Huber"], "venue": "KIT Scientific Publishing", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2015}, {"title": "Inference in sensor networks: Graphical models and particle methods", "author": ["A.T. Ihler"], "venue": "Ph.D. thesis. Massachusetts Institute of Technology", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2005}, {"title": "A general method for approximating nonlinear transformations of probability distributions", "author": ["S.J. Julier", "J.K. Uhlmann"], "venue": "Technical Report. RRG,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 1996}, {"title": "Online discriminative kernel density estimator with gaussian kernels", "author": ["M. Kristan", "A. Leonardis"], "venue": "Cybernetics, IEEE Trans. on", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2014}, {"title": "Multivariate online kernel density estimation with Gaussian kernels", "author": ["M. Kristan", "A. Leonardis", "D. Sko\u010daj"], "venue": "Pattern Recognition", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2011}, {"title": "Approximating the covariance matrix of GMMs with low-rank perturbations", "author": ["M. Magdon-Ismail", "J.T. Purnell"], "venue": "Int. J. Data Mining and Bioinformatics", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2009}, {"title": "Multidimensional gaussian probability density and its applications in the degenerate case", "author": ["P. Mikheev"], "venue": "Radiophysics and quantum electronics", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2006}, {"title": "DOGMA: a MATLAB toolbox for Online Learning. http://dogma.sourceforge.net", "author": ["F. Orabona"], "venue": null, "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2009}, {"title": "On-line independent support vector machines", "author": ["F. Orabona", "C. Castellini", "B. Caputo", "L. Jie", "G. Sandini"], "venue": "Pattern Recognition", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2010}, {"title": "Mean shift spectral clustering", "author": ["U. Ozertem", "D. Erdogmus", "R. Jenssen"], "venue": "Pattern Recognition", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2008}, {"title": "The gaussian toeplitz matrix. Linear algebra and its applications", "author": ["J. Pasupathy", "R. Damodar"], "venue": null, "citeRegEx": "21", "shortCiteRegEx": "21", "year": 1992}, {"title": "A user\u2019s guide to measure theoretic probability", "author": ["D. Pollard"], "venue": null, "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2002}, {"title": "Kullback-leibler approach to gaussian mixture reduction", "author": ["A.R. Runnalls"], "venue": "Aerospace and Electronic Systems, IEEE Trans. on", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2007}, {"title": "Density estimation for statistics and data analysis. volume 26", "author": ["B.W. Silverman"], "venue": "CRC press", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 1986}, {"title": "Rapid object detection using a boosted cascade of simple features", "author": ["P. Viola", "M. Jones"], "venue": "in: Computer Vision and Pattern Recognition,", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2001}, {"title": "Kernel smoothing. volume 60 of Monographs on statistics and applied probability", "author": ["M.P. Wand", "M.C. Jones"], "venue": null, "citeRegEx": "26", "shortCiteRegEx": "26", "year": 1994}], "referenceMentions": [{"referenceID": 6, "context": "In order to manage complexity, some approaches compress the model to a predefined number of components [7], or optimize some data-driven choice [3].", "startOffset": 103, "endOffset": 106}, {"referenceID": 2, "context": "In order to manage complexity, some approaches compress the model to a predefined number of components [7], or optimize some data-driven choice [3].", "startOffset": 144, "endOffset": 147}, {"referenceID": 17, "context": "A different approach [20] is to view model compression as a clustering", "startOffset": 21, "endOffset": 25}, {"referenceID": 7, "context": "However, it is sensitive to non-Gaussian areas due to skewed or heavy tailed data [9].", "startOffset": 82, "endOffset": 85}, {"referenceID": 3, "context": "A two-level approach [4], based on the idea that each component of the (non-overfitting) mixture is in turn represented by an underlying mixture that represents data very precisely (possibly overfitting), allows the model to be refined without sacrificing accuracy, as a merge in the upper level can later be accurately split.", "startOffset": 21, "endOffset": 24}, {"referenceID": 11, "context": "The online kernel density estimation (oKDE) approach [13, 14] builds a two-level model of the target distribution.", "startOffset": 53, "endOffset": 61}, {"referenceID": 12, "context": "The online kernel density estimation (oKDE) approach [13, 14] builds a two-level model of the target distribution.", "startOffset": 53, "endOffset": 61}, {"referenceID": 23, "context": "The structure of H can be approximated by the structure of the covariance of the observations [26, 5], i.", "startOffset": 94, "endOffset": 101}, {"referenceID": 4, "context": "The structure of H can be approximated by the structure of the covariance of the observations [26, 5], i.", "startOffset": 94, "endOffset": 101}, {"referenceID": 19, "context": "It may be quantified by using the Hellinger distance [22] (Eq.", "startOffset": 53, "endOffset": 57}, {"referenceID": 10, "context": "The Hellinger distance cannot be calculated analytically for mixture models and is approximated by the unscented transform [12] (Eq.", "startOffset": 123, "endOffset": 127}, {"referenceID": 10, "context": "In line with [12], m was set to 3.", "startOffset": 13, "endOffset": 17}, {"referenceID": 6, "context": "A hierachical approach avoids evaluating all possible assignments \u039e(M) [7, 11].", "startOffset": 71, "endOffset": 78}, {"referenceID": 9, "context": "A hierachical approach avoids evaluating all possible assignments \u039e(M) [7, 11].", "startOffset": 71, "endOffset": 78}, {"referenceID": 6, "context": "ps(x) is first split into two sub-mixtures using Goldberger\u2019s K-means [7].", "startOffset": 70, "endOffset": 73}, {"referenceID": 8, "context": "Otherwise, the component has been generated through clustering in previous compression steps: its detailed model is initialized by splitting \u03c6\u03a3i(x\u2212\u03bci) along its principal axis [10] into a two-component mixture, whose first two moments match those of the original component.", "startOffset": 176, "endOffset": 180}, {"referenceID": 5, "context": "This can be avoided by first whitening the data, by transforming it to have unit covariance, smoothing it using a radially symmetric kernel, and, finally, transforming it back [6].", "startOffset": 176, "endOffset": 179}, {"referenceID": 21, "context": "Whitening allows Hopt to better fit the global distribution of the data, leading to better models [24].", "startOffset": 98, "endOffset": 102}, {"referenceID": 12, "context": "To evaluate our approach and to critically compare our results, we follow the same evaluation strategies of the original oKDE paper [14].", "startOffset": 132, "endOffset": 136}, {"referenceID": 15, "context": "We used the online independent support vector machine (OISVM) [18, 19].", "startOffset": 62, "endOffset": 70}, {"referenceID": 16, "context": "We used the online independent support vector machine (OISVM) [18, 19].", "startOffset": 62, "endOffset": 70}, {"referenceID": 22, "context": "A Haar cascade [25, 1] detected frontal face poses, but also some high inclination and head rotation poses.", "startOffset": 15, "endOffset": 22}, {"referenceID": 0, "context": "A Haar cascade [25, 1] detected frontal face poses, but also some high inclination and head rotation poses.", "startOffset": 15, "endOffset": 22}, {"referenceID": 20, "context": "Other improvements include changing the hierarchical compression approach, to another, more paralelizable approach such as pairwise merging of components [23].", "startOffset": 154, "endOffset": 158}, {"referenceID": 13, "context": "Instead of using full or diagonal covariances, which are either too large for high dimensionality or too restrictive for certain non-linearly independent data, covariances could be approximated with low-rank perturbations [16], or using Toeplitz matrices [21, 2].", "startOffset": 222, "endOffset": 226}, {"referenceID": 18, "context": "Instead of using full or diagonal covariances, which are either too large for high dimensionality or too restrictive for certain non-linearly independent data, covariances could be approximated with low-rank perturbations [16], or using Toeplitz matrices [21, 2].", "startOffset": 255, "endOffset": 262}, {"referenceID": 1, "context": "Instead of using full or diagonal covariances, which are either too large for high dimensionality or too restrictive for certain non-linearly independent data, covariances could be approximated with low-rank perturbations [16], or using Toeplitz matrices [21, 2].", "startOffset": 255, "endOffset": 262}, {"referenceID": 14, "context": "Finally, improvements on numerical stability and model quality may be obtained by avoiding correction of degenerate covariances and, instead, compute pseudo-inverses and pseudo-logdeterminants [17].", "startOffset": 193, "endOffset": 197}], "year": 2016, "abstractText": "In this paper we present xokde++, a state-of-the-art online kernel density estimation approach that maintains Gaussian mixture models input data streams. The approach follows state-of-the-art work on online density estimation, but was redesigned with computational efficiency, numerical robustness, and extensibility in mind. Our approach produces comparable or better results than the current state-of-the-art, while achieving significant computational performance gains and improved numerical stability. The use of diagonal covariance Gaussian kernels, which further improve performance and stability, at a small loss of modelling quality, is also explored. Our approach is up to 40 times faster, while requiring 90% less memory than the closest state-of-the-art counterpart. c \u00a9 2016 Elsevier Ltd. All rights reserved.", "creator": "LaTeX with hyperref package"}}}