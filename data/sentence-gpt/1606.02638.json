{"id": "1606.02638", "review": {"conference": "ACL", "VERSION": "v1", "DATE_OF_SUBMISSION": "8-Jun-2016", "title": "Addressing Limited Data for Textual Entailment Across Domains", "abstract": "We seek to address the lack of labeled data (and high cost of annotation) for textual entailment in some domains. To that end, we first create (for experimental purposes) an entailment dataset for the clinical domain, and a highly competitive supervised entailment system, ENT, that is effective (out of the box) on two domains. We then explore self-training and active learning strategies to address the lack of labeled data. With self-training, we successfully exploit unlabeled data to improve over ENT by 15% F-score on the newswire domain, and 13% F-score on clinical data. On the other hand, our active learning experiments demonstrate that we can match (and even beat) ENT using only 6.6% of the training data in the clinical domain, and only 5.8% of the training data in the newswire domain.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n", "histories": [["v1", "Wed, 8 Jun 2016 16:56:19 GMT  (667kb,D)", "http://arxiv.org/abs/1606.02638v1", null]], "reviews": [], "SUBJECTS": "cs.CL", "authors": ["chaitanya p shivade", "preethi raghavan", "siddharth patwardhan"], "accepted": true, "id": "1606.02638"}, "pdf": {"name": "1606.02638.pdf", "metadata": {"source": "CRF", "title": "Addressing Limited Data for Textual Entailment Across Domains", "authors": ["Chaitanya Shivade", "Preethi Raghavan", "Siddharth Patwardhan"], "emails": ["shivade@cse.ohio-state.edu", "praghav@us.ibm.com", "siddharth@us.ibm.com"], "sections": [{"heading": "1 Introduction", "text": "Textual entailment is the task of automatically determining whether a natural language hypothesis can be inferred from a given piece of natural language text. The RTE challenges (Bentivogli et al., 2009; Bentivogli et al., 2011) have spurred considerable research in textual entailment over newswire data. This, along with the availability of large-scale datasets labeled with entailment information (Bowman et al., 2015), has resulted in a variety of approaches for textual entailment recognition.\n\u2217This work was conducted during an internship at IBM\nA variation of this task, dubbed textual entailment search, has been the focus of RTE-5 and subsequent challenges, where the goal is to find all sentences in a corpus that entail a given hypothesis. The mindshare created by those challenges and the availability of the datasets has spurred many creative solutions to this problem. However, the evaluations have been restricted primarily to these datasets, which are in the newswire domain. Thus, much of the existing state-of-the-art research has focused on solutions that are effective in this domain.\nIt is easy to see though, that entailment search has potential applications in other domains too. For instance, in the clinical domain we imagine entailment search can be applied for clinical trial matching as one example. Inclusion criteria for a clinical trial (for e.g., patient is a smoker) become the hypotheses, and the patient\u2019s electronic health records are the text for entailment search. Clearly, an effective textual entailment search system could possibly one day fully automate clinical trial matching.\nDeveloping an entailment system that works well in the clinical domain and, thus, automates this matching process, requires lots of labeled data, which is extremely scant in the clinical domain. Generating such a dataset is tedious and costly, primarily because it requires medical domain expertise. Moreover, there are always privacy concerns in releasing such a dataset to the community. Taking this into consideration, we investigate the problem of textual entailment in a low-resource setting.\nWe begin by creating a dataset in the clinical domain, and a supervised entailment system that\nar X\niv :1\n60 6.\n02 63\n8v 1\n[ cs\n.C L\n] 8\nJ un\n2 01\n6\nis competitive on multiple domains \u2013 newswire as well as clinical. We then present our work on selftraining and active learning to address the lack of a large-scale labeled dataset. Our self-training system results in significant gains in performance on clinical (+13% F-score) and on newswire (+15% F-score) data. Further, we show that active learning with uncertainty sampling reduces the number of required annotations for the entailment search task by more than 90% in both domains."}, {"heading": "2 Related work", "text": "Recognizing Textual Entailment (RTE) shared tasks (Dagan et al., 2013) conducted annually from 2006 up until 2011 have been the primary drivers of textual entailment research in recent years. Initially the task was defined as that of entailment recognition. RTE-5 (Bentivogli et al., 2009) then introduced the task of entailment search as a pilot. Subsequently, RTE-6 (Bentivogli et al., 2010) and RTE-7 (Bentivogli et al., 2011) featured entailment search as the primary task, but constrained the search space to only those candidate sentences that were first retrieved by Lucene, an open source search engine1. Based on the 80% recall from Lucene in RTE-5, the organizers of RTE-6 and RTE-7 deemed this filter to be an appropriate compromise between the size of the search space and the cost and complexity of the human annotation task.\nAnnotating data for these tasks has remained a challenge since they were defined in the RTE challenges. Successful approaches for entailment (Mirkin et al., 2009; Jia et al., 2010; Tsuchida and Ishikawa, 2011) have relied on annotated data to either train classifiers, or to develop rules for detecting entailing sentences. Operating under the assumption that more labeled data would improve system performance, some researchers have sought to augment their training data with automatically or semi-automatically obtained labeled pairs (Burger and Ferro, 2005; Hickl et al., 2006; Hickl and Bensley, 2007; Zanzotto and Pennacchiotti, 2010; Celikyilmaz et al., 2009).\nBurger and Ferro (2005) automatically create an entailment recognition corpus using the news headline and the first paragraph of a news article as near-paraphrases. Their approach has an estimated accuracy of 70% on a held out set of 500 pairs. The primary limitation of the approach is that it\n1http://lucene.apache.org\nonly generates positive training examples. Hickl et al. (2006) improves upon this work by including negative examples selected using heuristic rules (e.g., sentences connected by although, otherwise, and but). On RTE-2 their method achieves accuracy improvements of upto 10%. However, Hickl and Bensley (2007) achieves only a 1% accuracy improvement on RTE-3 using the same method, suggesting that it is not always as beneficial.\nRecent work by Bowman et al. (2015) describes a method for generating large scale annotated datasets, viz., the Stanford Natural Language Inference (SNLI) Corpus, for the problem of entailment recognition. They use Amazon Mechanical Turk to very inexpensively produce a large entailment annotated data set from image captions.\nZanzotto and Pennacchiotti (2010) create an entailment corpus using Wikipedia data. They handannotate original Wikipedia entries, and their associated revisions for entailment recognition. Using a previously published system for RTE (Zanzotto and Moschitti, 2006), they show that their expanded corpus does not result in improvement for RTE-1, RTE-2 or RTE-3.\nSimilarly, Celikyilmaz et al. (2009) address the lack of labeled data by semi-automatically creating an entailment corpus, which they use within their question answering system. They reuse texthypothesis pairs from RTE challenges in addition to manually annotated pairs from a newswire corpus (with pairs for annotation obtained through a Lucene search over the corpus).\nNote that all of the above research on expanding the labeled data for entailment has focused on entailment recognition. Our focus in this paper is on improving entailment search by exploiting unlabeled data with self-training and active learning."}, {"heading": "3 Datasets", "text": "In this section, we describe the data sets from two domains, newswire and clinical, that we use in the development and evaluation of our work."}, {"heading": "3.1 Newswire Domain", "text": "For the newswire domain, we use entailment search data from the PASCAL RTE-5, RTE-6 and RTE-7 challenges (Bentivogli et al., 2009; Bentivogli et al., 2010; Bentivogli et al., 2011). The dataset consists of a corpus of news documents, along with a set of hypotheses. The hypotheses come from a separate summarization task, where\nthe summary sentences about a news story (given a topic) were manually created by human annotators. These summary sentences are used as hypotheses in the dataset. Entailment annotations are then provided for a subset of sentences from the document corpus, based on a Lucene filter for each hypothesis.\nIn this work, we use the RTE-5 development data to train our system (Newswire-train), RTE-5 test data for evaluation of our systems (Newswiretest), and we use the combined RTE-6 development and test data for our system development and parameter estimation (Newswire-dev). We use all of the development and test data from RTE-7, without the human annotation labels, as our unlabeled data (Newswire-unlabeled) for self-training and active learning experiments. A summary of the newswire data is shown in Table 1."}, {"heading": "3.2 Clinical Domain", "text": "There are no public datasets available for textual entailment search in the clinical domain. In creating this dataset, we imagine a real-world clinical situation where hypotheses are facts about a patient that a physician seeing the patient might want to learn (e.g., The patient underwent a surgi-\ncal procedure within the last three months.). The unstructured notes in the patients electronic medical record (EMR) is the text against which a system would determine the entailment status of the given hypotheses.\nObserve that the aforementioned real-world clinical scenario is very closely related to a question answering problem, where instead of hypotheses a physician may pose natural language questions seeking information about the patient (e.g., Has this patient undergone a surgical procedure within the past three months?). Answers to such questions are words, phrases or passages from the patient\u2019s EMR. Since we have access to a patient-specific question answering dataset over EMRs2 (henceforth, referred to as the QA dataset), we use it here as our starting point in constructing the clinical domain textual entailment dataset.\nGiven a question answering dataset, how might one go about creating a dataset on textual entailment? We follow a methodology similar to that of RTE-1 through RTE-5 for entailment set derived from question answering data. The text corpus in our entailment dataset is the set of de-identified patient records associated with the QA dataset. To generate hypotheses, human annotators converted questions into multiple assertive sentences, which is somewhat similar to what was done in the first five RTE challenges (RTE-1 through RTE-5). For a given question, the human annotators plugged in clinically-plausible answers to convert the question into a statement that may or may not be true about a given patient. Table 2 shows example hypotheses and their source questions. Note that this procedure for hypothesis generation diverges slightly from the RTE procedure, where answers from a question answering system were plugged into the questions to produce assertive sentences.\nTo generate entailment annotations, we paired a hypothesis with every sentence in a subset of clinical notes of the EHR, and asked human annotators to determine if the note sentence enabled them to conclude an entailment relationship with the hypothesis. For example, the text: \u201cThe appearance is felt to be classic for early MS.\u201d entails the hypothesis: \u201cShe has multiple sclerosis\u201d. While in the RTE procedure, a Lucene search was used as a filter to limit the number of hypothesis-sentence pairs that are annotated, in our clinical dataset we\n2a publication describing the question-answering dataset is currently under review at another venue\nlimit the number of annotations by pairing each hypothesis only with sentences from EMR notes containing an answer to the original question in the QA dataset.\nThe entailment annotations were generated by two medical students with the help of the annotations generated for QA. 11 medical students created our QA dataset of 5696 questions over 71 patient records, of which 1747 questions have corresponding answers. This was generated intermittently over a period of 11 months. Given the QA dataset, the time taken to generate entailment annotations includes conversion of questions to hypotheses, and annotating entailment. While conversion of questions to hypotheses took approx. 2 hours for 20 questions, generating about 3000 hypothesis and text pairs took approx. 16 hours.\nAt the end of this process, we had a total of 243 hypotheses annotated against sentences from 380 clinical notes, to generate 25,584 text-hypothesis pairs. We split this into train, development and test sets, summarized in Table 1. Although we have a fairly limited number of labeled text-hypothesis pairs, we do have a large number of patient health records (besides the ones in the annotated set). We generated unlabeled data in the clinical domain, by pairing the hypotheses from our training data with sentences from a set of randomly sampled subset of health records outside of the annotated data.\nDatasets for the textual entailment search task are highly skewed towards the non-entailment class. Note that our clinical data, while smaller in size than the newswire data, maintains a similar class imbalance."}, {"heading": "4 Supervised Entailment System", "text": "We begin by defining, in this section, our supervised entailment system (called ENT) that is used as the basis of our self-training and active learn-\ning experiments. Our system draws upon characteristics and features of systems that have previously been successful in the RTE challenges in the newswire domain. We further enhance this system with new features targeting the clinical domain. The purpose of this section is to demonstrate, through an experimental comparison with other entailment systems, that ENT is competitive on both domains, and is a reasonable supervised system to use in our investigations into selftraining and active learning."}, {"heading": "4.1 System Description", "text": "Top systems (Tsuchida and Ishikawa, 2011; Mirkin et al., 2009) in the RTE challenges have used various types of passage matching approaches in combination with machine learning for entailment. We follow along these lines, and design a classifier-based entailment system. For every text-hypothesis pair in the dataset we extract a feature vector representative of that pair. Then, using the training data, we train a classifier to make entailment decisions on unseen examples. In our system, we employ a logistic regression with ridge estimator (the Weka implementation (Hall et al., 2009)), powered by a variety of passage matching features described below.\nUnderlying many of our passage match features is a more fine-grained notion of \u201cterm match\u201d. Term matchers are a set of algorithms that attempt to match tokens (including multi-word tokens, such as New York or heart attack) across a pair of passages. One of the simplest examples of these is exact string matcher. A token in one text passage that matches exactly, characterfor-character, with a token in another text passage would be considered a term match by this simple term matcher. However, these term matchers could be more sophisticated and match pairs of terms that are synonyms, or paraphrases, or\nequivalent to one another according to other criteria. ENT employs a series of term matchers listed in Table 3. Each of these may also produce a confidence score for every match they find. Because we are working with clinical data, we added some medical domain term matchers as well \u2013 using UMLS (Bodenreider, 2004) and a rule-based \u201ctranslator\u201d of medical terms to layman terms3.\nListed below are all of our features used in the ENT\u2019s classifier. Most passage match features aggregate the output of the term matchers along various linguistic dimensions \u2013 lexical, syntactic, semantic, and document/passage characteristics. Lexical: This set includes a feature aggregating exact string matches across text-hypothesis, one aggregating all term matchers, a feature counting skip-bigram matches (using all matchers), a measure of matched term coverage of text (ratio of matched terms to unmatched terms). Additionally, we have some medical domain features, viz. UMLS concept overlap, and a measure of UMLS-based similarity (Shivade et al., 2015; Pedersen et al., 2007) using the UMLS::Similarity tool (McInnes et al., 2009). Syntactic: Following the lead of several approaches textual entailment (Wang and Zhang, 2009; Mirkin et al., 2009; Kouylekov and Negri, 2010) we have a features measuring the similarity of parse trees. Our rule-based syntactic parser (McCord, 1989) produces dependency parses the text-hypothesis pair, whose nodes are aligned using all of the term matchers. The tree match feature is an aggregation of the aligned subgraphs in the tree (somewhat similar to a tree kernel (Moschitti, 2004)).\n3Rules for medical term translator were derived from http://www.globalrph.com/medterm.htm\nSemantic: We apply open domain as well as medical entity and relation detectors (Wang et al., 2011; Wang et al., 2012) to the texts, and post features measuring overlap in detected entities and overlap in the detected relations across the text-hypothesis pair. We also have a rule-based semantic frame detector for a \u201cmedical finding\u201d frame (patient presenting with symptom or disease). We post a feature that aggregates matched elements of detected frames. Passage Characteristics: Clinical notes typically have a structure and the content is often organized in sections (e.g. History of Illness followed by Physical Examination and ending with Assessment and Plan). We identified the section in which each note sentence was located and used them as features in the classifier. Clinical notes are also classified into many different categories (e.g., discharge summary, radiology report, etc.), which we generate features from. We also generate several features capturing the \u201creadability\u201d of the text segments \u2013 parse failure, list detector, number of verbs, word capitalization, no punctuation and sentence size. We also have a measure of passage topic relevance based on medical concepts in the pair of texts."}, {"heading": "4.2 System Performance", "text": "To compare effectiveness of ENT on the entailment task, we chose two publicly available systems \u2013 EDITS and TIE \u2013 for comparison. Both these system are available under the Excitement Open Platform (EOP), an initiative (Magnini et al., 2014) to make tools for textual entailment freely available4 to the NLP community. EDITS (Edit Distance Textual Entailment Suite) by Kouylekov and Negri (2010) is an open source textual entailment system that uses a set of rules and resources to perform \u201cedit\u201d operations on the text to convert it into the hypothesis. There are costs associated with the operations, and an overall cost is computed for the text-hypothesis pair, which determines the decision for that pair. This system has placed third (out of eight teams) in RTE5, and seventh (out of thirteen teams) in RTE-7. The Textual Inference Engine (TIE) (Wang and Zhang, 2009) is a maximum entropy based entailment system relying on predicate argument structure matching. While this system did not partici-\n4http://hltfbk.github.io/ Excitement-Open-Platform/\npate in the RTE challenges, it has been shown to be effective on the RTE datasets. In our experiments, we trained the EDITS system optimizing for Fscore (the default optimization criterion is accuracy) and TIE with its default settings. We also used a Lucene baseline similar to the one used in RTE-5, RTE-6 and RTE-7 entailment challenges.\nWe trained the systems on the training set of each domain and tested on the test set. The Lucene baseline considers the first N sentences (where N is 5, 10, 15 or 20) top-ranked by the search engine to be entailing the hypothesis. The configuration with the top 10 sentences performed the best, and is reported in the results. Note that this baseline is a strong one, and none of the systems participating in RTE-5 could beat it.\nTable 4 summarizes the system performance on newswire and clinical data. We observe that systems that did well on RTE datasets, were mediocre on the clinical dataset. We did not, however, put any effort into adaption of TIE and EDITS to the clinical data. So the mediocre performance on clinical is understandable. It is interesting to see though that ENT did well (comparatively) on both domains.\nWe note that our problem setting is most similar to the RTE-5 entailment search task. Of the 20 runs across eight teams that participated in RTE-5, the median F-Score was 0.30 and the best system (Mirkin et al., 2009) achieved an F-Score of 0.46. EDITS and TIE perform slightly above the median and ENT (with 0.39 F-score) would have ranked third in the challenge.\nThe performance of all systems on the clinical data is noticeably low as compared to the newswire data. An obvious difference in the two domains is the training data size (see Table 1). However, obtaining annotations for textual entailment search is expensive, particularly in the clinical domain. The remaining sections present our investigations into self-training and active learning, to overcome the lack of training data."}, {"heading": "5 Self-Training", "text": "Our goal is to exploit unlabeled data, with the hope of augmenting the limited annotated data in a given domain. Self-training is a method that has been successfully used to address limited training data on many NLP tasks, such as parsing (McClosky et al., 2006), information extraction (Huang and Riloff, 2012; Patwardhan and Riloff, 2007), word sense disambiguation (Mihalcea, 2004), etc. Self-training iteratively increases the size of the training set, by automatically assigning labels to unlabeled examples, using a model trained in a previous iteration of the self-training regime.\nFor our newswire and clinical datasets, using the set of unlabeled text-hypothesis pairs U , we ran the following training regime: A model was created using the training data Ln, and applied it to the unlabeled data U . From U , all such pairs that were classified by the model as entailing pairs with high confidence (above a threshold \u03c4 ) were added to the labeled training data Ln to generate Ln+1. Non-entailing pairs were ignored. A new model is trained on data Ln+1, and the above process repeated iteratively, until a stopping criteria is reached (in our case, all pairs from U are exhausted).\nThe threshold \u03c4 determines the confidence of our model for a text-hypothesis pair being classified to the entailment class. This threshold was tuned by varying it incrementally from 0.1 to 0.9 in steps of 0.1. The best \u03c4 was determined on the development set, and chosen for the self-training system. Figure 2 shows the effect of \u03c4 on the development data.\nAs such, we see that the F-score of the selftrained model is always above that of the baseline ENT system. The F-score increases upto a peak of 0.33 at threshold \u03c4 of 0.2 before dropping at higher thresholds. Using this tuned threshold on test set, the comparitive performance on the test set is outlined in Table 5. We observe an F-score\nof 0.36, which is significantly greater than that of the vanilla ENT system (0.23).\nThe effect of the threshold on performance correlates with the number of instances added to the training set. When the threshold is low, there are more instances being added (10,799 at threshold of 0.1) into the training set. Therefore, recall is likely to benefit, since the model is exposed to a larger variety of text-hypothesis pairs. However, the precision is low since noisy pairs are likely to be added. When the threshold is high, fewer instances are added (316 at threshold of 0.9). These are the ones that the model is most certain about, suggesting that these are likely to be less noisy. Therefore, the precision is comparatively high.\nWe also ran our self-training approach on the Newswire datasets. We observed similar variations in performance with newswire data as with the clinical data. At threshold of 0.9, fewer instances (49) are added to the training set from the unlabeled data, while a large number of instances (2,861) are added at a lower threshold \u03c4 of 0.1.\nThe best performance (F-score of 0.52) was obtained at threshold of 0.3, on the development set.\nThis threshold also resulted in the best performance (0.54) on the test set. Similar to the clinical domain, precision increased but recall decreased as the threshold increased. Again, it is evident from Table 5 that gains obtained from self-training are due to recall. It should be noted that the selftrained system achieves an F-score of 0.54 \u2013 substantially better than the best performing system of Mirkin et al. (2009) (F-score, 0.46) in RTE-5."}, {"heading": "6 Active Learning", "text": "Active learning is a popular training paradigm in machine learning (Settles, 2012) where a learning agent interacts with its environment in acquiring a training set, rather than passively receiving independent samples from an underlying distribution. This is especially pertinent in the clinical domain, where input from a medical professional should be sought only when really necessary, because of the high cost of such input. The purpose of exploring\nthis paradigm is to achieve the best possible generalization performance at the lowest cost.\nActive learning is an iterative process, and typically works as follows: a modelM is trained using a minimal training dataset L. A query framework is used to identify an instance from an unlabeled set U that, if added to L, will result in maximum expected benefit. Gold standard annotations are obtained for this instance and added to the original training set L to generate a new training set L\u2032. In the next iteration, a new modelM \u2032 is trained using L\u2032 and used to identify the next most beneficial instance for the training set L\u2032. This is repeated until a stopping criterion is met. This approach is often simulated using a training dataset L of reasonable size. The initial modelM is created using a subset A of L. Further, instead of querying a large unlabeled set U , the remaining training data (L \u2212 A) is treated as an unlabeled dataset and queried for the most beneficial addition.\nWe carried out active learning in this setting using a querying framework known as uncertainty sampling (Lewis and Gale, 1994). Here, the model M trained usingA, queries the instances in (L\u2212A) for instance(s) it is least certain for a prediction label. For probabilistic classifiers the most uncertain instance is the one where posterior probability for a given class is nearest to 0.5. To estimate the effectiveness of this framework, it is always compared with a random sampling framework, where random instances from the training data are incrementally added to the model.\nStarting with a model trained using a single randomly chosen instance, we carried out active learning using uncertainty sampling, adding one instance at a time. After the addition of each instance, the model was retrained and tested on a held out set. To minimize the effect of randomiza-\ntion associated with the first instance, we repeated the experiment ten times and averaged the performance scores across the ten runs.\nFollowing previous work (Settles and Craven, 2008; Reichart et al., 2008) we evaluate active learning using learning curves on the test set. Figure 3 shows the learning curves for newswire and clinical data.\nOn clinical data, uncertainty sampling achieves a performance equal to the baseline ENT with only 470 instances. With random sampling, over 2,200 instances are required. The active learner matches the performance of the ENT with only 6.6% of training data. Newswire shows a similar trend, with both sampling strategies outperforming ENT, using less than half the training, and uncertainty sampling learning faster than random. While uncertainty sampling matches ENT F-score with only 1,169 instances, random sampling requires 2,305. Here, the active learner matches ENT performance using only 5.8% of the training data."}, {"heading": "7 Effect of Class Distribution", "text": "After analyzing our experimental results, we considered that one possible explanation for the improvements over baseline ENT could plausibly be because of changes in the class distribution. From Table 1, we observe that the distribution of classes in both domains is highly skewed (only 4-5% positive instances). Self-training and active learning dramatically change the class distribution in training. To assess the effect of class distribution changes on performance, we ran additional experiments, described here.\nWe first investigated sub-sampling (Japkowicz, 2000) the training data to address class imbalance. This includes down-sampling the majority class or up-sampling the minority class until the classes are\nbalanced. We found no significant gains over the vanilla ENT baseline with both strategies. Specifically, down-sampling resulted in gains of only 0.002 and 0.001 F-score and up-sampling resulted in a drop of 0.011 and 0.013 F-score on clinicaldev and newswire-dev, respectively.\nAnother approach to addressing class imbalance is to apply Synthetic Minority Oversampling Technique (SMOTE) (Chawla et al., 2002). SMOTE creates instances of the minority class by taking a minority class sample and introducing synthetic examples between its k nearest neighbors. Using SMOTE on newswire and clinical datasets resulted in improvements over baseline ENT in both domains. The improvements using self-training, however, are significantly higher than SMOTE. Figure 4 shows a comparison of SMOTE and self-training on newswire data, where equal number of instances are added to the training set by both techniques.\nFinally, for active learning, we consider random sampling as a competing approach to uncertainty sampling. Figure 5 illustrates the percentage of positive and negative instances that get included in the training set for both sampling strategies, as active learning proceeds. The blue solid line shows that positive instances are consumed faster than the negative instances with uncertainty sampling. Thus, a higher percentage of positive instances (that approximately equals the number of negative instances getting added) get added and this helps maintain a balanced class distribution.\nOnce the positive instances are exhausted, more negative instances are added, resulting in some class imbalance that hurts performance (even though more training data is being added overall). In contrast, random sampling does not change the class balance, as it consumes a proportional number of positive and negative instances (result-\ning in more negative than positive instances). The plot indicates that when using uncertainty sampling 80% of the positive examples are added to the training set with less than 50% of the data. This also explains how the active learner matches the performance of the model using the entire labeled set, but with fewer training examples."}, {"heading": "8 Conclusion", "text": "We explored the problem of textual entailment search in two domains \u2013 newswire and clinical \u2013 and focused a spotlight on the cost of obtaining labeled data in certain domains. In the process, we first created an entailment dataset for the clinical domain, and a highly competitive supervised entailment system, called ENT, which is effective (out of the box) on two domains. We then explored two strategies \u2013 self-training and active learning \u2013 to address the lack of labeled data, and observed some interesting results. Our self-training system substantially improved over ENT, achieving an F-score gain of 15% on newswire and 13% on clinical, using only additional unlabeled data. On the other hand, our active learning experiments demonstrated that we could match (and even beat) the baseline ENT system with only 6.6% of the training data in the clinical domain, and only 5.8% of the training data in the newswire domain."}, {"heading": "Acknowledgments", "text": "We thank our in-house medical expert, Jennifer Liang, for guidance on the data annotation task, our medical annotators for annotating clinical data for us, and Murthy Devarakonda for valuable insights during the project. We also thank Eric Fosler-Lussier and Albert M. Lai for their help in conceptualizing this work."}], "references": [{"title": "The Fifth PASCAL Recognizing Textual Entailment Challenge", "author": ["Luisa Bentivogli", "Ido Dagan", "Hoa Trang Dang", "Danilo Giampiccolo", "Bernardo Magnini."], "venue": "Proceedings of the Second Text Analysis Conference, Gaithersburg, MD.", "citeRegEx": "Bentivogli et al\\.,? 2009", "shortCiteRegEx": "Bentivogli et al\\.", "year": 2009}, {"title": "The Sixth PASCAL Recognizing Textual Entailment Challenge", "author": ["Luisa Bentivogli", "Peter Clark", "Ido Dagan", "Danilo Giampiccolo."], "venue": "Proceedings of the Third Text Analysis Conference, Gaithersburg, MD.", "citeRegEx": "Bentivogli et al\\.,? 2010", "shortCiteRegEx": "Bentivogli et al\\.", "year": 2010}, {"title": "The Seventh PASCAL Recognizing Textual Entailment Challenge", "author": ["Luisa Bentivogli", "Peter Clark", "Ido Dagan", "Danilo Giampiccolo."], "venue": "Proceedings of the Fourth Text Analysis Conference, Gaithersburg, MD.", "citeRegEx": "Bentivogli et al\\.,? 2011", "shortCiteRegEx": "Bentivogli et al\\.", "year": 2011}, {"title": "The Unified Medical Language System (UMLS): Integrating Biomedical Terminology", "author": ["Olivier Bodenreider."], "venue": "Nucleic Acids Research, 32(Database Issue):D267\u2013D270.", "citeRegEx": "Bodenreider.,? 2004", "shortCiteRegEx": "Bodenreider.", "year": 2004}, {"title": "A large annotated corpus for learning natural language inference", "author": ["Samuel Bowman", "Gabor Angeli", "Christopher Potts", "Christopher Manning."], "venue": "Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 632\u2013", "citeRegEx": "Bowman et al\\.,? 2015", "shortCiteRegEx": "Bowman et al\\.", "year": 2015}, {"title": "Generating an Entailment Corpus from News Headlines", "author": ["John Burger", "Lisa Ferro."], "venue": "Proceedings of the ACL Workshop on Empirical Modeling of Semantic Equivalence and Entailment, pages 49\u201354, Ann Arbor, MI.", "citeRegEx": "Burger and Ferro.,? 2005", "shortCiteRegEx": "Burger and Ferro.", "year": 2005}, {"title": "A Graph-based Semi-Supervised Learning for Question-Answering", "author": ["Asli Celikyilmaz", "Marcus Thint", "Zhiheng Huang."], "venue": "Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on", "citeRegEx": "Celikyilmaz et al\\.,? 2009", "shortCiteRegEx": "Celikyilmaz et al\\.", "year": 2009}, {"title": "SMOTE: Synthetic Minority Over-sampling Technique", "author": ["Nitesh V. Chawla", "Kevin W. Bowyer", "Lawrence O. Hall", "W. Philip Kegelmeyer."], "venue": "Journal of Artificial Intelligence Research, 16:321\u2013357.", "citeRegEx": "Chawla et al\\.,? 2002", "shortCiteRegEx": "Chawla et al\\.", "year": 2002}, {"title": "Recognizing Textual Entailment: Models and Applications", "author": ["Ido Dagan", "Dan Roth", "Mark Sammons", "Fabio Massimo Zanzotto."], "venue": "Synthesis Lectures on Human Language Technologies, 6(4):1\u2013220.", "citeRegEx": "Dagan et al\\.,? 2013", "shortCiteRegEx": "Dagan et al\\.", "year": 2013}, {"title": "The WEKA Data Mining Software", "author": ["Mark Hall", "Eibe Frank", "Geoffrey Holmes", "Bernhard Pfahringer", "Peter Reutemann", "Ian H. Witten."], "venue": "ACM SIGKDD Explorations Newsletter, 11(1):10.", "citeRegEx": "Hall et al\\.,? 2009", "shortCiteRegEx": "Hall et al\\.", "year": 2009}, {"title": "A Discourse Commitment-based Framework for Recognizing Textual Entailment", "author": ["Andrew Hickl", "Jeremy Bensley."], "venue": "Proceedings of the Workshop on Textual Entailment and Paraphrasing, pages 171\u2013176.", "citeRegEx": "Hickl and Bensley.,? 2007", "shortCiteRegEx": "Hickl and Bensley.", "year": 2007}, {"title": "Recognizing Textual Entailment with LCCs Groundhog System", "author": ["Andrew Hickl", "Jeremy Bensley", "John Williams", "Kirk Roberts", "Bryan Rink", "Ying Shi."], "venue": "Proceedings of the Second PASCAL Challenges Workshop.", "citeRegEx": "Hickl et al\\.,? 2006", "shortCiteRegEx": "Hickl et al\\.", "year": 2006}, {"title": "Bootstrapped Training of Event Extraction Classifiers", "author": ["Ruihong Huang", "Ellen Riloff."], "venue": "Proceedings of the 13th Conference of the European Chapter of the Association for Computational Linguistics, pages 286\u2013295, Avignon, France.", "citeRegEx": "Huang and Riloff.,? 2012", "shortCiteRegEx": "Huang and Riloff.", "year": 2012}, {"title": "Learning from Imbalanced Data Sets: A Comparison of Various Strategies", "author": ["Nathalie Japkowicz."], "venue": "AAAI Workshop on Learning from Imbalanced Data Sets, pages 10\u201315.", "citeRegEx": "Japkowicz.,? 2000", "shortCiteRegEx": "Japkowicz.", "year": 2000}, {"title": "PKUTM Participation at TAC 2010 RTE and Summarization Track", "author": ["Houping Jia", "Xiaojiang Huang", "Tengfei Ma", "Xiaojun Wan", "Jianguo Xiao."], "venue": "Proceedings of the Third Text Analysis Conference, Gaithersburg, MD.", "citeRegEx": "Jia et al\\.,? 2010", "shortCiteRegEx": "Jia et al\\.", "year": 2010}, {"title": "An Open-Source Package for Recognizing Textual Entailment", "author": ["Milen Kouylekov", "Matteo Negri."], "venue": "Proceedings of the ACL 2010 System Demonstrations, Uppsala, Sweden.", "citeRegEx": "Kouylekov and Negri.,? 2010", "shortCiteRegEx": "Kouylekov and Negri.", "year": 2010}, {"title": "A Sequential Algorithm for Training Text Classifiers", "author": ["David D. Lewis", "William A. Gale."], "venue": "Proceedings of the 17th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, pages 3\u201312, Dublin, Ireland.", "citeRegEx": "Lewis and Gale.,? 1994", "shortCiteRegEx": "Lewis and Gale.", "year": 1994}, {"title": "The Excitement Open Platform for Textual Inferences", "author": ["Bernardo Magnini", "Roberto Zanoli", "Ido Dagan", "Kathrin Eichler", "Neumann Guenter", "Tae-Gil Noh", "Sebastian Pad\u00f3", "Asher Stern", "Omer Levy."], "venue": "Proceedings of 52nd Annual Meeting of the Associa-", "citeRegEx": "Magnini et al\\.,? 2014", "shortCiteRegEx": "Magnini et al\\.", "year": 2014}, {"title": "Effective Self-Training for Parsing", "author": ["David McClosky", "Eugene Charniak", "Mark Johnson."], "venue": "Proceedings of Human Language Technology Conference of the North American Chapter of the Association of Computational Linguistics, pages 152\u2013", "citeRegEx": "McClosky et al\\.,? 2006", "shortCiteRegEx": "McClosky et al\\.", "year": 2006}, {"title": "Slot Grammar: A System for Simpler Construction of Practical Natural Language Grammars", "author": ["Michael McCord."], "venue": "Proceedings of the International Symposium on Natural Language and Logic, pages 118\u2013145.", "citeRegEx": "McCord.,? 1989", "shortCiteRegEx": "McCord.", "year": 1989}, {"title": "UMLS-Interface and UMLSSimilarity : Open Source Software for Measuring Paths and Semantic Similarity", "author": ["Bridget T McInnes", "Ted Pedersen", "Serguei V.S. Pakhomov."], "venue": "Proceedings of the Annual Symposium of the American Medical In-", "citeRegEx": "McInnes et al\\.,? 2009", "shortCiteRegEx": "McInnes et al\\.", "year": 2009}, {"title": "Co-Training and Self-Training for Word Sense Disambiguation", "author": ["Rada Mihalcea."], "venue": "Proceedings of the Eighth Conference on Natural Language Learning, pages 33\u201340, Boston, MA.", "citeRegEx": "Mihalcea.,? 2004", "shortCiteRegEx": "Mihalcea.", "year": 2004}, {"title": "Addressing Discourse and Document Structure in the RTE Search Task", "author": ["Shachar Mirkin", "Roy Bar-Haim", "Jonathan Berant", "Ido Dagan", "Eyal Shnarch", "Asher Stern", "Idan Szpektor."], "venue": "Proceedings of the Second Text Analysis Conference, Gaithersburg,", "citeRegEx": "Mirkin et al\\.,? 2009", "shortCiteRegEx": "Mirkin et al\\.", "year": 2009}, {"title": "A Study on Convolution Kernels for Shallow Statistic Parsing", "author": ["Alessandro Moschitti."], "venue": "Proceedings of the 42nd Annual Meeting of the Association for Computational Linguistics, pages 335\u2013342, Barcelona, Spain.", "citeRegEx": "Moschitti.,? 2004", "shortCiteRegEx": "Moschitti.", "year": 2004}, {"title": "Effective Information Extraction with Semantic Affinity Patterns and Relevant Regions", "author": ["Siddharth Patwardhan", "Ellen Riloff."], "venue": "Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computa-", "citeRegEx": "Patwardhan and Riloff.,? 2007", "shortCiteRegEx": "Patwardhan and Riloff.", "year": 2007}, {"title": "Measures of Semantic Similarity and Relatedness in the Biomedical Domain", "author": ["Ted Pedersen", "Serguei V.S. Pakhomov", "Siddharth Patwardhan", "Christopher G Chute."], "venue": "Journal of Biomedical Informatics, 40(3):288\u201399.", "citeRegEx": "Pedersen et al\\.,? 2007", "shortCiteRegEx": "Pedersen et al\\.", "year": 2007}, {"title": "Multi-Task Active Learning for Linguistic Annotations", "author": ["Roi Reichart", "Katrin Tomanek", "Udo Hahn", "Ari Rappoport."], "venue": "Proceedings of ACL-08: HLT, pages 861\u2013869, Columbus, OH.", "citeRegEx": "Reichart et al\\.,? 2008", "shortCiteRegEx": "Reichart et al\\.", "year": 2008}, {"title": "An Analysis of Active Learning Strategies for Sequence Labeling Tasks", "author": ["Burr Settles", "Mark Craven."], "venue": "Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing, pages 1070\u20131079, Honolulu, HI.", "citeRegEx": "Settles and Craven.,? 2008", "shortCiteRegEx": "Settles and Craven.", "year": 2008}, {"title": "Active Learning", "author": ["Burr Settles."], "venue": "Synthesis Lectures on Artificial Intelligence and Machine Learning, 6(1):1\u2013114.", "citeRegEx": "Settles.,? 2012", "shortCiteRegEx": "Settles.", "year": 2012}, {"title": "Textual Inference for Eligibility Criteria Resolution in Clinical trials", "author": ["Chaitanya Shivade", "Courtney Hebert", "Marcelo Loptegui", "Marie-Catherine de Marneffe", "Eric Fosler-Lussier", "Albert M. Lai."], "venue": "Journal of Biomedical Informatics,", "citeRegEx": "Shivade et al\\.,? 2015", "shortCiteRegEx": "Shivade et al\\.", "year": 2015}, {"title": "IKOMA at TAC2011 : A Method for Recognizing Textual Entailment using Lexical-level and Sentence Structurelevel Features", "author": ["Masaaki Tsuchida", "Kai Ishikawa."], "venue": "Proceedings of the Fourth Text Analysis Conference, Gaithersburg, MD.", "citeRegEx": "Tsuchida and Ishikawa.,? 2011", "shortCiteRegEx": "Tsuchida and Ishikawa.", "year": 2011}, {"title": "Recognizing Textual Relatedness with Predicate-Argument Structures", "author": ["Rui Wang", "Yi Zhang."], "venue": "Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 784\u2013792, Singapore.", "citeRegEx": "Wang and Zhang.,? 2009", "shortCiteRegEx": "Wang and Zhang.", "year": 2009}, {"title": "Relation Extraction with Relation Topics", "author": ["Chang Wang", "James Fan", "Aditya Kalyanpur", "David Gondek."], "venue": "Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 1426\u20131436, Edinburgh, UK.", "citeRegEx": "Wang et al\\.,? 2011", "shortCiteRegEx": "Wang et al\\.", "year": 2011}, {"title": "Relation Extraction and Scoring in DeepQA", "author": ["Chang Wang", "Aditya Kalyanpur", "James Fan", "Branimir K. Boguraev", "David Gondek."], "venue": "IBM Journal of Research and Development, 56(3.4):9:1\u20139:12.", "citeRegEx": "Wang et al\\.,? 2012", "shortCiteRegEx": "Wang et al\\.", "year": 2012}, {"title": "Automatic Learning of Textual Entailments with Cross-Pair Similarities", "author": ["Fabio M. Zanzotto", "Alessandro Moschitti."], "venue": "Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the Association", "citeRegEx": "Zanzotto and Moschitti.,? 2006", "shortCiteRegEx": "Zanzotto and Moschitti.", "year": 2006}, {"title": "Expanding Textual Entailment Corpora from Wikipedia using Co-training", "author": ["Fabio M. Zanzotto", "Marco Pennacchiotti."], "venue": "Proceedings of the 2nd Workshop on The People\u2019s Web Meets NLP: Collaboratively Constructed Semantic Resources,", "citeRegEx": "Zanzotto and Pennacchiotti.,? 2010", "shortCiteRegEx": "Zanzotto and Pennacchiotti.", "year": 2010}], "referenceMentions": [{"referenceID": 0, "context": "The RTE challenges (Bentivogli et al., 2009; Bentivogli et al., 2011) have spurred considerable research in textual entailment over newswire data.", "startOffset": 19, "endOffset": 69}, {"referenceID": 2, "context": "The RTE challenges (Bentivogli et al., 2009; Bentivogli et al., 2011) have spurred considerable research in textual entailment over newswire data.", "startOffset": 19, "endOffset": 69}, {"referenceID": 4, "context": "This, along with the availability of large-scale datasets labeled with entailment information (Bowman et al., 2015), has resulted in a variety of approaches for textual entailment recognition.", "startOffset": 94, "endOffset": 115}, {"referenceID": 8, "context": "Recognizing Textual Entailment (RTE) shared tasks (Dagan et al., 2013) conducted annually", "startOffset": 50, "endOffset": 70}, {"referenceID": 0, "context": "RTE-5 (Bentivogli et al., 2009) then introduced the task of entailment", "startOffset": 6, "endOffset": 31}, {"referenceID": 1, "context": "Subsequently, RTE-6 (Bentivogli et al., 2010) and RTE-7 (Bentivogli et al.", "startOffset": 20, "endOffset": 45}, {"referenceID": 2, "context": ", 2010) and RTE-7 (Bentivogli et al., 2011) featured entailment search as the primary task, but constrained the search space to only those candidate sentences that were first retrieved by Lucene, an open source search engine1.", "startOffset": 18, "endOffset": 43}, {"referenceID": 22, "context": "Successful approaches for entailment (Mirkin et al., 2009; Jia et al., 2010; Tsuchida and Ishikawa, 2011) have relied on annotated data to either train classifiers, or to develop rules for detecting entailing sentences.", "startOffset": 37, "endOffset": 105}, {"referenceID": 14, "context": "Successful approaches for entailment (Mirkin et al., 2009; Jia et al., 2010; Tsuchida and Ishikawa, 2011) have relied on annotated data to either train classifiers, or to develop rules for detecting entailing sentences.", "startOffset": 37, "endOffset": 105}, {"referenceID": 30, "context": "Successful approaches for entailment (Mirkin et al., 2009; Jia et al., 2010; Tsuchida and Ishikawa, 2011) have relied on annotated data to either train classifiers, or to develop rules for detecting entailing sentences.", "startOffset": 37, "endOffset": 105}, {"referenceID": 5, "context": "Operating under the assumption that more labeled data would improve system performance, some researchers have sought to augment their training data with automatically or semi-automatically obtained labeled pairs (Burger and Ferro, 2005; Hickl et al., 2006; Hickl and Bensley, 2007; Zanzotto and Pennacchiotti, 2010; Celikyilmaz et al., 2009).", "startOffset": 212, "endOffset": 341}, {"referenceID": 11, "context": "Operating under the assumption that more labeled data would improve system performance, some researchers have sought to augment their training data with automatically or semi-automatically obtained labeled pairs (Burger and Ferro, 2005; Hickl et al., 2006; Hickl and Bensley, 2007; Zanzotto and Pennacchiotti, 2010; Celikyilmaz et al., 2009).", "startOffset": 212, "endOffset": 341}, {"referenceID": 10, "context": "Operating under the assumption that more labeled data would improve system performance, some researchers have sought to augment their training data with automatically or semi-automatically obtained labeled pairs (Burger and Ferro, 2005; Hickl et al., 2006; Hickl and Bensley, 2007; Zanzotto and Pennacchiotti, 2010; Celikyilmaz et al., 2009).", "startOffset": 212, "endOffset": 341}, {"referenceID": 35, "context": "Operating under the assumption that more labeled data would improve system performance, some researchers have sought to augment their training data with automatically or semi-automatically obtained labeled pairs (Burger and Ferro, 2005; Hickl et al., 2006; Hickl and Bensley, 2007; Zanzotto and Pennacchiotti, 2010; Celikyilmaz et al., 2009).", "startOffset": 212, "endOffset": 341}, {"referenceID": 6, "context": "Operating under the assumption that more labeled data would improve system performance, some researchers have sought to augment their training data with automatically or semi-automatically obtained labeled pairs (Burger and Ferro, 2005; Hickl et al., 2006; Hickl and Bensley, 2007; Zanzotto and Pennacchiotti, 2010; Celikyilmaz et al., 2009).", "startOffset": 212, "endOffset": 341}, {"referenceID": 10, "context": "Hickl et al. (2006) improves upon this work by including negative examples selected using heuristic rules (e.", "startOffset": 0, "endOffset": 20}, {"referenceID": 10, "context": "However, Hickl and Bensley (2007) achieves only a 1% accuracy improvement on RTE-3 using the same method, suggesting that it is not always as beneficial.", "startOffset": 9, "endOffset": 34}, {"referenceID": 4, "context": "Recent work by Bowman et al. (2015) describes a method for generating large scale annotated datasets, viz.", "startOffset": 15, "endOffset": 36}, {"referenceID": 34, "context": "Using a previously published system for RTE (Zanzotto and Moschitti, 2006), they show that their expanded corpus does not result in improvement for RTE-1, RTE-2 or RTE-3.", "startOffset": 44, "endOffset": 74}, {"referenceID": 6, "context": "Similarly, Celikyilmaz et al. (2009) address the", "startOffset": 11, "endOffset": 37}, {"referenceID": 0, "context": "For the newswire domain, we use entailment search data from the PASCAL RTE-5, RTE-6 and RTE-7 challenges (Bentivogli et al., 2009; Bentivogli et al., 2010; Bentivogli et al., 2011).", "startOffset": 105, "endOffset": 180}, {"referenceID": 1, "context": "For the newswire domain, we use entailment search data from the PASCAL RTE-5, RTE-6 and RTE-7 challenges (Bentivogli et al., 2009; Bentivogli et al., 2010; Bentivogli et al., 2011).", "startOffset": 105, "endOffset": 180}, {"referenceID": 2, "context": "For the newswire domain, we use entailment search data from the PASCAL RTE-5, RTE-6 and RTE-7 challenges (Bentivogli et al., 2009; Bentivogli et al., 2010; Bentivogli et al., 2011).", "startOffset": 105, "endOffset": 180}, {"referenceID": 30, "context": "Top systems (Tsuchida and Ishikawa, 2011; Mirkin et al., 2009) in the RTE challenges have used various types of passage matching ap-", "startOffset": 12, "endOffset": 62}, {"referenceID": 22, "context": "Top systems (Tsuchida and Ishikawa, 2011; Mirkin et al., 2009) in the RTE challenges have used various types of passage matching ap-", "startOffset": 12, "endOffset": 62}, {"referenceID": 9, "context": "In our system, we employ a logistic regression with ridge estimator (the Weka implementation (Hall et al., 2009)), powered by a variety of passage matching features described below.", "startOffset": 93, "endOffset": 112}, {"referenceID": 3, "context": "some medical domain term matchers as well \u2013 using UMLS (Bodenreider, 2004) and a rule-based \u201ctranslator\u201d of medical terms to layman terms3.", "startOffset": 55, "endOffset": 74}, {"referenceID": 29, "context": "UMLS concept overlap, and a measure of UMLS-based similarity (Shivade et al., 2015; Pedersen et al., 2007) using the UMLS::Similarity tool (McInnes et al.", "startOffset": 61, "endOffset": 106}, {"referenceID": 25, "context": "UMLS concept overlap, and a measure of UMLS-based similarity (Shivade et al., 2015; Pedersen et al., 2007) using the UMLS::Similarity tool (McInnes et al.", "startOffset": 61, "endOffset": 106}, {"referenceID": 20, "context": ", 2007) using the UMLS::Similarity tool (McInnes et al., 2009).", "startOffset": 40, "endOffset": 62}, {"referenceID": 31, "context": "Syntactic: Following the lead of several approaches textual entailment (Wang and Zhang, 2009; Mirkin et al., 2009; Kouylekov and Negri, 2010) we have a features measuring the similarity of parse trees.", "startOffset": 71, "endOffset": 141}, {"referenceID": 22, "context": "Syntactic: Following the lead of several approaches textual entailment (Wang and Zhang, 2009; Mirkin et al., 2009; Kouylekov and Negri, 2010) we have a features measuring the similarity of parse trees.", "startOffset": 71, "endOffset": 141}, {"referenceID": 15, "context": "Syntactic: Following the lead of several approaches textual entailment (Wang and Zhang, 2009; Mirkin et al., 2009; Kouylekov and Negri, 2010) we have a features measuring the similarity of parse trees.", "startOffset": 71, "endOffset": 141}, {"referenceID": 19, "context": "Our rule-based syntactic parser (McCord, 1989) produces dependency parses the text-hypothesis pair, whose nodes are aligned using all of the term matchers.", "startOffset": 32, "endOffset": 46}, {"referenceID": 23, "context": "The tree match feature is an aggregation of the aligned subgraphs in the tree (somewhat similar to a tree kernel (Moschitti, 2004)).", "startOffset": 113, "endOffset": 130}, {"referenceID": 32, "context": "htm Semantic: We apply open domain as well as medical entity and relation detectors (Wang et al., 2011; Wang et al., 2012) to the texts, and post features measuring overlap in detected entities and overlap in the detected relations across the text-hypothesis pair.", "startOffset": 84, "endOffset": 122}, {"referenceID": 33, "context": "htm Semantic: We apply open domain as well as medical entity and relation detectors (Wang et al., 2011; Wang et al., 2012) to the texts, and post features measuring overlap in detected entities and overlap in the detected relations across the text-hypothesis pair.", "startOffset": 84, "endOffset": 122}, {"referenceID": 17, "context": "Both these system are available under the Excitement Open Platform (EOP), an initiative (Magnini et al., 2014) to make tools for textual entailment freely available4 to the NLP community.", "startOffset": 88, "endOffset": 110}, {"referenceID": 31, "context": "The Textual Inference Engine (TIE) (Wang and Zhang, 2009) is a maximum entropy based entailment system relying on predicate argument structure matching.", "startOffset": 35, "endOffset": 57}, {"referenceID": 15, "context": "EDITS (Edit Distance Textual Entailment Suite) by Kouylekov and Negri (2010) is an open source textual entailment system that uses a set of rules and resources to perform \u201cedit\u201d operations on the text to convert it into the hypothesis.", "startOffset": 50, "endOffset": 77}, {"referenceID": 22, "context": "30 and the best system (Mirkin et al., 2009) achieved an F-Score of 0.", "startOffset": 23, "endOffset": 44}, {"referenceID": 18, "context": "Self-training is a method that has been successfully used to address limited training data on many NLP tasks, such as parsing (McClosky et al., 2006), information extraction (Huang and Riloff, 2012; Patwardhan and Riloff, 2007), word sense disambiguation (Mi-", "startOffset": 126, "endOffset": 149}, {"referenceID": 12, "context": ", 2006), information extraction (Huang and Riloff, 2012; Patwardhan and Riloff, 2007), word sense disambiguation (Mi-", "startOffset": 32, "endOffset": 85}, {"referenceID": 24, "context": ", 2006), information extraction (Huang and Riloff, 2012; Patwardhan and Riloff, 2007), word sense disambiguation (Mi-", "startOffset": 32, "endOffset": 85}, {"referenceID": 22, "context": "54 \u2013 substantially better than the best performing system of Mirkin et al. (2009) (F-score, 0.", "startOffset": 61, "endOffset": 82}, {"referenceID": 28, "context": "Active learning is a popular training paradigm in machine learning (Settles, 2012) where a learning agent interacts with its environment in acquiring a training set, rather than passively receiving independent samples from an underlying distribution.", "startOffset": 67, "endOffset": 82}, {"referenceID": 16, "context": "We carried out active learning in this setting using a querying framework known as uncertainty sampling (Lewis and Gale, 1994).", "startOffset": 104, "endOffset": 126}, {"referenceID": 27, "context": "Following previous work (Settles and Craven, 2008; Reichart et al., 2008) we evaluate active learning using learning curves on the test set.", "startOffset": 24, "endOffset": 73}, {"referenceID": 26, "context": "Following previous work (Settles and Craven, 2008; Reichart et al., 2008) we evaluate active learning using learning curves on the test set.", "startOffset": 24, "endOffset": 73}, {"referenceID": 13, "context": "We first investigated sub-sampling (Japkowicz, 2000) the training data to address class imbalance.", "startOffset": 35, "endOffset": 52}, {"referenceID": 7, "context": "ance is to apply Synthetic Minority Oversampling Technique (SMOTE) (Chawla et al., 2002).", "startOffset": 67, "endOffset": 88}], "year": 2016, "abstractText": "We seek to address the lack of labeled data (and high cost of annotation) for textual entailment in some domains. To that end, we first create (for experimental purposes) an entailment dataset for the clinical domain, and a highly competitive supervised entailment system, ENT, that is effective (out of the box) on two domains. We then explore self-training and active learning strategies to address the lack of labeled data. With self-training, we successfully exploit unlabeled data to improve over ENT by 15% F-score on the newswire domain, and 13% F-score on clinical data. On the other hand, our active learning experiments demonstrate that we can match (and even beat) ENT using only 6.6% of the training data in the clinical domain, and only 5.8% of the training data in the newswire domain.", "creator": "LaTeX with hyperref package"}}}