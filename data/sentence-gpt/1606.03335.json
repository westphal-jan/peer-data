{"id": "1606.03335", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "10-Jun-2016", "title": "WordNet2Vec: Corpora Agnostic Word Vectorization Method", "abstract": "A complex nature of big data resources demands new methods for structuring especially for textual content. WordNet is a good knowledge source for comprehensive abstraction of natural language as its good implementations exist for many languages. Since WordNet embeds natural language in the form of a complex network, a transformation mechanism WordNet2Vec is proposed in the paper. It creates vectors for each word from WordNet. These vectors encapsulate general position - role of a given word towards all other words in the natural language. Any list or set of such vectors contains knowledge about the context of its component within the whole language. Such word representation can be easily applied to many analytic tasks like classification or clustering. The usefulness of the WordNet2Vec method was demonstrated in sentiment analysis, i.e. classification with transfer learning for the real Amazon opinion textual dataset. The concept and model were further explored in the paper.", "histories": [["v1", "Fri, 10 Jun 2016 14:12:47 GMT  (3411kb,D)", "http://arxiv.org/abs/1606.03335v1", "29 pages, 16 figures, submitted to journal"]], "COMMENTS": "29 pages, 16 figures, submitted to journal", "reviews": [], "SUBJECTS": "cs.CL cs.AI cs.DC", "authors": ["roman bartusiak", "{\\l}ukasz augustyniak", "tomasz kajdanowicz", "przemys{\\l}aw kazienko", "maciej piasecki"], "accepted": false, "id": "1606.03335"}, "pdf": {"name": "1606.03335.pdf", "metadata": {"source": "CRF", "title": "WordNet2Vec: Corpora Agnostic Word Vectorization Method", "authors": ["Roman Bartusiaka", "Lukasz Augustyniak", "Tomasz Kajdanowicz", "Przemys law Kazienko", "Maciej Piasecki"], "emails": ["roman.bartusiak@pwr.edu.pl", "lukasz.augustyniak@pwr.edu.pl", "tomasz.kajdanowicz@pwr.edu.pl", "kazienko@pwr.edu.pl", "maciej.piasecki@pwr.edu.pl"], "sections": [{"heading": null, "text": "A complex nature of big data resources demands new methods for structuring especially for textual content. WordNet is a good knowledge source for comprehensive abstraction of natural language as its good implementations exist for many languages. Since WordNet embeds natural language in the form of a complex network, a transformation mechanism WordNet2Vec is proposed in the paper. It creates vectors for each word from WordNet. These vectors encapsulate general position - role of a given word towards all other words in the natural language. Any list or set of such vectors contains knowledge about the context of its component within the whole language. Such word representation can be easily applied to many analytic tasks like classification or clustering. The usefulness of the WordNet2Vec method was demonstrated in sentiment analysis, i.e. classification with transfer learning for the real Amazon opinion textual dataset. Keywords: natural language structuring, WordNet, WordNet2Vec, vectorization, network transformation, sentiment analysis, transfer learning, big data, complex networks\n\u2217Corresponding author Email addresses: roman.bartusiak@pwr.edu.pl (Roman Bartusiak),\nlukasz.augustyniak@pwr.edu.pl ( Lukasz Augustyniak), tomasz.kajdanowicz@pwr.edu.pl (Tomasz Kajdanowicz), kazienko@pwr.edu.pl (Przemys law Kazienko), maciej.piasecki@pwr.edu.pl (Maciej Piasecki)\nar X\niv :1\n60 6.\n03 33\n5v 1\n[ cs\n.C L\n] 1\n0 Ju"}, {"heading": "1. Introduction", "text": "With a fast technological growth, the ability to solve complex problems increased. More and more data continuously generated by social media and various IT systems requires more complex, accurate and efficient methods and algorithms in order to provide valuable insight. The tools of Big Data Analytics and accessible to everyone High Performance Computing facilitate to face these challenges but simultaneously a wide variety of new questions and problems raised and need to be addressed solely by scientists.\nThere are many aspects that must be taken into account while processing large amounts of data. One of them is a general problem of knowledge representation for complex structures. Textual, multimedia or networked content has an unstructured nature that is improper for application of most known analytic methods. Recently, most of the data sets come from texts, e.g. from social media, and they are directly impacted by the complex profile of natural languages. Transforming \u2019big data\u2019 of such kind into \u2019big knowledge\u2019 still remains a great challenge. An obvious and commonly performed step in such transformation is \u2019structuring\u2019. However, how to transform a complex nature of natural language to a structured form?\nOverall, there are two main sources of knowledge about the natural language: (1) text corpora and (2) WordNets, which are commonly verified by linguists. The first source strongly depends on the provenance and frequency of terms or n-gramms. The latter, in turn, reflects general snapshot of a given natural language. The largest WordNet is for Polish [1] and English [2]. Simplifying, WordNet is a network that embeds relationships between distinct conceptssynset (synonym sets) existing the language, i.e. it captures the nature of the language. In opposite to text corpora, it also includes rarely used words and their unusual meanings, which in corpora may either not exist or be damped by more popular words and meanings. Additionally, WordNet contains conceptualsemantic and lexical relationships linking synsets and verified by linguists. On the other hand, WordNet itself possesses a complex network structure, which\nis inappropriate for commonly used analytic and reasoning methods.\nThe main problem addressed in this paper, is to find a method for transformation of the complex network structure of the whole WordNet covering natural language into a simple structured form - vectors suitable for further processing by means of known methods. In particular, the transformation should encapsulate a position of each term in the WordNet network towards all other words in WordNet to preserve general WordNet knowledge about a single word in the whole natural language context. We propose a new method, called WordNet2Vec, for textual data representation in the vector space that is able to handle the above mentioned requirements. Based on the network of words from WordNet, we build a word representation in the vector space using its distance to any other word in the network. In order to present the pair-wise word distance, the method calculates all-pairs shortest paths in WordNet. Thanks to that any list of vectors - list of words also reflects the complex nature of the whole language encoded within these vectors.\nTo demonstrate the usefulness of the proposed vectorization method, we suggested a use case in which any textual document is transformed into a list of vectors from WordNet2Vec. Next, such representation is applied to classification problem, namely sentiment analysis and assignment. Finally, we compare the effectiveness of the baseline method and some recently emerged approaches with high popularity such as Doc2Vec [3]. Since, we derive knowledge from general language database - WordNet, our method enables to build more robust knowledge representation models and to achieve very high level of efficiency, generalization and stability, especially if applied to transfer learning scenarios.\nIn the experimental part of our research, the proposed WordNet2Vec vectorization method was utilized to sentiment analysis in the Amazon product review dataset. In general, sentiment analysis of texts means assigning a measure on how positive, neutral or negative the text is. Using WordNet2Vec representation and a supervised learning approach, the sentiment is assigned to the document according to the content of document. In other words, sentiment analysis is the process of determining the attitudes, opinions and emotions expressed within\na text.\nThe rest of this paper is organized as follows: in Section 2 related work is presented. Then, the new WordNet2Vec method and other comparative methods are described in Section 3. The experimental design and results are discussed in Section 6. Finally, the presented ideas are concluded and future work directions are sketched in Section 7."}, {"heading": "2. Related Work", "text": "Representation of the knowledge is an area of Artificial Intelligence concerned with how knowledge can be represented symbolically and manipulated in an automated way [4]. Textual documents are unstructured intrinsically and in order to provide their robust processing one need to involve resolving, aggregating, integrating and abstracting - via the various methodologies. From such textual data treatment we hope to obtain accurate estimation and prediction, data-mining, social network analysis, and semantic search and visualization. Derived knowledge can be represented in various structures including: semantic nets, frames, rules, and ontologies [5]. Due to fact that the majority of machine learning and supervised learning approaches are dealing with vector space, text representation should also be placed in a vector space. Recent achievements in the topics related to textual data representation will be briefly presented further part of the this section. In particular, there are recalled word embedding methods that are based on language corpuses. Then WordNet as a source encapsulating the knowledge of natural language that is used by the vectorization method proposed by us is familiarized with the reader. Additionally, there are some insights in complex network all-pair-shortest-paths (APSP) computation. Finally an introduction to the sentiment analysis is provided, because it was used for an exemplary application of the proposed method."}, {"heading": "2.1. Word Embedding Techniques", "text": "Word embedding is the generous name for a collection of language modelling and feature learning techniques where words from the vocabulary are mapped\nto vectors of real numbers. Word embedding can be equally called word vectorization method. The mapping is usually done to a low-dimensional space, but depends relatively on the vocabulary size. Historically, word embedding was related to statistical processing of big corpora and deriving features from words\u2019 concurrence in the documents.\nIn general word embedding techniques can be dived into two main groups. First of them is based on probabilistic prediction approach. Such methods trains a model, based on a context window composed of words from a corpus, generalizing the result into n dimensional space (n is chosen arbitrary). Then a word is represented as a vector in the space and preserves a context property, i.e. other words in the space that are located close to each other are frequently cooccurring in the corpus. Word2Vec (Word-2-Vector) [6, 7] is the most successful method from this group. It is based on skip-grams and continuous bag of words (CBOW). Given the neighboring words in the window CBOW model is used to predict a particular word w. In contrast, given a window size of n words around a word w, the skip-gram model predicts the neighboring words given the current word.\nThere were other deep and recurrent neural network architectures proposed for learning word representation in vector space before, i.e. [8, 9], but Word2Vec outperformed them and gained much more attention.\n\u201dCount-based\u201d models constitiute the second group of the word embedding techniques. GloVe algorithm, presented by Pennington et al. [10], is one of them. Count-based models learn their vectors based on the word co-occurrence frequency matrix. In order to shrink the size of word vectors the dimensionality reduction algorithms are applied. The main intuition for the GloVe model is the simple observation that ratios of word-to-word co-occurrence probabilities have the potential for encoding some form of meaning. Word vectors produced by GloVe method perform very well as the solution for word similarity tasks, and is similar to the Word2Vec approach. Lebret and Collobert [11] and Dhillon et al. [12] proposed some other count-based models. Lebret presented a method that simplifies the word embeddings computation through a Hellinger PCA of\nthe word co-occurrence matrix. Dhillon used a new spectral method based on CCA (canonical correlation analysis)\u2014Two Step CCA (TSCCA)\u2014to learn an eigenword dictionary. This procedure computes two set of CCAs: the first one between the left and right contexts of the given word and the second one between the projections resulting from this CCA and the word itself. Lebret and Collobert [13] presented alternative model based on counts. They used the Hellinger distance to extract semantic representations from the word cooccurence statistics of large text corpora.\nIn conclusion, Word2Vec is a \u201dpredictive\u201d model, whereas GloVe is a \u201dcountbased\u201d model [14]. However, there is no qualitative difference between predictive models and count-based models. They are different computational methods that produce a very similar type of a semantic models [15, 16]."}, {"heading": "2.2. WordNet", "text": "WordNet is a large lexical database of natural language. There are separate WordNet\u2019s for many different languages, e.g. for English [2, 17]. Nouns, verbs, adjectives and adverbs are grouped into sets of cognitive synonyms (synsets), each expressing a distinct concept. Synsets are interlinked using conceptualsemantic and lexical relations. Various types of links can be distinguished in WordNet:\n\u2022 There are 285,348 semantic links between synsets:\n\u2013 178,323 both hypernym and hyponym,\n\u2013 21,434 similarity,\n\u2013 18,215 both holonym and meronym,\n\u2013 67,376 others connections.\n\u2022 There are 92,508 lexical links between all words:\n\u2013 74,656 derivation,\n\u2013 7,981 antonym,\n\u2013 9,871 others connections.\nThe resulting network of meaningfully related words and concepts can be utilized in many different fields. WordNet is also freely and publicly available for download. WordNet\u2019s structure makes it a useful tool for computational linguistics and natural language processing."}, {"heading": "2.3. All-pairs Shortest Paths", "text": "All-pairs shortest paths (APSP) is one of the most fundamental problem in graph theory. The objective of that task is to compute distances between all vertices in a graph. Computations can be done for all types of graphs, i.e. directed, undirected, weighted, unweighted, etc. The field is extensively explored and analyzed, because of that, complexity of task is an open problem. multiple algorithms have been proposed to solve problem. Their complexity can vary, depending on type of graph that is taken into consideration. Current most known solution for the problem have been proposed by Robert Floyd [18]. After success in optimization of complexity, multiple new approaches that try to minimize computational and memory complexity emerged. Simple geometrical optimization allowed to decrease complexity to O( n 3\nlog(n) ) [19]. Work of Yijie\nHan [20] exceeds, unbreakable for long time, result of O( n 3\nlog(n) ). His solution has\ncomplexity of O(n3( log(log(n))log(n) ) 5 4 ), and is currently best one in terms of complexity. Downside of the mentioned solution is its theoretical complexity. Method proposed by Robert Floyd [18] thanks to its simplicity can be distributed in easiest way, thanks to what it can be easily used in distributed environment for big datasets ."}, {"heading": "2.4. Sentiment Analysis", "text": "Nowadays, the most commonly used methods to sentiment analysis are the classification approaches with classifiers such as Naive Bayes [21, 22, 23], Support Vector Machines (SVM) [24, 25, 26], Decision Tree [27, 28], Random Forest [29] or Logistic Regression [28]. In addition, feature selection can improve classification accuracy by reducing the high-dimensionality to a low-dimensional\nof feature space. Yousefpour et al. [30] proposed a hybrid method and two meta-heuristic algorithms are employed to find an optimal feature subset.\nAnother family of solution to sentiment classification are neural network based approach. Socher et al. [31] proposed recursive deep model for sentiment using treebank structure of sentences. Zhang and LeCun [32] used deep learning to text understanding from character-level inputs all the way up to abstract text concepts, using temporal convolutional networks (ConvNets). What is more, some systems leverages both hand-crafted features and word level embedding features, like Do2Vec, with the usage of classifiers such as SVM [33]."}, {"heading": "2.5. Transfer Learning", "text": "Transfer learning provides system ability to recognize and apply knowledge extraction (learn in the previous tasks) to the novel tasks (in new domains) [34]. Interestingly, it is based on human behaviour during learning. We can often transfer knowledge learned in one situation and adapt it to the new one. Yoshida et al. [35] proposed a model, where each word is associated with three factors: domain label, domain dependence/independence and word polarity. The main part of their method is Gibbs sampling for inferring the parameters of the model, from both labelled and unlabeled texts. Moreover, the method proposed by them may also determine whether each word\u2019s polarity is domaindependent or domain-independent. Zhou et al. [36] developed a solution to cross-domain sentiment classification for unlabeled data. To bridge the gap between domains, they proposed an algorithm, called topical correspondence transfer (TCT). TCT is achieved by learning the domain-specific information from different areas into unified topics."}, {"heading": "3. WordNet2Vec: WordNet-based Natural Language Representation", "text": "in the Vector Space \u2013 Word Vectors\nThe general idea of the WordNet2Vec method is to transfer knowledge about natural language encapsulated by the WordNet network database into word-\nbased vector structures suitable for further processing. Its principle steps are presented in Figure 1.\nWordNet, being developed by linguists for a given language, consists of many synsets - meanings of words as well as lexical and semantic relations linking them. If it is large enough \u2013 more than a hundred thousand synsets, it may be treated as a reliable and comprehensive representation of the general vocabulary used by people speaking a given language. Since WordNet has a complex network form, it is hardly suitable for commonly used analytic methods like reasoning by means of machine learning.\nTo overcome this limitation, we propose the WordNet2Vec method that provides a set of word vectors embedding the whole WordNet. It starts with simplification of the WordNet structure into a graph with words only and one kind of relations. The relation in the simplified graph exists if (1) there exists a direct lexical relation between words in the original WordNet, (2) there exists any semantic relation between two synsets containing two considered words, (3) both words belong to one synset \u2013 are synonyms.\nIn the next step, a structural measure is applied to evaluate distance from a given node-word to any other node-word in the simplified graph. We decided to utilize shortest paths for that purpose. It means that we had to compute all-pair shortest paths. The distribution of all pairs shortest paths is depicted in Figure 2.\nFinally, for each word \u2013 node in the simplified network \u2013 a separate vector is created. Its coordinates correspond to shortest path lengths to all other nodes so the word vector reflects a position of a given word towards all other words in the language. The set of such word vectors is the output of the method and can be used for further processing. Such word vector set encompasses the knowledge about a given natural language, in particular about relations between words."}, {"heading": "4. WordNet2Vec Implementation \u2013 Distributed Calculation of All-", "text": "pairs Shortest Paths in the Simplified WordNet Graph\nTo demonstrate the WordNet2Vec method, it was applied to English WordNet [2]. We have created a simplified network based on semantic and lexical relations present in WordNet. Hence, we received a graph composed of 147,478 words interconnected by 1,695,623 links.\nThe simplified network was utilized to compute all-pair shortest paths in this\nnetwork, so we obtained 147, 4782, i.e. over 21 billion path lengths.\nBecause of the size of the simplified network as well as computational and memory complexity of the path calculation task, we have decided to use heterogeneous computational cluster as a environment of our experiments. Nevertheless, further optimization had to be done. In our approach, we have used distributed implementation of Dijkistra [37] algorithm available in our SparklingGraph library [38]. Most the known solutions for the all-pair shortest paths (APSP) problem has memory complexity of O(n3), where n is the number of nodes in the graph [18]. Because of that, we had to do the computations in the iterative way. We have used \u2019divide and conquer approach\u2019 in order to split\nAPSP into smaller problems that can be computed efficiently on our cluster. In each iteration, we computed shortest paths for 1000 vertices to all vertices in the simplified graph. Afterwards, the results were gathered into a coherent set of word vectors that represented distances between words in terms of graph topology."}, {"heading": "5. Use Case: WordNet2Vec Application to Sentiment Analysis", "text": "Among a wide variety of possible application areas, the usage of WordNet2Vec method will be presented with sentiment analysis use case. In general, sentiment analysis of the texts consist in assigning a positive, neutral or negative measure the text. With usage of WordNet2Vec representation we use a supervised learning approach to generalize the sentiment classes assigned to the document. Then, according to the content of new document, it is possible with a trained model to infer its sentiment class. In such a scenario one can determine the attitudes, opinions and emotions expressed within a text.\nIn details, the sentiment analysis can be performed with appropriate sequence of processing steps that includes: text segmentation and lematization, vector look-up in the WordNet2Vec matrix for each word in the document, aggregation of vectors for all words within documents, train/test dataset split and finally classifier learning and testing (ex. within the same domain or across domains - transfer learning). In order to accomplish the learning and testing phase properly we should have some sentiment classes assigned to all documents. The overall flow of the sentiment assignment is presented in Figure 3 and all of the steps are discussed bellow."}, {"heading": "5.1. Text segmentation and lematization", "text": "Firstly, some basic natural language processing methods must be used: segmentation and lemmatization. In order to process each document has to be segmented into words. Due to the fact that WordNet2Vec matrix is prepared for words in their lemma form, each word from each document must be lemmatized. It has to be emphasized here that the method has a disadvantage as\nit works only for words that are present in WordNet. However, due to the fact that WordNet is authoritative and reliable representation of language, it may\nbe suitable to almost all possible application domains."}, {"heading": "5.2. Vector look-up in the WordNet2Vec Matrix for a word", "text": "The method for word vectorization proposed in the paper provides precomputed WordNet2Vec Matrix. It contains vector representation for each word from WordNet. In the next step of the flow, vectors for all words for all documents are retrieved from mentioned matrix with O(1) time."}, {"heading": "5.3. Aggregation of vectors for all words within documents", "text": "Due to the fact that sentiment assignment is performed for documents, they have to be represented in single vector. Some aggregation over all words from the document must be applied. In order to compute single vector that will represent document, we are proposing to sum up vectors of each word that appears in the document, see Eq. 1),\nv(d) = ||d||\u2211 i \u2212\u2212\u2212\u2192 v(di) (1)\nwhere \u2212\u2212\u2192 v(\u00b7) represents a vector from WordNet2Vec Matrix, ||d|| denotes a number of lemmas in a document and di is an ith lemma from document d."}, {"heading": "5.4. Dataset split, classifier learning and testing", "text": "Once, all the documents possess a single vector representation, they form a dataset appropriate for classifier training and testing. In order to examine classifiers generalization abilities it is proposed to accomplish two distinct scenarios: learning and testing within the same domain of the text (same type of documents) or transfer learning and testing (learning on one domain and testing on totally another one)."}, {"heading": "6. Experiments and Results", "text": "Following the scenario presented in the Section 5, we performed validity tests by examining sentiment assignment task. In the following sections we describe the data that utilized, the details of experiments and received results."}, {"heading": "6.1. Datasets", "text": "We chose data from one data source - Amazon e-commerce platform [39]. The data spans over a period of 18 years, including 35 million reviews up to March 2013. Reviews include product and user information, ratings, and a plain text review. Some basic statistical information about the dataset are presented in Table 1.\nThe whole experiment was conducted on selected part of the Amazon data that consisted of 7 domains, namely: Automotive, Sports and Outdoors, Books, Health, Video Games, Toys and Games, Movies and TV. Due to the fact that the distribution of classes is important while interpreting the results of classification validation, the proper histogram is presented in Figure 4. The domains of review dataset that were chosen for the experiment are listed in Table 2).\nIn order to check the accuracy of the proposed methods, we extracted the sentiment orientation from ratings expressed with stars. Ratings were mapped to the following classes: \u201dpositive\u201d, \u201dneutral\u201d and \u201dnegative\u201d, using 1 and 2 stars, 3 stars, 4 and 5 stars respectively, see Table 3."}, {"heading": "6.2. Experimental setup", "text": "Our experiments were divided into two distinct groups. First of them consisted of classical machine learning evaluation using train/test split done on each of seven mentioned datasets. In the second group of experiments we used one vs all transfer learning evaluation. Whenever one domain was used as a training set, it was evaluated on the rest of domains. Thanks to that we experienced the quality of both, classical sentiment analysis task based on given domain dataset, and on transfer learning between different domains. In both experiment groups we have used two methods of text vectorization: proposed by us WordNet2Vec and as a reference - Doc2Vec.\nDoc2Vec is the generalization of Word2Vec algorithm to document level. Word2Vec model shows how a word usually is used in a window context according to other words (how words co-occur with each other). The procedure of counting Doc2Vec is very similar to Word2Vec, except it generalizes the model by adding a document vector. There are two methods used in Doc2Vec: Distributed Memory (DM) and Distributed Bag of Words (DBOW). The first one attempts to predict a word given its previous words and a paragraph vector.\nEven though the context window moves across the text, the paragraph vector does not (hence distributed memory) and allows for some word-order to be captured. On the other hand, DBOW predicts a random group of words in a paragraph given only its paragraph vector. In our experiments, we used Distributed Memory method trained on the Amazon SNAP (see Section 6.1) review dataset. The separate model was trained for each domain and length of a vector equal to 400.\nIn order to evaluate standard classification approach we additionally provide a baseline F1weighted value that would be achieved if we would use a classifier that returns always a class that is a major one.\nLogistic regression was selected as a supervised learning model and in order to train it we have used limited memory BFGS algorithm [40]. It was a trade-off for computation vectorized documents in the space size dependent on Wordnet2Vec Matrix size. Due to the fact that datasets used in the experiments are imbalanced, see Figure 4), we express the evaluation result using appropriate measure - weighted F1 score (Equation 2). In order to compare two approaches (WordNet2Vec and Doc2Vec), we have used statistical test on paired measures for each of experiments. We have used Wilcoxon signed-rank test [41] with confidence level \u03b1 = 0.05 (Tables 4 and 5). To provide deeper insight of differences between results achieved by methods, we are presenting also histograms of differences between F1weighted of both methods (see Figures 6, 9).\nF1weighted = \u2211k i ||ci|| \u2217 F1ci\u2211k\ni ||ci|| (2)\nk \u2212 number of classes\nci \u2212 classification results for class i\nF1ci \u2212 F1 score for ci classification results\nF1 = 2 \u2217 precision \u2217 recall precision+ recall\n(3)"}, {"heading": "6.3. Generalization ability with regards to in-domain classification", "text": "For every domain from group of seven all together, that was examined in the experiments, Doc2Vec is slightly better then proposed the approach (Figure 5). The difference between the result achieved by Doc2Vec and WordNet2Vec is not so huge what can be observed in Figure 6. Nevertheless, it is statistically significant, what was shown by Wilcoxon rank-sum test (Table 4). It is important to notice that statistical analysis of results showed also that WordNet2Vec is not worse than baseline."}, {"heading": "6.4. Generalization ability with regards to transfer learning", "text": "The results achieved in transfer learning setting show the true power and abilities of WordNet2Vec method. We can observe that our method achieves better results than Doc2Vec (Figure 7, 8). It is important to notice that differences in results are much bigger in a favor of WordNet2Vec in comparison to results from standard classification (Figure 9). The analysis of variance of transfer learning results for both methods shows that WordNet2Vec is more stable and the results are less various in comparison to Doc2Vec (Figure 10). Additionally, we have used statistical tests in order to check statistical significance of differences in results (Table 5). The superiority of WordNet2Vec over Doc2Vec is statistically significant.\nTable 5: Wilcoxon rank-sum test results for transfer learning.\nMethod 1 Method 2 Ha p-value\nDoc2Vec WordNet2Vec F1WordNet2V ec > F1Doc2V ec 6.821 \u2217 10\u221213\n0 0.2 0.4 0.6 0\n5\n10\n15\nDifference\nC ou\nn t\nFigure 9: Histogram of differences between F1 Weighted measure for WordNet2Vec and Doc2Vec based classification in transfer learning experiments."}, {"heading": "7. Conclusions and Future Work", "text": "A novel method\u2014WordNet2Vec\u2014for word vectorization that enables to build more general knowledge representation of texts using WordNets was presented in the paper. It provides a word representation in the vector space using its distance to any other word in the network. In order to present the pair-wise word distance, the method calculates all-pairs shortest paths in WordNet.\nThe usefulness of the WordNet2Vec method was demonstrated in sentiment analysis problem, i.e. classification with transfer learning setting using Amazon reviews dataset. We compared WordNet2Vec-based classification of sentiment to Doc2Vec approach. Doc2Vec proved to be more accurate in homogeneous setting (learning and testing within the same domain). However, in case of cross domain application (transfer learning), our method outperformed the Doc2Vec results. Hence, we presented its generalization ability in text classification problems.\nIn the future work, we want to investigate the different methods of combining word vectors into documents, treat WordNet as multiplex network while\ncalculating shortest paths, reduce feature space of WordNet2Vec Matrix and validate our method on different sources of data such as Twitter or Facebook.\nAcknowledgments: The work was partially supported by The National Science Centre, decision no. DEC-2013/09/B/ST6/02317 and the European Commission under the 7th Framework Programme, Coordination and Support Action, Grant Agreement Number 316097, Engine project. This work was partially supported by the Faculty of Computer Science and Management, Wroc law University of Science and Technology statutory funds.The calculations were carried out in Wroclaw Centre for Networking and Supercomputing (http://www.wcss.wroc.pl), grant No 177."}], "references": [{"title": "A Wordnet from the Ground Up", "author": ["M. Piasecki", "S. Szpakowicz", "B. Broda"], "venue": "Oficyna Wydawnicza Politechniki Wroclawskiej, Wroc  law", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2009}, {"title": "Wordnet: A lexical database for english", "author": ["G.A. Miller"], "venue": "Commun. ACM 38 (11) ", "citeRegEx": "2", "shortCiteRegEx": null, "year": 1995}, {"title": "Knowledge Representation and Reasoning", "author": ["R. Brachman", "H. Levesque"], "venue": "Morgan Kaufmann Publishers Inc., San Francisco, CA, USA", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2004}, {"title": "Artificial Intelligence: A Modern Approach", "author": ["S.J. Russell", "P. Norvig"], "venue": "2nd Edition, Pearson Education", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2003}, {"title": "J", "author": ["T. Mikolov", "I. Sutskever", "K. Chen", "G.S. Corrado"], "venue": "Dean, Distributed representations of words and phrases and their compositionality ", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2013}, {"title": "Natural Language Processing (Almost) from Scratch", "author": ["R. Collobert", "J. Weston", "L. Bottou", "M. Karlen", "K. Kavukcuoglu", "P. Kuksa"], "venue": "Journal of Machine Learning Research 12 ", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2011}, {"title": "Glove: Global vectors for word representation", "author": ["J. Pennington", "R. Socher", "C.D. Manning"], "venue": "in: Empirical Methods in Natural Language Processing (EMNLP)", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2014}, {"title": "R", "author": ["R. Lebret"], "venue": "Collobert, Word Emdeddings through Hellinger PCA ", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2013}, {"title": "Rehabilitation of count-based models for word vector representations", "author": ["R. Lebret", "R. Collobert"], "venue": "Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics) 9041 ", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2015}, {"title": "Don\u2019t count", "author": ["M. Baroni", "G. Dinu", "G. Kruszewski"], "venue": "predict! a systematic comparison of context-counting vs. context-predicting semantic vectors, in: Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), Association for Computational Linguistics, Baltimore, Maryland", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2014}, {"title": "Factorization of Latent Variables in Distributional Semantic Models", "author": ["A. \u00d6Sterlund", "D. \u00d6dling", "M. Sahlgren"], "venue": "Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, Lisbon, Portugal, 17-21 September 2015 (September) ", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2015}, {"title": "Neural Word Embedding as Implicit Matrix Factorization", "author": ["O. Levy", "Y. Goldberg"], "venue": "Advances in Neural Information Processing Systems (NIPS) ", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2014}, {"title": "Algorithm 97: shortest path", "author": ["R. Floyd"], "venue": "Communications of the ACM 5 (6) ", "citeRegEx": "18", "shortCiteRegEx": null, "year": 1962}, {"title": "All-pairs shortest paths with real weights in o (n 3/log n) time", "author": ["T.M. Chan"], "venue": "in: Algorithms and Data Structures, Springer", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2005}, {"title": "An o (n 3 (loglogn/logn) 5/4) time algorithm for all pairs shortest paths", "author": ["Y. Han"], "venue": "in: Algorithms\u2013ESA 2006, Springer", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2006}, {"title": "Thumbs up?: sentiment classification using machine learning techniques, in: Proceedings of the ACL-02 conference on Empirical methods in natural language processing-Volume 10", "author": ["P. Bo", "L. Lillian", "V. Shivakumar"], "venue": "Association for Computational Linguistics,", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2002}, {"title": "Sentiment classification of online reviews to travel destinations by supervised machine learning approaches", "author": ["Q. Ye", "Z. Zhang", "R. Law"], "venue": "Expert Systems with Applications 36 (3, Part 2) ", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2009}, {"title": "C", "author": ["A.L. Maas", "R.E. Daly", "P.T. Pham", "D. Huang", "A.Y. Ng"], "venue": "Potts, Learning Word Vectors for Sentiment Analysis ", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2011}, {"title": "Predicting consumer sentiments from online text", "author": ["X. Bai"], "venue": "Decision Support Systems 50 (4) ", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2011}, {"title": "Sentiment classification on customer feedback data: Noisy data", "author": ["M. Gamon"], "venue": "large feature vectors, and the role of linguistic analysis, in: Proceedings of 27  the 20th International Conference on Computational Linguistics, COLING \u201904", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2004}, {"title": "The importance of neutral examples for learning sentiment", "author": ["J. Schler", "J. Schler"], "venue": "in: In Workshop on the Analysis of Informal and Formal Information Exchange during Negotiations (FINEXIN", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2005}, {"title": "Comprehensive study on lexicon-based ensemble classification sentiment analysis", "author": ["L. Augustyniak", "P. Szymanski", "T. Kajdanowicz", "W. Tuliglowicz"], "venue": "Entropy 18 (1) ", "citeRegEx": "28", "shortCiteRegEx": null, "year": 2016}, {"title": "Intelligent Information and Database Systems: 8th Asian Conference", "author": ["A. Yousefpour", "R. Ibrahim", "H.N.A. Hamed", "T. Yokoi"], "venue": "ACIIDS 2016, Da Nang, Vietnam, March 14-16, 2016, Proceedings, Part I, Springer Berlin Heidelberg, Berlin, Heidelberg", "citeRegEx": "30", "shortCiteRegEx": null, "year": 2016}, {"title": "Recursive deep models for semantic compositionality over a sentiment treebank", "author": ["R. Socher", "A. Perelygin", "J.Y. Wu", "J. Chuang", "C.D. Manning", "A.Y. Ng", "C. Potts"], "venue": "in: Proceedings of the conference on empirical methods in natural language processing (EMNLP), Vol. 1631, Citeseer", "citeRegEx": "31", "shortCiteRegEx": null, "year": 2013}, {"title": "Rosemerry: A baseline message-level sentiment classification system", "author": ["H. Liang", "R. Fothergill", "T. Baldwin"], "venue": "SemEval-2015 ", "citeRegEx": "33", "shortCiteRegEx": null, "year": 2015}, {"title": "A survey on transfer learning", "author": ["S.J. Pan", "Q. Yang"], "venue": "IEEE Trans. on Knowl. and Data Eng. 22 (10) ", "citeRegEx": "34", "shortCiteRegEx": null, "year": 2010}, {"title": "Y", "author": ["Y. Yoshida", "T. Hirao", "T. Iwata", "M. Nagata"], "venue": "Matsumoto, Transfer learning for multiple-domain sentiment analysis - identifying domain dependent/independent word polarity., in: W. Burgard, D. Roth (Eds.), AAAI, AAAI Press", "citeRegEx": "35", "shortCiteRegEx": null, "year": 2011}, {"title": "A note on two problems in connexion with graphs", "author": ["E.W. Dijkstra"], "venue": "Numerische mathematik 1 (1) ", "citeRegEx": "37", "shortCiteRegEx": null, "year": 1959}, {"title": "Hidden factors and hidden topics: Understanding rating dimensions with review text", "author": ["J. McAuley", "J. Leskovec"], "venue": "in: Proceedings of the 7th ACM Conference on Recommender Systems, RecSys \u201913, ACM", "citeRegEx": "39", "shortCiteRegEx": null, "year": 2013}, {"title": "On the limited memory bfgs method for large scale optimization", "author": ["D.C. Liu", "J. Nocedal"], "venue": "Mathematical programming 45 (1-3) ", "citeRegEx": "40", "shortCiteRegEx": null, "year": 1989}], "referenceMentions": [{"referenceID": 0, "context": "The largest WordNet is for Polish [1] and English [2].", "startOffset": 34, "endOffset": 37}, {"referenceID": 1, "context": "The largest WordNet is for Polish [1] and English [2].", "startOffset": 50, "endOffset": 53}, {"referenceID": 2, "context": "Representation of the knowledge is an area of Artificial Intelligence concerned with how knowledge can be represented symbolically and manipulated in an automated way [4].", "startOffset": 167, "endOffset": 170}, {"referenceID": 3, "context": "Derived knowledge can be represented in various structures including: semantic nets, frames, rules, and ontologies [5].", "startOffset": 115, "endOffset": 118}, {"referenceID": 4, "context": "Word2Vec (Word-2-Vector) [6, 7] is the most successful method from this group.", "startOffset": 25, "endOffset": 31}, {"referenceID": 5, "context": "[8, 9], but Word2Vec outperformed them and gained much more attention.", "startOffset": 0, "endOffset": 6}, {"referenceID": 6, "context": "[10], is one of them.", "startOffset": 0, "endOffset": 4}, {"referenceID": 7, "context": "Lebret and Collobert [11] and Dhillon et al.", "startOffset": 21, "endOffset": 25}, {"referenceID": 8, "context": "Lebret and Collobert [13] presented alternative model based on counts.", "startOffset": 21, "endOffset": 25}, {"referenceID": 9, "context": "In conclusion, Word2Vec is a \u201dpredictive\u201d model, whereas GloVe is a \u201dcountbased\u201d model [14].", "startOffset": 87, "endOffset": 91}, {"referenceID": 10, "context": "They are different computational methods that produce a very similar type of a semantic models [15, 16].", "startOffset": 95, "endOffset": 103}, {"referenceID": 11, "context": "They are different computational methods that produce a very similar type of a semantic models [15, 16].", "startOffset": 95, "endOffset": 103}, {"referenceID": 1, "context": "for English [2, 17].", "startOffset": 12, "endOffset": 19}, {"referenceID": 12, "context": "Current most known solution for the problem have been proposed by Robert Floyd [18].", "startOffset": 79, "endOffset": 83}, {"referenceID": 13, "context": "Simple geometrical optimization allowed to decrease complexity to O( n 3 log(n) ) [19].", "startOffset": 82, "endOffset": 86}, {"referenceID": 14, "context": "Work of Yijie Han [20] exceeds, unbreakable for long time, result of O( n 3 log(n) ).", "startOffset": 18, "endOffset": 22}, {"referenceID": 12, "context": "Method proposed by Robert Floyd [18] thanks to its simplicity can be distributed in easiest way, thanks to what it can be easily used in distributed environment for big datasets .", "startOffset": 32, "endOffset": 36}, {"referenceID": 15, "context": "Sentiment Analysis Nowadays, the most commonly used methods to sentiment analysis are the classification approaches with classifiers such as Naive Bayes [21, 22, 23], Support Vector Machines (SVM) [24, 25, 26], Decision Tree [27, 28], Random Forest [29] or Logistic Regression [28].", "startOffset": 153, "endOffset": 165}, {"referenceID": 16, "context": "Sentiment Analysis Nowadays, the most commonly used methods to sentiment analysis are the classification approaches with classifiers such as Naive Bayes [21, 22, 23], Support Vector Machines (SVM) [24, 25, 26], Decision Tree [27, 28], Random Forest [29] or Logistic Regression [28].", "startOffset": 153, "endOffset": 165}, {"referenceID": 17, "context": "Sentiment Analysis Nowadays, the most commonly used methods to sentiment analysis are the classification approaches with classifiers such as Naive Bayes [21, 22, 23], Support Vector Machines (SVM) [24, 25, 26], Decision Tree [27, 28], Random Forest [29] or Logistic Regression [28].", "startOffset": 197, "endOffset": 209}, {"referenceID": 18, "context": "Sentiment Analysis Nowadays, the most commonly used methods to sentiment analysis are the classification approaches with classifiers such as Naive Bayes [21, 22, 23], Support Vector Machines (SVM) [24, 25, 26], Decision Tree [27, 28], Random Forest [29] or Logistic Regression [28].", "startOffset": 197, "endOffset": 209}, {"referenceID": 19, "context": "Sentiment Analysis Nowadays, the most commonly used methods to sentiment analysis are the classification approaches with classifiers such as Naive Bayes [21, 22, 23], Support Vector Machines (SVM) [24, 25, 26], Decision Tree [27, 28], Random Forest [29] or Logistic Regression [28].", "startOffset": 197, "endOffset": 209}, {"referenceID": 20, "context": "Sentiment Analysis Nowadays, the most commonly used methods to sentiment analysis are the classification approaches with classifiers such as Naive Bayes [21, 22, 23], Support Vector Machines (SVM) [24, 25, 26], Decision Tree [27, 28], Random Forest [29] or Logistic Regression [28].", "startOffset": 225, "endOffset": 233}, {"referenceID": 21, "context": "Sentiment Analysis Nowadays, the most commonly used methods to sentiment analysis are the classification approaches with classifiers such as Naive Bayes [21, 22, 23], Support Vector Machines (SVM) [24, 25, 26], Decision Tree [27, 28], Random Forest [29] or Logistic Regression [28].", "startOffset": 225, "endOffset": 233}, {"referenceID": 21, "context": "Sentiment Analysis Nowadays, the most commonly used methods to sentiment analysis are the classification approaches with classifiers such as Naive Bayes [21, 22, 23], Support Vector Machines (SVM) [24, 25, 26], Decision Tree [27, 28], Random Forest [29] or Logistic Regression [28].", "startOffset": 277, "endOffset": 281}, {"referenceID": 22, "context": "[30] proposed a hybrid method and two meta-heuristic algorithms are employed to find an optimal feature subset.", "startOffset": 0, "endOffset": 4}, {"referenceID": 23, "context": "[31] proposed recursive deep model for sentiment using treebank structure of sentences.", "startOffset": 0, "endOffset": 4}, {"referenceID": 24, "context": "What is more, some systems leverages both hand-crafted features and word level embedding features, like Do2Vec, with the usage of classifiers such as SVM [33].", "startOffset": 154, "endOffset": 158}, {"referenceID": 25, "context": "Transfer Learning Transfer learning provides system ability to recognize and apply knowledge extraction (learn in the previous tasks) to the novel tasks (in new domains) [34].", "startOffset": 170, "endOffset": 174}, {"referenceID": 26, "context": "[35] proposed a model, where each word is associated with three factors: domain label, domain dependence/independence and word polarity.", "startOffset": 0, "endOffset": 4}, {"referenceID": 1, "context": "To demonstrate the WordNet2Vec method, it was applied to English WordNet [2].", "startOffset": 73, "endOffset": 76}, {"referenceID": 27, "context": "In our approach, we have used distributed implementation of Dijkistra [37] algorithm available in our SparklingGraph library [38].", "startOffset": 70, "endOffset": 74}, {"referenceID": 12, "context": "Most the known solutions for the all-pair shortest paths (APSP) problem has memory complexity of O(n), where n is the number of nodes in the graph [18].", "startOffset": 147, "endOffset": 151}, {"referenceID": 28, "context": "Datasets We chose data from one data source - Amazon e-commerce platform [39].", "startOffset": 73, "endOffset": 77}, {"referenceID": 29, "context": "Logistic regression was selected as a supervised learning model and in order to train it we have used limited memory BFGS algorithm [40].", "startOffset": 132, "endOffset": 136}], "year": 2016, "abstractText": "A complex nature of big data resources demands new methods for structuring especially for textual content. WordNet is a good knowledge source for comprehensive abstraction of natural language as its good implementations exist for many languages. Since WordNet embeds natural language in the form of a complex network, a transformation mechanism WordNet2Vec is proposed in the paper. It creates vectors for each word from WordNet. These vectors encapsulate general position role of a given word towards all other words in the natural language. Any list or set of such vectors contains knowledge about the context of its component within the whole language. Such word representation can be easily applied to many analytic tasks like classification or clustering. The usefulness of the WordNet2Vec method was demonstrated in sentiment analysis, i.e. classification with transfer learning for the real Amazon opinion textual dataset.", "creator": "LaTeX with hyperref package"}}}