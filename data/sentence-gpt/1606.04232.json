{"id": "1606.04232", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "14-Jun-2016", "title": "DCNNs on a Diet: Sampling Strategies for Reducing the Training Set Size", "abstract": "Large-scale supervised classification algorithms, especially those based on deep convolutional neural networks (DCNNs), require vast amounts of training data to achieve state-of-the-art performance. Decreasing this data requirement would significantly speed up the training process and possibly improve generalization. Motivated by this objective, we consider the task of adaptively finding concise training subsets which will be iteratively presented to the learner in the next few years.\n\n\nWe present a new algorithm for finding specific training subsets which will be iteratively presented to the learner in the next few years. We introduce a new algorithm for finding specific training subsets which will be iteratively presented to the learner in the next few years. The first version provides a method for learning a random set of a given pattern, and allows the learner to randomly choose a random set of a random set, in order to learn random sets of a certain pattern, which will be random. As a consequence of this method, we also incorporate a new method for learning a random set of a random set of a random set of a random set of a random set of a random set of a random set of a random set of a random set of a random set of a random set of a random set of a random set of a random set of a random set of a random set of a random set of a random set of a random set of a random set of a random set of a random set of a random set of a random set of a random set of a random set of a random set of a random set of a random set of a random set of a random set of a random set of a random set of a random set of a random set of a random set of a random set of a random set of a random set of a random set of a random set of a random set of a random set of a random set of a random set of a random set of a random set of a random set of a random set of a random set of a random set of a random set of a random set of a random set of a random set of a random set of a random set of a random set of a random set of a random set of a random set of a random set of a random set of a random set of a random set of a random set of a random set of a random set of a random set of a random set of a random set of a random set of a random set of a random set of a random set of a", "histories": [["v1", "Tue, 14 Jun 2016 07:38:13 GMT  (922kb,D)", "http://arxiv.org/abs/1606.04232v1", null]], "reviews": [], "SUBJECTS": "cs.CV cs.LG", "authors": ["maya kabkab", "azadeh alavi", "rama chellappa"], "accepted": false, "id": "1606.04232"}, "pdf": {"name": "1606.04232.pdf", "metadata": {"source": "META", "title": "DCNNs on a Diet: Sampling Strategies for Reducing the Training Set Size", "authors": ["Maya Kabkab", "Azadeh Alavi", "Rama Chellappa"], "emails": [], "sections": [{"heading": null, "text": "Large-scale supervised classification algorithms, especially those based on deep convolutional neural networks (DCNNs), require vast amounts of training data to achieve state-of-the-art performance. Decreasing this data requirement would significantly speed up the training process and possibly improve generalization. Motivated by this objective, we consider the task of adaptively finding concise training subsets which will be iteratively presented to the learner. We use convex optimization methods, based on an objective criterion and feedback from the current performance of the classifier, to efficiently identify informative samples to train on. We propose an algorithm to decompose the optimization problem into smaller per-class problems, which can be solved in parallel. We test our approach on standard classification tasks and demonstrate its effectiveness in decreasing the training set size without compromising performance. We also show that our approach can make the classifier more robust in the presence of label noise and class imbalance."}, {"heading": "1 Introduction", "text": "Deep learning has recently shown remarkable performance on many complex classification tasks. Currently, the best performing deep networks have many hidden layers and an extremely large number of trainable parameters, therefore requiring vast amounts of training data [1\u20133]. This raises the question of whether all this data is really necessary and whether all training samples are equally valuable in the learning process. While it is true that presenting the classifier with enough information is essential to achieving good performance, large training set sizes can be detrimental to generalization performance and invariably need significant training time. Such large training sets can often include redundant or noisy samples which only introduce unnecessary computations and could cause learning bias. In this paper, we address the problem of adaptively finding a smaller subset of training samples which allow fast learning without compromising performance.\nThis problem, sometimes referred to as exemplar or active selection, has been studied in the literature. Starting with a given set of labeled examples, active selection aims to identify a subset to use for training, while leveraging information obtained from the classifier trained on previous selections. One simple approach [4] repeatedly presents the same example if the network error exceeds a threshold. In [5], this problem is addressed in the context of feedforward neural networks. The authors propose a sequential method to select one training sample at a time such that, when added to the previous set of examples, it results in the largest decrease in a squared error estimate criterion. A similar objective is considered in [6] based on pattern informativeness \u2013 a measure of a sample\u2019s influence on the classifier output.\n.\nar X\niv :1\n60 6.\n04 23\n2v 1\n[ cs\n.C V\n] 1\n4 Ju\nn 20\nA closely related approach is active learning which starts with an unlabeled set of examples and sequentially identifies critical samples to label and train on [7\u201310]. It is shown that a classifier trained on a carefully chosen subset can sometimes outperform one that is trained on all the available data. Furthermore, [11] suggests that guiding a classifier by presenting training samples in an order of increasing difficulty can speed up learning and result in convergence to a better local minimum.\nIn this paper, we present strategies to make optimal use of available training data by adaptively selecting batches of training samples which will be iteratively presented to the classifier. In contrast with active learning, we assume a fully supervised setting where all training samples are labeled and available a priori. We are interested in incrementally training a deep neural network, using batches of training data carefully selected to meet four criteria: class balance, diversity, representativeness, and classifier uncertainty. The class balance criterion utilizes the a priori knowledge of labels to ensure that all classes are appropriately present in the new training batch. We propose a novel class balancing algorithm which uses immediate feedback from the classifier to allot a subset of training samples to each class based on the average classifier performance on that class. Diversity and representativeness are distance-based measures aiming to reduce redundancy while maximizing the quality of selected samples. Such strategies have been used in active learning [12, 13], subset selection [14, 15], and clustering [16]. Finally, the classifier uncertainty criterion favors samples that the classifier has not yet properly learnt, thus driving it to explore unvisited parts of the input space. We combine the last three criteria and use optimization techniques from [17, 18] to identify a near-optimal batch to train on at every iteration. We apply our methods on the problems of digit and face recognition. Our results indicate that the training set size can be significantly reduced without sacrificing performance.\nThe rest of the paper is organized as follows. The problem formulation is stated in Sections 2.1 - 2.5 and the proposed solution in Section 2.6. Experimental results, comparing our method to random sampling, are presented in Section 3."}, {"heading": "2 Problem statement", "text": "We assume we are given a fixed classifier architecture, and a set of labeled training data points: X = SLk=1 Xk, where Xk = {X1,k, X2,k, . . . , XNk,k} are the training samples belonging to class k, and L is the number of classes. At each time instance t, we select a subset Bt \u21e2 X , such that the classifier (which has previously been trained on Bt 1) exhibits good generalization performance when trained on Bt. To this end, we formulate a criterion for selecting new training examples which serves the following objectives:\n(O1) The samples in Bt must be such that the classifier is uncertain about classifying them (or certain but wrong in its classification).\n(O2) Bt should have a balanced selection from all classes. (O3) Bt should be sufficiently diverse. (O4) Bt should be representative of X .\nWe will mathematically formulate each of these objectives in the following sections."}, {"heading": "2.1 Classifier uncertainty and error", "text": "We assume that, at time instance t, the classifier produces L outputs for each training sample Xi,k from class k, denoted by\npt(Xi,k) = [p t 1(Xi,k), p t 2(Xi,k), . . . , p t L(Xi,k)], (1)\nwhere ptl(X) is interpreted as the classifier\u2019s estimate of the probability that X 2 X belongs to class l, and satisfies ptl(X) 0 8l, and PL l=1 p t l(X) = 1. In order to quantify classifier uncertainty and error we define:\nct(Xi,k) = LX\nl=1\nt \u00b7 1 [l = k] + (1 t) \u00b7 ptl(Xi,k) log ptl(Xi,k), (2)\nwhere t 2 [0, 1] is a chosen parameter. We can interpret ct(Xi,k) in two ways. First, ct(Xi,k) can be seen as a weighted sum of an error term: PLl=1 1 [l = k] \u00b7 log ptl(Xi,k) = log ptk(Xi,k) and an entropy term: PLl=1 ptl(Xi,k) log ptl(Xi,k). These two terms correspond to the correctness of the classifier\u2019s decision and the uncertainty in this decision, therefore satisfying objective (O1) above. Second, ct(Xi,k) can be interpreted as a bootstrapping technique to overcome possible label noise [19], in which case t1 [l = k] + (1 t)ptl(Xi,k) is a weighted \u201ccorrect label\u201d and ct(Xi,k) represents the cross-entropy between pt(Xi,k) and this weighted label. ct(\u00b7) being low on a given sample means that the classifier has enough information about this sample. In order to present the classifier with informative samples, we would therefore like to pick samples where ct(\u00b7) is large."}, {"heading": "2.2 Class balance", "text": "At each time instance t, we would like to select a total of M t samples, distributed among all classes in a balanced way. However, it might be counter-intuitive to simply impose that all classes be equally represented in the subset Bt, as the current classifier may be performing very well on some of them. Therefore, we assign a budget M tk to each class depending on the average performance on this class. This can be measured by ctk = 1 Nk PNk i=1 c t(Xi,k), where ct(Xi,k) is defined in equation (2). The\nlarger ctk is, the more samples we assign to class k. An objective function of PL k=1 c t k \u00b7 M tk would result in the trivial solution of assigning all the budget M t to the class with the largest uncertainty score, and would contradict the class balancing requirement. We therefore use a logarithmic objective function and formulate the problem as follows:\nmax Mtk2Z+\nLX\nk=1\nlog \u2713 1 + \u21b5 \u00b7 ctk\nM tk M t\n\u25c6 s. t.\nLX\nk=1\nM tk  M t; M tk  |Xk|, (3)\nwhere \u21b5 > 1 sets the sensitivity of the method (the smaller \u21b5, the larger the effect of differences in ctk) . This problem arises in information theory, in allocating power to a set of communication channels [20, Section 9.4]. We use a similar formulation since M tk represents the budget allocated to the kth class (channel), and 1/ctk is akin to channel quality. There exists a very efficient solution to this convex optimization problem, known as the water-filling algorithm [21, Section 5.5], where we interpret water levels as the number of samples allocated to each class. Our formulation differs from the standard formulation due to the addition of the last constraint (which ensures that we do not allocate more samples than available in the pool Xk). Another difference is that the feasible set in (3) is the set of non-negative integers.\nTheorem 1 The modified water-filling problem in (3) can be solved using Algorithm 1.\nAlgorithm 1 Integer water-filling algorithm with caps\n1: Sort the base levels M t \u21b5ctk in ascending order and take the ceiling of the base levels dMt \u21b5ctk e. 2: Place \u201ccaps\u201d at dMt \u21b5ctk\ne+ |Xk|. 3: repeat 4: Fill with one water unit at a time proceeding from left to right without exceeding any cap. 5: until M t water units are used or all empty spaces are filled.\n\u201cCaps\u201d enforce the M tk  |Xk| constraints. Each water unit corresponds to one training sample being assigned to a class. An illustration of the algorithm is found in Figure 1 for a budget M t = 10. The numbers on the water units show the order in which they have been assigned. Because of the balanced selection of budgets {M tk}, this formulation addresses the class balance objective (O2).\nRemark 1 Objectives (O3) and (O4) are only meaningful when considered as intraclass rather than globally. Two images from different classes trivially meet the diversity criterion but cannot be representative of each other. Since we are considering supervised learning settings, we can leverage the label information and focus on finding a diverse representative subset of each class separately. The budget selection algorithm in Theorem 1 allows us to do so by distributing our original budget M t amongst the various classes. We can therefore solve L independent problems. We drop the class\nsubscript k and assume that we would like to select a subset Bt from a pool of samples X , where all the samples belong to the same class. For notational convenience, we also drop the time superscript t, with the understanding that this procedure will be repeated at every time step."}, {"heading": "2.3 Subset diversity", "text": "As per (O3), we would like to select a diverse subset, i.e., one that does not have too much redundancy. To this end, we assume we have a distance metric d(\u00b7, \u00b7) such that d(Xi, Xj) represents the distance between samples Xi and Xj . This can, for example, be the Euclidean distance between Xi and Xj , or the Euclidean distance between their feature vectors, in some pre-defined feature space. In order to maximize diversity, we seek to maximize the average distance between all selected samples, i.e., find B such that:\n1\nM2\nX\nX2B\nX\nX02B d(X, X 0) (4)\nis maximized,1 where M is the budget allocated by the water-filling algorithm. Let N = |X |, the training set size of the class under consideration. We introduce a binary variable s 2 {0, 1}N , such that si = 1 if Xi 2 B, and si = 0 otherwise. We also group all the distances in a matrix D 2 RN\u21e5N such that Dij = d(Xi, Xj). As such, the objective can be re-written as\nmax s2{0,1}N\n1\nM2 s|Ds. (5)\nThis problem formulation ensures that the chosen samples are sufficiently distant from each other."}, {"heading": "2.4 Subset representativeness", "text": "Per (O4), we would also like to select a representative subset B, i.e., the non-selected samples must be well represented by the set B. To this end, we seek to minimize the average distance between selected and non-selected samples. As before, this can be re-written as\nmin s2{0,1}N\n1\nM(N M) (1 s) |Ds, (6)\nwhere 1 is the vector of all ones in RN ."}, {"heading": "2.5 Joint formulation", "text": "As previously mentioned, once the sub-problem of allocating budgets to each class has been solved, we seek to solve L independent problems of finding a diverse, representative subset over which the classifier performs poorly. We therefore combine the subset diversity, representativeness, and uncertainty criteria. We define the vector c , [c(X1), . . . , c(XN )]| where c(\u00b7) is as defined in\n1Other objective functions can be formulated such as maximizing the minimum distance between selected samples. While guaranteeing less redundancy, such objective functions are more difficult to solve.\nSection 2.1. To make the quantities comparable, we normalize D and c such that all their elements lie in [0, 1]. We denote the normalized quantities by D\u0303 and c\u0303 respectively. Our objective function is:\nmax s2{0,1}N\n1 \u00b7 1 M2 s|D\u0303s\n| {z } diversity\n2 \u00b7 1 M(N M) (1 s) |D\u0303s\n| {z } representativeness\n+ 3 \u00b7 1 M c\u0303|s\n| {z } classifier uncertainty\n, (7)\nwhere 1, 2, 3 0 are parameters which dictate the relative importance of each criterion. We need to add the constraint that |B| = M , where M is the budget allocated by the water-filling algorithm. The joint optimization problem for each class is therefore:\nmin s2{0,1}N\n1 \u00b7 1 M s|D\u0303s + 2 \u00b7\n1\nN M (1 s) |D\u0303s 3 \u00b7 c\u0303|s s. t. 1|s = M. (8)\nIt is important to note that the division of our problem into L independent sub-problems provides many advantages. First, formulating the problem on the entire training dataset would require a very large distance matrix D which would, in most cases, need excessive storage. Second, the L sub-problems are completely independent and can run in parallel, thus reducing computation time."}, {"heading": "2.6 Proposed solution", "text": "The problem in (8) is not convex for two reasons: (i) the set {0, 1}N is finite and therefore not convex, and (ii) the objective function is generally not convex. We change the constraint 1|s = M to its equivalent (1|s M)2 = 0 (as this guarantees zero duality gap [17]) and make the change of variable x = 2s 1, where x 2 { 1, 1}N . Let\nA , \u2713\n1 4M + 2 4(N M)\n\u25c6 D\u0303, b , 1\n2M D\u0303|1 3 2 c\u0303. (9)\nAn equivalent problem to (8) is given by:\nmin x2{ 1,1}N\nx|Ax + b|x s. t. (1|x 2M + N)2 = 0. (10)\nThis problem is known as constrained binary quadratic programming and is NP-hard [17]. We seek an efficient relaxation to this problem.\nTheorem 2 The solution x\u21e4 to (10) can be well-approximated by\nx\u0302\u21e4 = 1 2 (A + \u00b5\u21e411| + \u21e4I)\u2020 (b 2\u00b5\u21e4(2M N)1), (11)\nwhere (\u00b7)\u2020 denotes the pseudo-inverse, I denotes the identity matrix in RN\u21e5N , and \u00b5\u21e4, \u21e4 are the solution to the following semi-definite program (SDP):\nmax \u00b5, ,\u23272R\n(2M N)2\u00b5 N \u2327\ns. t.\n\u2327 12 (b 2\u00b5(2M N)1)|\n1 2 (b 2\u00b5(2M N)1) A + \u00b511| + I\n! \u232b 0 (12)\nWe select the samples corresponding to the largest M entries in x\u0302\u21e4."}, {"heading": "3 Experiments", "text": "In this section, we test the proposed method on several real-world classification tasks. We consider digit and face recognition problems. We compare our approach to the random selection of training samples as used in ordinary training algorithms. Our formulation does not assume a specific classifier structure. However, we will illustrate our results on deep neural networks as they are the current state-of-the-art. We use the Caffe framework [22] for the implementation of Convolutional Neural Networks (CNN) as well as the SDPA framework [23] to solve the SDP problem in (12). We also calculate distances between samples based on the Local Binary Patterns (LBP) features [24].\nFor each of these experiments, unless otherwise specified, we start from a randomly initialized CNN with a fixed architecture. First, we test this CNN on the entire pool of training examples to get the initial average uncertainty levels {c0k}Lk=1. Then, at every time step t, we use {ctk}Lk=1 to obtain a class-specific budget using Algorithm 1 and solve (11) and (12) independently for each class, resulting in a new selected batch. We resume training, starting from the previous classifier weights, on the union of all selected batches. At each time step, all candidate samples have a chance to be selected, i.e., previously selected examples are not removed from the set of candidates. We iterate until a stopping criterion is met. We choose this stopping criterion to be a threshold on the classifier error on the entire pool of training samples. As most samples in this pool have not been presented to the classifier, this is an adequate estimate of the generalization performance. The overall algorithm is illustrated in Figure 2.\nWe do not employ any type of data pre-processing or augmentation techniques which are widely used to achieve state-of-the-art performance, since these methods are not the focus of this work. Instead, we choose to focus on the effect of our training set selection method on the generalization performance, compared to picking the training samples in a random fashion. As our training is incremental, we add dropout layers [25] whenever necessary to combat the problem of catastrophic forgetting in deep neural networks. Catastrophic forgetting refers to the inability of a learning method to preserve previously learnt information when exclusively trained on new data [26, 27]."}, {"heading": "3.1 MNIST digit recognition", "text": "For the problem of digit recognition on the well-known MNIST dataset, we use the LeNet architecture [28]. We run our experiments using a randomly selected subset of the MNIST dataset consisting of 1000 images from each class. We use a total budget of 50 training images per one loop of our algorithm (see Figure 2)."}, {"heading": "3.1.1 Diversity vs. representativeness", "text": "We first illustrate the effect of the weights 1, 2, defined in (7), on the selection process. We set 3 = 0. Figure 3 shows the selected samples when 2 = 20 1 (top) and 2 = 1 (bottom). When 2 is large, more representative samples are chosen, as seen in Figure 3, top. When 1 = 2, more diverse samples are chosen. This validates our initial objective formulation in (7)."}, {"heading": "3.1.2 Clean labels", "text": "We compare our method of adaptively selecting training batches to the baseline of random selection. As discussed in [11], we introduce \u201ceasier\u201d samples first and gradually increase the difficulty. We achieve this by keeping 1 fixed, and starting with 2 = 10 1 and 3 = 0. Picking a large 2 puts more weight on the representativeness term in (7) and thus ensures that outliers are not picked. We gradually decrease 2 and increase 3 in order to allow for more difficult examples to be sampled. We present our findings in Figure 4(A). Our approach outperforms random selection by a margin of 4%. Furthermore, the number of samples required by our proposed method to reach a target performance level is much smaller than random sampling. For instance, for a target testing accuracy of 94%, around 700 samples are needed for random as opposed to less than 350 samples for our approach.\nTo assess the quality of the achieved local minimum obtained with our method, we train a CNN from scratch on all of the selected samples introduced at the same time (using the regular non-adapative CNN training). This results in a testing accuracy of 96.3%, inferior to the one obtained by our method (98%). This validates the claim that adaptive selection of training data guides the neural network towards a better local optimum. Our algorithm has selected easier training samples in the first few iterations, and more difficult samples later on, as dictated by the change in weights 1, 2, 3."}, {"heading": "3.1.3 Noisy labels", "text": "We now assess the performance of our algorithm in the presence of label noise. We randomly change the correct labels in 20% and 30% of the training samples. The results are shown in Figures 4(B) and 4(C), respectively. It is seen that our approach out-performs random selection by more than 5%. To combat label noise, we decrease the diversity weight 1 and adopt a more \u201ccautious\u201d approach by increasing 3 at a slower pace. This results in a slower but safer update of the network. In fact, the total number of noisy training images chosen by our algorithm for the case of 20% label noise is 93 images by the 12th loop (or 6.5% of the picked images), whereas random sampling obviously picks around 20% noisy samples."}, {"heading": "3.1.4 Data imbalance", "text": "Finally, we test our method on a scenario where there is a significant data imbalance between different classes. This can happen when acquiring labeled data for some classes is considerably more difficult than for others. We artificially introduce data imbalance by picking 4 classes at random and reducing their training set size to between 10 and 20 images per class. Our approach achieves 90.14% testing accuracy after only 9 loops of the algorithm (i.e., 450 picked samples), while random sampling achieves 86.91% using the entire training set. We are thus able to boost the performance by over 3% while only using a fraction of the available samples. In our algorithm, picked samples are not removed from the pool of available training images, thus allowing the network to revisit certain training samples if required. This is especially crucial in the case of data imbalance since random selection has very low probability of selecting images from the down-sampled classes. Figure 4(D) shows the number of classification mistakes made by a CNN trained with our algorithm and with random selection. Classes 1, 3, 6, and 7 were significantly down-sampled. Our approach allows the CNN to perform well on these classes compared to random sampling."}, {"heading": "3.2 VGG Face dataset", "text": "For the problem of face recognition, we choose to start with a pre-trained CNN in order to illustrate the use of our algorithm for transfer learning, as a fine-tuning sampling strategy. Using the methods and network described in [29], a CNN was pre-trained on the CASIA-WebFace dataset [30]. Instead of random initialization, we start with the pre-trained weights for the first 15 layers (up to the fifth pooling layer), and add two randomly initialized fully connected layers joined by a dropout layer. We train and test on the VGG Face dataset [31]. Since CASIA-WebFace and VGG Face have significant subject overlap, we choose 20 of the non-overlapping subjects. The VGG Face dataset consists of a large number of images, out of which a portion has been selected as part of the final curated set. We observe that the non-curated images are considerably more affected by label and bounding box noise. In order to get meaningful test results, we restrict our testing set to the curated images, while training on the entire dataset. We perform five-fold cross-validation using 5 random splits. We choose a budget of 100 training samples per loop of our algorithm.\nFigure 5 shows some examples of images selected by our algorithm. The top images are selected in the first loop, when the representativeness score 2 is large and the uncertainty score 3 zero. We notice that all chosen samples are frontal, of good quality, and typical of the subjects. The bottom images are chosen much later in the process, after 2 has considerably decreased and 3 increased. This time, our method chooses more difficult examples which include extreme poses, obstruction, blur, an additional person, and an unusually young version of the subject.\nWe present the performance results of this CNN trained using our algorithm and random sampling in Table 1. Using only one loop (i.e., 100 picked images), the testing accuracy increases to 89.69% compared to 80.05% for random sampling. After 13 loops, the performance of random sampling plateaus at 94.89%, while our approach achieves 97.15%, which cuts the error in half."}, {"heading": "4 Conclusion", "text": "In this paper, we addressed the problem of reducing the training data requirement of deep neural networks. We proposed an efficient iterative and adaptive algorithm based on convex optimization. We demonstrated its effectiveness on real-life datasets and robustness to label noise and class imbalance."}, {"heading": "Acknowledgments", "text": "This research is based upon work supported by the Office of the Director of National Intelligence (ODNI), Intelligence Advanced Research Projects Activity (IARPA), via IARPA R&D Contract No. 2014-14071600012. The views and conclusions contained herein are those of the authors and should not be interpreted as necessarily representing the official policies or endorsements, either expressed or implied, of the ODNI, IARPA, or the U.S. Government. The U.S. Government is authorized to reproduce and distribute reprints for Governmental purposes notwithstanding any copyright annotation thereon."}], "references": [{"title": "A recipe for semidefinite relaxation for (0", "author": ["S. Poljak", "F. Rendl", "H. Wolkowicz"], "venue": "1)-quadratic programming,\u201d Journal of Global Optimization, vol. 7, no. 1, pp. 51\u201373", "citeRegEx": "1", "shortCiteRegEx": null, "year": 1995}, {"title": "Convex optimization", "author": ["S. Boyd", "L. Vandenberghe"], "venue": "Cambridge University Press", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2004}, {"title": "Quadratic optimization problems,", "author": ["N. Shor"], "venue": "Soviet Journal of Computer and and Systems Sciences,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 1987}, {"title": "Conic and robust optimization,", "author": ["A. Ben-Tal"], "venue": null, "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2002}, {"title": "and Y", "author": ["X. Zheng", "X. Sun", "D. Li"], "venue": "Xia, \u201cDuality gap estimation of linear equality constrained binary quadratic programming,\u201d Mathematics of Operations Research, vol. 35, no. 4, pp. 864\u2013880", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2010}], "referenceMentions": [{"referenceID": 0, "context": "Currently, the best performing deep networks have many hidden layers and an extremely large number of trainable parameters, therefore requiring vast amounts of training data [1\u20133].", "startOffset": 174, "endOffset": 179}, {"referenceID": 1, "context": "Currently, the best performing deep networks have many hidden layers and an extremely large number of trainable parameters, therefore requiring vast amounts of training data [1\u20133].", "startOffset": 174, "endOffset": 179}, {"referenceID": 2, "context": "Currently, the best performing deep networks have many hidden layers and an extremely large number of trainable parameters, therefore requiring vast amounts of training data [1\u20133].", "startOffset": 174, "endOffset": 179}, {"referenceID": 3, "context": "One simple approach [4] repeatedly presents the same example if the network error exceeds a threshold.", "startOffset": 20, "endOffset": 23}, {"referenceID": 4, "context": "In [5], this problem is addressed in the context of feedforward neural networks.", "startOffset": 3, "endOffset": 6}, {"referenceID": 0, "context": "where t 2 [0, 1] is a chosen parameter.", "startOffset": 10, "endOffset": 16}, {"referenceID": 0, "context": "To make the quantities comparable, we normalize D and c such that all their elements lie in [0, 1].", "startOffset": 92, "endOffset": 98}], "year": 2016, "abstractText": "Large-scale supervised classification algorithms, especially those based on deep convolutional neural networks (DCNNs), require vast amounts of training data to achieve state-of-the-art performance. Decreasing this data requirement would significantly speed up the training process and possibly improve generalization. Motivated by this objective, we consider the task of adaptively finding concise training subsets which will be iteratively presented to the learner. We use convex optimization methods, based on an objective criterion and feedback from the current performance of the classifier, to efficiently identify informative samples to train on. We propose an algorithm to decompose the optimization problem into smaller per-class problems, which can be solved in parallel. We test our approach on standard classification tasks and demonstrate its effectiveness in decreasing the training set size without compromising performance. We also show that our approach can make the classifier more robust in the presence of label noise and class imbalance.", "creator": "LaTeX with hyperref package"}}}