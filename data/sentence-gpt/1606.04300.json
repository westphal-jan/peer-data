{"id": "1606.04300", "review": {"conference": "ACL", "VERSION": "v1", "DATE_OF_SUBMISSION": "14-Jun-2016", "title": "Neural Word Segmentation Learning for Chinese", "abstract": "Most previous approaches to Chinese word segmentation formalize this problem as a character-based sequence labeling task so that only contextual information within fixed sized local windows and simple interactions between adjacent tags can be captured. In this paper, we propose a novel neural framework which thoroughly eliminates context windows and can utilize complete segmentation history. Our model employs a gated combination neural network over characters to produce distributed representations of word candidates, which are then given to a long short-term memory (LSTM) language scoring model. We also propose a hierarchical analysis that has a single-layer approach to the word classification. We describe how this model captures and assigns a generalized and generalizable representation of words in different LSTM networks. Our framework is based on a single-layer approach which utilizes a simple model of LSTM language scoring model with a single-layer approach. This approach is implemented in our new approach to LSTM. The concept of a single-layer approach is one of the most attractive features of LSTM languages (see https://github.com/lstm/neurization-processing) and provides the ability to characterize LSTM language classification in the LSTM lexicon (see https://github.com/lstm/neurization-processing)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n", "histories": [["v1", "Tue, 14 Jun 2016 10:52:21 GMT  (703kb,D)", "http://arxiv.org/abs/1606.04300v1", "ACL2016"], ["v2", "Fri, 2 Dec 2016 08:06:10 GMT  (897kb,D)", "http://arxiv.org/abs/1606.04300v2", "ACL2016"]], "COMMENTS": "ACL2016", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["deng cai", "hai zhao"], "accepted": true, "id": "1606.04300"}, "pdf": {"name": "1606.04300.pdf", "metadata": {"source": "CRF", "title": "Neural Word Segmentation Learning for Chinese", "authors": ["Deng Cai", "Hai Zhao"], "emails": ["thisisjcykcd@gmail.com,", "zhaohai@cs.sjtu.edu.cn"], "sections": [{"heading": "1 Introduction", "text": "Most east Asian languages including Chinese are written without explicit word delimiters, therefore, word segmentation is a preliminary step for processing those languages. Since Xue (2003), most methods formalize the Chinese word segmentation (CWS) as a sequence labeling problem with character position tags, which can be handled with su-\n\u2217Corresponding author. This paper was partially supported by Cai Yuanpei Program (CSC No. 201304490199 and No. 201304490171), National Natural Science Foundation of China (No. 61170114 and No. 61272248), National Basic Research Program of China (No. 2013CB329401), Major Basic Research Program of Shanghai Science and Technology Committee (No. 15JC1400103), Art and Science Interdisciplinary Funds of Shanghai Jiao Tong University (No. 14JCRZ04), and Key Project of National Society Science Foundation of China (No. 15-ZDA041).\npervised learning methods such as Maximum Entropy (Berger et al., 1996; Low et al., 2005) and Conditional Random Fields (Lafferty et al., 2001; Peng et al., 2004; Zhao et al., 2006a). However, those methods heavily depend on the choice of handcrafted features.\nRecently, neural models have been widely used for NLP tasks for their ability to minimize the effort in feature engineering. For the task of CWS, Zheng et al. (2013) adapted the general neural network architecture for sequence labeling proposed in (Collobert et al., 2011), and used character embeddings as input to a two-layer network. Pei et al. (2014) improved upon (Zheng et al., 2013) by explicitly modeling the interactions between local context and previous tag. Chen et al. (2015a) proposed a gated recursive neural network to model the feature combinations of context characters. Chen et al. (2015b) used an LSTM architecture to capture potential long-distance dependencies, which alleviates the limitation of the size of context window but introduced another window for hidden states.\nDespite the differences, all these models are designed to solve CWS by assigning labels to the characters in the sequence one by one. At each time step of inference, these models compute the tag scores of character based on (i) context features within a fixed sized local window and (ii) tagging history of previous one.\nNevertheless, the tag-tag transition is insufficient to model the complicated influence from previous segmentation decisions, though it could sometimes be a crucial clue to later segmentation decisions. The fixed context window size, which is broadly adopted by these methods for feature engineering, also restricts the flexibility of modeling diverse distances. Moreover, word-level information, which is being the greater granularity unit as suggested in (Huang and Zhao, 2006), remains\nar X\niv :1\n60 6.\n04 30\n0v 1\n[ cs\n.C L\n] 1\n4 Ju\nn 20\n16\nunemployed. To alleviate the drawbacks inside previous methods and release those inconvenient constrains such as the fixed sized context window, this paper makes a latest attempt to re-formalize CWS as a direct segmentation learning task. Our method does not make tagging decisions on individual characters, but directly evaluates the relative likelihood of different segmented sentences and then search for a segmentation with the highest score. To feature a segmented sentence, a series of distributed vector representations (Bengio et al., 2003) are generated to characterize the corresponding word candidates. Such a representation setting makes the decoding quite different from previous methods and indeed much more challenging, however, more discriminative features can be captured.\nThough the vector building is word centered, our proposed scoring model covers all three processing levels from character, word until sentence. First, the distributed representation starts from character embedding, as in the context of word segmentation, the n-gram data sparsity issue makes it impractical to use word vectors immediately. Second, as the word candidate representation is derived from its characters, the inside character structure will also be encoded, thus it can be used to determine the word likelihood of its own. Third, to evaluate how a segmented sentence makes sense through word interacting, an LSTM (Hochreiter and Schmidhuber, 1997) is used to chain together word candidates incrementally and construct the representation of partially segmented sentence at each decoding step, so that the coherence between next word candidate and previous segmentation history can be depicted.\nTo our best knowledge, our proposed approach to CWS is the first attempt which explicitly models the entire contents of the segmenter\u2019s state, including the complete history of both segmentation decisions and input characters. The compar-\nisons of feature windows used in different models are shown in Table 1. Compared to both sequence labeling schemes and word-based models in the past, our model thoroughly eliminates context windows and can capture the complete history of segmentation decisions, which offers more possibilities to effectively and accurately model segmentation context."}, {"heading": "2 Overview", "text": "We formulate the CWS problem as finding a mapping from an input character sequence x to a word sequence y, and the output sentence y\u2217 satisfies:\ny\u2217 = arg max y\u2208GEN(x) ( n\u2211 i=1 score(yi|y1, \u00b7 \u00b7 \u00b7 , yi\u22121))\nwhere n is the number of word candidates in y, and GEN(x) denotes the set of possible segmentations for an input sequence x. Unlike all previous works, our scoring function is sensitive to the complete contents of partially segmented sentence.\nAs shown in Figure 1, to solve CWS in this way, a neural network scoring model is designed to evaluate the likelihood of a segmented sentence. Based on the proposed model, a decoder is developed to find the segmented sentence with the highest score. Meanwhile, a max-margin method is utilized to perform the training by comparing\nthe structured difference of decoder output and the golden segmentation. The following sections will introduce each of these components in detail."}, {"heading": "3 Neural Network Scoring Model", "text": "The score for a segmented sentence is computed by first mapping it into a sequence of word candidate vectors, then the scoring model takes the vector sequence as input, scoring on each word candidate from two perspectives: (1) how likely the word candidate itself can be recognized as a legal word; (2) how reasonable the link is for the word candidate to follow previous segmentation history immediately. After that, the word candidate is appended to the segmentation history, updating the state of the scoring system for subsequent judgements. Figure 2 illustrates the entire scoring neural network."}, {"heading": "3.1 Word Score", "text": "Character Embedding. While the scores are decided at the word-level, using word embedding (Bengio et al., 2003; Wang et al., 2016) immediately will lead to a remarkable issue that rare words and out-of-vocabulary words will be poorly estimated (Kim et al., 2015). In addition, the character-level information inside an n-gram can be helpful to judge whether it is a true word. Therefore, a lookup table of character embeddings is used as the bottom layer.\nFormally, we have a character dictionary D of size |D |. Then each character c \u2208 D is repre-\nsented as a real-valued vector (character embedding) c \u2208 Rd, where d is the dimensionality of the vector space. The character embeddings are then stacked into an embedding matrix M \u2208 Rd\u00d7|D|. For a character c \u2208 D , its character embedding c \u2208 Rd is retrieved by the embedding layer according to its index.\nGated Combination Neural Network. In order to obtain word representation through its characters, in the simplest strategy, character vectors are integrated into their word representation using a weight matrix W(L) that is shared across all words with the same length L, followed by a non-linear function g(\u00b7). Specifically, ci (1 \u2264 i \u2264 L) are d-dimensional character vector representations respectively, the corresponding word vector w will be d-dimensional as well:\nw = g(W(L)  c1... cL ) (1) where W(L) \u2208 Rd\u00d7Ld and g is a non-linear function as mentioned above.\nAlthough the mechanism above seems to work well, it can not sufficiently model the complicated combination features in practice, yet.\nGated structure in neural network can be useful for hybrid feature extraction according to (Chen et al., 2015a; Chung et al., 2014; Cho et al., 2014), we therefore propose a gated combination neural network (GCNN) especially for character com-\npositionality which contains two types of gates, namely reset gate and update gate. Intuitively, the reset gates decide which part of the character vectors should be mixed while the update gates decide what to preserve when combining the characters information. Concretely, for words with length L, the word vector w \u2208 Rd is computed as follows:\nw = zN w\u0302 + L\u2211 i=1 zi ci\nwhere zN , zi (1 \u2264 i \u2264 L) are update gates for new activation w\u0302 and governed characters respectively, and indicates element-wise multiplication.\nThe new activation w\u0302 is computed as:\nw\u0302 = tanh(W(L)  r1 c1... rL cL ) where W(L) \u2208 Rd\u00d7Ld and ri \u2208 Rd (1 \u2264 i \u2264 L) are the reset gates for governed characters respectively, which can be formalized as: r1...\nrL\n = \u03c3(R(L)  c1...\ncL ) where R(L) \u2208 RLd\u00d7Ld is the coefficient matrix of reset gates and \u03c3 denotes the sigmoid function.\nThe update gates can be formalized as: zN z1 ... zL  = exp(U(L)  w\u0302 c1 ... cL )  1/Z 1/Z ... 1/Z  where U(L) \u2208 R(L+1)d\u00d7(L+1)d is the coefficient matrix of update gates, and Z \u2208 Rd is the normalization vector,\nZk = L\u2211 i=1 [exp(U(L)  w\u0302 c1 ... cL )]d\u00d7i+k\nwhere 0 \u2264 k < d. According to the normalization condition, the update gates are constrained by:\nzN + L\u2211 i=1 zi = 1\nThe gated mechanism is capable of capturing both character and character interaction characteristics to give an efficient word representation (See Section 6.3).\nWord Score. Denote the learned vector representations for a segmented sentence y with [y1,y2, \u00b7 \u00b7 \u00b7 ,yn], where n is the number of word candidates in the sentence. word score will be computed by the dot products of vector yi(1 \u2264 i \u2264 n) and a trainable parameter vector u \u2208 Rd.\nWord Score(yi) = u \u00b7 yi (2)\nIt indicates how likely a word candidate by itself is to be a true word."}, {"heading": "3.2 Link Score", "text": "Inspired by the recurrent neural network language model (RNN-LM) (Mikolov et al., 2010; Sundermeyer et al., 2012), we utilize an LSTM system to capture the coherence in a segmented sentence.\nLong Short-Term Memory Networks. The LSTM neural network (Hochreiter and Schmidhuber, 1997) is an extension of the recurrent neural network (RNN), which is an effective tool for sequence modeling tasks using its hidden states for history information preservation. At each time step t, an RNN takes the input xt and updates its recurrent hidden state ht by\nht = g(Uht\u22121 + Wxt + b)\nwhere g is a non-linear function. Although RNN is capable, in principle, to process arbitrary-length sequences, it can be difficult to train an RNN to learn long-range dependencies due to the vanishing gradients. LSTM addresses this problem by introducing a memory cell to preserve states over long periods of time, and controls the update of hidden state and memory cell by three types of gates, namely input gate, forget gate and output gate. Concretely, each step of LSTM takes input xt,ht\u22121, ct\u22121 and produces\nht, ct via the following calculations:\nit = \u03c3(W ixt + U iht\u22121 + b i)\nft = \u03c3(W fxt + U fht\u22121 + b f ) ot = \u03c3(W oxt + U oht\u22121 + b o) c\u0302t = tanh(W cxt + U cht\u22121 + b c)\nct = ft ct\u22121 + it c\u0302t ht = ot tanh(ct)\nwhere \u03c3, are respectively the element-wise sigmoid function and multiplication, it, ft,ot, ct are respectively the input gate, forget gate, output gate and memory cell activation vector at time t, all of which have the same size as hidden state vector ht \u2208 RH .\nLink Score. LSTMs have been shown to outperform RNNs on many NLP tasks, notably language modeling (Sundermeyer et al., 2012).\nIn our model, LSTM is utilized to chain together word candidates in a left-to-right, incremental manner. At time step t, a prediction pt+1 \u2208 Rd about next word yt+1 is made based on the hidden state ht:\npt+1 = tanh(W pht + b p)\nlink score for next word yt+1 is then computed as:\nLink Score(yt+1) = pt+1 \u00b7 yt+1 (3)\nDue to the structure of LSTM, the prediction vector pt+1 carries useful information detected from the entire segmentation history, including previous segmentation decisions. In this way, our model gains the ability of sequence-level discrimination rather than local optimization."}, {"heading": "3.3 Sentence score", "text": "Sentence score for a segmented sentence y with n word candidates is computed by summing up word scores (2) and link scores (3) as follow:\ns(y[1:n], \u03b8) = n\u2211 t=1 (u \u00b7 yt + pt \u00b7 yt) (4)\nwhere \u03b8 is the parameter set used in our model."}, {"heading": "4 Decoding", "text": "The total number of possible segmented sentences grows exponentially with the length of character sequence, which makes it impractical to compute the scores of every possible segmentation. In order to get exact inference, most sequence-labeling systems address this problem with a Viterbi search, which takes the advantage of their hypothesis that the tag interactions only exist within adjacent characters (Markov assumption). However, since our model is intended to capture complete history of segmentation decisions, such dynamic programming algorithms can not be adopted in this situation.\nAlgorithm 1 Beam Search. Input: Parameters \u03b8, beam size k, maximum\nword lengthw and input character sequence c[1 : n]\nOutput: Approx. k best segmentations 1: \u03c0[0]\u2190 {(score = 0,h = h0, c = c0)} 2: for i = 1 to n do 3: . Generate Candidate Word Vectors 4: X \u2190 \u2205 5: for j = max(1, i\u2212 w) to i do 6: w = GCNN-Procedure(c[j : i]) 7: X.add((index = j \u2212 1, word = w)) 8: end for 9: . Join Segmentation 10: Y \u2190 { y.append(x) | y \u2208 \u03c0[x.index] and x \u2208 X} 11: . Filter k-Max 12: \u03c0[i]\u2190 k- arg max\ny\u2208Y y.score\n13: end for 14: return \u03c0[n]\nTo make our model efficient in practical use, we propose a beam-search algorithm with dynamic programming motivations as shown in Algorithm 1. The main idea is that any segmentation of the first i characters can be separated as two parts, the first part consists of characters with indexes from 0 to j that is denoted as y, the rest part is the word composed by c[j+1 : i]. The influence from previous segmentation y can be represented as a triple (y.score, y.h, y.c), where y.score, y.h, y.c indicate the current score, current hidden state vector and current memory cell vector respectively. Beam search ensures that the total time for segmenting a sentence of n characters is w \u00d7 k \u00d7 n,\nwhere w, k are maximum word length and beam size respectively."}, {"heading": "5 Training", "text": "We use the max-margin criterion (Taskar et al., 2005) to train our model. As reported in (Kummerfeld et al., 2015), the margin methods generally outperform both likelihood and perception methods. For a given character sequence x(i), denote the correct segmented sentence for x(i) as y(i). We define a structured margin loss \u2206(y(i), y\u0302) for predicting a segmented sentence y\u0302:\n\u2206(y(i), y\u0302) = m\u2211 t=1 \u00b51{y(i),t 6= y\u0302t}\nwherem is the length of sequence x(i) and \u00b5 is the discount parameter. The calculation of margin loss could be regarded as to count the number of incorrectly segmented characters and then multiple it with a fixed discount parameter for smoothing. Therefore, the loss is proportional to the number of incorrectly segmented characters.\nGiven a set of training set\u2126, the regularized objective function is the loss function J(\u03b8) including an `2 norm term:\nJ(\u03b8) = 1 |\u2126| \u2211\n(x(i),y(i))\u2208\u2126\nli(\u03b8) + \u03bb\n2 ||\u03b8||22\nli(\u03b8) = max y\u0302\u2208GEN(x(i))\n(s(y\u0302, \u03b8) + \u2206(y(i), y\u0302)\u2212 s(y(i), \u03b8))\nwhere the function s(\u00b7) is the sentence score defined in equation (4).\nDue to the hinge loss, the objective function is not differentiable, we use a subgradient method (Ratliff et al., 2007) which computes a gradientlike direction. Following (Socher et al., 2013), we use the diagonal variant of AdaGrad (Duchi et al., 2011) with minibatchs to minimize the objective. The update for the i-th parameter at time step t is as follows:\n\u03b8t,i = \u03b8t\u22121,i \u2212 \u03b1\u221a\u2211t \u03c4=1 g 2 \u03c4,i gt,i\nwhere \u03b1 is the initial learning rate and g\u03c4,i \u2208 R|\u03b8i| is the subgradient at time step \u03c4 for parameter \u03b8i."}, {"heading": "6 Experiments", "text": ""}, {"heading": "6.1 Datasets", "text": "To evaluate the proposed segmenter, we use two popular datasets, PKU and MSR, from the second\nInternational Chinese Word Segmentation Bakeoff (Emerson, 2005). These datasets are commonly used by previous state-of-the-art models and neural network models.\nBoth datasets are preprocessed by replacing the continuous English characters and digits with a unique token. All experiments are conducted with standard Bakeoff scoring program1 calculating precision, recall, and F1-score."}, {"heading": "6.2 Hyper-parameters", "text": "Hyper-parameters of neural network model significantly impact on its performance. To determine a set of suitable hyper-parameters, we divide the training data into two sets, the first 90% sentences as training set and the rest 10% sentences as development set. We choose the hyper-parameters as shown in Table 2.\nWe found that the character embedding size has a limited impact on the performance as long as it is large enough. The size 50 is chosen as a good trade-off between speed and performance. The number of hidden units is set to be the same as the character embedding. Maximum word length determines the number of parameters in GCNN part and the time consuming of beam search, since the words with a length l > 4 are relatively rare, 0.29% in PKU training data and 1.25% in MSR training data, we set the maximum word length to 4 in our experiments.2\nDropout is a popular technique for improving the performance of neural networks by reducing overfitting (Srivastava et al., 2014). We also drop the input layer of our model with dropout rate 20% to avoid overfitting.\n1http://www.sighan.org/bakeoff2003/score 2This 4-character limitation is just for consistence for both datasets. We are aware that this is a too strict setting, especially which makes additional performance loss in a dataset with larger average word length, i.e., MSR."}, {"heading": "6.3 Model Analysis", "text": "Beam Size. We first investigated the impact of beam size over segmentation performance. Figure 5 shows that a segmenter with beam size 4 is enough to get the best performance, which makes our model find a good balance between accuracy and efficiency.\nGCNN. We then studied the role of GCNN in our model. To reveal the impact of GCNN, we re-implemented a simplified version of our model, which replaces the GCNN part with a single nonlinear layer as in equation (1). The results are listed in Table 3, which demonstrate that the performance is significantly boosted by exploiting the GCNN architecture (94.0% to 95.5% on F1score), while the best performance that the simplified version can achieve is 94.7%, but using a much larger character embedding size.\nLink Score & Word Score. We conducted several experiments to investigate the individual effect of link score and word score, since these two types of scores are intended to estimate the\nsentence likelihood from two different perspectives: the semantic coherence between words and the existence of individual words. The learning curves of models with different scoring strategies are shown in Figure 6.\nThe model with only word score can be regarded as the situation that the segmentation decisions are made only based on local window information. The comparisons show that such a model gives moderate performance. By contrast, the model with only link score gives a much better performance close to the joint model, which demonstrates that the complete segmentation history, which can not be effectively modeled in previous schemes, possesses huge appliance value for word segmentation."}, {"heading": "6.4 Results", "text": "We first compare our model with the latest neural network methods as shown in Table 4. However, (Chen et al., 2015a; Chen et al., 2015b) used an extra preprocess to filter out Chinese idioms according to an external dictionary.3 Table 4 lists the results (F1-scores) with different dictionaries, which show that our models perform better when under the same settings.\n3In detail, when a dictionary is used, a preprocess is performed before training and test, which scans original text to find out Chinese idioms included in the dictionary and replace them with a unique token. This treatment does not strictly follow the convention of closed-set setting defined by SIGHAN Bakeoff, as no linguistic resources, either dictionary or corpus, other than the training corpus, should be adopted.\n4The dictionary used in (Chen et al., 2015a; Chen et al., 2015b) is neither publicly released nor specified the exact source until now. We have to re-run their code using our selected dictionary to make a fair comparison. Our dictionary has been submitted along with this submission.\nTable 5 gives comparisons among previous neural network models. In the first block of Table 5, the character embedding matrix M is randomly initialized. The results show that our proposed novel model outperforms previous neural network methods.\nPrevious works have found that the performance can be improved by pre-training the character embeddings on large unlabeled data. Therefore, we use word2vec (Mikolov et al., 2013) toolkit6 to pre-train the character embeddings on the Chinese Wikipedia corpus and use them for initialization. Table 5 also shows the results with additional pre-trained character embeddings. Again, our model achieves better performance than previous neural network models do.\nTable 6 compares our models with previous state-of-the-art systems. Recent systems such as (Zhang et al., 2013), (Chen et al., 2015b) and (Chen et al., 2015a) rely on both extensive feature engineering and external corpora to boost performance. Such systems are not directly compara-\nble with our models. In the closed-set setting, our models can achieve state-of-the-art performance on PKU dataset but a competitive result on MSR dataset, which can attribute to too strict maximum word length setting for consistence as it is well known that MSR corpus has a much longer average word length (Zhao et al., 2010). In later experiments. we found the performance can indeed be further improved by allowing longer words. (e.g., if 6-char words allowed, F1 score on MSR will be furthermore improved 0.3%.)\nFor the running cost, we roughly report the current computation consuming on PKU dataset.7 It takes about two days to finish 50 training epochs (for results in Figure 6 and the last row of Table 6) only with two cores of an Intel i7-5960X CPU. The requirement for RAM during training is less than 800MB. The trained model can be saved within 4MB on the hard disk."}, {"heading": "7 Related Work", "text": "Neural Network Models. Most modern CWS methods followed (Xue, 2003) treated CWS as a sequence labeling problems (Zhao et al., 2006b). Recently, researchers have tended to explore neural network based approaches (Collobert et al., 2011) to reduce efforts of feature engineering (Zheng et al., 2013; Qi et al., 2014; Chen et al., 2015a; Chen et al., 2015b). They modeled CWS\n5To make comparisons fair, we re-run their code (https://github.com/dalstonChen) without their unspecified Chinese idiom dictionary.\n6http://code.google.com/p/word2vec/ 7Our code is released at https://github.com/jcyk/CWS.\nas tagging problem as well, scoring tags on individual characters. In those models, tag scores are decided by context information within local windows and the sentence-level score is obtained via context-independently tag transitions. Pei et al. (2014) introduced the tag embedding as input to capture the combinations of context and tag history. However, in previous works, only the tag of previous one character was taken into consideration though theoretically the complete history of actions taken by the segmenter should be considered.\nAlternatives to Sequence Labeling. Besides sequence labeling schemes, Zhang and Clark (2007) proposed a word-based perceptron method. Zhang et al. (2012) used a linear-time incremental model which can also benefits from various kinds of features including word-based features. But both of them rely heavily on massive handcrafted features.\nAnother notable exception is (Ma and Hinrichs, 2015), which is also an embedding-based model, but models CWS as configuration-action matching. However, again, this method only uses the context information within limited sized windows.\nAt the same time of this work, some other neural models (Zhang et al., 2016a; Liu et al., 2016) have been proposed, which can also leverage word-level information but are quite different from ours in the basic framework.\nOther Techniques. The proposed model might furthermore benefit from some techniques in recent state-of-the-art systems, such as semisupervised learning (Zhao and Kit, 2008b; Zhao and Kit, 2008a; Sun and Xu, 2011; Zhao and Kit, 2011; Zeng et al., 2013; Zhang et al., 2013), incorporating global information (Zhao and Kit, 2007; Zhang et al., 2016b), and joint models (Qian and Liu, 2012; Li and Zhou, 2012)."}, {"heading": "8 Conclusion", "text": "This paper presents a novel neural framework for the task of Chinese word segmentation, which contains three main components: (1) a factory to produce word representation when given its governed characters; (2) a sentence-level likelihood evaluation system for segmented sentence; (3) an efficient and effective algorithm to find the best segmentation.\nThe proposed framework makes a latest attempt\nto formalize word segmentation as a direct structured learning procedure in terms of the recent distributed representation framework.\nThough our system outputs results that are better than the latest neural network segmenters but comparable to all previous state-of-the-art systems, the framework remains a great of potential that can be further investigated and improved in the future."}], "references": [{"title": "A neural probabilistic language model", "author": ["Yoshua Bengio", "R\u00e9jean Ducharme", "Pascal Vincent", "Christian Janvin."], "venue": "The Journal of Machine Learning Research, 3:1137\u20131155.", "citeRegEx": "Bengio et al\\.,? 2003", "shortCiteRegEx": "Bengio et al\\.", "year": 2003}, {"title": "A maximum entropy approach to natural language processing", "author": ["Adam L Berger", "Vincent J Della Pietra", "Stephen A Della Pietra."], "venue": "Computational linguistics, 22(1):39\u201371.", "citeRegEx": "Berger et al\\.,? 1996", "shortCiteRegEx": "Berger et al\\.", "year": 1996}, {"title": "Gated recursive neural network for chinese word segmentation", "author": ["Xinchi Chen", "Xipeng Qiu", "Chenxi Zhu", "Xuanjing Huang."], "venue": "Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint", "citeRegEx": "Chen et al\\.,? 2015a", "shortCiteRegEx": "Chen et al\\.", "year": 2015}, {"title": "Long short-term memory neural networks for chinese word segmentation", "author": ["Xinchi Chen", "Xipeng Qiu", "Chenxi Zhu", "Pengfei Liu", "Xuanjing Huang."], "venue": "Proceedings of the 2015 Conference on Empirical Methods in Natural Language Process-", "citeRegEx": "Chen et al\\.,? 2015b", "shortCiteRegEx": "Chen et al\\.", "year": 2015}, {"title": "Learning phrase representations using rnn encoder\u2013decoder for statistical machine translation", "author": ["Kyunghyun Cho", "Bart van Merrienboer", "Caglar Gulcehre", "Dzmitry Bahdanau", "Fethi Bougares", "Holger Schwenk", "Yoshua Bengio."], "venue": "Proceedings of", "citeRegEx": "Cho et al\\.,? 2014", "shortCiteRegEx": "Cho et al\\.", "year": 2014}, {"title": "Empirical evaluation of gated recurrent neural networks on sequence modeling", "author": ["Junyoung Chung", "Caglar Gulcehre", "KyungHyun Cho", "Yoshua Bengio."], "venue": "arXiv preprint arXiv:1412.3555.", "citeRegEx": "Chung et al\\.,? 2014", "shortCiteRegEx": "Chung et al\\.", "year": 2014}, {"title": "Natural language processing (almost) from scratch", "author": ["Ronan Collobert", "Jason Weston", "L\u00e9on Bottou", "Michael Karlen", "Koray Kavukcuoglu", "Pavel Kuksa."], "venue": "The Journal of Machine Learning Research, 12:2493\u20132537.", "citeRegEx": "Collobert et al\\.,? 2011", "shortCiteRegEx": "Collobert et al\\.", "year": 2011}, {"title": "Adaptive subgradient methods for online learning and stochastic optimization", "author": ["John Duchi", "Elad Hazan", "Yoram Singer."], "venue": "The Journal of Machine Learning Research, 12:2121\u20132159.", "citeRegEx": "Duchi et al\\.,? 2011", "shortCiteRegEx": "Duchi et al\\.", "year": 2011}, {"title": "The second international chinese word segmentation bakeoff", "author": ["Thomas Emerson."], "venue": "Proceedings of the fourth SIGHAN workshop on Chinese language Processing, volume 133.", "citeRegEx": "Emerson.,? 2005", "shortCiteRegEx": "Emerson.", "year": 2005}, {"title": "Long short-term memory", "author": ["Sepp Hochreiter", "J\u00fcrgen Schmidhuber."], "venue": "Neural computation, 9(8):1735\u20131780.", "citeRegEx": "Hochreiter and Schmidhuber.,? 1997", "shortCiteRegEx": "Hochreiter and Schmidhuber.", "year": 1997}, {"title": "Which is essential for chinese word segmentation: Character versus word", "author": ["Chang-Ning Huang", "Hai Zhao."], "venue": "The 20th Pacific Asia Conference on Language, Information and Computation, pages 1\u201312.", "citeRegEx": "Huang and Zhao.,? 2006", "shortCiteRegEx": "Huang and Zhao.", "year": 2006}, {"title": "Character-aware neural language models", "author": ["Yoon Kim", "Yacine Jernite", "David Sontag", "Alexander M Rush."], "venue": "arXiv preprint arXiv:1508.06615.", "citeRegEx": "Kim et al\\.,? 2015", "shortCiteRegEx": "Kim et al\\.", "year": 2015}, {"title": "An empirical analysis of optimization for max-margin nlp", "author": ["Jonathan K. Kummerfeld", "Taylor Berg-Kirkpatrick", "Dan Klein."], "venue": "Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 273\u2013279.", "citeRegEx": "Kummerfeld et al\\.,? 2015", "shortCiteRegEx": "Kummerfeld et al\\.", "year": 2015}, {"title": "Conditional random fields: Probabilistic models for segmenting and labeling sequence data", "author": ["John Lafferty", "Andrew McCallum", "Fernando CN Pereira."], "venue": "Proceedings of the Eighteenth Interntional Conference on Machine Learning.", "citeRegEx": "Lafferty et al\\.,? 2001", "shortCiteRegEx": "Lafferty et al\\.", "year": 2001}, {"title": "Unified dependency parsing of chinese morphological and syntactic structures", "author": ["Zhongguo Li", "Guodong Zhou."], "venue": "Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Lan-", "citeRegEx": "Li and Zhou.,? 2012", "shortCiteRegEx": "Li and Zhou.", "year": 2012}, {"title": "Exploring segment representations for neural segmentation models", "author": ["Yijia Liu", "Wanxiang Che", "Jiang Guo", "Bing Qin", "Ting Liu."], "venue": "arXiv preprint arXiv:1604.05499.", "citeRegEx": "Liu et al\\.,? 2016", "shortCiteRegEx": "Liu et al\\.", "year": 2016}, {"title": "A maximum entropy approach to chinese word segmentation", "author": ["Jin Kiat Low", "Hwee Tou Ng", "Wenyuan Guo."], "venue": "Proceedings of the Fourth SIGHAN Workshop on Chinese Language Processing, volume 1612164, pages 448\u2013455.", "citeRegEx": "Low et al\\.,? 2005", "shortCiteRegEx": "Low et al\\.", "year": 2005}, {"title": "Accurate linear-time chinese word segmentation via embedding matching", "author": ["Jianqiang Ma", "Erhard Hinrichs."], "venue": "Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference", "citeRegEx": "Ma and Hinrichs.,? 2015", "shortCiteRegEx": "Ma and Hinrichs.", "year": 2015}, {"title": "Recurrent neural network based language model", "author": ["Tomas Mikolov", "Martin Karafi\u00e1t", "Lukas Burget", "Jan Cernock\u1ef3", "Sanjeev Khudanpur."], "venue": "11th Annual Conference of the International Speech Communication Association, pages 1045\u20131048.", "citeRegEx": "Mikolov et al\\.,? 2010", "shortCiteRegEx": "Mikolov et al\\.", "year": 2010}, {"title": "Efficient estimation of word representations in vector space", "author": ["Tomas Mikolov", "Kai Chen", "Greg Corrado", "Jeffrey Dean."], "venue": "arXiv preprint arXiv:1301.3781.", "citeRegEx": "Mikolov et al\\.,? 2013", "shortCiteRegEx": "Mikolov et al\\.", "year": 2013}, {"title": "Maxmargin tensor neural network for chinese word segmentation", "author": ["Wenzhe Pei", "Tao Ge", "Baobao Chang."], "venue": "Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 293\u2013303.", "citeRegEx": "Pei et al\\.,? 2014", "shortCiteRegEx": "Pei et al\\.", "year": 2014}, {"title": "Chinese segmentation and new word detection using conditional random fields", "author": ["Fuchun Peng", "Fangfang Feng", "Andrew McCallum."], "venue": "Proceedings of the 20th international conference on Computational Linguistics, page 562.", "citeRegEx": "Peng et al\\.,? 2004", "shortCiteRegEx": "Peng et al\\.", "year": 2004}, {"title": "Deep learning for character-based information extraction", "author": ["Yanjun Qi", "Sujatha G Das", "Ronan Collobert", "Jason Weston."], "venue": "Advances in Information Retrieval, pages 668\u2013674.", "citeRegEx": "Qi et al\\.,? 2014", "shortCiteRegEx": "Qi et al\\.", "year": 2014}, {"title": "Joint chinese word segmentation, pos tagging and parsing", "author": ["Xian Qian", "Yang Liu."], "venue": "Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning, pages 501\u2013", "citeRegEx": "Qian and Liu.,? 2012", "shortCiteRegEx": "Qian and Liu.", "year": 2012}, {"title": "approximate) subgradient methods for structured prediction", "author": ["Nathan D Ratliff", "J Andrew Bagnell", "Martin Zinkevich."], "venue": "International Conference on Artificial Intelligence and Statistics, pages 380\u2013 387.", "citeRegEx": "Ratliff et al\\.,? 2007", "shortCiteRegEx": "Ratliff et al\\.", "year": 2007}, {"title": "Parsing with compositional vector grammars", "author": ["Richard Socher", "John Bauer", "Christopher D. Manning", "Ng Andrew Y."], "venue": "Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 455\u2013465.", "citeRegEx": "Socher et al\\.,? 2013", "shortCiteRegEx": "Socher et al\\.", "year": 2013}, {"title": "Dropout: A simple way to prevent neural networks from overfitting", "author": ["Nitish Srivastava", "Geoffrey Hinton", "Alex Krizhevsky", "Ilya Sutskever", "Ruslan Salakhutdinov."], "venue": "The Journal of Machine Learning Research, 15(1):1929\u20131958.", "citeRegEx": "Srivastava et al\\.,? 2014", "shortCiteRegEx": "Srivastava et al\\.", "year": 2014}, {"title": "Enhancing chinese word segmentation using unlabeled data", "author": ["Weiwei Sun", "Jia Xu."], "venue": "Proceedings of the Conference on Empirical Methods in Natural Language Processing, pages 970\u2013979.", "citeRegEx": "Sun and Xu.,? 2011", "shortCiteRegEx": "Sun and Xu.", "year": 2011}, {"title": "A discriminative latent variable chinese segmenter with hybrid word/character information", "author": ["Xu Sun", "Yaozhong Zhang", "Takuya Matsuzaki", "Yoshimasa Tsuruoka", "Jun\u2019ichi Tsujii"], "venue": "In Proceedings of Human Language Technologies:", "citeRegEx": "Sun et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Sun et al\\.", "year": 2009}, {"title": "Fast online training with frequency-adaptive learning rates for chinese word segmentation and new word detection", "author": ["Xu Sun", "Houfeng Wang", "Wenjie Li."], "venue": "Proceedings of the 50th Annual Meeting of the Association for Computational Linguis-", "citeRegEx": "Sun et al\\.,? 2012", "shortCiteRegEx": "Sun et al\\.", "year": 2012}, {"title": "Lstm neural networks for language modeling", "author": ["Martin Sundermeyer", "Ralf Schl\u00fcter", "Hermann Ney."], "venue": "13th Annual Conference of the International Speech Communication Association.", "citeRegEx": "Sundermeyer et al\\.,? 2012", "shortCiteRegEx": "Sundermeyer et al\\.", "year": 2012}, {"title": "Learning structured prediction models: A large margin approach", "author": ["Ben Taskar", "Vassil Chatalbashev", "Daphne Koller", "Carlos Guestrin."], "venue": "Proceedings of the 22nd international conference on Machine learning, pages 896\u2013903.", "citeRegEx": "Taskar et al\\.,? 2005", "shortCiteRegEx": "Taskar et al\\.", "year": 2005}, {"title": "A conditional random field word segmenter for sighan bakeoff 2005", "author": ["Huihsin Tseng", "Pichuan Chang", "Galen Andrew", "Daniel Jurafsky", "Christopher Manning."], "venue": "Proceedings of the fourth SIGHAN workshop on Chinese language Processing, volume", "citeRegEx": "Tseng et al\\.,? 2005", "shortCiteRegEx": "Tseng et al\\.", "year": 2005}, {"title": "Learning distributed word representations for bidirectional lstm recurrent neural network", "author": ["Peilu Wang", "Yao Qian", "Hai Zhao", "Frank K. Soong", "Lei He", "Ke Wu."], "venue": "Proceedings of the 2016 Conference of the North American Chapter of the Association", "citeRegEx": "Wang et al\\.,? 2016", "shortCiteRegEx": "Wang et al\\.", "year": 2016}, {"title": "Chinese word segmentation as character tagging", "author": ["Nianwen Xue."], "venue": "Computational Linguistics and Chinese Language Processing, 8(1):29\u201348.", "citeRegEx": "Xue.,? 2003", "shortCiteRegEx": "Xue.", "year": 2003}, {"title": "Graph-based semi-supervised model for joint chinese word segmentation and partof-speech tagging", "author": ["Xiaodong Zeng", "Derek F. Wong", "Lidia S. Chao", "Isabel Trancoso."], "venue": "Proceedings of the 51st Annual Meeting of the Association for Computational", "citeRegEx": "Zeng et al\\.,? 2013", "shortCiteRegEx": "Zeng et al\\.", "year": 2013}, {"title": "Chinese segmentation with a word-based perceptron algorithm", "author": ["Yue Zhang", "Stephen Clark."], "venue": "Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 840\u2013 847.", "citeRegEx": "Zhang and Clark.,? 2007", "shortCiteRegEx": "Zhang and Clark.", "year": 2007}, {"title": "Word segmentation on chinese mirco-blog data with a linear-time incremental model", "author": ["Kaixu Zhang", "Maosong Sun", "Changle Zhou."], "venue": "Second CIPSSIGHAN Joint Conference on Chinese Language Processing, pages 41\u201346.", "citeRegEx": "Zhang et al\\.,? 2012", "shortCiteRegEx": "Zhang et al\\.", "year": 2012}, {"title": "Exploring representations from unlabeled data with co-training for Chinese word segmentation", "author": ["Longkai Zhang", "Houfeng Wang", "Xu Sun", "Mairgup Mansur."], "venue": "Proceedings of the 2013 Conference on Empirical Methods in Natural Language Pro-", "citeRegEx": "Zhang et al\\.,? 2013", "shortCiteRegEx": "Zhang et al\\.", "year": 2013}, {"title": "Transition-based neural word segmentation", "author": ["Meishan Zhang", "Yue Zhang", "Guohong Fu."], "venue": "Proceedings of the 54nd Annual Meeting of the Association for Computational Linguistics.", "citeRegEx": "Zhang et al\\.,? 2016a", "shortCiteRegEx": "Zhang et al\\.", "year": 2016}, {"title": "Probabilistic graph-based dependency parsing with convolutional neural network", "author": ["Zhiong Zhang", "Hai Zhao", "Lianhui Qin."], "venue": "Proceedings of the 54nd Annual Meeting of the Association for Computational Linguistics.", "citeRegEx": "Zhang et al\\.,? 2016b", "shortCiteRegEx": "Zhang et al\\.", "year": 2016}, {"title": "Incorporating global information into supervised learning for chinese word segmentation", "author": ["Hai Zhao", "Chunyu Kit."], "venue": "Proceedings of the 10th Conference of the Pacific Association for Computational Linguistics, pages 66\u201374.", "citeRegEx": "Zhao and Kit.,? 2007", "shortCiteRegEx": "Zhao and Kit.", "year": 2007}, {"title": "Exploiting unlabeled text with different unsupervised segmentation criteria for chinese word segmentation", "author": ["Hai Zhao", "Chunyu Kit."], "venue": "Research in Computing Science, 33:93\u2013104.", "citeRegEx": "Zhao and Kit.,? 2008a", "shortCiteRegEx": "Zhao and Kit.", "year": 2008}, {"title": "Unsupervised segmentation helps supervised learning of character tagging for word segmentation and named entity recognition", "author": ["Hai Zhao", "Chunyu Kit."], "venue": "Proceedings of the Third International Joint Conference on Natural Language Pro-", "citeRegEx": "Zhao and Kit.,? 2008b", "shortCiteRegEx": "Zhao and Kit.", "year": 2008}, {"title": "Integrating unsupervised and supervised word segmentation: The role of goodness measures", "author": ["Hai Zhao", "Chunyu Kit."], "venue": "Information Sciences, 181(1):163\u2013183.", "citeRegEx": "Zhao and Kit.,? 2011", "shortCiteRegEx": "Zhao and Kit.", "year": 2011}, {"title": "An improved chinese word segmentation system with conditional random field", "author": ["Hai Zhao", "Chang-Ning Huang", "Mu Li."], "venue": "Proceedings of the Fifth SIGHAN Workshop on Chinese Language Processing, volume 1082117.", "citeRegEx": "Zhao et al\\.,? 2006a", "shortCiteRegEx": "Zhao et al\\.", "year": 2006}, {"title": "Effective tag set selection in chinese word segmentation via conditional random field modeling", "author": ["Hai Zhao", "Chang-Ning Huang", "Mu Li", "Bao-Liang Lu."], "venue": "Proceedings of the 9th Pacific Association for Computational Linguistics, volume 20,", "citeRegEx": "Zhao et al\\.,? 2006b", "shortCiteRegEx": "Zhao et al\\.", "year": 2006}, {"title": "A unified character-based tagging framework for chinese word segmentation", "author": ["Hai Zhao", "Chang-Ning Huang", "Mu Li", "Bao-Liang Lu."], "venue": "ACM Transactions on Asian Language Information Processing, 9(2):5.", "citeRegEx": "Zhao et al\\.,? 2010", "shortCiteRegEx": "Zhao et al\\.", "year": 2010}, {"title": "Deep learning for Chinese word segmentation and POS tagging", "author": ["Xiaoqing Zheng", "Hanyang Chen", "Tianyu Xu."], "venue": "Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 647\u2013657.", "citeRegEx": "Zheng et al\\.,? 2013", "shortCiteRegEx": "Zheng et al\\.", "year": 2013}], "referenceMentions": [{"referenceID": 34, "context": "Since Xue (2003), most methods formalize the Chinese word segmentation (CWS) as a sequence labeling problem with character position tags, which can be handled with su-", "startOffset": 6, "endOffset": 17}, {"referenceID": 1, "context": "pervised learning methods such as Maximum Entropy (Berger et al., 1996; Low et al., 2005) and Conditional Random Fields (Lafferty et al.", "startOffset": 50, "endOffset": 89}, {"referenceID": 16, "context": "pervised learning methods such as Maximum Entropy (Berger et al., 1996; Low et al., 2005) and Conditional Random Fields (Lafferty et al.", "startOffset": 50, "endOffset": 89}, {"referenceID": 13, "context": ", 2005) and Conditional Random Fields (Lafferty et al., 2001; Peng et al., 2004; Zhao et al., 2006a).", "startOffset": 38, "endOffset": 100}, {"referenceID": 21, "context": ", 2005) and Conditional Random Fields (Lafferty et al., 2001; Peng et al., 2004; Zhao et al., 2006a).", "startOffset": 38, "endOffset": 100}, {"referenceID": 45, "context": ", 2005) and Conditional Random Fields (Lafferty et al., 2001; Peng et al., 2004; Zhao et al., 2006a).", "startOffset": 38, "endOffset": 100}, {"referenceID": 6, "context": "(2013) adapted the general neural network architecture for sequence labeling proposed in (Collobert et al., 2011), and used character embeddings as input to a two-layer network.", "startOffset": 89, "endOffset": 113}, {"referenceID": 48, "context": "(2014) improved upon (Zheng et al., 2013) by explicitly modeling the interactions between local context and previous tag.", "startOffset": 21, "endOffset": 41}, {"referenceID": 10, "context": "Moreover, word-level information, which is being the greater granularity unit as suggested in (Huang and Zhao, 2006), remains ar X iv :1 60 6.", "startOffset": 94, "endOffset": 116}, {"referenceID": 1, "context": "pervised learning methods such as Maximum Entropy (Berger et al., 1996; Low et al., 2005) and Conditional Random Fields (Lafferty et al., 2001; Peng et al., 2004; Zhao et al., 2006a). However, those methods heavily depend on the choice of handcrafted features. Recently, neural models have been widely used for NLP tasks for their ability to minimize the effort in feature engineering. For the task of CWS, Zheng et al. (2013) adapted the general neural network architecture for sequence labeling proposed in (Collobert et al.", "startOffset": 51, "endOffset": 427}, {"referenceID": 1, "context": "pervised learning methods such as Maximum Entropy (Berger et al., 1996; Low et al., 2005) and Conditional Random Fields (Lafferty et al., 2001; Peng et al., 2004; Zhao et al., 2006a). However, those methods heavily depend on the choice of handcrafted features. Recently, neural models have been widely used for NLP tasks for their ability to minimize the effort in feature engineering. For the task of CWS, Zheng et al. (2013) adapted the general neural network architecture for sequence labeling proposed in (Collobert et al., 2011), and used character embeddings as input to a two-layer network. Pei et al. (2014) improved upon (Zheng et al.", "startOffset": 51, "endOffset": 616}, {"referenceID": 1, "context": "pervised learning methods such as Maximum Entropy (Berger et al., 1996; Low et al., 2005) and Conditional Random Fields (Lafferty et al., 2001; Peng et al., 2004; Zhao et al., 2006a). However, those methods heavily depend on the choice of handcrafted features. Recently, neural models have been widely used for NLP tasks for their ability to minimize the effort in feature engineering. For the task of CWS, Zheng et al. (2013) adapted the general neural network architecture for sequence labeling proposed in (Collobert et al., 2011), and used character embeddings as input to a two-layer network. Pei et al. (2014) improved upon (Zheng et al., 2013) by explicitly modeling the interactions between local context and previous tag. Chen et al. (2015a) proposed a gated recursive neural network to model the feature combinations of context characters.", "startOffset": 51, "endOffset": 751}, {"referenceID": 1, "context": "pervised learning methods such as Maximum Entropy (Berger et al., 1996; Low et al., 2005) and Conditional Random Fields (Lafferty et al., 2001; Peng et al., 2004; Zhao et al., 2006a). However, those methods heavily depend on the choice of handcrafted features. Recently, neural models have been widely used for NLP tasks for their ability to minimize the effort in feature engineering. For the task of CWS, Zheng et al. (2013) adapted the general neural network architecture for sequence labeling proposed in (Collobert et al., 2011), and used character embeddings as input to a two-layer network. Pei et al. (2014) improved upon (Zheng et al., 2013) by explicitly modeling the interactions between local context and previous tag. Chen et al. (2015a) proposed a gated recursive neural network to model the feature combinations of context characters. Chen et al. (2015b) used an LSTM architecture to capture potential long-distance dependencies, which alleviates the limitation of the size of context window but introduced another window for hidden states.", "startOffset": 51, "endOffset": 870}, {"referenceID": 48, "context": "character based (Zheng et al., 2013), .", "startOffset": 16, "endOffset": 36}, {"referenceID": 3, "context": "ci\u22122, ci\u22121, ci, ci+1, ci+2 ti\u22121ti (Chen et al., 2015b) c0, c1, .", "startOffset": 34, "endOffset": 54}, {"referenceID": 36, "context": "word based (Zhang and Clark, 2007), .", "startOffset": 11, "endOffset": 34}, {"referenceID": 0, "context": "To feature a segmented sentence, a series of distributed vector representations (Bengio et al., 2003) are generated to characterize the corresponding word candidates.", "startOffset": 80, "endOffset": 101}, {"referenceID": 9, "context": "Third, to evaluate how a segmented sentence makes sense through word interacting, an LSTM (Hochreiter and Schmidhuber, 1997) is used to chain together word candidates incrementally and construct the representation of partially segmented sentence at each decoding step, so that the coherence between next word candidate and previous segmentation history can be depicted.", "startOffset": 90, "endOffset": 124}, {"referenceID": 0, "context": "While the scores are decided at the word-level, using word embedding (Bengio et al., 2003; Wang et al., 2016) immediately will lead to a remarkable issue that rare words and out-of-vocabulary words will be poorly estimated (Kim et al.", "startOffset": 69, "endOffset": 109}, {"referenceID": 33, "context": "While the scores are decided at the word-level, using word embedding (Bengio et al., 2003; Wang et al., 2016) immediately will lead to a remarkable issue that rare words and out-of-vocabulary words will be poorly estimated (Kim et al.", "startOffset": 69, "endOffset": 109}, {"referenceID": 11, "context": ", 2016) immediately will lead to a remarkable issue that rare words and out-of-vocabulary words will be poorly estimated (Kim et al., 2015).", "startOffset": 121, "endOffset": 139}, {"referenceID": 2, "context": "Gated structure in neural network can be useful for hybrid feature extraction according to (Chen et al., 2015a; Chung et al., 2014; Cho et al., 2014), we therefore propose a gated combination neural network (GCNN) especially for character com-", "startOffset": 91, "endOffset": 149}, {"referenceID": 5, "context": "Gated structure in neural network can be useful for hybrid feature extraction according to (Chen et al., 2015a; Chung et al., 2014; Cho et al., 2014), we therefore propose a gated combination neural network (GCNN) especially for character com-", "startOffset": 91, "endOffset": 149}, {"referenceID": 4, "context": "Gated structure in neural network can be useful for hybrid feature extraction according to (Chen et al., 2015a; Chung et al., 2014; Cho et al., 2014), we therefore propose a gated combination neural network (GCNN) especially for character com-", "startOffset": 91, "endOffset": 149}, {"referenceID": 18, "context": "Inspired by the recurrent neural network language model (RNN-LM) (Mikolov et al., 2010; Sundermeyer et al., 2012), we utilize an LSTM system to capture the coherence in a segmented sentence.", "startOffset": 65, "endOffset": 113}, {"referenceID": 30, "context": "Inspired by the recurrent neural network language model (RNN-LM) (Mikolov et al., 2010; Sundermeyer et al., 2012), we utilize an LSTM system to capture the coherence in a segmented sentence.", "startOffset": 65, "endOffset": 113}, {"referenceID": 9, "context": "The LSTM neural network (Hochreiter and Schmidhuber, 1997) is an extension of the recurrent neural network (RNN), which is an effective tool for sequence modeling tasks using its hidden states for history information preservation.", "startOffset": 24, "endOffset": 58}, {"referenceID": 30, "context": "LSTMs have been shown to outperform RNNs on many NLP tasks, notably language modeling (Sundermeyer et al., 2012).", "startOffset": 86, "endOffset": 112}, {"referenceID": 31, "context": "We use the max-margin criterion (Taskar et al., 2005) to train our model.", "startOffset": 32, "endOffset": 53}, {"referenceID": 12, "context": "As reported in (Kummerfeld et al., 2015), the margin methods generally outperform both likelihood and perception methods.", "startOffset": 15, "endOffset": 40}, {"referenceID": 24, "context": "Due to the hinge loss, the objective function is not differentiable, we use a subgradient method (Ratliff et al., 2007) which computes a gradientlike direction.", "startOffset": 97, "endOffset": 119}, {"referenceID": 25, "context": "Following (Socher et al., 2013), we use the diagonal variant of AdaGrad (Duchi et al.", "startOffset": 10, "endOffset": 31}, {"referenceID": 7, "context": ", 2013), we use the diagonal variant of AdaGrad (Duchi et al., 2011) with minibatchs to minimize the objective.", "startOffset": 48, "endOffset": 68}, {"referenceID": 8, "context": "International Chinese Word Segmentation Bakeoff (Emerson, 2005).", "startOffset": 48, "endOffset": 63}, {"referenceID": 26, "context": "Dropout is a popular technique for improving the performance of neural networks by reducing overfitting (Srivastava et al., 2014).", "startOffset": 104, "endOffset": 129}, {"referenceID": 2, "context": "PKU MSR +Dictionary ours theirs ours theirs (Chen et al., 2015a) 94.", "startOffset": 44, "endOffset": 64}, {"referenceID": 3, "context": "2 (Chen et al., 2015b) 94.", "startOffset": 2, "endOffset": 22}, {"referenceID": 2, "context": "However, (Chen et al., 2015a; Chen et al., 2015b) used an extra preprocess to filter out Chinese idioms according to an external dictionary.", "startOffset": 9, "endOffset": 49}, {"referenceID": 3, "context": "However, (Chen et al., 2015a; Chen et al., 2015b) used an extra preprocess to filter out Chinese idioms according to an external dictionary.", "startOffset": 9, "endOffset": 49}, {"referenceID": 2, "context": "The dictionary used in (Chen et al., 2015a; Chen et al., 2015b) is neither publicly released nor specified the exact source until now.", "startOffset": 23, "endOffset": 63}, {"referenceID": 3, "context": "The dictionary used in (Chen et al., 2015a; Chen et al., 2015b) is neither publicly released nor specified the exact source until now.", "startOffset": 23, "endOffset": 63}, {"referenceID": 48, "context": "Models PKU MSR P R F P R F (Zheng et al., 2013) 92.", "startOffset": 27, "endOffset": 47}, {"referenceID": 20, "context": "3 (Pei et al., 2014) 93.", "startOffset": 2, "endOffset": 20}, {"referenceID": 2, "context": "4 (Chen et al., 2015a)* 94.", "startOffset": 2, "endOffset": 22}, {"referenceID": 3, "context": "1 (Chen et al., 2015b) * 94.", "startOffset": 2, "endOffset": 22}, {"referenceID": 48, "context": "+Pre-trained character embedding (Zheng et al., 2013) 93.", "startOffset": 33, "endOffset": 53}, {"referenceID": 20, "context": "9 (Pei et al., 2014) 94.", "startOffset": 2, "endOffset": 20}, {"referenceID": 2, "context": "9 (Chen et al., 2015a)* 94.", "startOffset": 2, "endOffset": 22}, {"referenceID": 3, "context": "4 (Chen et al., 2015b)* 95.", "startOffset": 2, "endOffset": 22}, {"referenceID": 32, "context": "Models PKU MSR PKU MSR (Tseng et al., 2005) 95.", "startOffset": 23, "endOffset": 43}, {"referenceID": 36, "context": "4 - (Zhang and Clark, 2007) 94.", "startOffset": 4, "endOffset": 27}, {"referenceID": 43, "context": "2 - (Zhao and Kit, 2008b) 95.", "startOffset": 4, "endOffset": 25}, {"referenceID": 28, "context": "6 - (Sun et al., 2009) 95.", "startOffset": 4, "endOffset": 22}, {"referenceID": 29, "context": "3 - (Sun et al., 2012) 95.", "startOffset": 4, "endOffset": 22}, {"referenceID": 38, "context": "4 - (Zhang et al., 2013) - - 96.", "startOffset": 4, "endOffset": 24}, {"referenceID": 2, "context": "4* (Chen et al., 2015a) 94.", "startOffset": 3, "endOffset": 23}, {"referenceID": 3, "context": "6* (Chen et al., 2015b) 94.", "startOffset": 3, "endOffset": 23}, {"referenceID": 19, "context": "Therefore, we use word2vec (Mikolov et al., 2013) toolkit6 to pre-train the character embeddings on the Chinese Wikipedia corpus and use them for initialization.", "startOffset": 27, "endOffset": 49}, {"referenceID": 38, "context": "Recent systems such as (Zhang et al., 2013), (Chen et al.", "startOffset": 23, "endOffset": 43}, {"referenceID": 3, "context": ", 2013), (Chen et al., 2015b) and (Chen et al.", "startOffset": 9, "endOffset": 29}, {"referenceID": 2, "context": ", 2015b) and (Chen et al., 2015a) rely on both extensive feature engineering and external corpora to boost performance.", "startOffset": 13, "endOffset": 33}, {"referenceID": 47, "context": "In the closed-set setting, our models can achieve state-of-the-art performance on PKU dataset but a competitive result on MSR dataset, which can attribute to too strict maximum word length setting for consistence as it is well known that MSR corpus has a much longer average word length (Zhao et al., 2010).", "startOffset": 287, "endOffset": 306}, {"referenceID": 34, "context": "Most modern CWS methods followed (Xue, 2003) treated CWS as a sequence labeling problems (Zhao et al.", "startOffset": 33, "endOffset": 44}, {"referenceID": 46, "context": "Most modern CWS methods followed (Xue, 2003) treated CWS as a sequence labeling problems (Zhao et al., 2006b).", "startOffset": 89, "endOffset": 109}, {"referenceID": 6, "context": "Recently, researchers have tended to explore neural network based approaches (Collobert et al., 2011) to reduce efforts of feature engineering (Zheng et al.", "startOffset": 77, "endOffset": 101}, {"referenceID": 48, "context": ", 2011) to reduce efforts of feature engineering (Zheng et al., 2013; Qi et al., 2014; Chen et al., 2015a; Chen et al., 2015b).", "startOffset": 49, "endOffset": 126}, {"referenceID": 22, "context": ", 2011) to reduce efforts of feature engineering (Zheng et al., 2013; Qi et al., 2014; Chen et al., 2015a; Chen et al., 2015b).", "startOffset": 49, "endOffset": 126}, {"referenceID": 2, "context": ", 2011) to reduce efforts of feature engineering (Zheng et al., 2013; Qi et al., 2014; Chen et al., 2015a; Chen et al., 2015b).", "startOffset": 49, "endOffset": 126}, {"referenceID": 3, "context": ", 2011) to reduce efforts of feature engineering (Zheng et al., 2013; Qi et al., 2014; Chen et al., 2015a; Chen et al., 2015b).", "startOffset": 49, "endOffset": 126}, {"referenceID": 20, "context": "Pei et al. (2014) introduced the tag embedding as input to capture the combinations of context and tag history.", "startOffset": 0, "endOffset": 18}, {"referenceID": 17, "context": "Another notable exception is (Ma and Hinrichs, 2015), which is also an embedding-based model, but models CWS as configuration-action matching.", "startOffset": 29, "endOffset": 52}, {"referenceID": 35, "context": "Besides sequence labeling schemes, Zhang and Clark (2007) proposed a word-based perceptron method.", "startOffset": 35, "endOffset": 58}, {"referenceID": 35, "context": "Besides sequence labeling schemes, Zhang and Clark (2007) proposed a word-based perceptron method. Zhang et al. (2012) used a linear-time incremental model which can also benefits from various kinds of features including word-based features.", "startOffset": 35, "endOffset": 119}, {"referenceID": 39, "context": "At the same time of this work, some other neural models (Zhang et al., 2016a; Liu et al., 2016) have been proposed, which can also leverage word-level information but are quite different from ours in the basic framework.", "startOffset": 56, "endOffset": 95}, {"referenceID": 15, "context": "At the same time of this work, some other neural models (Zhang et al., 2016a; Liu et al., 2016) have been proposed, which can also leverage word-level information but are quite different from ours in the basic framework.", "startOffset": 56, "endOffset": 95}, {"referenceID": 43, "context": "The proposed model might furthermore benefit from some techniques in recent state-of-the-art systems, such as semisupervised learning (Zhao and Kit, 2008b; Zhao and Kit, 2008a; Sun and Xu, 2011; Zhao and Kit, 2011; Zeng et al., 2013; Zhang et al., 2013), incorporating global information (Zhao and Kit, 2007; Zhang et al.", "startOffset": 134, "endOffset": 253}, {"referenceID": 42, "context": "The proposed model might furthermore benefit from some techniques in recent state-of-the-art systems, such as semisupervised learning (Zhao and Kit, 2008b; Zhao and Kit, 2008a; Sun and Xu, 2011; Zhao and Kit, 2011; Zeng et al., 2013; Zhang et al., 2013), incorporating global information (Zhao and Kit, 2007; Zhang et al.", "startOffset": 134, "endOffset": 253}, {"referenceID": 27, "context": "The proposed model might furthermore benefit from some techniques in recent state-of-the-art systems, such as semisupervised learning (Zhao and Kit, 2008b; Zhao and Kit, 2008a; Sun and Xu, 2011; Zhao and Kit, 2011; Zeng et al., 2013; Zhang et al., 2013), incorporating global information (Zhao and Kit, 2007; Zhang et al.", "startOffset": 134, "endOffset": 253}, {"referenceID": 44, "context": "The proposed model might furthermore benefit from some techniques in recent state-of-the-art systems, such as semisupervised learning (Zhao and Kit, 2008b; Zhao and Kit, 2008a; Sun and Xu, 2011; Zhao and Kit, 2011; Zeng et al., 2013; Zhang et al., 2013), incorporating global information (Zhao and Kit, 2007; Zhang et al.", "startOffset": 134, "endOffset": 253}, {"referenceID": 35, "context": "The proposed model might furthermore benefit from some techniques in recent state-of-the-art systems, such as semisupervised learning (Zhao and Kit, 2008b; Zhao and Kit, 2008a; Sun and Xu, 2011; Zhao and Kit, 2011; Zeng et al., 2013; Zhang et al., 2013), incorporating global information (Zhao and Kit, 2007; Zhang et al.", "startOffset": 134, "endOffset": 253}, {"referenceID": 38, "context": "The proposed model might furthermore benefit from some techniques in recent state-of-the-art systems, such as semisupervised learning (Zhao and Kit, 2008b; Zhao and Kit, 2008a; Sun and Xu, 2011; Zhao and Kit, 2011; Zeng et al., 2013; Zhang et al., 2013), incorporating global information (Zhao and Kit, 2007; Zhang et al.", "startOffset": 134, "endOffset": 253}, {"referenceID": 41, "context": ", 2013), incorporating global information (Zhao and Kit, 2007; Zhang et al., 2016b), and joint models (Qian and Liu, 2012; Li and Zhou, 2012).", "startOffset": 42, "endOffset": 83}, {"referenceID": 40, "context": ", 2013), incorporating global information (Zhao and Kit, 2007; Zhang et al., 2016b), and joint models (Qian and Liu, 2012; Li and Zhou, 2012).", "startOffset": 42, "endOffset": 83}, {"referenceID": 23, "context": ", 2016b), and joint models (Qian and Liu, 2012; Li and Zhou, 2012).", "startOffset": 27, "endOffset": 66}, {"referenceID": 14, "context": ", 2016b), and joint models (Qian and Liu, 2012; Li and Zhou, 2012).", "startOffset": 27, "endOffset": 66}], "year": 2017, "abstractText": "Most previous approaches to Chinese word segmentation formalize this problem as a character-based sequence labeling task so that only contextual information within fixed sized local windows and simple interactions between adjacent tags can be captured. In this paper, we propose a novel neural framework which thoroughly eliminates context windows and can utilize complete segmentation history. Our model employs a gated combination neural network over characters to produce distributed representations of word candidates, which are then given to a long shortterm memory (LSTM) language scoring model. Experiments on the benchmark datasets show that without the help of feature engineering as most existing approaches, our models achieve competitive or better performances with previous stateof-the-art methods.", "creator": "TeX"}}}