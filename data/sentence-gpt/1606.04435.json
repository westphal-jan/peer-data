{"id": "1606.04435", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "14-Jun-2016", "title": "Adversarial Perturbations Against Deep Neural Networks for Malware Classification", "abstract": "Deep neural networks have recently been shown to lack robustness against adversarially crafted inputs. These inputs are derived from regular inputs by minor yet carefully selected perturbations that deceive the neural network into desired misclassifications. Existing work in this emerging field was largely specific to the domain of image classification, since images with their high-entropy can be conveniently manipulated without changing the images' overall visual appearance. Yet, it remains unclear how such attacks translate to more security-sensitive applications such as malware detection - which may pose significant challenges in sample generation and arguably grave consequences for failure.\n\n\n\n\nThe work of M. Vadimaran et al., which were published in Proceedings of the National Academy of Sciences, can be found here.\nImage credit: G\u00fcliur Krami, S. Bae, J. R. Yung, P. J. Vadimaran et al., and J. R. Yung (2016).", "histories": [["v1", "Tue, 14 Jun 2016 16:01:52 GMT  (198kb,D)", "http://arxiv.org/abs/1606.04435v1", "13 pages"], ["v2", "Thu, 16 Jun 2016 08:14:12 GMT  (198kb,D)", "http://arxiv.org/abs/1606.04435v2", "version update: correcting typos, incorporating external feedback"]], "COMMENTS": "13 pages", "reviews": [], "SUBJECTS": "cs.CR cs.LG cs.NE", "authors": ["kathrin grosse", "nicolas papernot", "praveen manoharan", "michael backes", "patrick mcdaniel"], "accepted": false, "id": "1606.04435"}, "pdf": {"name": "1606.04435.pdf", "metadata": {"source": "CRF", "title": "Adversarial Perturbations Against Deep Neural Networks for Malware Classification", "authors": ["Kathrin Grosse", "Nicolas Papernot", "Praveen Manoharan", "Michael Backes", "Patrick McDaniel"], "emails": ["grosse@cs.uni-saarland.de", "ngp5056@cse.psu.edu", "manoharan@cs.uni-saarland.de", "backes@mpi-sws.org", "mcdaniel@cse.psu.edu"], "sections": [{"heading": null, "text": "In this paper, we show how to construct highly-effective adversarial sample crafting attacks for neural networks used as malware classifiers. Here, we face severely constrained limits on crafting suitable samples when the problem of image classification is replaced by malware classification: (i) continuous, differentiable input domains are replaced by discrete, often binary inputs; and (ii) the loose condition of leaving visual appearance unchanged is replaced by requiring equivalent functional behavior. We demonstrate the feasibility of these attacks on many different instances of malware classifiers that we trained using the DREBIN Android malware data set. We furthermore evaluate to which extent potential defensive mechanisms against adversarial crafting can be leveraged to the setting of malware classification. While feature reduction did not prove to have a positive impact, distillation and re-training on adversarially crafted samples show promising results."}, {"heading": "1. INTRODUCTION", "text": "Deep neural networks have revolutionized how artificial intelligence and machine learning solve computational tasks that rely on high-dimensional data. Examples include dominating Go [20], handling autonomous cars [3] and classifying images at a large scale [12]. Neural networks exhibit particularly outstanding results in settings that involve large amounts of data. They have also been shown to have the capacity of extracting increasingly complex data representations. Hence it is sound to consider the hypothetical application of neural networks to security-critical domains such as malware classification.\nWhile the benefits of neural networks are undisputed, recent work has shown they lack robustness against adversarially crafted inputs. These inputs are derived from regular inputs by minor yet carefully selected perturbations [7, 17] that induce the neural network into adversary-desired misclassifications. The vast majority of work in this emerging field is specific to the domain of image classification. Images have high entropy and can be conveniently manipulated by changing individual pixels on a real, continuous scale. The changes applied are hardly visible to our eyes. The approach that conceptually underlies many recent adversarial machine learning research effort involves gradients of the function F represented by the neural network in order to classify inputs: evaluating it on an input X, one can quickly (and fully automatically) either identify individual input features that should be perturbed iteratively to achieve misclassification [17] or compute a suitable minor change for each pixel all at once [7].\nAdversarially crafted inputs can hence be used for subverting systems that rely on image classification. An example is decision making for autonomous vehicles, which might impose security threats in certain situations. However, verifying whether adversarial crafting is also applicable to inherently security-critical domains that differ significantly in terms of input type and set of possible valid perturbations remains largely an open problem. Adversarial crafting for malware detection, for example, would arguably entail much more severe consequences. But it also imposes research challenges that did not occur in the well-studied settings of computer vision: inputs have significantly less entropy. They are usually not represented on a continuous scale of real numbers, but as binary values\u2014an application either uses a certain system call or not. Moreover, approaches perturbing a given application are considered valid only if they do not modify or destroy the application\u2019s functionality. This was a significantly easier task in the settings of computer vision where this condition was replaced by requiring indistinguishability of images for a human observer (technically: minimizing the distance between the adversarial image and the original image).\nContributions. In this paper we show how to successfully perform adversarial crafting attacks on neural networks for malware classification. We employ feed forward neural networks, achieving state-of-the-art performance in detecting malware on the\nar X\niv :1\n60 6.\n04 43\n5v 1\n[ cs\n.C R\n] 1\n4 Ju\nn 20\nDREBIN data set [1]. To craft adversarial samples, we follow the method originally proposed by Papernot et al. [17], but address challenges that appear in the transition from continuous and differentiable input domains in computer vision to discrete and restricted inputs in malware detection.\nSince to the best of our knowledge there is no mature and publicly available malware detection system that uses neural networks, we develop our own classifier. We train and evaluate it on the DREBIN dataset introduced by Arp et al. [1], which contains more than 120,000 android applications samples, among them over 5,000 malware samples. All features were extracted using static analysis on the given application. Our classifier achieves up to 97% accuracy with 7.6% false negatives and 2% false positives, despite minimal effort for hyperparameter selection. This matches state of the art malware detection systems that rely on static features.\nTo generate the adversarial samples, we adapt an algorithm based on the forward derivative of attacked neural network, originally presented by Papernot et al. [17]. We address additional constraints that appear in malware detection: A) We can only fully add or remove features, but not gradually change them. This contrasts to previous applications of adversarial crafting in computer vision. B) We must preserve the utility of the modified application, which we achieve by only adding features, and only those that do not interfere with the functionality of the application. C) We can only add a restricted amount of features. To simplify matters, we therefore decide to only add entries to the AndroidManifest.xml file. This file is contained in the APK (the android application package) of the android application that contains the application\u2019s code and that is used to distribute the application to the end user. Despite these restrictions, we achieve up to a 85% misclassification rate on malicious applications. We thus validate that adversarial crafting is indeed viable in security critical domains such as malware detection: neural networks should not be used in such domains without taking precautions for hardening them against adversarial crafting.\nAs a second contribution, we investigate potential methods for hardening neural network based malware detection systems against adversarial crafting: by applying these mechanisms we aim at reducing the sensitivity of networks to adversarial manipulations of their inputs, and thus increase their resilience to adversarial crafting. We first look at the impact of feature reduction on the the network\u2019s sensitivity to adversarial crafting. In a second step we then consider distillation [18] and re-training on adversarial samples [21] which have both been proposed as actual defensive mechanisms in the literature. The findings of our experimental evaluation of the aforementioned mechanisms is threefold. First, feature reduction does not protect against adversarial crafting. It can even have adverse effects in that it further simplifies crafting adversarial samples. Second, using distillation reduces the misclassification rates, but the range of improvement is rather small. And third, re-training on adversarially crafted samples improves the resistance of most neural networks, however the parameter choice for retraining has a noticeable impact.\nOur findings show that adversarially crafted inputs pose a threat in security-critical domains where the behavior of unknown programs is being analyzed and classified."}, {"heading": "2. BACKGROUND", "text": "In this section, we explain the general concepts used in this paper. We first briefly cover background on neural networks and in some detail how to craft adversarial samples."}, {"heading": "2.1 Neural Networks", "text": "Neural Networks are machine learning models capable of solving a variety of tasks ranging from classification [12, 4] to regression [15] and dimensionality reduction [11]. They use a graph of elementary computing units\u2014named neurons\u2014 organized in layers linked to each other to learn models. Each neuron applies an activation function, often non-linear, to its input to produce an output. Figure 1, taken from [17], illustrates the general structure of such neural neutworks and also introduces the notation that is used throughout the paper.\nStarting with the model input, each network layer produces an output used as an input by the next layer. Networks with a single intermediate\u2014hidden\u2014layer are qualified as shallow neural networks whereas models with multiple hidden layers are deep neural networks. Using multiple hidden layers is interpreted as hierarchically extracting representations from the input [6], eventually producing a representation relevant to solve the machine learning task and output a prediction.\nA neural network model F can be formalized as the composition of multi-dimensional and parametrized functions fi each corresponding to a layer of the network architecture\u2014 and a representation of the input:\nF : ~x 7\u2192 fn(...f2(f1(~x, \u03b81), \u03b82)..., \u03b8n) (1)\nwhere each vector \u03b8i parametrizes layer i of the network F and includes weights for the links connecting layer i to layer i\u22121. The set of model parameters \u03b8 = {\u03b8i} is learned during training. For instance, in supervised settings, parameter values are fixed by computing prediction errors f(x)\u2212 ~y on a collection of known input-output pairs (~x, ~y)."}, {"heading": "2.2 Adversarial Machine Learning", "text": "Neural networks, like numerous machine learning models, have been shown to be vulnerable to manipulations of their inputs [21]. These manipulations take the form of adversarial samples, inputs crafted by adding carefully selected and often humanly indistinguishable perturbations to inputs so as to force a targeted model to misclassify the sample. These samples exploit imperfections in the training phase as well as the underlying linearity of components used to learn models\u2014even if the overall model is non-linear like is the case for deep neural networks [7]. The space of adversaries was formalized for multi-class deep learning classifiers in a taxonomy [17]. Adversarial goals can vary from simple misclassification of the input in a class different from the legitimate source class to source-target misclassification where samples from any source class are to be misclassified in a chosen target class. Adversaries can also be taxonomized by the knowledge of the targeted model they must possess to perform their attacks.\nIn this paper, we study a binary classifier with only two output classes. Crafting an adversarial sample ~x\u2217\u2014 misclassified by model F\u2014from a legitimate sample ~x can be formalized as the following problem [21]:\n~x\u2217 = ~x+ \u03b4~x = ~x+ min \u2016~z\u2016 s.t. F(~x+ ~z) 6= F(~x) (2)\nwhere \u03b4~x is the minimal perturbation ~z yielding misclassification, according to a norm \u2016 \u00b7 \u2016 appropriate for the input domain. Due to the non-linearity and non-convexity of models learned by deep neural networks, a closed form solution to this problem is hard to find. Thus, algorithms were proposed to select perturbations approximatively minimizing the optimization problem stated in Equation 2. The fast gradient sign method introduced by Goodfellow et al. [7] linearizes the model\u2019s cost function around the input to be perturbed and selects a perturbation by differentiating this cost function with respect to the input itself and not the network parameters like is traditionally the case during training. The forward derivative based approach introduced by Papernot et al. [17] evaluates the model\u2019s output sensitivity to each\ninput component using its Jacobian matrix JF = [ \u2202Fj \u2202xi ] . A perturbation is then selected with adversarial saliency maps, which rank each input component\u2019s contribution to the adversarial goal by combining components of matrix JF. Both the fast gradient sign and forward derivative methods require full knowledge of the targeted model\u2019s architecture and parameters. However, a black-box attack leveraging both of these approaches to target unknown remotely hosted deep neural networks was proposed in [16]. It first approximates the targeted model by querying it for output labels to train a substitute model, which is then used to craft adversarial samples that are also misclassified by the originally targeted model.\nMachine learning models deployed in adversarial settings therefore need to be robust to manipulations of their inputs [14]. Solutions were proposed to improve the training algorithms used for deep neural networks, yielding models more robust to such perturbations. Goodfellow et al. demonstrated that explicitly training with adversarial samples reduced the error rate of models on samples crafted against the resulting improved model [7]. Papernot et al. proposed the use of distillation\u2014training with class probabilities produced by a teacher model instead of labels\u2014as a defense mechanism, and showed that this effectively reduces the sensitivity of models to small perturbations [18]. WardeFarley et al. [22] evaluated a simplified variant of distillation training models on softened indicator vectors instead of probabilities. They showed error rates reductions on sam-\nples crafted using the fast gradient sign method. These solutions however do not completely prevent misclassification of adversarial samples, which thus remains an open-problem."}, {"heading": "3. METHODOLOGY", "text": "This section describes the approach to adversarial crafting for malware detection. We start by describing how we represent applications for the classification, and how we train the classifiers, detailing which configurations we choose for the neural networks. Thereafter, we describe in detail how we craft adversarial samples based on the forward derivative of the trained neural network, and detail the restrictions on crafting adversarial samples for malware detection (we only add features to ensure functionality is preserved)."}, {"heading": "3.1 Application Model", "text": "Before we can start training a neural network based malware detection system, we first have to decide on a representation of applications that we use as input to our classifier. In this work, we focus on statically determined features of applications. As a feature, we understand some property that the statically evaluated code of the application exhibits. This includes for instance whether the application uses a specific system call or not, as well as a usage of specific hardware components or access to the Internet.\nA natural way to represent such features is using binary indicator vectors: Given features 1, . . . ,M , we represent an application using the binary vector X \u2208 {0, 1}M , where Xi indicate whether the application exhibits feature i, i.e. Xi = 1, or not, i.e. Xi = 0. Due to the varied nature of applications that are available, M will typically be very large, while each single application only exhibits very few features.\nThis leads to very sparse feature vectors, and overall, a very sparsely populated space of applications in which we try to successfully separate malicious from benign applications. Neural networks have shown to be very successful at separating classes in sparse populated domains. Hence, we will use them to build our malware detection system."}, {"heading": "3.2 Training the Malware Classifier", "text": "To the best of our knowledge, there is no publicly available malware detection system based on neural networks that considers static features. While Dahl et al. [5] use a neural\nnetworks to classify malware, their approach uses random projections and considers dynamic data. Since perturbing dynamically gathered features is a lot more challenging than modifying static features, we stick to the simpler, static case in this work and leave the dynamic case for future work.\nThe most popular neural network architectures used in the literature are convolutional networks for computer vision tasks, and recurrent neural networks for natural language processing and hand-writing recognition. These architectures, however, take advantage of special properties of their input domains to improve their classification performance. On the one hand, convolutional neural networks work well on input containing translation invariant properties, which can be found in images [13]. Recurrent neural networks, on the other hand, work well with input data that needs to be processed sequentially [8].\nThe binary indicator vector X we use to represent an application does not possess any of the above structural properties. We therefore stick to regular, feed-forward neural networks as described in Section 2 to solve our malware classification task. Regular feed-forward neural networks are known to not work as well on established use cases as the structured networks mentioned above. However the absence of such structures in our input domain only leaves unstructured feed-forward neural networks. As we will see in Section 4, these work well enough for our use case.\nWe train several classifiers using varying configurations: while each network takes the same binary indicator vector X as input, they differ in the amount of hidden layers (between one and four). Furthermore, we also vary the amount of neurons per layer, ranging between 10 and 300.\nWe use the rectified non-linearity as the activation function for each hidden neuron in our network, i.e.\n\u2200p \u2208 [1,mk] : fk,p(x) = max(0, x)\nwith\nx = mk\u22121\u2211 j=1 wj,p \u00b7 xj + bj,p,\nwhere the weight wj,p and the bias bj,p are trained values. After the final hidden layer, we use a softmax layer to normalize the output of the network to a probability distribution, i.e. the output of the network is computed by\nFi(X) = exi\nex0 + ex1 , xi = mn\u2211 j=1 wj,i \u00b7 xj + bj,i (3)\nTo train our network, we use standard gradient descent with batches of size 1000 that are split into training and validation sets, using 10 training epochs per iteration. The performance of the thus trained network is hugely influenced by the choice of the gradient descent hyperparameters: these parameters are not trained, but set at the beginning to control the behavior of the gradient descent algorithm. Usually, a large of effort is put toward finding the ideal hyperparameters for a given use case. In our case, we choose these hyperparameters based on previous experience and do not perform an exhaustive search for ideal hyperparameters. For instance, we choose a dropout of 50% between each hidden layer avoid over-fitting, i.e. the output of 50% of all neurons in each layer is ignored and set to 0. As we will see in the evaluation, we still achieve acceptable performance results. However, we expect that the classification perfor-\nAlgorithm 1 Crafting Adversarial Samples for Malware Detection Input: x, y, F, k, I 1: x\u2217 \u2190 x 2: \u0393 = {1 . . . |x|} 3: while argmaxj Fj(x\n\u2217) 6= y and ||\u03b4X|| < k do 4: Compute forward derivative \u2207F(x\u2217) 5: imax = argmaxj\u2208\u0393\u2229I,Xj=0 \u2202Fy(X) \u2202Xj 6: if imax \u2264 0 then 7: return Failure 8: end if 9: x\u2217imax = 1\n10: \u03b4x \u2190 x\u2217 \u2212 x 11: end while 12: return x\u2217\nmance could be greatly increased by putting more effort into the selection of these hyperparameters.\nSince the DREBIN dataset that we use for our evaluations (cf. Section 4) has a fairly unbalanced ratio between malware and benign applications, we experiment with different ratios of malware in each training batch to compare the achieved performance values. The number of training iterations is then set in such a way that all malware samples are at least used once. We evaluate the classification performance of each of these networks using accuracy, false negative and false positive rates as performance measures. Afterwards, we evaluate the best performing networks against the adversarial crafting attack we discuss next."}, {"heading": "3.3 Crafting Adversarial Malware Samples", "text": "The goal of adversarial sample crafting in malware detection is to mislead the detection system, causing the classification for a particular application to change according to the attackers wishes.\nDescribing more formally, we start with X \u2208 {0, 1}m, a binary indicator vector that indicates which features are present in an application. Given X, the classifier F returns a two dimensional vector F(X) = [F0(X),F1(X)] with F0(X) + F1(X) = 1 that encodes the classifiers belief that X is either benign (F0(X)) or malicious (F1(X)). We take as the classification result y the option that has the higher probability, i.e. y = argmaxi Fi(X). The goal of adversarial sample crafting now is to find a small perturbation \u03b4 such that the classification results y\u2032 of F(X+\u03b4) is different from the original results, i.e. y\u2032 6= y. We denote y\u2032 as our target class in the adversarial crafting process.\nOur goal is to have a malicious application classified as benign, i.e. given a malicious input X, we want a classification results y\u2032 = 0. The opposite case is to misclassify a benign application as malicious. While this is also possible, we assume that the perturbation of the application will be performed by the original author of this application. Since an honest author has no interest in having his benign application classified as malware, we ignore this case.\nWe adopt the adversarial crafting algorithm based on the jacobian matrix\nJF = \u2202F(X)\n\u2202X =\n[ \u2202Fi(X)\n\u2202Xj ] i\u22080,1,j\u2208[1,m]\nof the neural network F put forward by Papernot et al. [17] and which we already discussed in Section 2. Despite it orig-\ninally being defined for images, we show that an adaptation to a different domain is fairly straight forward.\nTo craft an adversarial sample, we take mainly two steps. In the first, we compute the gradient of F with respect to X to estimate the direction in which a perturbation in X would change F\u2019s output. In the second step, we choose a perturbation \u03b4 of X with maximal positive gradient into our target class y\u2032. For malware misclassification, this means that we choose the index i = argmaxj\u2208[1,m],Xj=0 F0(Xj) that maximizes the change into our target class 0 by changing Xi. Note that we only consider positive changes for positions j at which Xj = 0, which correspond to adding features the application represented by X (since X is a binary indicator vector). We discuss why we only consider adding features in the Section 3.4.\nIdeally, we keep this change small to make sure that we do not cause a negative change of F due to intermediate changes of the gradient. For computer vision, this is not an issue since the values of pixels are continuous and can be changes in very small steps. In the malware detection case, however, we do not have continuous data, but rather discrete input values: since X \u2208 0, 1m is a binary indicator vector, our only option is to increase one component in X by exactly 1 to retain a valid input to F.\nThe adversarial sample crafting process now works in iterative fashion: After computing the gradient, we choose a feature whose gradient is the largest for our target class and change it\u2019s value in X (i.e. by making corresponding changes to the application) to obtain our new input vector X(1). We then recompute the gradient under this new input X(1) and find the second feature to change. We repeat this process until either a) we reached the limit for maximum amount of allowed changes or b) we successfully cause a misclassification. A pseudo-code implementation of the algorithm is given in Algorithm 1. It is largely similar to the algorithms presented in [17], with the difference in the discrete changes of the input vector due to the input domain and also the additional restrictions below."}, {"heading": "3.4 Restrictions on Adversarial Crafting", "text": "To make sure that the modifications caused by the above algorithms do not change the application in question too much, we impose a bound on the distortion \u03b4 that we apply to the original sample. As in the computer vision case, we only allow distortions \u03b4 with \u2016\u03b4\u2016 \u2264 k. We differ, however, in the norm that we apply: in computer vision, you typically use the L\u221e norm to bound the maximum change to a single pixel. In our case, each modification to an entry will always change its value by exactly 1, therefore making the L\u221e norm obsolete. We instead use the L1 norm to bound the overall number of features that we modify.\nWhile the main goal of adversarial crafting is to achieve misclassification, for malware detection, this cannot happen at the cost of the application\u2019s functionality: feature changes determined by Algorithm 1 can cause the application in question to lose its functionality in parts or completely. To avoid this case, we adopt the following additional restrictions on the adversarial crafting algorithm: we only add features, and only add those that do not interfere with other features already present in the application. This protects us from unknowingly destroying the applications functionality. Formally, we encode the above restriction through the index set\nI: it contains the indices corresponding to features that can be added without affecting the applications functionality.\nIn Section 4, we show that we can successfully craft adversarial samples despite these additional restrictions."}, {"heading": "4. EXPERIMENTAL EVALUATION", "text": "We evaluate the training of the neural network based malware detector and adversarial sample-induced misclassification of inputs on it. Through our evaluation, we want to validate the following two hypotheses: first, that the neural network based malware classifier achieves performance comparable to state of the art malware classifiers (on static features) presented in the literature. Second, the adversarial sample crafting algorithm discussed in Section 3.3 allows us to successfully mislead the neural network we trained. As a measure of success, we consider the misclassification rate achieved by the adversarial crafting algorithm. The misclassification rate is defined by the percentage of malware samples that are misclassified after application of the adversarial crafting algorithm, but were correctly classified before that."}, {"heading": "4.1 Data set", "text": "We base our evaluations on the DREBIN dataset, originally introduced by Arp et al. [1]: DREBIN contains 129.013 android applications, of which 123.453 are benign and 5.560 are malicious. The dataset already provides extracted, static features for all applications. In total, the dataset contains 545.333 features that are divided into 8 feature classes, each of which is represented by a binary value that indicates whether the feature is present in an application or not. This directly translates to the binary indicator vector X that we use to represent applications, which now is a vector in {0, 1}M with M = 545.333.\nThe 8 feature classes in DREBIN cover various aspects of android applications, including: A) Permissions and hardware component access requested by each application (e.g. for CAMERA or INTERNET access). B) Restricted and suspicious (i.e. accessing sensitive data, e.g. getDeviceID()) API-calls made by the applications. C) application components such activities, service, content provider and broadcast receivers used by each applications, and D) intents used by applications to communicate with other applications. Table 1 lists each feature class and its cardinality.\nIn Table 2 we give average and quantile statistics on the amount of features exhibited by the applications in DREBIN. Given these numbers, we decide to set our distortion bound k = 20 \u2013 assuming we are modifying an ap-\nplication of average size, it still remains within the two main quartiles when adding at most 20 features.\nSince the DREBIN data set contains Android applications, we decide to only add features that can be added through modifications in the AndroidManifest.xml file of the android application\u2019s APK. The manifest is used by the application to announces its components (i.e. its activities, services, broadcast receivers, and content providers), the permissions it requests and further information about the application the system needs to run the application. Changes in the manifest are particularly easy to implement, since they only incur an additional line in the manifest and do not cause any interference with the application\u2019s functionality. Changes to the code, on the other hand, would require more effort and would have to be handled more carefully. Table 1 lists where each feature class in DREBIN originates from, identifying those features that originate in the manifest and that we will consider in the adversarial crafting algorithm. In Algorithm 1, we represent the set of valid features for modification by the index set I."}, {"heading": "4.2 Malware Detection", "text": "We train numerous neural networks with different architectures according to the training procedure described in Section 3. Our baseline architecture is the network with 2 hidden layers of 200 neurons each. From here, we vary the number of neurons per layer (to 10, 50, 100 and 300), and also the number of layers (to 1, 3 and 4). We also vary the malware ratio in each training batch by steps of 0.1 from 0.1 to 0.5 and measure its impact on the overall performance of the neural network in correctly classifying the applications in the DREBIN dataset.\nThe results for the different neural networks can be found in Table 3. WE first list previous classifiers from the literature, then the architecture (in neurons per layer) that we trained. As performance measures we consider the overall accuracy on the DREBIN dataset, as well as the false positive and false negative rates.\nUsing the malware ratio in our training batches as a parameter to be optimized, we achieve false negative rates at a level comparable to state of the art classifiers. This, however, happens at a trade-off with overall accuracy and false positive rates. In comparison, Arp et al. [1] achieve a 6.1% false negative rate at a 1% false positive rate. Sayfullina et al. [19] even achieve a 0.1% false negative rate, however at the cost of 17.9% false positives.\nWe can observe high accuracy (> 90%) across results. The network architecture has some impact on the trade-off between accuracy, false positive, and false negative rates at the various malware ratios. However, no clear trends can be observed that would indicate how many neurons should be chosen on the first and second layer. Overall, the baseline architecture with 200 neurons on 2 layers each achieves, according to our experimental setup, the best trade-off between false positive, false negatives and overall accuracy. With this architecture, we achieve around 98% overall accu-\nracy, with about 7% false negatives and 3.3% false positives. As discussed in Section 3.2, we expect that this performance can further greatly be improved by searching for hyperparameters better fitting this use case. Overall, we thus validate our hypothesis that a neural network based classifier can successfully classify the DREBIN malware data set."}, {"heading": "4.3 Adversarial Malware Crafting", "text": "We next apply the adversarial crafting algorithm described in Section 3 and observe on how many occasions we can successfully mislead our neural network based classifiers. We quantify the performance of our algorithm through the achieved misclassification rate, which measures the amount of previously correctly classified malware that is misclassified after the adversarial crafting. In addition, we also measure the average number of modifications required to achieve the measured misclassification rate to assess which architecture provided a harder time being mislead. As discussed above, allow at most 20 modification to any of the malware applications.\nThe performance results are listed in Table 4. As we can see, we achieve misclassification rates from at least 50% in the case of the two layer neural network with 200 and 10 neurons in each layer, to up to 84% in the case of the two layer network with 10 neurons in both layers. Again, we cannot directly observe any rule that directly connects network architecture to resistance against adversarial crafting.\nHowever, we can observe that the malware ratio used in the training batches is correlated to the misclassification rate: a higher malware ratio in general results in a lower misclassification rate, albeit with exceptions (e.g. in the case of the 2 layer network with 10 neurons in each).\nStill, due to the fact that our algorithm is able to successfully mislead most networks for a large majority of malware samples, we validate the hypothesis that our adversarial crafting algorithm for malware can be used to mislead neural network based malware detection systems."}, {"heading": "4.4 Discussion", "text": "As our evaluation results show, adversarial sampling is a real threat also for neural network based malware detection. We therefore confirm that findings from applications in the domain of computer vision can successfully be transferred to more security critical domains such as malware detection. Despite heavy restrictions on the type of modifications we are allowed to undertake, we are able to mislead our classifiers on about 60% to 80% of the malicious application samples (depending on considered architecture and ignoring corner cases). While this does not match the misclassification rates of up to 97% for misclassifying images of digits reported by Papernot et al. [17], a reduction in misclassification performance was to be expected given the challenge inherent to the malware detection domain.\nIn the next part of this paper, we are going to consider possible defensive mechanisms to harden our neural network against adversarial crafting. To ensure comparability, we\nwill restrict ourselves to the 200\u00d7 200 neurons architecture for our neural network models (i.e. 2 layers with 200 neurons each). It is trained with malware ratios between 0.3 and 0.5 for our subsequent evaluations, and we will use the networks of the same architecture and with the same malwario rates as our comparison baselines. In the above evaluations, this configuration provides a reasonable trade-off between false positive and false negative rates while retaining an overall high accuracy. Furthermore, this architecture remains comparatively (outside of corner cases) resistant to adversarial crafting, only being misled in about 64% of the cases while at the same time requiring a comparatively large amount of modifications."}, {"heading": "5. DEFENSES", "text": "In this section, we investigate potential defensive mechanisms for hardening our neural network-based malware detector against adversarial crafting. We measure the effectiveness of each these mechanisms by determining the reduction in misclassification rates, i.e. we determine the difference between the misclassification rates achieved against regular networks, and those to which we applied the mechanisms. Again, as in the previous section, the misclassification rate is defined as the percentage of malware samples that are misclassified after the application of the adversarial crafting algorithm, but were correctly classified before.\nWe first take a look at feature reduction as a potential defensive mechanism: by reducing the number of features we consider for classification, we hope to reduce the neural network\u2019s sensitivity to changes in the input, as well as reduce the number of feature options the adversary has for distorting an application and causing a misclassification. We consider simple feature reduction, a naive way to include features based on where and how often they appear, as well as a more involved feature reduction based on the mutual information. Our goal is to evaluate whether feature reduction in general can be used to harden a neural network against adversarial crafting.\nAfter looking at feature reduction, we consider two defensive mechanisms already proposed in the literature: distillation, as introduced by Papernot et al. [18], and re-training on adversarial samples, following methodology described by Szegedy et al. [21]."}, {"heading": "5.1 Simple Feature Reduction", "text": "We first look at \u201csimple\u201d feature reduction: we manually select features based on how they are expressed by the applications in our data set. We formulate these simple feature reduction strategies through feature restrictions that we impose on the feature set that we use for training our neural networks.\nThe first feature restriction is that we train only on features that are only expressed in the AnroidManifest.xml of the applications\u2019 APKs. Recall that, in the adversarial crafting algorithm, we decided to only add features that appear in the manifest to make sure that we do not interfere with any functionality of the application in question. By restricting the training of our neural networks to manifest features only, we focus the classifier on the features that we actually change. We denote this feature restriction with manifestonly. By applying manifestonly, we are left with 233727 of the original 545333 features that we use for training.\nAs the second feature restriction, we only consider features that do not appear in the r largest feature classes. We illustrate the cardinalities of our 8 feature classes in Table 1. A large part of the features (over 50%) consist of URLs that are mostly unique to each application, and would therefore not help with separating benign from malicious applications. Other features that appear in the largest feature classes show similar behavior. With this feature restriction, which we call onlysmall, we try to define a simple way to filter out all such features. We instantiate onlysmall for r = 1 and 2, which leaves us with 234845 and 49116 features, respectively, after applying the feature restriction.\nThe last feature restriction that we consider is a variant of onlysmall. With this restriction, we only consider features that appear in at least r applications. We thereby directly filter out features that do not consistently appear across representatives of one target class, and will therefore not be used to detect an application that belongs in them. We call this feature restriction onlyfreq.\nIn effect, onlyfreq tries to achieve a similar goal as onlysmall, however the set of restricted features is determined differently. This is also reflected in the number of features that remain after the applying the restriction: we instantiate onlyfreq for r \u2208 {1, 2, 3, 4}, which leaves us with 177438, 95871, 60052 and 44942 features respectively.\n5.1.1 Evaluation We use all three simple feature reduction methods to train\nnew neural networks, each with 2 hidden layers and 200 neurons on each layer. We then run our adversarial crafting algorithm on each of these networks to determine how frequently they are mislead by adversarial crafting. Figure 2 shows the difference between the original misclassification rates we achieves on the regular networks and the misclassification rates we achieve for each feature reduction method.\nThis difference is evaluated at the different malware ratios we used for training.\nAs we can see, the simple feature reduction generally weakens the neural network against adversarial crafting. In contrast to the about 62% misclassification rate we achieve on the regular neural network, the misclassification rate goes up to, in the worst case, 99% for the manifestonly feature restriction. Most other neural networks trained under feature restrictions also show an increase of the misclassification rate. While there are two exceptions, for instance under the restriction onlyfreq with r = 1 where we reduce the misclassification rate by around 8%, these cannot be observed consistently across architectures. We therefore leave them as exceptions to the rule.\nOverall, we can conclude that simple feature reduction is not a suitable mechanism to harden your neural network against adversarial crafting in the domain of malware detection."}, {"heading": "5.2 Feature Reduction via Mutual Information", "text": "Next, take a look at feature reduction using mutual information, instead of taking the naive approach we described in the previous section. We first briefly introduce the notion of mutual information, before we then investigate its influence on our networks\u2019 resistance against adversarial crafting.\nThe mutual information I(X;Y ) of two random variables X and Y is given by\nI(X;Y ) = \u2211 X,Y p(x, y) log( p(x, y) p(x)p(y) ). (4)\nIntuitively, the mutual information I(X;Y ) of two random variables X and Y quantifies the statistical dependence between X and Y . We use this concept to identify features in our data set that carry a large amount of information with regard to our target classes. To select features using mutual information, we compute it for each feature/target class pair, i.e. we compute the mutual information I(X;Y ), where X is the probability of a feature and Y the probability of a target class. We then order all features by their mutual information with the target classes and we pick the n highest of them. If we encounter a tie, we include all features with that same mutual information (and thus obtain a number of features slightly bigger than n). We finally use the set of features determined by this process to train new classifiers and evaluate their resistance against adversarial crafting.\nConsider that there is a second approach, which we also investigate. Given that a feature has a high value in mutual information, we also assume that its influence on the classification is high. We will thus train a classifier only on feature sets that contain many but equally important features and observe the misclassification rates.To do so, we inverse the ranking from the first experiment and consider with n the n lowest values of mutual information and their corresponding features.\nThere are few features with a high value of mutual information and many with a low value. As a consequence, the second approach will yield much bigger datasets to train on then the first.\n5.2.1 Evaluation first Approach In this approach we rank after the importance of features\ngiven MI. We trained several new neural networks for vary-\ning values of n, ranging from n = 50 to n = 2000. Each of these networks use the standard architecture (i.e. 2 hidden layers with 200 neurons each) and are trained with a 50% malware ratio during training. We then applied the adversarial crafting algorithm described in Section 3.3 to determine their susceptibility to the attack.\nOur evaluation results are presented in Figure 3. It shows the development of false negative rates (at accuracy > 93%), misclassification rates and average required distortion for an increasing amount of features selected through the mutual information method described above. As a point of comparison, we provide the performance values of the regular neural network with feature reduction as horizontal lines (i.e. 7% false negatives, 62% misclassification rate and an average distortion of around 15).\nFor very small n < 250, we experience very poor classification performance and therefore ignore those cases in our evaluation. For the other cases, performance is in general worse than in the regular case. For n \u2264 1600, we only achieve about 7% false negative rates in the classification, and the adversarial crafting algorithm is successful in at least 70% of the cases, requiring a smaller average distortion of 12.5.\nFor 1500 < n < 1900, we observe a peak where our newly trained networks achieve nearly the same performance as our regular networks: the false negative rate is around 7% and the misclassification rate also goes down to about 60% for n = 1700. The required average distortion, however remains low at at most 14. The misclassification rates differ however strongly for the similar models.\n5.2.2 Evaluation second Approach\nWe again trained several networks on different values of n, where n however now starts with the tied, low ranked features. For very low n, we observe that the network is very vulnerable to adversarial samples, having a misclassification rate of over 90%.\nThe false negative and accuracy rates are, however, in general comparable with the original model trained on the full data. The general misclassification rate decreases with more features and varies between 70.1% for n = 1050 and 87.6% for n = 800, average is 82%. This is much worse than the original model.\n5.2.3 Discussion As in the previous section, our findings indicate that us-\ning less features does not necessarily make the adversarial crafting harder. It seems that few numbers features lead to easy perturbations, since each single feature has a much higher impact for classifying an application. By using feature reduction based on mutual information, we thus make our network even more susceptible against adversarial crafting.\nIn the case of selecting the most important features, however, there are some cases where the trained model is less prone to adversarial samples. Hence, we do not want to exclude the possibility of improving resistance against adversarial crafting using feature reduction: a more fine-grained analysis could lead to positive results that we leave for future work."}, {"heading": "5.3 Distillation", "text": "We next take a look at distillation. While distillation was originally proposed by Hinton et al. [10] as as a way to transfer knowledge from large neural networks to a smaller ones,\nPapernot at al. [18] recently proposed using it as a defensive mechanism against adversarial crafting as well.\nThe basic idea behind distillation is the following: assume that we already have a neural network F that classifies a training data set X into the target classes Y and produces as output a probability distribution over Y (e.g. by using a final softmax layer we already introduced in Section 3.2). Further, assume that we want to train a second neural network F\u2032 on the same data set X that achieves the same (or even better) performance. Now, instead of using the target class labels that come with the data set, we use the output F(X) of the network F as the label of X \u2208 X to train F\u2032. The output produced by F will not assign a unique label Y \u2208 Y to X, but instead a probability distribution over Y. The new labels therefore contain more information about the membership of X to the different target classes, compared to a simple label that just chooses the most likely class.\nPapernot et al. [18] motivate the use of distillation as a defensive mechanism through its capability to improve the second networks generalization performance (i.e. classification performance on samples outside the training data set). Thus, instead of using distillation to train a second smaller network like was proposed in [10], they use the output of the first neural network F to train a second neural network F\u2032 with exactly the same architecture. An important detail in the distillation process is the slight modification of the final softmax layer (cf. Equation 3) in the original network F: instead of the regular softmax normalization, we use\nFi(X) = ( ezi(x)/T\u2211|Y| l=1 e zl(x)/T ), (5)\nwhere T is a distillation parameter called temperature. For T = 1, we obtain the regular softmax normalization that we already used for the training in Section 3.2. If T is large, the output probabilities approach a more uniform distribution, whereas for small T , the output of F will be less so. To achieve a good distillation result, we use the output of the original network F produced at a high temperature T and use it as class labels to train the new network F\u2032.\nThe overall procedure for hardening our classifier against adversarial crafting can thus be summarized in the following three steps.\n1. Given the original classifier F and the samples X , construct a new training data set D = {(X,F(X)) | X \u2208 X} that is labeled with F\u2019s output at high temperature.\n2. Construct a new neural network F\u2032 with the same architecture as F.\n3. Train F\u2032 on D.\nNote that both step two and step three are performed under the same high temperature T to achieve a good distillation performance.\nWe next apply the above procedure on our originally trained classifiers and examine the impact of distillation as a defensive mechanism against adversarial crafting in the domain of malware detection.\n5.3.1 Evaluation Figure 5 shows the effects of distillation on misclassifica-\ntion compared to the original models. We observe a general,\nstrong increase of the false negative rate, and a slight increase in the false positive rate. For ratio 0.5, it raises from 4 to 6.4, whereas for 0.3, it is equivalent. Due to the large size of the benign class, the accuracy only ranges in between 93-95%.\nOn the other hand, we observe that the misclassification rate drops significantly, in some cases to 38.5% for ratio 0.4. The difference in the average number of perturbed features, however, is rather small. The number of perturbed features is 14 for ratio 0.3 to 16 for the other two.\nUsing distillation, we can strengthen the neural network against adversarial samples. However, the misclassification rates are still around 40% and thus rather high. Additionally, we pay this robustness with a less good classifier. The effect is further not as strong as on computer vision data. Papernot et al. [18] reported rates around 5% after distillation for images. Further research should investigate whether distillation can be adapted to malware data or discrete, sparse and unbalanced data in general. Also the question remains which properties (or combinations thereof) influence the obtained improvement in terms of misclassification rates."}, {"heading": "5.4 Re-Training", "text": "As the last countermeasure, we try re-training our classifier with adversarially crafted samples. This method was originally proposed by Szegedy at al. [21] and involves the following steps:\n1. Train the classifier F on original data set D = B \u222aM , whereB is the set of benign, andM the set of malicious applications\n2. Craft adversarial samples A for F using the forward gradient method described in Section 3.3\n3. Iterate additional training epochs on F with the adversarial samples from the last step as additional, malicious samples.\nBy re-training, we aim at improving the generalization performance of F, i.e. improve the classification performance of F on samples outside our training set. A good generalization performance generally makes a classifier less sensitive to small perturbations, and therefore also more resilient to adversarial crafting.\n5.4.1 Evaluation We applied the above mechanism to the regular networks\nwith 200 neurons on 2 layers each that we have been consider throughout this section. We continued their training on n1 = 20, n2 = 100 and n3 = 250 additional, adversarially crafted malware samples. We combined the adversarial samples to create training batches by mixing them with benign samples at each network\u2019s malware ratio. We then trained the network for one more epoch on one training batch and re-evaluated their susceptibility against adversarial crafting.\nFigure 6 illustrates the performance (in false negative rate) of the re-trained networks and the misclassification performance Algorithm 1 achieved on them (in misclassification rate and average required distortion). We grouped networks by their malware ratio during training and give as comparison the performance values of the original networks before re-training.\nFor the network trained with malware ratio 0.3 and 0.4, we can observe a reduction of the misclassification rate, and an increase of the required average distortion for n1 and n2 additional training samples. For instance, we achieve a misclassification rate of 67% for the network trained with 100 additional samples at 0.3 malware ratio, down from 73% for the original network. A further increase of the adversarial training samples used for re-training, however, causes the misclassification rate to increase again, reaching up to 79% for both malware ratios.\nFor the last networks, trained with malware ratio 0.5, the misclassification rate only decreases if we use 250 adversarial training samples. Here, we reach 68% misclassification rate, down from 69% for the original network. For fewer amount of adversarial samples for re-training, the misclassification rate remains very similar to the original case. It seems that the network trained with 0.5 malware ratio is fitting very close to the malware samples it was trained on, and therefore requires more adversarial samples to generalize and improve its resistance against adversarial crafting.\nOverall, we can conclude that simple re-training on adversarially crafted malware samples does improve the neural network\u2019s resistance against adversarial crafting. The number of adversarial samples required to improve the resistance depend heavily on the training parameters we chose for training the original networks. However, choosing too many may also further degrade the network\u2019s resistance against adversarial crafting.\n5.4.2 Discussion In our evaluation above, we only considered one iteration\nof adversarial re-training. Ideally, the above method is continuously applied, by computing new adversarial samples for\neach newly generated network F\u2032. These samples are then fed back to re-train F\u2032 and generate a new network more resistant against adversarial crafting.\nGoodfellow et al. [7] propose an alternative approach to the general idea of adversarial re-training: instead of training on actually crafted adversarial samples, they formulate an adversarial loss function that incorporates the possibility of adversarial crafting through perturbations in direction of the network\u2019s gradient. This allows them to continuously incorporate adversarially crafted samples during training with an infinite supply thereof. We think that this approach should further be investigated also in security critical domains, and consider this a promising direction for future work."}, {"heading": "5.5 Summary of Results", "text": "We considered four potential defensive mechanisms and evaluated their impact on a neural networks susceptibility against adversarial crafting. Feature reduction, both simple as well through mutual information, usually make the neural network weaker against adversarial crafting. Having less features of greater importance makes it easier to craft adversarial samples. This is caused by the larger impact each feature has on the output distribution of the network. At this point we cannot recommend feature reduction as a defensive mechanism. Future work will have to identify, potentially more involved, feature reduction methods that actually increase a network\u2019s resistance against adversarial crafting.\nWe also investigated distillation and re-training, which were both originally proposed as defensive mechanism against adversarial crafting in the computer vision domain. Distillation does have a positive effect, but does not perform as well as in the computer vision domain. The reasons for this have to be investigated in future work. Simple re-training achieved consistent reduction of misclassification\nrates across different networks. However, choosing the right amount of adversarial training samples has a significant impact on this reduction. Iteratively applying re-training to a network might further improve the network\u2019s resistance."}, {"heading": "6. RELATED WORK", "text": "We discuss further related work in addition to those already discussed in Section 2. Security of Machine Learning is an active research area. Barreno et al. [2] give a broad overview of attacks against machine learning systems. They discriminate between exploratory attacks at test time or causative attacks that influence the training data to obtain the desired result. Adversarial samples, as used here, are employed at test time. There has already been done some work on why they exist in general[9, 7]. They can be constructed for different algorithms and also generalize between different architectures in some cases [7].\nGu et al. [9] claim that adversarial samples are mainly a product of the way feed-forward neural networks are trained and optimized. As a solution they propose deep contractive networks, which are however harder to optimize. These networks include a layer wise penalty which furthermore limits their capacity.\nGoodfellow at al. [7] proposed a linear explanation to the existence of adversarial samples. In their intuition, adversarial samples are not due to the overall non-linearity of neural networks, but to the linearity of their underlying components. Consequently, they show that adversarial samples generalize to other linear models, such as for example logistic regression. In contrast to those models, neural networks are however able to be hardened against adversarial samples."}, {"heading": "7. CONCLUSION AND FUTURE WORK", "text": "In this paper, we investigated the viability of adversarial crafting against neural networks in domains different from computer vision. Through our evaluation on the DREBIN data set, we were able to show that adversarial crafting is indeed a real threat in security critical domains such as malware detection as well: we achieved misclassification rates of up to 80% against neural network based classifiers that achieve classification performance on par with state of the art classifiers from the literature.\nAs a second contribution, we examined four potential defensive mechanisms for hardening our neural networks against adversarial crafting. Our evaluations of these mechanisms showed the following: first, feature reduction, a popular method to reduce input complexity and simplify the training of the classifier, generally makes the neural network weaker against adversarial crafting. Second, distillation does improve misclassification rates, but does not decrease them as strongly as observed in computer vision settings. And third, re-training on adversarially crafted samples achieves consistent reduction of misclassification rates across architectures.\nFuture work should more carefully examine the defensive mechanisms that we identified as potentially helpful For example in addition to distillation, Goodfellow et al.\u2019s [7] idea of using an adversarial loss functions during training should be more carefully examined in security relevant domains.\nThe applicability of adversarial crafting attacks to additional domains should also be examined. Such results could\ndirect researchers towards more effective defensive mechanisms."}, {"heading": "8. REFERENCES", "text": "[1] D. Arp, M. Spreitzenbarth, M. Hubner, H. Gascon, and\nK. Rieck. DREBIN: Effective and Explainable Detection of Android Malware in Your Pocket. In Proceedings of the 2014 Network and Distributed System Security Symposium (NDSS), 2014. [2] M. Barreno, B. Nelson, A. D. Joseph, and J. D. Tygar. The security of machine learning. Machine Learning, 81(2):121\u2013148, 2010. [3] M. Bojarski, D. Del Testa, D. Dworakowski, B. Firner, B. Flepp, P. Goyal, L. D. Jackel, M. Monfort, U. Muller, J. Zhang, et al. End to end learning for self-driving cars. arXiv preprint arXiv:1604.07316, 2016. [4] G. E. Dahl, J. W. Stokes, L. Deng, and D. Yu. Large-scale malware classification using random projections and neural networks. In Acoustics, Speech and Signal Processing (ICASSP), 2013 IEEE International Conference on, pages 3422\u20133426. IEEE, 2013. [5] G. E. Dahl, J. W. Stokes, L. Deng, and D. Yu. Large-scale malware classification using random projections and neural networks. In IEEE International Conference on Acoustics, Speech and Signal Processing, ICASSP 2013, Vancouver, BC, Canada, May 26-31, 2013, pages 3422\u20133426, 2013. [6] I. Goodfellow, Y. Bengio, and A. Courville. Deep learning. Book in preparation for MIT Press, 2016. [7] I. J. Goodfellow et al. Explaining and harnessing adversarial examples. In Proceedings of the 2015 International Conference on Learning Representations, 2015. [8] A. Graves. Generating sequences with recurrent neural networks. arXiv preprint arXiv:1308.0850, 2013. [9] S. Gu and L. Rigazio. Towards deep neural network architectures robust to adversarial examples. CoRR, abs/1412.5068, 2014.\n[10] G. Hinton, O. Vinyals, and J. Dean. Distilling the Knowledge in a Neural Network. ArXiv e-prints, 2015. [11] G. E. Hinton and R. R. Salakhutdinov. Reducing the dimensionality of data with neural networks. Science, 313(5786):504\u2013507, 2006. [12] A. Krizhevsky, I. Sutskever, and G. E. Hinton. Imagenet classification with deep convolutional neural networks. In Advances in neural information processing systems, pages 1097\u20131105, 2012. [13] S. Mallat. Understanding deep convolutional networks. arXiv preprint arXiv:1601.04920, 2016. [14] P. McDaniel, N. Papernot, et al. Machine Learning in Adversarial Settings. IEEE Security & Privacy, 14(3), May/June 2016. [15] J. Neter, M. H. Kutner, C. J. Nachtsheim, and W. Wasserman. Applied linear statistical models, volume 4. Irwin Chicago, 1996. [16] N. Papernot, P. McDaniel, I. Goodfellow, S. Jha, and al. Practical black-box attacks against deep learning systems using adversarial examples. arXiv preprint arXiv:1602.02697, 2016. [17] N. Papernot, P. McDaniel, S. Jha, M. Fredrikson, Z. B. Celik, and A. Swami. The Limitations of Deep Learning in Adversarial Settings. In Proceedings of the 1st IEEE European Symposium in Security and Privacy (EuroS&P), 2016. [18] N. Papernot, P. McDaniel, X. Wu, S. Jha, and A. Swami. Distillation as a defense to adversarial perturbations against deep neural networks. Proceedings of the 37th IEEE Symposium on Security and Privacy (S&P), 2015. [19] L. Sayfullina, E. Eirola, D. Komashinsky, P. Palumbo, Y. Miche\u0301, A. Lendasse, and J. Karhunen. Efficient detection of zero-day android malware using normalized\nbernoulli naive bayes. In 2015 IEEE TrustCom/BigDataSE/ISPA, Helsinki, Finland, August 20-22, 2015, Volume 1, pages 198\u2013205, 2015.\n[20] D. Silver, A. Huang, C. J. Maddison, A. Guez, L. Sifre, G. Van Den Driessche, J. Schrittwieser, I. Antonoglou, V. Panneershelvam, M. Lanctot, et al. Mastering the game of go with deep neural networks and tree search. Nature, 529(7587):484\u2013489, 2016. [21] C. Szegedy, W. Zaremba, I. Sutskever, J. Bruna, D. Erhan, I. Goodfellow, and R. Fergus. Intriguing properties of\nneural networks. In Proceedings of the 2014 International Conference on Learning Representations. Computational and Biological Learning Society, 2014.\n[22] D. Warde-Farley and I. Goodfellow. Adversarial perturbations of deep neural networks. In T. Hazan, G. Papandreou, and D. Tarlow, editors, Advanced Structured Prediction. 2016."}], "references": [{"title": "DREBIN: Effective and Explainable Detection of Android Malware in Your Pocket", "author": ["D. Arp", "M. Spreitzenbarth", "M. Hubner", "H. Gascon", "K. Rieck"], "venue": "Proceedings of the 2014 Network and Distributed System Security Symposium (NDSS)", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2014}, {"title": "The security of machine learning", "author": ["M. Barreno", "B. Nelson", "A.D. Joseph", "J.D. Tygar"], "venue": "Machine Learning, 81(2):121\u2013148", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2010}, {"title": "et al", "author": ["M. Bojarski", "D. Del Testa", "D. Dworakowski", "B. Firner", "B. Flepp", "P. Goyal", "L.D. Jackel", "M. Monfort", "U. Muller", "J. Zhang"], "venue": "End to end learning for self-driving cars. arXiv preprint arXiv:1604.07316", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2016}, {"title": "Large-scale malware classification using random projections and neural networks", "author": ["G.E. Dahl", "J.W. Stokes", "L. Deng", "D. Yu"], "venue": "Acoustics, Speech and Signal Processing (ICASSP), 2013 IEEE International Conference on, pages 3422\u20133426. IEEE", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2013}, {"title": "Large-scale malware classification using random projections and neural networks", "author": ["G.E. Dahl", "J.W. Stokes", "L. Deng", "D. Yu"], "venue": "IEEE International Conference on Acoustics, Speech and Signal Processing, ICASSP 2013, Vancouver, BC, Canada, May 26-31, 2013, pages 3422\u20133426", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2013}, {"title": "Deep learning", "author": ["I. Goodfellow", "Y. Bengio", "A. Courville"], "venue": "Book in preparation for MIT Press", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2016}, {"title": "Explaining and harnessing adversarial examples", "author": ["I.J. Goodfellow"], "venue": "In Proceedings of the 2015 International Conference on Learning Representations,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2015}, {"title": "Generating sequences with recurrent neural networks", "author": ["A. Graves"], "venue": "arXiv preprint arXiv:1308.0850", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2013}, {"title": "Towards deep neural network architectures robust to adversarial examples", "author": ["S. Gu", "L. Rigazio"], "venue": "CoRR, abs/1412.5068", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2014}, {"title": "Distilling the Knowledge in a Neural Network", "author": ["G. Hinton", "O. Vinyals", "J. Dean"], "venue": "ArXiv e-prints", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2015}, {"title": "Reducing the dimensionality of data with neural networks", "author": ["G.E. Hinton", "R.R. Salakhutdinov"], "venue": "Science, 313(5786):504\u2013507", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2006}, {"title": "Imagenet classification with deep convolutional neural networks", "author": ["A. Krizhevsky", "I. Sutskever", "G.E. Hinton"], "venue": "Advances in neural information processing systems, pages 1097\u20131105", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2012}, {"title": "Understanding deep convolutional networks", "author": ["S. Mallat"], "venue": "arXiv preprint arXiv:1601.04920", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2016}, {"title": "Machine Learning in Adversarial Settings", "author": ["P. McDaniel", "N. Papernot"], "venue": "IEEE Security & Privacy,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2016}, {"title": "Applied linear statistical models", "author": ["J. Neter", "M.H. Kutner", "C.J. Nachtsheim", "W. Wasserman"], "venue": "volume 4. Irwin Chicago", "citeRegEx": "15", "shortCiteRegEx": null, "year": 1996}, {"title": "and al", "author": ["N. Papernot", "P. McDaniel", "I. Goodfellow", "S. Jha"], "venue": "Practical black-box attacks against deep learning systems using adversarial examples. arXiv preprint arXiv:1602.02697", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2016}, {"title": "The Limitations of Deep Learning in Adversarial Settings", "author": ["N. Papernot", "P. McDaniel", "S. Jha", "M. Fredrikson", "Z.B. Celik", "A. Swami"], "venue": "Proceedings of the 1st IEEE European Symposium in Security and Privacy (EuroS&P)", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2016}, {"title": "Distillation as a defense to adversarial perturbations against deep neural networks", "author": ["N. Papernot", "P. McDaniel", "X. Wu", "S. Jha", "A. Swami"], "venue": "Proceedings of the 37th IEEE Symposium on Security and Privacy (S&P)", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2015}, {"title": "Efficient detection of zero-day android malware using normalized 12  bernoulli naive bayes", "author": ["L. Sayfullina", "E. Eirola", "D. Komashinsky", "P. Palumbo", "Y. Mich\u00e9", "A. Lendasse", "J. Karhunen"], "venue": "2015 IEEE TrustCom/BigDataSE/ISPA, Helsinki, Finland, August 20-22, 2015, Volume 1, pages 198\u2013205", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2015}, {"title": "G", "author": ["D. Silver", "A. Huang", "C.J. Maddison", "A. Guez", "L. Sifre"], "venue": "Van Den Driessche, J. Schrittwieser, I. Antonoglou, V. Panneershelvam, M. Lanctot, et al. Mastering the game of go with deep neural networks and tree search. Nature, 529(7587):484\u2013489", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2016}, {"title": "Intriguing properties of  neural networks", "author": ["C. Szegedy", "W. Zaremba", "I. Sutskever", "J. Bruna", "D. Erhan", "I. Goodfellow", "R. Fergus"], "venue": "Proceedings of the 2014 International Conference on Learning Representations. Computational and Biological Learning Society", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2014}, {"title": "Adversarial perturbations of deep neural networks", "author": ["D. Warde-Farley", "I. Goodfellow"], "venue": "Advanced Structured Prediction", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2016}], "referenceMentions": [{"referenceID": 19, "context": "Examples include dominating Go [20], handling autonomous cars [3] and classifying images at a large scale [12].", "startOffset": 31, "endOffset": 35}, {"referenceID": 2, "context": "Examples include dominating Go [20], handling autonomous cars [3] and classifying images at a large scale [12].", "startOffset": 62, "endOffset": 65}, {"referenceID": 11, "context": "Examples include dominating Go [20], handling autonomous cars [3] and classifying images at a large scale [12].", "startOffset": 106, "endOffset": 110}, {"referenceID": 6, "context": "These inputs are derived from regular inputs by minor yet carefully selected perturbations [7, 17] that induce the neural network into adversary-desired misclassifications.", "startOffset": 91, "endOffset": 98}, {"referenceID": 16, "context": "These inputs are derived from regular inputs by minor yet carefully selected perturbations [7, 17] that induce the neural network into adversary-desired misclassifications.", "startOffset": 91, "endOffset": 98}, {"referenceID": 16, "context": "The approach that conceptually underlies many recent adversarial machine learning research effort involves gradients of the function F represented by the neural network in order to classify inputs: evaluating it on an input X, one can quickly (and fully automatically) either identify individual input features that should be perturbed iteratively to achieve misclassification [17] or compute a suitable minor change for each pixel all at once [7].", "startOffset": 377, "endOffset": 381}, {"referenceID": 6, "context": "The approach that conceptually underlies many recent adversarial machine learning research effort involves gradients of the function F represented by the neural network in order to classify inputs: evaluating it on an input X, one can quickly (and fully automatically) either identify individual input features that should be perturbed iteratively to achieve misclassification [17] or compute a suitable minor change for each pixel all at once [7].", "startOffset": 444, "endOffset": 447}, {"referenceID": 0, "context": "DREBIN data set [1].", "startOffset": 16, "endOffset": 19}, {"referenceID": 16, "context": "[17], but address challenges that appear in the transition from continuous and differentiable input domains in computer vision to discrete and restricted inputs in malware detection.", "startOffset": 0, "endOffset": 4}, {"referenceID": 0, "context": "[1], which contains more than 120,000 android applications samples, among them over 5,000 malware samples.", "startOffset": 0, "endOffset": 3}, {"referenceID": 16, "context": "[17].", "startOffset": 0, "endOffset": 4}, {"referenceID": 17, "context": "In a second step we then consider distillation [18] and re-training on adversarial samples [21] which have both been proposed as actual defensive mechanisms in the literature.", "startOffset": 47, "endOffset": 51}, {"referenceID": 20, "context": "In a second step we then consider distillation [18] and re-training on adversarial samples [21] which have both been proposed as actual defensive mechanisms in the literature.", "startOffset": 91, "endOffset": 95}, {"referenceID": 11, "context": "Neural Networks are machine learning models capable of solving a variety of tasks ranging from classification [12, 4] to regression [15] and dimensionality reduction [11].", "startOffset": 110, "endOffset": 117}, {"referenceID": 3, "context": "Neural Networks are machine learning models capable of solving a variety of tasks ranging from classification [12, 4] to regression [15] and dimensionality reduction [11].", "startOffset": 110, "endOffset": 117}, {"referenceID": 14, "context": "Neural Networks are machine learning models capable of solving a variety of tasks ranging from classification [12, 4] to regression [15] and dimensionality reduction [11].", "startOffset": 132, "endOffset": 136}, {"referenceID": 10, "context": "Neural Networks are machine learning models capable of solving a variety of tasks ranging from classification [12, 4] to regression [15] and dimensionality reduction [11].", "startOffset": 166, "endOffset": 170}, {"referenceID": 16, "context": "Figure 1, taken from [17], illustrates the general structure of such neural neutworks and also introduces the notation that is used throughout the paper.", "startOffset": 21, "endOffset": 25}, {"referenceID": 5, "context": "Using multiple hidden layers is interpreted as hierarchically extracting representations from the input [6], eventually producing a representation relevant to solve the machine learning task and output a prediction.", "startOffset": 104, "endOffset": 107}, {"referenceID": 20, "context": "Neural networks, like numerous machine learning models, have been shown to be vulnerable to manipulations of their inputs [21].", "startOffset": 122, "endOffset": 126}, {"referenceID": 6, "context": "These samples exploit imperfections in the training phase as well as the underlying linearity of components used to learn models\u2014even if the overall model is non-linear like is the case for deep neural networks [7].", "startOffset": 211, "endOffset": 214}, {"referenceID": 16, "context": "The space of adversaries was formalized for multi-class deep learning classifiers in a taxonomy [17].", "startOffset": 96, "endOffset": 100}, {"referenceID": 20, "context": "Crafting an adversarial sample ~ x\u2217\u2014 misclassified by model F\u2014from a legitimate sample ~x can be formalized as the following problem [21]:", "startOffset": 133, "endOffset": 137}, {"referenceID": 16, "context": "[17]", "startOffset": 0, "endOffset": 4}, {"referenceID": 6, "context": "[7] linearizes the model\u2019s cost function around the input to be perturbed and selects a perturbation by differentiating this cost function with respect to the input itself and not the network parameters like is traditionally the case during training.", "startOffset": 0, "endOffset": 3}, {"referenceID": 16, "context": "[17] evaluates the model\u2019s output sensitivity to each", "startOffset": 0, "endOffset": 4}, {"referenceID": 15, "context": "However, a black-box attack leveraging both of these approaches to target unknown remotely hosted deep neural networks was proposed in [16].", "startOffset": 135, "endOffset": 139}, {"referenceID": 13, "context": "Machine learning models deployed in adversarial settings therefore need to be robust to manipulations of their inputs [14].", "startOffset": 118, "endOffset": 122}, {"referenceID": 6, "context": "demonstrated that explicitly training with adversarial samples reduced the error rate of models on samples crafted against the resulting improved model [7].", "startOffset": 152, "endOffset": 155}, {"referenceID": 17, "context": "proposed the use of distillation\u2014training with class probabilities produced by a teacher model instead of labels\u2014as a defense mechanism, and showed that this effectively reduces the sensitivity of models to small perturbations [18].", "startOffset": 227, "endOffset": 231}, {"referenceID": 21, "context": "[22] evaluated a simplified variant of distillation training models on softened indicator vectors instead of probabilities.", "startOffset": 0, "endOffset": 4}, {"referenceID": 4, "context": "[5] use a neural", "startOffset": 0, "endOffset": 3}, {"referenceID": 12, "context": "On the one hand, convolutional neural networks work well on input containing translation invariant properties, which can be found in images [13].", "startOffset": 140, "endOffset": 144}, {"referenceID": 7, "context": "Recurrent neural networks, on the other hand, work well with input data that needs to be processed sequentially [8].", "startOffset": 112, "endOffset": 115}, {"referenceID": 16, "context": "[17] and which we already discussed in Section 2.", "startOffset": 0, "endOffset": 4}, {"referenceID": 16, "context": "It is largely similar to the algorithms presented in [17], with the difference in the discrete changes of the input vector due to the input domain and also the additional restrictions below.", "startOffset": 53, "endOffset": 57}, {"referenceID": 0, "context": "[1]: DREBIN contains 129.", "startOffset": 0, "endOffset": 3}, {"referenceID": 0, "context": "[1] achieve a 6.", "startOffset": 0, "endOffset": 3}, {"referenceID": 18, "context": "[19] even achieve a 0.", "startOffset": 0, "endOffset": 4}, {"referenceID": 0, "context": "[1] \u2212 \u2212 6.", "startOffset": 0, "endOffset": 3}, {"referenceID": 18, "context": "[19] \u2212 \u2212 0.", "startOffset": 0, "endOffset": 4}, {"referenceID": 9, "context": "06 [10, 10] 0.", "startOffset": 3, "endOffset": 11}, {"referenceID": 9, "context": "06 [10, 10] 0.", "startOffset": 3, "endOffset": 11}, {"referenceID": 9, "context": "74 [10, 10] 0.", "startOffset": 3, "endOffset": 11}, {"referenceID": 9, "context": "74 [10, 10] 0.", "startOffset": 3, "endOffset": 11}, {"referenceID": 9, "context": "90 [10, 10] 0.", "startOffset": 3, "endOffset": 11}, {"referenceID": 9, "context": "90 [10, 10] 0.", "startOffset": 3, "endOffset": 11}, {"referenceID": 9, "context": "11 [10, 200] 0.", "startOffset": 3, "endOffset": 12}, {"referenceID": 9, "context": "04 [10, 200] 0.", "startOffset": 3, "endOffset": 12}, {"referenceID": 9, "context": "6 [10, 200] 0.", "startOffset": 2, "endOffset": 11}, {"referenceID": 9, "context": "71 [200, 10] 0.", "startOffset": 3, "endOffset": 12}, {"referenceID": 9, "context": "86 [200, 10] 0.", "startOffset": 3, "endOffset": 12}, {"referenceID": 9, "context": "82 [200, 10] 0.", "startOffset": 3, "endOffset": 12}, {"referenceID": 9, "context": "92 [10, 10] 0.", "startOffset": 3, "endOffset": 11}, {"referenceID": 9, "context": "92 [10, 10] 0.", "startOffset": 3, "endOffset": 11}, {"referenceID": 9, "context": "15 [10, 10] 0.", "startOffset": 3, "endOffset": 11}, {"referenceID": 9, "context": "15 [10, 10] 0.", "startOffset": 3, "endOffset": 11}, {"referenceID": 9, "context": "12 [10, 10] 0.", "startOffset": 3, "endOffset": 11}, {"referenceID": 9, "context": "12 [10, 10] 0.", "startOffset": 3, "endOffset": 11}, {"referenceID": 9, "context": "48 [10, 200] 0.", "startOffset": 3, "endOffset": 12}, {"referenceID": 9, "context": "89 [10, 200] 0.", "startOffset": 3, "endOffset": 12}, {"referenceID": 9, "context": "84 [10, 200] 0.", "startOffset": 3, "endOffset": 12}, {"referenceID": 9, "context": "96 [200, 10] 0.", "startOffset": 3, "endOffset": 12}, {"referenceID": 9, "context": "96 [200, 10] 0.", "startOffset": 3, "endOffset": 12}, {"referenceID": 9, "context": "79 [200, 10] 0.", "startOffset": 3, "endOffset": 12}, {"referenceID": 16, "context": "[17], a reduction in misclassification performance was to be expected given the challenge inherent to the malware detection domain.", "startOffset": 0, "endOffset": 4}, {"referenceID": 17, "context": "[18], and re-training on adversarial samples, following methodology described by Szegedy et al.", "startOffset": 0, "endOffset": 4}, {"referenceID": 20, "context": "[21].", "startOffset": 0, "endOffset": 4}, {"referenceID": 9, "context": "[10] as as a way to transfer knowledge from large neural networks to a smaller ones,", "startOffset": 0, "endOffset": 4}, {"referenceID": 17, "context": "[18] recently proposed using it as a defensive mechanism against adversarial crafting as well.", "startOffset": 0, "endOffset": 4}, {"referenceID": 17, "context": "[18] motivate the use of distillation as a defensive mechanism through its capability to improve the second networks generalization performance (i.", "startOffset": 0, "endOffset": 4}, {"referenceID": 9, "context": "Thus, instead of using distillation to train a second smaller network like was proposed in [10], they use the output of the first neural network F to train a second neural network F\u2032 with exactly the same architecture.", "startOffset": 91, "endOffset": 95}, {"referenceID": 17, "context": "[18] reported rates around 5% after distillation for images.", "startOffset": 0, "endOffset": 4}, {"referenceID": 20, "context": "[21] and involves the following steps:", "startOffset": 0, "endOffset": 4}, {"referenceID": 6, "context": "[7] propose an alternative approach to the general idea of adversarial re-training: instead of training on actually crafted adversarial samples, they formulate an adversarial loss function that incorporates the possibility of adversarial crafting through perturbations in direction of the network\u2019s gradient.", "startOffset": 0, "endOffset": 3}, {"referenceID": 1, "context": "[2] give a broad overview of attacks against machine learning systems.", "startOffset": 0, "endOffset": 3}, {"referenceID": 8, "context": "There has already been done some work on why they exist in general[9, 7].", "startOffset": 66, "endOffset": 72}, {"referenceID": 6, "context": "There has already been done some work on why they exist in general[9, 7].", "startOffset": 66, "endOffset": 72}, {"referenceID": 6, "context": "They can be constructed for different algorithms and also generalize between different architectures in some cases [7].", "startOffset": 115, "endOffset": 118}, {"referenceID": 8, "context": "[9] claim that adversarial samples are mainly a product of the way feed-forward neural networks are trained and optimized.", "startOffset": 0, "endOffset": 3}, {"referenceID": 6, "context": "[7] proposed a linear explanation to the existence of adversarial samples.", "startOffset": 0, "endOffset": 3}, {"referenceID": 6, "context": "\u2019s [7] idea of using an adversarial loss functions during training should be more carefully examined in security relevant domains.", "startOffset": 3, "endOffset": 6}], "year": 2016, "abstractText": "Deep neural networks have recently been shown to lack robustness against adversarially crafted inputs. These inputs are derived from regular inputs by minor yet carefully selected perturbations that deceive the neural network into desired misclassifications. Existing work in this emerging field was largely specific to the domain of image classification, since images with their high-entropy can be conveniently manipulated without changing the images\u2019 overall visual appearance. Yet, it remains unclear how such attacks translate to more security-sensitive applications such as malware detection\u2013which may pose significant challenges in sample generation and arguably grave consequences for failure. In this paper, we show how to construct highly-effective adversarial sample crafting attacks for neural networks used as malware classifiers. Here, we face severely constrained limits on crafting suitable samples when the problem of image classification is replaced by malware classification: (i) continuous, differentiable input domains are replaced by discrete, often binary inputs; and (ii) the loose condition of leaving visual appearance unchanged is replaced by requiring equivalent functional behavior. We demonstrate the feasibility of these attacks on many different instances of malware classifiers that we trained using the DREBIN Android malware data set. We furthermore evaluate to which extent potential defensive mechanisms against adversarial crafting can be leveraged to the setting of malware classification. While feature reduction did not prove to have a positive impact, distillation and re-training on adversarially crafted samples show promising results.", "creator": "LaTeX with hyperref package"}}}