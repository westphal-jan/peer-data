{"id": "1606.04778", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "15-Jun-2016", "title": "The Learning and Prediction of Application-level Traffic Data in Cellular Networks", "abstract": "Traffic learning and prediction is at the heart of the evaluation of the performance of telecommunications networks and attracts a lot of attention in wired broadband networks. Now, benefiting from the big data in cellular networks, it becomes possible to make the analyses one step further into the application level. In this paper, we firstly collect a significant amount of application-level traffic data from cellular network operators. Afterwards, with the aid of the traffic \"big data\", we make a comprehensive study over the modeling and prediction framework of cellular network traffic. Our results solidly demonstrate that there universally exist some traffic statistical modeling characteristics, including ALPHA-stable modeled property in the temporal domain and the sparsity in the spatial domain. Meanwhile, the results also demonstrate the distinctions originated from the uniqueness of different service types of applications. Furthermore, we propose a new traffic prediction framework to encompass and explore these aforementioned characteristics and then develop a dictionary learning-based alternating direction method to solve it. Besides, we validate the prediction accuracy improvement and the robustness of the proposed framework through extensive simulation results.\n\n\n\nThe next step is to test the performance of an application-level traffic analysis. We perform a test on the performance of cellular network operators and measure the correlation between the performance of their service types and its data quality, with the results of a statistical analysis. The result is not a linear regression, but rather an overall linear and independent variable of the observed performance, a condition called the statistical analysis.\nWe perform a regression analysis with a set of parameters that determine the accuracy of the service type and the time of the data analysis. This process can be used to generate data in a number of ways including the average speed of the data analysis and the time of the data analysis. The results are not based on a linear regression model, but rather an aggregate data set that is based on the time of the data analysis, an aggregate data set that has a large size of data sets that are only very small.\nSince data sets are only very small, the time of data analysis can be obtained from the data collection and the application performance. It is important to note that while this model does not solve the basic problems in this work, it is not able to offer the application to some degree of performance. Moreover, it is also not possible to perform an unbiased statistical analysis in the context of the application data collection and the application performance of a certain service type.\nSince data sets are only very small, the time of data analysis can be obtained from the data collection and the application performance. It", "histories": [["v1", "Wed, 15 Jun 2016 14:13:57 GMT  (170kb,D)", "http://arxiv.org/abs/1606.04778v1", null], ["v2", "Tue, 28 Mar 2017 03:52:23 GMT  (8109kb,D)", "http://arxiv.org/abs/1606.04778v2", "Accepted by IEEE Transactions on Wireless Communications on March 26, 2017"]], "reviews": [], "SUBJECTS": "cs.NI cs.LG", "authors": ["rongpeng li", "zhifeng zhao", "jianchao zheng", "chengli mei", "yueming cai", "honggang zhang"], "accepted": false, "id": "1606.04778"}, "pdf": {"name": "1606.04778.pdf", "metadata": {"source": "CRF", "title": "The Learning and Prediction of Application-level Traffic Data in Cellular Networks", "authors": ["Rongpeng Li", "Zhifeng Zhao", "Jianchao Zheng", "Yan Chen", "Chengli Mei", "Yueming Cai", "Honggang Zhang"], "emails": ["}@huawei.com).", "honggangzhang}@zju.edu.cn).", "longxingren.zjc.s@163.com,", "caiym@vip.sina.com).", "meichl@ctbri.com.cn)."], "sections": [{"heading": null, "text": "1 The Learning and Prediction of Application-level Traffic Data in Cellular Networks\nRongpeng Li, Zhifeng Zhao, Jianchao Zheng, Yan Chen, Chengli Mei, Yueming Cai, and Honggang Zhang\nAbstract\u2014Traffic learning and prediction is at the heart of the evaluation of the performance of telecommunications networks and attracts a lot of attention in wired broadband networks. Now, benefiting from the big data in cellular networks, it becomes possible to make the analyses one step further into the application level. In this paper, we firstly collect a significant amount of application-level traffic data from cellular network operators. Afterwards, with the aid of the traffic \u201cbig data\u201d, we make a comprehensive study over the modeling and prediction framework of cellular network traffic. Our results solidly demonstrate that there universally exist some traffic statistical modeling characteristics, including \u03b1-stable modeled property in the temporal domain and the sparsity in the spatial domain. Meanwhile, the results also demonstrate the distinctions originated from the uniqueness of different service types of applications. Furthermore, we propose a new traffic prediction framework to encompass and explore these aforementioned characteristics and then develop a dictionary learning-based alternating direction method to solve it. Besides, we validate the prediction accuracy improvement and the robustness of the proposed framework through extensive simulation results.\nIndex Terms\u2014Big data, cellular networks, traffic prediction, \u03b1stable models, dictionary learning, alternative direction method, sparse signal recovery.\nI. INTRODUCTION\nTraffic learning prediction in cellular networks, which is a classical yet still appealing field, yields a significant number of meaningful results. From a macroscopic perspective, it provides the commonly believed result that mobile Internet will witness a 1000-folded traffic growth in the next 10 years [1], which is acting as a crucial anchor for the design of next-generation cellular network architecture and embedded algorithms. On the other hand, the fine traffic prediction on a daily, hourly or even minutely basis could contribute to the optimization and management of cellular networks like energy savings [2], opportunistic scheduling [3], and network anomaly detection [4] . In other words, a precisely predicted future traffic load knowledge, which contributes to improving the\nR. Li and Y. Chen are with Huawei Technologies Co. Ltd., Shanghai 210256, China (email: {lirongpeng, bigbird.chenyan }@huawei.com).\nZ. Zhao, and H. Zhang are with the College of Information Science and Electronic Engineering, Zhejiang University, Hangzhou 310027, China (email:{zhaozf, honggangzhang}@zju.edu.cn).\nJ. Zheng, and Y. Cai are with the College of Communications Engineering, PLA University of Science and Technology, Nanjing 210007, China (email: longxingren.zjc.s@163.com, caiym@vip.sina.com).\nC. Mei is with the China Telecom Technology Innovation Center, Beijing, China (email: meichl@ctbri.com.cn).\nThis paper is supported by the National Basic Research Program of China (973Green, No. 2012CB316000), the Key (Key grant) Project of Chinese Ministry of Education (No. 313053), the Key Technologies R&D Program of China (No. 2012BAH75F01).\nnetwork energy efficiency by dynamically adjusting the cell working status [5], [6], plays an important role in designing greener traffic-aware cellular networks. For example, it is inevitable to precisely forecast the traffic in one single cell, so as to turn some base stations (BSs) into sleeping mode when the traffic demand becomes low. Moreover, there comes a trend to enhance the edge functionalities of cellular networks, by deploying more resources in edge nodes (e.g., BSs). Hence, it is also meaningful to learn the traffic characteristics in one single cell.\nOur previous research [7] has demonstrated the microscopic traffic predictability in cellular networks for circuit switching\u2019s voice and short message service and packet switching\u2019s data service. However, compared to the more accurate prediction performance for voice and text service in circuit switching domain, the state-of-the-art research in packet switching\u2019s data service is still not satisfactory enough. Therefore, a learning and prediction study over application-level data traffic might contribute to understanding data service\u2019s traffic characteristics, thus potentially leading to better prediction performance. Consequently, in this paper, we focus on the analyses for application-level traffic generated by three popular service types (i.e., instantaneous message (IM), web browsing, video). In order to obtain general results, we firstly collect a significant amount of practical traffic records from China Mobile1. By taking advantage of the traffic \u201cbig data\u201d, we then confirm the preciseness of fitting \u03b1-stable models to these typical service types of traffic and demonstrate \u03b1-stable models\u2019 universal existence in cellular network traffic. We later show that \u03b1stable models can be used to leverage the temporally long range dependence, and guide linear algorithms to conduct traffic prediction. Besides, we find that spatial sparsity is also applicable for the application-level traffic, and propose that the predicted traffic should be able to be mapped to some sparse signals. In this regard, benefiting from the latest progress in compressive sensing [8]\u2013[11], we could calibrate the traffic prediction results on the condition of not knowing the transform matrix a priori. Finally, in order to forecast the traffic with the aforementioned characteristics, we formulate the prediction problem by a new framework and then develop a dictionary learning based alternating direction method (ADM) [11] to solve it.\n1It is worthwhile to note here that we also collect another dataset from China Telecom to further verify the effectiveness of the thoughts inside in this paper. Due to the space limitation, we put the results related to China Telecom dataset in a separate file available at http://www.rongpeng.info/files/ sup file twc.pdf.\nar X\niv :1\n60 6.\n04 77\n8v 1\n[ cs\n.N I]\n1 5\nJu n\n20 16\n2"}, {"heading": "A. Related Works", "text": "Due to its apparent significance, there have already existed two research streams toward the fine traffic prediction issue in wired broadband networks and cellular networks [7]. One is based on fitting models (e.g., ON-OFF model [12], ARIMA model [13], FARIMA model [14] , mobility model [15], [16], network traffic model [16], and \u03b1-stable model [17], [18]) to explore the traffic characteristics, such as spatial and temporal relevancies [19] or self-similarity [20], [21], and obtain the future traffic by appropriate prediction methods. The other is based on modern signal processing techniques (e.g., principal components analysis method [22], [23], Kalman filtering method [23], [24] or compressive sensing method [2], [11], [22], [25]) to capture the evolution of traffic. Meanwhile, it is useful to firstly model large-scale traffic vectors as sparse linear combinations of basis elements. Therefore, some dictionary learning method [26] is necessary to learn and construct the basis sets or dictionaries.\nHowever, the existing traffic prediction methods in this microscopic case still lag behind the diverse requirements of various application scenarios. Firstly, most of them still focus on the traffic of all data services [27], and seldom shed light on a specific type of services (e.g., video, web browsing, IM, etc). Secondly, the existing prediction methods usually follow the analysis results in wired broadband networks like the \u03b1-stable models2 [28], [29] or the often accompanied self-similarity [20] to forecast future traffic values [14], [18], [21]. However, since cellular networks have more stringent constraints on radio resources [30], relatively expensive billing polices and different user behaviors due to mobility [31] and thus exhibit distinct traffic characteristics, the corresponding results need to be validated before being directly applied to cellular networks [7]."}, {"heading": "B. Contribution", "text": "Compared to the previous works, this paper, belonging to one of the pioneering works toward application-level traffic analyses, depends on a large amount of practical records (as summarized in Table I and Table II) from China Mobile, and provides the following key insights:\n\u2022 Firstly, this paper revisits \u03b1-stable models, and confirms their accuracy to model the application-level cellular network traffic for all three service types (i.e., IM, web browsing, video). Moreover, this paper shows the application-level traffic obeys the sparse property, and demonstrates the distinct characteristics among different service types. Therefore, the paper contributes to a general understanding of the cellular network traffic. \u2022 Secondly, in order to encompass and explore these aforementioned characteristics, this paper provides a traffic prediction framework in Fig. 1. Specifically, the proposed framework consists of an \u201c\u03b1-Stable Model & Prediction\u201d module to generate coarse prediction results, a \u201cSparsity\n2In this paper, the term \u201c\u03b1-stable models\u201d is interchangeable with \u03b1-stable distributions.\nSparsity &\nDictionary Learning\nAlternating\nDirection Method\n-stable Models\n& Prediction\nThe Framework\nThe remainder of the paper is organized as follows. In Section II, we firstly present some necessary background of required mathematical tools. In Section III, we introduce the dataset for traffic prediction analyses, and later talk about the characteristics (i.e., \u03b1-stable models, and spatial sparsity) of the application-level dataset. In Section IV, we propose a new traffic prediction framework and its corresponding solution. Section V evaluates the proposed schemes and presents the validity and effectiveness. Finally, we conclude this paper in Section VI.\nNotation: In the sequel, bold lowercase and uppercase letters (e.g., x and X) denote a vector and a matrix, respectively. (\u00b7)T denotes a transpose operation of a matrix or vector. \u2016x\u20160 is an l0-norm, counting the number of non-zero entries in x, while an lp-norm \u2016x\u2016p, p \u2265 1 of a 1 \u00d7 n vector x = (x1, \u00b7 \u00b7 \u00b7 , xn) is defined by p \u221a\u2211n i |xi|p. The operation \u3008x,y\u3009 denotes the summation operation of elementwise multiplication in x and y with the same size. sgn(x) with respect to x \u2208 R is defined as sgn(x) = x/|x| when x 6= 0; and sgn(x) = 0 when x = 0."}, {"heading": "II. MATHEMATICAL BACKGROUND", "text": ""}, {"heading": "A. \u03b1-Stable Models", "text": "Following the generalized central limit theorem, \u03b1-stable models manifest themselves in the capability to approximate the distribution of normalized sums of a relatively large number of independent identically distributed random variables [32] and lead to the accumulative property. Besides, \u03b1-stable models produce strong bursty results with properties of heavy tailed distributions and long range dependence. Therefore, they arise in a natural way to characterize the traffic in wired broadband networks [33], [34] and have been exploited in resource management analyses [35], [36]. \u03b1-stable models, with few exceptions, lack a closed-form expression of the probability density function (PDF), and are generally specified by their characteristic functions. Definition 1. A random variable T is said to obey \u03b1-stable models if there are parameters 0 < \u03b1 \u2264 2, \u03c3 \u2265 0, \u22121 \u2264 \u03b2 \u2264\n3\n1, and \u00b5 \u2208 R such that its characteristic function is of the following form:\n\u03a6(\u03c9) = E(exp j\u03c9T )\n=  exp { \u2212\u03c3\u03b1|\u03c9|\u03b1 ( 1\u2212 j\u03b2(sgn(\u03c9)) tan \u03c0\u03b1 2 ) + j\u00b5\u03c9 } , \u03b1 6= 1; exp { \u2212\u03c3|\u03c9| ( 1 + j 2\u03b2\n\u03c0 (sgn(\u03c9)) ln |\u03c9|\n) + j\u00b5\u03c9 } , \u03b1 = 1.\n(1) Here, the function E(\u00b7) represents the expectation operation with respect to a random variable. \u03b1 is called the characteristic exponent and indicates the index of stability, while \u03b2 is identified as the skewness parameter. \u03b1 and \u03b2 together determine the shape of the models. Moreover, \u03c3 and \u00b5 are called scale and shift parameters, respectively. In particular, if \u03b1 = 2, \u03b1-Stable models reduce to Gaussian distributions.\nFurthermore, for an \u03b1-stable modeled random variable T , there exists a linear relationship between the parameter \u03b1 and the function \u03a8(\u03c9) = ln {\u2212Re [ln (\u03a6(\u03c9))]} as\n\u03a8(\u03c9) = ln {\u2212Re [ln (\u03a6(\u03c9))]} = \u03b1 ln(\u03c9) + \u03b1 ln(\u03c3), (2)\nwhere the function Re(\u00b7) calculates the real part of the input variable.\nUsually, it\u2019s challenging to prove whether a dataset follows a specific distribution, especially for \u03b1-stable models without a closed-form expression for the PDF. Therefore, when a dataset is said to satisfy \u03b1-stable models, it usually means the dataset is consistent with the hypothetical distribution and the corresponding properties. In other words, the validation needs to firstly estimate parameters of \u03b1-stable models from the given dataset, and then compare the real distribution of the dataset with the estimated \u03b1-stable model [34]. Specifically, the corresponding parameters in \u03b1-stable models can be determined by maximum likelihood methods, quantile methods, or sample characteristic function methods [33], [34]."}, {"heading": "B. Sparse Representation and Dictionary Learning", "text": "In recent years, sparsity methods or the related compressive sensing (CS) methods have been significantly investigated [8]\u2013[11]. Mathematically, sparsity methods aim to tackle this sparse signal recovery problem in the form of\nmin \u2016s\u20160, s.t. y = Ds,\n(3)\nor min \u2016s\u20160,\ns.t. \u2016y \u2212Ds\u2016 \u2264 . (4)\nHere, s denotes a sparse signal vector while y denotes a measurement vector based on a transform matrix or dictionary D. Besides, is a predefined integer indicating the sparsity. By leveraging the embedded sparsity in the signals, sparsity methods could successfully recover the sparse signal with a high probability, depending on a small number of measurements fewer than that required in Nyquist sampling theorem. Basis pursuit (BP) [37], one of typical sparsity methods, solves the problem in terms of maximizing a posterior (MAP) criterion\nTABLE I DATASET 1 UNDER STUDY\nIM (Weixin) Web Browsing (HTTP) Video (QQLive)\nTraffic Resolution (Collection Interval) 5 min 5 min 5 min\nDuration 1 day 1 day 1 day\nNo. of Active Cells 2292 4507 4472\nLocation Info. (Latitude & Longitude) Yes Yes Yes\nby relaxing the l0-norm to an l1-norm. On the other hand, orthogonal matching pursuit (OMP) [38] greedily achieves the final outcome in a sequential manner, by computing inner products between the signal and dictionary columns, and possibly solving them using the least square criterion.\nFor sparsity methods above, there usually exists an assumption that the transform matrix or dictionary D is already known or fixed. However, in spite of their computation simplicity, such pre-specified transform matrices like Fourier transforms and overcomplete wavelets might not be suitable to lead to a sparse signal [39]. Consequently, some researchers proposed to design D based on learning [26], [39]. In other words, during the sparse signal recovery procedure, machine learning and statistics are leveraged to compute the vectors in D from the measurement vector y, so as to grant more flexibility to get a sparse representation s from y. Mathematically, dictionary learning methods would yield a final transform matrix by alternating between a sparse computation process based on the current dictionary and a dictionary update process to approach the measurement vector."}, {"heading": "III. APPLICATION-LEVEL TRAFFIC DATASET AND ITS CHARACTERISTICS", "text": ""}, {"heading": "A. Traffic Dataset Description", "text": "In this paper, our datasets are based on a significant number of practical traffic records from China Mobile in Hangzhou, an eastern provincial capital in China via the Gb interface of 2G/3G cellular networks or S1 interface of 4G cellular networks [40]. Specifically, these records are concerned with the packets from the Layer 7 (i.e., Application Layer) of OSI model. Specifically, the datasets encompass\n4 nearly 6000 cells\u2019 location records3 with more than 7 million subscribers involved. The datasets also contain the information like timestamp, corresponding cell ID, and trafficrelated application name, by taking advantage of matching traffic records with specific protocols. In particular, we can determine web browsing service and video service by the applied HTTP protocol and streaming protocol respectively, while we assume traffic records to belong to the IM service, after fitting them to learning results of regular IM packet pattern. Moreover, the traffic volume could be calculated after aggregating packets to each influx base station. Notably, the paper aims to predict the traffic volume in each BS instead of the entire cellular network.\nAccording to the traffic resolution (e.g., the traffic collection interval, namely 5 minutes and 30 minutes), the collected data can be sorted into two categories. Table I summarizes the information of per 5-minute traffic records collected on September 9th, 2014 with Weixin/Wechat4, HTTP Web Browsing, and QQLive Video5 selected as the representatives of these three service types. Here, the term \u201cno. of active cells\u201d refers to the number of cells where a specific type of service happened. Similarly, Table II lists the corresponding details of per 30- minute traffic records from July 14th, 2014 to July 27th, 2014 with QQ6, HTTP Web Browsing, and QQLive Video as the representatives, respectively.\nBased on the datasets in Table I and Table II, Fig. 2 illustrates the traffic variations generated by these applications in the randomly selected cells. Indeed, the phenomena in Fig. 2 universally exist in other individual cells and lead to the following insight.\nRemark 1. Different services exhibit distinct traffic characteristics. IM and HTTP web browsing services frequently produce traffic loads; while distinct from them, video service with more sporadic activities may generate more significant traffic loads.\nFor simplicity of representation, we introduce a traffic vector x, whose entries archives the volume of traffic in one given cell at different moments. Furthermore, by augmenting the traffic vectors for different cells, we refer to a traffic matrix X to denote the traffic records in an area of interest. Then, every row vector of traffic matrix indicates traffic loads at one specific cell with respect to the time while every column vector reflects volumes of traffic of several adjacent cells at one specific moment. Specifically, for a traffic resolution \u2206t, X(i, t) in a traffic matrix X denotes traffic loads of cell i from t to t+ \u2206t.\n3Indeed, at one specific location, there might exist several cells operating on different frequencies or modes. For simplicity of representation, in the following analyses, we merge the information for different cells at the same location into one.\n4Weixin/Wechat provides a Whatsapp-alike instant messaging service developed by Tencent Inc., and is one of the most popular mobile social applications in China with more than 400 million active users.\n5QQLive Video is a popular live streaming video platform in China. 6QQ is another instant messaging service developed by Tencent Inc. with more than 800 million active users. Due to some practical reasons, per 30- minute Weixin traffic records are unavailable. Therefore, Table II includes QQ\u2019s traffic records.\n00:00 12:00 00:00 0\n2000\n4000\nTime\nT ra\nff ic\nV o\nlu m e (M B it )\n(a) IM: Dateset 1\n00:00 12:00 00:00 0\n200\n400\n600\nTime\nT ra\nff ic\nV o\nlu m e (M B it )\n(c) Web Browsing: Dateset 1\n00:00 12:00 00:00 0\n50\n100\nTime\nT ra\nff ic\nV o\nlu m e (M B it )\n(e) Video: Dateset 1\n07/10 07/15 07/20 07/25 07/30 0\n5000\n10000\nDay\nT ra\nff ic\nV o\nlu m e (M B it )\n(a) IM: Dateset 2\n07/10 07/15 07/20 07/25 07/30 0\n5000\n10000\n15000\nDay\nT ra\nff ic\nV o\nlu m e (M B it )\n(c) Web Browsing: Dateset 2\n07/10 07/15 07/20 07/25 07/30 0\n5\n10\n15 x 10\n4\nDay\nT ra\nff ic\nV o\nlu m e (M B it )\n(e) Video: Dateset 2\nFig. 2. The traffic variations of applications in different service types in the randomly selected (single) cells.\nRemark 2. Traffic prediction can be regarded as the procedure to obtain a column vector x\u0302p = X\u0302(:, t)T at a future moment t, based on the already known traffic records. Each entry x\u0302p(\u00b7) in x\u0302p corresponds to the future traffic in one cell."}, {"heading": "B. The \u03b1-Stable Modeled and Sparse Properties", "text": "In this section, we examine the results of fitting the application-level dataset to \u03b1-stable models. Firstly, in Table III and Table IV , we list the parameter fitting results using quantile methods [41], when we take into consideration the traffic records in three randomly selected cells (each for one service type) of Table I and Table II.\nAfterwards, we use the \u03b1-stable models, produced by the aforementioned estimated parameters, to generate some random variable, and compare the induced cumulative distribution function (CDF) with the exact (empirical) one. Fig. 3 presents the corresponding comparison between the simulated results\n5 0 0.2 0.4 0.6 0.8 1 0 0.2 0.4 0.6 0.8 1 (a) IM\nFitting CDF\nR e a l C\nD F\n0 0.2 0.4 0.6 0.8 1 0\n0.2\n0.4\n0.6\n0.8\n1 (b) Web Browsing\nFitting CDF\nR e a l C\nD F\n0.5 0.6 0.7 0.8 0.9 1 0.5\n0.6\n0.7\n0.8\n0.9\n1 (c) Video\nFitting CDF\nR e a l C\nD F\nReal Line (Dataset 1)\nReal Line (Dataset 2)\nTrend Line\nFig. 3. For different service types, \u03b1-stable model fitting results versus the real (empirical) ones in terms of the cumulative distribution function (CDF).\nand the real ones. Recalling the statements in Section II-A, if the simulated dataset has the same or approximately same distribution as the real one, the empirical dataset could be deemed as \u03b1-stable modeled. Therefore, Fig. 3 indicates the traffic records in these selected areas could be simulated by \u03b1-stable models.\nOn the other hand, recalling the statements in Section II, for an \u03b1-stable modeled random variable X , there exists a linear relationship between the parameter \u03b1 and the function \u03a8(\u03c9) = ln {\u2212Re [ln (\u03a6(\u03c9))]}. Thus, we fit the estimated parameter \u03b1 with the computing function \u03a8(\u03c9), and provide the preciseness error CDF for all the cells in Fig. 4. According to Fig. 4, the normalized fitting errors for 80% cells in both datasets are less than 0.02. Therefore, the practical applicationlevel traffic records follow the property of \u03b1-stable models (in Eq. (2)), and further enhance the validation results by Fig. 3. Meanwhile, different application-level traffic exhibits different fitting accuracy. In that regard, the video traffic in Fig. 4(c) has the minimal fitting error, while the fitting error of the web browsing traffic in Fig. 4(b) is largest. Moreover, since a larger traffic resolution means a confluence of more application-level traffic packets and could better demonstrate the accumulative property of \u03b1-stable models, the fitting error quickly decreases along with the increase in traffic resolution.\nRemark 3. Due to their generality, \u03b1-stable models are suitable to characterize the application-level traffic loads in cellular networks, even though it might not be the most accurate one.\nIndeed, the universal existence of \u03b1-stable models also implies the self-similarity of application-level traffic [20]. Hence, in the following sections, it is sufficient to only present and discuss the results from Dataset 1 in Table I. On the other hand, the phenomena that application-level traffic universally obeys \u03b1-stable models can be explained as follows. Our previous study [42] unveiled that the message length of one individual IM activity follows a power-law distribution. Meanwhile, the traffic distribution within one cell can be regarded as the\n0 0.01 0.02 0.03 0.04 0.05 0\n0.2\n0.4\n0.6\n0.8\n1 (a) IM\nNormalized Fitting Error\nC D\nF\n0 0.02 0.04 0.06 0\n0.2\n0.4\n0.6\n0.8\n1 (b) Web Browsing\nNormalized Fitting Error\nC D\nF\n0 0.02 0.04 0.06 0\n0.2\n0.4\n0.6\n0.8\n1 (c) Video\nNormalized Fitting Error\nC D\nF\nDataset 1\nDataset 2\nFig. 4. The preciseness error CDF for all the cells after fitting \u03a8(\u03c9) with respect to ln(\u03c9) to a linear function.\naccumulation of lots of IM activities. Moreover, according to the generalized central limit theorem [43], the sum of a number of random variables with power-law distributions decreasing as |x|\u2212\u03b1\u22121 where 0 < \u03b1 < 2 (and therefore having infinite variance) will tend to an \u03b1-stable model as the number of summands grows. Hence, the application-level traffic within one cell follows \u03b1-stable models.\nAdditionally, data traffic in wired broadband networks [22] and voice and text traffic in circuit switching domain of cellular networks [2] prove to possess the spatio-temporal sparsity characteristic. Indeed, the application-level traffic spatially possesses this sparse property as well. Fig. 5 depicts the traffic density in 10AM and 4PM in randomly selected dense urban areas. As Fig. 5 shows, there appear a limited number of traffic hotspots. But these hotspots are closed and correlated with each other. Comparatively, there exists a larger area with less traffic. This spatially clustering property is also consistent with the findings in [19], and proves the traffic\u2019s spatial sparsity. Moreover, the number of hotspots for video service is smallest, which indicates QQLive Video traffic has the strongest sparsity.\nRemark 4. The application-level traffic dataset further validates that the traffic for different service types of applications follows a spatially sparse property. Besides, compared to IM and web browsing service, video service exhibits the strongest sparsity."}, {"heading": "IV. APPLICATION-LEVEL TRAFFIC PREDICTION FRAMEWORK", "text": "Section III unveils that the application-level cellular network traffic could be characterized by \u03b1-stable models and obey sparse property. In this section, we aim to fully take advantage of these results, and propose a new framework in Fig. 1 to predict the traffic. The proposed framework consists of three modules. Among them, the \u201c\u03b1-Stable Model & Prediction\u201d module would take advantage of the already known traffic knowledge to learn and distill the parameters in \u03b1-stable\n6\nmodels and provide a coarse prediction result. Meanwhile, the \u201cSparsity & Dictionary Learning\u201d module imposes constraints to make the final prediction results satisfy the spatial sparsity. But, these two modules inevitably add multiple parameters without known a priori, and thus need specific mathematical operations to obtain a solution. Hence, the proposed framework also contain a \u201cAlternating Direction Method\u201d module to iteratively process the other modules and yield the final result."}, {"heading": "A. Problem Formulation", "text": "Previous sections unearth several important characteristics in application-level traffic in cellular networks, including spatial sparsity, and temporally modeling by \u03b1-stable processes. All these factors could be leveraged for forecasting the future traffic vector x\u0302p.\n\u2022 Temporal modeling component. As Section III-B states, the application-level traffic loads follow \u03b1-stable models. Therefore, benefiting from the substantial body of works towards \u03b1-stable model based linear prediction [28], [34], coarse prediction results can be achieved by computing linear prediction coefficients in terms of the least mean square error criterion, the minimum dispersion criterion, or the covariation orthogonal criterion [17]. Due to its simplicity and comparatively low variability, the covariation orthogonal criterion [17], [18] is chosen in this paper to demonstrate the \u03b1-stable based linear prediction performance. Without loss of generality, assume that there exist N cells in the area of interest. For a cell i \u2208 N with a known nlength traffic vector x(i) = (x(i)(1), \u00b7 \u00b7 \u00b7 , x(i)(n)), x\u0302\u03b1(i) in \u03b1-stable models-based predicted traffic vector x\u0302\u03b1 =\n(x\u0302\u03b1(1), \u00b7 \u00b7 \u00b7 ) is approximated by\nx\u0303\u03b1(i) = m\u2211 j=1 a(i)(j)x(i)(n+ 1\u2212 j), (5)\nwith 1 < m \u2264 n, where a(i) = (a(i)(1), \u00b7 \u00b7 \u00b7 , a(i)(m)) denotes the prediction coefficients by \u03b1-stable modelsbased linear prediction algorithms. For example, in order to make the 1-step-ahead linear prediction x\u0303\u03b1(i) covariation orthogonal to x(i)(t),\u2200t \u2208 {1, \u00b7 \u00b7 \u00b7 , n}, coefficient ai,\u2200i \u2208 {1, \u00b7 \u00b7 \u00b7 ,m} should be given as [34]\nai = m\u2211 l=1  n\u2211 j=max(i,l) xj\u2212(l\u22121)x <\u03b1\u22121> j\u2212(i\u22121)  \u00d7\n n\u2211 j=l+k xjx <\u03b1\u22121> j\u2212k\u2212(l\u22121)  (6)\nHere, the signed-power \u03bd<\u03b1\u22121> = |\u03bd|(\u03b1\u22121)sgn(\u03bd). For simplicity of representation, the terminology \u201c(n = 36,m = 10, k = 1)-linear prediction\u201d is used to denote a prediction method, which firstly utilizes n = 36 consecutive traffic records in one randomly selected cell, then calculates m = 10 prediction coefficients, and finally predicts the traffic value at the next (i.e., k = 1) moment. \u2022 Noise component. For any prediction algorithm, there dooms to exist some prediction error. Therefore, final traffic prediction vector x\u0302p is approximated by x\u0302\u03b1 plus\n7 \u22121500 \u22121000 \u2212500 0 500 1000 1500 0 0.2 0.4 0.6 0.8\nPrediction Error (MBit)\nP D\nF IM Prediction Error Gaussian Fitting Result\n\u2212250 \u2212200 \u2212150 \u2212100 \u221250 0 50 100 150 200 250 0\n0.2\n0.4\n0.6\n0.8\nPrediction Error (MBit)\nP D\nF\nWeb Browsing\nPrediction Error Gaussian Fitting Result\n\u22122000 \u22121500 \u22121000 \u2212500 0 500 1000 1500 2000 0\n0.5\n1\nPrediction Error (MBit)\nP D\nF\nVideo\nPrediction Error Gaussian Fitting Result\n7There are two reasons leading to the assumption that noise is Gaussian distributed. Firstly, Gaussian distributed noise is widely used to characterize the fitting error between models and practical data. Secondly, we conducted a experiment to examine the prediction performance of a simple (n = 36,m = 10, k = 1)-linear prediction procedure, and found that the prediction procedure could well predict the traffic trend. However, there would exist some gap between the real traffic trace and the predicted one. Fortunately, as shown in Fig. 6, the prediction error can be approximated by the Gaussian distribution.\noptimization problem. Moreover, the exact representation of the dictionary, which the previous sparsity analyses do not mention, remains a problem and would be solved later.\nTherefore, it is natural to consider the original dataset as a mixture of these effects and propose a new framework to combine these two components together to get a superior forecasting performance.\nIn order to capture the temporal \u03b1-stable modeled variations while keeping the spatial sparsity, a new framework is proposed as follows:\nmin x\u0302p,x\u0302\u03b1,z,D,s\n\u2016x\u0302\u03b1 \u2212 x\u0303\u03b1\u201622 + \u03bb1\u2016z\u201622 + \u03bb2\u2016x\u0302p \u2212Ds\u201622,\ns.t. x\u0302p = x\u0302\u03b1 + z, (11) \u2016s\u20160 \u2264 .\nDue to the nonconvexity of l0-norm, the constraints in Eq. (11) are not directly tractable. Thanks to the sparsity methods discussed in Section II-B, an l1-norm relaxation is employed to make the problem convex while still preserving the sparsity property [44]. Therefore, Eq. (11) can be reformulated as\nmin x\u0302p,x\u0302\u03b1,z,D,s\n\u2016x\u0302\u03b1 \u2212 x\u0303\u03b1\u201622 + \u03bb1\u2016z\u201622 + \u03bb2\u2016x\u0302p \u2212Ds\u201622,\ns.t. x\u0302p = x\u0302\u03b1 + z, (12) \u2016s\u20161 \u2264 \u03b5.\nwhere \u03b5 is a predefined constraint, similar to .\nRemark 5. This proposed framework integrates the temporal modeling and spatial correlation together. Moreover, by adjusting \u03bb1 and \u03bb2 to some extreme values, it\u2019s easy to show that the framework in Eq. (12) is closely tied to some typical methods in other references.\n\u2022 If \u03bb1 and \u03bb2 are extremely small, the framework is simplified to a simple \u03b1-stable linear prediction method [17], [18]. \u2022 If \u03bb2 is extremely large, the spatial sparsity factor dominates in the framework [22]."}, {"heading": "B. Optimization Algorithm", "text": "In order to optimize the generalized framework in Eq. (12), we first reformulate Eq. (12) by taking advantage of the augmented Lagrangian function [45] and then develop an alternating direction method (ADM) [11] to solve it. Specifically, the corresponding augmented Lagrangian function can be formulated as\nL(x\u0302p, x\u0302\u03b1, z,D, s,m, \u03b3, \u03b7) , \u2016x\u0302\u03b1 \u2212 x\u0303\u03b1\u201622 + \u03bb1\u2016z\u201622 + \u03bb2\u2016x\u0302p \u2212Ds\u201622\n+\u3008m, x\u0302p \u2212 x\u0302\u03b1 \u2212 z\u3009 (13) +\u03b3 \u00b7 \u2016s\u20161 (14) +\u03b7 \u00b7 \u2016x\u0302p \u2212 x\u0302\u03b1 \u2212 z\u201622, (15)\nwhere the operation \u3008x,y\u3009 denotes the summation operation of element-wise multiplication in x and y with the same size. Besides, m and \u03b3 are the Lagrangian multipliers, while \u03b7 is a factor for the penalty term. Essentially, the augmented Lagrangian function includes the original objective,\n8 two Lagrange multiplier terms (i.e., Eq. (13) and Eq. (14)), and one penalty term converted from the equality constraint (i.e., Eq. (15)). Specifically, introducing Lagrange multipliers conveniently converts an optimization problem with equality constraints into an unconstrained one. Moreover, for any optimal solution that minimizes the (augmented) Lagrangian function, the partial derivatives with respect to the Lagrange multipliers must be zero [46]. Additionally, the penalty terms enforce the original equality constraints. Consequently, the original equality constraints are satisfied. Besides, by including Lagrange multiplier terms as well as the penalty terms, it\u2019s not necessary to iteratively increase \u03b7 to \u221e to solve the original constrained problem, thereby avoiding ill-conditioning [45].\nThe ADM algorithm progresses in an iterative manner. During each iteration, we alternate among the optimization of the augmented function by varying each one of (x\u0302p, x\u0302\u03b1, z,D, s,m, \u03b3, \u03b7) while fixing the other variables. Correspondingly, we present the reasons for each iteration of the ADM algorithm (i.e., Algorithm 2). Indeed, the ADM algorithm involves the following steps:\n1) Find x\u0302\u03b1 to minimize the augmented Lagrangian function L(x\u0302p, x\u0302\u03b1, z,D, s,m, \u03b3, \u03b7) with other variables fixed. Removing the fixed items, the objective turns into\narg min x\u0302\u03b1 \u2016x\u0302\u03b1 \u2212 x\u0303\u03b1\u201622 + \u3008m, x\u0302p \u2212 x\u0302\u03b1 \u2212 z\u3009\n+ \u03b7 \u00b7 \u2016x\u0302p \u2212 x\u0302\u03b1 \u2212 z\u201622,\nwhich can be further reformulated as\narg min x\u0302\u03b1\n1 \u03b7 \u00b7\u2016x\u0302\u03b1\u2212x\u0303\u03b1\u201622+\u2016x\u0302\u03b1\u2212(x\u0302p\u2212z+ m 2\u03b7 )\u201622. (16)\nLetting Jx\u0302\u03b1 = x\u0302p \u2212 z + m2\u03b7 and setting the gradient of the objective function in Eq. (16) to be zero, it yields\nx\u0302\u03b1 = 1\n\u03b7 + 1 \u00b7 (x\u0303\u03b1 + \u03b7 \u00b7 Jx\u0302\u03b1). (17)\n2) Find z to minimize the augmented Lagrangian function L(x\u0302p, x\u0302\u03b1, z,D, s,m, \u03b3, \u03b7) with other variables fixed. The corresponding mathematical formula is\narg min z \u03bb1\u2016z\u201622 + \u3008m, x\u0302p \u2212 x\u0302\u03b1 \u2212 z\u3009\n+ \u03b7 \u00b7 \u2016x\u0302p \u2212 x\u0302\u03b1 \u2212 z\u201622.\nSimilarly, it can be reformulated as\narg min x\u0302\u03b1 \u03bb1 \u03b7 \u00b7 \u2016z\u201622 + \u2016z \u2212 (x\u0302p \u2212 x\u0302\u03b1 + m 2\u03b7 )\u201622. (18)\nLetting Jz = x\u0302p \u2212 x\u0302\u03b1 + m2\u03b7 and setting the gradient of the objective function in Eq. (18) to be zero, it yields\nz = 1\n\u03bb1/\u03b7 + 1 \u00b7 Jz. (19)\n3) Find x\u0302p to minimize the augmented Lagrangian function L(x\u0302p, x\u0302\u03b1, z,D, s,m, \u03b3, \u03b7) with other variables fixed. It gives\narg min x\u0302p\n\u03bb2\u2016x\u0302p \u2212Ds\u201622 + \u3008m, x\u0302p \u2212 x\u0302\u03b1 \u2212 z\u3009\n+ \u03b7 \u00b7 \u2016x\u0302p \u2212 x\u0302\u03b1 \u2212 z\u201622.\nThat is\narg min x\u0302p \u03bb2 \u03b7 \u00b7 \u2016x\u0302p \u2212Ds\u201622 + \u2016x\u0302p \u2212 (x\u0302\u03b1 + z \u2212 m\n2\u03b7 )\u201622. (20)\nDefine Jx\u0302p = x\u0302\u03b1 + z \u2212 m2\u03b7 and set the corresponding gradient in Eq. (20) to be zero. It becomes\nx\u0302p = 1 / ( \u03bb2 \u03b7 + 1) \u00b7 (\u03bb2 \u03b7 Ds + Jx\u0302p). (21)\n4) Find D and s to minimize the augmented Lagrangian function L(x\u0302p, x\u0302\u03b1, z,D, s,m, \u03b3, \u03b7) with other variables fixed. In fact, the objective function turns into\narg min D,s\n\u03bb2\u2016x\u0302p \u2212Ds\u201622 + \u03b3 \u00b7 \u2016s\u20161. (22)\nObviously, this optimization problem in Eq. (22) is exactly the sparse signal recovery problem without the dictionary a priori in Section II-B. Inspired by the dictionary learning methodology (namely the means to learn the dictionary or basis sets of large-scale data) in [26], the corresponding solution alternatively determines D and s and thus involves two sub-procedures, namely online learning algorithm [26] and LARS-lasso algorithm [47]. Algorithm 1 provides the skeleton of this solution.\nAlgorithm 1 The Sparse Signal Recovery Algorithm without a Predetermined Dictionary\ninitialize the dictionary D as an input dictionary D(0) (which could be the dictionary learned in last calling this Algorithm), the number of iterations for learning a dictionary as T , two auxiliary matrices A(0) \u2208 RK\u00d7K and B(0) \u2208 RK\u00d7K with all elements therein equaling zero.\n1: for t = 1 to T do 2: Sparse coding: computing s(t) using LARS-Lasso al-\ngorithm [47] to obtain\ns(t) = arg min s \u03bb2\u2016x\u0302p \u2212D(t\u22121)s\u201622 + \u03b3 \u00b7 \u2016s\u20161. (23)\n3: Update A(t) according to\nA(t) \u2190 A(t) + s(t)(s(t))T .\n4: Update B(t) according to\nB(t) \u2190 B(t) + x\u0302p(s(t))T .\n5: Dictionary Update: computing D(t) online learning algorithm [26] to obtain\nD(t) = arg min D \u03bb2\u2016x\u0302p \u2212Ds(t)\u201622 + \u03b3 \u00b7 \u2016s(t)\u20161\n= arg min D Tr(DTDA(t))\u2212 2Tr(DTB(t)). (24)\n6: end for 7: return the learned dictionary D(t) and the sparse coding\nvector s(t).\nIn order to update the dictionary in Eq. (24), the proposed sparse signal recovery algorithm utilizes the\n9 concept of stochastic approximation, which is firstly introduced and mathematically proved convergent to a stationary point in [26]. On the other hand, based on the learned dictionary, the concerted effort to recover a sparse signal could be exploited. As mentioned above, the well known LARSlasso algorithm [47], which is a forward stagewise regression algorithm and gradually finds the most suitable solution along a equiangular path among the already known predictors, is used here to solve the problem in Eq. (23). Meanwhile, it is worthwhile to note that other compressive sensing algorithms [38] could also be used here.\n5) Update estimate for the Lagrangian multiplier m according to steepest gradient descent method [48], namely m\u2190m+\u03b7 \u00b7 (x\u0302p\u2212 x\u0302\u03b1\u2212z). Similarly, update estimate \u03b3 by \u03b3 \u2190 \u03b3 + \u03b7 \u00b7 \u2016s\u20161. 6) Update \u03b7 \u2190 \u03b7 \u00b7 \u03c1. In Algorithm 2, we summarize the steps during each iteration. Notably, without loss of generality, consider a known traffic vector x(0, \u00b7 \u00b7 \u00b7 , t) of a given cell at different moments (0, \u00b7 \u00b7 \u00b7 , t). Then, we could estimate the \u03b1-stable related parameters according to maximum likelihood methods, quantile methods, or sample characteristic function methods in [33], [34]. Afterwards, we could conduct Algorithm 2 to predict the traffic volume at moment t + 1. Similarly, we need reestimate the \u03b1-stable related parameters according to methods in [33], [34], in terms of the traffic vector x(0, \u00b7 \u00b7 \u00b7 , t+1), and perform Algorithm 2 to predict the traffic volume at moment t+2. It can be observed that, compared to Algorithm 1, which is an application of the lines in [26], Algorithm 2 is made up of some additional iterative procedures to procure the parameters without known a priori. Besides, most steps involved in Algorithm 2 are deterministic vector computations and thus computationally efficient. Therefore, the whole framework could effectively yield the traffic forecasting results."}, {"heading": "V. PERFORMANCE EVALUATION", "text": "We validate the prediction accuracy improvement of our proposed framework in Algorithm 2 relying on the practical traffic dataset. Specifically, we choose the traffic load records of these three service types of applications generated in 113 cells within a randomly selected region from Dataset 1. Moreover, we intentionally divide the traffic dataset into two part. One is used to learn and distill the parameters related to traffic characteristics, and the other part is to conduct the experiments to verify and validate the accuracy of the proposed framework in Algorithm 2. Specifically, we compare our prediction x\u0302p with the ground truth x in terms of the normalized mean absolute error (NMAE) [11], which is defined as\nNMAE = \u2211N i=1 |x\u0302p(i)\u2212 x(i)|\u2211N\ni=1 |x(i)| . (25)\nBesides, we utilize LARS-lasso algorithm [47] and OMP [38] respectively to perform the sparse recovery process, and choose the \u03b1-stable model based (36,10,1)-linear prediction algorithm in Section IV-A as the performance baseline. In\nAlgorithm 2 The Dictionary Learning-based Alternating Direction Method initialize x\u0302p, x\u0302\u03b1, z, D, s, m, \u03b3, \u03b7 according to x\u0302 (0) p , x\u0302 (0) \u03b1 ,\nz(0), D(0), s(0), m(0), \u03b3(0), \u03b7(0), and the number of iterations T . Compute x\u0303\u03b1 according to \u03b1-stable model based linear prediction algorithms [28], [34].\n1: for t = 1 to T do 2: Update x\u0302\u03b1 according to x\u0302\n(t) \u03b1 \u2190 1\u03b7(t\u22121)+1 \u00b7(\nx\u0303\u03b1 + \u03b7 (t\u22121) \u00b7 ( x\u0302 (t\u22121) p \u2212 z(t\u22121) + m (t\u22121)\n2\u03b7(t\u22121)\n)) .\n3: Update z according to z(t) \u2190 1 \u03bb1/\u03b7(t\u22121)+1 \u00b7( x\u0302 (t\u22121) p \u2212 x\u0302(t)\u03b1 + m (t\u22121)\n2\u03b7(t\u22121)\n) .\n4: Update x\u0302p according to x\u0302 (t) p \u2190 1 / ( \u03bb2 \u03b7(t\u22121)\n+ 1) \u00b7( \u03bb2\n\u03b7(t\u22121) D(t\u22121)s(t\u22121) + x\u0302 (t) \u03b1 + z(t) \u2212 m\n(t\u22121) 2\u03b7(t\u22121)\n) .\n5: Update D and s according to sparse signal recovery algorithm (i.e., Algorithm 1). In particular, use two sub-procedures namely online learning algorithm [26] and LARS-lasso algorithm [47] to update D and s, respectively. 6: Update m according to m(t) \u2190 m(t\u22121) + \u03b7(t\u22121) \u00b7 (x\u0302 (t) p \u2212 x\u0302(t)\u03b1 \u2212 z(t)). 7: Update \u03b3 by \u03b3(t) \u2190 \u03b3(t\u22121) + \u03b7(t\u22121) \u00b7 \u2016s(t)\u20161. 8: Update \u03b7 by \u03b7(t) \u2190 \u03b7(t\u22121) \u00b7 \u03c1, here \u03c1 is an iteration ratio. 9: end for\n10: return the predicted traffic vector x\u0302p.\nother words, we would exploit traffic records in the last three hours to train the parameters of \u03b1-stable models and predict traffic loads in the next 5 minutes.\nAs described in Algorithm 2, most of the parameters could be set easily and tuned dynamically within the framework. Therefore, we can benefit from this advantage and only need to examine the performance impact of few parameters, namely \u03bb1, \u03bb2, \u03b3 and \u03b7, by dynamically adjusting them. By default, we set \u03bb1 = 10, \u03bb2 = 1, \u03b3 = 1 and \u03b7 = 10\u22124, and the number of iterations8 in Algorithm 2 and sparse signal recovery algorithm (i.e., Algorithm 1) to be 20 and 3, respectively. We also use (36,10,1)-linear prediction algorithm in Section IV-A to provide the \u201ccoarse\u201d prediction results x\u0303\u03b1, and examine the corresponding performance improvement of the proposed ADM framework with different sparse signal recovery algorithm (i.e., LARS-Lasso algorithm [47] and OMP algorithm [38]). Besides, we impose no prior constraints on D, s, and z, and set them as zero vectors.\nFig. 7 gives a performance comparison in terms of NMAE, when we predict the traffic loads under the default settings. In order to provide a more comprehensive comparison, the simulations run in both busy moments (i.e., 9AM, 12PM, and 4PM) and idle ones (i.e., 6AM and 9PM) of one day. As\n8Actually, throughout the literature, the stopping criterion could be a predefined sufficiently large number, just as we did in our manuscript. Meanwhile, the iterative process could stop if the results between two consecutive iterations are sufficiently small. In our study, these predefined few iterations could still yield satisfactory good prediction performance.\n10\ndepicted in Fig. 7, the proposed framework significantly outperforms the \u03b1-stable model based (36,10,1)-linear prediction algorithm. In particular, the NMAE of the proposed framework can be as 12% small (e.g., prediction for 12PM video traffic) as that for the classical linear algorithm. This performance improvement can be interpreted as the gain by exploiting the embedded sparsity in traffic and taking account of the originally existing prediction error of linear prediction. Moreover, it can be observed that in most cases, different sparse signal recovery algorithm has little impact on the prediction accuracy. Therefore, the applications of the proposed framework could pay little attention to the involved sparsity methods.\nNext, we further evaluate the performance of our proposed ADM framework with LARS-Lasso algorithm and present more detailed sensitivity analyses in Fig. 8, Fig. 9, and Fig. 10, by varying \u03bb1, \u03bb2, and \u03b7. Fig. 8 shows that the prediction\naccuracy nearly stays the same irrespective of \u03bb1. This means that the noise component has limited contribution to the corresponding performance. It also implies that the choice of \u03bb1 could be flexible when we apply the framework in practice.\nAccording to Fig. 9, the influence of \u03bb2 is comparatively more obvious and even diverges for different service types. Specifically, a larger \u03bb2 has a slightly negative impact on predicting the traffic loads for IM and web browsing service, but it contributes to the prediction of video service. Recalling the sparsity analyses in Section III-B, video service demonstrates the strongest sparsity. Meanwhile, a larger \u03bb2 means to put more emphasis on the importance of sparsity. Hence, compared to the other two service types, it becomes natural to result in a better performance for video service, when \u03bb2 becomes larger. It\u2019s worthwhile to note here that, in Eq. (14), \u03bb2 and \u03b3 are coupled together as well, and should have inverse performance impact. Therefore, due to the space limitation, the performance impact of \u03b3 is omitted here.\nFig. 10 depicts the performance variation with respect to \u03b7, which is similar to that with respect to \u03bb2. But, a larger\n11\n\u03b7 has a positive impact on predicting the traffic loads for IM and web browsing service, but it degrades the prediction performance of video service. This phenomenon is potentially originated from the very distinct characteristics of these three services types (e.g., different \u03b1-stable models\u2019 parameters and different sparsity representation), and needs a further careful investigation. However, it safely comes to the conclusion that the proposed framework provides a superior and robust performance than the classical linear algorithm."}, {"heading": "VI. CONCLUSION", "text": "In this paper, we collected the application-level traffic data from one operator in China. With the aid of this practical traffic data, we re-confirmed several important statistical characteristics like temporally \u03b1-stable modeled property and spatial sparsity. Afterwards, we proposed a traffic prediction framework, which takes advantage of the already known traffic knowledge to distill the parameters related to aforementioned traffic characteristics and forecasts future traffic results bearing the same characteristics. We also developed a dictionary learning-based alternating direction method to solve the framework, and manifested the effectiveness and robustness of our algorithm through extensive simulation results."}], "references": [{"title": "Cisco Visual Networking Index: Global Mobile Data Traffic Forecast Update, 2012\u20132017", "author": ["Cisco"], "venue": "Feb. 2013. [Online]. Available: http://www.cisco.com/en/US/solutions/collateral/ ns341/ns525/ns537/ns705/ns827/white paper c11-520862.html", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2013}, {"title": "Energy savings scheme in radio access networks via compressive sensing-based traffic load prediction", "author": ["R. Li", "Z. Zhao", "X. Zhou", "H. Zhang"], "venue": "Trans. Emerg. Telecommun. Technol. (ETT), vol. 25, no. 4, pp. 468\u2013478, Apr. 2014.", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2014}, {"title": "Opportunistic traffic scheduling in cellular data networks", "author": ["U. Paul", "M. Buddhikot", "S. Das"], "venue": "Proc. IEEE DySPAN 2012, Bellevue, WA, USA, 2012, pp. 339\u2013348.", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2012}, {"title": "Device- Specific Traffic Characterization for Root Cause Analysis in Cellular Networks", "author": ["P. Romirer-Maierhofer", "M. Schiavone", "A. DAlconzo"], "venue": "Traffic Monitoring and Analysis, ser. Lecture Notes in Computer Science, M. Steiner, P. Barlet-Ros, and O. Bonaventure, Eds. Springer International Publishing, Apr. 2015, no. 9053, pp. 64\u201378. [Online]. Available: http://link.springer.com/chapter/10.1007/ 978-3-319-17172-2 5", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2015}, {"title": "Cell zooming for cost-efficient green cellular networks", "author": ["Z. Niu", "Y. Wu", "J. Gong", "Z. Yang"], "venue": "IEEE Commun. Mag., vol. 48, no. 11, pp. 74\u201379, Nov. 2010.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2010}, {"title": "TANGO: traffic-aware network planning and green operation", "author": ["Z. Niu"], "venue": "IEEE Wireless Commun., vol. 18, no. 5, pp. 25 \u201329, Oct. 2011.", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2011}, {"title": "The prediction analysis of cellular radio access network traffic: From entropy theory to networking practice", "author": ["R. Li", "Z. Zhao", "X. Zhou", "J. Palicot", "H. Zhang"], "venue": "IEEE Commun. Mag., vol. 52, no. 6, pp. 238\u2013244, Jun. 2014.", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2014}, {"title": "Compressive sensing [lecture notes", "author": ["R.G. Baraniuk"], "venue": "IEEE Signal Process. Mag., vol. 24, no. 4, pp. 118\u2013121, Jul. 2007. [Online]. Available: http://omni.isr.ist.utl.pt/\u223caguiar/CS notes.pdf", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2007}, {"title": "Compressed sensing: A tutorial", "author": ["J. Romberg", "M. Wakin"], "venue": "Proc. IEEE SSP Workshop 2007, Madison, Wisconsin, Aug. 2007. [Online]. Available: http://people.ee.duke.edu/\u223cwillett/SSP//Tutorials/ ssp07-cs-tutorial.pdf", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2007}, {"title": "Compressed sensing", "author": ["D. Donoho"], "venue": "IEEE Trans. Inf. Theory, vol. 52, no. 4, pp. 4036\u20134048, Apr. 2006.", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2006}, {"title": "Robust Network Compressive Sensing", "author": ["Y.-C. Chen", "L. Qiu", "Y. Zhang", "G. Xue", "Z. Hu"], "venue": "Proc. ACM Mobicom 2014, Maui, Hawaii, USA, Sep. 2014.", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2014}, {"title": "IEEE 802.16m Evaluation Methodology Document (EMD)", "author": ["I. . B.W.A.W. Group"], "venue": "Jul. 2008. [Online]. Available: http://ieee802.org/16", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2008}, {"title": "Network traffic modeling and prediction with ARIMA/GARCH", "author": ["B. Zhou", "D. He", "Z. Sun", "W.H. Ng"], "venue": "Proc. HET-NETs Conf., Ilkley, UK, Jul. 2005, 00033.", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2005}, {"title": "Long-range dependence and heavy-tail modeling for teletraffic data", "author": ["O. Cappe", "E. Moulines", "J.-C. Pesquet", "A. Petropulu", "Y. Xueshi"], "venue": "IEEE Signal Process. Mag., vol. 19, no. 3, pp. 14\u201327, May 2002.", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2002}, {"title": "Mobility modeling and analytical solution for spatial traffic distribution in wireless multimedia networks", "author": ["F. Ashtiani", "J. Salehi", "M. Aref"], "venue": "IEEE J. Sel. Area. Comm., vol. 21, no. 10, pp. 1699 \u2013 1709, Dec. 2003.", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2003}, {"title": "Spatial traffic estimation and characterization for mobile communication network design", "author": ["K. Tutschku", "P. Tran-Gia"], "venue": "IEEE Journal on Selected Areas in Communications, vol. 16, no. 5, pp. 804\u2013811, Jun. 1998.", "citeRegEx": "16", "shortCiteRegEx": null, "year": 1998}, {"title": "A new hybrid network traffic prediction method", "author": ["L. Xiang", "X. Ge", "C. Liu", "L. Shu", "C. Wang"], "venue": "Proc. IEEE Globecom 2010, Miami, Florida, USA, Dec. 2010.", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2010}, {"title": "A new prediction method of alpha-stable processes for self-similar traffic", "author": ["X. Ge", "S. Yu", "W.-S. Yoon", "Y.-D. Kim"], "venue": "Proc. IEEE Globecom 2004, Dallas, Texas, USA, Nov. 2004.", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2004}, {"title": "Geospatial and temporal dynamics of application usage in cellular data networks", "author": ["M.Z. Shafiq", "L. Ji", "A.X. Liu", "J. Pang", "J. Wang"], "venue": "IEEE Trans. Mob. Comput., 2014. [Online]. Available: http://myweb.uiowa.edu/mshafiq/files/spatialApp TMC.pdf", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2014}, {"title": "Self-similarity in World Wide Web traffic: evidence and possible causes", "author": ["M. Crovella", "A. Bestavros"], "venue": "IEEE/ACM Trans. Netw., vol. 5, no. 6, pp. 835\u2013846, Dec. 1997.", "citeRegEx": "20", "shortCiteRegEx": null, "year": 1997}, {"title": "On the self-similar nature of ethernet traffic", "author": ["W.E. Leland", "M.S. Taqqu", "W. Willinger", "D.V. Wilson"], "venue": "IEEE/ACM Trans. Netw., vol. 2, no. 1, pp. 1\u201315, Feb. 1994.", "citeRegEx": "21", "shortCiteRegEx": null, "year": 1994}, {"title": "Spatio-temporal compressive sensing and internet traffic matrices", "author": ["Y. Zhang", "M. Roughan", "W. Willinger", "L. Qiu"], "venue": "Proc. ACM SIGCOMM 2009, Barcelona, Spain, Aug. 2008.", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2009}, {"title": "Traffic matrices: balancing measurements, inference and modeling", "author": ["A. Soule", "A. Lakhina", "N. Taft"], "venue": "Proc. ACM SIGMETRICS 2005, Banff, Alberta, Canada, Jun. 2005.", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2005}, {"title": "Kalman filter for short-term load forecasting: an hourly predictor of municipal load", "author": ["M.C. Falvo", "M. Gastaldi", "A. Nardecchia", "A. Prudenzi"], "venue": "Proc. IASTED ASM 2007, Palma de Mallorca, Spain, Aug. 2007.", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2007}, {"title": "GM-PAB: a grid-based energy saving scheme with predicted traffic load guidance for cellular networks", "author": ["R. Li", "Z. Zhao", "Y. Wei", "X. Zhou", "H. Zhang"], "venue": "Proc. IEEE ICC 2012, Ottawa, Canada, Jun. 2012.", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2012}, {"title": "Online Learning for Matrix Factorization and Sparse Coding", "author": ["J. Mairal", "F. Bach", "J. Ponce", "G. Sapiro"], "venue": "J. Mach. Learn. Res., vol. 11, pp. 19\u2013 60, Mar. 2010.", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2010}, {"title": "Learning probabilistic models of cellular network traffic with applications to resource management", "author": ["U. Paul", "L. Ortiz", "S.R. Das", "G. Fusco", "M.M. Buddhikot"], "venue": "Proc. IEEE DySPAN 2014, McLean, VA, USA, Apr. 2014.", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2014}, {"title": "Minimum Dispersion and Unbiasedness: \u2019Best\u2019 Linear Predictors for Stationary ARMA a-Stable Processes", "author": ["J.B. Hill"], "venue": "University of Colorado at Boulder, Discussion Papers in Economics Working Paper No. 00-06, Sep. 2000.", "citeRegEx": "28", "shortCiteRegEx": null, "year": 2000}, {"title": "Network heavy traffic modeling using alpha-stable self-similar processes", "author": ["A. Karasaridis", "D. Hatzinakos"], "venue": "IEEE Trans. Commun., vol. 49, no. 7, pp. 1203\u20131214, Jul. 2001.", "citeRegEx": "29", "shortCiteRegEx": null, "year": 2001}, {"title": "Characterizing radio resource allocation for 3g networks", "author": ["F. Qian", "Z. Wang", "A. Gerber", "Z.M. Mao", "S. Sen", "O. Spatscheck"], "venue": "Proc. ACM SIGCOMM 2010, New York, NY, USA, May 2010.", "citeRegEx": "30", "shortCiteRegEx": null, "year": 2010}, {"title": "Mobility: A Double-Edged Sword for HSPA Networks: A Large-Scale Test on Hong Kong Mobile HSPA Networks", "author": ["F.P. Tso", "J. Teng", "W. Jia", "D. Xuan"], "venue": "Proc. ACM Mobihoc 2010, Sep. 2010.", "citeRegEx": "31", "shortCiteRegEx": null, "year": 2010}, {"title": "Stable Non-Gaussian Random Processes: Stochastic Models with Infinite Variance", "author": ["G. Samorodnitsky"], "venue": "New York: Chapman and Hall/CRC,", "citeRegEx": "32", "shortCiteRegEx": "32", "year": 1994}, {"title": "Use of alphastable self-similar stochastic processes for modeling traffic in broadband networks", "author": ["J.R. Gallardo", "D. Makrakis", "L. Orozco-Barbosa"], "venue": "Proc. SPIE Conf. P. Soc. Photo-Opt. Ins, vol. 3530, Boston. Massachusetts, Nov. 1998.", "citeRegEx": "33", "shortCiteRegEx": null, "year": 1998}, {"title": "On the testing for alpha-stable distributions of network traffic", "author": ["X. Ge", "G. Zhu", "Y. Zhu"], "venue": "Comput. Commun., vol. 27, no. 5, pp. 447\u2013457, Mar. 2004.", "citeRegEx": "34", "shortCiteRegEx": null, "year": 2004}, {"title": "Resource Reservation for Self-Similar Data Traffic in Cellular/WLAN Integrated Mobile Hotspots", "author": ["W. Song", "W. Zhuang"], "venue": "Proc. IEEE ICC 2010, Cape Town, South Africa, May 2010.  12", "citeRegEx": "35", "shortCiteRegEx": null, "year": 2010}, {"title": "Spectrum resource allocation for wireless packet access with application to advanced cellular Internet service", "author": ["J.C.-I. Chuang", "N. Sollenberger"], "venue": "IEEE J. Sel. Area. Comm., vol. 16, no. 6, pp. 820\u2013829, Aug. 1998.", "citeRegEx": "36", "shortCiteRegEx": null, "year": 1998}, {"title": "Atomic decomposition by basis pursuit", "author": ["S.S. Chen", "D.L. Donoho", "M.A. Saunders"], "venue": "SIAM J. Sci. Comput., vol. 20, no. 1, pp. 33\u201361, Aug. 1998.", "citeRegEx": "37", "shortCiteRegEx": null, "year": 1998}, {"title": "Orthogonal matching pursuit: recursive function approximation with applications to wavelet decomposition", "author": ["Y. Pati", "R. Rezaiifar", "P.S. Krishnaprasad"], "venue": "Proc. ACSSC 1993, Pacific Grove, CA, USA, Nov. 1993.", "citeRegEx": "38", "shortCiteRegEx": null, "year": 1993}, {"title": "K-SVD: An Algorithm for Designing Overcomplete Dictionaries for Sparse Representation", "author": ["M. Aharon", "M. Elad", "A. Bruckstein"], "venue": "IEEE Trans. Signal Process., vol. 54, no. 11, pp. 4311\u20134322, Nov. 2006.", "citeRegEx": "39", "shortCiteRegEx": null, "year": 2006}, {"title": "The Predictability of Cellular Networks Traffic", "author": ["X. Zhou", "Z. Zhao", "R. Li", "Y. Zhou", "H. Zhang"], "venue": "Proc. IEEE ISCIT 2012, Gold Coast, Australia, Oct. 2012.", "citeRegEx": "40", "shortCiteRegEx": null, "year": 2012}, {"title": "Simple consistent estimators of stable distribution parameters", "author": ["J.H. McCulloch"], "venue": "Commun. Stat. Simulat., vol. 15, no. 4, pp. 1109\u20131136, Jan. 1986.", "citeRegEx": "41", "shortCiteRegEx": null, "year": 1986}, {"title": "Understanding the Nature of Social Mobile Instant Messaging in Cellular Networks", "author": ["X. Zhou", "Z. Zhao", "R. Li", "Y. Zhou", "J. Palicot", "H. Zhang"], "venue": "IEEE Commun. Lett., vol. 18, no. 3, pp. 389 \u2013 392, Mar. 2014.", "citeRegEx": "42", "shortCiteRegEx": null, "year": 2014}, {"title": "Towards robust deconvolution of low-dose perfusion CT: sparse perfusion deconvolution using online dictionary learning", "author": ["R. Fang", "T. Chen", "P.C. Sanelli"], "venue": "Med. Image Anal., vol. 17, no. 4, pp. 417\u2013428, May 2013.", "citeRegEx": "44", "shortCiteRegEx": null, "year": 2013}, {"title": "Augmented Lagrangian method", "author": ["Wikipedia"], "venue": "Oct. 2014. [Online]. Available: http://en.wikipedia.org/w/index.php?title=Augmented Lagrangian method", "citeRegEx": "45", "shortCiteRegEx": null, "year": 2014}, {"title": "Least angle regression", "author": ["B. Efron", "T. Hastie", "I. Johnstone", "R. Tibshirani"], "venue": "Ann. Stat., vol. 32, pp. 407\u2013499, 2004.", "citeRegEx": "47", "shortCiteRegEx": null, "year": 2004}, {"title": "Reinforcement learning: An introduction", "author": ["R. Sutton", "A. Barto"], "venue": null, "citeRegEx": "48", "shortCiteRegEx": "48", "year": 1998}], "referenceMentions": [{"referenceID": 0, "context": "From a macroscopic perspective, it provides the commonly believed result that mobile Internet will witness a 1000-folded traffic growth in the next 10 years [1], which is acting as a crucial anchor for the design of next-generation cellular network architecture and embedded algorithms.", "startOffset": 157, "endOffset": 160}, {"referenceID": 1, "context": "On the other hand, the fine traffic prediction on a daily, hourly or even minutely basis could contribute to the optimization and management of cellular networks like energy savings [2], opportunistic scheduling [3], and network anomaly detection [4] .", "startOffset": 182, "endOffset": 185}, {"referenceID": 2, "context": "On the other hand, the fine traffic prediction on a daily, hourly or even minutely basis could contribute to the optimization and management of cellular networks like energy savings [2], opportunistic scheduling [3], and network anomaly detection [4] .", "startOffset": 212, "endOffset": 215}, {"referenceID": 3, "context": "On the other hand, the fine traffic prediction on a daily, hourly or even minutely basis could contribute to the optimization and management of cellular networks like energy savings [2], opportunistic scheduling [3], and network anomaly detection [4] .", "startOffset": 247, "endOffset": 250}, {"referenceID": 4, "context": "network energy efficiency by dynamically adjusting the cell working status [5], [6], plays an important role in designing greener traffic-aware cellular networks.", "startOffset": 75, "endOffset": 78}, {"referenceID": 5, "context": "network energy efficiency by dynamically adjusting the cell working status [5], [6], plays an important role in designing greener traffic-aware cellular networks.", "startOffset": 80, "endOffset": 83}, {"referenceID": 6, "context": "Our previous research [7] has demonstrated the microscopic traffic predictability in cellular networks for circuit switching\u2019s voice and short message service and packet switching\u2019s data service.", "startOffset": 22, "endOffset": 25}, {"referenceID": 7, "context": "In this regard, benefiting from the latest progress in compressive sensing [8]\u2013[11], we could calibrate the traffic prediction results on the condition of not knowing the transform matrix a priori.", "startOffset": 75, "endOffset": 78}, {"referenceID": 10, "context": "In this regard, benefiting from the latest progress in compressive sensing [8]\u2013[11], we could calibrate the traffic prediction results on the condition of not knowing the transform matrix a priori.", "startOffset": 79, "endOffset": 83}, {"referenceID": 10, "context": "Finally, in order to forecast the traffic with the aforementioned characteristics, we formulate the prediction problem by a new framework and then develop a dictionary learning based alternating direction method (ADM) [11] to solve it.", "startOffset": 218, "endOffset": 222}, {"referenceID": 6, "context": "Due to its apparent significance, there have already existed two research streams toward the fine traffic prediction issue in wired broadband networks and cellular networks [7].", "startOffset": 173, "endOffset": 176}, {"referenceID": 11, "context": ", ON-OFF model [12], ARIMA model [13], FARIMA model [14] , mobility model [15], [16], network traffic model [16], and \u03b1-stable model [17], [18]) to explore the traffic characteristics, such as spatial and temporal relevancies [19] or self-similarity [20], [21], and obtain the future traffic by appropriate prediction methods.", "startOffset": 15, "endOffset": 19}, {"referenceID": 12, "context": ", ON-OFF model [12], ARIMA model [13], FARIMA model [14] , mobility model [15], [16], network traffic model [16], and \u03b1-stable model [17], [18]) to explore the traffic characteristics, such as spatial and temporal relevancies [19] or self-similarity [20], [21], and obtain the future traffic by appropriate prediction methods.", "startOffset": 33, "endOffset": 37}, {"referenceID": 13, "context": ", ON-OFF model [12], ARIMA model [13], FARIMA model [14] , mobility model [15], [16], network traffic model [16], and \u03b1-stable model [17], [18]) to explore the traffic characteristics, such as spatial and temporal relevancies [19] or self-similarity [20], [21], and obtain the future traffic by appropriate prediction methods.", "startOffset": 52, "endOffset": 56}, {"referenceID": 14, "context": ", ON-OFF model [12], ARIMA model [13], FARIMA model [14] , mobility model [15], [16], network traffic model [16], and \u03b1-stable model [17], [18]) to explore the traffic characteristics, such as spatial and temporal relevancies [19] or self-similarity [20], [21], and obtain the future traffic by appropriate prediction methods.", "startOffset": 74, "endOffset": 78}, {"referenceID": 15, "context": ", ON-OFF model [12], ARIMA model [13], FARIMA model [14] , mobility model [15], [16], network traffic model [16], and \u03b1-stable model [17], [18]) to explore the traffic characteristics, such as spatial and temporal relevancies [19] or self-similarity [20], [21], and obtain the future traffic by appropriate prediction methods.", "startOffset": 80, "endOffset": 84}, {"referenceID": 15, "context": ", ON-OFF model [12], ARIMA model [13], FARIMA model [14] , mobility model [15], [16], network traffic model [16], and \u03b1-stable model [17], [18]) to explore the traffic characteristics, such as spatial and temporal relevancies [19] or self-similarity [20], [21], and obtain the future traffic by appropriate prediction methods.", "startOffset": 108, "endOffset": 112}, {"referenceID": 16, "context": ", ON-OFF model [12], ARIMA model [13], FARIMA model [14] , mobility model [15], [16], network traffic model [16], and \u03b1-stable model [17], [18]) to explore the traffic characteristics, such as spatial and temporal relevancies [19] or self-similarity [20], [21], and obtain the future traffic by appropriate prediction methods.", "startOffset": 133, "endOffset": 137}, {"referenceID": 17, "context": ", ON-OFF model [12], ARIMA model [13], FARIMA model [14] , mobility model [15], [16], network traffic model [16], and \u03b1-stable model [17], [18]) to explore the traffic characteristics, such as spatial and temporal relevancies [19] or self-similarity [20], [21], and obtain the future traffic by appropriate prediction methods.", "startOffset": 139, "endOffset": 143}, {"referenceID": 18, "context": ", ON-OFF model [12], ARIMA model [13], FARIMA model [14] , mobility model [15], [16], network traffic model [16], and \u03b1-stable model [17], [18]) to explore the traffic characteristics, such as spatial and temporal relevancies [19] or self-similarity [20], [21], and obtain the future traffic by appropriate prediction methods.", "startOffset": 226, "endOffset": 230}, {"referenceID": 19, "context": ", ON-OFF model [12], ARIMA model [13], FARIMA model [14] , mobility model [15], [16], network traffic model [16], and \u03b1-stable model [17], [18]) to explore the traffic characteristics, such as spatial and temporal relevancies [19] or self-similarity [20], [21], and obtain the future traffic by appropriate prediction methods.", "startOffset": 250, "endOffset": 254}, {"referenceID": 20, "context": ", ON-OFF model [12], ARIMA model [13], FARIMA model [14] , mobility model [15], [16], network traffic model [16], and \u03b1-stable model [17], [18]) to explore the traffic characteristics, such as spatial and temporal relevancies [19] or self-similarity [20], [21], and obtain the future traffic by appropriate prediction methods.", "startOffset": 256, "endOffset": 260}, {"referenceID": 21, "context": ", principal components analysis method [22], [23], Kalman filtering method [23], [24] or compressive sensing method [2], [11], [22], [25]) to capture the evolution of traffic.", "startOffset": 39, "endOffset": 43}, {"referenceID": 22, "context": ", principal components analysis method [22], [23], Kalman filtering method [23], [24] or compressive sensing method [2], [11], [22], [25]) to capture the evolution of traffic.", "startOffset": 45, "endOffset": 49}, {"referenceID": 22, "context": ", principal components analysis method [22], [23], Kalman filtering method [23], [24] or compressive sensing method [2], [11], [22], [25]) to capture the evolution of traffic.", "startOffset": 75, "endOffset": 79}, {"referenceID": 23, "context": ", principal components analysis method [22], [23], Kalman filtering method [23], [24] or compressive sensing method [2], [11], [22], [25]) to capture the evolution of traffic.", "startOffset": 81, "endOffset": 85}, {"referenceID": 1, "context": ", principal components analysis method [22], [23], Kalman filtering method [23], [24] or compressive sensing method [2], [11], [22], [25]) to capture the evolution of traffic.", "startOffset": 116, "endOffset": 119}, {"referenceID": 10, "context": ", principal components analysis method [22], [23], Kalman filtering method [23], [24] or compressive sensing method [2], [11], [22], [25]) to capture the evolution of traffic.", "startOffset": 121, "endOffset": 125}, {"referenceID": 21, "context": ", principal components analysis method [22], [23], Kalman filtering method [23], [24] or compressive sensing method [2], [11], [22], [25]) to capture the evolution of traffic.", "startOffset": 127, "endOffset": 131}, {"referenceID": 24, "context": ", principal components analysis method [22], [23], Kalman filtering method [23], [24] or compressive sensing method [2], [11], [22], [25]) to capture the evolution of traffic.", "startOffset": 133, "endOffset": 137}, {"referenceID": 25, "context": "Therefore, some dictionary learning method [26] is necessary to learn and construct the basis sets or dictionaries.", "startOffset": 43, "endOffset": 47}, {"referenceID": 26, "context": "Firstly, most of them still focus on the traffic of all data services [27], and seldom shed light on a specific type of services (e.", "startOffset": 70, "endOffset": 74}, {"referenceID": 27, "context": "Secondly, the existing prediction methods usually follow the analysis results in wired broadband networks like the \u03b1-stable models2 [28], [29] or the often accompanied self-similarity [20] to forecast future traffic values [14], [18], [21].", "startOffset": 132, "endOffset": 136}, {"referenceID": 28, "context": "Secondly, the existing prediction methods usually follow the analysis results in wired broadband networks like the \u03b1-stable models2 [28], [29] or the often accompanied self-similarity [20] to forecast future traffic values [14], [18], [21].", "startOffset": 138, "endOffset": 142}, {"referenceID": 19, "context": "Secondly, the existing prediction methods usually follow the analysis results in wired broadband networks like the \u03b1-stable models2 [28], [29] or the often accompanied self-similarity [20] to forecast future traffic values [14], [18], [21].", "startOffset": 184, "endOffset": 188}, {"referenceID": 13, "context": "Secondly, the existing prediction methods usually follow the analysis results in wired broadband networks like the \u03b1-stable models2 [28], [29] or the often accompanied self-similarity [20] to forecast future traffic values [14], [18], [21].", "startOffset": 223, "endOffset": 227}, {"referenceID": 17, "context": "Secondly, the existing prediction methods usually follow the analysis results in wired broadband networks like the \u03b1-stable models2 [28], [29] or the often accompanied self-similarity [20] to forecast future traffic values [14], [18], [21].", "startOffset": 229, "endOffset": 233}, {"referenceID": 20, "context": "Secondly, the existing prediction methods usually follow the analysis results in wired broadband networks like the \u03b1-stable models2 [28], [29] or the often accompanied self-similarity [20] to forecast future traffic values [14], [18], [21].", "startOffset": 235, "endOffset": 239}, {"referenceID": 29, "context": "However, since cellular networks have more stringent constraints on radio resources [30], relatively expensive billing polices and different user behaviors due to mobility [31] and thus exhibit distinct traffic characteristics, the corresponding results need to be validated before being directly applied to cellular networks [7].", "startOffset": 84, "endOffset": 88}, {"referenceID": 30, "context": "However, since cellular networks have more stringent constraints on radio resources [30], relatively expensive billing polices and different user behaviors due to mobility [31] and thus exhibit distinct traffic characteristics, the corresponding results need to be validated before being directly applied to cellular networks [7].", "startOffset": 172, "endOffset": 176}, {"referenceID": 6, "context": "However, since cellular networks have more stringent constraints on radio resources [30], relatively expensive billing polices and different user behaviors due to mobility [31] and thus exhibit distinct traffic characteristics, the corresponding results need to be validated before being directly applied to cellular networks [7].", "startOffset": 326, "endOffset": 329}, {"referenceID": 31, "context": "Following the generalized central limit theorem, \u03b1-stable models manifest themselves in the capability to approximate the distribution of normalized sums of a relatively large number of independent identically distributed random variables [32] and lead to the accumulative property.", "startOffset": 239, "endOffset": 243}, {"referenceID": 32, "context": "Therefore, they arise in a natural way to characterize the traffic in wired broadband networks [33], [34] and have been exploited in resource management analyses [35], [36].", "startOffset": 95, "endOffset": 99}, {"referenceID": 33, "context": "Therefore, they arise in a natural way to characterize the traffic in wired broadband networks [33], [34] and have been exploited in resource management analyses [35], [36].", "startOffset": 101, "endOffset": 105}, {"referenceID": 34, "context": "Therefore, they arise in a natural way to characterize the traffic in wired broadband networks [33], [34] and have been exploited in resource management analyses [35], [36].", "startOffset": 162, "endOffset": 166}, {"referenceID": 35, "context": "Therefore, they arise in a natural way to characterize the traffic in wired broadband networks [33], [34] and have been exploited in resource management analyses [35], [36].", "startOffset": 168, "endOffset": 172}, {"referenceID": 33, "context": "In other words, the validation needs to firstly estimate parameters of \u03b1-stable models from the given dataset, and then compare the real distribution of the dataset with the estimated \u03b1-stable model [34].", "startOffset": 199, "endOffset": 203}, {"referenceID": 32, "context": "Specifically, the corresponding parameters in \u03b1-stable models can be determined by maximum likelihood methods, quantile methods, or sample characteristic function methods [33], [34].", "startOffset": 171, "endOffset": 175}, {"referenceID": 33, "context": "Specifically, the corresponding parameters in \u03b1-stable models can be determined by maximum likelihood methods, quantile methods, or sample characteristic function methods [33], [34].", "startOffset": 177, "endOffset": 181}, {"referenceID": 7, "context": "In recent years, sparsity methods or the related compressive sensing (CS) methods have been significantly investigated [8]\u2013[11].", "startOffset": 119, "endOffset": 122}, {"referenceID": 10, "context": "In recent years, sparsity methods or the related compressive sensing (CS) methods have been significantly investigated [8]\u2013[11].", "startOffset": 123, "endOffset": 127}, {"referenceID": 36, "context": "Basis pursuit (BP) [37], one of typical sparsity methods, solves the problem in terms of maximizing a posterior (MAP) criterion TABLE I DATASET 1 UNDER STUDY", "startOffset": 19, "endOffset": 23}, {"referenceID": 37, "context": "On the other hand, orthogonal matching pursuit (OMP) [38] greedily achieves the final outcome in a sequential manner, by computing inner products between the signal and dictionary columns, and possibly solving them using the least square criterion.", "startOffset": 53, "endOffset": 57}, {"referenceID": 38, "context": "However, in spite of their computation simplicity, such pre-specified transform matrices like Fourier transforms and overcomplete wavelets might not be suitable to lead to a sparse signal [39].", "startOffset": 188, "endOffset": 192}, {"referenceID": 25, "context": "Consequently, some researchers proposed to design D based on learning [26], [39].", "startOffset": 70, "endOffset": 74}, {"referenceID": 38, "context": "Consequently, some researchers proposed to design D based on learning [26], [39].", "startOffset": 76, "endOffset": 80}, {"referenceID": 39, "context": "In this paper, our datasets are based on a significant number of practical traffic records from China Mobile in Hangzhou, an eastern provincial capital in China via the Gb interface of 2G/3G cellular networks or S1 interface of 4G cellular networks [40].", "startOffset": 249, "endOffset": 253}, {"referenceID": 40, "context": "Firstly, in Table III and Table IV , we list the parameter fitting results using quantile methods [41], when we take into consideration the traffic records in three randomly selected cells (each for one service type) of Table I and Table II.", "startOffset": 98, "endOffset": 102}, {"referenceID": 19, "context": "Indeed, the universal existence of \u03b1-stable models also implies the self-similarity of application-level traffic [20].", "startOffset": 113, "endOffset": 117}, {"referenceID": 41, "context": "Our previous study [42] unveiled that the message length of one individual IM activity follows a power-law distribution.", "startOffset": 19, "endOffset": 23}, {"referenceID": 21, "context": "Additionally, data traffic in wired broadband networks [22] and voice and text traffic in circuit switching domain of cellular networks [2] prove to possess the spatio-temporal sparsity characteristic.", "startOffset": 55, "endOffset": 59}, {"referenceID": 1, "context": "Additionally, data traffic in wired broadband networks [22] and voice and text traffic in circuit switching domain of cellular networks [2] prove to possess the spatio-temporal sparsity characteristic.", "startOffset": 136, "endOffset": 139}, {"referenceID": 18, "context": "This spatially clustering property is also consistent with the findings in [19], and proves the traffic\u2019s spatial sparsity.", "startOffset": 75, "endOffset": 79}, {"referenceID": 27, "context": "Therefore, benefiting from the substantial body of works towards \u03b1-stable model based linear prediction [28], [34], coarse prediction results can be achieved by computing linear prediction coefficients in terms of the least mean square error criterion, the minimum dispersion criterion, or the covariation orthogonal criterion [17].", "startOffset": 104, "endOffset": 108}, {"referenceID": 33, "context": "Therefore, benefiting from the substantial body of works towards \u03b1-stable model based linear prediction [28], [34], coarse prediction results can be achieved by computing linear prediction coefficients in terms of the least mean square error criterion, the minimum dispersion criterion, or the covariation orthogonal criterion [17].", "startOffset": 110, "endOffset": 114}, {"referenceID": 16, "context": "Therefore, benefiting from the substantial body of works towards \u03b1-stable model based linear prediction [28], [34], coarse prediction results can be achieved by computing linear prediction coefficients in terms of the least mean square error criterion, the minimum dispersion criterion, or the covariation orthogonal criterion [17].", "startOffset": 327, "endOffset": 331}, {"referenceID": 16, "context": "Due to its simplicity and comparatively low variability, the covariation orthogonal criterion [17], [18] is chosen in this paper to demonstrate the \u03b1-stable based linear prediction performance.", "startOffset": 94, "endOffset": 98}, {"referenceID": 17, "context": "Due to its simplicity and comparatively low variability, the covariation orthogonal criterion [17], [18] is chosen in this paper to demonstrate the \u03b1-stable based linear prediction performance.", "startOffset": 100, "endOffset": 104}, {"referenceID": 33, "context": "For example, in order to make the 1-step-ahead linear prediction x\u0303\u03b1(i) covariation orthogonal to x(t),\u2200t \u2208 {1, \u00b7 \u00b7 \u00b7 , n}, coefficient ai,\u2200i \u2208 {1, \u00b7 \u00b7 \u00b7 ,m} should be given as [34]", "startOffset": 177, "endOffset": 181}, {"referenceID": 42, "context": "Thanks to the sparsity methods discussed in Section II-B, an l1-norm relaxation is employed to make the problem convex while still preserving the sparsity property [44].", "startOffset": 164, "endOffset": 168}, {"referenceID": 16, "context": "\u2022 If \u03bb1 and \u03bb2 are extremely small, the framework is simplified to a simple \u03b1-stable linear prediction method [17], [18].", "startOffset": 110, "endOffset": 114}, {"referenceID": 17, "context": "\u2022 If \u03bb1 and \u03bb2 are extremely small, the framework is simplified to a simple \u03b1-stable linear prediction method [17], [18].", "startOffset": 116, "endOffset": 120}, {"referenceID": 21, "context": "\u2022 If \u03bb2 is extremely large, the spatial sparsity factor dominates in the framework [22].", "startOffset": 83, "endOffset": 87}, {"referenceID": 43, "context": "(12) by taking advantage of the augmented Lagrangian function [45] and then develop an alternating direction method (ADM) [11] to solve it.", "startOffset": 62, "endOffset": 66}, {"referenceID": 10, "context": "(12) by taking advantage of the augmented Lagrangian function [45] and then develop an alternating direction method (ADM) [11] to solve it.", "startOffset": 122, "endOffset": 126}, {"referenceID": 43, "context": "Besides, by including Lagrange multiplier terms as well as the penalty terms, it\u2019s not necessary to iteratively increase \u03b7 to \u221e to solve the original constrained problem, thereby avoiding ill-conditioning [45].", "startOffset": 205, "endOffset": 209}, {"referenceID": 25, "context": "Inspired by the dictionary learning methodology (namely the means to learn the dictionary or basis sets of large-scale data) in [26], the corresponding solution alternatively determines D and s and thus involves two sub-procedures, namely online learning algorithm [26] and LARS-lasso algorithm [47].", "startOffset": 128, "endOffset": 132}, {"referenceID": 25, "context": "Inspired by the dictionary learning methodology (namely the means to learn the dictionary or basis sets of large-scale data) in [26], the corresponding solution alternatively determines D and s and thus involves two sub-procedures, namely online learning algorithm [26] and LARS-lasso algorithm [47].", "startOffset": 265, "endOffset": 269}, {"referenceID": 44, "context": "Inspired by the dictionary learning methodology (namely the means to learn the dictionary or basis sets of large-scale data) in [26], the corresponding solution alternatively determines D and s and thus involves two sub-procedures, namely online learning algorithm [26] and LARS-lasso algorithm [47].", "startOffset": 295, "endOffset": 299}, {"referenceID": 44, "context": "1: for t = 1 to T do 2: Sparse coding: computing s using LARS-Lasso algorithm [47] to obtain", "startOffset": 78, "endOffset": 82}, {"referenceID": 25, "context": "5: Dictionary Update: computing D online learning algorithm [26] to obtain", "startOffset": 60, "endOffset": 64}, {"referenceID": 25, "context": "concept of stochastic approximation, which is firstly introduced and mathematically proved convergent to a stationary point in [26].", "startOffset": 127, "endOffset": 131}, {"referenceID": 44, "context": "As mentioned above, the well known LARSlasso algorithm [47], which is a forward stagewise regression algorithm and gradually finds the most suitable solution along a equiangular path among the already known predictors, is used here to solve the problem in Eq.", "startOffset": 55, "endOffset": 59}, {"referenceID": 37, "context": "Meanwhile, it is worthwhile to note that other compressive sensing algorithms [38] could also be used here.", "startOffset": 78, "endOffset": 82}, {"referenceID": 45, "context": "5) Update estimate for the Lagrangian multiplier m according to steepest gradient descent method [48], namely m\u2190m+\u03b7 \u00b7 (x\u0302p\u2212 x\u0302\u03b1\u2212z).", "startOffset": 97, "endOffset": 101}, {"referenceID": 32, "context": "Then, we could estimate the \u03b1-stable related parameters according to maximum likelihood methods, quantile methods, or sample characteristic function methods in [33], [34].", "startOffset": 160, "endOffset": 164}, {"referenceID": 33, "context": "Then, we could estimate the \u03b1-stable related parameters according to maximum likelihood methods, quantile methods, or sample characteristic function methods in [33], [34].", "startOffset": 166, "endOffset": 170}, {"referenceID": 32, "context": "Similarly, we need reestimate the \u03b1-stable related parameters according to methods in [33], [34], in terms of the traffic vector x(0, \u00b7 \u00b7 \u00b7 , t+1), and perform Algorithm 2 to predict the traffic volume at moment t+2.", "startOffset": 86, "endOffset": 90}, {"referenceID": 33, "context": "Similarly, we need reestimate the \u03b1-stable related parameters according to methods in [33], [34], in terms of the traffic vector x(0, \u00b7 \u00b7 \u00b7 , t+1), and perform Algorithm 2 to predict the traffic volume at moment t+2.", "startOffset": 92, "endOffset": 96}, {"referenceID": 25, "context": "It can be observed that, compared to Algorithm 1, which is an application of the lines in [26], Algorithm 2 is made up of some additional iterative procedures to procure the parameters without known a priori.", "startOffset": 90, "endOffset": 94}, {"referenceID": 10, "context": "Specifically, we compare our prediction x\u0302p with the ground truth x in terms of the normalized mean absolute error (NMAE) [11], which is defined as", "startOffset": 122, "endOffset": 126}, {"referenceID": 44, "context": "Besides, we utilize LARS-lasso algorithm [47] and OMP [38] respectively to perform the sparse recovery process, and choose the \u03b1-stable model based (36,10,1)-linear prediction algorithm in Section IV-A as the performance baseline.", "startOffset": 41, "endOffset": 45}, {"referenceID": 37, "context": "Besides, we utilize LARS-lasso algorithm [47] and OMP [38] respectively to perform the sparse recovery process, and choose the \u03b1-stable model based (36,10,1)-linear prediction algorithm in Section IV-A as the performance baseline.", "startOffset": 54, "endOffset": 58}, {"referenceID": 27, "context": "Compute x\u0303\u03b1 according to \u03b1-stable model based linear prediction algorithms [28], [34].", "startOffset": 75, "endOffset": 79}, {"referenceID": 33, "context": "Compute x\u0303\u03b1 according to \u03b1-stable model based linear prediction algorithms [28], [34].", "startOffset": 81, "endOffset": 85}, {"referenceID": 25, "context": "In particular, use two sub-procedures namely online learning algorithm [26] and LARS-lasso algorithm [47] to update D and s, respectively.", "startOffset": 71, "endOffset": 75}, {"referenceID": 44, "context": "In particular, use two sub-procedures namely online learning algorithm [26] and LARS-lasso algorithm [47] to update D and s, respectively.", "startOffset": 101, "endOffset": 105}, {"referenceID": 44, "context": ", LARS-Lasso algorithm [47] and OMP algorithm [38]).", "startOffset": 23, "endOffset": 27}, {"referenceID": 37, "context": ", LARS-Lasso algorithm [47] and OMP algorithm [38]).", "startOffset": 46, "endOffset": 50}], "year": 2016, "abstractText": "Traffic learning and prediction is at the heart of the evaluation of the performance of telecommunications networks and attracts a lot of attention in wired broadband networks. Now, benefiting from the big data in cellular networks, it becomes possible to make the analyses one step further into the application level. In this paper, we firstly collect a significant amount of application-level traffic data from cellular network operators. Afterwards, with the aid of the traffic \u201cbig data\u201d, we make a comprehensive study over the modeling and prediction framework of cellular network traffic. Our results solidly demonstrate that there universally exist some traffic statistical modeling characteristics, including \u03b1-stable modeled property in the temporal domain and the sparsity in the spatial domain. Meanwhile, the results also demonstrate the distinctions originated from the uniqueness of different service types of applications. Furthermore, we propose a new traffic prediction framework to encompass and explore these aforementioned characteristics and then develop a dictionary learning-based alternating direction method to solve it. Besides, we validate the prediction accuracy improvement and the robustness of the proposed framework through extensive simulation results.", "creator": "LaTeX with hyperref package"}}}