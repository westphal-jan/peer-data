{"id": "1606.05336", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "16-Jun-2016", "title": "On the Expressive Power of Deep Neural Networks", "abstract": "We study the expressivity of deep neural networks with random weights. We provide several results, both theoretical and experimental, precisely characterizing their functional properties in terms of the depth and width of the network. In doing so, we illustrate inherent connections between the length of a latent trajectory, local neuron transitions, and network activation patterns between the network (e.g., an area of attention in a network, where the maximum depth is an area of attention in a network and the smallest extent of activity in a network). These findings further support the notion that the neural network is able to serve as a model for general relativity and other relativity-like properties in space, a possible explanation of the phenomenon.\n\n\n\n\nThe neural network is a complex network which consists of a collection of three main components: the functional network, the network of the sub-par neural network, and the deep neural network. The basic basic system is the basic neural network. The system is the basis of the underlying principle of the deep neural network, as in the principle of the deep neural network, in which the neural network is a system of information that is able to transfer between the two systems. In this way, deep neural networks are considered to be independent and are not a part of the system. Moreover, they are not only connected but in part to the structure of an object. There is a natural and fundamental relationship between the two systems, which can be easily explained by the functional network as a whole.\nThe neural network consists of two components: the deep neural network, the deep neural network, the deep neural network, the deep neural network, the deep neural network, the deep neural network, and the deep neural network.\nDeep neural networks, as in the principle of the deep neural network, are the key features of the deep neural network. The deep neural network is a group of two main components: the deep neural network, the deep neural network, and the deep neural network.\nThe deep neural network, as in the principle of the deep neural network, is composed of three separate parts. The basic system is the basic neural network. It is composed of two main components: the deep neural network, the deep neural network, and the deep neural network. The deep neural network is a group of two main components: the deep neural network, the deep neural network, and the deep neural network. The deep neural network is a group of two main components: the deep neural network, the deep neural network, and the deep neural network. The deep neural network", "histories": [["v1", "Thu, 16 Jun 2016 19:55:29 GMT  (1051kb,D)", "http://arxiv.org/abs/1606.05336v1", null], ["v2", "Fri, 24 Jun 2016 20:26:47 GMT  (1789kb,D)", "http://arxiv.org/abs/1606.05336v2", null], ["v3", "Wed, 17 Aug 2016 22:21:25 GMT  (2267kb,D)", "http://arxiv.org/abs/1606.05336v3", null], ["v4", "Mon, 3 Oct 2016 15:44:39 GMT  (951kb,D)", "http://arxiv.org/abs/1606.05336v4", null], ["v5", "Wed, 1 Mar 2017 03:00:26 GMT  (3798kb,D)", "http://arxiv.org/abs/1606.05336v5", null], ["v6", "Sun, 18 Jun 2017 13:24:34 GMT  (4061kb,D)", "http://arxiv.org/abs/1606.05336v6", "Accepted to ICML 2017"]], "reviews": [], "SUBJECTS": "stat.ML cs.AI cs.LG", "authors": ["maithra raghu", "ben poole", "jon m kleinberg", "surya ganguli", "jascha sohl-dickstein"], "accepted": true, "id": "1606.05336"}, "pdf": {"name": "1606.05336.pdf", "metadata": {"source": "CRF", "title": "On the expressive power of deep neural networks", "authors": ["Maithra Raghu", "Ben Poole", "Surya Ganguli"], "emails": [], "sections": [{"heading": null, "text": "We study the expressivity of deep neural networks with random weights. We provide several results, both theoretical and experimental, precisely characterizing their functional properties in terms of the depth and width of the network. In doing so, we illustrate inherent connections between the length of a latent trajectory, local neuron transitions, and network activation patterns. The latter, a notion defined in this paper, is further studied using properties of hyperplane arrangements, which also help precisely characterize the effect of the neural network on the input space. We further show dualities between changes to the latent state and changes to the network weights, and between the number of achievable activation patterns and the number of achievable labellings over input data. We see that the depth of the network affects all of these quantities exponentially, while the width appears at most as a base. These results also suggest that the remaining depth of a neural network is an important determinant of expressivity, supported by experiments on MNIST and CIFAR-10."}, {"heading": "1 Introduction", "text": "Neural network architectures have proven \u201cunreasonably effective\u201d LeCun [2014], Karpathy [2015] on many tasks, including image classification Krizhevsky et al. [2012], identifying particles in high energy physics Baldi et al. [2014], playing Go Silver et al. [2016], and modeling human student learning Piech et al. [2015]. Despite their power, we have limited understanding of how and why neural networks work, and much of this understanding is qualitative and heuristic rather than precise and quantitative. While even qualitative knowledge is helpful, a more formal and fundamental framework is desperately needed to ease the laborious process of designing and training these networks, as well as to enable us to to better predict and protect against their failures.\nIn order to develop theoretical underpinnings for deep networks, it is first necessary to disentangle the many factors that influence their effectiveness. The effectiveness of deep neural networks on real tasks can be interpreted in terms of three broad properties: their trainability, or how well they can be fit to data; their generalizability, or how well they perform on novel examples; and their expressivity, or the set of functions they can compute.\nAll three of these properties are crucial for understanding the performance of neural networks. First, neural networks are useful only to the extent they can be sucessfully trained. Previous work has explored the objective function landscape of neural networks Dauphin et al. [2014], Goodfellow et al. [2014], Choromanska et al. [2014], designed optimization procedures specifically suited to neural networks Martens and Grosse [2015], and proposed architectural components such as ReLUs Nair and Hinton [2010], which we analyze in this paper, specifically designed to make them more trainable.\nar X\niv :1\n60 6.\n05 33\n6v 1\n[ st\nat .M\nL ]\n1 6\nJu n\nNeural network models are also useful only to the degree they can generalize beyond their training set. A bound on generalization error in terms of the number of stochastic gradient descent steps performed is developed in Hardt et al. [2015]. VC dimension provides an upper bound on generalization error, and has been studied for neural networks Sontag [1998], Bartlett and Maass [2003], Bartlett et al. [1998]. In model distillation, more efficient models are trained to emulate costly models which possess better generalization error Hinton et al. [2015].\nIn this paper, we focus on the third of these properties, expressivity \u2014 the power of the class of deep neural networks to represent highly complex functions. We characterize how function complexity, under several natural and related definitions, grows with both width and depth, and use one of the definitions, the activation pattern of a deep network, to provide theory and intuition for how a deep network subdivides input space into regions. We verify our results experimentally on random networks and on experiments on MNIST and CIFAR-10."}, {"heading": "1.1 Expressivity", "text": "The topic of neural network expressivity has a rich history, with different notions of expressivity studied. A particularly popular method of analysis has been examining achievable functions, the set of functions that are expressible by a fixed neural network architecture.\nThis approach has yielded many fascinating results: neural networks have been shown to be universal approximators Hornik et al. [1989], Cybenko [1989], their VC dimension studied Sontag [1998], Bartlett and Maass [2003], Bartlett et al. [1998], and connections between boolean and threshold networks to ReLU networks developed in Maass et al. [1994], Pan and Srikumar [2015]. VC dimension provides an upper bound on one of the properties we measure, and is further discussed in Appendix D.1.\nLooking at achievable functions also provides a method to compare different neural network architectures. A typical approach to this constructs classes of functions from one network architecture that another is incapable of approximating efficiently Eldan and Shamir [2015], Telgarsky [2015], Martens et al. [2013], Bianchini and Scarselli [2014]. This is often used to suggest the benefits of deep networks over shallow networks.\nOther work examines the complexity of a single function computed by a deep network, rather than the complexity of a set of functions. The number of linear regions was introduced as a measure of expressivity in Pascanu et al. [2013]. In Montufar et al. [2014], it was shown that a specific network achieves an exponential number of linear regions with depth. In Section 2.3 we generalize linear regions to network activation patterns, and develop a tight upper bound on the number of achievable activation patterns which the construction in Montufar et al. [2014] saturates.\nMany of these results, while compelling, also highlight a drawback of much of the existing work on expressivity \u2013 conclusions often rely on very particular functions or a specific weight setting of a neural network. In some sense, this misses the ultimate goal of understanding expressivity, which is to provide characteristic properties of networks and functions which apply for any typical task. Addressing this more general question requires determining a class of functions that are representative of the \u2018typical\u2019 or \u2018average\u2019 expressiveness of a particular architecture, and then understanding their salient features.\nRandom networks We consider an average case which arises naturally in the practical usage of neural network architectures, the behavior of networks after random initialization. The expressivity of these random networks is largely unexplored, though a connection between random networks and compositional kernels is developed in Daniely et al. [2016]. The class of random networks enables tractable theoretical and experimental analysis of interrelated aspects of expressivity: trajectory length, neuron transitions, network activation patterns, and number of dichotomies (number of labellings on inputs). We find that all of these properties grow exponentially with depth, but only polynomially with width.\nRandom data That we observe an exponential increase in these measures of expressivity, even when the network acts on random data, is particularly remarkable. A common explanation for the effectiveness of neural networks is that their functional form is well suited to the structure inherent in natural data, suggested empirically by e.g. the benefits of designing translation invariance into\nconvolutional networks LeCun et al. [1998], and explored theoretically in Cohen et al. [2015]. In particular the depth of the neural network is often seen to take advantage of the hierarchical and compositional structure of features in datasets. It is surprising therefore that we see quantifiable advantages arising from deep neural networks even for unstructured data.\nInterrelated measures of expressivity All our measures of network expressivity are linked together through trajectory length. We demonstrate, both theoretically and experimentally, that the length of an input trajectory grows exponentially with the depth of a neural network. We then show a linear relationship between trajectory length and the number of neuron transitions observed for random networks, both experimentally and theoretically in the limit of large network weights. Finally we experimentally demonstrate a connection between the number of neuron transitions and the number of achievable dichotomies (labelings over inputs).\nActivation patterns We further support the relationship between neuron transitions and achievable dichotomies by a novel analysis of the deep network in terms of hyperplane arrangements. We show that the global activation patterns of a deep network partition input space into convex polytopes, such that the same network activation pattern cannot occur more than once along a straight line in input space. We additionally develop a tight upper bound on the number of achievable activation patterns in a deep network.\nRemaining depth All of these results suggest that the importance of a layer in a network is related to the remaining depth of the network after that layer. We support this observation by training single layers in isolation in a random deep network, for classification tasks on MNIST and CIFAR-10. We observe monotonically decreasing performance with decreasing remaining depth \u2013 i.e. the layers with the greatest expressive power are those which occur earliest in the network (Section 4 and Figures 7, 8, 9).\nIn a companion paper Poole et al. [2016] we explore closely related questions by using mean field theory to explore the propagation of curvature through deep networks."}, {"heading": "2 Exponential Expressivity", "text": "In this section, we start by deriving bounds on how the length of a one dimensional trajectory changes as it propagates through a deep network. We establish a fundamental relation between this length and a local notion of expressivity, the number of transitions neurons undergo along an input trajectory. We then link these properties to a global notion of expressivity, the number of activation patterns of the entire network, studied through a novel link to the theory of hyperplane arrangements. These\nquantities are brought together in Section 3, where we study dichotomies \u2013 labelings over inputs. The theoretical and experimental measurements for all of these properties exhibit an exponential dependence on depth but not width. This is illustrated in Figure 1, which shows how the number of dichotomies grows with both depth and width.\nStudying dichotomies in this setting, where we sweep weights in a single layer through a trajectory, leads to the supposition that the expressive power of a particular layer is related to the remaining depth of the network after that layer. This is supported by experiments on networks trained on MNIST and CIFAR-10 (Section 4)."}, {"heading": "2.1 Notation and Definitions", "text": "Let FW denote a neural network. In this section, we consider architectures with input dimension m, n hidden layers all of width k, and a scalar readout layer. (So, FW : Rm \u2192 R.)\nWe use v(d)i to denote the i th neuron in hidden layer d. We also let x = z(0) be an input, h(d) be the hidden representation at layer d, and \u03c6 the non-linearity. The weights and bias are called W (d) and b(d) respectively. So we have the relations\nh(d+1) = W (d)z(d) + b(d), z(d+1) = \u03c6(h(d)). (1)\nOur results mostly examine the cases where \u03c6 is a hard-tanh Collobert and Bengio [2004] or ReLU nonlinearity. All hard-tanh results carry over to tanh with additional technical steps.\nWe list some important definitions below:\n(1) A neuron transitions when it switches linear region in its activation function (i.e. for ReLU, switching between zero and linear regimes, for hard-tanh, switching between negative saturation, unsaturated and positive saturation).\n(2) For hard-tanh, we refer to a sign transition as the neuron switching sign, and a saturation transition as switching from being saturated between \u00b11.\n(3) The Activation Pattern of the entire network is defined by the output regions of every neuron. More precisely, given an input x, we let A(FW , x) be a vector representing the activation region of every hidden neuron in the network. So for a ReLU network FW , we can take A(FW , x) \u2208 {\u22121, 1}nk with \u22121 meaning the neuron is in the zero regime, and 1 meaning it is in the linear regime. For hard-tanh network FW , we can (overloading notation slightly) take A(FW , x) \u2208 {\u22121, 0, 1}nk. The use of this notation will be clear by context.\n(4) Sign Pattern For a hard-tanh network, we also find it useful to consider the sign pattern of the network, SGN (FW , x) \u2208 {\u22121, 1}nk\n(5) Given a set of inputs S, we say a dichotomy over S is a labeling of each point in S as \u00b11.\nRandom networks: We assume the weight matrices of our neural networks are initialized as random Gaussian matrices, with appropriate variance scaling to account for width, i.e. W (d)ij \u223c N (0, \u03c32w/k). We draw biases b (d) i \u223c N (0, \u03c32b ).\nSweeping Input: In the analysis below, we sweep through a one dimensional input trajectory x(t). The results hold for almost any such smooth x(t), provided that at any point x(t), the trajectory direction has some non-zero magnitude perpendicular to x(t)."}, {"heading": "2.2 Neuron transitions and trajectory length", "text": "In this section, we analyze the number of sign transitions of FW , a random hard-tanh neural network, as the input x(t) is swept through a one dimensional trajectory. We rigorously derive how the length of the input curve x(t) changes as it propagates through FW , and then verify experimentally, and derive theoretically for the large weight limit, a linear relation between trajectory length and number of transitions."}, {"heading": "2.2.1 Bound on trajectory length growth", "text": "We would like to understand how the arc length of a one dimensional trajectory x(t) changes as it is pushed through a network FW . We prove: (with a more exact lower bound in the Appendix) Theorem 1. Bound on Growth of Trajectory Length Let FW be a hard tanh random neural network and x(t) a one dimensional trajectory in input space. Define z(d)(x(t)) = z(d)(t) to be the image\nof the trajectory in layer d of FW , and let l(z(d)(t)) = \u222b t \u2223\u2223\u2223\u2223\u2223\u2223dz(d)(t)dt \u2223\u2223\u2223\u2223\u2223\u2223 dt be the arc length of z(d)(t). Then\nE [ l(z(d)(t)) ] \u2265 O   \u03c3w\n(\u03c32w + \u03c3 2 b )\n1/4 \u00b7 \u221a k\u221a\u221a\n\u03c32w + \u03c3 2 b + k\nd  l(x(t))\nAn immediate Corollary for \u03c3b = 0, i.e. no bias, is Theorem 2. Bound on Growth of Trajectory Length Without Bias For FW with zero bias, we have\nE [ l(z(d)(t)) ] \u2265 O (( \u221a \u03c3wk\u221a \u03c3w + k )d) l(x(t))\nThe theorem shows that the image of a trajectory in layer d has grown exponentially in d, with the scaling \u03c3w and width of the network k determining the base. We additionally state and prove a simple O(\u03c3dw) growth upper bound in the Appendix. Figure 2 demonstrates this behavior in simulation, and compares against the bounds. Note also that if the variance of the bias is comparatively too large i.e. \u03c3b >> \u03c3w, then we no longer see exponential growth. This phase transition behavior is explored further in our companion paper Poole et al. [2016].\nThe proof can be found in the Appendix. A rough outline is as follows: to prove this growth rate, instead of working with the integral form, we look at the expected growth of the difference between a point z(d)(t) on the curve and a small perturbation z(d)(t+ dt), from layer d to layer d+ 1. Denoting this quantity\n\u2223\u2223\u2223\u2223\u03b4z(d)(t)\u2223\u2223\u2223\u2223, we derive a recurrence relating \u2223\u2223\u2223\u2223\u03b4z(d+1)(t)\u2223\u2223\u2223\u2223 and \u2223\u2223\u2223\u2223\u03b4z(d)(t)\u2223\u2223\u2223\u2223 which can be composed to give the desired growth rate. Note this growth rate is tight in the limits of large \u03c3w and k.\nThe analysis contains an additional layer of complexity as for tractability, we need statistical independence from the image of the input z(d+1)(t). So we instead form a recursion by looking at the component of the difference perpendicular to the image of the input in that layer, i.e. \u2223\u2223\u2223\u2223\u2223\u2223\u03b4z(d+1)\u22a5 (t)\u2223\u2223\u2223\u2223\u2223\u2223. For a typical trajectory, the perpendicular component preserves a fraction \u221a k\u22121 k of the total trajectory length, and our derived growth rate thus provides a close lower bound, demonstrated in Figure 2(c).\nWe further discuss the numerical properties of this growth rate in particular regimes of interest in Section 2.2.3."}, {"heading": "2.2.2 Relation to number of transitions", "text": "To understand the relation to the number of sign transitions, observe the following: for FW with n hidden layers as above, the linear, one dimensional, readout layer outputs a value by computing the inner product (z(n))TW (n). The sign of the output is then determined by whether this quantity is \u2265 0 or not. In particular, the decision boundary is a hyperplane, with equation (z(n))TW (n) = 0. The number of transitions we see the output neuron make as x(t) is traced in the input is then exactly the number of times z(n)(t) crosses the decision boundary. As FW is a random neural network, with signs of weight entries split purely randomly between \u00b11, we expect to see a direct proportionality between the length of the curve z(n)(t), and the number of times it crosses the decision boundary. This is unequivocally demonstrated by our experimental results in Figure 3. In Theorem 3 we prove this relationship for the large \u03c3w limit. We state it as a conjecture for the general case: Conjecture 1. In a random neural network FW with hard-tanh activations, the expected number of sign transitions induced by a one dimensional curve x(t) in the input is directly proportional to the length of the latent image of the curve, z(n)(t).\nIn Section 3 we discuss the implications of this result for the number of unique dichotomies on inputs we see arising from a class of functions {FW }."}, {"heading": "2.2.3 Trajectory length growth in different regimes", "text": "Returning to the statement of Theorem 2, we can consider what the trajectory length growth (and therefore number of transitions) looks like for different choices of k and \u03c3w. We see that for very large k, the trajectory length is exponential in the depth with base only dependent on \u03c3w. This appears\nintuitively correct, as for very large k, we expect to see a normalizing effect caused by the Central Limit Theorem \u2013 we sum together a very large number of independent random variables (for each neuron), and perform an implicit averaging through setting the variance of each weight coordinate to be \u03c32w/k. For very large \u03c3w, we see that the base of exponential growth is now \u221a k. This \u2018large weight limit\u2019 results in very tractable behaviour, because:\n(1) For any input (bounded away from zero) almost all neurons are saturated\n(2) Any neuron transitioning from 1 to \u22121 or vice versa over an interval x(t), x(t+ \u2206t) does so almost instantaneously. As a consequence, can assume that at most one neuron within a layer is transitioning at any point.\nUnder these two assumptions, we prove:\nTheorem 3. Number of transitions in large weight limit Given FW , a random neural network with hard-tanh activations, in the very large \u03c3w regime, the number of sign transitions of the network as an input x(t) is swept is:\n(i) For \u03c3b = 0: O(( \u221a k)n)\n(ii) For \u03c3b > 0:\nO  \u221ak\u221a 1 +\n\u03c32b \u03c32w n which is a specific case of Conjecture 1. The full proof is in the Appendix, and reduces the problem to one of the relative magnitudes of independent Gaussians."}, {"heading": "2.3 A Global Perspective: Transitions and Activation Patterns", "text": "Previously, we related input trajectory growth to a \u2018local\u2019 notion of expressivity, sign transitions of a single neuron. In fact, this local measure of expressivity has an elegant connection to the \u2018global\u2019 notion of expressivity of the number of activation patterns of a network. We can show, that for many well-behaved trajectories, the number of transitions (which is directly proportional to trajectory length) is exactly equal to the number of unique activation patterns.\nA helpful mental picture is obtained through the language of hyperplane arrangements. A hyperplane arrangement consists of a set of hyperplanes in an ambient space, in our case the input space Rm. An arrangement of this fashion divides up Rm into regions. Hyperplanes relate to neural networks in the following way: consider a ReLU random neural network. Given a particular neuron in the first layer, we can ask for which inputs it is \u2018on\u2019 (the linear regime), and for which inputs it is \u2018off\u2019 (the zero regime). This \u2018boundary\u2019 in the input space is a hyperplane defined by the points x for which the inner product of x with the weights into the neuron (plus a bias term) are zero. Drawing a hyperplane for every neuron in the first layer then gives a hyperplane arrangement, with every region of this arrangement in one-to-one correspondence with a particular subset of neurons being active.\nThe next crucial insight is realizing we can apply a similar analysis to the second layer: in particular, inside a particular region of the hyperplane arrangement defined by the first layer, a linear function of the input passes into the second layer. This means we can consider the on/off patterns of the second layer within this region, which defines another hyperplane arrangement subdividing this particular region into sub-regions. Applying this over all regions gives convex sub-divisions of the input space which are in one to one correspondence with the neuron activation patterns in the first and second layers.\nFigure 4 illustrates this subdivision of input space into convex polytopes. We make this precise in the theorem below, establishing a bijection between activation patterns of FW (for hard tanh and ReLU activations) and regions of a hyperplane arrangement.\nTheorem 4. Regions in Input Space Suppose FW : Rm \u2192 R, with ReLU or hard-tanh activations. The set of activation patterns of FW (activation patterns across all layers in the network) partitions the input space into convex regions (by definition bounded and unbounded polytopes), one for each activation pattern.\nProof. We show inductively that FW partitions the input space into convex polytopes via hyperplanes. Consider the image of the input space under the first hidden layer. Each neuron v(1)i defines hyperplane(s) on the input space: letting W (0)i be the ith column of W\n(0), b(0)i the bias, we have the hyperplane xTW (0)i + bi = 0 for a ReLU and hyperplanes x TW (0) i + bi = \u00b11 for a hard-tanh. Considering all such hyperplanes over neurons in the first layer, we get a hyperplane arrangement in the input space, each polytope corresponding to a specific activation pattern in the first hidden layer.\nNow, assume we have partitioned our input space into convex polytopes with hyperplanes from layers \u2264 d \u2212 1. Consider v(d)i and a specific polytope Ri. Then the activation pattern on layers \u2264 d \u2212 1 is constant on Ri, and so the input to v (d) i on Ri is a linear function of the inputs \u2211 j \u03bbjxj + b and some constant term, comprising of the bias and the output of saturated units. Setting this expression to zero (for ReLUs) or to \u00b11 (for hard-tanh) again gives a hyperplane equation, but this time, the equation is only valid in Ri (as we get a different linear function of the inputs in a different region.) So the defined hyperplane(s) either partition Ri (if they intersect Ri) or the output pattern of v (d) i is also constant on Ri. The theorem then follows.\nThis implies that any one dimensional trajectory x(t), that does not \u2018double back\u2019 on itself (i.e. reenter a polytope it has previously passed through), will not repeat activation patterns. In particular, after seeing a transition (crossing a hyperplane to a different region in input space) we will never return to the region we left. A simple example of such a trajectory is an affine trajectory:\nCorollary 1. Transitions and Output Patterns in an Affine Trajectory For any affine one dimensional trajectory x(t) = x0 + t(x1 \u2212 x0) input into a neural network FW , we partition R 3 t into intervals every time a neuron transitions. No two partitions have the same neuron output pattern on FW .\nWith the picture of the input space split into regions, the next natural question is to ask how many regions we can achieve. In other words, how many neuron output patterns (activation/saturation) can we see over the entire input? Considering a region R in the input space as in the proof of Theorem 4, we see that we have at most k (for ReLUs) or 2k (for hard-tanh) hyperplanes, in R \u2282 Rm. So a first step is determining how many regions we get when we have k hyperplanes in Rm. The question of the number of regions for a specific hyperplane arrangement is well studied Stanley [2011], with\nmany beautiful abstract constructions, such as intersection posets. Here we prove an upper bound on the number of regions induced by a hyperplane arragnement:\nTheorem 5. Upper Bound on Regions in a Hyperplane Arrangement Suppose we have k hyperplanes in Rm - i.e. k equations of form \u03b1ix = \u03b2i. for \u03b1i \u2208 Rm, \u03b2i \u2208 R. Let the number of regions (connected open sets bounded on some sides by the hyperplanes) be r(k,m). Then\nr(k,m) \u2264 m\u2211 i=0 ( k i )\nThe full proof is in the Appendix, and relies on the elegant recursion r(k,m) = r(k \u2212 1,m\u2212 1) + r(k \u2212 1,m) combined with an induction. With this result, we conclude that when considering the effect of layer d on the existing regions, each existing region can be partitioned into at most r(k,m) subregions (or for hard-tanh, r(2k,m).) So an upper bound on the number of regions (and therefore, activation patterns) achievable through this is given by (r(k,m))n, i.e. we have proved:\nTheorem 6. (Tight) Upper bound for Number of Activation Patterns Given a neural network FW , inputs in Rm, with ReLU or hard-tanh activations, and with n hidden layers of width k, the number of activation patterns grows at most like O(kmn) for ReLU, or O((2k)mn) for hard-tanh.\nIn Montufar et al. [2014], it is observed that a (trivial) upper bound is given by 2kmn (all possible subsets of all possible neurons) and a construction of linear regions increasing exponentially with depth is given. This upper bound is exponentially looser than the bound given through regions of a hyperplane arrangement, which is shown asymptotically tight with the construction in Montufar et al. [2014]."}, {"heading": "3 Function Space", "text": ""}, {"heading": "3.1 The Dual Perspective", "text": "In the previous sections, we have examined the effect of depth and width on the expressivity of a neural network when varying an input x along a one dimensional trajectory in input space. The supporting experiments show that even for simple trajectories, e.g. interpolating a line or circular arc, we observe the characteristic properties of exponential increase of curve length, transitions, and output patterns with depth.\nA dual perspective to sweeping the inputs over a trajectory x(t) while holding the weights W (0) fixed, is to instead view the trajectory as sweeping the first layer weights along some trajectory W (0)(t) while holding the inputs x fixed. More formally, if we pick two random inputs, x, x\u2032, and consider the arc x cos(t) + x\u2032 sin(t), our first layer input is xTW cos(t) + x\u2032TW sin(t). Instead, we could view a rotation as picking two random matrices, W,W \u2032, and doing a circular interpolation W cos(t) + W \u2032 sin(t) over weight matrices. Our different inputs are then xTW and xTW \u2032. This is not an exact duality \u2013 there is a trajectory in weight space corresponding to any great circle interpolation in input space, but there is not a trajectory in input space corresponding to any great circle interpolation in weight space. However, it provides a natural dual analogue for random inputs, changing an analysis over sweeping inputs, to a function class F produced by sweeping over weights. Having shifted perspective to consider function classes, a natural question is understanding how heterogeneous this they are. One way to measure this is to consider the set of dichotomies this function class gives over a set of (random) inputs. More precisely, given a set of inputs, S = {x1, .., xs} \u2282 Rm, how many of the 2s possible dichotomies does this function class produce on S?\nFor non-random inputs and non-random functions, this is a well known question closely related to the VC dimension of the function class, and answered by the Sauer-Shelah lemma Sauer [1972]. We discuss this further in Appendix D.1.\nPreviously we\u2019ve seen, theoretically and experimentally, that growth in trajectory length (influenced by depth and width) is linearly related to the number of transitions. Also, if the xi \u2208 S are sufficiently uncorrelated (e.g. random) class label transitions should occur independently for each xi. Together, this would suggest Observation 1. Depth and Expressivity in a Function Class. Given the function class F as above, the number of dichotomies expressible by F over a set of random inputs S by sweeping the first layer weights along a one dimensional trajectory W (0)(t) is exponential in the network depth n.\nThis is unambiguously supported by the growth in the number of dichotomies with depth in Figure 1. This is further supported by an exponential decrease in autocorrelation length in function space, which we derive in the Supplemental Material of our companion paper Poole et al. [2016].\nOur results further suggest the following conjectures: Conjecture 2. As network width k increases, the exploration of the space of dichotomies increasingly resembles a simple random walk on a hypercube with dimension equal to the number of inputs |S|.\nThis conjecture is supported by Figure 5, which compares the number of unique dichotomies achieved by networks of various widths to the number of unique dichotomies achieved by a random walk.\nConjecture 3. The expressive power of a single weight W (d)ij at layer d in a random network F , and for a set of random inputs S, is exponential in the remaining network depth dr = (n \u2212 d). Here expressive power is the number of dichotomies achievable by adjusting only that weight.\nThat is, the expressive power of weights in early layers in a deep hard-tanh network is exponentially greater than the expressive power of weights in later layers. This is supported by the invariance to layer number in the recurrence relations used in all proofs directly involving depth. It is also directly supported by simulation, as illustrated in Figure 6, and by experiments on MNIST and CIFAR10 as illustrated in Figures 7, 8, and 9."}, {"heading": "4 Experiments", "text": "We implemented the random network architecture described in Section 2.1. In separate experiments we then swept an input vector along a great circle trajectory for fixed weights, and swept weights along a great circle trajectory for a fixed set of inputs, as described in Section 3.1. In both cases, the trajectory was subdivided into 106 segments. We repeated this for a grid of network widths k, weight variances \u03c32w, and number of inputs s. Unless otherwise noted, \u03c3b = 0 for all experiments. We repeated each experiment 10 times and averaged over the results. The simulation results are discussed and plotted throughout the text.\nWe also studied the implications of these results for trained networks. In the experiments related to Figures 7, 8, 9 we randomly initialized a neural network, and froze all the layers except for one,\nwhich we trained (on MNIST and CIFAR-10). The results show that better performance results from training lower layers than from training higher layers, which supports the results on the expressive importance of remaining depth (Figure 6.) In our companion paper Poole et al. [2016], we explore the effect of width and total depth when training only the final layer in the network."}, {"heading": "5 Conclusion", "text": "In this paper, we studied the expressivity of neural networks with random weights. This framework enabled an average case analysis, and illustrated the relation between trajectory length, neuron transitions, activation patterns, and achievable classification dichotomies (see Table 1). Our analysis also provided precise theoretical and experimental results on the impact of depth and width on neural networks. In particular, greater depth results in an exponential increase of these values.\nThat exponentially large changes in output can be induced by small changes in the input may help explain the prevalence of adversarial examples Szegedy et al. [2013] in deep networks. Additionally, understanding this structure may inspire new optimization schemes that account for the differing leverage of weights at different layers. The greater leverage stemming from training earlier weights\nmay also motivate novel ways to adapt pre-trained networks to new tasks, beyond simply retraining the top layer (least expressive) weights.\nWe believe further \u2018average case\u2019 study of neural networks will continue to improve our understanding, and inspire practical algorithms and architectures."}, {"heading": "Acknowledgements", "text": "We thank Samy Bengio, Ian Goodfellow, Laurent Dinh, and Quoc Le for extremely helpful discussion."}, {"heading": "A Proofs and additional results from Section 2.2", "text": "Proof of Theorem 2 We prove this result for FW with zero bias for technical simplicity. The result also translates over to FW with bias with a couple of technical modifications.\nA.1 Notation and Preliminary Results\nDifference of points on trajectory Given x(t) = x, x(t + dt) = x + \u03b4x in the trajectory, let \u03b4z(d) = z(d)(x+ \u03b4x)\u2212 z(d)(x) Parallel and Perpendicular Components: Given vectors x, y, we can write y = y\u22a5 + y\u2016 where y\u22a5 is the component of y perpendicular to x, and y\u2016 is the component parallel to x. (Strictly speaking, these components should also have a subscript x, but we suppress it as the direction with respect to which parallel and perpendicular components are being taken will be explicitly stated.)\nThis notation can also be used with a matrix W , see Lemma 1.\nBefore stating and proving the main theorem, we need a few preliminary results.\nLemma 1. Matrix Decomposition Let x, y \u2208 Rk be fixed non-zero vectors, and let W be a (full rank) matrix. Then, we can write\nW = \u2016W\u2016 + \u2016W\u22a5 + \u22a5W\u2016 + \u22a5W\u22a5\nsuch that\n\u2016W\u22a5x = 0 \u22a5W\u22a5x = 0\nyT\u22a5W\u2016 = 0 y T\u22a5W\u22a5 = 0\ni.e. the row space of W is decomposed to perpendicular and parallel components with respect to x (subscript on right), and the column space is decomposed to perpendicular and parallel components of y (superscript on left).\nProof. Let V,U be rotations such that V x = (||x|| , 0..., 0)T and Uy = (||y|| , 0...0)T . Now let W\u0303 = UWV T , and let W\u0303 = \u2016W\u0303\u2016 + \u2016W\u0303\u22a5 + \u22a5W\u0303\u2016 + \u22a5W\u0303\u22a5, with \u2016W\u0303\u2016 having non-zero term exactly W\u030311, \u2016W\u0303\u22a5 having non-zero entries exactly W\u03031i for 2 \u2264 i \u2264 k. Finally, we let \u22a5W\u0303\u2016 have non-zero entries exactly W\u0303i1, with 2 \u2264 i \u2264 k and \u22a5W\u0303\u22a5 have the remaining entries non-zero. If we define x\u0303 = V x and y\u0303 = Uy, then we see that\n\u2016W\u0303\u22a5x\u0303 = 0 \u22a5W\u0303\u22a5x\u0303 = 0\ny\u0303T\u22a5W\u0303\u2016 = 0 y\u0303 T\u22a5W\u0303\u22a5 = 0\nas x\u0303, y\u0303 have only one non-zero term, which does not correspond to a non-zero term in the components of W\u0303 in the equations.\nThen, defining \u2016W\u2016 = UT \u2016W\u0303\u2016V , and the other components analogously, we get equations of the form\n\u2016W\u22a5x = U T \u2016W\u0303\u22a5V x = U T \u2016W\u0303\u22a5x\u0303 = 0\nObservation 2. Given W,x as before, and considering W\u2016, W\u22a5 with respect to x (wlog a unit vector) we can express them directly in terms of W as follows: Letting W (i) be the ith row of W , we have\nW\u2016 = ((W (0))T \u00b7 x)x\n... ((W (k))T \u00b7 x)x  i.e. the projection of each row in the direction of x. And of course\nW\u22a5 = W \u2212W\u2016\nThe motivation to consider such a decomposition of W is for the resulting independence between different components, as shown in the following lemma.\nLemma 2. Independence of Projections Let x be a given vector (wlog of unit norm.) If W is a random matrix with Wij \u223c N (0, \u03c32), then W\u2016 and W\u22a5 with respect to x are independent random variables.\nProof. There are two possible proof methods:\n(a) We use the rotational invariance of random Gaussian matrices, i.e. ifW is a Gaussian matrix, iid entries N (0, \u03c32), and R is a rotation, then RW is also iid Gaussian, entries N (0, \u03c32). (This follows easily from affine transformation rules for multivariate Gaussians.)\nLet V be a rotation as in Lemma 1. Then W\u0303 = WV T is also iid Gaussian, and furthermore, W\u0303\u2016 and W\u0303\u22a5 partition the entries of W\u0303 , so are evidently independent. But thenW\u2016 = W\u0303\u2016V T and W\u22a5 = W\u0303\u22a5V T are also independent.\n(b) From the observation note that W\u2016 and W\u22a5 have a centered multivariate joint Gaussian distribution (both consist of linear combinations of the entries Wij in W .) So it suffices to show that W\u2016 and W\u22a5 have covariance 0. Because both are centered Gaussians, this is equivalent to showing E(< W\u2016,W\u22a5 >) = 0. We have that\nE(< W\u2016,W\u22a5 >) = E(W\u2016WT\u22a5 ) = E(W\u2016WT )\u2212 E(W\u2016WT\u2016 )\nAs any two rows of W are independent, we see from the observation that E(W\u2016WT ) is a diagonal matrix, with the ith diagonal entry just ((W (0))T \u00b7 x)2. But similarly, E(W\u2016WT\u2016 ) is also a diagonal matrix, with the same diagonal entries - so the claim follows.\nIn the following two lemmas, we use the rotational invariance of Gaussians as well as the chi distribution to prove results about the expected norm of a random Gaussian vector.\nLemma 3. Norm of a Gaussian vector Let X \u2208 Rk be a random Gaussian vector, with Xi iid, \u223c N (0, \u03c32). Then\nE [||X||] = \u03c3 \u221a 2 \u0393((k + 1)/2)\n\u0393(k/2)\nProof. We use the fact that if Y is a random Gaussian, and Yi \u223c N (0, 1) then ||Y || follows a chi distribution. This means that E(||X/\u03c3||) = \u221a 2\u0393((k + 1)/2)/\u0393(k/2), the mean of a chi distribution with k degrees of freedom, and the result follows by noting that the expectation in the lemma is \u03c3 multiplied by the above expectation.\nWe will find it useful to bound ratios of the Gamma function (as appear in Lemma 3) and so introduce the following inequality, from Kershaw [1983] that provides an extension of Gautschi\u2019s Inequality.\nTheorem 7. An Extension of Gautschi\u2019s Inequality For 0 < s < 1, we have\n( x+ s\n2\n)1\u2212s \u2264 \u0393(x+ 1)\n\u0393(x+ s) \u2264\n( x\u2212 1\n2 +\n( s+ 1\n4\n) 1 2 )1\u2212s\nWe now show: Lemma 4. Norm of Projections Let W be a k by k random Gaussian matrix with iid entries \u223c N (0, \u03c32), and x, y two given vectors. Partition W into components as in Lemma 1 and let x\u22a5 be a nonzero vector perpendicular to x. Then\n(a)\nE [\u2223\u2223\u2223\u2223\u22a5W\u22a5x\u22a5\u2223\u2223\u2223\u2223] = ||x\u22a5||\u03c3\u221a2 \u0393(k/2)\n\u0393((k \u2212 1)/2 \u2265 ||x\u22a5||\u03c3\n\u221a 2\n( k\n2 \u2212 3 4 )1/2 (b) If 1A is an identity matrix with non-zeros diagonal entry i iff i \u2208 A \u2282 [k], and |A| > 2,\nthen\nE [\u2223\u2223\u2223\u22231A\u22a5W\u22a5x\u22a5\u2223\u2223\u2223\u2223] \u2265 ||x\u22a5||\u03c3\u221a2 \u0393(|A|/2)\n\u0393((|A| \u2212 1)/2) \u2265 ||x\u22a5||\u03c3\n\u221a 2 ( |A| 2 \u2212 3 4 )1/2 Proof. (a) Let U, V, W\u0303 be as in Lemma 1. As U, V are rotations, W\u0303 is also iid Gaussian.\nFurthermore for any fixed W , with a\u0303 = V a, by taking inner products, and square-rooting, we see that \u2223\u2223\u2223\u2223\u2223\u2223W\u0303 a\u0303\u2223\u2223\u2223\u2223\u2223\u2223 = ||Wa||. So in particular E [\u2223\u2223\u2223\u2223\u22a5W\u22a5x\u22a5\u2223\u2223\u2223\u2223] = E [\u2223\u2223\u2223\u2223\u2223\u2223\u22a5W\u0303\u22a5x\u0303\u22a5\u2223\u2223\u2223\u2223\u2223\u2223]\nBut from the definition of non-zero entries of \u22a5W\u0303\u22a5, and the form of x\u0303\u22a5 (a zero entry in the first coordinate), it follows that \u22a5W\u0303\u22a5x\u0303\u22a5 has exactly k \u2212 1 non zero entries, each a centered Gaussian with variance (k \u2212 1)\u03c32 ||x\u22a5||2. By Lemma 3, the expected norm is as in the statement. We then apply Theorem 7 to get the lower bound.\n(b) First note we can view 1A\u22a5W\u22a5 = \u22a51AW\u22a5. (Projecting down to a random (as W is random) subspace of fixed size |A| = m and then making perpendicular commutes with making perpendicular and then projecting everything down to the subspace.)\nSo we can view W as a random m by k matrix, and for x, y as in Lemma 1 (with y projected down ontom dimensions), we can again define U, V as k by k andm bym rotation matrices respectively, and W\u0303 = UWV T , with analogous properties to Lemma 1. Now we can finish as in part (a), except that \u22a5W\u0303\u22a5x\u0303 may have only m\u2212 1 entries, (depending on whether y is annihilated by projecting down by1A) each of variance (k \u2212 1)\u03c32 ||x\u22a5||2.\nLemma 5. Norm and Translation Let X be a centered multivariate Gaussian, with diagonal covariance matrix, and \u00b5 a constant vector.\nE(||X \u2212 \u00b5||) \u2265 E(||X||)\nProof. The inequality can be seen intuitively geometrically: as X has diagonal covariance matrix, the contours of the pdf of ||X|| are circular centered at 0, decreasing radially. However, the contours of the pdf of ||X \u2212 \u00b5|| are shifted to be centered around ||\u00b5||, and so shifting back \u00b5 to 0 reduces the norm.\nA more formal proof can be seen as follows: let the pdf of X be fX(\u00b7). Then we wish to show\u222b x ||x\u2212 \u00b5|| fX(x)dx \u2265 \u222b x ||x|| fX(x)dx\nNow we can pair points x,\u2212x, using the fact that fX(x) = fX(\u2212x) and the triangle inequality on the integrand to get\u222b\n|x| (||x\u2212 \u00b5||+ ||\u2212x\u2212 \u00b5||) fX(x)dx \u2265 \u222b |x| ||2x|| fX(x)dx = \u222b |x| (||x||+ ||\u2212x||) fX(x)dx\nA.2 Proof of Theorem\nProof. We first prove the zero bias case, Theorem 2. To do so, it is sufficient to prove that\nE [\u2223\u2223\u2223\u2223\u2223\u2223\u03b4z(d+1)(t)\u2223\u2223\u2223\u2223\u2223\u2223] \u2265 O ( \u221a\u03c3k\u221a \u03c3 + k )d+1\u2223\u2223\u2223\u2223\u2223\u2223\u03b4z(0)(t)\u2223\u2223\u2223\u2223\u2223\u2223 (**) as integrating over t gives us the statement of the theorem.\nFor ease of notation, we will suppress the t in z(d)(t).\nWe first write W (d) = W\n(d) \u22a5 +W (d) \u2016\nwhere the division is done with respect to z(d). Note that this means h(d+1) = W (d)\u2016 z (d) as the other component annihilates (maps to 0) z(d).\nWe can also define A W (d) \u2016 = {i : i \u2208 [k], |h(d+1)i | < 1} i.e. the set of indices for which the hidden representation is not saturated. Letting Wi denote the ith row of matrix W , we now claim that:\nEW (d) [\u2223\u2223\u2223\u2223\u2223\u2223\u03b4z(d+1)\u2223\u2223\u2223\u2223\u2223\u2223] = EW (d)\u2016 EW (d)\u22a5   \u2211 i\u2208A\nW (d) \u2016\n((W (d) \u22a5 )i\u03b4z (d) + (W (d) \u2016 )i\u03b4z (d))2  1/2  (*)\nIndeed, by Lemma 2 we first split the expectation over W (d) into a tower of expectations over the two independent parts of W to get\nEW (d) [\u2223\u2223\u2223\u2223\u2223\u2223\u03b4z(d+1)\u2223\u2223\u2223\u2223\u2223\u2223] = EW (d)\u2016 EW (d)\u22a5 [\u2223\u2223\u2223\u2223\u2223\u2223\u03c6(W (d)\u03b4z(d))\u2223\u2223\u2223\u2223\u2223\u2223]\nBut conditioning on W (d)\u2016 in the inner expectation gives us h (d+1) and A W (d)\n\u2016 , allowing us to replace\nthe norm over \u03c6(W (d)\u03b4z(d)) with the sum in the term on the right hand side of the claim.\nTill now, we have mostly focused on partitioning the matrix W (d). But we can also set \u03b4z(d) = \u03b4z\n(d) \u2016 + \u03b4z (d) \u22a5 where the perpendicular and parallel are with respect to z (d). In fact, to get the expression in (**), we derive a recurrence as below:\nEW (d) [\u2223\u2223\u2223\u2223\u2223\u2223\u03b4z(d+1)\u22a5 \u2223\u2223\u2223\u2223\u2223\u2223] \u2265 O ( \u221a \u03c3k\u221a \u03c3 + k ) EW (d) [\u2223\u2223\u2223\u2223\u2223\u2223\u03b4z(d)\u22a5 \u2223\u2223\u2223\u2223\u2223\u2223] To get this, we first need to define z\u0303(d+1) = 1A\nW (d) \u2016\nh(d+1) - the latent vector h(d+1) with all saturated\nunits zeroed out.\nWe then split the column space of W (d) = \u22a5W (d) + \u2016W (d), where the split is with respect to z\u0303(d+1). Letting \u03b4z(d+1)\u22a5 be the part perpendicular to z\n(d+1), and A the set of units that are unsaturated, we have an important relation: Claim \u2223\u2223\u2223\u2223\u2223\u2223\u03b4z(d+1)\u22a5 \u2223\u2223\u2223\u2223\u2223\u2223 \u2265 \u2223\u2223\u2223\u2223\u2223\u2223\u22a5W (d)\u03b4z(d)1A\u2223\u2223\u2223\u2223\u2223\u2223 (where the indicator in the right hand side zeros out coordinates not in the active set.)\nTo see this, first note, by definition,\n\u03b4z (d+1) \u22a5 = W (d)\u03b4z(d) \u00b7 1A \u2212 \u3008W (d)\u03b4z(d) \u00b7 1A, z\u0302(d+1)\u3009z\u0302(d+1) (1)\nwhere the \u00b7\u0302 indicates a unit vector.\nSimilarly \u22a5W (d)\u03b4z(d) = W (d)\u03b4z(d) \u2212 \u3008W (d)\u03b4z(d), \u02c6\u0303z(d+1)\u3009\u02c6\u0303z(d+1) (2)\nNow note that for any index i \u2208 A, the right hand sides of (1) and (2) are identical, and so the vectors on the left hand side agree for all i \u2208 A. In particular,\n\u03b4z (d+1) \u22a5 \u00b7 1A = \u22a5W (d)\u03b4z(d) \u00b7 1A Now the claim follows easily by noting that \u2223\u2223\u2223\u2223\u2223\u2223\u03b4z(d+1)\u22a5 \u2223\u2223\u2223\u2223\u2223\u2223 \u2265 \u2223\u2223\u2223\u2223\u2223\u2223\u03b4z(d+1)\u22a5 \u00b7 1A\u2223\u2223\u2223\u2223\u2223\u2223.\nReturning to (*), we split \u03b4z(d) = \u03b4z(d)\u22a5 + \u03b4z (d) \u2016 , W (d) \u22a5 = \u2016W (d) \u22a5 + \u22a5W (d) \u22a5 (and W (d) \u2016 analogously), and after some cancellation, we have\nEW (d) [\u2223\u2223\u2223\u2223\u2223\u2223\u03b4z(d+1)\u2223\u2223\u2223\u2223\u2223\u2223] = EW (d)\u2016 EW (d)\u22a5   \u2211 i\u2208A\nW (d) \u2016\n( (\u22a5W\n(d) \u22a5 + \u2016W (d) \u22a5 )i\u03b4z (d) \u22a5 + ( \u22a5W (d) \u2016 + \u2016W (d) \u2016 )i\u03b4z (d) \u2016 )2 1/2 \nWe would like a recurrence in terms of only perpendicular components however, so we first drop the \u2016W (d)\u22a5 , \u2016W (d) \u2016 (which can be done without decreasing the norm as they are perpendicular to the remaining terms) and using the above claim, have\nEW (d) [\u2223\u2223\u2223\u2223\u2223\u2223\u03b4z(d+1)\u22a5 \u2223\u2223\u2223\u2223\u2223\u2223] \u2265 EW (d)\u2016 EW (d)\u22a5   \u2211 i\u2208A\nW (d) \u2016\n( (\u22a5W\n(d) \u22a5 )i\u03b4z (d) \u22a5 + ( \u22a5W (d) \u2016 )i\u03b4z (d) \u2016 )2 1/2 \nBut in the inner expectation, the term \u22a5W (d)\u2016 \u03b4z (d) \u2016 is just a constant, as we are conditioning on W (d) \u2016 . So using Lemma 5 we have\nE W\n(d) \u22a5   \u2211 i\u2208A\nW (d) \u2016\n( (\u22a5W\n(d) \u22a5 )i\u03b4z (d) \u22a5 + ( \u22a5W (d) \u2016 )i\u03b4z (d) \u2016 )2 1/2  \u2265 EW (d)\u22a5   \u2211 i\u2208A\nW (d) \u2016\n( (\u22a5W\n(d) \u22a5 )i\u03b4z (d) \u22a5 )2 1/2 \nWe can then apply Lemma 4 to get\nE W\n(d) \u22a5   \u2211 i\u2208A\nW (d) \u2016\n( (\u22a5W\n(d) \u22a5 )i\u03b4z (d) \u22a5 )2 1/2  \u2265 \u03c3\u221ak\u221a2 \u221a 2|A W (d) \u2016 | \u2212 3 2 E [\u2223\u2223\u2223\u2223\u2223\u2223\u03b4z(d)\u22a5 \u2223\u2223\u2223\u2223\u2223\u2223]\nThe outer expectation on the right hand side only affects the term in the expectation through the size of the non-saturated set of units. Letting p = P(|h(d+1)i | < 1), and noting that we get a non-zero norm only if |A\nW (d) \u2016 | \u2265 2 (else we cannot project down a dimension), and for |A W (d) \u2016 | \u2265 2,\n\u221a 2\n\u221a 2|A\nW (d) \u2016 | \u2212 3\n2 \u2265 1\u221a\n2\n\u221a |A\nW (d) \u2016 |\nwe get\nEW (d) [\u2223\u2223\u2223\u2223\u2223\u2223\u03b4z(d+1)\u22a5 \u2223\u2223\u2223\u2223\u2223\u2223] \u2265 1\u221a2  k\u2211 j=2 ( k j ) pj(1\u2212 p)k\u2212j \u03c3\u221a k \u221a j E [\u2223\u2223\u2223\u2223\u2223\u2223\u03b4z(d)\u22a5 \u2223\u2223\u2223\u2223\u2223\u2223]\nWe use the fact that we have the probability mass function for an (k, p) binomial random variable to bound the \u221a j term:\nk\u2211 j=2 ( k j ) pj(1\u2212 p)k\u2212j \u03c3\u221a k \u221a j = \u2212 ( k 1 ) p(1\u2212 p)k\u22121 \u03c3\u221a k + k\u2211 j=0 ( k j ) pj(1\u2212 p)k\u2212j \u03c3\u221a k \u221a j\n= \u2212\u03c3 \u221a kp(1\u2212 p)k\u22121 + kp \u00b7 \u03c3\u221a\nk k\u2211 j=1 1\u221a j ( k \u2212 1 j \u2212 1 ) pj\u22121(1\u2212 p)k\u2212j\nBut by using Jensen\u2019s inequality with 1/ \u221a x, we get\nk\u2211 j=1 1\u221a j ( k \u2212 1 j \u2212 1 ) pj\u22121(1\u2212 p)k\u2212j \u2265 1\u221a\u2211k j=1 j ( k\u22121 j\u22121 ) pj\u22121(1\u2212 p)k\u2212j = 1\u221a (k \u2212 1)p+ 1\nwhere the last equality follows by recognising the expectation of a binomial(k\u22121, p) random variable. So putting together, we get\nEW (d) [\u2223\u2223\u2223\u2223\u2223\u2223\u03b4z(d+1)\u22a5 \u2223\u2223\u2223\u2223\u2223\u2223] \u2265 1\u221a2 ( \u2212\u03c3 \u221a kp(1\u2212 p)k\u22121 + \u03c3 \u00b7 \u221a kp\u221a 1 + (k \u2212 1)p ) E [\u2223\u2223\u2223\u2223\u2223\u2223\u03b4z(d)\u22a5 \u2223\u2223\u2223\u2223\u2223\u2223] (a)\nTo lower bound p, we first note that as h(d+1)i is a normal random variable with variance \u2264 \u03c32, if A \u223c N (0, \u03c32)\nP(|h(d+1)i | < 1) \u2265 P(|A| < 1) \u2265 1\n\u03c3 \u221a 2\u03c0 (b)\nwhere the last inequality holds for \u03c3 \u2265 1 and follows by Taylor expanding e\u2212x2/2 around 0. Similarly, we can also show that p \u2264 1\u03c3 . So this becomes\nE [\u2223\u2223\u2223\u2223\u2223\u2223\u03b4z(d+1)\u2223\u2223\u2223\u2223\u2223\u2223] \u2265  1\u221a 2  1 (2\u03c0)1/4 \u221a \u03c3k\u221a\n\u03c3 \u221a 2\u03c0 + (k \u2212 1) \u2212 \u221a k\n( 1\u2212 1\n\u03c3\n)k\u22121E [\u2223\u2223\u2223\u2223\u2223\u2223\u03b4z(d)\u22a5 \u2223\u2223\u2223\u2223\u2223\u2223]\n= O ( \u221a \u03c3k\u221a \u03c3 + k ) E [\u2223\u2223\u2223\u2223\u2223\u2223\u03b4z(d)\u22a5 \u2223\u2223\u2223\u2223\u2223\u2223]\nFinally, we can compose this, to get\nE [\u2223\u2223\u2223\u2223\u2223\u2223\u03b4z(d+1)\u2223\u2223\u2223\u2223\u2223\u2223] \u2265  1\u221a 2  1 (2\u03c0)1/4 \u221a \u03c3k\u221a\n\u03c3 \u221a 2\u03c0 + (k \u2212 1) \u2212 \u221a k\n( 1\u2212 1\n\u03c3 )k\u22121d+1 c \u00b7 ||\u03b4x(t)|| with the constant c being the ratio of ||\u03b4x(t)\u22a5|| to ||\u03b4x(t)||. So if our trajectory direction is almost orthogonal to x(t) (which will be the case for e.g. random circular arcs, c can be seen to be \u2248 1 by splitting into components as in Lemma 1, and using Lemmas 3, 4.)\nResult for non-zero bias In fact, we can easily extend the above result to the case of non-zero bias. The insight is to note that because \u03b4z(d+1) involves taking a difference between z(d+1)(t+ dt) and z(d+1)(t), the bias term does not enter at all into the expression for \u03b4z(d+1). So the computations above hold, and equation (a) becomes\nEW (d) [\u2223\u2223\u2223\u2223\u2223\u2223\u03b4z(d+1)\u22a5 \u2223\u2223\u2223\u2223\u2223\u2223] \u2265 1\u221a2 ( \u2212\u03c3w \u221a kp(1\u2212 p)k\u22121 + \u03c3w \u00b7 \u221a kp\u221a 1 + (k \u2212 1)p ) E [\u2223\u2223\u2223\u2223\u2223\u2223\u03b4z(d)\u22a5 \u2223\u2223\u2223\u2223\u2223\u2223]\nWe also now have that h(d+1)i is a normal random variable with variance \u2264 \u03c32w + \u03c32b (as the bias is drawn from N (0, \u03c32b )). So equation (b) becomes\nP(|h(d+1)i | < 1) \u2265 1\u221a\n(\u03c32w + \u03c3 2 b ) \u221a 2\u03c0\nThis gives Theorem 1\nE [\u2223\u2223\u2223\u2223\u2223\u2223\u03b4z(d+1)\u2223\u2223\u2223\u2223\u2223\u2223] \u2265 O  \u03c3w (\u03c32w + \u03c3 2 b ) 1/4 \u00b7 \u221a k\u221a\u221a\n\u03c32w + \u03c3 2 b + k\nE [\u2223\u2223\u2223\u2223\u2223\u2223\u03b4z(d)\u22a5 \u2223\u2223\u2223\u2223\u2223\u2223]\nStatement and Proof of Upper Bound for Trajectory Growth Replace hard-tanh with a linear coordinate-wise identity map, h(d+1)i = (W\n(d)z(d))i + bi. This provides an upper bound on the norm. We also then recover a chi distribution with k terms, each with standard deviation \u03c3w\nk 1 2\n,\nE [\u2223\u2223\u2223\u2223\u2223\u2223\u03b4z(d+1)\u2223\u2223\u2223\u2223\u2223\u2223] \u2264 \u221a2\u0393 ((k + 1)/2)\n\u0393 (k/2)\n\u03c3w k 1 2 \u2223\u2223\u2223\u2223\u2223\u2223\u03b4z(d)\u2223\u2223\u2223\u2223\u2223\u2223 (2) \u2264 \u03c3w ( k + 1\nk\n) 1 2 \u2223\u2223\u2223\u2223\u2223\u2223\u03b4z(d)\u2223\u2223\u2223\u2223\u2223\u2223 , (3)\nwhere the second step follows from Laforgia and Natalini [2013], and holds for k > 1."}, {"heading": "B Proofs and additional results from Section 2.2.3", "text": "Proof of Theorem 3\nProof. For \u03c3b = 0: For hidden layer d < n, consider neuron v(d)1 . This has as input \u2211k i=1W (d\u22121) i1 z (d\u22121) i . As we are in the large \u03c3 case, we assume that |z(d\u22121)i | = 1. Furthermore, as signs for z (d\u22121) i and W (d\u22121) i1 are both completely random, we can also assume wlog that z(d\u22121)i = 1. For a particular input, we can define v (d) 1 as sensitive to v (d\u22121) i if v (d\u22121) i transitioning (to wlog \u22121) will induce a transition in node v (d) 1 .\nA sufficient condition for this to happen is if |Wi1| \u2265 | \u2211 j 6=iWj1|. But X = Wi1 \u223c N (0, \u03c32/k)\nand \u2211 j 6=iWj1 = Y\n\u2032 \u223c N (0, (k \u2212 1)\u03c32/k). So we want to compute P(|X| > |Y \u2032|). For ease of computation, we instead look at P(|X| > |Y |), where Y \u223c N (0, \u03c32). But this is the same as computing P(|X|/|Y | > 1) = P(X/Y < \u22121) + P(X/Y > 1). But the ratio of two centered independent normals with variances \u03c321 , \u03c3 2 2 follows a Cauchy distribution, with\nparameter \u03c31/\u03c32, which in this case is 1/ \u221a k. Substituting this in to the cdf of the Cauchy distribution, we get that\nP ( |X| |Y | > 1 ) = 1\u2212 2 \u03c0 arctan( \u221a k)\nFinally, using the identity arctan(x) + arctan(1/x) and the Laurent series for arctan(1/x), we can evaluate the right hand side to be O(1/ \u221a k). In particular\nP ( |X| |Y | > 1 ) \u2265 O ( 1\u221a k ) (c)\nThis means that in expectation, any neuron in layer d will be sensitive to the transitions of \u221a k neurons in the layer below. Using this, and the fact the while v(d\u22121)i might flip very quickly from say \u22121 to 1, the gradation in the transition ensures that neurons in layer d sensitive to v(d\u22121)i will transition at distinct times, we get the desired growth rate in expectation as follows: let t(d) be the number of transitions in layer d, and let t(d)i be the number of transitions of neuron i in layer d. Let T (d+1) be a random variable for the number of transitions in layer d + 1, and T (d+1)i the\nequivalent for neuron i. Then E [ T (d+1) ] = \u2211 i E [ T (d+1) i ] = kE [ T (d+1) 1 ] by symmetry. But\nE [ T\n(d+1) 1\n] \u2265 E [\u2211 i 1(1,i)t (d+1) i ] where 1(1,i) is the indicator function of neuron 1 in layer d+ 1\nbeing sensitive to neuron i in layer d. Using linearity of expectation and the result from above, we get E [ T\n(d+1) 1\n] \u2265 t(d)/ \u221a k. And so, using the fact that all transitions happen at distinct points almost\nsurely, E [ T (d+1) ] \u2265 \u221a kt(d).\nFor \u03c3b > 0: We replace \u221a k with \u221a k(1 + \u03c32b/\u03c3 2 w), by noting that Y \u223c N (0, \u03c32w + \u03c32b ). This results in a growth\nrate of form O( \u221a k/ \u221a\n1 + \u03c32b \u03c32w )."}, {"heading": "C Proofs and additional results from Section 2.3", "text": "Proof of Theorem 5\nProof. Let the hyperplane arrangement be denoted H, and let H \u2208 H be one specific hyperplane. Then the number of regions in H is precisely the number of regions in H\u2212H plus the number of regions inH \u2229H . (This follows from the fact that H subdivides into two regions exactly all of the regions inH \u2229H , and does not affect any of the other regions.) In particular, we have the recursive formula\nr(k,m) = r(k \u2212 1,m) + r(k \u2212 1,m\u2212 1)\nWe now induct on k +m to assert the claim. The base cases of r(1, 0) = r(0, 1) = 1 are trivial, and assuming the claim for \u2264 k +m\u2212 1 as the induction hypothesis, we have\nr(k \u2212 1,m) + r(k \u2212 1,m\u2212 1) \u2264 m\u2211 i=0 ( k \u2212 1 i ) + m\u22121\u2211 i=0 ( k \u2212 1 i )\n\u2264 ( k \u2212 1\n0\n) + d\u22121\u2211 i=0 ( k \u2212 1 i ) + ( k \u2212 1 i+ 1 )\n\u2264 ( k\n0\n) + m\u22121\u2211 i=0 ( k i+ 1 ) where the last equality follows by the well known identity(\na\nb\n) + ( a\nb+ 1\n) = ( a+ 1\nb+ 1 ) This concludes the proof."}, {"heading": "D Proofs and additional results from Section 3", "text": "D.1 Upper Bound for Dichotomies\nThe Vapnik-Chervonenkis (VC) dimension of a function class is the cardinality of the largest set of points that it can shatter. The VC dimension provides an upper (worst case) bound on the generalization error for a function class Vapnik and Vapnik [1998]. Motivated by generalization error, VC dimension has been studied for neural networks Sontag [1998], Bartlett and Maass [2003]. In Bartlett et al. [1998] an upper bound on the VC dimension v of a neural network with piecewise polynomial activation function and binary output is derived. For hard-tanh units, this bound is\nv = 2 |W |n log (4e |W |nk) + 2 |W |n2 log 2 + 2n, (4)\nwhere |W | is the total number of weights, n is the depth, and k is the width of the network. The VC dimension provides an upper bound on the number of achievable dichotomies |F| by way of the Sauer\u2013Shelah lemma Sauer [1972],\n|F| \u2264 ( e|S| v )v . (5)\nBy combining Equations 4 and 5 an upper bound on the number of dichotomies is found, with a growth rate which is exponential in a low order polynomial of the network size."}], "references": [{"title": "The unreasonable effectiveness of deep learning", "author": ["Yann LeCun"], "venue": "In Seminar. Johns Hopkins University,", "citeRegEx": "LeCun.,? \\Q2014\\E", "shortCiteRegEx": "LeCun.", "year": 2014}, {"title": "The unreasonable effectiveness of recurrent neural networks", "author": ["Andrej Karpathy"], "venue": "In Andrej Karpathy blog,", "citeRegEx": "Karpathy.,? \\Q2015\\E", "shortCiteRegEx": "Karpathy.", "year": 2015}, {"title": "Imagenet classification with deep convolutional neural networks. In Advances in neural information processing", "author": ["Alex Krizhevsky", "Ilya Sutskever", "Geoffrey E Hinton"], "venue": null, "citeRegEx": "Krizhevsky et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Krizhevsky et al\\.", "year": 2012}, {"title": "Searching for exotic particles in high-energy physics with deep learning", "author": ["Pierre Baldi", "Peter Sadowski", "Daniel Whiteson"], "venue": "Nature communications,", "citeRegEx": "Baldi et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Baldi et al\\.", "year": 2014}, {"title": "Mastering the game of go with deep neural networks and tree", "author": ["David Silver", "Aja Huang", "Chris J Maddison", "Arthur Guez", "Laurent Sifre", "George Van Den Driessche", "Julian Schrittwieser", "Ioannis Antonoglou", "Veda Panneershelvam", "Marc Lanctot"], "venue": "search. Nature,", "citeRegEx": "Silver et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Silver et al\\.", "year": 2016}, {"title": "Deep knowledge tracing", "author": ["Chris Piech", "Jonathan Bassen", "Jonathan Huang", "Surya Ganguli", "Mehran Sahami", "Leonidas J Guibas", "Jascha Sohl-Dickstein"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Piech et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Piech et al\\.", "year": 2015}, {"title": "Identifying and attacking the saddle point problem in high-dimensional non-convex optimization", "author": ["Yann N Dauphin", "Razvan Pascanu", "Caglar Gulcehre", "Kyunghyun Cho", "Surya Ganguli", "Yoshua Bengio"], "venue": "In Advances in neural information processing systems,", "citeRegEx": "Dauphin et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Dauphin et al\\.", "year": 2014}, {"title": "Qualitatively characterizing neural network optimization problems", "author": ["Ian J Goodfellow", "Oriol Vinyals", "Andrew M Saxe"], "venue": "arXiv preprint arXiv:1412.6544,", "citeRegEx": "Goodfellow et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Goodfellow et al\\.", "year": 2014}, {"title": "The loss surfaces of multilayer networks", "author": ["Anna Choromanska", "Mikael Henaff", "Michael Mathieu", "G\u00e9rard Ben Arous", "Yann LeCun"], "venue": "arXiv preprint arXiv:1412.0233,", "citeRegEx": "Choromanska et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Choromanska et al\\.", "year": 2014}, {"title": "Optimizing neural networks with kronecker-factored approximate curvature", "author": ["James Martens", "Roger Grosse"], "venue": "arXiv preprint arXiv:1503.05671,", "citeRegEx": "Martens and Grosse.,? \\Q2015\\E", "shortCiteRegEx": "Martens and Grosse.", "year": 2015}, {"title": "Rectified linear units improve restricted boltzmann machines", "author": ["Vinod Nair", "Geoffrey E Hinton"], "venue": "In Proceedings of the 27th International Conference on Machine Learning", "citeRegEx": "Nair and Hinton.,? \\Q2010\\E", "shortCiteRegEx": "Nair and Hinton.", "year": 2010}, {"title": "Train faster, generalize better: Stability of stochastic gradient descent", "author": ["Moritz Hardt", "Benjamin Recht", "Yoram Singer"], "venue": "arXiv preprint arXiv:1509.01240,", "citeRegEx": "Hardt et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Hardt et al\\.", "year": 2015}, {"title": "Vc dimension of neural networks", "author": ["Eduardo D Sontag"], "venue": "NATO ASI SERIES F COMPUTER AND SYSTEMS SCIENCES,", "citeRegEx": "Sontag.,? \\Q1998\\E", "shortCiteRegEx": "Sontag.", "year": 1998}, {"title": "Vapnik-chervonenkis dimension of neural nets", "author": ["Peter L Bartlett", "Wolfgang Maass"], "venue": "The handbook of brain theory and neural networks,", "citeRegEx": "Bartlett and Maass.,? \\Q2003\\E", "shortCiteRegEx": "Bartlett and Maass.", "year": 2003}, {"title": "Almost linear vc-dimension bounds for piecewise polynomial networks", "author": ["Peter L Bartlett", "Vitaly Maiorov", "Ron Meir"], "venue": "Neural computation,", "citeRegEx": "Bartlett et al\\.,? \\Q1998\\E", "shortCiteRegEx": "Bartlett et al\\.", "year": 1998}, {"title": "Distilling the knowledge in a neural network", "author": ["Geoffrey Hinton", "Oriol Vinyals", "Jeff Dean"], "venue": "arXiv preprint arXiv:1503.02531,", "citeRegEx": "Hinton et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Hinton et al\\.", "year": 2015}, {"title": "Multilayer feedforward networks are universal approximators", "author": ["Kurt Hornik", "Maxwell Stinchcombe", "Halbert White"], "venue": "Neural networks,", "citeRegEx": "Hornik et al\\.,? \\Q1989\\E", "shortCiteRegEx": "Hornik et al\\.", "year": 1989}, {"title": "Approximation by superpositions of a sigmoidal function", "author": ["George Cybenko"], "venue": "Mathematics of control, signals and systems,", "citeRegEx": "Cybenko.,? \\Q1989\\E", "shortCiteRegEx": "Cybenko.", "year": 1989}, {"title": "A comparison of the computational power of sigmoid and Boolean threshold circuits", "author": ["Wolfgang Maass", "Georg Schnitger", "Eduardo D Sontag"], "venue": null, "citeRegEx": "Maass et al\\.,? \\Q1994\\E", "shortCiteRegEx": "Maass et al\\.", "year": 1994}, {"title": "Expressiveness of rectifier networks", "author": ["Xingyuan Pan", "Vivek Srikumar"], "venue": "arXiv preprint arXiv:1511.05678,", "citeRegEx": "Pan and Srikumar.,? \\Q2015\\E", "shortCiteRegEx": "Pan and Srikumar.", "year": 2015}, {"title": "The power of depth for feedforward neural networks", "author": ["Ronen Eldan", "Ohad Shamir"], "venue": "arXiv preprint arXiv:1512.03965,", "citeRegEx": "Eldan and Shamir.,? \\Q2015\\E", "shortCiteRegEx": "Eldan and Shamir.", "year": 2015}, {"title": "Representation benefits of deep feedforward networks", "author": ["Matus Telgarsky"], "venue": "arXiv preprint arXiv:1509.08101,", "citeRegEx": "Telgarsky.,? \\Q2015\\E", "shortCiteRegEx": "Telgarsky.", "year": 2015}, {"title": "On the representational efficiency of restricted boltzmann machines", "author": ["James Martens", "Arkadev Chattopadhya", "Toni Pitassi", "Richard Zemel"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Martens et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Martens et al\\.", "year": 2013}, {"title": "On the complexity of neural network classifiers: A comparison between shallow and deep architectures", "author": ["Monica Bianchini", "Franco Scarselli"], "venue": "Neural Networks and Learning Systems, IEEE Transactions on,", "citeRegEx": "Bianchini and Scarselli.,? \\Q2014\\E", "shortCiteRegEx": "Bianchini and Scarselli.", "year": 2014}, {"title": "On the number of response regions of deep feed forward networks with piece-wise linear activations", "author": ["Razvan Pascanu", "Guido Montufar", "Yoshua Bengio"], "venue": "arXiv preprint arXiv:1312.6098,", "citeRegEx": "Pascanu et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Pascanu et al\\.", "year": 2013}, {"title": "On the number of linear regions of deep neural networks", "author": ["Guido F Montufar", "Razvan Pascanu", "Kyunghyun Cho", "Yoshua Bengio"], "venue": "In Advances in neural information processing systems,", "citeRegEx": "Montufar et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Montufar et al\\.", "year": 2014}, {"title": "Toward deeper understanding of neural networks: The power of initialization and a dual view on expressivity", "author": ["Amit Daniely", "Roy Frostig", "Yoram Singer"], "venue": "arXiv preprint arXiv:1602.05897,", "citeRegEx": "Daniely et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Daniely et al\\.", "year": 2016}, {"title": "Exponential expressivity", "author": ["Ben Poole", "Subhaneil Lahiri", "Maithra Raghu", "Jascha Sohl-Dickstein", "Surya Ganguli"], "venue": null, "citeRegEx": "Poole et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Poole et al\\.", "year": 2015}, {"title": "Links between perceptrons, mlps and svms", "author": ["2016. Ronan Collobert", "Samy Bengio"], "venue": "In Proceedings of the twenty-first", "citeRegEx": "Collobert and Bengio.,? \\Q2016\\E", "shortCiteRegEx": "Collobert and Bengio.", "year": 2016}, {"title": "Hyperplane arrangements", "author": ["Richard Stanley"], "venue": "international conference on Machine learning,", "citeRegEx": "Stanley.,? \\Q2004\\E", "shortCiteRegEx": "Stanley.", "year": 2004}, {"title": "On some inequalities for the gamma function", "author": ["Andrea Laforgia", "Pierpaolo Natalini"], "venue": "Advances in Dynamical", "citeRegEx": "Laforgia and Natalini.,? \\Q1983\\E", "shortCiteRegEx": "Laforgia and Natalini.", "year": 1983}], "referenceMentions": [{"referenceID": 0, "context": "Neural network architectures have proven \u201cunreasonably effective\u201d LeCun [2014], Karpathy [2015] on many tasks, including image classification Krizhevsky et al.", "startOffset": 66, "endOffset": 79}, {"referenceID": 0, "context": "Neural network architectures have proven \u201cunreasonably effective\u201d LeCun [2014], Karpathy [2015] on many tasks, including image classification Krizhevsky et al.", "startOffset": 66, "endOffset": 96}, {"referenceID": 0, "context": "Neural network architectures have proven \u201cunreasonably effective\u201d LeCun [2014], Karpathy [2015] on many tasks, including image classification Krizhevsky et al. [2012], identifying particles in high energy physics Baldi et al.", "startOffset": 66, "endOffset": 167}, {"referenceID": 0, "context": "Neural network architectures have proven \u201cunreasonably effective\u201d LeCun [2014], Karpathy [2015] on many tasks, including image classification Krizhevsky et al. [2012], identifying particles in high energy physics Baldi et al. [2014], playing Go Silver et al.", "startOffset": 66, "endOffset": 233}, {"referenceID": 0, "context": "Neural network architectures have proven \u201cunreasonably effective\u201d LeCun [2014], Karpathy [2015] on many tasks, including image classification Krizhevsky et al. [2012], identifying particles in high energy physics Baldi et al. [2014], playing Go Silver et al. [2016], and modeling human student learning Piech et al.", "startOffset": 66, "endOffset": 266}, {"referenceID": 0, "context": "Neural network architectures have proven \u201cunreasonably effective\u201d LeCun [2014], Karpathy [2015] on many tasks, including image classification Krizhevsky et al. [2012], identifying particles in high energy physics Baldi et al. [2014], playing Go Silver et al. [2016], and modeling human student learning Piech et al. [2015]. Despite their power, we have limited understanding of how and why neural networks work, and much of this understanding is qualitative and heuristic rather than precise and quantitative.", "startOffset": 66, "endOffset": 323}, {"referenceID": 0, "context": "Neural network architectures have proven \u201cunreasonably effective\u201d LeCun [2014], Karpathy [2015] on many tasks, including image classification Krizhevsky et al. [2012], identifying particles in high energy physics Baldi et al. [2014], playing Go Silver et al. [2016], and modeling human student learning Piech et al. [2015]. Despite their power, we have limited understanding of how and why neural networks work, and much of this understanding is qualitative and heuristic rather than precise and quantitative. While even qualitative knowledge is helpful, a more formal and fundamental framework is desperately needed to ease the laborious process of designing and training these networks, as well as to enable us to to better predict and protect against their failures. In order to develop theoretical underpinnings for deep networks, it is first necessary to disentangle the many factors that influence their effectiveness. The effectiveness of deep neural networks on real tasks can be interpreted in terms of three broad properties: their trainability, or how well they can be fit to data; their generalizability, or how well they perform on novel examples; and their expressivity, or the set of functions they can compute. All three of these properties are crucial for understanding the performance of neural networks. First, neural networks are useful only to the extent they can be sucessfully trained. Previous work has explored the objective function landscape of neural networks Dauphin et al. [2014], Goodfellow et al.", "startOffset": 66, "endOffset": 1510}, {"referenceID": 0, "context": "Neural network architectures have proven \u201cunreasonably effective\u201d LeCun [2014], Karpathy [2015] on many tasks, including image classification Krizhevsky et al. [2012], identifying particles in high energy physics Baldi et al. [2014], playing Go Silver et al. [2016], and modeling human student learning Piech et al. [2015]. Despite their power, we have limited understanding of how and why neural networks work, and much of this understanding is qualitative and heuristic rather than precise and quantitative. While even qualitative knowledge is helpful, a more formal and fundamental framework is desperately needed to ease the laborious process of designing and training these networks, as well as to enable us to to better predict and protect against their failures. In order to develop theoretical underpinnings for deep networks, it is first necessary to disentangle the many factors that influence their effectiveness. The effectiveness of deep neural networks on real tasks can be interpreted in terms of three broad properties: their trainability, or how well they can be fit to data; their generalizability, or how well they perform on novel examples; and their expressivity, or the set of functions they can compute. All three of these properties are crucial for understanding the performance of neural networks. First, neural networks are useful only to the extent they can be sucessfully trained. Previous work has explored the objective function landscape of neural networks Dauphin et al. [2014], Goodfellow et al. [2014], Choromanska et al.", "startOffset": 66, "endOffset": 1536}, {"referenceID": 0, "context": "Neural network architectures have proven \u201cunreasonably effective\u201d LeCun [2014], Karpathy [2015] on many tasks, including image classification Krizhevsky et al. [2012], identifying particles in high energy physics Baldi et al. [2014], playing Go Silver et al. [2016], and modeling human student learning Piech et al. [2015]. Despite their power, we have limited understanding of how and why neural networks work, and much of this understanding is qualitative and heuristic rather than precise and quantitative. While even qualitative knowledge is helpful, a more formal and fundamental framework is desperately needed to ease the laborious process of designing and training these networks, as well as to enable us to to better predict and protect against their failures. In order to develop theoretical underpinnings for deep networks, it is first necessary to disentangle the many factors that influence their effectiveness. The effectiveness of deep neural networks on real tasks can be interpreted in terms of three broad properties: their trainability, or how well they can be fit to data; their generalizability, or how well they perform on novel examples; and their expressivity, or the set of functions they can compute. All three of these properties are crucial for understanding the performance of neural networks. First, neural networks are useful only to the extent they can be sucessfully trained. Previous work has explored the objective function landscape of neural networks Dauphin et al. [2014], Goodfellow et al. [2014], Choromanska et al. [2014], designed optimization procedures specifically suited to neural networks Martens and Grosse [2015], and proposed architectural components such as ReLUs Nair and Hinton [2010], which we analyze in this paper, specifically designed to make them more trainable.", "startOffset": 66, "endOffset": 1563}, {"referenceID": 0, "context": "Neural network architectures have proven \u201cunreasonably effective\u201d LeCun [2014], Karpathy [2015] on many tasks, including image classification Krizhevsky et al. [2012], identifying particles in high energy physics Baldi et al. [2014], playing Go Silver et al. [2016], and modeling human student learning Piech et al. [2015]. Despite their power, we have limited understanding of how and why neural networks work, and much of this understanding is qualitative and heuristic rather than precise and quantitative. While even qualitative knowledge is helpful, a more formal and fundamental framework is desperately needed to ease the laborious process of designing and training these networks, as well as to enable us to to better predict and protect against their failures. In order to develop theoretical underpinnings for deep networks, it is first necessary to disentangle the many factors that influence their effectiveness. The effectiveness of deep neural networks on real tasks can be interpreted in terms of three broad properties: their trainability, or how well they can be fit to data; their generalizability, or how well they perform on novel examples; and their expressivity, or the set of functions they can compute. All three of these properties are crucial for understanding the performance of neural networks. First, neural networks are useful only to the extent they can be sucessfully trained. Previous work has explored the objective function landscape of neural networks Dauphin et al. [2014], Goodfellow et al. [2014], Choromanska et al. [2014], designed optimization procedures specifically suited to neural networks Martens and Grosse [2015], and proposed architectural components such as ReLUs Nair and Hinton [2010], which we analyze in this paper, specifically designed to make them more trainable.", "startOffset": 66, "endOffset": 1662}, {"referenceID": 0, "context": "Neural network architectures have proven \u201cunreasonably effective\u201d LeCun [2014], Karpathy [2015] on many tasks, including image classification Krizhevsky et al. [2012], identifying particles in high energy physics Baldi et al. [2014], playing Go Silver et al. [2016], and modeling human student learning Piech et al. [2015]. Despite their power, we have limited understanding of how and why neural networks work, and much of this understanding is qualitative and heuristic rather than precise and quantitative. While even qualitative knowledge is helpful, a more formal and fundamental framework is desperately needed to ease the laborious process of designing and training these networks, as well as to enable us to to better predict and protect against their failures. In order to develop theoretical underpinnings for deep networks, it is first necessary to disentangle the many factors that influence their effectiveness. The effectiveness of deep neural networks on real tasks can be interpreted in terms of three broad properties: their trainability, or how well they can be fit to data; their generalizability, or how well they perform on novel examples; and their expressivity, or the set of functions they can compute. All three of these properties are crucial for understanding the performance of neural networks. First, neural networks are useful only to the extent they can be sucessfully trained. Previous work has explored the objective function landscape of neural networks Dauphin et al. [2014], Goodfellow et al. [2014], Choromanska et al. [2014], designed optimization procedures specifically suited to neural networks Martens and Grosse [2015], and proposed architectural components such as ReLUs Nair and Hinton [2010], which we analyze in this paper, specifically designed to make them more trainable.", "startOffset": 66, "endOffset": 1738}, {"referenceID": 11, "context": "A bound on generalization error in terms of the number of stochastic gradient descent steps performed is developed in Hardt et al. [2015]. VC dimension provides an upper bound on generalization error, and has been studied for neural networks Sontag [1998], Bartlett and Maass [2003], Bartlett et al.", "startOffset": 118, "endOffset": 138}, {"referenceID": 11, "context": "A bound on generalization error in terms of the number of stochastic gradient descent steps performed is developed in Hardt et al. [2015]. VC dimension provides an upper bound on generalization error, and has been studied for neural networks Sontag [1998], Bartlett and Maass [2003], Bartlett et al.", "startOffset": 118, "endOffset": 256}, {"referenceID": 11, "context": "A bound on generalization error in terms of the number of stochastic gradient descent steps performed is developed in Hardt et al. [2015]. VC dimension provides an upper bound on generalization error, and has been studied for neural networks Sontag [1998], Bartlett and Maass [2003], Bartlett et al.", "startOffset": 118, "endOffset": 283}, {"referenceID": 11, "context": "A bound on generalization error in terms of the number of stochastic gradient descent steps performed is developed in Hardt et al. [2015]. VC dimension provides an upper bound on generalization error, and has been studied for neural networks Sontag [1998], Bartlett and Maass [2003], Bartlett et al. [1998]. In model distillation, more efficient models are trained to emulate costly models which possess better generalization error Hinton et al.", "startOffset": 118, "endOffset": 307}, {"referenceID": 11, "context": "A bound on generalization error in terms of the number of stochastic gradient descent steps performed is developed in Hardt et al. [2015]. VC dimension provides an upper bound on generalization error, and has been studied for neural networks Sontag [1998], Bartlett and Maass [2003], Bartlett et al. [1998]. In model distillation, more efficient models are trained to emulate costly models which possess better generalization error Hinton et al. [2015]. In this paper, we focus on the third of these properties, expressivity \u2014 the power of the class of deep neural networks to represent highly complex functions.", "startOffset": 118, "endOffset": 453}, {"referenceID": 13, "context": "This approach has yielded many fascinating results: neural networks have been shown to be universal approximators Hornik et al. [1989], Cybenko [1989], their VC dimension studied Sontag [1998], Bartlett and Maass [2003], Bartlett et al.", "startOffset": 114, "endOffset": 135}, {"referenceID": 13, "context": "This approach has yielded many fascinating results: neural networks have been shown to be universal approximators Hornik et al. [1989], Cybenko [1989], their VC dimension studied Sontag [1998], Bartlett and Maass [2003], Bartlett et al.", "startOffset": 114, "endOffset": 151}, {"referenceID": 12, "context": "[1989], Cybenko [1989], their VC dimension studied Sontag [1998], Bartlett and Maass [2003], Bartlett et al.", "startOffset": 51, "endOffset": 65}, {"referenceID": 12, "context": "[1989], Cybenko [1989], their VC dimension studied Sontag [1998], Bartlett and Maass [2003], Bartlett et al.", "startOffset": 51, "endOffset": 92}, {"referenceID": 12, "context": "[1989], Cybenko [1989], their VC dimension studied Sontag [1998], Bartlett and Maass [2003], Bartlett et al. [1998], and connections between boolean and threshold networks to ReLU networks developed in Maass et al.", "startOffset": 51, "endOffset": 116}, {"referenceID": 12, "context": "[1989], Cybenko [1989], their VC dimension studied Sontag [1998], Bartlett and Maass [2003], Bartlett et al. [1998], and connections between boolean and threshold networks to ReLU networks developed in Maass et al. [1994], Pan and Srikumar [2015].", "startOffset": 51, "endOffset": 222}, {"referenceID": 12, "context": "[1989], Cybenko [1989], their VC dimension studied Sontag [1998], Bartlett and Maass [2003], Bartlett et al. [1998], and connections between boolean and threshold networks to ReLU networks developed in Maass et al. [1994], Pan and Srikumar [2015]. VC dimension provides an upper bound on one of the properties we measure, and is further discussed in Appendix D.", "startOffset": 51, "endOffset": 247}, {"referenceID": 12, "context": "[1989], Cybenko [1989], their VC dimension studied Sontag [1998], Bartlett and Maass [2003], Bartlett et al. [1998], and connections between boolean and threshold networks to ReLU networks developed in Maass et al. [1994], Pan and Srikumar [2015]. VC dimension provides an upper bound on one of the properties we measure, and is further discussed in Appendix D.1. Looking at achievable functions also provides a method to compare different neural network architectures. A typical approach to this constructs classes of functions from one network architecture that another is incapable of approximating efficiently Eldan and Shamir [2015], Telgarsky [2015], Martens et al.", "startOffset": 51, "endOffset": 638}, {"referenceID": 12, "context": "[1989], Cybenko [1989], their VC dimension studied Sontag [1998], Bartlett and Maass [2003], Bartlett et al. [1998], and connections between boolean and threshold networks to ReLU networks developed in Maass et al. [1994], Pan and Srikumar [2015]. VC dimension provides an upper bound on one of the properties we measure, and is further discussed in Appendix D.1. Looking at achievable functions also provides a method to compare different neural network architectures. A typical approach to this constructs classes of functions from one network architecture that another is incapable of approximating efficiently Eldan and Shamir [2015], Telgarsky [2015], Martens et al.", "startOffset": 51, "endOffset": 656}, {"referenceID": 12, "context": "[1989], Cybenko [1989], their VC dimension studied Sontag [1998], Bartlett and Maass [2003], Bartlett et al. [1998], and connections between boolean and threshold networks to ReLU networks developed in Maass et al. [1994], Pan and Srikumar [2015]. VC dimension provides an upper bound on one of the properties we measure, and is further discussed in Appendix D.1. Looking at achievable functions also provides a method to compare different neural network architectures. A typical approach to this constructs classes of functions from one network architecture that another is incapable of approximating efficiently Eldan and Shamir [2015], Telgarsky [2015], Martens et al. [2013], Bianchini and Scarselli [2014].", "startOffset": 51, "endOffset": 679}, {"referenceID": 12, "context": "[1989], Cybenko [1989], their VC dimension studied Sontag [1998], Bartlett and Maass [2003], Bartlett et al. [1998], and connections between boolean and threshold networks to ReLU networks developed in Maass et al. [1994], Pan and Srikumar [2015]. VC dimension provides an upper bound on one of the properties we measure, and is further discussed in Appendix D.1. Looking at achievable functions also provides a method to compare different neural network architectures. A typical approach to this constructs classes of functions from one network architecture that another is incapable of approximating efficiently Eldan and Shamir [2015], Telgarsky [2015], Martens et al. [2013], Bianchini and Scarselli [2014]. This is often used to suggest the benefits of deep networks over shallow networks.", "startOffset": 51, "endOffset": 711}, {"referenceID": 12, "context": "[1989], Cybenko [1989], their VC dimension studied Sontag [1998], Bartlett and Maass [2003], Bartlett et al. [1998], and connections between boolean and threshold networks to ReLU networks developed in Maass et al. [1994], Pan and Srikumar [2015]. VC dimension provides an upper bound on one of the properties we measure, and is further discussed in Appendix D.1. Looking at achievable functions also provides a method to compare different neural network architectures. A typical approach to this constructs classes of functions from one network architecture that another is incapable of approximating efficiently Eldan and Shamir [2015], Telgarsky [2015], Martens et al. [2013], Bianchini and Scarselli [2014]. This is often used to suggest the benefits of deep networks over shallow networks. Other work examines the complexity of a single function computed by a deep network, rather than the complexity of a set of functions. The number of linear regions was introduced as a measure of expressivity in Pascanu et al. [2013]. In Montufar et al.", "startOffset": 51, "endOffset": 1027}, {"referenceID": 12, "context": "[1989], Cybenko [1989], their VC dimension studied Sontag [1998], Bartlett and Maass [2003], Bartlett et al. [1998], and connections between boolean and threshold networks to ReLU networks developed in Maass et al. [1994], Pan and Srikumar [2015]. VC dimension provides an upper bound on one of the properties we measure, and is further discussed in Appendix D.1. Looking at achievable functions also provides a method to compare different neural network architectures. A typical approach to this constructs classes of functions from one network architecture that another is incapable of approximating efficiently Eldan and Shamir [2015], Telgarsky [2015], Martens et al. [2013], Bianchini and Scarselli [2014]. This is often used to suggest the benefits of deep networks over shallow networks. Other work examines the complexity of a single function computed by a deep network, rather than the complexity of a set of functions. The number of linear regions was introduced as a measure of expressivity in Pascanu et al. [2013]. In Montufar et al. [2014], it was shown that a specific network achieves an exponential number of linear regions with depth.", "startOffset": 51, "endOffset": 1054}, {"referenceID": 12, "context": "[1989], Cybenko [1989], their VC dimension studied Sontag [1998], Bartlett and Maass [2003], Bartlett et al. [1998], and connections between boolean and threshold networks to ReLU networks developed in Maass et al. [1994], Pan and Srikumar [2015]. VC dimension provides an upper bound on one of the properties we measure, and is further discussed in Appendix D.1. Looking at achievable functions also provides a method to compare different neural network architectures. A typical approach to this constructs classes of functions from one network architecture that another is incapable of approximating efficiently Eldan and Shamir [2015], Telgarsky [2015], Martens et al. [2013], Bianchini and Scarselli [2014]. This is often used to suggest the benefits of deep networks over shallow networks. Other work examines the complexity of a single function computed by a deep network, rather than the complexity of a set of functions. The number of linear regions was introduced as a measure of expressivity in Pascanu et al. [2013]. In Montufar et al. [2014], it was shown that a specific network achieves an exponential number of linear regions with depth. In Section 2.3 we generalize linear regions to network activation patterns, and develop a tight upper bound on the number of achievable activation patterns which the construction in Montufar et al. [2014] saturates.", "startOffset": 51, "endOffset": 1358}, {"referenceID": 26, "context": "The expressivity of these random networks is largely unexplored, though a connection between random networks and compositional kernels is developed in Daniely et al. [2016]. The class of random networks enables tractable theoretical and experimental analysis of interrelated aspects of expressivity: trajectory length, neuron transitions, network activation patterns, and number of dichotomies (number of labellings on inputs).", "startOffset": 151, "endOffset": 173}, {"referenceID": 0, "context": "convolutional networks LeCun et al. [1998], and explored theoretically in Cohen et al.", "startOffset": 23, "endOffset": 43}, {"referenceID": 0, "context": "convolutional networks LeCun et al. [1998], and explored theoretically in Cohen et al. [2015]. In particular the depth of the neural network is often seen to take advantage of the hierarchical and compositional structure of features in datasets.", "startOffset": 23, "endOffset": 94}, {"referenceID": 27, "context": "In a companion paper Poole et al. [2016] we explore closely related questions by using mean field theory to explore the propagation of curvature through deep networks.", "startOffset": 21, "endOffset": 41}, {"referenceID": 28, "context": "Our results mostly examine the cases where \u03c6 is a hard-tanh Collobert and Bengio [2004] or ReLU nonlinearity.", "startOffset": 60, "endOffset": 88}, {"referenceID": 27, "context": "This phase transition behavior is explored further in our companion paper Poole et al. [2016]. The proof can be found in the Appendix.", "startOffset": 74, "endOffset": 94}, {"referenceID": 29, "context": "The question of the number of regions for a specific hyperplane arrangement is well studied Stanley [2011], with", "startOffset": 92, "endOffset": 107}, {"referenceID": 25, "context": "In Montufar et al. [2014], it is observed that a (trivial) upper bound is given by 2 (all possible subsets of all possible neurons) and a construction of linear regions increasing exponentially with depth is given.", "startOffset": 3, "endOffset": 26}, {"referenceID": 25, "context": "In Montufar et al. [2014], it is observed that a (trivial) upper bound is given by 2 (all possible subsets of all possible neurons) and a construction of linear regions increasing exponentially with depth is given. This upper bound is exponentially looser than the bound given through regions of a hyperplane arrangement, which is shown asymptotically tight with the construction in Montufar et al. [2014].", "startOffset": 3, "endOffset": 406}, {"referenceID": 27, "context": "This is further supported by an exponential decrease in autocorrelation length in function space, which we derive in the Supplemental Material of our companion paper Poole et al. [2016]. Our results further suggest the following conjectures: Conjecture 2.", "startOffset": 166, "endOffset": 186}, {"referenceID": 27, "context": ") In our companion paper Poole et al. [2016], we explore the effect of width and total depth when training only the final layer in the network.", "startOffset": 25, "endOffset": 45}], "year": 2016, "abstractText": "We study the expressivity of deep neural networks with random weights. We provide several results, both theoretical and experimental, precisely characterizing their functional properties in terms of the depth and width of the network. In doing so, we illustrate inherent connections between the length of a latent trajectory, local neuron transitions, and network activation patterns. The latter, a notion defined in this paper, is further studied using properties of hyperplane arrangements, which also help precisely characterize the effect of the neural network on the input space. We further show dualities between changes to the latent state and changes to the network weights, and between the number of achievable activation patterns and the number of achievable labellings over input data. We see that the depth of the network affects all of these quantities exponentially, while the width appears at most as a base. These results also suggest that the remaining depth of a neural network is an important determinant of expressivity, supported by experiments on MNIST and CIFAR-10.", "creator": "LaTeX with hyperref package"}}}