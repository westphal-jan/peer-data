{"id": "1606.05704", "review": {"conference": "ACL", "VERSION": "v1", "DATE_OF_SUBMISSION": "17-Jun-2016", "title": "A Piece of My Mind: A Sentiment Analysis Approach for Online Dispute Detection", "abstract": "We investigate the novel task of online dispute detection and propose a sentiment analysis solution to the problem: we aim to identify the sequence of sentence-level sentiments expressed during a discussion and to use them as features in a classifier that predicts the DISPUTE/NON-DISPUTE label for the discussion as a whole. We evaluate dispute detection approaches on a newly created corpus of Wikipedia Talk page disputes and find that classifiers that rely on our sentiment tagging features outperform those that do not. The best model achieves a very promising F1 score of 0.4, so we could build on these results. We suggest that we apply a model that employs some of the principles of the DISPUTE/NON-DISPUTE label. This is a strong model that will allow us to assess the semantic diversity in terms of the problem-solving techniques used.\n\n\n\nThe Discussion\nWe hope that this paper could be a useful primer on the topic of online dispute detection and consider the idea that there is a widespread use of the DISPUTE/NON-DISPUTE label in the public domain.\nAcknowledgments\nWe wish to thank all the researchers who provided helpful feedback on this paper. We thank all our colleagues and colleagues for their invaluable input and understanding of the paper.", "histories": [["v1", "Fri, 17 Jun 2016 23:22:39 GMT  (110kb,D)", "http://arxiv.org/abs/1606.05704v1", "ACL 2014"]], "COMMENTS": "ACL 2014", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["lu wang", "claire cardie"], "accepted": true, "id": "1606.05704"}, "pdf": {"name": "1606.05704.pdf", "metadata": {"source": "CRF", "title": "A Piece of My Mind: A Sentiment Analysis Approach for Online Dispute Detection", "authors": ["Lu Wang", "Claire Cardie"], "emails": ["luwang@cs.cornell.edu", "cardie@cs.cornell.edu"], "sections": [{"heading": "1 Introduction", "text": "As the web has grown in popularity and scope, so has the promise of collaborative information environments for the joint creation and exchange of knowledge (Jones and Rafaeli, 2000; Sack, 2005). Wikipedia, a wiki-based online encyclopedia, is arguably the best example: its distributed editing environment allows readers to collaborate as content editors and has facilitated the production of over four billion articles1 of surprisingly high quality (Giles, 2005) in English alone since its debut in 2001.\nExisting studies of collaborative knowledge systems have shown, however, that the quality of the generated content (e.g. an encyclopedia article) is highly correlated with the effectiveness of the online collaboration (Kittur and Kraut, 2008; Kraut and Resnick, 2012); fruitful collaboration, in turn, inevitably requires dealing with the disputes and conflicts that arise (Kittur et al., 2007). Unfortunately, human monitoring of the often massive social media and collaboration sites to detect, much less mediate, disputes is not feasible.\nIn this work, we investigate the heretofore novel task of dispute detection in online discussions. Previous work in this general area has analyzed\n1 http://en.wikipedia.org\ndispute-laden content to discover features correlated with conflicts and disputes (Kittur et al., 2007). Research focused primarily on cues derived from the edit history of the jointly created content (e.g. the number of revisions, their temporal density (Kittur et al., 2007; Yasseri et al., 2012)) and relied on small numbers of manually selected discussions known to involve disputes. In contrast, we investigate methods for the automatic detection, i.e. prediction, of discussions involving disputes. We are also interested in understanding whether, and which, linguistic features of the discussion are important for dispute detection.\nDrawing inspiration from studies of human mediation of online conflicts (e.g. Billings and Watts (2010), Kittur et al. (2007), Kraut and Resnick (2012)), we hypothesize that effective methods for dispute detection should take into account the sentiment and opinions expressed by participants in the collaborative endeavor. As a result, we propose a sentiment analysis approach for online dispute detection that identifies the sequence of sentence-level sentiments (i.e. very negative, negative, neutral, positive, very positive) expressed during the discussion and uses them as features in a classifier that predicts the DISPUTE/NONDISPUTE label for the discussion as a whole. Consider, for example, the snippet in Figure 1 from the Wikipedia Talk page for the article on Philadelphia; it discusses the choice of a picture for the article\u2019s \u201cinfobox\u201d. The sequence of almost exclusively negative statements provides evidence of a dispute in this portion of the discussion.\nUnfortunately, sentence-level sentiment tagging for this domain is challenging in its own right due to the less formal, often ungrammatical, language and the dynamic nature of online conversations. \u201cReally, grow up\u201d (segment 3) should presumably be tagged as a negative sentence as should the sarcastic sentences \u201cSounds good?\u201d (in the same turn) and \u201ccongrats\u201d and \u201cthank you\u201d\nar X\niv :1\n60 6.\n05 70\n4v 1\n[ cs\n.C L\n] 1\n7 Ju\nn 20\n16\n(in segment 2). We expect that these, and other, examples will be difficult for the sentence-level classifier unless the discourse context of each sentence is considered. Previous research on sentiment prediction for online discussions, however, focuses on turn-level predictions (Hahn et al., 2006; Yin et al., 2012).2 As the first work that predicts sentence-level sentiment for online discussions, we investigate isotonic Conditional Random Fields (CRFs) (Mao and Lebanon, 2007) for the sentiment-tagging task as they preserve the advantages of the popular CRF-based sequential tagging models (Lafferty et al., 2001) while providing an efficient mechanism for encoding domain knowledge \u2014 in our case, a sentiment lexicon \u2014 through isotonic constraints on model parameters.\nWe evaluate our dispute detection approach using a newly created corpus of discussions from Wikipedia Talk pages (3609 disputes, 3609 nondisputes).3 We find that classifiers that employ the learned sentiment features outperform others that do not. The best model achieves a very promising F1 score of 0.78 and an accuracy of 0.80 on the Wikipedia dispute corpus. To the best of our knowledge, this represents the first computational approach to automatically identify online disputes on a dataset of scale.\nAdditional Related Work. Sentiment analysis has been utilized as a key enabling technique in a number of conversation-based applications. Previous work mainly studies the attitudes in spoken meetings (Galley et al., 2004; Hahn et al., 2006) or broadcast conversations (Wang et al., 2011) using\n2A notable exception is Hassan et al. (2010), which identifies sentences containing \u201cattitudes\u201d (e.g. opinions), but does not distinguish them w.r.t. sentiment. Context information is also not considered.\n3The talk page associated with each article records conversations among editors about the article content and allows editors to discuss the writing process, e.g. planning and organizing the content.\nvariants of Conditional Random Fields (Lafferty et al., 2001) and predicts sentiment at the turn-level, while our predictions are made for each sentence."}, {"heading": "2 Data Construction: A Dispute Corpus", "text": "We construct the first dispute detection corpus to date; it consists of dispute and non-dispute discussions from Wikipedia Talk pages. Step 1: Get Talk Pages of Disputed Articles. Wikipedia articles are edited by different editors. If an article is observed to have disputes on its talk page, editors can assign dispute tags to the article to flag it for attention. In this research, we are interested in talk pages whose corresponding articles are labeled with the following tags: DISPUTED, TOTALLYDISPUTED, DISPUTEDSECTION, TOTALLYDISPUTED-SECTION, POV. The tags indicate that an article is disputed, or the neutrality of the article is disputed (POV).\nWe use the 2013-03-04 Wikipedia data dump, and extract talk pages for articles that are labeled with dispute tags by checking the revision history. This results in 19,071 talk pages. Step 2: Get Discussions with Disputes. Dispute tags can also be added to talk pages themselves. Therefore, in addition to the tags mentioned above, we also consider the \u201cRequest for Comment\u201d (RFC) tag on talk pages. According to Wikipedia4, RFC is used to request outside opinions concerning the disputes.\n3609 discussions are collected with dispute tags found in the revision history. We further classify dispute discussions into three subcategories: CONTROVERSY, REQUEST FOR COMMENT (RFC), and RESOLVED based on the tags found in discussions (see Table 1). The numbers of discussions for the three types are 42, 3484, and 105, respectively. Note that dispute tags only appear in a small number of articles and talk pages. There may exist other discussions with disputes.\nStep 3: Get Discussions without Disputes. Likewise, we collect non-dispute discussions from pages that are never tagged with disputes. We consider non-dispute discussions with at least 3 dis-\n4 http://en.wikipedia.org/wiki/Wikipedia:\nRequests_for_comment\ntinct speakers and 10 turns. 3609 discussions are randomly selected with this criterion. The average turn numbers for dispute and non-dispute discussions are 45.03 and 22.95, respectively."}, {"heading": "3 Sentence-level Sentiment Prediction", "text": "This section describes our sentence-level sentiment tagger, from which we construct features for dispute detection (Section 4).\nConsider a discussion comprised of sequential turns; each turn consists of a sequence of sentences. Our model takes as input the sentences x = {x1, \u00b7 \u00b7 \u00b7 , xn} from a single turn, and outputs the corresponding sequence of sentiment labels y = {y1, \u00b7 \u00b7 \u00b7 , yn}, where yi \u2208 O,O = {NN,N,O,P,PP}. The labels in O represent very negative (NN), negative (N), neutral (O), positive (P), and very positive (PP), respectively.\nGiven that traditional Conditional Random Fields (CRFs) (Lafferty et al., 2001) ignore the ordinal relations among sentiment labels, we choose isotonic CRFs (Mao and Lebanon, 2007) for sentence-level sentiment analysis as they can enforce monotonicity constraints on the parameters consistent with the ordinal structure and domain knowledge (e.g. word-level sentiment conveyed via a lexicon). Concretely, we take a lexicon M =Mp\u222aMn, whereMp andMn are two sets of features (usually words) identified as strongly associated with positive and negative sentiment. Assume \u00b5\u3008\u03c3,w\u3009 encodes the weight between label \u03c3 and feature w, for each feature w \u2208 Mp; then the isotonic CRF enforces \u03c3 \u2264 \u03c3\u2032 \u21d2 \u00b5\u3008\u03c3,w\u3009 \u2264 \u00b5\u3008\u03c3\u2032,w\u3009. For example, when we observe \u201ctotally agree\u201d in the training data, the feature parameter for \u00b5\u3008PP,totally agree\u3009 is likely to increase. Similar constraints are defined onMn.\nOur lexicon is built by combining MPQA (Wilson et al., 2005), General Inquirer (Stone et al., 1966), and SentiWordNet (Esuli and Sebastiani, 2006) lexicons. Words with contradictory sentiments are removed. We use the features in Table 2 for sentiment prediction. Syntactic/Semantic Features. We have two versions of dependency relation features, the original form and a form that generalizes a word to its POS tag, e.g. \u201cnsubj(wrong, you)\u201d is generalized to \u201cnsubj(ADJ, you)\u201d and \u201cnsubj(wrong, PRP)\u201d. Discourse Features. We extract the initial unigram, bigram, and trigram of each utterance as discourse features (Hirschberg and Litman, 1993).\nSentiment Features. We gather connectives from the Penn Discourse TreeBank (Rashmi Prasad and Webber, 2008) and combine them with any sentiment word that precedes or follows it as new features. Sentiment dependency relations are the dependency relations that include a sentiment word. We replace those words with their polarity equivalents. For example, relation \u201cnsubj(wrong, you)\u201d becomes \u201cnsubj(SentiWordneg, you)\u201d."}, {"heading": "4 Online Dispute Detection", "text": ""}, {"heading": "4.1 Training A Sentiment Classifier", "text": "Dataset. We train the sentiment classifier using the Authority and Alignment in Wikipedia Discussions (AAWD) corpus (Bender et al., 2011) on a 5- point scale (i.e. NN, N, O, P, PP). AAWD consists of 221 English Wikipedia discussions with positive and negative alignment annotations. Annotators either label each sentence as positive, negative or neutral, or label the full turn. For instances that have only a turn-level label, we assume all sentences have the same label as the turn. We further transform the labels into the five sentiment labels. Sentences annotated as being a positive alignment by at least two annotators are treated as very positive (PP). If a sentence is only selected as positive by one annotator or obtains the label via turn-level annotation, it is positive (P). Very negative (NN) and negative (N) are collected in the same way. All others are neutral (O). Among all 16,501 sentences in AAWD, 1,930 and 1,102 are labeled as NN and N. 532 and 99 of them are PP and P. The other 12,648 are considered neutral. Evaluation. To evaluate the performance of the sentiment tagger, we compare to two baselines. (1) Baseline (Polarity): a sentence is predicted as positive if it has more positive words than negative words, or negative if more negative words are observed. Otherwise, it is neutral. (2) Baseline (Distance) is extended from (Hassan et al., 2010). Each sentiment word is associated with the closest\nsecond person pronoun, and a surface distance is computed. An SVM classifier (Joachims, 1999) is trained using features of the sentiment words and minimum/maximum/average of the distances.\nWe also compare with two state-of-the-art methods that are used in sentiment prediction for conversations: (1) an SVM (RBF kernel) that is employed for identifying sentiment-bearing sentences (Hassan et al., 2010), and (dis)agreement detection (Yin et al., 2012) in online debates; (2) a Linear CRF for (dis)agreement identification in broadcast conversations (Wang et al., 2011).\nWe evaluate the systems using standard F1 on classes of positive, negative, and neutral, where samples predicted as PP and P are positive alignment, and samples tagged as NN and N are negative alignment. Table 3 describes the main results on the AAWD dataset: our isotonic CRF based system significantly outperforms the alternatives for positive and negative alignment detection (paired-t test, p < 0.05)."}, {"heading": "4.2 Dispute Detection", "text": "We model dispute detection as a standard binary classification task, and investigate four major types of features as described below. Lexical Features. We first collect unigram and bigram features for each discussion. Topic Features. Articles on specific topics, such as politics or religions, tend to arouse more disputes. We thus extract the category information of the corresponding article for each talk page. We further utilize unigrams and bigrams of the category as topic features. Discussion Features. This type of feature aims to capture the structure of the discussion. Intuitively, the more turns or the more participants a discussion has, the more likely there is a dispute. Meanwhile, participants tend to produce longer utterances when they make arguments.\nWe choose number of turns, number of participants, average number of words in each turn as features. In addition, the frequency of revisions made during the discussion has been shown to be good indicator for controversial articles (Vuong et al., 2008), that are presumably prone to have disputes. Therefore, we encode the number of revisions that happened during the discussion as a feature. Sentiment Features. This set of features encode the sentiment distribution and transition in the discussion. We train our sentiment tagging model on the full AAWD dataset, and run it on the Wikipedia dispute corpus.\nGiven that consistent negative sentiment flow usually indicates an ongoing dispute, we first extract features from sentiment distribution in the form of number/probability of sentiment per type. We also estimate the sentiment transition probability P (St \u2192 St+1) from our predictions, where St and St+1 are sentiment labels for the current sentence and the next. We then have features as number/portion of sentiment transitions per type.\nFeatures described above mostly depict the global sentiment flow in the discussions. We further construct a local version of them, since sentiment distribution may change as discussion proceeds. For example, less positive sentiment can be observed as dispute being escalated. We thus split each discussion into three equal length stages, and create sentiment distribution and transition features for each stage.\nResults and Error Analysis. We experiment with logistic regression, SVM with linear and RBF kernels, which are effective methods in multiple text categorization tasks (Joachims, 1999; Zhang and J. Oles, 2001). We normalize the features by standardization and conduct a 5-fold cross-validation. Two baselines are listed: (1) labels are randomly assigned; (2) all discussions have disputes.\nMain results for different classifiers are displayed in Table 4. All learning based methods outperform the two baselines, and among them, SVM with the RBF kernel achieves the best F1 score and accuracy (0.78 and 0.80). Experimental results with various combinations of features sets are displayed in Table 5. As it can be seen, sentiment features obtains the best accuracy among the four types of features. A combination of topic, discussion, and sentiment features achieves the best performance on recall, F1, and accuracy. Specifically, the accuracy is significantly higher than all the other systems (paired-t test, p < 0.05).\nAfter a closer look at the results, we find two main reasons for incorrect predictions. Firstly, errors from sentiment prediction get propagated into dispute detection. Due to the limitation of existing general-purpose lexicons, some opinionated dialog-specific terms are hard to catch. For example, \u201cI told you over and over again...\u201d strongly suggests a negative sentiment, but no single word shows negative connotation. Constructing a lexicon tuned for conversational text might further im-\nprove the performance. Secondly, some dispute discussions are harder to detect than the others due to different dialog structures. For instance, the recalls for dispute discussions of \u201ccontroversy\u201d, \u201cRFC\u201d, and \u201cresolved\u201d are 0.78, 0.79, and 0.86 respectively. We intend to design models that are able to capture dialog structures, such as pragmatic information, in the future work. Sentiment Flow Visualization. We visualize the sentiment flow of two disputed discussions in Figure 2. The plots reveal persistent negative sentiment in unresolved disputes (top). For the resolved dispute (bottom), participants show gratitude when the problem is settled."}, {"heading": "5 Conclusion", "text": "We present a sentiment analysis-based approach to online dispute detection. We create a largescale dispute corpus from Wikipedia Talk pages to study the problem. A sentiment prediction model based on isotonic CRFs is proposed to output sentiment labels at the sentence-level. Experiments on our dispute corpus also demonstrate that classifiers trained with sentiment tagging features outperform others that do not. Acknowledgments We heartily thank the Cornell NLP\nGroup, the reviewers, and Yiye Ruan for helpful comments.\nWe also thank Emily Bender and Mari Ostendorf for provid-\ning the AAWD dataset. This work was supported in part\nby NSF grants IIS-0968450 and IIS-1314778, and DARPA\nDEFT Grant FA8750-13-2-0015. The views and conclusions\ncontained herein are those of the authors and should not be\ninterpreted as necessarily representing the official policies or\nendorsements, either expressed or implied, of NSF, DARPA\nor the U.S. Government."}], "references": [{"title": "Annotating social acts: Authority claims and alignment moves in wikipedia talk pages", "author": ["Jonathan T. Morgan", "Meghan Oxley", "Mark Zachry", "Brian Hutchinson", "Alex Marin", "Bin Zhang", "Mari Ostendorf"], "venue": null, "citeRegEx": "Bender et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Bender et al\\.", "year": 2011}, {"title": "Understanding dispute resolution online: using text to reflect personal and substantive issues in conflict", "author": ["Billings", "Watts2010] Matt Billings", "Leon Adam Watts"], "venue": null, "citeRegEx": "Billings et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Billings et al\\.", "year": 2010}, {"title": "Sentiwordnet: A publicly available lexical resource for opinion mining", "author": ["Esuli", "Sebastiani2006] Andrea Esuli", "Fabrizio Sebastiani"], "venue": "Proceedings of the 5th Conference on Language Resources and Evaluation", "citeRegEx": "Esuli et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Esuli et al\\.", "year": 2006}, {"title": "The conll-2010 shared task: Learning to detect hedges and their scope in natural language text", "author": ["Veronika Vincze", "Gy\u00f6rgy M\u00f3ra", "J\u00e1nos Csirik", "Gy\u00f6rgy Szarvas"], "venue": "In Proceedings of the Fourteenth Conference", "citeRegEx": "Farkas et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Farkas et al\\.", "year": 2010}, {"title": "Identifying agreement and disagreement in conversational speech: use of Bayesian networks to model pragmatic dependencies", "author": ["Galley et al.2004] Michel Galley", "Kathleen McKeown", "Julia Hirschberg", "Elizabeth Shriberg"], "venue": null, "citeRegEx": "Galley et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Galley et al\\.", "year": 2004}, {"title": "Internet encyclopaedias go head to head", "author": ["G. Giles"], "venue": "Nature,", "citeRegEx": "Giles.,? \\Q2005\\E", "shortCiteRegEx": "Giles.", "year": 2005}, {"title": "Agreement/disagreement classification: Exploiting unlabeled data using contrast classifiers", "author": ["Hahn et al.2006] Sangyun Hahn", "Richard Ladner", "Mari Ostendorf"], "venue": "In Proceedings of the Human Language Technology Conference of the NAACL,", "citeRegEx": "Hahn et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Hahn et al\\.", "year": 2006}, {"title": "What\u2019s with the attitude?: Identifying sentences with attitude in online discussions", "author": ["Hassan et al.2010] Ahmed Hassan", "Vahed Qazvinian", "Dragomir Radev"], "venue": "In Proceedings of the 2010 Conference on Empirical Methods in Natural Language Pro-", "citeRegEx": "Hassan et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Hassan et al\\.", "year": 2010}, {"title": "Advances in kernel methods. chapter Making Large-scale Support Vector Machine Learning Practical, pages 169\u2013184", "author": ["Thorsten Joachims"], "venue": null, "citeRegEx": "Joachims.,? \\Q1999\\E", "shortCiteRegEx": "Joachims.", "year": 1999}, {"title": "Time to split, virtually: discourse architecture and community building create vibrant virtual publics", "author": ["Jones", "Rafaeli2000] Q. Jones", "S. Rafaeli"], "venue": "Electronic Markets,", "citeRegEx": "Jones et al\\.,? \\Q2000\\E", "shortCiteRegEx": "Jones et al\\.", "year": 2000}, {"title": "Harnessing the wisdom of crowds in wikipedia: Quality through coordination", "author": ["Kittur", "Kraut2008] Aniket Kittur", "Robert E. Kraut"], "venue": "In Proceedings of the 2008 ACM Conference on Computer Supported Cooperative Work,", "citeRegEx": "Kittur et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Kittur et al\\.", "year": 2008}, {"title": "He says, she says: Conflict and coordination in wikipedia", "author": ["Kittur et al.2007] Aniket Kittur", "Bongwon Suh", "Bryan A. Pendleton", "Ed H. Chi"], "venue": "In Proceedings of the SIGCHI Conference on Human Factors in Computing Systems,", "citeRegEx": "Kittur et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Kittur et al\\.", "year": 2007}, {"title": "Building successful online communities: Evidence-based social design", "author": ["Kraut", "Resnick2012] R.E. Kraut", "P. Resnick"], "venue": null, "citeRegEx": "Kraut et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Kraut et al\\.", "year": 2012}, {"title": "Conditional random fields: Probabilistic models for segmenting and labeling sequence data", "author": ["Andrew McCallum", "Fernando C.N. Pereira"], "venue": "In Proceedings of the Eighteenth International Conference", "citeRegEx": "Lafferty et al\\.,? \\Q2001\\E", "shortCiteRegEx": "Lafferty et al\\.", "year": 2001}, {"title": "Isotonic conditional random fields and local sentiment flow", "author": ["Mao", "Lebanon2007] Yi Mao", "Guy Lebanon"], "venue": "In Advances in Neural Information Processing Systems", "citeRegEx": "Mao et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Mao et al\\.", "year": 2007}, {"title": "The penn discourse treebank 2.0", "author": ["Rashmi Prasad", "Nikhil Dinesh", "Bonnie Webber"], "venue": "In Proceedings of the Sixth International Conference on Lan-", "citeRegEx": "Prasad et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Prasad et al\\.", "year": 2008}, {"title": "Digital formations: It and new architectures in the global realm. chapter Discourse architecture and very large-scale conversation, pages 242\u2013282", "author": ["W. Sack"], "venue": null, "citeRegEx": "Sack.,? \\Q2005\\E", "shortCiteRegEx": "Sack.", "year": 2005}, {"title": "The General Inquirer: A Computer Approach to Content Analysis", "author": ["Dexter C. Dunphy", "Marshall S. Smith", "Daniel M. Ogilvie"], "venue": null, "citeRegEx": "Stone et al\\.,? \\Q1966\\E", "shortCiteRegEx": "Stone et al\\.", "year": 1966}, {"title": "On ranking controversies in wikipedia: Models and evaluation", "author": ["Vuong et al.2008] Ba-Quy Vuong", "Ee-Peng Lim", "Aixin Sun", "Minh-Tam Le", "Hady Wirawan Lauw", "Kuiyu Chang"], "venue": "In Proceedings of the 2008 International Conference", "citeRegEx": "Vuong et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Vuong et al\\.", "year": 2008}, {"title": "Detection of agreement and disagreement in broadcast conversations", "author": ["Wang et al.2011] Wen Wang", "Sibel Yaman", "Kristin Precoda", "Colleen Richey", "Geoffrey Raymond"], "venue": "In Proceedings of the 49th Annual Meeting of the Association", "citeRegEx": "Wang et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Wang et al\\.", "year": 2011}, {"title": "Recognizing contextual polarity in phrase-level sentiment analysis", "author": ["Janyce Wiebe", "Paul Hoffmann"], "venue": "In Proceedings of the Conference on Human Language Technology and Empirical Methods", "citeRegEx": "Wilson et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Wilson et al\\.", "year": 2005}, {"title": "Dynamics of conflicts in wikipedia. CoRR, abs/1202.3643", "author": ["Yasseri et al.2012] Taha Yasseri", "R\u00f3bert Sumi", "Andr\u00e1s Rung", "Andr\u00e1s Kornai", "J\u00e1nos Kert\u00e9sz"], "venue": null, "citeRegEx": "Yasseri et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Yasseri et al\\.", "year": 2012}, {"title": "Unifying local and global agreement and disagreement classification in online debates", "author": ["Yin et al.2012] Jie Yin", "Paul Thomas", "Nalin Narang", "Cecile Paris"], "venue": "In Proceedings of the 3rd Workshop in Computational Approaches to Subjectivity and Sen-", "citeRegEx": "Yin et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Yin et al\\.", "year": 2012}, {"title": "Text categorization based on regularized linear classification methods", "author": ["Zhang", "J. Oles2001] Tong Zhang", "Frank J. Oles"], "venue": "Inf. Retr.,", "citeRegEx": "Zhang et al\\.,? \\Q2001\\E", "shortCiteRegEx": "Zhang et al\\.", "year": 2001}], "referenceMentions": [{"referenceID": 16, "context": "vironments for the joint creation and exchange of knowledge (Jones and Rafaeli, 2000; Sack, 2005).", "startOffset": 60, "endOffset": 97}, {"referenceID": 5, "context": "content editors and has facilitated the production of over four billion articles1 of surprisingly high quality (Giles, 2005) in English alone since its debut in 2001.", "startOffset": 111, "endOffset": 124}, {"referenceID": 11, "context": "an encyclopedia article) is highly correlated with the effectiveness of the online collaboration (Kittur and Kraut, 2008; Kraut and Resnick, 2012); fruitful collaboration, in turn, inevitably requires dealing with the disputes and conflicts that arise (Kittur et al., 2007).", "startOffset": 252, "endOffset": 273}, {"referenceID": 11, "context": "org dispute-laden content to discover features correlated with conflicts and disputes (Kittur et al., 2007).", "startOffset": 86, "endOffset": 107}, {"referenceID": 11, "context": "poral density (Kittur et al., 2007; Yasseri et al., 2012)) and relied on small numbers of manually selected discussions known to involve disputes.", "startOffset": 14, "endOffset": 57}, {"referenceID": 21, "context": "poral density (Kittur et al., 2007; Yasseri et al., 2012)) and relied on small numbers of manually selected discussions known to involve disputes.", "startOffset": 14, "endOffset": 57}, {"referenceID": 10, "context": "Billings and Watts (2010), Kittur et al. (2007), Kraut and Resnick (2012)), we hypothesize that effective methods for dispute detection should take into account the sentiment and opinions expressed by participants in the collaborative endeavor.", "startOffset": 27, "endOffset": 48}, {"referenceID": 10, "context": "Billings and Watts (2010), Kittur et al. (2007), Kraut and Resnick (2012)), we hypothesize that effective methods for dispute detection should take into account the sentiment and opinions expressed by participants in the collaborative endeavor.", "startOffset": 27, "endOffset": 74}, {"referenceID": 6, "context": "Previous research on sentiment prediction for online discussions, however, focuses on turn-level predictions (Hahn et al., 2006; Yin et al., 2012).", "startOffset": 109, "endOffset": 146}, {"referenceID": 22, "context": "Previous research on sentiment prediction for online discussions, however, focuses on turn-level predictions (Hahn et al., 2006; Yin et al., 2012).", "startOffset": 109, "endOffset": 146}, {"referenceID": 13, "context": "dom Fields (CRFs) (Mao and Lebanon, 2007) for the sentiment-tagging task as they preserve the advantages of the popular CRF-based sequential tagging models (Lafferty et al., 2001) while providing an efficient mechanism for encoding domain", "startOffset": 156, "endOffset": 179}, {"referenceID": 4, "context": "Previous work mainly studies the attitudes in spoken meetings (Galley et al., 2004; Hahn et al., 2006) or broadcast conversations (Wang et al.", "startOffset": 62, "endOffset": 102}, {"referenceID": 6, "context": "Previous work mainly studies the attitudes in spoken meetings (Galley et al., 2004; Hahn et al., 2006) or broadcast conversations (Wang et al.", "startOffset": 62, "endOffset": 102}, {"referenceID": 19, "context": ", 2006) or broadcast conversations (Wang et al., 2011) using", "startOffset": 35, "endOffset": 54}, {"referenceID": 7, "context": "A notable exception is Hassan et al. (2010), which identifies sentences containing \u201cattitudes\u201d (e.", "startOffset": 23, "endOffset": 44}, {"referenceID": 13, "context": "Given that traditional Conditional Random Fields (CRFs) (Lafferty et al., 2001) ignore the or-", "startOffset": 56, "endOffset": 79}, {"referenceID": 20, "context": "Our lexicon is built by combining MPQA (Wilson et al., 2005), General Inquirer (Stone et al.", "startOffset": 39, "endOffset": 60}, {"referenceID": 17, "context": ", 2005), General Inquirer (Stone et al., 1966), and SentiWordNet (Esuli and Sebastiani, 2006) lexicons.", "startOffset": 26, "endOffset": 46}, {"referenceID": 3, "context": "Lexical Features Syntactic/Semantic Features - unigram/bigram - unigram with POS tag - number of words all uppercased - dependency relation - number of words Conversation Features Discourse Features - quote overlap with target - initial uni-/bi-/tri-gram - TFIDF similarity with target - repeated punctuations (remove quote first) - hedging phrases collected from Sentiment Features Farkas et al. (2010) - connective + sentiment words - number of negators - sentiment dependency relation - sentiment words", "startOffset": 383, "endOffset": 404}, {"referenceID": 0, "context": "We train the sentiment classifier using the Authority and Alignment in Wikipedia Discussions (AAWD) corpus (Bender et al., 2011) on a 5-", "startOffset": 107, "endOffset": 128}, {"referenceID": 7, "context": "(2) Baseline (Distance) is extended from (Hassan et al., 2010).", "startOffset": 41, "endOffset": 62}, {"referenceID": 8, "context": "An SVM classifier (Joachims, 1999) is trained using features of the sentiment words and minimum/maximum/average of the distances.", "startOffset": 18, "endOffset": 34}, {"referenceID": 7, "context": "methods that are used in sentiment prediction for conversations: (1) an SVM (RBF kernel) that is employed for identifying sentiment-bearing sentences (Hassan et al., 2010), and (dis)agreement detection (Yin et al.", "startOffset": 150, "endOffset": 171}, {"referenceID": 22, "context": ", 2010), and (dis)agreement detection (Yin et al., 2012) in online debates; (2)", "startOffset": 38, "endOffset": 56}, {"referenceID": 19, "context": "a Linear CRF for (dis)agreement identification in broadcast conversations (Wang et al., 2011).", "startOffset": 74, "endOffset": 93}, {"referenceID": 18, "context": "In addition, the frequency of revisions made during the discussion has been shown to be good indicator for controversial articles (Vuong et al., 2008), that are presumably prone to have disputes.", "startOffset": 130, "endOffset": 150}, {"referenceID": 8, "context": "logistic regression, SVM with linear and RBF kernels, which are effective methods in multiple text categorization tasks (Joachims, 1999; Zhang and J. Oles, 2001).", "startOffset": 120, "endOffset": 161}], "year": 2016, "abstractText": "We investigate the novel task of online dispute detection and propose a sentiment analysis solution to the problem: we aim to identify the sequence of sentence-level sentiments expressed during a discussion and to use them as features in a classifier that predicts the DISPUTE/NON-DISPUTE label for the discussion as a whole. We evaluate dispute detection approaches on a newly created corpus of Wikipedia Talk page disputes and find that classifiers that rely on our sentiment tagging features outperform those that do not. The best model achieves a very promising F1 score of 0.78 and an accuracy of 0.80.", "creator": "LaTeX with hyperref package"}}}