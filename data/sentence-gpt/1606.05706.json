{"id": "1606.05706", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "17-Jun-2016", "title": "Improving Agreement and Disagreement Identification in Online Discussions with A Socially-Tuned Sentiment Lexicon", "abstract": "We study the problem of agreement and disagreement detection in online discussions. An isotonic Conditional Random Fields (isotonic CRF) based sequential model is proposed to make predictions on sentence- or segment-level. We automatically construct a socially-tuned lexicon that is bootstrapped from existing general-purpose sentiment lexicons to further improve the performance of online discussions. We create a lexicon based on these concepts and use it as an experimental tool. It will be applied to all websites, but we do not intend to share these principles with any sites, and will do so in the future.", "histories": [["v1", "Fri, 17 Jun 2016 23:29:11 GMT  (24kb)", "http://arxiv.org/abs/1606.05706v1", "ACL WASSA workshop 2014"]], "COMMENTS": "ACL WASSA workshop 2014", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["lu wang", "claire cardie"], "accepted": false, "id": "1606.05706"}, "pdf": {"name": "1606.05706.pdf", "metadata": {"source": "CRF", "title": "Improving Agreement and Disagreement Identification in Online Discussions with A Socially-Tuned Sentiment Lexicon", "authors": ["Lu Wang"], "emails": ["luwang@cs.cornell.edu", "cardie@cs.cornell.edu"], "sections": [{"heading": null, "text": "ar X\niv :1\n60 6.\n05 70\n6v 1\n[ cs\n.C L\n] 1\n7 Ju\nn 20\n16"}, {"heading": "1 Introduction", "text": "We are in an era where people can easily voice and exchange their opinions on the internet through forums or social media. Mining public opinion and the social interactions from online discussions is an important task, which has a wide range of applications. For example, by analyzing the users\u2019 attitude in forum posts on social and political problems, it is able to identify ideological stance (Somasundaran and Wiebe, 2009) and user relations (Qiu et al., 2013), and thus further discover subgroups (Hassan et al., 2012; Abu-Jbara et al., 2012) with similar ideological viewpoint. Meanwhile, catching the sentiment in the conversation can help detect online disputes, reveal popular or controversial topics, and potentially disclose the public opinion formation process.\nIn this work, we study the problem of agreement and disagreement identification in online discussions. Sentence-level agreement and disagreement detection for this domain is challenging in its own right due to the dynamic nature of online conversations, and the less formal, and usually very emotional language used. As an example, consider a snippet of discussion from Wikipedia Talk page for article \u201cIraq War\u201d where editors argue on the correctness of the information in the opening paragraph (Figure 1). \u201cSo what?\u201d should presumably be tagged as a negative sentence as should the sentence \u201cIf you\u2019re going to troll, do us all a favor and stick to the guidelines.\u201d. We hypothesize that these, and other, examples will be difficult for the tagger unless the context surrounding each sentence is considered and in the absence of a sentiment lexicon tuned for conversational text (Ding et al., 2008; Choi and Cardie, 2009).\nAs a result, we investigate isotonic Conditional Random Fields (isotonic CRF) (Mao and Lebanon, 2007) for the sentiment tagging task since they preserve the advantages of the popular CRF sequential tagging models (Lafferty et al., 2001) while providing an efficient mechanism to encode domain knowledge \u2014 in our case, a sentiment lexicon \u2014 through isotonic constraints on the model parameters. In particular, we bootstrap the construction of a sentiment lexicon from Wikipedia talk pages using the lexical items in existing general-purpose sentiment lexicons as seeds and in conjunction with an existing label propagation algorithm (Zhu and Ghahramani, 2002).1\nTo summarize, our chief contributions include: (1) We propose an agreement and disagreement identification model based on isotonic Conditional Random Fields (Mao and Lebanon, 2007) to identify users\u2019 attitude in online discussion. Our predictions that are made on the sentence-\n1Our online discussion lexicon (Section 4) will be made publicly available.\nor segment-level, are able to discover fine-grained sentiment flow within each turn, which can be further applied in other applications, such as dispute detection or argumentation structure analysis. We employ two existing online discussion data sets: the Authority and Alignment in Wikipedia Discussions (AAWD) corpus of Bender et al. (2011) (Wikipedia talk pages) and the Internet Argument Corpus (IAC) of Walker et al. (2012a). Experimental results show that our model significantly outperforms state-of-the-art methods on the AAWD data (our F1 scores are 0.74 and 0.67 for agreement and disagreement, vs. 0.58 and 0.56 for the linear chain CRF approach) and IAC data (our F1 scores are 0.61 and 0.78 for agreement and dis-\nagreement, vs. 0.28 and 0.73 for SVM). (2) Furthermore, we construct a new sentiment lexicon for online discussion. We show that the learned lexicon significantly improves performance over systems that use existing generalpurpose lexicons (i.e. MPQA lexicon (Wilson et al., 2005), General Inquirer (Stone et al., 1966), and SentiWordNet (Esuli and Sebastiani, 2006)). Our lexicon is constructed from a very large-scale discussion corpus based on Wikipedia talk page, where previous work (Somasundaran and Wiebe, 2010) for constructing online discussion lexicon relies on human annotations derived from limited number of conversations.\nIn the remainder of the paper, we describe first the related work (Section 2). Then we introduce the sentence-level agreement and disagreement identification model (Section 3) as well as the label propagation algorithm for lexicon construction (Section 4). After explain the experimental setup, we display the results and provide further analysis in Section 6."}, {"heading": "2 Related Work", "text": "Sentiment analysis has been utilized as a key enabling technique in a number of conversationbased applications. Previous work mainly studies the attitudes in spoken meetings (Galley et al., 2004; Hahn et al., 2006) or broadcast conversations (Wang et al., 2011) using Conditional Random Fields (CRF) (Lafferty et al., 2001). Galley et al. (2004) employ Conditional Markov models to detect if discussants reach at an agreement in spoken meetings. Each state in their model is an individual turn and prediction is made on the turnlevel. In the same spirit, Wang et al. (2011) also propose a sequential model based on CRF for detecting agreements and disagreements in broadcast conversations, where they primarily show the efficiency of prosodic features. While we also exploit a sequential model extended from CRFs, our predictions are made for each sentence or segment rather than at the turn-level. Moreover, we experiment with online discussion datasets that exhibit a more realistic distribution of disagreement vs. agreement, where much more disagreement is observed due to its function and the relation between the participants. This renders the detection problem more challenging.\nOnly recently, agreement and disagreement detection is studied for online discussion, especially\nfor online debate. Abbott et al. (2011) investigate different types of features based on dependency relations as well as manually-labeled features, such as if the participants are nice, nasty, or sarcastic, and respect or insult the target participants. Automatically inducing those features from human annotation are challenging itself, so it would be difficult to reproduce their work on new datasets. We use only automatically generated features. Using the same dataset, Misra and Walker (2013) study the effectiveness of topicindependent features, e.g. discourse cues indicating agreement or negative opinion. Those cues, which serve a similar purpose as a sentiment lexicon, are also constructed manually. In our work, we create an online discussion lexicon automatically and construct sentiment features based on the lexicon. Also targeting online debate, Yin et al. (2012) train a logistic regression classifier with features aggregating posts from the same participant to predict the sentiment for each individual post. This approach works only when the speaker has enough posts on each topic, which is not applicable to newcomers. Hassan et al. (2010) focus on predicting the attitude of participants towards each other. They relate the sentiment words to the second person pronoun, which produces strong baselines. We also adopt their baselines in our work. Although there are available datasets with (dis)agreement annotated on Wikipedia talk pages, we are not aware of any published work that utilizes these annotations. Dialogue act recognition on talk pages (Ferschke et al., 2012) might be the most related.\nWhile detecting agreement and disagreement in conversations is useful on its own, it is also a key component for related tasks, such as stance prediction (Thomas et al., 2006; Somasundaran and Wiebe, 2009; Walker et al., 2012b) and subgroup detection (Hassan et al., 2012; Abu-Jbara et al., 2012). For instance, Thomas et al. (2006) train an agreement detection classifier with Support Vector Machines on congressional floor-debate transcripts to determine whether the speeches represent support of or opposition to the proposed legislation. Somasundaran and Wiebe (2009) design various sentiment constraints for inclusion in an integer linear programming framework for stance classification. For subgroup detection, Abu-Jbara et al. (2012) uses the polarity of the expressions in the discussions and partition discussants into sub-\ngroups based on the intuition that people in the same group should mostly agree with each other. Though those work highly relies on the component of agreement and disagreement detection, the evaluation is always performed on the ultimate application only."}, {"heading": "3 The Model", "text": "We first give a brief overview on isotonic Conditional Random Fields (isotonic CRF) (Mao and Lebanon, 2007), which is used as the backbone approach for our sentence- or segment-level agreement and disagreement detection model. We defer the explanation of online discussion lexicon construction in Section 4."}, {"heading": "3.1 Problem Description", "text": "Consider a discussion comprised of sequential turns uttered by the participants; each turn consists of a sequence of text units, where each unit can be a sentence or a segment of several sentences. Our model takes as input the text units x = {x1, \u00b7 \u00b7 \u00b7 , xn} in the same turn, and outputs a sequence of sentiment labels y = {y1, \u00b7 \u00b7 \u00b7 , yn}, where yi \u2208 O,O = {NN,N,O,P,PP}. The labels in O represent strongly disagree (NN), disagree (N), neutral (O), agree (P), strongly agree (PP), respectively. In addition, elements in the partially ordered set O possess an ordinal relation \u2264. Here, we differentiate agreement and disagreement with different intensity, because the output of our classifier can be used for other applications, such as dispute detection, where \u201cstrongly disagree\u201d (e.g. NN) plays an important role. Meanwhile, fine-grained sentiment labels potentially provide richer context information for the sequential model employed for this task."}, {"heading": "3.2 Isotonic Conditional Random Fields", "text": "Conditional Random Fields (CRF) have been successfully applied in numerous sequential labeling tasks (Lafferty et al., 2001). Given a sequence of utterances or segments x = {x1, \u00b7 \u00b7 \u00b7 , xn}, according to linear-chain CRF, the probability of the labels y for x is given by:\np(y|x) = 1\nZ(x) exp(\n\u2211\ni\n\u2211\n\u03c3,\u03c4\n\u03bb\u3008\u03c3,\u03c4\u3009f\u3008\u03c3,\u03c4\u3009(yi\u22121, yi)\n+ \u2211\ni\n\u2211\n\u03c3,w\n\u00b5\u3008\u03c3,w\u3009g\u3008\u03c3,w\u3009(yi, xi))\n(1)\nf\u3008\u03c3,\u03c4\u3009(yi\u22121, yi) and g\u3008\u03c3,w\u3009(yi, xi) are feature functions. Given that yi\u22121, yi, xi take values of \u03c3, \u03c4, w, the functions are indexed by pairs \u3008\u03c3, \u03c4\u3009 and \u3008\u03c3,w\u3009. \u03bb\u3008\u03c3,\u03c4\u3009, \u00b5\u3008\u03c3,w\u3009 are the parameters.\nCRF, as defined above, is not appropriate for ordinal data like sentiment, because it ignores the ordinal relation among sentiment labels. Isotonic Conditional Random Fields (isotonic CRF) are proposed by Mao and Lebanon (2007) to enforce a set of monotonicity constraints on the parameters that are consistent with the ordinal structure and domain knowledge (in our case, a sentiment lexicon automatically constructed from online discussions).\nGiven a lexicon M = Mp \u222a Mn, where Mp and Mn are two sets of features (usually words) identified as strongly associated with positive sentiment and negative sentiment. The constraints are encoded as below. For each feature w \u2208 Mp, isotonic CRF enforces \u03c3 \u2264 \u03c3\u2032 \u21d2 \u00b5\u3008\u03c3,w\u3009 \u2264 \u00b5\u3008\u03c3\u2032,w\u3009. Intuitively, the parameters \u00b5\u3008\u03c3,w\u3009 are intimately tied to the model probabilities. When a feature such as \u201ctotally agree\u201d is observed in the training data, the feature parameter for \u00b5\u3008PP,totally agree\u3009 is likely to increase. Similar constraints are also defined on Mn. In this work, we boostrap the construction of an online discussion sentiment lexicon used as M in the isotonic CRF (see Section 4).\nThe parameters can be found by maximizing the likelihood subject to the monotonicity constraints. We adopt the re-parameterization from Mao and Lebanon (2007) for a simpler optimization problem, and refer the readers to Mao and Lebanon (2007) for more details.2"}, {"heading": "3.3 Features", "text": "The features used in sentiment prediction are listed in Table 1. Features with numerical values are first normalized by standardization, then binned into 5 categories.\nSyntactic/Semantic Features. Dependency relations have been shown to be effective for various sentiment prediction tasks (Joshi and PensteinRose\u0301, 2009; Somasundaran and Wiebe, 2009; Hassan et al., 2010; Abu-Jbara et al., 2012). We have two versions of dependency relation as features, one being the original form, another gen-\n2The full implementation is based on MALLET (McCallum, 2002). We thank Yi Mao for sharing the implementation of the core learning algorithm.\neralizing a word to its POS tag in turn. For instance, \u201cnsubj(wrong, you)\u201d is generlized as the \u201cnsubj(ADJ, you)\u201d and \u201cnsubj(wrong, PRP)\u201d. We use Stanford parser (de Marneffe et al., 2006) to obtain parse trees and dependency relations.\nDiscourse Features. Previous work (Hirschberg and Litman, 1993; Abbott et al., 2011) suggests that discourse markers, such as what?, actually, may have their use for expressing opinions. We extract the initial unigram, bigram, and trigram of each utterance as discourse features (Hirschberg and Litman, 1993). Hedge words are collected from the CoNLL-2012 shared task (Farkas et al., 2010).\nConversation Features. Conversation features encode some useful information regarding the similarity between the current utterance(s) and the sentences uttered by the target participant. TFIDF similarity is computed. We also check if the current utterance(s) quotes target sentences and compute its length.\nSentiment Features. We gather connectives from Penn Discourse TreeBank (Rashmi Prasad and Webber, 2008) and combine them with any sentiment word that precedes or follows it as new features. Sentiment dependency relations are the subset of dependency relations with sentiment words. We replace those words with their polarity equivalents. For example, relation \u201cnsubj(wrong, you)\u201d becomes \u201cnsubj(SentiWordneg, you)\u201d."}, {"heading": "4 Online Discussion Sentiment Lexicon Construction", "text": "So far as we know, there is no lexicon available for online discussions. Thus, we create from a large-scale corpus via label propagation. The label propagation algorithm, proposed by Zhu and Ghahramani (2002), is a semi-supervised learning method. In general, it takes as input a set of seed samples (e.g. sentiment words in our case), and the similarity between pairwise samples, then iteratively assigns values to the unlabeled samples (see Algorithm 1). The construction of graph G is discussed in Section 4.1. Sample sentiment words in the new lexicon are listed in Table 2.\nInput : G = (V,E), wij \u2208 [0, 1], positive seed words P , negative seed words N , number of iterations T Output: {yi} |V |\u22121 i=0\nyi = 1.0, \u2200vi \u2208 P yi = \u22121.0, \u2200vi \u2208 N yi = 0.0, \u2200vi /\u2208 P \u222aN\nfor t = 1 \u00b7 \u00b7 \u00b7 T do\nyi =\n\u2211 (vi,vj )\u2208E\nwij\u00d7yj \u2211\n(vi,vj)\u2208E wij\n, \u2200vi \u2208 V\nyi = 1.0, \u2200vi \u2208 P yi = \u22121.0, \u2200vi \u2208 N\nend\nAlgorithm 1: The label propagation algorithm (Zhu and Ghahramani, 2002) used for constructing online discussion lexicon."}, {"heading": "4.1 Graph Construction", "text": "Node Set V . Traditional lexicons, like General Inquirer (Stone et al., 1966), usually consist of polarized unigrams. As we mentioned in Section 1, unigrams lack the capability of capturing the sentiment conveyed in online discussions. Instead, bigrams, dependency relations, and even punctuation can serve as supplement to the unigrams. Therefore, we consider four types of text units as nodes in the graph: unigrams, bigrams, dependency relations, sentiment dependency relations. Sentiment dependency relations are described in Section 3.3. We replace all relation names with a general label. Text units that appear in at least 10 discussions are retained as nodes to reduce noise.\nEdge Set E. As Velikovich et al. (2010) and Feng et al. (2013) notice, a dense graph with a large number of nodes is susceptible to propagating noise, and will not scale well. We thus adopt the algorithm in Feng et al. (2013) to construct a sparsely connected graph. For each text unit t, we first compute its representation vector ~a using Pairwise Mutual Information scores with respect to the top 50 co-occuring text units. We define \u201cco-occur\u201d as text units appearing in the same sentence. An edge is created between two text units t0 and t1 only if they ever co-occur. The similarity between t0 and t1 is calculated as the Cosine similarity between ~a0 and ~a1.\nSeed Words. The seed sentiment are collected from three existing lexicons: MPQA lexicon, General Inquirer, and SentiWordNet. Each word in SentiWordNet is associated with a positive score and a negative score; words with a polarity score\nlarger than 0.7 are retained. We remove words with conflicting sentiments."}, {"heading": "4.2 Data", "text": "The graph is constructed based on Wikipedia talk pages. We download the 2013-03-04 Wikipedia data dump, which contains 4,412,582 talk pages. Since we are interested in conversational languages, we filter out talk pages with fewer than 5 participants. This results in a dataset of 20,884 talk pages, from which the graph is constructed."}, {"heading": "5 Experimental Setup", "text": ""}, {"heading": "5.1 Datasets", "text": "Wikipedia Talk pages. The first dataset we use is Authority and Alignment in Wikipedia Discussions (AAWD) corpus (Bender et al., 2011). AAWD consists of 221 English Wikipedia discussions with agreement and disagreement annotations.3\nThe annotation of AAWD is made at utteranceor turn-level, where a turn is defined as continuous body of text uttered by the same participant. Annotators either label each utterance as agreement, disagreement or neutral, and select the corresponding spans of text, or label the full turn. Each turn is annotated by two or three people. To induce an utterance-level label for instances that have only a turn-level label, we assume they have the same label as the turn.\nTo train our sentiment model, we further transform agreement and disagreement labels (i.e. 3- way) into the 5-way labels. For utterances that are annotated as agreement and have the text span specified by at least two annotators, they are treated as \u201cstrongly agree\u201d (PP). If an utterance is only selected as agreement by one annotator or it gets the label by turn-level annotation, it is \u201cagree\u201d (P). \u201cStrongly disagree\u201d (NN) and \u201cdisagree\u201d (N) are collected in the same way from disagreement label. All others are neutral (O). In total, we have 16,501 utterances. 1,930 and 1,102 utterances are labeled as \u201cNN\u201d and \u201cN\u201d. 532 and 99 of them are \u201cPP\u201d and \u201cP\u201d. All other 12,648 are neutral samples. 4\n3Bender et al. (2011) originally use positive alignment and negative alignment to indicate two types of social moves. They define those alignment moves as \u201cagreeing or disagreeing\u201d with the target. We thus use agreement and disagreement instead of positive and negative alignment in this work.\n4345 samples with both positive and negative labels are treated as neutral.\nOnline Debate. The second dataset is the Internet Argument Corpus (IAC) (Walker et al., 2012a) collected from an online debate forum. Each discussion in IAC consists of multiple posts, where we treat each post as a turn. Most posts (72.3%) contain quoted content from the posts they target at or other resources. A post can have more than one quote, which naturally break the post into multiple segments. 1,806 discussions are annotated with agreement and disagreement on the segmentlevel from -5 to 5, with -5 as strongly disagree and 5 as strongly agree. We first compute the average score for each segment among different annotators and transform the score into sentiment label in the following way. We treat [\u22125,\u22123] as NN (1595 segments), (\u22123,\u22121] as N (4548 segments), [1, 3) as P (911 samples), [3, 5] as PP (199), all others as O (290 segments).\nIn the test phase, utterances or segments predicted with NN or N are treated as disagreement; the ones predicted as PP or P are agreement; O is neutral."}, {"heading": "5.2 Comparison", "text": "We compare with two baselines. (1) Baseline (Polarity) is based on counting the sentiment words from our lexicon. An utterance or segment is predicted as agreement if it contains more positive words than negative words, or disagreement if more negative words are observed. Otherwise, it is neutral. (2) Baseline (Distance) is extended from (Hassan et al., 2010). Each sentiment word is associated with the closest second person pronoun, and a surface distance can be computed between them. A classifier based on Support Vector Machines (Joachims, 1999) (SVM) is trained with the features of sentiment words, minimum/maximum/average of the distances.\nWe also compare with two state-of-the-art methods that are widely used in sentiment prediction for conversations. The first one is an RBF kernel SVM based approach, which has been used for sentiment prediction (Hassan et al., 2010), and (dis)agreement detection (Yin et al., 2012) in online debates. The second is linear chain CRF, which has been utilized for (dis)agreement identification in broadcast conversations (Wang et al., 2011)."}, {"heading": "6 Results", "text": "In this section, we first show the experimental results on sentence- and segment-level agreement and disagreement detection in two types of online discussions \u2013 Wikipedia Talk pages and online debates. Then we provide more detailed analysis for the features used in our model. Furthermore, we discuss several types of errors made in the model."}, {"heading": "6.1 Wikipedia Talk Pages", "text": "We evaluate the systems by standard F1 score on each of the three categories: agreement, disagreement, and neutral. For AAWD, we compute two versions of F1 scores. Strict F1 is computed against the true labels. For soft F1, if a sentence is never labeled by any annotator on the sentencelevel and adopts its agreement/disagreement label from the turn-level annotation, then it is treated as a true positive when predicted as neutral.\nTable 3 demonstrates our main results on the Wikipedia Talk pages (AAWD dataset). Without downsampling, our isotonic CRF based systems with the new lexicon significantly outperform the compared approaches for agreement and disagreement detection according to the pairedt test (p < 0.05). We also perform downsampling by removing the turns only containing neutral utterances. However, it does not always help with performance. We suspect that, with less neutral samples in the training data, the classifier is less likely to make neutral predictions, which thus decreases true positive predictions. For strict Fscores on agreement/disagreement, downsampling\nhas mixed effect, but mostly we get slightly better performance."}, {"heading": "6.2 Online Debates", "text": "Similarly, F1 scores for agreement, disagreement and neutral for online debates (IAC dataset) are displayed in Table 4. Both of our systems based on isotonic CRF achieve significantly better F1 scores than the comparison. Especially, our system with the new lexicon produces the best results. For SVM and linear-chain CRF based systems, we also add new sentiment features constructed from the new lexicon as described in Section 3.3. We\ncan see that those sentiment features also boost the performance for both of the compared approaches."}, {"heading": "6.3 Feature Evaluation", "text": "Moreover, we evaluate the effectiveness of features by adding one type of features each time. The results are listed in Table 5. As it can be seen, the performance gets improved incrementally with every new set of features.\nWe also utilize \u03c72-test to highlight some of the salient features on the two datasets. We can see from Table 6 that, for online debates (IAC), some features are highly topic related, such as \u201cthe male\u201d or \u201cthe scientist\u201d. This observation concurs with the conclusion in Misra and Walker (2013) that features with topic information are indicative for agreement and disagreement detection."}, {"heading": "6.4 Error Analysis", "text": "After a closer look at the data, we found two major types of errors. Firstly, people express disagreement not only by using opinionated words, but also by providing contradictory example. This needs a deeper understanding of the semantic information embedded in the text. Techniques like textual entailment can be used in the further work. Secondly, a sequence of sentences with sarcasm is hard to detect. For instance, \u201cBravo, my friends! Bravo! Goebbles would be proud of your abilities to whitewash information.\u201d We observe terms like \u201cBravo\u201d, \u201cfriends\u201d, and \u201cbe proud of\u201d that are indicators for positive sentiment; however, they are\nin sarcastic tone. We believe a model that is able to detect sarcasm would further improve the performance."}, {"heading": "7 Conclusion", "text": "We present an agreement and disagreement detection model based on isotonic CRFs that outputs labels at the sentence- or segment-level. We bootstrap the construction of a sentiment lexicon for online discussions, encoding it in the form of domain knowledge for the isotonic CRF learner. Our sentiment-tagging model is shown to outperform the state-of-the-art approaches on both Wikipedia Talk pages and online debates.\nAcknowledgments We heartily thank the Cornell NLP Group and the reviewers for helpful comments. This work was supported in part by NSF grants IIS-0968450 and IIS-1314778, and DARPA DEFT Grant FA8750-13-2-0015. The views and conclusions contained herein are those of the authors and should not be interpreted as necessarily representing the official policies or endorsements, either expressed or implied, of NSF, DARPA or the U.S. Government."}], "references": [{"title": "How can you say such things?!?: Recognizing disagreement in informal political argument", "author": ["Rob Abbott", "Marilyn Walker", "Pranav Anand", "Jean E. Fox Tree", "Robeson Bowmani", "Joseph King."], "venue": "Proceedings of the Workshop on Languages in So-", "citeRegEx": "Abbott et al\\.,? 2011", "shortCiteRegEx": "Abbott et al\\.", "year": 2011}, {"title": "Subgroup detection in ideological discussions", "author": ["Amjad Abu-Jbara", "Mona Diab", "Pradeep Dasigi", "Dragomir Radev."], "venue": "Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics: Long Papers - Volume 1, ACL \u201912,", "citeRegEx": "Abu.Jbara et al\\.,? 2012", "shortCiteRegEx": "Abu.Jbara et al\\.", "year": 2012}, {"title": "Annotating social acts: Authority claims and alignment moves in wikipedia talk pages", "author": ["Emily M. Bender", "Jonathan T. Morgan", "Meghan Oxley", "Mark Zachry", "Brian Hutchinson", "Alex Marin", "Bin Zhang", "Mari Ostendorf."], "venue": "Proceedings of", "citeRegEx": "Bender et al\\.,? 2011", "shortCiteRegEx": "Bender et al\\.", "year": 2011}, {"title": "Adapting a polarity lexicon using integer linear programming for domain-specific sentiment classification", "author": ["Yejin Choi", "Claire Cardie."], "venue": "Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing: Volume 2 -", "citeRegEx": "Choi and Cardie.,? 2009", "shortCiteRegEx": "Choi and Cardie.", "year": 2009}, {"title": "Generating typed dependency parses from phrase structure trees", "author": ["Marie-Catherine de Marneffe", "Bill MacCartney", "Christopher D. Manning."], "venue": "LREC.", "citeRegEx": "Marneffe et al\\.,? 2006", "shortCiteRegEx": "Marneffe et al\\.", "year": 2006}, {"title": "A holistic lexicon-based approach to opinion mining", "author": ["Xiaowen Ding", "Bing Liu", "Philip S. Yu."], "venue": "Proceedings of the 2008 International Conference on Web Search and Data Mining, WSDM \u201908, pages 231\u2013240, New York, NY, USA. ACM.", "citeRegEx": "Ding et al\\.,? 2008", "shortCiteRegEx": "Ding et al\\.", "year": 2008}, {"title": "Sentiwordnet: A publicly available lexical resource for opinion mining", "author": ["Andrea Esuli", "Fabrizio Sebastiani."], "venue": "In Proceedings of the 5th Conference on Language Resources and Evaluation (LREC06, pages 417\u2013422.", "citeRegEx": "Esuli and Sebastiani.,? 2006", "shortCiteRegEx": "Esuli and Sebastiani.", "year": 2006}, {"title": "The conll-2010 shared task: Learning to detect hedges and their scope in natural language text", "author": ["Rich\u00e1rd Farkas", "Veronika Vincze", "Gy\u00f6rgy M\u00f3ra", "J\u00e1nos Csirik", "Gy\u00f6rgy Szarvas."], "venue": "Proceedings of the Fourteenth Conference on Computational Natu-", "citeRegEx": "Farkas et al\\.,? 2010", "shortCiteRegEx": "Farkas et al\\.", "year": 2010}, {"title": "Connotation lexicon: A dash of sentiment beneath the surface meaning", "author": ["Song Feng", "Jun Seok Kang", "Polina Kuznetsova", "Yejin Choi."], "venue": "ACL, pages 1774\u20131784. The Association for Computer Linguistics.", "citeRegEx": "Feng et al\\.,? 2013", "shortCiteRegEx": "Feng et al\\.", "year": 2013}, {"title": "Behind the article: Recognizing dialog acts in wikipedia talk pages", "author": ["Oliver Ferschke", "Iryna Gurevych", "Yevgen Chebotar."], "venue": "Proceedings of the 13th Conference of the European Chapter of the Association for Computational Linguistics, EACL \u201912,", "citeRegEx": "Ferschke et al\\.,? 2012", "shortCiteRegEx": "Ferschke et al\\.", "year": 2012}, {"title": "Identifying agreement and disagreement in conversational speech: use of Bayesian networks to model pragmatic dependencies", "author": ["Michel Galley", "Kathleen McKeown", "Julia Hirschberg", "Elizabeth Shriberg."], "venue": "ACL \u201904: Proceedings of the 42nd", "citeRegEx": "Galley et al\\.,? 2004", "shortCiteRegEx": "Galley et al\\.", "year": 2004}, {"title": "Agreement/disagreement classification: Exploiting unlabeled data using contrast classifiers", "author": ["Sangyun Hahn", "Richard Ladner", "Mari Ostendorf."], "venue": "Proceedings of the Human Language Technology Conference of the NAACL, Companion Volume:", "citeRegEx": "Hahn et al\\.,? 2006", "shortCiteRegEx": "Hahn et al\\.", "year": 2006}, {"title": "What\u2019s with the attitude?: Identifying sentences with attitude in online discussions", "author": ["Ahmed Hassan", "Vahed Qazvinian", "Dragomir Radev."], "venue": "Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, EMNLP", "citeRegEx": "Hassan et al\\.,? 2010", "shortCiteRegEx": "Hassan et al\\.", "year": 2010}, {"title": "Detecting subgroups in online discussions by modeling positive and negative relations among participants", "author": ["Ahmed Hassan", "Amjad Abu-Jbara", "Dragomir Radev."], "venue": "Proceedings of the 2012 Joint Conference on Empirical Methods in", "citeRegEx": "Hassan et al\\.,? 2012", "shortCiteRegEx": "Hassan et al\\.", "year": 2012}, {"title": "Empirical studies on the disambiguation of cue phrases", "author": ["Julia Hirschberg", "Diane Litman."], "venue": "Comput. Linguist., 19(3):501\u2013530, September.", "citeRegEx": "Hirschberg and Litman.,? 1993", "shortCiteRegEx": "Hirschberg and Litman.", "year": 1993}, {"title": "Advances in kernel methods", "author": ["Thorsten Joachims."], "venue": "chapter Making Large-scale Support Vector Machine Learning Practical, pages 169\u2013184. MIT Press, Cambridge, MA, USA.", "citeRegEx": "Joachims.,? 1999", "shortCiteRegEx": "Joachims.", "year": 1999}, {"title": "Generalizing dependency features for opinion mining", "author": ["Mahesh Joshi", "Carolyn Penstein-Ros\u00e9."], "venue": "Proceedings of the ACL-IJCNLP 2009 Conference Short Papers, ACLShort \u201909, pages 313\u2013316, Stroudsburg, PA, USA. Association for Computa-", "citeRegEx": "Joshi and Penstein.Ros\u00e9.,? 2009", "shortCiteRegEx": "Joshi and Penstein.Ros\u00e9.", "year": 2009}, {"title": "Conditional random fields: Probabilistic models for segmenting and labeling sequence data", "author": ["John D. Lafferty", "Andrew McCallum", "Fernando C.N. Pereira."], "venue": "Proceedings of the Eighteenth International Conference on Machine Learning, ICML", "citeRegEx": "Lafferty et al\\.,? 2001", "shortCiteRegEx": "Lafferty et al\\.", "year": 2001}, {"title": "Isotonic conditional random fields and local sentiment flow", "author": ["Yi Mao", "Guy Lebanon."], "venue": "Advances in Neural Information Processing Systems.", "citeRegEx": "Mao and Lebanon.,? 2007", "shortCiteRegEx": "Mao and Lebanon.", "year": 2007}, {"title": "Mallet: A machine learning for language toolkit", "author": ["Andrew Kachites McCallum."], "venue": "http://mallet.cs.umass.edu.", "citeRegEx": "McCallum.,? 2002", "shortCiteRegEx": "McCallum.", "year": 2002}, {"title": "The penn discourse treebank", "author": ["nie Webber"], "venue": null, "citeRegEx": "Webber.,? \\Q2008\\E", "shortCiteRegEx": "Webber.", "year": 2008}, {"title": "The viability", "author": ["Hannan", "Ryan McDonald"], "venue": null, "citeRegEx": "Hannan and McDonald.,? \\Q2010\\E", "shortCiteRegEx": "Hannan and McDonald.", "year": 2010}, {"title": "A corpus for research on deliberation and debate", "author": ["Marilyn Walker", "Jean Fox Tree", "Pranav Anand", "Rob Abbott", "Joseph King."], "venue": "Proceedings of the Eight International Conference on Language Resources and Evaluation (LREC\u201912), Istan-", "citeRegEx": "Walker et al\\.,? 2012a", "shortCiteRegEx": "Walker et al\\.", "year": 2012}, {"title": "Stance classification using dialogic properties of persuasion", "author": ["Marilyn A. Walker", "Pranav Anand", "Rob Abbott", "Ricky Grant."], "venue": "HLT-NAACL, pages 592\u2013596. The Association for Computational Linguistics.", "citeRegEx": "Walker et al\\.,? 2012b", "shortCiteRegEx": "Walker et al\\.", "year": 2012}, {"title": "Detection of agreement and disagreement in broadcast conversations", "author": ["Wen Wang", "Sibel Yaman", "Kristin Precoda", "Colleen Richey", "Geoffrey Raymond."], "venue": "Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Hu-", "citeRegEx": "Wang et al\\.,? 2011", "shortCiteRegEx": "Wang et al\\.", "year": 2011}, {"title": "Recognizing contextual polarity in phraselevel sentiment analysis", "author": ["Theresa Wilson", "Janyce Wiebe", "Paul Hoffmann."], "venue": "Proceedings of the Conference on Human Language Technology and Empirical Methods in Natural Language Processing,", "citeRegEx": "Wilson et al\\.,? 2005", "shortCiteRegEx": "Wilson et al\\.", "year": 2005}, {"title": "Unifying local and global agreement and disagreement classification in online debates", "author": ["Jie Yin", "Paul Thomas", "Nalin Narang", "Cecile Paris."], "venue": "Proceedings of the 3rd Workshop in Computational Approaches to Subjectivity and Sentiment Analysis,", "citeRegEx": "Yin et al\\.,? 2012", "shortCiteRegEx": "Yin et al\\.", "year": 2012}, {"title": "Learning from labeled and unlabeled data with label propagation", "author": ["Xiaojin Zhu", "Zoubin Ghahramani."], "venue": "Technical Report CMU-CALD-02-107.", "citeRegEx": "Zhu and Ghahramani.,? 2002", "shortCiteRegEx": "Zhu and Ghahramani.", "year": 2002}], "referenceMentions": [{"referenceID": 13, "context": ", 2013), and thus further discover subgroups (Hassan et al., 2012; Abu-Jbara et al., 2012) with similar ideological viewpoint.", "startOffset": 45, "endOffset": 90}, {"referenceID": 1, "context": ", 2013), and thus further discover subgroups (Hassan et al., 2012; Abu-Jbara et al., 2012) with similar ideological viewpoint.", "startOffset": 45, "endOffset": 90}, {"referenceID": 5, "context": "We hypothesize that these, and other, examples will be difficult for the tagger unless the context surrounding each sentence is considered and in the absence of a sentiment lexicon tuned for conversational text (Ding et al., 2008; Choi and Cardie, 2009).", "startOffset": 211, "endOffset": 253}, {"referenceID": 3, "context": "We hypothesize that these, and other, examples will be difficult for the tagger unless the context surrounding each sentence is considered and in the absence of a sentiment lexicon tuned for conversational text (Ding et al., 2008; Choi and Cardie, 2009).", "startOffset": 211, "endOffset": 253}, {"referenceID": 18, "context": "As a result, we investigate isotonic Conditional Random Fields (isotonic CRF) (Mao and Lebanon, 2007) for the sentiment tagging task since they preserve the advantages of the popular CRF sequential tagging models (Lafferty et al.", "startOffset": 78, "endOffset": 101}, {"referenceID": 17, "context": "As a result, we investigate isotonic Conditional Random Fields (isotonic CRF) (Mao and Lebanon, 2007) for the sentiment tagging task since they preserve the advantages of the popular CRF sequential tagging models (Lafferty et al., 2001) while providing an efficient mechanism to encode domain knowledge \u2014 in our case, a sentiment lexicon \u2014 through isotonic constraints on the model parameters.", "startOffset": 213, "endOffset": 236}, {"referenceID": 27, "context": "isting general-purpose sentiment lexicons as seeds and in conjunction with an existing label propagation algorithm (Zhu and Ghahramani, 2002).", "startOffset": 115, "endOffset": 141}, {"referenceID": 18, "context": "(1) We propose an agreement and disagreement identification model based on isotonic Conditional Random Fields (Mao and Lebanon, 2007) to identify users\u2019 attitude in online discussion.", "startOffset": 110, "endOffset": 133}, {"referenceID": 2, "context": "cussions (AAWD) corpus of Bender et al. (2011) (Wikipedia talk pages) and the Internet Argu-", "startOffset": 26, "endOffset": 47}, {"referenceID": 22, "context": "ment Corpus (IAC) of Walker et al. (2012a). Experimental results show that our model signifi-", "startOffset": 21, "endOffset": 43}, {"referenceID": 25, "context": "MPQA lexicon (Wilson et al., 2005), General Inquirer (Stone et al.", "startOffset": 13, "endOffset": 34}, {"referenceID": 6, "context": ", 1966), and SentiWordNet (Esuli and Sebastiani, 2006)).", "startOffset": 26, "endOffset": 54}, {"referenceID": 10, "context": "Previous work mainly studies the attitudes in spoken meetings (Galley et al., 2004; Hahn et al., 2006) or broadcast conversations (Wang et al.", "startOffset": 62, "endOffset": 102}, {"referenceID": 11, "context": "Previous work mainly studies the attitudes in spoken meetings (Galley et al., 2004; Hahn et al., 2006) or broadcast conversations (Wang et al.", "startOffset": 62, "endOffset": 102}, {"referenceID": 24, "context": ", 2006) or broadcast conversations (Wang et al., 2011) using Conditional Random Fields (CRF) (Lafferty et al.", "startOffset": 35, "endOffset": 54}, {"referenceID": 17, "context": ", 2011) using Conditional Random Fields (CRF) (Lafferty et al., 2001).", "startOffset": 46, "endOffset": 69}, {"referenceID": 10, "context": "Previous work mainly studies the attitudes in spoken meetings (Galley et al., 2004; Hahn et al., 2006) or broadcast conversations (Wang et al., 2011) using Conditional Random Fields (CRF) (Lafferty et al., 2001). Galley et al. (2004) employ Conditional Markov models to detect if discussants reach at an agreement in spoken meetings.", "startOffset": 63, "endOffset": 234}, {"referenceID": 24, "context": "In the same spirit, Wang et al. (2011) also propose a sequential model based on CRF for de-", "startOffset": 20, "endOffset": 39}, {"referenceID": 9, "context": "Dialogue act recognition on talk pages (Ferschke et al., 2012) might be the most related.", "startOffset": 39, "endOffset": 62}, {"referenceID": 0, "context": "Abbott et al. (2011) investigate different types of features based on dependency relations as well as manually-labeled features, such as if the participants are nice, nasty, or sarcastic, and respect or insult the target participants.", "startOffset": 0, "endOffset": 21}, {"referenceID": 0, "context": "Abbott et al. (2011) investigate different types of features based on dependency relations as well as manually-labeled features, such as if the participants are nice, nasty, or sarcastic, and respect or insult the target participants. Automatically inducing those features from human annotation are challenging itself, so it would be difficult to reproduce their work on new datasets. We use only automatically generated features. Using the same dataset, Misra and Walker (2013) study the effectiveness of topicindependent features, e.", "startOffset": 0, "endOffset": 479}, {"referenceID": 0, "context": "Abbott et al. (2011) investigate different types of features based on dependency relations as well as manually-labeled features, such as if the participants are nice, nasty, or sarcastic, and respect or insult the target participants. Automatically inducing those features from human annotation are challenging itself, so it would be difficult to reproduce their work on new datasets. We use only automatically generated features. Using the same dataset, Misra and Walker (2013) study the effectiveness of topicindependent features, e.g. discourse cues indicating agreement or negative opinion. Those cues, which serve a similar purpose as a sentiment lexicon, are also constructed manually. In our work, we create an online discussion lexicon automatically and construct sentiment features based on the lexicon. Also targeting online debate, Yin et al. (2012) train a logistic regression classifier with features aggregating posts from the same participant to predict the sentiment for each individual post.", "startOffset": 0, "endOffset": 861}, {"referenceID": 0, "context": "Abbott et al. (2011) investigate different types of features based on dependency relations as well as manually-labeled features, such as if the participants are nice, nasty, or sarcastic, and respect or insult the target participants. Automatically inducing those features from human annotation are challenging itself, so it would be difficult to reproduce their work on new datasets. We use only automatically generated features. Using the same dataset, Misra and Walker (2013) study the effectiveness of topicindependent features, e.g. discourse cues indicating agreement or negative opinion. Those cues, which serve a similar purpose as a sentiment lexicon, are also constructed manually. In our work, we create an online discussion lexicon automatically and construct sentiment features based on the lexicon. Also targeting online debate, Yin et al. (2012) train a logistic regression classifier with features aggregating posts from the same participant to predict the sentiment for each individual post. This approach works only when the speaker has enough posts on each topic, which is not applicable to newcomers. Hassan et al. (2010) focus on predicting the attitude of participants towards each other.", "startOffset": 0, "endOffset": 1142}, {"referenceID": 23, "context": "conversations is useful on its own, it is also a key component for related tasks, such as stance prediction (Thomas et al., 2006; Somasundaran and Wiebe, 2009; Walker et al., 2012b) and subgroup detection (Hassan et al.", "startOffset": 108, "endOffset": 181}, {"referenceID": 18, "context": "We first give a brief overview on isotonic Conditional Random Fields (isotonic CRF) (Mao and Lebanon, 2007), which is used as the backbone approach for our sentence- or segment-level agreement and disagreement detection model.", "startOffset": 84, "endOffset": 107}, {"referenceID": 17, "context": "Conditional Random Fields (CRF) have been successfully applied in numerous sequential labeling tasks (Lafferty et al., 2001).", "startOffset": 101, "endOffset": 124}, {"referenceID": 18, "context": "Isotonic Conditional Random Fields (isotonic CRF) are proposed by Mao and Lebanon (2007) to enforce a set of monotonicity constraints on the parameters that are consistent with the ordinal structure and domain knowledge (in our case, a sentiment lexicon automatically constructed from online discussions).", "startOffset": 66, "endOffset": 89}, {"referenceID": 18, "context": "We adopt the re-parameterization from Mao and Lebanon (2007) for a simpler optimization prob-", "startOffset": 38, "endOffset": 61}, {"referenceID": 18, "context": "lem, and refer the readers to Mao and Lebanon (2007) for more details.", "startOffset": 30, "endOffset": 53}, {"referenceID": 12, "context": "Dependency relations have been shown to be effective for various sentiment prediction tasks (Joshi and PensteinRos\u00e9, 2009; Somasundaran and Wiebe, 2009; Hassan et al., 2010; Abu-Jbara et al., 2012).", "startOffset": 92, "endOffset": 197}, {"referenceID": 1, "context": "Dependency relations have been shown to be effective for various sentiment prediction tasks (Joshi and PensteinRos\u00e9, 2009; Somasundaran and Wiebe, 2009; Hassan et al., 2010; Abu-Jbara et al., 2012).", "startOffset": 92, "endOffset": 197}, {"referenceID": 19, "context": "The full implementation is based on MALLET (McCallum, 2002).", "startOffset": 43, "endOffset": 59}, {"referenceID": 7, "context": "Lexical Features - unigram/bigram - num of words all uppercased - num of words Discourse Features - initial uni-/bi-/trigram - repeated punctuations - hedging (Farkas et al., 2010) - number of negators Syntactic/Semantic Features - unigram with POS tag - dependency relation Conversation Features - quote overlap with target - TFIDF similarity with target (remove quote first) Sentiment Features - connective + sentiment words - sentiment dependency relation - sentiment words", "startOffset": 159, "endOffset": 180}, {"referenceID": 14, "context": "Previous work (Hirschberg and Litman, 1993; Abbott et al., 2011) suggests that discourse markers, such as what?, actually, may have their use for expressing opinions.", "startOffset": 14, "endOffset": 64}, {"referenceID": 0, "context": "Previous work (Hirschberg and Litman, 1993; Abbott et al., 2011) suggests that discourse markers, such as what?, actually, may have their use for expressing opinions.", "startOffset": 14, "endOffset": 64}, {"referenceID": 14, "context": "We extract the initial unigram, bigram, and trigram of each utterance as discourse features (Hirschberg and Litman, 1993).", "startOffset": 92, "endOffset": 121}, {"referenceID": 7, "context": "Hedge words are collected from the CoNLL-2012 shared task (Farkas et al., 2010).", "startOffset": 58, "endOffset": 79}, {"referenceID": 27, "context": "The label propagation algorithm, proposed by Zhu and Ghahramani (2002), is a semi-supervised learning method.", "startOffset": 45, "endOffset": 71}, {"referenceID": 27, "context": "Algorithm 1: The label propagation algorithm (Zhu and Ghahramani, 2002) used for constructing online discussion lexicon.", "startOffset": 45, "endOffset": 71}, {"referenceID": 8, "context": "(2010) and Feng et al. (2013) notice, a dense graph with a large number of nodes is susceptible to propagating noise, and will not scale well.", "startOffset": 11, "endOffset": 30}, {"referenceID": 8, "context": "(2010) and Feng et al. (2013) notice, a dense graph with a large number of nodes is susceptible to propagating noise, and will not scale well. We thus adopt the algorithm in Feng et al. (2013) to construct a sparsely connected graph.", "startOffset": 11, "endOffset": 193}, {"referenceID": 2, "context": "The first dataset we use is Authority and Alignment in Wikipedia Discussions (AAWD) corpus (Bender et al., 2011).", "startOffset": 91, "endOffset": 112}, {"referenceID": 22, "context": "The second dataset is the Internet Argument Corpus (IAC) (Walker et al., 2012a) collected from an online debate forum.", "startOffset": 57, "endOffset": 79}, {"referenceID": 12, "context": "(2) Baseline (Distance) is extended from (Hassan et al., 2010).", "startOffset": 41, "endOffset": 62}, {"referenceID": 15, "context": "port Vector Machines (Joachims, 1999) (SVM) is trained with the features of sentiment words, minimum/maximum/average of the distances.", "startOffset": 21, "endOffset": 37}, {"referenceID": 12, "context": "The first one is an RBF kernel SVM based approach, which has been used for sentiment prediction (Hassan et al., 2010), and", "startOffset": 96, "endOffset": 117}, {"referenceID": 26, "context": "(dis)agreement detection (Yin et al., 2012) in online debates.", "startOffset": 25, "endOffset": 43}], "year": 2016, "abstractText": "We study the problem of agreement and disagreement detection in online discussions. An isotonic Conditional Random Fields (isotonic CRF) based sequential model is proposed to make predictions on sentenceor segment-level. We automatically construct a socially-tuned lexicon that is bootstrapped from existing general-purpose sentiment lexicons to further improve the performance. We evaluate our agreement and disagreement tagging model on two disparate online discussion corpora \u2013 Wikipedia Talk pages and online debates. Our model is shown to outperform the state-of-the-art approaches in both datasets. For example, the isotonic CRF model achieves F1 scores of 0.74 and 0.67 for agreement and disagreement detection, when a linear chain CRF obtains 0.58 and 0.56 for the discussions on Wikipedia Talk pages.", "creator": "dvips(k) 5.991 Copyright 2011 Radical Eye Software"}}}