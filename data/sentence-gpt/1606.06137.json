{"id": "1606.06137", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "20-Jun-2016", "title": "LSTM-Based Predictions for Proactive Information Retrieval", "abstract": "We describe a method for proactive information retrieval targeted at retrieving relevant information during a writing task. In our method, the current task and the needs of the user are estimated, and the potential next steps are unobtrusively predicted based on the user's past actions. We focus on the task of writing, in which the user is coalescing previously collected information into a text. Our proactive system automatically recommends the user relevant background information. The proposed system incorporates text input prediction using a long short-term memory (LSTM) network. We present simulations, which show that the system is able to reach higher precision values in an exploratory search setting compared to both a baseline and a comparison system. A method is described to facilitate a simple retrieval procedure. The goal is to make a system aware of the impact of these factors, thereby creating an effective tool to guide the use of these strategies.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n", "histories": [["v1", "Mon, 20 Jun 2016 14:26:33 GMT  (322kb,D)", "http://arxiv.org/abs/1606.06137v1", "Neu-IR '16 SIGIR Workshop on Neural Information Retrieval, July 21, 2016, Pisa, Italy"]], "COMMENTS": "Neu-IR '16 SIGIR Workshop on Neural Information Retrieval, July 21, 2016, Pisa, Italy", "reviews": [], "SUBJECTS": "cs.IR cs.CL cs.NE", "authors": ["petri luukkonen", "markus koskela", "patrik flor\\'een"], "accepted": false, "id": "1606.06137"}, "pdf": {"name": "1606.06137.pdf", "metadata": {"source": "CRF", "title": "LSTM-Based Predictions for Proactive Information Retrieval", "authors": ["Petri Luukkonen", "Markus Koskela", "Patrik Flor\u00e9en"], "emails": ["first.last@helsinki.fi"], "sections": [{"heading": null, "text": "CCS Concepts \u2022Information systems \u2192 Query intent; Users and interactive retrieval; \u2022Human-centered computing \u2192 Text input; \u2022Computing methodologies \u2192 Neural networks;\nKeywords Task-based Information Retrieval; Proactive Search; Long short-term memory networks; Recurrent neural networks; Text prediction"}, {"heading": "1. INTRODUCTION", "text": "Proactive systems [31] anticipate the needs of the user and predict possible next steps based on the user\u2019s preferences and current context. The central component of proactive systems is the inference engine, which analyzes the current context to provide the highest-ranking suggestions. Proactive systems have recently gained popularity, and many of the contemporary major operating systems include proactive components, e.g., Google Now, Apple Siri, and Microsoft Cortana.\nPermission to make digital or hard copies of part or all of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for third-party components of this work must be honored. For all other uses, contact the owner/author(s).\nNeu-IR \u201916 SIGIR Workshop on Neural Information Retrieval, July 21, 2016, Pisa, Italy c\u00a9 2016 Copyright held by the owner/author(s).\nThe rationale for using search engines is to find information that helps us in our daily tasks, be they leisure or professional. An ideal search engine would support searching and identifying useful information that can then be used in solving these tasks [33]. In proactive information retrieval [5] the estimation of the current task and context are utilized to proactively retrieve and recommend relevant items. In particular, this can include associative forms of recall, e.g., in a situation where the user does not realize having forgotten about a specific resource [27]. A proactive retrieval system can be viewed as a digital personal assistant that knows the user\u2019s preferences and aims to provide useful and relevant information in the current task context.\nLong short-term memory (LSTM) networks [17] have recently shown remarkable performance in a variety of natural language processing tasks, including speech recognition [16], automatic translation [30], image captioning [35] and information retrieval [18, 26, 28]. LSTM networks have also been used to generate various kinds of sequences, including text [15, 29]. In sequence generation, the network is used to process input sequences one at a time and to sample the next item from the output distribution of the network. The sampled item is then fed to the network as the next input. This capability to predict future continuations of a sequence (viz. text in this case) makes LSTMs particularly attractive for proactive information retrieval.\nA typical example of a task is writing about a given topic [24, 27, 33]. In this paper, we propose a method for supporting the user by proactive information retrieval during a writing task. An LSTM network is used to expand the proactive queries by predicting the most likely continuations of the current written text. An overview of the proposed method is shown in Figure 1. We present simulated experiments to validate the utility of the LSTM predictions in providing relevant documents, and compare the method to a baseline and to another method based on user intent modeling.\nThe rest of the paper is organized as follows. In the follow-\nar X\niv :1\n60 6.\n06 13\n7v 1\n[ cs\n.I R\n] 2\n0 Ju\nn 20\n16\ning section a discussion of related work is provided. In Section 3 we describe the components of our proactive retrieval system: the query expansion methods (based on an LSTM and on user intent modeling), and the proactive query generation. In Section 4 we describe our simulation experiments. In Section 5 we present our experimental user interface for providing proactive recommendations based on the current task context. The user interface is used in forthcoming user studies. We conclude and discuss future work in Section 6."}, {"heading": "2. RELATED WORK", "text": "Recurrent neural networks (RNNs), and LSTMs in particular, have recently been used to generate sequences in various domains, such as music [7], text [15, 29], and handwriting [15]. In information retrieval, RNNs have been used, e.g., for extracting sentence-level semantic vectors [26] and context-aware query suggestion [28]. Other kinds of deep neural networks have been used to project queries and documents to low-dimensional semantic spaces [18] and to learn fixed-length vectors for variable-length pieces of texts, such as sentences, paragraphs, and documents [21].\nVarious types of task activities have been studied in the literature as a basis for query suggestion or query support. Motivated by the observation that a notable proportion of the user\u2019s information needs were triggered by previous web browsing activity, several authors have studied the correlation between web browsing behavior and consecutive searches [10, 20, 22]. Another popular approach is to use previous search queries [9, 36]. Our work is related to query auto-completion [4], in which possible completions of search engine queries are predicted. Query auto-completion for web and mobile queries has recently been studied in [25, 34].\nA much studied task scenario for intelligent assistants is writing. The existing work has largely focused on bibliographic tasks involved in writing scientific or professional texts [3, 23, 32]. These mostly target either the planning or review phases of writing, as translation, i.e., transforming the writer\u2019s mental ideas into sentences, is the most challenging process in terms of tolerance to undesired disruptions. In [24], the impact of proactive recommendations to different phases of the writing process is analyzed. The Reactive Keyboard [11] is an early prototype for predicting the succeeding words from a user-written text fragment. Completing written sentences has also been studied in [6, 14].\nThe setup of our work is closely related to what was envisioned already by Rhodes and Starner in their Remembrance Agent (RA) [27]. The user first indexes her personal data, e.g., emails and written notes. RA is then set to run continuously in the background and display a list of summaries of documents that are related to the current document being read or written. Other similar tools include Watson [8] and Implicit Query [12]. The main difference of our method compared to these systems is the of use an LSTM-based predictive model to perform query expansion. We also compare our method to a baseline and a method based on user intent modeling. In this paper, we evaluate our method using a simulated writing task, but our method can be applied to other sources of context, such as what is read on the screen."}, {"heading": "3. METHOD FOR PROACTIVE IR", "text": "The proactive recommendations produced by our method are based on user input observed during the current task. To\nimprove the recommendations, we propose to use an LSTMbased method for query expansion described in Section 3.1. For comparison, we use a method based on estimating user intent using a multi-armed bandit model (Section 3.2 and Appendix A). Proactive information retrieval using the expanded query is described in Section 3.3. An overview of the proposed method (and the comparison and baseline methods) is shown in Figure 1."}, {"heading": "3.1 LSTM Text Prediction", "text": "In LSTM-based query expansion, the most probable continuations are estimated for the current written text and the expansion words are selected from these estimated continuations. For computing the continuations, we use a beam search algorithm [19] described below.\nThe LSTM network f is first trained using a text corpus, like abstracts of scientific articles. The trained network can then serve as a model for text generation.\nLet us assume the user has written n input words and denote by w0 the latest word in the input sequence. When given an input word w, the network f will return a probability distribution for the next word, of which we will consider the b most probable candidates. For example, with the\nbranching coefficient b = 4 and w0 = \u201cneural\u201d, the top candidates from f could be {w11 = \u201cmodel\u201d, w12 = \u201cnetwork\u201d, w13 = \u201csurgery\u201d, w14 = \u201csystem\u201d}, where wji denotes the ith candidate for the jth word following the input word w0. The output probability of the word is denoted as p(wji ). Further continuations are computed by using previous candidate words as input to f . Note that the output of f depends not only on the latest input word, but on all input words so far. The different continuations form a tree structure, where the root node is the input word w0 from which the continuations or paths evolve (see Figure 2). We denote by d the depth of the tree, i.e., the number of words in the estimated continuations of the sentence.\nAs the size of the tree grows rather quickly, the number of generated paths is controlled by the beam width k, pruning nodes on each level based on word probabilities on their corresponding paths. The path from w0 to wji is denoted by \u03c0(wji ), e.g., in Figure 2, \u03c0(w 3 1) = (w 0, w11, w 2 3, w 3 1). The pruning score R(wji ) is defined as the product of the word probabilities on \u03c0(wji ):\nR(wji ) = \u220f\nw\u2032\u2208\u03c0(wji )\np(w\u2032) . (1)\nOn each level of the tree, nodes other than the k highest scoring ones are pruned out. After the pruning on level j, the level j+1 candidates are obtained using the k remaining words. Figures 2 and 3 show examples of the pruned tree and the corresponding continuations.\nContinuing our previous example, the second set of estimated words following the input word w0 are computed using the estimated words from the level 1, e.g. when feeding to network f the word w14 = \u201csystem\u201d, the top candidates returned could be {w29 = \u201cand\u201d, w210 = \u201cthat\u201d, w211 = \u201cof\u201d, w212 = \u201cis\u201d}. The third level of generated words are computed using the words from the second level and so forth: w212 = \u201cis\u201d\u2192 {w39 = \u201calso\u201d, w310 = \u201cwithout\u201d, w311 = \u201cproposed\u201d, w312 = \u201cderived\u201d}.\nThe query expansion is formed from the words remaining in the pruned tree. First, the words are filtered using a standard list of English stop words. The query expansion score of wij is defined as the product of its idf value and its probability defined by the model f :\nscore(wji ) = idf(w j i ) \u00b7 p(w j i ) . (2)\nThe idf values were computed from the same training data that was used to train the network using the form idf(w) = N Nw\n, whereNw is the number of documents where the word w appears in, and N is the total number of documents. Finally, the nexp words having the highest scores are added to the expanded query."}, {"heading": "3.2 User Intent Model", "text": "As a comparison method, we use an upper confidence bound algorithm, which is based on estimating the user intent using a multi-armed bandit model [13]. The model balances exploration with exploitation and selects words that have the highest upper confidence bound [2]. This allows the user to interact with words that are relevant, but that are also uncertain to the model. Further details of the method are described in Appendix A."}, {"heading": "3.3 Proactive Query", "text": "Whenever resources are retrieved proactively, our system works as follows; for simplicity we refer to this as a proactive query, despite that there is no explicit query from the user. Based on the n input words, our query expansion modules retrieve nexp suggested words and add them to the query.\nThe actual information retrieval is performed using the Lucene search engine with the standard cosine-similarity ranking."}, {"heading": "4. SIMULATION EXPERIMENTS", "text": "In order to test whether our method provides relevant documents, we performed two kinds of simulation experiments. In the simulations, the input corresponding to text the user types comes from a given document. We perform query expansion with the LSTM-based text prediction method and with the user intent model. In the baseline experiments, we use only the written input as the queries."}, {"heading": "4.1 Data Set", "text": "The simulations were performed using the abstracts of the Computer Science branch of the arXiv1 preprint database, downloaded on October 28, 2015. The branch contains a total of 40 subcategories, which are used as topics in our experiments. A document in the arXiv database can belong to several subcategories, i.e., several topics in our case."}, {"heading": "4.2 Parameters", "text": "We used the medium LSTM architecture of Zaremba et al. [37] with an Tensorflow implementation. The network has two layers, 650 units per layer, and is unrolled for 35 words. All the abstracts in the data set were used to train the LSTM network. The training data consisted of 15M words with a vocabulary of 10k words. Training the network took about 36 hours using two GeForce GTX TITAN GPUs. Due to memory requirements, a random 10% of the abstracts was used to form the document term matrix for the user intent model.\nIn the LSTM-based word prediction, the beam width was set to k = 80 and the depth to d = 3. The branching coefficient was set to b = 10 and nexp = 10.\n1http://arxiv.org/"}, {"heading": "4.3 Exploratory Search Task", "text": "We simulate a setting where a user is writing a text about a given topic. We choose at random a document and simulate inputting sequences of n consecutive words from the text. The variable n serves as the size of the context expressed as the number of words from the written text. For instance, if n = 3, and the test text is \u201cMachine learning is a subfield\u201d, the input sequences for the proactive search engine would be\u201cMachine learning is\u201d, \u201clearning is a\u201d, and\u201cis a subfield\u201d. We envision a situation where the user has to find some relevant background resources about the given topic. The aim is thus to find other documents about the same topic t as the input document. For the proactive queries (Section 3.3), we use n input words and nexp = 10. The Lucene search engine was set to return 10 documents.\nWe use all the documents in the database belonging to the same topic as the test document as the target set Dt. As a measure of performance, we use the precision of relevant documents with regard to the topic t of the input document."}, {"heading": "4.4 Known-item Search Task", "text": "We also run simulations of known-item search, in which the purpose is to study a setting where the user needs to re-find a certain previously seen document. The setting is the same as for the exploratory search task, except that now we have only one target document in Dt. We take a random document as the query and perform a Lucene search over the rest of the data set to retrieve the highest-scoring document. This is now our target document. Note that the target document is thus a different document than our input document, and in the simulation we either find the target document or not."}, {"heading": "4.5 Results", "text": "Figure 4 shows the results of the simulations. On the lefthand side, the retrieval precision in the exploratory search task is shown. The right-hand figure shows the fractions of known items found in the known-item search task. First of all, as expected, in both tasks the results improve as the size of the context increases. This was especially anticipated\nfor known-item search, due to how the target document was selected.\nSecond, the results show that the query expansion methods can improve the precision of the proactively retrieved documents on the exploratory search task. The LSTMbased query expansion improves the results when the context is long enough, i.e., when n > 10. The intent model based query expansion, on the other hand, is suited for small context sizes (n < 10). For known-item search, the query expansion methods are not equally beneficial. The intent model again improves the results for small context sizes, but the LSTM-based predictions degrade the results.\nThe simulation results agree rather well with intuition of the query expansion methods. The user intent model based method expands the query with terms that have high tf-idf values in the same documents as the input words. It is conceivable that this is primarily useful when the input context is small, as the expansion can then bring useful additional information to the query. The LSTM-based query expansion, on the other hand, dynamically models the written context and can predict upcoming words. It is unsurprising that this works better when there is enough input context. For exploratory search, the predictions made by the LSTM network are accurate enough to increase the retrieval precision; in known-item search the target is smaller, i.e., a single document, and the predictions are not equally useful."}, {"heading": "5. USER INTERFACE", "text": "Our research calls for a user study in order to assess the usefulness of the proactive search results in real-world tasks. For this reason we have implemented an experimental user interface intended to be shown in a corner of the screen, displaying the proactive recommendations; see Figure 5. The interface is designed to allow the user to maintain her focus on the current task at hand, while offering peripherallyshown contextual recommendations. Any data source, e.g., the user\u2019s own emails, a database of documents, or any web pages, can be used as information to be proactively retrieved. In Figure 5, the resources displayed are arXiv preprints.\nFor obtaining the current writing context, we have implemented a specific text editor, which transmits the current word surrounding the text cursor at each keypress to the proactive search application; this way the input of n words gradually builds up. Similarly, the context could be deduced from, e.g., the text read in a web browser.\nBy clicking on any of the resources, its contents (i.e., the corresponding arXiv page in the setting of Figure 5) are shown in a regular web browser."}, {"heading": "6. DISCUSSION AND CONCLUSIONS", "text": "We have described a method for query expansion to enhance proactive information retrieval. The method is based on predicting the most likely continuations of the current input using an LSTM network.\nThe performed experiments provide evidence that our method is able to proactively produce relevant resources. The query expansion computed using an LSTM network improved retrieval precision in an exploratory search task, when enough context data is available. The results with the method used as comparison, based on user intent modeling using an upper confidence bound algorithm, were partially complementary, improving the results when only limited context is available. This naturally suggests a further study on combining the two query expansion methods. Further experiments with different kinds of datasets and tasks are in any case needed to validate the results.\nIn this work, we concentrated on the writing task. We introduced a simple user interface for showing the proactive search results based on the written context received from a dedicated text editor. The text predictions produced by the LSTM network could also be used to to automatically suggest different continuations for the currently written text, as in the Reactive Keyboard [11], [6], or [14]. Furthermore, user studies where the users are performing real-world tasks need to be carried out.\nFinally, in contrast to many of the existing methods for producing proactive recommendations, the proposed method generalizes the context gathering in the sense that\nthe context data can be extracted from several sources, such as a word processing software, PDF reader, or web browser. The only requirement for the context data is that it has to be in textual form."}, {"heading": "7. ACKNOWLEDGMENTS", "text": "This work has been partly supported by the Finnish Funding Agency for Innovation (project Re:Know) and the Academy of Finland (Finnish Centre of Excellence in Computational Inference Research COIN, 251170)."}, {"heading": "8. REFERENCES", "text": "[1] K. Athukorala, A. Medlar, K. Ilves, and D. Glowacka.\nBalancing exploration and exploitation: Empirical parameterization of exploratory search systems. In Proc. CIKM, pages 1703\u20131706, 2015.\n[2] P. Auer. Using confidence bounds for exploitation-exploration trade-offs. J. Mach. Learn. Res., 3:397\u2013422, Mar. 2003. [3] T. Babaian, B. J. Grosz, and S. M. Shieber. A writer\u2019s collaborative assistant. In Proc. IUI, pages 7\u201314, 2002. [4] H. Bast and I. Weber. Type less, find more: Fast autocompletion search with a succinct index. In Proc. SIGIR, pages 364\u2013371, 2006. [5] S. Bhatia, D. Majumdar, and N. Aggarwal. Proactive information retrieval: Anticipating users\u2019 information need. In Proc. ECIR, pages 874\u2013877. 2016. [6] S. Bickel, P. Haider, and T. Scheffer. Learning to complete sentences. In Proc. ECML, pages 497\u2013504. 2005. [7] N. Boulanger-Lewandowski, Y. Bengio, and P. Vincent. Modeling temporal dependencies in high-dimensional sequences: Application to polyphonic music generation and transcription. In Proc. ICML, pages 1159\u20131166, 2012. [8] J. Budzik, K. Hammond, and L. Birnbaum. Information access in context. Knowledge-Based Systems, 14(1\u20132):37\u201353, 2001. [9] H. Cao, D. H. Hu, D. Shen, D. Jiang, J.-T. Sun, E. Chen, and Q. Yang. Context-aware query classification. In Proc. SIGIR, pages 3\u201310, 2009. [10] Z. Cheng, B. Gao, and T.-Y. Liu. Actively predicting diverse search intent from user browsing behaviors. In Proc. WWW, pages 221\u2013230, 2010.\n[11] J. J. Darragh, I. H. Witten, and M. L. James. The Reactive Keyboard: A predictive typing aid. Computer, 23(11):41\u201349, 1990. [12] S. Dumais, E. Cutrell, R. Sarin, and E. Horvitz. Implicit queries (IQ) for contextualized search. In Proc. SIGIR, pages 594\u2013594, 2004. [13] D. Glowacka, T. Ruotsalo, K. Konuyshkova, S. Kaski, G. Jacucci, et al. Directing exploratory search: Reinforcement learning from user interactions with keywords. In Proc. IUI, pages 117\u2013128, 2013. [14] K. Grabski and T. Scheffer. Sentence completion. In Proc. SIGIR, pages 433\u2013439, 2004. [15] A. Graves. Generating sequences with recurrent neural networks. arXiv preprint arXiv:1308.0850, 2013. [16] A. Graves, A. r. Mohamed, and G. Hinton. Speech recognition with deep recurrent neural networks. In Proc ICASSP, pages 6645\u20136649, 2013. [17] S. Hochreiter and J. Schmidhuber. Long short-term memory. Neural Computation, 9(8):1735\u20131780, 1997. [18] P.-S. Huang, X. He, J. Gao, L. Deng, A. Acero, and L. Heck. Learning deep structured semantic models for web search using clickthrough data. In Proc. CIKM, pages 2333\u20132338, 2013. [19] P. Koehn. Statistical machine translation. Cambridge University Press, 2009. [20] W. Kong, R. Li, J. Luo, A. Zhang, Y. Chang, and J. Allan. Predicting search intent based on pre-search context. In Proc. SIGIR, pages 503\u2013512, 2015. [21] Q. V. Le and T. Mikolov. Distributed representations of sentences and documents. arXiv preprint arXiv:1405.4053, 2014. [22] D. J. Liebling, P. N. Bennett, and R. W. White. Anticipatory search: Using context to initiate search. In Proc. SIGIR, pages 1035\u20131036, 2012. [23] A. Livne, V. Gokuladas, J. Teevan, S. T. Dumais, and E. Adar. Citesight: Supporting contextual citation recommendation using differential search. In Proc. SIGIR, pages 807\u2013816, 2014. [24] M. C. P. Melguizo, L. Boves, and O. M. Ramos. A proactive recommendation system for writing: Helping without disrupting. International Journal of Industrial Ergonomics, 39(3):516\u2013523, 2009. [25] B. Mitra and N. Craswell. Query auto-completion for rare prefixes. In Proc. CIKM, pages 1755\u20131758, 2015. [26] H. Palangi, L. Deng, Y. Shen, J. Gao, X. He, J. Chen, X. Song, and R. Ward. Deep sentence embedding using long short-term memory networks. arXiv:1502.06922, 2015. [27] B. Rhodes and T. Starner. Remembrance Agent: A continuously running automated information retrieval system. In Proc. PAAM96, pages 487\u2013495, 1996. [28] A. Sordoni, Y. Bengio, H. Vahabi, C. Lioma, J. Grue Simonsen, and J.-Y. Nie. A hierarchical recurrent encoder-decoder for generative context-aware query suggestion. In Proc. CIKM, pages 553\u2013562, 2015. [29] I. Sutskever, J. Martens, and G. E. Hinton. Generating text with recurrent neural networks. In Proc. ICML, pages 1017\u20131024, 2011. [30] I. Sutskever, O. Vinyals, and Q. V. Le. Sequence to sequence learning with neural networks. arXiv preprint arXiv:1409.3215, 2014. [31] D. Tennenhouse. Proactive computing. Commun. ACM, 43(5):43\u201350, May 2000. [32] M. B. Twidale, A. A. Gruzd, and D. M. Nichols. Writing in the library: Exploring tighter integration of digital library use with the writing process. Information Processing & Management, 44(2):558\u2013580, 2008. [33] P. Vakkari. A theory of the task-based information retrieval process: a summary and generalization of a longitudinal study. Journal of Documentation, 57(1):44\u201360, 2001. [34] S. Vargas, R. Blanco, and P. Mika. Term-by-term query auto-completion for mobile search. In Proc. WSDM, pages\n143\u2013152, 2016. [35] O. Vinyals, A. Toshev, S. Bengio, and D. Erhan. Show and\ntell: A neural image caption generator. In Proc. CVPR, pages 3156\u20133164, 2015.\n[36] B. Xiang, D. Jiang, J. Pei, X. Sun, E. Chen, and H. Li. Context-aware ranking in web search. In Proc. SIGIR, pages 451\u2013458, 2010. [37] W. Zaremba, I. Sutskever, and O. Vinyals. Recurrent neural network regularization. arXiv preprint arXiv:1409.2329, 2014.\nAPPENDIX"}, {"heading": "A. USER INTENT MODEL", "text": "For computing the user intent model, we use a training database consisting of M documents, from which N unique words are extracted by excluding stop words. The jth document in the database is represented by a feature vector xj \u2208 RN where xij is the tf-idf value of the ith word. We denote by X \u2208 RN\u00d7M the tf-idf matrix of the M documents, where each column of X corresponds to one document feature vector and each row corresponds to a distribution of the words over the documents.\nThe user intent model is estimated using the context formed using n previously written words. Based on this input, a set of word weights are computed by using the LinRel algorithm proposed in [2].\nWe denote the relevance vector of observed words by y \u2208 [0, 1]N , where yi = 1 corresponds to having observed the ith word in the input. If the ith word does not occur in the subsequent input words, its relevance value starts decreasing such that yi = n \u22121 i , where ni is the number of sets of input words since the last occurrence of the ith word. For omitting words having very low relevance values, we use a threshold \u03c4 = 0.1: when yi < \u03c4 the value of yi is set to zero.\nThe observed values in y, corresponding to the input words so far, are assumed to be formed from the model\ny = Xw\u0302 , (3)\nwhere w\u0302 \u2208 RM is the estimated user intent model describing what documents from the training set are currently estimated to be relevant for the user.\nGiven y and X, the user model w\u0302 can be obtained as\nw\u0302 = (XTX + \u00b5I)\u22121XT y , (4)\nwhere I is an identity matrix of size M \u00d7M and \u00b5 \u2265 0 is a regularization parameter, set to \u00b5 = 1.0 in our experiments.\nUsing Eq. (4) the relevance estimate y\u0302 of the words in the vocabulary is computed as\ny\u0302 = Xw\u0302 = X(XTX + \u00b5I)\u22121XT y = Ay (5)\nand the upper bound of the standard deviation of y\u0302i as\n\u03c3\u0302i = \u2016rowi(A)\u20162. (6)\nThe nexp words to be included in the expanded query correspond to the nexp maximum components of the vector\nv = y\u0302 + c\u03c3\u0302, (7)\nwith words appearing in the input excluded. Here c \u2265 0 is the exploration/exploitation parameter controlling the trade-off between exploring the search space (large c) and focusing on the currently most promising region (small c). We use here c = 1.0 as recommended in the literature [1]."}], "references": [{"title": "Balancing exploration and exploitation: Empirical parameterization of exploratory search systems", "author": ["K. Athukorala", "A. Medlar", "K. Ilves", "D. Glowacka"], "venue": "In Proc. CIKM,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2015}, {"title": "Using confidence bounds for exploitation-exploration trade-offs", "author": ["P. Auer"], "venue": "J. Mach. Learn. Res.,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2003}, {"title": "A writer\u2019s collaborative assistant", "author": ["T. Babaian", "B.J. Grosz", "S.M. Shieber"], "venue": "In Proc. IUI,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2002}, {"title": "Type less, find more: Fast autocompletion search with a succinct index", "author": ["H. Bast", "I. Weber"], "venue": "In Proc. SIGIR,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2006}, {"title": "Proactive information retrieval: Anticipating users\u2019 information need", "author": ["S. Bhatia", "D. Majumdar", "N. Aggarwal"], "venue": "In Proc. ECIR,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2016}, {"title": "Learning to complete sentences", "author": ["S. Bickel", "P. Haider", "T. Scheffer"], "venue": "In Proc. ECML,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2005}, {"title": "Modeling temporal dependencies in high-dimensional sequences: Application to polyphonic music generation and transcription", "author": ["N. Boulanger-Lewandowski", "Y. Bengio", "P. Vincent"], "venue": "In Proc. ICML,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2012}, {"title": "Information access in context", "author": ["J. Budzik", "K. Hammond", "L. Birnbaum"], "venue": "Knowledge-Based Systems,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2001}, {"title": "Context-aware query classification", "author": ["H. Cao", "D.H. Hu", "D. Shen", "D. Jiang", "J.-T. Sun", "E. Chen", "Q. Yang"], "venue": "In Proc. SIGIR,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2009}, {"title": "Actively predicting diverse search intent from user browsing behaviors", "author": ["Z. Cheng", "B. Gao", "T.-Y. Liu"], "venue": "In Proc. WWW,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2010}, {"title": "The Reactive Keyboard: A predictive typing aid", "author": ["J.J. Darragh", "I.H. Witten", "M.L. James"], "venue": null, "citeRegEx": "11", "shortCiteRegEx": "11", "year": 1990}, {"title": "Implicit queries (IQ) for contextualized search", "author": ["S. Dumais", "E. Cutrell", "R. Sarin", "E. Horvitz"], "venue": "In Proc. SIGIR,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2004}, {"title": "Directing exploratory search: Reinforcement learning from user interactions with keywords", "author": ["D. Glowacka", "T. Ruotsalo", "K. Konuyshkova", "S. Kaski", "G. Jacucci"], "venue": "In Proc. IUI,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2013}, {"title": "Sentence completion", "author": ["K. Grabski", "T. Scheffer"], "venue": "In Proc. SIGIR, pages 433\u2013439,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2004}, {"title": "Generating sequences with recurrent neural networks", "author": ["A. Graves"], "venue": "arXiv preprint arXiv:1308.0850,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2013}, {"title": "Speech recognition with deep recurrent neural networks", "author": ["A. Graves", "A. r. Mohamed", "G. Hinton"], "venue": "In Proc ICASSP,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2013}, {"title": "Long short-term memory", "author": ["S. Hochreiter", "J. Schmidhuber"], "venue": "Neural Computation,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 1997}, {"title": "Learning deep structured semantic models for web search using clickthrough data", "author": ["P.-S. Huang", "X. He", "J. Gao", "L. Deng", "A. Acero", "L. Heck"], "venue": "In Proc. CIKM,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2013}, {"title": "Statistical machine translation", "author": ["P. Koehn"], "venue": null, "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2009}, {"title": "Predicting search intent based on pre-search context", "author": ["W. Kong", "R. Li", "J. Luo", "A. Zhang", "Y. Chang", "J. Allan"], "venue": "In Proc. SIGIR,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2015}, {"title": "Distributed representations of sentences and documents", "author": ["Q.V. Le", "T. Mikolov"], "venue": "arXiv preprint arXiv:1405.4053,", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2014}, {"title": "Anticipatory search: Using context to initiate search", "author": ["D.J. Liebling", "P.N. Bennett", "R.W. White"], "venue": "In Proc. SIGIR,", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2012}, {"title": "Citesight: Supporting contextual citation recommendation using differential search", "author": ["A. Livne", "V. Gokuladas", "J. Teevan", "S.T. Dumais", "E. Adar"], "venue": "In Proc. SIGIR,", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2014}, {"title": "A proactive recommendation system for writing: Helping without disrupting", "author": ["M.C.P. Melguizo", "L. Boves", "O.M. Ramos"], "venue": "International Journal of Industrial Ergonomics,", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2009}, {"title": "Query auto-completion for rare prefixes", "author": ["B. Mitra", "N. Craswell"], "venue": "In Proc. CIKM,", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2015}, {"title": "Deep sentence embedding using long short-term memory networks", "author": ["H. Palangi", "L. Deng", "Y. Shen", "J. Gao", "X. He", "J. Chen", "X. Song", "R. Ward"], "venue": null, "citeRegEx": "26", "shortCiteRegEx": "26", "year": 2015}, {"title": "Remembrance Agent: A continuously running automated information retrieval system", "author": ["B. Rhodes", "T. Starner"], "venue": "In Proc. PAAM96,", "citeRegEx": "27", "shortCiteRegEx": "27", "year": 1996}, {"title": "A hierarchical recurrent encoder-decoder for generative context-aware query suggestion", "author": ["A. Sordoni", "Y. Bengio", "H. Vahabi", "C. Lioma", "J. Grue Simonsen", "J.-Y. Nie"], "venue": "In Proc. CIKM,", "citeRegEx": "28", "shortCiteRegEx": "28", "year": 2015}, {"title": "Generating text with recurrent neural networks", "author": ["I. Sutskever", "J. Martens", "G.E. Hinton"], "venue": "In Proc. ICML,", "citeRegEx": "29", "shortCiteRegEx": "29", "year": 2011}, {"title": "Sequence to sequence learning with neural networks", "author": ["I. Sutskever", "O. Vinyals", "Q.V. Le"], "venue": "arXiv preprint arXiv:1409.3215,", "citeRegEx": "30", "shortCiteRegEx": "30", "year": 2014}, {"title": "Proactive computing", "author": ["D. Tennenhouse"], "venue": "Commun. ACM,", "citeRegEx": "31", "shortCiteRegEx": "31", "year": 2000}, {"title": "Writing in the library: Exploring tighter integration of digital library use with the writing process", "author": ["M.B. Twidale", "A.A. Gruzd", "D.M. Nichols"], "venue": "Information Processing & Management,", "citeRegEx": "32", "shortCiteRegEx": "32", "year": 2008}, {"title": "A theory of the task-based information retrieval process: a summary and generalization of a longitudinal study", "author": ["P. Vakkari"], "venue": "Journal of Documentation,", "citeRegEx": "33", "shortCiteRegEx": "33", "year": 2001}, {"title": "Term-by-term query auto-completion for mobile search", "author": ["S. Vargas", "R. Blanco", "P. Mika"], "venue": "In Proc. WSDM,", "citeRegEx": "34", "shortCiteRegEx": "34", "year": 2016}, {"title": "Show and tell: A neural image caption generator", "author": ["O. Vinyals", "A. Toshev", "S. Bengio", "D. Erhan"], "venue": "In Proc. CVPR,", "citeRegEx": "35", "shortCiteRegEx": "35", "year": 2015}, {"title": "Context-aware ranking in web search", "author": ["B. Xiang", "D. Jiang", "J. Pei", "X. Sun", "E. Chen", "H. Li"], "venue": "In Proc. SIGIR,", "citeRegEx": "36", "shortCiteRegEx": "36", "year": 2010}, {"title": "Recurrent neural network regularization", "author": ["W. Zaremba", "I. Sutskever", "O. Vinyals"], "venue": "arXiv preprint arXiv:1409.2329,", "citeRegEx": "37", "shortCiteRegEx": "37", "year": 2014}], "referenceMentions": [{"referenceID": 30, "context": "Proactive systems [31] anticipate the needs of the user and predict possible next steps based on the user\u2019s preferences and current context.", "startOffset": 18, "endOffset": 22}, {"referenceID": 32, "context": "An ideal search engine would support searching and identifying useful information that can then be used in solving these tasks [33].", "startOffset": 127, "endOffset": 131}, {"referenceID": 4, "context": "In proactive information retrieval [5] the estimation of the current task and context are utilized to proactively retrieve and recommend relevant items.", "startOffset": 35, "endOffset": 38}, {"referenceID": 26, "context": ", in a situation where the user does not realize having forgotten about a specific resource [27].", "startOffset": 92, "endOffset": 96}, {"referenceID": 16, "context": "Long short-term memory (LSTM) networks [17] have recently shown remarkable performance in a variety of natural language processing tasks, including speech recognition [16], automatic translation [30], image captioning [35] and information retrieval [18, 26, 28].", "startOffset": 39, "endOffset": 43}, {"referenceID": 15, "context": "Long short-term memory (LSTM) networks [17] have recently shown remarkable performance in a variety of natural language processing tasks, including speech recognition [16], automatic translation [30], image captioning [35] and information retrieval [18, 26, 28].", "startOffset": 167, "endOffset": 171}, {"referenceID": 29, "context": "Long short-term memory (LSTM) networks [17] have recently shown remarkable performance in a variety of natural language processing tasks, including speech recognition [16], automatic translation [30], image captioning [35] and information retrieval [18, 26, 28].", "startOffset": 195, "endOffset": 199}, {"referenceID": 34, "context": "Long short-term memory (LSTM) networks [17] have recently shown remarkable performance in a variety of natural language processing tasks, including speech recognition [16], automatic translation [30], image captioning [35] and information retrieval [18, 26, 28].", "startOffset": 218, "endOffset": 222}, {"referenceID": 17, "context": "Long short-term memory (LSTM) networks [17] have recently shown remarkable performance in a variety of natural language processing tasks, including speech recognition [16], automatic translation [30], image captioning [35] and information retrieval [18, 26, 28].", "startOffset": 249, "endOffset": 261}, {"referenceID": 25, "context": "Long short-term memory (LSTM) networks [17] have recently shown remarkable performance in a variety of natural language processing tasks, including speech recognition [16], automatic translation [30], image captioning [35] and information retrieval [18, 26, 28].", "startOffset": 249, "endOffset": 261}, {"referenceID": 27, "context": "Long short-term memory (LSTM) networks [17] have recently shown remarkable performance in a variety of natural language processing tasks, including speech recognition [16], automatic translation [30], image captioning [35] and information retrieval [18, 26, 28].", "startOffset": 249, "endOffset": 261}, {"referenceID": 14, "context": "LSTM networks have also been used to generate various kinds of sequences, including text [15, 29].", "startOffset": 89, "endOffset": 97}, {"referenceID": 28, "context": "LSTM networks have also been used to generate various kinds of sequences, including text [15, 29].", "startOffset": 89, "endOffset": 97}, {"referenceID": 23, "context": "A typical example of a task is writing about a given topic [24, 27, 33].", "startOffset": 59, "endOffset": 71}, {"referenceID": 26, "context": "A typical example of a task is writing about a given topic [24, 27, 33].", "startOffset": 59, "endOffset": 71}, {"referenceID": 32, "context": "A typical example of a task is writing about a given topic [24, 27, 33].", "startOffset": 59, "endOffset": 71}, {"referenceID": 6, "context": "Recurrent neural networks (RNNs), and LSTMs in particular, have recently been used to generate sequences in various domains, such as music [7], text [15, 29], and handwriting [15].", "startOffset": 139, "endOffset": 142}, {"referenceID": 14, "context": "Recurrent neural networks (RNNs), and LSTMs in particular, have recently been used to generate sequences in various domains, such as music [7], text [15, 29], and handwriting [15].", "startOffset": 149, "endOffset": 157}, {"referenceID": 28, "context": "Recurrent neural networks (RNNs), and LSTMs in particular, have recently been used to generate sequences in various domains, such as music [7], text [15, 29], and handwriting [15].", "startOffset": 149, "endOffset": 157}, {"referenceID": 14, "context": "Recurrent neural networks (RNNs), and LSTMs in particular, have recently been used to generate sequences in various domains, such as music [7], text [15, 29], and handwriting [15].", "startOffset": 175, "endOffset": 179}, {"referenceID": 25, "context": ", for extracting sentence-level semantic vectors [26] and context-aware query suggestion [28].", "startOffset": 49, "endOffset": 53}, {"referenceID": 27, "context": ", for extracting sentence-level semantic vectors [26] and context-aware query suggestion [28].", "startOffset": 89, "endOffset": 93}, {"referenceID": 17, "context": "Other kinds of deep neural networks have been used to project queries and documents to low-dimensional semantic spaces [18] and to learn fixed-length vectors for variable-length pieces of texts, such as sentences, paragraphs, and documents [21].", "startOffset": 119, "endOffset": 123}, {"referenceID": 20, "context": "Other kinds of deep neural networks have been used to project queries and documents to low-dimensional semantic spaces [18] and to learn fixed-length vectors for variable-length pieces of texts, such as sentences, paragraphs, and documents [21].", "startOffset": 240, "endOffset": 244}, {"referenceID": 9, "context": "Motivated by the observation that a notable proportion of the user\u2019s information needs were triggered by previous web browsing activity, several authors have studied the correlation between web browsing behavior and consecutive searches [10, 20, 22].", "startOffset": 237, "endOffset": 249}, {"referenceID": 19, "context": "Motivated by the observation that a notable proportion of the user\u2019s information needs were triggered by previous web browsing activity, several authors have studied the correlation between web browsing behavior and consecutive searches [10, 20, 22].", "startOffset": 237, "endOffset": 249}, {"referenceID": 21, "context": "Motivated by the observation that a notable proportion of the user\u2019s information needs were triggered by previous web browsing activity, several authors have studied the correlation between web browsing behavior and consecutive searches [10, 20, 22].", "startOffset": 237, "endOffset": 249}, {"referenceID": 8, "context": "Another popular approach is to use previous search queries [9, 36].", "startOffset": 59, "endOffset": 66}, {"referenceID": 35, "context": "Another popular approach is to use previous search queries [9, 36].", "startOffset": 59, "endOffset": 66}, {"referenceID": 3, "context": "Our work is related to query auto-completion [4], in which possible completions of search engine queries are predicted.", "startOffset": 45, "endOffset": 48}, {"referenceID": 24, "context": "Query auto-completion for web and mobile queries has recently been studied in [25, 34].", "startOffset": 78, "endOffset": 86}, {"referenceID": 33, "context": "Query auto-completion for web and mobile queries has recently been studied in [25, 34].", "startOffset": 78, "endOffset": 86}, {"referenceID": 2, "context": "The existing work has largely focused on bibliographic tasks involved in writing scientific or professional texts [3, 23, 32].", "startOffset": 114, "endOffset": 125}, {"referenceID": 22, "context": "The existing work has largely focused on bibliographic tasks involved in writing scientific or professional texts [3, 23, 32].", "startOffset": 114, "endOffset": 125}, {"referenceID": 31, "context": "The existing work has largely focused on bibliographic tasks involved in writing scientific or professional texts [3, 23, 32].", "startOffset": 114, "endOffset": 125}, {"referenceID": 23, "context": "In [24], the impact of proactive recommendations to different phases of the writing process is analyzed.", "startOffset": 3, "endOffset": 7}, {"referenceID": 10, "context": "The Reactive Keyboard [11] is an early prototype for predicting the succeeding words from a user-written text fragment.", "startOffset": 22, "endOffset": 26}, {"referenceID": 5, "context": "Completing written sentences has also been studied in [6, 14].", "startOffset": 54, "endOffset": 61}, {"referenceID": 13, "context": "Completing written sentences has also been studied in [6, 14].", "startOffset": 54, "endOffset": 61}, {"referenceID": 26, "context": "The setup of our work is closely related to what was envisioned already by Rhodes and Starner in their Remembrance Agent (RA) [27].", "startOffset": 126, "endOffset": 130}, {"referenceID": 7, "context": "Other similar tools include Watson [8] and Implicit Query [12].", "startOffset": 35, "endOffset": 38}, {"referenceID": 11, "context": "Other similar tools include Watson [8] and Implicit Query [12].", "startOffset": 58, "endOffset": 62}, {"referenceID": 18, "context": "For computing the continuations, we use a beam search algorithm [19] described below.", "startOffset": 64, "endOffset": 68}, {"referenceID": 12, "context": "As a comparison method, we use an upper confidence bound algorithm, which is based on estimating the user intent using a multi-armed bandit model [13].", "startOffset": 146, "endOffset": 150}, {"referenceID": 1, "context": "The model balances exploration with exploitation and selects words that have the highest upper confidence bound [2].", "startOffset": 112, "endOffset": 115}, {"referenceID": 36, "context": "[37] with an Tensorflow implementation.", "startOffset": 0, "endOffset": 4}, {"referenceID": 10, "context": "The text predictions produced by the LSTM network could also be used to to automatically suggest different continuations for the currently written text, as in the Reactive Keyboard [11], [6], or [14].", "startOffset": 181, "endOffset": 185}, {"referenceID": 5, "context": "The text predictions produced by the LSTM network could also be used to to automatically suggest different continuations for the currently written text, as in the Reactive Keyboard [11], [6], or [14].", "startOffset": 187, "endOffset": 190}, {"referenceID": 13, "context": "The text predictions produced by the LSTM network could also be used to to automatically suggest different continuations for the currently written text, as in the Reactive Keyboard [11], [6], or [14].", "startOffset": 195, "endOffset": 199}], "year": 2016, "abstractText": "We describe a method for proactive information retrieval targeted at retrieving relevant information during a writing task. In our method, the current task and the needs of the user are estimated, and the potential next steps are unobtrusively predicted based on the user\u2019s past actions. We focus on the task of writing, in which the user is coalescing previously collected information into a text. Our proactive system automatically recommends the user relevant background information. The proposed system incorporates text input prediction using a long short-term memory (LSTM) network. We present simulations, which show that the system is able to reach higher precision values in an exploratory search setting compared to both a baseline and a comparison system.", "creator": "LaTeX with hyperref package"}}}