{"id": "1606.06461", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "21-Jun-2016", "title": "Neighborhood Mixture Model for Knowledge Base Completion", "abstract": "Knowledge bases are useful resources for many natural language processing tasks, however, they are far from complete. In this paper, we define a novel entity representation as a mixture of its neighborhood in the knowledge base and apply this technique on TransE-a well-known embedding model for knowledge base completion. Experimental results show that the neighborhood information significantly helps to improve the results of the TransE, leading to better performance than obtained by other state-of-the-art embedding models on three benchmark datasets for triple classification, entity prediction and relation prediction tasks.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n", "histories": [["v1", "Tue, 21 Jun 2016 07:54:35 GMT  (46kb,D)", "http://arxiv.org/abs/1606.06461v1", "To appear in Proceedings of CoNLL 2016"], ["v2", "Thu, 21 Jul 2016 16:08:32 GMT  (40kb,D)", "http://arxiv.org/abs/1606.06461v2", "V1: To appear in Proceedings of CoNLL 2016. V2: Corrected citation to (Krompa{\\ss} et al., 2015)"], ["v3", "Thu, 9 Mar 2017 12:51:31 GMT  (42kb,D)", "http://arxiv.org/abs/1606.06461v3", "V1: In Proceedings of the 20th SIGNLL Conference on Computational Natural Language Learning, CoNLL 2016. V2: Corrected citation to (Krompa{\\ss} et al., 2015). V3: A revised version of our CoNLL 2016 paper to update latest related work"]], "COMMENTS": "To appear in Proceedings of CoNLL 2016", "reviews": [], "SUBJECTS": "cs.CL cs.AI", "authors": ["dat quoc nguyen", "kairit sirts", "lizhen qu", "mark johnson"], "accepted": false, "id": "1606.06461"}, "pdf": {"name": "1606.06461.pdf", "metadata": {"source": "CRF", "title": "Neighborhood Mixture Model for Knowledge Base Completion\u2217", "authors": ["Dat Quoc Nguyen", "Kairit Sirts", "Lizhen Qu", "Mark Johnson"], "emails": ["dat.nguyen@students.mq.edu.au,", "mark.johnson}@mq.edu.au", "lizhen.qu@data61.csiro.au"], "sections": [{"heading": null, "text": "Keywords: Knowledge base completion, embedding model, mixture model, link prediction, triple classification, entity prediction, relation prediction."}, {"heading": "1 Introduction", "text": "Knowledge bases (KBs), such as WordNet (Miller, 1995), YAGO (Suchanek et al., 2007), Freebase (Bollacker et al., 2008) and DBpedia (Lehmann et al., 2015), represent relationships between entities as triples (head entity, relation, tail entity). Even very large knowledge bases are still far from complete (Socher et al., 2013; West et al., 2014). Knowledge base completion or link prediction systems (Nickel et al., 2015) predict which triples not in a knowledge base are likely to be true (Taskar et al., 2004; Bordes et al., 2011).\nEmbedding models for KB completion associate entities and/or relations with dense feature vectors or matrices. Such models obtain state-of-the-art\n\u2217To appear in Proceedings of CoNLL 2016.\nperformance (Bordes et al., 2012; Bordes et al., 2013; Socher et al., 2013; Wang et al., 2014; Guu et al., 2015; Nguyen et al., 2016) and generalize to large KBs (Krompa et al., 2015).\nMost embedding models for KB completion learn only from triples and by doing so, ignore lots of information implicitly provided by the structure of the knowledge graph. Recently, several authors have addressed this issue by incorporating relation path information into model learning (Garc\u0131\u0301aDura\u0301n et al., 2015; Lin et al., 2015a; Guu et al., 2015; Toutanova et al., 2016) and have shown that the relation paths between entities in KBs provide useful information and improve knowledge base completion. For instance, a three-relation path\n(head,born in hospital/r1, e1)\n\u21d2(e1, hospital located in city/r2, e2) \u21d2(e2, city in country/r3, tail)\nis likely to indicate that the fact (head, nationality, tail) could be true, so the relation path here p = {r1, r2, r3} is useful for predicting the relationship \u201cnationality\u201d between the head and tail entities.\nBesides the relation paths, there could be other useful information implicitly presented in the knowledge base that could be exploited for better KB completion. For instance, the whole neighborhood of entities could provide lots of useful information for predicting the relationship between two entities. Consider for example a KB fragment given in Figure 1. If we know that Ben Affleck has won an Oscar award and Ben Affleck lives in Los Angeles, then this can help us to predict that Ben Affleck is an actor or a film maker, rather than a lecturer or a doctor. If we additionally know that Ben Affleck\u2019s gender is male then there is a higher chance for him to be a film maker. This intuition can be formalized by representing an entity vector ar X\niv :1\n60 6.\n06 46\n1v 1\n[ cs\n.C L\n] 2\n1 Ju\nn 20\n16\nas a relation-specific mixture of its neighborhood as follows:\nBen Affleck = \u03c9r,1(Violet Anne, child of)\n+ \u03c9r,2(male, gender \u22121) + \u03c9r,3(Los Angeles, lives in \u22121) + \u03c9r,4(Oscar award,won \u22121),\nwhere \u03c9r,i are the mixing weights that indicate how important each neighboring relation is for predicting the relation r. For example, for predicting the occupation relationship, the knowledge about the child of relationship might not be that informative and thus the corresponding mixing coefficient can be close to zero, whereas it could be relevant for predicting some other relationship, such as parent or spouse, in which case the relation-specific mixing coefficient for the child of relationship could be high.\nThe primary contribution of this paper is introducing and formalizing the neighborhood mixture model. We demonstrate its usefulness by applying it to the well-known TransE model (Bordes et al., 2013). However, it could be applied to other embedding models as well, such as Bilinear models (Bordes et al., 2012; Yang et al., 2015) and STransE (Nguyen et al., 2016). While relation path models exploit extra information using longer paths existing in the KB, the neighborhood mixture model effectively incorporates information\nabout many paths simultaneously. Our extensive experiments on three benchmark datasets show that it achieves superior performance over competitive baselines in three KB completion tasks: triple classification, entity prediction and relation prediction."}, {"heading": "2 Neighborhood mixture modeling", "text": "In this section, we start by explaining how to formally construct the neighbor-based entity representations in section 2.1, and then describe the Neighborhood Mixture Model applied to the TransE model (Bordes et al., 2013) in section 2.2. Section 2.3 explains how we train our model."}, {"heading": "2.1 Neighbor-based entity representation", "text": "Let E denote the set of entities and R the set of relation types. Denote by R\u22121 the set of inverse relations r\u22121. Denote by G the knowledge graph consisting of a set of correct tiples (h, r, t), such that h, t \u2208 E and r \u2208 R. Let K denote the symmetric closure of G, i.e. if a triple (h, r, t) \u2208 G, then both (h, r, t) and (t, r\u22121, h) \u2208 K.\nDefine:\nNe,r = {e\u2032|(e\u2032, r, e) \u2208 K}\nas a set of neighboring entities connected to entity e with relation r. Then\nNe = {(e\u2032, r)|r \u2208 R \u222aR\u22121, e\u2032 \u2208 Ne,r}\nis the set of all entity and relation pairs that are neighbors for entity e.\nEach entity e is associated with a k-dimensional vector ve \u2208 Rk and relation-dependent vectors ue,r \u2208 Rk, r \u2208 R \u222aR\u22121. Now we can define the neighborhood-based entity representation \u03d1e,r for an entity e \u2208 E for predicting the relation r \u2208 R as follows:\n\u03d1e,r = aeve + \u2211\n(e\u2032,r\u2032)\u2208Ne\nbr,r\u2032ue\u2032,r\u2032 , (1)\nae and br,r\u2032 are the mixture weights that are constrained to sum to 1 for each neighborhood:\nae \u221d \u03b4 + exp\u03b1e (2) br,r\u2032 \u221d exp\u03b2r,r\u2032 (3)\nwhere \u03b4 > 0 is a hyper-parameter that controls the contribution of the entity vector ve to the neighbor-based mixture, \u03b1e and \u03b2r,r\u2032 are the learnable exponential mixture parameters.\nIn real-world factual KBs, e.g. Freebase (Bollacker et al., 2008), some entities, such as \u201cmale\u201d, can have thousands or millions neighboring entities sharing the same relation \u201cgender.\u201d For such entities, computing the neighbor-based vectors can be computationally very expensive. To overcome this problem, we introduce in our implementation a filtering threshold \u03c4 and consider in the neighbor-based entity representation construction only those relation-specific neighboring entity sets for which |Ne,r| \u2264 \u03c4 ."}, {"heading": "2.2 TransE-NMM: applying neighborhood mixtures to TransE", "text": "Embedding models define for each triple (h, r, t) \u2208 G, a score function f(h, r, t) that measures its implausibility. The goal is to choose f such that the score f(h, r, t) of a plausible triple (h, r, t) is smaller than the score f(h\u2032, r\u2032, t\u2032) of an implausible triple (h\u2032, r\u2032, t\u2032).\nTransE (Bordes et al., 2013) is a simple embedding model for knowledge base completion, which, despite of its simplicity, obtains very competitive results (Garc\u0131\u0301a-Dura\u0301n et al., 2016; Nickel et al., 2016). In TransE, both entities e and relations r are represented with k-dimensional vectors ve \u2208 Rk and vr \u2208 Rk, respectively. These vectors are chosen such that for each triple (h, r, t) \u2208 G:\nvh + vr \u2248 vt (4)\nThe score function of the TransE model is the norm of this translation:\nf(h, r, t)TransE = \u2016vh + vr \u2212 vt\u2016`1/2 (5)\nWe define the score function of our new model TransE-NMM in terms of the neighbor-based entity vectors as follows:\nf(h, r, t) = \u2016\u03d1h,r + vr \u2212 \u03d1t,r\u22121\u2016`1/2 , (6)\nusing either the `1 or the `2-norm, and \u03d1h,r and \u03d1t,r\u22121 are defined following the Equation 1. The relation-specific entity vectors ue,r used to construct the neighbor-based entity vectors \u03d1e,r are defined based on the TransE translation operator:\nue,r = ve + vr (7)\nin which vr\u22121 = \u2212vr. For each correct triple (h, r, t), the sets of neighboring entities Nh,r and Nt,r\u22121 exclude the entities t and h, respectively.\nIf we set the filtering threshold \u03c4 = 0 then \u03d1h,r = vh and \u03d1t,r\u22121 = vt for all triples. In this case, TransE-NMM reduces to the plain TransE model. In all our experiments presented in section 4, the baseline TransE results are obtained with the TransE-NMM with \u03c4 = 0."}, {"heading": "2.3 Parameter optimization", "text": "The TransE-NMM model parameters include the vectors ve,vr for entities and relation types, the entity-specific weights \u03b1 = {\u03b1e|e \u2208 E} and relation-specific weights \u03b2 = {\u03b2r,r\u2032 |r, r\u2032 \u2208 R \u222a R\u22121}. To learn these parameters, we minimize the L2-regularized margin-based objective function:\nL = \u2211\n(h,r,t)\u2208G (h\u2032,r,t\u2032)\u2208G\u2032\n(h,r,t)\n[\u03b3 + f(h, r, t)\u2212 f(h\u2032, r, t\u2032)]+\n+ \u03bb\n2\n( \u2016\u03b1\u201622 + \u2016\u03b2\u201622 ) , (8)\nwhere [x]+ = max(0, x), \u03b3 is the margin hyperparameter, \u03bb is the L2 regularization parameter and\nG\u2032(h,r,t) = {(h \u2032, r, t) | h\u2032 \u2208 E , (h\u2032, r, t) /\u2208 G}\n\u222a {(h, r, t\u2032) | t\u2032 \u2208 E , (h, r, t\u2032) /\u2208 G}\nis the set of incorrect triples generated by corrupting the correct triple (h, r, t) \u2208 G. We applied the \u201cBernoulli\u201d trick to choose whether to generate the head or tail entity when sampling an incorrect triple (Wang et al., 2014; Lin et al., 2015b; He et al., 2015; Ji et al., 2015; Ji et al., 2016).\nWe use Stochastic Gradient Descent (SGD) with RMSProp adaptive learning rate to minimize L, and impose the following hard constraints during training: \u2016ve\u20162 6 1 and \u2016vr\u20162 6 1. We employ alternating optimization to minimize L. We first initialize the entity and relation-specific mixing parameters \u03b1 and \u03b2 to zero and only learn the randomly initialized entity and relation vectors ve and vr. Then we fix the learned vectors and only optimize the mixing parameters. In the final step, we fix again the mixing parameters and fine-tune the vectors. In all experiments presented in section 4, we train for 200 epochs during each three optimization step."}, {"heading": "3 Related work", "text": "Table 1 summarizes related embedding models for link prediction and KB completion. The models\ndiffer in their score function f(h, r, t) and the algorithm used to optimize their margin-based objective function, e.g., SGD, AdaGrad (Duchi et al., 2011), AdaDelta (Zeiler, 2012) or L-BFGS (Liu and Nocedal, 1989).\nThe Unstructured model (Bordes et al., 2012) assumes that the head and tail entity vectors are similar. As the Unstructured model does not take the relationship into account, it cannot distinguish different relation types. The Structured Embedding (SE) model (Bordes et al., 2011) extends the Unstructured model by assuming that the head and tail entities are similar only in a relation-dependent subspace, where each relation is represented by two different matrices. Futhermore, the SME model (Bordes et al., 2012) uses four different matrices to project entity and relation vectors into a subspace. The TransH model (Wang et al., 2014) associates each relation with a relation-specific hyperplane and uses a projection vector to project entity vectors onto that hyperplane. TransD (Ji et al., 2015) and TransR/CTransR (Lin et al., 2015b) extend the TransH model by using two projection vectors and a matrix to project entity vectors into a relation-specific space, respectively. STransE (Nguyen et al., 2016) and TranSparse (Ji et al.,\n2016) are extensions of the TransR model, where head and tail entities are associated with their own projection matrices.\nThe DISTMULT model (Yang et al., 2015) is based on the Bilinear model (Nickel et al., 2011; Bordes et al., 2012; Jenatton et al., 2012) where each relation is represented by a diagonal rather than a full matrix. The neural tensor network (NTN) model (Socher et al., 2013) uses a bilinear tensor operator to represent each relation. Similar quadratic forms are used to model entities and relations in KG2E (He et al., 2015) and TATEC (Garc\u0131\u0301a-Dura\u0301n et al., 2016).\nRecently, Neelakantan et al. (2015), Gardner and Mitchell (2015), Luo et al. (2015), Lin et al. (2015a), Garc\u0131\u0301a-Dura\u0301n et al. (2015), Guu et al. (2015) and Toutanova et al. (2016) showed that relation paths between entities in KBs provide richer information and improve the relationship prediction. In fact, our new TransE-NMM model can be also viewed as a three-relation path model as it takes into account the neighborhood entity and relation information of both head and tail entities in each triple.\nLuo et al. (2015) constructed relation paths between entities and viewing entities and relations\nin the path as pseudo-words applied Word2Vec algorithms (Mikolov et al., 2013) to produce pretrained vectors for these pseudo-words. Luo et al. (2015) showed that using these pre-trained vectors for initialization helps to improve the performance of the TransE, SME and SE models. RTransE (Garc\u0131\u0301a-Dura\u0301n et al., 2015), PTransE (Lin et al., 2015a) and TransE-COMP (Guu et al., 2015) are extensions of the TransE model. These models similarly represent a relation path by a vector which is the sum of the vectors of all relations in the path, whereas in the Bilinear-COMP model (Guu et al., 2015), each relation is a matrix and so it represents the relation path by matrix multiplication. Our neighborhood mixture model can be adapted to both relation path models Bilinear-COMP and TransE-COMP, by replacing head and tail entity vectors by the neighborbased vector representations, thus combining advantages of both path and neighborhood information. Nickel et al. (2015) reviews other approaches for learning from KBs and multi-relational data."}, {"heading": "4 Experiments", "text": "To investigate the usefulness of the neighbor mixtures, we compare the performance of the TransENMM against the results of the baseline TransE and other state-of-the-art embedding models on the triple classification, entity prediction and relation prediction tasks."}, {"heading": "4.1 Datasets", "text": "We conduct experiments using three publicly available datasets WN11, FB13 and NELL186.\nFor all of them, the validation and test sets containing both correct and incorrect triples have already been constructed. Statistical information about these datasets is given in Table 2.\nThe two benchmark datasets1, WN11 and FB13, were produced by Socher et al. (2013) for triple classification. WN11 is derived from the large lexical KB WordNet (Miller, 1995) involving 11 relation types. FB13 is derived from the large real-world fact KB FreeBase (Bollacker et al., 2008) covering 13 relation types. The NELL186 dataset2 was introduced by Guo et al. (2015) for both triple classification and entity prediction tasks, containing 186 most frequent relations in the KB of the CMU Never Ending Language Learning project (Carlson et al., 2010)."}, {"heading": "4.2 Evaluation tasks", "text": "We evaluate our model on three commonly used benchmark tasks: triple classification, entity prediction and relation prediction. This subsection describes those tasks in detail.\nTriple classification: The triple classification task was first introduced by Socher et al. (2013), and since then it has been used to evaluate various embedding models. The aim of the task is to predict whether a triple (h, r, t) is correct or not.\nFor classification, we set a relation-specific threshold \u03b8r for each relation type r. If the implausibility score of an unseen test triple (h, r, t) is smaller than \u03b8r then the triple will be classified as correct, otherwise incorrect. Following Socher et al. (2013), the relation-specific thresholds are determined by maximizing the micro-averaged accuracy, which is a per-triple average, on the validation set. We also report the macro-averaged accuracy, which is a per-relation average.\nEntity prediction: The entity prediction task (Bordes et al., 2013) predicts the head or the tail entity given the relation type and the other entity, i.e. predicting h given (?, r, t) or predicting t given (h, r, ?) where ? denotes the missing element. The results are evaluated using a ranking induced by the function f(h, r, t) on test triples. Note that the incorrect triples in the validation and test sets are not used for evaluating the entity prediction task nor the relation prediction task.\n1http://cs.stanford.edu/people/danqi/data/nips13-dataset.tar.bz2 2http://aclweb.org/anthology/attachments/P/P15/\nP15-1009.Datasets.zip\nEach correct test triple (h, r, t) is corrupted by replacing either its head or tail entity by each of the possible entities in turn, and then we rank these candidates in ascending order of their implausibility score. This is called as the \u201cRaw\u201d setting protocol. For the \u201cFiltered\u201d setting protocol described in Bordes et al. (2013), we also filter out before ranking any corrupted triples that appear in the KB. Ranking a corrupted triple appearing in the KB (i.e. a correct triple) higher than the original test triple is also correct, but is penalized by the \u201cRaw\u201d score, thus the \u201cFiltered\u201d setting provides a clearer view on the ranking performance.\nIn addition to the mean rank and the Hits@10 (i.e., the proportion of test triples for which the target entity was ranked in the top 10 predictions), which were originally used in the entity prediction task (Bordes et al., 2013), we also report the mean reciprocal rank (MRR), which is commonly used in information retrieval. In both \u201cRaw\u201d and \u201cFiltered\u201d settings, mean rank is always greater or equal to 1 and lower mean rank indicates better entity prediction performance. The MRR and Hits@10 scores always range from 0.0 to 1.0, and higher score reflects better prediction result.\nRelation prediction: The relation prediction task (Lin et al., 2015a) predicts the relation type given the head and tail entities, i.e. predicting r given (h, ?, t) where ? denotes the missing element. We corrupt each correct test triple (h, r, t) by replacing its relation r by each possible relation type in turn, and then rank these candidates in ascending order of their implausibility score. Just as in the entity prediction task, we use two setting protocols, \u201cRaw\u201d and \u201cFiltered\u201d, and evaluate on mean rank, MRR and Hits@10."}, {"heading": "4.3 Hyper-parameter tuning", "text": "For all evaluation tasks, results for TransE are obtained with TransE-NMM with the filtering threshold \u03c4 = 0, while we set \u03c4 = 10 for TransE-NMM.\nFor triple classification, we first performed a grid search to choose the optimal hyperparameters for TransE by monitoring the microaveraged triple classification accuracy after each training epoch on the validation set. For all datasets, we chose either the `1 or `2 norm in the score function f and the initial RMSProp learning rate \u03b7 \u2208 {0.001, 0.01}. Following the previous work (Wang et al., 2014; Lin et al., 2015b; Ji et al., 2015; He et al., 2015; Ji et al., 2016), we selected\nthe margin hyper-parameter \u03b3 \u2208 {1, 2, 4} and the number of vector dimensions k \u2208 {20, 50, 100} on WN11 and FB13. On NELL186, we set \u03b3 = 1 and k = 50 (Guo et al., 2015; Luo et al., 2015). The highest accuracy on the validation set was obtained when using \u03b7 = 0.01 for all three datasets, and when using `2 norm for NELL186, \u03b3 = 4, k = 20 and `1 norm for WN11, and \u03b3 = 1, k = 100 and `2 norm for FB13.\nWe set the hyper-parameters \u03b7, \u03b3, k, and the `1 or the `2-norm in our TransE-NMM model to the same optimal hyper-parameters searched for TransE. We then used a grid search to select the hyper-parameter \u03b4 \u2208 {0, 1, 5, 10} and L2 regularizer \u03bb \u2208 {0.005, 0.01, 0.05} for TransE-NMM. By monitoring the micro-averaged accuracy after each training epoch, we obtained the highest accuracy on validation set when using \u03b4 = 1 and \u03bb = 0.05 for both WN11 and FB13, and \u03b4 = 0 and \u03bb = 0.01 for NELL186.\nFor both entity prediction and relation prediction tasks, we set the hyper-parameters \u03b7, \u03b3, k, and the `1 or the `2-norm for both TransE and TransE-NMM to be the same as the optimal parameters found for the triple classification task. We then monitored on TransE the filtered MRR on validation set after each training epoch. We chose the model with highest validation MRR, which was then used to evaluate the test set. For TransE-NMM, we searched the hyperparameter \u03b4 \u2208 {0, 1, 5, 10} and L2 regularizer \u03bb \u2208 {0.005, 0.01, 0.05}. By monitoring the filtered MRR after each training epoch, we selected the best model with the highest filtered MRR on the validation set. Specifically, for the entity prediction task, we selected \u03b4 = 10 and \u03bb = 0.005 for WN11, \u03b4 = 5 and \u03bb = 0.01 for FB13, and \u03b4 = 5 and \u03bb = 0.005 for NELL186. For the relation prediction task, we selected \u03b4 = 10 and \u03bb = 0.005 for WN11, \u03b4 = 10 and \u03bb = 0.05 for FB13, and \u03b4 = 1 and \u03bb = 0.05 for NELL186."}, {"heading": "5 Results", "text": ""}, {"heading": "5.1 Quantitative results", "text": "Table 3 presents the results of TransE and TransENMM on triple classification, entity prediction and relation prediction tasks on all experimental datasets. The results show that TransE-NMM generally performs better than TransE in all three evaluation tasks.\nSpecifically, TransE-NMM obtains higher triple\nclassification results than TransE in all three experimental datasets, for example, with 2.44% absolute improvement in the micro-averaged accuracy on the NELL186 dataset (i.e. 31% reduction in error). In terms of entity prediction, TransE-NMM obtains better mean rank, MRR and Hits@10 scores than TransE on both FB13 and NELL186 datasets. Specifically, on NELL186\nTransE-NMM gains a significant improvement of 279 \u2212 214 = 65 in the filtered mean rank (which is about 23% relative improvement), while on the FB13 dataset, TransE-NMM improves with 0.267\u22120.213 = 0.054 in the filtered MRR (which is about 25% relative improvement). On the WN11 dataset, TransE-NMM only achieves better mean rank for entity prediction. The relation prediction results of TransE-NMM and TransE are relatively similar on both WN11 and FB13 because the number of relation types is small in these two datasets. On NELL186, however, TransENMM does significantly better than TransE.\nIn Table 4, we compare the micro-averaged\ntriple classification accuracy of our TransE-NMM model with the previously reported results on the WN11 and FB13 datasets. The first five rows report the performance of models that use TransE to initialize the entity and relation vectors. The last eight rows present the accuracy of models with randomly initialized parameters.\nTable 4 shows that our TransE-NMM model obtains the highest accuracy on WN11 and achieves the second highest result on FB13. Note that there are higher results reported for NTN (Socher et al., 2013), Bilinear-COMP (Guu et al., 2015) and TransE-COMP when entity vectors are initialized by averaging the pre-trained word vectors (Mikolov et al., 2013; Pennington et al., 2014). It is not surprising as many entity names in WordNet and FreeBase are lexically meaningful. It is possible for all other embedding models to utilize the pre-trained word vectors as well. However, as pointed out by Wang et al. (2014) and Guu et al. (2015), averaging the pre-trained word vectors for initializing entity vectors is an open problem and it is not always useful since entity names in many domain-specific KBs are not lexically meaningful.\nTable 5 compares the accuracy for triple classification, the raw mean rank and raw Hits@10 scores for entity prediction on the NELL186 dataset. The first three rows present the best results reported in Guo et al. (2015), while the next three rows present the best results reported in Luo et al. (2015). TransE-NMM obtains the highest triple classification accuracy, the best raw mean rank and the second highest raw Hits@10 on the entity prediction task in this comparison."}, {"heading": "5.2 Qualitative results", "text": "Table 6 presents some examples to illustrate the useful information modeled by the neighbors. We took the relation-specific mixture weights from the learned TransE-NMM model optimized on the entity prediction task, and then extracted three neighbor relations with the largest mixture weights given a relation.\nTable 6 shows that those relations are semantically coherent. For example, if we know the place of birth and/or the place of death of a person and/or the location where the person is living, it is likely that we can predict the person\u2019s nationality. On the other hand, if we know that a person works for an organization and that this person is also the top member of that organization, then it is possible\nthat this person is the CEO of that organization."}, {"heading": "5.3 Discussion", "text": "Despite of the lower triple classification scores of TransE reported in Wang et al. (2014), Table 4 shows that TransE in fact obtains a very competitive accuracy. Particularly, compared to the relation path model TransE-COMP (Guu et al., 2015), when model parameters were randomly initialized, TransE obtains 85.2\u2212 80.3 = 4.9% absolute accuracy improvement on the WN11 dataset while achieving similar score on the FB13 dataset. Our high results of the TransE model are probably due to a careful grid search and using the \u201cBernoulli\u201d trick. Note that Lin et al. (2015b), Ji et al. (2015) and Ji et al. (2016) did not report the TransE results used for initializing TransR, TransD and TranSparse, respectively. They directly copied the TransE results previously reported in Wang et al. (2014). So it is difficult to determine exactly how much TransR, TransD and TranSparse gain over TransE. These models might obtain better results than previously reported when the TransE used for initalization performs as well as reported in this paper. Furthermore, Garc\u0131\u0301a-Dura\u0301n et al. (2015), Lin et al. (2015a), Garc\u0131\u0301a-Dura\u0301n et al. (2016) and Nickel et al. (2016) also showed that for entity prediction TransE obtains very competitive results which are much higher than the TransE results originally published in Bordes et al. (2013).3\nAs presented in Table 3, for entity predic-\n3They did not report the results on WN11 and FB13 datasets, which are used in this paper, though.\ntion using WN11, TransE-NMM with the filtering threshold \u03c4 = 10 only obtains better mean rank than TransE (about 15% relative improvement) but lower Hits@10 and mean reciprocal rank. The reason might be that in semantic lexical KBs such as WordNet where relationships between words or word groups are manually constructed, whole neighborhood information might be useful. So when using a small filtering threshold, the model ignores a lot of potential information that could help predicting relationships. Figure 2 presents relative improvements in entity prediction of TransE-NMM over TransE on WN11 when varying the filtering threshold \u03c4 . Figure 2 shows that TransE-NMM gains better scores with higher \u03c4 value. Specifically, when \u03c4 = 500 TransE-NMM does significantly better than TransE in all entity prediction metrics."}, {"heading": "6 Conclusion and future work", "text": "We introduced a neighborhood mixture model for knowledge base completion by constructing neighbor-based vector representations for entities. We demonstrated its effect by extending TransE (Bordes et al., 2013) with our neighborhood mixture model. On three different datasets, experimental results show that our model significantly\nimproves TransE and obtains better results than the other state-of-the-art embedding models on triple classification, entity prediction and relation prediction tasks. In future work, we plan to apply the neighborhood mixture model to other embedding models, especially to relation path models such as TransE-COMP, to combine the useful information from both relation paths and entity neighborhoods."}, {"heading": "Acknowledgments", "text": "This research was supported by a Google award through the Natural Language Understanding Focused Program, and under the Australian Research Council\u2019s Discovery Projects funding scheme (project number DP160102156). This research was also supported by NICTA, funded by the Australian Government through the Department of Communications and the Australian Research Council through the ICT Centre of Excellence Program. The first author was supported by an International Postgraduate Research Scholarship and a NICTA NRPA Top-Up Scholarship."}], "references": [{"title": "Freebase: A Collaboratively Created Graph Database for Structuring Human Knowledge", "author": ["Colin Evans", "Praveen Paritosh", "Tim Sturge", "Jamie Taylor"], "venue": "In Proceedings of the 2008 ACM SIGMOD Interna-", "citeRegEx": "Bollacker et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Bollacker et al\\.", "year": 2008}, {"title": "Learning Structured Embeddings of Knowledge Bases", "author": ["Jason Weston", "Ronan Collobert", "Yoshua Bengio"], "venue": "In Proceedings of the Twenty-Fifth AAAI Conference on Artificial Intelligence,", "citeRegEx": "Bordes et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Bordes et al\\.", "year": 2011}, {"title": "A Semantic Matching Energy Function for Learning with Multirelational Data", "author": ["Xavier Glorot", "Jason Weston", "Yoshua Bengio"], "venue": "Machine Learning,", "citeRegEx": "Bordes et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Bordes et al\\.", "year": 2012}, {"title": "Translating Embeddings for Modeling Multi-relational Data", "author": ["Nicolas Usunier", "Alberto Garcia-Duran", "Jason Weston", "Oksana Yakhnenko"], "venue": "In Advances in Neural Information Processing Systems", "citeRegEx": "Bordes et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Bordes et al\\.", "year": 2013}, {"title": "Toward an Architecture for Never-ending Language Learning", "author": ["Justin Betteridge", "Bryan Kisiel", "Burr Settles", "Estevam R. Hruschka", "Jr.", "Tom M. Mitchell"], "venue": "In Proceedings of the Twenty-Fourth AAAI Conference", "citeRegEx": "Carlson et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Carlson et al\\.", "year": 2010}, {"title": "Adaptive Subgradient Methods for Online Learning and Stochastic Optimization", "author": ["Duchi et al.2011] John Duchi", "Elad Hazan", "Yoram Singer"], "venue": "The Journal of Machine Learning Research,", "citeRegEx": "Duchi et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Duchi et al\\.", "year": 2011}, {"title": "Composing Relationships with Translations", "author": ["Antoine Bordes", "Nicolas Usunier"], "venue": "In Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing,", "citeRegEx": "Garc\u0131\u0301a.Dur\u00e1n et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Garc\u0131\u0301a.Dur\u00e1n et al\\.", "year": 2015}, {"title": "Combining Two and Three-Way Embedding Models for Link Prediction in Knowledge Bases", "author": ["Antoine Bordes", "Nicolas Usunier", "Yves Grandvalet"], "venue": "Journal of Artificial Intelligence Research,", "citeRegEx": "Garc\u0131\u0301a.Dur\u00e1n et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Garc\u0131\u0301a.Dur\u00e1n et al\\.", "year": 2016}, {"title": "Efficient and Expressive Knowledge Base Completion Using Subgraph Feature Extraction", "author": ["Gardner", "Mitchell2015] Matt Gardner", "Tom Mitchell"], "venue": "In Proceedings of the 2015 Conference on Empirical Methods in Natural Language Process-", "citeRegEx": "Gardner et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Gardner et al\\.", "year": 2015}, {"title": "Semantically Smooth Knowledge Graph Embedding", "author": ["Guo et al.2015] Shu Guo", "Quan Wang", "Bin Wang", "Lihong Wang", "Li Guo"], "venue": "In Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th Interna-", "citeRegEx": "Guo et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Guo et al\\.", "year": 2015}, {"title": "Traversing Knowledge Graphs in Vector Space", "author": ["Guu et al.2015] Kelvin Guu", "John Miller", "Percy Liang"], "venue": "In Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing,", "citeRegEx": "Guu et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Guu et al\\.", "year": 2015}, {"title": "Learning to Represent Knowledge Graphs with Gaussian Embedding", "author": ["He et al.2015] Shizhu He", "Kang Liu", "Guoliang Ji", "Jun Zhao"], "venue": "In Proceedings of the 24th ACM International on Conference on Information and Knowledge Management,", "citeRegEx": "He et al\\.,? \\Q2015\\E", "shortCiteRegEx": "He et al\\.", "year": 2015}, {"title": "A latent factor model for highly multirelational data", "author": ["Nicolas L. Roux", "Antoine Bordes", "Guillaume R Obozinski"], "venue": "In Advances in Neural Information Processing Systems", "citeRegEx": "Jenatton et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Jenatton et al\\.", "year": 2012}, {"title": "Knowledge Graph Embedding via Dynamic Mapping Matrix", "author": ["Ji et al.2015] Guoliang Ji", "Shizhu He", "Liheng Xu", "Kang Liu", "Jun Zhao"], "venue": "In Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th In-", "citeRegEx": "Ji et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Ji et al\\.", "year": 2015}, {"title": "Knowledge Graph Completion", "author": ["Ji et al.2016] Guoliang Ji", "Kang Liu", "Shizhu He", "Jun Zhao"], "venue": null, "citeRegEx": "Ji et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Ji et al\\.", "year": 2016}, {"title": "Type-Constrained Representation Learning in Knowledge Graphs", "author": ["Krompa et al.2015] Denis Krompa", "Stephan Baier", "Volker Tresp"], "venue": "In Proceedings of the 14th International Semantic Web Conference,", "citeRegEx": "Krompa et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Krompa et al\\.", "year": 2015}, {"title": "DBpedia - A Large-scale", "author": ["Lehmann et al.2015] Jens Lehmann", "Robert Isele", "Max Jakob", "Anja Jentzsch", "Dimitris Kontokostas", "Pablo N. Mendes", "Sebastian Hellmann", "Mohamed Morsey", "Patrick van Kleef", "S\u00f6ren Auer", "Christian Bizer"], "venue": null, "citeRegEx": "Lehmann et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Lehmann et al\\.", "year": 2015}, {"title": "Learning Plausible Inferences from Semantic Web Knowledge by Combining Analogical Generalization with Structured Logistic Regression", "author": ["Liang", "Forbus2015] Chen Liang", "Kenneth D. Forbus"], "venue": "In Proceedings of the Twenty-Ninth AAAI Con-", "citeRegEx": "Liang et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Liang et al\\.", "year": 2015}, {"title": "Modeling Relation Paths for Representation Learning of Knowledge Bases", "author": ["Lin et al.2015a] Yankai Lin", "Zhiyuan Liu", "Huanbo Luan", "Maosong Sun", "Siwei Rao", "Song Liu"], "venue": "In Proceedings of the 2015 Conference on Empirical Methods in Nat-", "citeRegEx": "Lin et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Lin et al\\.", "year": 2015}, {"title": "Learning Entity and Relation Embeddings for Knowledge Graph Completion", "author": ["Lin et al.2015b] Yankai Lin", "Zhiyuan Liu", "Maosong Sun", "Yang Liu", "Xuan Zhu"], "venue": "In Proceedings of the Twenty-Ninth AAAI Conference on Artificial Intelligence Learn-", "citeRegEx": "Lin et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Lin et al\\.", "year": 2015}, {"title": "On the Limited Memory BFGS Method for Large Scale Optimization", "author": ["Liu", "Nocedal1989] D.C. Liu", "J. Nocedal"], "venue": "Mathematical Programming,", "citeRegEx": "Liu et al\\.,? \\Q1989\\E", "shortCiteRegEx": "Liu et al\\.", "year": 1989}, {"title": "Context-Dependent Knowledge Graph Embedding", "author": ["Luo et al.2015] Yuanfei Luo", "Quan Wang", "Bin Wang", "Li Guo"], "venue": "In Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing,", "citeRegEx": "Luo et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Luo et al\\.", "year": 2015}, {"title": "Linguistic Regularities in Continuous Space Word Representations", "author": ["Wen-tau Yih", "Geoffrey Zweig"], "venue": "In Proceedings of the 2013 Conference of the North American Chapter of the Association", "citeRegEx": "Mikolov et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Mikolov et al\\.", "year": 2013}, {"title": "WordNet: A Lexical Database for English", "author": ["George A. Miller"], "venue": "Communications of the ACM,", "citeRegEx": "Miller.,? \\Q1995\\E", "shortCiteRegEx": "Miller.", "year": 1995}, {"title": "Compositional Vector Space Models for Knowledge", "author": ["Benjamin Roth", "Andrew McCallum"], "venue": null, "citeRegEx": "Neelakantan et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Neelakantan et al\\.", "year": 2015}, {"title": "STransE: a novel embedding model of entities and relationships in knowledge bases", "author": ["Kairit Sirts", "Lizhen Qu", "Mark Johnson"], "venue": "In Proceedings of the 15th Annual Conference of the North American Chapter", "citeRegEx": "Nguyen et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Nguyen et al\\.", "year": 2016}, {"title": "A Three-Way Model for Collective Learning on Multi-Relational Data", "author": ["Volker Tresp", "Hans-Peter Kriegel"], "venue": "In Proceedings of the 28th International Conference on Machine Learning,", "citeRegEx": "Nickel et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Nickel et al\\.", "year": 2011}, {"title": "A Review of Relational Machine Learning for Knowledge Graphs", "author": ["Kevin Murphy", "Volker Tresp", "Evgeniy Gabrilovich"], "venue": "Proceedings of the IEEE,", "citeRegEx": "Nickel et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Nickel et al\\.", "year": 2015}, {"title": "Holographic Embeddings of Knowledge Graphs", "author": ["Lorenzo Rosasco", "Tomaso A. Poggio"], "venue": "In Proceedings of the Thirtieth AAAI Conference on Artificial Intelligence,", "citeRegEx": "Nickel et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Nickel et al\\.", "year": 2016}, {"title": "Glove: Global Vectors for Word Representation", "author": ["Richard Socher", "Christopher Manning"], "venue": "In Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing,", "citeRegEx": "Pennington et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Pennington et al\\.", "year": 2014}, {"title": "Reasoning With Neural Tensor Networks for Knowledge Base Completion", "author": ["Danqi Chen", "Christopher D Manning", "Andrew Ng"], "venue": "In Advances in Neural Information Processing Systems", "citeRegEx": "Socher et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Socher et al\\.", "year": 2013}, {"title": "YAGO: A Core of Semantic Knowledge", "author": ["Gjergji Kasneci", "Gerhard Weikum"], "venue": "In Proceedings of the 16th International Conference on World Wide Web,", "citeRegEx": "Suchanek et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Suchanek et al\\.", "year": 2007}, {"title": "Link Prediction in Relational Data", "author": ["Taskar et al.2004] Ben Taskar", "Ming fai Wong", "Pieter Abbeel", "Daphne Koller"], "venue": "In Advances in Neural Information Processing Systems", "citeRegEx": "Taskar et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Taskar et al\\.", "year": 2004}, {"title": "Compositional Learning of Embeddings for Relation Paths in Knowledge Bases and Text", "author": ["Xi Victoria Lin", "Wen tau Yih", "Hoifung Poon", "Chris Quirk"], "venue": "In Proceedings of the 54th Annual Meeting of the As-", "citeRegEx": "Toutanova et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Toutanova et al\\.", "year": 2016}, {"title": "Knowledge Base Completion via Search-based Question Answering", "author": ["West et al.2014] Robert West", "Evgeniy Gabrilovich", "Kevin Murphy", "Shaohua Sun", "Rahul Gupta", "Dekang Lin"], "venue": "In Proceedings of the 23rd International Conference on World", "citeRegEx": "West et al\\.,? \\Q2014\\E", "shortCiteRegEx": "West et al\\.", "year": 2014}, {"title": "Embedding Entities and Relations for Learning and Inference in Knowledge Bases", "author": ["Yang et al.2015] Bishan Yang", "Wen-tau Yih", "Xiaodong He", "Jianfeng Gao", "Li Deng"], "venue": "In Proceedings of the International Conference on Learning Representations", "citeRegEx": "Yang et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Yang et al\\.", "year": 2015}, {"title": "ADADELTA: An Adaptive Learning Rate Method. CoRR, abs/1212.5701", "author": ["Matthew D. Zeiler"], "venue": null, "citeRegEx": "Zeiler.,? \\Q2012\\E", "shortCiteRegEx": "Zeiler.", "year": 2012}], "referenceMentions": [{"referenceID": 23, "context": "Knowledge bases (KBs), such as WordNet (Miller, 1995), YAGO (Suchanek et al.", "startOffset": 39, "endOffset": 53}, {"referenceID": 31, "context": "Knowledge bases (KBs), such as WordNet (Miller, 1995), YAGO (Suchanek et al., 2007), Freebase (Bollacker et al.", "startOffset": 60, "endOffset": 83}, {"referenceID": 0, "context": ", 2007), Freebase (Bollacker et al., 2008) and DBpedia (Lehmann et al.", "startOffset": 18, "endOffset": 42}, {"referenceID": 16, "context": ", 2008) and DBpedia (Lehmann et al., 2015), represent relationships between entities as triples (head entity, relation, tail entity).", "startOffset": 20, "endOffset": 42}, {"referenceID": 30, "context": "Even very large knowledge bases are still far from complete (Socher et al., 2013; West et al., 2014).", "startOffset": 60, "endOffset": 100}, {"referenceID": 34, "context": "Even very large knowledge bases are still far from complete (Socher et al., 2013; West et al., 2014).", "startOffset": 60, "endOffset": 100}, {"referenceID": 27, "context": "Knowledge base completion or link prediction systems (Nickel et al., 2015) predict which triples not in a knowledge base are likely to be true (Taskar et al.", "startOffset": 53, "endOffset": 74}, {"referenceID": 32, "context": ", 2015) predict which triples not in a knowledge base are likely to be true (Taskar et al., 2004; Bordes et al., 2011).", "startOffset": 76, "endOffset": 118}, {"referenceID": 1, "context": ", 2015) predict which triples not in a knowledge base are likely to be true (Taskar et al., 2004; Bordes et al., 2011).", "startOffset": 76, "endOffset": 118}, {"referenceID": 2, "context": "performance (Bordes et al., 2012; Bordes et al., 2013; Socher et al., 2013; Wang et al., 2014; Guu et al., 2015; Nguyen et al., 2016) and generalize to large KBs (Krompa et al.", "startOffset": 12, "endOffset": 133}, {"referenceID": 3, "context": "performance (Bordes et al., 2012; Bordes et al., 2013; Socher et al., 2013; Wang et al., 2014; Guu et al., 2015; Nguyen et al., 2016) and generalize to large KBs (Krompa et al.", "startOffset": 12, "endOffset": 133}, {"referenceID": 30, "context": "performance (Bordes et al., 2012; Bordes et al., 2013; Socher et al., 2013; Wang et al., 2014; Guu et al., 2015; Nguyen et al., 2016) and generalize to large KBs (Krompa et al.", "startOffset": 12, "endOffset": 133}, {"referenceID": 10, "context": "performance (Bordes et al., 2012; Bordes et al., 2013; Socher et al., 2013; Wang et al., 2014; Guu et al., 2015; Nguyen et al., 2016) and generalize to large KBs (Krompa et al.", "startOffset": 12, "endOffset": 133}, {"referenceID": 25, "context": "performance (Bordes et al., 2012; Bordes et al., 2013; Socher et al., 2013; Wang et al., 2014; Guu et al., 2015; Nguyen et al., 2016) and generalize to large KBs (Krompa et al.", "startOffset": 12, "endOffset": 133}, {"referenceID": 15, "context": ", 2016) and generalize to large KBs (Krompa et al., 2015).", "startOffset": 36, "endOffset": 57}, {"referenceID": 10, "context": "Recently, several authors have addressed this issue by incorporating relation path information into model learning (Garc\u0131\u0301aDur\u00e1n et al., 2015; Lin et al., 2015a; Guu et al., 2015; Toutanova et al., 2016) and have shown that the relation paths between entities in KBs provide useful information and improve knowledge base completion.", "startOffset": 115, "endOffset": 203}, {"referenceID": 33, "context": "Recently, several authors have addressed this issue by incorporating relation path information into model learning (Garc\u0131\u0301aDur\u00e1n et al., 2015; Lin et al., 2015a; Guu et al., 2015; Toutanova et al., 2016) and have shown that the relation paths between entities in KBs provide useful information and improve knowledge base completion.", "startOffset": 115, "endOffset": 203}, {"referenceID": 3, "context": "We demonstrate its usefulness by applying it to the well-known TransE model (Bordes et al., 2013).", "startOffset": 76, "endOffset": 97}, {"referenceID": 2, "context": "However, it could be applied to other embedding models as well, such as Bilinear models (Bordes et al., 2012; Yang et al., 2015) and STransE (Nguyen et al.", "startOffset": 88, "endOffset": 128}, {"referenceID": 35, "context": "However, it could be applied to other embedding models as well, such as Bilinear models (Bordes et al., 2012; Yang et al., 2015) and STransE (Nguyen et al.", "startOffset": 88, "endOffset": 128}, {"referenceID": 25, "context": ", 2015) and STransE (Nguyen et al., 2016).", "startOffset": 20, "endOffset": 41}, {"referenceID": 3, "context": "1, and then describe the Neighborhood Mixture Model applied to the TransE model (Bordes et al., 2013) in section 2.", "startOffset": 80, "endOffset": 101}, {"referenceID": 0, "context": "Freebase (Bollacker et al., 2008), some entities, such as \u201cmale\u201d, can have thousands or millions neighboring entities sharing the same relation \u201cgender.", "startOffset": 9, "endOffset": 33}, {"referenceID": 3, "context": "TransE (Bordes et al., 2013) is a simple embedding model for knowledge base completion, which, despite of its simplicity, obtains very competitive results (Garc\u0131\u0301a-Dur\u00e1n et al.", "startOffset": 7, "endOffset": 28}, {"referenceID": 7, "context": ", 2013) is a simple embedding model for knowledge base completion, which, despite of its simplicity, obtains very competitive results (Garc\u0131\u0301a-Dur\u00e1n et al., 2016; Nickel et al., 2016).", "startOffset": 134, "endOffset": 183}, {"referenceID": 28, "context": ", 2013) is a simple embedding model for knowledge base completion, which, despite of its simplicity, obtains very competitive results (Garc\u0131\u0301a-Dur\u00e1n et al., 2016; Nickel et al., 2016).", "startOffset": 134, "endOffset": 183}, {"referenceID": 11, "context": "We applied the \u201cBernoulli\u201d trick to choose whether to generate the head or tail entity when sampling an incorrect triple (Wang et al., 2014; Lin et al., 2015b; He et al., 2015; Ji et al., 2015; Ji et al., 2016).", "startOffset": 121, "endOffset": 210}, {"referenceID": 13, "context": "We applied the \u201cBernoulli\u201d trick to choose whether to generate the head or tail entity when sampling an incorrect triple (Wang et al., 2014; Lin et al., 2015b; He et al., 2015; Ji et al., 2015; Ji et al., 2016).", "startOffset": 121, "endOffset": 210}, {"referenceID": 14, "context": "We applied the \u201cBernoulli\u201d trick to choose whether to generate the head or tail entity when sampling an incorrect triple (Wang et al., 2014; Lin et al., 2015b; He et al., 2015; Ji et al., 2015; Ji et al., 2016).", "startOffset": 121, "endOffset": 210}, {"referenceID": 5, "context": ", SGD, AdaGrad (Duchi et al., 2011), AdaDelta (Zeiler, 2012) or L-BFGS (Liu and Nocedal, 1989).", "startOffset": 15, "endOffset": 35}, {"referenceID": 36, "context": ", 2011), AdaDelta (Zeiler, 2012) or L-BFGS (Liu and Nocedal, 1989).", "startOffset": 18, "endOffset": 32}, {"referenceID": 2, "context": "The Unstructured model (Bordes et al., 2012) assumes that the head and tail entity vectors are similar.", "startOffset": 23, "endOffset": 44}, {"referenceID": 1, "context": "The Structured Embedding (SE) model (Bordes et al., 2011) extends the Unstructured model by assuming that the head and tail entities are similar only in a relation-dependent subspace, where each relation is represented by two different matrices.", "startOffset": 36, "endOffset": 57}, {"referenceID": 2, "context": "Futhermore, the SME model (Bordes et al., 2012) uses four different matrices to project entity and relation vectors into a subspace.", "startOffset": 26, "endOffset": 47}, {"referenceID": 13, "context": "TransD (Ji et al., 2015) and TransR/CTransR (Lin et al.", "startOffset": 7, "endOffset": 24}, {"referenceID": 25, "context": "STransE (Nguyen et al., 2016) and TranSparse (Ji et al.", "startOffset": 8, "endOffset": 29}, {"referenceID": 14, "context": ", 2016) and TranSparse (Ji et al., 2016) are extensions of the TransR model, where head and tail entities are associated with their own projection matrices.", "startOffset": 23, "endOffset": 40}, {"referenceID": 35, "context": "The DISTMULT model (Yang et al., 2015) is based on the Bilinear model (Nickel et al.", "startOffset": 19, "endOffset": 38}, {"referenceID": 26, "context": ", 2015) is based on the Bilinear model (Nickel et al., 2011; Bordes et al., 2012; Jenatton et al., 2012) where each relation is represented by a diagonal rather than a full matrix.", "startOffset": 39, "endOffset": 104}, {"referenceID": 2, "context": ", 2015) is based on the Bilinear model (Nickel et al., 2011; Bordes et al., 2012; Jenatton et al., 2012) where each relation is represented by a diagonal rather than a full matrix.", "startOffset": 39, "endOffset": 104}, {"referenceID": 12, "context": ", 2015) is based on the Bilinear model (Nickel et al., 2011; Bordes et al., 2012; Jenatton et al., 2012) where each relation is represented by a diagonal rather than a full matrix.", "startOffset": 39, "endOffset": 104}, {"referenceID": 30, "context": "The neural tensor network (NTN) model (Socher et al., 2013) uses a bilinear tensor operator to represent each relation.", "startOffset": 38, "endOffset": 59}, {"referenceID": 11, "context": "Similar quadratic forms are used to model entities and relations in KG2E (He et al., 2015) and TATEC (Garc\u0131\u0301a-Dur\u00e1n et al.", "startOffset": 73, "endOffset": 90}, {"referenceID": 7, "context": ", 2015) and TATEC (Garc\u0131\u0301a-Dur\u00e1n et al., 2016).", "startOffset": 18, "endOffset": 46}, {"referenceID": 18, "context": "Recently, Neelakantan et al. (2015), Gardner and Mitchell (2015), Luo et al.", "startOffset": 10, "endOffset": 36}, {"referenceID": 18, "context": "Recently, Neelakantan et al. (2015), Gardner and Mitchell (2015), Luo et al.", "startOffset": 10, "endOffset": 65}, {"referenceID": 16, "context": "(2015), Gardner and Mitchell (2015), Luo et al. (2015), Lin et al.", "startOffset": 37, "endOffset": 55}, {"referenceID": 15, "context": "(2015), Lin et al. (2015a), Garc\u0131\u0301a-Dur\u00e1n et al.", "startOffset": 8, "endOffset": 27}, {"referenceID": 6, "context": "(2015a), Garc\u0131\u0301a-Dur\u00e1n et al. (2015), Guu et al.", "startOffset": 9, "endOffset": 37}, {"referenceID": 6, "context": "(2015a), Garc\u0131\u0301a-Dur\u00e1n et al. (2015), Guu et al. (2015) and Toutanova et al.", "startOffset": 9, "endOffset": 56}, {"referenceID": 6, "context": "(2015a), Garc\u0131\u0301a-Dur\u00e1n et al. (2015), Guu et al. (2015) and Toutanova et al. (2016) showed that relation paths between entities in KBs provide richer information and improve the relationship prediction.", "startOffset": 9, "endOffset": 84}, {"referenceID": 22, "context": "in the path as pseudo-words applied Word2Vec algorithms (Mikolov et al., 2013) to produce pretrained vectors for these pseudo-words.", "startOffset": 56, "endOffset": 78}, {"referenceID": 6, "context": "RTransE (Garc\u0131\u0301a-Dur\u00e1n et al., 2015), PTransE (Lin et al.", "startOffset": 8, "endOffset": 36}, {"referenceID": 10, "context": ", 2015a) and TransE-COMP (Guu et al., 2015) are extensions of the TransE model.", "startOffset": 25, "endOffset": 43}, {"referenceID": 10, "context": "These models similarly represent a relation path by a vector which is the sum of the vectors of all relations in the path, whereas in the Bilinear-COMP model (Guu et al., 2015), each relation is a matrix and so it represents the relation path by matrix multiplication.", "startOffset": 158, "endOffset": 176}, {"referenceID": 16, "context": "Luo et al. (2015) showed that using these pre-trained vectors for initialization helps to improve the performance of the TransE, SME and SE models.", "startOffset": 0, "endOffset": 18}, {"referenceID": 6, "context": "RTransE (Garc\u0131\u0301a-Dur\u00e1n et al., 2015), PTransE (Lin et al., 2015a) and TransE-COMP (Guu et al., 2015) are extensions of the TransE model. These models similarly represent a relation path by a vector which is the sum of the vectors of all relations in the path, whereas in the Bilinear-COMP model (Guu et al., 2015), each relation is a matrix and so it represents the relation path by matrix multiplication. Our neighborhood mixture model can be adapted to both relation path models Bilinear-COMP and TransE-COMP, by replacing head and tail entity vectors by the neighborbased vector representations, thus combining advantages of both path and neighborhood information. Nickel et al. (2015) reviews other approaches for learning from KBs and multi-relational data.", "startOffset": 9, "endOffset": 689}, {"referenceID": 23, "context": "WN11 is derived from the large lexical KB WordNet (Miller, 1995) involving 11 relation types.", "startOffset": 50, "endOffset": 64}, {"referenceID": 0, "context": "FB13 is derived from the large real-world fact KB FreeBase (Bollacker et al., 2008) covering 13 relation types.", "startOffset": 59, "endOffset": 83}, {"referenceID": 4, "context": "(2015) for both triple classification and entity prediction tasks, containing 186 most frequent relations in the KB of the CMU Never Ending Language Learning project (Carlson et al., 2010).", "startOffset": 166, "endOffset": 188}, {"referenceID": 26, "context": "The two benchmark datasets1, WN11 and FB13, were produced by Socher et al. (2013) for triple classification.", "startOffset": 61, "endOffset": 82}, {"referenceID": 0, "context": "FB13 is derived from the large real-world fact KB FreeBase (Bollacker et al., 2008) covering 13 relation types. The NELL186 dataset2 was introduced by Guo et al. (2015) for both triple classification and entity prediction tasks, containing 186 most frequent relations in the KB of the CMU Never Ending Language Learning project (Carlson et al.", "startOffset": 60, "endOffset": 169}, {"referenceID": 30, "context": "Triple classification: The triple classification task was first introduced by Socher et al. (2013), and since then it has been used to evaluate various embedding models.", "startOffset": 78, "endOffset": 99}, {"referenceID": 30, "context": "Following Socher et al. (2013), the relation-specific thresholds are determined by maximizing the micro-averaged accuracy, which is a per-triple average, on the validation set.", "startOffset": 10, "endOffset": 31}, {"referenceID": 3, "context": "Entity prediction: The entity prediction task (Bordes et al., 2013) predicts the head or the tail entity given the relation type and the other entity, i.", "startOffset": 46, "endOffset": 67}, {"referenceID": 1, "context": "For the \u201cFiltered\u201d setting protocol described in Bordes et al. (2013), we also filter out before ranking any corrupted triples that appear in the KB.", "startOffset": 49, "endOffset": 70}, {"referenceID": 3, "context": ", the proportion of test triples for which the target entity was ranked in the top 10 predictions), which were originally used in the entity prediction task (Bordes et al., 2013), we also report the mean reciprocal rank (MRR), which is commonly used in information retrieval.", "startOffset": 157, "endOffset": 178}, {"referenceID": 13, "context": "Following the previous work (Wang et al., 2014; Lin et al., 2015b; Ji et al., 2015; He et al., 2015; Ji et al., 2016), we selected the margin hyper-parameter \u03b3 \u2208 {1, 2, 4} and the number of vector dimensions k \u2208 {20, 50, 100} on WN11 and FB13.", "startOffset": 28, "endOffset": 117}, {"referenceID": 11, "context": "Following the previous work (Wang et al., 2014; Lin et al., 2015b; Ji et al., 2015; He et al., 2015; Ji et al., 2016), we selected the margin hyper-parameter \u03b3 \u2208 {1, 2, 4} and the number of vector dimensions k \u2208 {20, 50, 100} on WN11 and FB13.", "startOffset": 28, "endOffset": 117}, {"referenceID": 14, "context": "Following the previous work (Wang et al., 2014; Lin et al., 2015b; Ji et al., 2015; He et al., 2015; Ji et al., 2016), we selected the margin hyper-parameter \u03b3 \u2208 {1, 2, 4} and the number of vector dimensions k \u2208 {20, 50, 100} on WN11 and FB13.", "startOffset": 28, "endOffset": 117}, {"referenceID": 9, "context": "On NELL186, we set \u03b3 = 1 and k = 50 (Guo et al., 2015; Luo et al., 2015).", "startOffset": 36, "endOffset": 72}, {"referenceID": 21, "context": "On NELL186, we set \u03b3 = 1 and k = 50 (Guo et al., 2015; Luo et al., 2015).", "startOffset": 36, "endOffset": 72}, {"referenceID": 13, "context": "TransD (Ji et al., 2015) 86.", "startOffset": 7, "endOffset": 24}, {"referenceID": 14, "context": "TranSparse-S (Ji et al., 2016) 86.", "startOffset": 13, "endOffset": 30}, {"referenceID": 14, "context": "TranSparse-US (Ji et al., 2016) 86.", "startOffset": 14, "endOffset": 31}, {"referenceID": 30, "context": "NTN (Socher et al., 2013) 70.", "startOffset": 4, "endOffset": 25}, {"referenceID": 11, "context": "KG2E (He et al., 2015) 85.", "startOffset": 5, "endOffset": 22}, {"referenceID": 10, "context": "Bilinear-COMP (Guu et al., 2015) 77.", "startOffset": 14, "endOffset": 32}, {"referenceID": 10, "context": "TransE-COMP (Guu et al., 2015) 80.", "startOffset": 12, "endOffset": 30}, {"referenceID": 30, "context": "Note that there are higher results reported for NTN (Socher et al., 2013), Bilinear-COMP (Guu et al.", "startOffset": 52, "endOffset": 73}, {"referenceID": 10, "context": ", 2013), Bilinear-COMP (Guu et al., 2015) and TransE-COMP when entity vectors are initialized by averaging the pre-trained word vectors (Mikolov et al.", "startOffset": 23, "endOffset": 41}, {"referenceID": 22, "context": ", 2015) and TransE-COMP when entity vectors are initialized by averaging the pre-trained word vectors (Mikolov et al., 2013; Pennington et al., 2014).", "startOffset": 102, "endOffset": 149}, {"referenceID": 29, "context": ", 2015) and TransE-COMP when entity vectors are initialized by averaging the pre-trained word vectors (Mikolov et al., 2013; Pennington et al., 2014).", "startOffset": 102, "endOffset": 149}, {"referenceID": 10, "context": ", 2013), Bilinear-COMP (Guu et al., 2015) and TransE-COMP when entity vectors are initialized by averaging the pre-trained word vectors (Mikolov et al., 2013; Pennington et al., 2014). It is not surprising as many entity names in WordNet and FreeBase are lexically meaningful. It is possible for all other embedding models to utilize the pre-trained word vectors as well. However, as pointed out by Wang et al. (2014) and Guu et al.", "startOffset": 24, "endOffset": 418}, {"referenceID": 10, "context": ", 2013), Bilinear-COMP (Guu et al., 2015) and TransE-COMP when entity vectors are initialized by averaging the pre-trained word vectors (Mikolov et al., 2013; Pennington et al., 2014). It is not surprising as many entity names in WordNet and FreeBase are lexically meaningful. It is possible for all other embedding models to utilize the pre-trained word vectors as well. However, as pointed out by Wang et al. (2014) and Guu et al. (2015), averaging the pre-trained word vectors for initializing entity vectors is an open problem and it is not always useful since entity names in many domain-specific KBs are not lexically meaningful.", "startOffset": 24, "endOffset": 440}, {"referenceID": 9, "context": "The first three rows present the best results reported in Guo et al. (2015), while the next three rows present the best results reported in Luo et al.", "startOffset": 58, "endOffset": 76}, {"referenceID": 9, "context": "The first three rows present the best results reported in Guo et al. (2015), while the next three rows present the best results reported in Luo et al. (2015). TransE-NMM obtains the highest triple classification accuracy, the best raw mean rank and the second highest raw Hits@10 on the entity prediction task in this comparison.", "startOffset": 58, "endOffset": 158}, {"referenceID": 10, "context": "Particularly, compared to the relation path model TransE-COMP (Guu et al., 2015), when model parameters were randomly initialized, TransE obtains 85.", "startOffset": 62, "endOffset": 80}, {"referenceID": 5, "context": "Particularly, compared to the relation path model TransE-COMP (Guu et al., 2015), when model parameters were randomly initialized, TransE obtains 85.2\u2212 80.3 = 4.9% absolute accuracy improvement on the WN11 dataset while achieving similar score on the FB13 dataset. Our high results of the TransE model are probably due to a careful grid search and using the \u201cBernoulli\u201d trick. Note that Lin et al. (2015b), Ji et al.", "startOffset": 63, "endOffset": 406}, {"referenceID": 5, "context": "Particularly, compared to the relation path model TransE-COMP (Guu et al., 2015), when model parameters were randomly initialized, TransE obtains 85.2\u2212 80.3 = 4.9% absolute accuracy improvement on the WN11 dataset while achieving similar score on the FB13 dataset. Our high results of the TransE model are probably due to a careful grid search and using the \u201cBernoulli\u201d trick. Note that Lin et al. (2015b), Ji et al. (2015) and Ji et al.", "startOffset": 63, "endOffset": 424}, {"referenceID": 5, "context": "Particularly, compared to the relation path model TransE-COMP (Guu et al., 2015), when model parameters were randomly initialized, TransE obtains 85.2\u2212 80.3 = 4.9% absolute accuracy improvement on the WN11 dataset while achieving similar score on the FB13 dataset. Our high results of the TransE model are probably due to a careful grid search and using the \u201cBernoulli\u201d trick. Note that Lin et al. (2015b), Ji et al. (2015) and Ji et al. (2016) did not report the TransE results used for initializing TransR, TransD and TranSparse, respectively.", "startOffset": 63, "endOffset": 445}, {"referenceID": 5, "context": "Particularly, compared to the relation path model TransE-COMP (Guu et al., 2015), when model parameters were randomly initialized, TransE obtains 85.2\u2212 80.3 = 4.9% absolute accuracy improvement on the WN11 dataset while achieving similar score on the FB13 dataset. Our high results of the TransE model are probably due to a careful grid search and using the \u201cBernoulli\u201d trick. Note that Lin et al. (2015b), Ji et al. (2015) and Ji et al. (2016) did not report the TransE results used for initializing TransR, TransD and TranSparse, respectively. They directly copied the TransE results previously reported in Wang et al. (2014). So it is difficult to determine exactly how much TransR, TransD and TranSparse gain over TransE.", "startOffset": 63, "endOffset": 628}, {"referenceID": 3, "context": "Furthermore, Garc\u0131\u0301a-Dur\u00e1n et al. (2015), Lin et al.", "startOffset": 13, "endOffset": 41}, {"referenceID": 3, "context": "Furthermore, Garc\u0131\u0301a-Dur\u00e1n et al. (2015), Lin et al. (2015a), Garc\u0131\u0301a-Dur\u00e1n et al.", "startOffset": 13, "endOffset": 61}, {"referenceID": 3, "context": "Furthermore, Garc\u0131\u0301a-Dur\u00e1n et al. (2015), Lin et al. (2015a), Garc\u0131\u0301a-Dur\u00e1n et al. (2016) and Nickel et al.", "startOffset": 13, "endOffset": 90}, {"referenceID": 3, "context": "Furthermore, Garc\u0131\u0301a-Dur\u00e1n et al. (2015), Lin et al. (2015a), Garc\u0131\u0301a-Dur\u00e1n et al. (2016) and Nickel et al. (2016) also showed that for entity prediction TransE obtains very competitive results which are much higher than the TransE results originally published in Bordes et al.", "startOffset": 13, "endOffset": 115}, {"referenceID": 1, "context": "(2016) also showed that for entity prediction TransE obtains very competitive results which are much higher than the TransE results originally published in Bordes et al. (2013).3", "startOffset": 156, "endOffset": 177}, {"referenceID": 3, "context": "We demonstrated its effect by extending TransE (Bordes et al., 2013) with our neighborhood mixture model.", "startOffset": 47, "endOffset": 68}], "year": 2017, "abstractText": "Knowledge bases are useful resources for many natural language processing tasks, however, they are far from complete. In this paper, we define a novel entity representation as a mixture of its neighborhood in the knowledge base and apply this technique on TransE\u2014a well-known embedding model for knowledge base completion. Experimental results show that the neighborhood information significantly helps to improve the results of the TransE, leading to better performance than obtained by other state-of-the-art embedding models on three benchmark datasets for triple classification, entity prediction and relation prediction tasks.", "creator": "LaTeX with hyperref package"}}}