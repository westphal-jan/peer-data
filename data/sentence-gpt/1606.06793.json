{"id": "1606.06793", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "22-Jun-2016", "title": "Scalable Semi-supervised Learning with Graph-based Kernel Machine", "abstract": "Owing to the prevalence of unlabeled data, semisupervised learning has recently drawn significant attention and has found applicable in many real-world applications. In this paper, we present the so-called Graph-based Semi-supervised Support Vector Machine (gS3VM), a method that leverages the excellent generalization ability of kernel-based method with the geometrical and distributive information carried in a spectral graph for semi-supervised learning purpose. The proposed gS3VM can be solved directly in the primal form using the Stochastic Gradient Descent method with the ideal convergence rate $O(\\frac{1}{T})$. Besides, our gS3VM allows the combinations of a wide spectrum of loss functions (e.g., Hinge, smooth Hinge, Logistic, L1, and {\\epsilon}-insensitive) and smoothness functions (i.e., $l_p(t) = |t|^p$ with $p\\ge1$). We note that the well-known Laplacian Support Vector Machine falls into the spectrum of gS3VM corresponding to the combination of the Hinge loss and the smoothness function $l_2(.)$. We further validate our proposed method on several benchmark datasets to demonstrate that gS3VM is appropriate for the large-scale datasets since it is optimal in memory used and yields superior classification accuracy whilst simultaneously achieving a significant computation speedup in comparison with the state-of-the-art baselines.\n\n\n\n\nFigure 5: A, R and S. P. S. P. S. P. S. P. S. P. S. P. S. P. S. P. S. P. S. P.\nThe GFSV approach for the technique, the GRAT, uses the Gaussian linear gradient estimator, which generates an appropriate linear curve to generate its Gaussian gradient, resulting in a more robust and precise statistical prediction of the gS3VM algorithm for Gaussian linear distribution in the real world. The GFSV model for GFSV is modeled as follows.\nFigure 5: A, R, S. P. S. P. S. P. S. P. S. P. S. P. S. P. S. P. S. P. S. P. S. P. S. P. S. P. S. P. S. P. S. P", "histories": [["v1", "Wed, 22 Jun 2016 00:26:59 GMT  (2658kb,D)", "http://arxiv.org/abs/1606.06793v1", "18 pages"], ["v2", "Tue, 6 Sep 2016 02:09:35 GMT  (8755kb)", "http://arxiv.org/abs/1606.06793v2", "15 pages"], ["v3", "Thu, 6 Apr 2017 02:40:23 GMT  (2731kb,D)", "http://arxiv.org/abs/1606.06793v3", "21 pages"]], "COMMENTS": "18 pages", "reviews": [], "SUBJECTS": "cs.LG", "authors": ["trung le", "khanh nguyen", "van nguyen", "vu nguyen", "dinh phung"], "accepted": false, "id": "1606.06793"}, "pdf": {"name": "1606.06793.pdf", "metadata": {"source": "CRF", "title": "Scalable Support Vector Machine for Semi-supervised Learning", "authors": ["Trung Le", "Khanh Nguyen", "Van Nguyen", "Vu Nguyen", "Dinh Phung"], "emails": [], "sections": [{"heading": null, "text": "( 1 T ) . Besides, our gS3VM allows the combinations of\na wide spectrum of loss functions (e.g., Hinge, smooth Hinge, Logistic, L1, and \u03b5-insensitive) and smoothness functions (i.e., lp (t) = |t|p with p \u2265 1). We note that the well-known Laplacian Support Vector Machine falls into the spectrum of gS3VM corresponding with the combination of the Hinge loss and the smoothness function l2 (.). We further validate our proposed method on several benchmark datasets to demonstrate that gS3VM is appropriate for the large-scale datasets since it is optimal in memory used and yields superior classification accuracy whilst simultaneously achieving a significant computation speedup in comparison with the state-of-the-art baselines\nIndex Terms\u2014Semi-supervised Learning, Kernel Method, Support Vector Machine, Spectral Graph, Stochastic Gradient Descent.\nI. INTRODUCTION\nSemi-supervised learning matters. Semi-supervised learning (SSL) aims at utilizing the intrinsic information carried in unlabeled data to enhance the generalization capacity of the learning algorithms. During the past decade, SSL has attracted significant attention and has found applicable in a variety of real-world problems including text categorization [1], image retrieval [2], bioinformatics [3], natural language processing [4] to name a few. While obtaining pre-defined labels is a labor-intensive and time-consuming process [5], ones have found that unlabeled data, when used in conjunction with a small amount of labeled data, can bring a remarkable improvement in classification accuracy [1].\nA notable approach to semi-supervised learning paradigm is to employ spectral graph in order to represent the adjacent and distributive information carried in data. Graph-based methods are nonparametric, discriminative, and transductive in nature. The typical graph-based methods include min-cut [6, 7], harmonic function [8], graph random walk [9, 10], spectral graph transducer [11, 12], and manifold regularization [13].\nInspired from the pioneering work of [1], recent works have attempted to incorporate kernel methods such as Support Vector Machine (SVM) [14] with the semi-supervised learning paradigm. The underlying idea of this research line is to solve standard SVM problem while treating the unknown labels as optimization variables [5]. This leads to a nonconvex optimization problem with a combinatorial explosion of label assignments. A wide spectrum of techniques have been proposed to solve this non-convex optimization problem, e.g., local combination search [1], gradient descent [15], continuation techniques [16], convex-concave procedures [17, 18], deterministic annealing [19, 20, 21], and semi-definite programming [22]. Although these works can somehow handle the combinatorial intractability, their common requirement to repeatedly retrain the model limits their applicability to realworld applications, hence lacking the ability to perform online learning for large-scale applications.\nConjoining the advantages of kernel method and the spectral graph theory, several existing works have tried to incorporate information carried in a spectral graph for building a better kernel function [23, 24, 25, 26, 27]. Basically, these methods employ the Laplacian matrix induced from the spectral graph to construct kernel functions which can capture the features of the ambient space. Manifold regularization framework [13] exploits the geometric information of the probability distribution that generates data and incorporates it as an additional regularization term. Two regularization terms are introduced to control the complexity of the classifier in the ambient space and the complexity induced from the geometric information of the distribution. However, the computational complexity for manifold regularization approach is cubic in the training size n (i.e., O ( n3 ) ). Hence other researches have been carried out to enhance the scalability of the manifold regularization framework [28, 29, 30]. Specifically, the work of [30] makes use of the preconditioned conjugate gradient to solve the optimization problem encountered in manifold regularization framework in the primal form. By that way, the computational complexity is reduced from O ( n3 ) to O ( n2 ) . However, this approach is to actually solve the optimization problem in the first dual layer instead of the primal form, hence rendering the solution infeasible for online setting. In addition, the LapSVM in primal approach [30] requires to store the entire Hessian matrix of size n \u00d7 n in the memory, resulting in a memory complexity of O(n2). Our evaluating experiments ar X iv :1 60 6. 06 79\n3v 1\n[ cs\n.L G\n] 2\n2 Ju\nn 20\n16\n2 with LapSVM in primal show that it always consumes a huge amount of memory in its execution (cf. Tables VII and VIII).\nRecently, stochastic gradient descent (SGD) methods [31, 32, 33] have emerged as a promising framework to speed up the training process and enable the online learning paradigm. SGD possesses three key advantages: (1) fast; (2) ability to run in online mode; and (3) economic in memory usage. In this paper, we leverage three knowledge domains of kernel method, spectral graph theory and stochastic gradient descent to propose a novel approach to semi-supervised learning, termed as Graph-based Semi-supervised Support Vector Machine (gS3VM). Our gS3VM allows the combination of a wide spectrum of loss functions (cf. Section V) and smoothness functions lp (.) where p \u2265 1 (cf. Eq. (5)). We note that the well-known Laplacian Support Vector Machine (LapSVM) [13, 30] is the special case of gS3VM(s) when using Hinge loss and the smoothness function l2 (.). We then develop a new algorithm based on the SGD framework [33] to directly solve the optimization problem of gS3VM in its primal form with the ideal convergence rate O ( 1 T ) . At each iteration, a labeled instance and an edge in the spectral graph are randomly sampled. As the result, the computational cost at each iteration is very economic and this makes the proposed method efficient to deal with large-scale datasets while maintaining comparable predictive performance.\nTo summarize, our contributions in this paper are as follows: \u2022 We provide a novel view of jointly learning the kernel-\nbased method with the spectral graph to propose gS3VM for semi-supervised learning purpose. Our proposed gS3VM enables the combination of a wide spectrum of convex loss functions and smoothness functions. \u2022 We apply stochastic gradient descent (SGD) framework [33] to solve directly gS3VM in its primal form. Hence, gS3VM has all advantageous properties of SGD-based methods including fast computation, memory efficiency, and ability to run in online setting. Inheriting from the strength of SGD-based method, our gS3VM can perform online learning for large-scale applications. To our best of knowledge, the proposed gS3VM is the first semisupervised learning method that can deal with the online learning context wherein data arrive continuously and sequentially. \u2022 We provide a theoretical analysis to show that gS3VM has the ideal convergence rate O ( 1 T ) if using the loss\nfunction l (w;x, y) satisfying \u2225\u2225\u2225l\u2032 (w;x, y)\u2225\u2225\u2225 \u2264 A for some A > 0 and the smoothness function lp (.) = |.|p with p \u2265 1. We verify that the necessary condition\u2225\u2225\u2225l\u2032 (w;x, y)\u2225\u2225\u2225 \u2264 A holds for a wide class of loss functions including Hinge, smooth Hinge, and Logistic for classification task and L1, \u03b5-insensitive for regression task (cf. Section V). \u2022 We validate our proposed method on several benchmark datasets. The experimental results empirically confirm the ideal convergence rate O ( 1 T ) of gS3VM and show\nthat gS3VM is really scalable for large-scale datasets. In particular, it offers a comparable classification accuracy whilst achieving a significant computational speed-up in\ncomparison with the state-of-the-art baselines.\nII. RELATED WORK\nWe review the work in semi-supervised learning paradigm that are closely related to ours. Graph-based semi-supervised learning is an active research topic under semi-supervised learning paradigm. At its crux, graph-based semi-supervised methods define a graph where the vertices are labeled and unlabeled data of the training set and edges (may be weighted) reflect the similarity of data. Most of graph-based methods can be interpreted as estimating the prediction function f such that: it should predict the labeled data as accurate as possible; and it should be smooth on the graph.\nIn [6, 7], semi-supervised learning problem is viewed as graph mincut problem. In the binary case, positive labels act as sources and negative labels act as sinks. The objective is to find a minimum set of edges whose removal blocks all flow from the sources to the sinks. Another way to infer the labels of unlabeled data is to compute the marginal probability of the discrete Markov random field. In [34], Markov Chain Monte Carlo sampling techniques is used to approximate this marginal probability. The work of [35] proposes to compute the marginal probabilities of the discrete Markov random field at any temperature with the Multi-canonical Monte Carlo method, which seems to be able to overcome the energy trap faced by the standard Metropolis or Swendsen-Wang method. The harmonic functions used in [8] is regarded as a continuous relaxation of the discrete Markov random field. It does relaxation on the value of the prediction function and makes use of the quadratic loss with infinite weight so that the labeled data are clamped. The works of [23, 24, 25, 26, 27] utilize the Laplacian matrix induced from the spectral graph to form kernel functions which can capture the features of the ambient space.\nYet another successful approach in semi-supervised learning paradigm is the kernel-based approach. The kernel-based semisupervised methods are primarily driven by the idea to solve a standard SVM problem while treating the unknown labels as optimization variables [5]. This leads to a non-convex optimization problem with a combination explosion of label assignments. Many methods have been proposed to solve this optimization problem, for example local combinatorial search [1], gradient descent [15], continuation techniques [16], convex-concave procedures [17, 18], deterministic annealing [19, 20, 21], and semi-definite programming [22]. However, the requirement of retraining the whole dataset over and over preludes the applications of these kernel-based semisupervised methods to the real-world datasets.\nSome recent works on semi-supervised learning have primarily concentrated on the improvements of its safeness and classification accuracy. Li et al. [2015] assumes that the lowdensity separators can be diverse and an incorrect selection may result in a reduced performance and proposes S4VM to use multiple low-density separators to approximate the ground-truth decision boundary. S4VM is shown to be safe and to achieve the maximal performance improvement under the low-density assumption of S3VM [1]. Wang et al. [2015]\n3 extends [13, 36] to propose semi-supervised discriminationaware manifold regularization framework which considers the discrimination of all available instances in learning of manifold regularization. Tan et al. [2014] proposes using the p-norm as a regularization quantity in manifold regularization framework to perform dimension reduction in the context of semi-supervised learning.\nThe closest work to ours is manifold regularization framework [13] and its extensions [28, 29, 30]. However, the original work of manifold regularization [13] requires to invert a matrix of size n by n which costs cubically and hence is not scalable. Addressing this issue, Tsang et al. [29] scales up manifold regularization framework by adding in an \u03b5-insensitive loss into the energy function, i.e., replacing \u2211 wij (f (xi)\u2212 f (xj))2 by \u2211 wij ( |f (xi)\u2212 f (xj)|\u03b5 )2 , where |z|\u03b5 = max {|z| \u2212 \u03b5, 0}. The intuition is that most pairwise differences |f (xi)\u2212 f (xj)| are very small. By ignoring differences smaller than \u03b5, the solution becomes sparser. LapSVM (in primal) [30] employs the preconditioned conjugate gradient to solve the optimization problem of manifold regularization in the primal form. This allows the computational complexity to be scaled up from O ( n3 ) to O ( n2 ) . However, the optimization problem in [30] is indeed solved in the first dual layer rather than in the primal form. In addition, we empirically find that the LapSVM in primal [30] is unsatisfactory in terms of memory complexity. In particular, this method always consumes a huge amount of memory for computation (cf. Tables VII and VIII).\nIII. SPECTRAL-GRAPH-BASED SEMI-SUPERVISED LEARNING\nA. Spectral Graph\nSpectral graph is an useful tool to capture the geometrical and distributive information carried in the data. It is usually an undirected graph whose vertices are the data instances. In the context of semi-supervised learning, we are given a training set X = Xl \u222a Xu where Xl = {(xi, yi)}li=1 is labeled data and Xu = {xi}l+ui=l+1 is unlabeled data. We can start with constructing the spectral graph G = (V, E) where the vertex set V includes all labeled and unlabeled instances (i.e., V = {xi}l+ui=1). An edge eij = xixj \u2208 E between two vertices xi, xj represents the similarity of the two instances. Let \u00b5ij be the weight of this eij . The principle is that if \u00b5ij is large, then yi, yj are expected to receive the same label. The set of edges G and its weighs can be built using the following ways: \u2022 Fully connected graph: every pair of vertices xi, xj is\nconnected by an edge. The edge weight decreases when the distance \u2016xi \u2212 xj\u2016 increases. The Gaussian kernel weight function widely used is given by\n\u00b5ij = e \u2212 \u2016xi\u2212xj\u2016\n2\n2\u03c32s\nwhere \u03c3s is known as the bandwidth parameter and controls how quickly the weight decreases. \u2022 k-NN: each vertex xi determines its k nearest neighbors (k-NN) and makes an edge with each of its k-NN. The Gaussian kernel weight function can be used for the edge\nFigure 1. Visualization of a spectral graph on 3D dataset using k-NN with k = 5.\nweight. Empirically, k-NN graphs with small k tend to perform well. \u2022 \u03b5-NN: we connect xi and xj if \u2016xi \u2212 xj\u2016 \u2264 \u03b5. Again the Gaussian kernel weight function can be used to weight the connected edges. In practice, \u03b5-NN graphs are easier to construct than k-NN graphs.\nIt is noteworthy that when constructing the spectral graph, we avoid connecting the edge of two labeled instances since we do not need to propagate the label between them. Figure 1 demonstrates an example of spectral graph constructed on 3D dataset using k-NN with k = 5.\nB. Label Propagation\nAfter building the spectral graph, a semi-supervised learning problem is formulated as assigning label to the unlabeled vertices. We need the mechanism to rationally propagate labels from the labeled vertices to the unlabeled ones. The idea is that if \u00b5ij is large, then the two labels yi, yj are expected to be the same.\nTo assign labels to the unlabeled instances, it is desirable to learn a mapping function f : X \u2212\u2192 Y where X and Y are domains of data and label, respectively, such that\n\u2022 f (xi) is as closest to its label yi as possible for all labeled instances xi (1 \u2264 i \u2264 l). \u2022 f should be smooth on the whole graph G, i.e., if xi is very close to xj (i.e., xi, xj are very similar or \u00b5ij is large), the discrepancy between fi and fj (i.e., |fi\u2212 fj |) is small.\nTherefore, we arrive at the following optimization problem\nmin f \u221e. l\u2211 i=1 (fi \u2212 yi)2 + \u2211 (i,j)\u2208E \u00b5ij (fi \u2212 fj)2  (1)\nwhere by convention we define \u221e.0 = 0 and fi = f (xi). The optimization problem in Eq. (1) peaks its minimum as the first term is exactly 0 and the second term is as smallest\n4 as possible. It is therefore rewritten as follows\nmin f  \u2211 (i,j)\u2208E \u00b5ij (fi \u2212 fj)2  (2)\ns.t. : \u2200li=1 : fi = yi\nTo extend the representation ability of the prediction function f , we relax the discrete function f to be real-valued. The drawback of the relaxation is that in the solution, f(x) is now real-valued and hence does not directly correspond to a label. This can however be addressed by thresholding f(x) at zero to produce discrete label predictions, i.e., if f(x) \u2265 0, predict y = 1, and if f(x) < 0, predict y = \u22121.\nIV. GRAPH-BASED SEMI-SUPERVISED SUPPORT VECTOR MACHINE (GS3VM)\nIn this section, we present our proposed Graph-based Semisupervised Support Vector Machine (gS3VM). We start this section with introducing the optimization problem of gS3VM. We then describe SGD-based solution for gS3VM. Finally, we present the convergence analysis of gS3VM.\nA. gS3VM Optimization Problem\nLet \u03a6 : X \u2212\u2192 H be a transformation from the input space X to a Reproducing Hilbert Kernel Space (RHKS) H. We use the function f (x) = wT\u03a6 (x) \u2212 \u03c1 = \u2211l+u i=1 \u03b1iK (xi, x) \u2212 \u03c1,\nwhere w = \u2211l+u i=1 \u03b1i\u03a6 (xi) and K (., .) is kernel function, to predict label. Inspired from the optimization problem in Eq. (2), the following optimization problem is proposed\nmin w 1 2 \u2016w\u20162 + C l l\u2211 i=1 \u03bei + C \u2032 |E| \u2211 (i,j)\u2208E \u00b5ij (fi \u2212 fj)2  (3)\ns.t. : \u2200li=1 : yi ( wT\u03a6 (xi)\u2212 \u03c1 ) \u2265 1\u2212 \u03bei\n\u2200li=1 : \u03bei \u2265 0\nwhere fi = wT\u03a6 (xi)\u2212 \u03c1. In the optimization problem in Eq. (3), we minimize 12 \u2016w\u2016 2 to maximize the margin for motivating the generalization capacity. At the same time, we minimize \u2211 (i,j)\u2208E \u00b5ij (fi \u2212 fj) 2 to make the prediction function smoother on the spectral graph. We rewrite the optimization problem in Eq. (3) in the primal form as follows1\nmin w 1 2 \u2016w\u20162 + C l l\u2211 i=1 l (w; zi) + C \u2032 |E| \u2211 (i,j)\u2208E \u00b5ij l2 ( wT\u03a6ij ) (4)\nwhere zi = (xi, yi), l (w;x, y) = max { 0, 1\u2212 ywT\u03a6 (x) }\n, \u03a6ij = \u03a6 (xi)\u2212 \u03a6 (xj), lp (t) = |t|p with t \u2208 R, and p \u2265 1.\nIn the optimization problem in Eq. (4), the minimization of \u2211l i=1 l (w;xi, yi) encourages the fitness of\ngS3VM on the labeled portion while the minimization of \u2211 (i,j)\u2208E \u00b5ij l2 ( wT\u03a6ij ) guarantees the smoothness of gS3VM on the spectral graph. Naturally, we can extend the optimization of gS3VM by replacing the Hinge loss by any\n1We can eliminate the bias \u03c1 by simply adjusting the kernel.\nloss functions (e.g., Logistic, L1) and l2 (.) by lp (.) with p \u2265 1. We achieve the following optimization problem\nmin w 1 2 \u2016w\u20162 + C l l\u2211 i=1 l (w; zi) + C \u2032 |E| \u2211 (i,j)\u2208E \u00b5ij lp ( wT\u03a6ij ) (5) where l (w;x, y) is any convex loss function, and lp (t) = |t|p with p \u2265 1.\nIt is noteworthy that Laplacian Support Vector Machine (LapSVM) [13, 30] is the special case of gS3VM using the Hinge loss with the smoothness function l2 (.).\nB. Stochastic Gradient Descent Algorithm for gS3VM\nWe employ the SGD framework proposed in [33] to solve the optimization problem in Eq. (5) in the primal form. Let us denote the objective function as\nJ (w) , \u2016w\u2016 2\n2 + C l l\u2211 i=1 l (w;xi, yi)+ C \u2032 |E| \u2211 (i,j)\u2208E \u00b5ij lp ( wT\u03a6ij ) At the iteration t, we do the following:\n\u2022 Uniformly sample a labeled instance xit (1 \u2264 it \u2264 l) from the labeled portion Xl and an edge (ut, vt) from the set of edges E . \u2022 Define the instantaneous objective function\nJt (w) = \u2016w\u20162\n2 +Cl (w;xit , yit)+C\n\u2032\u00b5utvt lp ( wT\u03a6utvt ) \u2022 Define the stochastic gradient gt\ngt = J \u2032 t (wt)\n= wt + Cl \u2032 (wt;xit , yit) + C \u2032\u00b5utvt l \u2032 p ( wTt \u03a6utvt ) where l \u2032 (w;x, y) specifies the derivative or sub-gradient\nw.r.t to w. \u2022 Update wt+1 where the learning rate \u03b7t = 2t+1\nwt+1 = wt \u2212 \u03b7tgt\n= t\u2212 1 t+ 1 wt \u2212 2C t+ 1 l \u2032 (wt;xit , yit) \u2212 2C \u2032\u00b5utvt t+ 1 l \u2032 p ( wTt \u03a6utvt\n) \u2022 Update wt+1as follows\nwt+1 = t\u2212 1 t+ 1 wt + 2 t+ 1 wt+1\nWe note that the derivative l \u2032\np\n( wT\u03a6 ) w.r.t w can be computed\nas\nl \u2032\np\n( wT\u03a6 ) = psign ( wT\u03a6 ) |wT\u03a6|p\u22121\u03a6\n5 Algorithm 1 Algorithm for gUS3VM. Input : C, C \u2032, p , K (., .)\n1: w1 = 0 2: w1 = 0 3: for t = 1 to T do 4: Uniformly sample it from {1, 2, ..., l} and (ut, vt) from the set of edges E 5: Update wt+1\nwt+1 = t\u2212 1 t+ 1 wt \u2212 2C t+ 1 l \u2032 (wt;xit , yit)\n\u2212 2C \u2032\u00b5utvt t+ 1 l \u2032 p ( wTt \u03a6utvt ) 6: Update wt+1\nwt+1 = t\u2212 1 t+ 1 wt + 2 t+ 1 wt+1\n7: end for Output : wT+1\nThe pseudocode of gS3VM is presented Algorithm 1. We note that we store wt and wt as wt = \u2211 i \u03b1i\u03a6 (xi) and wt =\u2211\ni \u03b2i\u03a6 (xi). In line 6 of Algorithm 1, the update of wt+1 involves the coefficients of \u03a6 (xit), \u03a6 (xut), and \u03a6 (xvt). In line 4 of Algorithm 1, we need to sample the edge (ut, vt) from the set of edges E and compute the edge weight \u00b5utvt . It is noteworthy in gS3VM we use the fully connected spectral graph to maximize the freedom of label propagation and avoid the additional computation incurred in other kind of spectral graph (e.g., k-NN or \u03b5-NN spectral graph). In addition, the edge weight \u00b5utvt can be computed on the fly when necessary.\nC. Convergence Analysis\nIn what follows, we present the convergence analysis for gS3VM. In particular, assuming that the loss function satisfies the condition: \u2225\u2225\u2225l\u2032 (w;x, y)\u2225\u2225\u2225 \u2264 A, \u2200x, y, we prove that our gS3VM achieves the ideal convergence rate O ( 1 T ) with 1 \u2264 p < 2 and with p \u2265 2 under some condition of the parameters (cf. Theorem 5). We present the theoretical results and the rigorous proofs are given in Appendix A. Without loss of generality, we assume that the feature map \u03a6 (x) is bounded in the feature space, i.e., \u2016\u03a6 (x)\u2016 = K (x, x)1/2 \u2264 R, \u2200x. We denote the optimal solution by w\u2217, i.e., w\u2217 = argmin\nw\nJ (w).\nThe following lemma shows the formula for wt from its recursive formula.\nLemma 1. We have the following statement\nwt = 2\nt (t\u2212 1) t\u22121\u2211 i=1 iwi\nLemma 2 offers the foundation to establish an upper bound for \u2016wt\u2016 in the next lemma.\nLemma 2. Let us consider the function f (x; a, b, p) = axp\u22121\u2212x+b where x \u2265 0 and p \u2265 1, a, b > 0. The following statements guarantee\ni) If p < 2 then f (M ; a, b, p) \u2264 0 where M = max ( 1, (a+ b) 1 2\u2212p )\n. ii) If p = 2 and a < 1 then f (M ; a, b, p) \u2264 0 where\nM = b1\u2212a .\niii) If p > 2 and abp\u22122 \u2264 (p\u22122) p\u22122\n(p\u22121)p\u22121 then f (M ; a, b, p) \u2264 0 where M = (\n1 (p\u22121)a\n) 1 p\u22122\n.\nLemma 3 establishes an upper bound on \u2016wt\u2016 which is used to define the bound in the next lemma.\nLemma 3. We have the following statement\n\u2016wt\u2016 \u2264M, \u2200t\nwhere M is defined as\nM =  max ( 1, (a+ b) 1 2\u2212p ) if p < 2 b 1\u2212a if p = 2, a < 1( 1\n(p\u22121)a\n) 1 p\u22122\nif p > 2, abp\u22122 \u2264 (p\u22122) p\u22122\n(p\u22121)p\u22121\nwith a = C \u2032 (2R)p p and b = CA.\nLemma 4 establishes an upper bound on \u2016gt\u2016 which is used in the main theorems.\nLemma 4. We have the following statement\n\u2016gt\u2016 \u2264 G, \u2200t\nwhere G = M + CA+ C \u2032 (2R)p pMp\u22121. The following theorem shows the convergence rate O (\n1 T ) of gS3VM.\nTheorem 5. Considering the running of Algorithm 1, the following statements hold\ni) E [J (wT+1)]\u2212 J (w\u2217) \u2264 2GT , \u2200T ii) E [ \u2016wT+1 \u2212w\u2217\u20162 ] \u2264 4GT , \u2200T\nif 1 \u2264 p < 2 or p = 2, a < 1 or p > 2, abp\u22122 \u2264 (p\u22122) p\u22122\n(p\u22121)p\u22121\nwhere a = C \u2032 (2R)p p and b = CA.\nTheorem 5 states the regret in the form of expectation. We go further to prove that for all T \u2265 T0 = \u2308 2G \u03b5\u03b4 \u2309 , with a high confidence level, J (wT+1) approximates the optimal value J (w\u2217) within an \u03b5-precision in Theorem 6.\nTheorem 6. With the probability at least 1 \u2212 \u03b4, for all T \u2265 T0 = \u2308 2G \u03b5\u03b4 \u2309 , J (wT+1) approximates the optimal value J (w\u2217) within an \u03b5-precision, i.e., J (wT+1) \u2264 J (w\u2217) + \u03b5 if 1 \u2264 p < 2 or p = 2, a < 1 or p > 2, abp\u22122 \u2264 (p\u22122) p\u22122\n(p\u22121)p\u22121\nwhere a = C \u2032 (2R)p p and b = CA.\nV. SUITABILITY OF LOSS FUNCTION AND KERNEL FUNCTION\nIn this section, we present the suitability of the loss functions and kernel functions that can be used for gS3VM. We verify that most of the well-known loss functions satisfying the necessary condition:\n\u2225\u2225\u2225l\u2032 (w;x, y)\u2225\u2225\u2225 \u2264 A for an appropriate positive number A.\n6 \u2022 Hinge loss:\nl (w;x, y) = max { 0, 1\u2212 ywT\u03a6 (x) }\nl \u2032 (w;x, y) = \u2212I{ywT\u03a6(x)\u22641}y\u03a6 (x)\nTherefore, by choosing A = R we have\u2225\u2225\u2225l\u2032 (w;x, y)\u2225\u2225\u2225 \u2264 \u2016\u03a6 (x)\u2016 \u2264 R = A \u2022 Smooth Hinge loss [37]\nl (w;x, y) =  0 if yo > 1 1\u2212 yw>\u03a6 (x)\u2212 \u03c42 if yo < 1\u2212 \u03c4 1 2\u03c4 (1\u2212 yo) 2 otherwise\nl \u2032 (w;x, y) = \u2212I{yo<1\u2212\u03c4}y\u03a6 (x)\n+ \u03c4\u22121I1\u2212\u03c4\u2264yo\u22641 (yo\u2212 1) y\u03a6 (x)\nwhere o = w>\u03a6 (x). Therefore, by choosing A = R we have\u2225\u2225\u2225l\u2032 (w;x, y)\u2225\u2225\u2225 = \u2223\u2223I{yo<1\u2212\u03c4}\u2223\u2223R\n+ \u2223\u2223\u03c4\u22121I1\u2212\u03c4\u2264yo\u22641 (yo\u2212 1)\u2223\u2223R \u2264 \u2223\u2223I{yo<1\u2212\u03c4}\u2223\u2223R + \u03c4\u22121\u03c4 |I1\u2212\u03c4\u2264yo\u22641|R \u2264 R = A\n\u2022 Logistic loss: l (w;x, y) = log ( 1 + exp ( \u2212ywT\u03a6 (x) )) l \u2032 (w;x, y) = \u2212y exp ( \u2212ywT\u03a6 (x) ) \u03a6 (x)\nexp (\u2212ywT\u03a6 (x)) + 1 Therefore, by choosing A = R we have\u2225\u2225\u2225l\u2032 (w;x, y)\u2225\u2225\u2225 < \u2016\u03a6 (x)\u2016 \u2264 R = A\n\u2022 L1 loss:\nl (w;x, y) = |y \u2212wT\u03a6 (x) | l \u2032 (w;x, y) = sign ( wT\u03a6 (x)\u2212 y ) \u03a6 (x)\nTherefore, by choosing A = R we have\u2225\u2225\u2225l\u2032 (w;x, y)\u2225\u2225\u2225 \u2264 \u2016\u03a6 (x)\u2016 \u2264 R = A \u2022 \u03b5-insensitive loss:\nl (w;x, y) = max { 0, |y \u2212wT\u03a6 (x) | \u2212 \u03b5 }\nl \u2032 (w;x, y) = I{|y\u2212wT\u03a6(x)|>\u03b5}sign ( wT\u03a6 (x)\u2212 y ) \u03a6 (x)\nTherefore, by choosing A = R we have\u2225\u2225\u2225l\u2032 (w;x, y)\u2225\u2225\u2225 \u2264 \u2016\u03a6 (x)\u2016 \u2264 R = A Here IS is the indicator function which is equal to 1 if the statement S is true and 0 if otherwise.\nIt can be observed that the positive constant A coincides the radius R (i.e., A = R) for the aforementioned loss functions. To allow the ability to flexibly control the minimal sphere that encloses all \u03a6 (x)(s), we propose to use the squared exponential kernel function which is given by\nK (x, x\u2032) = \u03c32fe \u2212\u2016x\u2212x \u2032\u20162 2\u03c32 l (6)\nwhere \u03c3l is the length-scale parameter and \u03c3f is the output variance parameter.\nIt appears that using the kernel in Eq. (6) , we have the following equality\n\u2016\u03a6 (x)\u2016 = K (x, x)1/2 = \u03c3f \u2264 R\nRecall that if p = 2 or p > 2, Algorithm 1 converges to the optimal solution with the ideal convergence rate O ( 1 T ) under specific conditions. In particular, with p = 2 the corresponding condition is a < 1 and with p > 2 the corresponding condition is abp\u22122 \u2264 (p\u22122) p\u22122\n(p\u22121)p\u22121 where a = C \u2032 (2R) p p and b = CA.\nUsing the squared exponential kernel function in Eq. (6), we can adjust the output variance parameter \u03c3f to make the convergent condition valid. More specifically, we consider two cases: \u2022 p = 2: the convergent condition is derived as follows\na < 1\nC \u2032 (2R) p p < 1\nR < 1\n2 (pC \u2032)\n\u22121/p\nWe can simply choose the output variance parameter \u03c3f as\n\u03c3f = R = 1\n2 (pC \u2032) \u22121/p \u2212 \u03c1\n= 1\n2 (2C \u2032) \u22121/2 \u2212 \u03c1 = 1 2 \u221a 2C \u2032 \u2212 \u03c1\nwhere \u03c1 > 0 is a very small number. \u2022 p > 2: the convergent condition is derived as follows\nabp\u22122 \u2264 (p\u2212 2) p\u22122\n(p\u2212 1)p\u22121\nC \u2032 (2R) p p (CA) p\u22122 \u2264 (p\u2212 2) p\u22122\n(p\u2212 1)p\u22121\nC \u2032 (2R) p p (CR) p\u22122 \u2264 (p\u2212 2) p\u22122\n(p\u2212 1)p\u22121\nR2p\u22122 \u2264 (p\u2212 2) p\u22122\n2pCp\u22122C \u2032p (p\u2212 1)p\u22121\nR \u2264\n( (p\u2212 2)p\u22122\n2pCp\u22122C \u2032p (p\u2212 1)p\u22121\n) 1 2p\u22122\nWe can simply choose the output variance parameter \u03c3f as\n\u03c3f = R =\n( (p\u2212 2)p\u22122\n2pCp\u22122C \u2032p (p\u2212 1)p\u22121\n) 1 2p\u22122\nIn case we wish to use the popular radial basic function (RBF) kernel which is given by\nK (x, x\u2032) = e \u2212\u2016x\u2212x \u2032\u20162 2\u03c32 l\n, we can control the second trade-off parameter C \u2032 so as to ensure the ideal convergence rate O ( 1 T ) with p \u2265 2. More specifically, we also consider two cases:\n7 \u2022 p = 2: the convergent condition is derived as follows\na < 1\nC \u2032 < 1\n(2R) p p\n= 1\n8 (7)\n\u2022 p > 2: the convergent condition is derived as follows\nabp\u22122 \u2264 (p\u2212 2) p\u22122\n(p\u2212 1)p\u22121\nC \u2032 (2R) p p (CA) p\u22122 \u2264 (p\u2212 2) p\u22122\n(p\u2212 1)p\u22121\nC \u2032 (2R) p p (CR) p\u22122 \u2264 (p\u2212 2) p\u22122\n(p\u2212 1)p\u22121\nC \u2032 \u2264 (p\u2212 2) p\u22122\n2pCp\u22122p (p\u2212 1)p\u22121 (8)\nVI. EXPERIMENTS\nWe conduct the extensive experiments to investigate the influence of the parameters and factors to the model behavior and to compare our proposed gS3VM with the state-of-the-art baselines on the benchmark datasets. In particular, we design three kinds of experiment to analyze the influence of factors, e.g., the loss function l (w;x, y), the smoothness function lp ( wT\u03a6 ) , and the percentage of unlabeled portion to the model behavior. In the first experiment (cf. Section VI-C1), we empirically prove the theoretical convergence rate O ( 1 T\n) for all combinations of loss function (Hinge, Logistic) and smoothness function (p = 1, 2, 3) and we also investigate how the number of iterations affects the classification accuracy. In the second experiment (cf. Section VI-C2), we study the influence of various combinations of the loss function and the smoothness function to the quality of prediction and the training time when the percentage of unlabeled portion is either 80% or 90%. In the third experiment (cf. Section VI-C3), we examine the proposed method under the semisupervised setting where the proportion of unlabeled data is varied from 50% to 90%. Finally, we compare our proposed gS3VM with the state-of-the-art baselines on the benchmark datasets.\nA. Experimental Datasets\nWe conduct the experiments on 13 benchmark datasets2 for semi-supervised learning. The statistics of the experimental datasets are given in Table I.\nB. Baselines\nIn order to investigate the efficiency and accuracy of our proposed method (gUS3VM), we made comparison with the following baselines: \u2022 LapSVM in primal [30]: Laplacian Support Vector Ma-\nchine in primal is a state-of-the-art approach in semisupervised classification based on manifold regularization framework. It can reduce the computational complexity\n2Most of the experimental datasets can be conveniently downloaded from the URL https://www.csie.ntu.edu.tw/\u02dccjlin/libsvmtools/datasets/.\nDataset Size Dimension G50C 551 50 COIL20 145 1,014 USPST 601 256\nAUSTRALIAN 690 14 A1A 1,605 123 MUSHROOMS 8,124 112 SVMGUIDE3 1,243 21 SVMGUIDE1 3,089 4\nW5A 9,888 300 W8A 49,749 300 COD-RNA 59,535 8 IJCNN1 49,990 22 COVTYPE 581,012 54\nTable I THE STATISTICS OF EXPERIMENTAL DATASETS.\nof the original LapSVM [13] from O ( n3 ) to O ( kn2 ) using the preconditioned conjugate gradient and an early stopping strategy. \u2022 CCCP [18]: An approach is proposed to solve the nonconvex optimization problem occurred in the kernel semisupervised approach using convex-concave procedures. \u2022 Self-KNN: Self-training is one of the most classical technique using in semi-supervised classification. SelfKNN employs k\u2212NN method as a core classifier for self training. \u2022 SVM: Support Vector Machine which is implemented using LIBSVM solver [38] and trained with fully label setting. We use fully labeled SVM as a milestone to judge how good the semi-supervised methods are.\nAll compared methods are run on a Windows computer with the configuration of CPU Xeon 3.47 GHz and 96GB RAM. All codes of the baselines are achieved from the corresponding authors.\nC. Impact of Incident Factors to The Learning Behavior\n1) Simulation Study on Convergence Rate: In this simulation study, we want to empirically examine the convergence rate of gS3VM with various combinations of loss function (Hinge, Logistic) and smooth function (p = 1, 2, 3). We select two datasets G50C and USPST. For each dataset, we compute the quantity \u2206J T = (J (wT+1)\u2212 J (w\u2217))\u2217T and measure the accuracy when the number of iterations T is varied. We repeated each experiment five times to record the necessary quantities and their standard deviations. As observed from Figures 2 and 3, \u2206J T tends to decrease and when T is sufficiently large, this quantity is lower-bounded by a constant. Hence, we can empirically conclude the convergence rate O (\n1 T\n) of gS3VM. This conclusion also matches with the\ntheoretical analysis developed in Section IV-C. It is noteworthy that in this simulation study we use the RBF kernel and with p = 2 and p = 3 the second trade-off parameter C \u2032 is selected using Eqs. (7, 8) so as to theoretically guarantee the ideal convergence rate O ( 1 T ) of our gS3VM.\n2) Simulation Study on Influence of Loss Function and Smoothness Function to The Learning Performance : In this simulation study, we answer the question how the variation in loss function and smoothness function affects the learning\n8 g50c dataset (Hinge loss; p = 2)\ng50c dataset (Hinge loss; p = 3)\ng50c dataset (Hinge loss; p = 1)\n10\n11\nperformance on the real datasets. We experiment on the real datasets given in Table I with different combinations of loss function (Hinge, Logistic) and smoothness function (p = 1, 2, 3). Each experiment is performed five times and the average accuracy, F1 score, precision, and training time corresponding to 80% or 90% of hidden label are reported in Tables IV, III, IV, and V. We observe that the Hinge loss is slightly better than Logistic one and the combination of Hinge loss and the smoothness l1 (.) is slightly better than other combinations. It is noteworthy that in this simulation study we use the RBF kernel and with p = 2 and p = 3 the second trade-off parameter C \u2032 is selected using Eqs. (7, 8) so as to theoretically guarantee the ideal convergence rate O ( 1 T ) of our gS3VM. 3) Simulation Study on Influence on Percentage of Hidden Label to The Learning Performance : In this simulation study, we address the question how variation in percentage of hidden label influences the learning performance. We also experiment on the real datasets given in Table I with the various percentages of hidden label varied in the grid\n14\n{50%, 60%, 70%, 80%, 90%}. We observe that when the percentage of hidden label increases, the classification accuracy and the F1 score tend to decrease across the datasets except for two datasets COIL20 and MUSHROOMS which remain fairy stable (cf. Table VI and Figure 4 (left and right)). This observation is reasonable since when increasing the percentage of hidden label, we decrease the amount of information label provided to the classifier and hence make the label propagation more challenging.\nD. Experimental Results on The Benchmark Datasets\nIn this experiment, we compare our proposed method with the baselines, including LapSVM, CCCP, Self-KNN and SVM as described in VI-B. Based on the observation from the experiment in VI-C2, we use the combination of Hinge loss and the smoothness function l1 (.). Besides offering the best predictive performance, this combination also encourages the sparsity in the output solution.\nThe standard RBF kernel given by K ( x, x \u2032 ) =\ne \u2212\u03b3 \u2225\u2225\u2225x\u2212x\u2032\u2225\u2225\u22252 = e \u2212 \u2225\u2225\u2225\u2225x\u2212x\u2032 \u2225\u2225\u2225\u22252 2\u03c32 l is used in the experiments. With LapSVM, we use the parameter settings proposed in [30], wherein the parameters \u03b3A and \u03b3I are searched in the grid { 10\u22126, 10\u22124, 10\u22122, 10\u22121, 1, 10, 100 } . In all experiments with the LapSVM, we make use of the preconditioned conjugate gradient version, which seems more suitable for the LapSVM optimization problem [30]. With CCCP-TSVM, we use the setting CCCP-TSVM|s=0UC\u2217=LC . Only two parameters need to be tuned are the trade-off C and the width of kernel \u03b3. Akin to our proposed gS3VM, the trade-off parameters C \u2032 = C is tuned in the grid { 2\u22125, 2\u22123, . . . , 23, 25 } and the\nwidth of kernel \u03b3 is varied in the grid { 2\u22125, 2\u22123, . . . , 23, 25 }\n. In our proposed gS3VM, the bandwidth \u03c3s of Gaussian kernel weight function, which involves in computing the weights of the spectral graph, is set so as to \u03c3s = \u03c3l. We run crossvalidation with 5 folds. The optimal parameter set is selected according to the highest classification accuracy. We set the number of iterations T in gS3VM to 0.2\u00d7(l + u) for the largescale datasets such as MUSHROOMS, W5A, W8A, CODRNA, COVTYPE and IJCNN1 and to l+u for the remaining datasets. Each experiment is carried out five times to compute the average of the reported measures.\nWe measure the accuracy, F1 score, training time, and memory amount used in training when the percentage of hidden label is 80% and 90%. These measures are reported in Tabled VII and VIII. To improve the readability, in these two tables we emphasize the best method (not count the fulllabeled SVM) for each measure using boldface, italicizing, or underlining. Regarding the classification accuracy and the F1 score, it can be seen that gS3VM are comparable with LapSVM and CCCP while being much better than Self-KNN. Particularly, CCCP seems to be the best method on 80% unlabeled dataset (cf. Figure 5 (left) and Figure 6 (left)) while gS3VM a little outperforms others on 90% unlabeled datasets (cf. Figure 5 (right) and Figure 6 (right)). Comparing with the full-labeled SVM, except for three datasets IJCNN1, COD-RN, and COVTYPE, gS3VM produces the comparable\nclassification accuracies. Remarkably for the computational time, gS3VM outperforms the baselines by a wide margin especially for the large-scale datasets. On the large-scale datasets W8A, COD-RNA, IJCNN1, and COVTYPE, gS3VM is significantly tens of times faster than LapSVM, the second fastest method (cf. Figure 7 (left) and Figure 7 (right)). We also examine the memory consumption in training for each method. It can be observed that gS3VM is also economic in terms of memory amount used in training especially for the large-scale datasets (cf. Figure 8). In contrast, LapSVM always consumes a huge amount of memory during its training.\nVII. CONCLUSION\nIn this paper, we present the novel framework for semisupervised learning, called Graph-based Semi-supervised Support Vector Machine (gS3VM). Our framework connects three domains of kernel method, spectral graph, and stochastic gradient descent at the right place. Our gS3VM can be solved directly in the primal form with the ideal convergence rate O (\n1 T\n) and naturally inherits all strengths of a SGD-based\nmethod. We validate and compare gS3VM with other stateof-the-art methods under semi-supervised learning context on several benchmark datasets. The experimental results demonstrate that our proposed gS3VM offers comparable classification accuracy and is economic in memory amount used whilst achieving a significant speed-up comparing with the state-ofthe-art baselines. Moreover, our approach is the first semisupervised model offering the online setting that is essential in real-world application.\n15\nREFERENCES\n[1] T. Joachims, \u201cTransductive inference for text classification using support vector machines,\u201d in International Conference on Machine Learning (ICML), Bled, Slowenien, 1999, pp. 200\u2013209. [2] L. Wang, K. L. Chan, and Z. Zhang, \u201cBootstrapping svm active learning by incorporating unlabelled images for image retrieval.\u201d IEEE Computer Society, 2003, pp. 629\u2013634. [3] N. Kasabov, D. Zhang, and P. Pang, \u201cIncremental learning in autonomous systems: evolving connectionist systems for on-line image and speech recognition,\u201d in Advanced Robotics and its Social Impacts, 2005. IEEE Workshop on, june 2005, pp. 120 \u2013 125. [4] C. Goutte, H. De\u0301jean, E. Gaussier, N. Cancedda, and J.M. Renders, \u201cCombining labelled and unlabelled data: A case study on fisher kernels and transductive inference for biological entity recognition,\u201d in Proceedings of the 6th Conference on Natural Language Learning - Volume 20, ser. COLING-02, 2002, pp. 1\u20137. [5] O. Chapelle, V. Sindhwani, and S. Keerthi, \u201cOptimization techniques for semi-supervised support vector machines,\u201d Journal of Machine Learning Research, vol. 9, pp. 203\u2013 233, Jun. 2008. [6] A. Blum and S. Chawla, \u201cLearning from labeled and unlabeled data using graph mincuts,\u201d in Proceedings of the Eighteenth International Conference on Machine Learning, ser. ICML \u201901, 2001, pp. 19\u201326. [7] A. Blum, J. D. Lafferty, M. R. Rwebangira, and R. Reddy, \u201cSemi-supervised learning using randomized mincuts.\u201d in ICML, vol. 69, 2004. [8] X. Zhu, Z. Ghahramani, and J. D. Lafferty, \u201cSemisupervised learning using gaussian fields and harmonic functions,\u201d in IN ICML, 2003, pp. 912\u2013919. [9] M. Szummer and T. Jaakkola, \u201cPartially labeled classification with markov random walks,\u201d in Advances in Neural Information Processing Systems. MIT Press, 2002, pp. 945\u2013952. [10] A. Azran, \u201cThe rendezvous algorithm: Multiclass semisupervised learning with markov random walks,\u201d in Proceedings of the 24th International Conference on Machine Learning, ser. ICML \u201907, 2007, pp. 49\u201356. [11] T. Joachims, \u201cTransductive learning via spectral graph partitioning,\u201d in In ICML, 2003, pp. 290\u2013297. [12] P. Duong, V. Nguyen, M. Dinh, T. Le, D. Tran, and W. Ma, \u201cGraph-based semi-supervised support vector data description for novelty detection,\u201d in 2015 International Joint Conference on Neural Networks (IJCNN), July 2015, pp. 1\u20136. [13] M. Belkin, P. Niyogi, and V. Sindhwani, \u201cManifold regularization: A geometric framework for learning from labeled and unlabeled examples,\u201d J. Mach. Learn. Res., vol. 7, pp. 2399\u20132434, Dec. 2006. [14] C. Cortes and V. Vapnik, \u201cSupport-vector networks,\u201d in Machine Learning, 1995, pp. 273\u2013297. [15] O. Chapelle and A. Zien. (2005) Semi-Supervised Classification by Low Density Separation.\n[16] O. Chapelle, M. Chi, and A. Zien, \u201cA continuation method for semi-supervised svms,\u201d in Proceedings of the 23rd international conference on Machine learning, ser. ICML \u201906. ACM, 2006, pp. 185\u2013192. [17] G. Fung and O. Mangasarian, \u201cSemi-supervised support vector machines for unlabeled data classification,\u201d Optimization Methods and Software, vol. 15, pp. 29\u201344, 2001. [18] R. Collobert, F. Sinz, J. Weston, L. Bottou, and T. Joachims, \u201cLarge scale transductive svms,\u201d Journal of Machine Learning Research, 2006. [19] V. Sindhwani, S. Keerthi, and O. Chapelle, \u201cDeterministic annealing for semi-supervised kernel machines,\u201d in Proceedings of the 23rd international conference on Machine learning, ser. ICML \u201906, 2006, pp. 841\u2013848. [20] T. Le, D. Tran, T. Tran, K. Nguyen, and W. Ma, \u201cFuzzy entropy semi-supervised support vector data description,\u201d in 2013 International Joint Conference on Neural Networks (IJCNN), Aug 2013, pp. 1\u20135. [21] V. Nguyen, T. Le, T. Pham, M. Dinh, and T. H. Le, \u201cKernel-based semi-supervised learning for novelty detection,\u201d in 2014 International Joint Conference on Neural Networks (IJCNN), July 2014, pp. 4129\u20134136. [22] T. De Bie and N. Cristianini, \u201cSemi-supervised learning using semi-definite programming,\u201d in Semi-supervised Learning, Cambridge, MA, 2006. [23] O. Chapelle, J. Weston, and B. Scho\u0308lkopf, \u201cCluster kernels for semi-supervised learning,\u201d in Advances in Neural Information Processing Systems 15, S. Becker, S. Thrun, and K. Obermayer, Eds. MIT Press, 2003, pp. 601\u2013608. [24] C. Kemp, T. L. Griffiths, S. Stromsten, and J. B. Tenenbaum, \u201cSemi-supervised learning with trees,\u201d in Advances in Neural Information Processing Systems 16, S. Thrun, L. Saul, and B. Scho\u0308lkopf, Eds., 2004, pp. 257\u2013264. [25] R. I. Kondor and J. D. Lafferty, \u201cDiffusion kernels on graphs and other discrete input spaces.\u201d Morgan Kaufmann Publishers Inc., 2002, pp. 315\u2013322. [26] A. J. Smola and I. R. Kondor, \u201cKernels and regularization on graphs.\u201d in Proceedings of the Annual Conference on Computational Learning Theory, 2003. [27] X. Zhu, J. S. Kandola, Z. Ghahramani, and J. D. Lafferty, \u201cNonparametric transforms of graph kernels for semisupervised learning.\u201d 2004. [28] V. Sindhwani and P. Niyogi, \u201cLinear manifold regularization for large scale semi-supervised learning,\u201d in Proc. of the 22nd ICML Workshop on Learning with Partially Classified Training Data, 2005. [29] I. W. Tsang and J. T. Kwok, \u201cLarge-scale sparsified manifold regularization.\u201d MIT Press, 2006, pp. 1401\u2013 1408. [30] S. Melacci and M. Belkin, \u201cLaplacian support vector machines trained in the primal,\u201d J. Mach. Learn. Res., vol. 12, Jul. 2011. [31] S. Shalev-shwartz and Y. Singer, \u201cLogarithmic regret algorithms for strongly convex repeated games,\u201d in The Hebrew University, 2007. [32] S. Kakade and Shalev-Shwartz, \u201cMind the duality gap:\n16\nLogarithmic regret algorithms for online optimization,\u201d in NIPS, 2008.\n[33] S. Lacoste-Julien, M. W. Schmidt, and F. Bach, \u201cA simpler approach to obtaining an o(1/t) convergence rate for the projected stochastic subgradient method,\u201d CoRR, 2012. [34] X. Zhu and Z. Ghahramani, \u201cTowards semi-supervised classification with markov random fields,\u201d 2002. [35] G. Getz, N. Shental, and E. Domany, \u201cSemi-supervised learning \u2013 a statistical physics approach,\u201d CoRR, 2006. [36] F. Wu, W. Wang, Y. Yang, Y. Zhuang, and F. Nie, \u201cClassification by semi-supervised discriminative regularization,\u201d Neurocomputing, vol. 73, pp. 1641 \u2013 1651, 2010. [37] S. Shalev-Shwartz and T. Zhang, \u201cStochastic dual coordinate ascent methods for regularized loss,\u201d Journal of Machine Learning Research, vol. 14, no. 1, pp. 567\u2013599, 2013. [38] C.-C. Chang and C.-J. Lin, \u201cLibsvm: A library for support vector machines,\u201d ACM Trans. Intell. Syst. Technol., vol. 2, no. 3, pp. 27:1\u201327:27, May 2011.\n17\nAPPENDIX A CONVEX ANALYSIS\nProof of Lemma 1 We have\nwi = i\u2212 2 i wi\u22121 + 2 i wi\u22121\niwi = (i\u2212 2) wi\u22121 + 2wi\u22121 i (i\u2212 1) wi = (i\u2212 1) (i\u2212 2) wi\u22121 + 2 (i\u2212 1) wi\u22121 (9)\nTaking sum Eq. (9) when i = 2, . . . , t, we gain\nt (t\u2212 1) wt = 2 t\u2211 i=2 (i\u2212 1) wi\u22121 = 2 t\u22121\u2211 i=1 iwi\nwt = 2\n(t\u2212 1) t t\u22121\u2211 i=1 iwi\nProof of Lemma 2 We prove three cases as follows i) In this case, we have f (M ; a, b, p) = aMp\u22121 \u2212M + b = Mp\u22121 ( a\u2212M2\u2212p ) + b\nSince M = max ( 1, (a+ b) 1 2\u2212p )\n, we have Mp\u22121 > 1 and M2\u2212p \u2265 a+ b > a. Hence, we gain\nf (M ; a, b, p) < a\u2212M2\u2212p + b \u2264 0\nii) With p = 2 and a < 1, we have M = b1\u2212a > 0 and\nf (M ; p, a, b) = (a\u2212 1)M + b = 0\niii) In this case, we have\nf (M ; a, b, p) = a\n(p\u2212 1) p\u22121 p\u22122 a p\u22121 p\u22122 \u2212 1 (p\u2212 1) 1 p\u22122 a 1 p\u22122 + b\n= a\u2212 a (p\u2212 1) + b (p\u2212 1)\np\u22121 p\u22122 a p\u22121 p\u22122\n(p\u2212 1) p\u22121 p\u22122 a p\u22121 p\u22122 = a ( 2\u2212 p+ a 1 p\u22122 b (p\u2212 1) p\u22121 p\u22122 )\n(p\u2212 1) p\u22121 p\u22122 a p\u22121 p\u22122\n=\na (( abp\u22122 (p\u2212 1)p\u22121 ) 1 p\u22122 \u2212 ( (p\u2212 2) 1 p\u22122 ) 1 p\u22122 )\n(p\u2212 1) p\u22121 p\u22122 a p\u22121 p\u22122\n\u2264 0\nProof of Lemma 3 We prove that \u2016wt\u2016 \u2264M for all t by induction. Assume the hypothesis holds with t, we verify it for t+ 1. We start with\nwt+1 = wt \u2212 \u03b7tgt\n= t\u2212 1 t+ 1 wt \u2212 2C t+ 1 l \u2032 (wt;xit , yit) \u2212 2C \u2032\u00b5utvt t+ 1 l \u2032 p ( wTt \u03a6utvt\n) We have\n\u2016wt+1\u2016 \u2264 t\u2212 1 t+ 1 \u2016wt\u2016+ 2C t+ 1 \u2225\u2225\u2225l\u2032 (wt;xit , yit)\u2225\u2225\u2225 +\n2C \u2032\u00b5utvt t+ 1 \u2225\u2225\u2225l\u2032p (wTt \u03a6utvt)\u2225\u2225\u2225 \u2264 t\u2212 1 t+ 1 \u2016wt\u20162 + 2CA t+ 1 + 2C \u2032\u00b5max \u2225\u2225\u2225l\u2032p (wTt \u03a6utvt)\u2225\u2225\u2225 t+ 1\nRecall that\nl \u2032\np ( wTt \u03a6utvt ) = psign ( wTt \u03a6utvt ) |wTt \u03a6utvt |p\u22121\u03a6utvt\nHence, we gain\u2225\u2225\u2225l\u2032p (wTt \u03a6utvt)\u2225\u2225\u2225 \u2264 2Rp \u2016\u03a6utvt\u2016p\u22121 \u2016wt\u2016p\u22121 \u2264 (2R)p p \u2016wt\u2016p\u22121\nHere we note that \u2016\u03a6utvt\u2016 = \u2016\u03a6 (xut)\u2212 \u03a6 (xvt)\u2016 \u2264 2R. Therefore, we gain the following inequality\n\u2016wt+1\u2016 \u2264 t\u2212 1 t+ 1 \u2016wt\u2016+ 2CA t+ 1 +\n2C \u2032\u00b5utvt (2R) p p \u2016wt\u2016p\u22121\nt+ 1\n\u2264 t\u2212 1 t+ 1 \u2016wt\u2016+ 2CA t+ 1 +\n2C \u2032 (2R) p p \u2016wt\u2016p\u22121\nt+ 1 since \u00b5utvt \u2264 1\n= t\u2212 1 t+ 1 \u2016wt\u2016+ 2b t+ 1 +\n2a \u2016wt\u2016p\u22121\nt+ 1\nwhere we denote a = C \u2032 (2R)p p and b = CA. Recall that we define M as\nM =  max ( 1, (a+ b) 1 2\u2212p ) if p < 2 b 1\u2212a if p = 2, a < 1( 1\n(p\u22121)a\n) 1 p\u22122\nif p > 2, abp\u22122 \u2264 (p\u22122) p\u22122\n(p\u22121)p\u22121\nReferring to Lemma 2, we possess f (M ; p, a, b) \u2264 0. We then have\n\u2016wt+1\u2016 \u2264 t\u2212 1 t+ 1 M + 2b t+ 1 + 2aMp\u22121 t+ 1\n\u2264M + 2 ( aMp\u22121 \u2212M + b ) t+ 1 \u2264M + f (M ; p, a, b) t+ 1 \u2264M\nProof of Lemma 4 To bound \u2016gt\u2016, we derive as\n\u2016gt\u2016 \u2264 \u2016wt\u2016+ C \u2225\u2225\u2225l\u2032 (wt;xit , yit)\u2225\u2225\u2225+ C \u2032\u00b5utvt \u2225\u2225\u2225l\u2032p (wTt \u03a6utvt)\u2225\u2225\u2225\n\u2264M + CA+ C \u2032 (2R)p p \u2016wt\u2016p\u22121 \u2264M + CA+ C \u2032 (2R)p pMp\u22121 = G\nProof of Theorem 5 We have\n\u2016wt+1 \u2212w\u2217\u20162 = \u2016wt \u2212 \u03b7tgt \u2212w\u2217\u20162\n= \u2016wt \u2212w\u2217\u20162 + \u03b72t \u2016gt\u20162 \u2212 2\u03b7tgTt (wt \u2212w\u2217)\nTaking the conditional expectation w.r.t wt, we gain\nE [ \u2016wt+1 \u2212w\u2217\u20162 ] = E [ \u2016wt \u2212w\u2217\u20162 ] + \u03b72tE [ \u2016gt\u20162 ] \u2212 2\u03b7tE [ gTt (wt \u2212w\u2217)\n] = E [ \u2016wt \u2212w\u2217\u20162 ] + \u03b72tE [ \u2016gt\u20162\n] \u2212 2\u03b7t (wt \u2212w\u2217)T E [gt] = E [ \u2016wt \u2212w\u2217\u20162 ] + \u03b72tE [ \u2016gt\u20162\n] + 2\u03b7t (w \u2217 \u2212wt)T J \u2032 (wt)\n\u2264 E [ \u2016wt \u2212w\u2217\u20162 ] + \u03b72tE [ \u2016gt\u20162 ] + 2\u03b7t ( J (w\u2217)\u2212 J (wt)\u2212 1\n2 \u2016wt \u2212w\u2217\u20162\n)\n18\nTaking the expectation the above equation again, we yield E [ \u2016wt+1 \u2212w\u2217\u20162 ] \u2264 E [ \u2016wt \u2212w\u2217\u20162 ] + \u03b72tE [ \u2016gt\u20162 ] +2\u03b7t ( J (w\u2217)\u2212 E [J (wt)]\u2212 1 2 E [ \u2016wt \u2212w\u2217\u20162\n]) \u2264 (1\u2212 \u03b7t)E [ \u2016wt \u2212w\u2217\u20162 ] +G\u03b72t + 2\u03b7t (J (w\u2217)\u2212 E [J (wt)])\nE [J (wt)]\u2212 J (w\u2217) \u2264 ( 1\n2\u03b7t \u2212 1 2\n) E [ \u2016wt \u2212w\u2217\u20162 ] \u2212 1\n2\u03b7t E [ \u2016wt \u2212w\u2217\u20162 ] + G\u03b7t 2\nUsing the learning rate \u03b7t = 2t+1 , we gain\nE [J (wt)]\u2212 J (w\u2217) \u2264 t\u2212 1 4 E [ \u2016wt \u2212w\u2217\u20162 ] \u2212 t+ 1\n4 E [ \u2016wt+1 \u2212w\u2217\u20162 ] + G t+ 1\nE [tJ (wt)]\u2212 tJ (w\u2217) \u2264 (t\u2212 1) t 4 E [ \u2016wt \u2212w\u2217\u20162 ] \u2212 t (t+ 1)\n4 E [ \u2016wt+1 \u2212w\u2217\u20162 ] + Gt t+ 1\nTaking sum when t runs from 1 to T , we achieve\nE\n[ T\u2211\nt=1\ntJ (wt) ] \u2212 T (T + 1)\n2 J (w\u2217)\n\u2264 \u2212T (T + 1) 4\nE [ wT+1 \u2212w\u2217\u20162 ] + T\u2211 t=1 Gt t+ 1 \u2264 GT\nE\n[ 2\nT (T + 1) T\u2211 t=1 tJ (wt)\n] \u2212 J (w\u2217) \u2264 2G\nT\nE [ J ( 2\nT (T + 1) T\u2211 t=1 twt\n) \u2212 J (w\u2217) ] \u2264 2G\nT (10)\nthanks to the convexity of J (.)\nE [J (wT+1)\u2212 J (w\u2217)] \u2264 2G\nT where wT+1 = 2T (T+1) \u2211T\nt=1 twt. Furthermore, from the strong convexity of J (.) and w\u2217 is a minimizer, we have\nJ (wT+1)\u2212 J (w\u2217) \u2265 \u2329 J \u2032 (w\u2217) ,wT+1 \u2212w\u2217 \u232a + 1\n2 \u2016wT+1 \u2212w\u2217\u20162 \u2265\n1 2 \u2016wT+1 \u2212w\u2217\u20162\nE [ \u2016wT+1 \u2212w\u2217\u20162 ] \u2264 4G\nT (11)\nProof of Theorem 6 Let us denote the random variable ZT+1 = J (wT+1) \u2212 J (w\u2217) \u2265 0. According to Markov inequality, we have\nP [ZT+1 \u2265 \u03b5] \u2264 E [ZT+1]\n\u03b5\n= E [J (wT+1)\u2212 J (w\u2217)]\n\u03b5 \u2264 2G T\u03b5\nP [ZT+1 < \u03b5] \u2265 1\u2212 2G\nT\u03b5 By choosing T0 = \u2308 2L \u03b5\u03b4 \u2309 , for all T \u2265 T0, we have\nP [ZT+1 < \u03b5] \u2265 1\u2212 2G\nT\u03b5 \u2265 1\u2212 \u03b4"}], "references": [{"title": "Transductive inference for text classification using support vector machines", "author": ["T. Joachims"], "venue": "International Conference on Machine Learning (ICML), Bled, Slowenien, 1999, pp. 200\u2013209.", "citeRegEx": "1", "shortCiteRegEx": null, "year": 1999}, {"title": "Bootstrapping svm active learning by incorporating unlabelled images for image retrieval.", "author": ["L. Wang", "K.L. Chan", "Z. Zhang"], "venue": "IEEE Computer Society,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2003}, {"title": "Incremental learning in autonomous systems: evolving connectionist systems for on-line image and speech recognition", "author": ["N. Kasabov", "D. Zhang", "P. Pang"], "venue": "Advanced Robotics and its Social Impacts, 2005. IEEE Workshop on, june 2005, pp. 120 \u2013 125.", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2005}, {"title": "Combining labelled and unlabelled data: A case study on fisher kernels and transductive inference for biological entity recognition", "author": ["C. Goutte", "H. D\u00e9jean", "E. Gaussier", "N. Cancedda", "J.- M. Renders"], "venue": "Proceedings of the 6th Conference on Natural Language Learning - Volume 20, ser. COLING-02, 2002, pp. 1\u20137.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2002}, {"title": "Optimization techniques for semi-supervised support vector machines", "author": ["O. Chapelle", "V. Sindhwani", "S. Keerthi"], "venue": "Journal of Machine Learning Research, vol. 9, pp. 203\u2013 233, Jun. 2008.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2008}, {"title": "Learning from labeled and unlabeled data using graph mincuts", "author": ["A. Blum", "S. Chawla"], "venue": "Proceedings of the Eighteenth International Conference on Machine Learning, ser. ICML \u201901, 2001, pp. 19\u201326.", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2001}, {"title": "Semi-supervised learning using randomized mincuts.", "author": ["A. Blum", "J.D. Lafferty", "M.R. Rwebangira", "R. Reddy"], "venue": "in ICML, vol", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2004}, {"title": "Semisupervised learning using gaussian fields and harmonic functions", "author": ["X. Zhu", "Z. Ghahramani", "J.D. Lafferty"], "venue": "IN ICML, 2003, pp. 912\u2013919.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2003}, {"title": "Partially labeled classification with markov random walks", "author": ["M. Szummer", "T. Jaakkola"], "venue": "Advances in Neural Information Processing Systems. MIT Press, 2002, pp. 945\u2013952.", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2002}, {"title": "The rendezvous algorithm: Multiclass semisupervised learning with markov random walks", "author": ["A. Azran"], "venue": "Proceedings of the 24th International Conference on Machine Learning, ser. ICML \u201907, 2007, pp. 49\u201356.", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2007}, {"title": "Transductive learning via spectral graph partitioning", "author": ["T. Joachims"], "venue": "In ICML, 2003, pp. 290\u2013297.", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2003}, {"title": "Graph-based semi-supervised support vector data description for novelty detection", "author": ["P. Duong", "V. Nguyen", "M. Dinh", "T. Le", "D. Tran", "W. Ma"], "venue": "2015 International Joint Conference on Neural Networks (IJCNN), July 2015, pp. 1\u20136.", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2015}, {"title": "Manifold regularization: A geometric framework for learning from labeled and unlabeled examples", "author": ["M. Belkin", "P. Niyogi", "V. Sindhwani"], "venue": "J. Mach. Learn. Res., vol. 7, pp. 2399\u20132434, Dec. 2006.", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2006}, {"title": "Support-vector networks", "author": ["C. Cortes", "V. Vapnik"], "venue": "Machine Learning, 1995, pp. 273\u2013297.", "citeRegEx": "14", "shortCiteRegEx": null, "year": 1995}, {"title": "Semi-Supervised Classification by Low Density Separation", "author": ["O. Chapelle", "A. Zien"], "venue": null, "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2005}, {"title": "A continuation method for semi-supervised svms", "author": ["O. Chapelle", "M. Chi", "A. Zien"], "venue": "Proceedings of the 23rd international conference on Machine learning, ser. ICML \u201906. ACM, 2006, pp. 185\u2013192.", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2006}, {"title": "Semi-supervised support vector machines for unlabeled data classification", "author": ["G. Fung", "O. Mangasarian"], "venue": "Optimization Methods and Software, vol. 15, pp. 29\u201344, 2001.", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2001}, {"title": "Large scale transductive svms", "author": ["R. Collobert", "F. Sinz", "J. Weston", "L. Bottou", "T. Joachims"], "venue": "Journal of Machine Learning Research, 2006.", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2006}, {"title": "Deterministic annealing for semi-supervised kernel machines", "author": ["V. Sindhwani", "S. Keerthi", "O. Chapelle"], "venue": "Proceedings of the 23rd international conference on Machine learning, ser. ICML \u201906, 2006, pp. 841\u2013848.", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2006}, {"title": "Fuzzy entropy semi-supervised support vector data description", "author": ["T. Le", "D. Tran", "T. Tran", "K. Nguyen", "W. Ma"], "venue": "2013 International Joint Conference on Neural Networks (IJCNN), Aug 2013, pp. 1\u20135.", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2013}, {"title": "Kernel-based semi-supervised learning for novelty detection", "author": ["V. Nguyen", "T. Le", "T. Pham", "M. Dinh", "T.H. Le"], "venue": "2014 International Joint Conference on Neural Networks (IJCNN), July 2014, pp. 4129\u20134136.", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2014}, {"title": "Semi-supervised learning using semi-definite programming", "author": ["T. De Bie", "N. Cristianini"], "venue": "Semi-supervised Learning, Cambridge, MA, 2006.", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2006}, {"title": "Cluster kernels for semi-supervised learning", "author": ["O. Chapelle", "J. Weston", "B. Sch\u00f6lkopf"], "venue": "Advances in Neural Information Processing Systems 15, S. Becker, S. Thrun, and K. Obermayer, Eds. MIT Press, 2003, pp. 601\u2013608.", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2003}, {"title": "Semi-supervised learning with trees", "author": ["C. Kemp", "T.L. Griffiths", "S. Stromsten", "J.B. Tenenbaum"], "venue": "Advances in Neural Information Processing Systems 16, S. Thrun, L. Saul, and B. Sch\u00f6lkopf, Eds., 2004, pp. 257\u2013264.", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2004}, {"title": "Diffusion kernels on graphs and other discrete input spaces.", "author": ["R.I. Kondor", "J.D. Lafferty"], "venue": null, "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2002}, {"title": "Kernels and regularization on graphs.", "author": ["A.J. Smola", "I.R. Kondor"], "venue": "Proceedings of the Annual Conference on Computational Learning Theory,", "citeRegEx": "26", "shortCiteRegEx": "26", "year": 2003}, {"title": "Nonparametric transforms of graph kernels for semisupervised learning.", "author": ["X. Zhu", "J.S. Kandola", "Z. Ghahramani", "J.D. Lafferty"], "venue": null, "citeRegEx": "27", "shortCiteRegEx": "27", "year": 2004}, {"title": "Linear manifold regularization for large scale semi-supervised learning", "author": ["V. Sindhwani", "P. Niyogi"], "venue": "Proc. of the 22nd ICML Workshop on Learning with Partially Classified Training Data, 2005.", "citeRegEx": "28", "shortCiteRegEx": null, "year": 2005}, {"title": "Large-scale sparsified manifold regularization.", "author": ["I.W. Tsang", "J.T. Kwok"], "venue": null, "citeRegEx": "29", "shortCiteRegEx": "29", "year": 2006}, {"title": "Laplacian support vector machines trained in the primal", "author": ["S. Melacci", "M. Belkin"], "venue": "J. Mach. Learn. Res., vol. 12, Jul. 2011.", "citeRegEx": "30", "shortCiteRegEx": null, "year": 2011}, {"title": "Logarithmic regret algorithms for strongly convex repeated games", "author": ["S. Shalev-shwartz", "Y. Singer"], "venue": "The Hebrew University, 2007.", "citeRegEx": "31", "shortCiteRegEx": null, "year": 2007}, {"title": "Mind the duality gap:  16 Logarithmic regret algorithms for online optimization", "author": ["S. Kakade", "Shalev-Shwartz"], "venue": "NIPS, 2008.", "citeRegEx": "32", "shortCiteRegEx": null, "year": 2008}, {"title": "A simpler approach to obtaining an o(1/t) convergence rate for the projected stochastic subgradient method", "author": ["S. Lacoste-Julien", "M.W. Schmidt", "F. Bach"], "venue": "CoRR, 2012.", "citeRegEx": "33", "shortCiteRegEx": null, "year": 2012}, {"title": "Towards semi-supervised classification with markov random fields", "author": ["X. Zhu", "Z. Ghahramani"], "venue": "2002.", "citeRegEx": "34", "shortCiteRegEx": null, "year": 2002}, {"title": "Semi-supervised learning \u2013 a statistical physics approach", "author": ["G. Getz", "N. Shental", "E. Domany"], "venue": "CoRR, 2006.", "citeRegEx": "35", "shortCiteRegEx": null, "year": 2006}, {"title": "Classification by semi-supervised discriminative regularization", "author": ["F. Wu", "W. Wang", "Y. Yang", "Y. Zhuang", "F. Nie"], "venue": "Neurocomputing, vol. 73, pp. 1641 \u2013 1651, 2010.", "citeRegEx": "36", "shortCiteRegEx": null, "year": 2010}, {"title": "Stochastic dual coordinate ascent methods for regularized loss", "author": ["S. Shalev-Shwartz", "T. Zhang"], "venue": "Journal of Machine Learning Research, vol. 14, no. 1, pp. 567\u2013599, 2013.", "citeRegEx": "37", "shortCiteRegEx": null, "year": 2013}], "referenceMentions": [{"referenceID": 0, "context": "During the past decade, SSL has attracted significant attention and has found applicable in a variety of real-world problems including text categorization [1], image retrieval [2], bioinformatics [3], natural language processing [4] to name a few.", "startOffset": 155, "endOffset": 158}, {"referenceID": 1, "context": "During the past decade, SSL has attracted significant attention and has found applicable in a variety of real-world problems including text categorization [1], image retrieval [2], bioinformatics [3], natural language processing [4] to name a few.", "startOffset": 176, "endOffset": 179}, {"referenceID": 2, "context": "During the past decade, SSL has attracted significant attention and has found applicable in a variety of real-world problems including text categorization [1], image retrieval [2], bioinformatics [3], natural language processing [4] to name a few.", "startOffset": 196, "endOffset": 199}, {"referenceID": 3, "context": "During the past decade, SSL has attracted significant attention and has found applicable in a variety of real-world problems including text categorization [1], image retrieval [2], bioinformatics [3], natural language processing [4] to name a few.", "startOffset": 229, "endOffset": 232}, {"referenceID": 4, "context": "While obtaining pre-defined labels is a labor-intensive and time-consuming process [5], ones have found that unlabeled data, when used in conjunction with a small amount of labeled data, can bring a remarkable improvement in classification accuracy [1].", "startOffset": 83, "endOffset": 86}, {"referenceID": 0, "context": "While obtaining pre-defined labels is a labor-intensive and time-consuming process [5], ones have found that unlabeled data, when used in conjunction with a small amount of labeled data, can bring a remarkable improvement in classification accuracy [1].", "startOffset": 249, "endOffset": 252}, {"referenceID": 5, "context": "The typical graph-based methods include min-cut [6, 7], harmonic function [8], graph random walk [9, 10], spectral graph transducer [11, 12], and manifold regularization [13].", "startOffset": 48, "endOffset": 54}, {"referenceID": 6, "context": "The typical graph-based methods include min-cut [6, 7], harmonic function [8], graph random walk [9, 10], spectral graph transducer [11, 12], and manifold regularization [13].", "startOffset": 48, "endOffset": 54}, {"referenceID": 7, "context": "The typical graph-based methods include min-cut [6, 7], harmonic function [8], graph random walk [9, 10], spectral graph transducer [11, 12], and manifold regularization [13].", "startOffset": 74, "endOffset": 77}, {"referenceID": 8, "context": "The typical graph-based methods include min-cut [6, 7], harmonic function [8], graph random walk [9, 10], spectral graph transducer [11, 12], and manifold regularization [13].", "startOffset": 97, "endOffset": 104}, {"referenceID": 9, "context": "The typical graph-based methods include min-cut [6, 7], harmonic function [8], graph random walk [9, 10], spectral graph transducer [11, 12], and manifold regularization [13].", "startOffset": 97, "endOffset": 104}, {"referenceID": 10, "context": "The typical graph-based methods include min-cut [6, 7], harmonic function [8], graph random walk [9, 10], spectral graph transducer [11, 12], and manifold regularization [13].", "startOffset": 132, "endOffset": 140}, {"referenceID": 11, "context": "The typical graph-based methods include min-cut [6, 7], harmonic function [8], graph random walk [9, 10], spectral graph transducer [11, 12], and manifold regularization [13].", "startOffset": 132, "endOffset": 140}, {"referenceID": 12, "context": "The typical graph-based methods include min-cut [6, 7], harmonic function [8], graph random walk [9, 10], spectral graph transducer [11, 12], and manifold regularization [13].", "startOffset": 170, "endOffset": 174}, {"referenceID": 0, "context": "Inspired from the pioneering work of [1], recent works have attempted to incorporate kernel methods such as Support Vector Machine (SVM) [14] with the semi-supervised learning paradigm.", "startOffset": 37, "endOffset": 40}, {"referenceID": 13, "context": "Inspired from the pioneering work of [1], recent works have attempted to incorporate kernel methods such as Support Vector Machine (SVM) [14] with the semi-supervised learning paradigm.", "startOffset": 137, "endOffset": 141}, {"referenceID": 4, "context": "The underlying idea of this research line is to solve standard SVM problem while treating the unknown labels as optimization variables [5].", "startOffset": 135, "endOffset": 138}, {"referenceID": 0, "context": ", local combination search [1], gradient descent [15], continuation techniques [16], convex-concave procedures [17, 18], deterministic annealing [19, 20, 21], and semi-definite programming [22].", "startOffset": 27, "endOffset": 30}, {"referenceID": 14, "context": ", local combination search [1], gradient descent [15], continuation techniques [16], convex-concave procedures [17, 18], deterministic annealing [19, 20, 21], and semi-definite programming [22].", "startOffset": 49, "endOffset": 53}, {"referenceID": 15, "context": ", local combination search [1], gradient descent [15], continuation techniques [16], convex-concave procedures [17, 18], deterministic annealing [19, 20, 21], and semi-definite programming [22].", "startOffset": 79, "endOffset": 83}, {"referenceID": 16, "context": ", local combination search [1], gradient descent [15], continuation techniques [16], convex-concave procedures [17, 18], deterministic annealing [19, 20, 21], and semi-definite programming [22].", "startOffset": 111, "endOffset": 119}, {"referenceID": 17, "context": ", local combination search [1], gradient descent [15], continuation techniques [16], convex-concave procedures [17, 18], deterministic annealing [19, 20, 21], and semi-definite programming [22].", "startOffset": 111, "endOffset": 119}, {"referenceID": 18, "context": ", local combination search [1], gradient descent [15], continuation techniques [16], convex-concave procedures [17, 18], deterministic annealing [19, 20, 21], and semi-definite programming [22].", "startOffset": 145, "endOffset": 157}, {"referenceID": 19, "context": ", local combination search [1], gradient descent [15], continuation techniques [16], convex-concave procedures [17, 18], deterministic annealing [19, 20, 21], and semi-definite programming [22].", "startOffset": 145, "endOffset": 157}, {"referenceID": 20, "context": ", local combination search [1], gradient descent [15], continuation techniques [16], convex-concave procedures [17, 18], deterministic annealing [19, 20, 21], and semi-definite programming [22].", "startOffset": 145, "endOffset": 157}, {"referenceID": 21, "context": ", local combination search [1], gradient descent [15], continuation techniques [16], convex-concave procedures [17, 18], deterministic annealing [19, 20, 21], and semi-definite programming [22].", "startOffset": 189, "endOffset": 193}, {"referenceID": 22, "context": "Conjoining the advantages of kernel method and the spectral graph theory, several existing works have tried to incorporate information carried in a spectral graph for building a better kernel function [23, 24, 25, 26, 27].", "startOffset": 201, "endOffset": 221}, {"referenceID": 23, "context": "Conjoining the advantages of kernel method and the spectral graph theory, several existing works have tried to incorporate information carried in a spectral graph for building a better kernel function [23, 24, 25, 26, 27].", "startOffset": 201, "endOffset": 221}, {"referenceID": 24, "context": "Conjoining the advantages of kernel method and the spectral graph theory, several existing works have tried to incorporate information carried in a spectral graph for building a better kernel function [23, 24, 25, 26, 27].", "startOffset": 201, "endOffset": 221}, {"referenceID": 25, "context": "Conjoining the advantages of kernel method and the spectral graph theory, several existing works have tried to incorporate information carried in a spectral graph for building a better kernel function [23, 24, 25, 26, 27].", "startOffset": 201, "endOffset": 221}, {"referenceID": 26, "context": "Conjoining the advantages of kernel method and the spectral graph theory, several existing works have tried to incorporate information carried in a spectral graph for building a better kernel function [23, 24, 25, 26, 27].", "startOffset": 201, "endOffset": 221}, {"referenceID": 12, "context": "Manifold regularization framework [13] exploits the geometric information of the probability distribution that generates data and incorporates it as an additional regularization term.", "startOffset": 34, "endOffset": 38}, {"referenceID": 27, "context": "Hence other researches have been carried out to enhance the scalability of the manifold regularization framework [28, 29, 30].", "startOffset": 113, "endOffset": 125}, {"referenceID": 28, "context": "Hence other researches have been carried out to enhance the scalability of the manifold regularization framework [28, 29, 30].", "startOffset": 113, "endOffset": 125}, {"referenceID": 29, "context": "Hence other researches have been carried out to enhance the scalability of the manifold regularization framework [28, 29, 30].", "startOffset": 113, "endOffset": 125}, {"referenceID": 29, "context": "Specifically, the work of [30] makes use of the preconditioned conjugate gradient to solve the optimization problem encountered in manifold regularization framework in the primal form.", "startOffset": 26, "endOffset": 30}, {"referenceID": 29, "context": "In addition, the LapSVM in primal approach [30] requires to store the entire Hessian matrix of size n \u00d7 n in the memory, resulting in a memory complexity of O(n).", "startOffset": 43, "endOffset": 47}, {"referenceID": 30, "context": "Recently, stochastic gradient descent (SGD) methods [31, 32, 33] have emerged as a promising framework to speed up the training process and enable the online learning paradigm.", "startOffset": 52, "endOffset": 64}, {"referenceID": 31, "context": "Recently, stochastic gradient descent (SGD) methods [31, 32, 33] have emerged as a promising framework to speed up the training process and enable the online learning paradigm.", "startOffset": 52, "endOffset": 64}, {"referenceID": 32, "context": "Recently, stochastic gradient descent (SGD) methods [31, 32, 33] have emerged as a promising framework to speed up the training process and enable the online learning paradigm.", "startOffset": 52, "endOffset": 64}, {"referenceID": 12, "context": "We note that the well-known Laplacian Support Vector Machine (LapSVM) [13, 30] is the special case of gS3VM(s) when using Hinge loss and the smoothness function l2 (.", "startOffset": 70, "endOffset": 78}, {"referenceID": 29, "context": "We note that the well-known Laplacian Support Vector Machine (LapSVM) [13, 30] is the special case of gS3VM(s) when using Hinge loss and the smoothness function l2 (.", "startOffset": 70, "endOffset": 78}, {"referenceID": 32, "context": "We then develop a new algorithm based on the SGD framework [33] to directly solve the optimization problem of gS3VM in its primal form with the ideal convergence rate O ( 1 T ) .", "startOffset": 59, "endOffset": 63}, {"referenceID": 32, "context": "\u2022 We apply stochastic gradient descent (SGD) framework [33] to solve directly gS3VM in its primal form.", "startOffset": 55, "endOffset": 59}, {"referenceID": 5, "context": "In [6, 7], semi-supervised learning problem is viewed as graph mincut problem.", "startOffset": 3, "endOffset": 9}, {"referenceID": 6, "context": "In [6, 7], semi-supervised learning problem is viewed as graph mincut problem.", "startOffset": 3, "endOffset": 9}, {"referenceID": 33, "context": "In [34], Markov Chain Monte Carlo sampling techniques is used to approximate this marginal probability.", "startOffset": 3, "endOffset": 7}, {"referenceID": 34, "context": "The work of [35] proposes to compute the marginal probabilities of the discrete Markov random field at any temperature with the Multi-canonical Monte Carlo method, which seems to be able to overcome the energy trap faced by the standard Metropolis or Swendsen-Wang method.", "startOffset": 12, "endOffset": 16}, {"referenceID": 7, "context": "The harmonic functions used in [8] is regarded as a continuous relaxation of the discrete Markov random field.", "startOffset": 31, "endOffset": 34}, {"referenceID": 22, "context": "The works of [23, 24, 25, 26, 27] utilize the Laplacian matrix induced from the spectral graph to form kernel functions which can capture the features of the ambient space.", "startOffset": 13, "endOffset": 33}, {"referenceID": 23, "context": "The works of [23, 24, 25, 26, 27] utilize the Laplacian matrix induced from the spectral graph to form kernel functions which can capture the features of the ambient space.", "startOffset": 13, "endOffset": 33}, {"referenceID": 24, "context": "The works of [23, 24, 25, 26, 27] utilize the Laplacian matrix induced from the spectral graph to form kernel functions which can capture the features of the ambient space.", "startOffset": 13, "endOffset": 33}, {"referenceID": 25, "context": "The works of [23, 24, 25, 26, 27] utilize the Laplacian matrix induced from the spectral graph to form kernel functions which can capture the features of the ambient space.", "startOffset": 13, "endOffset": 33}, {"referenceID": 26, "context": "The works of [23, 24, 25, 26, 27] utilize the Laplacian matrix induced from the spectral graph to form kernel functions which can capture the features of the ambient space.", "startOffset": 13, "endOffset": 33}, {"referenceID": 4, "context": "The kernel-based semisupervised methods are primarily driven by the idea to solve a standard SVM problem while treating the unknown labels as optimization variables [5].", "startOffset": 165, "endOffset": 168}, {"referenceID": 0, "context": "Many methods have been proposed to solve this optimization problem, for example local combinatorial search [1], gradient descent [15], continuation techniques [16], convex-concave procedures [17, 18], deterministic annealing [19, 20, 21], and semi-definite programming [22].", "startOffset": 107, "endOffset": 110}, {"referenceID": 14, "context": "Many methods have been proposed to solve this optimization problem, for example local combinatorial search [1], gradient descent [15], continuation techniques [16], convex-concave procedures [17, 18], deterministic annealing [19, 20, 21], and semi-definite programming [22].", "startOffset": 129, "endOffset": 133}, {"referenceID": 15, "context": "Many methods have been proposed to solve this optimization problem, for example local combinatorial search [1], gradient descent [15], continuation techniques [16], convex-concave procedures [17, 18], deterministic annealing [19, 20, 21], and semi-definite programming [22].", "startOffset": 159, "endOffset": 163}, {"referenceID": 16, "context": "Many methods have been proposed to solve this optimization problem, for example local combinatorial search [1], gradient descent [15], continuation techniques [16], convex-concave procedures [17, 18], deterministic annealing [19, 20, 21], and semi-definite programming [22].", "startOffset": 191, "endOffset": 199}, {"referenceID": 17, "context": "Many methods have been proposed to solve this optimization problem, for example local combinatorial search [1], gradient descent [15], continuation techniques [16], convex-concave procedures [17, 18], deterministic annealing [19, 20, 21], and semi-definite programming [22].", "startOffset": 191, "endOffset": 199}, {"referenceID": 18, "context": "Many methods have been proposed to solve this optimization problem, for example local combinatorial search [1], gradient descent [15], continuation techniques [16], convex-concave procedures [17, 18], deterministic annealing [19, 20, 21], and semi-definite programming [22].", "startOffset": 225, "endOffset": 237}, {"referenceID": 19, "context": "Many methods have been proposed to solve this optimization problem, for example local combinatorial search [1], gradient descent [15], continuation techniques [16], convex-concave procedures [17, 18], deterministic annealing [19, 20, 21], and semi-definite programming [22].", "startOffset": 225, "endOffset": 237}, {"referenceID": 20, "context": "Many methods have been proposed to solve this optimization problem, for example local combinatorial search [1], gradient descent [15], continuation techniques [16], convex-concave procedures [17, 18], deterministic annealing [19, 20, 21], and semi-definite programming [22].", "startOffset": 225, "endOffset": 237}, {"referenceID": 21, "context": "Many methods have been proposed to solve this optimization problem, for example local combinatorial search [1], gradient descent [15], continuation techniques [16], convex-concave procedures [17, 18], deterministic annealing [19, 20, 21], and semi-definite programming [22].", "startOffset": 269, "endOffset": 273}, {"referenceID": 0, "context": "S4VM is shown to be safe and to achieve the maximal performance improvement under the low-density assumption of S3VM [1].", "startOffset": 117, "endOffset": 120}, {"referenceID": 12, "context": "extends [13, 36] to propose semi-supervised discriminationaware manifold regularization framework which considers the discrimination of all available instances in learning of manifold regularization.", "startOffset": 8, "endOffset": 16}, {"referenceID": 35, "context": "extends [13, 36] to propose semi-supervised discriminationaware manifold regularization framework which considers the discrimination of all available instances in learning of manifold regularization.", "startOffset": 8, "endOffset": 16}, {"referenceID": 12, "context": "The closest work to ours is manifold regularization framework [13] and its extensions [28, 29, 30].", "startOffset": 62, "endOffset": 66}, {"referenceID": 27, "context": "The closest work to ours is manifold regularization framework [13] and its extensions [28, 29, 30].", "startOffset": 86, "endOffset": 98}, {"referenceID": 28, "context": "The closest work to ours is manifold regularization framework [13] and its extensions [28, 29, 30].", "startOffset": 86, "endOffset": 98}, {"referenceID": 29, "context": "The closest work to ours is manifold regularization framework [13] and its extensions [28, 29, 30].", "startOffset": 86, "endOffset": 98}, {"referenceID": 12, "context": "However, the original work of manifold regularization [13] requires to invert a matrix of size n by n which costs cubically and hence is not scalable.", "startOffset": 54, "endOffset": 58}, {"referenceID": 28, "context": "[29] scales up manifold regularization framework by adding in an \u03b5-insensitive loss into the energy function, i.", "startOffset": 0, "endOffset": 4}, {"referenceID": 29, "context": "LapSVM (in primal) [30] employs the preconditioned conjugate gradient to solve the optimization problem of manifold regularization in the primal form.", "startOffset": 19, "endOffset": 23}, {"referenceID": 29, "context": "However, the optimization problem in [30] is indeed solved in the first dual layer rather than in the primal form.", "startOffset": 37, "endOffset": 41}, {"referenceID": 29, "context": "In addition, we empirically find that the LapSVM in primal [30] is unsatisfactory in terms of memory complexity.", "startOffset": 59, "endOffset": 63}, {"referenceID": 12, "context": "It is noteworthy that Laplacian Support Vector Machine (LapSVM) [13, 30] is the special case of gS3VM using the Hinge loss with the smoothness function l2 (.", "startOffset": 64, "endOffset": 72}, {"referenceID": 29, "context": "It is noteworthy that Laplacian Support Vector Machine (LapSVM) [13, 30] is the special case of gS3VM using the Hinge loss with the smoothness function l2 (.", "startOffset": 64, "endOffset": 72}, {"referenceID": 32, "context": "We employ the SGD framework proposed in [33] to solve the optimization problem in Eq.", "startOffset": 40, "endOffset": 44}, {"referenceID": 36, "context": "\u2022 Smooth Hinge loss [37]", "startOffset": 20, "endOffset": 24}, {"referenceID": 29, "context": "Baselines In order to investigate the efficiency and accuracy of our proposed method (gUS3VM), we made comparison with the following baselines: \u2022 LapSVM in primal [30]: Laplacian Support Vector Machine in primal is a state-of-the-art approach in semisupervised classification based on manifold regularization framework.", "startOffset": 163, "endOffset": 167}, {"referenceID": 12, "context": "of the original LapSVM [13] from O ( n ) to O ( kn )", "startOffset": 23, "endOffset": 27}, {"referenceID": 17, "context": "\u2022 CCCP [18]: An approach is proposed to solve the nonconvex optimization problem occurred in the kernel semisupervised approach using convex-concave procedures.", "startOffset": 7, "endOffset": 11}, {"referenceID": 29, "context": "With LapSVM, we use the parameter settings proposed in [30], wherein the parameters \u03b3A and \u03b3I are searched in the grid { 10\u22126, 10\u22124, 10\u22122, 10\u22121, 1, 10, 100 } .", "startOffset": 55, "endOffset": 59}, {"referenceID": 29, "context": "In all experiments with the LapSVM, we make use of the preconditioned conjugate gradient version, which seems more suitable for the LapSVM optimization problem [30].", "startOffset": 160, "endOffset": 164}], "year": 2017, "abstractText": "Owing to the prevalence of unlabeled data, semisupervised learning has recently drawn significant attention and has found applicable in many real-world applications. In this paper, we present the so-called Graph-based Semi-supervised Support Vector Machine (gS3VM), a method that leverages the excellent generalization ability of kernel-based method with the geometrical and distributive information carried in a spectral graph for semi-supervised learning purpose. The proposed gS3VM can be solved directly in the primal form using the Stochastic Gradient Descent method with the ideal convergence rate O ( 1 T ) . Besides, our gS3VM allows the combinations of a wide spectrum of loss functions (e.g., Hinge, smooth Hinge, Logistic, L1, and \u03b5-insensitive) and smoothness functions (i.e., lp (t) = |t| with p \u2265 1). We note that the well-known Laplacian Support Vector Machine falls into the spectrum of gS3VM corresponding with the combination of the Hinge loss and the smoothness function l2 (.). We further validate our proposed method on several benchmark datasets to demonstrate that gS3VM is appropriate for the large-scale datasets since it is optimal in memory used and yields superior classification accuracy whilst simultaneously achieving a significant computation speedup in comparison with the state-of-the-art baselines", "creator": "LaTeX with hyperref package"}}}