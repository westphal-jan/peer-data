{"id": "1606.06950", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "22-Jun-2016", "title": "A segmental framework for fully-unsupervised large-vocabulary speech recognition", "abstract": "Zero-resource speech technology is a growing research area that aims to develop methods for speech processing in the absence of transcriptions, lexicons, or language modelling text. Early systems focused on identifying isolated recurring terms in a corpus, while more recent full-coverage systems attempt to completely segment and cluster the audio into word-like units---effectively performing unsupervised speech recognition. To our knowledge, this article presents the first such system evaluated on large-vocabulary multi-speaker data sets.", "histories": [["v1", "Wed, 22 Jun 2016 13:51:57 GMT  (1022kb,D)", "http://arxiv.org/abs/1606.06950v1", "13 pages, 6 figures, 8 tables"], ["v2", "Sat, 16 Sep 2017 09:36:02 GMT  (1026kb,D)", "http://arxiv.org/abs/1606.06950v2", "15 pages, 6 figures, 8 tables"]], "COMMENTS": "13 pages, 6 figures, 8 tables", "reviews": [], "SUBJECTS": "cs.CL cs.LG", "authors": ["herman kamper", "aren jansen", "sharon goldwater"], "accepted": false, "id": "1606.06950"}, "pdf": {"name": "1606.06950.pdf", "metadata": {"source": "CRF", "title": "A segmental framework for fully-unsupervised large-vocabulary speech recognition", "authors": ["Herman Kamper", "Aren Jansen", "Sharon Goldwater"], "emails": ["kamperh@gmail.com,", "arenjansen@google.com,", "sgwater@inf.ed.ac.uk"], "sections": [{"heading": null, "text": "Zero-resource speech technology is a growing research area that aims to develop methods for speech processing in the absence of transcriptions, lexicons, or language modelling text. Early systems focused on identifying isolated recurring terms in a corpus, while more recent full-coverage systems attempt to completely segment and cluster the audio into word-like units\u2014effectively performing unsupervised speech recognition. To our knowledge, this article presents the first such system evaluated on largevocabulary multi-speaker data. The system uses a Bayesian modelling framework with segmental word representations: each word segment is represented as a fixed-dimensional acoustic embedding obtained by mapping the sequence of feature frames to a single embedding vector. We compare our system on English and Xitsonga datasets to state-of-the-art baselines, using a variety of measures including word error rate (obtained by mapping the unsupervised output to ground truth transcriptions). We show that by imposing a consistent top-down segmentation while also using bottom-up knowledge from detected syllable boundaries, both single-speaker and multi-speaker versions of our system outperform a purely bottom-up single-speaker syllable-based approach. We also show that the discovered clusters can be made less speaker- and gender-specific by using an unsupervised autoencoder-like feature extractor to learn better frame-level features (prior to embedding). Our system\u2019s discovered clusters are still less pure than those of two multi-speaker term discovery systems, but provide far greater coverage.\nKeywords: Unsupervised speech processing, representation learning, segmentation, clustering, language acquisition."}, {"heading": "1. Introduction", "text": "Despite major advances in supervised speech recognition over the last few years, current methods still rely on huge amounts of transcribed speech audio, pronunciation dictionaries, and texts for language modelling. The collection of these pose a major obstacle for speech technology in under-resourced languages. In some extreme cases, unlabelled speech data might be the only available resource. In this zero-resource scenario, unsupervised methods are required to learn representations and linguistic structure directly from the speech signal. Such methods can, for instance, make it possible to search through a corpus of unlabelled speech using voice queries [1], allow topics within speech utterances to be identified without supervision [2], or can be used to automatically cluster related spoken documents [3].\nSimilar techniques are required to model how human infants acquire language from speech input [4], and for developing robotic applications that can learn a new language in an unknown environment [5, 6].\nInterest in zero-resource speech processing has grown considerably in the last few years, with two central research areas emerging [7,8]. The first deals with unsupervised representation learning, where the task is to find speech features (often at the frame level) that make it easier to discriminate between meaningful linguistic units (phones or words). This task has been described as \u2018phonetic discovery\u2019, \u2018unsupervised acoustic modelling\u2019 and \u2018unsupervised subword modelling\u2019, depending on the type of feature representations that are produced. Approaches include those using bottom-up trained Gaussian mixture models (GMMs) to produce frame-level posteriorgrams [9, 10], using unsupervised hidden Markov models (HMMs) to obtain discrete categorical output in terms of discovered subword units [2, 11, 12], and using unsupervised neural networks (NNs) to obtain frame-level continuous vector representations [13\u201315].\nThe second area of zero-resource research deals with unsupervised segmentation and clustering of speech into meaningful units. This is important in tasks such as query-by-example search [16, 17], where a system needs to find all the utterances in a corpus containing a spoken query, or in unsupervised term discovery (UTD), where a system needs to automatically find repeated word- or phrase-like patterns in a speech collection [1, 18, 19]. UTD systems typically find and cluster only isolated acoustic segments, leaving the rest of the data as background. We are interested in full-coverage segmentation and clustering, where word boundaries and lexical categories are predicted for the entire input. Several recent studies share this goal [5, 20\u201323]. Successful full-coverage segmentation systems would perform a type of unsupervised speech recognition. This would allow downstream applications, such as query-byexample search and speech indexing (grouping together related utterances in a corpus), to be developed in a manner similar to when supervised systems are available.\nIn previous work [24] we introduced a novel unsupervised segmental Bayesian model for full-coverage segmentation and clustering of small-vocabulary speech. Other approaches mostly perform frame-by-frame modelling using subword discovery with subsequent or joint word discovery. In contrast, our approach models whole-word units directly using a fixed-dimensional embedding representation; any potential word segment (of arbitrary length) is mapped to a fixed-length vector, its acoustic word embedding, and the model builds a whole-word acoustic model in the embedding space while jointly performing segmentation.\nar X\niv :1\n60 6.\n06 95\n0v 1\n[ cs\n.C L\n] 2\n2 Ju\nn 20\n16\nIn [24] we evaluated the model in an unsupervised digit recognition task using the TIDigits corpus. Although it was able to accurately segment and cluster the small number of word types (lexical items) in the data, the same system could not be applied directly to multi-speaker data with larger vocabularies. This was due to the large number of embeddings that had to be computed, and the efficiency of the embedding method itself.\nIn this paper, we present a new system that uses the same overall framework as our previous small-vocabulary system, but with several changes designed to improve efficiency and speaker independence, allowing us to scale up to large-vocabulary multispeaker data. To our knowledge, this is the first full-coverage unsupervised speech recognition system to report results in this regime; previous systems have either focused on identifying isolated terms [1, 18, 19], were speaker-dependent [22, 23], or used only a small vocabulary [21, 24].\nFor our efficiency improvements, we use a bottom-up unsupervised syllable boundary detection method [23] to eliminate unlikely word boundaries, reducing the number of potential word segments that need to be considered. We also use a computationally much simpler embedding approach based on downsampling [25].\nFor better speaker-independent performance, we incorporate a frame-level representation learning method introduced in our previous work [26]: the correspondence autoencoder (cAE). The cAE uses noisy word pairs identified by an unsupervised term detection system to provide weak supervision for training a deep NN on aligned frame pairs; features are then extracted from one of the network layers. In [26] we showed that cAE frame-level features outperform traditional features (MFCCs) and GMM-based representations in a multi-speaker intrinsic evaluation. Here, we show that the cAE features also improve performance of our full-coverage multi-speaker segmentation and clustering system (relative to MFCC features).\nWe evaluate our approach in both speaker-dependent and speaker-independent settings on conversational speech datasets from two languages: English and Xitsonga. Xitsonga is an under-resourced southern African Bantu language [27]. These datasets were also used as part of the Zero Resource Speech Challenge (ZRS) at Interspeech 2015 [8] and we show that our system outperforms competing systems [8, 19, 23] on several of the ZRS metrics. In particular, we find that by proposing a consistent segmentation and clustering over a whole utterance, our approach makes better use of the bottom-up syllabic constraints than the purely bottom-up syllable-based system of [23]. Moreoever, we achieve similar F -scores for word tokens, types, and boundaries whether training in a speaker-dependent or speakerindependent mode."}, {"heading": "2. Related work", "text": "Below we first discuss related work on unsupervised representation learning, followed by unsupervised term discovery (which we also compare our approach to), and, finally, full-coverage segmentation and clustering of unlabelled speech."}, {"heading": "2.1. Unsupervised frame-level representation learning", "text": "Unsupervised representation learning, in this context, involves finding a frame-level mapping from input features to a new rep-\nresentation that makes it easier to discriminate between different linguistic units (normally subwords or words).\nEarly studies used bottom-up approaches operating directly on the acoustics. Zhang and Glass [9] successfully used posteriorgram features from an unsupervised GMM universal background model (UBM) for query-by-example search and term discovery. Similarly, Chen et al. [10] used posteriorgrams from a non-parameteric infinite GMM. Approaches using unsupervised HMMs to perform a bottom-up tokenization of speech include the successive state-splitting algorithm of Varadarajan et al. [11], the more traditional iterative re-estimation and unsupervised decoding procedure of Siu et al. [2], and the non-parameteric Bayesian HMM of Lee and Glass [12]. More recently, NNs have been used for bottom-up representation learning: stacked autoencoders (AEs), a type of unsupervised deep NN that tries to reconstruct its input, has been used in several studies [28\u201330].\nThe above approaches perform representation learning without regard to longer-spanning word- or phrase-like patterns in the data. In several recent studies, unsupervised term discovery (UTD) is used to automatically discover such patterns; these then serve as weak top-down constraints for subsequent representation learning. Jansen et al. showed that such constraints can be used to train HMMs [31] and GMM-UBMs [32] that significantly outperform their pure bottom-up counterparts. In our own work [26], we proposed the correspondence autoencoder (cAE): an AE-like deep NN that incorporates top-down constraints by using aligned frames from discovered words as input-output pairs. The model significantly outperformed the top-down GMM-UBM [32] and stacked AEs [28,29] in an intrinsic evaluation: isolated word discrimination. Since then, several researchers have used such weak top-down supervision in training unsupervised NN-based models [13, 15, 33]. In this paper we show that cAE-learned features also improve performance of our multi-speaker unsupervised segmentation and clustering system."}, {"heading": "2.2. Unsupervised term discovery", "text": "Unsupervised term discovery (UTD) is the task of finding meaningful word- or phrase-like patterns in unlabelled speech data. Most state-of-the-art UTD systems use a variant of dynamic time warping (DTW), called segmental DTW. This algorithm, developed by Park and Glass [1], identifies similar sub-sequences within two vector time series, rather than comparing entire sequences as in standard DTW. In most UTD systems, segmental DTW proposes pairs of matching segments which are then clustered using a graph-based method. Follow-up work has built on Park and Glass\u2019 original method in various ways, for example through improved feature representations [16] or by greatly improving its efficiency [18].\nThe baseline provided as part of the lexical discovery track of the Zero Resource Speech Challenge 2015 (ZRS) [8] is a UTD system based on the earlier work of [18]. The other UTD submission to the ZRS by Lyzinski et al. [19] extended the baseline system using improved graph clustering algorithms. In our evaluation, we compare to both these systems. Our approach shares the property of UTD systems that it has no subword level of representation and operates directly on wholeword representations. However, instead of representing each segment as a vector time series with variable duration as in UTD, we map each potential word segment to a fixed-dimensional\nacoustic word embedding; we can then define an acoustic model in the embedding space and use it to compare segments without performing DTW alignment. Our system also performs fullcoverage segmentation and clustering, in contrast to UTD, which segments and clusters only isolated acoustic patterns."}, {"heading": "2.3. Full-coverage segmentation and clustering of speech", "text": "Our goal of entirely segmenting a corpus of speech into wordlike clusters is shared by several researchers. Approaches include using non-negative matrix factorization [5], using iterative decoding and refinement for jointly training subword HMMs and a lexicon [20], and using discrete HMMs to model whole words in terms of discovered subword units [21]. Below we highlight two studies which have inspired our work in particular.\nIn [22], Lee et al. developed a non-parametric hierarchical Bayesian model for full-coverage speech segmentation. Their model consists of a bottom subword acoustic modelling layer, a noisy channel model for capturing pronunciation variability, a syllable layer, and a highest-level word layer. When applied to speech from single speakers in the MIT Lecture corpus, most words with high TF-IDF scores were successfully discovered. As in their model, we also follow a Bayesian approach, which is useful for incorporating prior knowledge and for finding sparser solutions [34]. However, where [22] only considered singlespeaker data, we additionally evaluate on large-vocabulary multispeaker data.\nFurthermore, in contrast to [20\u201322], our model operates directly at the whole-word level instead of having both word and subword models. By taking this different perspective, our segmental whole-word approach is a complementary contribution to the field of zero-resource speech processing. The approach is further motivated by the observation that it is often easier to identify cross-speaker similarities between words than between subwords [32], which is why most UTD systems focus on longer-spanning patterns. There is also evidence that infants are able to segment whole words from continuous speech while still learning phonetic contrasts in their native language [35, 36]. A benefit of the segmental embedding approach we use is that segments can be compared directly in a fixed-dimensional embedding space, meaning that word discovery can be performed using standard clustering methods (in our case using a Bayesian GMM acoustic model). Finally, segmental approaches do not make the frame-level independence assumptions of most of the models above; this assumption has long been argued against [37, 38].\nThe second study we draw from is the ZRS submission of Ra\u0308sa\u0308nen et al. [23], which we use to help scale our approach to larger vocabularies. Their full-coverage word segmentation system relies on an unsupervised method that predicts boundaries for syllable-like units, and then clusters these units on a perspeaker basis. Using a bottom-up greedy mapping, reoccurring syllable clusters are then predicted as words. From here onward we use syllable to refer to the syllable-like units detected in the first step of their approach.\nIn our model, we incorporate the syllable boundary detection method of [23] (the first component of their system) as a presegmentation method to eliminate unlikely word boundaries. Both human infants [39] and adults [40] use syllabic cues for word segmentation, and using such a bottom-up unsupervised syllabifier can therefore be seen as one way to incorporate prior knowledge of the speech signal into a zero-resource system [41]."}, {"heading": "3. Large-vocabulary segmental Bayesian model", "text": "In the following we describe our large-vocabulary system in detail, starting with a high-level overview of the model, illustrated in Figure 1.\nThe model takes as input raw speech (bottom) and converts it to frame-level acoustic features using a sliding window feeding into the feature extracting function fa. The sequence of frame-level vectors (e.g. MFCCs or cAE features) are denoted as y1:M = y1,y2, . . . ,yM . Suppose we have a hypothesis for where word boundaries occur in this stream of features (vertical black lines, bottom of figure). Each word1 segment is then mapped to to an acoustic word embedding (coloured horizontal vectors in the figure) in a fixed-dimensional space RD; this is done using the embedding function fe, which takes a sequence of frame-level features as input and outputs a single embedding vector xi \u2208 RD. Ideally, embeddings of different instances of the same word type should lie close together in this space. The different hypothesized word types are then modelled using a whole-word acoustic model: a GMM with Bayesian priors in the D-dimensional embedding space (top of figure). Effectively, if word boundaries are known, this is simply a clustering model, with every cluster (mixture component) of the GMM corresponding to a discovered word type.\nInitially, however, we do not know where words start and end in the stream of features. But if we have a GMM acoustic model, we can use this model to segment an utterance by choosing word boundaries that yield segments (acoustic word embeddings) that have high probability under the acoustic model. Our full system therefore initializes word boundaries at random, extracts word embeddings, clusters them using the Bayesian GMM, and then iteratively re-analyzes each utterance (jointly re-segmenting it and re-clustering the segments) based on the current acoustic model. The result is a complete segmentation of the input speech and a prediction of the component to which every word segment belongs. The model is implemented as a single blocked Gibbs sampler, and exact details are given next."}, {"heading": "3.1. Segmental Bayesian modelling", "text": "Given the embedded word vectors X = {xi}Ni=1 from the current segmentation hypothesis, the acoustic model needs to assign each acoustic word embedding xi to one of K clusters, with each cluster corresponding to a hypothesized word type. We use a Bayesian GMM as acoustic model, with a conjugate Dirichlet prior over its mixture weights \u03c0 and a conjugate diagonalcovariance Gaussian prior over its component means {\u00b5k}Kk=1, which allows us to integrate out these parameters. The model, illustrated in Figure 2, is formally defined as:\n\u03c0 \u223c Dir (a/K1) (1) zi \u223c \u03c0 (2)\n\u00b5k \u223c N (\u00b50, \u03c320I) (3) xi \u223c N (\u00b5zi , \u03c32I) (4)\nLatent variable zi indicates the component to which xi is assigned. All K components share the same fixed covariance\n1Throughout we use the term word to refer to a segment of speech that might in reality correspond to a true word, partial word, phrase or noise, depending on what the system discovers. A more accurate description would be pseudo term, but we use word instead to match usage in earlier work [1, 25, 42].\nmatrix \u03c32I. The hyperparameters of the mixture components are denoted together as \u03b2 = (\u00b50, \u03c3 2 0 , \u03c3\n2). Given X , we infer the component assignments z = (z1, z2, . . . , zN ) using a collapsed Gibbs sampler [43]. This is done in turn for each zi conditioned on all the other current component assignments [24]:\nP (zi = k|z\\i,X ; a,\u03b2) \u221d P (zi = k|z\\i; a)p(xi|Xk\\i;\u03b2) (5)\nwhere z\\i is all latent component assignments excluding zi and Xk\\i is the set of embedding vectors assigned to component k apart from xi. The first term in (5) can be calculated as:\nP (zi = k|z\\i; a) = Nk\\i + a/K\nN + a\u2212 1 (6)\nwhere Nk\\i is the number of embedding vectors from mixture component k without taking xi into account [44, p. 843]. This term can be interpreted as a discounted unigram language modelling probability. The term p(xi|Xk\\i;\u03b2) in (5) is the posterior predictive of xi, which (because of the conjugate prior) is a spherical covariance Gaussian distribution with analytic expressions for its mean and covariance parameters [45]. Intuitively, component assignment sampling in (5) is therefore based on a combination of language model and acoustic scores.\nAbove we described clustering given the current segmentation. But segmentation and clustering are performed jointly: for\nAlgorithm 1: Gibbs sampler of the segmental Bayesian model. 1: Choose an initial segmentation (e.g. random). 2: for j = 1 to J do . Gibbs sampling iterations 3: for i = randperm(1 to S) do . Select utterance si 4: Remove embeddings X (si) from acoustic model. 5: Resample word boundaries for si, yielding new X (si) 6: for embedding xi in newly sampled X (si) do 7: Sample zi for embedding xi using (5). 8: end for 9: end for 10: end for\nthe utterance under consideration, a segmentation is sampled using the current acoustic model (marginalizing over cluster assignments for each potential segment), and clusters are then resampled for the newly created segments. Pseudo-code for the blocked Gibbs sampler that implements this algorithm is given in Algorithm 1. The acoustic data is denoted as {si}Si=1, where every utterance si consists of acoustic frames y1:Mi , and X (si) denotes the embedding vectors under the current segmentation for utterance si. In Algorithm 1, an utterance si is randomly selected; the embeddings from the current segmentation X (si) are removed from the Bayesian GMM; a new segmentation is sampled; and finally the embeddings from this new segmentation are added back into the Bayesian GMM. Line 5 uses the forward filtering backward sampling dynamic programming algorithm [46] to sample the new embeddings; details of this step are given in Appendix A."}, {"heading": "3.2. Unsupervised syllable boundary detection", "text": "Without any constraints, the input at the bottom of Figure 1 could be segmented into any number of possible words using a huge number of possible segmentations. In [24], potential word segments were therefore required to be between 200 ms and 1 s in duration, and word boundaries were only considered at 20 ms intervals. This still results in a very large number of possible segments. Here we instead use a syllable boundary detection method to eliminate unlikely word boundaries, with word candidates spanning a maximum of six syllables. On the waveform in Figure 1, solid and dashed lines are used to\nindicate the only positions where boundaries are considered during sampling, as determined by the syllabification method.\nRa\u0308sa\u0308nen et al. [23] evaluated several syllable boundary detection algorithms, and we use the best of these. First the envelope of the raw waveform is calculated by downsampling the rectified signal and applying a low-pass filter. Inspired by neuropsychological studies which found that neural oscillations in the auditory cortex occur at frequencies similar to that of the syllabic rhythm in speech, the calculated envelope is used to drive a discrete time oscillation system with a centre frequency of typical syllabic rhythm. Minima in the oscillator\u2019s amplitude give the predicted syllable boundaries. In this work, we use the syllabification code kindly provided by the authors of [23] without any modification and with the default parameter settings."}, {"heading": "3.3. Acoustic word embeddings and unsupervised representation learning", "text": "A simple and fast approach to obtain acoustic word embeddings is to uniformly downsample so that any segment is represented by the same fixed number of vectors [25,47]. A similar approach is to divide a segment into a fixed number of intervals and average the frames in each interval [23,48]. The downsampled or averaged frames are then flattened to obtain a single fixed-length vector. Although these very simple approaches are less accurate at word discrimination than the approach used before in [24], they have been effectively used in several studies, including [23], and are computationally much more efficient. Here we use downsampling as our acoustic word embedding function fe in Figure 1; we keep ten equally-spaced vectors from a segment and use a Fourier-based method for smoothing [25].\nFigure 1 shows that fe takes as input a sequence of framelevel features from the feature extracting function fa. One option for fa is to simply use MFCCs. As an alternative, we incorporate unsupervised representation learning (Section 2.1) into our approach by using the cAE as a feature extractor. Complete details of the cAE are given in [26], but we briefly outline the training procedure here. The UTD system of [18] is used to discover word pairs which serve as weak top-down supervision. The cAE operates at the frame level, so the word-level constraints are converted to frame-level constraints by aligning each word pair using DTW. Taken together across all discovered pairs, this results in a set of F frame-level pairs {( yi,a,yi,b )}F i=1\n. Here, each frame is a single MFCC vector. For every pair (ya,yb), ya is presented as input to the cAE while yb is taken as output, and vice versa. The cAE consists of several nonlinear layers which are initialized by pretraining the network as a standard autoencoder. The cAE is then tasked with reconstructing yb from ya, using the loss ||yb \u2212 ya||2. To use the trained network as a feature extractor fa, the activations in one of its middle layers are taken as the new feature representation."}, {"heading": "4. Experiments", "text": ""}, {"heading": "4.1. Experimental setup", "text": "We use three datasets, summarized in Table 1. The first two are disjoint subsets extracted from the Buckeye corpus of conversational English [49], while the third is a portion of the Xitsonga section of the NCHLT corpus of languages spoken in South Africa [27]. Xitsonga is a Bantu language spoken in southern\nAfrica; although it is considered under-resourced, more than five million people use it as their first language.2\nThe two sets extracted from Buckeye, referred to as English1 and English2, respectively contain five and six hours of speech, each from twelve speakers (six female and six male). The Xitsonga dataset consists of 2.5 hours of speech from 24 speakers (twelve female, twelve male). English2 and the Xitsonga data were used as test sets in the ZRS challenge, so we can compare our system to others using the same data and evaluation framework [8]. English1 was extracted for development purposes from a disjoint portion of Buckeye to match the distribution of speakers in English2. For all three sets, speech activity regions are taken from forced alignments of the data, as was done in the ZRS. From Table 1, the average duration of a word in an English set is around 250 ms, while for Xitsonga it is about 450 ms.\nOur model is unsupervised, which means that the concepts of training and test data become blurred. We run our model on all sets separately\u2014in each case, unsupervised modelling and evaluation is performed on the same set. English1 is the only set used for any development (specifically for setting hyperparameters) in any of the experiments; both English2 and Xitsonga are treated as unseen final test sets. This allows us to see how hyperparameters generalize within language on data of similar size, as well as across language on a corpus with very different characteristics."}, {"heading": "4.2. Evaluation", "text": "The evaluation of zero-resource systems that segment and cluster speech is a research problem in itself [50]. We use a range of metrics that have been proposed before, all performing some mapping from the discovered structures to ground truth forced alignments of the data, as illustrated in Figure 3.\nAverage cluster purity first aligns every discovered token to the ground truth word token with which it overlaps most. In Figure 3 the token assigned to cluster 931 would be mapped to the true word \u2018yeah\u2019, and the 477-token mapped to \u2018mean\u2019. Every discovered word type (cluster) is then mapped to the most common ground truth word type in that cluster. E.g. if most of the other tokens in cluster 477 are also labelled as \u2018yeah\u2019, then cluster 477 would be labelled as \u2018yeah\u2019. Average purity is then defined as the total proportion of correctly mapped tokens in all clusters. For this metric, more than one cluster may be mapped to a single ground truth type (i.e. many-to-one) [5].\nUnsupervised word error rate (WER/WERm) uses a a similar word-level mapping and then aligns the mapped decoded output from a system to the ground truth transcriptions [20, 21]. Based on this alignment we calculate WER = S+D+IN , with S the number of substitutions, D deletions, I insertions, and\n2http://www.ethnologue.com/language/tso\nN the tokens in the ground truth. The cluster mapping can be done in one of two ways: many-to-one, where more than one cluster can be assigned the same word label (as in purity), or using a greedy one-to-one mapping, where at most one cluster is mapped to a ground truth word type. The latter, which we denote simply as WER, might leave some cluster unassigned and these are counted as errors [24]. For the former, denoted as WERm, all clusters are labelled. Depending on the downstream speech task, it might be acceptable to have multiple clusters that correspond to the same true word; WER penalizes such clusters, while WERm does not. WER is a useful metric since it is easily interpretable and well-known in the speech community.\nNormalized edit distance (NED) is the first of the ZRS metrics (the rest follow). These metrics use a phoneme-level mapping: each discovered token is mapped to the sequence of ground truth phonemes of which at least 50% or 30 ms are covered by the discovered segment [8, 50]. In Figure 3, the 931-token would be mapped to /y ae/ and the 477-token to /ay m iy n/. For a pair of discovered segments, the edit distance between the two phoneme strings is divided by the maximum of the length of the two strings. This is averaged over all pairs predicted to be of the same type (cluster), to obtain the final NED score. If all segments in each cluster have the same phoneme string, then NED = 0, while if all phonemes are different, NED = 1. NED is useful in that it does not make the assumptions that the discovered segments need to correspond to true words (as in cluster purity and WER), and it only considers the patterns returned by a system (so it does not require full coverage, as WER does). As an example, if a cluster contains /m iy/ from a realization of the word \u2018meaningful\u2019 and a token /m iy n/ from the true word \u2018mean\u2019, then NED would be 1/3 for this two-token cluster.\nWord boundary precision, recall, F -score are calculated by comparing word boundary positions proposed by a system to those from forced alignments of the data, falling within some tolerance. A tolerance of 20 ms is mostly used [22], but for the ZRS the tolerance is 30 ms or 50% of a phoneme (to match the mapping). In Figure 3 the detected boundary (dashed line) would be considered correct if it is within the tolerance from the true word boundary between \u2018yeah\u2019 and \u2018i\u2019.\nWord token precision, recall, F -score compare how accurately proposed word tokens match ground truth word tokens in the data. In contrast to the word boundary scores, both boundaries of a predicted word token need to be correct. In Figure 3, the system would receive credit for the 931-token since it is mapped to /y ae/ and therefore match the ground truth word token \u2018yeah\u2019. However, the system would be penalized for\nthe 477-token (mapped to /ay m iy n/) since it fails to predict word tokens corresponding to /ay/ and /m iy n/ (the ground truth words \u2018i\u2019 and \u2018mean\u2019). Both the word boundary and word token metrics give a measure of how accurately a system is segmenting its input into word-like units.\nWord type precision, recall, F -score compare the set of distinct phoneme mappings from the tokens returned by a system to the set of true word types in the ground truth alignments. If any discovered word token maps to a phoneme sequence that is also found as a word in the ground truth vocabulary, the system is credited for a correct discovery of that word type. For example if the type /y ae/ (as in \u2018yeah\u2019) occurs in the ground truth alignment, the system needs to return at least one token that is mapped to /y ae/.\nWe evaluate our model in both speaker-dependent and speaker-independent settings. Multiple speakers make it more difficult to discover accurate clusters: non-matching linguistic units might be more similar within-speaker than matching units across speakers. For the speaker-dependent case, the model is run and scores are computed on each speaker individually, then performance is averaged over speakers. In the speakerindependent case, the system is run and scores computed over the entire multi-speaker dataset at once. This typically results in worse purity, NED and WERm scores since the task is more difficult and clusters are noisier. WER is affected even more severely due to the one-to-one mapping that it uses; if there are two perfectly pure clusters that contain tokens from the same true word, but the two clusters are also perfectly speaker-dependent, then only one of these clusters would be mapped to the true word type and the other would be counted as errors. Despite the adverse effect on these metrics, it is of practical importance to evaluate a zero-resource system in the speaker-independent setting."}, {"heading": "4.3. Model development and hyperparameters", "text": "Most model hyperparameters are set according to previous work. Any changes are based exclusively on performance on English1.\nTraining parameters for the cAE (Section 3.3) are based on [14, 26]. The model is pretrained on all data (in a particular set) for 5 epochs using minibatch stochastic gradient descent with a batch size of 2048 and a fixed learning rate of 2 \u00b7 10\u22123. Subsequent correspondence training is performed for 120 epochs using a learning rate of 32 \u00b7 10\u22123. Each pair is presented in both directions as input and output. Pairs are extracted using the UTD system of [18]: for English1, 14 494 word pairs are discovered; for English2, 10 769 pairs; and for Xitsonga, 6979. The cAE is trained on each of these sets separately. In all cases, the model consists of nine hidden layers of 100 units each, except for the eighth layer which is a bottleneck layer of 13 units. We use tanh as non-linearity. The position of the bottleneck layer is based on intrinsic evaluation on English1. Although it is common in NN speech systems to use nine or eleven sliding frames as input, we use single-frame MFCCs with first and second order derivatives (39-dimensional), as also done in [14, 26]. For feature extraction, the cAE is cut at the bottleneck layer, resulting in 13-dimensional output (chosen to match the dimensionality of the static MFCCs). For both the MFCC and cAE acoustic word embeddings, we downsample a segment to ten frames, resulting in 130-dimensional embeddings. As in [24, 51, 52], embeddings are normalized to the unit sphere.\nFor the acoustic model (Section 3.1) we use the following\nhyperparameters, as in [24, 51, 52]: all-zero vector for \u00b50, \u03c3 2 0 = \u03c32/\u03ba0, \u03ba0 = 0.05 and a = 1. For MFCC embeddings we use \u03c32 = 1 \u00b7 10\u22123 for the fixed shared spherical covariance matrix, while for cAE embeddings we use \u03c32 = 1 \u00b7 10\u22124. This was based on speaker-dependent English1 performance. We found that \u03c32 is one of the parameters most sensitive to the input representation and often requires tuning; generally, however, it is robust if it is chosen small enough (in the ranges used here).\nWe use the oscillator-based syllabification system of Ra\u0308sa\u0308nen et al. [23] without modification. Word candidates are limited to span a maximum of six syllables. One difficulty is to decide beforehand how many potential word clusters (the number of components K in the acoustic model) we need. Here we follow the same approach as in [23]: we choose K as a proportion of the number of discovered syllable tokens. For the speaker-dependent settings, we set K as 20% of the number of syllables, based on English1 performance. On average, this amounts to K = 1549 on English1, K = 1195 on English2, and K = 298 on Xitsonga. Compared to the average number of word types per speaker shown in Table 1, these numbers are higher for the English sets and slightly lower for Xitsonga. For speaker-independent models, we use 5% of the syllable tokens, amounting to K = 4647 on English1, K = 3584 on English2, and K = 1789 on Xitsonga. These are lower than the true number of total word types shown in Table 1. On English1, speaker-independent performance did not improve when using a larger K and inference was much slower.\nTo improve sampler convergence, we use simulated annealing [24]. We found that convergence is improved by first running the sampler in Algorithm 1 without sampling boundaries. In all experiments we do this for 15 iterations. Subsequently, the complete sampler is run for J = 15 Gibbs sampling iterations with 3 annealing steps. Word boundaries are initialized randomly by setting boundaries at allowed locations with a 0.25 probability.\nGiven the common setup above, we consider three variants of our approach:\nBayesSeg is the most general segmental Bayesian model. In this model, a word segment can be of any duration, as long as it spans less than six syllables.\nBayesSegMinDur is the same as BayesSeg, but requires word candidates to be at least 250 ms in duration; on English1, this improved performance on several metrics. Such a minimum duration constraint is also used in most UTD systems [1, 18].\nSyllableBayesClust clusters the discovered syllable tokens using the Bayesian GMM, but does not sample word boundaries. It can be seen as a baseline for the two models above, where segmentation is turned off and the detected syllable boundaries are set as initial (and permanent) word boundaries. All word candidates therefore span a single syllable in this model."}, {"heading": "4.4. Results: Word error rates and analysis", "text": ""}, {"heading": "Speaker-dependent models", "text": "Table 2 shows one-to-one and many-to-one WERs for the different speaker-dependent models on the three datasets. The trends in WER using one-to-one and many-to-one mappings are similar, with the absolute performance of the latter consistently better by around 10% to 20% absolute. The performance on Xitsonga varies much more dramatically than on the English datasets, with WER ranging from around 140% to 75% and WERm from 135% to 69%.3 Table 1 shows that the characteristics of the Xitsonga data are quite different from the English sets. For the speaker-dependent case here, much less data is available per Xitsonga speaker (just over six minutes on average) than for an English speaker (more than ten minutes), which might (at least partially) explain why error rates vary much more dramatically on Xitsonga. Moreover, there is a much higher proportion of multisyllabic words in Xitsonga [23], as reflected in the average duration of words which is almost twice as long in the Xitsonga than in the English data (Section 4.1).\nComparing the results for the three systems using MFCC features indicates that, on all three datasets, allowing the system to infer word boundaries across multiple syllables (BayesSeg) yields better performance than treating each syllable as a word candidate (SyllableBayesClust). Incorporating a minimum duration constraint (BayesSegMinDur) improves performance further. The relative differences between these systems are much more pronounced in Xitsonga, presumably due to the higher proportion of multisyllabic words. Table 2 also shows that in most cases the cAE features perform similarly to MFCC features in these speaker-dependent systems, although there is a large improvement in Xitsonga for the BayesSeg system when switching to cAE features (from 116.2% to 107.9% in WER and from 109.5% to 100.5% in WERm).\nTo get a better insight into the types of errors that the models make, Tables 3 and 4 give a breakdown of word boundary detection scores, individual error rates, and average cluster purity on English2 and Xitsonga, respectively. A word boundary tolerance of 20 ms is used [22], with a greedy one-to-one mapping for calculating error rates. SyllableBayesClust gives an upper-bound for word boundary recall since every syllable boundary is set as a word boundary. The low recall (28.9% and 24.8%) could potentially be improved by using a better syllabification method, but we leave such an investigation for future work.\nTable 3 shows that on English2, the MFCC-based BayesSeg and BayesSegMinDur models under-segment compared to Syl-\n3From its definition, WER is more than 100% if there are more substitutions, deletions and insertions than ground truth tokens.\nlableBayesClust, causing systematically poorer word boundary recall and F -scores and an increase in deletion errors. However, this is accompanied by large reductions in substitution and insertion error rates, resulting in overall WER improvements and more accurate clusters when boundaries are inferred (45.1% purity, BayesSeg-MFCC) rather than using fixed syllable boundaries (42%, SyllableBayesClust), with further improvements when not allowing short word candidates (56%, BayesSegMinDur-MFCC).\nIn contrast to English2, Table 4 shows that on Xitsonga, SyllableBayesClust heavily over-segments causing a large number of insertion errors. This is not surprising since every syllable is treated as a word, while most of the true Xitsonga words are multisyllabic. At the cost of more deletions and poorer word boundary detection, BayesSeg-MFCC and BayesSegMinDurMFCC systematically reduces substitution and insertion errors, again resulting in better overall WER and average cluster purity. Where the cAE-based models on English2 performed moreor-less on par with their MFCC counterparts, on Xitsonga the cAE embeddings yield large improvements on some metrics: by switching to cAE embeddings, the WER of BayesSeg improves by 8.3% absolute, while average cluster purity is 13.6% better for BayesSegMinDur."}, {"heading": "Speaker-independent models", "text": "Table 5 gives the performance of different speaker-independent models. Compared to the speaker-dependent results of Table 2, performance is worse for all models and datasets. As in the speaker-dependent case, BayesSegMinDur is the best performing MFCC system, followed by BayesSeg, and SyllableBayesClust performs worst. In the speaker-dependent experiments, some MFCC-based models slightly outperformed their cAE counterparts. Here, however, the WERs of cAE models are identical or improved in all cases; for Xitsonga in particular, improvements are obtained by using cAE features in\nboth BayesSeg (improvement of 26.3% absolute in WER) and BayesSegMinDur (7.4%). The cAE-based BayesSegMinDur model is the only speaker-independent Xitsonga model with a WER less than 100%. Again, by allowing more than one cluster to be mapped the same true word type, WERm scores are lower than WER. On English, the cAE-based models don\u2019t yield better WERm than their MFCC counterparts, probably because WERm doesn\u2019t penalize for creating separate speakeror gender-specific clusters (these would just get mapped to the same word for scoring). Nevertheless, the cAE features still yield large improvements in Xitsonga. Word boundary scores and substitution, deletion and insertion errors (not shown) follow a similar pattern to that of the speaker-dependent models.\nTo better illustrate the benefits of unsupervised representation learning, Table 6 shows general purity measures for the speakerindependent MFCC- and cAE-based BayesSegMinDur models. Average cluster purity is as defined before. Average speaker purity is similarly defined, but instead of considering the mapped ground truth label of a segmented token, it considers the speaker who produced it: speaker purity is 100% if every cluster contains tokens from a single speaker, while it is 1/12 = 8.3% if all clusters are completely speaker balanced for the English sets and 1/24 = 4.2% for Xitsonga. Average gender purity is similarly defined: it is 100% if every cluster contains tokens from a single gender, while 1/2 = 50% indicates a perfectly gender-balanced cluster. Ideally, a speaker-independent system should have high cluster purity and low speaker and gender purities. Table 6 indicates that for all three datasets, cAE-based embeddings are less speaker and gender discriminative, and have higher or similar cluster purity compared to the MFCC-based embeddings."}, {"heading": "Qualitative analysis and summary", "text": "Qualitative analysis involved concatenating and listening to the audio from the tokens in some of the biggest clusters of the best speaker-dependent and -independent models. Apart from the\nEmbeds. Clust. Spk. Gndr Clust. Spk. Gndr Clust. Spk. Gndr\nMFCC 30.3 56.7 86.8 29.9 55.9 87.6 24.5 43.1 87.1 cAE 31.5 37.9 77.0 30.0 35.7 73.8 33.1 29.3 76.6\ntrends mentioned already, others also became immediately apparent. Despite the low average cluster purity ranging from 30% to 60% in the analyses above, we found that most of the clusters are acoustically very pure: often tokens correspond to the same syllable or partial word, but occur within different ground truth words. For example, a cluster with the word \u2018day\u2019 had the corresponding portions from \u2018daycare\u2019 and \u2018Tuesday\u2019. These are marked as errors for cluster purity and WER calculations. In the next section, we use NED as metric, which does not penalize such partial word matches. The biggest clusters often correspond to filler-words. As an example, speaker S38 from English1 had several clusters corresponding to \u2018yeah\u2019 and \u2018you know\u2019. But the BayesSegMinDur-MFCC model applied to S38 also discovered pure clusters corresponding to \u2018different\u2019, \u2018people\u2019 and \u2018five\u2019. For the speaker-independent BayesSegMinDur-cAE system, the biggest clusters consisted of instances of \u2018um\u2019, \u2018uh\u2019, \u2018oh\u2019, \u2018so\u2019 and \u2018yeah\u2019.\nIn summary, although under-segmentation occurs in the BayesSeg and BayesSegMinDur models, these models yield more accurate clusters and thereby improve overall purity and WER. In most cases, cAE embeddings either yield similar or improved performance compared to MFCCs. In particular in the speaker-independent case, cAE-based models discover clusters that are more speaker- and gender-independent. This illustrates the benefit of incorporating weak top-down supervision for unsupervised representation learning within a zero-resource system."}, {"heading": "4.5. Results: Comparison to other systems", "text": "We now compare our approach to others using the evaluation framework provided as part of the ZRS challenge [8]. We compare our approach to three systems:\nZRSBaselineUTD is the UTD system used as official baseline in the challenge [8] (see Section 2.2).\nUTDGraphCC is the best UTD system of [19], employing a connected component graph clustering algorithm to group discovered segments (also Section 2.2).\nSyllableSegOsc+ uses oscillator-based syllabification followed by speaker-dependent clustering and word discovery [23]\n(Section 2.3). We add the superscript + since, after publication of [23], Ra\u0308sa\u0308nen et al. further refined their syllable boundary detection method [53]. We use this updated version for presegmentation in our system. The authors of [23] kindly regenerated their full ZRS results for comparison here. The original results are included in Appendix B.\nFor our approach, we focus on systems that performed best on English1 in the previous section: for the speaker-dependent setting we use the MFCC-based BayesSegMinDur system, while for the speaker-independent setting we use the cAE-based BayesSegMinDur model. The performance of all our system variants using all of the ZRS metrics are given in Appendix B.\nFigure 4 shows the NED scores of the different systems on English2 and Xitsonga. ZRSBaselineUTD yields the best NED on both languages, with UTDGraphCC also performing well. UTD systems like these explicitly aim to discover high-precision clusters of isolated segments, but do not cover all the data. They are therefore tailored to NED, which only evaluates the patterns discovered by the method and does not evaluate recall on the rest of the data. In contrast, SyllableSegOsc+ and our own systems perform full-coverage segmentation. Of these, our systems achieve better NED than SyllableSegOsc+ on both languages, indicating that the discovered clusters in our approach are more consistent. Even when running our system in a speaker-independent setting (BayesSegMinDur-cAE in the figure), our approach outperforms the speaker-dependent SyllableSegOsc+.\nFigures 5 and 6 show the token, type and boundary F -scores on the two languages. Apart from word type F -score on Xit-\nsonga, our models outperform all other approaches. The UTD systems struggle on these metrics since the F -scores are based on precision and recall over the entire input. The full-coverage SyllableSegOsc+ is therefore our strongest competitor in most cases. The prediction of word candidates from reoccurring cluster sequences in SyllableSegOsc+ is done greedily and bottomup, without regard to other word mappings in an utterance. In contrast, BayesSegMinDur samples word boundaries and cluster assignments together by taking a whole utterance into account; it imposes a consistent top-down segmentation, while simultaneously adhering to bottom-up syllable boundary detection and minimum duration constraints. The result is a more accurate segmentation of the data. Note that in BayesSeg it is easy to incorporate additional bottom-up constraints (such as a minimum duration) and these are considered jointly with segmentation. In contrast, such a minimum duration constraint would require additional heuristics in the pure bottom-up approach of [23].\nThe results in Figures 5 and 6 also indicate that our speakerindependent system performs on par with the speaker-dependent system on these metrics; despite less accurate clusters (in terms of purity, WER and NED), the speaker-independent models still yields an accurate segmentation of the data, outperforming both speaker-independent UTD baselines and the speaker-dependent\nSyllableSegOsc+. We conclude that by hypothesizing word boundaries consistently over an utterance rather than taking these decisions in isolation, our approach yields more accurate clusters (NED) that correspond better to true words (word type F -score) than the full-coverage syllable-based approach of [23]. It also segments the data more accurately (word token and boundary F -scores), even when applying the model to data from multiple speakers. However, despite the benefits of our model, the algorithm of [23] is much simpler in terms of computational complexity and implementation. Compared to UTD systems which aim to find high-quality reoccurring patterns but do not cover all the data, the items in our clusters have a poorer match to each other (NED), but correspond better to true words on the English data (word type F -score). On both languages, our full-coverage method also segments the data better into word-like units (word boundary and token F -scores) than the UTD systems."}, {"heading": "5. Conclusion", "text": "We presented a segmental Bayesian model which segments and clusters conversational speech audio\u2014the first full-coverage zero-resource system to be evaluated on multi-speaker largevocabulary data. The system limits word boundary positions by using a bottom-up presegmentation method to detect syllablelike units, and relies on a segmental approach where word segments are represented as fixed-dimensional acoustic word embeddings.\nOur speaker-dependent systems achieves WERs of around 84% on English and 76% on Xitsonga data, outperforming a purely bottom-up method that treats each syllable as a word candidate. Despite much worse speaker-independent performance, here we achieve improvements by incorporating frame-level features from an autoencoder-like neural network trained using weak top-down constraints. This results in clusters that are purer and less speaker- and gender-specific than when using MFCCs, showing that unsupervised representation learning is especially useful for dealing with multiple speakers.\nWe compared our approach to state-of-the-art baselines on both languages. We found that, although the isolated patterns discovered by UTD are more consistent, the clusters of our fullcoverage approach are better matched to true words, measured in terms of word token, type and boundary F -scores. We also found that by proposing a consistent segmentation and clustering over whole utterances, our approach outperforms a purely bottom-up syllable-based full-coverage system on these metrics.\nFuture work will consider better acoustic word embedding approaches, improving the recall of the syllabic presegmentation method, and improving the overall efficiency of the model."}, {"heading": "Acknowledgements", "text": "We would like to thank Okko Ra\u0308sa\u0308nen and Shreyas Seshadri for providing the code for their syllable boundary detection algorithm and for regenerating their ZRS results. We also thank Roland Thiollie\u0300re and Maarten Versteegh for providing us the alignments used in the ZRS challenge. HK is funded by a Commonwealth Scholarship. This work was supported in part by a James S. McDonnell Foundation Scholar Award to SG."}, {"heading": "Appendices", "text": ""}, {"heading": "A. Forward filtering backward sampling for word segmentation", "text": "To sample the new set of embeddings in line 5 of Algorithm 1, the forward filtering backward sampling dynamic programming algorithm is used [46]. Forward variable \u03b1[t] is defined as the density of the frame sequence y1:t, with the last frame the end of a word: \u03b1[t] , p(y1:t|h\u2212). The embeddings and component assignments for all words not in the current utterance si, and the hyperparameters of the GMM, are denoted as\nh\u2212 = (X\\s, z\\s; a,\u03b2). The forward variables can be recursively calculated as [54]:\n\u03b1[t] = t\u2211 j=1 p(yt\u2212j+1:t|h\u2212)\u03b1[t\u2212 j] (7)\nstarting with \u03b1[0] = 1 and calculating (7) for 1 \u2264 t \u2264 M \u2212 1. The p(yt\u2212j+1:t|h\u2212) term in (7) is the value of a joint probability density function (PDF) over acoustic frames yt\u2212j+1:t. In analogy to a frame-based supervised model where this term would be calculated as the product of the PDF values of a GMM for all the frames involved, we define this term as\np(yt\u2212j+1:t|h\u2212) , [ p ( x\u2032|h\u2212 )]j (8)\nwhere x\u2032 = fe(yt\u2212j+1:t) is the acoustic word embedding calculated on the segment. Thus, as in the frame-based supervised case, each frame is assigned a PDF score; but in this case, all j frames in the segment are assigned the PDF value of the whole segment under the current acoustic model. The required marginal term in (8) can be calculated as:\np(x\u2032|h\u2212) = K\u2211\nk=1\nP (zh = k|z\\h; a)p(x\u2032|Xk\\h;\u03b2) (9)\nwith the two terms in the summation calculated in the same way as those in (5).\nOnce all \u03b1\u2019s have been calculated, a segmentation can be sampled backwards. Starting from the final positition t = M , we sample the preceding word boundary position using [54]:\nP (qt = j|y1:t, h\u2212) \u221d p(yt\u2212j+1:t|h\u2212)\u03b1[t\u2212 j] (10)\nVariable qt is the number of frames that we need to move backwards from position t to find the preceding word boundary. We calculate (10) for 1 \u2264 j \u2264 t and sample while t\u2212 j \u2265 1."}, {"heading": "B. Tables of complete results for all systems and metrics", "text": "In Section 4.4, several variants of our approach were considered. In Section 4.5, a subset of these were compared to other systems evaluated in the context of the Zero Resource Speech Challenge 2015 (ZRS) [8], using a subset of the challenge metrics. Tables 7 and 8 give the performance of all variants of our system on all the ZRS metrics on the English and Xitsonga data, respectively."}, {"heading": "Speaker-independent, cAE embeddings:", "text": ""}, {"heading": "Speaker-independent, MFCC embeddings:", "text": ""}, {"heading": "Speaker-dependent, cAE embeddings:", "text": ""}, {"heading": "Speaker-dependent, MFCC embeddings:", "text": ""}, {"heading": "Speaker-independent, cAE embeddings:", "text": ""}, {"heading": "Speaker-independent, MFCC embeddings:", "text": ""}, {"heading": "Speaker-dependent, cAE embeddings:", "text": ""}, {"heading": "Speaker-dependent, MFCC embeddings:", "text": ""}], "references": [{"title": "Unsupervised pattern discovery in speech", "author": ["A.S. Park", "J.R. Glass"], "venue": "IEEE Trans. Audio, Speech, Language Process., vol. 16, no. 1, pp. 186\u2013197, 2008.", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2008}, {"title": "Unsupervised training of an HMM-based self-organizing unit recognizer with applications to topic classification and keyword discovery", "author": ["M.-H. Siu", "H. Gish", "A. Chan", "W. Belfield", "S. Lowe"], "venue": "Comput. Speech Lang., vol. 28, no. 1, pp. 210\u2013223, 2014.", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2014}, {"title": "NLP on spoken documents without ASR", "author": ["M. Dredze", "A. Jansen", "G. Coppersmith", "K. Church"], "venue": "Proc. EMNLP, 2010.", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2010}, {"title": "Computational modeling of phonetic and lexical learning in early language acquisition: Existing models and future directions", "author": ["O.J. R\u00e4s\u00e4nen"], "venue": "Speech Commun., vol. 54, pp. 975\u2013997, 2012.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2012}, {"title": "Joint training of non-negative Tucker decomposition and discrete density hidden Markov models", "author": ["M. Sun", "H. Van hamme"], "venue": "Comput. Speech Lang., vol. 27, no. 4, pp. 969\u2013988, 2013.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2013}, {"title": "Symbol emergence in robotics: A survey", "author": ["T. Taniguchi", "T. Nagai", "T. Nakamura", "N. Iwahashi", "T. Ogata", "H. Asoh"], "venue": "arXiv preprint arXiv:1509.08973, 2015.", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2015}, {"title": "A summary of the 2012 JHU CLSP workshop on zero resource speech technologies and models of early language acquisition", "author": ["A. Jansen", "E. Dupoux", "S.J. Goldwater", "M. Johnson", "S. Khudanpur", "K. Church", "N. Feldman", "H. Hermansky", "F. Metze", "R. Rose", "M. Seltzer", "P. Clark", "I. McGraw", "B. Varadarajan", "E. Bennett", "B. Borschinger", "J. Chiu", "E. Dunbar", "A. Fourtassi", "D. Harwath", "C.-y. Lee", "K. Levin", "A. Norouzian", "V. Peddinti", "R. Richardson", "T. Schatz", "S. Thomas"], "venue": "Proc. ICASSP, 2013.", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2013}, {"title": "The Zero Resource Speech Challenge 2015", "author": ["M. Versteegh", "R. Thiolli\u00e8re", "T. Schatz", "X.N. Cao", "X. Anguera", "A. Jansen", "E. Dupoux"], "venue": "Proc. Interspeech, 2015.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2015}, {"title": "Towards multi-speaker unsupervised speech pattern discovery", "author": ["Y. Zhang", "J.R. Glass"], "venue": "Proc. ICASSP, 2010.", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2010}, {"title": "Parallel inference of Dirichlet process Gaussian mixture models for unsupervised acoustic modeling: A feasibility study", "author": ["H. Chen", "C.-C. Leung", "L. Xie", "B. Ma", "H. Li"], "venue": "Proc. Interspeech, 2015.", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2015}, {"title": "Unsupervised learning of acoustic sub-word units", "author": ["B. Varadarajan", "S. Khudanpur", "E. Dupoux"], "venue": "Proc. ACL, 2008.", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2008}, {"title": "A nonparametric Bayesian approach to acoustic model discovery", "author": ["C.-y. Lee", "J.R. Glass"], "venue": "Proc. ACL, 2012.", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2012}, {"title": "Phonetics embedding learning with side information", "author": ["G. Synnaeve", "T. Schatz", "E. Dupoux"], "venue": "Proc. SLT, 2014.", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2014}, {"title": "A comparison of neural network methods for unsupervised representation learning on the Zero Resource Speech Challenge", "author": ["D. Renshaw", "H. Kamper", "A. Jansen", "S.J. Goldwater"], "venue": "Proc. Interspeech, 2015.", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2015}, {"title": "A deep scattering spectrum-deep Siamese network pipeline for unsupervised acoustic modeling", "author": ["N. Zeghidour", "G. Synnaeve", "M. Versteegh", "E. Dupoux"], "venue": "Proc. ICASSP, 2016.", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2016}, {"title": "Resource configurable spoken query detection using deep Boltzmann machines", "author": ["Y. Zhang", "R. Salakhutdinov", "H.-A. Chang", "J.R. Glass"], "venue": "Proc. ICASSP, 2012.", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2012}, {"title": "Segmental acoustic indexing for zero resource keyword search", "author": ["K. Levin", "A. Jansen", "B. Van Durme"], "venue": "Proc. ICASSP, 2015.", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2015}, {"title": "Efficient spoken term discovery using randomized algorithms", "author": ["A. Jansen", "B. Van Durme"], "venue": "Proc. ASRU, 2011.", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2011}, {"title": "An evaluation of graph clustering methods for unsupervised term discovery", "author": ["V. Lyzinski", "G. Sell", "A. Jansen"], "venue": "Proc. Interspeech, 2015.", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2015}, {"title": "Unsupervised discovery of linguistic structure including two-level acoustic patterns using three cascaded stages of iterative optimization", "author": ["C.-T. Chung", "C.-a. Chan", "L.-s. Lee"], "venue": "Proc. ICASSP, 2013.", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2013}, {"title": "A hierarchical system for word discovery exploiting DTW-based initialization", "author": ["O. Walter", "T. Korthals", "R. Haeb-Umbach", "B. Raj"], "venue": "Proc. ASRU, 2013.", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2013}, {"title": "Unsupervised lexicon discovery from acoustic input", "author": ["C.-y. Lee", "T. O\u2019Donnell", "J.R. Glass"], "venue": "Trans. ACL, vol. 3, pp. 389\u2013403, 2015.", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2015}, {"title": "Unsupervised word discovery from speech using automatic segmentation into syllablelike units", "author": ["O.J. R\u00e4s\u00e4nen", "G. Doyle", "M.C. Frank"], "venue": "Proc. Interspeech, 2015.", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2015}, {"title": "Unsupervised word segmentation and lexicon discovery using acoustic word embeddings", "author": ["H. Kamper", "A. Jansen", "S.J. Goldwater"], "venue": "IEEE/ACM Trans. Audio, Speech, Language Process., vol. 24, no. 4, pp. 669\u2013679, 2016.", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2016}, {"title": "Fixeddimensional acoustic embeddings of variable-length segments in low-resource settings", "author": ["K. Levin", "K. Henry", "A. Jansen", "K. Livescu"], "venue": "Proc. ASRU, 2013.", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2013}, {"title": "Unsupervised neural network based feature extraction using weak top-down constraints", "author": ["H. Kamper", "M. Elsner", "A. Jansen", "S.J. Goldwater"], "venue": "Proc. ICASSP, 2015.", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2015}, {"title": "A smartphone-based ASR data collection tool for under-resourced languages", "author": ["N.J. De Vries", "M.H. Davel", "J. Badenhorst", "W.D. Basson", "F. De Wet", "E. Barnard", "A. De Waal"], "venue": "Speech Commun., vol. 56, pp. 119\u2013131, 2014.", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2014}, {"title": "On rectified linear units for speech processing", "author": ["M.D. Zeiler", "M. Ranzato", "R. Monga", "M. Mao", "K. Yang", "Q.V. Le", "P. Nguyen", "A. Senior", "V. Vanhoucke", "J. Dean", "G.E. Hinton"], "venue": "Proc. ICASSP, 2013.", "citeRegEx": "28", "shortCiteRegEx": null, "year": 2013}, {"title": "An autoencoder based approach to unsupervised learning of subword units", "author": ["L. Badino", "C. Canevari", "L. Fadiga", "G. Metta"], "venue": "Proc. ICASSP, 2014.", "citeRegEx": "29", "shortCiteRegEx": null, "year": 2014}, {"title": "Discovering discrete subword units with binarized autoencoders and hidden-markovmodel encoders", "author": ["L. Badino", "A. Mereta", "L. Rosasco"], "venue": "Proc. Interspeech, 2015.", "citeRegEx": "30", "shortCiteRegEx": null, "year": 2015}, {"title": "Towards unsupervised training of speaker independent acoustic models", "author": ["A. Jansen", "K. Church"], "venue": "Proc. Interspeech, 2011.", "citeRegEx": "31", "shortCiteRegEx": null, "year": 2011}, {"title": "Weak top-down constraints for unsupervised acoustic model training", "author": ["A. Jansen", "S. Thomas", "H. Hermansky"], "venue": "Proc. ICASSP, 2013.", "citeRegEx": "32", "shortCiteRegEx": null, "year": 2013}, {"title": "A hybrid dynamic time warping-deep neural network architecture for unsupervised acoustic modeling", "author": ["R. Thiolli\u00e8re", "E. Dunbar", "G. Synnaeve", "M. Versteegh", "E. Dupoux"], "venue": "Proc. Interspeech, 2015.", "citeRegEx": "33", "shortCiteRegEx": null, "year": 2015}, {"title": "A fully Bayesian approach to unsupervised part-of-speech tagging", "author": ["S.J. Goldwater", "T.L. Griffiths"], "venue": "Proc. ACL, 2007.", "citeRegEx": "34", "shortCiteRegEx": null, "year": 2007}, {"title": "Mommy and me: familiar names help launch babies into speechstream segmentation", "author": ["H. Bortfeld", "J.L. Morgan", "R.M. Golinkoff", "K. Rathbun"], "venue": "Psychol. Sci., vol. 16, no. 4, pp. 298\u2013304, 2005.", "citeRegEx": "35", "shortCiteRegEx": null, "year": 2005}, {"title": "Learning phonetic categories by learning a lexicon", "author": ["N.H. Feldman", "T.L. Griffiths", "J.L. Morgan"], "venue": "Proc. CCSS, 2009.", "citeRegEx": "36", "shortCiteRegEx": null, "year": 2009}, {"title": "SCARF: a segmental conditional random field toolkit for speech recognition", "author": ["G. Zweig", "P. Nguyen"], "venue": "Interspeech, 2010.", "citeRegEx": "37", "shortCiteRegEx": null, "year": 2010}, {"title": "Don\u2019t multiply lightly: Quantifying problems with the acoustic model assumptions in speech recognition", "author": ["D. Gillick", "L. Gillick", "S. Wegmann"], "venue": "Proc. ASRU, 2011. 11", "citeRegEx": "38", "shortCiteRegEx": null, "year": 2011}, {"title": "Segmental and syllabic representations in the perception of speech by young infants", "author": ["P.D. Eimas"], "venue": "J. Acoust. Soc. Am., vol. 105, no. 3, pp. 1901\u20131911, 1999.", "citeRegEx": "39", "shortCiteRegEx": null, "year": 1901}, {"title": "Segmentation of continuous speech using phonotactics", "author": ["J.M. McQueen"], "venue": "J. Memory Lang., vol. 39, no. 1, pp. 21\u201346, 1998.", "citeRegEx": "40", "shortCiteRegEx": null, "year": 1998}, {"title": "The Zero Resource Speech Challenge 2015: Proposed approaches and results", "author": ["M. Versteegh", "X. Anguera", "A. Jansen", "E. Dupoux"], "venue": "Proc. SLTU, 2016.", "citeRegEx": "41", "shortCiteRegEx": null, "year": 2016}, {"title": "Deep convolutional acoustic word embeddings using word-pair side information", "author": ["H. Kamper", "W. Wang", "K. Livescu"], "venue": "Proc. ICASSP, 2016.", "citeRegEx": "42", "shortCiteRegEx": null, "year": 2016}, {"title": "Gibbs sampling for the uninitiated", "author": ["P. Resnik", "E. Hardisty"], "venue": "University of Maryland, College Park, MD, Tech. Rep., 2010.", "citeRegEx": "43", "shortCiteRegEx": null, "year": 2010}, {"title": "Machine Learning: A Probabilistic Perspective", "author": ["K.P. Murphy"], "venue": null, "citeRegEx": "44", "shortCiteRegEx": "44", "year": 2012}, {"title": "Conjugate Bayesian analysis of the Gaussian distribution", "author": ["\u2014\u2014"], "venue": "2007. [Online]. Available: http://www.cs.ubc.ca/\u223cmurphyk/ mypapers.html", "citeRegEx": "45", "shortCiteRegEx": null, "year": 2007}, {"title": "Bayesian methods for hidden Markov models", "author": ["S.L. Scott"], "venue": "J. Am. Stat. Assoc., vol. 97, no. 457, pp. 337\u2013351, 2002.", "citeRegEx": "46", "shortCiteRegEx": null, "year": 2002}, {"title": "Deep segmental neural networks for speech recognition", "author": ["O. Abdel-Hamid", "L. Deng", "D. Yu", "H. Jiang"], "venue": "Proc. Interspeech, 2013.", "citeRegEx": "47", "shortCiteRegEx": null, "year": 2013}, {"title": "Enhanced spoken term detection using support vector machines and weighted pseudo examples", "author": ["H.-y. Lee", "L.-s. Lee"], "venue": "IEEE Trans. Audio, Speech, Language Process., vol. 21, no. 6, pp. 1272\u20131284, 2013.", "citeRegEx": "48", "shortCiteRegEx": null, "year": 2013}, {"title": "The Buckeye corpus of conversational speech: Labeling conventions and a test of transcriber reliability", "author": ["M.A. Pitt", "K. Johnson", "E. Hume", "S. Kiesling", "W. Raymond"], "venue": "Speech Commun., vol. 45, no. 1, pp. 89\u201395, 2005.", "citeRegEx": "49", "shortCiteRegEx": null, "year": 2005}, {"title": "Bridging the gap between speech technology and natural language processing: an evaluation toolbox for term discovery systems", "author": ["B. Ludusan", "M. Versteegh", "A. Jansen", "G. Gravier", "X.-N. Cao", "M. Johnson", "E. Dupoux"], "venue": "Proc. LREC, 2014.", "citeRegEx": "50", "shortCiteRegEx": null, "year": 2014}, {"title": "Unsupervised lexical clustering of speech segments using fixeddimensional acoustic embeddings", "author": ["H. Kamper", "A. Jansen", "S. King", "S.J. Goldwater"], "venue": "Proc. SLT, 2014.", "citeRegEx": "51", "shortCiteRegEx": null, "year": 2014}, {"title": "Fully unsupervised small-vocabulary speech recognition using a segmental Bayesian model", "author": ["H. Kamper", "S.J. Goldwater", "A. Jansen"], "venue": "Proc. Interspeech, 2015.", "citeRegEx": "52", "shortCiteRegEx": null, "year": 2015}, {"title": "Pre-linguistic rhythmic segmentation of speech into syllabic units", "author": ["O.J. R\u00e4s\u00e4nen", "G. Doyle", "M.C. Frank"], "venue": "submission, 2016.", "citeRegEx": "53", "shortCiteRegEx": null, "year": 2016}], "referenceMentions": [{"referenceID": 0, "context": "Such methods can, for instance, make it possible to search through a corpus of unlabelled speech using voice queries [1], allow topics within speech utterances to be identified without supervision [2], or can be used to automatically cluster related spoken documents [3].", "startOffset": 117, "endOffset": 120}, {"referenceID": 1, "context": "Such methods can, for instance, make it possible to search through a corpus of unlabelled speech using voice queries [1], allow topics within speech utterances to be identified without supervision [2], or can be used to automatically cluster related spoken documents [3].", "startOffset": 197, "endOffset": 200}, {"referenceID": 2, "context": "Such methods can, for instance, make it possible to search through a corpus of unlabelled speech using voice queries [1], allow topics within speech utterances to be identified without supervision [2], or can be used to automatically cluster related spoken documents [3].", "startOffset": 267, "endOffset": 270}, {"referenceID": 3, "context": "Similar techniques are required to model how human infants acquire language from speech input [4], and for developing robotic applications that can learn a new language in an unknown environment [5, 6].", "startOffset": 94, "endOffset": 97}, {"referenceID": 4, "context": "Similar techniques are required to model how human infants acquire language from speech input [4], and for developing robotic applications that can learn a new language in an unknown environment [5, 6].", "startOffset": 195, "endOffset": 201}, {"referenceID": 5, "context": "Similar techniques are required to model how human infants acquire language from speech input [4], and for developing robotic applications that can learn a new language in an unknown environment [5, 6].", "startOffset": 195, "endOffset": 201}, {"referenceID": 6, "context": "Interest in zero-resource speech processing has grown considerably in the last few years, with two central research areas emerging [7,8].", "startOffset": 131, "endOffset": 136}, {"referenceID": 7, "context": "Interest in zero-resource speech processing has grown considerably in the last few years, with two central research areas emerging [7,8].", "startOffset": 131, "endOffset": 136}, {"referenceID": 8, "context": "Approaches include those using bottom-up trained Gaussian mixture models (GMMs) to produce frame-level posteriorgrams [9, 10], using unsupervised hidden Markov models (HMMs) to obtain discrete categorical output in terms of discovered subword units [2, 11, 12], and using unsupervised neural networks (NNs) to obtain frame-level continuous vector representations [13\u201315].", "startOffset": 118, "endOffset": 125}, {"referenceID": 9, "context": "Approaches include those using bottom-up trained Gaussian mixture models (GMMs) to produce frame-level posteriorgrams [9, 10], using unsupervised hidden Markov models (HMMs) to obtain discrete categorical output in terms of discovered subword units [2, 11, 12], and using unsupervised neural networks (NNs) to obtain frame-level continuous vector representations [13\u201315].", "startOffset": 118, "endOffset": 125}, {"referenceID": 1, "context": "Approaches include those using bottom-up trained Gaussian mixture models (GMMs) to produce frame-level posteriorgrams [9, 10], using unsupervised hidden Markov models (HMMs) to obtain discrete categorical output in terms of discovered subword units [2, 11, 12], and using unsupervised neural networks (NNs) to obtain frame-level continuous vector representations [13\u201315].", "startOffset": 249, "endOffset": 260}, {"referenceID": 10, "context": "Approaches include those using bottom-up trained Gaussian mixture models (GMMs) to produce frame-level posteriorgrams [9, 10], using unsupervised hidden Markov models (HMMs) to obtain discrete categorical output in terms of discovered subword units [2, 11, 12], and using unsupervised neural networks (NNs) to obtain frame-level continuous vector representations [13\u201315].", "startOffset": 249, "endOffset": 260}, {"referenceID": 11, "context": "Approaches include those using bottom-up trained Gaussian mixture models (GMMs) to produce frame-level posteriorgrams [9, 10], using unsupervised hidden Markov models (HMMs) to obtain discrete categorical output in terms of discovered subword units [2, 11, 12], and using unsupervised neural networks (NNs) to obtain frame-level continuous vector representations [13\u201315].", "startOffset": 249, "endOffset": 260}, {"referenceID": 12, "context": "Approaches include those using bottom-up trained Gaussian mixture models (GMMs) to produce frame-level posteriorgrams [9, 10], using unsupervised hidden Markov models (HMMs) to obtain discrete categorical output in terms of discovered subword units [2, 11, 12], and using unsupervised neural networks (NNs) to obtain frame-level continuous vector representations [13\u201315].", "startOffset": 363, "endOffset": 370}, {"referenceID": 13, "context": "Approaches include those using bottom-up trained Gaussian mixture models (GMMs) to produce frame-level posteriorgrams [9, 10], using unsupervised hidden Markov models (HMMs) to obtain discrete categorical output in terms of discovered subword units [2, 11, 12], and using unsupervised neural networks (NNs) to obtain frame-level continuous vector representations [13\u201315].", "startOffset": 363, "endOffset": 370}, {"referenceID": 14, "context": "Approaches include those using bottom-up trained Gaussian mixture models (GMMs) to produce frame-level posteriorgrams [9, 10], using unsupervised hidden Markov models (HMMs) to obtain discrete categorical output in terms of discovered subword units [2, 11, 12], and using unsupervised neural networks (NNs) to obtain frame-level continuous vector representations [13\u201315].", "startOffset": 363, "endOffset": 370}, {"referenceID": 15, "context": "This is important in tasks such as query-by-example search [16, 17], where a system needs to find all the utterances in a corpus containing a spoken query, or in unsupervised term discovery (UTD), where a system needs to automatically find repeated word- or phrase-like patterns in a speech collection [1, 18, 19].", "startOffset": 59, "endOffset": 67}, {"referenceID": 16, "context": "This is important in tasks such as query-by-example search [16, 17], where a system needs to find all the utterances in a corpus containing a spoken query, or in unsupervised term discovery (UTD), where a system needs to automatically find repeated word- or phrase-like patterns in a speech collection [1, 18, 19].", "startOffset": 59, "endOffset": 67}, {"referenceID": 0, "context": "This is important in tasks such as query-by-example search [16, 17], where a system needs to find all the utterances in a corpus containing a spoken query, or in unsupervised term discovery (UTD), where a system needs to automatically find repeated word- or phrase-like patterns in a speech collection [1, 18, 19].", "startOffset": 302, "endOffset": 313}, {"referenceID": 17, "context": "This is important in tasks such as query-by-example search [16, 17], where a system needs to find all the utterances in a corpus containing a spoken query, or in unsupervised term discovery (UTD), where a system needs to automatically find repeated word- or phrase-like patterns in a speech collection [1, 18, 19].", "startOffset": 302, "endOffset": 313}, {"referenceID": 18, "context": "This is important in tasks such as query-by-example search [16, 17], where a system needs to find all the utterances in a corpus containing a spoken query, or in unsupervised term discovery (UTD), where a system needs to automatically find repeated word- or phrase-like patterns in a speech collection [1, 18, 19].", "startOffset": 302, "endOffset": 313}, {"referenceID": 4, "context": "Several recent studies share this goal [5, 20\u201323].", "startOffset": 39, "endOffset": 49}, {"referenceID": 19, "context": "Several recent studies share this goal [5, 20\u201323].", "startOffset": 39, "endOffset": 49}, {"referenceID": 20, "context": "Several recent studies share this goal [5, 20\u201323].", "startOffset": 39, "endOffset": 49}, {"referenceID": 21, "context": "Several recent studies share this goal [5, 20\u201323].", "startOffset": 39, "endOffset": 49}, {"referenceID": 22, "context": "Several recent studies share this goal [5, 20\u201323].", "startOffset": 39, "endOffset": 49}, {"referenceID": 23, "context": "In previous work [24] we introduced a novel unsupervised segmental Bayesian model for full-coverage segmentation and clustering of small-vocabulary speech.", "startOffset": 17, "endOffset": 21}, {"referenceID": 23, "context": "In [24] we evaluated the model in an unsupervised digit recognition task using the TIDigits corpus.", "startOffset": 3, "endOffset": 7}, {"referenceID": 0, "context": "To our knowledge, this is the first full-coverage unsupervised speech recognition system to report results in this regime; previous systems have either focused on identifying isolated terms [1, 18, 19], were speaker-dependent [22, 23], or used only a small vocabulary [21, 24].", "startOffset": 190, "endOffset": 201}, {"referenceID": 17, "context": "To our knowledge, this is the first full-coverage unsupervised speech recognition system to report results in this regime; previous systems have either focused on identifying isolated terms [1, 18, 19], were speaker-dependent [22, 23], or used only a small vocabulary [21, 24].", "startOffset": 190, "endOffset": 201}, {"referenceID": 18, "context": "To our knowledge, this is the first full-coverage unsupervised speech recognition system to report results in this regime; previous systems have either focused on identifying isolated terms [1, 18, 19], were speaker-dependent [22, 23], or used only a small vocabulary [21, 24].", "startOffset": 190, "endOffset": 201}, {"referenceID": 21, "context": "To our knowledge, this is the first full-coverage unsupervised speech recognition system to report results in this regime; previous systems have either focused on identifying isolated terms [1, 18, 19], were speaker-dependent [22, 23], or used only a small vocabulary [21, 24].", "startOffset": 226, "endOffset": 234}, {"referenceID": 22, "context": "To our knowledge, this is the first full-coverage unsupervised speech recognition system to report results in this regime; previous systems have either focused on identifying isolated terms [1, 18, 19], were speaker-dependent [22, 23], or used only a small vocabulary [21, 24].", "startOffset": 226, "endOffset": 234}, {"referenceID": 20, "context": "To our knowledge, this is the first full-coverage unsupervised speech recognition system to report results in this regime; previous systems have either focused on identifying isolated terms [1, 18, 19], were speaker-dependent [22, 23], or used only a small vocabulary [21, 24].", "startOffset": 268, "endOffset": 276}, {"referenceID": 23, "context": "To our knowledge, this is the first full-coverage unsupervised speech recognition system to report results in this regime; previous systems have either focused on identifying isolated terms [1, 18, 19], were speaker-dependent [22, 23], or used only a small vocabulary [21, 24].", "startOffset": 268, "endOffset": 276}, {"referenceID": 22, "context": "For our efficiency improvements, we use a bottom-up unsupervised syllable boundary detection method [23] to eliminate unlikely word boundaries, reducing the number of potential word segments that need to be considered.", "startOffset": 100, "endOffset": 104}, {"referenceID": 24, "context": "We also use a computationally much simpler embedding approach based on downsampling [25].", "startOffset": 84, "endOffset": 88}, {"referenceID": 25, "context": "For better speaker-independent performance, we incorporate a frame-level representation learning method introduced in our previous work [26]: the correspondence autoencoder (cAE).", "startOffset": 136, "endOffset": 140}, {"referenceID": 25, "context": "In [26] we showed that cAE frame-level features outperform traditional features (MFCCs) and GMM-based representations in a multi-speaker intrinsic evaluation.", "startOffset": 3, "endOffset": 7}, {"referenceID": 26, "context": "Xitsonga is an under-resourced southern African Bantu language [27].", "startOffset": 63, "endOffset": 67}, {"referenceID": 7, "context": "These datasets were also used as part of the Zero Resource Speech Challenge (ZRS) at Interspeech 2015 [8] and we show that our system outperforms competing systems [8, 19, 23] on several of the ZRS metrics.", "startOffset": 102, "endOffset": 105}, {"referenceID": 7, "context": "These datasets were also used as part of the Zero Resource Speech Challenge (ZRS) at Interspeech 2015 [8] and we show that our system outperforms competing systems [8, 19, 23] on several of the ZRS metrics.", "startOffset": 164, "endOffset": 175}, {"referenceID": 18, "context": "These datasets were also used as part of the Zero Resource Speech Challenge (ZRS) at Interspeech 2015 [8] and we show that our system outperforms competing systems [8, 19, 23] on several of the ZRS metrics.", "startOffset": 164, "endOffset": 175}, {"referenceID": 22, "context": "These datasets were also used as part of the Zero Resource Speech Challenge (ZRS) at Interspeech 2015 [8] and we show that our system outperforms competing systems [8, 19, 23] on several of the ZRS metrics.", "startOffset": 164, "endOffset": 175}, {"referenceID": 22, "context": "In particular, we find that by proposing a consistent segmentation and clustering over a whole utterance, our approach makes better use of the bottom-up syllabic constraints than the purely bottom-up syllable-based system of [23].", "startOffset": 225, "endOffset": 229}, {"referenceID": 8, "context": "Zhang and Glass [9] successfully used posteriorgram features from an unsupervised GMM universal background model (UBM) for query-by-example search and term discovery.", "startOffset": 16, "endOffset": 19}, {"referenceID": 9, "context": "[10] used posteriorgrams from a non-parameteric infinite GMM.", "startOffset": 0, "endOffset": 4}, {"referenceID": 10, "context": "[11], the more traditional iterative re-estimation and unsupervised decoding procedure of Siu et al.", "startOffset": 0, "endOffset": 4}, {"referenceID": 1, "context": "[2], and the non-parameteric Bayesian HMM of Lee and Glass [12].", "startOffset": 0, "endOffset": 3}, {"referenceID": 11, "context": "[2], and the non-parameteric Bayesian HMM of Lee and Glass [12].", "startOffset": 59, "endOffset": 63}, {"referenceID": 27, "context": "More recently, NNs have been used for bottom-up representation learning: stacked autoencoders (AEs), a type of unsupervised deep NN that tries to reconstruct its input, has been used in several studies [28\u201330].", "startOffset": 202, "endOffset": 209}, {"referenceID": 28, "context": "More recently, NNs have been used for bottom-up representation learning: stacked autoencoders (AEs), a type of unsupervised deep NN that tries to reconstruct its input, has been used in several studies [28\u201330].", "startOffset": 202, "endOffset": 209}, {"referenceID": 29, "context": "More recently, NNs have been used for bottom-up representation learning: stacked autoencoders (AEs), a type of unsupervised deep NN that tries to reconstruct its input, has been used in several studies [28\u201330].", "startOffset": 202, "endOffset": 209}, {"referenceID": 30, "context": "showed that such constraints can be used to train HMMs [31] and GMM-UBMs [32] that significantly outperform their pure bottom-up counterparts.", "startOffset": 55, "endOffset": 59}, {"referenceID": 31, "context": "showed that such constraints can be used to train HMMs [31] and GMM-UBMs [32] that significantly outperform their pure bottom-up counterparts.", "startOffset": 73, "endOffset": 77}, {"referenceID": 25, "context": "In our own work [26], we proposed the correspondence autoencoder (cAE): an AE-like deep NN that incorporates top-down constraints by using aligned frames from discovered words as input-output pairs.", "startOffset": 16, "endOffset": 20}, {"referenceID": 31, "context": "The model significantly outperformed the top-down GMM-UBM [32] and stacked AEs [28,29] in an intrinsic evaluation: isolated word discrimination.", "startOffset": 58, "endOffset": 62}, {"referenceID": 27, "context": "The model significantly outperformed the top-down GMM-UBM [32] and stacked AEs [28,29] in an intrinsic evaluation: isolated word discrimination.", "startOffset": 79, "endOffset": 86}, {"referenceID": 28, "context": "The model significantly outperformed the top-down GMM-UBM [32] and stacked AEs [28,29] in an intrinsic evaluation: isolated word discrimination.", "startOffset": 79, "endOffset": 86}, {"referenceID": 12, "context": "Since then, several researchers have used such weak top-down supervision in training unsupervised NN-based models [13, 15, 33].", "startOffset": 114, "endOffset": 126}, {"referenceID": 14, "context": "Since then, several researchers have used such weak top-down supervision in training unsupervised NN-based models [13, 15, 33].", "startOffset": 114, "endOffset": 126}, {"referenceID": 32, "context": "Since then, several researchers have used such weak top-down supervision in training unsupervised NN-based models [13, 15, 33].", "startOffset": 114, "endOffset": 126}, {"referenceID": 0, "context": "This algorithm, developed by Park and Glass [1], identifies similar sub-sequences within two vector time series, rather than comparing entire sequences as in standard DTW.", "startOffset": 44, "endOffset": 47}, {"referenceID": 15, "context": "Follow-up work has built on Park and Glass\u2019 original method in various ways, for example through improved feature representations [16] or by greatly improving its efficiency [18].", "startOffset": 130, "endOffset": 134}, {"referenceID": 17, "context": "Follow-up work has built on Park and Glass\u2019 original method in various ways, for example through improved feature representations [16] or by greatly improving its efficiency [18].", "startOffset": 174, "endOffset": 178}, {"referenceID": 7, "context": "The baseline provided as part of the lexical discovery track of the Zero Resource Speech Challenge 2015 (ZRS) [8] is a UTD system based on the earlier work of [18].", "startOffset": 110, "endOffset": 113}, {"referenceID": 17, "context": "The baseline provided as part of the lexical discovery track of the Zero Resource Speech Challenge 2015 (ZRS) [8] is a UTD system based on the earlier work of [18].", "startOffset": 159, "endOffset": 163}, {"referenceID": 18, "context": "[19] extended the baseline system using improved graph clustering algorithms.", "startOffset": 0, "endOffset": 4}, {"referenceID": 4, "context": "Approaches include using non-negative matrix factorization [5], using iterative decoding and refinement for jointly training subword HMMs and a lexicon [20], and using discrete HMMs to model whole words in terms of discovered subword units [21].", "startOffset": 59, "endOffset": 62}, {"referenceID": 19, "context": "Approaches include using non-negative matrix factorization [5], using iterative decoding and refinement for jointly training subword HMMs and a lexicon [20], and using discrete HMMs to model whole words in terms of discovered subword units [21].", "startOffset": 152, "endOffset": 156}, {"referenceID": 20, "context": "Approaches include using non-negative matrix factorization [5], using iterative decoding and refinement for jointly training subword HMMs and a lexicon [20], and using discrete HMMs to model whole words in terms of discovered subword units [21].", "startOffset": 240, "endOffset": 244}, {"referenceID": 21, "context": "In [22], Lee et al.", "startOffset": 3, "endOffset": 7}, {"referenceID": 33, "context": "As in their model, we also follow a Bayesian approach, which is useful for incorporating prior knowledge and for finding sparser solutions [34].", "startOffset": 139, "endOffset": 143}, {"referenceID": 21, "context": "However, where [22] only considered singlespeaker data, we additionally evaluate on large-vocabulary multispeaker data.", "startOffset": 15, "endOffset": 19}, {"referenceID": 19, "context": "Furthermore, in contrast to [20\u201322], our model operates directly at the whole-word level instead of having both word and subword models.", "startOffset": 28, "endOffset": 35}, {"referenceID": 20, "context": "Furthermore, in contrast to [20\u201322], our model operates directly at the whole-word level instead of having both word and subword models.", "startOffset": 28, "endOffset": 35}, {"referenceID": 21, "context": "Furthermore, in contrast to [20\u201322], our model operates directly at the whole-word level instead of having both word and subword models.", "startOffset": 28, "endOffset": 35}, {"referenceID": 31, "context": "The approach is further motivated by the observation that it is often easier to identify cross-speaker similarities between words than between subwords [32], which is why most UTD systems focus on longer-spanning patterns.", "startOffset": 152, "endOffset": 156}, {"referenceID": 34, "context": "There is also evidence that infants are able to segment whole words from continuous speech while still learning phonetic contrasts in their native language [35, 36].", "startOffset": 156, "endOffset": 164}, {"referenceID": 35, "context": "There is also evidence that infants are able to segment whole words from continuous speech while still learning phonetic contrasts in their native language [35, 36].", "startOffset": 156, "endOffset": 164}, {"referenceID": 36, "context": "Finally, segmental approaches do not make the frame-level independence assumptions of most of the models above; this assumption has long been argued against [37, 38].", "startOffset": 157, "endOffset": 165}, {"referenceID": 37, "context": "Finally, segmental approaches do not make the frame-level independence assumptions of most of the models above; this assumption has long been argued against [37, 38].", "startOffset": 157, "endOffset": 165}, {"referenceID": 22, "context": "[23], which we use to help scale our approach to larger vocabularies.", "startOffset": 0, "endOffset": 4}, {"referenceID": 22, "context": "In our model, we incorporate the syllable boundary detection method of [23] (the first component of their system) as a presegmentation method to eliminate unlikely word boundaries.", "startOffset": 71, "endOffset": 75}, {"referenceID": 38, "context": "Both human infants [39] and adults [40] use syllabic cues for word segmentation, and using such a bottom-up unsupervised syllabifier can therefore be seen as one way to incorporate prior knowledge of the speech signal into a zero-resource system [41].", "startOffset": 19, "endOffset": 23}, {"referenceID": 39, "context": "Both human infants [39] and adults [40] use syllabic cues for word segmentation, and using such a bottom-up unsupervised syllabifier can therefore be seen as one way to incorporate prior knowledge of the speech signal into a zero-resource system [41].", "startOffset": 35, "endOffset": 39}, {"referenceID": 40, "context": "Both human infants [39] and adults [40] use syllabic cues for word segmentation, and using such a bottom-up unsupervised syllabifier can therefore be seen as one way to incorporate prior knowledge of the speech signal into a zero-resource system [41].", "startOffset": 246, "endOffset": 250}, {"referenceID": 0, "context": "A more accurate description would be pseudo term, but we use word instead to match usage in earlier work [1, 25, 42].", "startOffset": 105, "endOffset": 116}, {"referenceID": 24, "context": "A more accurate description would be pseudo term, but we use word instead to match usage in earlier work [1, 25, 42].", "startOffset": 105, "endOffset": 116}, {"referenceID": 41, "context": "A more accurate description would be pseudo term, but we use word instead to match usage in earlier work [1, 25, 42].", "startOffset": 105, "endOffset": 116}, {"referenceID": 42, "context": ", zN ) using a collapsed Gibbs sampler [43].", "startOffset": 39, "endOffset": 43}, {"referenceID": 23, "context": "This is done in turn for each zi conditioned on all the other current component assignments [24]:", "startOffset": 92, "endOffset": 96}, {"referenceID": 44, "context": "The term p(xi|Xk\\i;\u03b2) in (5) is the posterior predictive of xi, which (because of the conjugate prior) is a spherical covariance Gaussian distribution with analytic expressions for its mean and covariance parameters [45].", "startOffset": 216, "endOffset": 220}, {"referenceID": 45, "context": "Line 5 uses the forward filtering backward sampling dynamic programming algorithm [46] to sample the new embeddings; details of this step are given in Appendix A.", "startOffset": 82, "endOffset": 86}, {"referenceID": 23, "context": "In [24], potential word segments were therefore required to be between 200 ms and 1 s in duration, and word boundaries were only considered at 20 ms intervals.", "startOffset": 3, "endOffset": 7}, {"referenceID": 22, "context": "[23] evaluated several syllable boundary detection algorithms, and we use the best of these.", "startOffset": 0, "endOffset": 4}, {"referenceID": 22, "context": "In this work, we use the syllabification code kindly provided by the authors of [23] without any modification and with the default parameter settings.", "startOffset": 80, "endOffset": 84}, {"referenceID": 24, "context": "A simple and fast approach to obtain acoustic word embeddings is to uniformly downsample so that any segment is represented by the same fixed number of vectors [25,47].", "startOffset": 160, "endOffset": 167}, {"referenceID": 46, "context": "A simple and fast approach to obtain acoustic word embeddings is to uniformly downsample so that any segment is represented by the same fixed number of vectors [25,47].", "startOffset": 160, "endOffset": 167}, {"referenceID": 22, "context": "A similar approach is to divide a segment into a fixed number of intervals and average the frames in each interval [23,48].", "startOffset": 115, "endOffset": 122}, {"referenceID": 47, "context": "A similar approach is to divide a segment into a fixed number of intervals and average the frames in each interval [23,48].", "startOffset": 115, "endOffset": 122}, {"referenceID": 23, "context": "Although these very simple approaches are less accurate at word discrimination than the approach used before in [24], they have been effectively used in several studies, including [23], and are computationally much more efficient.", "startOffset": 112, "endOffset": 116}, {"referenceID": 22, "context": "Although these very simple approaches are less accurate at word discrimination than the approach used before in [24], they have been effectively used in several studies, including [23], and are computationally much more efficient.", "startOffset": 180, "endOffset": 184}, {"referenceID": 24, "context": "Here we use downsampling as our acoustic word embedding function fe in Figure 1; we keep ten equally-spaced vectors from a segment and use a Fourier-based method for smoothing [25].", "startOffset": 176, "endOffset": 180}, {"referenceID": 25, "context": "Complete details of the cAE are given in [26], but we briefly outline the training procedure here.", "startOffset": 41, "endOffset": 45}, {"referenceID": 17, "context": "The UTD system of [18] is used to discover word pairs which serve as weak top-down supervision.", "startOffset": 18, "endOffset": 22}, {"referenceID": 48, "context": "The first two are disjoint subsets extracted from the Buckeye corpus of conversational English [49], while the third is a portion of the Xitsonga section of the NCHLT corpus of languages spoken in South Africa [27].", "startOffset": 95, "endOffset": 99}, {"referenceID": 26, "context": "The first two are disjoint subsets extracted from the Buckeye corpus of conversational English [49], while the third is a portion of the Xitsonga section of the NCHLT corpus of languages spoken in South Africa [27].", "startOffset": 210, "endOffset": 214}, {"referenceID": 7, "context": "English2 and the Xitsonga data were used as test sets in the ZRS challenge, so we can compare our system to others using the same data and evaluation framework [8].", "startOffset": 160, "endOffset": 163}, {"referenceID": 49, "context": "The evaluation of zero-resource systems that segment and cluster speech is a research problem in itself [50].", "startOffset": 104, "endOffset": 108}, {"referenceID": 4, "context": "many-to-one) [5].", "startOffset": 13, "endOffset": 16}, {"referenceID": 19, "context": "Unsupervised word error rate (WER/WERm) uses a a similar word-level mapping and then aligns the mapped decoded output from a system to the ground truth transcriptions [20, 21].", "startOffset": 167, "endOffset": 175}, {"referenceID": 20, "context": "Unsupervised word error rate (WER/WERm) uses a a similar word-level mapping and then aligns the mapped decoded output from a system to the ground truth transcriptions [20, 21].", "startOffset": 167, "endOffset": 175}, {"referenceID": 23, "context": "The latter, which we denote simply as WER, might leave some cluster unassigned and these are counted as errors [24].", "startOffset": 111, "endOffset": 115}, {"referenceID": 7, "context": "These metrics use a phoneme-level mapping: each discovered token is mapped to the sequence of ground truth phonemes of which at least 50% or 30 ms are covered by the discovered segment [8, 50].", "startOffset": 185, "endOffset": 192}, {"referenceID": 49, "context": "These metrics use a phoneme-level mapping: each discovered token is mapped to the sequence of ground truth phonemes of which at least 50% or 30 ms are covered by the discovered segment [8, 50].", "startOffset": 185, "endOffset": 192}, {"referenceID": 21, "context": "A tolerance of 20 ms is mostly used [22], but for the ZRS the tolerance is 30 ms or 50% of a phoneme (to match the mapping).", "startOffset": 36, "endOffset": 40}, {"referenceID": 13, "context": "3) are based on [14, 26].", "startOffset": 16, "endOffset": 24}, {"referenceID": 25, "context": "3) are based on [14, 26].", "startOffset": 16, "endOffset": 24}, {"referenceID": 17, "context": "Pairs are extracted using the UTD system of [18]: for English1, 14 494 word pairs are discovered; for English2, 10 769 pairs; and for Xitsonga, 6979.", "startOffset": 44, "endOffset": 48}, {"referenceID": 13, "context": "Although it is common in NN speech systems to use nine or eleven sliding frames as input, we use single-frame MFCCs with first and second order derivatives (39-dimensional), as also done in [14, 26].", "startOffset": 190, "endOffset": 198}, {"referenceID": 25, "context": "Although it is common in NN speech systems to use nine or eleven sliding frames as input, we use single-frame MFCCs with first and second order derivatives (39-dimensional), as also done in [14, 26].", "startOffset": 190, "endOffset": 198}, {"referenceID": 23, "context": "As in [24, 51, 52], embeddings are normalized to the unit sphere.", "startOffset": 6, "endOffset": 18}, {"referenceID": 50, "context": "As in [24, 51, 52], embeddings are normalized to the unit sphere.", "startOffset": 6, "endOffset": 18}, {"referenceID": 51, "context": "As in [24, 51, 52], embeddings are normalized to the unit sphere.", "startOffset": 6, "endOffset": 18}, {"referenceID": 23, "context": "hyperparameters, as in [24, 51, 52]: all-zero vector for \u03bc0, \u03c3 2 0 = \u03c3/\u03ba0, \u03ba0 = 0.", "startOffset": 23, "endOffset": 35}, {"referenceID": 50, "context": "hyperparameters, as in [24, 51, 52]: all-zero vector for \u03bc0, \u03c3 2 0 = \u03c3/\u03ba0, \u03ba0 = 0.", "startOffset": 23, "endOffset": 35}, {"referenceID": 51, "context": "hyperparameters, as in [24, 51, 52]: all-zero vector for \u03bc0, \u03c3 2 0 = \u03c3/\u03ba0, \u03ba0 = 0.", "startOffset": 23, "endOffset": 35}, {"referenceID": 22, "context": "[23] without modification.", "startOffset": 0, "endOffset": 4}, {"referenceID": 22, "context": "Here we follow the same approach as in [23]: we choose K as a proportion of the number of discovered syllable tokens.", "startOffset": 39, "endOffset": 43}, {"referenceID": 23, "context": "To improve sampler convergence, we use simulated annealing [24].", "startOffset": 59, "endOffset": 63}, {"referenceID": 0, "context": "Such a minimum duration constraint is also used in most UTD systems [1, 18].", "startOffset": 68, "endOffset": 75}, {"referenceID": 17, "context": "Such a minimum duration constraint is also used in most UTD systems [1, 18].", "startOffset": 68, "endOffset": 75}, {"referenceID": 22, "context": "Moreover, there is a much higher proportion of multisyllabic words in Xitsonga [23], as reflected in the average duration of words which is almost twice as long in the Xitsonga than in the English data (Section 4.", "startOffset": 79, "endOffset": 83}, {"referenceID": 21, "context": "A word boundary tolerance of 20 ms is used [22], with a greedy one-to-one mapping for calculating error rates.", "startOffset": 43, "endOffset": 47}, {"referenceID": 7, "context": "We now compare our approach to others using the evaluation framework provided as part of the ZRS challenge [8].", "startOffset": 107, "endOffset": 110}, {"referenceID": 7, "context": "We compare our approach to three systems: ZRSBaselineUTD is the UTD system used as official baseline in the challenge [8] (see Section 2.", "startOffset": 118, "endOffset": 121}, {"referenceID": 18, "context": "UTDGraphCC is the best UTD system of [19], employing a connected component graph clustering algorithm to group discovered segments (also Section 2.", "startOffset": 37, "endOffset": 41}, {"referenceID": 22, "context": "SyllableSegOsc+ uses oscillator-based syllabification followed by speaker-dependent clustering and word discovery [23] (Section 2.", "startOffset": 114, "endOffset": 118}, {"referenceID": 22, "context": "We add the superscript + since, after publication of [23], R\u00e4s\u00e4nen et al.", "startOffset": 53, "endOffset": 57}, {"referenceID": 52, "context": "further refined their syllable boundary detection method [53].", "startOffset": 57, "endOffset": 61}, {"referenceID": 22, "context": "The authors of [23] kindly regenerated their full ZRS results for comparison here.", "startOffset": 15, "endOffset": 19}, {"referenceID": 22, "context": "In contrast, such a minimum duration constraint would require additional heuristics in the pure bottom-up approach of [23].", "startOffset": 118, "endOffset": 122}, {"referenceID": 22, "context": "We conclude that by hypothesizing word boundaries consistently over an utterance rather than taking these decisions in isolation, our approach yields more accurate clusters (NED) that correspond better to true words (word type F -score) than the full-coverage syllable-based approach of [23].", "startOffset": 287, "endOffset": 291}, {"referenceID": 22, "context": "However, despite the benefits of our model, the algorithm of [23] is much simpler in terms of computational complexity and implementation.", "startOffset": 61, "endOffset": 65}], "year": 2016, "abstractText": "Zero-resource speech technology is a growing research area that aims to develop methods for speech processing in the absence of transcriptions, lexicons, or language modelling text. Early systems focused on identifying isolated recurring terms in a corpus, while more recent full-coverage systems attempt to completely segment and cluster the audio into word-like units\u2014effectively performing unsupervised speech recognition. To our knowledge, this article presents the first such system evaluated on largevocabulary multi-speaker data. The system uses a Bayesian modelling framework with segmental word representations: each word segment is represented as a fixed-dimensional acoustic embedding obtained by mapping the sequence of feature frames to a single embedding vector. We compare our system on English and Xitsonga datasets to state-of-the-art baselines, using a variety of measures including word error rate (obtained by mapping the unsupervised output to ground truth transcriptions). We show that by imposing a consistent top-down segmentation while also using bottom-up knowledge from detected syllable boundaries, both single-speaker and multi-speaker versions of our system outperform a purely bottom-up single-speaker syllable-based approach. We also show that the discovered clusters can be made less speakerand gender-specific by using an unsupervised autoencoder-like feature extractor to learn better frame-level features (prior to embedding). Our system\u2019s discovered clusters are still less pure than those of two multi-speaker term discovery systems, but provide far greater coverage.", "creator": "LaTeX with hyperref package"}}}