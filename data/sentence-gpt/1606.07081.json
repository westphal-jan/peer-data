{"id": "1606.07081", "review": {"conference": "NIPS", "VERSION": "v1", "DATE_OF_SUBMISSION": "22-Jun-2016", "title": "Finite Sample Prediction and Recovery Bounds for Ordinal Embedding", "abstract": "The goal of ordinal embedding is to represent items as points in a low-dimensional Euclidean space given a set of constraints in the form of distance comparisons like \"item $i$ is closer to item $j$ than item $k$\". Ordinal constraints like this often come from human judgments. To account for errors and variation in judgments, we consider the noisy situation in which the given constraints are independently corrupted by reversing the correct constraint with some probability.\n\n\nAn Ordinal embedding solution is shown in figure 1:\nIf there is one object in the space with a set of constraints, it is a higher-order version of the problem. This may be the case in the case of other objects in the space with a set of constraints. The difference between a higher-order embedding solution and a lower-order version is not necessarily explained by any means. The difference in an Ordinal embedding solution may be the size of the object, but it is the size of the object.\nThis is where the problem arises. We can only say that when we have a high-order embedding solution and a low-order embedding solution, there is no matter how far the object is, the same objects will have different properties. This may sound intuitive. We will have the choice to choose between the \"estimate\" and \"decimal\" of the problem. The question arises where the object is different from the one with the lowest-order embedding solution and a lower-order embedding solution. If this is the case in the case of other objects in the space with a set of constraints, it is a higher-order embedding solution.\nThis example shows that even though the low-order embedding solution is the same as the case in the case of other objects in the space with a set of constraints, it is a higher-order embedding solution. When an object has to use the same set of constraints (with the lowest-order embedding solution) to solve the problem, this is called \"the low-order embedding solution.\" This is, in turn, the opposite.\nSo it is the case in the case of other objects in the space with a set of constraints, and the same object will have different properties. Here the problem arises:\nThere are two objects with the highest-order embedding solution:\nOne is a lower-order embedding solution with the lowest-order embedding solution.\nTwo is a lower-order embedding solution with the", "histories": [["v1", "Wed, 22 Jun 2016 20:06:10 GMT  (987kb,D)", "http://arxiv.org/abs/1606.07081v1", null]], "reviews": [], "SUBJECTS": "stat.ML cs.LG", "authors": ["lalit jain", "kevin g jamieson", "robert d nowak"], "accepted": true, "id": "1606.07081"}, "pdf": {"name": "1606.07081.pdf", "metadata": {"source": "CRF", "title": "Finite Sample Prediction and Recovery Bounds for Ordinal Embedding", "authors": ["Lalit Jain", "Kevin Jamieson"], "emails": [], "sections": [{"heading": "1 Ordinal Embedding", "text": "Ordinal embedding, also known as non-metric multidimensional scaling, aims to represent items as points in Rd so that the distances between items agree as well as possible with a given set of ordinal comparisons such as item i is closer to item j than to item k. This is a classic problem that is often used to visualize perceptual similarities [1, 2]. Recently, several authors have proposed a variety of new algorithms for learning a metric embedding from ordinal information [3, 4, 5, 6, 7]. There has also been some theoretical progress towards characterizing the consistency of ordinal embedding methods. For example, it has been shown that the correct embedding can be learned in the limit as the number of items grows [8, 9, 10]. However, a major shortcoming of all prior work is the lack of generalization and embedding error bounds for problems involving a finite number of items and observations. This paper addresses this problem, developing error bounds for ordinal embedding algorithms with known observation models. The bounds explicitly show the dependence on the number of ordinal comparisons relative to the number of items and the dimension of the embedding. In addition, we propose two new algorithms for recoverying embeddings: the first is based on unbiasing a nuclear norm constrained optimization and the second is based on projected gradient descent onto the space of rank-d matrices, commonly referred to as hard-thresholding. Both methods match state-of-the-art performance while being simpler to implement and faster to converge.\nar X\niv :1\n60 6.\n07 08\n1v 1\n[ st\nat .M\nL ]\n2 2\nJu n"}, {"heading": "1.1 Ordinal Embedding from Noisy Data", "text": "Consider n points x1,x2, . . . ,xn \u2208 Rd. Let X = [x1 \u00b7 \u00b7 \u00b7xn] \u2208 Rd\u00d7n. The Euclidean distance matrix D? is defined to have elements D?ij = \u2016xi \u2212 xj\u201622. Ordinal embedding is the problem of recovering X given ordinal constraints on distances. This paper focuses on \u201ctriplet\u201d constraints of the form D?ij < D ? ik, where 1 \u2264 i 6= j 6= k \u2264 n. Furthermore, we only observe noisy indications of these constraints, as follows. Each triplet t = (i, j, k) has an associated probability pt satisfying\npt > 1/2 \u21d0\u21d2 \u2016xi \u2212 xj\u20162 < \u2016xi \u2212 xk\u20162 .\nLet S denote a collection of triplets drawn independently and uniformly at random. And for each t \u2208 S we observe an independent random variable yt = \u22121 with probability pt, and yt = 1 otherwise. The goal is to recover the embedding X from these data. Exact recovery of D? from such data requires a known link between pt and D?. To this end, our main focus is the following problem.\nOrdinal Embedding from Noisy Data\nConsider n points x1,x2 \u00b7 \u00b7 \u00b7 ,xn in d-dimensional Euclidean space. Let S denote a collection of triplets and for each t \u2208 S observe an independent random variable\nyt =  \u22121 w.p. f(D?ij \u2212D?ik)\n1 w.p. 1\u2212 f(D?ij \u2212D?ik) .\nwhere the link function f : R\u2192 [0, 1] is known. Estimate X from S , {yt}, and f . For example, if f is the logistic function, then for triplet t = (i, j, k)\npt = P(yt = \u22121) = f(D?ij \u2212D?ik) = 1\n1 + exp(D?ij \u2212D?ik) , (1)\nthen D?ij \u2212 D?ik = log (1\u2212pt\npt\n) . However, we stress that we only require the existence of a link function\nfor exact recovery of D?. Indeed, if one just wishes to predict the answers to unobserved triplets, then the results of Section 2 hold for arbitrary pt probabilities. We note that a problem known as one-bit matrix completion has similarly used link functions to recover structure [11]. However, while that work made direct measurements of the entries of the matrix, in this work we observe linear projections and as we will see later, the collection of these linear operators has a a non-empty kernel creating non-trivial challenges."}, {"heading": "1.2 Organization of Paper", "text": "This paper takes the following approach to ordinal embedding. First we derive prediction error bounds for ordinal embedding with noise in Section 2. These bounds exploit the fact that the rank of a distance matrix of points in Rd is at most d+ 2. Then we consider the special case of a known observation model and the Maximum Likelihood estimator in Section 3. The link function enables us to relate prediction errors to error bounds on estimates of differences {D?ij \u2212D?ik}. In Section 4, we study the non-trivial problem of recovering D?, and thus X , from the {D?ij \u2212D?ik}. Lastly, in Section 5, two new algorithms for ordinal embedding are proposed and experimentally evaluated."}, {"heading": "1.3 Notation and Assumptions", "text": "We will use (D?,G?) to denote the distance and Gram matrices of the latent embedding, and (D,G) to denote an arbitrary distance matrix and its corresponding Gram matrix. The observations {yt} carry\ninformation about D?, but distance matrices are invariant to rotation and translation, and therefore it may only be possible to recover X up to a rigid transformation. Therefore, we make the following assumption throughout the paper.\nAssumption 1. To eliminate the translational ambiguity, assume the points x1, . . .xn \u2208 Rd are centered at the origin (i.e., \u2211n i=1 xi = 0).\nDefine the centering matrix V := I \u2212 1n11 T . Note that under the above assumption, XV = X . Note that D? is determined by the Gram matrix G? = XTX . In addition, X can be determined from G up to a unitary transformation. Furthermore, we will assume that the Gram matrix is \u201ccentered\u201d so that V GV = G. Centering is equivalent to assuming the underlying points are centered at the origin (e.g., note that V G?V = V XTXV = XTX). It will be convenient in the paper to work with both the distance and Gram matrix representations, and the following identities will be useful to keep in mind. For any distance matrix D and its centered Gram matrix G\nG = V DV , (2)\nD = diag(G)1T \u2212 2G + 1diag(G)T , (3)\nwhere diag(G) is the column vector composed of the diagonal of G. In particular this establishes a bijection between centered Gram matrices and distance matrices. We refer the reader to [12] for an insightful and thorough treatment of the properties of distance matrices. We also define the set of all unique triplets\nT := { (i, j, k) : 1 \u2264 i 6= j 6= k \u2264 n, j < k } .\nAssumption 2. The observed triplets in S are drawn independently and unifomly from T ."}, {"heading": "2 Prediction Error Bounds", "text": "For t \u2208 T with t = (i, j, k) we define Lt to be the linear operator satisfying Lt(XTX) = \u2016xi \u2212 xj\u20162 \u2212 \u2016xi \u2212 xk\u20162 for all t \u2208 T . In general, for any Gram matrix G\nLt(G) := Gjj \u2212 2Gij \u2212Gkk + 2Gik.\nWe can naturally view Lt as a linear operator on Sn+, the space of n \u00d7 n symmetric positive semidefinite matrices. We can also represent Lt as a symmetric n\u00d7 n matrix Lt that is zero everywhere except on the submatrix corresponding to i, j, k which has the form 0 \u22121 1\u22121 1 0\n1 0 \u22121  and then\nLt(G) := \u3008Lt,G\u3009\nwhere \u3008A,B\u3009 = vec(A)T vec(B) for any compatible matrices A,B. Ordering the elements of T lexicographically, we arrange all the Lt(G) together to define the n ( n\u22121 2 ) -dimensional vector\nL(G) = [L123(G),L124(G), \u00b7 \u00b7 \u00b7 ,Lijk(G), \u00b7 \u00b7 \u00b7 ]T . (4)\nLet `(yt\u3008Lt,G\u3009) denote a loss function. For example we can consider the 0 \u2212 1 loss `(yt\u3008Lt,G\u3009) = 1{sign{yt\u3008Lt,G\u3009}6=1}, the hinge-loss `(yt\u3008Lt,G\u3009) = max{0, 1\u2212 yt\u3008Lt,G\u3009}, or the logistic loss\n`(yt\u3008Lt,G\u3009) = log(1 + exp(\u2212yt\u3008Lt,G\u3009)). (5)\nLet pt := P(yt = \u22121) and take the expectation of the loss with respect to both the uniformly random selection of the triple t and the observation yt, we have the risk of G\nR(G) := E[`(yt\u3008Lt,G\u3009)] = 1 |T | \u2211 t\u2208T pt`(\u2212\u3008Lt,G\u3009) + (1\u2212 pt)`(\u3008Lt,G\u3009).\nGiven a set of observations S under the model defined in the problem statement, the empirical risk is,\nR\u0302S(G) = 1 |S| \u2211 t\u2208S `(yt\u3008Lt,G\u3009) (6)\nwhich is an unbiased estimator of the true risk: E[R\u0302S(G)] = R(G). For any G \u2208 Sn+, let \u2016G\u2016\u2217 denote the nuclear norm and \u2016G\u2016\u221e := maxij |Gij |. Define the constraint set\nG\u03bb,\u03b3 := {G \u2208 Sn+ : \u2016G\u2016\u2217 \u2264 \u03bb, \u2016G\u2016\u221e \u2264 \u03b3} . (7)\nWe estimate G? by solving the optimization\nmin G\u2208G\u03bb,\u03b3 R\u0302S(G) . (8)\nSince G? is positive semidefinite, we expect the diagonal entries of G? to bound the off-diagonal entries. So an infinity norm constraint on the diagonal guarantees that the points x1, . . . ,xn corresponding to G? live inside a bounded `2 ball. The `\u221e constraint in (7) plays two roles: 1) if our loss function is Lipschitz, large magnitude values of \u3008Lt,G\u3009 can lead to large deviations of R\u0302S(G) from R(G); bounding ||G||\u221e bounds |\u3008Lt,G\u3009|. 2) Later we will define ` in terms of the link function f and as the magnitude of \u3008Lt,G\u3009 increases the magnitude of the derivative of the link function f typically becomes very small, making it difficult to \u201cinvert\u201d; bounding ||G||\u221e tends to keep \u3008Lt,G\u3009 within an invertible regime of f .\nTheorem 1. Fix \u03bb, \u03b3 and assume G? \u2208 G\u03bb,\u03b3 . Let G\u0302 be a solution to the program (4). If the loss function `(\u00b7) is L-Lipschitz (or | supy `(y)| \u2264 Lmax{1, 12\u03b3}) then with probability at least 1\u2212 \u03b4,\nR(G\u0302)\u2212R(G?) \u2264 4L\u03bb |S|\n(\u221a 18|S| log(n)\nn +\n\u221a 3\n3 log n\n) + L\u03b3 \u221a 288 log 2/\u03b4\n|S|\nProof. The proof follows from standard statistical learning theory techniques, see for instance [13]. By the bounded difference inequality, with probability 1\u2212 \u03b4\nR(G\u0302)\u2212R(G?) = R(G\u0302)\u2212 R\u0302S(G\u0302) + R\u0302S(G\u0302)\u2212 R\u0302S(G?) + R\u0302S(G?)\u2212R(G?)\n\u2264 2 sup G\u2208G\u03bb,\u03b3 |R\u0302S(G)\u2212R(G)| \u2264 2E[ sup G\u2208G\u03bb,\u03b3 |R\u0302S(G)\u2212R(G)|] +\n\u221a 2B2 log 2/\u03b4\n|S|\nwhere supG\u2208G\u03bb,\u03b3 `(yt\u3008Lt,G\u3009) \u2212 `(yt\u2032\u3008Lt\u2032 ,G\u3009) \u2264 supG\u2208G\u03bb,\u03b3 L|\u3008ytLt \u2212 yt\u2032Lt\u2032 ,G\u3009| \u2264 12L\u03b3 =: B using the facts that Lt has 6 non-zeros of magnitude 1 and ||G||\u221e \u2264 \u03b3.\nUsing standard symmetrization and contraction lemmas, we can introduce Rademacher random variables t \u2208 {\u22121, 1} for all t \u2208 S so that\nE sup G\u2208G\u03bb,\u03b3 |R\u0302S(G)\u2212R(G)| \u2264 E sup G\u2208G\u03bb,\u03b3\n2L |S| \u2223\u2223\u2223\u2223\u2223\u2211 t\u2208S t\u3008Lt,G\u3009 \u2223\u2223\u2223\u2223\u2223 .\nThe right hand side is just the Rademacher complexity of G\u03bb,\u03b3 . By definition,\n{G : \u2016G\u2016\u2217 \u2264 \u03bb} = \u03bb \u00b7 conv({uuT : |u| = 1}).\nwhere conv(U) is the convex hull of a set U . Since the Rademacher complexity of a set is the same as the Rademacher complexity of it\u2019s closed convex hull,\nE sup G\u2208G\u03bb,\u03b3 \u2223\u2223\u2223\u2223\u2223\u2211 t\u2208S t\u3008Lt,G\u3009 \u2223\u2223\u2223\u2223\u2223 \u2264 \u03bbE sup|u|=1 \u2223\u2223\u2223\u2223\u2223\u2211 t\u2208S t\u3008Lt, uuT \u3009 \u2223\u2223\u2223\u2223\u2223 = \u03bbE sup|u|=1 \u2223\u2223\u2223\u2223\u2223uT (\u2211 t\u2208S tLt ) u \u2223\u2223\u2223\u2223\u2223 which we recognize is just \u03bbE\u2016 \u2211 t\u2208S tLt\u2016. By [14, 6.6.1] we can bound the operator norm \u2016 \u2211 t\u2208S tLt\u2016\nin terms of the variance of \u2211\nt\u2208S L 2 t and the maximal eigenvalue of maxtLt. These are computed in Lemma\n1 given in the supplemental materials. Combining these results gives,\n2L\u03bb |S| E\u2016 \u2211 t\u2208S tLt\u2016 \u2264 2L\u03bb |S|\n(\u221a 18|S| log(n)\nn +\n\u221a 3\n3 log n\n) .\nWe remark that if G is a rank d < n matrix then\n\u2016G\u2016\u2217 \u2264 \u221a d\u2016G\u2016F \u2264 \u221a dn\u2016G\u2016\u221e\nso if G? is low rank, we really only need a bound on the infinity norm of our constraint set. Under the assumption that G? is rank d with ||G?||\u221e \u2264 \u03b3 and we set \u03bb = \u221a dn\u03b3, then Theorem 1 implies that for |S| > n log n/161\nR(G\u0302)\u2212R(G?) \u2264 8L\u03b3\n\u221a 18dn log(n)\n|S| + L\u03b3\n\u221a 288 log 2/\u03b4\n|S|\nwith probability at least 1\u2212 \u03b4. The above display says that |S| must scale like dn log(n) which is consistent with known finite sample bounds [5]."}, {"heading": "3 Maximum Likelihood Estimation", "text": "We now turn our attention to recovering metric information about G?. Let S be a collection of triplets sampled uniformly at random with replacement and let f : R \u2192 (0, 1) be a known probability function governing the observations. Any link function f induces a natural loss function `f , namely, the negative log-likelihood of a solution G given an observation yt defined as\n`f (yt\u3008Lt,G\u3009) = 1yt=\u22121 log( 1f(\u3008Lt,G\u3009)) + 1yt=1 log( 1 1\u2212f(\u3008Lt,G\u3009))\nFor example, the logistic link function of (1) induces the logistic loss of (5). Recalling that P(yt = \u22121) = f(\u3008Lt,G\u3009) we have\nE[`f (yt\u3008Lt,G\u3009)] = f(\u3008Lt,G?\u3009) log( 1f(\u3008Lt,G\u3009)) + (1\u2212 f(\u3008Lt,G ?\u3009) log( 11\u2212f(\u3008Lt,G\u3009))\n= H(f(\u3008Lt,G?\u3009)) +KL(f(\u3008Lt,G?\u3009)|f(\u3008Lt,G\u3009))\nwhere H(p) = p log(1p) + (1 \u2212 p) log( 1 1\u2212p) and KL(p, q) = p log( p q ) + (1 \u2212 p) log( 1\u2212p 1\u2212q ) are the entropy and KL divergence of Bernoulli RVs with means p, q. Recall that ||G||\u221e \u2264 \u03b3 controls the magnitude of\n\u3008Lt,G\u3009 so for the moment, assume this is small. Then by a Taylor series f(\u3008Lt,G\u3009) \u2248 12 + f \u2032(0)\u3008Lt,G\u3009 using the fact that f(0) = 12 , and by another Taylor series we have\nKL(f(\u3008Lt,G?\u3009)|f(\u3008Lt,G\u3009)) \u2248 KL(12 + f \u2032(0)\u3008Lt,G?\u3009|12 + f \u2032(0)\u3008Lt,G\u3009) \u2248 2f \u2032(0)2(\u3008Lt,G? \u2212G\u3009)2.\nThus, recalling the definition of L(G) from (4) we conclude that if G\u0303 \u2208 arg minGR(G) with R(G) = 1 |T | \u2211 t\u2208T E[`f (yt\u3008Lt,G\u3009)] then one would expect L(G\u0303) \u2248 L(G ?). Moreover, since R\u0302S(G) is an unbiased estimator of R(G), one expects L(G\u0302) to approximate L(G?). The next theorem, combined with Theorem 1, formalizes this observation; its proof is found in the appendix.\nTheorem 2. Let Cf = mint\u2208T infG\u2208G\u03bb,\u03b3 |f \u2032 ( \u3008Lt,G\u3009 ) | where f \u2032 denotes the derivative of f . Then for any G\n2C2f |T | \u2016L(G)\u2212 L(G?)\u20162F \u2264 R(G)\u2212R(G?) .\nNote that if f is the logistic link function of (1) then its straightforward to show that |f \u2032 ( \u3008Lt,G\u3009 ) | \u2265\n1 4 exp(\u2212|\u3008Lt,G\u3009|) \u2265 1 4 exp(\u22126||G||\u221e) for any t, G so it suffices to take Cf = 1 4 exp(\u22126\u03b3)."}, {"heading": "4 Maximum Likelihood Embedding", "text": "In this section, let G\u0302 be the maximum likelihood estimator; i.e., a solution to the optimization with LLipschitz log-likelihood loss function `f for a fixed \u03bb, \u03b3. We have shown that the maximum likelihood estimator allows us to bound \u2016L(G\u0302)\u2212L(G?)\u20162F . In this section, we discuss how this bound can lead us to recover an embedding. For the analysis in this section, it will be convenient to work with distance matrices in addition to Gram matrices. Analogous to the operators Lt(G) defined above, we define the operators \u2206t for t \u2208 T satisfying,\n\u2206t(D) := Dij \u2212Dik \u2261 Lt(G) .\nWe will view the \u2206t as linear operators on the space of symmetric hollow n\u00d7 n matrices Snh, which includes distance matrices as special cases. As with L, we can arrange all the \u2206t together, ordering the t \u2208 T lexicographically, to define the n ( n\u22121 2 ) -dimensional vector\n\u2206(D) = [D12 \u2212D13, \u00b7 \u00b7 \u00b7 , Dij \u2212Dik, \u00b7 \u00b7 \u00b7 ]T .\nWe will use the fact that L(G) \u2261 \u2206(D) heavily. Because \u2206(D) consists of differences of matrix entries, \u2206 has a non-trivial kernel. However, it is easy to see that D can be recovered given \u2206(D) and any one off-diagonal element of D, so the kernel is 1-dimensional. Also, the kernel is easy to identify by example. Consider the regular simplex in d dimensions. The distances between all n = d+ 1 vertices are equal and the distance matrix can easily be seen to be 11T \u2212 I. Thus \u2206(D) = 0 in this case. This gives us the following simple result.\nLemma 2. Let Snh denote the space of symmetric hollow matrices, which includes all distance matrices. For any D \u2208 Snh, the set of linear functionals {\u2206t(D), t \u2208 T } spans an ( n 2 ) \u2212 1 dimensional subspace of Snh, and the 1-dimensional kernel is given by the span of 11T \u2212 I .\nSo we see that the operator \u2206 is not invertible on Snh. Define J := 11T \u2212 I . For any D, let C, the centered distance matrix, be the component of D orthogonal to the kernel of L (i.e., tr(CJ) = 0). Then we have the orthogonal decomposition\nD = C + \u03c3D J ,\nwhere \u03c3D = trace(DJ)/\u2016J\u20162F . Since G is assumed to be centered, the value of \u03c3D has a simple interpretation:\n\u03c3D = 1 2 ( n 2 ) \u2211 1\u2264i\u2264j\u2264n Dij = 2 n\u2212 1 \u2211 1\u2264i\u2264n \u3008xi, xi\u3009 = 2\u2016G\u2016\u2217 n\u2212 1 , (9)\nthe average of the squared distances or alternatively a scaled version of the nuclear norm of G. Let D\u0302 and C\u0302 be the corresponding distance and centered distance matrices.\nNow following the notation of sections 2 and 3, assume that there is a true Gram matrix G? and a link function f as in section 3, and assume that we have observed a set of triples S. Though \u2206 is not invertible on all Snh, it is invertible on the subspace orthogonal to the kernel, namely J\n\u22a5. So if L(G\u0302) = \u2206(D\u0302) is close to L(G?) we expect \u2206(C\u0302) to be close to \u2206(C?). The next theorem quantifies this.\nTheorem 3. Consider the setting of Theorems 1 and 2 and let C\u0302,C? be defined as above. Then\n1 2 ( n 2 )\u2016C\u0302 \u2212C?\u20162F \u2264 L\u03bb4C2f |S| (\u221a 18|S| log(n) n + \u221a 3 3 log n ) + L\u03b3 4C2f \u221a 288 log 2/\u03b4 |S|\nProof. By combining Theorem 2 with the prediction error bounds obtainined in 1 we see that\n2C2f n ( n\u22121 2 )\u2016L(G\u0302)\u2212 L(G?)\u20162F \u2264 4L\u03bb|S| (\u221a 18|S| log(n) n + \u221a 3 3 log n ) + L\u03b3 \u221a 288 log 2/\u03b4 |S| .\nNext we employ the following restricted isometry property of \u2206 on the subspace J\u22a5 whose proof is in the supplementary materials.\nLemma 3. Let D and D\u2032 be two different distance matrices of n points in Rd and Rd\u2032 . Let C and C \u2032 be the components of D and D\u2032 orthogonal to J . Then\nn\u2016C \u2212C \u2032\u20162F \u2264 \u2016\u2206(C)\u2212\u2206(C \u2032)\u20162 = \u2016\u2206(D)\u2212\u2206(D\u2032)\u20162 \u2264 2(n\u2212 1)\u2016C \u2212C \u2032\u20162F .\nThe result then follows.\nThis implies that by collecting enough samples, we can recover the centered distance matrix. By applying the discussion following Theorem 1 when G? is rank d, we can state an upperbound of 1\n2(n2) \u2016C\u0302 \u2212C?\u20162F \u2264\nO ( L\u03b3 C2f \u221a dn log(n)+log(1/\u03b4) |S| ) . However, it is still not clear that this is enough to recover D? or G?.\nRemarkably, despite this unknown component being in the kernel, we show next that it can be recovered. Though \u2206 is not invertible on Snh in general, we can provide a heuristic argument for why we might still expect it to be invertible on the (non-linear) space of low rank distance matrices. Since a distance matrix of n points in Rd is at most rank d+ 2, the space of all distance matrices has at most n(d+ 2) degrees of freedom. Now \u2206 is a rank ( n 2 ) \u2212 1 operator and since ( n 2 ) \u2212 1 n(d + 2) for n > d + 2, it is not unreasonable to hope that the entries of \u2206(D) provide enough information to parametrize the set of rank d+ 2 Euclidean distance matrices. In fact as the next theorem will show, the intuition that \u2206 is invertible on the set of low rank distance matrices is in fact true. Perhaps even more surprisingly, merely knowing the centered distance matrix C uniquely determines D.\nTheorem 4. Let D be a distance matrix of n points in Rd, let C be the component of D orthogonal to the kernel of L, and let \u03bb2(C) denote the second largest eigenvalue of C. If n > d+ 2, then\nD = C + \u03bb2(C)J .\nThis shows that D is uniquely determined as a function of C. Therefore, since \u2206(D) = \u2206(C) and because C is orthogonal to the kernel of \u2206, the distance matrix D can be recovered from \u2206(D), even though the linear operator \u2206 is non-invertible.\nWe now provide a proof of Theorem 4. In preparing this paper for publication we became aware of an alterntive proof of this theorem [15]. We nevertheless present our independently derived proof next for completeness.\nProof. To prove Theorem 4 we first state two simple lemmas, which are proved in the supplementary material.\nLemma 4. Let D be a Euclidean distance matrix on n points. Then D is negative semidefinite on the subspace\n1\u22a5 := {x \u2208 Rn|1Tx = 0}.\nFurthermore, ker(D) \u2282 1\u22a5.\nLemma 5. If D is an n \u00d7 n distance matrix of rank r, then D has a single positive eigenvalue, n \u2212 r eigenvalues equal to 0, and r \u2212 1 negative eigenvalues.\nWe will use the following notation. For any matrix M , let \u03bbi(M) denote its ith largest eigenvalue. We will show that for \u03c3 > 0 and any n\u00d7 n distance matrix D with n > d+ 2,\n\u03bb2(D \u2212 \u03c3J) = \u03c3 .\nSince C = D \u2212 \u03c3DJ , this proves the theorem. Note that \u03bbi(D \u2212 \u03c311T + \u03c3I) = \u03bbi(D \u2212 \u03c311T ) + \u03c3, for 1 \u2264 i \u2264 n and \u03c3 arbitrary. So it suffices to show that \u03bb2(D \u2212 \u03c311T ) = 0. Let the rank of D be r where r \u2264 d+ 2 and consider an eigendecomposition D = USUT where U is unitary. If v is an eigenvector of D \u2212 \u03c311T with eigenvalue \u03bb, then\nDv \u2212 \u03c311Tv = \u03bbv \u21d2 USUTv \u2212 \u03c31(UT1)TUTv = \u03bbv \u21d2 SUTv \u2212 \u03c3UT1(UT1)TUTv = \u03bbUTv.\nSo UTv is an eigenvalue of S \u2212 \u03c3UT1(UT1)T with the same eigenvalue \u03bb. Denoting z = UT1, we see that \u03bbi(D \u2212 \u03c311T ) = \u03bbi(S \u2212 \u03c3zzT ) for all 1 \u2264 i \u2264 n.\nBy a result on interlaced eigenvalues [16, Cor 4.3.9],\n\u03bb1(S) \u2265 \u03bb1(S \u2212 \u03c3zzT ) \u2265 \u03bb2(S) \u2265 \u03bb2(S \u2212 \u03c3zzT ) \u2265 \u00b7 \u00b7 \u00b7 \u2265 \u03bbn(S) \u2265 \u03bbn(S \u2212 \u03c3zzT ).\nFrom the above inequality and using the fact that \u03bb2(S) = 0 (since n > d+ 2), it is clear that S \u2212 \u03c3zzT has at least one positive eigenvalue. It is also clear that since \u03c3 > 0, each eigenvalue of S \u2212 \u03c3zzT is bounded above by an eigenvalue of S, so S \u2212 \u03c3zzt has at least r \u2212 1 negative eigenvalues. Hence dim kerS \u2212 \u03c3zzT \u2264 n\u2212 r.\nIf x \u2208 kerD, then 1Tx = 0 by lemma 4. Thus x \u2208 kerD \u2212 \u03c311T and so UTx \u2208 kerS \u2212 \u03c3zzT . This implies that UT kerD \u2282 kerS \u2212 \u03c3zzT and dim kerS \u2212 \u03c3zzT \u2265 n \u2212 r. However from the above, dim kerS \u2212 \u03c3zzT \u2264 n\u2212 r. Hence we can conclude that \u03bb2(S \u2212 \u03c3zzT ) = 0.\nThe previous theorem along with Theorem 3 guarantees that we can recover G? as we increase the number of triplets sampled. We summarize this in our final theorem, which follows directly from Theorems 3 and 4.\nTheorem 5. Assume n > d+ 2 and consider the setting of Theorems 1 and 2. As |S| \u2192 \u221e, the maximum likelihood estimator\nG\u0302 = \u22121 2 V (C\u0302 + \u03bb2(C\u0302)J)V\nconverges to G?."}, {"heading": "5 Experimental Study", "text": "The section empirically studies the properties of our estimators suggested by our theory. It is not an attempt to perform an exhaustive empirical evaluation of different embedding techniques; for that see [17, 4, 6, 3]. In what follows each of the n points is generated randomly: xi \u223c N (0, 12dId) \u2208 R\nd, i = 1, . . . , n, motivated by the observation that\nE[|\u3008Lt,G?\u3009|] = E [\u2223\u2223 \u2016xi \u2212 xj\u201622 \u2212 ||xi \u2212 xk||22 \u2223\u2223] \u2264 E[\u2016xi \u2212 xj\u201622] = 2E[\u2016xi\u201622] = 1\nfor any triplet t = (i, j, k). We perform 36 trials to see the effect of different random samples of triplets. We report the prediction error on a holdout set of 10, 000 triplets and the error in Frobenius norm of the estimated Gram matrix. Three algorithms are considered. For each, the domain of the objective variable G is the space of symmetric positive semi-definite matrices. None of the methods impose the constraint maxij |Gij | \u2264 \u03b3 (as done in our theoretical analysis), since this was used to simplify the analysis and does not have a significant impact in practice. Rank-d Projected Gradient Descent (PGD) performs gradient descent on the objective R\u0302S(G) = 1|S| \u2211 t\u2208S log(1 + exp(\u2212yt\u3008Lt,G\u3009)) with line search, projecting onto the subspace spanned by the top d eigenvalues at each step (i.e. setting the bottom n \u2212 d eigenvalues to 0). Nuclear Norm PGD performs gradient descent on R\u0302S(G) projecting onto the nuclear norm ball with radius \u2016G?\u2016\u2217, where G? = XTX is the Gram matrix of latent embedding. The nuclear norm projection can have the undesirable effect of shrinking the non-zero eigenvalues toward the origin. To compensate for this potential bias, we also employ Nuclear Norm PGD Debiased, which takes the (biased) output of Nuclear Norm PGD, decomposes it into UEUT where U \u2208 Rn\u00d7d are the top d eigenvectors, and outputs Udiag(s\u0302)UT where s\u0302 = arg mins\u2208Rd R\u0302S(Udiag(s)U\nT ). This last algorithm is motivated by the fact observation that heuristics like minimizing \u2016 \u00b7 \u20161 or \u2016 \u00b7 \u2016\u2217 are good at identifying the true support or basis of a signal, but output biased magnitudes [18]. Rank-d PGD and Nuclear Norm PGD Debiased are novel ordinal embedding algorithms.\nFigure 1 presents how the algorithms behave in n = 64 and d = 2, 8 on the left and right, respectively. We observe that the unbiased nuclear norm solution behaves near-identically to the rank-d solution in this\ncase and remark that this was observed in all of our experiments (see the supplementary materials for a variety values of n, d, and scalings of G?). A popular technique for recovering rank d embeddings is to perform (stochastic) gradient descent on R\u0302S(UTU) with objective variable U \u2208 Rn\u00d7d taken as the embedding [17, 4, 6]. In all of our experiments this method produced Gram matrices that were nearly identical to those produced by our Rank-d-PGD method, but Rank-d-PGD was an order of magnitude faster in our implementation. Also, in light of our isometry theorem, we can show that the Hessian of E[R\u0302S(G)] is nearly a scaled identity, which leads us to hypothesize that a globally optimal linear convergence result for this non-convex optimization may be possible using the techniques of [19, 20]. Finally, we note that previous literature has reported that nuclear norm optimizations like Nuclear Norm PGD tend to produce less accurate embeddings than those of non-convex methods [4, 6]. We see from the results that the Nuclear Norm PGD Debiased appears to close the performance gap between the convex and non-convex solutions."}, {"heading": "6 Future work", "text": "For any fixed set of n points in Rd and randomly selected distance comparison queries, our results show that the component of G? (the Gram matrix associated with the points) in the span of all possible distance comparisons can be accurately recovered from O(dn log n) queries, and we conjecture these bounds to be tight. Moreover, we proved the existence of an estimator G\u0302 such that as the number of queries grows, we have G\u0302 \u2192 G?. A focus of our ongoing work is characterizing finite sample bounds for the rate at which G\u0302\u2192 G?. One way of approaching such a result is showing\nc1\u2016D \u2212D\u2032\u20162F \u2264 \u2016\u2206(D)\u2212\u2206(D\u2032)\u20162 \u2264 c2\u2016D \u2212D\u2032\u20162F\nfor the tightest possible constants c1 and c2. By inspecting our proofs, we can provide satisfying values for c1 and c2, however, they differ by a factor of n and hence we do not believe these to be tight. Empirically we observe that if n d+ 2, then the ratio of c1 to c2 appears to be independent of n."}, {"heading": "7 Supplementary Materials for \u201cFinite Sample Error Bounds for Ordinal", "text": "Embedding\u201d"}, {"heading": "7.1 Proof of Lemma 1", "text": "Lemma 1. For all t \u2208 T , \u03bb1(Lt) = \u2016Lt\u2016 = \u221a 3\nin addition if n \u2265 3 \u2016Et[L2t ]\u2016 = 6\nn\u2212 1 \u2264 9 n\nProof. Note that L3t \u2212 3Lt = 0 for all t \u2208 T . Thus by the Cayley-Hamilton theorem, \u221a\n3 is the largest eigenvalue of Lt. A computation shows that the submatrix of L2t corresponding to i, j, k is 2 \u22121 \u22121\u22121 2 \u22121\n\u22121 \u22121 2  and every other element of L2t is zero. Summing over the t \u2208 T then gives,\nE[L2t ] = 1 n ( n\u22121 2 )\u2211 t\u2208T L2t =  6 n\n\u22126 n(n\u22121) \u00b7 \u00b7 \u00b7\n\u22126 n(n\u22121)\n\u22126 n(n\u22121)\n6 n . . .\n\u22126 n(n\u22121)\n... . . . . . . ...\n\u22126 n(n\u22121) . . .\n\u22126 n(n\u22121)\n6 n  This matrix can be rewritten as 6nI \u2212 6 n(n\u22121)J . The eigenvalues of J are \u22121 with multiplicity n\u2212 1 and\nn\u2212 1 with multiplicity 1. Hence the largest eigenvalue of E[L2t ] is 6n\u22121 ."}, {"heading": "7.2 Proof of Theorem 2", "text": "Proof. For y, z \u2208 (0, 1) let g(z) = z log zy + (1 \u2212 z) log 1\u2212z 1\u2212y . Then g \u2032(z) = log z1\u2212z \u2212 log y\n1\u2212y and g\u2032\u2032(z) = 1z(1\u2212z) . By taking a Taylor series around y,\ng(z) \u2265 (z \u2212 y) 2/2\nsupx\u2208[0,1] x(1\u2212 x) \u2265 2(z \u2212 y)2.\nNow applying this to z = f(\u3008Lt,G?\u3009) and y = f(\u3008Lt,G\u3009) gives\nf(\u3008Lt,G?\u3009) log f(\u3008Lt,G ?\u3009) f(\u3008Lt,G\u3009) + (1\u2212 f(\u3008Lt,G ?\u3009)) log 1\u2212f(\u3008Lt,G ?\u3009) 1\u2212f(\u3008Lt,G\u3009) \u2265 2(f(\u3008Lt,G ?\u3009)\u2212 f(\u3008Lt,G\u3009))2\n\u2265 2C2f (\u3008Lt,G ?\u3009 \u2212 \u3008Lt,G\u3009)2\nwhere the last line comes from applying Taylor\u2019s theorem to f , f(x)\u2212 f(y) \u2265 infz\u2208[x,y] f \u2032(z)(x\u2212 y) for any x, y. Thus\nR(G)\u2212R(G?) = 1 |T | \u2211 t\u2208T f(\u3008Lt,G?\u3009) log f(\u3008Lt,G ?\u3009) f(\u3008Lt,G\u3009) + (1\u2212 f(\u3008Lt,G ?\u3009)) log 1\u2212f(\u3008Lt,G ?\u3009) 1\u2212f(\u3008Lt,G\u3009)\n\u2265 2C2f |T | \u2211 t\u2208T (\u3008Lt,G?\u3009 \u2212 \u3008Lt,G\u3009)2 = 2C2f |T | \u2211 t\u2208T (\u3008Lt,G\u2212G?\u3009)2 = 2C2f |T | \u2016L(G)\u2212 L(G\u2217)\u201622."}, {"heading": "7.3 Proof of Lemma 3", "text": "Lemma 3. Let D and D\u2032 be two different distance matrices of n points in Rd and Rd\u2032 respectively. Let C and C \u2032 be the components of D and D\u2032 orthogonal to J . Then\nn\u2016C \u2212C \u2032\u20162F \u2264 \u2016\u2206(C)\u2212\u2206(C \u2032)\u20162 = \u2016\u2206(D)\u2212\u2206(D\u2032)\u20162 \u2264 2(n\u2212 1)\u2016C \u2212C \u2032\u20162F .\nWe can view the operator \u2206 defined above as acting on the space R( n 2) where each symmetric hollow matrix is identified with vectorization of it\u2019s upper triangular component. With respect to this basis \u2206 is an n ( n\u22121 2 ) \u00d7 ( n 2 ) matrix, which we will denote by \u2206. Since C and C \u2032 are orthogonal to the kernel of \u2206, the lemma follows immediately from the following characterization of the eigenvalues of \u2206T\u2206.\nLemma 6. \u2206T\u2206 : Snh \u2192 Snh has the following eigenvalues and eigenspaces,\n\u2022 Eigenvalue 0, with a one dimensional eigenspace.\n\u2022 Eigenvalue n, with a n\u2212 1 dimensional eigenspace. \u2022 Eigenvalue 2(n\u2212 1), with a ( n 2 ) \u2212 n dimensional eigenspace.\nProof. The rows of \u2206 are indexed by triplets t \u2208 T and columns indexed by pairs i, j with 1 \u2264 i < j \u2264 n and vice-versa for \u2206T . The row of \u2206T corresponding to the pair i, j is supported on columns corresponding to triplets t = (l,m, n) where m < n and l and one of m or n form the pair i, j or j, i. Specifically, letting [\u2206T ](i,j),t denote the entry of \u2206T corresponding to row i, j and column t,\n\u2022 if l = i,m = j then [\u2206T ](i,j),t = 1\n\u2022 if l = i, n = j then [\u2206T ](i,j),t = \u22121\n\u2022 if l = j,m = i then [\u2206T ](i,j),t = 1\n\u2022 if l = j, n = i then [\u2206T ](i,j),t = \u22121\nUsing this one can easily check that\n[\u2206T\u2206D]i,j = \u2211\n(i,j,k)\u2208T\nDij \u2212Dik \u2212 \u2211\n(i,k,j)\u2208T\nDik \u2212Dij + \u2211\n(j,i,k)\u2208T\nDji \u2212Djk \u2212 \u2211\n(j,k,i)\u2208T\nDjk \u2212Dji\n= 2(n\u2212 1)Dij \u2212 \u2211 n6=i Din \u2212 \u2211 n 6=j Djn. (10)\nThis representation allows us to find the eigenspaces mentioned above very quickly. Eigenvalue 0. From the above discussion, we know the kernel is generated by J = 11T \u2212 I . Eigenvalue 2(n\u2212 1). This eigenspace corresponds to all symmetric hollow matrices such that D1 = 0. For such a matrix each row and column sum is zero and so in particular, the sums in (10) are both zero. Hence for such a D, [\u2206T\u2206D]i,j = 2(n\u2212 1)Dij The dimension of this subspace is ( n 2 ) \u2212 n, indeed there are ( n 2 ) degree of freedom to choose the elements of D and D1 = 0 adds n constraints. Eigenvalue n. This eigenspace corresponds to the span of the matrices D(i) defined as,\nD(i) = \u2212n(ei1T + 1eTi \u2212 2eieTi ) + 2J\nwhere ei is the standard basis vector with a 1 in the ith row and 0 elsewhere. As an example,\nD(1) =  0 \u2212n+ 2 \u00b7 \u00b7 \u00b7 \u2212n+ 2... 2 \u00b7 \u00b7 \u00b7 2 \u2212n+ 2 2 \u00b7 \u00b7 \u00b7 0  . If i, j 6= m, then D(m)ij := [D\n(m)]ij = 2, and we can compute the row and column sums\u2211 n6=i D (m) in = \u2211 n6=j D (m) jn = 2(n\u2212 2)\u2212 n+ 2 = n\u2212 2.\nThis implies that D(m)ij = n, and so by (10)\n[\u2206T\u2206D(m)]i,j = 2(n\u2212 1) \u00b7 2\u2212 (n\u2212 2)\u2212 (n\u2212 2) = 2n = nD(m)ij .\nOtherwise, without loss of generality we can assume that i = m, j 6= m in which case, [D(m)]ij = \u2212n+ 2, the row and columns sums can be computed as\u2211\nn6=i D\n(m) in = (n\u2212 1)(\u2212n+ 2)\nand \u2211 n6=j D (m) in = n\u2212 2.\nPutting it all together,\n[\u2206T\u2206D(m)]m,j = 2(n\u2212 1) \u00b7 (\u2212n+ 2)\u2212 (n\u2212 1)(\u2212n+ 2)\u2212 (n\u2212 2) = (n\u2212 1)(\u2212n+ 2) + (\u2212n+ 2) = n(\u2212n+ 2)\n= nD (m) m,j\nand \u2206T\u2206D = nD. Note that the dimension of span\u3008D(i)\u3009 = n\u2212 1 since\u2211 m D(m) = 0."}, {"heading": "7.4 Proofs of Lemmas 4 and 5", "text": "Lemma 4. Let D be a Euclidean distance matrix on n points. Then D is negative semidefinite on the subspace\n1\u22a5 := {x \u2208 Rn|1Tx = 0}.\nFurthermore, ker(D) \u2282 1\u22a5.\nProof. The associated Gram matrix G = \u221212V DV is a positive semidefinite matrix. For x \u2208 1 \u22a5, Jx = \u2212x so xT ( \u22121\n2 V DV\n) x = \u22121\n2 xTDx \u2264 0\nestablishing the first part of the theorem. Now if x \u2208 kerD,\n0 \u2264 \u22121 2 xTV DV x = \u22121 2 xT11TD11Tx = \u22121 2 1TD1(1Tx)2 \u2264 0 ,\nwhere the last inequality follows from the fact that 1TD1 > 0 since D is non-negative. Hence 1Tx = 0 and kerD \u2282 1\u22a5.\nLet \u03bbi(M) denote the i-th largest eigenvalue of a matrix M .\nLemma 5. If D is an n\u00d7 n distance matrix of rank r, then\n\u2022 D has a single positive eigenvalue\n\u2022 D has n\u2212 r zero eigenvalues.\n\u2022 D has r \u2212 1 negative eigenvalues.\nProof. From above we know that r \u2264 d+ 2. We use the Courant-Fisher theorem [16] .\n\u03bb1(D) = max dimS=1 min 0 6=x\u2208S\nxTDx\nxTx\n\u2265 1 TD1\n1T1 > 0.\nSo we see the largest eigenvalue of D is necessarily positive. Now for 2 \u2264 k \u2264 n,\n\u03bbk(D) = min dimS=k max 06=x\u2208S\nxTDx\nxTx\n\u2264 min U\u22821\u22a5,dimU=k max 06=x\u2208U\nxTDx\nxTx\n\u2264 0,\nwhere the last step follows from the negative definiteness of D on 1\u22a5.\n8 Additional Empirical Results"}], "references": [{"title": "The analysis of proximities: Multidimensional scaling with an unknown distance function", "author": ["Roger N Shepard"], "venue": "i. Psychometrika,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 1962}, {"title": "Nonmetric multidimensional scaling: a numerical method", "author": ["Joseph B Kruskal"], "venue": null, "citeRegEx": "2", "shortCiteRegEx": "2", "year": 1964}, {"title": "Generalized non-metric multidimensional scaling", "author": ["Sameer Agarwal", "Josh Wills", "Lawrence Cayton", "Gert Lanckriet", "David J Kriegman", "Serge Belongie"], "venue": "In International Conference on Artificial Intelligence and Statistics,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2007}, {"title": "Adaptively learning the crowd kernel", "author": ["Omer Tamuz", "Ce Liu", "Ohad Shamir", "Adam Kalai", "Serge J Belongie"], "venue": "In Proceedings of the 28th International Conference on Machine Learning", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2011}, {"title": "Low-dimensional embedding using adaptively selected ordinal data", "author": ["Kevin G Jamieson", "Robert D Nowak"], "venue": "In Communication, Control, and Computing (Allerton),", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2011}, {"title": "Stochastic triplet embedding", "author": ["Laurens Van Der Maaten", "Kilian Weinberger"], "venue": "In Machine Learning for Signal Processing (MLSP),", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2012}, {"title": "Learning multi-modal similarity", "author": ["Brian McFee", "Gert Lanckriet"], "venue": "The Journal of Machine Learning Research,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2011}, {"title": "Uniqueness of ordinal embedding", "author": ["Matth\u00e4us Kleindessner", "Ulrike von Luxburg"], "venue": "In COLT, pages 40\u201367,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2014}, {"title": "Local ordinal embedding", "author": ["Yoshikazu Terada", "Ulrike V Luxburg"], "venue": "In Proceedings of the 31st International Conference on Machine Learning", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2014}, {"title": "Some theory for ordinal embedding", "author": ["Ery Arias-Castro"], "venue": "arXiv preprint arXiv:1501.02861,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2015}, {"title": "1-bit matrix completion", "author": ["Mark A Davenport", "Yaniv Plan", "Ewout van den Berg", "Mary Wootters"], "venue": "Information and Inference, page iau006,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2014}, {"title": "Convex Optimization & Euclidean Distance Geometry", "author": ["Jon Dattorro"], "venue": "Meboo Publishing USA,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2011}, {"title": "Theory of classification: A survey of some recent advances", "author": ["St\u00e9phane Boucheron", "Olivier Bousquet", "G\u00e1bor Lugosi"], "venue": "ESAIM: probability and statistics,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2005}, {"title": "An introduction to matrix concentration inequalities", "author": ["Joel A. Tropp"], "venue": null, "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2015}, {"title": "Euclidean distance matrices: new characterization and boundary properties", "author": ["Pablo Tarazaga", "Juan E. Gallardo"], "venue": "Linear and Multilinear Algebra,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2009}, {"title": "Next: A system for real-world development, evaluation, and application of active learning", "author": ["Kevin G Jamieson", "Lalit Jain", "Chris Fernandez", "Nicholas J Glattard", "Rob Nowak"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2015}, {"title": "Conditional gradient with enhancement and truncation for atomic norm regularization", "author": ["Nikhil Rao", "Parikshit Shah", "Stephen Wright"], "venue": "In NIPS workshop on Greedy Algorithms,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2013}, {"title": "Sharp time\u2013data tradeoffs for linear inverse problems", "author": ["Samet Oymak", "Benjamin Recht", "Mahdi Soltanolkotabi"], "venue": "arXiv preprint arXiv:1507.04793,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2015}], "referenceMentions": [{"referenceID": 0, "context": "This is a classic problem that is often used to visualize perceptual similarities [1, 2].", "startOffset": 82, "endOffset": 88}, {"referenceID": 1, "context": "This is a classic problem that is often used to visualize perceptual similarities [1, 2].", "startOffset": 82, "endOffset": 88}, {"referenceID": 2, "context": "Recently, several authors have proposed a variety of new algorithms for learning a metric embedding from ordinal information [3, 4, 5, 6, 7].", "startOffset": 125, "endOffset": 140}, {"referenceID": 3, "context": "Recently, several authors have proposed a variety of new algorithms for learning a metric embedding from ordinal information [3, 4, 5, 6, 7].", "startOffset": 125, "endOffset": 140}, {"referenceID": 4, "context": "Recently, several authors have proposed a variety of new algorithms for learning a metric embedding from ordinal information [3, 4, 5, 6, 7].", "startOffset": 125, "endOffset": 140}, {"referenceID": 5, "context": "Recently, several authors have proposed a variety of new algorithms for learning a metric embedding from ordinal information [3, 4, 5, 6, 7].", "startOffset": 125, "endOffset": 140}, {"referenceID": 6, "context": "Recently, several authors have proposed a variety of new algorithms for learning a metric embedding from ordinal information [3, 4, 5, 6, 7].", "startOffset": 125, "endOffset": 140}, {"referenceID": 7, "context": "For example, it has been shown that the correct embedding can be learned in the limit as the number of items grows [8, 9, 10].", "startOffset": 115, "endOffset": 125}, {"referenceID": 8, "context": "For example, it has been shown that the correct embedding can be learned in the limit as the number of items grows [8, 9, 10].", "startOffset": 115, "endOffset": 125}, {"referenceID": 9, "context": "For example, it has been shown that the correct embedding can be learned in the limit as the number of items grows [8, 9, 10].", "startOffset": 115, "endOffset": 125}, {"referenceID": 0, "context": "where the link function f : R\u2192 [0, 1] is known.", "startOffset": 31, "endOffset": 37}, {"referenceID": 10, "context": "We note that a problem known as one-bit matrix completion has similarly used link functions to recover structure [11].", "startOffset": 113, "endOffset": 117}, {"referenceID": 11, "context": "We refer the reader to [12] for an insightful and thorough treatment of the properties of distance matrices.", "startOffset": 23, "endOffset": 27}, {"referenceID": 12, "context": "The proof follows from standard statistical learning theory techniques, see for instance [13].", "startOffset": 89, "endOffset": 93}, {"referenceID": 4, "context": "The above display says that |S| must scale like dn log(n) which is consistent with known finite sample bounds [5].", "startOffset": 110, "endOffset": 113}, {"referenceID": 14, "context": "In preparing this paper for publication we became aware of an alterntive proof of this theorem [15].", "startOffset": 95, "endOffset": 99}, {"referenceID": 15, "context": "It is not an attempt to perform an exhaustive empirical evaluation of different embedding techniques; for that see [17, 4, 6, 3].", "startOffset": 115, "endOffset": 128}, {"referenceID": 3, "context": "It is not an attempt to perform an exhaustive empirical evaluation of different embedding techniques; for that see [17, 4, 6, 3].", "startOffset": 115, "endOffset": 128}, {"referenceID": 5, "context": "It is not an attempt to perform an exhaustive empirical evaluation of different embedding techniques; for that see [17, 4, 6, 3].", "startOffset": 115, "endOffset": 128}, {"referenceID": 2, "context": "It is not an attempt to perform an exhaustive empirical evaluation of different embedding techniques; for that see [17, 4, 6, 3].", "startOffset": 115, "endOffset": 128}, {"referenceID": 16, "context": "This last algorithm is motivated by the fact observation that heuristics like minimizing \u2016 \u00b7 \u20161 or \u2016 \u00b7 \u2016\u2217 are good at identifying the true support or basis of a signal, but output biased magnitudes [18].", "startOffset": 198, "endOffset": 202}, {"referenceID": 15, "context": "A popular technique for recovering rank d embeddings is to perform (stochastic) gradient descent on R\u0302S(UU) with objective variable U \u2208 Rn\u00d7d taken as the embedding [17, 4, 6].", "startOffset": 164, "endOffset": 174}, {"referenceID": 3, "context": "A popular technique for recovering rank d embeddings is to perform (stochastic) gradient descent on R\u0302S(UU) with objective variable U \u2208 Rn\u00d7d taken as the embedding [17, 4, 6].", "startOffset": 164, "endOffset": 174}, {"referenceID": 5, "context": "A popular technique for recovering rank d embeddings is to perform (stochastic) gradient descent on R\u0302S(UU) with objective variable U \u2208 Rn\u00d7d taken as the embedding [17, 4, 6].", "startOffset": 164, "endOffset": 174}, {"referenceID": 17, "context": "Also, in light of our isometry theorem, we can show that the Hessian of E[R\u0302S(G)] is nearly a scaled identity, which leads us to hypothesize that a globally optimal linear convergence result for this non-convex optimization may be possible using the techniques of [19, 20].", "startOffset": 264, "endOffset": 272}, {"referenceID": 3, "context": "Finally, we note that previous literature has reported that nuclear norm optimizations like Nuclear Norm PGD tend to produce less accurate embeddings than those of non-convex methods [4, 6].", "startOffset": 183, "endOffset": 189}, {"referenceID": 5, "context": "Finally, we note that previous literature has reported that nuclear norm optimizations like Nuclear Norm PGD tend to produce less accurate embeddings than those of non-convex methods [4, 6].", "startOffset": 183, "endOffset": 189}], "year": 2016, "abstractText": "The goal of ordinal embedding is to represent items as points in a low-dimensional Euclidean space given a set of constraints in the form of distance comparisons like \u201citem i is closer to item j than item k\u201d. Ordinal constraints like this often come from human judgments. To account for errors and variation in judgments, we consider the noisy situation in which the given constraints are independently corrupted by reversing the correct constraint with some probability. This paper makes several new contributions to this problem. First, we derive prediction error bounds for ordinal embedding with noise by exploiting the fact that the rank of a distance matrix of points in R is at most d+ 2. These bounds characterize how well a learned embedding predicts new comparative judgments. Second, we investigate the special case of a known noise model and study the Maximum Likelihood estimator. Third, knowledge of the noise model enables us to relate prediction errors to embedding accuracy. This relationship is highly non-trivial since we show that the linear map corresponding to distance comparisons is non-invertible, but there exists a nonlinear map that is invertible. Fourth, two new algorithms for ordinal embedding are proposed and evaluated in experiments.", "creator": "LaTeX with hyperref package"}}}