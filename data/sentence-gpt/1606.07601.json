{"id": "1606.07601", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "24-Jun-2016", "title": "Evaluation method of word embedding by roots and affixes", "abstract": "Word embedding has been shown to be remarkably effective in a lot of Natural Language Processing tasks. However, existing models still have a couple of limitations in interpreting the dimensions of word vector. In this paper, we provide a new approach---roots and affixes model(RAAM)---to interpret it from the intrinsic structures of natural language. The new method is designed to understand the complexity of the RAMs used in word vector.\n\n\n\nFirst, we take a look at the various features associated with RAMs in the word vector. For example, the top of the head of the RAM on a given word contains the word\u203a(e.g. the number of characters). This model is very simple--the first thing you need to do is create an RAM and let the RAM appear in the corresponding area. Let\u203as say that the rAM is the highest score on the first word in the RAM. Then, the next RAM appears in the corresponding area, as in the RAM on the second word.\nNext, we apply the RAM to the RAM to determine the RAM that fits our description. Here, we use the RAM to define the values from the RAM to the RAM to the RAM to the RAM to the RAM to the RAM to the RAM to the RAM to the RAM to the RAM to the RAM to the RAM to the RAM to the RAM to the RAM to the RAM to the RAM to the RAM to the RAM to the RAM to the RAM to the RAM to the RAM to the RAM to the RAM to the RAM to the RAM to the RAM to the RAM to the RAM to the RAM to the RAM to the RAM to the RAM to the RAM to the RAM to the RAM to the RAM to the RAM to the RAM to the RAM to the RAM to the RAM to the RAM to the RAM to the RAM to the RAM to the RAM to the RAM to the RAM to the RAM to the RAM to the RAM to the RAM to the RAM to the RAM to the RAM to the RAM to the RAM to the RAM to the RAM to the RAM to the RAM to the RAM to the RAM to the R", "histories": [["v1", "Fri, 24 Jun 2016 08:35:01 GMT  (40kb)", "http://arxiv.org/abs/1606.07601v1", "7 pages,3 figures"]], "COMMENTS": "7 pages,3 figures", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["kebin peng"], "accepted": false, "id": "1606.07601"}, "pdf": {"name": "1606.07601.pdf", "metadata": {"source": "CRF", "title": null, "authors": [], "emails": ["kebinpeng@act.buaa.edu.cn"], "sections": [{"heading": null, "text": "ar X\niv :1\n60 6.\n07 60\n1v 1\n[ cs\n.C L\n] 2\n4 Ju\nn 20\n16\nWord embedding has been shown to be remarkably effective in a lot of Natural Language Processing tasks. However, existing models still have a couple of limitations in interpreting the dimensions of word vector. In this paper, we provide a new approach\u2014roots and affixes model(RAAM)\u2014to interpret it from the intrinsic structures of natural language. Also it can be used as an evaluation measure of the quality of word embedding. We introduce the information entropy into our model and divide the dimensions into two categories, just like roots and affixes in lexical semantics. Then considering each category as a whole rather than individually. We experimented with English Wikipedia corpus. Our result show that there is a negative linear relation between the two attributes and a high positive correlation between our model and downstream semantic evaluation tasks."}, {"heading": "1 Introduction", "text": "Distributed representation of word has become increasingly popular currently. It can be utilized to various downstream NLP tasks. For example, bilingual word embedding for phrase-based machine translation (Zou et al., 2013), enhancing the coverage of POS tagging (Huang et al., 2014). Success of word embedding inspires us to go deep into the structure of distributional representations. Existing works such as CBOW and skip-gram in the toolbox word2vec (Mikolov et al., 2013) using large unlabeled corpora to train the model. However, every single dimension in the vector has no obvious meaning. We are not yet clear why it is efficient. Hence, interpreting the meaning of the dimension may be essential to answer these questions.\nThere are some notable interpretation embedding model includes Subspace Alignment (Tsvetkov et al., 2015), Non-distributional Word Vector Representations (Faruqui and Dyer, 2015). Non-Negative Sparse Embedding (Murphy et al., 2012), Online Learning of Interpretable Word Embedding (Luo et al., 2015) and so on. They use existing tools, for example, WordNet, to allocate the dimension with a certain meaning. Although these models have revealed some dimensions in vector do have clearly meaning, it turns out that they are not suited for high dimension vector because the length of WordNet is limited but the vector can be very long. Meanwhile, they do not consider the sequence of allocation.\nIn this paper, we propose a new efficient model named roots and affixes model(RAAM), to clarify the meaning of the dimensions in a word vector from the intrinsic structures of natural language\u2014word and sentence. On the one hand, word and sentence are the natural structure in text, we assume this structure should be incarnated in word vector. On the other hand, we introduce the information entropy into our model and define two attributes to a dimension which named: word entropy and sentence entropy. Through the two attributes, we can divide the dimensions in word vector into two classes at different level: word level and sentence level, just like roots and affixes in vocabulary. Different roots and affixes represent different semantic meaning. Analogously, the different level of word vector represents different\nThis work is licensed under a Creative Commons Attribution 4.0 International Licence. Licence details: http://creativecommons.org/licenses/by/4.0/\nsemantic meaning in word vector. So, we believe these two attributes represent two aspects of word vector.\nCompared with previous works, we have made three main contributions: (1) we promote a new conception named roots and affixes model(RAAM) which are inspired by the entropy and lexical semantics. The conception can explain RAAM clearly (2) we define two new attributes for the dimensions in word vector which can reflect two distinct levels of semantic. (3) we discuss the interpretable embedding from a new point. Our model gives a very obvious hierarchies relationship among and sentence. These hierarchies are the natural structure, not man-made.\nThe rest of this paper is organized as follows: In the next section, we summarize some previous works. In section 3, we present our model formally. In section 4, we report experimental results. Finally, we conclude in section 5."}, {"heading": "2 Related works", "text": "In this section, we discuss some related works. Several interpretable embedding models have been developed to explain the meaning of the dimensions in a word vector. It can be partitioned into two kinds of methods. First, In (Tsvetkov et al., 2015) (Faruqui and Dyer, 2015), word level information from linguistic resources such as WordNet is extracted to construct word vectors. It dimensions can be integer or decimal. But, this representation cannot give meaning to all the dimensions because the length of the word vector is much longer than the attributes of a word in WordNet. Second, In(Murphy et al., 2012), distributional models which apply matrix factorization are introduced. In this model, word representations learned by Non-Negative Sparse Embedding is sparse, effective, and interpretable. The second method utilizes various object functions to train the word vector. Such as citeluoonline(Fyshe et al., 2014)(Lin, 2007). These models apply some new object functions such as projected gradient descent and no negative constraints for optimization. Nonetheless, Only few dimensions can be interpreted clearly. In addition, all the approach mentioned above do not show the relationship between words and sentences. Meanwhile, if we look the word vector as a kind of encoding, a dimension may be meaningless unless get them together. Hence, we are motivated to consider the dimensions as a whole and try to reveal the relationship between words and sentences."}, {"heading": "3 Word Vector Dimension Interpret Model", "text": "In this section, we introduce the basic structure of our model. Our model comprises 3 phases. In the first phase, We introduce the information entropy into our model and define two attributes for a dimension in the word vector: word entropy and sentence entropy. In the second phase, we calculate the two different attributes according to the formulations which defined in the first phrase. In the third phrase, we separate the word vector dimensions into two parts which we named word level and sentence level using the attributes stated above. In this section, we formally describe the model, which we call RAAM.\nThe RAAM\u2019s underlying hypothesis is that ith dimension in word vector is contributing to ith dimension in sentence vector or paragraph vector. It is motivated by two reasons: first, sentence is a natural structure in text rather than manual work. We guess the semantic meaning could be reflected in these natural structure. Second, explaining a single dimension is problematic. Because the vector dimension can be 300 or more. It is intricate to give every dimension a clearly meaning. So we attempt to get the dimension together and give an interpret them in the natural structure that a document has.\nFormally, there is a word (sentence) vector v={ p1 . . . pi . . . pl }. We look the pi as a random variables. Also, we have a matrix\n\n  dimention1 \u00b7 \u00b7 \u00b7 dimention1 wordvector1 p11 \u00b7 \u00b7 \u00b7 p1m ... ... ...\n... wordvectorm pn1 \u00b7 \u00b7 \u00b7 pnm\n\n  , (1)\npnm is as same as pi. And then, we use following two formulas to calculate the entropy of i dimension\nto the word level.\npj = exp[\u2212 (pi \u2212 \u00b5)2\n2\u03c32 ] (2)\nEwk (i) = l \u2211\nj=1\npjlogpj (3)\nEwk (i) Means the entropy of i dimension of k word to the word level.Second, we calculate the entropy of the sentence level. We have a sentenced vector v={ p1 . . . pi . . . pt }. So, according to our model, the entropy to the sentence vector is calculated by following formula:\nEsm(i) = t \u2211\nj=1\npj logpj (4)\nFinally, we concern the interpretation as a process which could decrease the information entropy. When we look at a word in a sentence, we get some information. That means the information entropy has been lowered. Further more, as information in word and sentence contains each other, we use mutual information which defines as\nI(X;Y ) = \u2211\nx\u2208X\n\u2211\ny\u2208Y\npxy(i, j)log pxy(i, j)\np(x)p(y) (5)\nThe number of I(x; y) means how much information does y have when we get x. x or y can be word, sentence, and paragraph. Information that y contains is inversely proportion to the number of I(xi; yi).\nwe calculate the entropy of the i dimension to the sentence level. pw(i) is the element of the matrix. Polk is the element in the sentence vector. pk is the element in the sentence vector.\nPxy(i, j) = 1\u221a 2\u03c0\u03c30\u03c31 exp(\u2212(pw(i)\u2212 \u00b50) 2 2\u03c32 0 )exp(\u2212(pk(i)\u2212 \u00b51) 2 2\u03c32 1 ) (6)\nIf we write exactly, we can get following formula:\nI(X;Y ) = l \u2211\ni=1\nt \u2211\nj=1\nPxy(i, j)log( Pxy(i, j)\nPj |Esm(i)| ) (7)\nSo, for every dimension, it has two attributes Esm(i) and E w k (i). This two attributes is a measure to a dimension. Through the experiment, we find the that there is a negative linear relationship between the two attributes. Thus, we can decide which attribute is more important and divide the whole dimension into two level: word level and sentenced level. For a dimension in word vector, for example, if the Esm(i) of this dimension is larger thanE w k (i), we can get a conclusion that this dimension of the word contributes more information to the sentence which the word belongs to than word itself. We can get all this kind of dimension into a class. we believe that class interpret the meaning of the whole sentence. The finally score is\nET =\nl \u2211\ni=1\nmax(Esm(i), E w k (i)) (8)"}, {"heading": "4 Experiment", "text": "We conducted sufficient experiment on real-world dataset to verify our model. All the codes were implemented in MATLAB. We select some state-of-the-art word vector models."}, {"heading": "4.1 Word vector models", "text": "We chose some state-of-art word vector models and compare those models with our RAAM model. We used English Wikipedia dataset of RAAMX. The vector dimension is 50, 100, 150, 200, 250, 300, 350, 400 500.\nSkip-Gram(SG) and CBOW. Word2Vec tool (Mikolov et al., 2013) (Mikolov et al., 2010) (Le and Mikolov, 2014) is very popular in nature language processing and effectively. This model uses Huffman code to represent the word. Then considering it as input to log classifier. The word is predicted within a given context window.\nGlove. GloVe is an unsupervised learning algorithm for obtaining vector representations for words. Training is performed on aggregated global word-word co-occurrence statistics from a corpus, and the resulting representations showcase interesting linear substructures of the word vector space(Pennington et al., 2014).\nGlove+WN, Glove+PPDB. We used the WordNet(WN), paraphrase database(PPDB) (Ganitkevitch et al., 2013) and (Faruqui et al., 2014) to enrich the semantic information of the word vector."}, {"heading": "4.2 Evaluation Benchmarks", "text": "We compare our RAAMX model with some standard semantic tasks. We will introduce these tasks briefly.\nWord Similarity. There are three different kinds of benchmarks: WS-353, MEN, SimLex-999. The WS-353 contains two sets of English word pairs along with human-assigned similarity judgements. The collection can be utilized to train and/or test computer algorithms implementing semantic similarity measures(Finkelstein et al., 2001). The MEN contains two sets of English word pairs (one for training and one for testing) together with human-assigned similarity judgments. The collection can be used to train and/or test computer algorithms implementing semantic similarity and relatedness measures(Bruni et al., 2014). SimLex-999 is a gold standard resource for the evaluation of models that learn the meaning of words and concepts(Hill et al., 2015). It can overcome the shortcomings of WS353 and contains 999 pairs of adjectives, nouns and verbs(Tsvetkov et al., 2015). We computer word similarity by cosine similarity. Semantic Task. Our semantic task is sentiment analysis task(Senti). Senti(Socher et al., 2013) is a classification task which between positive and negative movie reviews."}, {"heading": "5 Results", "text": "At the beginning, we display the relationship between the two hierarchies: word and sentence. From the figure1, 2, 3, we can draw a conclusion the there is a negative linear relationship between the two levels. And the linear relationship is not changed among the different of parameter. This conclusion proves our hypothesis that there is difference between the two hierarchies for one dimension.\nLater, according to (Tsvetkov et al., 2015), we compute the Pearons correlation coefficient r between RAAMXs scores and other word vector models. Our purpose is checking the correctness between these models. First, we use the task named Senti to make comparison. We set the vector dimensions in 300.\nAs we display on table 1, the Pearsons correlation between the RAAMX scores and other models scores is r=0.7903. Second, we show the correlation between two different train ways and RAAM scores in different dataset. From the table, we can see there is a high positive correlation between RAAM scores and two different train ways.\nSecond, we show the correlation between two different train ways and RAAM scores in different dataset. From the table 2 we can see there is high positive correlation between RAAM scores and two different train ways.\nNext, we show the correlation of RAAM with the different task and different word vector models with different dimensionality:\nTable 3 show that RAAM obtains a high positive correlation with downstream tasks. Finally, we show the correlation between RAAM score and Senti in different dimension in table 4. From the table we can see, the highest correlation is in the 400dimensions. After this point, the correlation starts to fall\nTo summarize, we observe a high positive correlation between RAAM and the downstream tasks, and across discrete models with vectors of different dimensionalities."}, {"heading": "6 Conclusion", "text": "In this paper, we visit the problem of the interpretable embedding and propose a new approach to interpret the meaning of the dimensions in a word vector. We design two attributes of a dimension in a word vector. According to the two attributes, we aggregate dimensions into two classes. We have experimented on several dataset. Our result suggests that the difference two class captures two levels of semantics of a word. We believe our approach delivers valuable information and provide a good way that interpret the meaning of a dimension for future research."}, {"heading": "Acknowledgement", "text": "We want to say thank you to all the anonymous reviewers for constructive feedback."}], "references": [{"title": "Multimodal distributional semantics", "author": ["Bruni et al.2014] Elia Bruni", "Nam-Khanh Tran", "Marco Baroni"], "venue": "J. Artif. Intell. Res.(JAIR),", "citeRegEx": "Bruni et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Bruni et al\\.", "year": 2014}, {"title": "Non-distributional word vector representations. arXiv preprint arXiv:1506.05230", "author": ["Faruqui", "Dyer2015] Manaal Faruqui", "Chris Dyer"], "venue": null, "citeRegEx": "Faruqui et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Faruqui et al\\.", "year": 2015}, {"title": "Retrofitting word vectors to semantic lexicons", "author": ["Jesse Dodge", "Sujay K Jauhar", "Chris Dyer", "Eduard Hovy", "Noah A Smith"], "venue": "arXiv preprint arXiv:1411.4166", "citeRegEx": "Faruqui et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Faruqui et al\\.", "year": 2014}, {"title": "Placing search in context: The concept revisited", "author": ["Evgeniy Gabrilovich", "Yossi Matias", "Ehud Rivlin", "Zach Solan", "Gadi Wolfman", "Eytan Ruppin"], "venue": "In Proceedings of the 10th international conference on World Wide Web,", "citeRegEx": "Finkelstein et al\\.,? \\Q2001\\E", "shortCiteRegEx": "Finkelstein et al\\.", "year": 2001}, {"title": "Interpretable semantic vectors from a joint model of brain-and text-based meaning", "author": ["Fyshe et al.2014] Alona Fyshe", "Partha P Talukdar", "Brian Murphy", "Tom M Mitchell"], "venue": "In Proceedings of the conference. Association for Computational Linguistics. Meeting,", "citeRegEx": "Fyshe et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Fyshe et al\\.", "year": 2014}, {"title": "Ppdb: The paraphrase database", "author": ["Benjamin Van Durme", "Chris Callison-Burch"], "venue": "In HLT-NAACL,", "citeRegEx": "Ganitkevitch et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Ganitkevitch et al\\.", "year": 2013}, {"title": "Simlex-999: Evaluating semantic models with (genuine) similarity estimation", "author": ["Hill et al.2015] Felix Hill", "Roi Reichart", "Anna Korhonen"], "venue": null, "citeRegEx": "Hill et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Hill et al\\.", "year": 2015}, {"title": "Learning representations for weakly supervised natural language processing", "author": ["Huang et al.2014] Fei Huang", "Arun Ahuja", "Doug Downey", "Yi Yang", "Yuhong Guo", "Alexander Yates"], "venue": null, "citeRegEx": "Huang et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Huang et al\\.", "year": 2014}, {"title": "Distributed representations of sentences and documents. arXiv preprint arXiv:1405.4053", "author": ["Le", "Mikolov2014] Quoc V Le", "Tomas Mikolov"], "venue": null, "citeRegEx": "Le et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Le et al\\.", "year": 2014}, {"title": "Projected gradient methods for nonnegative matrix factorization", "author": ["Chih-Jen Lin"], "venue": "Neural computation,", "citeRegEx": "Lin.,? \\Q2007\\E", "shortCiteRegEx": "Lin.", "year": 2007}, {"title": "Online learning of interpretable word embeddings", "author": ["Luo et al.2015] Hongyin Luo", "Zhiyuan Liu", "Huanbo Luan", "Maosong Sun"], "venue": "In Proceedings of EMNLP,", "citeRegEx": "Luo et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Luo et al\\.", "year": 2015}, {"title": "Recurrent neural network based language model", "author": ["Martin Karafi\u00e1t", "Lukas Burget", "Jan Cernock\u1ef3", "Sanjeev Khudanpur"], "venue": "In INTERSPEECH,", "citeRegEx": "Mikolov et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Mikolov et al\\.", "year": 2010}, {"title": "Efficient estimation of word representations in vector space. arXiv preprint arXiv:1301.3781", "author": ["Kai Chen", "Greg Corrado", "Jeffrey Dean"], "venue": null, "citeRegEx": "Mikolov et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Mikolov et al\\.", "year": 2013}, {"title": "Learning effective and interpretable semantic models using non-negative sparse embedding", "author": ["Murphy et al.2012] Brian Murphy", "Partha Pratim Talukdar", "Tom M Mitchell"], "venue": "In COLING,", "citeRegEx": "Murphy et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Murphy et al\\.", "year": 2012}, {"title": "Glove: Global vectors for word representation", "author": ["Richard Socher", "Christopher D Manning"], "venue": "In EMNLP,", "citeRegEx": "Pennington et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Pennington et al\\.", "year": 2014}, {"title": "Recursive deep models for semantic compositionality over a sentiment treebank", "author": ["Alex Perelygin", "Jean Y Wu", "Jason Chuang", "Christopher D Manning", "Andrew Y Ng", "Christopher Potts"], "venue": "In Proceedings of the conference on empirical methods in natural language processing (EMNLP),", "citeRegEx": "Socher et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Socher et al\\.", "year": 2013}, {"title": "Evaluation of word vector representations by subspace alignment", "author": ["Manaal Faruqui", "Wang Ling", "Guillaume Lample", "Chris Dyer"], "venue": null, "citeRegEx": "Tsvetkov et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Tsvetkov et al\\.", "year": 2015}, {"title": "Bilingual word embeddings for phrase-based machine translation", "author": ["Zou et al.2013] Will Y Zou", "Richard Socher", "Daniel M Cer", "Christopher D Manning"], "venue": "In EMNLP,", "citeRegEx": "Zou et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Zou et al\\.", "year": 2013}], "referenceMentions": [{"referenceID": 17, "context": "For example, bilingual word embedding for phrase-based machine translation (Zou et al., 2013), enhancing the coverage of POS tagging (Huang et al.", "startOffset": 75, "endOffset": 93}, {"referenceID": 7, "context": ", 2013), enhancing the coverage of POS tagging (Huang et al., 2014).", "startOffset": 47, "endOffset": 67}, {"referenceID": 12, "context": "Existing works such as CBOW and skip-gram in the toolbox word2vec (Mikolov et al., 2013) using large unlabeled corpora to train the model.", "startOffset": 66, "endOffset": 88}, {"referenceID": 16, "context": "There are some notable interpretation embedding model includes Subspace Alignment (Tsvetkov et al., 2015), Non-distributional Word Vector Representations (Faruqui and Dyer, 2015).", "startOffset": 82, "endOffset": 105}, {"referenceID": 13, "context": "Non-Negative Sparse Embedding (Murphy et al., 2012), Online Learning of Interpretable Word Embedding (Luo et al.", "startOffset": 30, "endOffset": 51}, {"referenceID": 10, "context": ", 2012), Online Learning of Interpretable Word Embedding (Luo et al., 2015) and so on.", "startOffset": 57, "endOffset": 75}, {"referenceID": 16, "context": "First, In (Tsvetkov et al., 2015) (Faruqui and Dyer, 2015), word level information from linguistic resources such as WordNet is extracted to construct word vectors.", "startOffset": 10, "endOffset": 33}, {"referenceID": 13, "context": "Second, In(Murphy et al., 2012), distributional models which apply matrix factorization are introduced.", "startOffset": 10, "endOffset": 31}, {"referenceID": 4, "context": "Such as citeluoonline(Fyshe et al., 2014)(Lin, 2007).", "startOffset": 21, "endOffset": 41}, {"referenceID": 9, "context": ", 2014)(Lin, 2007).", "startOffset": 7, "endOffset": 18}, {"referenceID": 12, "context": "Word2Vec tool (Mikolov et al., 2013) (Mikolov et al.", "startOffset": 14, "endOffset": 36}, {"referenceID": 11, "context": ", 2013) (Mikolov et al., 2010) (Le and Mikolov, 2014) is very popular in nature language processing and effectively.", "startOffset": 8, "endOffset": 30}, {"referenceID": 14, "context": "Training is performed on aggregated global word-word co-occurrence statistics from a corpus, and the resulting representations showcase interesting linear substructures of the word vector space(Pennington et al., 2014).", "startOffset": 193, "endOffset": 218}, {"referenceID": 5, "context": "We used the WordNet(WN), paraphrase database(PPDB) (Ganitkevitch et al., 2013) and (Faruqui et al.", "startOffset": 51, "endOffset": 78}, {"referenceID": 2, "context": ", 2013) and (Faruqui et al., 2014) to enrich the semantic information of the word vector.", "startOffset": 12, "endOffset": 34}, {"referenceID": 3, "context": "The collection can be utilized to train and/or test computer algorithms implementing semantic similarity measures(Finkelstein et al., 2001).", "startOffset": 113, "endOffset": 139}, {"referenceID": 0, "context": "The collection can be used to train and/or test computer algorithms implementing semantic similarity and relatedness measures(Bruni et al., 2014).", "startOffset": 125, "endOffset": 145}, {"referenceID": 6, "context": "SimLex-999 is a gold standard resource for the evaluation of models that learn the meaning of words and concepts(Hill et al., 2015).", "startOffset": 112, "endOffset": 131}, {"referenceID": 16, "context": "It can overcome the shortcomings of WS353 and contains 999 pairs of adjectives, nouns and verbs(Tsvetkov et al., 2015).", "startOffset": 95, "endOffset": 118}, {"referenceID": 15, "context": "Senti(Socher et al., 2013) is a classification task which between positive and negative movie reviews.", "startOffset": 5, "endOffset": 26}, {"referenceID": 16, "context": "Later, according to (Tsvetkov et al., 2015), we compute the Pearons correlation coefficient r between RAAMXs scores and other word vector models.", "startOffset": 20, "endOffset": 43}], "year": 2016, "abstractText": "Word embedding has been shown to be remarkably effective in a lot of Natural Language Processing tasks. However, existing models still have a couple of limitations in interpreting the dimensions of word vector. In this paper, we provide a new approach\u2014roots and affixes model(RAAM)\u2014to interpret it from the intrinsic structures of natural language. Also it can be used as an evaluation measure of the quality of word embedding. We introduce the information entropy into our model and divide the dimensions into two categories, just like roots and affixes in lexical semantics. Then considering each category as a whole rather than individually. We experimented with English Wikipedia corpus. Our result show that there is a negative linear relation between the two attributes and a high positive correlation between our model and downstream semantic evaluation tasks.", "creator": "LaTeX with hyperref package"}}}