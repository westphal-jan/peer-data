{"id": "1606.07659", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "24-Jun-2016", "title": "Hybrid Recommender System based on Autoencoders", "abstract": "A standard model for Recommender Systems is the Matrix Completion setting: given partially known matrix of ratings given by users (rows) to items (columns), infer the unknown ratings. In the last decades, few attempts where done to handle that objective with Neural Networks, but recently an architecture based on Autoencoders proved to be a promising approach. In current paper, we enhanced that architecture (i) by using a loss function adapted to input data with missing values, and (ii) by incorporating side information. The experiments demonstrate that while side information only slightly improve the test error averaged on all users/items, it has more impact on cold users/items.\n\n\n\n\n\nThe above code is based on some assumptions by the authors. In addition to the original, they did not change the results when we adjusted the models. They chose to ignore the fact that these results were used by the authors to adjust the model, instead providing an alternative to all the assumptions made by the authors. The paper explains these assumptions by pointing out the absence of side information at the start of the experiments.\nFor our model, a linear model, for example, would look like an image of a graph showing the state of the two points on an image with the given value. The point in the graph is a graph showing the state of the two points on an image with the given value. The point in the graph is a graph showing the state of the two points on an image with the given value.\nFor our model, a linear model, for example, would look like an image of a graph showing the state of the two points on an image with the given value. The point in the graph is a graph showing the state of the two points on an image with the given value. The point in the graph is a graph showing the state of the two points on an image with the given value.\nFor our model, a linear model, for example, would look like an image of a graph showing the state of the two points on an image with the given value. The point in the graph is a graph showing the state of the two points on an image with the given value. The point in the graph is a graph showing the state of the two points on an image with the given value.\nFor our model, a linear model, for example, would look like an image of a graph showing the state of the two points on an image with the given value. The point in the graph is a graph showing the state of", "histories": [["v1", "Fri, 24 Jun 2016 12:37:04 GMT  (39kb)", "http://arxiv.org/abs/1606.07659v1", "arXiv admin note: substantial text overlap witharXiv:1603.00806"], ["v2", "Fri, 2 Dec 2016 15:41:21 GMT  (36kb)", "http://arxiv.org/abs/1606.07659v2", "arXiv admin note: substantial text overlap witharXiv:1603.00806"]], "COMMENTS": "arXiv admin note: substantial text overlap witharXiv:1603.00806", "reviews": [], "SUBJECTS": "cs.LG cs.IR", "authors": ["florian strub", "romaric gaudel", "j\\'er\\'emie mary"], "accepted": false, "id": "1606.07659"}, "pdf": {"name": "1606.07659.pdf", "metadata": {"source": "CRF", "title": "Hybrid Recommender System based on Autoencoders", "authors": ["Florian Strub", "Romaric Gaudel"], "emails": ["florian.strub@inria.fr", "jeremie.mary@inria.fr", "romaric.gaudel@inria.fr"], "sections": [{"heading": null, "text": "ar X\niv :1\n60 6.\n07 65\n9v 1\n[ cs\n.L G\n] 2\n4 Ju\nn 20\n16"}, {"heading": "1. INTRODUCTION", "text": "Recommendation systems advise users on which items (movies, musics, books etc.) they are more likely to be interested in. A good recommendation system may dramatically increase the amount of sales of a firm or retain customers. For instance, 80% of movies watched on Netflix come from the recommender system of the company [8]. One efficient way to design such algorithm is to predict how a user would rate a given item. Two key methods co-exist to tackle this issue: Content-Based Filtering (CBF) and Collaborative Filtering (CF).\nCBF uses the user/item knowledge to estimate a new rating. For instance, user information can be the age, gender, or graph of friends etc. Item information can be the movie genre, a short description, or the tags. On the other side, CF uses the ratings history of users and items. The feedback of one user on some items is combined with the feedback of all other users on all items to predict a new rating. For instance, if someone rated a few books, Collaborative Filtering aims at estimating the ratings he would have given to thousands of other books by using the ratings of all the other readers. CF is often preferred to CBF because it wins the agnostic vs. studied contest: CF only relies on the ratings of the users while CBF requires advanced engineering\nPermission to make digital or hard copies of part or all of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for third-party components of this work must be honored. For all other uses, contact the owner/author(s).\nc\u00a9 2016 Copyright held by the owner/author(s).\nACM ISBN .\nDOI:\non items to well perform [20]. The most successful approach in CF is to retrieve potential latent factors from the sparse matrix of ratings. Book latent factors are likely to encapsulate the book genre (spy novel, fantasy, etc.) or some writing styles. Common latent factor techniques compute a low-rank rating matrix by applying Singular Value Decomposition through gradient descent [10] or Regularized Alternating Least Square algorithm [39]. However, these methods are linear and cannot catch subtle factors. Newer algorithms were explored to face those constraints such as Factorization Machines [25]. More recent works combine several low-rank matrices such as Local Low Rank Matrix Approximation [16] or WEMAREC [3] to enhance the recommendation.\nAnother limitation of CF is known as the cold start problem: how to recommend an item to a user when no rating exists for either the user or the item? To overcome this issue, one idea is to build a hybrid model mixing CF and CBF where side information is integrated into the training process. The goal is to supplant the lack of ratings through side information. A successful approach [1, 24] extends the Bayesian Probabilistic Matrix Factorization Framework [26] to integrate side information. However, recent algorithms outperform them in the general case [17].\nIn this paper we introduce a CF approach based on Stacked Denoising Autoencoders [35, 40] which tackles both challenges: learning a non-linear representation of users and items, and alleviating the cold start problem by integrating side information. Compared to previous attempts in that direction [27, 28, 30, 5, 38], our framework integrates the sparse matrix of ratings and side information in a unique Network. This joint model leads to a scalable and robust approach which beats state-of-the-art results in CF. Reusable source code is provided in Lua/Torch to reproduce the results. Last but not least, we show that CF approaches based on Matrix Factorization have a strong link with our approach.\nThe paper is organized as follows. First, Sec. 2 fixes the setting and gives state of the art on related approaches. Then, our model is described in Sec. 3. Finally, experimental results are given and discussed in Sec. 4 and Sec. 5 discusses algorithmic aspects."}, {"heading": "2. PRELIMINARIES", "text": ""}, {"heading": "2.1 Matrix Completion", "text": "A standard setting for CF is Matrix Completion [10]. Given N users andM items, the rating rij is the rating given\nby the ith user for the jth item. It entails a matrix of ratings R \u2208 RN\u00d7M , for which only a few entries are known. The goal of Matrix Completion is to infer the unknown value. Namely, the algorithm returns a matrix R\u0302 \u2208 RN\u00d7M which hopefully minimizes the reconstruction error\nL(R, R\u0302) = \u2211\n(i,j)/\u2208K(R) (rij \u2212 r\u0302ij)\n2 ,\nwhere K(R) is the set of indices of known ratings of R."}, {"heading": "2.2 Denoising Autoencoders", "text": "The proposed approach builds upon Autoencoders which are feed-forward Neural Networks popularized by Kramer [11]. They are unsupervised Networks where the output of the Network aims at reconstructing the initial input. The Network is trained by back-propagating the squared error loss on the reconstruction. When the network limits itself to one hidden layer, its output is given by\nnn(x) def = \u03c3(W2\u03c3(W1x+ b1) + b2),\nwith x \u2208 RN the input, W1 \u2208 R k\u00d7N and W2 \u2208 R N\u00d7k the weight matrices, b1 \u2208 R k and b2 \u2208 R N the bias vectors, and \u03c3(.) a non-linear transfer function. The size k \u226a N of the hidden layer is also known as the Autoencoder\u2019s bottleneck.\nRecent work in Deep Learning advocates to stack pretrained encoders to initialize Deep Neural Networks [6]. This process enables the lowest layers of the Network to find lowdimensional representations. It experimentally increases the quality of the whole Network. Yet, classic Autoencoders may degenerate into identity Networks and they fail to learn the latent relationship between data. [35] tackle this issue by corrupting inputs, pushing the Network to denoise the final outputs. One method is to add Gaussian noise on a random fraction of the input. Another method is to mask a random fraction of the input by replacing them with zero. In this case, the Denoising AutoEncoder (DAE) loss function is modified to emphasize the denoising aspect of the Network. The loss is based on two main hyperparameters \u03b1, \u03b2. They balance whether the Network would focus on denoising the input (\u03b1) or reconstructing the input (\u03b2):\nL\u03b1,\u03b2(x, x\u0303) = \u03b1\n  \u2211\nj\u2208C(x\u0303) [nn(x\u0303)j \u2212 xj ]\n2\n\n+\n\u03b2\n  \u2211\nj 6\u2208C(x\u0303) [nn(x\u0303)j \u2212 xj ]\n2\n\n ,\nwhere x\u0303 \u2208 RN is a corrupted version of the input x, C is the set of corrupted elements in x\u0303, and nn(x)j is the j\nth output of the Network while fed with x."}, {"heading": "2.3 Related Work", "text": "Neural Networks have attracted little attention in the CF community. In a preliminary work, [27] tackled the Netflix challenge using Restricted Boltzmann Machines but little published work had follow [33]. While Deep Learning has tremendous success in image and speech recognition [13], sparse data has received less attention and remains a challenging problem for Neural Networks.\nNevertheless, Neural Networks are able to discover nonlinear latent variables with heterogeneous data [13] which\nmakes them a promising tool for CF. [28, 30, 5] directly train Autoencoders to provide the best predicted ratings. Those methods report excellent results in the general case. However, the cold start initialization problem is ignored. For instance, AutoRec [28] replaces unpredictable ratings by an arbitrary selected score. In our case, we apply a training loss designed for sparse rating inputs and we integrate side information to lessen the cold start effect.\nOther contributions deal with this cold start problem by using Neural Networks properties for CBF: Neural Networks are first trained to learn a feature representation from the item which is then processed by a CF approach such as Probabilistic Matrix Factorization [23] to provide the final rating. For instance, [7, 36] respectively auto-encode bag-of-words from restaurant reviews and movie plots, [19] auto-encode heterogeneous side information from users and items. Finally, [34, 37] use Convolutional Networks on music samples. In our case, side information and ratings are used together without any unsupervised pretreatment."}, {"heading": "2.4 Notation", "text": "In the rest of the paper, we use the following notations:\n\u2022 ui, vj are the partially known rows/columns of R;\n\u2022 u\u0303i, v\u0303j are corrupted versions of ui, vj;\n\u2022 u\u0302i, v\u0302j are rows/columns of R\u0302, which is an estimate any entry of R."}, {"heading": "3. AUTOENCODERS AND COLLABORATIVE FILTERING", "text": "We propose to turn the sparse vectors ui/vj , into dense vectors u\u0302i/v\u0302j with Autoencoders. To do so, we need to define two types of Autoencoders:\n\u2022 U-CFN is defined as u\u0302i = nn(ui),\n\u2022 V-CFN is defined as v\u0302j = nn(vj).\nNote that CF is part of the few applications requiring to infer missing values, and not only to compress the available information."}, {"heading": "3.1 Sparse Inputs", "text": "There is no standard approach for using sparse vectors as inputs of Neural Networks. Most of the papers dealing with sparse inputs get around by pre-computing an estimate of the missing values [32, 2]. In our case, we want the Autoencoder to handle this prediction issue by itself. Such problems have already been studied in industry [22] where 5% of the values are missing. However in Collaborative Filtering we often face datasets with more than 95% missing values. Furthermore, missing values are not known during training in Collaborative Filtering which makes the task even more difficult.\nOur approach includes three ingredients to handle the training of sparse Autoencoders:\n\u2022 inhibit the edges of the input layers by zeroing out values in the input,\n\u2022 inhibit the edges of the output layers by zeroing out back-propagated values,\n\u2022 use a denoising loss to emphasize rating prediction over rating reconstruction.\nOne way to inhibit the input edges is to turn missing values to zero. To keep the Autoencoder from always returning zero, we also use an empirical loss that disregards the loss of unknown values. No error is back-propagated for missing values. Therefore, the error is back-propagated for actual zero values while it is discarded for missing values. In other words, missing values do not bring information to the Network. This operation is equivalent to removing the neurons with missing values described in [27, 28]. However, Our method has important computational advantages because only one Neural Networks is trained whereas other techniques has to share the weights among thousands of Networks.\nFinally, we take advantage of the masking noise from the Denoising AutoEncoders (DAE) empirical loss. By simulating missing values in the training process, Autoencoders are trained to predict them. In Collaborative Filtering, this prediction aspect is actually the final target. Thus, emphasizing the prediction criterion turns the classic unsupervised training of Autoencoders into a simulated supervised learning. By mixing both the reconstruction and prediction criteria, the training can be thought as a pseudo-semi-supervised learning. This makes the DAE loss a promising objective function. After regularization, the final training loss is:\nL\u03b1,\u03b2(x, x\u0303) = \u03b1\n  \u2211\nj\u2208K(x)\u2229C(x\u0303) [nn(x\u0303)j \u2212 xj ]\n2\n\n+\n\u03b2\n  \u2211\nj\u2208K(x)\\C(x\u0303) [nn(x\u0303)j \u2212 xj ]\n2\n\n+ \u03bb\u2016W\u20162F ,\nwhere K(x) are the indices of known values of x, W is the flatten vector of weights of the Network and \u03bb is the regularization hyperparameter. This loss is optimized thanks to standard forward/backward process on mini-batches. Importantly, Autoencoders with sparse inputs differs from sparseAutoencoders [15] or Dropout regularization [29] in the sense that Sparse Autoencoders and Droupout inhibit the hidden neurons for regularization purpose. Every inputs/outputs are also known."}, {"heading": "3.2 Integrating Side Information", "text": "From the time being U/V-CFN only relies on the feedback of users regarding a set of items. Lets now incorporate additional information about the users and the items. We will show that these information help in several ways: increase the prediction accuracy, speed up the training, increase the robustness of the model, etc. Last but not least, incorporating side information is a well-known approach to tackle the cold start problem: when very little information is available on a user/item, Collaborative Filtering will have difficulties to infer its ratings.\nInstead of only adding the side information to the first layer of the Autoencoder, we propose to inject that information to every layer inputs of the Network. As an example, the model of U-CFN becomes\nnn({ui, zi}) = \u03c3(W \u2032 2{\u03c3(W \u2032 1{ui, zi}+ b1), zi}+ b2),\nwhere zi \u2208 R P is the vector of side informations, W\u20321 \u2208 R k\u00d7(N+P ) and W\u20322 \u2208 R N\u00d7(k+P ) are the weight matrices,\n{x, zi} represents the concatenation of both vectors x and zi, and biases b1 and b2 respectively belong to R\nN+P and R\nk+P . By injecting the side information in every layer, the dynamic Autoencoders representation is forced to integrate this new data. However, to avoid side information to overstep the dense rating representation, we enforce the following constraints. The dimension of the sparse input must be greater than the dimension of the Autoencoder bottleneck which must be greater than the dimension of the side information1. Therefore, we get:"}, {"heading": "P \u226a k \u226a N and Q \u226a k \u226a M.", "text": ""}, {"heading": "4. EXPERIMENTS", "text": ""}, {"heading": "4.1 Benchmark Models", "text": "We benchmark CFN with two Matrix Factorization techniques that are broadly used in the industry. Alternating Least Squares with Weighted-\u03bb-Regularization (ALS-WR) [39] solves a low-rank matrix factorization problem by alternatively fixing U and V and solving the resulting linear problem. Experiments are run with the Apache Mahout Software2.\nSVDFeature [4] is a Machine Learning Toolkit for featurebased Collaborative Filtering. He won the KDD Cup for two consecutive years. Ratings are given by the following equation:\nr\u0302 =\n  N+P\u2211\np\nxpb (u) p +\nM+Q\u2211\nq\nyqb (v) q +\n\u2016R\u2016\u2211\nr\nzrb (g) r\n\n+\n( N+P\u2211\np\nxpup\n)T (M+Q\u2211\nq\nyqvq\n)\nwhere b(u) \u2208 RN+P , b(i) \u2208 RM+Q, b(g) \u2208 R\u2016R\u2016 are the the side information bias, and U \u2208 RN+P\u00d7K , V \u2208 RM+Q\u00d7K encode the latent factors. The model parameters are computed by gradient descent."}, {"heading": "4.2 Data", "text": "Experiments are conducted on MovieLens and Douban datasets. The MovieLens-1M, MovieLens-10M and MovieLens-20M datasets respectively provide 1/10/20 millions discrete ratings from 6/72/138 thousands users on 4/10/27 thousands movies. Side information for MovieLens-1M is the age, sex and gender of the user and the movie category (action, thriller etc.). Side information for MovieLens-10/20M is a matrix of tags T where Tij is the occurrence of the j\nth tag for the ith movie and the movie category. No side information is provided for users.\nThe Douban dataset [21] provides 17 million discrete ratings from 129 thousands users on 58 thousands movies. Side information is the bi-directional user/friend relations for the user. The user/friend relation are treated like the matrix of tags from MovieLens. No side information is provided for items.\n1When side information is sparse, the dimension of the side information can be assimilated to the number of non-zero parameters 2http://mahout.apache.org/\nPreprocessing. For each dataset, the full dataset is considered and the ratings are normalized from -1 to 1. We split the dataset into random 90%-10% train-test datasets and inputs are unbiased before the training process: denoting \u00b5 the mean over the training set, bi the mean of the i\nth user and bj the mean of the jth item, U-CFN and V-CFN respectively learn from runbiasedij = rij \u2212 bi and r unbiased ij = rij \u2212 bj . The bias computed on the training set is added back while evaluating the learned matrix.\nSide Information. In order to enforce the side information constraint, Q \u226a Kv \u226a M , Principal Component Analysis is performed on the matrix of tags. We keep the 50 greatest eigenvectors3 and normalize them by the square root of their respective eigenvalue: given T = PDQT with D the diagonal matrix of eigenvalues sorted in descending order, the movie tags are represented by Y = PJ\u00d7K\u2032D 0.5 K\u2032\u00d7K\u2032 with K\n\u2032 the number of kept eigenvectors. Binary representation such as the movie category is then concatenated to Y."}, {"heading": "4.3 Error Function", "text": "The algorithms are compared based on their respective Root Mean Square Error (RMSE) on test data. Denoting\nRtest the matrix of test ratings and R\u0302 the full matrix returned by the learning algorithm, the RMSE is:\nRMSE(R\u0302,Rtest) = \u221a\n1\n|K(Rtest)|\n\u2211\n(i,j)\u2208K(Rtest) (rtest,ij \u2212 r\u0302ij)2,\nwhere |K(Rtest)| is the number of ratings in the testing dataset. Note that for the sake of fair comparison, in the\ncase of Autoencoders R\u0302 is computed by feeding the network with training data. As such, r\u0302ij stands for nn(utrain,i)j for U-CFN, and nn(vtrain,j)i for V-CFN."}, {"heading": "4.4 Training Settings", "text": "We train 2-layers Autoencoders for MovieLens-1/10/20M and the Douban datasets. The layers have from 500 to 700 hidden neurons. Weights are initialized using the fan-in rule [14]: Wij \u223c U [ \u2212 1\u221a\nn , 1\u221a n\n] . Transfer functions are hy-\nperbolic tangents. The Neural Network is optimized with stochastic backpropagation with minibatch of size 30 and a\n3The number of eigenvalues is arbitrary selected. We do not focus on optimizing the quality of this representation.\nweight decay is added for regularization. Hyperparameters4 are tuned by a genetic algorithm already used by [31] in a different context."}, {"heading": "4.5 General Results", "text": "Table 1 summarizes the RMSE on MovieLens and Douban datasets. Confidence intervals correspond to a 95% range. V-CFNs have excellent performance in our experiments for every dataset we run. It is competitive compared to the state-of-the-art Collaborative Filtering algorithms and clearly outperforms them for MovieLens-10M. To the best of our knowledge, the best result published regarding MovieLens10M (without side information) are reported by [18] and [3] with a final RMSE of respectively 0.7682 and 0.7769. However, those two methods require to recompute the full matrix for every new ratings. CFN has the key advantage to provide similar performance while being able to refine its prediction on the fly for new ratings. More generally, we are not aware of recent works that both manage to reach state of the art results while successfully integrating side information. For instance, [9, 12] reports a global RMSE above 0.8 on MovieLens-10M.\nIn the experiments, V-CFN outperforms U-CFN. It suggests that the structure on the items is stronger than the one on users i.e. it is easier to guess tastes based on movies you liked than to find some users similar to you. Of course, the behavior could be different on some other data ."}, {"heading": "4.6 Impact of Side Information", "text": "At first sight, the use of side information has a limited impact on the RMSE. This statement has to be mitigated: as the repartition of known entries in the dataset is not uniform, the estimates are biased towards users and items with a lot of ratings. For theses users and movies, the dataset already contains a lot of information, thus having some extra information will have a marginal effect. Users and items with few ratings should benefit more from some side information but the estimation biais hides them.\nIn order to exhibit the utility of side information, we report in Table 2 the RMSE conditionally to the number of missing values for items. As expected, the fewer number of ratings for an item, the more important the side information. A more careful analysis of the RMSE improvement in this setting shows that the improvement is uniformly distributed over the users whatever their number of ratings. This corresponds to the fact that the available side information is only about items. This is very desirable for a real system: the\n4Hyperparameters used for the experiments are provided with the source code.\neffective use of side information to the new items is crucial to deal with the flow of new products.\nIn the end, we trained V-CFN on MovieLens-10M with either the movie genre or the matrix of tags. Both side information increase the global RMSE by 0.13% and concatenating them increase the final score by a small margin of 0.20%. Therefore, V-CFN could handle the heterogeneity of side information. However, the U-CFN failed to use the friendship relationship to increase the RMSE."}, {"heading": "5. REMARKS", "text": ""}, {"heading": "5.1 Source code", "text": "Torch is a powerful framework written in Lua to quickly prototype Neural Networks. It is a widely used (Facebook, Deep Mind) industry standard. However, Torch lacks some important basic tools to deal with sparse inputs. Thus, we develop several new modules to deal with DAE loss, sparse DAE loss and sparse inputs on both CPU and GPU. They can easily be plugged into existing code. An out-of-the-box tutorial is available to directly run the experiments. The code is freely available on Github and Luarocks5."}, {"heading": "5.2 Scalability", "text": "One major problem that most Collaborative Filtering have to resolve is scalability since dataset often have hundred of thousands users and items. An efficient algorithm must be trained in a reasonable amount of time and provide quick feedback during evaluation time.\nRecent advances in GPU computation managed to reduce the training time of Neural Networks by several orders of magnitude. However, Collaborative Filtering deals with sparse data and GPUs are designed to perform well on dense data. [27, 28] face this sparsity constraint by building small dense Networks with shared weights. Yet, this approach may lead to important synchronisation latencies. In our case, we tackle the issue by selectively densifying the inputs just before sending them to the GPUs cores without modification of the result of the computation. It introduces an overhead on the computational complexity but this implementation allows the GPUs to work at their full strength. In practice, vectorial operations overtake the extra cost. Such approach is an efficient strategy to handle sparse data which achieves a balance between memory footprint and computational time.\n5https://github.com/fstrub95/Autoencoders cf\nWe are able to train Large Neural Networks within a few minutes as shown in Table 3. At the time of writing, alternative strategies to train networks with sparse inputs on GPUs are under development."}, {"heading": "6. CONCLUSION", "text": "In this paper, we have introduced a Neural Network architecture, aka CFN, to perform Collaborative Filtering with side information. Contrary to other attempts with Neural Networks, this joint Network integrate side information and learn a non-linear representation of users or items into a unique Neural Network. This approach manages to both beats state of the art results in CF and ease the cold start problem on the MovieLens and Douban datasets. CFN is also scalable and robust to deal with large size dataset. We made several claims that Autoencoders are closely linked to low-rank Matrix Factorization in Collaborative Filtering. Finally, a reusable source code is provided in Torch and hyperparameters are provided to reproduce the results."}, {"heading": "7. REFERENCES", "text": "[1] R. P. Adams and G. E. D. I. Murray. Incorporating side information in probabilistic matrix factorization with gaussian processes. arXiv preprint arXiv:1003.4944, 2010.\n[2] C. M. Bishop. Neural networks for pattern recognition. Oxford univ. press, 1995.\n[3] C. Chen, D. Li, Y. Zhao, Q. Lv, and L. Shang. Wemarec: Accurate and scalable recommendation through weighted and ensemble matrix approximation. In Proc. of the International ACM SIGIR Conference on Research and Development in Information\nRetrieval, pages 303\u2013312. ACM, 2015.\n[4] T. Chen, W. Zhang, Q. Lu, K. Chen, Z. Zheng, and Y. Yong. Svdfeature: a toolkit for feature-based collaborative filtering. JMLR, 13(1):3619\u20133622, 2012.\n[5] G. Dziugaite and D. Roy. Neural network matrix factorization. arXiv preprint arXiv:1511.06443, 2015.\n[6] X. Glorot and Y. Bengio. Understanding the difficulty of training deep feedforward neural networks. In Proc. of AISTATS\u201910, pages 249\u2013256, 2010.\n[7] X. Glorot, A. Bordes, and Y. Bengio. Deep sparse rectifier neural networks. In Proc. of AISTATS\u201911, pages 315\u2013323, 2011.\n[8] C. Gomez-Uribe and N. Hunt. The netflix recommender system: Algorithms, business value, and innovation. ACM Trans. Manage. Inf. Syst., 6(4):13:1\u201313:19, 2015.\n[9] Y.-D. Kim and S. Choi. Scalable variational bayesian matrix factorization with side information. In Proc. of AISTATS\u201914, Reykjavik, Iceland, 2014.\n[10] Y. Koren, R. Bell, and C. Volinsky. Matrix factorization techniques for recommender systems. Computer, (8):30\u201337, 2009.\n[11] M. A. Kramer. Nonlinear principal component analysis using autoassociative neural networks. AIChE journal, 37(2):233\u2013243, 1991.\n[12] R. Kumar, B. K. Verma, and S. S. Rastogi. Social popularity based svd++ recommender system. International Journal of Computer Applications, 87(14), 2014.\n[13] Y. LeCun, Y. Bengio, and G. Hinton. Deep learning. Nature, 521(7553):436\u2013444, 2015.\n[14] Y. LeCun, L. Bottou, G. Orr, and K. Muller. Efficient backprop. In Neural networks: Tricks of the trade, pages 9\u201348. Springer, 1998.\n[15] H. Lee, A. Battle, R. Raina, and A. Ng. Efficient sparse coding algorithms. In Advances in neural information processing systems, pages 801\u2013808, 2006.\n[16] J. Lee, S. Kim, G. Lebanon, and Y. Singerm. Local low-rank matrix approximation. In Proc. of ICML\u201913, pages 82\u201390, 2013.\n[17] J. Lee, M. Sun, and G. Lebanon. A comparative study of collaborative filtering algorithms. arXiv preprint arXiv:1205.3193, 2012.\n[18] D. Li, C. Chen, Q. Lv, J. Yan, L. Shang, and S. Chu. Low-rank matrix approximation with stability. In Proc. of ICML\u201916, 2016.\n[19] S. Li, J. Kawale, and Y. Fu. Deep collaborative filtering via marginalized denoising auto-encoder. In Proc. of CIKM\u201915, pages 811\u2013820. ACM, 2015.\n[20] P. Lops, M. D. Gemmis, and G. Semeraro. Content-based recommender systems: State of the art and trends. In Recommender systems handbook, pages 73\u2013105. Springer, 2011.\n[21] H. Ma, D. Zhou, C. Liu, M. R. Lyu, and I. King. Recommender systems with social regularization. In Proceedings of the fourth ACM international\nconference on Web search and data mining, WSDM \u201911, pages 287\u2013296, Hong Kong, China, 2011.\n[22] V. Miranda, J. Krstulovic, H. Keko, C. Moreira, and J. Pereira. Reconstructing Missing Data in State Estimation With Autoencoders. IEEE Transactions on Power Systems, 27(2):604\u2013611, 2012.\n[23] A. Mnih and R. Salakhutdinov. Probabilistic matrix factorization. In Advances in neural information processing systems, pages 1257\u20131264, 2007.\n[24] I. Porteous and M. W. A. U. Asuncion. Bayesian matrix factorization with side information and\ndirichlet process mixtures. In Proc. of AAAI\u201910, 2010.\n[25] S. Rendle. Factorization machines. In Proc. of ICDM\u201910, pages 995\u20131000, 2010.\n[26] R. Salakhutdinov and A. Mnih. Bayesian probabilistic matrix factorization using markov chain monte carlo. In Proc. of ICML\u201908, pages 880\u2013887. ACM, 2008.\n[27] R. Salakhutdinov, A. Mnih, and G. Hinton. Restricted boltzmann machines for collaborative filtering. In Proc. of ICML\u201907, pages 791\u2013798. ACM, 2007.\n[28] S. Sedhain, A. K. Menon, S. Sanner, and L. Xie. Autorec: Autoencoders meet collaborative filtering. In Proc. of Int. Conf. on World Wide Web Companion, pages 111\u2013112, 2015.\n[29] N. Srivastava, G. Hinton, A. Krizhevsk, I. Sutskever, and R. Salakhutdinov. Dropout: A simple way to prevent neural networks from overfitting. The Journal of Machine Learning Research, 15(1):1929\u20131958, 2014.\n[30] F. Strub and J. Mary. Collaborative Filtering with Stacked Denoising AutoEncoders and Sparse Inputs. In NIPS Workshop on Machine Learning for eCommerce, Montreal, Canada, 2015.\n[31] O. Teytaud, S. Gelly, and J. Mary. Active learning in regression, with application to stochastic dynamic programming. In A. International Conference On Informatics in Control and Robotics, editors, ICINCO and CAP, pages 373\u2013386, 2007.\n[32] V. Tresp, S. Ahmad, and R. Neuneier. Training Neural Networks with Deficient Data. Advances in Neural Information Processing Systems 6, pages 128\u2013135, 1994.\n[33] T. T. Truyen, D. Phung, and S. Venkatesh. Ordinal boltzmann machines for collaborative filtering. In Proc. of UAI\u201909, pages 548\u2013556. AUAI Press, 2009.\n[34] A. Van den Oord, S. Dieleman, and B. Schrauwen. Deep content-based music recommendation. In Proc. of NIPS\u201913, pages 2643\u20132651, 2013.\n[35] P. Vincent, H. Larochelle, I. Lajoie, Y. Bengio, and P. Manzagol. Stacked Denoising Autoencoders: Learning Useful Representations in a Deep Network with a Local Denoising Criterion. Jour. of Mach. Learn. Res., 11(3):3371\u20133408, 2010.\n[36] H. Wang, N. Wang, and D. Y. Yeung. Collaborative deep learning for recommender systems. arXiv preprint arXiv:1409.2944, 2014.\n[37] H. Wang, N. Wang, and D. Y. Yeung. Improving content-based and hybrid music recommendation using deep learning. In Proceedings of the ACM International Conference on Multimedia, pages 627\u2013636. ACM, 2014.\n[38] Y. Wu, C. DuBois, A. Zheng, and M. Ester. Collaborative denoising auto-encoders for top-n recommender systems. In Proc. of WSDM\u201916, pages 153\u2013162. ACM, 2016.\n[39] Y. Zhou, D. Wilkinson, R. Schreiber, and R. Pan. Large-scale parallel collaborative filtering for the netflix prize. In Algorithmic Aspects in Information and Management, pages 337\u2013348. Springer, 2008.\n[40] F. Zhuang, D. Luo, X. Jin, H. Xiong, P. Luo, and Q. He. Representation learning via semi-supervised autoencoder for multi-task learning. In Proc. of ICDM\u201915, 2015."}], "references": [{"title": "Incorporating side information in probabilistic matrix factorization with gaussian processes", "author": ["R.P. Adams", "G.E.D.I. Murray"], "venue": "arXiv preprint arXiv:1003.4944", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2010}, {"title": "Neural networks for pattern recognition", "author": ["C.M. Bishop"], "venue": "Oxford univ. press", "citeRegEx": "2", "shortCiteRegEx": null, "year": 1995}, {"title": "Wemarec: Accurate and scalable recommendation through weighted and ensemble matrix approximation", "author": ["C. Chen", "D. Li", "Y. Zhao", "Q. Lv", "L. Shang"], "venue": "Proc. of the International ACM SIGIR Conference on Research and Development in Information Retrieval, pages 303\u2013312. ACM", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2015}, {"title": "Svdfeature: a toolkit for feature-based collaborative filtering", "author": ["T. Chen", "W. Zhang", "Q. Lu", "K. Chen", "Z. Zheng", "Y. Yong"], "venue": "JMLR, 13(1):3619\u20133622", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2012}, {"title": "Neural network matrix factorization", "author": ["G. Dziugaite", "D. Roy"], "venue": "arXiv preprint arXiv:1511.06443", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2015}, {"title": "Understanding the difficulty of training deep feedforward neural networks", "author": ["X. Glorot", "Y. Bengio"], "venue": "Proc. of AISTATS\u201910, pages 249\u2013256", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2010}, {"title": "Deep sparse rectifier neural networks", "author": ["X. Glorot", "A. Bordes", "Y. Bengio"], "venue": "Proc. of AISTATS\u201911, pages 315\u2013323", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2011}, {"title": "The netflix recommender system: Algorithms", "author": ["C. Gomez-Uribe", "N. Hunt"], "venue": "business value, and innovation. ACM Trans. Manage. Inf. Syst., 6(4):13:1\u201313:19", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2015}, {"title": "Scalable variational bayesian matrix factorization with side information", "author": ["Y.-D. Kim", "S. Choi"], "venue": "Proc. of AISTATS\u201914, Reykjavik, Iceland", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2014}, {"title": "Matrix factorization techniques for recommender systems", "author": ["Y. Koren", "R. Bell", "C. Volinsky"], "venue": "Computer, (8):30\u201337", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2009}, {"title": "Nonlinear principal component analysis using autoassociative neural networks", "author": ["M.A. Kramer"], "venue": "AIChE journal, 37(2):233\u2013243", "citeRegEx": "11", "shortCiteRegEx": null, "year": 1991}, {"title": "Social popularity based svd++ recommender system", "author": ["R. Kumar", "B.K. Verma", "S.S. Rastogi"], "venue": "International Journal of Computer Applications, 87(14)", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2014}, {"title": "Deep learning", "author": ["Y. LeCun", "Y. Bengio", "G. Hinton"], "venue": "Nature, 521(7553):436\u2013444", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2015}, {"title": "Efficient backprop", "author": ["Y. LeCun", "L. Bottou", "G. Orr", "K. Muller"], "venue": "Neural networks: Tricks of the trade, pages 9\u201348. Springer", "citeRegEx": "14", "shortCiteRegEx": null, "year": 1998}, {"title": "Efficient sparse coding algorithms", "author": ["H. Lee", "A. Battle", "R. Raina", "A. Ng"], "venue": "Advances in neural information processing systems, pages 801\u2013808", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2006}, {"title": "Local low-rank matrix approximation", "author": ["J. Lee", "S. Kim", "G. Lebanon", "Y. Singerm"], "venue": "Proc. of ICML\u201913, pages 82\u201390", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2013}, {"title": "A comparative study of collaborative filtering algorithms", "author": ["J. Lee", "M. Sun", "G. Lebanon"], "venue": "arXiv preprint arXiv:1205.3193", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2012}, {"title": "Low-rank matrix approximation with stability", "author": ["D. Li", "C. Chen", "Q. Lv", "J. Yan", "L. Shang", "S. Chu"], "venue": "Proc. of ICML\u201916", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2016}, {"title": "Deep collaborative filtering via marginalized denoising auto-encoder", "author": ["S. Li", "J. Kawale", "Y. Fu"], "venue": "Proc. of CIKM\u201915, pages 811\u2013820. ACM", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2015}, {"title": "Content-based recommender systems: State of the art and trends", "author": ["P. Lops", "M.D. Gemmis", "G. Semeraro"], "venue": "Recommender systems handbook, pages 73\u2013105. Springer", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2011}, {"title": "Recommender systems with social regularization", "author": ["H. Ma", "D. Zhou", "C. Liu", "M.R. Lyu", "I. King"], "venue": "Proceedings of the fourth ACM international conference on Web search and data mining, WSDM \u201911, pages 287\u2013296, Hong Kong, China", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2011}, {"title": "Reconstructing Missing Data in State Estimation With Autoencoders", "author": ["V. Miranda", "J. Krstulovic", "H. Keko", "C. Moreira", "J. Pereira"], "venue": "IEEE Transactions on Power Systems, 27(2):604\u2013611", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2012}, {"title": "Probabilistic matrix factorization", "author": ["A. Mnih", "R. Salakhutdinov"], "venue": "Advances in neural information processing systems, pages 1257\u20131264", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2007}, {"title": "Bayesian matrix factorization with side information and  dirichlet process mixtures", "author": ["I. Porteous", "M.W.A.U. Asuncion"], "venue": "Proc. of AAAI\u201910", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2010}, {"title": "Factorization machines", "author": ["S. Rendle"], "venue": "Proc. of ICDM\u201910, pages 995\u20131000", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2010}, {"title": "Bayesian probabilistic matrix factorization using markov chain monte carlo", "author": ["R. Salakhutdinov", "A. Mnih"], "venue": "Proc. of ICML\u201908, pages 880\u2013887. ACM", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2008}, {"title": "Restricted boltzmann machines for collaborative filtering", "author": ["R. Salakhutdinov", "A. Mnih", "G. Hinton"], "venue": "Proc. of ICML\u201907, pages 791\u2013798. ACM", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2007}, {"title": "Autorec: Autoencoders meet collaborative filtering", "author": ["S. Sedhain", "A.K. Menon", "S. Sanner", "L. Xie"], "venue": "Proc. of Int. Conf. on World Wide Web Companion, pages 111\u2013112", "citeRegEx": "28", "shortCiteRegEx": null, "year": 2015}, {"title": "Dropout: A simple way to prevent neural networks from overfitting", "author": ["N. Srivastava", "G. Hinton", "A. Krizhevsk", "I. Sutskever", "R. Salakhutdinov"], "venue": "The Journal of Machine Learning Research, 15(1):1929\u20131958", "citeRegEx": "29", "shortCiteRegEx": null, "year": 2014}, {"title": "Collaborative Filtering with Stacked Denoising AutoEncoders and Sparse Inputs", "author": ["F. Strub", "J. Mary"], "venue": "NIPS Workshop on Machine Learning for eCommerce, Montreal, Canada", "citeRegEx": "30", "shortCiteRegEx": null, "year": 2015}, {"title": "Active learning in regression", "author": ["O. Teytaud", "S. Gelly", "J. Mary"], "venue": "with application to stochastic dynamic programming. In A. International Conference On Informatics in Control and Robotics, editors, ICINCO and CAP, pages 373\u2013386", "citeRegEx": "31", "shortCiteRegEx": null, "year": 2007}, {"title": "Training Neural Networks with Deficient Data", "author": ["V. Tresp", "S. Ahmad", "R. Neuneier"], "venue": "Advances in Neural Information Processing Systems 6, pages 128\u2013135", "citeRegEx": "32", "shortCiteRegEx": null, "year": 1994}, {"title": "Ordinal boltzmann machines for collaborative filtering", "author": ["T.T. Truyen", "D. Phung", "S. Venkatesh"], "venue": "Proc. of UAI\u201909, pages 548\u2013556. AUAI Press", "citeRegEx": "33", "shortCiteRegEx": null, "year": 2009}, {"title": "Deep content-based music recommendation", "author": ["A. Van den Oord", "S. Dieleman", "B. Schrauwen"], "venue": "In Proc. of NIPS\u201913,", "citeRegEx": "34", "shortCiteRegEx": "34", "year": 2013}, {"title": "Stacked Denoising Autoencoders: Learning Useful Representations in a Deep Network with a Local Denoising Criterion", "author": ["P. Vincent", "H. Larochelle", "I. Lajoie", "Y. Bengio", "P. Manzagol"], "venue": "Jour. of Mach. Learn. Res., 11(3):3371\u20133408", "citeRegEx": "35", "shortCiteRegEx": null, "year": 2010}, {"title": "Collaborative deep learning for recommender systems", "author": ["H. Wang", "N. Wang", "D.Y. Yeung"], "venue": "arXiv preprint arXiv:1409.2944", "citeRegEx": "36", "shortCiteRegEx": null, "year": 2014}, {"title": "Improving content-based and hybrid music recommendation using deep learning", "author": ["H. Wang", "N. Wang", "D.Y. Yeung"], "venue": "Proceedings of the ACM International Conference on Multimedia, pages 627\u2013636. ACM", "citeRegEx": "37", "shortCiteRegEx": null, "year": 2014}, {"title": "Collaborative denoising auto-encoders for top-n recommender systems", "author": ["Y. Wu", "C. DuBois", "A. Zheng", "M. Ester"], "venue": "Proc. of WSDM\u201916, pages 153\u2013162. ACM", "citeRegEx": "38", "shortCiteRegEx": null, "year": 2016}, {"title": "Large-scale parallel collaborative filtering for the netflix prize", "author": ["Y. Zhou", "D. Wilkinson", "R. Schreiber", "R. Pan"], "venue": "Algorithmic Aspects in Information and Management, pages 337\u2013348. Springer", "citeRegEx": "39", "shortCiteRegEx": null, "year": 2008}, {"title": "Representation learning via semi-supervised autoencoder for multi-task learning", "author": ["F. Zhuang", "D. Luo", "X. Jin", "H. Xiong", "P. Luo", "Q. He"], "venue": "Proc. of ICDM\u201915", "citeRegEx": "40", "shortCiteRegEx": null, "year": 2015}], "referenceMentions": [{"referenceID": 7, "context": "For instance, 80% of movies watched on Netflix come from the recommender system of the company [8].", "startOffset": 95, "endOffset": 98}, {"referenceID": 19, "context": "DOI: on items to well perform [20].", "startOffset": 30, "endOffset": 34}, {"referenceID": 9, "context": "Common latent factor techniques compute a low-rank rating matrix by applying Singular Value Decomposition through gradient descent [10] or Regularized Alternating Least Square algorithm [39].", "startOffset": 131, "endOffset": 135}, {"referenceID": 38, "context": "Common latent factor techniques compute a low-rank rating matrix by applying Singular Value Decomposition through gradient descent [10] or Regularized Alternating Least Square algorithm [39].", "startOffset": 186, "endOffset": 190}, {"referenceID": 24, "context": "Newer algorithms were explored to face those constraints such as Factorization Machines [25].", "startOffset": 88, "endOffset": 92}, {"referenceID": 15, "context": "More recent works combine several low-rank matrices such as Local Low Rank Matrix Approximation [16] or WEMAREC [3] to enhance the recommendation.", "startOffset": 96, "endOffset": 100}, {"referenceID": 2, "context": "More recent works combine several low-rank matrices such as Local Low Rank Matrix Approximation [16] or WEMAREC [3] to enhance the recommendation.", "startOffset": 112, "endOffset": 115}, {"referenceID": 0, "context": "A successful approach [1, 24] extends the Bayesian Probabilistic Matrix Factorization Framework [26] to integrate side information.", "startOffset": 22, "endOffset": 29}, {"referenceID": 23, "context": "A successful approach [1, 24] extends the Bayesian Probabilistic Matrix Factorization Framework [26] to integrate side information.", "startOffset": 22, "endOffset": 29}, {"referenceID": 25, "context": "A successful approach [1, 24] extends the Bayesian Probabilistic Matrix Factorization Framework [26] to integrate side information.", "startOffset": 96, "endOffset": 100}, {"referenceID": 16, "context": "However, recent algorithms outperform them in the general case [17].", "startOffset": 63, "endOffset": 67}, {"referenceID": 34, "context": "In this paper we introduce a CF approach based on Stacked Denoising Autoencoders [35, 40] which tackles both challenges: learning a non-linear representation of users and items, and alleviating the cold start problem by integrating side information.", "startOffset": 81, "endOffset": 89}, {"referenceID": 39, "context": "In this paper we introduce a CF approach based on Stacked Denoising Autoencoders [35, 40] which tackles both challenges: learning a non-linear representation of users and items, and alleviating the cold start problem by integrating side information.", "startOffset": 81, "endOffset": 89}, {"referenceID": 26, "context": "Compared to previous attempts in that direction [27, 28, 30, 5, 38], our framework integrates the sparse matrix of ratings and side information in a unique Network.", "startOffset": 48, "endOffset": 67}, {"referenceID": 27, "context": "Compared to previous attempts in that direction [27, 28, 30, 5, 38], our framework integrates the sparse matrix of ratings and side information in a unique Network.", "startOffset": 48, "endOffset": 67}, {"referenceID": 29, "context": "Compared to previous attempts in that direction [27, 28, 30, 5, 38], our framework integrates the sparse matrix of ratings and side information in a unique Network.", "startOffset": 48, "endOffset": 67}, {"referenceID": 4, "context": "Compared to previous attempts in that direction [27, 28, 30, 5, 38], our framework integrates the sparse matrix of ratings and side information in a unique Network.", "startOffset": 48, "endOffset": 67}, {"referenceID": 37, "context": "Compared to previous attempts in that direction [27, 28, 30, 5, 38], our framework integrates the sparse matrix of ratings and side information in a unique Network.", "startOffset": 48, "endOffset": 67}, {"referenceID": 9, "context": "A standard setting for CF is Matrix Completion [10].", "startOffset": 47, "endOffset": 51}, {"referenceID": 10, "context": "The proposed approach builds upon Autoencoders which are feed-forward Neural Networks popularized by Kramer [11].", "startOffset": 108, "endOffset": 112}, {"referenceID": 5, "context": "Recent work in Deep Learning advocates to stack pretrained encoders to initialize Deep Neural Networks [6].", "startOffset": 103, "endOffset": 106}, {"referenceID": 34, "context": "[35] tackle this issue by corrupting inputs, pushing the Network to denoise the final outputs.", "startOffset": 0, "endOffset": 4}, {"referenceID": 26, "context": "In a preliminary work, [27] tackled the Netflix challenge using Restricted Boltzmann Machines but little published work had follow [33].", "startOffset": 23, "endOffset": 27}, {"referenceID": 32, "context": "In a preliminary work, [27] tackled the Netflix challenge using Restricted Boltzmann Machines but little published work had follow [33].", "startOffset": 131, "endOffset": 135}, {"referenceID": 12, "context": "While Deep Learning has tremendous success in image and speech recognition [13], sparse data has received less attention and remains a challenging problem for Neural Networks.", "startOffset": 75, "endOffset": 79}, {"referenceID": 12, "context": "Nevertheless, Neural Networks are able to discover nonlinear latent variables with heterogeneous data [13] which makes them a promising tool for CF.", "startOffset": 102, "endOffset": 106}, {"referenceID": 27, "context": "[28, 30, 5] directly train Autoencoders to provide the best predicted ratings.", "startOffset": 0, "endOffset": 11}, {"referenceID": 29, "context": "[28, 30, 5] directly train Autoencoders to provide the best predicted ratings.", "startOffset": 0, "endOffset": 11}, {"referenceID": 4, "context": "[28, 30, 5] directly train Autoencoders to provide the best predicted ratings.", "startOffset": 0, "endOffset": 11}, {"referenceID": 27, "context": "For instance, AutoRec [28] replaces unpredictable ratings by an arbitrary selected score.", "startOffset": 22, "endOffset": 26}, {"referenceID": 22, "context": "Other contributions deal with this cold start problem by using Neural Networks properties for CBF: Neural Networks are first trained to learn a feature representation from the item which is then processed by a CF approach such as Probabilistic Matrix Factorization [23] to provide the final rating.", "startOffset": 265, "endOffset": 269}, {"referenceID": 6, "context": "For instance, [7, 36] respectively auto-encode bag-of-words from restaurant reviews and movie plots, [19] auto-encode heterogeneous side information from users and items.", "startOffset": 14, "endOffset": 21}, {"referenceID": 35, "context": "For instance, [7, 36] respectively auto-encode bag-of-words from restaurant reviews and movie plots, [19] auto-encode heterogeneous side information from users and items.", "startOffset": 14, "endOffset": 21}, {"referenceID": 18, "context": "For instance, [7, 36] respectively auto-encode bag-of-words from restaurant reviews and movie plots, [19] auto-encode heterogeneous side information from users and items.", "startOffset": 101, "endOffset": 105}, {"referenceID": 33, "context": "Finally, [34, 37] use Convolutional Networks on music samples.", "startOffset": 9, "endOffset": 17}, {"referenceID": 36, "context": "Finally, [34, 37] use Convolutional Networks on music samples.", "startOffset": 9, "endOffset": 17}, {"referenceID": 31, "context": "Most of the papers dealing with sparse inputs get around by pre-computing an estimate of the missing values [32, 2].", "startOffset": 108, "endOffset": 115}, {"referenceID": 1, "context": "Most of the papers dealing with sparse inputs get around by pre-computing an estimate of the missing values [32, 2].", "startOffset": 108, "endOffset": 115}, {"referenceID": 21, "context": "Such problems have already been studied in industry [22] where 5% of the values are missing.", "startOffset": 52, "endOffset": 56}, {"referenceID": 26, "context": "This operation is equivalent to removing the neurons with missing values described in [27, 28].", "startOffset": 86, "endOffset": 94}, {"referenceID": 27, "context": "This operation is equivalent to removing the neurons with missing values described in [27, 28].", "startOffset": 86, "endOffset": 94}, {"referenceID": 14, "context": "Importantly, Autoencoders with sparse inputs differs from sparseAutoencoders [15] or Dropout regularization [29] in the sense that Sparse Autoencoders and Droupout inhibit the hidden neurons for regularization purpose.", "startOffset": 77, "endOffset": 81}, {"referenceID": 28, "context": "Importantly, Autoencoders with sparse inputs differs from sparseAutoencoders [15] or Dropout regularization [29] in the sense that Sparse Autoencoders and Droupout inhibit the hidden neurons for regularization purpose.", "startOffset": 108, "endOffset": 112}, {"referenceID": 38, "context": "Alternating Least Squares with Weighted-\u03bb-Regularization (ALS-WR) [39] solves a low-rank matrix factorization problem by alternatively fixing U and V and solving the resulting linear problem.", "startOffset": 66, "endOffset": 70}, {"referenceID": 3, "context": "SVDFeature [4] is a Machine Learning Toolkit for featurebased Collaborative Filtering.", "startOffset": 11, "endOffset": 14}, {"referenceID": 20, "context": "The Douban dataset [21] provides 17 million discrete ratings from 129 thousands users on 58 thousands movies.", "startOffset": 19, "endOffset": 23}, {"referenceID": 13, "context": "rule [14]: Wij \u223c U [ \u2212 1 \u221a n , 1 \u221a n ] .", "startOffset": 5, "endOffset": 9}, {"referenceID": 30, "context": "Hyperparameters are tuned by a genetic algorithm already used by [31] in a different context.", "startOffset": 65, "endOffset": 69}, {"referenceID": 17, "context": "To the best of our knowledge, the best result published regarding MovieLens10M (without side information) are reported by [18] and [3] with a final RMSE of respectively 0.", "startOffset": 122, "endOffset": 126}, {"referenceID": 2, "context": "To the best of our knowledge, the best result published regarding MovieLens10M (without side information) are reported by [18] and [3] with a final RMSE of respectively 0.", "startOffset": 131, "endOffset": 134}, {"referenceID": 8, "context": "For instance, [9, 12] reports a global RMSE above 0.", "startOffset": 14, "endOffset": 21}, {"referenceID": 11, "context": "For instance, [9, 12] reports a global RMSE above 0.", "startOffset": 14, "endOffset": 21}, {"referenceID": 26, "context": "[27, 28] face this sparsity constraint by building small dense Networks with shared weights.", "startOffset": 0, "endOffset": 8}, {"referenceID": 27, "context": "[27, 28] face this sparsity constraint by building small dense Networks with shared weights.", "startOffset": 0, "endOffset": 8}], "year": 2016, "abstractText": "A standard model for Recommender Systems is the Matrix Completion setting: given partially known matrix of ratings given by users (rows) to items (columns), infer the unknown ratings. In the last decades, few attempts where done to handle that objective with Neural Networks, but recently an architecture based on Autoencoders proved to be a promising approach. In current paper, we enhanced that architecture (i) by using a loss function adapted to input data with missing values, and (ii) by incorporating side information. The experiments demonstrate that while side information only slightly improve the test error averaged on all users/items, it has more impact on cold users/items.", "creator": "LaTeX with hyperref package"}}}