{"id": "1606.07767", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "24-Jun-2016", "title": "Sampling-based Gradient Regularization for Capturing Long-Term Dependencies in Recurrent Neural Networks", "abstract": "Vanishing (and exploding) gradients effect is a common problem for recurrent neural networks with nonlinear activation functions which use backpropagation method for calculation of derivatives. Deep feedforward neural networks with many hidden layers also suffer from this effect. In this paper we propose a novel universal technique that makes the norm of the gradient stay in the suitable range. We construct a way to estimate a contribution of each training example to the norm of the long-term components of the target function s gradient. Using this subroutine we can construct mini-batches for the stochastic gradient descent (SGD) training that leads to high performance and accuracy of the trained network even for very complex tasks. We provide a straightforward mathematical estimation of minibatch s impact on for the gradient norm and prove its correctness theoretically. To check our framework experimentally we use some special synthetic benchmarks for testing RNNs on ability to capture long-term dependencies. Our network can detect links between events in the (temporal) sequence at the range approx. 100 and longer. An optimal network can also detect network interaction with neighboring nodes. Our training is based on a technique that takes an intermediate step back to the first approximation. An algorithm must also perform the steps from the current model. The best predictor is that the network will behave as expected, and thus will behave as expected. If we try to minimize the gradient effects in our experiments, we need to change the parameters for the time frame that can be generated. Therefore, we also need to specify the time of training on the input dataset from the first approximation.\n\n\nThe maximum-maximum-mean, maximum-mean gradient gradient gradient gradient gradient gradient gradient gradient gradient gradient gradient gradient gradient gradient gradient gradient gradient gradient gradient gradient gradient gradient gradient gradient gradient gradient gradient gradient gradient gradient gradient gradient gradient gradient gradient gradient gradient gradient gradient gradient gradient gradient gradient gradient gradient gradient gradient gradient gradient gradient gradient gradient gradient gradient gradient gradient gradient gradient gradient gradient gradient gradient gradient gradient gradient gradient gradient gradient gradient gradient gradient gradient gradient gradient gradient gradient gradient gradient gradient gradient gradient gradient gradient gradient gradient gradient gradient gradient gradient gradient gradient gradient gradient gradient gradient gradient gradient gradient gradient gradient gradient gradient gradient gradient gradient gradient gradient gradient gradient gradient gradient gradient gradient gradient gradient gradient gradient gradient gradient gradient gradient gradient gradient gradient gradient gradient gradient gradient gradient gradient gradient gradient gradient gradient gradient gradient gradient gradient gradient gradient gradient gradient gradient gradient gradient gradient gradient gradient gradient gradient gradient gradient gradient gradient gradient gradient gradient gradient gradient gradient gradient gradient gradient gradient gradient gradient gradient gradient gradient gradient gradient gradient gradient gradient gradient gradient gradient gradient gradient", "histories": [["v1", "Fri, 24 Jun 2016 17:31:02 GMT  (3731kb,D)", "http://arxiv.org/abs/1606.07767v1", null], ["v2", "Tue, 31 Jan 2017 21:30:29 GMT  (3987kb,D)", "http://arxiv.org/abs/1606.07767v2", null], ["v3", "Mon, 13 Feb 2017 21:25:26 GMT  (3916kb,D)", "http://arxiv.org/abs/1606.07767v3", null]], "reviews": [], "SUBJECTS": "cs.NE cs.LG", "authors": ["artem chernodub", "dimitri nowicki"], "accepted": false, "id": "1606.07767"}, "pdf": {"name": "1606.07767.pdf", "metadata": {"source": "CRF", "title": "Sampling-based Gradient Regularization for Capturing Long-Term Dependencies in Recurrent Neural Networks", "authors": ["Artem Chernodub", "Dimitri Nowicki"], "emails": ["a.chernodub@gmail.com", "nowicki@nnteam.ogr.ua"], "sections": [{"heading": null, "text": "Index Terms\u2014simple recurrent networks, gradient vanishing, regularization\nI. INTRODUCTION\nRecurrent Neural Networks (RNNs) are known as universal approximators of dynamic systems [1], [2]. Since RNNs are able to simulate any open dynamical system, they have a broad spectrum of applications such as time series forecasting [3], control of plants [4], language modeling [5], speech recognition [6], neural machine translation [7] and other domains. The easiest way to create an RNN is adding the feedback connections to the hidden layer of multilayer perceptron. This architecture is known as Simple Recurrent Network (SRN). Despite of the simplicity, it has rich dynamical approximation capabilities mentioned above. However, in practice training of SRNs using first-order optimization methods is difficult [8]. The main problem is well-known vanishing/exploding gradients effect that prevents capturing of long-term dependencies in data. Vanishing gradients effect is a common problem for recurrent neural networks with sigmoid-like activation functions which uses a backpropagation method for calculation of derivatives. Deep feedforward neural networks with many hidden layers also feels this effect. Hochreiter and Schmidhuber designed a set of special synthetic benchmarks for testing RNNs on ability to capture long-term dependencies [9]. They\nshowed that ordinary SRNs are very ineffective to learn correlations in sequential data if distance between the target events is more than 10 time steps. The solution could be using more advanced second-order optimization algorithms such as Extended Kalman Filter [10], LBFGS, Hessian-Free optimization [11], but they require much more memory and computational resources for state-of-the-art networks. Another and the most popular solution is designing a neural networks architecture with more suitable dynamics. Echo State Networks (ESNs) proposed by Jaeger [12] may be considered as big reservoirs of sparsely connected neurons and randomly initialized weights which produces chaotic dynamics. For ESNs the gradients are computed for the last non-recurrent weights layer only. Experiments show that this may be enough for capturing longterm dynamics [13]. At the same time, ESNs often seems to have abundant number of free parameters. We also mention such an alternative to temporal neural networks as hierarchical sequence processing with auto-associative memories [14] that use distributed coding.\nAnother approach that was specially designed for catching the long-term dependencies is Long-Short Term Memory (LSTM) [9]. These neural networks are designed to adaptively reset or update their memory. They have specially designed complex structure that includes input and forgetting gates and they have constant error flow carousel. Currently it is probably the most popular family of RNNs models that shows stateof-the-art performance in several domains including speech recognition [6], image captioning [15] and neural machine translation. An idea of using input/forgetting gates inspired a lot of followers, Gated Recurrent Units (GRU) networks is probably one of the most successful of them [7]. Finally, the united team from Google and Facebook performed a grand experiment on finding the best architecture for RNNs [16]. They numerically evaluated 10,000 random architectures with 230,000 hyperparameter configurations in total and obtained a couple of new advanced models and some recommendations for easy improving of standard LSTMs. At the same time, a question how to train SRNs for catching the longterm dependencies is still a topic of current interest. It is highly desirable at least for better understanding of underlying processes of the training inside the recurrent and deep neural\nIEEE 1 | P a g e\nar X\niv :1\n60 6.\n07 76\n7v 1\n[ cs\n.N E\n] 2\n4 Ju\nn 20\nnetworks. Also, SRNs are more compact and fast working models of RNNs in comparison with ESNs and LSTMs that is very important for implementation to mobile and embedded devices. Recent research shows the ability to train SRNs for long term dependencies up to 100 time steps and more using several new techniques [17], [18]\u2013[20]. It includes very accurate initialization of start weights, scaling down the big gradients, using advanced Nesterov momentum. One of the most common methods for preventing the vanishing gradients effect from this pool is known as \u201d\u2018gradient regularization\u201d\u2019 [20], [21]; also it was independently proposed in [22] as \u201d\u2018method of pseudoregularization\u201d\u2019. The idea of gradient regularization is adding to the target cost function an additional term that controls the norm of the backpropagated gradients. In this way, neural network learns not only to fit the train data but also to keep the flow of gradients into the certain range. In this paper we propose a new method to perform the gradient regularization by selection of proper samples in dataset using the simple criteria."}, {"heading": "II. BACKPROPAGATION MECHANISM REVISITED", "text": "Consider a Simple Recurrent Neural Network (Fig. 1) that at each time step k receives an external input u(k), previous internal state z(k \u2212 1) and produces output y(k + 1):\na(k) = u(k)win + z(k \u2212 1)wrec + b, z(k) = f(a(k)), y(k + 1) = g(z(k)wout),\n(1)\nhere win is a matrix of input weights, wrec is matrix of recurrent weights, wout is matrix of output weights, a(k) is known as \u201cpresynaptic activations\u201d, z(k) is a network\u2019s state, f(\u00b7) and g(\u00b7) are nonlinear activation functions for hidden and output layer respectively. In this work we always use tanh function for hidden layer and optionally softmax or linear function depending on the target problem (classification or regression). Also in our equations data vectors u(k) are vector rows (not vector columns) because it is often implemented in many software tools like MATLAB and therefore is more useful for practice.\nAfter processing an input sequence u(1),u(1) . . . ,u(k) and producing the output y(k + 1), target error E(k + 1) is calculated. There are different options for target error function, the most popular choices are mean squared error for regression\nand cross-entropy error for classification. To train neural network using gradient-based optimization algorithm we have to calculate derivatives of error function subs to network\u2019s weights \u2202E\u2202w . To perform this, a well-known Backpropagation Through Time (BPTT) method for calculation of dynamic derivatives is used. The idea of BPTT is unrolling the recurrent neural network back in time. In this way, RNN is presented as a deep feedforward neural network with shared weights where each layer is referring to one of retrospective time steps.\nThen a standard backpropagation is applied to this deep network, immediate derivatives \u2202E\u2202w(n) are calculated. In derivations below we use a framework very similar to [20] but based on studying evolution of local gradients \u03b4(k) for the backpropagation procedure.\nThe dynamic derivative is a sum of immediate derivatives: \u2202E \u2202w = \u2211h n=1 \u2202E \u2202w(k\u2212n) , where n = 1, ..., h, where h is BPTT\u2019s truncation depth. An intermediate variable \u03b4 \u2261 \u2202E\u2202a called a \u201clocal gradients\u201d or simply \u201cdeltas\u201d is usually introduced for convenience. If deltas are known for specific layer n then corresponding immediate derivatives can be obtained easily:\n\u2202E \u2202win(n) = u(n)T \u03b4(n) and \u2202E\u2202wrec(n) = z(n\u2212 1) T \u03b4(n).\nFor the last layer \u03b4 is an error residual, for the intermediate layers deltas are incrementally calculated according to very famous backpropagation formula:\n\u03b4j(n\u2212 1) = f \u2032(aj(n\u2212 1)) \u2211 i wijrec\u03b4i(n). (2)\nLet\u2019s write this equation in a matrix form:\n\u03b4(n\u2212 1) = \u03b4(n)wTrecdiag(f \u2032(a(n\u2212 1))). (3)\nwhere diag converts a vector into diagonal matrix. Equation (3) may be rewritten using is Jacobian matrix J(n) = \u2202z(n)\u2202z(n\u22121) :\n\u03b4(n\u2212 1) = \u03b4(n)J(n), (4)\nwhere\nJ(n) = wTrecdiag(f \u2032(a(n\u2212 1))). (5)\nNow we can use an intuitive understanding of exploding/vanishing gradients problem that was deeply investigated in classic [8], [23] and modern papers [18], [20]. As it can be seen from (5), norm of the backpropagated deltas is strongly dependent on norm of the Jacobians. Moreover, they actually are product of Jacobians: \u03b4(n\u2212h) = \u03b4(n)J(n)J(n\u22121)...J(n\u2212 h+1). The \u201colder\u201d deltas are, the more Jacobian matrices were multiplied. If norm of Jacobians are more than 1, the gradients will grow exponentially in most cases. It refers to the RNN\u2019s behavior where long-term components are more important than short-term ones. Vice versa, if norm of Jacobians are less than 1, this leads to vanishing gradients and \u201cforgetting\u201d the longterm events.\nAt the same time, even if both factors in (5) have norm 1, it still not guarantee the same norm of J(n). In [20] the power iteration method was used to formally analyze product of Jacobian matrices and obtain tight conditions for when the\nIEEE 2 | P a g e\ngradients explode or vanish. It was shown that it is sufficient for the largest eigenvalue of the recurrent weight matrix wTrec to be smaller than 1 for long term components to vanish and necessary for it to be larger than 1 for gradients to explode."}, {"heading": "III. GRADIENT REGULARIZATION", "text": "In [22] one can find an approach called \u201cpseudoregularization\u201d for forcing the recurrent neural networks to capture long-term behavior. The idea is to control the backpropagation flow during the training. The target cost function L(w) was modified there in order to perform a multi-objective optimization by adding an additional term \u2126(w) responsible for the size of backpropagated gradients:\nL(w) = E(w) + \u03bb\u2126(w), (6)\nwhere E(w) is a target error function, \u2126(w) is a regularizer that prevents \u201covervanishing\u201d the gradients, \u03bb is a gradient regularization rate. This regularizer was a function of the deltas:\n\u2126(w) = \u2211 k (\u3008\u2016\u03b4(w, k)\u2016\u3009 \u2212 1)2 . (7)\nit\u2019s goal was set to make a mean norm of deltas 1. For this purpose derivatives \u2202\u2126(w)\u2202w were derived and used in gradientsbased optimization algorithm. However, this approach shows unsatisfactory sensitivity from the gradients regularization rate \u03bb and has very capricious behavior of training in general.\nSimilar, but more advanced approach called \u201cgradient regularization\u201d [21] was independently proposed by [20]. They used the following regularizer:\n\u2126(w) = \u2211 k\n \u2225\u2225\u2225 \u2202E\u2202z(k+1) \u2202z(k+1)\u2202z(k) \u2225\u2225\u2225\n\u2202E \u2202z(k+1)\n\u2212 1 2 . (8) It forces the Jacobian matrices \u2202z(k+1)\u2202z(k) to preserve norm in the relevant direction \u2202E\u2202z(k) , not for any direction like in the works mentioned above. Also, these constraints are softer. However, analytic derivatives \u2202\u2126(w)\u2202w for (8) are rather bulky; their calculation is a hard task. Authors used Theano [Bergstra, 2010] that has a built-in symbolic mathematical engine for automatic calculation of derivatives. However, such functionality is uncommon right now that complicates using this approach."}, {"heading": "IV. DIFFERENTIATION OF THE GRADIENT\u2019S NORM", "text": "Consider we have a mini-batch of training data d = {u1; t1; ...;uN , tN} containing N samples. We made a forward and backward passes using this mini-batch and calculated the derivative dw = \u2202E\u2202w . Now we want to check whether applying dw to the network\u2019s weights w minimizes or maximizes norm of the backpropagated gradients. We control \u2016\u03b4(\u00b7)\u2016, because it is clear that norms of the derivatives \u2202E \u2202wrec and \u2202E\u2202win are strongly dependent from norms of local gradients.\nTheorem 1: Let \u2202wrec is a recurrent weights matrix of Simple Recurrent Network. Assume we already did forward and backward passes and calculated the weights update dwrec for the recurrent matrix, w(i+1)rec = w (i) rec + dwrec. Sufficient condition for increasing \u2016\u03b4(k \u2212 h)\u2016Fro (norm of local gradients propagated h time steps back from the current time step k) is dS > 0, where\ndS = (G, dG), G = (\u220f1\ni=h diag(f \u2032(a(k \u2212 i+ 1)))wrec\n) \u03b4(k),\ndG = \u2211h\ni=1 ((\u220f1 j=h diag [f \u2032(a(k \u2212 j + 1))]v ) \u03b4(k) ) ,\nv = dwrec, if i = j; v = wrec, if i 6= j. (9)\nSimilarly, sufficient condition for decreasing \u2016\u03b4(k \u2212 h)\u2016Fro is dS < 0.\nProof: Consider function S\u2032(wrec) that is Frobenius (Euclidean) norm of local gradients (2):\nS\u2032(wrec) = \u2016\u03b4(k \u2212 h,wrec)\u2016 . (10)\nUsing (3), (4) we get:\n\u03b4(k\u2212h) = \u03b4(k)wTrecdiag(f \u2032(a(k\u22121)))...wTrecdiag(f \u2032(a(k\u2212h+1))). (11)\nFor convenience, let\u2019s denote Dn \u2261 diag(f \u2032(a(n))) and change the indexes for time steps as follows: (k \u2212 h)-th step is the 1-st time step, (k\u2212 h+ 1)-th is the 2-nd time step, k-th is the H-th time step. So far, equation (11) becomes:\n\u03b4(1) = \u03b4(H)wTrecDH\u22121w T recDH\u22122...w T rec.D1 (12)\nSince differentiation of the squared norm is simpler than differentiation of the norm itself, \u2016A\u2016Fro = \u2225\u2225AT\u2225\u2225 Fro\n, (A \u00b7 B)T = BTAT , we may introduce an intermediate variable G:\nG = D1wrecD2wrec...DHwrec\u03b4(H). (13)\nSo, we are interested in catching a behavior of function S(wrec) around the current point, where\nS(wrec) = \u2016G\u20162Fro . (14)\nTo understand behavior of S let\u2019s calculate a differential dS. The sign of dS defines either S increases or decreases; absolute value of dS defines the speed. Thus, we obtain:\ndS = 2(G, dG), (15)\nwhere G is (13) and dG is calculated as follows:\ndG = H\u2211 i=1 \u03b4(k) \u00b7Di \u00b7wrec \u00b7 ... \u00b7Di \u00b7dwrec \u00b7 ... \u00b7D1 \u00b7wrec, (16)\nIEEE 3 | P a g e\nor, in another form:\ndG = \u2211H\ni=1 ((\u220fH j=1 Djv ) \u03b4(k) ) ,\nv = dwrec, if i = j; v = wrec, if i 6= j. (17)\nThe theorem is proven. For calculation of (15) we use scalar product (,) that may be\nformally introduced as (A,B) = \u2211\ni,j aijbij . Another option is to reshape matrices to the vectors and use standard scalar products for vectors.\nIn this paper we propose a novel method for gradient regularization using the simple analytic criteria for selection of samples for training."}, {"heading": "V. SAMPLING-BASED GRADIENT REGULARIZATION", "text": "It is generally known that training recurrent networks is a rather capricious process. Modification of cost function for preserving the norm of gradients causes difficulties even if gradients vectors are modified in relevant direction \u2202E\u2202z(k) . Training of SRNs on sequences containing short-term dependencies using modified cost function (6) produces worse accuracy than training using default cost function. Clearly, difficulty of training RNNs is related to the complexity of error surface. Adding an additional goal of optimization may only to complicate the training process. Also, inaccurate gradients causes a natural for RNNs so-called \u201cbutterfly effect\u201d where a little perturbations at the start leads to large divergences in the end of produced sequences. This is a reason why using popular for DNN regularization methods like dropout doesn\u2019t work well for RNNs [24].\nIdea of our method is selection of \u201cproper\u201d samples of data for training. As it was shown in previous section, now we can clearly understand an impact of each mini-batch of input data to the norm of backpropagated gradients. It is possible to propose different strategies of multi-objective optimization to prevent vanishing the gradients. However, in this paper we decided to use the simplest and the most straightforward method: we watch a norm of the gradients; if norm becomes very small, we omit such mini-batches of data, which decrease this norm. Vice versa, if norm becomes very large, we skip mini-batches increasing this norm even more. Note also that it is better to skip minibatches with large |dS|: they can cause high \u201d\u2018leaps\u201d\u2019 of the gradient norm and therefore its selfoscillations.\nTo evaluate quality of backpropagation let\u2019s introduce a new auxiliary variable called Q-factor:\nQ(\u03b4, h) = log10( \u2016\u03b4(k)\u2016 \u2016\u03b4(k \u2212 h)\u2016 ), (18)\nthat measures how much the norm of the gradient is decreased during the backpropagation. For ideal catching of long-term dependencies Q-factor must be close to 0."}, {"heading": "A. Computationally Efficient Cached Algorithm", "text": "Target differential dS which is responsible for the norm of backpropagated gradients includes two parts: G and dG\n(second and 3d formulae in (9) respectively). Let\u2019s rewrite it in more simple form:\ndG = D1wrec...DHwrec\u03b4(H), dG = \u2211H j=1 D1wrec...Djdw...DHwrec\u03b4(H), (19)\nwhere H is equal to the horizon of BPTT. Let\u2019s estimate the complexity of algorithm if we calculate it in a straightforward way. We see that G has 2H multiplications of matrices, so it\u2019s complexity is 2H \u00b7 O(N3w) where size of recurrent matrix wrec is Nw \u00b7 Nw. However, 50% of these matrices are diagonal matrices which have multiplication complexity O(N2w). Therefore, total calculation complexity of G may be estimated as O(HN3w). At the same time, dG consists of H summands of products of matrices. Their calculation complexity is O(HN3w), so the total calculation complexity of dS is H(H + 1) \u00b7O(N3w) or \u2248 O(H2N3w).\nMeanwhile, we can propose much more numerically effective algorithm for calculation of dS. It is clear from (19) that each summand of dG contains a lot of common parts which can be cached. Let\u2019s introduce additional matrices Lj and Rj which are responsible for left and right parts against Djdwrec in (19):\ndG = H\u2211 j=1 Lj (Djdwrec)Rj , (20)\nj = 1, ...,H. Matrices Lj and Rj may be calculated recursively.\nL1 = I, Lj+1 = Lj\u22121Djwrec,\n(21)\nwhere I is an identity matrix. Similarly,\nRH = I, Rj\u22121 = DjwrecRj .\n(22)\nThus, we need 2(N3w) operations for calculation Lj and Rj for each summand in (20). Also, we need 2(N3w) operations for multiplication of these matrices and Djwrec in (20). Finally, the complexity of algorithm now becomes O(HN3w). This cached method is much more effective numerically in comparison to the straightforward method because it is linear function of horizon of backpropagation instead of quadratic function.\nWe performed a numerical experiment to test the performance of the methods discussed above. We used a neural network with 100 hidden units that is described in the \u201cExperiments\u201d section.\nAlgorithm description We have a training data that contains train data {U,T}. Let\u2019s call an epoch a set of M weights updates. In our experiments one epoch means processing of 100 mini-batches.\nIEEE 4 | P a g e\nAlgorithm 1 Main Algorithm Input: training data {U,T} , r0 > 0. for each minibatch ui;di with NMD vectors do\ncalculate an the differential dS, if |dS| > 0 break make forward and backward propagation f the network compute the Q factor Q(\u03b4, h) using formula 18 if Q((\u03b4, h) \u2208 [Qmin;Qmax] then\nuse current minibatch di to compute the derivative \u2202E\u2202w else\nif (Q((\u03b4, h) < Qmin and dS > 0) or (Q(\u03b4, h) > Qmax and dS < 0) then\nuse current minibatch for training else\nbreak end if\nend if end for"}, {"heading": "VI. EXPERIMENTS", "text": ""}, {"heading": "A. Experimental tasks", "text": "We refer to [11] and [20] for descripton of experiment. They used pathological synthetic test set from [9] that requires long-term correlations. We used four problems from this set: \u201cAddition\u201d, \u201cMultiplication\u201d, \u201cTemporal order\u201d, \u201cTemporal order 3-bit\u201d.\nFor \u201cAddition problem\u201d the input consists of a sequence of random numbers, where two random positions (one in the beginning and one in the middle of the sequence) are marked. The model needs to predict the sum of the two random numbers after the entire sequence was seen. The first position is sampled from [1;T \u2032/10], while the second position is sampled from [T \u2032/10;T \u2032/2], where T\u2019 is sampled from [T ; 11/10T ], T is the length of the sequence in the paper.\nFor description of other problems, please, see [20]."}, {"heading": "B. Description of experiments", "text": "We use SRNs with 100 hidden units and tanh activation function. For output layer linear activation function was used for regression and softmax for classification. To perform better comparison, two sets containing 10 networks each were\ninitialized by random values and saved. Thus, for training using different methods we varied training algorithms only, initial weights of neural networks were the same. \u201cSafe\u201d range [QMIN ;QMAX ] was set to [\u22121; 1].\nWe use Stochastic Gradient Descent (SGD) as optimization algorithm, training speed \u03b1 = 10\u22125...10\u22123, momentum \u00b5 = 0.9, size of mini-batch is 10. Dataset contains 20,000 samples for training, 1000 samples for validation and 10,000 samples for tests. Training process consists 2000 epochs, each epoch consists 50 iterations, i.e. 100,000 corrections of weights at all. Selection of the best copy of trained network is performed using \u201d\u2018save best\u201d\u2019 method: after each epoch, network\u2019s performance is tested on validation dataset; network that has the best performance on the validation dataset is tested on the test dataset, this result is recognized as the final result."}, {"heading": "C. Experimental results", "text": "For recurrent neural networks good initialization of weights is very important because vanishing/exploding gradients has monotonous flow in most cases because gradients are propagated through the same matrix of recurrent weights. In our experiments weights were initialized by small values from Gaussian distribution with zero mean and standard deviation \u03c3 = 0.01 as in [20]. On Fig. 4 average norms of gradients as function of backpropagation depth (before training, further referred as initial gradients) are graphed for different values \u03c3. This experiment was made for the \u201cTemporal order problem\u201d.\nEach chart at Fig 4 contains three curves: average norms of local gradients \u03b4(k) (blue) and average norms of gradients \u2206w(k)in \u2261 \u2202E\u2202win and \u2206w(k)rec \u2261 \u2202E \u2202wrec\n(red and green). From the graphs at Fig. 3.5 one can ensure on practice that to control the norms \u2202E\u2202w which actually make changes to the weights and are under the main scope of our interest it is enough to control the norms of local gradients \u03b4(k) because they are highly correlated.\nIt can be seen from Fig. 4 that backpropagated gradient flow is very sensitive to the parameters of random initializing of weights. Size of standard deviation that was used for our experiments from [Pascanu, 2012] ensures smooth flow of gradients\u2019 norms (Fig.4 , a), the Q-factor for this case is Q(\u03b4, h) \u2248 0.7 for h = 100. This is good for catching both long-term and short-term dependencies. But decreasing or increasing of parameter \u03c3 causes catastrophic effects for initial gradients flow, ( see Fig. 4 b) and c)). For \u03c3 = 0.005 average norm of gradients\u2019 falls down during h = 100 steps backwards to less than 10\u221225, here Q(\u03b4, h) > 25, it is classic\nIEEE 5 | P a g e\ncase of vanishing gradients effect: \u201cold\u201d inputs doesn\u2019t have impact on final error E(w) and on training of neural network at all.\nVice versa, for \u03c3 = 0.02 average norm of gradients is explosively growing, here Q(\u03b4, h) < \u22126, only \u201cold\u201d values make important changes to network\u2019s weights.\nPoorly initialized networks have bad chances to be successfully trained on long-term temporal problems using 1-st order optimization methods because i) low sensitivity of 1-st order methods to small variations in gradients values and ii) local nature of gradient optimization methods at all.\nHowever, proper initialization doesn\u2019t guarantee successful training. Particular case of forward and backward dynamics during training of SRN network is shown on Fig. 5. Norms of the backpropagated gradients are depicted on the top, mean and median activation values are shown on the bottom of each subfigure there . Neural network that is depicted on Fig.5 was initialized with \u03c3 = 0.01 and initial norms of backpropagated gradients were similar to Fig. 4 a). However, after 500 iterations we got norm of gradients less than 10\u22127 after h=100 backpropagations via BPTT. After that almost all the time neural networks had small gradients in the range 10\u22127...10\u22128 . From the graphs on Fig. 5, on the left, we see that area of small gradients is related to area of saturation for neuron\u2019s activations. This is a symptom of bad network abilities for successful training and obtaining good generalization properties.\nUsing gradient regularization enables keeping the norms of backpropagated gradients from vanishing or explosion. On Fig. 6 forward and backward dynamics of the previous neural network is shown; but now it is trained using our sampling-based gradient regularization. It follows from the graphs that norms of gradients are all time in the safe range, here Q(\u03b4, h) \u2208 [\u22121; 1]. Graphs of activation values in (Fig. 5) shows more proper forward dynamics that is almost all the time are out of saturation area.\nOn Fig. 7 network\u2019s performance for \u201cTemporal order problem\u201d with and without our proposed sampling-based gradient regularization for pre-initialized sets of 10 SRNs.\nUsing our sampling-based gradients regularization allows to refine quality of training. For lengths T = 50 and using\nfinely selected hyperparameters we got accuracies up to almost 100% correct answers without using gradients regularization. For lengths T = 100 and T = 150 improvement is 10-20% in average. If we believe that model is successful if it produces more than 99% correct answers then for T=100 it was very crucial. Results for other experiments are shown in Table I.\nFor sequences of length T = 100 we were able to train at least one successful model (with accuracy >99% as required [Hochreiter, 1997]) for all four problems using the proposed method. For two problems (Adding and Temporal order) it was impossible using the traditional training. Unfortunately, we were not able to train the successful model for T = 150 neither using the proposed method nor using the traditional method. Meanwhile, sampling-based gradient regularization also improved best and mean accuracies almost for all cases.\nSamples rejected by the algorithm during the training not necessarily are lost for using in future training process because\nIEEE 6 | P a g e\nthey may be used when network is in \u201csafe region\u201d or we may need to change norms of gradients in the opposite direction. Range for \u201csafe\u201d norms of gradients Q(\u03b4, h) \u2208 [\u22121; 1] was found empirically for our problems. For different problems and lengths of sequences it could be different."}, {"heading": "VII. CONCLUSION", "text": "We provided a novel solution of the problem of exploding and vanishing gradient effects, applied to the Simple Recurrent Networks. Basing on estimation of the gradient norm\u2019s differential we can predict each minibatch\u2019s influence.\nUsing this technique we build the algorithm that controls magnitude of the gradient operating solely with presence of the minbatches in the training sequence. We have shown mathematical correctness of this algorithm, and introduced mechanisms of negative feedback that prevent self-oscillations inside the training process.\nThis frameworks was tested for long-term prediction on a comprehensive set of appropriate benchmarks. Resulting accuracy outperforms best known SRN learning algorithms by 10-20%. This paradigm could be generalized to deep and\nmulti-layered recurrent networks, that is a subject of our future research."}, {"heading": "ACKNOWLEDGMENTS", "text": "We thank FlyElephant (http://flyelephant.net) and Dmitry Spodarets for computational resources kindly given for our experiments."}], "references": [{"title": "Computational capabilities of recurrent narx neural networks", "author": ["B.G. Horne H.T. Siegelmann"], "venue": "IEEE Transactions on Systems,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 1997}, {"title": "Recurrent neural networks are universal approximators", "author": ["H.G. Zimmermann A.M. Schfer"], "venue": "In Lecture Notes in Computer Science, International Conference on Artificial Neural Networks (ICANN 2006),", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2006}, {"title": "Advanced Methods for Time Series Prediction Using Recurrent Neural Networks, chapter Advanced Methods for Time Series Prediction Using Recurrent Neural Networks, page 1536", "author": ["H. Cardot R. Bone"], "venue": "Intech, Croatia,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2011}, {"title": "Toyota prius hev neurocontrol and diagnostics", "author": ["D.V. Prokhorov"], "venue": "Neural Networks,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2008}, {"title": "Recurrent neural network based language model", "author": ["et. al T. Mikolov", "M. Karafiat"], "venue": null, "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2010}, {"title": "Speech recognition with deep recurrent neural networks", "author": ["G. Hinton A. Graves", "A.R. Mohamed"], "venue": "In IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP),", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2013}, {"title": "On the properties of neural machine translation: Encoderdecoder approaches", "author": ["D. Bahdanau K. Cho", "B. van Merrienboer"], "venue": "In Eighth Workshop on Syntax, Semantics and Structure in Statistical Translation (SSST-8),", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2014}, {"title": "Learning long-term dependencies with gradient descent is difficult", "author": ["P. Frasconi Y. Bengio", "P. Simard"], "venue": "IEEE Trans. Neural Networks,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 1994}, {"title": "Long short-term memory", "author": ["S.J. Schmidhuber S. Hochreiter"], "venue": "Neural Computation,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 1997}, {"title": "Kalman Filtering and Neural Networks", "author": ["S. Haykin", "editor"], "venue": null, "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2001}, {"title": "Learning recurrent neural networks with hessianfree optimization", "author": ["I. Sutskever J. Martens"], "venue": "In Proceedings of the 28th International Conference on Machine Learning (ICML),", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2011}, {"title": "The echo state approach to analysing and training recurrent neural networks-with an erratum note", "author": ["Herbert Jaeger"], "venue": "Bonn, Germany: German National Research Center for Information Technology GMD Technical Report,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2001}, {"title": "Long short-term memory in echo state networks: Details of a simulation study", "author": ["H. Jaeger"], "venue": "Technical Report 27, Jacobs University,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2012}, {"title": "Multilevel assembly neural architecture and processing of sequences", "author": ["EM Kussul", "DA Rachkovskij"], "venue": "Neurocomputers and Attention: Connectionism and neurocomputers,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 1991}, {"title": "Deep visual-semantic alignments for generating image descriptions", "author": ["L. Fei-Fei A. Karpathy"], "venue": "In CVPR,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2015}, {"title": "An empirical exploration of recurrent network architectures", "author": ["I. Sutskever R. Jozefowicz", "W. Zaremba"], "venue": "In Proceedings of the 32-nd International Conference on Machine Learning,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2015}, {"title": "Statistical Language Models based on Neural Networks", "author": ["T. Mikolov"], "venue": "Ph.d. thesis, Brno University of Technology,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2012}, {"title": "Advances in optimizing recurrent networks", "author": ["R. Pascanu Y. Bengio", "N. Boulanger-Lewandowski"], "venue": "In Proceedings of IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP),", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2013}, {"title": "Training Recurrent Neural Networks", "author": ["Ilya Sutskever"], "venue": "Ph.d. thesis, University of Toronto,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2013}, {"title": "On the difficulty of training recurrent neural networks", "author": ["Y. Bengio R. Pascanu"], "venue": "Technical report, Universite de Montreal,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2012}, {"title": "Deep Learning", "author": ["A. Courville. Y. Bengio", "I.J. Goodfellow"], "venue": "book in preparation", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2015}, {"title": "Training neuroemulators using multicriteria extended kalman filter and pseudoregularization for model reference adaptive neurocontrol", "author": ["A.N. Chernodub"], "venue": "In Proceedings of IEEE IV International Congress on Ultra Modern Telecommunications and Control Systems (ICUMT),", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2012}, {"title": "Untersuchungen zu dynamischen neuronalen netzen", "author": ["S. Hochreiter"], "venue": "Master\u2019s thesis, TU Munich,", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 1991}, {"title": "Recurrent neural network regularization", "author": ["O. Vinyals W. Zaremba", "I. Sutskever"], "venue": "In ICLR 2015,", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2015}], "referenceMentions": [{"referenceID": 0, "context": "Recurrent Neural Networks (RNNs) are known as universal approximators of dynamic systems [1], [2].", "startOffset": 89, "endOffset": 92}, {"referenceID": 1, "context": "Recurrent Neural Networks (RNNs) are known as universal approximators of dynamic systems [1], [2].", "startOffset": 94, "endOffset": 97}, {"referenceID": 2, "context": "Since RNNs are able to simulate any open dynamical system, they have a broad spectrum of applications such as time series forecasting [3], control of plants [4], language modeling [5], speech recognition [6], neural machine translation [7] and other domains.", "startOffset": 134, "endOffset": 137}, {"referenceID": 3, "context": "Since RNNs are able to simulate any open dynamical system, they have a broad spectrum of applications such as time series forecasting [3], control of plants [4], language modeling [5], speech recognition [6], neural machine translation [7] and other domains.", "startOffset": 157, "endOffset": 160}, {"referenceID": 4, "context": "Since RNNs are able to simulate any open dynamical system, they have a broad spectrum of applications such as time series forecasting [3], control of plants [4], language modeling [5], speech recognition [6], neural machine translation [7] and other domains.", "startOffset": 180, "endOffset": 183}, {"referenceID": 5, "context": "Since RNNs are able to simulate any open dynamical system, they have a broad spectrum of applications such as time series forecasting [3], control of plants [4], language modeling [5], speech recognition [6], neural machine translation [7] and other domains.", "startOffset": 204, "endOffset": 207}, {"referenceID": 6, "context": "Since RNNs are able to simulate any open dynamical system, they have a broad spectrum of applications such as time series forecasting [3], control of plants [4], language modeling [5], speech recognition [6], neural machine translation [7] and other domains.", "startOffset": 236, "endOffset": 239}, {"referenceID": 7, "context": "However, in practice training of SRNs using first-order optimization methods is difficult [8].", "startOffset": 90, "endOffset": 93}, {"referenceID": 8, "context": "Hochreiter and Schmidhuber designed a set of special synthetic benchmarks for testing RNNs on ability to capture long-term dependencies [9].", "startOffset": 136, "endOffset": 139}, {"referenceID": 9, "context": "The solution could be using more advanced second-order optimization algorithms such as Extended Kalman Filter [10], LBFGS, Hessian-Free optimization [11], but they require much more memory and computational resources for state-of-the-art networks.", "startOffset": 110, "endOffset": 114}, {"referenceID": 10, "context": "The solution could be using more advanced second-order optimization algorithms such as Extended Kalman Filter [10], LBFGS, Hessian-Free optimization [11], but they require much more memory and computational resources for state-of-the-art networks.", "startOffset": 149, "endOffset": 153}, {"referenceID": 11, "context": "Echo State Networks (ESNs) proposed by Jaeger [12] may be considered as big reservoirs of sparsely connected neurons and randomly initialized weights which produces chaotic dynamics.", "startOffset": 46, "endOffset": 50}, {"referenceID": 12, "context": "Experiments show that this may be enough for capturing longterm dynamics [13].", "startOffset": 73, "endOffset": 77}, {"referenceID": 13, "context": "We also mention such an alternative to temporal neural networks as hierarchical sequence processing with auto-associative memories [14] that use distributed coding.", "startOffset": 131, "endOffset": 135}, {"referenceID": 8, "context": "Another approach that was specially designed for catching the long-term dependencies is Long-Short Term Memory (LSTM) [9].", "startOffset": 118, "endOffset": 121}, {"referenceID": 5, "context": "Currently it is probably the most popular family of RNNs models that shows stateof-the-art performance in several domains including speech recognition [6], image captioning [15] and neural machine translation.", "startOffset": 151, "endOffset": 154}, {"referenceID": 14, "context": "Currently it is probably the most popular family of RNNs models that shows stateof-the-art performance in several domains including speech recognition [6], image captioning [15] and neural machine translation.", "startOffset": 173, "endOffset": 177}, {"referenceID": 6, "context": "An idea of using input/forgetting gates inspired a lot of followers, Gated Recurrent Units (GRU) networks is probably one of the most successful of them [7].", "startOffset": 153, "endOffset": 156}, {"referenceID": 15, "context": "Finally, the united team from Google and Facebook performed a grand experiment on finding the best architecture for RNNs [16].", "startOffset": 121, "endOffset": 125}, {"referenceID": 16, "context": "Recent research shows the ability to train SRNs for long term dependencies up to 100 time steps and more using several new techniques [17], [18]\u2013[20].", "startOffset": 134, "endOffset": 138}, {"referenceID": 17, "context": "Recent research shows the ability to train SRNs for long term dependencies up to 100 time steps and more using several new techniques [17], [18]\u2013[20].", "startOffset": 140, "endOffset": 144}, {"referenceID": 19, "context": "Recent research shows the ability to train SRNs for long term dependencies up to 100 time steps and more using several new techniques [17], [18]\u2013[20].", "startOffset": 145, "endOffset": 149}, {"referenceID": 19, "context": "One of the most common methods for preventing the vanishing gradients effect from this pool is known as \u201d\u2018gradient regularization\u201d\u2019 [20], [21]; also it was independently proposed in [22] as \u201d\u2018method of pseudoregularization\u201d\u2019.", "startOffset": 132, "endOffset": 136}, {"referenceID": 20, "context": "One of the most common methods for preventing the vanishing gradients effect from this pool is known as \u201d\u2018gradient regularization\u201d\u2019 [20], [21]; also it was independently proposed in [22] as \u201d\u2018method of pseudoregularization\u201d\u2019.", "startOffset": 138, "endOffset": 142}, {"referenceID": 21, "context": "One of the most common methods for preventing the vanishing gradients effect from this pool is known as \u201d\u2018gradient regularization\u201d\u2019 [20], [21]; also it was independently proposed in [22] as \u201d\u2018method of pseudoregularization\u201d\u2019.", "startOffset": 182, "endOffset": 186}, {"referenceID": 19, "context": "In derivations below we use a framework very similar to [20] but based on studying evolution of local gradients \u03b4(k) for the backpropagation procedure.", "startOffset": 56, "endOffset": 60}, {"referenceID": 7, "context": "Now we can use an intuitive understanding of exploding/vanishing gradients problem that was deeply investigated in classic [8], [23] and modern papers [18], [20].", "startOffset": 123, "endOffset": 126}, {"referenceID": 22, "context": "Now we can use an intuitive understanding of exploding/vanishing gradients problem that was deeply investigated in classic [8], [23] and modern papers [18], [20].", "startOffset": 128, "endOffset": 132}, {"referenceID": 17, "context": "Now we can use an intuitive understanding of exploding/vanishing gradients problem that was deeply investigated in classic [8], [23] and modern papers [18], [20].", "startOffset": 151, "endOffset": 155}, {"referenceID": 19, "context": "Now we can use an intuitive understanding of exploding/vanishing gradients problem that was deeply investigated in classic [8], [23] and modern papers [18], [20].", "startOffset": 157, "endOffset": 161}, {"referenceID": 19, "context": "In [20] the power iteration method was used to formally analyze product of Jacobian matrices and obtain tight conditions for when the", "startOffset": 3, "endOffset": 7}, {"referenceID": 21, "context": "In [22] one can find an approach called \u201cpseudoregularization\u201d for forcing the recurrent neural networks to capture long-term behavior.", "startOffset": 3, "endOffset": 7}, {"referenceID": 20, "context": "Similar, but more advanced approach called \u201cgradient regularization\u201d [21] was independently proposed by [20].", "startOffset": 69, "endOffset": 73}, {"referenceID": 19, "context": "Similar, but more advanced approach called \u201cgradient regularization\u201d [21] was independently proposed by [20].", "startOffset": 104, "endOffset": 108}, {"referenceID": 23, "context": "This is a reason why using popular for DNN regularization methods like dropout doesn\u2019t work well for RNNs [24].", "startOffset": 106, "endOffset": 110}, {"referenceID": 10, "context": "We refer to [11] and [20] for descripton of experiment.", "startOffset": 12, "endOffset": 16}, {"referenceID": 19, "context": "We refer to [11] and [20] for descripton of experiment.", "startOffset": 21, "endOffset": 25}, {"referenceID": 8, "context": "They used pathological synthetic test set from [9] that requires long-term correlations.", "startOffset": 47, "endOffset": 50}, {"referenceID": 19, "context": "For description of other problems, please, see [20].", "startOffset": 47, "endOffset": 51}, {"referenceID": 10, "context": "3: An illustration of the addition problem [11], a typical problem with pathological long term dependencies.", "startOffset": 43, "endOffset": 47}, {"referenceID": 19, "context": "01 as in [20].", "startOffset": 9, "endOffset": 13}], "year": 2016, "abstractText": "Vanishing (and exploding) gradients effect is a common problem for recurrent neural networks with nonlinear activation functions which use backpropagation method for calculation of derivatives. Deep feedforward neural networks with many hidden layers also suffer from this effect. In this paper we propose a novel universal technique that makes the norm of the gradient stay in the suitable range. We construct a way to estimate a contribution of each training example to the norm of the long-term components of the target functions gradient. Using this subroutine we can construct mini-batches for the stochastic gradient descent (SGD) training that leads to high performance and accuracy of the trained network even for very complex tasks. We provide a straightforward mathematical estimation of minibatch\u2019s impact on for the gradient norm and prove its correctness theoretically. To check our framework experimentally we use some special synthetic benchmarks for testing RNNs on ability to capture long-term dependencies. Our network can detect links between events in the (temporal) sequence at the range 100 and longer.", "creator": "LaTeX with hyperref package"}}}