{"id": "1606.07786", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "23-Jun-2016", "title": "Precise deep neural network computation on imprecise low-power analog hardware", "abstract": "There is an urgent need for compact, fast, and power-efficient hardware implementations of state-of-the-art artificial intelligence. Here we propose a power-efficient approach for real-time inference, in which deep neural networks (DNNs) are implemented through low-power analog circuits. Although analog implementations can be extremely compact, they have been largely supplanted by digital designs, partly because of device mismatch effects due to fabrication of high-performance and non-failing systems. We propose a simple process for designing such algorithms. Using a high-level network, we can calculate the total computing power required in each DNN in a single direction by applying the input parameter to compute the average current current state of the DNN:\n\n\n\nThe first parameter in the DNN is the expected current state, and then the expected current state and the expected current state:\nWe estimate the state at a given time and then the current state:\nUsing a high-level network, we can use the same technique:\nUsing a high-level network, we can calculate the state at a given time and then the expected current state:\nNow we can calculate the state at a given time and then the expected current state:\nHere's the process for calculating the state at a given time:\nNow we can compute the state at a given time and then the expected current state:\nNow we can calculate the state at a given time and then the expected current state:\nNow we can compute the state at a given time and then the expected current state:\nNow we can compute the state at a given time and then the expected current state:\nNow we can compute the state at a given time and then the expected current state:\nNow we can compute the state at a given time and then the expected current state:\nNow we can compute the state at a given time and then the expected current state:\nNow we can compute the state at a given time and then the expected current state:\nThis algorithm can be used by constructing a simple neural network:\nWe use this to perform this task, but we must introduce a low-power analog circuit. The high-level network can be easily reached by the computer's system (the network does not require special software or hardware, for example). We are using the \"low-power analog circuit\" as a starting point in a low-power analog circuit, to implement the process.\nHere's the algorithm:\nOur algorithm is shown below:\nLet's", "histories": [["v1", "Thu, 23 Jun 2016 18:32:43 GMT  (533kb,D)", "http://arxiv.org/abs/1606.07786v1", null]], "reviews": [], "SUBJECTS": "cs.NE cs.LG", "authors": ["jonathan binas", "daniel neil", "giacomo indiveri", "shih-chii liu", "michael pfeiffer"], "accepted": false, "id": "1606.07786"}, "pdf": {"name": "1606.07786.pdf", "metadata": {"source": "CRF", "title": "Precise deep neural network computation on imprecise low-power analog hardware", "authors": ["Jonathan Binas", "Daniel Neil", "Giacomo Indiveri", "Shih-Chii Liu", "Michael Pfeiffer"], "emails": ["jbinas@ini.ethz.ch."], "sections": [{"heading": null, "text": "Modern information technology requires increasing computational power to process massive amounts of data in real time. This rapidly growing need for computing power has led to the exploration of computing technologies beyond the predominant von Neumann architecture. In particular, due to the separation of memory and processing elements, traditional computing systems experience a bottleneck when dealing with problems involving great amounts of highdimensional data [4, 25], such as image processing, object recognition, probabilistic inference, or speech recognition. These problems can often best be tackled by conceptually simple but powerful and highly parallel methods, such as deep neural networks (DNNs), which in recent years have delivered state-of-the-art performance on exactly those applications [29, 47]. DNNs are characterized by stereotypical and simple operations at each unit, of which many can be performed in parallel. For this reason they map favorably e.g. onto the processing style of graphics\nar X\niv :1\n60 6.\n07 78\n6v 1\n[ cs\n.N E\nprocessing units (GPUs) [46]. The large computational demands of DNNs have simultaneously sparked interest in methods that make neural network inference faster and more power efficient, whether through new algorithmic inventions [19, 21, 11], dedicated digital hardware implementations [6, 17, 8], or by taking inspiration from real nervous systems [14, 37, 33, 23, 34].\nWith synchronous digital logic being the established standard of the electronics industry, first attempts towards hardware deep network accelerators have focused on this approach [6, 18, 7, 38]. However, the massively parallel style of computation of neural networks is not reflected in the mostly serial and time-multiplexed nature of digital systems. An arguably more natural way of developing a hardware neural network emulator is to implement its computational primitives as multiple physical and parallel instances of analog computing nodes, where memory and processing elements are co-localized, and state variables are directly represented by analog currents or voltages, rather than being encoded digitally [43, 1, 49, 5, 3, 45]. By directly representing neural network operations in the physical properties of silicon transistors, such analog implementations can outshine their digital counterparts in terms of simplicity, allowing for significant advances in speed, size, and power consumption [20, 32]. The main reason why engineers have been discouraged from following this approach is that the properties of analog circuits are affected by the physical imperfections inherent to any chip fabrication process, which can lead to significant functional differences between individual devices [40].\nIn this work we propose a new approach, whereby rather than brute-force engineering more homogeneous circuits (e.g. by increasing transistor sizes and burning more power), we employ neural network training methods as an effective optimization framework to automatically compensate for the device mismatch effects of analog VLSI circuits. We use the diverse measured characteristics of individual VLSI devices as constraints in an off-line training process, to yield network configurations that are tailored to the particular analog device used, thereby compensating the inherent variability of chip fabrication. Finally, the network parameters, in particular the synaptic weights found during the training phase can be programmed in the network, and the analog circuits can be operated at run-time in the sub-threshold region for significantly lower power consumption.\nIn this article, in addition to introducing a novel training method for both device and network, we also propose compact and low-power candidate VLSI circuits. A closed-loop demonstration of the framework is shown, based on a fabricated prototype chip, as well as detailed, large-scale simulations. The resulting analog electronic neural network performs as well as an ideal network, while offering at least a threefold lower power consumption over its digital counterpart."}, {"heading": "1 Results", "text": "A deep neural network processes input signals in a number of successive layers of neurons, where each neuron computes a weighted sum of its inputs followed by a non-linearity, such as a sigmoid or rectification. Specifically, the output of a neuron i is given by xi = f (\u2211 j wijxj ) ,\nwhere f is the non-linearity, and wij is the weight of the connection from neuron j to neuron i. Thus, the basic operations comprising a neural network are summation, multiplication by scalars, and simple non-linear transformations. All of these operations can be implemented in analog electronic circuitry very efficiently, that is with very few transistors, whereby numeric values are represented by actual voltage or current values, rather than a digital code. Analog circuits are affected by fabrication mismatch, i.e. small fluctuations in the fabrication process that lead to fixed distortions of functional properties of elements on the same device, as well as multiple sources of noise. As a consequence, the response of an analog hardware neuron is slightly different for every instance of the circuit, such that xi = f\u0302i (\u2211 j wijxj ) , where f\u0302i approximately corresponds to f , but is slightly different for every neuron i."}, {"heading": "1.1 Training with heterogeneous transfer functions", "text": "The weights of multi-layered networks are typically learned from labeled training data using the backpropagation algorithm [44], which minimizes the training error by computing error gradients and passing them backwards through the layers. In order for this to work in practice, the transfer function f needs to be at least piece-wise differentiable, as is the case for the commonly used rectified linear unit (ReLU) [16]. Although it is common practice in neural network training, it is not necessary for all neurons to have identical activation functions f . In fact, having different activation functions makes no difference to backpropagation as long as their derivatives can be computed. Here this principle is exploited by inserting the heterogeneous but measured transfer curves f\u0302i from a physical analog neural network implementation into the training algorithm, with the goal of finding weight parameters that are tailored for a particular heterogeneous system given by f\u03021, . . . , f\u0302N .\nThe process of implementing a target functionality in such a heterogeneous system is illustrated in Fig. 1. Once a neural network architecture with modifiable weights is implemented in silicon, the transfer characteristics of the different neuron instances can be measured by controlling the inputs specific cells receive and recording their output at the same time (see Methods). If the transfer curves are sufficiently simple (depending on the actual implemented analog neuron circuit), a small number of discrete measurements yield sufficient information to fit a continuous, (piece-wise) differentiable model to the hardware response. For instance, the rectified linear neuron f(r) = max{0, a \u00b7 r} is fully described by a single parameter a, which is simply the ratio of output to input, and therefore can easily be measured. The continuous, parameterized description is then used by the training algorithm, which is run on traditional computing hardware, such as CPUs or GPUs, to generate a network configuration that is tailored to the particular task and the physical device that has been characterized."}, {"heading": "1.2 Analog circuit implementation", "text": "To achieve a compact and low-power solution, we construct a multilayer network using the circuits shown in Fig. 2 and operate them in the subthreshold region. The subthreshold current\nof a transistor is exponential in the gate voltage, rather than polynomial as is the case for above threshold operation, and can span many orders of magnitude. Thus, a system based on this technology can be operated at orders of magnitude lower currents than a digital one. In turn, this means that the device mismatch arising due to imperfections in the fabrication process can have an exponentially larger impact. Fortunately, as our method neither depends on the specific form nor the magnitude of the mismatch, it can handle a wide variety of mismatch conditions.\nAs a demonstration of our framework, a feed-forward network is implemented in which every neuron consists of one soma and multiple synapse circuits, then train it for different classification tasks. As illustrated in Fig. 2a, multiple layers of soma circuits are connected through matrices of synapse circuits. A soma circuit (Fig. 2b) takes a current as input and communicates its output in terms of voltages, which are passed as input signals to a row of synapse circuits. A synapse circuit (Fig. 2c), in turn, provides a current as output, such that the outputs of a column of synapses can be summed up simply by connecting them together. The resulting current is then fed as an input current to the somata of the next layer. The first transistor\nof the soma circuit rectifies the input current. The remaining elements of the soma circuit, together with a connected synapse circuit, form a set of scaling current mirrors, i.e. rudimentary amplifiers, a subset of which can be switched on or off to achieve a particular weight value by setting the respective synapse configuration bits. Thus, the output of a synapse corresponds to a scaled version of the rectified input current of the soma, similar to the ReLU transfer function. In our proposed example implementation we use signed 3-bit synapses, which are based on 2\u00d73 current mirrors of different dimensions (3 for positive and 3 for negative values). One of 24 possible weight values is then selected by switching the respective current mirrors on or off. The scaling factor of a particular current mirror, and thus its contribution to the total weight value, is proportional to the ratio of the widths of the two transistors forming it. The weight configuration of an individual synapse can be stored digitally in memory elements that are part of the actual synapse circuit. Thus, in contrast to digital processing systems, our circuit computes in memory and thereby avoids the bottleneck of expensive data transfer between memory and processing elements.\nAlthough this is just one out of many possible analog circuits implementations, the simple circuits chosen offer several advantages besides the fact that they can be implemented in small areas: First, numeric values are conveyed only through current mirrors, and therefore are temperature-independent. Second, most of the fabrication-induced variability is due to the devices in the soma with five consecutive transistors, whereas only one layer of transistors affects the signal in the synapse. This means that the synapse-induced mismatch can be neglected in a first order approximation.\nOnce an analog electronic neural network has been implemented physically as a VLSI device, the transfer characteristics of the individual soma circuits are obtained through measurements. The transfer function implemented by our circuits can be well described by a rectified linear curve, where the only free parameter is the slope, and thus can be determined from a single measurement per neuron. Specifically, the transfer curves of all neurons in a layer k can be measured through a simple procedure: A single neuron in layer k \u2212 1 is connected, potentially through some intermediate neurons, to the input layer and is defined to be the \u2018source\u2019. Similarly, a neuron in layer k+1 is connected, potentially through intermediate neurons, to the output layer and is called the \u2018monitor\u2019. All neurons of layer k can now be probed individually using the source and monitor neurons, whereby the signal to the input layer is held fixed and the signal recorded at the output layer is proportional to the slope of the measured neuron. Note that the absolute scale of the responses is not relevant, i.e. only the relative scale within one layer matters, as the output of individual layers can be scaled arbitrarily without altering the network function. The same procedure can be applied to all layers to obtain a complete characterization of the network. The measurements can be parallelized by defining multiple source and monitor neurons per measurement to probe several neurons in one layer simultaneously, or by introducing additional readout circuitry between layers to measure multiple layers simultaneously."}, {"heading": "1.3 Handwritten and spoken digit classification", "text": "Large-scale SPICE simulations of systems consisting of hundreds of thousands of transistors are employed to assess power consumption, processing speed, and the accuracy of such an analog implementation.\nAfter simulating measurements and parameterizing the transfer characteristics of the circuits as described previously, software networks were trained on the MNIST dataset of handwritten digits [30] and the TIDIGITS dataset of spoken digits [31] by means of the ADAM training method [27]. In order to optimize the network for the use of discrete weights in the synaptic circuits dual-copy rounding [48, 10] was used (see Methods). By evaluating the responses of the simulated circuit on subsets of the respective test sets, its classification accuracy was found to be comparable to the abstract software neural network (see Tab. 1 for comparison). Fig. 3 shows how inputs are processed by a small example circuit implementing a 196 \u2212 50 \u2212 10 network, containing around 10k synapses and over 100k transistors. Starting with the presentation of an input pattern in the top layer, where currents are proportional to input stimulus intensity, the higher layers react almost instantaneously and provide the correct classification, i.e. the index of the maximally active output unit, within a few microseconds. After a switch of input patterns, the signals quickly propagate through the network and the outputs of different nodes converge to their asymptotic values. The time it takes the circuit to converge to its final output defines the\n\u2018time to output\u2019, constraining the maximum frequency at which input patterns can be presented and evaluated correctly. Measured convergence times are summarized in Fig. 4 for different patterns from the MNIST test set, and are found to be in the range of microseconds for a trained 196\u2212 100\u2212 50\u2212 10 network, containing over 25k synapses and around 280k transistors. Note that observed timescale is not fixed as the network can be run faster or slower by changing the input current, while the average energy dissipated per operation remains roughly constant.\nThe processing efficiency of the system (energy per operation) was computed for different input patterns by integrating the power dissipated between the time at which the input pattern\nwas switched and the time to output. Fig. 4 shows the processing efficiency for the same network with different input examples and under different operating currents. With the average input currents scaled to either 15 or 45 nA per neuron respectively, the network takes several microseconds to converge and consumes tens or hundreds of microwatts in total, which amounts to a few nanowatts per multiply-accumulate operation. With the supply voltage set to 1.8 V, this corresponds to less than 0.1 pJ per operation in most cases. With the average input current set to 15 nA per neuron, the network produces the correct output within 15\u00b5s in over 99 % of all cases (mean 8.5\u00b5s; std. 2.3\u00b5s). Running the circuit for 15\u00b5s requires 0.12\u00b1 0.01 pJ per operation, such that about 1.7 trillion multiply-accumulate operations can be computed per second at a power budget of around 200\u00b5W if input patterns are presented at a rate of 66 kHz. Without major optimizations to either process or implementation, this leads to an efficiency of around 8 TOp/J, to our knowledge a performance at least four times greater than that achieved by digital single-purpose neural network accelerators in similar scenarios [6, 38]. General purpose digital systems are far behind such specialized systems in terms of efficiency, with the latest GPU generation achieving around 0.05 TOp/J [36].\nTab. 1 summarizes the classification accuracy for different architectures and datasets for a software simulation of an ideal network without mismatch, a behavioral simulation of the inhomogeneous system with the parameterized transfer curves implemented in an abstract software model, and the full circuit simulation of the inhomogeneous hardware network. Additionally, the computed power efficiency is shown for the different architectures."}, {"heading": "1.4 VLSI implementation", "text": "As a closed-loop demonstration of our framework, we designed a prototype VLSI chip and trained it for a classification task. A design based on the circuits shown in Fig. 2, containing\nthree layers of seven neurons each, was fabricated in 180 nm CMOS technology. After characterizing the individual neuron circuits through measurements as described in Sect. 1.2 we trained a 4 \u2212 7 \u2212 3 network on 80 % of the Iris flower dataset [15], programmed the device with the found parameters, and used the remaining 20 % of the data to test the classification performance. The hardware implementation was able to classify 100% of the test data correctly (see Fig. 5e for the output of the network)."}, {"heading": "2 Discussion", "text": "The theory of analog neural networks and electronic realizations thereof have a substantial history that goes back to the1950s [43, 1]. However, the demonstrated accuracy of the electronic networks is typically below the theoretical performance and therefore, their full low-power potential was never fully leveraged.\nInstead, digital designs have flourished in the interim and almost all current deep network designs are implemented in digital form [6, 7, 38]. Although small transistors are possible\nin digital implementations, the typical size of a multiplier-accumulator (MAC) block usually means that these implementations use a smaller subset of functional blocks and therefore the use of MACs is time-multiplexed by shifting data around accordingly. As a consequence, the processing speed of digital implementations is limited by their clock frequency.\nThe simplicity of the analog VLSI circuits needed for addition - namely connecting together wires - allows an explicit implementation of each processing unit or neuron where no element is shared or time-multiplexed within the network implementation. The resulting VLSI network is maximally parallel and eliminates the bottleneck of transferring data between memory and processing elements. Using digital technology, such fully parallel implementations would quickly become prohibitively large due to the much greater circuit complexity of digital processing elements. While the focus in this work has been on an efficient analog VLSI implementation, hardware implementations using new forms of nano devices can also benefit from this training method. For example, the memristive computing technology which is currently being pursued for implementing large-scale cognitive neuromorphic and other technologies still suffers from the mismatch of fabricated devices [2, 26, 41]. The proposed training method in this work can be used to account for device non-idealities in this technology [35].\nIn fact, any system that can be properly characterized and has configurable elements stands to benefit from this approach. For example, spike-based neuromorphic systems [24] often have configurable weights between neurons. These systems communicate via biologically inspired digital-like pulses called spikes. Similar to the method outlined in this work, the relationship between an input spike rate and an output spike rate of a neuron can be measured in such a system, and the transfer functions then used as a constraint during the training process so as to achieve accurate results from the whole network even if the neuron circuits themselves are varied and non-ideal. In addition to the alternate hardware implementations, other network topologies such as convolutional networks can be trained using this proposed method. However, as all weights are implemented explicitly in silicon, the system design here would not benefit from the small memory footprint achieved via weight sharing in traditional convolutional network implementations. In principle, even recurrent architectures such as LSTM networks [22] can be trained using the same methods, where not only the static properties of the circuit are taken into account but also their dynamics.\nWith every device requiring an individual training procedure, an open question is how the per-device training time can be reduced. Initializing the network to a pre-trained ideal network, which is then fine-tuned for the particular devices is likely to reduce training time.\nIn the current setting, the efficiency of our system is limited by the worst-case per-example runtime, i.e. there may be a few samples where outputs require significantly longer to converge to the correct classification result than the majority. This can lead to unnecessarily long presentation times for many samples, thereby causing unnecessary power consumption. Smart methods of estimating presentation times from the input data could e.g. accelerate convergence for slowly converging samples by using higher input currents, and conversely, faster samples could be slowed down to lower the variability of convergence times and overall reduce energy consumption. Future research will focus on such estimators, and alternatively explore ways of\nreducing convergence time variability during network training. This proof-of-principle study is an important step towards the construction of large scale, possibly ultra-low-power analog VLSI deep neural network processors, paving the way for specialized applications which had not been feasible before due to speed or power constraints. Small, efficient implementations could allow autonomous systems to achieve almost immediate reaction times under strict power limitations. Scaled-up versions can allow for substantially more efficient processing in data centers, allowing for a greatly reduced energy footprint or permitting substantially more data to be effectively processed. Conversely, digital approaches and GPU technology are aiming for general purpose deep network acceleration, and thus naturally have an advantage in terms of flexibility compared to the fixed physical implementation of the proposed analog devices. However, there is increasing evidence that neural networks pretrained on large datasets such as ImageNet provide excellent generic feature detectors [13, 42], which means that fast and efficient analog input pre-processors could be used as an important building blocks for a large variety of applications."}, {"heading": "3 Methods", "text": ""}, {"heading": "3.1 Description of the example circuit", "text": "The example networks described in Sect. 1.2 have been implemented based on the circuits shown in Fig. 2. With M0 as a diode-connected nFET, the soma circuit essentially performs a rectification of the input current Iin. Further, the current is copied to M1 and, through M2 and M3, also to M4, such that M2 together with pFETs from connected synapse circuits, as well as M4 together with nFETs from connected synapse circuits form scaling current mirrors, generating scaled copies of the rectified input current Iin. The scaling factor is thereby determined by the dimensions of M10 to M15. The transistors M16 to M20 operate as switches and are controlled by the digital signals w\u00b1 w0, w1, and w2. The value of w\u00b1 determines whether the positive branch (pFETs M13 to M15; adding current to the node Iout) or the negative branch (nFETs M10 to M12; subtracting current from the node Iout) is switched on and thereby the sign of the synaptic multiplication factor. Setting w0, w1, and w2 allows switching on or off specific contributions to the output current. In the example implementation the widths of M10 to M12, and M13 to M15, respectively, were scaled by powers of 2 (see Tab. 2), such that a synapse would implement a multiplication by a factor approximately corresponding to the binary value of (w0, w1, w2). While our results are based on a signed 3-bit version of the circuit, arbitrary precision can be implemented by changing the number of scaling transistors and corresponding switches. The dimensions of M3 and M4 were adjusted such that the currents through transistors of the positive and the negative branch of one particular bit of a synapse were roughly matched when switched on.\nMultilayer networks were constructed using the circuits described above by connecting layers of soma circuits through matrices made up of synapse circuits. The first stage of a network constructed in this way thereby is a layer of soma circuits, rather than a weight matrix, as is typically the case in artificial neural network implementations. This is because we prefer to provide input currents rather than voltages and only soma circuits take currents as inputs. As a consequence, due to the rectification, our network can not handle negative input signals. To obtain current outputs rather than voltages, one synapse is connected to each unit of the output layer and its weight set to 1 to convert the output voltages to currents."}, {"heading": "3.2 Circuit simulation details", "text": "All circuits were simulated using NGSPICE release 26 and BSIM3 version 3.3.0 models of a TSMC 180 nm process. The SPICE netlist for a particular network was generated using custom Python software and then passed to NGSPICE for DC and transient simulations. Input patterns were provided to the input layer by current sources fixed to the respective values. The parameters from Tab. 2 were used in all simulations and Vdd was set to 1.8 V. Synapses were configured by setting their respective configuration bits w\u00b1, w0, w1, and w2 to either Vdd or ground, emulating a digital memory element. The parasitic capacitances and resistances to be found in an implementation of our circuits were estimated from post-layout simulations of single soma and synapse cells. The main slowdown of the circuit can be attributed to the parasitic capacitances of the synapses, which were found to amount to 11 fF per synapse.\nIndividual hardware instances of our system were simulated by randomly assigning small deviations to all transistors of the circuit. Since the exact nature of mismatch is not relevant for our main result (our training method compensates for any kind of deviation, regardless of its cause), the simple but common method of threshold matching was applied to introduce deviceto-device deviations [28]. Specifically, for every device, a shift in threshold voltage was drawn from a Gaussian distribution with zero mean and standard deviation \u03c3\u2206V T = AV T/ \u221a W/L, where the proportionality constant AV T was set to 3.3 mV\u00b5m, approximately corresponding to measurements from a 180 nm process [39].\n3.3 Characterization of the simulated circuit\nTo determine the transfer curves of individual neurons, the input-output relations of the respective soma circuits need to be measured. To save simulation time, a parallel measurement scheme was applied, based on the assumption that each neuron can be measured directly, rather than just the neurons in the output layer. Rather than measuring the log domain output voltages Vn and Vp we chose to record the input currents Iin to subsequent layers. The advantages of this approach are that quantities are not log-transformed and that potential distortions arising from the synapse circuits are taken into account. Furthermore, with this method only one probe is required per neuron, rather than two separate ones for in- and output signals. Moreover, the unit weight of a synapse (which is not know a priori) here becomes a property of the soma, so that weights are automatically normalized. To determine the transfer curves of the units in the different layers the weights were set to a number of different configurations and the input currents to the various units were measured for different input patterns provided to the network. Specifically, by setting the respective synapse circuits to their maximum value, every unit was configured to receive input from exactly one unit of the previous layer. One such configuration is shown in Fig. 6. The input currents to all units of the input layer were then set to the same value and the inputs to the units of the deeper layers were recorded. By generating many such connectivity patterns by permuting the connectivity matrix, and setting the input currents to different values, multiple data points (input-output relations) were recorded for each unit, such that continuous transfer curves could be fitted to the data. For the example networks described in Sect. 1.2, 40 measurements turned out to be sufficient, resulting in roughly 10 data points per unit. Rectified linear functions f(r) = max{0, a \u00b7 r} were fitted to the data and the resulting parameters a were used as part of the training algorithm. The parameters were normalized layer-wise to a mean slope of 1. Even though the sizes of the transistors implementing the positive and negative weight contributions are identical, their responses are not matched. To characterize their relative contributions, inputs were given to neurons through positive and negative connections simultaneously. Comparing the neuron response to its response with the negative connection switched off allows to infer the strength of the unit negative weight, which can then be used in the training algorithm."}, {"heading": "3.4 Training and evaluation details", "text": "The 196 \u2212 100 \u2212 50 \u2212 10 networks were trained on the MNIST and TIDIGITS datasets using the ADAM optimizer [27] and the mean squared error as loss function. The low-precision training (three signed bits per synapse) was done using a high-precision store and low-precision activations in the manner of the method simultaneously described in [48, 10]. An L1 regularization scheme was applied to negative weights only to reduce the number of negative inputs to neurons, as they would slow down the circuits. The Keras software toolkit [9] was used to perform the training. A custom layer consisting of the parameterized activation function f(x) = max{0, a \u00b7Wx} , using the extracted parameter a was added and used to model the neuron activation function.\nDifferent sets of empirically found hyperparameters were used during training for the MNIST\nand TIDIGITS datasets. A reduced resolution version (14\u00d7 14 pixels) of the MNIST dataset was generated by identifying the 196 most active pixels (highest average value) in the dataset and only using those as input to the network. The single images were normalized to a mean pixel value of 0.04. The learning rate was set to 0.0065, the L1 penalty for negative weights was set to 10\u22126, and the networks were trained for 50 epochs with batch sizes of 200.\nEach spoken digit of the TIDIGITS dataset was converted to 12 mel-spectrum cepstral coefficients (MFCCs) per time slice, with a maximum frequency of 8 kHz and a minimum frequency of 0 kHz, using 2048 FFT points and a skip duration of 1536 samples. To convert the variablelength TIDIGITS data to a fixed-size input, the input was padded to a maximum length of 11 time slices, forming a 12x11 input for each digit. First derivative and second derivatives of the MFCCs were not used. To increase robustness, a stretch factor was applied, changing the skip duration of the MFCCs by a factor of 0.8, 0.9, 1.0, 1.1, and 1.3, allowing fewer or more columns of data per example, as this was found to increase accuracy and model robustness. A selection of hyperparameters for the MFCCs were evaluated, with these as the most successful. The resulting dataset was scaled pixel-wise to values between 0 and 1. Individual samples were then scaled to yield a mean value of 0.03. The networks were trained for 512 epochs on batches of size 200 with the learning rate set to 0.0073, and the L1 penalty to 10\u22126."}, {"heading": "3.5 Performance measurements", "text": "The accuracy of the abstract software model was determined after training by running the respective test sets through the network. Due to prohibitively long simulation times, only subsets of the respective test sets were used to determine the accuracy of the SPICE-simulated circuits. Specifically, the first 500 samples of the MNIST test set and 500 randomly picked samples from the TIDIGITS test set were used to obtain an estimate of the classification accuracy of the simulated circuits. The data was presented to the networks in terms of currents, by connecting current sources to the Iin nodes of the input layer. Individual samples were scaled to yield mean input currents of 15 nA or 45 nA per pixel, respectively. The time to output for a particular pattern was computed by applying one (random) input pattern from the test set and then, once the circuit had converged to a steady state, replaced by the input pattern to be tested. In this way, the more realistic scenario of a transition between two patterns is simulated, rather than a \u2018switching on\u2019 of the circuit. The transient analysis was run for 7\u00b5s and 15\u00b5s with the mean input strength set to 45 nA and 15 nA, respectively, and a maximum step size of 20 ns. At any point in time, the output class of the network was defined as the index of the output layer unit that was the most active. The time to output for each pair of input patterns was determined by checking at which time the output class of the network corresponded to its asymptotic state (determined through an operating point analysis of the circuit with the input pattern applied) and would not change anymore. The energy consumed by the network in a period of time was computed by integrating the current dissipated by the circuit over the decision time and multiplying it by the value of Vdd (1.8 V in all simulations)."}, {"heading": "3.6 VLSI prototype implementation", "text": "A 7\u22127\u22127 network, consisting of 21 neurons and 98 synapses was fabricated in 180 nm CMOS technology (AMS 1P6M). The input currents were provided through custom bias generators, optimized for sub-threshold operation [12]. Custom current-to-frequency converters were used to read out the outputs of neurons and send them off chip in terms of inter-event intervals. The weight parameters were stored on the device in latches, directly connected to the configuration lines of the synapse circuits. Custom digital logic was implemented on the chip for programming biases, weights, and monitors. Furthermore, the chip was connected to a PC, through a Xilinx Spartan 6 FPGA containing custom interfacing logic and a Cypress FX2 device providing a USB interface. Custom software routines were implemented to communicate with the chip and carry out the experiments. The fabricated VLSI chip was characterized through measurements as described in Sect. 1.2, by probing individual neurons one by one. The measurements were repeated several times through different source and monitor neurons for each neuron to be characterized to average out mismatch effects arising from the synapse or readout circuits. The mean values of the measured slopes were used in a software model to train a network on the Iris flower dataset. The Iris dataset was randomly split into 120 and 30 samples used for training and testing, respectively. The resulting weight parameters were programmed into the chip and individual samples of the dataset were presented to the network in terms of currents scaled to values between 0 and 325 nA. The index of the maximally active output unit was used as the output label of the network and to compute the classification accuracy."}], "references": [{"title": "A neuromorphic VLSI learning system", "author": ["J. Alspector", "R.B. Allen"], "venue": "P. Losleben, editor, Proceedings of the 1987 Stanford Conference on Advanced Research in VLSI, pages 313\u2013 349, Cambridge, MA, USA,", "citeRegEx": "1", "shortCiteRegEx": null, "year": 1987}, {"title": "Spike-timing dependent plasticity in a transistor-selected resistive switching memory", "author": ["S Ambrogio", "S Balatti", "F Nardi", "S Facchinetti", "D Ielmini"], "venue": "Nanotechnology, 24(38):384012,", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2013}, {"title": "Current-mode subthreshold MOS circuits for analog VLSI neural systems", "author": ["Andreas G Andreou", "Kwabena Boahen", "Philippe O Pouliquen", "Aleksandra Pavasovic", "Robert E Jenkins", "Kim Strohbehn"], "venue": "IEEE Transactions on neural networks,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 1991}, {"title": "Can programming be liberated from the von neumann style?: A functional style and its algebra of programs", "author": ["John Backus"], "venue": "Commun. ACM,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 1978}, {"title": "Programmable current-mode neural network for implementation in analogue MOS VLSI", "author": ["T.H. Borgstrom", "M Ismail", "S.B. Bibyk"], "venue": "IEE Proceedings G, 137(2):175\u2013184,", "citeRegEx": "5", "shortCiteRegEx": null, "year": 1990}, {"title": "Origami: A convolutional network accelerator", "author": ["Lukas Cavigelli", "David Gschwend", "Christoph Mayer", "Samuel Willi", "Beat Muheim", "Luca Benini"], "venue": "In Proceedings of the 25th edition on Great Lakes Symposium on VLSI,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2015}, {"title": "eyeriss: An energyefficient reconfigurable accelerator for deep convolutional neural networks", "author": ["Yu-Hsin Chen", "Tushar Krishna", "Joel Emer", "Vivienne Sze"], "venue": "IEEE International Solid-State Circuits Conference (ISSCC),", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2016}, {"title": "Dadiannao: A machine-learning supercomputer", "author": ["Yunji Chen", "Tao Luo", "Shaoli Liu", "Shijin Zhang", "Liqiang He", "Jia Wang", "Ling Li", "Tianshi Chen", "Zhiwei Xu", "Ninghui Sun"], "venue": "In Microarchitecture,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2014}, {"title": "Low precision arithmetic for deep learning", "author": ["Matthieu Courbariaux", "Yoshua Bengio", "Jean-Pierre David"], "venue": "arXiv preprint arXiv:1412.7024,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2014}, {"title": "Binaryconnect: Training deep neural networks with binary weights during propagations", "author": ["Matthieu Courbariaux", "Yoshua Bengio", "Jean-Pierre David"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2015}, {"title": "32-bit configurable bias current generator with sub-off-current capability", "author": ["Tobi Delbruck", "Raphael Berner", "Patrick Lichtsteiner", "Carlos Dualibe"], "venue": "In Proceedings of 2010 IEEE International Symposium on Circuits and Systems,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2010}, {"title": "DeCAF: A deep convolutional activation feature for generic visual recognition", "author": ["Jeff Donahue", "Yangqing Jia", "Oriol Vinyals", "Judy Hoffman", "Ning Zhang", "Eric Tzeng", "Trevor Darrell"], "venue": "In ICML,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2014}, {"title": "Comparison between frame-constrained fix-pixel-value and frame-free spiking-dynamic-pixel convnets for visual processing", "author": ["Cl\u00e9ment Farabet", "R Paz-Vicente", "JA P\u00e9rez-Carrasco", "Carlos Zamarre\u00f1o-Ramos", "Alejandro Linares-Barranco", "Yann LeCun", "Eugenio Culurciello", "Teresa Serrano-Gotarredona", "Bernabe Linares-Barranco"], "venue": "Frontiers in Neuroscience,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2012}, {"title": "Deep sparse rectifier neural networks", "author": ["Xavier Glorot", "Antoine Bordes", "Yoshua Bengio"], "venue": "In International Conference on Artificial Intelligence and Statistics,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2011}, {"title": "A 240 g-ops/s mobile coprocessor for deep neural networks", "author": ["Vinayak Gokhale", "Jonghoon Jin", "Aysegul Dundar", "Ben Martini", "Eugenio Culurciello"], "venue": "In Computer Vision and Pattern Recognition Workshops (CVPRW),", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2014}, {"title": "An 11-million transistor neural network execution engine", "author": ["Matthew Griffin", "Gary Tahara", "Kurt Knorpp", "Ray Pinkham", "Bob Riley"], "venue": "In Solid-State Circuits Conference,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 1991}, {"title": "Deep compression: Compressing deep neural networks with pruning, trained quantization and huffman coding", "author": ["Song Han", "Huizi Mao", "William J Dally"], "venue": "arXiv preprint arXiv:1510.00149,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2015}, {"title": "Finding a roadmap to achieve large neuromorphic hardware systems", "author": ["Jennifer Hasler", "Bo Marr"], "venue": "Frontiers in neuroscience,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2013}, {"title": "Distilling the knowledge in a neural network", "author": ["Geoffrey Hinton", "Oriol Vinyals", "Jeff Dean"], "venue": "arXiv preprint arXiv:1503.02531,", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2015}, {"title": "Long short-term memory", "author": ["Sepp Hochreiter", "J\u00fcrgen Schmidhuber"], "venue": "Neural computation,", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 1997}, {"title": "Neuromorphic architectures for spiking deep neural networks", "author": ["Giacomo Indiveri", "Federico Corradi", "Ning Qiao"], "venue": "In IEEE International Electron Devices Meeting (IEDM),", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2015}, {"title": "Neuromorphic silicon neuron circuits", "author": ["Giacomo Indiveri", "Bernabe Linares-Barranco", "Tara Julia Hamilton", "Andr\u00e9 van Schaik", "Ralph Etienne-Cummings", "Tobi Delbruck", "Shih-Chii Liu", "Piotr Dudek", "Philipp H\u00e4fliger", "Sylvie Renaud", "Johannes Schemmel", "Gert Cauwenberghs", "John Arthur", "Kai Hynna", "Fopefolu Folowosele", "Sylvain SA\u00cfGHI", "Teresa Serrano-Gotarredona", "Jayawan Wijekoon", "Yingxue Wang", "Kwabena Boahen"], "venue": "Frontiers in Neuroscience,", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2011}, {"title": "Memory and information processing in neuromorphic systems", "author": ["Giacomo Indiveri", "Shih-Chii Liu"], "venue": "Proceedings of the IEEE,", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2015}, {"title": "A functional hybrid memristor crossbar-array/CMOS system for data storage and neuromorphic applications", "author": ["Kuk-Hwan Kim", "Siddharth Gaba", "Dana Wheeler", "Jose M Cruz-Albrecht", "Tahir Hussain", "Narayan Srinivasa", "Wei Lu"], "venue": "Nano letters,", "citeRegEx": "26", "shortCiteRegEx": "26", "year": 2011}, {"title": "Adam: A method for stochastic optimization", "author": ["Diederik Kingma", "Jimmy Ba"], "venue": "arXiv preprint arXiv:1412.6980,", "citeRegEx": "27", "shortCiteRegEx": "27", "year": 2014}, {"title": "Characterisation and modeling of mismatch in MOS transistors for precision analog design", "author": ["Kadaba R Lakshmikumar", "Robert Hadaway", "Miles Copeland"], "venue": "IEEE Journal of Solid-State Circuits,", "citeRegEx": "28", "shortCiteRegEx": "28", "year": 1986}, {"title": "A high-speed analog neural processor", "author": ["Peter Masa", "Klaas Hoen", "Hans Wallinga"], "venue": "Micro, IEEE,", "citeRegEx": "32", "shortCiteRegEx": "32", "year": 1994}, {"title": "A million spiking-neuron integrated circuit with a scalable communication network and interface", "author": ["Paul A Merolla", "John V Arthur", "Rodrigo Alvarez-Icaza", "Andrew S Cassidy", "Jun Sawada", "Filipp Akopyan", "Bryan L Jackson", "Nabil Imam", "Chen Guo", "Yutaka Nakamura", "Bernard Brezzo", "Ivan Vo", "Steven K Esser", "Rathinakumar Appuswamy", "Brian Taba", "Arnon Amir", "Myron D Flickner", "William P Risk", "Rajit Manohar", "Dharmendra S Modha"], "venue": "Science, 345(6197):668\u2013673,", "citeRegEx": "33", "shortCiteRegEx": "33", "year": 2014}, {"title": "Learning to be efficient: Algorithms for training low-latency, low-compute deep spiking neural networks", "author": ["Daniel Neil", "Michael Pfeiffer", "Shih-Chii Liu"], "venue": "In ACM Symposium on Applied Computing,", "citeRegEx": "34", "shortCiteRegEx": "34", "year": 2016}, {"title": "Impact of process variations on emerging memristor", "author": ["Dimin Niu", "Yiran Chen", "Cong Xu", "Yuan Xie"], "venue": "In Design Automation Conference (DAC),", "citeRegEx": "35", "shortCiteRegEx": "35", "year": 2010}, {"title": "Realtime classification and sensor fusion with a spiking deep belief network", "author": ["Peter O\u2019Connor", "Daniel Neil", "Shih-Chii Liu", "Tobi Delbruck", "Michael Pfeiffer"], "venue": "Frontiers in Neuromorphic Engineering,", "citeRegEx": "37", "shortCiteRegEx": "37", "year": 2013}, {"title": "An energy-efficient and scalable deep learning/inference processor with tetra-parallel MIMD architecture for big data applications", "author": ["SW Park", "J Park", "K Bong", "D Shin", "J Lee", "S Choi", "HJ Yoo"], "venue": "IEEE transactions on biomedical circuits and systems,", "citeRegEx": "38", "shortCiteRegEx": null, "year": 2016}, {"title": "Transistor matching in analog CMOS applications", "author": ["Marcel JM Pelgrom", "Hans P Tuinhout", "Maarten Vertregt"], "venue": "IEDM Tech. Dig, pages 915\u2013918,", "citeRegEx": "39", "shortCiteRegEx": "39", "year": 1998}, {"title": "Welbers. Matching properties of MOS transistors", "author": ["M.J.M. Pelgrom", "Aad C.J. Duinmaijer", "A.P.G"], "venue": "IEEE Journal of Solid-State Circuits,", "citeRegEx": "40", "shortCiteRegEx": "40", "year": 1989}, {"title": "Training and operation of an integrated neuromorphic network based on metal-oxide memristors", "author": ["Mirko Prezioso", "Farnood Merrikh-Bayat", "BD Hoskins", "GC Adam", "Konstantin K Likharev", "Dmitri B Strukov"], "venue": null, "citeRegEx": "41", "shortCiteRegEx": "41", "year": 2015}, {"title": "CNN features off-the-shelf: an astounding baseline for recognition", "author": ["Ali Sharif Razavian", "Hossein Azizpour", "Josephine Sullivan", "Stefan Carlsson"], "venue": "In Proc. of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR) Workshops,", "citeRegEx": "42", "shortCiteRegEx": "42", "year": 2014}, {"title": "The perceptron: a probabilistic model for information storage and organization in the brain", "author": ["F. Rosenblatt"], "venue": "Psychological review, 65(6):386\u2013408, nov", "citeRegEx": "43", "shortCiteRegEx": null, "year": 1958}, {"title": "Parallel distributed processing: Explorations in the microstructure of cognition, vol", "author": ["D.E. Rumelhart", "G.E. Hinton", "R.J. Williams"], "venue": "1. chapter Learning Internal Representations by Error Propagation, pages 318\u2013362. MIT Press, Cambridge, MA, USA,", "citeRegEx": "44", "shortCiteRegEx": null, "year": 1986}, {"title": "A reconfigurable VLSI neural network", "author": ["S. Satyanarayana", "Y.P. Tsividis", "H.P. Graf"], "venue": "IEEE Journal of Solid-State Circuits, 27(1):67\u201381, Jan", "citeRegEx": "45", "shortCiteRegEx": null, "year": 1992}, {"title": "Accelerating large-scale convolutional neural networks with parallel graphics multiprocessors", "author": ["Dominik Scherer", "Hannes Schulz", "Sven Behnke"], "venue": "In Artificial Neural Networks\u2013ICANN", "citeRegEx": "46", "shortCiteRegEx": "46", "year": 2010}, {"title": "Deep learning in neural networks: An overview", "author": ["J. Schmidhuber"], "venue": "Neural Networks, 61:85\u2013 117, January", "citeRegEx": "47", "shortCiteRegEx": null, "year": 2015}, {"title": "Robustness of spiking deep belief networks to noise and reduced bit precision of neuro-inspired hardware platforms", "author": ["Evangelos Stromatias", "Daniel Neil", "Michael Pfeiffer", "Francesco Galluppi", "Steve B Furber", "Shih-Chii Liu"], "venue": "Frontiers in neuroscience,", "citeRegEx": "48", "shortCiteRegEx": "48", "year": 2015}, {"title": "Analog VLSI implementation of neural networks", "author": ["E.A. Vittoz"], "venue": "Proc. IEEE Int. Symp. Circuit and Systems, pages 2524\u20132527, New Orleans,", "citeRegEx": "49", "shortCiteRegEx": null, "year": 1990}], "referenceMentions": [{"referenceID": 3, "context": "In particular, due to the separation of memory and processing elements, traditional computing systems experience a bottleneck when dealing with problems involving great amounts of highdimensional data [4, 25], such as image processing, object recognition, probabilistic inference, or speech recognition.", "startOffset": 201, "endOffset": 208}, {"referenceID": 22, "context": "In particular, due to the separation of memory and processing elements, traditional computing systems experience a bottleneck when dealing with problems involving great amounts of highdimensional data [4, 25], such as image processing, object recognition, probabilistic inference, or speech recognition.", "startOffset": 201, "endOffset": 208}, {"referenceID": 40, "context": "These problems can often best be tackled by conceptually simple but powerful and highly parallel methods, such as deep neural networks (DNNs), which in recent years have delivered state-of-the-art performance on exactly those applications [29, 47].", "startOffset": 239, "endOffset": 247}, {"referenceID": 39, "context": "processing units (GPUs) [46].", "startOffset": 24, "endOffset": 28}, {"referenceID": 16, "context": "The large computational demands of DNNs have simultaneously sparked interest in methods that make neural network inference faster and more power efficient, whether through new algorithmic inventions [19, 21, 11], dedicated digital hardware implementations [6, 17, 8], or by taking inspiration from real nervous systems [14, 37, 33, 23, 34].", "startOffset": 199, "endOffset": 211}, {"referenceID": 18, "context": "The large computational demands of DNNs have simultaneously sparked interest in methods that make neural network inference faster and more power efficient, whether through new algorithmic inventions [19, 21, 11], dedicated digital hardware implementations [6, 17, 8], or by taking inspiration from real nervous systems [14, 37, 33, 23, 34].", "startOffset": 199, "endOffset": 211}, {"referenceID": 9, "context": "The large computational demands of DNNs have simultaneously sparked interest in methods that make neural network inference faster and more power efficient, whether through new algorithmic inventions [19, 21, 11], dedicated digital hardware implementations [6, 17, 8], or by taking inspiration from real nervous systems [14, 37, 33, 23, 34].", "startOffset": 199, "endOffset": 211}, {"referenceID": 5, "context": "The large computational demands of DNNs have simultaneously sparked interest in methods that make neural network inference faster and more power efficient, whether through new algorithmic inventions [19, 21, 11], dedicated digital hardware implementations [6, 17, 8], or by taking inspiration from real nervous systems [14, 37, 33, 23, 34].", "startOffset": 256, "endOffset": 266}, {"referenceID": 14, "context": "The large computational demands of DNNs have simultaneously sparked interest in methods that make neural network inference faster and more power efficient, whether through new algorithmic inventions [19, 21, 11], dedicated digital hardware implementations [6, 17, 8], or by taking inspiration from real nervous systems [14, 37, 33, 23, 34].", "startOffset": 256, "endOffset": 266}, {"referenceID": 7, "context": "The large computational demands of DNNs have simultaneously sparked interest in methods that make neural network inference faster and more power efficient, whether through new algorithmic inventions [19, 21, 11], dedicated digital hardware implementations [6, 17, 8], or by taking inspiration from real nervous systems [14, 37, 33, 23, 34].", "startOffset": 256, "endOffset": 266}, {"referenceID": 12, "context": "The large computational demands of DNNs have simultaneously sparked interest in methods that make neural network inference faster and more power efficient, whether through new algorithmic inventions [19, 21, 11], dedicated digital hardware implementations [6, 17, 8], or by taking inspiration from real nervous systems [14, 37, 33, 23, 34].", "startOffset": 319, "endOffset": 339}, {"referenceID": 30, "context": "The large computational demands of DNNs have simultaneously sparked interest in methods that make neural network inference faster and more power efficient, whether through new algorithmic inventions [19, 21, 11], dedicated digital hardware implementations [6, 17, 8], or by taking inspiration from real nervous systems [14, 37, 33, 23, 34].", "startOffset": 319, "endOffset": 339}, {"referenceID": 27, "context": "The large computational demands of DNNs have simultaneously sparked interest in methods that make neural network inference faster and more power efficient, whether through new algorithmic inventions [19, 21, 11], dedicated digital hardware implementations [6, 17, 8], or by taking inspiration from real nervous systems [14, 37, 33, 23, 34].", "startOffset": 319, "endOffset": 339}, {"referenceID": 20, "context": "The large computational demands of DNNs have simultaneously sparked interest in methods that make neural network inference faster and more power efficient, whether through new algorithmic inventions [19, 21, 11], dedicated digital hardware implementations [6, 17, 8], or by taking inspiration from real nervous systems [14, 37, 33, 23, 34].", "startOffset": 319, "endOffset": 339}, {"referenceID": 28, "context": "The large computational demands of DNNs have simultaneously sparked interest in methods that make neural network inference faster and more power efficient, whether through new algorithmic inventions [19, 21, 11], dedicated digital hardware implementations [6, 17, 8], or by taking inspiration from real nervous systems [14, 37, 33, 23, 34].", "startOffset": 319, "endOffset": 339}, {"referenceID": 5, "context": "With synchronous digital logic being the established standard of the electronics industry, first attempts towards hardware deep network accelerators have focused on this approach [6, 18, 7, 38].", "startOffset": 179, "endOffset": 193}, {"referenceID": 15, "context": "With synchronous digital logic being the established standard of the electronics industry, first attempts towards hardware deep network accelerators have focused on this approach [6, 18, 7, 38].", "startOffset": 179, "endOffset": 193}, {"referenceID": 6, "context": "With synchronous digital logic being the established standard of the electronics industry, first attempts towards hardware deep network accelerators have focused on this approach [6, 18, 7, 38].", "startOffset": 179, "endOffset": 193}, {"referenceID": 31, "context": "With synchronous digital logic being the established standard of the electronics industry, first attempts towards hardware deep network accelerators have focused on this approach [6, 18, 7, 38].", "startOffset": 179, "endOffset": 193}, {"referenceID": 36, "context": "An arguably more natural way of developing a hardware neural network emulator is to implement its computational primitives as multiple physical and parallel instances of analog computing nodes, where memory and processing elements are co-localized, and state variables are directly represented by analog currents or voltages, rather than being encoded digitally [43, 1, 49, 5, 3, 45].", "startOffset": 362, "endOffset": 383}, {"referenceID": 0, "context": "An arguably more natural way of developing a hardware neural network emulator is to implement its computational primitives as multiple physical and parallel instances of analog computing nodes, where memory and processing elements are co-localized, and state variables are directly represented by analog currents or voltages, rather than being encoded digitally [43, 1, 49, 5, 3, 45].", "startOffset": 362, "endOffset": 383}, {"referenceID": 42, "context": "An arguably more natural way of developing a hardware neural network emulator is to implement its computational primitives as multiple physical and parallel instances of analog computing nodes, where memory and processing elements are co-localized, and state variables are directly represented by analog currents or voltages, rather than being encoded digitally [43, 1, 49, 5, 3, 45].", "startOffset": 362, "endOffset": 383}, {"referenceID": 4, "context": "An arguably more natural way of developing a hardware neural network emulator is to implement its computational primitives as multiple physical and parallel instances of analog computing nodes, where memory and processing elements are co-localized, and state variables are directly represented by analog currents or voltages, rather than being encoded digitally [43, 1, 49, 5, 3, 45].", "startOffset": 362, "endOffset": 383}, {"referenceID": 2, "context": "An arguably more natural way of developing a hardware neural network emulator is to implement its computational primitives as multiple physical and parallel instances of analog computing nodes, where memory and processing elements are co-localized, and state variables are directly represented by analog currents or voltages, rather than being encoded digitally [43, 1, 49, 5, 3, 45].", "startOffset": 362, "endOffset": 383}, {"referenceID": 38, "context": "An arguably more natural way of developing a hardware neural network emulator is to implement its computational primitives as multiple physical and parallel instances of analog computing nodes, where memory and processing elements are co-localized, and state variables are directly represented by analog currents or voltages, rather than being encoded digitally [43, 1, 49, 5, 3, 45].", "startOffset": 362, "endOffset": 383}, {"referenceID": 17, "context": "By directly representing neural network operations in the physical properties of silicon transistors, such analog implementations can outshine their digital counterparts in terms of simplicity, allowing for significant advances in speed, size, and power consumption [20, 32].", "startOffset": 266, "endOffset": 274}, {"referenceID": 26, "context": "By directly representing neural network operations in the physical properties of silicon transistors, such analog implementations can outshine their digital counterparts in terms of simplicity, allowing for significant advances in speed, size, and power consumption [20, 32].", "startOffset": 266, "endOffset": 274}, {"referenceID": 33, "context": "The main reason why engineers have been discouraged from following this approach is that the properties of analog circuits are affected by the physical imperfections inherent to any chip fabrication process, which can lead to significant functional differences between individual devices [40].", "startOffset": 288, "endOffset": 292}, {"referenceID": 37, "context": "The weights of multi-layered networks are typically learned from labeled training data using the backpropagation algorithm [44], which minimizes the training error by computing error gradients and passing them backwards through the layers.", "startOffset": 123, "endOffset": 127}, {"referenceID": 13, "context": "In order for this to work in practice, the transfer function f needs to be at least piece-wise differentiable, as is the case for the commonly used rectified linear unit (ReLU) [16].", "startOffset": 177, "endOffset": 181}, {"referenceID": 24, "context": "After simulating measurements and parameterizing the transfer characteristics of the circuits as described previously, software networks were trained on the MNIST dataset of handwritten digits [30] and the TIDIGITS dataset of spoken digits [31] by means of the ADAM training method [27].", "startOffset": 282, "endOffset": 286}, {"referenceID": 41, "context": "In order to optimize the network for the use of discrete weights in the synaptic circuits dual-copy rounding [48, 10] was used (see Methods).", "startOffset": 109, "endOffset": 117}, {"referenceID": 8, "context": "In order to optimize the network for the use of discrete weights in the synaptic circuits dual-copy rounding [48, 10] was used (see Methods).", "startOffset": 109, "endOffset": 117}, {"referenceID": 5, "context": "Without major optimizations to either process or implementation, this leads to an efficiency of around 8 TOp/J, to our knowledge a performance at least four times greater than that achieved by digital single-purpose neural network accelerators in similar scenarios [6, 38].", "startOffset": 265, "endOffset": 272}, {"referenceID": 31, "context": "Without major optimizations to either process or implementation, this leads to an efficiency of around 8 TOp/J, to our knowledge a performance at least four times greater than that achieved by digital single-purpose neural network accelerators in similar scenarios [6, 38].", "startOffset": 265, "endOffset": 272}, {"referenceID": 36, "context": "The theory of analog neural networks and electronic realizations thereof have a substantial history that goes back to the1950s [43, 1].", "startOffset": 127, "endOffset": 134}, {"referenceID": 0, "context": "The theory of analog neural networks and electronic realizations thereof have a substantial history that goes back to the1950s [43, 1].", "startOffset": 127, "endOffset": 134}, {"referenceID": 5, "context": "Instead, digital designs have flourished in the interim and almost all current deep network designs are implemented in digital form [6, 7, 38].", "startOffset": 132, "endOffset": 142}, {"referenceID": 6, "context": "Instead, digital designs have flourished in the interim and almost all current deep network designs are implemented in digital form [6, 7, 38].", "startOffset": 132, "endOffset": 142}, {"referenceID": 31, "context": "Instead, digital designs have flourished in the interim and almost all current deep network designs are implemented in digital form [6, 7, 38].", "startOffset": 132, "endOffset": 142}, {"referenceID": 1, "context": "For example, the memristive computing technology which is currently being pursued for implementing large-scale cognitive neuromorphic and other technologies still suffers from the mismatch of fabricated devices [2, 26, 41].", "startOffset": 211, "endOffset": 222}, {"referenceID": 23, "context": "For example, the memristive computing technology which is currently being pursued for implementing large-scale cognitive neuromorphic and other technologies still suffers from the mismatch of fabricated devices [2, 26, 41].", "startOffset": 211, "endOffset": 222}, {"referenceID": 34, "context": "For example, the memristive computing technology which is currently being pursued for implementing large-scale cognitive neuromorphic and other technologies still suffers from the mismatch of fabricated devices [2, 26, 41].", "startOffset": 211, "endOffset": 222}, {"referenceID": 29, "context": "The proposed training method in this work can be used to account for device non-idealities in this technology [35].", "startOffset": 110, "endOffset": 114}, {"referenceID": 21, "context": "For example, spike-based neuromorphic systems [24] often have configurable weights between neurons.", "startOffset": 46, "endOffset": 50}, {"referenceID": 19, "context": "In principle, even recurrent architectures such as LSTM networks [22] can be trained using the same methods, where not only the static properties of the circuit are taken into account but also their dynamics.", "startOffset": 65, "endOffset": 69}, {"referenceID": 11, "context": "However, there is increasing evidence that neural networks pretrained on large datasets such as ImageNet provide excellent generic feature detectors [13, 42], which means that fast and efficient analog input pre-processors could be used as an important building blocks for a large variety of applications.", "startOffset": 149, "endOffset": 157}, {"referenceID": 35, "context": "However, there is increasing evidence that neural networks pretrained on large datasets such as ImageNet provide excellent generic feature detectors [13, 42], which means that fast and efficient analog input pre-processors could be used as an important building blocks for a large variety of applications.", "startOffset": 149, "endOffset": 157}], "year": 2016, "abstractText": "There is an urgent need for compact, fast, and power-efficient hardware implementations of state-of-the-art artificial intelligence. Here we propose a power-efficient approach for real-time inference, in which deep neural networks (DNNs) are implemented through low-power analog circuits. Although analog implementations can be extremely compact, they have been largely supplanted by digital designs, partly because of device mismatch effects due to fabrication. We propose a framework that exploits the power of Deep Learning to compensate for this mismatch by incorporating the measured variations of the devices as constraints in the DNN training process. This eliminates the use of mismatch minimization strategies such as the use of very large transistors, and allows circuit complexity and powerconsumption to be reduced to a minimum. Our results, based on large-scale simulations as well as a prototype VLSI chip implementation indicate at least a 3-fold improvement of processing efficiency over current digital implementations. Modern information technology requires increasing computational power to process massive amounts of data in real time. This rapidly growing need for computing power has led to the exploration of computing technologies beyond the predominant von Neumann architecture. In particular, due to the separation of memory and processing elements, traditional computing systems experience a bottleneck when dealing with problems involving great amounts of highdimensional data [4, 25], such as image processing, object recognition, probabilistic inference, or speech recognition. These problems can often best be tackled by conceptually simple but powerful and highly parallel methods, such as deep neural networks (DNNs), which in recent years have delivered state-of-the-art performance on exactly those applications [29, 47]. DNNs are characterized by stereotypical and simple operations at each unit, of which many can be performed in parallel. For this reason they map favorably e.g. onto the processing style of graphics 1 ar X iv :1 60 6. 07 78 6v 1 [ cs .N E ] 2 3 Ju n 20 16 processing units (GPUs) [46]. The large computational demands of DNNs have simultaneously sparked interest in methods that make neural network inference faster and more power efficient, whether through new algorithmic inventions [19, 21, 11], dedicated digital hardware implementations [6, 17, 8], or by taking inspiration from real nervous systems [14, 37, 33, 23, 34]. With synchronous digital logic being the established standard of the electronics industry, first attempts towards hardware deep network accelerators have focused on this approach [6, 18, 7, 38]. However, the massively parallel style of computation of neural networks is not reflected in the mostly serial and time-multiplexed nature of digital systems. An arguably more natural way of developing a hardware neural network emulator is to implement its computational primitives as multiple physical and parallel instances of analog computing nodes, where memory and processing elements are co-localized, and state variables are directly represented by analog currents or voltages, rather than being encoded digitally [43, 1, 49, 5, 3, 45]. By directly representing neural network operations in the physical properties of silicon transistors, such analog implementations can outshine their digital counterparts in terms of simplicity, allowing for significant advances in speed, size, and power consumption [20, 32]. The main reason why engineers have been discouraged from following this approach is that the properties of analog circuits are affected by the physical imperfections inherent to any chip fabrication process, which can lead to significant functional differences between individual devices [40]. In this work we propose a new approach, whereby rather than brute-force engineering more homogeneous circuits (e.g. by increasing transistor sizes and burning more power), we employ neural network training methods as an effective optimization framework to automatically compensate for the device mismatch effects of analog VLSI circuits. We use the diverse measured characteristics of individual VLSI devices as constraints in an off-line training process, to yield network configurations that are tailored to the particular analog device used, thereby compensating the inherent variability of chip fabrication. Finally, the network parameters, in particular the synaptic weights found during the training phase can be programmed in the network, and the analog circuits can be operated at run-time in the sub-threshold region for significantly lower power consumption. In this article, in addition to introducing a novel training method for both device and network, we also propose compact and low-power candidate VLSI circuits. A closed-loop demonstration of the framework is shown, based on a fabricated prototype chip, as well as detailed, large-scale simulations. The resulting analog electronic neural network performs as well as an ideal network, while offering at least a threefold lower power consumption over its digital counterpart.", "creator": "LaTeX with hyperref package"}}}