{"id": "1606.08514", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "27-Jun-2016", "title": "Towards Verified Artificial Intelligence", "abstract": "Verified artificial intelligence (AI) is the goal of designing AI-based systems that are provably correct with respect to mathematically-specified requirements. This paper considers Verified AI from a formal methods perspective. We describe five challenges for achieving Verified AI, and five corresponding principles for addressing these challenges.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n", "histories": [["v1", "Mon, 27 Jun 2016 23:51:04 GMT  (22kb,D)", "http://arxiv.org/abs/1606.08514v1", null], ["v2", "Sat, 2 Jul 2016 06:27:03 GMT  (24kb,D)", "http://arxiv.org/abs/1606.08514v2", null], ["v3", "Sat, 21 Oct 2017 09:50:36 GMT  (27kb,D)", "http://arxiv.org/abs/1606.08514v3", null]], "reviews": [], "SUBJECTS": "cs.AI", "authors": ["sanjit a seshia", "dorsa sadigh", "s shankar sastry"], "accepted": false, "id": "1606.08514"}, "pdf": {"name": "1606.08514.pdf", "metadata": {"source": "CRF", "title": "Towards Verified Artificial Intelligence", "authors": ["Sanjit A. Seshia", "Dorsa Sadigh"], "emails": ["sseshia@berkeley.edu", "dsadigh@berkeley.edu"], "sections": [{"heading": "1 Introduction", "text": "Artificial Intelligence (AI), as defined by Russell and Norvig [43], is the study of general principles of rational agents and components for constructing these agents, i.e., designing machines that leverage various techniques to mimic \u2018cognitive\u2019 functions we intuitively associate with human minds, such as \u2018learning\u2019 and \u2018problem solving.\u2019 We interpret the term AI broadly to include closely-related areas such as machine learning [32]. Systems that heavily use AI, henceforth referred to as AI-based systems, have had a significant impact in society in domains that include healthcare, transportation, social networking, e-commerce, education, etc. This growing societal-scale impact has brought with it a set of risks and concerns including errors in AI software, cyber-attacks, and safety of AI-based systems [42, 13, 2]. Therefore, the question of verification and validation of AI-based systems has begun to demand the attention of the research community. We define \u201cVerified AI\u201d as the goal of designing AI-based systems that are proven to satisfy desired properties specified in a mathematical formalism. How can we achieve this goal?\nA natural starting point is to consider formal methods \u2014 a field of computer science and engineering concerned with the rigorous mathematical specification, design, and verification of systems [49, 11]. At its core, formal methods is about proof: formulating specifications that form proof obligations, designing systems to meet those obligations, and verifying, via algorithmic proof search, that the systems indeed meet their specifications. Verification techniques such as model checking [9, 40, 10] and theorem proving (see, e.g. [38, 25, 22]) are used routinely in the computer-aided design of integrated circuits and have been widely applied to find bugs in software, analyze embedded systems, and find security vulnerabilities. At the heart of these advances are computational proof engines such as Boolean satisfiability (SAT) solvers [31], Boolean reasoning and manipulation routines based on Binary Decision Diagrams (BDDs) [7], and satisfiability modulo theories (SMT) solvers [4].\nIn this paper, we consider the challenge of Verified AI from a formal methods perspective. Figure 1 shows the typical formal verification process, which begins with the following three inputs: 1. A model of the system to be verified, S; 2. A model of the environment, E, and\nar X\niv :1\n60 6.\n08 51\n4v 1\n[ cs\n.A I]\n2 7\nJu n\n3. The property to be verified, \u03a6. The verifier generates as output a YES/NO answer, indicating whether or not S satisfies the property \u03a6 in environment E. Typically, a NO output is accompanied by a counterexample, also called an error trace, which is an execution of the system that indicates how \u03a6 is violated. Some formal verification tools also include a proof or certificate of correctness with a YES answer. In this paper, we use the term \u201cformal\nverification\u201d to apply to any verification technique that uses some aspect of formal methods. For instance, in hardware verification, the term \u201csemi-formal verification\u201d is often used for simulation-based verification methods that, while based on formal specifications (assertions), employ heuristic simulation-based methods to find behaviors that violate those specifications. Such semi-formal simulation-based verification methods have also found practical use in industrial verification of cyber-physical systems, e.g., for automotive systems.\nIn order to apply formal verification to AI-based systems, at a minimum, one must be able to generate the three inputs S, E and \u03a6 in formalisms for which (ideally) there exist decision procedures to answer the YES/NO question as described above. Additionally, these decision procedures must be efficient. Meeting these requirements, however, is not straightforward. Indeed, in our view, the challenges for Verified AI stem directly from these requirements. We outline these challenges in Section 2 below, and describe ideas to address each of these challenges in Section 3."}, {"heading": "2 Challenges for Verified AI", "text": "We identify five major challenges to achieving formally-verified AI-based systems. In this section, we sketch out these challenges, illustrating them with examples from the domain of (semi-)autonomous driving."}, {"heading": "2.1 Environment Modeling", "text": "In the traditional success stories for formal verification, such as verifying cache coherence protocols or device drivers, the interface between the system S and its environment E is well defined. Moreover, while the environment itself may not be known, it is usually acceptable to model it as a non-deterministic process subject to constraints specified in a suitable logic or automata-based formalism. Typically such an environment model is \u201cover-approximate\u201d, meaning that it may include more environment behaviors than it should.\nWe see systems based on AI or machine learning (ML) as being quite different. Consider an autonomous vehicle operating in rush-hour traffic in an urban environment. It is impossible even to precisely define the interface between the system and environment (i.e., to identify the variables/features of the environment that must be modeled), let alone to model all possible behaviors of the environment. Even if the interface is known, non-deterministic or over-approximate modeling is likely to produce too many spurious bug reports, rendering the verification process useless in practice.\nSimilarly, for systems involving joint human-machine control, such as semi-autonomous vehicles, human agents are a key part of the environment and/or system. Researchers have attempted modeling humans as nondeterministic or stochastic processes with the goal of verifying the correctness of the overall system [41, 44]. Given the variability and uncertainty in human behavior, a data-driven approach based on machine learning is usually necessary. Such an approach, in turn, is sensitive to the quality of data. For example, the technique of inverse reinforcement learning [35] can be used for learning the reward function of human agents [1, 50]. However, accuracy of the learned reward function depends on the expressivity of the hand-coded features by the designer and the amount and variety of the data collected. In order to achieve Verified AI for such human-in-the-loop systems, we need to address the limitations of the current human modeling techniques and provide guarantees about their prediction accuracy and convergence.\nThe first challenge, then, is to come up with a method of environment modeling that allows one to provide provable guarantees on the system\u2019s behavior without necessarily being precise or even over-approximate."}, {"heading": "2.2 Formal Specification", "text": "Formal verification critically relies on having a formal specification \u2013 a precise, mathematical statement of what the system is supposed to do. However, the challenge of coming up with a high-quality formal specification is well known, even in application domains in which formal verification has found considerable success (see, e.g., [5]).\nThis challenge is only exacerbated in AI-based systems. Consider a module in an autonomous vehicles that performs object recognition, distinguishing humans from other objects. What is the specification for such a module? How might it differ from the specifications used in traditional applications of formal methods? What should the specification language be, and what tools can one use to construct a specification?\nThus, the second challenge is to find an effective method to specify desired and undesired properties of systems that use AI- or ML-based components."}, {"heading": "2.3 Modeling Systems that Learn", "text": "In most traditional applications of formal verification, the system S is precisely known: it is a C program, or a circuit described in a hardware description language. The system modeling problem is primarily concerned with reducing the size of the S to a more tractable representation by abstracting away irrelevant details.\nAI-based systems lead to a very different challenge for system modeling. A major challenge is the use of machine learning, where the system evolves as it encounters new data and new situations. Modeling a deep neural network that has been trained on millions of data points can be challenging enough even if one \u201cfreezes\u201d the training process: new abstraction techniques will be necessary. Additionally, the verification procedure must account for future changes in the learner as new data arrives. New techniques must be devised to formally model components based on machine learning."}, {"heading": "2.4 Generating Training Data", "text": "Formal methods has proved effective for the systematic generation of test data in various settings including simulation-based verification of circuits (e.g., [26]) and finding security exploits in commodity software (e.g., [3]). In these cases, even though the end result is not a proof of correctness of the system S, the generated tests raise the level of assurance in the system\u2019s correctness. Can the testing of AI-based systems leverage formal methods in a similar manner?\nRecent efforts have shown that various machine learning algorithms can fail under small adversarial perturbations [36, 18, 34, 21]. Learning algorithms promise to generalize from data, but such simple perturbations that fool the algorithms create concerns regarding their use in safety-critical applications such as autonomous\ndriving. Such small perturbations might be even unrecognizable to naked eyes, but drive the algorithm to misclassify the perturbed data.\nThe fourth challenge we identify is to devise techniques based on formal methods to systematically generate training and testing data for ML-based components."}, {"heading": "2.5 Scalability of Verification Engines", "text": "A criticism of formal verification has always been whether it can scale up to handle industrial designs. Much progress has been made in this regard, especially in the area of hardware verification.\nHowever, in systems that use AI or ML, the scalability challenge is even greater. In addition to the scale of systems as measured by traditional metrics (dimension of state space, number of components, etc.), the types of components can be much more complex. For instance, in (semi-)autonomous driving, autonomous vehicles and their controllers need to be modeled as hybrid systems combining both discrete and continuous dynamics. Moreover, agents in the environment (humans, other vehicles) may need to be modeled as probabilistic processes. Finally, the requirements may involve not only traditional Boolean specifications on safety and liveness, but also quantitative requirements on system robustness and performance."}, {"heading": "3 Principles for Verified AI", "text": "For each of the challenges raised in the preceding section, we suggest a corresponding set of principles to follow in the design/verification process to address that challenge. Taken together, we believe these principles can point a way towards the goal of Verified AI."}, {"heading": "3.1 Introspective Environment Modeling", "text": "Recall from Sec. 2.1 the challenge of modeling the environment E of an AI-based system S. We believe that a promising strategy to meet this challenge is to develop design and verification methods that are introspective, i.e., they identify assumptions A that system S makes about the environment E that are sufficient to guarantee the satisfaction of the specification \u03a6. The assumptions A must be such that, at run time, S can efficiently monitor A so as to ensure that they always hold. Moreover, if there is a human operator involved, one might want A to be translatable into an explanation that is human understandable, so that S can \u201cexplain\u201d to the human why it may not be able to satisfy the specification \u03a6.\nIdeally, the assumptions A must be the weakest set of such assumptions that S makes about its environment. However, given the other requirements for A to be efficiently monitorable and human understandable, one may need to settle for a stronger assumption.\nAs an example, consider an autonomous vehicle that is trying to maintain a minimum distance from any other object while being in motion \u2014 this forms the specification \u03a6. Note that \u03a6 defines an interface and a set of sensors that the vehicle S must use to check for itself that \u03a6 is satisfied. On top of this minimal interface, suppose that S tracks other features of the environment E such as the state of traffic lights, the number of vehicles in its vicinity, their state such as their velocity, whether they are human-driven, an estimate of those human drivers\u2019 intent and driving style, etc. It will then need to generate assumptions A to monitor over this expanded interface (as well as its internal state) so as to ensure that when A is satsified, so is \u03a6.\nExtracting good assumptions may be easier during the design process, e.g., while synthesizing a controller for S. Preliminary work by the authors has shown that such extraction of monitorable assumptions is feasible in simple cases [28, 29, 20], although much more research is required to make this practical."}, {"heading": "3.2 End-to-End Specifications and Specification Mining", "text": "Writing formal specifications for AI/ML components is hard, even impossible if the task involves a version of the Turing test. How can we address this challenge described in Sec. 2.2?\nAs researchers often say: when the problem is too hard, perhaps we should change the problem! We believe that formally specifying the behavior of an AI/ML component may be unnecessary. Instead, one should focus on specifying the end-to-end behavior of the entire AI-based system. We believe that this latter task, in many if not most cases, can be done more easily.\nConsider again our autonomous vehicle scenario from the previous section. It should be straightforward to specify the property \u03a6 corresponding to maintaining a minimum distance from any object during motion. This property says nothing about any component that uses machine learning.\nOf course, in order to test the ML-based component, it is useful to have a formal specification on its interface. However, we believe this specification does not need to be exact: a \u201clikely specification\u201d could suffice. We suggest the use of techniques for inferring specifications from behaviors and other artifacts \u2014 so-called specification mining techiques (e.g., [16, 27, 24]), for this purpose."}, {"heading": "3.3 Abstractions and Explanations for Machine Learning", "text": "Let us now consider the challenge, described in Sec. 2.3, of modeling systems S that learn from experience. We believe a combination of automated abstraction and explanation-based learning will be needed to model such systems for purposes of formal verification.\nFirst, effective techniques need to be developed to abstract ML components into a formalism for which efficient verification techniques exist or can be developed. Since the guarantees many ML algorithms give are probabilistic, this will require the development of probabilistic logics and similar formalisms that can capture these guarantees (e.g., [45]). Additionally, if the output of a learning algorithm is accompanied by a measure of uncertainty about its correctness, then that uncertainty must be propagated to the rest of the system and represented in the model of the overall system. For example, the formalism of convex Markov decision processes (convex MDPs) [37, 39, 44] provide a way of representing uncertainty in the values of learned transition probabilities. Algorithms for verification and control may then need to be extended to handle these new abstractions (see, e.g., [39]).\nThe task of modeling a learning system can be made easier if the learner accompanies its predictions with explanations of how those predictions result from the data and background knowledge. In fact, this idea is not new \u2013 it has long been investigated by the ML community under terms such as explanation-based generalization [33]. When such explanations are made compatible with the modeling languages used in formal methods, the task of system modeling for verification will be considerably easier."}, {"heading": "3.4 Randomized Formal Methods", "text": "Consider the challenge, described in Sec. 2.4, of generating training data for a ML component in an AI-based system. More concretely, suppose we wish to systematically test a classifier f : Rn \u2192 R that given a set of data points (images, audio, etc.) x \u2208 Rn assigns a real-valued label to them f (x) \u2208 R. One testing problem is to find a perturbation r \u2208 Rn such that the algorithm flips the label it assigns to (many) examples upon perturbation, i.e., f (x) 6= f (x+ r).\nSuch perturbations cannot be done arbitrarily. One challenge is to define the space of \u201clegal\u201d perturbations so that the resulting examples are still legal inputs that look \u201crealistic\u201d. Additionally, one might need to impose constraints on the distribution of the generated examples in order to obtain guarantees about convergence of the learning algorithm to the true concept. How do we meet all these requirements?\nWe believe that the answer may lie in a new class of randomized formal methods \u2013 randomized algorithms for generating test inputs subject to formal constraints and distribution requirements. Specifically, a recently\ndefined class of techniques, termed control improvisation [19], holds much promise. An improviser is a generator of random strings (examples) x that satisfy three constraints: (i) a hard constraint that defines the space of legal x; (ii) a soft constraint defining how the generated x must be similar to real-world examples, and (iii) a randomness requirement defining a constraint on the output distribution. The theory of control improvisation is still in its infancy, and we are just starting to understand the computational complexity and to devise efficient algorithms. Much more remains to be done.\nAnother challenge for the systematic generation of training data for ML components is to model those components and their requirements in a suitable manner. To deal with this, we think techniques for black-box test generation for cyber-physical systems (e.g. [14]) can be useful in generating training and test data for ML components. These methods would treat ML components as black-box components and generate training data using an end-to-end formal specification."}, {"heading": "3.5 Engines for Run-Time, Quantitative, and Learning-Based Verification", "text": "Once effective methods for formalizing S, E, and \u03a6 are devised for AI-based systems, then one must design engines for deciding whether S satisfies the specification \u03a6 in the environment E. At this point, the scalability challenge described in Sec. 2.5 will loom large. We will need the analogs of SAT, BDDs, and SMT for AI-based systems.\nAlthough it is too early to know exactly what formalisms will be best suited for AI-based systems, we believe a few directions are clear.\nFirst, the complexity and heterogeneity of AI-based systems means that, in general, many decision problems underlying formal verification are likely to be undecidable. (For example, even deciding whether a state of a linear hybrid system is reachable is undecidable.) To overcome this obstacle posed by computational complexity, one must either (i) find tractable but realistic problem classes, or (ii) settle for incomplete or unsound formal verification methods (i.e., semi decision procedures). Techniques for simulation-based verification or run-time verification can prove very fruitful in this regard, as has been recently obtained for industrial automotive systems (e.g. [15, 17, 12]).\nFurther, we believe that it will be important to move from verification problems with purely \u201cBoolean\u201d YES/NO answers to problems with quantitative solutions. Such a move is motivated by the use of probabilistic modeling as well as a need for defining cost-based specifications with the goal of minimizing cost. Some initial steps have been taken with the use of logics with quantitative semantics, such as signal temporal logic [30], and automata-theoretic formalisms such as weighted automata [8], but much more remains to be done. Similarly, work on SMT solving must be extended to more effectively handle cost constraints \u2014 in other words, combining SMT solving with optimization methods (e.g., [48, 6]).\nFinally, we believe that verification methods themselves are benefiting from the use of inductive learning [46, 47]. An emerging topic in formal methods, termed formal inductive synthesis [23], shows much promise for tackling hard problems in verification and synthesis by leveraging expert human insight in combination with induction and deduction."}, {"heading": "4 Conclusion", "text": "Taking a formal methods perspective, we have analyzed the challenge of formally verifying systems that use artificial intelligence or machine learning. We identified five main challenges: environment modeling, formal specification, system modeling, generating training/test data, and scalability. For each of these five challenges, we have identified corresponding principles for design and verification that hold promise for addressing that challenge. We are currently engaged in developing the theory behind these principles, and applying them to the design of human cyber-physical systems, with a special focus on semi-autonomous driving, and expect to\nreport on the results in the years to come."}, {"heading": "Acknowledgments", "text": "The authors\u2019 work is supported in part by NSF grants CCF-1139138 and CCF-1116993, by an NDSEG Fellowship, and by the TerraSwarm Research Center, one of six centers supported by the STARnet phase of the Focus Center Research Program (FCRP) a Semiconductor Research Corporation program sponsored by MARCO and DARPA. We gratefully acknowledge the many colleagues with whom our conversations and collaborations have helped shape this document."}], "references": [{"title": "Apprenticeship learning via inverse reinforcement learning", "author": ["Pieter Abbeel", "Andrew Y Ng"], "venue": "In Proceedings of the twenty-first international conference on Machine learning,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2004}, {"title": "Concrete problems in AI safety", "author": ["Dario Amodei", "Chris Olah", "Jacob Steinhardt", "Paul Christiano", "John Schulman", "Dan Man\u00e9"], "venue": "arXiv preprint arXiv:1606.06565,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2016}, {"title": "Automatic exploit generation", "author": ["Thanassis Avgerinos", "Sang Kil Cha", "Alexandre Rebert", "Edward J. Schwartz", "Maverick Woo", "David Brumley"], "venue": "Commun. ACM,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2014}, {"title": "Satisfiability modulo theories", "author": ["Clark Barrett", "Roberto Sebastiani", "Sanjit A. Seshia", "Cesare Tinelli"], "venue": "Handbook of Satisfiability,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2009}, {"title": "Efficient detection of vacuity in ACTL formulas", "author": ["I. Beer", "S. Ben-David", "C. Eisner", "Y. Rodeh"], "venue": "Formal Methods in System Design,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2001}, {"title": "\u03bdz-an optimizing SMT solver", "author": ["Nikolaj Bj\u00f8rner", "Anh-Dung Phan", "Lars Fleckenstein"], "venue": "In International Conference on Tools and Algorithms for the Construction and Analysis of Systems,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2015}, {"title": "Graph-based algorithms for Boolean function manipulation", "author": ["Randal E. Bryant"], "venue": "IEEE Transactions on Computers,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 1986}, {"title": "Design and synthesis of synchronization skeletons using branching-time temporal logic", "author": ["Edmund M. Clarke", "E. Allen Emerson"], "venue": "In Logic of Programs,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 1981}, {"title": "Formal methods: State of the art and future directions", "author": ["Edmund M Clarke", "Jeannette M Wing"], "venue": "ACM Computing Surveys (CSUR),", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 1996}, {"title": "Robust online monitoring of signal temporal logic", "author": ["Jyotirmoy Deshmukh", "Alexandre Donz\u00e9", "Shromona Ghosh", "Xiaoqing Jin", "Garvit Juniwal", "Sanjit A. Seshia"], "venue": "In Proceedings of the International Conference on Runtime Verification (RV),", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2015}, {"title": "Rise of concerns about AI: reflections and directions", "author": ["Thomas G Dietterich", "Eric J Horvitz"], "venue": "Communications of the ACM,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2015}, {"title": "Breach, a toolbox for verification and parameter synthesis of hybrid systems", "author": ["Alexandre Donz\u00e9"], "venue": "In CAV,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2010}, {"title": "Automotive systems requirement mining using Breach", "author": ["Alexandre Donz\u00e9", "Xiaoqing Jin", "Jyotirmoy V. Deshmukh", "Sanjit A. Seshia"], "venue": "In American Control Conference (ACC),", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2015}, {"title": "Dynamically Discovering Likely Program Invariants", "author": ["Michael Ernst"], "venue": "PhD thesis,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2000}, {"title": "Automotive control design bug-finding with the S-TaLiRo tool", "author": ["Georgios E. Fainekos"], "venue": "In American Control Conference (ACC),", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2015}, {"title": "Analysis of classifiers\u2019 robustness to adversarial perturbations", "author": ["Alhussein Fawzi", "Omar Fawzi", "Pascal Frossard"], "venue": "arXiv preprint arXiv:1502.02590,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2015}, {"title": "Control improvisation", "author": ["Daniel J. Fremont", "Alexandre Donz\u00e9", "Sanjit A. Seshia", "David Wessel"], "venue": "IARCS Annual Conference on Foundations of Software Technology and Theoretical Computer Science (FSTTCS", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2015}, {"title": "Diagnosis and repair for synthesis from signal temporal logic specifications", "author": ["Shromona Ghosh", "Dorsa Sadigh", "Pierluigi Nuzzo", "Vasumathi Raman", "Alexandre Donz\u00e9", "S. Shankar Sastry", "Sanjit A. Seshia"], "venue": "In Proceedings of the 9th International Conference on Hybrid Systems: Computation and Control (HSCC),", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2016}, {"title": "Explaining and harnessing adversarial examples", "author": ["Ian J Goodfellow", "Jonathon Shlens", "Christian Szegedy"], "venue": "arXiv preprint arXiv:1412.6572,", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2014}, {"title": "Introduction to HOL: A Theorem Proving Environment for Higher- Order Logic", "author": ["M.J.C. Gordon", "T.F. Melham"], "venue": null, "citeRegEx": "22", "shortCiteRegEx": "22", "year": 1993}, {"title": "A Theory of Formal Synthesis via Inductive Learning", "author": ["S. Jha", "S.A. Seshia"], "venue": "ArXiv e-prints,", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2015}, {"title": "Mining requirements from closed-loop control models", "author": ["Xiaoqing Jin", "Alexandre Donz\u00e9", "Jyotirmoy Deshmukh", "Sanjit A. Seshia"], "venue": "In Proceedings of the International Conference on Hybrid Systems: Computation and Control (HSCC),", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2013}, {"title": "Computer-Aided Reasoning: An Approach", "author": ["Matt Kaufmann", "Panagiotis Manolios", "J. Strother Moore"], "venue": "Kluwer Academic Publishers,", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2000}, {"title": "Stimulus generation for constrained random simulation", "author": ["Nathan Kitchen", "Andreas Kuehlmann"], "venue": "In Proceedings of the 2007 IEEE/ACM International Conference on Computer-Aided Design (ICCAD),", "citeRegEx": "26", "shortCiteRegEx": "26", "year": 2007}, {"title": "Specification Mining: New Formalisms, Algorithms and Applications", "author": ["Wenchao Li"], "venue": "PhD thesis, EECS Department,", "citeRegEx": "27", "shortCiteRegEx": "27", "year": 2014}, {"title": "Mining assumptions for synthesis", "author": ["Wenchao Li", "Lili Dworkin", "Sanjit A. Seshia"], "venue": "In Proceedings of the Ninth ACM/IEEE International Conference on Formal Methods and Models for Codesign (MEM- OCODE),", "citeRegEx": "28", "shortCiteRegEx": "28", "year": 2011}, {"title": "Synthesis for human-in-the-loop control systems", "author": ["Wenchao Li", "Dorsa Sadigh", "S. Shankar Sastry", "Sanjit A. Seshia"], "venue": "In Proceedings of the 20th International Conference on Tools and Algorithms for the Construction and Analysis of Systems (TACAS),", "citeRegEx": "29", "shortCiteRegEx": "29", "year": 2014}, {"title": "Monitoring temporal properties of continuous signals", "author": ["Oded Maler", "Dejan Nickovic"], "venue": "In FOR- MATS/FTRTFT,", "citeRegEx": "30", "shortCiteRegEx": "30", "year": 2004}, {"title": "Boolean satisfiability: From theoretical hardness to practical success", "author": ["Sharad Malik", "Lintao Zhang"], "venue": "Communications of the ACM (CACM),", "citeRegEx": "31", "shortCiteRegEx": "31", "year": 2009}, {"title": "Explanation-based generalization: A unifying view", "author": ["Tom M Mitchell", "Richard M Keller", "Smadar T Kedar-Cabelli"], "venue": "Machine learning,", "citeRegEx": "33", "shortCiteRegEx": "33", "year": 1986}, {"title": "Deepfool: a simple and accurate method to fool deep neural networks", "author": ["Seyed-Mohsen Moosavi-Dezfooli", "Alhussein Fawzi", "Pascal Frossard"], "venue": "arXiv preprint arXiv:1511.04599,", "citeRegEx": "34", "shortCiteRegEx": "34", "year": 2015}, {"title": "Algorithms for inverse reinforcement learning", "author": ["Andrew Y. Ng", "Stuart J. Russell"], "venue": "In Proceedings of the Seventeenth International Conference on Machine Learning (ICML),", "citeRegEx": "35", "shortCiteRegEx": "35", "year": 2000}, {"title": "Deep neural networks are easily fooled: High confidence predictions for unrecognizable images", "author": ["Anh Nguyen", "Jason Yosinski", "Jeff Clune"], "venue": "In 2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR),", "citeRegEx": "36", "shortCiteRegEx": "36", "year": 2015}, {"title": "Robust Control of Markov Decision Processes with Uncertain Transition Matrices", "author": ["A. Nilim", "L. El Ghaoui"], "venue": "Journal of Operations Research,", "citeRegEx": "37", "shortCiteRegEx": "37", "year": 2005}, {"title": "PVS: A prototype verification system", "author": ["S. Owre", "J.M. Rushby", "N. Shankar"], "venue": "editor, 11th International Conference on Automated Deduction (CADE),", "citeRegEx": "38", "shortCiteRegEx": "38", "year": 1992}, {"title": "Polynomialtime verification of PCTL properties of MDPs with convex uncertainties", "author": ["Alberto Puggelli", "Wenchao Li", "Alberto Sangiovanni-Vincentelli", "Sanjit A. Seshia"], "venue": "In Proceedings of the 25th International Conference on Computer-Aided Verification (CAV),", "citeRegEx": "39", "shortCiteRegEx": "39", "year": 2013}, {"title": "Specification and verification of concurrent systems in CESAR", "author": ["Jean-Pierre Queille", "Joseph Sifakis"], "venue": "In Symposium on Programming,", "citeRegEx": "40", "shortCiteRegEx": "40", "year": 1982}, {"title": "Using model checking to help discover mode confusions and other automation surprises", "author": ["John Rushby"], "venue": "Reliability Engineering & System Safety,", "citeRegEx": "41", "shortCiteRegEx": "41", "year": 2002}, {"title": "Letter to the editor: Research priorities for robust and beneficial artificial intelligence: An open letter", "author": ["Stuart Russell", "Tom Dietterich", "Eric Horvitz", "Bart Selman", "Francesca Rossi", "Demis Hassabis", "Shane Legg", "Mustafa Suleyman", "Dileep George", "Scott Phoenix"], "venue": "AI Magazine,", "citeRegEx": "42", "shortCiteRegEx": "42", "year": 2015}, {"title": "Artificial intelligence: a modern approach", "author": ["Stuart Jonathan Russell", "Peter Norvig"], "venue": "Prentice hall,", "citeRegEx": "43", "shortCiteRegEx": "43", "year": 2010}, {"title": "Data-driven probabilistic modeling and verification of human driver behavior", "author": ["Dorsa Sadigh", "Katherine Driggs-Campbell", "Alberto Puggelli", "Wenchao Li", "Victor Shia", "Ruzena Bajcsy", "S. Shankar Sastry", "Sanjit A. Seshia"], "venue": "In Formal Verification and Modeling in Human- Machine Systems,", "citeRegEx": "44", "shortCiteRegEx": "44", "year": 2014}, {"title": "Safe control under uncertainty with probabilistic signal temporal logic", "author": ["Dorsa Sadigh", "Ashish Kapoor"], "venue": "In Proceedings of Robotics: Science and Systems,", "citeRegEx": "45", "shortCiteRegEx": "45", "year": 2016}, {"title": "Sciduction: Combining induction, deduction, and structure for verification and synthesis", "author": ["Sanjit A. Seshia"], "venue": "In Proceedings of the Design Automation Conference (DAC),", "citeRegEx": "46", "shortCiteRegEx": "46", "year": 2012}, {"title": "Combining induction, deduction, and structure for verification and synthesis", "author": ["Sanjit A. Seshia"], "venue": "Proceedings of the IEEE,", "citeRegEx": "47", "shortCiteRegEx": "47", "year": 2015}, {"title": "Imhotep-SMT: A satisfiability modulo theory solver for secure state estimation", "author": ["Yasser Shoukry", "Pierluigi Nuzzo", "Alberto Puggelli", "Sanjit A. Seshia", "Mani Srivastava", "Paulo Tabuada"], "venue": "In In 13th International Workshop on Satisfiability Modulo Theories (SMT),", "citeRegEx": "48", "shortCiteRegEx": "48", "year": 2015}, {"title": "A specifier\u2019s introduction to formal methods", "author": ["Jeannette M Wing"], "venue": "IEEE Computer,", "citeRegEx": "49", "shortCiteRegEx": "49", "year": 1990}, {"title": "Maximum entropy inverse reinforcement learning", "author": ["Brian D Ziebart", "Andrew L Maas", "J Andrew Bagnell", "Anind K Dey"], "venue": "In AAAI,", "citeRegEx": "50", "shortCiteRegEx": "50", "year": 2008}], "referenceMentions": [{"referenceID": 39, "context": "Artificial Intelligence (AI), as defined by Russell and Norvig [43], is the study of general principles of rational agents and components for constructing these agents, i.", "startOffset": 63, "endOffset": 67}, {"referenceID": 38, "context": "This growing societal-scale impact has brought with it a set of risks and concerns including errors in AI software, cyber-attacks, and safety of AI-based systems [42, 13, 2].", "startOffset": 162, "endOffset": 173}, {"referenceID": 10, "context": "This growing societal-scale impact has brought with it a set of risks and concerns including errors in AI software, cyber-attacks, and safety of AI-based systems [42, 13, 2].", "startOffset": 162, "endOffset": 173}, {"referenceID": 1, "context": "This growing societal-scale impact has brought with it a set of risks and concerns including errors in AI software, cyber-attacks, and safety of AI-based systems [42, 13, 2].", "startOffset": 162, "endOffset": 173}, {"referenceID": 45, "context": "How can we achieve this goal? A natural starting point is to consider formal methods \u2014 a field of computer science and engineering concerned with the rigorous mathematical specification, design, and verification of systems [49, 11].", "startOffset": 223, "endOffset": 231}, {"referenceID": 8, "context": "How can we achieve this goal? A natural starting point is to consider formal methods \u2014 a field of computer science and engineering concerned with the rigorous mathematical specification, design, and verification of systems [49, 11].", "startOffset": 223, "endOffset": 231}, {"referenceID": 7, "context": "Verification techniques such as model checking [9, 40, 10] and theorem proving (see, e.", "startOffset": 47, "endOffset": 58}, {"referenceID": 36, "context": "Verification techniques such as model checking [9, 40, 10] and theorem proving (see, e.", "startOffset": 47, "endOffset": 58}, {"referenceID": 34, "context": "[38, 25, 22]) are used routinely in the computer-aided design of integrated circuits and have been widely applied to find bugs in software, analyze embedded systems, and find security vulnerabilities.", "startOffset": 0, "endOffset": 12}, {"referenceID": 22, "context": "[38, 25, 22]) are used routinely in the computer-aided design of integrated circuits and have been widely applied to find bugs in software, analyze embedded systems, and find security vulnerabilities.", "startOffset": 0, "endOffset": 12}, {"referenceID": 19, "context": "[38, 25, 22]) are used routinely in the computer-aided design of integrated circuits and have been widely applied to find bugs in software, analyze embedded systems, and find security vulnerabilities.", "startOffset": 0, "endOffset": 12}, {"referenceID": 28, "context": "At the heart of these advances are computational proof engines such as Boolean satisfiability (SAT) solvers [31], Boolean reasoning and manipulation routines based on Binary Decision Diagrams (BDDs) [7], and satisfiability modulo theories (SMT) solvers [4].", "startOffset": 108, "endOffset": 112}, {"referenceID": 6, "context": "At the heart of these advances are computational proof engines such as Boolean satisfiability (SAT) solvers [31], Boolean reasoning and manipulation routines based on Binary Decision Diagrams (BDDs) [7], and satisfiability modulo theories (SMT) solvers [4].", "startOffset": 199, "endOffset": 202}, {"referenceID": 3, "context": "At the heart of these advances are computational proof engines such as Boolean satisfiability (SAT) solvers [31], Boolean reasoning and manipulation routines based on Binary Decision Diagrams (BDDs) [7], and satisfiability modulo theories (SMT) solvers [4].", "startOffset": 253, "endOffset": 256}, {"referenceID": 37, "context": "Researchers have attempted modeling humans as nondeterministic or stochastic processes with the goal of verifying the correctness of the overall system [41, 44].", "startOffset": 152, "endOffset": 160}, {"referenceID": 40, "context": "Researchers have attempted modeling humans as nondeterministic or stochastic processes with the goal of verifying the correctness of the overall system [41, 44].", "startOffset": 152, "endOffset": 160}, {"referenceID": 31, "context": "For example, the technique of inverse reinforcement learning [35] can be used for learning the reward function of human agents [1, 50].", "startOffset": 61, "endOffset": 65}, {"referenceID": 0, "context": "For example, the technique of inverse reinforcement learning [35] can be used for learning the reward function of human agents [1, 50].", "startOffset": 127, "endOffset": 134}, {"referenceID": 46, "context": "For example, the technique of inverse reinforcement learning [35] can be used for learning the reward function of human agents [1, 50].", "startOffset": 127, "endOffset": 134}, {"referenceID": 4, "context": ", [5]).", "startOffset": 2, "endOffset": 5}, {"referenceID": 23, "context": ", [26]) and finding security exploits in commodity software (e.", "startOffset": 2, "endOffset": 6}, {"referenceID": 2, "context": ", [3]).", "startOffset": 2, "endOffset": 5}, {"referenceID": 32, "context": "Can the testing of AI-based systems leverage formal methods in a similar manner? Recent efforts have shown that various machine learning algorithms can fail under small adversarial perturbations [36, 18, 34, 21].", "startOffset": 195, "endOffset": 211}, {"referenceID": 15, "context": "Can the testing of AI-based systems leverage formal methods in a similar manner? Recent efforts have shown that various machine learning algorithms can fail under small adversarial perturbations [36, 18, 34, 21].", "startOffset": 195, "endOffset": 211}, {"referenceID": 30, "context": "Can the testing of AI-based systems leverage formal methods in a similar manner? Recent efforts have shown that various machine learning algorithms can fail under small adversarial perturbations [36, 18, 34, 21].", "startOffset": 195, "endOffset": 211}, {"referenceID": 18, "context": "Can the testing of AI-based systems leverage formal methods in a similar manner? Recent efforts have shown that various machine learning algorithms can fail under small adversarial perturbations [36, 18, 34, 21].", "startOffset": 195, "endOffset": 211}, {"referenceID": 25, "context": "Preliminary work by the authors has shown that such extraction of monitorable assumptions is feasible in simple cases [28, 29, 20], although much more research is required to make this practical.", "startOffset": 118, "endOffset": 130}, {"referenceID": 26, "context": "Preliminary work by the authors has shown that such extraction of monitorable assumptions is feasible in simple cases [28, 29, 20], although much more research is required to make this practical.", "startOffset": 118, "endOffset": 130}, {"referenceID": 17, "context": "Preliminary work by the authors has shown that such extraction of monitorable assumptions is feasible in simple cases [28, 29, 20], although much more research is required to make this practical.", "startOffset": 118, "endOffset": 130}, {"referenceID": 13, "context": ", [16, 27, 24]), for this purpose.", "startOffset": 2, "endOffset": 14}, {"referenceID": 24, "context": ", [16, 27, 24]), for this purpose.", "startOffset": 2, "endOffset": 14}, {"referenceID": 21, "context": ", [16, 27, 24]), for this purpose.", "startOffset": 2, "endOffset": 14}, {"referenceID": 41, "context": ", [45]).", "startOffset": 2, "endOffset": 6}, {"referenceID": 33, "context": "For example, the formalism of convex Markov decision processes (convex MDPs) [37, 39, 44] provide a way of representing uncertainty in the values of learned transition probabilities.", "startOffset": 77, "endOffset": 89}, {"referenceID": 35, "context": "For example, the formalism of convex Markov decision processes (convex MDPs) [37, 39, 44] provide a way of representing uncertainty in the values of learned transition probabilities.", "startOffset": 77, "endOffset": 89}, {"referenceID": 40, "context": "For example, the formalism of convex Markov decision processes (convex MDPs) [37, 39, 44] provide a way of representing uncertainty in the values of learned transition probabilities.", "startOffset": 77, "endOffset": 89}, {"referenceID": 35, "context": ", [39]).", "startOffset": 2, "endOffset": 6}, {"referenceID": 29, "context": "In fact, this idea is not new \u2013 it has long been investigated by the ML community under terms such as explanation-based generalization [33].", "startOffset": 135, "endOffset": 139}, {"referenceID": 16, "context": "defined class of techniques, termed control improvisation [19], holds much promise.", "startOffset": 58, "endOffset": 62}, {"referenceID": 11, "context": "[14]) can be useful in generating training and test data for ML components.", "startOffset": 0, "endOffset": 4}, {"referenceID": 12, "context": "[15, 17, 12]).", "startOffset": 0, "endOffset": 12}, {"referenceID": 14, "context": "[15, 17, 12]).", "startOffset": 0, "endOffset": 12}, {"referenceID": 9, "context": "[15, 17, 12]).", "startOffset": 0, "endOffset": 12}, {"referenceID": 27, "context": "Some initial steps have been taken with the use of logics with quantitative semantics, such as signal temporal logic [30], and automata-theoretic formalisms such as weighted automata [8], but much more remains to be done.", "startOffset": 117, "endOffset": 121}, {"referenceID": 44, "context": ", [48, 6]).", "startOffset": 2, "endOffset": 9}, {"referenceID": 5, "context": ", [48, 6]).", "startOffset": 2, "endOffset": 9}, {"referenceID": 42, "context": "Finally, we believe that verification methods themselves are benefiting from the use of inductive learning [46, 47].", "startOffset": 107, "endOffset": 115}, {"referenceID": 43, "context": "Finally, we believe that verification methods themselves are benefiting from the use of inductive learning [46, 47].", "startOffset": 107, "endOffset": 115}, {"referenceID": 20, "context": "An emerging topic in formal methods, termed formal inductive synthesis [23], shows much promise for tackling hard problems in verification and synthesis by leveraging expert human insight in combination with induction and deduction.", "startOffset": 71, "endOffset": 75}], "year": 2017, "abstractText": "Verified artificial intelligence (AI) is the goal of designing AI-based systems that are provably correct with respect to mathematically-specified requirements. This paper considers Verified AI from a formal methods perspective. We describe five challenges for achieving Verified AI, and five corresponding principles for addressing these challenges.", "creator": "LaTeX with hyperref package"}}}