{"id": "1606.08571", "review": {"conference": "AAAI", "VERSION": "v1", "DATE_OF_SUBMISSION": "28-Jun-2016", "title": "Alternating Back-Propagation for Generator Network", "abstract": "The supervised learning of the discriminative convolutional neural network (ConvNet) is powered by back-propagation on the parameters. In this paper, we show that the unsupervised learning of a popular top-down generative ConvNet model with latent continuous factors can be accomplished by a learning algorithm that consists of alternatively performing back-propagation on both the latent factors and the parameters. The model is a non-linear generalization of factor analysis, where the high-dimensional observed data, is assumed to be the noisy version of a vector generated by a non-linear transformation of a low-dimensional vector of continuous latent factors. Furthermore, it is assumed that these latent factors follow known independent distributions, such as standard normal distributions, and the non-linear transformation is assumed to be parametrized by a top-down ConvNet, which is capable of approximating the highly non-linear mapping from the latent factors to the image. We explore a simple and natural learning algorithm for this model that alternates between the following two steps: (1) inferring the latent factors by Langevin dynamics or gradient descent, and (2) updating the parameters of the ConvNet by gradient descent. Step (1) is based on the gradient of the reconstruction error with respect to the latent factors, which is available by back-propagation. We call this step inferential back-propagation. Step (2) is based on the gradient of the reconstruction error with respect to the parameters, and is also obtained by back-propagation. We refer to this step as learning back-propagation. The code for inferential back-propagation is actually part of the code for learning back-propagation, and thus the inferential back-propagation is actually a by-product of the learning back-propagation. We show that our algorithm can learn realistic generative models of images and sounds. We test this model by generating a ConvNet based on a convolutional neural network with latent continuous factors. We evaluate this model by using a normal vector of continuous variable data and learning the discriminative convolutional neural network with latent continuous factors and a sparse sparse representation. We can then evaluate a discriminative convolutional neural network with latent continuous factors and a sparse representation. We test this model by performing a deep learning on the convolutional neural network with latent continuous factors. To find the posterior distribution for the neural network with latent continuous factors, we are using two", "histories": [["v1", "Tue, 28 Jun 2016 06:46:05 GMT  (5581kb,D)", "https://arxiv.org/abs/1606.08571v1", null], ["v2", "Sat, 2 Jul 2016 15:11:00 GMT  (5585kb,D)", "http://arxiv.org/abs/1606.08571v2", null], ["v3", "Thu, 15 Sep 2016 04:38:01 GMT  (7268kb,D)", "http://arxiv.org/abs/1606.08571v3", null], ["v4", "Tue, 6 Dec 2016 04:04:19 GMT  (7393kb,D)", "http://arxiv.org/abs/1606.08571v4", null]], "reviews": [], "SUBJECTS": "stat.ML cs.CV cs.LG cs.NE", "authors": ["tian han", "yang lu", "song-chun zhu", "ying nian wu"], "accepted": true, "id": "1606.08571"}, "pdf": {"name": "1606.08571.pdf", "metadata": {"source": "CRF", "title": "Alternating Back-Propagation for Generator Network", "authors": ["Tian Han", "Yang Lu", "Song-Chun Zhu", "Ying Nian Wu"], "emails": [], "sections": [{"heading": "1 Introduction", "text": "This paper studies the fundamental problem of learning and inference in the generator network (Goodfellow et al., 2014), which is a generative model that has become popular recently. Specifically, we propose an alternating back-propagation algorithm for learning and inference in this model."}, {"heading": "1.1 Non-linear factor analysis", "text": "The generator network is a non-linear generalization of factor analysis. Factor analysis is a prototype model in unsupervised learning of distributed representations. There are two directions one can pursue in order to generalize the factor analysis model. One direction is to generalize the prior model or the prior assumption about the latent factors. This led to methods such as independent component analysis (Hyv\u00e4rinen, Karhunen, and Oja, 2004), sparse coding (Olshausen and Field, 1997), non-negative matrix factorization (Lee and Seung, 2001), matrix factorization and completion for recommender systems (Koren, Bell, and Volinsky, 2009), etc.\nThe other direction to generalize the factor analysis model is to generalize the mapping from the continuous latent factors to the observed signal. The generator network is an example in this direction. It generalizes the linear mapping in factor analysis to a non-linear mapping that is defined by\n\u2020 Equal contributions.\na convolutional neural network (ConvNet or CNN) (LeCun et al., 1998; Krizhevsky, Sutskever, and Hinton, 2012; Dosovitskiy, Springenberg, and Brox, 2015). It has been shown recently that the generator network is capable of generating realistic images (Denton et al., 2015; Radford, Metz, and Chintala, 2016).\nThe generator network is a fundamental representation of knowledge, and it has the following properties: (1) Analysis: The model disentangles the variations in the observed signals into independent variations of latent factors. (2) Synthesis: The model can synthesize new signals by sampling the factors from the known prior distribution and transforming the factors into the signal. (3) Embedding: The model embeds the high-dimensional non-Euclidean manifold formed by the observed signals into the low-dimensional Euclidean space of the latent factors, so that linear interpolation in the lowdimensional factor space results in non-linear interpolation in the data space."}, {"heading": "1.2 Alternating back-propagation", "text": "The factor analysis model can be learned by the Rubin-Thayer EM algorithm (Rubin and Thayer, 1982; Dempster, Laird, and Rubin, 1977), where both the E-step and the M-step are based on multivariate linear regression. Inspired by this algorithm, we propose an alternating back-propagation algorithm for learning the generator network that iterates the following two-steps:\n(1) Inferential back-propagation: For each training example, infer the continuous latent factors by Langevin dynamics or gradient descent.\n(2) Learning back-propagation: Update the parameters given the inferred latent factors by gradient descent.\nThe Langevin dynamics (Neal, 2011) is a stochastic sampling counterpart of gradient descent. The gradient computations in both steps are powered by back-propagation. Because of the ConvNet structure, the gradient computation in step (1) is actually a by-product of the gradient computation in step (2) in terms of coding.\nGiven the factors, the learning of the ConvNet is a supervised learning problem (Dosovitskiy, Springenberg, and Brox, 2015) that can be accomplished by the learning backpropagation. With factors unknown, the learning becomes an unsupervised problem, which can be solved by adding the inferential back-propagation as an inner loop of the learning\nar X\niv :1\n60 6.\n08 57\n1v 4\n[ st\nat .M\nL ]\n6 D\nec 2\n01 6\nprocess. We shall show that the alternating back-propagation algorithm can learn realistic generator models of natural images, video sequences, and sounds.\nThe alternating back-propagation algorithm follows the tradition of alternating operations in unsupervised learning, such as alternating linear regression in the EM algorithm for factor analysis, alternating least squares algorithm for matrix factorization (Koren, Bell, and Volinsky, 2009; Kim and Park, 2008), and alternating gradient descent algorithm for sparse coding (Olshausen and Field, 1997). All these unsupervised learning algorithms alternate an inference step and a learning step, as is the case with alternating back-propagation."}, {"heading": "1.3 Explaining-away inference", "text": "The inferential back-propagation solves an inverse problem by an explaining-away process, where the latent factors compete with each other to explain each training example. The following are the advantages of the explaining-away inference of the latent factors:\n(1) The latent factors may follow sophisticated prior models. For instance, in textured motions (Wang and Zhu, 2003) or dynamic textures (Doretto et al., 2003), the latent factors may follow a dynamic model such as vector auto-regression. By inferring the latent factors that explain the observed examples, we can learn the prior model.\n(2) The observed data may be incomplete or indirect. For instance, the training images may contain occluded objects. In this case, the latent factors can still be obtained by explaining the incomplete or indirect observations, and the model can still be learned as before."}, {"heading": "1.4 Learning from incomplete or indirect data", "text": "We venture to propose that a main advantage of a generative model is to learn from incomplete or indirect data, which are not uncommon in practice. The generative model can then be evaluated based on how well it recovers the unobserved original data, while still learning a model that can generate new data. Learning the generator network from incomplete data can be considered a non-linear generalization of matrix completion.\nWe also propose to evaluate the learned generator network by the reconstruction error on the testing data."}, {"heading": "1.5 Contribution and related work", "text": "The main contribution of this paper is to propose the alternating back-propagation algorithm for training the generator network. Another contribution is to evaluate the generative models by learning from incomplete or indirect training data.\nExisting training methods for the generator network avoid explain-away inference of latent factors. Two methods have recently been devised to accomplish this. Both methods involve an assisting network with a separate set of parameters in addition to the original network that generates the signals. One method is variational auto-encoder (VAE) (Kingma and Welling, 2014; Rezende, Mohamed, and Wierstra, 2014; Mnih and Gregor, 2014), where the assisting network is an inferential or recognition network that seeks to approximate the posterior distribution of the latent factors. The other method\nis the generative adversarial network (GAN) (Goodfellow et al., 2014; Denton et al., 2015; Radford, Metz, and Chintala, 2016), where the assisting network is a discriminator network that plays an adversarial role against the generator network.\nUnlike alternating back-propagation, VAE does not perform explicit explain-away inference, while GAN avoids inferring the latent factors altogether. In comparison, the alternating back-propagation algorithm is simpler and more basic, without resorting to an extra network. While it is difficult to compare these methods directly, we illustrate the strength of alternating back-propagation by learning from incomplete and indirect data, where we only need to explain whatever data we are given. This may prove difficult or less convenient for VAE and GAN.\nMeanwhile, alternating back-propagation is complementary to VAE and GAN training. It may use VAE to initialize the inferential back-propagation, and as a result, may improve the inference in VAE. The inferential back-propagation may help infer the latent factors of the observed examples for GAN, thus providing a method to test if GAN can explain the entire training set.\nThe generator network is based on a top-down ConvNet. One can also obtain a probabilistic model based on a bottomup ConvNet that defines descriptive features (Xie et al., 2016; Lu, Zhu, and Wu, 2016)."}, {"heading": "2 Factor analysis with ConvNet", "text": ""}, {"heading": "2.1 Factor analysis and beyond", "text": "Let Y be a D-dimensional observed data vector, such as an image. Let Z be the d-dimensional vector of continuous latent factors, Z = (zk, k = 1, ..., d). The traditional factor analysis model is Y = WZ + , where W is D \u00d7 d matrix, and is a D-dimensional error vector or the observational noise. We assume that Z \u223c N(0, Id), where Id stands for the d-dimensional identity matrix. We also assume that \u223c N(0, \u03c32ID), i.e., the observational errors are Gaussian white noises. There are three perspectives to view W . (1) Basis vectors. Write W = (W1, ...,Wd), where each Wk is a D-dimensional column vector. Then Y = \u2211d k=1 zkWk + , i.e., Wk are the basis vectors and zk are the coefficients. (2) Loading matrix. Write W = (w1, ..., wD)>, where w>j is the j-th row of W . Then yj = \u3008wj , Z\u3009 + j , where yj and j are the j-th components of Y and respectively. Each yj is a loading of the d factors where wj is a vector of loading weights, indicating which factors are important for determining yj . W is called the loading matrix. (3) Matrix factorization. Suppose we observe Y = (Y1, ..., Yn), whose factors are Z = (Z1, ..., Zn), then Y \u2248WZ.\nThe factor analysis model can be learned by the RubinThayer EM algorithm, which involves alternating regressions of Z on Y in the E-step and of Y on Z in the M-step, with both steps powered by the sweep operator (Rubin and Thayer, 1982; Liu, Rubin, and Wu, 1998).\nThe factor analysis model is the prototype of many subsequent models that generalize the prior model of Z. (1) Independent component analysis (Hyv\u00e4rinen, Karhunen, and Oja, 2004), d = D, = 0, and zk are assumed to follow independent heavy tailed distributions. (2) Sparse coding\n(Olshausen and Field, 1997), d > D, and Z is assumed to be a redundant but sparse vector, i.e., only a small number of zk are non-zero or significantly different from zero. (3) Non-negative matrix factorization (Lee and Seung, 2001), it is assumed that zk \u2265 0. (4) Recommender system (Koren, Bell, and Volinsky, 2009), Z is a vector of a customer\u2019s desires in different aspects, and wj is a vector of product j\u2019s desirabilities in these aspects."}, {"heading": "2.2 ConvNet mapping", "text": "In addition to generalizing the prior model of the latent factors Z, we can also generalize the mapping from Z to Y . In this paper, we consider the generator network model (Goodfellow et al., 2014) that retains the assumptions that d < D, Z \u223c N(0, Id), and \u223c N(0, \u03c32ID) as in traditional factor analysis, but generalizes the linear mapping WZ to a non-linear mapping f(Z;W ), where f is a ConvNet, and W collects all the connection weights and bias terms of the ConvNet. Then the model becomes\nY = f(Z;W ) + ,\nZ \u223c N(0, Id), \u223c N(0, \u03c32ID), d < D. (1) The reconstruction error is ||Y \u2212 f(Z;W )||2. We may assume more sophisticated models for , such as colored noise or non-Gaussian texture. If Y is binary, we can emit Y by a probability map P = 1/[1 + exp(\u2212f(Z;W ))], where the sigmoid transformation and Bernoulli sampling are carried out pixel-wise. If Y is multi-level, we may assume multinomial logistic emission model or some ordinal emission model.\nAlthough f(Z;W ) can be any non-linear mapping, the ConvNet parameterization of f(Z;W ) makes it particularly close to the original factor analysis. Specifically, we can write the top-down ConvNet as follows:\nZ(l\u22121) = fl(WlZ (l) + bl), (2)\nwhere fl is element-wise non-linearity at layer l, Wl is the matrix of connection weights, bl is the vector of bias terms at layer l, and W = (Wl, bl, l = 1, ..., L). Z(0) = f(Z;W ), and Z(L) = Z. The top-down ConvNet (2) can be considered a recursion of the original factor analysis model, where the factors at the layer l \u2212 1 are obtained by the linear superposition of the basis vectors or basis functions that are column vectors of Wl, with the factors at the layer l serving as the coefficients of the linear superposition. In the case of ConvNet, the basis functions are shift-invariant versions of one another, like wavelets. See Appendix for an in-depth understanding of the model."}, {"heading": "3 Alternating back-propagation", "text": "If we observe a training set of data vectors {Yi, i = 1, ..., n}, then each Yi has a corresponding Zi, but all the Yi share the same ConvNet W . Intuitively, we should infer {Zi} and learn W to minimize the reconstruction error \u2211n i=1 ||Yi \u2212 f(Zi;W )||2 plus a regularization term that corresponds to the prior on Z.\nMore formally, the model can be written as Z \u223c p(Z) and [Y |Z,W ] \u223c p(Y |Z,W ). Adopting the language of the EM\nalgorithm (Dempster, Laird, and Rubin, 1977), the completedata model is given by\nlog p(Y,Z;W ) = log [p(Z)p(Y |Z,W )]\n= \u2212 1 2\u03c32 \u2016Y \u2212 f(Z;W )\u20162 \u2212 1 2 \u2016Z\u20162 + const. (3)\nThe observed-data model is obtained by integrating out Z: p(Y ;W ) = \u222b p(Z)p(Y |Z,W )dZ. The posterior distribution of Z is given by p(Z|Y,W ) = p(Y,Z;W )/p(Y ;W ) \u221d p(Z)p(Y |Z,W ) as a function of Z.\nFor the training data {Yi}, the complete-data loglikelihood is L(W, {Zi}) = \u2211n i=1 log p(Yi, Zi;W ), where we assume \u03c32 is given. Learning and inference can be accomplished by maximizing the complete-data log-likelihood, which can be obtained by the alternating gradient descent algorithm that iterates the following two steps: (1) Inference step: update Zi by running l steps of gradient descent. (2) Learning step: update W by one step of gradient descent.\nA more rigorous method is to maximize the observed-data log-likelihood, which is L(W ) = \u2211n i=1 log p(Yi;W ) =\u2211n\ni=1 log \u222b p(Yi, Zi;W )dZi. The observed-data loglikelihood takes into account the uncertainties in inferring Zi. See Appendix for an in-depth understanding.\nThe gradient of L(W ) can be calculated according to the following well-known fact that underlies the EM algorithm:\n\u2202\n\u2202W log p(Y ;W ) =\n1\nP (Y ;W )\n\u2202\n\u2202W\n\u222b p(Y,Z;W )dZ\n= Ep(Z|Y,W )\n[ \u2202\n\u2202W log p(Y,Z;W )\n] . (4)\nThe expectation with respect to p(Z|Y,W ) can be approximated by drawing samples from p(Z|Y,W ) and then computing the Monte Carlo average.\nThe Langevin dynamics for sampling Z \u223c p(Z|Y,W ) iterates\nZ\u03c4+1 = Z\u03c4 + sU\u03c4 +\ns2\n2\n[ 1\n\u03c32 (Y \u2212 f(Z\u03c4 ;W ))\n\u2202\n\u2202Z f(Z\u03c4 ;W )\u2212 Z\u03c4\n] , (5)\nwhere \u03c4 denotes the time step for the Langevin sampling, s is the step size, and U\u03c4 denotes a random vector that follows N(0, Id). The Langevin dynamics (5) is an explain-away process, where the latent factors in Z compete to explain away the current residual Y \u2212 f(Z\u03c4 ;W ).\nTo explain Langevin dynamics, its continuous time version for sampling \u03c0(x) \u221d exp[\u2212E(x)] is xt+\u2206t = xt \u2212 \u2206tE \u2032(xt)/2 + \u221a \u2206tUt. The dynamics has \u03c0 as its stationary distribution, because it can be shown that for any wellbehaved testing function h, if xt \u223c \u03c0, then E[h(xt+\u2206t)] \u2212 E[h(xt)] \u2192 0, as \u2206t \u2192 0, so that xt+\u2206t \u223c \u03c0. Alternatively, given xt = x, suppose xt+\u2206t \u223c K(x, y), then [\u03c0(y)K(y, x)]/[\u03c0(x)K(x, y)]\u2192 1 as \u2206t\u2192 0.\nThe stochastic gradient algorithm of (Younes, 1999) can be used for learning, where in each iteration, for each Zi, only a single copy of Zi is sampled from p(Zi|Yi,W ) by running a finite number of steps of Langevin dynamics starting from the current value of Zi, i.e., the warm start. With {Zi} sampled\nin this manner, we can update the parameter W based on the gradient L\u2032(W ), whose Monte Carlo approximation is:\nL\u2032(W ) \u2248 n\u2211 i=1 \u2202 \u2202W log p(Yi, Zi;W )\n= \u2212 n\u2211 i=1 \u2202 \u2202W 1 2\u03c32 \u2016Yi \u2212 f(Zi;W )\u20162\n= n\u2211 i=1 1 \u03c32 (Yi \u2212 f(Zi;W )) \u2202 \u2202W f(Zi;W ).(6)\nAlgorithm 1 describes the details of the learning and sampling algorithm.\nAlgorithm 1 Alternating back-propagation Require:\n(1) training examples {Yi, i = 1, ..., n} (2) number of Langevin steps l (3) number of learning iterations T\nEnsure: (1) learned parameters W (2) inferred latent factors {Zi, i = 1, ..., n}\n1: Let t\u2190 0, initialize W . 2: Initialize Zi, for i = 1, ..., n. 3: repeat 4: Inferential back-propagation: For each i, run l steps\nof Langevin dynamics to sample Zi \u223c p(Zi|Yi,W ) with warm start, i.e., starting from the current Zi, each step follows equation (5).\n5: Learning back-propagation: Update W \u2190 W + \u03b3tL \u2032(W ), where L\u2032(W ) is computed according to equation (6), with learning rate \u03b3t. 6: Let t\u2190 t+ 1 7: until t = T\nIf the Gaussian noise U\u03c4 in the Langevin dynamics (5) is removed, then the above algorithm becomes the alternating gradient descent algorithm. It is possible to update both W and {Zi} simultaneously by joint gradient descent.\nBoth the inferential back-propagation and the learning back-propagation are guided by the residual Yi \u2212 f(Zi;W ). The inferential back-propagation is based on \u2202f(Z;W )/\u2202Z, whereas the learning back-propagation is based on \u2202f(Z;W )/\u2202W . Both gradients can be efficiently computed by back-propagation. The computations of the two gradients share most of their steps. Specifically, for the top-down ConvNet defined by (2), \u2202f(Z;W )/\u2202W and \u2202f(Z;W )/\u2202Z share the same code for the chain rule computation of \u2202Z(l\u22121)/\u2202Z(l) for l = 1, ..., L. Thus, the code for \u2202f(Z;W )/\u2202Z is part of the code for \u2202f(Z;W )/\u2202W .\nIn Algorithm 1, the Langevin dynamics samples from a gradually changing posterior distribution p(Zi|Yi,W ) becauseW keeps changing. The updating of bothZi andW collaborate to reduce the reconstruction error \u2016Yi\u2212f(Zi;W )||2. The parameter \u03c32 plays the role of annealing or tempering in Langevin sampling. If \u03c32 is very large, then the posterior\nis close to the prior N(0, Id). If \u03c32 is very small, then the posterior may be multi-modal, but the evolving energy landscape of p(Zi|Yi,W ) may help alleviate the trapping of the local modes. In practice, we tune the value of \u03c32 instead of estimating it. The Langevin dynamics can be extended to Hamiltonian Monte Carlo (Neal, 2011) or more sophisticated versions (Girolami and Calderhead, 2011)."}, {"heading": "4 Experiments", "text": "The code in our experiments is based on the MatConvNet package of (Vedaldi and Lenc, 2015).\nThe training images and sounds are scaled so that the intensities are within the range [\u22121, 1]. We adopt the structure of the generator network of (Radford, Metz, and Chintala, 2016; Dosovitskiy, Springenberg, and Brox, 2015), where the topdown network consists of multiple layers of deconvolution by linear superposition, ReLU non-linearity, and up-sampling, with tanh non-linearity at the bottom-layer (Radford, Metz, and Chintala, 2016) to make the signals fall within [\u22121, 1]. We also adopt batch normalization (Ioffe and Szegedy, 2015).\nWe fix \u03c3 = .3 for the standard deviation of the noise vector . We use l = 10 or 30 steps of Langevin dynamics within each learning iteration, and the Langevin step size s is set at .1 or .3. We run T = 600 learning iterations, with learning rate .0001, and momentum .5. The learning algorithm produces the learned network parameters W and the inferred latent factors Z for each signal Y in the end. The synthesized signals are obtained by f(Z;W ), where Z is sampled from the prior distribution N(0, Id)."}, {"heading": "4.1 Qualitative experiments", "text": "Experiment 1. Modeling texture patterns. We learn a separate model from each texture image. The images are collected from the Internet, and then resized to 224\u00d7 224. The synthesized images are 448 \u00d7 448. Figures 1 shows four examples.\nThe factors Z at the top layer form a \u221a d\u00d7 \u221a d image, with each pixel following N(0, 1) independently. The \u221a d \u00d7 \u221a d\nimage Z is then transformed to Y by the top-down ConvNet. We use d = 72 in the learning stage for all the texture experiments. In order to obtain the synthesized image, we randomly sample a 14\u00d7 14 Z from N(0, I), and then expand the learned networkW to generate the 448\u00d7 448 synthesized image f(Z;W ).\nThe training network is as follows. Starting from 7 \u00d7 7 imageZ, the network has 5 layers of deconvolution with 5\u00d75 kernels (i.e., linear superposition of 5 \u00d7 5 basis functions), with an up-sampling factor of 2 at each layer (i.e., the basis functions are 2 pixels apart). The number of channels in the first layer is 512 (i.e., 512 translation invariant basis functions), and is decreased by a factor 2 at each layer. The Langevin steps l = 10 with step size s = .1.\nExperiment 2. Modeling sound patterns. A sound signal can be treated as a one-dimensional texture image (McDermott and Simoncelli, 2011). The sound data are collected from the Internet. Each training signal is a 5 second clip with the sampling rate of 11025 Hertz and is represented as a 1\u00d760000 vector. We learn a separate model from each sound signal.\nThe latent factors Z form a sequence that follows N(0, Id), with d = 6. The top-down network consists of 4 layers of deconvolution with kernels of size 1\u00d7 25, and up-sampling factor of 10. The number of channels in the first layer is 256, and decreases by a factor of 2 at each layer. For synthesis, we start from a longer Gaussian white noise sequence Z with d = 12 and generate the synthesized sound by expanding the learned network. Figure 2 shows the waveforms of the observed sound signal in the first row and the synthesized sound signal in the second row.\nExperiment 3. Modeling object patterns. We model object patterns using the network structure that is essentially the same as the network for the texture model, except that we include a fully connected layer under the latent factors Z, now a d-dimensional vector. The images are 64\u00d7 64. We use ReLU with a leaking factor .2 (Maas, Hannun, and Ng, 2013; Xu et al., 2015). The Langevin steps l = 30 with step size s = .3.\nIn the first experiment, we learn a model where Z has two components, i.e., Z = (z1, z2), and d = 2. The training data are 11 images of 6 tigers and 5 lions. After training the model, we generate images using the learned top-down ConvNet for\n(z1, z2) \u2208 [\u22122, 2]2, where we discretize both z1 and z2 into 9 equally spaced values. The left panel of Figure 3 displays the synthesized images on the 9\u00d7 9 panel.\nIn the second experiment, we learn a model with d = 100 from 1000 face images randomly selected from the CelebA dataset (Liu et al., 2015). The left panel of Figure 4 displays the images generated by the learned model. The middle panel displays the interpolation results. The images at the four corners are generated by the Z vectors of four images randomly selected from the training set. The images in the middle are obtained by first interpolating the Z\u2019s of the four corner images using the sphere interpolation (Dinh, Sohl-Dickstein, and Bengio, 2016) and then generating the images by the learned ConvNet.\nWe also provide qualitative comparison with Deep Convolutional Generative Adversarial Net (DCGAN) (Goodfellow et al., 2014; Radford, Metz, and Chintala, 2016). The right panel of Figure 3 shows the generated results\nfor the lion-tiger dataset using 2-dimensional Z. The right panel of Figure 4 displays the generated results trained on 1000 aligned faces from celebA dataset, with d = 100. We use the code from https://github.com/carpedm20/ DCGAN-tensorflow, with the tuning parameters as in (Radford, Metz, and Chintala, 2016). We run T = 600 iterations as in our method.\nExperiment 4. Modeling dynamic patterns. We model a textured motion (Wang and Zhu, 2003) or a dynamic texture (Doretto et al., 2003) by a non-linear dynamic system Yt = f(Zt;W ) + t, and Zt+1 = AZt + \u03b7t, where we assume the latent factors follow a vector auto-regressive model, whereA is a d\u00d7dmatrix, and \u03b7t \u223c N(0, Q) is the innovation. This model is a direct generalization of the linear dynamic system of (Doretto et al., 2003), where Yt is reduced to Zt by principal component analysis (PCA) via singular value decomposition (SVD). We learn the model in two steps. (1) Treat {Yt} as independent examples and learn W and infer {Zt} as before. (2) Treat {Zt} as the training data, learn A and Q as in (Doretto et al., 2003). After that, we can synthesize a new dynamic texture. We start from Z0 \u223c N(0, Id), and then generate the sequence according to the learned model (we discard a burn-in period of 15 frames). Figure 5 shows some experiments, where we set d = 20. The first row is a segment of the sequence generated by our model, and the second row is generated by the method of (Doretto et al., 2003), with the same dimensionality of Z. It is possible to generalize the auto-regressive model of Zt to recurrent network. We may also treat the video sequences as 3D images, and learn generator networks with 3D spatial-temporal filters or basis functions."}, {"heading": "4.2 Quantitative experiments", "text": "Experiment 5. Learning from incomplete data. Our method can learn from images with occluded pixels. This task is inspired by the fact that most of the images contain occluded objects. It can be considered a non-linear generalization of matrix completion in recommender system.\nOur method can be adapted to this task with minimal modification. The only modification involves the computation of \u2016Y \u2212 f(Z;W )\u20162. For a fully observed image, it is com-\nputed by summing over all the pixels. For a partially observed image, we compute it by summing over only the observed pixels. Then we can continue to use the alternating backpropagation algorithm to infer Z and learn W . With inferred Z and learned W , the image can be automatically recovered by f(Z;W ). In the end, we will be able to accomplish the following tasks: (T1) Recover the occluded pixels of training images. (T2) Synthesize new images from the learned model. (T3) Recover the occluded pixels of testing images using the learned model.\nWe want to emphasize that in our experiments, all the training images are partially occluded. Our experiments are different from (1) de-noising auto-encoder (Vincent et al., 2008), where the training images are fully observed, and noises are added as a matter of regularization, (2) in-painting or de-noising, where the prior model or regularization has already been learned or given. (2) is about task (T3) mentioned above, but not about tasks (T1) and (T2).\nLearning from incomplete data can be difficult for GAN and VAE, because the occluded pixels are different for different training images.\nWe evaluate our method on 10,000 images randomly selected from CelebA dataset. We design 5 experiments, with two types of occlusions: (1) 3 experiments are about salt and pepper occlusion, where we randomly place 3\u00d7 3 masks on the 64\u00d7 64 image domain to cover roughly 50%, 70% and 90% of pixels respectively. These 3 experiments are denoted P.5, P.7, and P.9 respectively (P for pepper). (2) 2 experiments are about single region mask occlusion, where we randomly place a 20\u00d720 or 30\u00d730 mask on the 64\u00d764 image domain. These 2 experiments are denoted M20 and M30 respectively (M for mask). We set d = 100. Table 1 displays the recovery errors of the 5 experiments, where the error is defined as per pixel difference (relative to the range of the pixel values) between the original image and the recovered image on the occluded pixels. We emphasize that the recovery errors are not training errors, because the intensities of the occluded\npixels are not observed in training. Figure 6 displays recovery results. In experiment P.9, 90% of pixels are occluded, but we can still learn the model and recover the original images.\nExperiment 6. Learning from indirect data. We can learn the model from the compressively sensed data (Cand\u00e8s, Romberg, and Tao, 2006). We generate a set of white noise images as random projections. We then project the training images on these white noise images. We can learn the model from the random projections instead of the original images. We only need to replace \u2016Y \u2212 f(Z;W )\u20162 by \u2016SY \u2212 Sf(Z;W )\u20162, where S is the given white noise sensing matrix, and SY is the observation. We can treat S as a fully connected layer of known filters below f(Z;W ), so that we can continue to use alternating back-propagation to infer Z and learn W , thus recovering the image by f(Z;W ). In the end, we will be able to (T1) Recover the original images from their projections during learning. (T2) Synthesize new images from the learned model. (T3) Recover testing images from their projections based on the learned model. Our experiments are different from traditional compressed sensing, which is task (T3), but not tasks (T1) and (T2). Moreover, the image recovery in our work is based on non-linear dimension reduction instead of linear sparsity.\nWe evaluate our method on 1000 face images randomly selected from CelebA dataset. These images are projected ontoK = 1000 white noise images with each pixel randomly sampled from N(0, .52). After this random projection, each image of size 64\u00d7 64\u00d7 3 becomes a K-dimensional vector. We show the recovery errors for different latent dimensions d in Table 2, where the recovery error is defined as the per pixel difference (relative to the range of the pixel values) between the original image and the recovered image. Figure 7 shows some recovery results.\nExperiment 7. Model evaluation by reconstruction error on testing data. After learning the model from the training images (now assumed to be fully observed), we can evaluate the model by the reconstruction error on the testing images. We randomly select 1000 face images for training and 300 images for testing from CelebA dataset. After learning, we infer the latent factors Z for each testing image using inferential back-propagation, and then reconstruct the testing image\nby f(Z;W ) using the inferred Z and the learned W . In the inferential back-propagation for inferring Z, we initialize Z \u223c N(0, Id), and run 300 Langevin steps with step size .05. Table 3 shows the reconstruction errors of alternating backpropagation learning (ABP) as compared to PCA learning for different latent dimensions d. Figure 8 shows some reconstructed testing images. For PCA, we learn the d eigenvectors from the training images, and then project the testing images on the learned eigenvectors for reconstruction.\nExperiments 5-7 may be used to evaluate generative models in general. Experiments 5 and 6 appear new, and we have not found comparable methods that can accomplish all three tasks (T1), (T2), and (T3) simultaneously."}, {"heading": "5 Conclusion", "text": "This paper proposes an alternating back-propagation algorithm for training the generator network. We recognize that the generator network is a non-linear generalization of the factor analysis model, and develop the alternating backpropagation algorithm as the non-linear generalization of the alternating regression scheme of the Rubin-Thayer EM algorithm for fitting the factor analysis model. The alternating back-propagation algorithm iterates the inferential backpropagation for inferring the latent factors and the learning back-propagation for updating the parameters. Both backpropagation steps share most of their computing steps in the chain rule calculations.\nOur learning algorithm is perhaps the most canonical algorithm for training the generator network. It is based on maximum likelihood, which is theoretically the most accurate estimator. The maximum likelihood learning seeks to explain and charge the whole dataset uniformly, so that there is little concern of under-fitting or biased fitting.\nAs an unsupervised learning algorithm, the alternating back-propagation algorithm is a natural generalization of the original back-propagation algorithm for supervised learning. It adds an inferential back-propagation step to the learn-\ning back-propagation step, with minimal overhead in coding and affordable overhead in computing. The inferential back-propagation seeks to perform accurate explaining-away inference of the latent factors. It can be worthwhile for tasks such as learning from incomplete or indirect data, or learning models where the latent factors themselves follow sophisticated prior models with unknown parameters. The inferential back-propagation may also be used to evaluate the generators learned by other methods on tasks such as reconstructing or completing testing data.\nOur method or its variants can be applied to non-linear matrix factorization and completion. It can also be applied to problems where some components or aspects of the factors are supervised.\nCode, images, sounds, and videos http://www.stat.ucla.edu/~ywu/ABP/main.html"}, {"heading": "Acknowledgement", "text": "We thank Yifei (Jerry) Xu for his help with the experiments during his 2016 summer visit. We thank Jianwen Xie for helpful discussions.\nThe work is supported by NSF DMS 1310391, DARPA SIMPLEX N66001-15-C-4035, ONR MURI N00014-16-12007, and DARPA ARO W911NF-16-1-0579."}, {"heading": "6 Appendix", "text": ""}, {"heading": "6.1 ReLU and piecewise factor analysis", "text": "The generator network is Y = f(Z;W ) + , Z(l\u22121) = fl(WlZ\n(l) + bl), l = 1, ..., L, with Z(0) = f(Z;W ), and Z(L) = Z. The element-wise non-linearity fl in modern ConvNet is usually the two-piece linearity, such as rectified linear unit (ReLU) (Krizhevsky, Sutskever, and Hinton, 2012) or the leaky ReLU (Maas, Hannun, and Ng, 2013; Xu et al., 2015). Each ReLU unit corresponds to a binary switch. For the case of non-leaky ReLU, following the analysis of (Pascanu, Montufar, and Bengio, 2013), we can write Z(l\u22121) = \u03b4l(WlZ\n(l) + bl), where \u03b4l = diag(1(WlZ(l) + bl > 0)) is a diagonal matrix, 1() is an element-wise indicator function. For the case of leaky ReLU, the 0 values on the diagonal are replaced by a leaking factor (e.g., .2). \u03b4 = (\u03b4l, l = 1, ..., L) forms a classification of Z according to the network W . Specifically, the factor space of Z is divided into a large number of pieces by the hyperplanes WlZ(l) + bl = 0, and each piece is indexed by an instantiation of \u03b4. We can write \u03b4 = \u03b4(Z;W ) to make explicit its dependence on Z and W . On the piece indexed by \u03b4, f(Z;W ) = W\u03b4Z + b\u03b4. Assuming bl = 0,\u2200l, for simplicity, we have W\u03b4 = \u03b41W1...\u03b4LWL. Thus each piece defined by \u03b4 = \u03b4(Z;W ) corresponds to a linear factor analysis Y = W\u03b4Z + , whose basis W\u03b4 is a multiplicative recomposition of the basis functions at multiple layers (Wl, l = 1, ..., L), and the recomposition is controlled by the binary switches at multiple layers \u03b4 = (\u03b4l, l = 1, ..., L). Hence the top-down ConvNet amounts to a reconfigurable basis W\u03b4 for representing Y , and the model is a piecewise linear factor analysis. If we retain the bias term, we will have\nY = W\u03b4Z + b\u03b4 + , for an overall bias term that depends on \u03b4. So the distribution of Y is essentially piecewise Gaussian.\nThe generator model can be considered an explicit implementation of the local linear embedding (Roweis and Saul, 2000), where Z is the embedding of Y . In local linear embedding, the mapping between Z and Y is implicit. In the generator model, the mapping from Z to Y is explicit. With ReLU ConvNet, the mapping is piecewise linear, which is consistent with local linear embedding, except that the partition of the linear pieces by \u03b4(Z;W ) in the generator model is learned automatically.\nThe inferential back-propagation is a Langevin dynamics on the energy function \u2016Y \u2212 f(Z;W )\u20162/(2\u03c32) + \u2016Z\u20162/2. With f(Z;W ) = W\u03b4Z, \u2202f(Z;W )/\u2202Z = W\u03b4. If Z belongs to the piece defined by \u03b4, then the inferential backpropagation seeks to approximate Y by the basis W\u03b4 via a ridge regression. Because Z keeps changing during the Langevin dynamics, \u03b4(Z;W ) may also be changing, and the algorithm searches for the optimal reconfigurable basis W\u03b4 to approximate Y . We may solve Z by second-order methods such as iterated ridge regression, which can be computationally more expensive than the simple gradient descent."}, {"heading": "6.2 EM, density mapping, and density shifting", "text": "Suppose the training data {Yi, i = 1, ..., n} come from a data distribution Pdata(Y ). To understand how the alternating back-propagation algorithm or its EM idealization maps the prior distribution of the latent factors p(Z) to the data distribution Pdata(Y ) by the learned g(Z;W ), we define\nPdata(Z, Y ;W ) = Pdata(Y )p(Z|Y,W ) = Pdata(Z;W )Pdata(Y |Z,W ), (7)\nwhere Pdata(Z;W ) = \u222b p(Z|Y,W )Pdata(Y )dY is obtained by averaging the posteriors p(Z|Y ;W ) over the observed data Y \u223c Pdata. That is, Pdata(Z;W ) can be considered the data prior. The data prior Pdata(Z;W ) is close to the true prior p(Z) in the sense that\nKL(Pdata(Z;W )|p(Z)) \u2264 KL(Pdata(Y )|p(Y ;W )) (8) = KL(Pdata(Z, Y ;W )|p(Z, Y ;W )).\nThe right hand side of (8) is minimized at the maximum likelihood estimate W\u0302 , hence the data prior Pdata(Z; W\u0302 ) at W\u0302 should be especially close to the true prior p(Z). In other words, at W\u0302 , the posteriors p(Z|Y, W\u0302 ) of all the data points Y \u223c Pdata tend to pave the true prior p(Z).\nFrom Rubin\u2019s multiple imputation point of view (Rubin, 2004) of the EM algorithm, the E-step of EM infers Z\n(m) i \u223c p(Zi|Yi,Wt) for m = 1, ...,M , where M is the number of multiple imputations or multiple guesses of Zi. The multiple guesses account for the uncertainty in inferring Zi from Yi. The M-step of EM maximizes Q(W ) =\u2211n i=1 \u2211M m=1 log p(Yi, Z (m) i ;W ) to obtain Wt+1. For each data point Yi,Wt+1 seeks to reconstruct Yi by g(Z;W ) from the inferred latent factors {Z(m)i ,m = 1, ...,M}. In other words, the M-step seeks to map {Z(m)i } to Yi. Pooling over all i = 1, ..., n, {Z(m)i ,\u2200i,m} \u223c Pdata(Z;Wt), hence the\nM-step seeks to map Pdata(Z;Wt) to the data distribution Pdata(Y ). Of course the mapping from {Z(m)i } to Yi cannot be exact. In fact, g(Z;W ) maps {Z(m)i } to a d-dimensional patch around the D-dimensional Yi. The local patches for all {Yi,\u2200i} patch up the d-dimensional manifold form by the D-dimensional observed examples and their interpolations. The EM algorithm is a process of density shifting, so that Pdata(Z;W ) shifts towards p(Z), thus g(Z;W ) maps p(Z) to Pdata(Y )."}, {"heading": "6.3 Factor analysis and alternating regression", "text": "The alternating back-propagation algorithm is inspired by Rubin-Thayer EM algorithm for factor analysis, where both the observed data model p(Y |W ) and the posterior distribution p(Z|Y,W ) are available in closed form. The EM algorithm for factor analysis can be interpreted as alternating linear regression (Rubin and Thayer, 1982; Liu, Rubin, and Wu, 1998).\nIn the factor analysis model Z \u223c N(0, Id), Y = WZ + , \u223c N(0, \u03c32ID). The joint distribution of (Z, Y ) is[\nZ Y\n] \u223c N ([ 0 0 ] , [ Id W >\nW WW> + \u03c32ID\n]) . (9)\nDenote\nS = [ SZZ SZY SY Z SY Y ] = [ E[ZZ>] E[ZY >] E[Y Z>] E[Y Y >] ] = [ Id W >\nW WW> + \u03c32ID\n] . (10)\nThe posterior distribution p(Z|Y,W ) can be obtained by linear regression of Z on Y , [Z|Y,W ] \u223c N(\u03b2Y, V ), where\n\u03b2 = SZY S \u22121 Y Y , (11) V = SZZ \u2212 SZY S\u22121Y Y SY Z . (12)\nThe above computation can be carried out by the sweep operator on S, with SY Y being the pivotal matrix.\nSuppose we have observations {Yi, i = 1, ..., n}. In the E-step, we compute\nE[Zi|Yi,W ] = \u03b2Yi, (13) E[ZiZ > i |Yi,W ] = V + \u03b2YiY >i \u03b2>. (14)\nIn the M-step, we compute\nS = [ SZZ SZY SY Z SY Y ] = [\u2211n i=1 E[ZiZ > i ]/n \u2211n i=1 E[Zi]Y > i /n\u2211n\ni=1 YiE[Zi] >/n\n\u2211n i=1 YiY > i /n\n] , (15)\nwhere we use E[Zi] and E[ZiZ>i ] to denote the conditional expectations in (13) and (14). Then we regress Y on Z to obtain the coefficient vector and residual variance-covariance matrix\nW = SY ZS \u22121 ZZ (16) \u03a3 = SY Y \u2212 SY ZS\u22121ZZSZY . (17)\nIf \u03c32 is unknown, it can be obtained by averaging the diagonal elements of \u03a3. The computation can again be done by the sweep operator on S, with SZZ being the pivotal matrix.\nThe E-step is based on the multivariate linear regression of Z on Y given W . The M-step updates W by the multivariate linear regression of Y on Z. Both steps can be accomplished by the sweep operator. We use the notation S and S for the Gram matrices to highlight the analogy between the two steps. The EM algorithm can then be considered alternating linear regression or alternating sweep operation, which serves as a prototype for alternating back-propagation."}], "references": [{"title": "Robust uncertainty principles: Exact signal reconstruction from highly incomplete frequency information", "author": ["E.J. Cand\u00e8s", "J. Romberg", "T. Tao"], "venue": "IEEE Transactions on information theory 52(2):489\u2013509.", "citeRegEx": "Cand\u00e8s et al\\.,? 2006", "shortCiteRegEx": "Cand\u00e8s et al\\.", "year": 2006}, {"title": "Maximum likelihood from incomplete data via the em algorithm", "author": ["A.P. Dempster", "N.M. Laird", "D.B. Rubin"], "venue": "Journal of the Royal Statistical Society: B 1\u201338.", "citeRegEx": "Dempster et al\\.,? 1977", "shortCiteRegEx": "Dempster et al\\.", "year": 1977}, {"title": "Deep generative image models using a laplacian pyramid of adversarial networks", "author": ["E.L. Denton", "S. Chintala", "R Fergus"], "venue": null, "citeRegEx": "Denton et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Denton et al\\.", "year": 2015}, {"title": "Density estimation using real nvp", "author": ["L. Dinh", "J. Sohl-Dickstein", "S. Bengio"], "venue": "CoRR abs/1605.08803.", "citeRegEx": "Dinh et al\\.,? 2016", "shortCiteRegEx": "Dinh et al\\.", "year": 2016}, {"title": "Dynamic textures", "author": ["G. Doretto", "A. Chiuso", "Y. Wu", "S. Soatto"], "venue": "IJCV 51(2):91\u2013109.", "citeRegEx": "Doretto et al\\.,? 2003", "shortCiteRegEx": "Doretto et al\\.", "year": 2003}, {"title": "Learning to generate chairs with convolutional neural networks", "author": ["E. Dosovitskiy", "J.T. Springenberg", "T. Brox"], "venue": "CVPR.", "citeRegEx": "Dosovitskiy et al\\.,? 2015", "shortCiteRegEx": "Dosovitskiy et al\\.", "year": 2015}, {"title": "Riemann manifold langevin and hamiltonian monte carlo methods", "author": ["M. Girolami", "B. Calderhead"], "venue": "Journal of the Royal Statistical Society: B 73(2):123\u2013214.", "citeRegEx": "Girolami and Calderhead,? 2011", "shortCiteRegEx": "Girolami and Calderhead", "year": 2011}, {"title": "Generative adversarial nets", "author": ["I. Goodfellow", "J. Pouget-Abadie", "M. Mirza", "B. Xu", "D. WardeFarley", "S. Ozair", "A. Courville", "Y. Bengio"], "venue": "NIPS, 2672\u20132680.", "citeRegEx": "Goodfellow et al\\.,? 2014", "shortCiteRegEx": "Goodfellow et al\\.", "year": 2014}, {"title": "Independent component analysis", "author": ["A. Hyv\u00e4rinen", "J. Karhunen", "E. Oja"], "venue": "John Wiley & Sons.", "citeRegEx": "Hyv\u00e4rinen et al\\.,? 2004", "shortCiteRegEx": "Hyv\u00e4rinen et al\\.", "year": 2004}, {"title": "Batch normalization: Accelerating deep network training by reducing internal covariate shift", "author": ["S. Ioffe", "C. Szegedy"], "venue": "ICML.", "citeRegEx": "Ioffe and Szegedy,? 2015", "shortCiteRegEx": "Ioffe and Szegedy", "year": 2015}, {"title": "Nonnegative matrix factorization based on alternating nonnegativity constrained least squares and active set method", "author": ["H. Kim", "H. Park"], "venue": "SIAM Journal on Matrix Analysis and Applications 30(2):713\u2013730.", "citeRegEx": "Kim and Park,? 2008", "shortCiteRegEx": "Kim and Park", "year": 2008}, {"title": "Auto-encoding variational bayes", "author": ["D.P. Kingma", "M. Welling"], "venue": "ICLR.", "citeRegEx": "Kingma and Welling,? 2014", "shortCiteRegEx": "Kingma and Welling", "year": 2014}, {"title": "Matrix factorization techniques for recommender systems", "author": ["Y. Koren", "R. Bell", "C. Volinsky"], "venue": "Computer 42(8):30\u201337.", "citeRegEx": "Koren et al\\.,? 2009", "shortCiteRegEx": "Koren et al\\.", "year": 2009}, {"title": "Imagenet classification with deep convolutional neural networks", "author": ["A. Krizhevsky", "I. Sutskever", "G.E. Hinton"], "venue": "NIPS, 1097\u20131105.", "citeRegEx": "Krizhevsky et al\\.,? 2012", "shortCiteRegEx": "Krizhevsky et al\\.", "year": 2012}, {"title": "Gradient-based learning applied to document recognition", "author": ["Y. LeCun", "L. Bottou", "Y. Bengio", "P. Haffner"], "venue": "Proceedings of the IEEE 86(11):2278\u20132324.", "citeRegEx": "LeCun et al\\.,? 1998", "shortCiteRegEx": "LeCun et al\\.", "year": 1998}, {"title": "Algorithms for nonnegative matrix factorization", "author": ["D.D. Lee", "H.S. Seung"], "venue": "NIPS, 556\u2013562.", "citeRegEx": "Lee and Seung,? 2001", "shortCiteRegEx": "Lee and Seung", "year": 2001}, {"title": "Deep learning face attributes in the wild", "author": ["Z. Liu", "P. Luo", "X. Wang", "X. Tang"], "venue": "ICCV, 3730\u20133738.", "citeRegEx": "Liu et al\\.,? 2015", "shortCiteRegEx": "Liu et al\\.", "year": 2015}, {"title": "Parameter expansion to accelerate em: The px-em algorithm", "author": ["C. Liu", "D.B. Rubin", "Y.N. Wu"], "venue": "Biometrika 85(4):755\u2013770.", "citeRegEx": "Liu et al\\.,? 1998", "shortCiteRegEx": "Liu et al\\.", "year": 1998}, {"title": "Learning FRAME models using CNN filters", "author": ["Y. Lu", "S.-C. Zhu", "Y.N. Wu"], "venue": "AAAI.", "citeRegEx": "Lu et al\\.,? 2016", "shortCiteRegEx": "Lu et al\\.", "year": 2016}, {"title": "Rectifier nonlinearities improve neural network acoustic models", "author": ["A.L. Maas", "A.Y. Hannun", "A.Y. Ng"], "venue": "ICML.", "citeRegEx": "Maas et al\\.,? 2013", "shortCiteRegEx": "Maas et al\\.", "year": 2013}, {"title": "Sound texture perception via statistics of the auditory periphery: evidence from sound synthesis", "author": ["J.H. McDermott", "E.P. Simoncelli"], "venue": "Neuron 71(5):926\u2013940.", "citeRegEx": "McDermott and Simoncelli,? 2011", "shortCiteRegEx": "McDermott and Simoncelli", "year": 2011}, {"title": "Neural variational inference and learning in belief networks", "author": ["A. Mnih", "K. Gregor"], "venue": "ICML.", "citeRegEx": "Mnih and Gregor,? 2014", "shortCiteRegEx": "Mnih and Gregor", "year": 2014}, {"title": "Mcmc using hamiltonian dynamics", "author": ["R.M. Neal"], "venue": "Handbook of Markov Chain Monte Carlo 2.", "citeRegEx": "Neal,? 2011", "shortCiteRegEx": "Neal", "year": 2011}, {"title": "Sparse coding with an overcomplete basis set: A strategy employed by v1? Vision Research 37(23):3311\u20133325", "author": ["B.A. Olshausen", "D.J. Field"], "venue": null, "citeRegEx": "Olshausen and Field,? \\Q1997\\E", "shortCiteRegEx": "Olshausen and Field", "year": 1997}, {"title": "On the number of response regions of deep feed forward networks with piece-wise linear activations", "author": ["R. Pascanu", "G. Montufar", "Y. Bengio"], "venue": "arXiv:1312.6098.", "citeRegEx": "Pascanu et al\\.,? 2013", "shortCiteRegEx": "Pascanu et al\\.", "year": 2013}, {"title": "Unsupervised representation learning with deep convolutional generative adversarial networks", "author": ["A. Radford", "L. Metz", "S. Chintala"], "venue": "ICLR.", "citeRegEx": "Radford et al\\.,? 2016", "shortCiteRegEx": "Radford et al\\.", "year": 2016}, {"title": "Stochastic backpropagation and approximate inference in deep generative models", "author": ["D.J. Rezende", "S. Mohamed", "D. Wierstra"], "venue": "NIPS, 1278\u20131286.", "citeRegEx": "Rezende et al\\.,? 2014", "shortCiteRegEx": "Rezende et al\\.", "year": 2014}, {"title": "Nonlinear dimensionality reduction by locally linear embedding", "author": ["S.T. Roweis", "L.K. Saul"], "venue": "Science 290(5500):2323\u20132326.", "citeRegEx": "Roweis and Saul,? 2000", "shortCiteRegEx": "Roweis and Saul", "year": 2000}, {"title": "Em algorithms for ml factor analysis", "author": ["D.B. Rubin", "D.T. Thayer"], "venue": "Psychometrika 47(1):69\u201376.", "citeRegEx": "Rubin and Thayer,? 1982", "shortCiteRegEx": "Rubin and Thayer", "year": 1982}, {"title": "Multiple imputation for nonresponse in surveys, volume 81", "author": ["D.B. Rubin"], "venue": "John Wiley & Sons.", "citeRegEx": "Rubin,? 2004", "shortCiteRegEx": "Rubin", "year": 2004}, {"title": "Matconvnet \u2013 convolutional neural networks for matlab", "author": ["A. Vedaldi", "K. Lenc"], "venue": "Int. Conf. on Multimedia.", "citeRegEx": "Vedaldi and Lenc,? 2015", "shortCiteRegEx": "Vedaldi and Lenc", "year": 2015}, {"title": "Extracting and composing robust features with denoising autoencoders", "author": ["P. Vincent", "H. Larochelle", "Y. Bengio", "P.A. Manzagol"], "venue": "ICML, 1096\u20131103.", "citeRegEx": "Vincent et al\\.,? 2008", "shortCiteRegEx": "Vincent et al\\.", "year": 2008}, {"title": "Modeling textured motion: Particle, wave and sketch", "author": ["Y. Wang", "Zhu", "S.-C."], "venue": "ICCV, 213\u2013220.", "citeRegEx": "Wang et al\\.,? 2003", "shortCiteRegEx": "Wang et al\\.", "year": 2003}, {"title": "A theory of generative convnet", "author": ["J. Xie", "Y. Lu", "S.-C. Zhu", "Y.N. Wu"], "venue": "ICML.", "citeRegEx": "Xie et al\\.,? 2016", "shortCiteRegEx": "Xie et al\\.", "year": 2016}, {"title": "Empirical evaluation of rectified activations in convolutional network", "author": ["B. Xu", "N. Wang", "T. Chen", "M. Li"], "venue": "CoRR abs/1505.00853.", "citeRegEx": "Xu et al\\.,? 2015", "shortCiteRegEx": "Xu et al\\.", "year": 2015}, {"title": "On the convergence of markovian stochastic algorithms with rapidly decreasing ergodicity rates", "author": ["L. Younes"], "venue": "Stochastics: An International Journal of Probability and Stochastic Processes 65(3-4):177\u2013228.", "citeRegEx": "Younes,? 1999", "shortCiteRegEx": "Younes", "year": 1999}], "referenceMentions": [{"referenceID": 7, "context": "This paper studies the fundamental problem of learning and inference in the generator network (Goodfellow et al., 2014), which is a generative model that has become popular recently.", "startOffset": 94, "endOffset": 119}, {"referenceID": 23, "context": "This led to methods such as independent component analysis (Hyv\u00e4rinen, Karhunen, and Oja, 2004), sparse coding (Olshausen and Field, 1997), non-negative matrix factorization (Lee and Seung, 2001), matrix factorization and completion for recommender systems (Koren, Bell, and Volinsky, 2009), etc.", "startOffset": 111, "endOffset": 138}, {"referenceID": 15, "context": "This led to methods such as independent component analysis (Hyv\u00e4rinen, Karhunen, and Oja, 2004), sparse coding (Olshausen and Field, 1997), non-negative matrix factorization (Lee and Seung, 2001), matrix factorization and completion for recommender systems (Koren, Bell, and Volinsky, 2009), etc.", "startOffset": 174, "endOffset": 195}, {"referenceID": 14, "context": "a convolutional neural network (ConvNet or CNN) (LeCun et al., 1998; Krizhevsky, Sutskever, and Hinton, 2012; Dosovitskiy, Springenberg, and Brox, 2015).", "startOffset": 48, "endOffset": 152}, {"referenceID": 2, "context": "It has been shown recently that the generator network is capable of generating realistic images (Denton et al., 2015; Radford, Metz, and Chintala, 2016).", "startOffset": 96, "endOffset": 152}, {"referenceID": 28, "context": "The factor analysis model can be learned by the Rubin-Thayer EM algorithm (Rubin and Thayer, 1982; Dempster, Laird, and Rubin, 1977), where both the E-step and the M-step are based on multivariate linear regression.", "startOffset": 74, "endOffset": 132}, {"referenceID": 22, "context": "The Langevin dynamics (Neal, 2011) is a stochastic sampling counterpart of gradient descent.", "startOffset": 22, "endOffset": 34}, {"referenceID": 10, "context": "The alternating back-propagation algorithm follows the tradition of alternating operations in unsupervised learning, such as alternating linear regression in the EM algorithm for factor analysis, alternating least squares algorithm for matrix factorization (Koren, Bell, and Volinsky, 2009; Kim and Park, 2008), and alternating gradient descent algorithm for sparse coding (Olshausen and Field, 1997).", "startOffset": 257, "endOffset": 310}, {"referenceID": 23, "context": "The alternating back-propagation algorithm follows the tradition of alternating operations in unsupervised learning, such as alternating linear regression in the EM algorithm for factor analysis, alternating least squares algorithm for matrix factorization (Koren, Bell, and Volinsky, 2009; Kim and Park, 2008), and alternating gradient descent algorithm for sparse coding (Olshausen and Field, 1997).", "startOffset": 373, "endOffset": 400}, {"referenceID": 4, "context": "For instance, in textured motions (Wang and Zhu, 2003) or dynamic textures (Doretto et al., 2003), the latent factors may follow a dynamic model such as vector auto-regression.", "startOffset": 75, "endOffset": 97}, {"referenceID": 11, "context": "One method is variational auto-encoder (VAE) (Kingma and Welling, 2014; Rezende, Mohamed, and Wierstra, 2014; Mnih and Gregor, 2014), where the assisting network is an inferential or recognition network that seeks to approximate the posterior distribution of the latent factors.", "startOffset": 45, "endOffset": 132}, {"referenceID": 21, "context": "One method is variational auto-encoder (VAE) (Kingma and Welling, 2014; Rezende, Mohamed, and Wierstra, 2014; Mnih and Gregor, 2014), where the assisting network is an inferential or recognition network that seeks to approximate the posterior distribution of the latent factors.", "startOffset": 45, "endOffset": 132}, {"referenceID": 7, "context": "The other method is the generative adversarial network (GAN) (Goodfellow et al., 2014; Denton et al., 2015; Radford, Metz, and Chintala, 2016), where the assisting network is a discriminator network that plays an adversarial role against the generator network.", "startOffset": 61, "endOffset": 142}, {"referenceID": 2, "context": "The other method is the generative adversarial network (GAN) (Goodfellow et al., 2014; Denton et al., 2015; Radford, Metz, and Chintala, 2016), where the assisting network is a discriminator network that plays an adversarial role against the generator network.", "startOffset": 61, "endOffset": 142}, {"referenceID": 33, "context": "One can also obtain a probabilistic model based on a bottomup ConvNet that defines descriptive features (Xie et al., 2016; Lu, Zhu, and Wu, 2016).", "startOffset": 104, "endOffset": 145}, {"referenceID": 28, "context": "The factor analysis model can be learned by the RubinThayer EM algorithm, which involves alternating regressions of Z on Y in the E-step and of Y on Z in the M-step, with both steps powered by the sweep operator (Rubin and Thayer, 1982; Liu, Rubin, and Wu, 1998).", "startOffset": 212, "endOffset": 262}, {"referenceID": 23, "context": "(Olshausen and Field, 1997), d > D, and Z is assumed to be a redundant but sparse vector, i.", "startOffset": 0, "endOffset": 27}, {"referenceID": 15, "context": "(3) Non-negative matrix factorization (Lee and Seung, 2001), it is assumed that zk \u2265 0.", "startOffset": 38, "endOffset": 59}, {"referenceID": 7, "context": "In this paper, we consider the generator network model (Goodfellow et al., 2014) that retains the assumptions that d < D, Z \u223c N(0, Id), and \u223c N(0, \u03c3ID) as in traditional factor analysis, but generalizes the linear mapping WZ to a non-linear mapping f(Z;W ), where f is a ConvNet, and W collects all the connection weights and bias terms of the ConvNet.", "startOffset": 55, "endOffset": 80}, {"referenceID": 35, "context": "The stochastic gradient algorithm of (Younes, 1999) can be used for learning, where in each iteration, for each Zi, only a single copy of Zi is sampled from p(Zi|Yi,W ) by running a finite number of steps of Langevin dynamics starting from the current value of Zi, i.", "startOffset": 37, "endOffset": 51}, {"referenceID": 22, "context": "The Langevin dynamics can be extended to Hamiltonian Monte Carlo (Neal, 2011) or more sophisticated versions (Girolami and Calderhead, 2011).", "startOffset": 65, "endOffset": 77}, {"referenceID": 6, "context": "The Langevin dynamics can be extended to Hamiltonian Monte Carlo (Neal, 2011) or more sophisticated versions (Girolami and Calderhead, 2011).", "startOffset": 109, "endOffset": 140}, {"referenceID": 30, "context": "The code in our experiments is based on the MatConvNet package of (Vedaldi and Lenc, 2015).", "startOffset": 66, "endOffset": 90}, {"referenceID": 9, "context": "We also adopt batch normalization (Ioffe and Szegedy, 2015).", "startOffset": 34, "endOffset": 59}, {"referenceID": 20, "context": "A sound signal can be treated as a one-dimensional texture image (McDermott and Simoncelli, 2011).", "startOffset": 65, "endOffset": 97}, {"referenceID": 34, "context": "2 (Maas, Hannun, and Ng, 2013; Xu et al., 2015).", "startOffset": 2, "endOffset": 47}, {"referenceID": 16, "context": "In the second experiment, we learn a model with d = 100 from 1000 face images randomly selected from the CelebA dataset (Liu et al., 2015).", "startOffset": 120, "endOffset": 138}, {"referenceID": 7, "context": "We also provide qualitative comparison with Deep Convolutional Generative Adversarial Net (DCGAN) (Goodfellow et al., 2014; Radford, Metz, and Chintala, 2016).", "startOffset": 98, "endOffset": 158}, {"referenceID": 4, "context": "We model a textured motion (Wang and Zhu, 2003) or a dynamic texture (Doretto et al., 2003) by a non-linear dynamic system", "startOffset": 69, "endOffset": 91}, {"referenceID": 4, "context": "This model is a direct generalization of the linear dynamic system of (Doretto et al., 2003), where Yt is reduced to Zt by principal component analysis (PCA) via singular value decomposition (SVD).", "startOffset": 70, "endOffset": 92}, {"referenceID": 4, "context": "(2) Treat {Zt} as the training data, learn A and Q as in (Doretto et al., 2003).", "startOffset": 57, "endOffset": 79}, {"referenceID": 4, "context": "The first row is a segment of the sequence generated by our model, and the second row is generated by the method of (Doretto et al., 2003), with the same dimensionality of Z.", "startOffset": 116, "endOffset": 138}, {"referenceID": 4, "context": "Row 2: a sequence by the method of (Doretto et al., 2003).", "startOffset": 35, "endOffset": 57}, {"referenceID": 31, "context": "Our experiments are different from (1) de-noising auto-encoder (Vincent et al., 2008), where the training images are fully observed, and noises are added as a matter of regularization, (2) in-painting or de-noising, where the prior model or regularization has already been learned or given.", "startOffset": 63, "endOffset": 85}, {"referenceID": 34, "context": "The element-wise non-linearity fl in modern ConvNet is usually the two-piece linearity, such as rectified linear unit (ReLU) (Krizhevsky, Sutskever, and Hinton, 2012) or the leaky ReLU (Maas, Hannun, and Ng, 2013; Xu et al., 2015).", "startOffset": 185, "endOffset": 230}, {"referenceID": 27, "context": "The generator model can be considered an explicit implementation of the local linear embedding (Roweis and Saul, 2000), where Z is the embedding of Y .", "startOffset": 95, "endOffset": 118}, {"referenceID": 29, "context": "From Rubin\u2019s multiple imputation point of view (Rubin, 2004) of the EM algorithm, the E-step of EM infers", "startOffset": 47, "endOffset": 60}, {"referenceID": 28, "context": "The EM algorithm for factor analysis can be interpreted as alternating linear regression (Rubin and Thayer, 1982; Liu, Rubin, and Wu, 1998).", "startOffset": 89, "endOffset": 139}], "year": 2016, "abstractText": "This paper proposes an alternating back-propagation algorithm for learning the generator network model. The model is a nonlinear generalization of factor analysis. In this model, the mapping from the continuous latent factors to the observed signal is parametrized by a convolutional neural network. The alternating back-propagation algorithm iterates the following two steps: (1) Inferential back-propagation, which infers the latent factors by Langevin dynamics or gradient descent. (2) Learning back-propagation, which updates the parameters given the inferred latent factors by gradient descent. The gradient computations in both steps are powered by back-propagation, and they share most of their code in common. We show that the alternating back-propagation algorithm can learn realistic generator models of natural images, video sequences, and sounds. Moreover, it can also be used to learn from incomplete or indirect training data.", "creator": "LaTeX with hyperref package"}}}