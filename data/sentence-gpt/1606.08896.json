{"id": "1606.08896", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "28-Jun-2016", "title": "On the Semantic Relationship between Probabilistic Soft Logic and Markov Logic", "abstract": "Markov Logic Networks (MLN) and Probabilistic Soft Logic (PSL) are widely applied formalisms in Statistical Relational Learning, an emerging area in Artificial Intelligence that is concerned with combining logical and statistical AI. Despite their resemblance, the relationship has not been formally stated. In this paper, we describe the precise semantic relationship between them from a logical perspective. This is facilitated by first extending fuzzy logic to allow weights, which can be also viewed as a generalization of PSL, and then relate that generalization to MLN. We observe that the relationship between PSL and MLN is analogous to the known relationship between fuzzy logic and Boolean logic, and furthermore the weight scheme of PSL is essentially a generalization of the weight scheme of MLN for the many-valued setting. The underlying concept is that, given the generalization of the weights, MLN is the basis of the weight scheme of MLN for all of the possible possible values (for example, for a given value, a weight = 1). Using NP Theory, we can observe that, given the weight scheme of MLN, the weights can be used to perform the functions of these parameters, to approximate the weights and hence its relation to PSL. This is explained in a chapter 2.2.2.\n\n\nThe MLN weight scheme of MLN is a generalization of PSL, the common structure of the MLN-based weight scheme of MLN. These generalizations can also be interpreted as following (1) The MLN-based weight scheme of MLN is a generalization of the weight scheme of MLN. This is expressed in the following terms.\nThe MLN-based weight scheme of MLN is a generalization of MLN. These generalizations can also be viewed as following (2) The MLN-based weight scheme of MLN is a generalization of MLN. This is expressed in the following terms.\nThe MLN-based weight scheme of MLN is a generalization of MLN. These generalizations can also be viewed as following (3) The MLN-based weight scheme of MLN is a generalization of MLN. This is expressed in the following terms.\nThe MLN-based weight scheme of MLN is a generalization of MLN. This is expressed in the following terms.\nThe MLN-based weight scheme of MLN is a generalization of MLN. This is expressed in the following terms.\nThe MLN-based weight", "histories": [["v1", "Tue, 28 Jun 2016 21:43:19 GMT  (22kb)", "http://arxiv.org/abs/1606.08896v1", "In Working Notes of the 6th International Workshop on Statistical Relational AI"]], "COMMENTS": "In Working Notes of the 6th International Workshop on Statistical Relational AI", "reviews": [], "SUBJECTS": "cs.AI", "authors": ["joohyung lee", "yi wang"], "accepted": false, "id": "1606.08896"}, "pdf": {"name": "1606.08896.pdf", "metadata": {"source": "CRF", "title": null, "authors": ["Yi Wang"], "emails": ["joolee@asu.edu", "ywang485@asu.edu"], "sections": [{"heading": null, "text": "ar X\niv :1\n60 6.\n08 89\n6v 1\n[ cs\n.A I]\n2 8\nJu n\n20 16"}, {"heading": "Introduction", "text": "Statistical relational learning (SRL) is an emerging area in Artificial Intelligence that is concerned with combining logical and statistical AI. Markov Logic Networks (MLN) (Richardson and Domingos 2006) and Probabilistic Soft Logic (PSL) (Kimmig et al. 2012; Bach et al. 2015) are well-known formalisms in statistical relational learning, and have been successfully applied to a wide range of AI applications, such as natural language processing, entity resolution, collective classification, and social network modeling. Both of them combine logic and probabilistic graphical model in a single representation, where each formula is associated with a weight, and the probability distribution over possible worlds is derived from the weights of the formulas that are satisfied by the possible worlds. However, despite their resemblance to each other, the precise relationship between their semantics is not obvious. PSL is based on fuzzy interpretations that range over reals in [0, 1], and in this sense is more general than MLN. On the other hand, its syntax is restricted to formulas in clausal form, unlike MLN that allows any complex formulas. It is also not obvious how their models\u2019 weights are related to each other due to the different ways that the weights are associated with models. Originating from the machine learning research, these formalisms\n1In Working Notes of the 6th International Workshop on Statistical Relational AI (StarAI 2016)\nare equipped with several efficient inference and learning algorithms, and some paper compares the suitability of one formalism over the other by experiments on specific applications (Beltagy, Erk, and Mooney 2014). On the other hand, the precise relationship between the two formalisms has not been formally stated.\nIn this paper, we present a precise semantic relationship between them. We observe that the relationship is analogous to the well-known relationship between fuzzy logic and classical logic. Moreover, despite the different ways that weights of models are defined in each formalism, it turns out that they are essentially of the same kind. Towards this end, we introduce a weighted fuzzy logic as a proper generalization of PSL, which is also interesting on its own as an extension of the standard fuzzy logic to incorporate weighted models. The weighted fuzzy logic uses the same weight scheme as PSL, but associates weights to arbitrary fuzzy formulas. This intermediate formalism facilitates the comparison between PSL and MLN. We observe that the same analogy between fuzzy logic and Boolean logic carries over to between PSL and MLN. Analogous to that fuzzy logic agrees with Boolean logic on crisp interpretations, PSL and MLN agree on crisp interpretations, where their weights are proportional to each other. However, their maximum a posteriori (MAP) estimates do not necessarily coincide due to the differences between many-valued vs. Boolean models.\nThe paper is organized as follows. We first review each of MLN, fuzzy propositional logic, and PSL. Then we define a weighted fuzzy logic as a generalization of PSL. Using this we study the semantic relationship between PSL and MLN."}, {"heading": "Preliminaries", "text": "Although both PSL and MLN allow atoms to contain variables, those variables are understood in terms of grounding over finite domains where a universally quantified sentence is turned into multiple conjunctions and an existentially quantified sentence is turned into multiple disjunctions, essentially resulting in propositional theories. For example, the ground atoms of the first-order signature \u03c3 = {p, a, b}, where p is a unary predicate constant and a, b are object constants, can be identified with the propositional atoms of the propositional signature {p(a), p(b)}. Thus for simplicity but without losing generality, we assume that the programs are\npropositional.2"}, {"heading": "Review: Markov Logic Networks", "text": "The following is a review of Markov Logic from (Richardson and Domingos 2006). A Markov Logic Network (MLN) L of a propositional signature \u03c3 is a finite set of pairs \u3008w : F \u3009, where F is a propositional formula of \u03c3 and w is a real number.\nFor any MLN L of signature \u03c3, we define LI to be the set of weighted formulas w : F in L such that I |= F . The unnormalized weight of an interpretation I under L is defined as\nWL(I) = exp\n(\n\u2211\nw:F\u2208LI\nw\n)\n,\nand the normalized weight (a.k.a. probability) of I under L is defined as\nPL(I) = WL(I) \u2211\nJ\u2208PW WL(J) ,\nwhere PW (\u201cPossible Worlds\u201d) is the set of all interpretations of \u03c3.\nThe basic idea of Markov Logic is to allow formulas to be soft constrained, where a model does not have to satisfy all formulas, but is associated with the weight that is obtained from the satisfied formulas. An interpretation that does not satisfy certain formulas receives an \u201c(indirect) penalty\u201d because such formulas do not contribute to the weight of that interpretation."}, {"heading": "Review: Fuzzy Propositional Formula", "text": "The following is a review of fuzzy propositional formulas from (Hajek 1998). A fuzzy propositional signature \u03c3 is a set of symbols called fuzzy atoms. In addition, we assume the presence of a set CONJ of fuzzy conjunction symbols, a set DISJ of fuzzy disjunction symbols, a set NEG of fuzzy negation symbols, and a set IMPL of fuzzy implication symbols.\nA fuzzy (propositional) formula of \u03c3 is defined recursively as follows.\n\u2022 every fuzzy atom p \u2208 \u03c3 is a fuzzy formula;\n\u2022 every numeric constant c, where c is a real number in [0, 1], is a fuzzy formula;\n\u2022 if F is a fuzzy formula, then \u00acF is a fuzzy formula, where \u00ac \u2208 NEG;\n\u2022 if F and G are fuzzy formulas, then F \u2297 G, F \u2295G, and F \u2192 G are fuzzy formulas, where \u2297 \u2208 CONJ, \u2295 \u2208 DISJ, and \u2192 \u2208 IMPL.\nThe models of a fuzzy formula are defined as follows. The fuzzy truth values are the real numbers in the range [0, 1]. A fuzzy interpretation I of \u03c3 is a mapping from \u03c3 into [0, 1].\n2Inference and learning algorithms in these languages indeed utilize the relational structure, but in terms of defining the semantics, the assumption simplifies the presentation without the need to refer to fuzzy predicate logic.\nThe fuzzy operators are functions mapping one or a pair of truth values into a truth value. Among the operators,\u00ac denotes a function from [0, 1] into [0, 1]; \u2297, \u2295, and \u2192 denote functions from [0, 1]\u00d7 [0, 1] into [0, 1]. The actual mapping performed by each operator can be defined in many different ways, but all of them satisfy the properties that they are generalizations of the corresponding Boolean connectives. Figure 1 lists some examples of fuzzy operators.\nThe truth value of a fuzzy propositional formula F under I , denoted \u03c5I(F ), is defined recursively as follows: \u2022 for any atom p \u2208 \u03c3, \u03c5I(p) = I(p);\n\u2022 for any numeric constant c, \u03c5I(c) = c;\n\u2022 \u03c5I(\u00acF ) = \u00ac(\u03c5I(F ));\n\u2022 \u03c5I(F \u2299G) = \u2299(\u03c5I(F ), \u03c5I(G)) (\u2299 \u2208 {\u2297,\u2295,\u2192}). (For simplicity, we identify the symbols for the fuzzy operators with the truth value functions represented by them.)\nDefinition 1 We say that a fuzzy interpretation I satisfies a fuzzy formula F if \u03c5I(F ) = 1, and denote it by I |= F . We call such I a fuzzy model of F .\nWe say that a fuzzy interpretation I is Boolean if I(p) is either 0 or 1 for each fuzzy atom p. Clearly, we may identify a Boolean fuzzy interpretation I with the classical propositional interpretation by identifying 1 with TRUE and 0 with FALSE.\nAny fuzzy propositional formula whose numeric constants are restricted to 0 and 1 can be identified with a classical propositional formula. For such a formula F , due to the fact that fuzzy operators are generalizations of their Boolean counterparts, it is clear that Boolean fuzzy models of F are precisely the Boolean models of F when F is viewed as a classical propositional formula."}, {"heading": "Review: Probabilistic Soft Logic", "text": "The following is a review of PSL from (Kimmig et al. 2012), but is stated using the terminology from fuzzy logic. A PSL program \u03a0 is a set of weighted formulas \u3008w : R \u02c6k\u3009 where \u2022 w is a nonnegative real number,\n\u2022 R is a fuzzy propositional formula of the form 3\na \u2190l b1 \u2297l . . .\u2297l bn (1) 3We understand G\u2190 F as an alternative notation for F \u2192 G.\nwhere n \u2265 0, each of a, b1, . . . , bn is a fuzzy atom possibly preceded by the standard negator, and\n\u2022 k \u2208 {1, 2}.4\nFor each rule R of the form (1), the distance to satisfaction under interpretation I is defined as\ndR(I) = max{0, \u03c5I(b1 \u2297l \u00b7 \u00b7 \u00b7 \u2297l bn)\u2212 \u03c5I(a)}. (2)\nGiven an interpretation I of \u03a0, the unnormalized density function over I under \u03a0 is defined as\nf\u0302\u03a0(I) = exp\n(\n\u2212 \u2211\n\u3008w:R\u02c6k\u3009\u2208\u03a0\nw \u00b7 dR(I) k\n)\n,\nand the probability density function over I under \u03a0 is defined as\nf\u03a0(I) = f\u0302\u03a0(I)\nZ\u03a0 ,\nwhere Z\u03a0 is the normalization factor \u222b\nI\nf\u0302\u03a0(I).\nThe probability density function f\u03a0(I) is defined similar to the weight WL(I) in MLN. Different from MLN where the weight of an interpretation comes from the sum over the weights of all formulas that are satisfied (thus the penalty is implicit), in PSL, the probability density function of an interpretation is obtained from the sum over the \u201cpenalty\u201d (i.e., the weight times the distance to satisfaction) from each formula, where the penalty is 0 when the formula is satisfied, and becomes bigger as the formula gets unsatisfied more (i.e., the fuzzy truth value of the body gets bigger than the fuzzy truth value of the head). When the formula is most unsatisfied (i.e., the body evaluates to 1 and the head evaluates to 0), the penalty is w, the maximum. A novel idea here is that each formula contributes to the penalty to a certain graded truth degree (including 0). Along with the restriction imposed on the syntax of fuzzy formulas (using the rule form (1)), MAP inference in PSL can be reduced to a convex optimization problem in continuous space, thereby enabling efficient computation."}, {"heading": "Weighted Fuzzy Logic as a Generalization of PSL", "text": ""}, {"heading": "Weighted Fuzzy Logic", "text": "Here we define a weighted fuzzy logic as a generalization of PSL. The idea is simple. We take the standard fuzzy logic and extend it by applying the log-linear weight scheme of PSL.\nA weighted propositional fuzzy logic theory \u03a0 is a set of weighted formulas\u3008w : F\u02c6k\u3009, where\n\u2022 w is a real number,\n\u2022 F is a fuzzy propositional formula, and\n4PSL also allows linear equality and inequality constraints, which is outside logical theories, and we omit here for simplicity. Interpretation I that violates any of them gets f\u03a0(I) = 0.\n\u2022 k \u2208 {1, 2}.\nThe unnormalized density function of a fuzzy interpretation I under \u03a0 is defined as\nf\u0302\u03a0(I) = exp\n(\n\u2212 \u2211\n\u3008w:F\u02c6k\u3009\u2208\u03a0\nw \u00b7 (1\u2212 \u03c5I(F )) k\n)\n,\nand the probability density function of I under \u03a0 is defined as\nf\u03a0(I) = f\u0302\u03a0(I)\nZ\u03a0 ,\nwhere Z\u03a0 is the normalization factor \u222b\nI\nf\u0302\u03a0(I).\nNotice that 1 \u2212 \u03c5I(F ) represents the distance to satisfaction in the general case. It is 0 when I satisfies F , and becomes bigger as \u03c5I(F ) gets farther from 1. This notion of distance to satisfaction for an arbitrary formula is also used in Probabilistic Similarity Logic (Bro\u0308cheler, Mihalkova, and Getoor 2010), and indeed, the weighted fuzzy logic is very similar to Probabilistic Similarity Logic. Both of them employ arbitrary fuzzy operators, not restricted to the Lukasiewicz fuzzy operators. However, the languages are not the same. In Probabilistic Similarity Logic, atomic sentences are of the form called similarity statements, A s = B, where s is some similarity measure, and A, B are entities or sets that can even be represented in an object-oriented syntax. On the other hand, atomic sentences of the weighted logic is a fuzzy atom, same as in PSL. As we show below it is easy to view the weighted fuzzy logic as a generalization of PSL, and it serves as a convenient intermediate language to relate PSL and MLN.5"}, {"heading": "Relation to PSL", "text": "The following lemma tells us how the notions of distance to satisfaction in PSL and in the weighted fuzzy logic are related.\nLemma 1 For any rule R of the form (1) and any interpretation I ,\ndR(I) = 1\u2212 \u03c5I(R).\nProof.\n1\u2212 \u03c5I(R)\n= max{0, 1\u2212 \u03c5I(R)}\n= max{0, 1\u2212 \u03c5I(a\u2190l b1 \u2297l . . .\u2297l bn)}\n= max{0, 1\u2212min{1\u2212 \u03c5I(b1 \u2297l \u00b7 \u00b7 \u00b7 \u2297l bn) + \u03c5I(a), 1}}\n= max{0, \u03c5I(b1 \u2297l \u00b7 \u00b7 \u00b7 \u2297l bn)\u2212 \u03c5I(a)})\n= dR(I).\nIn Lemma 1, it is essential that rules (1) use Lukasiewicz fuzzy operators. The lemma does not hold with an arbitrary selection of fuzzy operators as the following example indicates.\n5Although PSL and Probabilistic Similarity Logic seem to be closely related, the formal relationship between them has not been discussed in the literature to the best of our knowledge.\nExample 1 Consider Go\u0308del t-norm \u2297m and its residual implicator \u2192r. Let R be q \u2190r p and I an interpretation {(p, 0.6), (q, 0.4)}. dR(I) is 0.2, while 1 \u2212 \u03c5I(p \u2192r q) is 1\u2212 0.4 = 0.6.\nIt follows from Lemma 1 that PSL can be easily viewed as a special case of the weighted fuzzy logic.\nTheorem 1 Given any PSL program \u03a0 and any fuzzy interpretation I , the definition of f\u03a0(I) when \u03a0 is viewed as the weighted fuzzy logic coincides with the definition of f\u03a0(I) when \u03a0 is viewed as a PSL program.\nProof. Immediate from Lemma 1.\nDue to this theorem, we will call the weighted fuzzy logic also as generalized PSL (GPSL).\nViewing PSL as a special case of the weighted fuzzy logic allows us to apply the mathematical results known from fuzzy logic to the context of PSL. Here is one example, which tells us that the different versions of PSL defined in (Kimmig et al. 2012) and (Bach et al. 2015) are equivalent despite the different syntax adopted in each of them. To be precise, PSL in (Bach et al. 2015) is defined for clausal form only, such as (3) below, while in (Kimmig et al. 2012) it is defined for rule form (1) only.\nWhen L is either an atom A or \u00acsA, by \u00f8L we denote a literal complementary to L, i.e., \u00f8L = \u00acsA if L is A, and \u00f8L = A if L = \u00acsA. The following equivalences are known from fuzzy logic.\nLemma 2 For any formulas F and G, and any literals Li (1 \u2264 i \u2264 n),\n(a) F \u2192l G is equivalent to \u00acsF \u2295l G. (b) \u00acs(L1\u2297l \u00b7 \u00b7 \u00b7\u2297lLn) is equivalent to (\u00f8L1\u2295l \u00b7 \u00b7 \u00b7\u2295l \u00f8Ln).\nThe following lemma tells us that the clausal form using Lukasiewicz t-conorm can be written in many different forms.\nLemma 3 For any literals Li (1 \u2264 i \u2264 n),\nL1 \u2295l \u00b7 \u00b7 \u00b7 \u2295l Lm \u2295l Lm+1 \u2295l \u00b7 \u00b7 \u00b7 \u2295l Ln (3)\nis equivalent to\n\u00f8L1 \u2297l \u00b7 \u00b7 \u00b7 \u2297l \u00f8Lm \u2192l Lm+1 \u2295l \u00b7 \u00b7 \u00b7 \u2295l Ln\nwhere n \u2265 m \u2265 0.\nProof. By Lemma 2 (a), formula (3) is equivalent to\n\u00acs(L1 \u2295l \u00b7 \u00b7 \u00b7 \u2295l Lm) \u2192l Lm+1 \u2295l \u00b7 \u00b7 \u00b7 \u2295l Ln\nand by Lemma 2 (b), the latter is equivalent to\n\u00f8L1 \u2297l \u00b7 \u00b7 \u00b7 \u2297l \u00f8Lm \u2192l Lm+1 \u2295l \u00b7 \u00b7 \u00b7 \u2295l Ln.\nIt follows from Lemma 1 that the probability density of an interpretation does not change when the formula is replaced with another equivalent formula. This tells us that PSL rules of the form (1) can be rewritten as any other equivalent formulas. For instance, PSL rule\nw : a \u2190l b\u2297l c \u02c61 (4)\ncan be equivalently rewritten as any of the following ones.\nw : \u00acsb \u2190l \u00acsa\u2297l c \u02c61, w : \u00acsc \u2190l \u00acsa\u2297l b \u02c61, w : a\u2295l \u00acsb \u2190l c \u02c61, w : a\u2295l \u00acsc \u2190l b \u02c61, w : \u00acsb\u2295l \u00acsc \u2190l \u00acsa \u02c61, w : a\u2295l \u00acsb\u2295l \u00acsc \u02c61, w : 0 \u2190l \u00acsa\u2297l b\u2297l c \u02c61. (5) As noted above, the syntax of PSL in (Bach et al. 2015) is clausal form only, such as the second to the last formula in (5), while the syntax of PSL in (Kimmig et al. 2012) is rule form such as (4). The result above tells us that the definitions of PSL defined in (Kimmig et al. 2012) and (Bach et al. 2015) are equivalent despite the different syntax adopted there.\nOn the other hand, similar rewriting using other t-norms and their derived operators may not necessarily yield an equivalent formula because not every selection of fuzzy operators satisfy Lemma 2 even if they are generalizations of the corresponding Boolean connectives.\nExample 2 Consider again Go\u0308del t-norm \u2297m and its residual implicator \u2192r. The negation \u00acm induced from \u00acmx = x \u2192r 0 is\n\u00acmx =\n{\n1 if x = 0 0 if x > 0.\nFor the interpretation I = {(p, 0.4), (q, 0.5)}, we have \u03c5I(\u00acmp\u2295m q) = 0\u2295m 0.5 = 0.5, but \u03c5I(p \u2192r q) = 1. In other words, \u00acmp\u2295m q is not equivalent to p \u2192r q.\nIn the literature on PSL (Kimmig et al. 2012; Bach et al. 2015), the selection of Lukasiewicz t-norm is motivated by the computational efficiency gained by reducing MAP inferences to convex optimization problems. This section presents yet another justification of Lukasiewicz t-norm in PSL from the logical perspective."}, {"heading": "GPSL : MLN = Fuzzy Logic : Boolean Logic", "text": "Like fuzzy logic is a many-valued extension of Boolean logic, we may view GPSL as a many-valued extension of MLN.\nFor any classical propositional formula F , let F fuzzy be the fuzzy formula obtained from F by replacing\u22a5 with 0, \u22a4 with 1, \u00ac with any fuzzy negation symbol, \u2227 with any fuzzy conjunction symbol, \u2228 with any fuzzy disjunction symbol, and \u2192 with any fuzzy implication symbol.\nFor any GPSL program \u03a0, by TW\u03a0 (\u201ctotal weight\u201d) we denote\nexp\n\n\n\u2211\n\u3008w:F\u02c6k\u3009\u2208\u03a0\nw\n\n .\nFor any MLN L, let \u03a0L be the GPSL program obtained from L by replacing each weighted formula w : F in L with w : F fuzzy\u02c6k, where k is either 1 or 2. The following theorem tells us that, for any Boolean interpretation I , its weight under MLN L is proportional to the unnormalized probability density under the GPSL program \u03a0L.\nTheorem 2 For any MLN L and any Boolean interpretation I ,\nWL(I) = TW\u03a0L \u00b7 f\u0302\u03a0L(I).\nProof.\nWL(I) = exp\n(\n\u2211\n\u3008w,F \u3009\u2208LI\nw\n)\n= exp\n(\n\u2211\n\u3008w,F \u3009\u2208L\nw \u2212 \u2211\n\u3008w,F \u3009\u2208L\\LI\nw\n)\n= exp\n(\n\u2211\n\u3008w,F \u3009\u2208L\nw\n)\n\u00b7 exp\n(\n\u2212 \u2211\n\u3008w,F \u3009\u2208L\\LI\nw\n)\n= exp\n(\n\u2211\n\u3008w,F \u3009\u2208L\nw\n)\n\u00d7 exp\n\n\u2212\n(\n\u2211\n\u3008w,F \u3009\u2208L\\LI\n(w \u00b7 1) + \u2211\n\u3008w,F \u3009\u2208LI\n(w \u00b7 0)\n)\n\n .\n(6)\nNote that when I is Boolean, 1 \u2212 \u03c5I(F ) = 1 if I 6|= F , and 1\u2212 \u03c5I(F ) = 0 if I |= F . So (6) is equal to\nTW\u03a0L \u00b7 exp\n(\n\u2212 \u2211\n\u3008w:F fuzzy\u02c6k\u3009\u2208\u03a0L\nw \u00b7 (1\u2212 \u03c5I(F fuzzy ))k\n)\n= TW\u03a0L \u00b7 f\u0302\u03a0L(I).\nThis theorem tells us that the problem of computing the weight of an interpretation in MLN can be reduced to computing the probability density of an interpretation in GPSL.\nBy Theorem 1, since PSL is a special case of GPSL, the following relation between PSL and MLN follows easily.\nCorollary 1 For any PSL program \u03a0 and any fuzzy Boolean interpretation I , let L be the MLN obtained from \u03a0 by replacing each fuzzy operator with its Boolean counterpart. We have\nf\u0302\u03a0(I) = WL(I)\nTW\u03a0 .\nExample 3 Let \u03a0 be the following PSL program\n1 : p \u2190l q \u02c61 2 : q \u2190l p \u02c61\nand let L be the corresponding MLN as described in Corollary 1.\nThe following table shows, for each Boolean interpretation I , its weight according to the MLN semantics (WL(I)) is TW\u03a0, which is e3, multiplied by its unnormalized probability density function (f\u0302\u03a0(I)) according to the PSL semantics. (We identify a Boolean interpretation with the set of atoms that are true in it.)\nInterpretation (I) WL(I) f\u0302\u03a0(I) \u2205 e3 e0\n{p} e1 e\u22122 {q} e2 e\u22121\n{p, q} e3 e0\nHowever, MAP states in MLN and PSL can be different because most probable interpretations in PSL may be nonBoolean.\nExample 4 Consider the PSL program:\n1 : p \u2190l \u00acsp \u02c61 1 : \u00acsp \u02c61\n(7)\nand the corresponding MLN:\n1 : p \u2190 \u00acsp 1 : \u00acsp\n(8)\nThe most probable Boolean interpretations for MLN (8) are I1 = \u2205 and I2 = {p}, each with weight e1. Their unnormalized probability density for PSL program (7) is e\u22121. However, they are not the most probable interpretations according to the PSL semantics: I3 = {(p, 0.5)} has the largest unnormalized probability density e\u22120.5.\nThe difference can be closed by adding to the weighted propositional fuzzy logic theory a \u201ccrispifying\u201d rule for each atom. For any atom p \u2208 \u03c3, let CRSP(p) be the formula defined as\nCRSP(p) = p\u2295l p \u2192l p.\nIt is easy to check that \u03c5I(p\u2295lp \u2192l p) = 1 iff \u03c5I(p) is either 0 or 1. Note that although this formula uses Lukasiewicz operators, it is not expressible in PSL because \u2295l occurs in the body of the rule.\nFor any MLN L of signature \u03c3, let \u03a0L be the GPSL program obtained from L by replacing each weighted formula w : F in L with w : F fuzzy\u02c6k where k could be either 1 or 2. Let CR be the GPSL program\n{\u3008\u03b1 : CRSP(p)\u02c61\u3009 | p \u2208 \u03c3\u3009}.\nThe following theorem tells us that the most probable interpretations of MLN L coincides with the most probable interpretations of GPSL program \u03a0L \u222a CR.\nTheorem 3 For any MLN L, when \u03b1 \u2192 \u221e,\nargmaxI(WL(I)) = argmaxJ(f\u0302\u03a0L\u222aCR(J))\nwhere I ranges over all Boolean interpretations and J ranges over all fuzzy interpretations.\nProof. We first show that, when \u03b1 \u2192 \u221e, for any Boolean interpretation I and any non-Boolean interpretation J , we have f\u0302\u03a0L\u222aCR(J) < f\u0302\u03a0L\u222aCR(I), which implies that no nonBoolean interpretation can be the most probable interpretations.\nFirst, for any non-Boolean interpretation J , let\nW\u03a0L(J) = \u2212 \u2211\n\u3008w:F\u02c6k\u3009\u2208\u03a0L\n(\nw \u00b7 (1\u2212 \u03c5J(F )) k\n)\nand\nWCR(J) = \u2212 \u2211\n\u3008w:F\u02c61\u3009\u2208CR\n(\n\u03b1 \u00b7 (1\u2212 \u03c5J(F ))\n)\n.\nThen\nf\u0302\u03a0L\u222aCR(J) = exp(W\u03a0L(J) +WCR(J))\n\u2264 exp(WCR(J)).\nSince J is not Boolean, there is at least one weighted formula \u03b1 : p\u2295l p \u2192l p \u02c61 \u2208 CR that is not satisfied by J , so that\nf\u0302\u03a0L\u222aCR(J) \u2264 exp ( \u2212 \u03b1 \u00b7 ( 1\u2212 \u03c5J(p\u2295l p \u2192l p) ))\nwhere 1\u2212 \u03c5J(p\u2295l p \u2192l p) > 0. Notice that\nlim \u03b1\u2192\u221e f\u0302\u03a0L\u222aCR(J) \u2264\nlim \u03b1\u2192\u221e\nexp ( \u2212 \u03b1 \u00b7 ( 1\u2212 \u03c5J (p\u2295l p \u2192l p) )) = 0.\nOn the other hand, for any Boolean interpretation I ,\nf\u0302\u03a0L\u222aCR(I) = exp(W\u03a0L(I) +WCR(I)) = exp(W\u03a0L(I)).\nSince exp(W\u03a0L(I)) does not contain \u03b1, we have\nlim \u03b1\u2192\u221e f\u0302\u03a0L\u222aCR(I) > 0.\nThus we have f\u0302\u03a0L\u222aCR(J) < f\u0302\u03a0L\u222aCR(I) when \u03b1 \u2192 \u221e. It follows that any fuzzy interpretation K that satisfies argmaxJ(P\u03a0L\u222aCR(J)) = K must be Boolean. By Theorem 2, for any Boolean interpretation I , we have\nf\u0302\u03a0L\u222aCR(I) = exp(|\u03c3| \u00b7 \u03b1)\nTW\u03a0L\u222aCR \u00d7WL(I).\nSince exp(|\u03c3| \u00b7 \u03b1)\nTW\u03a0L\u222aCR is constant for all interpretations,\nf\u0302\u03a0L\u222aCR(I) \u221d WL(I). It follows that argmaxI(WL(I)) = argmaxJ(f\u0302\u03a0L\u222aCR(J)).\nExample 5 Consider the GPSL program:\n1 : p \u2190l \u00acsp \u02c61 1 : \u00acsp \u02c61 \u03b1 : p \u2190l p\u2295l p \u02c61.\nWhen \u03b1 \u2192 \u221e the most probable fuzzy interpretations are Boolean, and they are the same as the most probable interpretations for the MLN (8).\nIt is known that the MAP problem in PSL can be solved in polynomial time (Bro\u0308cheler, Mihalkova, and Getoor 2010), while the same problem in MLN is #P-hard. The reduction from MLN to GPSL in Theorem 3 tells us that the MAP problem in GPSL is #P-hard as well. This implies that GPSL is strictly more expressive than PSL even when we restrict attention to Lukasiewicz operators.\nRelated to Theorem 3, relation between discrete and soft MAP states was also studied in (Bach, Huang, and Getoor 2015; Bach et al. 2015), but from a different, computational perspective. There, inference on discrete MAP states is viewed as an instance of MAX SAT problems, and then approximated by relaxation to linear programming with rounding guarantee of solutions. The result indirectly tells us how MAP states in PSL are related to MAP states in MLN, but this is different from Theorem 3, which completely closes the semantic gap between them via crispifying rules."}, {"heading": "Conclusion", "text": "In this note, we studied the two well-known formalisms in statistical relational learning from a logical perspective. Viewing PSL in terms of the weighted fuzzy logic gives us some useful insights known from fuzzy logic. Besides the reducibility to convex optimization problems, the restriction to the Lukasiewicz fuzzy operators in clausal form allows intuitive equivalent transformations resembling those from Boolean logic. On the other hand, it prohibits us from using some other intuitive fuzzy operators.\nIn our previous work (Lee and Wang 2014; Lee and Wang 2016a) we used fuzzy answer set programs to describe temporal projection in dynamic domains, where we had to use Go\u0308del t-norm as well as Lukasiewicz t-norm.6 There, Go\u0308del t-norm is necessary in expressing the commonsense law of inertia. For example,\nTrust(a, b, t)\u2297m\u00acs\u00acsTrust(a, b, t+1) \u2192r Trust(a, b, t+1), (9) expresses that the degree that a trusts b at time t+1 is equal to the degree at time t if it can be assumed without contradicting any of the facts that can be derived. 7 The fuzzy conjunction \u2297 used here needs to satisfy that \u2297(x, y) is equal to either x or y (otherwise the trust degree at next time step would change for no reason). Obviously Lukasiewicz t-norm does not satisfy the requirement: \u2297l(x, y) < x when y < 1. In other words, if we replace \u2297m with \u2297l, the trust degree at next time drops for no reason, which is unintuitive. The restriction to Lukasiewicz t-norm in PSL accounts for the difficulty in directly applying PSL to temporal reasoning problems like the above example. Indeed, most work on PSL has been limited to static domains.\nSince computing marginal probabilities in MLN can be reduced to computing marginal probabilities in GPSL as indicated by Theorem 2, computing marginal probabilities in GPSL is at least #P-hard. However, a sampling method could be used for such an inference. A naive sampling method is outlined below: suppose we are approximating the probability that the truth value of formula F falls into (l, u) for some 0 \u2264 l \u2264 u \u2264 1 (denoted as P (l \u2264 F \u2264 u)).\n1. Generate N interpretations at random;\n2. For each of the N interpretations, compute its probability density;\n3. Approximate P (l \u2264 F \u2264 u) by X N , where X is the number of interpretations I that satisfies l \u2264 \u03c5I(F ) \u2264 u among the N interpretations.\nIt can be shown that X N is the estimation of P (l \u2264 F \u2264 u) that maximizes the likelihood of the N samples.\nThe way that MLN extends propositional logic is similar to the way that PSL extends a restricted version of fuzzy propositional logic. GPSL is simply taking the fuzzy propositional logic in full generality and applying the log-linear weight scheme. In our recent work (Lee and Wang 2016b),\n6The main example was how the trust degree between people changes over time.\n7We refer the reader to (Lee and Wang 2016a) for the precise semantics of this language.\nwe adopted the similar weight scheme to answer set programs in order to overcome the deterministic nature of the stable model semantics providing ways to resolve inconsistencies in answer set programs, to rank stable models, to associate probability to stable models, and to apply statistical inference to computing weighted stable models. Perhaps this indicates the universality of the log-linear weight scheme first adopted in MLN, which provides a uniform method to turn the crisp logic (be it fuzzy logic, propositional logic, or answer set programs) \u201csoft.\u201d\nAcknowledgements We are grateful to Michael Bartholomew and the anonymous referees for their useful comments. This work was partially supported by the National Science Foundation under Grants IIS-1319794, IIS-1526301, and a gift funding from Robert Bosch LLC."}], "references": [{"title": "S", "author": ["Bach"], "venue": "H.; Broecheler, M.; Huang, B.; and Getoor, L.", "citeRegEx": "Bach et al. 2015", "shortCiteRegEx": null, "year": 2015}, {"title": "S", "author": ["Bach"], "venue": "H.; Huang, B.; and Getoor, L.", "citeRegEx": "Bach. Huang. and Getoor 2015", "shortCiteRegEx": null, "year": 2015}, {"title": "Probabilistic soft logic for semantic textual similarity", "author": ["Erk Beltagy", "I. Mooney 2014] Beltagy", "K. Erk", "R. Mooney"], "venue": "Proceedings of Association for Computational Linguistics", "citeRegEx": "Beltagy et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Beltagy et al\\.", "year": 2014}, {"title": "Probabilistic similarity logic", "author": ["Mihalkova Br\u00f6cheler", "M. Getoor 2010] Br\u00f6cheler", "L. Mihalkova", "L. Getoor"], "venue": "UAI", "citeRegEx": "Br\u00f6cheler et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Br\u00f6cheler et al\\.", "year": 2010}, {"title": "A short introduction to probabilistic soft logic", "author": ["Kimmig"], "venue": "In Proceedings of the NIPS Workshop on Probabilistic Programming: Foundations and Applications,", "citeRegEx": "Kimmig,? \\Q2012\\E", "shortCiteRegEx": "Kimmig", "year": 2012}, {"title": "and Wang", "author": ["J. Lee"], "venue": "Y.", "citeRegEx": "Lee and Wang 2014", "shortCiteRegEx": null, "year": 2014}, {"title": "Fuzzy propositional formulas under the stable model semantics. the Special Issue on Logics for Reasoning about Preferences, Uncertainty and Vagueness of the IfCoLog", "author": ["Lee", "J. Wang 2016a] Lee", "Y. Wang"], "venue": "Journal of Logics and their Applications. To appear", "citeRegEx": "Lee et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Lee et al\\.", "year": 2016}, {"title": "Weighted rules under the stable model semantics", "author": ["Lee", "J. Wang 2016b] Lee", "Y. Wang"], "venue": "In Proceedings of International Conference on Principles of Knowledge Representation and Reasoning (KR)", "citeRegEx": "Lee et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Lee et al\\.", "year": 2016}, {"title": "and Domingos", "author": ["M. Richardson"], "venue": "P.", "citeRegEx": "Richardson and Domingos 2006", "shortCiteRegEx": null, "year": 2006}], "referenceMentions": [], "year": 2016, "abstractText": "Markov Logic Networks (MLN) and Probabilistic Soft Logic (PSL) are widely applied formalisms in Statistical Relational Learning, an emerging area in Artificial Intelligence that is concerned with combining logical and statistical AI. Despite their resemblance, the relationship has not been formally stated. In this paper, we describe the precise semantic relationship between them from a logical perspective. This is facilitated by first extending fuzzy logic to allow weights, which can be also viewed as a generalization of PSL, and then relate that generalization to MLN. We observe that the relationship between PSL and MLN is analogous to the known relationship between fuzzy logic and Boolean logic, and furthermore the weight scheme of PSL is essentially a generalization of the weight scheme of MLN for the many-valued setting. Introduction Statistical relational learning (SRL) is an emerging area in Artificial Intelligence that is concerned with combining logical and statistical AI. Markov Logic Networks (MLN) (Richardson and Domingos 2006) and Probabilistic Soft Logic (PSL) (Kimmig et al. 2012; Bach et al. 2015) are well-known formalisms in statistical relational learning, and have been successfully applied to a wide range of AI applications, such as natural language processing, entity resolution, collective classification, and social network modeling. Both of them combine logic and probabilistic graphical model in a single representation, where each formula is associated with a weight, and the probability distribution over possible worlds is derived from the weights of the formulas that are satisfied by the possible worlds. However, despite their resemblance to each other, the precise relationship between their semantics is not obvious. PSL is based on fuzzy interpretations that range over reals in [0, 1], and in this sense is more general than MLN. On the other hand, its syntax is restricted to formulas in clausal form, unlike MLN that allows any complex formulas. It is also not obvious how their models\u2019 weights are related to each other due to the different ways that the weights are associated with models. Originating from the machine learning research, these formalisms In Working Notes of the 6th International Workshop on Statistical Relational AI (StarAI 2016) are equipped with several efficient inference and learning algorithms, and some paper compares the suitability of one formalism over the other by experiments on specific applications (Beltagy, Erk, and Mooney 2014). On the other hand, the precise relationship between the two formalisms has not been formally stated. In this paper, we present a precise semantic relationship between them. We observe that the relationship is analogous to the well-known relationship between fuzzy logic and classical logic. Moreover, despite the different ways that weights of models are defined in each formalism, it turns out that they are essentially of the same kind. Towards this end, we introduce a weighted fuzzy logic as a proper generalization of PSL, which is also interesting on its own as an extension of the standard fuzzy logic to incorporate weighted models. The weighted fuzzy logic uses the same weight scheme as PSL, but associates weights to arbitrary fuzzy formulas. This intermediate formalism facilitates the comparison between PSL and MLN. We observe that the same analogy between fuzzy logic and Boolean logic carries over to between PSL and MLN. Analogous to that fuzzy logic agrees with Boolean logic on crisp interpretations, PSL and MLN agree on crisp interpretations, where their weights are proportional to each other. However, their maximum a posteriori (MAP) estimates do not necessarily coincide due to the differences between many-valued vs. Boolean models. The paper is organized as follows. We first review each of MLN, fuzzy propositional logic, and PSL. Then we define a weighted fuzzy logic as a generalization of PSL. Using this we study the semantic relationship between PSL and MLN.", "creator": "LaTeX with hyperref package"}}}