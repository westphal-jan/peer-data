{"id": "1606.08963", "review": {"conference": "AAAI", "VERSION": "v1", "DATE_OF_SUBMISSION": "29-Jun-2016", "title": "Non-Linear Label Ranking for Large-Scale Prediction of Long-Term User Interests", "abstract": "We consider the problem of personalization of online services from the viewpoint of ad targeting, where we seek to find the best ad categories to be shown to each user, resulting in improved user experience and increased advertisers' revenue. We propose to address this problem as a task of ranking the ad categories depending on a user's preference, and introduce a novel label ranking approach capable of efficiently learning non-linear, highly accurate models in large-scale settings. Experiments on a real-world advertising data set with more than 3.2 million users show that the proposed algorithm outperforms the existing solutions in terms of both rank loss and top-K retrieval performance, strongly suggesting the benefit of using the proposed model on large-scale ranking problems.", "histories": [["v1", "Wed, 29 Jun 2016 06:00:35 GMT  (32kb)", "http://arxiv.org/abs/1606.08963v1", "28th AAAI Conference on Artificial Intelligence (AAAI-14)"]], "COMMENTS": "28th AAAI Conference on Artificial Intelligence (AAAI-14)", "reviews": [], "SUBJECTS": "cs.AI cs.LG stat.ML", "authors": ["nemanja djuric", "mihajlo grbovic", "vladan radosavljevic", "narayan bhamidipati", "slobodan vucetic"], "accepted": true, "id": "1606.08963"}, "pdf": {"name": "1606.08963.pdf", "metadata": {"source": "CRF", "title": "Non-linear Label Ranking for Large-scale Prediction of Long-Term User Interests", "authors": ["Nemanja Djuric", "Mihajlo Grbovic", "Vladan Radosavljevic", "Narayan Bhamidipati", "Slobodan Vucetic"], "emails": ["narayanb}@yahoo-inc.com", "vucetic@temple.edu"], "sections": [{"heading": null, "text": "ar X\niv :1\n60 6.\n08 96\n3v 1\n[ cs\n.A I]\n2 9\nJu n\n20 16"}, {"heading": "Introduction", "text": "Personalization of online content has become an important topic in the recent years. It has been defined as \u201dthe ability to proactively tailor products and product purchasing experiences to tastes of individual consumers based upon their personal and preference information\u201d (Chellappa and Sin 2005), which may lead to improved user experience and directly translate into financial gains for online businesses (Riecken 2000). In addition, personalization fosters stronger bond between users and companies, and can help in increasing user loyalty and retention (Alba et al. 1997). For these reasons it has been recognized as an important strategic goal of major internet companies (Manber, Patel, and Robison 2000; Das et al. 2007), and is a focus of significant research efforts. Personalized content has already become an integral part of many popular online services, a trend likely to continue in the future (Tuzhilin 2009).\nWe consider content personalization from the viewpoint of targeted advertising (Essex 2009), an increasingly important aspect of online businesses. Here, for each individual user the task is to find the best matching ads to be displayed, which improves user\u2019s online experience (as only relevant and interesting ads are shown to the user) and can lead to increased revenue for the advertisers (as users are more likely to click on the ad and make a purchase). Due to its large impact and many open research questions, targeted advertising\nCopyright c\u00a9 2016, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved.\nhas garnered significant interest from the machine learning community, as witnessed by a large number of recent workshops and publications (Broder 2008; Pandey et al. 2011; Majumder and Shrivastava 2013).\nOne of the most popular approaches in present-day targeting, particularly in brand awareness campaigns, is to assign categories to the display ads, such as \u201dsports\u201d or \u201dfinance\u201d, and then separately learn to predict user interest in each of these categories using historical records (Ahmed et al. 2011; Pandey et al. 2011; Tyler et al. 2011). Typically, a taxonomy is used to decide on the categories, and depending on how detailed it is hundreds of separate category qualification tasks may need to be solved. Thus, for each ad category, a separate predictive model is trained, able to estimate the probability of an ad click for the entire user population. Then, for each category,N users with the highest click probability are selected for ad exposure. Known issues with the approach include overexposure, where a single user may be among the top N users for many categories, and starvation, where some users do not qualify for any of the categories.\nAn alternative avenue, known in the industry as a userinterest model, is to sort for each user outputs of the predictive models, and qualify users based on their top K categories. The approach guarantees that a user is qualified into several categories, eliminating overexposure and starvation issues. However, this method may still be suboptimal, as the predictive models are trained in isolation and do not consider relationships between different categories. In this paper we explore methods capable of capturing more complex class dependencies, and consider the user-interest model from a label ranking standpoint (Vembu and Ga\u0308rtner 2011). However, the sheer scale of ad targeting problems, with data sets comprising millions of users and features and hundreds of categories, renders many existing label ranking approaches intractable, presenting new challenges to the researchers.\nTo address this issue, we propose a novel label ranking algorithm suitable for large-scale settings. The method lends ideas from the state-of-the-art AMM classifiers (Wang et al. 2011), efficiently learning accurate, non-linear models on limited resources. Empirical evaluation was performed in a real-world ad targeting setting, using, to the best of our knowledge, the largest dataset considered thus far in the label ranking literature. The results show that the algorithm significantly outperformed the existing methods, indi-\ncating the benefits of the proposed approach to label ranking tasks."}, {"heading": "Background", "text": "In this section we present works and ideas that led to the proposed algorithm. We first discuss label ranking setting, and then describe Adaptive Multi-hyperplane Machine (AMM), a non-linear, multi-class model used to develop a novel large-scale label ranking approach introduced in this paper."}, {"heading": "Label ranking", "text": "Unlike standard machine learning problems such as multiclass or multi-label classification, label ranking is a relatively novel topic which involves a complex task of label preference learning. More specifically, rather than predicting one or more class labels for a newly observed example, we seek to find a strict ranking of classes by their importance or relevance to the given example. For instance, let us consider targeted advertising domain, and assume that the examples are internet users and class labels are user preferences from the set Y = {\u201dsports\u201d, \u201dtravel\u201d, \u201dfinance\u201d}. Then, instead of simply inferring that the user is a \u201dsports\u201d person, which would result in user being shown only sports-related ads, it is more informative to know that the user prefers sports over finance over travel, resulting in more diverse and more effective ad targeting. Note that the label ranking problem differs from the learning-to-rank setup (Cao et al. 2007), where the task is to rank the examples and not labels, and can also be seen as a generalization of classification and multi-label problems (Dekel, Manning, and Singer 2003).\nMore formally, in the label ranking scenario the input is defined by a feature vector x \u2208 X \u2282 Rd, and the output is defined by a ranking \u03c0 \u2208 \u03a0 of class labels. Here, the labels originate from a predefined set Y = {1, 2, . . . , L} (e.g., \u03c0 = [3, 1, 4, 2] for L = 4), and \u03a0 is a set of all label permutations. Let us denote by \u03c0i a class label at the ith position in the label ranking \u03c0, and by \u03c0\u22121i a position (or rank) of label i in the ranking \u03c0. For instance, in the above example we would have \u03c01 = 3 and \u03c0 \u22121 1 = 2. Then, for any i and j, where 0 \u2264 i < j \u2264 L, we say that label \u03c0i is preferred over label \u03c0j , or equivalently \u03c0i \u227b \u03c0j . Moreover, in the case of an incomplete order \u03c0, we say that any label i \u2208 \u03c0 is preferred over the missing ones. Further, let us assume that we are given a sample from the underlying distribution D = {(xt , \u03c0t ), t = 1, ...,T}, where \u03c0t is a vector containing either a total or a partial order of class labels Y . The learning goal is to find a model f that maps input examples x into a total ordering of labels, f : X \u2192 \u03a0.\nIn the recent years the problem has seen increased attention by the machine learning community (e.g., see recent workshops and tutorials at ICML, NIPS, and other venues), and many effective algorithms have been proposed in the literature (Har-Peled, Roth, and Zimak 2003; Dekel, Manning, and Singer 2003; Kamishima and Akaho 2006; Cheng, Hu\u0308hn, and Hu\u0308llermeier 2009; Grbovic, Djuric, and Vucetic 2013); for an excellent review see (Vembu and Ga\u0308rtner 2011).\nIn (Cheng, Hu\u0308hn, and Hu\u0308llermeier 2009; Cheng, Dembczyn\u0301ski, and Hu\u0308llermeier 2010) authors propose instance-based methods for label ranking, where training examples are first clustered according to their feature vectors, and then centroid and mean ranking are found for each cluster and used for inference. This idea was extended in (Grbovic et al. 2013; Grbovic, Djuric, and Vucetic 2013), where authors use feature vectors to supervise clustering, resulting in improved performance. Apart from the prototype-based methods, often considered approaches include learning a scoring function gi for each class, i = 1, . . . , L, and sorting their output in order to infer label ranking (Elisseeff and Weston 2001; Dekel, Manning, and Singer 2003; Har-Peled, Roth, and Zimak 2003), or training a number of binary classification models to predict pairwise label preferences and aggregating their output into a total order (Hu\u0308llermeier et al. 2008; Hu\u0308llermeier and Vanderlooy 2010)."}, {"heading": "Adaptive Multi-hyperplane Machine", "text": "The AMM algorithm is a budgeted, multi-class method suitable for large-scale problems (Wang et al. 2011; Djuric et al. 2014). It is an SVM-like algorithm that formulates a non-linear model by assigning a number of linear hyperplanes to each class in order to capture data non-linearity. Given a d-dimensional example x and a set Y of L possible classes, AMM has the following form,\nf(x) = argmax i\u2208Y g(i,x), (1)\nwhere the scoring function g(i,x) for the ith class,\ng(i,x) = max j\nw T i,jx, (2)\nis parameterized by a weight matrix W written as\nW =\n[\nw1,1 . . .w1,b1 |w2,1 . . .w2,b2 | . . . |wL,1 . . .wL,bL\n]\n,\n(3) where b1, . . . , bL are the numbers of weights (i.e., hyperplanes) assigned to each of the L classes, and each block in (3) is a set of class-specific weights. Thus, from (1) we can see that the predicted label of the example x is the class of the weight vector that achieves the maximum value g(i,x).\nAMM is trained by minimizing the following convex problem at each tth training iteration,\nL(t)(W|z) \u2261 \u03bb\n2 ||W||2F + l(W; (xt, yt); zt), (4)\nwhere \u03bb is the regularization parameter, and the instantaneous loss l(\u00b7) is computed as\nl(W; (xt, yt); zt) = max\n(\n0, 1 + max i\u2208Y\\yt\ng(i,xt)\u2212w T yt,zt xt\n)\n.\n(5) Element zt of vector z = [z1 . . . zT ] determines which weight belonging to the true class of the tth example is used\nto calculate (5), and can be fixed prior to the start of a training epoch or, as done in this paper, can be computed onthe-fly as an index of a true-class weight that provides the highest score (Wang et al. 2011).\nAMM uses Stochastic Gradient Descent (SGD) to solve (4). The SGD is initialized with the zero-matrix (i.e., W\n(0) = 0), which comprises infinite number of zerovectors for each class. This is followed by an iterative procedure, where training examples are observed one by one and the weight matrix is modified accordingly. Upon receiving example (xt, yt) \u2208 D at the tth round, w (t) ij is updated as\nw (t+1) ij = w (t) ij \u2212 \u03b7 (t)\u2207 (t) ij , (6)\nwhere \u03b7(t) = 1/(\u03bbt) is a learning rate, and \u2207(t)ij is the subgradient of (4) with respect to w(t)i,j ,\n\u2207 (t) i,j =\n\n \n \n\u03bbw (t) i,j + xt, if i = it, j = jt, \u03bbw (t) i,j \u2212 xt, if i = yt, j = zt, \u03bbw (t) i,j , otherwise,\n(7)\nwith\nit = argmax k\u2208Y\\yt g(k,x) and jt = argmax k\n(w (t) it,k )Txt. (8)\nIf the loss (5) at the tth iteration is positive, class weight from the true class yt indexed by zt is moved towards xt during the update, while the class weight w(t)it,jt with the maximum prediction from the remaining classes is pushed away. If the updated weight is a zero-weight then it becomes non-zero, thus increasing the weight count bi for that class by one. In this way, complexity of the model adapts to complexity of the data, and bi, i = 1, . . . , L, are learned during training."}, {"heading": "Methodology", "text": "It has been shown that the existing label ranking methods achieve good performance on many tasks, however, in the large-scale setting considered in this paper, they might not be as effective. When faced with non-linear problems comprising millions of examples and features, the proposed methods are either too costly to train and use, or may not be expressive enough to learn complex problems. To address this issue, in this section we present a novel ranking algorithm, called AMM-rank, that extends the idea of adaptability and online learning from AMM to label ranking setting, allowing large-scale training of accurate ranking models."}, {"heading": "AMM-rank algorithm", "text": "Before detailing the training procedure of AMM-rank, we first consider its predictive label ranking model. As discussed previously, we assume that the tth training example xt is associated with (possibly) incomplete label ranking \u03c0t of length Lt \u2264 L. Given a trained AMM-rank model (3) and a test example x, a score for each class is found using equation (2), and the predicted label ranking is obtained by sorting the scores in the descending order,\n\u03c0\u0302 = sort([g(1,x), g(2,x), . . . , g(L,x)]), (9)\nwhere the sort function returns indices of the sorted scores. Training of AMM-rank resembles the training of AMM multi-class model described in the previous section. Learning is initialized with a zero-matrix comprising an infinite number of zero-vectors for each class, followed by iteratively observing examples one by one and modifying the weight matrix. At each tth training iteration we minimize the following regularized instanteneous rank loss,\nL (t) rank(W|z) \u2261\n\u03bb 2 \u2016W\u20162F + lrank(W; (xt, yt); zt). (10)\nThe ranking loss lrank(\u00b7) is defined as\nlrank(W; (xt, yt); zt) =\nLt \u2211\ni=1\n\u03bd(i)\nL \u2211\nj=1\nI(\u03c0i \u227b j)max(0, 1 + g(j,xt)\u2212w T \u03c0i,zti xt),\n(11) where \u03bd(i) is a predefined importance assigned to the ith rank, and function I(arg) returns 1 if arg evaluates to true, and 0 otherwise. As in label ranking setting we need to keep track of predicted scores of all L classes and not only the top one, note that we introduced vector zt instead of a scalar zt as in (4), whose element zti determines which weight belonging to label i is used to compute (10) for the tth example.\nDepending on the problem at hand, using the function \u03bd(i) a modeler can emphasize the importance of some ranks over the others. For example, let us assume \u03bd(i) = 1/i. Then, in the ranking loss defined in (11), the factor i\u22121 enforces higher penalty for misranking of topranked topics, while the mistakes made for lower-ranked topics incur progressively smaller costs. This approach has been explored previously in information retrieval setting (Weston et al. 2012). However, it is also applicable in the context of targeted advertising, where lower-ranked classes have progressively lower relevance to an ad publisher than the higher-ranked ones. Furthermore, penalty is incurred whenever the lower-ranked label was either predicted to be preferred over the higher-ranked one, or the score of the preferred label was higher with a margin smaller than 1.\nWe use SGD at each training iteration to minimize the objective function (10). Subgradient of the instantaneous rank loss with respect to the weights can be computed as\n\u2207 (t) i,j = \u03bbw (t) i,j \u2212 xt I(j = zti) \u03bd(\u03c0 \u22121 i )\nL \u2211\nk=1\n(\nI ( i \u227b k)\u00b7\nI(1 + g(k,xt) > (w (t) ij ) T xt )\n)\n+ xt I(j = zti)\u00b7\nL \u2211\nk=1\n(\n\u03bd(k) I(k \u227b i) I ( 1 + (w (t) ij ) T xt > (w (t) kztk )Txt )\n)\n.\n(12) An SGD update step (12) can be summarized as follows. At every training round all model weights are reduced towards zero by multiplying them with (1 \u2212 1/t) (the first term on the RHS). In addition, if the j th weight of the ith class was used to compute the score for the tth label (i.e., I(j = zti) equals 1), it is pushed further towards xt whenever the ith\nlabel was either wrongly predicted to be less preferred or correctly predicted with margin smaller than 1 (the second term on the RHS). Moreover, the weight is pushed further away from xt whenever the score of the class preferred over the ith class was either lower or higher with margin less than 1 (the third term on the RHS). Similarly to the AMM model, the complexity of AMM-rank ranking model is automatically learned during training, and adapts to the complexity of the considered label ranking problem."}, {"heading": "Experiments", "text": "In this section we describe the problem setting and present a large-scale, real-world data set that was used for evaluation, followed by description and analysis of empirical results."}, {"heading": "Dataset", "text": "We are addressing a problem from display advertising domain which consists of several key players: 1) advertisers, companies that want to advertise their products; 2) publishers, websites that host the advertisements (such as Yahoo or Google); and 3) online users. The web environment provides publishers with the means to track user behavior in much greater detail than in the offline setting, including capturing user\u2019s registered information (e.g., demographics, location) and activity logs that comprise search queries, page views, email activity, ad clicks, and purchases. This brings the ability to target users based on their past behavior, which is typically referred to as ad targeting (Ahmed et al. 2011; Pandey et al. 2011; Tyler et al. 2011; Agarwal, Pandey, and Josifovski 2012; Aly et al. 2012). Having this in mind, the main motivation for the following experimental setup was the task of estimating user\u2019s ad click interests using their past activities. The idea is that, if we sort the interests in descending order of preference and attempt to predict this ranking, this task can be formulated as a label ranking problem.\nThe data set that was used in the empirical evaluation was generated using the information about users\u2019 online activities collected at Yahoo servers. The activities are temporal sequences of raw events that were extracted from server logs and are represented as tuples (ui, ei, ti), i = 1, . . . , N , where ui is ID of a user that generated the ith tuple, ei is an event type, ti is a timestamp, and N is a total number of recorded tuples. For each user we considered events belonging to one of the following six groups:\n\u2022 page views (\u201dpv\u201d) - website pages that the user visited;\n\u2022 search queries (\u201dsq\u201d) - user-generated search queries;\n\u2022 search link clicks (\u201dslc\u201d) - user clicks on search links;\n\u2022 sponsored link clicks (\u201dolc\u201d) - user clicks on searchadvertising links that appear next to actual search links;\n\u2022 ad views (\u201dadv\u201d) - display ads that the user viewed;\n\u2022 ad clicks (\u201dadc\u201d) - display ads that the user clicked on.\nEvents from these six groups are all categorized into an inhouse hierarchical taxonomy by an automatic categorization system and human editors. Each event is assigned to\na category from a leaf of the taxonomy, and then propagated upwards toward parent categories. Considering that the server logs for each user are retained for several months, the recorded events can be used to capture users\u2019 interests in categories over long periods of time.\nFollowing the ad categorization step, we can compute intensity and recency measures for each of L considered categories in each of the six groups. Let Dugct denote a set of all tuples that were generated by user u, where ei belongs to group g and is labeled with category c, with timestamp ti \u2264 t. Then, intensity and recency are defined as follows, \u2022 intensity is an exponentially time-decayed count of all\ntuples in Dugct, computed as\nintensity(u, g, c, t) = \u2211\n(ui,ei,ti)\u2208Dugct\n\u03b1t\u2212ti , (13)\nwhere \u03b1 is a fixed decay factor, with 0 < \u03b1 < 1 (we omit the exact value as it represents sensitive information).\n\u2022 recency is a difference between timestamp t and a timestamp of the most recent event from Dugct, computed as\nrecency(u, g, c, t) = min (ui,ei,ti)\u2208Dugct (t\u2212 ti). (14)\nThe intensity and recency measures were used to generate both the features and the label ranks for each user. In particular, we first chose two timestamps that were one month apart, Tfeatures and Tlabels, where Tfeatures < Tlabels. Then, at timestamp Tfeatures we used (13) and (14) to compute intensity and recency of L categories in each of \u201dpv\u201d, \u201dsq\u201d, \u201dslc\u201d, and \u201dolc\u201d groups separately, which, together with user\u2019s age (split into 9 buckets and represented as 9 binary features) and gender (represented as 2 binary features) was used as a feature vector x, resulting in input space dimensionality d of (2 \u00b7 4 \u00b7 L+ 9+ 2). In addition, in order to evaluate the influence of user ad views to their ad clicks, we also considered the case where intensity and recency of L categories in the \u201dadv\u201d group were appended to the feature vector, increasing the dimensionality by 2L.\nFurthermore, to quantify user interests and generate ground-truth ranks \u03c0, we considered only \u201dadc\u201d events between Tfeatures and Tlabels, and computed intensity of L categories at timestamp Tlabels. We consider level of interest of user u in category c to be equal to intensity of c in \u201dadc\u201d group, and preference ranking of categories is obtained simply by sorting their intensities. Note that the ground-truth ranking is in most cases incomplete, as users usually do not interact with all categories from the taxonomy.\nWe considered L = 50 second-level categories of the taxonomy (e.g., \u201dfinance/loans\u201d, \u201dretail/apparel\u201d), and collected data comprising 3,289,229 anonymous users that clicked on more than 2 categories. Category distribution in the ground-truth ranks is given in Fig. 1, where we see that a large fraction of ad clicks would be missed if users were targeted only with the most clicked categories, which directly results in lost revenue for both publishers and advertisers."}, {"heading": "Results", "text": "We compared AMM-rank to following approaches: a) multiclass AMM (Wang et al. 2011), where the top-ranked cat-\negory was used as a true class and the output scores for all categories were sorted to obtain ranking, used as a na\u0131\u0308ve baseline; b) Central-Mal, which always predicts central ranking of the training set computed using the Mallows model (Mallows 1957); c) AG-Mal, which computes Central-Mal over all users grouped in different age (\u201d13- 17\u201d, \u201d18-20\u201d, \u201d21-24\u201d, \u201d25-29\u201d, \u201d30-34\u201d, \u201d35-44\u201d, \u201d45-54\u201d, \u201d55-64\u201d, \u201d65+\u201d) and gender (male/female) buckets; d) IBMal, which computes Central-Mal over k nearest neighbors (Cheng, Hu\u0308hn, and Hu\u0308llermeier 2009); e) logistic regression (LR), where L binary models were trained and we sorted their outputs to obtain a ranking; and f) pairwise approach (Hu\u0308llermeier et al. 2008), where L(L\u2212 1)/2 binary LR models were trained and we sorted the sum of their soft votes towards each label to obtain a ranking (PWLR). AMM-rank and PW-LR have O(NL2) and IB-Mal has O(N2L) time complexity, while the remaining methods are O(NL) approaches.\nCentral-Mal is a very simple and efficient baseline, and is an often-used method for basic content personalization. As the method simply predicts population\u2019s mean ranking, to improve its performance we considered AG-Mal, a method commonly used in practice, where we first compute mean rank for each age-gender group, and then use the group\u2019s mean rank as a prediction for qualified users. Further, IB-Mal is an instance-based method which is extremely competitive to the other state-of-the-art approaches (e.g., see Grbovic, Djuric, and Vucetic 2013), where we first find k nearest neighbors by considering feature vectors x and then predict Mallows mean ranking over the neighbors (due to large time cost, for each user we search for nearest neighbors in a subsampled set of 100,000 users). Lastly, we considered LR since it represents industry standard for ad targeting tasks, and PW-LR as it was shown to achieve state-of-the-art performance on a number of ranking tasks (Grbovic, Djuric, and Vucetic 2012b; Grbovic, Djuric, and Vucetic 2013). Due to large scale of the problem, we did not consider state-of-the-art methods such as mixture models which require iterative training (Grbovic, Djuric, and Vucetic 2012a; Grbovic et al. 2013). We also did not consider log-\nlinear model (Dekel, Manning, and Singer 2003), shown in (Grbovic, Djuric, and Vucetic 2013) to be outperformed by the IB-Mal, and do not report results of instance-based Plackett-Luce (Cheng, Dembczyn\u0301ski, and Hu\u0308llermeier 2010) due to observed limited performance.\nWe used Vowpal Wabbit package1 for logistic regression, BudgetedSVM (Djuric et al. 2014) for AMM, that we also modified to implement AMM-rank. We set \u03bd(i) = 1, i = 1, . . . , L, and used the default parameters from BudgetedSVM package for AMM-rank, with the exception of the \u03bb parameter which, together with competitors\u2019 parameters,\n1github.com/JohnLangford/vowpal_wabbit\nwas configured through cross-validation on a small held-out set; this resulted in k = 10 for IB-Mal. As discussed previously, we considered two versions of the ad targeting data:\n\u2022 adv - feature vector x does not include recency and intensity of categories from \u201dadv\u201d group (with d = 411);\n\u2022 adv, feature vector x does include recency and intensity of categories from \u201dadv\u201d group (with d = 511).\nBefore comparing the ranking approaches, it is informative to consider the examples of label ranks found by AGMal on adv data, given in Figure 2. We can see that there exist significant differences between different gender and age groups. Albeit the obtained ranks seem very intuitive, we will see shortly that AG-Mal is significantly outperformed by the other methods, illustrating complexity of the ranking task and the need for more involved approaches. In the following, we compare the algorithms using disagreement error \u01ebdis (Dekel, Manning, and Singer 2003), computed as a fraction of pairwise category preferences predicted incorrectly,\n\u01ebdis = 1\nNtest\nNtest \u2211\nt=1\nL \u2211\ni,j=1\nI ( (\u03c0ti \u227b \u03c0tj) \u2227 (\u03c0\u0302 \u22121 t\u03c0tj > \u03c0\u0302\u22121t\u03c0ti ) )\nLt ( L\u2212 0.5(Lt + 1) ) ,\n(15) as well as precision, recall, and F1 at the top K ranks,\nprecision@K = 1\nNtest\nNtest \u2211\nt=1\nK \u2211\ni=1\nI(\u03c0\u0302ti \u2208 \u03c0t)\nK ,\nrecall@K = 1\nNtest\nNtest \u2211\nt=1\nK \u2211\ni=1\nI(\u03c0\u0302ti \u2208 \u03c0t)\nLt ,\nF1@K = 2 \u00b7 precision@K \u00b7 recall@K precision@K + recall@K ,\n(16)\nwhich are commonly used measures for ranking problems. Here, \u03c0\u0302t denotes predicted label rank for the tth example.\nPerformance of the competing methods in terms of \u01ebdis, following 5-fold cross-validation, is reported in Table 1. We can see that the inclusion of ad view features resulted in large performance improvement, confirming findings from (Gupta et al. 2012) that past exposure to an ad increases propensity of a user to actually click the ad. As expected, multi-class AMM achieved poor performance as it optimizes only for the topmost category, and this result represents a lower bound on the disagreement loss. A simple baseline Central-Mal achieved higher error, which was decreased by\nonly a small margin using AG-Mal. We can see that IB-Mal resulted in significant performance improvement, however in large-scale, online setting it may be very inefficient. Logistic regression, a commonly used method in ad targeting tasks, obtained low error, further improved using the pairwise approach. However, state-of-the-art PW-LR was significantly outperformed by the proposed AMM-rank which achieved more than 10% better result. We note that, other than IB-Mal, the methods are very efficient, obtaining training and test times of less than 10 minutes on a regular machine.\nHowever, the main goal in ad targeting campaigns is not to infer the complete list of preferences for a user. Instead, we aim to find the top K most preferred categories, due to the constraint that we only have a limited budget for ad display, in terms of both time and space. Therefore, it is not of importance when two less preferred categories are misranked, and in the second set of experiments we explore how the label ranking methods perform in such setting. We considered showing K = {1, 2, . . . , 10} display ads, and for the top K ranks measure precision, recall, and F1 score of the categories on which the user clicked during the testing period. The results obtained by the label ranking algorithms are illustrated in Figure 3. We can see that AMM-rank outperformed the competitors, achieving better performance for all values ofK . This becomes even more relevant when we consider that even a small improvement in a web-scale setting of targeted advertising may result in a significant revenue increase for the publisher. We can conclude that the results strongly suggest advantages of the proposed approach over the competing algorithms in large-scale label ranking tasks."}, {"heading": "Conclusion", "text": "In order to address challenges brought about by the scale of the online advertising tasks that renders many state-of-theart methods inefficient, we introduced AMM-rank, a novel, non-linear algorithm for large-scale label ranking. We evaluated its performance on a real-world ad targeting data comprising more than 3 million users, thus far the largest label ranking data considered in the literature. The results show that the method outperformed the competing approaches by a large margin in terms of both rank loss and retrieval measures, indicating that the AMM-rank algorithm is a very suitable method for solving large-scale label ranking problems."}, {"heading": "1814. ACM.", "text": "[Riecken 2000] Riecken, D. 2000. Personalized views of personalization. Communications of the ACM 43(8):27\u201328.\n[Tuzhilin 2009] Tuzhilin, A. 2009. Personalization: The state of the art and future directions. Business Computing 3:3.\n[Tyler et al. 2011] Tyler, S. K.; Pandey, S.; Gabrilovich, E.; and Josifovski, V. 2011. Retrieval models for audience selection in display advertising. In Proceedings of the 20th ACM International Conference on Information and Knowledge Management, 593\u2013598. ACM.\n[Vembu and Ga\u0308rtner 2011] Vembu, S., and Ga\u0308rtner, T. 2011. Label ranking algorithms: A survey. In Preference learning. Springer. 45\u201364.\n[Wang et al. 2011] Wang, Z.; Djuric, N.; Crammer, K.; and Vucetic, S. 2011. Trading representability for scalability: Adaptive multi-hyperplane machine for nonlinear classification. In ACM SIGKDD Conf. on Knowledge Discovery and Data Mining.\n[Weston et al. 2012] Weston, J.; Wang, C.; Weiss, R.; and Berenzweig, A. 2012. Latent collaborative retrieval. In Proceedings of the 29th International Conference on Machine Learning, 9\u201316."}], "references": [{"title": "Targeting converters for new campaigns through factor models", "author": ["Pandey Agarwal", "D. Josifovski 2012] Agarwal", "S. Pandey", "V. Josifovski"], "venue": "In Proceedings of the 21st International Conference on World Wide Web,", "citeRegEx": "Agarwal et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Agarwal et al\\.", "year": 2012}, {"title": "Scalable distributed inference of dynamic user interests for behavioral targeting", "author": ["Ahmed"], "venue": null, "citeRegEx": "Ahmed,? \\Q2011\\E", "shortCiteRegEx": "Ahmed", "year": 2011}, {"title": "Web-scale user modeling for targeting", "author": ["Aly"], "venue": null, "citeRegEx": "Aly,? \\Q2012\\E", "shortCiteRegEx": "Aly", "year": 2012}, {"title": "Computational advertising and recommender systems", "author": ["A.Z. Broder 2008] Broder"], "venue": "In Proceedings of the ACM conference on Recommender systems,", "citeRegEx": "Broder,? \\Q2008\\E", "shortCiteRegEx": "Broder", "year": 2008}, {"title": "Learning to rank: From pairwise approach to listwise approach", "author": ["Cao"], "venue": null, "citeRegEx": "Cao,? \\Q2007\\E", "shortCiteRegEx": "Cao", "year": 2007}, {"title": "Personalization versus privacy: An empirical examination of the online consumers dilemma. Information Technology and Management 6(2-3):181\u2013202", "author": ["Chellappa", "R.K. Sin 2005] Chellappa", "R.G. Sin"], "venue": null, "citeRegEx": "Chellappa et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Chellappa et al\\.", "year": 2005}, {"title": "Label ranking methods based on the Plackett-Luce model", "author": ["Dembczy\u0144ski Cheng", "W. H\u00fcllermeier 2010] Cheng", "K. Dembczy\u0144ski", "E. H\u00fcllermeier"], "venue": "In Proceedings of the 27th International Conference on Machine Learning,", "citeRegEx": "Cheng et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Cheng et al\\.", "year": 2010}, {"title": "Decision tree and instance-based learning for label ranking", "author": ["H\u00fchn Cheng", "W. H\u00fcllermeier 2009] Cheng", "J. H\u00fchn", "E. H\u00fcllermeier"], "venue": "In Proceedings of the 26th International Conference on Machine Learning,", "citeRegEx": "Cheng et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Cheng et al\\.", "year": 2009}, {"title": "Google news personalization: Scalable online collaborative filtering", "author": ["Das"], "venue": null, "citeRegEx": "Das,? \\Q2007\\E", "shortCiteRegEx": "Das", "year": 2007}, {"title": "Log-linear models for label ranking", "author": ["Manning Dekel", "O. Singer 2003] Dekel", "C. Manning", "Y. Singer"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Dekel et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Dekel et al\\.", "year": 2003}, {"title": "BudgetedSVM: A toolbox for scalable SVM approximations", "author": ["Djuric"], "venue": "Journal of Machine Learning Research", "citeRegEx": "Djuric,? \\Q2014\\E", "shortCiteRegEx": "Djuric", "year": 2014}, {"title": "A kernel method for multi-labelled classification", "author": ["Elisseeff", "A. Weston 2001] Elisseeff", "J. Weston"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Elisseeff et al\\.,? \\Q2001\\E", "shortCiteRegEx": "Elisseeff et al\\.", "year": 2001}, {"title": "Supervised clustering of label ranking data using label preference information", "author": ["Grbovic"], "venue": "Machine Learning", "citeRegEx": "Grbovic,? \\Q2013\\E", "shortCiteRegEx": "Grbovic", "year": 2013}, {"title": "Learning from pairwise preference data using Gaussian mixture model. Preference Learning: Problems and Applications in AI 33\u201335", "author": ["Djuric Grbovic", "M. Vucetic 2012a] Grbovic", "N. Djuric", "S. Vucetic"], "venue": null, "citeRegEx": "Grbovic et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Grbovic et al\\.", "year": 2012}, {"title": "Supervised clustering of label ranking data", "author": ["Djuric Grbovic", "M. Vucetic 2012b] Grbovic", "N. Djuric", "S. Vucetic"], "venue": "In SDM,", "citeRegEx": "Grbovic et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Grbovic et al\\.", "year": 2012}, {"title": "Multi-prototype label ranking with novel pairwise-to-total-rank aggregation", "author": ["Djuric Grbovic", "M. Vucetic 2013] Grbovic", "N. Djuric", "S. Vucetic"], "venue": "In Proceedings of the 23rd International Joint Conference on Artificial Intelligence", "citeRegEx": "Grbovic et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Grbovic et al\\.", "year": 2013}, {"title": "Factoring past exposure in display advertising targeting", "author": ["Gupta"], "venue": "In Proceedings of the ACM SIGKDD International Conference on Knowledge Discovery and Data mining,", "citeRegEx": "Gupta,? \\Q2012\\E", "shortCiteRegEx": "Gupta", "year": 2012}, {"title": "Constraint classification for multiclass classification and ranking", "author": ["Roth Har-Peled", "S. Zimak 2003] Har-Peled", "D. Roth", "D. Zimak"], "venue": "In Proceedings of the 16th Annual Conference on Neural Information Processing Systems,", "citeRegEx": "Har.Peled et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Har.Peled et al\\.", "year": 2003}, {"title": "Combining predictions in pairwise classification: An optimal adaptive voting strategy and its relation to weighted voting. Pattern Recognition 43(1):128\u2013142", "author": ["H\u00fcllermeier", "E. Vanderlooy 2010] H\u00fcllermeier", "S. Vanderlooy"], "venue": null, "citeRegEx": "H\u00fcllermeier et al\\.,? \\Q2010\\E", "shortCiteRegEx": "H\u00fcllermeier et al\\.", "year": 2010}, {"title": "Label ranking by learning pairwise preferences", "author": ["H\u00fcllermeier"], "venue": "Artificial Intelligence", "citeRegEx": "H\u00fcllermeier,? \\Q2008\\E", "shortCiteRegEx": "H\u00fcllermeier", "year": 2008}, {"title": "Efficient clustering for orders", "author": ["Kamishima", "T. Akaho 2006] Kamishima", "S. Akaho"], "venue": "In ICDM Workshops,", "citeRegEx": "Kamishima et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Kamishima et al\\.", "year": 2006}, {"title": "Know your personalization: Learning", "author": ["Majumder", "A. Shrivastava 2013] Majumder", "N. Shrivastava"], "venue": null, "citeRegEx": "Majumder et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Majumder et al\\.", "year": 2013}, {"title": "Experience with personalization on Yahoo! Communications of the ACM 43(8):35", "author": ["Patel Manber", "U. Robison 2000] Manber", "A. Patel", "J. Robison"], "venue": null, "citeRegEx": "Manber et al\\.,? \\Q2000\\E", "shortCiteRegEx": "Manber et al\\.", "year": 2000}, {"title": "Learning to target: what works for behavioral targeting", "author": ["Pandey"], "venue": "In Proceedings of the 20th ACM International Conference on Information and Knowledge Management,", "citeRegEx": "Pandey,? \\Q2011\\E", "shortCiteRegEx": "Pandey", "year": 2011}, {"title": "Retrieval models for audience selection in display advertising", "author": ["Tyler"], "venue": "In Proceedings of the 20th ACM International Conference on Information and Knowledge Management,", "citeRegEx": "Tyler,? \\Q2011\\E", "shortCiteRegEx": "Tyler", "year": 2011}, {"title": "Label ranking algorithms: A survey", "author": ["Vembu", "S. G\u00e4rtner 2011] Vembu", "T. G\u00e4rtner"], "venue": "In Preference learning", "citeRegEx": "Vembu et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Vembu et al\\.", "year": 2011}, {"title": "Trading representability for scalability: Adaptive multi-hyperplane machine for nonlinear classification", "author": ["Wang"], "venue": "In ACM SIGKDD Conf. on Knowledge Discovery and Data Mining", "citeRegEx": "Wang,? \\Q2011\\E", "shortCiteRegEx": "Wang", "year": 2011}, {"title": "Latent collaborative retrieval", "author": ["Weston"], "venue": "In Proceedings of the 29th International Conference on Machine Learning,", "citeRegEx": "Weston,? \\Q2012\\E", "shortCiteRegEx": "Weston", "year": 2012}], "referenceMentions": [{"referenceID": 3, "context": "has garnered significant interest from the machine learning community, as witnessed by a large number of recent workshops and publications (Broder 2008; Pandey et al. 2011; Majumder and Shrivastava 2013).", "startOffset": 139, "endOffset": 203}, {"referenceID": 15, "context": "This idea was extended in (Grbovic et al. 2013; Grbovic, Djuric, and Vucetic 2013), where authors use feature vectors to supervise clustering, resulting in improved performance.", "startOffset": 26, "endOffset": 82}, {"referenceID": 15, "context": "Due to large scale of the problem, we did not consider state-of-the-art methods such as mixture models which require iterative training (Grbovic, Djuric, and Vucetic 2012a; Grbovic et al. 2013).", "startOffset": 136, "endOffset": 193}], "year": 2016, "abstractText": "We consider the problem of personalization of online services from the viewpoint of ad targeting, where we seek to find the best ad categories to be shown to each user, resulting in improved user experience and increased advertisers\u2019 revenue. We propose to address this problem as a task of ranking the ad categories depending on a user\u2019s preference, and introduce a novel label ranking approach capable of efficiently learning non-linear, highly accurate models in large-scale settings. Experiments on a real-world advertising data set with more than 3.2 million users show that the proposed algorithm outperforms the existing solutions in terms of both rank loss and top-K retrieval performance, strongly suggesting the benefit of using the proposed model on large-scale ranking problems.", "creator": "LaTeX with hyperref package"}}}