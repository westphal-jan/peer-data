{"id": "1606.09296", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "29-Jun-2016", "title": "How Many Folders Do You Really Need?", "abstract": "Email classification is still a mostly manual task. Consequently, most Web mail users never define a single folder. Recently however, automatic classification offering the same categories to all users has started to appear in some Web mail clients, such as AOL or Gmail, and many of these users now know exactly how to categorize them as users.\n\n\n\nNow that we know the actual types of web mail, a lot of Web mail users don't know how to categorize the types of web mail users want, it's possible to use a standard classification system to classify web mail as users without knowing how to categorize them as users.", "histories": [["v1", "Wed, 29 Jun 2016 21:35:24 GMT  (234kb,D)", "http://arxiv.org/abs/1606.09296v1", "10 pages, 12 figures, Proceedings of the 23rd ACM International Conference on Information and Knowledge Management (CIKM 2014), Shanghai, China"]], "COMMENTS": "10 pages, 12 figures, Proceedings of the 23rd ACM International Conference on Information and Knowledge Management (CIKM 2014), Shanghai, China", "reviews": [], "SUBJECTS": "cs.AI cs.HC cs.SI", "authors": ["mihajlo grbovic", "guy halawi", "zohar karnin", "yoelle maarek"], "accepted": false, "id": "1606.09296"}, "pdf": {"name": "1606.09296.pdf", "metadata": {"source": "CRF", "title": "How Many Folders Do You Really Need? Classifying Email into a Handful of Categories", "authors": ["Mihajlo Grbovic", "Guy Halawi", "Zohar Karnin", "Yoelle Maarek"], "emails": ["zkarnin}@yahoo-inc.com,", "yoelle@ymail.com", "permissions@acm.org."], "sections": [{"heading": null, "text": "Categories and Subject Descriptors H.4.3 [Information Systems Applications]: Communications Applications\u2014Electronic Email\nGeneral Terms Information Systems; Algorithms; Experimentation\nKeywords Email Classification; Machine-generated Email; LDA\nPermission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than ACM must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org. CIKM\u201914, November 3\u20137, 2014, Shanghai, China. Copyright 2014 ACM 978-1-4503-2598-1/14/11 ...$15.00. http://dx.doi.org/10.1145/2661829.2662018."}, {"heading": "1. INTRODUCTION", "text": "Email classification has been a user\u2019s pain point since the early days of email systems. In the last decade, most attempts at automating this task have consisted in mimicking each individual user\u2019s personal classification habits [12]. This highly personalized approach exhibits two main weaknesses: it relies on small datasets as each individual inbox is analyzed independently, and it requires from users to have defined meaningful folders in the first place. We believe that relying on user-defined folders is the reason for which such methods were never widely adopted. It was shown in [13] that 70% of users do not create even a single folder. Furthermore, we have verified on a large dataset, generated from Yahoo mail,that only 10% of users are \u201cactive classifiers\u201d, the latter being defined as users who moved more than 10 messages into folders over a period of 6 months.\nThis fact has not remained unnoticed. A few of commercial Web services have changed their approach towards email classification by offering \u201cclasses\u201d (implemented as labels, or categories for instance) that are common to all users. Thus, AOL mail, with its \u201cBulk Senders\u201d category, or Gmail, with its inbox tabs1, offer a few \u201ccommon\u201d classes. On the other end of the range, Koren et al. in [13], have analyzed popular folders, in order to identify not just a few but several thousand possible common classes or \u201ctags\u201d. While the wide range of tags is attractive, one weakness of this approach is the high variance in their level of abstraction. Highly subjective tags such as mom co-exist with more objective ones like recipes, or sender-based tags such as amazon overlap with related ones like shopping. In addition, they achieve recall and precision rates of only 80% for a coverage of 72%. This work explores a middle ground direction, where we propose, like in [13], to derive high level-classes from mail data, and especially folders data provided by the minority of users who do define folders. However, similarly to AOL or Gmail, we give preference to fewer, consistent classes, for a more user-friendly experience, and offer these classes to all users, including those who did not define any folder.\nFirst, we propose to exploit the fact that today\u2019s commercial Web mail traffic is dominated by machine-generated messages, originating from mass senders, such as social networks, e-commerce sites, travel operators etc. In their experiments, Ailon et al. stated that machine-generated mail represent more than 60% of Yahoo mail traffic, [1]. We argue\n1These tabs include \u201cPromotions\u201d, \u201cSocial\u201d, \u201cUpdates\u201d and \u201cForums\u201d, see https://support.google.com/mail/ answer/3055016?hl=en&p=inboxtabs&rd=1\nar X\niv :1\n60 6.\n09 29\n6v 1\n[ cs\n.A I]\n2 9\nJu n\n20 16\nhere that, given the difference in syntax and semantics between human- and machine-generated messages, one should first distinguish between machine and human-generated messages before attempting any finer classification. Once focusing on machine-generated messages, with huge numbers (millions in some cases) of users receiving the same type of machine-generated messages, data will be much less sparse, and classification techniques should become more precise. In order to validate our intuition, we estimated the actual volume of machine-generated messages on a very large Yahoo mail dataset. This dataset, built for the purpose of this work, covers 6 months of email traffic and more than 500 billion messages. For privacy reasons, it includes messages belonging exclusively to users who voluntarily opted-in for such studies and message bodies were not inspected by humans. We conducted experiments on this dataset (as detailed later on) and verified that non-spam machine-generated messages actually represent 90% of the entire dataset, which encouraged us to proceed.\nIn the same dataset, we considered the subset of folders data that include messages classified by users into folders as well as folders labels, generated by more than 40 million users. Instead of a priori fixing high level classes, or deriving them from popular folders labels, as done by the previously mentioned efforts, we attempt here another approach. Namely, we propose to discover these categories by applying an LDA (Latent Dirichlet Allocation) approach [4], on the folder dataset. We explain how \u201clatent\u201d categories can be identified in Section 2. More specifically, we detail how one human and five machine-generated categories can be inferred directly from the data. We introduce in Section 3 our email classification model and its associated features, such as content, sender, behavioral, and temporal behavioral features. We also justify the different aggregation levels we considered, namely message-, sender- and domain-level aggregation. An additional challenge is to obtain a sufficiently large training set. Due to privacy issues, the cost of editorial work, and the skewness of the email data, we use various automated methods. For the machine latent categories, our main data source is the folder data. Using our LDA analysis we obtain for a large number of messages a clear linkage to our latent categories. For the human category, folder data is not effective, and we use instead various heuristics leveraging sender information and extrapolated data via cotraining inspired techniques. We detail how we generated the training data in Section 4. Section 5 describes our classification mechanism and Section 6 presents our results. In section 7 we provide some additional statistics to demonstrate the impact of our classification system. Note that we exclusively consider consumer Web mail as opposed to corporate email, whose traffic is drastically different in size and nature. Consequently, we did not compare our results to the Enron dataset. Related work is discussed as relevant throughout the paper.\nThe key contributions of our work are the following: (1) we provide new insights on the importance of distinguishing between human- and machine-generated email, (2) we give to the best of our knowledge, the first large scale datadriven validation of the intuition that a few consistent automatically discovered categories cover most of today\u2019s Web email traffic, and (3) we describe an end-to-end Web mail categorization solution, including detailed models, learning mechanisms, evaluation methods, and results on real traffic."}, {"heading": "2. DISCOVERING LATENT CATEGORIES", "text": "Following [16], we define email categorization as the task of \u201cassigning a Boolean value to each pair (dj , ci) \u2208 D \u00d7 C, where D is a domain of documents\u201d (here mail messages) \u201cand C is a set of predefined categories.\u201d Previous research work on personalized email classification used one C set per user and populated it by reusing the user\u2019s existing folders, see [15, 7, 2, 10]. The approach we advocate for here is to adopt one single C for all users. Still C can be defined in many different manners: categories can relate to message importance (see Gmail Priority Inbox2), number of recipients (see AOL Bulk Senders), senders (see popular folders such as Amazon, Facebook), or types (see Gmail Labs Smart Labels2 Social, Promotion, Updates, Forums, Travel, Finance) for instance. In addition, C should be derived from data (rather than predefined) and should fulfill the following requirements.\n1. The number of categories should be small so as not to overwhelm the user. Indeed, it has been shown that, with more than 20 folders, folder-based discovery becomes less effective than search [3].\n2. Categories must be easily interpretable and at the same level of abstraction.\n3. Categories should cover a significant amount of email volume.\nWe first examined all messages that have the potential to be classified by retrieving the most \u201cpopular\u201d folders in terms of users\u2019s \u201cfoldering\u201d actions. We ignored system folders (e.g., \u201ctrash\u201d, \u201cspam\u201d) and folders created by third-party services such as OtherInbox3. We also removed small folders that count less than 1,000 messages, since they would not cover common needs. We remained with 100, 000 humangenerated folders. For illustration, we list here the top folders thus identified: ebay, accounts, personal, school, saved mail, jobs, amazon, taxes, recipes, business, college, bills, house ,facebook, paypal, save, food, linkedin, pictures, surveys, travel, jobs, misc. etc.\nWe then associated with each of these 100, 000 folders an artificial document obtained by concatenating across all relevant inboxes, all messages classified into this folder. We applied LDA [4], to these \u201cdocument folders\u201d. LDA is commonly used in natural language processing in order to discover a set of latent topics that generated a given set of documents. In our context, we hoped that latent topics would map into \u201clatent categories\u201d , which would then define our set C for further classification. To this effect, we trained an online LDA model [9]. Recall that LDA receives as input a set of documents (folders), each associated with a bag of words (words of messages put in the folder), and a number of topics K. It outputs for each topic the top words associated with it and the topic mixture of each document. We iterated over several values of number of topics, from K = 50 to K = 5. To choose the right K, we looked at coverage, in terms of portion of traffic covered by folders that were dominantly (> 50%) associated with topics. For a value of K and a specific topic, the coverage of the topic is defined as the amount of traffic covered by folders that are associated\n2http://gmailblog.blogspot.co.il/2011/03/ new-in-gmail-labs-smart-labels.html 3http://www.otherinbox.com\nwith said topic by at least 50% by the LDA output. The total coverage associated with a value K is the sum of coverages associated with the topics. Our objective was to find a value of K that would ensure that each individual topic as well as the overall set of topics achieve significant coverage. We observed that larger values of K produced topics that were narrow, e.g. Knitting, Cooking, Astrology with small coverage. The traffic coverage at K = 50 was 81%, and went down to 74% at K = 25, to about 70% at K = 6. At K = 6 the coverages of the topics were sufficiently large (see Table 9), while at larger values of K narrow topics emerged.\nWe further examined the topics obtained for K = 6, as this value exposed a good balance between total and individual coverage, by inspecting their associated vector of words and weights. As evidenced in the five rightmost columns of Table 1, the words associated with each of these fives topics clearly shared a common underlying concept, respectively career, shopping, travel, financial and social. One topic remained that was a bit puzzling at first, as it contained many highly frequent (stop) words and no key concept seemed to emerge from them. After some quick examination of the messages (simply looking at senders) associated with this topic, we discovered that most were human-generated, as opposed to the five other topics that are mostly machine generated. The word associated with this first topic are quite frequent in personal communications, consequently we named this last topic human.\nGiven these 6 labeled topics, we further verified their distribution over a sample of popular folders, as shown in Table 2. Some mappings between folders and latent topics were almost perfect, like the folder \u201cfinancials\u201d being associated with the latent topic financial with a 96% weight, or \u201chotels\u201d with the topics travel at 74%, with still a flavor of financial at 15%. More interestingly, a highly subjective folder like \u201cmom and dad\u201d had no clear winner. While it did reflect the human topic at 31%, it also (and ironically so) had financial at 55%! Based on this analysis, we finalized our decision of choosing as C {human, career, shopping, travel, financial}, as it meets our 3 requirements of small size, same abstraction level and coverage."}, {"heading": "3. MODELING EMAIL", "text": "One key challenge in any classification task is data modeling and fixing the level of granularity of data points i.e., the objects to classify. One option is to consider each individual message as a single data point, associated with various features extracted from the message header and body. A second approach consists in aggregating messages at higher levels, for instance at the SMTP address level [11], (referred to as \u201csender\u201d), or at the mail domain level (easily extracted from the SMTP address by truncating its prefix up to the character\u201c@\u201d.) This latter approach not only helps with sparseness issues but more importantly allows for much more efficient processing, without requiring expensive body analysis and feature extraction. This is critical at the scale of Web mail, when huge numbers of messages need to be classified at delivery time. We explain how we use a combination of both approaches in Section 5. Feature extraction and different aggregation levels are described in the rest of this Section."}, {"heading": "3.1 Extracting Features", "text": "We follow the general categorization framework where numerical features are extracted from each object (in our case, message, sender or domain) later to be fed to a learning mechanism that given labelled examples, e.g. messages with manually assigned categories, will be able to categorize unlabeled objects. In this section we describe the types of features and how they are obtained.\nA data point consists of content features, address features, and behavioral features which include a special subcategory of temporal behavior features.\nContent features include features derived from the message subject and body. We extract words from the subject line and message body, as well as the subject character length, body character length and the number of urls occurring in the body. To form sender features, we use the total counts of observed words across all messages from that sender, as well as the average, the minimum and the maximum subject and body lengths and url counts. In large databases, such as the one we are dealing with, the most frequent words can easily occur millions of times (e.g. \u201cthe\u201d, \u201ca\u201d, \u201cin\u201d). We eliminate the top 400 words found across most senders, as well as the usual stop words4 for several common languages. Finally, we filtered out the low frequency words that occur in less than 100 different senders.\nAddress features include features extracted from the sender email address, including the subdomain, e.g. (.edu, .gov, etc.), subname (e.g. billing, noreply). To extract the subdomains we split the domain part of the address (after\n4http://anoncvs.postgresql.org/cvsweb.cgi/pgsql/ src/backend/snowball/stopwords\nthe \u201c@\u201d character) at the delimiter \u201c.\u201d location, and use the resulting words as features. We do the same for the subnames, but on the name portion of the email (before the \u201c@\u201d sign). We also extract \u201ccommercial\u201d keyword from senders. To this effect, we use a list of more than 500 keywords, typically used as bid words in ad targeting, such as itinerary, flight, ticket, order, confirmation, billing, payslip, payment, transaction, stocktrade, career, shopping, travel, etc., and look for a match in the email address. To counter the imbalance between rare and frequent words, we remove the 800 most frequent sender substrings. This avoids poor generalization when using biased training data. For example, very common domain names, such as \u201cyahoo.com\u201d, or \u201cgmail.com\u201d, could get picked as the most informative features in a model trained to detects human senders. This rule clearly does not generalize well outside the training dataset. In addition to removing the most frequent substrings, we remove the substrings that appear in less than 100 senders. We also map very rare or random strings into a canonical form, for instance 5tsdfocfyf66c@bookstore.com, is mapped into a single feature \u201crandom string\u201d.\nBehavioral features include features extracted from the sender\u2019s and recipient\u2019s actions over a given message. They can be split into outbound, inbound and action features. The outbound features for the n-th sender cover the sender\u2019s outgoing activity, such as weekly and monthly volumes of sent messages, histogram of the number of recipients in their messages, volumes of messages sent as a reply (indicated by\u201cre:\u201d in the subject line), volumes of messages sent as forward (with FW: in the subject line). The inbound features for the n-th sender cover the sender\u2019s incoming activity, such as volume of the messages received by the sender, as regular, reply or forward messages. The action features for the nth sender cover the activity of the sender\u2019s email recipients, such as how many times the messages from the n-th sender were read, deleted, replied to, forwarded, ended up in spam or any other folder. To create these features, raw counts are converted to percentages of total inbound volume (e.g., percentage of n-th sender messages moved to trash). The names of the folders to which a given sender messages have been moved, are especially interesting: they can be seen as a human label of a specific sender. Therefore, for the n-th sender, we also use the names of folders (with the counts of moves to that folder ) as folder features. We remove the folders that contain less than 100 different senders, as well as the 50 folders with the highest number of senders, including system folders (\u201ctrash\u201d) and third-party services folders.\nTemporal behavior features form a subcategory of the behavioral features. They reflect the frequency of specific actions over a given period of time. For instance, we record whether a sender sends more than X messages in an hour. These features are represented as a histogram, where X takes as values: 10, 60, 80, 100, 120. We refer to them as the burst features.\nMany of the features described above are counts of actions or words. To handle large counts, we normalize the data at an instance level by using log(1+x(k)) instead of the original k-th feature value x(k). Features that represent percentages of total traffic (e.g. moves to trash, spam, etc.) are left in the 0 to 1 range."}, {"heading": "3.2 Email Aggregation Levels", "text": "We consider three levels of aggregation, by message, sender\nand domain. To choose the right level of aggregation for our system, we consider what we lose and what we gain by aggregating to the next level, starting from the level of the email message. We consider several dimensions: generalization loss, data size and classification scalability.\nGeneralization loss: Aggregating messages offers some risks, if by aggregating we put in the same class messages that should have been mapped to different categories. One typical mistake that occurs when aggregating by sender occurs when a given sender changes its behavior. It happens for instance when a sender (such as a recruiter, finance broker or a travel agent) sends machine generated messages to clients in 90% of the cases, and personal messages otherwise. We refer to this issue as the machine-human mix. In some rare cases, we observed another type of loss we call the multiple businesses loss, where a sender is a company that has more than one type of business and sends all messages from the same address. Although many companies conduct multiple types of business, they typically use different addresses for each type, hence the multiple businesses loss is quite low. Domain-level aggregation, as the most aggressive type, suffers the most loss; in particular the multiple businesses loss is quite common there. Table 3 shows some examples of generalization loss due to aggregating at a domain level.\nData size: Data size is a key factor in deciding the right aggregation level as learning and categorizing at this scale is most challenging. Table 4 shows the number of data points in our dataset for the various aggregation levels. As expected sender-level aggregation drastically reduces the data size, going from more than one trillion messages to about 330 million senders after aggressive filtering. Domain-level aggregation brings an additional decrease, bringing us to about 75 million domains, a significant drop but not as drastic as the one brought by sender-level aggregation.\nClassification scalability: The highest gain in scalability is between sender and message aggregation. Indeed,\nextracting features from each message and running a classifier upon delivery might be extremely costly. On the other hand, mapping each message into its sender presents huge performance advantages. The classification process can be conducted offline on all the previously seen senders, and an incoming message can be assigned the category associated with its sender via a simple (yet very large) table lookup.\nConsequently, we propose here a two-stage approach. The majority of the email traffic should be classified by sender for performance optimization. However, whenever the classifier returns a low confidence score for a given message sender, the more costly message-based classification should be applied. This approach is detailed in Section 5."}, {"heading": "4. TRAINING DATA", "text": "One key challenge in Web mail classification is to generate a labeled training dataset given the inherent size of the data as well as privacy issues. We consider here 3 types of labeling techniques: manual, heuristic-based and automatic, which we discuss below. We used as labels our 6 latent categories, as well as the human and machine labels to differentiate between human- and machine-generated messages."}, {"heading": "4.1 Manual labeling", "text": "Manual labeling consists of having human editors assign labels to specific examples. This method has some clear weaknesses. Given the data sensitivity, even with users that agreed having their mail examined, only a limited number of responsible editors can be involved. Then it becomes more difficult to have multiple editors looking at the same data, which is a must given the nature of the task. Indeed, looking at a confusion matrix, we observed many inconsistencies among editors, for instance between Shopping and Finance or Travel and Finance. We nevertheless generated such a dataset, denoted as Dman, by giving to 5 paid editors a pool of about 18, 000 messages originating from about 14, 000 senders. This pool contained multiple messages per sender, with some overlap between editors\u2019 assignments. Close to 400 inconsistent labels were generated, which were resolved via additional editor intervention."}, {"heading": "4.2 Heuristic labeling", "text": "Heuristic labeling consists of applying heuristic rules derived from world knowledge. Such labeling achieves high precision but is limited in coverage. We used this type of labeling mostly for differentiating between human and machine senders. One key challenge here lies in the fact that SMTP domain information is not sufficient. Indeed, large Web mail services are not only used by humans. Nonspam machine-generated messages can originate from domains such as gmail.com, hotmail.com, or yahoo.com, even for small distribution volumes. Conversely, domains such as amazon.com or ibm.com, intel.com or hp.com can be associated with both machine and personal communications of corporate employees. We focused on corporate domains as they represent the main source of errors between human and machine labels, with the following simple heuristics. In order to identify corporate machine senders, we spotted reserved words such as mailer-daemon or no-reply, or repeating occurrences of words such as unsubscribe in message headers. We also used signals such as spam votes. In order to identify human senders, we looked for patterns such as <first name>.<lastname> and other var-\nAlgorithm 1 Folder-based voting for assigning labels to senders Inputs: Set of labeled folders F , Unlabeled sender dataset Du, vote threshold \u03c4v = 50, folder threshold \u03c4f = 2 Output: Labeled sender dataset Dl (subset of entire dataset Du)\n1. Initialize Dl to empty set 2. For each unlabeled sender xn = 1, ..., Nu 3. Find labeled subset of folders fi \u2208 F that sender n\nwas moved to by the user, with counts of moves cfi 4. Sum the move counts by labels y \u2208 Y, cy = \u2211 fi\u2208y\ncfi 5. Identify the class with most votes yn and number of\nfolders that voted for that class numfi\u2208y 6. If cy > \u03c4v and numfi\u2208y > \u03c4f 7. Label sender n with class yn, add it to Dl 8. End 9. End\nious regular expressions. We also applied a semi-supervised approach similar to co-training [5], in order to further increase our heuristic dataset, remove false positives, and most importantly obtain a less skewed sample. In a nutshell, the idea is to split the features into two independent sets, train a classifier on one set, then use the labels with high confidence to train a classifier on the second set, then possibly iterate on all features with all of the labels. Eventually, a heuristic dataset denoted Dh\u2212m was generated, which consists of 60, 000 human senders and 80, 000 machine senders. Most of the machine senders were further assigned machine latent categories as labels, using the techniques outlined below. The remaining ones were labeled as Other in the final dataset used for training of the production model."}, {"heading": "4.3 Automatic labeling", "text": "For automatic labeling we apply two types of techniques, folder-based majority and LDA voting. Folder-based majority voting (see Algorithm 1) leverages user-generated folders to obtain ground truth labels for senders. There are hundreds of folder names that relate to \u201cshopping\u201d, \u201cfinance\u201d, or \u201ctravel\u201d, etc. Since there are much fewer folders than there are senders, we manually labeled close to 600 folders that carry the meaning of our latent categories, and let the folders vote for senders label. A subset of the labeled folders is shown in Table 5. Note that we did not include the Human category since we hardy observed any folder that would be perfectly aligned with it. Even our previous example of mom and dad folder in Table 2 includes machine-generated messages as evidenced by its strong Finance topic. We also saw multiple examples of human folders, related to family and friends, that contained messages from known vendors or travel companies. The labeling procedure is detailed in Algorithm 1, with threshold parameters chosen via grid search.\nFor each sender, we identify the folders with labels that contain messages originating from this sender. Then, the folders vote for the labels. If there is enough confidence for the winning label, as per threshold, the sender is labeled accordingly. Several examples of the labeled senders with individual folder votes are given in Table 7. After applying this procedure to the entire dataset of senders, we end up with more than 81, 000 labeled senders that form Df . In the process of merging Df with Dman and Dh\u2212m, there were approximately 5, 000 matches (same label assigned) and 700 conflicts (different label assigned). Senders that were labeled as machine in Dh\u2212m and were assigned a machine latent category in Df or Dman, were not counted as conflicts. The\nconflicts were resolved by human intervention. We denote the resulting merged dataset by Dv1.\nFolder-based LDA voting is the second technique we used in order to scale-up the majority vote technique. It is needed mostly because the majority of folders cannot be labeled, either because the folder name is not descriptive enough or it contains a mix of messages. As one of the results of LDA training, we know the topic distributions of 100, 000 folders counting more than 1, 000 messages, as illustrated in Table 2. We used these topics for \u201csoft voting\u201d: instead of assigning 1 vote, each folder assigns partial votes to the senders of the messages it contains, using topic weights. We then re-normalized the votes to sum up to 1 in order to create a labeled set from the senders with a topic score above 80%. This process produced 42, 000 labeled senders that already existed in Dv1 as well as 56, 000 additional new senders. The labels of senders that were already in Dv1 mostly agreed with the existing ones, with only 121 conflicts, not counting the machine labels that received a subcategory label. The final, merged dataset Dv2 contains 210, 000 senders. Table 6 shows the counts of human and machine labels in Dv2."}, {"heading": "5. CLASSIFICATION MECHANISM", "text": "The classification mechanism put in place has two major elements. The first is the online mechanism, designed in a cascading manner mainly to account for scalability. The online system leverages both message-level and sender-level classifiers in order to categorize incoming messages, while taking into consideration the strict requirements of a real time Web mail system. The system has three stages: (1) lightweight classification, (2) sender-based classification, and (3) heavy-weight message-based classification. The second component is the offline component where (1) we periodically classify the known email senders into the 6 categories and (2) train a multi-label message classifier. The online system diagram is given in Figure 1. We describe its components in detail below.\nOnline lightweight classification: The initial classification is at a message-level, consisting of hard-coded rules designed to quickly classify a significant portion of email traffic. Easy cases include messages from the top 100 senders that cover a significant percentage of the total traffic and are category consistent. Also, as a heuristic decision, we categorized all reply/forward messages as human. By taking precedence over sender-level, message-level classification\nhelps avoid generalization errors due to mixed human-machine senders. As for performance, the process described requires very few resources and covers 32% of the email traffic.\nOnline sender-based classification: The second phase in our cascade classification process involves looking for the sender in a lookup table containing senders with known categories. This table is created in our offline component, described below. A sender that does not appear in said table is absent due to one of the two reasons; (1) it was either never, or hardly seen in the past meaning that the sender features are too sparse and noisy to be useful, or (2) the offline classification process did not have an answer with a sufficient confidence. The second type of missing senders are those with the most chance of being mixed, meaning that the messages originating from them should not all be assigned the same category. The amount of traffic that is not covered by this phase is roughly 8%.\nOnline Heavy-weight classification: Email messages whose sender did not appear in the classified sender table are sent to a heavy-weight message based classifier. As only 8% of the traffic end up in this last phase we can afford slightly heavier computations than we would have afforded had we employed the classifier on all incoming email. Here, we use all relevant feature, pertaining to the message body, subject line and sender name.\nOffline creation of classified senders table: We use the training set Dv2 described in Section 4 in order to train a logistic regression model. The details of the implementation for both training and classification are detailed in Section 6. For each category we train a separate model in a one-vs-all\nmanner, meaning that the senders assigned any different label are considered negative examples of equal weights. The classification process is run performed periodically to account for new senders, and modified behavior patterns (the features of the senders are refreshed at each run) of existing senders. We output for each sender a category and confidence score; if the confidence is sufficiently high then the sender enters the table. The process consists of the following: All 6 classifiers (one for each category) are run on all of the senders in our data base. To determine the category of a sender we choose, from the classifiers returning a positive answer, the one with the highest confidence, and set this score as the final confidence. If all of the classifiers returned a negative answer, the chosen category is \u2018other\u2019, meaning machine generated that does not fit our predefined categories; the category confidence is set as the lowest confidence score of the individual classifiers.\nOffline training the message-level classifier: This task is not done periodically but is done once based on the labeled data described in Section 4. The training process is quite similar to the sender classification in the sense that a logistic regression model is trained for each category in a one-vs-all model. The major difference is the training set, which is of course different as it contains messages rather than senders. To obtain it, from each sender in Dv2 we choose 5 random email messages, and assign them the label of their sender. The features associated with the messages do not include any data on the sender. The other difference when compared to the sender-based classifier is that here we do not allow a non-decision, as this component is used in the final level of the online cascade classifier. Hence, there is no issue of having sufficiently large confidence. Due to space constraints, we do not elaborate on this process further but note that it is quite similar to the sender classification process and has relatively low impact (only 8% of the traffic is run through this process)."}, {"heading": "6. EXPERIMENTS AND EVALUATION", "text": "For the production system we trained 5 sender-based classifiers for machine latent categories: Shopping, Financial, Travel, Career and Social, and 1 sender-based machine vs. human classifier. This section covers the main experimental results of our email categorization study, demonstrating how well the category classifiers generalize on a holdout dataset.\nWe start by discussing the data distribution required for testing. The most intuitive sampling is uniform sampling, in which each incoming message is assigned the same weight and the system is evaluated accordingly. In the machinegenerated latent categories, our experiments did not consider this distribution as it would rate our system way too favorably: even an average system becomes indistinguishable from an excellent one due to the large volume of messages sent by top senders, as demonstrated in Figure 5.\nSpecifically, a small number of senders are responsible for the majority of the email traffic, hence a system that avoids classifying the low-weight senders might achieve a good score, but still incur a bad user experience. One example for this phenomenon is in the social category. Here, Facebook contains only a handful of (canonized) senders5\n5We define a canonized sender as a regular expression over senders, such that all senders matching it are essentially the same. In the context of Facebook, an example would be update+.*@facebookmail.com\nyet is in charge of a huge portion of the email traffic in the social category; as a matter of fact, setting a classifier for social that only outputs \u2018True\u2019 for a message from Facebook would reach reasonable results when measured w.r.t the uniform distribution over email messages; clearly such a system would be horrible in terms of user\u2019s experience. The distribution we use for testing is thus the uniform distribution over email senders, where canonized email senders e.g. .*noreply@xyz.com are viewed as a single sender. Due to skewed data and privacy issues, we did not rely on a manually labeled data set but rather on the data set Dv2 (ignoring the senders marked as human) described in Section 4. Although this set does not contain an actual uniform sample of senders, it covers senders whose categories are interesting to users, as their associated messages were manually assigned to folders. We mention that the machine sub-categories in the labeled set where chosen solely based on the folder data. For this reason, we excluded the folder features from the dataset. This was required to avoid over-fitting.\nFor the human-vs-machine classifier the training was performed on the set Dv2; however we used a different test set. The main reason is that unlike machine-labeled senders, human-labeled senders are not based on folder data but rather on features that we do use in our classification process. For this reason, testing on Dv2 would provide unfair results. Instead, we sampled more than 600 senders uniformly from the pool of senders, then from each sender we sampled 1 email message thus creating a test set of more than 600l messages. The samples were made from the email repository after excluding easy-to-verify machine traffic such as senders whose outgoing traffic is larger than 100, 000 messages per month, or senders that were never replied to despite the fact that they sent over 1, 000 messages. This initial exclusion was necessary as human traffic is rather small compared to machine meaning that in order to get sufficiently many human examples without exclusion we would have required a large amount of manual labor. We verified that a negligible amount of these excluded messages were written by a human by manually observing that out of 200 uniformly drawn excluded examples none were human generated.\nFor the machine sub-category we randomly split Dv2 into a training set, used to train the models, and a test set, used to evaluate the models. Since a single domain may have multiple senders, they were all put in either the training or the test set. This way, we can test whether the model truly generalizes well outside of the domains it observed in training. The labeled dataset was split into 65% for training and 35% for testing purposes. The procedure was repeated 5 times. The average numbers for the 5 splits were reported.\nIn our experiments, as well as in the production system, we leveraged the MapReduce paradigm [8], implemented in\nthe Hadoop6 open source platform. All of our data processing, including aggregation of messages per sender, feature extraction, training and scoring, were done under Hadoop. The models were trained in a single MapReduce job, where the mapper reads Dv2, and prints every data point 6 times, once for each problem at hand, playing a role of either a positive or a negative example in the corresponding problems. The reducer trains a separate binary classification model:\nfj(xi)\u2192 yi\nfor each problem j \u2208 {Shopping, Financial, Travel, Career, Social, Human}. We used a logistic regression model, where the feature vector x is parameterized using a weight vector w, with one weight per modeled feature. Specifically, we utilized the highly scalable Vowpal Wabbit 7 implementation of logistic regression [14], which was found to work well in conjunction with MapReduce. In these experiments, the categorization was treated as a multi-class \u201cone-vs.-all\u201d problem. Therefore, an additional step of merging the predictions is required, where the final prediction is made based on the class with the highest probability.\nIn Figure 2, we show the averaged results achieved on the test set when classifiers were trained using all feature types except folders. The figure shows the Receiver Operating Characteristic (ROC) curve that trades-off True Positive Rate (TPR) and False Positive Rate (FPR) as well as the Precision Recall (PR) curve that trades-off Precision and Recall. It can be observed that the models achieved desirable performance, with Precision and Recall numbers both in the 0.9 range in most cases. Overall, the performance suggests that even with a very low FPR threshold, e.g. 0.5%,\n6http://hadoop.apache.org/ 7https://github.com/JohnLangford/vowpal_wabbit\nset for production purposes, large portions of true positives can be covered, i.e. , more than 85% in most cases.\nTo evaluate the influence of the feature types, table 8 shows the Area Under the ROC curve (AUC) results for each class, when our model was trained on subsets of features: (1) content features (email body, subject, etc.), (2) address features (subname, subdomain, commercial keywords) and (3) behavioral features (number of messages, sent, received, etc.) without folders. It should be noted that temporal behavior features (burst features) were treated as behavioral features in this experiment. Even though there is an evident drop when compared to using all feature types, content and address feature subsets achieved competent AUC performance that around 0.9. As expected, behavioral features showed good performance in distinguishing between human and machine senders, but could not on their own discriminate between different machine categories.\nThe second portion of Table 8 shows the performance when some features were removed. Specifically, we were interested in the performance drop when burst features, and body words features were removed.\nFigure 3 gives some insights on the human vs. machine classification. Figure 3a shows the histogram of the read ratio Rrr for human and machine senders, where Rrr is defined as Rrr =\n#read messages #sent messages\n. A clear difference in the distribution can be observed. Furthermore, Figures 3b and 3c show to what extent certain binary features are observed in the human examples versus the machine examples. It can be observed that when the word unsubscribe is present in the message body, it is much more likely to be associated with a machine sender. Also, when common first names, such as \u201cmichael\u201d, \u201csusan\u201d, etc., are present in the email address, they are much more likely to be associated with a human sender. Finally, Fig-\nure 3d shows the presence of \u201cburst 100\u201d feature, i.e. indicator of whether or not the sender sent more than 100 messages per hour on some occasion. It can be observed that the feature is present in more than half machine senders and in very few human senders. Other interesting facts that were not depicted in the figures are that: 1) 34.56% of machine senders send messages with average subject character count higher than 30, while only 5.88% of human senders do the same; 2) 77.55% of machine senders send messages with average body character count higher than 300, while only 1% of human senders do the same; and 3) 41.09% of machine senders send messages with more than 3 urls in the body on average, while only 2.24% of human senders do the same.\nTo get more insight into the latent category models, we generated feature clouds for j \u2208 { Shopping, Financial, Travel, Career} as illustrated in Figure 4. Features of higher \u201cimportance\u201d are printed using larger fonts. In Logistic Regression, large positive feature weights w are associated with a higher likelihood of user belonging to class j. Therefore, to calculate the feature importance scores with respect to class j we use the following procedure. First, we isolate only the features that have a positive weight in j-th class model wj . Next, we calculate the score for each of those features as score(k) = Nk( N+ k /Nk\nN+/N \u2212 1), where N is the total\nnumber of examples, N+ is the total number of class j examples, Nk is the number of examples that have feature k and N+k is the number of class j examples that have feature k. Finally, we generate a word cloud using the 1, 000 features with the highest score. By examining the figures we can conclude that the main concepts of each class are very well captured using indicative body and subject words, as well as the address substrings."}, {"heading": "7. SIGNIFICANCE OF RESULTS", "text": "In this section, we discuss the significance and potential impact of our mail categorization system. We estimated the coverage of latent categories in two different manners. First, we measured overall email traffic coverage by estimating the percentage of email messages that are mapped into one of our categories. Then, we measured how these categories cover mail search by manually categorizing a sample of mail search queries. The results are given in Table 9.\nFor email traffic coverage, we ran our classification system over real email traffic and counted the number of messages classified in each category. The percentages listed in the email traffic column in Table 9 represent the number of messages classified as topic X divided by the total number of messages. For the search query coverage, a team of professional editors manually labeled 2,500 common queries. To ensure privacy, we chose only queries that were used by several users, and in particular avoided queries that are unique to a single user. The percentages listed in the search query column in Table 9 represent the number of queries classified as topic X divided by the total number of queries. Table 10 gives a description of the typical queries for each category.\nTable 9 clearly illustrates the fact that coverage by email traffic and coverage by search query are very different in nature. One notable difference is the human category that\nattracts (somewhat unsurprisingly, given our previous comments on the dominance of machine-generated traffic) a disproportionate amount of searches as compared to its relatively small email volume. The finance category is similar although the ratio between the coverages is not as large as in the human category. An opposite example is that of social networks. Despite their huge coverage of the email traffic, the coverage in terms of search queries is much lower. Indeed when considering the type of messages sent by social sites, these are mostly updates and summaries of the recent events, hence are typically not messages that will be read more than once, thus will not be searched for. Overall, the coverage of the latent categories both in terms of email traffic and search queries is larger than 70%. These figures confirm the fact that latent categories could be used not only for browsing but probably even more for searching, and thus answer the needs of the two traditional discovery paradigms [6] since the early days of the Web."}, {"heading": "8. CONCLUSIONS AND FUTURE WORK", "text": "We presented here a Web-scale categorization approach that combines offline learning and online classification components. One of our key contributions is the identification of categories common to all users. We discovered latent categories by conducting a large-scale analysis of user folder data, which highligted the distinction between human and machine-generated email. Our categories cover more than 70% of both email traffic and email search queries. Our classification mechanism achieved precision and recall rates close to 90%. These results are achieved via an extremely scalable online system that assigns a category to incoming messages delivered to any user, including those who never defined a folder. We believe that this study shows a great deal of promises for this domain, as it demonstrates that email classification can be applied in production systems of the scale of Web mail.\nDiscussing how categories should be exposed to users, if at all, to users is out of the scope of this work but is clearly a critical challenge. Following the traditional Web discovery paradigms, they could be surfaced by explicit categories and search facets (for browsing) or behind the scenes, as an additional search signal. We hope this work will encourage other researchers to build upon these categories in order to explore new email discovery paradigms. On the backend side, which remains our focus, we plan to investigate whether sub-types within latent categories could be discovered, e.g., Travel promotions under Travel. We also intend to explore the \u201csender cold-start\u201d issue, which occurs when a new sender appears, and its associated messages are too rare for us to conduct appropriate learning. As new senders keep appearing, we will need to devise appropriate methods to handle new senders in order to maintain quality."}, {"heading": "Acknowledgements", "text": "We are grateful to Andrei Broder for inspiring this work. Yehuda Koren and Roman Sandler spent a huge number of hours manipulating thousands of folders. We owe a great deal to Edo Liberty, who discovered the power of machinegenerated email, and Nemanja Djuric who conducted LDA experiments during his internship at Yahoo. Finally, this work would not be possible without the constant support of the Mail engineering and product teams at Yahoo."}, {"heading": "9. REFERENCES", "text": "[1] Nir Ailon, Zohar S. Karnin, Edo Liberty, and Yoelle\nMaarek. Threading machine generated email. In Proceedings of WSDM\u20192013, pages 405\u2013414, New York, NY, USA, 2013. ACM.\n[2] Inge Alberts and Dominic Forest. Email pragmatics and automatic classification: A study in the organizational context. JASIST, 63(5):904\u2013922, 2012.\n[3] Olle Ba\u0308lter. Keystroke level analysis of email message organization. In Proceedings of CHI\u20192000, pages 105\u2013112. ACM, 2000.\n[4] David M Blei, Andrew Y Ng, and Michael I Jordan. Latent dirichlet allocation. JMLR, 3:993\u20131022, 2003.\n[5] Avrim Blum and Tom Mitchell. Combining labeled and unlabeled data with co-training. In Proceedings of COLT\u20191998, pages 92\u2013100. ACM, 1998.\n[6] C. M. Bowman, P. B. Danzig, U. Manber, and M. F. Schwartz. Scalable internet: Resource discovery. Communications of the ACM, 37(8), August 1994.\n[7] Jake D Brutlag and Christopher Meek. Challenges of the email domain for text classification. In Proceedings of ICML\u20192000, pages 103\u2013110, 2000.\n[8] Jeffrey Dean and Sanjay Ghemawat. Mapreduce: simplified data processing on large clusters. Communications of the ACM, 51(1):107\u2013113, 2008.\n[9] Matthew Hoffman, Francis R Bach, and David M Blei. Online learning for Latent Dirichlet Allocation. In Advances in Neural Information Processing Systems, pages 856\u2013864, 2010.\n[10] Svetlana Kiritchenko and Stan Matwin. Email classification with co-training. In Proceedings of the 2011 Conference of the Center for Advanced Studies on Collaborative Research, pages 301\u2013312, 2011.\n[11] J. Klensin. Simple mail transfer protocol, rfc 2821, April 2001.\n[12] Bryan Klimt and Yiming Yang. The enron corpus: A new dataset for email classification research. In Proceedings of ECML\u20192004, pages 217\u2013226. 2004.\n[13] Yehuda Koren, Edo Liberty, Yoelle Maarek, and Roman Sandler. Automatically tagging email by leveraging other users\u2019 folders. In Proceedings of KDD\u20192011, pages 913\u2013921. ACM, 2011.\n[14] John Langford, Lihong Li, and Tong Zhang. Sparse online learning via truncated gradient. JMLR, 10:777\u2013801, 2009.\n[15] Jefferson Provost. Na\u0131ve-bayes vs. rule-learning in classification of email. University of Texas at Austin, 1999.\n[16] Fabrizio Sebastiani. Machine learning in automated text categorization. ACM Computing Surveys, 34(1), March 2002."}], "references": [{"title": "Threading machine generated email", "author": ["Nir Ailon", "Zohar S. Karnin", "Edo Liberty", "Yoelle Maarek"], "venue": "In Proceedings of WSDM\u20192013,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2013}, {"title": "Email pragmatics and automatic classification: A study", "author": ["Inge Alberts", "Dominic Forest"], "venue": "in the organizational context. JASIST,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2012}, {"title": "Keystroke level analysis of email message organization", "author": ["Olle B\u00e4lter"], "venue": "In Proceedings of CHI\u20192000,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2000}, {"title": "Combining labeled and unlabeled data with co-training", "author": ["Avrim Blum", "Tom Mitchell"], "venue": "In Proceedings of COLT\u20191998,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 1998}, {"title": "Scalable internet: Resource discovery", "author": ["C.M. Bowman", "P.B. Danzig", "U. Manber", "M.F. Schwartz"], "venue": "Communications of the ACM,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 1994}, {"title": "Challenges of the email domain for text classification", "author": ["Jake D Brutlag", "Christopher Meek"], "venue": "In Proceedings of ICML\u20192000,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2000}, {"title": "Mapreduce: simplified data processing on large clusters", "author": ["Jeffrey Dean", "Sanjay Ghemawat"], "venue": "Communications of the ACM,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2008}, {"title": "Online learning for Latent Dirichlet Allocation", "author": ["Matthew Hoffman", "Francis R Bach", "David M Blei"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2010}, {"title": "Email classification with co-training", "author": ["Svetlana Kiritchenko", "Stan Matwin"], "venue": "In Proceedings of the 2011 Conference of the Center for Advanced Studies on Collaborative Research,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2011}, {"title": "The enron corpus: A new dataset for email classification research", "author": ["Bryan Klimt", "Yiming Yang"], "venue": "In Proceedings of ECML\u20192004,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2004}, {"title": "Automatically tagging email by leveraging other users\u2019 folders", "author": ["Yehuda Koren", "Edo Liberty", "Yoelle Maarek", "Roman Sandler"], "venue": "In Proceedings of KDD\u20192011,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2011}, {"title": "Sparse online learning via truncated gradient", "author": ["John Langford", "Lihong Li", "Tong Zhang"], "venue": "JMLR, 10:777\u2013801,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2009}, {"title": "Na\u0131ve-bayes vs. rule-learning in classification of email", "author": ["Jefferson Provost"], "venue": "University of Texas at Austin,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 1999}, {"title": "Machine learning in automated text categorization", "author": ["Fabrizio Sebastiani"], "venue": "ACM Computing Surveys,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2002}], "referenceMentions": [{"referenceID": 9, "context": "In the last decade, most attempts at automating this task have consisted in mimicking each individual user\u2019s personal classification habits [12].", "startOffset": 140, "endOffset": 144}, {"referenceID": 10, "context": "It was shown in [13] that 70% of users do not create even a single folder.", "startOffset": 16, "endOffset": 20}, {"referenceID": 10, "context": "in [13], have analyzed popular folders, in order to identify not just a few but several thousand possible common classes or \u201ctags\u201d.", "startOffset": 3, "endOffset": 7}, {"referenceID": 10, "context": "This work explores a middle ground direction, where we propose, like in [13], to derive high level-classes from mail data, and especially folders data provided by the minority of users who do define folders.", "startOffset": 72, "endOffset": 76}, {"referenceID": 0, "context": "stated that machine-generated mail represent more than 60% of Yahoo mail traffic, [1].", "startOffset": 82, "endOffset": 85}, {"referenceID": 13, "context": "DISCOVERING LATENT CATEGORIES Following [16], we define email categorization as the task of \u201cassigning a Boolean value to each pair (dj , ci) \u2208 D \u00d7 C, where D is a domain of documents\u201d (here mail messages) \u201cand C is a set of predefined categories.", "startOffset": 40, "endOffset": 44}, {"referenceID": 12, "context": "\u201d Previous research work on personalized email classification used one C set per user and populated it by reusing the user\u2019s existing folders, see [15, 7, 2, 10].", "startOffset": 147, "endOffset": 161}, {"referenceID": 5, "context": "\u201d Previous research work on personalized email classification used one C set per user and populated it by reusing the user\u2019s existing folders, see [15, 7, 2, 10].", "startOffset": 147, "endOffset": 161}, {"referenceID": 1, "context": "\u201d Previous research work on personalized email classification used one C set per user and populated it by reusing the user\u2019s existing folders, see [15, 7, 2, 10].", "startOffset": 147, "endOffset": 161}, {"referenceID": 8, "context": "\u201d Previous research work on personalized email classification used one C set per user and populated it by reusing the user\u2019s existing folders, see [15, 7, 2, 10].", "startOffset": 147, "endOffset": 161}, {"referenceID": 2, "context": "Indeed, it has been shown that, with more than 20 folders, folder-based discovery becomes less effective than search [3].", "startOffset": 117, "endOffset": 120}, {"referenceID": 7, "context": "To this effect, we trained an online LDA model [9].", "startOffset": 47, "endOffset": 50}, {"referenceID": 3, "context": "We also applied a semi-supervised approach similar to co-training [5], in order to further increase our heuristic dataset, remove false positives, and most importantly obtain a less skewed sample.", "startOffset": 66, "endOffset": 69}, {"referenceID": 6, "context": "In our experiments, as well as in the production system, we leveraged the MapReduce paradigm [8], implemented in", "startOffset": 93, "endOffset": 96}, {"referenceID": 11, "context": "Specifically, we utilized the highly scalable Vowpal Wabbit 7 implementation of logistic regression [14], which was found to work well in conjunction with MapReduce.", "startOffset": 100, "endOffset": 104}], "year": 2016, "abstractText": "Email classification is still a mostly manual task. Consequently, most Web mail users never define a single folder. Recently however, automatic classification offering the same categories to all users has started to appear in some Web mail clients, such as AOL or Gmail. We adopt this approach, rather than previous (unsuccessful) personalized approaches because of the change in the nature of consumer email traffic, which is now dominated by (non-spam) machine-generated email. We propose here a novel approach for (1) automatically distinguishing between personal and machine-generated email and (2) classifying messages into latent categories, without requiring users to have defined any folder. We report how we have discovered that a set of 6 \u201clatent\u201d categories (one for humanand the others for machine-generated messages) can explain a significant portion of email traffic. We describe in details the steps involved in building a Web-scale email categorization system, from the collection of ground-truth labels, the selection of features to the training of models. Experimental evaluation was performed on more than 500 billion messages received during a period of six months by users of Yahoo mail service, who elected to be part of such research studies. Our system achieved precision and recall rates close to 90% and the latent categories we discovered were shown to cover 70% of both email traffic and email search queries. We believe that these results pave the way for a change of approach in the Web mail industry, and could support the invention of new large-scale email discovery paradigms that had not been possible before.", "creator": "LaTeX with hyperref package"}}}