{"id": "1606.09370", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "30-Jun-2016", "title": "Relation extraction from clinical texts using domain invariant convolutional neural network", "abstract": "In recent years extracting relevant information from biomedical and clinical texts such as research articles, discharge summaries, or electronic health records have been a subject of many research efforts and shared challenges. Relation extraction is the process of detecting and classifying the semantic relation among entities in a given piece of texts. Existing models for this task in biomedical domain use either manually engineered features or kernel methods to create feature vector. These features are then fed to classifier for the prediction of the correct class. It turns out that the results of these methods are highly dependent on quality of user designed features and also suffer from curse of dimensionality. In this work we focus on extracting relations from clinical discharge summaries. Our main objective is to exploit the power of convolution neural network (CNN) to learn features automatically and thus reduce the dependency on manual feature engineering. We evaluate performance of the proposed model on i2b2-2010 clinical relation extraction challenge dataset. Our results indicate that convolution neural network can be a good model for relation exaction in clinical text without being dependent on expert's knowledge on defining quality features.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n", "histories": [["v1", "Thu, 30 Jun 2016 07:10:07 GMT  (148kb,D)", "http://arxiv.org/abs/1606.09370v1", "This paper has been accepted in ACL BioNLP 2016 Workshop"]], "COMMENTS": "This paper has been accepted in ACL BioNLP 2016 Workshop", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["sunil kumar sahu", "ashish anand", "krishnadev oruganty", "mahanandeeshwar gattu"], "accepted": false, "id": "1606.09370"}, "pdf": {"name": "1606.09370.pdf", "metadata": {"source": "CRF", "title": "Relation extraction from clinical texts using domain invariant convolutional neural network", "authors": ["Sunil Kumar Sahu", "Ashish Anand", "Krishnadev Oruganty", "Mahanandeeshwar Gattu"], "emails": [], "sections": [{"heading": "1 Introduction", "text": "The increasing amount of biomedical and clinical texts such as research articles, clinical trials, discharge summaries, and other texts created\n\u2217Part of this work was done while Sunil Kumar Sahu was doing internship at Excelra Knowledge Solutions Pvt Ltd, Hyderabad, Telangana, India.\nby social network users, represents immeasurable source of information. Automatic extraction of relevant information from these resources can be useful for many applications such as drug repositioning, medical knowledge base creation etc. The performance of concept entity recognition systems for detecting mention of proteins, genes, drugs, diseases, tests and treatments has achieved sufficient level of accuracy, which gives us opportunity for using these data to do next level tasks of natural language processing (NLP). Relation extraction is the process of identifying how given entities are related in considered sentence or text. As given in the example sentence [S1] below, the entities Lexix and congestive heart failure are related by treatment administered medical problem relation. These relations are important for other upper level NLP tasks and also in biomedical and clinical research (Shang et al., 2011).\n[S1]: He was given Lexix to prevent him from congestive heart failure .\nRelation extraction task in unstructured text has been modeled in many different ways. cooccurrence based methods due to their simplicity and flexibility are most widely used methods in biomedical and clinical domain. In co-occurrence analysis it is assumed that if two entities are coming together in many sentences, their must be a relation between them (Bunescu et al., 2006; Song et al., 2011). Quite obviously this method can not differentiate types of relations and suffers from low precision and recall. To improve its results, different statistical measures such as point wise mutual information, chi-square or log-likelihood ratio has been used in this approach (Stapley and Benoit, 2000).\nRule based methods are another commonly adapted methods for relation extraction task (Thomas et al., 2000; Park et al., 2001; Leroy et al., 2003). Rules are created by carefully observing the syntactic and semantic patterns in rela-\nar X\niv :1\n60 6.\n09 37\n0v 1\n[ cs\n.C L\n] 3\n0 Ju\nn 20\n16\ntion instances. Bootstrapping method (Xu, 2008) is used to improve the performance of rule based methods. Bootstrapping uses small number of known relation pair of each relation type as a seed and use these seeds to search patterns in huge unannotated text (Xu, 2008) in iterative fashion. Bootstrapping method generates lots of irrelevant patterns too, which can be controlled by distantly supervised approach. Distantly supervised method uses large knowledge base such as UMLS or Freebase as an input and extract patterns from huge corpus for all pair of relations present in knowledge base (Mintz et al., 2009; Riedel et al., 2010; Roller and Stevenson, 2014). The advantage of bootstrapping and distantly supervised methods over supervised methods is that they do not require lots of manually labeled training data which is generally very hard to get.\nFeature based methods use sentences with predefined entities to construct feature vector through feature extraction (Hong, 2005; Minard et al., 2011b; Rink et al., 2011). Feature extraction is mainly based on linguistic and domain knowledge. Extracted feature vectors are used to decide correct class of relation present between entities in the sentence through any classification techniques. Kernel methods are extension of feature based methods which utilize kernel functions to exploit rich syntactic information such as parse trees (Zelenko et al., 2003; Culotta and Sorensen, 2004; Qian and Zhou, 2012; Zeng et al., 2014). State of the art results have been obtained by these class of methods.\nHowever, the performance of feature and kernel based methods are highly dependent on suitable feature set selection, which is not only tedious and time consuming task but also require domain knowledge and is dependent on other NLP systems. Often such dependencies make many existing work less reproducible simply because of absence of the full and finer details of feature extraction. Further often these methods lead to huge number of features and may get affected from curse of dimensionality issues (Bengio et al., 2003; Collobert et al., 2011). Another issue faced by these methods is feature extraction will have to be adjusted according to the data source. As discussed earlier we are having multiple but diverse information resources such as research articles, discharge summaries, clinical trials outcome etc. While in one hand multiple sources bring\nmore information but the other hand it makes it challenging to extract meaningful information automatically simply because of diverse nature of the data source. For example, sentences in research articles are well formed and likely to use only well accepted technical terms. But sentences in clinical discharge summaries may not be well formed sentences instead it could be fragmented sentences with lots of acronyms or terms used only locally. Similarly social media articles may use slang or terms which are not technically used. This makes it difficult for above discussed methods.\nMotivated by these issues, this work aims to exploit recent advances in machine learning and NLP domains to reduce such dependencies and utilize convolutional neural network to learn important features with minimal manual dependencies. Convolution neural network has shown to be a powerful model for image processing, computer vision (Krizhevsky et al., 2012; Karpathy and Fei-Fei, 2014) and subsequently in natural language processing it has given state of the art results in different tasks such as sentence classification (Kim, 2014; Kalchbrenner et al., 2014; Hu et al., 2014; Sharma et al., 2016), relation classification (Zeng et al., 2014; dos Santos et al., 2015) and semantic role labeling (Collobert et al., 2011).\nIn this paper we propose a new framework for extracting relations among problem, treatment and test in clinical discharge summaries. In particular we use data available under clinical relation extraction task organized by Informatics for Integrating Biology and the Bedside (i2b2) in 2010 as part of i2b2/VA challenge (Uzuner et al., 2011). Extracting relations in clinical texts is more challenging compared to research articles as it contains incomplete or fragmented sentences, and lots of acronyms. Current state of the art methods heavily depend on manual feature engineering and use hundreds of thousands of features (Minard et al., 2011b; Rink et al., 2011). Our result indicates the proposed model can outperform the current state of the art models by using only a small fraction of features. However the main observation is the features used in our model is easy to replicate and adapt as per the data source compared to the feature sets generally used in these tasks."}, {"heading": "2 Related Research", "text": "i2b2 organized a shared task in 2010 (Uzuner et al., 2011). In this challenge discharge sum-\nmaries from three different sources were annotated for extracting relations among clinical entities such as problem, treatment and test. Most of the participants in this challenge used support vector machine (SVM) with manually designed features (Uzuner et al., 2010). Model proposed by Rink et al. (2011) had first place in this task, which used six classes of features namely, context features, similarity features, nested related relation features, Wikipedia features, single concept features and vicinity features. They formulated the relation extraction task as a multiclass classification problem and SVM with linear kernel were used for classification.\nFor extracting relation among disease and treatment, Rosario and Hearst (2004) used various graphical and neural network models. They used variety of lexical, semantic and syntactic features for classification and found that semantic features were contributing most among all. The dataset used in this study was relatively smaller and was prepared from biomedical research articles. Li et al. (2008) proposed kernel methods for relation extraction between entities in MEDLINE R\u00a9 articles. They modified the tree kernel function by incorporating trace kernel to capture richer contextual features for classifying the relation. Their results shows that tree kernel outperform other kernel methods such as word and sequence kernels for the considered task.\nConditional random field (CRF) has been used for relation extraction between disease treatment and gene by (Bundschus et al., 2008). In this experiment setting, they did not assume that entities were given, instead their model also predicted en-\ntities and its type. They developed two variants of CRF both modeled relation extraction task as sequence labeling task. Recently Bravo et al. (2015) proposed a system for identifying association between drug disease and target in EU-ADR dataset (van Mulligen et al., 2012) and named it BeeFree. BeeFree usese combination of shallow linguistic kernel and dependency kernel for identifying relations.\nIn contrast to above methods recently there are few work applying convolution neural network based models (Zeng et al., 2014; dos Santos et al., 2015) for relation classification in SemEval 2010 relation classification dataset (Hendrickx et al., 2009). Convolution neural network used in this models are using constant length filters, and word embedding and distance embedding as features. Our model leverage on the linguistic features also and we considered relation extraction task in clinical notes which is much more informal, rich with acronyms and number of samples for each relations are not stable (Uzuner et al., 2011)."}, {"heading": "3 CNN for Clinical Relation Extraction", "text": "The proposed model based on CNN is first summarized in the next section. Subsequent sections describe it in more detail."}, {"heading": "3.1 Model Architecture", "text": "The proposed model architecture is shown in the figure 1, which takes a complete sentence with mentioned entities as an input and outputs a probability vector corresponding to all possible relation types. Each feature is having vector representation\nwhich is initialized randomly except word embedding feature. For word embedding, we used pretrained word vector (TH et al., 2015) learned on Pubmed articles using word2vec tool (Mikolov et al., 2013b).\nEmbedding layer maps every feature value with its corresponding feature vectors and concatenate them. In order to get local features from each part of the sentence we have used multiple filters of different lengths (Kim, 2014) in all possible continuous n-gram of the sentence, where n is the length of filter (We have shown four filters with constant length three in the figure 1). We use max pooling over time to get global features through all filters. Here time indicates filter running over the length of the sentence. Pooled features are then fed to fully connected feed-forward neural network to make inference. In the output layer we use softmax classifier with number of outputs equal to number of possible relations between entities."}, {"heading": "3.2 Feature Layer", "text": "We represent each word in the sentence with 6 discrete features namely word itself (W), distance from the first entity (P1), distance from the second entity (P2), parts of speech tag of the word (PoS), chunk tag of the word (Chunk) and entity type (T). Each feature is briefly described below:\n1. W : Exact word appeared in the sentence.\n2. P1: Distance from the first entity in terms of number of words (Collobert and Weston, 2008). For instance in our earlier example [S1] He is at \u22123 distance and prevent is at +2 distance away from the first entity Lexis. This value would be zero for all words which is a part of the first entity.\n3. P2: Similar to P1 but considers distance from the second entity.\n4. PoS: Parts of speech tag of the considered word. We use genia tagger1 to obtain pos tag of each word.\n5. Chunk: Chunk tag of considered word. Again we use genia tagger to obtain chunk tag of each word.\n6. T : Type of the considered word. For example, it would be entity type such as B\u2212Prob,\n1http://www.nactem.ac.uk/GENIA/tagger/\nI \u2212 Prob etc. for entity word and Other for rest words following the BIO tagging convention.\nThis way a word w \u2208 D1 \u00d7 D2 \u00d7 .....D6, where Di is the dictionary for ith local features."}, {"heading": "3.3 Embedding Layer", "text": "In lookup or embedding layer each feature value is mapped to its vector representation using feature embedding matrix. Lets say M i \u2208 Rn\u00d7N is the feature embedding matrix for ith local feature (here n represents dimension of feature embedding and N is number of possible values or size of the dictionary for ith local feature). Each column of M i is vector of corresponding value of ith features. Mapping can be done by taking product of one hot vector of feature value with its embedding matrix (Collobert and Weston, 2008). Suppose a(i)j is the one hot vector for j\nth feature value of ith feature then:\nf (i) j = M i a (i) j (1)\nxi = f (i) 1 \u2295 f (i) 2 ....\u2295 f (i) 6 (2)\nHere \u2295 is concatenation operation so xi \u2208 R(n1+....n6) is feature vector for ith word in sentence and nk is dimension of kth feature. For word embedding we used pre-trained word vector obtained after running word2vec tool (Mikolov et al., 2013b; Mikolov et al., 2013a) on huge Pubmed open source articles (TH et al., 2015). Other feature matrix were initialized randomly at the beginning. Since number of elements in all feature dictionary except word dictionary (D1) are not huge, we assume that while training these vectors will get sufficient updation."}, {"heading": "3.4 Convolution Layer", "text": "We apply convolution on text to get local features from each part of the sentence (Collobert and Weston, 2008). Consider x1x2.....xm is the sequence of feature vectors of a sentence, where xi \u2208 Rd is a vector obtained by concatenating all feature vector of ith word. Let xi:i+j represents concatenation of xi.....xi+j feature vectors. Suppose there is a filter parameterized by weight vector w \u2208 Rcd where c is the length of filter (in figure 1 filter length is three). Then output sequence of convolution layer would be\nhi = f(w \u00b7 xi:i+c\u22121 + b) (3)\nWhere i = 1, 2, . . .m \u2212 c + 1, . is dot product, f is rectify linear unit (ReLu) function and b \u2208 R is biased term. w and b are the learning parameters and will remain same for all i = 1, 2, . . .m\u2212c+1."}, {"heading": "3.5 Max Pooling Layer", "text": "Output of convolution layer length (m\u2212c+1) will vary based on number of words m in the sentence. We applied max pooling (Collobert and Weston, 2008) over time to get fixed length global features for whole sentence. The intuition behind using max pooling is to consider only most useful feature from entire sentence.\nz = max 1\u2264i\u2264(m\u2212c+1)\n[hi] (4)\nWe have just explained the process of extracting one feature from a whole sentence using one filter. In figure 1 we extracted four features using four filters of the same length three. In our experiment we use multiple such filters of variable length (Kim, 2014; Yin and Schtze, 2015). The objective of using different length filter is to accommodate context in varying window size around words."}, {"heading": "3.6 Fully Connected Layer", "text": "The output of max pooling layer is sequence z came with different filters. We call this global feature because it came by taking max over entire sentence. To make classifier over extracted global feature, we used fully connected feed forward layer. Suppose zi \u2208 Rl is output of max pooling layer for entire filters then output of fully-connected layer would be\no(i) = W ozi + bo (5)\nHere W o \u2208 R[r]\u00d7l and bo \u2208 R[r] are parameters of neural network and [r] denotes number of classes."}, {"heading": "3.7 Softmax Layer", "text": "In output layer we used softmax classifier for which objective function would be minimization of\nLi = \u2212 log\n( eo\n(i) yi\u2211\n\u2200j e o(i) j\n) (6)\nfor ith sentence. Here yi is correct class of relation for ith instance."}, {"heading": "3.8 Implementation", "text": "We experiment with filter lengths in two different experiment settings. In first, we use 100 different filters of a fixed length in the convolutional layer,\nwhile in another set of experiments we use varying length filters, but used 100 different filters for each varying length. So, in the first setting, we obtain 100 features after max pooling, while in the second, we obtain 100 times number of different length filter features. For regularization (Srivastava et al., 2014), we follow (Kim, 2014) and use dropout technique in output of max pooling layer. Dropout prevents co-adaptation of hidden units by randomly dropping few nodes. We set this value to 0.5 during training and 1 while testing. We use Adam technique (Kingma and Ba, 2014) to optimize our loss function. Entire neural network parameters and feature vectors are updated while training. We have implemented the proposed model in Python language using tensorflow package (Abadi et al., 2015) and will make it available on request. Results of each filter length were explained in results section. Dimension of word vector is set to 50 and rest all feature embedding size is kept to 5."}, {"heading": "4 Dataset and Experimental Settings", "text": "In recent years several challenges have been organized to automatically extract information from clinical texts (Uzuner et al., 2007; Uzuner et al., 2008; Uzuner et al., 2011; Uzuner et al., 2010; Sun et al., 2013). i2b2 has released dataset for clinical concept extraction, assertion classification and relation extraction as a part of i2b2-2010 shared task challenge. This dataset was collected from three different hospitals and was manually annotated by medical practitioners for identifying problems, treatments and test entities, and eight relation types among them. These relations were: treatment caused medical problems (TrCP), treatment administered medical problem (TrAP), treatment worsen medical problem (TrWP), treatment improve or cure medical problem (TrIP), treatment was not administered because of medical problem (TrNAP), test reveal medical problem (TeRP), Test conducted to investigate medical problem (TeCP), Medical problem indicates medical problems (PIP). (Uzuner et al., 2011) has given the exact definition of each relation type.\nWhile during the challenge original dataset had 394 documents for training and 477 documents for testing but when we downloaded this dataset from i2b2 website we got only 170 documents for training and 256 documents for testing. After preliminary experiment we found that we did not have\nenough training samples for all relation classes present in the dataset, therefore we decided to remove 3 relation classes along with their instances (TrWP (132 instances), TrIP (202 instances) and TrNAP (173 instances)). Statistics of the dataset is shown in the Table 1.\nFor extracting relations among entities we considered all sentences having more than one entities in each discharge summary to check whether any relation exists between them or not. In our experiment we assume that entities and their types are already known like other existing works (Rink et al., 2011; Minard et al., 2011a; Minard et al., 2011b). We created data sample for every pair of entities present in the sentence and labeled it with the existing relation type. For example in sentence [S2] (all continuous bold phrases are entities) entity pairs (\u201cher white count\u201d, \u201celevated\u201d) label would be \u201cTeRP\u201d, for entity pair (\u201cher gcsf\u201d, \u201celevated\u201d) label would be \u201cTrNAP\u201d and for (\u201cher white count\u201d, \u201cher G-CSF\u201d) label would be \u201dNone\u201d.\n[S2]: Her white count remained elevated despite discontinuing her G-CSF ."}, {"heading": "5 Results and Discussion", "text": ""}, {"heading": "5.1 Influence of filter lengths", "text": "We combined the training and testing data and performed five-fold cross-validation on the available limited i2b2 dataset for all our evaluations. First we evaluate the influence of filter lengths. We experiment with selection of filter length using all features. Results as average of five-fold experiment are shown in the Table 2.\nIn case of single filter, the results indicate increasing the size of filter length generally tends to improve the performance. Using only single filter the best performance with F1 score as 70.43% was obtained by using filter length of 6. However further increasing the filter length did not improve the\nresult. Intuitively it also seems that selection of either of too small or too large filter length may not be a good option. Filter length gives the window size to capture context features. One can expect that too small filter length (window size) may not capture enough good context feature and too big filter length may include noise or irrelevant contexts.\nFurther, we used multiple filters to see whether it improves the result. Results indicate that combination of small and mid-length filter size is perhaps the better choice. For example, combination of filter lengths 3 and 4 together did not improve the performance compared to the single filter length of 3 or 4. On the other hand combination of filter lengths 3 and 5, and 4 and 5 improved the performance compared to use of single filters of either length. It can be seen, the best result with F1 score as 71.16% is obtained by using filter lengths of 4 and 6 together. But adding more than two filters did not lead to performance improvement."}, {"heading": "5.2 Classwise Performance", "text": "We took the best combination of filter lengths and looked at the classwise performance. Results are described in the Table 3.\nWe see from the results that as number of training examples (see Table 1) increases, performance of the model also improves. The relation class\nTeRP has the maximum number of training examples and the model obtained quite a good F1 score. On the other hand, the model could not perhaps able to learn well for the relation classes TeCP and TrCP having relatively lesser number of training examples."}, {"heading": "5.3 Contribution of Each Features", "text": "In order to investigate the contribution of each feature in final result we gradually include one feature in our model and compared the performance. Table 4 shows the obtained results. First we use only random vector (RV) representation along with entity types (T) (first row in the table) as a baseline for our comparison. Adding position features (2nd row) lead to approximately 15% increase in recall, 7% in precision and 11.7% in F1 score. However including PoS and Chunk features although improved recall and F1 score by 4.3% and 1.3% but precision was decreased by 3.6%. In the second set of experiments, we first use pre-trained word vectors along with entity types (4th row) and later repeated the similar experiments as previously. Here again, inclusion of position features improved the recall by more than 14% and F1 score by around 11%. This clearly indicates word position relative to the entities of interest plays important role in deciding their influence in the context. Further including PoS and Chunk features also led to performance improvement."}, {"heading": "5.4 Comparison with Feature Based Method", "text": "We could not compare our results directly with the state of the art results obtained on the i2b2 dataset as we did not have the complete dataset. We build a linear SVM classifier using similar features as defined in earlier studies (Rink et al., 2011) as a baseline for comparison. The following features are used for each entity pair instance:\n\u2022 Any word between relation arguments\n\u2022 Any PoS tags between relation arguments. We used genia tagger for PoS\n\u2022 Any bigram between relation arguments\n\u2022 Word preceding first and second argument\n\u2022 Any three words succeeding the first and second arguments\n\u2022 Sequence of chunk tags between relation arguments. We used genia tagger for chunk tag\n\u2022 String of words between relation arguments\n\u2022 First and second argument type (problem, treatment and test)\n\u2022 Order of argument type appeared in sentence\n\u2022 Distance between two arguments in terms of number of words\n\u2022 Presence of only punctuation sign between arguments.\nThis way we prepared attribute-value and numerical features for each instances. Table 5 shows the comparison of best results obtained by the proposed model and SVM based model. Linear SVM classifier with different cost parameter C was implemented using scikit learn (Pedregosa et al., 2011). Here again results shown are average over the 5-folds.\nBased on the results, We can make following observations:\n\u2022 Instead of SVM, other classifier could have been also used. We decided to use SVM as SVM based model with similar features obtained the best performance in the 2010 challenge.\n\u2022 In any case we still would have to define huge number of features and only few of them would have non-zero values in any given sample or instance.\n\u2022 The proposed model with limited number of features (75 * number of words in the sentence; 5 dimensional vector for 5 features other than word embedding, which is 50 dimensional vector) still gave the better performance.\n\u2022 Consistent with our observations in the section 5.1, too many features trying to capture more contexts adversely affect the performance of classifier. If we look at the features defined above it includes features which try to capture context of all possible window size between the mentioned entities."}, {"heading": "6 Conclusion", "text": "In this work we present a new framework based on CNN for extracting relations among clinical entities in clinical texts. The proposed model has shown better performance by using only a small fraction of features compared to the SVM based baseline model. Our results indicate that CNN is able to learn global features which can capture contextual features quite well and thus helps in improving the performance."}, {"heading": "Acknowledgments", "text": "We would like to thank i2b2 National Center for Biomedical Computing funded by U54LM008748, for providing the clinical records originally prepared for the Shared Tasks for Challenges in NLP for Clinical Data organized by Dr. Ozlem Uzuner, i2b2 and SUNY."}], "references": [{"title": "A neural probabilistic language model", "author": ["Bengio et al.2003] Yoshua Bengio", "R\u00e9jean Ducharme", "Pascal Vincent", "Christian Janvin"], "venue": "J. Mach. Learn. Res.,", "citeRegEx": "Bengio et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Bengio et al\\.", "year": 2003}, {"title": "Extraction of relations between genes and diseases from text and large-scale data analysis: implications for translational research", "author": ["Bravo et al.2015] \u00c0lex Bravo", "Janet Pi\u00f1ero", "N\u00faria Queralt-Rosinach", "Michael Rautschka", "Laura I Furlong"], "venue": null, "citeRegEx": "Bravo et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Bravo et al\\.", "year": 2015}, {"title": "Extraction of semantic biomedical relations from text using conditional random fields", "author": ["Mathaeus Dejori", "Martin Stetter", "Volker Tresp", "Hans-Peter Kriegel"], "venue": "BMC bioinformatics,", "citeRegEx": "Bundschus et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Bundschus et al\\.", "year": 2008}, {"title": "Integrating co-occurrence statistics with information extraction for robust retrieval of protein interactions from medline", "author": ["Raymond Mooney", "Arun Ramani", "Edward Marcotte"], "venue": null, "citeRegEx": "Bunescu et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Bunescu et al\\.", "year": 2006}, {"title": "A unified architecture for natural language processing: Deep neural networks with multitask learning", "author": ["Collobert", "Weston2008] Ronan Collobert", "Jason Weston"], "venue": "In Proceedings of the 25th international conference on Machine learning,", "citeRegEx": "Collobert et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Collobert et al\\.", "year": 2008}, {"title": "Natural language processing (almost) from scratch", "author": ["Jason Weston", "L\u00e9on Bottou", "Michael Karlen", "Koray Kavukcuoglu", "Pavel Kuksa"], "venue": "J. Mach. Learn. Res.,", "citeRegEx": "Collobert et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Collobert et al\\.", "year": 2011}, {"title": "Dependency tree kernels for relation extraction", "author": ["Culotta", "Sorensen2004] Aron Culotta", "Jeffrey Sorensen"], "venue": "In Proceedings of the 42nd Annual Meeting on Association for Computational Linguistics,", "citeRegEx": "Culotta et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Culotta et al\\.", "year": 2004}, {"title": "Classifying relations by ranking with convolutional neural networks", "author": ["Bing Xiang", "Bowen Zhou"], "venue": "In Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics", "citeRegEx": "Santos et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Santos et al\\.", "year": 2015}, {"title": "Semeval-2010 task 8: Multi-way classification", "author": ["Su Nam Kim", "Zornitsa Kozareva", "Preslav Nakov", "Diarmuid \u00d3 S\u00e9aghdha", "Sebastian Pad\u00f3", "Marco Pennacchiotti", "Lorenza Romano", "Stan Szpakowicz"], "venue": null, "citeRegEx": "Hendrickx et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Hendrickx et al\\.", "year": 2009}, {"title": "Relation extraction using support vector machine", "author": ["Gumwon Hong"], "venue": "In Natural Language Processing\u2013IJCNLP", "citeRegEx": "Hong.,? \\Q2005\\E", "shortCiteRegEx": "Hong.", "year": 2005}, {"title": "Convolutional neural network architectures for matching natural language sentences", "author": ["Hu et al.2014] Baotian Hu", "Zhengdong Lu", "Hang Li", "Qingcai Chen"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Hu et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Hu et al\\.", "year": 2014}, {"title": "A convolutional neural network for modelling sentences", "author": ["Edward Grefenstette", "Phil Blunsom"], "venue": "arXiv preprint arXiv:1404.2188", "citeRegEx": "Kalchbrenner et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Kalchbrenner et al\\.", "year": 2014}, {"title": "Deep visual-semantic alignments for generating image descriptions", "author": ["Karpathy", "Fei-Fei2014] Andrej Karpathy", "Li Fei-Fei"], "venue": "arXiv preprint arXiv:1412.2306", "citeRegEx": "Karpathy et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Karpathy et al\\.", "year": 2014}, {"title": "Convolutional neural networks for sentence classification", "author": ["Yoon Kim"], "venue": "In Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP),", "citeRegEx": "Kim.,? \\Q2014\\E", "shortCiteRegEx": "Kim.", "year": 2014}, {"title": "Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980", "author": ["Kingma", "Ba2014] Diederik Kingma", "Jimmy Ba"], "venue": null, "citeRegEx": "Kingma et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Kingma et al\\.", "year": 2014}, {"title": "Imagenet classification with deep convolutional neural networks", "author": ["Ilya Sutskever", "Geoffrey E Hinton"], "venue": null, "citeRegEx": "Krizhevsky et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Krizhevsky et al\\.", "year": 2012}, {"title": "A shallow parser based on closed-class words to capture relations in biomedical text", "author": ["Leroy et al.2003] Gondy Leroy", "Hsinchun Chen", "Jesse D Martinez"], "venue": "Journal of biomedical Informatics,", "citeRegEx": "Leroy et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Leroy et al\\.", "year": 2003}, {"title": "Kernel-based learning for biomedical relation extraction", "author": ["Li et al.2008] Jiexun Li", "Zhu Zhang", "Xin Li", "Hsinchun Chen"], "venue": "Journal of the American Society for Information Science and Technology,", "citeRegEx": "Li et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Li et al\\.", "year": 2008}, {"title": "Efficient estimation of word representations in vector space. arXiv preprint arXiv:1301.3781", "author": ["Kai Chen", "Greg Corrado", "Jeffrey Dean"], "venue": null, "citeRegEx": "Mikolov et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Mikolov et al\\.", "year": 2013}, {"title": "Distributed representations of words and phrases and their compositionality", "author": ["Ilya Sutskever", "Kai Chen", "Greg S Corrado", "Jeff Dean"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Mikolov et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Mikolov et al\\.", "year": 2013}, {"title": "Hybrid methods for improving information", "author": ["Anne-Laure Ligozat", "Asma Ben Abacha", "Delphine Bernhard", "Bruno Cartoni", "Louise Del\u00e9ger", "Brigitte Grau", "Sophie Rosset", "Pierre Zweigenbaum", "Cyril Grouin"], "venue": null, "citeRegEx": "Minard et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Minard et al\\.", "year": 2011}, {"title": "Multi-class svm for relation extraction from clinical reports", "author": ["Anne-Laure Ligozat", "Brigitte Grau"], "venue": "In RANLP,", "citeRegEx": "Minard et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Minard et al\\.", "year": 2011}, {"title": "Distant supervision for relation extraction without labeled data", "author": ["Mintz et al.2009] Mike Mintz", "Steven Bills", "Rion Snow", "Dan Jurafsky"], "venue": "In Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International", "citeRegEx": "Mintz et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Mintz et al\\.", "year": 2009}, {"title": "Bidirectional incremental parsing for automatic pathway identification with combinatory categorial grammar", "author": ["Park et al.2001] Jong C Park", "Hyun Sook Kim", "Jung-Jae Kim"], "venue": "In Pacific Symposium on Biocomputing,", "citeRegEx": "Park et al\\.,? \\Q2001\\E", "shortCiteRegEx": "Park et al\\.", "year": 2001}, {"title": "Tree kernel-based proteinprotein interaction extraction from biomedical literature", "author": ["Qian", "Zhou2012] Longhua Qian", "Guodong Zhou"], "venue": "Journal of Biomedical Informatics,", "citeRegEx": "Qian et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Qian et al\\.", "year": 2012}, {"title": "Modeling relations and their mentions without labeled text", "author": ["Limin Yao", "Andrew McCallum"], "venue": "In Machine Learning and Knowledge Discovery in Databases,", "citeRegEx": "Riedel et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Riedel et al\\.", "year": 2010}, {"title": "Automatic extraction of relations between medical concepts in clinical texts", "author": ["Rink et al.2011] Bryan Rink", "Sanda Harabagiu", "Kirk Roberts"], "venue": "Journal of the American Medical Informatics Association,", "citeRegEx": "Rink et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Rink et al\\.", "year": 2011}, {"title": "Applying umls for distantly supervised relation detection", "author": ["Roller", "Stevenson2014] Roland Roller", "Mark Stevenson"], "venue": "In Proceedings of the 5th International Workshop on Health Text Mining and Information Analysis (Louhi),", "citeRegEx": "Roller et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Roller et al\\.", "year": 2014}, {"title": "Classifying semantic relations in bioscience texts", "author": ["Rosario", "Hearst2004] Barbara Rosario", "Marti A. Hearst"], "venue": "In Proceedings of the 42Nd Annual Meeting on Association for Computational Linguistics,", "citeRegEx": "Rosario et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Rosario et al\\.", "year": 2004}, {"title": "Enhancing biomedical text summarization using semantic relation extraction", "author": ["Shang et al.2011] Yue Shang", "Yanpeng Li", "Hongfei Lin", "Zhihao Yang"], "venue": "PLoS ONE,", "citeRegEx": "Shang et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Shang et al\\.", "year": 2011}, {"title": "Predicting online doctor ratings from user reviews using convolutional neural networks", "author": ["Samarth Tripathi", "Sunil K Sahu", "Sudhanshu Mittal", "Ashish Anand"], "venue": "International Journal of Machine Learning and Computing,", "citeRegEx": "Sharma et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Sharma et al\\.", "year": 2016}, {"title": "Relationship extraction methods based on co-occurrence in web pages and files", "author": ["Song et al.2011] Qiang Song", "Yousuke Watanabe", "Haruo Yokota"], "venue": "In Proceedings of the 13th International Conference on Information Integration and Web-based Applica-", "citeRegEx": "Song et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Song et al\\.", "year": 2011}, {"title": "Dropout: A simple way to prevent neural networks from overfitting", "author": ["Geoffrey Hinton", "Alex Krizhevsky", "Ilya Sutskever", "Ruslan Salakhutdinov"], "venue": "The Journal of Machine Learning Research,", "citeRegEx": "Srivastava et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Srivastava et al\\.", "year": 2014}, {"title": "Biobibliometrics: information retrieval and visualization from co-occurrences of gene names in medline abstracts", "author": ["Stapley", "Benoit2000] Benjamin J Stapley", "Gerry Benoit"], "venue": "In Pac Symp Biocomput,", "citeRegEx": "Stapley et al\\.,? \\Q2000\\E", "shortCiteRegEx": "Stapley et al\\.", "year": 2000}, {"title": "Evaluating temporal relations in clinical text: 2012 i2b2 challenge", "author": ["Sun et al.2013] Weiyi Sun", "Anna Rumshisky", "Ozlem Uzuner"], "venue": "Journal of the American Medical Informatics Association,", "citeRegEx": "Sun et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Sun et al\\.", "year": 2013}, {"title": "Evaluating distributed word representations for capturing semantics of biomedical concepts", "author": ["TH et al.2015] MUNEEB TH", "Sunil Sahu", "Ashish Anand"], "venue": "In Proceedings of BioNLP", "citeRegEx": "TH et al\\.,? \\Q2015\\E", "shortCiteRegEx": "TH et al\\.", "year": 2015}, {"title": "Automatic extraction of protein interactions from scientific", "author": ["Thomas et al.2000] James Thomas", "David Milward", "Christos Ouzounis", "Stephen Pulman", "Mark Carroll"], "venue": "In Pacific symposium on biocomputing,", "citeRegEx": "Thomas et al\\.,? \\Q2000\\E", "shortCiteRegEx": "Thomas et al\\.", "year": 2000}, {"title": "Evaluating the state-of-the-art in automatic de-identification", "author": ["Uzuner et al.2007] \u00d6zlem Uzuner", "Yuan Luo", "Peter Szolovits"], "venue": "Journal of the American Medical Informatics Association,", "citeRegEx": "Uzuner et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Uzuner et al\\.", "year": 2007}, {"title": "Identifying patient smoking status from medical discharge records", "author": ["Uzuner et al.2008] \u00d6zlem Uzuner", "Ira Goldstein", "Yuan Luo", "Isaac Kohane"], "venue": "Journal of the American Medical Informatics Association,", "citeRegEx": "Uzuner et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Uzuner et al\\.", "year": 2008}, {"title": "Extracting medication information from clinical text", "author": ["Uzuner et al.2010] \u00d6zlem Uzuner", "Imre Solti", "Eithon Cadag"], "venue": "Journal of the American Medical Informatics Association,", "citeRegEx": "Uzuner et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Uzuner et al\\.", "year": 2010}, {"title": "Bootstrapping Relation Extraction from Semantic Seeds", "author": ["Fei-Yu Xu"], "venue": "Ph.D. thesis,", "citeRegEx": "Xu.,? \\Q2008\\E", "shortCiteRegEx": "Xu.", "year": 2008}, {"title": "Kernel methods for relation extraction", "author": ["Chinatsu Aone", "Anthony Richardella"], "venue": "The Journal of Machine Learning Research,", "citeRegEx": "Zelenko et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Zelenko et al\\.", "year": 2003}, {"title": "Relation classification via convolutional deep neural network", "author": ["Zeng et al.2014] Daojian Zeng", "Kang Liu", "Siwei Lai", "Guangyou Zhou", "Jun Zhao"], "venue": "COLING", "citeRegEx": "Zeng et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Zeng et al\\.", "year": 2014}], "referenceMentions": [{"referenceID": 29, "context": "These relations are important for other upper level NLP tasks and also in biomedical and clinical research (Shang et al., 2011).", "startOffset": 107, "endOffset": 127}, {"referenceID": 3, "context": "In co-occurrence analysis it is assumed that if two entities are coming together in many sentences, their must be a relation between them (Bunescu et al., 2006; Song et al., 2011).", "startOffset": 138, "endOffset": 179}, {"referenceID": 31, "context": "In co-occurrence analysis it is assumed that if two entities are coming together in many sentences, their must be a relation between them (Bunescu et al., 2006; Song et al., 2011).", "startOffset": 138, "endOffset": 179}, {"referenceID": 36, "context": "Rule based methods are another commonly adapted methods for relation extraction task (Thomas et al., 2000; Park et al., 2001; Leroy et al., 2003).", "startOffset": 85, "endOffset": 145}, {"referenceID": 23, "context": "Rule based methods are another commonly adapted methods for relation extraction task (Thomas et al., 2000; Park et al., 2001; Leroy et al., 2003).", "startOffset": 85, "endOffset": 145}, {"referenceID": 16, "context": "Rule based methods are another commonly adapted methods for relation extraction task (Thomas et al., 2000; Park et al., 2001; Leroy et al., 2003).", "startOffset": 85, "endOffset": 145}, {"referenceID": 40, "context": "Bootstrapping method (Xu, 2008) is used to improve the performance of rule based methods.", "startOffset": 21, "endOffset": 31}, {"referenceID": 40, "context": "Bootstrapping uses small number of known relation pair of each relation type as a seed and use these seeds to search patterns in huge unannotated text (Xu, 2008) in iterative fashion.", "startOffset": 151, "endOffset": 161}, {"referenceID": 22, "context": "Distantly supervised method uses large knowledge base such as UMLS or Freebase as an input and extract patterns from huge corpus for all pair of relations present in knowledge base (Mintz et al., 2009; Riedel et al., 2010; Roller and Stevenson, 2014).", "startOffset": 181, "endOffset": 250}, {"referenceID": 25, "context": "Distantly supervised method uses large knowledge base such as UMLS or Freebase as an input and extract patterns from huge corpus for all pair of relations present in knowledge base (Mintz et al., 2009; Riedel et al., 2010; Roller and Stevenson, 2014).", "startOffset": 181, "endOffset": 250}, {"referenceID": 9, "context": "Feature based methods use sentences with predefined entities to construct feature vector through feature extraction (Hong, 2005; Minard et al., 2011b; Rink et al., 2011).", "startOffset": 116, "endOffset": 169}, {"referenceID": 26, "context": "Feature based methods use sentences with predefined entities to construct feature vector through feature extraction (Hong, 2005; Minard et al., 2011b; Rink et al., 2011).", "startOffset": 116, "endOffset": 169}, {"referenceID": 41, "context": "Kernel methods are extension of feature based methods which utilize kernel functions to exploit rich syntactic information such as parse trees (Zelenko et al., 2003; Culotta and Sorensen, 2004; Qian and Zhou, 2012; Zeng et al., 2014).", "startOffset": 143, "endOffset": 233}, {"referenceID": 42, "context": "Kernel methods are extension of feature based methods which utilize kernel functions to exploit rich syntactic information such as parse trees (Zelenko et al., 2003; Culotta and Sorensen, 2004; Qian and Zhou, 2012; Zeng et al., 2014).", "startOffset": 143, "endOffset": 233}, {"referenceID": 0, "context": "Further often these methods lead to huge number of features and may get affected from curse of dimensionality issues (Bengio et al., 2003; Collobert et al., 2011).", "startOffset": 117, "endOffset": 162}, {"referenceID": 5, "context": "Further often these methods lead to huge number of features and may get affected from curse of dimensionality issues (Bengio et al., 2003; Collobert et al., 2011).", "startOffset": 117, "endOffset": 162}, {"referenceID": 15, "context": "Convolution neural network has shown to be a powerful model for image processing, computer vision (Krizhevsky et al., 2012; Karpathy and Fei-Fei, 2014) and subsequently in natural language processing it has given state of the art results in different tasks such as sentence classification (Kim, 2014; Kalchbrenner et al.", "startOffset": 98, "endOffset": 151}, {"referenceID": 13, "context": ", 2012; Karpathy and Fei-Fei, 2014) and subsequently in natural language processing it has given state of the art results in different tasks such as sentence classification (Kim, 2014; Kalchbrenner et al., 2014; Hu et al., 2014; Sharma et al., 2016), relation classification (Zeng et al.", "startOffset": 173, "endOffset": 249}, {"referenceID": 11, "context": ", 2012; Karpathy and Fei-Fei, 2014) and subsequently in natural language processing it has given state of the art results in different tasks such as sentence classification (Kim, 2014; Kalchbrenner et al., 2014; Hu et al., 2014; Sharma et al., 2016), relation classification (Zeng et al.", "startOffset": 173, "endOffset": 249}, {"referenceID": 10, "context": ", 2012; Karpathy and Fei-Fei, 2014) and subsequently in natural language processing it has given state of the art results in different tasks such as sentence classification (Kim, 2014; Kalchbrenner et al., 2014; Hu et al., 2014; Sharma et al., 2016), relation classification (Zeng et al.", "startOffset": 173, "endOffset": 249}, {"referenceID": 30, "context": ", 2012; Karpathy and Fei-Fei, 2014) and subsequently in natural language processing it has given state of the art results in different tasks such as sentence classification (Kim, 2014; Kalchbrenner et al., 2014; Hu et al., 2014; Sharma et al., 2016), relation classification (Zeng et al.", "startOffset": 173, "endOffset": 249}, {"referenceID": 42, "context": ", 2016), relation classification (Zeng et al., 2014; dos Santos et al., 2015) and semantic role labeling (Collobert et al.", "startOffset": 33, "endOffset": 77}, {"referenceID": 5, "context": ", 2015) and semantic role labeling (Collobert et al., 2011).", "startOffset": 35, "endOffset": 59}, {"referenceID": 26, "context": "Current state of the art methods heavily depend on manual feature engineering and use hundreds of thousands of features (Minard et al., 2011b; Rink et al., 2011).", "startOffset": 120, "endOffset": 161}, {"referenceID": 39, "context": "Most of the participants in this challenge used support vector machine (SVM) with manually designed features (Uzuner et al., 2010).", "startOffset": 109, "endOffset": 130}, {"referenceID": 26, "context": "Model proposed by Rink et al. (2011) had first place in this task, which used six classes of features namely, context features, similarity features, nested related relation features, Wikipedia features, single concept features and vicinity features.", "startOffset": 18, "endOffset": 37}, {"referenceID": 17, "context": "Li et al. (2008) proposed kernel methods for relation extraction between entities in MEDLINE R \u00a9 articles.", "startOffset": 0, "endOffset": 17}, {"referenceID": 2, "context": "Conditional random field (CRF) has been used for relation extraction between disease treatment and gene by (Bundschus et al., 2008).", "startOffset": 107, "endOffset": 131}, {"referenceID": 1, "context": "Recently Bravo et al. (2015) proposed a system for identifying association between drug disease and target in EU-ADR dataset (van Mulligen et al.", "startOffset": 9, "endOffset": 29}, {"referenceID": 42, "context": "In contrast to above methods recently there are few work applying convolution neural network based models (Zeng et al., 2014; dos Santos et al., 2015) for relation classification in SemEval 2010 relation classification dataset (Hendrickx et al.", "startOffset": 106, "endOffset": 150}, {"referenceID": 8, "context": ", 2015) for relation classification in SemEval 2010 relation classification dataset (Hendrickx et al., 2009).", "startOffset": 84, "endOffset": 108}, {"referenceID": 35, "context": "For word embedding, we used pretrained word vector (TH et al., 2015) learned on Pubmed articles using word2vec tool (Mikolov et al.", "startOffset": 51, "endOffset": 68}, {"referenceID": 13, "context": "In order to get local features from each part of the sentence we have used multiple filters of different lengths (Kim, 2014) in all possible continuous n-gram of the sentence, where n is the length of filter (We have shown four filters with constant length three in the figure 1).", "startOffset": 113, "endOffset": 124}, {"referenceID": 35, "context": ", 2013a) on huge Pubmed open source articles (TH et al., 2015).", "startOffset": 45, "endOffset": 62}, {"referenceID": 13, "context": "In our experiment we use multiple such filters of variable length (Kim, 2014; Yin and Schtze, 2015).", "startOffset": 66, "endOffset": 99}, {"referenceID": 32, "context": "For regularization (Srivastava et al., 2014), we follow (Kim, 2014) and use dropout technique in output of max pooling layer.", "startOffset": 19, "endOffset": 44}, {"referenceID": 13, "context": ", 2014), we follow (Kim, 2014) and use dropout technique in output of max pooling layer.", "startOffset": 19, "endOffset": 30}, {"referenceID": 37, "context": "In recent years several challenges have been organized to automatically extract information from clinical texts (Uzuner et al., 2007; Uzuner et al., 2008; Uzuner et al., 2011; Uzuner et al., 2010; Sun et al., 2013).", "startOffset": 112, "endOffset": 214}, {"referenceID": 38, "context": "In recent years several challenges have been organized to automatically extract information from clinical texts (Uzuner et al., 2007; Uzuner et al., 2008; Uzuner et al., 2011; Uzuner et al., 2010; Sun et al., 2013).", "startOffset": 112, "endOffset": 214}, {"referenceID": 39, "context": "In recent years several challenges have been organized to automatically extract information from clinical texts (Uzuner et al., 2007; Uzuner et al., 2008; Uzuner et al., 2011; Uzuner et al., 2010; Sun et al., 2013).", "startOffset": 112, "endOffset": 214}, {"referenceID": 34, "context": "In recent years several challenges have been organized to automatically extract information from clinical texts (Uzuner et al., 2007; Uzuner et al., 2008; Uzuner et al., 2011; Uzuner et al., 2010; Sun et al., 2013).", "startOffset": 112, "endOffset": 214}, {"referenceID": 26, "context": "In our experiment we assume that entities and their types are already known like other existing works (Rink et al., 2011; Minard et al., 2011a; Minard et al., 2011b).", "startOffset": 102, "endOffset": 165}, {"referenceID": 26, "context": "We build a linear SVM classifier using similar features as defined in earlier studies (Rink et al., 2011) as a baseline for comparison.", "startOffset": 86, "endOffset": 105}], "year": 2016, "abstractText": "In recent years extracting relevant information from biomedical and clinical texts such as research articles, discharge summaries, or electronic health records have been a subject of many research efforts and shared challenges. Relation extraction is the process of detecting and classifying the semantic relation among entities in a given piece of texts. Existing models for this task in biomedical domain use either manually engineered features or kernel methods to create feature vector. These features are then fed to classifier for the prediction of the correct class. It turns out that the results of these methods are highly dependent on quality of user designed features and also suffer from curse of dimensionality. In this work we focus on extracting relations from clinical discharge summaries. Our main objective is to exploit the power of convolution neural network (CNN) to learn features automatically and thus reduce the dependency on manual feature engineering. We evaluate performance of the proposed model on i2b2-2010 clinical relation extraction challenge dataset. Our results indicate that convolution neural network can be a good model for relation exaction in clinical text without being dependent on expert\u2019s knowledge on defining quality features.", "creator": "LaTeX with hyperref package"}}}