{"id": "1606.09383", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "30-Jun-2016", "title": "On Approximate Dynamic Programming with Multivariate Splines for Adaptive Control", "abstract": "We define a SDP framework based on the RLSTD algorithm and multivariate simplex B-splines. We introduce a local forget factor capable of preserving the continuity of the simplex splines. This local forget factor is integrated with the RLSTD algorithm, resulting in a modified RLSTD algorithm that is capable of tracking time-varying systems. We present the results of two numerical experiments, one validating SDP and comparing it with NDP and another to show the advantages of the modified RLSTD algorithm over the original. While SDP requires more computations per time-step, the experiment shows that for the same amount of function approximator parameters, there is an increase in performance in terms of stability and learning rate compared to NDP. The second experiment shows that SDP in combination with the modified RLSTD algorithm allows for faster recovery compared to the original RLSTD algorithm when system parameters are altered, paving the way for an adaptive high-performance non-linear control method.\n\n\n\n\n\nIntroduction", "histories": [["v1", "Thu, 30 Jun 2016 08:12:21 GMT  (37kb,D)", "http://arxiv.org/abs/1606.09383v1", "23 pages"]], "COMMENTS": "23 pages", "reviews": [], "SUBJECTS": "cs.LG cs.SY", "authors": ["willem eerland", "coen de visser", "erik-jan van kampen"], "accepted": false, "id": "1606.09383"}, "pdf": {"name": "1606.09383.pdf", "metadata": {"source": "CRF", "title": "On Approximate Dynamic Programming with Multivariate Splines for Adaptive Control", "authors": ["W.J. Eerland", "C.C. de Visser", "E. van Kampen"], "emails": ["[w.j.eerland@soton.ac.uk]"], "sections": [{"heading": "1 Introduction", "text": "Bellman described multi-stage decision processes from a mathematical point of view in Bellman [1957], this algorithm was called Dynamic Programming (DP). The DP principle comes down to describing each state with a value in a value function and moving the system to the state with the highest value. The value function is also called the cost-to-go function and stores the expected sum of future rewards for each state. In general this function cannot be found directly, therefore an iterative approach like Temporal Difference (TD)-learning [Sutton and Barto, 1998, Sutton, 1988] is used.\nDP has the ability to solve complex control problems in diverse environments, and it is possible to view the environment as a black-box by modelling it using system identification techniques. The advantage of combining DP and system identification techniques, is that it leads to an adaptive control scheme. These adaptive controllers have already been successfully trained off-line for many purposes, ranging from agile missile interception [Han and Balakrishnan, 1999] to aircraft auto-landing and control [Saini and Balakrishnan, 1997]. Also,\n\u2217University of Southampton, Transportation Research Group [w.j.eerland@soton.ac.uk] \u2020Delft University of Technology, Control and Simulation \u2021Delft University of Technology, Control and Simulation\nar X\niv :1\n60 6.\n09 38\n3v 1\n[ cs\n.L G\n] 3\n0 Ju\nn 20\n16\nan on-line adaptive critic flight control was implemented on a six-degree-offreedom business jet aircraft over its full operating envelope, improving its performance when unexpected conditions are encountered for the first time [Ferrari and Stengel, 2004].\nBarto, Sutton and Anderson used neural networks to parametrise the value function [Barto et al., 1983]. However, this function approximator is non-linear in the parameters, with the result that stability can only be guaranteed if bounded network weights are used, where bounds are determined by off-line analysis. This approach has been applied to examples [Han and Balakrishnan, 1999, Saini and Balakrishnan, 1997, Ferrari and Stengel, 2004] mentioned before. More recently, an adaptive controller has been introduced in Anderson et al. [2007], and again the neural network weights are bounded to guarantee stability. A drawback is that for a time-varying system these bounds shift and stability can no longer be guaranteed. This combination of neural networks and DP is commonly referred to as Neuro Dynamic Programming (NDP) [Bertsekas and Tsitsiklis, 1996].\nThere is a proof of convergence when linear-in-the-parameters function approximators are used in Tsitsiklis and Van Roy [1997], however, this proof demands knowledge of the shape of the optimal value function. With the Recursive Least Squares Temporal Difference (RLS TD) algorithm convergence in a stochastic framework is assured [Bradtke and Barto, 1996], even when the linear regression basis cannot perfectly fit the value function. In the last decade, the DP theory in continuous time and space has been further developed in Doya [1996, 2000], Morimoto and Doya [2005]. More recently, the proof of convergence has been extended to include the optimal policy [Ma and Powell, 2009], however one of the problems that remains is the a-priori unknown shape of the value function.\nUsing polynomials as a function approximator in a DP framework was investigated by Bellman in 1963 [Bellman et al., 1963]. In 1985, Schweitzer [1985] concentrates on the use of global polynomials in combination with DP. With the development of the RLS TD algorithm, it is possible to obtain a proven convergence by combining it with a polynomial approximation as discussed in Ma and Powell [2009]. However the limitation is that the approximation power of global polynomials can only be increased by increasing the order of the polynomials, which will also lead to numerical instabilities in the solution schemes of the approximation. According to Summers et al. [2013], using the sum of squares allows the use of higher order polynomials, but will eventually still lead to numerical instability.\nA recommendation in Ma and Powell [2009] is to use local polynomial regression to treat Markov Decision Process (MDP) problems with value functions of unknown form. This recommendation is supported by Daniel [1976], which states that it is highly desirable from an efficiency point of view to use local polynomial regression. Recently a novel method based on multivariate simplex B-splines has been applied in a linear regression framework [de Visser et al., 2009]. The use of a local polynomial basis allows for transparency and efficient (sparse) computational schemes. Furthermore, the spatial location of the B-coefficients and modularity of the triangulation allow for local model modification and refinement [Lai and Schumaker, 2007]. These properties make multivariate simplex B-splines an excellent candidate for use in the DP framework.\nThere are existing approaches which combines Multivariate Adaptive Regression Splines (MARS) [Friedman, 1991] and DP [Chen, 1999, Chen et al., 1999, Cervellera et al., 2007]. However, multivariate simplex B-splines distinguishes itself from MARS in terms of computational efficiency by using B-splines [Bakin et al., 2000]. And they are supported by a triangulation of simplices, allowing functionality in a non-square domain [Lai and Schumaker, 2007].\nThe contribution of this paper is a framework that allows the use of multivariate simplex B-splines in combination with the RLS TD algorithm, giving rise to Spline Dynamic Programming (SDP) that enables control of non-linear stochastic systems. A method for continuous local value function adaptation is presented which is enabled by the spatial location property of the coefficients of the multivariate splines; this is achieved by implementing a new formulation for the covariance update step. The effectiveness of this SDP framework is investigated by comparing it with NDP in terms of computational complexity and performance. Furthermore, the RLS TD algorithm is modified to allow for adaptive control of time-varying systems. The validation and comparison of both cases are investigated with a non-linear 2D control problem, the pendulum swing-up.\nIn section 2, we briefly introduce the DP framework and show how the value function and greedy policy are constructed. In section 3, we give a brief introduction on the mathematical background of multivariate simplex B-splines, the function approximator used in the SDP framework. The SDP framework itself is explained in section 4; the main purpose of this section is to address the steps specific to using multivariate simplex B-splines for value function approximation in combination with the RLS TD algorithm. Section 5 is there to demonstrate that the SDP framework indeed works for the given control problem. SDP is compared with neural networks for system performance on a stochastic system and a time-varying system. These results are discussed in section 6 and finally conclusions and recommendations are presented in section 7."}, {"heading": "2 Preliminaries on Dynamic Programming", "text": "In this section, we present the preliminaries on DP, the algorithm that is part of the SDP framework. For a more complete description we refer to Sutton and Barto [1998], Si et al. [2004], Powell [2007], Busoniu et al. [2010], Bertsekas [2007]. We start with a brief overview of the MDP followed by the policy evaluation and concluded with the policy improvement."}, {"heading": "2.1 Markov Decision Processes", "text": "The policy evaluation problem associated with discrete-time stochastic optimal control problems is referred to as a MDP. Finding the solution to an MDP is a sequential optimization problem where the goal is to find a policy that maximizes the sum of the expected infinite-horizon discounted rewards.\nLet xt \u2208 X be the state vector and ut \u2208 U be the input vector, both at time t, where ut is determined by the policy \u03c0 and X and U denote the finite sets of states and inputs. The reward function is rt+1(xt+1,ut) and 0 \u2264 \u03b3 < 1 is the discount factor. The goal is to find a policy \u03c0 that obtains the maximum total reward.\nFor each policy \u03c0 there exists a value function V \u03c0(xt) that indicates a measure of long-term performance at each state:\nV \u03c0(xt) = \u221e\u2211 k=t \u03b3k\u2212trk+1(xk+1,uk) (1)\nThe objective can now be formulated as finding a policy \u03c0\u2217 such that V \u03c0 \u2217 (x) \u2265 V \u03c0(x) for all x \u2208 X and for all policies \u03c0. This policy \u03c0\u2217 is called the optimal policy and can be found by applying both policy evaluation and policy improvement Sutton and Barto [1998].\nThe policy evaluation determines the V \u03c0(xt) of the current policy \u03c0, where the policy improvement uses this knowledge to adjust the policy \u03c0 such that it ends up in the most valuable states."}, {"heading": "2.2 Policy evaluation", "text": "In order to evaluate the value function in an iterative fashion, Sutton [1988] uses TD-learning. For TD-learning, the following must hold:\nV \u03c0(xt) = rt+1 + \u2211\u221e k=t+1 \u03b3\nk\u2212trk+1 = rt+1 + \u03b3V \u03c0(xt+1) (2)\nIf this equality does not hold, the difference is called the TD-error:\net = rt+1 + \u03b3V \u03c0(xt+1)\u2212 V \u03c0(xt) (3)\nBy minimizing this TD-error, the value function can be constructed in an iterative approach. In order to construct a value function for a continuous problem, there is need for parameterization to describe a complete state space with a finite number of parameters. The value function now becomes V \u03c0(xt, ct), where ct are the parameters at time t that shape the continuous value function in domain X.\nIn Bradtke and Barto [1996] the RLS TD algorithm was introduced; this\nalgorithm and its computational complexity is visible in Table 1. Here d\u0302 and a\u0302 represent the number of coefficients per simplex and total number of coefficients respectively, B represents the linear regression matrix and P is the parameter covariance matrix. These parameters will be further explained in section 3. RLS TD converges to the least squares approximation of the optimal value function c\u2217, given that each state x \u2208 X is visited infinitely often. Although RLS TD requires more computations per time-step than TD(\u03bb) algorithms [Sutton, 1988], it is more efficient in the statistical sense as more information is extracted from training experience, allowing it to converge faster [Bradtke and Barto, 1996]. Furthermore, while Least Mean Square (LMS) aims to decrease the mean square error et at each time-step separately, RLS TD minimizes this objective function:\nJt = 1\nt t\u2211 k=0 [ek] 2\n(4)"}, {"heading": "2.3 Policy improvement", "text": "The computation of the value function is called policy evaluation. Using this value function, a greedy action can be selected. If a policy is updated in this manner, it is called (greedy) policy improvement:\nut(xt) = max ut\u2208U\n[V \u03c0(xt)] (5)\nThe repetition of the policy evaluation and policy improvement is called policy iteration and will result in an optimal policy [Sutton and Barto, 1998]. According to Doya [1996], the optimal non-linear feedback control law is a function of value function\u2019s gradient:\nut(xt) = u max g\n( 1\nc \u03c4 \u2202V \u03c0(xt)\n\u2202xt\n\u2202f(xt,ut)\n\u2202ut\n) (6)\nwhere umax is the maximum control input, g(x) = tanh(\u03c02x), c is the control cost parameter, \u03c4 is the step-size parameter and f(xt,ut) are the system dynamics.\nThis optimal control law is applied to the pendulum swing-up task, with the result visible in A. Having discussed both the policy evaluation and improvement, section 3 will now describe the parametrization of the value function using multivariate simplex B-splines."}, {"heading": "3 Preliminaries on Multivariate Simplex B-Splines", "text": "This section serves as a brief introduction to the mathematical theory of the simplex B-splines. For a more extensive and general introduction to multivariate spline theory we refer to Lai and Schumaker [2007]. We start by introducing the basic concept of a single basis polynomial and B-form, then introduce the triangulation, followed by the vector notation of the B-form. Finally the Recursive Least Squares (RLS) estimator for simplex splines is reviewed."}, {"heading": "3.1 Simplex and barycentric coordinates", "text": "The polynomial basis of a multivariate simplex B-spline is defined on a simplex. A simplex is defined by the non-degenerate vertices (\u03c50, \u03c51, . . . , \u03c5n) \u2208 Rn and thus creates a span in n-dimensional space. Any point x = (x1, x2, . . . , xn) can be transformed to a barycentric coordinate b(x) = (b0, b1, . . . , bn) with respect to a simplex. The relation between Cartesian coordinate x and barycentric coordinate b(x) is:\nx = n\u2211 i=0 bi\u03c5i\nn\u2211 i=0 bi = 1 (7)"}, {"heading": "3.2 Triangulation", "text": "Any number of simplices can be combined into a triangulation, where a triangulation T is a special partitioning of a domain into a set of J non-overlapping simplices and is defined in Lai and Schumaker [2007] as:\nT \u2261 J\u22c3 i=1 ti, ti \u2229 tj \u2208 {\u2205, t\u0303}, \u2200ti, tj \u2208 T , i 6= j (8)\nwith t\u0303 a simplex of dimension (< n). A popular triangulation method is Delaunay triangulation [Lee and Schachter, 1980]. Recently, a new method for creating globally optimal triangulations named Intersplines was introduced in de Visser et al. [2012]."}, {"heading": "3.3 Basis functions", "text": "The polynomial basis of the simplex spline are the Bernstein basis polynomials Bd\u03ba(b), where d is the degree of the spline and b is the barycentric coordinate discussed earlier:\nBd\u03ba(b) = d!\n\u03ba! b\u03ba (9)\nhere \u03ba \u2265 0 is a multi-index, which has properties: \u03ba! = \u03ba0!\u03ba1! \u00b7 \u00b7 \u00b7\u03ban!, |\u03ba| = \u03ba0 +\u03ba1 + \u00b7 \u00b7 \u00b7+\u03ban and b\u03ba = b\u03ba00 b \u03ba1 1 \u00b7 \u00b7 \u00b7 b\u03bann . The valid permutations of \u03ba with the constraint |\u03ba| = d equal the total number of B-coefficients and basis polynomials per simplex and is equal to:\nd\u0302 = (d+ n)!\nn!d! (10)\nIn combination with a set of J non-overlapping simplices, the total number of B-coefficients for a complete triangulation is:\na\u0302 = J \u00b7 d\u0302 (11)\nThe multi-index \u03ba has a requirement on the ordering, called a lexicographical sorting order which is introduced in Lai and Schumaker [2007]. This means \u03ba\u03bd\u00b5\u03ba comes before \u03baijk provided that \u03bd > i, or if \u03bd = i, then \u00b5 > j, or if \u03bd = i and \u00b5 = j, then \u03ba > k. Thus for d = 2 the order is:\n\u03ba2,0,0, \u03ba1,1,0, \u03ba1,0,1, \u03ba0,2,0, \u03ba0,1,1, \u03ba0,0,2 (12)\nEach B-coefficient has a unique position within the simplex based on \u03ba; this relation between B-coefficient and spatial position allows the creation of a Bnet as seen in Figure 1."}, {"heading": "3.4 Vector formulation of the B-form", "text": "In order to complete the vector formulation, Bdtj (b) \u2208 R d\u0302\u00d71 is introduced as a vector of Bernstein basis polynomials (Eq. 9) of simplex tj , which are sorted lexicographically as indicated by Eq. 12. Adapted from de Visser and Verhaegen [2013], we define the B-form as a row vector on simplex tj :\np(b) =\n{ Bd(b)> \u00b7 ctj , x \u2208 tj\n0, x /\u2208 tj (13)\nwhere ctj are the B-coefficients on simplex tj . The matrix operation to evaluate the simplex B-spline function of degree d and continuity order r, defined on a triangulation TJ is:\nsrd(b) \u2261 B(b)> \u00b7 c \u2208 R, x \u2208 TJ (14)\nNow B(b) (note the absence of the superscript \u2019d\u2019) is the global vector of basis polynomials:\nB(b) \u2261 [ Bd(b)>t1 B d(b)>t2 \u00b7 \u00b7 \u00b7 B d(b)>tJ ]> \u2208 Ra\u0302 x 1 (15) The global vector of B-coefficients c is defined as:\nc \u2261 [ ct1 > ct2 > \u00b7 \u00b7 \u00b7 ctJ> ]> \u2208 Ra\u0302 x 1 (16)\nThe spline space is the space of all spline functions srd in the triangulation T . We use the definition of the spline space from Lai and Schumaker [2007]:\nSrd(T ) \u2261 {srd \u2208 Cr(T ) : srd|t \u2208 Pd,\u2200t \u2208 T } (17)\nwhere Pd is the space of polynomials of degree d."}, {"heading": "3.5 Continuity", "text": "Since p(b) is a linear combination of continuous functions, srd(b) is naturally continuous on each simplex. However, in order to assure continuity of Srd(T ) between simplices, constraints are imposed on the relations between the coefficients of different simplices. The continuity order r fixes the derivatives d\nrp dbr\non the edges between neighboring simplices. The required continuity conditions can be calculated using Lai and Schumaker [2007]:\ncti(\u03ba0,...,\u03ban\u22121,m) = \u2211 |\u03b3|=m c tj (\u03ba0,...,\u03ban\u22121,0)+\u03b3 Bm\u03b3 (w)\n0 \u2264 m \u2264 r (18)\nHere w is a vertex of simplex tj which is not found on the edge that is shared with simplex ti. All constraints required for continuity are collected in the smoothness matrix H, with each row containing a new constraint and the columns consisting of the coefficients de Visser et al. [2009, 2011]. These equations are all equaled to zero, resulting in the following matrix form:\nHc = 0 (19)\nwith c as in Eq. 16."}, {"heading": "3.6 Approximation power", "text": "To describe the approximation power, the following definition from Lai and Schumaker [2007], chapter 10.1 is used:\nDefinition (Approximation power of Srd(T )) Fix 0 \u2264 r < d and 0 < \u03b8 \u2264 \u03c0/3. Let m be the largest integer such that for every polygonal domain \u2126 and every regular triangulation T of \u2126 with smallest angle \u03b8, for every f \u2208Wmq (\u2126), there exists a spline s \u2208 Srd(T ) with\n||f \u2212 s||q,\u2126 \u2264 K |T |m |f |m,q,\u2126 (20)\nwhere the constant K depends only on r, d, \u03b8, and the Lipschitz constant of the boundary of \u2126. Then we say that Srd has approximation power m in the q-norm. If this holds for m = d+ 1, we say that Srd has full approximation power in the q-norm.\nThe theory behind this is extensive and available in Lai and Schumaker [2007], however for now it is important to realize that |T |m is a function of the longest edge in triangulation T . This reveals that it is possible to locally increase the approximation power by reducing the length of the longest edge in T ."}, {"heading": "3.7 Recursive least squares", "text": "RLS is a method which allows the estimated parameters c to be updated online with the use of the parameter covariance matrix P de Visser et al. [2011]. The algorithm and the computational complexity is found in Table 2. Note that it is essential to keep the column relations of P intact to enforce the constraints Hc = 0 from Eq. 19. To initialize the P matrix, we use Z, the orthogonal projector on the null-space of H Lawson and Hanson [1974]:\nP1 = \u03b21 Z (21)\nwhere Z = (I \u2212 H+H), in which I and H are the identity and smoothness matrix respectively. The parameter \u03b21 > 0 indicates the confidence in the initial estimated parameters, where a larger \u03b21 indicates a lower confidence level. To initialize c, it is important to pick these such that the continuity constraints are not violated. For this, a constrained Least Squares (LS) fit of the estimated shape can be used, using the approach from de Visser et al. [2009]. If no knowledge is available, initialization of all coefficients at zero will satisfy the constraints, resulting in:\nc1 = 0 (22)"}, {"heading": "4 Spline Dynamic Programming", "text": "This section has a dual purpose; it will introduce the framework that combines both simplex splines and the RLS TD algorithm, and the modified RLS TD algorithm with the ability to track time-varying systems will be defined."}, {"heading": "4.1 The SDP framework", "text": "To successfully represent the optimal value function with a simplex spline, a spline space has to have sufficient approximation power at each point of the value function domain. While it is theoretically possible to have an infinite refinement in terms of triangulation, the idea behind the parametrization of the value function is that there is no need for an infinite amount of states, but only a limited amount of parameters to describe the entire state space X. With no a-priori information available, a triangulation consisting of nodes positioned in a grid is a good initial estimate, as it evenly distributes the approximation power over the domain according to Eq. 20. What remains is to construct a spline space Srd(T ) are the polynomial degree and continuity order, parameters with a global effect on the spline function. Finally, to start the procedure, only the initial coefficients and covariance matrix have to be constructed. There are additional settings required for exploration (included in the policy) and the adaptability (to be discussed in the next section). Note that there is an efficient method to derive the directional derivatives, which are used in the optimal policy, available in de Visser et al. [2011].\nWhile the framework functions in an infinite-time setting, the simulation has a time limit after which a new trial is started. This to have a successful convergence to the optimal value function by visiting every state due to a random initial state x0. At each time step an action is selected, the next state is determined, the reward is calculated and the RLS TD algorithm updates the parameter coefficients c (Eq. 16) and covariance matrix P (Eq. 21). An overview of the complete algorithm is available in Table 3.\nThe requirement of each state x \u2208 X being visited is essential to guarantee the convergence to the optimal coefficients c\u2217. Therefore an explorer is introduced into the framework that explores the entire state-space X. Note that this proof assumes that the entire state-space is reachable, which is not true for every environment.\nIn this situation, the SDP framework consists of a direct implementation of the RLS TD algorithm, using multivariate simplex B-splines as a linear-in-theparameters function approximator. As a consequence, the convergence proof as given in Bradtke and Barto [1996] applies.\nA block diagram of the control scheme is presented in Figure 2. Both the state x and input u are in the diagram, as well as the reward r and the system disturbance w. The DP components are the Policy, Reward and Value, however where both the Policy and Reward components are identical for NDP and SDP, the Value component differs. The method to construct the value function in the\nSDP framework is described in section 2.2."}, {"heading": "4.2 Recursive weighted least squares", "text": "While the objective function in Eq. 4 will converge to the classic LS solution, it is unable to cope with time-varying systems as it weighs each measurement equally, driving the P matrix to zero. For the RLS algorithm to track timevarying systems, a popular and effective solution in adaptive control is using the forget factor, changing the quadratic objective function to:\nJt = 1\nt t\u2211 k=1 \u03b2t\u2212k [ek] 2\n(23)\nwhere \u03b2 represents the forget factor. This equation can be rewritten as:\nJt = 1\nt\n[ \u03b2Jt\u22121 + [et]2 ] (24)\nmaking it clear that that \u03b2 has a discounting effect on the past errors, reducing the importance given to old data. Therefore, by applying a forget factor the LS solution is converted to a Weighted Least-Squares (WLS) solution, where the newest data-points have the most influence on parameters. According to Wellstead and Zarrop [1991] the forget factor can be applied to the covariance matrix as:\nPt+1 = \u03b2 \u22121 Pt (25)\nApplying the forget factor in this form has the disadvantage that it scales all elements of P equally. This will result in covariance wind-up when no new information is available over a long period, caused by certain elements of P becoming very large. This is a direct consequence of the spatial influence of the B-coefficients, visible in the B-net seen in Figure 1. The forget method perceived in Eq. 25 is therefore only used at initialization, represented by \u03b21 seen in Eq. 21.\nA solution to prevent the covariance wind-up is to apply the forget factor only to the updated parameters. This approach is called directional forgetting Wellstead and Zarrop [1991] and updates the covariance matrix as follows:\nPt+1 = Pt + \u03b22 BtB > t (26)\nwhere \u03b22 represents a forget factor applied to the updated parameters. However, the problem with this approach is that it destroys the continuity by ignoring the constraints set by H in Eq. 19. Therefore, in order to keep c in the null-space of H, the following update is proposed:\nPt+1 = Pt + \u03b22 [ ZBtB > t Z ]\n(27)\nwhere Z = (I\u2212H+H), which is the projection on the null-space of H, introduced in Eq. 21.\nThis approach can be implemented by altering step (2) in the original RLS algorithm from Table 2 to:\nPt+1 = Pt \u2212 PtBtB\n> t Pt\n1 + B>t PtBt + \u03b22\n[ ZBtB > t Z ]\n(28)\nresulting in our new formulation for the covariance matrix update step. The modified RLS TD algorithm with the additional term in step (2) is visible in Table 4, including the computational complexity. To immediately apply the forget factor at time t, step (3) employs the Pt+1 matrix, as done in Ljung and So\u0308derstro\u0308m [1983].\nThe RLS algorithm with directional forgetting is simply convergent for a system where the data generation mechanism is deterministic Bittanti et al. [1990]. It should be noted that under this assumption, LMS algorithms also have proven convergence Tsitsiklis and Van Roy [1997].\nAdditionally, the modified RLS TD algorithm is capable of filtering out the residual noise to end up near the optimal coefficients c\u2217. With \u03b22 = 0, the filter has an infinite window in time, while at \u03b22 > 0, the window is infinite no longer, which has the advantage of being able to track time-varying systems and disadvantage of being susceptible to noise. This trade-off between noise filtering and tracking is an often returning phenomenon in adaptive control Wellstead and Zarrop [1991]. In principle \u03b22 > 0 only has a beneficial effect on the control of a time-varying system. Luckily, this approach allows the use of a variable \u03b22, able to increase and decrease as desired. The design of a successful variable forget factor will result in both good tracking behavior and a good performance with residual noise."}, {"heading": "5 Performance Evaluation of SDP", "text": "The proposed SDP framework has been implemented on a pendulum swingup non-linear control problem, as seen in A. At the start, the controller has\nno knowledge about the optimal value function, and has to learn from online measurements. The gain input of the plant is assumed to be known; it will increase the learning time for both algorithms with the same amount if it is to be identified using model identification. The objective of the experiment is to move and keep the pendulum in an upwards position by using a limited torque. The controller receives reinforcement at each state, where the top position is most beneficial. The system dynamics are simulated using an Euler integration scheme in combination with the equations of motion as presented in A.\nThe performance of each trial is measured by the maximum amount of time tup the pendulum is consecutively kept in an upwards position, where the upwards position is defined as:\n|\u03b8up| < \u03c0\n4 = 45\u25e6 (29)\nA trial itself consists of 20 s, with time steps of 0.02 s. As each trial is initialized in a random angle \u03b8 and a zero angle rate \u03b8\u0307 (consistent with the experiment in Morimoto and Doya [2005]), some trials require more swings to reach the top. Therefore a lower tup does not mean a worse performance per se, but it may have been initialized in a lower position. However, while \u03b80 is random, it is identical for each trial over the different methods. This is done to remove the chance of one method having better initializations than the other, degrading the quality of the comparison.\nThe neural networks used for NDP are constructed using either radial basis function or using a sigmoid function as a basis. This corresponds to the radial basis network and feedforward network respectively. In case of the radial basis network, the TD(\u03bb) algorithm is used for training to increase the performance, while the feedforward network is trained using the gradient descent approach. More information on how these networks are constructed and trained is available in Bertsekas and Tsitsiklis [1996] and Rojas [1996].\nFor SDP, a 4th degree spline space with 1st order continuity, without (\u03b22 = 0) and with (\u03b22 = 0.4) forget factor (see Eq. 28) has been selected. The polynomial degree has been determined by trial and error, such that the simplex spline is capable of estimating the optimal value function. Because the optimal policy is based on the first derivative and u has no rate restrictions, a discontinuous first derivative would give an unfair advantage to SDP. Therefore the continuity degree has been chosen such that the first derivative is continuous, identical to NDP. The value of \u03b22 is selected such that an increase of tracking behavior is witnessed in the experiments. A type III Delaunay triangulation of nodes positioned in a grid is used to produce the triangulation T32 seen in Figure 3.\nSDP\nParameter Srd \u03b21 \u03b22 Triangulation Total parameters Value S14 10 0 / 0.4 Type III T32 480\nAs explained in section 4.1, it is essential that this triangulation is capable of approximating the optimal value function.\nThe parameters of NDP used in the simulation have been selected such that there is a comparable amount of parameters in each function approximator; the S14(T32) has a total of 480 coefficients, the feedforward network has 480 weights, and the 12x12 radial basis network has 432 weights. The centers of the radial basis network are positioned in a grid, including one centered in x = [0 0]>. A result of the continuity constraints is that the number of free parameters is less than the total amount of parameters, thus effectively lowering the approximation power. In this case, there are 151 free parameters as the rank of H is 329.\nA search for the best set of learning parameters for the feedforward and radial basis network was performed in an attempt to have a strong comparison between NDP and SDP. An overview of the parameters used in the experiments is visible in Table 5. The initialization of network weights, center weights or nodes has a significant impact; for the feedforward network and simplex splines half of the initializations failed when the networks weights or nodes were selected randomly, while for the radial basis network only 4% failed. In this case, success is described as scoring a tup > 10 s for at least one trial. In the experiment the center weights and nodes were defined a-priori, removing the dependency on the initialization.\nFirst, in section 5.1 the four control methods are tasked with controlling a stochastic system, which will demonstrate the influence of system noise. Secondly, in section 5.2 the methods are tasked with controlling a time-varying system; this is meant to exhibit the adaptability of the control methods."}, {"heading": "5.1 Experiment I - The Stochastic System", "text": "In the first experiment the four methods are tasked with controlling a stochastic system. The stochastic system has a system disturbance of \u03c3w = 3\n\u25e6/s2. This means that the standard deviation of the system noise is 60% of the input\u2019s influence, as the maximum influence is Tmax/ml2 = 5\u25e6/s2. Since the system noise is applied to the equation of motion, the effect of the noise is only directly connected to \u03b8\u0308. Furthermore, in order to identify the influence of system noise\non each method, the deterministic system (\u03c3w = 0 \u25e6/s2) is used as a baseline.\nThe results of the four methods are visible in the figures 4 and 6 for the deterministic system, and in the figures 5 and 7 for the stochastic system. It shows that while the learning parameters and initialized weights are identical for both systems, the NDP radial basis has learned to swing the pendulum up in less trials for the stochastic system. This increased learning rate of a dynamic programming algorithm in a stochastic system is a well known phenomenon, and is a result of the extra exploration that occurs due to the system noise Bertsekas and Tsitsiklis [1996]. Nevertheless the learning rate of SDP remains the highest in both the deterministic and stochastic system.\nAnother observation is that the stability of NDP is effected most by the presence of system noise, visible by an overall decrease of tup. For SDP, only SDP - \u03b22 = 0.4 shows a decrease of tup larger than 5 s in trial 36 and 68, thus it can be concluded that SDP shows most resilience to system noise. To support this claim with numerical arguments both the mean and standard deviation of tup of the 100 trials are presented in Table 6. While it does not reveal the decrease in stability of the NDP methods because it is masked by the increased learning rate, it does show that SDP outperforms NDP in both scenarios."}, {"heading": "5.2 Experiment II - The Time-Varying System", "text": "The second experiment involves the control of a time-varying system. To simulate this, the control system has first been allowed to converge to the optimal value function by executing 1000 learning trials. The pendulum\u2019s mass is then changed from m = 1 kg to m = 1.5 kg and then 100 trials are simulated, similar to the first experiment.\nThe tup of 100 trials after the change are visible in figures 8 and 9 for the NDP and SDP methods respectively. By increasing the pendulum\u2019s mass by 50% the old control system does not stop working, in fact, in many cases the top can still be reached, albeit not as close to \u03b8 = 0 as before the change (i.e. the pendulum is held stationary at a slight angle). The changed optimal value function introduces a TD-error which propagates through the network. For the feedforward network a minor adaptation of the parameters is sufficient to adapt the global shape of the estimated value function, allowing the NDP - feedforward to continue controlling the altered system without temporary decreased performance. This is in contrast to NDP - radial basis, which does produce a decrease in performance as the TD-error propagates through the estimated value function. SDP - \u03b22 = 0 gives each data-point an equal weight, it is slow in adapting itself to a new situation, spending a long period in the transition phase where performance is reduced. SDP - \u03b22 = 0.4 has a shorter transition phase and has adapted itself to the new system before the 50th trial.\nIn Table 7, the mean and standard deviation of tup of the 100 trials are presented. This quantification identifies NDP - feedforward as the method with the best performance, and SDP - \u03b22 = 0.4 as the method with the second best performance. As stated before, the performance of the feedforward network is a direct consequence of its ability to generalize. However, there is no guarantee of convergence. Furthermore, it identifies NDP - radial basis and SDP - \u03b22 = 0 as equally bad, however in the figure 8 it can be seen that NDP - radial basis has recovered from the system change in the last 20 trials. This indicates that NDP - radial basis is capable of recovering although it takes more trials than the other methods."}, {"heading": "6 Discussion", "text": "The most important difference between neural networks and simplex splines for a DP framework, is that neural networks are non-linear in the parameters, while simplex splines are linear-in-the-parameters. Using a linear-in-the-parameters function approximator allows for the use of the RLS TD algorithm, which has a fast and proven convergence in a stochastic framework Bradtke and Barto [1996]. As a result, the SDP framework without forget factor (\u03b22 = 0) has proven con-\nvergence demonstrates stable performance when learning online. Nevertheless, in certain circumstances it is beneficial to trade these properties for adaptability, which is done by using the modified RLS TD algorithm from Table 4 and thus introducing a forget factor (\u03b22 > 0). These circumstances arise when the environment is susceptible to system parameter changes and quick adaptation is required.\nTo successfully implement the SDP framework, it is important to construct the proper spline space. Because, even though SDP has proven convergence to the best fit, there is no guarantee of system performance. To obtain this guarantee, the optimal value function, or its shape, must be known a-priori. In practice this will mean that either an off-line simulation is used, or the system is tested to see if the desired performance is reached. Another option is to treat all unknown parameters as an additional optimization, and solve the entire optimization problem using Intersplines de Visser et al. [2012]. Unfortunately, at the moment Intersplines are limited to two-dimensional inputs, and require too much calculation power to make it attractive for real-time applications.\nWhile it is possible to use the multivariate simplex B-splines in higher dimensions de Boor [1986], there are two problems that arise. First there is the construction of the triangulation, which is not automated and is already tedious in a dimension N = 3. Secondly, due to the \u201ccurse of dimensionality\u201d Bellman [1957], the computational costs of dynamic programming are very high when moving to higher dimensions. The effects of this curse are reduced by selecting a triangulation that has fewer simplices, but retains the ability to represent the optimal value function. However, the first argument was that constructing a triangulation becomes increasingly difficult at higher dimensions and is not yet automated. This creates difficulties when applying the SDP framework to a problem with the dimension N > 2, since a similar grid-approach as used in the simulation, produces high computation costs.\nA final remark on the methods used in the experiments. The four methods were considered to be comparable in terms of the number of parameters. However, the continuity constraints reduced the amount of free parameters in the SDP methods; thereby reducing the approximation power of the splines relative to the neural networks. Experiments with NDP have shown that reducing the amount of parameters reduces the performance, which implies that SDP is potentially better than concluded in the experiments."}, {"heading": "7 Conclusion", "text": "In this paper the SDP framework was introduced; a combination of the RLS TD algorithm and multivariate simplex B-splines. It was shown that it is capable of solving the non-linear control problem of the pendulum swing-up with nothing but a reward function as feedback. This was done in significantly less trials than NDP systems supported by function approximators with a comparable amount of parameters. In addition, SDP presented a greater resilience to system noise than NDP, demonstrating no decrease in performance in the presence of a disturbance. Furthermore, a forget method that preserves the continuity constraints is introduced, which is merged with the RLS TD algorithm to create an adaptive control system. In conclusion it can be said that the high convergence rate of the RLS TD algorithm, in combination with the high\napproximation power of the multivariate simplex B-splines, provides a basis for high performance non-linear control at the cost of a higher computational load requirement. By using the modified RLS TD algorithm, it is even possible to keep track of time-varying systems, resulting in an adaptive control method for non-linear systems.\nIn order to have a good trade-off between noise filtering and adaptability, it is important to design a forget factor that is capable of detecting a parameter change in the controlled system. There is extensive literature available on this subject and it is recommended to investigate which is the most effective in the SDP framework.\nThe unsolved issue of multivariate simplex B-splines remains the search for the optimal triangulation. While the assumed static triangulation performs adequately, as seen in the experiments, much can be gained in terms of computational complexity and performance by optimizing the triangulation. The advantage of finding the optimal triangluation grows exponentially, as it is connected to the \u201cCurse of Dimensionality\u201d."}, {"heading": "A Pendulum Swing-Up Task", "text": "The dynamics of the non-linear control task determined by:\nml2\u03b8\u0308 = \u2212\u00b5\u03b8\u0307 +mgl sin \u03b8 + T (30)\nwhere \u03b8 is the angle from upright position, T is the limited input torque, \u00b5 = 0.01 is the friction coefficient, m = 1.0kg is the point mass at the tip of the rod, l = 1.0m is the length of the pendulum, and g = 9.8m/s2 is the gravity acceleration. A schematic overview is visible in Fig. 10.\nThe state vector is defined as x = [\u03b8 \u03b8\u0307]> and the action as u = T . The complete system description is now:\nx\u0307 = f(x,u) =\n( \u03b8\u0307\ng l sin \u03b8 \u2212 \u00b5 ml2 \u03b8\u0307\n) + ( 0 1 ml2 ) u + ( 0 1 ) w (31)\nwhere w represents white noise with a standard deviation of \u03c3w. The reward\nfunction of the system is: r(x,u) = cx(cos \u03b8 \u2212 1) + cu \u222b T Tmax\n0\ntan( \u03c0\n2 s)ds (32)\nwhere cx = 1, cu = 0.1 and T max = 5. As determined in section 2.3, the optimal control law is:\nu = Tmax tanh( \u03c0\n2\n1 cj \u03c4 \u2202V \u2217 \u2202x \u2202f(x,u) \u2202u + n) (33)\nwhere n represents white noise with a standard deviation of \u03c3n = 0.01, used as an explorer."}], "references": [{"title": "Robust Reinforcement Learning Control Using Integral Quadratic Constrains for Recurrent Neural Networks", "author": ["C.W. Anderson", "P.M. Young", "M.R. Buehner", "J.N. Knight", "K.A. Bush", "D.C. Hittle"], "venue": "IEEE Transaction on neural networks,", "citeRegEx": "Anderson et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Anderson et al\\.", "year": 2007}, {"title": "Parallel MARS algorithm based on B-splines", "author": ["S. Bakin", "M. Hegland", "M. Osborne"], "venue": "Computational Statistics,", "citeRegEx": "Bakin et al\\.,? \\Q2000\\E", "shortCiteRegEx": "Bakin et al\\.", "year": 2000}, {"title": "Neuronlike adaptive elements that can solve difficult learning control problems", "author": ["A.G. Barto", "R.S. Sutton", "C.W. Anderson"], "venue": "IEEE transactions on systems, man and cybernetics,", "citeRegEx": "Barto et al\\.,? \\Q1983\\E", "shortCiteRegEx": "Barto et al\\.", "year": 1983}, {"title": "Dynamic Programming", "author": ["R. Bellman"], "venue": null, "citeRegEx": "Bellman.,? \\Q1957\\E", "shortCiteRegEx": "Bellman.", "year": 1957}, {"title": "Polynomial Approximation - A New Computional Technique in Dynamic Programming: Allocation Processes", "author": ["R. Bellman", "R. Kalaba", "B. Kotkin"], "venue": "Mathematics of Computation,", "citeRegEx": "Bellman et al\\.,? \\Q1963\\E", "shortCiteRegEx": "Bellman et al\\.", "year": 1963}, {"title": "Dynamic Programming and Optimal Control, volume 2", "author": ["D.P. Bertsekas"], "venue": "Athena Scientific,", "citeRegEx": "Bertsekas.,? \\Q2007\\E", "shortCiteRegEx": "Bertsekas.", "year": 2007}, {"title": "Neuro-dynamic programming", "author": ["D.P. Bertsekas", "J.N. Tsitsiklis"], "venue": "Athena Scientific,", "citeRegEx": "Bertsekas and Tsitsiklis.,? \\Q1996\\E", "shortCiteRegEx": "Bertsekas and Tsitsiklis.", "year": 1996}, {"title": "Convergence and exponential convergence of identification algorithms with directional forgetting factor", "author": ["S. Bittanti", "P. Bolzern", "M. Campi"], "venue": null, "citeRegEx": "Bittanti et al\\.,? \\Q1990\\E", "shortCiteRegEx": "Bittanti et al\\.", "year": 1990}, {"title": "Linear Least-Squares Algorithms for Temporal Difference Learning", "author": ["S.J. Bradtke", "A.G. Barto"], "venue": "Machine Learning,", "citeRegEx": "Bradtke and Barto.,? \\Q1996\\E", "shortCiteRegEx": "Bradtke and Barto.", "year": 1996}, {"title": "Reinforcement Learning and Dynamic Programming Using Function Approximators", "author": ["L. Busoniu", "R. Babuska", "B. De Schutter", "D. Ernst"], "venue": "CRC Press,", "citeRegEx": "Busoniu et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Busoniu et al\\.", "year": 2010}, {"title": "Neural network and regression spline value function approximations for stochastic dynamic programming", "author": ["C. Cervellera", "A. Wen", "V. Chen"], "venue": "Computers and Operations Research,", "citeRegEx": "Cervellera et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Cervellera et al\\.", "year": 2007}, {"title": "Application of orthogonal arrays and mars to inventory forecasting stochastic dynamic programs", "author": ["V. Chen"], "venue": "Computational Statistics and Data Analysis,", "citeRegEx": "Chen.,? \\Q1999\\E", "shortCiteRegEx": "Chen.", "year": 1999}, {"title": "Applying experimental design and regression splines to high-dimensional continuous-state stochastic dynamic programming", "author": ["V. Chen", "D. Ruppert", "C. Shoemaker"], "venue": "Operations Research,", "citeRegEx": "Chen et al\\.,? \\Q1999\\E", "shortCiteRegEx": "Chen et al\\.", "year": 1999}, {"title": "Splines and Efficiency in Dynamic Programming", "author": ["J.W. Daniel"], "venue": "Mathematical Analysis and Applications,", "citeRegEx": "Daniel.,? \\Q1976\\E", "shortCiteRegEx": "Daniel.", "year": 1976}, {"title": "B (asic)-spline basics", "author": ["C. de Boor"], "venue": "Mathematics Research Center, University of Wisconsin-Madison,", "citeRegEx": "Boor.,? \\Q1986\\E", "shortCiteRegEx": "Boor.", "year": 1986}, {"title": "Wavefront reconstruction in adaptive optics systems using nonlinear multivariate splines", "author": ["C.C. de Visser", "M. Verhaegen"], "venue": "JOSA A,", "citeRegEx": "Visser and Verhaegen.,? \\Q2013\\E", "shortCiteRegEx": "Visser and Verhaegen.", "year": 2013}, {"title": "A new approach to linear regression with multivariate splines", "author": ["C.C. de Visser", "Q.P. Chu", "J.A. Mulder"], "venue": null, "citeRegEx": "Visser et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Visser et al\\.", "year": 2009}, {"title": "Differential constraints for bounded recursive identification with multivariate splines", "author": ["C.C. de Visser", "Q.P. Chu", "J.A. Mulder"], "venue": null, "citeRegEx": "Visser et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Visser et al\\.", "year": 2011}, {"title": "Intersplines: A New Approach to Globally Optimal Multivariate Splines Using Interval Analysis", "author": ["C.C. de Visser", "E. van Kampen", "Q.P. Chu", "J.A. Mulder"], "venue": "Reliable Computing,", "citeRegEx": "Visser et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Visser et al\\.", "year": 2012}, {"title": "Temporal Difference Learning in Continuous Time and Space", "author": ["K. Doya"], "venue": "Advances in Neural Information Processing Systems,", "citeRegEx": "Doya.,? \\Q1996\\E", "shortCiteRegEx": "Doya.", "year": 1996}, {"title": "Reinforcement Learning in Continuous Time and Space", "author": ["K. Doya"], "venue": "Neural Computation,", "citeRegEx": "Doya.,? \\Q2000\\E", "shortCiteRegEx": "Doya.", "year": 2000}, {"title": "Online Adaptive Critic Flight Control", "author": ["S. Ferrari", "R.F. Stengel"], "venue": "Journal of Guidance, Control and Dynamics,", "citeRegEx": "Ferrari and Stengel.,? \\Q2004\\E", "shortCiteRegEx": "Ferrari and Stengel.", "year": 2004}, {"title": "Multivariate Adaptive Regression Splines", "author": ["J.H. Friedman"], "venue": "The Annals of Statistics,", "citeRegEx": "Friedman.,? \\Q1991\\E", "shortCiteRegEx": "Friedman.", "year": 1991}, {"title": "Adaptive Critic Based Neural Networks for Control-Constrained Agile Missile Control", "author": ["D. Han", "S.N. Balakrishnan"], "venue": "Proceedings of the American Control Conference,", "citeRegEx": "Han and Balakrishnan.,? \\Q1999\\E", "shortCiteRegEx": "Han and Balakrishnan.", "year": 1999}, {"title": "Spline functions on triangulations", "author": ["M.J. Lai", "L.L. Schumaker"], "venue": null, "citeRegEx": "Lai and Schumaker.,? \\Q2007\\E", "shortCiteRegEx": "Lai and Schumaker.", "year": 2007}, {"title": "Solving least squares problems", "author": ["C.L. Lawson", "R.J. Hanson"], "venue": null, "citeRegEx": "Lawson and Hanson.,? \\Q1974\\E", "shortCiteRegEx": "Lawson and Hanson.", "year": 1974}, {"title": "Two algorithms for constructing a delaunay triangulation", "author": ["D.T. Lee", "B.J. Schachter"], "venue": "International Journal of Computer and Information Sciences,", "citeRegEx": "Lee and Schachter.,? \\Q1980\\E", "shortCiteRegEx": "Lee and Schachter.", "year": 1980}, {"title": "Theory and practice of recursive identification", "author": ["L. Ljung", "T. S\u00f6derstr\u00f6m"], "venue": null, "citeRegEx": "Ljung and S\u00f6derstr\u00f6m.,? \\Q1983\\E", "shortCiteRegEx": "Ljung and S\u00f6derstr\u00f6m.", "year": 1983}, {"title": "A convergent recursive least squares approximate policy iteration algorithm for multi-dimensional markov decision process with continuous state and action spaces", "author": ["J. Ma", "W.B. Powell"], "venue": "IEEE symposium on adaptive dynamic programming and reinforcement learning,", "citeRegEx": "Ma and Powell.,? \\Q2009\\E", "shortCiteRegEx": "Ma and Powell.", "year": 2009}, {"title": "Robust Reinforcement Learning", "author": ["J. Morimoto", "K. Doya"], "venue": "Neural Computation,", "citeRegEx": "Morimoto and Doya.,? \\Q2005\\E", "shortCiteRegEx": "Morimoto and Doya.", "year": 2005}, {"title": "Approximate Dynamic Programming", "author": ["W.B. Powell"], "venue": null, "citeRegEx": "Powell.,? \\Q2007\\E", "shortCiteRegEx": "Powell.", "year": 2007}, {"title": "Adaptive Critic Based Neurocontroller for Autolanding of Aircraft", "author": ["G. Saini", "S.N. Balakrishnan"], "venue": "Proceedings of the American Control conference,", "citeRegEx": "Saini and Balakrishnan.,? \\Q1997\\E", "shortCiteRegEx": "Saini and Balakrishnan.", "year": 1997}, {"title": "Generalized Polynomial Approximations in Markovian Decision Processes", "author": ["P.J. Schweitzer"], "venue": "Journal of Mathematical Analysis and Applications,", "citeRegEx": "Schweitzer.,? \\Q1985\\E", "shortCiteRegEx": "Schweitzer.", "year": 1985}, {"title": "Handbook of Learning and Approximate Dynamic Programming", "author": ["J. Si", "A. Barto", "W. Powell", "D. Wunsch"], "venue": "IEEE Press,", "citeRegEx": "Si et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Si et al\\.", "year": 2004}, {"title": "Approximate dynamic programming via sum of squares programming", "author": ["T.H. Summers", "K. Kunz", "N. Kariotoglou", "M. Kamgarpour", "S. Summers", "J. Lygeros"], "venue": "In European Control Conference (ECC),", "citeRegEx": "Summers et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Summers et al\\.", "year": 2013}, {"title": "Learning to Predict by the Methods of Temporal Differences", "author": ["R.S. Sutton"], "venue": "Machine Learning,", "citeRegEx": "Sutton.,? \\Q1988\\E", "shortCiteRegEx": "Sutton.", "year": 1988}, {"title": "Reinforcement learning: an introduction", "author": ["R.S. Sutton", "A.G. Barto"], "venue": null, "citeRegEx": "Sutton and Barto.,? \\Q1998\\E", "shortCiteRegEx": "Sutton and Barto.", "year": 1998}, {"title": "An Analysis of Temporal-Difference Learning with Function Approximation", "author": ["J.N. Tsitsiklis", "B. Van Roy"], "venue": "IEEE transactions on automatic control,", "citeRegEx": "Tsitsiklis and Roy.,? \\Q1997\\E", "shortCiteRegEx": "Tsitsiklis and Roy.", "year": 1997}, {"title": "Self-tuning Systems: Control and Signal Processing", "author": ["P.E. Wellstead", "M.B. Zarrop"], "venue": null, "citeRegEx": "Wellstead and Zarrop.,? \\Q1991\\E", "shortCiteRegEx": "Wellstead and Zarrop.", "year": 1991}], "referenceMentions": [{"referenceID": 23, "context": "These adaptive controllers have already been successfully trained off-line for many purposes, ranging from agile missile interception [Han and Balakrishnan, 1999] to aircraft auto-landing and control [Saini and Balakrishnan, 1997].", "startOffset": 134, "endOffset": 162}, {"referenceID": 31, "context": "These adaptive controllers have already been successfully trained off-line for many purposes, ranging from agile missile interception [Han and Balakrishnan, 1999] to aircraft auto-landing and control [Saini and Balakrishnan, 1997].", "startOffset": 200, "endOffset": 230}, {"referenceID": 21, "context": "an on-line adaptive critic flight control was implemented on a six-degree-offreedom business jet aircraft over its full operating envelope, improving its performance when unexpected conditions are encountered for the first time [Ferrari and Stengel, 2004].", "startOffset": 228, "endOffset": 255}, {"referenceID": 2, "context": "Barto, Sutton and Anderson used neural networks to parametrise the value function [Barto et al., 1983].", "startOffset": 82, "endOffset": 102}, {"referenceID": 6, "context": "This combination of neural networks and DP is commonly referred to as Neuro Dynamic Programming (NDP) [Bertsekas and Tsitsiklis, 1996].", "startOffset": 102, "endOffset": 134}, {"referenceID": 8, "context": "With the Recursive Least Squares Temporal Difference (RLS TD) algorithm convergence in a stochastic framework is assured [Bradtke and Barto, 1996], even when the linear regression basis cannot perfectly fit the value function.", "startOffset": 121, "endOffset": 146}, {"referenceID": 28, "context": "More recently, the proof of convergence has been extended to include the optimal policy [Ma and Powell, 2009], however one of the problems that remains is the a-priori unknown shape of the value function.", "startOffset": 88, "endOffset": 109}, {"referenceID": 4, "context": "Using polynomials as a function approximator in a DP framework was investigated by Bellman in 1963 [Bellman et al., 1963].", "startOffset": 99, "endOffset": 121}, {"referenceID": 24, "context": "Furthermore, the spatial location of the B-coefficients and modularity of the triangulation allow for local model modification and refinement [Lai and Schumaker, 2007].", "startOffset": 142, "endOffset": 167}, {"referenceID": 0, "context": "More recently, an adaptive controller has been introduced in Anderson et al. [2007], and again the neural network weights are bounded to guarantee stability.", "startOffset": 61, "endOffset": 84}, {"referenceID": 0, "context": "More recently, an adaptive controller has been introduced in Anderson et al. [2007], and again the neural network weights are bounded to guarantee stability. A drawback is that for a time-varying system these bounds shift and stability can no longer be guaranteed. This combination of neural networks and DP is commonly referred to as Neuro Dynamic Programming (NDP) [Bertsekas and Tsitsiklis, 1996]. There is a proof of convergence when linear-in-the-parameters function approximators are used in Tsitsiklis and Van Roy [1997], however, this proof demands knowledge of the shape of the optimal value function.", "startOffset": 61, "endOffset": 528}, {"referenceID": 0, "context": "More recently, an adaptive controller has been introduced in Anderson et al. [2007], and again the neural network weights are bounded to guarantee stability. A drawback is that for a time-varying system these bounds shift and stability can no longer be guaranteed. This combination of neural networks and DP is commonly referred to as Neuro Dynamic Programming (NDP) [Bertsekas and Tsitsiklis, 1996]. There is a proof of convergence when linear-in-the-parameters function approximators are used in Tsitsiklis and Van Roy [1997], however, this proof demands knowledge of the shape of the optimal value function. With the Recursive Least Squares Temporal Difference (RLS TD) algorithm convergence in a stochastic framework is assured [Bradtke and Barto, 1996], even when the linear regression basis cannot perfectly fit the value function. In the last decade, the DP theory in continuous time and space has been further developed in Doya [1996, 2000], Morimoto and Doya [2005]. More recently, the proof of convergence has been extended to include the optimal policy [Ma and Powell, 2009], however one of the problems that remains is the a-priori unknown shape of the value function.", "startOffset": 61, "endOffset": 975}, {"referenceID": 0, "context": "More recently, an adaptive controller has been introduced in Anderson et al. [2007], and again the neural network weights are bounded to guarantee stability. A drawback is that for a time-varying system these bounds shift and stability can no longer be guaranteed. This combination of neural networks and DP is commonly referred to as Neuro Dynamic Programming (NDP) [Bertsekas and Tsitsiklis, 1996]. There is a proof of convergence when linear-in-the-parameters function approximators are used in Tsitsiklis and Van Roy [1997], however, this proof demands knowledge of the shape of the optimal value function. With the Recursive Least Squares Temporal Difference (RLS TD) algorithm convergence in a stochastic framework is assured [Bradtke and Barto, 1996], even when the linear regression basis cannot perfectly fit the value function. In the last decade, the DP theory in continuous time and space has been further developed in Doya [1996, 2000], Morimoto and Doya [2005]. More recently, the proof of convergence has been extended to include the optimal policy [Ma and Powell, 2009], however one of the problems that remains is the a-priori unknown shape of the value function. Using polynomials as a function approximator in a DP framework was investigated by Bellman in 1963 [Bellman et al., 1963]. In 1985, Schweitzer [1985] concentrates on the use of global polynomials in combination with DP.", "startOffset": 61, "endOffset": 1331}, {"referenceID": 0, "context": "More recently, an adaptive controller has been introduced in Anderson et al. [2007], and again the neural network weights are bounded to guarantee stability. A drawback is that for a time-varying system these bounds shift and stability can no longer be guaranteed. This combination of neural networks and DP is commonly referred to as Neuro Dynamic Programming (NDP) [Bertsekas and Tsitsiklis, 1996]. There is a proof of convergence when linear-in-the-parameters function approximators are used in Tsitsiklis and Van Roy [1997], however, this proof demands knowledge of the shape of the optimal value function. With the Recursive Least Squares Temporal Difference (RLS TD) algorithm convergence in a stochastic framework is assured [Bradtke and Barto, 1996], even when the linear regression basis cannot perfectly fit the value function. In the last decade, the DP theory in continuous time and space has been further developed in Doya [1996, 2000], Morimoto and Doya [2005]. More recently, the proof of convergence has been extended to include the optimal policy [Ma and Powell, 2009], however one of the problems that remains is the a-priori unknown shape of the value function. Using polynomials as a function approximator in a DP framework was investigated by Bellman in 1963 [Bellman et al., 1963]. In 1985, Schweitzer [1985] concentrates on the use of global polynomials in combination with DP. With the development of the RLS TD algorithm, it is possible to obtain a proven convergence by combining it with a polynomial approximation as discussed in Ma and Powell [2009]. However the limitation is that the approximation power of global polynomials can only be increased by increasing the order of the polynomials, which will also lead to numerical instabilities in the solution schemes of the approximation.", "startOffset": 61, "endOffset": 1578}, {"referenceID": 0, "context": "More recently, an adaptive controller has been introduced in Anderson et al. [2007], and again the neural network weights are bounded to guarantee stability. A drawback is that for a time-varying system these bounds shift and stability can no longer be guaranteed. This combination of neural networks and DP is commonly referred to as Neuro Dynamic Programming (NDP) [Bertsekas and Tsitsiklis, 1996]. There is a proof of convergence when linear-in-the-parameters function approximators are used in Tsitsiklis and Van Roy [1997], however, this proof demands knowledge of the shape of the optimal value function. With the Recursive Least Squares Temporal Difference (RLS TD) algorithm convergence in a stochastic framework is assured [Bradtke and Barto, 1996], even when the linear regression basis cannot perfectly fit the value function. In the last decade, the DP theory in continuous time and space has been further developed in Doya [1996, 2000], Morimoto and Doya [2005]. More recently, the proof of convergence has been extended to include the optimal policy [Ma and Powell, 2009], however one of the problems that remains is the a-priori unknown shape of the value function. Using polynomials as a function approximator in a DP framework was investigated by Bellman in 1963 [Bellman et al., 1963]. In 1985, Schweitzer [1985] concentrates on the use of global polynomials in combination with DP. With the development of the RLS TD algorithm, it is possible to obtain a proven convergence by combining it with a polynomial approximation as discussed in Ma and Powell [2009]. However the limitation is that the approximation power of global polynomials can only be increased by increasing the order of the polynomials, which will also lead to numerical instabilities in the solution schemes of the approximation. According to Summers et al. [2013], using the sum of squares allows the use of higher order polynomials, but will eventually still lead to numerical instability.", "startOffset": 61, "endOffset": 1851}, {"referenceID": 0, "context": "More recently, an adaptive controller has been introduced in Anderson et al. [2007], and again the neural network weights are bounded to guarantee stability. A drawback is that for a time-varying system these bounds shift and stability can no longer be guaranteed. This combination of neural networks and DP is commonly referred to as Neuro Dynamic Programming (NDP) [Bertsekas and Tsitsiklis, 1996]. There is a proof of convergence when linear-in-the-parameters function approximators are used in Tsitsiklis and Van Roy [1997], however, this proof demands knowledge of the shape of the optimal value function. With the Recursive Least Squares Temporal Difference (RLS TD) algorithm convergence in a stochastic framework is assured [Bradtke and Barto, 1996], even when the linear regression basis cannot perfectly fit the value function. In the last decade, the DP theory in continuous time and space has been further developed in Doya [1996, 2000], Morimoto and Doya [2005]. More recently, the proof of convergence has been extended to include the optimal policy [Ma and Powell, 2009], however one of the problems that remains is the a-priori unknown shape of the value function. Using polynomials as a function approximator in a DP framework was investigated by Bellman in 1963 [Bellman et al., 1963]. In 1985, Schweitzer [1985] concentrates on the use of global polynomials in combination with DP. With the development of the RLS TD algorithm, it is possible to obtain a proven convergence by combining it with a polynomial approximation as discussed in Ma and Powell [2009]. However the limitation is that the approximation power of global polynomials can only be increased by increasing the order of the polynomials, which will also lead to numerical instabilities in the solution schemes of the approximation. According to Summers et al. [2013], using the sum of squares allows the use of higher order polynomials, but will eventually still lead to numerical instability. A recommendation in Ma and Powell [2009] is to use local polynomial regression to treat Markov Decision Process (MDP) problems with value functions of unknown form.", "startOffset": 61, "endOffset": 2019}, {"referenceID": 0, "context": "More recently, an adaptive controller has been introduced in Anderson et al. [2007], and again the neural network weights are bounded to guarantee stability. A drawback is that for a time-varying system these bounds shift and stability can no longer be guaranteed. This combination of neural networks and DP is commonly referred to as Neuro Dynamic Programming (NDP) [Bertsekas and Tsitsiklis, 1996]. There is a proof of convergence when linear-in-the-parameters function approximators are used in Tsitsiklis and Van Roy [1997], however, this proof demands knowledge of the shape of the optimal value function. With the Recursive Least Squares Temporal Difference (RLS TD) algorithm convergence in a stochastic framework is assured [Bradtke and Barto, 1996], even when the linear regression basis cannot perfectly fit the value function. In the last decade, the DP theory in continuous time and space has been further developed in Doya [1996, 2000], Morimoto and Doya [2005]. More recently, the proof of convergence has been extended to include the optimal policy [Ma and Powell, 2009], however one of the problems that remains is the a-priori unknown shape of the value function. Using polynomials as a function approximator in a DP framework was investigated by Bellman in 1963 [Bellman et al., 1963]. In 1985, Schweitzer [1985] concentrates on the use of global polynomials in combination with DP. With the development of the RLS TD algorithm, it is possible to obtain a proven convergence by combining it with a polynomial approximation as discussed in Ma and Powell [2009]. However the limitation is that the approximation power of global polynomials can only be increased by increasing the order of the polynomials, which will also lead to numerical instabilities in the solution schemes of the approximation. According to Summers et al. [2013], using the sum of squares allows the use of higher order polynomials, but will eventually still lead to numerical instability. A recommendation in Ma and Powell [2009] is to use local polynomial regression to treat Markov Decision Process (MDP) problems with value functions of unknown form. This recommendation is supported by Daniel [1976], which states that it is highly desirable from an efficiency point of view to use local polynomial regression.", "startOffset": 61, "endOffset": 2193}, {"referenceID": 22, "context": "There are existing approaches which combines Multivariate Adaptive Regression Splines (MARS) [Friedman, 1991] and DP [Chen, 1999, Chen et al.", "startOffset": 93, "endOffset": 109}, {"referenceID": 1, "context": "However, multivariate simplex B-splines distinguishes itself from MARS in terms of computational efficiency by using B-splines [Bakin et al., 2000].", "startOffset": 127, "endOffset": 147}, {"referenceID": 24, "context": "And they are supported by a triangulation of simplices, allowing functionality in a non-square domain [Lai and Schumaker, 2007].", "startOffset": 102, "endOffset": 127}, {"referenceID": 31, "context": "For a more complete description we refer to Sutton and Barto [1998], Si et al.", "startOffset": 44, "endOffset": 68}, {"referenceID": 30, "context": "For a more complete description we refer to Sutton and Barto [1998], Si et al. [2004], Powell [2007], Busoniu et al.", "startOffset": 69, "endOffset": 86}, {"referenceID": 28, "context": "[2004], Powell [2007], Busoniu et al.", "startOffset": 8, "endOffset": 22}, {"referenceID": 8, "context": "[2004], Powell [2007], Busoniu et al. [2010], Bertsekas [2007].", "startOffset": 23, "endOffset": 45}, {"referenceID": 5, "context": "[2010], Bertsekas [2007]. We start with a brief overview of the MDP followed by the policy evaluation and concluded with the policy improvement.", "startOffset": 8, "endOffset": 25}, {"referenceID": 35, "context": "This policy \u03c0\u2217 is called the optimal policy and can be found by applying both policy evaluation and policy improvement Sutton and Barto [1998]. The policy evaluation determines the V (xt) of the current policy \u03c0, where the policy improvement uses this knowledge to adjust the policy \u03c0 such that it ends up in the most valuable states.", "startOffset": 119, "endOffset": 143}, {"referenceID": 35, "context": "In order to evaluate the value function in an iterative fashion, Sutton [1988] uses TD-learning.", "startOffset": 65, "endOffset": 79}, {"referenceID": 35, "context": "Although RLS TD requires more computations per time-step than TD(\u03bb) algorithms [Sutton, 1988], it is more efficient in the statistical sense as more information is extracted from training experience, allowing it to converge faster [Bradtke and Barto, 1996].", "startOffset": 79, "endOffset": 93}, {"referenceID": 8, "context": "Although RLS TD requires more computations per time-step than TD(\u03bb) algorithms [Sutton, 1988], it is more efficient in the statistical sense as more information is extracted from training experience, allowing it to converge faster [Bradtke and Barto, 1996].", "startOffset": 231, "endOffset": 256}, {"referenceID": 8, "context": "In Bradtke and Barto [1996] the RLS TD algorithm was introduced; this algorithm and its computational complexity is visible in Table 1.", "startOffset": 3, "endOffset": 28}, {"referenceID": 8, "context": "Table 1: RLS TD algorithm, from Bradtke and Barto [1996] Step Action Computational Complexity (1) et = rt+1 \u2212 (Bt \u2212 \u03b3Bt+1)ct O(d\u0302) (2) Pt+1 = Pt \u2212 PtBt(Bt\u2212\u03b3Bt+1) Pt 1+(Bt\u2212\u03b3Bt+1)>PtBt O(\u00e2 ) (3) ct+1 = ct + Pt 1+(Bt\u2212\u03b3Bt+1)>PtBt Btet O(\u00e2 )", "startOffset": 32, "endOffset": 57}, {"referenceID": 36, "context": "The repetition of the policy evaluation and policy improvement is called policy iteration and will result in an optimal policy [Sutton and Barto, 1998].", "startOffset": 127, "endOffset": 151}, {"referenceID": 19, "context": "According to Doya [1996], the optimal non-linear feedback control law is a function of value function\u2019s gradient:", "startOffset": 13, "endOffset": 25}, {"referenceID": 24, "context": "For a more extensive and general introduction to multivariate spline theory we refer to Lai and Schumaker [2007]. We start by introducing the basic concept of a single basis polynomial and B-form, then introduce the triangulation, followed by the vector notation of the B-form.", "startOffset": 88, "endOffset": 113}, {"referenceID": 24, "context": "Any number of simplices can be combined into a triangulation, where a triangulation T is a special partitioning of a domain into a set of J non-overlapping simplices and is defined in Lai and Schumaker [2007] as:", "startOffset": 184, "endOffset": 209}, {"referenceID": 26, "context": "A popular triangulation method is Delaunay triangulation [Lee and Schachter, 1980].", "startOffset": 57, "endOffset": 82}, {"referenceID": 16, "context": "Recently, a new method for creating globally optimal triangulations named Intersplines was introduced in de Visser et al. [2012].", "startOffset": 108, "endOffset": 129}, {"referenceID": 24, "context": "The multi-index \u03ba has a requirement on the ordering, called a lexicographical sorting order which is introduced in Lai and Schumaker [2007]. This means \u03ba\u03bd\u03bc\u03ba comes before \u03baijk provided that \u03bd > i, or if \u03bd = i, then \u03bc > j, or if \u03bd = i and \u03bc = j, then \u03ba > k.", "startOffset": 115, "endOffset": 140}, {"referenceID": 15, "context": "Adapted from de Visser and Verhaegen [2013], we define the B-form as a row vector on simplex tj :", "startOffset": 16, "endOffset": 44}, {"referenceID": 24, "context": "We use the definition of the spline space from Lai and Schumaker [2007]: S d(T ) \u2261 {sd \u2208 C(T ) : sd|t \u2208 Pd,\u2200t \u2208 T } (17)", "startOffset": 47, "endOffset": 72}, {"referenceID": 24, "context": "The required continuity conditions can be calculated using Lai and Schumaker [2007]: ci (\u03ba0,.", "startOffset": 59, "endOffset": 84}, {"referenceID": 24, "context": "To describe the approximation power, the following definition from Lai and Schumaker [2007], chapter 10.", "startOffset": 67, "endOffset": 92}, {"referenceID": 24, "context": "The theory behind this is extensive and available in Lai and Schumaker [2007], however for now it is important to realize that |T | is a function of the longest edge in triangulation T .", "startOffset": 53, "endOffset": 78}, {"referenceID": 16, "context": "RLS is a method which allows the estimated parameters c to be updated online with the use of the parameter covariance matrix P de Visser et al. [2011]. The algorithm and the computational complexity is found in Table 2.", "startOffset": 130, "endOffset": 151}, {"referenceID": 16, "context": "RLS is a method which allows the estimated parameters c to be updated online with the use of the parameter covariance matrix P de Visser et al. [2011]. The algorithm and the computational complexity is found in Table 2. Note that it is essential to keep the column relations of P intact to enforce the constraints Hc = 0 from Eq. 19. To initialize the P matrix, we use Z, the orthogonal projector on the null-space of H Lawson and Hanson [1974]:", "startOffset": 130, "endOffset": 445}, {"referenceID": 16, "context": "For this, a constrained Least Squares (LS) fit of the estimated shape can be used, using the approach from de Visser et al. [2009]. If no knowledge is available, initialization of all coefficients at zero will satisfy the constraints, resulting in: c1 = 0 (22)", "startOffset": 110, "endOffset": 131}, {"referenceID": 16, "context": "Table 2: RLS algorithm from de Visser et al. [2011] Step Action Computational Complexity (1) t = yt \u2212Bt ct O(d\u0302) (2) Pt+1 = Pt \u2212 PtBtB > t Pt 1+Bt PtBt O(\u00e2) (3) ct+1 = ct + Pt+1Bt t O(\u00e2)", "startOffset": 31, "endOffset": 52}, {"referenceID": 15, "context": "Note that there is an efficient method to derive the directional derivatives, which are used in the optimal policy, available in de Visser et al. [2011]. While the framework functions in an infinite-time setting, the simulation has a time limit after which a new trial is started.", "startOffset": 132, "endOffset": 153}, {"referenceID": 8, "context": "As a consequence, the convergence proof as given in Bradtke and Barto [1996] applies.", "startOffset": 52, "endOffset": 77}, {"referenceID": 38, "context": "According to Wellstead and Zarrop [1991] the forget factor can be applied to the covariance matrix as: Pt+1 = \u03b2 \u22121 Pt (25)", "startOffset": 13, "endOffset": 41}, {"referenceID": 38, "context": "This approach is called directional forgetting Wellstead and Zarrop [1991] and updates the covariance matrix as follows: Pt+1 = Pt + \u03b22 BtB > t (26) where \u03b22 represents a forget factor applied to the updated parameters.", "startOffset": 47, "endOffset": 75}, {"referenceID": 26, "context": "To immediately apply the forget factor at time t, step (3) employs the Pt+1 matrix, as done in Ljung and S\u00f6derstr\u00f6m [1983]. The RLS algorithm with directional forgetting is simply convergent for a system where the data generation mechanism is deterministic Bittanti et al.", "startOffset": 95, "endOffset": 123}, {"referenceID": 7, "context": "The RLS algorithm with directional forgetting is simply convergent for a system where the data generation mechanism is deterministic Bittanti et al. [1990]. It should be noted that under this assumption, LMS algorithms also have proven convergence Tsitsiklis and Van Roy [1997].", "startOffset": 133, "endOffset": 156}, {"referenceID": 7, "context": "The RLS algorithm with directional forgetting is simply convergent for a system where the data generation mechanism is deterministic Bittanti et al. [1990]. It should be noted that under this assumption, LMS algorithms also have proven convergence Tsitsiklis and Van Roy [1997]. Additionally, the modified RLS TD algorithm is capable of filtering out the residual noise to end up near the optimal coefficients c\u2217.", "startOffset": 133, "endOffset": 278}, {"referenceID": 7, "context": "The RLS algorithm with directional forgetting is simply convergent for a system where the data generation mechanism is deterministic Bittanti et al. [1990]. It should be noted that under this assumption, LMS algorithms also have proven convergence Tsitsiklis and Van Roy [1997]. Additionally, the modified RLS TD algorithm is capable of filtering out the residual noise to end up near the optimal coefficients c\u2217. With \u03b22 = 0, the filter has an infinite window in time, while at \u03b22 > 0, the window is infinite no longer, which has the advantage of being able to track time-varying systems and disadvantage of being susceptible to noise. This trade-off between noise filtering and tracking is an often returning phenomenon in adaptive control Wellstead and Zarrop [1991]. In principle \u03b22 > 0 only has a beneficial effect on the control of a time-varying system.", "startOffset": 133, "endOffset": 770}, {"referenceID": 17, "context": "As each trial is initialized in a random angle \u03b8 and a zero angle rate \u03b8\u0307 (consistent with the experiment in Morimoto and Doya [2005]), some trials require more swings to reach the top.", "startOffset": 122, "endOffset": 134}, {"referenceID": 5, "context": "More information on how these networks are constructed and trained is available in Bertsekas and Tsitsiklis [1996] and Rojas [1996].", "startOffset": 83, "endOffset": 115}, {"referenceID": 5, "context": "More information on how these networks are constructed and trained is available in Bertsekas and Tsitsiklis [1996] and Rojas [1996]. For SDP, a 4 degree spline space with 1 order continuity, without (\u03b22 = 0) and with (\u03b22 = 0.", "startOffset": 83, "endOffset": 132}, {"referenceID": 5, "context": "This increased learning rate of a dynamic programming algorithm in a stochastic system is a well known phenomenon, and is a result of the extra exploration that occurs due to the system noise Bertsekas and Tsitsiklis [1996]. Nevertheless the learning rate of SDP remains the highest in both the deterministic and stochastic system.", "startOffset": 192, "endOffset": 224}, {"referenceID": 8, "context": "Using a linear-in-the-parameters function approximator allows for the use of the RLS TD algorithm, which has a fast and proven convergence in a stochastic framework Bradtke and Barto [1996]. As a result, the SDP framework without forget factor (\u03b22 = 0) has proven con-", "startOffset": 165, "endOffset": 190}, {"referenceID": 14, "context": "Another option is to treat all unknown parameters as an additional optimization, and solve the entire optimization problem using Intersplines de Visser et al. [2012]. Unfortunately, at the moment Intersplines are limited to two-dimensional inputs, and require too much calculation power to make it attractive for real-time applications.", "startOffset": 145, "endOffset": 166}, {"referenceID": 13, "context": "While it is possible to use the multivariate simplex B-splines in higher dimensions de Boor [1986], there are two problems that arise.", "startOffset": 87, "endOffset": 99}, {"referenceID": 3, "context": "Secondly, due to the \u201ccurse of dimensionality\u201d Bellman [1957], the computational costs of dynamic programming are very high when moving to higher dimensions.", "startOffset": 47, "endOffset": 62}], "year": 2016, "abstractText": "We define a SDP framework based on the RLS TD algorithm and multivariate simplex B-splines. We introduce a local forget factor capable of preserving the continuity of the simplex splines. This local forget factor is integrated with the RLS TD algorithm, resulting in a modified RLS TD algorithm that is capable of tracking time-varying systems. We present the results of two numerical experiments, one validating SDP and comparing it with NDP and another to show the advantages of the modified RLS TD algorithm over the original. While SDP requires more computations per time-step, the experiment shows that for the same amount of function approximator parameters, there is an increase in performance in terms of stability and learning rate compared to NDP. The second experiment shows that SDP in combination with the modified RLS TD algorithm allows for faster recovery compared to the original RLS TD algorithm when system parameters are altered, paving the way for an adaptive highperformance non-linear control method.", "creator": "LaTeX with hyperref package"}}}