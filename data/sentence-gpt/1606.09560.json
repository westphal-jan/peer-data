{"id": "1606.09560", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "30-Jun-2016", "title": "Neural Network-based Word Alignment through Score Aggregation", "abstract": "We present a simple neural network for word alignment that builds source and target word window representations to compute alignment scores for sentence pairs. To enable unsupervised training, we use an aggregation operation that summarizes the alignment scores for a given target word. A soft-margin objective increases scores for true target words while decreasing scores for target words that are not present in words that are not present in words that are present in words that are not present in words that are present in words that are not present in words that are not present in words that are not present in words that are not present in words that are not present in words that are not present in words that are not present in words that are not present in words that are not present in words that are not present in words that are not present in words that are not present in words that are not present in words that are not present in words that are not present in words that are not present in words that are not present in words that are not present in words that are not present in words that are not present in words that are not present in words that are not present in words that are not present in words that are not present in words that are not present in words that are not present in words that are not present in words that are not present in words that are not present in words that are not present in words that are not present in words that are not present in words that are not present in words that are not present in words that are not present in words that are not present in words that are not present in words that are not present in words that are not present in words that are not present in words that are not present in words that are not present in words that are not present in words that are not present in words that are not present in words that are not present in words that are not present in words that are not present in words that are not present in words that are not present in words that are not present in words that are not present in words that are not present in words that are not present in words that are not present in words that are not present in words that are not present in words that are not present in words that are not present in words that are not present in words that are not present in words that are not present in words that are not present in words that are not present in words that are not present in words that are not present in words that are not present in words that are not present in words that are not present in words that are not present in", "histories": [["v1", "Thu, 30 Jun 2016 16:32:00 GMT  (91kb,D)", "http://arxiv.org/abs/1606.09560v1", null]], "reviews": [], "SUBJECTS": "cs.CL", "authors": ["joel legrand", "michael auli", "ronan collobert"], "accepted": false, "id": "1606.09560"}, "pdf": {"name": "1606.09560.pdf", "metadata": {"source": "CRF", "title": "Neural Network-based Word Alignment through Score Aggregation", "authors": ["Jo\u00ebl Legrand", "Michael Auli", "Ronan Collobert"], "emails": [], "sections": [{"heading": null, "text": "We present a simple neural network for word alignment that builds source and target word window representations to compute alignment scores for sentence pairs. To enable unsupervised training, we use an aggregation operation that summarizes the alignment scores for a given target word. A soft-margin objective increases scores for true target words while decreasing scores for target words that are not present. Compared to the popular Fast Align model, our approach improves alignment accuracy by 7 AER on EnglishCzech, by 6 AER on Romanian-English and by 1.7 AER on English-French alignment."}, {"heading": "1 Introduction", "text": "Word alignment is the task of finding the correspondence between source and target words in a pair of sentences that are translations of each other. Generative models for this task (Brown et al., 1990; Och and Ney, 2003; Vogel et al., 1996) still form the basis for many machine translation systems (Koehn et al., 2003; Chiang, 2007).\nRecent neural approaches include Yang et al. (2013) who introduce a feed-forward networkbased model trained on alignments that were generated by a traditional generative model. This treats potentially erroneous alignments as supervision. Tamura et al. (2014) sidesteps this issue by negative sampling to train a recurrent-neural network on unlabeled data. They optimize a global loss that requires an expensive beam search to approximate the sum over all alignments.\n\u2020This work was conducted while the first author did an internship at Facebook AI Research.\nIn this paper we introduce a word alignment model that is simpler in structure and which relies on a more tractable training procedure. Our model is a neural network that extracts context information from source and target sentences and then computes simple dot products to estimate alignment links. Our objective function is wordfactored and does not require the expensive computation associated with global loss functions. The model can be easily trained on unlabeled data via a novel but simple aggregation operation which has been successfully applied in the computer vision literature (Pinheiro and Collobert, 2015). The aggregation combines the scores of all source words for a particular target word and promotes source words which are likely to be aligned with a given target word according to the knowledge the model has learned so far. At test time, the aggregation operation is removed and source words are aligned to target words by choosing the highest scoring candidates (\u00a72, \u00a73).\nWe evaluate several forms for our aggregation operation such as computing the sum, max and LogSumExp over alignment scores. Results on English-French, English-Romanian, and CzechEnglish alignment show that our model significantly outperforms Fast Align, a popular loglinear reparameterization of IBM Model 2 (Dyer et al., 2013; \u00a74)."}, {"heading": "2 Aggregation Model", "text": "In the following, we consider a target-source sentence pair (e, f), with e = (e1, . . . , e|e|) and f = (f1, . . . , f|f |). Words are represented by fj and ei, which are indices in source and target dictionaries. For simplicity, we assume here that word indices are the only feature fed to our architecture. Given a source word fj and a target word ei, our architecture embeds a window (of size d f win\nar X\niv :1\n60 6.\n09 56\n0v 1\n[ cs\n.C L\n] 3\n0 Ju\nn 20\n16\nand dewin, respectively) centered around each of these words into a demb-dimensional vector space. The embedding operation is performed with two distinct neural networks:\nnete([e] dewin i ) \u2208 Rdemb\nand netf ([f ] dfwin j ) \u2208 Rdemb ,\nwhere we denote the window operator as\n[x]di = (xi\u2212d/2, . . . , xi+d/2) .\nThe matching score between a source word fj and a target word ei is then given by the dot-product:\ns(i, j) = nete([e] dewin i ) \u00b7 netf ([f ] dfwin j ) . (1)\nIf ei is aligned to fai , the score s(i, ai) should be high, while scores s(i, j) \u2200j 6= ai should be low."}, {"heading": "2.1 Unsupervised Training", "text": "In this paper, we consider an unsupervised setup where the alignment is not known at training time. We thus cannot minimize or maximize matching scores (1) in a direct manner. Instead, given a target word ei we consider the aggregated matching scores over the source sentence:\nsaggr(i, f) = |f |\nAggr j=1 s(i, j) , (2)\nwhere Aggr is an aggregation operator (\u00a72.2). Consider a matching (positive) sentence pair (e+, f) and a negative sentence pair (e\u2212, f). Given a word at index i+ in the positive target sentence, we want to maximize the aggregated score saggr(i+, f) (1 \u2264 i+ \u2264 |e+|) because we know it should be aligned to at least one source word.1 Conversely, given a word at index i\u2212 in the negative target sentence, we want to minimize saggr(i\n\u2212, f) (1 \u2264 i\u2212 \u2264 |e\u2212|) because it is unlikely that the source sentence can explain the negative target word. Following these principles, we consider a simple soft-margin loss:\nL(e+, e\u2212, f) = |e+|\u2211 i+=1 log(1 + e\u2212saggr(i +,f))\n+ |e\u2212|\u2211 i\u2212=1 log(1 + e+saggr(i \u2212,f)) .\n(3)\nTraining is achieved by minimizing (3) and by sampling over triplets (e+, e\u2212, f) from the training data.\n1We discuss how we handle unaligned target words in \u00a72.3. Also, depending on the decoding algorithm the model can be used to predict many-to-many alignments."}, {"heading": "2.2 Choosing the Aggregation", "text": "The aggregation operation (2) is only present during training and acts as a filter which aims to explain a given target word ei by one or more source words. If we had the word alignments, then we would sum over the source words fj aligned with ei. However, in our setup alignments are not available at training time, so we must rely on what the model has learned so far to filter the source words. We consider the following strategies:\n\u2022 Sum: ignore the knowledge learned so far, and assign the same weight to all source words fj to explain ei.2 In this case, we have\nsaggr(i, f) = |f|\u2211 j=1 s(i, j) .\n\u2022 Max: encourage the best aligned source word fj , according to what the model has learned so far. In this case, the aggregation is written as:\nsaggr(i, f) = |f|\nmax j=1 s(i, j) .\n\u2022 LSE: give similar weights to source words with similar scores. This can be achieved with a LogSumExp aggregation operation (also called LogAdd), and is defined as:\nsaggr(i, f) = 1\nr log  |f|\u2211 j=1 er s(i, j)  , (4) where r is a positive scalar (to be chosen) controlling the smoothness of the aggregation. For small r, the aggregation is equivalent to a sum, and for large r, the aggregation acts as a max."}, {"heading": "2.3 Decoding", "text": "At test time, we align each target word ei with the source word fj for which the matching score s(i, j) in (1) is highest.3 However, not every target word is aligned, so we consider only alignments with a matching score above a threshold:\ns(i, j) > \u00b5\u2212(ei) + \u03b1\u03c3 \u2212(ei) , (5)\n2This can be seen by observing that the gradients for all source words are the same.\n3This may result in a source word being aligned to multiple target words.\nwhere \u03b1 is a tunable hyper-parameter, and\n\u00b5\u2212(ei) = E {e\u0303k=ei \u2208 e\u0303, f\u0303j\u2212 \u2208 f\u0303\u2212}\n[ s(k, j\u2212) ] is the expectation over all training sentences e\u0303 containing the word ei, and all words f\u0303\u2212j belonging to a corresponding negative source sentence f\u0303\u2212, and \u03c3\u2212(ei) is the respective variance."}, {"heading": "3 Neural Network Architecture", "text": "Our model consists of two convolutional neural networks nete and netf as shown in (1). Both of them take the same form, so we detail only the target architecture."}, {"heading": "3.1 Word embeddings", "text": "The discrete features [e]d e win\ni are embedded into a deemb-dimensional vector space via a lookuptable operation as first introduced in Bengio et al. (2000):\nxei = LTW e([e] dewin i )\n= (LTW e(ei\u2212dewin/2), . . . , LTW e(ei+dewin/2)) ,\nwhere the lookup-table operation applied at index k returns the kth column of the parameter matrix W e:\nLTW e(k) =W e\u2022, k .\nThe matrix W e is of size |Ve| \u00d7 deemb, where Ve is the target vocabulary, and deemb is the word embedding size for the target words."}, {"heading": "3.2 Convolutional layers", "text": "The word embeddings output by the lookup-table are concatenated and fed through two successive 1-D convolution layers. The convolutions use a step size of one and extract context features for each word. The kernel sizes ke1 and k e 2 determine the size of the window dewin = k e 1 + k e 2 \u2212 1 over which features will be extracted by nete. In order to obtain windows centered around each word, we add (ke1+k e 2)/2\u22121 padding words at the beginning and at the end of each sentence. The first layer cnne applies the linear transformationM e,1 exactly ke2 times to consecutive spans of size ke1 to the d e win words in a given window:\ncnne(xei ) =M e,1  LTW e([e] ke1 i\u2212a)\n... LTW e([e] ke1 i+a)\n ,\nwhere a = bk e 2 2 c, M e,1 \u2208 Rd e hu\u00d7(d e emb k e 1) is a matrix of parameters, and dehu is the number of hidden units (hu). The outputs of the first layer cnne are concatenated to form a matrix of size ke2 d e hu which is fed to the second layer:\nnete(xei ) =M e,2 tanh(cnne(xei )) (6)\nwhere M e,2 \u2208 Rdemb\u00d7(ke2 dehu) is a matrix of parameters, and the tanh(\u00b7) operation is applied element wise. The parameters W e, M e,1 and M e,2 are trained by stochastic gradient descent to minimize the loss (3) introduced in \u00a72.1."}, {"heading": "3.3 Additional Features", "text": "In addition to the raw word indices, we consider two additional discrete features which were handled in the same way as word features by introducing an additional lookup-table for each of them. The output of all lookup-tables was concatenated, and fed to the two-layer neural network architecture (6).\nDistance to the diagonal. This feature can be computed for a target word ei and a source word fj :\ndiag(i, j) = \u2223\u2223\u2223\u2223 i|e| \u2212 j|f | \u2223\u2223\u2223\u2223 ,\nThis feature allows the model to learn that aligned sentence pairs use roughly the same word order and that alignment links remain close to the diagonal. We use this feature only for the source network because it encodes relative position information which only needs to be encoded once. If we would use absolute position instead, then we would need to encode this information both on the source and the target side.\nPart-of-speech Words pairs that are good translations of each other are likely to carry the same part of speech in both languages (Melamed, 1995). We therefore add the part-of-speech information to the model.\nChar n-gram. We consider unigram character position features. Let K be the maximum size for a word in a dictionary. We denote the dictionary of characters as C. Every character is represented by its index c (with 1 < c < |C|). We associate every character c at position k with a vector at position ((k \u2212 1) \u2217 |C|) + c in a lookup-table. For a given word, we extract all unigram character position embeddings, and average them to obtain a character embedding for a given word."}, {"heading": "4 Experiments", "text": ""}, {"heading": "4.1 Datasets", "text": "We use the English-French Hansards corpus as distributed by the NAACL 2003 shared task (Mihalcea and Pedersen, 2003). This dataset contains 1.1M sentence pairs and the test and validation sets contain 447 and 37 examples respectively. We also evaluate on the Romanian-English dataset of the ACL 2005 shared task (Martin et al., 2005) comprising 48K sentence pairs for training, 248 for testing and 17 for validation. For EnglishCzech experiments, we use the WMT news commentary corpus for training (150K sentence pairs) and a set of 515 sentences for testing (Bojar and Prokopova\u0301, 2006)."}, {"heading": "4.2 Evaluation", "text": "Our models are evaluated in terms of precision, recall, F-measure and Alignment Error Rate (AER). We train models in each language direction and then symmetrize the resulting alignments using either the intersection or the grow-diag-final-and heuristic (Och and Ney, 2003; Koehn et al., 2003). We validated the choice of symmetrization heuristic on each language pair and chose the best one for each model considering the two aforementioned types as well as grow-diag-final and growdiag.\nAdditionally, we train phrase-based machine translation models with our alignments using the popular Moses toolkit (Koehn et al., 2007). For English-French, we train on the news commentary corpus v10, for English-Czech we used news commentary corpus v11, and for Romanian-English we used the Europarl corpus v8. We tuned our models on the WMT2015 test set for EnglishCzech as well as for Romanian-English; for English-French we tuned on the WMT2014 test set. Final results are reported on the WMT2016 test set for English-Czech as well as RomanianEnglish, and for English-French we report results on the WMT2015 test set (as there is no track for this language-pair in 2016).\nWe compare our model to Fast Align, a popular log-linear reparameterization of IBM Model 2 (Dyer et al., 2013)."}, {"heading": "4.3 Setup", "text": "The kernel sizes of the target network nete(\u00b7) are set to ke1 = k e 2 = 3 for all language pairs. The kernel sizes of the source network netf (\u00b7) are set\nto kf1 = k f 2 = 3 for Romanian-English as well as English-Czech; and for English-French we used kf1 = k f 2 = 1.\nThe number of hidden units are dehu = d f hu = 256 and demb is set to 256, The source Vf and target Ve dictionaries consist of the 30K most common words for English, French and Romanian, and 80K for Czech. All other words are mapped to a unique UNK token. The word embedding sizes deemb and d f emb, as well as the char-n-gram embedding size is 128. For LSE, we set r = 1 in (4).\nWe initialize the word embeddings with a simple PCA computed over the matrix of word cooccurrence counts (Lebret and Collobert, 2014). The co-occurrence counts were computed over the common crawl corpus provided by WMT16. For part of speech tagging we used the Stanford parser on English-French data, and MarMoT (Mueller et al., 2013) for Romanian-English as well as English-Czech.\nWe trained 4 systems for the ensembles, each using a different random seed to vary the weight initialization as well as the shuffling of the training set. We averaged the alignment scores predicted by each system before decoding. The alignment threshold variables \u00b5\u2212(ei) and \u03c3\u2212(ei) for decoding (\u00a72.3) were estimated on 1000 random training sentences, using 100 negative sentences for each of them. Words not appearing in this training subset were assigned \u00b5\u2212(ei) = \u03c3\u2212(ei) = 0.\nFor systems where dewin > 1 and d f win > 1, we saw a tendency of aligning frequent words regardless on if they appeared in the center of the context window or not. For instance, a common mistake would be to align \u201dthe cat sat\u201d, with \u201dPADDING le chat\u201d. To prevent such behavior, we occasionally replaced the center word in a target window by a random word during training. We do this for every second training example on average and we tuned this rate on the validation set."}, {"heading": "4.4 Results", "text": "We first explore different choices for the aggregation operator (\u00a72.2), followed by an ablation to investigate the impact of the different additional features (\u00a73.3). Next we compare to the Fast Align baseline. Finally, we evaluate our alignments within a full translation system for all language pairs."}, {"heading": "4.4.1 Aggregation operation", "text": "Table 1 shows that the LogSumExp (LSE) aggregator performs best on all datasets for every direction as well as in the symmetrized setting using the grow-diag-final heuristic. All results are based on a single model trained with the \u2019distance to the diagonal\u2019 feature detailed above.4 We therefore use LSE for the remaining experiments."}, {"heading": "4.4.2 Additional features", "text": "Table 2 shows the effect of the different input features. Both POS and the distance to the diagonal feature significantly improve accuracy. Position information via the \u2019distance to the diagonal\u2019 feature is helpful for all language pairs, and POS information is more effective for RomanianEnglish and English-Czech which involve morphologically rich languages. We use the POS and \u2019distance to the diagonal feature\u2019 for the remaining experiments."}, {"heading": "4.4.3 Comparison with the baseline", "text": "In the following results we label our model as NNSA (Neural network score aggregation). On English-French data (Table 3) our model outperforms the baseline (Dyer et al., 2013) in each individual language direction as well as for the symmetrized setting. With an ensemble of four models, we outperform the baseline by 1.7 AER (from 11.4 to 9.7), and with an individual model we outperform it by 1.2 AER (from 11.4 to 10.2). Note that the choice of symmetrization heuristic greatly affects accuracy, both for the baseline and NNSA.\n4We use kernel sizes ke1 = ke2 = 3 and k f 1 = k f 2 = 1 for\nall language pairs in this experiment.\nOn Romanian-English (Table 4) our model outperforms the baseline in both directions as well. Adding ensembles further improves accuracy and leads to a significant improvement of 6 AER over the best symmetrized baseline result (from 32 to 26).\nOn English-Czech (Table 5) our model outperforms the baseline in both directions as well. We added the character feature to better deal with the morphologically rich nature of Czech and the feature reduced AER by 2.1 in the symmetrized setting. An ensemble improved accuracy further and led to a 7 AER improvement over the best symmetrized baseline result (from 22.8 to 15.8)."}, {"heading": "4.4.4 BLEU evaluation", "text": "Table 6 presents the BLEU evaluation of our alignments. For each language-pair, we select the best alignment model reported in Tables 3, 4 and 5, and align the training data. We use the alignments to\nrun the standard phrase-based training pipeline using those alignments. Our BLEU results show the average BLEU score and standard deviation for five runs of minimum error rate training (MERT; Och 2003).\nOur alignments achieve slightly better results for Romanian-English as well as English-Czech while performing on par with Fast Align on English-French translation."}, {"heading": "5 Analysis", "text": "In this section, we analyze the word representations learned by our model. We first focus on the source representations: given a source window, we obtain its distributional representation and then compute the Euclidean distance to all other source windows in the training corpus. Table 7 shows the nearest windows for two source windows; the closest windows tend to have similar meanings.\nWe then analyze the relation between source and target representations: given a source window we compute the alignment scores for all target sentences in the training corpus. Table 8 shows for two source windows which target words have\nthe largest alignment scores. The example \u201din working together\u201d is particularly interesting since the aligned target words collabore, coordone\u0301s, and concerte\u0301s mean collaborate, coordinated, and concerted, which all carry the same meaning as the source window phrase."}, {"heading": "6 Conclusion", "text": "In this paper, we present a simple neural network alignment model trained on unlabeled data. Our model computes alignment scores as dot products between representations of windows around source and target words. We apply an aggregation operation borrowed from the computer vision literature to make unsupervised training possible. The aggregation operation acts as a filter over alignment scores and allows us to determine which source words explain a given target word.\nWe improve over Fast Align, a popular loglinear reparameterization of IBM Model 2 (Dyer et al., 2013) by up to 6 AER on RomanianEnglish, 7 AER on English-Czech data and 1.7 AER on English-French alignment. Furthermore,\nwe evaluated our model as part of a full machine translation pipeline and showed that our alignments are better or on par compared to Fast Align in terms of BLEU."}], "references": [{"title": "A Neural Probabilistic Language Model", "author": ["Yoshua Bengio", "R\u00e9jean Ducharme", "Pascal Vincent"], "venue": "In NIPS,", "citeRegEx": "Bengio et al\\.,? \\Q2000\\E", "shortCiteRegEx": "Bengio et al\\.", "year": 2000}, {"title": "CzechEnglish Word Alignment", "author": ["Ond\u0159ej Bojar", "Magdalena Prokopov\u00e1"], "venue": "In Proceedings of the Fifth International Conference on Language Resources and Evaluation", "citeRegEx": "Bojar and Prokopov\u00e1.,? \\Q2006\\E", "shortCiteRegEx": "Bojar and Prokopov\u00e1.", "year": 2006}, {"title": "Hierarchical Phrase-Based Translation", "author": ["David Chiang"], "venue": "Computational Linguistics,", "citeRegEx": "Chiang.,? \\Q2007\\E", "shortCiteRegEx": "Chiang.", "year": 2007}, {"title": "A simple, fast, and effective reparameterization of IBM Model 2", "author": ["Chris Dyer", "Victor Chahuneau", "Noah A. Smith"], "venue": "In Proc. of NAACL,", "citeRegEx": "Dyer et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Dyer et al\\.", "year": 2013}, {"title": "Statistical Phrase-based Translation", "author": ["Philipp Koehn", "Franz J. Och", "Daniel Marcu"], "venue": "In Proc. of NAACL,", "citeRegEx": "Koehn et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Koehn et al\\.", "year": 2003}, {"title": "Open source toolkit for statistical machine translation", "author": ["Herbst. Moses"], "venue": "In Proceedings of the 45th Annual Meeting of the ACL on Interactive Poster and Demonstration Sessions, ACL", "citeRegEx": "Moses,? \\Q2007\\E", "shortCiteRegEx": "Moses", "year": 2007}, {"title": "Word Embeddings through Hellinger PCA", "author": ["R\u00e9mi Lebret", "Ronan Collobert"], "venue": "In Proc. of EACL,", "citeRegEx": "Lebret and Collobert.,? \\Q2014\\E", "shortCiteRegEx": "Lebret and Collobert.", "year": 2014}, {"title": "Word Alignment For Languages With Scarce Resources", "author": ["Joel Martin", "Rada Mihalcea", "Ted Pedersen"], "venue": "In Proc. of WPT,", "citeRegEx": "Martin et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Martin et al\\.", "year": 2005}, {"title": "Automatic evaluation and uniform filter cascades for inducing n-best translation lexicons", "author": ["Dan I. Melamed"], "venue": "In Third Workshop on Very Large Corpora,", "citeRegEx": "Melamed.,? \\Q1995\\E", "shortCiteRegEx": "Melamed.", "year": 1995}, {"title": "An Evaluation Exercise for Word Alignment", "author": ["Rada Mihalcea", "Ted Pedersen"], "venue": "In Proc. of WPT,", "citeRegEx": "Mihalcea and Pedersen.,? \\Q2003\\E", "shortCiteRegEx": "Mihalcea and Pedersen.", "year": 2003}, {"title": "Efficient higher-order CRFs for morphological tagging", "author": ["Thomas Mueller", "Helmut Schmid", "Hinrich Sch\u00fctze"], "venue": "In Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing,", "citeRegEx": "Mueller et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Mueller et al\\.", "year": 2013}, {"title": "A Systematic Comparison of Various Statistical Alignment Models", "author": ["Franz J. Och", "Hermann Ney"], "venue": "Computational Linguistics,", "citeRegEx": "Och and Ney.,? \\Q2003\\E", "shortCiteRegEx": "Och and Ney.", "year": 2003}, {"title": "Minimum error rate training in statistical machine translation", "author": ["Franz Josef Och"], "venue": "In Proc of ACL,", "citeRegEx": "Och.,? \\Q2003\\E", "shortCiteRegEx": "Och.", "year": 2003}, {"title": "From Image-level to Pixel-level Labeling with Convolutional Networks", "author": ["Pedro O. Pinheiro", "Ronan Collobert"], "venue": "In Proc. of CVPR,", "citeRegEx": "Pinheiro and Collobert.,? \\Q2015\\E", "shortCiteRegEx": "Pinheiro and Collobert.", "year": 2015}, {"title": "Recurrent Neural Networks for Word Alignment Model", "author": ["Akihiro Tamura", "Taro Watanabe", "Eiichiro Sumita"], "venue": "In Proc. of ACL,", "citeRegEx": "Tamura et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Tamura et al\\.", "year": 2014}, {"title": "HMM-Based Word Alignment in Statistical Translation", "author": ["Stephan Vogel", "Hermann Ney", "Christoph Tillmann"], "venue": "In Proc. of COLING,", "citeRegEx": "Vogel et al\\.,? \\Q1996\\E", "shortCiteRegEx": "Vogel et al\\.", "year": 1996}, {"title": "Word Alignment Modeling with Context Dependent Deep Neural Network", "author": ["Nan Yang", "Shujie Liu", "Mu Li", "Ming Zhou", "Nenghai Yu"], "venue": "In Proc. of ACL,", "citeRegEx": "Yang et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Yang et al\\.", "year": 2013}], "referenceMentions": [{"referenceID": 11, "context": "Generative models for this task (Brown et al., 1990; Och and Ney, 2003; Vogel et al., 1996) still form the basis for many machine translation systems (Koehn et al.", "startOffset": 32, "endOffset": 91}, {"referenceID": 15, "context": "Generative models for this task (Brown et al., 1990; Och and Ney, 2003; Vogel et al., 1996) still form the basis for many machine translation systems (Koehn et al.", "startOffset": 32, "endOffset": 91}, {"referenceID": 4, "context": ", 1996) still form the basis for many machine translation systems (Koehn et al., 2003; Chiang, 2007).", "startOffset": 66, "endOffset": 100}, {"referenceID": 2, "context": ", 1996) still form the basis for many machine translation systems (Koehn et al., 2003; Chiang, 2007).", "startOffset": 66, "endOffset": 100}, {"referenceID": 15, "context": "Recent neural approaches include Yang et al. (2013) who introduce a feed-forward networkbased model trained on alignments that were generated by a traditional generative model.", "startOffset": 33, "endOffset": 52}, {"referenceID": 14, "context": "Tamura et al. (2014) sidesteps this issue by negative sampling to train a recurrent-neural network on unlabeled data.", "startOffset": 0, "endOffset": 21}, {"referenceID": 13, "context": "The model can be easily trained on unlabeled data via a novel but simple aggregation operation which has been successfully applied in the computer vision literature (Pinheiro and Collobert, 2015).", "startOffset": 165, "endOffset": 195}, {"referenceID": 0, "context": "1 Word embeddings The discrete features [e] e win i are embedded into a demb-dimensional vector space via a lookuptable operation as first introduced in Bengio et al. (2000):", "startOffset": 153, "endOffset": 174}, {"referenceID": 8, "context": "Part-of-speech Words pairs that are good translations of each other are likely to carry the same part of speech in both languages (Melamed, 1995).", "startOffset": 130, "endOffset": 145}, {"referenceID": 9, "context": "We use the English-French Hansards corpus as distributed by the NAACL 2003 shared task (Mihalcea and Pedersen, 2003).", "startOffset": 87, "endOffset": 116}, {"referenceID": 7, "context": "We also evaluate on the Romanian-English dataset of the ACL 2005 shared task (Martin et al., 2005) comprising 48K sentence pairs for training, 248 for testing and 17 for validation.", "startOffset": 77, "endOffset": 98}, {"referenceID": 1, "context": "For EnglishCzech experiments, we use the WMT news commentary corpus for training (150K sentence pairs) and a set of 515 sentences for testing (Bojar and Prokopov\u00e1, 2006).", "startOffset": 142, "endOffset": 169}, {"referenceID": 11, "context": "We train models in each language direction and then symmetrize the resulting alignments using either the intersection or the grow-diag-final-and heuristic (Och and Ney, 2003; Koehn et al., 2003).", "startOffset": 155, "endOffset": 194}, {"referenceID": 4, "context": "We train models in each language direction and then symmetrize the resulting alignments using either the intersection or the grow-diag-final-and heuristic (Och and Ney, 2003; Koehn et al., 2003).", "startOffset": 155, "endOffset": 194}, {"referenceID": 3, "context": "We compare our model to Fast Align, a popular log-linear reparameterization of IBM Model 2 (Dyer et al., 2013).", "startOffset": 91, "endOffset": 110}, {"referenceID": 6, "context": "We initialize the word embeddings with a simple PCA computed over the matrix of word cooccurrence counts (Lebret and Collobert, 2014).", "startOffset": 105, "endOffset": 133}, {"referenceID": 10, "context": "For part of speech tagging we used the Stanford parser on English-French data, and MarMoT (Mueller et al., 2013) for Romanian-English as well as English-Czech.", "startOffset": 90, "endOffset": 112}, {"referenceID": 3, "context": "On English-French data (Table 3) our model outperforms the baseline (Dyer et al., 2013) in each individual language direction as well as for the symmetrized setting.", "startOffset": 68, "endOffset": 87}, {"referenceID": 3, "context": "We improve over Fast Align, a popular loglinear reparameterization of IBM Model 2 (Dyer et al., 2013) by up to 6 AER on RomanianEnglish, 7 AER on English-Czech data and 1.", "startOffset": 82, "endOffset": 101}], "year": 2016, "abstractText": "We present a simple neural network for word alignment that builds source and target word window representations to compute alignment scores for sentence pairs. To enable unsupervised training, we use an aggregation operation that summarizes the alignment scores for a given target word. A soft-margin objective increases scores for true target words while decreasing scores for target words that are not present. Compared to the popular Fast Align model, our approach improves alignment accuracy by 7 AER on EnglishCzech, by 6 AER on Romanian-English and by 1.7 AER on English-French alignment.", "creator": "LaTeX with hyperref package"}}}