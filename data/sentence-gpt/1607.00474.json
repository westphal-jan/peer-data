{"id": "1607.00474", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "2-Jul-2016", "title": "Adaptive Neighborhood Graph Construction for Inference in Multi-Relational Networks", "abstract": "A neighborhood graph, which represents the instances as vertices and their relations as weighted edges, is the basis of many semi-supervised and relational models for node labeling and link prediction. Most methods employ a sequential process to construct the neighborhood graph. This process often consists of generating a candidate graph, pruning the candidate graph to make a neighborhood graph, and then performing inference on the variables (i.e., nodes) in the neighborhood graph. In this paper, we propose a framework that can dynamically adapt the neighborhood graph based on the states of variables from intermediate inference results, as well as structural properties of the relations connecting them. A key strength of our framework is its ability to handle multi-relational data and employ varying amounts of relations for each instance based on the intermediate inference results. We formulate the link prediction task as inference on neighborhood graphs, and include preliminary results illustrating the effects of different strategies in our proposed framework.\n\n\n\nThis paper explores the computational utility of clustering in clustering and describes the benefits of clustering over a set of variables. In the future, we will use several new techniques for generating the neighborhood graph using the following two methods:\nFASAR\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n", "histories": [["v1", "Sat, 2 Jul 2016 07:41:45 GMT  (643kb,D)", "http://arxiv.org/abs/1607.00474v1", "Presented at SIGKDD 12th International Workshop on Mining and Learning with Graphs (MLG'16)"]], "COMMENTS": "Presented at SIGKDD 12th International Workshop on Mining and Learning with Graphs (MLG'16)", "reviews": [], "SUBJECTS": "cs.SI cs.AI cs.LG", "authors": ["shobeir fakhraei", "dhanya sridhar", "jay pujara", "lise getoor"], "accepted": false, "id": "1607.00474"}, "pdf": {"name": "1607.00474.pdf", "metadata": {"source": "META", "title": "Adaptive Neighborhood Graph Construction for Inference in Multi-Relational Networks", "authors": ["Shobeir Fakhraei", "Dhanya Sridhar", "Jay Pujara", "Lise Getoor"], "emails": ["shobeir@cs.umd.edu", "dsridhar@ucsc.edu", "jay@cs.umd.edu", "getoor@soe.ucsc.edu"], "sections": [{"heading": "1. INTRODUCTION", "text": "Neighborhood graphs which capture interdependencies between instances, are the underlying structure of reasoning in many graph-based predictive models. These models are used in domains such as collaborative filtering, link prediction, and image classification. For example, when determining characteristics of individuals, the characteristics of the people who are most similar to them, their friends, family, and co-workers, can all influence a model\u2019s prediction. In these models the data is represented as a neighborhood graph or network, where nodes are instances and weighted edges represent relations (e.g., similarities) between them. Neighborhood graph-based methods include popular semi-supervised modeling techniques.\nMost methods that make predictions based on a neighborhood graph can be characterized in terms of three basic operations [1]: Candidate graph Generation, Selection and Inference. The first step, is the candidate graph generation, which often includes defining the relations or similarities between instances. The process of constructing the candidate graph is generally problem-specific. If the original input data is relational, or in the form of a graph (which we call data graph), some of the explicit relations such as relationships in a social network, or adjacencies in an image may be used as an approximation of the affinity or dependency of instances. When the original input data includes instance attributes, a similarity or kernel function is defined to es-\ntimate the pairwise affinity of the items. The abundance of pairwise similarities or relations often hinders a model\u2019s scalability as well as its predictive performance and makes the candidate graph generally unsuitable for modeling approaches.\nThe next step is selection, reducing the candidate graph by pruning similarities or relations to a more manageable neighborhood graph. Examples of these methods that are often considered a pre-processing step to inference include knearest neighbors, -neighborhood selection, and b-matching [1]. This is an important step in all the neighborhood graphbased methods as unnecessary relations in the graph reduces the scalability. More importantly, similar to the negative effect of a large k in a simple k -nearest neighbors classifier, unnecessary relations can harm the performance of a neighborhood graph-based model. Fakhraei et al. [2] show this negative effect in a drug target prediction setting.\nThe third step is the algorithm to perform inference using the neighborhood graph. Methods such as Mincut, graph random walk, Gaussian random fields, local and global consistency, spectral graph transducer, manifold regularization, and label propagation are examples that perform inference on the neighborhood graph [3].\nWe consider several challenges in this sequential process for constructing the neighborhood graph especially based on multi-relational data:\n\u2013 Most methods are designed to handle a single similarity, dependency, or relation type when constructing the neighborhood graph. However, there are often multiple relations than each can serve as a noisy approximation for the affinity that is important for the predictive task at hand. Constructing a multi-relational neighborhood graph that can effectively combine different affinity signals from multiple sources is highly important. \u2013 The model-agnostic nature of the pruning methods can result in neighborhood graphs that may not adequately capture the most important similarities for a particular model. We assert that successful graph construction requires a model-based approach that includes the context of the prediction problem as one of its components. In other words, model-aware graph construction methods can leverage the information about the uncertainties and decision boundaries of the model that sequential model-agnostics methods can not. \u2013 The pruning process often solely relies on the value assigned to the similarity or relation and does not consider the characteristics of instances that are being\nar X\niv :1\n60 7.\n00 47\n4v 1\n[ cs\n.S I]\n2 J\nul 2\n01 6\nconnected in the neighborhood graph. For example, a relatively low similarity to an instance of a rare class may be more important than a high similarity to an instance of the majority class. \u2013 With the rapid growth in size of the datasets, performing complex search processes on all pairwise relations between instances increases the computational cost. Therefore, depending on the size of the dataset an algorithm may only be able to make approximate decisions based on partial observations.\nIn this paper, we address these challenges by developing a general framework for dynamically constructing a neighborhood graph. Our framework enables rich models for active inference that interleave inference and neighborhood graph construction, efficiently using multi-relational data while maintaining scalable performance. We have developed the LINA framework, which consists of four parts; Learning the relative importance of each relation in multirelational settings, Inferring labels for interrelated instances, Nominating instances that can benefit from additional relations in the neighborhood graph, and Activating new relations between instances in the neighborhood graph to improve the inference results. Our main contribution in this paper include:\n\u2013 We propose a general unified framework to actively learn multi-relational neighborhood graphs and demonstrate its use for collective link prediction. \u2013 We explore the ramifications of modeling choices for the important components in our framework and their combination; Nomination and Activation. \u2013 We lay out a general formulation for the link prediction task as an inference problem on a neighborhood graph. \u2013 We present preliminary results of applying different components of our framework for link prediction on a drug-target interaction network dataset.\nIn Section 2 we formally define the problem statement, and in Section 3 we describe our proposed framework. We explain Hinge-loss Markov random fields that we use for inference in Section 4. We then present our nomination, activation, and learning methods in Section 5. Section 6 includes discussion on modeling the link prediction task with a neighborhood graph. We then provide preliminary experimental results of applying our approach in Section 7."}, {"heading": "2. GENERAL PROBLEM STATEMENT", "text": "Let candidate graph be represented via Gc , \u3008X,R\u3009,\nwhere vertices X is a set of n data points xi = [ x1i , . . . , x p\u22121 i , yi ] , and y represent the set of all labels (y = {y1, . . . , yn}) where a subset is observed (yo) and others are unobserved (yu), and edges R are a set of relations rxi,xj (e.g., pairwise similarities, or connections in social network) connecting pairs of data points xi and xj , based on some notion of affinity. Graph-based method generally make the assumption that for two data points xi and xj that are connected in this candidate graph Gc, their labels yi and yj are close to each other, where the strength of this assumption depends on the value or weight associated with the relation rxi,xj .\nModels that perform collective inference [4] based on both known and unknown labels find an optimal state of all unknowns variables by optimizing an objective function f over all target variables yu and jointly assigning values to all of them. Thus, collective methods propagate inferred values of the labels based on the relations on the candidate graph. Formally:\ny\u0302u = arg opt yu\nf(yu,yo,Gc;\u03c9)\ne.g., in a probabilistic setting:\ny\u0302u = arg max yu\nP(yu|yo,Gc;\u03c9)\nwhere, \u03c9 = {\u03c91, . . . , \u03c9m} represents the model parameters. Furthermore, we are interested in settings where R can be of different types (i.e., R= {R1, . . . ,Rn}), for example we can have pairwise similarities computed based on different methods or features, or observed relations in a social network with different semantic (friendship, follow, etc.).\nThe task of interest in this paper is given a fixed activation quota q \u2265 1 to dynamically select a subset Rs (such that |Rs| \u2264 q) from all known relations R to improve the performance and scalability of inference. We call the reduced candidate graph with less relations a neighborhood graph (Gn). We select some of the relations rxi,xj from each relation type Rk form candidate graph (Gc) to include in the neighborhood graph (Gn) and discard the rest of relations. Figure 1 shows an overview of our main task.\nMore formally we aim to find an activation method \u03c4 (i.e., inclusion function or map) such that Rs = \u03c4(R,y, q\u03c4 ,\u03c9)\nand |Rs| \u2264 q\u03c4 , where using a graph Gn , \u3008X,Rs\u3009 with Rs instead of R improves the inference\u2019s result, i.e.,\ny\u0302u = arg max yu,Rs\nP(yu|yo, \u3008X,Rs\u3009;\u03c9) (1)\nNext, we describe our proposed methods to maximize the objective in (1)."}, {"heading": "3. PROPOSED GENERAL FRAMEWORK", "text": "Maximizing the objective in (1) is a non-convex combinatorial optimization problem. Hence, we break this objective into three parts; one part to nominate instance (Xn) with unknown labels that require more evidence, another part to activate or select more relations for those nominated instances (Rs), and the third part to jointly infer all unknown labels for instances given the results from the two previous steps.\nTo achieve this we introduce a nomination method \u03b7 such that Xn = \u03b7(X, y\u0302u, q\u03b7) and |Xn| \u2264 q\u03b7 where q\u03b7 indicates the nomination quota. Note that nomination and activation method each have a quota of their own that we can tune. For simplicity we assume that the activation quota is related to the nomination quota by a constant \u03ba (i.e. q\u03c4 = \u03ba\u00d7 q\u03b7), which means for each nominated instance we can activate up to a maximum of \u03ba relations r. We then modify the activation method to depend on the nominated instances such that Rs = \u03c4(\u3008Xn,R\u3009,y, q\u03c4 ,\u03c9)\nThen (1) will be approximated via three components of nomination, activation, and inference as following:\nXn = \u03b7(X, y\u0302u, q\u03b7) Rs = \u03c4(\u3008Xn,R\u3009,y, q\u03c4 ,\u03c9) y\u0302u = arg max\nyu\nP(yu|yo, \u3008X,Rs\u3009;\u03c9)\nDue to obvious dependencies between Xn,Rs, and y\u0302u, we developed an iterative algorithm to preform these steps and update the assignments. Algorithm 1 shows the overall iterative code performing each step.\nAlgorithm 1 LINA Framework\n1: Rs \u2190Rinit 2: \u03c9 \u2190 Learn the model parameters based on Rinit 3: Rs \u2190 \u2205 4: for i \u2208 0 . . . total iterations do 5: yu \u2190 arg max P(yu|yo, \u3008X,Rs\u3009;\u03c9) 6: Xn \u2190 \u03b7(X,yu, q\u03b7) 7: Rs \u2190 \u03c4(\u3008Xn,R\u3009,y, q\u03c4 ,\u03c9) 8: return yu\nWe introduce a set of approaches for nomination method \u03c4 and activation method \u03b7 described in Section 5. We also discuss an approach to learn the model parameters \u03c9, as weights that capture the importance of each relation type Rk in Section 4.2."}, {"heading": "4. HINGE-LOSS MRFS", "text": "The methods introduced in this paper generally apply to most neighborhood graph-based probabilistic models that perform collective inference on all unknown variables. One particular model of interest in this paper is an instance\nof continuous-valued Markov random field models (MRFs) with a strongly convex MAP inference objective function, known as hinge-loss Markov random fields (HL-MRFs) [5]. An HL-MRF is a continuous-valued Markov network in which the potentials are hinge functions of the variables. Our choice of HL-MRFs comes from technical considerations: MAP inference in HL-MRFs is provably and empirically efficient, in theory growing O(N3) with the number of potentials, N , but in practice often converging in O(N) time. Models built using HL-MRFs have achieves state-of-the-art performance for a variety of applications including drug target prediction [2], drug interaction prediction [6] recommender systems [7], student engagement analysis [8], knowledge graph identification [9], and social spammer detection [10]. Finally, HL-MRFs are easily specified through probabilistic soft logic (PSL) [5], a probabilistic programming language with a first-order logic-like syntax.\nA hinge-loss MRF defines a joint probability density function of the form\nP (yu|X,yo) = 1Z exp ( \u2212 M\u2211\nr=1\n\u03c9r\u03c6r(yu,X,yo) ) , (2)\nwhere the entries of target variables yu and observed variables X and yo are in [0, 1], \u03c9 is a vector of weight parameters, Z is a normalization constant, and\n\u03c6r(yu,X,yo) = (max {lr(yu,X,yo), 0})\u03c1r (3)\nis a hinge-loss potential specified by a linear function lr and optional exponent \u03c1r \u2208 {1, 2}. Relaxations of first-order logic rules are one way to derive the linear functions lr(\u00b7) in the hinge-loss potentials \u03c6r. Thus, a set of logical rules described in the PSL framework is a template for an HL-MRF model. Given a collection of logical implications based on domain knowledge described in PSL and a set of observations from data, the rules are instantiated, or grounded out, with known entities in the dataset. Each instantiation of the rules maps to a hinge-loss potential function as in (3), and the potential functions define the HL-MRF model.\nTo illustrate modeling in PSL, we consider a similaritybased rule that encourages transitive closure for link prediction between entities a, b, and c:\nSimilar(a, b) \u2227 Link(b, c)\u2192 Link(a, c)\nwhere instantiations of the predicate Link represent continuous target variables for a link prediction task and instantiations of Similar are continuous observed variables. The convex relaxation of this logical implication derived using the well-known Lukasiewicz logic for continuous truth values is equivalent to the hinge-loss function\nmax(Similar(a, b) + Link(b, c)\u2212 Link(a, c)\u2212 1, 0)\nand can be understood as its distance to satisfaction. The distance to satisfaction of this ground rule is a linear function of the variables and thus, exactly corresponds to\n\u03c6r(Link(b, c),Link(a, c),Similar(a, b))\nthe feature function that scores configurations of assignments to the three variables. Intuitively, distance to satisfaction represents the degree to which the rule is violated by assignments to the random variables conditioned on the observations. We describe MAP inference and parameter\nestimation in HL-MRF models below. Intuitively, MAP inference minimizes the weighted, convex distances to satisfaction to find an consistent joint assignment for all the target variables and the weight parameters convey relative importance of each rule by varying the penalty for violating that rule."}, {"heading": "4.1 MAP Inference", "text": "We perform MAP inference in HL-MRFs to find the best assignment to all target variables given evidence. Formally, the MAP inference objective is of the form\narg max yu\u2208[0,1]n\n1 Z exp ( \u2212 M\u2211\nr=1\n\u03c9r\u03c6r(yu,X,yo) ) (4)\n\u2261 arg min yu\nm\u2211\nr=1\n\u03c9r max{lr(yu,X,yo), 0} (5)\nHL-MRFs has an advantage over other Markov networks since the MAP problem can be solved exactly and in polynomial time as a convex optimization problem. There are many off-the-shelf convex optimization solvers such as interiorpoint methods, but here we use the notable Alternating Direction Method of Multiples algorithm (ADMM) [11]. The ADMM algorithm uses consensus optimization to divide the MAP problem into independent subproblems. For full details on consensus optimization with ADMM for HL-MRF MAP inference, see [5]."}, {"heading": "4.2 Maximum Likelihood Parameter Learning", "text": "Each logical rule in PSL that templates a set of hinge-loss potentials when ground out has an associated weight \u03c9r. The vector of weights \u03c9 are the parameters of an HL-MRF model and can be learned from training data. The canonical approach for parameter estimation is maximum likelihood estimation (MLE) which maximizes the log-likelihood of the training data. The partial derivative of an HL-MRF with respect to any \u03c9r is\n\u2202 logP (yu|X,yo) \u2202\u03c9r =\nE\u03c9\n[\u2211\nj\u2208gr \u03c6r(yu,X,yo)\n] \u2212 \u2211\nj\u2208gr \u03c6r(yu,X,yo)\n(6)\nwhere gr are the groundings of rule r on the training data and E\u03c9 is the expectation of the HL-MRF distribution parameterized by \u03c9. Intuitively, the gradient with respect \u03c9r compares the expected sum of the potentials defined by r to the actual sum based on training data. Smaller gradients indicates better fit to the training data. Gradient descent is performed using the structured voted perceptron algorithm [5]. However, as in other joint models, E\u03c9 is intractable to compute and so we use a common approximation, the values of the potential functions \u03c6r at the MAP state."}, {"heading": "5. METHODS", "text": "We use HL-MRFs for inference and parameter learning of our framework. In this section we discuss our approach for nomination and activation method and explain how we leverage the optimization terms and model parameters in our proposed methods."}, {"heading": "5.1 Nominating Instances", "text": "The nomination phase of our framework selects instances that may benefit from additional evidence. The process of nominating instances is similar to the problem of active learning [12], where instances are labeled based on a utility function. However, in contrast to active learning, nomination does not acquire labels, but selects those instances for which we introduce new relations in the neighborhood graph.\nOur general framework is compatible with arbitrary nomination techniques, allowing us to leverage the diverse active learning strategies developed over the past decades. In addition, our choice of HL-MRFs for modeling inference problems provides the opportunity to use unique nomination strategies that incorporate partial inference outputs and the model state. Here, we present nomination strategies that use inference context and model features to choose instances.\n5.1.1 Model-Aware Nomination Probabilistic models that use a neighborhood graph de-\nfine a rich set of relations between instances that can provide useful structural features when nominating instances. We introduce nomination methods that use these structural features to provide a model-aware nomination method.\nWe build on a method from Pujara et al. [13], which derives features for instance selection from the optimization process underlying inference. In prior work, these features were used in an online inference setting, where instances were selectively updated in response to new evidence. In our setting, we use these model features to determine which instances would benefit from additional evidence.\nPujara et al. [13] observe that model structure (in our setting, relations between instances in the neighborhood graph) translate directly into optimization terms in the inference objective. Features from these optimization terms allow model-based scoring of instances that are difficult to optimize, and thus might benefit from additional evidence. Moreover, since the optimization is central to inference, these features can be generated with little or no overhead.\nThe methods we present identify features from the popular consensus optimization algorithm, the alternating direction method of multipliers (ADMM) [11]. ADMM decomposes the optimization objective into independent subproblems, optimizes each subproblem independently, and introduces a constraint that the all subproblems agree on the optimal value of each inferred variable. This optimization is often expressed using the augmented Lagrangian seen in (7). Here, \u03c9r and \u03c6r are the parameter and potential associated with a given relation r, y\u0303r is the local optimizer of a subproblem and yr is the consensus estimate. Consensus between subproblems is enforced by introducing a Lagrange multiplier, \u03b1r, associated with the constraint, and increasing the optimization penalty, \u03c1, associated with violating this constraint to guarantee eventual convergence.\nmin y\u0303r\n\u03c9r \u03c6r(x, y\u0303r) + \u03c1\n2 \u2225\u2225\u2225y\u0303r \u2212 yr + 1 \u03c1 \u03b1r \u2225\u2225\u2225 2\n(7)\nA useful intuition is that instances where existing relations cause disagreement on a label are useful candidates for nomination. This intuition can be expressed in terms of the Lagrange multipliers associated with each optimization term. At convergence, the value of this Lagrange multiplier captures the disagreement of a given optimization term with the consensus estimate. Thus, by nominating instances as-\nsociated with a potential with high Lagrange multipliers, we can improve our estimate of controversial instances. We use an uncertainty measures based on the Lagrange multipliers from ADMM optimization. The average weighted Lagrange multiplier (AWL) [13], measures the overall discrepancy between the local and consensus copies of the label:\n1 |R| \u2211\nr\u2208R \u03c9r\u03b1r(i) (8)\nwhere R indicates all the local copies of a consensus variable."}, {"heading": "5.2 Activating Relations", "text": "In the relation activation step, we select a subset of relations Rs from the candidate graph Gc to include in the neighborhood graph Gn. Formally, the activated relations Rs = \u03c4(Xn,Gn, q\u03c4 ,\u03c9) are chosen by function \u03c4 given the nominated instances and the set of all relations R. In addition to considering the weight, or value, of each relation edge rkxi,xj , we design \u03c4 to rank relations based on structural properties of the neighborhood graph. At a high level, we use multiple features to score each rkxi,xj and select the top q\u03c4 relations based on their combined scores. In addition to the edge weight feature, we introduce features that use x\u03b1 \u2208 Gn that are incident to rkxi,xj . Intuitively, these structural features measure the informativeness of a relation for inferring multiple unknown instances, and its ability to effectively propagate evidence through Gn.\nFirst we introduce and define two additional structural features along with the relation value feature. Then, we fully describe how \u03c4 combines these features and selects the top q\u03c4 relations from multiple relation types.\n5.2.1 Value Feature We use strength or value associated with a relation edge\nrkxi,xj as a basic feature. Relations of higher value convey a greater dependence between assignments to labels of instances xi and xj. If xi or xj is an instance with known label, then a high valued rkxi,xj effectively propagates that label to the unknown instance.\n5.2.2 Nominated Instance Count Feature For each rkxi,xj in Gn, we compute the number of nomi-\nnated unknown instances Xn that are incident upon r k xi,xj . We require the use of an incident operator I(x\u03b1, r\u03b2) that returns 1 if x\u03b1 shares an endpoint with r k xi,xj and 0 otherwise.\nFormally, the nominated instance count score for rkxi,xj is:\n\u2211 x\u2208Xn I ( x, rkxi,xj )\nwhere we only consider nominated unknown instances incident to rkxi,xj . Intuitively, if many x \u2208 Xn have endpoints in rkxi,xj , then the relation will be informative for many predictions and introduce multiple useful dependencies in the inference step.\n5.2.3 Observed Instance Count Feature For each relation rkxi,xj , we also compute the number of\ninstances with observed labels, instances with yo that are incident to rkxi,xj . Observed links are important because they propagate evidence to unknown link instances through the activated relations. Formally, this feature score for rkxi,xj is:\n\u2211\nx,y|y\u2208yo\nI ( x, rkxi,xj )\nwhere we only consider observed instances incident to rkxi,xj . Higher valued rkxi,xj potentially connect nominated unknown instances Xn to many instances with yo. The observed instances provide valuable evidence for predictions of the unknowns.\n5.2.4 Combining Features and Selecting Relations with \u03c4\nFinally, we require selection function \u03c4 that uses the proposed features over relations to select the most useful q\u03c4 as evidence for predictions of Xn. In our work, for each xi \u2208 Xn we consider the set of relations Ri = {r\u03b2 |I(xi, r\u03b2)} to which unknown instance xi is incident on. For each rkxj ,xl \u2208 Ri, we compute scores for each feature and take the product of scores, which we denote skxj ,xl . We rank Ri by skxj ,xl \u00d7 \u03c9k where \u03c9k is the parameter, or importance, of relation type k learned in Section 4.2. For each xi, we select the top \u03ba relations from Ri. Since q\u03c4 = \u03ba\u00d7 q\u03b7, where q\u03b7 is the number of nominated instances, the activation quota is never exceeded."}, {"heading": "6. GRAPH-BASED LINK PREDICTION", "text": "While the framework proposed in the paper is generally applicable to all neighborhood graph-based models, we focus our arguments on the link prediction task. Inferring information about links is the basis for many machine learning and data science tasks. Link prediction, such as predicting which people would become friends on a social networks or which authors will cite each other in a scholar network; recommender systems, such as predicting which article or item is more relevant to a user; or biological predictions, such as which two drugs will interact with each other, or which drug will interact with a protein are examples of link predictionin different networks. When the inferred labels are binary such as click prediction the task is often called link prediction, and when the label is continuous or multi-valued such as ratings prediction the closely related task is often called link regression [14].\nTo apply a neighborhood graphbased learning method for a link prediction task, the nodes X in the candidate graph Gc , \u3008X,R\u3009 should represent the links in the original data graph Gd, and relations R should represent similarities or relations between links in the original data graph. More formally, let Gd , \u3008V, E\u3009 denote a data graph where V is the set of vertices, and E is the set of edges or links. In a multi-relational network, vertices and edges can be of different types. For example in a drug-target interaction network, V is the set of all drugs and protein targets and E is the set of all the drug-target interactions as well as similarities with different semantics between drugs and between targets. Similarities can be extracted from multiple sources, for example, based on chemical structures of the drugs, or nucleotide sequence of the targets [2].\nIn such settings, X is a subset of E , and R is derived from Gd based on a modeling decision. For example, Kashima et al. [15] use the Kronecker sum and product to derive similarities between links, and Fakhraei et al. [2] use triadic closure principles and define the similarities between links\nbased on the similarities between their end nodes as the following:\nSimilar(v2, v3) \u2227 Link(v1, v2)\u21d2 Link(v1, v3) (9) Similar(v2, v3) \u2227 \u00acLink(v1, v2)\u21d2 \u00acLink(v1, v3) This model achieves state-of-the-art performance in vari-\nous domains such as drug-target interaction prediction [2], drug-drug interaction prediction [6], and hybrid recommender systems [7]. Figure 2 shows an example of creating a candidate graph for links based on the data graph. In this example, the original data graph has three types of edges (ei, ej , ek) and we are interested to infer the values of ei1,2 and ei1,4 based on the other observed edges. In this setting, both nodes and relations in the candidate graph are based on the edges of data graph. e.g., xp = e i 1,2, xq = e i 1,4 and rjxp,xq = r j\nei1,2,e i 1,4\n= ej2,4.\nFor Gn in the link prediction setting, we define I(x\u03b1, r\u03b2) as\nI ( x\u03b1 = e i s,t, r j \u03b2 = e j u,v ) = { 1 if {s, t} \u2229 {u, v} 6= \u2205 0 otherwise (10)\nLink instance x\u03b1 is incident to r j \u03b2 = e j u,v if it has an end point at either u or v. Intuitively, links are incident to relations via the nodes connected by the relation."}, {"heading": "7. EXPERIMENTAL VALIDATION", "text": "In this section we present preliminary results of the main components of our framework, activation and nomination methods, on a link prediction dataset. In this dataset we have two types of nodes (drugs and targets), and the task is given the similarities between nodes and partially observed interactions (i.e, links) between them, to predict the held out set of interactions in the network. We use 10-fold cross validation for our experiments where we hold out 10% of the observed links (i.e., positive class) and use the rest as observed instance to predict their values. We also samples 10% of the absent or missing links (i.e., negative class) and include them in each held out fold. Due to high class imbalance in link prediction tasks, the most informative performance measure is the precision and recall of the minority\npositive class (the presence of link). Therefore, we evaluate our methods based on Area Under the Precision-Recall Curve (AUPR).\nThe following sections describes the dataset and present our primarily results on it."}, {"heading": "7.1 Dataset", "text": "In this dataset we want to predict new interactions between drug compounds and target proteins. We follow the link predictionmodeling approach described in Section 6 and proposed by Fakhraei et al. [2]. We use known interactions and biologically relevant similarity relations to predict heldout interactions. We describe the interactions and similarities used for our experimental evaluation below.\n7.1.1 Drug-Target Interactions The interactions between drugs and target are gathered\nfrom Drugbank, KEGG Drug, Drug Combination Database (DCB) and Matador. The dataset includes 1,306 known interactions between 315 drugs and 250 targets. We use five types of similarity between each pair of drugs and three types of similarity for each pair of targets. We describe each briefly below. For full details, refer to [2]. In this dataset the ratio of positive class (i.e., links presence y = 1) to negative class (i.e., link absence y = 0) is 1.6%.\nBetween drug similarities for this dataset include the following: Chemical-based relations obtained using the chemical development kit (CDK) and compare the chemical structure of the drug molecules, Ligand-based relations computed with the similarity ensemble approach (SEA) search tool and measure the closeness between protein-receptor families for each drug, Expression-based relations obtained from the Connectivity Map Project and compare gene expression levels in response to the administration of each drug, Sideeffect-based relations acquired from the SIDER database and compare the reported side-effects for each drug, and Annotation-based relations from the World Health Organization ATC classification system that compares ontological characterizations of drugs.\nBetween target similarities for this dataset include Sequencebased relations computed using the Smith-Waterman sequencealignment procedure and measure the goodness of alignment between the genetic codes of each target, Proteinprotein interaction network-based relations computed using the protein-protein interaction network in humans and compare the graph distance between proteins encoded by each target gene, Gene Ontology-based relations obtained by downloading Gene Ontology annotations from UniProt and compare the semantic similarity between genes based on their ontological classification."}, {"heading": "7.2 Results", "text": "We use a baseline of selecting k relations or similarities for all instances, and increasing the k at each step. It is important to note that this baseline does not have a nomination quota and basically nominates all instance to receive more relations at each step. Our nomination method in contrast is limited by a quota an can not explore the space as freely as the selected baseline. Figure 3a depicts the performance of only the average weighted Lagrange multiplier (AWL) nomination method with quota of 10% in comparison with the baseline on the drug-target interaction dataset. In this setting, limitation imposed on the search space by AWL nomi-\nnation method improves the link prediction performance. It is also notable that number of selected similarities at each step by using the nomination method is less than the baseline. Figure 3b shows the performance of only the activation method in comparison to the baseline method. In this setting although all instance were nominated to get more relations, the relations with higher activation score were prioritized to be included in the neighborhood graph, the performance achieved via this method is even higher than the nomination method. It is also notable that the number of similarities selected in this experiment is less than the number of similarities of the baseline method in Figure 3a, which can suggest limiting the search space based on the relation can be more restrictive.\nThe nomination method focuses the search by prioritizing the instance to get more relations, while the activation method directs the search by prioritizing which relations to be selected for the neighborhood graph. Figure 3c shows using combination of nomination and activation methods together, where it achieves higher performances with less numbers of similarities in the beginning and reduces the number of selected similarities in later iterations"}, {"heading": "8. DISCUSSION AND CONCLUSION", "text": "In this paper, we highlight the limitations of sequential neighborhood graph construction and introduce a general unified framework to dynamically construct multi-relational neighborhood graphs during inference. We base our dynamic neighborhood graph construction on the states of variables from intermediate inference results, the structural properties of the relations connecting them, and weight parameters learned by the model. We then formulate the general link prediction task as inference on neighborhood graphs, and present initial results on a drug-target interaction network showing effectiveness of our methods. In future work, we plan to extend our studies with experiments on more datasets with various characteristics, experiments with different parameter learning setups, and considering additional methods for the components of our framework such as value-based and probabilistic nomination methods, and label and other structural-based activation methods."}, {"heading": "Acknowledgements", "text": "This work is partially supported by the National Science Foundation (NSF) under contract number IIS0746930. Any opinions, findings, and conclusions or recommendations expressed in this material are those of the author(s) and do not necessarily reflect the views of the supporting institutions."}], "references": [{"title": "Graph construction and b-matching for semi-supervised learning", "author": ["Tony Jebara", "Jun Wang", "Shih-Fu Chang"], "venue": "In Proceedings of the 26th Annual International Conference on Machine Learning,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2009}, {"title": "Network-based drug-target interaction prediction with probabilistic soft logic", "author": ["Shobeir Fakhraei", "Bert Huang", "Louiqa Raschid", "Lise Getoor"], "venue": "IEEE/ACM Transactions on Computational Biology and Bioinformatics,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2014}, {"title": "Some new directions in graph-based semi-supervised learning", "author": ["Xiaojin Zhu", "Andrew B Goldberg", "Tushar Khot"], "venue": "In Multimedia and Expo,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2009}, {"title": "Collective classification of network data", "author": ["Ben London", "Lise Getoor"], "venue": "Data Classification: Algorithms and Applications. CRC Press,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2013}, {"title": "Hinge-loss markov random fields and probabilistic soft logic", "author": ["Stephen H Bach", "Matthias Broecheler", "Bert Huang", "Lise Getoor"], "venue": "arXiv preprint arXiv:1505.04406,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2015}, {"title": "A probabilistic approach for collective similarity-based drug-drug interaction prediction", "author": ["Dhanya Sridhar", "Shobeir Fakhraei", "Lise Getoor"], "venue": null, "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2016}, {"title": "Hyper: A flexible and extensible probabilistic framework for hybrid recommender systems", "author": ["Pigi Kouki", "Shobeir Fakhraei", "James Foulds", "Magdalini Eirinaki", "Lise Getoor"], "venue": "In 9th ACM Conference on Recommender Systems (RecSys). ACM,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2015}, {"title": "Learning latent engagement patterns of students in online courses", "author": ["Arti Ramesh", "Dan Goldwasser", "Bert Huang", "Hal Daume III", "Lise Getoor"], "venue": "In Proceedings of the Twenty-Eighth AAAI Conference on Artificial Intelligence. AAAI Press,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2014}, {"title": "Knowledge graph identification", "author": ["Jay Pujara", "Hui Miao", "Lise Getoor", "William Cohen"], "venue": "In International Semantic Web Conference (ISWC),", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2013}, {"title": "Collective spammer detection in evolving multi-relational social networks", "author": ["Shobeir Fakhraei", "James Foulds", "Madhusudana Shashanka", "Lise Getoor"], "venue": "In ACM SIGKDD Conference on Knowledge Discovery and Data Mining (KDD). ACM, ACM Press,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2015}, {"title": "Distributed optimization and statistical learning via the alternating direction method of multipliers", "author": ["S. Boyd", "N. Parikh", "E. Chu", "B. Peleato", "J. Eckstein"], "venue": "Foundations and Trends Machine Learning,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2011}, {"title": "Active learning", "author": ["Burr Settles"], "venue": "Synthesis Lectures on Artificial Intelligence and Machine Learning,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2012}, {"title": "Budgeted online collective inference", "author": ["Jay Pujara", "Ben London", "Lise Getoor"], "venue": "In Uncertainty in Artificial Intelligence,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2015}, {"title": "Relational learning via collective matrix factorization", "author": ["Ajit P Singh", "Geoffrey J Gordon"], "venue": "In Proceedings of the 14th ACM SIGKDD international conference on Knowledge discovery and data mining,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2008}, {"title": "Link propagation: A fast semi-supervised learning algorithm for link prediction", "author": ["Hisashi Kashima", "Tsuyoshi Kato", "Yoshihiro Yamanishi", "Masashi Sugiyama", "Koji Tsuda"], "venue": "In SDM,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2009}], "referenceMentions": [{"referenceID": 0, "context": "Most methods that make predictions based on a neighborhood graph can be characterized in terms of three basic operations [1]: Candidate graph Generation, Selection and Inference.", "startOffset": 121, "endOffset": 124}, {"referenceID": 0, "context": "Examples of these methods that are often considered a pre-processing step to inference include knearest neighbors, -neighborhood selection, and b-matching [1].", "startOffset": 155, "endOffset": 158}, {"referenceID": 1, "context": "[2] show this negative effect in a drug target prediction setting.", "startOffset": 0, "endOffset": 3}, {"referenceID": 2, "context": "Methods such as Mincut, graph random walk, Gaussian random fields, local and global consistency, spectral graph transducer, manifold regularization, and label propagation are examples that perform inference on the neighborhood graph [3].", "startOffset": 233, "endOffset": 236}, {"referenceID": 3, "context": "Models that perform collective inference [4] based on both known and unknown labels find an optimal state of all unknowns variables by optimizing an objective function f over all target variables yu and jointly assigning values to all of them.", "startOffset": 41, "endOffset": 44}, {"referenceID": 4, "context": "One particular model of interest in this paper is an instance of continuous-valued Markov random field models (MRFs) with a strongly convex MAP inference objective function, known as hinge-loss Markov random fields (HL-MRFs) [5].", "startOffset": 225, "endOffset": 228}, {"referenceID": 1, "context": "Models built using HL-MRFs have achieves state-of-the-art performance for a variety of applications including drug target prediction [2], drug interaction prediction [6] recommender systems [7], student engagement analysis [8], knowledge graph identification [9], and social spammer detection [10].", "startOffset": 133, "endOffset": 136}, {"referenceID": 5, "context": "Models built using HL-MRFs have achieves state-of-the-art performance for a variety of applications including drug target prediction [2], drug interaction prediction [6] recommender systems [7], student engagement analysis [8], knowledge graph identification [9], and social spammer detection [10].", "startOffset": 166, "endOffset": 169}, {"referenceID": 6, "context": "Models built using HL-MRFs have achieves state-of-the-art performance for a variety of applications including drug target prediction [2], drug interaction prediction [6] recommender systems [7], student engagement analysis [8], knowledge graph identification [9], and social spammer detection [10].", "startOffset": 190, "endOffset": 193}, {"referenceID": 7, "context": "Models built using HL-MRFs have achieves state-of-the-art performance for a variety of applications including drug target prediction [2], drug interaction prediction [6] recommender systems [7], student engagement analysis [8], knowledge graph identification [9], and social spammer detection [10].", "startOffset": 223, "endOffset": 226}, {"referenceID": 8, "context": "Models built using HL-MRFs have achieves state-of-the-art performance for a variety of applications including drug target prediction [2], drug interaction prediction [6] recommender systems [7], student engagement analysis [8], knowledge graph identification [9], and social spammer detection [10].", "startOffset": 259, "endOffset": 262}, {"referenceID": 9, "context": "Models built using HL-MRFs have achieves state-of-the-art performance for a variety of applications including drug target prediction [2], drug interaction prediction [6] recommender systems [7], student engagement analysis [8], knowledge graph identification [9], and social spammer detection [10].", "startOffset": 293, "endOffset": 297}, {"referenceID": 4, "context": "Finally, HL-MRFs are easily specified through probabilistic soft logic (PSL) [5], a probabilistic programming language with a first-order logic-like syntax.", "startOffset": 77, "endOffset": 80}, {"referenceID": 0, "context": "where the entries of target variables yu and observed variables X and yo are in [0, 1], \u03c9 is a vector of weight parameters, Z is a normalization constant, and", "startOffset": 80, "endOffset": 86}, {"referenceID": 0, "context": "arg max yu\u2208[0,1]n 1 Z exp ( \u2212 M \u2211", "startOffset": 11, "endOffset": 16}, {"referenceID": 10, "context": "There are many off-the-shelf convex optimization solvers such as interiorpoint methods, but here we use the notable Alternating Direction Method of Multiples algorithm (ADMM) [11].", "startOffset": 175, "endOffset": 179}, {"referenceID": 4, "context": "For full details on consensus optimization with ADMM for HL-MRF MAP inference, see [5].", "startOffset": 83, "endOffset": 86}, {"referenceID": 4, "context": "Gradient descent is performed using the structured voted perceptron algorithm [5].", "startOffset": 78, "endOffset": 81}, {"referenceID": 11, "context": "The process of nominating instances is similar to the problem of active learning [12], where instances are labeled based on a utility function.", "startOffset": 81, "endOffset": 85}, {"referenceID": 12, "context": "[13], which derives features for instance selection from the optimization process underlying inference.", "startOffset": 0, "endOffset": 4}, {"referenceID": 12, "context": "[13] observe that model structure (in our setting, relations between instances in the neighborhood graph) translate directly into optimization terms in the inference objective.", "startOffset": 0, "endOffset": 4}, {"referenceID": 10, "context": "The methods we present identify features from the popular consensus optimization algorithm, the alternating direction method of multipliers (ADMM) [11].", "startOffset": 147, "endOffset": 151}, {"referenceID": 12, "context": "The average weighted Lagrange multiplier (AWL) [13], measures the overall discrepancy between the local and consensus copies of the label:", "startOffset": 47, "endOffset": 51}, {"referenceID": 13, "context": "When the inferred labels are binary such as click prediction the task is often called link prediction, and when the label is continuous or multi-valued such as ratings prediction the closely related task is often called link regression [14].", "startOffset": 236, "endOffset": 240}, {"referenceID": 1, "context": "Similarities can be extracted from multiple sources, for example, based on chemical structures of the drugs, or nucleotide sequence of the targets [2].", "startOffset": 147, "endOffset": 150}, {"referenceID": 14, "context": "[15] use the Kronecker sum and product to derive similarities between links, and Fakhraei et al.", "startOffset": 0, "endOffset": 4}, {"referenceID": 1, "context": "[2] use triadic closure principles and define the similarities between links", "startOffset": 0, "endOffset": 3}, {"referenceID": 1, "context": "This model achieves state-of-the-art performance in various domains such as drug-target interaction prediction [2], drug-drug interaction prediction [6], and hybrid recommender systems [7].", "startOffset": 111, "endOffset": 114}, {"referenceID": 5, "context": "This model achieves state-of-the-art performance in various domains such as drug-target interaction prediction [2], drug-drug interaction prediction [6], and hybrid recommender systems [7].", "startOffset": 149, "endOffset": 152}, {"referenceID": 6, "context": "This model achieves state-of-the-art performance in various domains such as drug-target interaction prediction [2], drug-drug interaction prediction [6], and hybrid recommender systems [7].", "startOffset": 185, "endOffset": 188}, {"referenceID": 1, "context": "[2].", "startOffset": 0, "endOffset": 3}, {"referenceID": 1, "context": "For full details, refer to [2].", "startOffset": 27, "endOffset": 30}], "year": 2016, "abstractText": "A neighborhood graph, which represents the instances as vertices and their relations as weighted edges, is the basis of many semi-supervised and relational models for node labeling and link prediction. Most methods employ a sequential process to construct the neighborhood graph. This process often consists of generating a candidate graph, pruning the candidate graph to make a neighborhood graph, and then performing inference on the variables (i.e., nodes) in the neighborhood graph. In this paper, we propose a framework that can dynamically adapt the neighborhood graph based on the states of variables from intermediate inference results, as well as structural properties of the relations connecting them. A key strength of our framework is its ability to handle multi-relational data and employ varying amounts of relations for each instance based on the intermediate inference results. We formulate the link prediction task as inference on neighborhood graphs, and include preliminary results illustrating the effects of different strategies in our proposed framework.", "creator": "LaTeX with hyperref package"}}}