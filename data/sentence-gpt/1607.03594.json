{"id": "1607.03594", "review": {"conference": "AAAI", "VERSION": "v1", "DATE_OF_SUBMISSION": "13-Jul-2016", "title": "Estimating Uncertainty Online Against an Adversary", "abstract": "Assessing uncertainty within machine learning systems is an important step towards ensuring their safety and reliability. Existing uncertainty estimation techniques may fail when their modeling assumptions are not met, e.g. by relying on a model-based approach. For example, the task of creating and interpreting prediction data is difficult to test on in large numbers. But we can identify uncertainty that does not exist, for example, if the model-based modeling system is to be able to accurately estimate the value of an object's uncertainty. To be precise, the accuracy of a prediction-based model does not depend on the type of prediction in the dataset.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n", "histories": [["v1", "Wed, 13 Jul 2016 05:07:33 GMT  (1202kb,D)", "https://arxiv.org/abs/1607.03594v1", null], ["v2", "Sun, 22 Jan 2017 04:25:30 GMT  (543kb,D)", "http://arxiv.org/abs/1607.03594v2", null]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["volodymyr kuleshov", "stefano ermon"], "accepted": true, "id": "1607.03594"}, "pdf": {"name": "1607.03594.pdf", "metadata": {"source": "CRF", "title": "Estimating Uncertainty Online Against an Adversary", "authors": ["Volodymyr Kuleshov", "Stefano Ermon"], "emails": ["kuleshov@cs.stanford.edu", "ermon@cs.stanford.edu"], "sections": [{"heading": "Introduction", "text": "Assessing uncertainty is an important step towards ensuring the safety and reliability of machine learning systems. In many applications of machine learning \u2014 including medical diagnosis (Jiang et al. 2012), natural language understanding (Nguyen and O\u2019Connor 2015), and speech recognition (Yu, Li, and Deng 2011) \u2014 assessing confidence can be as important as obtaining high accuracy. This work explores confidence estimation for classification problems.\nAn important limitation of existing methods is the assumption that data is sampled i.i.d. from a distribution P(x, y); when test-time data is distributed according to a different P\u2217, these methods may become overconfident and erroneous. Here, we introduce new, robust uncertainty estimation algorithms guaranteed to produce reliable confidence estimates on out-of-distribution input, including input generated by an adversary.\nIn the classification setting, the most natural way of measuring an algorithm\u2019s uncertainty is via calibrated probability estimates that match the true empirical frequencies of an outcome. For example, if an algorithm predicted a 60% chance of rain 100 times in a given year, its forecast would be calibrated if it rained on about 60 of those 100 days.\nCopyright c\u00a9 2017, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved.\nBackground. Calibrated confidence estimates are typically constructed via recalibration, using methods such as Platt scaling (Platt 1999) or isotonic regression (NiculescuMizil and Caruana 2005). In the context of binary classification, these methods reduce recalibration to a onedimensional regression problem that, given data (xi, yi)ni=1, trains a model g(s) (e.g. logistic regression) to predict probabilities pi = g(si) from uncalibrated scores si = h(xi) produced by a classifier h (e.g. SVM margins). Fitting g is equivalent to performing density estimation targeting P(Y = 1|h(X) = si) and hence may fail on out-ofdistribution testing data.\nThe methods we introduce in this work are instead based on calibration techniques developed in the literature on online learning in mathematical games (Foster and Vohra 1998; Abernethy, Bartlett, and Hazan 2011). These classical methods are not suitable for standard prediction tasks in their current form. For one, they do not admit covariates xi that might be available to improve the prediction of yi; hence, they also do not consider the predictive power of the forecasts. For example, predicting 0.5 on a sequence 01010... formed by alternating 0s and 1s is considered a valid calibrated forecaster. The algorithms we present here combine the advantages of online calibration (adversarial assumptions), and of batch probability recalibration (covariates and forecast sharpness).\nOnline learning with uncertainty. Whereas classical online optimization aims to accurately predict targets y given x (via a convex loss `(x, y)), our algorithms aim to accurately predict uncertainties p(y = y\u0302). The p here are defined as empirical frequencies over data seen so far; it turns out that these probability-like quantities can be estimated under the standard adversarial assumptions of online learning. We thus see our work as extending classical online optimization to handle uncertainty in addition to guaranteeing accuracy.\nExample. As a concrete motivating example, consider a medical system that diagnoses a long stream of patients indexed by t = 1, 2, ..., outputting a disease risk pt \u2208 [0, 1] for each patient based on their medical record xt. Provably calibrated probabilities in this setting may be helpful for making informed policy decisions (e.g. by providing guaranteed up-\nar X\niv :1\n60 7.\n03 59\n4v 2\n[ cs\n.L G\n] 2\n2 Ja\nn 20\n17\nper bounds on the number of patients readmitted after a discharge) and may be used to communicate risks to patients in a more intuitive way. This setting is also inherently online, since patients are typically observed one at a time, and may not be i.i.d. due to e.g., seasonal disease outbreaks.\nContributions. More formally, our contributions are to: \u2022 Formulate a new problem called online recalibration,\nwhich requires producing calibrated probabilities on potentially adversarial input, while retaining the predictive power of a given baseline uncalibrated forecaster.\n\u2022 Propose a meta-algorithm for online recalibration that uses classical online calibration as a black box subroutine.\n\u2022 Show that our technique can recalibrate the forecasts of any existing classifier at the cost of an O(1/ \u221a ) overhead\nin the convergence rate of A, where > 0 is the desired level of accuracy.\n\u2022 Surprisingly, both online and standard batch recalibration (e.g., Platt scaling) may be performed only when accuracy is measured using specific loss functions; our work characterizes the losses which admit a recalibration procedure in both the online and batch settings."}, {"heading": "Background", "text": "Below, we will use IE denote the indicator function of E, [N ] and [N ]0 to (respectively) denote the sets {1, 2, ..., N} and {0, 1, 2, ..., N}, and \u2206d to denote the d-dimensional simplex."}, {"heading": "Learning with Expert Advice", "text": "Learning with expert advice (Cesa-Bianchi and Lugosi 2006) is a special case of the general online optimization framework (Shalev-Shwartz 2007) that underlies online calibration algorithms. At each time t = 1, 2, ..., the forecaster F receives advice from N experts and chooses a distribution wt \u2208 \u2206N\u22121 over their advice. Nature then reveals an outcome yt and F incurs an expected loss of\u2211N i=1 wti`(yt, ait), where `(yt, ait) is the loss under expert i\u2019s advice ait. Performance in this setting is measured using two notions of regret.\nDefinition 1. The external regret RextT and the internal regret RintT are defined as\nRextT = T\u2211 t=1 \u00af\u0300(yt, pt)\u2212 min i\u2208[N ] T\u2211 t=1 `(yt, ait)\nRintT = max i,j\u2208[N ] T\u2211 t=1 pt,i (`(yt, ait)\u2212 `(yt, ajt)) ,\nwhere \u00af\u0300(y, p) = \u2211N i=1 pi`(y, ait) is the expected loss.\nExternal regret measures loss with respect to the best fixed expert, while internal regret is a stronger notion that measures the gain from retrospectively switching all the plays of action i to j. Both definitions admit algorithms with sublinear, uniformly bounded regret.\nIn this paper, we will be particularly interested in proper losses `, whose expectation over y is minimized by the probability corresponding to the average y.\nDefinition 2. A loss `(y, p) : {0, 1}\u00d7 [0, 1]\u2192 R+ is proper if p \u2208 arg minq Ey\u223cBer(p)`(y, q) \u2200p.\nExamples of proper losses include the L2 loss `2(y, p) = (y\u2212p)2, the log-loss `log(y, p) = y log(p)+(1\u2212y) log(1\u2212 p), and the the misclassification loss `mc(y, p) = (1 \u2212 y)Ip<0.5 + yIp\u22650.5. Counter-examples include the L1 and the hinge losses."}, {"heading": "Calibration in Online Learning", "text": "Intuitively, calibration means that the true and predicted frequencies of an event should match. For example, if an algorithm predicts a 60% chance of rain 100 times in a given year, then we should see rain on about 60 of those 100 days. More formally, let F cal be a forecaster making predictions in the set { iN | i = 0, ..., N}, where 1/N is called the resolu-\ntion of F cal; consider the quantities \u03c1T (p) = \u2211T t=1 ytIpt=p\u2211T t=1 Ipt=p and\nCpT = N\u2211 i=0 \u2223\u2223\u2223\u2223\u03c1T (i/N)\u2212 iN \u2223\u2223\u2223\u2223p ( 1 T T\u2211 t=1 I{pt= iN } ) . (1)\nThe term \u03c1T (p) denotes the frequency at which event y = 1 occurred over the times when we predicted p. Our intuition was that \u03c1T (p) and p should be close to each other; we capture this using the notion of calibration error CpT for p \u2265 1; this corresponds to the weighted `p distance between the \u03c1T (i/N) and the predicted probabilities iN ; typically one assumes that p = 1 or p = 2. To simplify notation, we will use the term CT when the exact p is unambiguous.\nDefinition 3. We say that F cal is an ( , `p)-calibrated algorithm with resolution 1/N if lim supT\u2192\u221e C p T \u2264 a.s.\nThere exists a vast literature on calibration in the online setting (Cesa-Bianchi and Lugosi 2006) which is primarily concerned with constructing calibrated predictions pt \u2208 [0, 1] of a binary outcome yt \u2208 {0, 1} based solely\non the past sequence y1, ..., yt\u22121. Surprisingly, this is possible even when the yt are chosen adversarially by reducing the problem to internal regret minimization relative to N + 1 experts with losses (yt\u2212 i/N)2 and proposed predictions i/N for i \u2208 [N ]0. All such algorithms are randomized, hence our results will hold almost surely (a.s.). See Chapter 4 in Cesa-Bianchi and Lugosi for details."}, {"heading": "Online Recalibration", "text": "Unfortunately, existing online calibration methods are not directly applicable in real-world settings. For one, they do not take into account covariates xt that might be available to improve the prediction of yt. As a consequence, they cannot produce accurate forecasts: for example, they would constantly predict 0.5 on a sequence 01010... formed by alternating 0s and 1s.\nTo address these shortcomings, we define here a new problem called online recalibration, in which the task is to transform a sequence of uncalibrated forecasts pFt into predictions pt that are calibrated and almost as accurate as the original pFt . The forecasts p F t may come from any existing machine learning system F ; our methods treat it as a black box and preserve its favorable convergence properties.\nFormally, we define the online recalibration task as a generalization of the classical online optimization framework (Shalev-Shwartz 2007; Cesa-Bianchi and Lugosi 2006). At every step t = 1, 2, ...:\n1: Nature reveals features xt \u2208 Rd. 2: Forecaster F predicts pFt = \u03c3(wt\u22121 \u00b7 xt) \u2208 [0, 1]. 3: A recalibration algorithm A produces a calibrated prob-\nability pt = A(pFt ) \u2208 [0, 1]. 4: Nature reveals label yt \u2208 {0, 1}; F incurs loss of `(yt, pt), where ` : [0, 1] \u00d7 {0, 1} \u2192 R+ is convex in pt for all yt.\n5: F chooses wt+1; A updates itself based on yt. Here, \u03c3 is a transfer function chosen such that the task is convex in wt. In the medical diagnosis example, xt represents medical or genomic features for patient t; we use feature weights wt to predict the probability pFt that the patient is ill; the true outcome is encoded by yt. We would like A to produce pFt that are accurate and well-calibrated in the following sense. Definition 4. We say that A is an ( , `cal)-accurate online recalibration algorithm for the loss `acc if (a) the forecasts pt = A(p F t ) are ( , `\ncal)-calibrated and (b) the regret of pt with respect to pFt is a.s. small in terms of ` acc:\nlim sup T\u2192\u221e\n1\nT T\u2211 t=1 ( `acc(yt, pt)\u2212 `acc(yt, pFt ) ) \u2264 . (2)"}, {"heading": "Algorithms for Online Recalibration", "text": "Next, we propose an algorithm for performing online probability recalibration; we refer to our approach as a metaalgorithm because it repeatedly invokes a regular online calibration algorithm as a black-box subroutine. Algorithm 1 outlines this procedure.\nAt a high level, Algorithm 1 partitions the uncalibrated forecasts pFt into M buckets/intervals I =\nAlgorithm 1 Online Recalibration\nRequire: Online calibration subroutine F cal and number of buckets M\n1: Let I = {[0, 1M ), [ 1 M , 2 M ), ..., [ M\u22121 M , 1]} be a set of in-\ntervals that partition [0, 1]. 2: Let F = {F calj | j = 0, ...,M \u2212 1} be a set of M\nindependent instances of F cal. 3: for t = 1, 2, ...: do 4: Observe uncalibrated forecast pFt . 5: Let Ij \u2208 I be the interval containing pFt . 6: Let pt be the forecast of F calj . 7: Output pt. Observe yt and pass it to F calj .\n{[0, 1M ), [ 1 M , 2 M ), ..., [ M\u22121 M , 1]}; it trains an independent instance of F cal on the data {pFt , yt | pFt \u2208 Ij} belonging to each bucket Ij \u2208 I; at prediction time, it calls the instance of F cal associated with the bucket of the uncalibrated forecast pFt .\nAlgorithm 1 works because a calibrated predictor is at least as accurate as any constant predictor; in particular, each subroutine F calj is at least as accurate as the prediction j M , which also happens to be approximately pFt when F cal j was called. Thus, each F calj is as accurate as its input sequence of pFt . One can then show that if each each F cal j is accurate and calibrated, then so it their aggrgate, Algorithm 1. The rest of this section provides a formal version of this argument; due to space limitations, we defer most of our full proofs to the appendix."}, {"heading": "Calibration and Accuracy of Online Recalibration", "text": "Notation. We define the calibration error of F calj and of Algorithm 1 at i/N as (respectively)\nC (j) T,i = \u2223\u2223\u2223\u2223\u03c1(j)T (i/N)\u2212 iN \u2223\u2223\u2223\u2223p ( 1 Tj T\u2211 t=1 I(j)t,i )\nCT,i = \u2223\u2223\u2223\u2223\u03c1T (i/N)\u2212 iN \u2223\u2223\u2223\u2223p ( 1 T T\u2211 t=1 It,i ) ,\nwhere It,i = I{pt = i/N}. Terms marked with a (j) denote the restriction of the usual definition to the input of subroutine F calj (see the appendix for details). We may write the calibration losses of F calj and Algorithm 1 as C (j) T =\u2211N\ni=0 C (j) T,i and CT = \u2211N i=0 CT,i.\nAssumptions. In this section, we will assume that the subroutine F cal used in Algorithm 1 is ( , `1)-calibrated and that C(j)Tj \u2264 RTj + uniformly (RTj = o(1) as Tj \u2192\u221e; Tj is the number of calls to instance F calj ). This also implies `pcalibration (by continuity of `p), albeit with different rates RTj and a different . Abernethy, Bartlett, and Hazan introduce ( , `1)-calibrated Fj . We also provide proofs for the `2 loss in the appendix.\nCrucially, we assume that the loss ` used for measuring accuracy is proper and bounded with `(\u00b7, i/N) < B for i \u2208 [N ]0; since the set of predictions is finite, this is a mild requirement. Finally, we make additional continuity assumptions on ` in Lemma 2.\nRecalibration with proper losses. Surprisingly, not every loss ` admits a recalibration procedure. Consider, for example, the following continuously repeating sequence 001001001... of yt\u2019s. A calibrated forecaster must converge to predicting 1/3 (a constant prediction) with an `1 loss of \u22480.44; however predicting 0 for all t has an `1 loss of 1/3 < 0.44. Thus we cannot recalibrate this sequence and also remain equally accurate under the `1 loss. The same argument also applies to batch recalibration (e.g. Platt scaling): we only need to assume that yt \u223c Ber(1/3) i.i.d.\nHowever, recalibration is possible for a very large class of proper losses. Establishing this fact will rely on the following key technical lemma. Lemma 1. If ` is a proper loss bounded by B > 0, then an ( , `1)-calibrated F cal a.s. has a small internal regret w.r.t. ` and satisfies uniformly over time T the bound\nRintT = max ij T\u2211 t=1 Ipt=i/N (`(yt, i/N)\u2212 `(yt, j/N)) \u2264 2B(RT + ).\nAccording to Lemma 1, if a set of predictions is calibrated, then we never want to retrospectively switch to predicting p2 at times when we predicted p1. Intuitively, this makes sense: if predictions are calibrated, then p1 should minimize the total (or average) loss \u2211 t:pt=p1\n`(yt, p) over the times t when p1 was predicted (at least better so than p2). However, our `1 counter-example above shows that this intuition does not hold for every loss; we need to explicitly enforce our intuition, which amounts to assuming that ` is proper, i.e. that p \u2208 arg minq Ey\u223cBer(p)`(y, q).\nAccuracy and calibration. An important consequence of Lemma 1 is that a calibrated algorithm has vanishing regret relative to any fixed prediction (since minimizing internal regret also minimizes external regret). Using this fact, it becomes straightforward to establish that Algorithm 1 is at least as accurate as the baseline forecaster F . Lemma 2 (Recalibration preserves accuracy). Consider Algorithm 1 with parameters M \u2265 N > 1/ and let ` be a bounded proper loss for which\n1. `(yt, p) \u2264 `(yt, j/M)+B/M for p \u2208 [j/M, (j+1)/M); 2. `(yt, p) \u2264 `(yt, i/N) +B/N for p \u2208 [i/N, (i+ 1)/N); Then the recalibrated pt a.s. have vanishing `-loss regret relative to pFt and we have uniformly:\n1\nT T\u2211 t=1 `(yt, pt)\u2212 1 T T\u2211 t=1 `(yt, p F t ) < NB M\u2211 j=1 Tj T RTj +3B .\nProof (sketch). When pt is the output of a given Fj , we have `(yt, p F t ) \u2248 `(yt, j/M) \u2248 `(yt, ij/M) (since pFt is in the jth bucket, and since M \u2265 N is sufficiently high resolution).\nSince Fj is calibrated, Lemma 1 implies the pt have vanishing regret relative to the fixed prediction ij/N ; aggregating over j yields our result.\nThe assumptions of Lemma 2 essentially require that ` be Lipschitz with constant B, which holds e.g. for convex bounded losses that are studied in online learning. Our assumption is slightly more general since ` may also be discontinuous (like the misclassification loss). When ` is unbounded (like the log-loss), its values at the baseline algorithm\u2019s predictions must be bounded away from infinity.\nNext, we also establish that combining the predictions of each F calj preserves their calibration.\nLemma 3 (Preserving calibration). If each F calj is ( , `p)calibrated, then Algorithm 1 is also ( , `p)-calibrated and the bound CT \u2264 \u2211M j=1 Tj T RTj + holds uniformly over T .\nThese two lemmas lead to our main claim: that Algorithm 1 solves the online recalibration problem.\nTheorem 1. Let F cal be an (`1, /3B)-calibrated online subroutine with resolution N \u2265 3B/ . and let ` be a proper loss satisfying the assumptions of Lemma 2. Then Algorithm 1 with parameters F cal and M = N is an -accurate online recalibration algorithm for the loss `.\nProof. By Lemma 3, Algorithm 1 is (`1, /3B)-calibrated and by Lemma 2, its regret w.r.t. the raw pFt tends to < 3B/N < . Hence, Theorem 1 follows.\nIn the appendix, we provide a detailed argument for how ` can be chosen to be the misclassificaiton loss.\nInterestingly, it also turns out that if ` is not a proper loss, then recalibration is not possible for some > 0.\nTheorem 2. If ` is not proper, then no algorithm achieves recalibration w.r.t. ` for all > 0.\nThe proof of this algorithm is a slight generalization of the counter-example provided for the `1 loss. Interestingly, it holds equally for online and batch settings. To our knowledge, it is one of the first characterizations of the limitations of recalibration algorithms.\nConvergence rates. Next, we are interested in the rate of convergence RT of the calibration error CT of Algorithm 1. For most online calibration subroutines F cal, RT \u2264 f( )/ \u221a T for some f( ). In such cases, we can further\nbound the calibration error in Lemma 3 as \u2211M j=1 Tj T RTj \u2264\nj=1\n\u221a\nTjf( )\nT \u2264\nf( )\u221a\nT\n. In the second inequality, we set the\nTj to be equal. Thus, our recalibration procedure introduces an overhead of 1\u221a\nin the convergence rate of the calibration errorCT and of the regret in Lemma 2. In addition, Algorithm 1 requires 1 times more memory (we run 1/ instances of F\ncal j ), but\nhas the same per-iteration runtime (we activate one F calj per step). Table 1 summarizes the convergence rates of Algorithm 1 when the subroutine is either the method of Abernethy, Bartlett, and Hazan based on Blackwell approachability or the simpler but slower approach based on internal regret minimization (Mannor and Stoltz 2010).\nMulticlass prediction. In the multiclass setting, we seek a recalibrator A : \u2206K\u22121 \u2192 \u2206K\u22121 producing calibrated probabilities pt \u2208 \u2206K\u22121 that target class labels yt \u2208 {1, 2, ...,K}. In analogy to binary recalibration, we may discretize the input space \u2206K\u22121 into a K-dimensional grid and train a classical multi-class calibration algorithm F cal (Cesa-Bianchi and Lugosi 2006) on each subset of pFt associated with a cell. Just like in the binary setting, a classical calibration method F calj predicts calibrated pt \u2208 \u2206K\u22121 based solely on past multiclass labels y1, y2, ..., yt\u22121; it can serve as a subroutine within Algorithm 1.\nHowever, in the multi-class setting, this construction will require O(1/ K) running time per iteration, O(1/ 2K) memory, and will have a convergence rate of O(1/( 2K \u221a T )). The exponential dependence on K cannot be avoided, since the calibration problem is fundamentally PPAD-hard (Hazan and Kakade 2012). However, there may exist practical workarounds inspired by popular heuristics for the batch setting, such as one-vs-all classification (Zadrozny and Elkan 2002)."}, {"heading": "Experiments", "text": "We now proceed to study Algorithm 1 empirically. Algorithm 1\u2019s subroutine is the standard internal regret minimization approach of Cesa-Bianchi and Lugosi (\"REGMIN\"). We measure calibration and accuracy in the `2 norm.\nPredicting a Bernoulli sequence. We start with a simple setting where we observe an i.i.d. sequence of yt \u223c Ber(p) as well as uncalibrated predictions (pFt ) T t=1 that equal 0.3 whenever yt = 0 and 0.7 when yt = 1. The forecaster F is essentially a perfect predictor, but is not calibrated.\nIn Figure 2, we compare the performance of REGMIN (which does not observe pFt ) to Algorithm 1 and to the uncalibrated predictor F . Both methods achieve low calibration error after about 300 observations, while the expert is clearly uncalibrated (Figure 2b); however, REGMIN is a terrible predictor: it always forecasts pt = 0.5 and therefore has high `2 loss (Figure 2a). Algorithm 1, on the other hand, makes perfect predictions by recalibrating the input pFt .\nPrediction against an adversary. Next, we test the ability of our method to achieve calibration on adversarial input. At each step t, we choose yt = 0 if pt > 0.5 and yt = 1 otherwise; we sample pFt \u223c Ber(0.5), which is essentially a form of noise. In Figure 2 (c, d), we see that Algorithm 1 successfully ignores the noisy forecaster F and instead quickly converges to making calibrated (albeit not very accurate) predictions (it reduces to REGMIN).\nNatural language understanding. We used Algorithm 1 to recalibrate a state-of-the-art question answering system (Berant and Liang 2014) on the popular Free917 dataset (641 training, 276 testing examples). We trained the system on the training set as described in (Berant et al. 2013) and then calibrated probabilities using Algorithm 1 in one pass over first the training, and then the testing examples. This setup emulates a pre-trained system that further improves itself from user feedback.\nFigure 3 (left) compares our predicted pt to the raw system probabilities pFt via calibration curves. Given pairs of predictions and outcomes pt, yt, we compute for each of N buckets B \u2208 {[ iN , i+1 N ) | 0 \u2264 i \u2264 1}, averages\np\u0304B = \u2211 t:pt\u2208B pt/NB and y\u0304B = \u2211 t:pt\u2208B yt/NB , where NB = |{pt \u2208 B}|. A calibration curve plots the y\u0304B as a function of p\u0304B ; perfect calibration corresponds to a straight line.\nCalibration curves indicate that the pFt are poorly calibrated in buckets below 0.9, while Algorithm 1 fares better. Figure 3a confirms that our accuracy (measured by the `2 loss) tracks the baseline forecaster.\nMedical diagnosis. Our last task is predicting the risk of type 1 diabetes from genomic data. We use genotypes of 3,443 subjects (1,963 cases, 1,480 controls) over 447,221 SNPs (The Wellcome Trust Case Control Consortium 2007), with alleles encoded as 0, 1, 2 (major, heterozygous and minor homozygous resp.). We use an online `1-regularized linear support vector machine (SVM) to predict outcomes one patient at a time, and report performance for each t \u2208 [T ]. Uncalibrated probabilities are normalized raw SVM scores st, i.e. pFt = (st +mt)/2mt, where mt = max1\u2264r\u2264t |sr|.\nFigure 3 (right) measures calibration after observing all the data. Raw scores are not well-calibrated outside of the interval [0.4, 0.6]; recalibration makes them almost perfectly\ncalibrated. Figure 3 further shows that the calibration error of Algorithm 1 is consistently lower throughout the entire learning process, while accuracy approaches to within 0.01 of that of pFt ."}, {"heading": "Previous Work", "text": "Calibrated probabilities are widely used as confidence measures in the context of binary classification. Such probabilities are obtained via recalibration methods, of which Platt scaling (Platt 1999) and isotonic regression (NiculescuMizil and Caruana 2005) are by far the most popular. Recalibration methods also possess multiclass extensions, which typically involve training multiple one-vs-all predictors (Zadrozny and Elkan 2002), as well as extensions to ranking losses (Menon et al. 2012), combinations of estimators (Zhong and Kwok 2013), and structured prediction (Kuleshov and Liang 2015).\nIn the online setting, the calibration problem was formalized by Dawid; online calibration techniques were first proposed by Foster and Vohra. Existing algorithms are based on internal regret minimization (Cesa-Bianchi and Lugosi 2006) or on Blackwell approachability (Foster 1997); recently, these approaches were shown to be closely related (Abernethy, Bartlett, and Hazan 2011; Mannor and Stoltz 2010). Recent work has shown that online calibration is PPAD-hard (Hazan and Kakade 2012).\nThe concepts of calibration and sharpness were first formalized in the statistics literature (Murphy 1973; Gneiting, Balabdaoui, and Raftery 2007). These metrics are captured by a class of proper losses and can be used both for evaluating (Buja, Stuetzle, and Shen 2005; Brocker 2009) and constructing (Kuleshov and Liang 2015) calibrated forecasts."}, {"heading": "Discussion and Conclusion", "text": "Online vs batch. Algorithm 1 can be understood as a direct analogue of a simple density estimation technique called the histogram method. This technique divides the pFt into N bins and estimates the average y in each bin. By the i.i.d. assumption, output probabilities will be calibrated; sharpness will be determined by the bin width. Note that by Hoeffding\u2019s inequality, the average in a given bin with converge at a rate ofO(1/ \u221a Tj) (Devroye, Gy\u00f6rfi, and Lugosi 1996). This\nis faster than the O(1/ \u221a Tj) rate of Abernethy, Bartlett,\nand Hazan and suggests that calibration is more challenging in the online setting.\nChecking rules. An alternative way to avoid uninformative predictions (e.g. 0.5 on 010101...) is via the framework of checking rules (Cesa-Bianchi and Lugosi 2006). However, these rules must be specified in advance (e.g. the pattern 010101 must be known) and this framework does not explicitly admit covariates xt. Our approach on the other hand recalibrates any xt, yt in a black-box manner.\nDefensive forecasting. Vovk, Takemura, and Shafer developed simultaneously calibrated and accurate online learning methods under the notion of weak calibration (Abernethy and Mannor 2011). We use strong calibration, which implies weak, although it requires different (e.g. randomized) algorithms. Vovk et al. also use a different notion of precision; their algorithm ensures a small difference between average predicted pt and true yt at times t when pt \u2248 p\u2217 and xt \u2248 x\u2217, for any p\u2217, x\u2217. The relation \u2248 is determined by a user-specified kernel (over e.g. sentences or genomes xt). Our approach, on the other hand, does not require specifying a kernel, and matches the accuracy of any given baseline forecaster; this may be simpler in some settings. Interestingly, we arrive at the same rates of convergence under different assumptions.\nConclusion. Current recalibration techniques implicitly require that the data is distributed i.i.d., which potentially makes them unreliable when this assumption does not hold. In this work, we introduced the first recalibration technique that provably recalibrates any existing forecaster with a vanishingly small degradation in accuracy. This method does not make i.i.d. assumptions, and is provably calibrated even on adversarial input. We analyzed our method\u2019s theoretical properties and showed excellent empirical performance on several real-world benchmarks, where the method converges quickly and retains good accuracy.\nAcknowledgements. This work is supported by the NSF (grant #1649208) and by the Future of Life Institute (grant 2016-158687)."}], "references": [{"title": "and Mannor", "author": ["J.D. Abernethy"], "venue": "S.", "citeRegEx": "Abernethy and Mannor 2011", "shortCiteRegEx": null, "year": 2011}, {"title": "P", "author": ["Abernethy, J.", "Bartlett"], "venue": "L.; and Hazan, E.", "citeRegEx": "Abernethy. Bartlett. and Hazan 2011", "shortCiteRegEx": null, "year": 2011}, {"title": "and Liang", "author": ["J. Berant"], "venue": "P.", "citeRegEx": "Berant and Liang 2014", "shortCiteRegEx": null, "year": 2014}, {"title": "Semantic parsing on freebase from questionanswer pairs", "author": ["Berant"], "venue": null, "citeRegEx": "Berant,? \\Q2013\\E", "shortCiteRegEx": "Berant", "year": 2013}, {"title": "Loss functions for binary class probability estimation and classification: Structure and applications", "author": ["Stuetzle Buja", "A. Shen 2005] Buja", "W. Stuetzle", "Y. Shen"], "venue": null, "citeRegEx": "Buja et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Buja et al\\.", "year": 2005}, {"title": "and Lugosi", "author": ["N. Cesa-Bianchi"], "venue": "G.", "citeRegEx": "Cesa.Bianchi and Lugosi 2006", "shortCiteRegEx": null, "year": 2006}, {"title": "A", "author": ["Dawid"], "venue": "P.", "citeRegEx": "Dawid 1982", "shortCiteRegEx": null, "year": 1982}, {"title": "A probabilistic theory of pattern recognition", "author": ["Gy\u00f6rfi Devroye", "L. Lugosi 1996] Devroye", "L. Gy\u00f6rfi", "G. Lugosi"], "venue": "Applications of mathematics", "citeRegEx": "Devroye et al\\.,? \\Q1996\\E", "shortCiteRegEx": "Devroye et al\\.", "year": 1996}, {"title": "R", "author": ["D.P. Foster", "Vohra"], "venue": "V.", "citeRegEx": "Foster and Vohra 1998", "shortCiteRegEx": null, "year": 1998}, {"title": "D", "author": ["Foster"], "venue": "P.", "citeRegEx": "Foster 1997", "shortCiteRegEx": null, "year": 1997}, {"title": "A", "author": ["T. Gneiting", "F. Balabdaoui", "Raftery"], "venue": "E.", "citeRegEx": "Gneiting. Balabdaoui. and Raftery 2007", "shortCiteRegEx": null, "year": 2007}, {"title": "S", "author": ["E. Hazan", "Kakade"], "venue": "M.", "citeRegEx": "Hazan and Kakade 2012", "shortCiteRegEx": null, "year": 2012}, {"title": "Calibrating predictive model estimates to support personalized medicine", "author": ["Jiang"], "venue": "JAMIA", "citeRegEx": "Jiang,? \\Q2012\\E", "shortCiteRegEx": "Jiang", "year": 2012}, {"title": "and Liang", "author": ["V. Kuleshov"], "venue": "P.", "citeRegEx": "Kuleshov and Liang 2015", "shortCiteRegEx": null, "year": 2015}, {"title": "and Stoltz", "author": ["S. Mannor"], "venue": "G.", "citeRegEx": "Mannor and Stoltz 2010", "shortCiteRegEx": null, "year": 2010}, {"title": "A", "author": ["Menon"], "venue": "K.; Jiang, X.; Vembu, S.; Elkan, C.; and Ohno-Machado, L.", "citeRegEx": "Menon et al. 2012", "shortCiteRegEx": null, "year": 2012}, {"title": "A", "author": ["Murphy"], "venue": "H.", "citeRegEx": "Murphy 1973", "shortCiteRegEx": null, "year": 1973}, {"title": "and O\u2019Connor", "author": ["K. Nguyen"], "venue": "B.", "citeRegEx": "Nguyen and O.Connor 2015", "shortCiteRegEx": null, "year": 2015}, {"title": "and Caruana", "author": ["A. Niculescu-Mizil"], "venue": "R.", "citeRegEx": "Niculescu.Mizil and Caruana 2005", "shortCiteRegEx": null, "year": 2005}, {"title": "J", "author": ["Platt"], "venue": "C.", "citeRegEx": "Platt 1999", "shortCiteRegEx": null, "year": 1999}, {"title": "G", "author": ["V. Vovk", "A. Takemura", "Shafer"], "venue": "2005. Defensive forecasting. In Proceedings of the Tenth International Workshop on Artificial Intelligence and Statistics, AISTATS 2005, Bridgetown, Barbados, January 6-8,", "citeRegEx": "Vovk. Takemura. and Shafer 2005", "shortCiteRegEx": null, "year": 2005}, {"title": "Calibration of confidence measures in speech recognition", "author": ["Li Yu", "D. Deng 2011] Yu", "J. Li", "L. Deng"], "venue": "Trans. Audio, Speech and Lang. Proc. 19(8):2461\u20132473", "citeRegEx": "Yu et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Yu et al\\.", "year": 2011}, {"title": "and Elkan", "author": ["B. Zadrozny"], "venue": "C.", "citeRegEx": "Zadrozny and Elkan 2002", "shortCiteRegEx": null, "year": 2002}, {"title": "J", "author": ["L.W. Zhong", "Kwok"], "venue": "T.", "citeRegEx": "Zhong and Kwok 2013", "shortCiteRegEx": null, "year": 2013}], "referenceMentions": [], "year": 2017, "abstractText": "Assessing uncertainty is an important step towards ensuring the safety and reliability of machine learning systems. Existing uncertainty estimation techniques may fail when their modeling assumptions are not met, e.g. when the data distribution differs from the one seen at training time. Here, we propose techniques that assess a classification algorithm\u2019s uncertainty via calibrated probabilities (i.e. probabilities that match empirical outcome frequencies in the long run) and which are guaranteed to be reliable (i.e. accurate and calibrated) on out-of-distribution input, including input generated by an adversary. This represents an extension of classical online learning that handles uncertainty in addition to guaranteeing accuracy under adversarial assumptions. We establish formal guarantees for our methods, and we validate them on two real-world problems: question answering and medical diagnosis from genomic data.", "creator": "LaTeX with hyperref package"}}}