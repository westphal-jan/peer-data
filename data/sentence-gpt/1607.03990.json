{"id": "1607.03990", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "14-Jul-2016", "title": "Fast Algorithms for Segmented Regression", "abstract": "We study the fixed design segmented regression problem: Given noisy samples from a piecewise linear function $f$, we want to recover $f$ up to a desired accuracy in mean-squared error. The distribution is expected to increase in frequency with each value. This is expected to be as much as 100% for a simple piecewise linear function $f$.\n\n\n\nTo determine whether the probability of an incorrect estimate is correct for $f$ and if the value is correct for $f$, we can use $f$, and $x$, respectively, to investigate the accuracy of a given estimate using a linear function $f$. We use $f$, to estimate the probability of an incorrect estimate of $f$, and $x$, respectively, to investigate the accuracy of a given estimate using a linear function $f$.\n\nLet's compare the estimated variance of a given estimate to the given error in the regression problem, with a continuous distribution. The $f$ distribution is very close to that of a given estimate, which shows an error of $f$, for $f$, and $x$, respectively, and has an uncertainty of $f$, in the range of $\\ellspace$ that is $f$.\nIn other words, $f$ can be easily seen in the regression problem in the regression problem with a single $f$, and a more complete model is provided using a linear function $f$, and $x$. The probability of error estimates is about the expected accuracy in this estimate. As the estimate is more general, we can use $f$, as we saw above, to estimate a better estimate of the accuracy of the estimates of the predicted error in the regression problem, with a continuous distribution.\nWe need to measure the variance of a given estimate from the same parameter by using $\\ellspace$ (or $\\ellspace$) as the error vector. We will return the estimated variance of the estimates that are correct for $f$, and we will use the $f$. We will use $\\ellspace$ in the regression problem using a linear function $f$, as a linear function $f$, and $x$, respectively. If we know $f$ is correct for $f$, then we can use $f$, and $x$, respectively. The estimated variance of $f$ is about the expected accuracy in the regression problem with a single $f$, and $x$, respectively, and has an uncertainty of $f$, in the range of $f$, and $x$, respectively. For a", "histories": [["v1", "Thu, 14 Jul 2016 04:52:53 GMT  (606kb,D)", "http://arxiv.org/abs/1607.03990v1", "27 pages, appeared in ICML 2016"]], "COMMENTS": "27 pages, appeared in ICML 2016", "reviews": [], "SUBJECTS": "cs.LG cs.DS math.ST stat.TH", "authors": ["jayadev acharya", "ilias diakonikolas", "jerry li 0001", "ludwig schmidt"], "accepted": true, "id": "1607.03990"}, "pdf": {"name": "1607.03990.pdf", "metadata": {"source": "CRF", "title": "Fast Algorithms for Segmented Regression", "authors": ["Jayadev Acharya", "Ilias Diakonikolas", "Jerry Li"], "emails": ["jayadev@csail.mit.edu", "ilias.d@ed.ac.uk", "jerryzli@csail.mit.edu", "ludwigs@mit.edu"], "sections": [{"heading": null, "text": "Previous rigorous approaches for this problem rely on dynamic programming (DP) and, while sample efficient, have running time quadratic in the sample size. As our main contribution, we provide new sample near-linear time algorithms for the problem that \u2013 while not being minimax optimal \u2013 achieve a significantly better sample-time tradeoff on large datasets compared to the DP approach. Our experimental evaluation shows that, compared with the DP approach, our algorithms provide a convergence rate that is only off by a factor of 2 to 4, while achieving speedups of three orders of magnitude.\nar X\niv :1\n60 7.\n03 99\n0v 1\n[ cs\n.L G\n] 1\n4 Ju"}, {"heading": "1 Introduction", "text": "We study the regression problem \u2013 a fundamental inference task with numerous applications that has received tremendous attention in machine learning and statistics during the past fifty years (see, e.g., [MT77] for a classical textbook). Roughly speaking, in a (fixed design) regression problem, we are given a set of n observations (xi, yi), where the yi\u2019s are the dependent variables and the xi\u2019s are the independent variables, and our goal is to model the relationship between them. The typical assumptions are that (i) there exists a simple function f that (approximately) models the underlying relation, and (ii) the dependent observations are corrupted by random noise. More specifically, we assume that there exists a family of functions F such that for some f \u2208 F the following holds: yi = f(xi) + i, where the i\u2019s are i.i.d. random variables drawn from a \u201ctame\u201d distribution such as a Gaussian (later, we also consider model misspecification).\nThroughout this paper, we consider the classical notion of Mean Squared Error (MSE) to measure the performance (risk) of an estimator. As expected, the minimax risk depends on the family F that f comes from. The natural case that f is linear is fully understood: It is well-known that the leastsquares estimator is statistically efficient and runs in sample-linear time. The more general case that f is non-linear, but satisfies some well-defined structural constraint has been extensively studied in a variety of contexts (see, e.g., [GA73, Fed75, Fri91, BP98, YP13, KRS15, ASW13, Mey08, CGS15]). In contrast to the linear case, this more general setting is not well-understood from an informationtheoretic and/or computational aspect.\nIn this paper, we focus on the case that the function f is promised to be piecewise linear with a given number k of unknown pieces (segments). This is known as fixed design segmented regression, and has received considerable attention in the statistics community [GA73, Fed75, BP98, YP13]. The special case of piecewise polynomial functions (splines) has been extensively used in the context of inference, including density estimation and regression, see, e.g., [WW83, Fri91, Sto94, SHKT97, Mey08].\nInformation-theoretic aspects of the segmented regression problem are well-understood: Roughly speaking, the minimax risk is inversely proportional to the number of samples. In contrast, the computational complexity of the problem is poorly understood: Prior to our work, known algorithms for this problem with provable guarantees were quite slow. Our main contribution is a set of new provably fast algorithms that outperform previous approaches both in theory and in practice. Our algorithms run in time that is nearly-linear in the number of data points n and the number of intervals k. Their computational efficiency is established both theoretically and experimentally. We also emphasize that our algorithms are robust to model misspecification, i.e., they perform well even if the function f is only approximately piecewise linear.\nNote that if the segments of f were known a priori, the segmented regression problem could be immediately reduced to k independent linear regression problems. Roughly speaking, in the general case (where the location of the segment boundaries is unknown) one needs to \u201cdiscover\u201d the right segments using information provided by the samples. To address this algorithmic problem, previous works [BP98, YP13] relied on dynamic programming that, while being statistically efficient, is computationally quite slow: its running time scales at least quadratically with the size n of the data, hence it is rather impractical for large datasets.\nOur main motivation comes from the availability of large datasets that has made computational efficiency the main bottleneck in many cases. In the words of [Jor13]: \u201cAs data grows, it may be beneficial to consider faster inferential algorithms, because the increasing statistical strength of the data can compensate for the poor algorithmic quality.\u201d Hence, it is sometimes advantageous to\nsacrifice statistical efficiency in order to achieve faster running times because we can then achieve the desired error guarantee faster (provided more samples). In our context, instead of using a slow dynamic program, we employ a subtle iterative greedy approach that runs in sample-linear time.\nOur iterative greedy approach builds on the work of [ADH+15, ADLS15], but the details of our algorithms here and their analysis are substantially different. In particular, as we explain in the body of the paper, the natural adaptation of their analysis to our setting fails to provide any meaningful statistical guarantees. To obtain our results, we introduce novel algorithmic ideas and carefully combine them with additional probabilistic arguments."}, {"heading": "2 Preliminaries", "text": "In this paper, we study the problem of fixed design segmented regression. We are given samples xi \u2208 Rd for i \u2208 [n] ( = {1, . . . , n}), and we consider the following classical regression model:\nyi = f(xi) + i . (1)\nHere, the i are i.i.d. sub-Gaussian noise variables with variance proxy \u03c32, mean E[ i] = 0, and variance s2 = E[ 2i ] for all i.1 We will let = ( 1, . . . , n) denote the vector of noise variables. We also assume that f : Rd \u2192 R is a k-piecewise linear function. Formally, this means:\nDefinition 1. The function f : Rd \u2192 R is a k-piecewise linear function if there exists a partition of the real line into k disjoint intervals I1, . . . , Ik, k corresponding parameters \u03b81, . . . ,\u03b8k \u2208 Rd, and a fixed, known j such that for all x = (x1, . . . , xd) \u2208 Rd we have that f(x) = \u3008\u03b8i,x\u3009 if xj \u2208 Ii. Let Lk,j denote the space of k-piecewise linear functions with partition defined on coordinate j.\nMoreover, we say f is flat on an interval I \u2286 R if I \u2286 Ii for some i = 1, . . . , k, otherwise, we say that f has a jump on the interval I.\nLater in the paper (see Section 6), we also discuss the setting where the ground truth f is not piecewise linear itself but only well-approximated by a k-piecewise linear function. For simplicity of exposition, we assume that the partition coordinate j is 1 in the rest of the paper.\nFollowing this generative model, a regression algorithm receives the n pairs (xi, yi) as input. The goal of the algorithm is then to produce an estimate f\u0302 that is close to the true, unknown f with high probability over the noise terms i and any randomness in the algorithm. We measure the distance between our estimate f\u0302 and the unknown function f with the classical mean-squared error:\nMSE(f\u0302) = 1\nn n\u2211 i=1 (f(xi)\u2212 f\u0302(xi))2 .\nThroughout this paper, we let X \u2208 Rn\u00d7d be the data matrix, i.e., the matrix whose j-th row is xTj for every j, and we let r denote the rank of X. We also assume that no xi is the all-zeros vector, since such points are trivially fit by any linear function.\nThe following notation will also be useful. For any function f : Rd \u2192 R, we let f \u2208 Rn denote the vector with components fi = f(xi) for i \u2208 [n]. For any interval I, we let XI denote the data matrix consisting of all data points xi for i \u2208 I, and for any vector v \u2208 Rn, we let vI \u2208 R|I| be the vector of vi for i \u2208 I.\n1We observe that s2 is guaranteed to be finite since the i are sub-Gaussian. The variance s2 is in general not equal to the variance proxy \u03c32, however, it is well-known that s2 \u2264 \u03c32.\nWe remark that this model also contains the problem of (fixed design) piecewise polynomial regression as an important subcase. Indeed, imagine we are given x1, . . . , xn \u2208 R and y1, . . . , yn \u2208 R so that there is some k-piece degree-d polynomial p so that for all i = 1, . . . , n we have yi = p(xi)+ i, and our goal is to recover p in the same mean-squared sense as above. We can write this problem as a k-piecewise linear fit in Rd+1, by associating the data vector vi = (1, xi, x2i , . . . , xdi ) to each xi. If the piecewise polynomial p has breakpoints at z1, . . . zk\u22121, the associated partition for the k-piecewise linear function is I = {(\u2212\u221e, z1), [z1, z2), . . . , (zk\u22122, zk\u22121], (zk\u22121,\u221e)}. If the piecewise polynomial is of the form p(x) = \u2211d `=0 a I `x ` for any interval I \u2208 I, then for any vector v = (v1, v2, . . . , vd+1) \u2208\nRd+1, the ground truth k-piecewise linear function is simply the linear function f(v) = \u2211d+1\n`=1 a I `\u22121v`\nfor v2 \u2208 I. Moreover, the data matrix associated with the data points v is a Vandermonde matrix. For n \u2265 d + 1 it is well-known that this associated Vandermonde matrix has rank exactly d + 1 (assuming xi 6= xj for any i 6= j)."}, {"heading": "2.1 Our Contributions", "text": "Our main contributions are new, fast algorithms for the aforementioned segmented regression problem. We now informally state our main results and refer to later sections for more precise theorems.\nTheorem 2 (informal statement of Theorems 18 and 19). There is an algorithm GreedyMerge, which, given X (of rank r), y, a target number of pieces k, and the variance of the noise s2, runs in time O(nd2 log n) and outputs an O(k)-piecewise linear function f\u0302 so that with probability at least 0.99, we have\nMSE(f\u0302) \u2264 O ( \u03c32 kr\nn + \u03c3\n\u221a k\nn log n\n) .\nThat is, our algorithm runs in time which is nearly linear in n and still achieves a reasonable rate of convergence. While this rate is asymptotically slower than the rate of the dynamic programming estimator, our algorithm is significantly faster than the DP so that in order to achieve a comparable MSE, our algorithm takes less total time given access to a sufficient number of samples. For more details on this comparision and an experimental evaluation, see Sections 2.2 and 7.\nAt a high level, our algorithm proceeds as follows: it first forms a fine partition of [n] and then iteratively merges pieces of this partitions until only O(k) pieces are left. In each iteration, the algorithm reduces the number of pieces in the following manner: we group consecutive intervals into pairs which we call \u201ccandidates\u201d. For each candidate interval, the algorithm computes an error term that is the error of a least squares fit combined with a regularizer depending on the variance of the noise s2. The algorithm then finds the O(k) candidates with the largest errors. We do not merge these candidates, but do merge all other candidate intervals. We repeat this process until only O(k) pieces are left.\nA drawback of this algorithm is that we need to know the variance of the noise s2, or at least have a good estimate of it. In practice, we might be able to obtain such an estimate, but ideally our algorithm would work without knowing s2. By extending our greedy algorithm, we obtain the following result:\nTheorem 3 (informal statement of Theorems 21 and 22). There is an algorithm BucketGreedyMerge, which, givenX (of rank r), y, and a target number of pieces k, runs in time O(nd2 log n) and outputs\nan O(k log n)-piecewise linear function f\u0302 so that with probability at least 0.99, we have\nMSE(f\u0302) \u2264 O ( \u03c32 kr log n\nn + \u03c3\n\u221a k\nn log n\n) .\nAt a high level, there are two fundamental changes to the algorithm: first, instead of merging with respect to the sum squared error of the least squares fit regularized by a term depending on s2, we instead merge with respect to the average error the least squares fit incurs within the current interval. The second change is more substantial: instead of finding the top O(k) candidates with largest error and merging the rest, we now split the candidates into log n buckets based on the lengths of the candidate intervals. In this bucketing scheme, bucket \u03b1 contains all candidates with length between 2\u03b1 and 2\u03b1+1, for \u03b1 = 0, . . . , log n\u22121. Then we find the k+1 candidates with largest error within each bucket and merge the remaining candidate intervals. We continue this process until we are left with O(k log n) buckets. Intuitively, this bucketing allows us to control the variance of the noise without knowing s2 because all candidate intervals have roughly the same length.\nA potential disadvantage of our algorithms above is that they produce O(k) or O(k log n) pieces, respectively. In order to address this issue, we also provide a postprocessing algorithm that converts the output of any of our algorithms and decreases the number of pieces down to 2k + 1 while preserving the statistical guarantees above. The guarantee of this postprocessing algorithm is as follows.\nTheorem 4 (informal statement of Theorems 23 and 24). There is an algorithm Postprocessing that takes as input the output of either GreedyMerge or BucketGreedyMerge together with a target number of pieces k, runs in time O ( k3d2 log n ) , and outputs a (2k + 1)-piecewise linear\nfunction f\u0302p so that with probability at least 0.99, we have\nMSE(f\u0302p) \u2264 O ( \u03c32 kr\nn + \u03c3\n\u221a k\nn log n\n) .\nQualitatively, an important aspect this algorithm is that its running time depends only logarithmically on n. In practice, we expect k to be much smaller than n, and hence the running time of this postprocessing step will usually be dominated by the running time of the piecewise linear fitting algorithm run before it."}, {"heading": "2.2 Comparison to prior work", "text": "Dynamic programming. Previous work on segmented regression with statistical guarantees [BP98, YP13] relies heavily on dynamic programming-style algorithms to find the k-piecewise linear least-squares estimator. Somewhat surprisingly, we are not aware of any work which explicitly gives the best possible running time and statistical guarantee for this algorithm. For completeness, we prove the following result (Theorem 5), which we believe to be folklore. The techniques used in its proof will also be useful for us later.\nTheorem 5 (informal statement of Theorems 15 and 16). The exact dynamic program runs in time O(n2(d2 +k)) and outputs an k-piecewise linear estimator f\u0302 so that with probability at least 0.99 we have\nMSE(f\u0302) \u2264 O ( \u03c32 kr\nn\n) .\nWe now compare our guarantees with those of the DP. The main advantage of our approaches is computational efficiency \u2013 our algorithm runs in linear time, while the running time of the DP has a quadratic dependence on n. While our statistical rate of convergence is slower, we are able to achieve the same MSE as the DP in asymptotically less time (and also in practice) if enough samples are available.\nFor instance, suppose we wish to obtain a MSE of \u03b7 with a k-piecewise linear function, and suppose for simplicity that d = O(1) so that r = O(1) as well. Then Theorem 5 tells us that the DP requires n = k/\u03b7 samples and runs in time O(k3/\u03b72). On the other hand, Theorem 2 tells us that GreedyMerging requires n = O\u0303(k/\u03b72) samples (ignoring log factors) and thus runs in time O\u0303(k/\u03b72). For non-trivial values of k, this is a significant improvement in time complexity.\nThis gap also manifests itself strongly in our experiments (see Section 7). When given the same number of samples, our MSE is a factor of 2-4 times worse than that achieved by the DP, but our algorithm is about 1, 000 times faster already for 104 data points. When more samples are available for our algorithm, it achieves the same MSE as the DP about 100 times faster.\nHistogram Approximation Our results build upon the techniques of [ADH+15], who consider the problem of histogram approximation. In this setting, one is given a function f : [n] \u2192 R, and the goal is to find the best k-flat approximation to f in sum-squared error. This problem bears nontrivial resemblance to the problem of piecewise constant regression, and indeed, the results in this paper establish a connection between the two problems. The histogram approximation problem has received significant attention in the database community (e.g. [JKM+98, GKS06, ADH+15]), and it is natural to ask whether these results also imply algorithms for segmented regression. Indeed, it is possible to convert the algorithms of [GKS06] and [ADH+15] to the segmented regression setting, but the corresponding guarantees are too weak to obtain good statistical results. At a high level, the algorithms in these papers can be adapted to output an O(k)-piecewise linear function f\u0302 so that \u2211n i=1(yi \u2212 f\u0302(xi))2 \u2264 C \u00b7 \u2211n i=1(yi \u2212 fLS(xi))2, where C > 1 is a fixed constant and fLS is the best k-piecewise linear least-squares fit to the data. However, this guarantee by itself does not give meaningful results for segmented regression.\nAs a toy example, consider the k = 1 case. Let x1, . . . ,xn \u2208 Rn be arbitrary, let f(x) = 0 for all x, and i \u223c N (0, 1), for i = 1, . . . , n. Then it is not too hard to see that the least squares fit has squared error \u2211n i=1(yi \u2212 fLS(xi))2 = \u0398(n) with high probability. Hence the function g which is constantly C/2 for all x achieves the approximation guarantee n\u2211 i=1 (yi \u2212 g(xi))2 \u2264 C n\u2211 i=1 (yi \u2212 fLS(xi))2\ndescribed above with non-negligible probability. However, this function clearly does not converge to f as n\u2192\u221e, and indeed its MSE is always \u0398(1).\nTo get around this difficulty, we must extend the algorithms presented in histogram approximation literature in order to adapt them to the segmented regression setting. In the process, we introduce new algorithmic ideas and use more sophisticated proof techniques to obtain a meaningful rate of convergence for our algorithms."}, {"heading": "2.3 Agnostic guarantees", "text": "We also consider the agnostic model, also known as learning with model misspecification. So far, we assumed that the ground truth is exactly a piecewise linear function. In reality, such a notion\nis probably only an approximation. While the ground truth may be close to a piecewise linear function, generally we do not believe that it exactly follows a piecewise linear function. In this case, our goal should be to recover a piecewise linear function that is competitive with the best piecewise linear approximation to the ground truth.\nFormally, we consider the following problem. We assume the same generative model as in (1), however, we no longer assume that f is a piecewise linear function. Instead, the function f can now be arbitrary. We define\nOPTk = min g\u2208Lk MSE(g)\nto be the error of the best fit k-piecewise linear function to f , and we let f\u2217 be any k-piecewise linear function that achieves this minimum. Then the goal of our algorithm is to achieve an MSE as close to OPTk as possible. We remark that the qualitatively interesting regime is when OPTk is small and comparable to the statistical error, and we encourage readers to think of OPTk as being in that regime.\nFor clarity of exposition, we first prove non-agnostic guarantees. In Section 6, we then show how to modify these proofs to obtain agnostic guarantees with roughly the same rate of convergence."}, {"heading": "2.4 Mathematical Preliminaries", "text": "In this section, we state some preliminaries that our analysis builds on.\nTail bounds. We require the following tail bound on sub-exponential random variables. Recall that for any sub-exponential random variable X, the sub-exponential norm of X, denoted \u2016X\u2016\u03c81 , is defined to be the smallest parameter K so that (E[|X|p])1/p \u2264 Kp for all p \u2265 1 (see [Ver10]). Moreover, if Y is a sub-Gaussian random variable with variance \u03c32, then Y 2 \u2212 \u03c32 is a centered sub-exponential random with \u2016Y 2 \u2212 \u03c32\u2016\u03c81 = O(\u03c32).\nFact 6 (Bernstein-type inequality, c.f. Proposition 5.16 in [Ver10]). Let X1, . . . , XN be centered sub-exponential random variables, and let K = maxi \u2016Xi\u2016\u03c81 . Then for all t > 0, we have\nPr [\u2223\u2223\u2223\u2223\u2223 1\u221aN N\u2211 i=1 Xi \u2223\u2223\u2223\u2223\u2223 \u2265 t ] \u2264 2 exp ( \u2212cmin ( t2 K2 , t \u221a N K )) .\nThis yields the following straightforward corollary:\nCorollary 7. Fix \u03b4 > 0 and let 1, . . . , n be as in (1). Recall that s = E[ 2i ]. Then, with probability 1\u2212 \u03b4, we have that simultaneously for all intervals I \u2286 [n] the following inequality holds:\u2223\u2223\u2223\u2223\u2223\u2211\ni\u2208I 2i \u2212 s2|I| \u2223\u2223\u2223\u2223\u2223 \u2264 O(\u03c32 \u00b7 log(n/\u03b4))\u221a|I| . (2) Proof. Let \u03b4\u2032 = O(\u03b4/n2). For any interval I \u2286 [n], by Fact 6 applied to the random variables 2i \u2212s2 for i \u2208 I, we have that (2) holds with probability 1 \u2212 \u03b4\u2032. Thus, by a union bound over all O(n2) subintervals of [n], we conclude that Equation 2 holds with probability 1\u2212 \u03b4.\nLinear regression. Our analysis builds on the classical results for fixed design linear regression. In linear regression, the generative model is exactly of the form described in (1), except that f is restricted to be a 1-piecewise linear function (as opposed to a k-piecewise linear function), i.e., f(x) = \u3008\u03b8\u2217,x\u3009 for some unknown \u03b8\u2217.\nThe problem of linear regression is very well understood, and the asymptotically best estimator is known to be the least-squares estimator.\nDefinition 8. Given x1, . . . ,xn and y1, . . . , yn, the least squares estimator fLS is defined to be the linear function which minimizes \u2211n i=1(yi \u2212 f(xi))2 over all linear functions f . For any interval I, we let fLSI denote the least squares estimator for the data points restricted to I, i.e. for the data pairs {(xi, yi)}i\u2208I . We also let LeastSquares (X, y, I) denote an algorithm which solves linear least squares for these data points, i.e., which outputs the coefficients of the linear least squares fit for these points.\nFollowing our previous definitions, we let fLS \u2208 Rn denote the vector whose i-th coordinate is fLS(xi), and similarly for any I \u2286 [n] we let fLSI \u2208 R|I| denote the vector whose i-th coordinate for i \u2208 I is fLSI (xi).\nThe following prediction error rate is known for the least-squares estimator:\nFact 9. Let x1, . . . ,xn and y1, . . . , yn be generated as in (1), where f is a linear function. Let fLS(x) be the least squares estimator. Then,\nE [ MSE(fLS) ] = O ( \u03c32 r\nn\n) .\nMoreover, with probability 1\u2212 \u03b4, we have\nMSE(fLS) = O ( \u03c32 r + log 1/\u03b4\nn\n) .\nFact 9 can be proved with the following lemma, which we also use in our analysis. The lemma bounds the correlation of a random vector with any fixed r-dimensional subspace:\nLemma 10 (c.f. proof of Theorem 2.2 in [Rig15]). Fix \u03b4 > 0. Let 1, . . . , n be i.i.d. sub-Gaussian random variables with variance proxy \u03c32. Let = ( 1, . . . , n), and let S be a fixed r-dimensional affine subspace of Rn. Then, with probability 1\u2212 \u03b4, we have\nsup v\u2208S\\{0} |vT | \u2016v\u2016\n\u2264 O ( \u03c3 \u221a r + log(1/\u03b4) ) .\nThis lemma also yields the two following consequences. The first corollary bounds the correlation between sub-Gaussian random noise and any linear function on any interval:\nCorollary 11. Fix \u03b4 > 0 and v \u2208 Rn. Let = ( 1, . . . , n) be as in Lemma 10. Then with probability at least 1\u2212 \u03b4, we have that for all intervals I, and for all non-zero linear functions f : Rd \u2192 R,\n|\u3008 I ,fI + vI\u3009| \u2016fI + vI\u2016\n\u2264 O(\u03c3 \u221a r + log(n/\u03b4)) .\nProof. Fix any interval I \u2286 [n]. Observe that any linear function on I can only take values in the range {XI\u03b8 : \u03b8 \u2208 Rd}, and hence the range of functions of the form f(xi) + v is at most an r-dimensional affine subspace. Thus, by Lemma 10, we know that for any linear function f ,\n|\u3008 I ,fI + vI\u3009| \u2016fI + vI\u2016\n\u2264 O(\u03c3 \u221a r + log(n/\u03b4)) .\nwith probability 1 \u2212 O(\u03b4/n2). By union bounding over all O(n2) intervals, we achieve the desired result.\nCorollary 12. Fix \u03b4 > 0 and v \u2208 Rn. Let = ( 1, . . . , n) be as in Lemma 10. Then with probability at least 1\u2212 \u03b4, we have\nsup f\u2208Lk |\u3008 ,fI + vI\u3009| \u2016fI + vI\u2016\n\u2264 O ( \u03c3 \u221a kr + k log n\n\u03b4\n) .\nProof. Fix any partition of [n] into k intervals I, and let SI be the set of k-piecewise linear functions which are flat on each I \u2208 I. It is not hard to see that SI is a kr-dimensional linear subspace, and hence the set of all k-piecewise linear functions which are flat on each I \u2208 I when translated by v is a kr-dimensional affine subspace. Hence Lemma 10 implies that\nsup f\u2208SI |\u3008 ,fI + vI\u3009| \u2016fI + vI\u2016 \u2264 O\n( \u03c3 \u221a kr + log 1\n\u03b4\u2032\n)\nwith probability at least 1\u2212\u03b4\u2032. A basic combinatorial argument shows that there are ( n+k\u22121 k\u22121 ) = nO(k)\nsuch different partitions I. Let \u03b4\u2032 = \u03b4/ ( n\u2212k k ) . Then the result follows by union bounding over all the different possible partitions."}, {"heading": "2.5 Runtime of linear least squares", "text": "The appeal of linear least squares is not only its statistical properties, but also that the estimator can be computed efficiently. In our algorithm, we invoke linear least squares multiple times as a black-box subrountine. Primarily, we use the following theorem:\nFact 13. Let A \u2208 Rn\u00d7d be an arbitrary data matrix, and let y be the set of responses. Then there is an algorithm LeastSquares(A, y) that runs in time O(nd2) and computes the least squares fit to this data.\nThere has been work on faster, approximate algorithms that would suffice for our purposes in theory. These algorithms offer a better dependence on the dimension d in exchange for slightly more complicated approximation guarantees and somewhat more complicated algorithms (for instance, see [CW13]). However, the classical algorithms for least squares with time complexity O(nd2) are more commonly used in practice. For this reason and to simplify our exposition, we thus present our results using the running time of the classical algorithms for least squares regression. Specifically, we write LeastSquares(X,y, I) to denote an algorithm that computes a least squares fit on a given interval I \u2286 [n] and assume that LeastSquares(X,y, I) runs in time O(|I| \u00b7 d2) for all I \u2286 [n]."}, {"heading": "3 Finding the least squares estimator via dynamic programming", "text": "In this section, we first present a dynamic programming approach (DP) to piecewise linear regression. We do not believe these results to be novel, but to the best of our knowledge, these results appear primarily as folklore in the literature. For completeness, we demonstrate the fastest DP we are aware of, and we also prove its statistical guarantees. Not only will this serve as a good warm-up for the later, more complex proofs, but we will also need a variant of this result in the later analyses."}, {"heading": "3.1 The exact DP", "text": "We first describe the exact dynamic program. It will simply find the k-piecewise linear function which minimizes the sum-squared error to the data. In other words, it outputs\narg min f\u2208Lk n\u2211 i=1 (yi \u2212 f(xi))2 = arg min f\u2208Lk \u2016y \u2212 f\u20162 ,\nwhich is simply the least-squares fit to the data amongst all k-piecewise linear functions. The dynamic program computes the estimator f as follows. For i = 1, . . . , n and j = 1, . . . , k, let A(i, j) denote the best error achievable by a j-piecewise linear function on the data pairs {(x`, y`)}i`=1, that is, the best error achievable by j pieces for the first i data points. Then it is not hard to see that\nA(i, j) = min i\u2032<i\n( err(X,y, {i\u2032 + 1, . . . , i}) +A(i\u2032, j \u2212 1) ) ,\nwhere for any interval I, we let err(X,y, I) denote the sum-squared error to the data of the best least squares fit to the data points {(x`, y`)}`\u2208I . That is, if fLSI is the least squares fit to the data in I, then err(X,y, I) = \u2016yI \u2212 fLSI \u2016\n2. The algorithm then uses dynamic programming to fill out this n \u00d7 k sized table of A values, starting at i = 1 and j = 1. After having done so, the algorithm does one additional pass backwards over the table to actually find the path through the table which achieves the best error. We can optimize this by first computing the error quantities err(X,y, {i\u2032+ 1, . . . , i}) for all i\u2032 < i, and then using a lookup table to find their values while actually executing the DP. Given such a lookup table, the DP runs in time O(n2k). Naively, the construction of this look-up table would take time which is O(n3d2) since there are O(n2) linear regression problems of size O(n), each of which takes O(nd2) time to solve.\nHowever, we can speed this up. Consider a fixed interval I \u2282 [n]. Then the least squares fit on this interval is of the form XTI XI\u03b8I = X T I yI . Assuming the matrix X T I XI is invertible, we have\nthat \u03b8I = ( XTI XI )\u22121 XTI yI . Moreover, the error of the fit is given by\n\u2016XI\u03b8I \u2212 yI\u20162 = (XI\u03b8I \u2212 yI)T (XI\u03b8I \u2212 yI) = \u03b8TI X T I XI\u03b8I \u2212 2yTXI\u03b8I + yTI yI\n= yTI XI [( XTI XI )]\u22121 XTI yI \u2212 2yTI XI\u03b8I + yTI yI .\nThe main point is the following: given ( XTI XI )\u22121 and XTI yI , we can compute all the remaining quantities in time which is O(d2). To compute ( XTI XI\n)\u22121 and XTI yI , we use the fact that our regression instances are not arbitrary but very closely related. As a result, we can re-use computation\nfrom previous regression instances that we have already solved in order to speed up later regression instances.\nConcretely, the calculations above indicate that we need to compute XTI yI for all intervals I \u2286 [n]. But these are all just sub-vectors of the vector XTy, which we compute once in time O(nd) and then use later on in all of the remaining computations. More non-trivially, we also need to compute (XIXI)\u22121 for all I \u2286 [n]. However, observe that if we let I = {`, . . . , p \u2212 1} for ` < p, then XTI\u222a{p}XI\u222a{p} = X T I XI + xpx T p , so that adding a single data point to the data matrix corresponds to a rank-one update of the data matrix. Moreover, the effect of a rank-one update to the inverse of the matrix is well-known:\nTheorem 14 (Sherman-Morrison formula). SupposeM \u2208 Rd\u00d7d is an invertible square matrix, and let v \u2208 Rd be such that 1 + vTM\u22121v 6= 0. Then\n( M + vvT )\u22121 = M\u22121 \u2212M \u22121vvTM\u22121\n1 + vTM\u22121v .\nThus, given M\u22121 and v satisfying these conditions, we may compute ( M + vvT )\u22121 in time O(d2). Therefore, the dynamic program can do as follows: for each ` = 1, . . . , n, first compute ( XTI XI\n)\u22121 for the interval I of length d starting at `, and then use the Sherman-Morrison update formula2 to compute ( XTI XI\n)\u22121 for every interval I starting at ` in total time O(nd2). Thus the algorithm takes O(n2d2) time total to compute all of the ( XTI XI\n)\u22121. As demonstrated earlier, this implies the following:\nTheorem 15. The exact dynamic program runs in time O(n2(d2 + k))."}, {"heading": "3.2 Error analysis for the exact DP", "text": "We now turn our attention to the learning rate of the exact DP. We show:\nTheorem 16. Let \u03b4 > 0, and let fLS be the k-piecewise linear estimator returned by the exact DP. Then, with probability 1\u2212 \u03b4, we we have that\nMSE(fLS) \u2264 O ( \u03c32\nkr + k log n\u03b4 n\n) .\nProof. We follow the proof technique for convergence of linear least squares as presented in [Rig15]. Recall f is the true k-piecewise linear function. Then, by the definition of the least squares fit, we have that\n\u2016y \u2212 fLS\u20162 \u2264 \u2016y \u2212 f\u20162 = \u2016 \u20162 .\nBy expanding out \u2016y \u2212 fLS\u20162, we have that\n\u2016y \u2212 fLS\u20162 = \u2016f + \u2212 fLS\u20162\n= \u2016fLS \u2212 f\u20162 + 2 \u3008 ,f \u2212 fLS\u3009+ \u2016 \u20162 . (3) 2In general, our matrices may not be invertible, or the update vector may not satisfy the condition in the ShermanMorrison formula, but in practice it seems these issues don\u2019t come up and thus we do not worry about them here. In general there are more complicated formulas for rank one updates for pseudo-inverses here but we will not cover them for simplicity.\nFrom these two calculations, we gather that\n\u2016fLS \u2212 f\u20162 \u2264 2 \u3008 ,f \u2212 fLS\u3009 \u2264 O ( \u03c3 \u221a kr + k log n\n\u03b4\n) \u00b7 \u2016fLS \u2212 f\u2016 ,\nwith probability 1\u2212\u03b4, where the last line follows from Corollary 12. A simple algebraic manipulation from this last inequality yields the desired statement.\nThe same proof technique can also be easily adapted to yield the following slight extension of Theorem 16, which can be proven via a union bound over all sets of k disjoint intervals. We omit a proof here for conciseness.\nLemma 17. Fix \u03b4 > 0. Then with probability 1\u2212 \u03b4 we have the following: for all disjoint sets of k intervals I1, . . . , Ik of [n] so that f is flat on each I`, the following inequality holds:\nk\u2211 `=1 \u2016fLSI` \u2212 fI`\u2016 2 \u2264 O(\u03c32 k(r + log(n/\u03b4))) ."}, {"heading": "4 A simple greedy merging algorithm", "text": "In this section, we give a novel greedy algorithm which runs much faster than the DP, but which achieves a somewhat worse learning rate. However, we show both theoretically and experimentally that the tradeoff between speed and statistical accuracy for this algorithm is markedly better than it is for the exact DP."}, {"heading": "4.1 The greedy merging algorithm", "text": "The overall structure of the algorithm is quite similar to [ADH+15], however, the merging criterion is different, and as explained above, the guarantees proved in that paper are insufficient to give non-trivial learning guarantees for regression.\nOur algorithm here also requires an additional input s2, which is defined to be the variance of the i variables, i.e., s2 = E[ 2i ]. Requiring that we know s2 is a drawback, and in Section 5 we give a slightly more complicated algorithm which does not require knowledge of s.\nWe give the formal pseudocode for the procedure in Algorithm 1. In the pseudocode we provide two additional tuning parameters \u03c4, \u03b3. This is because in general our algorithm cannot provide a k-histogram, but instead provides an O(k) histogram, which for most practical applications suffices. The tuning parameters allow us to trade off running time for fewer pieces. In the typical use case we will have \u03c4, \u03b3 = \u0398(1), in which case our algorithm will output an O(k)-piecewise linear function in time O(nd2 log n) time.\n4.2 Runtime of GreedyMerging\nIn this section we prove that our algorithm has the following, nearly-linear running time. The analysis is similar to the analysis presented in [ADH+15].\nTheorem 18. LetX and y be as in (1). Then GreedyMerging(\u03c4, \u03b3, s,X,y) outputs a ( (2 + 2\u03c4 )k + \u03b3 ) - piecewise linear function and runs in time O(nd2 log(n/\u03b3)).\nAlgorithm 1 Piecewise linear regression by greedy merging. 1: function GreedyMerging(\u03c4, \u03b3, s,X,y) 2: . Initial partition of [n] into intervals of length 1. 3: I0 \u2190 {{1}, {2}, . . . , {n}} 4: . Iterative greedy merging (we start with j \u2190 0). 5: while |Ij | > (2 + 2\u03c4 )k + \u03b3 do 6: Let sj be the current number of intervals. 7: . Compute the least squares fit and its error for merging neighboring pairs of intervals. 8: for u \u2208 {1, 2, . . . , sj2 } do 9: \u03b8u \u2190 LeastSquares(X,y, I2u\u22121 \u222a I2u) 10: eu = \u2016yI \u2212XI\u03b8u\u201622 \u2212 s2|I2u\u22121 \u222a I2u| 11: end for 12: Let L be the set of indices u with the (1 + 1\u03c4 )k largest errors eu, breaking ties arbitrarily. 13: Let M be the set of the remaining indices. 14: . Keep the intervals with large merging errors. 15: Ij+1 \u2190\n\u22c3 u\u2208L {I2u\u22121, I2u}\n16: . Merge the remaining intervals. 17: Ij+1 \u2190 Ij+1 \u222a {I2u\u22121 \u222a I2u |u \u2208M} 18: j \u2190 j + 1 19: end while 20: return the least squares fit to the data on every interval in Ij 21: end function\nBefore we prove this theorem, we compare this with the running time for the exact DP as given in Theorem 15. Our main advantage is that our runtime scales linearly with n instead of quadratically. This manifests itself as a substantially win theoretically in most reasonable regimes, and also as a big win in practice\u2014see our experiments for more details there.\nProof of Theorem 18. We first bound the time it takes to run any single iteration of the algorithm. In any iteration j, we do a linear least squares regression problem on each interval I \u2208 Ij ; each such problem takes O(|I|d2) time; hence solving them all takes O(nd2) time. Computing the eu given the least squares fit takes no additional time asymptotically, and finding the ( 1 + 1\u03c4 ) largest errors takes linear time [CLRS09]. Afterwards the remaining computations in this iteration can easily be seen to be done in linear time. Hence each iteration takes O(nd2) time to complete.\nWe now bound the number of iterations of the algorithm. By the same analysis as that done in the proof of Theorem 3.4 in [ADH+15] one can show that the algorithm terminates after at most log(n/\u03b3) iterations. Thus the whole algorithm runs in time O(nd2 log(n/\u03b3)) time, as claimed.\n4.3 Analysis of GreedyMerging\nTheorem 19. Let \u03b4 > 0, and let f\u0302 be the estimator returned by GreedyMerging. Let m = (2 + 2\u03c4 )k + \u03b3 be the number of pieces in f\u0302 . Then, with probability 1\u2212 \u03b4, we have that\nMSE(f\u0302) \u2264 O ( \u03c32 ( m(r + log(n/\u03b4))\nn\n) + \u03c3 \u03c4 + \u221a k\u221a n log (n \u03b4 )) .\nProof. We first condition on the event that Corollaries 7, 11 and 12, and Lemma 17 all hold with error parameter O(\u03b4), so that together they all hold with probability at least 1\u2212 \u03b4. Let I = {I1, . . . , Im} be the final partition of [n] that our algorithm produces. Recall f is the ground truth k-piecewise linear function. We partition the intervals in I into two sets:\nF = {I \u2208 I : f is flat on I} ,"}, {"heading": "J = {I \u2208 I : f has a jump on I} .", "text": "We first bound the error over the intervals in F . By Lemma 17, we have\u2211 I\u2208F \u2016fI \u2212 f\u0302I\u20162 \u2264 O(\u03c32|F|(r + log(n/\u03b4))) , (4)\nwith probability at least 1\u2212O(\u03b4). We now turn our attention to the intervals in J and distinguish two further cases. We let J1 be the set of intervals in J which were never merged, and we let J2 be the remaining intervals. If the interval I \u2208 J1 was never merged, the interval contains one point, call it i. Because we may assume that xi 6= 0, we know that for this one point, our estimator satisfies f\u0302(xi) = yi, since this is clearly the least squares fit for a linear estimator on one nonzero point. Hence Corollary 7 implies that the following inequality holds with probability at least 1\u2212O(\u03b4):\u2211\nI\u2208J1 \u2016fI \u2212 f\u0302I\u20162 = \u2211 I\u2208J1 \u2016 I\u20162\n\u2264 \u03c32 \u2211 I\u2208J1 |I|+O ( log n \u03b4 )\u221a\u2211 I\u2208J1 |I|  \u2264 \u03c32 ( m+O ( log n\n\u03b4\n)\u221a m ) . (5)\nWe now finally turn our attention to the intervals in J2. Fix an interval I \u2208 J2. By definition, the interval I was merged in some iteration of the algorithm. This implies that in that iteration, there were (1 + 1/\u03c4)k intervals M1, . . . ,M(1+1/\u03c4)k so that for each interval M`, we have\n\u2016yI \u2212 f\u0302I\u20162 \u2212 s2|I| \u2264 \u2016yM` \u2212 f LS M` \u20162 \u2212 s2|M`| . (6)\nSince the true, underlying k-piecewise linear function f has at most k jumps, this means that there are at least k/\u03c4 intervals of the M` on which f is flat. WLOG assume that these intervals are M1, . . . ,Mk/\u03c4 .\nFix any ` = 1, . . . , k/\u03c4 . Expanding out the RHS of (6) using the definition of yi gives\u2225\u2225yM` \u2212 fLSM`\u2225\u22252 \u2212 s2|M`| = \u2225\u2225fM` \u2212 fLSM`\u2225\u22252 + 2\u3008 M` ,fM` \u2212 fLSM`\u3009+ \u2016 M`\u20162 \u2212 s2|M`| = \u2225\u2225fM` \u2212 fLSM`\u2225\u22252 + 2\u3008 M` ,fM` \u2212 fLSM`\u3009+ \u2211\ni\u2208M`\n( 2i \u2212 s2) .\nThus, we have that in aggregate, k/\u03c4\u2211 `=1 \u2225\u2225yM` \u2212 fLSM`\u2225\u22252 \u2212 s2|M`| = k/\u03c4\u2211 `=1 \u2225\u2225fM` \u2212 fLSM`\u2225\u22252 + 2 k/\u03c4\u2211 `=1 \u3008 M` ,fM` \u2212 f LS M` \u3009+ k/\u03c4\u2211 `=1 \u2211 i\u2208M` ( 2i \u2212 s2) .\n(7)\nWe will upper bound each term on the RHS in turn. First, since the function f is flat on each M` for ` = 1, . . . , k/\u03c4 , Lemma 17 implies\nk/\u03c4\u2211 `=1 \u2225\u2225fM` \u2212 fLSM`\u2225\u22252 \u2264 O(\u03c32k\u03c4 (r + log n\u03b4 ) ) , (8)\nwith probability at least 1\u2212O(\u03b4). Moreover, note that the function fLSM` is a linear function on M` of the form f LS M`\n(x) = xT \u03b2\u0302, where \u03b2\u0302 \u2208 Rd is the least-squares fit on M`. Because f is just a fixed vector, the vector fM` \u2212 fLSM` lives in the affine subspace of vectors of the form fM` + (XM`)\u03b7 where \u03b7 \u2208 Rd is arbitrary. So Corollary 12 and (8) imply that\nk/\u03c4\u2211 `=1 \u3008 M` ,fM` \u2212 f LS M` \u3009 \u2264 \u221a\u221a\u221a\u221ak/\u03c4\u2211 `=1 \u3008 M` ,fM` \u2212 fLSM`\u3009 \u00b7 sup\u03b7 |\u3008 M` ,X\u03b7\u3009| \u2016X\u03b7\u2016\n\u2264 O ( \u03c32 k\n\u03c4\n( r + log n\n\u03b4\n)) . (9)\nwith probability 1\u2212O(\u03b4). By Corollary 7, we get that with probability 1\u2212O(\u03b4),\nk/\u03c4\u2211 `=1 \u2211 i\u2208M` 2i \u2212 s2|M`|  \u2264 O (\u03c3 log n \u03b4 )\u221a n .\nPutting it all together, we get that k/\u03c4\u2211 i=1 (\u2225\u2225yM` \u2212 fLSM`\u2225\u22252 \u2212 s2|M`|) \u2264 O(k\u03c4 \u03c32 (r + log n\u03b4 ) ) +O ( \u03c3 log n \u03b4 )\u221a n (10)\nwith probability 1 \u2212 O(\u03b4). Since the LHS of (6) is bounded by each individual summand above, this implies that the LHS is also bounded by their average, which implies that\n\u2016yI \u2212 f\u0302I\u20162 \u2212 s2|I| \u2264 \u03c4\nk k/\u03c4\u2211 i=1 ( \u2016yM` \u2212 f LS M` \u20162 \u2212 s2|M`| ) \u2264 O ( \u03c32 ( r + log (n \u03b4 ))) +O ( \u03c3 log n \u03b4 ) \u03c4\u221an k . (11)\nWe now similarly expand out the LHS of (6):\n\u2016yI \u2212 f\u0302I\u20162 \u2212 s2|I| = \u2016fI + I \u2212 f\u0302I\u20162 \u2212 s2|I|\n= \u2016fI \u2212 f\u0302I\u20162 + 2\u3008 I ,fI \u2212 f\u0302I\u3009+ \u2016 I\u20162 \u2212 s2|I| . (12)\nFrom this we are interested in obtaining an upper bound on \u2211\ni\u2208I(f(xi) \u2212 f\u0302(xi))2, hence we seek to lower bound the second and third terms of (12). The calculations here will very closely mirror those done above.\nBy Corollary 11, we have that\n2\u3008 I ,fI \u2212 f\u0302I\u3009 \u2265 \u2212O ( \u03c3 \u221a r + log (n \u03b4 )) \u2016fI \u2212 f\u0302I\u2016 ,\nand by Corollary 7 we have\n\u2016 I\u20162 \u2212 s2|I| \u2265 \u2212O ( \u03c3 log n\n\u03b4\n)\u221a |I| ,\nand so \u2016yI \u2212 f\u0302I\u20162 \u2212 s2|I| \u2265 \u2016fI \u2212 f\u0302I\u20162 \u2212O ( \u03c3 \u221a r + log (n \u03b4 )) \u2016fI \u2212 f\u0302I\u2016 \u2212O ( \u03c3 log n \u03b4 )\u221a |I| . (13)\nPutting (11) and (13) together yields that with probability 1\u2212O(\u03b4),\n\u2016fI \u2212 f\u0302I\u20162 \u2264 O ( \u03c32 ( r + log (n \u03b4 ))) +O ( \u03c3 \u221a r + log (n \u03b4 )) \u2016fI \u2212 f\u0302I\u2016+O ( \u03c3 log n \u03b4 )(\u03c4\u221an k + \u221a |I| ) .\nLetting z2 = \u2016f\u0302I \u2212 fI\u20162, then this inequality is of the form z2 \u2264 bz + c where b, c \u2265 0. In this specific case, we have that\nb = O ( \u03c3 \u221a r + log n\n\u03b4\n) , and\nc = O ( \u03c32 ( r + log (n \u03b4 ))) +O ( \u03c3 log n \u03b4 )(\u03c4\u221an k + \u221a |I| )\nWe now prove the following lemma about the behavior of such quadratic inequalities:\nLemma 20. Suppose z2 \u2264 bz + c where b, c \u2265 0. Then z2 \u2264 O(b2 + c).\nProof. From the quadratic formula, the inequality implies that z \u2264 b+ \u221a b2+4c 2 . From this, it is straightforward to demonstrate the desired claim.\nThus, from the lemma, we have\n\u2016fI \u2212 f\u0302I\u20162 \u2264 O ( \u03c32 ( r + log (n \u03b4 ))) +O ( \u03c3 log n \u03b4 )(\u03c4\u221an k + \u221a |I| ) .\nHence the total error over all intervals in J2 can be bounded by:\u2211 I\u2208J2 \u2016fI \u2212 f\u0302I\u20162 \u2264 O ( k\u03c32 ( r + log (n \u03b4 ))) +O ( \u03c3 log n \u03b4 )( \u03c4 \u221a n+ \u2211 I\u2208J \u221a |I| ) \u2264 O ( k\u03c32 ( r + log (n \u03b4\u2032 ))) +O ( \u03c3 log n \u03b4 )( \u03c4 \u221a n+ \u221a kn ) . (14)\nIn the last line we use that the intervals I \u2208 J2 are disjoint (and hence their cardinalities sum up to at most n), and that there are at most k intervals in J2 because the function f is k-piecewise linear. Finally, applying a union bound and summing (4), (5), and (14) yields the desired conclusion."}, {"heading": "5 A variance-free merging algorithm", "text": "In this section, we give a variant of the above algorithm that does not require knowledge of the noise variance s2. The formal pseudo code is given in Algorithm 2.\nAlgorithm 2 Variance-free greedy merging with bucketing. 1: function BucketGreedyMerging(\u03b3,X,y) 2: . Initial histogram. 3: Let I0 \u2190 {{1}, {2}, . . . , {n}} be the initial partition of [n] into intervals of length 1. 4: . Iterative greedy merging (we start with j = 0). 5: while |Ij | > (2(k + 1) + \u03b3) log n do 6: Let sj be the current number of intervals. 7: . Compute the least squares fit and its error for merging neighboring pairs of intervals. 8: for u \u2208 {1, 2, . . . , sj2 } do 9: I \u2032u \u2190 I2u\u22121 \u222a I2u 10: \u03b8u \u2190 LeastSquares(X,y, I \u2032u) 11: eu =\n1 |I\u2032u| \u2016yI \u2212XI\u03b8u\u201622\n12: end for 13: for \u03b1 = 0, . . . , log(n)\u2212 1 do 14: Let B\u03b1 be the set of indices u so that 2\u03b1 \u2264 |I \u2032u| \u2264 2\u03b1+1 15: Let L\u03b1 be the set of indices u \u2208 B\u03b1 with the k + 1 largest eu amongst u \u2208 B\u03b1, breaking ties arbitrarily. 16: Let M\u03b1 be the set of the remaining indices u in B\u03b1. 17: . Keep the intervals in each B\u03b1 with large merging errors. 18: Ij+1 \u2190\n\u22c3 u\u2208L\u03b1 {I2u\u22121, I2u}\n19: . Merge the remaining intervals in each B\u03b1. 20: Ij+1 \u2190 Ij+1 \u222a {I \u2032u |u \u2208M\u03b1} 21: end for 22: j \u2190 j + 1 23: end while 24: return the least squares fit to the data on every interval in Ij 25: end function\nWe now state the running time for BucketGreedyMerge. The running time analysis for BucketGreedyMerge is almost identical to that of GreedyMerge; hence we omit its proof.\nTheorem 21. Let X and y be as in (1). Then BucketGreedyMerging(\u03b3,X,y) outputs a (2(k + 1) + \u03b3) log n-piecewise linear function and runs in time O(nd2 log(n/\u03b3)).\n5.1 Analysis of BucketGreedyMerge\nThis section is dedicated to the proof of the following theorem:\nTheorem 22. Let f\u0302 be them-piecewise linear function that is returned by BucketGreedyMerge, where m = (2(k + 1) + \u03b3) log n. Then, with probability 1\u2212 \u03b4, we have\nMSE(f\u0302) \u2264 O ( \u03c32 ( m(r + log(n/\u03b4))\nn\n) + \u03c3 \u221a k n log (n \u03b4 )) .\nProof. As in the proof of Theorem 19, we let I be the final partition of [n] that our algorithm produces. We also similarly condition on the event that Corollaries 7 and 12 and Lemma 17 all hold with parameter O(\u03b4). We again partition I into two sets F and J as before, and further subdivide J into J1 and J2. The error on F and J1 is the same as the error given in (4) and (5). The only difference is the error on intervals in J2.\nLet I \u2208 J2 be fixed. By definition, this means there was some iteration and a collection of some k + 1 disjoint intervals M1, . . . ,Mk+1 so that |M`|/2 \u2264 |I| \u2264 2|M`| and\n1\n|I| \u2225\u2225\u2225yI \u2212 f\u0302I\u2225\u2225\u22252 \u2264 1|M`| \u2225\u2225yM` \u2212 fLSM`\u2225\u22252 for all ` = 1, . . . , k + 1. Since f has at most k jumps, this means that there is at least one interval M` so that f is flat on M`. WLOG assume that f is flat on M1. By the same kinds of calculations done in the proof of Theorem 19, we have that with probability 1\u2212 \u03b4,\u2225\u2225yM1 \u2212 fLSM1\u2225\u22252 \u2264 O (\u03c32(r + log n\u03b4 ))+ \u03c32|M1|+O(\u03c3 log(n/\u03b4))\u221a|M1| and \u2225\u2225\u2225yI \u2212 f\u0302I\u2225\u2225\u22252 \u2265 \u2225\u2225\u2225fI \u2212 f\u0302I\u2225\u2225\u22252 \u2212O (\u03c3\u221ar + log(n/\u03b4))\u2225\u2225\u2225yI \u2212 f\u0302I\u2225\u2225\u2225+ \u03c32|I| \u2212O(\u03c3 log(n/\u03b4))\u221a|I| , and putting these two equations together and rearranging, we have\u2225\u2225\u2225fI \u2212 f\u0302I\u2225\u2225\u22252 \u2264 O (\u03c32(r + log(n/\u03b4)))+ \u03c32 \u00b7O(log(n/\u03b4))( |I|\u221a|M`| + \u221a |I|\n) +O ( \u03c3 \u221a r + log(n/\u03b4)\n)\u2225\u2225\u2225yI \u2212 f\u0302I\u2225\u2225\u2225 \u2264 O ( \u03c32(r + log(n/\u03b4)) ) +O ( \u03c32 log(n/\u03b4) \u221a |I| ) +O ( \u03c3 \u221a r + log(n/\u03b4)\n)\u2225\u2225\u2225yI \u2212 f\u0302I\u2225\u2225\u2225 , where in the last inequality we used that |M`| \u2265 |I|/2. By Lemma 20, this implies that\u2225\u2225\u2225fI \u2212 f\u0302I\u2225\u2225\u22252 \u2264 O (\u03c32(r + log(n/\u03b4)))+O (\u03c32 log(n/\u03b4)\u221a|I|) . Since there are at most k elements in J2, and they are all disjoint, this yields that\u2211\nI\u2208J2\n\u2225\u2225\u2225fI \u2212 f\u0302I\u2225\u2225\u22252 \u2264 O (k\u03c32 (r + log n \u03b4 )) +O ( \u03c3 log (n \u03b4 )\u221a kn )\n(15)\nand putting this equation together with (4) and (5) yields the desired conclusion."}, {"heading": "5.2 Postprocessing", "text": "One unsatisfying aspect of BucketGreedyMerge is that it outputs O(k log n) pieces. Not only does this increase the size of the representation nontrivially when n is large, but it also increases the error rate: it is the reason why the first term in the error rate given in Theorem 22 has an additional log n factor as opposed the rate in Theorem 19. In this section, we give an efficient postprocessing procedure for BucketGreedyMerge which, when run on the output of BucketGreedyMerge, outputs an O(k) histogram with the same rate as before. In fact, the rate is slightly improved as we are able to remove the log n factor mentioned above.\nThe postprocessing algorithm Postprocessing takes as input a partition I of [n] of size O(k log n) and a target number of pieces k. It then performs the following steps: starting from the O(k log n) partition I, run the dynamic program (DP) on intervals with breakpoints amongst the breakpoints of I to find the 2k + 1 partition Ip (whose endpoints are also endpoints of I) that minimizes the sum squared error to the data. The running time analysis is identical to that of the DP with two exceptions: first, we only need to fill out a O(k log n)\u00d7(2k+1) size table (as compared to an n\u00d7 k sized table). Second, we are no longer performing rank one updates because we instead compute updates in large chunks, we cannot use the Sherman-Morrison formula to speed up the computation of the least-squares fits. Hence, we obtain the following theorem:\nTheorem 23. Given a partition I of [n] into O(k log n) pieces, Postprocessing(I, k) runs in time O\u0303(k3d2), and outputs a (2k+1)-piecewise linear function, where the O\u0303 hides poly log(n) factors.\nWe now show that the algorithm still provides the same (in fact, slightly better) statistical guarantees as the original partition:\nTheorem 24. Fix \u03b4 > 0. Let f\u0302p be the estimator output by PostprocessedBucketGreedyMerge. Then, with probability 1\u2212 \u03b4, we have\nMSE(f\u0302p) \u2264 O ( \u03c32 k ( r + log n\u03b4 ) n + \u03c3 log (n \u03b4 )\u221ak n ) .\nProof. Let I and J be as in the proof of Theorem 22. Let Ip = {J1, . . . , J2k+1} be the intervals in the partition that we return. We again condition on the event that Corollaries 7 and 12 and Lemma 17 all hold with parameter O(\u03b4). The following will then all hold with probability 1\u2212 \u03b4.\nDefine the partition K to be the partition that contains every interval in J and exactly one interval between any two non-consecutive intervals in J (i.e., K merges all flat intervals). Moreover, let g be the (2k + 1)-piecewise linear function which is the least squares fit to the data on each interval in I \u2208 K. This is clearly a possible solution for the dynamic program, and therefore we have\n\u2016f\u0302 \u2212 y\u20162 \u2264 \u2016g \u2212 y\u20162 = \u2211\nI\u2208K\\J \u2016gI \u2212 yI\u20162 + \u2211 I\u2208J \u2016gI \u2212 yI\u20162 . (16)\nWe will expand and then upper bound the RHS of (16). First, by calculations similar to those employed in the proof of Theorem 19, we have that\u2211\nI\u2208K\\J\n\u2016gI \u2212 yI\u20162 \u2264 O ( k\u03c32 ( r + log n\n\u03b4\n)) + \u2211\nI\u2208K\\J\n\u2016 I\u20162 ,\nand from (15) and Corollary 12 we have\u2211 I\u2208J \u2016gI \u2212 yI\u20162 = \u2211 I\u2208J \u2016fI + I \u2212 gI\u20162\n= \u2211 I\u2208J \u2016fI \u2212 gI\u20162 + 2 \u2211 I\u2208J \u3008 I ,fI \u2212 gI\u3009+ \u2211 I\u2208J \u2016 I\u20162\n\u2264 O ( k\u03c32 ( r + log n\n\u03b4\n)) +O ( \u03c3 log (n \u03b4 )\u221a kn ) + \u2211 I\u2208J \u2016 I\u20162 ,\nso all together now we have\u2211 I\u2208K \u2016gI \u2212 yI\u20162 \u2264 O ( k\u03c32 ( r + log n \u03b4 )) +O ( \u03c3 log (n \u03b4 )\u221a kn ) + \u2016 \u20162 .\nMoreover, by the same kinds of calculations, we can expand out the LHS of (16):\n\u2016f\u0302 \u2212 y\u20162 \u2265 \u2016f\u0302 \u2212 f\u20162 + \u3008 , f\u0302 \u2212 f\u3009+ \u2016 \u20162 \u2265 \u2016f\u0302 \u2212 f\u20162 \u2212O (\u221a k \u00b7 \u03c3 \u221a r + log n\n\u03b4\n) \u2016f\u0302 \u2212 f\u2016+ \u2016 \u20162\nand hence, combining, cancelling, and moving terms around, we get that\n\u2016f\u0302 \u2212 f\u20162 \u2264 O (\u221a k \u00b7 \u03c3 \u221a r + log n\n\u03b4\n)( 1 + \u2016f\u0302 \u2212 f\u2016 ) +O ( \u03c3 log (n \u03b4 )\u221a kn ) .\nBy Lemma 20, this implies that \u2016f\u0302 \u2212 f\u20162 \u2264 O ( k\u03c32 ( r + log n\n\u03b4\n) + \u03c3 log (n \u03b4 )\u221a kn ) ,\nwith probability 1\u2212 \u03b4, as claimed.\nWe remark that Postprocessing can also be run on the output of GreedyMerging to decrease the number of pieces from O(k) to 2k+ 1 if so desired. The proof that it maintains similar statistical guarantees is almost identical to the one presented above."}, {"heading": "6 Obtaining agnostic guarantees", "text": "In this section, we demonstrate how to show agnostic guarantees for the algorithms in the previous sections. Recall now f is arbitrary, and f\u2217 is a k-piecewise linear function which obtains the best approximation in mean-squared error to f amongst all k-piecewise linear functions. For all i = 1, . . . , n, define \u03b6i = f(xi)\u2212 f\u2217(xi) to be the error at data point i of the approximation, so that for all i, we have\nyi = f \u2217(xi) + \u03b6i + i . (17)\nBy definition, we have that \u2016\u03b6\u20162 = n \u00b7OPTk. As a warm-up, we first show the following agnostic guarantee for the exact DP:\nTheorem 25. Fix \u03b4 > 0. Let fLS be the k-piecewise linear function returned by the exact DP. Then, with probability 1\u2212 \u03b4, we have\nMSE(fLS) \u2264 O ( \u03c32 kr + k log n\u03b4\nn + \u03c3 log\n( 1\n\u03b4 )\u221a OPTk n + OPTk ) In the regimes that are most interesting, i.e., when OPTk is small, the middle term does not\ncontribute significantly to the error.\nProof. The overall proof structure stays the same. From the definition of the least-squares fit, we have\n\u2016y \u2212 fLS\u20162 \u2264 \u2016y \u2212 f\u2217\u20162\n= \u2016 + \u03b6\u20162\n= \u2016 \u20162 + 2\u3008 , \u03b6\u30092 + n \u00b7OPTk \u2264 \u2016 \u20162 +O ( \u03c3 log 1\n\u03b4\n) (n \u00b7OPTk)1/2 + n \u00b7OPTk ,\nwith probability 1\u2212O(\u03b4), where the last inequality follows from the r = 1 case of Lemma 10. The LHS can be expanded out exactly as before in (3), and putting these two sides together we obtain that\n\u2016fLS \u2212 f\u20162 \u2264 2\u3008 ,fLS \u2212 f\u3009+O ( \u03c3 log 1\n\u03b4\n) (n \u00b7OPTk)1/2 + n \u00b7OPTk\n\u2264 O ( \u03c3 \u221a kr + k log n\n\u03b4\n) \u00b7 \u2016fLS \u2212 f\u2016+O ( \u03c3 log 1\n\u03b4\n) (n \u00b7OPTk)1/2 + n \u00b7OPTk\nwith probability at least 1\u2212O(\u03b4) and so by Lemma 20 we obtain that \u2016fLS \u2212 f\u20162 \u2264 O ( \u03c32 ( kr + k log n\n\u03b4\n) + \u03c3 log ( 1\n\u03b4\n) (n \u00b7OPTk)1/2 + n \u00b7OPTk ) .\nDividing both sides by n yields the desired conclusion.\nReviewing the proof, we observe the following general pattern: in all upper bounds we wish to obtain (generally for formulas which occur on the RHS of the equations above) we obtain new OPTk terms, whereas in the lower bounds (generally for formulas on the LHS of the equations above) nothing changes. In fact, all folllowing proofs exhibit this pattern. We first establish some useful concentration bounds. By the same union-bound technique used throughout this paper, one can show the following. We omit the proof for conciseness.\nLemma 26. Fix \u03b4 > 0. Let i and \u03b6i be as in (17), for i = 1, . . . , n. Then, with probability 1 \u2212 \u03b4, we have that for all collections of k disjoint intervals J1, . . . , Jk, the following holds:\nk\u2211 `=1 \u3008 J` , \u03b6J`\u3009 \u2264 O ( k\u03c3 log n \u03b4 ) \u00b7 ( k\u2211 `=1 \u2016\u03b6J`\u2016 2 )1/2 \u2264 O ( k\u03c3 log n\n\u03b4\n) \u00b7 (n \u00b7OPTk)1/2\nObserve that with this lemma, we can easily adapt the proof of Theorem 25 to show the following agnostic version of Lemma 17:\nLemma 27. Fix \u03b4 > 0. Then with probability 1\u2212 \u03b4, we have that for all disjoint sets of k intervals J1, . . . , Jk of [n] so that f\u2217 is flat on each J`, the following inequality holds:\nk\u2211 `=1 \u2016fLSJ` \u2212 fJ`\u2016 2 \u2264 O ( \u03c32 ( kr + k log n \u03b4 ) + k\u03c3 log (n \u03b4 ) (n \u00b7OPTk)1/2 + n \u00b7OPTk ) .\nWith this lemma, we can now prove the following theorem for our greedy algorithm, which shows that in the presence of model misspecification, our algorithm still performs at a good rate:\nTheorem 28. Let \u03b4 > 0, and let f\u0302 be the estimator returned by GreedyMerging. Let m = (2 + 2\u03c4 )k + \u03b3 be the number of pieces in f\u0302 . Then, with probability 1\u2212 \u03b4, we have that\nMSE(f\u0302) \u2264 O ( \u03c32 ( m(r + log(n/\u03b4))\nn\n) + \u03c3 \u03c4 + \u221a k\u221a n log (n \u03b4 ) + k\u03c3 log (n \u03b4 )\u221aOPTk n + \u03c4 \u00b7OPTk ) .\nProof. As before, we condition on the event that Corollaries 7, 11, and 12, and Lemmas 26 and 27 all hold with error parameter O(\u03b4), so that together they all hold with probability at least 1 \u2212 \u03b4. We also let I,F ,J1, and J2 denote the same quantities as before, except we define the partition with respect to the jumps of f\u2217 instead of f , since the latter does not have well-defined jumps. For instance, F is the set of intervals in I on which f\u2217 has no jumps.\nWe again bound the error on these three sets separately. First, by Lemma 27 we have that\u2211 I\u2208F \u2016fI \u2212 f\u0302I\u20162 \u2264 O ( \u03c32 ( mr +m log n \u03b4 ) +m\u03c3 log (n \u03b4 ) (n \u00b7OPTk)1/2 + n \u00b7OPTk ) . (18)\nSecond, we bound the error on J1. By modifying the calculations as those preceding (5) in the same way as we did above in the proof of Theorem 25, we may show that\u2211\nI\u2208J1\n\u2016fI \u2212 f\u0302I\u20162 \u2264 \u03c32 ( k +O ( log n\n\u03b4\n)\u221a m ) +O ( k\u03c3 log n\n\u03b4 (n \u00b7OPTk)1/2\n) + n \u00b7OPTk . (19)\nFinally, we bound the error on J2. Fix an I \u2208 J2, and let M1, . . . ,Mk/\u03c4 be as before. Recall that this proof had two components: an upper bound for the M1, . . . ,M` and a lower bound for I. We first compute the upper bound. By following the calculations for (11) and using Lemma 27, we have\nk/\u03c4\u2211 `=1 ( \u2016yM` \u2212 f LS M` \u20162 \u2212 s2|M`| ) \u2264 O ( k \u03c4 \u03c32 ( r + log (n \u03b4 )) + \u03c3 log (n \u03b4 )\u221a n\n+ k\n\u03c4 \u03c3 log (n \u03b4 ) (n \u00b7OPTk)1/2 + n \u00b7OPTk ) The lower bound is in fact unchanged: we may use (13) as stated. Putting these two terms together and simplifying as we did previously, we obtain that\n\u2016fI \u2212 f\u0302I\u20162 \u2264 O ( \u03c32 ( r + log (n \u03b4 )) + \u03c3 log n \u03b4 ( \u03c4 \u221a n k + \u221a |I| )\n+\u03c3 log (n \u03b4 ) (n \u00b7OPTk)1/2 + \u03c4 k n \u00b7OPTk ) .\nAs before, summing up all the bounds we have achieved proves the desired claim.\nThrough virtually identical methods we also obtain the same upper bound for BucketGreedyMerge and PostprocessedBucketGreedyMerge. We state the results but omit their proofs for this reason.\nTheorem 29. Let f\u0302 be them-piecewise linear function that is returned by BucketGreedyMerge, where m = (2 + 2\u03c4 )k log n+ \u03b3. Then, with probability 1\u2212 \u03b4, we have\nMSE(f\u0302) \u2264 O ( \u03c32 ( m(r + log(n/\u03b4))\nn\n) + \u03c3 \u221a k n log (n \u03b4 ) + k\u03c3 log (n \u03b4 )\u221aOPTk n + \u03c4 \u00b7OPTk ) .\nTheorem 30. Fix \u03b4 > 0. Let f\u0302p be the estimator output by PostprocessedBucketGreedyMerge. Then, with probability 1\u2212 \u03b4, we have\nMSE(f\u0302p) \u2264 O ( \u03c32 k ( r + log n\u03b4 ) n + \u03c3 log (n \u03b4 )\u221ak n + k\u03c3 log (n \u03b4 )\u221aOPTk n + \u03c4 \u00b7OPTk ) ."}, {"heading": "7 Experiments", "text": "In addition to our theoretical analysis above, we also study the empirical performance of our new estimator for segmented regression on both real and synthetic data. As baseline, we compare our estimator (GreedyMerging) to the dynamic programming approach. Since our algorithm combines both combinatorial and linear-algebraic operations, we use the Julia programming language3 (version 0.4.2) for our experiments because Julia achieves performance close to C on both types of operations. All experiments were conducted on a laptop computer with a 2.8 GHz Intel Core i7 CPU and 16 GB of RAM.\nSynthetic data. Experiments with synthetic data allow us to study the statistical and computational performance of our estimator as a function of the problem size n. Our theoretical bounds indicate that the worst-case performance of the merging algorithm scales as O(kdn + \u221a k/n log n) for constant error variance. Compared to the O(kdn ) rate of the dynamic program, this indicates that the relative performance of our algorithm can depend on the number of features d. Hence we use two types of synthetic data: a piecewise-constant function f (effectively d = 1) and a piecewise linear function f with d = 10 features.\nWe generate the piecewise constant function f by randomly choosing k = 10 integers from the 3http://julialang.org/\nset {1, . . . , 10} as function value in each of the k segments.4 Then we draw n/k samples from each segment by adding an i.i.d. Gaussian noise term with variance 1 to each sample.\nFor the piecewise linear case, we generate a n \u00d7 d data matrix X with i.i.d. Gaussian entries (d = 10). In each segment I, we choose the parameter values \u03b2I independently and uniformly at random from the interval [\u22121, 1]. So the true function values in this segment are given by fI = XI\u03b2I . As before, we then add an i.i.d. Gaussian noise term with variance 1 to each function value.\nFigure 1 shows the results of the merging algorithm and the exact dynamic program for sample size n ranging from 102 to 104. Since the merging algorithm can produce a variable number of output segments, we run the merging algorithm with three different parameter settings corresponding to k, 2k, and 4k output segments, respectively. As predicted by our theory, the plots show that the exact dynamic program has a better statistical performance. However, the MSE of the merging algorithm with 2k pieces is only worse by a factor of 2 to 4, and this ratio empirically increases only slowly with n (if at all). The experiments also show that forcing the merging algorithm to return at most k pieces can lead to a significantly worse MSE.\nIn terms of computational performance, the merging algorithm has a significantly faster running time, with speed-ups of more than 1, 000\u00d7 for n = 104 samples. As can be seen in Figure 2, this combination of statistical and computational performance leads to a significantly improved trade-off between the two quantities. When we have a sufficient number of samples, the merging algorithm achieves a given MSE roughly 100\u00d7 faster than the dynamic program.\nReal data. We also investigate whether the merging algorithm can empirically be used to find linear trends in a real dataset. We use a time series of the Dow Jones index as input, and fit a piecewise linear function (d = 2) with 5 segments using both the dynamic program and our merging algorithm with k = 5 output pieces. As can be seen from Figure 3, the dynamic program produces a slightly better fit for the rightmost part of the curve, but the merging algorithm identifies roughly the same five main segments. As before, the merging algorithm is significantly faster and achieves a 200\u00d7 speed-up compared to the dynamic program (0.013 vs 3.2 seconds)."}, {"heading": "Acknowledgements", "text": "Part of this research was conducted while Ilias Diakonikolas was at the University of Edinburgh, Jerry Li was an intern at Microsoft Research Cambridge (UK), and Ludwig Schmidt was visiting the EECS department at UC Berkeley.\nJayadev Acharya was supported by a grant from the MIT-Shell Energy Initiative. Ilias Diakonikolas was supported in part by EPSRC grant EP/L021749/1, a Marie Curie Career Integration Grant, and a SICSA grant. Jerry Li was supported by NSF grant CCF-1217921, DOE grant DE-SC0008923, NSF CAREER Award CCF-145326, and a NSF Graduate Research Fellowship. Ludwig Schmidt was supported by grants from the MIT-Shell Energy Initiative, MADALGO, and the Simons Foundation.\n4We also repeated the experiment for other values of k. Since the results are not qualitatively different, we only report the k = 10 case here."}], "references": [{"title": "Fast and near-optimal algorithms for approximating distributions by histograms", "author": ["J. Acharya", "I. Diakonikolas", "C. Hegde", "J.Z. Li", "L. Schmidt"], "venue": "In PODS,", "citeRegEx": "Acharya et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Acharya et al\\.", "year": 2015}, {"title": "Sample-optimal density estimation in nearly-linear time", "author": ["J. Acharya", "I. Diakonikolas", "J. Zheng Li", "L Schmidt"], "venue": "CoRR, abs/1506.00671,", "citeRegEx": "Acharya et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Acharya et al\\.", "year": 2015}, {"title": "Sketching structured matrices for faster nonlinear regression", "author": ["H. Avron", "V. Sindhwani", "D. Woodruff"], "venue": "In NIPS,", "citeRegEx": "Avron et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Avron et al\\.", "year": 2013}, {"title": "Estimating and testing linear models with multiple structural changes", "author": ["J. Bai", "P. Perron"], "venue": null, "citeRegEx": "Bai and Perron.,? \\Q1998\\E", "shortCiteRegEx": "Bai and Perron.", "year": 1998}, {"title": "On risk bounds in isotonic and other shape restricted regression problems", "author": ["S. Chatterjee", "A. Guntuboyina", "B. Sen"], "venue": "Annals of Statistics, 43(4):1774\u20131800,", "citeRegEx": "Chatterjee et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Chatterjee et al\\.", "year": 2015}, {"title": "Introduction to Algorithms", "author": ["T.H. Cormen", "C.E. Leiserson", "R.L. Rivest", "C. Stein"], "venue": "3rd edition,", "citeRegEx": "Cormen et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Cormen et al\\.", "year": 2009}, {"title": "Low rank approximation and regression in input sparsity time", "author": ["K.L. Clarkson", "D.P. Woodruff"], "venue": "In STOC,", "citeRegEx": "Clarkson and Woodruff.,? \\Q2013\\E", "shortCiteRegEx": "Clarkson and Woodruff.", "year": 2013}, {"title": "On asymptotic distribution theory in segmented regression problems\u2013 identified case", "author": ["P.I. Feder"], "venue": "Annals of Statistics, 3(1):49\u201383,", "citeRegEx": "Feder.,? \\Q1975\\E", "shortCiteRegEx": "Feder.", "year": 1975}, {"title": "Multivariate adaptive regression splines", "author": ["J.H. Friedman"], "venue": "Annals of Statistics,", "citeRegEx": "Friedman.,? \\Q1991\\E", "shortCiteRegEx": "Friedman.", "year": 1991}, {"title": "Fitting segmented polynomial regression models whose join points have to be estimated", "author": ["A.R. Gallant", "Fuller W. A"], "venue": "Journal of the American Statistical Association,", "citeRegEx": "Gallant and A.,? \\Q1973\\E", "shortCiteRegEx": "Gallant and A.", "year": 1973}, {"title": "Approximation and streaming algorithms for histogram construction problems", "author": ["S. Guha", "N. Koudas", "K. Shim"], "venue": "ACM Trans. Database Syst.,", "citeRegEx": "Guha et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Guha et al\\.", "year": 2006}, {"title": "Optimal histograms with quality guarantees", "author": ["H.V. Jagadish", "Nick Koudas", "S. Muthukrishnan", "Viswanath Poosala", "Kenneth C. Sevcik", "Torsten Suel"], "venue": "In VLDB", "citeRegEx": "Jagadish et al\\.,? \\Q1998\\E", "shortCiteRegEx": "Jagadish et al\\.", "year": 1998}, {"title": "On statistics, computation and scalability", "author": ["M.I. Jordan"], "venue": "Bernoulli, 19(4):1378\u20131390,", "citeRegEx": "Jordan.,? \\Q2013\\E", "shortCiteRegEx": "Jordan.", "year": 2013}, {"title": "Fast, provable algorithms for isotonic regression in all lp-norms", "author": ["R. Kyng", "A. Rao", "S. Sachdeva"], "venue": "In NIPS,", "citeRegEx": "Kyng et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Kyng et al\\.", "year": 2015}, {"title": "Inference using shape-restricted regression splines", "author": ["M.C. Meyer"], "venue": "Annals of Applied Statistics, 2(3):1013\u20131033,", "citeRegEx": "Meyer.,? \\Q2008\\E", "shortCiteRegEx": "Meyer.", "year": 2008}, {"title": "Data analysis and regression: a second course in statistics", "author": ["F. Mosteller", "J.W. Tukey"], "venue": null, "citeRegEx": "Mosteller and Tukey.,? \\Q1977\\E", "shortCiteRegEx": "Mosteller and Tukey.", "year": 1977}, {"title": "High dimensional statistics", "author": ["P. Rigollet"], "venue": null, "citeRegEx": "Rigollet.,? \\Q2015\\E", "shortCiteRegEx": "Rigollet.", "year": 2015}, {"title": "Polynomial splines and their tensor products in extended linear modeling: 1994 wald memorial lecture", "author": ["C.J. Stone", "M.H. Hansen", "C. Kooperberg", "Y.K. Truong"], "venue": "Annals of Statistics,", "citeRegEx": "Stone et al\\.,? \\Q1997\\E", "shortCiteRegEx": "Stone et al\\.", "year": 1997}, {"title": "The use of polynomial splines and their tensor products in multivariate function estimation", "author": ["C.J. Stone"], "venue": "Annals of Statistics,", "citeRegEx": "Stone.,? \\Q1994\\E", "shortCiteRegEx": "Stone.", "year": 1994}, {"title": "Introduction to the non-asymptotic analysis of random matrices. Chapter 5 of: Compressed Sensing, Theory and Applications", "author": ["Roman Vershynin"], "venue": "Edited by Y. Eldar and G. Kutyniok", "citeRegEx": "Vershynin.,? \\Q2012\\E", "shortCiteRegEx": "Vershynin.", "year": 2012}, {"title": "Splines in statistics", "author": ["E.J. Wegman", "I.W. Wright"], "venue": "Journal of the American Statistical Association,", "citeRegEx": "Wegman and Wright.,? \\Q1983\\E", "shortCiteRegEx": "Wegman and Wright.", "year": 1983}, {"title": "Estimating and testing multiple structural changes in linear models using band spectral regressions", "author": ["Y. Yamamoto", "P. Perron"], "venue": "Econometrics Journal,", "citeRegEx": "Yamamoto and Perron.,? \\Q2013\\E", "shortCiteRegEx": "Yamamoto and Perron.", "year": 2013}], "referenceMentions": [], "year": 2016, "abstractText": "We study the fixed design segmented regression problem: Given noisy samples from a piecewise linear function f , we want to recover f up to a desired accuracy in mean-squared error. Previous rigorous approaches for this problem rely on dynamic programming (DP) and, while sample efficient, have running time quadratic in the sample size. As our main contribution, we provide new sample near-linear time algorithms for the problem that \u2013 while not being minimax optimal \u2013 achieve a significantly better sample-time tradeoff on large datasets compared to the DP approach. Our experimental evaluation shows that, compared with the DP approach, our algorithms provide a convergence rate that is only off by a factor of 2 to 4, while achieving speedups of three orders of magnitude. ar X iv :1 60 7. 03 99 0v 1 [ cs .L G ] 1 4 Ju l 2 01 6", "creator": "LaTeX with hyperref package"}}}