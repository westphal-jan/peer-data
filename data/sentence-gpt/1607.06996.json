{"id": "1607.06996", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "24-Jul-2016", "title": "Scaling Up Sparse Support Vector Machines by Simultaneous Feature and Sample Reduction", "abstract": "Sparse support vector machine (SVM) is a popular classification technique that can simultaneously learn a small set of the most interpretable features. It has achieved great success in many real-world applications. However, for large-scale problems involving a huge number of samples and extremely high-dimensional features, solving sparse SVM remains challenging as many problems as possible.\n\n\n\n\n\nWhat does it take?\nIt is quite clear that the biggest benefit of the large-scale SVM approach is that it has a strong and consistent training structure, allowing researchers to better understand how deep the SVM can be.\nThe first significant benefit from this approach is that it has the ability to efficiently compare and measure the strengths of the data sets (e.g., SVM) in comparison to existing approaches such as R, and many more.\nIt is important that the large-scale SVM algorithm is also not an absolute, finite data set (e.g., R).\nIt is also important to note that SVM does not perform deep inference, and is not an absolute data set (e.g., R).\nFor example, in C++3 a large-scale SVM program could use a series of SVM operations to perform two operations (the first in a simple program) and a second operation (the second in a simple program). This is important because the C++ API can use much of the complexity of the API, such as the ability to map multiple datasets (e.g., for example, to create a CSV file), to generate a new dataset, or to generate a new list of the relevant functions to analyze in parallel. This is useful for finding a complete data set of functions that need to be extracted in a given context or sequence of the SVM.\nThere is an open source C# compiler called MvE, which allows C# to run as many programs as possible, and its features can be found in other C++ programming languages.\nHow can you implement the high-level SVM approach in real-world application development?\nI hope this will help us answer some of the most pressing questions.\nI would like to express a few points with this simple comment that this approach can be used in real-world applications (e.g., real-world applications, databases, real-world applications). I would also like to extend this statement and give you a list of relevant functions to work on. I hope you enjoy.\nI can't say which types of programs you", "histories": [["v1", "Sun, 24 Jul 2016 04:00:30 GMT  (568kb,D)", "http://arxiv.org/abs/1607.06996v1", null], ["v2", "Sun, 14 Aug 2016 18:45:52 GMT  (570kb,D)", "http://arxiv.org/abs/1607.06996v2", null], ["v3", "Sat, 25 Feb 2017 14:25:20 GMT  (826kb,D)", "http://arxiv.org/abs/1607.06996v3", null], ["v4", "Wed, 8 Mar 2017 02:07:36 GMT  (825kb,D)", "http://arxiv.org/abs/1607.06996v4", null], ["v5", "Fri, 23 Jun 2017 08:33:14 GMT  (928kb,D)", "http://arxiv.org/abs/1607.06996v5", null]], "reviews": [], "SUBJECTS": "stat.ML cs.LG", "authors": ["weizhong zhang", "bin hong", "wei liu", "jieping ye", "deng cai", "xiaofei he", "jie wang"], "accepted": true, "id": "1607.06996"}, "pdf": {"name": "1607.06996.pdf", "metadata": {"source": "CRF", "title": "Scaling Up Sparse Support Vector Machine by Simultaneous Feature and Sample Reduction", "authors": ["Weizhong Zhang", "Bin Hong", "Jieping Ye", "Deng Cai", "Xiaofei He", "Jie Wang"], "emails": [], "sections": null, "references": [{"title": "Dimensionality reduction via sparse support vector machines", "author": ["Jinbo Bi", "Kristin Bennett", "Mark Embrechts", "Curt Breneman", "Minghu Song"], "venue": "The Journal of Machine Learning Research,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2003}, {"title": "A dynamic screening principle for the lasso", "author": ["Antoine Bonnefoy", "Valentin Emiya", "Liva Ralaivola", "R\u00e9mi Gribonval"], "venue": "In Signal Processing Conference (EUSIPCO),", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2014}, {"title": "Fast support vector machine training and classification on graphics processors", "author": ["Bryan Catanzaro", "Narayanan Sundaram", "Kurt Keutzer"], "venue": "In Proceedings of the 25th international conference on Machine learning,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2008}, {"title": "Libsvm: a library for support vector machines", "author": ["Chih-Chung Chang", "Chih-Jen Lin"], "venue": "ACM Transactions on Intelligent Systems and Technology (TIST),", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2011}, {"title": "Safe feature elimination in sparse supervised learning", "author": ["Laurent El Ghaoui", "Vivian Viallon", "Tarek Rabbani"], "venue": "Pacific Journal of Optimization,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2012}, {"title": "Liblinear: A library for large linear classification", "author": ["Rong-En Fan", "Kai-Wei Chang", "Cho-Jui Hsieh", "Xiang-Rui Wang", "Chih-Jen Lin"], "venue": "The Journal of Machine Learning Research,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2008}, {"title": "The entire regularization path for the support vector machine", "author": ["Trevor Hastie", "Saharon Rosset", "Robert Tibshirani", "Ji Zhu"], "venue": "The Journal of Machine Learning Research,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2004}, {"title": "Statistical learning with sparsity: the lasso and generalizations", "author": ["Trevor Hastie", "Robert Tibshirani", "Martin Wainwright"], "venue": null, "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2015}, {"title": "A dual coordinate descent method for large-scale linear svm", "author": ["Cho-Jui Hsieh", "Kai-Wei Chang", "Chih-Jen Lin", "S Sathiya Keerthi", "Sellamanickam Sundararajan"], "venue": "In Proceedings of the 25th international conference on Machine learning,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2008}, {"title": "Text categorization with support vector machines: Learning with many relevant features", "author": ["Thorsten Joachims"], "venue": null, "citeRegEx": "10", "shortCiteRegEx": "10", "year": 1998}, {"title": "Facial expression recognition in image sequences using geometric deformation features and support vector machines", "author": ["Irene Kotsia", "Ioannis Pitas"], "venue": "Image Processing, IEEE Transactions on,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2007}, {"title": "A topographic support vector machine: Classification using local label configurations", "author": ["Johannes Mohr", "Klaus Obermayer"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2004}, {"title": "Svm pauc tight: a new support vector method for optimizing partial auc based on a tight convex upper bound", "author": ["Harikrishna Narasimhan", "Shivani Agarwal"], "venue": "In Proceedings of the 19th ACM SIGKDD international conference on Knowledge discovery and data mining,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2013}, {"title": "Safe screening of non-support vectors in pathwise svm computation", "author": ["Kohei Ogawa", "Yoshiki Suzuki", "Ichiro Takeuchi"], "venue": "In Proceedings of the 30th International Conference on Machine Learning,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2013}, {"title": "Pegasos: Primal estimated sub-gradient solver for svm", "author": ["Shai Shalev-Shwartz", "Yoram Singer", "Nathan Srebro", "Andrew Cotter"], "venue": "Mathematical programming,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2011}, {"title": "Accelerated proximal stochastic dual coordinate ascent for regularized loss minimization", "author": ["Shai Shalev-Shwartz", "Tong Zhang"], "venue": "Mathematical Programming,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2016}, {"title": "Simultaneous safe screening of features and samples in doubly sparse modeling", "author": ["Atsushi Shibagaki", "Masayuki Karasuyama", "Kohei Hatano", "Ichiro Takeuchi"], "venue": "In Proceedings of The 33rd International Conference on Machine Learning,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2016}, {"title": "Strong rules for discarding predictors in lasso-type problems", "author": ["Robert Tibshirani", "Jacob Bien", "Jerome Friedman", "Trevor Hastie", "Noah Simon", "Jonathan Taylor", "Ryan J Tibshirani"], "venue": "Journal of the Royal Statistical Society: Series B (Statistical Methodology),", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2012}, {"title": "Scaling svm and least absolute deviations via exact data reduction", "author": ["Jie Wang", "Peter Wonka", "Jieping Ye"], "venue": "Proceedings of the 31st International Conference on Machine Learning", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2014}, {"title": "Multi-layer feature reduction for tree structured group lasso via hierarchical projection", "author": ["Jie Wang", "Jieping Ye"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2015}, {"title": "Safe screening for multi-task feature learning with multiple data matrices", "author": ["Jie Wang", "Jieping Ye"], "venue": "In Proceedings of The 32nd International Conference on Machine Learning,", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2015}, {"title": "A safe screening rule for sparse logistic regression", "author": ["Jie Wang", "Jiayu Zhou", "Jun Liu", "Peter Wonka", "Jieping Ye"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2014}, {"title": "Lasso screening rules via dual polytope projection", "author": ["Jie Wang", "Jiayu Zhou", "Peter Wonka", "Jieping Ye"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2013}, {"title": "The doubly regularized support vector machine", "author": ["Li Wang", "Ji Zhu", "Hui Zou"], "venue": "Statistica Sinica,", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2006}, {"title": "Fast lasso screening tests based on correlations", "author": ["Zhen James Xiang", "Peter J Ramadge"], "venue": "In Acoustics, Speech and Signal Processing (ICASSP),", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2012}, {"title": "Latent support measure machines for bag-of-words data classification", "author": ["Yuya Yoshikawa", "Tomoharu Iwata", "Hiroshi Sawada"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "26", "shortCiteRegEx": "26", "year": 2014}], "referenceMentions": [{"referenceID": 0, "context": "Sparse support vector machine (SVM) [1, 24] is a powerful technique that can simultaneously perform classification by margin maximization and variable selection by the `1-norm penalty [27].", "startOffset": 36, "endOffset": 43}, {"referenceID": 23, "context": "Sparse support vector machine (SVM) [1, 24] is a powerful technique that can simultaneously perform classification by margin maximization and variable selection by the `1-norm penalty [27].", "startOffset": 36, "endOffset": 43}, {"referenceID": 9, "context": "The last few years have witnessed many successful applications of sparse SVM, such as text mining [10, 26], bio-informatics [13], and image processing [12, 11].", "startOffset": 98, "endOffset": 106}, {"referenceID": 25, "context": "The last few years have witnessed many successful applications of sparse SVM, such as text mining [10, 26], bio-informatics [13], and image processing [12, 11].", "startOffset": 98, "endOffset": 106}, {"referenceID": 12, "context": "The last few years have witnessed many successful applications of sparse SVM, such as text mining [10, 26], bio-informatics [13], and image processing [12, 11].", "startOffset": 124, "endOffset": 128}, {"referenceID": 11, "context": "The last few years have witnessed many successful applications of sparse SVM, such as text mining [10, 26], bio-informatics [13], and image processing [12, 11].", "startOffset": 151, "endOffset": 159}, {"referenceID": 10, "context": "The last few years have witnessed many successful applications of sparse SVM, such as text mining [10, 26], bio-informatics [13], and image processing [12, 11].", "startOffset": 151, "endOffset": 159}, {"referenceID": 6, "context": "Many algorithms [7, 6, 3, 9, 15] have been proposed to efficiently solve the sparse SVM problems.", "startOffset": 16, "endOffset": 32}, {"referenceID": 5, "context": "Many algorithms [7, 6, 3, 9, 15] have been proposed to efficiently solve the sparse SVM problems.", "startOffset": 16, "endOffset": 32}, {"referenceID": 2, "context": "Many algorithms [7, 6, 3, 9, 15] have been proposed to efficiently solve the sparse SVM problems.", "startOffset": 16, "endOffset": 32}, {"referenceID": 8, "context": "Many algorithms [7, 6, 3, 9, 15] have been proposed to efficiently solve the sparse SVM problems.", "startOffset": 16, "endOffset": 32}, {"referenceID": 14, "context": "Many algorithms [7, 6, 3, 9, 15] have been proposed to efficiently solve the sparse SVM problems.", "startOffset": 16, "endOffset": 32}, {"referenceID": 4, "context": "An emerging technique, called screening [5], has been shown to be promising in accelerating large-scale sparse learning techniques.", "startOffset": 40, "endOffset": 43}, {"referenceID": 17, "context": "In the past few years, many screening methods are reported for a large set of sparse learning techniques, such as Lasso [18, 25, 23], tree structured group lasso [20], `1-regularized logistic regression [22], multi-task learning [21], and SVM [19, 14].", "startOffset": 120, "endOffset": 132}, {"referenceID": 24, "context": "In the past few years, many screening methods are reported for a large set of sparse learning techniques, such as Lasso [18, 25, 23], tree structured group lasso [20], `1-regularized logistic regression [22], multi-task learning [21], and SVM [19, 14].", "startOffset": 120, "endOffset": 132}, {"referenceID": 22, "context": "In the past few years, many screening methods are reported for a large set of sparse learning techniques, such as Lasso [18, 25, 23], tree structured group lasso [20], `1-regularized logistic regression [22], multi-task learning [21], and SVM [19, 14].", "startOffset": 120, "endOffset": 132}, {"referenceID": 19, "context": "In the past few years, many screening methods are reported for a large set of sparse learning techniques, such as Lasso [18, 25, 23], tree structured group lasso [20], `1-regularized logistic regression [22], multi-task learning [21], and SVM [19, 14].", "startOffset": 162, "endOffset": 166}, {"referenceID": 21, "context": "In the past few years, many screening methods are reported for a large set of sparse learning techniques, such as Lasso [18, 25, 23], tree structured group lasso [20], `1-regularized logistic regression [22], multi-task learning [21], and SVM [19, 14].", "startOffset": 203, "endOffset": 207}, {"referenceID": 20, "context": "In the past few years, many screening methods are reported for a large set of sparse learning techniques, such as Lasso [18, 25, 23], tree structured group lasso [20], `1-regularized logistic regression [22], multi-task learning [21], and SVM [19, 14].", "startOffset": 229, "endOffset": 233}, {"referenceID": 18, "context": "In the past few years, many screening methods are reported for a large set of sparse learning techniques, such as Lasso [18, 25, 23], tree structured group lasso [20], `1-regularized logistic regression [22], multi-task learning [21], and SVM [19, 14].", "startOffset": 243, "endOffset": 251}, {"referenceID": 13, "context": "In the past few years, many screening methods are reported for a large set of sparse learning techniques, such as Lasso [18, 25, 23], tree structured group lasso [20], `1-regularized logistic regression [22], multi-task learning [21], and SVM [19, 14].", "startOffset": 243, "endOffset": 251}, {"referenceID": 16, "context": "However, most existing screening methods study either feature screening or sample screening individually [17] and their applications have very different scenarios.", "startOffset": 105, "endOffset": 109}, {"referenceID": 16, "context": "[17] consider problems where both n and p are very large.", "startOffset": 0, "endOffset": 4}, {"referenceID": 1, "context": "Specifically, by noting that a class of sparse learning techniques, like sparse SVM, induce sparsities in both feature and sample spaces, they propose to simultaneously identify the inactive features and samples in a dynamic manner [2]; that is, during the optimization process, they trigger their testing rule when there is a sufficient decrease in duality gap.", "startOffset": 232, "endOffset": 235}, {"referenceID": 16, "context": "Thus, the methods in [17] can discard more inactive features and samples as the optimization proceeds and we may have small-scale problems to solve in the late stage of the optimization.", "startOffset": 21, "endOffset": 25}, {"referenceID": 16, "context": "Another appealing feature of SIFS is the so-called synergy effect [17].", "startOffset": 66, "endOffset": 70}, {"referenceID": 16, "context": "Experiments (see Section 5) on both synthetic and real data sets demonstrate that SIFS significantly outperforms the state-of-the-art [17] in improving the efficiency of sparse SVM and the speedup can be orders of magnitude.", "startOffset": 134, "endOffset": 138}, {"referenceID": 15, "context": "loss that has strong theoretical guarantees [16] and it takes the form of", "startOffset": 44, "endOffset": 48}, {"referenceID": 7, "context": ", x\u0304n) and S\u03b2(\u00b7) be the soft-thresholding operator [8], i.", "startOffset": 51, "endOffset": 54}, {"referenceID": 0, "context": "min \u03b8\u2208[0,1]n D(\u03b8;\u03b1, \u03b2) = 1 2\u03b1 \u2225\u2225\u2225\u2225S\u03b2 ( 1 n )\u2225\u2225\u2225\u22252 + \u03b3 2n\u2016\u03b8\u20162 \u2212 1 n \u03b8\u3009, (D\u2217) where 1 \u2208 Rn is a vector with all components equal to 1.", "startOffset": 6, "endOffset": 11}, {"referenceID": 0, "context": "[\u03b8\u2217(\u03b1, \u03b2)]i = \uf8f4\uf8f2\uf8f4\uf8f3 0, if 1\u2212 \u3008x\u0304i,w(\u03b1, \u03b2)\u3009 < 0, 1 \u03b3 (1\u2212 \u3008x\u0304i,w \u2217(\u03b1, \u03b2)\u3009) \u2208 [0, 1], if 0 \u2264 1\u2212 \u3008x\u0304i,w(\u03b1, \u03b2)\u3009 \u2264 \u03b3, 1, if 1\u2212 \u3008x\u0304i,w(\u03b1, \u03b2)\u3009 > \u03b3.", "startOffset": 74, "endOffset": 80}, {"referenceID": 0, "context": "min \u03b8\u0302\u2208[0,1]|D\u0302| 1 2\u03b1 \u2225\u2225\u2225\u2225S\u03b2 ( 1 n1 + 1 n2 )\u2225\u2225\u2225\u22252 + \u03b3 2n\u2016\u03b8\u0302\u20162 \u2212 1 n \u03b8\u0302\u3009, (scaled-D\u2217) (iii) : Suppose that \u03b8\u2217(\u03b1, \u03b2) is known.", "startOffset": 7, "endOffset": 12}, {"referenceID": 4, "context": "Inspired by the idea in [5], we can first estimate regions W and \u0398 that contains w\u2217(\u03b1, \u03b2) and \u03b8\u2217(\u03b1, \u03b2), respectively.", "startOffset": 24, "endOffset": 27}, {"referenceID": 17, "context": "Inspired by Strong Rule [18] and SAFE [5], we develop a sequential version of SIFS as in Algorithm 1.", "startOffset": 24, "endOffset": 28}, {"referenceID": 4, "context": "Inspired by Strong Rule [18] and SAFE [5], we develop a sequential version of SIFS as in Algorithm 1.", "startOffset": 38, "endOffset": 41}, {"referenceID": 15, "context": "In this experiment, we use Proximal Stochastic Dual Coordinate Ascent (SCDA) [16], as it is one of the state-of-the-arts.", "startOffset": 77, "endOffset": 81}, {"referenceID": 3, "context": "Dataset feature size: p sample size:n real-sim 20,958 72,309 rcv1-train 47,236 20,242 rcv1-test 47,236 677, 399 url 3,231,961 2,396,130 kddb 29,890,095 19,264,097 In this experiment, we evaluate the performance of SIFS on five large-scale real data sets: real-sim, rcv1-train, rcv1-test, url, and kddb, which are all collected from the project page of LibSVM [4].", "startOffset": 359, "endOffset": 362}, {"referenceID": 16, "context": "However, the method in [17] detects inactive features and samples in a dynamic manner [2], i.", "startOffset": 23, "endOffset": 27}, {"referenceID": 1, "context": "However, the method in [17] detects inactive features and samples in a dynamic manner [2], i.", "startOffset": 86, "endOffset": 89}, {"referenceID": 16, "context": "Thus, comparing SIFS with the method in [17] in terms of rejection ratios is inapplicable.", "startOffset": 40, "endOffset": 44}, {"referenceID": 16, "context": "We compare the performance of SIFS with the method in [17] in terms of speedup.", "startOffset": 54, "endOffset": 58}, {"referenceID": 16, "context": "Specifically, we compare the speedup gained by SIFS and the method in [17] for solving problem (P\u2217) at 1000 pairs of parameter values.", "startOffset": 70, "endOffset": 74}, {"referenceID": 16, "context": "The code we use to evaluate the method in [17] is from (https://github.", "startOffset": 42, "endOffset": 46}, {"referenceID": 16, "context": "Table 3 reports the running time of solver without and with the method in [17] and SIFS for solving problem (P\u2217) at 1000 pairs of parameter values on five real data sets.", "startOffset": 74, "endOffset": 78}, {"referenceID": 16, "context": "Moreover, the proposed SIFS significantly outperforms the method in [17] in terms of speedup\u2014by about 30 to 40 times on the aforementioned three data sets.", "startOffset": 68, "endOffset": 72}, {"referenceID": 16, "context": "Table 3: Running time (in seconds) of solver without and with the method in [17] and SIFS for solving problem (P\u2217) at 1000 pairs of parameter values on five real data sets.", "startOffset": 76, "endOffset": 80}, {"referenceID": 16, "context": "data set solver method in [17]+solver SIFS+solver screen solver speedup screen solver speedup real-sim 3.", "startOffset": 26, "endOffset": 30}, {"referenceID": 16, "context": "Instead, we can see that the solver with SIFS is about 25 times faster than the solver with the method in [17] on both data sets url and kddb.", "startOffset": 106, "endOffset": 110}, {"referenceID": 16, "context": "The solver with SIFS takes about 13 hours in solving problem (P\u2217) for all 1000 pairs of parameter values, while the solver with the method in [17] needs 11 days to finish the same task.", "startOffset": 142, "endOffset": 146}], "year": 2017, "abstractText": "Sparse support vector machine (SVM) is a popular classification technique that can simultaneously learn a small set of the most interpretable features. It has achieved great success in many real-world applications. However, for large-scale problems involving a huge number of samples and extremely high-dimensional features, solving sparse SVM remains challenging. By noting that sparse SVM induces sparsities in both feature and sample spaces, we propose a novel approach\u2014that is based on accurate estimations of the primal and dual optimums of sparse SVM\u2014to simultaneously identify the features and samples that are guaranteed to be irrelevant to the outputs. Thus, we can remove the identified samples and features from the training phase, which may lead to substantial savings in both memory usage and computational cost without sacrificing accuracy. To the best of our knowledge, the proposed method is the first static feature and sample reduction method for sparse SVM. Experiments on both synthetic and real data sets (e.g., the kddb data set with about 20 million of samples and 30 million of features) demonstrate that our approach significantly outperforms existing state-of-the-art methods and the speedup gained by our approach can be orders of magnitude.", "creator": "LaTeX with hyperref package"}}}