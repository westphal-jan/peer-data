{"id": "1608.03542", "review": {"conference": "ACL", "VERSION": "v1", "DATE_OF_SUBMISSION": "11-Aug-2016", "title": "WikiReading: A Novel Large-scale Language Understanding Task over Wikipedia", "abstract": "We present WikiReading, a large-scale natural language understanding task and publicly-available dataset with 18 million instances. The task is to predict textual values from the structured knowledge base Wikidata by reading the text of the corresponding Wikipedia articles. The task contains a rich variety of challenging classification and extraction sub-tasks, making it well-suited for end-to-end models such as deep neural networks (DNNs). We compare various state-of-the-art DNN-based architectures for document classification, information extraction, and question answering. We find that models supporting a rich answer space, such as word or character sequences, perform best. Our best-performing model, a word-level sequence to sequence model with a mechanism to copy out-of-vocabulary words, obtains an accuracy of 71.8%. These results suggest that there are two main approaches to working with DNN: the first involves a deep learning system. First, it is possible to model a language in a very large number of datasets, and the second involves a computational process. First, it is possible to model a language in a very large number of datasets, and the third involves a computational process. To work with a high-dimensional representation of a given sentence, we have implemented a simple process called 'decoding' for all the documents that we write. The simplest of two approaches is by introducing a 'decoding' technique, which is available from the WikiReading wiki. The first method is by introducing a 'decoding' technique, which is available from the WikiReading wiki. The second method is by introducing a 'decoding' method, which is available from the WikiReading wiki. The third method is by introducing a 'decoding' technique, which is available from the WikiReading wiki. The fourth method is by introducing a 'decoding' technique, which is available from the WikiReading wiki. The third method is by introducing a 'decoding' technique, which is available from the WikiReading wiki. The fourth method is by introducing a 'decoding' technique, which is available from the WikiReading wiki. The fourth method is by introducing a 'decoding' method, which is available from the WikiReading wiki. The fourth method is by introducing a 'decoding' technique, which is available from the WikiReading wiki. The fourth method is by introducing a 'decoding' technique, which is available from the WikiReading wiki. The fourth method is by introducing a 'decoding' method, which is available", "histories": [["v1", "Thu, 11 Aug 2016 17:34:12 GMT  (103kb,D)", "http://arxiv.org/abs/1608.03542v1", "ACL 2016"], ["v2", "Wed, 15 Mar 2017 19:58:44 GMT  (103kb,D)", "http://arxiv.org/abs/1608.03542v2", null]], "COMMENTS": "ACL 2016", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["daniel hewlett", "alexandre lacoste", "llion jones", "illia polosukhin", "andrew fandrianto", "jay han", "matthew kelcey", "david berthelot"], "accepted": true, "id": "1608.03542"}, "pdf": {"name": "1608.03542.pdf", "metadata": {"source": "CRF", "title": "WIKIREADING: A Novel Large-scale Language Understanding Task over Wikipedia", "authors": ["Daniel Hewlett", "Alexandre Lacoste", "Llion Jones", "Illia Polosukhin", "Andrew Fandrianto", "Jay Han", "Matthew Kelcey", "David Berthelot"], "emails": ["dhewlett@google.com", "allac@google.com", "llion@google.com", "ipolosukhin@google.com", "fto@google.com", "hanjay@google.com", "matkelcey@google.com", "dberth@google.com"], "sections": [{"heading": "1 Introduction", "text": "A growing amount of research in natural language understanding (NLU) explores end-to-end deep neural network (DNN) architectures for tasks such as text classification (Zhang et al., 2015), relation extraction (Nguyen and Grishman, 2015), and question answering (Weston et al., 2015). These models offer the potential to remove the intermediate steps traditionally involved in processing natural language data by operating on increasingly raw forms of text input, even unprocessed character or byte sequences. Furthermore, while these tasks are often studied in isolation, DNNs have the potential to combine multiple forms of reasoning within a single model.\nSupervised training of DNNs often requires a\nlarge amount of high-quality training data. To this end, we introduce a novel prediction task and accompanying large-scale dataset with a range of sub-tasks combining text classification and information extraction. The dataset is made publiclyavailable at http://goo.gl/wikireading. The task, which we call WIKIREADING, is to predict textual values from the open knowledge base Wikidata (Vrandec\u030cic\u0301 and Kro\u0308tzsch, 2014) given text from the corresponding articles on Wikipedia (Ayers et al., 2008). Example instances are shown in Table 1, illustrating the variety of subject matter and sub-tasks. The dataset contains 18.58M instances across 884 sub-tasks, split roughly evenly between classification and extraction (see Section 2 for more details).\nIn addition to its diversity, the WIKIREADING dataset is also at least an order of magnitude larger than related NLU datasets. Many natural language datasets for question answering (QA), such as WIKIQA (Yang et al., 2015), have only thousands of examples and are thus too small for training end-to-end models. Hermann et al. (2015) proposed a task similar to QA, predicting entities in news summaries from the text of the original news articles, and generated a NEWS dataset with 1M instances. The bAbI dataset (Weston et al., 2015) requires multiple forms of reasoning, but is composed of synthetically generated documents. WIKIQA and NEWS only involve pointing to locations within the document, and text classification datasets often have small numbers of output classes. In contrast, WIKIREADING has a rich output space of millions of answers, making it a challenging benchmark for state-of-the-art DNN architectures for QA or text classification.\nWe implemented a large suite of recent models, and for the first time evaluate them on common grounds, placing the complexity of the task in context and illustrating the tradeoffs inherent in each\nar X\niv :1\n60 8.\n03 54\n2v 1\n[ cs\n.C L\n] 1\n1 A\nug 2\n01 6\napproach. The highest score of 71.8% is achieved by a sequence to sequence model (Kalchbrenner and Blunsom, 2013; Cho et al., 2014) operating on word-level input and output sequences, with special handing for out-of-vocabulary words."}, {"heading": "2 WIKIREADING", "text": "We now provide background information relating to Wikidata, followed by a detailed description of the WIKIREADING prediction task and dataset."}, {"heading": "2.1 Wikidata", "text": "Wikidata is a free collaborative knowledge base containing information about approximately 16M items (Vrandec\u030cic\u0301 and Kro\u0308tzsch, 2014). Knowledge related to each item is expressed in a set of statements, each consisting of a (property, value) tuple. For example, the item Paris might have associated statements asserting (instance of, city) or (country, France). Wikidata contains over 80M such statements across 884 properties. Items may be linked to articles on Wikipedia."}, {"heading": "2.2 Dataset", "text": "We constructed the WIKIREADING dataset from Wikidata and Wikipedia as follows: We consolidated all Wikidata statements with the same item and property into a single (item, property, answer) triple, where answer is a set of values. Replacing each item with the text of the linked Wikipedia article (discarding unlinked items) yields a dataset of 18.58M (document, property, answer) instances. Importantly, all elements in each instance are human-readable strings, making the task entirely textual. The only modification we made to these strings was to\nconvert timestamps into a human-readable format (e.g., \u201c4 July 1776\u201d).\nThe WIKIREADING task, then, is to predict the answer string for each tuple given the document and property strings. This setup can be seen as similar to information extraction, or question answering where the property acts as a \u201cquestion\u201d. We assigned all instances for each document randomly to either training (12.97M instances), validation (1.88M), and test (3.73M ) sets following a 70/10/20 distribution. This ensures that, during validation and testing, all documents are unseen."}, {"heading": "2.3 Documents", "text": "The dataset contains 4.7M unique Wikipedia articles, meaning that roughly 80% of the Englishlanguage Wikipedia is represented. Multiple instances can share the same document, with a mean of 5.31 instances per article (median: 4, max: 879). The most common categories of documents are human, taxon, film, album, and human settlement, making up 48.8% of the documents and 9.1% of the instances. The mean and median document lengths are 489.2 and 203 words."}, {"heading": "2.4 Properties", "text": "The dataset contains 884 unique properties, though the distribution of properties across instances is highly skewed: The top 20 properties cover 75% of the dataset, with 99% coverage achieved after 180 properties. We divide the properties broadly into two groups: Categorical properties, such as instance of, gender and country, require selecting between a relatively small number of possible answers, while relational properties, such as date of birth,\nparent, and capital, typically require extracting rare or totally unique answers from the document.\nTo quantify this difference, we compute the entropy of the answer distribution A for each property p, scaled to the [0, 1] range by dividing by the entropy of a uniform distribution with the same number of values, i.e., H\u0302(p) = H(Ap)/ log |Ap|. Properties that represent essentially one-to-one mappings score near 1.0, while a property with just a single answer would score 0.0. Table 2 lists entropy values for a subset of properties, showing that the dataset contains a spectrum of sub-tasks. We label properties with an entropy less than 0.7 as categorical, and those with a higher entropy as relational. Categorical properties cover 56.7% of the instances in the dataset, with the remaining 43.3% being relational."}, {"heading": "2.5 Answers", "text": "The distribution of properties described above has implications for the answer distribution. There are a relatively small number of very high frequency \u201chead\u201d answers, mostly for categorical properties, and a vast number of very low frequency \u201ctail\u201d answers, such as names and dates. At the extremes, the most frequent answer human accounts for almost 7% of the dataset, while 54.7% of the answers in the dataset are unique. There are some special categories of answers which are systematically related, in particular dates, which comprise 8.9% of the dataset (with 7.2% being unique). This distribution means that methods focused on either head or tail answers can each perform moderately well, but only a method that handles both types of answers can achieve maximum performance. Another consequence of the long tail of answers is that many (30.0%) of the answers in the test set never appear in the training set, meaning they must be read out of the document. An answer\nis present verbatim in the document for 45.6% of the instances."}, {"heading": "3 Methods", "text": "Recently, neural network architectures for NLU have been shown to meet or exceed the performance of traditional methods (Zhang et al., 2015; Dai and Le, 2015). The move to deep neural networks also allows for new ways of combining the property and document, inspired by recent research in the field of question answering (with the property serving as a question). In sequential models such as Recurrent Neural Networks (RNNs), the question could be prepended to the document, allowing the model to \u201cread\u201d the document differently for each question (Hermann et al., 2015). Alternatively, the question could be used to compute a form of attention (Bahdanau et al., 2014) over the document, to effectively focus the model on the most predictive words or phrases (Sukhbaatar et al., 2015; Hermann et al., 2015). As this is currently an ongoing field of research, we implemented a range of recent models and for the first time compare them on common grounds. We now describe these methods, grouping them into broad categories by general approach and noting necessary modifications. Later, we introduce some novel variations of these models."}, {"heading": "3.1 Answer Classification", "text": "Perhaps the most straightforward approach to WIKIREADING is to consider it as a special case of document classification. To fit WIKIREADING into this framework, we consider each possible answer as a class label, and incorporate features based on the property so that the model can make different predictions for the same document. While the number of potential answers is too large to be practical (and unbounded in principle), a substantial portion of the dataset can be covered by a model with a tractable number of answers."}, {"heading": "3.1.1 Baseline", "text": "The most common approach to document classification is to fit a linear model (e.g., Logistic Regression) over bag of words (BoW) features. To serve as a baseline for our task, the linear model needs to make different predictions for the same Wikipedia article depending on the property. We enable this behavior by computing two Nw element BoW vectors, one each for the document\nand property, and concatenating them into a single 2Nw feature vector."}, {"heading": "3.1.2 Neural Network Methods", "text": "All of the methods described in this section encode the property and document into a joint representation y \u2208 Rdout , which serves as input for a final softmax layer computing a probability distribution over the top Nans answers. Namely, for each answer i \u2208 {1, . . . , Nans}, we have:\nP (i|x) = ey>ai/ \u2211Nans\nj=1 e y>aj , (1)\nwhere ai \u2208 Rdout corresponds to a learned vector associated with answer i. Thus, these models differ primarily in how they combine the property and document to produce the joint representation. For existing models from the literature, we provide a brief description and note any important differences in our implementation, but refer the reader to the original papers for further details.\nExcept for character-level models, documents and properties are tokenized into words. The Nw most frequent words are mapped to a vector in Rdin using a learned embedding matrix1. Other words are all mapped to a special out of vocabulary (OOV) token, which also has a learned embedding. din and dout are hyperparameters for these models.\nAveraged Embeddings (BoW): This is the neural network version of the baseline method described in Section 3.1.1. Embeddings for words in the document and property are separately averaged. The concatenation of the resulting vectors forms the joint representation of size 2din.\nParagraph Vector: We explore a variant of the previous model where the document is encoded as a paragraph vector (Le and Mikolov, 2014). We apply the PV-DBOW variant that learns an embedding for a document by optimizing the prediction of its constituent words. These unsupervised document embeddings are treated as a fixed input to the supervised classifier, with no fine-tuning.\nLSTM Reader: This model is a simplified version of the Deep LSTM Reader proposed by Hermann et al. (2015). In this model, an LSTM (Hochreiter and Schmidhuber, 1997) reads the property and document sequences word-by-word\n1Limited experimentation with initialization from publicly-available word2vec embeddings (Mikolov et al., 2013) yielded no improvement in performance.\nand the final state is used as the joint representation. This is the simplest model that respects the order of the words in the document. In our implementation we use a single layer instead of two and a larger hidden size. More details on the architecture can be found in Section 4.1 and in Table 4.\nAttentive Reader: This model, also presented in Hermann et al. (2015), uses an attention mechanism to better focus on the relevant part of the document for a given property. Specifically, Attentive Reader first generates a representation u of the property using the final state of an LSTM while a second LSTM is used to read the document and generate a representation zt for each word. Then, conditioned on the property encoding u, a normalized attention is computed over the document to produce a weighted average of the word representations zt, which is then used to generate the joint representation y. More precisely:\nmt = tanh(W1 concat(zt,u))\n\u03b1t = exp (v \u1d40mt) r = \u2211\nt \u03b1t\u2211 \u03c4 \u03b1\u03c4 zt\ny = tanh(W2 concat(r,u)),\nwhere W1, W2, and v are learned parameters.\nMemory Network: Our implementation closely follows the End-to-End Memory Network proposed in Sukhbaatar et al. (2015). This model maps a property p and a list of sentences x1, . . . ,xn to a joint representation y by attending over sentences in the document as follows: The input encoder I converts a sequence of words xi = (xi1, . . . , xiLi) into a vector using an embedding matrix (equation 2), where Li is the length of sentence i.2 The property is encoded with the embedding matrix U (eqn. 3). Each sentence is encoded into two vectors, a memory vector (eqn. 4) and an output vector (eqn. 5), with embedding matricesM and C, respectively. The property encoding is used to compute a normalized attention vector over the memories (eqn. 6).3 The joint representation is the sum of the output vectors weighted\n2Our final results use the position encoding method proposed by Sukhbaatar et al. (2015), which incorporates positional information in addition to word embeddings.\n3Instead of the linearization method of Sukhbaatar et al. (2015), we applied an entropy regularizer for the softmax attention as described in Kurach et al. (2015).\nby this attention (eqn. 7).\nI(xi,W ) = \u2211 jWxij (2)\nu = I(p, U) (3)\nmi = I(xi,M) (4)\nci = I(xi, C) (5) pi = softmax(q \u1d40mi) (6)\ny = u+ \u2211\ni pici (7)"}, {"heading": "3.2 Answer Extraction", "text": "Relational properties involve mappings between arbitrary entities (e.g., date of birth, mother, and author) and thus are less amenable to document classification. For these, approaches from information extraction (especially relation extraction) are much more appropriate. In general, these methods seek to identify a word or phrase in the text that stands in a particular relation to a (possibly implicit) subject. Section 5 contains a discussion of prior work applying NLP techniques involving entity recognition and syntactic parsing to this problem.\nRNNs provide a natural fit for extraction, as they can predict a value at every position in a sequence, conditioned on the entire previous sequence. The most straightforward application to WIKIREADING is to predict the probability that a word at a given location is part of an answer. We test this approach using an RNN that operates on the sequence of words. At each time step, we use a sigmoid activation for estimating whether the current word is part of the answer or not. We refer to this model as the RNN Labeler and present it graphically in Figure 1a.\nFor training, we label all locations where any answer appears in the document with a 1, and other positions with a 0 (similar to distant supervision (Mintz et al., 2009)). For multi-word an-\nswers, the word sequences in the document and answer must fully match4. Instances where no answer appears in the document are discarded for training. The cost function is the average crossentropy for the outputs across the sequence. When performing inference on the test set, sequences of consecutive locations scoring above a threshold are chunked together as a single answer, and the top-scoring answer is recorded for submission.5"}, {"heading": "3.3 Sequence to Sequence", "text": "Recently, sequence to sequence learning (or seq2seq) has shown promise for natural language tasks, especially machine translation (Cho et al., 2014). These models combine two RNNs: an encoder, which transforms the input sequence into a vector representation, and a decoder, which converts the encoder vector into a sequence of output tokens, one token at a time. This makes them capable, in principle, of approximating any function mapping sequential inputs to sequential outputs. Importantly, they are the first model we consider that can perform any combination of answer classification and extraction."}, {"heading": "3.3.1 Basic seq2seq", "text": "This model resembles LSTM Reader augmented with a second RNN to decode the answer as a sequence of words. The embedding matrix is shared across the two RNNs but their state to state transition matrices are different (Figure 1b). This method extends the set of possible answers to any sequence of words from the document vocabulary."}, {"heading": "3.3.2 Placeholder seq2seq", "text": "While Basic seq2seq already expands the expressiveness of LSTM Reader, it still has a limited vocabulary and thus is unable to generate some answers. As mentioned in Section 3.2, RNN Labeler can extract any sequence of words present in the document, even if some are OOV. We extend the basic seq2seq model to handle OOV words by adding placeholders to our vocabulary, increasing the vocabulary size fromNw toNw+Ndoc. Then, when an OOV word occurs in the document, it is replaced at random (without replacement). by one of these placeholders. We also replace the corresponding OOV words in the target output se-\n4Dates were matched semantically to increase recall. 5We chose an arbitrary threshold of 0.5 for chunking. The score of each chunk is obtained from the harmonic mean of the predicted probabilities of its elements.\nquence by the same placeholder,6 as shown in Figure 1c. Luong et al. (2015) developed a similar procedure for dealing with rare words in machine translation, copying their locations into the output sequence for further processing.\nThis makes the input and output sequences a mixture of known words and placeholders, and allows the model to produce any answer the RNN Labeler can produce, in addition to the ones that the basic seq2seq model could already produce. This approach is comparable to entity anonymization used in Hermann et al. (2015), which replaces named entities with random ids, but simpler because we use word-level placeholders without entity recognition."}, {"heading": "3.3.3 Basic Character seq2seq", "text": "Another way of handling rare words is to process the input and output text as sequences of characters or bytes. RNNs have shown some promise working with character-level input, including state-of-the-art performance on a Wikipedia text classification benchmark (Dai and Le, 2015). A model that outputs answers character by character can in principle generate any of the answers in the test set, a major advantage for WIKIREADING.\nThis model, shown in Figure 2, operates only on sequences of mixed-case characters. The property encoder RNN transforms the property, as a character sequence, into a fixed-length vector. This property encoding becomes the initial hidden state for the second layer of a two-layer document encoder RNN, which reads the document, again, character by character. Finally, the answer decoder RNN uses the final state of the previous RNN to decode the character sequence for the answer.\n6The same OOV word may occur several times in the document. Our simplified approach will attribute a different placeholder for each of these and will use the first occurrence for the target answer."}, {"heading": "3.3.4 Character seq2seq with Pretraining", "text": "Unfortunately, at the character level the length of all sequences (documents, properties, and answers) is greatly increased. This adds more sequential steps to the RNN, requiring gradients to propagate further, and increasing the chance of an error during decoding. To address this issue in a classification context, Dai and Le (2015) showed that initializing an LSTM classifier with weights from a language model (LM) improved its accuracy. Inspired by this result, we apply this principle to the character seq2seq model with a twophase training process: In the first phase, we train a character-level LM on the input character sequences from the WIKIREADING training set (no new data is introduced). In the second phase, the weights from this LM are used to initialize the first layer of the encoder and the decoder (purple and green blocks in Figure 2). After initialization, training proceeds as in the basic character seq2seq model."}, {"heading": "4 Experiments", "text": "We evaluated all methods from Section 3 on the full test set with a single scoring framework. An answer is correct when there is an exact string match between the predicted answer and the gold answer. However, as describe in Section 2.2, some answers are composed from a set of values (e.g. third example in Table 1). To handle this, we define the Mean F1 score as follows: For each instance, we compute the F1-score (harmonic mean of precision and recall) as a measure of the degree of overlap between the predicted answer set and the gold set for a given instance. The resulting perinstance F1 scores are then averaged to produce a single dataset-level score. This allows a method to obtain partial credit for an instance when it answers with at least one value from the golden set. In this paper, we only consider methods for answering with a single value, and most answers in the dataset are also composed of a single value, so this Mean F1 metric is closely related to accuracy. More precisely, a method using a single value as answer is bounded by a Mean F1 of 0.963."}, {"heading": "4.1 Training Details", "text": "We implemented all models in a single framework based on TensorFlow (Abadi et al., 2015) with shared pre-processing and comparable hyperparameters whenever possible. All documents are\ntruncated to the first 300 words except for Character seq2seq, which uses 400 characters. The embedding matrix used to encode words in the document uses din = 300 dimensions for the Nw = 100, 000 most frequent words. Similarly, answer classification over the Nans = 50, 000 most frequent answers is performed using an answer representation of size dout = 300.7 The first 10 words of the properties are embedded using the document embedding matrix. Following Cho et al. (2014), RNNs in seq2seq models use a GRU cell with a hidden state size of 1024. More details on parameters are reported in Table 4.\nOptimization was performed with the Adam stochastic optimizer8 (Kingma and Adam, 2015) over mini-batches of 128 samples. Gradient clipping 9 (Graves, 2013) is used to prevent instability in training RNNs. We performed a search over\n7For models like Averaged Embedding and Paragraph Vector, the concatenation imposes a greater dout.\n8Using \u03b21 = 0.9, \u03b22 = 0.999 and = 10\u22128. 9When the norm of gradient g exceeds a threshold C, it is\n50 randomly-sampled hyperparameter configurations for the learning rate and gradient clip threshold, selecting the one with the highest Mean F1 on the validation set. Learning rate and clipping threshold are sampled uniformly, on a logarithmic scale, over the range [10\u22125, 10\u22122] and [10\u22123, 101] respectively."}, {"heading": "4.2 Results and Discussion", "text": "Results for all models on the held-out set of test instances are presented in Table 3. In addition to the overall Mean F1 scores, the model families differ significantly in Mean F1 upper bound, and their relative performance on the relational and categorical properties defined in Section 2.4. We also report scores for properties containing dates, a subset of relational properties, as a separate column since they have a distinct format and organization. For examples of model performance on individual properties, see Table 5.\nAs expected, all classifier models perform well for categorical properties, with more sophisticated classifiers generally outperforming simpler ones. The difference in precision reading ability between models that use broad document statistics, like Averaged Embeddings and Paragraph Vectors, and the RNN-based classifiers is revealed in the scores for relational and especially date properties. As shown in Table 5, this difference is magnified in situations that are more difficult for a classifier, such as relational properties or properties with fewer training examples, where Attentive Reader outperforms Averaged Embeddings by a wide margin. This model family also has a high scaled down i.e. g\u2190 g \u00b7min ( 1, C||g|| ) .\nupper bound, as perfect classification across the 50, 000 most frequent answers would yield a Mean F1 of 0.831. However, none of them approaches this limit. Part of the reason is that their accuracy for a given answer decreases quickly as the frequency of the answer in the training set decreases, as illustrated in Figure 3. As these models have to learn a separate weight vector for each answer as part of the softmax layer (see Section 3.1), this may suggest that they fail to generalize across answers effectively and thus require significant number of training examples per answer.\nThe only answer extraction model evaluated,\nRNN Labeler, shows a complementary set of strengths, performing better on relational properties than categorical ones. While the Mean F1 upper bound for this model is just 0.434 because it can only produce answers that are present verbatim in the document text, it manages to achieve most of this potential. The improvement on date properties over the classifier models demonstrates its ability to identify answers that are typically present in the document. We suspect that answer extraction may be simpler than answer classification because the model can learn robust patterns that indicate a location without needing to learn about each answer, as the classifier models must.\nThe sequence to sequence models show a greater degree of balance between relational and categorical properties, reaching performance consistent with classifiers on the categorical questions and with RNN Labeler on relational questions. Placeholder seq2seq can in principle produce any answer that RNN Labeler can, and the performance on relational properties is indeed similar. As shown in Table 5, Placeholder seq2seq performs especially well for properties where the answer typically contains rare words such as the name of a place or person. When the set of possible answer tokens is more constrained, such\nas in categorical or date properties, the Basic seq2seq often performs slightly better. Character seq2seq has the highest upper bound, limited to 0.963 only because it cannot produce an answer set with multiple elements. LM pretraining consistently improves the performance of the Character seq2seq model, especially for relational properties as shown in Table 5. The performance of the Character seq2seq, especially with LM pretraining, is a surprising result: It performs comparably to the word-level seq2seq models even though it must copy long character strings when doing extraction and has access to a smaller portion of the document. We found the character based models to be particularly sensitive to hyperparameters. However, using a pretrained language model reduced this issue and significantly accelerated training while improving the final score. We believe that further research on pretraining for character based models could improve this result."}, {"heading": "5 Related Work", "text": "The goal of automatically extracting structured information from unstructured Wikipedia text was first advanced by Wu and Weld (2007). As Wikidata did not exist at that time, the authors relied on the structured infoboxes included in some Wikipedia articles for a relational representation of Wikipedia content. Wikidata is a cleaner data source, as the infobox data contains many slight variations in schema related to page formatting. Partially to get around this issue, the authors restrict their prediction model Kylin to 4 specific infobox classes, and only common attributes within each class.\nA substantial body of work in relation extraction (RE) follows the distant supervision paradigm (Craven and Kumlien, 1999), where sentences containing both arguments of a knowledge base (KB) triple are assumed to express the triple\u2019s relation. Broadly, these models use these distant labels to identify syntactic features relating the subject and object entities in text that are indicative of the relation. Mintz et al. (2009) apply distant supervision to extracting Freebase triples (Bollacker et al., 2008) from Wikipedia text, analogous to the relational part of WIKIREADING. Extensions to distant supervision include explicitly modelling whether the relation is actually expressed in the sentence (Riedel et al., 2010), and jointly reasoning over larger sets of sentences and relations (Sur-\ndeanu et al., 2012). Recently, Rockta\u0308schel et al. (2015) developed methods for reducing the number of distant supervision examples required by sharing information between relations."}, {"heading": "6 Conclusion", "text": "We have demonstrated the complexity of the WIKIREADING task and its suitability as a benchmark to guide future development of DNN models for natural language understanding. After comparing a diverse array of models spanning classification and extraction, we conclude that end-to-end sequence to sequence models are the most promising. These models simultaneously learned to classify documents and copy arbitrary strings from them. In light of this finding, we suggest some focus areas for future research.\nOur character-level model improved substantially after language model pretraining, suggesting that further training optimizations may yield continued gains. Document length poses a problem for RNN-based models, which might be addressed with convolutional neural networks that are easier to parallelize. Finally, we note that these models are not intrinsically limited to English, as they rely on little or no pre-processing with traditional NLP systems. This means that they should generalize effectively to other languages, which could be demonstrated by a multilingual version of WIKIREADING."}, {"heading": "Acknowledgments", "text": "We thank Jonathan Berant for many helpful comments on early drafts of the paper, and Catherine Finegan-Dollak for an early implementation of RNN Labeler."}], "references": [{"title": "Tensorflow: Largescale machine learning on heterogeneous systems", "author": ["Abadi et al.2015] Mart\u0131n Abadi", "Ashish Agarwal", "Paul Barham", "Eugene Brevdo", "Zhifeng Chen", "Craig Citro", "Greg S Corrado", "Andy Davis", "Jeffrey Dean", "Matthieu Devin"], "venue": null, "citeRegEx": "Abadi et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Abadi et al\\.", "year": 2015}, {"title": "How Wikipedia works: And how you can be a part of it", "author": ["Ayers et al.2008] Phoebe Ayers", "Charles Matthews", "Ben Yates"], "venue": null, "citeRegEx": "Ayers et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Ayers et al\\.", "year": 2008}, {"title": "Neural machine translation by jointly learning to align and translate", "author": ["Kyunghyun Cho", "Yoshua Bengio"], "venue": "International Conference on Learning Representations (ICLR)", "citeRegEx": "Bahdanau et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Bahdanau et al\\.", "year": 2014}, {"title": "Freebase: A collaboratively created graph database for structuring human knowledge", "author": ["Colin Evans", "Praveen Paritosh", "Tim Sturge", "Jamie Taylor"], "venue": "In Proceedings of the 2008 ACM SIGMOD International", "citeRegEx": "Bollacker et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Bollacker et al\\.", "year": 2008}, {"title": "Learning phrase representations using rnn encoder\u2013decoder", "author": ["Bart Van Merri\u00ebnboer", "\u00c7alar G\u00fcl\u00e7ehre", "Dzmitry Bahdanau", "Fethi Bougares", "Holger Schwenk", "Yoshua Bengio"], "venue": null, "citeRegEx": "Merri\u00ebnboer et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Merri\u00ebnboer et al\\.", "year": 2014}, {"title": "Constructing biological knowledge bases by extracting information from text sources", "author": ["Craven", "Kumlien1999] Mark Craven", "Johan Kumlien"], "venue": "In Proceedings of the Seventh International Conference on Intelligent Systems for Molecular Biology,", "citeRegEx": "Craven et al\\.,? \\Q1999\\E", "shortCiteRegEx": "Craven et al\\.", "year": 1999}, {"title": "Semi-supervised sequence learning", "author": ["Dai", "Le2015] Andrew M Dai", "Quoc V Le"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Dai et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Dai et al\\.", "year": 2015}, {"title": "Generating sequences with recurrent neural networks. CoRR, abs/1308.0850", "author": ["Alex Graves"], "venue": null, "citeRegEx": "Graves.,? \\Q2013\\E", "shortCiteRegEx": "Graves.", "year": 2013}, {"title": "Teaching machines to read and comprehend", "author": ["Tomas Kocisky", "Edward Grefenstette", "Lasse Espeholt", "Will Kay", "Mustafa Suleyman", "Phil Blunsom"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Hermann et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Hermann et al\\.", "year": 2015}, {"title": "Long short-term memory", "author": ["Hochreiter", "Schmidhuber1997] Sepp Hochreiter", "J\u00fcrgen Schmidhuber"], "venue": "Neural computation,", "citeRegEx": "Hochreiter et al\\.,? \\Q1997\\E", "shortCiteRegEx": "Hochreiter et al\\.", "year": 1997}, {"title": "Recurrent convolutional neural networks for discourse compositionality", "author": ["Kalchbrenner", "Blunsom2013] Nal Kalchbrenner", "Phil Blunsom"], "venue": "In Proceedings of the CVSC Workshop,", "citeRegEx": "Kalchbrenner et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Kalchbrenner et al\\.", "year": 2013}, {"title": "A method for stochastic optimization", "author": ["Kingma", "Adam2015] Diederik P Kingma", "Jimmy Ba Adam"], "venue": "In International Conference on Learning Representation", "citeRegEx": "Kingma et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Kingma et al\\.", "year": 2015}, {"title": "Neural randomaccess machines", "author": ["Kurach et al.2015] Karol Kurach", "Marcin Andrychowicz", "Ilya Sutskever"], "venue": "In International Conference on Learning Representations (ICLR)", "citeRegEx": "Kurach et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Kurach et al\\.", "year": 2015}, {"title": "Distributed representations of sentences and documents", "author": ["Le", "Mikolov2014] Quoc V. Le", "Tomas Mikolov"], "venue": "In Proceedings of The 31st International Conference on Machine Learning,", "citeRegEx": "Le et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Le et al\\.", "year": 2014}, {"title": "Addressing the rare word problem in neural machine translation", "author": ["Luong et al.2015] Thang Luong", "Ilya Sutskever", "Quoc Le", "Oriol Vinyals", "Wojciech Zaremba"], "venue": "In Proceedings of the 53rd Annual Meeting of the Association for Computational Lin-", "citeRegEx": "Luong et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Luong et al\\.", "year": 2015}, {"title": "Distributed representations of words and phrases and their compositionality", "author": ["Ilya Sutskever", "Kai Chen", "Greg S Corrado", "Jeff Dean"], "venue": "In Advances in neural information processing systems,", "citeRegEx": "Mikolov et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Mikolov et al\\.", "year": 2013}, {"title": "Distant supervision for relation extraction without labeled data", "author": ["Mintz et al.2009] Mike Mintz", "Steven Bills", "Rion Snow", "Dan Jurafsky"], "venue": "In Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International", "citeRegEx": "Mintz et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Mintz et al\\.", "year": 2009}, {"title": "Relation extraction: Perspective from convolutional neural networks", "author": ["Nguyen", "Grishman2015] Thien Huu Nguyen", "Ralph Grishman"], "venue": "In Proceedings of NAACL-HLT,", "citeRegEx": "Nguyen et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Nguyen et al\\.", "year": 2015}, {"title": "Modeling relations and their mentions without labeled text", "author": ["Limin Yao", "Andrew McCallum"], "venue": "In Machine Learning and Knowledge Discovery in Databases,", "citeRegEx": "Riedel et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Riedel et al\\.", "year": 2010}, {"title": "Injecting Logical Background Knowledge into Embeddings for Relation Extraction", "author": ["Sameer Singh", "Sebastian Riedel"], "venue": "In Annual Conference of the North American Chapter of the Association", "citeRegEx": "Rockt\u00e4schel et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Rockt\u00e4schel et al\\.", "year": 2015}, {"title": "End-to-end memory networks", "author": ["Jason Weston", "Rob Fergus"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Sukhbaatar et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Sukhbaatar et al\\.", "year": 2015}, {"title": "Multi-instance multi-label learning for relation extraction", "author": ["Julie Tibshirani", "Ramesh Nallapati", "Christopher D Manning"], "venue": "In Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Lan-", "citeRegEx": "Surdeanu et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Surdeanu et al\\.", "year": 2012}, {"title": "Wikidata: A free collaborative knowledgebase", "author": ["Vrande\u010di\u0107", "Kr\u00f6tzsch2014] Denny Vrande\u010di\u0107", "Markus Kr\u00f6tzsch"], "venue": "Commun. ACM,", "citeRegEx": "Vrande\u010di\u0107 et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Vrande\u010di\u0107 et al\\.", "year": 2014}, {"title": "Towards ai-complete question answering: A set of prerequisite toy", "author": ["Weston et al.2015] Jason Weston", "Antoine Bordes", "Sumit Chopra", "Tomas Mikolov"], "venue": null, "citeRegEx": "Weston et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Weston et al\\.", "year": 2015}, {"title": "Autonomously semantifying wikipedia", "author": ["Wu", "Weld2007] Fei Wu", "Daniel S Weld"], "venue": "In Proceedings of the sixteenth ACM conference on Conference on information and knowledge management,", "citeRegEx": "Wu et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Wu et al\\.", "year": 2007}, {"title": "Wikiqa: A challenge dataset for open-domain question answering", "author": ["Yang et al.2015] Yi Yang", "Wen-tau Yih", "Christopher Meek"], "venue": "In Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing,", "citeRegEx": "Yang et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Yang et al\\.", "year": 2015}, {"title": "Character-level convolutional networks for text classification", "author": ["Zhang et al.2015] Xiang Zhang", "Junbo Zhao", "Yann LeCun"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Zhang et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Zhang et al\\.", "year": 2015}], "referenceMentions": [{"referenceID": 26, "context": "A growing amount of research in natural language understanding (NLU) explores end-to-end deep neural network (DNN) architectures for tasks such as text classification (Zhang et al., 2015), relation extraction (Nguyen and Grishman, 2015), and question answering (Weston et al.", "startOffset": 167, "endOffset": 187}, {"referenceID": 23, "context": ", 2015), relation extraction (Nguyen and Grishman, 2015), and question answering (Weston et al., 2015).", "startOffset": 81, "endOffset": 102}, {"referenceID": 1, "context": "The task, which we call WIKIREADING, is to predict textual values from the open knowledge base Wikidata (Vrande\u010di\u0107 and Kr\u00f6tzsch, 2014) given text from the corresponding articles on Wikipedia (Ayers et al., 2008).", "startOffset": 191, "endOffset": 211}, {"referenceID": 25, "context": "Many natural language datasets for question answering (QA), such as WIKIQA (Yang et al., 2015), have only thousands of examples and are thus too small for train-", "startOffset": 75, "endOffset": 94}, {"referenceID": 23, "context": "The bAbI dataset (Weston et al., 2015) requires multiple forms of reasoning, but is composed of synthetically generated documents.", "startOffset": 17, "endOffset": 38}, {"referenceID": 8, "context": "Hermann et al. (2015) proposed a task similar to QA, predicting entities in news summaries from the text of the original news articles, and generated a NEWS dataset with 1M instances.", "startOffset": 0, "endOffset": 22}, {"referenceID": 26, "context": "Recently, neural network architectures for NLU have been shown to meet or exceed the performance of traditional methods (Zhang et al., 2015; Dai and Le, 2015).", "startOffset": 120, "endOffset": 158}, {"referenceID": 2, "context": "Alternatively, the question could be used to compute a form of attention (Bahdanau et al., 2014) over the document, to effectively focus the model on the most predictive words or phrases (Sukhbaatar et al.", "startOffset": 73, "endOffset": 96}, {"referenceID": 20, "context": ", 2014) over the document, to effectively focus the model on the most predictive words or phrases (Sukhbaatar et al., 2015; Hermann et al., 2015).", "startOffset": 98, "endOffset": 145}, {"referenceID": 8, "context": ", 2014) over the document, to effectively focus the model on the most predictive words or phrases (Sukhbaatar et al., 2015; Hermann et al., 2015).", "startOffset": 98, "endOffset": 145}, {"referenceID": 8, "context": "LSTM Reader: This model is a simplified version of the Deep LSTM Reader proposed by Hermann et al. (2015). In this model, an LSTM (Hochreiter and Schmidhuber, 1997) reads the property and document sequences word-by-word", "startOffset": 84, "endOffset": 106}, {"referenceID": 15, "context": "Limited experimentation with initialization from publicly-available word2vec embeddings (Mikolov et al., 2013) yielded no improvement in performance.", "startOffset": 88, "endOffset": 110}, {"referenceID": 8, "context": "Attentive Reader: This model, also presented in Hermann et al. (2015), uses an attention mechanism to better focus on the relevant part of the document for a given property.", "startOffset": 48, "endOffset": 70}, {"referenceID": 20, "context": "Memory Network: Our implementation closely follows the End-to-End Memory Network proposed in Sukhbaatar et al. (2015). This model", "startOffset": 93, "endOffset": 118}, {"referenceID": 19, "context": "Our final results use the position encoding method proposed by Sukhbaatar et al. (2015), which incorporates positional information in addition to word embeddings.", "startOffset": 63, "endOffset": 88}, {"referenceID": 19, "context": "Our final results use the position encoding method proposed by Sukhbaatar et al. (2015), which incorporates positional information in addition to word embeddings. Instead of the linearization method of Sukhbaatar et al. (2015), we applied an entropy regularizer for the softmax attention as described in Kurach et al.", "startOffset": 63, "endOffset": 227}, {"referenceID": 12, "context": "(2015), we applied an entropy regularizer for the softmax attention as described in Kurach et al. (2015).", "startOffset": 84, "endOffset": 105}, {"referenceID": 16, "context": "For training, we label all locations where any answer appears in the document with a 1, and other positions with a 0 (similar to distant supervision (Mintz et al., 2009)).", "startOffset": 149, "endOffset": 169}, {"referenceID": 14, "context": "Luong et al. (2015) developed a similar procedure for dealing with rare words in machine translation, copying their locations into the output sequence for further processing.", "startOffset": 0, "endOffset": 20}, {"referenceID": 8, "context": "This approach is comparable to entity anonymization used in Hermann et al. (2015), which replaces named entities with random ids, but simpler because we use word-level placeholders without en-", "startOffset": 60, "endOffset": 82}, {"referenceID": 0, "context": "We implemented all models in a single framework based on TensorFlow (Abadi et al., 2015) with shared pre-processing and comparable hyperparameters whenever possible.", "startOffset": 68, "endOffset": 88}, {"referenceID": 7, "context": "Gradient clipping 9 (Graves, 2013) is used to prevent instability in training RNNs.", "startOffset": 20, "endOffset": 34}, {"referenceID": 3, "context": "(2009) apply distant supervision to extracting Freebase triples (Bollacker et al., 2008) from Wikipedia text, analogous to the relational part of WIKIREADING.", "startOffset": 64, "endOffset": 88}, {"referenceID": 18, "context": "Extensions to distant supervision include explicitly modelling whether the relation is actually expressed in the sentence (Riedel et al., 2010), and jointly reasoning over larger sets of sentences and relations (Surdeanu et al.", "startOffset": 122, "endOffset": 143}, {"referenceID": 21, "context": ", 2010), and jointly reasoning over larger sets of sentences and relations (Surdeanu et al., 2012).", "startOffset": 75, "endOffset": 98}, {"referenceID": 15, "context": "Mintz et al. (2009) apply distant supervision to extracting Freebase triples (Bollacker et al.", "startOffset": 0, "endOffset": 20}, {"referenceID": 3, "context": "(2009) apply distant supervision to extracting Freebase triples (Bollacker et al., 2008) from Wikipedia text, analogous to the relational part of WIKIREADING. Extensions to distant supervision include explicitly modelling whether the relation is actually expressed in the sentence (Riedel et al., 2010), and jointly reasoning over larger sets of sentences and relations (Surdeanu et al., 2012). Recently, Rockt\u00e4schel et al. (2015) developed methods for reducing the num-", "startOffset": 65, "endOffset": 431}], "year": 2017, "abstractText": "We present WIKIREADING, a large-scale natural language understanding task and publicly-available dataset with 18 million instances. The task is to predict textual values from the structured knowledge base Wikidata by reading the text of the corresponding Wikipedia articles. The task contains a rich variety of challenging classification and extraction sub-tasks, making it well-suited for end-to-end models such as deep neural networks (DNNs). We compare various state-of-the-art DNNbased architectures for document classification, information extraction, and question answering. We find that models supporting a rich answer space, such as word or character sequences, perform best. Our best-performing model, a word-level sequence to sequence model with a mechanism to copy out-of-vocabulary words, obtains an accuracy of 71.8%.", "creator": "LaTeX with hyperref package"}}}