{"id": "1608.05001", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "16-Aug-2016", "title": "An image compression and encryption scheme based on deep learning", "abstract": "Stacked Auto-Encoder (SAE) is a kind of deep learning algorithm for unsupervised learning. Which has multi layers that project the vector representation of input data into a lower vector space. These projection vectors are dense representations of the input data and contain some common structures to make their representations look good (e.g. input data and input data). Since our model predicts an individual vector vector (or any other, common, vector), we also make the representations as small as possible to make their shape even more impressive. In a way, this means that when we set the value of each vector on top of the value of a vector, the input vector will look very similar.\n\n\nThe same thing is true for vector representation. As we move away from vector representation, we also change our model, so that the vector representation in each vector's model becomes a more robust representation. But it is also true that the vector representation in each vector's model is also an approximation to the vector representation in each vector's model. The same happens for each vector's model.\nA vector representation with a fixed value of a number of its own is often associated with the vector representation in a higher vector. A vector representation with a constant number of its own is often associated with the vector representation in the lower vector. This is especially true with vector representation, which means that our model only does a small portion of the vector representation. That means that when the model is fully formed, a vector representation is often associated with a single vector representing a vector representing a vector representing a vector representing a vector representing a vector representing a vector representing a vector representing a vector representing a vector representing a vector representing a vector representing a vector representing a vector representing a vector representing a vector representing a vector representing a vector representing a vector representing a vector representing a vector representing a vector representing a vector representing a vector representing a vector representing a vector representing a vector representing a vector representing a vector representing a vector representing a vector representing a vector representing a vector representing a vector representing a vector representing a vector representing a vector representing a vector representing a vector representing a vector representing a vector representing a vector representing a vector representing a vector representing a vector representing a vector representing a vector representing a vector representing a vector representing a vector representing a vector representing a vector representing a vector representing a vector representing a vector representing a vector representing a vector representing a vector representing a vector representing a vector representing a vector representing a vector representing a vector representing a vector representing a vector representing a vector representing a vector representing a vector representing a", "histories": [["v1", "Tue, 16 Aug 2016 14:51:25 GMT  (1009kb)", "http://arxiv.org/abs/1608.05001v1", "6 pages, 6 figures"], ["v2", "Sun, 9 Oct 2016 02:27:20 GMT  (932kb)", "http://arxiv.org/abs/1608.05001v2", "6 pages, 6 figures"]], "COMMENTS": "6 pages, 6 figures", "reviews": [], "SUBJECTS": "cs.CV cs.LG cs.MM", "authors": ["fei hu", "changjiu pu", "haowei gao", "mengzi tang", "li li"], "accepted": false, "id": "1608.05001"}, "pdf": {"name": "1608.05001.pdf", "metadata": {"source": "CRF", "title": "AN IMAGE COMPRESSION AND ENCRYPTION SCHEME BASED ON DEEP LEARNING", "authors": ["Fei Hu", "Changjiu Pu", "Haowei Gao", "Mengzi Tang", "Li Li"], "emails": [], "sections": [{"heading": null, "text": "Stacked Auto-Encoder (SAE) is a kind of deep learning algorithm for unsupervised learning. Which has mult i layers that project the vector representation of input data into a lower vector space. These projection vectors are\ndense representations of the input data. As a result, SAE can be used for image compres sion. Using chaotic logistic map, the compression ones can further be encrypted. In this study, an application of image compression and encryption is suggested using SAE and chaotic logistic map. Experiments show that this application is feasible\nand effective. It can be used for image transmission and image protection on internet simultaneously.\nKeywords: Stacked Auto-Encode, deep learning, image protection, image feature, image compression, image encryption."}, {"heading": "1. INTRODUCTION", "text": "With the development of multimedia\ntechnology and communication technology, multimedia entertainment has played an important role in people\u2019s daily lives. Pictures and videos took\nup the main part of multimedia entertainment. It brought austere challenge to store and transmit those data, and put forward higher requirement on the\nlimited-bandwidth internet, especially for large and high-quality digital images. The limited bandwidth of internet greatly restricted the development of\nimage communication, and thus the image compression technology has been more and more aroused people\u2019s attention [1]. The purpose of image\ncompression is to represent and transmit the original large image with minimal bytes, and to restore the image with not-so-bad quality. Image compression\nreduced the burden of image storage and transmission on the network, and achieved rapid real-time processing on line. The information of an\nimage is fixed, but the different representations of the image lead to different changes in the amount of data stored in the image. So in the representation\nwith larger amount of data, some data must be useless or represent the information that is represented by other data, they are irrelevant or\nredundant. The main purpose of image compression is to compress the image by removing redundant or irrelevant information, and to store and transmit\ndigital compressed data on a low bandwidth network.\nImage compression techniques can be traced\nback to the digital television signals proposed in the\nyear of 1948. There is almost 70 years of history.\nDuring this period there have been a variety of image compression coding methods. Especially in\nthe late 1980s, due to the wavelet transform theory, the fractal theory, the artificial neural network theory and the visual simulation method, image\ncompression technology was well developed. Image compression methods can be classified into two kinds: one may lose information during compression,\nand the other one can keep full information, that is, lossless coding methods and limited-distortion coding methods. Lossless coding methods will not\nsuffer loss of information after compressing images, yet without a good compression ratio. The basic principle of this kind of methods is: an image\nconsists of features, using the statistical features of the image, if a feature appears many times in the image, it will be encoded in shorter bits, and if a\nfeature appears only once or limited times, it will be encoded in longer bits. And a complete image is always composed of a large number of repeated\nfeatures. According to that, the image will be represented by many short-bits coding features and little long-bits coding features. On the basis of\nguaranteeing the image quality after compression, limited-distortion coding methods maximize the compression ratio. The original image and the\ncompressed image looks very similar though some information has changed. The normal used limited - distortion coding methods are: the predictive coding\nmethod, the transform coding method and the statistical coding method. The limited-distortion coding method is more frequently uses than the\nlossless coding method because the former one has\na larger compression ratio. Guaranteed under premise visual effects, which remove the\ninformation that the human eyes are not sensitive to.\nThe features of images can be learned\nautomatically using deep learning models, rather\nthan proposed manually. Suitable features can improve the performance of image recognition. Over the past years, features of images were always\nspecified manually that depended on the designers\u2019 prior knowledge, and the number of features were very limited. Deep learning models can\nautomatically learn unlimited number of features automatically. A good feature-extraction method is a prerequisite for optimization of image processing.\nUsing deep learning models, unpredictable features of images can be learned, and these unpredictable features can also be used for image protection. In\nthis study, we proposed a model to compress and encrypt images. Based on SAE, a multi-layer model is constructed. An image is put into the first layer\nand the output data from different level of layers reconstruct the original image in different level of comprehension. If the size of the output data from\nan arbitrary layer is less than the size of the original image, the representation in this layer is a compression representation. Because the model has\nmore than one hidden layer whose neurons are less than the input layer\u2019s, the model can achieve multiple levels of features, and each level of features\nrepresent a compressed image. So, multiple\ncompression ratio can be obtained using this model. The compressed image is further encrypted using\nchaotic logistic map. This model can be used in tasks that have certain requirements for image transmission speed and security."}, {"heading": "2. RELATED WORKS", "text": ""}, {"heading": "2.1. Stacked Auto-Encoder", "text": "Auto-Encoder (AE) is a single hidden layer\nmodel, and is an unsupervised learning neural network, see Fig. 1 (b). It is actually generated by two identical Restricted Boltzmann Machine models\n(RBMs) [2], see Fig. 1 (a). A RBM and a reversed RBM generate an AE model. Stacked Auto-Encoder (SAE) is a multilayer AE, it is composed with\nseveral AEs. The previous AE\u2019s output is the later AE\u2019s input, i.e. several AEs\u2019 encoding sections are put together one by one and their decoding sections\nare put together in reverse order. This is a more complex AE model, having several hidden layers rather than one. Using greedy training methods,\nmonolayer AE can be trained to learn weights directly, however, it is hard for SAE because more several hidden layers would consume too more\ncomputation time. In order to cut down the training time, the training process of SAE is divided into two steps: pre-training and fine-training. At first, each\nhidden layer is trained one by one, then the entire model is trained using the Contrastive Divergence (CD-k) [3].\n(a)\n(b)\nFig.1. RBM & Auto-encoder, (a) Restricted Boltzmann Machine (RBM), (b) Auto-Encoder.\nIn Fig. 2, the left three layers \uff08X, h1, h2\uff09 constitute the encoding part of the SAE model. In\nthe pre-training phase, the input data X is encoded and yield h1, and then h1 is decoded and yield X\u2019, the error e=X\u2019-X, e is used to adjust the weights\nbetween the layer X and the layer h1; then the output value of previous AE is set as the input data of layer\nh1, it is encoded and yield h2, h2 is decoded and yield h1\u2019, the weights between the layer h1 and the\nlayer h2 are adjusted using e=h1\u2019-h1; after multiple encoding and decoding operations, pre-optimal parameters (W, b) are got, and they make the model\neasy to train in the fine-training phase.\nThe encoding part \uff08X, h1, h2\uff09 of the SAE\nmodel is flipped to get a decoding part: \uff08h2, h1T,\nXT\uff09. The two parts are combined to form a model that has the functions of encoding and decoding, see\nFig.2. Stacked Auto-encoder.\n1) Feedforward processing is performed by\ncomputing the activation for each middle layer; 2) The grade error for the output layer is\n\ud835\udeff(\ud835\udc5b\ud835\udc56) = \u2212(\ud835\udc66 \u2212 \ud835\udc4e (\ud835\udc5b\ud835\udc56) ) \u2219 \ud835\udc53\u2032(\ud835\udc67(\ud835\udc5b\ud835\udc56) ) (1)\n3) The grade error for every middle layer is \ud835\udeff(\ud835\udc59) = ((\ud835\udc4a(\ud835\udc59) )\ud835\udc47 \ud835\udeff(\ud835\udc59+1) ) \u2219 \ud835\udc53\u2032 (\ud835\udc67(\ud835\udc59) ) (2) 4) The partial derivatives:\n\u2207\ud835\udc4a(\ud835\udc59) J(\ud835\udc4a , \ud835\udc4f; \ud835\udc65, \ud835\udc66) = \ud835\udeff (\ud835\udc59+1) (\ud835\udc4e(\ud835\udc59) )\ud835\udc47 (3)\n\u2207\ud835\udc4f(\ud835\udc59) J(\ud835\udc4a, \ud835\udc4f; \ud835\udc65, \ud835\udc66) = \ud835\udeff (\ud835\udc59+1) (4)\n5) the overall cost function is set as the\nfollowing:\n\ud835\udc3d(\ud835\udc4a , \ud835\udc4f) = [ 1\n\ud835\udc5a \u2211 \ud835\udc3d(\ud835\udc4a , \ud835\udc4f; \ud835\udc65\n(\ud835\udc56) ,\ud835\udc66 (\ud835\udc56) )\ud835\udc5a\ud835\udc56=1 ] (5)\nSAE has strong representation expression\nability and advantages of deep neural networks. AE\ncan learn the characteristics of input data, then SAE can learn multi-level characteristics. In the first hidden layer, SAE can learn first-order features of\nthe input data; in the second hidden layer, SAE can learn second-order features of the input data. E.g. the input data is a set of images, the first hidden layer\nmay learn a collection of edges, and the second hidden layers may learn how to combine a number\nof edges together to form an outline, a higher hidden layer may learn much more vivid, special and meaningful features. Features of each level can help\nus better operate image processing, such as image classification, information retrieval of images, and so on. These features also can be used to compress\nimages. For example, an image with 100 pixels is put into the input layer, the input layer has 100 neurons, each pixel is put into a corresponding\nneuron, then a hidden layer with only 10 neurons yields a 10-dimensional vector, which owns features of the input data and can be considered as a\nreconstruction of the input image, so this image is compressed and the compression ratio is 10. 2.2. Image encryption schema using chaotic\nlogistic map\nChaos-based cryptographic algorithms have\nsuggested efficient ways to develop secure image\nencryption. These algorithms are sensitive to their initial conditions. Any tiny change can cause greatly different responses that guarantees the efficiency of\nencryption schemas. The logistic map is one of them. It is an iterated logistic map that has proved great importance in many fields of information processing.\nSuch fields include but are not limited in the following: population biology, chemistry, encryption, communication and ecology. It also\nworks in modelling the dynamics of a single species. The stability and bifurcation of the logistic map has been studied a lot, such as Cohen-Grossberg neural\nnetworks with delays [5] and the Neimark-Sacker bifurcation with delay [6].\nThe logistic map is of a non-linear recursive\nrelation. It can suggest deterministic chaos. Its mathematical equation is written as: \ud835\udc65\ud835\udc5b+1 = \ud835\udc5f\ud835\udc65\ud835\udc5b(1 \u2212 \ud835\udc65\ud835\udc5b) (6) where x0 is an initial condition which is a float number between 0 and 1 (exclude 0 or 1), r is a\npositive constant which is also a float number between 3.5699 and 4 (include 4, and 3.5699 is an approximation). After N iterations, a sequence will\nbe got. The sequence is like the form of {x1, x2, x3, \u2026, xN}. It is a stochastic sequence which can further be used for encryption tasks. In this study,\nthe initialized number x0 is generated from the SAE model, and then is used for image encryption. 3. IMAGE COMPRESSION ENCRYP TIO N\nMODEL\nFig.3. The compression encryption model.\nThe image is compressed using the SAE, and\nthen the compressed image is encrypted using the\nsequence generated by the chaotic logistic map.\nThe diagram in Fig. 3 shows the algorithm of\nthis model. See the following for detail:\n1) Initialization: A five-layer SAE model is constructed (see Fig. 2). In the model, the second layer has a less number\nof neurons than the input layer in order to realize the primary image compression, and the third layer has a less number of neurons than the second layer in\norder to realize the second-stage image compression. The rest fourth and fifth layers are separately the mirrors for the second and first layers. In the study\nof image processing using Convolutional Neural Networks (CNN), it was supposed and has been proved by a large number of experiments [7] that an\nimage could be divided into a number of regions and the characteristics learned by CNN from different regions are similar or even the same. We use this\nsupposition in our study. The image is divided into pieces which have the same size. Every piece is a sample. A training set consists of all pieces from one\nsingle image. For the convenience of handling images in SAE, values in this data set will be normalized as float numbers in the range of 0 to 1\nbefore they are put into the model. Our model is trained using this normalized data set. Levels of learned features will be anti-normalized to get the\nfinal output, which are values of pixels for a compressed image. According to the dense representation, the image is compressed. The\nconstant r is initialized; 2) Learning compression representation using the SAE model:\nThe activation function f (\u2219) is a nonlinear\nfunction, and the sigmoid function is used in the experiment. By training the SAE model, we get a\ncompression representation from an arbitrary hidden layer. This representation then forms a compressed image. Because the sigmoid function output is a\nfloat number between 0 and 1, which meets the requirements for initializing x0, a certain one from the output values is chosen as x0. In the experiment , the first one is chosen; 3) Generating a chaotic sequence: Using chaotic logistic map with x0 and r, a sequence S is generated, S= {x1, x2, x3, \u2026, xN}, where N is the size of the compressed image, e g, a compressed image has 100*100 pixels ,\nN=100*100=10000; 4) Image encryption and image decryption: The encryption and decryption functions are\ndescribed following, where E is the encrypted image, C is the plain image (the compressed image), S is the sequence generated in step 3), bitxor(-) is a bit XOR\nfunction;\n\ud835\udc38 = \ud835\udc4f\ud835\udc56\ud835\udc61\ud835\udc65\ud835\udc5c\ud835\udc5f(\ud835\udc36, \ud835\udc46) (7) \ud835\udc36 = \ud835\udc4f\ud835\udc56\ud835\udc61\ud835\udc65\ud835\udc5c\ud835\udc5f(\ud835\udc38, \ud835\udc46) (8)\n5) Image reconstruction:\nThe compressed image is recovered through\nthe SAE model. See Fig. 2, if the compressed image came from the layer of h2, a new model is\nreconstructed with only the layers of {X, h1, h2} which shares the parameters learned in step 2). The compressed image is normalized and is put into the\nlayer of h2, then the output from the layer of X represent the recovered image.\nThe practicability of the algorithm will be\nverified in the next section. 4. EXPERIMENTS\nThis new model was evaluated on several\nimages taken from the standard set of images. They are Lena, baboon, lake and pepper. They have the same size as 512 by 512. Images split into pieces\nwith the same size of 8 by 8. The number of neurons in the input layer was 64, and the number of neurons in the hidden layers was adjusted to achieve\ndifferent compression ratios (CRs), that is, 4:1 and 16:1 for 16 and 4 neurons. In the back part of this section, the compressed images were encrypted.\nCorrelation Analysis was performed to evaluate the effect of the encryption schema.\nCompression effects are shown in Fig. 4. In\norder to quantitatively verify the effects, Mean\nSquare Error (MSE) and Peak Signal-to-Noise Ratio (PSNR) are introduced. MSE [8] is the average of the square of the difference between the expected\nresponse and the actual output. It is also called squared error loss. PSNR [9] is the ratio of maximum power of the signal and the power of\nnoise. It is commonly used to measure the quality of reconstruction in image compression. Their mathematical definitions are following equations:\nMSE = 1\nm\u00d7n \u2211 \u2211 [IO (i, j)-IR (i, j)] 2n-1 j=0 m-1 i=0 (9)\n\ud835\udc43\ud835\udc46\ud835\udc41\ud835\udc45 = 10 log10( \ud835\udc40\ud835\udc34\ud835\udc4b\ud835\udc3c\n2\n\ud835\udc40\ud835\udc46\ud835\udc38 ) (10)\nwhere MAXI is the maximum possible pixel value of the image, that is 255 in this experiment, m*n is the image size, IO is the original image and IR is the reconstructed image.\n(a) Original image of Lena, image reconstructed at\na CR of 4:1, image reconstructed at a CR of 16:1\n(b) Original image of baboon, image reconstructed\nat a CR of 4:1, image reconstructed at a CR of 16:1\n(c) Original image of lake, image reconstructed at\na CR of 4:1, image reconstructed at a CR of 16:1\n(d) Original image of pepper, image reconstructed\nat a CR of 4:1, image reconstructed at a CR of 16:1\nFig.4. Compression effects.\nMSE and PSNR were computed for three\nprimary colour channels (Red, Green and Blue, also called RGB), respectively. And the results at the CRs of 4:1 and 16:1 are listed in Table 1 and Table\n2, respectively.\nThe encrypted images of compression ones at\nCRs of 4:1 and 16:1 are shown in Fig. 5 and Fig. 6,\nrespectively. Correlation Analysis was performed to quantitatively evaluate the effect of the encryption schema. The correlation coefficient is used to\nevaluate the correlation of a pair of adjacent pixels , and it is defined below [10]:\n\ud835\udc5f\ud835\udc65\ud835\udc66 = (\ud835\udc38(\ud835\udc65\ud835\udc66) \u2212 \ud835\udc38(\ud835\udc65)\ud835\udc38(\ud835\udc66))/(\u221a\ud835\udc37(\ud835\udc65)\u221a\ud835\udc37(\ud835\udc66)) (11) where rxy is the correlation coefficient of the variables x and y, E (\u2219) is the mean function, D (\u2219) is\nthe variance function, x and y are adjacent pixels.\nFig.5. encrypted images of compression ones at a"}, {"heading": "CR of 4:1, (a) encrypted Lena, (b) encrypted", "text": "baboon, (c) encrypted lake, (d) encrypted pepper.\n(a)\n(b)\n(c)\n(d)\nFig.6. encrypted images of compression ones at a"}, {"heading": "CR of 16:1, (a) encrypted Lena, (b) encrypted baboon, (c) encrypted lake, (d) encrypted pepper.", "text": "To simplify the processing, colour images were\nconverted to grayscale before use. Experiments were repeated ten times, correlation coefficients of each image were averaged. Results for reconstructed\nimages and encrypted compression images at the CRs of 4:1 and 16:1 are listed in Table 3.\nTable 3. Correlation Analysis for reconstructed images and encrypted compression images at the CRs of 4:1.\nImages\nCR of 4:1 CR of 16:1\nComp-\nressed\nEncry-\npted\nComp-\nressed\nEncry-\npted\nLena 0.9877 0.0179 0.9832 -0.0052\nBaboon 0.9227 0.0601 0.9726 0.0132\nLake 0.9820 -0.0087 0.9855 0.0267\nPepper 0.9796 0.0488 0.9902 0.0261\nThe experimental results show that this new\nmodel is effective and it can be used for image transmission and image protection on internet simultaneously."}, {"heading": "5. CONCLUSIONS", "text": "An application of deep learning in image\ncompression and encryption was proposed. Based\non SAE neural networks, images are compressed. And then the compressed ones are encrypted using chaotic logistic map. This application can be used\nfor image transmission and image protection on internet simultaneously."}, {"heading": "Acknowledgements", "text": "This work was supported by Scientific and Technological Research Program of Chongqing\nMunicipal Education Commission (No. KJ1501405,\nNo. KJ1501409); Scientific and Technological Research Program of Chongqing University of\nEducation (No. KY201522B, No. KY201520B); Fundamental Research Funds for the Central Universities (No. XDJK2016E068); Natural\nScience Foundation of China (No. 61170192) and National High-tech R&D Program (No. 2013AA013801)."}, {"heading": "6. REFERENCES", "text": "1. Zhang, Chunhong (2016), \"Application of MultiWavelet Analysis in Image Compression\", REVISTA TECNICA DE LA\nFACULTAD DE INGENIERIA UNIVERSIDAD DEL ZULIA, 39 (3), pp. 76-82. 2. Fischer, A., & Igel, C. (2014), \"Training restricted\nBoltzmann machines: An introduction\", Pattern Recognition, 47(1), 25-39. 3. Ma, X., & Wang, X. (2016), \"Average Contrastive\nDivergence for Training Restricted Boltzmann Machines\", Entropy, 18(1), 35. 4. Sarangi, P. P., Sahu, A., & Panda, M. (2013), \"A\nhybrid differential evolution and back-propagation algorithm for feedforward neural network training\", International Journal of Computer\nApplications, 84(14). 5. Liu, Q., & Yang, S. (2014), \"Stability and bifurcation of a class of discrete-time Cohen\u2013\nGrossberg neural networks with discrete delays\", Neural Processing Letters, 40(3), 289-300. 6. Sarmah, H. K. R., Das, M. C., & Baishya, T. K.\nR. (2014), \"Neimark-Sacker bifurcation in delayed logistic map\", International Journal of Applied Mathematics & Statistical Sciences, 3(1), 19-34.\n7. LeCun, Y., Bengio, Y., & Hinton, G. (2015) , \"Deep learning\", Nature, 521(7553), 436-444. 8. Tiwari, M., & Gupta, B. (2015), \"Image\nDenoising Using Spatial Gradient Based Bilateral Filter and Minimum Mean Square Error Filtering\", Procedia Computer Science, 54, 638-645.\n9. Karamchandani, S. H., Gandhi, K. J., Gosalia, S. R., Madan, V. K., Merchant, S. N., & Desai, U. B. (2015), \"PCA encrypted short acoustic data\ninculcated in Digital Color Images\", International Journal of Computers Communications & Control, 10(5), 678-685.\n10. Zhang, Y. Q., & Wang, X. Y. (2015), \"A new image encryption algorithm based on non-adjacent coupled map lattices\", Applied Soft Computing, 26,\n10-20."}], "references": [{"title": "Application of MultiWavelet Analysis in  Image Compression", "author": ["Zhang", "Chunhong"], "venue": "REVISTA TECNICA DE LA FACULTAD DE INGENIERIA UNIVERSIDAD DEL ZULIA,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2016}, {"title": "Training restricted Boltzmann machines: An introduction", "author": ["A. Fischer", "C. Igel"], "venue": "Pattern Recognition,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2014}, {"title": "2016), \"Average Contrastive Divergence for Training", "author": ["X. Ma", "X. Wang"], "venue": "Restricted Boltzmann Machines\",", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2016}, {"title": "A hybrid differential evolution and back-propagation algorithm for feedforward neural network training", "author": ["P.P. Sarangi", "A. Sahu", "M. Panda"], "venue": "International Journal of Computer Applications,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2013}, {"title": "Stability and bifurcation of a class of discrete-time Cohen\u2013 Grossberg neural networks with discrete delays", "author": ["Q. Liu", "S. Yang"], "venue": "Neural Processing Letters,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2014}, {"title": "Neimark-Sacker bifurcation in delayed logistic map", "author": ["H.K.R. Sarmah", "M.C. Das", "T.K.R. Baishya"], "venue": "International Journal of Applied Mathematics & Statistical Sciences,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2014}, {"title": "Image Denoising Using Spatial Gradient Based Bilateral Filter and Minimum Mean Square Error Filtering", "author": ["M. Tiwari", "B. Gupta"], "venue": "Procedia Computer Science,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2015}, {"title": "PCA encrypted short acoustic data inculcated in Digital Color Images", "author": ["S.H. Karamchandani", "K.J. Gandhi", "S.R. Gosalia", "V.K. Madan", "S.N. Merchant", "U.B. Desai"], "venue": "International Journal of Computers Communications & Control,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2015}, {"title": "A new image encryption algorithm based on non-adjacent coupled map lattices", "author": ["Y.Q. Zhang", "X.Y. Wang"], "venue": "Applied Soft Computing,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2015}], "referenceMentions": [{"referenceID": 0, "context": "image communication, and thus the image compression technology has been more and more aroused people\u2019s attention [1].", "startOffset": 113, "endOffset": 116}, {"referenceID": 1, "context": "(RBMs) [2], see Fig.", "startOffset": 7, "endOffset": 10}, {"referenceID": 2, "context": "hidden layer is trained one by one, then the entire model is trained using the Contrastive Divergence (CD-k) [3].", "startOffset": 109, "endOffset": 112}, {"referenceID": 3, "context": "The detailed fine-training process is described as the following steps [4]:", "startOffset": 71, "endOffset": 74}, {"referenceID": 4, "context": "The stability and bifurcation of the logistic map has been studied a lot, such as Cohen-Grossberg neural networks with delays [5] and the Neimark-Sacker bifurcation with delay [6].", "startOffset": 126, "endOffset": 129}, {"referenceID": 5, "context": "The stability and bifurcation of the logistic map has been studied a lot, such as Cohen-Grossberg neural networks with delays [5] and the Neimark-Sacker bifurcation with delay [6].", "startOffset": 176, "endOffset": 179}, {"referenceID": 6, "context": "MSE [8] is the average of the square of the difference between the expected response and the actual output.", "startOffset": 4, "endOffset": 7}, {"referenceID": 7, "context": "PSNR [9] is the ratio of maximum power of the signal and the power of noise.", "startOffset": 5, "endOffset": 8}, {"referenceID": 8, "context": "The correlation coefficient is used to evaluate the correlation of a pair of adjacent pixels , and it is defined below [10]: rxy = (E(xy) \u2212 E(x)E(y))/(\u221aD(x)\u221aD(y)) (11) where rxy is the correlation coefficient of the variables x and y, E (\u2219) is the mean function, D (\u2219) is the variance function, x and y are adjacent pixels.", "startOffset": 119, "endOffset": 123}], "year": 2016, "abstractText": "Stacked Auto-Encoder (SAE) is a kind of deep learning algorithm for unsupervised learning. Which has mult i layers that project the vector representation of input data into a lower vector space. These projection vectors are dense representations of the input data. As a result, SAE can be used for image compres sion. Using chaotic logistic map, the compression ones can further be encrypted. In this study, an application of image compression and encryption is suggested using SAE and chaotic logistic map. Experiments show that this application is feasible and effective. It can be used for image transmission and image protection on internet simultaneously.", "creator": "Microsoft\u00ae Word 2013"}}}