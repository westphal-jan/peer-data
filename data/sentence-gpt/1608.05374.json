{"id": "1608.05374", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "18-Aug-2016", "title": "DNN-based Speech Synthesis for Indian Languages from ASCII text", "abstract": "Text-to-Speech synthesis in Indian languages has a seen lot of progress over the decade partly due to the annual Blizzard challenges. These systems assume the text to be written in Devanagari or Dravidian scripts which are nearly phonemic orthography scripts. However, the most common form of computer interaction among Indians is ASCII written transliterated text (e.g. in Dravidian or Dravidian) and is typically used to understand text. Some are not particularly common in some regions. As of this writing the text is still written in Dravidian and Dravidian languages.\n\n\nIn a number of languages, however, the need for more complex, complex communication between languages for understanding text is evident. In this example the script in Dravidian is written in a different language. The script in Dravidian in Dravidian is written in the same language. The Dravidian in Dravidian is written in the same language. In this example the script in Dravidian is written in the same language. The script in Dravidian is written in the same language.\nMany languages in Dravidian are often used in different languages. Some are not especially common in some parts of the world. In the case of Dravidian the script in Dravidian is written in a different language. In this example the script in Dravidian is written in a different language. The script in Dravidian is written in a different language. In this example the script in Dravidian is written in a different language. In this example the script in Dravidian is written in the same language. In this example the script in Dravidian is written in the same language.\nMany languages in Dravidian are often used in different languages. Some are not especially common in some parts of the world. In the case of Dravidian the script in Dravidian is written in the same language. In this example the script in Dravidian is written in a different language. In this example the script in Dravidian is written in the same language. In this example the script in Dravidian is written in the same language. In this example the script in Dravidian is written in the same language. In this example the script in Dravidian is written in the same language. In this example the script in Dravidian is written in the same language. In this example the script in Dravidian is written in the same language.\nThere are several common", "histories": [["v1", "Thu, 18 Aug 2016 18:58:39 GMT  (155kb,D)", "http://arxiv.org/abs/1608.05374v1", "6 pages, 5 figures -- Accepted in 9th ISCA Speech Synthesis Workshop"]], "COMMENTS": "6 pages, 5 figures -- Accepted in 9th ISCA Speech Synthesis Workshop", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["srikanth ronanki", "siva reddy", "bajibabu bollepalli", "simon king"], "accepted": false, "id": "1608.05374"}, "pdf": {"name": "1608.05374.pdf", "metadata": {"source": "CRF", "title": "DNN-based Speech Synthesis for Indian Languages from ASCII text", "authors": ["Srikanth Ronanki", "Siva Reddy", "Bajibabu Bollepalli", "Simon King"], "emails": ["srikanth.ronanki@ed.ac.uk"], "sections": [{"heading": "1. Introduction", "text": "Though a large number of Indian languages have indigenous scripts, the lack of a standardized keyboard, and the ubiquity of QWERTY keyboards, means that people most often write using ASCII1 [1] text using spellings motivated largely by pronunciation [2]. Increasingly, many technologies such as Web search and natural language processing are adapting to this phenomenon [3, 4, 5]. In the area of Speech Synthesis, although the efforts of the 2013, 2014 and 2015 Blizzard Challenges2 [6, 7] resulted in improvements to the naturalness of speech synthesis of Indian languages, the text was assumed to be written in native script. In this work, we transliterate Blizzard data to informal chat-style ASCII text using Mechanical Turkers, and synthesize speech from the resulting transliterated ASCII text. This represents a more realistic use case than in the Blizzard Challenge.\nSynthesizing speech from ASCII text is challenging: Since there is no standard way to spell pronunciations, people often spell same word in multiple ways, e.g., the word start in Telugu can be ASCII spelled prarhambham, prarambham, prarambam, praranbam, etc. whilst words that differ in both pronunciation\n1The ASCII character set is the union of Roman alphabets, digits, and a few punctuation marks.\n2http://www.synsig.org/index.php/Blizzard_ Challenge\nand meaning might be spelled the same, e.g., the words ledhu and ledu in Telugu could both be spelled ledu.\nWe address these problems by first converting ASCII graphemes to phonemes, followed by a DNN to synthesise the speech. We propose three methods for converting graphemes to phonemes. The first model is a naive model which assumes that every grapheme corresponds to a phoneme. In the second model, we enhance the naive model by treating frequently cooccurring character bi-grams as additional phonemes. In the final model, we learn a Grapheme-to-Phoneme transducer from parallel ASCII text and gold-standard phonetic transcriptions. The contributions of this paper are:\n\u2022 to synthesize speech from ASCII transliterated text for Indian languages, which to our knowledge is the first such attempt. Our results show that our Graphemeto-Phoneme conversion model combined with a DNN acoustic model performs competitively with state-of-theart speech synthesizers that use native script text.\n\u2022 the release of parallel ASCII transliterations of Blizzard data to foster research in this area."}, {"heading": "2. Related work", "text": ""}, {"heading": "2.1. Transliteration of Indian Languages", "text": "Many standard transliteration systems exist for Indian languages. Table 1 shows different transliterations for an example sentence. Among these, CPS (Common Phone Set) [8] and IT3 [9] are widely used by the speech technology community, ITRANS3 [10] is used in publishing houses, and WX4 [11] by the Natural Language Processing (NLP) community. Though these scripts provide umambiguous conversion to native Indian scripts, due to their lack of readability, and the overhead in learning how to use them, people still spell their words motivated by pronunciation. One such transliteration is shown in the row Informal of Table 1.\nThe most common trend observed in the literature is to treat transliteration as a machine translation and discriminative ranking problem [12]. Our work aims to exploit the fact that transliterations are phonetically motivated, and therefore treat transliteration as a conversion problem. Specifically, we convert informal transliterations to phonetic script, and then synthesize speech from the phonetic script using a DNN."}, {"heading": "2.2. Statistical Speech Synthesis", "text": "Most existing work in speech synthesis for Indian languages uses unit selection [13] with syllable-like units [14, 15]. Re-\n3https://en.wikipedia.org/wiki/ITRANS 4https://en.wikipedia.org/wiki/WX_notation\nar X\niv :1\n60 8.\n05 37\n4v 1\n[ cs\n.C L\n] 1\n8 A\nug 2\n01 6\ncently, based on the observation that Indian languages share many commonalities in phonetics, a language independent phone set was proposed, and was used in building statistical parametric (HMM-based) speech synthesis systems [8]. We make use of this common phone set in one of our models.\nOur work also aligns with the recent literature on unsupervised learning for text-to-speech synthesis which aims to reduce the reliance on human knowledge and the manual effort required for building language-specific resources [16, 17, 18]. These approaches are able to learn from noisy input representations where there is no standard orthography. Following the success of DNNs for speech recognition [19] and synthesis [20, 21, 22], we also use a DNN as the acoustic model."}, {"heading": "3. Our Approach", "text": "Our speech synthesis pipeline consists of two steps: 1) Converting the input ASCII transliterated text to a phonetic script; 2) learning a DNN based speech synthesizer from the parallel phonetic text and audio signal."}, {"heading": "3.1. Converting ASCII text to Phonetic Script", "text": "We explore three different approaches which vary in the degree of supervision in defining a phoneme."}, {"heading": "3.1.1. Uni-Grapheme Model", "text": "In this approach, we assume each ASCII grapheme acts as a phoneme. We assume that the DNN will learn to map these \u201cphonemes\u201d to speech sounds. We normalize the data to lowercase and remove all punctuation marks. This ensures that the phone-set contains 26 letters and an extra /sil/ phone to mark beginning and end of the sentence."}, {"heading": "3.1.2. Multi-Grapheme Model", "text": "In this approach, in addition to uni-graphemes, we also include some frequently co-occurring bi-graphemes as \u201cphonemes\u201d.\nFrom manual inspection of the top 50 bi-graphemes, we found that the phonemes indicating stop consonants such as /kh/, /ch/, /th/, /ph/, /bh/ and long vowels such as /aa/, /ii/, /ee/, /oo/, /uu/ and dipthongs such as /ai/, /au/, /ou/ appear most frequently across languages. We selected 17 of these bi-graphemes as phonemes in addition to the above 27 unigraphemes, making a total of 44 phonemes."}, {"heading": "3.1.3. Grapheme-to-Phoneme (G2P) Model", "text": "In this model, we assume the phoneme-set is given. We use the common phone set (CPS, [8]) to work with the languages of interest. We convert the native text to CPS phonetic text using deterministic converters [9, 23]. We then align the phonetic transcriptions to the ASCII transliterations from Mechanical Turkers to create a pronunciation table. Table 2 shows the parallel data with the native text in the first column, the informal ASCII transliteration in the second column, and the CPS phonetic transcription in the third column.\nGiven the pronunciation lexicon, we train a G2P transducer [24] for each language separately with varying n-gram sequences. The corpus used for training is described in Section 4.1. Figure 1 displays the phone error rate of the G2P model with varying n-grams. The 6-gram model achieved the lowest phone error rate across the three languages. Telugu and Tamil achieved lower phonetic error rates compared to Hindi. This can be attributed to the ineffective handling of intricate schwa deletion, a well-known phenomenon in Indo-Aryan languages.\nAn advantage with this model is that, since the phonemeset is standard, we can train G2P and DNN on two independent datasets \u2013 G2P on parallel transliterations of a very large corpus that could be obtained via crowdsourcing, and DNN model on gold phonetic speech transcriptions independently of the G2P model\u2019s performance. We leave this aspect of our work for future. In this work, we train a DNN model on the output of G2P aligned with natural speech."}, {"heading": "3.2. DNN Speech Synthesizer", "text": "We use a DNN for learning to synthesize speech from the phonetic strings obtained in the previous step. We use two independent DNNs \u2013 one for duration and the other for acoustic modeling.\nLet xi = [xi(1), ..., xi(dx)]T and yi = [yi(1), ..., yi(dy)]T be static input and output feature vectors of the DNN, where dx and dy denote the dimensions of xi and yi, respectively, and T denotes transposition.\nDuration Model: For duration modeling, the input comprises binary features (xp) derived from a subset of the questions used by the decision-tree clustering in the standard HTS synthesiser. Similar to [20, 21], frame-aligned data for DNN training is created by forced alignment using the HMM system. The output is an eight-dimensional vector (yp) of durations for every phone, comprising five sub-state durations, the overall phone duration, syllable duration and whole word duration. We use this form of multi-task learning to improve the model; the three additional features (phone, syllable, and word durations) act as a secondary task to help the network learn more about suprasegmental variations in duration at word level. At synthesis time, these features are predicted, but ignored.\nAcoustic Model: The input uses the same features as duration prediction, to which 9 numerical features are appended. These capture frame position in the HMM state and phoneme, state position in phoneme, and state and phoneme duration. The DNN outputs comprise MCCs, BAPs and continuous logf0 (all with deltas and delta-deltas) plus a voiced/unvoiced binary value.\nIn both acoustic and duration model, all the input features are normalized to the range of [0.01, 0.99] and output features are normalized to zero mean and unit variance. The DNNs are then trained to map the linguistic features of input text to duration and acoustic features respectively. If D(xi) denotes the DNN mapping of xi, then the error of the mapping is given by:\n= \u2211 ||yi \u2212D(xi)||2 (1)\nD(xi) = d\u0303(zn+1) (2)\nzn+1 = d(w (n)d(zn)) (3)\nd(\u03d1) = a tanh(b\u03d1), d\u0303(\u03d1) = \u03d1 (4)\nwhere n indexes layer and w(n) is the weight matrix of the nth layer of the DNN model. At synthesis time, duration is predicted first, and is used as an input to the acoustic model to predict the speech parameters. Maximum likelihood parameter generation (MLPG) using pre-computed variances from the training data is applied to the output features for synthesis, and spectral enhancement postfiltering is applied to the resulting MCC trajectories. Finally, the STRAIGHT vocoder [25] is used to synthesize the waveform."}, {"heading": "4. Experimental Setup", "text": ""}, {"heading": "4.1. Speech Databases", "text": "Our languages of interest are Hindi, Tamil and Telugu, all of which are widely-spoken Indian languages. We train and test on the 2015 Blizzard Challenge data which contains about four hours of speech and corresponding text for each language. The data-set contains 1710 utterances for Hindi, 1462 utterances for Tamil, and 2481 utterances for Telugu, with a single speaker per language. We used 92% of the data for training, 4% for development and 4% for testing."}, {"heading": "4.2. Annotation", "text": "Starting from the original transcriptions in native script, we asked crowdsourced human annotators to ASCII transliterate them using pronunciation as their main motivation for spelling. For Hindi and Tamil, we recruited paid workers via Mechanical Turk who could read and speak the language fluently (as\nself-reported); for Telugu we had access to a trusted pool of native speakers. We tokenize each sentence to words with whitespace and punctuations as the delimiters. The annotators were provided with a web-interface containing a text box for each word. This ensures transliteration of every word given in the input sentence. The total number of annotators for Telugu, Tamil and Hindi are 50, 66 and 82 respectively. We diversified train, dev and test splits by having different set of annotators for each split."}, {"heading": "4.3. Experimental Settings", "text": "We used the same DNN architectures (Section 3.2) for both duration and acoustic modeling. The number of hidden layers used was 6 with each layer consisting of 1024 nodes. As shown in equation 4, the tanh function was used as the hidden activation function, and a linear activation function was employed at the output layer. During training, L2 regularization was applied to the weights with penalty factor of 0.00001, the mini-batch size was 256 for the acoustic model and 64 for the duration model. For the first 10 epochs, momentum was 0.3 with a fixed learning rate of 0.002. After 10 epochs, the momentum was increased to 0.9 and from that point on, the learning rate was halved at each epoch. The learning rate of the top two layers was always half that of other layers. Learning rate was fine-tuned in duration models to achieve best performance. The maximum number of epochs was set to 30 (i.e., early stopping)."}, {"heading": "4.4. Our Models", "text": "As outlined in Section 3.1, we train three different models for each language. The number of questions used in DNN were different from system to system. For Uni-Grapheme model (labelled as UGM), the questions based on quin-phone identity were used, and other questions include suprasegmental features such as syllable, word, phrase and positional features. For Multi-Grapheme model (labelled as MGM) and Grapheme-toPhoneme model (labelled as G2P), other questions based on position and manner of articulation were additionaly included."}, {"heading": "4.5. Benchmark", "text": "As a benchmark, we use the DNN speech synthesizer trained on CPS phonetic transcriptions of the speech data. The goal is thus to synthesize speech from ASCII text that is as close as possible in quality to this benchmark (labelled as BMK)."}, {"heading": "5. Results", "text": ""}, {"heading": "5.1. Objective Evaluations", "text": ""}, {"heading": "5.1.1. Duration Model", "text": "To evaluate the duration prediction DNN, we calculated the root-mean-square error (RMSE) and Pearson correlation between reference and predicted durations, where the reference durations are estimated from the forced-alignment step in HTS. Tables 4 and 5 present the results on test data."}, {"heading": "G2P 0.818 0.564 0.657", "text": "Overall, the benchmark system showed better performance than other systems in all languages. Among the proposed approaches, G2P performed slightly better than the other two in terms of correlation, whereas RMSE performance was not consistent across the languages. A possible explanation for this is that G2P uses superior phone set defined manually whereas UGM and MGM use unsupervised phones. Nevertheless, the proposed systems are not too far from the benchmark.\nCompared to Telugu, Hindi and Tamil show worse objective scores. For these two languages, punctuation marks were not retained in the corpus, which made pauses harder to predict. As a consequence, occasional pauses in the acoustics were frequently forced-aligned to non-pause phones, introducing errors in the reference durations. These unpredictable elongations inflated the objective measures, without perturbing the actual predictions too much. (Telugu, in contrast, used oracle pauses, inserted using Festvox\u2019s ehmm based on the acoustics.)"}, {"heading": "5.1.2. Acoustic Model", "text": "We used following four objective evaluations to assess the performance of the proposed methods in comparison to the benchmark system.\n\u2022 MCD: Mel-Cepstral Distortion (MCD) to measure MCC prediction performance.\n\u2022 BAP: to measure distortion of BAPs \u2022 F0 RMSE: Root Mean Squared Error (RMSE) to mea-\nsure the accuracy of F0 prediction. The error value was calculated on a linear scale instead of log-scale which was used to model the F0 values.\n\u2022 V/UV: to measure voiced/unvoiced error. In all these metrics, a lower value gives the better performance. While the objective metrics do not map directly to perceptual quality, they are often useful for system tuning. Table 3 presents the results on test data. As expected, the benchmark model performs well on most metrics. While the G2P Model performs well on Telugu and Hindi, the Uni-Grapheme model\ndoes well on Tamil. Overall, the proposed approaches compare favourably with the benchmark."}, {"heading": "5.2. Subjective Evaluations", "text": "Three MUSHRA (MUltiple Stimuli with Hidden Reference and Anchor)5 [26] tests were conducted to assess the naturalness of the synthesized speech. For each language, 16 native listeners were exposed to 20 sentences, chosen randomly from the test set. For each sentence, 5 unlabelled stimuli were presented in parallel: one for each of the four synthesis systems speaking that sentence plus copy-synthesis speech (i.e., vocoded speech, labelled as VOC) used as the hidden reference. Listeners were asked to rate each stimulus from 0 (extremely bad for naturalness) to 100 (same naturalness as the reference speech), and also instructed to give exactly one of the 5 stimuli in every set a rating of 100.\nFor Telugu and Hindi, we had access to a trusted pool of native speakers from IIIT-Hyderabad, while for Tamil we recruited paid workers via Amazon Mechanical Turk as listeners. The Mean Opinion Scores (MOS) from the tests are presented in Figure 2 with their standard deviation represented in log-scale. The benchmark model achieves a higher MOS in Telugu and Hindi, as expected, while in Tamil the Uni-Grapheme model\n5https://github.com/HSU-ANT/beaqlejs\nachieves best performance. However, according to paired t-tests with Holm-Bonferroni correction for multiple comparisons, the difference with next best system is significant only in Telugu and Hindi. Among the proposed approaches, G2P performed significantly better than other two in Telugu and Hindi. However in Tamil, both G2P and benchmark performed worse than the rest. This strange behaviour can be attributed to two reasons: 1) the absence of a mechanism for detecting outliers in turker judgements (as opposed to the use of trusted pool of listeners for Hindi and Telugu); 2) the lack of our expertize in enhancing letter to sound rules specific to Tamil. The difference in ratings suggest that some additional rules or fine-tuning of lexicon may be required for Tamil.\nThe MUSHRA scores combined across all three languages for each system are presented in Fig. 3. For further analysis, each set of fifteen parallel listener scores was converted to ranks from 1 (worst) to 5 (best), with tied ranks set to the mean of the tied position. A box plot of these rank scores aggregated across all sentences and listeners is shown in Figure 4. Listener pref-\nerences between systems are also illustrated in Figure 5. All these figures indicate, G2P performed the best among the proposed approaches.\nAn interesting issue is that some test sentences include English-language words (e.g.: road, page, congress) due to frequent code-switching among the native speakers (also reflected in the text corpus). This affected the performance of G2P conversion for those sentences, in turn creating a marginal difference between G2P and benchmark over the listening test. G2P trained on large corpora of parallel text may remove such errors in the future, thereby improving the synthesis quality and reducing the gap towards the benchmark. [27] is one such recent attempt for synthesizing speech from code-mixed text.\nNo intelligibility evaluation was conducted since transcription word error rate (WER) has been found to be a poor metric for Indian languages, cf. [6]. However, we believe listeners do take into account intelligibility while rating the stimuli, even though they were asked to rate the naturalness."}, {"heading": "6. Applications", "text": "The grapheme-to-phoneme conversion described herein enabled us to build indic-search6, a search engine that helps endusers use ASCII to search for pages written in Unicode. Textto-speech interfaces with ASCII input also enable users to type in their own pronunciation rather than conforming to a specific notation."}, {"heading": "7. Conclusions", "text": "In this paper, we considered the problem of synthesizing speech from ASCII transliterated text of Indian languages. Our proposed approach first converts ASCII text to phonetic script, and then learns a DNN to synthesize speech from the phonetic script. We experimented with three approaches, which vary in the degree of manual supervision in defining phonemes. Our results show that G2P model with few assumptions is competitive with manually-defined phoneme models. All the data, and samples used in the listening tests are available online at: http://srikanthr.in/indic-speech-synthesis.\n6http://srikanthr.in/indic-search\nAcknowledgements: Thanks to Nivedita Chennupati and Spandana Gella for their contribution in data collection with Amazon Mechanical Turk. Also, thanks to Sivanada Achanta for evaluating the systems through listening tests. We thank Gustav Henter for proofreading. However, the errors that remain are the authors\u2019 responsibilities."}, {"heading": "8. References", "text": "[1] A. N. S. Institute, \u201c7-bit american standard code for information\ninterchange,\u201d ANSI X3, vol. 4, 1986.\n[2] U. Z. Ahmed, K. Bali, M. Choudhury, and S. VB, \u201cChallenges in designing input method editors for indian lan-guages: The role of word-origin and context,\u201d in Proceedings of the Workshop on Advances in Text Input Methods (WTIM 2011). Chiang Mai, Thailand: Asian Federation of Natural Language Processing, November 2011, pp. 1\u20139. [Online]. Available: http://www.aclweb.org/anthology/W11-3501\n[3] R. S. Roy, M. Choudhury, P. Majumder, and K. Agarwal, \u201cOverview of the fire 2013 track on transliterated search,\u201d in Proceedings of the 5th 2013 Forum on Information Retrieval Evaluation, ser. FIRE \u201913. New York, NY, USA: ACM, 2007, pp. 4:1\u20134:7. [Online]. Available: http://doi.acm.org/10.1145/ 2701336.2701636\n[4] P. Gupta, K. Bali, R. E. Banchs, M. Choudhury, and P. Rosso, \u201cQuery expansion for mixed-script information retrieval,\u201d in Proceedings of the 37th International ACM SIGIR Conference on Research &#38; Development in Information Retrieval, ser. SIGIR \u201914. New York, NY, USA: ACM, 2014, pp. 677\u2013 686. [Online]. Available: http://doi.acm.org/10.1145/2600428. 2609622\n[5] Y. Vyas, S. Gella, J. Sharma, K. Bali, and M. Choudhury, \u201cPOS tagging of English-Hindi code-mixed social media content,\u201d in Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP). Doha, Qatar: Association for Computational Linguistics, October 2014, pp. 974\u2013979. [Online]. Available: http://www.aclweb.org/anthology/ D14-1105\n[6] K. Prahallad, A. Vadapalli, S. Kesiraju, H. Murthy, S. Lata, T. Nagarajan, M. Prasanna, H. Patil, A. Sao, S. King et al., \u201cThe blizzard challenge 2014,\u201d in Proc. Blizzard Challenge workshop, 2014.\n[7] K. Prahallad, A. Vadapalli, N. Elluru, G. Mantena, B. Pulugundla, P. Bhaskararao, H. Murthy, S. King, V. Karaiskos, and A. Black, \u201cThe blizzard challenge 2013\u2013indian language task,\u201d in Proc. Blizzard Challenge Workshop, 2013.\n[8] R. B, S. L. Christina, G. A. Rachel, S. Solomi V, M. K. Nandwana, A. Prakash, A. S. S, R. Krishnan, S. K. Prahalad, K. Samudravijaya, P. Vijayalakshmi, T. Nagarajan, and H. Murthy, \u201cA common attribute based unified hts framework for speech synthesis in indian languages,\u201d in Proc. SSW, Barcelona, Spain, August 2013, pp. 311\u2013316.\n[9] P. Lavanya, P. Kishore, and G. T. Madhavi, \u201cA simple approach for building transliteration editors for indian languages,\u201d Journal of Zhejiang University Science A, vol. 6, no. 11, pp. 1354\u20131361, 2005.\n[10] G. Madhavi, B. Mini, N. Balakrishnan, and R. Raj, \u201cOm: One tool for many (indian) languages,\u201d Journal of Zhejiang University Science A, vol. 6, no. 11, pp. 1348\u20131353, 2005.\n[11] R. Gupta, P. Goyal, and S. Diwakar, \u201cTransliteration among indian languages using wx notation.\u201d in Proc. of KONVENS, 2010, pp. 147\u2013150.\n[12] H. Li, A. Kumaran, V. Pervouchine, and M. Zhang, \u201cReport of news 2009 machine transliteration shared task,\u201d in Proceedings of the 2009 Named Entities Workshop: Shared Task on Transliteration, ser. NEWS \u201909. Stroudsburg, PA, USA: Association for Computational Linguistics, 2009, pp. 1\u201318. [Online]. Available: http://dl.acm.org/citation.cfm?id=1699705. 1699707\n[13] E. V. Raghavendra, S. Desai, B. Yegnanarayana, A. W. Black, and K. Prahallad, \u201cGlobal syllable set for building speech synthesis in indian languages,\u201d in Proc. IEEE Spoken Language Technology workshop, 2008, pp. 49\u201352.\n[14] S. Kishore, R. Kumar, and R. Sangal, \u201cA data driven synthesis approach for indian languages using syllable as basic unit,\u201d in Proceedings of Intl. Conf. on NLP (ICON), 2002, pp. 311\u2013316.\n[15] H. Patil, T. Patel, N. Shah, H. Sailor, R. Krishnan, G. Kasthuri, T. Nagarajan, L. Christina, N. Kumar, V. Raghavendra, S. Kishore, S. Prasanna, N. Adiga, S. Singh, K. Anand, P. Kumar, B. Singh, S. Binil Kumar, T. Bhadran, T. Sajini, A. Saha, T. Basu, K. Rao, N. Narendra, A. Sao, R. Kumar, P. Talukdar, P. Acharyaa, S. Chandra, S. Lata, and H. Murthy, \u201cA syllablebased framework for unit selection synthesis in 13 indian languages,\u201d in Oriental COCOSDA held jointly with 2013 Conference on Asian Spoken Language Research and Evaluation (OCOCOSDA/CASLRE), 2013 International Conference, Nov 2013, pp. 1\u20138.\n[16] W. Oliver, \u201cUnsupervised learning for text-to-speech synthesis,\u201d Ph.D. dissertation, University of Edinburgh, 2012.\n[17] S. Sitaram, S. Palkar, Y. Chen, A. Parlikar, and A. W. Black, \u201cBootstrapping text-to-speech for speech processing in languages without an orthography,\u201d in Proc. ICASSP, 2013, pp. 7992\u20137996.\n[18] O. Watts, S. Ronanki, Z. Wu, T. Raitio, and A. Suni, \u201cThe NST\u2013 GlottHMM entry to the Blizzard Challenge 2015,\u201d in Proc. Blizzard Challenge Workshop, 2015.\n[19] G. Hinton, L. Deng, D. Yu, G. Dahl, A. Mohamed, N. Jaitly, A. Senior, V. Vanhoucke, P. Nguyen, T. Sainath, and B. Kingsbury, \u201cDeep neural networks for acoustic modeling in speech recognition: The shared views of four research groups,\u201d Signal Processing Magazine, IEEE, vol. 29, no. 6, pp. 82\u201397, Nov 2012.\n[20] H. Zen, A. Senior, and M. Schuster, \u201cStatistical parametric speech synthesis using deep neural networks,\u201d in Proc. ICASSP, 2013, pp. 7962\u20137966.\n[21] Z. Wu, C. Valentini-Botinhao, O. Watts, and S. King, \u201cDeep neural networks employing multi-task learning and stacked bottleneck features for speech synthesis,\u201d in Proc. ICASSP, 2015.\n[22] H. Zen, K. Tokuda, and A. W. Black, \u201cStatistical parametric speech synthesis,\u201d Speech Commun., vol. 51, no. 11, pp. 1039\u2013 1064, 2009.\n[23] A. A. Raj, T. Sarkar, S. C. Pammi, S. Yuvaraj, M. Bansal, K. Prahallad, and A. W. Black, \u201cText processing for text-to-speech systems in indian languages.\u201d in Proc. SSW, 2007, pp. 188\u2013193.\n[24] M. Bisani and H. Ney, \u201cJoint-sequence models for grapheme-tophoneme conversion,\u201d Speech Commun., vol. 50, no. 5, pp. 434 \u2013 451, 2008. [Online]. Available: http://www.sciencedirect.com/ science/article/pii/S0167639308000046\n[25] H. Kawahara, M. Morise, T. Takahashi, R. Nisimura, T. Irino, and H. Banno, \u201cTandem-straight: A temporally stable power spectral representation for periodic signals and applications to interference-free spectrum, f0, and aperiodicity estimation,\u201d in Proc. ICASSP, March 2008, pp. 3933\u20133936.\n[26] S. Kraft and U. Zlzer, BeaqleJS: HTML5 and JavaScript based Framework for the Subjective Evaluation of Audio Quality, Linux Audio Conference, Karlsruhe, Germany, 2014.\n[27] S. Sitaram and A. W. Black, \u201cSpeech Synthesis of Code Mixed Text,\u201d in Proc. LREC, 2016, pp. 3422\u20133428."}], "references": [{"title": "7-bit american standard code for information interchange", "author": ["A.N.S. Institute"], "venue": "ANSI X3, vol. 4, 1986.", "citeRegEx": "1", "shortCiteRegEx": null, "year": 1986}, {"title": "Challenges in designing input method editors for indian lan-guages: The role of word-origin and context", "author": ["U.Z. Ahmed", "K. Bali", "M. Choudhury", "S. VB"], "venue": "Proceedings of the Workshop on Advances in Text Input Methods (WTIM 2011). Chiang Mai, Thailand: Asian Federation of Natural Language Processing, November 2011, pp. 1\u20139. [Online]. Available: http://www.aclweb.org/anthology/W11-3501", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2011}, {"title": "Overview of the fire 2013 track on transliterated search", "author": ["R.S. Roy", "M. Choudhury", "P. Majumder", "K. Agarwal"], "venue": "Proceedings of the 5th 2013 Forum on Information Retrieval Evaluation, ser. FIRE \u201913. New York, NY, USA: ACM, 2007, pp. 4:1\u20134:7. [Online]. Available: http://doi.acm.org/10.1145/ 2701336.2701636", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2013}, {"title": "Query expansion for mixed-script information retrieval", "author": ["P. Gupta", "K. Bali", "R.E. Banchs", "M. Choudhury", "P. Rosso"], "venue": "Proceedings of the 37th International ACM SIGIR Conference on Research &#38; Development in Information Retrieval, ser. SIGIR \u201914. New York, NY, USA: ACM, 2014, pp. 677\u2013 686. [Online]. Available: http://doi.acm.org/10.1145/2600428. 2609622", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2014}, {"title": "POS tagging of English-Hindi code-mixed social media content", "author": ["Y. Vyas", "S. Gella", "J. Sharma", "K. Bali", "M. Choudhury"], "venue": "Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP). Doha, Qatar: Association for Computational Linguistics, October 2014, pp. 974\u2013979. [Online]. Available: http://www.aclweb.org/anthology/ D14-1105", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2014}, {"title": "The blizzard challenge 2014", "author": ["K. Prahallad", "A. Vadapalli", "S. Kesiraju", "H. Murthy", "S. Lata", "T. Nagarajan", "M. Prasanna", "H. Patil", "A. Sao", "S. King"], "venue": "Proc. Blizzard Challenge workshop, 2014.", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2014}, {"title": "The blizzard challenge 2013\u2013indian language task", "author": ["K. Prahallad", "A. Vadapalli", "N. Elluru", "G. Mantena", "B. Pulugundla", "P. Bhaskararao", "H. Murthy", "S. King", "V. Karaiskos", "A. Black"], "venue": "Proc. Blizzard Challenge Workshop, 2013.", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2013}, {"title": "A common attribute based unified hts framework for speech synthesis in indian languages", "author": ["R. B", "S.L. Christina", "G.A. Rachel", "S. Solomi V", "M.K. Nandwana", "A. Prakash", "A.S. S", "R. Krishnan", "S.K. Prahalad", "K. Samudravijaya", "P. Vijayalakshmi", "T. Nagarajan", "H. Murthy"], "venue": "Proc. SSW, Barcelona, Spain, August 2013, pp. 311\u2013316.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2013}, {"title": "A simple approach for building transliteration editors for indian languages", "author": ["P. Lavanya", "P. Kishore", "G.T. Madhavi"], "venue": "Journal of Zhejiang University Science A, vol. 6, no. 11, pp. 1354\u20131361, 2005.", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2005}, {"title": "Om: One tool for many (indian) languages", "author": ["G. Madhavi", "B. Mini", "N. Balakrishnan", "R. Raj"], "venue": "Journal of Zhejiang University Science A, vol. 6, no. 11, pp. 1348\u20131353, 2005.", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2005}, {"title": "Transliteration among indian languages using wx notation.", "author": ["R. Gupta", "P. Goyal", "S. Diwakar"], "venue": "in Proc. of KONVENS,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2010}, {"title": "Report of news 2009 machine transliteration shared task", "author": ["H. Li", "A. Kumaran", "V. Pervouchine", "M. Zhang"], "venue": "Proceedings of the 2009 Named Entities Workshop: Shared Task on Transliteration, ser. NEWS \u201909. Stroudsburg, PA, USA: Association for Computational Linguistics, 2009, pp. 1\u201318. [Online]. Available: http://dl.acm.org/citation.cfm?id=1699705. 1699707", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2009}, {"title": "Global syllable set for building speech synthesis in indian languages", "author": ["E.V. Raghavendra", "S. Desai", "B. Yegnanarayana", "A.W. Black", "K. Prahallad"], "venue": "Proc. IEEE Spoken Language Technology workshop, 2008, pp. 49\u201352.", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2008}, {"title": "A data driven synthesis approach for indian languages using syllable as basic unit", "author": ["S. Kishore", "R. Kumar", "R. Sangal"], "venue": "Proceedings of Intl. Conf. on NLP (ICON), 2002, pp. 311\u2013316.", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2002}, {"title": "A syllablebased framework for unit selection synthesis in 13 indian languages", "author": ["H. Patil", "T. Patel", "N. Shah", "H. Sailor", "R. Krishnan", "G. Kasthuri", "T. Nagarajan", "L. Christina", "N. Kumar", "V. Raghavendra", "S. Kishore", "S. Prasanna", "N. Adiga", "S. Singh", "K. Anand", "P. Kumar", "B. Singh", "S. Binil Kumar", "T. Bhadran", "T. Sajini", "A. Saha", "T. Basu", "K. Rao", "N. Narendra", "A. Sao", "R. Kumar", "P. Talukdar", "P. Acharyaa", "S. Chandra", "S. Lata", "H. Murthy"], "venue": "Oriental COCOSDA held jointly with 2013 Conference on Asian Spoken Language Research and Evaluation (O- COCOSDA/CASLRE), 2013 International Conference, Nov 2013, pp. 1\u20138.", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2013}, {"title": "Unsupervised learning for text-to-speech synthesis", "author": ["W. Oliver"], "venue": "Ph.D. dissertation, University of Edinburgh, 2012.", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2012}, {"title": "Bootstrapping text-to-speech for speech processing in languages without an orthography", "author": ["S. Sitaram", "S. Palkar", "Y. Chen", "A. Parlikar", "A.W. Black"], "venue": "Proc. ICASSP, 2013, pp. 7992\u20137996.", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2013}, {"title": "The NST\u2013 GlottHMM entry to the Blizzard Challenge 2015", "author": ["O. Watts", "S. Ronanki", "Z. Wu", "T. Raitio", "A. Suni"], "venue": "Proc. Blizzard Challenge Workshop, 2015.", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2015}, {"title": "Deep neural networks for acoustic modeling in speech recognition: The shared views of four research groups", "author": ["G. Hinton", "L. Deng", "D. Yu", "G. Dahl", "A. Mohamed", "N. Jaitly", "A. Senior", "V. Vanhoucke", "P. Nguyen", "T. Sainath", "B. Kingsbury"], "venue": "Signal Processing Magazine, IEEE, vol. 29, no. 6, pp. 82\u201397, Nov 2012.", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2012}, {"title": "Statistical parametric speech synthesis using deep neural networks", "author": ["H. Zen", "A. Senior", "M. Schuster"], "venue": "Proc. ICASSP, 2013, pp. 7962\u20137966.", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2013}, {"title": "Deep neural networks employing multi-task learning and stacked bottleneck features for speech synthesis", "author": ["Z. Wu", "C. Valentini-Botinhao", "O. Watts", "S. King"], "venue": "Proc. ICASSP, 2015.", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2015}, {"title": "Statistical parametric speech synthesis", "author": ["H. Zen", "K. Tokuda", "A.W. Black"], "venue": "Speech Commun., vol. 51, no. 11, pp. 1039\u2013 1064, 2009.", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2009}, {"title": "Text processing for text-to-speech systems in indian languages.", "author": ["A.A. Raj", "T. Sarkar", "S.C. Pammi", "S. Yuvaraj", "M. Bansal", "K. Prahallad", "A.W. Black"], "venue": "in Proc. SSW,", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2007}, {"title": "Joint-sequence models for grapheme-tophoneme conversion", "author": ["M. Bisani", "H. Ney"], "venue": "Speech Commun., vol. 50, no. 5, pp. 434 \u2013 451, 2008. [Online]. Available: http://www.sciencedirect.com/ science/article/pii/S0167639308000046", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2008}, {"title": "Tandem-straight: A temporally stable power spectral representation for periodic signals and applications to interference-free spectrum, f0, and aperiodicity estimation", "author": ["H. Kawahara", "M. Morise", "T. Takahashi", "R. Nisimura", "T. Irino", "H. Banno"], "venue": "Proc. ICASSP, March 2008, pp. 3933\u20133936.", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2008}, {"title": "Zlzer, BeaqleJS: HTML5 and JavaScript based Framework for the Subjective Evaluation of Audio Quality, Linux Audio", "author": ["U.S. Kraft"], "venue": null, "citeRegEx": "26", "shortCiteRegEx": "26", "year": 2014}, {"title": "Speech Synthesis of Code Mixed Text", "author": ["S. Sitaram", "A.W. Black"], "venue": "Proc. LREC, 2016, pp. 3422\u20133428.", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2016}], "referenceMentions": [{"referenceID": 0, "context": "Though a large number of Indian languages have indigenous scripts, the lack of a standardized keyboard, and the ubiquity of QWERTY keyboards, means that people most often write using ASCII [1] text using spellings motivated largely by pronunciation [2].", "startOffset": 189, "endOffset": 192}, {"referenceID": 1, "context": "Though a large number of Indian languages have indigenous scripts, the lack of a standardized keyboard, and the ubiquity of QWERTY keyboards, means that people most often write using ASCII [1] text using spellings motivated largely by pronunciation [2].", "startOffset": 249, "endOffset": 252}, {"referenceID": 2, "context": "Increasingly, many technologies such as Web search and natural language processing are adapting to this phenomenon [3, 4, 5].", "startOffset": 115, "endOffset": 124}, {"referenceID": 3, "context": "Increasingly, many technologies such as Web search and natural language processing are adapting to this phenomenon [3, 4, 5].", "startOffset": 115, "endOffset": 124}, {"referenceID": 4, "context": "Increasingly, many technologies such as Web search and natural language processing are adapting to this phenomenon [3, 4, 5].", "startOffset": 115, "endOffset": 124}, {"referenceID": 5, "context": "In the area of Speech Synthesis, although the efforts of the 2013, 2014 and 2015 Blizzard Challenges [6, 7] resulted in improvements to the naturalness of speech synthesis of Indian languages, the text was assumed to be written in native script.", "startOffset": 101, "endOffset": 107}, {"referenceID": 6, "context": "In the area of Speech Synthesis, although the efforts of the 2013, 2014 and 2015 Blizzard Challenges [6, 7] resulted in improvements to the naturalness of speech synthesis of Indian languages, the text was assumed to be written in native script.", "startOffset": 101, "endOffset": 107}, {"referenceID": 7, "context": "Among these, CPS (Common Phone Set) [8] and IT3 [9] are widely used by the speech technology community, ITRANS [10] is used in publishing houses, and WX [11] by the Natural Language Processing (NLP) community.", "startOffset": 36, "endOffset": 39}, {"referenceID": 8, "context": "Among these, CPS (Common Phone Set) [8] and IT3 [9] are widely used by the speech technology community, ITRANS [10] is used in publishing houses, and WX [11] by the Natural Language Processing (NLP) community.", "startOffset": 48, "endOffset": 51}, {"referenceID": 9, "context": "Among these, CPS (Common Phone Set) [8] and IT3 [9] are widely used by the speech technology community, ITRANS [10] is used in publishing houses, and WX [11] by the Natural Language Processing (NLP) community.", "startOffset": 111, "endOffset": 115}, {"referenceID": 10, "context": "Among these, CPS (Common Phone Set) [8] and IT3 [9] are widely used by the speech technology community, ITRANS [10] is used in publishing houses, and WX [11] by the Natural Language Processing (NLP) community.", "startOffset": 153, "endOffset": 157}, {"referenceID": 11, "context": "The most common trend observed in the literature is to treat transliteration as a machine translation and discriminative ranking problem [12].", "startOffset": 137, "endOffset": 141}, {"referenceID": 12, "context": "Most existing work in speech synthesis for Indian languages uses unit selection [13] with syllable-like units [14, 15].", "startOffset": 80, "endOffset": 84}, {"referenceID": 13, "context": "Most existing work in speech synthesis for Indian languages uses unit selection [13] with syllable-like units [14, 15].", "startOffset": 110, "endOffset": 118}, {"referenceID": 14, "context": "Most existing work in speech synthesis for Indian languages uses unit selection [13] with syllable-like units [14, 15].", "startOffset": 110, "endOffset": 118}, {"referenceID": 7, "context": "cently, based on the observation that Indian languages share many commonalities in phonetics, a language independent phone set was proposed, and was used in building statistical parametric (HMM-based) speech synthesis systems [8].", "startOffset": 226, "endOffset": 229}, {"referenceID": 15, "context": "Our work also aligns with the recent literature on unsupervised learning for text-to-speech synthesis which aims to reduce the reliance on human knowledge and the manual effort required for building language-specific resources [16, 17, 18].", "startOffset": 227, "endOffset": 239}, {"referenceID": 16, "context": "Our work also aligns with the recent literature on unsupervised learning for text-to-speech synthesis which aims to reduce the reliance on human knowledge and the manual effort required for building language-specific resources [16, 17, 18].", "startOffset": 227, "endOffset": 239}, {"referenceID": 17, "context": "Our work also aligns with the recent literature on unsupervised learning for text-to-speech synthesis which aims to reduce the reliance on human knowledge and the manual effort required for building language-specific resources [16, 17, 18].", "startOffset": 227, "endOffset": 239}, {"referenceID": 18, "context": "Following the success of DNNs for speech recognition [19] and synthesis [20, 21, 22], we also use a DNN as the acoustic model.", "startOffset": 53, "endOffset": 57}, {"referenceID": 19, "context": "Following the success of DNNs for speech recognition [19] and synthesis [20, 21, 22], we also use a DNN as the acoustic model.", "startOffset": 72, "endOffset": 84}, {"referenceID": 20, "context": "Following the success of DNNs for speech recognition [19] and synthesis [20, 21, 22], we also use a DNN as the acoustic model.", "startOffset": 72, "endOffset": 84}, {"referenceID": 21, "context": "Following the success of DNNs for speech recognition [19] and synthesis [20, 21, 22], we also use a DNN as the acoustic model.", "startOffset": 72, "endOffset": 84}, {"referenceID": 7, "context": "We use the common phone set (CPS, [8]) to work with the languages of interest.", "startOffset": 34, "endOffset": 37}, {"referenceID": 8, "context": "We convert the native text to CPS phonetic text using deterministic converters [9, 23].", "startOffset": 79, "endOffset": 86}, {"referenceID": 22, "context": "We convert the native text to CPS phonetic text using deterministic converters [9, 23].", "startOffset": 79, "endOffset": 86}, {"referenceID": 23, "context": "Given the pronunciation lexicon, we train a G2P transducer [24] for each language separately with varying n-gram sequences.", "startOffset": 59, "endOffset": 63}, {"referenceID": 19, "context": "Similar to [20, 21], frame-aligned data for DNN training is created by forced alignment using the HMM system.", "startOffset": 11, "endOffset": 19}, {"referenceID": 20, "context": "Similar to [20, 21], frame-aligned data for DNN training is created by forced alignment using the HMM system.", "startOffset": 11, "endOffset": 19}, {"referenceID": 24, "context": "Finally, the STRAIGHT vocoder [25] is used to synthesize the waveform.", "startOffset": 30, "endOffset": 34}, {"referenceID": 25, "context": "Three MUSHRA (MUltiple Stimuli with Hidden Reference and Anchor) [26] tests were conducted to assess the naturalness of the synthesized speech.", "startOffset": 65, "endOffset": 69}, {"referenceID": 26, "context": "[27] is one such recent attempt for synthesizing speech from code-mixed text.", "startOffset": 0, "endOffset": 4}, {"referenceID": 5, "context": "[6].", "startOffset": 0, "endOffset": 3}], "year": 2016, "abstractText": "Text-to-Speech synthesis in Indian languages has a seen lot of progress over the decade partly due to the annual Blizzard challenges. These systems assume the text to be written in Devanagari or Dravidian scripts which are nearly phonemic orthography scripts. However, the most common form of computer interaction among Indians is ASCII written transliterated text. Such text is generally noisy with many variations in spelling for the same word. In this paper we evaluate three approaches to synthesize speech from such noisy ASCII text: a naive UniGrapheme approach, a Multi-Grapheme approach, and a supervised Grapheme-to-Phoneme (G2P) approach. These methods first convert the ASCII text to a phonetic script, and then learn a Deep Neural Network to synthesize speech from that. We train and test our models on Blizzard Challenge datasets that were transliterated to ASCII using crowdsourcing. Our experiments on Hindi, Tamil and Telugu demonstrate that our models generate speech of competetive quality from ASCII text compared to the speech synthesized from the native scripts. All the accompanying transliterated datasets are released for public access.", "creator": "LaTeX with hyperref package"}}}