{"id": "1608.06154", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "22-Aug-2016", "title": "Multi-Sensor Prognostics using an Unsupervised Health Index based on LSTM Encoder-Decoder", "abstract": "Many approaches for estimation of Remaining Useful Life (RUL) of a machine, using its operational sensor data, make assumptions about how a system degrades or a fault evolves, e.g., exponential degradation of a function, or changes in the behavior of an automatic system.\n\n\n\nThe results of a systematic study that identified the most important factor that caused most of the problems identified in our research, were found in a number of computer-generated statistics, including the fraction of the real-world occurrences of \"normal\" or \"normal\" errors.\nTo be fair, the results of a systematic research program that assessed several major factors associated with machine-learning, showed that the greatest effect of a process such as the number of real-world \"experts\" was overstated.\nWe did not know why the most important factor was not the amount of data used in our research. It was a fact that our current research methods were inadequate. The fact that a lot of data are already available for use in a machine learning system is a very telling point in our study, but it is clear that there is an increasing amount of information and that there is a tremendous amount of information that can be put into this system. We now know how to determine which areas of the human brain have received the most valuable information from our current study programs.\nWe did not want to be too technical in any way. We wanted to be as thorough as possible and share some of the information in this paper. Our paper presents the following general guidelines for evaluating the best use of machine-learning methods:\nThe primary objective is to understand what happens when the machine learns more about the data and what it is doing to the machine to make decisions and, in particular, which areas of the human brain do not benefit from.\nThis is a very important question. Why does a machine learning machine learn this data more in a machine learning system?\nThe main reason is because we have limited understanding of the data in the literature. What we do know is that the average human is able to easily see the differences in data used in a machine learning program or other machine learning methods. In the literature we have no such information. We use information that is important to the machine learning system. There is a great deal of evidence in this literature for this problem. For example, the American Academy of Pediatrics (AAP) has released some articles that support machine learning methods to better understand how humans perform their work. However, the American Academy of Pediatrics (AAP)", "histories": [["v1", "Mon, 22 Aug 2016 12:59:31 GMT  (1237kb,D)", "http://arxiv.org/abs/1608.06154v1", "Presented at 1st ACM SIGKDD Workshop on Machine Learning for Prognostics and Health Management, San Francisco, CA, USA, 2016. 10 pages"]], "COMMENTS": "Presented at 1st ACM SIGKDD Workshop on Machine Learning for Prognostics and Health Management, San Francisco, CA, USA, 2016. 10 pages", "reviews": [], "SUBJECTS": "cs.LG cs.AI", "authors": ["pankaj malhotra", "vishnu tv", "anusha ramakrishnan", "gaurangi anand", "lovekesh vig", "puneet agarwal", "gautam shroff"], "accepted": false, "id": "1608.06154"}, "pdf": {"name": "1608.06154.pdf", "metadata": {"source": "CRF", "title": "Multi-Sensor Prognostics using an Unsupervised Health Index based on LSTM Encoder-Decoder", "authors": ["Pankaj Malhotra", "Anusha Ramakrishnan", "Gaurangi Anand", "Lovekesh Vig", "Puneet Agarwal", "Gautam Shroff"], "emails": ["anusha.ramakrishnan}@tcs.com", "gautam.shroff}@tcs.com"], "sections": [{"heading": "1. INTRODUCTION", "text": "Industrial Internet has given rise to availability of sensor data from numerous machines belonging to various domains such as agriculture, energy, manufacturing etc. These sensor readings can indicate health of the machines. This has led to increased business desire to perform maintenance of these machines based on their condition rather than following the current industry practice of time-based maintenance. It has also been shown that condition-based maintenance can lead to significant financial savings. Such goals can be achieved by building models for prediction of remaining useful life (RUL) of the machines, based on their sensor readings.\nTraditional approach for RUL prediction is based on an assumption that the health degradation curves (drawn w.r.t. time) follow specific shape such as exponential or linear. Under this assumption we can build a model for health index (HI) prediction, as a function of sensor readings. Extrapolation of HI is used for prediction of RUL [29, 24, 25]. However, we observed that such assumptions do not hold in the real-world datasets, making the problem harder to solve. Some of the important challenges in solving the prognostics problem are: i) health degradation curve may not necessarily follow a fixed shape, ii) time to reach same level of degradation by machines of same specifications is often different, iii) each instance has a slightly different initial health or wear, iv) sensor readings if available are\nPresented at 1st ACM SIGKDD Workshop on Machine Learning for Prognostics and Health Management, San Francisco, CA, USA, 2016. Copyright 2016 Tata Consultancy Services Ltd.\nnoisy, v) sensor data till end-of-life is not easily available because in practice periodic maintenance is performed.\nApart from the health index (HI) based approach as described above, mathematical models of the underlying physical system, fault propagation models and conventional reliability models have also been used for RUL estimation [5, 26]. Data-driven models which use readings of sensors carrying degradation or wear information such as vibration in a bearing have been effectively used to build RUL estimation models [28, 29, 37]. Typically, sensor readings over the entire operational life of multiple instances of a system from start till failure are used to obtain common degradation behavior trends or to build models of how a system degrades by estimating health in terms of HI. Any new instance is then compared with these trends and the most similar trends are used to estimate the RUL [40].\nLSTM networks are recurrent neural network models that have been successfully used for many sequence learning and temporal modeling tasks [12, 2] such as handwriting recognition, speech recognition, sentiment analysis, and customer behavior prediction. A variant of LSTM networks, LSTM encoder-decoder (LSTM-ED) model has been successfully used for sequence-to-sequence learning tasks [8, 34, 4] like machine translation, natural language generation and reconstruction, parsing, and image captioning. LSTM-ED works as follows: An LSTM-based encoder is used to map a multivariate input sequence to a fixed-dimensional vector representation. The decoder is another LSTM network which uses this vector representation to produce the target sequence. We provide further details on LSTM-ED in Sections 4.1 and 4.2.\nLSTM Encoder-decoder based approaches have been proposed for anomaly detection [21, 23]. These approaches learn a model to reconstruct the normal data (e.g. when machine is in perfect health) such that the learned model could reconstruct the subsequences which belong to normal behavior. The learnt model leads to high reconstruction error for anomalous or novel subsequences, since it has not seen such data during training. Based on similar ideas, we use Long Short-Term Memory [14] Encoder-Decoder (LSTM-ED) for RUL estimation. In this paper, we propose an unsupervised technique to obtain a health index (HI) for a system using multi-sensor time-series data, which does not make any assumption on the shape of the degradation curve. We use LSTM-ED to learn a model of normal behavior of a system, which is trained to reconstruct multivariate time-series corresponding to normal behavior. The reconstruction error at a point in a time-series is used ar X\niv :1\n60 8.\n06 15\n4v 1\n[ cs\n.L G\n] 2\n2 A\nug 2\n01 6\nto compute HI at that point. In this paper, we show that:\n\u2022 LSTM-ED based HI learnt in an unsupervised manner is able to capture the degradation in a system: the HI decreases as the system degrades.\n\u2022 LSTM-ED based HI can be used to learn a model for RUL estimation instead of relying on domain knowledge, or exponential/linear degradation assumption, while achieving comparable performance.\nThe rest of the paper is organized as follows: We formally introduce the problem and provide an overview of our approach in Section 2. In Section 3, we describe Linear Regression (LR) based approach to estimate the health index and discuss commonly used assumptions to obtain these estimates. In Section 4, we describe how LSTM-ED can be used to learn the LR model without relying on domain knowledge or mathematical models for degradation evolution. In Section 5, we explain how we use the HI curves of train instances and a new test instance to estimate the RUL of the new instance. We provide details of experiments and results on three datasets in Section 6. Finally, we conclude with a discussion in Section 8, after a summary of related work in Section 7."}, {"heading": "2. APPROACH OVERVIEW", "text": "We consider the scenario where historical instances of a system with multi-sensor data readings till end-of-life are available. The goal is to estimate the RUL of a currently operational instance of the system for which multi-sensor data is available till current time-instance.\nMore formally, we consider a set of train instances U of a system. For each instance u \u2208 U , we consider a multivariate time-series of sensor readings X(u) = [x\n(u) 1 x (u) 2 ... x (u)\nL(u) ]\nwith L(u) cycles where the last cycle corresponds to the end-of-life, each point x (u) t \u2208 Rm is an m-dimensional vector corresponding to readings for m sensors at time-instance t. The sensor data is z-normalized such that the sensor reading x (u) tj at time t for jth sensor for instance u is transformed to x (u) tj \u2212\u00b5j \u03c3j , where \u00b5j and \u03c3j are mean and standard deviation for the jth sensor\u2019s readings over all cycles from all instances in U . (Note: When multiple modes of normal operation exist, each point can normalized based on the \u00b5j and \u03c3j for that mode of operation, as suggested in [39].) A subsequence of length l for time-series X(u) starting from time instance t is denoted by X(u)(t, l) = [x (u) t x (u) t+1 ... x (u) t+l\u22121] with 1 \u2264 t \u2264 L(u) \u2212 l + 1. In many real-world multi-sensor data, sensors are correlated. As in [39, 24, 25], we use Principal Components Analysis to obtain derived sensors from the normalized sensor data with reduced linear correlations between them. The multivariate time-series for the derived sensors is represented as Z(u) = [z\n(u) 1 z (u) 2 ... z (u)\nL(u) ], where z\n(u) t \u2208 Rp, p\nis the number of principal components considered (The best value of p can be obtained using a validation set).\nFor a new instance u\u2217 with sensor readings X(u \u2217) over L(u \u2217) cycles, the goal is to estimate the remaining useful life R(u \u2217) in terms of number of cycles, given the time-series data for train instances {X(u) : u \u2208 U}. We describe our approach assuming same operating regime for the entire life of the system. The approach can be easily extended to\nmultiple operating regimes scenario by treating data for each regime separately (similar to [39, 17]), as described in one of our case studies on milling machine dataset in Section 6.3."}, {"heading": "3. LINEAR REGRESSION BASED", "text": "HEALTH INDEX ESTIMATION\nLet H(u) = [h (u) 1 h (u) 2 ... h (u) L(u) ] represent the HI curve H(u) for instance u, where each point h (u) t \u2208 R, L(u) is the total number of cycles. We assume 0 \u2264 h(u)t \u2264 1, s.t. when u is in perfect health h\n(u) t = 1, and when u performs below an\nacceptable level (e.g. instance is about to fail), h (u) t = 0.\nOur goal is to construct a mapping f\u03b8 : z (u) t \u2192 h (u) t s.t.\nf\u03b8(z (u) t ) = \u03b8 T z (u) t + \u03b80 (1)\nwhere \u03b8 \u2208 Rp, \u03b80 \u2208 R, which computes HI h(u)t from the derived sensor readings z\n(u) t at time t for instance u.\nGiven the target HI curves for the training instances, the parameters \u03b8 and \u03b80 are estimated using Ordinary Least Squares method."}, {"heading": "3.1 Domain-specific target HI curves", "text": "The parameters \u03b8 and \u03b80 of the above mentioned Linear Regression (LR) model (Eq. 1) are usually estimated by assuming a mathematical form for the target H(u), with an exponential function being the most common and successfully employed target HI curve (e.g. [10, 33, 40, 29, 6]), which assumes the HI at time t for instance u as\nh (u) t = 1\u2212exp\n( log(\u03b2).(L(u) \u2212 t)\n(1\u2212 \u03b2).L(u)\n) , t \u2208 [\u03b2.L(u), (1\u2212\u03b2).L(u)].\n(2) 0 < \u03b2 < 1. The starting and ending \u03b2 fraction of cycles are assigned values of 1 and 0, respectively.\nAnother possible assumption is: assume target HI values of 1 and 0 for data corresponding to healthy condition and failure conditions, respectively. Unlike the exponential HI curve which uses the entire time-series of sensor readings, the sensor readings corresponding to only these points are used to learn the regression model (e.g. [38]).\nThe estimates \u03b8\u0302 and \u03b8\u03020 based on target HI curves for train instances are used to obtain the final HI curves H(u) for all the train instances and a new test instance for which RUL is to be estimated. The HI curves thus obtained are used to estimate the RUL for the test instance based on similarity of train and test HI curves, as described later in Section 5."}, {"heading": "4. LSTM-ED BASED TARGET HI CURVE", "text": "We learn an LSTM-ED model to reconstruct the time-series of the train instances during normal operation. For example, any subsequence corresponding to starting few cycles when the system can be assumed to be in healthy state can be used to learn the model. The reconstruction model is then used to reconstruct all the subsequences for all train instances and the pointwise reconstruction error is used to obtain a target HI curve for each instance. We briefly describe LSTM unit and LSTM-ED based reconstruction model, and then explain how the reconstruction errors obtained from this model are used to obtain the target HI curves."}, {"heading": "4.1 LSTM unit", "text": "An LSTM unit is a recurrent unit that uses the input zt, the hidden state activation at\u22121, and memory cell activation ct\u22121 to compute the hidden state activation at at time t. It uses a combination of a memory cell c and three types of gates: input gate i, forget gate f , and output gate o to decide if the input needs to be remembered (using input gate), when the previous memory needs to be retained (forget gate), and when the memory content needs to be output (using output gate).\nMany variants and extensions to the original LSTM unit as introduced in [14] exist. We use the one as described in [42]. Consider Tn1,n2 : R\nn1 \u2192 Rn2 is an affine transform of the form z 7\u2192 Wz + b for matrix W and vector b of appropriate dimensions. The values for input gate i, forget gate f , output gate o, hidden state a, and cell activation c at time t are computed using the current input zt, the previous hidden state at\u22121, and memory cell value ct\u22121 as given by Eqs. 3-5.\n it ft\not\ngt\n =  \u03c3 \u03c3 \u03c3\ntanh\nTm+n,4n ( zt\nat\u22121\n) (3)\nHere \u03c3(z) = 1 1+e\u2212z and tanh(z) = 2\u03c3(2z) \u2212 1. The operations \u03c3 and tanh are applied elementwise. The four equations from the above simplifed matrix notation read as: it = \u03c3(W1zt + W2at\u22121 + bi), etc. Here, xt \u2208 Rm, and all others it, ft, ot, gt, at, ct \u2208 Rn.\nct = ftct\u22121 + itgt (4)\nat = ottanh(ct) (5)"}, {"heading": "4.2 Reconstruction Model", "text": "We consider sliding windows to obtain L \u2212 l + 1 subsequences for a train instance with L cycles. LSTM-ED is trained to reconstruct the normal (healthy) subsequences of length l from all the training instances. The LSTM encoder learns a fixed length vector representation of the input time-series and the LSTM decoder uses this representation to reconstruct the time-series using the current hidden state and the value predicted at the previous time-step. Given a time-series Z = [z1 z2 ... zl], a (E) t is the hidden state of encoder at time t for each t \u2208 {1, 2, ..., l}, where a(E)t \u2208 Rc, c is the number of LSTM units in the hidden layer of the\nencoder. The encoder and decoder are jointly trained to reconstruct the time-series in reverse order (similar to [34]), i.e., the target time-series is [zl zl\u22121 ... z1]. (Note: We consider derived sensors from PCA s.t. m = p and n = c in Eq. 3.)\nFig. 2 depicts the inference steps in an LSTM-ED reconstruction model for a toy sequence with l = 3. The value xt at time instance t and the hidden state a (E) t\u22121 of the encoder at time t \u2212 1 are used to obtain the hidden state a (E) t of the encoder at time t. The hidden state a (E) l of the encoder at the end of the input sequence is used as the initial state a (D) l of the decoder s.t. a (D) l = a (E) l . A linear layer with weight matrix w of size c\u00d7m and bias vector b \u2208 Rm on top of the decoder is used to compute z\u2032t = w Ta (D) t + b. During training, the decoder uses zt as input to obtain the state a (D) t\u22121, and then predict z \u2032 t\u22121 corresponding to target zt\u22121. During inference, the predicted value z \u2032 t is input to the decoder to obtain a (D) t\u22121 and predict z \u2032 t\u22121. The reconstruction error e (u) t for a point z (u) t is given by:\ne (u) t = \u2016z (u) t \u2212 z \u2032(u) t \u2016 (6)\nThe model is trained to minimize the objective E =\u2211 u\u2208U \u2211l t=1(e (u) t )\n2. It is to be noted that for training, only the subsequences which correspond to perfect health of an instance are considered. For most cases, the first few operational cycles can be assumed to correspond to healthy state for any instance."}, {"heading": "4.3 Reconstruction Error based Target HI", "text": "A point zt in a time-series Z is part of multiple\noverlapping subsequences, and is therefore predicted by multiple subsequences Z(j, l) corresponding to j = t \u2212 l + 1, t\u2212 l+2, ..., t. Hence, each point in the original time-series for a train instance is predicted as many times as the number of subsequences it is part of (l times for each point except for points zt with t < l or t > L \u2212 l which are predicted fewer number of times). An average of all the predictions for a point is taken to be final prediction for that point. The difference in actual and predicted values for a point is used as an unnormalized HI for that point.\nError e (u) t is normalized to obtain the target HI h (u) t as:\nh (u) t =\ne (u) M \u2212 e (u) t e (u) M \u2212 e (u) m\n(7)\nwhere e (u) M and e (u) m are the maximum and minimum values of reconstruction error for instance u over t = 1 2 ... L(u), respectively. The target HI values thus obtained for all train instances are used to obtain the estimates \u03b8\u0302 and \u03b8\u03020 (see Eq. 1). Apart from e (u) t , we also consider (e (u) t )\n2 to obtain target HI values for our experiments in Section 6 such that large reconstruction errors imply much smaller HI value."}, {"heading": "5. RUL ESTIMATION USING HI CURVE MATCHING", "text": "Similar to [39, 40], the HI curve for a test instance u\u2217 is compared to the HI curves of all the train instances u \u2208 U . The test instance and train instance may take different number of cycles to reach the same degradation level (HI value). Fig. 3 shows a sample scenario where HI curve for a test instance is matched with HI curve for a train instance by varying the time-lag. The time-lag which corresponds to minimum Euclidean distance between the HI curves of the train and test instance is shown. For a given time-lag, the number of remaining cycles for the train instance after the last cycle of the test instance gives the RUL estimate for the test instance. Let u\u2217 be a test instance and u be a train instance. Similar to [29, 9, 13], we take into account the following scenarios for curve matching based RUL estimation: 1) Varying initial health across instances: The initial health of an instance varies depending on various factors such as the inherent inconsistencies in the manufacturing process. We assume initial health to be close to 1. In order to ensure this, the HI values for an instance are divided by the average of its first few HI values (e.g. first 5% cycles). Also, while comparing HI curves H(u \u2217) and H(u), we allow for a time-lag\nt such that the HI values of u\u2217 may be close to the HI values of H(u)(t, L(u\n\u2217)) at time t such that t \u2264 \u03c4 (see Eqs. 8-10). This takes care of instance specific variances in degree of initial wear and degradation evolution. 2) Multiple time-lags with high similarity : The HI curve H(u \u2217) may have high similarity with H(u)(t, L(u\u2217)) for multiple values of time-lag t. We consider multiple RUL estimates for u\u2217 based on total life of u, rather than considering only the RUL estimate corresponding to the time-lag t with minimum Euclidean distance between the curves H(u \u2217) and H(u)(t, L(u \u2217)). The multiple RUL estimates corresponding to each time-lag are assigned weights proportional to the similarity of the curves to get the final RUL estimate (see Eq. 10). 3) Non-monotonic HI : Due to inherent noise in sensor readings, HI curves obtained using LR are non-monotonic. To reduce the noise in the estimates of HI, we use moving average smoothening. 4) Maximum value of RUL estimate: When an instance is in very good health or has been operational for few cycles, estimating RUL is difficult. We limit the maximum RUL estimate for any test instance to Rmax. Also, the maximum RUL estimate for the instance u\u2217 based on HI curve comparison with instance u is limited by L(u)\u2212L(u\n\u2217). This implies that the maximum RUL estimate for any test instance u will be such that the total length R\u0302(u \u2217) +L(u \u2217) \u2264 Lmax, where Lmax is the maximum length for any training instance available. Fewer the number of cycles available for a test instance, more difficult it becomes to estimate the RUL.\nWe define similarity between HI curves of test instance u\u2217\nand train instance u with time-lag t as:\ns(u\u2217, u, t) = exp(\u2212d2(u\u2217, u, t)/\u03bb) (8)\nwhere,\nd2(u\u2217, u, t) = 1\nL(u\u2217)\nL(u \u2217)\u2211\ni=1\n(h (u\u2217) i \u2212 h (u) i+t) 2 (9)\nis the squared Euclidean distance between H(u \u2217)(1, L(u \u2217))\nand H(u)(t, L(u \u2217)), and \u03bb > 0, t \u2208 {1, 2, ..., \u03c4}, t + L(u \u2217) \u2264 L(u). Here, \u03bb controls the notion of similarity: a small value of \u03bb would imply large difference in s even when d is not large. The RUL estimate for u\u2217 based on the HI curve for u and for time-lag t is given by R\u0302(u \u2217)(u, t) = L(u) \u2212L(u \u2217) \u2212 t.\nThe estimate R\u0302(u \u2217)(u, t) is assigned a weight of s(u\u2217, u, t)\nsuch that the weighted average estimate R\u0302(u \u2217) for R(u \u2217) is given by\nR\u0302(u \u2217) =\n\u2211 s(u\u2217, u, t).R\u0302(u\n\u2217)(u, t)\u2211 s(u\u2217, u, t)\n(10)\nwhere the summation is over only those combinations of u and t which satisfy s(u\u2217, u, t) \u2265 \u03b1.smax, where smax = maxu\u2208U,t\u2208{1 ... \u03c4}{s(u\u2217, u, t)}, 0 \u2264 \u03b1 \u2264 1.\nIt is to be noted that the parameter \u03b1 decides the number of RUL estimates R\u0302(u \u2217)(u, t) to be considered to get the final RUL estimate R\u0302(u \u2217). Also, variance of the RUL estimates R\u0302(u \u2217)(u, t) considered for computing R\u0302(u\n\u2217) can be used as a measure of confidence in the prediction, which is useful in practical applications (for example, see Section 6.2.2). During the initial stages of an instance\u2019s usage, when it is in good health and a fault has still not appeared, estimating\nRUL is tough, as it is difficult to know beforehand how exactly the fault would evolve over time once it appears."}, {"heading": "6. EXPERIMENTAL EVALUATION", "text": "We evaluate our approach on two publicly available datasets: C-MAPSS Turbofan Engine Dataset [33] and Milling Machine Dataset [1], and a real world dataset from a pulverizer mill. For the first two datasets, the ground truth in terms of the RUL is known, and we use RUL estimation performance metrics to measure efficacy of our algorithm (see Section 6.1). The Pulverizer mill undergoes repair on timely basis (around one year), and therefore ground truth in terms of actual RUL is not available. We therefore draw comparison between health index and the cost of maintenance of the mills.\nFor the first two datasets, we use different target HI curves for learning the LR model (refer Section 3): LR-Lin and LR-Exp models assume linear and exponential form for the target HI curves, respectively. LR-ED1 and LR-ED2 use normalized reconstruction error and normalized squared-reconstruction error as target HI (refer Section 4.3), respectively. The target HI values for LR-Exp are obtained using Eq. 2 with \u03b2 = 5% as suggested in [40, 39, 29]."}, {"heading": "6.1 Performance metrics considered", "text": "Several metrics have been proposed for evaluating the performance of prognostics models [32]. We measure the performance in terms of Timeliness Score (S), Accuracy (A), Mean Absolute Error (MAE), Mean Squared Error (MSE), and Mean Absolute Percentage Error (MAPE1 and MAPE2) as mentioned in Eqs. 11-15, respectively. For test instance u\u2217, the error \u2206(u \u2217) = R\u0302(u \u2217) \u2212 R(u\n\u2217) between the estimated RUL (R\u0302(u \u2217)) and actual RUL (R(u \u2217)). The score S used to measure the performance of a model is given by:\nS = N\u2211 u\u2217=1 (exp(\u03b3.|\u2206(u \u2217)|)\u2212 1) (11)\nwhere \u03b3 = 1/\u03c41 if \u2206 (u\u2217) < 0, else \u03b3 = 1/\u03c42. Usually, \u03c41 > \u03c42 such that late predictions are penalized more compared to early predictions. The lower the value of S, the better is the performance.\nA = 100\nN N\u2211 u\u2217=1 I(\u2206(u \u2217)) (12)\nwhere I(\u2206(u \u2217)) = 1 if \u2206(u \u2217) \u2208 [\u2212\u03c41, \u03c42], else I(\u2206(u \u2217)) = 0, \u03c41 > 0, \u03c42 > 0.\nMAE = 1\nN N\u2211 u\u2217=1 |\u2206(u \u2217)|, MSE = 1 N N\u2211 u\u2217=1 (\u2206(u \u2217))2 (13)\nMAPE1 = 100\nN N\u2211 u\u2217=1 |\u2206(u \u2217)| R(u\u2217) (14)\nMAPE2 = 100\nN N\u2211 u\u2217=1\n|\u2206(u \u2217)|\nR(u\u2217) + L(u\u2217) (15)\nA prediction is considered a false positive (FP) if \u2206(u \u2217) <\n\u2212\u03c41, and false negative (FN) if \u2206(u \u2217) > \u03c42."}, {"heading": "6.2 C-MAPSS Turbofan Engine Dataset", "text": "We consider the first dataset from the simulated turbofan engine data [33] (NASA Ames Prognostics Data Repository). The dataset contains readings for 24 sensors (3 operation setting sensors, 21 dependent sensors) for 100 engines till a failure threshold is achieved, i.e., till end-of-life in train FD001.txt. Similar data is provided for 100 test engines in test FD001.txt where the time-series for engines are pruned some time prior to failure. The task is to predict RUL for these 100 engines. The actual RUL values are provided in RUL FD001.txt. There are a total of 20631 cycles for training engines, and 13096 cycles for test engines. Each engine has a different degree of initial wear. We use \u03c41 = 13, \u03c42 = 10 as proposed in [33].\n6.2.1 Model learning and parameter selection We randomly select 80 engines for training the LSTM-ED\nmodel and estimating parameters \u03b8 and \u03b80 of the LR model (refer Eq. 1). The remaining 20 training instances are used for selecting the parameters. The trajectories for these 20 engines are randomly truncated at five different locations s.t. five different cases are obtained from each instance. Minimum truncation is 20% of the total life and maximum truncation is 96%. For training LSTM-ED, only the first subsequence of length l for each of the selected 80 engines is used. The parameters number of principal components p, the number of LSTM units in the hidden layers of encoder and decoder c, window/subsequence length l, maximum allowed time-lag \u03c4 , similarity threshold \u03b1 (Eq. 10), maximum predicted RUL Rmax, and parameter \u03bb (Eq. 8) are estimated using grid search to minimize S on the validation set. The parameters obtained for the best model (LR-ED2) are p = 3, c = 30, l = 20, \u03c4 = 40, \u03b1 = 0.87, Rmax = 125, and \u03bb = 0.0005.\n6.2.2 Results and Observations LSTM-ED based Unsupervised HI: Fig. 4 shows\nthe average pointwise reconstruction error (refer Eq. 6) given by the model LSTM-ED which uses the pointwise reconstruction error as an unnormalized measure of health (higher the reconstruction error, poorer the health) of all the 100 test engines w.r.t. percentage of life passed (for derived sensor sequences with p = 3 as used for model LR-ED1 and LR-ED2). During initial stages of an engine\u2019s life, the average reconstruction error is small. As the number of cycles passed increases, the reconstruction error increases. This suggests that reconstruction error can be used as an\n\u221270 \u221250 \u221230 \u221210 10 30 50 Error (R\u0302-R)\n0\n5\n10\n15\n20\n25\n30\n35\nNo . o\nf E ng\nin es\n(a) LSTM-ED\n\u221270 \u221250 \u221230 \u221210 10 30 50 Error (R\u0302-R)\n0\n5\n10\n15\n20\n25\n30\n35\nNo . o\nf E ng\nin es\n(b) LR-Exp\n\u221270 \u221250 \u221230 \u221210 10 30 50 Error (R\u0302-R)\n0\n5\n10\n15\n20\n25\n30\n35\nNo . o\nf E ng\nin es\n(c) LR-ED1\n\u221270 \u221250 \u221230 \u221210 10 30 50 Error (R\u0302-R)\n0\n5\n10\n15\n20\n25\n30\n35\nNo . o\nf E ng\nin es\n(d) LR-ED2\nFigure 5: Turbofan Engine: Histograms of prediction errors for Turbofan Engine dataset.\n0 10 20 30 40 50 60 70 80 90 100 Engine (Increasing RUL)\n0\n20\n40\n60\n80\n100\n120\n140\n160\nRU L\nActual LR-Exp LR-ED1 LR-ED2\n(a) RUL estimates given by LR-Exp, LR-ED1, and LR-ED2\n0.0-0.1 0.1-0.2 0.2-0.3 0.3-0.4 0.4-0.5 0.5-0.6 0.6-0.7 0.7-0.8 0.8-0.9 Predicted Health Index Interval\n0 10 20 30 40 50 60 70\nMax - Min Standard Deviation Absolute Error\n(b) Standard deviation, max-min, and absolute error w.r.t HI at last cycle\nFigure 6: Turbofan Engine: (a) RUL estimates for all 100 engines, (b) Absolute error w.r.t. HI at last cycle\nindicator of health of a machine. Fig. 5(a) and Table 1 suggest that RUL estimates given by HI from LSTM-ED are fairly accurate. On the other hand, the 1-sigma bars in Fig. 4 also suggest that the reconstruction error at a given point in time (percentage of total life passed) varies significantly from engine to engine.\nPerformance comparison: Table 1 and Fig. 5 show the performance of the four models LSTM-ED (without using linear regression), LR-Exp, LR-ED1, and LR-ED2. We found that LR-ED2 performs significantly better compared to the other three models. LR-ED2 is better than the LR-Exp model which uses domain knowledge in the form of exponential degradation assumption. We also provide comparison with RULCLIPPER (RC) [29] which (to the best of our knowledge) has the best performance in terms of timeliness S, accuracy A, MAE, and MSE [31] reported in the literature1 on the turbofan dataset considered and four other turbofan engine datasets (Note: Unlike RC, we learn the parameters of the model on a validation set rather than test set.) RC relies on the exponential assumption to estimate a HI polygon and uses intersection of areas of polygons of train and test engines as a measure of similarity to estimate RUL (similar to Eq. 10, see [29] for details). The results show that LR-ED2 gives performance comparable to RC without relying on the domain-knowledge based exponential assumption.\nThe worst predicted test instance for LR-Exp, LR-ED1 and LR-ED2 contributes 23%, 17%, and 23%, respectively, to the timeliness score S. For LR-Exp and LR-ED2 it is nearly 1/4th of the total score, and suggests that for other 99 test engines the timeliness score S is very good.\n1For comparison with some of the other benchmarks readily available in literature, see Table 4. It is to be noted that the comparison is not exhaustive as a survey of approaches for the turbofan engine dataset since [31] is not available.\nHI at last cycle and RUL estimation error: Fig. 6(a) shows the actual and estimated RULs for LR-Exp, LR-ED1, and LR-ED2. For all the models, we observe that as the actual RUL increases, the error in predicted values increases. Let R (u\u2217) all denote the set of all the RUL estimates R\u0302(u \u2217)(u, t) considered to obtain the final RUL estimate R\u0302(u \u2217) (see Eq. 10). Fig. 6(b) shows the average values of absolute error, standard deviation of the elements in R (u\u2217) all , and the difference of the maximum and the minimum value of the elements in R (u\u2217) all w.r.t. HI value at last cycle. It suggests that when an instance is close to failure, i.e., HI at last cycle is low, RUL estimate is very accurate with low standard deviation of the elements in R (u\u2217) all . On the other hand, when an instance is in good health, i.e., when HI at last cycle is close to 1, the error in RUL estimate is high, and the elements in R (u\u2217) all have high standard deviation."}, {"heading": "6.3 Milling Machine Dataset", "text": "This data set presents milling tool wear measurements from a lab experiment. Flank wear is measured for 16 cases with each case having varying number of runs of varying durations. The wear is measured after runs but not necessarily after every run. The data contains readings for 10 variables (3 operating condition variables, 6 dependent\n0.0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1.0 1.1 Fraction of total life passed 0.0\n0.5\n1.0\n1.5\n2.0\nRe co\nns tr uc\ntio n Er ro r\n(a) Recon. Error Material-1\n\u221244 \u221234 \u221224 \u221214 \u22124 6 16 26 36 46 Error (R\u0302-R)\n0\n50\n100\n150\n200\n250\n300\n350\nNo . o\nf i ns\nta nc\nes\n(b) PCA1 Material-1\n\u221244\u221234\u221224\u221214\u22124 6 16 26 36 46 Error (R\u0302-R)\n0\n50\n100\n150\n200\n250\n300\n350\nNo . o\nf i ns\nta nc\nes\n(c) LR-ED1 Material-1\n\u221244 \u221234 \u221224 \u221214 \u22124 6 16 26 36 46 Error (R\u0302-R)\n0\n50\n100\n150\n200\n250\n300\n350\nNo . o\nf i ns\nta nc\nes\n(d) LR-ED2 Material-1\n0.0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1.0 1.1 Fraction of total life passed 0.0\n0.5\n1.0\n1.5\n2.0\nRe co\nns tr uc\ntio n Er ro r\n(e) Recon. Error Material-2\n\u22129 \u22127 \u22125 \u22123 \u22121 1 3 5 Error (R\u0302-R)\n0\n10\n20\n30\n40 50 No . o f i ns ta nc es\n(f) PCA1 Material-2\n\u22129 \u22127 \u22125 \u22123 \u22121 1 3 5 Error (R\u0302-R)\n0\n10\n20\n30\n40\n50\nNo . o\nf i ns\nta nc\nes\n(g) LR-ED1 Material-2\n\u22129 \u22127 \u22125 \u22123 \u22121 1 3 5 Error (R\u0302-R)\n0\n10\n20\n30\n40\n50\nNo . o\nf i ns\nta nc\nes\n(h) LR-ED2 Material-2\nFigure 7: Milling Machine: Reconstruction errors w.r.t. cycles passed and histograms of prediction errors for milling machine dataset.\nsensors, 1 variable measuring time elapsed until completion of that run). A snapshot sequence of 9000 points during a run for the 6 dependent sensors is provided. We assume each run to represent one cycle in the life of the tool. We consider two operating regimes corresponding to the two types of material being milled, and learn a different model for each material type. There are a total of 167 runs across cases with 109 runs and 58 runs for material types 1 and 2, respectively. Case number 6 of material 2 has only one run, and hence not considered for experiments.\n6.3.1 Model learning and parameter selection Since number of cases is small, we use leave one out\nmethod for model learning and parameters selection. For training the LSTM-ED model, the first run of each case is considered as normal with sequence length of 9000. An average of the reconstruction error for a run is used to the get the target HI for that run/cycle. We consider mean and standard deviation of each run (9000 values) for the 6 sensors to obtain 2 derived sensors per sensor (similar to [9]). We reduce the gap between two consecutive runs, via linear interpolation, to 1 second (if it is more); as a result HI curves for each case will have a cycle of one second. The tool wear is also interpolated in the same manner and the data for each case is truncated until the point when the tool wear crosses a value of 0.45 for the first time. The target HI from LSTM-ED for the LR model is also interpolated appropriately for learning the LR model.\nThe parameters obtained for the best models (based on minimum MAPE1) for material-1 are p = 1, \u03bb = 0.025, \u03b1 = 0.98, \u03c4 = 15 for PCA1, for material-2 are p = 2, \u03bb = 0.005, \u03b1 = 0.87, \u03c4 = 13, and c = 45 for LR-ED1. The best results are obtained without setting any limit Rmax. For both cases, l = 90 (after downsampling by 100) s.t. the time-series for first run is used for learning the LSTM-ED model.\n6.3.2 Results and observations\nFigs. 7(a) and 7(e) show the variation of average reconstruction error from LSTM-ED w.r.t. the fraction of life passed for both materials. As shown, this error increases with amount of life passed, and hence is an appropriate indicator of health.\nFigs. 7 and 8 show results based on every cycle of the data after interpolation, except when explicitly mentioned in the figure. The performance metrics on the original data points in the data set are summarized in Table 2. We observe that the first PCA component (PCA1, p = 1) gives better results than LR-Lin and LR-Exp models with p \u2265 2, and hence we present results for PCA1 in Table 2. It is to be noted that for p = 1, all the four models LR-Lin, LR-Exp, LR-ED1, and LR-ED2 will give same results since all models will predict a different linearly scaled value of the first PCA component. PCA1 and LR-ED1 are the best models for material-1 and material-2, respectively. We observe that our best models perform well as depicted in histograms in Fig. 7. For the last few cycles, when actual RUL is low, an error of even 1 in RUL estimation leads to MAPE1 of 100%. Figs. 7(b-d), 7(f-h) show the error distributions for different models for the two materials. As can be noted, most of the RUL prediction errors (around 70%) lie in the ranges [-4, 6] and [-3, 1] for material types 1 and 2, respectively. Also, Figs. 8(a) and 8(b) show predicted and actual RULs for different models for the two materials."}, {"heading": "6.4 Pulverizer Mill Dataset", "text": "This dataset consists of readings for 6 sensors (such as bearing vibration, feeder speed, etc.) for over three years of operation of a pulverizer mill. The data corresponds to sensor readings taken every half hour between four consecutive scheduled maintenances M0, M1, M2, and M3, s.t. the operational period between any two maintenances is roughly one year. Each day\u2019s multivariate time-series data with length l = 48 is considered to be one subsequence. Apart from these scheduled maintenances, maintenances are done in between whenever the mill develops any unexpected fault affecting its normal operation. The costs incurred for any of the scheduled maintenances and unexpected maintenances are available. We consider the (z-normalized) original sensor readings directly for analysis rather than computing the PCA based derived sensors.\nWe assume the mill to be healthy for the first 10% of\nthe days of a year between any two consecutive time-based maintenances Mi and Mi+1, and use the corresponding subsequences for learning LSTM-ED models. This data is divided into training and validation sets. A different LSTM-ED model is learnt after each maintenance. The architecture with minimum average reconstruction error over a validation set is chosen as the best model. The best models learnt using data after M0, M1 and M2 are obtained for c = 40, 20, and 100, respectively. The LSTM-ED based reconstruction error for each day is z-normalized using the mean and standard deviation of the reconstruction errors over the sequences in validation set.\nFrom the results in Table 3, we observe that average reconstruction error E on the last day before M1 is the least, and so is the cost C(M1) incurred during M1. For M2 and M3, E as well as corresponding C(Mi) are higher compared to those of M1. Further, we observe that for the days when average value of reconstruction error E > tE , a large fraction (>0.61) of them have a high ongoing maintenance cost C > tC . The significant correlation between reconstruction error and cost incurred suggests that the LSTM-ED based reconstruction error is able to capture the health of the mill."}, {"heading": "7. RELATED WORK", "text": "Similar to our idea of using reconstruction error for modeling normal behavior, variants of Bayesian Networks have been used to model the joint probability distribution over the sensor variables, and then using the evidence probability based on sensor readings as an indicator of normal behaviour (e.g. [11, 35]). [25] presents an unsupervised approach which does not assume a form for the HI curve and uses discrete Bayesian Filter to recursively estimate the health index values, and then use the k-NN classifier to find the most similar offline models. Gaussian Process Regression (GPR) is used to extrapolate the HI curve if the best class-probability is lesser than a threshold.\nRecent review of statistical methods is available in [31, 36, 15]. We use HI Trajectory Similarity Based Prediction (TSBP) for RUL estimation similar to [40] with the key difference being that the model proposed in [40] relies on exponential assumption to learn a regression model for HI estimation, whereas our approach uses LSTM-ED based HI to learn the regression model. Similarly, [29] proposes RULCLIPPER (RC) which to the best of our knowledge has shown the best performance on C-MAPSS when compared to various approaches evaluated using the dataset [31]. RC tries various combinations of features to select the best set of features and learns linear regression (LR) model over these features to predict health index which is mapped to a HI polygon, and similarity of HI polygons is used instead of univariate curve matching. The parameters of LR model are estimated by assuming exponential target HI (refer Eq. 2). Another variant of TSBP [39] directly uses multiple PCA sensors for multi-dimensional curve matching which is used for RUL estimation, whereas we obtain a univariate HI by taking a weighted combination of the PCA sensors learnt\nusing the unsupervised HI obtained from LSTM-ED as the target HI. Similarly, [20, 19] learn a composite health index based on the exponential assumption.\nMany variants of neural networks have been proposed for prognostics (e.g. [3, 27, 13, 18]): very recently, deep Convolutional Neural Networks have been proposed in [3] and shown to outperform regression methods based on Multi-Layer Perceptrons, Support Vector Regression and Relevance Vector Regression to directly estimate the RUL (see Table 4 for performance comparison with our approach on the Turbofan Engine dataset). The approach uses deep CNNs to learn higher level abstract features from raw sensor values which are then used to directly estimate the RUL, where RUL is assumed to be constant for a fixed number of starting cycles and then assumed to decrease linearly. Similarly, Recurrent Neural Network has been used to directly estimate the RUL [13] from the time-series of sensor readings. Whereas these models directly estimate the RUL, our approach first estimates health index and then uses curve matching to estimate RUL. To the best of our knowledge, our LSTM Encoder-Decoder based approach performs significantly better than Echo State Network [27] and deep CNN based approaches [3] (refer Table 4).\nReconstruction models based on denoising autoencoders [23] have been proposed for novelty/anomaly detection but have not been evaluated for RUL estimation task. LSTM networks have been used for anomaly/fault detection [22, 7, 41], where deep LSTM networks are used to learn prediction models for the normal time-series. The likelihood of the prediction error is then used as a measure of anomaly. These models predict time-series in the future and then use prediction errors to estimate health or novelty of a point. Such models rely on the the assumption that the normal time-series should be predictable, whereas LSTM-ED learns a representation from the entire sequence which is then used to reconstruct the sequence, and therefore does not depend on the predictability assumption for normal time-series."}, {"heading": "8. DISCUSSION", "text": "We have proposed an unsupervised approach to estimate health index (HI) of a system from multi-sensor time-series data. The approach uses time-series instances corresponding to healthy behavior of the system to learn a reconstruction model based on LSTM Encoder-Decoder (LSTM-ED). We then show how the unsupervised HI can be used for estimating remaining useful life instead of relying on domain knowledge based degradation models. The proposed approach shows promising results overall, and in some cases performs better than models which rely on assumptions about health degradation for the Turbofan Engine and Milling Machine datasets. Whereas models relying on assumptions such as exponential health degradation cannot attune themselves to instance specific behavior, the LSTM-ED based HI uses the temporal history of sensor readings to predict the HI at a point. A case study on real-world industry dataset of a pulverizer mill shows signs of correlation between LSTM-ED based reconstruction error and the cost incurred for maintaining the mill, suggesting that LSTM-ED is able to capture the severity of fault."}, {"heading": "9. REFERENCES", "text": "[1] A. Agogino and K. Goebel. Milling data set. BEST\nlab, UC Berkeley, 2007.\n[2] G. Anand, A. H. Kazmi, P. Malhotra, L. Vig, P. Agarwal, and G. Shroff. Deep temporal features to predict repeat buyers. In NIPS 2015 Workshop: Machine Learning for eCommerce, 2015.\n[3] G. S. Babu, P. Zhao, and X.-L. Li. Deep convolutional neural network based regression approach for estimation of remaining useful life. In Database Systems for Advanced Applications. Springer, 2016.\n[4] S. Bengio, O. Vinyals, N. Jaitly, and N. Shazeer. Scheduled sampling for sequence prediction with recurrent neural networks. In Advances in Neural Information Processing Systems, 2015.\n[5] F. Cadini, E. Zio, and D. Avram. Model-based monte carlo state estimation for condition-based component replacement. Reliability Engg. & System Safety, 2009.\n[6] F. Camci, O. F. Eker, S. Bas\u0327kan, and S. Konur. Comparison of sensors and methodologies for effective prognostics on railway turnout systems. Proceedings of the Institution of Mechanical Engineers, Part F: Journal of Rail and Rapid Transit, 230(1):24\u201342, 2016.\n[7] S. Chauhan and L. Vig. Anomaly detection in ecg time signals via deep long short-term memory networks. In Data Science and Advanced Analytics (DSAA), 2015. 36678 2015. IEEE International Conference on, pages 1\u20137. IEEE, 2015.\n[8] K. Cho, B. Van Merrie\u0308nboer, C. Gulcehre, D. Bahdanau, F. Bougares, H. Schwenk, and Y. Bengio. Learning phrase representations using rnn encoder-decoder for statistical machine translation. arXiv preprint arXiv:1406.1078, 2014.\n[9] J. B. Coble. Merging data sources to predict remaining useful life\u2013an automated method to identify prognostic parameters. 2010.\n[10] C. Croarkin and P. Tobias. Nist/sematech e-handbook of statistical methods. NIST/SEMATECH, 2006.\n[11] A. Dubrawski, M. Baysek, M. S. Mikus, C. McDaniel, B. Mowry, L. Moyer, J. Ostlund, N. Sondheimer, and T. Stewart. Applying outbreak detection algorithms to prognostics. In AAAI Fall Symposium on Artificial Intelligence for Prognostics, 2007.\n[12] A. Graves, M. Liwicki, S. Ferna\u0301ndez, R. Bertolami, H. Bunke, and J. Schmidhuber. A novel connectionist system for unconstrained handwriting recognition. Pattern Analysis and Machine Intelligence, IEEE Transactions on, 31(5):855\u2013868, 2009.\n[13] F. O. Heimes. Recurrent neural networks for remaining useful life estimation. In International Conference on Prognostics and Health Management, 2008.\n[14] S. Hochreiter and J. Schmidhuber. Long short-term memory. Neural computation, 9(8):1735\u20131780, 1997.\n[15] M. Jouin, R. Gouriveau, D. Hissel, M.-C. Pe\u0301ra, and N. Zerhouni. Particle filter-based prognostics: Review, discussion and perspectives. Mechanical Systems and Signal Processing, 2015.\n[16] R. Khelif, S. Malinowski, B. Chebel-Morello, and N. Zerhouni. Rul prediction based on a new similarity-instance based approach. In IEEE International Symposium on Industrial Electronics, 2014.\n[17] J. Lam, S. Sankararaman, and B. Stewart. Enhanced trajectory based similarity prediction with uncertainty quantification. PHM 2014, 2014.\n[18] J. Liu, A. Saxena, K. Goebel, B. Saha, and W. Wang. An adaptive recurrent neural network for remaining useful life prediction of lithium-ion batteries. Technical report, DTIC Document, 2010.\n[19] K. Liu, A. Chehade, and C. Song. Optimize the signal quality of the composite health index via data fusion for degradation modeling and prognostic analysis. 2015.\n[20] K. Liu, N. Z. Gebraeel, and J. Shi. A data-level fusion model for developing composite health indices for degradation modeling and prognostic analysis. Automation Science and Engineering, IEEE Transactions on, 10(3):652\u2013664, 2013.\n[21] P. Malhotra, A. Ramakrishnan, G. Anand, L. Vig, P. Agarwal, and G. Shroff. Lstm-based encoder-decoder for multi-sensor anomaly detection. 33rd International Conference on Machine Learning, Anomaly Detection Workshop. arXiv preprint arXiv:1607.00148, 2016.\n[22] P. Malhotra, L. Vig, G. Shroff, and P. Agarwal. Long short term memory networks for anomaly detection in time series. In ESANN, 23rd European Symposium on Artificial Neural Networks, Computational Intelligence and Machine Learning, 2015.\n[23] E. Marchi, F. Vesperini, F. Eyben, S. Squartini, and B. Schuller. A novel approach for automatic acoustic novelty detection using a denoising autoencoder with bidirectional lstm neural networks. In Acoustics, Speech and Signal Processing (ICASSP). IEEE, 2015.\n[24] A. Mosallam, K. Medjaher, and N. Zerhouni. Data-driven prognostic method based on bayesian approaches for direct remaining useful life prediction. Journal of Intelligent Manufacturing, 2014.\n[25] A. Mosallam, K. Medjaher, and N. Zerhouni. Component based data-driven prognostics for complex systems: Methodology and applications. In Reliability Systems Engineering (ICRSE), 2015 First International Conference on, pages 1\u20137. IEEE, 2015.\n[26] E. Myo\u0308tyri, U. Pulkkinen, and K. Simola. Application of stochastic filtering for lifetime prediction. Reliability Engineering & System Safety, 91(2):200\u2013208, 2006.\n[27] Y. Peng, H. Wang, J. Wang, D. Liu, and X. Peng. A modified echo state network based remaining useful life estimation approach. In IEEE Conference on Prognostics and Health Management (PHM), 2012.\n[28] H. Qiu, J. Lee, J. Lin, and G. Yu. Robust performance degradation assessment methods for enhanced rolling element bearing prognostics. Advanced Engineering Informatics, 17(3):127\u2013140, 2003.\n[29] E. Ramasso. Investigating computational geometry for failure prognostics in presence of imprecise health indicator: Results and comparisons on c-mapss datasets. In 2nd Europen Confernce of the Prognostics and Health Management Society., 2014.\n[30] E. Ramasso, M. Rombaut, and N. Zerhouni. Joint prediction of continuous and discrete states in time-series based on belief functions. Cybernetics, IEEE Transactions on, 43(1):37\u201350, 2013.\n[31] E. Ramasso and A. Saxena. Performance benchmarking and analysis of prognostic methods for cmapss datasets. Int. J. Progn. Health Manag, 2014.\n[32] A. Saxena, J. Celaya, E. Balaban, K. Goebel, B. Saha,\nS. Saha, and M. Schwabacher. Metrics for evaluating performance of prognostic techniques. In Prognostics and health management, 2008. phm 2008. international conference on, pages 1\u201317. IEEE, 2008.\n[33] A. Saxena, K. Goebel, D. Simon, and N. Eklund. Damage propagation modeling for aircraft engine run-to-failure simulation. In Prognostics and Health Management, 2008. PHM 2008. International Conference on, pages 1\u20139. IEEE, 2008.\n[34] I. Sutskever, O. Vinyals, and Q. V. Le. Sequence to sequence learning with neural networks. In Z. Ghahramani, M. Welling, C. Cortes, N. D. Lawrence, and K. Q. Weinberger, editors, Advances in Neural Information Processing Systems. 2014.\n[35] D. Tobon-Mejia, K. Medjaher, and N. Zerhouni. Cnc machine tool\u2019s wear diagnostic and prognostic by using dynamic bayesian networks. Mechanical Systems and Signal Processing, 28:167\u2013182, 2012.\n[36] K. L. Tsui, N. Chen, Q. Zhou, Y. Hai, and W. Wang. Prognostics and health management: A review on data driven approaches. Mathematical Problems in Engineering, 2015, 2015.\n[37] P. Wang and G. Vachtsevanos. Fault prognostics using dynamic wavelet neural networks. AI EDAM, 2001.\n[38] P. Wang, B. D. Youn, and C. Hu. A generic probabilistic framework for structural health prognostics and uncertainty management. Mechanical Systems and Signal Processing, 28:622\u2013637, 2012.\n[39] T. Wang. Trajectory similarity based prediction for remaining useful life estimation. PhD thesis, University of Cincinnati, 2010.\n[40] T. Wang, J. Yu, D. Siegel, and J. Lee. A similarity-based prognostics approach for remaining useful life estimation of engineered systems. In Prognostics and Health Management, International Conference on. IEEE, 2008.\n[41] M. Yadav, P. Malhotra, L. Vig, K. Sriram, and G. Shroff. Ode-augmented training improves anomaly detection in sensor data from machines. In NIPS Time Series Workshop. arXiv preprint arXiv:1605.01534, 2015.\n[42] W. Zaremba, I. Sutskever, and O. Vinyals. Recurrent neural network regularization. arXiv preprint arXiv:1409.2329, 2014.\nAPPENDIX"}, {"heading": "A. BENCHMARKS ON TURBOFAN ENGINE DATASET", "text": ""}], "references": [{"title": "Milling data set", "author": ["A. Agogino", "K. Goebel"], "venue": "BEST lab, UC Berkeley,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2007}, {"title": "Deep temporal features to predict repeat buyers", "author": ["G. Anand", "A.H. Kazmi", "P. Malhotra", "L. Vig", "P. Agarwal", "G. Shroff"], "venue": "In NIPS 2015 Workshop: Machine Learning for eCommerce,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2015}, {"title": "Deep convolutional neural network based regression approach for estimation of remaining useful life", "author": ["G.S. Babu", "P. Zhao", "X.-L. Li"], "venue": "In Database Systems for Advanced Applications", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2016}, {"title": "Scheduled sampling for sequence prediction with recurrent neural networks", "author": ["S. Bengio", "O. Vinyals", "N. Jaitly", "N. Shazeer"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2015}, {"title": "Model-based monte carlo state estimation for condition-based component replacement", "author": ["F. Cadini", "E. Zio", "D. Avram"], "venue": "Reliability Engg. & System Safety,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2009}, {"title": "Comparison of sensors and methodologies for effective prognostics on railway turnout systems", "author": ["F. Camci", "O.F. Eker", "S. Ba\u015fkan", "S. Konur"], "venue": "Proceedings of the Institution of Mechanical Engineers, Part F: Journal of Rail and Rapid Transit,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2016}, {"title": "Anomaly detection in ecg time signals via deep long short-term memory networks", "author": ["S. Chauhan", "L. Vig"], "venue": "In Data Science and Advanced Analytics (DSAA),", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2015}, {"title": "Learning phrase representations using rnn encoder-decoder for statistical machine translation", "author": ["K. Cho", "B. Van Merri\u00ebnboer", "C. Gulcehre", "D. Bahdanau", "F. Bougares", "H. Schwenk", "Y. Bengio"], "venue": "arXiv preprint arXiv:1406.1078,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2014}, {"title": "Merging data sources to predict remaining useful life\u2013an automated method to identify prognostic parameters", "author": ["J.B. Coble"], "venue": null, "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2010}, {"title": "Nist/sematech e-handbook of statistical methods", "author": ["C. Croarkin", "P. Tobias"], "venue": "NIST/SEMATECH,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2006}, {"title": "Applying outbreak detection algorithms to prognostics", "author": ["A. Dubrawski", "M. Baysek", "M.S. Mikus", "C. McDaniel", "B. Mowry", "L. Moyer", "J. Ostlund", "N. Sondheimer", "T. Stewart"], "venue": "In AAAI Fall Symposium on Artificial Intelligence for Prognostics,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2007}, {"title": "A novel connectionist system for unconstrained handwriting recognition", "author": ["A. Graves", "M. Liwicki", "S. Fern\u00e1ndez", "R. Bertolami", "H. Bunke", "J. Schmidhuber"], "venue": "Pattern Analysis and Machine Intelligence, IEEE Transactions on,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2009}, {"title": "Recurrent neural networks for remaining useful life estimation", "author": ["F.O. Heimes"], "venue": "In International Conference on Prognostics and Health Management,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2008}, {"title": "Long short-term memory", "author": ["S. Hochreiter", "J. Schmidhuber"], "venue": "Neural computation,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 1997}, {"title": "Particle filter-based prognostics: Review, discussion and perspectives", "author": ["M. Jouin", "R. Gouriveau", "D. Hissel", "M.-C. P\u00e9ra", "N. Zerhouni"], "venue": "Mechanical Systems and Signal Processing,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2015}, {"title": "Rul prediction based on a new similarity-instance based approach", "author": ["R. Khelif", "S. Malinowski", "B. Chebel-Morello", "N. Zerhouni"], "venue": "In IEEE International Symposium on Industrial Electronics,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2014}, {"title": "Enhanced trajectory based similarity prediction with uncertainty quantification", "author": ["J. Lam", "S. Sankararaman", "B. Stewart"], "venue": "PHM 2014,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2014}, {"title": "An adaptive recurrent neural network for remaining useful life prediction of lithium-ion batteries", "author": ["J. Liu", "A. Saxena", "K. Goebel", "B. Saha", "W. Wang"], "venue": "Technical report, DTIC Document,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2010}, {"title": "Optimize the signal quality of the composite health index via data fusion for degradation modeling and prognostic analysis", "author": ["K. Liu", "A. Chehade", "C. Song"], "venue": null, "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2015}, {"title": "A data-level fusion model for developing composite health indices for degradation modeling and prognostic analysis", "author": ["K. Liu", "N.Z. Gebraeel", "J. Shi"], "venue": "Automation Science and Engineering, IEEE Transactions on,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2013}, {"title": "Lstm-based encoder-decoder for multi-sensor anomaly detection", "author": ["P. Malhotra", "A. Ramakrishnan", "G. Anand", "L. Vig", "P. Agarwal", "G. Shroff"], "venue": "33rd International Conference on Machine Learning, Anomaly Detection Workshop. arXiv preprint arXiv:1607.00148,", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2016}, {"title": "Long short term memory networks for anomaly detection in time series", "author": ["P. Malhotra", "L. Vig", "G. Shroff", "P. Agarwal"], "venue": "In ESANN, 23rd European Symposium on Artificial Neural Networks, Computational Intelligence and Machine Learning,", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2015}, {"title": "A novel approach for automatic acoustic novelty detection using a denoising autoencoder with bidirectional lstm neural networks", "author": ["E. Marchi", "F. Vesperini", "F. Eyben", "S. Squartini", "B. Schuller"], "venue": "In Acoustics, Speech and Signal Processing (ICASSP). IEEE,", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2015}, {"title": "Data-driven prognostic method based on bayesian approaches for direct remaining useful life prediction", "author": ["A. Mosallam", "K. Medjaher", "N. Zerhouni"], "venue": "Journal of Intelligent Manufacturing,", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2014}, {"title": "Component based data-driven prognostics for complex systems: Methodology and applications", "author": ["A. Mosallam", "K. Medjaher", "N. Zerhouni"], "venue": "In Reliability Systems Engineering (ICRSE),", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2015}, {"title": "Application of stochastic filtering for lifetime prediction", "author": ["E. My\u00f6tyri", "U. Pulkkinen", "K. Simola"], "venue": "Reliability Engineering & System Safety,", "citeRegEx": "26", "shortCiteRegEx": "26", "year": 2006}, {"title": "A modified echo state network based remaining useful life estimation approach", "author": ["Y. Peng", "H. Wang", "J. Wang", "D. Liu", "X. Peng"], "venue": "In IEEE Conference on Prognostics and Health Management (PHM),", "citeRegEx": "27", "shortCiteRegEx": "27", "year": 2012}, {"title": "Robust performance degradation assessment methods for enhanced rolling element bearing prognostics", "author": ["H. Qiu", "J. Lee", "J. Lin", "G. Yu"], "venue": "Advanced Engineering Informatics,", "citeRegEx": "28", "shortCiteRegEx": "28", "year": 2003}, {"title": "Investigating computational geometry for failure prognostics in presence of imprecise health indicator: Results and comparisons on c-mapss datasets", "author": ["E. Ramasso"], "venue": "Europen Confernce of the Prognostics and Health Management Society.,", "citeRegEx": "29", "shortCiteRegEx": "29", "year": 2014}, {"title": "Joint prediction of continuous and discrete states in time-series based on belief functions", "author": ["E. Ramasso", "M. Rombaut", "N. Zerhouni"], "venue": "Cybernetics, IEEE Transactions on,", "citeRegEx": "30", "shortCiteRegEx": "30", "year": 2013}, {"title": "Performance benchmarking and analysis of prognostic methods for cmapss datasets", "author": ["E. Ramasso", "A. Saxena"], "venue": "Int. J. Progn. Health Manag,", "citeRegEx": "31", "shortCiteRegEx": "31", "year": 2014}, {"title": "Metrics for evaluating performance of prognostic techniques", "author": ["A. Saxena", "J. Celaya", "E. Balaban", "K. Goebel", "B. Saha", "S. Saha", "M. Schwabacher"], "venue": "In Prognostics and health management,", "citeRegEx": "32", "shortCiteRegEx": "32", "year": 2008}, {"title": "Damage propagation modeling for aircraft engine run-to-failure simulation", "author": ["A. Saxena", "K. Goebel", "D. Simon", "N. Eklund"], "venue": "In Prognostics and Health Management,", "citeRegEx": "33", "shortCiteRegEx": "33", "year": 2008}, {"title": "Sequence to sequence learning with neural networks", "author": ["I. Sutskever", "O. Vinyals", "Q.V. Le"], "venue": "Advances in Neural Information Processing Systems", "citeRegEx": "34", "shortCiteRegEx": "34", "year": 2014}, {"title": "Cnc machine tool\u2019s wear diagnostic and prognostic by using dynamic bayesian networks", "author": ["D. Tobon-Mejia", "K. Medjaher", "N. Zerhouni"], "venue": "Mechanical Systems and Signal Processing,", "citeRegEx": "35", "shortCiteRegEx": "35", "year": 2012}, {"title": "Prognostics and health management: A review on data driven approaches", "author": ["K.L. Tsui", "N. Chen", "Q. Zhou", "Y. Hai", "W. Wang"], "venue": "Mathematical Problems in Engineering,", "citeRegEx": "36", "shortCiteRegEx": "36", "year": 2015}, {"title": "Fault prognostics using dynamic wavelet neural networks", "author": ["P. Wang", "G. Vachtsevanos"], "venue": "AI EDAM,", "citeRegEx": "37", "shortCiteRegEx": "37", "year": 2001}, {"title": "A generic probabilistic framework for structural health prognostics and uncertainty management", "author": ["P. Wang", "B.D. Youn", "C. Hu"], "venue": "Mechanical Systems and Signal Processing,", "citeRegEx": "38", "shortCiteRegEx": "38", "year": 2012}, {"title": "Trajectory similarity based prediction for remaining useful life estimation", "author": ["T. Wang"], "venue": "PhD thesis, University of Cincinnati,", "citeRegEx": "39", "shortCiteRegEx": "39", "year": 2010}, {"title": "A similarity-based prognostics approach for remaining useful life estimation of engineered systems", "author": ["T. Wang", "J. Yu", "D. Siegel", "J. Lee"], "venue": "In Prognostics and Health Management, International Conference on. IEEE,", "citeRegEx": "40", "shortCiteRegEx": "40", "year": 2008}, {"title": "Ode-augmented training improves anomaly detection in sensor data from machines", "author": ["M. Yadav", "P. Malhotra", "L. Vig", "K. Sriram", "G. Shroff"], "venue": "In NIPS Time Series Workshop. arXiv preprint arXiv:1605.01534,", "citeRegEx": "41", "shortCiteRegEx": "41", "year": 2015}, {"title": "Recurrent neural network regularization", "author": ["W. Zaremba", "I. Sutskever", "O. Vinyals"], "venue": "arXiv preprint arXiv:1409.2329,", "citeRegEx": "42", "shortCiteRegEx": "42", "year": 2014}], "referenceMentions": [{"referenceID": 28, "context": "Extrapolation of HI is used for prediction of RUL [29, 24, 25].", "startOffset": 50, "endOffset": 62}, {"referenceID": 23, "context": "Extrapolation of HI is used for prediction of RUL [29, 24, 25].", "startOffset": 50, "endOffset": 62}, {"referenceID": 24, "context": "Extrapolation of HI is used for prediction of RUL [29, 24, 25].", "startOffset": 50, "endOffset": 62}, {"referenceID": 4, "context": "Apart from the health index (HI) based approach as described above, mathematical models of the underlying physical system, fault propagation models and conventional reliability models have also been used for RUL estimation [5, 26].", "startOffset": 223, "endOffset": 230}, {"referenceID": 25, "context": "Apart from the health index (HI) based approach as described above, mathematical models of the underlying physical system, fault propagation models and conventional reliability models have also been used for RUL estimation [5, 26].", "startOffset": 223, "endOffset": 230}, {"referenceID": 27, "context": "Data-driven models which use readings of sensors carrying degradation or wear information such as vibration in a bearing have been effectively used to build RUL estimation models [28, 29, 37].", "startOffset": 179, "endOffset": 191}, {"referenceID": 28, "context": "Data-driven models which use readings of sensors carrying degradation or wear information such as vibration in a bearing have been effectively used to build RUL estimation models [28, 29, 37].", "startOffset": 179, "endOffset": 191}, {"referenceID": 36, "context": "Data-driven models which use readings of sensors carrying degradation or wear information such as vibration in a bearing have been effectively used to build RUL estimation models [28, 29, 37].", "startOffset": 179, "endOffset": 191}, {"referenceID": 39, "context": "Any new instance is then compared with these trends and the most similar trends are used to estimate the RUL [40].", "startOffset": 109, "endOffset": 113}, {"referenceID": 11, "context": "LSTM networks are recurrent neural network models that have been successfully used for many sequence learning and temporal modeling tasks [12, 2] such as handwriting recognition, speech recognition, sentiment analysis, and customer behavior prediction.", "startOffset": 138, "endOffset": 145}, {"referenceID": 1, "context": "LSTM networks are recurrent neural network models that have been successfully used for many sequence learning and temporal modeling tasks [12, 2] such as handwriting recognition, speech recognition, sentiment analysis, and customer behavior prediction.", "startOffset": 138, "endOffset": 145}, {"referenceID": 7, "context": "A variant of LSTM networks, LSTM encoder-decoder (LSTM-ED) model has been successfully used for sequence-to-sequence learning tasks [8, 34, 4] like machine translation, natural language generation and reconstruction, parsing, and image captioning.", "startOffset": 132, "endOffset": 142}, {"referenceID": 33, "context": "A variant of LSTM networks, LSTM encoder-decoder (LSTM-ED) model has been successfully used for sequence-to-sequence learning tasks [8, 34, 4] like machine translation, natural language generation and reconstruction, parsing, and image captioning.", "startOffset": 132, "endOffset": 142}, {"referenceID": 3, "context": "A variant of LSTM networks, LSTM encoder-decoder (LSTM-ED) model has been successfully used for sequence-to-sequence learning tasks [8, 34, 4] like machine translation, natural language generation and reconstruction, parsing, and image captioning.", "startOffset": 132, "endOffset": 142}, {"referenceID": 20, "context": "LSTM Encoder-decoder based approaches have been proposed for anomaly detection [21, 23].", "startOffset": 79, "endOffset": 87}, {"referenceID": 22, "context": "LSTM Encoder-decoder based approaches have been proposed for anomaly detection [21, 23].", "startOffset": 79, "endOffset": 87}, {"referenceID": 13, "context": "Based on similar ideas, we use Long Short-Term Memory [14] Encoder-Decoder (LSTM-ED) for RUL estimation.", "startOffset": 54, "endOffset": 58}, {"referenceID": 38, "context": "(Note: When multiple modes of normal operation exist, each point can normalized based on the \u03bcj and \u03c3j for that mode of operation, as suggested in [39].", "startOffset": 147, "endOffset": 151}, {"referenceID": 38, "context": "As in [39, 24, 25], we use Principal Components Analysis to obtain derived sensors from the normalized sensor data with reduced linear correlations between them.", "startOffset": 6, "endOffset": 18}, {"referenceID": 23, "context": "As in [39, 24, 25], we use Principal Components Analysis to obtain derived sensors from the normalized sensor data with reduced linear correlations between them.", "startOffset": 6, "endOffset": 18}, {"referenceID": 24, "context": "As in [39, 24, 25], we use Principal Components Analysis to obtain derived sensors from the normalized sensor data with reduced linear correlations between them.", "startOffset": 6, "endOffset": 18}, {"referenceID": 38, "context": "The approach can be easily extended to multiple operating regimes scenario by treating data for each regime separately (similar to [39, 17]), as described in one of our case studies on milling machine dataset in Section 6.", "startOffset": 131, "endOffset": 139}, {"referenceID": 16, "context": "The approach can be easily extended to multiple operating regimes scenario by treating data for each regime separately (similar to [39, 17]), as described in one of our case studies on milling machine dataset in Section 6.", "startOffset": 131, "endOffset": 139}, {"referenceID": 9, "context": "[10, 33, 40, 29, 6]), which assumes the HI at time t for instance u as", "startOffset": 0, "endOffset": 19}, {"referenceID": 32, "context": "[10, 33, 40, 29, 6]), which assumes the HI at time t for instance u as", "startOffset": 0, "endOffset": 19}, {"referenceID": 39, "context": "[10, 33, 40, 29, 6]), which assumes the HI at time t for instance u as", "startOffset": 0, "endOffset": 19}, {"referenceID": 28, "context": "[10, 33, 40, 29, 6]), which assumes the HI at time t for instance u as", "startOffset": 0, "endOffset": 19}, {"referenceID": 5, "context": "[10, 33, 40, 29, 6]), which assumes the HI at time t for instance u as", "startOffset": 0, "endOffset": 19}, {"referenceID": 37, "context": "[38]).", "startOffset": 0, "endOffset": 4}, {"referenceID": 13, "context": "Many variants and extensions to the original LSTM unit as introduced in [14] exist.", "startOffset": 72, "endOffset": 76}, {"referenceID": 41, "context": "We use the one as described in [42].", "startOffset": 31, "endOffset": 35}, {"referenceID": 33, "context": "The encoder and decoder are jointly trained to reconstruct the time-series in reverse order (similar to [34]), i.", "startOffset": 104, "endOffset": 108}, {"referenceID": 38, "context": "RUL ESTIMATION USING HI CURVE MATCHING Similar to [39, 40], the HI curve for a test instance u\u2217 is compared to the HI curves of all the train instances u \u2208 U .", "startOffset": 50, "endOffset": 58}, {"referenceID": 39, "context": "RUL ESTIMATION USING HI CURVE MATCHING Similar to [39, 40], the HI curve for a test instance u\u2217 is compared to the HI curves of all the train instances u \u2208 U .", "startOffset": 50, "endOffset": 58}, {"referenceID": 28, "context": "Similar to [29, 9, 13], we take into account the following scenarios for curve matching based RUL estimation: 1) Varying initial health across instances: The initial health of an instance varies depending on various factors such as the inherent inconsistencies in the manufacturing process.", "startOffset": 11, "endOffset": 22}, {"referenceID": 8, "context": "Similar to [29, 9, 13], we take into account the following scenarios for curve matching based RUL estimation: 1) Varying initial health across instances: The initial health of an instance varies depending on various factors such as the inherent inconsistencies in the manufacturing process.", "startOffset": 11, "endOffset": 22}, {"referenceID": 12, "context": "Similar to [29, 9, 13], we take into account the following scenarios for curve matching based RUL estimation: 1) Varying initial health across instances: The initial health of an instance varies depending on various factors such as the inherent inconsistencies in the manufacturing process.", "startOffset": 11, "endOffset": 22}, {"referenceID": 32, "context": "We evaluate our approach on two publicly available datasets: C-MAPSS Turbofan Engine Dataset [33] and Milling Machine Dataset [1], and a real world dataset from a pulverizer mill.", "startOffset": 93, "endOffset": 97}, {"referenceID": 0, "context": "We evaluate our approach on two publicly available datasets: C-MAPSS Turbofan Engine Dataset [33] and Milling Machine Dataset [1], and a real world dataset from a pulverizer mill.", "startOffset": 126, "endOffset": 129}, {"referenceID": 39, "context": "2 with \u03b2 = 5% as suggested in [40, 39, 29].", "startOffset": 30, "endOffset": 42}, {"referenceID": 38, "context": "2 with \u03b2 = 5% as suggested in [40, 39, 29].", "startOffset": 30, "endOffset": 42}, {"referenceID": 28, "context": "2 with \u03b2 = 5% as suggested in [40, 39, 29].", "startOffset": 30, "endOffset": 42}, {"referenceID": 31, "context": "Several metrics have been proposed for evaluating the performance of prognostics models [32].", "startOffset": 88, "endOffset": 92}, {"referenceID": 32, "context": "We consider the first dataset from the simulated turbofan engine data [33] (NASA Ames Prognostics Data Repository).", "startOffset": 70, "endOffset": 74}, {"referenceID": 32, "context": "We use \u03c41 = 13, \u03c42 = 10 as proposed in [33].", "startOffset": 39, "endOffset": 43}, {"referenceID": 28, "context": "We also provide comparison with RULCLIPPER (RC) [29] which (to the best of our knowledge) has the best performance in terms of timeliness S, accuracy A, MAE, and MSE [31] reported in the literature on the turbofan dataset considered and four other turbofan engine datasets (Note: Unlike RC, we learn the parameters of the model on a validation set rather than test set.", "startOffset": 48, "endOffset": 52}, {"referenceID": 30, "context": "We also provide comparison with RULCLIPPER (RC) [29] which (to the best of our knowledge) has the best performance in terms of timeliness S, accuracy A, MAE, and MSE [31] reported in the literature on the turbofan dataset considered and four other turbofan engine datasets (Note: Unlike RC, we learn the parameters of the model on a validation set rather than test set.", "startOffset": 166, "endOffset": 170}, {"referenceID": 28, "context": "10, see [29] for details).", "startOffset": 8, "endOffset": 12}, {"referenceID": 30, "context": "It is to be noted that the comparison is not exhaustive as a survey of approaches for the turbofan engine dataset since [31] is not available.", "startOffset": 120, "endOffset": 124}, {"referenceID": 8, "context": "We consider mean and standard deviation of each run (9000 values) for the 6 sensors to obtain 2 derived sensors per sensor (similar to [9]).", "startOffset": 135, "endOffset": 138}, {"referenceID": 5, "context": "As can be noted, most of the RUL prediction errors (around 70%) lie in the ranges [-4, 6] and [-3, 1] for material types 1 and 2, respectively.", "startOffset": 82, "endOffset": 89}, {"referenceID": 0, "context": "As can be noted, most of the RUL prediction errors (around 70%) lie in the ranges [-4, 6] and [-3, 1] for material types 1 and 2, respectively.", "startOffset": 94, "endOffset": 101}, {"referenceID": 10, "context": "[11, 35]).", "startOffset": 0, "endOffset": 8}, {"referenceID": 34, "context": "[11, 35]).", "startOffset": 0, "endOffset": 8}, {"referenceID": 24, "context": "[25] presents an unsupervised approach which does not assume a form for the HI curve and uses discrete Bayesian Filter to recursively estimate the health index values, and then use the k-NN classifier to find the most similar offline models.", "startOffset": 0, "endOffset": 4}, {"referenceID": 30, "context": "Recent review of statistical methods is available in [31, 36, 15].", "startOffset": 53, "endOffset": 65}, {"referenceID": 35, "context": "Recent review of statistical methods is available in [31, 36, 15].", "startOffset": 53, "endOffset": 65}, {"referenceID": 14, "context": "Recent review of statistical methods is available in [31, 36, 15].", "startOffset": 53, "endOffset": 65}, {"referenceID": 39, "context": "We use HI Trajectory Similarity Based Prediction (TSBP) for RUL estimation similar to [40] with the key difference being that the model proposed in [40] relies on exponential assumption to learn a regression model for HI estimation, whereas our approach uses LSTM-ED based HI to learn the regression model.", "startOffset": 86, "endOffset": 90}, {"referenceID": 39, "context": "We use HI Trajectory Similarity Based Prediction (TSBP) for RUL estimation similar to [40] with the key difference being that the model proposed in [40] relies on exponential assumption to learn a regression model for HI estimation, whereas our approach uses LSTM-ED based HI to learn the regression model.", "startOffset": 148, "endOffset": 152}, {"referenceID": 28, "context": "Similarly, [29] proposes RULCLIPPER (RC) which to the best of our knowledge has shown the best performance on C-MAPSS when compared to various approaches evaluated using the dataset [31].", "startOffset": 11, "endOffset": 15}, {"referenceID": 30, "context": "Similarly, [29] proposes RULCLIPPER (RC) which to the best of our knowledge has shown the best performance on C-MAPSS when compared to various approaches evaluated using the dataset [31].", "startOffset": 182, "endOffset": 186}, {"referenceID": 38, "context": "Another variant of TSBP [39] directly uses multiple PCA sensors for multi-dimensional curve matching which is used for RUL estimation, whereas we obtain a univariate HI by taking a weighted combination of the PCA sensors learnt", "startOffset": 24, "endOffset": 28}, {"referenceID": 19, "context": "Similarly, [20, 19] learn a composite health index based on the exponential assumption.", "startOffset": 11, "endOffset": 19}, {"referenceID": 18, "context": "Similarly, [20, 19] learn a composite health index based on the exponential assumption.", "startOffset": 11, "endOffset": 19}, {"referenceID": 2, "context": "[3, 27, 13, 18]): very recently, deep Convolutional Neural Networks have been proposed in [3] and shown to outperform regression methods based on Multi-Layer Perceptrons, Support Vector Regression and Relevance Vector Regression to directly estimate the RUL (see Table 4 for performance comparison with our approach on the Turbofan Engine dataset).", "startOffset": 0, "endOffset": 15}, {"referenceID": 26, "context": "[3, 27, 13, 18]): very recently, deep Convolutional Neural Networks have been proposed in [3] and shown to outperform regression methods based on Multi-Layer Perceptrons, Support Vector Regression and Relevance Vector Regression to directly estimate the RUL (see Table 4 for performance comparison with our approach on the Turbofan Engine dataset).", "startOffset": 0, "endOffset": 15}, {"referenceID": 12, "context": "[3, 27, 13, 18]): very recently, deep Convolutional Neural Networks have been proposed in [3] and shown to outperform regression methods based on Multi-Layer Perceptrons, Support Vector Regression and Relevance Vector Regression to directly estimate the RUL (see Table 4 for performance comparison with our approach on the Turbofan Engine dataset).", "startOffset": 0, "endOffset": 15}, {"referenceID": 17, "context": "[3, 27, 13, 18]): very recently, deep Convolutional Neural Networks have been proposed in [3] and shown to outperform regression methods based on Multi-Layer Perceptrons, Support Vector Regression and Relevance Vector Regression to directly estimate the RUL (see Table 4 for performance comparison with our approach on the Turbofan Engine dataset).", "startOffset": 0, "endOffset": 15}, {"referenceID": 2, "context": "[3, 27, 13, 18]): very recently, deep Convolutional Neural Networks have been proposed in [3] and shown to outperform regression methods based on Multi-Layer Perceptrons, Support Vector Regression and Relevance Vector Regression to directly estimate the RUL (see Table 4 for performance comparison with our approach on the Turbofan Engine dataset).", "startOffset": 90, "endOffset": 93}, {"referenceID": 12, "context": "Similarly, Recurrent Neural Network has been used to directly estimate the RUL [13] from the time-series of sensor readings.", "startOffset": 79, "endOffset": 83}, {"referenceID": 26, "context": "To the best of our knowledge, our LSTM Encoder-Decoder based approach performs significantly better than Echo State Network [27] and deep CNN based approaches [3] (refer Table 4).", "startOffset": 124, "endOffset": 128}, {"referenceID": 2, "context": "To the best of our knowledge, our LSTM Encoder-Decoder based approach performs significantly better than Echo State Network [27] and deep CNN based approaches [3] (refer Table 4).", "startOffset": 159, "endOffset": 162}, {"referenceID": 22, "context": "Reconstruction models based on denoising autoencoders [23] have been proposed for novelty/anomaly detection but have not been evaluated for RUL estimation task.", "startOffset": 54, "endOffset": 58}, {"referenceID": 21, "context": "LSTM networks have been used for anomaly/fault detection [22, 7, 41], where deep LSTM networks are used to learn prediction models for the normal time-series.", "startOffset": 57, "endOffset": 68}, {"referenceID": 6, "context": "LSTM networks have been used for anomaly/fault detection [22, 7, 41], where deep LSTM networks are used to learn prediction models for the normal time-series.", "startOffset": 57, "endOffset": 68}, {"referenceID": 40, "context": "LSTM networks have been used for anomaly/fault detection [22, 7, 41], where deep LSTM networks are used to learn prediction models for the normal time-series.", "startOffset": 57, "endOffset": 68}], "year": 2016, "abstractText": "Many approaches for estimation of Remaining Useful Life (RUL) of a machine, using its operational sensor data, make assumptions about how a system degrades or a fault evolves, e.g., exponential degradation. However, in many domains degradation may not follow a pattern. We propose a Long Short Term Memory based Encoder-Decoder (LSTM-ED) scheme to obtain an unsupervised health index (HI) for a system using multi-sensor time-series data. LSTM-ED is trained to reconstruct the time-series corresponding to healthy state of a system. The reconstruction error is used to compute HI which is then used for RUL estimation. We evaluate our approach on publicly available Turbofan Engine and Milling Machine datasets. We also present results on a real-world industry dataset from a pulverizer mill where we find significant correlation between LSTM-ED based HI and maintenance costs.", "creator": "LaTeX with hyperref package"}}}