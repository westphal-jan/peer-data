{"id": "1608.06378", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "23-Aug-2016", "title": "Towards Machine Comprehension of Spoken Content: Initial TOEFL Listening Comprehension Test by Machine", "abstract": "Multimedia or spoken content presents more attractive information than plain text content, but it's more difficult to display on a screen and be selected by a user. As a result, accessing large collections of the former is much more difficult and time-consuming than the latter for humans. It's highly attractive to develop a machine which can automatically understand spoken content and summarize the key information for humans to browse over. In this endeavor, we propose a new task of machine comprehension of spoken content. We define the initial goal as the listening comprehension test of TOEFL, a challenging academic English examination for English learners whose native language is not English. We further propose an Attention-based Multi-hop Recurrent Neural Network (AMRNN) architecture for this task, achieving encouraging results in the initial tests. Initial results also have shown that word-level attention is probably more robust than sentence-level attention for this task with ASR errors.\n\n\n\nThe task is one of the tasks most important for learning about word-level attention. The task consists of following a short list of steps, with a list of steps needed to acquire the ability to see and process sentences.\n1. The first step: Search for and extract words for a word in a given sentence. This task is only able to search for words by comparing each word in a specific sentence or sentence at any time. The process is run on a single thread and must perform all steps by one.\n2. The first step: Search for and extract words for a word in a given sentence. This task is only able to search for words by comparing each word in a specific sentence or sentence at any time. The process is run on a single thread and must perform all steps by one. 2. The first step: Search for and extract words for a word in a given sentence. This task is only able to search for words by comparing each word in a particular sentence or sentence at any time. The process is run on a single thread and must perform all steps by one. 3. The first step: Search for and extract words for a word in a given sentence. This task is only able to search for words by comparing each word in a specific sentence or sentence at any time. The process is run on a single thread and must perform all steps by one. 4. The first step: Search for and extract words for a word in a given sentence. This task is only able to search for words by comparing each word in a specific sentence or sentence at any time. The process is", "histories": [["v1", "Tue, 23 Aug 2016 04:27:41 GMT  (7038kb,D)", "http://arxiv.org/abs/1608.06378v1", "Accepted conference paper: \"The Annual Conference of the International Speech Communication Association (Interspeech), 2016\""]], "COMMENTS": "Accepted conference paper: \"The Annual Conference of the International Speech Communication Association (Interspeech), 2016\"", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["bo-hsiang tseng", "sheng-syun shen", "hung-yi lee", "lin-shan lee"], "accepted": false, "id": "1608.06378"}, "pdf": {"name": "1608.06378.pdf", "metadata": {"source": "CRF", "title": "Towards Machine Comprehension of Spoken Content: Initial TOEFL Listening Comprehension Test by Machine", "authors": ["Bo-Hsiang Tseng", "Sheng-Syun Shen", "Hung-Yi Lee", "Lin-Shan Lee"], "emails": ["r02942037@ntu.edu.tw,", "r03942071@ntu.edu.tw,", "tlkagkb93901106@gmail.com,", "lslee@gate.sinica.edu.tw"], "sections": [{"heading": null, "text": "formation than plain text content, but it\u2019s more difficult to display on a screen and be selected by a user. As a result, accessing large collections of the former is much more difficult and time-consuming than the latter for humans. It\u2019s highly attractive to develop a machine which can automatically understand spoken content and summarize the key information for humans to browse over. In this endeavor, we propose a new task of machine comprehension of spoken content. We define the initial goal as the listening comprehension test of TOEFL, a challenging academic English examination for English learners whose native language is not English. We further propose an Attention-based Multi-hop Recurrent Neural Network (AMRNN) architecture for this task, achieving encouraging results in the initial tests. Initial results also have shown that word-level attention is probably more robust than sentence-level attention for this task with ASR errors. Index Terms: spoken question answering, TOEFL, deep learning, attention model, recurrent neural networks"}, {"heading": "1. Introduction", "text": "With the popularity of shared videos, social networks, online course, etc, the quantity of multimedia or spoken content is growing much faster beyond what human beings can view or listen to. Accessing large collections of multimedia or spoken content is difficult and time-consuming for humans, even if these materials are more attractive for humans than plain text information. Hence, it will be great if the machine can automatically listen to and understand the spoken content, and even visualize the key information for humans. This paper presents an initial attempt towards the above goal: machine comprehension of spoken content. In an initial task, we wish the machine can listen to and understand an audio story, and answer the questions related to that audio content. TOEFL listening comprehension test is for human English learners whose native language is not English. This paper reports how today\u2019s machine can perform with such a test.\nThe listening comprehension task considered here is highly related to Spoken Question Answering (SQA) [1, 2]. In SQA, when the users enter questions in either text or spoken form, the machine needs to find the answer from some audio files. SQA usually worked with ASR transcripts of the spoken content, and used information retrieval (IR) techniques [3] or relied on knowledge bases [4] to find the proper answer. Sibyl [5], a\nThis work was supported by Ministry and Science Technology R.O.C and MediaTek Inc. under Contract MOST 104-2622-8-002-002\nfactoid SQA system, used some IR techniques and utilized several levels of linguistic information to deal with the task. Question Answering in Speech Transcripts (QAST) [6\u20138] has been a well-known evaluation program of SQA for years. However, most previous works on SQA mainly focused on factoid questions like \u201cWhat is name of the highest mountain in Taiwan?\u201d. Sometimes this kind of questions may be correctly answered by simply extracting the key terms from a properly chosen utterance without understanding the given spoken content. More difficult questions that cannot be answered without understanding the whole spoken content seemed rarely dealt with previously.\nWith the fast development of deep learning, neural networks have successfully applied to speech recognition [9\u201311] or NLP tasks [12,13]. A number of recent efforts have explored various ways to understand multimedia in text form [14\u201319]. They incorporated attention mechanisms [17] with Long ShortTerm Memory based networks [20]. In Question Answering field, most of the works focused on understanding text documents [21\u201324]. Even though [25] tried to answer the question related to the movie, they only used the text and image in the movie for that. It seems that none of them have studied and focused on comprehension of spoken content yet."}, {"heading": "2. Task Definition and Contributions", "text": "In this paper, we develop and propose a new task of machine comprehension of spoken content which was never mentioned before to our knowledge. We take TOEFL listening comprehension test as an corpus for this work. TOEFL is an English examination which tests the knowledge and skills of academic English for English learners whose native languages is not English. In this examination, the subjects would first listen to an audio story around five minutes and then answer several question according to that story. The story is related to the college life such as conversation between the student and the profes-\nar X\niv :1\n60 8.\n06 37\n8v 1\n[ cs\n.C L\n] 2\n3 A\nug 2\n01 6\nsor or a lecture in the class. Each question has four choices where only one is correct. An real example in the TOEFL examination is shown in Fig. 1. The upper part is the manual transcription of a small part of the audio story. The questions and four choices are listed too. The correct choice to the question in Fig. 1 is choice A. The questions in TOEFL are not simple even for a human with relatively good knowledge because the question cannot be answered by simply matching the words in the question and in the choices with those in the story, and key information is usually buried by many irrelevant utterances. To answer the questions like \u201cWhy does the student go to professor\u2019s office?\u201d, the listeners have to understand the whole audio story and draw the inferences to answer the question correctly. As a result, this task is believed to be very challenging for the state-of-the-art spoken language understanding technologies.\nWe propose a listening comprehension model for the task defined above, the Attention-based Multi-hop Recurrent Neural Network (AMRNN) framework, and show that this model is able to perform reasonably well for the task. In the proposed approach, the audio of the stories is first transcribed into text by ASR, and the proposed model is developed to process the transcriptions for selecting the correct answer out of 4 choices given the question. The initial experiments showed that the proposed model achieves encouraging scores on the TOEFL listening comprehension test. The attention-mechanism proposed in this paper can be applied on either word or sentence levels. We found that sentence-level attention achieved better results on the manual transcriptions without ASR errors, but word-level attention outperformed the sentence-level on ASR transcriptions with errors."}, {"heading": "3. Proposed Approach", "text": "The overall structure of the proposed model is in Fig 2. The input of model includes the transcriptions of an audio story, a question and four answer choices, all represented as word se-\nquences. The word sequence of the input question is first represented as a question vector VQ in Section 3.1. With the question vector VQ, the attention mechanism is applied to extract the question-related information from the story in Section 3.2. The machine then goes through the story by the attention mechanism several times and obtain an answer selection vector VQn in Section 3.3. This answer selection vector VQn is finally used to evaluate the confidence of each choice in Section 3.4, and the choice with the highest score is taken as the output. All the model parameters in the above procedure are jointly trained with the target where 1 for the correct choice and 0 otherwise."}, {"heading": "3.1. Question Representation", "text": "Fig. 3 (A) shows the procedure of encoding the input question into a vector representation VQ. The input question is a sequence of T words, w1, w2, ..., wT , every word Wi represented in 1-Of-N encoding. A bidirectional Gated Recurrent Unit (GRU) network [26\u201328] takes one word from the input question sequentially at a time. In Fig 3 (A), the hidden layer output of the forward GRU (green rectangle) at time index t is denoted by yf (t), and that of the backward GRU (blue rectangle) is by yb(t). After looking through all the words in the question, the hidden layer output of forward GRU network at the last time index yf (T ), and that of backward GRU network at the first time index yb(1), are concatenated to form the question vector representation VQ, or VQ = [yf (T )\u2016yb(1)]1."}, {"heading": "3.2. Story Attention Module", "text": "Fig. 3 (B) shows the attention mechanism which takes the question vector VQ obtained in Fig. 3 (A) and the story transcriptions as the input to encode the whole story into a story vector representation VS . The story transcription is a very long word sequence with many sentences, so we only show two sentences each with 4 words for simplicity. There is a bidirectional GRU in Fig 3 (B) encoding the whole story into a story vector representation VS . The word vector representation of the t-th word St is constructed by concatenating the hidden layer outputs of forward and backward GRU networks, that is St = [yf (t)\u2016yb(t)]. Then the attention value \u03b1t for each time index t is the cosine similarity between the question vector VQ and the word vector representation St of each word, \u03b1t = St VQ2. With attention values \u03b1t, there can be two different attention mechanisms, word-level and sentence-level, to encode the whole story into the story vector representations VS .\nWord-level Attention: We normalize all the attention values \u03b1t into \u03b1\u2032t such that they sum to one over the whole story. Then all the word vector St from the bidirectional GRU network for every word in the story are weighted with this normalized attention value \u03b1\u2032t and sum to give the story vector, that is VS = \u2211 t \u03b1 \u2032 tSt.\nSentence-level Attention: Sentence-level attention means the model collects the information only at the end of each sentence. Therefore, the normalization is only performed over those words at the end of the sentences to obtain \u03b1\u2032\u2032t . The story vector representation is then VS = \u2211 t=eos \u03b1 \u2032\u2032 t \u2217 St, where only those words at the end of sentences (eos) contribute to the weighted sum. So VS = \u03b1\u2032\u20324 \u2217 S4 + \u03b1\u2032\u20328 \u2217 S8 in the example of the Fig.3\n1The symbol [\u00b7\u2016\u00b7] denotes concatenation of two vectors in this paper. 2The symbol denotes cosine similarity between two vectors."}, {"heading": "3.3. Hopping", "text": "The overall picture of the proposed model is shown in Fig 2, in which Fig. 3 (A) and (B) are component modules (labeled as Fig. 3 (A) and (B)) of the complete proposed model. In the left of Fig. 2, the input question is first converted into a question vector VQ0 by the module in Fig. 3 (A). This VQ0 is used to compute the attention values \u03b1t to obtain the story vector VS1 by the module in Fig. 3 (B). Then VQ0 and VS1 are summed to form a new question vector VQ1 . This process is called the first hop (hop 1) in Fig. 2. The output of the first hop VQ1 can be used to compute the new attention to obtain a new story vector VS1 . This can be considered as the machine going over the story again to re-focus the story with a new question vector. Again, VQ1 and VS1 are summed to form VQ2 (hop 2). After n hops (n should be pre-defined), the output of the last hop VQn is used for the answer selection in the Section 3.4."}, {"heading": "3.4. Answer Selection", "text": "As in the upper part of Fig. 2, the same way previously used to encode the question into VQ in Fig. 3 (A) is used here to encode four choice into choice vector representations VA, VB , VC , VD . Then the cosine similarity between the output of the last hop VQn and the choice vectors are computed, and the choice with highest similarity is chosen."}, {"heading": "4. Experiments", "text": ""}, {"heading": "4.1. Experimental Setup", "text": "\u2022 Dataset Collection: The collected TOEFL dataset included 963 examples in total (717 for training, 124 for validation, 122 for testing). Each example included a story, a question and 4 choices. Besides the audio recording of each story, the manual transcriptions of the story are also available. We used a pydub library [29] to segment the full audio recording into utterances. Each audio recording has 57.9 utterances in average. There are in average 657.7 words in a story, 12.01 words in question and 10.35 words in each choice.\n\u2022 Speech Recognition: We used the CMU speech recognizer - Sphinx [30] to transcribe the audio story. The recognition word error rate (WER) was 34.32%.\n\u2022 Pre-processing: We used a pre-trained 300 dimension glove vector model [31] to obtain the vector representation for each word. Each utterance in the stories, question and each choice can be represented as a fixed length vector by adding the vectors of the all component words. Before training, we pruned the utterances in the story whose vector representation has cosine distance far from the question\u2019s. The percentage of the pruned utterances was determined by the performance of the model on the development set. The vector representations of utterances, questions and choices were only used in this preprocessing stage and the baseline approaches in Section 4.2, not used in the proposed model.\n\u2022 Training Details: The size of the hidden layer for both the forward and backward GRU networks were 128. All the bidirectional GRU networks in the proposed model shared the same set of parameters to avoid overfitting. We used RmsProp [32] with initial learning rate of 1e-5 with momentum 0.9. Dropout rate was 0.2. Batch size was 40. The number of hop was tuned from 1 to 3 by development set."}, {"heading": "4.2. Baselines", "text": "We compared the proposed model with some commonly used simple baselines in [25] and the memory network [17]. \u2022 Choice Length: The most naive baseline is to select the choices based on the number of words in it without listening to the stories and looking at the questions. This included: (i) selecting the longest choice, (ii) selecting the shortest choice or (iii) selecting the choice with the length most different from the rest choices. \u2022 Within-Choices similarity: With the vector representations for the choices in pre-processing of Section 4.1, we computed the cosine distance among the four choices and selected the one which is (i) the most similar to or (ii) the most different from the others. \u2022 Question and Choice Similarity: With the vector representations for the choices and questions in pre-processing of Section 4.1, the choice with the highest cosine similarity to the question is selected. \u2022 Sliding Window [25, 33]: This model try to found a window of W utterances in the story with the maximum similarity to the question. The similarity between a window of utterances and a question was the averaged cosine similarity of the utterances in the window and the question by their glove vector representation. After obtaining the window with the largest cosine similarity to the question, the confidence score of each choice is the average cosine similarity between the utterances in the window and the choice. The choice with the highest score is selected as the answer. \u2022 Memory Network [17]: We implemented the memory network with some modifications for this task to find out if memory network was able to deal it. The original memory network didn\u2019t have the embedding module for the choices, so we used the module for question in the memory network to embed the choices. Besides, in order to have the memory network select the answer out of four choices, instead of outputting a word in its original version, we computed the cosine similarity between the the output of the last hop and the choices to select the closest choice as the answer. We shared all the parameters of embedding layers in the memory network for avoiding overfitting. Without this modification, very poor results were obtained on the testing set. The embedding size of the memory network was set 128, stochastic gradient descent was used as [17] with initial learning rate of 0.01. Batch size was 40. The size of hop was tuned from 1 to 3 by development set."}, {"heading": "4.3. Results", "text": "We used the accuracy (number of question answered correctly / total number of questions) as our evaluation metric. The results are showed in Table 1. We trained the model on the manual transcriptions of the stories, while tested the model on the test-\ning set with both manual transcriptions (column labelled \u201cManual\u201d) and ASR transcriptions (column labelled \u201cASR\u201d). \u2022 Choice Length: Part (a) shows the performance of three models for selecting the answer with the longest, shortest or most different length, ranging from 23% to 35%. \u2022 Within Choices similarity: Part (b) shows the performance of two models for selecting the choice which is most similar to or the most different from the others. The accuracy are 36.09% and 27.87% respectively. \u2022 Question and Choice Similarity: In part (c), selecting the choice which is the most similar to the question only yielded 24.59%, very close to randomly guess. \u2022 Sliding Window: Part (d) for sliding window is the first baseline model considering the transcription of the stories. We tried the window size {1,2,3,5,10,15,20,30} and found the best window size to be 5 on the development set. This implied the useful information for answering the questions is probably within 5 sentences. The performance of 31.15% and 33.61% with and without ASR errors respectively tells how ASR errors affected the results, and the task here is too difficult for this approach to get good results. \u2022 Memory Network: The results of memory network in part (e) shows this task is relatively difficult for it, even though memory network was successful in some other tasks. However, the performance of 39.17% accuracy was clearly better than all approaches mentioned above, and it\u2019s interesting that this result was independent of the ASR errors and the reason is under investigation. The performance was 31% accuracy when we didn\u2019t use the shared embedding layer in the memory network. \u2022 AMRNN model: The results of the proposed model are listed in part (f), respectively for the attention mechanism on word-level and sentence-level. Without the ASR errors, the proposed model with sentence-level attention gave an accuracy as high as 51.67%, and slightly lower for word-level attention. It\u2019s interesting that without ASR errors, sentence-level attention is about 2.5% higher than word-level attention. Very possibly because that getting the information from the whole sentence is more useful than listening carefully at every words, especially for the conceptual and high-level questions in this task. Paying too much attention to every single word may be a bit noisy. On the other hand, the 34.32% ASR errors affected the model on sentence-level more than on word-level. This is very possibly because the incorrectly recognized words may seriously change the meaning of the whole sentences. However, with attention on word-level, when a word is incorrectly recognized, the model may be able to pay attention on other correctly recognized words to compensate for ASR errors and still come up with correct answer."}, {"heading": "4.4. Analysis on a typical example", "text": "Fig 4 shows the visualization of the attention weights obtained for a typical example story in the testing set, with the proposed AMRNN model using word-level or sentence-level attention on manual or ASR transcriptions respectively. The darker the color, the higher the weights. Only a small part of the story is shown where the response of the model made good difference. This story was mainly talking about the thick cloud and some mysteries on Venus. The question for this story is \u201cWhat is a possible origin of Venus\u2019clouds?\u201d and the correct choice is \u201cGases released as a result of volcanic activity\u201d. In the manual transcriptions cases (left half of Fig 4), both models, with wordlevel or sentence-level attention, answered the question right and focused on the core and informative words/sentences to the question. The sentence-level model successfully captured the sentence including \u201c...volcanic eruptions often omits gases.\u201d; while the word-level model captured some important key words like \u201cvolcanic eruptions\u201d, \u201cemit gases\u201d. However, in ASR cases (right half of Fig 4), the ASR errors misled both models to put some attention on some irrelevant words/sentences. The sentence-level model focus on the irrelevant sentence \u201cIn other area, you got canyons...\u201d; while the word-level model focused on some irrelevant words \u201ccanyons\u201d, \u201crift malaise\u201d, but still capture some correct important words like \u201cvolcanic\u201d or \u201ceruptions\u201d to answer correctly. By the darkness of the color, we can observe that the problem caused by ASR errors was more serious for the sentence-level attention when capturing the key concepts needed for the question. This may explain why in part (f) of Table 1 we find degradation caused by ASR errors was less for word-level model than for sentence-level model."}, {"heading": "5. Conclusions", "text": "In this paper we create a new task with the TOEFL corpus. TOEFL is an English examination, where the English learner is asked to listen to a story up to 5 minutes and then answer some corresponding questions. The learner needs to do deduction, logic and summarization for answering the question. We built a model which is able to deal with this challenging task. On manual transcriptions, the proposed model achieved 51.56% accuracy, while the very capable memory network got only 39.17% accuracy. Even on ASR transcriptions with WER of 34.32%, the proposed model still yielded 48.33% accuracy. We also found that although sentence-level attention achieved the best results on the manual transcription, word-level attention outperformed the sentence-level when there were ASR errors."}, {"heading": "6. References", "text": "[1] P. R. C. i Umbert, \u201cFactoid question answering for spoken docu-\nments,\u201d Ph.D. dissertation, Universitat Polite\u0300cnica de Catalunya, 2012.\n[2] P. R. C. i Umbert, J. T. Borra\u0300s, and L. M. Villodre, \u201cSpoken question answering.\u201d\n[3] S.-R. Shiang, H.-y. Lee, and L.-s. Lee, \u201cSpoken question answering using tree-structured conditional random fields and two-layer random walk.\u201d in INTERSPEECH, 2014, pp. 263\u2013267.\n[4] B. Hixon, P. Clark, and H. Hajishirzi, \u201cLearning knowledge graphs for question answering through conversational dialog.\u201d\n[5] P. R. Comas, J. Turmo, and L. Ma\u0300rquez, \u201cSibyl, a factoid question-answering system for spoken documents,\u201d ACM Trans. Inf. Syst., 2012.\n[6] J. Turmo, P. R. Comas, S. Rosset, O. Galibert, N. Moreau, D. Mostefa, P. Rosso, and D. Buscaldi, Multilingual Information Access Evaluation I. Text Retrieval Experiments: 10th Workshop of the Cross-Language Evaluation Forum, CLEF 2009, Corfu, Greece, September 30 - October 2, 2009, Revised Selected Papers. Springer Berlin Heidelberg, 2010, ch. Overview of QAST 2009, pp. 197\u2013211.\n[7] J. Turmo, P. Comas, S. Rosset, L. Lamel, N. Moreau, and D. Mostefa, \u201cOverview of QAST 2008,\u201d in Working Notes for the CLEF 2008 Workshop,, 2008.\n[8] D. Giampiccolo, P. Forner, J. Herrera, A. Pen\u0303as, C. Ayache, C. Forascu, V. Jijkoun, P. Osenova, P. Rocha, B. Sacaleanu, and R. Sutcliffe, Advances in Multilingual and Multimodal Information Retrieval: 8th Workshop of the Cross-Language Evaluation Forum. Springer Berlin Heidelberg, 2008, ch. Overview of the CLEF 2007 Multilingual Question Answering Track, pp. 200\u2013 236.\n[9] G. E. Dahl, T. N. Sainath, and G. E. Hinton, \u201cImproving deep neural networks for lvcsr using rectified linear units and dropout,\u201d in Acoustics, Speech and Signal Processing (ICASSP), 2013 IEEE International Conference on. IEEE, 2013, pp. 8609\u20138613.\n[10] L. Deng, G. Hinton, and B. Kingsbury, \u201cNew types of deep neural network learning for speech recognition and related applications: An overview,\u201d in Acoustics, Speech and Signal Processing (ICASSP), 2013 IEEE International Conference on. IEEE, 2013, pp. 8599\u20138603.\n[11] A. Graves, A.-r. Mohamed, and G. Hinton, \u201cSpeech recognition with deep recurrent neural networks,\u201d in Acoustics, Speech and Signal Processing (ICASSP), 2013 IEEE International Conference on. IEEE, 2013, pp. 6645\u20136649.\n[12] N. Kalchbrenner, E. Grefenstette, and P. Blunsom, \u201cA convolutional neural network for modelling sentences,\u201d arXiv preprint arXiv:1404.2188, 2014.\n[13] R. Collobert, J. Weston, L. Bottou, M. Karlen, K. Kavukcuoglu, and P. Kuksa, \u201cNatural language processing (almost) from scratch,\u201d The Journal of Machine Learning Research, vol. 12, pp. 2493\u20132537, 2011.\n[14] J. Weston, S. Chopra, and A. Bordes, \u201cMemory networks,\u201d arXiv preprint arXiv:1410.3916, 2014.\n[15] K. M. Hermann, T. Kocisky, E. Grefenstette, L. Espeholt, W. Kay, M. Suleyman, and P. Blunsom, \u201cTeaching machines to read and comprehend,\u201d in Advances in Neural Information Processing Systems, 2015, pp. 1684\u20131692.\n[16] A. Bordes, N. Usunier, S. Chopra, and J. Weston, \u201cLargescale simple question answering with memory networks,\u201d arXiv preprint arXiv:1506.02075, 2015.\n[17] S. Sukhbaatar, J. Weston, R. Fergus et al., \u201cEnd-to-end memory networks,\u201d in Advances in Neural Information Processing Systems, 2015, pp. 2431\u20132439.\n[18] A. Kumar, O. Irsoy, J. Su, J. Bradbury, R. English, B. Pierce, P. Ondruska, I. Gulrajani, and R. Socher, \u201cAsk me anything: Dynamic memory networks for natural language processing,\u201d arXiv preprint arXiv:1506.07285, 2015.\n[19] A. M. Rush, S. Chopra, and J. Weston, \u201cA neural attention model for abstractive sentence summarization,\u201d arXiv preprint arXiv:1509.00685, 2015.\n[20] S. Hochreiter and J. Schmidhuber, \u201cLong short-term memory,\u201d Neural computation, vol. 9, no. 8, pp. 1735\u20131780, 1997.\n[21] A. Bordes, S. Chopra, and J. Weston, \u201cQuestion answering with subgraph embeddings,\u201d arXiv preprint arXiv:1406.3676, 2014.\n[22] N. P. Er and I. Cicekli, \u201cA factoid question answering system using answer pattern matching.\u201d in IJCNLP, 2013, pp. 854\u2013858.\n[23] M. Iyyer, J. L. Boyd-Graber, L. M. B. Claudino, R. Socher, and H. Daume\u0301 III, \u201cA neural network for factoid question answering over paragraphs.\u201d in EMNLP, 2014, pp. 633\u2013644.\n[24] A. Fader, L. Zettlemoyer, and O. Etzioni, \u201cOpen question answering over curated and extracted knowledge bases,\u201d in Proceedings of the 20th ACM SIGKDD international conference on Knowledge discovery and data mining. ACM, 2014, pp. 1156\u20131165.\n[25] M. Tapaswi, Y. Zhu, R. Stiefelhagen, A. Torralba, R. Urtasun, and S. Fidler, \u201cMovieqa: Understanding stories in movies through question-answering,\u201d arXiv preprint arXiv:1512.02902, 2015.\n[26] J. Chung, C. Gulcehre, K. Cho, and Y. Bengio, \u201cEmpirical evaluation of gated recurrent neural networks on sequence modeling,\u201d arXiv preprint arXiv:1412.3555, 2014.\n[27] K. Cho, B. van Merrie\u0308nboer, D. Bahdanau, and Y. Bengio, \u201cOn the properties of neural machine translation: Encoder-decoder approaches,\u201d arXiv preprint arXiv:1409.1259, 2014.\n[28] D. Bahdanau, K. Cho, and Y. Bengio, \u201cNeural machine translation by jointly learning to align and translate,\u201d arXiv preprint arXiv:1409.0473, 2014.\n[29] Pydub Library. [Online]. Available: https://github.com/jiaaro/ pydub\n[30] W. Walker, P. Lamere, P. Kwok, B. Raj, R. Singh, E. Gouvea, P. Wolf, and J. Woelfel, \u201cSphinx-4: A flexible open source framework for speech recognition,\u201d 2004.\n[31] J. Pennington, R. Socher, and C. D. Manning, \u201cGlove: Global vectors for word representation.\u201d in EMNLP, vol. 14, 2014, pp. 1532\u20131543.\n[32] T. Tieleman and G. Hinton, \u201cLecture 6.5-rmsprop: Divide the gradient by a running average of its recent magnitude,\u201d COURSERA: Neural Networks for Machine Learning, vol. 4, p. 2, 2012.\n[33] M. Datar, A. Gionis, P. Indyk, and R. Motwani, \u201cMaintaining stream statistics over sliding windows,\u201d SIAM Journal on Computing, vol. 31, no. 6, pp. 1794\u20131813, 2002."}], "references": [{"title": "Factoid question answering for spoken documents", "author": ["P.R.C. i Umbert"], "venue": "Ph.D. dissertation, Universitat Polit\u00e8cnica de Catalunya, 2012.", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2012}, {"title": "Spoken question answering using tree-structured conditional random fields and two-layer random walk.", "author": ["S.-R. Shiang", "H.-y. Lee", "L.-s. Lee"], "venue": "INTERSPEECH,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2014}, {"title": "Sibyl, a factoid question-answering system for spoken documents", "author": ["P.R. Comas", "J. Turmo", "L. M\u00e0rquez"], "venue": "ACM Trans. Inf. Syst., 2012.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2012}, {"title": "Multilingual Information Access Evaluation I. Text Retrieval Experiments: 10th Workshop of the Cross-Language Evaluation Forum, CLEF 2009, Corfu, Greece, September 30 - October 2, 2009, Revised Selected Papers", "author": ["J. Turmo", "P.R. Comas", "S. Rosset", "O. Galibert", "N. Moreau", "D. Mostefa", "P. Rosso", "D. Buscaldi"], "venue": "ch. Overview of QAST", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2010}, {"title": "Overview of QAST 2008", "author": ["J. Turmo", "P. Comas", "S. Rosset", "L. Lamel", "N. Moreau", "D. Mostefa"], "venue": "Working Notes for the CLEF 2008 Workshop,, 2008.", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2008}, {"title": "Advances in Multilingual and Multimodal Information Retrieval: 8th Workshop of the Cross-Language Evaluation Forum", "author": ["D. Giampiccolo", "P. Forner", "J. Herrera", "A. Pe\u00f1as", "C. Ayache", "C. Forascu", "V. Jijkoun", "P. Osenova", "P. Rocha", "B. Sacaleanu", "R. Sutcliffe"], "venue": "ch. Overview of the CLEF 2007 Multilingual Question Answering Track,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2008}, {"title": "Improving deep neural networks for lvcsr using rectified linear units and dropout", "author": ["G.E. Dahl", "T.N. Sainath", "G.E. Hinton"], "venue": "Acoustics, Speech and Signal Processing (ICASSP), 2013 IEEE International Conference on. IEEE, 2013, pp. 8609\u20138613.", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2013}, {"title": "New types of deep neural network learning for speech recognition and related applications: An overview", "author": ["L. Deng", "G. Hinton", "B. Kingsbury"], "venue": "Acoustics, Speech and Signal Processing (ICASSP), 2013 IEEE International Conference on. IEEE, 2013, pp. 8599\u20138603.", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2013}, {"title": "Speech recognition with deep recurrent neural networks", "author": ["A. Graves", "A.-r. Mohamed", "G. Hinton"], "venue": "Acoustics, Speech and Signal Processing (ICASSP), 2013 IEEE International Conference on. IEEE, 2013, pp. 6645\u20136649.", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2013}, {"title": "A convolutional neural network for modelling sentences", "author": ["N. Kalchbrenner", "E. Grefenstette", "P. Blunsom"], "venue": "arXiv preprint arXiv:1404.2188, 2014.", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2014}, {"title": "Natural language processing (almost) from scratch", "author": ["R. Collobert", "J. Weston", "L. Bottou", "M. Karlen", "K. Kavukcuoglu", "P. Kuksa"], "venue": "The Journal of Machine Learning Research, vol. 12, pp. 2493\u20132537, 2011.", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2011}, {"title": "Memory networks", "author": ["J. Weston", "S. Chopra", "A. Bordes"], "venue": "arXiv preprint arXiv:1410.3916, 2014.", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2014}, {"title": "Teaching machines to read and comprehend", "author": ["K.M. Hermann", "T. Kocisky", "E. Grefenstette", "L. Espeholt", "W. Kay", "M. Suleyman", "P. Blunsom"], "venue": "Advances in Neural Information Processing Systems, 2015, pp. 1684\u20131692.", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2015}, {"title": "Largescale simple question answering with memory networks", "author": ["A. Bordes", "N. Usunier", "S. Chopra", "J. Weston"], "venue": "arXiv preprint arXiv:1506.02075, 2015.", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2015}, {"title": "End-to-end memory networks", "author": ["S. Sukhbaatar", "J. Weston", "R. Fergus"], "venue": "Advances in Neural Information Processing Systems, 2015, pp. 2431\u20132439.", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2015}, {"title": "Ask me anything: Dynamic memory networks for natural language processing", "author": ["A. Kumar", "O. Irsoy", "J. Su", "J. Bradbury", "R. English", "B. Pierce", "P. Ondruska", "I. Gulrajani", "R. Socher"], "venue": "arXiv preprint arXiv:1506.07285, 2015.", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2015}, {"title": "A neural attention model for abstractive sentence summarization", "author": ["A.M. Rush", "S. Chopra", "J. Weston"], "venue": "arXiv preprint arXiv:1509.00685, 2015.", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2015}, {"title": "Long short-term memory", "author": ["S. Hochreiter", "J. Schmidhuber"], "venue": "Neural computation, vol. 9, no. 8, pp. 1735\u20131780, 1997.", "citeRegEx": "20", "shortCiteRegEx": null, "year": 1997}, {"title": "Question answering with subgraph embeddings", "author": ["A. Bordes", "S. Chopra", "J. Weston"], "venue": "arXiv preprint arXiv:1406.3676, 2014.", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2014}, {"title": "A factoid question answering system using answer pattern matching.", "author": ["N.P. Er", "I. Cicekli"], "venue": "in IJCNLP,", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2013}, {"title": "Open question answering over curated and extracted knowledge bases", "author": ["A. Fader", "L. Zettlemoyer", "O. Etzioni"], "venue": "Proceedings of the 20th ACM SIGKDD international conference on Knowledge discovery and data mining. ACM, 2014, pp. 1156\u20131165.", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2014}, {"title": "Movieqa: Understanding stories in movies through question-answering", "author": ["M. Tapaswi", "Y. Zhu", "R. Stiefelhagen", "A. Torralba", "R. Urtasun", "S. Fidler"], "venue": "arXiv preprint arXiv:1512.02902, 2015.", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2015}, {"title": "Empirical evaluation of gated recurrent neural networks on sequence modeling", "author": ["J. Chung", "C. Gulcehre", "K. Cho", "Y. Bengio"], "venue": "arXiv preprint arXiv:1412.3555, 2014.", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2014}, {"title": "On the properties of neural machine translation: Encoder-decoder approaches", "author": ["K. Cho", "B. van Merri\u00ebnboer", "D. Bahdanau", "Y. Bengio"], "venue": "arXiv preprint arXiv:1409.1259, 2014.", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2014}, {"title": "Neural machine translation by jointly learning to align and translate", "author": ["D. Bahdanau", "K. Cho", "Y. Bengio"], "venue": "arXiv preprint arXiv:1409.0473, 2014.", "citeRegEx": "28", "shortCiteRegEx": null, "year": 2014}, {"title": "Sphinx-4: A flexible open source framework for speech recognition", "author": ["W. Walker", "P. Lamere", "P. Kwok", "B. Raj", "R. Singh", "E. Gouvea", "P. Wolf", "J. Woelfel"], "venue": "2004.", "citeRegEx": "30", "shortCiteRegEx": null, "year": 2004}, {"title": "Glove: Global vectors for word representation.", "author": ["J. Pennington", "R. Socher", "C.D. Manning"], "venue": "in EMNLP, vol", "citeRegEx": "31", "shortCiteRegEx": "31", "year": 2014}, {"title": "Lecture 6.5-rmsprop: Divide the gradient by a running average of its recent magnitude", "author": ["T. Tieleman", "G. Hinton"], "venue": "COURSERA: Neural Networks for Machine Learning, vol. 4, p. 2, 2012.", "citeRegEx": "32", "shortCiteRegEx": null, "year": 2012}, {"title": "Maintaining stream statistics over sliding windows", "author": ["M. Datar", "A. Gionis", "P. Indyk", "R. Motwani"], "venue": "SIAM Journal on Computing, vol. 31, no. 6, pp. 1794\u20131813, 2002.", "citeRegEx": "33", "shortCiteRegEx": null, "year": 1813}], "referenceMentions": [{"referenceID": 0, "context": "The listening comprehension task considered here is highly related to Spoken Question Answering (SQA) [1, 2].", "startOffset": 102, "endOffset": 108}, {"referenceID": 1, "context": "SQA usually worked with ASR transcripts of the spoken content, and used information retrieval (IR) techniques [3] or relied on knowledge bases [4] to find the proper answer.", "startOffset": 110, "endOffset": 113}, {"referenceID": 2, "context": "Sibyl [5], a", "startOffset": 6, "endOffset": 9}, {"referenceID": 3, "context": "Question Answering in Speech Transcripts (QAST) [6\u20138] has been a well-known evaluation program of SQA for years.", "startOffset": 48, "endOffset": 53}, {"referenceID": 4, "context": "Question Answering in Speech Transcripts (QAST) [6\u20138] has been a well-known evaluation program of SQA for years.", "startOffset": 48, "endOffset": 53}, {"referenceID": 5, "context": "Question Answering in Speech Transcripts (QAST) [6\u20138] has been a well-known evaluation program of SQA for years.", "startOffset": 48, "endOffset": 53}, {"referenceID": 6, "context": "With the fast development of deep learning, neural networks have successfully applied to speech recognition [9\u201311] or NLP tasks [12,13].", "startOffset": 108, "endOffset": 114}, {"referenceID": 7, "context": "With the fast development of deep learning, neural networks have successfully applied to speech recognition [9\u201311] or NLP tasks [12,13].", "startOffset": 108, "endOffset": 114}, {"referenceID": 8, "context": "With the fast development of deep learning, neural networks have successfully applied to speech recognition [9\u201311] or NLP tasks [12,13].", "startOffset": 108, "endOffset": 114}, {"referenceID": 9, "context": "With the fast development of deep learning, neural networks have successfully applied to speech recognition [9\u201311] or NLP tasks [12,13].", "startOffset": 128, "endOffset": 135}, {"referenceID": 10, "context": "With the fast development of deep learning, neural networks have successfully applied to speech recognition [9\u201311] or NLP tasks [12,13].", "startOffset": 128, "endOffset": 135}, {"referenceID": 11, "context": "A number of recent efforts have explored various ways to understand multimedia in text form [14\u201319].", "startOffset": 92, "endOffset": 99}, {"referenceID": 12, "context": "A number of recent efforts have explored various ways to understand multimedia in text form [14\u201319].", "startOffset": 92, "endOffset": 99}, {"referenceID": 13, "context": "A number of recent efforts have explored various ways to understand multimedia in text form [14\u201319].", "startOffset": 92, "endOffset": 99}, {"referenceID": 14, "context": "A number of recent efforts have explored various ways to understand multimedia in text form [14\u201319].", "startOffset": 92, "endOffset": 99}, {"referenceID": 15, "context": "A number of recent efforts have explored various ways to understand multimedia in text form [14\u201319].", "startOffset": 92, "endOffset": 99}, {"referenceID": 16, "context": "A number of recent efforts have explored various ways to understand multimedia in text form [14\u201319].", "startOffset": 92, "endOffset": 99}, {"referenceID": 14, "context": "They incorporated attention mechanisms [17] with Long ShortTerm Memory based networks [20].", "startOffset": 39, "endOffset": 43}, {"referenceID": 17, "context": "They incorporated attention mechanisms [17] with Long ShortTerm Memory based networks [20].", "startOffset": 86, "endOffset": 90}, {"referenceID": 18, "context": "In Question Answering field, most of the works focused on understanding text documents [21\u201324].", "startOffset": 87, "endOffset": 94}, {"referenceID": 19, "context": "In Question Answering field, most of the works focused on understanding text documents [21\u201324].", "startOffset": 87, "endOffset": 94}, {"referenceID": 20, "context": "In Question Answering field, most of the works focused on understanding text documents [21\u201324].", "startOffset": 87, "endOffset": 94}, {"referenceID": 21, "context": "Even though [25] tried to answer the question related to the movie, they only used the text and image in the movie for that.", "startOffset": 12, "endOffset": 16}, {"referenceID": 22, "context": "A bidirectional Gated Recurrent Unit (GRU) network [26\u201328] takes one word from the input question sequentially at a time.", "startOffset": 51, "endOffset": 58}, {"referenceID": 23, "context": "A bidirectional Gated Recurrent Unit (GRU) network [26\u201328] takes one word from the input question sequentially at a time.", "startOffset": 51, "endOffset": 58}, {"referenceID": 24, "context": "A bidirectional Gated Recurrent Unit (GRU) network [26\u201328] takes one word from the input question sequentially at a time.", "startOffset": 51, "endOffset": 58}, {"referenceID": 25, "context": "\u2022 Speech Recognition: We used the CMU speech recognizer - Sphinx [30] to transcribe the audio story.", "startOffset": 65, "endOffset": 69}, {"referenceID": 26, "context": "\u2022 Pre-processing: We used a pre-trained 300 dimension glove vector model [31] to obtain the vector representation for each word.", "startOffset": 73, "endOffset": 77}, {"referenceID": 27, "context": "We used RmsProp [32] with initial learning rate of 1e-5 with momentum 0.", "startOffset": 16, "endOffset": 20}, {"referenceID": 21, "context": "We compared the proposed model with some commonly used simple baselines in [25] and the memory network [17].", "startOffset": 75, "endOffset": 79}, {"referenceID": 14, "context": "We compared the proposed model with some commonly used simple baselines in [25] and the memory network [17].", "startOffset": 103, "endOffset": 107}, {"referenceID": 21, "context": "\u2022 Sliding Window [25, 33]: This model try to found a window of W utterances in the story with the maximum similarity to the question.", "startOffset": 17, "endOffset": 25}, {"referenceID": 28, "context": "\u2022 Sliding Window [25, 33]: This model try to found a window of W utterances in the story with the maximum similarity to the question.", "startOffset": 17, "endOffset": 25}, {"referenceID": 14, "context": "\u2022 Memory Network [17]: We implemented the memory network with some modifications for this task to find out if memory network was able to deal it.", "startOffset": 17, "endOffset": 21}, {"referenceID": 14, "context": "The embedding size of the memory network was set 128, stochastic gradient descent was used as [17] with initial learning rate of 0.", "startOffset": 94, "endOffset": 98}], "year": 2016, "abstractText": "Multimedia or spoken content presents more attractive information than plain text content, but it\u2019s more difficult to display on a screen and be selected by a user. As a result, accessing large collections of the former is much more difficult and time-consuming than the latter for humans. It\u2019s highly attractive to develop a machine which can automatically understand spoken content and summarize the key information for humans to browse over. In this endeavor, we propose a new task of machine comprehension of spoken content. We define the initial goal as the listening comprehension test of TOEFL, a challenging academic English examination for English learners whose native language is not English. We further propose an Attention-based Multi-hop Recurrent Neural Network (AMRNN) architecture for this task, achieving encouraging results in the initial tests. Initial results also have shown that word-level attention is probably more robust than sentence-level attention for this task with ASR errors.", "creator": "LaTeX with hyperref package"}}}