{"id": "1608.06459", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "23-Aug-2016", "title": "Tracking Amendments to Legislation and Other Political Texts with a Novel Minimum-Edit-Distance Algorithm: DocuToads", "abstract": "Political scientists often find themselves tracking amendments to political texts. As different actors weigh in, texts change as they are drafted and redrafted, reflecting political preferences and power. This study provides a novel solution to the prob- lem of detecting amendments to political text based upon minimum edit distances and political affiliations. We used a model-based analysis of presidential results of the political text distribution of the Republican and Democratic parties as it were created. It shows that the median Republican and Democratic parties have the lowest partisan polarization among both parties in the history of the campaign.\n\n\n\nThe analysis, which compared Democratic party and Republican party, shows that the partisan polarization among both parties was greater in the past three years than in the previous three decades. In addition, partisan polarization was also more pronounced in the 2016 presidential campaign than in the same period in the 2012 presidential campaign, when the left and right split. This is an even more pronounced finding in the 2016 presidential campaign, where Republicans were more likely than Democrats to give each other a more negative rating than Democrats on the issue (p = 0.02).\nFinally, the research shows that there were similar partisan polarization patterns across party lines in 2016, where they differed slightly. Both parties were less likely than Democrats to give each other a more positive rating than Republicans on the issue.\nThe next study, which is now in its seventh quarter, will examine a key distinction between the two parties: it has two primary parties and the two other candidates are candidates. It also highlights the stark differences between the two parties when it comes to the nature of the political process, which has been dominated by the parties' political apparatus. This shift can be seen in the evolution of the 2016 presidential campaign, where the parties have the most influence in the political process, and both parties were more ideologically aligned than the other.\nAs the political process continues to evolve, there is little evidence that political parties, with fewer than 10,000 delegates to vote, will have as much influence over the 2016 presidential campaign as a few candidates. For example, the most recent study from the American Political Science Association (APA), published in The American Political Science Association (ASA), found that in 2016 the parties had more than twice as many political consultants in the GOP, but less than 30 percent of the consultants in the Democratic Party. The results of the ASA study suggest that the Republican Party has been more divided on issues of interest in the Democratic Party in recent years than it was in the 1990s, when Democrats were in office.", "histories": [["v1", "Tue, 23 Aug 2016 10:55:23 GMT  (154kb,D)", "http://arxiv.org/abs/1608.06459v1", null]], "reviews": [], "SUBJECTS": "cs.CL cs.CY", "authors": ["henrik hermansson", "james p cross"], "accepted": false, "id": "1608.06459"}, "pdf": {"name": "1608.06459.pdf", "metadata": {"source": "CRF", "title": "Tracking Amendments to Legislation and Other Political Texts with a Novel Minimum-Edit-Distance Algorithm: DocuToads", "authors": ["Henrik Hermansson", "James P. Cross"], "emails": ["(hajh@ifs.ku.dk).", "(james.cross@ucd.ie)."], "sections": [{"heading": null, "text": "Political scientists often find themselves tracking amendments to political texts. As different actors weigh in, texts change as they are drafted and redrafted, reflecting political preferences and power. This study provides a novel solution to the problem of detecting amendments to political text based upon minimum edit distances. We demonstrate the usefulness of two language-insensitive, transparent, and efficient minimum-edit-distance algorithms suited for the task. These algorithms are capable of providing an account of the types (insertions, deletions, substitutions, and transpositions) and substantive amount of amendments made between version of texts. To illustrate the usefulness and efficiency of the approach we replicate two existing studies from the field of legislative studies. Our results demonstrate that minimum edit distance methods can produce superior measures of text amendments to hand-coded efforts in a fraction of the time and resource costs.\n\u2217Post-Doctoral Researcher at the Centre for European Politics, Department of Political Science, University of Copenhagen (hajh@ifs.ku.dk). \u2020Lecturer in European Public Policy, School of Politics and International Relations, University College Dublin (james.cross@ucd.ie).\nar X\niv :1\n60 8.\n06 45\n9v 1\n[ cs\n.C L"}, {"heading": "Introduction", "text": "Political actors often engage in drafting political or legal texts to represent the policies or decision outcomes made within a political system at a given point in time. This is the case for fundamentally important documents like constitutions and international treaties, but also true for secondary legislation, party manifestos, policy statements, and even political speeches. As different actors weigh in, texts are carefully (for the most part!) drafted and redrafted until they reflect actor preferences and their relative influence over policy outcomes. Actors will insert different amendments reflecting their policy positions and what they deem to be feasible given the other actors and institutions in play.1 Analysing how these documents evolve, and how different actors amend documents at different points in the drafting process, can therefore tell us a lot about what actors want and how successful they are in shaping political texts to reflect these demands.\nThe fact that politics is often expressed through the drafting of political texts has encouraged political scientists to track the progression of these texts in order to study a whole host of interesting questions in the field. For instance, one can study principle-agent problems in which agents are delegated with preparing a draft of a political text, and principles have incomplete control over how agents complete this task. The degree to which a text changes between a principle\u2019s version and an agent\u2019s version of a text captures not only of the power of the principal but also of the accuracy with which the agent has succeeded in predicting the preferences of the principal. Examples of this process include political speeches drafted by speechwriters and amended by the politician giving the speech, or legislative proposals drafted by committees and amended in plenary (Ritter and Medhurst, 2004; Vaughn and Villalobos, 2006; Dille, 2000; Schlesinger, 2008).\nCapturing amendments made to a text, especially if broken down by topic, can also be a useful indicator of extent and focus of political censorship (Cross, 2014, 2013; Cross and B\u00f8lstad, 2015). Prominent examples include the altering of websites and news articles by government agencies or editorial staff, or the detection of redaction in legislative records. One can get a strong indication of the types of issues to which censorship is being applied by focusing on what changes between an\n1Essentially what is meant by this is that positions and the documents that represent them are strategic in nature.\nopen and censored version of a text.\nFinally, measuring text amendments is useful in the study of routine decision-making, when the decisions being taken are committed to text. In such cases, differences detected between documents can aids our understanding of how formalized, Weberian bureaucracies work and communicate. Document drafting processes may of course simultaneously reflect power struggles, quality control mechanisms, censorship and routine drafting work. What is clear is that the changes that occur between different versions of a text reflect real-world political and bureaucratic processes that have been the subject of academic interest for a long time. Finding an efficient, transparent and replicable method for tracking text amendment processes is thus a worthwhile undertaking.\nThis study introduces minimum-edit-distance algorithms to the study of the evolution of political texts. These methods have been developed in the fields of bio-informatics and computational linguistics to compare strings of DNA and speech texts respectively. The study builds upon these efforts and adapts minimum edit distances so that they become suitable for the particular aspects of the text-drafting process found in political settings. We demonstrate the usefulness of these measures by comparing their output to existing efforts to track text amendments based upon costly and time consuming hand-coded effort. We demonstrate that our approach provides a transparent and easily replicable measure of text change that is comparable across different political contexts. Our approach produces more detailed and reliable outputs that are unperturbed by human error when compared to hand-coded measurements. We also demonstrate a significant gain in efficiency, as our approach can be applied to a much larger selection of texts than would be feasible with hand coding alone. We show that our method is language-insensitive in the sense that it can trace changes between texts of the same language, irrespective of what that language is, making it uniquely suited for cross-national comparative studies. Our method can also handle different alphabets and thereby accommodate the study of a more diverse set of political systems. Finally, minimum edit distance algorithms can identify exact amendments, and the semantic context of these amendments, allowing for a much more fine-grained analysis than has previously been possible with existing automated methods in the field.\nText-as-data and the data-generating process of legisla-\ntive texts\nAs stated above, political science researchers have long been interested in tracking the evolution of a large diversity of political texts. Such texts include political speeches, party manifestos, legislation, court decisions, international treaties, and articles in the press. However, these different types of text are produced in very different ways, and considering how they are produced is of vital importance when deciding upon the appropriate methods for their analysis. For instance, the manner in which a political speech is drafted and redrafted is likely to be a much more fluid and \u201ccreative\u201d process than the drafting and redrafting of a piece of legislation. From this it naturally follows that different methods will be more or less suitable for analysing different texts, depending on the data-generating process from which a text emerges.\nMinimum edit distances are most appropriately used to compare texts where one expects parts of each text to directly correspond to one another. This is because the algorithms detects similarities between two texts using sequences of words common to both. Such a (text-as-) data structure is likeliest to emerge when political texts are drafted and redrafted, rather than created anew each time. The most obvious case of such a data generating process is in the drafting of legislation, in which each iteration of a legislative proposal will contain sections that remain un-amended and sections that are amended. Another example could be routine bureaucratic forms or communication templates which are re-used time and again with only small alterations. To demonstrate the usefulness of our method, we focus on the legislative drafting process in the European Union and German Bundestag.\nWhile minimum edit distances can reliably show the amount and type of alterations required to change one text into another, researchers must consider whether those changes are substantively meaningful. Very often political scientists think about meaningfulness in terms of impact on society. In contrast to other written or verbal messages, legislative texts (when entering into force) have a unique impact on the real world, establishing the rights and responsibilities of states, citizens,\ncompanies etc.. Any single change in such texts may therefore have substantive consequences for those subject to the law.2\nFurthermore, the legislative drafting process usually consists of definitions of concepts or of the rights and responsibilities of one party to another. Over time, and in order to reduce legal ambiguities, the structure, style, vocabulary and grammar of these definitions become subject to very strong norms and best practices within a polity.3 The formalistic and precision-centered nature of legislative texts means that there are very few, if any, alternative ways of expressing the same legislative message. Precision is indeed one of the guiding principles of legislative drafting. Ambiguities may of course arise by accident, or be unavoidable, but recent research shows that they often represent a conscious attempt by the drafter to refer the interpretation to relevant courts (Wallace, 2012). Even ambiguities are thus most often deliberate.4\nThe singular characteristics of legislative texts imply that political conflict in legislative bodies is almost always focused on the exact wording of laws. The institutional rules involved in the writing of laws, i.e. voting procedures and veto players, further ensures that making changes to legislative texts is difficult, meaning that spurious or non-salient changes are unlikely to be successful.5\nFor texts where \u2018every word matters\u2019, substantively, legally, and politically, the number of words that have been edited between versions is a credible indicator of the degree of substantively important change that has been made. In our view, legislative texts provide the best example of these conditions holding true. For other types of political texts, this is not always the case and we advise users of minimum edit distance algorithms to consider carefully the question of\n2The same is of course also true for international treaties, trade agreements as well as judicial rulings. Minimum edit distances may therefore provide a useful method for the study of the evolution of such texts.\n3As an example, in the European Union these are summarised here: http://ec.europa.eu/governance/ better_regulation/documents/legis_draft_comm_en.pdf and here http://eur-lex.europa.eu/en/ techleg/index.htm\n4In contrast, other political texts such as speeches are not subject to the same norms. These types of rhetorically motivated texts are instead expected to include as much linguistic variation as possible, to engage the listener or reader. Consequently, precision is also much less of a concern.\n5Similar voting procedures regarding the production of texts also exist outside legislatures and other representative bodies. Examples include any texts amended and adopted at general meetings of organisations, including political parties and notably their party manifestos and programs. Wikipedia entries and other texts developed in a similarly decentralised and participant-driven way could also be candidates. Finally, documents created through a process of scientific review such as the often politically controversial Intergovernmental Panel of Climate Change reports show some of these characteristics.\nmeasurement validity when applying these algorithms.6 If it is expected that the ordering of words in a pair of texts is important, that sections of one text will be found in another text, and finally that commonalities and differences between these texts have substantive meaning, then minimum edit distances may well be useful in examining how these texts are related. The key consideration in deciding whether or not to use our method to examine a pair of political texts is whether or not the data generating process from which a pair of texts emerge is amenable to such an analysis. Before describing our method in more detail, we provide an overview of existing approaches to tracking the evolution of political texts found in the literature."}, {"heading": "Existing literature", "text": "Measuring the evolution of political documents has to date proven challenging in terms of replicability and validity of the measures applied to capturing such changes, and the time and resources spent constructing such measures. Ideally, political scientists want exact measures that capture the nature and extent of changes, so that the success of individual actors in affecting outcomes can be examined. Having such measures allows one to link the (strategic) position taking of actors, the institutional context in which such positions are taken, and the final outcome that is achieved, thus providing a comprehensive account of the decision-making process. As a result, huge efforts have been made across a whole set of subfields in political science to create such measures. This literature varies significantly in the methodologies employed, with methods including close manual readings of relevant documents and large-scale quantitative analysis of political texts and speeches. Here we provide a brief overview of examples from this literature, organised by the methodology employed to capture text amendments."}, {"heading": "Manual coding", "text": "To date, probably the most fruitful and convincing attempts to capture the amendments introduced by actors and their success in incorporating these amendments into political texts has involved\n6The commonly cited phrase \u2018garbage in, garbage out\u2019 is particularly pertinent here.\nmajor hand-coding efforts of records pertaining to the political processes of interest.\nIn a comparative politics context, assessing how actors in different legislatures seek to amend and influence political texts has provided important insights into how the process of legislative review can ameliorate agency problems in political systems with multiparty governments (Martin and Vanberg, 2005, 2011, 2004). The authors argued that \u201cgiven the technical nature of most modern legislation, grasping the policy significance of changes to a draft bill by classifying the substantive content and language of such changes requires extensive expertise in the policy areas dealt with by the bill. [...] Any measure based on our perceptions of substantive policy impact is therefore bound to be highly unreliable, especially when applied to a large number of bills across a variety of policy areas\u201d. Instead, they develop \u201ca more objective measure of change, [defined] as the number of article changes made to the draft version of a government bill\u201d (Martin and Vanberg, 2005, p.9). The data used in these studies was collected through hand coding the changes to legislative texts between the initial proposal and the final piece of legislation decided upon, providing a quantitative, objective and reliable account of how legislation evolved and was amended. In the Martin and Vanberg (2011) study, five distinct Parliaments are considered, with a total of 1,300 legislative proposals across these Parliaments examined. The authors convincingly argue that measuring degrees of change between bills and laws is a good way to examine the inner workings of multi-party governments.\nIn an international context, the EU has received the most attention in terms of tracking the evolution of political texts, as it is the most well developed international organisation, and holds significant legislative powers. Tsebelis et al. (2001) were the first to examine the amendment success of different EU institutions in the legislative process. The authors examined nearly 5,000 separate amendments to a selection of 231 examples of EU legislation negotiated under both co-decision (79) and co-operation (152) between 1988 and 1997. They focused on the exact amendments offered by the actors and the degree to which these are adopted or rejected in the final text. The work by Tsebelis et al. (2001) demonstrates that tracking amendments offers a way to objectively measure salient policy developments and the realisation of policy preferences. According to the authors, such tracking is easily quantifiable, non-reliant on subjective experts and undisturbed by\ndimensionality issues. While advantageous, human coding of amendments is however still time and resource consuming and may be constrained by language issues particularly in cross-national studies."}, {"heading": "Quantitative text analysis", "text": "Building upon the efforts of Tsebelis et al. (2001), Franchino and Mariotto (2012) utilised automated text analysis methods (Wordfish (Slapin and Proksch, 2008)) developed for ascertaining (ideological) policy positions from political texts, to compare the bargaining success of the Commission, Council, and Parliament in the conciliation committees of the EU. This study is especially useful for the purposes of studying the effectiveness of automated text analysis methods, as the authors make a significant effort to compare the hand-coding scheme used by Tsebelis et al. (2001) to the automated method they employ. Franchino and Mariotto (2012) demonstrate that Wordfish is capable of producing document-level binary results (i.e. which actor\u2019s position is most reflected in the final text) similar to those of hand-coding efforts for a subset (9/20) of the most clear-cut cases, leaving much room for improvement for automated text analysis methods.\nOne important question that arises when applying automated text analysis to political documents, is whether or not the chosen method is appropriate for the task at hand, given the datagenerating processes under consideration. The Wordfish algorithm designed by Slapin and Proksch (2008) aims to place actors responsible for particular political texts on latent ideological policy dimensions, based upon the word frequencies found in the political texts of interest. An important assumption of such an approach made clear by the authors themselves is that the language used in each document can be reduced to a word frequency distribution and that differences between these distributions represent differences between actors on the latent policy dimension being estimated. When one considers legislative texts, in which language is highly formalised, this is a rather strong assumption, which may go some way to explaining the rather disappointing success rate the algorithm had in replicating the hand-coding efforts of Franchino and Mariotto (2012). As argued in Grimmer\u2019s (2013) review of the state-of-the-art of automated text analysis, one must pay\ncareful attention to the data-generating process associated with the political texts being analysed in order to be sure that the chosen method is appropriate for task. All of that being said, when such algorithms are applied to the types of texts for which Slapin and Proksch (2008) originally intended them, the usefulness of automated text analysis become clear. Slapin & Proksch, and those using their Wordfish method appropriately have been very successful in providing important insights into position taking in different parliamentary contexts (Slapin and Proksch, 2010; Proksch and Slapin, 2009; Proksch et al., 2011).\nIn research fields outside of political science, other automated methods have been employed to complete tasks similar to that of tracking changes to political texts. In particular, minimum edit distance (MED) algorithms have been developed in bio-informatics, computer science, and natural language processing to measure the number of edit operations (insertions, deletions, or substitutions) required to change one string of characters or words into another (Wagner, 1974). They have successfully been applied to problems as diverse as creating accurate spell checkers (Wagner, 1974; Wagner and Fischer, 1974; Wong and Chandra, 1976), assessing differences between different dialects in computational linguistics (Kessler, 1995; Nerbonne and Heeringa, 1997), and assessing genetic alignments in computational biology (Fitch and Margoliash, 1967; Dayhoff and Schwartz, 1978; Henikoff and Henikoff, 1992). The basic structure of the problem of capturing changes between two versions of political text is very similar to the above mentioned applications and minimum edit distance algorithms therefore offer a promising avenue for automated analysis of text amendments.\nHitherto existing minimum edit distance algorithms share one limitation that have hindered their applicability to political texts. They are unable to account for changes involving the transposition of text within a document (commonly called cut-paste changes) wherein one whole section of text is moved from one place to another within a document. Existing MED algorithms record such a change as the entire section being deleted and then rewritten \u201cfrom scratch\u201d in the new location, very much over-inflating the degree of change between documents in terms of their contents and the policy implications thereof.\nIn applications of text similarity techniques where cut-paste type edits are common (for example\nplagiarism detection), two types of solutions are commonly observed. Both however have significant drawbacks. The first solution, referred to as fingerprinting, randomly draws sample words from two texts and infers their similarity based on the similarity of the samples (Hoad and Zobel, 2003). While suitably insensitive to cut-paste reorganisation of text and computationally highly efficient, this technique is rather imprecise and best used to identify possibly similar documents for further similarity checks. The second solution entails the construction of so-called suffix trees, which are essentially indexes of every possible combination of the words in each text. The technique is highly accurate, but constructing suffix trees is computationally very demanding and storing them requires significantly more memory space than storing the texts, making analysis of large bodies of texts impractical (Smyth, 2003, p.39). When analysing large corpora of text, such computational demands imply that scholars are limited by the hardware that they have access to.\nAs cut-paste type operations are common in the drafting of political texts, where the order of particular sections of text matter less than the fact that they are included in the document, a new MED algorithm able to efficiently and accurately discern cut-paste operations without overinflating their semantic and political impact thus has great potential for applications in political science.\nWith this in mind we now introduce the Levenshtein minimum edit distance algorithm, and\nour own minimum edit distance algorithm referred to as DocuToads.7\nLevenshtein Distance\nThe Levenshtein minimum edit distance algorithm is used to assess the differences between two strings of text units, and is calculated as the minimum number of editing operations required to change one string into another (Levenshtein, 1966).8 Three distinct editing operations are allowed by the algorithm, and each has an assigned weight. The allowed editing operations are the deletion of a unit of text (weighted 1), the insertion of a unit of text (weighted 1), or the substitution of\n7DOCUument Transpose Or Add, Delete, Substitute 8For those interested in a more detailed exposition of the Levenshtein distance algorithm we recommend\nManning et al. (2008)\na unit of text (weighted 0 if the unit of text does not change and weighted 2 if it does). The algorithm is formalised as follows. S1(i) represents the word in string X at position i and S2(j) is the word at position j. The minimum edit distance D(i, j) between two strings, X = x1 \u00b7 \u00b7 \u00b7xm and Y = y1 \u00b7 \u00b7 \u00b7 yn is the minimum cost of a series of editing operations required to convert text X into text Y . The minimum edit distance is computed using a dynamic programming approach, which is a method for solving larger problems by considering a larger problem to be the sum of the solutions to a series of sub-problems (Bellman, 1957). A dynamic programming approach allows one to avoid the often high cost associated with recalculating the solution to sub-problems, as such solutions are stored once calculated in a process referred to as memoisation.\nD(i, j) =  D(i\u2212 1, j) + 1, D(i, j \u2212 1) + 1, D(i\u2212 1, j \u2212 1) + { 2; if S1(i) 6= S2(j),\n0; if S1(i) = S2(j).\n(1)\nAs can be seen from the formula, there are three values to be computed at each iteration of the algorithm, and each matrix element mij corresponds to the minimum of these three values. D(i\u2212 1, j) + 1 corresponds to a deletion, D(i, j\u2212 1) + 1 represents an insertion, and D(i\u2212 1, j\u2212 1) represents a substitution. At each iteration, each element in the matrix is calculated one at a time, taking the values from the previously solved sub-problems as inputs into Formula 1 and solving. In this way the larger problem of converting one string into another is broken down into many distinct individual edit operations, with the minimal path being taken in each iteration.\nA second stage of the algorithm allows one to determine the actual edits used to generate the final minimum edit distance score. This is done by starting at position Mmn, finding which of the three previous possible moves was the least costly, and working backwards through the matrix in this manner. The resulting vector of edit operations is referred to as the backtrace and is useful as it allows one to determine the edit alignment that translates string X into string Y .9 This can\n9It should be noted that there can be more than one path through the matrix that delivers the minimum edit distance, so a backtrace is not necessarily unique.\nlater be used to reconstitute the complete edit history of the documents of interest and determine what has been added, removed, or substituted.\nAs alluded to above, one weakness of the Levenshtein distance is that it not very efficient at accounting for large text transpositions, and places a heavy penalisation on such moves. This is problematic for our application, as such transpositions do not generally imply major policy changes, but instead reflect decisions to change the ordering of provisions within a document. In order to avoid the heavy penalisation associated with moving large sections of a text string using the standard Levenshtein distance algorithm, while retaining accuracy and computational efficiency, we have developed a second minimum edit distance algorithm, referred to as DocuToads that can account for such transpositions.\nDocuToads algorithm\nThe new DocuToads algorithm presented here proceeds in two stages, similar to the Levenshtein algorithm.10 In the first stage, the two texts are reduced to a (sparse) matrix, M , in which every cell, mi,j , with a non-zero value indicates that a word in the first text, S1(i), is the same as a word in the second text, S2(j). The exact positive value given to each of those cells when a word match between documents is detected equals the value of the cell above and to the left plus one, mi,j = mi\u22121,j\u22121 + 1 (if i or j = 1 and S1(i) = S2(j), mi,j = 1). Applying this algorithm iteratively over the entire matrix similar to before creates a matrix with some very useful features for comparing the two texts:\n\u2022 Sequences of matching words are represented in the matrix as diagonal sequences of rising\nnumbers (see Table 1(a));\n\u2022 Edit operations are represented by \u201cgaps\u201d of zeroes interrupting these sequences;\n\u2022 Deletions are represented by a column of zeroes (see Table 1(b));\n\u2022 Additions are represented by a row of zeroes (see Table 1(c)); 10The code used in this project can be found here: https://github.com/ajhhermansson/DocuToads\n\u2022 Substitutions are represented by a row and a column of zeroes (see Table 1(d));\n\u2022 Transpositions are represented by sequences of rising numbers that have been \u201cshifted\u201d hor-\nizontally accompanied by columns of zeroes at a corresponding location (see Table 1(e)).\nIt is possible to draw paths from the cell representing the start of both texts, m1,1, to the cell representing the end of both texts, mI,J . Each such path represents a possible edit history, i.e. a way in which S1 could have been transformed into S2. In the case of two identical texts (see Table 1(a)) the path that involves the least edits would simply be to leave the first text as it stands (following the path set by the rising numbers along the matrix diagonal), but it would also be possible to delete all the words and rewrite them again (following a path vertically down from m1,1 to m1,J and then horizontally to mI,J). For any two texts, there thus exists a very large number of potential edit paths, with the edit path that reflects the actual edit procedure used to create a document being the most substantively interesting. The second stage of the algorithm is designed to find the specific edit path from mI,J upwards through the matrix, row by row, to m1,1 that involves the fewest edit operations.\nBefore moving on to describing that second stage, it should be noted that there are limits to which edit paths represent a valid edit history (there is always at least one valid path). First, any edit path that does not pass through every row and every column is invalid, since that implies the full texts have not been considered. Furthermore, edit paths that pass more than one cell per row or column are also invalid as they imply copy-paste (not cut-paste) edit operations. We argue that copy-paste edit operations are not a realistic way of describing the drafting of legislative text, in which pure repetition would be superfluous. If a path, Di,j,p, is described as a three-dimensional vector where indices i and j indicate a position in Mi,j and a step on the path is described as di,j,p and dp(i, j) = mi,j . The above two rules for detecting a valid edit path can then be expressed as (i) Di,j,p is valid when Di(p > 0) \u2286 {1 : I} and Dj(p > 0) \u2286 {1 : J} or (ii) di,j,p is valid when di,j(p > 0) is not in Di(p > 0) or Dj(p > 0). The set of steps di,j that are allowed under the above rules are termed Qi,j .\nFinding the edit path with the fewest edit operations that satisfies the above rules is a problem that is once again solvable using dynamic-programming methods, i.e. by breaking the problem down into the sub-problem(s) of choosing what the next step of the path should be.11 As with the Levenshtein algorithm, this is achieved by assigning different penalties to different steps and having the algorithm choose the step with the lowest associated penalty. These penalties must be assigned in such a way that following the locally lowest penalty produces the least costly overall path. In the DocuToads algorithm, the possible steps dk,l from starting point mi,j to destination mk,l and their respective penalties and conditions are detailed in Table 2. 12\nSetting aside the highest penalty for the moment, the least efficient types of steps are additions and deletions. This is because substitutions, when possible, replace one of each. The conditions under which additions and deletions are allowed never coincide while the conditions for a more efficient substitution are unfulfilled. Transpositions, when a large enough section of text has been moved, also represent a more efficient edit operation than additions and deletions.13 Moving for example 100 words to a different position in a text would either require deleting 100 words and adding 100 words at the new location or one cut-paste operation. These two operations cannot be\n11Another type of solution might be to iterate over all possible paths and choose that which produces the lowest MED, but this would be exponentially more computationally demanding.\n12Different MED algorithms feature different sets of penalties, as illustrated by comparing these penalties to those of the Levenshtein algorithm.\n13Technically, the shortest but still efficient transposition involves moving two words. However, to avoid finding spurious cut-paste operations when a more likely cause is the recurrence of common words such as \u201cthe\u201d, \u201ca\u201d, etc., the shortest transposition length to be considered is set at 5. This can be adjusted as desired, although a trade-off between avoiding finding spurious cut-paste operations and missing short but non-spurious matching sequences exist.\ndirectly compared in terms of efficiency because there is no way to replace a transposition with a substitution. They therefore share the same penalty in the algorithm, but the conditions under which they are possible never coincide. If it is possible to turn one string of text into another without editing it, performing no edit operations is trivially more efficient than performing edit operations and so carries the lowest possible penalty.\nSince we are uninterested in copy-paste operations (because of their assumed irrelevance and non-application in legislative drafting where large repeated sections of text would serve little function) we have to consider that a word in one text can only be matched with a single word in the other text. In particular, the algorithmic rules must prevent steps involving non-edits or transpositions from precluding other longer matching sequences. The highest penalty (corresponding to the last row of Table 2) is therefore given to these operations when ml,k < max(mk), because taking such a step will invariably cause at least one edit operation further down the path. Excluding this rule would mean prioritising shorter matching sequences towards the end of texts above longer sequences earlier in texts, but would still produce edit paths with the same number of edit operations. The transposition conditions a) mi\u22121,l = max(Qp(i\u22121)) and b) |l\u2212j| = min({|Ql(i\u22121, p = mi\u22121,l)\u2212j|}) ensure that only the best transpositions for each row are considered (the closest highest target value).\nAs DocuToads applies these rules from a starting point at mI,J it generates a backtrace through the matrix similar to the Levenshtein algorithm. In practice as both algorithms are applied, we store each step, it\u2019s coordinates in the matrix, the two words from their respective texts, and the type of (non-)edit operation that occurs at each algorithm iteration. These data can subsequently be used t o complete a number of very useful operations that canprovide substantive insight into how texts have evolved, including:\n1. Reconstruction of one text from the other in order to see how a document evolved;\n2. Sorting of edit operations into articles (or paragraphs, sections or any other sub-unit of the\ntexts); for article-level analysis (with the help of regular expressions to detect article breaks)\n3. Extraction of each change (or similarity) and its context within a particular document;\nThese different operations provide huge scope for exploring how draft legislation and other types of political texts evolve, and for demonstrating how different actors have influenced the legislative process. Table 3 demonstrates the format of the backtrace produced by DocuToads.14\nUsers of minimum edit distance algorithms can observe the semantic context of changes by extracting a section of the backtrace immediately around the edit operations of interest, as demonstrated in table 3.15 For well formatted text, it is possible to extract the specific sentence(s) in which changes occurred. It is also possible to draw on word frequency or dictionary methods to weight the changes found therein. The extensive output produced by minimum edit distance algorithms of course also facilitates human reliability checks.\nIt should be well noted that minimum edit distance algorithms can identify the most efficient way of turning one text into another, in terms of the number of needed edit operations. They do\n14The Levenshtein algorithm can produce a backtrace of the same format. 15This type of output is then amenable for further analysis.\nnot and cannot however identify which edit operations that were actually performed by the human editor to affect the observed change. For example, while it may be most efficient to substitute a section of text, the editor may in fact have deleted a section of text and rewritten a new section on a different topic. Transpositions in particular often offer a very efficient way of transforming a text, more efficient than actual human use. The backtrace produced by MED algorithms and a (hypothetical) log of actual changes performed will therefore not match completely. With this caution in mind, we now move on to compare the performance of MED algorithms with other existing methods.\nComparison with existing measures\nIn this section we show that our method can replicate and improve upon the results of hand-coded analyses found in the existing literature. Since legislative texts represent an ideal case for our method and simultaneously have received intensive scholarly attention, we focus on replicating studies from this field. We shall first replicate the coding of substantively important legislative amendments to secondary European law performed by Franchino and Mariotto (2012). We aim to demonstrate that automated methods can perform just as well as and sometimes better than hand-coding efforts, regardless of the chosen algorithm, and that our new DocuToads algorithm outperforms the existing Levenshtein algorithm due to its ability to detect transpositions in legislative texts. We then move on to compare the DocuToads algorithm to a sample of the hand-coding performed by Martin and Vanberg (2005), aiming to replicate their results and to compare the counting of article changes to counting edit operations."}, {"heading": "The Conciliation Committee of the EU", "text": "In the EU, most legislation is enacted through the ordinary legislative procedure (formerly known as the codecision procedure) in which power is shared between the European Commission, which puts forward legislative proposals, and the European Parliament and the Council of Ministers,\nwho can amend those proposals. For legislation to be enacted, majorities of both the Parliament and the Council have to agree on a particular version of the law. If a compromise text cannot be agreed upon after two rounds of negotiation within and between the institutions, the proposal is referred to a Conciliation Committee where representatives of all three institutions negotiate a joint compromise text or the law finally fails (if the joint text is voted down by either the Council or Parliament). This decision-making mechanism has become an important institutional feature of the EU (Farrell and He\u0301ritier, 2004).\nEssentially, the different versions of a piece of legislation put forward by the Council and Parliament reveal overt conflict between the central institutions and the final text drafted by the Conciliation Committee represents the resolution of that conflict. Taking the Council of Minister\u2019s version as a reference document, Franchino and Mariotto (2012) have encoded substantive differences between the three versions of twenty legislative acts that were resolved by Conciliation Committee on an article-by-article basis, using the same methodology and terminology as Tsebelis et al. (2001). All-in-all, the authors categorised 525 substantive amendments in the recitals and articles of the texts and associated them with particular legislative articles.16\nIn order to demonstrate that minimum edit distance measures are capable of replicating these hand-coding efforts, we show the similarity of the Levenshtein and DocuToads minimum edit distances to this hand-coded scheme in two ways. First, we take each document-level dyad (e.g. Council-Parliament, Council-final text, Parliament-final text) as the unit of analysis, resulting in 60 document combinations to analyse (20 dossiers with three document combinations in each). For each such dyad, we add up the number of amendments found by Franchino and Mariotto (2012) and compare what they find to the Levenshtein and DocuToads minimum edit distances between the documents of interest. While the Levenshtein edit distance and human coding are highly correlated, with an R-squared of 0.73 in a simple linear model, DocuToads shows an even higher correlation with the hand-coded measure, comparison yields an R-squared of 0.84. The relationship between the DocuToads edit distance and the Franchino & Mariotto measure is demonstrated in Figure 1.\n16They also coded amendments in the annexes and appendices, but we ignore these as the formatting is too irregular to identify the same subsections as Franchino and Mariotto, since their cleaning of the texts removed subsection names.\nFurthermore, when comparing the two minimum edit distance algorithms, we find that they are strongly correlated with an R-squared of 0.93. This suggests that both algorithms do a good job of replicating the efforts of the hand coders.\nSecond, we add up the amendments on an article-by-article basis.17 We then compare that number of amendments to the article-specific minimum edit distances produced by DocuToads and the Levenshtein algorithm. We do this for all three sets of document dyads. The unit of analysis is thus an article in one of the texts.18 There are in total 2, 328 such articles. Once again, while the Levenshtein minimum edit distance and hand-coding are highly correlated (R-squared of 0.70), DocuToads and hand-coding are more so (R-squared of 0.76) in a simple linear model. The articlelevel relationship between the DocuToads results and hand-coding of amendments is demonstrated\n17The Franchino and Mariotto coding scheme allows for multiple amendments to the same article. 18The reference document is the Council\u2019s version, except in the comparison between the Parliament and\nfinal text, when the parliamentary version is taken as the reference document.\nin Figure 2. Comparison of the two minimum edit distances yielded an R-squared of 0.94. These correlations are a large step forward compared to the performance of the Wordfish algorithm in this setting, which could only provide document-level results correctly indicating in a binary fashion whether the Parliament or Council was closest to the final text in nine out of twenty cases. We emphasise that the poor performance of the Wordfish algorithm is a result of its application to phenomena in which the data generating processes are very different from those for which the method was developed.\nEven though the results presented above are highly encouraging in terms of the validity of minimum edit distances in general and DocuToads in particular for replicating hand-coding of substantive amendments, we shall now further explore the differences that do exist between the measures, as this can inform us about the advantages and disadvantages of using the proposed method. To do so, we focus on type-I (false positive) and type-II (false negative) errors in order to examine when each method under cosideration fails to account for substantive changes to a\nlegislative text. We examine the 100 articles where DocuToads and the hand-coding by Franchino and Mariotto disagree the most, in terms of the fifty largest positive and negative residuals, respectively.19 By studying these 100 legislative articles (and, when necessary, the surrounding context), the DocuToads output, and the hand-coding notes, we classified the following main sources of disagreement:\n\u2022 DocuToads and human coders identify the same changes but place them in adjacent articles\n(39/100 articles);20\n\u2022 Small changes in article language still counted as an amendment by human coders (31/100\narticles);21\n\u2022 DocuToads identifies substantive changes that human coders missed (15/100 articles);22\n\u2022 Large changes in article language counted as only one amendment by human coders (11/100\narticles);23\n\u2022 DocuToads detects substitutions where human coders identify added and removed sections\nof text as two distinct amendments (4/100 articles).24\nThe most common source of disagreement in our sample of articles can be traced primarily to the addition of new articles. In such cases, new text was added between two previously existing articles in the reference document. DocuToads, however, has to record the additions at an index position in the reference text which will correspond to either the preceding or following article. Depending on the edit operations performed inside those two articles, it will be less costly to place the added text at the end of the preceding or beginning of the following article (see Table 1). There is in other words a random component which may cause DocuToads to disagree with human coders\n19We focus on the DocuToads errors given the high correlation between both algorithms, and the fact that it performs best in replicating hand-coding efforts.\n20Removing these articles reduced the residual variance by 29% 21Removing these articles reduced the residual variance by 1% 22Removing these articles reduced the residual variance by 10% 23Removing these articles reduced the residual variance by 25% 24Removing these articles reduced the residual variance by 1%\nwho can implement any single rule on the placement of new articles.25 This constitutes a form of negative serial correlation, a common concern found in time-series analysis. Users of DocuToads are advised to consider this issue if correlating article-level results with other article-level variables assigned by human coders.26 This source of disagreement has no effect on the document-level results.\nThe second and fourth most common sources of disagreement concern the length vis-a-vis substantive importance of individual amendments. The underlying assumption we make when interpreting the DocuToads output is that larger amendments are likely to be more substantively important. This assumption is justified to a large extent by the results presented above, but there are certain language features that still require attention. In particular, negations and altered numbers (setting for example a budget) can cause large semantic changes with small alterations to the text. In such cases, our assumption about the size of an amendment corresponding to its importance is challenged, resulting in a false-negative type-II error or an under-estimation of the substantive importance of a amendment.\nOn the other hand, the amendments coded by Franchino and Mariotto (2012) are binary and thus conceal variation in the importance and size of amendments, which DocuToads arguably captures more accurately. When hand-coders fail to account for the correspondence between the size and importance of an amendment, false-negative type-II errors or an under-estimation of the importance of an amendment can arise. There are in other words arguments both in favour of the automated and human-coded description of these amendments.\nEven though it wasn\u2019t a major driver of the differences between DocuToads results and human coding, another potential source of disagreement could be that DocuToads picks up unsubstantive language changes or spelling errors that human coders ignore. Some of these spelling errors could be the result of the pre-processing of documents necessary to run the automated text analysis. In\n25Franchino and Mariotto (2012) did not indicate where in the reference document an added article belongs, we therefore placed such articles as belonging to the preceding article when implementing their coding system.\n26A potential solution is to use regular expressions to detect and index the end of articles, thereby creating the possibility to place edit operations between articles. This solution was not implemented for this set of documents, as it is a side issue unrelated to the core of the method.\nparticular, care must be taken when converting PDF documents to pure text as machine-reading errors, column and footnote placement, as well as certain copy-protections may interfere. These obstacles can generally be overcome but do require thorough consideration.27 Applying spellchecking software on the texts can sometimes be advantageous to reduce noise.\nThe third most common source of disagreement is that DocuToads identified substantively important changes to the text that human coders missed. This represents another form of type-II error on the part of the human coders. In these cases, our re-reading of the articles confirmed the substantive changes identified by DocuToads. Finally, in four of the articles we examined, articles with a high number of amendments made to them, the main cause of disagreement was that DocuToads recorded changes as substituted text while human coders recognised that the removed text was on a different topic than the added text and that it was more appropriate to record the change as two distinct amendments rather than one substituted section of text. This type of error can be thought of as a type-II error on the part of the DocuToads algorithm, as two substantively important amendments are counted as one resulting in an under-estimation of the true amount of substantive change involved.\nAs mentioned above, the two minimum edit distance algorithms are, as is to be expected, very strongly correlated. Out of the sixty document dyads we examined, there were however four cases for which there was substantial disagreement between the two measures, and which caused the Levenshtein algorithm to disagree more with the hand-coded measure. These four cases involved two documents. The first of these was the Council of Ministers\u2019 version of the 1998 act establishing the EU educational programme Socrates. Specifically, DocuToads recorded 396 and 972 edit operations between the Council version and Parliament version and joint text, respectively, while the Levenshtein algorithm recorded 1,768 and 1,695 edit operations. The second document was the joint text version of the 1995 act on government procurement from third countries, in relation to which the Levenshtein algorithm recorded 1,460 and 1,479 changes while DocuToads recorded 893 and 887 changes, respectively. In these cases the Levenshtein algorithm in other words recorded between 1.7 and 4.4 times more edit operations than DocuToads. Comparison\n27Remember \u2018garbage in, garbage out\u2019.\nof the backtraces of both algorithms reveals that these are cases in which transposition of large sections of text played an important role. For example, the list of possible Socrates actions was moved from Article 2 to Article 3 while the transitional measures were moved further down within Article 5 and preambles concerning the system for transfer of European university credits were reshuffled. The Levenshtein algorithm, unable to correct for transpositions, commits a type-I error by recording these examples as first deleted and later re-added sections of the text, overestimating the substantive importance of these rather cosmetic changes that had been ignored by hand coders. In conclusion, DocuToads is to be preferred in cases where cut-paste operations are common or expected while the Levenshtein algorithm is slightly faster and equivalent in the absence of such operations. In cases where no transpositions but overall high numbers of edit operations are to be expected, the Levenshtein algorithm will also provide a backtrace more closely corresponding to a (hypothetical) log of actual changes performed by the writer.\nIn conclusion to this section, we are very confident in the ability of minimum edit distance algorithms in general and DocuToads in particular to replicate and replace hand coding for the coding of substantive amendments to legislative text. Our results are highly consistent with hand coding, and much more consistent than the automated method employed by Franchino and Mariotto (2012). The differences that do exist are almost as likely to be the result of human error as of the failures of minimum edit distance algorithms. Given the fact that minimum edit distances are transparent and very reliable in how they operate, and that they produce the same result every time without the inter-coder sources of error common to all hand-coding efforts, minimum edit distance algorithms are in many cases superior to hand coding."}, {"heading": "Parliamentary amendments in Germany", "text": "In a series of valuable contributions to the literature on multiparty governments, Martin and Vanberg (Martin and Vanberg, 2005, 2011, 2004) developed a quantitative measure of the amount of change between draft bills proposed by governments and final laws passed by parliamentary bodies. Arguing that each article of a bill represents a distinct aspect of the proposed legislation,\nthe authors counted the number of articles subjected to change, and produced a document-level measure of amendment success. While the authors did not provide us with the original texts they had worked with, we were able to identify and attain 66 out of 148 document pairs from their work on the German Bundestag (Martin and Vanberg, 2005). In a first step towards replicating their measure, we ran the DocuToads algorithm on the document pairs (bill-law) and then sorted the changes into the articles in which they happened (using a regular expression to delineate the articles). We then calculated the number of articles subjected to more than 25 edit operations as a threshold for identifying amended articles to allow for minor language changes that do not represent substantively important amendments.28 The results are presented in Figure 3 and are highly encouraging. A linear regression of the two measures showed a high degree of correlation (R-squared is 0.87) between the automated and hand-coded measure. Close reading of the bills and laws revealed somewhat different standards regarding the way in which other pieces of legislation was referred to; bills tended to cite while laws tended to reference with paragraph numbers. This introduced some error to the DocuToads results which accounted for a large part of the disagreement between the measures and can be understood as a difference in the data-generating processes of the two document types.\nAn interesting aspect of our analysis is that it revealed significant variation in the amount of changes necessary for Martin and Vanberg to code an article as amended. For some document pairs, the average number of amended words needed for a change to be recorded by human coders was as low as three. For other documents, especially those with long articles, that number was in the order of hundreds. This reflects the difficulty for human coders to reliably implement a binary measure of change at the article level, and serves to highlight the usefulness of a continuous and fully replicable automated measure.29 We similarly found that the number of articles changed between bill and law, in contrast to the Franchino and Mariotto (2012) measure which could detect several substantive amendments within each article, was unable to convey the extent of changes an article had been subjected to. Some articles had received amendments in the order of several thousand\n28We optimised the cutoff point to achieve the best fit, but this threshold is easily adjustable. 29Binary on the article level, a count variable on the document level.\nwords but were naturally only coded as a single amendment at the article level. Whether this fact is problematic depends on whether we believe a thousand word amendment is more substantively important, in terms of policy consequences or political capital, than a very short amendment. Figure 4 illustrates the correlation between the number of amended words as captured by DocuToads and number of amended articles as captured by hand coding (R-squared is 0.26).\nThe average document length in this sample was just over 26,000 words and the average time to process one document pair was just under half a minute on a standard desktop computer using DocuToads, highlighting the great gain in efficiency possible compared to hand-coding.30 Parallel processing on several computing cores simultaneously is available and further greatly reduces the time needed.\n30Both minimum edit distances were implemented using the Python programming language. This is a powerful and fast language that is designed to interact well with other applications, features useful modules for processing text and data in all formats, runs on any operating system and is free."}, {"heading": "Conclusions", "text": "In this study we have demonstrated the usefulness of minimum edit distance algorithms for quantifying differences between versions of political texts. We demonstrate that these algorithms are particularly suitable to detecting amendments or changes to legislation. Furthermore, we introduced a new minimum edit distance algorithm which can handle text transpositions, i.e. cut-paste operations, in a way that previously existing MED algorithms could not.\nIn the empirical section of the study we compared the results of the DocuToads algorithm and the previously existing Levenshtein algorithm with the substantive amendments identified and hand-coded by Franchino and Mariotto (2012), who followed the same procedure as Tsebelis et al. (2001). We found a strong correlation between both MED results and the hand-coded measure on the document as well as article level. The two algorithms performed equally well except for the set of cases where transpositions played an important role. In such cases, the suitability of the\nDocuToads algorithm in particular was demonstrated due to its ability to account for transpositions and avoid the type-I errors that the Levenshtein algorithm is vulnerable to. Furthermore, we compared the DocuToads algorithm to the hand-coded scheme employed by Martin and Vanberg (2005, 2011, 2004). Our method proved capable of replicating their measure with a very high degree of accuracy while also providing more detail and overcoming some unavoidable difficulties associated with hand-coded binary measurements. We also demonstrated the speed of the new method. The results presented in this study suggest that our proposed method is capable of capturing changes to political texts in a very fine-grained and replicable manner.\nBearing these strengths in mind, the method naturally also has some limitations. We have highlighted that users must carefully consider first of all the purpose of study and precisely what the size of amendments between text versions signifies in the context of interest. Second, it is supremely important to consider the data-generating process underlying the production of texts. In particular, when applying minimum edit distances, it is important that the texts are redrafted rather than rewritten, and that every word matters, both substantively and politically. Finally we want to emphasize that as with all automated text analysis methods the principle of \u2018garbage in garbage out\u2019 applies. Users should ensure a consistent and error-minimizing pre-processing of documents to achieve best results.\nThe new methodology applied in this study has huge potential to inform us about the manner in which political documents are produced, and the influence that different actors have over their drafting. In situations where the data-generating process resembles an iterative adaptation of a political text, and in which the actor associated with each iteration can be identified, our method can produce a fine-grained measure of the success each actor had in influencing the final document. Applications that come to mind include further investigations of legislative decision making from a comparative politics perspective, drafting of international treaties, routine bureaucratic decisions and the drafting of party manifestos. This study should thus act as a catalyst for further explorations into the usefulness of these methods to investigate the drafting of political texts."}], "references": [{"title": "Dynamic Programming", "author": ["R. Bellman"], "venue": null, "citeRegEx": "Bellman,? \\Q1957\\E", "shortCiteRegEx": "Bellman", "year": 1957}, {"title": "council of ministers of the european union", "author": ["J.P. Cross", "J. B\u00f8lstad"], "venue": "Journal of European Public Policy,", "citeRegEx": "Cross and B\u00f8lstad,? \\Q2015\\E", "shortCiteRegEx": "Cross and B\u00f8lstad", "year": 2015}, {"title": "rupted time series analysis", "author": ["M.O. Dayhoff", "R.M. Schwartz"], "venue": "European Union Politics,", "citeRegEx": "Dayhoff and Schwartz,? \\Q1978\\E", "shortCiteRegEx": "Dayhoff and Schwartz", "year": 1978}, {"title": "Interorganizational Negotiation and Intraorganizational", "author": ["H. Farrell", "A. H\u00e9ritier"], "venue": "at-a-distance measurements. Political Psychology,", "citeRegEx": "Farrell and H\u00e9ritier,? \\Q2004\\E", "shortCiteRegEx": "Farrell and H\u00e9ritier", "year": 2004}, {"title": "Comparative Political Studies", "author": ["Parliament", "Council"], "venue": "Construction of phylogenetic trees", "citeRegEx": "Parliament and Council.,? \\Q1967\\E", "shortCiteRegEx": "Parliament and Council.", "year": 1967}, {"title": "Explaining negotiations in the conciliation committee", "author": ["F. Franchino", "C. Mariotto"], "venue": null, "citeRegEx": "284", "shortCiteRegEx": "284", "year": 2012}, {"title": "Amino acid substitution matrices from protein", "author": ["J.G. Henikoff"], "venue": null, "citeRegEx": "Henikoff and Henikoff,? \\Q1992\\E", "shortCiteRegEx": "Henikoff and Henikoff", "year": 1992}, {"title": "Methods for identifying versioned and plagiarized documents", "author": ["T.C. Hoad", "J. Zobel"], "venue": "Proceedings of the National Academy of Sciences,", "citeRegEx": "Hoad and Zobel,? \\Q2003\\E", "shortCiteRegEx": "Hoad and Zobel", "year": 2003}, {"title": "Computational dialectology in irish gaelic", "author": ["B. Kessler"], "venue": "In Proceedings of the seventh confer-", "citeRegEx": "Kessler,? \\Q1995\\E", "shortCiteRegEx": "Kessler", "year": 1995}, {"title": "Binary codes capable of correcting deletions, insertions and reversals", "author": [], "venue": null, "citeRegEx": "Levenshtein,? \\Q1966\\E", "shortCiteRegEx": "Levenshtein", "year": 1966}, {"title": "Introduction to information retrieval", "author": ["C.D. 10:707. Manning", "P. Raghavan", "H. Sch\u00fctze"], "venue": null, "citeRegEx": "Manning et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Manning et al\\.", "year": 2008}, {"title": "Policing the bargain: Coalition government and parliamen", "author": ["L.W. Cambridge. Martin", "G. Vanberg"], "venue": null, "citeRegEx": "Martin and Vanberg,? \\Q2004\\E", "shortCiteRegEx": "Martin and Vanberg", "year": 2004}, {"title": "Coalition policymaking and legislative review", "author": ["L.W. Martin", "G. Vanberg"], "venue": "tary scrutiny. American Journal of Political Science,", "citeRegEx": "Martin and Vanberg,? \\Q2005\\E", "shortCiteRegEx": "Martin and Vanberg", "year": 2005}, {"title": "Parliaments and coalitions: The role of legislative institu", "author": ["L.W. Martin", "G. Vanberg"], "venue": "Political Science Review,", "citeRegEx": "Martin and Vanberg,? \\Q2011\\E", "shortCiteRegEx": "Martin and Vanberg", "year": 2011}, {"title": "Measuring dialect distance phonetically", "author": ["J. Press. Nerbonne", "W. Heeringa"], "venue": "Proksch, S.-O. and Slapin, J. B", "citeRegEx": "Nerbonne and Heeringa,? \\Q1997\\E", "shortCiteRegEx": "Nerbonne and Heeringa", "year": 1997}, {"title": "Party system dynamics in post-war Japan", "author": ["Proksch", "S.-O", "J.B. Slapin", "M.F. Thies"], "venue": "Journal of Political Science,", "citeRegEx": "Proksch et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Proksch et al\\.", "year": 2011}, {"title": "A quantitative content analysis of electoral pledges", "author": ["K. Ritter", "M.J. Medhurst"], "venue": "Electoral Studies,", "citeRegEx": "Ritter and Medhurst,? \\Q2004\\E", "shortCiteRegEx": "Ritter and Medhurst", "year": 2004}, {"title": "White House ghosts: Presidents and their speechwriters", "author": ["R. Press. Schlesinger"], "venue": "Simon and Schuster. Slapin, J. B. and Proksch, S.-O", "citeRegEx": "Schlesinger,? \\Q2008\\E", "shortCiteRegEx": "Schlesinger", "year": 2008}, {"title": "Look who\u2019s talking: Parliamentary debate in the European", "author": ["J.B. Slapin", "S.O. Proksch"], "venue": null, "citeRegEx": "Slapin and Proksch,? \\Q2010\\E", "shortCiteRegEx": "Slapin and Proksch", "year": 2010}, {"title": "Computing patterns in strings", "author": ["G. Education. Tsebelis", "C.B. Jensen", "A. Kalandrakis", "A. Kreppel"], "venue": "Union. European Union Politics,", "citeRegEx": "Tsebelis et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Tsebelis et al\\.", "year": 2003}, {"title": "The string-to-string correction problem", "author": ["R.A. Wagner", "M.J. Fischer"], "venue": null, "citeRegEx": "Wagner and Fischer,? \\Q1974\\E", "shortCiteRegEx": "Wagner and Fischer", "year": 1974}, {"title": "Bounds for the string editing problem", "author": ["Wong", "C.-K", "A.K. Chandra"], "venue": null, "citeRegEx": "Wong et al\\.,? \\Q1976\\E", "shortCiteRegEx": "Wong et al\\.", "year": 1976}], "referenceMentions": [{"referenceID": 16, "context": "Examples of this process include political speeches drafted by speechwriters and amended by the politician giving the speech, or legislative proposals drafted by committees and amended in plenary (Ritter and Medhurst, 2004; Vaughn and Villalobos, 2006; Dille, 2000; Schlesinger, 2008).", "startOffset": 196, "endOffset": 284}, {"referenceID": 17, "context": "Examples of this process include political speeches drafted by speechwriters and amended by the politician giving the speech, or legislative proposals drafted by committees and amended in plenary (Ritter and Medhurst, 2004; Vaughn and Villalobos, 2006; Dille, 2000; Schlesinger, 2008).", "startOffset": 196, "endOffset": 284}, {"referenceID": 1, "context": "Capturing amendments made to a text, especially if broken down by topic, can also be a useful indicator of extent and focus of political censorship (Cross, 2014, 2013; Cross and B\u00f8lstad, 2015).", "startOffset": 148, "endOffset": 192}, {"referenceID": 11, "context": "In a comparative politics context, assessing how actors in different legislatures seek to amend and influence political texts has provided important insights into how the process of legislative review can ameliorate agency problems in political systems with multiparty governments (Martin and Vanberg, 2005, 2011, 2004). The authors argued that \u201cgiven the technical nature of most modern legislation, grasping the policy significance of changes to a draft bill by classifying the substantive content and language of such changes requires extensive expertise in the policy areas dealt with by the bill. [...] Any measure based on our perceptions of substantive policy impact is therefore bound to be highly unreliable, especially when applied to a large number of bills across a variety of policy areas\u201d. Instead, they develop \u201ca more objective measure of change, [defined] as the number of article changes made to the draft version of a government bill\u201d (Martin and Vanberg, 2005, p.9). The data used in these studies was collected through hand coding the changes to legislative texts between the initial proposal and the final piece of legislation decided upon, providing a quantitative, objective and reliable account of how legislation evolved and was amended. In the Martin and Vanberg (2011) study, five distinct Parliaments are considered, with a total of 1,300 legislative proposals across these Parliaments examined.", "startOffset": 282, "endOffset": 1297}, {"referenceID": 19, "context": "Tsebelis et al. (2001) were the first to examine the amendment success of different EU institutions in the legislative process.", "startOffset": 0, "endOffset": 23}, {"referenceID": 19, "context": "Tsebelis et al. (2001) were the first to examine the amendment success of different EU institutions in the legislative process. The authors examined nearly 5,000 separate amendments to a selection of 231 examples of EU legislation negotiated under both co-decision (79) and co-operation (152) between 1988 and 1997. They focused on the exact amendments offered by the actors and the degree to which these are adopted or rejected in the final text. The work by Tsebelis et al. (2001) demonstrates that tracking amendments offers a way to objectively measure salient policy developments and the realisation of policy preferences.", "startOffset": 0, "endOffset": 483}, {"referenceID": 18, "context": "Building upon the efforts of Tsebelis et al. (2001), Franchino and Mariotto (2012) utilised automated text analysis methods (Wordfish (Slapin and Proksch, 2008)) developed for ascertaining (ideological) policy positions from political texts, to compare the bargaining success of the Commission, Council, and Parliament in the conciliation committees of the EU.", "startOffset": 29, "endOffset": 52}, {"referenceID": 18, "context": "Building upon the efforts of Tsebelis et al. (2001), Franchino and Mariotto (2012) utilised automated text analysis methods (Wordfish (Slapin and Proksch, 2008)) developed for ascertaining (ideological) policy positions from political texts, to compare the bargaining success of the Commission, Council, and Parliament in the conciliation committees of the EU.", "startOffset": 29, "endOffset": 83}, {"referenceID": 18, "context": "(2001), Franchino and Mariotto (2012) utilised automated text analysis methods (Wordfish (Slapin and Proksch, 2008)) developed for ascertaining (ideological) policy positions from political texts, to compare the bargaining success of the Commission, Council, and Parliament in the conciliation committees of the EU. This study is especially useful for the purposes of studying the effectiveness of automated text analysis methods, as the authors make a significant effort to compare the hand-coding scheme used by Tsebelis et al. (2001) to the automated method they employ.", "startOffset": 90, "endOffset": 537}, {"referenceID": 18, "context": "(2001), Franchino and Mariotto (2012) utilised automated text analysis methods (Wordfish (Slapin and Proksch, 2008)) developed for ascertaining (ideological) policy positions from political texts, to compare the bargaining success of the Commission, Council, and Parliament in the conciliation committees of the EU. This study is especially useful for the purposes of studying the effectiveness of automated text analysis methods, as the authors make a significant effort to compare the hand-coding scheme used by Tsebelis et al. (2001) to the automated method they employ. Franchino and Mariotto (2012) demonstrate that Wordfish is capable of producing document-level binary results (i.", "startOffset": 90, "endOffset": 604}, {"referenceID": 18, "context": "(2001), Franchino and Mariotto (2012) utilised automated text analysis methods (Wordfish (Slapin and Proksch, 2008)) developed for ascertaining (ideological) policy positions from political texts, to compare the bargaining success of the Commission, Council, and Parliament in the conciliation committees of the EU. This study is especially useful for the purposes of studying the effectiveness of automated text analysis methods, as the authors make a significant effort to compare the hand-coding scheme used by Tsebelis et al. (2001) to the automated method they employ. Franchino and Mariotto (2012) demonstrate that Wordfish is capable of producing document-level binary results (i.e. which actor\u2019s position is most reflected in the final text) similar to those of hand-coding efforts for a subset (9/20) of the most clear-cut cases, leaving much room for improvement for automated text analysis methods. One important question that arises when applying automated text analysis to political documents, is whether or not the chosen method is appropriate for the task at hand, given the datagenerating processes under consideration. The Wordfish algorithm designed by Slapin and Proksch (2008) aims to place actors responsible for particular political texts on latent ideological policy dimensions, based upon the word frequencies found in the political texts of interest.", "startOffset": 90, "endOffset": 1197}, {"referenceID": 18, "context": "(2001), Franchino and Mariotto (2012) utilised automated text analysis methods (Wordfish (Slapin and Proksch, 2008)) developed for ascertaining (ideological) policy positions from political texts, to compare the bargaining success of the Commission, Council, and Parliament in the conciliation committees of the EU. This study is especially useful for the purposes of studying the effectiveness of automated text analysis methods, as the authors make a significant effort to compare the hand-coding scheme used by Tsebelis et al. (2001) to the automated method they employ. Franchino and Mariotto (2012) demonstrate that Wordfish is capable of producing document-level binary results (i.e. which actor\u2019s position is most reflected in the final text) similar to those of hand-coding efforts for a subset (9/20) of the most clear-cut cases, leaving much room for improvement for automated text analysis methods. One important question that arises when applying automated text analysis to political documents, is whether or not the chosen method is appropriate for the task at hand, given the datagenerating processes under consideration. The Wordfish algorithm designed by Slapin and Proksch (2008) aims to place actors responsible for particular political texts on latent ideological policy dimensions, based upon the word frequencies found in the political texts of interest. An important assumption of such an approach made clear by the authors themselves is that the language used in each document can be reduced to a word frequency distribution and that differences between these distributions represent differences between actors on the latent policy dimension being estimated. When one considers legislative texts, in which language is highly formalised, this is a rather strong assumption, which may go some way to explaining the rather disappointing success rate the algorithm had in replicating the hand-coding efforts of Franchino and Mariotto (2012). As argued in Grimmer\u2019s (2013) review of the state-of-the-art of automated text analysis, one must pay", "startOffset": 90, "endOffset": 1960}, {"referenceID": 18, "context": "(2001), Franchino and Mariotto (2012) utilised automated text analysis methods (Wordfish (Slapin and Proksch, 2008)) developed for ascertaining (ideological) policy positions from political texts, to compare the bargaining success of the Commission, Council, and Parliament in the conciliation committees of the EU. This study is especially useful for the purposes of studying the effectiveness of automated text analysis methods, as the authors make a significant effort to compare the hand-coding scheme used by Tsebelis et al. (2001) to the automated method they employ. Franchino and Mariotto (2012) demonstrate that Wordfish is capable of producing document-level binary results (i.e. which actor\u2019s position is most reflected in the final text) similar to those of hand-coding efforts for a subset (9/20) of the most clear-cut cases, leaving much room for improvement for automated text analysis methods. One important question that arises when applying automated text analysis to political documents, is whether or not the chosen method is appropriate for the task at hand, given the datagenerating processes under consideration. The Wordfish algorithm designed by Slapin and Proksch (2008) aims to place actors responsible for particular political texts on latent ideological policy dimensions, based upon the word frequencies found in the political texts of interest. An important assumption of such an approach made clear by the authors themselves is that the language used in each document can be reduced to a word frequency distribution and that differences between these distributions represent differences between actors on the latent policy dimension being estimated. When one considers legislative texts, in which language is highly formalised, this is a rather strong assumption, which may go some way to explaining the rather disappointing success rate the algorithm had in replicating the hand-coding efforts of Franchino and Mariotto (2012). As argued in Grimmer\u2019s (2013) review of the state-of-the-art of automated text analysis, one must pay", "startOffset": 90, "endOffset": 1991}, {"referenceID": 18, "context": "Slapin & Proksch, and those using their Wordfish method appropriately have been very successful in providing important insights into position taking in different parliamentary contexts (Slapin and Proksch, 2010; Proksch and Slapin, 2009; Proksch et al., 2011).", "startOffset": 185, "endOffset": 259}, {"referenceID": 15, "context": "Slapin & Proksch, and those using their Wordfish method appropriately have been very successful in providing important insights into position taking in different parliamentary contexts (Slapin and Proksch, 2010; Proksch and Slapin, 2009; Proksch et al., 2011).", "startOffset": 185, "endOffset": 259}, {"referenceID": 20, "context": "They have successfully been applied to problems as diverse as creating accurate spell checkers (Wagner, 1974; Wagner and Fischer, 1974; Wong and Chandra, 1976), assessing differences between different dialects in computational linguistics (Kessler, 1995; Nerbonne and Heeringa, 1997), and assessing genetic alignments in computational biology (Fitch and Margoliash, 1967; Dayhoff and Schwartz, 1978; Henikoff and Henikoff, 1992).", "startOffset": 95, "endOffset": 159}, {"referenceID": 8, "context": "They have successfully been applied to problems as diverse as creating accurate spell checkers (Wagner, 1974; Wagner and Fischer, 1974; Wong and Chandra, 1976), assessing differences between different dialects in computational linguistics (Kessler, 1995; Nerbonne and Heeringa, 1997), and assessing genetic alignments in computational biology (Fitch and Margoliash, 1967; Dayhoff and Schwartz, 1978; Henikoff and Henikoff, 1992).", "startOffset": 239, "endOffset": 283}, {"referenceID": 14, "context": "They have successfully been applied to problems as diverse as creating accurate spell checkers (Wagner, 1974; Wagner and Fischer, 1974; Wong and Chandra, 1976), assessing differences between different dialects in computational linguistics (Kessler, 1995; Nerbonne and Heeringa, 1997), and assessing genetic alignments in computational biology (Fitch and Margoliash, 1967; Dayhoff and Schwartz, 1978; Henikoff and Henikoff, 1992).", "startOffset": 239, "endOffset": 283}, {"referenceID": 2, "context": "They have successfully been applied to problems as diverse as creating accurate spell checkers (Wagner, 1974; Wagner and Fischer, 1974; Wong and Chandra, 1976), assessing differences between different dialects in computational linguistics (Kessler, 1995; Nerbonne and Heeringa, 1997), and assessing genetic alignments in computational biology (Fitch and Margoliash, 1967; Dayhoff and Schwartz, 1978; Henikoff and Henikoff, 1992).", "startOffset": 343, "endOffset": 428}, {"referenceID": 6, "context": "They have successfully been applied to problems as diverse as creating accurate spell checkers (Wagner, 1974; Wagner and Fischer, 1974; Wong and Chandra, 1976), assessing differences between different dialects in computational linguistics (Kessler, 1995; Nerbonne and Heeringa, 1997), and assessing genetic alignments in computational biology (Fitch and Margoliash, 1967; Dayhoff and Schwartz, 1978; Henikoff and Henikoff, 1992).", "startOffset": 343, "endOffset": 428}, {"referenceID": 13, "context": "All of that being said, when such algorithms are applied to the types of texts for which Slapin and Proksch (2008) originally intended them, the usefulness of automated text analysis become clear.", "startOffset": 89, "endOffset": 115}, {"referenceID": 7, "context": "The first solution, referred to as fingerprinting, randomly draws sample words from two texts and infers their similarity based on the similarity of the samples (Hoad and Zobel, 2003).", "startOffset": 161, "endOffset": 183}, {"referenceID": 9, "context": "The Levenshtein minimum edit distance algorithm is used to assess the differences between two strings of text units, and is calculated as the minimum number of editing operations required to change one string into another (Levenshtein, 1966).", "startOffset": 222, "endOffset": 241}, {"referenceID": 9, "context": "The Levenshtein minimum edit distance algorithm is used to assess the differences between two strings of text units, and is calculated as the minimum number of editing operations required to change one string into another (Levenshtein, 1966).8 Three distinct editing operations are allowed by the algorithm, and each has an assigned weight. The allowed editing operations are the deletion of a unit of text (weighted 1), the insertion of a unit of text (weighted 1), or the substitution of DOCUument Transpose Or Add, Delete, Substitute For those interested in a more detailed exposition of the Levenshtein distance algorithm we recommend Manning et al. (2008)", "startOffset": 4, "endOffset": 661}, {"referenceID": 0, "context": "The minimum edit distance is computed using a dynamic programming approach, which is a method for solving larger problems by considering a larger problem to be the sum of the solutions to a series of sub-problems (Bellman, 1957).", "startOffset": 213, "endOffset": 228}, {"referenceID": 9, "context": "We aim to demonstrate that automated methods can perform just as well as and sometimes better than hand-coding efforts, regardless of the chosen algorithm, and that our new DocuToads algorithm outperforms the existing Levenshtein algorithm due to its ability to detect transpositions in legislative texts. We then move on to compare the DocuToads algorithm to a sample of the hand-coding performed by Martin and Vanberg (2005), aiming to replicate their results and to compare the counting of article changes to counting edit operations.", "startOffset": 218, "endOffset": 427}, {"referenceID": 3, "context": "This decision-making mechanism has become an important institutional feature of the EU (Farrell and H\u00e9ritier, 2004).", "startOffset": 87, "endOffset": 115}, {"referenceID": 3, "context": "This decision-making mechanism has become an important institutional feature of the EU (Farrell and H\u00e9ritier, 2004). Essentially, the different versions of a piece of legislation put forward by the Council and Parliament reveal overt conflict between the central institutions and the final text drafted by the Conciliation Committee represents the resolution of that conflict. Taking the Council of Minister\u2019s version as a reference document, Franchino and Mariotto (2012) have encoded substantive differences between the three versions of twenty legislative acts that were resolved by Conciliation Committee on an article-by-article basis, using the same methodology and terminology as Tsebelis et al.", "startOffset": 88, "endOffset": 473}, {"referenceID": 3, "context": "This decision-making mechanism has become an important institutional feature of the EU (Farrell and H\u00e9ritier, 2004). Essentially, the different versions of a piece of legislation put forward by the Council and Parliament reveal overt conflict between the central institutions and the final text drafted by the Conciliation Committee represents the resolution of that conflict. Taking the Council of Minister\u2019s version as a reference document, Franchino and Mariotto (2012) have encoded substantive differences between the three versions of twenty legislative acts that were resolved by Conciliation Committee on an article-by-article basis, using the same methodology and terminology as Tsebelis et al. (2001). All-in-all, the authors categorised 525 substantive amendments in the recitals and articles of the texts and associated them with particular legislative articles.", "startOffset": 88, "endOffset": 710}, {"referenceID": 3, "context": "This decision-making mechanism has become an important institutional feature of the EU (Farrell and H\u00e9ritier, 2004). Essentially, the different versions of a piece of legislation put forward by the Council and Parliament reveal overt conflict between the central institutions and the final text drafted by the Conciliation Committee represents the resolution of that conflict. Taking the Council of Minister\u2019s version as a reference document, Franchino and Mariotto (2012) have encoded substantive differences between the three versions of twenty legislative acts that were resolved by Conciliation Committee on an article-by-article basis, using the same methodology and terminology as Tsebelis et al. (2001). All-in-all, the authors categorised 525 substantive amendments in the recitals and articles of the texts and associated them with particular legislative articles.16 In order to demonstrate that minimum edit distance measures are capable of replicating these hand-coding efforts, we show the similarity of the Levenshtein and DocuToads minimum edit distances to this hand-coded scheme in two ways. First, we take each document-level dyad (e.g. Council-Parliament, Council-final text, Parliament-final text) as the unit of analysis, resulting in 60 document combinations to analyse (20 dossiers with three document combinations in each). For each such dyad, we add up the number of amendments found by Franchino and Mariotto (2012) and compare what they find to the Levenshtein and DocuToads minimum edit distances between the documents of interest.", "startOffset": 88, "endOffset": 1441}, {"referenceID": 9, "context": "The Levenshtein algorithm, unable to correct for transpositions, commits a type-I error by recording these examples as first deleted and later re-added sections of the text, overestimating the substantive importance of these rather cosmetic changes that had been ignored by hand coders. In conclusion, DocuToads is to be preferred in cases where cut-paste operations are common or expected while the Levenshtein algorithm is slightly faster and equivalent in the absence of such operations. In cases where no transpositions but overall high numbers of edit operations are to be expected, the Levenshtein algorithm will also provide a backtrace more closely corresponding to a (hypothetical) log of actual changes performed by the writer. In conclusion to this section, we are very confident in the ability of minimum edit distance algorithms in general and DocuToads in particular to replicate and replace hand coding for the coding of substantive amendments to legislative text. Our results are highly consistent with hand coding, and much more consistent than the automated method employed by Franchino and Mariotto (2012). The differences that do exist are almost as likely to be the result of human error as of the failures of minimum edit distance algorithms.", "startOffset": 4, "endOffset": 1125}, {"referenceID": 12, "context": "While the authors did not provide us with the original texts they had worked with, we were able to identify and attain 66 out of 148 document pairs from their work on the German Bundestag (Martin and Vanberg, 2005).", "startOffset": 188, "endOffset": 214}, {"referenceID": 9, "context": "In the empirical section of the study we compared the results of the DocuToads algorithm and the previously existing Levenshtein algorithm with the substantive amendments identified and hand-coded by Franchino and Mariotto (2012), who followed the same procedure as Tsebelis et al.", "startOffset": 117, "endOffset": 230}, {"referenceID": 9, "context": "In the empirical section of the study we compared the results of the DocuToads algorithm and the previously existing Levenshtein algorithm with the substantive amendments identified and hand-coded by Franchino and Mariotto (2012), who followed the same procedure as Tsebelis et al. (2001). We found a strong correlation between both MED results and the hand-coded measure on the document as well as article level.", "startOffset": 117, "endOffset": 289}], "year": 2016, "abstractText": "Political scientists often find themselves tracking amendments to political texts. As different actors weigh in, texts change as they are drafted and redrafted, reflecting political preferences and power. This study provides a novel solution to the problem of detecting amendments to political text based upon minimum edit distances. We demonstrate the usefulness of two language-insensitive, transparent, and efficient minimum-edit-distance algorithms suited for the task. These algorithms are capable of providing an account of the types (insertions, deletions, substitutions, and transpositions) and substantive amount of amendments made between version of texts. To illustrate the usefulness and efficiency of the approach we replicate two existing studies from the field of legislative studies. Our results demonstrate that minimum edit distance methods can produce superior measures of text amendments to hand-coded efforts in a fraction of the time and resource costs. \u2217Post-Doctoral Researcher at the Centre for European Politics, Department of Political Science, University of Copenhagen (hajh@ifs.ku.dk). \u2020Lecturer in European Public Policy, School of Politics and International Relations, University College Dublin (james.cross@ucd.ie). ar X iv :1 60 8. 06 45 9v 1 [ cs .C L ] 2 3 A ug 2 01 6", "creator": "LaTeX with hyperref package"}}}