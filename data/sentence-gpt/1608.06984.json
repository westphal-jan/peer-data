{"id": "1608.06984", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "24-Aug-2016", "title": "Learning an Optimization Algorithm through Human Design Iterations", "abstract": "There is evidence that humans can be more efficient than existing algorithms at searching for good solutions in high-dimensional and non-convex design or control spaces, potentially due to our prior knowledge and learning capability. This work attempts to quantify the search strategy of human beings to enhance a Bayesian optimization (BO) algorithm for an optimal design and control problem. We consider the sequence of human solutions (called a search trajectory) as generated from BO, and propose to recover the algorithmic parameters of BO through maximum likelihood estimation. The method is first verified through simulation studies and then applied to human solutions crowdsourced from a gamified design problem. We learn BO parameters from a player who achieved fast improvement in his/her solutions and show that applying the learned parameters to BO achieves better convergence than using a self-adaptive BO. The proposed method is different from inverse reinforcement learning in that it only requires a good search strategy, rather than near-optimal solutions from humans.\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe algorithm consists of the first steps (by the player's goal) and the second step (by the player's goal).\n\nThe algorithm consists of the first steps (by the player's goal) and the second step (by the player's goal).\nThe algorithm consists of the first steps (by the player's goal).\nThe algorithm consists of the first steps (by the player's goal).\nThe algorithm consists of the first steps (by the player's goal).\nThe algorithm consists of the first steps (by the player's goal).\nThe algorithm consists of the first steps (by the player's goal).\nThe algorithm consists of the first steps (by the player's goal).\nThe algorithm consists of the first steps (by the player's goal).\nThe algorithm consists of the first steps (by the player's goal).\nThe algorithm consists of the first steps (by the player's goal).\nThe algorithm consists of the first steps (by the player's goal).\nThe algorithm consists of the first steps (by the player's goal).\nThe algorithm consists of the first steps (by the player's goal).\nThe algorithm consists of the first steps (by the player's goal).\nThe algorithm consists of the first steps (by the player's goal).\nThe algorithm consists of the first steps (by the player's goal).\nThe algorithm consists of the first steps (by the player's goal).\nThe algorithm consists of the first steps (by the player's", "histories": [["v1", "Wed, 24 Aug 2016 23:22:06 GMT  (1914kb,D)", "http://arxiv.org/abs/1608.06984v1", "28 pages, 7 figures, to be submitted to Journal of Mechanical Design"], ["v2", "Fri, 26 Aug 2016 17:40:37 GMT  (1898kb,D)", "http://arxiv.org/abs/1608.06984v2", "28 pages, 7 figures, to be submitted to Journal of Mechanical Design"], ["v3", "Tue, 6 Dec 2016 17:30:21 GMT  (1758kb,D)", "http://arxiv.org/abs/1608.06984v3", "submitted to Journal of Mechanical Design"], ["v4", "Wed, 26 Apr 2017 18:00:36 GMT  (1861kb,D)", "http://arxiv.org/abs/1608.06984v4", "accepted to Journal of Mechanical Design"]], "COMMENTS": "28 pages, 7 figures, to be submitted to Journal of Mechanical Design", "reviews": [], "SUBJECTS": "cs.LG", "authors": ["thurston sexton", "max yi ren"], "accepted": false, "id": "1608.06984"}, "pdf": {"name": "1608.06984.pdf", "metadata": {"source": "CRF", "title": "Learning Human Search Strategies from a Crowdsourcing Game", "authors": ["Thurston Sexton", "Yi Ren"], "emails": ["tbsexton@asu.edu", "yiren@asu.edu"], "sections": [{"heading": "1 Introduction", "text": "Optimal control and/or design problems often have large variable spaces and highly non-convex objectives and constraints, preventing effective or even tractable searches via existing algorithms. Despite this, human beings\n\u2217tbsexton@asu.edu \u2020yiren@asu.edu\nar X\niv :1\n60 8.\n06 98\n4v 1\n[ cs\n.L G\nhave demonstrated ability at finding good solutions for high-dimensional optimization problems. Examples can be found from crowdsourcing scientific solutions for protein folding [2, 3], RNA synthesis [4, 5], genome sequence alignment [6], robot arm movements [7], to name a few, as well as from our daily achievements in packing luggage, scheduling conference sessions [8], and playing games [1,9,10]. As a particular example, our previous study investigated the value of crowdsourcing optimal design and control problems with nondeterministic polynomial time by comparing the search performance of an anonymous crowd with a Bayesian Optimization (BO) algorithm on an electric vehicle time-trial game [1]. In the game, the player needs to both determine the control policy and the final gear ratio to minimize energy consumption for completing a given track in 36 seconds. We found that the majority of the crowd had worse convergence performance, yet a small group of players identified good solutions earlier than the algorithm (see Fig. 1). While often attributed to the \u201ccreativity\u201d or \u201cintuition\u201d of human beings, we consider such ability as prior knowledge, in the form of algorithmic parameters, that helps to enhance an inherent search algorithm used by human beings to find good solutions. The variability in the prior knowledge for a particular problem leads to different search capabilities of players. The above evidence and hypothesis motivate the question: Can we recover a player\u2019s knowledge and use it to improve an optimization algorithm?\nTo answer this question, we investigate in this paper (1) how algorithmic parameters, when they exist, can be estimated based on demonstrated search trajectories, and (2) whether recovered parameters from a player can be used to improve the convergence of the solver, using the ecoRacer game as a case study. Specifically, we assume that a player\u2019s search trajectory is produced from BO (also called \u201cEfficient Global Optimization\u201d [11]) where each new trial optimizes an expected improvement function learned and adjusted by the player during the search. We make the assumption for two reasons: First, BO has been widely used for non-convex optimization problems with high evaluation costs [11,12], which includes the design of dynamical systems where the evaluation of the objective involves long-term simulation of the system [12,13]. Secondly, it is recently found that BO best resembles human search, which outperforms an extensive set of optimization algorithms in 1D problems [14]. Based on this assumption, we propose a model that calculates the likelihood of BO parameters given the observed search trajectory, and discuss the derivation of the maximum likelihood estimation (MLE) of the BO parameters. We call the process of estimating BO parameters using human search trajectory \u201cInverse BO\u201d (IBO).\nThis paper has two contributions: First, we use simulation studies to show that IBO can successfully recover BO parameters when the search trajectories are created by BO. We also use IBO to learn BO parameters from a human player with fast improvement, and apply the resultant BO to the ecoRacer game. Results show that the BO with learned parameters achieves better convergence than a self-adaptive BO. This indicates that IBO can be used to accelerate a search even when only a good search strategy, rather than good solutions, are observed. Secondly, we elucidate the connection between IBO and Inverse Reinforcement Learning (IRL), and use the connections between the two to explain the current limitations and opportunities for IBO in design crowdsourcing."}, {"heading": "2 Preliminaries", "text": ""}, {"heading": "2.1 Terminologies and notations", "text": "Let an optimization problem be minx\u2208X f(x) where X \u2286 Rp is the solution space, i.e., the joint space for design variables d and control variables \u03b8. A search trajectory withK iterations can be represented by hK :=< XK , fK >, where XK and fK represent the collection of K samples in X and their objective values. h0 :=< X0, f0 > represents an initial exploration set with K0 samples. Human knowledge is represented by algorithmic parameters \u03bb that govern the search behavior: During the search, each new solution xk+1 is determined by hk and \u03bb through maximizing a goodness measure of the solution: xk+1 = argmaxx\u2208XQ(x). We use \u039b to represent the diag(\u03bb) and \u03bb\u0302 (\u039b\u0302) the estimation. In the following, we will introduce two forms of Q corresponding to BO and the max-min distance sampling scheme."}, {"heading": "2.2 Bayesian optimization", "text": "BO is an iterative search algorithm with two major steps in each iteration. Model update: BO first updates a Gaussian Process (GP) model [15] to predict objective values, based on current observations hk and Gaussian parameters \u03bb. Without considering random noise in evaluating the objective, the GP model can be derived as f\u0302(x;hk,\u03bb) = b+ r\nTR\u22121(fk \u2212 b), where b = 1T R\u22121fk 1T R\u221211 , r is a column vector with elements ri = exp ( \u2212(x\u2212 xi)T\u039b(x\u2212 xi) ) for i = 1, \u00b7 \u00b7 \u00b7 , k, R is a symmetric matrix withRij = exp ( \u2212(xi \u2212 xj)T\u039b(xi \u2212 xj)\n) for i, j = 1, \u00b7 \u00b7 \u00b7 , k, and 1 is a column vector with ones. Without prior knowl-\nedge, the MLE of \u03bb for the GP model can be derived by solving\n\u03bb\u0302GP = argmin\u03bb log(\u03c3 k|R|\n1 2 ), (1)\nwhere \u03c32 = (fk \u2212 1b)TR\u22121(fk \u2212 1b)/n is the MLE of the GP variance. Sampling the solution space: The second step is to determine the next sample using the GP model. A common sampling strategy is to pick the new solution in X that maximizes the expected improvement from the current best objective value fmin := min fk (assuming a minimization problem):\nQEI(x;hk,\u03bb) = (fmin\u2212 f\u0302)\u03a6 ( fmin\u2212f\u0302\n\u03c3\n) +\u03c3\u03c6 ( fmin\u2212f\u0302\n\u03c3\n) . Here \u03a6(\u00b7) and \u03c6(\u00b7) are\nthe cumulative distribution function and probability density function of the standard normal distribution, respectively. The new sample is thus obtained by solving\nxk+1 = argmaxx\u2208XQEI(x;hk,\u03bb). (2)\nNote that QEI is non-convex with respect to x and its maximization may involve a nested global optimization routine, such as Genetic Algorithm, DIRECT [16], and BARON [17]. The exploration set h0 that is necessary to initialize the GP model is usually created by Latin Hypercube sampling (LHS, see [11] for details). Both the solution of Eq. 2 and LHS introduce randomness to the BO search trajectory. Fig. 2 demonstrates four iterations of BO in optimizing a 1D function, with the GP model and the expected improvement function updated in each iteration."}, {"heading": "3 Inverse BO", "text": "IBO estimates \u03bb, along with the size of the initial exploration set K0, given the trajectory hK . To do so, we minimize a cost function consisting of the exploration cost of h0, denoted as LINI , and the BO cost of the rest of hK , denoted as LBO. LINI := \u2212 logDp(X0) where p(X0) is the joint probability of the exploration set and D := |X | is the size of the solution space. And LBO := \u2212 logDp(hK \u2212h0|h0) = \u2212 \u2211K\u22121 k=0 logDp(xk+1|hk) where p(xk+1|hk) is the density for choosing xk+1. Here log(\u00b7) stands for natural logarithm. The derivation of LINI and LBO are as follows: To calculate LINI , we assume that each new sample during the exploration phase, xi for i = 1, \u00b7 \u00b7 \u00b7 ,K0, tends to maximize its minimum Euclidean distance d(xi,X<i) to previous samples X<i, this is referred to as the max-min sampling scheme in the sequel. Let the joint probability of the exploration set be p(X0) = p(x1)p(x2|x1) \u00b7 \u00b7 \u00b7 p(xK0 |X<K0) and each conditional probability follow a Boltzmann distribution: p(xi|X<i) = exp (\u03b1INId(xi,X<i)) /ZINI(xi, \u03b1INI). Here\nthe scalar \u03b1INI represents how strictly each sample from X0 follows the maxmin sampling scheme, and ZINI(xi, \u03b1INI) = \u222b x\u2208X exp (\u03b1INId(x,X<i)) dx is\na partition function that ensures that \u222b X p(xi|X<i)dx = 1. Note that the first sample in the exploration set is considered to be uniformly drawn, and thus only contributes a constant to the cost.\nTo calculate LBO, the conditional probability density of sampling x \u2208 X based on current hk can be similarly modeled as a Boltzmann distribution:\np(x|hk) = exp (\u03b1BOQEI(x;hk,\u03bb)) /ZBO(hk,\u03bb, \u03b1BO), (3)\nwhere ZBO(hk,\u03bb, \u03b1BO) = \u222b x\u2208X exp (\u03b1BOQEI(x;hk,\u03bb)) dx is also a partition function. The parameter \u03b1BO serves for a similar purpose as \u03b1INI . For simplicity, we define l\u0303i := \u2212 log (Dp(xi|X<i)) and lk := \u2212 log (Dp(x|hk)), so that LINI = \u2211K0 i=1 l\u0303i and LBO = \u2211K\u22121 k=0 lk. A lower value of l\u0303 or l represents higher probability density of the current sample to be drawn using INI or BO, respectively, and a zero value indicates that the sample can be considered as uniformly drawn. The IBO solves the following problem\nmin \u03b1INI ,\u03b1BO,\u03bb,K0 L := LINI + LBO (4)\nto derive \u03bb\u0302. Note that to find the optimal K0 for any given \u03b1INI , \u03b1BO, and \u03bb, one can first calculate the optimal l\u0303i and lk for i, k = 2, \u00b7 \u00b7 \u00b7 ,K, with respect to \u03b1INI , \u03b1BO, and \u03bb, and then scan K0 = 2, \u00b7 \u00b7 \u00b7 ,K to find the lowest value of LINI + LBO. The scan starts at K0 = 2 because it is not meaningful to initialize BO with a single sample.\n4 Numerical Integration for ZBO\nThe calculation of each l requires an approximation of ZBO(hk,\u03bb, \u03b1BO). Note that QEI(x;hk,\u03bb) is usually a highly non-convex function with respect to x, with function values dropping significantly around local maxima. See Fig.2 for example. Thus we propose to approximate ZBO with importance sampling using a customized proposal density function that combines a uniform distribution with density p(x) = 1/D and a multivariate normal distribution with density q(x) = ( \u221a 2\u03c0\u03c3pI )\n\u22121 exp(\u2212||x\u2212\u00b5||2/2\u03c32I ), where \u03c3I and \u00b5 are parameters of q(x). The uniform distribution is used to sample over X , while the normal distribution helps to improve the approximation by capturing the potential peak at the current sample xk+1. Thus we set \u00b5 := xk+1. Let x u i \u2208 U for i = 1, ..., I and xnj \u2208 N for j = 1, ..., J be\nsamples from the uniform and the normal distributions, respectively. The approximation Z\u0302BO can be calculated by\nZ\u0302BO := \u2211 U DQEI(x u i ) I (1 +Dq(xui )) + \u2211 N\nDQEI(x n j ) J ( 1 +Dq(xnj ) ) , (5)\nwith arguments of QEI omitted for simplicity. The derivation of Eq. (5) is deferred to the appendix. Note that this approximation works under the assumption that \u222b x\u2208X q(x)dx \u2248 1, which is plausible as the normal distribution is designed to have a narrow shape to match the local peak at xk+1. In this paper, the shape of this normal distribution is set by \u03c3I = 0.01 universally. While the setting of \u03c3I affects the variance of the approximation, we found this setting performs well in practice. For ZINI , since the minimum Euclidean distance function in a high dimensional space with limited samples is a relatively smooth function, we use Monte Carlo sampling for its approximation."}, {"heading": "5 Simulation studies", "text": "We use a simulation study to show that (1) for a given search trajectory, IBO can correctly identify the true \u03bb except when the trajectory resembles a random search, and (2) learning from others (i.e., estimating \u03bb through IBO of an observed effective search trajectory) can lead to better BO convergence than self-learning (i.e., updating \u03bb\u0302 by maximizing the likelihood of the observations)."}, {"heading": "5.1 IBO performance", "text": ""}, {"heading": "5.1.1 Simulation settings and results", "text": "The simulation study is detailed as follows: We apply BO to a 30-dimensional Rosenbrock function constrained by X := [\u22122, 2]30. To initialize BO, we use LHS to draw 10 samples from X . BO terminates when the expected improvement for the next iteration is less than 10\u22123. At each iteration, the expected improvement is maximized using a multi-start gradient descent algorithm [18] with 100 LHS initial guesses. A set of BO parameters, \u039b = 0.01I, 0.1I, 1.0I, 10.0I, are used to perform the search, where I is the identity matrix. For each of the four settings, 30 independent trials are recorded.\nFor each BO setting \u039b, each candidate estimator \u039b\u0302, and each trajectory of length K = 5, ..., 20, we solve Eq. (4) using a grid search with G\u03b1BO :=\n{0.01, 0.1, 1.0, 10.0} and GK0 := {2, \u00b7 \u00b7 \u00b7 ,K}. We fix \u03b1INI to 1.0 and 10.0, and will discuss its influence to the estimation. Fig. 3 presents the resulting minimal L for all four cases and under all guesses. Each curve in each subplot shows how the minimal L (with respect to \u03b1BO and K0) changes as the search continues. The means and standard deviations of L are calculated using the 30 trials. ZINI is approximated using a sample size of 10, 000. In ZBO, samples from the normal and the uniform distributions are of equal sizes (I = J = 5, 000)."}, {"heading": "5.1.2 Analysis of the results", "text": "To understand the results from Fig. 3, we shall first discuss the properties of the costs l and l\u0303.\nProperties of l and l\u0303 From Sec. 4, the unbiased estimation of l(x, \u03b1BO) through importance sampling is:\nl\u0302(x, \u03b1BO) = \u2212 log exp(\u03b1BOQEI(x))\nZ\u0302BO/D . (6)\nl\u0302(x, \u03b1BO) has the following properties. Property 1: \u03b1BO = 0 leads l\u0302(x, 0) = 0, indicating that x is uniformly sampled. One can see that the optimal cost of LBO is non-positive, as one can always achieve LBO = 0 by considering samples to be uniformly drawn. Property 2: When the expected improvement function is constant almost everywhere, i.e., Pr(QEI(x) = C) = 1, we have Pr(l\u0302(x, \u03b1BO) = 0) = 1. Property 3: Notice that 1 + Dq(xi) \u2248 1 for xi \u2208 U due to the small \u03c3I (see Sec. 4), and exp(\u03b1BOQEI(xi))1+Dq(xi) \u2248 0 for large D and small \u03b1BO. The gradient of l\u0302(x, 0) is approximated as:\n\u2202l\u0302(x, 0)\n\u2202\u03b1BO = c(\u03b1BO) \u2211 U \u2206ai, (7)\nwhere c(\u03b1BO) > 0 and \u2206ai := QEI(xi)\u2212QEI(x). Here we introduce a conjecture: Let Q\u0304EI := \u222b X QEI(x)dx/D be the average expected improvement,\nand A := \u222b X 1(QEI(x) > Q\u0304EI)dx be the measure of a subspace where the sampled expected improvement value is higher than Q\u0304EI . A decreases from above to below D/2 along the increase of the BO sample size. In other words, a uniformly drawn sample has a chance of more (less) than 50% to have an expected improvement value higher than Q\u0304EI at the early (late) stage of BO.\nOne evidence of the conjecture is illustrated in Fig. 2: In the first iteration, Q\u0304EI is slightly lower than 7.5 while the majority of X has QEI > Q\u0304EI ; in the fourth iteration, however, only a small region around the peak has QEI > Q\u0304EI . Using this conjecture, we can show that \u2211 U \u2206ai < 0 when the sample size is small, thus \u2202l\u0302(x,0)\u2202\u03b1BO < 0. Together with Property 1, we have l\u0302(x, \u03b1BO) < 0 for small \u03b1BO and small sample size. Property 4: We notice that in this experiment, the discrepancy between LHS and the modeled max-min sampling scheme leads to overall high l\u0303 values, indicating that the samples do not follow the model. Nonetheless, negative l\u0303 values can still be observed when \u03b1INI is low. This further suggests that given the LHS samples, a loosely executed max-min sampling is more probable than a strict one.\nMajor findings from Fig. 3 Finding 1: A comparison between \u03b1INI = 1.0 and 10.0 leads to a finding consistent with Property 4. Since the samples are not likely to be drawn from a strictly executed max-min sampling scheme, the entire search trajectory is considered to be created from BO in the case of \u03b1INI = 10.0. While the early samples (less than 10) can be considered as from exploration when \u03b1INI = 1.0 (l\u0303 < 0), the low magnitude of l\u0303 causes this difference to be only visible in the case of \u039b = 10.0I, where the magnitude of l is also low. Finding 2: IBO correctly identifies the true \u039b within a few iterations after the initial exploration, except for the case of \u039b = 10.0I. To explain this inconsistency in IBO performance, we first note that \u039b = 10.0I leads to an expected improvement function that is constant almost everywhere (except for the sampled locations where QEI = 0) and thus a BO that reduces to uniform sampling. From Property 2, LBO = 0 almost surely when we have the correct guess on \u039b. Also recall from Property 4 that LINI > 0 when \u03b1INI is high. The above two together explain why with the correct guess of \u039b = 10.0I, we have L close to zero when \u03b1INI = 10.0 and slightly negative when \u03b1INI = 1.0.\n1 To explain the negative L values for the incorrect guesses of \u039b, we use Property 3 to show that when the sample size is small and the expected improvement function is not flat, LBO < 0 for a small \u03b1BO, and thus L < 0. As a takeaway point, Finding 2 suggests that for a search trajectory with a limited length that resembles a random search, the proposed IBO approach will consider it being derived from a BO that loosely solves Eq. 4. However, this caveat is of little practical concern, since (1) a random search rarely outperforms BO with non-trivial settings, and (2) a BO with low \u03b1BO (instead of high \u039b) can equally simulate a random search."}, {"heading": "5.2 Learning from others vs. self-adaptation", "text": "The above study showed that the correct BO setting \u03bb can be learned through IBO. This subsection further demonstrates the advantage of \u201clearning from others\u201d, i.e., updating \u03bb through IBO, over \u201dself-adaptation\u201d, i.e., finding the MLE of \u03bb using hk. The settings follow the last study and results\n1But why does the guess of \u039b = 10.0I lead to significantly decreasing L in the other three cases? This is because in those cases, BO does not resemble random sampling, i.e., the sequences of samples are more clustered. When a new sample is among this cluster, its similarities to existing ones are non-zero even when a large \u039b is assumed, due to the small Euclidean distance among the pairs. And in turn, the expected improvement function has peaks within the clusters, and remain constant far away from them. As a result, the optimal value of l\u0302(x, \u03b1BO) with respect to \u03b1BO becomes negative, even when \u039b is incorrectly guessed as 10.0I.\nare shown in Fig. 4. First, to show the significant influence of \u03bb on search effectiveness, we show the convergence of two fixed search strategies with \u039b = 0.01 and 10.0. Note that while neither converges to the optimal solution within 50 iteration, the former is significantly more effective than the latter. For \u201cself-adaptive BO\u201d, we use a grid search (G\u039b = {0.01I, 0.1I, 1.0I, 10.0I}) to find \u039b\u0302GP that maximizes Eq. (1) at each iteration, and use it to find the next sample. We show in Fig. 4b the percentages of the four guesses being \u039b\u0302GP along the search, using G\u039b as the initial guesses. We can see that \u039b\u0302GP does not converge to \u039b = 0.01I as quickly as in IBO through self-adaptation. The \u201clearning from others\u201d case starts with \u039b = 10.0I, and uses IBO to derive \u039b\u0302 from the trajectory produced by \u039b = 0.01I. Since IBO identifies the good BO parameter earlier than the MLE of GP in most cases, \u201clearning from others\u201d outperforms \u201cself-adaptation\u201d. It is worth noting that in contrary to this result, we found the two strategies to have similar convergence performance when applied to 2D functions. One potential explanation for this is that effective GP parameters for BO search can be identified with a smaller number of samples in a lower dimensional space."}, {"heading": "6 A case study on ecoRacer", "text": "With support from the simulation studies, this section investigates how IBO may improve the effectiveness of BO in finding the optimal design and control solution for the ecoRacer game when effective searches are observed through crowdsourcing. To do so, we first apply Independent Component Analysis (ICA) to reduce the dimensionality of the crowdsourced solutions from all players. We then use these ICA bases to encode the plays from a player who showed fast convergence but did not have the best solution. The resulting data is then used to obtain the IBO estimator \u03bb\u0302 and a GP estimator \u03bb\u0302GP . The convergence performance in playing ecoRacer under these two settings are compared."}, {"heading": "6.1 Dimension reduction for player\u2019s control signals", "text": "Each game play dataset consists of (1) a final gear ratio used for the vehicle, (2) a control signal with acceleration and braking records, and (3) a corresponding game score. The length of a raw control signal matches that of the track, i.e., actions are stored for each of the 18160 distance steps. Due to the fact that control signals across plays share common patterns, and that BO is ineffective for high-dimensional problems, it is necessary to encode the control signals from all players to a lower dimensional space. In a previous\nstudy on ecoRacer [1], this was done by introducing state-dependent basis functions (i.e., polynomials of the velocity of the car, slope of the track, distance to the terminal, remaining battery energy, and time spent) to parameterize the control signals. Nonetheless, the underlying assumption that human players are aware of all the state-dependent bases is untested.\nThis work explores a different approach to dimension reduction by ICA. The justification is as follows. In experimental psychological literature, research has shown that humans are prone to focusing on only salient features of their environment or problem, filtering out other information. In fact, concurrent performance of multiple tasks consistently leads to impairment in one or more of those tasks [19]. This causes us to switch between tasks quickly if they must be performed at the same time, and the time we spend on each task before switching back is related to our observed productivity at it, called discretionary task interleaving. Such a view is derived from optimal foraging theory, which understands animal foraging as an optimization of the rate of energy gain, and then viewing human behavioral solutions as ones that optimize the rate of information gain in the problem [20]. An example of this is a studying strategy while preparing for an exam, and the tendency we have to switch between difficult and easy topics depending on our perceived productivity at learning them [21]. Similarly, we propose that a human will split the problem presented to them in ecoRacer into separate sub-tasks, in such a way as to maximize their rate of information gain as they switch between exclusive focus on each one. Each sub-task will be an update decision for a salient feature of the control space. One natural separation of the tasks is to spatially segment the large track into smaller stretches of track. To maximize the information gained from exclusive focus on each, we hypothesize that these segments will contain a minimum amount of shared information with each other. The control decisions in each segment will therefore be minimally influenced by the shape of other segments.\nSpecifically, we hypothesize that human players segment the solution space X into m discrete sections: {Xi|\u2200i\u2264mXi \u2286 X}, such that some measure of independence F (X1, \u00b7 \u00b7 \u00b7 ,Xm) is maximized. Separation of observed signals into a number of independent components is the task of blind source separation [22], which can be elegantly addressed using ICA [23]. Compared with Principal Component Analysis which minimizes the covariance of the data, our implementation of ICA maximizes the Kullback-Leibler divergence, thus identifying independent components for non-Gaussian signals, such as the control signal data. By applying ICA to the recorded human plays, we reduce the dimensionality of the control space from 18160 to 30. Because ICA tends to separate complicated non-periodic domains into a set\nof edge-filter-like bases [24], the components arrived at using ICA are naturally correlated with spatial locations, much like wavelets, with each having a peak at some unique position (see Fig. 5). While it is theoretically possible to find some \u201cmost likely\u201d number of bases using information-theoretic criteria for model selection [25]2, the choice of 30 basis is reasonable because (1) over 95% of the variance is explained, and (2) the resultant solution space (30 control variables and one design variable) is small enough for BO to be effective."}, {"heading": "6.2 Derivation of \u03bb\u0302 and \u03bb\u0302GP", "text": "Based on the hypothesis that a player acquires a good search strategy when he/she improves his performance quickly, we pick the search trajectory from a player, referred to as \u201cP2\u201d, who achieved the second highest score within 31 plays, much less than the 150 plays from whom achieved the highest score. To apply IBO, we first encode all control solutions from P2 using the learned ICA bases. Together with the final drive ratios, all 31 solutions are then normalized to be within [\u22121, 1]31. We also found that the probability for P2 to have followed the max-min sampling scheme is lower than that of following BO, as the minimal values of l\u0303(xk, \u03b1INI) for k = 2, ..., 31 (with respect to \u03b1INI) are dominated by those of l(xk, \u03b1BO). This suggests that either the player did not perform pure exploration following the max-min scheme, or this modeled scheme is worse than BO at explaining the exploration. Thus the search for \u03bb\u0302 is performed by solving Eq. (4) using \u03bb \u2208 [0.01, 10.0]31, \u03b1BO \u2208 G\u03b1BO , and with a minimal number of initial samples (K0 = 2) required for BO. For comparison, we obtain \u03bb\u0302GP using the same 31 plays. Note that both Eq. (4) and Eq. (1) are non-convex. In order to avoid inferior local solutions, gradient-based search is applied to a series of initial guesses. Due to the high estimation cost, \u03bb\u0302 and \u03bb\u0302GP are not updated during the search in this case study.\n2For completeness, we used 1000 PCA components as preprocessing to obtain the most likely number of ICA components under MDL [25], AIC, and KIC information criteria as 187, 464, and 373, respectively, using the method from [25]. While these dimensionalities could make sense from a neurological perspective (e.g., given that the game takes 36 seconds, a decision interval of 36s/187 = 192ms is close to the range for the time-frame of attentional blink, which is 200-500 ms [19]), the resultant high-dimensional solution spaces are unfavorable for BO."}, {"heading": "6.3 Comparison of BO performance", "text": "Fig.6 compares the BO performance under \u03bb\u0302, \u03bb\u0302GP and \u039b = I. In each case, we start with the first two plays from P2, and run 180 BO iterations. Similar to the simulation study, results are reported using 20 trials due to the stochastic nature of BO. Due to the small trial number, bootstrap variance estimator is reported (the shades around the average). We can see that \u03bb\u0302 outperforms the other two settings."}, {"heading": "7 Discussion", "text": "The above study provides a starting point for investigations on quantitative incorporation of human solution-search data into optimization algorithms. Yet, many urging questions are not answered in this work. This section will address a few notable ones: We discuss first the connection between IBO and the existing work on Inverse Reinforcement Learning [26\u201328] (IRL, also called apprenticeship learning [29,30] and inverse optimal control [31]). Understanding this connection is important, as we will use analogies from IRL to explain various failure modes of IBO and its potential usage."}, {"heading": "7.1 The difference between learning from searches and learning from solutions", "text": "The proposed IBO approach can be considered as a way to design an optimization algorithm (referred to as DO in the sequel), and is conceptually related to IRL. In order to explain the similarity and difference between the two, we first introduce Markov Decision Process (MDP) and Reinforcement Learning (RL), and to make analogies between (1) MDP and the optimization process, and (2) RL and DO."}, {"heading": "7.1.1 Preliminaries on MDP and RL", "text": "Formally, an MDP consists of a tuple < S,A, T ,R, \u03b3, b0 > where: S is a finite set of states; A is a finite set of actions; the state transition function T (s,a, s\u2032) determines the probability of changing from state s to s\u2032 when action a is taken; R(s,a) is the instantaneous reward of taking action a at state s; \u03b3 \u2208 [0, 1) is the discount factor of the cumulative reward; b0(s) specifies the probability of starting the process at state s. In RL, a control policy \u03c0 is a mapping from a state to an action, i.e., \u03c0 : S \u2192 A. The longterm value of \u03c0 for a given state s can be calculated by V \u03c0(s) = R(s, \u03c0(s))+ \u03b3 \u2211\ns\u2032\u2208S T (s, \u03c0(s), s \u2032)V \u03c0(s\u2032), and thus the value of \u03c0 is the expectation: V \u03c0 =\u2211\ns\u2208S b0(s)V \u03c0(s). A common way to represent a policy is to introduce Qfunction Q(s,a;\u03bb) with unknown control parameters \u03bb, and let the policy be a(s) = argmaxAQ(s,a;\u03bb). RL identifies the optimal \u03bb that maximizes V \u03c0."}, {"heading": "7.1.2 MDP vs. optimization process", "text": "We first note that an optimization process can be modeled as a sequential decision process and shares elements with MDP: Its instantaneous reward is the improvement in the objective value achieved by each new sample, and the cumulative reward represents the total improvement in the objective value within a finite number of iterations. Its state contains the current solution (in X ), the corresponding objective value, and potentially the gradient and higher-order derivatives of the objective function; its action is the next solution to evaluate; and the state transition is governed by the optimization algorithm, parameterized by some algorithmic settings (e.g., BO settings), which is analogous to the state transition in MDP that is affected by the control parameters. We shall note that the actions in the optimization, i.e., the next solutions, are determined not necessarily by the current state, such as in MDP, but by the entire search trajectory, such as in BO. To\nconsider the optimization process as an MDP, we need to redefine the state as the continuously growing search trajectory, i.e., elements in the state set S represent all possible search trajectories, rather than samples in X ."}, {"heading": "7.1.3 RL vs. DO", "text": "The relationship between DO and an optimization process is analogous to that between RL and an MDP: As RL searches for the control parameters that maximizes the value of a MDP, DO similarly searches for settings of an optimization algorithm that maximizes the improvement in the objective. Since MDP is one type of decision process, RL belongs to DO. Further, DO is recursive: The optimal design of any DO algorithm is by itself a DO problem. We shall note that there is a significant difference between RL and DO: While RL algorithms can be developed based on the Bellman\u2019s optimality principle and the Markovian property of MDP, DO in general is usually done empirically and manually. For example, in either gradient-based methods (e.g., sequential quadratic programming) or heuristic methods (e.g., genetic algorithms), good algorithmic parameters are suggested based on experience, rather than being optimized for a specific problem."}, {"heading": "7.1.4 IRL vs. IBO", "text": "While RL identifies an optimal control policy for an MDP with a given reward function, real-world applications hardly define a reward function, e.g., the reward for \u201cdrives well\u201d cannot be explicitly defined. Alternatively, it is easier for a human expert to demonstrate state-action sequences that are near-optimal for a certain task. IRL techniques have thus been developed to identify the inherent reward (and thus the Q-function) that explains human demonstrations, either by estimating the reward parameters so that the demonstrated policy has a higher value than any other policies by a margin [26,29,32,33], or by maximizing the likelihood of control parameters by assuming near-optimal control of the demonstration [27,34].\nThe IBO approach introduced in this paper, as a type of DO, is closely related to latter type of IRLs, and in particular to the maximum entropy method proposed by Ziebart et al. [27]. Briefly, the maximum entropy IRL proposes the following MLE of parameters \u03bb based on a set of demonstrations h:\n\u03bb\u0302 = argmax\u03bb logP (h|\u03bb)\n= argmax\u03bb log exp\n(\u2211 (si,ai)\u2208hR(si,ai,\u03bb) ) \u220f\n(si,ai)\u2208h Zi(\u03bb) ,\n(8)\nwhere Zi(\u03bb) is a partition function for the visited state si. One can notice the similarities between Eqs. (8) and (4): (1) Both are maximum likelihood estimations of parameters related to a instantaneous cost, the reward in Eq. (8) and the expected improvement in IBO. (2) Both involves partition functions that are computationally expensive, and dependent on the state s and the parameters \u03bb. Due to this dependency of partition function on \u03bb, a direct Markov-Chain Monte Carlo (MCMC) sampling in the space of \u03bb (e.g., as in [34]) cannot be applied to optimize the likelihood function since the partition values for two different samples of \u03bb do not cancel. Ziebart et al. discussed on alternative approach to address this computational challenge by using an \u201cExpected Edge Frequency Calculation\u201d algorithm that has a complexity of O(N |S||A|) for each gradient calculation of the objective in Eq. (8), where N is a large number [27]. However, this approach can be infeasible for the IBO estimation problem in Eq. (4) since (1) the space X is usually continuous, and (2) even with a discretization of X , the enormous size of S and A can easily make the calculation intractable, based on the discussion in Sec. 7.1.2.\nBesides the computational challenge of IBO due to the underlying nonMarkovian optimization process and the large solution space X , there is another subtle difference between IRL and IBO in their assumptions about human demonstrations and thus their applications. IRL is developed for RL problems where the reward function cannot be quantitatively defined, but rather has to be learned through human demonstrations. Since these demonstrations fulfil the inherent requirements of the designer, they are assumed to be near-optimal, i.e., they lead to near-optimal cumulative rewards over a finite time horizon. In BO, however, the reward is explicitly defined as the improvement in the objective. Nonetheless, evaluating the reward can be costly for a machine due to its lack of knowledge about the state transition function, e.g., unlike human beings, the machine cannot \u201cimagine\u201d but has to calculate the outcome of ecoRacer. To this end, BO uses the GP model and the expected improvement function to analytically approximate the state transition and the reward functions, respectively. Since there lacks a goodness measure of this approximation, we resort to IBO to estimate \u201cgood\u201d GP parameters, by assuming that the search trajectories used for the estimation are near-optimal, i.e., they achieve near-optimal improvement in the objective value in a fixed number of iterations. It should be noted that this assumption does not require the availability of near-optimal solutions in the demonstration.\nTo summarize: IRL can be used when the machine is told to mimic existing solutions, by understanding why these solutions are considered as\nbeing good. IBO can be used when the machine is meant to mimic the process of searching for good solutions, by understanding how to evaluate the expected improvement of solutions."}, {"heading": "7.2 The potential value of IBO", "text": "We saw that IBO has limited appeal in the presented ecoRacer case as it only led to marginal improvement from a default BO setting after observing 31 solutions from P2. In fact, one could achieve faster convergence with a default BO (\u03bb = 1.0I) by initializing the search with these solutions. In the following, we discuss three situations when IBO and DO in general will fail, and what future research are needed to avoid failures.\nThe first situation is when the human search strategy cannot be explained by BO. Studies in cognitive science have identified some of the core ingredients of human intelligence, including intuitive physics [35\u201338], problem decomposition skills [39\u201341], the ability of learning-to-learn [42], and others [43]. While evidence have shown the connection between BO and human search [14], suitable models of human search strategies can very likely be problem dependent. Specific to ecoRacer, and other design and control problems of dynamical systems, a potentially more reasonable human search model could be to incorporate human intuitive physics models into the evaluation of expected improvement. Thus instead of estimating GP parameters, one could estimate a statistical model of the state transition function.\nThe second situation is when the demonstration is insufficient to provide a good estimation of \u03bb of an effective search. An evidence is shown in Fig. 7 where we apply BO to ecoRacer with \u03bb\u0302 learned from the first 11 plays of P2. Its performance is compared with that of a default BO, and is shown to only allows marginal improvements in a short term. One potential solution to this would be to use the covariance of the estimator, i.e., the Hessian of the likelihood function, to inform an incentive mechanism. For example, if \u03bb\u0302 learned from an effective search trajectory has a high variance, the crowdsourcing system could track down the player and offer higher reward for more solutions. It would also be interesting to understand the influence of the properties of the problem, e.g., the size of the solution space, on the convergence of the variance of the estimator.\nThe third situation is when the demonstration is insufficient for the designer, or the crowdsourcing system, to tell who is actually more effective at searching, i.e., learning an effective search within a fixed number of iterations may not guarantee that the search will continue to be effective. We\nargue, however, that this is a universal challenge in IRL as well. For example, demonstrations on pancake flipping does not guarantee a good solution to egg flipping due to the change in physical properties and thus in state transition. The fundamental challenge across IRL, IBO and other potential DO approaches, is that we may not be able to identify a fixed set of optimal \u03bb\u0302 when the true optimum changes. A common solution to this in IRL is to adapt \u03bb\u0302 to the observed new environments. We shall investigate if such adaption, e.g., through the maximum likelihood estimator of the GP, will work for IBO.\nTo summarize, IBO is useful, as in the simulation study, when (1) the underlying human search mechanism follows BO, (2) the search trajectory is sufficient for an estimation with low variance, and (3) the true optimal BO setting for a long-term search can be estimated from the trajectory of an effective short-term search."}, {"heading": "8 Conclusions", "text": "This paper was motivated by the observation that it is more likely to crowdsource effective solution search data than optimal design solutions. We hypothesized that human search data contain useful information to enhance an optimization algorithm. To test this hypothesis, we introduced the Inverse Bayesian Optimization method that attempts to mimic human searches through Bayesian Optimization. Simulation studies on the 30D Rosenbrock function showed that IBO can correctly identify the best BO parameters using a small number of observations and the resultant BO can outperform the one with self-adaptive parameters. We than applied IBO to the real human search data collected from the game ecoRacer. The marginal improvement achieved by IBO from default and self-adaptive BO settings indicated the limitations of this approach. Potential solutions to these limitations are discussed in depth, with the help of an analogy between IBO and IRL.\nWe conclude with a bigger picture. Design crowdsourcing faces a dilemma: It is believed to be a promising solution to computationally challenging design problems according to speculations, yet in reality its success is highly doubted even by the academia due to its high overhead cost and the high uncertainty solution quality. This paper establishes a line of research that aims to turn high quality search into high quality solutions. Since the former is more often available from the crowd, this research could potentially lowering the uncertainty on crowdsourced solution quality and thus improve the chance of success of design crowdsourcing activities."}, {"heading": "Acknowledgement", "text": "This work has been supported by the National Science Foundation under Grant No. CMMI-1266184 and the start-up funding from Arizona State University. These supports are gratefully acknowledged."}], "references": [{"title": "ecoracer: Game-based optimal electric vehicle design and driver control using human players", "author": ["Y. Ren", "A.E. Bayrak", "P.Y. Papalambros"], "venue": "In ASME 2015 International Design Engineering Technical Conferences and Computers and Information in Engineering Con-", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2015}, {"title": "Foldit. http: //fold.it", "author": ["S. Cooper", "F. Khatib", "A. Treuille", "J. Barbero", "J. Lee", "M. Beenen", "A. Leaver-Fay", "D. Baker", "Z Popovi\u0107"], "venue": null, "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2010}, {"title": "Algorithm discovery by protein folding game players", "author": ["F. Khatib", "S. Cooper", "M.D. Tyka", "K. Xu", "I. Makedon", "Z. Popovi\u0107", "D. Baker", "F. Players"], "venue": "Proceedings of the National Academy of Sciences,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2011}, {"title": "eterna. http://eterna.cmu.edu", "author": ["J. Lee", "W. Kladwang", "M. Lee", "D. Cantu", "M. Azizyan", "H. Kim", "A. Limpaecher", "S. Yoon", "A. Treuille", "R. Das"], "venue": null, "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2014}, {"title": "Rna design rules from a massive open laboratory", "author": ["J. Lee", "W. Kladwang", "M. Lee", "D. Cantu", "M. Azizyan", "H. Kim", "A. Limpaecher", "S. Yoon", "A. Treuille", "R. Das"], "venue": "Proceedings of the National Academy of Sciences,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2014}, {"title": "Phylo: a citizen science approach for improving multiple sequence alignment", "author": ["A. Kawrykow", "G. Roumanis", "A. Kam", "D. Kwak", "C. Leung", "C. Wu", "E. Zarour", "L. Sarmenta", "M. Blanchette", "J Waldisp\u00fchl"], "venue": "PloS one,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2012}, {"title": "Robobarista: Object part based transfer of manipulation trajectories from crowd-sourcing in 3d pointclouds", "author": ["J. Sung", "S.H. Jin", "A. Saxena"], "venue": "arXiv preprint arXiv:1504.03071", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2015}, {"title": "Frenzy: collaborative data organization for creating conference sessions", "author": ["L.B. Chilton", "J. Kim", "P. Andr\u00e9", "F. Cordeiro", "J.A. Landay", "D.S. Weld", "S.P. Dow", "R.C. Miller", "H. Zhang"], "venue": "In Proceedings of the 32nd annual ACM conference on Human factors in computing systems,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2014}, {"title": "Games with a purpose", "author": ["L. Von Ahn"], "venue": null, "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2006}, {"title": "Playing atari with deep reinforcement learning", "author": ["V. Mnih", "K. Kavukcuoglu", "D. Silver", "A. Graves", "I. Antonoglou", "D. Wierstra", "M. Riedmiller"], "venue": "arXiv preprint arXiv:1312.5602", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2013}, {"title": "Efficient global optimization of expensive black-box functions", "author": ["D. Jones", "M. Schonlau", "W. Welch"], "venue": "Journal of Global Optimization,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 1998}, {"title": "A tutorial on bayesian optimization of expensive cost functions, with application to active user modeling and hierarchical reinforcement learning", "author": ["E. Brochu", "V.M. Cora", "N. De Freitas"], "venue": "arXiv preprint arXiv:1012.2599", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2010}, {"title": "Bayesian gait optimization for bipedal locomotion", "author": ["R. Calandra", "N. Gopalan", "A. Seyfarth", "J. Peters", "M.P. Deisenroth"], "venue": "In International Conference on Learning and Intelligent Optimization,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2014}, {"title": "Bayesian optimization explains human active search", "author": ["A. Borji", "L. Itti"], "venue": "In Advances in neural information processing systems,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2013}, {"title": "Gaussian processes for machine learning", "author": ["C.E. Rasmussen"], "venue": null, "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2006}, {"title": "Lipschitzian optimization without the lipschitz constant", "author": ["D.R. Jones", "C.D. Perttunen", "B.E. Stuckman"], "venue": "Journal of Optimization Theory and Applications,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 1993}, {"title": "Baron: A general purpose global optimization software package", "author": ["N.V. Sahinidis"], "venue": "Journal of global optimization,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 1996}, {"title": "L-bfgs-b: Fortran subroutines for large scale bound constrained optimization", "author": ["C. Zhu", "R.H. Byrd", "P. Lu", "J. Nocedal"], "venue": null, "citeRegEx": "18", "shortCiteRegEx": "18", "year": 1994}, {"title": "A unified attentional bottleneck in the human brain", "author": ["M.N. Tombu", "C.L. Asplund", "P.E. Dux", "D. Godwin", "J.W. Martin", "R. Marois"], "venue": "Proceedings of the National Academy of Sciences,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2011}, {"title": "Discretionary task interleaving: heuristics for time allocation in cognitive foraging.", "author": ["S.J. Payne", "G.B. Duggan", "H. Neth"], "venue": "Journal of Experimental Psychology: General,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2007}, {"title": "Is study time allocated selectively to a region of proximal learning?", "author": ["J. Metcalfe"], "venue": "Journal of Experimental Psychology: General,", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2002}, {"title": "Blind source separation and independent component analysis: A review", "author": ["S. Lee"], "venue": null, "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2005}, {"title": "Independent component analysis", "author": ["J.V. Stone"], "venue": "Wiley Online Library", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2004}, {"title": "The \u201dindependent components\u201d of natural scenes are edge filters", "author": ["A.J. Bell", "T.J. Sejnowski"], "venue": "Vision research,", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 1997}, {"title": "An empirical comparison of information-theoretic criteria in estimating the number of independent components of fmri data", "author": ["M. Hui", "J. Li", "X. Wen", "L. Yao", "Z. Long"], "venue": "PloS one,", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2011}, {"title": "Algorithms for inverse reinforcement learning.", "author": ["A.Y. Ng", "Russell", "S. J"], "venue": "In Icml,", "citeRegEx": "26", "shortCiteRegEx": "26", "year": 2000}, {"title": "Maximum entropy inverse reinforcement learning.", "author": ["B.D. Ziebart", "A.L. Maas", "J.A. Bagnell", "A.K. Dey"], "venue": "In AAAI,", "citeRegEx": "27", "shortCiteRegEx": "27", "year": 2008}, {"title": "Nonlinear inverse reinforcement learning with gaussian processes", "author": ["S. Levine", "Z. Popovic", "V. Koltun"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "28", "shortCiteRegEx": "28", "year": 2011}, {"title": "Apprenticeship learning via inverse reinforcement learning", "author": ["P. Abbeel", "A.Y. Ng"], "venue": "In Proceedings of the twenty-first international conference on Machine learning,", "citeRegEx": "29", "shortCiteRegEx": "29", "year": 2004}, {"title": "Autonomous helicopter aerobatics through apprenticeship learning", "author": ["P. Abbeel", "A. Coates", "A.Y. Ng"], "venue": "The International Journal of Robotics Research", "citeRegEx": "30", "shortCiteRegEx": "30", "year": 2010}, {"title": "Inverse optimal control with linearly-solvable mdps", "author": ["K. Dvijotham", "E. Todorov"], "venue": "In Proceedings of the 27th International Conference on Machine Learning", "citeRegEx": "31", "shortCiteRegEx": "31", "year": 2010}, {"title": "Maximum margin planning", "author": ["N.D. Ratliff", "J.A. Bagnell", "M.A. Zinkevich"], "venue": "In Proceedings of the 23rd international conference on Machine learning,", "citeRegEx": "32", "shortCiteRegEx": "32", "year": 2006}, {"title": "A game-theoretic approach to apprenticeship learning", "author": ["U. Syed", "R.E. Schapire"], "venue": "In Advances in neural information processing systems,", "citeRegEx": "33", "shortCiteRegEx": "33", "year": 2007}, {"title": "Bayesian inverse reinforcement learning", "author": ["D. Ramachandran", "E. Amir"], "venue": null, "citeRegEx": "34", "shortCiteRegEx": "34", "year": 2007}, {"title": "The development of object perception.", "author": ["E.S. Spelke", "G. Gutheil", "G. Van de Walle"], "venue": null, "citeRegEx": "35", "shortCiteRegEx": "35", "year": 1995}, {"title": "An account of infants\u2019 physical reasoning", "author": ["R. Baillargeon", "J. Li", "W. Ng", "S. Yuan"], "venue": "Learning and the infant mind,", "citeRegEx": "36", "shortCiteRegEx": "36", "year": 2009}, {"title": "Humans predict liquid dynamics using probabilistic simulation", "author": ["C.J. Bates", "I. Yildirim", "J.B. Tenenbaum", "P.W. Battaglia"], "venue": "In Proceedings of the 37th annual conference of the cognitive science society", "citeRegEx": "37", "shortCiteRegEx": "37", "year": 2015}, {"title": "Computational rationality: A converging paradigm for intelligence in brains, minds, and machines", "author": ["S.J. Gershman", "E.J. Horvitz", "J.B. Tenenbaum"], "venue": "Science, 349(6245),", "citeRegEx": "38", "shortCiteRegEx": "38", "year": 2015}, {"title": "Recognition-by-components: a theory of human image understanding.", "author": ["I. Biederman"], "venue": "Psychological review,", "citeRegEx": "40", "shortCiteRegEx": "40", "year": 1987}, {"title": "Hierarchical deep reinforcement learning: Integrating temporal abstraction and intrinsic motivation", "author": ["T.D. Kulkarni", "K.R. Narasimhan", "A. Saeedi", "J.B. Tenenbaum"], "venue": "arXiv preprint arXiv:1604.06057", "citeRegEx": "41", "shortCiteRegEx": "41", "year": 2016}, {"title": "The formation of learning sets.", "author": ["H.F. Harlow"], "venue": "Psychological review,", "citeRegEx": "42", "shortCiteRegEx": "42", "year": 1949}, {"title": "Building machines that learn and think like people", "author": ["B.M. Lake", "T.D. Ullman", "J.B. Tenenbaum", "S.J. Gershman"], "venue": "arXiv preprint arXiv:1604.00289", "citeRegEx": "43", "shortCiteRegEx": "43", "year": 2016}], "referenceMentions": [{"referenceID": 0, "context": "The method is first verified through simulation studies and then applied to human solutions crowdsourced from a gamified design problem [1].", "startOffset": 136, "endOffset": 139}, {"referenceID": 1, "context": "Examples can be found from crowdsourcing scientific solutions for protein folding [2, 3], RNA synthesis [4, 5], genome sequence alignment [6], robot arm movements [7], to name a few, as well as from our daily achievements in packing luggage, scheduling conference sessions [8], and playing games [1,9,10].", "startOffset": 82, "endOffset": 88}, {"referenceID": 2, "context": "Examples can be found from crowdsourcing scientific solutions for protein folding [2, 3], RNA synthesis [4, 5], genome sequence alignment [6], robot arm movements [7], to name a few, as well as from our daily achievements in packing luggage, scheduling conference sessions [8], and playing games [1,9,10].", "startOffset": 82, "endOffset": 88}, {"referenceID": 3, "context": "Examples can be found from crowdsourcing scientific solutions for protein folding [2, 3], RNA synthesis [4, 5], genome sequence alignment [6], robot arm movements [7], to name a few, as well as from our daily achievements in packing luggage, scheduling conference sessions [8], and playing games [1,9,10].", "startOffset": 104, "endOffset": 110}, {"referenceID": 4, "context": "Examples can be found from crowdsourcing scientific solutions for protein folding [2, 3], RNA synthesis [4, 5], genome sequence alignment [6], robot arm movements [7], to name a few, as well as from our daily achievements in packing luggage, scheduling conference sessions [8], and playing games [1,9,10].", "startOffset": 104, "endOffset": 110}, {"referenceID": 5, "context": "Examples can be found from crowdsourcing scientific solutions for protein folding [2, 3], RNA synthesis [4, 5], genome sequence alignment [6], robot arm movements [7], to name a few, as well as from our daily achievements in packing luggage, scheduling conference sessions [8], and playing games [1,9,10].", "startOffset": 138, "endOffset": 141}, {"referenceID": 6, "context": "Examples can be found from crowdsourcing scientific solutions for protein folding [2, 3], RNA synthesis [4, 5], genome sequence alignment [6], robot arm movements [7], to name a few, as well as from our daily achievements in packing luggage, scheduling conference sessions [8], and playing games [1,9,10].", "startOffset": 163, "endOffset": 166}, {"referenceID": 7, "context": "Examples can be found from crowdsourcing scientific solutions for protein folding [2, 3], RNA synthesis [4, 5], genome sequence alignment [6], robot arm movements [7], to name a few, as well as from our daily achievements in packing luggage, scheduling conference sessions [8], and playing games [1,9,10].", "startOffset": 273, "endOffset": 276}, {"referenceID": 0, "context": "Examples can be found from crowdsourcing scientific solutions for protein folding [2, 3], RNA synthesis [4, 5], genome sequence alignment [6], robot arm movements [7], to name a few, as well as from our daily achievements in packing luggage, scheduling conference sessions [8], and playing games [1,9,10].", "startOffset": 296, "endOffset": 304}, {"referenceID": 8, "context": "Examples can be found from crowdsourcing scientific solutions for protein folding [2, 3], RNA synthesis [4, 5], genome sequence alignment [6], robot arm movements [7], to name a few, as well as from our daily achievements in packing luggage, scheduling conference sessions [8], and playing games [1,9,10].", "startOffset": 296, "endOffset": 304}, {"referenceID": 9, "context": "Examples can be found from crowdsourcing scientific solutions for protein folding [2, 3], RNA synthesis [4, 5], genome sequence alignment [6], robot arm movements [7], to name a few, as well as from our daily achievements in packing luggage, scheduling conference sessions [8], and playing games [1,9,10].", "startOffset": 296, "endOffset": 304}, {"referenceID": 0, "context": "As a particular example, our previous study investigated the value of crowdsourcing optimal design and control problems with nondeterministic polynomial time by comparing the search performance of an anonymous crowd with a Bayesian Optimization (BO) algorithm on an electric vehicle time-trial game [1].", "startOffset": 299, "endOffset": 302}, {"referenceID": 10, "context": "Specifically, we assume that a player\u2019s search trajectory is produced from BO (also called \u201cEfficient Global Optimization\u201d [11]) where each new trial optimizes an expected improvement function learned and adjusted by the player during the search.", "startOffset": 123, "endOffset": 127}, {"referenceID": 10, "context": "We make the assumption for two reasons: First, BO has been widely used for non-convex optimization problems with high evaluation costs [11,12], which includes the design of dynamical systems where the evaluation of the objective involves long-term simulation of the", "startOffset": 135, "endOffset": 142}, {"referenceID": 11, "context": "We make the assumption for two reasons: First, BO has been widely used for non-convex optimization problems with high evaluation costs [11,12], which includes the design of dynamical systems where the evaluation of the objective involves long-term simulation of the", "startOffset": 135, "endOffset": 142}, {"referenceID": 11, "context": "system [12,13].", "startOffset": 7, "endOffset": 14}, {"referenceID": 12, "context": "system [12,13].", "startOffset": 7, "endOffset": 14}, {"referenceID": 13, "context": "Secondly, it is recently found that BO best resembles human search, which outperforms an extensive set of optimization algorithms in 1D problems [14].", "startOffset": 145, "endOffset": 149}, {"referenceID": 0, "context": "Image is reproduced from [1].", "startOffset": 25, "endOffset": 28}, {"referenceID": 14, "context": "Model update: BO first updates a Gaussian Process (GP) model [15] to predict objective values, based on current observations hk and Gaussian parameters \u03bb.", "startOffset": 61, "endOffset": 65}, {"referenceID": 15, "context": "Note that QEI is non-convex with respect to x and its maximization may involve a nested global optimization routine, such as Genetic Algorithm, DIRECT [16], and BARON [17].", "startOffset": 151, "endOffset": 155}, {"referenceID": 16, "context": "Note that QEI is non-convex with respect to x and its maximization may involve a nested global optimization routine, such as Genetic Algorithm, DIRECT [16], and BARON [17].", "startOffset": 167, "endOffset": 171}, {"referenceID": 10, "context": "The exploration set h0 that is necessary to initialize the GP model is usually created by Latin Hypercube sampling (LHS, see [11] for details).", "startOffset": 125, "endOffset": 129}, {"referenceID": 0, "context": "Image is modified from [1].", "startOffset": 23, "endOffset": 26}, {"referenceID": 17, "context": "At each iteration, the expected improvement is maximized using a multi-start gradient descent algorithm [18] with 100 LHS initial guesses.", "startOffset": 104, "endOffset": 108}, {"referenceID": 0, "context": "study on ecoRacer [1], this was done by introducing state-dependent basis functions (i.", "startOffset": 18, "endOffset": 21}, {"referenceID": 18, "context": "In fact, concurrent performance of multiple tasks consistently leads to impairment in one or more of those tasks [19].", "startOffset": 113, "endOffset": 117}, {"referenceID": 19, "context": "Such a view is derived from optimal foraging theory, which understands animal foraging as an optimization of the rate of energy gain, and then viewing human behavioral solutions as ones that optimize the rate of information gain in the problem [20].", "startOffset": 244, "endOffset": 248}, {"referenceID": 20, "context": "An example of this is a studying strategy while preparing for an exam, and the tendency we have to switch between difficult and easy topics depending on our perceived productivity at learning them [21].", "startOffset": 197, "endOffset": 201}, {"referenceID": 21, "context": "separation [22], which can be elegantly addressed using ICA [23].", "startOffset": 11, "endOffset": 15}, {"referenceID": 22, "context": "separation [22], which can be elegantly addressed using ICA [23].", "startOffset": 60, "endOffset": 64}, {"referenceID": 23, "context": "of edge-filter-like bases [24], the components arrived at using ICA are naturally correlated with spatial locations, much like wavelets, with each having a peak at some unique position (see Fig.", "startOffset": 26, "endOffset": 30}, {"referenceID": 24, "context": "While it is theoretically possible to find some \u201cmost likely\u201d number of bases using information-theoretic criteria for model selection [25]2, the choice of 30 basis is reasonable because (1) over 95% of the variance is explained, and (2) the resultant solution space (30 control variables and one design variable) is small enough for BO to be effective.", "startOffset": 135, "endOffset": 139}, {"referenceID": 24, "context": "For completeness, we used 1000 PCA components as preprocessing to obtain the most likely number of ICA components under MDL [25], AIC, and KIC information criteria as 187, 464, and 373, respectively, using the method from [25].", "startOffset": 124, "endOffset": 128}, {"referenceID": 24, "context": "For completeness, we used 1000 PCA components as preprocessing to obtain the most likely number of ICA components under MDL [25], AIC, and KIC information criteria as 187, 464, and 373, respectively, using the method from [25].", "startOffset": 222, "endOffset": 226}, {"referenceID": 18, "context": ", given that the game takes 36 seconds, a decision interval of 36s/187 = 192ms is close to the range for the time-frame of attentional blink, which is 200-500 ms [19]), the resultant high-dimensional solution spaces are unfavorable for BO.", "startOffset": 162, "endOffset": 166}, {"referenceID": 25, "context": "This section will address a few notable ones: We discuss first the connection between IBO and the existing work on Inverse Reinforcement Learning [26\u201328] (IRL, also called apprenticeship learning [29,30] and inverse optimal control [31]).", "startOffset": 146, "endOffset": 153}, {"referenceID": 26, "context": "This section will address a few notable ones: We discuss first the connection between IBO and the existing work on Inverse Reinforcement Learning [26\u201328] (IRL, also called apprenticeship learning [29,30] and inverse optimal control [31]).", "startOffset": 146, "endOffset": 153}, {"referenceID": 27, "context": "This section will address a few notable ones: We discuss first the connection between IBO and the existing work on Inverse Reinforcement Learning [26\u201328] (IRL, also called apprenticeship learning [29,30] and inverse optimal control [31]).", "startOffset": 146, "endOffset": 153}, {"referenceID": 28, "context": "This section will address a few notable ones: We discuss first the connection between IBO and the existing work on Inverse Reinforcement Learning [26\u201328] (IRL, also called apprenticeship learning [29,30] and inverse optimal control [31]).", "startOffset": 196, "endOffset": 203}, {"referenceID": 29, "context": "This section will address a few notable ones: We discuss first the connection between IBO and the existing work on Inverse Reinforcement Learning [26\u201328] (IRL, also called apprenticeship learning [29,30] and inverse optimal control [31]).", "startOffset": 196, "endOffset": 203}, {"referenceID": 30, "context": "This section will address a few notable ones: We discuss first the connection between IBO and the existing work on Inverse Reinforcement Learning [26\u201328] (IRL, also called apprenticeship learning [29,30] and inverse optimal control [31]).", "startOffset": 232, "endOffset": 236}, {"referenceID": 25, "context": "IRL techniques have thus been developed to identify the inherent reward (and thus the Q-function) that explains human demonstrations, either by estimating the reward parameters so that the demonstrated policy has a higher value than any other policies by a margin [26,29,32,33], or by maximizing the likelihood of control parameters by assuming near-optimal control of the demonstration [27,34].", "startOffset": 264, "endOffset": 277}, {"referenceID": 28, "context": "IRL techniques have thus been developed to identify the inherent reward (and thus the Q-function) that explains human demonstrations, either by estimating the reward parameters so that the demonstrated policy has a higher value than any other policies by a margin [26,29,32,33], or by maximizing the likelihood of control parameters by assuming near-optimal control of the demonstration [27,34].", "startOffset": 264, "endOffset": 277}, {"referenceID": 31, "context": "IRL techniques have thus been developed to identify the inherent reward (and thus the Q-function) that explains human demonstrations, either by estimating the reward parameters so that the demonstrated policy has a higher value than any other policies by a margin [26,29,32,33], or by maximizing the likelihood of control parameters by assuming near-optimal control of the demonstration [27,34].", "startOffset": 264, "endOffset": 277}, {"referenceID": 32, "context": "IRL techniques have thus been developed to identify the inherent reward (and thus the Q-function) that explains human demonstrations, either by estimating the reward parameters so that the demonstrated policy has a higher value than any other policies by a margin [26,29,32,33], or by maximizing the likelihood of control parameters by assuming near-optimal control of the demonstration [27,34].", "startOffset": 264, "endOffset": 277}, {"referenceID": 26, "context": "IRL techniques have thus been developed to identify the inherent reward (and thus the Q-function) that explains human demonstrations, either by estimating the reward parameters so that the demonstrated policy has a higher value than any other policies by a margin [26,29,32,33], or by maximizing the likelihood of control parameters by assuming near-optimal control of the demonstration [27,34].", "startOffset": 387, "endOffset": 394}, {"referenceID": 33, "context": "IRL techniques have thus been developed to identify the inherent reward (and thus the Q-function) that explains human demonstrations, either by estimating the reward parameters so that the demonstrated policy has a higher value than any other policies by a margin [26,29,32,33], or by maximizing the likelihood of control parameters by assuming near-optimal control of the demonstration [27,34].", "startOffset": 387, "endOffset": 394}, {"referenceID": 26, "context": "[27].", "startOffset": 0, "endOffset": 4}, {"referenceID": 33, "context": ", as in [34]) cannot be applied to optimize the likelihood function since the partition values for two different samples of \u03bb do not cancel.", "startOffset": 8, "endOffset": 12}, {"referenceID": 26, "context": "(8), where N is a large number [27].", "startOffset": 31, "endOffset": 35}, {"referenceID": 34, "context": "Studies in cognitive science have identified some of the core ingredients of human intelligence, including intuitive physics [35\u201338], problem decomposition skills [39\u201341], the ability of learning-to-learn [42], and others [43].", "startOffset": 125, "endOffset": 132}, {"referenceID": 35, "context": "Studies in cognitive science have identified some of the core ingredients of human intelligence, including intuitive physics [35\u201338], problem decomposition skills [39\u201341], the ability of learning-to-learn [42], and others [43].", "startOffset": 125, "endOffset": 132}, {"referenceID": 36, "context": "Studies in cognitive science have identified some of the core ingredients of human intelligence, including intuitive physics [35\u201338], problem decomposition skills [39\u201341], the ability of learning-to-learn [42], and others [43].", "startOffset": 125, "endOffset": 132}, {"referenceID": 37, "context": "Studies in cognitive science have identified some of the core ingredients of human intelligence, including intuitive physics [35\u201338], problem decomposition skills [39\u201341], the ability of learning-to-learn [42], and others [43].", "startOffset": 125, "endOffset": 132}, {"referenceID": 38, "context": "Studies in cognitive science have identified some of the core ingredients of human intelligence, including intuitive physics [35\u201338], problem decomposition skills [39\u201341], the ability of learning-to-learn [42], and others [43].", "startOffset": 163, "endOffset": 170}, {"referenceID": 39, "context": "Studies in cognitive science have identified some of the core ingredients of human intelligence, including intuitive physics [35\u201338], problem decomposition skills [39\u201341], the ability of learning-to-learn [42], and others [43].", "startOffset": 163, "endOffset": 170}, {"referenceID": 40, "context": "Studies in cognitive science have identified some of the core ingredients of human intelligence, including intuitive physics [35\u201338], problem decomposition skills [39\u201341], the ability of learning-to-learn [42], and others [43].", "startOffset": 205, "endOffset": 209}, {"referenceID": 41, "context": "Studies in cognitive science have identified some of the core ingredients of human intelligence, including intuitive physics [35\u201338], problem decomposition skills [39\u201341], the ability of learning-to-learn [42], and others [43].", "startOffset": 222, "endOffset": 226}, {"referenceID": 13, "context": "While evidence have shown the connection between BO and human search [14], suitable models of human search strategies can very likely be problem dependent.", "startOffset": 69, "endOffset": 73}], "year": 2017, "abstractText": "There is evidence that humans can be more efficient than existing algorithms at searching for good solutions in high-dimensional and nonconvex design or control spaces, potentially due to our prior knowledge and learning capability. This work attempts to quantify the search strategy of human beings to enhance a Bayesian optimization (BO) algorithm for an optimal design and control problem. We consider the sequence of human solutions (called a search trajectory) as generated from BO, and propose to recover the algorithmic parameters of BO through maximum likelihood estimation. The method is first verified through simulation studies and then applied to human solutions crowdsourced from a gamified design problem [1]. We learn BO parameters from a player who achieved fast improvement in his/her solutions and show that applying the learned parameters to BO achieves better convergence than using a self-adaptive BO. The proposed method is different from inverse reinforcement learning in that it only requires a good search strategy, rather than near-optimal solutions from humans.", "creator": "LaTeX with hyperref package"}}}