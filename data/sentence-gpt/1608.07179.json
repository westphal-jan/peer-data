{"id": "1608.07179", "review": {"conference": "NIPS", "VERSION": "v1", "DATE_OF_SUBMISSION": "25-Aug-2016", "title": "Minimizing Quadratic Functions in Constant Time", "abstract": "A sampling-based optimization method for quadratic functions is proposed. Our method approximately solves the following $n$-dimensional quadratic minimization problem in constant time, which is independent of $n$: $z^*=\\min_{\\mathbf{v} \\in \\mathbb{R}^n}\\langle\\mathbf{v}, A \\mathbf{v}\\rangle + n\\langle\\mathbf{v}, \\mathrm{diag}(\\mathbf{d})\\mathbf{v}\\rangle + n\\langle\\mathbf{b}, \\mathbf{v}\\rangle$, where $A \\in \\mathbb{R}^{n \\times n}$ is a matrix and $\\mathbf{d},\\mathbf{b} \\in \\mathbb{R}^n$ are vectors. Our theoretical analysis specifies the number of samples $k(\\delta, \\epsilon)$ such that the approximated solution $z$ satisfies $|z - z^*| = O(\\epsilon n^2)$ with probability $1-\\delta$, or $z$ with probability $0, or $z$ with probability $0, or $z$ with probability $0, or $z$ with probability $0, or $z$ with probability $0, or $z$ with probability $0, or $z$ with probability $0, or $z$ with probability $0, or $z$ with probability $0, or $z$ with probability $0, or $z$ with probability $0, or $z$ with probability $0, or $z$ with probability $0, or $z$ with probability $0, or $z$ with probability $0, or $z$ with probability $0, or $z$ with probability $0, or $z$ with probability $0, or $z$ with probability $0, or $z$ with probability $0, or $z$ with probability $0, or $z$ with probability $0, or $z$ with probability $0, or $z$ with probability $0, or $z$ with probability $0, or $z$ with probability $0, or $z$ with probability $0, or $z$ with probability $0, or $z$ with probability $0, or $z$ with probability $", "histories": [["v1", "Thu, 25 Aug 2016 14:43:17 GMT  (186kb)", "http://arxiv.org/abs/1608.07179v1", "An extended abstract will appear in the proceedings of NIPS'16"]], "COMMENTS": "An extended abstract will appear in the proceedings of NIPS'16", "reviews": [], "SUBJECTS": "cs.LG cs.DS stat.ML", "authors": ["kohei hayashi", "yuichi yoshida"], "accepted": true, "id": "1608.07179"}, "pdf": {"name": "1608.07179.pdf", "metadata": {"source": "CRF", "title": "Minimizing Quadratic Functions in Constant Time", "authors": ["Kohei Hayashi"], "emails": ["hayashi.kohei@gmail.com", "yyoshida@nii.ac.jp"], "sections": [{"heading": null, "text": "ar X\niv :1\n60 8.\n07 17\n9v 1\n[ cs\n.L G\n] 2\n5 A"}, {"heading": "1 Introduction", "text": "A quadratic function is one of the most important function classes in machine learning, statistics, and data mining. Many fundamental problems such as linear regression, k-means clustering, principal component analysis, support vector machines, and kernel methods [14] can be formulated as a minimization problem of a quadratic function.\nIn some applications, it is sufficient to compute the minimum value of a quadratic function rather than its solution. For example, Yamada et al. [21] proposed an efficient method for estimating the Pearson divergence, which provides useful information about data, such as the density ratio [18]. They formulated the estimation problem as the minimization of a squared loss and showed that the Pearson divergence can be estimated from the minimum value. The least-squares mutual information [19] is another example that can be computed in a similar manner.\nDespite its importance, the minimization of a quadratic function has the issue of scalability. Let n \u2208 N be the number of variables (the \u201cdimension\u201d of the problem). In general, such a minimization problem can be solved by quadratic programming (QP), which requires poly(n) time. If the problem is convex and there are no constraints, then the problem is reduced to solving a system of linear equations, which requires O(n3) time. Both methods easily become infeasible, even for mediumscale problems, say, n > 10000.\nAlthough several techniques have been proposed to accelerate quadratic function minimization, they require at least linear time in n. This is problematic when handling problems with an ultrahigh dimension, for which even linear time is slow or prohibitive. For example, stochastic gradient descent (SGD) is an optimization method that is widely used for large-scale problems. A nice property of this method is that, if the objective function is strongly convex, it outputs a point that is sufficiently close to an optimal solution after a constant number of iterations [5]. Nevertheless, in each iteration, we need at least O(n) time to access the variables. Another technique is lowrank approximation such as Nystro\u0308m\u2019s method [20]. The underlying idea is the approximation of the problem by using a low-rank matrix, and by doing so, we can drastically reduce the time\n29th Conference on Neural Information Processing Systems (NIPS 2016), Barcelona, Spain.\ncomplexity. However, we still need to compute the matrix\u2013vector product of size n, which requires O(n) time. Clarkson et al. [7] proposed sublinear-time algorithms for special cases of quadratic function minimization. However, it is \u201csublinear\u201d with respect to the number of pairwise interactions of the variables, which is O(n2), and their algorithms require O(n logc n) time for some c \u2265 1.\nOur contributions: Let A \u2208 Rn\u00d7n be a matrix and d, b \u2208 Rn be vectors. Then, we consider the following quadratic problem:\nminimize v\u2208Rn pn,A,d,b(v), where pn,A,d,b(v) = \u3008v, Av\u3009 + n\u3008v, diag(d)v\u3009+ n\u3008b,v\u3009. (1)\nHere, \u3008\u00b7, \u00b7\u3009 denotes the inner product and diag(d) denotes the matrix whose diagonal entries are specified by d. Note that a constant term can be included in (1); however, it is irrelevant when optimizing (1), and hence we ignore it.\nLet z\u2217 \u2208 R be the optimal value of (1) and let \u01eb, \u03b4 \u2208 (0, 1) be parameters. Then, the main goal of this paper is the computation of z with |z\u2212 z\u2217| = O(\u01ebn2) with probability at least 1\u2212 \u03b4 in constant time, that is, independent of n. Here, we assume the real RAM model [6], in which we can perform basic algebraic operations on real numbers in one step. Moreover, we assume that we have query accesses to A, b, and d, with which we can obtain an entry of them by specifying an index. We note that z\u2217 is typically \u0398(n2) because \u3008v, Av\u3009 consists of \u0398(n2) terms, and \u3008v, diag(d)v\u3009 and \u3008b,v\u3009 consist of \u0398(n) terms. Hence, we can regard the error of \u0398(\u01ebn2) as an error of \u0398(\u01eb) for each term, which is reasonably small in typical situations.\nLet \u00b7|S be an operator that extracts a submatrix (or subvector) specified by an index set S \u2282 N; then, our algorithm is defined as follows, where the parameter k := k(\u01eb, \u03b4) will be determined later.\nAlgorithm 1\nInput: An integer n \u2208 N, query accesses to the matrix A \u2208 Rn\u00d7n and to the vectors d, b \u2208 Rn, and \u01eb, \u03b4 > 0\n1: S \u2190 a sequence of k = k(\u01eb, \u03b4) indices independently and uniformly sampled from {1, 2, . . . , n}. 2: return n 2\nk2 minv\u2208Rn pk,A|S,d|S,b|S (v).\nIn other words, we sample a constant number of indices from the set {1, 2, . . . , n}, and then solve the problem (1) restricted to these indices. Note that the number of queries and the time complexity are O(k2) and poly(k), respectively. In order to analyze the difference between the optimal values of pn,A,d,b and pk,A|S ,d|S,b|S , we want to measure the \u201cdistances\u201d between A and A|S , d and d|S , and b and b|S , and want to show them small. To this end, we exploit graph limit theory, initiated by Lova\u0301sz and Szegedy [11] (refer to [10] for a book), in which we measure the distance between two graphs on different number of vertices by considering continuous versions. Although the primary interest of graph limit theory is graphs, we can extend the argument to analyze matrices and vectors.\nUsing synthetic and real settings, we demonstrate that our method is orders of magnitude faster than standard polynomial-time algorithms and that the accuracy of our method is sufficiently high.\nRelated work: Several constant-time approximation algorithms are known for combinatorial optimization problems such as the max cut problem on dense graphs [8, 13], constraint satisfaction problems [1, 22], and the vertex cover problem [15, 16, 25]. However, as far as we know, no such algorithm is known for continuous optimization problems.\nA related notion is property testing [9, 17], which aims to design constant-time algorithms that distinguish inputs satisfying some predetermined property from inputs that are \u201cfar\u201d from satisfying it. Characterizations of constant-time testable properties are known for the properties of a dense graph [2, 3] and the affine-invariant properties of a function on a finite field [23, 24].\nOrganization In Section 2, we introduce the basic notions from graph limit theory. In Section 3, we show that we can obtain a good approximation to (a continuous version of) a matrix by sampling a constant-size submatrix in the sense that the optimizations over the original matrix and the submatrix are essentially equivalent. Using this fact, we prove the correctness of Algorithm 1 in Section 4. We show our experimental results in Section 5."}, {"heading": "2 Preliminaries", "text": "For an integer n, let [n] denote the set {1, 2, . . . , n}. The notation a = b\u00b1 c means that b\u2212 c \u2264 a \u2264 b+ c. In this paper, we only consider functions and sets that are measurable.\nLet S = (x1, . . . , xk) be a sequence of k indices in [n]. For a vector v \u2208 Rn, we denote the restriction of v to S by v|S \u2208 Rk; that is, (v|S)i = vxi for every i \u2208 [k]. For the matrix A \u2208 R\nn\u00d7n, we denote the restriction of A to S by A|S \u2208 Rk\u00d7k; that is, (A|S)ij = Axixj for every i, j \u2208 [k]."}, {"heading": "2.1 Dikernels", "text": "Following [12], we call a (measurable) function f : [0, 1]2 \u2192 R a dikernel. A dikernel is a generalization of a graphon [11], which is symmetric and whose range is bounded in [0, 1]. We can regard a dikernel as a matrix whose index is specified by a real value in [0, 1]. We stress that the term dikernel has nothing to do with kernel methods. For two functions f, g : [0, 1] \u2192 R, we define their inner product as \u3008f, g\u3009 = \u222b 1 0 f(x)g(x)dx. For a dikernel W : [0, 1]2 \u2192 R and a function f : [0, 1] \u2192 R, we define a function Wf : [0, 1] \u2192 R as (Wf)(x) = \u3008W (x, \u00b7), f\u3009.\nLet W : [0, 1]2 \u2192 R be a dikernel. The Lp norm \u2016W\u2016p for p \u2265 1 and the cut norm \u2016W\u2016 of W are defined as \u2016W\u2016p = (\u222b 1\n0 \u222b 1 0 |W (x, y)|pdxdy )1/p and \u2016W\u2016 = supS,T\u2286[0,1] \u2223\u2223\u2223 \u222b S \u222b T W (x, y)dxdy \u2223\u2223\u2223, respectively, where the supremum is over all pairs of subsets. We note that these norms satisfy the triangle inequalities and \u2016W\u2016 \u2264 \u2016W\u20161.\nLet \u03bb be a Lebesgue measure. A map \u03c0 : [0, 1] \u2192 [0, 1] is said to be measure-preserving, if the pre-image \u03c0\u22121(X) is measurable for every measurable set X , and \u03bb(\u03c0\u22121(X)) = \u03bb(X). A measure-preserving bijection is a measure-preserving map whose inverse map exists and is also measurable (and then also measure-preserving). For a measure preserving bijection \u03c0 : [0, 1] \u2192 [0, 1] and a dikernel W : [0, 1]2 \u2192 R, we define the dikernel \u03c0(W ) : [0, 1]2 \u2192 R as \u03c0(W )(x, y) = W (\u03c0(x), \u03c0(y))."}, {"heading": "2.2 Matrices and Dikernels", "text": "Let W : [0, 1]2 \u2192 R be a dikernel and S = (x1, . . . , xk) be a sequence of elements in [0, 1]. Then, we define the matrix W |S \u2208 Rk\u00d7k so that (W |S)ij = W (xi, xj).\nWe can construct the dikernel A\u0302 : [0, 1]2 \u2192 R from the matrix A \u2208 Rn\u00d7n as follows. Let I1 = [0, 1n ], I2 = ( 1 n , 2 n ], . . . , In = ( n\u22121 n , . . . , 1]. For x \u2208 [0, 1], we define in(x) \u2208 [n] as a unique integer such that x \u2208 Ii. Then, we define A\u0302(x, y) = Ain(x)in(y). The main motivation for creating a dikernel from a matrix is that, by doing so, we can define the distance between two matrices A and B of different sizes via the cut norm, that is, \u2016A\u0302\u2212 B\u0302\u2016 .\nWe note that the distribution of A|S , where S is a sequence of k indices that are uniformly and independently sampled from [n] exactly matches the distribution of A\u0302|S , where S is a sequence of k elements that are uniformly and independently sampled from [0, 1]."}, {"heading": "3 Sampling Theorem and the Properties of the Cut Norm", "text": "In this section, we prove the following theorem, which states that, given a sequence of dikernels W 1, . . . ,WT : [0, 1]2 \u2192 [\u2212L,L], we can obtain a good approximation to them by sampling a sequence of a small number of elements in [0, 1]. Formally, we prove the following:\nTheorem 3.1. Let W 1, . . . ,WT : [0, 1]2 \u2192 [\u2212L,L] be dikernels. Let S be a sequence of k elements uniformly and independently sampled from [0, 1]. Then, with a probability of at least 1\u2212 exp(\u2212\u2126(kT/ log2 k)), there exists a measure-preserving bijection \u03c0 : [0, 1] \u2192 [0, 1] such that, for any functions f, g : [0, 1] \u2192 [\u2212K,K] and t \u2208 [T ], we have\n|\u3008f,W tg\u3009 \u2212 \u3008f, \u03c0(W\u0302 t|S)g\u3009| = O ( LK2 \u221a T/ log2 k ) .\nWe start with the following lemma, which states that, if a dikernel W : [0, 1]2 \u2192 R has a small cut norm, then \u3008f,Wf\u3009 is negligible no matter what f is. Hence, we can focus on the cut norm when proving Theorem 3.1.\nLemma 3.2. Let \u01eb \u2265 0 and W : [0, 1]2 \u2192 R be a dikernel with \u2016W\u2016 \u2264 \u01eb. Then, for any functions f, g : [0, 1] \u2192 [\u2212K,K], we have |\u3008f,Wg\u3009| \u2264 \u01ebK2.\nProof. For \u03c4 \u2208 R and the function h : [0, 1] \u2192 R, let L\u03c4 (h) := {x \u2208 [0, 1] | h(x) = \u03c4} be the level set of h at \u03c4 . For f \u2032 = f/K and g\u2032 = g/K , we have\n|\u3008f,Wg\u3009| = K2|\u3008f \u2032,Wg\u2032\u3009| = K2 \u2223\u2223\u2223 \u222b 1\n\u22121\n\u222b 1\n\u22121\n\u03c41\u03c42\n\u222b\nL\u03c41(f \u2032)\n\u222b\nL\u03c42 (g \u2032)\nW (x, y)dxdyd\u03c41d\u03c42 \u2223\u2223\u2223\n\u2264 K2 \u222b 1\n\u22121\n\u222b 1\n\u22121\n|\u03c41||\u03c42| \u2223\u2223\u2223\u2223\u2223 \u222b\nL\u03c41(f \u2032)\n\u222b\nL\u03c42(g \u2032)\nW (x, y)dxdy \u2223\u2223\u2223\u2223\u2223 d\u03c41d\u03c42\n\u2264 \u01ebK2 \u222b 1\n\u22121\n\u222b 1\n\u22121\n|\u03c41||\u03c42|d\u03c41d\u03c42 = \u01ebK 2.\nTo introduce the next technical tool, we need several definitions. We say that the partition Q is a refinement of the partitionP = (V1, . . . , Vp) if Q is obtained by splitting each set Vi into one or more parts. The partition P = (V1, . . . , Vp) of the interval [0, 1] is called an equipartition if \u03bb(Vi) = 1/p for every i \u2208 [p]. For the dikernel W : [0, 1]2 \u2192 R and the equipartition P = (V1, . . . , Vp) of [0, 1], we define WP : [0, 1]2 \u2192 R as the function obtained by averaging each Vi \u00d7 Vj for i, j \u2208 [p]. More formally, we define\nWP (x, y) = 1\n\u03bb(Vi)\u03bb(Vj)\n\u222b\nVi\u00d7Vj\nW (x\u2032, y\u2032)dx\u2032dy\u2032 = p2 \u222b\nVi\u00d7Vj\nW (x\u2032, y\u2032)dx\u2032dy\u2032,\nwhere i and j are unique indices such that x \u2208 Vi and y \u2208 Vj , respectively.\nThe following lemma states that any function W : [0, 1]2 \u2192 R can be well approximated by WP for the equipartition P into a small number of parts.\nLemma 3.3 (Weak regularity lemma for functions on [0, 1]2 [8]). Let P be an equipartition of [0, 1] into k sets. Then, for any dikernel W : [0, 1]2 \u2192 R and \u01eb > 0, there exists a refinement Q of P with |Q| \u2264 k2C/\u01eb 2 for some constant C > 0 such that\n\u2016W \u2212WQ\u2016 \u2264 \u01eb\u2016W\u20162.\nCorollary 3.4. Let W 1, . . . ,WT : [0, 1]2 \u2192 R be dikernels. Then, for any \u01eb > 0, there exists an equipartition P into |P| \u2264 2CT/\u01eb 2 parts for some constant C > 0 such that, for every t \u2208 [T ],\n\u2016W t \u2212W tP\u2016 \u2264 \u01eb\u2016W t\u20162.\nProof. Let P0 be a trivial partition, that is, a partition consisting of a single part [n]. Then, for each t \u2208 [T ], we iteratively apply Lemma 3.3 with Pt\u22121, W t, and \u01eb, and we obtain the partition Pt into at most |Pt\u22121|2C/\u01eb 2\nparts such that \u2016W t \u2212W tPt\u2016 \u2264 \u01eb\u2016W t\u20162. Since Pt is a refinement of Pt\u22121,\nwe have \u2016W i \u2212W iPt\u2016 \u2264 \u2016W i \u2212W iPt\u22121\u2016 for every i \u2208 [t \u2212 1]. Then, P T satisfies the desired property with |PT | \u2264 (2C/\u01eb 2 )T = 2CT/\u01eb 2 .\nAs long as S is sufficiently large, W and W\u0302 |S are close in the cut norm:\nLemma 3.5 ((4.15) of [4]). Let W : [0, 1]2 \u2192 [\u2212L,L] be a dikernel and S be a sequence of k elements uniformly and independently sampled from [0, 1]. Then, we have\n\u2212 2L\nk \u2264 ES\u2016W\u0302 |S\u2016 \u2212 \u2016W\u2016 <\n8L\nk1/4 .\nFinally, we need the following concentration inequality.\nLemma 3.6 (Azuma\u2019s inequality). Let (\u2126, A, P ) be a probability space, k be a positive integer, and C > 0. Let z = (z1, . . . , zk), where z1, . . . , zk are independent random variables, and zi takes values in some measure space (\u2126i, Ai). Let f : \u21261 \u00d7 \u00b7 \u00b7 \u00b7 \u00d7 \u2126k \u2192 R be a function. Suppose that |f(x)\u2212 f(y)| \u2264 C whenever x and y only differ in one coordinate. Then\nPr [ |f(z)\u2212Ez[f(z)]| > \u03bbC ] < 2e\u2212\u03bb 2/2k.\nNow we prove the counterpart of Theorem 3.1 for the cut norm.\nLemma 3.7. Let W 1, . . . ,WT : [0, 1]2 \u2192 [\u2212L,L] be dikernels. Let S be a sequence of k elements uniformly and independently sampled from [0, 1]. Then, with a probability of at least 1\u2212 exp(\u2212\u2126(kT/ log2 k)), there exists a measure-preserving bijection \u03c0 : [0, 1] \u2192 [0, 1] such that, for every t \u2208 [T ], we have\n\u2016W t \u2212 \u03c0(W\u0302 t|S)\u2016 = O ( L \u221a T/ log2 k ) .\nProof. First, we bound the expectations and then prove their concentrations. We apply Corollary 3.4 to W 1, . . . ,WT and \u01eb, and let P = (V1, . . . , Vp) be the obtained partition with p \u2264 2CT/\u01eb 2\nparts such that\n\u2016W t \u2212W tP\u2016 \u2264 \u01ebL.\nfor every t \u2208 [T ]. By Lemma 3.5, for every t \u2208 [T ], we have\nES\u2016W\u0302 tP |S \u2212 W\u0302 t|S\u2016 = ES\u2016(W t P \u2212W t)|S\u0302\u2016 \u2264 \u01ebL+ 8L\nk1/4 .\nThen, for any measure-preserving bijection \u03c0 : [0, 1] \u2192 [0, 1] and t \u2208 [T ], we have\nES\u2016W t \u2212 \u03c0(W\u0302 t|S)\u2016 \u2264 \u2016W t \u2212W tP\u2016 +ES\u2016W t P \u2212 \u03c0(W\u0302 t P |S)\u2016 +ES\u2016\u03c0(W\u0302 t P |S)\u2212 \u03c0(W\u0302 t|S)\u2016\n\u2264 2\u01ebL+ 8L\nk1/4 +ES\u2016W\nt P \u2212 \u03c0(W\u0302 t P |S)\u2016 . (2)\nThus, we are left with the problem of sampling from P . Let S = {x1, . . . , xk} be a sequence of independent random variables that are uniformly distributed in [0, 1], and let Zi be the number of points xj that fall into the set Vi. It is easy to compute that\nE[Zi] = k\np and Var[Zi] = (1 p \u2212 1 p2 ) k < k p .\nThe partition P \u2032 of [0, 1] is constructed into the sets V \u20321 , . . . , V \u2032 p such that \u03bb(V \u2032 i ) = Zi/k and \u03bb(Vi \u2229 V \u2032i ) = min(1/p, Zi/k). For each t \u2208 [T ], we construct the dikernel W t : [0, 1] \u2192 R such that the value of W t\non V \u2032i \u00d7 V \u2032 j is the same as the value of W t P on Vi \u00d7 Vj . Then, W t agrees with W tP on\nthe set Q = \u22c3\ni,j\u2208[p](Vi\u2229V \u2032 i )\u00d7(Vj\u2229V \u2032 j ). Then, there exists a bijection \u03c0 such that \u03c0(W\u0302 t P |S) = W\nt\nfor each t \u2208 [T ]. Then, for every t \u2208 [T ], we have\n\u2016W tP \u2212 \u03c0(W\u0302 t P |S)\u2016 = \u2016W t P \u2212W\nt \u2016 \u2264 \u2016W\nt P \u2212W t \u20161 \u2264 2L(1\u2212 \u03bb(Q))\n= 2L ( 1\u2212 (\u2211\ni\u2208[p]\nmin (1 p , Zi k ))2) \u2264 4L ( 1\u2212 \u2211\ni\u2208[p]\nmin (1 p , Zi k ))\n= 2L \u2211\ni\u2208[p]\n\u2223\u2223\u22231 p \u2212 Zi k \u2223\u2223\u2223 \u2264 2L ( p \u2211\ni\u2208[p]\n(1 p \u2212 Zi k )2)1/2 ,\nwhich we rewrite as\n\u2016W tP \u2212 \u03c0(W\u0302 t P |S)\u2016 2 \u2264 4L2p\n\u2211\ni\u2208[p]\n(1 p \u2212 Zi k )2 .\nThe expectation of the right hand side is (4L2p/k2) \u2211\ni\u2208[p] Var(Zi) < 4L 2p/k. By the Cauchy-\nSchwartz inequality, E\u2016W tP \u2212 \u03c0(W\u0302 t P |S)\u2016 \u2264 2L\n\u221a p/k.\nInserted this into (2), we obtain\nE\u2016W t \u2212 \u03c0(W\u0302 t|S)\u2016 \u2264 2\u01ebL+ 8L\nk1/4 + 2L\n\u221a p\nk \u2264 2\u01ebL+\n8L\nk1/4 +\n2L\nk1/2 2CT/\u01eb\n2\n.\nChoosing \u01eb = \u221a CT/(log2 k 1/4) = \u221a 4CT/(log2 k), we obtain the upper bound\nE\u2016W t \u2212 \u03c0(W\u0302 t|S)\u2016 \u2264 2L\n\u221a 4CT\nlog2 k +\n8L\nk1/4 +\n2L\nk1/4 = O\n( L\n\u221a T\nlog2 k\n) .\nObserving that \u2016W t \u2212 \u03c0(W\u0302 t|S)\u2016 changes by at most O(L/k) if one element in S changes, we apply Azuma\u2019s inequality with \u03bb = k \u221a T/ log2 k and the union bound to complete the proof.\nThe proof of Theorem 3.1 is immediately follows from Lemmas 3.2 and 3.7."}, {"heading": "4 Analysis of Algorithm 1", "text": "In this section, we analyze Algorithm 1. Because we want to use dikernels for the analysis, we introduce a continuous version of pn,A,d,b (recall (1)). The real-valued function Pn,A,d,b on the functions f : [0, 1] \u2192 R is defined as\nPn,A,d,b(f) = \u3008f, A\u0302f\u3009+ \u3008f 2, d\u03021\u22a41\u3009+ \u3008f, b\u03021\u22a41\u3009,\nwhere f2 : [0, 1] \u2192 R is a function such that f2(x) = f(x)2 for every x \u2208 [0, 1] and 1 : [0, 1] \u2192 R is the constant function that has a value of 1 everywhere. The following lemma states that the minimizations of pn,A,d,b and Pn,A,d,b are equivalent:\nLemma 4.1. Let A \u2208 Rn\u00d7n be a matrix and d, b \u2208 Rn\u00d7n be vectors. Then, we have\nmin v\u2208[\u2212K,K]n\npn,A,d,b(v) = n 2 \u00b7 inf\nf :[0,1]\u2192[\u2212K,K] Pn,A,d,b(f).\nfor any K > 0.\nProof. First, we show that n2 \u00b7 inff :[0,1]\u2192[\u2212K,K] Pn,A,d,b(f) \u2264 minv\u2208[\u2212K,K]n pn,A,d,b(v). Given a vector v \u2208 [\u2212K,K]n, we define f : [0, 1] \u2192 [\u2212K,K] as f(x) = vin(x). Then,\n\u3008f, A\u0302f\u3009 = \u2211\ni,j\u2208[n]\n\u222b\nIi\n\u222b\nIj\nAijf(x)f(y)dxdy = 1\nn2\n\u2211\ni,j\u2208[n]\nAijvivj = 1\nn2 \u3008v, Av\u3009,\n\u3008f2, d\u03021\u22a41\u3009 = \u2211\ni,j\u2208[n]\n\u222b\nIi\n\u222b\nIj\ndif(x) 2dxdy =\n\u2211\ni\u2208[n]\n\u222b\nIi\ndif(x) 2dx =\n1\nn\n\u2211\ni\u2208[n]\ndiv 2 i =\n1 n \u3008v, diag(d)v\u3009,\n\u3008f, b\u03021\u22a41\u3009 = \u2211\ni,j\u2208[n]\n\u222b\nIi\n\u222b\nIj\nbif(x)dxdy = \u2211\ni\u2208[n]\n\u222b\nIi\nbif(x)dx = 1\nn\n\u2211\ni\u2208[n]\nbivi = 1\nn \u3008v, b\u3009.\nThen, we have n2Pn,A,d,b(f) \u2264 pn,A,d,b(v).\nNext, we show that minv\u2208[\u2212K,K]n pn,A,d,b(v) \u2264 n2 \u00b7 inff :[0,1]\u2192[\u2212K,K] Pn,A,d,b(f). Let f : [0, 1] \u2192 [\u2212K,K] be a measurable function. Then, for x \u2208 [0, 1], we have\n\u2202Pn,A,d,b(f(x))\n\u2202f(x) =\n\u2211\ni\u2208[n]\n\u222b\nIi\nAiin(x)f(y)dy + \u2211\nj\u2208[n]\n\u222b\nIj\nAin(x)jf(y)dy + 2din(x)f(x) + bin(x).\nNote that the form of this partial derivative only depends on in(x); hence, in the optimal solution f\u2217 : [0, 1] \u2192 [\u2212K,K], we can assume f\u2217(x) = f\u2217(y) if in(x) = in(y). In other words, f\u2217 is constant on each of the intervals I1, . . . , In. For such f\u2217, we define the vector v \u2208 Rn as vi = f \u2217(x), where x \u2208 [0, 1] is any element in Ii. Then, we have\n\u3008v, Av\u3009 = \u2211\ni,j\u2208[n]\nAijvivj = n 2\n\u2211\ni,j\u2208[n]\n\u222b\nIi\n\u222b\nIj\nAijf \u2217(x)f\u2217(y)dxdy = n2\u3008f\u2217, A\u0302f\u2217\u3009,\n\u3008v, diag(d)v\u3009 = \u2211\ni\u2208[n]\ndiv 2 i = n\n\u2211\ni\u2208[n]\n\u222b\nIi\ndif \u2217(x)2dx = n\u3008(f\u2217)2, d\u03021T 1\u3009,\n\u3008v, b\u3009 = \u2211\ni\u2208[n]\nbivi = n \u2211\ni\u2208[n]\n\u222b\nIi\nbif \u2217(x)dx = n\u3008f\u2217, b\u03021T 1\u3009.\nFinally, we have pn,A,d,b(v) \u2264 n2Pn,A,d,b(f\u2217).\nNow we show that Algorithm 1 well-approximates the optimal value of (1) in the following sense:\nTheorem 4.2. Let v\u2217 and z\u2217 be an optimal solution and the optimal value, respectively, of problem (1). By choosing k(\u01eb, \u03b4) = 2\u0398(1/\u01eb\n2) + \u0398(log 1\u03b4 log log 1 \u03b4 ), with a probability of at least\n1 \u2212 \u03b4, a sequence S of k indices independently and uniformly sampled from [n] satisfies the following: Let v\u0303\u2217 and z\u0303\u2217 be an optimal solution and the optimal value, respectively, of the problem minv\u2208Rk pk,A|S ,d|S,b|S (v). Then, we have\n\u2223\u2223\u2223n 2\nk2 z\u0303\u2217 \u2212 z\u2217\n\u2223\u2223\u2223 \u2264 \u01ebLK2n2,\nwhere K = max{maxi\u2208[n] |v\u2217i |,maxi\u2208[n] |v\u0303 \u2217 i |} and L = max{maxi,j |Aij |,maxi |di|,maxi |bi|}.\nProof. We instantiate Theorem 3.1 with k = 2\u0398(1/\u01eb 2) + \u0398(log 1\u03b4 log log 1 \u03b4 ) and the dikernels A\u0302, d\u03021\u22a4, and b\u03021\u22a4. Then, with a probability of at least 1\u2212 \u03b4, there exists a measure preserving bijection \u03c0 : [0, 1] \u2192 [0, 1] such that\nmax { |\u3008f, (A\u0302\u2212 \u03c0(A\u0302|S))f\u3009|, |\u3008f 2, (d\u03021\u22a4 \u2212 \u03c0(d\u03021\u22a4|S))1\u3009|, |\u3008f, (b\u03021\u22a4 \u2212 \u03c0(b\u03021\u22a4|S))1\u3009| } \u2264 \u01ebLK2\n3\nfor any function f : [0, 1] \u2192 [\u2212K,K]. Then, we have\nz\u0303\u2217 = min v\u2208Rk pk,A|S ,d|S,b|S(v) = min v\u2208[\u2212K,K]k pk,A|S,d|S ,b|S(v)\n= k2 \u00b7 inf f :[0,1]\u2192[\u2212K,K] Pk,A|S ,d|S,b|S(f) (By Lemma 4.1)\n= k2 \u00b7 inf f :[0,1]\u2192[\u2212K,K]\n( \u3008f, (\u03c0(A\u0302|S)\u2212 A\u0302)f\u3009+ \u3008f, A\u0302f\u3009+ \u3008f 2, (\u03c0(d\u03021\u22a4|S)\u2212 d\u03021\u22a4)1\u3009+ \u3008f 2, d\u03021\u22a41\u3009+\n\u3008f, (\u03c0(b\u03021\u22a4|S)\u2212 b\u03021\u22a4)1\u3009+ \u3008f, b\u03021\u22a41\u3009 )\n\u2264 k2 \u00b7 inf f :[0,1]\u2192[\u2212K,K]\n( \u3008f, A\u0302f\u3009+ \u3008f2, d\u03021\u22a41\u3009+ \u3008f, b\u03021\u22a41\u3009 \u00b1 \u01ebLK2 )\n= k2\nn2 \u00b7 min v\u2208[\u2212K,K]n\npn,A,d,b(v)\u00b1 \u01ebLK 2k2. (By Lemma 4.1)\n= k2\nn2 \u00b7 min v\u2208Rn\npn,A,d,b(v)\u00b1 \u01ebLK 2k2 =\nk2 n2 z\u2217 \u00b1 \u01ebLK2k2.\nRearranging the inequality, we obtain the desired result.\nWe can show that K is bounded when A is symmetric and full rank. To see this, we first note that we can assume A + ndiag(d) is positive-definite, as otherwise pn,A,d,b is not bounded and the problem is uninteresting. Then, for any set S \u2286 [n] of k indices, (A + ndiag(d))|S is again positive-definite because it is a principal submatrix. Hence, we have v\u2217 = (A + ndiag(d))\u22121nb/2 and v\u0303\u2217 = (A|S + ndiag(d|S))\u22121nb|S/2, which means that K is bounded."}, {"heading": "5 Experiments", "text": "In this section, we demonstrate the effectiveness of our method by experiment. All experiments were conducted on an Amazon EC2 c3.8xlarge instance. Error bars indicate the standard deviations over ten trials with different random seeds.\nNumerical simulation We investigated the actual relationships between n, k, and \u01eb. To this end, we prepared synthetic data as follows. We randomly generated inputs as Aij \u223c U[\u22121,1], di \u223c U[0,1], and bi \u223c U[\u22121,1] for i, j \u2208 [n], where U[a,b] denotes the uniform distribution with the support [a, b]. After that, we solved (1) by using Algorithm 1 and compared it with the exact solution obtained by QP.1 The result (Figure 1) show the approximation errors were evenly controlled regardless of n, which meets the error analysis (Theorem 4.2).\nApplication to kernel methods Next, we considered the kernel approximation of the Pearson divergence [21]. The problem is defined as follows. Suppose we have the two different data sets x = (x1, . . . , xn) \u2208 R n and x\u2032 = (x\u20321, . . . , x \u2032 n\u2032) \u2208 R n\u2032 where n, n\u2032 \u2208 N. Let H \u2208 Rn\u00d7n be a gram matrix such that Hl,m = \u03b1n \u2211n i=1 \u03c6(xi, xl)\u03c6(xi, xm) + 1\u2212\u03b1 n\u2032 \u2211n\u2032 j=1 \u03c6(x \u2032 j , xl)\u03c6(x \u2032 j , xm), where \u03c6(\u00b7, \u00b7) is a kernel function and \u03b1 \u2208 (0, 1) is a parameter. Also, let h \u2208 Rn be a vector such that hl = 1n \u2211n i=1 \u03c6(xi, xl). Then, an estimator of the \u03b1-relative Pearson divergence between the distributions of x and x\u2032 is obtained by \u2212 12 \u2212 minv\u2208Rn 1 2 \u3008v, Hv\u3009 \u2212 \u3008h,v\u3009 + \u03bb 2 \u3008v,v\u3009. Here, \u03bb > 0 is a regularization parameter. In this experiment, we used the Gaussian kernel \u03c6(x, y) = exp((x\u2212 y)2/2\u03c32) and set n\u2032 = 200 and \u03b1 = 0.5; \u03c32 and \u03bb were chosen by 5-fold cross-validation as suggested in [21]. We randomly generated the data sets as xi \u223c N(1, 0.5) for i \u2208 [n] and x\u2032j \u223c N(1.5, 0.5) for j \u2208 [n\n\u2032] where N(\u00b5, \u03c32) denotes the Gaussian distribution with mean \u00b5 and variance \u03c32.\nWe encoded this problem into (1) by setting A = 12H , b = \u2212h, and d = \u03bb 2n1n, where 1n denotes the n-dimensional vector whose elements are all one. After that, given k, we computed the second step of Algorithm 1 with the pseudoinverse of A|S+kdiag(d|S). Absolute approximation errors and runtimes were compared with Nystro\u0308m\u2019s method whose approximated rank was set to k. In terms of accuracy, our method clearly outperformed Nystro\u0308m\u2019s method (Table 2). In addition, the runtimes of our method were nearly constant, whereas the runtimes of Nystro\u0308m\u2019s method grew linearly in k (Table 1).\n1We used GLPK (https://www.gnu.org/software/glpk/) for the QP solver."}, {"heading": "6 Acknowledgments", "text": "We would like to thank Makoto Yamada for suggesting a motivating problem of our method. K. H. is supported by MEXT KAKENHI 15K16055. Y. Y. is supported by MEXT Grant-in-Aid for Scientific Research on Innovative Areas (No. 24106001), JST, CREST, Foundations of Innovative Algorithms for Big Data, and JST, ERATO, Kawarabayashi Large Graph Project."}], "references": [{"title": "Random sampling and approximation of MAX- CSP problems", "author": ["N. Alon", "W.F. de la Vega", "R. Kannan", "M. Karpinski"], "venue": "In STOC,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2002}, {"title": "A combinatorial characterization of the testable graph properties: It\u2019s all about regularity", "author": ["N. Alon", "E. Fischer", "I. Newman", "A. Shapira"], "venue": "SIAM Journal on Computing,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2009}, {"title": "Graph limits and parameter testing", "author": ["C. Borgs", "J. Chayes", "L. Lov\u00e1sz", "V.T. S\u00f3s", "B. Szegedy", "K. Vesztergombi"], "venue": "In STOC,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2006}, {"title": "Convergent sequences of dense graphs i: Subgraph frequencies, metric properties and testing", "author": ["C. Borgs", "J.T. Chayes", "L. Lov\u00e1sz", "V.T. S\u00f3s", "K. Vesztergombi"], "venue": "Advances in Mathematics,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2008}, {"title": "Stochastic learning", "author": ["L. Bottou"], "venue": "In Advanced Lectures on Machine Learning,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2004}, {"title": "Feasible real random access machines", "author": ["V. Brattka", "P. Hertling"], "venue": "Journal of Complexity,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 1998}, {"title": "Sublinear optimization for machine learning", "author": ["K.L. Clarkson", "E. Hazan", "D.P. Woodruff"], "venue": "Journal of the ACM,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2012}, {"title": "The regularity lemma and approximation schemes for dense problems", "author": ["A. Frieze", "R. Kannan"], "venue": "In FOCS,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 1996}, {"title": "Property testing and its connection to learning and approximation", "author": ["O. Goldreich", "S. Goldwasser", "D. Ron"], "venue": "Journal of the ACM,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 1998}, {"title": "Large Networks and Graph Limits", "author": ["L. Lov\u00e1sz"], "venue": "American Mathematical Society,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2012}, {"title": "Limits of dense graph sequences", "author": ["L. Lov\u00e1sz", "B. Szegedy"], "venue": "Journal of Combinatorial Theory, Series B,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2006}, {"title": "Non-deterministic graph property testing", "author": ["L. Lov\u00e1sz", "K. Vesztergombi"], "venue": "Combinatorics, Probability and Computing,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2013}, {"title": "Yet another algorithm for dense max cut: go greedy", "author": ["C. Mathieu", "W. Schudy"], "venue": "In SODA,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2008}, {"title": "Machine learning: a probabilistic perspective", "author": ["K.P. Murphy"], "venue": null, "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2012}, {"title": "Constant-time approximation algorithms via local improvements", "author": ["H.N. Nguyen", "K. Onak"], "venue": "In FOCS,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2008}, {"title": "A near-optimal sublinear-time algorithm for approximating the minimum vertex cover size", "author": ["K. Onak", "D. Ron", "M. Rosen", "R. Rubinfeld"], "venue": "In SODA,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2012}, {"title": "Robust characterizations of polynomials with applications to program testing", "author": ["R. Rubinfeld", "M. Sudan"], "venue": "SIAM Journal on Computing,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 1996}, {"title": "Density Ratio Estimation in Machine Learning", "author": ["M. Sugiyama", "T. Suzuki", "T. Kanamori"], "venue": null, "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2012}, {"title": "Least-Squares Independent Component Analysis", "author": ["T. Suzuki", "M. Sugiyama"], "venue": "Neural Computation,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2011}, {"title": "Using the nystr\u00f6m method to speed up kernel machines", "author": ["C.K.I. Williams", "M. Seeger"], "venue": "In NIPS,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2001}, {"title": "Relative density-ratio estimation for robust distribution comparison", "author": ["M. Yamada", "T. Suzuki", "T. Kanamori", "H. Hachiya", "M. Sugiyama"], "venue": "In NIPS,", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2011}, {"title": "Optimal constant-time approximation algorithms and (unconditional) inapproximability results for every bounded-degree CSP", "author": ["Y. Yoshida"], "venue": "In STOC,", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2011}, {"title": "A characterization of locally testable affine-invariant properties via decomposition theorems", "author": ["Y. Yoshida"], "venue": "In STOC,", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2014}, {"title": "Gowers norm, function limits, and parameter estimation", "author": ["Y. Yoshida"], "venue": "In SODA,", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2016}, {"title": "Improved constant-time approximation algorithms for maximum matchings and other optimization problems", "author": ["Y. Yoshida", "M. Yamamoto", "H. Ito"], "venue": "SIAM Journal on Computing,", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2012}], "referenceMentions": [{"referenceID": 13, "context": "Many fundamental problems such as linear regression, k-means clustering, principal component analysis, support vector machines, and kernel methods [14] can be formulated as a minimization problem of a quadratic function.", "startOffset": 147, "endOffset": 151}, {"referenceID": 20, "context": "[21] proposed an efficient method for estimating the Pearson divergence, which provides useful information about data, such as the density ratio [18].", "startOffset": 0, "endOffset": 4}, {"referenceID": 17, "context": "[21] proposed an efficient method for estimating the Pearson divergence, which provides useful information about data, such as the density ratio [18].", "startOffset": 145, "endOffset": 149}, {"referenceID": 18, "context": "The least-squares mutual information [19] is another example that can be computed in a similar manner.", "startOffset": 37, "endOffset": 41}, {"referenceID": 4, "context": "A nice property of this method is that, if the objective function is strongly convex, it outputs a point that is sufficiently close to an optimal solution after a constant number of iterations [5].", "startOffset": 193, "endOffset": 196}, {"referenceID": 19, "context": "Another technique is lowrank approximation such as Nystr\u00f6m\u2019s method [20].", "startOffset": 68, "endOffset": 72}, {"referenceID": 6, "context": "[7] proposed sublinear-time algorithms for special cases of quadratic function minimization.", "startOffset": 0, "endOffset": 3}, {"referenceID": 5, "context": "Here, we assume the real RAM model [6], in which we can perform basic algebraic operations on real numbers in one step.", "startOffset": 35, "endOffset": 38}, {"referenceID": 10, "context": "To this end, we exploit graph limit theory, initiated by Lov\u00e1sz and Szegedy [11] (refer to [10] for a book), in which we measure the distance between two graphs on different number of vertices by considering continuous versions.", "startOffset": 76, "endOffset": 80}, {"referenceID": 9, "context": "To this end, we exploit graph limit theory, initiated by Lov\u00e1sz and Szegedy [11] (refer to [10] for a book), in which we measure the distance between two graphs on different number of vertices by considering continuous versions.", "startOffset": 91, "endOffset": 95}, {"referenceID": 7, "context": "Related work: Several constant-time approximation algorithms are known for combinatorial optimization problems such as the max cut problem on dense graphs [8, 13], constraint satisfaction problems [1, 22], and the vertex cover problem [15, 16, 25].", "startOffset": 155, "endOffset": 162}, {"referenceID": 12, "context": "Related work: Several constant-time approximation algorithms are known for combinatorial optimization problems such as the max cut problem on dense graphs [8, 13], constraint satisfaction problems [1, 22], and the vertex cover problem [15, 16, 25].", "startOffset": 155, "endOffset": 162}, {"referenceID": 0, "context": "Related work: Several constant-time approximation algorithms are known for combinatorial optimization problems such as the max cut problem on dense graphs [8, 13], constraint satisfaction problems [1, 22], and the vertex cover problem [15, 16, 25].", "startOffset": 197, "endOffset": 204}, {"referenceID": 21, "context": "Related work: Several constant-time approximation algorithms are known for combinatorial optimization problems such as the max cut problem on dense graphs [8, 13], constraint satisfaction problems [1, 22], and the vertex cover problem [15, 16, 25].", "startOffset": 197, "endOffset": 204}, {"referenceID": 14, "context": "Related work: Several constant-time approximation algorithms are known for combinatorial optimization problems such as the max cut problem on dense graphs [8, 13], constraint satisfaction problems [1, 22], and the vertex cover problem [15, 16, 25].", "startOffset": 235, "endOffset": 247}, {"referenceID": 15, "context": "Related work: Several constant-time approximation algorithms are known for combinatorial optimization problems such as the max cut problem on dense graphs [8, 13], constraint satisfaction problems [1, 22], and the vertex cover problem [15, 16, 25].", "startOffset": 235, "endOffset": 247}, {"referenceID": 24, "context": "Related work: Several constant-time approximation algorithms are known for combinatorial optimization problems such as the max cut problem on dense graphs [8, 13], constraint satisfaction problems [1, 22], and the vertex cover problem [15, 16, 25].", "startOffset": 235, "endOffset": 247}, {"referenceID": 8, "context": "A related notion is property testing [9, 17], which aims to design constant-time algorithms that distinguish inputs satisfying some predetermined property from inputs that are \u201cfar\u201d from satisfying it.", "startOffset": 37, "endOffset": 44}, {"referenceID": 16, "context": "A related notion is property testing [9, 17], which aims to design constant-time algorithms that distinguish inputs satisfying some predetermined property from inputs that are \u201cfar\u201d from satisfying it.", "startOffset": 37, "endOffset": 44}, {"referenceID": 1, "context": "Characterizations of constant-time testable properties are known for the properties of a dense graph [2, 3] and the affine-invariant properties of a function on a finite field [23, 24].", "startOffset": 101, "endOffset": 107}, {"referenceID": 2, "context": "Characterizations of constant-time testable properties are known for the properties of a dense graph [2, 3] and the affine-invariant properties of a function on a finite field [23, 24].", "startOffset": 101, "endOffset": 107}, {"referenceID": 22, "context": "Characterizations of constant-time testable properties are known for the properties of a dense graph [2, 3] and the affine-invariant properties of a function on a finite field [23, 24].", "startOffset": 176, "endOffset": 184}, {"referenceID": 23, "context": "Characterizations of constant-time testable properties are known for the properties of a dense graph [2, 3] and the affine-invariant properties of a function on a finite field [23, 24].", "startOffset": 176, "endOffset": 184}, {"referenceID": 11, "context": "1 Dikernels Following [12], we call a (measurable) function f : [0, 1] \u2192 R a dikernel.", "startOffset": 22, "endOffset": 26}, {"referenceID": 0, "context": "1 Dikernels Following [12], we call a (measurable) function f : [0, 1] \u2192 R a dikernel.", "startOffset": 64, "endOffset": 70}, {"referenceID": 10, "context": "A dikernel is a generalization of a graphon [11], which is symmetric and whose range is bounded in [0, 1].", "startOffset": 44, "endOffset": 48}, {"referenceID": 0, "context": "A dikernel is a generalization of a graphon [11], which is symmetric and whose range is bounded in [0, 1].", "startOffset": 99, "endOffset": 105}, {"referenceID": 0, "context": "We can regard a dikernel as a matrix whose index is specified by a real value in [0, 1].", "startOffset": 81, "endOffset": 87}, {"referenceID": 0, "context": "For two functions f, g : [0, 1] \u2192 R, we define their inner product as \u3008f, g\u3009 = \u222b 1 0 f(x)g(x)dx.", "startOffset": 25, "endOffset": 31}, {"referenceID": 0, "context": "For a dikernel W : [0, 1] \u2192 R and a function f : [0, 1] \u2192 R, we define a function Wf : [0, 1] \u2192 R as (Wf)(x) = \u3008W (x, \u00b7), f\u3009.", "startOffset": 19, "endOffset": 25}, {"referenceID": 0, "context": "For a dikernel W : [0, 1] \u2192 R and a function f : [0, 1] \u2192 R, we define a function Wf : [0, 1] \u2192 R as (Wf)(x) = \u3008W (x, \u00b7), f\u3009.", "startOffset": 49, "endOffset": 55}, {"referenceID": 0, "context": "For a dikernel W : [0, 1] \u2192 R and a function f : [0, 1] \u2192 R, we define a function Wf : [0, 1] \u2192 R as (Wf)(x) = \u3008W (x, \u00b7), f\u3009.", "startOffset": 87, "endOffset": 93}, {"referenceID": 0, "context": "Let W : [0, 1] \u2192 R be a dikernel.", "startOffset": 8, "endOffset": 14}, {"referenceID": 0, "context": "The Lp norm \u2016W\u2016p for p \u2265 1 and the cut norm \u2016W\u2016 of W are defined as \u2016W\u2016p = (\u222b 1 0 \u222b 1 0 |W (x, y)|dxdy )1/p and \u2016W\u2016 = supS,T\u2286[0,1] \u2223\u2223 \u222b S \u222b T W (x, y)dxdy \u2223\u2223, respectively, where the supremum is over all pairs of subsets.", "startOffset": 125, "endOffset": 130}, {"referenceID": 0, "context": "A map \u03c0 : [0, 1] \u2192 [0, 1] is said to be measure-preserving, if the pre-image \u03c0(X) is measurable for every measurable set X , and \u03bb(\u03c0(X)) = \u03bb(X).", "startOffset": 10, "endOffset": 16}, {"referenceID": 0, "context": "A map \u03c0 : [0, 1] \u2192 [0, 1] is said to be measure-preserving, if the pre-image \u03c0(X) is measurable for every measurable set X , and \u03bb(\u03c0(X)) = \u03bb(X).", "startOffset": 19, "endOffset": 25}, {"referenceID": 0, "context": "For a measure preserving bijection \u03c0 : [0, 1] \u2192 [0, 1] and a dikernel W : [0, 1] \u2192 R, we define the dikernel \u03c0(W ) : [0, 1] \u2192 R as \u03c0(W )(x, y) = W (\u03c0(x), \u03c0(y)).", "startOffset": 39, "endOffset": 45}, {"referenceID": 0, "context": "For a measure preserving bijection \u03c0 : [0, 1] \u2192 [0, 1] and a dikernel W : [0, 1] \u2192 R, we define the dikernel \u03c0(W ) : [0, 1] \u2192 R as \u03c0(W )(x, y) = W (\u03c0(x), \u03c0(y)).", "startOffset": 48, "endOffset": 54}, {"referenceID": 0, "context": "For a measure preserving bijection \u03c0 : [0, 1] \u2192 [0, 1] and a dikernel W : [0, 1] \u2192 R, we define the dikernel \u03c0(W ) : [0, 1] \u2192 R as \u03c0(W )(x, y) = W (\u03c0(x), \u03c0(y)).", "startOffset": 74, "endOffset": 80}, {"referenceID": 0, "context": "For a measure preserving bijection \u03c0 : [0, 1] \u2192 [0, 1] and a dikernel W : [0, 1] \u2192 R, we define the dikernel \u03c0(W ) : [0, 1] \u2192 R as \u03c0(W )(x, y) = W (\u03c0(x), \u03c0(y)).", "startOffset": 117, "endOffset": 123}, {"referenceID": 0, "context": "2 Matrices and Dikernels Let W : [0, 1] \u2192 R be a dikernel and S = (x1, .", "startOffset": 33, "endOffset": 39}, {"referenceID": 0, "context": ", xk) be a sequence of elements in [0, 1].", "startOffset": 35, "endOffset": 41}, {"referenceID": 0, "context": "We can construct the dikernel \u00c2 : [0, 1] \u2192 R from the matrix A \u2208 R as follows.", "startOffset": 34, "endOffset": 40}, {"referenceID": 0, "context": "For x \u2208 [0, 1], we define in(x) \u2208 [n] as a unique integer such that x \u2208 Ii.", "startOffset": 8, "endOffset": 14}, {"referenceID": 0, "context": "We note that the distribution of A|S , where S is a sequence of k indices that are uniformly and independently sampled from [n] exactly matches the distribution of \u00c2|S , where S is a sequence of k elements that are uniformly and independently sampled from [0, 1].", "startOffset": 256, "endOffset": 262}, {"referenceID": 0, "context": ",W : [0, 1] \u2192 [\u2212L,L], we can obtain a good approximation to them by sampling a sequence of a small number of elements in [0, 1].", "startOffset": 5, "endOffset": 11}, {"referenceID": 0, "context": ",W : [0, 1] \u2192 [\u2212L,L], we can obtain a good approximation to them by sampling a sequence of a small number of elements in [0, 1].", "startOffset": 121, "endOffset": 127}, {"referenceID": 0, "context": ",W : [0, 1] \u2192 [\u2212L,L] be dikernels.", "startOffset": 5, "endOffset": 11}, {"referenceID": 0, "context": "Let S be a sequence of k elements uniformly and independently sampled from [0, 1].", "startOffset": 75, "endOffset": 81}, {"referenceID": 0, "context": "Then, with a probability of at least 1\u2212 exp(\u2212\u03a9(kT/ log2 k)), there exists a measure-preserving bijection \u03c0 : [0, 1] \u2192 [0, 1] such that, for any functions f, g : [0, 1] \u2192 [\u2212K,K] and t \u2208 [T ], we have |\u3008f,W g\u3009 \u2212 \u3008f, \u03c0(\u0174 |S)g\u3009| = O ( LK \u221a T/ log2 k ) .", "startOffset": 109, "endOffset": 115}, {"referenceID": 0, "context": "Then, with a probability of at least 1\u2212 exp(\u2212\u03a9(kT/ log2 k)), there exists a measure-preserving bijection \u03c0 : [0, 1] \u2192 [0, 1] such that, for any functions f, g : [0, 1] \u2192 [\u2212K,K] and t \u2208 [T ], we have |\u3008f,W g\u3009 \u2212 \u3008f, \u03c0(\u0174 |S)g\u3009| = O ( LK \u221a T/ log2 k ) .", "startOffset": 118, "endOffset": 124}, {"referenceID": 0, "context": "Then, with a probability of at least 1\u2212 exp(\u2212\u03a9(kT/ log2 k)), there exists a measure-preserving bijection \u03c0 : [0, 1] \u2192 [0, 1] such that, for any functions f, g : [0, 1] \u2192 [\u2212K,K] and t \u2208 [T ], we have |\u3008f,W g\u3009 \u2212 \u3008f, \u03c0(\u0174 |S)g\u3009| = O ( LK \u221a T/ log2 k ) .", "startOffset": 161, "endOffset": 167}, {"referenceID": 0, "context": "We start with the following lemma, which states that, if a dikernel W : [0, 1] \u2192 R has a small cut norm, then \u3008f,Wf\u3009 is negligible no matter what f is.", "startOffset": 72, "endOffset": 78}, {"referenceID": 0, "context": "Let \u01eb \u2265 0 and W : [0, 1] \u2192 R be a dikernel with \u2016W\u2016 \u2264 \u01eb.", "startOffset": 18, "endOffset": 24}, {"referenceID": 0, "context": "Then, for any functions f, g : [0, 1] \u2192 [\u2212K,K], we have |\u3008f,Wg\u3009| \u2264 \u01ebK.", "startOffset": 31, "endOffset": 37}, {"referenceID": 0, "context": "For \u03c4 \u2208 R and the function h : [0, 1] \u2192 R, let L\u03c4 (h) := {x \u2208 [0, 1] | h(x) = \u03c4} be the level set of h at \u03c4 .", "startOffset": 31, "endOffset": 37}, {"referenceID": 0, "context": "For \u03c4 \u2208 R and the function h : [0, 1] \u2192 R, let L\u03c4 (h) := {x \u2208 [0, 1] | h(x) = \u03c4} be the level set of h at \u03c4 .", "startOffset": 62, "endOffset": 68}, {"referenceID": 0, "context": ", Vp) of the interval [0, 1] is called an equipartition if \u03bb(Vi) = 1/p for every i \u2208 [p].", "startOffset": 22, "endOffset": 28}, {"referenceID": 0, "context": "For the dikernel W : [0, 1] \u2192 R and the equipartition P = (V1, .", "startOffset": 21, "endOffset": 27}, {"referenceID": 0, "context": ", Vp) of [0, 1], we define WP : [0, 1] \u2192 R as the function obtained by averaging each Vi \u00d7 Vj for i, j \u2208 [p].", "startOffset": 9, "endOffset": 15}, {"referenceID": 0, "context": ", Vp) of [0, 1], we define WP : [0, 1] \u2192 R as the function obtained by averaging each Vi \u00d7 Vj for i, j \u2208 [p].", "startOffset": 32, "endOffset": 38}, {"referenceID": 0, "context": "The following lemma states that any function W : [0, 1] \u2192 R can be well approximated by WP for the equipartition P into a small number of parts.", "startOffset": 49, "endOffset": 55}, {"referenceID": 0, "context": "3 (Weak regularity lemma for functions on [0, 1] [8]).", "startOffset": 42, "endOffset": 48}, {"referenceID": 7, "context": "3 (Weak regularity lemma for functions on [0, 1] [8]).", "startOffset": 49, "endOffset": 52}, {"referenceID": 0, "context": "Let P be an equipartition of [0, 1] into k sets.", "startOffset": 29, "endOffset": 35}, {"referenceID": 0, "context": "Then, for any dikernel W : [0, 1] \u2192 R and \u01eb > 0, there exists a refinement Q of P with |Q| \u2264 k2 2 for some constant C > 0 such that \u2016W \u2212WQ\u2016 \u2264 \u01eb\u2016W\u20162.", "startOffset": 27, "endOffset": 33}, {"referenceID": 0, "context": ",W : [0, 1] \u2192 R be dikernels.", "startOffset": 5, "endOffset": 11}, {"referenceID": 3, "context": "15) of [4]).", "startOffset": 7, "endOffset": 10}, {"referenceID": 0, "context": "Let W : [0, 1] \u2192 [\u2212L,L] be a dikernel and S be a sequence of k elements uniformly and independently sampled from [0, 1].", "startOffset": 8, "endOffset": 14}, {"referenceID": 0, "context": "Let W : [0, 1] \u2192 [\u2212L,L] be a dikernel and S be a sequence of k elements uniformly and independently sampled from [0, 1].", "startOffset": 113, "endOffset": 119}, {"referenceID": 0, "context": ",W : [0, 1] \u2192 [\u2212L,L] be dikernels.", "startOffset": 5, "endOffset": 11}, {"referenceID": 0, "context": "Let S be a sequence of k elements uniformly and independently sampled from [0, 1].", "startOffset": 75, "endOffset": 81}, {"referenceID": 0, "context": "Then, with a probability of at least 1\u2212 exp(\u2212\u03a9(kT/ log2 k)), there exists a measure-preserving bijection \u03c0 : [0, 1] \u2192 [0, 1] such that, for every t \u2208 [T ], we have \u2016W t \u2212 \u03c0(\u0174 |S)\u2016 = O ( L \u221a T/ log2 k ) .", "startOffset": 109, "endOffset": 115}, {"referenceID": 0, "context": "Then, with a probability of at least 1\u2212 exp(\u2212\u03a9(kT/ log2 k)), there exists a measure-preserving bijection \u03c0 : [0, 1] \u2192 [0, 1] such that, for every t \u2208 [T ], we have \u2016W t \u2212 \u03c0(\u0174 |S)\u2016 = O ( L \u221a T/ log2 k ) .", "startOffset": 118, "endOffset": 124}, {"referenceID": 0, "context": "Then, for any measure-preserving bijection \u03c0 : [0, 1] \u2192 [0, 1] and t \u2208 [T ], we have ES\u2016W t \u2212 \u03c0(\u0174 |S)\u2016 \u2264 \u2016W t \u2212W t P\u2016 +ES\u2016W t P \u2212 \u03c0(\u0174 t P |S)\u2016 +ES\u2016\u03c0(\u0174 t P |S)\u2212 \u03c0(\u0174 |S)\u2016 \u2264 2\u01ebL+ 8L k1/4 +ES\u2016W t P \u2212 \u03c0(\u0174 t P |S)\u2016 .", "startOffset": 47, "endOffset": 53}, {"referenceID": 0, "context": "Then, for any measure-preserving bijection \u03c0 : [0, 1] \u2192 [0, 1] and t \u2208 [T ], we have ES\u2016W t \u2212 \u03c0(\u0174 |S)\u2016 \u2264 \u2016W t \u2212W t P\u2016 +ES\u2016W t P \u2212 \u03c0(\u0174 t P |S)\u2016 +ES\u2016\u03c0(\u0174 t P |S)\u2212 \u03c0(\u0174 |S)\u2016 \u2264 2\u01ebL+ 8L k1/4 +ES\u2016W t P \u2212 \u03c0(\u0174 t P |S)\u2016 .", "startOffset": 56, "endOffset": 62}, {"referenceID": 0, "context": ", xk} be a sequence of independent random variables that are uniformly distributed in [0, 1], and let Zi be the number of points xj that fall into the set Vi.", "startOffset": 86, "endOffset": 92}, {"referenceID": 0, "context": "The partition P \u2032 of [0, 1] is constructed into the sets V \u2032 1 , .", "startOffset": 21, "endOffset": 27}, {"referenceID": 0, "context": "For each t \u2208 [T ], we construct the dikernel W t : [0, 1] \u2192 R such that the value of W t on V \u2032 i \u00d7 V \u2032 j is the same as the value of W t P on Vi \u00d7 Vj .", "startOffset": 51, "endOffset": 57}, {"referenceID": 0, "context": "The real-valued function Pn,A,d,b on the functions f : [0, 1] \u2192 R is defined as Pn,A,d,b(f) = \u3008f, \u00c2f\u3009+ \u3008f , d\u03021\u22a41\u3009+ \u3008f, b\u03021\u22a41\u3009, where f : [0, 1] \u2192 R is a function such that f(x) = f(x) for every x \u2208 [0, 1] and 1 : [0, 1] \u2192 R is the constant function that has a value of 1 everywhere.", "startOffset": 55, "endOffset": 61}, {"referenceID": 0, "context": "The real-valued function Pn,A,d,b on the functions f : [0, 1] \u2192 R is defined as Pn,A,d,b(f) = \u3008f, \u00c2f\u3009+ \u3008f , d\u03021\u22a41\u3009+ \u3008f, b\u03021\u22a41\u3009, where f : [0, 1] \u2192 R is a function such that f(x) = f(x) for every x \u2208 [0, 1] and 1 : [0, 1] \u2192 R is the constant function that has a value of 1 everywhere.", "startOffset": 138, "endOffset": 144}, {"referenceID": 0, "context": "The real-valued function Pn,A,d,b on the functions f : [0, 1] \u2192 R is defined as Pn,A,d,b(f) = \u3008f, \u00c2f\u3009+ \u3008f , d\u03021\u22a41\u3009+ \u3008f, b\u03021\u22a41\u3009, where f : [0, 1] \u2192 R is a function such that f(x) = f(x) for every x \u2208 [0, 1] and 1 : [0, 1] \u2192 R is the constant function that has a value of 1 everywhere.", "startOffset": 199, "endOffset": 205}, {"referenceID": 0, "context": "The real-valued function Pn,A,d,b on the functions f : [0, 1] \u2192 R is defined as Pn,A,d,b(f) = \u3008f, \u00c2f\u3009+ \u3008f , d\u03021\u22a41\u3009+ \u3008f, b\u03021\u22a41\u3009, where f : [0, 1] \u2192 R is a function such that f(x) = f(x) for every x \u2208 [0, 1] and 1 : [0, 1] \u2192 R is the constant function that has a value of 1 everywhere.", "startOffset": 214, "endOffset": 220}, {"referenceID": 0, "context": "Then, we have min v\u2208[\u2212K,K]n pn,A,d,b(v) = n 2 \u00b7 inf f :[0,1]\u2192[\u2212K,K] Pn,A,d,b(f).", "startOffset": 55, "endOffset": 60}, {"referenceID": 0, "context": "First, we show that n \u00b7 inff :[0,1]\u2192[\u2212K,K] Pn,A,d,b(f) \u2264 minv\u2208[\u2212K,K]n pn,A,d,b(v).", "startOffset": 30, "endOffset": 35}, {"referenceID": 0, "context": "Given a vector v \u2208 [\u2212K,K], we define f : [0, 1] \u2192 [\u2212K,K] as f(x) = vin(x).", "startOffset": 41, "endOffset": 47}, {"referenceID": 0, "context": "Next, we show that minv\u2208[\u2212K,K]n pn,A,d,b(v) \u2264 n \u00b7 inff :[0,1]\u2192[\u2212K,K] Pn,A,d,b(f).", "startOffset": 56, "endOffset": 61}, {"referenceID": 0, "context": "Let f : [0, 1] \u2192 [\u2212K,K] be a measurable function.", "startOffset": 8, "endOffset": 14}, {"referenceID": 0, "context": "Then, for x \u2208 [0, 1], we have \u2202Pn,A,d,b(f(x)) \u2202f(x) = \u2211", "startOffset": 14, "endOffset": 20}, {"referenceID": 0, "context": "Note that the form of this partial derivative only depends on in(x); hence, in the optimal solution f : [0, 1] \u2192 [\u2212K,K], we can assume f(x) = f(y) if in(x) = in(y).", "startOffset": 104, "endOffset": 110}, {"referenceID": 0, "context": "For such f, we define the vector v \u2208 R as vi = f (x), where x \u2208 [0, 1] is any element in Ii.", "startOffset": 64, "endOffset": 70}, {"referenceID": 0, "context": "Then, with a probability of at least 1\u2212 \u03b4, there exists a measure preserving bijection \u03c0 : [0, 1] \u2192 [0, 1] such that", "startOffset": 91, "endOffset": 97}, {"referenceID": 0, "context": "Then, with a probability of at least 1\u2212 \u03b4, there exists a measure preserving bijection \u03c0 : [0, 1] \u2192 [0, 1] such that", "startOffset": 100, "endOffset": 106}, {"referenceID": 0, "context": "max { |\u3008f, (\u00c2\u2212 \u03c0(\u00c2|S))f\u3009|, |\u3008f , (d\u03021\u22a4 \u2212 \u03c0(d\u03021|S))1\u3009|, |\u3008f, (b\u03021\u22a4 \u2212 \u03c0(b\u03021|S))1\u3009| } \u2264 \u01ebLK 3 for any function f : [0, 1] \u2192 [\u2212K,K].", "startOffset": 112, "endOffset": 118}, {"referenceID": 0, "context": "Then, we have z\u0303 = min v\u2208Rk pk,A|S ,d|S,b|S(v) = min v\u2208[\u2212K,K]k pk,A|S,d|S ,b|S(v) = k \u00b7 inf f :[0,1]\u2192[\u2212K,K] Pk,A|S ,d|S,b|S(f) (By Lemma 4.", "startOffset": 95, "endOffset": 100}, {"referenceID": 0, "context": "1) = k \u00b7 inf f :[0,1]\u2192[\u2212K,K] ( \u3008f, (\u03c0(\u00c2|S)\u2212 \u00c2)f\u3009+ \u3008f, \u00c2f\u3009+ \u3008f , (\u03c0(d\u03021|S)\u2212 d\u03021\u22a4)1\u3009+ \u3008f , d\u03021\u22a41\u3009+", "startOffset": 16, "endOffset": 21}, {"referenceID": 0, "context": "\u2264 k \u00b7 inf f :[0,1]\u2192[\u2212K,K] ( \u3008f, \u00c2f\u3009+ \u3008f, d\u03021\u22a41\u3009+ \u3008f, b\u03021\u22a41\u3009 \u00b1 \u01ebLK )", "startOffset": 13, "endOffset": 18}, {"referenceID": 0, "context": "We randomly generated inputs as Aij \u223c U[\u22121,1], di \u223c U[0,1], and bi \u223c U[\u22121,1] for i, j \u2208 [n], where U[a,b] denotes the uniform distribution with the support [a, b].", "startOffset": 53, "endOffset": 58}, {"referenceID": 20, "context": "Application to kernel methods Next, we considered the kernel approximation of the Pearson divergence [21].", "startOffset": 101, "endOffset": 105}, {"referenceID": 20, "context": "5; \u03c3 and \u03bb were chosen by 5-fold cross-validation as suggested in [21].", "startOffset": 66, "endOffset": 70}], "year": 2016, "abstractText": "A sampling-based optimization method for quadratic functions is proposed. Our method approximately solves the following n-dimensional quadratic minimization problem in constant time, which is independent of n: z = minv\u2208Rn\u3008v, Av\u3009 + n\u3008v, diag(d)v\u3009 + n\u3008b,v\u3009, where A \u2208 R is a matrix and d, b \u2208 R are vectors. Our theoretical analysis specifies the number of samples k(\u03b4, \u01eb) such that the approximated solution z satisfies |z \u2212 z| = O(\u01ebn) with probability 1\u2212 \u03b4. The empirical performance (accuracy and runtime) is positively confirmed by numerical experiments.", "creator": "LaTeX with hyperref package"}}}