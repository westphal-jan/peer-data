{"id": "1608.07625", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "26-Aug-2016", "title": "Large Scale Behavioral Analytics via Topical Interaction", "abstract": "We propose the split-diffuse (SD) algorithm that takes the output of an existing dimension reduction algorithm, and distributes the data points uniformly across the visualization space. The result, called the topic grids, is a set of grids on various topics which are generated from the free-form text content of any domain of interest. The topic grids efficiently utilizes the visualization space to provide visual summaries for massive data and are then split into a single set of grids.\n\n\n\n\n\nThe topic grids, as illustrated in Figure 1, are organized to represent the number of nodes each of the two graphs have. The data points are arranged across different regions of the graph to provide a broad set of data points and for each node to represent the number of nodes each of the two graphs have. This allows for a large set of graphs that are split across multiple dimensions from each other, with different parts of the graph and nodes in separate layers.\nThis grid can then serve as a data-based image or a reference to a network of nodes.\nFigure 1 illustrates a diagram of a distributed graph for the distribution of each data points in this article. The grid and a large graph are shown in the figure 2.\nFigure 2 shows the distribution of each data point on each graph as a set of graphs (or a subset of graphs from the open source source sources). This grid consists of the graph nodes distributed by the network.\nFigure 2 illustrates a graph of distributed graph for the distribution of each data points in this article.\nFigure 2 illustrates the distribution of each data point on each graph as a set of graphs (or a subset of graphs from the open source sources). This grid consists of the graph nodes distributed by the network. The graphs are divided into two grids: the center and the right. The graph nodes are distributed by each other, with different parts of the graph, including a different part of the graph.\nFigure 2 illustrates the distribution of each data point on each graph as a set of graphs (or a subset of graphs from the open source sources). This grid consists of the graph nodes distributed by the network. The graph nodes are distributed by each other, with different parts of the graph.\nFigure 3 illustrates the distribution of each data point on each graph as a set of graphs (or a subset of graphs from the open source sources).\nFigure 3 shows the distribution of each data point on each graph as a set of graphs (or a subset of graphs from the open source sources). This grid consists of the graph", "histories": [["v1", "Fri, 26 Aug 2016 23:07:43 GMT  (1450kb,D)", "http://arxiv.org/abs/1608.07625v1", null]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["shih-chieh su"], "accepted": false, "id": "1608.07625"}, "pdf": {"name": "1608.07625.pdf", "metadata": {"source": "CRF", "title": "Large Scale Behavioral Analytics via Topical Interaction", "authors": ["Shih-Chieh Su"], "emails": ["shihchie@qualcomm.com"], "sections": [{"heading": null, "text": "I. Introduction\nWhen there are multiple measures of the each sample, the data is described in the a high dimensional space H by these measures. For example, the samples collected by a network of sensors have the dimensionality of the number of sensors. Each stock symbol can be described by multiple quantifiers which make up the data space. Documents of a corpus can be described by the vector of word counts (\u201cword vector of features\u201d), which can be all or most of the words appearing in the corpus. On these cases, it is possible that H has tens of thousands dimensions. To make these high dimensional data points visible to human, a word embedding (or dimension reduction) technique is employed to map the data points to a lower dimensional space L. Usually L is a two-dimensional (2D) or three-dimensional (3D) space. The word embedding technique attempts to preserve some relationship among the data points in H after mapping them to L. For example, the principal component analysis (PCA) [1] attempts to combine the dimensions of H to form the dimensions in L that best explain the variance in the data. However, the directions maximizing the variance do not always maximize the information, or optimize some other desirable metric. In some case, one would prefer the interpoint relationship in H being maintained in L. The multidimensional scaling (MDS) [2] M tries to preserve the distance between data points {p}, during the mapping from H to L, so that\nM = argmin \u2200M :p(H)\u2192p(L) \u2211 \u2200i,j ( ||pi \u2212 pj ||(H) \u2212 ||pi \u2212 pj ||(L) ||pi \u2212 pj ||(H) )2 (1)\nThe stochastic neighbor embedding (SNE) [3] type of algorithms further emphasize the local distances. In a broader sense, SNE algorithms try to preserve the local relationship ahead of preserving the global relationship. There are other techniques emphasizing different favored metrics over the relationship among data points. Some popular choices include isomap [4], spectral embedding [5], and totally random trees embedding [6]). On a specific situation, one particular dimension reduction technique could be more suitable than others.\nIn this paper, we do not discuss the use cases for different dimension reduction algorithms. Instead, we focus on the techniques in the 2D or 3D visual space to help human perceive and interact with the rendered information more easily."}, {"heading": "II. Background", "text": "Consider the case to visualize the traffic drift of a network node, over two periods of time. There could be hundreds of million entries in the activity log during each time period. Rather than showing these entries line-by-line, a common approach is to classify the entries into topics, based on the entry content. Then we can compare the topical metrics across different time periods. Some example metrics can be entry counts, total file size, total packet size, counts in source or destination entities. Similarly, one may be interested in comparing the shipping patterns on millions of shipments to the New York City against that to the Los Angeles City. Or compare the reimbursement patterns before and after a policy change of a healthcare insurance plan. In these cases, showing the massive amount of entries directly in the charts can be overwhelming and difficult to make comparison. Instead, we want a visual summary for the behavioral content. The same summarization and visualization method can be performed on different metrics, on different targets, or over different period of time. Thus the human expert can compare the behavior, and can interact with the differences. We start from the log of activities. Each entry has the identifier of the entity that incurred the activity, the timestamp, and the activity detail. The activity detail can be any free-form text and is properly punctuated. We collect all these entries to form a corpus. Each entry (document) ar X iv :1 60 8. 07 62\n5v 1\n[ cs\n.L G\n] 2\n6 A\nug 2\n01 6\nbecomes a data point in the high dimensional word vector space H defined by the corpus. Topics are generated in H to summarize the activities, using techniques like the latent Dirichlet allocation (LDA) model [7]. These topics, in the form of data points, are than mapped to L with the existing dimension algorithm of choice.\nThe output from existing dimension reduction algorithm is a set of data points that are non-uniformly scattered around the visualization space. This helps to explain the clustering behavior, including inter-cluster and intracluster, among the data points. However, there are also some drawbacks:\n1) The clusters tangle with others; some data points overlap with others. Overlap makes the information less perceivable. 2) The data points are denser in some area. The heterogeneity makes human interaction with the data points more difficult. 3) When comparing different metrics over the same space, the geometrical relationship between the data points tends to outshine the comparison between the metrics.\nMoreover, there is always area where points are sparse. The white space that is not occupied by any points does not provide any extra information other than having no data point. We cannot interact with the white space the way we interact with the topic data points. In our case, we want to know the nearby topics of a specific topic, but the distance accuracy can be traded for the ease of topical comparison and topical interaction. In order to better utilize the visualization space, we suggest to distribute the data points evenly over the visualization space. The cloud of data points is deformed in the same space L defined by the dimension reduction algorithm. This deformation is denoted as S. In the meanwhile, it is desirable to preserve the point-wise relationship maintained by the dimension reduction algorithm. Our strategy in approaching this goal is prioritized as follows:\n1) Points are equally spaced. 2) Point-wise topology is preserved.\nAlgorithm 1 Split-diffuse algorithm (k-dimensional) Input: data points {p} of length g1\u00d7g2\u00d7\u00b7 \u00b7 \u00b7\u00d7gk, allocation vector ~c = (0, . . . , 0) \u2208 (Z+)k split-diffuse ({p}, ~g, ~c) k \u2190 length of {p} if k = 0, then\nreturn null else if k = 1, then\nresolve S(p) from ~c, return p end if a\u2190 argmaxa (ga) wl \u2190 ga integer divides 2; wr \u2190 ga \u2212 wl ~gl \u2190 ~g with ga being wl; ~gr \u2190 ~g with ga being wr ~cl \u2190 ~c; ~cr \u2190 ~c with ca being ca + wr m\u2190 wl \u220f i6=a gi {q} \u2190 {p} sorted in dimension a return ([split-diffuse ({qi : i \u2264 m}, ~gl, ~cl)],\n[split-diffuse ({qi : i > m}, ~gr, ~cr)])\na) When the point pi is to the right of point pj in space L, S attempts to make S(pi) to the right of S(pj) in the same space L, for all i, j, within all dimensions of L.\n3) Point-wise geometry is loosely followed. a) When pi is far from pj , S(pi) is far from S(pj). b) When {pi} forms a cluster, {S(pi)} still forms a\ncluster (the definition on cluster may vary over different situations)."}, {"heading": "III. The Split-Diffuse Algorithm", "text": "We propose an algorithm called the split-diffuse (SD) algorithm to realize the strategy above. The idea of the SD algorithm is shown as a simple example in Figure 1. There are 16 points to be placed over a 4\u00d7 4 layout space S. The SD algorithm first picks the x-dimension to split, and splits the data points into two groups, the ones smaller or equal to the median, and the ones not. Each group goes through this split step again over the y-dimension, as in\nFigure 1 (b). In this case, we recursively split the points in x- and y-dimension iteratively, until there is only one point in current recursion. We keep track of the splitting path in the string c. At the end of the recursion, the placement of the single point p in {p} is resolved as S(p). The values of S(p) reflect the mapped position in the final placement and are integers in all dimension of S. For example, the black dot p\u2032 in Figure 1 (c) is placed at S(p\u2032) = (2, 2), where the indexes begin with zero at the upper left corner. As a result, the mapped data points are equally spaced in the predefined layout of shape g1 \u00d7 g2 \u00d7 \u00b7 \u00b7 \u00b7 \u00d7 gk. To achieve this uniformity in the space L, the data points are essentially diffused from the denser area to the coarser area by the SD algorithm \u2014 hence the name split-diffuse. Some examples on the input and the output of the SD algorithm are shown in Figure 2. We generate 64 topics regarding to the content of a repository access logs. The topics are mapped from H to L via MDS in Figure 2 (a)(e) and t-SNE [8] in Figure 2 (c)(g). A topic is represented by the encrypted first three letters of the most descriptive word. Topics sharing the same representative word are the topics close to each other in H. In both of our MDS and t-SNE cases, the topics with the same representative word tend to form clusters. The corresponding output of the SD algorithm on MDS and t-SNE inputs are alsow shown in Figure 2. The clustering topology is maintained in a way when the uniformity of the point placement is enforced across all dimensions. The topical behavior can be further visualized on S, as in Figure 3. The behavioral metrics will be discussed later in Section IV."}, {"heading": "IV. Behavior Anomaly and the Topic Grids", "text": "Consider the case that terabytes of network logs comes in everyday. Is there a way to quickly detect the anomaly entities and provide the visual breakdown about their behavioral change? In the cyber security domain, the SD algorithm is applied to analyze behavioral content via the proposed system in Figure 4. It starts with the log files that record the user behavior of a repository. This data goes through and trains three sub-systems: the topic model, the dimension reduction, and the SD map. We use six month worth of data to ensure good content coverage fed into the topic model. Once trained, the topics, their coordinates in L, and the SD map are then fixed for the incoming data. The system analyzes the risk over all topics for each user. Behavior anomaly is ranked and rendered to the human experts. The goal of the system is to detect behavioral anomaly based on the access logs. After proper punctuation, the\naccessed path (or URL), the content, and/or any meta data regarding to an access can be viewed as a document, called the content document. We use the LDA model to decide the topics among all these content documents over the benchmark period of time. In one of our implementations, 64 topics are generated at a word vector space of 19K+ dimensions. The relevance between a content document and each individual topic is measured. The anomaly, or risk, of an access is quantified by the difference of topical relevance between this access and the benchmark profile of the entity. The topics are then projected to the 2D visualization space via any existing dimension reduction algorithm. We use MDS to maintain the global similarity. But it could be other algorithms emphasizing other pointwise relationships \u2014 as long as the semantically similar topics are mapped close to each other.\nThe SD algorithm then places these 64 topics over an 8\u00d7 8 layout, forming a set of topic grids. Topical activities of an entity (user, group, node, etc), of the peers of the entity, of the history of the entity can all be compared on\nthe same set of grids. Other topical metrics, such as risk or trend, can also be visualized over the same set of grids. One use case is presented in Figure 3.\nThe activities of a specific user over a specific period of time is shown in Figure 3 (a), and is compared with the same metric of the same user over a historical benchmark period in Figure 3 (b). The color in Figure 3 (a) and (b) reflects the volume of the activities over the topic. Red means higher volume of topical activities. The difference between Figure 3 (a) and (b) is the anomaly against the historical behavior of the user, and is quantified in Figure 3 (c), where the red color indicates higher topical risk. One candidate topical risk metric between time periods T1 and T2 over topic t for the same entity e is\nR (Be,T1 ,Be,T2 ) t = log( \u2211 a\u2208Be,T2 ra + 1)\u2212 log( \u2211 a\u2208Be,T1 ra + 1),\n(2) where ra is the relevancy for activity a to topic t, and B is the set of activities defined by the unique content documents of all activities logged within the corresponding\ntime period. In our example, the red block at the upper left corner of Figure 3 (c) comes from the recent activities of the same topic (location) in Figure 3 (a), while having no historical activities on the same topic in Figure 3 (b). Similar comparison is made against the peers of the user as in Figure 3 (d) and (e). Changing the definition on B or adding other quantifiers in Equation 2 leads to different activity and risk metrics, which can be visualized similarly as in Figure 3.\nWhen not directly displaying the detail keywords about a topic, the topic grids requires less space. At the same time, the human expert still can easily keep track of the topics based on their indexes over all dimensions and compare the difference between different sets of topic grids. Human interaction, which is the ultimate goal of the uniform placement of the data points, can be done more easily on the topic grids than on the raw dimension reduction output as in Figure 2 (a) and (c). Example interactions shown in Figure 5 are the mouse over event to popup the topical summary, and the click event to overlay the detailed topical activities. It is also possible to reserve one dimension of the visualization space as the time axis, to better analyze the behavior change over time. In this case, an one-dimensional SD algorithm can be applied to place the topics in the 1D space. The result is as in Figure 6(a), where x-axis represents the indexes of the topics and the y-axis is the time. In this implementation, we accumulate the behavior over the month. Therefore, the risk remain high for the rest of the month when a user touches a risky topic. This memory effect creates pattern like a curtain, thus the name topic curtain. Meanwhile, we can pile up the 2D topic grids on the time axis over the 3D L, as the topic shower shown in Figure 6(b). With normal or usual behavior, it is expected to see the consistent hot grids at the same locations over time."}, {"heading": "V. Performance of the Split-Diffuse Algorithm", "text": "In order to measure the performance of the split-diffuse algorithm, we randomly generate the topics in the 2D space and let the algorithm evenly space these topics over a predefined layout in the space of same dimensionality. The random topics are generated over a 2D space via two types of sampling approaches, namely the uniform approach,\nU(\u03c1) : [ X Y ]T [ \u03c1 0 0 1 ] , X, Y \u223c U(\u22120.5, 0.5) (3)\nand the Gaussian approach. G(\u03b8, \u03c6) : [ X Y ]T [ \u03c6 0 0 1 ] [ cos \u03b8 sin \u03b8 \u2212 sin \u03b8 cos \u03b8 ] ,\n[ X Y ] \u223c N ([ 0 0 ] , I2 ) (4) As mentioned in Section II, the best attempts will be made to preserve the the topology of these points in L\nbefore the S mapping. That is, when pi is to the left of pj , S(pi) should be to the left of S(pj), for any dimension l of L. For the case of placing n , \u2016{p}\u2016 data points over the space L of k dimensions, there are totally k ( n 2 ) constraints to be met. The error on such topology-maintaining attempt is then measured as\nErrI , eI,1 + eI,2( n 2 ) \u00b7 k\n(5)\nwhere eI,1 , \u2016{pi < pj ,S(pi) \u2265 S(pj)}\u2016l eI,2 , \u2016{pi \u2265 pj ,S(pi) < S(pj)}\u2016l\nfor all i, j and for all lth-dimension of L. One can generally regard the mapped value S(p) to be integer. There will be some amount of points sharing the same S(p) value in each of the dimension. Therefore, a loosen version of the error metric is: when pi is to the left of pj , S(pi) should not be to the right of S(pj), for any dimension l of L. This loosen metric can be defined as\nErrII \u2261 eII,1 + eII,2( n 2 ) \u00b7 k\n(6)\nwhere eII,1 \u2261 \u2016{pi < pj ,S(pi) > S(pj)}\u2016l eII,2 \u2261 \u2016{pi > pj ,S(pi) < S(pj)}\u2016l\nfor all i, j and for all lth-dimension of L."}, {"heading": "A. Errors on Grid Layouts", "text": "The error ratios on split-diffusing U(1) and G( 4\u03c0 , 2) over various grid layouts are shown in Table I. ErrI decreases as the grid set size increases on both U(1) and G( 4\u03c0 , 2) sampling schemes. However, ErrII only decreases as the grid set size increases on the U(1) sampling. ErrII increases as the grid set size increases on the G( 4\u03c0 , 2) sampling. Another observation is that although ErrI decreases as the grid set size increases on both U(1) and G( 4\u03c0 , 2) samplings, the decrease rate varies. On the U(1) sampling, ErrI decreases almost 90% when the layout grows from 4 \u00d7 4 to 64 \u00d7 64. However, ErrI only decreases around half of its value during the same layout growth for the G( 4\u03c0 , 2) sampling. By definition, ErrI \u2265 ErrII on any given layout-sampling combination. Therefore, over the G( 4\u03c0 , 2) sampling on Table I, the decreasing of ErrI and the increasing of ErrII will not incur a cross-over as the layout becomes larger."}, {"heading": "B. Errors on Distribution of Input Data Points", "text": "The error ratios on split-diffusing various sampling schemes over the same 8 \u00d7 8 grid layout are shown in Figure 7. The error ratio reflects the portion of the constraints in the specific dimension that are not met. We also ask the SD algorithm to start the splitting in the y-dimension. On the test over U(\u03c1), the sampled points uniformly distributed within a unit square are being shrunk on their xvalues by the ratio \u03c1 (Equation 3). When the SD algorithm\nstarts the splitting in the y-dimension first, ErrI(y) < ErrI(x) and ErrII(y) < ErrII(x), as in Figure 7 (e). \u03c1 does not impact on all error metrics. If opting to start the SD algorithm in the x-dimension first, the resulting error ratios in the x- and the y-dimension would be swapped. (Plot is omitted)\nIn Figure 7 (f), we evaluate the error ratios of G(\u03b8, 2). Under this sampling scheme, the error ratios vary with \u03b8. With a fixed \u03c6, both ErrI and ErrII have maximum values at \u03b8 = \u03c0/4. However, the error ratio in the x-dimension (or the dimension not being chosen to start the split) is more sensitive to \u03b8 than that in the y-dimension, for both types of error. In comparison, when \u03c6 is fixed at a larger value, the Gaussian sample distribution will be narrower in shape. The narrower shape incurs higher error ratios, especially when \u03b8 is around \u03c0/4.\nBased on Figure 7 (f)(g), we find that Gaussian samples\nfrom Equation 4 with larger \u03c6 and \u03b8 around \u03c0/4 incur higher error ratios. To observe further in this type of Gaussian samples, we plotted G(\u03c02 , \u03c6) in Figures 7 (h). As expected, all error ratios increase as \u03c6 increases, where the error ratios on the x-dimension are more sensitive to \u03c6. The errors converge as \u03c6 becomes larger."}, {"heading": "C. Error Bounds", "text": "As mentioned above, each pair of points has one constraint on each dimension within the space. In 2D space, there are totally n(n\u22121) constraints, where n is the number of points in {p}. We start from inspecting the constraints toward one single point. In Figure 1 (c), the split path of a particular (black) data point is illustrated over a 4\u00d7 4 layout. The split #1 ensures that the 8 blue points to the left of the split #1 line are mapped to the left of the mapped black point. So 8 constraints in the x-dimension is met. Similarly, the split #2 line guarantees 4 constraints\nin the y-dimension is met, and so on. Upon the final split relating to the black point, that is when it is separated from all other points, a total of 15 constraint on the black point will be met. The same applies to all other points.\nTherefore, the SD algorithm satisfies ( n 2 ) constraints in the 2D space. In other words, ErrI is bounded by 1/2. As a particular example, if the input data contains 4 points along the diagonal, the SD algorithm meets 7 of the 12 constraints over the 2\u00d7 2 layout (3 in x-dimension and 4 in y-dimension if we split in the y-dimension first). A more general bound is as follows.\nTheorem 1. In a space L of k dimensions, the SD algorithm satisfies 1/k of the type I constraints measured by ErrI in Equation 5. That is,\nErrI \u2264 k \u2212 1 k\n(7)\nProof: At the end of the SD algorithm, each data point p is split into a cell that only contains p. The splits toward the point p satisfy at least (n\u22121) type I constraints involving p, across all dimensions of L. Among all the points in {p}, a total of ( n 2 ) type I constraints out of all [( n 2 ) \u00b7 k ] type I constraints over all dimensions of L are met. On the trivial case of k = 1, the data points will be\nevenly placed onto a one-dimensional space. There are ( n 2 ) type I constraints, and all of them are met. The error bound become looser with larger k, which can be seen as another curse of dimensionality. Fortunately, we are only interested in visualizing the data points in a space having k \u2264 3."}, {"heading": "D. Error on Non-square Layout", "text": "On the general SD algorithm, one could argue on the dimension to start the first split. Consider the 2\u00d7 4 layout in Figure 8 (a). The upper part of Figure 8 (a) illustrates the strategy to split iteratively over x- and y-dimensions\nwhile starting with the split in the y-dimension as default. The lower part of Figure 8 (a) adopts greedy fashion of the SD algorithm with argmaxa (ga) breaks tie in favor of the y-dimension. The error bound in Theorem 1 holds on both splitting strategies, since both of them ensure the splitting to end with every point being placed into a single-point cell. We exercise both strategies in Figure 8 (a) on an example set of 8 data points as shown in Figure 8 (b). Inspecting the errors incurred by the first split, the iterative strategy in the upper part of Figure 8 (b) guarantees the y-dimension constraints (Equation 5) between the group above split #1 and the group below will be met. However, as the consequence of this split, the black points are eventually placed into the \u201cA\u201d and \u201cH\u201d positions. This placement violates the x-dimension constraint by four grids. The first split of the greedy splitting in the lower Figure 8 (b), on the other hand, leads the black points into the \u201cC\u201d and \u201cF\u201d positions. The consequence y-dimension constraint violation has a displacement of two grids. Although both above cases count for one error in ErrI , the chance of incurring error with larger displacement makes the iterative splitting strategy less favored than the greedy splitting strategy of Algorithm 1.\nThe error ratio comparison between both strategies over G(\u03b8, 2) input samples are plotted in Figure 8 (c). The overall G(\u03c0/4, 2) ErrI for the iterative splitting strategy is 0.2640, compared to 0.2541 from the greedy strategy. ErrII for the these two strategies over the same setup are 0.0497 and 0.0399, correspondingly. From Figure 8 (c) one can tell that the area between the dotted blue line and the solid blue line is smaller than the area between the dotted and solid black lines. The difference between these two areas reflects the ErrI gain/improvement for choosing the greedy splitting over the iterative splitting. Without\nconsidering the amount of displacement on the error, the greedy splitting shows better ErrI than the iterative splitting, with the maximum gain of 0.0098 happening at \u03b8 = \u03c0/4 of G(\u03b8, 2). Similarly, greedy splitting also provides better ErrII across all \u03b8. Therefore, via actively looking for the dimension that has the most allocation spots to split, the greedy splitting strategy avoids having larger displacement. In addition, the better error ratios also led us to use this strategy in Algorithm 1. There could possibly exist a better strategy toward a specific displacement metric, to be explored by further studies.\nE. Relationship to the k-d Tree The SD algorithm is similar to the generation algorithm of the k-d tree[9] with some major differences 1) The k-d tree creates a tree; the SD algorithm creates\na map. 2) The k-d tree splits in each dimension in turns; not\nalways true for the SD algorithm. 3) The k-d tree stores the median point at the split\nnode; the SD algorithm divides points in two groups at each split. 4) The k-d tree is used for fast similarity search; the SD algorithm places points evenly-spaced. 5) The k-d tree is evaluated by the scale and speed of search; we evaluate the SD algorithm, or similar algorithms that map data points uniformly, by the number of constraints it satisfies.\nWhile k-d tree can be used on high dimensional data points and suffers from the curse of dimensionality, the SD algorithm is usually performed on the 2D or 3D space. The complexity of the SD algorithm is comparable to that of the k-d tree. When using the O(n) median of medians algorithm [10] in selecting the median at each depth level, the worst-case complexity is O(n logn)."}, {"heading": "VI. Conclusion and Future Work", "text": "In this paper, we introduce the topic grids to organize and visualize massive amount of behavioral data. Once trained, the same set of topic grids can be used to visualize different metrics on different targets over different periods of time, and compare the difference. Human can perceive the difference easily via the location of the integer-indexed grids. Topical details can be further rendered when the user interacts with the topic grids. Although we use the squared layout in the paper, the technique can produce layouts in rectangles, cubes or cuboids. The goal behind the technique is to utilize the visualization and interaction space as much as we can, for making topical comparison. We present the use case to analyze the behavioral risk over the network activities. Beyond risk, the technique can be used to analyze topical trend, expertise, learning rates, or other metrics of interest. It can be applied to other domains beyond cyber security. Working with the e-commerce logs, the topic grids can be used to analyze\nthe shopping behavior and preference of the customers, or the inventory of the vendor, over peer groups defined on location, age, or other attributes. Applications can further include credit card transactions, customer complaints, reviews of the shared economy companies, or any data source having free-form text from which we can extract topics."}, {"heading": "A. Conditioned Topic Placement", "text": "The SD algorithm delegates the maintenance of the H-toL point-wise relationship to the existing word embedding algorithms of choice. There could be a possible way to map and maintain the H-to-L point-wise relationship while enforcing the uniform spacing in L. Such algorithm, when available, is a more direct algorithm to embed words uniformly. The desired aspect can be measured as displacement in the uniform L space, while the algorithm optimizes this metric. On a specific situation, it is desirable to keep the clusters inH intact after mapped to the uniform L space. Currently the SD algorithm cannot guarantee such property."}, {"heading": "B. Topic Drift over Time", "text": "Over time, the topics on the input corpus may drift. There could be new topics while old topics fade away. On the assumption that most of the topics are still the same and having similar relationship to each other, it will be desirable to generate a new SD map S\u2032(p) which looks similar to the old S(p)."}, {"heading": "C. Application to Structured Data", "text": "It is also possible to apply the topic grids to the structured data, on which an arbitrary clustering algorithm can generate cluster centers. The data points are then organized into these cluster centers, the same way we use the topic to represent the log entries related to it."}], "references": [{"title": "On lines and planes of closest fit to systems of points in space", "author": ["K. Pearson"], "venue": "The London, Edinburgh, and Dublin Philosophical Magazine and Journal of Science, vol. 2, no. 11, pp. 559\u2013572, 1901.", "citeRegEx": "1", "shortCiteRegEx": null, "year": 1901}, {"title": "Multidimensional scaling: I. theory and method", "author": ["W.S. Torgerson"], "venue": "Psychometrika, vol. 17, no. 4, pp. 401\u2013419, 1952.", "citeRegEx": "2", "shortCiteRegEx": null, "year": 1952}, {"title": "Stochastic neighbor embedding", "author": ["G.E. Hinton", "S.T. Roweis"], "venue": "Advances in Neural Information Processing Systems, 2002, pp. 833\u2013840.", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2002}, {"title": "A global geometric framework for nonlinear dimensionality reduction", "author": ["J.B. Tenenbaum", "V.D. Silva", "J.C. Langford"], "venue": "Science, vol. 290, no. 5500, pp. 2319\u20132323, 2000.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2000}, {"title": "Laplacian eigenmaps for dimensionality reduction and data representation", "author": ["M. Belkin", "P. Niyogi"], "venue": "Neural computation, vol. 15, no. 6, pp. 1373\u20131396, 2003.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2003}, {"title": "Extremely randomized trees", "author": ["P. Geurts", "D. Ernst", "L. Wehenkel"], "venue": "Machine Learning, vol. 63, no. 1, pp. 3\u201342, 2006.", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2006}, {"title": "Latent dirichlet allocation", "author": ["D.M. Blei", "A.Y. Ng", "M.I. Jordan"], "venue": "Journal of Machine Learning Research, vol. 3, pp. 993\u20131022, 2003.", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2003}, {"title": "Visualizing data using t-SNE", "author": ["L. Van der Maaten", "G.E. Hinton"], "venue": "Journal of Machine Learning Research, vol. 9, no. 2579- 2605, p. 85, 2008.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2008}, {"title": "Multidimensional binary search trees used for associative searching", "author": ["J.L. Bentley"], "venue": "Communications of the ACM, vol. 18, no. 9, pp. 509\u2013517, 1975.", "citeRegEx": "9", "shortCiteRegEx": null, "year": 1975}, {"title": "Time bounds for selection", "author": ["M. Blum", "R.W. Floyd", "V. Pratt", "R.L. Rivest", "R.E. Tarjan"], "venue": "Journal of Computer and System Sciences, vol. 7, no. 4, pp. 448\u2013461, 1973.", "citeRegEx": "10", "shortCiteRegEx": null, "year": 1973}], "referenceMentions": [{"referenceID": 0, "context": "For example, the principal component analysis (PCA) [1] attempts to combine the dimensions of H to form the dimensions in L that best explain the variance in the data.", "startOffset": 52, "endOffset": 55}, {"referenceID": 1, "context": "The multidimensional scaling (MDS) [2] M tries to preserve the distance between data points {p}, during the mapping from H to L, so that", "startOffset": 35, "endOffset": 38}, {"referenceID": 2, "context": "(1) The stochastic neighbor embedding (SNE) [3] type of algorithms further emphasize the local distances.", "startOffset": 44, "endOffset": 47}, {"referenceID": 3, "context": "Some popular choices include isomap [4], spectral embedding [5], and totally random trees embedding [6]).", "startOffset": 36, "endOffset": 39}, {"referenceID": 4, "context": "Some popular choices include isomap [4], spectral embedding [5], and totally random trees embedding [6]).", "startOffset": 60, "endOffset": 63}, {"referenceID": 5, "context": "Some popular choices include isomap [4], spectral embedding [5], and totally random trees embedding [6]).", "startOffset": 100, "endOffset": 103}, {"referenceID": 6, "context": "Topics are generated in H to summarize the activities, using techniques like the latent Dirichlet allocation (LDA) model [7].", "startOffset": 121, "endOffset": 124}, {"referenceID": 7, "context": "The topics are mapped from H to L via MDS in Figure 2 (a)(e) and t-SNE [8] in Figure 2 (c)(g).", "startOffset": 71, "endOffset": 74}, {"referenceID": 8, "context": "of the k-d tree[9] with some major differences", "startOffset": 15, "endOffset": 18}, {"referenceID": 9, "context": "When using the O(n) median of medians algorithm [10] in selecting the median at each depth level, the worst-case complexity is O(n logn).", "startOffset": 48, "endOffset": 52}], "year": 2016, "abstractText": "We propose the split-diffuse (SD) algorithm that takes the output of an existing dimension reduction algorithm, and distributes the data points uniformly across the visualization space. The result, called the topic grids, is a set of grids on various topics which are generated from the free-form text content of any domain of interest. The topic grids efficiently utilizes the visualization space to provide visual summaries for massive data. Topical analysis, comparison and interaction can be performed on the topic grids in a more perceivable way.", "creator": "LaTeX with hyperref package"}}}