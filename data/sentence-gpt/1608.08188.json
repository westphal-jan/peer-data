{"id": "1608.08188", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "29-Aug-2016", "title": "Visual Question: Predicting If a Crowd Will Agree on the Answer", "abstract": "Visual question answering (VQA) systems are emerging from a desire to empower users to ask any natural language question about visual content and receive a valid answer in response. However, close examination of the VQA problem reveals an unavoidable, entangled problem that multiple humans may or may not always agree on a single answer to a visual question. We train a model to automatically predict from a visual question whether a crowd would agree on a single answer to a visual question or not.\n\n\n\n\n\nWe have recently been working on a neural network to determine if a visual question is a natural-language question. The neural network consists of a large number of neural network layers, each containing about 30 trained neural networks. The first layer consists of two layers of neural network layers, with the second layer consisting of an internal network layer that receives more data from an external source. Each layer consists of an internal network layer, with the third layer consisting of an external network layer that receives more data from an external source.\nThe first layer consists of a neural network layer that receives more data from an external source. Each layer consists of an internal network layer, with the fourth layer consisting of an internal network layer that receives more data from an external source. The second layer consists of an internal network layer that receives more data from an external source. A series of layers of each layer consists of one layer consisting of three layers of one layer consisting of two layers of two layers of two layers of two layers of two layers of two layers of two layers of two layers of two layers of two layers of two layers of two layers of two layers of two layers of two layers of two layers of two layers of two layers of two layers of two layers of two layers of two layers of two layers of two layers of two layers of two layers of two layers of two layers of two layers of two layers of two layers of two layers of two layers of two layers of two layers of two layers of two layers of two layers of two layers of two layers of two layers of two layers of two layers of two layers of two layers of two layers of two layers of two layers of two layers of two layers of two layers of two layers of two layers of two layers of two layers of two layers of two layers of two layers of two layers of two layers of two layers of two layers of two layers of two layers of two layers of two layers of two layers of two layers of two layers of two layers of two layers of two layers of two layers of two layers of two layers", "histories": [["v1", "Mon, 29 Aug 2016 19:24:25 GMT  (5041kb,D)", "http://arxiv.org/abs/1608.08188v1", null]], "reviews": [], "SUBJECTS": "cs.AI cs.CL cs.CV cs.HC", "authors": ["danna gurari", "kristen grauman"], "accepted": false, "id": "1608.08188"}, "pdf": {"name": "1608.08188.pdf", "metadata": {"source": "CRF", "title": "Visual Question: Predicting If a Crowd Will Agree on the Answer", "authors": ["Danna Gurari", "Kristen Grauman"], "emails": [], "sections": [{"heading": null, "text": "1 Visual Question: Predicting If a Crowd Will Agree on the Answer\nDanna Gurari and Kristen Grauman Department of Computer Science The University of Texas at Austin\nVisual question answering (VQA) systems are emerging from a desire to empower users to ask any natural language question about visual content and receive a valid answer in response. However, close examination of the VQA problem reveals an unavoidable, entangled problem that multiple humans may or may not always agree on a single answer to a visual question. We train a model to automatically predict from a visual question whether a crowd would agree on a single answer. We then propose how to exploit this system in a novel application to efficiently allocate human effort to collect answers to visual questions. Specifically, we propose a crowdsourcing system that automatically solicits fewer human responses when answer agreement is expected and more human responses when answer disagreement is expected. Our system improves upon existing crowdsourcing systems, typically eliminating at least 20% of human effort with no loss to the information collected from the crowd.\nI. INTRODUCTION\nWhat would be possible if a person had an oracle that could immediately provide the answer to any question about the visual world? Sight-impaired users could quickly and reliably figure out the denomination of their currency and so whether they spent the appropriate amount for a product [1]. Hikers could immediately learn about their bug bites and whether to seek out emergency medical care. Pilots could learn how many birds are in their path to decide whether to change course and so avoid costly, life-threatening collisions. These examples illustrate several of the interests from a visual question answering (VQA) system, including tackling problems that involve classification, detection, and counting. More generally, the goal for VQA is to have a single system that can accurately answer any natural language question about an image or video [2], [3], [4].\nEntangled in the dream of a VQA system is an unavoidable issue that, when asking multiple people a visual question, sometimes they all agree on a single answer while other times they offer different answers (Figure 1). In fact, as we show in the paper, these two outcomes arise in approximately equal proportions in today\u2019s largest publicly-shared VQA benchmark that contains over 450,000 visual questions. Figure 1 illustrates that human disagreements arise for a variety of reasons including different descriptions of the same concept (e.g., \u201cminor\u201d and \u201cunderage\u201d), different concepts (e.g., \u201cghost\u201d and \u201cphotoshop\u201d), and irrelevant responses (e.g., \u201cno\u201d).\nOur goal is to account for whether different people would agree on a single answer to a visual question to improve upon today\u2019s VQA systems. We propose multiple prediction systems to automatically decide whether a visual question will lead to human agreement and demonstrate the value of these\npredictions for a new task of capturing the diversity of all plausible answers with less human effort.\nOur work is partially inspired by the goal to improve how to employ crowds as the computing power at run-time. Towards satisfying existing users, gaining new users, and supporting a wide range of applications, a crowd-powered VQA system should be low cost, have fast response times, and yield high quality answers. Today\u2019s status quo is to assume a fixed number of human responses per visual question and so a fixed cost, delay, and potential diversity of answers for every visual question [3], [1], [5]. We instead propose to dynamically solicit the number of human responses based on each visual question. In particular, we aim to accrue additional costs and delays from collecting extra answers only when extra responses are needed to discover all plausible answers. We show in our experiments that our system saves 19 40-hour work weeks and $1800 to answer 121,512 visual questions, compared to today\u2019s status quo approach [1].\nOur work is also inspired by the goal to improve how to employ crowds to produce the information needed to train\nar X\niv :1\n60 8.\n08 18\n8v 1\n[ cs\n.A I]\n2 9\nA ug\n2 01\n6\n2 and evaluate automated methods. Specifically, researchers in fields as diverse as computer vision [3], computational linguistics [2], and machine learning [4] rely on large datasets to improve their VQA algorithms. These datasets include visual questions and human-supplied answers. Such data is critical for teaching machine learning algorithms how to answer questions by example. Such data is also critical for evaluating how well VQA algorithms perform. In general, \u201cbigger\u201d data is better. Current methods to create these datasets assume a fixed number of human answers per visual question [3], [5], thereby either compromising on quality by not collecting all plausible answers or cost by collecting additional answers when they are redundant. We offer an economical way to spend a human budget to collect answers from crowd workers. In particular, we aim to actively allocate additional answers only to visual questions likely to have multiple answers.\nThe key contributions of our work are as follows: \u2022 Analysis demonstrating the prevalence and reasons for\nhuman answer disagreements in today\u2019s largest, freelyavailable VQA benchmark. \u2022 A new problem and system for predicting whether a crowd will (dis)agree when answering a visual question. \u2022 A novel application for efficient answer collection which solicits additional answers from additional members of a crowd when disagreement is anticipated."}, {"heading": "II. RELATED WORK", "text": "Visual Question Answering Services: Researchers spanning communities as diverse as human computer interaction, machine learning, computational linguistics, and computer vision have proposed a variety of ways to answer questions about images [2], [3], [1], [4]. Yet a commonality across these communities is they adopt a one-size-fits-all approach when deciding the number of answers for any visual question. For example, crowd-powered systems aim to supply a prespecified, fixed number of answers per visual question [1] and automated systems return a single answer for every visual question [2], [3], [4]. Inspired by the observation that there can be multiple plausible answers per visual question, we propose a richer representation of visual question answering that accounts for whether different people would agree on a single answer. We propose a system that automatically predicts whether humans will disagree. We demonstrate the predictive advantage of our system over relying on the uncertainty of a VQA algorithm in its predicted answer [3].\nAnswer Collection from a Crowd: Our work relates to methods that propose how to employ crowd workers to answer questions about images. Such approaches aim to collect a pre-specified, fixed number of answers per visual question. For those systems that treat response time as a first priority, a variable number of answers may arise but this is due to varying crowdsourcing conditions such as the available supply of workers [1], [6]. Other systems ensure a fixed number of answers are collected per visual question [3], [5]. Unlike prior work, our goal is to collect answers in a way that is both economical and complete in capturing the diversity of plausible answers for all visual questions. To our knowledge,\nour work is the first to predict the number of answers to collect for a visual question. Experiments demonstrate that our disagreement predictions are useful to significantly reduce human effort for capturing the diversity of valid answers for 121,512 visual questions.\nAnalyses of Crowd Disagreement: More broadly, our work relates to efforts to account for crowd disagreement. For example, researchers have suggested ways to resolve crowd disagreement due to task difficulty [7] and ambiguity/specificity [8], [9]. Some methods demonstrate which workers to trust most when aggregating multiple responses into a final, single response [10], [7]. Other methods leverage context to automatically disambiguate which of multiple outcomes is the desired outcome [9]. Unlike prior work, we focus on the task of visual question answering. Moreover, while prior work focuses on resolving specific sources of crowd disagreement (e.g., task difficulty or ambiguity), we instead propose a single, integrated system that jointly detects various sources of crowd disagreement that arise for visual question answering. The advantage of this approach is to separate \u201ceasy to answer\u201d instances from all instances that would require additional effort to resolve the disagreement; e.g., collect multiple answers for ambiguous and subjective tasks or apply an aggregation scheme to produce a single answer from multiple answers when crowd workers are unreliable.\nHigh Quality Work with Fixed Human Budget: Our work aligns with methods that actively allocate a limited human budget to where it will best contribute to improving the quality of results. For example, one method distributes a budget between three different levels of human effort when deciding how to segment images [11]. Another method spends a budget between less costly crowd workers and more costly expert efforts to improve outcomes for biomedical citation screening [12]. Another method predicts when to employ algorithms versus crowd workers to segment images [13]. To our knowledge, our work is the first towards deciding how to spend a budget for the task of visual question answering, which is distinct from prior work which focused on spending a budget for image analysis or language analysis alone. Furthermore, our aim is to spend a budget to capture the diversity of all valid results for every task rather than to collect a single result for every task.\nMinimizing Human Labeling: Our aim to actively decide how to allocate human effort to improve results is also somewhat related to active learning [14]. Specifically, active learners try to use as little human effort as possible to train accurate prediction models. Some methods iteratively supplement a training dataset with the most informative images for training a classifier [15], [16]. Other methods solicit redundant labels to prevent incorrect/noisy labels from teaching prediction models to make mistakes [17], [18]. While active learners aim to minimize human input to improve the accuracy of a prediction model, our method aims to minimize human input while still exhaustively capturing all plausible answers to all visual questions.\nContinuous Dialogue with the Crowd: Two services - Be My Eyes [19] and Chorus:View [20] - offer users a continuous communication channel with members of the crowd to answer\n3 visual questions. The aim is to expedite arriving at desired answers to, for example, clarify ambiguous questions. Our work offers an alternative by demonstrating how a crowdsourcing service might instead solicit multiple answers for a one time back-and-forth rather than enacting a more costly, continuous communication channel with a single voice, whether from a single person [19] or the consensus of a crowd [20]."}, {"heading": "III. PAPER OVERVIEW", "text": "The remainder of the paper is organized into four sections. We first describe a study where we investigate: 1) How much answer diversity arises for visual questions? and 2) Why do people disagree (Section IV)? Next, we explore the following two questions: 1) Given a novel visual question, can a machine correctly predict whether multiple independent members of a crowd would supply the same answer? and 2) If so, what insights does our machine-learned system reveal regarding what humans are most likely to agree about (Section V)? In the following section, we propose a novel resource allocation system for efficiently capturing the diversity of all answers for a set of visual questions (Section VI). Finally, we end with concluding remarks (Section VII).\nIV. VQA - ANALYSIS OF ANSWER (DIS)AGREEMENTS\nOur first aim is to answer the following questions: 1) How much answer diversity arises for visual questions? and 2) Why do people disagree?\nVQA Datasets: We conduct our analysis on a total of 459,861 visual questions and 4,598,610 answers coming from today\u2019s largest freely-available VQA benchmark [3]. We chose this benchmark because it both represents a diversity of visual questions and includes many crowdsourced answers for every visual question.\nThe benchmark consists of two datasets that reflect VQAs for real images and abstract scenes. Specifically, 80% (i.e., 369,861) of VQAs are about real images that show 91 types of objects that would be \u201ceasily recognizable by a 4 year old\u201d in their natural context [21]. The remaining 90,000 VQAs are about abstract scenes that were created with clipart and show 100 types of everyday objects often observed in real images [3].\nThe benchmark includes a diversity of visual questions intentionally collected to be both grounded in images and taskindependent. Towards this aim, visual questions were collected by asking three Amazon Mechanical Turk (AMT) crowd workers to look at a given image and generate a text-based question about it that would \u201cstump a smart robot\u201d [3]. Three open-ended questions were collected about each of 153,287 images, resulting in a total of 459,861 visual questions.\nThe benchmark also includes 10 open-ended natural language answers from 10 AMT crowd workers per visual question. Each answer was collected by showing a worker an image with associated question and asking him/her to respond with \u201ca brief phrase and not a complete sentence\u201d [3].\nFinally, to enrich our analysis, we leverage the included labels which indicate the type of answer elicited for each visual question. Specifically, each visual question is labeled\nas eliciting one of the following types of answers: \u201cyes/no\u201d, \u201cnumber\u201d, or \u201cother\u201d. This label was determined as the most popular option from 10 labels assigned to the associated 10 answers for each visual question.\nDefining Answer Diversity: We compute answer diversity for a visual question by counting how many unique and valid answers are observed in the set of answers. We derive results using the 10 crowdsourced answers per visual question.\nWe establish unique answers by pre-processing each answer to eliminate cosmetic differences and then applying exact string matching to identify the number of different answers. We pre-process each answer by converting all letters to lower case, converting numbers to digits, and removing punctuation and articles (i.e., \u201ca\u201d, \u201can\u201d, \u201cthe\u201d), as was done in prior work [3]. While this approach does not fully resolve all conceptually equivalent responses, it does reveal an upper bound of expected answer diversity. In other words, more lenient agreement schemes (e.g., employing sophisticated natural language processing methods) would lead to either the same or less answer diversity.\nWe establish valid answers by tallying the number of times each unique answer is given and then only accepting answers observed from at least m people, where m is an application-specific parameter to set. In practice, prior work deems answers as 100% valid using blind trust (i.e., m = 1 person) [22] as well as more conservative answer validation schemes (i.e., m = 3 people) [3].\nMeasuring Answer Diversity: We now turn to the question of how much answer diversity is observed in practice for visual questions. Across all 459,861 visual questions, we tally how many visual questions yield k unique, valid answers where k = {1, 2, ..., 10}.\nWe enrich our analysis on measured answer diversity by examining the influence of different levels of trust in the crowd as well as the influence of different datasets. Specifically, we tally the number of unique, valid answers observed when requiring a minimum of m = 1, m = 2, or m = 3 members of the crowd to offer the same answer for an answer to be valid. We conduct our analysis on the two datasets of visual questions about real images and abstract scenes independently.\nThe majority of visual questions lead to at most three unique answers, across all crowd trust levels and both datasets (Figure 2). This gives an upper bound of expected answer diversity. We anticipate measured answer diversity will drop with less stringent answer agreement schemes than exact string matching, such as inferring agreement when an answer is a synonym or plurality of another answer.\nOur results show the same trend for the amount of answer diversity with all three agreement thresholds, for both datasets (Figure 2). Most commonly there is one unique answer, followed by two and three answers respectively. In addition, as expected, moving from requiring no answer agreement to a more conservative agreement between three people shifts the overall distribution to more sharply peak at less overall diversity (i.e., 1 unique answer). Our results also show that VQA statistics can transfer from one source of images to another, revealing a possible benefit of using artificially-\n4 Real Images - 369,861 VQAs Abstract Scenes - 90,000 VQAs\nAnswer Type: Yes/No Number Other Yes/No Number Other # VQAs (%): 140,777 (38%) 45,822 (12%) 183,262 (50%) 36,717 (41%) 12,956 (14%) 40,327 (45%)\nAt Most One Disagreement 74% 49% 35% 74% 79% 36% - Unanimous Agreement 54% 35% 22% 57% 65% 22% - Exactly One Disagreement 20% 14% 13% 17% 14% 14%\nTABLE I: Correlation between answer agreement and visual questions that elicit three different types of answers for VQAs on real images and abstract scenes. Shown for each answer type is the percentage of visual questions that lead to at most one disagreement (row 1), unanimous agreement (row 2), and exactly one disagreement (row 3) from 10 crowdsourced answers. On average, across all answer types for both datasets, the crowd agrees on the answer for nearly half (i.e., 53%) of all VQAs. Moreover, we observe crowd disagreement arises often for all three answer types, highlighting that the significance of crowd disagreement is applicable across various types of visual questions.\ngenerated images to learn trends when more costly real world images are not readily-available (Figure 2a vs 2b).\nFrom the 369,861 visual questions about real images, we found that only 1% (i.e., 3,992) and 5% (i.e., 19,682) of visual questions have no valid answer when limiting valid answers to those which have at least two or three people agreeing on them respectively. This suggests that a crowd is able to reach some level of consensus on what are acceptable answers for the vast majority of visual questions.\nReasons for Answer (Dis)Agreements: Our second aim in analyzing the VQA benchmark is to better understand why people disagree when answering visual questions.\nFigure 3 highlights various reasons for why crowd workers disagree on an answer. Disagreements can arise due to crowd worker skill, both because a difficult task necessitates domain expertise and because a crowd worker may inadequately answer a seemingly simple question (Figure 3b,c). Crowds disagree also because of ambiguity in the question and visual content (Figure 3d,e). Further reasons for disagreement include insufficient visual evidence to answer the question, subjective questions, synonymous answers, and varying levels of answer granularity (Figure 3f\u2013i). We capitalize on these observations in the next section to design prediction systems that automatically separate visual questions that lead to agreement.\nWe enrich our understanding of why crowds disagree by examining how frequently crowds (dis)agree with respect to visual questions that elicit different types of answers. We tally the number of visual questions that lead to \u201cyes/no\u201d, \u201cnumber\u201d, and \u201cother\u201d answers. We report results for both when crowds unanimously agree as well as when nine of the ten people agree for both datasets (Table I). These results capture when at most one untrusted result is permitted from the crowd when inferring whether a crowd agrees. Overall, we observe at most one disagreement for 51.6% of real images and 57.6% for abstract scenes. We find that disagreement arises often for all types of answers, highlighting that the interest in crowd disagreement is of widespread interest for many types of visual questions and different datasets.\nWe observe similar crowd agreement trends across the two datasets for two of the three answer types (Table I). We find high agreement for \u201cyes/no\u201d images for both datasets. We hypothesize that the remaining quarter of asked \u201cyes/no\u201d questions that lead to greater amounts of disagreement are subjective questions and so lead to split opinions among a crowd (e.g., \u201cDoes this picture look scary?\u201d, Figure 3g). We observe moderate agreement levels for \u201cother\u201d visual questions, possibly due to a greater diversity of opinions regarding the true answer as well as ways to express the same concept. We find the greatest difference between results for the two datasets on \u201cnumber\u201d visual questions. We hypothesize counting problems are easier for less complex images that show few objects, as is consistently the case for the abstract scenes but not the real images.\n5\nV. VISUAL QUESTION - PREDICTING IF A CROWD WILL (DIS)AGREE ON AN ANSWER\nWe now explore the following two questions: 1) Given a novel visual question, can a machine correctly predict whether multiple independent members of a crowd would supply the same answer? and 2) If so, what insights does our machinelearned system reveal regarding what humans are most likely to agree about?\nPrediction Systems\nWe pose the prediction task as a binary classification problem. Specifically, given an image and associated question, a system outputs a binary label indicating whether a crowd will agree on the same answer. Our goal is to design a system that can detect which visual questions to assign a disagreement label, regardless of the disagreement cause (e.g., subjectivity, ambiguity, difficulty). We implement both random forest and deep learning classifiers.\nAnswer (Dis)Agreement Labels: A visual question is assigned either an answer agreement or disagreement label. To assign labels, we employ 10 crowdsourced answers for each visual question. A visual question is assigned an answer agreement label when there is an exact string match for 9 of the 10 crowdsourced answers (after answer pre-preprocessing, as discussed in the previous section) and an answer disagreement label otherwise. Our rationale is to permit the possibility of up to one \u201ccareless/spam\u201d answer per visual question. The outcome of our labeling scheme is that a disagreement label is agnostic to the specific cause of disagreement and rather represents the many causes (described above).\nRandom Forest System: For our first system, we use domain knowledge to guide the learning process. We compile a set of features that we hypothesize inform whether a crowd will arrive at an undisputed, single answer. Then we apply a machine learning tool to reveal the significance of each feature. We propose features based on the observation that answer agreement often arises when 1) a lay person\u2019s attention can be easily concentrated to a single, undisputed region in an image and 2) a lay person would find the requested task easy to address.\nWe employ five image-based features coming from the salient object subitizing [23] (SOS) method, which produces five probabilities that indicate whether an image contains 0, 1, 2, 3, or 4+ salient objects. Intuitively, the number of salient objects shows how many regions in an image are competing for an observer\u2019s attention, and so may correlate with the ease in identifying a region of interest. Moreover, we hypothesize this feature will capture our observation from the previous study that counting problems typically leads to disagreement for images showing many objects, and agreement otherwise.\nWe employ a 2,492-dimensional feature vector to represent the question-based features. One feature is the number of words in the question. Intuitively, a longer question offers more information and we hypothesize additional information makes a question more precise. The remaining features come from two one-hot vectors describing each of the first two words in the question. Each one-hot vector is created using the learned vocabularies that define all possible words at the first and second word location of a question respectively (using training data, as described in the next section). Intuitively, early words in a question inform the type of answers that\n6\nmight be possible and, in turn, possible reasons/frequency for answer disagreement. For example, we expect \u201cwhy is\u201d to regularly elicit many opinions and so disagreement. This intuition about the beginning words of a question is also supported by our analysis in the previous section which shows that different answer types yield different biases of eliciting answer agreement versus disagreement.\nWe leverage a random forest classification model [24] to predict an answer (dis)agreement label for a given visual question. This model consists of an ensemble of decision tree classifiers. We train the system to learn the unique weighted combinations of the aforementioned 2,497 features that each decision tree applies to make a prediction. At test time, given a novel visual question, the trained system converts a 2,497 feature descriptor of the visual question into a final prediction that reflects the majority vote prediction from the ensemble of decision trees. The system returns the final prediction along with a probability indicating the system\u2019s confidence in that prediction. We employ the Matlab implementation of random forests, using 25 trees and the default parameters.\nDeep Learning System: We next adapt a VQA deep learning architecture [25] to learn the predictive combination of visual and textual features. The question is encoded with a 1024-dimensional LSTM model that takes in a onehot descriptor of each word in the question. The image is described with the 4096-dimensional output from the last fully connected layer of the Convolutional Neural Network (CNN), VGG16 [26]. The system performs an element-wise multiplication of the image and question features, after linearly transforming the image descriptor to 1024 dimensions. The final layer of the architecture is a softmax layer.\nWe train the system to predict (dis)agreement labels with training examples, where each example includes an image and question. At test time, given a novel visual question, the system outputs an unnormalized log probability indicating its belief in both the agreement and disagreement label. For our system\u2019s prediction, we convert the belief in the disagreement label into a normalized probability. Consequently, predicted\nvalues range from 0 to 1 with lower values reflecting greater likelihood for crowd agreement.\nAnalysis of Prediction System We now describe our studies to assess the predictive power of our classification systems to decide whether visual questions will lead to answer (dis)agreement.\nWe capitalize on today\u2019s largest visual question answering dataset [3] to evaluate our prediction system, which includes 369,861 visual questions about real images. Of these, 248,349 visual questions (i.e., Training questions 2015 v1.0) are kept for for training and the remaining 121,512 visual questions (i.e., Validation questions 2015 v1.0) are employed for testing our classification system. This separation of training and testing samples enables us to estimate how well a classifier will generalize when applied to an unseen, independent set of visual questions.\nTo our knowledge, no prior work has directly addressed predicting answer (dis)agreement for visual questions. Therefore, we employ as a baseline a related VQA algorithm [25], [3] which produces for a given visual question an answer with a confidence score. This system parallels the deep learning architecture we adapt. However, it predicts the system\u2019s uncertainty in its own answer, whereas we are interested in the humans\u2019 collective disagreement on the answer. Still, it is a useful baseline to see if an existing algorithm could serve our purpose.\nClassification Performance: We evaluate the predictive power of the classification systems based on each classifier\u2019s predictions for the 121,512 visual questions in the test dataset. We first show performance of the baseline and our two prediction systems using precision-recall curves. The goals are to achieve a high precision, to minimize wasting crowd effort when their efforts will be redundant, and a high recall, to avoid missing out on collecting the diversity of accepted answers from a crowd. We also report the average precision (AP), which indicates the area under a precision-recall curve. AP values range from 0 to 1 with better-performing prediction systems having larger values.\n7\nFigure 4a shows precision-recall curves for all prediction systems. Both our proposed classification systems outperform the VQA Algorithm [3] baseline; e.g., Ours - RF yields a 12 percentage point improvement with respect to AP. This is interesting because it shows there is value in learning the disagreement task specifically, rather than employing an algorithm\u2019s confidence in its answers. More generally, our results demonstrate it is possible to predict whether a crowd will agree on a single answer from a given image and associated question. Despite the significant variety of questions and image content, and despite the variety of reasons for which the crowd can disagree, our learned model is able to produce quite accurate results1.\nWe observe our Random Forest classifier outperforms our deep learning classifier; e.g., Ours: RF yields a three percentage point improvement with respect to AP while consistently yielding improved precision-recall values over Ours: LSTM-CNN (Figure 4a). In general, deep learning systems hold promise to replace handcrafted features to pick out the discriminative features. Our baselines highlight a possible value in developing a different deep learning architecture for the problem of learning answer disagreement than applied for predicting answers to visual questions.\nWe show examples of prediction results where our topperforming RF classifier makes its most confident predictions (Figure 4b). In these examples, the predictor expects human agreement for \u201cwhat room... ?\u201d visual questions and disagreement for \u201cwhy... ?\u201d visual questions. These examples highlight that the classifier may have a strong language prior towards making predictions, as we will discuss in the next section.\nPredictive Cues: We now explore what makes a visual question lead to crowd answer agreement versus disagreement. We examine the influence of whether visual questions lead to the three types of answers (\u201cyes/no\u201d, \u201cnumber\u201d, \u201cother\u201d) for both our random forest (RF) and deep learning (DL) classification systems. We enrich our analysis by examining the predictive performance of both classifiers when they are trained and tested exclusively with image and question features respectively. Figure 5 shows precision-recall curves for both classification systems with question features alone (Q), image\n1We observe no change to predictive performance of the random forest classifier when instead training the model such that an agreement label is assigned to a visual question only when all 10 answers match. In other words, we see no difference in predictive power when we flip labels for all examples where nine people agree and one person disagrees.\nfeatures alone (I), and both question and image features together (Q+I).\nWhen comparing AP scores (Figure 5), we observe our Q+I predictors yield the greatest predictive performance for visual questions that lead to \u201cother\u201d answers, followed by \u201cnumber\u201d answers, and finally \u201cyes/no\u201d answers. One possible reason for this finding is that the question wording strongly drives whether a crowd will disagree for \u201cother\u201d visual questions, whereas some notion of common sense may be required to learn whether a crowd will agree for \u201cyes/no\u201d visual questions (e.g., Figure 3a vs Figure 3g).\nWe observe that question-based features yield greater predictive performance than image-based features for all visual questions, when comparing AP scores for Q and I classification results (Figure 5). In fact, image features contribute to performance improvements only for our random forest classifier for visual questions that lead to \u201cnumber\u201d answers, as illustrated by comparing AP scores for Our RF: Q+I and Our RF: Q (Figure 5b). Our overall finding that most of the predictive power stems from language-based features parallels feature analysis findings in the automated VQA literature [3], [22]. This does not mean, however, that the image content is not predictive. Further work improving visual content cues for VQA agreement is warranted.\nOur findings suggest that our Random Forest classifier\u2019s overall advantage over our deep learning system arises because of counting questions, as indicated by higher AP scores (Figure 5). For example, the advantage of the initial higher precision (Figure 4a; Ours: RF vs Ours: DL) is also observed for counting questions (Figure 5b; Ours: RF - Q+I vs Ours: DL - Q+I). We hypothesize this advantage arises due to the strength of the Random Forest classifier in pairing the question prior (\u201cHow many?\u201d) with the imagebased SOS features that indicates the number of objects in an image. Specifically, we expect \u201chow many\u201d to lead to agreement only for small counting problems."}, {"heading": "VI. CAPTURING ANSWER DIVERSITY WITH LESS EFFORT", "text": "We next present a novel resource allocation system for efficiently capturing the diversity of true answers for a batch of visual questions. Today\u2019s status quo is to either uniformly collect N answers for every visual question [3] or collect multiple answers where the number is determined by external crowdsourcing conditions [1]. Our system instead spends a human budget by predicting the number of answers to collect\n8\nfor each visual question based on whether multiple human answers are predicted to be redundant.\nAnswer Collection System\nSuppose we have a budget B which we can allocate to collect extra answers for a subset of visual questions. Our system automatically decides to which visual questions to allocate the \u201cextra\u201d answers in order to maximize captured answer diversity for all visual questions.\nThe aim of our system is to accrue additional costs and delays from collecting extra answers only when extra responses will provide more information. Towards this aim, our system involves three steps to collect answers for all N visual questions (Figure 6a). First, the system applies our topperforming random forest classifier to every visual question in the batch. Then, the system ranks the N visual questions based on predicted scores from the classifier, from visual questions most confidently predicted to lead to answer \u201cagreement\u201d from a crowd to those most confidently predicted to lead to answer \u201cdisagreement\u201d from a crowd. Finally, the system solicits more (R) human answers for the B visual questions predicted to reflect the greatest likelihood for crowd disagreement and fewer (S) human answers for the remaining visual questions. More details below.\nAnalysis of Answer Collection System\nWe now describe our studies to assess the benefit of our allocation system to reduce human effort to capture the diversity of all answers to visual questions.\nExperimental Design: We evaluate the impact of actively allocating extra human effort to answer visual questions as a function of the available budget of human effort. Specifically, for a range of budget levels, we compute the total measured answer diversity (as defined below) resulting for the batch of visual questions. The goal is to capture a large amount of answer diversity with little human effort.\nWe conduct our studies on the 121,512 test visual questions about real images (i.e., Validation questions 2015 v1.0). For each visual question, we establish the set of true answers as all unique answers which are observed at least twice in the 10 crowdsourced answers per visual question. We require agreement by two workers to avoid the possibility that \u201ccareless/spam\u201d answers are treated as ground truth.\nSystem Implementation: We collect either the minimum of S = 1 answer per visual question or the maximum of R = 5 answers per visual question. Our number of answers roughly aligns with existing crowd-powered VQA systems, for example with VizWiz, \u201cOn average, participants received 3.3 (SD=1.8) answers for each question\u201d [1]. Our maximum number of answers also supports the possibility of capturing the maximum of three unique, valid answers typically observed in practice (recall study above). While more elaborate schemes for distributing responses may be possible, we will show this approach already proves quite effective in our experiments. We simulate answer collection by randomly selecting answers from the 10 crowd answers per visual question.\nBaselines: We compare our approach to the following baselines:\nVQA Algorithm [3]: As in the previous section, we leverage the output\n9 confidence score from the publicly-shared model [25] learned from a LSTM-CNN deep learning architecture to rank the order of priority for visual questions to receive redundancy.\nStatus Quo: The system randomly prioritizes which images receive redundancy. This predictor illustrates the best a user can achieve today with crowd-powered systems [1], [6] or with current dataset collection methods [3], [5].\nEvaluation Methodology: We quantify the total diversity of answers captured by a resource allocation system for a batch of visual questions Q as follows:\nD(Q) = |B|\u2211 i=1 |ri \u2229 qi|+ |Q\\B|\u2211 j=1 |sj \u2229 qj | (1)\nwhere qi represents the set of all true answers for the i-th visual question, ri represents the set of unique answers captured in the R answers collected for the i-th visual question, and sj represents the set of unique answers captured in the S answers collected for the j-th visual question. Given no extra human budget, total diversity comes from the second term which indicates the diversity captured when only S answers are collected for every visual question. Given a maximum available extra human budget (B), total diversity comes from the first term which indicates the diversity captured when R answers are collected for every visual question. Given a partial extra human budget (B), the aim is to have perfect predictions such that the minimum number of answers (S) are allocated only for visual questions with one true answer so that all diverse answers are safely captured.\nWe measure diversity per visual question as the number of all true answers collected per visual question (|a\u2229 b|). Larger values reflect greater captured diversity. The motivation for this measure is to only give total credit to visual questions when all valid, unique human answers are collected.\nResults: Our system consistently offers significant gains over today\u2019s status quo approach (Figure 6b). For example, our system accelerates the collection of 70% of the diversity by 21% over the Status Quo baseline. In addition, our system accelerates the collection of the 82% of diversity one would observe with VizWiz by 23% (i.e., average of 3.3 answers per visual question). In absolute terms, this means eliminating the collection of 92,180 answers with no loss to captured answer diversity. This translates to eliminating 19 40-hour work weeks and saving over $1800, assuming workers are paid $0.02 per answer and take 30 seconds to answer a visual question. Our approach fills an important gap in the crowdsourcing answer collection literature for targeting the allocation of extra answers only to visual questions where a diversity of answers is expected.\nFigure 6b also illustrates the advantage of our system over a related VQA algorithm [3] for our novel application of costsensitive answer collection from a crowd. As observed, relying on an algorithm\u2019s confidence in its answer offers a valuable indicator over today\u2019s status quo of passively budgeting.\nWhile we acknowledge this method is not intended for our task specifically, it still serves as an important baseline (as discussed above). We attribute the further performance gains of our prediction system to it directly predicting whether humans will disagree rather than predicting a property of a specific algorithm (e.g., confidence of the Antol et al. algorithm in its answer prediction)."}, {"heading": "VII. CONCLUSIONS", "text": "We proposed a new problem of predicting whether different people would answer with the same response to the same visual question. Towards motivating the practical implications for this problem, we analyzed nearly half a million visual questions and demonstrated there is nearly a 50/50 split between visual questions that lead to answer agreement versus disagreement. We observed that crowd disagreement arose for various types of answers (yes/no, counting, other) for many different reasons. We next proposed a system that automatically predicts whether a visual question will lead to a single versus multiple answers from a crowd. Our method outperforms a strong existing VQA system limited to estimating system uncertainty rather than crowd disagreement. Finally, we demonstrated how to employ the prediction system to accelerate the collection of diverse answers from a crowd by typically at least 20% over today\u2019s status quo of fixed redundancy allocation."}, {"heading": "ACKNOWLEDGMENTS", "text": "The authors gratefully acknowledge funding from the Office of Naval Research (ONR YIP N00014-12-1-0754) and National Science Foundation (IIS-1065390). We thank Dinesh Jayaraman, Yu-Chuan Su, Suyog Jain, and Chao-Yeh Chen for their assistance with experiments."}], "references": [{"title": "Vizwiz: Nearly realtime answers to visual questions", "author": ["J.P. Bigham", "C. Jayant", "H. Ji", "G. Little", "A. Miller", "R.C. Miller", "R. Miller", "A. Tatarowicz", "B. White", "S. White", "T. Yeh"], "venue": "ACM symposium on User interface software and technology (UIST), 2010, pp. 333\u2013342.", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2010}, {"title": "Learning to compose neural networks for question answering", "author": ["J. Andreas", "M. Rohrbach", "T. Darrell", "D. Klein"], "venue": "Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (NAACL HLT), 2016, pp. 1545\u20141554.", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2016}, {"title": "VQA: Visual Question Answering", "author": ["S. Antol", "A. Agrawal", "J. Lu", "M. Mitchell", "D. Batra", "C.L. Zitnick", "D. Parikh"], "venue": "IEEE International Conference on Computer Vision (ICCV), 2015, pp. 2425\u20132433.", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2015}, {"title": "A multi-world approach to question answering about real-world scenes based on uncertain input", "author": ["M. Malinowski", "M. Fritz"], "venue": "Advances in Neural Information Processing Systems (NIPS), 2014, pp. 1682\u20131690.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2014}, {"title": "Visual madlibs: Fill in the blank image generation and question answering", "author": ["L. Yu", "E. Park", "A.C. Berg", "T.L. Berg"], "venue": "IEEE International Conference on Computer Vision (ICCV), 2015, pp. 2461\u20132469.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2015}, {"title": "Crowdsourcing subjective fashion advice using VizWiz: Challenges and opportunities", "author": ["M.A. Burton", "E. Brady", "R. Brewer", "C. Neylan", "J.P. Bigham", "A. Hurst"], "venue": "ACM SIGACCESS conference on Computers and accessibility (ASSETS), 2012, pp. 135\u2013142.", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2012}, {"title": "The multidimensional wisdom of crowds", "author": ["P. Welinder", "S. Branson", "S. Belongie", "P. Perona"], "venue": "Advances in Neural Information Processing Systems (NIPS), 2010, pp. 2424\u20132432.", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2010}, {"title": "Image specificity", "author": ["M. Jas", "D. Parikh"], "venue": "IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2015, pp. 2727\u2013 2736.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2015}, {"title": "Multiview triplet embedding: Learning attributes in multiple maps", "author": ["E. Amid", "A. Ukkonen"], "venue": "International Conference on Machine Learning (ICML), 2015, pp. 1472\u20131480.  10", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2015}, {"title": "SQUARE: A benchmark for research on computing crowd consensus", "author": ["A. Sheshadri", "M. Lease"], "venue": "AAAI Conference on Human Computation and Crowdsourcing (HCOMP), 2013.", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2013}, {"title": "Predicting sufficient annotation strength for interactive foreground segmentation", "author": ["S.D. Jain", "K. Grauman"], "venue": "IEEE International Conference on Computer Vision (ICCV), 2013, pp. 1313\u20131320.", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2013}, {"title": "Combining crowd and expert labels using decision theoretic active learning", "author": ["A.T. Nguyen", "B.C. Wallace", "M. Lease"], "venue": "AAAI Conference on Human Computation and Crowdsourcing (HCOMP), 2015.", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2015}, {"title": "Pull the plug? predicting if computers or humans should segment images", "author": ["D. Gurari", "S.D. Jain", "M. Betke", "K. Grauman"], "venue": "IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2016, pp. 382\u2013391.", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2016}, {"title": "Active learning literature survey", "author": ["B. Settles"], "venue": "University of Wisconsin, Madison, Tech. Rep., 2010.", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2010}, {"title": "Tropel: Crowdsourcing detectors with minimal training", "author": ["G. Patterson", "G.V. Horn", "S. Belongie", "P. Perona", "J. Hays"], "venue": "AAAI Conference on Human Computation and Crowdsourcing (HCOMP), 2015.", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2015}, {"title": "Cost-sensitive active visual category learning", "author": ["S. Vijayanarasimhan", "K. Grauman"], "venue": "International Journal of Computer Vision (IJCV), vol. 91, no. 1, 2011, pp. 24\u201344.", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2011}, {"title": "To re(label), or not to re(label)", "author": ["C.H. Lin", "Mausam", "D.S. Weld"], "venue": "AAAI Conference on Human Computation and Crowdsourcing (HCOMP), 2014.", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2014}, {"title": "Get another label? Improving data quality and data mining using multiple, noisy labelers", "author": ["V.S. Sheng", "F. Provost", "P.G. Ipeirotis"], "venue": "International Conference on Knowledge Discovery and Data Mining (KDD), 2008, pp. 614\u2013622.", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2008}, {"title": "Answering visual questions with conversational crowd assistants", "author": ["W.S. Lasecki", "P. Thiha", "Y. Zhong", "E. Brady", "J.P. Bigham"], "venue": "ACM SIGACCESS Conference on Computers and Accessibility (ASSETS), 2013.", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2013}, {"title": "Microsoft COCO: Common objects in context", "author": ["T. Lin", "M. Maire", "S. Belongie", "J. Hays", "P. Perona", "D. Ramanan", "P. Dollar", "C.L. Zitnick"], "venue": "IEEE European Conference on Computer Vision (ECCV), 2014, pp. 740\u2013 755.", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2014}, {"title": "Ask your neurons: A neural-based approach to answering questions about images", "author": ["M. Malinowski", "M. Rohrbach", "M. Fritz"], "venue": "IEEE European Conference on Computer Vision (ECCV), 2015, pp. 1\u20139.", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2015}, {"title": "Salient object subitizing", "author": ["J. Zhang", "S. Ma", "M. Sameki", "S. Sclaroff", "M. Betke", "Z. Lin", "X. Shen", "B. Price", "R. Mech"], "venue": "IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2015, pp. 4045\u2013 4054.", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2015}, {"title": "Random forests", "author": ["L. Breiman"], "venue": "Machine Learning, vol. 45, no. 1, pp. 5\u201332, 2001.", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2001}, {"title": "Deeper lstm and normalized cnn visual question answering model", "author": ["J. Lu", "X. Lin", "D. Batra", "D. Parikh"], "venue": "https://github.com/VT-vision-lab/ VQA LSTM CNN, 2015.", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2015}, {"title": "Very deep convolutional networks for large-scale image recognition", "author": ["K. Simonyan", "A. Zisserman"], "venue": "arXiv preprint arXiv:1409.1556, 2014.", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2014}], "referenceMentions": [{"referenceID": 0, "context": "What would be possible if a person had an oracle that could immediately provide the answer to any question about the visual world? Sight-impaired users could quickly and reliably figure out the denomination of their currency and so whether they spent the appropriate amount for a product [1].", "startOffset": 288, "endOffset": 291}, {"referenceID": 1, "context": "an image or video [2], [3], [4].", "startOffset": 18, "endOffset": 21}, {"referenceID": 2, "context": "an image or video [2], [3], [4].", "startOffset": 23, "endOffset": 26}, {"referenceID": 3, "context": "an image or video [2], [3], [4].", "startOffset": 28, "endOffset": 31}, {"referenceID": 2, "context": "Today\u2019s status quo is to assume a fixed number of human responses per visual question and so a fixed cost, delay, and potential diversity of answers for every visual question [3], [1], [5].", "startOffset": 175, "endOffset": 178}, {"referenceID": 0, "context": "Today\u2019s status quo is to assume a fixed number of human responses per visual question and so a fixed cost, delay, and potential diversity of answers for every visual question [3], [1], [5].", "startOffset": 180, "endOffset": 183}, {"referenceID": 4, "context": "Today\u2019s status quo is to assume a fixed number of human responses per visual question and so a fixed cost, delay, and potential diversity of answers for every visual question [3], [1], [5].", "startOffset": 185, "endOffset": 188}, {"referenceID": 0, "context": "We show in our experiments that our system saves 19 40-hour work weeks and $1800 to answer 121,512 visual questions, compared to today\u2019s status quo approach [1].", "startOffset": 157, "endOffset": 160}, {"referenceID": 2, "context": "fields as diverse as computer vision [3], computational linguistics [2], and machine learning [4] rely on large datasets to improve their VQA algorithms.", "startOffset": 37, "endOffset": 40}, {"referenceID": 1, "context": "fields as diverse as computer vision [3], computational linguistics [2], and machine learning [4] rely on large datasets to improve their VQA algorithms.", "startOffset": 68, "endOffset": 71}, {"referenceID": 3, "context": "fields as diverse as computer vision [3], computational linguistics [2], and machine learning [4] rely on large datasets to improve their VQA algorithms.", "startOffset": 94, "endOffset": 97}, {"referenceID": 2, "context": "Current methods to create these datasets assume a fixed number of human answers per visual question [3], [5], thereby either compromising on quality by not collecting all plausible answers or cost by collecting additional answers when they are redundant.", "startOffset": 100, "endOffset": 103}, {"referenceID": 4, "context": "Current methods to create these datasets assume a fixed number of human answers per visual question [3], [5], thereby either compromising on quality by not collecting all plausible answers or cost by collecting additional answers when they are redundant.", "startOffset": 105, "endOffset": 108}, {"referenceID": 1, "context": "Visual Question Answering Services: Researchers spanning communities as diverse as human computer interaction, machine learning, computational linguistics, and computer vision have proposed a variety of ways to answer questions about images [2], [3], [1], [4].", "startOffset": 241, "endOffset": 244}, {"referenceID": 2, "context": "Visual Question Answering Services: Researchers spanning communities as diverse as human computer interaction, machine learning, computational linguistics, and computer vision have proposed a variety of ways to answer questions about images [2], [3], [1], [4].", "startOffset": 246, "endOffset": 249}, {"referenceID": 0, "context": "Visual Question Answering Services: Researchers spanning communities as diverse as human computer interaction, machine learning, computational linguistics, and computer vision have proposed a variety of ways to answer questions about images [2], [3], [1], [4].", "startOffset": 251, "endOffset": 254}, {"referenceID": 3, "context": "Visual Question Answering Services: Researchers spanning communities as diverse as human computer interaction, machine learning, computational linguistics, and computer vision have proposed a variety of ways to answer questions about images [2], [3], [1], [4].", "startOffset": 256, "endOffset": 259}, {"referenceID": 0, "context": "For example, crowd-powered systems aim to supply a prespecified, fixed number of answers per visual question [1] and automated systems return a single answer for every visual", "startOffset": 109, "endOffset": 112}, {"referenceID": 1, "context": "question [2], [3], [4].", "startOffset": 9, "endOffset": 12}, {"referenceID": 2, "context": "question [2], [3], [4].", "startOffset": 14, "endOffset": 17}, {"referenceID": 3, "context": "question [2], [3], [4].", "startOffset": 19, "endOffset": 22}, {"referenceID": 2, "context": "advantage of our system over relying on the uncertainty of a VQA algorithm in its predicted answer [3].", "startOffset": 99, "endOffset": 102}, {"referenceID": 0, "context": "of workers [1], [6].", "startOffset": 11, "endOffset": 14}, {"referenceID": 5, "context": "of workers [1], [6].", "startOffset": 16, "endOffset": 19}, {"referenceID": 2, "context": "Other systems ensure a fixed number of answers are collected per visual question [3], [5].", "startOffset": 81, "endOffset": 84}, {"referenceID": 4, "context": "Other systems ensure a fixed number of answers are collected per visual question [3], [5].", "startOffset": 86, "endOffset": 89}, {"referenceID": 6, "context": "For example, researchers have suggested ways to resolve crowd disagreement due to task difficulty [7] and ambiguity/specificity [8], [9].", "startOffset": 98, "endOffset": 101}, {"referenceID": 7, "context": "For example, researchers have suggested ways to resolve crowd disagreement due to task difficulty [7] and ambiguity/specificity [8], [9].", "startOffset": 128, "endOffset": 131}, {"referenceID": 8, "context": "For example, researchers have suggested ways to resolve crowd disagreement due to task difficulty [7] and ambiguity/specificity [8], [9].", "startOffset": 133, "endOffset": 136}, {"referenceID": 9, "context": "Some methods demonstrate which workers to trust most when aggregating multiple responses into a final, single response [10], [7].", "startOffset": 119, "endOffset": 123}, {"referenceID": 6, "context": "Some methods demonstrate which workers to trust most when aggregating multiple responses into a final, single response [10], [7].", "startOffset": 125, "endOffset": 128}, {"referenceID": 8, "context": "Other methods leverage context to automatically disambiguate which of multiple outcomes is the desired outcome [9].", "startOffset": 111, "endOffset": 114}, {"referenceID": 10, "context": "For example, one method distributes a budget between three different levels of human effort when deciding how to segment images [11].", "startOffset": 128, "endOffset": 132}, {"referenceID": 11, "context": "Another method spends a budget between less costly crowd workers and more costly expert efforts to improve outcomes for biomedical citation screening [12].", "startOffset": 150, "endOffset": 154}, {"referenceID": 12, "context": "Another method predicts when to employ algorithms versus crowd workers to segment images [13].", "startOffset": 89, "endOffset": 93}, {"referenceID": 13, "context": "Minimizing Human Labeling: Our aim to actively decide how to allocate human effort to improve results is also somewhat related to active learning [14].", "startOffset": 146, "endOffset": 150}, {"referenceID": 14, "context": "Some methods iteratively supplement a training dataset with the most informative images for training a classifier [15], [16].", "startOffset": 114, "endOffset": 118}, {"referenceID": 15, "context": "Some methods iteratively supplement a training dataset with the most informative images for training a classifier [15], [16].", "startOffset": 120, "endOffset": 124}, {"referenceID": 16, "context": "Other methods solicit redundant labels to prevent incorrect/noisy labels from teaching prediction models to make mistakes [17], [18].", "startOffset": 122, "endOffset": 126}, {"referenceID": 17, "context": "Other methods solicit redundant labels to prevent incorrect/noisy labels from teaching prediction models to make mistakes [17], [18].", "startOffset": 128, "endOffset": 132}, {"referenceID": 18, "context": "Continuous Dialogue with the Crowd: Two services - Be My Eyes [19] and Chorus:View [20] - offer users a continuous", "startOffset": 83, "endOffset": 87}, {"referenceID": 18, "context": "Our work offers an alternative by demonstrating how a crowdsourcing service might instead solicit multiple answers for a one time back-and-forth rather than enacting a more costly, continuous communication channel with a single voice, whether from a single person [19] or the consensus of a crowd [20].", "startOffset": 297, "endOffset": 301}, {"referenceID": 2, "context": "VQA Datasets: We conduct our analysis on a total of 459,861 visual questions and 4,598,610 answers coming from today\u2019s largest freely-available VQA benchmark [3].", "startOffset": 158, "endOffset": 161}, {"referenceID": 19, "context": ", 369,861) of VQAs are about real images that show 91 types of objects that would be \u201ceasily recognizable by a 4 year old\u201d in their natural context [21].", "startOffset": 148, "endOffset": 152}, {"referenceID": 2, "context": "images [3].", "startOffset": 7, "endOffset": 10}, {"referenceID": 2, "context": "Towards this aim, visual questions were collected by asking three Amazon Mechanical Turk (AMT) crowd workers to look at a given image and generate a text-based question about it that would \u201cstump a smart robot\u201d [3].", "startOffset": 211, "endOffset": 214}, {"referenceID": 2, "context": "image with associated question and asking him/her to respond with \u201ca brief phrase and not a complete sentence\u201d [3].", "startOffset": 111, "endOffset": 114}, {"referenceID": 2, "context": ", \u201ca\u201d, \u201can\u201d, \u201cthe\u201d), as was done in prior work [3].", "startOffset": 47, "endOffset": 50}, {"referenceID": 20, "context": ", m = 1 person) [22] as well as more conservative answer validation schemes (i.", "startOffset": 16, "endOffset": 20}, {"referenceID": 2, "context": ", m = 3 people) [3].", "startOffset": 16, "endOffset": 19}, {"referenceID": 21, "context": "salient object subitizing [23] (SOS) method, which produces five probabilities that indicate whether an image contains 0, 1, 2, 3, or 4+ salient objects.", "startOffset": 26, "endOffset": 30}, {"referenceID": 22, "context": "We leverage a random forest classification model [24] to predict an answer (dis)agreement label for a given visual question.", "startOffset": 49, "endOffset": 53}, {"referenceID": 23, "context": "Deep Learning System: We next adapt a VQA deep learning architecture [25] to learn the predictive combination of visual and textual features.", "startOffset": 69, "endOffset": 73}, {"referenceID": 24, "context": "fully connected layer of the Convolutional Neural Network (CNN), VGG16 [26].", "startOffset": 71, "endOffset": 75}, {"referenceID": 2, "context": "We capitalize on today\u2019s largest visual question answering dataset [3] to evaluate our prediction system, which includes 369,861 visual questions about real images.", "startOffset": 67, "endOffset": 70}, {"referenceID": 23, "context": "Therefore, we employ as a baseline a related VQA algorithm [25], [3]", "startOffset": 59, "endOffset": 63}, {"referenceID": 2, "context": "Therefore, we employ as a baseline a related VQA algorithm [25], [3]", "startOffset": 65, "endOffset": 68}, {"referenceID": 2, "context": "Both our proposed classification systems outperform the VQA Algorithm [3] baseline; e.", "startOffset": 70, "endOffset": 73}, {"referenceID": 2, "context": "Our overall finding that most of the predictive power stems from language-based features parallels feature analysis findings in the automated VQA literature [3], [22].", "startOffset": 157, "endOffset": 160}, {"referenceID": 20, "context": "Our overall finding that most of the predictive power stems from language-based features parallels feature analysis findings in the automated VQA literature [3], [22].", "startOffset": 162, "endOffset": 166}, {"referenceID": 2, "context": "Today\u2019s status quo is to either uniformly collect N answers for every visual question [3] or collect multiple answers where the number is determined by external crowdsourcing conditions [1].", "startOffset": 86, "endOffset": 89}, {"referenceID": 0, "context": "Today\u2019s status quo is to either uniformly collect N answers for every visual question [3] or collect multiple answers where the number is determined by external crowdsourcing conditions [1].", "startOffset": 186, "endOffset": 189}, {"referenceID": 0, "context": "8) answers for each question\u201d [1].", "startOffset": 30, "endOffset": 33}, {"referenceID": 2, "context": "VQA Algorithm [3]: As in the previous section, we leverage the output", "startOffset": 14, "endOffset": 17}, {"referenceID": 23, "context": "confidence score from the publicly-shared model [25]", "startOffset": 48, "endOffset": 52}, {"referenceID": 0, "context": "This predictor illustrates the best a user can achieve today with crowd-powered systems [1], [6] or with current dataset collection methods [3], [5].", "startOffset": 88, "endOffset": 91}, {"referenceID": 5, "context": "This predictor illustrates the best a user can achieve today with crowd-powered systems [1], [6] or with current dataset collection methods [3], [5].", "startOffset": 93, "endOffset": 96}, {"referenceID": 2, "context": "This predictor illustrates the best a user can achieve today with crowd-powered systems [1], [6] or with current dataset collection methods [3], [5].", "startOffset": 140, "endOffset": 143}, {"referenceID": 4, "context": "This predictor illustrates the best a user can achieve today with crowd-powered systems [1], [6] or with current dataset collection methods [3], [5].", "startOffset": 145, "endOffset": 148}, {"referenceID": 2, "context": "Figure 6b also illustrates the advantage of our system over a related VQA algorithm [3] for our novel application of costsensitive answer collection from a crowd.", "startOffset": 84, "endOffset": 87}], "year": 2016, "abstractText": "Visual question answering (VQA) systems are emerging from a desire to empower users to ask any natural language question about visual content and receive a valid answer in response. However, close examination of the VQA problem reveals an unavoidable, entangled problem that multiple humans may or may not always agree on a single answer to a visual question. We train a model to automatically predict from a visual question whether a crowd would agree on a single answer. We then propose how to exploit this system in a novel application to efficiently allocate human effort to collect answers to visual questions. Specifically, we propose a crowdsourcing system that automatically solicits fewer human responses when answer agreement is expected and more human responses when answer disagreement is expected. Our system improves upon existing crowdsourcing systems, typically eliminating at least 20% of human effort with no loss to the information collected from the crowd.", "creator": "LaTeX with hyperref package"}}}