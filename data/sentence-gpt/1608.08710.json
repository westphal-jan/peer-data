{"id": "1608.08710", "review": {"conference": "iclr", "VERSION": "v1", "DATE_OF_SUBMISSION": "31-Aug-2016", "title": "Pruning Filters for Efficient ConvNets", "abstract": "Convolutional Neural Networks (CNNs) are extensively used in image and video recognition, natural language processing and other machine learning applications. The success of CNNs in these areas corresponds with a significant increase in the number of parameters and computation costs. Recent approaches towards reducing these overheads involve pruning and compressing the weights of various layers without hurting the overall CNN performance.\n\n\n\nThe overall impact of CNNs on performance is that it is one of the most cost-effective of the approaches. A reduction in the cost of the implementation of CNNs, with the resulting cost reduction, is significant and may even bring benefits to people with low or no learning level.\nThis has been discussed in depth in previous papers.\nCognitive Networks (CNNs) are the core of network learning applications. CNNs use functional programming techniques to simulate networks that are computationally efficient. These techniques use networks that do not rely on network inference to provide the basis for the optimal behavior of network network representations.\nThe CNNs can be trained using a number of different methods. The methods use networks that can perform different tasks with respect to the input and input values. As a result, network learning can improve performance and increase learning speed, decrease costs and improve efficiency.\nIn contrast, network learning can improve learning speed when used in the context of traditional neural networks. As a result, learning speed can increase faster, decrease costs, and decrease error rates. As a result, learning speed is more accurate and less efficient when using more powerful algorithms.\nLearning speed can increase performance and decrease error rates. As a result, learning speed can increase faster, decrease costs and decrease error rates.\nNetwork-Learning Techniques\nCNNs are a core of network learning applications. They are used by large organizations and organizations to help provide for the application of network and information processing. In addition, CNNs are used in high-performance data processing.\nA common CNNs are:\nCNNs that allow for many operations in a network.\nNetwork-learning techniques such as network learning, model learning, and clustering.\nNetwork-learning techniques such as network learning, model learning, and clustering.\nNetwork-learning techniques such as network learning, model learning, and clustering.\nNetwork-learning techniques such as network learning, model learning, and clustering. Network-learning techniques such as network learning, model learning, and clustering. Network-learning techniques such as network learning, model learning, and clust", "histories": [["v1", "Wed, 31 Aug 2016 02:29:59 GMT  (2623kb,D)", "http://arxiv.org/abs/1608.08710v1", null], ["v2", "Thu, 15 Sep 2016 02:12:36 GMT  (2624kb,D)", "http://arxiv.org/abs/1608.08710v2", null], ["v3", "Fri, 10 Mar 2017 17:57:56 GMT  (7203kb,D)", "http://arxiv.org/abs/1608.08710v3", "Published as a conference paper at ICLR 2017"]], "reviews": [], "SUBJECTS": "cs.CV cs.LG", "authors": ["hao li", "asim kadav", "igor durdanovic", "hanan samet", "hans peter graf"], "accepted": true, "id": "1608.08710"}, "pdf": {"name": "1608.08710.pdf", "metadata": {"source": "CRF", "title": "Pruning Filters for Efficient ConvNets", "authors": ["Hao Li", "Asim Kadav", "Igor Durdanovic"], "emails": ["haoli@cs.umd.edu", "asim@nec-labs.com", "igord@nec-labs.com", "hjs@cs.umd.edu", "hpg@nec-labs.com"], "sections": [{"heading": "1 Introduction", "text": "The ImageNet challenge has led to significant advancements in exploring various architectural choices in CNNs [1, 2]. The general trend since the past few years has been that the networks have grown deeper, with an overall increase in the number of parameters and convolution operations. These high capacity networks have significant inference costs especially when used with embedded sensors or mobile devices where computational and power resources can be limited. For these applications, in addition to accuracy, computational efficiency and small network sizes are crucial enabling factors [3]. In addition, for web services like image search and image classification APIs that operate on a time budget, often serving hundreds of thousands of images per second, benefit significantly from lower inference times.\nRecent work in improving the efficiency of large and deep CNNs includes pruning parameters across these deep networks [4]. This creates a sparsely populated network of parameters across all layers of CNNs. This approach suffers from three problems: First, just pruning parameters does not significantly reduce the computation since majority of the parameters removed are from the fully connected layers where the computation required is low. For example, when training the ImageNet\n\u2217Work done at NEC Labs\nar X\niv :1\n60 8.\n08 71\n0v 1\n[ cs\n.C V\n] 3\n1 A\ndataset over VGG-16, the convolution operations contribute about 90.9% of the overall compute floating point operations (FLOP). Hence, pruning parameters from the fully connected layers may not significantly reduce the overall computation required during inference. Second, pruning parameters creates a sparse networks and modern libraries that provide speedup using sparse operations over CNNs are limited [5]. Third, maintaining sparse data structures creates an additional storage overhead which can be significant for low-precision weights.\nTo address these challenges, we propose to prune the convolutional filters or feature maps from the trained CNNs instead of removing its weights across the network. We then restore the accuracy of the pruned network by retraining. This reduces the computational overheads from convolution operations and does not involve using any sparse libraries or any specialized hardware [6]. In our results, we find that our technique can reduce up to 34-38% of model inference costs without affecting the original accuracy of the trained model."}, {"heading": "2 Related Work", "text": "There has been a significant amount of work on reducing the storage and computational cost by model compression [7, 8, 4, 9]. Optimal Brain Damage [7] uses a theoretically justified saliency measure to prune weights and Optimal Brain Surgeon [8] removes unimportant weights determined by the second-order derivative information. Recently, Han et al. [4, 10] report 90% compression rates on AlexNet [2] and VGGNet [11] by pruning weights with small magnitudes, followed by retraining without hurting the accuracy. However, for these networks the fully connected layers occupies more than 85% of all parameters, which contributes towards most of the pruned weights. They also demonstrate that the convolutional layers can be compressed and accelerated [12], but with the additionally required sparse BLAS libraries and even additional hardware [6]. Mariet et al. [9] reduce the network redundancy by identifying a subset of diverse neurons that does not require retraining. However, these methods only operate on the fully-connected layers.\nRecent winners of the ImageNet competition have consistently used very deep CNNs [5, 3, 13, 14]. Here, the computation cost is reduced by downsampling the image at an early stage to reduce the size of feature maps for the following convolutional layers. The fully connected layers have also been replaced with average pooling layers [15], which reduces the number of parameters significantly. Nevertheless, as the networks continue to become deeper, the computational costs of convolutional layers continue to dominate [14]. Recently Huang et al. [16] reduce the training times for deep ResNets by randomly skipping some layers. This method does not modify the original network and does not reduce the inference time.\nFilter pruning has been previously explored in [17, 18]. Anwar et al. [17] introduce a three-level pruning of the weights and locate the pruning candidates using particle filtering techniques. Polyak et al. [18] detect the less activated feature maps with sample input data for face detection applications. We instead use a simple magnitude based strategy to prune filters, and introduce network wide holistic approaches to prune filters for simple as well as more complex convolutional network architectures.\nOther approaches to reduce the convolutional overheads including using FFT based convolutions that compute the convolutions in Fourier domain with low overheads [19]. These costs can be further reduced by using minimal filtering algorithms [20]. Additionally, quantization can be used to reduce the model parameter sizes and lower the compute overheads. Our methods can be used in addition to existing techniques to reduce computation costs without incurring additional overheads."}, {"heading": "3 Pruning Filters and Feature Maps", "text": "Let ni denote the number of input channels for the ith convolutional layer and hi/wi be the height/width of the input feature map. The convolutional layer transforms the 3D input feature maps xi \u2208 Rni\u00d7hi\u00d7wi into the 3D output feature maps xi+1 \u2208 Rni+1\u00d7hi+1\u00d7wi+1 , which is used as input feature maps for the next convolutional layer. This is achieved by applying ni+1 filters Fi,j \u2208 Rni\u00d7k\u00d7k on the ni input channels, in which one filter generates one feature map. Each filter is composed by ni two-dimensional kernels K \u2208 Rk\u00d7k (e.g., 3\u00d7 3). All the filters, together, constitute the kernel matrix Fi \u2208 Rni\u00d7ni+1\u00d7k\u00d7k. The operations of the convolutional layer is ni+1nik 2hi+1wi+1.\nAs shown in Figure 1, when a filter Fi,j is pruned, its corresponding feature map xi+1,j is removed, which reduces nik2hi+1wi+1 operations. The kernels that apply on the removed feature maps from the filters of the next convolutional layer are also removed, which saves an additional ni+2k2hi+2wi+2 operations. For example, removing 50% filters of layer i leads to 50% of the computation cost reduction for both layer i and i+ 1."}, {"heading": "3.1 Determining which filters to prune", "text": "Our filter pruning method drops the less useful filters from a network for computational efficiency while maintaining the accuracy. To do so, we measure the importance of a filter in each layer by calculating its absolute weight sum \u2211 |Fi,j |. Since the number of input channels, ni, is the same\nacross filters, \u2211 |Fi,j | also represents the average magnitude of its kernel weights. This value gives an expectation of the magnitude of the output feature map. Filters with smaller kernel weights tends to produce feature maps with weak activations as compared to the other filters in that layer.\nFigure 2(a) illustrates the distribution of this filter weight sum for each layer in a VGG-16 network trained using the CIFAR-10 dataset. The filters in a layer are ordered by its sum of kernel weights, which is divided by the maximum value max(si).\nThe procedure of pruning m filters from the ith convolutional layer is as follows:\n1. For each filter Fi,j , calculate the absolute sum of its kernel weights sj = \u2211ni\nl=1\n\u2211 |Kl|.\n2. Sort the filters by sj .\n3. Prune m filters with smallest sum values and their corresponding feature maps. The kernels in the next convolutional layer corresponding to the pruned feature maps are also removed.\n4. A new kernel matrix is created for both ith and i+ 1th layer, and the remaining kernels are copied to the new one.\nRelationship to pruning weights: Pruning filters with low absolute weight sum values is similar to pruning low magnitude weights [4]. Pruning weights may also prune away filters when all the kernel weights of a filter are lower than a threshold. This requires a careful tuning of the threshold and makes it difficult to predict the exact number of filters that will be eventually pruned. Furthermore, this technique generates sparse convolutional kernels which can be hard to accelerate given the lack of efficient sparse libraries."}, {"heading": "3.2 Determining single layer\u2019s sensitivity to pruning", "text": "To understand the sensitivity of each layer, we prune each layer independently and and test the resulting pruned network\u2019s accuracy on the validation set. Figure 2(b) shows the layers that maintain their accuracy as the filters are pruned away and they correspond to filters with larger slopes in Figure 2(a). On the contrary, layers with relatively flat slopes (as in Figure 2(a)) are more sensitive to pruning. We empirically determine the number of filters to prune for each layer based on their sensitivity to pruning. For layers that are sensitive to pruning, we prune a smaller percentage of these layers or completely skip pruning them."}, {"heading": "3.3 Pruning filters across multiple layers", "text": "We now discuss how to prune filters across the network. Previous work prunes the weights on a layer by layer basis, followed by iteratively retraining and compensating for any loss of accuracy [4]. However, understanding how to prune multiple filters at once can be useful: 1) For deep networks, pruning and retraining on a layer by layer basis can be extremely time-consuming 2) Pruning layers across the network gives a holistic view of the robustness of the network resulting in a smaller network 3) For complex networks, a holistic approach may be necessary. For example, for the ResNet, pruning the identity feature maps or the second layer of each residual block results in additional pruning of other layers.\nTo prune filters across multiple layers, we use the following two criterion for filter selection:\n\u2022 Independent pruning determines which filters should be pruned at each layer independent of other layers.\n\u2022 Greedy pruning accounts for the filters that have been already removed in the previous layers. This strategy does not consider the kernels from the previously pruned feature maps.\nFigure 3 illustrates the difference between two approaches in weight sum calculation. The greedy approach, though not globally optimal, is more holistic and results in pruned networks with higher accuracy especially when large number of filters are used.\nFor simpler CNNs like VGGNet or AlexNet, we can easily prune any of the filters in any convolutional layer. However, for more complex network architectures such as Residual networks [14], pruning filters may not be straight forward. The architecture of ResNet imposes several restrictions and\nthe filters need to pruned carefully. We show the filter pruning for residual blocks with projection mapping in Figure 4. Here, the filters of the first layer in the residual block can be arbitrarily pruned, as it does not change the number of output feature maps of the block. However, starting from the second convolutional layer onwards, the output feature maps correspond to identity feature maps making it difficult to prune them. Hence, to prune the second convolutional layer of the residual block, the corresponding projected feature maps must also be pruned. Since the identical feature maps are more important than the added residual maps, the feature map to be pruned should be determined by the pruning results of the shortcut layer. To determine which identity feature maps are to be pruned, we use the same selection criterion based on the filters of the shortcut convolutional layers (with 1 \u00d7 1 kernels). The second layer of the residual block is pruned with the same filter index as selected by the pruning of shortcut layer."}, {"heading": "3.4 Retraining pruned networks to regain accuracy", "text": "After pruning the filters, any performance degradation should be compensated by retraining the network. There are two strategies to prune the filters across multiple layers:\n1. Prune once and retrain: Prune filters of multiple layers at once and retrain them until the original accuracy is restored. 2. Prune and retrain iteratively: Prune filters layer by layer or filter by filter and then retrain iteratively. The model is retrained before pruning the next layer for the weights to adapt to the changes from the pruning process.\nWe find that for the layers that are resilient to pruning, the pruning and re-train once strategy can be used to prune away significant portions of the network and any loss in accuracy can be regained by retraining for a short period of time (less than the original training times). However, when some sensitive layers are pruned away or a very large portions of the networks are pruned away, it may not be possible to recover the original accuracy. Iterative pruning and retraining may yield better results, but the iterative process requires many more epochs especially for very deep networks."}, {"heading": "4 Experiments", "text": "We prune two types of networks: simple CNNs (VGG-16 on CIFAR-10) and Residual networks (ResNet-56/110 on CIFAR-10 and ResNet-34 on ImageNet). Unlike AlexNet that is often used to demonstrate efficiency speedups, both VGG and Residual networks have fewer parameters in the fully connected layers. Hence, pruning a large percentage of parameters from these networks is challenging. We implement our filter pruning methods in Torch. When filters are pruned, a new model with fewer filters is created and the remaining parameters of the modified layers as well as unaffected layers are copied into a new model. Furthermore, if a convolutional layer is pruned, the weights of the subsequent batch normalization layer are also removed. To get the baseline accuracies for each network, we train each model from scratch and follow the same pre-processing and hyper-parameters as ResNet [14]. For retraining, we use a constant learning rate 0.001 and retrain 40 epochs for CIFAR-10 and 20 epochs for ImageNet, which represents one-fourth of the original training epochs. Past work has reported up to 3X original training times to retrain pruned networks [4]."}, {"heading": "4.1 VGG-16 on CIFAR-10", "text": "VGG-16 [11] is a large capacity network originally designed for the Imagenet dataset. However, recent work [21] has found that a slightly modified version produces state of the art results on the CIFAR-10 dataset. VGG-16 on CIFAR-10 consists of 13 convolutional layers and 2 fully connected layers. We use the model described in [21] but use the Batch Normalization [22] after each convolutional layer and the first linear layer, instead of the Dropout layer [23]. The detailed configuration of the model is shown in Table 2. For VGG-16 on CIFAR-10, unlike ImageNet, the fully connected layers do not occupy large portions of parameters due to the small size of the input channels. If the last convolutional layer is pruned, the input to the linear layer is changed and its connections are also removed.\nAs seen in Figure 2, the deeper convolutional layers with 512 feature maps are redundant. These layers can be pruned at least 60%, and do not require any retraining to maintain the original accuracy. Figure 5 shows that with retraining, almost 100% of these layers can be removed without affecting the accuracy. One possible explanation can be that these filters operate on 4 \u00d7 4 or 2 \u00d7 2 feature maps, which have no meaningful spatial connections in such small dimensions. Even ResNets, over CIFAR-10 do not perform any additional convolutions for smaller feature maps below 8 \u00d7 8 dimensions. Unlike previous work [24, 4], we find that the first layer is quite robust to pruning as\ncompared to the next few layers. This is possible because perhaps even when 80% of the filters of the first layer are pruned, there are still 12 filters remaining. This number is larger than the number of raw input channels. However, when removing 80% filters of the second layer, the layer input corresponds to a 64 to 12 mapping, which may lose significant information from previous layers hurting the accuracy. With about 50% of the filters being pruned in layer 1 and from 8 to 13, we achieve 34% FLOP reduction for the same original accuracy."}, {"heading": "4.2 ResNet-56/110 on CIFAR-10", "text": "ResNets for CIFAR-10 have three stages of residual blocks for feature maps with the sizes of 32\u00d7 32, 16 \u00d7 16 and 8 \u00d7 8. Each stage has the same number of residual blocks. When the feature maps increase, the shortcut layer provides an identity mapping with an additional zero padding for the increased dimensions. As there is no projection mapping for choosing the identity feature maps, we only consider pruning the first layer of the residual block. As shown in Figure 6, most of the layers are robust to pruning. For ResNet-110, pruning some single layers without retraining even improves the performance. In addition, layers that are sensitive to pruning (layer 20, 38 and 54 for ResNet-56, layer 36, 38 and 74 for ResNet-110) lie at the residual blocks close to the layers where the number of feature maps change, e.g., the first and the last residual blocks for each stage. We believe this happens because the precise residual errors are necessary for the new added empty feature maps.\nThe retraining performance can be improved by skipping these sensitive layers. As shown in Table 1, ResNet-56-pruned-A improves the performance by pruning 10% filters while skipping the sensitive layers 16, 20, 38 and 54. In addition, we find that the deeper layers are more sensitive to pruning than ones in the earlier stages of the network. Hence, we use a different pruning rate for each stage. We use pi to denote the pruning rate for layers in the ith stage. ResNet-56-pruned-B skips more layers (16, 18, 20, 34, 38, 54) and prunes layers with p1=60%, p2=30% and p3=10%. For ResNet-110, the first pruned model gets a slightly better result with p1=50 and layer 36 skipped. ResNet-110-pruned-B skips layer 36, 38, 74 and prunes with p1=50%, p2=40% and p3=30. When there are more than two residual blocks at each stage, the middle residual blocks may be redundant and be easily pruned. This might explain why ResNet-110 is easier to prune than ResNet-56."}, {"heading": "4.3 ResNet-34 on ImageNet", "text": "ResNets for ImageNet have four stages of residual blocks for feature maps with sizes of 56 \u00d7 56, 28\u00d7 28, 14\u00d7 14 and 7\u00d7 7. ResNet-34 on ImageNet uses the projection shortcut when the feature maps are down-sampled. We first prune the first layer of each residual block. Figure 7 shows the sensitivity of the first layer of each residual block. Similar to ResNet-56/110, the first and the last residual blocks of each stage are more sensitive to pruning than the intermediate blocks (i.e., layer 2, 8, 14, 16, 26, 28, 30, 32). We skip those layers and prune the remaining layers at each stage equally. In Table 1 we compare two configurations of pruning percentages for the first three stages: (A) p1=30%, p2=30%, p3=30%; (B) p1=50%, p2=60%, p3=40%. Option-B provides 24% FLOP reduction with about 1% loss in accuracy. As seen in ResNet-50/110 results with CIFAR-10, we can predict that ResNet-34 is relatively more difficult to prune as compared to the deeper ResNet-50/110.\nWe also prune the identity feature maps and the second convolutional layer of the residual blocks. As these layers have the same number of filters, they are pruned equally. As can be seen in Figure 7(b), these layers are more sensitive to pruning than the first layers. With retraining, ResNet-34-pruned-C prunes the third stage with p3=20% and can only reduce 7.5% FLOP with 0.75% loss in accuracy. Therefore, pruning the first layer of the residual block is more effective at reducing the overall FLOP. This finding also correlates with the bottleneck block design for deeper ResNets, which first reduce the dimension of input feature maps for the residual layer and then increase the dimension to match the identity mapping."}, {"heading": "5 Discussion and Conclusions", "text": "In the future, instead of statically pruning the network at once and then retraining, we plan to explore dynamic pruning and retraining. Here, instead of statically detecting each layer\u2019s sensitivity and then pruning, one needs to gently drive down the less useful filter weights to zero.\nTo conclude, current CNNs are deep and often over-capacity with large training and inference costs. In this paper, we prune feature maps and corresponding filters with relatively low weight magnitudes from the pre-trained networks and retrain them. The generated CNNs are computationally efficient networks with reduced computation costs and do not require the use of sparse computation libraries.\nWe find that our techniques achieve about 10% to 30% reduction in FLOP for VGGNet and the deep ResNet without any significant loss in the original accuracy."}], "references": [{"title": "Imagenet Classification with Deep Convolutional Neural Networks", "author": ["A. Krizhevsky", "I. Sutskever", "G.E. Hinton"], "venue": null, "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2012}, {"title": "Rethinking the Inception Architecture for Computer Vision", "author": ["C. Szegedy", "V. Vanhoucke", "S. Ioffe", "J. Shlens", "Z. Wojna"], "venue": "arXiv preprint arXiv:1512.00567", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2015}, {"title": "Learning both Weights and Connections for Efficient Neural Network", "author": ["S. Han", "J. Pool", "J. Tran", "W. Dally"], "venue": null, "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2015}, {"title": "Going Deeper with Convolutions", "author": ["C. Szegedy", "W. Liu", "Y. Jia", "P. Sermanet", "S. Reed", "D. Anguelov", "D. Erhan", "V. Vanhoucke", "A. Rabinovich"], "venue": null, "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2015}, {"title": "EIE: Efficient Inference Engine on Compressed Deep Neural Network", "author": ["S. Han", "X. Liu", "H. Mao", "J. Pu", "A. Pedram", "M.A. Horowitz", "W.J. Dally"], "venue": null, "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2016}, {"title": "Optimal brain damage", "author": ["Y. Le Cun", "J.S. Denker", "S.A. Solla"], "venue": "In: NIPS", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 1989}, {"title": "Second Order Derivatives for Network Pruning: Optimal Brain Surgeon", "author": ["B. Hassibi", "D.G. Stork"], "venue": null, "citeRegEx": "8", "shortCiteRegEx": "8", "year": 1993}, {"title": "Diversity Networks. In: ICLR", "author": ["Z. Mariet", "S. Sra"], "venue": null, "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2016}, {"title": "A Deep Neural Network Compression Pipeline: Pruning, Quantization, Huffman Encoding", "author": ["S. Han", "H. Mao", "W.J. Dally"], "venue": null, "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2016}, {"title": "Very Deep Convolutional Networks for Large-Scale Image Recognition", "author": ["K. Simonyan", "A. Zisserman"], "venue": "ICLR", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2015}, {"title": "SqueezeNet: AlexNet-level accuracy with 50x fewer parameters and < 1MB model size", "author": ["F. Iandola", "M. Moskewicz", "K. Ashraf", "S. Han", "W. Dally", "K. Kurt"], "venue": null, "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2016}, {"title": "Convolutional Neural Networks at Constrained Time Cost", "author": ["K. He", "J. Sun"], "venue": null, "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2015}, {"title": "Deep Residual Learning for Image Recognition", "author": ["K. He", "X. Zhang", "S. Ren", "J. Sun"], "venue": "arXiv preprint arXiv:1512.03385", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2015}, {"title": "Network in Network", "author": ["M. Lin", "Q. Chen", "S. Yan"], "venue": "arXiv preprint arXiv:1312.4400", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2013}, {"title": "Deep Networks with Stochastic Depth", "author": ["G. Huang", "Y. Sun", "Z. Liu", "D. Sedra", "K. Weinberger"], "venue": "arXiv preprint", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2016}, {"title": "Structured Pruning of Deep Convolutional Neural Networks", "author": ["S. Anwar", "K. Hwang", "W. Sung"], "venue": "arXiv preprint arXiv:1512.08571", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2015}, {"title": "Channel-Level Acceleration of Deep Face Representations", "author": ["A. Polyak", "L. Wolf"], "venue": "IEEE Access", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2015}, {"title": "Fast Training of Convolutional Networks through FFTs", "author": ["M. Mathieu", "M. Henaff", "Y. LeCun"], "venue": "arXiv preprint arXiv:1312.5851", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2013}, {"title": "Fast Algorithms for Convolutional Neural Networks", "author": ["A. Lavin"], "venue": "arXiv preprint arXiv:1509.09308", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2015}, {"title": "Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift", "author": ["S. Ioffe", "C. Szegedy"], "venue": "arXiv preprint arXiv:1502.03167", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2015}, {"title": "Dropout: A Simple Way to Prevent Neural Networks from Overfitting", "author": ["N. Srivastava", "G. Hinton", "A. Krizhevsky", "I. Sutskever", "R. Salakhutdinov"], "venue": null, "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2014}, {"title": "Visualizing and Understanding Convolutional Networks", "author": ["M.D. Zeiler", "R. Fergus"], "venue": "ECCV", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2014}], "referenceMentions": [{"referenceID": 0, "context": "The ImageNet challenge has led to significant advancements in exploring various architectural choices in CNNs [1, 2].", "startOffset": 110, "endOffset": 116}, {"referenceID": 1, "context": "For these applications, in addition to accuracy, computational efficiency and small network sizes are crucial enabling factors [3].", "startOffset": 127, "endOffset": 130}, {"referenceID": 2, "context": "Recent work in improving the efficiency of large and deep CNNs includes pruning parameters across these deep networks [4].", "startOffset": 118, "endOffset": 121}, {"referenceID": 3, "context": "Second, pruning parameters creates a sparse networks and modern libraries that provide speedup using sparse operations over CNNs are limited [5].", "startOffset": 141, "endOffset": 144}, {"referenceID": 4, "context": "This reduces the computational overheads from convolution operations and does not involve using any sparse libraries or any specialized hardware [6].", "startOffset": 145, "endOffset": 148}, {"referenceID": 5, "context": "There has been a significant amount of work on reducing the storage and computational cost by model compression [7, 8, 4, 9].", "startOffset": 112, "endOffset": 124}, {"referenceID": 6, "context": "There has been a significant amount of work on reducing the storage and computational cost by model compression [7, 8, 4, 9].", "startOffset": 112, "endOffset": 124}, {"referenceID": 2, "context": "There has been a significant amount of work on reducing the storage and computational cost by model compression [7, 8, 4, 9].", "startOffset": 112, "endOffset": 124}, {"referenceID": 7, "context": "There has been a significant amount of work on reducing the storage and computational cost by model compression [7, 8, 4, 9].", "startOffset": 112, "endOffset": 124}, {"referenceID": 5, "context": "Optimal Brain Damage [7] uses a theoretically justified saliency measure to prune weights and Optimal Brain Surgeon [8] removes unimportant weights determined by the second-order derivative information.", "startOffset": 21, "endOffset": 24}, {"referenceID": 6, "context": "Optimal Brain Damage [7] uses a theoretically justified saliency measure to prune weights and Optimal Brain Surgeon [8] removes unimportant weights determined by the second-order derivative information.", "startOffset": 116, "endOffset": 119}, {"referenceID": 2, "context": "[4, 10] report 90% compression rates on AlexNet [2] and VGGNet [11] by pruning weights with small magnitudes, followed by retraining without hurting the accuracy.", "startOffset": 0, "endOffset": 7}, {"referenceID": 8, "context": "[4, 10] report 90% compression rates on AlexNet [2] and VGGNet [11] by pruning weights with small magnitudes, followed by retraining without hurting the accuracy.", "startOffset": 0, "endOffset": 7}, {"referenceID": 0, "context": "[4, 10] report 90% compression rates on AlexNet [2] and VGGNet [11] by pruning weights with small magnitudes, followed by retraining without hurting the accuracy.", "startOffset": 48, "endOffset": 51}, {"referenceID": 9, "context": "[4, 10] report 90% compression rates on AlexNet [2] and VGGNet [11] by pruning weights with small magnitudes, followed by retraining without hurting the accuracy.", "startOffset": 63, "endOffset": 67}, {"referenceID": 10, "context": "They also demonstrate that the convolutional layers can be compressed and accelerated [12], but with the additionally required sparse BLAS libraries and even additional hardware [6].", "startOffset": 86, "endOffset": 90}, {"referenceID": 4, "context": "They also demonstrate that the convolutional layers can be compressed and accelerated [12], but with the additionally required sparse BLAS libraries and even additional hardware [6].", "startOffset": 178, "endOffset": 181}, {"referenceID": 7, "context": "[9] reduce the network redundancy by identifying a subset of diverse neurons that does not require retraining.", "startOffset": 0, "endOffset": 3}, {"referenceID": 3, "context": "Recent winners of the ImageNet competition have consistently used very deep CNNs [5, 3, 13, 14].", "startOffset": 81, "endOffset": 95}, {"referenceID": 1, "context": "Recent winners of the ImageNet competition have consistently used very deep CNNs [5, 3, 13, 14].", "startOffset": 81, "endOffset": 95}, {"referenceID": 11, "context": "Recent winners of the ImageNet competition have consistently used very deep CNNs [5, 3, 13, 14].", "startOffset": 81, "endOffset": 95}, {"referenceID": 12, "context": "Recent winners of the ImageNet competition have consistently used very deep CNNs [5, 3, 13, 14].", "startOffset": 81, "endOffset": 95}, {"referenceID": 13, "context": "The fully connected layers have also been replaced with average pooling layers [15], which reduces the number of parameters significantly.", "startOffset": 79, "endOffset": 83}, {"referenceID": 12, "context": "Nevertheless, as the networks continue to become deeper, the computational costs of convolutional layers continue to dominate [14].", "startOffset": 126, "endOffset": 130}, {"referenceID": 14, "context": "[16] reduce the training times for deep ResNets by randomly skipping some layers.", "startOffset": 0, "endOffset": 4}, {"referenceID": 15, "context": "Filter pruning has been previously explored in [17, 18].", "startOffset": 47, "endOffset": 55}, {"referenceID": 16, "context": "Filter pruning has been previously explored in [17, 18].", "startOffset": 47, "endOffset": 55}, {"referenceID": 15, "context": "[17] introduce a three-level pruning of the weights and locate the pruning candidates using particle filtering techniques.", "startOffset": 0, "endOffset": 4}, {"referenceID": 16, "context": "[18] detect the less activated feature maps with sample input data for face detection applications.", "startOffset": 0, "endOffset": 4}, {"referenceID": 17, "context": "Other approaches to reduce the convolutional overheads including using FFT based convolutions that compute the convolutions in Fourier domain with low overheads [19].", "startOffset": 161, "endOffset": 165}, {"referenceID": 18, "context": "These costs can be further reduced by using minimal filtering algorithms [20].", "startOffset": 73, "endOffset": 77}, {"referenceID": 2, "context": "Relationship to pruning weights: Pruning filters with low absolute weight sum values is similar to pruning low magnitude weights [4].", "startOffset": 129, "endOffset": 132}, {"referenceID": 2, "context": "Previous work prunes the weights on a layer by layer basis, followed by iteratively retraining and compensating for any loss of accuracy [4].", "startOffset": 137, "endOffset": 140}, {"referenceID": 12, "context": "However, for more complex network architectures such as Residual networks [14], pruning filters may not be straight forward.", "startOffset": 74, "endOffset": 78}, {"referenceID": 12, "context": "To get the baseline accuracies for each network, we train each model from scratch and follow the same pre-processing and hyper-parameters as ResNet [14].", "startOffset": 148, "endOffset": 152}, {"referenceID": 2, "context": "Past work has reported up to 3X original training times to retrain pruned networks [4].", "startOffset": 83, "endOffset": 86}, {"referenceID": 9, "context": "VGG-16 [11] is a large capacity network originally designed for the Imagenet dataset.", "startOffset": 7, "endOffset": 11}, {"referenceID": 19, "context": "We use the model described in [21] but use the Batch Normalization [22] after each convolutional layer and the first linear layer, instead of the Dropout layer [23].", "startOffset": 67, "endOffset": 71}, {"referenceID": 20, "context": "We use the model described in [21] but use the Batch Normalization [22] after each convolutional layer and the first linear layer, instead of the Dropout layer [23].", "startOffset": 160, "endOffset": 164}, {"referenceID": 21, "context": "Unlike previous work [24, 4], we find that the first layer is quite robust to pruning as", "startOffset": 21, "endOffset": 28}, {"referenceID": 2, "context": "Unlike previous work [24, 4], we find that the first layer is quite robust to pruning as", "startOffset": 21, "endOffset": 28}], "year": 2016, "abstractText": "Convolutional Neural Networks (CNNs) are extensively used in image and video recognition, natural language processing and other machine learning applications. The success of CNNs in these areas corresponds with a significant increase in the number of parameters and computation costs. Recent approaches towards reducing these overheads involve pruning and compressing the weights of various layers without hurting the overall CNN performance. However, using model compression to generate sparse CNNs mostly reduces parameters from the fully connected layers and may not significantly reduce the final computation costs. In this paper, we present a compression technique for CNNs, where we prune the filters from CNNs that are identified as having a small effect on the output accuracy. By removing whole planes in the network, together with their connecting convolution kernels, the computational costs are reduced significantly. In contrast to other techniques proposed for pruning networks, this approach does not result in sparse connectivity patterns. Hence, our techniques do not need the support of sparse convolution libraries and can work with the most efficient BLAS operations for matrix multiplications. In our results, we show that even simple filter pruning techniques can reduce inference costs for VGG-16 by up to 34% and ResNet-110 by upto 38% while regaining close to the original accuracy by retraining the networks.", "creator": "LaTeX with hyperref package"}}}