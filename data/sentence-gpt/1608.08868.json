{"id": "1608.08868", "review": {"conference": "EMNLP", "VERSION": "v1", "DATE_OF_SUBMISSION": "31-Aug-2016", "title": "Demographic Dialectal Variation in Social Media: A Case Study of African-American English", "abstract": "Though dialectal language is increasingly abundant on social media, few resources exist for developing NLP tools to handle such language. We conduct a case study of dialectal language in online conversational text by investigating African-American English (AAE) on Twitter. We propose a distantly supervised model to identify AAE-like language from demographics associated with geo-located messages, and we verify that this language follows well-known AAE linguistic phenomena. We use the online English lexicon (BAS) and AAC-like language to identify AAE-like language using an analytical approach in which BAS is associated with BAS. AAE vocabulary is represented in the BAS-language with which AAE-like language is associated with AAE-like language and AAC-like language is associated with AAE-like language. These words (i.e., BAS and AAC-like) are considered AAE language of linguistic importance, with AAE and AAC-like language appearing to be associated with AAE. An AAE language of low frequency (BAS) that is associated with AAE-like language is associated with AAE-like language and AAC-like language is associated with AAE-like language. Using a multi-language approach to identify AAE language of a higher frequency (BAS) and AAC-like language is associated with AAE-like language is associated with AAE-like language. We present this case study in a series of online conversational text by using BAS. Our method is similar to our BAS-like language. To learn about the AAE-like language of the BAS-like language of AAE, we perform three key tests: first, to identify a BAS-like language from a high frequency (BAS) network with which the AAE network may be located, and second, to identify a BAS-like language with which BAS network may be located. First, we use a network with which AAE-like language may be located, and second, to identify a BAS-like language with which BAS network may be located. Second, we use the network with which BAS network may be located. Third, we use a network with which AAE-like language may be located. The network is identified and identified through its proximity to a human-initiated AAE network, and it can contain AAE-like language.\n\n\n\n\n\n\n\n\n\n\n\n\n", "histories": [["v1", "Wed, 31 Aug 2016 14:12:01 GMT  (80kb,D)", "http://arxiv.org/abs/1608.08868v1", "To be published in EMNLP 2016, 15 pages"]], "COMMENTS": "To be published in EMNLP 2016, 15 pages", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["su lin blodgett", "lisa green", "brendan t o'connor"], "accepted": true, "id": "1608.08868"}, "pdf": {"name": "1608.08868.pdf", "metadata": {"source": "CRF", "title": "Demographic Dialectal Variation in Social Media: A Case Study of African-American English", "authors": ["Su Lin Blodgett", "Lisa Green", "Brendan O\u2019Connor"], "emails": [], "sections": [{"heading": null, "text": "Data and software resources are available at: http://slanglab.cs.umass.edu/TwitterAAE\n(This is an expanded version of our EMNLP 2016 paper, including the appendix at end.)"}, {"heading": "1 Introduction", "text": "Owing to variation within a standard language, regional and social dialects exist within languages across the world. These varieties or dialects differ from the standard variety in syntax (sentence structure), phonology (sound structure), and the inventory of words and phrases (lexicon). Dialect communities often align with geographic and sociological factors, as language variation emerges within\ndistinct social networks, or is affirmed as a marker of social identity.\nAs many of these dialects have traditionally existed primarily in oral contexts, they have historically been underrepresented in written sources. Consequently, NLP tools have been developed from text which aligns with mainstream languages. With the rise of social media, however, dialectal language is playing an increasingly prominent role in online conversational text, for which traditional NLP tools may be insufficient. This impacts many applications: for example, dialect speakers\u2019 opinions may be mischaracterized under social media sentiment analysis or omitted altogether (Hovy and Spruit, 2016). Since this data is now available, we seek to analyze current NLP challenges and extract dialectal language from online data.\nSpecifically, we investigate dialectal language in publicly available Twitter data, focusing on AfricanAmerican English (AAE), a dialect of Standard American English (SAE) spoken by millions of people across the United States. AAE is a linguistic variety with defined syntactic-semantic, phonological, and lexical features, which have been the subject of a rich body of sociolinguistic literature. In addition to the linguistic characterization, reference to its speakers and their geographical location or speech communities is important, especially in light of the historical development of the dialect. Not all African-Americans speak AAE, and not all speakers of AAE are African-American; nevertheless, speakers of this variety have close ties with specific communities of African-Americans (Green, 2002). Due to its widespread use, established history in the sociolinguistic literature, and demographic associations, ar X iv :1\n60 8.\n08 86\n8v 1\n[ cs\n.C L\n] 3\n1 A\nug 2\nAAE provides an ideal starting point for the development of a statistical model that uncovers dialectal language. In fact, its presence in social media is attracting increasing interest for natural language processing (J\u00f8rgensen et al., 2016) and sociolinguistic (Stewart, 2014; Eisenstein, 2015; Jones, 2015) research.1 In this work we:\n\u2022 Develop a method to identify demographically-aligned text and language from geo-located messages (\u00a72), based on distant supervision of geographic census demographics through a statistical model that assumes a soft correlation between demographics and language.\n\u2022 Validate our approach by verifying that text aligned with African-American demographics follows well-known phonological and syntactic properties of AAE, and document the previously unattested ways in which such text diverges from SAE (\u00a73). \u2022 Demonstrate racial disparity in the efficacy\nof NLP tools for language identification and dependency parsing\u2014they perform poorly on this text, compared to text associated with white speakers (\u00a74, \u00a75). \u2022 Improve language identification for U.S. on-\nline conversational text with a simple ensemble classifier using our demographicallybased distant supervision method, aiming to eliminate racial disparity in accuracy rates (\u00a74.2). \u2022 Provide a corpus of 830,000 tweets aligned\nwith African-American demographics."}, {"heading": "2 Identifying AAE from Demographics", "text": "The presence of AAE in social media and the generation of resources of AAE-like text for NLP tasks has attracted recent interest in sociolinguistic and natural language processing research; Jones (2015) shows that nonstandard AAE orthography on Twitter aligns with historical patterns of AfricanAmerican migration in the U.S., while J\u00f8rgensen et al. (2015) investigate to what extent it supports\n1Including a recent linguistics workshop: http://linguistlaura.blogspot.co.uk/2016/06/ using-twitter-for-linguistic-research.html\nwell-known sociolinguistics hypotheses about AAE. Both, however, find AAE-like language on Twitter through keyword searches, which may not yield broad corpora reflective of general AAE use. More recently, J\u00f8rgensen et al. (2016) generated a large unlabeled corpus of text from hip-hop lyrics, subtitles from The Wire and The Boondocks, and tweets from a region of the southeast U.S. While this corpus does indeed capture a wide variety of language, we aim to discover AAE-like language by utilizing finer-grained, neighborhood-level demographics from across the country.\nOur approach to identifying AAE-like text is to first harvest a set of messages from Twitter, cross-referenced against U.S. Census demographics (\u00a72.1), then to analyze words against demographics with two alternative methods, a seedlist approach (\u00a72.2) and a mixed-membership probabilistic model (\u00a72.3)."}, {"heading": "2.1 Twitter and Census data", "text": "In order to create a corpus of demographicallyassociated dialectal language, we turn to Twitter, whose public messages contain large amounts of casual conversation and dialectal speech (Eisenstein, 2015). It is well-established that Twitter can be used to study both geographic dialectal varieties2 and minority languages.3\nSome methods exist to associate messages with authors\u2019 races; one possibility is to use birth record statistics to identify African-American-associated names, which has been used in (non-social media) social science studies (Sweeney, 2013; Bertrand and Mullainathan, 2003). However, metadata about authors is fairly limited on Twitter and most other social media services, and many supplied names are obviously not real.\nInstead, we turn to geo-location and induce a distantly supervised mapping between authors and the demographics of the neighborhoods they live in (O\u2019Connor et al., 2010a; Eisenstein et al., 2011b; Stewart, 2014). We draw on a set of geo-located Twitter messages, most of which are sent on mobile phones, by authors in the U.S. in 2013. (These are selected from a general archive of the \u201cGar-\n2For example, of American English (Huang et al., 2015; Doyle, 2014).\n3For example, Lynn et al. (2015) develop POS corpora and taggers for Irish tweets; see also related work in \u00a74.1.\ndenhose/Decahose\u201d sample stream of public Twitter messages (Morstatter et al., 2013)). Geolocated users are a particular sample of the userbase (Pavalanathan and Eisenstein, 2015), but we expect it is reasonable to compare users of different races within this group.\nWe look up the U.S. Census blockgroup geographic area that the message was sent in; blockgroups are one of the smallest geographic areas defined by the Census, typically containing a population of 600\u20133000 people. We use race and ethnicity information for each blockgroup from the Census\u2019 2013 American Community Survey, defining four covariates: percentages of the population that are non-Hispanic whites, non-Hispanic blacks, Hispanics (of any race), and Asian.4 Finally, for each user u, we average the demographic values of all their messages in our dataset into a length-four vector \u03c0(census)u . Under strong assumptions, this could be interpreted as the probability of which race the user is; we prefer to think of it as a rough proxy for likely demographics of the author and the neighborhood they live in.\nMessages were filtered in order to focus on casual conversational text; we exclude tweets whose authors had 1000 or more followers, or that (a) contained 3 or more hashtags, (b) contained the strings \u201chttp\u201d, \u201cfollow\u201d, or \u201cmention\u201d (messages designed to generate followers), or (c) were retweeted (either containing the string \u201crt\u201d or marked by Twitter\u2019s metadata as re-tweeted).\nOur initial Gardenhose/Decahose stream archive had 16 billion messages in 2013; 90 million were geo-located with coordinates that matched a U.S. Census blockgroup. 59.2 million tweets from 2.8 million users remained after pre-processing; each user is associated with a set of messages and averaged demographics \u03c0(census)u ."}, {"heading": "2.2 Direct Word-Demographic Analysis", "text": "Given a set of messages and demographics associated with their authors, a number of methods could be used to infer statistical associations between language and demographics.\nDirect word-demographic analysis methods use the \u03c0(census)u quantities to calculate statistics at the word level in a single pass. An intuitive approach\n4See appendix for additional details.\nis to calculate the average demographics per word. For a token in the corpus indexed by t (across the whole corpus), let u(t) be the author of the message containing that token, andwt be the word token. The average demographics of word type w is:5\n\u03c0(softcount)w \u2261 \u2211 t 1{wt = w}\u03c0 (census) u(t)\u2211\nt 1{wt = w}\nWe find that terms with the highest \u03c0w,AA values (denoting high average African-American demographics of their authors\u2019 locations) are very non-standard, while Stewart (2014) and Eisenstein (2013) find large \u03c0w,AA associated with certain AAE linguistic features.\nOne way to use the \u03c0w,k values to construct a corpus is through a seedlist approach. In early experiments, we constructed a corpus of 41,774 users (2.3 million messages) by first selecting the n = 100 highest-\u03c0w,AA terms occurring at least m = 3000 times across the data set, then collecting all tweets from frequent authors who have at least 10 tweets and frequently use these terms, defined as the case when at least p = 20% of their messages contain at least one of the seedlist terms. Unfortunately, the n,m, p thresholds are ad-hoc."}, {"heading": "2.3 Mixed-Membership Demographic-Language Model", "text": "The direct word-demographics analysis gives useful validation that the demographic information may yield dialectal corpora, and the seedlist approach can assemble a set of users with heavy dialectal usage. However, the approach requires a number of ad-hoc thresholds, cannot capture authors who only occasionally use demographically-aligned language, and cannot differentiate language use at the message-level. To address these concerns, we develop a mixed-membership model for demographics and language use in social media.\nThe model directly associates each of the four demographic variables with a topic; i.e. a unigram language model over the vocabulary.6 The model as-\n5 \u03c0w,k has the flavor of \u201csoft counts\u201d in multinomial EM. By changing the denominator to \u2211 t \u03c0 (census)\nu(t) , it calculates a unigram language model that sums to one across the vocabulary. This hints at a more complete modeling approach (\u00a72.3).\n6To build the vocabulary, we select all words used by at least 20 different users, resulting in 191,873 unique words; other words are mapped to an out-of-vocabulary symbol.\nsumes an author\u2019s mixture over the topics tends to be similar to their Census-associated demographic weights, and that every message has its own topic distribution. This allows for a single author to use different types of language in different messages, accommodating multidialectal authors. The messagelevel topic probabilities \u03b8m are drawn from an asymmetric Dirichlet centered on \u03c0(census)u , whose scalar concentration parameter \u03b1 controls whether authors\u2019 language is very similar to the demographic prior, or can have some deviation. A token t\u2019s latent topic zt is drawn from \u03b8m, and the word itself is drawn from \u03c6zt , the language model for the topic (Figure 1).\nThus the model learns demographically-aligned language models for each demographic category. The model is much more tightly constrained than a topic model\u2014for example, if \u03b1 \u2192 \u221e, \u03b8 becomes fixed and the likelihood is concave as a function of \u03c6\u2014but it still has more joint learning than a direct calculation approach, since the inference of a messages\u2019 topic memberships \u03b8m is affected not just by the Census priors, but also by the language used. A tweet written by an author in a highly AA neighborhood may be inferred to be non-AAE-aligned if it uses non-AAE-associated terms; as inference proceeeds, this information is used to learn sharper language models.\nWe fit the model with collapsed Gibbs sampling (Griffiths and Steyvers, 2004) with repeated sample updates for each token t in the corpus,\np(zt = k | w, z\u2212t) \u221d Nwk + \u03b2/V\nNk + \u03b2 Nmk + \u03b1\u03c0uk Nm + \u03b1\nwhere Nwk is the number of tokens where word w occurs under topic z = k, Nmk is the number of tokens in the current message with topic k, etc.; all\ncounts exclude the current t position. We observed convergence of the log-likelihood within 100 to 200 iterations, and ran for 300 total.7 We average together count tables from the last 50 Gibbs samples for analysis of posterior topic memberships at the word, message, and user level; for example, the posterior probability a particular user u uses topic k, P (z = k | u), can be calculated as the fraction of tokens with topic k within messages authored by u.\nWe considered \u03b1 to be a fixed control parameter; setting it higher increases the correlations between P (z = k | u) and \u03c0(census)u,k . We view the selection of \u03b1 as an inherently difficult problem, since the correlation between race and AAE usage is already complicated and imperfect at the author-level, and census demographics allow only for rough associations. We set \u03b1 = 10 which yields posterior user-level correlations of P (z = AA | u) against \u03c0u,AA to be approximately 0.8.\nThis model has broadly similar goals as nonlatent, log-linear generative models of text that condition on document-level covariates (Monroe et al., 2008; Eisenstein et al., 2011a; Taddy, 2013). The formulation here has the advantage of fast inference with large vocabularies (since the partition function never has to be computed), and gives probabilistic admixture semantics at arbitrary levels of the data. This model is also related to topic models where the selection of \u03b8 conditions on covariates (Mimno and McCallum, 2008; Ramage et al., 2011; Roberts et al., 2013), though it is much simpler without full latent topic learning.\nIn early experiments, we used only two classes (AA and not AA), and found Spanish terms being included in the AA topic. Thus we turned to four race categories in order to better draw out non-AAE language. This removed Spanish terms from the AA topic; interestingly, they did not go to the Hispanic topic, but instead to Asian, along with other foreign languages. In fact, the correlation between users\u2019 Census-derived proportions of Asian populations, versus this posterior topic\u2019s proportions, is only 0.29, while the other three topics correlate to their respective Census priors in the range 0.83 to 0.87. This indicates the \u201cAsian\u201d topic actually functions as a background topic (at least in part).\n7Our straightforward single core implementation (in Julia) spends 80 seconds for each iteration over 586 million tokens.\nBetter modeling of demographics and non-English language interactions is interesting potential future work.\nBy fitting the model to data, we can directly analyze unigram probabilities within the model parameters \u03c6, but for other analyses, such as analyzing larger syntactic constructions and testing NLP tools, we require an explicit corpus of messages.\nTo generate a user-based AA-aligned corpus, we collected all tweets from users whose posterior probability of using AA-associated terms under the model was at least 80%, and generated a corresponding white-aligned corpus as well. In order to remove the effects of non-English languages, and given uncertainty about what the model learned in the Hispanic and Asian-aligned demographic topics, we focused only on AA- and white-aligned language by imposing the additional constraint that each user\u2019s combined posterior proportion of Hispanic or Asian language was less than 5%. Our two resulting user corpora contain 830,000 and 7.3 million tweets, for which we are making their message IDs available for further research (in conformance with the Twitter API\u2019s Terms of Service). In the rest of the work, we refer to these as the AA- and white-aligned corpora, respectively."}, {"heading": "3 Linguistic Validation", "text": "Because validation by manual inspection of our AAaligned text is impractical, we turn to the wellstudied phonological and syntactic phenomena that traditionally distinguish AAE from SAE. We validate our model by reproducing these phenomena, and document a variety of other ways in which our AA-aligned text diverges from SAE."}, {"heading": "3.1 Lexical-Level Variation", "text": "We begin by examining how much AA- and whitealigned lexical items diverge from a standard dictionary. We used SCOWL\u2019s largest wordlist with level 1 variants as our dictionary, totaling 627,685 words.8\nWe calculated, for each word w in the model\u2019s vocabulary, the ratio\nrk(w) = p(w|z = k) p(w|z 6= k)\n8http://wordlist.aspell.net/\nwhere the p(.|.) probabilities are posterior inferences, derived from averaged Gibbs samples of the sufficient statistic count tables Nwk.\nWe selected heavily AA- and white-aligned words as those where rAA(w) \u2265 2 and rwhite(w) \u2265 2, respectively. We find that while 58.2% of heavily white-aligned words were not in our dictionary, fully 79.1% of heavily AA-aligned words were not. While a high number of out-of-dictionary lexical items is expected for Twitter data, this disparity suggests that the AA-aligned lexicon diverges from SAE more strongly than the white-aligned lexicon."}, {"heading": "3.2 Internet-Specific Orthographic Variation", "text": "We performed an \u201copen vocabulary\u201d unigram analysis by ranking all words in the vocabulary by rAA(w) and browsed them and samples of their usage. Among the words with high rAA, we observe a number of Internet-specific orthographic variations, which we separate into three types: abbreviations (e.g. llh, kmsl), shortenings (e.g. dwn, dnt), and spelling variations which do not correlate to the word\u2019s pronunciation (e.g. axx, bxtch). These variations do not reflect features attested in the literature; rather, they appear to be purely orthographic variations highly specific to AAE-speaking communities online. They may highlight previously unknown linguistic phenomena; for example, we observe that thoe (SAE though) frequently appears in the role of a discourse marker instead of its standard SAE usage (e.g. Girl Madison outfit THOE). This new use of though as a discourse marker, which is difficult to observe using the SAE spelling amidst many instances of the SAE usage, is readily identifiable in examples containing the thoe variant. Thus, nonstandard spellings provide valuable windows into a variety of linguistic phenomena.\nIn the next section, we turn to variations which do appear to arise from known phonological processes."}, {"heading": "3.3 Phonological Variation", "text": "Many phonological features are closely associated with AAE (Green, 2002). While there is not a perfect correlation between orthographic variations and people\u2019s pronunciations, Eisenstein (2013) shows that some genuine phonological phenomena, including a number of AAE features, are accurately reflected in orthographic variation on social media. We therefore validate our model by verifying that\nspellings reflecting known AAE phonological features align closely with the AA topic.\nWe selected 31 variants of SAE words from previous studies of AAE phonology on Twitter (J\u00f8rgensen et al., 2015; Jones, 2015). These variations display a range of attested AAE phonological features, such as derhotacization (e.g. brotha), deletion of initial g and d (e.g. iont), and realization of voiced th as d (e.g. dey) (Rickford, 1999).\nTable 1 shows the top five of these words by their rAA(w) ratio. For 30 of the 31 words, r \u2265 1, and for 13 words, r \u2265 100, suggesting that our model strongly identifies words displaying AAE phonological features with the AA topic. The sole exception is the word brotha, which appears to have been adopted into general usage as its own lexical item."}, {"heading": "3.4 Syntactic Variation", "text": "We further validate our model by verifying that it reproduces well-known AAE syntactic constructions, investigating three well-attested AAE aspectual or preverbal markers: habitual be, future gone, and completive done (Green, 2002). Table 2 shows examples of each construction.\nTo search for the constructions, we tagged the corpora using the ARK Twitter POS tagger (Gimpel et al., 2011; Owoputi et al., 2013),9 which J\u00f8rgensen et al. (2015) show has similar accuracy rates on both AAE and non-AAE tweets, unlike other POS taggers. We searched for each construction by searching for sequences of unigrams and POS tags characterizing the construction; e.g. for habitual be we searched for the sequences O-be-V and O-b-V. Nonstandard spellings for the unigrams in the patterns were identified from the ranked analysis of \u00a73.2.\nWe examined how a message\u2019s likelihood of using each construction varies with the message\u2019s pos-\n9Version 0.3.2: http://www.cs.cmu.edu/\u223cark/TweetNLP/\nterior probability of AA. We split all messages into deciles based on the messages\u2019 posterior probability of AA. From each decile, we sampled 200,000 messages and calculated the proportion of messages containing the three syntactic constructions.\nFor all three constructions, we observed the clear pattern that as messages\u2019 posterior probabilities of AA increase, so does their likelihood of containing the construction. Interestingly, for all three constructions, frequency of usage peaks at approximately the [0.7, 0.8) decile. One possible reason for the decline in higher deciles might be tendency of high-AA messages to be shorter; while the mean number of tokens per message across all deciles in our samples is 9.4, the means for the last two deciles are 8.6 and 7.1, respectively.\nGiven the important linguistic differences between our demographically-aligned subcorpora, we hypothesize that current NLP tools may behave differently. We investigate this hypothesis in \u00a74 and \u00a75."}, {"heading": "4 Lang ID Tools on AAE", "text": ""}, {"heading": "4.1 Evaluation of Existing Classifiers", "text": "Language identification, the task of classifying the major world language in which a message is written, is a crucial first step in almost any web or social media text processing pipeline. For example, in order to analyze the opinions of U.S. Twitter users, one might throw away all non-English messages before running an English sentiment analyzer.\nHughes et al. (2006) review language identification methods; social media language identification is challenging since messages are short, and also use non-standard and multiple (often related) languages (Baldwin et al., 2013). Researchers have sought to model code-switching in social media language (Rosner and Farrugia, 2007; Solorio and Liu, 2008; Maharjan et al., 2015; Zampieri et al., 2013; King and Abney, 2013), and recent workshops have focused on code-switching (Solorio et al., 2014) and general language identification (Zubiaga et al., 2014). For Arabic dialect classification, work has developed corpora in both traditional and Romanized script (Cotterell et al., 2014; Malmasi et al., 2015) and tools that use n-gram and morphological analysis to identify code-switching between dialects and with English (Elfardy et al., 2014).\nWe take the perspective that since AAE is a dialect of American English, it ought to be classified as English for the task of major world language identification. Lui and Baldwin (2012) develop langid.py, one of the most popular open source language identification tools, training it on over 97 languages from texts including Wikipedia, and evaluating on both traditional corpora and Twitter messages. We hypothesize that if a language identification tool is trained on standard English data, it may exhibit disparate performance on AA- versus whitealigned tweets. Since language identifiers are typically based on character n-gram features, they may get confused by the types of lexical/orthographic di-\nvergences seen in \u00a73. To evaluate this hypothesis, we compare the behavior of existing language identifiers on our subcorpora.\nWe test langid.py as well as the output of Twitter\u2019s in-house identifier, whose predictions are included in a tweet\u2019s metadata (from 2013, the time of data collection); the latter may give a language code or a missing value (unk or an empty/null value). We record the proportion of non-English predictions by these systems; Twitter-1 does not consider missing values to be a non-English prediction, and Twitter-2 does.\nWe noticed emojis had seemingly unintended consequences on langid.py\u2019s classifications, so removed all emojis by characters from the relevant Unicode ranges. We also removed @-mentions.\nUser-level analysis We begin by comparing the classifiers\u2019 behavior on the AA- and white-aligned corpora. Of the AA-aligned tweets, 13.2% were classified by langid.py as non-English; in contrast, 7.6% of white-aligned tweets were classified as such. We observed similar disparities for Twitter-1 and Twitter-2, illustrated in Table 3.\nIt turns out these \u201cnon-English\u201d tweets are, for the most part, actually English. We sampled and annotated 50 tweets from the tweets classified as nonEnglish by each run. Of these 300 tweets, only 3 could be unambiguously identified as written in a language other than English.\nMessage-level analysis We examine how a message\u2019s likelihood of being classified as non-English varies with its posterior probability of AA. As in \u00a73.4, we split all messages into deciles based on the messages\u2019 posterior probability of AA, and predicted language identifications on 200,000 sampled messages from each decile.\nFor all three systems, the proportion of messages classified as non-English increases steadily as the messages\u2019 posterior probabilities of AA increase. As before, we sampled and annotated from the tweets classified as non-English, sampling 50 tweets from each decile for each of the three systems. Of the 1500 sampled tweets, only 13 (\u223c0.87%) could be unambiguously identified as being in a language other than English."}, {"heading": "4.2 Adapting Language Identification for AAE", "text": "Natural language processing tools can be improved to better support dialects; for example, J\u00f8rgensen\net al. (2016) use domain adaptation methods to improve POS tagging on AAE corpora. In this section, we contribute a fix to language identification to correctly identify AAE and other social media messages as English."}, {"heading": "4.2.1 Ensemble Classifier", "text": "We observed that messages where our model infers a high probability of AAE, white-aligned, or \u201cHispanic\u201d-aligned language almost always are written in English; therefore we construct a simple ensemble classifier by combining it with langid.py.\nFor a new message ~w, we predict its demographic-language proportions \u03b8\u0302 via posterior inference with our trained model, given a symmetric \u03b1 prior over demographic-topic proportions (see appendix for details). The ensemble classifier, given a message, is as follows:\n\u2022 Calculate langid.py\u2019s prediction y\u0302. \u2022 If y\u0302 is English, accept it as English. \u2022 If y\u0302 is non-English, and at least one of the\nmessage\u2019s tokens are in demographic model\u2019s vocabulary: Infer \u03b8\u0302 and return English only if the combined AA, Hispanic, and white posterior probabilities are at least 0.9. Otherwise return the non-English y\u0302 decision.\nAnother way to view this method is that we are effectively training a system on an extended Twitterspecific English language corpus softly labeled by our system\u2019s posterior inference; in this respect, it is related to efforts to collect new language-specific\nTwitter corpora (Bergsma et al., 2012) or minority language data from the web (Ghani et al., 2001)."}, {"heading": "4.2.2 Evaluation", "text": "Our analysis from \u00a74.1 indicates that this method would correct erroneous false negatives for AAE messages in the training set for the model. We further confirm this by testing the classifier on a sample of 2.2 million geolocated tweets sent in the U.S. in 2014, which are not in the training set.\nIn addition to performance on the entire sample, we examine our classifier\u2019s performance on messages whose posterior probability of using AA- or white-associated terms was greater than 0.8 within the sample, which in this section we will call high AA and high white messages, respectively. Our classifier\u2019s precision is high across the board, at 100% across manually annotated samples of 200 messages from each sample.10 Since we are concerned about the system\u2019s overall recall, we impute recall (Table 4) by assuming that all high AA and high white messages are indeed English. Recall for langid.py alone is calculated by nN , where n is the number of messages predicted to be English by langid.py, and N is the total number of messages in the set. (This is the complement of Table 3, except evaluated on the test set.) We estimate the ensemble\u2019s recall as n+mN , where m = (nflip)P (English | flip) is the expected number of correctly changed classifications (from non-English to English) by the ensemble and the second term is the precision (estimated as 1.0). We observe the baseline system has considerable difference in recall between the groups which is solved by the ensemble.\nWe also apply the same calculation to the general set of all 2.2 million messages; the baseline classifies\n10We annotated 600 messages as English, not English, or not applicable, from 200 sampled each from general, high AA, and high white messages. Ambiguous tweets which were too short (e.g. \u201dGm\u201d) or contained only named entities (e.g. \u201dTennessee\u201d) were excluded from the final calculations. The resulting samples have 197/197, 198/198, and 200/200 correct English classifications, respectively.\n88% as English. This is a less accurate approximation of recall since we have observed a substantial presence of non-English messages. The ensemble classifies an additional 5.4% of the messages as English; since these are all (or nearly all) correct, this reflects at least a 5.4% gain to recall."}, {"heading": "5 Dependency Parser Evaluation", "text": "Given the lexical and syntactic variation of AAE compared to SAE, we hypothesize that syntactic analysis tools also have differential accuracy. J\u00f8rgensen et al. (2015) demonstrate this for part-ofspeech tagging, finding that SAE-trained taggers had disparate accuracy on AAE versus non-AAE tweets.\nWe assess a publicly available syntactic dependency parser on our AAE and white-aligned corpora. Syntactic parsing for tweets has received some research attention; Foster et al. (2011) create a corpus of constituent trees for English tweets, and Kong et al. (2014)\u2019s Tweeboparser is trained on a Twitter corpus annotated with a customized unlabeled dependency formalism; since its data was uniformly sampled from tweets, we expect it may have low disparity between demographic groups.\nWe focus on widely used syntactic representations, testing the SyntaxNet neural network-based dependency parser (Andor et al., 2016),11 which reports state-of-the-art results, including for web corpora. We evaluate it against a new manual annotation of 200 messages, 100 randomly sampled from each of the AA- and white-aligned corpora described in \u00a72.3.\nSyntaxNet outputs grammatical relations conforming to the Stanford Dependencies (SD) system (de Marneffe and Manning, 2008), which we used to annotate messages using Brat,12 comparing to predicted parses for reference. Message order was randomized and demographic inferences were hidden from the annotator. To increase statistical power relative to annotation effort, we developed a partial annotation approach to only annotate edges for the root word of the first major sentence in a message. Generally, we found that that SD worked well as a descriptive formalism for tweets\u2019 syntax; we describe handling of AAE and Internet-specific non-standard\n11Using the publicly available mcparseface model: https:// github.com/tensorflow/models/tree/master/syntaxnet\n12http://brat.nlplab.org/\nissues in the appendix. We evaluate labeled recall of the annotated edges for each message set:\nParser AA Wh. Difference SyntaxNet 64.0 (2.5) 80.4 (2.2) 16.3 (3.4) CoreNLP 50.0 (2.7) 71.0 (2.5) 21.0 (3.7)\nBootstrapped standard errors (from 10,000 message resamplings) are in parentheses; differences are statistically significant (p < 10\u22126 in both cases).\nThe white-aligned accuracy rate of 80.4% is broadly in line with previous work (compare to the parser\u2019s unlabeled accuracy of 89% on English Web Treebank full annotations), but parse quality is much worse on AAE tweets at 64.0%. We test the Stanford CoreNLP neural network dependency parser (Chen and Manning, 2014) using the english SD model that outputs this formalism;13 its disparity is worse. Soni et al. (2014) used a similar parser14 on Twitter text; our analysis suggests this approach may suffer from errors caused by the parser."}, {"heading": "6 Discussion and Conclusion", "text": "We have presented a distantly supervised probabilistic model that employs demographic correlations of a dialect and its speaker communities to uncover dialectal language on Twitter. Our model can also close the gap between NLP tools\u2019 performance on dialectal and standard text.\nThis represents a case study in dialect identification, characterization, and ultimately language technology adaptation for the dialect. In the case of AAE, dialect identification is greatly assisted since AAE speakers are strongly associated with a demographic group for which highly accurate governmental records (the U.S. Census) exist, which we leverage to help identify speaker communities. The notion of non-standard dialectal language implies that the dialect is underrepresented or underrecognized in some way, and thus should be inherently difficult to collect data on; and of course, many other language communities and groups are not necessarily officially recognized. An interesting direction for future research would be to combine distant supervision with unsupervised linguistic models to automatically uncover such underrecognized dialectal\n13pos,depparse options in version 2015-04-20, using tokenizations output by SyntaxNet.\n14The older Stanford englishPCFG model with dependency transform (via pers. comm.).\nlanguage.\nAcknowledgments: We thank Jacob Eisenstein, Taylor Jones, Anna J\u00f8rgensen, Dirk Hovy, and the anonymous reviewers for discussion and feedback."}, {"heading": "A Census demographics (\u00a72.1)", "text": "These four \u201craces\u201d (non-Hispanic whites, Hispanics, non-Hispanic African-Americans, and Asians) are commonly used in sociological studies of the U.S. The Census tracks other categories as well, such as Native Americans. The exact options the Census uses are somewhat complicated (e.g. Hispanic is not a \u201crace\u201d but a separate variable); in a small minority of cases, these values do not sum to one, so we re-normalize them for analysis and discard the small fraction of cases where their sum is less than 0.5. For simplicity, we sometimes refer to these four variables as races; this is a simplification since the Census considers race and ethnicity to be separate variables, and the relationship between the actual concepts of race and ethnicity is fraught on many levels."}, {"heading": "B Unicode ranges for emoji removal (\u00a74.1)", "text": "We observed that emoji symbols often caused langid.py to give strange results, so we preprocessed the data (improving langid.py\u2019s predictions) to remove emoji and other symbolic-type characters by removing all characters falling into particular Unicode ranges. We were not able to find effective preexisting solutions, so developed range lists by consulting documentation on Unicode and emoji standards, and observing message samples and how our proposed rules changed them. See emoji.py for our implementation. Note that Unicode consists of 17 planes of 65,536 codepoints each; each plane contains a number of variable-sized blocks. The first three planes are most often used today (Basic Multilingual, Supplemental Multilingual, Supplemental Ideographic). We remove all characters from the following ranges.\n\u2022 10000\u20131FFFF: The entire Supplemental Multilingual Plane, which contains emoji and other symbols, plus some rarely used scripts such as Egyptian hieroglyphics.\n\u2022 30000\u201310FFFF: The fourth and higher planes.\n\u2022 02500-02BFF: A collection of symbol blocks within the Basic Multilingual Plane, including: Box Drawing, Box Elements, Miscellaneous Symbols, Dingbats, Miscellaneous Mathematical Symbols-A, Supplemental Arrows-A,\nBraille Patterns, Supplemental Arrows-B, Miscellaneous Mathematical Symbols-B, Supplemental Mathematical Operators, Miscellaneous Symbols and Arrows.\n\u2022 0E000\u20130EFFF: A \u201cprivate use\u201d area in the BMP, for which we observed at least one nonstandard character in Twitter data (U+E056).\n\u2022 0200B\u20130200D: Zero-width spaces, joiner, and nonjoiner. The ZWJ is used to create compositional emoji.15\n\u2022 0FE0E, 0FE0F: Variation sequences, which are invisible post-modifiers allowed for certain emoji.16\nC Posterior inference via CVB0 for ensemble classifier (\u00a74.2.1)\nThe posterior inference task is to calculate the posterior expectation of\nP (\u03b8 | w, \u03c6, \u03b1) \u221d P (\u03b8 | \u03b1)P (w | \u03b8, \u03c6)\nwhere \u03c6 are the trained topic-word language models and \u03b8 \u223c Dir(\u03b1) is a prior over topic proportions, with a fixed symmetric prior \u03b1k = 1/16.\nThe \u03c6 topic-word distributions are calculated via training-time posterior inference by averaging Gibbs samples N\u0304wk = (1/S) \u2211 s (where s indexes the last 50 samples of the Gibbs sampler), as well as adding a pseudocount of 1 and normalizing:\n\u03c6k,w \u221d (N\u0304k,w + 1)\n(The detailed balance theory of MCMC implies no pseudocount should be added, but we found it seemed to help since it prevents rare words from having overly low posterior expected counts.)\nThe \u03b8\u0302 prediction is inferred as the posterior mean given the words in the message by using the \u201cCVB0\u201d version of variational Bayes (Asuncion et al., 2009), which is closely related to both Gibbs sampling and EM. It iteratively updates the soft posterior for each token position t = 1..T ,\nqt(k) \u221d (N\u2212t,k + \u03b1k) \u03c6k,wt 15http://www.unicode.org/emoji/charts/\nemoji-zwj-sequences.html 16http://unicode.org/reports/tr51/\nwhere N\u2212t,k = \u2211\nt\u2032 6=t qt\u2032(k) is the soft topic count from other tokens in the message. The final posterior mean of \u03b8 is estimated as \u03b8\u0302k = (1/T ) \u2211 t qt(k). We find, similar to Asuncion et al. (2009), that CVB0 has the advantage of simplicity and rapid convergence; \u03b8\u0302 converges to within absolute 0.001 of a fixed point within five iterations on test cases."}, {"heading": "D Syntactic dependency annotations (\u00a75)", "text": "The SyntaxNet model outputs grammatical relations based on Stanford Dependencies version 3.3.0;17 thus we sought to annotate messages with this formalism, as described in a 2013 revision to de Marneffe and Manning (2008).18 For each message, we parsed it and displayed the output in the Brat annotation software19 alongside an unannotated copy of the message, which we added dependency edges to. This allowed us to see the proposed analysis to improve annotation speed and conformance with the grammatical standard. For difficult cases, we parsed shortened, Standard English toy sentences to confirm what relations were intended to be used to capture specific syntactic constructs. Sometimes this clearly contradicted the annotation standards (probably due to mismatch between the annotations it was trained on versus the version of the dependencies manual we viewed); we typically deferred to the parser\u2019s interpretation in such cases.\nIn order to save annotation effort for this evaluation, we took a partial annotation approach: for each message, we identified the root word of the first major sentence20 in the message\u2014typically the main verb\u2014and annotated its immediate dependent edges. Thus for every tweet, the gold standard included one or more labeled edges, all rooted in a single token. As opposed to completely annotating all words in a message, this allowed us to cover a broader set of messages, increasing statistical power\n17Personal communication with the authors. 18We only had access to the 2015 version, currently available online. We also considered follow-up works de Marneffe et al. (2014) and http://universaldependencies.org/, deferring to de Marneffe and Manning (2008) when in conflict.\n19http://brat.nlplab.org/ 20We take Kong et al. (2014)\u2019s view that a tweet consists of a sequence of one or more disconnected utterances. We sought to exclude minor utterances like \u201cNo\u201d in \u201cNo. I do not see it\u201d from annotation; in this case, we would prefer to annotate \u201csee.\u201d. A short utterance of all interjections was considered \u201cminor\u201d; a noun phrase or verb-headed sentence was considered \u201cmajor.\u201d\nfrom the perspective of sampling from a message population. It also alleviated the need to make fewer difficult annotation decisions - linguistic phenomena such as mistokenized fragments of emoticons, symbolic discourse markers, and (possibly multiword) hashtags.\nWe use the twokenize Twitter-specific tokenizer for the messages, which separates emoticons, symbols and URLs from the text (O\u2019Connor et al., 2010b; Owoputi et al., 2013)21 and use the spaceseparated tokenizations as input to SyntaxNet, allowing it to tokenize further. This substantially improves accuracy by correctly splitting contractions like \u201cdo n\u2019t\u201d and \u201cwan na\u201d (following Penn and English Web Treebank conventions). However, as expected, it fails to split apostrophe-less forms like \u201cdont\u201d and more complicated multiword tokens like \u201cima\u201d (I am going to, which Gimpel et al. (2011) sought to give a joint Pronoun-Verb grammatical category), typically leading to misanalysis as nouns. It also erroneously splits apart emoticons and other multi-character symbolic expressions; fortunately, these are never the the head of an utterance, so they do not need to be annotated under our partial annotation design.\nWe find that the Stanford Dependencies system worked well as a descriptive formalism for tweets\u2019 syntax, including AAE constructions; for example, several cases of \u201cgone-V,\u201d \u201cdone-V,\u201d and habitual be were analyzed as auxiliary verbs (e.g. aux(let,done) in \u201cI done let alot of time go by\u201d), and the SD treatment of copulas trivially extends to null copulas.\nMultiword tokens like an untokenized \u201cdont\u201d (do not) or \u201caf\u201d (as fuck, syntactically a PP) pose a challenge as well. We adopt the following convention: for all incoming edges that would have gone to any of their constituent words (had the token been translated into a multitoken sequence), we annotate that edge as going to the token. If there are mulitple conflicting edges\u2014which happens if the subgraph of the constituent words has multiple roots\u2014the earliest token gets precedence. For example, \u201cI \u2019m tired\u201d has the analysis nsubj(I,tired), cop(\u2019m,tired); thus the multiword token \u201cIm\u201d in \u201cIm tired\u201d would be internally multirooted. \u201cI\u201c has priority over \u201c\u2019m\u201d, yielding the analysis nsubj(tired,Im).\n21Using Myle Ott\u2019s implementation: https://github.com/ myleott/ark-twokenize-py\nPunctuation edges (punct) were not annotated. We found discourse edges (discourse) to be difficult annotation decisions, since in many cases the dependent was debatably in a different utterance. We tended to defer to the parser\u2019s predictions when in doubt. The partial labeling approach does not penalize the parser if the annotator gives too few edges, but these issues would have to be tackled to create a full treebank in future work."}, {"heading": "E Annotation materials", "text": "We supply our annotations with the online materials22 as well as working notes about difficult cases. Annotations are formatted in Brat\u2019s plaintext format.\n22http://slanglab.cs.umass.edu/TwitterAAE/"}], "references": [{"title": "Globally normalized transition-based neural networks", "author": ["Daniel Andor", "Chris Alberti", "David Weiss", "Aliaksei Severyn", "Alessandro Presta", "Kuzman Ganchev", "Slav Petrov", "Michael Collins"], "venue": "arXiv preprint arXiv:1603.06042,", "citeRegEx": "Andor et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Andor et al\\.", "year": 2016}, {"title": "On smoothing and inference for topic models", "author": ["Arthur Asuncion", "Max Welling", "Padhraic Smyth", "Yee Whye Teh"], "venue": "In Uncertainty in Artificial Intelligence,", "citeRegEx": "Asuncion et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Asuncion et al\\.", "year": 2009}, {"title": "How noisy social media text, how diffrnt social media sources", "author": ["Timothy Baldwin", "Paul Cook", "Marco Lui", "Andrew MacKinlay", "Li Wang"], "venue": "In International Joint Conference on Natural Language Processing,", "citeRegEx": "Baldwin et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Baldwin et al\\.", "year": 2013}, {"title": "Are Emily and Greg more employable than Lakisha and Jamal? A field experiment on labor market discrimination", "author": ["Marianne Bertrand", "Sendhil Mullainathan"], "venue": "Technical report, National Bureau of Economic Research,", "citeRegEx": "Bertrand and Mullainathan.,? \\Q2003\\E", "shortCiteRegEx": "Bertrand and Mullainathan.", "year": 2003}, {"title": "A fast and accurate dependency parser using neural networks", "author": ["Danqi Chen", "Christopher Manning"], "venue": "In Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP),", "citeRegEx": "Chen and Manning.,? \\Q2014\\E", "shortCiteRegEx": "Chen and Manning.", "year": 2014}, {"title": "Stanford typed dependencies manual", "author": ["M.C. de Marneffe", "C.D. Manning"], "venue": "Technical report, last revised April 2015 edition,", "citeRegEx": "Marneffe and Manning.,? \\Q2008\\E", "shortCiteRegEx": "Marneffe and Manning.", "year": 2008}, {"title": "Universal Stanford dependencies: A cross-linguistic typology", "author": ["Marie-Catherine de Marneffe", "Timothy Dozat", "Natalia Silveira", "Katri Haverinen", "Filip Ginter", "Joakim Nivre", "Christopher D. Manning"], "venue": "In Proceedings of LREC,", "citeRegEx": "Marneffe et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Marneffe et al\\.", "year": 2014}, {"title": "Mapping dialectal variation by querying social media", "author": ["Gabriel Doyle"], "venue": "In Proceedings of EACL,", "citeRegEx": "Doyle.,? \\Q2014\\E", "shortCiteRegEx": "Doyle.", "year": 2014}, {"title": "Phonological factors in social media writing", "author": ["Jacob Eisenstein"], "venue": "In Proc. of the Workshop on Language Analysis in Social Media,", "citeRegEx": "Eisenstein.,? \\Q2013\\E", "shortCiteRegEx": "Eisenstein.", "year": 2013}, {"title": "Identifying regional dialects in online social media", "author": ["Jacob Eisenstein"], "venue": "Handbook of Dialectology. Wiley,", "citeRegEx": "Eisenstein.,? \\Q2015\\E", "shortCiteRegEx": "Eisenstein.", "year": 2015}, {"title": "Sparse additive generative models of text", "author": ["Jacob Eisenstein", "Amr Ahmed", "Eric P. Xing"], "venue": "In Proceedings of ICML,", "citeRegEx": "Eisenstein et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Eisenstein et al\\.", "year": 2011}, {"title": "Identifying code switching in informal Arabic text", "author": ["Heba Elfardy", "Mohamed Al-Badrashiny", "Mona Diab. Aida"], "venue": "Proceedings of EMNLP 2014,", "citeRegEx": "Elfardy et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Elfardy et al\\.", "year": 2014}, {"title": "Mladeni\u0107. Mining the web to create minority language corpora", "author": ["Rayid Ghani", "Rosie Jones", "Dunja"], "venue": "In Proceedings of the Tenth International Conference on Information and Knowledge Management,", "citeRegEx": "Ghani et al\\.,? \\Q2001\\E", "shortCiteRegEx": "Ghani et al\\.", "year": 2001}, {"title": "African American English: A Linguistic Introduction", "author": ["Lisa J. Green"], "venue": null, "citeRegEx": "Green.,? \\Q2002\\E", "shortCiteRegEx": "Green.", "year": 2002}, {"title": "Finding scientific topics", "author": ["T.L. Griffiths", "M. Steyvers"], "venue": "Proceedings of the National Academy of Sciences of the United States of America,", "citeRegEx": "Griffiths and Steyvers.,? \\Q2004\\E", "shortCiteRegEx": "Griffiths and Steyvers.", "year": 2004}, {"title": "Understanding US regional linguistic variation with Twitter data analysis", "author": ["Yuan Huang", "Diansheng Guo", "Alice Kasakoff", "Jack Grieve"], "venue": "Computers, Environment and Urban Systems,", "citeRegEx": "Huang et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Huang et al\\.", "year": 2015}, {"title": "Toward a description of African American Vernacular English dialect regions using \u201cBlack Twitter", "author": ["Taylor Jones"], "venue": "American Speech,", "citeRegEx": "Jones.,? \\Q2015\\E", "shortCiteRegEx": "Jones.", "year": 2015}, {"title": "Learning a POS tagger for AAVE-like language", "author": ["Anna J\u00f8rgensen", "Dirk Hovy", "Anders S\u00f8gaard"], "venue": "In Proceedings of NAACL. Association for Computational Linguistics,", "citeRegEx": "J\u00f8rgensen et al\\.,? \\Q2016\\E", "shortCiteRegEx": "J\u00f8rgensen et al\\.", "year": 2016}, {"title": "Labeling the languages of words in mixed-language documents using weakly supervised methods", "author": ["Ben King", "Steven P Abney"], "venue": "In Proceedings of HLT-NAACL,", "citeRegEx": "King and Abney.,? \\Q2013\\E", "shortCiteRegEx": "King and Abney.", "year": 2013}, {"title": "A dependency parser for tweets", "author": ["Lingpeng Kong", "Nathan Schneider", "Swabha Swayamdipta", "Archna Bhatia", "Chris Dyer", "Noah A. Smith"], "venue": "In Proceedings of the 2014 Conference on Empirical Methods", "citeRegEx": "Kong et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Kong et al\\.", "year": 2014}, {"title": "langid. py: An off-the-shelf language identification tool", "author": ["M. Lui", "T. Baldwin"], "venue": "In Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics (ACL", "citeRegEx": "Lui and Baldwin.,? \\Q2012\\E", "shortCiteRegEx": "Lui and Baldwin.", "year": 2012}, {"title": "Minority language Twitter: Part-of-speech tagging and analysis of Irish tweets", "author": ["Teresa Lynn", "Kevin Scannell", "Eimear Maguire"], "venue": "Proceedings of ACL-IJCNLP 2015,", "citeRegEx": "Lynn et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Lynn et al\\.", "year": 2015}, {"title": "Developing language-tagged corpora for code-switching tweets. In The 9th Linguistic Annotation Workshop held in conjuncion with NAACL", "author": ["Suraj Maharjan", "Elizabeth Blair", "Steven Bethard", "Thamar Solorio"], "venue": null, "citeRegEx": "Maharjan et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Maharjan et al\\.", "year": 2015}, {"title": "Arabic dialect identification using a parallel multidialectal corpus", "author": ["Shervin Malmasi", "Eshrag Refaee", "Mark Dras"], "venue": "In International Conference of the Pacific Association for Computational Linguistics,", "citeRegEx": "Malmasi et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Malmasi et al\\.", "year": 2015}, {"title": "Topic models conditioned on arbitrary features with Dirichlet-Multinomial regression", "author": ["David Mimno", "Andrew McCallum"], "venue": "In Uncertainty in Artificial Intelligence,", "citeRegEx": "Mimno and McCallum.,? \\Q2008\\E", "shortCiteRegEx": "Mimno and McCallum.", "year": 2008}, {"title": "Fightin\u2019 Words: Lexical feature selection and evaluation for identifying the content of political conflict", "author": ["B.L. Monroe", "M.P. Colaresi", "K.M. Quinn"], "venue": "Political Analysis,", "citeRegEx": "Monroe et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Monroe et al\\.", "year": 2008}, {"title": "A mixture model of demographic lexical variation", "author": ["Brendan O\u2019Connor", "Jacob Eisenstein", "Eric P. Xing", "Noah A. Smith"], "venue": "In NIPS Workshop on Machine Learning for Social Computing,", "citeRegEx": "O.Connor et al\\.,? \\Q2010\\E", "shortCiteRegEx": "O.Connor et al\\.", "year": 2010}, {"title": "TweetMotif: Exploratory search and topic summarization for Twitter", "author": ["Brendan O\u2019Connor", "Michel Krieger", "David Ahn"], "venue": "In Proceedings of the International AAAI Conference on Weblogs and Social Media,", "citeRegEx": "O.Connor et al\\.,? \\Q2010\\E", "shortCiteRegEx": "O.Connor et al\\.", "year": 2010}, {"title": "Improved part-of-speech tagging for online conversational text with word clusters", "author": ["Olutobi Owoputi", "Brendan O\u2019Connor", "Chris Dyer", "Kevin Gimpel", "Nathan Schneider", "Noah A. Smith"], "venue": "In Proceedings of NAACL,", "citeRegEx": "Owoputi et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Owoputi et al\\.", "year": 2013}, {"title": "Confounds and consequences in geotagged Twitter data", "author": ["Umashanthi Pavalanathan", "Jacob Eisenstein"], "venue": "In Proceedings of Empirical Methods for Natural Language Processing (EMNLP),", "citeRegEx": "Pavalanathan and Eisenstein.,? \\Q2015\\E", "shortCiteRegEx": "Pavalanathan and Eisenstein.", "year": 2015}, {"title": "African American Vernacular English: Features, Evolution, Educational Implications", "author": ["John Russell Rickford"], "venue": "Wiley-Blackwell,", "citeRegEx": "Rickford.,? \\Q1999\\E", "shortCiteRegEx": "Rickford.", "year": 1999}, {"title": "A tagging algorithm for mixed language identification in a noisy domain", "author": ["Mike Rosner", "Paulseph-John Farrugia"], "venue": "In Eighth Annual Conference of the International Speech Communication Association,", "citeRegEx": "Rosner and Farrugia.,? \\Q2007\\E", "shortCiteRegEx": "Rosner and Farrugia.", "year": 2007}, {"title": "Learning to predict code-switching points", "author": ["Thamar Solorio", "Yang Liu"], "venue": "In Proceedings of the Conference on Empirical Methods in Natural Language Processing,", "citeRegEx": "Solorio and Liu.,? \\Q2008\\E", "shortCiteRegEx": "Solorio and Liu.", "year": 2008}, {"title": "Now we stronger than ever: AfricanAmerican syntax in Twitter", "author": ["Ian Stewart"], "venue": "Proceedings of EACL,", "citeRegEx": "Stewart.,? \\Q2014\\E", "shortCiteRegEx": "Stewart.", "year": 2014}, {"title": "Discrimination in online ad delivery", "author": ["Latanya Sweeney"], "venue": "ACM Queue,", "citeRegEx": "Sweeney.,? \\Q2013\\E", "shortCiteRegEx": "Sweeney.", "year": 2013}, {"title": "Multinomial inverse regression for text analysis", "author": ["Matt Taddy"], "venue": "Journal of the American Statistical Association,", "citeRegEx": "Taddy.,? \\Q2013\\E", "shortCiteRegEx": "Taddy.", "year": 2013}, {"title": "Diwersy. N-gram language models and pos distribution for the identification of Spanish varieties", "author": ["Marcos Zampieri", "Binyam Gebrekidan Gebre", "Sascha"], "venue": "Proceedings of TALN2013,", "citeRegEx": "Zampieri et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Zampieri et al\\.", "year": 2013}, {"title": "Overview of TweetLID: Tweet language identification", "author": ["Arkaitz Zubiaga", "Inaki San Vincente", "Pablo Gamallo", "Jose Ramom Pichel", "Inaki Algeria", "Nora Aranberri", "Aitzol Ezeiza", "Victor Fresno"], "venue": "SEPLN", "citeRegEx": "Zubiaga et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Zubiaga et al\\.", "year": 2014}, {"title": "Spanish Society for Natural Language Processing", "author": ["Tweet Language Identification Workshop", "Girona", "Spain", "September"], "venue": "URL http://ceur-ws. org/Vol-1228/.", "citeRegEx": "Workshop et al\\.,? 2014", "shortCiteRegEx": "Workshop et al\\.", "year": 2014}, {"title": "CVB0 has the advantage of simplicity and rapid convergence; \u03b8\u0302 converges to within", "author": ["Asuncion"], "venue": null, "citeRegEx": "Asuncion,? \\Q2009\\E", "shortCiteRegEx": "Asuncion", "year": 2009}, {"title": "2014)\u2019s view that a tweet consists of a sequence of one or more disconnected utterances. We sought to exclude minor utterances like \u201cNo\u201d in \u201cNo. I do not see it", "author": ["Kong"], "venue": null, "citeRegEx": "Kong,? \\Q2014\\E", "shortCiteRegEx": "Kong", "year": 2014}], "referenceMentions": [{"referenceID": 13, "context": "Not all African-Americans speak AAE, and not all speakers of AAE are African-American; nevertheless, speakers of this variety have close ties with specific communities of African-Americans (Green, 2002).", "startOffset": 189, "endOffset": 202}, {"referenceID": 17, "context": "In fact, its presence in social media is attracting increasing interest for natural language processing (J\u00f8rgensen et al., 2016) and sociolinguistic (Stewart, 2014; Eisenstein, 2015; Jones, 2015) research.", "startOffset": 104, "endOffset": 128}, {"referenceID": 33, "context": ", 2016) and sociolinguistic (Stewart, 2014; Eisenstein, 2015; Jones, 2015) research.", "startOffset": 28, "endOffset": 74}, {"referenceID": 9, "context": ", 2016) and sociolinguistic (Stewart, 2014; Eisenstein, 2015; Jones, 2015) research.", "startOffset": 28, "endOffset": 74}, {"referenceID": 16, "context": ", 2016) and sociolinguistic (Stewart, 2014; Eisenstein, 2015; Jones, 2015) research.", "startOffset": 28, "endOffset": 74}, {"referenceID": 16, "context": "The presence of AAE in social media and the generation of resources of AAE-like text for NLP tasks has attracted recent interest in sociolinguistic and natural language processing research; Jones (2015) shows that nonstandard AAE orthography on Twitter aligns with historical patterns of AfricanAmerican migration in the U.", "startOffset": 190, "endOffset": 203}, {"referenceID": 16, "context": "The presence of AAE in social media and the generation of resources of AAE-like text for NLP tasks has attracted recent interest in sociolinguistic and natural language processing research; Jones (2015) shows that nonstandard AAE orthography on Twitter aligns with historical patterns of AfricanAmerican migration in the U.S., while J\u00f8rgensen et al. (2015) investigate to what extent it supports", "startOffset": 190, "endOffset": 357}, {"referenceID": 17, "context": "More recently, J\u00f8rgensen et al. (2016) generated a large unlabeled corpus of text from hip-hop lyrics, subtitles from The Wire and The Boondocks, and tweets from a region of the southeast U.", "startOffset": 15, "endOffset": 39}, {"referenceID": 9, "context": "In order to create a corpus of demographicallyassociated dialectal language, we turn to Twitter, whose public messages contain large amounts of casual conversation and dialectal speech (Eisenstein, 2015).", "startOffset": 185, "endOffset": 203}, {"referenceID": 34, "context": "Some methods exist to associate messages with authors\u2019 races; one possibility is to use birth record statistics to identify African-American-associated names, which has been used in (non-social media) social science studies (Sweeney, 2013; Bertrand and Mullainathan, 2003).", "startOffset": 224, "endOffset": 272}, {"referenceID": 3, "context": "Some methods exist to associate messages with authors\u2019 races; one possibility is to use birth record statistics to identify African-American-associated names, which has been used in (non-social media) social science studies (Sweeney, 2013; Bertrand and Mullainathan, 2003).", "startOffset": 224, "endOffset": 272}, {"referenceID": 33, "context": "Instead, we turn to geo-location and induce a distantly supervised mapping between authors and the demographics of the neighborhoods they live in (O\u2019Connor et al., 2010a; Eisenstein et al., 2011b; Stewart, 2014).", "startOffset": 146, "endOffset": 211}, {"referenceID": 15, "context": "For example, of American English (Huang et al., 2015; Doyle, 2014).", "startOffset": 33, "endOffset": 66}, {"referenceID": 7, "context": "For example, of American English (Huang et al., 2015; Doyle, 2014).", "startOffset": 33, "endOffset": 66}, {"referenceID": 7, "context": ", 2015; Doyle, 2014). For example, Lynn et al. (2015) develop POS corpora and taggers for Irish tweets; see also related work in \u00a74.", "startOffset": 8, "endOffset": 54}, {"referenceID": 29, "context": "Geolocated users are a particular sample of the userbase (Pavalanathan and Eisenstein, 2015), but we expect it is reasonable to compare users of different races within this group.", "startOffset": 57, "endOffset": 92}, {"referenceID": 31, "context": "We find that terms with the highest \u03c0w,AA values (denoting high average African-American demographics of their authors\u2019 locations) are very non-standard, while Stewart (2014) and Eisenstein (2013) find large \u03c0w,AA associated with certain AAE linguistic features.", "startOffset": 160, "endOffset": 175}, {"referenceID": 8, "context": "We find that terms with the highest \u03c0w,AA values (denoting high average African-American demographics of their authors\u2019 locations) are very non-standard, while Stewart (2014) and Eisenstein (2013) find large \u03c0w,AA associated with certain AAE linguistic features.", "startOffset": 179, "endOffset": 197}, {"referenceID": 14, "context": "We fit the model with collapsed Gibbs sampling (Griffiths and Steyvers, 2004) with repeated sample updates for each token t in the corpus,", "startOffset": 47, "endOffset": 77}, {"referenceID": 25, "context": "This model has broadly similar goals as nonlatent, log-linear generative models of text that condition on document-level covariates (Monroe et al., 2008; Eisenstein et al., 2011a; Taddy, 2013).", "startOffset": 132, "endOffset": 192}, {"referenceID": 35, "context": "This model has broadly similar goals as nonlatent, log-linear generative models of text that condition on document-level covariates (Monroe et al., 2008; Eisenstein et al., 2011a; Taddy, 2013).", "startOffset": 132, "endOffset": 192}, {"referenceID": 24, "context": "This model is also related to topic models where the selection of \u03b8 conditions on covariates (Mimno and McCallum, 2008; Ramage et al., 2011; Roberts et al., 2013), though it is much simpler without full latent topic learning.", "startOffset": 93, "endOffset": 162}, {"referenceID": 13, "context": "Many phonological features are closely associated with AAE (Green, 2002).", "startOffset": 59, "endOffset": 72}, {"referenceID": 8, "context": "While there is not a perfect correlation between orthographic variations and people\u2019s pronunciations, Eisenstein (2013) shows that some genuine phonological phenomena, including a number of AAE features, are accurately reflected in orthographic variation on social media.", "startOffset": 102, "endOffset": 120}, {"referenceID": 16, "context": "We selected 31 variants of SAE words from previous studies of AAE phonology on Twitter (J\u00f8rgensen et al., 2015; Jones, 2015).", "startOffset": 87, "endOffset": 124}, {"referenceID": 30, "context": "dey) (Rickford, 1999).", "startOffset": 5, "endOffset": 21}, {"referenceID": 13, "context": "We further validate our model by verifying that it reproduces well-known AAE syntactic constructions, investigating three well-attested AAE aspectual or preverbal markers: habitual be, future gone, and completive done (Green, 2002).", "startOffset": 218, "endOffset": 231}, {"referenceID": 28, "context": "To search for the constructions, we tagged the corpora using the ARK Twitter POS tagger (Gimpel et al., 2011; Owoputi et al., 2013),9 which J\u00f8rgensen et al.", "startOffset": 88, "endOffset": 131}, {"referenceID": 17, "context": ", 2013),9 which J\u00f8rgensen et al. (2015) show has similar accuracy rates on both AAE and non-AAE tweets, unlike other POS taggers.", "startOffset": 16, "endOffset": 40}, {"referenceID": 2, "context": "(2006) review language identification methods; social media language identification is challenging since messages are short, and also use non-standard and multiple (often related) languages (Baldwin et al., 2013).", "startOffset": 190, "endOffset": 212}, {"referenceID": 31, "context": "Researchers have sought to model code-switching in social media language (Rosner and Farrugia, 2007; Solorio and Liu, 2008; Maharjan et al., 2015; Zampieri et al., 2013; King and Abney, 2013), and recent workshops have focused on code-switching (Solorio et al.", "startOffset": 73, "endOffset": 191}, {"referenceID": 32, "context": "Researchers have sought to model code-switching in social media language (Rosner and Farrugia, 2007; Solorio and Liu, 2008; Maharjan et al., 2015; Zampieri et al., 2013; King and Abney, 2013), and recent workshops have focused on code-switching (Solorio et al.", "startOffset": 73, "endOffset": 191}, {"referenceID": 22, "context": "Researchers have sought to model code-switching in social media language (Rosner and Farrugia, 2007; Solorio and Liu, 2008; Maharjan et al., 2015; Zampieri et al., 2013; King and Abney, 2013), and recent workshops have focused on code-switching (Solorio et al.", "startOffset": 73, "endOffset": 191}, {"referenceID": 36, "context": "Researchers have sought to model code-switching in social media language (Rosner and Farrugia, 2007; Solorio and Liu, 2008; Maharjan et al., 2015; Zampieri et al., 2013; King and Abney, 2013), and recent workshops have focused on code-switching (Solorio et al.", "startOffset": 73, "endOffset": 191}, {"referenceID": 18, "context": "Researchers have sought to model code-switching in social media language (Rosner and Farrugia, 2007; Solorio and Liu, 2008; Maharjan et al., 2015; Zampieri et al., 2013; King and Abney, 2013), and recent workshops have focused on code-switching (Solorio et al.", "startOffset": 73, "endOffset": 191}, {"referenceID": 37, "context": ", 2014) and general language identification (Zubiaga et al., 2014).", "startOffset": 44, "endOffset": 66}, {"referenceID": 23, "context": "For Arabic dialect classification, work has developed corpora in both traditional and Romanized script (Cotterell et al., 2014; Malmasi et al., 2015) and tools that use n-gram and morphological analysis to identify code-switching between dialects and with English (Elfardy et al.", "startOffset": 103, "endOffset": 149}, {"referenceID": 11, "context": ", 2015) and tools that use n-gram and morphological analysis to identify code-switching between dialects and with English (Elfardy et al., 2014).", "startOffset": 122, "endOffset": 144}, {"referenceID": 20, "context": "Lui and Baldwin (2012) develop langid.", "startOffset": 0, "endOffset": 23}, {"referenceID": 12, "context": ", 2012) or minority language data from the web (Ghani et al., 2001).", "startOffset": 47, "endOffset": 67}, {"referenceID": 17, "context": "J\u00f8rgensen et al. (2015) demonstrate this for part-ofspeech tagging, finding that SAE-trained taggers had disparate accuracy on AAE versus non-AAE tweets.", "startOffset": 0, "endOffset": 24}, {"referenceID": 19, "context": "(2011) create a corpus of constituent trees for English tweets, and Kong et al. (2014)\u2019s Tweeboparser is trained on a Twitter corpus annotated with a customized unlabeled dependency formalism; since its data was uniformly sampled from tweets, we expect it may have low disparity between demographic groups.", "startOffset": 68, "endOffset": 87}, {"referenceID": 0, "context": "We focus on widely used syntactic representations, testing the SyntaxNet neural network-based dependency parser (Andor et al., 2016),11 which reports state-of-the-art results, including for web corpora.", "startOffset": 112, "endOffset": 132}, {"referenceID": 4, "context": "We test the Stanford CoreNLP neural network dependency parser (Chen and Manning, 2014) using the english SD model that outputs this formalism;13 its disparity is worse.", "startOffset": 62, "endOffset": 86}, {"referenceID": 4, "context": "We test the Stanford CoreNLP neural network dependency parser (Chen and Manning, 2014) using the english SD model that outputs this formalism;13 its disparity is worse. Soni et al. (2014) used a similar parser14 on Twitter text; our analysis suggests this approach may suffer from errors caused by the parser.", "startOffset": 63, "endOffset": 188}], "year": 2016, "abstractText": "Though dialectal language is increasingly abundant on social media, few resources exist for developing NLP tools to handle such language. We conduct a case study of dialectal language in online conversational text by investigating African-American English (AAE) on Twitter. We propose a distantly supervised model to identify AAE-like language from demographics associated with geo-located messages, and we verify that this language follows well-known AAE linguistic phenomena. In addition, we analyze the quality of existing language identification and dependency parsing tools on AAE-like text, demonstrating that they perform poorly on such text compared to text associated with white speakers. We also provide an ensemble classifier for language identification which eliminates this disparity and release a new corpus of tweets containing AAE-like language. Data and software resources are available at: http://slanglab.cs.umass.edu/TwitterAAE (This is an expanded version of our EMNLP 2016 paper, including the appendix at end.)", "creator": "LaTeX with hyperref package"}}}