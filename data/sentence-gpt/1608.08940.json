{"id": "1608.08940", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "31-Aug-2016", "title": "Hash2Vec, Feature Hashing for Word Embeddings", "abstract": "In this paper we propose the application of feature hashing to create word embeddings for natural language processing. Feature hashing has been used successfully to create document vectors in related tasks like document classification. In this work we show that feature hashing can be applied to obtain word embeddings in linear time with the size of the data provided by the program. We show that features hashing can be used in the linear time with the size of the data provided by the program. The results shown here illustrate the computational power of feature hashing. We show that feature hashing can be applied to obtain word embeddings in linear time with the size of the data provided by the program. In this paper we show that feature hashing can be applied to obtain word embeddings in linear time with the size of the data provided by the program. In this paper we show that feature hashing can be applied to obtain word embeddings in linear time with the size of the data provided by the program. In this paper we show that feature hashing can be applied to obtain word embeddings in linear time with the size of the data provided by the program. In this paper we show that feature hashing can be applied to obtain word embeddings in linear time with the size of the data provided by the program. In this paper we show that feature hashing can be applied to obtain word embeddings in linear time with the size of the data provided by the program. In this paper we show that feature hashing can be applied to obtain word embeddings in linear time with the size of the data provided by the program. In this paper we show that feature hashing can be applied to obtain word embeddings in linear time with the size of the data provided by the program. In this paper we show that feature hashing can be applied to obtain word embeddings in linear time with the size of the data provided by the program. In this paper we show that feature hashing can be applied to obtain word embeddings in linear time with the size of the data provided by the program. In this paper we show that feature hashing can be applied to obtain word embeddings in linear time with the size of the data provided by the program. In this paper we show that feature hashing can be applied to obtain word embeddings in linear time with the size of the data provided by the program. In this paper we show that feature hashing can be applied to obtain word embeddings in linear time with the size of the data provided by the program. In this paper we show that feature hashing", "histories": [["v1", "Wed, 31 Aug 2016 17:01:09 GMT  (596kb,D)", "http://arxiv.org/abs/1608.08940v1", "ASAI 2016, 45JAIIO"]], "COMMENTS": "ASAI 2016, 45JAIIO", "reviews": [], "SUBJECTS": "cs.CL cs.IR cs.LG", "authors": ["luis argerich", "joaqu\\'in torr\\'e zaffaroni", "mat\\'ias j cano"], "accepted": false, "id": "1608.08940"}, "pdf": {"name": "1608.08940.pdf", "metadata": {"source": "CRF", "title": "Hash2Vec: Feature Hashing for Word Embeddings", "authors": ["Luis Argerich", "Matias J. Cano", "Joaquin Torre Zaffaroni"], "emails": ["lrargerich@gmail.com"], "sections": [{"heading": null, "text": "Keywords: feature hashing, natural language processing, word embedding"}, {"heading": "1 Introduction", "text": "The main objective of word embeddings is to form a vector space of words that makes \u201csense\u201d. Usually, this means that semantically similar words are together. One use for these language models is to estimate the probability of an n-gram being correct. [1] suggested that word embeddings can also be used to create reverse-dictionaries, in which one writes the definition of a word and the algorithm suggests a concept that fits the definition. Even more, it is possible to create bilingual reverse dictionaries which can be quite useful in translation tasks. In recent works, several interesting properties of the resulting vector spaces were found [2]. There are many other applications of word embeddings [3] which make the topic a very active area of research in NLP.\nOne way to create word embeddings is by using the bag of words (BOW) model where the word co-occurrence matrix is calculated. In this matrix, each row represents a unique word so that the i,j-element is the amount of times word j has co-occurred with word i. This matrix can become huge in the order of millions by millions, making its use difficult in any application.\nAs a result, modern models (GloVe [3], Word2Vec [4] learn to represent words with a fixed reduced dimensionality.\nar X\niv :1\n60 8.\n08 94\n0v 1\n[ cs\n.C L\n] 3\n1 A\nug 2\n2 A simple technique for dimensionality reduction is Feature Hashing [5], also known as the hashing trick. The idea is to apply a hashing function to each feature of a high dimensional vector to determine a new dimension for the feature in a reduced space. Feature hashing has been used successfully to reduce the dimensionality of the BOW model for texts [5]. [6] used feature hashing to classify mail as spam or ham.\nTo mitigate the effect of collisions in the resulting vectors [7] proposes the use of a second hash function \u03be that determines the sign of a feature.\nIt has been shown that Feature Hashing preserves the inner product between vectors and the error can be bounded. This is explained using the JohnsonLindenstrauss lemma [8] [9] and showing that feature hashing is a particular case of a J-L projection where the projection matrix has exactly one +1 or \u22121 in each row.\nBecause of this, if we apply the hashing trick to the word co-occurrence matrix we are able to obtain an embedding where the inner products between the embedded vectors accurately represent the inner products between the original vectors in the co-occurrence matrix. Our experiments confirm that the distortion between using the full vectors and this embedding is minimal. This means that vectors that are close in the original matrix will also be close in the embedded space.\nInterestingly, embeddings can be constructed without the need of computing the word full-size matrix. This makes the algorithm efficient, as memory consumption is reduced from O(n2) to O(n\u00d7 k), where n is the size of the corpus in words and k is a fixed dimensionality that can be small (in the order of hundreds.)"}, {"heading": "2 Algorithm", "text": "The main formula for the algorithm can be seen in (1). It is a variation of the feature hashing equation shown in [5] with the addition of the second hashing function proposed by [7], as well as the domain-specific part which is the weight function for each co-occurrence.\nw\u0304 (k) j = \u2211 w(c)\u2208Ck;h(w(c))=j \u03be(w(c)) n(k)(w(c))\u2211 i=1 f (k,c) i (1)\nw(k) represents the k-th word, and w\u0304(k) represents the reduced vector for such word and therefore w\u0304j(k) is the j-th component for the k-th word vector. Ck represents all the contexts of the k-th word and h is a hashing function as [5] shows. \u03be is the additional hashing function such that \u03be : String \u2192 {\u22121, 1} proposed by [7]. n(k)(x) represents the amount of times word x has appeared with word k, and finally, fi(k, c) is the aging (or weighting) for the word c according to word k in the i-th time they have appeared together.\nWords are processed from the text in linear fashion and for each new word an embedding vector is created. A window of size k is defined to determine\n3 which words co-occur in the context of another. We iterate through the context applying Feature Hashing to construct the embeddings. A simplified version of the algorithm can be seen in Algorithm 1.\nAlgorithm 1 Hash2Vec\nParameters: n the embedding size, k the context size, h hash function, \u03be hash signfunction and f aging function.\n1: words \u2190 Dictionary() 2: for every word w in text do 3: if w /\u2208 keys(words) then words[w] \u2190 Array(n) 4: for every context word cw with distance d do 5: weight \u2190 f(d) 6: sign \u2190 \u03be(cw) 7: words[w][h(cw)] \u2190 words[w][h(cw)] + sign\u00d7 weight\nIn practice we only need to keep track of the k past words and update both the embedding of the current one and the previous ones. This scheme allows the embeddings to be computed in a streamlined way with O(n) complexity. For simplicity we assume that for each word the embedding is updated based on the words within the k-sized window.\nThe weight function f is a parameter and a deciding factor on the performance of the algorithm. For example, if f(d) = 1 \u2200d, then we are simply calculating a reduced version of the co-occurrence matrix. We obtained better results using f similar to a Gaussian distribution, i.e., f(d) = e\u2212( d \u03c3 ) 2\n. One interesting property of Hash2Vec is that it always constructs the same embeddings for the same training corpus, while word2vec depends on the starting seed and GloVe uses stochastic optimization."}, {"heading": "2.1 Variations on the algorithm", "text": "In order to improve the quality of embeddings, we decided to try some variations on the basic idea and performed various preprocessing tasks before running Hash2Vec.\nWhile running the model with huge corpora there are words that appear frequently but that don\u2019t carry meaning as they only serve a syntactical purpose. To prevent these words from being a source of noise for Hash2Vec, we preprocess the corpus to remove them. We used two criteria to select the words to filter: calculating a certain percentile, avoiding the words above it or using a stoplist of words to be removed. Both methods resulted in an improvement on the similarity tests.\nWe applied the algorithm proposed on [4] to adjoin phrases. This is very important because otherwise \u201cNew York\u201d, \u201cSan Francisco\u201d, etc. would not exist as a single token.\n4 We also obtained modestly better results using homogeneous sentence selection, in which each sentence in an original text moves to a final text according to a uniform probability distribution, instead of simply truncating the text. Our hypothesis is that with better sampling, the context words are less biased to local articles (if using a source like Wikipedia) inherent to the corpus, making the final vector better in the sense that it was trained with many different articles and usages of the word. Moreover, our experiments revealed that this algorithm is sensible to polysemy. [10] explore this idea to construct different representations for the same word."}, {"heading": "2.2 Applications to speed up other embedding algorithms", "text": "The algorithm that we propose constructs a lower-dimensional approximation of the word co-occurrence matrix. The embeddings can then be used directly or can be the starting point for a matrix factorization algorithm, like the one in GloVe. In the case of word2vec, [11] have shown that the SGNS model learns an explicit matrix factorization (EMF) and that very similar results can be obtained by using applying the EMF on the original matrix. The reduced-form matrix that Hash2Vec creates can be used instead of the original to obtain again very similar results without the memory footprint of an O(n2) matrix."}, {"heading": "2.3 Streaming applications and parallelization", "text": "Since the embeddings are refined as text is processed, the algorithm is practical for streaming applications. The model can also adapt to changes in language, e.g. new words being used. Thus the embeddings can constantly be updated while being always usable, which is something GloVe cannot do.\nBy nature Hash2Vec is easily parallelizable since the embeddings can be updated from different portions of text in parallel and then just added up to construct the final vectors. This associative property makes it trivial for the algorithm to be ported to a model like MapReduce. The algorithm could be run on Apache Spark, where each node processes a fragment of text and a single final reduce operation can output the final embeddings. Even more, if no regularization function is applied on the final vectors, embeddings trained in different contexts can be combined by adding the individual vectors of each word in both corpora."}, {"heading": "3 Results", "text": "For benchmarking we compared our results with two commonly used datasets. The first is wordsim353 [12] and the second is from Amazon\u2019s Mechanical Turk, as used in [13]. Both datasets hold word pairs with similarity scores for each, representing human assigned similarity judgements. We then calculate the Spearman correlation between our results and the dataset\u2019s to obtain a metric on the\n5\nquality of the embeddings. The similarity measure we used is cosine similarity, like the one used in [3].\nBoth graphs in Figure 1 show that Hash2Vec approximates the full vectors when the vector dimension increases. A very good approximation can be obtained with dimensions in the order of the hundreds, orders of magnitude smaller than full vectors. As expected, the co-ocurrence matrix appears to be a theoretical limit on the performance of Hash2Vec."}, {"heading": "3.1 Comparison with GloVe", "text": "We trained GloVe using the same corpus with k = 15 and n = 600 in both cases. Table 1 shows the 5 most similar words returned by GloVe and Hash2Vec for some queries.\n6 We observe the results of GloVe and Hash2Vec to be very similar. Both models capture the semantic meaning of words and in some cases Hash2Vec seems to outperform Glove."}, {"heading": "3.2 Embeddings visualization", "text": "To understand how the algorithm behaves and to further observe the resulting vector space, we applied t-SNE [14], a widely used dimensionality reduction algorithm.\nAfter training Hash2Vec on a 110-million-word corpus (k = 5 and n = 600), we applied t-SNE. Given that the algorithm groups together similar words, we should be able to see it in the graph. Since t-SNE is about O(n2) in memory, we only used the 15, 000 most common words. Zipf\u2019s law gives some guarantees to only keeping the most common words.\nIn Figure 2 we show an interesting sector extracted from the reduced matrix cartesian plot."}, {"heading": "3.3 The myth behind word analogies and Hash2Vec for word analogies", "text": "Word analogies have been used to show the expressiveness power of a word embedding model [2]. GloVe and Word2Vec produce embeddings where a linear relationship exists between analog words. The most popular example is the analogy \u201cprince is to princess like king is to . . . \u201d where the model should show\n7 that prince-princess \u2261 king-queen. In [3] the authors claim that models that capture this linearity have a superior understanding to others. In contrast, [16] show that the word analogy task is just a derivation of similarity. When asking the model \u201cx is to y like z is to w\u201d we are actually maximizing the dot product of w(x + y \u2212 z) which is the same as maximizing wx + wy \u2212 wz so the word analogy task is actually asking the model about words that are similar to x and y but different to z.\nWord embeddings like GloVe or Word2Vec are not capable of capturing words that are different to a given word so they usually fail to find antonyms, this means that the word analogy task is strongly dependent on the words that are similar to the positive words in the query.\nTable 2 shows how Hash2Vec is able to solve some word analogy tasks. The vectors are not trained to preserve linearity in the word analogy task, the original matrix captures some of these properties and therefore Hash2Vec can use them. The case of \u201cCow is to Milk like Pig is to Meat\u201d is not about the model being \u201cclever\u201d or capturing a magic linearity but just a case of \u201cMeat\u201d being used in similar contexts as \u201cCow\u201d and \u201cMilk\u201d.\nWhat the embedding models really capture is then not exactly a linearity. Good embeddings are capable of capturing different levels of similarity simultaneously. We have shown examples of semantic similarity, but there are other similarities captured by the model like \u201cdiction\u201d similarity where words that are spelled similarly are close together in the embeddings."}, {"heading": "4 Conclusions", "text": "In this work we detailed a very simple algorithm that is able to construct word embeddings in linear time. The algorithm does not require training and has minimal memory footprint. We showed the results of the algorithm to be comparable to GloVe [3] in the word similarity and analogy tasks, which is one of the state of the art algorithms for word embeddings. The algorithm can also be used to approximate the word co-occurrence matrix to train models like GloVe or EMF with minimal memory consumption.\nThe results show that feature hashing is a very powerful technique when applied to text as it can reduce the dimensionality of word vectors from millions to only hundreds while capturing the semantic of each token in the vocabulary.\n8 Hash2Vec can be used as a very fast way to construct word embeddings with minimal code for streams or dynamic corpora and then use these embeddings in NLP applications."}], "references": [{"title": "Linguistic Regularities in Continuous Space Word Representations", "author": ["T. Mikolov", "W.T. Yih", "G. Zweig"], "venue": "In HLT-NAACL, 746\u2013751", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2013}, {"title": "Glove: Global Vectors for Word Representation", "author": ["J. Pennington", "R. Socher", "C.D. Manning"], "venue": "In EMNLP,Vol. 14, 1532\u20131543", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2014}, {"title": "Hash kernels", "author": ["Q. Shi", "J. Petterson", "G. Dror", "J. Langford", "A.L. Strehl", "A.J. Smola", "S.V.N. Vishwanathan"], "venue": "In International Conference on Artificial Intelligence and Statistics, 496\u2013503", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2009}, {"title": "Collaborative Email-Spam Filtering with the Hashing Trick", "author": ["J. Attenberg", "K. Weinberger", "A. Dasgupta", "A. Smola", "M. Zinkevich"], "venue": "In Proceedings of the Sixth Conference on Email and Anti-Spam", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2009}, {"title": "Feature hashing for large scale multitask learning", "author": ["K. Weinberger", "A. Dasgupta", "J. Langford", "A. Smola", "J. Attenberg"], "venue": "In Proceedings of the 26th Annual International Conference on Machine Learning, 1113\u20131120", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2009}, {"title": "Extensions of Lipschitz mappings into a Hilbert space", "author": ["W.B. Johnson", "J. Lindenstrauss"], "venue": "Contemporary mathematics, 26,1, 189\u2013206", "citeRegEx": "8", "shortCiteRegEx": null, "year": 1984}, {"title": "Database-friendly random projections: Johnson-Lindenstrauss with binary coins", "author": ["D. Achlioptas"], "venue": "Journal of computer and System Sciences, 66, 4, 671\u2013687", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2003}, {"title": "Improving word representations via global context and multiple word prototypes", "author": ["E.H. Huang", "R. Socher", "C.D. Manning", "A.Y. Ng"], "venue": "In Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics: Long PapersVolume 1, 873\u2013882", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2012}, {"title": "Word embedding revisited: A new representation learning and explicit matrix factorization perspective", "author": ["Y. Li", "L. Xu", "F. Tian", "L. Jiang", "X. Zhong", "E. Chen"], "venue": "In Proceedings of the 24th International Joint Conference on Artificial Intelligence, 3650\u20133656", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2015}, {"title": "Placing search in context: The concept revisited", "author": ["L. Finkelstein", "E. Gabrilovich", "Y. Matias", "E. Rivlin", "Z. Solan", "G. Wolfman", "E. Ruppin"], "venue": "In Proceedings of the 10th international conference on World Wide Web, 406\u2013414", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2001}, {"title": "A word at a time: computing word relatedness using temporal semantic analysis", "author": ["K. Radinsky", "E. Agichtein", "E. Gabrilovich", "S. Markovitch"], "venue": "In Proceedings of the 20th international conference on World wide web, 337\u2013346", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2011}, {"title": "Visualizing data using t-SNE", "author": ["L. Van der Maaten", "G. Hinton"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2008}, {"title": "Another stemmer", "author": ["D.P. Chris"], "venue": "In ACM SIGIR Forum, 24, 3, 56\u201361", "citeRegEx": "15", "shortCiteRegEx": null, "year": 1990}, {"title": "Linguistic Regularities in Sparse and Explicit Word Representations", "author": ["O. Levy", "Y. Goldberg", "I. Ramat-Gan"], "venue": "In CoNLL, 171\u2013180", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2014}], "referenceMentions": [{"referenceID": 0, "context": "In recent works, several interesting properties of the resulting vector spaces were found [2].", "startOffset": 90, "endOffset": 93}, {"referenceID": 1, "context": "There are many other applications of word embeddings [3] which make the topic a very active area of research in NLP.", "startOffset": 53, "endOffset": 56}, {"referenceID": 1, "context": "As a result, modern models (GloVe [3], Word2Vec [4] learn to represent words with a fixed reduced dimensionality.", "startOffset": 34, "endOffset": 37}, {"referenceID": 2, "context": "A simple technique for dimensionality reduction is Feature Hashing [5], also known as the hashing trick.", "startOffset": 67, "endOffset": 70}, {"referenceID": 2, "context": "Feature hashing has been used successfully to reduce the dimensionality of the BOW model for texts [5].", "startOffset": 99, "endOffset": 102}, {"referenceID": 3, "context": "[6] used feature hashing to classify mail as spam or ham.", "startOffset": 0, "endOffset": 3}, {"referenceID": 4, "context": "To mitigate the effect of collisions in the resulting vectors [7] proposes the use of a second hash function \u03be that determines the sign of a feature.", "startOffset": 62, "endOffset": 65}, {"referenceID": 5, "context": "This is explained using the JohnsonLindenstrauss lemma [8] [9] and showing that feature hashing is a particular case of a J-L projection where the projection matrix has exactly one +1 or \u22121 in each row.", "startOffset": 55, "endOffset": 58}, {"referenceID": 6, "context": "This is explained using the JohnsonLindenstrauss lemma [8] [9] and showing that feature hashing is a particular case of a J-L projection where the projection matrix has exactly one +1 or \u22121 in each row.", "startOffset": 59, "endOffset": 62}, {"referenceID": 2, "context": "It is a variation of the feature hashing equation shown in [5] with the addition of the second hashing function proposed by [7], as well as the domain-specific part which is the weight function for each co-occurrence.", "startOffset": 59, "endOffset": 62}, {"referenceID": 4, "context": "It is a variation of the feature hashing equation shown in [5] with the addition of the second hashing function proposed by [7], as well as the domain-specific part which is the weight function for each co-occurrence.", "startOffset": 124, "endOffset": 127}, {"referenceID": 2, "context": "Ck represents all the contexts of the k-th word and h is a hashing function as [5] shows.", "startOffset": 79, "endOffset": 82}, {"referenceID": 4, "context": "\u03be is the additional hashing function such that \u03be : String \u2192 {\u22121, 1} proposed by [7].", "startOffset": 80, "endOffset": 83}, {"referenceID": 7, "context": "[10] explore this idea to construct different representations for the same word.", "startOffset": 0, "endOffset": 4}, {"referenceID": 8, "context": "In the case of word2vec, [11] have shown that the SGNS model learns an explicit matrix factorization (EMF) and that very similar results can be obtained by using applying the EMF on the original matrix.", "startOffset": 25, "endOffset": 29}, {"referenceID": 9, "context": "The first is wordsim353 [12] and the second is from Amazon\u2019s Mechanical Turk, as used in [13].", "startOffset": 24, "endOffset": 28}, {"referenceID": 10, "context": "The first is wordsim353 [12] and the second is from Amazon\u2019s Mechanical Turk, as used in [13].", "startOffset": 89, "endOffset": 93}, {"referenceID": 1, "context": "The similarity measure we used is cosine similarity, like the one used in [3].", "startOffset": 74, "endOffset": 77}, {"referenceID": 11, "context": "To understand how the algorithm behaves and to further observe the resulting vector space, we applied t-SNE [14], a widely used dimensionality reduction algorithm.", "startOffset": 108, "endOffset": 112}, {"referenceID": 0, "context": "Word analogies have been used to show the expressiveness power of a word embedding model [2].", "startOffset": 89, "endOffset": 92}, {"referenceID": 1, "context": "In [3] the authors claim that models that capture this linearity have a superior understanding to others.", "startOffset": 3, "endOffset": 6}, {"referenceID": 13, "context": "In contrast, [16] show that the word analogy task is just a derivation of similarity.", "startOffset": 13, "endOffset": 17}, {"referenceID": 1, "context": "We showed the results of the algorithm to be comparable to GloVe [3] in the word similarity and analogy tasks, which is one of the state of the art algorithms for word embeddings.", "startOffset": 65, "endOffset": 68}], "year": 2016, "abstractText": "In this paper we propose the application of feature hashing to create word embeddings for natural language processing. Feature hashing has been used successfully to create document vectors in related tasks like document classification. In this work we show that feature hashing can be applied to obtain word embeddings in linear time with the size of the data. The results show that this algorithm, that does not need training, is able to capture the semantic meaning of words. We compare the results against GloVe showing that they are similar. As far as we know this is the first application of feature hashing to the word embeddings problem and the results indicate this is a scalable technique with practical results for NLP applications.", "creator": "LaTeX with hyperref package"}}}