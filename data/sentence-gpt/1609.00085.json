{"id": "1609.00085", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "1-Sep-2016", "title": "A Novel Progressive Learning Technique for Multi-class Classification", "abstract": "In this paper, a progressive learning technique for multi-class classification is proposed. This newly developed learning technique is independent of the number of class constraints and it can learn new classes while still retaining the knowledge of previous classes. Whenever a new class (non-native to the knowledge learnt thus far) is encountered, the neural network structure gets remodeled automatically by facilitating new neurons and interconnections, and the parameters are calculated in such a way that it retains the knowledge learnt thus far, thus preserving the learning learned as a whole. In this paper, we describe how to train a neural network with a single classifier: A-1.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n", "histories": [["v1", "Thu, 1 Sep 2016 01:50:18 GMT  (1032kb)", "http://arxiv.org/abs/1609.00085v1", "23 pages, 13 tables, 11 figures"], ["v2", "Sun, 22 Jan 2017 09:52:06 GMT  (1342kb)", "http://arxiv.org/abs/1609.00085v2", "23 pages, 13 tables, 11 figures"]], "COMMENTS": "23 pages, 13 tables, 11 figures", "reviews": [], "SUBJECTS": "cs.LG cs.AI cs.NE", "authors": ["rajasekar venkatesan", "meng joo er"], "accepted": false, "id": "1609.00085"}, "pdf": {"name": "1609.00085.pdf", "metadata": {"source": "CRF", "title": "A Novel Progressive Learning Technique for Multi-class Classification", "authors": ["Rajasekar Venkatesan", "Meng Joo Er"], "emails": ["RAJA0046@e.ntu.edu.sg", "EMJER@ntu.edu.sg"], "sections": [{"heading": null, "text": "learning technique is independent of the number of class constraints and it can learn new classes while still retaining the knowledge of previous classes. Whenever a new class (non-native to the knowledge learnt thus far) is encountered, the neural network structure gets remodeled automatically by facilitating new neurons and interconnections, and the parameters are calculated in such a way that it retains the knowledge learnt thus far. This technique is suitable for realworld applications where the number of classes is often unknown and online learning from real-time data is required. The consistency and the complexity of the progressive learning technique are analyzed. Several standard datasets are used to evaluate the performance of the developed technique. A comparative study shows that the developed technique is superior.\nKey Words\u2014Classification, machine learning, multi-class, sequential learning, progressive learning."}, {"heading": "1. INTRODUCTION", "text": "HE study on feedforward neural network (FNN) has gained prominence since the advent of back propagation (BP) algorithm [1]. Several improved and optimized variants of the BP algorithm have then been developed and analyzed [2-7]. In the past two decades, single hidden layer feedforward neural networks (SLFNs) has\ngained significant importance due to its widespread applications in recognition, classification and function approximation area [8-12]. Several learning techniques have been proposed since then for effective training of the SLFN [13-14]. The learning techniques can be grouped under two basic categories; Batch learning and Sequential learning [15].\nBatch learning algorithms require pre collection of training data. The collected data set is then used for training the neural network. The network parameters are calculated and updated by processing all the training data together. There are several batch learning algorithms in the literature. One of the relatively new batch learning scheme called Extreme Learning Machines (ELM) is proposed by Huang et al in 2004 [16]. The special nature of ELM is that the input weights and the hidden node biases can be chosen at random [17]. A key feature of ELM is that it maintains the universal approximation capability of SLFN [17-19]. It has gained much attention to it and several research works are further made in it due to its special nature of random input weight initialization and its unique advantage of extreme learning speed [20]. The advantages of ELM over other traditional feedforward neural network are analyzed in the literature [9,21]. Many new variants and developments are made to the ELM and significant results are achieved in the approximation, classification and regression areas [22-24]. Batch learning involves processing of the complete data set concurrently for updating the weights. This technique is limited to its applications as batch learning techniques are more time consuming and requires the complete data set prior to training. On the other hand, in online/sequential learning algorithms, the network parameters are updated as and when a new training data arrives. To overcome the shortcomings of the batch learning techniques, several sequential and/or online learning algorithms are developed [25- 28].\nT\nIn many cases, sequential learning algorithms are preferred over batch learning algorithms as they do not require retraining whenever a new data sample is received [8]. Online-sequential learning method that combines ELM and Recursive Least Square (RLS) algorithm is later developed and is called Online-Sequential extreme learning machine (OS-ELM) [15]. Several variants of ELM and OS-ELM were developed and proposed in the literature [8,9,17,22,23,29].\nThe issue with existing multi-class classification techniques such as ELM and SVM is that, once they are trained to classify a specific number of classes, learning of new classes is not possible. In order to learn new class of data it requires retraining all the classes anew again.\nThe existing techniques require a priori information on the number of classes that will be present in the training dataset. The information on the number of classes is required to be either specified directly or is identified by analyzing the complete training data set. Based on this parameter, the network model will be designed and only the parameters or the weights of the networks are updated depending on the sequential input data. This makes the existing techniques \u201cstatic\u201d with respect to the number of classes it can learn.\nWhile the existing techniques are suited for applications with pre-known dataset, it might not be well suited for applications such as cognitive robotics or those involving real-time data where the nature of training data is unknown. For such real-world and real-time data, where the number of classes to be learnt is often unknown, the learning technique must be self-developing to meet the dynamic needs. To overcome this shortcoming, a novel learning paradigm is proposed, called the \u201cprogressive learning\u201d.\nProgressive learning is the next stage of advancement to the online learning methods. Existing online sequential techniques only learn to classify data among a fixed set of classes which are initialized during the initialization phase of the algorithm. They fail to dynamically adapt when introduced to new class/classes on the run. The progressive learning technique is independent of the number of class constraint and it can learn several new classes on the go by retaining the knowledge of previous classes. This is achieved by modifying the network structure by itself upon encountering a new class and updating the network parameters in such a way that it learns the new class and retains the knowledge learnt thus far.\nThe existing online sequential learning methods do not require retraining when a \u201cnew data sample\u201d is received. But it fails when a \u201cnew class of data\u201d which is unknown to the existing knowledge is encountered. Progressive learning technique overcomes this shortcoming by allowing the network to learn multiple new classes\u2019 alien to existing knowledge, encountered at any point of time.\n2. PRELIMINARIES\nThis section gives a brief review of the ELM and the OS-ELM techniques to provide basic background information."}, {"heading": "2.1 Extreme Learning Machines", "text": "A condensed overview of the batch learning ELM technique as proposed by Huang et. al. [16] is given below.\nConsider there are N training samples represented as {(xj,tj)} where j varies from 1 to N, xj denotes the input data vector: xj = [xj1,xj2,\u2026xjn]T \u03f5 Rn and tj = [tj1,tj2,\u2026,tjm]T \u03f5 Rm denotes the target class labels. Let there be P number of hidden layer neurons in the network, the output of the standard SLFN can be given as\n\u2211 \ud835\udf37\ud835\udc8a\ud835\udc54\ud835\udc56(\ud835\udc99\ud835\udc8b) = \u2211 \ud835\udf37\ud835\udc8a\ud835\udc54(\ud835\udc98\ud835\udc8a . \ud835\udc99\ud835\udc8b + \ud835\udc4f\ud835\udc56) = \ud835\udc90\ud835\udc8b\n\ud835\udc43\n\ud835\udc56=1\n\ud835\udc43\n\ud835\udc56=1\n(1)\nwhere, j = 1, 2\u2026.N, wi = [wi1,wi2,\u2026win]T denotes the weight vector from input nodes to ith hidden node, \u03b2i = [\u03b2i1, \u03b2i2,\u2026 \u03b2im]T denotes the weight vector connecting ith hidden node to the output nodes and bi is the hidden layer bias value.\nFor the standard SLFN mentioned in the equation above to perform as a classifier, the output of the network should\nbe equal to the corresponding target class of the input data given to the classifier. Hence, for the SLFN in equation 1 to be a classifier, there exist a \u03b2i, g(x), wi and bi such that\n\u2211\u2016\ud835\udc90\ud835\udc8b \u2212 \ud835\udc95\ud835\udc8b\u2016\n\ud835\udc43\n\ud835\udc57=1\n= 0 (2)\nTherefore, the equation for the output of the network can be written as,\n\u2211 \ud835\udf37\ud835\udc8a\ud835\udc54(\ud835\udc98\ud835\udc8a . \ud835\udc99\ud835\udc8b + \ud835\udc4f\ud835\udc56) = \ud835\udc95\ud835\udc8b\n\ud835\udc43\n\ud835\udc56=1\n(3)\nwhere j = 1,2,\u2026N, and tj denotes the target class corresponding to the input data vector xj. This equation can be\nwritten in compact form as\nH\u03b2 = T (4)\nWhere\nH(w1,\u2026wP,b1,\u2026,bP,x1,\u2026.,xN)= [ \ud835\udc54(\ud835\udc641 . \ud835\udc651 + \ud835\udc4f1) \u22ef \ud835\udc54(\ud835\udc64\ud835\udc43 . \ud835\udc651 + \ud835\udc4f\ud835\udc43)\n\u22ee \u22f1 \u22ee \ud835\udc54(\ud835\udc641 . \ud835\udc65\ud835\udc41 + \ud835\udc4f1) \u22ef \ud835\udc54(\ud835\udc64\ud835\udc43 . \ud835\udc65\ud835\udc41 + \ud835\udc4f\ud835\udc43) ]\n\ud835\udc41\ud835\udc4b\ud835\udc43\n(5)\n\ud835\udf37 = [ \ud835\udefd1\n\ud835\udc47\n\u22ee \ud835\udefd\ud835\udc43 \ud835\udc47 ]\n\ud835\udc43\ud835\udc4b\ud835\udc5a\n(6)\n\ud835\udc7b = [ \ud835\udc611\n\ud835\udc47\n\u22ee \ud835\udc61\ud835\udc41 \ud835\udc47 ]\n\ud835\udc41\ud835\udc4b\ud835\udc5a\n(7)\nH is called the hidden layer output matrix of the neural network where each column of H gives corresponding output of the hidden layers for a given input xi. The mathematical framework and the training process are extensively described in the literature [9]. The key results are restated.\nLemma 1: [9] Given a standard SLFN with N hidden nodes and activation function g: R \u2192 R which is infinitely differentiable in any interval, for N arbitrary distinct samples (xi,ti), where xi \u03f5 Rn and ti \u03f5 Rm, for any wi and bi randomly chosen from any intervals of Rn and R, respectively, according to any continuous probability distribution, then with probability one, the hidden layer output matrix H of the SLFN is invertible and ||H\u03b2 \u2013 T|| = 0.\nLemma 2: [9] Given any small positive value \u03b5 > 0 and activation function g: R \u2192 R which is infinitely differentiable in any interval, there exists P \u2264 N such that for N arbitrary distinct samples (xi,ti), where xi \u03f5 Rn and ti \u03f5 Rm, for any wi and bi randomly chosen from any intervals of Rn and R, respectively, according to any continuous probability distribution, then with probability one, ||HNxP \u03b2PXm \u2013 TNXm|| < \u03b5.\nThus it can be seen that for an ELM, the input weights wi, and the hidden layer neuron bias bi can be randomly\nassigned. Training of the ELM involves estimating the output weights \u03b2 such that the relation H\u03b2 = T is true.\nThe output weight \u03b2 for the ELM can be estimated using the Moore-Penrose generalized inverse as \u03b2 = H+T, where\nH+ is the Moore-Penrose inverse of the hidden layer output matrix H.\nThe overall batch learning algorithm of the ELM for training set of form {(xi,ti)|xi \u03f5 Rn, ti \u03f5 Rm, i = 1,\u2026N} with P\nhidden layer neurons can be summarized as,\nSTEP 1: Random assignment of input weights wi and hidden layer bias bi, i = 1,\u2026.P.\nSTEP 2: Computation of the hidden layer output matrix H.\nSTEP 3: Estimation of output weights using \u03b2 = H+T where H+ is the Moore-Penrose inverse of H and T = [t1,\u2026tN]T."}, {"heading": "2.2 Online Sequential \u2013 Extreme Learning Machine", "text": "Based on the batch learning method of the ELM, sequential modification is performed and Online Sequential-ELM\n(OS-ELM) is proposed in literature [15]. OS-ELM operates on online data.\nIn the batch learning method ELM the output weight \u03b2 is estimated using the formula\n\u03b2 = H+T, where H+ is the Moore-Penrose inverse of the hidden layer output matrix H. The H+ can be written as,\nH+ = (HTH)-1HT (8)\nAs stated in [15], this solution gives the least square solution to H\u03b2 = T. The OS-ELM uses RLS algorithm to update the output weight matrix sequentially as the data arrives online. It has been well studied in the literature and the summary is given below.\nLet N0 be the number of samples in the initial block of data that is provided to the network.\nCalculate M0 = (H0TH0)-1 and \u03b20 = M0H0TT0.\nFor each of the subsequent sequentially arriving data, the output weights can be updated as\n\ud835\udc74\ud835\udc8c+\ud835\udfcf = \ud835\udc74\ud835\udc8c \u2212\n\ud835\udc74\ud835\udc8c\ud835\udc89\ud835\udc8c+\ud835\udfcf\ud835\udc89\ud835\udc8c+\ud835\udfcf \ud835\udc7b \ud835\udc74\ud835\udc8c\n\ud835\udfcf + \ud835\udc89\ud835\udc8c+\ud835\udfcf \ud835\udc7b \ud835\udc74\ud835\udc8c\ud835\udc89\ud835\udc8c+\ud835\udfcf\n(9)\n\ud835\udf37\ud835\udc8c+\ud835\udfcf = \ud835\udf37\ud835\udc8c + \ud835\udc74\ud835\udc8c+\ud835\udfcf\ud835\udc89\ud835\udc8c+\ud835\udfcf(\ud835\udc95\ud835\udc8c+\ud835\udfcf \ud835\udc7b \u2212 \ud835\udc89\ud835\udc8c+\ud835\udfcf \ud835\udc7b \ud835\udf37\ud835\udc8c) (10)\nWhere k = 0,1,2\u2026. N-N0-1.\nThe steps in the Online-Sequential ELM based on the RLS algorithm are summarized below.\nINITIALIZATION PHASE\nSTEP 1: The input weights and the hidden layer bias are assigned in random.\nSTEP 2: For the initial block of N0 samples of data, the hidden layer output matrix H0 is calculated.\nH0 = [h1,\u2026.hP]T, where hi = [g(w1.xi+b1),\u2026.g(wP.xi+bP)]T, i = 1,2\u2026N0 (11)\nSTEP 3: From the value of H0, the initial values of M0 and \u03b20 are estimated as\nM0 = (H0TH0)-1 (12)\n\u03b20 = M0H0TT0 (13)\nSEQUENTIAL LEARNING PHASE\nSTEP 4: For each of the subsequent sequentially arriving data, the hidden layer output vector hk+1 is calculated.\nSTEP 5: The output weight is updated based on the RLS algorithm as,\n\ud835\udc74\ud835\udc8c+\ud835\udfcf = \ud835\udc74\ud835\udc8c \u2212 \ud835\udc74\ud835\udc8c\ud835\udc89\ud835\udc8c+\ud835\udfcf \ud835\udc7b (\ud835\udc70 + \ud835\udc89\ud835\udc8c+\ud835\udfcf\ud835\udc74\ud835\udc8c\ud835\udc89\ud835\udc8c+\ud835\udfcf\n\ud835\udc7b ) \u2212\ud835\udfcf \ud835\udc89\ud835\udc8c+\ud835\udfcf\ud835\udc74\ud835\udc8c (14)\n\ud835\udf37\ud835\udc8c+\ud835\udfcf = \ud835\udf37\ud835\udc8c + \ud835\udc74\ud835\udc8c+\ud835\udfcf\ud835\udc89\ud835\udc8c+\ud835\udfcf \ud835\udc7b (\ud835\udc95\ud835\udc8c+\ud835\udfcf \u2212 \ud835\udc89\ud835\udc8c+\ud835\udfcf\ud835\udf37\ud835\udc8c) (15)\nThe theory and the formulation behind the operation of the OS-ELM and ELM have been discussed in detail in\nseveral papers [8,9,13,15,30]. The standard variants of activation function used in ELM [31] and other special mapping functions and their variants are discussed in detail in the literature [32-35]. The other variants of ELM includes ELM Kernel [36], ELM for imbalanced data [37], ELM for noisy data [38], Incremental ELM [19], ELM ensemble [39-41] and many other variants are summarized in [31]"}, {"heading": "3. PROGRESSIVE LEARNING TECHNIQUE", "text": ""}, {"heading": "3.1 Learning like children", "text": "The proposed progressive learning algorithm is adapted from the natural learning process exhibited by the children. Peter Jarvis in his book [42] has described in detail the nature of the human learning process. As opposed to traditional machine learning algorithm\u2019s training-testing cycle, human learning is a continuous process. The learning / training phase is never ending. Whenever human brain is stumbled upon with a new phenomenon, the learning resumes [42]. The key feature of human learning is that, the learning of new phenomenon does not affect the knowledge learnt. The new knowledge is leant and is added along with existing knowledge.\nThough there are several online and sequential learning methods, the information of number of classes is fixed during initialization. This restricts the possibility of learning newer classes on the run. Existing machine learning algorithms fails to resume learning when an entirely new class / classes of data are encountered after the initialization. For applications such as cognitive robotics, real-world learning, etc. the system should be robust and dynamic to learn new classes on the run. The number of classes it encounters is not known beforehand. The system should be able to redesign itself and adapt to meet the learning of the new class as it arrives.\nThe proposed learning method introduces a novel technique of progressive learning which showcases continuous learning. The progressive learning technique enables to learn new classes dynamically on the run. Whenever a new class is encountered, the neural network \u201cgrows\u201d and redesign its interconnections and weights so as to incorporate the learning of the new classification. Another key feature of the proposed method is that the newer classes are learnt in addition to the existing knowledge as if they were present from the beginning."}, {"heading": "3.2 Proposed Algorithm", "text": "As foreshadowed, the key objective of the progressive learning technique is that it can dynamically learn new classes on the run. Suppose the network is initially trained to classify \u2018m\u2019 number of classes. Consider the network encounters \u2018c\u2019 number of new classes which are alien to the previously learnt class, the Progressive Learning Technique (PLT) will adapt automatically and starts to learn the new class by maintaining the knowledge of previously learnt classes.\nThe introduction of new class(es) to the network, results in changes in the dimension of the output vector and the output weight matrix. Also the newly formed matrices with increased dimension should be evaluated in such a way that it still retains the knowledge learnt thus far and also facilitates the learning of the newly introduced class(es). The method of increasing the dimension of the matrix, the weight update and matrix recalibration methods of the proposed algorithm are significantly different from the class-incremental extreme learning machine [43]. The proposed algorithm can not only learn sequential introduction of single new class, but also simultaneous (multiple new classes in same block of the online data) and sequential introduction of multiple new classes. The proposed algorithm is also independent of the time of introduction of the new class(es).\nConsider there are P hidden layer neurons, and the training data is of the form (xi, ti), the steps of the PLT algorithm\nare:\nINITIALIZATION PHASE\nSTEP 1: The input weights and the hidden layer bias are assigned at random.\nSTEP 2: For the initial block of N0 samples of data, the hidden layer output matrix H0 is calculated.\nH0 = [h1,\u2026.hP]T, where hi=[g(w1.xi+b1),\u2026.g(wP.xi+bP)]T, i = 1,2\u2026N0 (16)\nSTEP 3: From the value of H0, the initial values of M0 and \u03b20 are estimated as\nM0 = (H0TH0)-1 (17)\n\u03b20 = M0H0TT0 (18)\nSEQUENTIAL LEARNING PHASE\nThe subsequent data that arrives to the network can be trained either on one-by-one or chunk-by-chunk basis. Let\n\u2018b\u2019 be the chunk size. Unity value for b results in training the network on one-by-one basis.\nWhen a new data sample/chunk of data is arrived, it can fall into either of the two categories.\ni) Absence of new class of data\nii) Presence of new class / classes of data\nIf there are no new classes in the current set of data, the PLT is similar to OS-ELM and the usual process of calculating and updating the output weights is performed. The subsequent algorithm steps for the case of no new classes in current chunk of data are as follows.\nSTEP 4: The hidden layer output vector hk+1 is calculated.\nSTEP 5: The output weight is updated based on the RLS algorithm as,\n\ud835\udc74\ud835\udc8c+\ud835\udfcf = \ud835\udc74\ud835\udc8c \u2212 \ud835\udc74\ud835\udc8c\ud835\udc89\ud835\udc8c+\ud835\udfcf \ud835\udc7b (\ud835\udc70 + \ud835\udc89\ud835\udc8c+\ud835\udfcf\ud835\udc74\ud835\udc8c\ud835\udc89\ud835\udc8c+\ud835\udfcf\n\ud835\udc7b ) \u2212\ud835\udfcf \ud835\udc89\ud835\udc8c+\ud835\udfcf\ud835\udc74\ud835\udc8c (19)\n\ud835\udf37\ud835\udc8c+\ud835\udfcf = \ud835\udf37\ud835\udc8c + \ud835\udc74\ud835\udc8c+\ud835\udfcf\ud835\udc89\ud835\udc8c+\ud835\udfcf \ud835\udc7b (\ud835\udc95\ud835\udc8c+\ud835\udfcf \u2212 \ud835\udc89\ud835\udc8c+\ud835\udfcf\ud835\udf37\ud835\udc8c) (20)\nIf there is a new class(es) in the chunk of data arrived, a novel progressive learning technique is used to recalibrate\nthe network to accommodate new class by retaining old knowledge.\nThe algorithm maintains the classes learnt thus far in a separate set. When a new data sample/block of data arrives, the data are analyzed for the class it belongs to. If the target class of new data block is equal to or a subset of existing classes, no new classification has been encountered. When the new data block\u2019s target class set is not a subset of existing classes, it means that the system has encountered new classification and a special recalibrate routine is initiated.\nIn the recalibration routine, the number of new classes encountered is determined and class labels are identified. Let \u2018c\u2019 be the number of new classes encountered. Upon identifying the number of new classes introduced, the set containing the classes learnt thus far is updated accordingly.\nThe neural network is redesigned with the number of output neurons increased accordingly and the interconnections redone. The weights of the new network are determined from the current and the previous weights of the old network. The weight update is made such that the knowledge learnt by the old network is retained and the knowledge of new classes is included along with it.\nConsider there are P hidden layer neurons in the network, m classes of data are currently learnt by the network and b be the chunk size of the sequential learning. The introduction of \u2018c\u2019 new classes at any instant k+1, will modify the dimensions of the output weight matrix \u03b2 from \u03b2PXm to \u03b2PXm+c.\nThe output weight matrix \u03b2 is of critical importance in ELM based networks. Since the input weights and the hidden layer bias are randomly assigned, the values in the \u03b2 matrix control the number of classes learnt and the accuracy of each class. The algorithm steps are continued as follows.\nSTEP 4: The values of \u03b2PXm+c are calculated based on the current values of \u03b2PXm, (hk)bXP and (Mk)PXP.\nThe current \u03b2 matrix is of the dimension (\ud835\udf37\ud835\udc8c)\ud835\udc77\ud835\udc7f\ud835\udc8e and \u2018c\u2019 new classes are introduced. Therefore, to accommodate the output weight matrix for the increased number of output layer neurons, the \u03b2 matrix is transformed to \ud835\udf37\ud835\u0303\udc8c as given in equation.\n\ud835\udf37\ud835\u0303\udc8c = (\ud835\udf37\ud835\udc8c)\ud835\udc77\ud835\udc7f\ud835\udc8e \ud835\udc70\ud835\udc8e\ud835\udc7f\ud835\udc8e+\ud835\udc84 (21)\nWhere \ud835\udc70\ud835\udc8e\ud835\udc7f\ud835\udc8e+\ud835\udc84 is a rectangular identity matrix of dimension m X m+c.\n\ud835\udf37\ud835\u0303\udc8c\ud835\udc43\ud835\udc4b\ud835\udc5a+\ud835\udc50 = (\ud835\udf37\ud835\udc8c)\ud835\udc43\ud835\udc4b\ud835\udc5a [ 1 0 \u2026 0 1 \u2026 0 0 0 0 \u2026 \u2026 0 0 0 0 ]\n\ud835\udc5a\ud835\udc4b\ud835\udc5a+\ud835\udc50\n(22)\n\ud835\udf37\ud835\u0303\udc8c\ud835\udc43\ud835\udc4b\ud835\udc5a+\ud835\udc50 = [(\ud835\udf37\ud835\udc8c)\ud835\udc43\ud835\udc4b\ud835\udc5a \ud835\udc76\ud835\udc43\ud835\udc4b\ud835\udc50]\ud835\udc43\ud835\udc4b\ud835\udc5a+\ud835\udc50 (23)\nWhere \ud835\udc76\ud835\udc43\ud835\udc4b\ud835\udc50 is zero matrix.\nUpon extending the weight matrix to accommodate the increased number of output neurons, the learning learnt thus far has to be incorporated in the newly upgraded weight matrix. Appending zero matrix is a trivial way to increase the dimensions. The matrix values have to be updated such that the network retains the knowledge of existing classes and can learn new classes as if they were available from the beginning of the training phase.\nFrom equation 18, it can be seen that, the error difference between the target class \ud835\udc95\ud835\udc8c+\ud835\udfcf and the predicted class \ud835\udc89\ud835\udc8c+\ud835\udfcf\ud835\udf37\ud835\udc8c is scaled by a learning factor and is added to \ud835\udf37\ud835\udc8c. Since \u2018c\u2019 new classes are introduced only at the k+1th time instant, for the initial k data samples, the target class label value corresponding to the new class is -1. Therefore, the k-learning step update for the \u2018c\u2019 new classes ((\u2206\ud835\udf37\ud835\udc58)\ud835\udc43\ud835\udc4b\ud835\udc50) can be written as,\n(\u2206\ud835\udf37\ud835\udc58)\ud835\udc43\ud835\udc4b\ud835\udc50 = (\ud835\udc74\ud835\udc58)\ud835\udc43\ud835\udc4b\ud835\udc43 (\ud835\udc89\ud835\udc8c \ud835\udc7b) \ud835\udc43\ud835\udc4b\ud835\udc4f\n[ \u22121 \u22ef \u22121\n\u22ee \u22f1 \u22ee \u22121 \u22ef \u22121 ]\n\ud835\udc4f\ud835\udc4b\ud835\udc50\n(24)\n(\u2206\ud835\udf37\ud835\udc58)\ud835\udc43\ud835\udc4b\ud835\udc50 = \u2212(\ud835\udc74\ud835\udc58)\ud835\udc43\ud835\udc4b\ud835\udc43 (\ud835\udc89\ud835\udc58 \ud835\udc47)\ud835\udc43\ud835\udc4b\ud835\udc4f \ud835\udc71\ud835\udc4f\ud835\udc4b\ud835\udc50 (25)\nwhere \ud835\udc71\ud835\udc4f\ud835\udc4b\ud835\udc50 is an all-ones matrix.\n\ud835\udc71\ud835\udc4f\ud835\udc4b\ud835\udc50 = [ 1 \u22ef 1 \u22ee \u22f1 \u22ee 1 \u22ef 1 ]\n\ud835\udc4f\ud835\udc4b\ud835\udc50\n(26)\nThe k-learning step update for the new classes is then incorporated with the \ud835\udf37\ud835\u0303\udc58\ud835\udc5a\ud835\udc4b\ud835\udc5a+\ud835\udc50 to provide the upgraded\n(\ud835\udf37\ud835\udc58)\ud835\udc43\ud835\udc4b(\ud835\udc5a+\ud835\udc50) matrix which is recalibrated to adapt learning \u2018c\u2019 new classes.\n(\u2206\ud835\u0303\udf37\ud835\udc58)\ud835\udc43\ud835\udc4b\ud835\udc5a+\ud835\udc50 = [\ud835\udc76\ud835\udc43\ud835\udc4b\ud835\udc5a \u2212(\ud835\udc74\ud835\udc58)\ud835\udc43\ud835\udc4b\ud835\udc43 (\ud835\udc89\ud835\udc58 \ud835\udc47)\ud835\udc43\ud835\udc4b\ud835\udc4f \ud835\udc71\ud835\udc4f\ud835\udc4b\ud835\udc50] (27)\nThe recalibrated output weight matrix (\ud835\udf37\ud835\udc58)\ud835\udc41\u2032\ud835\udc4b(\ud835\udc41\u2032+\ud835\udc50) is calculated as,\n(\ud835\udf37\ud835\udc58)\ud835\udc43\ud835\udc4b(\ud835\udc5a+\ud835\udc50) = \ud835\udf37\ud835\u0303\udc58\ud835\udc43\ud835\udc4b\ud835\udc5a+\ud835\udc50 + (\u2206\ud835\u0303\udf37\ud835\udc58)\ud835\udc43\ud835\udc4b\ud835\udc5a+\ud835\udc50 (28)\nUpon simplification, (\ud835\udf37\ud835\udc58)\ud835\udc43\ud835\udc4b(\ud835\udc5a+\ud835\udc50) can be expressed as,\n(\ud835\udf37\ud835\udc58)\ud835\udc43\ud835\udc4b(\ud835\udc5a+\ud835\udc50) = [ (\ud835\udf37\ud835\udc58)\ud835\udc43\ud835\udc4b\ud835\udc5a (\u2206\ud835\udf37\ud835\udc58)\ud835\udc43\ud835\udc4b\ud835\udc50 ] (29)\n(\ud835\udf37\ud835\udc58)\ud835\udc43\ud835\udc4b\ud835\udc5a represents the knowledge previously learnt. The dimension of \u03b2 is increased from m to m+c. As opposed to populating the increased dimension with identity matrix values, the new entries (\u2206\ud835\udf37\ud835\udc58)\ud835\udc43\ud835\udc4b\ud835\udc50 are calculated in such a\nway that the newly introduced classes will appear to the neural network as if they are present from the beginning of the training procedure and the training data samples thus far does not belong to the newly introduced class.\nThe network is recalibrated such that the (\u2206\ud835\udf37\ud835\udc58)\ud835\udc43\ud835\udc4b\ud835\udc50 matrix represents the learning of the new class from the beginning of the training phase to the current data sample considering that none of the previous data samples belong to the newly introduced class. i.e. The (\u2206\ud835\udf37\ud835\udc58)\ud835\udc43\ud835\udc4b\ud835\udc50 is computed which is equivalent to the k-learning step equivalent of the \u2018c\u2019 new classes from the beginning of the training phase.\nTherefore the updated (\ud835\udf37\ud835\udc58)\ud835\udc43\ud835\udc4b(\ud835\udc5a+\ud835\udc50) matrix represents the network with (m+c) classes with \u2018m\u2019 previously existing\nclasses and \u2018c\u2019 new classes.\nSTEP 5: The hidden layer output vector hk+1 is calculated.\nSTEP 6: The output weight matrix of increased dimension to facilitate learning of new class is updated based on\nthe RLS algorithm as,\n\ud835\udc74\ud835\udc8c+\ud835\udfcf = \ud835\udc74\ud835\udc8c \u2212 \ud835\udc74\ud835\udc8c\ud835\udc89\ud835\udc8c+\ud835\udfcf \ud835\udc7b (\ud835\udc70 + \ud835\udc89\ud835\udc8c+\ud835\udfcf\ud835\udc74\ud835\udc8c\ud835\udc89\ud835\udc8c+\ud835\udfcf\n\ud835\udc7b ) \u2212\ud835\udfcf \ud835\udc89\ud835\udc8c+\ud835\udfcf\ud835\udc74\ud835\udc8c (30)\n\ud835\udf37\ud835\udc8c+\ud835\udfcf = \ud835\udf37\ud835\udc8c + \ud835\udc74\ud835\udc8c+\ud835\udfcf\ud835\udc89\ud835\udc8c+\ud835\udfcf \ud835\udc7b (\ud835\udc95\ud835\udc8c+\ud835\udfcf \u2212 \ud835\udc89\ud835\udc8c+\ud835\udfcf\ud835\udf37\ud835\udc8c) (31)\nWhenever a new class(es) are encountered, the training resume learning the new class/classes by retaining the existing knowledge. The algorithm also supports recalibration with multiple new classes introduced simultaneously and sequentially. Also, the new classes can be introduced at any instant of time and any number of times to the network.\nThe algorithm of the progressive learning technique (PLT) is summarized in flow chart given in Fig. 1."}, {"heading": "4 EXPERIMENTATION", "text": "Proposed progressive learning algorithm exhibit \u201cdynamic\u201d learning of new class of data. Current multiclass classification algorithms fails to adapt when encountered with new class and hence the accuracy drops when introduced with one or more new classes. The proposed algorithm redesigns itself to adapt to new classifications and still retaining the knowledge learnt thus far.\nThe proposed progressive learning algorithm is tested with several real world and standard datasets. The standard datasets are in general uniformly distributed. But to test the performance of progressive learning effectively, it should be presented with conditions where new classes are introduced in a non \u2013 uniform manner at different time instants. Hence the standard datasets cannot be used directly to test the progressive learning algorithm efficiently. The datasets should be in such a way that only a subset of classes is available for training initially and new classes should be introduced at arbitrary time instances during the latter part of training. Thus, some of the standard datasets are modified and used for testing the proposed algorithm.\nBy default, classification problems involve two classes: 1. Presence of class and 2. Absence of class. These are the two trivial classes that are available in any of the classification problem. Since the minimum number of classes in a classification is two, learning of new classes is absent in bivariate datasets. For binary classification datasets, since there are only two classes and no new classes are introduced, the proposed algorithm performs similar to the existing online sequential algorithm. The unique feature of progressive learning is clearly evident only in multiclass classification.\nThus the proposed algorithm is tested with multiclass classification datasets such as iris, balance scale, waveform, wine, satellite image, digit and character datasets. The specifications of the datasets are shown in TABLE 1. The proposed technique is experimented with both balanced and unbalanced datasets. Balanced dataset is one in which each of the class has equal or almost equal number of training data. Unbalanced dataset is a skewed dataset where a subset of classes has a high number of training samples and other classes have fewer training samples. The number of hidden layer neurons for the experimentation is chosen such that the overfitting problem is mitigated. The test dataset consists data samples corresponding to all the class labels used for progressive learning of the network.\nThe proposed algorithm also works for introduction of multiple new classes. The number of classes can be increased from 2 to 3, and then from 3 to 4 and 4 to 5 and so on. For testing multiple new classes, the proposed method is tested with character recognition dataset which is described in the latter part of this section. The introduction of multiple classes both sequentially and simultaneously at multiple time instances are experimented and verified."}, {"heading": "5 RESULTS AND DISCUSSIONS", "text": "The functionality of the technique, consistency and complexity are the three key features to be tested for any new technique. The functional testing is used to validate that the proposed algorithm is functional and results in its expected behavior. The functionality of the technique is tested using iris, waveform and balance-scale datasets. The operational working of the concept of progressive learning in the proposed algorithm is tested in the functionality test. Consistency is another key feature that is essential for any new technique. The proposed algorithm should provide consistent results for multiple trials with minimal variance. Being an ELM based algorithm, the consistency of the proposed method across several trials of same dataset and also the consistency across 10-fold cross validation are tested. Complexity analysis is essential for a new technique. The number of operations performed and calculations involved in the proposed method is computed and is compared against the existing method. Also the performance of the proposed algorithm is evaluated by introducing new classes at different time instances (1. Very early during training, 2. In the middle of training and 3. Towards the end of training) are evaluated. Both sequential and simultaneous introduction of new classes are experimented and results are analyzed and discussed."}, {"heading": "5.1 Functionality", "text": "The proposed technique is experimented with iris, waveform and balance scale datasets to verify the basic intended functionality of the technique. The iris dataset consists of three classes which are uniformly distributed over the 150 instances. To facilitate testing of progressive learning, the dataset is redistributed such that first 50 samples consists of only two classes (sentosa, versicolor) and the third class (virginica) is introduced only after the 51st sample. This type of redistribution closely emulates the real time scenario of encountering a new class on the run. The ability of the proposed algorithm to recognize, adapt and learn the new class can be verified by this testing. The distribution details of the dataset used is given in TABLE 2.\nThe Progressive Learning algorithm is tested with the specified iris dataset and the learning curve or the testing accuracy graph is plotted. The result obtained is shown in Fig. 2. The testing accuracy is continuously calculated with the test data set for every new training data. It can be seen from the graph that until the sample index 50, the testing accuracy is only 66.6 %. This implies that the system has learnt only two of the three classes thus far. When the third new class is introduced in the 51st sample, the system recognizes the introduction of a new class and recalibrates itself by sufficiently increasing the number of neurons in the output layer. The weight matrix is suitably increased in dimension and the weights are updated based on the special recalibration technique proposed. A new network structure and weight parameters are formed from the current network parameters and the data obtained from the new class. In the forthcoming iterations, the system then trains for recognition of the new class in addition to previously learnt classes and reaches a steady state testing accuracy. This process results in the sudden rise in the testing accuracy of the network, which then settles at a final testing accuracy value. This sudden increase in the testing accuracy is due to the fact that the network can now recognize the newly encountered class of data.\nThe same procedure is repeated for waveform and balance scale dataset. The dataset specifications of the waveform and balance scale dataset are shown in TABLE 3-4. The result obtained by the progressive learning method is shown in Fig. 3-4 respectively.\nFrom the test results, the expected behavior of the progressive learning method is verified. The result shows that the algorithm is able to learn new classes dynamically on the run and the learning of new class does not significantly affect the accuracy of the classes previously learnt. The consistency and performance of the proposed method is evaluated using six benchmark datasets."}, {"heading": "5.2 Consistency", "text": "Consistency is a critical characteristic to be tested for any new technique. The proposed technique is verified for its consistency in its results. Consistency is a key virtue that any technique should exhibit. The learning technique which provides inconsistent results is not reliable for practical applications. Being an ELM based technique, the input weights and the hidden layer bias values are initialized at random. Hence multiple executions of the same dataset and same specification results in different results. Therefore, the same dataset with same specification is executed multiple times to determine the consistency across multiple executions. The consistency results of repeated multiple executions of the three datasets are shown in TABLE 5.\nCross validation is the most common method to evaluate the consistency of any given technique. The proposed algorithm is tested with each of the datasets for 5-fold cross validation (5-fcv) and 10-fold cross validation (10-fcv) and the resulting testing accuracy is tabulated. TABLE 6 gives the consistency of the proposed algorithm for cross\nvalidation performance. It can be seen from the table that the proposed algorithm is consistently accurate in each of the attempts. The deviation of the testing accuracy is in order of about 1 % from the mean value which is nominal. Thus, the results show that the proposed method gives consistent and reliable testing accuracy for both balanced and unbalanced datasets."}, {"heading": "5.3 Computational Reduction", "text": "The number of computations required for the proposed progressive learning technique is analyzed and compared with the existing OS-ELM method. Though learning of new classes dynamically on the run causes overhead to the computations and seemingly increases the complexity of the technique, the actual computational complexity of the proposed technique is lesser than the OS-ELM method. The decrease in complexity is due to two reasons.\n1. The overhead computations responsible for increasing the number of output neurons, creating new interconnections and recalibration of weights occur only during the samples when a new class is introduced. Thus, the recalibration routine is invoked only when there is a new class, henceforth causing minimal increase in the computation complexity. For example, when only one new class is introduced, the recalibration procedure is invoked only once.\n2. The progressive learning method also provides another distinct advantage. Since the new classes are learnt dynamically, it results in lesser number of weight calculations when compared with other static online sequential training techniques like OS-ELM.\nFor example, in the iris dataset considered, the traditional algorithm needs to update the weight for all three output neurons for the entire 150 instances of the training set. But in the proposed method, there are only two output neurons till the occurrence of the third class. The third output neuron is introduced only during the recalibration stage triggered by the introduction of a new class. Thus, the number of weight calculations is effectively reduced. This effectively reduces the number of computations performed and thereby reducing the computational complexity. The reduction in number of weight calculations is shown in TABLE 7. The number of computations in the OS-ELM is normalized to 100 and the computational complexity of OS-ELM and the progressive learning method are compared in Fig. 5.\nThough the new classes are learnt only from halfway through the datasets, the testing accuracy of the algorithm is nearly maintained or even improved when compared to algorithms with a static number of classes. The reason for the change in accuracy is due to the fact that new classes are learnt on the run after the learning of previous classes. If the previously learnt classes and the new class are fairly distinctive the learning accuracy will be improved. On other hand, in some cases due to the feature set of the learnt class and new class, the learning of the new class will affect the existing knowledge but only to a little extent thereby marginally reducing the overall accuracy.\nThe testing accuracy of the proposed algorithm is compared with the existing OS-ELM and its variants such as voting based OS-ELM (VOS-ELM), enhanced OS-ELM (EOS-ELM), robust OS-ELM (ROS-ELM), robust bayesian ELM (RB-ELM) and generalized pruning ELM (GP-ELM) and is tabulated as shown in TABLE 8. It can be seen from the table that despite learning the new classes dynamically at a later stage of training, the testing accuracy is either improved or maintained nearly equal to the testing accuracy of the OS-ELM based methods. But the proposed method provides two key advantages over the existing methods.\n1. Reduction in computational complexity.\n2. Flexibility to learn new classes at any instant of time.\nFrom the results obtained thus far, it is evident that the proposed progressive learning algorithm learns new class of\ndata in a dynamic way."}, {"heading": "5.4 Introduction of New Class at Different Time Instants", "text": "The new class is introduced at different stages of the training period and its effect on learning rate is analyzed. In order to analyze the response, three different test cases are experimented and performance is measured. The new class is introduced at three different time instances. 1. Very early during training, 2. In the middle of training, 3. Towards the end of training. The testing accuracy curve in each of the test case is plotted and the results are evaluated and compared. The point of introduction of a new class for each of the test case is tabulated and is given in TABLE 9. The performance of the proposed network for each of the test cases is given in Fig. 6. It can be seen from the figure that, independent of the point of introduction of a new class to the system, the network is capable of learning the new class and the final steady state testing accuracy is the same across the test cases."}, {"heading": "5.5 Multiple New Classes", "text": "The performance of the proposed technique when introduced with multiple new classes both sequentially and simultaneously is discussed in this section. Learning of multiple new classes by the proposed algorithm is tested by using the Character recognition dataset. Several combinations of tests are made such as\n1. Sequential introduction of 2 new classes (4 classes)\n2. Sequential introduction of 3 new classes (5 classes)\n3. Simultaneous introduction of 2 new classes along with one new class sequentially (5 classes)\nThe performance of the proposed algorithm on each of the test case is observed."}, {"heading": "5.5.1 Sequential Introduction of 2 new classes", "text": "Character dataset with 4 classes (A, B, C and D) is used to test the sequential introduction of two new classes in the proposed algorithm. The dataset is redistributed to meet the testing requirements for progressive learning. The specifications of the dataset are given in TABLE 10.\nInitially the network is sequentially trained with only two classes A and B up to 800 samples. A new class \u2018C\u2019 is introduced to the training data in the 801st sample and a fourth class \u2018D\u2019 is introduced as 1601st sample. The proposed algorithm identifies both the new classes and recalibrates itself each time and continues learning. This results in two sudden rise in the learning curve of the network. The first rise corresponding occurring at 801st sample corresponds to the learning of class \u2018C\u2019 and the second rise occurring at 1601st sample corresponds to learning of class \u2018D\u2019. The learning curve graph is shown in Fig. 7."}, {"heading": "5.5.2 Sequential introduction of 3 new classes", "text": "Character dataset with 5 classes (A, B, C, D and E) is used for testing sequential introduction of 3 new classes. The network is initially trained to recognize only two classes. Three new classes (C, D and E) are introduced one after another after the initial training of two classes. The specifications of the dataset are shown in TABLE 11.\nEach of the new classes is introduced sequentially at later time instants and the algorithm adapts to new class each\ntime and also maintains the testing accuracy at the same level. The testing accuracy curve is shown in Fig. 8.\nTo verify that learning of each new class is independent of previously learnt classes, the overall testing accuracy is broken down into individual testing accuracy of each of the classes and is shown in Fig. 9. It can be seen that, the testing accuracy of each of the classes remains over 90%. Also, whenever a new class is introduced, a new learning curve is formed which contributes towards the overall accuracy along with the existing classes.\nThe network is initially trained with two classes A and B. Third class C is introduced after 800 samples and the learning curve of the class C is shown in black line. Another new class \u2018D\u2019 who\u2019s testing accuracy as shown in red is introduced after the 1600 sample. A fifth class, \u2018E\u2019 is introduced in the 2001st sample and its learning curve is shown in light blue.\nFrom the graph it can be seen that each class introduced is learnt anew without affecting much the existing knowledge. The learning accuracy of each of the classes is collectively responsible for the overall accuracy of the network. Further, it can be seen that the testing accuracy of each of the classes is over 90% and the overall accuracy of 94% is achieved.\nThe testing accuracy obtained by introducing one, two and three new classes is summarized in TABLE 12.\nFrom the table it can be observed that learning of multiple new classes does not affect the testing accuracy of previously learnt class. Hence this method can be used to learn a large number of multiple new classes in a progressive manner without affecting the testing accuracy of previously learnt classes."}, {"heading": "5.5.3 Simultaneous introduction of new classes", "text": "To verify that the proposed algorithm performs effectively when multiple classes are introduced simultaneously (introduced in the same block), character dataset with specifications as shown in TABLE 13 is used. Here, the two classes C and D are introduced together and the new class E at a later stage. The testing accuracy is shown in Fig. 10.\nThe first rise observed at the sample instant of 800 in the testing accuracy curve corresponds to the introduction of two new classes (characters C and D). The algorithm identifies both the new classes and recalibrates to facilitate multiple class addition. The second rise in the curve corresponds to the introduction of the third class (character E).\nIn order to show that the previous knowledge is retained and new knowledge is added along with the existing, the\ntesting accuracy is split up for each of the five alphabets and is shown in Fig. 11.\nIt can be seen that, two new learning curves corresponding to each new class C and D is introduced in the 800th sample. Both of the newly introduced classes are learnt simultaneously along with the existing classes A and B. The learning curve at 1600th sample index corresponds to the introduction of class E.\nAlso, from the graph it is clear that the learning of additional classes does not significantly affect the testing accuracy of the classes previously learnt. Thus, enabling the proposed algorithm to learn multiple new classes both sequentially and simultaneously in a progressive manner.\nThe proposed algorithm introduces new neurons in the output layer and recalibrates the network by itself to facilitate learning of new classes. Since only the output layer neurons are increased and the number of hidden layer neurons is the same, the learning of new classes that can be progressively learnt is limited by the number of classes that can be learnt by the given number of hidden layer neurons. Further, the proposed algorithm can be extended such that both the output neurons and hidden layer neurons are increased such that any number of new classes can be learnt progressively."}, {"heading": "6. CONCLUSIONS", "text": "In this paper, a novel learning technique of progressive learning for multi-class classification is developed. Progressive learning enables the network to learn multiple new classes dynamically on the run. The new classes can be learnt in both sequential and simultaneous manner. Hence this technique is much suited for applications where the number of classes to be learned is unknown. Progressive learning enables the network to recalibrate and adapt when encountered\nwith a new class of data. The proposed progressive learning technique will perform effectively in applications such as cognitive robotics where the system is trained by real time experienced based data."}, {"heading": "ACKNOWLEDGMENT", "text": "The first author would like to thank Nanyang Technological University, Singapore for the NTU Research Student\nScholarship."}], "references": [{"title": "Learning Representations by Back-propagating Errors", "author": ["D.E. Rumelhart", "G.E. Hinton", "R.J. Williams"], "venue": "Nature, vol. 323, no. 9, pp. 533-536", "citeRegEx": "1", "shortCiteRegEx": null, "year": 1986}, {"title": "Training Feedforward Networks with the Marquardt Algorithm", "author": ["M.T. Hagan", "M.B. Menhaj"], "venue": "IEEE Trans. on Neural Networks, vol. 5, no. 6, pp. 989-993", "citeRegEx": "2", "shortCiteRegEx": null, "year": 1994}, {"title": "Neural Network Learning without Backpropagation", "author": ["B.M. Wilamowski", "H. Yu"], "venue": "IEEE Trans. on Neural Networks, vol. 21. No. 11, pp. 1793-1803", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2010}, {"title": "Orthogonal Least Squares Learning Algorithm for Radial Basis Function Networks", "author": ["S. Chen", "C. Cowan", "P. Grant"], "venue": "IEEE Trans. on Neural Networks, vol. 2, no. 2, pp. 302-309", "citeRegEx": "4", "shortCiteRegEx": null, "year": 1991}, {"title": "A Fast Nonlinear Model Identification Method", "author": ["K. Li", "J.X. Peng", "G.W. Irwin"], "venue": "IEEE Trans. on Automatic Control, vol. 50, no. 8, pp. 1211- 1216", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2005}, {"title": "Evolutionary Algorithms for Neural Network Design and Training", "author": ["J. Branke"], "venue": "Proc. Of first Nordic workshop on genetic algorithms and its applications", "citeRegEx": "6", "shortCiteRegEx": null, "year": 1995}, {"title": "A Review of Evolutionary Artificial Neural Networks", "author": ["X. Yao"], "venue": "International Journal of Intelligent Systems, vol. 8, no. 4, pp. 539-567", "citeRegEx": "7", "shortCiteRegEx": null, "year": 1993}, {"title": "An Improved On-Line Sequential Learning Algorithm for Extreme Learning Machine", "author": ["B. Li", "J. Wang", "Y. Li", "Y. Song"], "venue": "Advances in Neural Networks \u2013 ISNN 2007, vol. 4491, pp. 1087-1093", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2007}, {"title": "Extreme Learning Machine: Theory and applications", "author": ["G.B. Huang", "Q.Y. Zhu", "C.K. Siew"], "venue": "Neurocomputing, vol. 70, pp. 489-501", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2006}, {"title": "Learning Internal Representations by Error Propagation", "author": ["D.E. Rumelhart", "G.E. Hinton", "R.J. Williams"], "venue": "No. ICS-8506, California University San Diego La Jolla Inst. for Cognitive Science", "citeRegEx": "10", "shortCiteRegEx": null, "year": 1985}, {"title": "Smooth Function Approximation Using Neural Networks", "author": ["S. Ferrari", "R.F. Stengel"], "venue": "IEEE Transactions on Neural Networks, vol. 16, pp. 24-38", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2005}, {"title": "Classification Ability of Single Hidden Layer Feedforward Neural Networks", "author": ["G.B. Huang", "Y.Q. Chen", "H.A. Babri"], "venue": "IEEE Transactions on Neural Networks, vol. 11, pp. 799-801", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2000}, {"title": "Extreme Learning Machines: A Survey", "author": ["G.B. Huang", "D.H. Wang", "Y. Lan"], "venue": "International Journal of Machine Learning and Cybernetics, vol. 2, pp. 107-122", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2011}, {"title": "Generalized Single-hidden Layer Feedforward Networks for Regression Problems", "author": ["N. Wang", "M.J. Er", "M. Han"], "venue": "IEEE Transactions on Neural Networks and Learning Systems, vol. 26, no. 6, pp. 1161-1176", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2015}, {"title": "A Fast and Accurate Online Sequential Learning Algorithm for Feedforward Networks", "author": ["N.Y. Liang", "G.B. Huang", "P. Saratchandran", "N. Sundararajan"], "venue": "IEEE Transactions on Neural Networks, vol. 17, pp. 1411-1423", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2006}, {"title": "Extreme Learning Machine: A New Learning Scheme of Feedforward Neural Networks", "author": ["G.B. Huang", "Q.Y. Zhu", "C.K. Siew"], "venue": "Proceedings of International Joint Conference on Neural Networks, vol. 2, pp. 985-990", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2004}, {"title": "Universal Approximation Using Incremental Constructive Feedforward Networks with Random Hidden Nodes", "author": ["G.B. Huang", "L. Chen", "C.K. Siew"], "venue": "IEEE Transactions on Neural Networks, vol. 17, pp. 879-892", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2006}, {"title": "Convex Incremental Extreme Learning Machine", "author": ["G.B. Huang", "L. Chen"], "venue": "Neurocomputing, vol. 70, no. 16, pp. 3056-3062", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2007}, {"title": "Enhanced Random Search based Incremental Extreme Learning Machine", "author": ["G.B. Huang", "L. Chen"], "venue": "Neurocomputing, vol. 71, no. 16, pp. 3460-3468", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2008}, {"title": "A Study on Effectiveness of Extreme Learning Machine", "author": ["Y. Wang", "F. Cao", "Y. Yuan"], "venue": "Neurocomputing, vol. 74, pp. 2483-2490", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2011}, {"title": "Extreme Learning Machine for Regression and Multiclass Classification", "author": ["G.B. Huang", "H. Zhou", "X. Ding", "R. Zhang"], "venue": "IEEE Trans. on Systems, Man and Cybernetics, Part B: Cybernetics, vol. 42, no. 2, pp. 513-529", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2012}, {"title": "Evolutionary Extreme Learning Machine", "author": ["Q.Y. Zhu", "A.K. Qin", "P.N. Suganthan", "G.B. Huang"], "venue": "Pattern Recognition, vol. 38, pp. 1759- 1763", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2005}, {"title": "Fully Complex Extreme Learning Machine", "author": ["M.B. Li", "G.B. Huang", "P. Saratchandran", "N. Sundararajan"], "venue": "Neurocomputing, vol.68, pp.306- 314", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2005}, {"title": "Parsimonious Extreme Learning Machine Using Recursive Orthogonal Least Squares", "author": ["N. Wang", "M.J. Er", "M. Han"], "venue": "IEEE Transactions on Neural Networks and Learning Systems, vol. 25, no. 10, pp. 1828-1841", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2014}, {"title": "An Efficient Sequential Learning Algorithm for Growing and Pruning RBF (GAP-RBF) Networks", "author": ["G.B. Huang", "P. Saratchandran", "N. Sundararajan"], "venue": "IEEE Transactions on Systems, Man, and Cybernetics, Part B: vol. 34, pp. 2284-2292", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2004}, {"title": "A Generalized Growing and Pruning RBF (GGAP-RBF) Neural Network for Function Approximation", "author": ["G.B. Huang", "P. Saratchandran", "N. Sundararajan"], "venue": "IEEE Transactions on Neural Networks, vol. 16, pp. 57-67", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2005}, {"title": "Hybrid Recursive Least Squares Algorithm for Online Sequential Identification Using Data Chunks", "author": ["N. Wang", "J.C. Sun", "M.J. Er", "Y.C. Liu"], "venue": "Neurocomputing, vol. 174, pp. 651-660", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2016}, {"title": "Constructive Multi-output Extreme Learning Machine with Application to Large Tanker Motion Dynamics Identification", "author": ["N. Wang", "M. Han", "N. Dong", "M.J. Er"], "venue": "Neurocomputing, vol. 128, pp. 59-72", "citeRegEx": "28", "shortCiteRegEx": null, "year": 2014}, {"title": "Online Sequential Fuzzy Extreme Learning Machine for Function Approximation and Classification Problems", "author": ["H.J. Rong", "G.B. Huang", "N. Sundararajan", "P. Saratchandran"], "venue": "IEEE Transactions on Systems, Man, and Cybernetics, Part B: vol. 39, pp. 1067-1072", "citeRegEx": "29", "shortCiteRegEx": null, "year": 2009}, {"title": "On-Line Sequential Extreme Learning Machine", "author": ["G.B. Huang", "N.Y. Liang", "H.J. Rong", "P. Saratchandran", "N. Sundararajan"], "venue": "Computational Intelligence, vol. 2005, pp. 232-237", "citeRegEx": "30", "shortCiteRegEx": null, "year": 2005}, {"title": "Trends in Extreme Learning Machines: A Review", "author": ["G. Huang", "G.B. Huang", "S. Song", "K. You"], "venue": "Neural Networks, vol. 61, pp. 32-48", "citeRegEx": "31", "shortCiteRegEx": null, "year": 2015}, {"title": "A Hybrid Automatic System for the Diagnosis of Lung Cancer Based on Genetic Algorithm and Fuzzy Extreme Learning Machines", "author": ["M.R. Daliri"], "venue": "Journal of Medical Systems, vol. 36, no. 2, pp. 1001-1005", "citeRegEx": "32", "shortCiteRegEx": null, "year": 2012}, {"title": "Fuzzy Extreme Learning Machine for Classification", "author": ["W.B. Zhang", "H.B. Ji"], "venue": "Electronics Letters, vol. 49, no. 7, pp. 448-449", "citeRegEx": "33", "shortCiteRegEx": null, "year": 2013}, {"title": "A New Automatic Target Recognition System based on Wavelet Extreme Learning Machine", "author": ["E. Avci", "R. Coteli"], "venue": "Expert Systems with Applications, vol. 39, no. 16, pp. 12340-12348", "citeRegEx": "34", "shortCiteRegEx": null, "year": 2012}, {"title": "Application of Extreme Learning Machine for Series Compensated Transmission Line Protection", "author": ["V. Malathi", "N.S. Marimuthu", "S. Baskar", "K. Ramar"], "venue": "Engineering Applications of Artificial Intelligence, vol. 24, no. 5, pp. 880-887", "citeRegEx": "35", "shortCiteRegEx": null, "year": 2011}, {"title": "Parameter-insensitive Kernel in Extreme Learning for Non-linear Support Vector Regression", "author": ["B. Freney", "M. Verleysen"], "venue": "Neurocomputing, vol. 74, no. 16, pp. 2526-2531", "citeRegEx": "36", "shortCiteRegEx": null, "year": 2011}, {"title": "Weighted Extreme learning Machine for Imbalance Learning", "author": ["W.W. Zong", "G.B. Huang", "Y.Q. Chen"], "venue": "Neurocomputing, vol. 101, pp. 229- 242", "citeRegEx": "37", "shortCiteRegEx": null, "year": 2013}, {"title": "A New Robust Training Algorithm for a Class of Single Hidden Layer feedforward neural networks", "author": ["Z.H. Man", "K. Lee", "D.H. Wang", "Z.W. Cao", "C.Y. Miao"], "venue": "Neurocomputing, vol. 74, no. 16, pp. 2491-2501", "citeRegEx": "38", "shortCiteRegEx": null, "year": 2011}, {"title": "Voting based Extreme Learning Machine", "author": ["J.W. Cao", "Z.P. Lin", "G.B. Huang", "N. Liu"], "venue": "Information Sciences, vol 185, no. 1, pp. 66-77", "citeRegEx": "39", "shortCiteRegEx": null, "year": 2012}, {"title": "Prediction of Protein-protein Interactions from Amino Acid Sequences with Ensemble Extreme Learning Machines and Principal Component Analysis", "author": ["Z.H. You", "Y.K. Lei", "L. Zhu", "J.F. Xia", "B. Wang"], "venue": "BMC Bioinformatics, 14", "citeRegEx": "40", "shortCiteRegEx": null, "year": 2013}, {"title": "Dynamic Ensemble Extreme Learning Machine based on Sample Entropy", "author": ["J.H. Zhai", "H.Y. Xu", "X.Z. Wang"], "venue": "Soft Computing, vol. 16, no. 9, pp. 1493-1502", "citeRegEx": "41", "shortCiteRegEx": null, "year": 2012}, {"title": "A Class Incremental Extreme learning machine for activity recognition", "author": ["Z. Zhao", "Z. Chen", "Y. Chen", "S. Wang", "H. Wang"], "venue": "Cognitive Computation, vol. 6 no. 3, pp. 423-431", "citeRegEx": "43", "shortCiteRegEx": null, "year": 2014}], "referenceMentions": [{"referenceID": 0, "context": "INTRODUCTION HE study on feedforward neural network (FNN) has gained prominence since the advent of back propagation (BP) algorithm [1].", "startOffset": 132, "endOffset": 135}, {"referenceID": 1, "context": "Several improved and optimized variants of the BP algorithm have then been developed and analyzed [2-7].", "startOffset": 98, "endOffset": 103}, {"referenceID": 2, "context": "Several improved and optimized variants of the BP algorithm have then been developed and analyzed [2-7].", "startOffset": 98, "endOffset": 103}, {"referenceID": 3, "context": "Several improved and optimized variants of the BP algorithm have then been developed and analyzed [2-7].", "startOffset": 98, "endOffset": 103}, {"referenceID": 4, "context": "Several improved and optimized variants of the BP algorithm have then been developed and analyzed [2-7].", "startOffset": 98, "endOffset": 103}, {"referenceID": 5, "context": "Several improved and optimized variants of the BP algorithm have then been developed and analyzed [2-7].", "startOffset": 98, "endOffset": 103}, {"referenceID": 6, "context": "Several improved and optimized variants of the BP algorithm have then been developed and analyzed [2-7].", "startOffset": 98, "endOffset": 103}, {"referenceID": 7, "context": "In the past two decades, single hidden layer feedforward neural networks (SLFNs) has gained significant importance due to its widespread applications in recognition, classification and function approximation area [8-12].", "startOffset": 213, "endOffset": 219}, {"referenceID": 8, "context": "In the past two decades, single hidden layer feedforward neural networks (SLFNs) has gained significant importance due to its widespread applications in recognition, classification and function approximation area [8-12].", "startOffset": 213, "endOffset": 219}, {"referenceID": 9, "context": "In the past two decades, single hidden layer feedforward neural networks (SLFNs) has gained significant importance due to its widespread applications in recognition, classification and function approximation area [8-12].", "startOffset": 213, "endOffset": 219}, {"referenceID": 10, "context": "In the past two decades, single hidden layer feedforward neural networks (SLFNs) has gained significant importance due to its widespread applications in recognition, classification and function approximation area [8-12].", "startOffset": 213, "endOffset": 219}, {"referenceID": 11, "context": "In the past two decades, single hidden layer feedforward neural networks (SLFNs) has gained significant importance due to its widespread applications in recognition, classification and function approximation area [8-12].", "startOffset": 213, "endOffset": 219}, {"referenceID": 12, "context": "Several learning techniques have been proposed since then for effective training of the SLFN [13-14].", "startOffset": 93, "endOffset": 100}, {"referenceID": 13, "context": "Several learning techniques have been proposed since then for effective training of the SLFN [13-14].", "startOffset": 93, "endOffset": 100}, {"referenceID": 14, "context": "The learning techniques can be grouped under two basic categories; Batch learning and Sequential learning [15].", "startOffset": 106, "endOffset": 110}, {"referenceID": 15, "context": "One of the relatively new batch learning scheme called Extreme Learning Machines (ELM) is proposed by Huang et al in 2004 [16].", "startOffset": 122, "endOffset": 126}, {"referenceID": 16, "context": "The special nature of ELM is that the input weights and the hidden node biases can be chosen at random [17].", "startOffset": 103, "endOffset": 107}, {"referenceID": 16, "context": "A key feature of ELM is that it maintains the universal approximation capability of SLFN [17-19].", "startOffset": 89, "endOffset": 96}, {"referenceID": 17, "context": "A key feature of ELM is that it maintains the universal approximation capability of SLFN [17-19].", "startOffset": 89, "endOffset": 96}, {"referenceID": 18, "context": "A key feature of ELM is that it maintains the universal approximation capability of SLFN [17-19].", "startOffset": 89, "endOffset": 96}, {"referenceID": 19, "context": "It has gained much attention to it and several research works are further made in it due to its special nature of random input weight initialization and its unique advantage of extreme learning speed [20].", "startOffset": 200, "endOffset": 204}, {"referenceID": 8, "context": "The advantages of ELM over other traditional feedforward neural network are analyzed in the literature [9,21].", "startOffset": 103, "endOffset": 109}, {"referenceID": 20, "context": "The advantages of ELM over other traditional feedforward neural network are analyzed in the literature [9,21].", "startOffset": 103, "endOffset": 109}, {"referenceID": 21, "context": "Many new variants and developments are made to the ELM and significant results are achieved in the approximation, classification and regression areas [22-24].", "startOffset": 150, "endOffset": 157}, {"referenceID": 22, "context": "Many new variants and developments are made to the ELM and significant results are achieved in the approximation, classification and regression areas [22-24].", "startOffset": 150, "endOffset": 157}, {"referenceID": 23, "context": "Many new variants and developments are made to the ELM and significant results are achieved in the approximation, classification and regression areas [22-24].", "startOffset": 150, "endOffset": 157}, {"referenceID": 7, "context": "In many cases, sequential learning algorithms are preferred over batch learning algorithms as they do not require retraining whenever a new data sample is received [8].", "startOffset": 164, "endOffset": 167}, {"referenceID": 14, "context": "Online-sequential learning method that combines ELM and Recursive Least Square (RLS) algorithm is later developed and is called Online-Sequential extreme learning machine (OS-ELM) [15].", "startOffset": 180, "endOffset": 184}, {"referenceID": 7, "context": "Several variants of ELM and OS-ELM were developed and proposed in the literature [8,9,17,22,23,29].", "startOffset": 81, "endOffset": 98}, {"referenceID": 8, "context": "Several variants of ELM and OS-ELM were developed and proposed in the literature [8,9,17,22,23,29].", "startOffset": 81, "endOffset": 98}, {"referenceID": 16, "context": "Several variants of ELM and OS-ELM were developed and proposed in the literature [8,9,17,22,23,29].", "startOffset": 81, "endOffset": 98}, {"referenceID": 21, "context": "Several variants of ELM and OS-ELM were developed and proposed in the literature [8,9,17,22,23,29].", "startOffset": 81, "endOffset": 98}, {"referenceID": 22, "context": "Several variants of ELM and OS-ELM were developed and proposed in the literature [8,9,17,22,23,29].", "startOffset": 81, "endOffset": 98}, {"referenceID": 28, "context": "Several variants of ELM and OS-ELM were developed and proposed in the literature [8,9,17,22,23,29].", "startOffset": 81, "endOffset": 98}, {"referenceID": 15, "context": "[16] is given below.", "startOffset": 0, "endOffset": 4}, {"referenceID": 8, "context": "The mathematical framework and the training process are extensively described in the literature [9].", "startOffset": 96, "endOffset": 99}, {"referenceID": 8, "context": "Lemma 1: [9] Given a standard SLFN with N hidden nodes and activation function g: R \u2192 R which is infinitely differentiable in any interval, for N arbitrary distinct samples (xi,ti), where xi \u03b5 R and ti \u03b5 R, for any wi and bi randomly chosen from any intervals of R and R, respectively, according to any continuous probability distribution, then with probability one, the hidden layer output matrix H of the SLFN is invertible and ||H\u03b2 \u2013 T|| = 0.", "startOffset": 9, "endOffset": 12}, {"referenceID": 8, "context": "Lemma 2: [9] Given any small positive value \u03b5 > 0 and activation function g: R \u2192 R which is infinitely differentiable in any interval, there exists P \u2264 N such that for N arbitrary distinct samples (xi,ti), where xi \u03b5 R and ti \u03b5 R, for any wi and bi randomly chosen from any intervals of R and R, respectively, according to any continuous probability distribution, then with probability one, ||HNxP \u03b2PXm \u2013 TNXm|| < \u03b5.", "startOffset": 9, "endOffset": 12}, {"referenceID": 14, "context": "2 Online Sequential \u2013 Extreme Learning Machine Based on the batch learning method of the ELM, sequential modification is performed and Online Sequential-ELM (OS-ELM) is proposed in literature [15].", "startOffset": 192, "endOffset": 196}, {"referenceID": 14, "context": "As stated in [15], this solution gives the least square solution to H\u03b2 = T.", "startOffset": 13, "endOffset": 17}, {"referenceID": 7, "context": "several papers [8,9,13,15,30].", "startOffset": 15, "endOffset": 29}, {"referenceID": 8, "context": "several papers [8,9,13,15,30].", "startOffset": 15, "endOffset": 29}, {"referenceID": 12, "context": "several papers [8,9,13,15,30].", "startOffset": 15, "endOffset": 29}, {"referenceID": 14, "context": "several papers [8,9,13,15,30].", "startOffset": 15, "endOffset": 29}, {"referenceID": 29, "context": "several papers [8,9,13,15,30].", "startOffset": 15, "endOffset": 29}, {"referenceID": 30, "context": "The standard variants of activation function used in ELM [31] and other special mapping functions and their variants are discussed in detail in the literature [32-35].", "startOffset": 57, "endOffset": 61}, {"referenceID": 31, "context": "The standard variants of activation function used in ELM [31] and other special mapping functions and their variants are discussed in detail in the literature [32-35].", "startOffset": 159, "endOffset": 166}, {"referenceID": 32, "context": "The standard variants of activation function used in ELM [31] and other special mapping functions and their variants are discussed in detail in the literature [32-35].", "startOffset": 159, "endOffset": 166}, {"referenceID": 33, "context": "The standard variants of activation function used in ELM [31] and other special mapping functions and their variants are discussed in detail in the literature [32-35].", "startOffset": 159, "endOffset": 166}, {"referenceID": 34, "context": "The standard variants of activation function used in ELM [31] and other special mapping functions and their variants are discussed in detail in the literature [32-35].", "startOffset": 159, "endOffset": 166}, {"referenceID": 35, "context": "The other variants of ELM includes ELM Kernel [36], ELM for imbalanced data [37], ELM for noisy data [38], Incremental ELM [19], ELM ensemble [39-41] and many other variants are summarized in [31] 3.", "startOffset": 46, "endOffset": 50}, {"referenceID": 36, "context": "The other variants of ELM includes ELM Kernel [36], ELM for imbalanced data [37], ELM for noisy data [38], Incremental ELM [19], ELM ensemble [39-41] and many other variants are summarized in [31] 3.", "startOffset": 76, "endOffset": 80}, {"referenceID": 37, "context": "The other variants of ELM includes ELM Kernel [36], ELM for imbalanced data [37], ELM for noisy data [38], Incremental ELM [19], ELM ensemble [39-41] and many other variants are summarized in [31] 3.", "startOffset": 101, "endOffset": 105}, {"referenceID": 18, "context": "The other variants of ELM includes ELM Kernel [36], ELM for imbalanced data [37], ELM for noisy data [38], Incremental ELM [19], ELM ensemble [39-41] and many other variants are summarized in [31] 3.", "startOffset": 123, "endOffset": 127}, {"referenceID": 38, "context": "The other variants of ELM includes ELM Kernel [36], ELM for imbalanced data [37], ELM for noisy data [38], Incremental ELM [19], ELM ensemble [39-41] and many other variants are summarized in [31] 3.", "startOffset": 142, "endOffset": 149}, {"referenceID": 39, "context": "The other variants of ELM includes ELM Kernel [36], ELM for imbalanced data [37], ELM for noisy data [38], Incremental ELM [19], ELM ensemble [39-41] and many other variants are summarized in [31] 3.", "startOffset": 142, "endOffset": 149}, {"referenceID": 40, "context": "The other variants of ELM includes ELM Kernel [36], ELM for imbalanced data [37], ELM for noisy data [38], Incremental ELM [19], ELM ensemble [39-41] and many other variants are summarized in [31] 3.", "startOffset": 142, "endOffset": 149}, {"referenceID": 30, "context": "The other variants of ELM includes ELM Kernel [36], ELM for imbalanced data [37], ELM for noisy data [38], Incremental ELM [19], ELM ensemble [39-41] and many other variants are summarized in [31] 3.", "startOffset": 192, "endOffset": 196}, {"referenceID": 41, "context": "The method of increasing the dimension of the matrix, the weight update and matrix recalibration methods of the proposed algorithm are significantly different from the class-incremental extreme learning machine [43].", "startOffset": 211, "endOffset": 215}], "year": 2016, "abstractText": "In this paper, a progressive learning technique for multi-class classification is proposed. This newly developed learning technique is independent of the number of class constraints and it can learn new classes while still retaining the knowledge of previous classes. Whenever a new class (non-native to the knowledge learnt thus far) is encountered, the neural network structure gets remodeled automatically by facilitating new neurons and interconnections, and the parameters are calculated in such a way that it retains the knowledge learnt thus far. This technique is suitable for realworld applications where the number of classes is often unknown and online learning from real-time data is required. The consistency and the complexity of the progressive learning technique are analyzed. Several standard datasets are used to evaluate the performance of the developed technique. A comparative study shows that the developed technique is superior. Key Words\u2014Classification, machine learning, multi-class, sequential learning, progressive learning.", "creator": "Microsoft\u00ae Word 2013"}}}