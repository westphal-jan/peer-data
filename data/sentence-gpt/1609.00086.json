{"id": "1609.00086", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "1-Sep-2016", "title": "A novel online multi-label classifier for high-speed streaming data applications", "abstract": "In this paper, a high-speed online neural network classifier based on extreme learning machines for multi-label classification is proposed. In multi-label classification, each of the input data sample belongs to one or more than one of the target labels. The traditional binary and multi-class classification where each sample belongs to only one target class forms the subset of multi-label classification. Multi-label classification problems are far more complex than binary and multi-class classification problems, as both the number of target labels and each of the target labels corresponding to each of the input samples are to be identified. The proposed work exploits the high-speed nature of the extreme learning machines to achieve real-time multi-label classification of streaming data. A new threshold-based online sequential learning algorithm is proposed for high speed and streaming data classification of multi-label problems. The proposed method is experimented with six different datasets from different application domains such as multimedia, text, and biology. The hamming loss, accuracy, training time and testing time of the proposed technique is compared with nine different state-of-the-art methods. Experimental studies shows that the proposed technique outperforms the existing multi-label classifiers in terms of performance and speed.\n\n\nThe goal of this paper is to demonstrate that the implementation of multiple-label classification problems is not a mere implementation of multiple-label classification problems. The current algorithm for classification problems can be defined by the following criteria:", "histories": [["v1", "Thu, 1 Sep 2016 01:58:50 GMT  (306kb)", "http://arxiv.org/abs/1609.00086v1", "18 pages, 7 tables, 3 figures. arXiv admin note: text overlap witharXiv:1608.08898"]], "COMMENTS": "18 pages, 7 tables, 3 figures. arXiv admin note: text overlap witharXiv:1608.08898", "reviews": [], "SUBJECTS": "cs.LG cs.AI cs.NE", "authors": ["rajasekar venkatesan", "meng joo er", "mihika dave", "mahardhika pratama", "shiqian wu"], "accepted": false, "id": "1609.00086"}, "pdf": {"name": "1609.00086.pdf", "metadata": {"source": "CRF", "title": "A Novel Online Multi-label Classifier for High-Speed Streaming Data Applications", "authors": ["Rajasekar Venkatesan", "Meng Joo Er", "Mihika Dave", "Mahardhika Pratama", "Shiqian Wu"], "emails": [], "sections": [{"heading": null, "text": "Keywords: Classification, multi-label, extreme learning machines, high speed, real-time."}, {"heading": "1. Introduction", "text": "Classification is a problem of identifying which of the target categories a data sample belongs to. In machine learning, classification can be defined as \u201cGiven a set of training examples composed of pairs, find a function f(x) that maps each attribute vector xi to its associated class yi, i = 1,2,\u2026.,n, where n is the total number of training samples\u201d (de Carvalho and Freitas 2009). This is the most common type of classification problem called single-label classification. In single-label classification, each of the data sample belongs to only one of the target labels. But in real world applications, there may be several cases in which each data sample belongs to more than one target labels. This results in the need for multi-label classification. The multi-label classification problems have gained much importance due to its rapidly increasing application areas. The application areas of multi-label classification include but are not limited to text categorization (Gon\u00e7alves and Quaresma 2003; Joachims 1998; Luo and Zincir-Heywood 2005; Tikk and Bir\u00f3 2003; Yu et al. 2005), bioinformatics (Elisseeff and Weston 2001; Min-Ling and Zhi-Hua 2005), medical diagnosis (Karali and Pirnat 1991), image/scene and video categorization (Boutell et al. 2003; Shen et al. 2003), genomics, map labeling (Zhu and Poon 1999), marketing, multimedia, emotion, music categorization, etc. In recent years, the multi-label classification has drawn increased research attention due to the realization of the omnipresence of multi-label prediction tasks in several areas (Tsoumakas et al. 2010). Due to the wide range of applications and increasing importance, several multilabel classification techniques have been developed and are available in the literature.\nThe learning techniques in machine learning can be grouped into two broad categories: Batch Learning and Online Learning. In batch learning, all the training data are collected in prior, and the parameters of the network are calculated by processing all the training data concurrently. This poses a major limitation on the batch learning techniques as they are unable to learn from streaming data. On the other hand, in online/sequential learning techniques the network parameters are updated iteratively with single-pass learning procedure (Pratama et al. 2015a; Pratama et al. 2015d). Several books (Angelov 2012; Gama 2010; Kasabov 2007; Sayed-Mouchaweh and Lughofer 2012) are available in the literature that comprehensively elaborates the data stream classification. In many cases, online learning is preferred over batch\nlearning as they can learn from data streams (Pratama et al. 2015b; Pratama et al. 2015c)and do not require re-training whenever a new data sample is received.\nAs foreshadowed, in single-label classification problems, each of the sample data is associated with a unique target class label from a pool of target class labels. Single-label classifiers can be further classified into binary classifiers and multi-class classifiers. Binary classification is the most trivial classification problem in which the input sample belongs to one of the two target class labels. Medical diagnosis, biometric security, and other similar applications are examples of binary classification. When the total number of target class labels is greater than two, it is called multi-class classification. In multi-class classification, each of the input samples corresponds to a unique class among a pool of target class labels. Character recognition (Mohiuddin and Mao 2014), biometric identification (Song et al. 2013; Srivastava et al. 2015), and other related applications are examples of multi-class classification. Several online machine learning classifiers for single-label classification are available in literature (Lughofer and Buchtala 2013; Polikar et al. 2001). Evolving classifiers (Bouchachia 2010; Iglesias et al. 2010) and fuzzy systems based classifiers (Angelov et al. 2008; Lemos et al. 2013; Xydeas et al. 2006) have also been developed for streaming data applications. However, there are several real-world classification problems in which the target labels are not mutually exclusive, and each of the data samples corresponds to more than one target labels resulting in need for multi-label classification. The traditional binary and multi-class classification are special cases of multi-label classification problems. Thus, being the superset of binary and multi-class classification problems, it can be stated that the multi-label classification forms the generalization of all classification problems. Due to its generality, the multi-label classification problems are more difficult and more complex when compared to single-label classification problems (Zhang and Zhou 2007).\nSeveral approaches for solving multi-label problems are available in the literature. But most of the available approaches are based on batch learning techniques. Online techniques for multi-label classification are still greatly to be explored. The paper on streaming multi-label classification by Reed and his team (Read et al. 2011) list out the existing classifiers on multi-label classification for streaming applications. The existing techniques listed belongs to the category of problem transformation methods. In problem transformation methods, the multi-label classification problem is converted into multiple single-label classification problem and it uses existing single-label techniques for classification. The proposed method, on the other hand extends the base algorithm itself to adapt to the multi-label problems. Therefore, the proposed method differs significantly from the existing problem transformation based techniques. It is also to be highlighted that the proposed method is the first extreme learning machine based real-time online multi-label classifier. The proposed method employs a new threshold-based classification for multi-label problems. Unlike single-label classification, the number of target labels differs for every data sample. Therefore, in multi-label classification, both the number of labels and the corresponding labels are unknown. Also, different multi-label datasets differs significantly from each other with respect to label density and label cardinality characteristics. A classifier that performs well in one dataset may not necessarily perform well in a different dataset. Due to the increased complexity of the multilabel classification caused by its generality, the time taken for training the classifier is high for most of the techniques. Also, the highly complex nature of the multi-label classification problems poses a considerable challenge in developing high-speed real-time online classifiers. The application areas of multilabel classification are increasing rapidly due to its generality. Several real world applications require the need for multi-label classification. High-speed processing of streaming data for multi-label classification is highly essential for real-world real-time applications. The proposed work exploits the high-speed nature of extreme learning machines, and a novel online multi-label classifier is developed. The proposed ELM based online multi-label classifier outperforms the existing multi-label classifiers in performance and speed.\nThe rest of the paper is organized as follows. A condensed overview of multi-label classification and different types of multi-label classifiers are discussed in Section 2. Details of the proposed approach are described in Section 3. Section 4 describes the experimental specifications and the different benchmark metrics used for analyzing the multi-label classification datasets. The performance of the proposed\nmethod, performance comparison with existing methods and related discussions are carried out in Section 5. Finally, concluding remarks are given in Section 6."}, {"heading": "2. Multi-label Classifier", "text": "In single-label classifications such as binary and multi-class classification, the target labels of each sample are unique and the target labels are mutually exclusive, i.e. Consider there are M target classes, and pi denotes the probability that the input sample is assigned to ith class. Then, for single-label classification the following equality condition holds true.\n\ufffd\ud835\udc5d\ud835\udc5d\ud835\udc56\ud835\udc56 = 1 (1)\nOn contrary, this equality does not hold true for multi-label problems. Also, from (Elisseeff and Weston 2001), it can be seen that the binary classification problem, multi-class classification problems and ordinal regression problems are special cases of the multi-label problems with the number of target labels corresponding to each of the data sample restricted to 1. The definition of multi-label learning as given by (Sorower 2010) is, \u201cGiven a training set, S = (xi, yi), 1 \u2264 i \u2264 n, consisting of n training instances, (xi \u03f5 X, yi \u03f5 Y) drawn from an unknown distribution D, the goal of multi-label learning is to produce a multilabel classifier h:X\u2192Y that optimizes some specific evaluation function or loss function\u201d.\nThe problem of multi-label learning can be summarized as follows:\n There exists an input space X of feature dimension D. xi \u03f5 X, xi = (xi1,xi2,\u2026.xiD)  There exists a label space L of dimension M. L = {\u03b61, \u03b62,\u2026., \u03b6M}  Consider there are N training samples, each of the training samples can be represented by a pair\nof tuples (input space and label space). {(xi,yi) | xi \u03f5 X, yi \u03f5 Y, Y \u2286 L, 1\u2264i\u2264N}  A training model that maps the input tuple to output tuple.\nThere are several multi-label classifiers available in the literature. The existing techniques can be broadly classified into two categories: Batch learning techniques and online learning techniques. The batch learning based multi-label classifiers are further classified by (Madjarov et al. 2012; Tsoumakas and Katakis 2006) into Problem Transformation (PT) methods, Algorithm Adaptation (AA) methods and Ensemble (EN) methods. There are very limited number of online multi-label classifiers available in the literature. A brief summary of existing multi-label classifiers is discussed in this section. An overview of multilabel methods is shown in Fig. 1."}, {"heading": "2.1 Batch Learning Methods", "text": "There are several batch learning based multi-label classification techniques available in the literature. (Tsoumakas and Katakis 2006) categorized the techniques into two categories: PT methods and AA methods. Later, (Madjarov et al. 2012) extended the classification to include a third category of methods: EN methods."}, {"heading": "2.1.1 Problem Transformation (PT) methods", "text": "PT methods, as the name implies, transform the multi-label classification problems into multiple singlelabel classification problems and employ existing single-label classifiers to perform the classification and finally combines the individual classifier results to provide the multi-label classification results. The PT methods are further divided into three categories: Binary relevance methods (Binary Relevance (BR), Classifier Chaining (CC)), Pairwise methods (Calibrated Label Ranking (CLR), Qweighted multi-label (QWML) approach) and Label powerset method (HOMER)."}, {"heading": "2.1.2 Algorithm Adaptation (AA) Methods", "text": "In AA methods, the base algorithm corresponding to the classification is itself extended to adapt to multilabel problems. AA methods are algorithm-dependent methods. Several AA methods for multi-label classification are available such as Predictive Clustering Trees (PCTs), Multi-label k-nearest neighbors (ML-kNN), ML-C4.5, etc. Techniques like SVM, neural networks, Boosting also have multi-label variants."}, {"heading": "2.1.3 Ensemble (EN) methods", "text": "EN methods employ an ensemble of PT or AA methods. Ensemble Classifier Chains (ECC) is an EN method that uses CC as base technique and forms an ensemble of multiple CC methods to address the multi-label problems. Techniques like PCT, decision trees (DT), ML-C4.5 are used in ensemble with Random Forest (RF) to form RF-PCT, RDT, and RFML-C4.5 respectively. Random-k label sets (RAkEL) uses label power set for classifying each of the label sets."}, {"heading": "2.2 Online Methods", "text": "Due to the complicated nature of multi-label problems, very few works are available in online learning for multi-label classification. Some of the significant works are briefly reviewed. (Crammer 2004) proposed Passive-Aggressive (PA) method for multi-label classification. In the year 2010, (Zhang et al. 2010) proposed a method called Bayesian Online Multi-label Classification (BOMC). Due to its various real life applications, Microsoft focused its research on multi-label classification and developed an online multi-label active learning technique for multimedia applications (Hua and Qi 2008). From the lack of mentioning of any online multi-label classification methods in any of the multi-label review articles thus far, it is evident that there are no generic online multi-label classification techniques that can be applied to a wide range of application domains. The PA and the BOMC techniques are implemented only for text categorization datasets and the Active Learning framework from Microsoft is application specific to multimedia datasets.\nThis paper proposes an ELM based online multi-label classifier that is capable of performing online multi-label classification on streaming data in real-time. There are no online multi-label techniques available in the literature which can perform real-time multi-label classification. The proposed technique is experimented on six datasets from different application domains."}, {"heading": "3. Proposed Approach", "text": "ELM is a single-hidden layer feedforward neural network based learning technique (Ding et al. 2015; Huang et al. 2006). A key feature of ELM is that it maintains the universal approximation capability of\nsingle hidden-layer feedforward neural network. It has gained much attention due to its special nature of random input weight initialization and its unique advantage of extreme learning speed (Wang et al. 2011).In ELM, the initial weights and the hidden layer bias can be selected at random, and the network can be trained for the output weights to perform the classification (Huang 2015; Huang et al. 2011; Ning et al. 2014; Ning et al. 2015; Wang et al. 2014; Wang et al. 2015). This results in a fast learning speed and generalization of performance. The proposed method exploits these advantages of the ELM for online multi-label classification.\nThe pre-processing and post-processing of data is of prime importance in extending ELM technique for online multi-label problems. As opposed to single-label classification in which each of the input samples belongs to only one of the target labels, in multi-label problems, each input sample may belong to one or more samples. Therefore, the classifier should be able to predict both the number of labels an input sample belongs to and each of the target labels that corresponds to the input sample. It is also to be noted that, not all multi-label datasets are equally multi-labelled. The degree of multi-labelness varies among different datasets and different applications. This results in increased complexity of the multi-label problem resulting in much longer training and testing time of the multi-label classification technique. A brief review on ELM and Online Sequential ELM (OS-ELM) is presented to provide basic background information"}, {"heading": "3.1 Extreme Learning Machines", "text": "Consider there are N training samples of the form {(xi,yi)}, xi = [xi1,xi2,\u2026,xin]T \u03f5 Rn and yi = [yi1,yi2,\u2026yim]T. In multi-label classification, each input sample belongs to a subset of labels from the label space given as Y\u2286L, L = {\u03b61, \u03b62,\u2026., \u03b6m}. Let \ud835\udc41\ud835\udc41\ufffd be the number of hidden layer neurons, the output \u2018o\u2019 of the SLFN is given by\n\ufffd\ud835\udefd\ud835\udefd\ud835\udc56\ud835\udc56\ud835\udc54\ud835\udc54\ud835\udc56\ud835\udc56\ufffd\ud835\udc65\ud835\udc65\ud835\udc57\ud835\udc57\ufffd = \ufffd\ud835\udefd\ud835\udefd\ud835\udc56\ud835\udc56\ud835\udc54\ud835\udc54\ufffd\ud835\udc64\ud835\udc64\ud835\udc56\ud835\udc56 . \ud835\udc65\ud835\udc65\ud835\udc57\ud835\udc57 + \ud835\udc4f\ud835\udc4f\ud835\udc56\ud835\udc56\ufffd = \ud835\udc5c\ud835\udc5c\ud835\udc57\ud835\udc57\n\ud835\udc41\ud835\udc41\ufffd\n\ud835\udc56\ud835\udc56=1\n\ud835\udc41\ud835\udc41\ufffd\n\ud835\udc56\ud835\udc56=1\n(2)\nwhere, \u03b2i = [\u03b2i1,\u03b2i2,\u2026\u03b2im]T is the output weight, g(x) is the activation function, wi = [wi1,wi2,\u2026win]T is the input weight and bi is the hidden layer bias.\nThe input weights wi and the hidden layer bias bi are randomly assigned in ELM. Therefore, the network must be trained for \u03b2i such that the output of the network is equal to the target class so that the error difference between the actual output and the predicted output is 0.\n\ufffd\ufffd\ud835\udc5c\ud835\udc5c\ud835\udc57\ud835\udc57 \u2212 \ud835\udc66\ud835\udc66\ud835\udc57\ud835\udc57\ufffd \ud835\udc41\ud835\udc41\ufffd \ud835\udc57\ud835\udc57=1 = 0 (3)\nThus, the ELM classifier output can be as follows:\n\ufffd\ud835\udefd\ud835\udefd\ud835\udc56\ud835\udc56\ud835\udc54\ud835\udc54\ufffd\ud835\udc64\ud835\udc64\ud835\udc56\ud835\udc56 . \ud835\udc65\ud835\udc65\ud835\udc57\ud835\udc57 + \ud835\udc4f\ud835\udc4f\ud835\udc56\ud835\udc56\ufffd = \ud835\udc66\ud835\udc66\ud835\udc57\ud835\udc57\n\ud835\udc41\ud835\udc41\ufffd\n\ud835\udc56\ud835\udc56=1\n(4)\nThe above equation can be written in following matrix form:\nH\u03b2 = Y (5)\nwhere,\n\ud835\udc3b\ud835\udc3b = \ufffd \ud835\udc54\ud835\udc54(\ud835\udc64\ud835\udc641 . \ud835\udc65\ud835\udc651 + \ud835\udc4f\ud835\udc4f1) \u22ef \ud835\udc54\ud835\udc54(\ud835\udc64\ud835\udc64\ud835\udc41\ud835\udc41\ufffd . \ud835\udc65\ud835\udc651 + \ud835\udc4f\ud835\udc4f\ud835\udc41\ud835\udc41\ufffd) \u22ee \u22f1 \u22ee\n\ud835\udc54\ud835\udc54(\ud835\udc64\ud835\udc641 . \ud835\udc65\ud835\udc65\ud835\udc41\ud835\udc41 + \ud835\udc4f\ud835\udc4f1) \u22ef \ud835\udc54\ud835\udc54(\ud835\udc64\ud835\udc64\ud835\udc41\ud835\udc41\ufffd . \ud835\udc65\ud835\udc65\ud835\udc41\ud835\udc41 + \ud835\udc4f\ud835\udc4f\ud835\udc41\ud835\udc41\ufffd) \ufffd \ud835\udc41\ud835\udc41\ud835\udc41\ud835\udc41\ud835\udc41\ud835\udc41\ufffd\n(6)\n\ud835\udefd\ud835\udefd = \ufffd \ud835\udefd\ud835\udefd1\ud835\udc47\ud835\udc47 \u22ee \ud835\udefd\ud835\udefd\ud835\udc41\ud835\udc41\ufffd \ud835\udc47\ud835\udc47 \ufffd\n\ud835\udc41\ud835\udc41\ufffd\ud835\udc41\ud835\udc41\ud835\udc4b\ud835\udc4b\n(7)\n\ud835\udc4c\ud835\udc4c = \ufffd \ud835\udc66\ud835\udc661\ud835\udc47\ud835\udc47 \u22ee \ud835\udc66\ud835\udc66\ud835\udc41\ud835\udc41\ud835\udc47\ud835\udc47 \ufffd \ud835\udc41\ud835\udc41\ud835\udc41\ud835\udc41\ud835\udc4b\ud835\udc4b\n(8)\nThe output weights of the ELM network can be estimated using the equation\n\u03b2 = H+Y (9)\nwhere H+ is the Moore-Penrose inverse of the hidden layer output matrix H, and it can be calculated as follows:\nH+ = (HTH)-1HT (10)\nThe theory and mathematics behind the ELM have been extensively discussed in (Ding et al. 2015; Huang 2015; Huang et al. 2011; Huang et al. 2006) and hence are not re-stated here."}, {"heading": "3.2 The Proposed OSML-ELM", "text": "The various steps involved in the proposed method are briefly stated. The key novelty of the proposed method is that, there are no online multi-label classification techniques available in the literature that can perform classification in real-time on streaming data. The proposed method is the multi-label formulation of the online sequential extreme learning machine and hence called Online Sequential Multi-label ELM (OSML-ELM).\nInitialization of Parameters. Fundamental parameters such as the number of hidden layer neurons and the activation function are initialized. Sigmoidal activation function is used for the experimentation. The problem of overfitting is tackled by using the early stopping technique. In early stopping technique, the point at which the training accuracy increases at the expense of generalization error is identified and further training is stopped. The number of hidden neurons are selected depending upon the nature and complexity of the dataset while preventing the overfitting of data.\nProcessing of Inputs. In traditional single-label problems, the target class will be a single-label associated with the input sample. But, in the multi-label case, each input sample can be associated with more than one class labels. Hence, each of the input samples will have the associated output label as an m-tuple with 0 or 1 representing the belongingness to each of the labels in the label space L. This is a key difference between the inputs available for single-label and multi-label problems. As opposed to single-label classification with a single target label, the multi-label problem has a target label set which is a subset of label space L. The label set denoting the belongingness for each of the labels is converted from unipolar representation to bipolar representation.\nELM Training. The processed input is then supplied to the online sequential variant of ELM technique. Let H be the hidden layer output matrix, \u03b2 be the output weights and Y be the target label, the ELM can\nbe represented in a compact form as H\u03b2 = Y where Y\u2286L, L = {\u03b61, \u03b62,\u2026., \u03b6m}. During the training phase, Let N0 be the number of samples in the initial block of data that is provided to the network. The initial output weight \u03b20 is calculated from equation 9 and 10.\n\u03b2 = H+Y and H+ = (HTH)-1HT,\nConsider M0 = (H0TH0)-1, therefore, \u03b20 = M0H0TY0.\nFor each of the subsequent sequentially arriving data, the output weights can be updated by incorporating the recursive least square algorithm with the ELM learning as\n\ud835\udc74\ud835\udc74\ud835\udc8c\ud835\udc8c+\ud835\udfcf\ud835\udfcf = \ud835\udc74\ud835\udc74\ud835\udc8c\ud835\udc8c \u2212 \ud835\udc74\ud835\udc74\ud835\udc8c\ud835\udc8c\ud835\udc89\ud835\udc89\ud835\udc8c\ud835\udc8c+\ud835\udfcf\ud835\udfcf\ud835\udc89\ud835\udc89\ud835\udc8c\ud835\udc8c+\ud835\udfcf\ud835\udfcf\ud835\udc7b\ud835\udc7b \ud835\udc74\ud835\udc74\ud835\udc8c\ud835\udc8c \ud835\udfcf\ud835\udfcf + \ud835\udc89\ud835\udc89\ud835\udc8c\ud835\udc8c+\ud835\udfcf\ud835\udfcf\ud835\udc7b\ud835\udc7b \ud835\udc74\ud835\udc74\ud835\udc8c\ud835\udc8c\ud835\udc89\ud835\udc89\ud835\udc8c\ud835\udc8c+\ud835\udfcf\ud835\udfcf\n(11)\n\ud835\udf37\ud835\udf37\ud835\udc8c\ud835\udc8c+\ud835\udfcf\ud835\udfcf = \ud835\udf37\ud835\udf37\ud835\udc8c\ud835\udc8c + \ud835\udc74\ud835\udc74\ud835\udc8c\ud835\udc8c+\ud835\udfcf\ud835\udfcf\ud835\udc89\ud835\udc89\ud835\udc8c\ud835\udc8c+\ud835\udfcf\ud835\udfcf\ufffd\ud835\udc80\ud835\udc80\ud835\udc8c\ud835\udc8c+\ud835\udfcf\ud835\udfcf\ud835\udc7b\ud835\udc7b \u2212 \ud835\udc89\ud835\udc89\ud835\udc8c\ud835\udc8c+\ud835\udfcf\ud835\udfcf\ud835\udc7b\ud835\udc7b \ud835\udf37\ud835\udf37\ud835\udc8c\ud835\udc8c\ufffd (12)\nwhere k = 0,1,2\u2026. N-N0-1.\nThe detailed mathematics and derivation behind the recursive least square based online ELM learning called online-sequential extreme learning machine is discussed in detail in several literatures (Li et al. 2007; Liang et al. 2006).\nELM Testing. In the testing phase, the test data sample is evaluated using the values of \u03b2 obtained during the training phase. The input data that can be a combination of Boolean, discrete and continuous data type is given to the network. The network then computes Y = H\u03b2. The predicted output Y obtained is a set of real numbers of dimension equal to the number of labels.\nPost-processing and Multi-label Identification. The prime step in extending the ELM based technique for online multi-label problems is the post-processing and thresholding. In binary and multi-class classification, each of the input sample belongs to only one target label and, therefore, can be identified as the index of the maximum value in the predicted output. On contrary, in multi-label classification the number of labels each sample belongs to is not constant. Each input sample may belong to one or more than one of the target labels. Therefore, the classifier must predict both the number of labels and each of the corresponding labels for the input data sample. The number of labels corresponding to a data sample is completely unknown. Hence, in the proposed method, a thresholding-based label association is proposed. The threshold value is selected during the training phase such that it maximizes the separation between the family of labels the input belongs to and the family of labels the input does not belong to, based on the raw output values Y. Setting up of the threshold value is of prime importance as it directly affects the performance of the classifier. The L dimensioned raw-predicted output is compared with a unique threshold value. The index values of the predicted output Y which are greater than the fixed threshold represent the belongingness of the input sample to the corresponding class.\nSetting the threshold value is of critical importance. The threshold value is selected such that it maximizes the difference between the category of labels to which the sample belongs to and the category of labels to which the sample does not belong to with respect to the raw output values Y obtained during the training phase. The distribution of the raw output values of Y for categories of labels that the input sample belongs to (YA) and the categories of labels the input sample does not belong to (YB) are identified. Based on the distribution of YA and YB, a threshold value is identified using the formula,\nThreshold value = (min(YA) + max(YB))/2 (13)\nAs a trivial case, the threshold can be set as zero. In which case, the raw predicted output values will be passed as arguments to a bipolar step function. The threshold value is compared with the raw output values of Y estimated by the classifier, and the number of target labels corresponding to the data sample\nis identified. Then, based on the threshold value, the subset of labels that corresponds to the input data sample is recognized. The threshold value is determined by analyzing the distribution of the raw predicted output values during the training phase. From the distribution, a particular value is chosen that maximizes the separation between the two categories of the labels. The proposed method belongs to the category of algorithm adaptation method, where the base algorithm is adapted to perform multi-label classification problems. It is to be highlighted that there are no ELM-based online multi-label classifiers in the literature thus far. The proposed method is the first to adapt the ELM for online multi-label problems and make extensive experimentation, results comparison and analysis with the state-of-the-art techniques. The overview of the proposed algorithm is summarized.\nAlgorithm: Proposed OSML-ELM algorithm for multi-label classification\n1. The parameters of the network are initialized\n2. The raw input data is processed for classification\n3. ELM Training \u2013 Initial phase Processing of initial block of data M0 = (H0TH0)-1 \u03b20 = M0H0TY0\n4. ELM Training \u2013 Sequential phase Online processing of sequential data\n\ud835\udc74\ud835\udc74\ud835\udc8c\ud835\udc8c+\ud835\udfcf\ud835\udfcf = \ud835\udc74\ud835\udc74\ud835\udc8c\ud835\udc8c \u2212 \ud835\udc74\ud835\udc74\ud835\udc8c\ud835\udc8c\ud835\udc89\ud835\udc89\ud835\udc8c\ud835\udc8c+\ud835\udfcf\ud835\udfcf\ud835\udc89\ud835\udc89\ud835\udc8c\ud835\udc8c+\ud835\udfcf\ud835\udfcf\ud835\udc7b\ud835\udc7b \ud835\udc74\ud835\udc74\ud835\udc8c\ud835\udc8c \ud835\udfcf\ud835\udfcf + \ud835\udc89\ud835\udc89\ud835\udc8c\ud835\udc8c+\ud835\udfcf\ud835\udfcf\ud835\udc7b\ud835\udc7b \ud835\udc74\ud835\udc74\ud835\udc8c\ud835\udc8c\ud835\udc89\ud835\udc89\ud835\udc8c\ud835\udc8c+\ud835\udfcf\ud835\udfcf\n\ud835\udf37\ud835\udf37\ud835\udc8c\ud835\udc8c+\ud835\udfcf\ud835\udfcf = \ud835\udf37\ud835\udf37\ud835\udc8c\ud835\udc8c + \ud835\udc74\ud835\udc74\ud835\udc8c\ud835\udc8c+\ud835\udfcf\ud835\udfcf\ud835\udc89\ud835\udc89\ud835\udc8c\ud835\udc8c+\ud835\udfcf\ud835\udfcf\ufffd\ud835\udc80\ud835\udc80\ud835\udc8c\ud835\udc8c+\ud835\udfcf\ud835\udfcf\ud835\udc7b\ud835\udc7b \u2212 \ud835\udc89\ud835\udc89\ud835\udc8c\ud835\udc8c+\ud835\udfcf\ud835\udfcf\ud835\udc7b\ud835\udc7b \ud835\udf37\ud835\udf37\ud835\udc8c\ud835\udc8c\ufffd\n5. ELM Testing Estimation of raw output values using Y = H\u03b2\n6. Thresholding Applying the threshold value based on separation between two categories of labels (YA and YB). Threshold value = (min(YA) + max(YB))/2 Identifying the number of labels corresponding to input data sample Identifying the target class labels for the input data sample"}, {"heading": "4. Experimentation", "text": "This section describes the different multi-label dataset metrics and gives the experimental design used to evaluate the proposed method.\nMulti-label datasets have a unique property called the degree of multi-labelness. In order to quantitatively measure the multi-labelness of a dataset, two dataset metrics are available in the literature. They are Label Cardinality (LC) and Label Density (LD). Not all datasets are equally multi-labelled. The number of labels, the number of samples having multiple labels, the average number of labels corresponding to a particular sample varies among different datasets resulting in a varied degree of multi-labelness to a dataset.\nConsider there are N training samples and the dataset is of the form {(xi,yi)} where xi is the input data and yi is the target label set. The target label set is a subset of labels from the label space L of dimensinality m, given as Y\u2286L, L = {\u03b61, \u03b62\u2026 \u03b6m}.\nDefinition 4.1 (Tsoumakas and Katakis 2006) Label Cardinality of the dataset is the average number of labels of the examples in the dataset.\n\ud835\udc3f\ud835\udc3f\ud835\udc3f\ud835\udc3f\ud835\udc4f\ud835\udc4f\ud835\udc3f\ud835\udc3f\ud835\udc3f\ud835\udc3f \u2212 \ud835\udc36\ud835\udc36\ud835\udc3f\ud835\udc3f\ud835\udc36\ud835\udc36\ud835\udc36\ud835\udc36\ud835\udc36\ud835\udc36\ud835\udc36\ud835\udc36\ud835\udc3f\ud835\udc3f\ud835\udc3f\ud835\udc3f\ud835\udc36\ud835\udc36\ud835\udc36\ud835\udc36\ud835\udc66\ud835\udc66 = 1 \ud835\udc41\ud835\udc41\n\ufffd|\ud835\udc4c\ud835\udc4c\ud835\udc56\ud835\udc56| \ud835\udc41\ud835\udc41 \ud835\udc56\ud835\udc56=1\n(14)\nLabel Cardinality is independent of the number of labels present in the dataset and signifies the average number of labels present in the dataset.\nDefinition 4.2 (Tsoumakas and Katakis 2006) Label Density of the dataset is the average number of labels of the examples in the dataset divided by |m| where m is the dimensionality of label set L.\n\ud835\udc3f\ud835\udc3f\ud835\udc3f\ud835\udc3f\ud835\udc4f\ud835\udc4f\ud835\udc3f\ud835\udc3f\ud835\udc3f\ud835\udc3f \u2212 \ud835\udc37\ud835\udc37\ud835\udc3f\ud835\udc3f\ud835\udc36\ud835\udc36\ud835\udc37\ud835\udc37\ud835\udc36\ud835\udc36\ud835\udc36\ud835\udc36\ud835\udc66\ud835\udc66 = 1 \ud835\udc41\ud835\udc41 \ufffd |\ud835\udc4c\ud835\udc4c\ud835\udc56\ud835\udc56| |\ud835\udc5a\ud835\udc5a|\n\ud835\udc41\ud835\udc41\n\ud835\udc56\ud835\udc56=1\n(15)\nLabel density takes into consideration the number of labels present in the dataset. Bernardini et al. (2014) analyzed the effect of label density and label cardinality on multi-label learning. It is to be noted that, two datasets with same label cardinality but different label density can significantly vary and may result in different behavior of the training algorithm (Zhang and Zhou 2007). The influence of label density and label cardinality on multi-label learning is analyzed by (Bernardini et al. 2014).\nThe proposed method is experimented with five benchmark datasets comprising of different application areas such as multimedia, text, and biology. The performance of the proposed method is compared with that of 9 existing methods from batch learning and 1 method from online learning method. The proposed method is experimented with datasets from different application domains and exhibit wide range of label density and label cardinality. The number of target class labels ranges from 6 labels to 374 labels, and the number of features or attributes in the dataset ranges from 103 to 1449. The dataset metrics such as label cardinality varies from as low as 1.07 to as high as 4.24. Label cardinality of 1.07 represents that each of the input samples corresponds to 1.07 labels on average. Label cardinality of 4.24 signifies that each sample on an average corresponds to 4.24 labels. Since the label density is inversely proportional to the number of labels present in the dataset, lower the label density value indicates that only fewer samples correspond to a particular label, thus, posing a challenge for the multi-label techniques to train fast enough so as to learn the target label set within the limited samples. The specifications of the datasets are given in Table 1. The number of samples in each of the dataset used for training and testing phase and the feature dimension are included in the dataset specifications. The datasets are obtained from KEEL multi-label dataset repository.\nThe hamming loss, training and testing time of the proposed method are compared to 9 different multilabel techniques available in the literature. The 9 techniques are chosen such that they are from PT, AA and EN methods. The implementation procedure of all the 9 techniques are adapted from the extensive experimental comparison work on multi-label classifiers by Madjarov and team (Madjarov et al. 2012). Also, the chosen techniques belong to different learning paradigms such as SVM, decision trees, and nearest neighbors. The details of state-of-the-arts multi-label techniques used for result comparison are given in Table 2."}, {"heading": "5. Results and Discussions", "text": "The proposed method is experimented with each of the datasets mentioned in Table 3 and is compared with 9 state-of-the-art multi-label classification techniques. Also, the performance of the proposed method is compared with the state-of-the-art online multi-label technique. This section discusses the results obtained by the proposed method and compares it with the existing methods. The results obtained from the proposed method are evaluated for consistency, performance, and speed."}, {"heading": "5.1 Consistency", "text": "Consistency is a key feature that is essential for any new technique proposed. Any technique proposed should provide consistent results for multiple trials with minimal variance. The consistency of a technique can be identified using cross-validation procedure. Therefore, a 5-fold cross validation and a 10- fold cross validation is performed on the proposed technique for each of the 5 datasets. Since the initial weights are assigned randomly for an ELM based technique, it is critical to evaluate the consistency of the proposed technique.\nThe unique feature of multi-label classification is the possibility of the partial correctness of the classifier. Therefore, calculating the error rate for multi-label problems is not same as that of traditional binary or multi-class problems. One or more of the multiple labels to which the sample instance belongs and/or the number of labels the sample instance belongs can be identified partially correctly resulting in the partial correctness of the classifier. Hence, the hamming loss performance metric is used to quantitatively measure the correctness of the classifier. The hamming loss is a measure of the misclassification rate of the learning technique. The lower the hamming loss, the better is the classification accuracy.\nHamming loss gives the percentage of wrong labels to the total number of labels. It represents the number of times the sample-label pair is misclassified (Madjarov et al. 2012).\nThe hamming loss for an ideal classifier is zero. The hamming loss is calculated using the following expression,\n\ud835\udc3b\ud835\udc3b\ud835\udc3f\ud835\udc3f\ud835\udc5a\ud835\udc5a\ud835\udc5a\ud835\udc5a\ud835\udc36\ud835\udc36\ud835\udc36\ud835\udc36\ud835\udc54\ud835\udc54 \ud835\udc3f\ud835\udc3f\ud835\udc5c\ud835\udc5c\ud835\udc37\ud835\udc37\ud835\udc37\ud835\udc37 = 1 \ud835\udc41\ud835\udc41 \ufffd 1 \ud835\udc5a\ud835\udc5a\n|\ud835\udc40\ud835\udc40\ud835\udc3f\ud835\udc3f\ud835\udc36\ud835\udc36(\ud835\udc65\ud835\udc65\ud835\udc56\ud835\udc56)\u2206 \ud835\udc4c\ud835\udc4c\ud835\udc56\ud835\udc56| \ud835\udc41\ud835\udc41\n\ud835\udc56\ud835\udc56=1\n(16)\nwhere, MLC(xi) denotes the predicted output of the multi-label classifier, and Yi gives the target result to be achieved.\nTo evaluate the consistency of the proposed method, a 5-fold and a 10-fold cross validation of hamming loss metric is carried out for each of the 5 datasets and is tabulated.\nTable 3. Consistency table \u2013 cross validation\nDataset Hamming Loss - 5-fcv Hamming Loss - 10-fcv\nYeast 0.206 \u00b1 0.001 0.206 \u00b1 0.002\nScene 0.098 \u00b1 0.002 0.098 \u00b1 0.002\nCorel5k 0.009 \u00b1 0.000 0.009 \u00b1 0.000\nEnron 0.049 \u00b1 0.001 0.049 \u00b1 0.001\nMedical 0.011 \u00b1 0.001 0.011 \u00b1 0.001\nFrom Table 3, it can be seen that the proposed technique is consistent in its performance over repeated executions and cross validations, thus, demonstrating the consistency of the technique."}, {"heading": "5.2 Performance Metrics", "text": "Due to the possibility of the partial correctness of the classifier result, one specific metric will not be sufficient to quantitatively measure the performance of a technique. Therefore, a set of quantitative performance evaluation metrics is used to validate the performance of the multi-label classifier. The performance metrics used are hamming loss, accuracy, precision, recall and F1-measure. (Madjarov et al. 2012)\nHamming Loss. The hamming loss is a measure of misclassification rate of the learning technique. The lower the hamming loss, the better is the classification accuracy. The correctness of the classification of the learning technique can be analyzed by comparing the hamming loss metric. The definition of the hamming loss and the mathematical equation are foretold in equation 16.\nAccuracy. Accuracy of a classifier is defined as the ratio of the total number of correctly predicted labels to the total number of labels of that sample instance. The accuracy measure can be evaluated using the following expression,\n\ud835\udc34\ud835\udc34\ud835\udc34\ud835\udc34\ud835\udc34\ud835\udc34\ud835\udc34\ud835\udc34\ud835\udc36\ud835\udc36\ud835\udc3f\ud835\udc3f\ud835\udc34\ud835\udc34\ud835\udc66\ud835\udc66 = 1 \ud835\udc41\ud835\udc41 \ufffd\ufffd |\ud835\udc40\ud835\udc40\ud835\udc3f\ud835\udc3f\ud835\udc36\ud835\udc36(\ud835\udc65\ud835\udc65\ud835\udc56\ud835\udc56) \u2229 \ud835\udc4c\ud835\udc4c\ud835\udc56\ud835\udc56| |\ud835\udc40\ud835\udc40\ud835\udc3f\ud835\udc3f\ud835\udc36\ud835\udc36(\ud835\udc65\ud835\udc65\ud835\udc56\ud835\udc56) \u222a \ud835\udc4c\ud835\udc4c\ud835\udc56\ud835\udc56|\n\ufffd \ud835\udc41\ud835\udc41\n\ud835\udc56\ud835\udc56=1\n(17)\nPrecision. Precision is the proportion of the predicted correct labels to the total number of actual labels averaged over all instances. In other words, it is the ratio of true positives to the sum of true positives and false positives averaged over all instances. Precision can be computed as follows,\n\ud835\udc43\ud835\udc43\ud835\udc36\ud835\udc36\ud835\udc3f\ud835\udc3f\ud835\udc34\ud835\udc34\ud835\udc36\ud835\udc36\ud835\udc37\ud835\udc37\ud835\udc36\ud835\udc36\ud835\udc5c\ud835\udc5c\ud835\udc36\ud835\udc36 = 1 \ud835\udc41\ud835\udc41 \ufffd\ufffd |\ud835\udc40\ud835\udc40\ud835\udc3f\ud835\udc3f\ud835\udc36\ud835\udc36(\ud835\udc65\ud835\udc65\ud835\udc56\ud835\udc56) \u2229 \ud835\udc4c\ud835\udc4c\ud835\udc56\ud835\udc56| |\ud835\udc40\ud835\udc40\ud835\udc3f\ud835\udc3f\ud835\udc36\ud835\udc36(\ud835\udc65\ud835\udc65\ud835\udc56\ud835\udc56)| \ufffd\n\ud835\udc41\ud835\udc41\n\ud835\udc56\ud835\udc56=1\n(18)\nRecall. Recall is the proportion of the predicted correct labels to the total number of predicted labels averaged over all instances. In other words, it is the ratio of true positives to the sum of true positives and false negatives averaged over all instances. The expression for recall is given as follows:\n\ud835\udc45\ud835\udc45\ud835\udc3f\ud835\udc3f\ud835\udc34\ud835\udc34\ud835\udc3f\ud835\udc3f\ud835\udc3f\ud835\udc3f\ud835\udc3f\ud835\udc3f = 1 \ud835\udc41\ud835\udc41 \ufffd\ufffd |\ud835\udc40\ud835\udc40\ud835\udc3f\ud835\udc3f\ud835\udc36\ud835\udc36(\ud835\udc65\ud835\udc65\ud835\udc56\ud835\udc56) \u2229 \ud835\udc4c\ud835\udc4c\ud835\udc56\ud835\udc56| |\ud835\udc4c\ud835\udc4c\ud835\udc56\ud835\udc56| \ufffd\n\ud835\udc41\ud835\udc41\n\ud835\udc56\ud835\udc56=1\n(19)\nF1 measure. F1 measure is given by the harmonic mean of Precision and Recall. The expression to evaluate F1 measure is given by,\n\ud835\udc39\ud835\udc391 \u2212\ud835\udc5a\ud835\udc5a\ud835\udc3f\ud835\udc3f\ud835\udc3f\ud835\udc3f\ud835\udc37\ud835\udc37\ud835\udc34\ud835\udc34\ud835\udc36\ud835\udc36\ud835\udc3f\ud835\udc3f = 1 \ud835\udc41\ud835\udc41 \ufffd\ufffd 2 \u2217 |\ud835\udc40\ud835\udc40\ud835\udc3f\ud835\udc3f\ud835\udc36\ud835\udc36(\ud835\udc65\ud835\udc65\ud835\udc56\ud835\udc56) \u2229 \ud835\udc4c\ud835\udc4c\ud835\udc56\ud835\udc56| |\ud835\udc40\ud835\udc40\ud835\udc3f\ud835\udc3f\ud835\udc36\ud835\udc36(\ud835\udc65\ud835\udc65\ud835\udc56\ud835\udc56)| + |\ud835\udc4c\ud835\udc4c\ud835\udc56\ud835\udc56|\n\ufffd \ud835\udc41\ud835\udc41\n\ud835\udc56\ud835\udc56=1\n(19)\nThe proposed method is experimented on five different datasets for the five different performance metrics, and the results are tabulated. From Table 4, it can be seen that, the proposed method has a very low hamming loss and better performance metric measures for a wide range of datasets irrespective of the label density and label cardinality values."}, {"heading": "5.3 Comparison with State-of-the-Arts Techniques", "text": "The performance of the proposed method is compared with nine state-of-the-art techniques as specified in Table 2. Hamming loss performance metric provides the percentage of wrong labels to the total number of labels. Accuracy performance metric provides the ratio of the total number of correctly predicted labels to the total number of labels of that sample instance. Therefore, hamming loss and accuracy are the key performance metrics for evaluating the performance of the proposed method. The hamming loss and accuracy metrics are used to compare the performance of the proposed technique with the state-ofthe-art techniques. The comparison results are given in Fig 2 and Fig 3 respectively.\nHamming loss is the measure of misclassification in the dataset. Lower the hamming loss, better the performance of the classifier. For an ideal classifier, the hamming loss is equal to zero. Accuracy is the ratio of number of correctly predicted labels to the total number of labels for a given sample. Higher the accuracy, better the performance of the classifier. It is evident from the figure that, among the 10 different multi-label classifiers, the proposed method ranks among the top methods for all datasets, thus, outperforming most of the existing state-of-the-art techniques."}, {"heading": "5.4 Comparison of Execution Speed", "text": "The performance of the proposed method in terms of execution speed is evaluated by comparing the training time and the testing time of the algorithm used. The proposed method is applied to 5 different datasets from various application domains and a wide range of label density and label cardinality values. The comparison of training time and testing time of the proposed method with existing state-of-the-art methods are tabulated in Tables 5 and 6.\nFrom Tables 5 and 6, it can be clearly seen that the proposed OSML-ELM outperforms all the existing techniques in terms of execution speed. Despite being an online learning algorithm, the speed of the proposed OSML-ELM is several folds faster than most of the existing batch learning techniques. This high speed nature of the OSML-ELM will enable it to perform real-time multi-label classification on streaming data. It is to be highlighted that there are no existing techniques in the literature that can perform real-time online multi-label classification."}, {"heading": "5.5 Real-Time Classification", "text": "For an online classifier to perform classification in real-time, the time taken for executing a single block of data (epoch) should be very low. If the time taken for processing an epoch is more than the rate of arrival of the sequential data, real-time processing of the streaming data cannot be achieved. From the results obtained for the training time of the classifier, the average time required for the execution of a single block of data can be estimated. The number of epochs is identified by the number of times the sequential learning phase is executed while experimenting with the specific dataset. The average time of execution to process a single block of data for the five different datasets are tabulated.\nFrom Table 7, it can be seen that the proposed OSML-ELM can perform multi-label classification for streaming data applications with high accuracy and high speed. Also, the proposed method is compared with the state-of-the-art online multi-label technique by (Hua and Qi 2008). Since the active learning technique is multimedia specific, scene dataset is used to compare the performance. The paper lists the F1 score of the active learning method for scene dataset to be 0.5612. The proposed method achieves a F1 score of 0.6371 for the same scene dataset and is achieved in real-time streaming data. This shows that the proposed method has performed better in terms of speed and performance over the existing multilabel classifiers. The key advantage of the proposed method is that OSML-ELM is capable of performing multi-label classification in real time. It is to be highlighted that there are no online multi-label classifiers that can perform the multi-label classification in real time."}, {"heading": "6. Conclusions", "text": "The proposed OSML-ELM classifier outperforms the existing state-of-the-arts multi-label classification techniques in terms of speed and performance. The application areas of multi-label classification are rapidly increasing due to its generality and several real-world applications require the need for multilabel classification. Due to its increased complexity and wide variations in the characteristics of multilabel datasets based on label density and label cardinality, a classifier that performs well for one dataset might not perform well for a different dataset. Also, high-speed real-time classification of multi-label data is required for real-world applications. The OSML-ELM method is a novel generic real-time multilabel classifier that performs uniformly well on datasets of a wide range of label density, label cardinality and application domains. The performance of the proposed method is compared with five datasets and nine different state-of-the-arts techniques. It can be seen from the results that the proposed OSML-ELM is a key progress in achieving real-time multi-label classification for streaming data applications. The proposed OSML-ELM can be extended further to learn new data labels progressively from the data stream by retaining the previously learnt knowledge without the need for retraining."}], "references": [{"title": "Autonomous Learning Systems: From Data Streams to Knowledge in Real-time", "author": ["P Angelov"], "venue": null, "citeRegEx": "Angelov,? \\Q2012\\E", "shortCiteRegEx": "Angelov", "year": 2012}, {"title": "Evolving fuzzy classifiers using different model architectures Fuzzy Sets and Systems", "author": ["P Angelov", "E Lughofer", "X Zhou"], "venue": null, "citeRegEx": "Angelov et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Angelov et al\\.", "year": 2008}, {"title": "An evolving classification cascade with self-learning Evolving Systems", "author": ["A Bouchachia"], "venue": null, "citeRegEx": "Bouchachia,? \\Q2010\\E", "shortCiteRegEx": "Bouchachia", "year": 2010}, {"title": "Multi-label semantic scene classification", "author": ["M Boutell", "X Shen", "J Luo", "C Brown"], "venue": "dept. comp. sci. u. rochester,", "citeRegEx": "Boutell et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Boutell et al\\.", "year": 2003}, {"title": "Online learning of complex categorical problems", "author": ["YS Crammer"], "venue": "Hebrew University of Jerusalem", "citeRegEx": "Crammer,? \\Q2004\\E", "shortCiteRegEx": "Crammer", "year": 2004}, {"title": "A Tutorial on Multi-label Classification Techniques", "author": ["de Carvalho APLF", "Freitas A"], "venue": "Foundations of Computational Intelligence Volume 5,", "citeRegEx": "APLF and A,? \\Q2009\\E", "shortCiteRegEx": "APLF and A", "year": 2009}, {"title": "Extreme learning machine: algorithm, theory and applications", "author": ["S Ding", "H Zhao", "Y Zhang", "X Xu", "R Nie"], "venue": "Artif Intell Rev", "citeRegEx": "Ding et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Ding et al\\.", "year": 2015}, {"title": "A kernel method for multi-labelled classification", "author": ["A Elisseeff", "J Weston"], "venue": "Advances in neural information processing systems,", "citeRegEx": "Elisseeff and Weston,? \\Q2001\\E", "shortCiteRegEx": "Elisseeff and Weston", "year": 2001}, {"title": "Knowledge discovery from data streams", "author": ["J Gama"], "venue": null, "citeRegEx": "Gama,? \\Q2010\\E", "shortCiteRegEx": "Gama", "year": 2010}, {"title": "A Preliminary Approach to the Multilabel Classification Problem of Portuguese Juridical Documents", "author": ["T Gon\u00e7alves", "P Quaresma"], "venue": "Progress in Artificial Intelligence,", "citeRegEx": "Gon\u00e7alves and Quaresma,? \\Q2003\\E", "shortCiteRegEx": "Gon\u00e7alves and Quaresma", "year": 2003}, {"title": "Online multi-label active learning for large-scale multimedia annotation", "author": ["Hua X-S", "Qi G-J"], "venue": "TechReport MSR-TR-2008-103,", "citeRegEx": "X.S and G.J,? \\Q2008\\E", "shortCiteRegEx": "X.S and G.J", "year": 2008}, {"title": "What are extreme learning machines? Filling the gap between Frank Rosenblatt\u2019s dream and John von Neumann\u2019s puzzle Cognitive Computation", "author": ["Huang G-B"], "venue": null, "citeRegEx": "G.B,? \\Q2015\\E", "shortCiteRegEx": "G.B", "year": 2015}, {"title": "Extreme learning machines: a survey", "author": ["G-B Huang", "D Wang", "Y Lan"], "venue": "Int J Mach Learn & Cyber", "citeRegEx": "Huang et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Huang et al\\.", "year": 2011}, {"title": "Extreme learning machine: Theory and applications Neurocomputing", "author": ["Huang G-B", "Zhu Q-Y", "Siew C-K"], "venue": null, "citeRegEx": "G.B et al\\.,? \\Q2006\\E", "shortCiteRegEx": "G.B et al\\.", "year": 2006}, {"title": "Evolving classification of agents\u2019 behaviors: a general approach Evolving Systems", "author": ["JA Iglesias", "P Angelov", "A Ledezma", "A Sanchis"], "venue": null, "citeRegEx": "Iglesias et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Iglesias et al\\.", "year": 2010}, {"title": "Text categorization with Support Vector Machines: Learning with many relevant features", "author": ["T Joachims"], "venue": "Machine Learning: ECML-98,", "citeRegEx": "Joachims,? \\Q1998\\E", "shortCiteRegEx": "Joachims", "year": 1998}, {"title": "Significance level based multiple tree classification", "author": ["A Karali", "V Pirnat"], "venue": null, "citeRegEx": "Karali and Pirnat,? \\Q1991\\E", "shortCiteRegEx": "Karali and Pirnat", "year": 1991}, {"title": "Evolving connectionist systems: the knowledge engineering approach", "author": ["N Kasabov"], "venue": null, "citeRegEx": "Kasabov,? \\Q2007\\E", "shortCiteRegEx": "Kasabov", "year": 2007}, {"title": "Adaptive fault detection and diagnosis using an evolving fuzzy classifier Information Sciences", "author": ["A Lemos", "W Caminhas", "F Gomide"], "venue": null, "citeRegEx": "Lemos et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Lemos et al\\.", "year": 2013}, {"title": "An improved on-line sequential learning algorithm for extreme learning machine Advances in Neural Networks\u2013ISNN", "author": ["B Li", "J Wang", "Y Li", "Y Song"], "venue": null, "citeRegEx": "Li et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Li et al\\.", "year": 2007}, {"title": "A fast and accurate online sequential learning algorithm for feedforward networks", "author": ["Liang N-Y", "Huang G-B", "Saratchandran P", "Sundararajan N"], "venue": "Neural Networks, IEEE Transactions on 17:14111423", "citeRegEx": "N.Y et al\\.,? \\Q2006\\E", "shortCiteRegEx": "N.Y et al\\.", "year": 2006}, {"title": "Reliable all-pairs evolving fuzzy classifiers Fuzzy Systems, IEEE Transactions on", "author": ["E Lughofer", "O Buchtala"], "venue": null, "citeRegEx": "Lughofer and Buchtala,? \\Q2013\\E", "shortCiteRegEx": "Lughofer and Buchtala", "year": 2013}, {"title": "Evaluation of Two Systems on Multi-class Multi-label Document Classification", "author": ["Luo X", "Zincir-Heywood"], "venue": "Foundations of Intelligent Systems,", "citeRegEx": "X and Zincir.Heywood,? \\Q2005\\E", "shortCiteRegEx": "X and Zincir.Heywood", "year": 2005}, {"title": "An extensive experimental comparison of methods for multi-label learning Pattern Recognition", "author": ["G Madjarov", "D Kocev", "D Gjorgjevikj", "S D\u017eeroski"], "venue": null, "citeRegEx": "Madjarov et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Madjarov et al\\.", "year": 2012}, {"title": "A k-nearest neighbor based algorithm for multi-label classification", "author": ["Z Min-Ling", "Z Zhi-Hua"], "venue": "Granular Computing,", "citeRegEx": "Min.Ling and Zhi.Hua,? \\Q2005\\E", "shortCiteRegEx": "Min.Ling and Zhi.Hua", "year": 2005}, {"title": "A comparative study of different classifiers for handprinted character recognition Pattern Recognition in Practice", "author": ["K Mohiuddin", "J Mao"], "venue": null, "citeRegEx": "Mohiuddin and Mao,? \\Q2014\\E", "shortCiteRegEx": "Mohiuddin and Mao", "year": 2014}, {"title": "Parsimonious Extreme Learning Machine Using Recursive Orthogonal Least Squares Neural Networks and Learning", "author": ["W Ning", "E Meng Joo", "H Min"], "venue": null, "citeRegEx": "Ning et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Ning et al\\.", "year": 2014}, {"title": "Generalized Single-Hidden Layer Feedforward Networks for Regression Problems Neural Networks and Learning Systems", "author": ["W Ning", "E Meng Joo", "H Min"], "venue": "IEEE Transactions on 26:11611176", "citeRegEx": "Ning et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Ning et al\\.", "year": 2015}, {"title": "Learn++: An incremental learning algorithm for supervised neural networks Systems, Man, and Cybernetics, Part C: Applications and Reviews", "author": ["R Polikar", "L Upda", "SS Upda", "V Honavar"], "venue": "IEEE Transactions on 31:497-508", "citeRegEx": "Polikar et al\\.,? \\Q2001\\E", "shortCiteRegEx": "Polikar et al\\.", "year": 2001}, {"title": "Recurrent Classifier based on An Incremental Meta-Cognitivebased Scaffolding Algorithm Fuzzy Systems", "author": ["M Pratama", "S Anavatti", "J Lu"], "venue": "IEEE Transactions on PP:1-1", "citeRegEx": "Pratama et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Pratama et al\\.", "year": 2015}, {"title": "2015b) pClass: An Effective Classifier for Streaming Examples Fuzzy Systems", "author": ["M Pratama", "SG Anavatti", "J Meng", "ED Lughofer"], "venue": "IEEE Transactions on 23:369-386", "citeRegEx": "Pratama et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Pratama et al\\.", "year": 2015}, {"title": "2015c) An incremental meta-cognitive-based scaffolding fuzzy neural network Neurocomputing", "author": ["M Pratama", "J Lu", "S Anavatti", "E Lughofer", "C-P Lim"], "venue": null, "citeRegEx": "Pratama et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Pratama et al\\.", "year": 2015}, {"title": "Evolving Type-2 Fuzzy Classifier", "author": ["M Pratama", "J Lu", "G Zhang"], "venue": "Fuzzy Systems, IEEE Transactions on PP:1-1", "citeRegEx": "Pratama et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Pratama et al\\.", "year": 2015}, {"title": "Streaming Multi-label Classification", "author": ["J Read", "A Bifet", "G Holmes", "B Pfahringer"], "venue": "WAPA,", "citeRegEx": "Read et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Read et al\\.", "year": 2011}, {"title": "Learning in non-stationary environments: methods and applications", "author": ["M Sayed-Mouchaweh", "E Lughofer"], "venue": null, "citeRegEx": "Sayed.Mouchaweh and Lughofer,? \\Q2012\\E", "shortCiteRegEx": "Sayed.Mouchaweh and Lughofer", "year": 2012}, {"title": "Multilabel machine learning and its application to semantic scene classification", "author": ["X Shen", "M Boutell", "J Luo", "C Brown"], "venue": "Electronic Imaging,", "citeRegEx": "Shen et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Shen et al\\.", "year": 2003}, {"title": "System level user behavior biometrics using Fisher features and Gaussian mixture models. In: Security and Privacy Workshops (SPW)", "author": ["Y Song", "M Ben Salem", "S Hershkop", "SJ Stolfo"], "venue": null, "citeRegEx": "Song et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Song et al\\.", "year": 2013}, {"title": "A literature survey on algorithms for multi-label learning Oregon State University, Corvallis", "author": ["MS Sorower"], "venue": null, "citeRegEx": "Sorower,? \\Q2010\\E", "shortCiteRegEx": "Sorower", "year": 2010}, {"title": "Human identification using Linear Multiclass SVM and Eye Movement biometrics", "author": ["N Srivastava", "U Agrawal", "SK Roy", "U Tiwary"], "venue": "Contemporary Computing", "citeRegEx": "Srivastava et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Srivastava et al\\.", "year": 2015}, {"title": "Experiments with multi-label text classifier on the Reuters collection", "author": ["D Tikk", "G Bir\u00f3"], "venue": "Proceedings of the international conference on computational cybernetics (ICCC", "citeRegEx": "Tikk and Bir\u00f3,? \\Q2003\\E", "shortCiteRegEx": "Tikk and Bir\u00f3", "year": 2003}, {"title": "Multi-label classification: An overview Dept of Informatics, Aristotle University of Thessaloniki, Greece", "author": ["G Tsoumakas", "I Katakis"], "venue": null, "citeRegEx": "Tsoumakas and Katakis,? \\Q2006\\E", "shortCiteRegEx": "Tsoumakas and Katakis", "year": 2006}, {"title": "Mining Multi-label Data. In: Maimon O, Rokach L (eds) Data Mining and Knowledge Discovery Handbook", "author": ["G Tsoumakas", "I Katakis", "I Vlahavas"], "venue": null, "citeRegEx": "Tsoumakas et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Tsoumakas et al\\.", "year": 2010}, {"title": "Constructive multi-output extreme learning machine with application to large tanker motion dynamics identification", "author": ["N Wang", "M Han", "N Dong", "MJ Er"], "venue": null, "citeRegEx": "Wang et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Wang et al\\.", "year": 2014}, {"title": "A Novel Extreme Learning Control Framework of Unmanned Surface Vehicles Cybernetics", "author": ["N Wang", "JC Sun", "MJ Er", "YC Liu"], "venue": "IEEE Transactions on PP:1-1", "citeRegEx": "Wang et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Wang et al\\.", "year": 2015}, {"title": "A study on effectiveness of extreme learning machine Neurocomputing", "author": ["Y Wang", "F Cao", "Y Yuan"], "venue": null, "citeRegEx": "Wang et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Wang et al\\.", "year": 2011}, {"title": "Advances in classification of EEG signals via evolving fuzzy classifiers and dependant multiple HMMs Computers in biology and medicine", "author": ["C Xydeas", "P Angelov", "S-Y Chiao", "M Reoullas"], "venue": null, "citeRegEx": "Xydeas et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Xydeas et al\\.", "year": 2006}, {"title": "Multi-label informed latent semantic indexing", "author": ["K Yu", "S Yu", "V Tresp"], "venue": "Paper presented at the Proceedings of the 28th annual international ACM SIGIR conference on Research and development in information retrieval,", "citeRegEx": "Yu et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Yu et al\\.", "year": 2005}, {"title": "ML-KNN: A lazy learning approach to multi-label learning Pattern Recognition", "author": ["Zhang M-L", "Zhou Z-H"], "venue": null, "citeRegEx": "M.L and Z.H,? \\Q2007\\E", "shortCiteRegEx": "M.L and Z.H", "year": 2007}, {"title": "Bayesian online learning for multi-label and multi-variate performance measures", "author": ["X Zhang", "T Graepel", "R Herbrich"], "venue": "In: International Conference on Artificial Intelligence and Statistics,", "citeRegEx": "Zhang et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Zhang et al\\.", "year": 2010}, {"title": "Efficient Approximation Algorithms for Multi-label Map Labeling. In: Algorithms and Computation, vol 1741", "author": ["B Zhu", "CK Poon"], "venue": "Lecture Notes in Computer Science", "citeRegEx": "Zhu and Poon,? \\Q1999\\E", "shortCiteRegEx": "Zhu and Poon", "year": 1999}], "referenceMentions": [{"referenceID": 9, "context": "The application areas of multi-label classification include but are not limited to text categorization (Gon\u00e7alves and Quaresma 2003; Joachims 1998; Luo and Zincir-Heywood 2005; Tikk and Bir\u00f3 2003; Yu et al. 2005), bioinformatics (Elisseeff and Weston 2001; Min-Ling and Zhi-Hua 2005), medical diagnosis (Karali and Pirnat 1991), image/scene and video categorization (Boutell et al.", "startOffset": 103, "endOffset": 212}, {"referenceID": 15, "context": "The application areas of multi-label classification include but are not limited to text categorization (Gon\u00e7alves and Quaresma 2003; Joachims 1998; Luo and Zincir-Heywood 2005; Tikk and Bir\u00f3 2003; Yu et al. 2005), bioinformatics (Elisseeff and Weston 2001; Min-Ling and Zhi-Hua 2005), medical diagnosis (Karali and Pirnat 1991), image/scene and video categorization (Boutell et al.", "startOffset": 103, "endOffset": 212}, {"referenceID": 39, "context": "The application areas of multi-label classification include but are not limited to text categorization (Gon\u00e7alves and Quaresma 2003; Joachims 1998; Luo and Zincir-Heywood 2005; Tikk and Bir\u00f3 2003; Yu et al. 2005), bioinformatics (Elisseeff and Weston 2001; Min-Ling and Zhi-Hua 2005), medical diagnosis (Karali and Pirnat 1991), image/scene and video categorization (Boutell et al.", "startOffset": 103, "endOffset": 212}, {"referenceID": 46, "context": "The application areas of multi-label classification include but are not limited to text categorization (Gon\u00e7alves and Quaresma 2003; Joachims 1998; Luo and Zincir-Heywood 2005; Tikk and Bir\u00f3 2003; Yu et al. 2005), bioinformatics (Elisseeff and Weston 2001; Min-Ling and Zhi-Hua 2005), medical diagnosis (Karali and Pirnat 1991), image/scene and video categorization (Boutell et al.", "startOffset": 103, "endOffset": 212}, {"referenceID": 7, "context": "2005), bioinformatics (Elisseeff and Weston 2001; Min-Ling and Zhi-Hua 2005), medical diagnosis (Karali and Pirnat 1991), image/scene and video categorization (Boutell et al.", "startOffset": 22, "endOffset": 76}, {"referenceID": 24, "context": "2005), bioinformatics (Elisseeff and Weston 2001; Min-Ling and Zhi-Hua 2005), medical diagnosis (Karali and Pirnat 1991), image/scene and video categorization (Boutell et al.", "startOffset": 22, "endOffset": 76}, {"referenceID": 16, "context": "2005), bioinformatics (Elisseeff and Weston 2001; Min-Ling and Zhi-Hua 2005), medical diagnosis (Karali and Pirnat 1991), image/scene and video categorization (Boutell et al.", "startOffset": 96, "endOffset": 120}, {"referenceID": 3, "context": "2005), bioinformatics (Elisseeff and Weston 2001; Min-Ling and Zhi-Hua 2005), medical diagnosis (Karali and Pirnat 1991), image/scene and video categorization (Boutell et al. 2003; Shen et al. 2003), genomics, map labeling (Zhu and Poon 1999), marketing, multimedia, emotion, music categorization, etc.", "startOffset": 159, "endOffset": 198}, {"referenceID": 35, "context": "2005), bioinformatics (Elisseeff and Weston 2001; Min-Ling and Zhi-Hua 2005), medical diagnosis (Karali and Pirnat 1991), image/scene and video categorization (Boutell et al. 2003; Shen et al. 2003), genomics, map labeling (Zhu and Poon 1999), marketing, multimedia, emotion, music categorization, etc.", "startOffset": 159, "endOffset": 198}, {"referenceID": 49, "context": "2003), genomics, map labeling (Zhu and Poon 1999), marketing, multimedia, emotion, music categorization, etc.", "startOffset": 30, "endOffset": 49}, {"referenceID": 41, "context": "In recent years, the multi-label classification has drawn increased research attention due to the realization of the omnipresence of multi-label prediction tasks in several areas (Tsoumakas et al. 2010).", "startOffset": 179, "endOffset": 202}, {"referenceID": 0, "context": "Several books (Angelov 2012; Gama 2010; Kasabov 2007; Sayed-Mouchaweh and Lughofer 2012) are available in the literature that comprehensively elaborates the data stream classification.", "startOffset": 14, "endOffset": 88}, {"referenceID": 8, "context": "Several books (Angelov 2012; Gama 2010; Kasabov 2007; Sayed-Mouchaweh and Lughofer 2012) are available in the literature that comprehensively elaborates the data stream classification.", "startOffset": 14, "endOffset": 88}, {"referenceID": 17, "context": "Several books (Angelov 2012; Gama 2010; Kasabov 2007; Sayed-Mouchaweh and Lughofer 2012) are available in the literature that comprehensively elaborates the data stream classification.", "startOffset": 14, "endOffset": 88}, {"referenceID": 34, "context": "Several books (Angelov 2012; Gama 2010; Kasabov 2007; Sayed-Mouchaweh and Lughofer 2012) are available in the literature that comprehensively elaborates the data stream classification.", "startOffset": 14, "endOffset": 88}, {"referenceID": 25, "context": "Character recognition (Mohiuddin and Mao 2014), biometric identification (Song et al.", "startOffset": 22, "endOffset": 46}, {"referenceID": 36, "context": "Character recognition (Mohiuddin and Mao 2014), biometric identification (Song et al. 2013; Srivastava et al. 2015), and other related applications are examples of multi-class classification.", "startOffset": 73, "endOffset": 115}, {"referenceID": 38, "context": "Character recognition (Mohiuddin and Mao 2014), biometric identification (Song et al. 2013; Srivastava et al. 2015), and other related applications are examples of multi-class classification.", "startOffset": 73, "endOffset": 115}, {"referenceID": 21, "context": "Several online machine learning classifiers for single-label classification are available in literature (Lughofer and Buchtala 2013; Polikar et al. 2001).", "startOffset": 104, "endOffset": 153}, {"referenceID": 28, "context": "Several online machine learning classifiers for single-label classification are available in literature (Lughofer and Buchtala 2013; Polikar et al. 2001).", "startOffset": 104, "endOffset": 153}, {"referenceID": 2, "context": "Evolving classifiers (Bouchachia 2010; Iglesias et al. 2010) and fuzzy systems based classifiers (Angelov et al.", "startOffset": 21, "endOffset": 60}, {"referenceID": 14, "context": "Evolving classifiers (Bouchachia 2010; Iglesias et al. 2010) and fuzzy systems based classifiers (Angelov et al.", "startOffset": 21, "endOffset": 60}, {"referenceID": 1, "context": "2010) and fuzzy systems based classifiers (Angelov et al. 2008; Lemos et al. 2013; Xydeas et al. 2006) have also been developed for streaming data applications.", "startOffset": 42, "endOffset": 102}, {"referenceID": 18, "context": "2010) and fuzzy systems based classifiers (Angelov et al. 2008; Lemos et al. 2013; Xydeas et al. 2006) have also been developed for streaming data applications.", "startOffset": 42, "endOffset": 102}, {"referenceID": 45, "context": "2010) and fuzzy systems based classifiers (Angelov et al. 2008; Lemos et al. 2013; Xydeas et al. 2006) have also been developed for streaming data applications.", "startOffset": 42, "endOffset": 102}, {"referenceID": 33, "context": "The paper on streaming multi-label classification by Reed and his team (Read et al. 2011) list out the existing classifiers on multi-label classification for streaming applications.", "startOffset": 71, "endOffset": 89}, {"referenceID": 7, "context": "Also, from (Elisseeff and Weston 2001), it can be seen that the binary classification problem, multi-class classification problems and ordinal regression problems are special cases of the multi-label problems with the number of target labels corresponding to each of the data sample restricted to 1.", "startOffset": 11, "endOffset": 38}, {"referenceID": 37, "context": "The definition of multi-label learning as given by (Sorower 2010) is, \u201cGiven a training set, S = (xi, yi), 1 \u2264 i \u2264 n, consisting of n training instances, (xi \u03b5 X, yi \u03b5 Y) drawn from an unknown distribution D, the goal of multi-label learning is to produce a multilabel classifier h:X\u2192Y that optimizes some specific evaluation function or loss function\u201d.", "startOffset": 51, "endOffset": 65}, {"referenceID": 23, "context": "The batch learning based multi-label classifiers are further classified by (Madjarov et al. 2012; Tsoumakas and Katakis 2006) into Problem Transformation (PT) methods, Algorithm Adaptation (AA) methods and Ensemble (EN) methods.", "startOffset": 75, "endOffset": 125}, {"referenceID": 40, "context": "The batch learning based multi-label classifiers are further classified by (Madjarov et al. 2012; Tsoumakas and Katakis 2006) into Problem Transformation (PT) methods, Algorithm Adaptation (AA) methods and Ensemble (EN) methods.", "startOffset": 75, "endOffset": 125}, {"referenceID": 40, "context": "(Tsoumakas and Katakis 2006) categorized the techniques into two categories: PT methods and AA methods.", "startOffset": 0, "endOffset": 28}, {"referenceID": 23, "context": "Later, (Madjarov et al. 2012) extended the classification to include a third category of methods: EN methods.", "startOffset": 7, "endOffset": 29}, {"referenceID": 4, "context": "(Crammer 2004) proposed Passive-Aggressive (PA) method for multi-label classification.", "startOffset": 0, "endOffset": 14}, {"referenceID": 48, "context": "In the year 2010, (Zhang et al. 2010) proposed a method called Bayesian Online Multi-label Classification (BOMC).", "startOffset": 18, "endOffset": 37}, {"referenceID": 6, "context": "ELM is a single-hidden layer feedforward neural network based learning technique (Ding et al. 2015; Huang et al. 2006).", "startOffset": 81, "endOffset": 118}, {"referenceID": 44, "context": "It has gained much attention due to its special nature of random input weight initialization and its unique advantage of extreme learning speed (Wang et al. 2011).", "startOffset": 144, "endOffset": 162}, {"referenceID": 12, "context": "In ELM, the initial weights and the hidden layer bias can be selected at random, and the network can be trained for the output weights to perform the classification (Huang 2015; Huang et al. 2011; Ning et al. 2014; Ning et al. 2015; Wang et al. 2014; Wang et al. 2015).", "startOffset": 165, "endOffset": 268}, {"referenceID": 26, "context": "In ELM, the initial weights and the hidden layer bias can be selected at random, and the network can be trained for the output weights to perform the classification (Huang 2015; Huang et al. 2011; Ning et al. 2014; Ning et al. 2015; Wang et al. 2014; Wang et al. 2015).", "startOffset": 165, "endOffset": 268}, {"referenceID": 27, "context": "In ELM, the initial weights and the hidden layer bias can be selected at random, and the network can be trained for the output weights to perform the classification (Huang 2015; Huang et al. 2011; Ning et al. 2014; Ning et al. 2015; Wang et al. 2014; Wang et al. 2015).", "startOffset": 165, "endOffset": 268}, {"referenceID": 42, "context": "In ELM, the initial weights and the hidden layer bias can be selected at random, and the network can be trained for the output weights to perform the classification (Huang 2015; Huang et al. 2011; Ning et al. 2014; Ning et al. 2015; Wang et al. 2014; Wang et al. 2015).", "startOffset": 165, "endOffset": 268}, {"referenceID": 43, "context": "In ELM, the initial weights and the hidden layer bias can be selected at random, and the network can be trained for the output weights to perform the classification (Huang 2015; Huang et al. 2011; Ning et al. 2014; Ning et al. 2015; Wang et al. 2014; Wang et al. 2015).", "startOffset": 165, "endOffset": 268}, {"referenceID": 6, "context": "The theory and mathematics behind the ELM have been extensively discussed in (Ding et al. 2015; Huang 2015; Huang et al. 2011; Huang et al. 2006) and hence are not re-stated here.", "startOffset": 77, "endOffset": 145}, {"referenceID": 12, "context": "The theory and mathematics behind the ELM have been extensively discussed in (Ding et al. 2015; Huang 2015; Huang et al. 2011; Huang et al. 2006) and hence are not re-stated here.", "startOffset": 77, "endOffset": 145}, {"referenceID": 19, "context": "The detailed mathematics and derivation behind the recursive least square based online ELM learning called online-sequential extreme learning machine is discussed in detail in several literatures (Li et al. 2007; Liang et al. 2006).", "startOffset": 196, "endOffset": 231}, {"referenceID": 40, "context": "1 (Tsoumakas and Katakis 2006) Label Cardinality of the dataset is the average number of labels of the examples in the dataset.", "startOffset": 2, "endOffset": 30}, {"referenceID": 40, "context": "2 (Tsoumakas and Katakis 2006) Label Density of the dataset is the average number of labels of the examples in the dataset divided by |m| where m is the dimensionality of label set L.", "startOffset": 2, "endOffset": 30}, {"referenceID": 23, "context": "The implementation procedure of all the 9 techniques are adapted from the extensive experimental comparison work on multi-label classifiers by Madjarov and team (Madjarov et al. 2012).", "startOffset": 161, "endOffset": 183}, {"referenceID": 23, "context": "It represents the number of times the sample-label pair is misclassified (Madjarov et al. 2012).", "startOffset": 73, "endOffset": 95}, {"referenceID": 23, "context": "(Madjarov et al. 2012) Hamming Loss.", "startOffset": 0, "endOffset": 22}], "year": 2016, "abstractText": "In this paper, a high-speed online neural network classifier based on extreme learning machines for multi-label classification is proposed. In multi-label classification, each of the input data sample belongs to one or more than one of the target labels. The traditional binary and multi-class classification where each sample belongs to only one target class forms the subset of multi-label classification. Multi-label classification problems are far more complex than binary and multi-class classification problems, as both the number of target labels and each of the target labels corresponding to each of the input samples are to be identified. The proposed work exploits the high-speed nature of the extreme learning machines to achieve real-time multi-label classification of streaming data. A new threshold-based online sequential learning algorithm is proposed for high speed and streaming data classification of multi-label problems. The proposed method is experimented with six different datasets from different application domains such as multimedia, text, and biology. The hamming loss, accuracy, training time and testing time of the proposed technique is compared with nine different state-of-the-art methods. Experimental studies shows that the proposed technique outperforms the existing multi-label classifiers in terms of performance and speed.", "creator": "Acrobat PDFMaker 15 for Word"}}}