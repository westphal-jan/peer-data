{"id": "1609.00150", "review": {"conference": "NIPS", "VERSION": "v1", "DATE_OF_SUBMISSION": "1-Sep-2016", "title": "Reward Augmented Maximum Likelihood for Neural Structured Prediction", "abstract": "A key problem in structured output prediction is direct optimization of the task reward function that matters for test evaluation. This paper presents a simple and computationally efficient approach to incorporate task reward into a maximum likelihood framework. We establish a connection between the log-likelihood and regularized expected reward objectives, showing that at a zero temperature, they are approximately equivalent in the vicinity of the optimal solution. We show that optimal regularized expected reward is achieved when the conditional distribution of the outputs given the inputs is proportional to their exponentiated (temperature adjusted) rewards. Based on this observation, we optimize conditional log-probability of edited outputs that are sampled proportionally to their scaled exponentiated reward. We apply this framework to optimize edit distance in the output label space. Experiments on speech recognition and machine translation for neural sequence to sequence models show notable improvements over a maximum likelihood baseline by using edit distance augmented maximum likelihood.\n\n\n\n\n\n\n\nThe first set of results was obtained from a simple task-based regression model that generates the desired set of parameterizations (invertical or invertical) in a two-dimensional hierarchical model of data storage in the form of the distribution of the outputs. The resulting set of parameterizations is a series of data fields in which each model outputs data that is derived from the resulting model. The model predicts that the output of a new input is always identical to the input in the previous set.\n\nThe models have a set of parameters, including the parameterization value and the original value. These parameters are defined to determine the expected output value of each parameterization (the value of the input in the output set is 0) in the output set. In these parameters the values are the corresponding value of each parameterization and their corresponding values.\nThe data fields are a range of input fields of the parameters (e.g., the input value of the input in the output set is 1) in the output set. This range includes the parameters associated with the input in the output set, the parameters associated with the output in the output set, the parameters associated with the input in the output set, and the parameters associated with the input in the output set, the parameters associated with the input in the output set, and the parameters associated with the input in the output set.\nThe parameters associated with the output in the output set are a range of input fields in which each model outputs data that is derived from the corresponding value. The parameters associated with the input in the output set are a range of input", "histories": [["v1", "Thu, 1 Sep 2016 09:00:19 GMT  (69kb)", "http://arxiv.org/abs/1609.00150v1", "NIPS 2016"], ["v2", "Sat, 10 Sep 2016 01:07:04 GMT  (55kb)", "http://arxiv.org/abs/1609.00150v2", "NIPS 2016"], ["v3", "Wed, 4 Jan 2017 18:10:36 GMT  (48kb)", "http://arxiv.org/abs/1609.00150v3", "NIPS 2016"]], "COMMENTS": "NIPS 2016", "reviews": [], "SUBJECTS": "cs.LG", "authors": ["mohammad norouzi", "samy bengio", "zhifeng chen", "navdeep jaitly", "mike schuster", "yonghui wu", "dale schuurmans"], "accepted": true, "id": "1609.00150"}, "pdf": {"name": "1609.00150.pdf", "metadata": {"source": "CRF", "title": "Reward Augmented Maximum Likelihood for Neural Structured Prediction", "authors": ["Mohammad Norouzi", "Samy Bengio", "Zhifeng Chen", "Navdeep Jaitly", "Mike Schuster", "Yonghui Wu", "Dale Schuurmans"], "emails": [], "sections": [{"heading": null, "text": "ar X\niv :1\n60 9.\n00 15\n0v 1\n[ cs\n.L G\n] 1\nS ep"}, {"heading": "1 Introduction", "text": "Structured output prediction is ubiquitous in machine learning. Recent advances in natural language processing, machine translation, and speech recognition hinge on the development of better discriminative models for structured outputs and sequences. The foundations of learning structured output models were established by seminal work on graph transformer networks [21], conditional random fields (CRFs) [20], and structured large margin methods [40, 38], which demonstrate how generalization performance can be significantly improved, when one considers the joint effects of predictions across multiple output components during training. These models have evolved into their deep neural counterparts [35, 1] by the use of recurrent neural networks (RNN) with LSTM [16] and GRU [8] cells and attention mechanisms [3].\nA key problem in structured output prediction has always been enabling direct optimization of the task reward (loss) that is used for test evaluation. For example, in machine translation one seeks better BLEU scores, and in speech recognition better word error rates. Not\nsurprisingly, almost all task reward metrics are not differentiable, and hence hard to optimize. Neural sequence models (e.g. [35, 3]) use a maximum likelihood (ML) framework to maximize the conditional probability of the ground-truth outputs given corresponding inputs. These models do not explicitly consider the task reward during training, hoping that conditional log-likelihood would serve as a good surrogate for the task reward. Such methods make no distinction between alternative incorrect outputs: log-probability is only measured on the ground-truth input-output pairs, and all alternative outputs are equally penalized, whether near or far from the ground-truth target. We believe that one can improve upon maximum likelihood sequence models, if the difference in the rewards of alternative outputs is taken into account.\nStandard ML training, despite its limitations, enables training deep RNN models leading to revolutionary advances in machine translation [35, 3, 25] and speech recognition [7, 9, 10]. A key property\nof ML training for locally normalized RNN models is that the objective function factorizes into individual loss terms, which could be efficiently optimized using stochastic gradient descend (SGD). In particular, ML training does not require any form of inference or sampling from the model during training, which leads to computationally efficient and easy to implementations. By contrast, one may consider large margin and search-based structured prediction [11] formulations for training RNNs (e.g. [46]). Such methods incorporate some task reward approximation during training, but the behavior of the approximation is not well understood, especially for deep neural nets. Moreover, these methods require some form of inference at training time, which slows down training.\nAlternatively, one can use reinforcement learning (RL) algorithms, such as policy gradient [44], to optimize expected task reward during training [30, 2]. Even though expected task reward seems like a natural objective, direct policy optimization faces significant challenges: unlike ML, the gradient for a mini-batch of training examples is extremely noisy and has a high variance; gradients need to be estimated via sampling from the model, which is a non-stationary distribution; the reward is often sparse in a high-dimensional output space, which makes it difficult to find any high value predictions, preventing learning from getting off the ground; and, finally, maximizing reward does not explicitly consider the supervised labels, which seems inefficient. In fact, all previous attempts at direct policy optimization for structured output prediction has started by bootstrapping from a previously trained ML solution [30, 2, 32] and they use several heuristics and tricks to make learning stable.\nThis paper presents a new approach to task reward optimization that combines the computational efficiency and simplicity of ML with the conceptual advantages of expected reward maximization. Our algorithm called reward augmented maximum likelihood (RML) simply adds a sampling step on top of the typical likelihood objective. Instead of optimizing conditional log-likelihood on training input-output pairs, given each training input, we first sample an output proportional to its exponentiated scaled reward. Then, we optimize log-likelihood on such auxiliary output samples given corresponding inputs. When the reward for an output is defined as its similarity to a ground-truth output, then the output sampling distribution is peaked at the ground-truth output, and its concentration is controlled by a temperature hyper-parameter.\nOur theoretical analysis shows that the RML and regularized expected reward objectives optimize a KL divergence between the exponentiated reward and model distributions in opposite directions. Further, we show that at non-zero temperatures, the gap between the two criteria can be expressed by a difference of variances measured on interpolating distributions. This observation reveals how entropy regularized expected reward can be estimated by sampling from exponentiated scaled rewards, rather than sampling from the model distribution.\nRemarkably, we find that the RML approach achieves significantly improved results over state of the art maximum likelihood RNNs. We show consistent improvement on both speech recognition (TIMIT dataset) and machine translation (WMT\u201914 dataset), where output sequences are sampled according to their edit distance to the ground-truth outputs. Surprisingly, we find that the best performance is achieved with output sampling distributions that put a lot of the weight away from the ground-truth outputs. In fact, in our experiments, the training algorithm rarely sees the original unperturbed outputs. Our results give further evidence that models trained with imperfect outputs and their reward values can improve upon models that are only exposed to a single ground-truth output per input [15, 24, 42]."}, {"heading": "2 Reward augmented maximum likelihood", "text": "Given a dataset of input-output pairs, D \u2261 {(x(i),y\u2217(i))}Ni=1, structured output models learn a parametric score function p\u03b8(y | x), which scores different output hypotheses, y \u2208 Y . We assume that the set of possible output, Y is finite, e.g. English sentences up to a maximum length. In a probabilistic model, the score function is normalized, while in a large-margin model the score may not be normalized. In either case, once the score function is learned, given an input x, the model predicts an output y\u0302 achieving maximal score,\ny\u0302(x) = argmax y p\u03b8(y | x) . (1)\nIf this optimization is intractable, approximate inference (e.g. beam search) is used. We use a reward function r(y,y\u2217) to evaluate different outputs against ground-truth outputs. Given a test dataset D\u2032, one computes \u2211 (x,y\u2217)\u2208D\u2032 r(y\u0302(x),y\n\u2217) as a measure of empirical reward, and models with larger empirical reward are preferred. Ideally, one hopes to optimize empirical reward during training too.\nHowever, since empirical reward is not amenable to numerical optimization, one often considers optimizing alternative differentiable objectives. Maximum likelihood (ML) framework tries to minimize negative log-likelihood of the parameters given the data,\nLML(\u03b8;D) = \u2211\n(x,y\u2217)\u2208D\n\u2212 log p\u03b8(y \u2217 | x) . (2)\nMinimizing this objective, increases the conditional probability of the target outputs, log p\u03b8(y\u2217 | x), while decreasing the conditional probability of alternative wrong outputs. According to this objective, all of the negative outputs are equally wrong, and none is preferred over the rest.\nBy contrast, reinforcement learning (RL) advocates optimizing expected reward (with a maximum entropy regularizer [45, 27]), which is formulated as minimization of the following objective,\nLRL(\u03b8; \u03c4,D) = \u2211\n(x,y\u2217)\u2208D\n{ \u2212 \u03c4H (p\u03b8(y | x))\u2212 \u2211\ny\u2208Y\np\u03b8(y | x) r(y,y \u2217) } , (3)\nwhere r(y,y\u2217) denotes the reward function, e.g. negative edit distance or BLEU score, \u03c4 controls the degree of regularization, and H (p) is the entropy of a distribution p, i.e. H (p(y)) = \u2212 \u2211\ny\u2208Y p(y) log p(y). It is well-known that optimizing LRL(\u03b8; \u03c4) using SGD is challenging because of the large variance of the gradients. Below we describe how ML and RL objectives are related, and propose a hybrid between the two that combines their benefits for supervised learning.\nLet\u2019s define a distribution in the output space, termed exponentiated payoff distribution, that is central in linking ML and RL objectives:\nq(y | y\u2217; \u03c4) = 1\nZ(y\u2217, \u03c4) exp {r(y,y\u2217)/\u03c4} , (4)\nwhere Z(y\u2217, \u03c4) = \u2211\ny\u2208Y exp {r(y,y \u2217)/\u03c4}. One can verify that the global minimum of LRL(\u03b8; \u03c4),\ni.e. optimal regularized expected reward, is achieved when the model distribution matches exactly with the exponentiated payoff distribution, i.e. p\u03b8(y | x) = q(y | y\u2217; \u03c4). To see this, we re-express the objective function in (3) in terms of a KL divergence between p\u03b8(y | x) and q(y | y\u2217; \u03c4),\n\u2211\n(x,y\u2217)\u2208D\nDKL (p\u03b8(y | x) \u2016 q(y | y \u2217; \u03c4)) =\n1 \u03c4 LRL(\u03b8; \u03c4) + constant , (5)\nwhere the constant on the RHS is \u2211\n(x,y\u2217)\u2208D logZ(y \u2217, \u03c4). Thus, the minimum of DKL (p\u03b8 \u2016 q)\nand LRL is achieved when p\u03b8 = q. At \u03c4 = 0, when there is no entropy regularization, the optimal p\u03b8 is a delta distribution, p\u03b8(y | x) = \u03b4(y | y\u2217), where \u03b4(y | y\u2217) = 1 at y = y\u2217 and 0 at y 6= y\u2217. Note that \u03b4(y | y\u2217) is equivalent to the exponentiated payoff distribution in the limit as \u03c4 \u2192 0.\nGoing back to the log-likelihood objective, one can verify that (2) is equivalent to a KL divergence in the opposite direction between a delta distribution \u03b4(y | y\u2217) and the model distribution p\u03b8(y | x),\n\u2211\n(x,y\u2217)\u2208D\nDKL (\u03b4(y | y \u2217) \u2016 p\u03b8(y | x)) = LML(\u03b8) . (6)\nThere is no constant on the RHS, as the entropy of a delta distribution is zero, i.e. H (\u03b4(y | y\u2217)) = 0.\nWe propose a method called reward-augmented maximum likelihood (RML) which generalizes ML by allowing a non-zero temperature parameter in the exponentiated payoff distribution, while still optimizing the KL divergence in the ML direction. The RML objective function takes the form,\nLRML(\u03b8; \u03c4,D) = \u2211\n(x,y\u2217)\u2208D\n{ \u2212 \u2211\ny\u2208Y\nq(y | y\u2217; \u03c4) log p\u03b8(y | x) } , (7)\nwhich can be re-expressed in terms of a KL divergence as follows, \u2211\n(x,y\u2217)\u2208D\nDKL (q(y | y \u2217; \u03c4) \u2016 p\u03b8(y | x)) = LRML(\u03b8; \u03c4) + constant , (8)\nwhere the constant is \u2212 \u2211\n(x,y\u2217)\u2208D H (q(y | y \u2217, \u03c4)). Note that the temperature parameter, \u03c4 \u2265 0,\nserves as a hyper-parameter that controls the smoothness of the optimal distribution around correct\ntargets by taking into account the reward function in the output space. The objective functions LRL(\u03b8; \u03c4) and LRML(\u03b8; \u03c4), have the same global optimum of p\u03b8 , but they optimize a KL divergence in opposite directions. We characterize the difference between these two objectives below, showing that they are equivalent up to their first order Taylor approximations. For optimization convenience, we focus on minimizing LRML(\u03b8; \u03c4) to achieve a good solution for LRL(\u03b8; \u03c4)."}, {"heading": "2.1 Optimization", "text": "Optimizing the reward augmented maximum likelihood (RML) objective, LRML(\u03b8; \u03c4), is straightforward if one can draw unbiased samples from q(y | y\u2217; \u03c4). We can express the gradient of LRML in terms of an expectation over samples from q(y | y\u2217; \u03c4),\n\u2207\u03b8LRML(\u03b8; \u03c4) = Eq(y|y\u2217;\u03c4) [ \u2212\u2207\u03b8 log p\u03b8(y | x) ] . (9)\nThus, to estimate \u2207\u03b8LRML(\u03b8; \u03c4) for a mini-batch of examples for SGD, one draws y samples given mini-batch y\u2217\u2019s and then optimizes log-likelihood on such samples by following the mean gradient. At a temperature \u03c4 = 0, this reduces to always sampling y\u2217, hence ML training with no sampling.\nBy contrast, the gradient of LRL(\u03b8; \u03c4), based on likelihood ratio methods, takes the form,\n\u2207\u03b8LRL(\u03b8; \u03c4) = Ep\u03b8(y|x) [ \u2212\u2207\u03b8 log p\u03b8(y | x) \u00b7 r(y,y \u2217) ] . (10)\nThere are several critical differences between (9) and (10) that make SGD optimization of LRML(\u03b8; \u03c4) more desirable. First, in (9), one has to sample from a stationary distribution, the so called exponentiated payoff distribution, whereas in (10) one has to sample from the model distribution as it is evolving. Not only sampling from the model could slow down training, but also one needs to employ several tricks to get a better estimate of the gradient of LRL [30]. A body of literature in reinforcement learning focuses on reducing the variance of (10) by using smart techniques such as actor-critique methods [36, 12]. Further, the reward is often sparse in a high-dimensional output space, which makes finding any reasonable predictions challenging, when (10) is used to refine a randomly initialized model. Thus, smart model initialization is needed. By contrast, we initialize the models randomly and refine them using (9)."}, {"heading": "2.2 Sampling from the exponentiated payoff distribution", "text": "For computing the gradient of the model, using the RML approach, one needs to sample auxiliary outputs from the exponentiated payoff distribution, q(y | y\u2217; \u03c4). This sampling is the price that we have to pay to learn with rewards. One should contrast this with loss-augmented inference in structured large margin methods, and sampling from the model in RL. We believe sampling outputs proportional to exponentiated rewards is more efficient and effective in many cases.\nExperiments in this paper use reward values defined by either negative Hamming distance or negative edit distance. We sample from q(y | y\u2217; \u03c4) by stratified sampling, where we first select a particular distance, and then sample an output with that distance value. Here we focus on edit distance sampling, as Hamming distance sampling is a simpler special case. Given a sentence y\u2217 of lengthm, we count the number of sentences within an edit distance e, where e \u2208 {0, . . . , 2m}. Then, we reweight the counts by exp{\u2212e/\u03c4} and normalize. Let c(e,m) denote the number of sentences at an edit distance e from a sentence of length m. First, note that a deletion can be thought as a substitution with a nil token. This works out nicely because given a vocabulary of length v, for each insertion we have v options, and for each substitution we have v \u2212 1 options, but including the nil token, there are v options for substitutions too. When e = 1, there are m possible substitutions and m+ 1 insertions. Hence, in total there are (2m+ 1)v sentences at an edit distance of 1. Note, that exact computation of c(e,m) is difficult if we consider all edge cases, for example when there are repetitive words in y\u2217, but ignoring such edge cases we can come up with approximate counts that are reliable for sampling. When e > 1, we estimate c(e,m) by\nc(e,m) =\nm\u2211\ns=0\n( m\ns\n)( m+ e\u2212 2s\ne \u2212 s\n) ve , (11)\nwhere s enumerates over the number of substitutions. Once s tokens are substituted, then those s positions lose their significance, and the insertions before and after such tokens could be merged. Hence, given s substitutions, there are really m\u2212 s reference positions for e\u2212 s possible insertions. Finally, one can sample according to BLEU score or other sequence metrics by importance sampling where the proposal distribution could be edit distance sampling above."}, {"heading": "3 RML analysis", "text": "In the RML framework, we find the model parameters by minimizing the objective (7) instead of optimizing the RL objective, i.e. regularized expected reward in (3). The difference lies in minimizing DKL (q(y | y\u2217; \u03c4) \u2016 p\u03b8(y | x)) instead of DKL (p\u03b8(y | x) \u2016 q(y | y\u2217; \u03c4)). For convenience, let\u2019s refer to q(y | y\u2217; \u03c4) as q, and p\u03b8(y | x) as p. Here, we characterize the difference between the two divergences, DKL (q \u2016 p)\u2212DKL (p \u2016 q), and use this analysis to motivate the RML approach.\nWe will initially consider the KL divergence in its more general form as a Bregman divergence, which will make some of the key properties clearer. A Bregman divergence is defined by a strictly convex, differentiable, closed potential function F : F \u2192 R [5]. Given F and two points p, q \u2208 F , the corresponding Bregman divergence DF : F \u00d7 F \u2192 R+ is defined by\nDF (p \u2016 q) = F (p)\u2212 F (q)\u2212 (p\u2212 q) T \u2207F (q) , (12)\nthe difference between the strictly convex potential at p and its first order Taylor approximation expanded about q. Clearly this definition is not symmetric between p and q. By the strict convexity of F it follows that DF (p \u2016 q) \u2265 0 with DF (p \u2016 q) = 0 if and only if p = q. To characterize the difference between opposite Bregman divergences, we provide a simple result that relates the two directions for an arbitrary Bregman divergence. Let HF denote the Hessian of F .\nProposition 1. For any twice differentiable strictly convex closed potential F , and p, q \u2208 int(F):\nDF (q \u2016 p) = DF (p \u2016 q) + 1 4 (q \u2212 p) T ( HF (b)\u2212HF (a) ) (q \u2212 p) (13)\nfor some a = (1\u2212\u03b1)p+\u03b1q, (0 \u2264 \u03b1 \u2264 12 ), b = (1\u2212\u03b2)q+\u03b2p, (0 \u2264 \u03b2 \u2264 1 2 ). (see supp. material)\nFor probability vectors p, q \u2208 \u2206|Y| and a potential F (p) = \u2212\u03c4H (p), DF (p \u2016 q) = \u03c4DKL (p \u2016 q). Let f\u2217 : R|Y| \u2192 \u2206|Y| denote a normalized exponential operator that takes a real-valued logit vector and turns it into a probability vector. Let r and s denote real-valued logit vectors such that q = f\u2217(r/\u03c4) and p = f\u2217(s/\u03c4). Below, we characterize the gap between DKL (p(y) \u2016 q(y)) and DKL (q(y) \u2016 p(y)) in terms of the difference between s(y) and r(y).\nProposition 2. The KL divergence between p and q in two directions can be expressed as,\nDKL (p \u2016 q) = DKL (q \u2016 p) + 1 4\u03c42 Vary\u223cf\u2217(b/\u03c4) [s(y)\u2212 r(y)]\u2212 1 4\u03c42 Vary\u223cf\u2217(a/\u03c4) [s(y)\u2212 r(y)]\n< DKL (q \u2016 p) + \u2016s\u2212 r\u2016 2 2,\nfor some a = (1\u2212\u03b1)r+\u03b1s, (0 \u2264 \u03b1 \u2264 12 ), b = (1\u2212\u03b2)s+\u03b2r, (0 \u2264 \u03b2 \u2264 1 2 ). (see supp. material)\nGiven Proposition 2, one can relate RL and RML objectives, LRL(\u03b8; \u03c4) (5) and LRML(\u03b8; \u03c4) (8), as,\nLRL = \u03c4LRML+ 1 4\u03c4\n\u2211\n(x,y\u2217)\u2208D\n{ Vary\u223cf\u2217(b/\u03c4) [s(y)\u2212 r(y)]\u2212Vary\u223cf\u2217(a/\u03c4) [s(y)\u2212 r(y)] } , (14)\nwhere s(y) denotes \u03c4 -scaled logits predicted by the model such that p\u03b8(y | x) = f\u2217(s(y)/\u03c4), and r(y) = r(y,y\u2217). The gap between regularized expected reward (5) and \u03c4 -scaled RML criterion (8) is simply a difference of two variances, whose magnitude decreases with increasing regularization. Proposition 2 also shows an opportunity for learning algorithms: if \u03c4 is chosen so that q = f\u2217(r/\u03c4), then f\u2217(a/\u03c4) and f\u2217(b/\u03c4) have lower variance than p (which can always be achieved for sufficiently small \u03c4 provided p is not deterministic), then the expected regularized reward under p, and its gradient for training, can be exactly estimated, in principle, by including the extra variance terms and sampling from more focused distributions than p. Although we have not yet incorporated approximations to the additional variance terms into RML, this is an interesting research direction."}, {"heading": "4 Related Work", "text": "The literature on structure output prediction is now quite vast, with three broad categories: (a) supervised learning approaches that ignore task reward and use only supervision; (b) reinforcement learning approaches that use only task reward and ignore supervision; and (c) hybrid approaches that attempt to exploit both supervision and task reward.\nWork in category (a) includes classical conditional random field approaches and the more recent maximum likelihood training of RNNs. It also includes more recent approaches, such as [6, 19, 34],\nthat ignore task reward, but attempt to perturb the training inputs and supervised training structures in a way that improves the robustness (and hopefully the generalization) of the resulting prediction model. Related are ideas for improving approximate maximum likelihood training for intractable models by passing the gradient calculation through an approximate inference procedure [13, 33]. Although these approaches offer improvements to standard maximum likelihood, we feel they are fundamentally limited by not incorporating task reward. The DAGGER method [31] also focuses on using supervision only, but can be extended by replacing the surrogate loss with a task loss; even then, this approach assumes that an expert is available to label every alternative sequence, which does not fit the current scenario.\nBy contrast, work in category (b) includes reinforcement learning approaches that only consider task reward and do not use any other supervision. Beyond the traditional reinforcement learning approaches, such as policy gradient [44, 37], and actor-critic [36], Q-learning [41], this category includes SEARN [11]. There is some relationship to the work presented here and work on relative entropy policy search [28], and policy optimization via expectation maximization [43] and KLdivergence [17, 39], however none of these bridge the gap between the two directions of the KLdivergence, nor do they consider any supervision data as we do here.\nThis paper clearly falls in category (c), where there is also a substantial body of related work that has considered how to both exploit supervision information and inform training by task reward. We have already mentioned large margin structured output training [38], which explicitly uses supervision but only considers an upper bound surrogate for task loss. Recently, this line of research has been improved to directly consider task reward [14], but is limited to a perceptron like training approach and continues to require reward augmented inference that cannot be efficiently achieved for general task rewards. A recent extension to gradient based training of deep RNN models has recently been achieved for structured prediction [4], but reward augmented inference remains required, and many heuristics were needed to apply the technique in practice. We have also already mentioned work that attempts to maximize task reward by bootstrapping from a maximum likelihood policy [30, 32], but such an approach only makes limited use of supervision. Some work in robotics has considered exploiting supervision as a means to provide indirect sampling guidance to improve policy search methods that maximize task reward [22, 23], but these approaches do not make use of maximum likelihood training. Most relevant is the work [18] which explicitly incorporates supervision in the policy evaluation phase of a policy iteration procedure that otherwise seeks to maximize task reward. Although interesting, this approach only considers a greedy policy form that does not lend itself to being represented as a deep RNN, and has not been applied to structured output prediction.\nOne advantage of the RML framework is its computational efficiency at training time. By contrast, RL and scheduled sampling [6] require sampling from the model, which can slow down the gradient computation by 2\u00d7. Structural SVM requires loss-augmented inference which is often more expensive than sampling from the model. Our framework only requires sampling from a fixed exponentated payoff distribution, which can be thought as a form of input pre-processing. This preprocessing can be parallelized by model training by having a specific thread handling loading the data and augmentation."}, {"heading": "5 Experiments", "text": "We compare our approach, reward augmented maximum likelihood (RML), with standard maximum likelihood (ML) training on sequence prediction tasks using state-of-the-art attention-based recurrent neural networks [35, 3]. Our experiments demonstrate that the RML approach considerably outperforms ML baseline on both speech recognition and machine translation tasks."}, {"heading": "5.1 Speech recognition", "text": "For experiments on speech recognition, we use the TIMIT dataset; a standard benchmark for clean phone recognition. This dataset comprises recordings from different speakers reading ten phonetically rich sentences covering major dialects of American English. We use the standard train / dev / test splits suggested by the Kaldi toolkit [29].\nAs the sequence prediction model, we use an attention-based encoder-decoder recurrent model of [7] with three 256-dimensional LSTM layers for encoding and one 256-dimensional LSTM layer for decoding. We do not modify the neural network architecture or its gradient computation in any way, but we only change the output targets fed into the network for gradient computation and SGD update.\nThe input to the network is a standard sequence of 123-dimensional log-mel filter response statistics. Given each input, we generate new outputs around ground truth targets by sampling according to the exponentiated payoff distribution. We use negative edit distance as the measure of reward. Our output augmentation process allows insertions, deletions, and substitutions.\nAn important hyper-parameter in our framework is the temperature parameter, \u03c4 , controlling the degree of output augmentation. We investigate the impact of this hyper-parameter and report results for \u03c4 selected from a candidate set of \u03c4 \u2208 {0.6, 0.65, 0.7, 0.75, 0.8, 0.85, 0.9, 0.95, 1.0}. At a temperature of \u03c4 = 0, outputs are not augmented at all, but as \u03c4 increases, more augmentation is generated. Figure 1 depicts the fraction of different numbers of edits applied to a sequence of length 20 for different values of \u03c4 . These edits typically include very small number of deletions, and roughly equal number of insertions and substitutions. For insertions and substitutions we uniformly sample elements from a vocabulary of 61 phones. According to Figure 1, at \u03c4 = 0.6, more than 60% of the outputs remain intact, while at \u03c4 = 0.9, almost all target outputs are being augmented with 5 to 9 edits being sampled with a probability larger than 0.1. We note that the augmentation becomes more severe as the outputs get longer.\nThe phone error rates (PER) on both dev and test sets for different values of \u03c4 and the ML baseline are reported in Table 1. Each model is trained and tested 4 times, using different random seeds. In Table 1, we report average PER across the runs, and in parenthesis the difference of average error to minimum and maximum error. We observe that a temperature of \u03c4 = 0.9 provides the best results, outperforming the ML baseline by 2.9% PER on the dev set and 2.3% PER on the test set. The results consistently improve when the temperature increases from 0.6 to 0.9, and they get worse beyond \u03c4 = 0.9. It is surprising to us that not only the model trains with such a large amount of augmentation at \u03c4 = 0.9, but also it significantly improves upon the baseline. Finally, we note that previous work [9, 10] suggests several refinements to improve sequence to sequence models on TIMIT by adding noise to the weights and using more focused forward-moving attention mechanism. While these refinements are interesting and they could be combined with the RML framework, in this work, we do not implement such refinements, and focus specifically on a fair comparison between the ML baseline and the RML method."}, {"heading": "5.2 Machine translation", "text": "We evaluate the effectiveness of the proposed approach on WMT\u201914 English to French machine translation benchmark. Translation quality is assessed using tokenized BLEU score, to be consistent with previous work on neural machine translation [35, 3, 26]. Models are trained on the full 36M sentence pairs from the training set of WMT\u201914, and evaluated on 3003 sentence pairs from newstest-2014 test set. To keep the sampling process efficient and simple on such a large corpus, we again augment output sentences based on edit distance, but we only allow substitution (no insertion or deletion). One may consider insertions and deletions or sampling according to exponentiated sentence BLEU scores, but we leave that to future work.\nAs the conditional sequence prediction model, we use an attention-based encoder-decoder recurrent neural network similar to [3], but we use multi-layer encoder and decoder networks comprising three layers of 1024 LSTM cells. As suggested by [3], for computing the softmax attention vectors, we build a feedforward neural network with 1024 hidden units, which is fed with the last encoder and the first decoder layers. Across all of our experiments we keep the network architecture and the hyper-parameters the same. We find that all of the models achieve their peak performance after about 4 epochs of training, once we anneal the learning rates. To reduce the noise in BLEU score evaluation, we report both peak BLEU score and BLEU score averaged among about 70 evaluations of the model while doing the fifth epoch of training.\nTable 2 summarizes our experimental results on WMT\u201914. We note that our ML translation baseline is quite strong, if not the best among neural machine translation models [35, 3, 26], achieving very competitive performance for a single model. Even given such a strong baseline, the RML approach consistently improves the results. Our best model with a temperature \u03c4 = 0.85 improves average BLEU by 0.4, and best BLEU by 0.35 points, which is a considerable improvement. Again we observe that as we increase the amount of augmentation from \u03c4 = 0.75 to \u03c4 = 0.85 the results consistently get better, and then they start to get worse with more augmentation.\nDetails. We train the models using asynchronous SGD with 12 replicas without momentum. We use mini-batches of size 128. We initially use a learning rate of 0.5, which we exponentially decay down to 0.05 after 800K training steps. We keep evaluating the models between 1.1 and 1.3 million steps and we report average and peak BLEU scores in Table 2. We use a vocabulary 200K words for the source language and 80K for the target language. We shard the 80K-way softmax onto 8 GPUs for speedup. We only consider training sentences that are up to 80 tokens. We replace rare words with several UNK tokens based on their first and last characters. At inference time, we replace UNK tokens in the output sentences by copying source words according to largest attention dimensions. This rare word handling bares some similarity to [26]."}, {"heading": "6 Conclusion", "text": "We presented a learning algorithm for structured output prediction, which generalizes maximum likelihood training by enabling direct optimization of the task evaluation metric. Our method is computationally efficient and simple to implement, and it only requires augmenting the output targets used for training a maximum likelihood model. We present a method for sampling from output augmentations with increasing edit distance, and we show how using such augmented outputs for training improves maximum likelihood models by a considerable margin, on both machine translation and speech recognition tasks. We believe this framework is applicable to a wide range of probabilistic models with arbitrary reward functions. In future work, we intend to explore the applicability of this framework to other probabilistic models on tasks with other evaluations metrics."}, {"heading": "A Proofs", "text": "Proposition 1. For any twice differentiable strictly convex closed potential F , and p, q \u2208 int(F):\nDF (q \u2016 p) = DF (p \u2016 q) + 1 4 (q \u2212 p)T\n( HF (b)\u2212HF (a) ) (q \u2212 p) (15)\nfor some a = (1\u2212 \u03b1)p+ \u03b1q, (0 \u2264 \u03b1 \u2264 1 2 ), b = (1\u2212 \u03b2)q + \u03b2p, (0 \u2264 \u03b2 \u2264 1 2 ).\nProof. Let f(p) denote \u2207F (p) and consider the midpoint q+p 2 . One can express F ( q+p 2 ) by two Taylor expansions around p and q. By Taylor\u2019s theorem there is an a = (1 \u2212 \u03b1)p + \u03b1q for 0 \u2264 \u03b1 \u2264 1\n2 and\nb = \u03b2p+ (1\u2212 \u03b2)q for 0 \u2264 \u03b2 \u2264 1 2 such that\nF ( q+p 2 ) = F (p) + ( q+p 2 \u2212 p)\u22a4f(p) + 1 2 ( q+p 2 \u2212 p)\u22a4HF (a)( q+p 2 \u2212 p) (16)\n= F (q) + ( q+p 2 \u2212 q)\u22a4f(q) + 1 2 ( q+p 2 \u2212 q)\u22a4HF (b)( q+p 2 \u2212 q), (17)\nhence, 2F ( q+p 2 ) = 2F (p) + (q \u2212 p)\u22a4f(p) + 1 4 (q \u2212 p)\u22a4HF (a)(q \u2212 p) (18)\n= 2F (q) + (p\u2212 q)\u22a4f(q) + 1 4 (p\u2212 q)\u22a4HF (b)(p\u2212 q). (19)\nTherefore,\nF (p) + F (q)\u2212 2F ( q+p 2 ) = F (p)\u2212 F (q)\u2212 (p\u2212 q)\u22a4f(q) \u2212 1 4 (p\u2212 q)\u22a4HF (b)(p\u2212 q) (20)\n= F (q)\u2212 F (p)\u2212 (q \u2212 p)\u22a4f(p)\u2212 1 4 (q \u2212 p)\u22a4HF (a)(q \u2212 p) (21) = DF (p \u2016 q)\u2212 1 4 (p\u2212 q)\u22a4HF (b)(p\u2212 q) (22) = DF (q \u2016 p)\u2212 1 4 (q \u2212 p)\u22a4HF (a)(q \u2212 p), (23)\nleading to the result.\nFor the proof of Proposition 2, we first need to introduce a few definitions and background results. A Bregman divergence is defined from a strictly convex, differentiable, closed potential function F : F \u2192 R, whose strictly convex conjugate F \u2217 : F\u2217 \u2192 R is given by F \u2217(r) = supr\u2208F\u3008r, q\u3009\u2212F (q) [5]. Each of these potential functions have corresponding transfers, f : F \u2192 F\u2217 and f\u2217 : F\u2217 \u2192 F , given by the respective gradient maps f = \u2207F and f\u2217 = \u2207F \u2217. A key property is that f\u2217 = f\u22121 [5], which allows one to associate each object q \u2208 F with its transferred image r = f(q) \u2208 F\u2217 and vice versa. The main property of Bregman divergences we exploit is that a divergence between any two domain objects can always be equivalently expressed as a divergence between their transferred images; that is, for any p \u2208 F and q \u2208 F , one has [5]:\nDF (p \u2016 q) = F (p)\u2212 \u3008p, r\u3009+ F \u2217(r) = DF\u2217 (r \u2016 s) , (24)\nDF (q \u2016 p) = F \u2217(s)\u2212 \u3008s, q\u3009+ F (q) = DF\u2217 (s \u2016 r) , (25)\nwhere s= f(p) and r = f(q). These relations also hold if we instead chose s\u2208F\u2217 and r \u2208F\u2217 in the range space, and used p=f\u2217(s) and q=f\u2217(r). In general (24) and (25) are not equal.\nTwo special cases of the potential functions F and F \u2217 are interesting as they give rise to KL divergences. These two cases include F\u03c4 (p) = \u2212\u03c4H (p) and F \u2217\u03c4 (s) = \u03c4 lse(s/\u03c4 ) = \u03c4 log \u2211\ny exp (s(y)/\u03c4 ), where lse(\u00b7) denotes the log-sum-exp operator. The respective gradient maps are f\u03c4 (p) = \u03c4 (log(p) + 1) and f\u2217\u03c4 (s) = f \u2217(s/\u03c4 ) =\n1\u2211 yexp(s(y)/\u03c4) exp(s/\u03c4 ), where f\u2217\u03c4 denotes the normalized exponential operator for 1 \u03c4 -scaled logits. Below, we derive DF\u2217\u03c4 (r \u2016 s) for such F \u2217 \u03c4 :\nDF\u2217\u03c4 (s \u2016 r) = F \u2217 \u03c4 (s)\u2212 F \u2217 \u03c4 (r)\u2212 (s\u2212 r) T\u2207F \u2217\u03c4 (r)\n= \u03c4 lse(s/\u03c4 )\u2212 \u03c4 lse(r/\u03c4 )\u2212 (s\u2212 r)Tf\u2217\u03c4 (r)\n= \u2212 \u03c4 ( (s/\u03c4 \u2212 lse(s/\u03c4 ))\u2212 (r/\u03c4 \u2212 lse(r/\u03c4 )) )T f\u2217\u03c4 (r)\n= \u03c4f\u2217\u03c4 (r) T ( (r/\u03c4 \u2212 lse(r/\u03c4 ))\u2212 (s/\u03c4 \u2212 lse(s/\u03c4 )) )\n= \u03c4f\u2217\u03c4 (r) T ( log f\u2217\u03c4 (r)\u2212 log f \u2217 \u03c4 (s) )\n= \u03c4DKL (f \u2217 \u03c4 (r) \u2016 f \u2217 \u03c4 (s))\n= \u03c4DKL (q \u2016 p)\n(26)\nProposition 2. The KL divergence between p and q in two directions can be expressed as,\nDKL (p \u2016 q) = DKL (q \u2016 p) + 1\n4\u03c42 Vary\u223cf\u2217(b/\u03c4) [s(y)\u2212 r(y)]\u2212 1 4\u03c42 Vary\u223cf\u2217(a/\u03c4) [s(y)\u2212 r(y)] (27)\n< DKL (q \u2016 p) + \u2016s \u2212 r\u2016 2 2, (28)\nfor some a = (1\u2212 \u03b1)r + \u03b1s, (0 \u2264 \u03b1 \u2264 1 2 ), b = (1\u2212 \u03b2)s+ \u03b2r, (0 \u2264 \u03b2 \u2264 1 2 ).\nProof. For a potential function F \u2217\u03c4 (r) = \u03c4 lse(r/\u03c4 ), given (26), one can re-express Proposition 2 by multiplying both sides by \u03c4 as,\nDF\u2217\u03c4 (r \u2016 s) = DF\u2217\u03c4 (s \u2016 r) + 1 4\u03c4 Vary\u223cf\u2217(b/\u03c4) [s(y)\u2212 r(y)]\u2212 1 4\u03c4 Vary\u223cf\u2217(a/\u03c4) [s(y)\u2212 r(y)] . (29)\nMoreover, for the choice of F \u2217\u03c4 (a) = \u03c4 lse(a/\u03c4 ), it is easy to verify that,\nHF\u2217\u03c4 (a) = 1 \u03c4 (Diag(f\u2217\u03c4 (a))\u2212 f \u2217 \u03c4 (a)f \u2217 \u03c4 (a) \u22a4) , (30)\nwhere Diag(v) returns a square matrix the main diagonal of which comprises a vector v. Substituting this form for HF\u2217\u03c4 into the the quadratic terms in (15), one obtains,\n(s\u2212 r)\u22a4HF\u2217\u03c4 (a)(s\u2212 r) = 1 \u03c4 (s\u2212 r)\u22a4\n(\nDiag(f\u2217\u03c4 (a))\u2212 f \u2217 \u03c4 (a)f \u2217 \u03c4 (a) \u22a4 ) (s\u2212 r) (31)\n= 1 \u03c4\n( Ey\u223cf\u2217\u03c4 (a) [ (s(y)\u2212 r(y))2 ] \u2212 Ey\u223cf\u2217\u03c4 (a) [s(y)\u2212 r(y)] 2 ) (32) = 1 \u03c4 Vary\u223cf\u2217\u03c4 (a) [s(y)\u2212 r(y)] . (33)\nFinally, we can substitude (33) and its variant for HF\u2217\u03c4 (b) into (15) to obtain (29) and equivalently (27).\nNext, consider the inequality in (28). Let \u03b4 = s\u2212 r and note that\nDF\u2217\u03c4 (r \u2016 s)\u2212DF\u2217\u03c4 (s \u2016 r) = 1 4\u03c4 \u03b4\u22a4 ( HF\u2217\u03c4 (b)\u2212HF\u2217\u03c4 (a) ) \u03b4 (34)\n= 1 4\u03c4 \u03b4\u22a4Diag(f\u2217\u03c4 (b)\u2212 f \u2217 \u03c4 (a))\u03b4 + 1 4\u03c4\n( \u03b4\u22a4f\u2217\u03c4 (a) )2 \u2212 1 4\u03c4 ( \u03b4\u22a4f\u2217\u03c4 (b) )2\n(35)\n\u2264 1 4\u03c4 \u2016\u03b4\u201622\u2016f \u2217 \u03c4 (b)\u2212 f \u2217 \u03c4 (a)\u2016\u221e + 1 4\u03c4 \u2016\u03b4\u201622\u2016f \u2217 \u03c4 (a)\u2016 2 2 (36) \u2264 1 2\u03c4 \u2016\u03b4\u201622 + 1 4\u03c4 \u2016\u03b4\u201622 (37)\nsince \u2016f\u2217\u03c4 (b)\u2212 f \u2217 \u03c4 (a)\u2016\u221e \u2264 2 and \u2016f \u2217 \u03c4 (a)\u2016 2 2 \u2264 \u2016f \u2217 \u03c4 (a)\u2016 2 1 \u2264 1."}], "references": [], "referenceMentions": [], "year": 2017, "abstractText": "A key problem in structured output prediction is direct optimization of the task<lb>reward function that matters for test evaluation. This paper presents a simple and<lb>computationally efficient approach to incorporate task reward into a maximum<lb>likelihood framework. We establish a connection between the log-likelihood and<lb>regularized expected reward objectives, showing that at a zero temperature, they<lb>are approximately equivalent in the vicinity of the optimal solution. We show<lb>that optimal regularized expected reward is achieved when the conditional distri-<lb>bution of the outputs given the inputs is proportional to their exponentiated (tem-<lb>perature adjusted) rewards. Based on this observation, we optimize conditional<lb>log-probability of edited outputs that are sampled proportionally to their scaled<lb>exponentiated reward. We apply this framework to optimize edit distance in the<lb>output label space. Experiments on speech recognition and machine translation<lb>for neural sequence to sequence models show notable improvements over a maxi-<lb>mum likelihood baseline by using edit distance augmented maximum likelihood.", "creator": "LaTeX with hyperref package"}}}