{"id": "1609.00629", "review": {"conference": "NIPS", "VERSION": "v1", "DATE_OF_SUBMISSION": "2-Sep-2016", "title": "SEBOOST - Boosting Stochastic Learning Using Subspace Optimization Techniques", "abstract": "We present SEBOOST, a technique for boosting the performance of existing stochastic optimization methods. SEBOOST applies a secondary optimization process in the subspace spanned by the last steps and descent directions. The method was inspired by the SESOP optimization method for large-scale problems, and has been adapted for the stochastic learning framework. It can be applied on top of any existing optimization method with no need to tweak the internal algorithm. We show that the method is able to boost the performance of different algorithms, and make them more robust to changes in their hyper-parameters. As the boosting steps of SEBOOST are applied between large sets of descent steps, the additional subspace optimization hardly increases the overall computational burden. We introduce two hyper-parameters that control the balance between the baseline method and the secondary optimization process. The method was evaluated on several deep learning tasks, demonstrating promising results. We show that the secondary optimization method is not only superior to the secondary optimization method, but can be applied on a scale similar to the primary optimization method. The primary optimization method is optimized for all of the time series. The secondary optimization method is not only superior to the secondary optimization method, but can be applied on a scale similar to the primary optimization method. The secondary optimization method is optimized for all of the time series. The primary optimization method is not only superior to the secondary optimization method, but can be applied on a scale similar to the primary optimization method. The secondary optimization method is optimized for all of the time series. We show that the secondary optimization method is not only superior to the secondary optimization method, but can be applied on a scale similar to the primary optimization method. The secondary optimization method is optimized for all of the time series. The secondary optimization method is optimized for all of the time series. The secondary optimization method is optimized for all of the time series.\n\n\n\n\nIn this article, we use a framework to build an independent, automated optimization algorithm. This framework aims to achieve a complete, complete and independent performance improvement at all points. The following are basic steps to build an independent, automated algorithm:\n\nAn optimization method:\nA step-by-step method:\nA step-by-step method:\nA step-by-step method:\nA step-by-step method:\nA step-by-step method:\nA step-by-step method:\nA step-by-step method:\nA step-by-step method", "histories": [["v1", "Fri, 2 Sep 2016 14:48:16 GMT  (422kb,D)", "http://arxiv.org/abs/1609.00629v1", null]], "reviews": [], "SUBJECTS": "cs.CV cs.LG stat.ML", "authors": ["elad richardson", "rom herskovitz", "boris ginsburg", "michael zibulevsky"], "accepted": true, "id": "1609.00629"}, "pdf": {"name": "1609.00629.pdf", "metadata": {"source": "CRF", "title": "SEBOOST \u2013 Boosting Stochastic Learning Using Subspace Optimization Techniques", "authors": ["Elad Richardson", "Rom Herskovitz", "Boris Ginsburg", "Michael Zibulevsky"], "emails": ["eladrich@cs.technion.ac.il", "mzib@cs.technion.ac.il", "fornoch@gmail.com", "boris.ginsburg@gmail.com"], "sections": [{"heading": "1 Introduction", "text": "Stochastic Gradient Descent (SGD) based optimization methods are widely used for many different learning problems. Given some objective function that we want to optimize, a vanilla gradient descent method would simply take some fixed step in the direction of the current gradient. In many learning problems the objective, or loss, function is averaged over the set of given training examples. In that scenario calculating the loss over the entire training set would be expensive, and is therefore approximated on a small batch, resulting in a stochastic algorithm that requires relatively few calculations per step. The simplicity and efficiency of SGD algorithms have made them a standard choice for many learning tasks, and specifically for deep learning [9, 6, 5, 10] . Although the vanilla SGD has no memory of previous steps, they are usually utilized in some way, for example using momentum [13]. Alternatively, the AdaGrad method uses the previous gradients in order to normalize each component in the new gradient adaptively [3], while the ADAM method uses them to estimate an adaptive moment [8]. In this work we utilize the knowledge of previous steps in spirit of the Sequential Subspace Optimization (SESOP) framework [11]. The nature of SESOP allows it to be easily merged with existing algorithms. Several such extensions were introduced over the years to different fields, such as PCD-SESOP and SSF-SESOP, showing state-of-the-art results in their matching fields [4, 16, 15].\nThe core idea of our method is as follows. At every outer iteration we first perform several steps of a baseline stochastic optimization algorithm which are then summed up as an inner cumulative stochastic step. Afterwards, we minimize the objective function over the affine subspace spanned by the cumulative stochastic step, several previous outer steps and optional other directions. The subspace optimization boosts the performance of the baseline algorithm, therefore our method is called the Sequential Subspace Optimization Boosting method (SEBOOST).\n*Equal contribution\nar X\niv :1\n60 9.\n00 62\n9v 1\n[ cs\n.C V\n] 2\nS ep\n2 01"}, {"heading": "2 The algorithm", "text": "As our algorithm tries to find the balance between SGD and SESOP, we start by a brief review of the original algorithms, and then move to the SEBOOST algorithm."}, {"heading": "2.1 Vanilla SGD", "text": "In many different large-scale optimization problems, applying complex optimization methods is not practical. Thus, popular optimization methods for those problems are usually based on a stochastic estimation of the gradient. Let minx\u2208Rn f(x) be some minimization problem, and let g(x) be the gradient of f(x). The general stochastic approach applies the following optimization rule\nxk+1 = xk \u2212 \u03b7g\u2217(xk)\nwhere xi is the result of the ith iteration, \u03b7 is the learning rate and g\u2217(xk) is an approximation of g(xk) obtained using only a small subset (mini-batch) of the training data. These stochastic descent methods have proved themselves in many different problems, specifically in the context of deep learning algorithms, providing a combination of simplicity and speed. Notice that the vanilla SGD algorithm has no memory of previous iterations. Different optimization methods which are based on SGD usually utilize the previous iterations in order to make a more informed descent process."}, {"heading": "2.2 Vanilla SESOP", "text": "The SEquential Subspace OPtimization Method [11, 15] is an optimization technique used for large scale optimization problems. The core idea of SESOP is to perform the optimization of the objective function in the subspace spanned by the current gradient direction and a set of directions obtained from the previous optimization steps. Following the notations in Section 2.1, a subspace structure for SESOP is usually defined based on the following directions:\n1. Gradients: Current gradient and [optionally] older ones {g (xi) : i = k, k \u2212 1, . . . k \u2212 s1} 2. Previous directions: {pi = xi \u2212 xi\u22121 : i = k, k \u2212 1, . . . k \u2212 s2}\nIn the SESOP formulation the current gradient and the last step are mandatory and any other set can be used to enrich the subspace. From a theoretical point of view, one can enrich the subspace by two Nemirovsky directions: A weighted average of the previous gradients and the direction to the starting point. This will provide optimal worst case complexity of the method (see also [12].) Denoting Pk as the set of directions at iteration k, the SESOP algorithm would solve the minimization problem\n\u03b1k = argmin \u03b1 f (xk + Pk\u03b1)\nxk+1 = xk + Pk\u03b1k\nThus SESOP reduces the optimization problem to the subspace spanned by Pk at each iteration. This means that instead of solving an optimization problem in Rn the dimensionality of the subspace is governed by the size of Pk and can be controlled."}, {"heading": "2.3 The SEBOOST algorithm", "text": "As explained in Section 2.1, when dealing with large-scale optimization problems, stochastic learning methods are usually better fitted to the task then many more involved optimization methods. However, when applied correctly those methods can still be used to boost the optimization process and achieve faster convergence rates. We propose to start with some SGD algorithm as a baseline, and then apply a SESOP-like optimization method over it in an alternating manner. The subspace for the SESOP algorithm arises from the descent directions of the baseline, utilizing the previous iterations.\nA description of the method is given in Algorithm 1. Note that the subset of the training data used for the secondary optimization in step 7 isn\u2019t necessarily the same as that of the baseline in step 2, as will be shown in Section 3. Also, note that in step 8 the last added direction is changed, that is done in order to incorporate the step performed by the secondary optimization into the subspace.\nAlgorithm 1 The SEBOOST algorithm 1: for k = 1, . . . do 2: Perform ` steps of baseline stochastic optimization method to get from xk0 to x k `\n3: Add the direction of the cumulative step xk` \u2212 xk0 to the optimization subspace P 4: if Subspace dimension exceeded the limit: dim(P ) > M then 5: Remove oldest direction from the optimization subspace P 6: end if 7: Perform optimization over subspace P to get from xk` to x k+1 0 8: Change the last added direction to p = xk+10 \u2212 xk0 9: end for\nIt is clear that SEBOOST offers an attractive balance between the baseline stochastic steps and the more costly subspace optimizations. Firstly, as the number ` of stochastic steps grows, the effect of subspace optimization over the result subsides, where taking `\u2192\u221e reduces the algorithm back to the baseline method. Secondly, the dimensionality of the subspace optimization problem is governed by the size of P and can be reduced to as few parameters as required. Notice also that as SEBOOST is added on top of baseline stochastic optimization method, it does not require any internal changes to be made to the original algorithm. Thus, it can be applied on top of any such method with minimal implementation cost, while potentially boosting the base method."}, {"heading": "2.4 Enriching the subspace", "text": "Although the core elements of our optimization subspace are the directions of last M \u2212 1 external steps and the new stochastic cumulative direction, many more elements can be added to enrich the subspace.\nAnchor points As only the last (M \u2212 1) directions are saved in our subspace, the subspace has knowledge only of recent history of the optimization process. The subspace might benefit from directions dependent on preceding directions as well. For example, one could think of the overall descent achieved by the algorithm p = xk0 \u2212 x00 as a possible direction, or the descent over the second half of the optimization process p = xk0 \u2212 x k/2 0 .\nWe formulate this idea by defining anchor points. Anchors points are locations chosen throughout the descent process which we fix and update only rarely. For each anchor point ai the direction p = xk0 \u2212 ai is added to the subspace. Different techniques can be chosen for setting and changing the anchors. In our formulation each point is associated with a parameter ri which describes the number of boosting steps between each update of the point. After every ri steps the corresponding point ai is initialized back to the current x. That way we can control the number of iterations before an anchor point becomes irrelevant and is initialized again. Algorithm 2 shows how the anchor points can be added to Algorithm 1, by incorporating it before step 7.\nCurrent gradient As in the SESOP formulation, the current gradient can be added to the subspace.\nMomentum Similarly to the idea of momentum in SGD methods one can save a weighted average of the previous updates and add it to the optimization subspace. Denoting the current momentum as mk and the last step as p = xk+10 \u2212 xk0 , the momentum is updated as mk+1 = \u00b5\u00b7mk + p, where \u00b5 is some hyper-parameter, as in regular SGD momentum. Note that we also found it useful to normalize the anchor directions.\nAlgorithm 2 Controlling anchors in SEBOOST 1: for i = 1, . . . ,#anchors do 2: if ri%k == 0 then 3: Change the anchor ai to xk` 4: end if 5: Normalize the direction p = xk` \u2212 ai and add it to the subspace 6: end for"}, {"heading": "3 Experiments", "text": "Following the recent rise of interest in deep learning tasks we focus our evaluation on different neural networks problems. We start with a small, yet challenging, regression problem and then proceed to the known problems of the MNIST autoencoder and CIFAR-10 classifier. For each problem we compare the results of baseline stochastic methods with our boosted variants, showing that SEBOOST can give significant improvement over the base method. Note that the purpose of our work is not to directly compete with existing methods, but rather to show that SEBOOST can improve each learning method compared to its\u2019 original variant, while preserving the original qualities of these algorithms. The chosen baselines were SGD with momentum, Nesterov\u2019s Accelerated Gradient (NAG) [13] and AdaGrad [3]. The Conjugate Gradient (CG) [7] was used for the subspace optimization.\nOur algorithm was implemented and evaluated using the Torch7 framework [1], and will be publicly available. The main hyper-parameters that were altered during the experiments were:\n\u2022 lrmethod - The learning rate of a baseline method. \u2022 M - Maximal number of old directions. \u2022 ` - Number of baseline steps between each subspace optimization.\nFor all experiments the weight decay was set at 0.0001 and the momentum was fixed at 0.9 for SGD and NAG. Unless stated otherwise, the number of function evaluations for CG was set at 20. The baseline method used a mini-batch of size 100, while the subspace optimization was applied with a mini-batch of size 1000. Note that subspace optimization is applied over a significantly larger batch. That is because while a \u201cbad\u201d stochastic step will be canceled by the next ones, a single secondary step has a bigger effect on the overall result and therefore requires better approximation of the gradient. As the boosting step is applied only between large sets of the base method, the added cost does not hinder the algorithm.\nFor each experiment a different architecture will be defined. We will use the notation a \u2192L b to denote a classic linear layer with a inputs and b outputs followed by a non-linear Tanh function. Notice that when presenting our results we show two different graphs. The right one always shows the error as a function of the number of passes of the baseline algorithms over the data (i.e. epochs), while the left one shows the error as a function of the actual processor time, taking into account the additional work required by the boosted algorithms."}, {"heading": "3.1 Simple regression", "text": "We will start by evaluating our method on a small regression problem. The dataset in question is a set of 20,000 values simulating some continuous function f : R6 \u2192 R. The dataset was divided into 18,000 training examples and 2,000 test examples. The problem was solved using a tiny neural network with the architecture 6\u2192L 12\u2192L 8\u2192L 4\u2192L 1. Although the network size is very small\nthe resulting optimization problem remains challenging and gives clear indication of SEBOOST\u2019s behavior. Figure 1 shows the optimization process for the different methods. In all examples the boosted variant converged faster. Note that the different variants of SEBOOST behave differently, governed by the corresponding baseline."}, {"heading": "3.2 MNIST autoencoder", "text": "One of the classic neural network formulation is that of an autoencoder, a network that tries to learn efficient representation for a given set of data. An autoencoder is usually composed of two parts, the encoder which takes the input and produces the compact representation and the decoder which takes the representation and tries to reconstruct the original input. In our experiment the MNIST dataset was used, with 60,000 training images of size 28\u00d7 28 and 10,000 test images. The encoder was defined as three layer network with an architecture of form 784\u2192L 200\u2192L 100\u2192L 64, with a matching decoder 64\u2192L 100\u2192L 200\u2192L 784. Figure 3 shows the optimization process for the autoencoder problem. A similar trend can be seen to that of experiment 3.1, SEBOOST is able to significantly improve SGD and NAG and shows some improvement over AdaGrad, although not as noticeable. A nice byproduct of working with an autoencoding problem is that one can visualize the quality of the reconstructions as a function of the iterations. Figure 2 shows the change in reconstructions quality for SGD and SESOP-SGD, and shows that the boosting achieved is significant in terms on the actual results."}, {"heading": "3.3 CIFAR-10 classifier", "text": "For classification purposes a standard benchmark is the CIFAR-10 dataset. The dataset is composed of 60,000 images of size 32 \u00d7 32 from 10 different classes, where each class has 6,000 different images. 50,000 images are used for training and 10,000 for testing. In order to check SEBOOST\u2019s ability to deal with large and modern networks the ResNet [6] architecture, winner of the ILSVRC 2015 classification task, is used.\nFigure 4 shows the optimization process and the achieved accuracy for ResNet of depth 32. Note that we did not manually tweak the learning rate as was done in the original paper. While AdaGrad is not\nboosted for this experiment, SGD and NAG achieve significant boosting and reach a better minimum. The boosting step was applied only once every epoch, applying too frequent boosting steps resulted in a less stable optimization and higher minima, while applying infrequent steps also lead to higher minima. Experiment 3.4 shows similar results for MNIST and discusses them."}, {"heading": "3.4 Understanding the hyper-parameters", "text": "SEBOOST introduces two hyper-parameters: ` the number of baseline steps between each subspace optimization andM the number of old directions to use. The purpose of the following two experiments is to measure the effect of those parameters on the achieved result and to give some intuition as to their meaning. All experiments are based on the MNIST autoencoder problem defined in Section 3.2.\nFirst, let us consider the parameter `, which controls the balance between the baseline SGD algorithm and the more involved optimization process. Taking small values of ` results in more steps of the secondary optimization process, however each direction in the subspace is then composed of fewer steps from the stochastic algorithm, making it less stable. Furthermore, recalling that our secondary optimization is more costly than regular optimization steps, applying it too often would hinder the algorithm\u2019s performance. On the other hand, taking large values of ` weakens the effect of SEBOOST over the baseline algorithm.\nFigure 5a shows how ` affects the optimization process. One can see that applying the subspace optimization too frequently increases the algorithm\u2019s runtime and reaches an higher minimum than the other variants, as expected. Although taking a large value of ` reaches a better minimum, taking a value which is too large slows the algorithm. We can see that for this experiment taking ` = 200 balances correctly the trade-offs.\nLet us now consider the effect of M , which governs the size of the subspace in which the secondary optimization is applied. Although taking large values of M allows us to hold more directions and apply the optimization in a larger subspace it also makes the optimization process more involved. Figure 5b shows how M affects the optimization process. Interestingly, the lower M is, the faster the algorithm starts descending. However, larger M values tend to reach better minima. For M = 20 the algorithm reaches the same minimum as M = 50, but starts the descent process faster, making it a good choice for this experiment.\nTo conclude, the introduced hyper-parameters M and ` affect the overall boosting effect achieved by SEBOOST. Both parameters incorporate different trade-offs of the optimization problem and should be considered when using the algorithm. Our own experiments show that a good initialization would be to set ` so the algorithm runs about once or twice per epoch, and to set M between 10 to 20."}, {"heading": "3.5 Investigating the subspace", "text": "One of the key components of SEBOOST is the structure of the subspace in which the optimization is applied. The purpose of the following two experiments is to see how changes in the baseline algorithm, or the addition of more directions, affect the algorithm. All experiments are based on the MNIST autoencoder problem defined in Section 3.2.\nIn the basic formulation of SEBOOST the subspace is composed only from the directions of the baseline algorithm. In Section 3.2 we saw how choosing different baselines affect the algorithm. Another experiment of interest is to see how our algorithm is influenced by changes in the hyperparameters of the baseline algorithm. Figure 6a shows the effect of the learning rate over the baseline algorithms and their boosted variants. It can be seen that the change in the original baseline affects our algorithm, however the impact is noticeably smaller, showing that the algorithm has some robustness to the original learning rate.\nIn Section 2.4 a set of additional directions which can be added to the subspace were defined, these directions can possibly enrich the subspace and improve the optimization process. Figure 6b shows the influence of those directions on the overall result. In SEBOOST-anchors a set of anchor points were added with the r values of 500, 250, 100, 50 and 20. In SEBOOST-momnetum a momentum vector with \u00b5 = 0.9 was used. It can be seen that using the proposed anchor directions can significantly boost the algorithm. The momentum direction is less useful, giving a small boost on its own and actually slightly hinders the performance when used in conjunction with the anchor directions."}, {"heading": "4 Conclusion", "text": "In this paper we presented SEBOOST, a technique for boosting stochastic learning algorithms via a secondary optimization process. The secondary optimization is applied in the subspace spanned by the preceding descent steps, which can be further extended with additional directions. We evaluated SEBOOST on different deep learning tasks, showing the achieved results of our methods compared to their original baselines. We believe that the flexibility of SEBOOST could make it useful for different learning tasks. One can easily change the frequency of the secondary optimization step, ranging from\nfrequent and more risky steps, to the more stable one step per epoch. Changing the baseline algorithm and the structure of the subspace allows us to further alter SEBOOST\u2019s behavior.\nAlthough this is not the focus of our work, an interesting research direction for SEBOOST is that of parallel computing. Similarly to [2, 14], one can look at a framework composed of a single master and a set of workers, where each worker optimizes a local model and the master saves a global set of parameters which is based on the workers. Inspired by SEBOOST, one can take the descent directions from each of the workers and apply a subspace optimization in the spanned subspace, allowing the master to take a more efficient step based on information from each of its workers.\nAnother interesting direction for future work is the investigation of pruning techniques. In our work, when the subspace if fully occupied the oldest direction is simply removed. One might consider more advanced pruning techniques, such as eliminating the direction which contributed the least for the secondary optimization step, or even randomly removing one of the subspace directions. A good pruning technique can potentially have a significant effect on the overall result. These two ideas will be further researched in future work. Overall, we believe SEBOOST provides a promising balance between popular stochastic descent methods and more involved optimization techniques."}, {"heading": "Acknowledgements", "text": "The research leading to these results has received funding from the European Research Council under European Unions Seventh Framework Program, ERC Grant agreement no. 320649 and was supported by the Intel Collaborative Research Institute for Computational Intelligence (ICRI-CI)."}], "references": [{"title": "Torch7: A matlab-like environment for machine learning", "author": ["Ronan Collobert", "Koray Kavukcuoglu", "Cl\u00e9ment Farabet"], "venue": "In BigLearn, NIPS Workshop, number EPFL-CONF-192376,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2011}, {"title": "Large scale distributed deep networks", "author": ["Jeffrey Dean", "Greg Corrado", "Rajat Monga", "Kai Chen", "Matthieu Devin", "Mark Mao", "Andrew Senior", "Paul Tucker", "Ke Yang", "Quoc V Le"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2012}, {"title": "Adaptive subgradient methods for online learning and stochastic optimization", "author": ["John Duchi", "Elad Hazan", "Yoram Singer"], "venue": "The Journal of Machine Learning Research,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2011}, {"title": "Coordinate and subspace optimization methods for linear least squares with non-quadratic regularization", "author": ["Michael Elad", "Boaz Matalon", "Michael Zibulevsky"], "venue": "Applied and Computational Harmonic Analysis,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2007}, {"title": "Rich feature hierarchies for accurate object detection and semantic segmentation", "author": ["Ross Girshick", "Jeff Donahue", "Trevor Darrell", "Jitendra Malik"], "venue": "In Proceedings of the IEEE conference on computer vision and pattern recognition,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2014}, {"title": "Deep residual learning for image recognition", "author": ["Kaiming He", "Xiangyu Zhang", "Shaoqing Ren", "Jian Sun"], "venue": "arXiv preprint arXiv:1512.03385,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2015}, {"title": "Methods of conjugate gradients for solving linear systems, volume", "author": ["Magnus Rudolph Hestenes", "Eduard Stiefel"], "venue": "NBS,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 1952}, {"title": "Adam: A method for stochastic optimization", "author": ["Diederik Kingma", "Jimmy Ba"], "venue": "arXiv preprint arXiv:1412.6980,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2014}, {"title": "Imagenet classification with deep convolutional neural networks. In Advances in neural information processing", "author": ["Alex Krizhevsky", "Ilya Sutskever", "Geoffrey E Hinton"], "venue": null, "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2012}, {"title": "Fully convolutional networks for semantic segmentation", "author": ["Jonathan Long", "Evan Shelhamer", "Trevor Darrell"], "venue": "In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2015}, {"title": "Sequential subspace optimization method for large-scale unconstrained problems", "author": ["Guy Narkiss", "Michael Zibulevsky"], "venue": "Technion-IIT, Department of Electrical Engineering,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2005}, {"title": "Orth-method for smooth convex optimization", "author": ["Arkadi Nemirovski"], "venue": "Izvestia AN SSSR, Transl.: Eng. Cybern. Soviet J. Comput. Syst. Sci,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 1982}, {"title": "On the importance of initialization and momentum in deep learning", "author": ["Ilya Sutskever", "James Martens", "George Dahl", "Geoffrey Hinton"], "venue": "In Proceedings of the 30th international conference on machine learning", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2013}, {"title": "Deep learning with elastic averaging sgd", "author": ["Sixin Zhang", "Anna E Choromanska", "Yann LeCun"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2015}, {"title": "Speeding-up convergence via sequential subspace optimization: Current state and future directions", "author": ["Michael Zibulevsky"], "venue": "arXiv preprint arXiv:1401.0159,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2013}, {"title": "L1-l2 optimization in signal and image processing", "author": ["Michael Zibulevsky", "Michael Elad"], "venue": "Signal Processing Magazine, IEEE,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2010}], "referenceMentions": [{"referenceID": 8, "context": "The simplicity and efficiency of SGD algorithms have made them a standard choice for many learning tasks, and specifically for deep learning [9, 6, 5, 10] .", "startOffset": 141, "endOffset": 154}, {"referenceID": 5, "context": "The simplicity and efficiency of SGD algorithms have made them a standard choice for many learning tasks, and specifically for deep learning [9, 6, 5, 10] .", "startOffset": 141, "endOffset": 154}, {"referenceID": 4, "context": "The simplicity and efficiency of SGD algorithms have made them a standard choice for many learning tasks, and specifically for deep learning [9, 6, 5, 10] .", "startOffset": 141, "endOffset": 154}, {"referenceID": 9, "context": "The simplicity and efficiency of SGD algorithms have made them a standard choice for many learning tasks, and specifically for deep learning [9, 6, 5, 10] .", "startOffset": 141, "endOffset": 154}, {"referenceID": 12, "context": "Although the vanilla SGD has no memory of previous steps, they are usually utilized in some way, for example using momentum [13].", "startOffset": 124, "endOffset": 128}, {"referenceID": 2, "context": "Alternatively, the AdaGrad method uses the previous gradients in order to normalize each component in the new gradient adaptively [3], while the ADAM method uses them to estimate an adaptive moment [8].", "startOffset": 130, "endOffset": 133}, {"referenceID": 7, "context": "Alternatively, the AdaGrad method uses the previous gradients in order to normalize each component in the new gradient adaptively [3], while the ADAM method uses them to estimate an adaptive moment [8].", "startOffset": 198, "endOffset": 201}, {"referenceID": 10, "context": "In this work we utilize the knowledge of previous steps in spirit of the Sequential Subspace Optimization (SESOP) framework [11].", "startOffset": 124, "endOffset": 128}, {"referenceID": 3, "context": "Several such extensions were introduced over the years to different fields, such as PCD-SESOP and SSF-SESOP, showing state-of-the-art results in their matching fields [4, 16, 15].", "startOffset": 167, "endOffset": 178}, {"referenceID": 15, "context": "Several such extensions were introduced over the years to different fields, such as PCD-SESOP and SSF-SESOP, showing state-of-the-art results in their matching fields [4, 16, 15].", "startOffset": 167, "endOffset": 178}, {"referenceID": 14, "context": "Several such extensions were introduced over the years to different fields, such as PCD-SESOP and SSF-SESOP, showing state-of-the-art results in their matching fields [4, 16, 15].", "startOffset": 167, "endOffset": 178}, {"referenceID": 10, "context": "2 Vanilla SESOP The SEquential Subspace OPtimization Method [11, 15] is an optimization technique used for large scale optimization problems.", "startOffset": 60, "endOffset": 68}, {"referenceID": 14, "context": "2 Vanilla SESOP The SEquential Subspace OPtimization Method [11, 15] is an optimization technique used for large scale optimization problems.", "startOffset": 60, "endOffset": 68}, {"referenceID": 11, "context": "This will provide optimal worst case complexity of the method (see also [12].", "startOffset": 72, "endOffset": 76}, {"referenceID": 12, "context": "The chosen baselines were SGD with momentum, Nesterov\u2019s Accelerated Gradient (NAG) [13] and AdaGrad [3].", "startOffset": 83, "endOffset": 87}, {"referenceID": 2, "context": "The chosen baselines were SGD with momentum, Nesterov\u2019s Accelerated Gradient (NAG) [13] and AdaGrad [3].", "startOffset": 100, "endOffset": 103}, {"referenceID": 6, "context": "The Conjugate Gradient (CG) [7] was used for the subspace optimization.", "startOffset": 28, "endOffset": 31}, {"referenceID": 0, "context": "Our algorithm was implemented and evaluated using the Torch7 framework [1], and will be publicly available.", "startOffset": 71, "endOffset": 74}, {"referenceID": 5, "context": "In order to check SEBOOST\u2019s ability to deal with large and modern networks the ResNet [6] architecture, winner of the ILSVRC 2015 classification task, is used.", "startOffset": 86, "endOffset": 89}, {"referenceID": 1, "context": "Similarly to [2, 14], one can look at a framework composed of a single master and a set of workers, where each worker optimizes a local model and the master saves a global set of parameters which is based on the workers.", "startOffset": 13, "endOffset": 20}, {"referenceID": 13, "context": "Similarly to [2, 14], one can look at a framework composed of a single master and a set of workers, where each worker optimizes a local model and the master saves a global set of parameters which is based on the workers.", "startOffset": 13, "endOffset": 20}], "year": 2016, "abstractText": "We present SEBOOST, a technique for boosting the performance of existing stochastic optimization methods. SEBOOST applies a secondary optimization process in the subspace spanned by the last steps and descent directions. The method was inspired by the SESOP optimization method for large-scale problems, and has been adapted for the stochastic learning framework. It can be applied on top of any existing optimization method with no need to tweak the internal algorithm. We show that the method is able to boost the performance of different algorithms, and make them more robust to changes in their hyper-parameters. As the boosting steps of SEBOOST are applied between large sets of descent steps, the additional subspace optimization hardly increases the overall computational burden. We introduce two hyper-parameters that control the balance between the baseline method and the secondary optimization process. The method was evaluated on several deep learning tasks, demonstrating promising results.", "creator": "LaTeX with hyperref package"}}}