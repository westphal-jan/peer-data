{"id": "1609.00777", "review": {"conference": "ACL", "VERSION": "v1", "DATE_OF_SUBMISSION": "3-Sep-2016", "title": "Towards End-to-End Reinforcement Learning of Dialogue Agents for Information Access", "abstract": "This paper proposes \\emph{KB-InfoBot}---a dialogue agent that provides users with an entity from a knowledge base (KB) by interactively asking for its attributes. All components of the KB-InfoBot are trained in an end-to-end fashion using reinforcement learning. Goal-oriented dialogue systems typically need to interact with an external database to access real-world knowledge (e.g., the Knowledge Base). As such, the knowledge base is the same for all aspects of the real world.\n\n\n\nFor instance, the data structure of the KB-InfoBot is provided by the SQL:DB-InfoBot database. To help with that, we want to add additional values (e.g., \"data\") to the database. For example, a data set is specified with an optional value: *data.\nFor example, the user interface (A) is defined as a \"database\" in this section:\nTo provide information about the data structure of a data set, an API (B) is provided by the SQL:DB-InfoBot database. To create a database, we specify the values (B) and the values (C) to the database.\nTo write a database, we specify the values (C) and the values (D) to the database.\nTo create a database, we specify the values (C) and the values (D) to the database.\nTo create a database, we specify the values (C) and the values (D) to the database.\nTo write a database, we specify the values (C) and the values (D) to the database.\nTo write a database, we specify the values (C) and the values (D) to the database.\nTo set the value (C) and the values (D) to the database.\nTo write a database, we specify the values (C) and the values (D) to the database.\nTo create a database, we specify the values (C) and the values (D) to the database.\nTo create a database, we specify the values (C) and the values (D) to the database.\nTo create a database, we specify the values (C) and the values (D) to the database.\nTo create a database, we specify the values (C) and the values (D) to the database.\nTo create a database, we specify the values (C) and the values (D) to", "histories": [["v1", "Sat, 3 Sep 2016 01:02:51 GMT  (2704kb,D)", "http://arxiv.org/abs/1609.00777v1", null], ["v2", "Mon, 31 Oct 2016 21:39:31 GMT  (2654kb,D)", "http://arxiv.org/abs/1609.00777v2", null], ["v3", "Thu, 20 Apr 2017 17:26:35 GMT  (2748kb,D)", "http://arxiv.org/abs/1609.00777v3", "Accepted at ACL 2017"]], "reviews": [], "SUBJECTS": "cs.CL cs.LG", "authors": ["bhuwan dhingra", "lihong li", "xiujun li", "jianfeng gao", "yun-nung chen", "faisal ahmed 0001", "li deng"], "accepted": true, "id": "1609.00777"}, "pdf": {"name": "1609.00777.pdf", "metadata": {"source": "CRF", "title": "End-to-End Reinforcement Learning of Dialogue Agents for Information Access", "authors": ["Bhuwan Dhingra", "Lihong Li", "Xiujun Li", "Jianfeng Gao", "Yun-Nung Chen", "Faisal Ahmed", "Li Deng"], "emails": ["bdhingra@andrew.cmu.edu", "lihongli@microsoft.com", "xiul@microsoft.com", "jfgao@microsoft.com", "vivic@microsoft.com", "fiahmed@microsoft.com", "deng@microsoft.com"], "sections": [{"heading": "1 Introduction", "text": "Goal-oriented dialogue systems help users complete specific tasks, such as booking a flight or searching a database, by interacting with them via\n\u2217Work completed while BD and YNC were with Microsoft.\nnatural language. In this work, we present KBInfoBot, a dialogue agent that identifies entities of interest to the user from a knowledge base (KB), by interactively asking for attributes of that entity which helps constrain the search. Such an agent finds application in interactive search settings. Figure 1 shows a dialogue example between a user searching for a movie and the proposed KBInfoBot.\nA typical goal-oriented dialogue system consists of four basic components: a language understanding (LU) module for predicting user intents and extracting associated slots (Yao et al., 2014; Hakkani-Tu\u0308r et al., 2016; Chen et al., 2016), a dialogue state tracker which tracks the user goal and dialogue history (Henderson et al., 2014; Henderson, 2015), a dialogue policy which selects the next system action based on the current state (Young et al., 2013), and a natural language generator (NLG) for converting dialogue acts into natural language (Wen et al., 2015; Wen et al., 2016a). For successful completion of user goals, it is also necessary to equip the dialogue policy with real-world knowledge from a database. Previous end-to-end systems achieved this by conar X\niv :1\n60 9.\n00 77\n7v 1\n[ cs\n.C L\n] 3\nS ep\n2 01\n6\nstructing a symbolic query from the current belief states of the agent and retrieving results from the database which match the query (Wen et al., 2016b; Williams and Zweig, 2016; Zhao and Eskenazi, 2016). Unfortunately, such operations make the model non-differentiable, and various components in a dialogue system are usually trained separately.\nIn our work, we replace SQL-like queries with a probabilistic framework for inducing a posterior distribution of the user target over KB entities. We build this distribution from the belief tracker multinomials over attribute-values and binomial probabilities of the user not knowing the value of an attribute. The policy network receives as input this full distribution to select its next action. In addition to making the model end-to-end trainable, this operation also provides a principled framework to propagate the uncertainty inherent in language understanding to the dialogue policy.\nOur entire model is differentiable, which means that in theory our system can be trained completely end-to-end using only a reinforcement signal from the user that indicates whether a dialogue is successful or not. However, in practice, we find that with random initialization the agent is unable to see any rewards if the database is large; even when it does, credit assignment is tough. Hence, at the beginning of training, we first have an imitation-learning phase (Argall et al., 2009) where both the belief tracker and policy network are trained to mimic a rule-based agent. Then, on switching to reinforcement learning, the agent is able to improve further and increase its average reward. Such a bootstrapping approach has been shown effective when applying reinforcement learning to solve hard problems, especially those with long decision horizons (Silver et al., 2016).\nOur key contributions are three-fold. First, we present a probabilistic framework for inducing a posterior distribution over the entities in a knowledge base. Second, we use the above framework to develop, to our knowledge, the first fully endto-end differentiable model of a multi-turn information providing dialogue agent (KB-InfoBot), whose parameters can be tuned using standard gradient descent methods. Third, we present a modified version of the episodic REINFORCE (Peters and Bagnell, 2011; Williams, 1992) update rule for training the above model based on user feed-\nback, which allows the agent to explore both the set of possible dialogue acts at each turn and the set of possible entity results from the KB at the final turn."}, {"heading": "2 Related Work", "text": "Statistical goal-oriented dialogue systems have long been modeled as partially observable Markov decision processes (POMDPs) (Young et al., 2013), which are trained using reinforcement learning based on user feedback. Recently, there has been growing interest in designing \u201cend-toend\u201d systems, which combine feature extraction and policy optimization using deep neural networks, with the aim of eliminating the need of hand-crafted representations. We discuss these works below and highlight their methods of interfacing with the external database.\nCuaya\u0301huitl (2016) proposed SimpleDS, which uses a multi-layer feed-forward network to directly map environment states to agent actions. The network is trained using Q-learning and a simulated user; however it does not interact with a structured database, leaving that task to a server, which may be suboptimal as we show in our experiments below.\nWen et al. (2016b) introduced a modular dialogue agent, which consists of several neuralnetwork-based components trained using supervised learning. One key component is the database operator, which forms a query as follows:\nqt = \u222as\u2032\u2208SI arg max v pts\u2032 , (1)\nwhere pts\u2032 are distributions over the possible values of each slot and are output from the belief tracker. The query is issued to the database which returns a list of the matched entries. We refer to this operation from here on as a Hard-KB lookup. Hard-KB lookup breaks the differentiability of the whole system, and as a result training needs to be performed in two stages\u2013the intent network and belief trackers are trained using supervised labels specifically collected for them; while the policy network and generation network are trained separately on the system utterances. In this paper, we retain modularity of the network by keeping the belief trackers separate, but replace the query with a differentiable lookup over the database which computes a posterior distribution denoting the probability that the user is looking for a particular entry.\nAn alternative way for the dialogue agent to interface with the database is by augmenting its action space with predefined API calls (Williams and Zweig, 2016; Zhao and Eskenazi, 2016; Bordes and Weston, 2016). The API calls modify a query hypothesis maintained outside the end-to-end system which is used to retrieve results from this KB. These results are then appended to the next system input based on which the agent selects its next action. The resulting model is end-to-end differentiable, albeit with the database falling out of it. This framework does not deal with uncertainty in language understanding since the query hypothesis can only hold one slot-value at a time. Our approach, on the other hand, directly models the uncertainty to come up with a posterior distribution over entities in the knowledge base.\nWu et al. (2015) recently presented an entropy minimization dialogue management (EMDM) strategy for KB-InfoBots, which always asks for the value of the slot with maximum entropy over the remaining entries in the database. This approach is optimal in the absence of LU errors, but may suffer from error propagation issues in their presence. The rule-based policy serves as a baseline to compare to our proposed approach.\nOur work is motivated by the neural GenQA (Yin et al., 2016a) and neural enquirer (Yin et al., 2016b) models for querying KB and tables via natural language in a fully \u201cneuralized\u201d way. These works handle single-turn dialogues and are trained using supervised learning, while our model is designed for multi-turn dialogues and trained using reinforcement learning. Moreover, instead of defining an attention distribution directly over the KB entities, which may be very large, we instead induce it from the smaller distributions over each relation (or slot in dialogue terminology) in the KB. A separate line of work\u2013TensorLog (Cohen, 2016)\u2013investigates reasoning over KB facts in a differentiable manner to derive new facts. Instead, our focus is on retrieving facts from the KB.\nReinforcement Learning Neural Turing Machines (RL-NTM), introduced by Zaremba and Sutskever (2015), also allow neural controllers to interact with discrete external interfaces. The particular form of interface considered in that work is a one-dimensional memory tape along which a read head can move. Our work is in a similar vein, but assumes a different interface\u2014an entitycentric KB. We exploit the structure of such KBs\nto provide differentiable access to an KB-InfoBot agent for making decisions.\nLi et al. (2016) recently applied deep reinforcement leaning successfully to train non-goal oriented chatbot type dialogue agents. They show that reinforcement learning allows the agent to model long-term rewards and generate more diverse and coherent responses as compared to supervised learning. Chatbot systems, however, typically do not need to interface with an external database, which is the primary focus of the current paper."}, {"heading": "3 Probabilistic Framework for KB Lookup", "text": "In this section we describe a probabilistic framework for querying a KB given the agent\u2019s beliefs over the slots or attributes in the KB."}, {"heading": "3.1 Entity-Centric Knowledge Base (EC-KB)", "text": "A Knowledge Base consists of triples of the form (h, r, t), which denotes that relation r holds between the head h and tail t. In this work we assume that the KB-InfoBot has access to a domain-specific entity-centric knowledge base (EC-KB) (Zwicklbauer et al., 2013) where all head entities are of a particular type, and the relations correspond to attributes of these head entities. Examples of the type include movies, persons, or academic papers. Such a KB can be converted to a table format whose rows correspond to the unique head entities, columns correspond to the unique relation types (slots from here on), and some of the entries may be missing. A small example is shown in Figure 2."}, {"heading": "3.2 Notations and Assumptions", "text": "Let T denote the KB table described above and Ti,j denote the jth slot-value of ith entity. 1 \u2264 i \u2264 N and 1 \u2264 j \u2264 M . We let V j denote the vocabulary of each slot, i.e. the set of all distinct values in j-th column. We denote missing values from the table with a special token and write Ti,j = \u03a8. Mj = {i : Ti,j = \u03a8} denotes the set of entities for which the value of slot j is missing. Note that the user may still know the actual value of Ti,j , and we assume this lies in V j . Hence, we do not deal with OOV entities or relations at test time.\nThe user goal is sampled uniformly G \u223c U [{1, ...N}] and points to a particular row in the table T . To make the problem realistic we also sample binary random variables \u03a6j \u2208 {0, 1} to indicate whether the user knows the value of slot j or not. The agent maintains M multinomial distributions for its belief over user-goals given user utterances U t1 till turn t. A slot distribution p t j(v) for v \u2208 V j is the probability at turn t that the user constraint for slot j is v. The agent also maintains M binomials qtj = Pr(\u03a6j = 1) which denote the probability that the user does not know the value of slot j.\nWe also assume that column values are independently distributed to each other. This is a strong assumption but allows us to model the user goal for each slot independently, as opposed to modeling the user goal over KB entities directly. Typically maxj |V j | < N and hence this assumption makes the problem more scalable."}, {"heading": "3.3 Soft-KB Lookup", "text": "Let the posterior probability at turn t that the user is interested in row i of the table, given the utterances be ptT (G = i|U t1). We assume all probabilities are conditioned on user inputs U t1 and drop it from the notation below. From our assumption of independence of slot values:\nptT (G = i) \u221d M\u220f j=1 Pr(Gj = i), (2)\nwhere Pr(Gj = i) denotes the posterior probability of user goal for slot j pointing to Ti,j . We can\nmarginalize this over \u03a6j to get:\nPr(Gj = i) = 1\u2211\n\u03c6=0\nPr(Gj = i,\u03a6j = \u03c6) (3)\n= qtj Pr(Gj = i|\u03a6j = 1)+ (1\u2212 qtj) Pr(Gj = i|\u03a6j = 0).\nFor \u03a6j = 1, the user does not know the value of the slot, hence we assume a uniform prior over the rows of T :\nPr(Gj = i|\u03a6j = 1) = 1\nN , 1 \u2264 i \u2264 N (4)\nFor \u03a6j = 0, the user knows the value of slot j, but this may be missing from T , and we again have two cases:\nPr(Gj = i|\u03a6j = 0) = { 1 N , i \u2208Mj ptj(v)\n#jv\n( 1\u2212 |Mj |N ) , i 6\u2208Mj\n(5) Here, ptj(v) is the slot distribution from the belief tracker, and #jv is the count of value v in slot j. Detailed derivation for (5) is provided in the appendix. Combining (2), (3), (4), and (5) gives us the procedure for computing the posterior over KB entities."}, {"heading": "4 End-to-End KB-InfoBot", "text": "Figure 3 shows an overview of the end-to-end KBInfoBot. At each turn, the agent receives a natural language utterance ut as input, and selects an action at \u2208 A as output. The action spaceA consists of M + 1 actions \u2013 request(slot=i) for 1 \u2264 i \u2264M will ask the user for the value of slot i, and inform(I) will inform the user with an ordered list of results I from the KB. The dialogue ends once the agent chooses inform. We describe each of the components in detail below.\nFeature Extractor: The feature extractor converts user input ut into a vector representation xt. In our implementation we use a simple bag of ngrams (with n = 2) representation, where each element of xt is an integer indicating the count of a particular n-gram in ut. We let V n denote the number of unique n-grams, hence xt \u2208 RV n . This module could potentially be replaced with a more sophisticated NLU unit, but for the user simulator we consider below the vocabulary size is relatively small (V n = 3078), and doing so did not yield any improvements.\nBelief Trackers: The KB-InfoBot consists ofM belief trackers, one for each slot. Each tracker has input xt and produces two outputs, ptj and q t j , which we shall collectively call the belief state: ptj is a multinomial distribution over the slot values v, and qtj is a scalar probability of the user not knowing the value of that slot. It is common to use recurrent neural networks for belief tracking (Henderson et al., 2014; Wen et al., 2016b) since the output distribution at turn t depends on all user inputs till that turn. We use a Gated Recurrent Unit (GRU) (Cho et al., 2014) for each tracker, which, starting from h0j = 0 maintains a summary state htj as follows:\nrtj = \u03c3(W r j x t + U rj h t\u22121 j + b r) ztj = \u03c3(W z j x t + U zj h t\u22121 j + b z) h\u0303tj = tanh(W h j x t + Uhj (r t j \u00b7 ht\u22121j ) + b h) htj = (1\u2212 ztj) \u00b7 ht\u22121j + z t j \u00b7 h\u0303tj . (6)\nHere, r and z are called the reset and update gates respectively, h\u0303 the candidate output, and the subscript j and superscript t stand for the tracker index and dialogue turn respectively. \u03c3 denotes the sigmoid nonlinearity. The trainable paramters include W rj , W z j , W h j which are d \u00d7 V n matrices, U rj , U z j , U h j which are d \u00d7 d matrices, and br, bz , bh which are d \u00d7 1 vectors. The output htj \u2208 Rd can be interpreted as a summary of what the user has said about slot j till turn t. The belief states are computed from this vector as follows:\nptj = softmax(W p j h t j + b p j ) (7) qtj = \u03c3(W \u03a6 j h t j + b \u03a6 j ) (8)\nHere W pj \u2208 RV j\u00d7d, bpj \u2208 RV j , W\u03a6j \u2208 Rd and b\u03a6j \u2208 R, are all trainable parameters. The key differences between the belief tracker described here and the one presented in (Wen et\nal., 2016b) are as follows: (1) we model the probability that user does not know the value of a slot separately, as opposed to treating it as a special value for the slot, since this is a very different type of object; (2) we use GRU units instead of a Jordan-type RNN, and use summary states htj instead of tying together RNN weights; (3) we use n-gram features for simplicity instead of CNN features.\nSoft-KB Lookup: This module uses the procedure described in section 3.3 to compute the posterior over the EC-KB ptT \u2208 RN from the belief states above. Note that this is a fixed, differentiable operation without any trainable parameters.\nCollectively, outputs of the belief trackers and the soft-KB lookup can be viewed as the current dialogue state internal to the KB-InfoBot. Let st = [pt1, p t 2, ..., p t M , q t 1, q t 2, ..., q t M , p t T ] be the vec-\ntor of size \u2211\nj V j +M +N denoting this state.\nBeliefs Summary: At this stage it is possible for the agent to directly use the state vector st to select its next action at. However, the large size of the state vector would lead to a large number of parameters in the policy network. To improve efficiency we extract summary statistics from the belief states, similar to previous work in dialogue policy management (Williams and Young, 2005; Gas\u030cic\u0301 et al., 2009).\nFor each slot, we summarize the multinomial over slot-values into a weighted entropy statistic. The weights are computed from elements of the KB posterior ptT as follows:\nwtj(v) = \u2211\ni:Ti,j=v ptT (i) + p 0 j \u2211 i:Ti,j=\u03a8 ptT (i) . (9)\nHere, p0j is a prior distribution over the values of slot j, which we estimate using counts of each value in the KB. Intuitively, we want to weight the probability mass of v by the confidence of the agent that the user goal has value v in slot j. This confidence is a sum of two terms: (i) sum of KB posterior probabilities of rows which have value v, and (ii) sum of KB posterior probabilities of rows whose value is unknown, multiplied by the prior probability that an unknown might in fact be v. These two terms correspond to the two terms in (9). We define the weighted probability distribution of slot-values as follows:\np\u0303tj(v) \u221d ptj(v)wtj(v) . (10)\nThe summary statistic for slot j is then the entropy of this weighted probability distribution (after normalization):\nH(p\u0303tj) = \u2212 \u2211 v\u2208V j p\u0303tj(v) log p\u0303 t j(v) . (11)\nThe KB posterior ptT is summarized into a simple entropy measure:\nH(ptT ) = \u2212 N\u2211 i=1 ptT (i) log p t T (i) (12)\nThe scalar probabilities of the user not knowing the value of a slot are passed as is to the policy network. Hence, the final summary vector which is input to the policy network is s\u0303t = [H(p\u0303t1), ...,H(p\u0303 t M ), q t 1, ..., q t M , H(p t T )]. Note that this vector has size 2M + 1.\nPolicy Network: The policy network\u2019s job is to select the next action based on the current summary state s\u0303t and the dialogue history. Similar to (Williams and Zweig, 2016; Zhao and Eskenazi, 2016), we use a recurrent neural network to allow the network to maintain an internal state of dialogue history. Specifically, we use a GRU unit (see eq 6) followed by fully-connected layer and softmax nonlinearity to model the policy:\nht\u03c0 = GRU(s\u0303 1, ..., s\u0303t) (13) \u03c0 = softmax(W \u03c0ht\u03c0 + b \u03c0) . (14)\nHere, W \u03c0 is a A \u00d7 d matrix and b\u03c0 is a size A vector.\nAction Selection: During the course of the dialogue, the agent samples its actions from the policy \u03c0. If this action is inform(), it must also provide an ordered set I = (i1, i2, . . . , iR) of R results from the KB to the user. Since we want to learn the KB posterior ptT using reinforcement learning, we can view it as another policy, and sample results from the following distribution:\n\u00b5(I) = ptT (i1)\u00d7 ptT (i2)\n1\u2212 ptT (i1) \u00d7 \u00b7 \u00b7 \u00b7 . (15)\nIn the next section, we describe the episodic REINFORCE objective used to optimize both the above policies."}, {"heading": "5 Training", "text": ""}, {"heading": "5.1 Reinforcement Learning", "text": "The KB-InfoBot agent described above samples system actions from the policy \u03c0 and KB results from the distribution \u00b5. This allows the agent to explore both the space of actions A as well as the space of all possible KB results. This formulation leads to a modified version of the episodic REINFORCE algorithm (Williams, 1992) which we describe below.\nWe can write the expected discounted return of the agent under policy \u03c0 as follows:\nJ(\u03b8) = E [ H\u2211 h=0 \u03b3hrh ] (16)\nHere, the expectation is over all possible trajectories \u03c4 of the dialogue, \u03b8 denotes the parameters of the end-to-end system, H is the maximum length of an episode, \u03b3 is the discounting factor, and rh the reward observed at turn h. We can use the likelihood ratio trick (Peters and Bagnell, 2011) to write the gradient of the objective as follows:\n\u2207\u03b8J(\u03b8) = E [ \u2207\u03b8 log p\u03b8(\u03c4)\nH\u2211 h=0 \u03b3hrh\n] , (17)\nwhere p\u03b8(\u03c4) is the probability of observing a particular trajectory under the current policy. With a Markovian assumption, we can write\np\u03b8(\u03c4) =\n[ p(s0)\nH\u220f k=0 p(sk+1|sk, ak)\u03c0\u03b8(ak|sk)\n] \u00b5\u03b8(I),\n(18) where we use the subscript \u03b8 to denote which distributions depend on the neural network parameters. Plugging this into eq. 17, we obtain\n\u2207\u03b8J(\u03b8) =Ea\u223c\u03c0,I\u223c\u00b5 [( \u2207\u03b8 log\u00b5\u03b8(I)+\nH\u2211 h=0 \u2207\u03b8 log \u03c0\u03b8(ah) ) H\u2211 k=0 \u03b3krk ] , (19)\nwhere the expectation is now over all possible action sequences and the KB results, since gradient of the other terms in p\u03b8(\u03c4) is 0. This expectation is estimated using a mini-batch of B dialogues, and we use RMSProp (Hinton et al., 2012) updates to train the parameters \u03b8."}, {"heading": "5.2 Imitation Learning", "text": "In theory, both the belief trackers and policy network can be trained from scratch using only the reinforcement learning objective described above. In practice, however, for a moderately sized KB, the agent almost always fails if starting from random initialization. In this case, credit assignment is difficult for the agent, since it does not know whether it is failing due to an incorrect sequence of actions or incorrect set of results from the KB. Hence, at the beginning of training we have an imitation learning phase where the belief trackers and policy network are trained to mimic a simple handdesigned rule-based agent. The rule-based agent is described in detail in the next section. Here, we give the imitation learning objective used to bootstrap the KB-InfoBot.\nAssume that p\u0302tj and q\u0302 t j are the belief states from a rule-based agent, and a\u0302t its action at turn t. Then the loss function in imitation learning is:\nL(\u03b8) = E [ D(p\u0302tj ||ptj(\u03b8))+H(q\u0302tj , qtj(\u03b8))\u2212\nlog \u03c0\u03b8(a\u0302 t) ] , (20)\nwhere D(p||q) denotes the Kullback-Leibler divergence between p and q, and H(p, q) denotes the cross-entropy between p and q. The last term is a standard supervised cross-entropy loss between the rule-based agent\u2019s action and its probability in the KB-InfoBot\u2019s policy. The expectation is estimated as an average over the minibatch, and we use standard Stochastic Gradient Descent to optimize the loss."}, {"heading": "5.3 User Simulator", "text": "To evaluate the performance of the KB-InfoBot, we make use of a rule-based stochastic simulated user. At the beginning of each dialogue, the simulated user randomly samples a target entity from the EC-KB and a random combination of informable slots for which it knows the value of the target. The remaining slot-values are unknown to the user. The user initiates the dialogue by providing a subset of its informable slots to the agent and requesting for an entity which matches them. In subsequent turns, if the agent requests for the value of a slot, the user complies by providing it or informs the agent that it does not know that value. If the agent informs results from the KB, the simulated user checks whether its target is among them, and provides an appropriate reward.\nWe convert dialogue acts from the user into natural language utterances using a separately trained natural language generator (NLG). The NLG is trained in a sequence-to-sequence fashion, using conversations between humans collected by crowd-sourcing. It takes the dialogue actions (DAs) as input, and generates template-like sentences with slot placeholders via an LSTM decoder. Then, a post-processing scan is used to replace the slot placeholders with their actual values, which is similar to the decoder module in (Wen et al., 2015; Wen et al., 2016a). In the LSTM decoder, we apply beam search, which iteratively considers the top k best sentences up to time step t when generating the token of the time step t+ 1. Generally, a beam size of 20 can gain 2 BLEU points on average. For the sake of the trade-off between the speed and performance, we use the beam size of 3 in the following experiments.\nThere are several sources of error in user utterances. Any value provided by the user may be corrupted with some noise, or substituted completely with an incorrect value of the same type (e.g., \u201cBill Murray\u201d might become just \u201cBill\u201d or \u201cTom Cruise\u201d). The NLG described above is inherently stochastic, and may sometimes ignore the agent request. By increasing the temperature of the output softmax in the NLG we may also increase the noise in user utterances."}, {"heading": "6 Experiments and Results", "text": ""}, {"heading": "6.1 Baselines", "text": "We compare our end-to-end model with two sets of baselines. Rule-Based agents consist of handdesigned belief trackers and a hand-designed policy. The belief trackers search for tokens in the user input which match a slot-value in the KB, and do a Bayesian update on the probability mass p\u0302tj(v) associated with the values found. If the agent asks for a slot, but does not find any value for that slot in the user response, then the corresponding don\u2019t-care probability q\u0302tj for that slot is set to 1. We compare three variants of the handdesigned policy, which differ in terms of the KBlookup method. The No-KB version ignores the KB and selects its actions by always asking for the slot with maximum entropy. The Hard-KB version performs a hard-KB lookup (eq. 1) and selects the next action based on the entropy of the slots in the retrieved results. Finally, the Soft-KB version computes the full posterior over the KB and selects\nactions based on the weighted entropy statistic described in eq. 11. All these agents are variants of the EMDM strategy proposed in (Wu et al., 2015), with the difference being in the way the entropy is computed. At the end of the dialogue, all three agents inform the user with the top results from the KB posterior ptT , hence the difference only lies in the policy for action selection.\nThe second set of baselines, Simple-RL agents, retain the hand-designed belief trackers from above, but use the GRU policy network described in section 4 instead of a hand-designed policy. We again compare three variants of these agents, which differ only in the inputs going into the policy network. The No-KB version only takes entropy of each of the slot distributions. The HardKB version takes entropy of the slots in the retrieved results from the KB, along with the number of retrieved results. The Soft-KB version takes the weighted entropy of each slot, along with the entropy of the full posterior over KB. The policy network produces a distribution over the M + 1 valid actions available to the agent. During training an action is sampled from this distribution to encourage exploration, but for evaluation the argmax is taken. For each version, the inform action is accompanied with results from the KB posterior ptT ."}, {"heading": "6.2 Movies-KB", "text": "In our experiments, we use a movie-centric knowledge base constructed using the IMDBPy1 package. We selected a subset of movies released after 2007, and retained 6 slots. Statistics for this KB are given in Table 1. The original KB was modified to reduce the number of actors and directors in order to make the task more challenging. We also randomly remove 20% of the values from the\n1http://imdbpy.sourceforge.net/\nagent\u2019s copy of the KB to simulate a real-world scenario where the KB may be incomplete. The user, however, may still know these values."}, {"heading": "6.3 Hyperparameters", "text": "We use a hidden state size of d = 100 for all GRUs, a learning rate of 0.05 for the imitation learning phase and 0.005 for the reinforcement learning phase, and minibatch size 128. The maximum length of a dialogue is limited to 10 turns,2 beyond which the dialogue is deemed a failure. The input vocabulary is constructed from the NLG vocabulary and bigrams in the KB, and its size is 3078. The agent receives a positive reward if the user target is in top R = 5 results returned by it; this reward is computed as 2(1\u2212(r\u22121)/R), where r is the actual rank of the target. For a failed dialogue the agent receives a reward of \u22121, and at each turn it receives a reward of \u22120.1 since we want it to complete the task in the shortest time possible. The discounting factor \u03b3 is set to 0.99."}, {"heading": "6.4 Performance Comparison", "text": "We compare each of the discussed models along three metrics: the average rewards obtained, success rate (where success is defined as providing the user target among top R results), and the average number of turns per dialogue.\nFigure 4 shows how the reinforcement-learning agents perform as training progresses. This figure was generated by fixing the model every 100 updates, and performing 2000 simulations while selecting greedy policy actions. Table 2 shows the\n2A turn consists of one user action and one agent action.\nperformance of each model over a further 5000 simulations, after selecting the best model during training, and selecting greedy policy actions without exploration.\nThe Soft-KB versions outperform their HardKB counterparts, which in turn outperform NoKB versions, in terms of average reward. The main benefit comes from a reduced number of average turns. The similar success rate for all baseline agents is expected since they all share the same belief trackers, and use the same posterior ptT to inform the results. However, having information about the current state of the KB helps the policy conduct shorter dialogues. These decisions are further helped by having the complete posterior (Soft-KB) rather than just the current set of matching results (Hard-KB).\nReinforcement learning helps discover better policies than the hand-crafted rule-based agents, and hence Simple-RL agents outperform the RuleBased agents. All of these baseline agents, however, are limited by the rule-based belief trackers,\nand hence have a similar success rate. The endto-end agent is not limited as such, and is able to achieve a higher success rate and a higher average reward. This validates our motivation for introducing the Soft-KB lookup \u2014 the agent is able to improve both the belief trackers and policy network from user feedback directly, without the need for supervision signals.\nFigure 5 shows how the average reward of three of the agents varies as the temperature of the output softmax in the user simulator NLG is increased. A higher temperature means a more uniform output distribution, which would lead to generic user responses irrelevant to the agent questions. This is a simple way of introducing noise in the user responses. The performance of all three agents drops as the temperature is increased, but less so for the end-to-end agent, which can adapt its belief tracker to the inputs it receives."}, {"heading": "7 Conclusion", "text": "We have presented an end-to-end differentiable dialogue agent for multi-turn information access. All components of the agent are trained using reinforcement learning from user feedback, by optimizing a modified version of the episodic REINFORCE objective. We have shown that starting from an imitation learning phase where the agent learns to mimic a rule-based belief tracker and a rule-based policy, it can successfully improve on its own through reinforcement learning. The gain in performance is especially high when the noise in user inputs is high.\nA KB-InfoBot is a specific type of goal-oriented dialogue agent. Future work should focus on extending the techniques described here to other more general dialogue agents, such as a restaurant reservation agent or flight booking agent. We have also ignored scalability issues in our work, which\nwould be a concern for real-world sized knowledge bases. This is another direction for future research."}, {"heading": "A Sample Dialogues", "text": "Table 3 shows some sample dialogues between the user simulator and SimpleRL-SoftKB and End2End-RL agents. User utterances are generated using the NLG described in text. Value of the critic rating slot is a common source of error in the user simulator, and hence all learned policies tend to ask for this value multiple times."}, {"heading": "B Posterior Derivation", "text": "Here, we present a derivation for equation 5, i.e., the posterior over the KB slot when the user knows the value of that slot. For brevity, we drop \u03a6j = 0 from the condition in all probabilities below.\nFor the case when i \u2208Mj , we can write:\nPr(Gj = i)\n= Pr(Gj \u2208Mj) Pr(Gj = i|Gj \u2208Mj) = |Mj | N 1 |Mj | = 1 N , (21)\nwhere we assume all missing values to be equally likely, and estimate the prior probability of the goal being missing from the count of missing values in that slot.\nFor the case when i = v 6\u2208Mj :\nPr(Gj = i)\n= Pr(Gj 6\u2208Mj) Pr(Gj = i|Gj 6\u2208Mj)\n= ( 1\u2212 |Mj |\nN\n) \u00d7 ptj(v)\n#jv , (22)\nwhere the second term comes from taking the probability mass associated with v in the belief tracker and dividing it equally among all rows with value v.\nWe can also verify that the above distribution is valid: i.e., it sums to 1:\u2211\ni\nPr(Gj = i)\n= \u2211 i\u2208Mj Pr(Gj = i) + \u2211 i 6\u2208Mj Pr(Gj = i)\n= \u2211 i\u2208Mj 1 N + \u2211 i 6\u2208Mj ( 1\u2212 |Mj | N ) ptj(v) #jv\n= |Mj | N +\n( 1\u2212 |Mj |\nN ) \u2211 i 6\u2208Mj ptj(v) #jv\n= |Mj | N +\n( 1\u2212 |Mj |\nN )\u2211 i\u2208V j #jv ptj(v) #jv\n= |Mj | N +\n( 1\u2212 |Mj |\nN\n) \u00d7 1\n= 1 ."}], "references": [{"title": "A survey of robot learning from demonstration", "author": ["Sonia Chernova", "Manuela Veloso", "Brett Browning"], "venue": "Robotics and autonomous systems,", "citeRegEx": "Argall et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Argall et al\\.", "year": 2009}, {"title": "Learning end-to-end goal-oriented dialog. arXiv preprint arXiv:1605.07683", "author": ["Bordes", "Weston2016] Antoine Bordes", "Jason Weston"], "venue": null, "citeRegEx": "Bordes et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Bordes et al\\.", "year": 2016}, {"title": "End-to-end memory networks with knowledge carryover for multi-turn spoken language understanding", "author": ["Chen et al.2016] Yun-Nung Chen", "Dilek Hakkani-T\u00fcr", "Gokhan Tur", "Jianfeng Gao", "Li Deng"], "venue": "In Proceedings of The 17th Annual Meeting", "citeRegEx": "Chen et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Chen et al\\.", "year": 2016}, {"title": "Learning phrase representations using rnn encoder-decoder for statistical", "author": ["Bart Van Merri\u00ebnboer", "Caglar Gulcehre", "Dzmitry Bahdanau", "Fethi Bougares", "Holger Schwenk", "Yoshua Bengio"], "venue": null, "citeRegEx": "Merri\u00ebnboer et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Merri\u00ebnboer et al\\.", "year": 2014}, {"title": "Tensorlog: A differentiable deductive database. arXiv preprint arXiv:1605.06523", "author": ["William W Cohen"], "venue": null, "citeRegEx": "Cohen.,? \\Q2016\\E", "shortCiteRegEx": "Cohen.", "year": 2016}, {"title": "Simpleds: A simple deep reinforcement learning dialogue system", "author": ["Heriberto Cuay\u00e1huitl"], "venue": "International Workshop on Spoken Dialogue Systems (IWSDS)", "citeRegEx": "Cuay\u00e1huitl.,? \\Q2016\\E", "shortCiteRegEx": "Cuay\u00e1huitl.", "year": 2016}, {"title": "Back-off action selection in summary space-based POMDP dialogue systems", "author": ["Ga\u0161i\u0107 et al.2009] Milica Ga\u0161i\u0107", "Fabrice Lefevre", "Filip Jurcicek", "Simon Keizer", "Francois Mairesse", "Blaise Thomson", "Kai Yu", "Steve Young"], "venue": null, "citeRegEx": "Ga\u0161i\u0107 et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Ga\u0161i\u0107 et al\\.", "year": 2009}, {"title": "Multidomain joint semantic frame parsing using bidirectional RNN-LSTM", "author": ["Gokhan Tur", "Asli Celikyilmaz", "Yun-Nung Chen", "Jianfeng Gao", "Li Deng", "Ye-Yi Wang"], "venue": null, "citeRegEx": "Hakkani.T\u00fcr et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Hakkani.T\u00fcr et al\\.", "year": 2016}, {"title": "Word-based dialog state tracking with recurrent neural networks", "author": ["Blaise Thomson", "Steve Young"], "venue": "In Proceedings of the 15th Annual Meeting of the Special Interest Group on Discourse and Dialogue", "citeRegEx": "Henderson et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Henderson et al\\.", "year": 2014}, {"title": "Machine learning for dialog state tracking: A review", "author": ["Matthew Henderson"], "venue": null, "citeRegEx": "Henderson.,? \\Q2015\\E", "shortCiteRegEx": "Henderson.", "year": 2015}, {"title": "Lecture 6a overview of mini\u2013batch gradient descent", "author": ["N Srivastava", "Kevin Swersky"], "venue": "Coursera Lecture slides https://class. coursera", "citeRegEx": "Hinton et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Hinton et al\\.", "year": 2012}, {"title": "Deep reinforcement learning for dialogue generation", "author": ["Li et al.2016] Jiwei Li", "Will Monroe", "Alan Ritter", "Michel Galley", "Jianfeng Gao", "Dan Jurafsky"], "venue": null, "citeRegEx": "Li et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Li et al\\.", "year": 2016}, {"title": "Policy gradient methods", "author": ["Peters", "Bagnell2011] Jan Peters", "J Andrew Bagnell"], "venue": "In Encyclopedia of Machine Learning,", "citeRegEx": "Peters et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Peters et al\\.", "year": 2011}, {"title": "Mastering the game of Go with deep neural networks and tree search", "author": ["Nal Kalchbrenner", "Ilya Sutskever", "Timothy Lillicrap", "Madeleine Leach", "Koray Kavukcuoglu", "Thore Graepel", "Demis Hassabis."], "venue": "Nature, 529:484\u2013489.", "citeRegEx": "Kalchbrenner et al\\.,? 2016", "shortCiteRegEx": "Kalchbrenner et al\\.", "year": 2016}, {"title": "Semantically conditioned lstm-based natural language generation for spoken dialogue systems", "author": ["Wen et al.2015] Tsung-Hsien Wen", "Milica Ga\u0161i\u0107", "Nikola Mrk\u0161i\u0107", "Pei-Hao Su", "David Vandyke", "Steve Young"], "venue": null, "citeRegEx": "Wen et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Wen et al\\.", "year": 2015}, {"title": "Conditional generation and snapshot learning in neural dialogue systems", "author": ["Wen et al.2016a] Tsung-Hsien Wen", "Milica Ga\u0161i\u0107", "Nikola Mrk\u0161i\u0107", "Lina M. Rojas-Barahona", "Pei-Hao Su", "Stefan Ultes", "David Vandyke", "Steve Young"], "venue": null, "citeRegEx": "Wen et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Wen et al\\.", "year": 2016}, {"title": "2016b. A network-based end-to-end trainable task-oriented dialogue system", "author": ["Wen et al.2016b] Tsung-Hsien Wen", "Milica Ga\u0161i\u0107", "Nikola Mrk\u0161i\u0107", "Lina M. Rojas-Barahona", "PeiHao Su", "Stefan Ultes", "David Vandyke", "Steve Young"], "venue": null, "citeRegEx": "Wen et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Wen et al\\.", "year": 2016}, {"title": "Scaling up POMDPs for dialog management: The \u201cSummary POMDP", "author": ["Williams", "Young2005] Jason D Williams", "Steve Young"], "venue": "IEEE Workshop on Automatic Speech Recognition and Understanding,", "citeRegEx": "Williams et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Williams et al\\.", "year": 2005}, {"title": "End-to-end lstm-based dialog control optimized with supervised and reinforcement learning", "author": ["Williams", "Zweig2016] Jason D Williams", "Geoffrey Zweig"], "venue": "arXiv preprint arXiv:1606.01269", "citeRegEx": "Williams et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Williams et al\\.", "year": 2016}, {"title": "Simple statistical gradient-following algorithms for connectionist reinforcement learning", "author": ["Ronald J Williams"], "venue": "Machine learning,", "citeRegEx": "Williams.,? \\Q1992\\E", "shortCiteRegEx": "Williams.", "year": 1992}, {"title": "A probabilistic framework for representing dialog systems and entropy-based dialog management through dynamic stochastic state evolution", "author": ["Wu et al.2015] Ji Wu", "Miao Li", "Chin-Hui Lee"], "venue": null, "citeRegEx": "Wu et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Wu et al\\.", "year": 2015}, {"title": "Spoken language understanding using long shortterm memory neural networks", "author": ["Yao et al.2014] Kaisheng Yao", "Baolin Peng", "Yu Zhang", "Dong Yu", "Geoffrey Zweig", "Yangyang Shi"], "venue": "In Spoken Language Technology Workshop (SLT), 2014 IEEE,", "citeRegEx": "Yao et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Yao et al\\.", "year": 2014}, {"title": "Neural generative question answering", "author": ["Yin et al.2016a] Jun Yin", "Xin Jiang", "Zhengdong Lu", "Lifeng Shang", "Hang Li", "Xiaoming Li"], "venue": "International Joint Conference on Artificial Intelligence", "citeRegEx": "Yin et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Yin et al\\.", "year": 2016}, {"title": "Neural enquirer: Learning to query tables", "author": ["Yin et al.2016b] Pengcheng Yin", "Zhengdong Lu", "Hang Li", "Ben Kao"], "venue": "International Joint Conference on Artificial Intelligence", "citeRegEx": "Yin et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Yin et al\\.", "year": 2016}, {"title": "POMDPbased statistical spoken dialog systems: A review", "author": ["Young et al.2013] Steve Young", "Milica Ga\u0161i\u0107", "Blaise Thomson", "Jason D Williams"], "venue": "Proceedings of the IEEE,", "citeRegEx": "Young et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Young et al\\.", "year": 2013}, {"title": "Reinforcement learning neural Turing machines-revised", "author": ["Zaremba", "Sutskever2015] Wojciech Zaremba", "Ilya Sutskever"], "venue": "arXiv preprint arXiv:1505.00521", "citeRegEx": "Zaremba et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Zaremba et al\\.", "year": 2015}, {"title": "Towards end-to-end learning for dialog state tracking and management using deep reinforcement learning", "author": ["Zhao", "Eskenazi2016] Tiancheng Zhao", "Maxine Eskenazi"], "venue": "arXiv preprint arXiv:1606.02560", "citeRegEx": "Zhao et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Zhao et al\\.", "year": 2016}, {"title": "Do we need entity-centric knowledge bases for entity disambiguation", "author": ["Christin Seifert", "Michael Granitzer"], "venue": "In Proceedings of the 13th International Conference on Knowledge Management and", "citeRegEx": "Zwicklbauer et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Zwicklbauer et al\\.", "year": 2013}], "referenceMentions": [{"referenceID": 21, "context": "A typical goal-oriented dialogue system consists of four basic components: a language understanding (LU) module for predicting user intents and extracting associated slots (Yao et al., 2014; Hakkani-T\u00fcr et al., 2016; Chen et al., 2016), a dialogue state tracker which tracks the user goal and dialogue history (Henderson et al.", "startOffset": 172, "endOffset": 235}, {"referenceID": 7, "context": "A typical goal-oriented dialogue system consists of four basic components: a language understanding (LU) module for predicting user intents and extracting associated slots (Yao et al., 2014; Hakkani-T\u00fcr et al., 2016; Chen et al., 2016), a dialogue state tracker which tracks the user goal and dialogue history (Henderson et al.", "startOffset": 172, "endOffset": 235}, {"referenceID": 2, "context": "A typical goal-oriented dialogue system consists of four basic components: a language understanding (LU) module for predicting user intents and extracting associated slots (Yao et al., 2014; Hakkani-T\u00fcr et al., 2016; Chen et al., 2016), a dialogue state tracker which tracks the user goal and dialogue history (Henderson et al.", "startOffset": 172, "endOffset": 235}, {"referenceID": 8, "context": ", 2016), a dialogue state tracker which tracks the user goal and dialogue history (Henderson et al., 2014; Henderson, 2015), a dialogue policy which selects the next system action based on the current state (Young et al.", "startOffset": 82, "endOffset": 123}, {"referenceID": 9, "context": ", 2016), a dialogue state tracker which tracks the user goal and dialogue history (Henderson et al., 2014; Henderson, 2015), a dialogue policy which selects the next system action based on the current state (Young et al.", "startOffset": 82, "endOffset": 123}, {"referenceID": 24, "context": ", 2014; Henderson, 2015), a dialogue policy which selects the next system action based on the current state (Young et al., 2013), and a natural language generator (NLG) for converting dialogue acts into natural language (Wen et al.", "startOffset": 108, "endOffset": 128}, {"referenceID": 14, "context": ", 2013), and a natural language generator (NLG) for converting dialogue acts into natural language (Wen et al., 2015; Wen et al., 2016a).", "startOffset": 99, "endOffset": 136}, {"referenceID": 0, "context": "Hence, at the beginning of training, we first have an imitation-learning phase (Argall et al., 2009) where both the belief tracker and policy network are trained to mimic a rule-based agent.", "startOffset": 79, "endOffset": 100}, {"referenceID": 19, "context": "Third, we present a modified version of the episodic REINFORCE (Peters and Bagnell, 2011; Williams, 1992) update rule for training the above model based on user feedback, which allows the agent to explore both the set of possible dialogue acts at each turn and the set of possible entity results from the KB at the final turn.", "startOffset": 63, "endOffset": 105}, {"referenceID": 24, "context": "Statistical goal-oriented dialogue systems have long been modeled as partially observable Markov decision processes (POMDPs) (Young et al., 2013), which are trained using reinforcement learning based on user feedback.", "startOffset": 125, "endOffset": 145}, {"referenceID": 4, "context": "A separate line of work\u2013TensorLog (Cohen, 2016)\u2013investigates reasoning over KB facts in a differentiable manner to derive new facts.", "startOffset": 34, "endOffset": 47}, {"referenceID": 27, "context": "In this work we assume that the KB-InfoBot has access to a domain-specific entity-centric knowledge base (EC-KB) (Zwicklbauer et al., 2013) where all head entities are of a particular type, and the relations correspond to attributes of these head entities.", "startOffset": 113, "endOffset": 139}, {"referenceID": 8, "context": "It is common to use recurrent neural networks for belief tracking (Henderson et al., 2014; Wen et al., 2016b) since the output distribution at turn t depends on all user inputs till that turn.", "startOffset": 66, "endOffset": 109}, {"referenceID": 6, "context": "To improve efficiency we extract summary statistics from the belief states, similar to previous work in dialogue policy management (Williams and Young, 2005; Ga\u0161i\u0107 et al., 2009).", "startOffset": 131, "endOffset": 177}, {"referenceID": 19, "context": "This formulation leads to a modified version of the episodic REINFORCE algorithm (Williams, 1992) which we describe below.", "startOffset": 81, "endOffset": 97}, {"referenceID": 10, "context": "This expectation is estimated using a mini-batch of B dialogues, and we use RMSProp (Hinton et al., 2012) updates to train the parameters \u03b8.", "startOffset": 84, "endOffset": 105}, {"referenceID": 14, "context": "Then, a post-processing scan is used to replace the slot placeholders with their actual values, which is similar to the decoder module in (Wen et al., 2015; Wen et al., 2016a).", "startOffset": 138, "endOffset": 175}, {"referenceID": 20, "context": "All these agents are variants of the EMDM strategy proposed in (Wu et al., 2015), with the difference being in the way the entropy is computed.", "startOffset": 63, "endOffset": 80}], "year": 2016, "abstractText": "This paper proposes KB-InfoBot\u2014a dialogue agent that provides users with an entity from a knowledge base (KB) by interactively asking for its attributes. All components of the KB-InfoBot are trained in an end-to-end fashion using reinforcement learning. Goal-oriented dialogue systems typically need to interact with an external database to access real-world knowledge (e.g. movies playing in a city). Previous systems achieved this by issuing a symbolic query to the database and adding retrieved results to the dialogue state. However, such symbolic operations break the differentiability of the system and prevent end-to-end training of neural dialogue agents. In this paper, we address this limitation by replacing symbolic queries with an induced \u201csoft\u201d posterior distribution over the KB that indicates which entities the user is interested in. We also provide a modified version of the episodic REINFORCE algorithm, which allows the KBInfoBot to explore and learn both the policy for selecting dialogue acts and the posterior over the KB for retrieving the correct entities. Experimental results show that the end-to-end trained KB-InfoBot outperforms competitive rule-based baselines, as well as agents which are not end-to-end trainable.", "creator": "LaTeX with hyperref package"}}}