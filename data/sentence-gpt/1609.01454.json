{"id": "1609.01454", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "6-Sep-2016", "title": "Attention-Based Recurrent Neural Network Models for Joint Intent Detection and Slot Filling", "abstract": "Attention-based encoder-decoder neural network models have recently shown promising results in machine translation and speech recognition. In this work, we propose an attention-based neural network model for joint intent detection and slot filling, both of which are critical steps for many speech understanding and dialog systems. Unlike in machine translation and speech recognition, alignment is explicit in slot filling. We explore different strategies in incorporating this alignment information to the encoder-decoder framework. Learning from the attention mechanism in encoder-decoder model, we further propose introducing attention to the alignment-based RNN models. Such attentions provide additional information to the intent classification and slot label prediction. Our independent task models achieve state-of-the-art intent detection error rate and slot filling F1 score on the benchmark ATIS task. Our joint training model further obtains 0.56% absolute (23.8% relative) error reduction on intent detection and 0.23% absolute gain on slot filling over the independent task models.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n", "histories": [["v1", "Tue, 6 Sep 2016 09:29:12 GMT  (132kb,D)", "http://arxiv.org/abs/1609.01454v1", "Accepted at Interspeech 2016"]], "COMMENTS": "Accepted at Interspeech 2016", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["bing liu", "ian lane"], "accepted": false, "id": "1609.01454"}, "pdf": {"name": "1609.01454.pdf", "metadata": {"source": "CRF", "title": "Attention-Based Recurrent Neural Network Models for Joint Intent Detection and Slot Filling", "authors": ["Bing Liu", "Ian Lane"], "emails": ["liubing@cmu.edu,", "lane@cmu.edu"], "sections": [{"heading": "1. Introduction", "text": "Spoken language understanding (SLU) system is a critical component in spoken dialogue systems. SLU system typically involves identifying speaker\u2019s intent and extracting semantic constituents from the natural language query, two tasks that are often referred to as intent detection and slot filling.\nIntent detection and slot filling are usually processed separately. Intent detection can be treated as a semantic utterance classification problem, and popular classifiers like support vector machines (SVMs) [1] and deep neural network methods [2] can be applied. Slot filling can be treated as a sequence labeling task. Popular approaches to solving sequence labeling problems include maximum entropy Markov models (MEMMs) [3], conditional random fields (CRFs) [4], and recurrent neural networks (RNNs) [5, 6, 7]. Joint model for intent detection and slot filling has also been proposed in literature [8, 9]. Such joint model simplifies the SLU system, as only one model needs to be trained and fine-tuned for the two tasks.\nRecently, encoder-decoder neural network models have been successfully applied in many sequence learning problems such as machine translation [10] and speech recognition [11]. The main idea behind the encoder-decoder model is to encode input sequence into a dense vector, and then use this vector to generate corresponding output sequence. The attention mechanism introduced in [12] enables the encoder-decoder architecture to learn to align and decode simultaneously.\nIn this work, we investigate how an SLU model can benefit from the strong modeling capacity of the sequence models. Attention-based encoder-decoder model is capable of mapping sequences that are of different lengths when no alignment information is given. In slot filling, however, alignment is explicit, and thus alignment-based RNN models typically work well. We would like to investigate the combination of the attention-based and alignment-based methods. Specifically, we want to explore how the alignment information in slot filling can be best utilized in the encoder-decoder models, and on the other hand, whether the alignment-based RNN slot filling models can be further improved with the attention mechanism that introduced from the encoder-decoder architecture. Moreover, we want to investigate how slot filling and intent detection can be jointly modeled under such schemes.\nThe remainder of the paper is organized as follows. In section 2, we introduce the background on using RNN for slot filling and using encoder-decoder models for sequence learning. In section 3, we describe two approaches for jointly modeling intent and slot filling. Section 4 discusses the experiment setup and results on ATIS benchmarking task. Section 5 concludes the work."}, {"heading": "2. Background", "text": ""}, {"heading": "2.1. RNN for Slot Filling", "text": "Slot filling can be treated as a sequence labeling problem, where we have training examples of { (x(n),y(n)) : n = 1, ..., N } and we want to learn a function f : X \u2192 Y that maps an input sequence x to the corresponding label sequence y. In slot filling, the input sequence and label sequence are of the same length, and thus there is explicit alignment.\nRNNs have been widely used in many sequence modeling problems [6, 13]. At each time step of slot filling, RNN reads a word as input and predicts its corresponding slot label considering all available information from the input and the emitted output sequences. The model is trained to find the best parameter set \u03b8 that maximizes the likelihood:\nargmax \u03b8 T\u220f t=1 P (yt|yt\u221211 ,x; \u03b8) (1)\nar X\niv :1\n60 9.\n01 45\n4v 1\n[ cs\n.C L\n] 6\nS ep\n2 01\n6\nwhere x represents the input word sequence, yt\u221211 represents the output label sequence prior to time step t. During inference, we want to find the best label sequence y given an input sequence x such that:\ny\u0302 = argmax y\nP (y|x) (2)"}, {"heading": "2.2. RNN Encoder-Decoder", "text": "The RNN encoder-decoder framework is firstly introduced in [10] and [14]. The encoder and decoder are two separate RNNs. The encoder reads a sequence of input (x1, ..., xT ) to a vector c. This vector encodes information of the whole source sequence, and is used in decoder to generate the target output sequence. The decoder defines the probability of the output sequence as:\nP (y) = T\u220f t=1 P (yt|yt\u221211 , c) (3)\nwhere yt\u221211 represents the predicted output sequence prior to time step t. Comparing to an RNN model for sequence labeling, the RNN encoder-decoder model is capable of mapping sequence to sequence with different lengths. There is no explicit alignment between source and target sequences. The attention mechanism later introduced in [12] enables the encoder-decoder model to learn a soft alignment and to decode at the same time."}, {"heading": "3. Proposed Methods", "text": "In this section, we first describe our approach on integrating alignment information to the encoder-decoder architecture for slot filling and intent detection. Following that, we describe the proposed method on introducing attention mechanism from the encoder-decoder architecture to the alignment-based RNN models."}, {"heading": "3.1. Encoder-Decoder Model with Aligned Inputs", "text": "The encoder-decoder model for joint intent detection and slot filling is illustrated in Figure 2. On encoder side, we use a bidirectional RNN. Bidirectional RNN has been successfully applied in speech recognition [15] and spoken language understanding [6]. We use LSTM [16] as the basic recurrent network unit for its ability to better model long-term dependencies comparing to simple RNN.\nIn slot filling, we want to map a word sequence x = (x1, ..., xT ) to its corresponding slot label sequence y = (y1, ..., yT ). The bidirectional RNN encoder reads the source word sequence forward and backward. The forward RNN reads the word sequence in its original order and generates a hidden state fhi at each time step. Similarly, the backward RNN reads the word sequence in its reverse order and generate a sequence of hidden states (bhT , ..., bh1). The final encoder hidden state hi at each time step i is a concatenation of the forward state fhi and backward state bhi, i.e. hi = [fhi, bhi].\nThe last state of the forward and backward encoder RNN carries information of the entire source sequence. We use the last state of the backward encoder RNN to compute the initial decoder hidden state following the approach in [12]. The decoder is a unidirectional RNN. Again, we use an LSTM cell as the basic RNN unit. At each decoding step i, the decoder state si is calculated as a function of the previous decoder state si\u22121, the previous emitted label yi\u22121, the aligned encoder hidden state hi, and the context vector ci:\nsi = f(si\u22121, yi\u22121, hi, ci) (4)\nwhere the context vector ci is computed as a weighted sum of the encoder states h = (h1, ..., hT ) [12]:\nci = T\u2211 j=1 \u03b1i,jhj (5)\nand\n\u03b1i,j = exp(ei,j)\u2211T k=1 exp(ei,k)\nei,k = g(si\u22121, hk)\n(6)\ng a feed-forward neural network. At each decoding step, the explicit aligned input is the encoder state hi. The context vector ci provides additional information to the decoder and can be seen as a continuous bag of weighted features (h1, ..., hT ).\nFor joint modeling of intent detection and slot filling, we add an additional decoder for intent detection (or intent classification) task that shares the same encoder with slot filling decoder. During model training, costs from both decoders are back-propagated to the encoder. The intent decoder generates only one single output which is the intent class distribution of the sentence, and thus alignment is not required. The intent decoder state is a function of the shared initial decoder state s0, which encodes information of the entire source sequence, and the context vector cintent, which indicates part of the source sequence that the intent decoder pays attention to."}, {"heading": "3.2. Attention-Based RNN Model", "text": "The attention-based RNN model for joint intent detection and slot filling is illustrated in Figure 3. The idea of introducing attention to the alignment-based RNN sequence labeling model\nis motivated by the use of attention mechanism in encoderdecoder models. In bidirectional RNN for sequence labeling, the hidden state at each time step carries information of the whole sequence, but information may gradually lose along the forward and backward propagation. Thus, when making slot label prediction, instead of only utilizing the aligned hidden state hi at each step, we would like to see whether the use of context vector ci gives us any additional supporting information, especially those require longer term dependencies that is not being fully captured by the hidden state.\nIn the proposed model, a bidirectional RNN (BiRNN) reads the source sequence in both forward and backward directions. We use LSTM cell for the basic RNN unit. Slot label dependencies are modeled in the forward RNN. Similar to the encoder module in the above described encoder-decoder architecture, the hidden state hi at each step is a concatenation of the forward state fhi and backward state bhi, hi = [fhi, bhi]. Each hidden state hi contains information of the whole input word sequence, with strong focus on the parts surrounding the word at step i. This hidden state hi is then combined with the context vector ci to produce the label distribution, where the context vector ci is calculated as a weighted average of the RNN hidden states h = (h1, ..., hT ).\nFor joint modeling of intent detection and slot filling, we reuse the pre-computed hidden states h of the bidirectional RNN to produce intent class distribution. If attention is not used, we apply mean-pooling [17] over time on the hidden states h followed by logistic regression to perform the intent classification. If attention is enabled, we instead take the weighted average of the hidden states h over time.\nComparing to the attention-based encoder-decoder model that utilizes explicit aligned inputs, the attention-based RNN model is more computational efficient. During model training, the encoder-decoder slot filling model reads through the input sequence twice, while the attention-based RNN model reads through the input sequence only once."}, {"heading": "4. Experiments", "text": ""}, {"heading": "4.1. Data", "text": "ATIS (Airline Travel Information Systems) data set [18] is widely used in SLU research. The data set contains audio recordings of people making flight reservations. In this work, we follow the ATIS corpus1 setup used in [6, 7, 9, 19]. The training set contains 4978 utterances from the ATIS-2 and ATIS-3 corpora, and the test set contains 893 utterances from the ATIS-3 NOV93 and DEC94 data sets. There are in total 127 distinct slot labels and 18 different intent types. We evaluate the system performance on slot filling using F1 score, and the performance on intent detection using classification error rate.\nWe obtained another ATIS text corpus that was used in [9] and [20] for SLU evaluation. This corpus contains 5138 utterances with both intent and slot labels annotated. In total there are 110 different slot labels and 21 intent types. We use the same 10-fold cross validation setup as in [9] and [20]."}, {"heading": "4.2. Training Procedure", "text": "LSTM cell is used as the basic RNN unit in the experiments. Our LSTM implementation follows the design in [21]. Given the size the data set, we set the number of units in LSTM cell as 128. The default forget gate bias is set to 1 [22]. We use only one layer of LSTM in the proposed models, and deeper models by stacking the LSTM layers are to be explored in future work.\nWord embeddings of size 128 are randomly initialized and fine-tuned during mini-batch training with batch size of 16. Dropout rate 0.5 is applied to the non-recurrent connections [21] during model training for regularization. Maximum norm for gradient clipping is set to 5. We use Adam optimization method following the suggested parameter setup in [23]."}, {"heading": "4.3. Independent Training Model Results: Slot Filling", "text": "We first report the results on our independent task training models. Table 1 shows the slot filling F1 scores using our proposed architectures. Table 2 compares our proposed model performance on slot filling to previously reported results.\nIn Table 1, the first set of results are for variations of encoder-decoder models described in section 3.1. Not to our surprise, the pure attention-based slot filling model that does not utilize explicit alignment information performs poorly. Letting the model to learn the alignment from training data does not seem to be appropriate for slot filling task. Line 2 and line 3 show the F1 scores of the non-attention and attention-based encode-decoder models that utilize the aligned inputs. The\n1We thank Gokhan Tur and Puyang Xu for sharing the ATIS data set.\nattention-based model gives slightly better F1 score than the non-attention-based one, on both the average and best scores. By investigating the attention learned by the model, we find that the attention weights are more likely to be evenly distributed across words in the source sequence. There are a few cases where we observe insightful attention (Figure 4) that the decoder pays to the input sequence, and that might partly explain the observed performance gain when attention is enabled.\nThe second set of results in Table 1 are for bidirectional RNN models described in section 3.2. Similar to the previous set of results, we observe slightly improved F1 score on the model that uses attentions. The contribution from the context vector for slot filling is not very obvious. It seems that for sequence length at such level (average sentence length is 11 for this ATIS corpus), the hidden state hi that produced by the bidirectional RNN is capable of encoding most of the information that is needed to make the slot label prediction.\nTable 2 compares our slot filling models to previous approaches. Results from both of our model architectures advance the best F1 scores reported previously."}, {"heading": "4.4. Independent Training Model Results: Intent Detection", "text": "Table 3 compares intent classification error rate between our intent models and previous approaches. Intent error rate of our proposed models outperform the state-of-the-art results by a large margin. The attention-based encoder-decoder intent model advances the bidirectional RNN model. This might be attributed to the sequence level information passed from the encoder and additional layer of non-linearity in the decoder RNN."}, {"heading": "4.5. Joint Model Results", "text": "Table 4 shows our joint training model performance on intent detection and slot filling comparing to previous reported results. As shown in this table, the joint training model using\nencoder-decoder architecture achieves 0.09% absolute gain on slot filling and 0.45% absolute gain (22.2% relative improvement) on intent detection over the independent training model. For the attention-based bidirectional RNN architecture, the join training model achieves 0.23% absolute gain on slot filling and 0.56% absolute gain (23.8% relative improvement) on intent detection over the independent training models. The attentionbased RNN model seems to benefit more from the joint training. Results from both of our joint training approaches outperform the best reported joint modeling results.\nTo further verify the performance of our joint training models, we apply the proposed models on the additional ATIS data set and evaluate them with 10-fold cross validation same as in [9] and [20]. Both the encoder-decoder and attention-based RNN methods achieve promising results."}, {"heading": "5. Conclusions", "text": "In this paper, we explored strategies in utilizing explicit alignment information in the attention-based encoder-decoder neural network models. We further proposed an attention-based bidirectional RNN model for joint intent detection and slot filling. Using a joint model for the two SLU tasks simplifies the dialog system, as only one model needs to be trained and deployed. Our independent training models achieved state-of-the-art performance for both intent detection and slot filling on the benchmark ATIS task. The proposed joint training models improved the intent detection accuracy and slot filling F1 score further over the independent training models."}, {"heading": "6. References", "text": "[1] P. Haffner, G. Tur, and J. H. Wright, \u201cOptimizing svms for com-\nplex call classification,\u201d in Acoustics, Speech, and Signal Processing, 2003. Proceedings.(ICASSP\u201903). 2003 IEEE International Conference on, vol. 1. IEEE, 2003, pp. I\u2013632.\n[2] R. Sarikaya, G. E. Hinton, and B. Ramabhadran, \u201cDeep belief nets for natural language call-routing,\u201d in Acoustics, Speech and Signal Processing (ICASSP), 2011 IEEE International Conference on. IEEE, 2011, pp. 5680\u20135683.\n[3] A. McCallum, D. Freitag, and F. C. Pereira, \u201cMaximum entropy markov models for information extraction and segmentation.\u201d in ICML, vol. 17, 2000, pp. 591\u2013598.\n[4] C. Raymond and G. Riccardi, \u201cGenerative and discriminative algorithms for spoken language understanding.\u201d in INTERSPEECH, 2007, pp. 1605\u20131608.\n[5] K. Yao, B. Peng, Y. Zhang, D. Yu, G. Zweig, and Y. Shi, \u201cSpoken language understanding using long short-term memory neural networks,\u201d in Spoken Language Technology Workshop (SLT), 2014 IEEE. IEEE, 2014, pp. 189\u2013194.\n[6] G. Mesnil, Y. Dauphin, K. Yao, Y. Bengio, L. Deng, D. HakkaniTur, X. He, L. Heck, G. Tur, D. Yu et al., \u201cUsing recurrent neural networks for slot filling in spoken language understanding,\u201d Audio, Speech, and Language Processing, IEEE/ACM Transactions on, vol. 23, no. 3, pp. 530\u2013539, 2015.\n[7] B. Liu and I. Lane, \u201cRecurrent neural network structured output prediction for spoken language understanding,\u201d in Proc. NIPS Workshop on Machine Learning for Spoken Language Understanding and Interactions, 2015.\n[8] D. Guo, G. Tur, W.-t. Yih, and G. Zweig, \u201cJoint semantic utterance classification and slot filling with recursive neural networks,\u201d in Spoken Language Technology Workshop (SLT), 2014 IEEE. IEEE, 2014, pp. 554\u2013559.\n[9] P. Xu and R. Sarikaya, \u201cConvolutional neural network based triangular crf for joint intent detection and slot filling,\u201d in Automatic Speech Recognition and Understanding (ASRU), 2013 IEEE Workshop on. IEEE, 2013, pp. 78\u201383.\n[10] I. Sutskever, O. Vinyals, and Q. V. Le, \u201cSequence to sequence learning with neural networks,\u201d in Advances in neural information processing systems, 2014, pp. 3104\u20133112.\n[11] W. Chan, N. Jaitly, Q. V. Le, and O. Vinyals, \u201cListen, attend and spell,\u201d arXiv preprint arXiv:1508.01211, 2015.\n[12] D. Bahdanau, K. Cho, and Y. Bengio, \u201cNeural machine translation by jointly learning to align and translate,\u201d arXiv preprint arXiv:1409.0473, 2014.\n[13] T. Mikolov, S. Kombrink, L. Burget, J. H. C\u030cernocky\u0300, and S. Khudanpur, \u201cExtensions of recurrent neural network language model,\u201d in Acoustics, Speech and Signal Processing (ICASSP), 2011 IEEE International Conference on. IEEE, 2011, pp. 5528\u20135531.\n[14] K. Cho, B. Van Merrie\u0308nboer, C. Gulcehre, D. Bahdanau, F. Bougares, H. Schwenk, and Y. Bengio, \u201cLearning phrase representations using rnn encoder-decoder for statistical machine translation,\u201d arXiv preprint arXiv:1406.1078, 2014.\n[15] A. Graves, N. Jaitly, and A.-r. Mohamed, \u201cHybrid speech recognition with deep bidirectional lstm,\u201d in Automatic Speech Recognition and Understanding (ASRU), 2013 IEEE Workshop on. IEEE, 2013, pp. 273\u2013278.\n[16] S. Hochreiter and J. Schmidhuber, \u201cLong short-term memory,\u201d Neural computation, vol. 9, no. 8, pp. 1735\u20131780, 1997.\n[17] X. Zhang, J. Zhao, and Y. LeCun, \u201cCharacter-level convolutional networks for text classification,\u201d in Advances in Neural Information Processing Systems, 2015, pp. 649\u2013657.\n[18] C. T. Hemphill, J. J. Godfrey, and G. R. Doddington, \u201cThe atis spoken language systems pilot corpus,\u201d in Proceedings, DARPA speech and natural language workshop, 1990, pp. 96\u2013101.\n[19] G. Tur, D. Hakkani-Tur, and L. Heck, \u201cWhat is left to be understood in atis?\u201d in Spoken Language Technology Workshop (SLT), 2010 IEEE. IEEE, 2010, pp. 19\u201324.\n[20] M. Jeong and G. Geunbae Lee, \u201cTriangular-chain conditional random fields,\u201d Audio, Speech, and Language Processing, IEEE Transactions on, vol. 16, no. 7, pp. 1287\u20131302, 2008.\n[21] W. Zaremba, I. Sutskever, and O. Vinyals, \u201cRecurrent neural network regularization,\u201d arXiv preprint arXiv:1409.2329, 2014.\n[22] R. Jozefowicz, W. Zaremba, and I. Sutskever, \u201cAn empirical exploration of recurrent network architectures,\u201d in Proceedings of the 32nd International Conference on Machine Learning (ICML15), 2015, pp. 2342\u20132350.\n[23] D. Kingma and J. Ba, \u201cAdam: A method for stochastic optimization,\u201d arXiv preprint arXiv:1412.6980, 2014.\n[24] B. Peng and K. Yao, \u201cRecurrent neural networks with external memory for language understanding,\u201d arXiv preprint arXiv:1506.00195, 2015.\n[25] G. Kurata, B. Xiang, B. Zhou, and M. Yu, \u201cLeveraging sentencelevel information with encoder lstm for natural language understanding,\u201d arXiv preprint arXiv:1601.01530, 2016.\n[26] G. Tur, D. Hakkani-Tu\u0308r, L. Heck, and S. Parthasarathy, \u201cSentence simplification for spoken language understanding,\u201d in Acoustics, Speech and Signal Processing (ICASSP), 2011 IEEE International Conference on. IEEE, 2011, pp. 5628\u20135631."}], "references": [{"title": "Optimizing svms for complex call classification", "author": ["P. Haffner", "G. Tur", "J.H. Wright"], "venue": "Acoustics, Speech, and Signal Processing, 2003. Proceedings.(ICASSP\u201903). 2003 IEEE International Conference on, vol. 1. IEEE, 2003, pp. I\u2013632.", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2003}, {"title": "Deep belief nets for natural language call-routing", "author": ["R. Sarikaya", "G.E. Hinton", "B. Ramabhadran"], "venue": "Acoustics, Speech and Signal Processing (ICASSP), 2011 IEEE International Conference on. IEEE, 2011, pp. 5680\u20135683.", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2011}, {"title": "Maximum entropy markov models for information extraction and segmentation.", "author": ["A. McCallum", "D. Freitag", "F.C. Pereira"], "venue": "in ICML, vol", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2000}, {"title": "Generative and discriminative algorithms for spoken language understanding.", "author": ["C. Raymond", "G. Riccardi"], "venue": "in INTERSPEECH,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2007}, {"title": "Spoken language understanding using long short-term memory neural networks", "author": ["K. Yao", "B. Peng", "Y. Zhang", "D. Yu", "G. Zweig", "Y. Shi"], "venue": "Spoken Language Technology Workshop (SLT), 2014 IEEE. IEEE, 2014, pp. 189\u2013194.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2014}, {"title": "Using recurrent neural networks for slot filling in spoken language understanding", "author": ["G. Mesnil", "Y. Dauphin", "K. Yao", "Y. Bengio", "L. Deng", "D. Hakkani- Tur", "X. He", "L. Heck", "G. Tur", "D. Yu"], "venue": "Audio, Speech, and Language Processing, IEEE/ACM Transactions on, vol. 23, no. 3, pp. 530\u2013539, 2015.", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2015}, {"title": "Recurrent neural network structured output prediction for spoken language understanding", "author": ["B. Liu", "I. Lane"], "venue": "Proc. NIPS Workshop on Machine Learning for Spoken Language Understanding and Interactions, 2015.", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2015}, {"title": "Joint semantic utterance classification and slot filling with recursive neural networks", "author": ["D. Guo", "G. Tur", "W.-t. Yih", "G. Zweig"], "venue": "Spoken Language Technology Workshop (SLT), 2014 IEEE. IEEE, 2014, pp. 554\u2013559.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2014}, {"title": "Convolutional neural network based triangular crf for joint intent detection and slot filling", "author": ["P. Xu", "R. Sarikaya"], "venue": "Automatic Speech Recognition and Understanding (ASRU), 2013 IEEE Workshop on. IEEE, 2013, pp. 78\u201383.", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2013}, {"title": "Sequence to sequence learning with neural networks", "author": ["I. Sutskever", "O. Vinyals", "Q.V. Le"], "venue": "Advances in neural information processing systems, 2014, pp. 3104\u20133112.", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2014}, {"title": "Listen, attend and spell", "author": ["W. Chan", "N. Jaitly", "Q.V. Le", "O. Vinyals"], "venue": "arXiv preprint arXiv:1508.01211, 2015.", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2015}, {"title": "Neural machine translation by jointly learning to align and translate", "author": ["D. Bahdanau", "K. Cho", "Y. Bengio"], "venue": "arXiv preprint arXiv:1409.0473, 2014.", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2014}, {"title": "Extensions of recurrent neural network language model", "author": ["T. Mikolov", "S. Kombrink", "L. Burget", "J.H. \u010cernock\u1ef3", "S. Khudanpur"], "venue": "Acoustics, Speech and Signal Processing (ICASSP), 2011 IEEE International Conference on. IEEE, 2011, pp. 5528\u20135531.", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2011}, {"title": "Learning phrase representations using rnn encoder-decoder for statistical machine translation", "author": ["K. Cho", "B. Van Merri\u00ebnboer", "C. Gulcehre", "D. Bahdanau", "F. Bougares", "H. Schwenk", "Y. Bengio"], "venue": "arXiv preprint arXiv:1406.1078, 2014.", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2014}, {"title": "Hybrid speech recognition with deep bidirectional lstm", "author": ["A. Graves", "N. Jaitly", "A.-r. Mohamed"], "venue": "Automatic Speech Recognition and Understanding (ASRU), 2013 IEEE Workshop on. IEEE, 2013, pp. 273\u2013278.", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2013}, {"title": "Long short-term memory", "author": ["S. Hochreiter", "J. Schmidhuber"], "venue": "Neural computation, vol. 9, no. 8, pp. 1735\u20131780, 1997.", "citeRegEx": "16", "shortCiteRegEx": null, "year": 1997}, {"title": "Character-level convolutional networks for text classification", "author": ["X. Zhang", "J. Zhao", "Y. LeCun"], "venue": "Advances in Neural Information Processing Systems, 2015, pp. 649\u2013657.", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2015}, {"title": "The atis spoken language systems pilot corpus", "author": ["C.T. Hemphill", "J.J. Godfrey", "G.R. Doddington"], "venue": "Proceedings, DARPA speech and natural language workshop, 1990, pp. 96\u2013101.", "citeRegEx": "18", "shortCiteRegEx": null, "year": 1990}, {"title": "What is left to be understood in atis?", "author": ["G. Tur", "D. Hakkani-Tur", "L. Heck"], "venue": "Spoken Language Technology Workshop (SLT),", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2010}, {"title": "Triangular-chain conditional random fields", "author": ["M. Jeong", "G. Geunbae Lee"], "venue": "Audio, Speech, and Language Processing, IEEE Transactions on, vol. 16, no. 7, pp. 1287\u20131302, 2008.", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2008}, {"title": "Recurrent neural network regularization", "author": ["W. Zaremba", "I. Sutskever", "O. Vinyals"], "venue": "arXiv preprint arXiv:1409.2329, 2014.", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2014}, {"title": "An empirical exploration of recurrent network architectures", "author": ["R. Jozefowicz", "W. Zaremba", "I. Sutskever"], "venue": "Proceedings of the 32nd International Conference on Machine Learning (ICML- 15), 2015, pp. 2342\u20132350.", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2015}, {"title": "Adam: A method for stochastic optimization", "author": ["D. Kingma", "J. Ba"], "venue": "arXiv preprint arXiv:1412.6980, 2014.", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2014}, {"title": "Recurrent neural networks with external memory for language understanding", "author": ["B. Peng", "K. Yao"], "venue": "arXiv preprint arXiv:1506.00195, 2015.", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2015}, {"title": "Leveraging sentencelevel information with encoder lstm for natural language understanding", "author": ["G. Kurata", "B. Xiang", "B. Zhou", "M. Yu"], "venue": "arXiv preprint arXiv:1601.01530, 2016.", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2016}, {"title": "Sentence simplification for spoken language understanding", "author": ["G. Tur", "D. Hakkani-T\u00fcr", "L. Heck", "S. Parthasarathy"], "venue": "Acoustics, Speech and Signal Processing (ICASSP), 2011 IEEE International Conference on. IEEE, 2011, pp. 5628\u20135631.", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2011}], "referenceMentions": [{"referenceID": 0, "context": "Intent detection can be treated as a semantic utterance classification problem, and popular classifiers like support vector machines (SVMs) [1] and deep neural network methods [2] can be applied.", "startOffset": 140, "endOffset": 143}, {"referenceID": 1, "context": "Intent detection can be treated as a semantic utterance classification problem, and popular classifiers like support vector machines (SVMs) [1] and deep neural network methods [2] can be applied.", "startOffset": 176, "endOffset": 179}, {"referenceID": 2, "context": "Popular approaches to solving sequence labeling problems include maximum entropy Markov models (MEMMs) [3], conditional random fields (CRFs) [4], and recurrent neural networks (RNNs) [5, 6, 7].", "startOffset": 103, "endOffset": 106}, {"referenceID": 3, "context": "Popular approaches to solving sequence labeling problems include maximum entropy Markov models (MEMMs) [3], conditional random fields (CRFs) [4], and recurrent neural networks (RNNs) [5, 6, 7].", "startOffset": 141, "endOffset": 144}, {"referenceID": 4, "context": "Popular approaches to solving sequence labeling problems include maximum entropy Markov models (MEMMs) [3], conditional random fields (CRFs) [4], and recurrent neural networks (RNNs) [5, 6, 7].", "startOffset": 183, "endOffset": 192}, {"referenceID": 5, "context": "Popular approaches to solving sequence labeling problems include maximum entropy Markov models (MEMMs) [3], conditional random fields (CRFs) [4], and recurrent neural networks (RNNs) [5, 6, 7].", "startOffset": 183, "endOffset": 192}, {"referenceID": 6, "context": "Popular approaches to solving sequence labeling problems include maximum entropy Markov models (MEMMs) [3], conditional random fields (CRFs) [4], and recurrent neural networks (RNNs) [5, 6, 7].", "startOffset": 183, "endOffset": 192}, {"referenceID": 7, "context": "Joint model for intent detection and slot filling has also been proposed in literature [8, 9].", "startOffset": 87, "endOffset": 93}, {"referenceID": 8, "context": "Joint model for intent detection and slot filling has also been proposed in literature [8, 9].", "startOffset": 87, "endOffset": 93}, {"referenceID": 9, "context": "Recently, encoder-decoder neural network models have been successfully applied in many sequence learning problems such as machine translation [10] and speech recognition [11].", "startOffset": 142, "endOffset": 146}, {"referenceID": 10, "context": "Recently, encoder-decoder neural network models have been successfully applied in many sequence learning problems such as machine translation [10] and speech recognition [11].", "startOffset": 170, "endOffset": 174}, {"referenceID": 11, "context": "The attention mechanism introduced in [12] enables the encoder-decoder architecture to learn to align and decode simultaneously.", "startOffset": 38, "endOffset": 42}, {"referenceID": 5, "context": "RNNs have been widely used in many sequence modeling problems [6, 13].", "startOffset": 62, "endOffset": 69}, {"referenceID": 12, "context": "RNNs have been widely used in many sequence modeling problems [6, 13].", "startOffset": 62, "endOffset": 69}, {"referenceID": 9, "context": "The RNN encoder-decoder framework is firstly introduced in [10] and [14].", "startOffset": 59, "endOffset": 63}, {"referenceID": 13, "context": "The RNN encoder-decoder framework is firstly introduced in [10] and [14].", "startOffset": 68, "endOffset": 72}, {"referenceID": 11, "context": "The attention mechanism later introduced in [12] enables the encoder-decoder model to learn a soft alignment and to decode at the same time.", "startOffset": 44, "endOffset": 48}, {"referenceID": 14, "context": "Bidirectional RNN has been successfully applied in speech recognition [15] and spoken language understanding [6].", "startOffset": 70, "endOffset": 74}, {"referenceID": 5, "context": "Bidirectional RNN has been successfully applied in speech recognition [15] and spoken language understanding [6].", "startOffset": 109, "endOffset": 112}, {"referenceID": 15, "context": "We use LSTM [16] as the basic recurrent network unit for its ability to better model long-term dependencies comparing to simple RNN.", "startOffset": 12, "endOffset": 16}, {"referenceID": 11, "context": "We use the last state of the backward encoder RNN to compute the initial decoder hidden state following the approach in [12].", "startOffset": 120, "endOffset": 124}, {"referenceID": 11, "context": ", hT ) [12]:", "startOffset": 7, "endOffset": 11}, {"referenceID": 16, "context": "If attention is not used, we apply mean-pooling [17] over time on the hidden states h followed by logistic regression to perform the intent classification.", "startOffset": 48, "endOffset": 52}, {"referenceID": 17, "context": "ATIS (Airline Travel Information Systems) data set [18] is widely used in SLU research.", "startOffset": 51, "endOffset": 55}, {"referenceID": 5, "context": "In this work, we follow the ATIS corpus setup used in [6, 7, 9, 19].", "startOffset": 54, "endOffset": 67}, {"referenceID": 6, "context": "In this work, we follow the ATIS corpus setup used in [6, 7, 9, 19].", "startOffset": 54, "endOffset": 67}, {"referenceID": 8, "context": "In this work, we follow the ATIS corpus setup used in [6, 7, 9, 19].", "startOffset": 54, "endOffset": 67}, {"referenceID": 18, "context": "In this work, we follow the ATIS corpus setup used in [6, 7, 9, 19].", "startOffset": 54, "endOffset": 67}, {"referenceID": 8, "context": "We obtained another ATIS text corpus that was used in [9] and [20] for SLU evaluation.", "startOffset": 54, "endOffset": 57}, {"referenceID": 19, "context": "We obtained another ATIS text corpus that was used in [9] and [20] for SLU evaluation.", "startOffset": 62, "endOffset": 66}, {"referenceID": 8, "context": "We use the same 10-fold cross validation setup as in [9] and [20].", "startOffset": 53, "endOffset": 56}, {"referenceID": 19, "context": "We use the same 10-fold cross validation setup as in [9] and [20].", "startOffset": 61, "endOffset": 65}, {"referenceID": 20, "context": "Our LSTM implementation follows the design in [21].", "startOffset": 46, "endOffset": 50}, {"referenceID": 21, "context": "The default forget gate bias is set to 1 [22].", "startOffset": 41, "endOffset": 45}, {"referenceID": 20, "context": "5 is applied to the non-recurrent connections [21] during model training for regularization.", "startOffset": 46, "endOffset": 50}, {"referenceID": 22, "context": "We use Adam optimization method following the suggested parameter setup in [23].", "startOffset": 75, "endOffset": 79}, {"referenceID": 8, "context": "CNN-CRF [9] 94.", "startOffset": 8, "endOffset": 11}, {"referenceID": 6, "context": "35 RNN with Label Sampling [7] 94.", "startOffset": 27, "endOffset": 30}, {"referenceID": 5, "context": "89 Hybrid RNN [6] 95.", "startOffset": 14, "endOffset": 17}, {"referenceID": 4, "context": "06 Deep LSTM [5] 95.", "startOffset": 13, "endOffset": 16}, {"referenceID": 23, "context": "08 RNN-EM [24] 95.", "startOffset": 10, "endOffset": 14}, {"referenceID": 24, "context": "25 Encoder-labeler Deep LSTM [25] 95.", "startOffset": 29, "endOffset": 33}, {"referenceID": 7, "context": "Recursive NN [8] 4.", "startOffset": 13, "endOffset": 16}, {"referenceID": 18, "context": "60 Boosting [19] 4.", "startOffset": 12, "endOffset": 16}, {"referenceID": 25, "context": "38 Boosting + Simplified sentences [26] 3.", "startOffset": 35, "endOffset": 39}, {"referenceID": 7, "context": "Model F1 Score Intent Error (%) RecNN [8] 93.", "startOffset": 38, "endOffset": 41}, {"referenceID": 7, "context": "60 RecNN+Viterbi [8] 93.", "startOffset": 17, "endOffset": 20}, {"referenceID": 8, "context": "To further verify the performance of our joint training models, we apply the proposed models on the additional ATIS data set and evaluate them with 10-fold cross validation same as in [9] and [20].", "startOffset": 184, "endOffset": 187}, {"referenceID": 19, "context": "To further verify the performance of our joint training models, we apply the proposed models on the additional ATIS data set and evaluate them with 10-fold cross validation same as in [9] and [20].", "startOffset": 192, "endOffset": 196}, {"referenceID": 19, "context": "TriCRF [20] 94.", "startOffset": 7, "endOffset": 11}, {"referenceID": 8, "context": "93 CNN TriCRF [9] 95.", "startOffset": 14, "endOffset": 17}], "year": 2016, "abstractText": "Attention-based encoder-decoder neural network models have recently shown promising results in machine translation and speech recognition. In this work, we propose an attention-based neural network model for joint intent detection and slot filling, both of which are critical steps for many speech understanding and dialog systems. Unlike in machine translation and speech recognition, alignment is explicit in slot filling. We explore different strategies in incorporating this alignment information to the encoder-decoder framework. Learning from the attention mechanism in encoder-decoder model, we further propose introducing attention to the alignment-based RNN models. Such attentions provide additional information to the intent classification and slot label prediction. Our independent task models achieve state-of-the-art intent detection error rate and slot filling F1 score on the benchmark ATIS task. Our joint training model further obtains 0.56% absolute (23.8% relative) error reduction on intent detection and 0.23% absolute gain on slot filling over the independent task models.", "creator": "LaTeX with hyperref package"}}}