{"id": "1609.01704", "review": {"conference": "iclr", "VERSION": "v1", "DATE_OF_SUBMISSION": "6-Sep-2016", "title": "Hierarchical Multiscale Recurrent Neural Networks", "abstract": "Learning both hierarchical and temporal representation has been among the long-standing challenges of recurrent neural networks. Multiscale recurrent neural networks have been considered as a promising approach to resolve this issue, yet there has been a lack of empirical evidence showing that this type of models can actually capture the temporal dependencies by discovering the latent hierarchical structure of the sequence. In this paper, we propose a novel multiscale approach, called the hierarchical multiscale recurrent neural networks, which can capture the latent hierarchical structure in the sequence by encoding the temporal dependencies with different timescales using a novel update mechanism. The hierarchical multiscale recurrent neural network is an unguided network of small groups of individuals that adapt to random events, and has a large temporal range. The multiscale recurrent neural networks are an independent, and therefore perform well in the face of multiple local conditions and a robust hierarchical neural network. Using their modular approach, we focus primarily on hierarchical networks that are capable of identifying a subset of time-dependent and long-lasting structural changes in the structure of the sequence.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n", "histories": [["v1", "Tue, 6 Sep 2016 19:37:57 GMT  (2030kb,D)", "http://arxiv.org/abs/1609.01704v1", null], ["v2", "Wed, 7 Sep 2016 18:33:08 GMT  (2030kb,D)", "http://arxiv.org/abs/1609.01704v2", null], ["v3", "Mon, 31 Oct 2016 06:37:30 GMT  (1143kb,D)", "http://arxiv.org/abs/1609.01704v3", null], ["v4", "Wed, 16 Nov 2016 07:44:27 GMT  (1143kb,D)", "http://arxiv.org/abs/1609.01704v4", null], ["v5", "Wed, 14 Dec 2016 18:41:53 GMT  (1144kb,D)", "http://arxiv.org/abs/1609.01704v5", null], ["v6", "Wed, 8 Mar 2017 07:33:38 GMT  (1144kb,D)", "http://arxiv.org/abs/1609.01704v6", null], ["v7", "Thu, 9 Mar 2017 05:22:52 GMT  (1144kb,D)", "http://arxiv.org/abs/1609.01704v7", null]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["junyoung chung", "sungjin ahn", "yoshua bengio"], "accepted": true, "id": "1609.01704"}, "pdf": {"name": "1609.01704.pdf", "metadata": {"source": "CRF", "title": "Hierarchical Multiscale Recurrent Neural Networks", "authors": ["Junyoung Chung", "Sungjin Ahn", "Yoshua Bengio"], "emails": ["junyoung.chung@umontreal.ca"], "sections": [{"heading": "1 Introduction", "text": "One of the key principles of learning in deep neural networks as well as in the human brain (Bengio, 2009; LeCun et al., 2015; Schmidhuber, 2015) is to obtain a hierarchical representation with increasing levels of abstraction. A stack of representation layers, learned from the data in a way to optimize a supervised or unsupervised target task, make deep neural networks entertain advantages such as generalization to unseen examples (Hoffman et al., 2013; Santoro et al., 2016), sharing learned knowledge among multiple tasks (Yosinski et al., 2014) and discovering disentangling factors of variation (Kingma and Welling, 2013). The remarkable recent successes of the deep convolutional neural networks are particularly based on this ability to learn hierarchical representation for spatial data (Krizhevsky et al., 2012; Mnih et al., 2015; Silver et al., 2016).\nFor modelling temporal data, the recent resurgence of recurrent neural networks (RNN) has led to remarkable advances (Graves et al., 2008, 2013; Cho et al., 2014; Sutskever et al., 2014; Vinyals et al., 2015). However, unlike the spatial data, learning both hierarchical and temporal representation has been among the long-standing challenges of RNNs in spite of the fact that hierarchical multiscale structures naturally exist in many temporal data (Schmidhuber, 1991; Mozer, 1993; El Hihi and Bengio, 1995; Kleinberg, 2003; Koutn\u00edk et al., 2014; Chung et al., 2016). For example, speech signals have a structure of (phone -> phoneme -> morpheme -> sentence), a text corpus has a structure of (character -> word -> phrase -> sentence -> paragraph) and computer programs have a structure of (statement -> procedure -> module).\nA promising approach to model such hierarchical and temporal representation is the multiscale RNNs (Schmidhuber, 1992; El Hihi and Bengio, 1995; Koutn\u00edk et al., 2014). Based on the observation that high-level abstraction changes slowly with temporal coherency while low-level abstraction has quickly changing features sensitive to the precise local timing (El Hihi and Bengio, 1995), the multiscale RNNs group hidden units into multiple modules of different timescales. In addition to the fact that the architecture fits naturally to the latent hierarchical structures in many temporal data, the multiscale approach provides the following advantages that resolve some inherent problems of\nar X\niv :1\n60 9.\n01 70\n4v 1\n[ cs\n.L G\n] 6\nS ep\n2 01\nstandard RNNs: (a) computational efficiency obtained by updating the states at high-level layers less frequently, (b) efficiently delivering long-term dependencies with fewer updates at the high-level layers, which mitigates the vanishing gradient problem, (c) efficient resource allocation (e.g., more hidden units to the higher layers that focus on long-term dependencies and less hidden units to the lower layers which are in charge of learning short-term dependencies). In addition, the learned latent hierarchical temporal structure can provide useful information to other downstream tasks such as module structures in computer program learning, the sub-task structure in hierarchical reinforcement learning and story segments in video understanding.\nThere have been various approaches to implementing the multiscale RNNs. The most popular approach is to set the timescales as hyperparameters (El Hihi and Bengio, 1995; Koutn\u00edk et al., 2014; Bahdanau et al., 2016) instead of treating them as dynamic variables that can be learned from the data (Schmidhuber, 1991, 1992; Chung et al., 2015a, 2016). However, considering the fact that non-stationarity is prevalent in temporal data and that many entities of abstraction such as words and sentences are in variable-length, we claim that it is important for an RNN to dynamically adapt its timescales to the particulars of the input of its length. While this is trivial if the hierarchical boundary structure is provided (Sordoni et al., 2015), it has been a challenge for an RNN to discover the latent hierarchical structure in temporal data without explicit boundary information.\nIn this paper, we propose a novel multiscale RNN model, which can learn the hierarchical multiscale structure from temporal data without explicit boundary information. This model, called a hierarchical multiscale recurrent neural network (HM-RNN), does not assign fixed update rates, but adaptively determines proper update times corresponding to different abstraction levels of the layers using the context inputs. We find that this model tends to learn fine timescales for low-level layers and coarse timescales for high-level layers. To do this, we introduce a binary boundary detector at each layer. The boundary detector is turned on only at the time steps where a segment of the corresponding abstraction level is completely processed. Otherwise, i.e., during the within segment processing, it stays turned off. Using the hierarchical boundary states, we implement three operations, UPDATE, COPY and FLUSH, from which we choose one operation at each time step. The UPDATE operation is similar to the usual update rule of the long short-term memory (LSTM) (Hochreiter and Schmidhuber, 1997), except that it is executed sparsely according to the detected boundaries. The COPY operation simply copies the cell and hidden states of the previous time step. Unlike the leaky integration of the LSTM or gated recurrent unit (GRU) (Cho et al., 2014), the COPY retains the whole states without any loss of information. The FLUSH operation is executed when a boundary is detected, where it first ejects the summarized representation of the current segment to the upper layer and then reinitializes the states to start processing the next segment. Learning to select a proper operation at each time step and to detect the boundaries, the HM-RNN discovers the latent hierarchical structure of the sequences. We find that the straight-through estimator (Bengio et al., 2013; Courbariaux et al., 2015) is efficient for training this model containing discrete variables.\nWe evaluate our model on two tasks: character-level language modelling and handwriting sequence generation. For the character-level language modelling, the HM-RNN achieves the state-of-the-art results on the Penn Treebank and Text8 datasets, and for the Hutter Prize Wikipedia dataset, the model obtains a tie result to the state-of-the-art result. The HM-RNN also outperforms the standard RNN on the handwriting sequence generation task using the IAM-OnDB dataset. In addition, we demonstrate that the hierarchical structure found by the HM-RNN is indeed very similar to the intrinsic structure observed in the data. The contributions of this paper are:\n\u2022 We propose for the first time an RNN model that can learn a latent hierarchical structure of a sequence without using explicit boundary information.\n\u2022 We show that it is beneficial to utilize the above structure through empirical evaluation.\n\u2022 We show that the straight-through estimator is an efficient way of training a model containing discrete variables.\n\u2022 We propose the slope annealing trick to improve the training procedure based on the straight-through estimator."}, {"heading": "2 Related Work", "text": "Two notable early attempts inspiring our model are Schmidhuber (1992) and El Hihi and Bengio (1995). In these works, it is advocated to stack multiple layers of RNNs in a decreasing order of update frequency for computational and learning efficiency. In Schmidhuber (1992), the author shows a model that can self-organize a hierarchical multiscale structure. Particularly in El Hihi and Bengio (1995), the advantages of incorporating a priori knowledge, \u201ctemporal dependencies are structured hierarchically\", into the RNN architecture is studied. The authors propose an RNN architecture that updates each layer with a fixed but different rate, called a hierarchical RNN.\nThe LSTM (Hochreiter and Schmidhuber, 1997) is probably the most popular RNN that employs the multiscale update concept. That is, the hidden units of the LSTM or stacked LSTM (Fern\u00e1ndez et al., 2007) have different forget and update rates and can operate with different timescales, however unlike to our model, these timescales are not organized hierarchically. Although the update of the LSTM by leaky integration helps to reduce the difficulty of learning long-term dependency by mitigating the vanishing gradient problem, the model still remains computationally expensive because it has to perform the update at every time step for each unit. Also, in practice, the long-term dependency captured by the LSTM units are still limited to a few hundred time steps due to the leaky integration by which the contents to memorize for a long-term is gradually diluted at every time step. However, our model is less prone to this problem because it learns a hierarchical structure such that, by design, high-level layers perform less frequent updates than low-level layers. We hypothesize that this property mitigates the vanishing gradient problem more efficiently while also being computationally more efficient.\nA more recent model, the clockwork RNN (CW-RNN) (Koutn\u00edk et al., 2014) extends the hierarchical RNN (El Hihi and Bengio, 1995) and tries to solve the issue of using soft timescales in the LSTM, by explicitly assigning hard timescales. In the CW-RNN, hidden units are partitioned into several modules, and different timescales are assigned to the modules such that a module i updates its hidden units at every 2(i\u22121)-th time step. The CW-RNN is computationally more efficient than the standard RNN including the LSTM since hidden units are updated only at the assigned clock rates. However, finding proper timescales in CW-RNN remains as a challenge, unlike our model, which learns the intrinsic timescales from the data.\nMore recently, in the Gated-Feedback RNNs (GF-RNN) (Chung et al., 2015a) and the Biscale RNNs (BiS-RNN) (Chung et al., 2016), the authors proposed to model layer-wise timescales more explicitly by having additional gating units to stacked RNN architecture. While the GF-RNN proposed a general framework where information can flow across multiple RNN layers via a gating mechanism, the BiS-RNN architecture is applied to character-level machine translation where two different timescales (e.g., character-level and word-level) seem to clearly exist in the data. However, similar to the LSTM, these two approaches rely on the soft gating mechanism, making the model suffer from the curse of update at every time step.\nOther forms of Hierarchical RNN (HRNN) architectures have been used in the case where the hierarchical boundary structure is explicitly observed in the data and easy to obtain. In Ling et al. (2015), the HRNN architecture is used for neural machine translation by modelling the characters and words using the first and second RNN layers, respectively, after obtaining the word boundary via tokenization. A similar HRNN architecture is also adopted in Sordoni et al. (2015) to model dialogue utterances. However, in many cases, hierarchical boundary information is not explicitly observed or expensive to obtain. Also, it is unclear how to deploy more layers than the number of boundary levels that is explicitly observed in the data.\nWhile the above models focus on the online prediction problem, where a prediction needs to be made by using only the past data, in some cases, predictions are made after observing the whole sequence. In this setting, the input sequence can be regarded as 1-D spatial data, using convolutional neural networks with 1-D kernels as proposed in Kim (2014) and Kim et al. (2015) for language modelling and sentence classification. Also, in Chan et al. (2016) and Bahdanau et al. (2016), the authors proposed to obtain high-level representation of the sequences of reduced length by repeatedly merging or pooling the lower level representation of the sequences.\nThe COPY operation used in our model can be related to Zoneout (Krueger et al., 2016) which is a recurrent generalization of stochastic depth (Huang et al., 2016). In Zoneout, an identity transformation is randomly applied to each hidden unit at each time step according to a Bernoulli\ndistribution. This results in the occasional copy operations of previous hidden states. While the focus of Zoneout is to propose a regularization technique similar to dropout (Srivastava et al., 2014) (where the regularization strength is controlled by a hyperparameter), our model learns (a) to dynamically determine when to copy from the context inputs and (b) to discover the latent hierarchical multiscale structure and representation. Although the main goal of our proposed model is not regularization, we found that our model also shows very good generalization performance while not requiring to set a fixed copy rate as it is done in Zoneout."}, {"heading": "3 Hierarchical Multiscale Recurrent Neural Networks", "text": ""}, {"heading": "3.1 Motivation", "text": "To begin with, we provide an example of how a stacked RNN can model temporal data in an ideal setting, i.e., when the hierarchy of segments is provided, e.g., spaces to separate words and periods to separate sentences (Sordoni et al., 2015; Ling et al., 2015). In Figure 1 (a), we depict a hierarchical RNN (HRNN) for language modelling with two layers: the first layer receives characters as inputs and generates word-level representations (C2W-RNN), and the second layer takes the word-level representations as inputs and yields phrase-level representations (W2P-RNN).\nAs shown, by means of the provided end-of-word labels, the C2W-RNN obtains word-level representation after processing the last character of each word and passes the word-level representation to the W2P-RNN. Then, the W2P-RNN performs an update of the phrase-level representation. Note that the hidden state of W2P-RNN remains unchanged while all the characters of the word are processed by the C2W-RNN. When the C2W-RNN starts to process the next word, its hidden state is reinitialized using the latest hidden state of the W2P-RNN, which contains a summarized history of all the words that have been processed by that time step, in that phrase. From this simple example, we can see the advantages of having a hierarchical multiscale structure: (1) as the W2P-RNN is updated at a much slower update rate than the C2W-RNN, a considerable amount of computation can be saved and (2) hidden units of the W2P-RNN level can be focused on modelling the dependencies at the word level, and gradients are backpropagated through a much smaller number of time steps. Furthermore, (3) layer-wise capacity control becomes possible (e.g., use a smaller number of hidden units in the first layer which models short-term dependencies but whose updates are invoked much more often).\nCan an RNN discover the latent hierarchical multiscale structure without explicit hierarchical boundary information? Considering the fact that the boundary information is difficult to obtain (for example consider languages where words are not always cleanly separated by spaces or punctuation symbols, and imperfect rules are used to separately perform segmentation) or not provided at all, this is a legitimate problem. It gets worse when we consider higher-level concepts which we would like the RNN to discover autonomously. In Section 2, we discussed the limitations of the existing RNN models under this setting, which either have to update all units at every time step or use fixed update frequencies (El Hihi and Bengio, 1995; Koutn\u00edk et al., 2014; Bahdanau et al., 2016). Unfortunately, this kind of approach is not well suited to the case where different segments in the hierarchical\ndecomposition have different lengths: for example, different words have different lengths, so a fixed hierarchy would not update its upper-level units in synchrony with the natural boundaries in the data."}, {"heading": "3.2 The Proposed Model", "text": "A key element of our model is the introduction of a parametrized boundary detector, which outputs a binary value, in each layer of a stacked RNN, and learns when a segment should end in such a way to optimize the overall target objective. Whenever the boundary detector is turned on at a time step of layer ` (i.e., when the boundary state is 1), the model considers this to be the end of a segment corresponding to the latent abstraction level of that layer (e.g., word or phrase) and feeds the summarized representation of the detected segment into the upper layer (` + 1). At each time step, each layer selects one of the following operations: UPDATE, COPY or FLUSH. The selection is determined by two boundary states, (1) current boundary state in the layer below z`\u22121t and (2) the boundary state of the previous time step of the same layer z`t\u22121.\nIn the following, we describe an HM-RNN based on the LSTM update rule. We call this model a hierarchical multiscale LSTM (HM-LSTM). Consider an HM-LSTM model of L layers (` = 1, . . . , L) which, at each layer `, performs the following update at each time step t:\nh`t, c ` t, z ` t = f ` HM-LSTM(c ` t\u22121,h ` t\u22121,h `\u22121 t ,h `+1 t\u22121, z ` t\u22121, z `\u22121 t ). (1)\nHere, we use h and c to denote the hidden and cell states, respectively. The function f `HM-LSTM is implemented as follows. First, using the two boundary states z`t\u22121 and z `\u22121 t , the cell state is updated by:\nc`t =  f `t c`t\u22121 + i`t g`t if z`t\u22121 = 0 and z`\u22121t = 1 (UPDATE) c`t\u22121 if z ` t\u22121 = 0 and z `\u22121 t = 0 (COPY)\ni`t g`t if z`t\u22121 = 1 (FLUSH). (2)\nand then the hidden state is obtained by:\nh`t = { h`t\u22121 if COPY, o`t tanh(c`t) otherwise.\n(3)\nHere (f , i,o) are forget, input, output gates, and g is a cell proposal. We shall describe the update equations of these variables. Note that unlike the LSTM, we do not need to compute these gates and cell proposal values at every time step. For example, in the case of COPY operation, we do not need to compute any of these values and thus can save computations.\nThe COPY operation, which simply performs (c`t,h ` t)\u2190 (c`t\u22121,h`t\u22121), implements the observation that an upper layer should keep its state unchanged until it receives the summarized input from the lower layer. If the boundary z`\u22121t is detected from the layer below but the boundary z ` t\u22121 was not found at the previous time step, the UPDATE operation is performed to update the summary representation of the layer `. Hence, this UPDATE operation is executed sparsely unlike the standard RNNs where it is executed at every time step, making it computationally inefficient. If a boundary is detected, the FLUSH operation is executed. The FLUSH operation consists of two sub-operations:\n(a) EJECT to pass the current state to the upper layer and then (b) RESET to reinitialize the state before starting to read a new segment. This operation implicitly forces the upper layer to absorb the summary information of the lower layer segment, because otherwise it will be lost. Note that the FLUSH operation is a hard reset in the sense that it completely erases all the previous states of the same layer, which is different from the soft reset or forget operation by leaky integration in the GRU or LSTM.\nWhenever needed (depending on the chosen operation), the gate values (f `t , i ` t,o ` t), the cell proposal g`t and the pre-activation of the boundary detector z\u0303 ` t are then obtained by:\nf `t i`t o`t g`t z\u0303`t\n =  sigm sigm sigm tanh\nhard sigm  fslice (srecurrent(`)t + stop-down(`)t + sbottom-up(`)t + b(`)) , (4) where\ns recurrent(`) t = U ` `h ` t\u22121, (5)\ns top-down(`) t = z ` t\u22121W ` `+1h `+1 t\u22121, (6)\ns bottom-up(`) t = z `\u22121 t W ` `\u22121h `\u22121 t . (7)\nHere, we use W ba , U b a \u2208 R(4dim(h)+1)\u00d7dim(h) to denote state transition parameters from layer a to layer b. The fslice denotes a function that slices a given vector. In the last layer L, the top-down connection is ignored, and we use h0t = xt. Since the input should not be omitted, we set z 0 t = 1 for all t. Also, we do not use the boundary detector for the last layer. The hard sigm is defined by hard sigm(x) = max ( 0,min ( 1, ax+12 )) with a being the slope variable.\nUnlike the standard LSTM, the HM-LSTM has a top-down connection from (`+ 1) to `, which is allowed to be activated only if a boundary is detected at the previous time step of the layer ` (see Eq. 6). This makes the layer ` to be initialized with more long-term information after the boundary is detected and execute the FLUSH operation. In addition, the input from the lower layer (` \u2212 1) becomes effective only when a boundary is detected at the current time step in the layer (`\u2212 1) due to the binary gate z`\u22121t . Figure 2 (left) shows the gating mechanism of the HM-LSTM at time step t.\nFinally, the binary boundary state z`t is obtained by:\nz`t = fbound(z\u0303 ` t ). (8)\nFor binarization function fbound : R\u2192 {0, 1}, we can either use a deterministic step function:\nz`t = { 1 if z\u0303`t > 0.5 0 otherwise,\n(9)\nor sample from the Bernoulli distribution z`t \u223c Bernoulli(z\u0303`t ). Although this binary decision is a key to our model, it is usually difficult to use stochastic gradient descent to train such model with discrete decisions as it is not differentiable. We shall discuss more on this in the next section."}, {"heading": "3.3 Computing Gradient of Boundary Detector", "text": "Training neural networks with discrete variables requires more efforts since the standard backpropagation is no longer applicable due to the non-differentiability. Among a few methods for training a neural network with discrete variables such as the REINFORCE (Williams, 1992; Mnih and Gregor, 2014) and the straight-through estimator (Bengio et al., 2013), we use the straight-through estimator to train our model. The straight-through estimator is a biased estimator because the non-differentiable function used in the forward pass (the step function in our case) is replaced by a differentiable function during the backward pass (the hard sigmoid function in our case). The straight-through estimator, however, is much simpler and often works more efficiently in practice than other unbiased but high-variance estimators such as the REINFORCE. The straight-through estimator has also been used in Courbariaux et al. (2015) and Mnih et al. (2016).\nThe Slope Annealing Trick. In our experiment, we use the slope annealing trick to reduce the bias of the straight-through estimator. The idea is to reduce the discrepancy between the two functions\nused during the forward pass and the backward pass. That is, by gradually increasing the slope a of the hard sigmoid function, we make the hard sigmoid be close to the step function. Note that starting with a high slope value from the beginning makes the training difficult while it is more applicable later when the model parameters become more stable. In our experiments, starting from slope a = 1, we slowly increase the slope until it reaches a threshold with an appropriate scheduling."}, {"heading": "4 Experiments", "text": "We evaluate the proposed model on two tasks, character-level language modelling and handwriting sequence generation. Character-level language modelling is a representative example of discrete sequence modelling, where the discrete symbols form a distinct hierarchical multiscale structure. The performance on real-valued sequences is tested on the handwriting sequence generation task in which a relatively clear hierarchical multiscale structure exists compared to other data such as speech signals."}, {"heading": "4.1 Character-Level Language modelling", "text": "A sequence modelling task aims at learning the probability distribution over sequences by minimizing the negative log-likelihood of the training sequences:\nmin \u03b8 \u2212 1 N N\u2211 n=1 Tn\u2211 t=1 log p (xnt | xn<t; \u03b8) , (10)\nwhere \u03b8 is the model parameter, N is the number of training sequences, and Tn is the length of the n-th sequence. A word at time t of sequence n is denoted by xnt , and x n <t denotes all previous words at time t. We evaluate our model on three benchmark text corpora: (1) Penn Treebank, (2) Text8 and (3) Hutter Prize Wikipedia. We use the bits-per-character (BPC), E[\u2212 log2 p(xt+1 | x\u2264t)], as the evaluation metric.\nModel We use a model consisting of an RNN module and an output module. The RNN module is the HM-LSTM, described in Section 3, with three layers. The output module is a feedforward neural network with two hidden layers, an embedding layer and an output layer. Figure 2 (right) shows a diagram of the output module. At each time step, the embedding layer receives the hidden states of the three RNN layers as input. In order to adaptively control the importance of each layer at each time step, we also introduce three scalar gating units g`t \u2208 R to each of the layer outputs:\ng`t = sigm(w `[h1t ; \u00b7 \u00b7 \u00b7 ;hLt ]), (11)\nwhere w` \u2208 R \u2211L\n`=1 dim(h `) is the weight parameter. The output embedding het is computed by:\nhet = ReLU ( L\u2211 `=1 g`tW e `h ` t ) , (12)\nwhere L = 3 and ReLU(x) = max(0, x) (Nair and Hinton, 2010). Finally, the probability distribution for the next target character is computed by the softmax function, softmax(xj) = e\nxj\u2211K k=1 e xk , where\neach output class is a character.\nPenn Treebank We process the Penn Treebank dataset (Marcus et al., 1993) by following the procedure introduced in Mikolov et al. (2012). Each update is done by using a mini-batch of 64 examples of length 100 to prevent the memory overflow problem when unfolding the RNN in time for backpropagation. The last hidden state of a sequence is used to initialize the hidden state of the next sequence to approximate the full backpropagation. We train the model using Adam (Kingma and Ba, 2014) with an initial learning rate of 0.002. The norm of the gradient is clipped with a threshold of 1 (Pascanu et al., 2012). For all of the character-level language modelling experiments, we apply the same procedure, but only change the number of hidden units, mini-batch size and the initial learning rate.\nFor the Penn Treebank dataset, we use 500 units in each layer of the HM-LSTM and 500 units for the embedding layer of the output module. We apply batch normalization (Ioffe and Szegedy, 2015; Cooijmans et al., 2016). In Table 1, we compare the test BPCs of four variants of our model to other baseline models. Note that the HM-LSTM using the step function for the hard boundary decision outperforms the other HM-LSTMs using either sampling or soft boundary decision (i.e., hard sigmoid). The test BPC is further improved with the slope annealing trick, which reduces the bias of the straight-through estimator. We increased the slope from a = 1 to a = 5 with the following schedule a = min (5, 1 + 0.04 \u00b7Nepoch), where Nepoch is the maximum number of epochs. The best BPC 1.27 is achieved by the HM-LSTM. For the next two tasks on Text8 and Hutter Prize Wikipedia datasets, we fixed the hard boundary decision using the step function without slope annealing due to the difficulty of finding a good annealing schedule on large-scale datasets.\nText8 The Text8 dataset (Mahoney, 2009) consists of 100M characters extracted from the Wikipedia corpus. Text8 contains only alphabets and spaces, and thus we have total 27 symbols. In order\nto compare with other previous works, we follow the data splits used in Mikolov et al. (2012); Cooijmans et al. (2016); Wu et al. (2016). We use 1024 units for each HM-LSTM layer and 2048 units for the embedding layer of the output module. The mini-batch size and the initial learning rate are set to 128 and 0.001, respectively. The results are shown in Table 2. Our model obtains the state-of-the-art BPC 1.30.\nHutter Prize Wikipedia The third dataset for language modelling is the Hutter Prize Wikipedia (also known as enwik8) dataset (Hutter, 2012). This corpus contains 205 symbols including XML markups and special characters. We follow the data preparation process used in Mikolov et al. (2012) and Graves (2013) where the first 90M characters are used to train the model, the next 5M characters for validation, and the remainders for the test set. We use the same model size as in the Text8 experiments but apply layer normalization (Ba et al., 2016) to the model. The mini-batch size and the initial learning rate are set to 128 and 0.001, respectively. In Table 3, we show the HM-LSTM achieving the test BPC 1.40, which is a tie result to the current state-of-the-art BPC.\nVisualizing Learned Hierarchical Multiscale Structure In Figure 3, we visualize the boundaries detected by the boundary detectors of the HM-LSTM while reading a character sequence of total length 270 taken from the validation set of the Penn Treebank corpus. Due to the page width limit, the figure contains the sequence partitioned into three segments of length 90. The white blocks indicate boundaries z`t = 1 while the black blocks indicate the non-boundaries z ` t = 0. Figure 4 shows the result of the same procedure but on a validation sequence from the Hutter Prize Wikipedia dataset.\nInterestingly, in both Figure 3 and Figure 4, we can observe that the boundary detector of the first layer, z1, tends to be turned on when it sees a space, which is a reasonable breakpoint to separate between words. This is somewhat surprising because the model self-organizes this structure without any explicit boundary information. Another interesting point is that the model tends to skip flushing at spaces inside a phrase of a frequent n-gram. For example, the following phrases, \u201cweeks ago\u201d, \u201cmost famous\u201d, \u201cmore useful\u201d, and \u201cAncient cultures\u201d, where the model skips flushing at space, all seem to be highly frequent 2-grams. Conversely, we also see flushing in the middle of a word, e.g., \u201ctele-FLUSH-phone\u201d and \u201chigh-FLUSH-land\u201d. Note that \u201ctele\u201d and \u201chigh\u201d are prefixes after which a various number of postfixes can follow. From these, it seems that the model uses to some extent the concept of surprise to learn the boundary. Although interpretation of the second layer boundaries is not as apparent as the first layer boundaries, it seems to segment at reasonable semantic / syntactic boundaries, e.g., \u201cconsumers may\u201d - \u201cwant to move their telephones a\u201d - \u201clittle closer to the tv set <unk>\u201d, and so on.\nAnother remarkable point is the fact that we do not pose any constraint on the number of boundaries that the model can fire up. The model, however, learns that it is more beneficial to delay the information ejection to some extent. This is somewhat counterintuitive because it might look more beneficial to feed the fresh update to the upper layers at every time step without any delay. We conjecture the reason that the model works in this way is due to the FLUSH operation. That is, the FLUSH poses an implicit constraint on the frequency of boundary detection because it contains both a reward (feeding fresh information to upper layers) and a penalty (erasing accumulated information). The model finds an optimal balance between the reward and the penalty.\nTo understand the update mechanism more intuitively, in Figure 5, we also depict the heatmap of the `2-norm of the hidden states along with the states of the boundary detectors. As we expect, we can see that there is no change in the norm value within segments due to the COPY operation. Also, the color of \u2016h1\u2016 changes quickly (at every time step) because there is no COPY operation in the first layer. The color of \u2016h2\u2016 changes less frequently based on the states of z1t and z2t\u22121. The color of \u2016h3\u2016 changes even slowly, i.e., only when z2t = 1. A notable advantage of the proposed architecture is that the internal process of the RNN becomes more interpretable. For example, we can substitute the states of z1t and z 2 t\u22121 into Eq. 2 and infer which operation among the UPDATE, COPY and FLUSH was applied to the second layer at time step t. We can also inspect the update frequencies of the layers simply by counting how many UPDATE and FLUSH operations were made in each layer. For example in Figure 3, we see that the first layer updates at every time step (which is 270 UPDATE operations), the second layer updates 54 times, and only 7 updates has made in the third layer. Note that, by design, the first layer performs UPDATE operation at every time step and then the number of UPDATE operations decreases as the layer level increases. In this example, the total number of updates is 331 for the HM-LSTM which is 60% of reduction from the 810 updates of the standard RNN architecture."}, {"heading": "4.2 Handwriting Sequence Generation", "text": "We extend the evaluation of the HM-LSTM to a real-valued sequence modelling task using IAMOnDB (Liwicki and Bunke, 2005) dataset. The IAM-OnDB dataset consists of 12, 179 handwriting examples, each of which is a sequence of (x, y) coordinate and a binary indicator p for pen-tip location, giving us (x1:Tn , y1:Tn , p1:Tn), where n is an index of a sequence. At each time step, the model receives (xt, yt, pt), and the goal is to predict (xt+1, yt+1, pt+1). The pen-up (pt = 1) indicates an end of a stroke, and the pen-down (pt = 0) indicates that a stroke is in progress. There is usually a large shift in the (x, y) coordinate to start a new stroke after the pen-up happens. We remove all sequences whose length is shorter than 300 or longer than 1200. This leaves us 10, 000 sequences for training, 587 for validation, 587 for test. The average length of the sequences is 642. We rescale the range of the (x, y) coordinates of each sequence by dividing with a factor of 100. Note that this process makes our numbers in Table 4 to become not directly comparable to the previous works (Graves, 2013; Chung et al., 2015b). However, we train both the baseline model and proposed\nmodel using the same dataset. We use the mini-batch size of 32, and the initial learning rate is set to 0.0003.\nFor this experiment, we use the same model as the one used in the character-level language model, except that the output layer is modified to predict real-valued outputs. We use the mixture density network (Bishop, 1994) as the output layer following Graves (2013), and use 400 units for each HM-LSTM layer and 800 units for the embedding layer of the output module. For comparison, we also evaluate a stacked LSTM which has the same model size as the HM-LSTM. In Table 4, we compare the log-likelihood averaged over the test sequences of the IAM-OnDB dataset. We observe that the HM-LSTM outperforms the standard LSTM.\nLearned Boundary Detector We let the HM-LSTM read some validation sequences and visualize the states of the boundary detector of the first layer, z1. In Figure 6-(a), we print the ground truth of pen-tip location on the left and the states of the z1 on the right. A blue dot means either pt = 0 or z1t = 0, and a red dot indicates either pt = 1 or z 1 t = 1. We observe that the states of the boundary detector, z1, shown in (a)-right are almost the same as the states of the pen-tip location shown in (a)-left. In Figure 6-(b), we present the visualization of handwriting examples by segments based on either the states of z1 or the states of pen-tip location1. In Figure 7-(a), the z1 is turned on at a point, which is not an end of a stroke in the ground truth. But in most of the cases, the states of z1 match well with the states of pen-up location. This suggests that the boundary detector, z1, finds the ends of strokes as useful breakpoints of the segments for the first level, and that the first layer will propagate the summarized representation of each stroke to its upper layer. In Figure 7-(b), we again, present the visualization of handwriting examples by segments based on either the states of pen-tip location or states of z1. We did not find any interesting pattern from the boundary detector of the second layer, z2.\n1The plot function could be found at http://blog.otoro.net/2015/12/12/handwriting-generation-demo-in-tensorflow/."}, {"heading": "5 Conclusion", "text": "In this paper, we proposed a novel multiscale RNN architecture that can capture the latent hierarchical structure of the sequences. We introduced two types of new operations to the RNN, which are the COPY and FLUSH operations. In order to implement these operations, we introduced a set of binary variables and a novel update rule that is dependent on the states of these binary variables. Each binary variable is learned to find segments at its level, therefore, we call this binary variable, a boundary detector. On the character-level language modelling task, the HM-LSTM achieved state-of-the-art results on the Penn Treebank and Text8 datasets and a tie result to the state-of-the-art result on the Hutter Prize Wikipedia dataset. Also, the HM-LSTM outperformed the standard LSTM on the handwriting sequence generation task. Our results and analysis suggest that the proposed HM-RNN can discover the latent hierarchical structure of the sequences and can learn efficient hierarchical multiscale representation that leads to better generalization performance."}, {"heading": "Acknowledgments", "text": "The authors would like to thank Alex Graves, Tom Schaul and Hado van Hasselt for their fruitful comments and discussion. We acknowledge the support of the following agencies for research funding and computing support: Ubisoft, Samsung, NSERC, Calcul Qu\u00e9bec, Compute Canada, the Canada Research Chairs and CIFAR. The authors thank the developers of Theano (Team et al., 2016). JC would like to thank Arnaud Bergenon and Fr\u00e9d\u00e9ric Bastien for their technical support. JC would also like to thank Guillaume Alain, Kyle Kastner and David Ha for providing us useful pieces of code."}], "references": [{"title": "Layer normalization", "author": ["J.L. Ba", "J.R. Kiros", "G.E. Hinton"], "venue": "arXiv preprint arXiv:1607.06450.", "citeRegEx": "Ba et al\\.,? 2016", "shortCiteRegEx": "Ba et al\\.", "year": 2016}, {"title": "End-to-end attention-based large vocabulary speech recognition", "author": ["D. Bahdanau", "J. Chorowski", "D. Serdyuk", "Y Bengio"], "venue": "In 2016 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP),", "citeRegEx": "Bahdanau et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Bahdanau et al\\.", "year": 2016}, {"title": "Learning deep architectures for ai", "author": ["Y. Bengio"], "venue": "Foundations and trends R", "citeRegEx": "Bengio,? 2009", "shortCiteRegEx": "Bengio", "year": 2009}, {"title": "Estimating or propagating gradients through stochastic neurons for conditional computation", "author": ["Y. Bengio", "N. L\u00e9onard", "A. Courville"], "venue": "arXiv preprint arXiv:1308.3432.", "citeRegEx": "Bengio et al\\.,? 2013", "shortCiteRegEx": "Bengio et al\\.", "year": 2013}, {"title": "Mixture density networks", "author": ["C.M. Bishop"], "venue": null, "citeRegEx": "Bishop,? \\Q1994\\E", "shortCiteRegEx": "Bishop", "year": 1994}, {"title": "Listen, attend and spell: A neural network for large vocabulary conversational speech recognition", "author": ["W. Chan", "N. Jaitly", "Q. Le", "O. Vinyals"], "venue": "2016 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), pages 4960\u20134964. IEEE.", "citeRegEx": "Chan et al\\.,? 2016", "shortCiteRegEx": "Chan et al\\.", "year": 2016}, {"title": "Learning phrase representations using RNN encoder-decoder for statistical machine translation", "author": ["K. Cho", "B. van Merrienboer", "C. Gulcehre", "F. Bougares", "H. Schwenk", "Y. Bengio"], "venue": "In Proceedings of the Empiricial Methods in Natural Language Processing (EMNLP", "citeRegEx": "Cho et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Cho et al\\.", "year": 2014}, {"title": "Gated feedback recurrent neural networks", "author": ["J. Chung", "C. Gulcehre", "K. Cho", "Y. Bengio"], "venue": "Proceedings of the 32nd International Conference on Machine Learning.", "citeRegEx": "Chung et al\\.,? 2015a", "shortCiteRegEx": "Chung et al\\.", "year": 2015}, {"title": "A recurrent latent variable model for sequential data", "author": ["J. Chung", "K. Kastner", "L. Dinh", "K. Goel", "A. Courville", "Y. Bengio"], "venue": "Advances in Neural Information Processing Systems.", "citeRegEx": "Chung et al\\.,? 2015b", "shortCiteRegEx": "Chung et al\\.", "year": 2015}, {"title": "A character-level decoder without explicit segmentation for neural machine translation", "author": ["J. Chung", "K. Cho", "Y. Bengio"], "venue": "arXiv preprint arXiv:1603.06147.", "citeRegEx": "Chung et al\\.,? 2016", "shortCiteRegEx": "Chung et al\\.", "year": 2016}, {"title": "Recurrent batch normalization", "author": ["T. Cooijmans", "N. Ballas", "C. Laurent", "A. Courville"], "venue": "arXiv preprint arXiv:1603.09025.", "citeRegEx": "Cooijmans et al\\.,? 2016", "shortCiteRegEx": "Cooijmans et al\\.", "year": 2016}, {"title": "Binaryconnect: Training deep neural networks with binary weights during propagations", "author": ["M. Courbariaux", "Y. Bengio", "David", "J.-P."], "venue": "Advances in Neural Information Processing Systems, pages 3123\u20133131.", "citeRegEx": "Courbariaux et al\\.,? 2015", "shortCiteRegEx": "Courbariaux et al\\.", "year": 2015}, {"title": "Hierarchical recurrent neural networks for long-term dependencies", "author": ["S. El Hihi", "Y. Bengio"], "venue": "Advances in Neural Information Processing Systems, pages 493\u2013499. Citeseer.", "citeRegEx": "Hihi and Bengio,? 1995", "shortCiteRegEx": "Hihi and Bengio", "year": 1995}, {"title": "Sequence labelling in structured domains with hierarchical recurrent neural networks", "author": ["S. Fern\u00e1ndez", "A. Graves", "J. Schmidhuber"], "venue": "Proceedings of the 20th international joint conference on Artifical intelligence, pages 774\u2013779. Morgan Kaufmann Publishers Inc.", "citeRegEx": "Fern\u00e1ndez et al\\.,? 2007", "shortCiteRegEx": "Fern\u00e1ndez et al\\.", "year": 2007}, {"title": "Generating sequences with recurrent neural networks", "author": ["A. Graves"], "venue": "arXiv preprint arXiv:1308.0850.", "citeRegEx": "Graves,? 2013", "shortCiteRegEx": "Graves", "year": 2013}, {"title": "Unconstrained on-line handwriting recognition with recurrent neural networks", "author": ["A. Graves", "M. Liwicki", "H. Bunke", "J. Schmidhuber", "S. Fern\u00e1ndez"], "venue": "Advances in Neural Information Processing Systems, pages 577\u2013584.", "citeRegEx": "Graves et al\\.,? 2008", "shortCiteRegEx": "Graves et al\\.", "year": 2008}, {"title": "Speech recognition with deep recurrent neural networks", "author": ["A. Graves", "Mohamed", "A.-R.", "G. Hinton"], "venue": "Acoustics, Speech and Signal Processing (ICASSP), 2013 IEEE International Conference on, pages 6645\u20136649. IEEE.", "citeRegEx": "Graves et al\\.,? 2013", "shortCiteRegEx": "Graves et al\\.", "year": 2013}, {"title": "Long short-term memory", "author": ["S. Hochreiter", "J. Schmidhuber"], "venue": "Neural computation, 9(8), 1735\u20131780.", "citeRegEx": "Hochreiter and Schmidhuber,? 1997", "shortCiteRegEx": "Hochreiter and Schmidhuber", "year": 1997}, {"title": "One-shot adaptation of supervised deep convolutional models", "author": ["J. Hoffman", "E. Tzeng", "J. Donahue", "Y. Jia", "K. Saenko", "T. Darrell"], "venue": "arXiv preprint arXiv:1312.6204.", "citeRegEx": "Hoffman et al\\.,? 2013", "shortCiteRegEx": "Hoffman et al\\.", "year": 2013}, {"title": "Deep networks with stochastic depth", "author": ["G. Huang", "Y. Sun", "Z. Liu", "D. Sedra", "K. Weinberger"], "venue": "arXiv preprint arXiv:1603.09382.", "citeRegEx": "Huang et al\\.,? 2016", "shortCiteRegEx": "Huang et al\\.", "year": 2016}, {"title": "The human knowledge compression contest", "author": ["M. Hutter"], "venue": null, "citeRegEx": "Hutter,? \\Q2012\\E", "shortCiteRegEx": "Hutter", "year": 2012}, {"title": "Batch normalization: Accelerating deep network training by reducing internal covariate shift", "author": ["S. Ioffe", "C. Szegedy"], "venue": "arXiv preprint arXiv:1502.03167.", "citeRegEx": "Ioffe and Szegedy,? 2015", "shortCiteRegEx": "Ioffe and Szegedy", "year": 2015}, {"title": "Grid long short-term memory", "author": ["N. Kalchbrenner", "I. Danihelka", "A. Graves"], "venue": "arXiv preprint arXiv:1507.01526.", "citeRegEx": "Kalchbrenner et al\\.,? 2015", "shortCiteRegEx": "Kalchbrenner et al\\.", "year": 2015}, {"title": "Convolutional neural networks for sentence classification", "author": ["Y. Kim"], "venue": "arXiv preprint arXiv:1408.5882.", "citeRegEx": "Kim,? 2014", "shortCiteRegEx": "Kim", "year": 2014}, {"title": "Character-aware neural language models", "author": ["Y. Kim", "Y. Jernite", "D. Sontag", "A.M. Rush"], "venue": "arXiv preprint arXiv:1508.06615.", "citeRegEx": "Kim et al\\.,? 2015", "shortCiteRegEx": "Kim et al\\.", "year": 2015}, {"title": "Adam: A method for stochastic optimization", "author": ["D.P. Kingma", "J. Ba"], "venue": "arXiv preprint arXiv:1412.6980.", "citeRegEx": "Kingma and Ba,? 2014", "shortCiteRegEx": "Kingma and Ba", "year": 2014}, {"title": "Auto-encoding variational bayes", "author": ["D.P. Kingma", "M. Welling"], "venue": "arXiv preprint arXiv:1312.6114.", "citeRegEx": "Kingma and Welling,? 2013", "shortCiteRegEx": "Kingma and Welling", "year": 2013}, {"title": "Bursty and hierarchical structure in streams", "author": ["J. Kleinberg"], "venue": "Data Mining and Knowledge Discovery, 7(4), 373\u2013397.", "citeRegEx": "Kleinberg,? 2003", "shortCiteRegEx": "Kleinberg", "year": 2003}, {"title": "A clockwork rnn", "author": ["J. Koutn\u00edk", "K. Greff", "F. Gomez", "J. Schmidhuber"], "venue": "Proceedings of the 31st International Conference on Machine Learning (ICML 2014).", "citeRegEx": "Koutn\u00edk et al\\.,? 2014", "shortCiteRegEx": "Koutn\u00edk et al\\.", "year": 2014}, {"title": "Imagenet classification with deep convolutional neural networks", "author": ["A. Krizhevsky", "I. Sutskever", "G.E. Hinton"], "venue": "Advances in Neural Information Processing Systems, pages 1097\u20131105.", "citeRegEx": "Krizhevsky et al\\.,? 2012", "shortCiteRegEx": "Krizhevsky et al\\.", "year": 2012}, {"title": "Regularizing rnns by stabilizing activations", "author": ["D. Krueger", "R. Memisevic"], "venue": "arXiv preprint arXiv:1511.08400.", "citeRegEx": "Krueger and Memisevic,? 2015", "shortCiteRegEx": "Krueger and Memisevic", "year": 2015}, {"title": "Zoneout: Regularizing rnns by randomly preserving hidden activations", "author": ["D. Krueger", "T. Maharaj", "J. Kram\u00e1r", "M. Pezeshki", "N. Ballas", "N.R. Ke", "A. Goyal", "Y. Bengio", "H. Larochelle", "A Courville"], "venue": "arXiv preprint arXiv:1606.01305", "citeRegEx": "Krueger et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Krueger et al\\.", "year": 2016}, {"title": "Deep learning", "author": ["Y. LeCun", "Y. Bengio", "G. Hinton"], "venue": "Nature, 521(7553), 436\u2013444.", "citeRegEx": "LeCun et al\\.,? 2015", "shortCiteRegEx": "LeCun et al\\.", "year": 2015}, {"title": "Character-based neural machine translation", "author": ["W. Ling", "I. Trancoso", "C. Dyer", "A.W. Black"], "venue": "arXiv preprint arXiv:1511.04586.", "citeRegEx": "Ling et al\\.,? 2015", "shortCiteRegEx": "Ling et al\\.", "year": 2015}, {"title": "Iam-ondb-an on-line english sentence database acquired from handwritten text on a whiteboard", "author": ["M. Liwicki", "H. Bunke"], "venue": "Eighth International Conference on Document Analysis and Recognition (ICDAR\u201905), pages 956\u2013961. IEEE.", "citeRegEx": "Liwicki and Bunke,? 2005", "shortCiteRegEx": "Liwicki and Bunke", "year": 2005}, {"title": "Large text compression benchmark", "author": ["M. Mahoney"], "venue": "URL: http://www. mattmahoney. net/text/text. html.", "citeRegEx": "Mahoney,? 2009", "shortCiteRegEx": "Mahoney", "year": 2009}, {"title": "Building a large annotated corpus of english: The penn treebank", "author": ["M.P. Marcus", "M.A. Marcinkiewicz", "B. Santorini"], "venue": "Computational linguistics, 19(2), 313\u2013330.", "citeRegEx": "Marcus et al\\.,? 1993", "shortCiteRegEx": "Marcus et al\\.", "year": 1993}, {"title": "Subword language modeling with neural networks", "author": ["T. Mikolov", "I. Sutskever", "A. Deoras", "Le", "H.-S.", "S. Kombrink", "J. Cernocky"], "venue": "Preprint.", "citeRegEx": "Mikolov et al\\.,? 2012", "shortCiteRegEx": "Mikolov et al\\.", "year": 2012}, {"title": "Neural variational inference and learning in belief networks", "author": ["A. Mnih", "K. Gregor"], "venue": "Proceedings of the 31st International Conference on Machine Learning (ICML-14), pages 1791\u20131799.", "citeRegEx": "Mnih and Gregor,? 2014", "shortCiteRegEx": "Mnih and Gregor", "year": 2014}, {"title": "Human-level control through deep reinforcement learning", "author": ["V. Mnih", "K. Kavukcuoglu", "D. Silver", "A.A. Rusu", "J. Veness", "M.G. Bellemare", "A. Graves", "M. Riedmiller", "A.K. Fidjeland", "G Ostrovski"], "venue": null, "citeRegEx": "Mnih et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Mnih et al\\.", "year": 2015}, {"title": "Strategic attentive writer for learning macro-actions", "author": ["V. Mnih", "J. Agapiou", "S. Osindero", "A. Graves", "O. Vinyals", "K Kavukcuoglu"], "venue": "arXiv preprint arXiv:1606.04695", "citeRegEx": "Mnih et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Mnih et al\\.", "year": 2016}, {"title": "Induction of multiscale temporal structure", "author": ["M.C. Mozer"], "venue": "Advances in neural information processing systems, pages 275\u2013275.", "citeRegEx": "Mozer,? 1993", "shortCiteRegEx": "Mozer", "year": 1993}, {"title": "Rectified linear units improve restricted boltzmann machines", "author": ["V. Nair", "G.E. Hinton"], "venue": "Proceedings of the 27th International Conference on Machine Learning (ICML-10), pages 807\u2013814.", "citeRegEx": "Nair and Hinton,? 2010", "shortCiteRegEx": "Nair and Hinton", "year": 2010}, {"title": "Regularization and nonlinearities for neural language models: when are they needed? arXiv preprint arXiv:1301.5650", "author": ["M. Pachitariu", "M. Sahani"], "venue": null, "citeRegEx": "Pachitariu and Sahani,? \\Q2013\\E", "shortCiteRegEx": "Pachitariu and Sahani", "year": 2013}, {"title": "On the difficulty of training recurrent neural networks", "author": ["R. Pascanu", "T. Mikolov", "Y. Bengio"], "venue": "arXiv preprint arXiv:1211.5063.", "citeRegEx": "Pascanu et al\\.,? 2012", "shortCiteRegEx": "Pascanu et al\\.", "year": 2012}, {"title": "Recurrent memory array structures", "author": ["K.M. Rocki"], "venue": "arXiv preprint arXiv:1607.03085.", "citeRegEx": "Rocki,? 2016a", "shortCiteRegEx": "Rocki", "year": 2016}, {"title": "Surprisal-driven feedback in recurrent networks", "author": ["K.M. Rocki"], "venue": "arXiv preprint arXiv:1608.06027.", "citeRegEx": "Rocki,? 2016b", "shortCiteRegEx": "Rocki", "year": 2016}, {"title": "One-shot learning with memory-augmented neural networks", "author": ["A. Santoro", "S. Bartunov", "M. Botvinick", "D. Wierstra", "T. Lillicrap"], "venue": "arXiv preprint arXiv:1605.06065.", "citeRegEx": "Santoro et al\\.,? 2016", "shortCiteRegEx": "Santoro et al\\.", "year": 2016}, {"title": "Neural sequence chunkers", "author": ["J. Schmidhuber"], "venue": null, "citeRegEx": "Schmidhuber,? \\Q1991\\E", "shortCiteRegEx": "Schmidhuber", "year": 1991}, {"title": "Learning complex, extended sequences using the principle of history compression", "author": ["J. Schmidhuber"], "venue": "Neural Computation, 4(2), 234\u2013242. Schmidhuber, J. (2015). Deep learning in neural networks: An overview. Neural Networks, 61, 85\u2013117.", "citeRegEx": "Schmidhuber,? 1992", "shortCiteRegEx": "Schmidhuber", "year": 1992}, {"title": "Mastering the game of go with deep neural networks and tree", "author": ["D. Silver", "A. Huang", "C.J. Maddison", "A. Guez", "L. Sifre", "G. Van Den Driessche", "J. Schrittwieser", "I. Antonoglou", "V. Panneershelvam", "M Lanctot"], "venue": "search. Nature,", "citeRegEx": "Silver et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Silver et al\\.", "year": 2016}, {"title": "A hierarchical recurrent encoder-decoder for generative context-aware query suggestion", "author": ["A. Sordoni", "Y. Bengio", "H. Vahabi", "C. Lioma", "J. Grue Simonsen", "Nie", "J.-Y."], "venue": "Proceedings of the 24th ACM International on Conference on Information and Knowledge Management, pages 553\u2013562. ACM.", "citeRegEx": "Sordoni et al\\.,? 2015", "shortCiteRegEx": "Sordoni et al\\.", "year": 2015}, {"title": "Dropout: a simple way to prevent neural networks from overfitting", "author": ["N. Srivastava", "G.E. Hinton", "A. Krizhevsky", "I. Sutskever", "R. Salakhutdinov"], "venue": "Journal of Machine Learning Research, 15(1), 1929\u20131958.", "citeRegEx": "Srivastava et al\\.,? 2014", "shortCiteRegEx": "Srivastava et al\\.", "year": 2014}, {"title": "Generating text with recurrent neural networks", "author": ["I. Sutskever", "J. Martens", "G.E. Hinton"], "venue": "Proceedings of the 28th International Conference on Machine Learning (ICML\u201911), pages 1017\u20131024.", "citeRegEx": "Sutskever et al\\.,? 2011", "shortCiteRegEx": "Sutskever et al\\.", "year": 2011}, {"title": "Sequence to sequence learning with neural networks", "author": ["I. Sutskever", "O. Vinyals", "Q.V. Le"], "venue": "Advances in Neural Information Processing Systems, pages 3104\u20133112.", "citeRegEx": "Sutskever et al\\.,? 2014", "shortCiteRegEx": "Sutskever et al\\.", "year": 2014}, {"title": "Theano: A python framework for fast computation of mathematical expressions. arXiv preprint arXiv:1605.02688", "author": ["T.T.D. Team", "R. Al-Rfou", "G. Alain", "A. Almahairi", "C. Angermueller", "D. Bahdanau", "N. Ballas", "F. Bastien", "J. Bayer", "A Belikov"], "venue": null, "citeRegEx": "Team et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Team et al\\.", "year": 2016}, {"title": "Show and tell: A neural image caption generator", "author": ["O. Vinyals", "A. Toshev", "S. Bengio", "D. Erhan"], "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 3156\u20133164.", "citeRegEx": "Vinyals et al\\.,? 2015", "shortCiteRegEx": "Vinyals et al\\.", "year": 2015}, {"title": "Simple statistical gradient-following algorithms for connectionist reinforcement learning", "author": ["R.J. Williams"], "venue": "Machine learning, 8(3-4), 229\u2013256.", "citeRegEx": "Williams,? 1992", "shortCiteRegEx": "Williams", "year": 1992}, {"title": "On multiplicative integration with recurrent neural networks", "author": ["Y. Wu", "S. Zhang", "Y. Zhang", "Y. Bengio", "R. Salakhutdinov"], "venue": "arXiv preprint arXiv:1606.06630.", "citeRegEx": "Wu et al\\.,? 2016", "shortCiteRegEx": "Wu et al\\.", "year": 2016}, {"title": "How transferable are features in deep neural networks", "author": ["J. Yosinski", "J. Clune", "Y. Bengio", "H. Lipson"], "venue": "In Advances in neural information processing systems,", "citeRegEx": "Yosinski et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Yosinski et al\\.", "year": 2014}, {"title": "Architectural complexity measures of recurrent neural networks", "author": ["S. Zhang", "Y. Wu", "T. Che", "Z. Lin", "R. Memisevic", "R. Salakhutdinov", "Y. Bengio"], "venue": "arXiv preprint arXiv:1602.08210.", "citeRegEx": "Zhang et al\\.,? 2016", "shortCiteRegEx": "Zhang et al\\.", "year": 2016}, {"title": "Recurrent highway networks", "author": ["J.G. Zilly", "R.K. Srivastava", "J. Koutn\u00edk", "J. Schmidhuber"], "venue": "arXiv preprint arXiv:1607.03474.", "citeRegEx": "Zilly et al\\.,? 2016", "shortCiteRegEx": "Zilly et al\\.", "year": 2016}], "referenceMentions": [{"referenceID": 2, "context": "One of the key principles of learning in deep neural networks as well as in the human brain (Bengio, 2009; LeCun et al., 2015; Schmidhuber, 2015) is to obtain a hierarchical representation with increasing levels of abstraction.", "startOffset": 92, "endOffset": 145}, {"referenceID": 32, "context": "One of the key principles of learning in deep neural networks as well as in the human brain (Bengio, 2009; LeCun et al., 2015; Schmidhuber, 2015) is to obtain a hierarchical representation with increasing levels of abstraction.", "startOffset": 92, "endOffset": 145}, {"referenceID": 18, "context": "A stack of representation layers, learned from the data in a way to optimize a supervised or unsupervised target task, make deep neural networks entertain advantages such as generalization to unseen examples (Hoffman et al., 2013; Santoro et al., 2016), sharing learned knowledge among multiple tasks (Yosinski et al.", "startOffset": 208, "endOffset": 252}, {"referenceID": 47, "context": "A stack of representation layers, learned from the data in a way to optimize a supervised or unsupervised target task, make deep neural networks entertain advantages such as generalization to unseen examples (Hoffman et al., 2013; Santoro et al., 2016), sharing learned knowledge among multiple tasks (Yosinski et al.", "startOffset": 208, "endOffset": 252}, {"referenceID": 59, "context": ", 2016), sharing learned knowledge among multiple tasks (Yosinski et al., 2014) and discovering disentangling factors of variation (Kingma and Welling, 2013).", "startOffset": 56, "endOffset": 79}, {"referenceID": 26, "context": ", 2014) and discovering disentangling factors of variation (Kingma and Welling, 2013).", "startOffset": 59, "endOffset": 85}, {"referenceID": 29, "context": "The remarkable recent successes of the deep convolutional neural networks are particularly based on this ability to learn hierarchical representation for spatial data (Krizhevsky et al., 2012; Mnih et al., 2015; Silver et al., 2016).", "startOffset": 167, "endOffset": 232}, {"referenceID": 39, "context": "The remarkable recent successes of the deep convolutional neural networks are particularly based on this ability to learn hierarchical representation for spatial data (Krizhevsky et al., 2012; Mnih et al., 2015; Silver et al., 2016).", "startOffset": 167, "endOffset": 232}, {"referenceID": 50, "context": "The remarkable recent successes of the deep convolutional neural networks are particularly based on this ability to learn hierarchical representation for spatial data (Krizhevsky et al., 2012; Mnih et al., 2015; Silver et al., 2016).", "startOffset": 167, "endOffset": 232}, {"referenceID": 6, "context": "For modelling temporal data, the recent resurgence of recurrent neural networks (RNN) has led to remarkable advances (Graves et al., 2008, 2013; Cho et al., 2014; Sutskever et al., 2014; Vinyals et al., 2015).", "startOffset": 117, "endOffset": 208}, {"referenceID": 54, "context": "For modelling temporal data, the recent resurgence of recurrent neural networks (RNN) has led to remarkable advances (Graves et al., 2008, 2013; Cho et al., 2014; Sutskever et al., 2014; Vinyals et al., 2015).", "startOffset": 117, "endOffset": 208}, {"referenceID": 56, "context": "For modelling temporal data, the recent resurgence of recurrent neural networks (RNN) has led to remarkable advances (Graves et al., 2008, 2013; Cho et al., 2014; Sutskever et al., 2014; Vinyals et al., 2015).", "startOffset": 117, "endOffset": 208}, {"referenceID": 48, "context": "However, unlike the spatial data, learning both hierarchical and temporal representation has been among the long-standing challenges of RNNs in spite of the fact that hierarchical multiscale structures naturally exist in many temporal data (Schmidhuber, 1991; Mozer, 1993; El Hihi and Bengio, 1995; Kleinberg, 2003; Koutn\u00edk et al., 2014; Chung et al., 2016).", "startOffset": 240, "endOffset": 357}, {"referenceID": 41, "context": "However, unlike the spatial data, learning both hierarchical and temporal representation has been among the long-standing challenges of RNNs in spite of the fact that hierarchical multiscale structures naturally exist in many temporal data (Schmidhuber, 1991; Mozer, 1993; El Hihi and Bengio, 1995; Kleinberg, 2003; Koutn\u00edk et al., 2014; Chung et al., 2016).", "startOffset": 240, "endOffset": 357}, {"referenceID": 27, "context": "However, unlike the spatial data, learning both hierarchical and temporal representation has been among the long-standing challenges of RNNs in spite of the fact that hierarchical multiscale structures naturally exist in many temporal data (Schmidhuber, 1991; Mozer, 1993; El Hihi and Bengio, 1995; Kleinberg, 2003; Koutn\u00edk et al., 2014; Chung et al., 2016).", "startOffset": 240, "endOffset": 357}, {"referenceID": 28, "context": "However, unlike the spatial data, learning both hierarchical and temporal representation has been among the long-standing challenges of RNNs in spite of the fact that hierarchical multiscale structures naturally exist in many temporal data (Schmidhuber, 1991; Mozer, 1993; El Hihi and Bengio, 1995; Kleinberg, 2003; Koutn\u00edk et al., 2014; Chung et al., 2016).", "startOffset": 240, "endOffset": 357}, {"referenceID": 9, "context": "However, unlike the spatial data, learning both hierarchical and temporal representation has been among the long-standing challenges of RNNs in spite of the fact that hierarchical multiscale structures naturally exist in many temporal data (Schmidhuber, 1991; Mozer, 1993; El Hihi and Bengio, 1995; Kleinberg, 2003; Koutn\u00edk et al., 2014; Chung et al., 2016).", "startOffset": 240, "endOffset": 357}, {"referenceID": 49, "context": "A promising approach to model such hierarchical and temporal representation is the multiscale RNNs (Schmidhuber, 1992; El Hihi and Bengio, 1995; Koutn\u00edk et al., 2014).", "startOffset": 99, "endOffset": 166}, {"referenceID": 28, "context": "A promising approach to model such hierarchical and temporal representation is the multiscale RNNs (Schmidhuber, 1992; El Hihi and Bengio, 1995; Koutn\u00edk et al., 2014).", "startOffset": 99, "endOffset": 166}, {"referenceID": 28, "context": "The most popular approach is to set the timescales as hyperparameters (El Hihi and Bengio, 1995; Koutn\u00edk et al., 2014; Bahdanau et al., 2016) instead of treating them as dynamic variables that can be learned from the data (Schmidhuber, 1991, 1992; Chung et al.", "startOffset": 70, "endOffset": 141}, {"referenceID": 1, "context": "The most popular approach is to set the timescales as hyperparameters (El Hihi and Bengio, 1995; Koutn\u00edk et al., 2014; Bahdanau et al., 2016) instead of treating them as dynamic variables that can be learned from the data (Schmidhuber, 1991, 1992; Chung et al.", "startOffset": 70, "endOffset": 141}, {"referenceID": 51, "context": "While this is trivial if the hierarchical boundary structure is provided (Sordoni et al., 2015), it has been a challenge for an RNN to discover the latent hierarchical structure in temporal data without explicit boundary information.", "startOffset": 73, "endOffset": 95}, {"referenceID": 17, "context": "The UPDATE operation is similar to the usual update rule of the long short-term memory (LSTM) (Hochreiter and Schmidhuber, 1997), except that it is executed sparsely according to the detected boundaries.", "startOffset": 94, "endOffset": 128}, {"referenceID": 6, "context": "Unlike the leaky integration of the LSTM or gated recurrent unit (GRU) (Cho et al., 2014), the COPY retains the whole states without any loss of information.", "startOffset": 71, "endOffset": 89}, {"referenceID": 3, "context": "We find that the straight-through estimator (Bengio et al., 2013; Courbariaux et al., 2015) is efficient for training this model containing discrete variables.", "startOffset": 44, "endOffset": 91}, {"referenceID": 11, "context": "We find that the straight-through estimator (Bengio et al., 2013; Courbariaux et al., 2015) is efficient for training this model containing discrete variables.", "startOffset": 44, "endOffset": 91}, {"referenceID": 17, "context": "The LSTM (Hochreiter and Schmidhuber, 1997) is probably the most popular RNN that employs the multiscale update concept.", "startOffset": 9, "endOffset": 43}, {"referenceID": 13, "context": "That is, the hidden units of the LSTM or stacked LSTM (Fern\u00e1ndez et al., 2007) have different forget and update rates and can operate with different timescales, however unlike to our model, these timescales are not organized hierarchically.", "startOffset": 54, "endOffset": 78}, {"referenceID": 28, "context": "A more recent model, the clockwork RNN (CW-RNN) (Koutn\u00edk et al., 2014) extends the hierarchical RNN (El Hihi and Bengio, 1995) and tries to solve the issue of using soft timescales in the LSTM, by explicitly assigning hard timescales.", "startOffset": 48, "endOffset": 70}, {"referenceID": 7, "context": "More recently, in the Gated-Feedback RNNs (GF-RNN) (Chung et al., 2015a) and the Biscale RNNs (BiS-RNN) (Chung et al.", "startOffset": 51, "endOffset": 72}, {"referenceID": 9, "context": ", 2015a) and the Biscale RNNs (BiS-RNN) (Chung et al., 2016), the authors proposed to model layer-wise timescales more explicitly by having additional gating units to stacked RNN architecture.", "startOffset": 40, "endOffset": 60}, {"referenceID": 31, "context": "The COPY operation used in our model can be related to Zoneout (Krueger et al., 2016) which is a recurrent generalization of stochastic depth (Huang et al.", "startOffset": 63, "endOffset": 85}, {"referenceID": 19, "context": ", 2016) which is a recurrent generalization of stochastic depth (Huang et al., 2016).", "startOffset": 64, "endOffset": 84}, {"referenceID": 33, "context": "Two notable early attempts inspiring our model are Schmidhuber (1992) and El Hihi and Bengio (1995).", "startOffset": 51, "endOffset": 70}, {"referenceID": 1, "context": "Two notable early attempts inspiring our model are Schmidhuber (1992) and El Hihi and Bengio (1995). In these works, it is advocated to stack multiple layers of RNNs in a decreasing order of update frequency for computational and learning efficiency.", "startOffset": 86, "endOffset": 100}, {"referenceID": 1, "context": "Two notable early attempts inspiring our model are Schmidhuber (1992) and El Hihi and Bengio (1995). In these works, it is advocated to stack multiple layers of RNNs in a decreasing order of update frequency for computational and learning efficiency. In Schmidhuber (1992), the author shows a model that can self-organize a hierarchical multiscale structure.", "startOffset": 86, "endOffset": 273}, {"referenceID": 1, "context": "Two notable early attempts inspiring our model are Schmidhuber (1992) and El Hihi and Bengio (1995). In these works, it is advocated to stack multiple layers of RNNs in a decreasing order of update frequency for computational and learning efficiency. In Schmidhuber (1992), the author shows a model that can self-organize a hierarchical multiscale structure. Particularly in El Hihi and Bengio (1995), the advantages of incorporating a priori knowledge, \u201ctemporal dependencies are structured hierarchically\", into the RNN architecture is studied.", "startOffset": 86, "endOffset": 401}, {"referenceID": 1, "context": "Two notable early attempts inspiring our model are Schmidhuber (1992) and El Hihi and Bengio (1995). In these works, it is advocated to stack multiple layers of RNNs in a decreasing order of update frequency for computational and learning efficiency. In Schmidhuber (1992), the author shows a model that can self-organize a hierarchical multiscale structure. Particularly in El Hihi and Bengio (1995), the advantages of incorporating a priori knowledge, \u201ctemporal dependencies are structured hierarchically\", into the RNN architecture is studied. The authors propose an RNN architecture that updates each layer with a fixed but different rate, called a hierarchical RNN. The LSTM (Hochreiter and Schmidhuber, 1997) is probably the most popular RNN that employs the multiscale update concept. That is, the hidden units of the LSTM or stacked LSTM (Fern\u00e1ndez et al., 2007) have different forget and update rates and can operate with different timescales, however unlike to our model, these timescales are not organized hierarchically. Although the update of the LSTM by leaky integration helps to reduce the difficulty of learning long-term dependency by mitigating the vanishing gradient problem, the model still remains computationally expensive because it has to perform the update at every time step for each unit. Also, in practice, the long-term dependency captured by the LSTM units are still limited to a few hundred time steps due to the leaky integration by which the contents to memorize for a long-term is gradually diluted at every time step. However, our model is less prone to this problem because it learns a hierarchical structure such that, by design, high-level layers perform less frequent updates than low-level layers. We hypothesize that this property mitigates the vanishing gradient problem more efficiently while also being computationally more efficient. A more recent model, the clockwork RNN (CW-RNN) (Koutn\u00edk et al., 2014) extends the hierarchical RNN (El Hihi and Bengio, 1995) and tries to solve the issue of using soft timescales in the LSTM, by explicitly assigning hard timescales. In the CW-RNN, hidden units are partitioned into several modules, and different timescales are assigned to the modules such that a module i updates its hidden units at every 2(i\u22121)-th time step. The CW-RNN is computationally more efficient than the standard RNN including the LSTM since hidden units are updated only at the assigned clock rates. However, finding proper timescales in CW-RNN remains as a challenge, unlike our model, which learns the intrinsic timescales from the data. More recently, in the Gated-Feedback RNNs (GF-RNN) (Chung et al., 2015a) and the Biscale RNNs (BiS-RNN) (Chung et al., 2016), the authors proposed to model layer-wise timescales more explicitly by having additional gating units to stacked RNN architecture. While the GF-RNN proposed a general framework where information can flow across multiple RNN layers via a gating mechanism, the BiS-RNN architecture is applied to character-level machine translation where two different timescales (e.g., character-level and word-level) seem to clearly exist in the data. However, similar to the LSTM, these two approaches rely on the soft gating mechanism, making the model suffer from the curse of update at every time step. Other forms of Hierarchical RNN (HRNN) architectures have been used in the case where the hierarchical boundary structure is explicitly observed in the data and easy to obtain. In Ling et al. (2015), the HRNN architecture is used for neural machine translation by modelling the characters and words using the first and second RNN layers, respectively, after obtaining the word boundary via tokenization.", "startOffset": 86, "endOffset": 3516}, {"referenceID": 1, "context": "Two notable early attempts inspiring our model are Schmidhuber (1992) and El Hihi and Bengio (1995). In these works, it is advocated to stack multiple layers of RNNs in a decreasing order of update frequency for computational and learning efficiency. In Schmidhuber (1992), the author shows a model that can self-organize a hierarchical multiscale structure. Particularly in El Hihi and Bengio (1995), the advantages of incorporating a priori knowledge, \u201ctemporal dependencies are structured hierarchically\", into the RNN architecture is studied. The authors propose an RNN architecture that updates each layer with a fixed but different rate, called a hierarchical RNN. The LSTM (Hochreiter and Schmidhuber, 1997) is probably the most popular RNN that employs the multiscale update concept. That is, the hidden units of the LSTM or stacked LSTM (Fern\u00e1ndez et al., 2007) have different forget and update rates and can operate with different timescales, however unlike to our model, these timescales are not organized hierarchically. Although the update of the LSTM by leaky integration helps to reduce the difficulty of learning long-term dependency by mitigating the vanishing gradient problem, the model still remains computationally expensive because it has to perform the update at every time step for each unit. Also, in practice, the long-term dependency captured by the LSTM units are still limited to a few hundred time steps due to the leaky integration by which the contents to memorize for a long-term is gradually diluted at every time step. However, our model is less prone to this problem because it learns a hierarchical structure such that, by design, high-level layers perform less frequent updates than low-level layers. We hypothesize that this property mitigates the vanishing gradient problem more efficiently while also being computationally more efficient. A more recent model, the clockwork RNN (CW-RNN) (Koutn\u00edk et al., 2014) extends the hierarchical RNN (El Hihi and Bengio, 1995) and tries to solve the issue of using soft timescales in the LSTM, by explicitly assigning hard timescales. In the CW-RNN, hidden units are partitioned into several modules, and different timescales are assigned to the modules such that a module i updates its hidden units at every 2(i\u22121)-th time step. The CW-RNN is computationally more efficient than the standard RNN including the LSTM since hidden units are updated only at the assigned clock rates. However, finding proper timescales in CW-RNN remains as a challenge, unlike our model, which learns the intrinsic timescales from the data. More recently, in the Gated-Feedback RNNs (GF-RNN) (Chung et al., 2015a) and the Biscale RNNs (BiS-RNN) (Chung et al., 2016), the authors proposed to model layer-wise timescales more explicitly by having additional gating units to stacked RNN architecture. While the GF-RNN proposed a general framework where information can flow across multiple RNN layers via a gating mechanism, the BiS-RNN architecture is applied to character-level machine translation where two different timescales (e.g., character-level and word-level) seem to clearly exist in the data. However, similar to the LSTM, these two approaches rely on the soft gating mechanism, making the model suffer from the curse of update at every time step. Other forms of Hierarchical RNN (HRNN) architectures have been used in the case where the hierarchical boundary structure is explicitly observed in the data and easy to obtain. In Ling et al. (2015), the HRNN architecture is used for neural machine translation by modelling the characters and words using the first and second RNN layers, respectively, after obtaining the word boundary via tokenization. A similar HRNN architecture is also adopted in Sordoni et al. (2015) to model dialogue utterances.", "startOffset": 86, "endOffset": 3790}, {"referenceID": 1, "context": "Two notable early attempts inspiring our model are Schmidhuber (1992) and El Hihi and Bengio (1995). In these works, it is advocated to stack multiple layers of RNNs in a decreasing order of update frequency for computational and learning efficiency. In Schmidhuber (1992), the author shows a model that can self-organize a hierarchical multiscale structure. Particularly in El Hihi and Bengio (1995), the advantages of incorporating a priori knowledge, \u201ctemporal dependencies are structured hierarchically\", into the RNN architecture is studied. The authors propose an RNN architecture that updates each layer with a fixed but different rate, called a hierarchical RNN. The LSTM (Hochreiter and Schmidhuber, 1997) is probably the most popular RNN that employs the multiscale update concept. That is, the hidden units of the LSTM or stacked LSTM (Fern\u00e1ndez et al., 2007) have different forget and update rates and can operate with different timescales, however unlike to our model, these timescales are not organized hierarchically. Although the update of the LSTM by leaky integration helps to reduce the difficulty of learning long-term dependency by mitigating the vanishing gradient problem, the model still remains computationally expensive because it has to perform the update at every time step for each unit. Also, in practice, the long-term dependency captured by the LSTM units are still limited to a few hundred time steps due to the leaky integration by which the contents to memorize for a long-term is gradually diluted at every time step. However, our model is less prone to this problem because it learns a hierarchical structure such that, by design, high-level layers perform less frequent updates than low-level layers. We hypothesize that this property mitigates the vanishing gradient problem more efficiently while also being computationally more efficient. A more recent model, the clockwork RNN (CW-RNN) (Koutn\u00edk et al., 2014) extends the hierarchical RNN (El Hihi and Bengio, 1995) and tries to solve the issue of using soft timescales in the LSTM, by explicitly assigning hard timescales. In the CW-RNN, hidden units are partitioned into several modules, and different timescales are assigned to the modules such that a module i updates its hidden units at every 2(i\u22121)-th time step. The CW-RNN is computationally more efficient than the standard RNN including the LSTM since hidden units are updated only at the assigned clock rates. However, finding proper timescales in CW-RNN remains as a challenge, unlike our model, which learns the intrinsic timescales from the data. More recently, in the Gated-Feedback RNNs (GF-RNN) (Chung et al., 2015a) and the Biscale RNNs (BiS-RNN) (Chung et al., 2016), the authors proposed to model layer-wise timescales more explicitly by having additional gating units to stacked RNN architecture. While the GF-RNN proposed a general framework where information can flow across multiple RNN layers via a gating mechanism, the BiS-RNN architecture is applied to character-level machine translation where two different timescales (e.g., character-level and word-level) seem to clearly exist in the data. However, similar to the LSTM, these two approaches rely on the soft gating mechanism, making the model suffer from the curse of update at every time step. Other forms of Hierarchical RNN (HRNN) architectures have been used in the case where the hierarchical boundary structure is explicitly observed in the data and easy to obtain. In Ling et al. (2015), the HRNN architecture is used for neural machine translation by modelling the characters and words using the first and second RNN layers, respectively, after obtaining the word boundary via tokenization. A similar HRNN architecture is also adopted in Sordoni et al. (2015) to model dialogue utterances. However, in many cases, hierarchical boundary information is not explicitly observed or expensive to obtain. Also, it is unclear how to deploy more layers than the number of boundary levels that is explicitly observed in the data. While the above models focus on the online prediction problem, where a prediction needs to be made by using only the past data, in some cases, predictions are made after observing the whole sequence. In this setting, the input sequence can be regarded as 1-D spatial data, using convolutional neural networks with 1-D kernels as proposed in Kim (2014) and Kim et al.", "startOffset": 86, "endOffset": 4403}, {"referenceID": 1, "context": "Two notable early attempts inspiring our model are Schmidhuber (1992) and El Hihi and Bengio (1995). In these works, it is advocated to stack multiple layers of RNNs in a decreasing order of update frequency for computational and learning efficiency. In Schmidhuber (1992), the author shows a model that can self-organize a hierarchical multiscale structure. Particularly in El Hihi and Bengio (1995), the advantages of incorporating a priori knowledge, \u201ctemporal dependencies are structured hierarchically\", into the RNN architecture is studied. The authors propose an RNN architecture that updates each layer with a fixed but different rate, called a hierarchical RNN. The LSTM (Hochreiter and Schmidhuber, 1997) is probably the most popular RNN that employs the multiscale update concept. That is, the hidden units of the LSTM or stacked LSTM (Fern\u00e1ndez et al., 2007) have different forget and update rates and can operate with different timescales, however unlike to our model, these timescales are not organized hierarchically. Although the update of the LSTM by leaky integration helps to reduce the difficulty of learning long-term dependency by mitigating the vanishing gradient problem, the model still remains computationally expensive because it has to perform the update at every time step for each unit. Also, in practice, the long-term dependency captured by the LSTM units are still limited to a few hundred time steps due to the leaky integration by which the contents to memorize for a long-term is gradually diluted at every time step. However, our model is less prone to this problem because it learns a hierarchical structure such that, by design, high-level layers perform less frequent updates than low-level layers. We hypothesize that this property mitigates the vanishing gradient problem more efficiently while also being computationally more efficient. A more recent model, the clockwork RNN (CW-RNN) (Koutn\u00edk et al., 2014) extends the hierarchical RNN (El Hihi and Bengio, 1995) and tries to solve the issue of using soft timescales in the LSTM, by explicitly assigning hard timescales. In the CW-RNN, hidden units are partitioned into several modules, and different timescales are assigned to the modules such that a module i updates its hidden units at every 2(i\u22121)-th time step. The CW-RNN is computationally more efficient than the standard RNN including the LSTM since hidden units are updated only at the assigned clock rates. However, finding proper timescales in CW-RNN remains as a challenge, unlike our model, which learns the intrinsic timescales from the data. More recently, in the Gated-Feedback RNNs (GF-RNN) (Chung et al., 2015a) and the Biscale RNNs (BiS-RNN) (Chung et al., 2016), the authors proposed to model layer-wise timescales more explicitly by having additional gating units to stacked RNN architecture. While the GF-RNN proposed a general framework where information can flow across multiple RNN layers via a gating mechanism, the BiS-RNN architecture is applied to character-level machine translation where two different timescales (e.g., character-level and word-level) seem to clearly exist in the data. However, similar to the LSTM, these two approaches rely on the soft gating mechanism, making the model suffer from the curse of update at every time step. Other forms of Hierarchical RNN (HRNN) architectures have been used in the case where the hierarchical boundary structure is explicitly observed in the data and easy to obtain. In Ling et al. (2015), the HRNN architecture is used for neural machine translation by modelling the characters and words using the first and second RNN layers, respectively, after obtaining the word boundary via tokenization. A similar HRNN architecture is also adopted in Sordoni et al. (2015) to model dialogue utterances. However, in many cases, hierarchical boundary information is not explicitly observed or expensive to obtain. Also, it is unclear how to deploy more layers than the number of boundary levels that is explicitly observed in the data. While the above models focus on the online prediction problem, where a prediction needs to be made by using only the past data, in some cases, predictions are made after observing the whole sequence. In this setting, the input sequence can be regarded as 1-D spatial data, using convolutional neural networks with 1-D kernels as proposed in Kim (2014) and Kim et al. (2015) for language modelling and sentence classification.", "startOffset": 86, "endOffset": 4425}, {"referenceID": 1, "context": "Two notable early attempts inspiring our model are Schmidhuber (1992) and El Hihi and Bengio (1995). In these works, it is advocated to stack multiple layers of RNNs in a decreasing order of update frequency for computational and learning efficiency. In Schmidhuber (1992), the author shows a model that can self-organize a hierarchical multiscale structure. Particularly in El Hihi and Bengio (1995), the advantages of incorporating a priori knowledge, \u201ctemporal dependencies are structured hierarchically\", into the RNN architecture is studied. The authors propose an RNN architecture that updates each layer with a fixed but different rate, called a hierarchical RNN. The LSTM (Hochreiter and Schmidhuber, 1997) is probably the most popular RNN that employs the multiscale update concept. That is, the hidden units of the LSTM or stacked LSTM (Fern\u00e1ndez et al., 2007) have different forget and update rates and can operate with different timescales, however unlike to our model, these timescales are not organized hierarchically. Although the update of the LSTM by leaky integration helps to reduce the difficulty of learning long-term dependency by mitigating the vanishing gradient problem, the model still remains computationally expensive because it has to perform the update at every time step for each unit. Also, in practice, the long-term dependency captured by the LSTM units are still limited to a few hundred time steps due to the leaky integration by which the contents to memorize for a long-term is gradually diluted at every time step. However, our model is less prone to this problem because it learns a hierarchical structure such that, by design, high-level layers perform less frequent updates than low-level layers. We hypothesize that this property mitigates the vanishing gradient problem more efficiently while also being computationally more efficient. A more recent model, the clockwork RNN (CW-RNN) (Koutn\u00edk et al., 2014) extends the hierarchical RNN (El Hihi and Bengio, 1995) and tries to solve the issue of using soft timescales in the LSTM, by explicitly assigning hard timescales. In the CW-RNN, hidden units are partitioned into several modules, and different timescales are assigned to the modules such that a module i updates its hidden units at every 2(i\u22121)-th time step. The CW-RNN is computationally more efficient than the standard RNN including the LSTM since hidden units are updated only at the assigned clock rates. However, finding proper timescales in CW-RNN remains as a challenge, unlike our model, which learns the intrinsic timescales from the data. More recently, in the Gated-Feedback RNNs (GF-RNN) (Chung et al., 2015a) and the Biscale RNNs (BiS-RNN) (Chung et al., 2016), the authors proposed to model layer-wise timescales more explicitly by having additional gating units to stacked RNN architecture. While the GF-RNN proposed a general framework where information can flow across multiple RNN layers via a gating mechanism, the BiS-RNN architecture is applied to character-level machine translation where two different timescales (e.g., character-level and word-level) seem to clearly exist in the data. However, similar to the LSTM, these two approaches rely on the soft gating mechanism, making the model suffer from the curse of update at every time step. Other forms of Hierarchical RNN (HRNN) architectures have been used in the case where the hierarchical boundary structure is explicitly observed in the data and easy to obtain. In Ling et al. (2015), the HRNN architecture is used for neural machine translation by modelling the characters and words using the first and second RNN layers, respectively, after obtaining the word boundary via tokenization. A similar HRNN architecture is also adopted in Sordoni et al. (2015) to model dialogue utterances. However, in many cases, hierarchical boundary information is not explicitly observed or expensive to obtain. Also, it is unclear how to deploy more layers than the number of boundary levels that is explicitly observed in the data. While the above models focus on the online prediction problem, where a prediction needs to be made by using only the past data, in some cases, predictions are made after observing the whole sequence. In this setting, the input sequence can be regarded as 1-D spatial data, using convolutional neural networks with 1-D kernels as proposed in Kim (2014) and Kim et al. (2015) for language modelling and sentence classification. Also, in Chan et al. (2016) and Bahdanau et al.", "startOffset": 86, "endOffset": 4505}, {"referenceID": 1, "context": "(2016) and Bahdanau et al. (2016), the authors proposed to obtain high-level representation of the sequences of reduced length by repeatedly merging or pooling the lower level representation of the sequences.", "startOffset": 11, "endOffset": 34}, {"referenceID": 52, "context": "While the focus of Zoneout is to propose a regularization technique similar to dropout (Srivastava et al., 2014) (where the regularization strength is controlled by a hyperparameter), our model learns (a) to dynamically determine when to copy from the context inputs and (b) to discover the latent hierarchical multiscale structure and representation.", "startOffset": 87, "endOffset": 112}, {"referenceID": 51, "context": ", spaces to separate words and periods to separate sentences (Sordoni et al., 2015; Ling et al., 2015).", "startOffset": 61, "endOffset": 102}, {"referenceID": 33, "context": ", spaces to separate words and periods to separate sentences (Sordoni et al., 2015; Ling et al., 2015).", "startOffset": 61, "endOffset": 102}, {"referenceID": 28, "context": "In Section 2, we discussed the limitations of the existing RNN models under this setting, which either have to update all units at every time step or use fixed update frequencies (El Hihi and Bengio, 1995; Koutn\u00edk et al., 2014; Bahdanau et al., 2016).", "startOffset": 179, "endOffset": 250}, {"referenceID": 1, "context": "In Section 2, we discussed the limitations of the existing RNN models under this setting, which either have to update all units at every time step or use fixed update frequencies (El Hihi and Bengio, 1995; Koutn\u00edk et al., 2014; Bahdanau et al., 2016).", "startOffset": 179, "endOffset": 250}, {"referenceID": 57, "context": "Among a few methods for training a neural network with discrete variables such as the REINFORCE (Williams, 1992; Mnih and Gregor, 2014) and the straight-through estimator (Bengio et al.", "startOffset": 96, "endOffset": 135}, {"referenceID": 38, "context": "Among a few methods for training a neural network with discrete variables such as the REINFORCE (Williams, 1992; Mnih and Gregor, 2014) and the straight-through estimator (Bengio et al.", "startOffset": 96, "endOffset": 135}, {"referenceID": 3, "context": "Among a few methods for training a neural network with discrete variables such as the REINFORCE (Williams, 1992; Mnih and Gregor, 2014) and the straight-through estimator (Bengio et al., 2013), we use the straight-through estimator to train our model.", "startOffset": 171, "endOffset": 192}, {"referenceID": 2, "context": "Among a few methods for training a neural network with discrete variables such as the REINFORCE (Williams, 1992; Mnih and Gregor, 2014) and the straight-through estimator (Bengio et al., 2013), we use the straight-through estimator to train our model. The straight-through estimator is a biased estimator because the non-differentiable function used in the forward pass (the step function in our case) is replaced by a differentiable function during the backward pass (the hard sigmoid function in our case). The straight-through estimator, however, is much simpler and often works more efficiently in practice than other unbiased but high-variance estimators such as the REINFORCE. The straight-through estimator has also been used in Courbariaux et al. (2015) and Mnih et al.", "startOffset": 172, "endOffset": 762}, {"referenceID": 2, "context": "Among a few methods for training a neural network with discrete variables such as the REINFORCE (Williams, 1992; Mnih and Gregor, 2014) and the straight-through estimator (Bengio et al., 2013), we use the straight-through estimator to train our model. The straight-through estimator is a biased estimator because the non-differentiable function used in the forward pass (the step function in our case) is replaced by a differentiable function during the backward pass (the hard sigmoid function in our case). The straight-through estimator, however, is much simpler and often works more efficiently in practice than other unbiased but high-variance estimators such as the REINFORCE. The straight-through estimator has also been used in Courbariaux et al. (2015) and Mnih et al. (2016). The Slope Annealing Trick.", "startOffset": 172, "endOffset": 785}, {"referenceID": 30, "context": "Penn Treebank Model BPC Norm-stabilized RNN (Krueger and Memisevic, 2015) 1.", "startOffset": 44, "endOffset": 73}, {"referenceID": 28, "context": "48 Clockwork RNN (Koutn\u00edk et al., 2014) 1.", "startOffset": 17, "endOffset": 39}, {"referenceID": 37, "context": "46 HF-MRNN (Mikolov et al., 2012) 1.", "startOffset": 11, "endOffset": 33}, {"referenceID": 58, "context": "41 MI-RNN (Wu et al., 2016) 1.", "startOffset": 10, "endOffset": 27}, {"referenceID": 37, "context": "39 ME n-gram (Mikolov et al., 2012) 1.", "startOffset": 13, "endOffset": 35}, {"referenceID": 10, "context": "37 Batch-normalized LSTM (Cooijmans et al., 2016) 1.", "startOffset": 25, "endOffset": 49}, {"referenceID": 31, "context": "32 Zoneout RNN (Krueger et al., 2016) 1.", "startOffset": 15, "endOffset": 37}, {"referenceID": 60, "context": "Text8 Model BPC td-LSTM (Zhang et al., 2016) 1.", "startOffset": 24, "endOffset": 44}, {"referenceID": 37, "context": "63 HF-MRNN (Mikolov et al., 2012) 1.", "startOffset": 11, "endOffset": 33}, {"referenceID": 58, "context": "54 MI-RNN (Wu et al., 2016) 1.", "startOffset": 10, "endOffset": 27}, {"referenceID": 43, "context": "52 Skipping-RNN (Pachitariu and Sahani, 2013) 1.", "startOffset": 16, "endOffset": 45}, {"referenceID": 58, "context": "48 MI-LSTM (Wu et al., 2016) 1.", "startOffset": 11, "endOffset": 28}, {"referenceID": 10, "context": "44 Batch-normalized LSTM (Cooijmans et al., 2016) 1.", "startOffset": 25, "endOffset": 49}, {"referenceID": 46, "context": "Hutter Prize Wikipedia Model BPC SF-LSTM (Rocki, 2016b)\u2217 1.", "startOffset": 41, "endOffset": 55}, {"referenceID": 14, "context": "39 Stacked LSTM (Graves, 2013) 1.", "startOffset": 16, "endOffset": 30}, {"referenceID": 53, "context": "67 MRNN (Sutskever et al., 2011) 1.", "startOffset": 8, "endOffset": 32}, {"referenceID": 7, "context": "60 GF-LSTM (Chung et al., 2015a) 1.", "startOffset": 11, "endOffset": 32}, {"referenceID": 22, "context": "58 Grid-LSTM (Kalchbrenner et al., 2015) 1.", "startOffset": 13, "endOffset": 40}, {"referenceID": 0, "context": "47 Layer-normalized LSTM (Ba et al., 2016)\u2020 1.", "startOffset": 25, "endOffset": 42}, {"referenceID": 58, "context": "46 MI-LSTM (Wu et al., 2016) 1.", "startOffset": 11, "endOffset": 28}, {"referenceID": 61, "context": "44 Recurrent Highway Networks (Zilly et al., 2016) 1.", "startOffset": 30, "endOffset": 50}, {"referenceID": 45, "context": "42 Recurrent Memory Array Structures (Rocki, 2016a) 1.", "startOffset": 37, "endOffset": 51}, {"referenceID": 0, "context": "(\u2020) This model is implemented by the authors as the standard LSTM architecture using layer normalization (Ba et al., 2016).", "startOffset": 105, "endOffset": 122}, {"referenceID": 42, "context": "where L = 3 and ReLU(x) = max(0, x) (Nair and Hinton, 2010).", "startOffset": 36, "endOffset": 59}, {"referenceID": 36, "context": "Penn Treebank We process the Penn Treebank dataset (Marcus et al., 1993) by following the procedure introduced in Mikolov et al.", "startOffset": 51, "endOffset": 72}, {"referenceID": 25, "context": "We train the model using Adam (Kingma and Ba, 2014) with an initial learning rate of 0.", "startOffset": 30, "endOffset": 51}, {"referenceID": 44, "context": "The norm of the gradient is clipped with a threshold of 1 (Pascanu et al., 2012).", "startOffset": 58, "endOffset": 80}, {"referenceID": 21, "context": "We apply batch normalization (Ioffe and Szegedy, 2015; Cooijmans et al., 2016).", "startOffset": 29, "endOffset": 78}, {"referenceID": 10, "context": "We apply batch normalization (Ioffe and Szegedy, 2015; Cooijmans et al., 2016).", "startOffset": 29, "endOffset": 78}, {"referenceID": 32, "context": "Penn Treebank We process the Penn Treebank dataset (Marcus et al., 1993) by following the procedure introduced in Mikolov et al. (2012). Each update is done by using a mini-batch of 64 examples of length 100 to prevent the memory overflow problem when unfolding the RNN in time for backpropagation.", "startOffset": 52, "endOffset": 136}, {"referenceID": 35, "context": "Text8 The Text8 dataset (Mahoney, 2009) consists of 100M characters extracted from the Wikipedia corpus.", "startOffset": 24, "endOffset": 39}, {"referenceID": 36, "context": "to compare with other previous works, we follow the data splits used in Mikolov et al. (2012); Cooijmans et al.", "startOffset": 72, "endOffset": 94}, {"referenceID": 10, "context": "(2012); Cooijmans et al. (2016); Wu et al.", "startOffset": 8, "endOffset": 32}, {"referenceID": 10, "context": "(2012); Cooijmans et al. (2016); Wu et al. (2016). We use 1024 units for each HM-LSTM layer and 2048 units for the embedding layer of the output module.", "startOffset": 8, "endOffset": 50}, {"referenceID": 20, "context": "Hutter Prize Wikipedia The third dataset for language modelling is the Hutter Prize Wikipedia (also known as enwik8) dataset (Hutter, 2012).", "startOffset": 125, "endOffset": 139}, {"referenceID": 0, "context": "We use the same model size as in the Text8 experiments but apply layer normalization (Ba et al., 2016) to the model.", "startOffset": 85, "endOffset": 102}, {"referenceID": 13, "context": "(2012) and Graves (2013) where the first 90M characters are used to train the model, the next 5M characters for validation, and the remainders for the test set.", "startOffset": 11, "endOffset": 25}, {"referenceID": 34, "context": "We extend the evaluation of the HM-LSTM to a real-valued sequence modelling task using IAMOnDB (Liwicki and Bunke, 2005) dataset.", "startOffset": 95, "endOffset": 120}, {"referenceID": 14, "context": "Note that this process makes our numbers in Table 4 to become not directly comparable to the previous works (Graves, 2013; Chung et al., 2015b).", "startOffset": 108, "endOffset": 143}, {"referenceID": 8, "context": "Note that this process makes our numbers in Table 4 to become not directly comparable to the previous works (Graves, 2013; Chung et al., 2015b).", "startOffset": 108, "endOffset": 143}, {"referenceID": 4, "context": "We use the mixture density network (Bishop, 1994) as the output layer following Graves (2013), and use 400 units for each HM-LSTM layer and 800 units for the embedding layer of the output module.", "startOffset": 35, "endOffset": 49}, {"referenceID": 4, "context": "We use the mixture density network (Bishop, 1994) as the output layer following Graves (2013), and use 400 units for each HM-LSTM layer and 800 units for the embedding layer of the output module.", "startOffset": 36, "endOffset": 94}, {"referenceID": 55, "context": "The authors thank the developers of Theano (Team et al., 2016).", "startOffset": 43, "endOffset": 62}], "year": 2016, "abstractText": "Learning both hierarchical and temporal representation has been among the longstanding challenges of recurrent neural networks. Multiscale recurrent neural networks have been considered as a promising approach to resolve this issue, yet there has been a lack of empirical evidence showing that this type of models can actually capture the temporal dependencies by discovering the latent hierarchical structure of the sequence. In this paper, we propose a novel multiscale approach, called the hierarchical multiscale recurrent neural network, which can capture the latent hierarchical structure in the sequence by encoding the temporal dependencies with different timescales using a novel update mechanism. We show some evidence that our proposed multiscale architecture can discover underlying hierarchical structure in the sequences without using explicit boundary information. We evaluate our proposed model on character-level language modelling and handwriting sequence generation.", "creator": "LaTeX with hyperref package"}}}