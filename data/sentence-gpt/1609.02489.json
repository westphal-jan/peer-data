{"id": "1609.02489", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "8-Sep-2016", "title": "Fashion DNA: Merging Content and Sales Data for Recommendation and Article Mapping", "abstract": "We present a method to determine Fashion DNA, coordinate vectors locating fashion items in an abstract space. Our approach is based on a deep neural network architecture that ingests curated article information such as tags and images, and is trained to predict sales for a large set of frequent customers. In the process, a dual space of customer style preferences naturally arises where the user wants to find fashion items that appear in a larger number of searches, then an even more difficult task is to discover which items to pick up and search.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n", "histories": [["v1", "Thu, 8 Sep 2016 16:48:20 GMT  (7601kb,D)", "http://arxiv.org/abs/1609.02489v1", "10 pages, 13 figures. Paper presented at the workshop \"Machine Learning Meets Fashion,\" KDD 2016 Conference, San Francisco, USA, March 14, 2016"]], "COMMENTS": "10 pages, 13 figures. Paper presented at the workshop \"Machine Learning Meets Fashion,\" KDD 2016 Conference, San Francisco, USA, March 14, 2016", "reviews": [], "SUBJECTS": "cs.IR cs.LG", "authors": ["christian bracher", "sebastian heinz", "roland vollgraf"], "accepted": false, "id": "1609.02489"}, "pdf": {"name": "1609.02489.pdf", "metadata": {"source": "CRF", "title": "Fashion DNA: Merging Content and Sales Data for Recommendation and Article Mapping", "authors": ["Christian Bracher", "Sebastian Heinz", "Roland Vollgraf"], "emails": ["roland.vollgraf}@zalando.de"], "sections": [{"heading": null, "text": "CCS Concepts \u2022Information systems\u2192Recommender systems; Content analysis and feature selection; \u2022Human-centered computing \u2192 Collaborative filtering; \u2022Computing methodologies \u2192 Neural networks;\nKeywords Fashion data, neural networks, recommendations"}, {"heading": "1. INTRODUCTION", "text": "Compared to other commodities, fashion articles are tough to characterize: They are extremely varied in function and\n\u2217postal address of all authors: Zalando SE, Charlottenstr. 4, 10969 Berlin, Germany\nPermission to make digital or hard copies of part or all of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for third-party components of this work must be honored. For all other uses, contact the owner/author(s). KDD 2016 Fashion Workshop, August 14, 2016, San Francisco, CA, USA c\u00a9 2016 Copyright held by the owner/author(s).\nACM ISBN 978-1-4503-2138-9.\nDOI: 10.1145/1235\nappearance and lack standardization, and are described by a virtually open-ended set of properties and attributes, including labels such as brand (\u201cLevi\u2019s\u201d) and silhouette (\u201cjeans\u201d), physical properties like shape (\u201cslim cut\u201d), color (\u201cblue\u201d), and material (\u201cstonewashed cotton denim\u201d), target groups (\u201cadult,\u201d \u201cmale\u201d), price, imagery (photos, videos), and customer and expert sentiments. Many of these attributes are subjective or evolve over time; others are highly variable among customers, such as fit and style.\nZalando is Europe\u2019s leading online fashion platform, operating in 15 countries, with a base of > 107 customers and a catalog comprising > 106 articles (SKUs), with > 105 SKUs available for sale online at any given moment. Given the incomprehensibly large catalog and the heterogeneity of the customer base, matching shoppers and articles requires an automated pre-selection process that however has to be personalized to conform to the customers\u2019 individual styles and preferences: The degree to which items are similar or complement each other is largely in the eye of the beholder. At the same time, proper planning at a complex organization like Zalando requires an \u201cobjective\u201d description of each fashion item \u2013 one that is traditionally given by a curated set of labels like the above-mentioned. Still, these \u201cexpert labels\u201d are often ambiguous, cannot consider the variety of opinion found among shoppers, and often do not even match the consensus amongst customers.\nWe set ourselves the task to find a mathematical representation of fashion items that permits a notion of similarity and distance based on the expert labels, as well as visual and other information, but that is designed to maximize sales by enabling tailored recommendations to individual clients. Our vehicle is a deep feedforward neural network that is trained to forecast the individual purchases of a sizable number (104 . . . 105) of frequent Zalando customers, currently using expert labels and catalog images for \u223c 106 past and active Zalando articles as input. For each SKU we compute the activation of the topmost hidden layer in the network which we call its Fashion DNA (fDNA). The idiosyncratic style and taste of our shoppers is then simultaneously encoded as a vector of equal dimension in the weight matrix of the output layer. Inner products between vectors in these two dual spaces yield a likelihood of purchase and also express the similarity of fashion articles. Section 2 explains the mathematical model and presents the architecture of our network.\nCollaborative-filtering based recommender systems [3, 1] suffer from the \u201ccold-start problem\u201d [10]: Individual predictions for new customers and articles cannot be calculated in\nar X\niv :1\n60 9.\n02 48\n9v 1\n[ cs\n.I R\n] 8\nS ep\n2 01\n6\nthe absence of prior purchase data. Training of our network relies on sales records and content-based information on the article side, while only purchase data is used for customers. Once trained, the model computes fDNA vectors based on content article data alone, thus alleviating the \u201ccold-start problem.\u201d Our experiments show that the quality of such recommendations is only moderately affected by the lack of sales data. Likewise, it is easy to establish style vectors for unseen customers (by standard logistic regression), and their forecast performance is almost indistinguishable from training set customers. The generalization performance of our Fashion DNA model is demonstrated in Section 3.\nAs Fashion DNA combines item properties and customer sales behavior, distances in the fDNA space provide a natural measure of article similarity. In Section 4, we examine the traits shared by neighboring SKUs, and use dimensionality reduction to unveil a instructive, surprisingly intricate landscape of the fashion universe."}, {"heading": "2. THE FASHION DNA MODEL", "text": "We now proceed to describe the design of the network that yields the Fashion DNA. It ingests images and/or attribute information about articles, and tries to predict their likelihood of purchase for a group of frequent customers. After training, item fDNA is extracted from the internal activation of the network."}, {"heading": "2.1 Input data", "text": "For our experiments, we selected a \u201cstandard\u201d SKU set of 1.33 million items that were offered for sale at Zalando in the range 2011\u20132016, mostly clothing, shoes, and accessories, but also cosmetics and housewares. For each of these articles, one JPEG image of size 177 \u00d7 256 was available (some article images had to be resized to fit this format). Most of these images depict the article on a neutral white background; a few were photographed worn by a model.\nWe furthermore used up to six \u201cexpert labels\u201d to describe each item. These tags were selected to represent distinct qualities of the article under consideration: Besides the brand and main color, they comprise the commodity group (encoding silhouette, gender, age, and function), pattern, and labels for the retail price and fabrics composition. Price labels were created using k\u2013means clustering [4] of the logarithm of manufacturer suggested prices (MSRPs). For the fabrics composition labels, a linear space spanned by possible combinations of about 40 different fibers (e. g., cotton and wool) was segmented into 80 clusters. Labels were issued only where applicable; for instance, fabric clusters are restricted to textile items. Generally, only labels with a minimum number of 50 SKUs were retained. The number of different classes per label, and the percentage of articles tagged, are shown in Table 1. Label information was supplied to the network in the form of one-hot encoded vectors of combined length 3,855. Missing tags were replaced by zero vectors.\nFor validation purposes, the item set was randomly split 9:1 into a training subset It of Nt \u223c 1.2m and a validation subset Iv of Nv \u223c 130k articles."}, {"heading": "2.2 Purchase matrix", "text": "The Fashion DNA model is trained and evaluated on purchase information which we supply in the form of a sparse boolean matrix \u03a0, indicating the articles that have been\nbought by a group of 30,000 individuals that are among Zalando\u2019s top customers. Their likelihood of purchase, averaged over all customer\u2013SKU combinations, was pavg = 1.14 \u00b7 10\u22124, amounting to 150 orders per customer in the standard SKU set. We task the network to forecast these purchases by assigning a likelihood for a \u201cmatch\u201d between article and customer.\nAs we are interested in the generalization of these predictions to the many Zalando customers not included in the selection, we likewise split the base of 30,000 customers 9:1 into a training set Jt of Kt = 27, 000 customers and a validation set Jv of Kv = 3, 000 customers, where care has been taken that the purchase frequency distributions in the two sets are aligned.\nHence, the purchase matrix is made of 4 parts, compare Figure 1:\ntraining data \u03a0tt = ( \u03a0i,j ) i\u2208It,j\u2208Jt ,\nSKU validation data \u03a0vt = ( \u03a0i,j ) i\u2208Iv,j\u2208Jt ,\ncustomer validation data \u03a0tv = ( \u03a0i,j ) i\u2208It,j\u2208Jv ,\nSKU-customer validation data \u03a0vv = ( \u03a0i,j ) i\u2208Iv,j\u2208Jv .\nWe are going to deal with each of these parts separately."}, {"heading": "2.3 Mathematical model", "text": "A possible strategy to solve a recommender problem is by logistic factorization [7], [8] of the purchase matrix \u03a0. Following that strategy, the probability pij is defined by\npij = P (\u03a0ij = 1) = \u03c3 (fi\u00b7wj + bj) , (1)\nwhere \u03c3(x) = [1+exp(\u2212x)]\u22121 is the logistic function. Moreover, fi is a factor associated with SKU i, wj a factor and bj a scalar associated with customer j. During training, when\nwe are dealing with the segment \u03a0tt of the purchase matrix, the quantities fi, wj and bj are adjusted such that the (mean) cross entropy loss\nLtt = \u2212 1\nNtKt \u2211 i\u2208It \u2211 j\u2208Jt [\u03a0ij log pij + (1\u2212\u03a0ij) log (1\u2212pij)]\n(2) gets minimized. We define cross entropy losses on the remaining parts \u03a0vt, \u03a0tv and \u03a0vv by changing the index sets in (2) appropriately.\nOur method is based on Equations (1) and (2), yet, it yields a \u201cconditioned factorization\u201d only. Instead of learning the factor fi directly from the purchase data, we derive fi in a deterministic way via the SKU feature mapping\nfi = f(\u03c6i; \u03b8), (3)\nwhere \u03c6i collects article features and metadata for SKU i. Moreover, f is a family of nonlinear functions given by a deep neural network (DNN), and \u03b8 are parameters which are learned by our model. We remark that \u201cdeep structured semantic models\u201d [5] solve problems similar to the one studied here.\nFor every article i let us identify fi as its fDNA. Likewise, we can think of wj as the \u201cstyle DNA\u201d reflecting the individual preferences of customer j. The bias bj is a measure for the overall propensity of customer j to buy.\nIn addition to training, there are three validation procedures:\n1. Training: The purchase data in \u03a0tt is used to train the network parameters \u03b8 and the customer weights wj and biases bj , j \u2208 Jt, in order to perform the logistic matrix \u201cconditioned factorization\u201d via (1), as sketched in Figure 2. The loss is given by Equation (2).\n2. SKU validation: For SKUs not seen during training, \u03a0vt contains purchases of the training customers. Validation is straightforward: Part 1 yields the weights wj and bias bj for a training customer j \u2208 Jt, as well as the parameters \u03b8. We then compute the fDNA fi for SKUs i \u2208 Iv via Equation (3).\n3. Customer validation: Validation of new customers only works indirectly. It requires additional training of parameters, i. e., we have to find weights wj and biases bj for the new customers j \u2208 Jv. This amounts to Kv simultaneous logistic regression problems from fi to \u03a0ij , where the fi are taken from Part 1 (see Figure 3). The validation loss Ltv measures the fDNA\u2019s ability to linearly predict purchase behavior for such customers. High values compared to the training loss Ltt indicate that the SKU feature mapping f(\u00b7; \u03b8) does not generalize well to validation set customers.\n4. SKU-customer validation: Eventually, on \u03a0vv, we measure whether the logistic regression learned during customer validation in Part 3 generalizes well to unseen SKUs, analogous to the procedure in Part 2. The factors wj now correspond to the validation customers j \u2208 Jv, which were derived in Part 3 by means of logistic regression."}, {"heading": "2.4 Neural network architecture", "text": "We experimented with three different multi-layer neural network models (DNNs) that transform attribute and/or visual item information into fDNA. The width of their output layers defines the dimension d of the Fashion DNA space (in our case, d = 256). This fDNA then enters the logistic regression layer (Figures 2 and 3).\n2.4.1 Attribute-based model Here, we used only the six attributes listed in Table 1\nas input data. As described in Section 2.1, these \u201cexpert\u201d tags were combined into a sparse one-hot encoded vector that was supplied to a four-layer, fully connected deep neural network with steadily diminishing layer size. Activation was rendered nonlinear by standard ReLUs, and we used drop-out to address overfitting. The output yields\u201cattribute Fashion DNA\u201d based only on the six tags.\n2.4.2 Image-based model This model rests on purely visual information. It feeds\nZalando catalog images (see Section 2.1 for details) into an eight-layer convolutional neural network based on the AlexNet architecture [9], resulting in \u201cimage fDNA.\u201d.\n2.4.3 Combined model Finally, we integrated the attribute- and image-based ar-\nchitectures into a combined model. We first trained both\nnetworks and then froze their weights, so their outputs are the fDNA in either model. The outputs are concatenated into a single input vector, then supplied to a fully connected layer with adjustable weights that condenses this vector into the fDNA vector of the model. This layer, together with the logistic regressor, was trained by backpropagation as described above. The architecture is sketched in Figure 4.\nAll three models were implemented in the Caffe framework [6] and used to generate fDNA vectors for the standard SKU set, with corresponding customer weights and biases. For training, the weights of the logistic regression layer are initialized with Gaussian noise, while the bias is adjusted to reflect the individual purchase rate of customers on the training SKU set. The resulting fDNA vectors are sparse (about 30% non-zero components), leading to a compact representation of articles."}, {"heading": "3. FDNA-BASED RECOMMENDATIONS", "text": "A key objective of Fashion DNA is to match articles with prospective buyers. Recall that the network yields a sale probability pij for every pair of article i and customer j; it is therefore natural to rank the probabilities by SKU for a given customer to provide a personalized recommendation, or to order them by customer to find likely prospective buyers for a fashion item. In this section, we examine the properties of fDNA-based item recommendations for customers."}, {"heading": "3.1 Probability distribution", "text": "Predicted probabilities span a surprisingly large range: The least likely customer\u2013item matches are assigned probabilities pij < 10\n\u221212, too low to confirm empirically, while the classifier can be extraordinarily confident for likely pairings, with pij approaching 50%.\nTo be valuable, quantitative recommendations need to be unbiased: The predicted probability pij should accurately reflect the observed chance of sale of the item to the customer. As sales are binary events, comparing the probability\nwith the ground truth requires aggregating many customer\u2013 article pairs with similar predicted likelihoods in order to evaluate the accuracy of the forecast. As the average likelihood of an article-customer match is small (on the order of only 10\u22124 even for the frequent shoppers included here), comparisons are affected by statistical noise, unless large sample sizes are used. Fortunately, our model yields about 4 \u00b7 109 combinations of customers and fashion items in the respective validation sets.\nAn analysis of this type reveals that our models indeed are largely devoid of such bias, as Figure 5 illustrates. For the analysis, 2 \u00b7 108 customer\u2013item pairs have been sampled in the validation space, and then sorted into 200 bins by probability. In the figure, the average probability of the 106 members of each bin is compared to the number of actual purchases per pair. The predicted and empirical rates track each other closely, save for pairs considered very unlikely matches by the regressor. In this regime, the model underestimates the empirical purchase rate which settles to a base value of about 1.5 \u00b7 10\u22126."}, {"heading": "3.2 Recommendation quality", "text": "In some contexts, quantitative knowledge of the predicted purchase likelihood is less relevant than ranking a set of fashion articles by the preference of individual customers. The overall quality of such a ranking can be illustrated by receiver operating characteristic (ROC) analysis.\n3.2.1 ROC analysis An ideal model would rank those SKUs in the hold-out\nset highest that were actually bought by the customer. In reality, of course, the model will give preference to some items not chosen by the customer, and the number of \u201chits\u201d and \u201cmisses\u201d will both grow with the number of recommendations issued, or, equivalently, with a decreasing purchase probability threshold. A common technique to analyze the performance of such a binary classifier with variable threshold is ROC analysis [4]. The ROC diagram displays the number of recommended sold items (the \u201chits,\u201d or in the parlance of ROC, the \u201ctrue positives\u201d), as a function of recommended, but not purchased items (the \u201cmisses,\u201d or \u201cfalse positives\u201d). As the threshold probability is reduced from 1 to 0, the true and false positive rates trace out a ROC curve leading from the lower left to the upper right corner of the diagram. As overall higher true positive rates indicate a more discriminating recommender, the area under the ROC curve (AUC score) is a simple indicator of the relative performance of the underlying model [2], with AUC = 1.0 being ideal, whereas AUC = 0.5 is no better than guessing (diagonal line in diagram).\nIn principle, ROC analysis yields an individual curve and AUC score for each customer j, and will depend on the choice of fDNA model. To evaluate generalization performance, we aggregated numerous customer-article pairs (2 \u00b7 108) for each of the four training/validation combinations \u03a0tt, \u03a0vt, \u03a0tv, \u03a0vv, predicted purchase likelihood, and conducted a\u201csynthetic\u201dROC analysis on this data that presents an average over many customers instead. We repeated this calculation for each fDNA model. The outcomes are listed in condensed form as average AUC scores in Table 2.\n3.2.2 Generalization performance Before we discuss model performance in detail, we exam-\nine to which extent information gleaned from the training of our models is transferable to fashion items (e.g., SKUs newly added to the catalog) and customers outside the training set.\nTable 2 tells us that for a given fDNA model, the AUC score is highest if both customers and articles are taken from the training sets (\u03a0tt), diminishes as one of the groups is switched to the validation set (\u03a0vt, \u03a0tv), and becomes lowest when both customers and articles are taken from the holdout sets (\u03a0vv), as expected. But the scores conspicuously pair up: For a given SKU set (training or validation), there is very little difference in AUC score between the training and validation set customers. This observation holds irrespective of the specific fDNA model employed.\nPicking the most powerful fDNA model based on images and attributes, we determined ROC curves for the four cases laid out in Figure 1. Adopt the color code defined there, we display the curves in Figure 6. Indeed, the ROC curves for \u03a0tt (blue) and \u03a0tv (green) track each other closely, as do the curves for validation set articles \u03a0vt (red) and \u03a0vv (orange). We conclude that our approach generalizes extremely well to hold-out customers, at least when they shop at a similar\nrate. Next, we address another related aspect of generalization performance for our model architecture: How well does recommendation quality in the article training set correlate with the quality for items in the hold-out set for a given customer? Recommendation theory sometimes posits that most customers fall in well-defined groups that have aligned preferences, except for a few\u201cblack sheep\u201dwith idiosyncratic behavior that cannot be forecast well. However, given the extreme amount of choice, and the sparsity of sales coverage, it can be argued that idiosyncrasy in fashion taste and style is the norm rather than the exception.\nTo investigate, we compiled recommendations for a large group of individual customers for both article training and validation sets, and compared them to their actual purchases in both sets. Then, we examined the resulting pairs of customer-specific AUC scores for correlation. Figure 7 displays a scatterplot of such scores for the combined attributeimage fDNA model, using the hold-out customer set. Although regression analysis reveals a weak dependency (Pearson coefficient of correlation R2 \u2248 0.09), the plot is dominated by random statistical fluctuation, likely caused by the small number of sales events for any given customer. There is little evidence for\u201cblack sheep\u201dwith consistently low AUC scores, nor are there visible clusters.\n3.2.3 Model comparison While switching from training to validation set customers\nhardly affects recommendation performance, there is a clear difference between the three fDNA models introduced in Section 2.4 when the generalization to validation set articles is examined instead (Table 2). Although attribute- and imagebased fDNA perform at nearly equal levels in the training SKU set, the image-based model is distinctly inferior for articles in the validation set. Although this observation suggests that mere visual similarity is a worse predictor for customer interest than matching attributes, it might also involuntarily result from the widespread use of article attributes in search filters and recommendation algorithms that prejudice choice when customers browse the online shop.\nAs we are particularly interested in the ability of the models to generalize to unseen items and validation set customers, we calculated ROC curves from the probability forecasts for the segment \u03a0vv (orange sector in Figure 1). They are shown in Figure 8. The plot reveals that a ranking based on only six attributes (dashed curve) beats recommendations based on the much richer image fDNA (dotted curve). As possible causes behind this observation, we note that items with very different function may look strikingly similar (e. g., earmuffs and headphones), and that minor details may alter the association of an article with a customer group (for instance, men\u2019s and women\u2019s sport shoes often look much alike). As mentioned, users commonly select attributes as filter settings for the Zalando online catalog, so the better performance of attribute fDNA may also reflect the shopping habits of our customers. Importantly, we point out that integrating attribute and image information boosts recommendation performance considerably, as the ROC curve for the combined model fDNA (solid curve) shows. This indicates that images and attributes carry complementary information.\nFor a more practical interpretation of the ROC analysis, we turn to Figure 9, which provides a magnified view of the ROC curves near the origin, i.e., for small true and false positive rates. We note that the \u201cpositive\u201d (articles purchased) and\u201cnegatives\u201dclasses in this case are extremely unbalanced\n(the average customer only acquires 0.01% of the article set). Hence, to a very good approximation, all 120k articles in the validation set are negatives, and the rate of false positives shown on the abscissa is essentially equivalent to the ratio of the number of recommended articles to the validation set size. (For instance, a false positive rate of 10\u22124 represents the top-12 recommended articles.) From the figure, we infer that the top recommendation represents about 0.8% of validation set purchases in the combined fDNA model \u2014 for the average validation customer buying 17 items in the validation SKU set, this translates to a 13% \u201csuccess rate\u201d of the recommendation. For top-12 (top-120) recommendation, one captures 3.5% (13%) of the items bought by shoppers similar to the ones in the customer training set. (Of course, this ex-post-facto analysis cannot predict to which extent a customer would have bought the top-rated items, had they been suggested to her as recommendations. This question properly belongs to the domain of A/B testing.)"}, {"heading": "3.3 A Case Study", "text": "For illustration, we now present the leading recommendations of validation set articles for a sample frequent customer, a member of the training set, and compare results from the three Fashion DNA models introduced earlier. (For lack of space, we only discuss a single case here. Our observations hold quite generally, though.)\nFigure 10 displays the top-20 recommended fashion items for the attribute-based model (top row), image-based model (second from top), and the combination model, our strongest\ncontender (third row). The estimated likelihood of purchase decreases from left to right, with the first choice exceeding 10%; the mean forecast probability of sale for the articles on display hovers around 5%. For contrast, we also display the items deemed least likely in the combination model (fourth row). There, the model estimates a negligible chance of sale (around 10\u22128 %). Actual consumer purchases are on view in the bottom rows.\nNote that the items suggested by attribute fDNA, due to the lack of visual information in training, appear quite heterogeneous. The underlying similarity, typically matching brands, remains concealed in the image. (Among the attributes in Table 1, the brand and commodity group tags claim the lion\u2019s share of information, and major brands commonly try to gain market share by covering the whole fashion universe, from shoes to accessories.) Although quite successful for our sample customer (one item highlighted in green was bought, and the top\u2013100 recommendations capture four sales), the selection does not reflect her predilection with red and pink hues. Unsurprisingly, image-based fDNA yields a visually more uniform result, often aggregating items with a very similar look, here striped and pink shirts. Although pleasing to the eye, the approach is less successful. (None of the items shown was acquired, and there was a single \u201chit\u201d in the top 100 suggestions.) The combination model integrates the hidden features from the attribute model, and the visual trends (note that recommended SKUs from both models are taken over), and offers a compact, yet more varied selection. In line with the general observation, it is also the most performant model in our case study, with one item shown bought, and five within the top 100 selection. We finally remark that the least favorable items (bottom row) are by themselves a coherent group (boys\u2019 clothes) that summarizes qualities this customer is not interested in: kids\u2019 and men\u2019s apparel, plaid patterns. This suggests that the models\u2019 negative recommendations may be actionable as well."}, {"heading": "4. EXPLORING ARTICLE SIMILARITY", "text": "As laid out in Section 2, a central tenet of our approach is the assignment of articles i to vectors fi in a Fashion\nDNA space. Any metric in this space naturally defines a distance measure Dik between pairs of fashion items. Cosine similarity is a simple choice that works well in practice:\nDik = 1\u2212 fi \u00b7 fk\u221a |fi|2|fk|2 . (4)\nEvery fDNA model gives rise to its own geometrical structure that emphasizes different aspects of similarity."}, {"heading": "4.1 Next Neighbor Items", "text": "We start out exploring local neighborhoods in the fDNA spaces. Picking a sample article (here, a maternity dress and a bow tie), we determine their angular distances (4) to the standard SKUs, and select the nearest neighbors according to the three fDNA models.. They are displayed in Figure 11.\nFor both SKUs, attribute-based fDNA yields a heterogeneous mixture of articles that hides the underlying abstract similarity: The nearest neighbors to the dress all share the same brand, and in the case of the bow tie, men\u2019s accessories offered by various luxury brands are found. For the image-based fDNA, visual similarity is paramount; the search identifies many dresses aligned in color and style, photographed in a similar fashion. The same analysis for the bow tie reveals the risks underlying the visual strategy: Here, the algorithm picks completely unrelated objects that superficially look similar, like bras and a dog pendant. In either case, the most sensible results are returned by the combined attribute\u2013image approach. For the dress, the nearby items are generally maternity attire that matches the original brand (note that the mismatched objects in the attribute-based selection are now filtered out). In the case of the bow tie, regular ties are identified as nearest neighbors: The algorithm has learned that ties and bow ties share the same role in fashion. We also point out that the combined fDNA has included a baby romper (of the same brand) in the neighborhood of the maternity dress \u2013 a clear sign that information propagates backward from the customer\u2013item sales matrix into the fDNA model. Such conclusions are also inescapable when the global distribution of items in the fDNA space is studied."}, {"heading": "4.2 Large-Scale Structure", "text": "While the number of fashion items offered by Zalando is huge from a human perspective, they occupy the 256\u2013 dimensional fDNA space only sparsely. In order to visualize the arrangement of SKUs on a larger scale, we resort to dimensionality reduction techniques, and find that t\u2013SNE (stochastic neighborhood embedding) is a suitable tool [11] to reveal the structure hidden in the data. The resulting maps are rather fascinating descriptions of a fashion landscape that combines hierarchical order at several levels with smooth transitions.\nFigure 12 displays such a map, generated from 4,096 randomly drawn articles from the catalog (subject to the weak restriction that the articles be sold at least 10 times to our 100,000 most frequent customers). As underlying model, we used the combined attribute\u2013image fDNA. A glance at the map already shows the presence of orderly clusters, but much interesting detail is revealed only when studying the arrangement in close-up view. (A high-resolution image of the t-SNE map is posted online.) In Figure 13, we provide a guide to some \u201clandmarks\u201d we found examining the image.\nOn the largest scale, the map is divided into three \u201ccon-\ntinents\u201d: On the upper right, men\u2019s fashion items are assembled (A). The structure to the lower right contains kids\u2019 apparel and shoes, neatly separated into boys\u2019 (B) and girls\u2019 fashion (C), connected by a narrow isthmus of baby (D) and maternity items (E). The huge remainder of the map (F) is devoted to womens\u2019 fashion gear, with a core body of assorted clothing (upper left), surrounded by satellites of casual and fashion shoes (G, lower left), accessories (H), lingerie, hosiery, and swimwear (I), and sport shoes and gear (J). Each of the regions is further subdivided (e. g., accessories (H) are separated into scarves (red), belts (green), hats (brown), bags (blue), jewelry (purple), and sunglasses (cyan)). It is remarkable that the separation at the top level is essentially complete, as there is for instance no mixing between male and female items. Instead, subcategories are replicated in each section (such as a male sports cluster (K) that, naturally enough, faces the much larger female coun-\nterpart (J)). But there is also gradual progression in some structures: For instance, heel heights increase as one travels clockwise along the outer rim of the shoe cluster (with high heels (L) being opposite dresses (M) on the apparel side). Another example is kids\u2019 clothing, where size and age group increases as one travels away from the maternity \u201ccenter\u201d (E). It is notable that at the lower levels of organization, clustering mostly occurs by shape or pattern, and only rarely by function (e. g., there is a \u201csoccer cluster\u201d (N) in the boys\u2019 section), or brand loyalty, the Desigual \u201cisland\u201d (O) at the very bottom being the only conspicuous example."}, {"heading": "5. CONCLUSIONS", "text": "In this paper, we introduced the concept of Fashion DNA, a mapping of fashion articles to vectors using deep neural networks that is optimized to forecast purchases across a large group of customers. Being based on item properties,\nour approach is able to circumvent the cold start problem and provide article recommendations even in the absence of prior purchase records. Likewise, the model is flexible enough to generate sales probability predictions of comparable quality for validation customers. We demonstrated that an fDNA model based on article attributes and images generalizes well and suggests relevant items to shoppers. The combination of article and sales information imprints a wealth of structure onto the item distribution in fDNA space.\nWe plan to enrich the model with additional types of fashion-related data, such as ratings, reviews, sentiment, and social media, which will require extensions of the deep network handling the information, e. g. natural language processing capability. With an increasing number of information channels, for many items only partial data will be available. To render the network resilient to missing information, we will further experiment with drop-out layers in our architectures.\nAn important aspect currently absent in our model is the temporal order of sales events. Customer interest and item relevance evolve over time, whether by season, by fashion trends, or by personal circumstances. Capturing and forecasting such variations is a difficult task, but also a valuable business proposition that may be tackled by introducing long short-term memory (LSTM) elements into our network."}, {"heading": "6. ACKNOWLEDGMENTS", "text": "We would like to thank Urs Bergmann for valuable comments on this paper."}, {"heading": "7. REFERENCES", "text": "[1] R. Bell, Y. Koren, and C. Volinsky. Modeling\nrelationships at multiple scales to improve accuracy of large recommender systems. In Proceedings of the 13th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, KDD \u201907, pages 95\u2013104, New York, NY, USA, 2007. ACM.\n[2] P. Flach, J. Herna\u0301ndez-Orallo, and C. Ferri. A coherent interpretation of AUC as a measure of aggregated classification performance. In Proceedings of the 28th International Conference on Machine Learning (ICML\u201311), pages 657\u2013664, 2011.\n[3] D. Goldberg, D. Nichols, B. M. Oki, and D. Terry. Using collaborative filtering to weave an information tapestry. Communications of the ACM, 35(12):61\u201370, 1992.\n[4] T. Hastie, R. Tibshirani, and J. H. Friedman. The Elements of Statistical Learning: Data Mining, Inference, and Prediction (2nd ed.). Springer, New York, 2009.\n[5] P.-S. Huang, X. He, J. Gao, L. Deng, A. Acero, and L. Heck. Learning deep structured semantic models for web search using clickthrough data. In Proceedings of the 22nd ACM International Conference on Information and Knowledge Management (CIKM), pages 2333\u20132338, 2013.\n[6] Y. Jia, E. Shelhamer, J. Donahue, S. Karayev, J. Long, R. Girshick, S. Guadarrama, and T. Darrell. Caffe: Convolutional architecture for fast feature embedding. arXiv preprint arXiv:1408.5093, 2014.\n[7] C. Johnson. Logistic matrix factorization for implicit feedback data. In NIPS Workshop on Distributed Matrix Computations, 2014.\n[8] Y. Koren, R. Bell, and C. Volinsky. Matrix factorization techniques for recommender systems. IEEE Computer, pages 42\u201349, 2009.\n[9] A. Krizhevsky, I. Sutskever, and G. E. Hinton. Imagenet classification with deep convolutional neural networks. In Advances in neural information processing systems, pages 1097\u20131105, 2012.\n[10] A. I. Schein, A. Popescul, L. H. Ungar, and D. M. Pennock. Methods and metrics for cold-start recommendations. In Proceedings of the 25th annual international ACM SIGIR conference on Research and development in information retrieval, pages 253\u2013260, 2002.\n[11] L. van der Maaten and G. Hinton. Visualizing data using t-SNE. Journal of Machine Learning Research, pages 2579\u20132605, 2008."}], "references": [{"title": "Modeling relationships at multiple scales to improve accuracy of large recommender systems", "author": ["R. Bell", "Y. Koren", "C. Volinsky"], "venue": "Proceedings of the 13th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, KDD \u201907, pages 95\u2013104, New York, NY, USA", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2007}, {"title": "A coherent interpretation of AUC as a measure of aggregated classification performance", "author": ["P. Flach", "J. Hern\u00e1ndez-Orallo", "C. Ferri"], "venue": "Proceedings of the 28 International Conference on Machine Learning (ICML\u201311), pages 657\u2013664", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2011}, {"title": "Using collaborative filtering to weave an information tapestry", "author": ["D. Goldberg", "D. Nichols", "B.M. Oki", "D. Terry"], "venue": "Communications of the ACM, 35(12):61\u201370", "citeRegEx": "3", "shortCiteRegEx": null, "year": 1992}, {"title": "The Elements of Statistical Learning: Data Mining", "author": ["T. Hastie", "R. Tibshirani", "J.H. Friedman"], "venue": "Inference, and Prediction (2nd ed.). Springer, New York", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2009}, {"title": "Learning deep structured semantic models for web search using clickthrough data", "author": ["P.-S. Huang", "X. He", "J. Gao", "L. Deng", "A. Acero", "L. Heck"], "venue": "Proceedings of the 22nd ACM International Conference on Information and Knowledge Management (CIKM), pages 2333\u20132338", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2013}, {"title": "Caffe: Convolutional architecture for fast feature embedding", "author": ["Y. Jia", "E. Shelhamer", "J. Donahue", "S. Karayev", "J. Long", "R. Girshick", "S. Guadarrama", "T. Darrell"], "venue": "arXiv preprint arXiv:1408.5093", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2014}, {"title": "Logistic matrix factorization for implicit feedback data", "author": ["C. Johnson"], "venue": "NIPS Workshop on Distributed Matrix Computations", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2014}, {"title": "Matrix factorization techniques for recommender systems", "author": ["Y. Koren", "R. Bell", "C. Volinsky"], "venue": "IEEE Computer, pages 42\u201349", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2009}, {"title": "Imagenet classification with deep convolutional neural networks", "author": ["A. Krizhevsky", "I. Sutskever", "G.E. Hinton"], "venue": "Advances in neural information processing systems, pages 1097\u20131105", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2012}, {"title": "Methods and metrics for cold-start recommendations", "author": ["A.I. Schein", "A. Popescul", "L.H. Ungar", "D.M. Pennock"], "venue": "Proceedings of the 25th annual international ACM SIGIR conference on Research and development in information retrieval, pages 253\u2013260", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2002}, {"title": "Visualizing data using t-SNE", "author": ["L. van der Maaten", "G. Hinton"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2008}], "referenceMentions": [{"referenceID": 2, "context": "Collaborative-filtering based recommender systems [3, 1] suffer from the \u201ccold-start problem\u201d [10]: Individual predictions for new customers and articles cannot be calculated in ar X iv :1 60 9.", "startOffset": 50, "endOffset": 56}, {"referenceID": 0, "context": "Collaborative-filtering based recommender systems [3, 1] suffer from the \u201ccold-start problem\u201d [10]: Individual predictions for new customers and articles cannot be calculated in ar X iv :1 60 9.", "startOffset": 50, "endOffset": 56}, {"referenceID": 9, "context": "Collaborative-filtering based recommender systems [3, 1] suffer from the \u201ccold-start problem\u201d [10]: Individual predictions for new customers and articles cannot be calculated in ar X iv :1 60 9.", "startOffset": 94, "endOffset": 98}, {"referenceID": 3, "context": "Price labels were created using k\u2013means clustering [4] of the logarithm of manufacturer suggested prices (MSRPs).", "startOffset": 51, "endOffset": 54}, {"referenceID": 6, "context": "A possible strategy to solve a recommender problem is by logistic factorization [7], [8] of the purchase matrix \u03a0.", "startOffset": 80, "endOffset": 83}, {"referenceID": 7, "context": "A possible strategy to solve a recommender problem is by logistic factorization [7], [8] of the purchase matrix \u03a0.", "startOffset": 85, "endOffset": 88}, {"referenceID": 4, "context": "We remark that \u201cdeep structured semantic models\u201d [5] solve problems similar to the one studied here.", "startOffset": 49, "endOffset": 52}, {"referenceID": 8, "context": "1 for details) into an eight-layer convolutional neural network based on the AlexNet architecture [9], resulting in \u201cimage fDNA.", "startOffset": 98, "endOffset": 101}, {"referenceID": 5, "context": "All three models were implemented in the Caffe framework [6] and used to generate fDNA vectors for the standard SKU set, with corresponding customer weights and biases.", "startOffset": 57, "endOffset": 60}, {"referenceID": 3, "context": "A common technique to analyze the performance of such a binary classifier with variable threshold is ROC analysis [4].", "startOffset": 114, "endOffset": 117}, {"referenceID": 1, "context": "As overall higher true positive rates indicate a more discriminating recommender, the area under the ROC curve (AUC score) is a simple indicator of the relative performance of the underlying model [2], with AUC = 1.", "startOffset": 197, "endOffset": 200}, {"referenceID": 10, "context": "In order to visualize the arrangement of SKUs on a larger scale, we resort to dimensionality reduction techniques, and find that t\u2013SNE (stochastic neighborhood embedding) is a suitable tool [11] to reveal the structure hidden in the data.", "startOffset": 190, "endOffset": 194}], "year": 2016, "abstractText": "We present a method to determine Fashion DNA, coordinate vectors locating fashion items in an abstract space. Our approach is based on a deep neural network architecture that ingests curated article information such as tags and images, and is trained to predict sales for a large set of frequent customers. In the process, a dual space of customer style preferences naturally arises. Interpretation of the metric of these spaces is straightforward: The product of Fashion DNA and customer style vectors yields the forecast purchase likelihood for the customer\u2013item pair, while the angle between Fashion DNA vectors is a measure of item similarity. Importantly, our models are able to generate unbiased purchase probabilities for fashion items based solely on article information, even in absence of sales data, thus circumventing the \u201ccold\u2013start problem\u201d of collaborative recommendation approaches. Likewise, it generalizes easily and reliably to customers outside the training set. We experiment with Fashion DNA models based on visual and/or tag item data, evaluate their recommendation power, and discuss the resulting article similarities.", "creator": "LaTeX with hyperref package"}}}