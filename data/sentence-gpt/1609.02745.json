{"id": "1609.02745", "review": {"conference": "EMNLP", "VERSION": "v1", "DATE_OF_SUBMISSION": "9-Sep-2016", "title": "A Hierarchical Model of Reviews for Aspect-based Sentiment Analysis", "abstract": "Opinion mining from customer reviews has become pervasive in recent years. Sentences in reviews, however, are usually classified independently, even though they form part of a review's argumentative structure. Intuitively, sentences in a review build and elaborate upon each other; knowledge of the review structure and sentential context should thus inform the classification of each sentence. Thus, the author is a rationalist with an understanding of what makes a review useful.\n\n\nFor example, in the following example, I will show you the example of a review, which I believe shows that the reviewer is an expert in writing a review, and that the reviewer has a great deal more experience than anyone would expect. In this example, I will show you some helpful information that is helpful in making the review useful.\nYou can check out how many sentences I have written here.\nWhy Are Reviews Good?\nMany reviews are bad. It is simply because one of them is not one of those.\nFor example, there is no good reason to recommend an entire book.\nA review is not recommended because it might be a very long and tedious one.\nAs far as reviews are concerned, reviewers are usually very good at doing things that people may not realize, as long as they are not really doing them.\nWhat are some of the best words you can say about reviews?\nI am a reviewer.\nThe book I've written will have a clear, concise, engaging, concise, and engaging point of view. In this example, I recommend that you read it carefully.\nThe reviewer should also review everything that someone is reading and how much they read.\nIf you don't read all the important books that people read and know, the review should not make you feel bad or uncomfortable, but you should definitely not be reading them.\nThe only way to review reviews is to read them as much as possible before you have done them, because in the first place, it is very possible to go into a review without getting into a full review.\nI have a good reason for this, because the review system isn't perfect. The first two books in the book will not give you a complete view on the topic you read.\nA review may come in a couple of sections that you cannot completely read, and will likely only give you one of two points. The first two will be extremely helpful when reading a short story, but this is not the way to make things.\nOne of the most important things that can be written", "histories": [["v1", "Fri, 9 Sep 2016 11:16:15 GMT  (912kb,D)", "http://arxiv.org/abs/1609.02745v1", "To be published at EMNLP 2016, 7 pages"]], "COMMENTS": "To be published at EMNLP 2016, 7 pages", "reviews": [], "SUBJECTS": "cs.CL cs.LG", "authors": ["sebastian ruder", "parsa ghaffari", "john g breslin"], "accepted": true, "id": "1609.02745"}, "pdf": {"name": "1609.02745.pdf", "metadata": {"source": "CRF", "title": "A Hierarchical Model of Reviews for Aspect-based Sentiment Analysis", "authors": ["Sebastian Ruder", "Parsa Ghaffari", "John G. Breslin"], "emails": ["sebastian.ruder@insight-centre.org", "john.breslin@insight-centre.org", "sebastian@aylien.com", "parsa@aylien.com"], "sections": [{"heading": null, "text": "Opinion mining from customer reviews has become pervasive in recent years. Sentences in reviews, however, are usually classified independently, even though they form part of a review\u2019s argumentative structure. Intuitively, sentences in a review build and elaborate upon each other; knowledge of the review structure and sentential context should thus inform the classification of each sentence. We demonstrate this hypothesis for the task of aspect-based sentiment analysis by modeling the interdependencies of sentences in a review with a hierarchical bidirectional LSTM. We show that the hierarchical model outperforms two non-hierarchical baselines, obtains results competitive with the state-of-the-art, and outperforms the state-of-the-art on five multilingual, multi-domain datasets without any handengineered features or external resources."}, {"heading": "1 Introduction", "text": "Sentiment analysis (Pang and Lee, 2008) is used to gauge public opinion towards products, to analyze customer satisfaction, and to detect trends. With the proliferation of customer reviews, more fine-grained aspect-based sentiment analysis (ABSA) has gained in popularity, as it allows aspects of a product or service to be examined in more detail.\nReviews \u2013 just with any coherent text \u2013 have an underlying structure. A visualization of the discourse structure according to Rhetorical Structure Theory (RST) (Mann and Thompson, 1988) for the example review in Figure 1 reveals that sentences\nand clauses are connected via different rhetorical relations, such as Elaboration and Background.\nIntuitively, knowledge about the relations and the sentiment of surrounding sentences should inform the sentiment of the current sentence. If a reviewer of a restaurant has shown a positive sentiment towards the quality of the food, it is likely that his opinion will not change drastically over the course of the review. Additionally, overwhelmingly positive or negative sentences in the review help to disambiguate sentences whose sentiment is equivocal.\nNeural network-based architectures that have recently become popular for sentiment analysis and ABSA, such as convolutional neural networks (Severyn and Moschitti, 2015), LSTMs (Vo and Zhang, 2015), and recursive neural networks (Nguyen and Shirai, 2015), however, are only able to consider intra-sentence relations such as Background in Figure 1 and fail to capture inter-sentence relations, e.g. Elaboration that rely on discourse structure and provide valuable clues for sentiment prediction.\nWe introduce a hierarchical bidirectional long short-term memory (H-LSTM) that is able to leverage both intra- and inter-sentence relations. The sole dependence on sentences and their structure\nar X\niv :1\n60 9.\n02 74\n5v 1\n[ cs\n.C L\n] 9\nS ep\n2 01\nwithin a review renders our model fully languageindependent. We show that the hierarchical model outperforms strong sentence-level baselines for aspect-based sentiment analysis, while achieving results competitive with the state-of-the-art and outperforming it on several datasets without relying on any hand-engineered features or sentiment lexica."}, {"heading": "2 Related Work", "text": "Aspect-based sentiment analysis. Past approaches use classifiers with expensive hand-crafted features based on n-grams, parts-of-speech, negation words, and sentiment lexica (Pontiki et al., 2014; Pontiki et al., 2015). The model by Zhang and Lan (2015) is the only approach we are aware of that considers more than one sentence. However, it is less expressive than ours, as it only extracts features from the preceding and subsequent sentence without any notion of structure. Neural network-based approaches include an LSTM that determines sentiment towards a target word based on its position (Tang et al., 2015) as well as a recursive neural network that requires parse trees (Nguyen and Shirai, 2015). In contrast, our model requires no feature engineering, no positional information, and no parser outputs, which are often unavailable for low-resource languages. We are also the first \u2013 to our knowledge \u2013 to frame sentiment analysis as a sequence tagging task.\nHierarchical models. Hierarchical models have been used predominantly for representation learning and generation of paragraphs and documents: Li et al. (2015) use a hierarchical LSTM-based autoencoder to reconstruct reviews and paragraphs of Wikipedia articles. Serban et al. (2016) use a hierarchical recurrent encoder-decoder with latent variables for dialogue generation. Denil et al. (2014) use a hierarchical ConvNet to extract salient sentences from reviews, while Kotzias et al. (2015) use the same architecture to learn sentence-level labels from review-level labels using a novel cost function. The model of Lee and Dernoncourt (2016) is perhaps the most similar to ours. While they also use a sentencelevel LSTM, their class-level feed-forward neural network is only able to consider a limited number of preceding texts, while our review-level bidirectional LSTM is (theoretically) able to consider an unlimited number of preceding and successive sentences."}, {"heading": "3 Model", "text": "In the following, we will introduce the different components of our hierarchical bidirectional LSTM architecture displayed in Figure 2."}, {"heading": "3.1 Sentence and Aspect Representation", "text": "Each review consists of sentences, which are padded to length l by inserting padding tokens. Each review in turn is padded to length h by inserting sentences containing only padding tokens. We represent each sentence as a concatentation of its word embeddings x1:l where xt \u2208 Rk is the k-dimensional vector of the t-th word in the sentence.\nEvery sentence is associated with an aspect. Aspects consist of an entity and an attribute, e.g. FOOD#QUALITY. Similarly to the entity representation of Socher et al. (2013), we represent every aspect a as the average of its entity and attribute embeddings 12(xe + xa) where xe, xa \u2208 R\nm are the m-dimensional entity and attribute embeddings respectively1."}, {"heading": "3.2 LSTM", "text": "We use a Long Short-Term Memory (LSTM) (Hochreiter and Schmidhuber, 1997), which adds input, output, and forget gates to a recurrent cell, which allow it to model long-range dependencies that are essential for capturing sentiment.\nFor the t-th word in a sentence, the LSTM takes as input the word embedding xt, the previous output ht\u22121 and cell state ct\u22121 and computes the next output ht and cell state ct. Both h and c are initialized with zeros."}, {"heading": "3.3 Bidirectional LSTM", "text": "Both on the review and on the sentence level, sentiment is dependent not only on preceding but also successive words and sentences. A Bidirectional LSTM (Bi-LSTM) (Graves et al., 2013) allows us to look ahead by employing a forward LSTM, which processes the sequence in chronological order, and a backward LSTM, which processes the sequence in reverse order. The output ht at a given time step is then the concatenation of the corresponding states of the forward and backward LSTM.\n1Averaging embeddings produced slightly better results than using a separate embedding for every aspect."}, {"heading": "3.4 Hierarchical Bidirectional LSTM", "text": "Stacking a Bi-LSTM on the review level on top of sentence-level Bi-LSTMs yields the hierarchical bidirectional LSTM (H-LSTM) in Figure 2.\nThe sentence-level forward and backward LSTMs receive the sentence starting with the first and last word embedding x1 and xl respectively. The final output hl of both LSTMs is then concatenated with the aspect vector a2 and fed as input into the reviewlevel forward and backward LSTMs. The outputs of both LSTMs are concatenated and fed into a final softmax layer, which outputs a probability distribution over sentiments3 for each sentence."}, {"heading": "4 Experiments", "text": ""}, {"heading": "4.1 Datasets", "text": "For our experiments, we consider datasets in five domains (restaurants, hotels, laptops, phones, cam-\n2We experimented with other interactions, e.g. rescaling the word embeddings by their aspect similarity, an attention-like mechanism, as well as summing and multiplication, but found that simple concatenation produced the best results.\n3The sentiment classes are positive, negative, and neutral.\neras) and eight languages (English, Spanish, French, Russian, Dutch, Turkish, Arabic, Chinese) from the recent SemEval-2016 Aspect-based Sentiment Analysis task (Pontiki et al., 2016), using the provided train/test splits. In total, there are 11 domainlanguage datasets containing 300-400 reviews with 1250-6000 sentences4. Each sentence is annotated with none, one, or multiple domain-specific aspects and a sentiment value for each aspect."}, {"heading": "4.2 Training Details", "text": "Our LSTMs have one layer and an output size of 200 dimensions. We use 300-dimensional word embeddings. We use pre-trained GloVe (Pennington et al., 2014) embeddings for English, while we train embeddings on frWaC5 for French and on the Leipzig Corpora Collection6 for all other languages.7 Entity\n4Exact dataset statistics can be seen in (Pontiki et al., 2016). 5http://wacky.sslmit.unibo.it/doku.php? id=corpora 6http://corpora2.informatik.uni-leipzig. de/download.html 7Using 64-dimensional Polyglot embeddings (Al-Rfou et al., 2013) yielded generally worse performance.\nand attribute embeddings of aspects have 15 dimensions and are initialized randomly. We use dropout of 0.5 after the embedding layer and after LSTM cells, a gradient clipping norm of 5, and no l2 regularization.\nWe unroll the aspects of every sentence in the review, e.g. a sentence with two aspects occurs twice in succession, once with each aspect. We remove sentences with no aspect8 and ignore predictions for all sentences that have been added as padding to a review so as not to force our model to learn meaningless predictions, as is commonly done in sequenceto-sequence learning (Sutskever et al., 2014). We segment Chinese data before tokenization.\nWe train our model to minimize the cross-entropy loss, using stochastic gradient descent, the Adam update rule (Kingma and Ba, 2015), mini-batches of size 10, and early stopping with a patience of 10."}, {"heading": "4.3 Comparison models", "text": "We compare our model using random (H-LSTM) and pre-trained word embeddings (HP-LSTM) against the best model of the SemEval-2016 Aspectbased Sentiment Analysis task (Pontiki et al., 2016) for each domain-language pair (Best) as well as against the two best single models of the competition: IIT-TUDA (Kumar et al., 2016), which uses large sentiment lexicons for every language, and XRCE (Brun et al., 2016), which uses a parser aug-\n8Labeling them with a NONE aspect and predicting neutral slightly decreased performance.\nmented with hand-crafted, domain-specific rules. In order to ascertain that the hierarchical nature of our model is the deciding factor, we additionally compare against the sentence-level convolutional neural network of Ruder et al. (2016) (CNN) and against a sentence-level Bi-LSTM (LSTM), which is identical to the first layer of our model.9"}, {"heading": "5 Results and Discussion", "text": "We present our results in Table 1. Our hierarchical model achieves results superior to the sentencelevel CNN and the sentence-level Bi-LSTM baselines for almost all domain-language pairs by taking the structure of the review into account. We highlight examples where this improves predictions in Table 2.\nIn addition, our model shows results competitive with the best single models of the competition, while requiring no expensive hand-crafted features or external resources, thereby demonstrating its language and domain independence. Overall, our model compares favorably to the state-of-the-art, particularly for low-resource languages, where few hand-engineered features are available. It outperforms the state-of-the-art on four and five datasets using randomly initialized and pre-trained embeddings respectively.\n9To ensure that the additional parameters do not account for the difference, we increase the number of layers and dimensions of LSTM, which does not impact the results."}, {"heading": "5.1 Pre-trained embeddings", "text": "In line with past research (Collobert et al., 2011), we observe significant gains when initializing our word vectors with pre-trained embeddings across almost all languages. Pre-trained embeddings improve our model\u2019s performance for all languages except Russian, Arabic, and Chinese and help it achieve stateof-the-art in the Dutch phones domain. We release our pre-trained multilingual embeddings so that they may facilitate future research in multilingual sentiment analysis and text classification10."}, {"heading": "5.2 Leveraging additional information", "text": "As annotation is expensive in many real-world applications, learning from only few examples is important. Our model was designed with this goal in mind and is able to extract additional information inherent in the training data. By leveraging the structure of the review, our model is able to inform and improve its sentiment predictions as evidenced in Table 2.\nThe large performance differential to the state-ofthe-art for the Turkish dataset where only 1104 sentences are available for training and the performance gaps for high-resource languages such as English, Spanish, and French, however, indicate the limits of an approach such as ours that only uses data available at training time.\nWhile using pre-trained word embeddings is an 10https://s3.amazonaws.com/aylien-main/\ndata/multilingual-embeddings/index.html\neffective way to mitigate this deficit, for highresource languages, solely leveraging unsupervised language information is not enough to perform onpar with approaches that make use of large external resources (Kumar et al., 2016) and meticulously hand-crafted features (Brun et al., 2016).\nSentiment lexicons are a popular way to inject additional information into models for sentiment analysis. We experimented with using sentiment lexicons by Kumar et al. (2016) but were not able to significantly improve upon our results with pre-trained embeddings11. In light of the diversity of domains in the context of aspect-based sentiment analysis and many other applications, domain-specific lexicons (Hamilton et al., 2016) are often preferred. Finding better ways to incorporate such domain-specific resources into models as well as methods to inject other forms of domain information, e.g. by constraining them with rules (Hu et al., 2016) is thus an important research avenue, which we leave for future work."}, {"heading": "6 Conclusion", "text": "In this paper, we have presented a hierarchical model of reviews for aspect-based sentiment analysis. We demonstrate that by allowing the model to take into account the structure of the review and the sentential context for its predictions, it is able to outperform models that only rely on sentence information and achieves performance competitive with models that leverage large external resources and handengineered features. Our model achieves state-ofthe-art results on 5 out of 11 datasets for aspectbased sentiment analysis."}, {"heading": "Acknowledgments", "text": "We thank the anonymous reviewers, Nicolas P\u00e9cheux, and Hugo Larochelle for their constructive feedback. This publication has emanated from research conducted with the financial support of the Irish Research Council (IRC) under Grant Number EBPPG/2014/30 and with Aylien Ltd. as Enterprise Partner as well as from research supported by a research grant from Science Foundation Ireland (SFI) under Grant Number SFI/12/RC/2289.\n11We tried bucketing and embedding of sentiment scores as well as filtering and pooling as in (Vo and Zhang, 2015)"}], "references": [{"title": "Polyglot: Distributed Word Representations for Multilingual NLP", "author": ["Rami Al-Rfou", "Bryan Perozzi", "Steven Skiena."], "venue": "Proceedings of the Seventeenth Conference on Computational Natural Language Learning, pages 183\u2013192.", "citeRegEx": "Al.Rfou et al\\.,? 2013", "shortCiteRegEx": "Al.Rfou et al\\.", "year": 2013}, {"title": "XRCE at SemEval-2016 Task 5: Feedbacked Ensemble Modelling on Syntactico-Semantic Knowledge for Aspect Based Sentiment Analysis", "author": ["Caroline Brun", "Julien Perez", "Claude Roux."], "venue": "Proceedings of the 10th International Workshop on Semantic Evaluation", "citeRegEx": "Brun et al\\.,? 2016", "shortCiteRegEx": "Brun et al\\.", "year": 2016}, {"title": "Natural Language Processing (almost) from Scratch", "author": ["Ronan Collobert", "Jason Weston", "Leon Bottou", "Michael Karlen", "Koray Kavukcuoglu", "Pavel Kuksa."], "venue": "Journal of Machine Learning Research, 12(Aug):2493\u20132537.", "citeRegEx": "Collobert et al\\.,? 2011", "shortCiteRegEx": "Collobert et al\\.", "year": 2011}, {"title": "Extraction of Salient Sentences from Labelled Documents", "author": ["Misha Denil", "Alban Demiraj", "Nando de Freitas."], "venue": "arXiv preprint arXiv:1412.6815, pages 1\u20139.", "citeRegEx": "Denil et al\\.,? 2014", "shortCiteRegEx": "Denil et al\\.", "year": 2014}, {"title": "Speech Recognition with Deep Recurrent Neural Networks", "author": ["Alex Graves", "Abdel-rahman Mohamed", "Geoffrey Hinton."], "venue": "IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), (3):6645\u20136649.", "citeRegEx": "Graves et al\\.,? 2013", "shortCiteRegEx": "Graves et al\\.", "year": 2013}, {"title": "Inducing Domain-Specific Sentiment Lexicons from Unlabeled Corpora", "author": ["William L. Hamilton", "Kevin Clark", "Jure Leskovec", "Dan Jurafsky."], "venue": "Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics.", "citeRegEx": "Hamilton et al\\.,? 2016", "shortCiteRegEx": "Hamilton et al\\.", "year": 2016}, {"title": "Long Short-Term Memory", "author": ["Sepp Hochreiter", "J\u00fcrgen Schmidhuber."], "venue": "Neural Computation, 9(8):1735\u20131780.", "citeRegEx": "Hochreiter and Schmidhuber.,? 1997", "shortCiteRegEx": "Hochreiter and Schmidhuber.", "year": 1997}, {"title": "Harnessing Deep Neural Networks with Logic Rules", "author": ["Zhiting Hu", "Xuezhe Ma", "Zhengzhong Liu", "Eduard Hovy", "Eric Xing."], "venue": "Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics, pages 1\u201318.", "citeRegEx": "Hu et al\\.,? 2016", "shortCiteRegEx": "Hu et al\\.", "year": 2016}, {"title": "Adam: a Method for Stochastic Optimization", "author": ["Diederik P. Kingma", "Jimmy Lei Ba."], "venue": "International Conference on Learning Representations, pages 1\u201313.", "citeRegEx": "Kingma and Ba.,? 2015", "shortCiteRegEx": "Kingma and Ba.", "year": 2015}, {"title": "From Group to Individual Labels using Deep Features", "author": ["Dimitrios Kotzias", "Misha Denil", "Nando de Freitas", "Padhraic Smyth."], "venue": "Proceedings of the 21th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, pages 597\u2014-606.", "citeRegEx": "Kotzias et al\\.,? 2015", "shortCiteRegEx": "Kotzias et al\\.", "year": 2015}, {"title": "IIT-TUDA at SemEval2016 Task 5: Beyond Sentiment Lexicon: Combining Domain Dependency and Distributional Semantics Features for Aspect Based Sentiment Analysis", "author": ["Ayush Kumar", "Sarah Kohail", "Amit Kumar", "Asif Ekbal", "Chris Biemann."], "venue": "Pro-", "citeRegEx": "Kumar et al\\.,? 2016", "shortCiteRegEx": "Kumar et al\\.", "year": 2016}, {"title": "Sequential Short-Text Classification with Recurrent and Convolutional Neural Networks", "author": ["Ji Young Lee", "Franck Dernoncourt."], "venue": "Proceedings of NAACL-HLT 2016.", "citeRegEx": "Lee and Dernoncourt.,? 2016", "shortCiteRegEx": "Lee and Dernoncourt.", "year": 2016}, {"title": "A Hierarchical Neural Autoencoder for Paragraphs and Documents", "author": ["Jiwei Li", "Minh-Thang Luong", "Daniel Jurafsky."], "venue": "Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics, pages 1106\u20131115.", "citeRegEx": "Li et al\\.,? 2015", "shortCiteRegEx": "Li et al\\.", "year": 2015}, {"title": "Rhetorical Structure Theory: Toward a functional theory of text organization", "author": ["William C. Mann", "Sandra A. Thompson"], "venue": null, "citeRegEx": "Mann and Thompson.,? \\Q1988\\E", "shortCiteRegEx": "Mann and Thompson.", "year": 1988}, {"title": "PhraseRNN: Phrase Recursive Neural Network for Aspect-based Sentiment Analysis", "author": ["Thien Hai Nguyen", "Kiyoaki Shirai."], "venue": "Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, (September):2509\u20132514.", "citeRegEx": "Nguyen and Shirai.,? 2015", "shortCiteRegEx": "Nguyen and Shirai.", "year": 2015}, {"title": "Opinion Mining and Sentiment Analysis", "author": ["Bo Pang", "Lillian Lee."], "venue": "Foundations and trends in information retrieval, 2(1-2):1\u2013135.", "citeRegEx": "Pang and Lee.,? 2008", "shortCiteRegEx": "Pang and Lee.", "year": 2008}, {"title": "Glove: Global Vectors for Word Representation", "author": ["Jeffrey Pennington", "Richard Socher", "Christopher D. Manning."], "venue": "Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing, pages 1532\u20131543.", "citeRegEx": "Pennington et al\\.,? 2014", "shortCiteRegEx": "Pennington et al\\.", "year": 2014}, {"title": "SemEval-2014 Task 4: Aspect Based Sentiment Analysis", "author": ["Maria Pontiki", "Dimitrios Galanis", "John Pavlopoulos", "Haris Papageorgiou", "Ion Androutsopoulos", "Suresh Manandhar."], "venue": "Proceedings of the 8th International Workshop on Semantic Evaluation (Se-", "citeRegEx": "Pontiki et al\\.,? 2014", "shortCiteRegEx": "Pontiki et al\\.", "year": 2014}, {"title": "SemEval-2015 Task 12: Aspect Based Sentiment Analysis", "author": ["Maria Pontiki", "Dimitris Galanis", "Haris Papageorgiou", "Suresh Manandhar", "Ion Androutsopoulos."], "venue": "Proceedings of the 9th International Workshop on Semantic Evaluation (SemEval 2015), pages", "citeRegEx": "Pontiki et al\\.,? 2015", "shortCiteRegEx": "Pontiki et al\\.", "year": 2015}, {"title": "SemEval-2016 Task 5: Aspect-Based Sentiment Analysis", "author": ["Evgeny Kotelnikov", "Nuria Bel", "Salud Mar\u00eda Jim\u00e9nezZafra", "G\u00fcl\u015fen Eryi\u011fit."], "venue": "Proceedings of the 10th International Workshop on Semantic Evaluation, San Diego, California. Association for Com-", "citeRegEx": "Kotelnikov et al\\.,? 2016", "shortCiteRegEx": "Kotelnikov et al\\.", "year": 2016}, {"title": "INSIGHT-1 at SemEval-2016 Task 5: Deep Learning for Multilingual Aspect-based Sentiment Analysis", "author": ["Sebastian Ruder", "Parsa Ghaffari", "John G. Breslin."], "venue": "Proceedings of the 10th International Workshop on Semantic Evaluation (SemEval 2016).", "citeRegEx": "Ruder et al\\.,? 2016", "shortCiteRegEx": "Ruder et al\\.", "year": 2016}, {"title": "A Hierarchical Latent Variable Encoder-Decoder Model for Generating Dialogues", "author": ["Iulian Vlad Serban", "Alessandro Sordoni", "Ryan Lowe", "Laurent Charlin", "Joelle Pineau", "Aaron Courville", "Yoshua Bengio."], "venue": "Proceedings of the Advances in Neural Information", "citeRegEx": "Serban et al\\.,? 2016", "shortCiteRegEx": "Serban et al\\.", "year": 2016}, {"title": "UNITN: Training Deep Convolutional Neural Network for Twitter Sentiment Classification", "author": ["Aliaksei Severyn", "Alessandro Moschitti."], "venue": "Proceedings of the 9th International Workshop on Semantic Evaluation (SemEval 2015), pages 464\u2013469.", "citeRegEx": "Severyn and Moschitti.,? 2015", "shortCiteRegEx": "Severyn and Moschitti.", "year": 2015}, {"title": "Reasoning With Neural Tensor Networks for Knowledge Base Completion", "author": ["Richard Socher", "Danqi Chen", "Christopher D. Manning", "Andrew Y. Ng."], "venue": "Proceedings of the Advances in Neural Information Processing Systems 26 (NIPS 2013), pages 1\u201310.", "citeRegEx": "Socher et al\\.,? 2013", "shortCiteRegEx": "Socher et al\\.", "year": 2013}, {"title": "Sequence to sequence learning with neural networks", "author": ["Ilya Sutskever", "Oriol Vinyals", "Quoc V. Le."], "venue": "Advances in Neural Information Processing Systems, page 9.", "citeRegEx": "Sutskever et al\\.,? 2014", "shortCiteRegEx": "Sutskever et al\\.", "year": 2014}, {"title": "Target-Dependent Sentiment Classification with Long Short Term Memory", "author": ["Duyu Tang", "Bing Qin", "Xiaocheng Feng", "Ting Liu."], "venue": "arXiv preprint arXiv:1512.01100.", "citeRegEx": "Tang et al\\.,? 2015", "shortCiteRegEx": "Tang et al\\.", "year": 2015}, {"title": "Target-Dependent Twitter Sentiment Classification with Rich Automatic Features", "author": ["Duy-tin Vo", "Yue Zhang."], "venue": "IJCAI International Joint Conference on Artificial Intelligence, pages 1347\u20131353.", "citeRegEx": "Vo and Zhang.,? 2015", "shortCiteRegEx": "Vo and Zhang.", "year": 2015}, {"title": "ECNU: Extracting Effective Features from Multiple Sequential Sentences for Target-dependent Sentiment Analysis in Reviews", "author": ["Zhihua Zhang", "Man Lan."], "venue": "Proceedings of the 9th International Workshop on Semantic Evaluation (SemEval 2015), pages 736\u2013741.", "citeRegEx": "Zhang and Lan.,? 2015", "shortCiteRegEx": "Zhang and Lan.", "year": 2015}], "referenceMentions": [{"referenceID": 15, "context": "Sentiment analysis (Pang and Lee, 2008) is used to gauge public opinion towards products, to analyze customer satisfaction, and to detect trends.", "startOffset": 19, "endOffset": 39}, {"referenceID": 13, "context": "A visualization of the discourse structure according to Rhetorical Structure Theory (RST) (Mann and Thompson, 1988) for the example review in Figure 1 reveals that sentences Elaboration Background", "startOffset": 90, "endOffset": 115}, {"referenceID": 22, "context": "Neural network-based architectures that have recently become popular for sentiment analysis and ABSA, such as convolutional neural networks (Severyn and Moschitti, 2015), LSTMs (Vo and Zhang, 2015), and recursive neural networks (Nguyen and Shirai, 2015), however, are only able to consider intra-sentence relations such as Background in Figure 1 and fail to capture inter-sentence relations, e.", "startOffset": 140, "endOffset": 169}, {"referenceID": 26, "context": "Neural network-based architectures that have recently become popular for sentiment analysis and ABSA, such as convolutional neural networks (Severyn and Moschitti, 2015), LSTMs (Vo and Zhang, 2015), and recursive neural networks (Nguyen and Shirai, 2015), however, are only able to consider intra-sentence relations such as Background in Figure 1 and fail to capture inter-sentence relations, e.", "startOffset": 177, "endOffset": 197}, {"referenceID": 14, "context": "Neural network-based architectures that have recently become popular for sentiment analysis and ABSA, such as convolutional neural networks (Severyn and Moschitti, 2015), LSTMs (Vo and Zhang, 2015), and recursive neural networks (Nguyen and Shirai, 2015), however, are only able to consider intra-sentence relations such as Background in Figure 1 and fail to capture inter-sentence relations, e.", "startOffset": 229, "endOffset": 254}, {"referenceID": 17, "context": "Past approaches use classifiers with expensive hand-crafted features based on n-grams, parts-of-speech, negation words, and sentiment lexica (Pontiki et al., 2014; Pontiki et al., 2015).", "startOffset": 141, "endOffset": 185}, {"referenceID": 18, "context": "Past approaches use classifiers with expensive hand-crafted features based on n-grams, parts-of-speech, negation words, and sentiment lexica (Pontiki et al., 2014; Pontiki et al., 2015).", "startOffset": 141, "endOffset": 185}, {"referenceID": 25, "context": "Neural network-based approaches include an LSTM that determines sentiment towards a target word based on its position (Tang et al., 2015) as well as a recursive neural network that requires parse trees (Nguyen and Shirai, 2015).", "startOffset": 118, "endOffset": 137}, {"referenceID": 14, "context": ", 2015) as well as a recursive neural network that requires parse trees (Nguyen and Shirai, 2015).", "startOffset": 72, "endOffset": 97}, {"referenceID": 16, "context": "Past approaches use classifiers with expensive hand-crafted features based on n-grams, parts-of-speech, negation words, and sentiment lexica (Pontiki et al., 2014; Pontiki et al., 2015). The model by Zhang and Lan (2015) is the only approach we are aware of that considers more than one sentence.", "startOffset": 142, "endOffset": 221}, {"referenceID": 9, "context": "Hierarchical models have been used predominantly for representation learning and generation of paragraphs and documents: Li et al. (2015) use a hierarchical LSTM-based autoencoder to reconstruct reviews and paragraphs of Wikipedia articles.", "startOffset": 121, "endOffset": 138}, {"referenceID": 9, "context": "Hierarchical models have been used predominantly for representation learning and generation of paragraphs and documents: Li et al. (2015) use a hierarchical LSTM-based autoencoder to reconstruct reviews and paragraphs of Wikipedia articles. Serban et al. (2016) use a hierarchical recurrent encoder-decoder with latent variables for dialogue generation.", "startOffset": 121, "endOffset": 262}, {"referenceID": 3, "context": "Denil et al. (2014) use a hierarchical ConvNet to extract salient sentences from reviews, while Kotzias et al.", "startOffset": 0, "endOffset": 20}, {"referenceID": 3, "context": "Denil et al. (2014) use a hierarchical ConvNet to extract salient sentences from reviews, while Kotzias et al. (2015) use the same architecture to learn sentence-level labels from review-level labels using a novel cost function.", "startOffset": 0, "endOffset": 118}, {"referenceID": 3, "context": "Denil et al. (2014) use a hierarchical ConvNet to extract salient sentences from reviews, while Kotzias et al. (2015) use the same architecture to learn sentence-level labels from review-level labels using a novel cost function. The model of Lee and Dernoncourt (2016) is perhaps the most similar to ours.", "startOffset": 0, "endOffset": 269}, {"referenceID": 23, "context": "Similarly to the entity representation of Socher et al. (2013), we represent every aspect a as the average of its entity and attribute embeddings 12(xe + xa) where xe, xa \u2208 R m are the m-dimensional entity and attribute embeddings respectively1.", "startOffset": 42, "endOffset": 63}, {"referenceID": 6, "context": "We use a Long Short-Term Memory (LSTM) (Hochreiter and Schmidhuber, 1997), which adds input, output, and forget gates to a recurrent cell, which allow it to model long-range dependencies that are essential for capturing sentiment.", "startOffset": 39, "endOffset": 73}, {"referenceID": 4, "context": "A Bidirectional LSTM (Bi-LSTM) (Graves et al., 2013) allows us to look ahead by employing a forward LSTM, which processes the sequence in chronological order, and a backward LSTM, which processes the sequence in reverse order.", "startOffset": 31, "endOffset": 52}, {"referenceID": 16, "context": "We use pre-trained GloVe (Pennington et al., 2014) embeddings for English, while we train embeddings on frWaC5 for French and on the Leipzig Corpora Collection6 for all other languages.", "startOffset": 25, "endOffset": 50}, {"referenceID": 0, "context": "html Using 64-dimensional Polyglot embeddings (Al-Rfou et al., 2013) yielded generally worse performance.", "startOffset": 46, "endOffset": 68}, {"referenceID": 24, "context": "We remove sentences with no aspect8 and ignore predictions for all sentences that have been added as padding to a review so as not to force our model to learn meaningless predictions, as is commonly done in sequenceto-sequence learning (Sutskever et al., 2014).", "startOffset": 236, "endOffset": 260}, {"referenceID": 8, "context": "We train our model to minimize the cross-entropy loss, using stochastic gradient descent, the Adam update rule (Kingma and Ba, 2015), mini-batches of size 10, and early stopping with a patience of 10.", "startOffset": 111, "endOffset": 132}, {"referenceID": 10, "context": ", 2016) for each domain-language pair (Best) as well as against the two best single models of the competition: IIT-TUDA (Kumar et al., 2016), which uses large sentiment lexicons for every language, and XRCE (Brun et al.", "startOffset": 120, "endOffset": 140}, {"referenceID": 1, "context": ", 2016), which uses large sentiment lexicons for every language, and XRCE (Brun et al., 2016), which uses a parser aug-", "startOffset": 74, "endOffset": 93}, {"referenceID": 20, "context": "In order to ascertain that the hierarchical nature of our model is the deciding factor, we additionally compare against the sentence-level convolutional neural network of Ruder et al. (2016) (CNN) and against a sentence-level Bi-LSTM (LSTM), which is identical to the first layer of our model.", "startOffset": 171, "endOffset": 191}, {"referenceID": 2, "context": "In line with past research (Collobert et al., 2011), we observe significant gains when initializing our word vectors with pre-trained embeddings across almost all languages.", "startOffset": 27, "endOffset": 51}, {"referenceID": 10, "context": "html effective way to mitigate this deficit, for highresource languages, solely leveraging unsupervised language information is not enough to perform onpar with approaches that make use of large external resources (Kumar et al., 2016) and meticulously hand-crafted features (Brun et al.", "startOffset": 214, "endOffset": 234}, {"referenceID": 1, "context": ", 2016) and meticulously hand-crafted features (Brun et al., 2016).", "startOffset": 47, "endOffset": 66}, {"referenceID": 5, "context": "In light of the diversity of domains in the context of aspect-based sentiment analysis and many other applications, domain-specific lexicons (Hamilton et al., 2016) are often preferred.", "startOffset": 141, "endOffset": 164}, {"referenceID": 7, "context": "by constraining them with rules (Hu et al., 2016) is thus an important research avenue, which we leave for future work.", "startOffset": 32, "endOffset": 49}, {"referenceID": 8, "context": "We experimented with using sentiment lexicons by Kumar et al. (2016) but were not able to significantly improve upon our results with pre-trained embeddings11.", "startOffset": 49, "endOffset": 69}, {"referenceID": 26, "context": "We tried bucketing and embedding of sentiment scores as well as filtering and pooling as in (Vo and Zhang, 2015)", "startOffset": 92, "endOffset": 112}], "year": 2016, "abstractText": "Opinion mining from customer reviews has become pervasive in recent years. Sentences in reviews, however, are usually classified independently, even though they form part of a review\u2019s argumentative structure. Intuitively, sentences in a review build and elaborate upon each other; knowledge of the review structure and sentential context should thus inform the classification of each sentence. We demonstrate this hypothesis for the task of aspect-based sentiment analysis by modeling the interdependencies of sentences in a review with a hierarchical bidirectional LSTM. We show that the hierarchical model outperforms two non-hierarchical baselines, obtains results competitive with the state-of-the-art, and outperforms the state-of-the-art on five multilingual, multi-domain datasets without any handengineered features or external resources.", "creator": "LaTeX with hyperref package"}}}