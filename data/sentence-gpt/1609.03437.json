{"id": "1609.03437", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "12-Sep-2016", "title": "First-Order Bayesian Network Specifications Capture the Complexity Class PP", "abstract": "The point of this note is to prove that a language is in the complexity class PP if and only if the strings of the language encode valid inferences in a Bayesian network defined using function-free first-order logic with equality. For example, as the result of the \"test\", then we can use any language defined by the given set of rules. We can use any language defined by the set of rules to produce any particular class of rules, and that is for all possible transformations in any given set of rules. We can use all classes defined by the given set of rules in each step of our model, to define a single class of rules. In this way, we can use any language defined by the given set of rules. Let's say we have an example with all rules defined by the given set of rules: let's say we have the following: let's say we have the following rule: let's say we have the following rules: let's say we have the following rules: let's say we have a rule that has an equal or equal value for each of the rules: let's say we have a rule that has equal or equal value for each of the rules: let's say we have a rule that has an equal or equal value for each of the rules: let's say we have a rule that has equal or equal value for each of the rules: let's say we have a rule that has equal or equal value for each of the rules: let's say we have a rule that has equal or equal value for each of the rules: let's say we have a rule that has equal or equal value for each of the rules: let's say we have a rule that has equal or equal value for each of the rules: let's say we have a rule that has equal or equal value for each of the rules: let's say we have a rule that has equal or equal value for each of the rules: let's say we have a rule that has equal or equal value for each of the rules: let's say we have a rule that has equal or equal value for each of the rules: let's say we have a rule that has equal or equal value for each of the rules: let's say we have a rule that has equal or equal value for each of the rules: let's say we have a rule that has equal or equal value for each of the rules: let's say we have a rule that has equal or equal value for each of the rules: let's say we", "histories": [["v1", "Mon, 12 Sep 2016 15:11:58 GMT  (8kb)", "http://arxiv.org/abs/1609.03437v1", "7 pages, 1 figure"]], "COMMENTS": "7 pages, 1 figure", "reviews": [], "SUBJECTS": "cs.AI cs.CC cs.LO", "authors": ["fabio gagliardi cozman"], "accepted": false, "id": "1609.03437"}, "pdf": {"name": "1609.03437.pdf", "metadata": {"source": "CRF", "title": "First-Order Bayesian Network Specifications Capture the Complexity Class PP", "authors": ["Fabio G. Cozman"], "emails": [], "sections": [{"heading": null, "text": "ar X\niv :1\n60 9.\n03 43\n7v 1\n[ cs\n.A I]\n1 2\nSe p\n20 16"}, {"heading": "First-Order Bayesian Network Specifications", "text": ""}, {"heading": "Capture the Complexity Class PP", "text": ""}, {"heading": "Fabio G. Cozman", "text": "September 13, 2016"}, {"heading": "1 Introduction", "text": "The point of this note is to prove that a language is in the complexity class PP if and only if the strings of the language encode valid inferences in a Bayesian network defined using function-free first-order logic with equality. Before this statement can be made precise, a number of definitions are needed. Section 2 summarizes the necessary background and Section 3 defines first-order Bayesian network specifications and the complexity class PP. Section 4 states and proves the former captures the latter."}, {"heading": "2 Background", "text": "We collect a number of definitions here [1, 2], so as to fix our terminology and notation.\nWe consider input strings in the alphabet {0, 1}; that is, a string is a sequence of 0s and 1s. A language is a set of strings; a complexity class is a set of languages. A language is decided by a Turing machine if the machine accepts each string in the language, and rejects each string not in the language. The complexity class NP contains each languages that can be decided by a nondeterministic Turing machine with a polynomial time bound.\nWe focus on function-free first-order logic with equality (denoted by FFFO). That is, all formulas we contemplate are well-formed formulas of first-order logic with equality but without functions, containing predicates, negation (\u00ac), conjunction (\u2227), disjunction (\u2228), implication (\u21d2), equivalence (\u21d4), existential quatification (\u2203) and universal quantification (\u2200). The set of predicates is the vocabulary.\nA formula \u03c6 in existential function-free second-order logic (denoted by ESO) is a formula of the form \u2203r1 . . . \u2203rm\u03c6\n\u2032, where \u03c6\u2032 is a sentence of FFFO containing predicates r1, . . . , rm. Such a sentence allows existential quantification over the predicates themselves. Note that again we have equality in the language (that is, the built-in predicate = is always available).\nFor a given vocabulary, a structure A is a pair consisting of a domain and an interpretation. A domain is simply a set. An interpretation is a truth assignment for every grounding of every predicate that is not existentially quantified. As an example, consider the following formula of ESO as discussed by Gr\u00e4del [1]:\n\u2203partition : \u2200x : \u2200y : ( edge(x , y) \u21d2 (partition(x ) \u21d4 \u00acpartition(y)) ) .\nA domain is then a set that can be taken as the set of nodes of an input graph. An interpretation is a truth assignment for the edge predicate and can be taken as the set of edges of the input graph. The formula is satisfied if and only if it is possible to partition the vertices into two subsets such that if a node is in one subset, it is not in the other. That is, the formula is satisfied if and only if the input graph is bipartite.\nWe only consider finite vocabularies and finite domains in this note. If a formula \u03c6(x\u0302 ) has free logical variables x\u0302 , then denote by A |= \u03c6(a\u0302) the fact that formula \u03c6(a\u0302) is true in structure A when the logical variables x\u0302 are replaced by elements of the domain a\u0302. In this case say that A is a model of \u03c6(a\u0302).\nNote that if \u03c6(x\u0302 ) is a formula in ESO as in the previous paragraphs, then its interpretations runs over the groundings of the non-quantified predicates; that is, if \u03c6 contains predicates r1, . . . , rm and s1, . . . , sM , but r1, . . . , rm are all existentially quantified, then a model for \u03c6 contains an intepretation for s1, . . . , sM .\nThere is an isomorphism between structures A1 and A2 when there is a bijective mapping g between the domains such that if r(a1, . . . , ak) is true in A1, then r(g(a1), . . . , g(ak)) is true in A2, and moreover if r(a1, . . . , ak) is true in A2, then r(g 1(a1), . . . , g \u22121(ak)) is true in A1 (where g\n\u22121 denotes the inverse of g). A set of structures is isomorphism-closed if whenever a structure is in the set, all structures that are isomorphic to it are also in the set.\nWe assume that every structure is given as a string, encoded as follows for a fixed vocabulary [2, Section 6.1]. First, if the domain contains elements a1, . . . , an, then the string begins with n symbols 0 followed by 1. The vocabulary is fixed, so we take some order for the predicates, r1, . . . , rm. We then append, in this order, the encoding of the interpretation of each predicate. Focus on predicate ri of arity k. To encode it with respect to a domain, we need to order the elements of the domain, say a1 < a2 < \u00b7 \u00b7 \u00b7 < an. This total ordering is assumed for now to be always available; it will be important later to check that the ordering itself can be defined. In any case, with a total ordering we can enumerate lexicographically all k-tuples over the domain. Now suppose a\u0302j is the jth tuple in this enumeration; then the jthe bit of the encoding of ri is 1 if r(a\u0302j) is true in the given interpretation, and 0 otherwise. Thus the encoding is a string containing n+ 1 + \u2211m\ni=1 n arity(ri) symbols (either 0 or 1).\nWe can now state Fagin\u2019s theorem:\nTheorem 1. Let S be an isomorphism-closed set of finite structures of some non-empty finite vocabulary. Then S is in NP if and only if S is the class of finite models of a sentence in existential function-free second-order logic.\nDenote by CHECK the problem of deciding whether an input structure is a model of a fixed existential function-free second-order sentence. Fagin theorem means first that CHECK is in NP (this is the easy part of the theorem). Second, the theorem means that every language that can be decided by a polynomialtime nonderministic Turing machine can be exactly encoded as the set of models for a sentence in existential second-order logic (this is the surprising part of the theorem). This implies that CHECK is NP-hard, but the theorem is much more elegant (because it says that there is no need for some polynomial processing outside of the specification provided by existential second-order logic).\nThe significance of Fagin\u2019s theorem is that it offers a definition of NP that is not tied to any computational model; rather, it is tied to the expressivity of the language that is used to specify problems. Any language that can be decided by a polynomial nondeterministic Turing machine can equivalently be be decided using first-order logic with some added quantification over predicates."}, {"heading": "3 First-order Bayesian network specifications and", "text": "the complexity class PP\nWe start by defining our two main characters: on one side we have Bayesian networks that are specified using FFFO; on the other side we have the complexity class PP.\nIt will now be convenient to view each grounded predicate r(a\u0302) as a random variable once we have a fixed vocabulary and domain. So, given a domain D, we understand r(a\u0302) as a function over all possible interpretations of the vocabulary, so that r(a\u0302)(I) yields 1 if r(a\u0302) is true in interpretation I, and 0 otherwise."}, {"heading": "3.1 First-order Bayesian network specifications", "text": "A first-order Bayesian network specification is a directed graph where each node is a predicate, and where each root node r is associated with a probabilistic assessment\nP(r(x\u0302 ) = 1) = \u03b1,\nwhile each non-root node s(x\u0302 ) is associated with a formula (called the definition of s)\ns(x\u0302) \u21d4 \u03c6(x\u0302 ),\nwhere \u03c6(x\u0302 ) is a formula in FFFO with free variables x\u0302 . Given a domain, a first-order Bayesian network specification can be grounded into a unique Bayesian network. This is done:\n1. by producing every grounding of the predicates,\n2. by associating with each grounding r(a\u0302) of a root predicate the grounded assessment P(r(a\u0302) = 1) = \u03b1;\n3. by associating with each grounding s(a\u0302) of a non-root predicate the grounded definition s(a\u0302) \u21d4 \u03c6(a\u0302);\n4. finally, by drawing a graph where each node is a grounded predicate and where there is an edge into each grounded non-root predicate s(a\u0302) from each grounding of a predicate that appears in the grounded definition of s(a\u0302).\nConsider, as an example, the following model of asymmetric friendship, where an individual is always a friend of herself, and where two individuals are friends if they are both fans (of some writer, say) or if there is some \u201cother\u201d reason for it:\nP(fan(x )) = 0.2,\nP ( friends(x , y) ) \u21d4 (x = y) \u2228\n(fan(x ) \u2227 fan(y)) \u2228 (1)\nother(x , y),\nP ( other(x , y) ) = 0.1.\nSuppose we have domain D = {a, b, c}. Figure 1 depicts the Bayesian network generated by D and Expression (1).\nFor a given Bayesian network specification \u03c4 and a domain D, denote by B(\u03c4,D) the Bayesian network obtained by grouning \u03c4 with respect to D. The set of all first-order Bayesian network specifications is denoted by B(FFFO)."}, {"heading": "3.2 Probabilistic Turing machines and the complexity class", "text": "PP\nIf a Turing machine is such that, whenever its transition function maps to a nonsingleton set, the transition is selected with uniform probability within that set, then the Turing machine is a probabilistic Turing machine. The complexity class PP is the set of languages that are decided by a probabilistic Turing machine in polynomial time, with an error probability strictly less than 1/2 for all input strings.\nIntuitively, PP represents the complexity of computing probabilities for a phenomenon that can be simulated by a polynomial probabilistic Turing machine.\nThis complexity class can be equivalently defined as follows: a language is in PP if and only if there is a polynomial nondeterministic Turing machine such that a string is in the language if and only if more than half of the computation paths of the machine end in the accepting state when the string is the input. We can imagine that there is a special class of nondeterministic Turing machines that, given an input, not only accept it or not, but actually write in some special tape whether that input is accepted in the majority of computation paths. Such a special machine could then be used directly to decide a language in PP.\n4 B(FFFO) captures PP\nGiven a first-order Bayesian network specification and a domain, an evidence piece E is a partial interpretation; that is, an evidence piece assigns a truth value for some groundings of predicates.\nWe encode a pair domain/evidence (D,E) using the same strategy used before to encode a structure; however, we must take into account the fact that a particular grounding of a predicate can be either assigned true or false or be left without assignment. So we use a pair of symbols in {0, 1} to encode each grounding; we assume that 00 means \u201cfalse\u201d and 11 means \u201ctrue\u201d, while say 01 means lack of assignment.\nSay there is an isomorphism between pairs (D1,E1) and (D2,E2) when there is a bijective mapping g between the domains such that if r(a1, . . . , ak) is true in E1, then r(g(a1), . . . , g(ak)) is true in E2, and moreover if r(a1, . . . , ak) is true in E2, then r(g 1(a1), . . . , g \u22121(ak)) is true in E1 (where again g\n\u22121 denotes the inverse of g). A set of pairs domain/evidence is isomorphism-closed if whenever a pair is in the set, all pairs that are isomorphic to it are also in the set.\nSuppose a set of pairs domain/evidence is given with respect to a fixed vocabulary \u03c3. Once encoded, these pairs form a language L that can for instance belong to NP or to PP. One can imagine building a Bayesian network specification \u03c4 on an extended vocabulary consisting of \u03c3 plus some additional predicates, so as to decide this language L of domain/evidence pairs. For a given input pair (D,E), the Bayesian network specification and the domain lead to a Bayesian network B(\u03c4,D); this network can be used to compute the probability\nof some groundings, and that probabiility in turn can be used to accept/reject the input. This is the sort of strategy we pursue.\nThe point is that we must determine some prescription by which, given a Bayesian network and an evidence piece, one can generate an actual decision so as to accept/reject the input pair domain/evidence. Suppose we take the following strategy. Assume that in the extended vocabulary of \u03c4 there are two sets of distinguished auxiliary predicates A1, . . . , Am\u2032 and B1, . . . , Bm\u2032\u2032 that are not in \u03c3. We can use the Bayesian network B(\u03c4,D) to compute the probability P(A|B,E) where A and B are interpretations of A1, . . . , Am\u2032 and B1, . . . , Bm\u2032\u2032 respectively. And then we might accept/reject the input on the basis of P(A|B,E). However, we cannot specify particular intepretations A and B as the related predicates are not in the vocabulary \u03c3. Thus the sensible strategy is to fix attention to some selected pair of intepretations; we simply take the interpretations that assign true to every grounding.\nIn short: use the Bayesian network B(\u03c4,D) to determine whether or not P(A|B,E) > 1/2, where A assigns true to every grounding of A1, . . . , Am\u2032 , and B assigns true to every grounding of B1, . . . , Bm\u2032\u2032 . If this inequality is satisfied, the input pair is accepted; if not, the input pair is rejected.\nWe refer to A1, . . . , Am\u2032 as the conditioned predicates, and to B1, . . . , Bm\u2032\u2032\nas the conditioning predicates. Here is the main result:\nTheorem 2. Let S be an isomorphism-closed set of pairs domain/evidence of some non-empty finite vocabulary, where all domains are finite. Then S is in PP if and only if S is the class of domain/evidence pairs that are accepted by a fixed first-order Bayesian network specification with fixed conditioned and conditioning predicates.\nProof. First, if S is a class of domain/query pairs that are accepted by a fixed first-order Bayesian network specification, they can be decided by a polynomial time probability Turing machine. To see that, note that we can build a nondeterministic Turing machine that guesses the truth value of all groundings that do not appear in the query (that is, not in A\u222aB\u222aE), and then verify whether the resulting complete interpretation is a model of the first-order Bayesian network specification (as model checking of a fixed first-order sentence is in P [2]).\nTo prove the other direction, we must adapt the proof of Fagin\u2019s theorem as described by Gr\u00e4del [1], along the same lines as the proof of Theorem 1 by Saluja et al. [3]. So, suppose that L is a language decided by some probabilistic Turing machine. So equivalently there is a nondeterministic Turing machine that determines whether the majority of its computation paths accept an input, and accepts/rejects the input accordingly. By the mentioned proof of Fagin\u2019s theorem, there is a first-order sentence \u03c6\u2032 with vocabulary consisting of the vocabulary of the input plus additional auxiliary predicates, such that each interpretation of this joint vocabulary is a model of the sentence if it is encodes a computation path of the Turing machine, as long as there is an available additional predicate that is guaranteed to be a linear order on the domain. Denote by A the zero arity predicate with associated definition A \u21d4 \u03c6\u2032. Suppose\na linear order is indeed available; then by creating a first-order Bayesian network specification where all groundings are associated with probability 1/2, and where a non-root node is associated with the sentence in the proof of Fagin\u2019s theorem, we have that the probability of the query is larger than 1/2 iff the majority of computation paths accept. The challenge is to encode a linear order. To do so, introduce a new predicate < and the first-order sentence \u03c6\u2032\u2032 that forces < to be a total order, and a zero arity predicate B that is associated with definition B \u21d4 \u03c6\u2032\u2032. Now an input domain/pair (D,E) is accepted by the majority of computation paths in the Turing machine if and only if we have P(A|B,E) > 1/2. Note that there are actually n! linear orders that satisfy B, but for each one of these linear orders we have the same assignments for all other predicates, hence the ratio between accepting computations and all computations is as desired.\nWe might picture this as follows. There is always a Turing machine TM and a corresponding triple (\u03c4, A,B) such that for any pair (D,E), we have\n(D,E) as input to TM with output given by P(TM accepts (D,E)) > 1/2,\nif and only if\n(D,E) as \u201cinput\u201d to (\u03c4, A,B) with \u201coutput\u201d given by P\u03c4,D(A|B,E) > 1/2,\nwhere P\u03c4,D(A|B,E) denotes probability with respect to B(\u03c4,D). (Of course, there is no need to use only zero-arity predicates A and B, as Theorem 2 allows for sets of predicates.)\nNote that the same result could be proved if every evidence piece was taken to be a complete interpretation for the vocabulary \u03c3. In that case we could directly speak of structures as inputs, and then the result would more closely mirror Fagin\u2019s theorem. However it is very appropriate, and entirely in line with practical use, to take the inputs to a Bayesian network as the groundings of a partially observed interpretation. Hence we have preferred to present our main result as stated in Theorem 2."}], "references": [{"title": "Finite model theory and descriptive complexity", "author": ["Erich Gr\u00e4del"], "venue": "Finite Model Theory and Its Applications,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2007}, {"title": "Descriptive complexity of#P functions", "author": ["Sanjeev Saluja", "K.V. Subrahmanyam", "Madhukar N. Thakur"], "venue": "Journal of Computer and System Sciences,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 1995}], "referenceMentions": [{"referenceID": 0, "context": "We collect a number of definitions here [1, 2], so as to fix our terminology and notation.", "startOffset": 40, "endOffset": 46}, {"referenceID": 0, "context": "As an example, consider the following formula of ESO as discussed by Gr\u00e4del [1]:", "startOffset": 76, "endOffset": 79}, {"referenceID": 0, "context": "To prove the other direction, we must adapt the proof of Fagin\u2019s theorem as described by Gr\u00e4del [1], along the same lines as the proof of Theorem 1 by Saluja et al.", "startOffset": 96, "endOffset": 99}, {"referenceID": 1, "context": "[3].", "startOffset": 0, "endOffset": 3}], "year": 2016, "abstractText": "The point of this note is to prove that a language is in the complexity class PP if and only if the strings of the language encode valid inferences in a Bayesian network defined using function-free first-order logic with equality. Before this statement can be made precise, a number of definitions are needed. Section 2 summarizes the necessary background and Section 3 defines first-order Bayesian network specifications and the complexity class PP. Section 4 states and proves the former captures the latter.", "creator": "LaTeX with hyperref package"}}}