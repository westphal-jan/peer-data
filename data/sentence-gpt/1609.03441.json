{"id": "1609.03441", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "12-Sep-2016", "title": "Read, Tag, and Parse All at Once, or Fully-neural Dependency Parsing", "abstract": "We present a dependency parser implemented as a single deep neural network that reads orthographic representations of words and directly generates dependencies and their labels. Unlike typical approaches to parsing, the model doesn't require part-of-speech (POS) tagging of the sentences. With proper regularization and additional supervision achieved with multitask learning we reach state-of-the-art performance on Slavic languages from the Universal Dependencies treebank: with no linguistic features other than characters, our parser is as accurate as a transition- based system trained on perfect POS tags.", "histories": [["v1", "Mon, 12 Sep 2016 15:16:43 GMT  (110kb,D)", "http://arxiv.org/abs/1609.03441v1", null], ["v2", "Mon, 5 Jun 2017 18:46:40 GMT  (180kb,D)", "http://arxiv.org/abs/1609.03441v2", null]], "reviews": [], "SUBJECTS": "cs.CL", "authors": ["jan chorowski", "micha{\\l} zapotoczny", "pawe{\\l} rychlikowski"], "accepted": false, "id": "1609.03441"}, "pdf": {"name": "1609.03441.pdf", "metadata": {"source": "CRF", "title": null, "authors": [], "emails": ["jan.chorowski@ii.uni.wroc.pl"], "sections": [{"heading": "1 Introduction", "text": "The ability to communicate using natural language is one of the long-term goals of artificial intelligence. Moreover, due to the huge amount of natural language texts there is a growing need to develop effective algorithms to handle them in a satisfactory manner. In the last decades one could observe a shift of focus from linguistics to statistical text analysis and more recently to machine learning systems and neural networks.\nDeep learning methods have led to many breakthrough in NLP tasks, such as language modeling (Mikolov et al., 2010), machine translation (Bahdanau et al., 2014; Sutskever et al., 2014), caption generation (Xu et al., 2015), question answering (Sukhbaatar et al., 2015), speech regognition, POS-taggers and so on.\nFinding the syntactical structure of sentences is one of the essential needs in natural language text analysis. Parsing is a key component required for automated natural language understanding. Virtually all NLP task could benefit from having a good quality parse tree for analyzed sentence.\nIn this contribution we build a deep learning dependency parser that operates directly on characters. We show how the parser can be trained in an end-to-end way, without the need of auxiliary tools such as a separate part-of-speech (POS) tagger. On morphologically rich languages, such as the Slavic family, the parser achieves state-of-the-art performance without the need of a full NLP pipeline. Furthermore, we show how proper regularization and multi-task learning can greatly reduce overfitting and make our model competitive even on languages with limited training resources.\nar X\niv :1\n60 9.\n03 44\n1v 1\n[ cs\n.C L\n] 1"}, {"heading": "2 Description of the Model", "text": "A dependency parser reads a sentence and finds a set of dependencies, that are triples composed of a head word, a dependent word, and a label describing the dependency type. Each word has exactly one head, with one word in the sentence (typically the verb) having an artificial <ROOT> token attached as its head. Therefore the set of dependencies can be interpreted as an oriented tree linking words of the sentence. Please see Figure 1 for an exemplary dependency tree.\nOur dependency parser is implemented as a single neural network with three parts, as depicted in Figure 2. First, the reader subnetwork finds word embeddings based on their orthographic representations using convolutional and highway layers (Kim et al., 2015; Srivastava et al., 2015).\nSecond, a bidirectional recurrent tagger subnetwork puts the individual words into their contexts (Schuster and Paliwal, 1997). Finally, the parser subnetwork uses the soft-attention mechanism to point each word to its head (Vinyals et al., 2015; Bahdanau et al., 2014). Once the head is found, it is used to compute the dependency label."}, {"heading": "2.1 Reader subnetwork", "text": "The reader subnetwork is tasked with finding embeddings of words given their orthographic representations. For many languages the spelling of a word is a strong indicator of its grammatical function. Following Kim et al. (2015), we use a convolutional filterbank followed by a few layers of nonlinear transformations. Each word w is represented by a sequence of its characters, fenced with special beginning-of-word and end-of-word markers. We find low-dimensional embeddings of characters and concatenate them to form a matrix Cw.\nNext, the matrix Cw is reduced to a vector of filter responses Rw \u2208 Rnf, where nf denotes the number of filters. Each filter response is computed as:\nRwi = max(C w ~ F i),\nwhere F i is the i-th filter and ~ denotes convolution over the length of the word. Intuitively, the convolutions act like pattern matches that react to specific parts of the word. Furthermore, the filters can differentiate between prefixes, suffixes and infixes by reacting to the beginning- and end-of-word markers that are added to each word.\nFinally, the reader applies a nonlinear transformation to filter responses Rw. First, a linear transformation is used to reduce the dimensionality of the representation. Then a stack of highway layers (Srivastava et al., 2015) is applied to obtain the final word embeddings Ew."}, {"heading": "2.2 Tagging subnetwork", "text": "The tagging subnetwork works on sequences of representations of words Ew produced by the reader. It uses bidirectional recurrent layers (BiRNN) to put them into a broader context (Schuster and Paliwal, 1997). Specifically, we use GRU recurrent units (Cho et al., 2014) to scan the sequence forward and backward. The hidden representations are combined using addition, and passed to another layer of recurrent units.\nThe tagger can be trained based solely on the gradient signal flowing into it from the parsing subnetwork. However, it is also possible to branch off the signal from one of the BiRNN layers and use it to predict part of speech (POS) tags of individual words. This additional supervision typically helps to prevent overfitting of the parser. In the experimental results section we present the impact of explicit POS-tag training."}, {"heading": "2.3 Parsing subnetwork", "text": "The parsing subnetwork has two objectives: first, to match dependent words to their heads and second, to label each pair of matched words with the proper dependency type.\nWe have chosen to use the pointer network (Vinyals et al., 2015) approach to find head words. For each sentence the parser obtains a sequence H1, H2, . . . ,Hn of vectors of word annotations produced by the tagger. We prepend to this sequence a special vector H0 denoting the root word. This guarantees, that each word of the original sentence has exactly one head word. To train the pointer network we construct a probability distribution over possible head word locations l \u2208 0, 1, . . . , n. First, for each word w \u2208 1, 2, . . . n we compute a score over all possible locations l:\ns(w, l) = f(Hw, Hl), (1)\nWhere the scorer f(\u00b7, \u00b7) is implemented as a small feed-forward neural network.\nThe scores are normalized over locations using the SoftMax function to p(w, l), which are interpreted as the probabilities that the head of word w is at location l:\np(w, l) = s(w, l)\u2211n\nl\u2032=0 s(w, l \u2032)\n(2)\nFinally, the dependency label is computed by a small Maxout network (Goodfellow et al., 2013) using the annotations of both the head Hh and the dependent Hw words. We have analyzed two variants:\n1. a soft attention labeler, which uses the expected annotation of the head word computed under the probability distribution given by eq. (2) Hh = \u2211 l p(w, l)Hl;\n2. a hard attention labeler, which uses the annotation Hh taken at the location of the correct head word.\nWe have briefly experimented with adding a recurrent hidden state to the computation of scores s. The recurrent state was updated after processing each word of the sequence. However, experiments have show little benefits of this additional computation."}, {"heading": "2.4 Training criterion", "text": "The network receives training signal from three sources:\n1. The negative log-likelihood loss on predicting dependency labels Ll. With the soft attention labeler this loss is backpropagated through the entire network and theoretically could be used to train the entire network. With the hard attention labeler this error is not backprogated to the scorer.\n2. The negative log-likelihood loss on finding proper head words Ls by the scorer. This loss is backpropagated through the reader and tagger subnetworks, but not through the labeler.\n3. The optional POS-tagging negative log-likelihood loss Lt. This loss is backpropagated only thorough a few layers of the tagger and through the reader.\nThe final loss is computed as a linear combination of the individual losses:\nL = \u03b1lLl + \u03b1sLs + \u03b1tLt (3)"}, {"heading": "2.5 Parsing algorithm", "text": "At its core, the network produces, for each pair of words, scores that reflect the probability that the words for a dependency. These scores can be used to form a parse tree by finding a set of dependencies that satisfy some constraints (exactly one word is dependent on the root token, there are no cycles, the tree is projective).\nHowever, we have found that at the end of training the scores computed by eq. (2) typically lead to a very peaked probability distribution that is concentrated on just a single location. Therefore good results are obtained with a greedy parsing strategy that for each word simply chooses the best scoring parent. Only approximately 0.5% of the parses obtained by this procedure have cycles, so using Chu-Liu-Edmonds (Edmonds, 1966) maximum spanning arborescence algorithm (which deletes cycles) gives only a subtle improvement and is not presented in the Table 2."}, {"heading": "3 Related work", "text": "There are two basic views on syntactic structure of the sentence:\n\u2022 constituent based, where words are organized in nested constituents \u2022 dependency based, where words are connected by dependency relation\nThis work focuses on the dependency parsing. We believe that currently two approaches are the most important ones: transition and graph based. A transition based parser aims to predict the best parser\naction (such as moving the word to stack or add a dependency between current word and the word on a stack) looking at some features (Nivre, 2008). A graph based parser finds the structure which maximizes a global score while preserving some constraints (i.e. forces the output to be well formed tree). Recently, deep neural networks were used wih a great success in dependency parsing, both transition (Chen and Manning, 2014; Dyer et al., 2015; Kiperwasser and Goldberg, 2016; Andor et al., 2016) and graph (Pei et al., 2015) based. Our parser is most similar to the graph-based variant of (Kiperwasser and Goldberg, 2016). However, we replace the POS tagger with our reader subnetwork thus reducing the need for feature engineering, which is an important aspect of parser construction which requires knowledge of linguistics. Powerful learning techniques reduce the burden of this somewhat language-specific work.\nOur parsing network brings together ideas from many recent contributions. Ling et al. (2015) successfully applied character-based word embeddings computed with small BiRNNs (another possible implementation of our \u201creading\u201d subnetwork) to POS-tagging and language modeling with recurrent networks. The character-based word embeddings that we have used were described by Kim et al. (2015) and extensively analyzed by Jozefowicz et al. (2016).\nA purely neural constituency parser was shown by Socher et al. (2011). It built a parse tree by repeatedly joining words or subtrees using a recursive network. Later, Vinyals et al. (2014) have shown that good constituency parsers can be created by learning to \u201ctranslate\u201d between a given sentence and the linearization of its parse tree. The parser accessed the source sentence through word embeddings, which were initialized with Word2Vec (Mikolov et al., 2013) and adapted during parser training. We build on their work by directly using the attention matrix as pointers into the source sentence locations that correspond head words. This change greatly simplifies the parser: there is no recurrent generator and no need for an approximate search during evaluation.\nTo the best of our knowledge, we have built the first parser that is trained in an end-to-end manner reading directly characters of words and producing the parse tree, rather than relying on a separately trained tagger. This approach is especially beneficial morphologically rich languages for which the orthographic representation is meaningful."}, {"heading": "4 Experimental Setup", "text": "We have evaluated our parser on three languages, English, Czech, and Polish from the Universal Dependencies (UD) v. 1.2 dataset (Nivre et al., 2015). We have chosen this dataset because of its wide availability1 and because in the future we want to investigate the possibility of cross-lingual training. While the English treebank used in UD is rather small and non-standard, treebanks for other languages are often the typical and standard ones. In particular, we evaluate on Polish for which the UD project uses the only polish treebank \u201cSk\u0142adnica\u201d (S\u0301widzin\u0301ski and Wolin\u0301ski, 2010) and on Czech for which UD uses the large and standard \u201cPrague Treebank\u201d (Bejc\u030cek et al., 2013). Properties of the dataset are gathered in Table 1.\nModel selection We have conducted a hyperparameter search on the Polish treebank, which is the smallest one. We have used the Spearmint system to choose network layer sizes and regularization hyperparameters (Snoek et al., 2012). Based on the experiments on Polish we have chosen three network configurations that we have used for Czech and English. On all languages we have trained the networks on the provided training splits and performed model selection and early stopping on the development split.\nThe best overall network used 1050 filters in the reader subnetwork (50 \u00b7 k filters of length k for k = 1, 2, . . . , 6) whose outputs were projected into 512 dimensions and transformed by 3 Highway\n1Unfortunately we couldn\u2019t access the more typically used CoNLL \u201909 shared task data because the licensing webpage was not operational during the preparation of this manuscript.\nlayers. The tagging network consisted of 2 BiRNN layers each composed of 548 GRU units whose hidden states were aggregated using addition. Head words were pointed to by a small MLP with 384 tanh units in the hidden layer and the labeler used one hidden layer of 256 Maxout units, each using 2 pieces. For evaluation on the gold POS tags provided with the treebanks we have used a reader that embedded the base forms and each of the tag attributes into 192 dimensions.\nWe have added a projection layer between filters and highway units in the reader to speed the computations: we have discovered that generally increasing the number of filters is beneficial, however with a large number of filters the highway layers became a bottleneck (they are twice more expensive to evaluate than standard fully connected layers). Likewise, we have used a large dropout fraction in the BiRNN encoder and we have decided to add, rather than concatenate the hidden activations to reduce the input into the next recurrent layers.\nTraining procedure All non-recurrent weights were initialized from a normal Gaussian distribution with standard deviation set to 0.01, while the recurrent weights were orthogonalized. Initial states of recurrent layers were learned. We have used the AdaDelta (Zeiler, 2012) learning rule with parameters = 10\u22128 and \u03c1 = 0.95. We have routinely used an adaptive gradient clipping mechanism (Chorowski et al., 2014). All runs were early stopped based on the Unlabeled Attachment Score (UAS).\nThe primary training criterion was a linear combination of negative log-likelihoods of proper head word detection (taken with a weight of \u03b1s = 0.6) and dependency label prediction (taken with a weight of \u03b1l = 0.4). In experiments in which POS tags were used as auxiliary training targets we have split the POS tags into individual attributes and added their negative log-likelihood costs with a weight \u03b1t = 1.\nRegularization Polish and English treebanks are rather small and proper regularization was crucial to achieve optimum performance. We have obtained best results with Dropout (Srivastava et al., 2014). We have applied 20% Dropout just after the Reader subnetwork, 70% after every BiRNN layer in the tagger subnetwork (Pham et al., 2013) and 50% in the labeler. In contrast to Vinyals et al. (2014) we have not used data augmentations techniques."}, {"heading": "5 Results", "text": "Our parser has reached competitive performance with transition-based dependency parsers, as demonstrated in Table 2. For all datasets we report: the percentage of correctly labeled dependencies (LA), the percentage of correctly attached heads (Unlabeled Attachment Score, UAS), and the percentage of both correctly attached heads and labels (Labeled attachment Score, LAS) measured on the test set for the model that achieved the highest performance on the development set. The results were computed using the MaltEval tool."}, {"heading": "5.1 Baseline Models", "text": "We have used MaltParser v. 1.8.1 tuned with MaltOptimizer (Nivre et al., 2005; Ballesteros and Nivre, 2012) on all information available in UD treebanks (gold POS). This gave us an optimistic baseline, since during normal use POS tags will contain errors due to the tagger. This error has been analyzed on UD v. 1.0 by Tiedemann (2015). As an additional optimized baseline we include also results from Straka et al. (2015) that were reported on the same version of UD treebanks that we use."}, {"heading": "5.2 Neural Parser on Golden Tags", "text": "To compare our parser with the optimistic baseline, we have trained it on gold POS tags. We have observed that best results were obtained when the POS attributes were split and given to the network as several categorical inputs. On Czech and Polish the neural network improves the optimistic baseline error rates, while on English the results are comparable."}, {"heading": "5.3 Neural Parser Without POS Tags", "text": "In the next experiment we have evaluated the network without POS tag information. When trained on individual words treated as discrete entities, the performance of the parsing network has dropped significantly, which can be seen in Table 2. One solution, outlined by Vinyals et al. (2014) involves pre-training word embeddings on a large corpus and using them in the input look-up tables. However, we wanted to use the information present in the spelling of each word and decided to use the characterbased embedder by Kim et al. (2015). Intuitively, in morphologically rich languages such as Czech or Polish the spelling of a word conveys many hints about its grammatical function.\nWe have tested four variants of the networks: with and without an auxiliary training objective consisting of predicting POS tags and with the hard and soft attention in labelers (c.f. Section 2.3). We have established that on Polish for which has the smallest treebank multitask learning increased the UAS score when the POS tag prediction used hidden states of the penultimate BiRNN layer. On the larger datasets available on Czech and English the extra supervision added by predicting POS tags slightly decreases the results. However, on all languages the best setup involved multitask learning and soft attention.\nUsing hard attention is also beneficial. We interpret this fact as follows: with hard attention, the labeler always sees the annotations of the correct head and dependent words, while with soft attention the head annotation may refer to possibly many incorrect words chosen by the attention mechanism. With hard attention gradients from the labeler are backpropagated to the correct head word only, which helps training. On the other hand, with soft attention the gradients from the head nodes sometimes are backpropagated to incorrect locations. This adds noise during training, but possibly makes the labeler more robust to errors in localization of head words.\nWe can make the following observation based on Table 2: on all tested languages we can see that, according to our expectations, we have generally that NN with characters outerperforms NN with words. Czech and Polish belong to morphologically reach languages, and on these languages we can observe clear benefit from using POS-tags as a additional learning objective (the greater role of tags in Czech and Polish is also visible, when we look at the difference between versions with and without golden POS tags). Furthermore, when we don\u2019t use golden tags, for Polish and Czech our best algorithm achieves best UAS and LAS (for Polish this remains true even when compared with MaltParser trained on gold POS tags).\nDecoding algorithm The decoding algorithm has little impact on the parser\u2019s performance. We have investigated the attention outputs which show, for each word, probabilities assigned by the network to the locations of the head word. These probability distributions are very sharp, with virtually no ambiguities. The attention matrix for one sentence is shown in Figure 3. Therefore the greedy head attachment strategy works very well in practice.\nOn the three languages tested, about 98% of parses produced by the greedy strategy were correct trees, with a single ROOT and no cycles."}, {"heading": "6 Conclusions and Future Works", "text": "We have presented a dependency parser that is able to operate directly on characters, obviating the need for a traditional NLP pipeline. The parser is trained in an end-to-end manner, and has separate cost terms that pertain to label accuracy, head word localization and optionally POS tagging. On morphologically rich languages the parser is competitive with traditional transition-based solutions that use gold POS tag information, despite the fact that no hand-designed linguistic features are used and all information comes directly from the orthographic form of words.\nIn future work we plan to investigate the possibility of co-training parsers on multiple languages to improve the models on languages with very small corporas, such as Polish or Slovenian."}, {"heading": "7 Acknowledgments", "text": "The experiments were conducted using Theano (Bergstra et al., 2010; Bastien et al., 2012), Blocks and Fuel (van Merri\u00ebnboer et al., 2015) libraries.\nThe authors would like to acknowledge the support of the following agencies for research funding and computing support: National Science Center (Poland) grant Sonata 8 2014/15/D/ST6/04402, National Center for Research and Development (Poland) grant Audioscope (Applied Research Program, 3rd contest, submission no. 245755)."}], "references": [{"title": "Globally Normalized Transition-Based Neural Networks", "author": ["Daniel Andor", "Chris Alberti", "David Weiss"], "venue": "[cs],", "citeRegEx": "Andor et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Andor et al\\.", "year": 2016}, {"title": "Neural Machine Translation by Jointly Learning to Align and Translate", "author": ["Dzmitry Bahdanau", "Kyunghyun Cho", "Yoshua Bengio"], "venue": null, "citeRegEx": "Bahdanau et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Bahdanau et al\\.", "year": 2014}, {"title": "MaltOptimizer: an optimization tool for MaltParser. In Proceedings of the Demonstrations at the 13th Conference of the European Chapter of the Association for Computational Linguistics, pages 58\u201362", "author": ["Miguel Ballesteros", "Joakim Nivre"], "venue": "Association for Computational Linguistics,", "citeRegEx": "Ballesteros and Nivre.,? \\Q2012\\E", "shortCiteRegEx": "Ballesteros and Nivre.", "year": 2012}, {"title": "Theano: new features and speed improvements. Deep Learning and Unsupervised Feature Learning", "author": ["Fr\u00e9d\u00e9ric Bastien", "Pascal Lamblin", "Razvan Pascanu"], "venue": null, "citeRegEx": "Bastien et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Bastien et al\\.", "year": 2012}, {"title": "Theano: a CPU and GPU math expression compiler", "author": ["James Bergstra", "Olivier Breuleux", "Fr\u00e9d\u00e9ric Bastien"], "venue": "In Proc. SciPy,", "citeRegEx": "Bergstra et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Bergstra et al\\.", "year": 2010}, {"title": "A fast and accurate dependency parser using neural networks", "author": ["Danqi Chen", "Christopher D Manning"], "venue": "In Empirical Methods in Natural Language Processing (EMNLP),", "citeRegEx": "Chen and Manning.,? \\Q2014\\E", "shortCiteRegEx": "Chen and Manning.", "year": 2014}, {"title": "On the Properties of Neural Machine Translation: Encoder-Decoder Approaches", "author": ["Kyunghyun Cho", "Bart van Merrienboer", "Dzmitry Bahdanau", "Yoshua Bengio"], "venue": null, "citeRegEx": "Cho et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Cho et al\\.", "year": 2014}, {"title": "End-to-end Continuous Speech Recognition using Attention-based Recurrent NN: First Results", "author": ["Jan Chorowski", "Dzmitry Bahdanau", "Kyunghyun Cho", "Yoshua Bengio"], "venue": null, "citeRegEx": "Chorowski et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Chorowski et al\\.", "year": 2014}, {"title": "Transition-based dependency parsing with stack long short-term memory", "author": ["Chris Dyer", "Miguel Ballesteros", "Wang Ling", "Austin Matthews", "Noah A. Smith"], "venue": "arXiv preprint arXiv:1505.08075,", "citeRegEx": "Dyer et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Dyer et al\\.", "year": 2015}, {"title": "Optimim Branchings", "author": ["Jack Edmonds"], "venue": "JOURNAL OF RESEARCH of the National Bureau of Standards - B.,", "citeRegEx": "Edmonds.,? \\Q1966\\E", "shortCiteRegEx": "Edmonds.", "year": 1966}, {"title": "Exploring the Limits of Language Modeling", "author": ["Rafal Jozefowicz", "Oriol Vinyals", "Mike Schuster", "Noam Shazeer", "Yonghui Wu"], "venue": "[cs],", "citeRegEx": "Jozefowicz et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Jozefowicz et al\\.", "year": 2016}, {"title": "Character-aware neural language models", "author": ["Yoon Kim", "Yacine Jernite", "David Sontag", "Alexander M. Rush"], "venue": "arXiv preprint arXiv:1508.06615,", "citeRegEx": "Kim et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Kim et al\\.", "year": 2015}, {"title": "Simple and Accurate Dependency Parsing Using Bidirectional LSTM Feature Representations", "author": ["Eliyahu Kiperwasser", "Yoav Goldberg"], "venue": "[cs],", "citeRegEx": "Kiperwasser and Goldberg.,? \\Q2016\\E", "shortCiteRegEx": "Kiperwasser and Goldberg.", "year": 2016}, {"title": "Finding function in form: Compositional character models for open vocabulary word representation", "author": ["Wang Ling", "Tiago Lu\u00eds", "Lu\u00eds Marujo"], "venue": "arXiv preprint arXiv:1508.02096,", "citeRegEx": "Ling et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Ling et al\\.", "year": 2015}, {"title": "Efficient Estimation of Word Representations in Vector Space", "author": ["Tomas Mikolov", "Kai Chen", "Greg Corrado", "Jeffrey Dean"], "venue": "[cs],", "citeRegEx": "Mikolov et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Mikolov et al\\.", "year": 2013}, {"title": "Recurrent neural network based language model", "author": ["Tom\u00e1s Mikolov", "Martin Karafi\u00e1t", "Luk\u00e1s Burget", "Jan Cernocky", "Sanjeev Khudanpur"], "venue": null, "citeRegEx": "Mikolov et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Mikolov et al\\.", "year": 2010}, {"title": "Algorithms for Deterministic Incremental Dependency Parsing", "author": ["Joakim Nivre"], "venue": "Comput. Linguist.,", "citeRegEx": "Nivre.,? \\Q2008\\E", "shortCiteRegEx": "Nivre.", "year": 2008}, {"title": "MaltParser: A language-independent system for data-driven dependency parsing", "author": ["Joakim Nivre", "Johan Hall", "Jens Nilsson"], "venue": "Natural Language Engineering,", "citeRegEx": "Nivre et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Nivre et al\\.", "year": 2005}, {"title": "Universal Dependencies", "author": ["Joakim Nivre", "\u017deljko Agi\u0107", "Maria Jesus Aranzabe"], "venue": "http://universaldependencies.github.io/docs/,", "citeRegEx": "Nivre et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Nivre et al\\.", "year": 2015}, {"title": "An effective neural network model for graph-based dependency parsing", "author": ["Wenzhe Pei", "Tao Ge", "Baobao Chang"], "venue": "In Proc. of ACL,", "citeRegEx": "Pei et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Pei et al\\.", "year": 2015}, {"title": "Dropout improves Recurrent Neural Networks for Handwriting Recognition", "author": ["Vu Pham", "Th\u00e9odore Bluche", "Christopher Kermorvant", "J\u00e9r\u00f4me Louradour"], "venue": "[cs],", "citeRegEx": "Pham et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Pham et al\\.", "year": 2013}, {"title": "Bidirectional recurrent neural networks", "author": ["M. Schuster", "Kuldip K. Paliwal"], "venue": "IEEE Transactions on Signal Processing,", "citeRegEx": "Schuster and Paliwal.,? \\Q1997\\E", "shortCiteRegEx": "Schuster and Paliwal.", "year": 1997}, {"title": "Practical bayesian optimization of machine learning algorithms", "author": ["Jasper Snoek", "Hugo Larochelle", "Ryan P. Adams"], "venue": "In Advances in neural information processing systems,", "citeRegEx": "Snoek et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Snoek et al\\.", "year": 2012}, {"title": "Parsing natural scenes and natural language with recursive neural networks", "author": ["Richard Socher", "Cliff C. Lin", "Chris Manning", "Andrew Y. Ng"], "venue": "In Proceedings of the 28th International Conference on Machine Learning", "citeRegEx": "Socher et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Socher et al\\.", "year": 2011}, {"title": "Dropout: A Simple Way to Prevent Neural Networks from Overfitting", "author": ["Nitish Srivastava", "Geoffrey Hinton", "Alex Krizhevsky", "Ilya Sutskever", "Ruslan Salakhutdinov"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Srivastava et al\\.,? \\Q1929\\E", "shortCiteRegEx": "Srivastava et al\\.", "year": 1929}, {"title": "Parsing universal dependency treebanks using neural networks and search-based oracle", "author": ["Milan Straka", "Jan Haji\u010d", "Jana Strakov\u00e1", "jr. Jan Haji\u010d"], "venue": "In 14th International Workshop on Treebanks and Linguistic Theories (TLT", "citeRegEx": "Straka et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Straka et al\\.", "year": 2015}, {"title": "End-To-End Memory Networks", "author": ["Sainbayar Sukhbaatar", "Arthur Szlam", "Jason Weston", "Rob Fergus"], "venue": "[cs],", "citeRegEx": "Sukhbaatar et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Sukhbaatar et al\\.", "year": 2015}, {"title": "Sequence to Sequence Learning with Neural Networks", "author": ["Ilya Sutskever", "Oriol Vinyals", "Quoc V. Le"], "venue": "arXiv preprint arXiv:1409.3215,", "citeRegEx": "Sutskever et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Sutskever et al\\.", "year": 2014}, {"title": "Cross-Lingual Dependency Parsing with Universal Dependencies and Predicted PoS Labels", "author": ["J\u00f6rg Tiedemann"], "venue": "Depling", "citeRegEx": "Tiedemann.,? \\Q2015\\E", "shortCiteRegEx": "Tiedemann.", "year": 2015}, {"title": "Blocks and fuel: Frameworks for deep learning", "author": ["Bart van Merri\u00ebnboer", "Dzmitry Bahdanau", "Vincent Dumoulin"], "venue": null, "citeRegEx": "Merri\u00ebnboer et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Merri\u00ebnboer et al\\.", "year": 2015}, {"title": "Grammar as a Foreign Language", "author": ["Oriol Vinyals", "Lukasz Kaiser", "Terry Koo"], "venue": null, "citeRegEx": "Vinyals et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Vinyals et al\\.", "year": 2014}, {"title": "Show, Attend and Tell: Neural Image Caption Generation with Visual Attention", "author": ["Kelvin Xu", "Jimmy Ba", "Ryan Kiros"], "venue": "[cs],", "citeRegEx": "Xu et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Xu et al\\.", "year": 2015}, {"title": "ADADELTA: An Adaptive Learning Rate Method", "author": ["Matthew D. Zeiler"], "venue": "[cs],", "citeRegEx": "Zeiler.,? \\Q2012\\E", "shortCiteRegEx": "Zeiler.", "year": 2012}, {"title": "Towards a bank of constituent parse trees for Polish", "author": ["Marek \u015awidzi\u0144ski", "Marcin Woli\u0144ski"], "venue": "In Text, Speech and Dialogue,", "citeRegEx": "\u015awidzi\u0144ski and Woli\u0144ski.,? \\Q2010\\E", "shortCiteRegEx": "\u015awidzi\u0144ski and Woli\u0144ski.", "year": 2010}], "referenceMentions": [{"referenceID": 15, "context": "Deep learning methods have led to many breakthrough in NLP tasks, such as language modeling (Mikolov et al., 2010), machine translation (Bahdanau et al.", "startOffset": 92, "endOffset": 114}, {"referenceID": 1, "context": ", 2010), machine translation (Bahdanau et al., 2014; Sutskever et al., 2014), caption generation (Xu et al.", "startOffset": 29, "endOffset": 76}, {"referenceID": 27, "context": ", 2010), machine translation (Bahdanau et al., 2014; Sutskever et al., 2014), caption generation (Xu et al.", "startOffset": 29, "endOffset": 76}, {"referenceID": 31, "context": ", 2014), caption generation (Xu et al., 2015), question answering (Sukhbaatar et al.", "startOffset": 28, "endOffset": 45}, {"referenceID": 26, "context": ", 2015), question answering (Sukhbaatar et al., 2015), speech regognition, POS-taggers and so on.", "startOffset": 28, "endOffset": 53}, {"referenceID": 11, "context": "First, the reader subnetwork finds word embeddings based on their orthographic representations using convolutional and highway layers (Kim et al., 2015; Srivastava et al., 2015).", "startOffset": 134, "endOffset": 177}, {"referenceID": 21, "context": "Second, a bidirectional recurrent tagger subnetwork puts the individual words into their contexts (Schuster and Paliwal, 1997).", "startOffset": 98, "endOffset": 126}, {"referenceID": 1, "context": "Finally, the parser subnetwork uses the soft-attention mechanism to point each word to its head (Vinyals et al., 2015; Bahdanau et al., 2014).", "startOffset": 96, "endOffset": 141}, {"referenceID": 11, "context": "Following Kim et al. (2015), we use a convolutional filterbank followed by a few layers of nonlinear transformations.", "startOffset": 10, "endOffset": 28}, {"referenceID": 21, "context": "It uses bidirectional recurrent layers (BiRNN) to put them into a broader context (Schuster and Paliwal, 1997).", "startOffset": 82, "endOffset": 110}, {"referenceID": 6, "context": "Specifically, we use GRU recurrent units (Cho et al., 2014) to scan the sequence forward and backward.", "startOffset": 41, "endOffset": 59}, {"referenceID": 9, "context": "5% of the parses obtained by this procedure have cycles, so using Chu-Liu-Edmonds (Edmonds, 1966) maximum spanning arborescence algorithm (which deletes cycles) gives only a subtle improvement and is not presented in the Table 2.", "startOffset": 82, "endOffset": 97}, {"referenceID": 16, "context": "action (such as moving the word to stack or add a dependency between current word and the word on a stack) looking at some features (Nivre, 2008).", "startOffset": 132, "endOffset": 145}, {"referenceID": 5, "context": "Recently, deep neural networks were used wih a great success in dependency parsing, both transition (Chen and Manning, 2014; Dyer et al., 2015; Kiperwasser and Goldberg, 2016; Andor et al., 2016) and graph (Pei et al.", "startOffset": 100, "endOffset": 195}, {"referenceID": 8, "context": "Recently, deep neural networks were used wih a great success in dependency parsing, both transition (Chen and Manning, 2014; Dyer et al., 2015; Kiperwasser and Goldberg, 2016; Andor et al., 2016) and graph (Pei et al.", "startOffset": 100, "endOffset": 195}, {"referenceID": 12, "context": "Recently, deep neural networks were used wih a great success in dependency parsing, both transition (Chen and Manning, 2014; Dyer et al., 2015; Kiperwasser and Goldberg, 2016; Andor et al., 2016) and graph (Pei et al.", "startOffset": 100, "endOffset": 195}, {"referenceID": 0, "context": "Recently, deep neural networks were used wih a great success in dependency parsing, both transition (Chen and Manning, 2014; Dyer et al., 2015; Kiperwasser and Goldberg, 2016; Andor et al., 2016) and graph (Pei et al.", "startOffset": 100, "endOffset": 195}, {"referenceID": 19, "context": ", 2016) and graph (Pei et al., 2015) based.", "startOffset": 18, "endOffset": 36}, {"referenceID": 12, "context": "Our parser is most similar to the graph-based variant of (Kiperwasser and Goldberg, 2016).", "startOffset": 57, "endOffset": 89}, {"referenceID": 14, "context": "The parser accessed the source sentence through word embeddings, which were initialized with Word2Vec (Mikolov et al., 2013) and adapted during parser training.", "startOffset": 102, "endOffset": 124}, {"referenceID": 0, "context": ", 2015; Kiperwasser and Goldberg, 2016; Andor et al., 2016) and graph (Pei et al., 2015) based. Our parser is most similar to the graph-based variant of (Kiperwasser and Goldberg, 2016). However, we replace the POS tagger with our reader subnetwork thus reducing the need for feature engineering, which is an important aspect of parser construction which requires knowledge of linguistics. Powerful learning techniques reduce the burden of this somewhat language-specific work. Our parsing network brings together ideas from many recent contributions. Ling et al. (2015) successfully applied character-based word embeddings computed with small BiRNNs (another possible implementation of our \u201creading\u201d subnetwork) to POS-tagging and language modeling with recurrent networks.", "startOffset": 40, "endOffset": 571}, {"referenceID": 0, "context": ", 2015; Kiperwasser and Goldberg, 2016; Andor et al., 2016) and graph (Pei et al., 2015) based. Our parser is most similar to the graph-based variant of (Kiperwasser and Goldberg, 2016). However, we replace the POS tagger with our reader subnetwork thus reducing the need for feature engineering, which is an important aspect of parser construction which requires knowledge of linguistics. Powerful learning techniques reduce the burden of this somewhat language-specific work. Our parsing network brings together ideas from many recent contributions. Ling et al. (2015) successfully applied character-based word embeddings computed with small BiRNNs (another possible implementation of our \u201creading\u201d subnetwork) to POS-tagging and language modeling with recurrent networks. The character-based word embeddings that we have used were described by Kim et al. (2015) and extensively analyzed by Jozefowicz et al.", "startOffset": 40, "endOffset": 865}, {"referenceID": 0, "context": ", 2015; Kiperwasser and Goldberg, 2016; Andor et al., 2016) and graph (Pei et al., 2015) based. Our parser is most similar to the graph-based variant of (Kiperwasser and Goldberg, 2016). However, we replace the POS tagger with our reader subnetwork thus reducing the need for feature engineering, which is an important aspect of parser construction which requires knowledge of linguistics. Powerful learning techniques reduce the burden of this somewhat language-specific work. Our parsing network brings together ideas from many recent contributions. Ling et al. (2015) successfully applied character-based word embeddings computed with small BiRNNs (another possible implementation of our \u201creading\u201d subnetwork) to POS-tagging and language modeling with recurrent networks. The character-based word embeddings that we have used were described by Kim et al. (2015) and extensively analyzed by Jozefowicz et al. (2016). A purely neural constituency parser was shown by Socher et al.", "startOffset": 40, "endOffset": 918}, {"referenceID": 0, "context": ", 2015; Kiperwasser and Goldberg, 2016; Andor et al., 2016) and graph (Pei et al., 2015) based. Our parser is most similar to the graph-based variant of (Kiperwasser and Goldberg, 2016). However, we replace the POS tagger with our reader subnetwork thus reducing the need for feature engineering, which is an important aspect of parser construction which requires knowledge of linguistics. Powerful learning techniques reduce the burden of this somewhat language-specific work. Our parsing network brings together ideas from many recent contributions. Ling et al. (2015) successfully applied character-based word embeddings computed with small BiRNNs (another possible implementation of our \u201creading\u201d subnetwork) to POS-tagging and language modeling with recurrent networks. The character-based word embeddings that we have used were described by Kim et al. (2015) and extensively analyzed by Jozefowicz et al. (2016). A purely neural constituency parser was shown by Socher et al. (2011). It built a parse tree by repeatedly joining words or subtrees using a recursive network.", "startOffset": 40, "endOffset": 989}, {"referenceID": 0, "context": ", 2015; Kiperwasser and Goldberg, 2016; Andor et al., 2016) and graph (Pei et al., 2015) based. Our parser is most similar to the graph-based variant of (Kiperwasser and Goldberg, 2016). However, we replace the POS tagger with our reader subnetwork thus reducing the need for feature engineering, which is an important aspect of parser construction which requires knowledge of linguistics. Powerful learning techniques reduce the burden of this somewhat language-specific work. Our parsing network brings together ideas from many recent contributions. Ling et al. (2015) successfully applied character-based word embeddings computed with small BiRNNs (another possible implementation of our \u201creading\u201d subnetwork) to POS-tagging and language modeling with recurrent networks. The character-based word embeddings that we have used were described by Kim et al. (2015) and extensively analyzed by Jozefowicz et al. (2016). A purely neural constituency parser was shown by Socher et al. (2011). It built a parse tree by repeatedly joining words or subtrees using a recursive network. Later, Vinyals et al. (2014) have shown that good constituency parsers can be created by learning to \u201ctranslate\u201d between a given sentence and the linearization of its parse tree.", "startOffset": 40, "endOffset": 1108}, {"referenceID": 18, "context": "2 dataset (Nivre et al., 2015).", "startOffset": 10, "endOffset": 30}, {"referenceID": 33, "context": "In particular, we evaluate on Polish for which the UD project uses the only polish treebank \u201cSk\u0142adnica\u201d (\u015awidzi\u0144ski and Woli\u0144ski, 2010) and on Czech for which UD uses the large and standard \u201cPrague Treebank\u201d (Bej\u010dek et al.", "startOffset": 104, "endOffset": 135}, {"referenceID": 22, "context": "We have used the Spearmint system to choose network layer sizes and regularization hyperparameters (Snoek et al., 2012).", "startOffset": 99, "endOffset": 119}, {"referenceID": 25, "context": "8 (Straka et al., 2015) - 87.", "startOffset": 2, "endOffset": 23}, {"referenceID": 28, "context": "5 (Tiedemann, 2015) - - 85.", "startOffset": 2, "endOffset": 19}, {"referenceID": 28, "context": "3 Predicted POS tags or no POS tags (Tiedemann, 2015) - - 81.", "startOffset": 36, "endOffset": 53}, {"referenceID": 32, "context": "We have used the AdaDelta (Zeiler, 2012) learning rule with parameters = 10\u22128 and \u03c1 = 0.", "startOffset": 26, "endOffset": 40}, {"referenceID": 7, "context": "We have routinely used an adaptive gradient clipping mechanism (Chorowski et al., 2014).", "startOffset": 63, "endOffset": 87}, {"referenceID": 20, "context": "We have applied 20% Dropout just after the Reader subnetwork, 70% after every BiRNN layer in the tagger subnetwork (Pham et al., 2013) and 50% in the labeler.", "startOffset": 115, "endOffset": 134}, {"referenceID": 20, "context": "We have applied 20% Dropout just after the Reader subnetwork, 70% after every BiRNN layer in the tagger subnetwork (Pham et al., 2013) and 50% in the labeler. In contrast to Vinyals et al. (2014) we have not used data augmentations techniques.", "startOffset": 116, "endOffset": 196}, {"referenceID": 17, "context": "1 tuned with MaltOptimizer (Nivre et al., 2005; Ballesteros and Nivre, 2012) on all information available in UD treebanks (gold POS).", "startOffset": 27, "endOffset": 76}, {"referenceID": 2, "context": "1 tuned with MaltOptimizer (Nivre et al., 2005; Ballesteros and Nivre, 2012) on all information available in UD treebanks (gold POS).", "startOffset": 27, "endOffset": 76}, {"referenceID": 2, "context": ", 2005; Ballesteros and Nivre, 2012) on all information available in UD treebanks (gold POS). This gave us an optimistic baseline, since during normal use POS tags will contain errors due to the tagger. This error has been analyzed on UD v. 1.0 by Tiedemann (2015). As an additional optimized baseline we include also results from Straka et al.", "startOffset": 8, "endOffset": 265}, {"referenceID": 2, "context": ", 2005; Ballesteros and Nivre, 2012) on all information available in UD treebanks (gold POS). This gave us an optimistic baseline, since during normal use POS tags will contain errors due to the tagger. This error has been analyzed on UD v. 1.0 by Tiedemann (2015). As an additional optimized baseline we include also results from Straka et al. (2015) that were reported on the same version of UD treebanks that we use.", "startOffset": 8, "endOffset": 352}, {"referenceID": 29, "context": "One solution, outlined by Vinyals et al. (2014) involves pre-training word embeddings on a large corpus and using them in the input look-up tables.", "startOffset": 26, "endOffset": 48}, {"referenceID": 11, "context": "However, we wanted to use the information present in the spelling of each word and decided to use the characterbased embedder by Kim et al. (2015). Intuitively, in morphologically rich languages such as Czech or Polish the spelling of a word conveys many hints about its grammatical function.", "startOffset": 129, "endOffset": 147}], "year": 2017, "abstractText": "We present a dependency parser implemented as a single deep neural network that reads orthographic representations of words and directly generates dependencies and their labels. Unlike typical approaches to parsing, the model doesn\u2019t require part-of-speech (POS) tagging of the sentences. With proper regularization and additional supervision achieved with multitask learning we reach state-of-the-art performance on Slavic languages from the Universal Dependencies treebank: with no linguistic features other than characters, our parser is as accurate as a transitionbased system trained on perfect POS tags.", "creator": "LaTeX with hyperref package"}}}