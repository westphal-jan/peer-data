{"id": "1609.03628", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "12-Sep-2016", "title": "Co-active Learning to Adapt Humanoid Movement for Manipulation", "abstract": "In this paper we address the problem of robot movement adaptation under various environmental constraints interactively. Motion primitives are generally adopted to generate target motion from demonstrations. However, their generalization capability is weak while facing novel environments. In practice, this has been achieved by combining two basic methods. We used an inter-temporal gradient, where linear-gradient interpolation (m2) is applied to real-world data with a linear gradients in the first step of the gradient. We chose a nonlinear gradient to produce an invariant vector. In this paper we use the first phase of gradient interpolation (m2) to evaluate the effect of the gradient on real-world data in the third step of the gradient. We applied a time function, m2, to produce a time function. The time function is a nonlinear gradient, based on its initial time from 1.0 to 1.0. For a given time, we computed the linear gradient at the beginning of the gradient in the second step of the gradient at a time of 0.25. The second step of gradient interpolation is based on the first phase of gradient interpolation (m2) and the first phase of gradient interpolation (m2) for a given time. For the second phase of gradient interpolation, we computed the linear gradient at the beginning of the gradient in the second step of the gradient at a time of 0.25.\n\n\n\nThe main effect of m2 is to generate a time function. The gradient is a nonlinear gradient, based on its initial time from 1.0 to 1.0. We measured the linear gradient at the beginning of the gradient in the second step of the gradient at a time of 0.25. The second step of gradient interpolation is based on the first phase of gradient interpolation (m2) and the second phase of gradient interpolation (m2) for a given time. The second step of gradient interpolation is based on the first phase of gradient interpolation (m2) and the second phase of gradient interpolation (m2) for a given time. The first step of gradient interpolation is based on the first phase of gradient interpolation (m2) and the second phase of gradient interpolation (m2) for a given time.\n\nOur algorithm is simple, in principle. In the first step of gradient interpolation, we choose a zero-dimensional vector (MM), an object that is either a small-dimensional object or a large-dimensional object.", "histories": [["v1", "Mon, 12 Sep 2016 22:57:37 GMT  (8120kb,D)", "http://arxiv.org/abs/1609.03628v1", null]], "reviews": [], "SUBJECTS": "cs.RO cs.LG cs.SY", "authors": ["ren mao", "john s baras", "yezhou yang", "cornelia fermuller"], "accepted": false, "id": "1609.03628"}, "pdf": {"name": "1609.03628.pdf", "metadata": {"source": "CRF", "title": "Co-active Learning to Adapt Humanoid Movement for Manipulation", "authors": ["Ren Mao", "John S. Baras", "Yezhou Yang", "Cornelia Ferm\u00fcller"], "emails": [], "sections": [{"heading": null, "text": "I. Introduction\nTrajectories learning from human demonstrations has been studied in the field of Robotics for decades due to its wide range of applications, in both industrial and domestic scenarios. Among the various approaches, Motion Primitives (MPs) aims to parameterize observed human motion and then reproduce it, given different initial and target states. However, it is widely known that general MPs methods, such as Dynamic Movement Primitives (DMPs) [1], have limited capability to generalize towards novel environments involving other constraints. Moreover, standard MPs learning method ignores user preferences of the tasks and the environments. For real world humanoid applications, a practical robot movement learning framework needs to take user preferences and environment constraints into consideration.\nLet\u2019s start from a common example, that a human user teaches a humanoid how to transfer a bottle from different start and end states. Using off-the-shelf approach, the robot is able to learn the motion by acquiring MPs from the demonstrated trajectories and apply them to generate new trajectories given different initial and target states. However, solely following the generated trajectories may fail if the environment has slight alteration, such as having a bowl blocking the trajectory as illustrated in Fig. 2(a). Here we assume that the robot can only be able to receive these constraints during task execution phase (testing phase), and these constrains are not presented during the training phase. In this work, we propose an optimization based framework to\n1R. Mao and J. Baras are with the Department of Electrical and Computer Engineering and the ISR, 2Y. Yang and C. Fermu\u0308ller are with the Department of Computer Science and the UMIACS, University of Maryland, College Park, Maryland 20742, USA. {neroam, baras} at umd.edu, yzyang at cs.umd.edu and fer at umiacs.umd.edu.\nadapt trained movements for novel environments. The first goal of our system is to generate adapted trajectories, as shown in Fig. 2(b), that can: 1) follow the demonstrated trajectories for the purpose of preserving movement patterns, and 2) fulfill novel constraints perceived from the environment during testing phase.\nMoreover, novel environment constraints perceived during testing phase could be more complicated than just obstacles. Following the example mentioned before, this time let\u2019s consider a situation where the target bottle is leaking. Ideally an intelligent robot that understands the situation should avoid moving the bottle over the bowl, but follows the movement path around it. Even though we could adjust the objective function during optimization for movement adaptation, what if in another scenario the robot is asked to transfer a knife while avoiding obstacles above them to prevent potential scratches? Such constraints are not only associated with the task context, i.e, leaking bottle or knife as the manipulated object, but also associated with user\u2019s preference, i.e, avoiding the bowl with a certain manner. Therefore, a human-in-the-loop on-line adaptation system is necessary to generate manipulation trajectories for different preferences. In the optimization framework presented in this paper for movement adaptation, we first treat the reward weights as the adjustable parameters to alter the quality of the trajectory. Then based on user feedback, the framework learns the preferred behavior, that fulfills constraints, by updating the reward weights. Therefore, the learned behavior can be generalized towards different situations where similar constraints are encountered. As illustrated in Fig. 2(c), after a ar X iv :1 60 9.\n03 62\n8v 1\n[ cs\n.R O\n] 1\n2 Se\np 20\n16\nfew iterations of on-line learning, the robot is able to generate an adapted trajectory according to the learned preferences.\nThis paper proposes an approach for interactively learning movement adaptation for manipulation tasks. Fig. 1 illustrates the proposed system. The main contributions of this work are: 1) A system for robot to generalize movement learned from demonstrations to fulfill constraints perceived from novel environment. It is able to adapt trajectories for various situations according to user preferences; 2) An approach for robot learning to adapt trajectories by updating reward weights based on users\u2019 feedback. The user thus can co-actively train the robot in-the-loop by demonstrating desired trajectories; 3) An implementation of the optimization schema to adapt transferring skill considering obstacles and different manners. We validate the implementation on a humanoid platform (Baxter) and the experimental results support our claims.\nII. RelatedWork\nVarious approaches have been proposed to enable robot learning manipulation movements. Among them, imitation learning [2] focuses on mimicking human demonstrations, while learning from demonstration (LfD) techniques [3] are applicable. However, with these approaches, the robot could only reproduce learned movement in a similar environment. To deal with novel environments, extended approaches [4] augmented the trajectory generation with additional cost terms or different objective function as a criterion of trajectories\u2019 quality. The criterion is based on human experts\u2019 prior knowledge about the task or environment before execution phase. Then, the motion is generalized with these predefined constraints in similar situations. These approaches do not consider various user preferences. Here, we present another layer of exploration and learning to adapt the trained movement considering novel environment constraints, such as observed obstacles and task preferences.\nApproaches [5] for encoding trajectory as motion primitives have been proposed for various forms of generalization and modulation, such as Gaussian mixture regression and Gaussian mixture models [6], [7]. In [8], a mixture model was used to estimate the entire movement skill from several sample trajectories. Another school of approaches derive from Hidden Markov models [9]. One popular representation to encode motion from demonstrated trajectories\nis Dynamic Movement Primitives (DMPs), as introduced in [1]. It consists of differential equations with well-defined attractor properties and a non-linear learnable component that allows modeling of almost arbitrarily complex motion. Recently, Probabilistic Movement Primitives (ProMPs) [10] was proposed as an alternative representation in probabilistic formulation. It learns a trajectory distribution from multiple demonstrations and modulates the movement by conditioning on desired target states. Incorporating the variance of demonstrations, ProMPs approach handles noise from different demonstrations and provides increased flexibility for reproducing movement. However, all these approaches hardly deal with novel environments such as involving different obstacles. In our work, we first train our robot using ProMPs and then generalize these trained motion primitives to newly introduced environment constraints.\nIn order to enable MPs to adapt to novel environments with obstacles [11], [12], Kober et al. [4] proposed an augmented version of DMPs which incorporates perceptual coupling to an external variable. They firstly learned the initial dynamic models by standard imitation learning and subsequently used a reinforcement learning method for selfimprovement. Ghalamzan et al. [13] proposed a three-tiered approach for robot learning from demonstrations that can generalize noisy task demonstrations to a new target state and to an environment with obstacles. They encoded the nominal path generated from a Gaussian Mixture Model with DMPs and generated trajectory for a new target state. Then they adapted the DMP-generated trajectory to avoid obstacles by formulating an optimal control problem regarding the reward function learned from demonstrations by inverse optimal control. This approach allows an non-expert user to teach a robot the desired response to different objects but requires offline training in the environment involving those obstacles for learning the reward function. However, in real world scenarios, the human users often have different preferences for trajectories generation according to various environments and tasks, while it is extremely challenging for them to provide the optimal trajectories in every situation. Instead, in our approach, the human users can interactively provide sub-optimal suggestions on how to improve the trajectory and the robot learns the preference for different constraints, and also incorporate it in generating more applicable trajectories.\nUser preferences over robot\u2019s trajectories have been stud-\nied in the field of human robot interaction (HRI). Sisbot et al. [14] proposed to model user specified preferences as constraints on the distance of the robot from the user, the visibility of the robot and the users arm comfort. Then a path planner fulfilling such user preferences is provided. Ashesh Jain et al. [15] proposed a co-active learning method to learn user preferences over generated trajectories for manipulation tasks by iteratively taking user sub-optimal feedback, thereafter the optimal trajectory was selected based on the learned reward function. In our work, we adopt the co-active learning paradigm and further propose a reward formulation to model user preferences over constraints for movement generation. Then we integrate it with movement adaptation through optimization based planning.\nIII. Co-active Learning forMovement Generalization\nFor the problem of robot learning from demonstrations [3], a common practice is to offline learn the skills by encoding the trajectories with movement patterns such as DMPs [16]. They can be then, during the testing phase, used to generalize the movement to novel situations with slight alterations, such as different initial and target states. Nevertheless, this generalization capability does not apply to novel environments with different obstacles or to a new task contexts with a variety of manipulated objects. In this paper, we propose a complementary framework for generalizing movement skills, which are offline learned from demonstrations, to novel situations, and in addition incorporate on-line learning preferences of how to generalize from human\u2019s feedback coactively.\nWhile facing a novel situation, the robot is given a manipulation task context xc that describes the environment, the objects and any other task-related information. It could compute an imitation movement trajectory yD by generalizing offline learned skills to new initial and target states. Such a trajectory can be executed if the new environment does not have obstacles and there is no other constraints inherited from the task.\nTo further generalize learned movement skills to more challenging situations, the robot has to generate an adapted trajectory y based on the task contexts xc and the computed imitation trajectory yD. Here we use a reward function f \u2217(y,xc,yD) to reflect how much reward the adapted trajectory y can achieve for different contexts. Therefore, we can adapt the movement by solving an optimal control problem which outputs an adapted trajectory by maximizing the reward function f \u2217. The reward function consists of a Imitation Reward fD describing the tendency to follow the imitation trajectory yD, a Control Reward fC describing the smoothness of executing the adapted trajectory y and a Response Reward fE describing the expected response given the environment. Although this reward function can be recovered from demonstrations by Inverse Optimal Control, as [13] suggests, it assumes that demonstrations are from experts, which bears an oracle reward function. In fact, it is common for non-expert users to provide non-optimal trajectories in practice. Also, [13] requires the manipulated\nobjects or obstacles existing during these demonstrations and is hard to update the learned reward function online when the robot is facing situations that involve new objects. To learn the reward function which controls how the robot adapts trajectories under new contexts, we applied a co-active learning technique [15] in which the user only corrects the robot by providing an improved trajectory y\u0304 and then the robot updates the parameter w of f (\u00b7;w) based on user\u2019s feedback. It is worth to note that this feedback only indicates f \u2217(y\u0304,xc,yD) > f \u2217(y,xc,yD), and y\u0304 may be non-optimal trajectories. With iterations of improvement, the robot could learn a function that approximates the oracle f \u2217(\u00b7) tightly.\nIV. Our System\nOverall, after the robot has offline learned the movement skill from demonstrations, when facing a different task context xc in a novel environment, the testing phase includes three stages: 1) Movement Imitation, which computes an imitation trajectory yD by generalizing demonstrated movement to new initial and target states; 2) Movement Adaptation, which generates an adapted trajectory y under new task and environment contexts by maximizing the given reward function; 3) Rewards Learning, which updates the parameters of estimated reward function according to user\u2019s feedback through co-active learning. Fig. 1 demonstrates our proposed framework. In the following sections, we entail and formulate each stage."}, {"heading": "A. Movement Imitation", "text": "At the beginning, our system offline learns movement skill in an environment without obstacle or other constraints. In this work, we adopt the Probabilistic Movement Primitives (ProMPs) [10] for offline learning and movement imitation. It obtains a distribution over trajectories from multiple demonstrations, which captures the variations, and can be easily generalized to new initial and target states while imitating the movement.\nTo be specific, we consider that a robot\u2019s end-effector has d degrees of freedom (DOF) along with its arm, with its state denoted as y(t) = [y1(t), . . . , yd(t)]T . The trajectory of the robot\u2019s end effector is represented as a sequence T = {y(t)}t=0,...,T . We model each dimension i of y(t) using linear regression with n Gaussian time-dependent basis functions \u03c8 and a n-dimensional weight vectors wi as\nyi(t) = \u03c8(t)T wi + y, (1)\nwhere y \u223c N(0, \u03c32y) denotes zero-mean i.i.d. Gaussian noise. With the underlying weight vectors w = [wT1 , . . . ,w T d ]\nT , the probability of observing a trajectory T can be given by\np(T |w) = \u220f\nt\np(y(t)|w) = \u220f\nt\nN(y(t)|\u03a8(t)Tw,\u03a3y) (2)\nwhere \u03a8(t) = diag( d\ufe37 \ufe38\ufe38 \ufe37 \u03c8(t), . . . , \u03c8(t)) and \u03a3y = \u03c32yId\u00d7d.\n1) Learning from Demonstrations: For each demonstration, the trajectory can be easily represented by a weight vector w which has fewer dimensions than the number of time steps. To capture trajectory variations from multiple demonstrations of the movement, a Gaussian distribution p(w;\u03b8) = N(w|\u00b5w,\u03a3w) over the weights w is estimated. Therefore, the distribution of the trajectory p(T |w) can be represented as\np(T ;\u03b8) = \u222b p(T |w)p(w;\u03b8)dw (3)\n= \u220f\nt\nN(y(t)|\u03a8(t)T\u00b5w,\u03a8(t)T\u03a3w\u03a8(t)T + \u03a3y) (4)\nWe can then estimate the parameters \u03b8 = {\u00b5w,\u03a3w} by using maximum likelihood estimation as suggested in [10].\n2) Trajectory Generation: In novel situations, the trajectory could be modulated by conditioning with different observed states. By adding an observation vector of Y \u2217 = [y\u2217T0 ,y \u2217T T ]\nT indicating desired initial state y\u22170 and target state y\u2217T with the accuracy \u03a3 \u2217 y , we could apply Bayes theorem and represent conditional distribution for w as\np(w|Y \u2217) = N(w|\u00b5\u2032w,\u03a3\u2032w) \u221d N ( Y \u2217|\u03a8\u2217Tw,\u03a3\u2217Y ) p(w)\n\u00b5\u2032w = \u00b5w + \u03a3w\u03a8 \u2217 ( \u03a3\u2217Y + \u03a8 \u2217T\u03a3w\u03a8 \u2217 )\u22121 ( Y \u2217 \u2212\u03a8\u2217T\u00b5w )\n\u03a3\u2032w = \u03a3w \u2212\u03a3w\u03a8\u2217 ( \u03a3\u2217Y + \u03a8 \u2217T\u03a3w\u03a8 \u2217 )\u22121\n\u03a8\u2217T\u03a3w (5)\nwhere \u03a8\u2217 = [\u03a8(0),\u03a8(T )] and \u03a3\u2217Y = diag(\u03a3 \u2217 y ,\u03a3 \u2217 y) are augmented for observation vector Y \u2217. With a conditional distribution of w, we could generate conditional trajectory distribution and easily evaluate the mean yD and the variance \u03a3D of the trajectory T for any time point t according to Eq.( 2) and Eq.( 3). Therefore, the mean trajectory yD(t) can be used as the imitation trajectory in movement adaptation and the variance \u03a3D(t) can be used to indicate which parts or dimensions of the trajectory are more flexible to adapt. The larger variance reflects higher variations in demonstrations. It means more flexibility to modify the corresponding part of the trajectory.\nIt is worth to mention that, although we adopt ProMPs for movement imitation in this work, the proposed Movement Adaptation framework can be integrated with any other movement imitation learning techniques."}, {"heading": "B. Movement Adaptation", "text": "As mentioned before, if the environment of a new situation is exactly the same as the one during demonstration when ProMPs are learned, e.g, no obstacle, safety constraints or other new considerations, the robot can perform movement optimally by directly following the imitation trajectory yD \u2208 d in discrete time generated by learned ProMPs.\nIn this work, we want to have a system that can adapt to an environment with novel constraints. Thus, we model the movement adaptation as an optimal control problem with fixed time horizon T in discrete time. The output of the adaptation system is a new trajectory y \u2208 d in discrete time. The input consists of the task context xc that\ndescribes the environment, the objects and any other taskrelated information which are obtained from the perception module, the imitation trajectory yD which is generated from learned ProMPs, and the reward function f (y,xc,yD) which represents the reward of the adapted trajectory y corresponding to the new situation.\n1) Optimization with Constraints: Let\u2019s consider that the perception module detects Nob j objects in the environment, which may be obstacles during the manipulation. Each object is abstracted as a sphere in the space represented by its center location and semi-diameter {Ok, dk}, k = 1, . . . ,Nob j. Assuming the reward function can be modeled as accumulated sum of rewards from each state y(t) at time step t:\nf (y,xc,yD) = T\u2211\nt=0\nft(y(t),xc,yD). (6)\nBecause we are only modulating the trajectory, we can model the adaptation system as linear dynamics with the control signal a \u2208 m, as it does not involve real physical dynamics. According to the embodiment of robotic end-effector based on its design, we could compute the end-effector\u2019s position in spatial space E(y) following the kinematics modeling [17]. Then, considering obstacles avoidance in spatial space, the target optimal policy \u03c0\u2217 = {a(t)\u2217}t=0,...,T\u22121 could be defined from Eq. (7) with constraints.\n\u03c0\u2217 = arg max \u03c0\n\u2211T t=0 ft(y(t),xc,yD) (7)\nsubj. to \u2200t = 0, \u00b7 \u00b7 \u00b7 ,T \u2212 1 (8) z(t + 1) = Az(t) +Ba(t) (9)\ny(t) = Cz(t) (10) U \u2265 y(t) \u2265 L (11)\n\u2016E(y(t)) \u2212Ok\u20162 \u2265 d2k , \u2200k = 1, \u00b7 \u00b7 \u00b7 ,Nob j (12) y(T ) = yD(T ), (13)\nwhere A,B,C are system matrices, Eq.( 13) constrains the final position of the adapted trajectory, Eq.( 11) constrains the trajectory within feasible limits, and Eq. 12 ensures the adapted trajectory can avoid obstacles safely by keeping a minimum distance dk between the robot\u2019s end-effector and any object.\n2) Model Predictive Control: In order to find an optimal solution of such a system with continuous state and action spaces, we adopt Model Predictive Control which computes the optimal actions in a finite prediction horizon. Therefore, by considering a prediction time horizon Tp, the optimal action a(i)\u2217, at time step i = 0, . . . ,T \u2212 1, can be solved by:\nmax (a(i),\u00b7\u00b7\u00b7 ,a(i+Tp\u22121))\n\u2211i+Tp t=i+1 ft(y(t),xc,yD)\nsubj. to \u2200t = i, \u00b7 \u00b7 \u00b7 , i + Tp \u2212 1 z(t + 1) = Az(t) +Ba(t)\ny(t) = Cz(t) U \u2265 y(t) \u2265 L\n\u2016E(y(t)) \u2212Ok\u20162 \u2265 d2k , \u2200k = 1, \u00b7 \u00b7 \u00b7 ,Nob j y(T ) = yD(T ).\n(14)\nAt each step i, the optimal actions {a(i)\u2217, \u00b7 \u00b7 \u00b7 ,a(i + Tp \u2212 1)\u2217} for Tp decision steps in future are computed but only the action for current step a(i)\u2217 is performed. Therefore, it can deal with changing environments as these changes could be considered in the next decision steps.\n3) Reward Function: In order to adapt robot movements to perform well in novel situations, considering only hard constraints such as obstacle avoidance, Eq.( 12), does not suffice. Thus, our framework further models a reward function f (y,xc,yD) that reflects the amount of rewards that an adapted trajectory y can gain within the context xc and yD. As the reward function f (y) is assumed temporally discrete in Eq.( 6), we model the reward function ft(y(t)) at t by three parts:\nft(y(t);w) = fD,t(y(t);wD) + fC,t(y(t);wC) + fE,t(y(t);wE), (15) where the Imitation Reward fD models the tendency to follow the imitation trajectory yD, the Control Reward fC models the smoothness of executing the adapted trajectory y and the Response Reward fE characterize the expected response to the environment. Meanwhile, w = [wTD,w T C ,w T E ]\nT are parameters that affect the behavior of the movement adaptation. We describe each reward function in detail as follows.\na) Imitation Reward: Imitation Reward characterizes how well the adapted trajectory can imitate the demonstrations through the distance between points on y and yD. Recall that we have the variance \u03a3D(t) of the imitation trajectory yD by Movement Imitation IV-A.2, which indicates how flexible we could adapt the trajectory. Considering \u03a3D(t) = diag(\u03c321(t), . . . , \u03c3 2 d(t)) to be diagonal for the sake of simplicity, we model the Imitation Reward by the weighted distance:\nfD,t(y(t);wD) = \u2212(y(t) \u2212 yD(t))TV (t)(y(t) \u2212 yD(t)) (16) V (t) = diag(wD)diag(e\u2212\u03c3 2 1(t), . . . , e\u2212\u03c3 2 d(t)), (17)\nwhere V (t) is a weight matrix consisting of parameters wD and {e\u2212\u03c32i (t)} in which the variances learned from demonstrations \u03a3D(t) are modeled to affect adaptation rewards.\nb) Control Reward: Control Reward fC characterizes the smoothness of executing the adapted trajectory y through the following formulation:\nfC,t(y(t);wC) = \u2212wC\u2016(y(t) \u2212 y(t \u2212 1))\u20162, (18)\nwhere wC is the parameter to weigh this reward. c) Response Reward: Response reward fE describes the expected response to the environment such as safety considerations for obstacles and objects under manipulation. Here we give intuitive examples for Response Reward. Although we can ensure minimum distance to avoid obstacles using Eq.( 14), as human users we still expect the robot to transfer a cup full of water around a laptop instead of above it, in case of spilling. Another example is that the user would prefer the robot manipulating sharp objects, such as a knife, to keep a relatively larger distance from the human for safety consideration. These examples indicate that we would have preferences towards how the robot avoids obstacles.\nMoreover, for safety consideration, we also prefer the robot to transfer a fragile object closer to the table top to maintain a safety margin. All the above preferences are specific to objects under manipulation and the exact environment. Thus, we set the Response Reward to ensure that the better the adapted trajectory fulfills these preferences, the higher the reward is.\nTo formally represent the Response Reward, let us consider a scenario with Nob j obstacles on the table. The leftmost and rightmost locations of the table are B1,B2 and the table surface is S, we then can formulate Response Rewards as follows:\nfE,t(y(t);wE) = \u2212 Nob j\u2211\nk=1\nwTO,k\u03c6O,k + wB\u03c6B + wS\u03c6S  (19) \u03c6TO,k = [ \u2212\u2016E(y(t)) \u2212Ok\u2016, (E(yD(t)) \u2212E(y(t)))T\n] \u00b7 exp ( \u2212 \u2016E(y(t))\u2212Ok\u2016 2\ndk\n) (20)\n\u03c6B = 2\u2211 i=1 exp ( \u2212\u2016E(y(t)) \u2212Bi\u2016 2 dmin ) (21)\n\u03c6S = \u2016E(y(t)) \u2212 S \u20162, (22) where \u03c6O,k represents the feature vector for preferences in avoiding obstacle Ok, of which the first element denotes avoiding distance and the second element denotes the deviation vector as shown in Fig. 3. The preferred deviation vector is given as reward weights and the inner product between two vectors indicates the rewards of deviation considering the given preference. The exponential decay function is applied so that the features are only effective when the robot\u2019s endeffector is close to the obstacles. \u03c6B and \u03c6S are features related to safety by considering boarders and surface of the table. wE = [wTO,1, . . . ,w T O,Nob j\n,wB,wS ]T are weights corresponding to the features respectively.\nGiven a set of parameters w = [wTD,w T C ,w T E ] T , the MPC module generates an adapted trajectory by maximizing f (\u00b7;w). The robot could follow the adapted trajectory and execute the task facing the novel situation. However, the generated trajectory may not be satisfying enough from user\u2019s perspective, since the given or initialized parameters may not be accurate for modeling the rewards. To accommodate the issue, after the movement execution, our system allows the user to provide a better trajectory as feedback to update the parameters during the following Rewards Learning section."}, {"heading": "C. Rewards Learning", "text": "In this section, we describe how our system learns the reward function. Assuming there is an oracle reward function f \u2217(y,xc,yD) that reflects exactly how much reward the adapted trajectory y can gain for each context. The goal of this module is to estimate such a reward function f (y,xc,yD;w), where w are the parameters to be learned, that approximate the oracle reward f \u2217(\u00b7) tightly.\nBy rewriting Eq.( 6) and Eq.( 15) for the entire trajectory, we can have the reward function in a linear form represented by features and weights:\nf (y,xc,yD;w) = wTD\u03c6D +w T C\u03c6C +w T E\u03c6E (23) \u03c6D = [ \u03c6D,1, . . . , \u03c6D,d ]T , \u03c6D,i = \u2212 T\u2211 t=0 ( yi(t) \u2212 yD,i(t) )2 e\u2212\u03c32i (t) (24)\n\u03c6C = \u2212 T\u2211\nt=1\n\u2016(y(t) \u2212 y(t \u2212 1))\u20162 (25)\n\u03c6E = \u2212 T\u2211\nt=0\n[ \u03c6TO,1(y(t)), . . . ,\u03c6 T O,Nob j (y(t)), \u03c6B(y(t)), \u03c6S (y(t)) ]T (26)\nwhere \u03c6D,\u03c6C ,\u03c6E represent features of the entire trajectory corresponding to Imitation, Control and Response Rewards.\nSince the user only provides a feedback trajectory y\u0304 and the system can not directly observe the reward function, we apply the co-active learning technique [15] in which the robot iteratively updates the parameterw of f (\u00b7;w) based on user\u2019s feedback. Note that this feedback only needs to indicate f \u2217(y\u0304,xc,yD) > f \u2217(y,xc,yD) and y\u0304 could be non-optimal trajectories. Algorithm 1 gives our learning algorithm for movement adaptation.\nNote that \u03b1 is a learning rate, which decays along iterations and C in the weights projection part is a bounded set to ensure updated parameters w are in feasible space. After iterations of improvements, the robot can learn an estimated reward function f (\u00b7;w\u2217) that approximates the oracle reward function f \u2217(\u00b7) as proved in [18]. By maximizing the estimated reward function f (y,xc,yD;w\u2217), the robot can generate an adapted trajectory y that maximizes the rewards facing situation xc based on imitation trajectory yD.\nV. Experiments\nTo validate the system described above, we design and conduct the following experiments on a Baxter humanoid platform. The Baxter robot is asked to do manipulation tasks such as cleaning on a table top, with the surface as S = (0, 0,\u22120.1), the leftmost location as B1 = (0, 0.8, 0) and the rightmost location as B2 = (0,\u22120.8, 0) in robot spatial space in meter. It needs to learn transferring the manipulated object between different locations while avoiding obstacles with desired manners.\nDuring an off-line learning phase, the robot learns the movement skill from multiple kinethestic demonstrations with no obstacles on the table. During the online learning\nAlgorithm 1 Rewards Learning for Movement Adaptation\nstage, a variety of obstacles are located randomly on the table and we assume the robot can obtain their locations from perception modules. The system learns iteratively to adapt the movement skill in novel situations such as with different manners avoiding obstacles, at the same time follows the similar movement pattern from off-line demonstrations."}, {"heading": "A. Movement Imitation", "text": "In the first stage of the experiments, we have our robot learn off-line the movement skill from demonstrations. All trajectories are sampled discretely and normalized to T = 200 steps for transferring movement in joint space, and the left arm of the Baxter has d = 7 degree of freedom. The training trajectories are encoded by ProMPs with n = 10 Gaussian basis functions so that the movement skill can be generalized to different initial and target states.\nFig. 4(a) shows an example of our generated imitation trajectory in spatial space for new task contexts using ProMPs. Fig. 4(b) shows the corresponding imitation trajectory of joint s0 in joint space. The blue crosses here are desired new initial and target states, and the shaded area is the estimated variance for imitation trajectory, which reflects the variations of demonstrations. True trajectory here means a trajectory recorded from user demonstration in the testing scenario for comparison. It is not hard to see that the predicted mean of the imitation trajectory is well generalized to new initial and target states and follows the same movement pattern as the prior mean trajectory learned from demonstrations. Therefore, the robot can perform the task well by following this imitation trajectory if there is no obstacles or other safety constraints under new situations."}, {"heading": "B. Learning Adaptation", "text": "While facing a task of transferring a leaking bottle, the robot may find a bowl with food inside as an obstacle on the table where its center location O1 and minimum safety distance d1 are assumed to be obtained through perception.\nFor movement adaptation, we set the prediction horizon Tp = 11 in model predictive control and select system matrices A = 0.9 \u00b7 I ,B = C = I to make the system stable in the prediction window as suggested in [13]. The limits of joints could be found from the Baxter hardware specification. The minimum safety distance with table boarder is set as dmin = 0.1. And the weights for reward function are initialized to be wD = 30 \u00b7 1,wC = 10,wE = 0. And then we apply the native Matlab Gradient-based optimization method fmincon to solve the optimization at each time step.\nFig. 5(a) shows the output from movement imitation for transferring the leaking bottle, which failed to avoid the obstacle even though the trajectory generalizes to a novel initial and target states. Fig. 5(b) shows the movement adaptation with initial weights. There is no preference specified in reward function about how to avoid obstacles or take safety considerations about boarders. Therefore, even though the\nadapted trajectory could avoid the obstacle successfully, it may be not an ideal trajectory.\nTo learn the user preference, we then provide feedback via kinethestic demonstration illustrated in Fig. 6(a) and the feedback trajectory is shown in Fig. 5(b) as dash line to indicate user preferences. Following Algo. 1, the robot iteratively updates the rewards weights based on the user feedback. Weights are limited via projection in the feasible set C where wD \u2208 [1, 100]7,wC \u2208 [1, 100],wE \u2208 [0, 100] except that last two parameters in wO,k indicating preferred deviation direction could be [\u2212100, 100]. To quantitatively validate the performance of our method in movement adaptation, we consider the metric of cumulative error between the adapted trajectory and the feedback trajectory e(i) = 1 T \u2211T t=0 ( y\u0304(i)(t) \u2212 y(i)(t) )2 as the learning error at iteration i. Since the metric is affected by different situations such as obstacles\u2019 locations, we consider the feedback trajectory as fixed and let the robot iteratively learn several times to see how it performs and record the \u201clearning curve\u201d under the same feedback. From Fig. 6(b), we can see that the error decreases and converges after several iterations, and it only requires a few of iterations to achieve an adapted trajectory as desired preference according to the feedback.\nAfter learning, the robot uses the updated weights for movement adaptation in a different situation with novel\ninitial/target states and the obstacles\u2019 locations. Fig. 5(c) shows the adapted trajectory based on the updated weights after one iteration, where it successfully avoids the obstacle via the desired direction.\nIn a second scenario where a robot is transferring a knife around some fragile obstacle, the user may prefer robot to avoid the obstacle above it instead of around it. With the same methods here, we could also generate adapted trajectories as shown in Fig. 7(a) and Fig. 7(c) for initial weights. With the user provided feedback trajectory, the robot successfully learns the user specified preferences for movement adaptation and generates the improved adapted trajectories for different situations as shown in Fig. 7(b) and Fig. 7(d).\nVI. Conclusion and FutureWork\nWe present a framework for learning to adapt robot end effector movement for manipulation tasks. The proposed method generalizes offline learned movement skills to novel situations considering obstacle avoidance and other taskdependent constraints. It adapts the imitation trajectory generated from demonstrations, while maintaining the learned movement pattern and considering the variations, to avoid obstacles with desired directions and distances and keep a safety margin within a workspace. Also it provides a way to learn how to adapt the movement by on-line interactions from user\u2019s feedback.\nBesides learning how to adapt movement from user\u2019s feedback, the visual information of the objects and the environment could also indicate the preferences of movement adaptation. For instance, the deviation direction for avoiding\na knife could be inferred directly from the location of its blade from visual space. We are further investigating the possibility of directly learning the preferences to adapt movement from visual perception of the task context.\nReferences [1] A. J. Ijspeert, J. Nakanishi, H. Hoffmann, P. Pastor, and S. Schaal,\n\u201cDynamical movement primitives: learning attractor models for motor behaviors,\u201d Neural computation, vol. 25, no. 2, pp. 328\u2013373, 2013. [2] T. Asfour, P. Azad, F. Gyarfas, and R. Dillmann, \u201cImitation learning of dual-arm manipulation tasks in humanoid robots,\u201d International Journal of Humanoid Robotics, vol. 5, no. 02, pp. 183\u2013202, 2008. [3] P. Pastor, H. Hoffmann, T. Asfour, and S. Schaal, \u201cLearning and generalization of motor skills by learning from demonstration,\u201d in Robotics and Automation, 2009. ICRA\u201909. IEEE International Conference on. IEEE, 2009, pp. 763\u2013768. [4] J. Kober, B. Mohler, and J. Peters, \u201cLearning perceptual coupling for motor primitives,\u201d in Intelligent Robots and Systems, 2008. IROS 2008. IEEE/RSJ International Conference on. IEEE, 2008, pp. 834\u2013839. [5] A. Gams, A. J. Ijspeert, S. Schaal, and J. Lenarc\u030cic\u030c, \u201cOn-line learning and modulation of periodic movements with nonlinear dynamical systems,\u201d Autonomous robots, vol. 27, no. 1, pp. 3\u201323, 2009. [6] F. Guenter, M. Hersch, S. Calinon, and A. Billard, \u201cReinforcement learning for imitating constrained reaching movements,\u201d Advanced Robotics, vol. 21, no. 13, pp. 1521\u20131544, 2007. [7] S. Calinon, F. D\u2019halluin, E. L. Sauser, D. G. Caldwell, and A. G. Billard, \u201cLearning and reproduction of gestures by imitation,\u201d Robotics & Automation Magazine, IEEE, vol. 17, no. 2, pp. 44\u201354, 2010. [8] S. M. Khansari-Zadeh and A. Billard, \u201cImitation learning of globally stable non-linear point-to-point robot motions using nonlinear programming,\u201d in Intelligent Robots and Systems (IROS), 2010 IEEE/RSJ International Conference on. IEEE, 2010, pp. 2676\u20132683. [9] T. Inamura, I. Toshima, H. Tanie, and Y. Nakamura, \u201cEmbodied symbol emergence based on mimesis theory,\u201d The International Journal of Robotics Research, vol. 23, no. 4-5, pp. 363\u2013377, 2004. [10] A. Paraschos, C. Daniel, J. R. Peters, and G. Neumann, \u201cProbabilistic movement primitives,\u201d in Advances in neural information processing systems, 2013, pp. 2616\u20132624. [11] D.-H. Park, P. Pastor, S. Schaal et al., \u201cMovement reproduction and obstacle avoidance with dynamic movement primitives and potential fields,\u201d in Humanoid Robots, 2008. Humanoids 2008. 8th IEEE-RAS International Conference on. IEEE, 2008, pp. 91\u201398. [12] H. Hoffmann, P. Pastor, D.-H. Park, and S. Schaal, \u201cBiologicallyinspired dynamical systems for movement generation: automatic realtime goal adaptation and obstacle avoidance,\u201d in Robotics and Automation, 2009. ICRA\u201909. IEEE International Conference on. IEEE, 2009, pp. 2587\u20132592. [13] A. M. Ghalamzan E., C. Paxton, G. D. Hager, and L. Bascetta, \u201cAn incremental approach to learning generalizable robot tasks from human demonstration,\u201d in Robotics and Automation (ICRA), 2015 IEEE International Conference on. IEEE, 2015, pp. 5616\u20135621. [14] E. A. Sisbot, L. F. Marin, and R. Alami, \u201cSpatial reasoning for human robot interaction,\u201d in Intelligent Robots and Systems, 2007. IROS 2007. IEEE/RSJ International Conference on. IEEE, 2007, pp. 2281\u20132287. [15] A. Jain, S. Sharma, T. Joachims, and A. Saxena, \u201cLearning preferences for manipulation tasks from online coactive feedback,\u201d The International Journal of Robotics Research, p. 0278364915581193, 2015. [16] R. Mao, Y. Yang, C. Fermuller, Y. Aloimonos, and J. S. Baras, \u201cLearning hand movements from markerless demonstrations for humanoid tasks,\u201d in Humanoid Robots (Humanoids), 2014 14th IEEERAS International Conference on. IEEE, 2014, pp. 938\u2013943. [17] Z. Ju, C. Yang, and H. Ma, \u201cKinematics modeling and experimental verification of baxter robot,\u201d in Control Conference (CCC), 2014 33rd Chinese. IEEE, 2014, pp. 8518\u20138523. [18] G. Cina\u0301 and U. Endriss, \u201cProving classical theorems of social choice theory in modal logic,\u201d Autonomous Agents and Multi-Agent Systems, pp. 1\u201327, 2016."}], "references": [{"title": "Dynamical movement primitives: learning attractor models for motor behaviors", "author": ["A.J. Ijspeert", "J. Nakanishi", "H. Hoffmann", "P. Pastor", "S. Schaal"], "venue": "Neural computation, vol. 25, no. 2, pp. 328\u2013373, 2013.", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2013}, {"title": "Imitation learning of dual-arm manipulation tasks in humanoid robots", "author": ["T. Asfour", "P. Azad", "F. Gyarfas", "R. Dillmann"], "venue": "International Journal of Humanoid Robotics, vol. 5, no. 02, pp. 183\u2013202, 2008.", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2008}, {"title": "Learning and generalization of motor skills by learning from demonstration", "author": ["P. Pastor", "H. Hoffmann", "T. Asfour", "S. Schaal"], "venue": "Robotics and Automation, 2009. ICRA\u201909. IEEE International Conference on. IEEE, 2009, pp. 763\u2013768.", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2009}, {"title": "Learning perceptual coupling for motor primitives", "author": ["J. Kober", "B. Mohler", "J. Peters"], "venue": "Intelligent Robots and Systems, 2008. IROS 2008. IEEE/RSJ International Conference on. IEEE, 2008, pp. 834\u2013839.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2008}, {"title": "On-line learning and modulation of periodic movements with nonlinear dynamical systems", "author": ["A. Gams", "A.J. Ijspeert", "S. Schaal", "J. Lenar\u010di\u010d"], "venue": "Autonomous robots, vol. 27, no. 1, pp. 3\u201323, 2009.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2009}, {"title": "Reinforcement learning for imitating constrained reaching movements", "author": ["F. Guenter", "M. Hersch", "S. Calinon", "A. Billard"], "venue": "Advanced Robotics, vol. 21, no. 13, pp. 1521\u20131544, 2007.", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2007}, {"title": "Learning and reproduction of gestures by imitation", "author": ["S. Calinon", "F. D\u2019halluin", "E.L. Sauser", "D.G. Caldwell", "A.G. Billard"], "venue": "Robotics & Automation Magazine, IEEE, vol. 17, no. 2, pp. 44\u201354, 2010.", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2010}, {"title": "Imitation learning of globally stable non-linear point-to-point robot motions using nonlinear programming", "author": ["S.M. Khansari-Zadeh", "A. Billard"], "venue": "Intelligent Robots and Systems (IROS), 2010 IEEE/RSJ International Conference on. IEEE, 2010, pp. 2676\u20132683.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2010}, {"title": "Embodied symbol emergence based on mimesis theory", "author": ["T. Inamura", "I. Toshima", "H. Tanie", "Y. Nakamura"], "venue": "The International Journal of Robotics Research, vol. 23, no. 4-5, pp. 363\u2013377, 2004.", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2004}, {"title": "Probabilistic movement primitives", "author": ["A. Paraschos", "C. Daniel", "J.R. Peters", "G. Neumann"], "venue": "Advances in neural information processing systems, 2013, pp. 2616\u20132624.", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2013}, {"title": "Movement reproduction and obstacle avoidance with dynamic movement primitives and potential fields", "author": ["D.-H. Park", "P. Pastor", "S. Schaal"], "venue": "Humanoid Robots, 2008. Humanoids 2008. 8th IEEE-RAS International Conference on. IEEE, 2008, pp. 91\u201398.", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2008}, {"title": "Biologicallyinspired dynamical systems for movement generation: automatic realtime goal adaptation and obstacle avoidance", "author": ["H. Hoffmann", "P. Pastor", "D.-H. Park", "S. Schaal"], "venue": "Robotics and Automation, 2009. ICRA\u201909. IEEE International Conference on. IEEE, 2009, pp. 2587\u20132592.", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2009}, {"title": "An incremental approach to learning generalizable robot tasks from human demonstration", "author": ["A.M. Ghalamzan E.", "C. Paxton", "G.D. Hager", "L. Bascetta"], "venue": "Robotics and Automation (ICRA), 2015 IEEE International Conference on. IEEE, 2015, pp. 5616\u20135621.", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2015}, {"title": "Spatial reasoning for human robot interaction", "author": ["E.A. Sisbot", "L.F. Marin", "R. Alami"], "venue": "Intelligent Robots and Systems, 2007. IROS 2007. IEEE/RSJ International Conference on. IEEE, 2007, pp. 2281\u20132287.", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2007}, {"title": "Learning preferences for manipulation tasks from online coactive feedback", "author": ["A. Jain", "S. Sharma", "T. Joachims", "A. Saxena"], "venue": "The International Journal of Robotics Research, p. 0278364915581193, 2015.", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2015}, {"title": "Learning hand movements from markerless demonstrations for humanoid tasks", "author": ["R. Mao", "Y. Yang", "C. Fermuller", "Y. Aloimonos", "J.S. Baras"], "venue": "Humanoid Robots (Humanoids), 2014 14th IEEE- RAS International Conference on. IEEE, 2014, pp. 938\u2013943.", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2014}, {"title": "Kinematics modeling and experimental verification of baxter robot", "author": ["Z. Ju", "C. Yang", "H. Ma"], "venue": "Control Conference (CCC), 2014 33rd Chinese. IEEE, 2014, pp. 8518\u20138523.", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2014}, {"title": "Proving classical theorems of social choice theory in modal logic", "author": ["G. Cin\u00e1", "U. Endriss"], "venue": "Autonomous Agents and Multi-Agent Systems, pp. 1\u201327, 2016.", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2016}], "referenceMentions": [{"referenceID": 0, "context": "However, it is widely known that general MPs methods, such as Dynamic Movement Primitives (DMPs) [1], have limited capability to generalize towards novel environments involving other constraints.", "startOffset": 97, "endOffset": 100}, {"referenceID": 1, "context": "Among them, imitation learning [2] focuses on mimicking human demonstrations, while learning from demonstration (LfD) techniques [3] are applicable.", "startOffset": 31, "endOffset": 34}, {"referenceID": 2, "context": "Among them, imitation learning [2] focuses on mimicking human demonstrations, while learning from demonstration (LfD) techniques [3] are applicable.", "startOffset": 129, "endOffset": 132}, {"referenceID": 3, "context": "To deal with novel environments, extended approaches [4]", "startOffset": 53, "endOffset": 56}, {"referenceID": 4, "context": "Approaches [5] for encoding trajectory as motion primitives have been proposed for various forms of generalization and modulation, such as Gaussian mixture regression and Gaussian mixture models [6], [7].", "startOffset": 11, "endOffset": 14}, {"referenceID": 5, "context": "Approaches [5] for encoding trajectory as motion primitives have been proposed for various forms of generalization and modulation, such as Gaussian mixture regression and Gaussian mixture models [6], [7].", "startOffset": 195, "endOffset": 198}, {"referenceID": 6, "context": "Approaches [5] for encoding trajectory as motion primitives have been proposed for various forms of generalization and modulation, such as Gaussian mixture regression and Gaussian mixture models [6], [7].", "startOffset": 200, "endOffset": 203}, {"referenceID": 7, "context": "In [8], a mixture", "startOffset": 3, "endOffset": 6}, {"referenceID": 8, "context": "Another school of approaches derive from Hidden Markov models [9].", "startOffset": 62, "endOffset": 65}, {"referenceID": 0, "context": "One popular representation to encode motion from demonstrated trajectories is Dynamic Movement Primitives (DMPs), as introduced in [1].", "startOffset": 131, "endOffset": 134}, {"referenceID": 9, "context": "Recently, Probabilistic Movement Primitives (ProMPs) [10] was proposed as an alternative representation in probabilistic formulation.", "startOffset": 53, "endOffset": 57}, {"referenceID": 10, "context": "In order to enable MPs to adapt to novel environments with obstacles [11], [12], Kober et al.", "startOffset": 69, "endOffset": 73}, {"referenceID": 11, "context": "In order to enable MPs to adapt to novel environments with obstacles [11], [12], Kober et al.", "startOffset": 75, "endOffset": 79}, {"referenceID": 3, "context": "[4] proposed an augmented version of DMPs which incorporates perceptual coupling to an external variable.", "startOffset": 0, "endOffset": 3}, {"referenceID": 12, "context": "[13] proposed a three-tiered approach for robot learning from demonstrations that can generalize noisy task demonstrations to a new target state and to an environment with obstacles.", "startOffset": 0, "endOffset": 4}, {"referenceID": 13, "context": "[14] proposed to model user specified preferences as constraints on the distance of the robot from the user, the visibility of the robot and the users arm comfort.", "startOffset": 0, "endOffset": 4}, {"referenceID": 14, "context": "[15] proposed a co-active learning method to learn user preferences over generated trajectories for", "startOffset": 0, "endOffset": 4}, {"referenceID": 2, "context": "For the problem of robot learning from demonstrations [3], a common practice is to offline learn the skills by encoding the trajectories with movement patterns such as DMPs [16].", "startOffset": 54, "endOffset": 57}, {"referenceID": 15, "context": "For the problem of robot learning from demonstrations [3], a common practice is to offline learn the skills by encoding the trajectories with movement patterns such as DMPs [16].", "startOffset": 173, "endOffset": 177}, {"referenceID": 12, "context": "as [13] suggests, it assumes that demonstrations are from experts, which bears an oracle reward function.", "startOffset": 3, "endOffset": 7}, {"referenceID": 12, "context": "Also, [13] requires the manipulated objects or obstacles existing during these demonstrations and is hard to update the learned reward function online when the robot is facing situations that involve new objects.", "startOffset": 6, "endOffset": 10}, {"referenceID": 14, "context": "learning technique [15] in which the user only corrects the robot by providing an improved trajectory \u0233 and then the robot updates the parameter w of f (\u00b7;w) based on user\u2019s feedback.", "startOffset": 19, "endOffset": 23}, {"referenceID": 9, "context": "In this work, we adopt the Probabilistic Movement Primitives (ProMPs) [10] for offline learning and movement imitation.", "startOffset": 70, "endOffset": 74}, {"referenceID": 9, "context": "We can then estimate the parameters \u03b8 = {\u03bcw,\u03a3w} by using maximum likelihood estimation as suggested in [10].", "startOffset": 103, "endOffset": 107}, {"referenceID": 16, "context": "According to the embodiment of robotic end-effector based on its design, we could compute the end-effector\u2019s position in spatial space E(y) following the kinematics modeling [17].", "startOffset": 174, "endOffset": 178}, {"referenceID": 14, "context": "Since the user only provides a feedback trajectory \u0233 and the system can not directly observe the reward function, we apply the co-active learning technique [15] in which the robot iteratively updates the parameterw of f (\u00b7;w) based on user\u2019s feedback.", "startOffset": 156, "endOffset": 160}, {"referenceID": 17, "context": "After iterations of improvements, the robot can learn an estimated reward function f (\u00b7;w\u2217) that approximates the oracle reward function f \u2217(\u00b7) as proved in [18].", "startOffset": 157, "endOffset": 161}, {"referenceID": 12, "context": "9 \u00b7 I ,B = C = I to make the system stable in the prediction window as suggested in [13].", "startOffset": 84, "endOffset": 88}, {"referenceID": 0, "context": "Weights are limited via projection in the feasible set C where wD \u2208 [1, 100],wC \u2208 [1, 100],wE \u2208 [0, 100] except that last two parameters in wO,k indicating preferred deviation direction could be [\u2212100, 100].", "startOffset": 68, "endOffset": 76}, {"referenceID": 0, "context": "Weights are limited via projection in the feasible set C where wD \u2208 [1, 100],wC \u2208 [1, 100],wE \u2208 [0, 100] except that last two parameters in wO,k indicating preferred deviation direction could be [\u2212100, 100].", "startOffset": 82, "endOffset": 90}], "year": 2016, "abstractText": "In this paper we address the problem of robot movement adaptation under various environmental constraints interactively. Motion primitives are generally adopted to generate target motion from demonstrations. However, their generalization capability is weak while facing novel environments. Additionally, traditional motion generation methods do not consider the versatile constraints from various users, tasks, and environments. In this work, we propose a co-active learning framework for learning to adapt robot end-effector\u2019s movement for manipulation tasks. It is designed to adapt the original imitation trajectories, which are learned from demonstrations, to novel situations with various constraints. The framework also considers user\u2019s feedback towards the adapted trajectories, and it learns to adapt movement through human-in-the-loop interactions. The implemented system generalizes trained motion primitives to various situations with different constraints considering user preferences. Experiments on a humanoid platform validate the effectiveness of our approach.", "creator": "LaTeX with hyperref package"}}}