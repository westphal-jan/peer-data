{"id": "1609.04243", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "14-Sep-2016", "title": "Convolutional Recurrent Neural Networks for Music Classification", "abstract": "We introduce a convolutional recurrent neural network (CRNN) for music tagging. CRNNs take advantage of convolutional neural networks (CNNs) for local feature extraction and recurrent neural networks for temporal summarisation of the extracted features. We compare CRNN with two CNN structures that have been used for music tagging while controlling the number of parameters with respect to their performance and training time per sample. These results indicate that we have found a significant overlap between our two network structures.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n", "histories": [["v1", "Wed, 14 Sep 2016 12:52:08 GMT  (100kb,D)", "http://arxiv.org/abs/1609.04243v1", "5 pages, ICASSP 2017 submitted"], ["v2", "Thu, 3 Nov 2016 07:50:14 GMT  (112kb,D)", "http://arxiv.org/abs/1609.04243v2", "5 pages, ICASSP 2017 submitted. Revised to fix previous CNN architectures and update experiment results"], ["v3", "Wed, 21 Dec 2016 06:52:30 GMT  (112kb,D)", "http://arxiv.org/abs/1609.04243v3", "5 pages, ICASSP 2017 submitted. Revised to fix previous CNN architectures and update experiment results"]], "COMMENTS": "5 pages, ICASSP 2017 submitted", "reviews": [], "SUBJECTS": "cs.NE cs.LG cs.MM cs.SD", "authors": ["keunwoo choi", "george fazekas", "mark sandler", "kyunghyun cho"], "accepted": false, "id": "1609.04243"}, "pdf": {"name": "1609.04243.pdf", "metadata": {"source": "CRF", "title": "CONVOLUTIONAL RECURRENT NEURAL NETWORKS FOR MUSIC CLASSIFICATION", "authors": ["Keunwoo Choi", "Gy\u00f6rgy Fazekas", "Mark Sandler", "Kyunghyun Cho"], "emails": ["keunwoo.choi@qmul.ac.uk", "kyunghyun.cho@nyu.edu"], "sections": [{"heading": null, "text": "We introduce a convolutional recurrent neural network (CRNN) for music tagging. CRNNs take advantage of convolutional neural networks (CNNs) for local feature extraction and recurrent neural networks for temporal summarisation of the extracted features. We compare CRNN with two CNN structures that have been used for music tagging while controlling the number of parameters with respect to their performance and training time per sample. Overall, we found that CRNNs show strong performance with respect to the number of parameter and training time, indicating the effectiveness of its hybrid structure in music feature extraction and feature summarisation.\nIndex Terms\u2014 convolutional neural networks, recurrent neural networks, music classification"}, {"heading": "1. INTRODUCTION", "text": "Convolutional neural networks (CNNs) have been actively used for various music classification tasks such as music tagging [1, 2], genre classification [3, 4], and user-item latent feature prediction for recommendation [5].\nCNNs assume features that are in different levels of hierarchy and can be extracted by convolutional kernels. The hierarchical features are learned to achieve a given task during supervised training. For example, learned features from a CNN that is trained for genre classification exhibit low-level features (e.g., onset) to high-level features (e.g., percussive instrument patterns) [6].\nRecently, CNNs have been combined with recurrent neural networks (RNNs) which are often used to model temporal data such as audio signals or word sequences. This hybrid model is called a convolutional recurrent neural network (CRNN). A CRNN can be described as a modified CNN by replacing the last convolutional layers with a RNN. In\n\u2217This work has been part funded by FAST IMPACt EPSRC Grant EP/L019981/1 and the European Commission H2020 research and innovation grant AudioCommons (688382). Mark Sandler acknowledges the support of the Royal Society as a recipient of a Wolfson Research Merit Award. \u2020Kyunghyun Cho thanks the support by Facebook, Google (Google Faculty Award 2016) and NVidia (GPU Center of Excellence 2015-2016)\nCRNNs, CNNs and RNNs play the roles of feature extractor and temporal summariser, respectively. Adopting an RNN for aggregating the features enables the networks to take the global structure into account while local features are extracted by the remaining convolutional layers. This structure was first proposed in [7] for document classification and later applied to image classification [8] and music transcription [9].\nCRNNs fit the music tagging task well. First, RNNs are more flexible in selecting how to summarise the local features than CNNs which are rather static by using weighted average (convolution) and subsampling. This flexibility can be helpful because some of the tags (e.g., mood tags) may be affected by the global structure while other tags such as instruments can be affected by local and short-segment information. Second, RNNs can effectively deal with variable length of music signal, while CNNs expect a fixed input size.\nIn this paper, we introduce CRNNs for music tagging. We compare the proposed CRNN with two existing CNNs. For correct comparisons, we carefully control the hardware, data, and optimisation techniques, while varying two attributes of the structure: i) the number of parameters and ii) computation time. In other words, the representation power of the structures is evaluated with respect to memory usage and computation complexity."}, {"heading": "2. MODELS", "text": "We compare CRNN with Conv1D and Conv2D, all of which are illustrated in Figure 1. The specifications are shown in Table 1. For all networks, the input is assumed to be of size 96\u00d71366 (mel-frequency band\u00d7time frame). Sigmoid functions are used as activation at output nodes because music tagging is a multi-label classification task.\nFor a correct comparison, it is necessary to equip the networks with identical optimisation techniques since they greatly improve the performances of networks having essentially the same structure. For example, techniques such as batch normalization (BN) [10] and exponential linear unit (ELU) [11] improve both training speed and accuracy by nudging the distribution of the outputs of intermediate layers to be standardised without changing the structure.\nIn this paper, therefore, all the convolutional and fully-\nar X\niv :1\n60 9.\n04 24\n3v 1\n[ cs\n.N E\n] 1\n4 Se\np 20\nconnected layers are equipped with identical optimisation techniques and activation functions \u2013 a dropout of 0.5 [12], batch normalization [10], and ELU activation function [11]."}, {"heading": "2.1. Conv1D", "text": "Conv1D in Figure 1a is motivated by structures for music tagging [1] and genre classification [13]. The network consists of 4 convolutional layers that are followed by 2 fully-connected layers. One-dimensional convolutional layers (1\u00d78 for all, i.e., convolution along time-axis) and maxpooling layers ((1\u00d74)-(1\u00d75)-(1\u00d78)-(1\u00d78)) alternate, and resulting in narrow-and-tall feature maps. They are flattened and fed into a fully-connected layer, which acts as the classifier.\nThis model relies on the assumption of non-stationarity along the frequency axis of music spectrograms. It preserves the frequency band information until the end of convolutional layers. In other words, each element of the last feature map (the output of the 4-th sub-sampling layer) encodes a feature for each band.\nIn [1], 29s-long signal are trimmed into less than 4-sec subsegments, and then the final tag predictions are averaged. The author\u2019s more recent work1 directly takes the whole 30slong signal as an input. We adopt the approach from the latter in this paper."}, {"heading": "2.2. Conv2D", "text": "CNN structures with two-dimensional convolution have been used in music tagging [2] and vocal/instrumental classification [14]. Conv2D consists of five convolutional layers of 3\u00d73 kernels and max-pooling layers ((2\u00d74)-(2\u00d74)-(2\u00d74)- (3\u00d75)-(4\u00d74)) as illustrated in Figure 1b. The network reduces the size of feature maps to 1\u00d71 at the final layer, where each feature covers the whole input rather than each frequency band. Temporal information is also gradually aggregated during the size reduction.\nThis model assumes that there are two-dimensional patterns in the time-frequency plane. Also, using twodimensional subsampling enables the network to be fullyconvolutional, which ultimately results in fewer parameters.\n1http://benanne.github.io/2014/08/05/ spotify-cnns.html\nThis is because fully-connected layers are often responsible for large portion of the network\u2019s parameters."}, {"heading": "2.3. CRNN", "text": "CRNN uses a 2-layer RNN with gated recurrent units (GRU) [15] to summarise temporal patterns on the top of twodimensional 4-layer CNNs as shown in Figure 1c. The assumption underlying this model is that the temporal pattern can be aggregated better with RNNs then CNNs, while relying on CNNs on input side for local feature extraction.\nThis structure is first proposed in [7] for document classification. In CRNN, RNNs are used to aggregate the temporal patterns instead of, for instance, averaging the results from shorter segments as in [1] or convolution and sub-sampling as in Conv2D. In its CNN sub-structure, the sizes of convolutional layers and max-pooling layers are 3\u00d73 and (2\u00d72)-(3\u00d73)-(4\u00d74)-(4\u00d74). This sub-sampling results in a feature map size of N\u00d71\u00d715 (number of feature maps\u00d7frequency\u00d7time). They are then fed into a 2-layer RNN, of which the last hidden state is connected to the output of the network."}, {"heading": "2.4. Scaling networks", "text": "The models are scaled by controlling the number of parameters to be 100,000, 250,000, 0.5 million, 1M, 3M with 2% tolerance. Considering the limitation of current hardware and the dataset size, 3M-parameter networks are presumed to provide an approximate upper bound of the structure complexity. Table 1 summarises the details of different structures including the layer width (the number of feature maps or hidden units).\nThe widths of layers are based on [1] for Conv1D and [2] for Conv2D. For CRNN, the widths are determined based on preliminary experiments which showed the relative importance of the numbers of the feature maps of convolutional layers over the number of hidden units in RNNs.\nLayer widths are changed to control the number of parameters of a network while the depths and the convolutional kernel shapes are kept constant. Therefore, the hierarchy of learned features is preserved while the numbers of the features in each hierarchical level (i.e., each layer) are changed. This is to maximise the representation capabilities of networks, considering the relative importance of depth over width [16]."}, {"heading": "3. EXPERIMENTS", "text": "We use the Million Song Dataset [17] with last.fm tags. We train the networks to predict the top-50 tag, which includes genres (e.g., rock, pop), moods (e.g., sad, happy), instruments (e.g., female vocalist, guitar), and eras (60s \u2013 00s). 214,284 (201,680 for training and 12,605 for validation) and 25,940 clips are selected by using the originally provided training/test splitting and filtering out items without any top50 tags. The occurrences of tags range from 52,944 (rock) to 1,257 (happy).\nWe use 30-60s preview clips which are provided after trimming to represent the highlight of the song. We trim audio signals to 29 seconds at the centre of preview clips and downsample them from 22.05 kHz to 12 kHz using Librosa [18]. Log-amplitude mel-spectrograms are used as input since they have outperformed STFT and MFCCs, and linear-amplitude mel-spectrograms in earlier research [2, 1]. The number of mel-bins is 96 and the hop-size is 256 samples, resulting in an input shape of 96\u00d71366.\nThe model is built with Keras [19] and Theano [20]. We use ADAM for learning rate control [21] and binary crossentropy as a loss function. The reported performance is measured on test set and by AUC-ROC (Area Under Receiver Operating Characteristic Curve) given that tagging is a multilabel classification.\nWe use early-stopping for the all structures \u2013 the training is stopped if there is no improvement bigger than a threshold (0.002 AUC) on the validation set while iterating half of the training data. An exception is applied to 3M-networks, for which the threshold is set to 0 and iterating for the whole training data because the convergence rate per training samples is slower in more complex networks."}, {"heading": "3.1. Memory-controlled experiment", "text": "Figure 2 shows the AUCs for each network against the number of parameters. With the same number of parameters, the ranking of AUC is CRNN > Conv2D > Conv1D except for 3M-parameter structures. This indicates that CRNN can be preferred when the bottleneck is memory usage. For 3M-\nparameter structures, Conv2D outperforms CRNN while both of them outperform the state-of-the-art structure [2].\nCRNN outperforms Conv2D in all cases except when using 3M parameters. Because they share the same 2Dconvolutional layers, this difference is probably a consequence of the difference in RNNs and CNNs the ability of summarising the features over time. This may indicate that learning a global structure is more important than focusing on local structures for summarisation. One may focus on the different layer widths of two structures \u2013 because recurrent layers use less parameters than convolutional layers, CRNN has wider convolutional layers than Conv2D with same number of parameters. However, even CRNN with narrower layer widths (0.1M parameters) shows better performance than Conv2D with wider widths (0.25M and 0.5M parameters). With 3M parameters, CRNN suffers from slower convergence rate than Conv2D due to its deeper RNN structure. This results in a lower AUC for CRNN compared to Conv2D.\nConv2D shows higher AUCs than Conv1D in all cases. This shows that the model of Conv2D, which encodes local invariance and captures local time-frequency relationships, is more effective than Conv1D, which ignores local frequency relationships. Conv2D also uses parameters in a more flexible way with its fully-convolutional structure, while Conv1D allocates only a small proportion of the parameters to the feature extraction stage. For example, with 0.5M parameters, only 13% of the parameters are used by convolutional layers while the rest, 87%, are used by the fully-connected layers."}, {"heading": "3.2. Computation-controlled comparison", "text": "We further investigate the computational complexity of each structure. The computational complexity is directly related to the training and prediction time and varies depending not only on the number of parameters but also on the structure. The wall-clock training times for 2500 samples are summarised in Table 1 and plotted in Figure 2.\nWith similar training time, CRNN shows the best performance for training time < 150s. However, Conv2D with training time of 180s (3M parameters) outperforms CRNN with similar or longer training times (0.5M, 1M, and 3M parameters). This result indicates that either Conv2D or CRNN can be used depending on the target time budget.\nWith the same number of parameters, the ranking of training speed is always Conv2D > Conv1D > CRNN. In other words, speed is negatively correlated with the depths of the networks (5, 6, and 20, respectively)."}, {"heading": "3.3. Performance per tag", "text": "Figure 3 visualises the AUC score of each tag of 1Mparameter structures. Each tag is categorised as one of genres, moods, instruments and eras, and sorted by AUC within its category. Under this categorisation, music tagging task can be considered as a multiple-task problem equivalent to four classification tasks with these four categories.\nThe CRNN outperforms Conv2D and Conv1D in every tag, and Conv2D outperforms Conv1D for 47 out of 50 tags. From the multiple-task classification perspective, this result indicates that a structure that outperforms in one of the four tasks may perform best in the other tasks as well.\nAlthough the dataset is imbalanced, the tag popularity (number of occurrence of each tag) is not correlated to the performance. Spearman rank correlation between tag popularity and the ranking of AUC scores of all tags is 0.077."}, {"heading": "4. CONCLUSIONS", "text": "We proposed a convolutional recurrent neural network (CRNN) for music tagging. In the experiment, we controlled the size of the networks by varying the numbers of parameters to investigate two scenarios, memory-controlled and computation-controlled comparison. Our experiments revealed that Conv2D and CRNN perform comparably to each other with a large number of parameters. However, with a modest number of parameters, however, we observed an interesting trade-off between speed and memory. The feedforward pass of Conv2D is faster than that of CRNN across all parameter settings, while the CRNN tends to outperform Conv2D with the same number of parameters. Future work will investigate RNN-based structures and audio input requirements for deep learning approaches."}, {"heading": "5. REFERENCES", "text": "[1] Sander Dieleman and Benjamin Schrauwen, \u201cEnd-toend learning for music audio,\u201d in Acoustics, Speech and Signal Processing (ICASSP), 2014 IEEE International Conference on. IEEE, 2014, pp. 6964\u20136968.\n[2] Keunwoo Choi, George Fazekas, and Mark Sandler, \u201cAutomatic tagging using deep convolutional neural networks,\u201d in International Society of Music Information Retrieval Conference. ISMIR, 2016.\n[3] Siddharth Sigtia and Simon Dixon, \u201cImproved music feature learning with deep neural networks,\u201d in 2014 IEEE international conference on acoustics, speech and signal processing (ICASSP). IEEE, 2014.\n[4] Paulo Chiliguano and Gyorgy Fazekas, \u201cHybrid music recommender using content-based and social information,\u201d in 2016 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). IEEE, 2016, pp. 2618\u20132622.\n[5] Aaron Van den Oord, Sander Dieleman, and Benjamin Schrauwen, \u201cDeep content-based music recommendation,\u201d in Advances in Neural Information Processing Systems, 2013, pp. 2643\u20132651.\n[6] Keunwoo Choi, George Fazekas, and Mark Sandler, \u201cExplaining deep convolutional neural networks on music classification,\u201d arXiv preprint arXiv:1607.02444, 2016.\n[7] Duyu Tang, Bing Qin, and Ting Liu, \u201cDocument modeling with gated recurrent neural network for sentiment classification,\u201d in Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, 2015, pp. 1422\u20131432.\n[8] Zhen Zuo, Bing Shuai, Gang Wang, Xiao Liu, Xingxing Wang, Bing Wang, and Yushi Chen, \u201cConvolutional recurrent neural networks: Learning spatial dependencies for image representation,\u201d in Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition Workshops, 2015, pp. 18\u201326.\n[9] Siddharth Sigtia, Emmanouil Benetos, and Simon Dixon, \u201cAn end-to-end neural network for polyphonic piano music transcription,\u201d IEEE/ACM Transactions on Audio, Speech, and Language Processing, vol. 24, no. 5, 2016.\n[10] Sergey Ioffe and Christian Szegedy, \u201cBatch normalization: Accelerating deep network training by reducing internal covariate shift,\u201d arXiv preprint arXiv:1502.03167, 2015.\n[11] Djork-Arne\u0301 Clevert, Thomas Unterthiner, and Sepp Hochreiter, \u201cFast and accurate deep network learning by exponential linear units (elus),\u201d arXiv preprint arXiv:1511.07289, 2015.\n[12] Nitish Srivastava, Geoffrey Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdinov, \u201cDropout: A simple way to prevent neural networks from overfitting,\u201d The Journal of Machine Learning Research, vol. 15, no. 1, pp. 1929\u20131958, 2014.\n[13] Jan Wu\u0308lfing and Martin Riedmiller, \u201cUnsupervised learning of local features for music classification.,\u201d in International Society of Music Information Retrieval Conference. ISMIR, 2012, pp. 139\u2013144.\n[14] Jan Schlu\u0308ter, \u201cLearning to pinpoint singing voice from weakly labeled examples,\u201d in International Society of Music Information Retrieval Conference. ISMIR, 2016.\n[15] Kyunghyun Cho, Bart Van Merrie\u0308nboer, Dzmitry Bahdanau, and Yoshua Bengio, \u201cOn the properties of neural machine translation: Encoder-decoder approaches,\u201d arXiv preprint arXiv:1409.1259, 2014.\n[16] Ronen Eldan and Ohad Shamir, \u201cThe power of depth for feedforward neural networks,\u201d arXiv preprint arXiv:1512.03965, 2015.\n[17] Thierry Bertin-Mahieux, Daniel PW Ellis, Brian Whitman, and Paul Lamere, \u201cThe million song dataset,\u201d in Proceedings of the 12th International Society for Music Information Retrieval Conference, Miami, Florida, USA, October 24-28, 2011, 2011.\n[18] Brian McFee, Colin Raffel, Dawen Liang, Daniel PW Ellis, Matt McVicar, Eric Battenberg, and Oriol Nieto, \u201clibrosa: Audio and music signal analysis in python,\u201d in Proceedings of the 14th Python in Science Conference, 2015.\n[19] Franc\u0327ois Chollet, \u201cKeras,\u201d GitHub repository: https://github. com/fchollet/keras, 2015.\n[20] The Theano Development Team, Rami Al-Rfou, Guillaume Alain, Amjad Almahairi, Christof Angermueller, Dzmitry Bahdanau, Nicolas Ballas, Fre\u0301de\u0301ric Bastien, Justin Bayer, Anatoly Belikov, et al., \u201cTheano: A python framework for fast computation of mathematical expressions,\u201d arXiv preprint arXiv:1605.02688, 2016.\n[21] Diederik P. Kingma and Jimmy Ba, \u201cAdam: A method for stochastic optimization,\u201d CoRR, vol. abs/1412.6980, 2014."}], "references": [{"title": "End-toend learning for music audio", "author": ["Sander Dieleman", "Benjamin Schrauwen"], "venue": "Acoustics, Speech and Signal Processing (ICASSP), 2014 IEEE International Conference on. IEEE, 2014, pp. 6964\u20136968.", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2014}, {"title": "Automatic tagging using deep convolutional neural networks", "author": ["Keunwoo Choi", "George Fazekas", "Mark Sandler"], "venue": "International Society of Music Information Retrieval Conference. ISMIR, 2016.", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2016}, {"title": "Improved music feature learning with deep neural networks", "author": ["Siddharth Sigtia", "Simon Dixon"], "venue": "2014 IEEE international conference on acoustics, speech and signal processing (ICASSP). IEEE, 2014.", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2014}, {"title": "Hybrid music recommender using content-based and social information", "author": ["Paulo Chiliguano", "Gyorgy Fazekas"], "venue": "2016 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). IEEE, 2016, pp. 2618\u20132622.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2016}, {"title": "Deep content-based music recommendation", "author": ["Aaron Van den Oord", "Sander Dieleman", "Benjamin Schrauwen"], "venue": "Advances in Neural Information Processing Systems, 2013, pp. 2643\u20132651.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2013}, {"title": "Explaining deep convolutional neural networks on music classification", "author": ["Keunwoo Choi", "George Fazekas", "Mark Sandler"], "venue": "arXiv preprint arXiv:1607.02444, 2016.", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2016}, {"title": "Document modeling with gated recurrent neural network for sentiment classification", "author": ["Duyu Tang", "Bing Qin", "Ting Liu"], "venue": "Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, 2015, pp. 1422\u20131432.", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2015}, {"title": "Convolutional recurrent neural networks: Learning spatial dependencies for image representation", "author": ["Zhen Zuo", "Bing Shuai", "Gang Wang", "Xiao Liu", "Xingxing Wang", "Bing Wang", "Yushi Chen"], "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition Workshops, 2015, pp. 18\u201326.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2015}, {"title": "An end-to-end neural network for polyphonic piano music transcription", "author": ["Siddharth Sigtia", "Emmanouil Benetos", "Simon Dixon"], "venue": "IEEE/ACM Transactions on Audio, Speech, and Language Processing, vol. 24, no. 5, 2016.", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2016}, {"title": "Batch normalization: Accelerating deep network training by reducing internal covariate shift", "author": ["Sergey Ioffe", "Christian Szegedy"], "venue": "arXiv preprint arXiv:1502.03167, 2015.", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2015}, {"title": "Fast and accurate deep network learning by exponential linear units (elus)", "author": ["Djork-Arn\u00e9 Clevert", "Thomas Unterthiner", "Sepp Hochreiter"], "venue": "arXiv preprint arXiv:1511.07289, 2015.", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2015}, {"title": "Dropout: A simple way to prevent neural networks from overfitting", "author": ["Nitish Srivastava", "Geoffrey Hinton", "Alex Krizhevsky", "Ilya Sutskever", "Ruslan Salakhutdinov"], "venue": "The Journal of Machine Learning Research, vol. 15, no. 1, pp. 1929\u20131958, 2014.", "citeRegEx": "12", "shortCiteRegEx": null, "year": 1929}, {"title": "Unsupervised learning of local features for music classification", "author": ["Jan W\u00fclfing", "Martin Riedmiller"], "venue": "International Society of Music Information Retrieval Conference. ISMIR, 2012, pp. 139\u2013144.", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2012}, {"title": "Learning to pinpoint singing voice from weakly labeled examples", "author": ["Jan Schl\u00fcter"], "venue": "International Society of Music Information Retrieval Conference. ISMIR, 2016.", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2016}, {"title": "On the properties of neural machine translation: Encoder-decoder approaches", "author": ["Kyunghyun Cho", "Bart Van Merri\u00ebnboer", "Dzmitry Bahdanau", "Yoshua Bengio"], "venue": "arXiv preprint arXiv:1409.1259, 2014.", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2014}, {"title": "The power of depth for feedforward neural networks", "author": ["Ronen Eldan", "Ohad Shamir"], "venue": "arXiv preprint arXiv:1512.03965, 2015.", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2015}, {"title": "The million song dataset", "author": ["Thierry Bertin-Mahieux", "Daniel PW Ellis", "Brian Whitman", "Paul Lamere"], "venue": "Proceedings of the 12th International Society for Music Information Retrieval Conference, Miami, Florida, USA, October 24-28, 2011, 2011.", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2011}, {"title": "librosa: Audio and music signal analysis in python", "author": ["Brian McFee", "Colin Raffel", "Dawen Liang", "Daniel PW Ellis", "Matt McVicar", "Eric Battenberg", "Oriol Nieto"], "venue": "Proceedings of the 14th Python in Science Conference, 2015.", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2015}, {"title": "Keras", "author": ["Fran\u00e7ois Chollet"], "venue": "GitHub repository: https://github. com/fchollet/keras, 2015.", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2015}, {"title": "Theano: A python framework for fast computation of mathematical expressions", "author": ["The Theano Development Team", "Rami Al-Rfou", "Guillaume Alain", "Amjad Almahairi", "Christof Angermueller", "Dzmitry Bahdanau", "Nicolas Ballas", "Fr\u00e9d\u00e9ric Bastien", "Justin Bayer", "Anatoly Belikov"], "venue": "arXiv preprint arXiv:1605.02688, 2016.", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2016}, {"title": "Adam: A method for stochastic optimization", "author": ["Diederik P. Kingma", "Jimmy Ba"], "venue": "CoRR, vol. abs/1412.6980, 2014.", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2014}], "referenceMentions": [{"referenceID": 0, "context": "Convolutional neural networks (CNNs) have been actively used for various music classification tasks such as music tagging [1, 2], genre classification [3, 4], and user-item latent feature prediction for recommendation [5].", "startOffset": 122, "endOffset": 128}, {"referenceID": 1, "context": "Convolutional neural networks (CNNs) have been actively used for various music classification tasks such as music tagging [1, 2], genre classification [3, 4], and user-item latent feature prediction for recommendation [5].", "startOffset": 122, "endOffset": 128}, {"referenceID": 2, "context": "Convolutional neural networks (CNNs) have been actively used for various music classification tasks such as music tagging [1, 2], genre classification [3, 4], and user-item latent feature prediction for recommendation [5].", "startOffset": 151, "endOffset": 157}, {"referenceID": 3, "context": "Convolutional neural networks (CNNs) have been actively used for various music classification tasks such as music tagging [1, 2], genre classification [3, 4], and user-item latent feature prediction for recommendation [5].", "startOffset": 151, "endOffset": 157}, {"referenceID": 4, "context": "Convolutional neural networks (CNNs) have been actively used for various music classification tasks such as music tagging [1, 2], genre classification [3, 4], and user-item latent feature prediction for recommendation [5].", "startOffset": 218, "endOffset": 221}, {"referenceID": 5, "context": ", percussive instrument patterns) [6].", "startOffset": 34, "endOffset": 37}, {"referenceID": 6, "context": "This structure was first proposed in [7] for document classification and later applied to image classification [8] and music transcription [9].", "startOffset": 37, "endOffset": 40}, {"referenceID": 7, "context": "This structure was first proposed in [7] for document classification and later applied to image classification [8] and music transcription [9].", "startOffset": 111, "endOffset": 114}, {"referenceID": 8, "context": "This structure was first proposed in [7] for document classification and later applied to image classification [8] and music transcription [9].", "startOffset": 139, "endOffset": 142}, {"referenceID": 9, "context": "For example, techniques such as batch normalization (BN) [10] and exponential linear unit (ELU) [11] improve both training speed and accuracy by nudging the distribution of the outputs of intermediate layers to be standardised without changing the structure.", "startOffset": 57, "endOffset": 61}, {"referenceID": 10, "context": "For example, techniques such as batch normalization (BN) [10] and exponential linear unit (ELU) [11] improve both training speed and accuracy by nudging the distribution of the outputs of intermediate layers to be standardised without changing the structure.", "startOffset": 96, "endOffset": 100}, {"referenceID": 11, "context": "5 [12], batch normalization [10], and ELU activation function [11].", "startOffset": 2, "endOffset": 6}, {"referenceID": 9, "context": "5 [12], batch normalization [10], and ELU activation function [11].", "startOffset": 28, "endOffset": 32}, {"referenceID": 10, "context": "5 [12], batch normalization [10], and ELU activation function [11].", "startOffset": 62, "endOffset": 66}, {"referenceID": 0, "context": "Conv1D in Figure 1a is motivated by structures for music tagging [1] and genre classification [13].", "startOffset": 65, "endOffset": 68}, {"referenceID": 12, "context": "Conv1D in Figure 1a is motivated by structures for music tagging [1] and genre classification [13].", "startOffset": 94, "endOffset": 98}, {"referenceID": 0, "context": "In [1], 29s-long signal are trimmed into less than 4-sec subsegments, and then the final tag predictions are averaged.", "startOffset": 3, "endOffset": 6}, {"referenceID": 1, "context": "CNN structures with two-dimensional convolution have been used in music tagging [2] and vocal/instrumental classification [14].", "startOffset": 80, "endOffset": 83}, {"referenceID": 13, "context": "CNN structures with two-dimensional convolution have been used in music tagging [2] and vocal/instrumental classification [14].", "startOffset": 122, "endOffset": 126}, {"referenceID": 14, "context": "CRNN CRNN uses a 2-layer RNN with gated recurrent units (GRU) [15] to summarise temporal patterns on the top of twodimensional 4-layer CNNs as shown in Figure 1c.", "startOffset": 62, "endOffset": 66}, {"referenceID": 6, "context": "This structure is first proposed in [7] for document classification.", "startOffset": 36, "endOffset": 39}, {"referenceID": 0, "context": "In CRNN, RNNs are used to aggregate the temporal patterns instead of, for instance, averaging the results from shorter segments as in [1] or convolution and sub-sampling as in Conv2D.", "startOffset": 134, "endOffset": 137}, {"referenceID": 0, "context": "The widths of layers are based on [1] for Conv1D and [2] for Conv2D.", "startOffset": 34, "endOffset": 37}, {"referenceID": 1, "context": "The widths of layers are based on [1] for Conv1D and [2] for Conv2D.", "startOffset": 53, "endOffset": 56}, {"referenceID": 15, "context": "This is to maximise the representation capabilities of networks, considering the relative importance of depth over width [16].", "startOffset": 121, "endOffset": 125}, {"referenceID": 16, "context": "EXPERIMENTS We use the Million Song Dataset [17] with last.", "startOffset": 44, "endOffset": 48}, {"referenceID": 17, "context": "05 kHz to 12 kHz using Librosa [18].", "startOffset": 31, "endOffset": 35}, {"referenceID": 1, "context": "Log-amplitude mel-spectrograms are used as input since they have outperformed STFT and MFCCs, and linear-amplitude mel-spectrograms in earlier research [2, 1].", "startOffset": 152, "endOffset": 158}, {"referenceID": 0, "context": "Log-amplitude mel-spectrograms are used as input since they have outperformed STFT and MFCCs, and linear-amplitude mel-spectrograms in earlier research [2, 1].", "startOffset": 152, "endOffset": 158}, {"referenceID": 18, "context": "The model is built with Keras [19] and Theano [20].", "startOffset": 30, "endOffset": 34}, {"referenceID": 19, "context": "The model is built with Keras [19] and Theano [20].", "startOffset": 46, "endOffset": 50}, {"referenceID": 20, "context": "We use ADAM for learning rate control [21] and binary crossentropy as a loss function.", "startOffset": 38, "endOffset": 42}, {"referenceID": 1, "context": "851 [2].", "startOffset": 4, "endOffset": 7}, {"referenceID": 1, "context": "parameter structures, Conv2D outperforms CRNN while both of them outperform the state-of-the-art structure [2].", "startOffset": 107, "endOffset": 110}], "year": 2016, "abstractText": "We introduce a convolutional recurrent neural network (CRNN) for music tagging. CRNNs take advantage of convolutional neural networks (CNNs) for local feature extraction and recurrent neural networks for temporal summarisation of the extracted features. We compare CRNN with two CNN structures that have been used for music tagging while controlling the number of parameters with respect to their performance and training time per sample. Overall, we found that CRNNs show strong performance with respect to the number of parameter and training time, indicating the effectiveness of its hybrid structure in music feature extraction and feature summarisation.", "creator": "LaTeX with hyperref package"}}}