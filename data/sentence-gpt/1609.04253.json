{"id": "1609.04253", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "14-Sep-2016", "title": "Neural Machine Transliteration: Preliminary Results", "abstract": "Machine transliteration is the process of automatically transforming the script of a word from a source language to a target language, while preserving pronunciation. Sequence to sequence learning has recently emerged as a new paradigm in supervised learning. In this paper a character-based encoder-decoder model has been proposed that consists of two Recurrent Neural Networks. The encoder is a Bidirectional recurrent neural network that encodes a sequence of symbols into a fixed-length vector representation, and the decoder generates the target sequence using an attention-based recurrent neural network. The encoder, the decoder and the attention mechanism are jointly trained to maximize the conditional probability of a target sequence given a source sequence. Our experiments on different datasets show that the proposed encoder-decoder model is able to achieve significantly higher transliteration quality over traditional statistical models.\n\n\n\n\nIn this paper we use data from the corpus of 150 million Chinese speakers as part of an extensive series of randomized, random-effects control trials. The encoder was tested using an open-source Python library. The encoder was tested with a set of supervised training and two supervised (random-effects) conditions. The encoder was able to produce a \"super\" average of 0.9% of the variance in a sentence as presented in the Figure 1.\n\n\n\nWe have used Python and C++ libraries to evaluate the effectiveness of this model.\nOur main aim is to create a high-quality decoder that can be developed with a set of trained trained neural networks.\nThe encoder is used as an input training algorithm for training in training environments.\nWe propose a similar encoding algorithm and a new encoding algorithm, called L2A and L3A, to encode sequence-related information on input and output.\nThe encoder provides a fast, fast encoding system to capture data that is encoded in a single encoded word and then produces the encoded information over one frame. In order to accurately encode the information, we first use the input training algorithm that is designed to ensure that the encoding system can accurately encode each word and all of its encoded input.\nIn order to accurately encode the information, we first use the input training algorithm that is designed to ensure that the encoding system can accurately encode each word and all of its encoded input. In order to accurately encode the information, we first use the input training algorithm that is designed to ensure that the encoding system can accurately encode each word and all of its encoded input. The encoding system consists of a model that", "histories": [["v1", "Wed, 14 Sep 2016 13:12:12 GMT  (63kb,D)", "http://arxiv.org/abs/1609.04253v1", null]], "reviews": [], "SUBJECTS": "cs.CL", "authors": ["amir h jadidinejad"], "accepted": false, "id": "1609.04253"}, "pdf": {"name": "1609.04253.pdf", "metadata": {"source": "CRF", "title": "Neural Machine Transliteration: Preliminary Results", "authors": ["Amir H. Jadidinejad"], "emails": ["jadidinejad@bayan.co.ir"], "sections": [{"heading": "1 Introduction", "text": "Machine Transliteration is defined as phonetic transformation of names across languages (Zhang et al., 2015; Karimi et al., 2011). Transliteration of named entities is the essential part of many multilingual applications, such as machine translation (Koehn, 2010) and cross-language information retrieval (Jadidinejad and Mahmoudi, 2010).\nRecent studies pay a great attention to the task of Neural Machine Translation (Cho et al., 2014a; Sutskever et al., 2014). In neural machine translation, a single neural network is responsible for reading a source sentence and generates its trans-\nlation. From a probabilistic perspective, translation is equivalent to finding a target sentence y that maximizes the conditional probability of y given a source sentence x, i.e., argmaxy p(y | x). The whole neural network is jointly trained to maximize the conditional probability of a correct translation given a source sentence, using the bilingual corpus.\nTransforming a name from spelling to phonetic and then use the constructed phonetic to generate the spelling on the target language is a very complex task (Oh et al., 2006; Finch et al., 2015). Based on successful studies on Neural Machine Translation (Cho et al., 2014a; Sutskever et al., 2014; Hirschberg and Manning, 2015), in this paper, we proposed a character-based encoder\u2013 decoder model which learn to transliterate endto-end. In the opposite side of classical models which contains different components, the proposed model is trained end-to-end, so it able to apply to any language pairs without tuning for a spacific one."}, {"heading": "2 Proposed Model", "text": "Here, we describe briefly the underlying framework, called RNN Encoder\u2013Decoder, proposed by (Cho et al., 2014b) and (Sutskever et al., 2014) upon which we build a machine transliteration model that learns to transliterate end-to-end.\nThe enoder is a character-based recurrent neural network that learns a highly nonlinear mapping from a spelling to the phonetic of the input sequence. This network reads the source name x = (x1, . . . , xT ) and encodes it into a sequence of hidden states h = (h1, \u00b7 \u00b7 \u00b7 , hT ):\nht = f (xt, ht\u22121) (1)\nEach hidden state hi is a bidirectional recurrent representation with forward and backward sequence information around the ith character. The\nar X\niv :1\n60 9.\n04 25\n3v 1\n[ cs\n.C L\n] 1\n4 Se\np 20\n16\nrepresentation of a forward sequence and a backward sequence of the input character sequence is estimated and concatenated to form a context set C = {h1, h2, ..., hT } (Dong et al., 2015; Chung et al., 2016). Then, the decoder, another recurrent neural network, computes the conditional distribution over all possible transliteration based on this context set and generates the corresponding transliteration y = (y1, \u00b7 \u00b7 \u00b7 , yT \u2032) based on the encoded sequence of hidden states h.\nThe whole model is jointly trained to maximize the conditional log-probability of the correct transliteration given a source sequence with respect to the parameters \u03b8 of the model:\n\u03b8\u2217 = argmax \u03b8 N\u2211 n=1 Tn\u2211 t=1 log p(ynt | yn<t, xn), (2)\nwhere (xn, yn) is the n-th training pair of character sequences, and Tn is the length of the n-th target sequence (yn). For each conditional term in Equation 2, the decoder updates its hidden state by:\nht\u2032 = f (yt\u2032\u22121, ht\u2032\u22121, ct\u2032) (3)\nwhere ct\u2032 is a context vector computed by a soft attention mechanism:\nct\u2032 = fa (yt\u2032\u22121, ht\u2032\u22121, C) (4)\nThe soft attention mechanism fa weights each vector in the context set C according to its relevance given what has been transliterated.\nFinally, the hidden state ht\u2032 , together with the previous target symbol yt\u2032\u22121 and the context vector ct\u2032 , is fed into a feedforward neural network to result in the conditional distribution described in Equation 2. The whole model, consisting of the encoder, decoder and soft attention mechanism, is trained end-to-end to minimize the negative loglikelihood using stochastic gradient descent."}, {"heading": "3 Experiments", "text": "We conducted a set of experiments to show the effectiveness of RNN Encoder\u2013Decoder model (Cho et al., 2014b; Sutskever et al., 2014) in the task of machine transliteration using standard benchmark datasets provided by NEWS 2015-16 shared task (Banchs et al., 2015). Table 1 shows different datasets in our experiments. Each dataset covers different levels of difficulty and training set size. The proposed model has been applied on\neach dataset without tuning the algorithm for each specific language pairs. Also, we don\u2019t apply any preprocessing on the source or target language in order to evaluate the effectiveness of the proposed model in a fair situation. \u2018TaskID\u2019 is a unique identifier in the following experiments.\nWe leveraged a character-based encoder\u2013 decoder model (Bojanowski et al., 2015; Chung et al., 2016) with soft attention mechanism (Cho et al., 2014b). In this model, input sequences in both source and target languages have been represented as characters. Using characters instead of words leads to longer sequences, so Gated Recurrent Units (Cho et al., 2014a) have been used for the encoder network to model long term dependencies. The encoder has 128 hidden units for each direction (forward and backward), and the decoder has 128 hidden units with soft attention mechanism (Cho et al., 2014b). We train the model using stochastic gradient descent with Adam (Kingma and Ba, 2014). Each update is computed using a minibatch of 128 sequence pairs. The norm of the gradient is clipped with a threshold 1 (Pascanu et al., 2013). Also, beamsearch has been used to approximately find the most likely transliteration given a source sequence (Koehn, 2010).\nTable 2 shows the effectiveness of the proposed model on different datasets using standard measures (Banchs et al., 2015). The proposed neural machine transliteration model has been compared to the baseline method provided by NEWS 2016 organizers (Banchs et al., 2015). Baseline results are based on a machine translation implementation at the character level using MOSES (Koehn et al., 2007). Experimental results shows that the proposed model is significantly better than the robust baseline using different metrics.\nFigure 1 shows the learning curve of the pro-\nposed model on different datasets. It is clear that in most datasets, the trained model is capable of robust transliteration after a few number of iterations. As shown in Table 1, each dataset has different number of training set and also different number of characters in the source and target language. For example, when transliterating from English to Chinese (TaskID=\u2018En-Ch\u2019) and English to Hebrew, the target names contains 548 and 37 different tokens respectively. Since we leverage a same model for different datasets without tuning the model for each dataset, differences in the learning curves are expectable. For some datasets (such as \u2018En-Ch\u2019), it takes more time to fit the model to the training data while for some others (such as \u2018En-He\u2019), the model fit to the training data after a few iterations."}, {"heading": "4 Conclusion", "text": "In this paper we proposed Neural Machine Transliteration based on successful studies in sequence to sequence learning (Sutskever et al., 2014) and Neural Machine Translation (Ling et al., 2015; Costa-Jussa\u0300 and Fonollosa, 2016; Bahdanau et al., 2015; Cho et al., 2014a). Neural Machine Transliteration typically consists of two components, the first of which encodes a source name sequence x and the second decodes to a target name sequence y. Different parts of the proposed model jointly trained using stochastic gradient descent to minimize the log-likelihood. Experiments on different datasets using benchmark measures revealed that the proposed model is able to achieve significantly higher transliteration quality over traditional statistical models (Koehn, 2010). In this paper we did not concentrate on improving the model for achieving state-of-the-art results, so applying hyperparameter optimization (Bergstra and Bengio, 2012), multi-task sequence to sequence learning (Luong et al., 2015) and multiway transliteration (Firat et al., 2016; Dong et al., 2015) are quite promising for future works."}, {"heading": "Acknowledgments", "text": "The authors would like to thank the developers of Theano (Theano Development Team, 2016) and DL4MT 1 projects. Also, the author would like to acknowledge the support of Bayan Inc. for research funding and computing support. The author\n1https://github.com/nyu-dl/ dl4mt-tutorial\nalso thank Yasser Souri for valuable comments."}], "references": [{"title": "Neural machine translation by jointly learning to align and translate", "author": ["Dzmitry Bahdanau", "Kyunghyun Cho", "Yoshua Bengio."], "venue": "International Conference on Learning Representations, volume abs/1409.0473.", "citeRegEx": "Bahdanau et al\\.,? 2015", "shortCiteRegEx": "Bahdanau et al\\.", "year": 2015}, {"title": "Report of news 2015 machine transliteration shared task", "author": ["Rafael E. Banchs", "Min Zhang", "Xiangyu Duan", "Haizhou Li", "A. Kumaran."], "venue": "Proceedings of the Fifth Named Entity Workshop, pages 10\u201323, Beijing, China, July. Association for Computational", "citeRegEx": "Banchs et al\\.,? 2015", "shortCiteRegEx": "Banchs et al\\.", "year": 2015}, {"title": "Random search for hyper-parameter optimization", "author": ["James Bergstra", "Yoshua Bengio."], "venue": "J. Mach. Learn. Res., 13(1):281\u2013305, February.", "citeRegEx": "Bergstra and Bengio.,? 2012", "shortCiteRegEx": "Bergstra and Bengio.", "year": 2012}, {"title": "Alternative structures for character-level rnns", "author": ["Piotr Bojanowski", "Armand Joulin", "Tomas Mikolov."], "venue": "arXiv preprint arXiv:1511.06303.", "citeRegEx": "Bojanowski et al\\.,? 2015", "shortCiteRegEx": "Bojanowski et al\\.", "year": 2015}, {"title": "On the properties of neural machine translation: Encoder\u2013decoder approaches", "author": ["Kyunghyun Cho", "Bart van Merrienboer", "Dzmitry Bahdanau", "Yoshua Bengio."], "venue": "Proceedings of SSST-8, Eighth Workshop on Syntax, Semantics and Structure in Statisti-", "citeRegEx": "Cho et al\\.,? 2014a", "shortCiteRegEx": "Cho et al\\.", "year": 2014}, {"title": "Learning phrase representations using rnn encoder\u2013decoder for statistical machine translation", "author": ["Kyunghyun Cho", "Bart van Merrienboer", "Caglar Gulcehre", "Dzmitry Bahdanau", "Fethi Bougares", "Holger Schwenk", "Yoshua Bengio."], "venue": "Proceedings of", "citeRegEx": "Cho et al\\.,? 2014b", "shortCiteRegEx": "Cho et al\\.", "year": 2014}, {"title": "A character-level decoder without explicit segmentation for neural machine translation", "author": ["Junyoung Chung", "Kyunghyun Cho", "Yoshua Bengio."], "venue": "CoRR, abs/1603.06147.", "citeRegEx": "Chung et al\\.,? 2016", "shortCiteRegEx": "Chung et al\\.", "year": 2016}, {"title": "Character-based neural machine translation", "author": ["Marta R. Costa-Juss\u00e0", "Jos\u00e9 A.R. Fonollosa."], "venue": "CoRR, abs/1603.00810.", "citeRegEx": "Costa.Juss\u00e0 and Fonollosa.,? 2016", "shortCiteRegEx": "Costa.Juss\u00e0 and Fonollosa.", "year": 2016}, {"title": "Multi-task learning for multiple language translation", "author": ["Daxiang Dong", "Hua Wu", "Wei He", "Dianhai Yu", "Haifeng Wang."], "venue": "Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint", "citeRegEx": "Dong et al\\.,? 2015", "shortCiteRegEx": "Dong et al\\.", "year": 2015}, {"title": "Neural network transduction models", "author": ["Andrew Finch", "Lemao Liu", "Xiaolin Wang", "Eiichiro Sumita"], "venue": null, "citeRegEx": "Finch et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Finch et al\\.", "year": 2015}, {"title": "MultiWay, Multilingual Neural Machine Translation with a Shared Attention Mechanism", "author": ["O. Firat", "K. Cho", "Y. Bengio."], "venue": "ArXiv e-prints, January.", "citeRegEx": "Firat et al\\.,? 2016", "shortCiteRegEx": "Firat et al\\.", "year": 2016}, {"title": "Advances in natural language processing", "author": ["Julia Hirschberg", "Christopher D. Manning."], "venue": "Science, 349(6245):261\u2013266.", "citeRegEx": "Hirschberg and Manning.,? 2015", "shortCiteRegEx": "Hirschberg and Manning.", "year": 2015}, {"title": "Cross-language information retrieval using meta-language index construction and structural queries", "author": ["AmirHossein Jadidinejad", "Fariborz Mahmoudi."], "venue": "Carol Peters, GiorgioMaria Di Nunzio, Mikko Kurimo, Thomas Mandl, Djamel Mostefa,", "citeRegEx": "Jadidinejad and Mahmoudi.,? 2010", "shortCiteRegEx": "Jadidinejad and Mahmoudi.", "year": 2010}, {"title": "Machine transliteration survey", "author": ["Sarvnaz Karimi", "Falk Scholer", "Andrew Turpin."], "venue": "ACM Comput. Surv., 43(3):17:1\u201317:46, April.", "citeRegEx": "Karimi et al\\.,? 2011", "shortCiteRegEx": "Karimi et al\\.", "year": 2011}, {"title": "Adam: A method for stochastic optimization", "author": ["Diederik P. Kingma", "Jimmy Ba."], "venue": "CoRR, abs/1412.6980.", "citeRegEx": "Kingma and Ba.,? 2014", "shortCiteRegEx": "Kingma and Ba.", "year": 2014}, {"title": "Statistical Machine Translation", "author": ["Philipp Koehn."], "venue": "Cambridge University Press, New York, NY, USA, 1st edition.", "citeRegEx": "Koehn.,? 2010", "shortCiteRegEx": "Koehn.", "year": 2010}, {"title": "Character-based neural machine translation", "author": ["Wang Ling", "Isabel Trancoso", "Chris Dyer", "Alan W. Black."], "venue": "CoRR, abs/1511.04586.", "citeRegEx": "Ling et al\\.,? 2015", "shortCiteRegEx": "Ling et al\\.", "year": 2015}, {"title": "Multitask sequence to sequence learning", "author": ["Minh-Thang Luong", "Quoc V. Le", "Ilya Sutskever", "Oriol Vinyals", "Lukasz Kaiser."], "venue": "CoRR, abs/1511.06114.", "citeRegEx": "Luong et al\\.,? 2015", "shortCiteRegEx": "Luong et al\\.", "year": 2015}, {"title": "A comparison of different machine transliteration models", "author": ["Jong-Hoon Oh", "Key-Sun Choi", "Hitoshi Isahara."], "venue": "J. Artif. Intell. Res. (JAIR), 27:119\u2013 151.", "citeRegEx": "Oh et al\\.,? 2006", "shortCiteRegEx": "Oh et al\\.", "year": 2006}, {"title": "How to construct deep recurrent neural networks", "author": ["Razvan Pascanu", "\u00c7aglar G\u00fcl\u00e7ehre", "Kyunghyun Cho", "Yoshua Bengio."], "venue": "CoRR, abs/1312.6026.", "citeRegEx": "Pascanu et al\\.,? 2013", "shortCiteRegEx": "Pascanu et al\\.", "year": 2013}, {"title": "Sequence to sequence learning with neural networks", "author": ["Ilya Sutskever", "Oriol Vinyals", "Quoc V. Le."], "venue": "CoRR, abs/1409.3215.", "citeRegEx": "Sutskever et al\\.,? 2014", "shortCiteRegEx": "Sutskever et al\\.", "year": 2014}, {"title": "Theano: A Python framework for fast computation of mathematical expressions", "author": ["Theano Development Team."], "venue": "arXiv e-prints, abs/1605.02688, May.", "citeRegEx": "Team.,? 2016", "shortCiteRegEx": "Team.", "year": 2016}, {"title": "Whitepaper of news 2015 shared task on machine transliteration", "author": ["Min Zhang", "Haizhou Li", "Rafael E. Banchs", "A. Kumaran."], "venue": "Proceedings of the Fifth Named Entity Workshop, pages 1\u20139, Beijing, China, July. Association for Computational Linguis-", "citeRegEx": "Zhang et al\\.,? 2015", "shortCiteRegEx": "Zhang et al\\.", "year": 2015}], "referenceMentions": [{"referenceID": 22, "context": "Machine Transliteration is defined as phonetic transformation of names across languages (Zhang et al., 2015; Karimi et al., 2011).", "startOffset": 88, "endOffset": 129}, {"referenceID": 13, "context": "Machine Transliteration is defined as phonetic transformation of names across languages (Zhang et al., 2015; Karimi et al., 2011).", "startOffset": 88, "endOffset": 129}, {"referenceID": 15, "context": "Transliteration of named entities is the essential part of many multilingual applications, such as machine translation (Koehn, 2010) and cross-language information retrieval (Jadidinejad and Mahmoudi, 2010).", "startOffset": 119, "endOffset": 132}, {"referenceID": 12, "context": "Transliteration of named entities is the essential part of many multilingual applications, such as machine translation (Koehn, 2010) and cross-language information retrieval (Jadidinejad and Mahmoudi, 2010).", "startOffset": 174, "endOffset": 206}, {"referenceID": 4, "context": "Recent studies pay a great attention to the task of Neural Machine Translation (Cho et al., 2014a; Sutskever et al., 2014).", "startOffset": 79, "endOffset": 122}, {"referenceID": 20, "context": "Recent studies pay a great attention to the task of Neural Machine Translation (Cho et al., 2014a; Sutskever et al., 2014).", "startOffset": 79, "endOffset": 122}, {"referenceID": 18, "context": "Transforming a name from spelling to phonetic and then use the constructed phonetic to generate the spelling on the target language is a very complex task (Oh et al., 2006; Finch et al., 2015).", "startOffset": 155, "endOffset": 192}, {"referenceID": 9, "context": "Transforming a name from spelling to phonetic and then use the constructed phonetic to generate the spelling on the target language is a very complex task (Oh et al., 2006; Finch et al., 2015).", "startOffset": 155, "endOffset": 192}, {"referenceID": 4, "context": "Based on successful studies on Neural Machine Translation (Cho et al., 2014a; Sutskever et al., 2014; Hirschberg and Manning, 2015), in this paper, we proposed a character-based encoder\u2013 decoder model which learn to transliterate endto-end.", "startOffset": 58, "endOffset": 131}, {"referenceID": 20, "context": "Based on successful studies on Neural Machine Translation (Cho et al., 2014a; Sutskever et al., 2014; Hirschberg and Manning, 2015), in this paper, we proposed a character-based encoder\u2013 decoder model which learn to transliterate endto-end.", "startOffset": 58, "endOffset": 131}, {"referenceID": 11, "context": "Based on successful studies on Neural Machine Translation (Cho et al., 2014a; Sutskever et al., 2014; Hirschberg and Manning, 2015), in this paper, we proposed a character-based encoder\u2013 decoder model which learn to transliterate endto-end.", "startOffset": 58, "endOffset": 131}, {"referenceID": 5, "context": "Here, we describe briefly the underlying framework, called RNN Encoder\u2013Decoder, proposed by (Cho et al., 2014b) and (Sutskever et al.", "startOffset": 92, "endOffset": 111}, {"referenceID": 20, "context": ", 2014b) and (Sutskever et al., 2014) upon which we build a machine transliteration model that learns to transliterate end-to-end.", "startOffset": 13, "endOffset": 37}, {"referenceID": 8, "context": ", hT } (Dong et al., 2015; Chung et al., 2016).", "startOffset": 7, "endOffset": 46}, {"referenceID": 6, "context": ", hT } (Dong et al., 2015; Chung et al., 2016).", "startOffset": 7, "endOffset": 46}, {"referenceID": 5, "context": "We conducted a set of experiments to show the effectiveness of RNN Encoder\u2013Decoder model (Cho et al., 2014b; Sutskever et al., 2014) in the task of machine transliteration using standard benchmark datasets provided by NEWS 2015-16 shared task (Banchs et al.", "startOffset": 89, "endOffset": 132}, {"referenceID": 20, "context": "We conducted a set of experiments to show the effectiveness of RNN Encoder\u2013Decoder model (Cho et al., 2014b; Sutskever et al., 2014) in the task of machine transliteration using standard benchmark datasets provided by NEWS 2015-16 shared task (Banchs et al.", "startOffset": 89, "endOffset": 132}, {"referenceID": 1, "context": ", 2014) in the task of machine transliteration using standard benchmark datasets provided by NEWS 2015-16 shared task (Banchs et al., 2015).", "startOffset": 118, "endOffset": 139}, {"referenceID": 1, "context": "Table 1: Datasets provided by NEWS 2015 (Banchs et al., 2015).", "startOffset": 40, "endOffset": 61}, {"referenceID": 3, "context": "We leveraged a character-based encoder\u2013 decoder model (Bojanowski et al., 2015; Chung et al., 2016) with soft attention mechanism (Cho et al.", "startOffset": 54, "endOffset": 99}, {"referenceID": 6, "context": "We leveraged a character-based encoder\u2013 decoder model (Bojanowski et al., 2015; Chung et al., 2016) with soft attention mechanism (Cho et al.", "startOffset": 54, "endOffset": 99}, {"referenceID": 5, "context": ", 2016) with soft attention mechanism (Cho et al., 2014b).", "startOffset": 38, "endOffset": 57}, {"referenceID": 4, "context": "Using characters instead of words leads to longer sequences, so Gated Recurrent Units (Cho et al., 2014a) have been used for the encoder network to model long term dependencies.", "startOffset": 86, "endOffset": 105}, {"referenceID": 5, "context": "The encoder has 128 hidden units for each direction (forward and backward), and the decoder has 128 hidden units with soft attention mechanism (Cho et al., 2014b).", "startOffset": 143, "endOffset": 162}, {"referenceID": 14, "context": "We train the model using stochastic gradient descent with Adam (Kingma and Ba, 2014).", "startOffset": 63, "endOffset": 84}, {"referenceID": 19, "context": "The norm of the gradient is clipped with a threshold 1 (Pascanu et al., 2013).", "startOffset": 55, "endOffset": 77}, {"referenceID": 15, "context": "Also, beamsearch has been used to approximately find the most likely transliteration given a source sequence (Koehn, 2010).", "startOffset": 109, "endOffset": 122}, {"referenceID": 1, "context": "Table 2 shows the effectiveness of the proposed model on different datasets using standard measures (Banchs et al., 2015).", "startOffset": 100, "endOffset": 121}, {"referenceID": 1, "context": "The proposed neural machine transliteration model has been compared to the baseline method provided by NEWS 2016 organizers (Banchs et al., 2015).", "startOffset": 124, "endOffset": 145}, {"referenceID": 20, "context": "In this paper we proposed Neural Machine Transliteration based on successful studies in sequence to sequence learning (Sutskever et al., 2014) and Neural Machine Translation (Ling et al.", "startOffset": 118, "endOffset": 142}, {"referenceID": 16, "context": ", 2014) and Neural Machine Translation (Ling et al., 2015; Costa-Juss\u00e0 and Fonollosa, 2016; Bahdanau et al., 2015; Cho et al., 2014a).", "startOffset": 39, "endOffset": 133}, {"referenceID": 7, "context": ", 2014) and Neural Machine Translation (Ling et al., 2015; Costa-Juss\u00e0 and Fonollosa, 2016; Bahdanau et al., 2015; Cho et al., 2014a).", "startOffset": 39, "endOffset": 133}, {"referenceID": 0, "context": ", 2014) and Neural Machine Translation (Ling et al., 2015; Costa-Juss\u00e0 and Fonollosa, 2016; Bahdanau et al., 2015; Cho et al., 2014a).", "startOffset": 39, "endOffset": 133}, {"referenceID": 4, "context": ", 2014) and Neural Machine Translation (Ling et al., 2015; Costa-Juss\u00e0 and Fonollosa, 2016; Bahdanau et al., 2015; Cho et al., 2014a).", "startOffset": 39, "endOffset": 133}, {"referenceID": 15, "context": "Experiments on different datasets using benchmark measures revealed that the proposed model is able to achieve significantly higher transliteration quality over traditional statistical models (Koehn, 2010).", "startOffset": 192, "endOffset": 205}, {"referenceID": 2, "context": "In this paper we did not concentrate on improving the model for achieving state-of-the-art results, so applying hyperparameter optimization (Bergstra and Bengio, 2012), multi-task sequence to sequence learning (Luong et al.", "startOffset": 140, "endOffset": 167}, {"referenceID": 17, "context": "In this paper we did not concentrate on improving the model for achieving state-of-the-art results, so applying hyperparameter optimization (Bergstra and Bengio, 2012), multi-task sequence to sequence learning (Luong et al., 2015) and multiway transliteration (Firat et al.", "startOffset": 210, "endOffset": 230}, {"referenceID": 10, "context": ", 2015) and multiway transliteration (Firat et al., 2016; Dong et al., 2015) are quite promising for future works.", "startOffset": 37, "endOffset": 76}, {"referenceID": 8, "context": ", 2015) and multiway transliteration (Firat et al., 2016; Dong et al., 2015) are quite promising for future works.", "startOffset": 37, "endOffset": 76}], "year": 2016, "abstractText": "Machine transliteration is the process of automatically transforming the script of a word from a source language to a target language, while preserving pronunciation. Sequence to sequence learning has recently emerged as a new paradigm in supervised learning. In this paper a character-based encoder-decoder model has been proposed that consists of two Recurrent Neural Networks. The encoder is a Bidirectional recurrent neural network that encodes a sequence of symbols into a fixed-length vector representation, and the decoder generates the target sequence using an attention-based recurrent neural network. The encoder, the decoder and the attention mechanism are jointly trained to maximize the conditional probability of a target sequence given a source sequence. Our experiments on different datasets show that the proposed encoderdecoder model is able to achieve significantly higher transliteration quality over traditional statistical models.", "creator": "TeX"}}}