{"id": "1609.04508", "review": {"conference": "AAAI", "VERSION": "v1", "DATE_OF_SUBMISSION": "15-Sep-2016", "title": "Column Networks for Collective Classification", "abstract": "Relational learning deals with data that are characterized by relational structures. An important task is collective classification, which is to jointly classify networked objects. While it holds a great promise to produce a better accuracy than non-collective classifiers, collective classification is computational challenging and has not leveraged on the recent breakthroughs of deep learning to solve complex systems.\n\n\n\n\n\nTo summarize, we consider a set of three ways to learn about a set of cognitive networks:\nA hierarchical model of the network:\nAn intuitive hierarchical model of the network:\nA hierarchical model of the network:\nAn intuitive hierarchical model of the network:\nA hierarchical model of the network:\nAn intuitive hierarchical model of the network:\nA hierarchical model of the network:\nAn intuitive hierarchical model of the network:\nAn intuitive hierarchical model of the network:\nA intuitive hierarchical model of the network:\nAn intuitive hierarchical model of the network:\nA intuitive hierarchical model of the network:\nAn intuitive hierarchical model of the network:\nAn intuitive hierarchical model of the network:\nAn intuitive hierarchical model of the network:\nAn intuitive hierarchical model of the network:\nAn intuitive hierarchical model of the network:\nAn intuitive hierarchical model of the network:\nAn intuitive hierarchical model of the network:\nAn intuitive hierarchical model of the network:\nAn intuitive hierarchical model of the network:\nAn intuitive hierarchical model of the network:\nAn intuitive hierarchical model of the network:\nAn intuitive hierarchical model of the network:\nAn intuitive hierarchical model of the network:\nAn intuitive hierarchical model of the network:\nAn intuitive hierarchical model of the network:\nAn intuitive hierarchical model of the network:\nAn intuitive hierarchical model of the network:\nAn intuitive hierarchical model of the network:\nAn intuitive hierarchical model of the network:\nAn intuitive hierarchical model of the network:\nAn intuitive hierarchical model of the network:\nAn intuitive hierarchical model of the network:\nAn intuitive hierarchical model of the network:\nAn intuitive hierarchical model of the network:\nAn intuitive hierarchical model of the network:\nAn intuitive hierarchical model of the network:\nAn intuitive hierarchical model of the network:\nAn intuitive hierarchical model of the network:\nAn intuitive hierarchical model of the network:\nAn intuitive hierarchical model of the network:\nAn intuitive hierarchical model of the network:\nAn intuitive hierarchical model of the network:\nAn intuitive hierarchical model of the network:\nAn intuitive hierarchical model of the network:\nAn intuitive hierarchical model of", "histories": [["v1", "Thu, 15 Sep 2016 04:45:11 GMT  (726kb,D)", "https://arxiv.org/abs/1609.04508v1", null], ["v2", "Tue, 29 Nov 2016 03:59:26 GMT  (280kb,D)", "http://arxiv.org/abs/1609.04508v2", "Accepted at AAAI'17"]], "reviews": [], "SUBJECTS": "cs.LG cs.AI stat.ML", "authors": ["trang pham", "truyen tran 0001", "dinh q phung", "svetha venkatesh"], "accepted": true, "id": "1609.04508"}, "pdf": {"name": "1609.04508.pdf", "metadata": {"source": "META", "title": "Column Networks for Collective Classification", "authors": ["Trang Pham", "Truyen Tran", "Dinh Phung", "Svetha Venkatesh"], "emails": ["phtra@deakin.edu.au", "truyen.tran@deakin.edu.au", "dinh.phung@deakin.edu.au", "svetha.venkatesh@deakin.edu.au"], "sections": [{"heading": "1 Introduction", "text": "Relational data are characterized by relational structures between objects or data instances. For example, research publications are linked by citations, web pages are connected by hyperlinks and movies are related through same directors or same actors. Using relations may improve performance in classification as relations between entities may be indicative of relations between classes. A canonical task in learning from this data type is collective classification in which networked data instances are classified simultaneously rather than independently to exploit the dependencies in the data (Macskassy and Provost 2007; Neville and Jensen 2007; Richardson and Domingos 2006; Sen et al. 2008). Collective classification is, however, highly challenging. Exact collective inference under general dependencies is intractable. For tractable learning, we often resort to surrogate loss functions such as (structured) pseudo-likelihood (Sutton and McCallum 2007), approximate gradient (Hinton 2002), or iterative schemes, stacked\nlearning (Choetkiertikul et al. 2015; Kou and Cohen 2007; Macskassy and Provost 2007; Neville and Jensen 2000).\nExisting models designed for collective classification are mostly shallow and do not emphasize learning of local and relational features. Deep neural networks, on the other hand, offer automatic feature learning, which is arguably the key behind recent record-breaking successes in vision, speech, games and NLP (LeCun, Bengio, and Hinton 2015; Mnih et al. 2015). With known challenges in relational learning, can we design a deep neural network that is efficient and accurate for collective classification? There has been recent work that combines deep learning with structured prediction but the main learning and inference problems for general multirelational settings remain open (Belanger and McCallum 2016; Do, Arti, and others 2010; Tompson et al. 2014; Yu, Wang, and Deng 2010; Zheng et al. 2015).\nIn this paper, we present Column Network (CLN), an efficient deep learning model for multi-relational data, with emphasis on collective classification. The design of CLN is partly inspired by the columnar organization of neocortex (Mountcastle 1997), in which cortical neurons are organized in vertical, layered mini-columns, each of which is responsible for a small receptive field. Communications between mini-columns are enabled through short-range horizontal connections. In CLN, each mini-column is a feedforward net that takes an input vector \u2013 which plays the role of a receptive field \u2013 and produces an output class. Each mini-column net not only learns from its own data but also exchanges features with neighbor mini-columns along the pathway from the input to output. Despite the short-range exchanges, the interaction range between mini-columns increases with depth, thus enabling long-range dependencies between data objects.\nTo be able to learn with hundreds of layers, we leverage the recently introduced highway nets (Srivastava, Greff, and Schmidhuber 2015) as models for mini-columns. With this design choice, CLN becomes a network of interacting highway nets. But unlike the original highway nets, CLN\u2019s hidden layers share the same set of parameters, allowing the depth to grow without introducing new parameters (Liao and Poggio 2016; Pham et al. 2016). Functionally, if feedforward nets and highway nets are functional approximators for an input vector, CLN can be thought as an approximator of a grand function that takes a complex network of vectors as input and returns multiple outputs. CLN has many desirable theoretical\nar X\niv :1\n60 9.\n04 50\n8v 2\n[ cs\n.L G\n] 2\n9 N\nov 2\n01 6\nproperties: (i) it encodes multi-relations between any two instances; (ii) it is deep and compact, allowing complex functions to be approximated at the network level with a small set of free parameters; (iii) local and relational features are learned simultaneously; (iv) long-range, higher-order dependencies between instances are supported naturally; and (v) crucially, learning and inference are efficient, linear in the size of network and the number of relations.\nWe evaluate CLN on real-world applications: (a) delay prediction in software projects, (b) PubMed Diabetes publication classification and (c) film genre classification. In all applications, CLN demonstrates a higher accuracy than state-of-theart rivals."}, {"heading": "2 Preliminaries", "text": "Notation convention: We use capital letters for matrices and bold lowercase letters for vectors. The sigmoid function of a scalar x is defined as \u03c3(x) = [1 + exp(\u2212x)]\u22121, x \u2208 R. A function g of a vector x is defined as g(x) = (g(x1), ..., g(xn)). The operator \u2217 is used to denote elementwise multiplication. We use superscript t (e.g. ht) to denote layers or computational steps in neural networks , and subscript i for the ith element in a set (e.g. hti is the hidden activation at layer t of entity i in a graph)."}, {"heading": "2.1 Collective Classification in Multi-relational Setting", "text": "We describe the collective classification setting under multiple relations. Given a graph of entities G={E, R, X, Y} where E = {e1, ..., eN} are N entities that connect through relations in R. Each tuple {ej , ei, r} \u2208 R describes a relation of type r (r = 1...R, where R is the number of relation types in G) from entity ej to entity ei. Two entities can connect through multiple relations. A relation can be unidirectional or bidirectional. For example, movie A and movie B may be linked by a unidirectional relation sequel(A,B) and two bidirectional relations: same-actor(A,B) and same-director(A,B).\nEntities and relations can be represented in an entity graph where a node represents an entity and an edge exists between two nodes if they have at least one relation. Furthermore, ej is a neighbor of ei if there is a link from ej to ei. Let N (i) be the set of all neighbors of ei andNr(i) be the set of neighbors related to ei through relation r. This immediately implies N (i) = \u222ar\u2208RNr(i).\nX = {x1, ...,xN} is the set of local features, where xi is feature vector of entity ei; and Y = {y1, ..., yN} with each yi \u2208 {1, ..., L} is the label of ei. yi can either be observed or latent. Given a set of known label entities Eobs, a collective classification algorithm simultaneously infers unknown labels of entities in the set Ehid = E\\Eobs. In our probabilistic setting, we assume the classifier produces estimate of the joint conditional distribution P (Y | G).\nIt is challenging to learn and infer about P (Y | G). A popular strategy is to employ approximate but efficient iterative methods (Macskassy and Provost 2007). In the next subsection, we describe a highly effective strategy known as stacked learning, which partly inspires our work.\n2.2 Stacked Learning\nStacked learning (Fig. 1) is a multi-step learning procedure for collective classification (Choetkiertikul et al. 2015; Kou and Cohen 2007; Yu, Wang, and Deng 2010). At step t\u2212 1, a classifier is used to predict class probabilities for entity ej , i.e., pt\u22121j = [ P t\u22121 (yj = 1) , ..., P t\u22121 (yj = L) ] . These intermediate outputs are then used as relational features for neighbor classifiers in the next step. In (Choetkiertikul et al. 2015), each relation produces one set of contextual features, where all features of the same relation are averaged:\nctir = 1 |Nr(i)| \u2211\nj\u2208Nr(i)\npt\u22121j (1)\nwhere ctir is the relational feature vector for relation r at step t. The output at step t is predicted as follows\nP t (yi) = f t ( xi,p t\u22121 i , [ cti1, c t i2..., c t iR ]) (2)\nwhere f t is the classifier at step t. When t = 1, the model uses local features of entities for classification, i.e., c1ir = 0 and p0i = 0. At each step, classifiers are trained sequentially with known-label entities.\n3 Column Networks In this section we present our main contribution, the Column Network (CLN)."}, {"heading": "3.1 Architecture", "text": "Inspired by the columnar organization in neocortex (Mountcastle 1997), the CLN has one mini-column per entity (or data instance), which is akin to a sensory receptive field. Each column is a feedforward net that passes information from a lower layer to a higher layer of its own, and higher layers of neighbors (see Fig. 2 for a CLN that models the graph in Fig.1(Left)). The nature of the inter-column communication is dictated by the relations between the two entities.\nThrough multiple layers, long-range dependencies are established (see Sec. 3.4 for more in-depth discussion). This somewhat resembles the strategy used in stacked learning as described in Sec. 2.2. The main difference is that in CLN\nthe intermediate steps do not output class probabilities but learn higher abstraction of instance features and relational features. As such, our model is end-to-end in the sense that receptive signals are passed from the bottom to the top, and abstract features are inferred along the way. Likewise, the training signals are passed from the top to the bottom.\nDenote byxi \u2208 RM andhti \u2208 RKt the input feature vector and the hidden activation at layer t of entity ei, respectively. If there is a connection from entity ej to ei, ht\u22121j serves as an input for hti . Generally, h t i is a non-linear function of ht\u22121i and previous hidden states of its neighbors:\nhti = g ( ht\u22121i ,h t\u22121 j1 , ...,ht\u22121j|N(i)| ) where j \u2208 N (i) and h0i is the input vector xi.\nWe borrow the idea of stacked learning (Sec. 2.2) to handle multiple relations in CLN. The context of relation r (r = 1, ..., R) at layer t in Eq. (1) is replaced by\nctir = 1 |Nr(i)| \u2211\nj\u2208Nr(i)\nht\u22121j (3)\nFurthermore, different from stacked learning, the context in CLN are abstracted features, i.e., we replace Eq. (2) by\nhti = g ( bt +W tht\u22121i + 1\nz R\u2211 r=1 V tr c t jr\n) (4)\nwhere W t \u2208 RKt\u00d7Kt\u22121 and V tr \u2208 RK t\u00d7Kt\u22121 are weight matrices and bt is a bias vector for some activation function g; z is a pre-defined constant which is used to prevent the sum of parameterized contexts from growing too large for complex relations.\nAt the top layer T , for example, the label probability for entity i is given as: P (yi = l) = softmax ( bl +Wlh T i ) Remark: There are several similarities between CLN and existing neural network operations. Eq. (3) implements meanpooling, the operation often seen in CNN. The main difference with the standard CNN is that the mean pooling does not reduce the graph size. This suggests other forms of pooling such as max-pooling or sum-pooling. Asymmetric pooling can also be implemented based on the concept of attention, that is, Eq. (3) can be replaced by:\nctir = \u2211\nj\u2208Nr(i)\n\u03b1jh t\u22121 j\nsubject to \u2211\nj\u2208Nr(i) \u03b1j = 1 and \u03b1j \u2265 0. Eq. (4) implements a convolution. For example, standard 3x3 convolutional kernels in images implement 8 relations: left, right, above, below, above-left, above-right, below-left, below-right. Supposed that the relations are shared between nodes, the CLN achieves translation invariance, similar to that in CNN."}, {"heading": "3.2 Highway Network as Mini-Column", "text": "We now specify the detail of a mini-column, which we implement by extending a recently introduced feedforward net called Highway Network (Srivastava, Greff, and Schmidhuber 2015). Recall that traditional feedforward nets have a major difficulty of learning with high number of layers. This is due to the nested non-linear structure that prevents the ease of passing information and gradient along the computational path. Highway nets solve this problem by partially opening the gate that lets previous states to propagate through layers, as follows:\nht = \u03b11 \u2217 h\u0303 t +\u03b12 \u2217 ht\u22121 (5)\nwhere h\u0303 t is a nonlinear candidate function of ht\u22121 and where \u03b11,\u03b12 \u2208 (0,1) are learnable gates. Since the gates are never shut down completely, data signals and error gradients can propagate very far in a deep net.\nFor modeling relations, the candidate function h\u0303 t\nin Eq. (5) is computed using Eq. (4). Likewise, the gates are modeled as:\n\u03b11 = \u03c3 ( bt\u03b1 +W t \u03b1h t\u22121 i + 1\nz R\u2211 r=1 V t\u03b1rc t jr\n) (6)\nand \u03b12 = 1\u2212\u03b11 as for compactness (Srivastava, Greff, and Schmidhuber 2015). Other gating options exists, for example, the p-norm gates where \u03b1p1 +\u03b1 p 2 = 1 for p > 0 (Pham et al. 2016)."}, {"heading": "3.3 Parameter Sharing for Compactness", "text": "For feedforward nets, the number of parameters grow with number of hidden layers. In CLN, the number is multiplied by the number of relations (see Eq. (4)). In highway network implementation of mini-columns, a set of parameters for the gates is used thus doubling the number of parameters (see Eq. (6)). For a deep CLN with many relations, the number of parameters may grow faster than the size of training data, leading to overfitting and a high demand of memory. To address this challenge, we borrow the idea of parameter sharing in Recurrent Neural Network (RNN), that is, layers have identical parameters. There has been empirical evidence supporting this strategy in non-relational data (Liao and Poggio 2016; Pham et al. 2016).\nWith parameter sharing, the depth of the CLN can grow without increasing in model size. This may lead to good performance on small and medium datasets. See Sec. 4 provides empirical evidences."}, {"heading": "3.4 Capturing Long-range Dependencies", "text": "An important property of our proposed deep CLN is the ability to capture long-range dependencies despite only local state exchange as shown in Eqs. (4,6). To see how, let us consider the example in Fig. 2, where x1 is modeled in h13 and h 1 3 is modeled in h24, therefore although e1 does not directly connect to e4 but information of e1 is still embedded in h24 through h13. More generally, after k hidden layers, a hidden activation of an entity can contains information of its expanded neighbors of radius k. When the number of layers is large, the representation of an entity at the top layer contains not only its local features and its directed neighbors, but also the information of the entire graph. With highway networks, all of these levels of representations are accumulated through layers and used to predict output labels."}, {"heading": "3.5 Training with mini-batch", "text": "As described in Sec. 3.1, hti is a function of h t\u22121 i and the previous layer of its neighbors. hti therefore can contains information of the entire graph if the network is deep enough. This requires full-batch training which is expensive and not scalable. We propose a very simple yet efficient approximation method that allows mini-batch training. For each mini-batch, the neighbor activations are temporarily frozen to scalars, i.e., gradients are not propagated through this \u201cblanket\u201d. After the parameter update, the activations are recomputed as usual. Experiments showed that the procedure did converge and its performance is comparative with the full-batch training method.\n4 Experiments and Results In this section, we report three real-world applications of CLN on networked data: software delay estimate, PubMed paper classification and film genre classification."}, {"heading": "4.1 Baselines", "text": "For comparison, we employed a comprehensive suit of baseline methods which include: (a) those designed for collective classification, and (b) deep neural nets for non-collective classification. For the former, we used NetKit1, an open source toolkit for classification in networked data (Macskassy and Provost 2007). NetKit offers a classification framework consisting of 3 components: a local classifier, a relational classifier and a collective inference method. In our experiments, the local classifier is the Logistic Regression (LR) for all settings; relational classifiers are (i) weighted-vote Relational Neighbor (wvRN), (ii) logistic regression link-based classifier with normalized values (nbD), and (iii) logistic regression link-based classifier with absolute count values (nbC). Collective inference methods include Relaxation Labeling (RL) and Iterative Classification (IC). In total, there are 6 pairs of \u201crelational classifier \u2013 collective inference\u201d: wvRN-RL, wvRN-IC, nbD-RL, nbD-IC, nbC-RL and nbC-IC. For each dataset, results of two best settings will be reported.\nWe also implemented the state-of-the-art collective classifiers following (Choetkiertikul et al. 2015; Kou and Cohen\n1http://netkit-srl.sourceforge.net/\n2007; Yu, Wang, and Deng 2010): stacked learning with logistic regression (SL-LR) and with random forests (SL-RF).\nFor deep neural nets, following the latest results in (Liao and Poggio 2016; Pham et al. 2016), we implemented highway network with shared parameters among layers (HWNnoRel). This is essentially a special case of CLN without relational connections."}, {"heading": "4.2 Experiment Settings", "text": "We report three variants of the CLN: a basic version that uses standard Feedforward Neural Network as mini-column (CLNFNN) and two versions of CLN-HWN that use highway nets with shared parameters (CLN-HWN-full for full-batch mode and CLN-HWN-mini for mini-batch mode, as described in Sections 3.2, 3.3 and 3.5). All neural nets use ReLU in the hidden layers.\nDropout is applied before and after the recurrent layers of CLN-HWNs and at every hidden layers of CLN-FNN. Each dataset is divided into 3 separated sets: training, validation and test sets. For hyper-parameter tuning, we search for (i) number of hidden layers: 2, 6, 10, ..., 30, (ii) hidden dimensions, and (iii) optimizers: Adam or RMSprop. CLN-FNN has 2 hidden layers and the same hidden dimension with CLN-HWN so that the two models have equal number of parameters. The best training setting is chosen by the validation set and the results of the test set are reported. The result of each setting is reported by the mean result of 5 runs. Code for our model can be found on Github 2"}, {"heading": "4.3 Software Delay Prediction", "text": "This task is to predict potential delay for an issue, which is an unit of task in an iterative software development lifecycle (Choetkiertikul et al. 2015). The prediction point is when issue planning has completed. Due to the dependencies between issues, the prediction of delay for an issue must take into account all related issues. We use the largest dataset reported in (Choetkiertikul et al. 2015), the JBoss, which contains 8,206 issues. Each issue is a vector of 15 features and connects to other issues through 12 relations (unidirectional such as blocked-by or bidirectional such as same-developer). The task is to predict whether a software issue is at risk of getting delays (i.e., binary classification).\nFig. 3 visualizes CLN-HWN-full performance with different numbers of layers ranging from 2 to 30 and hidden dimensions from 5, 10 to 20. The F1-score peaks at 10 hidden layers and dimension size of 10.\nTable 1 reports the F1-scores of all methods. The two best classifiers in NetKit are wvRN-IC and wvRN-RL. The non-collective HWN-noRel works surprisingly well \u2013 almost reaching the performance of the best collective SL-RF with 2 points short. This demonstrates that deep neural nets are highly competitive in this domain, and to the best of our knowledge, this fact has not been established. CLN-HWN-full beats the best collective-method, the SL-RF by 3.1 points. We lost 0.7% in mini-batch training mode but the gain of training speed was substantial - roughly 6x.\n2https://github.com/trangptm/Column_networks"}, {"heading": "4.4 PubMed Publication Classification", "text": "We used the Pubmed Diabetes dataset consisting of 19,717 scientific publications and 44,338 citation links among them3. Each publication is described by a TF/IDF weighted word vector from a dictionary which consists of 500 unique words. We conducted experiments of classifying each publication into one of three classes: Diabetes Melitus - Experimental, Diabetes Melitus type 1, and Diabetes Mellitus type 2.\nVisualization of hidden layers We randomly picked 2 samples of each class and visualized their ReLU units activations through 10 layers of the CLN-HWN (Fig. 4). Interestingly, the activation strength seems to grow with higher layers, suggesting that learn features are more discriminative as they are getting closer to the outcomes. For each class a number of hidden units is turned off in every layer. Figures of samples in the same class have similar patterns while figures of samples from different classes are very different.\nClassification accuracy The best setting for CLN-HWN is with 40 hidden dimensions and 10 recurrent layers. Results are measured in MicroF1-score and MacroF1-score (See Table 2). The non-relational highway net (HWN-noRel) outperforms two best baselines from NetKit. The two version of CLN-HWN perform best in both F1-score measures."}, {"heading": "4.5 Film Genre Prediction", "text": "We used the MovieLens Latest Dataset (Harper and Konstan 2016) which consists of 33,000 movies. The task is to predict genres for each movie given plot summary. Local features were extracted from movie plot summary downloaded from IMDB database4. After removing all movies without plot summary, the dataset remains 18,352 movies. Each movie\n3Download: http://linqs.umiacs.umd.edu/projects//projects/lbc/ 4http://www.imdb.com\nis described by a Bag-of-Words vector of 1,000 most frequent words. Relations between movies are (same-actor, same-director). To create a rather balanced dataset, 20 genres are collapsed into 9 labels: (1) Drama, (2) Comedy, (3) Horror + Thriller, (4) Adventure + Action, (5) Mystery + Crime + Film-Noir, (6) Romance, (7) Western + War + Documentary, (8) Musical + Animation + Children, and (9) Fantasy + Sci-Fi. The frequencies of 9 labels are reported in Table 3.\nOn this dataset, CLN-HWNs work best with 30 hidden dimensions and 10 recurrent layers. Table 4 reports the F-scores. The two best settings with NetKit are nbC-IC and nbC-RL. CLN-FNN performs well on Micro-F1 but fails to improve MacroF1-score of prediction. CLN-HWN-mini outperforms CLN-HWN-full by 1.3 points on Macro-F1.\nFig. 5 shows why CLN-FNN performs badly on MacroF1 (MacroF1 is the average of all classes\u2019 F1-scores). While CLN-FNN works well with balanced classes (in the first three classes, its performance is nearly as good as CLN-HWN), it fails to handle imbalanced classes (See Table 3 for label frequencies). For example, F1-score is only 5.4% for label 7 and 13.3% for label 8. In contrast, CLN-HWN performs well on all classes.\n5 Related Work This paper sits at the intersection of two recent independently developed areas: Statistical Relational Learning (SRL) and Deep Learning (DL). Started in the late 1990s, SRL has advanced significantly with noticeable works such as Probabilistic Relational Models (Getoor and Sahami 1999), Conditional Random Fields (Lafferty, McCallum, and Pereira 2001), Relational Markov Network (Taskar, Pieter, and Koller 2002) and Markov Logic Networks (Richardson and Domingos 2006). Collective classification is a canonical task in SRL, also known in various forms as structured prediction (Dietterich et al. 2008) and classification on networked data (Macskassy and Provost 2007).\nTwo key components of collective classifiers are relational classifier and collective inference (Macskassy and Provost 2007). Relational classifier makes use of predicted classes (or class probabilities) of entities from neighbors as features. Examples are wvRN (Macskassy and Provost 2007), logistic based (Domke 2013) or stacked graphical learning (Choetkiertikul et al. 2015; Kou and Cohen 2007). Collective inference is the task of jointly inferring labels for entities. This is a subject of AI with abundance of solutions including message passing algorithms (Pearl 1988), variational meanfield (Opper and Saad 2001) and discrete optimization (Tran and Dinh Phung 2014). Among existing collective classifiers, the closest to ours is stacked graphical learning where collective inference is bypassed through stacking (Kou and Cohen 2007; Yu, Wang, and Deng 2010). The idea is based on learning a stack of models that take intermediate prediction of neighborhood into account.\nThe other area is Deep Learning (DL), where the cur-\nrent wave has offered compact and efficient ways to build multilayered networks for function approximation (via feedforward networks) and program construction (via recurrent networks) (LeCun, Bengio, and Hinton 2015; Schmidhuber 2015). However, much less attention has been paid to general networked data (Monner, Reggia, and others 2013), although there has been work on pairing structured outputs with deep networks (Belanger and McCallum 2016; Do, Arti, and others 2010; Tompson et al. 2014; Yu, Wang, and Deng 2010; Zheng et al. 2015). Parameter sharing in feedforward networks was recently analyzed in (Liao and Poggio 2016; Pham et al. 2016). The sharing eventually transforms the networks in to recurrent neural networks (RNNs) with only one input at the first layer. The empirical findings were that the performance is good despite the compactness of the model. Among deep neural nets, the closest to our work is RNCC model (Monner, Reggia, and others 2013), which also aims at collective classification using RNNs. There are substantial differences, however. RNCC shuffles neighbors of an entities to a random sequence and uses horizontal RNN to integrate the sequence of neighbors. Ours emphasizes on vertical depth, where parameter sharing gives rise to the vertical RNNs. Ours is conceptually simpler \u2013 all nodes are trained simultaneously, not separately as in RNCC.\n6 Discussion This paper has proposed Column Network (CLN), a deep neural network with an emphasis on fast and accurate collective classification. CLN has linear complexity in data size and number of relations in both training and inference. Empirically, CLN demonstrates a competitive performance against rival collective classifiers on three real-world applications: (a) delay prediction in software projects, (b) PubMed Diabetes publication classification and (c) film genre classification.\nAs the name suggests, CLN is a network of narrow deep networks, where each layer is extended to incorporate as input the preceding neighbor layers. It somewhat resembles the columnar structure in neocortex (Mountcastle 1997), where each narrow deep network plays a role of a mini-column. We wish to emphasize that although we use highway networks in actual implementation due to its excellent performance (Pham et al. 2016; Srivastava, Greff, and Schmidhuber 2015; Tran, Phung, and Venkatesh 2016), any feedforward networks can be potentially be used in our architecture. When parameter sharing is used, the feedforward networks become recurrent networks, and CLN becomes a network of interacting RNNs. Indeed, the entire network can be collapsed into a giant feedforward network with n\u2212n input/output mappings. When relations are shared among all nodes across the network, CLN enables translation invariance across the network, similar to those in CNN. However, the CLN is not limited to a single network with shared relations. Alternatively, networks can be IID according to some distribution and this allows relations to be specific to nodes.\nThere are open rooms for future work. One extension is to learn the pooling operation using attention mechanisms. We have considered only homogeneous prediction tasks here, assuming instances are of the same type. However, the same framework can be easily extended to multiple instance types."}, {"heading": "Acknowledgement", "text": "Dinh Phung is partially supported by the Australian Research Council under the Discovery Project DP150100031"}], "references": [{"title": "Structured prediction energy networks", "author": ["D. Belanger", "A. McCallum"], "venue": "ICML.", "citeRegEx": "Belanger and McCallum,? 2016", "shortCiteRegEx": "Belanger and McCallum", "year": 2016}, {"title": "Predicting delays in software projects using networked classification", "author": ["M. Choetkiertikul", "H.K. Dam", "T. Tran", "A. Ghose"], "venue": "30th IEEE/ACM International Conference on Automated Software Engineering.", "citeRegEx": "Choetkiertikul et al\\.,? 2015", "shortCiteRegEx": "Choetkiertikul et al\\.", "year": 2015}, {"title": "Structured machine learning: the next ten years", "author": ["T.G. Dietterich", "P. Domingos", "L. Getoor", "S. Muggleton", "P. Tadepalli"], "venue": "Machine Learning 73(1):3\u201323.", "citeRegEx": "Dietterich et al\\.,? 2008", "shortCiteRegEx": "Dietterich et al\\.", "year": 2008}, {"title": "Neural conditional random fields", "author": ["T. Do", "T Arti"], "venue": "In International Conference on Artificial Intelligence and Statistics,", "citeRegEx": "Do and Arti,? \\Q2010\\E", "shortCiteRegEx": "Do and Arti", "year": 2010}, {"title": "Structured learning via logistic regression", "author": ["J. Domke"], "venue": "Advances in Neural Information Processing Systems, 647\u2013 655.", "citeRegEx": "Domke,? 2013", "shortCiteRegEx": "Domke", "year": 2013}, {"title": "Using probabilistic relational models for collaborative filtering", "author": ["L. Getoor", "M. Sahami"], "venue": "Workshop on Web Usage Analysis and User Profiling (WEBKDD\u201999).", "citeRegEx": "Getoor and Sahami,? 1999", "shortCiteRegEx": "Getoor and Sahami", "year": 1999}, {"title": "The movielens datasets: History and context", "author": ["F.M. Harper", "J.A. Konstan"], "venue": "ACM Transactions on Interactive Intelligent Systems (TiiS) 5(4):19.", "citeRegEx": "Harper and Konstan,? 2016", "shortCiteRegEx": "Harper and Konstan", "year": 2016}, {"title": "Training products of experts by minimizing contrastive divergence", "author": ["G. Hinton"], "venue": "Neural Computation 14:1771\u20131800.", "citeRegEx": "Hinton,? 2002", "shortCiteRegEx": "Hinton", "year": 2002}, {"title": "Stacked Graphical Models for Efficient Inference in Markov Random Fields", "author": ["Z. Kou", "W.W. Cohen"], "venue": "SDM, 533\u2013538. SIAM.", "citeRegEx": "Kou and Cohen,? 2007", "shortCiteRegEx": "Kou and Cohen", "year": 2007}, {"title": "Conditional random fields: Probabilistic models for segmenting and labeling sequence data", "author": ["J. Lafferty", "A. McCallum", "F. Pereira"], "venue": "Proceedings of the International Conference on Machine learning (ICML), 282\u2013289.", "citeRegEx": "Lafferty et al\\.,? 2001", "shortCiteRegEx": "Lafferty et al\\.", "year": 2001}, {"title": "Deep learning", "author": ["Y. LeCun", "Y. Bengio", "G. Hinton"], "venue": "Nature 521(7553):436\u2013444.", "citeRegEx": "LeCun et al\\.,? 2015", "shortCiteRegEx": "LeCun et al\\.", "year": 2015}, {"title": "Bridging the gaps between residual learning, recurrent neural networks and visual cortex", "author": ["Q. Liao", "T. Poggio"], "venue": "arXiv preprint arXiv:1604.03640.", "citeRegEx": "Liao and Poggio,? 2016", "shortCiteRegEx": "Liao and Poggio", "year": 2016}, {"title": "Classification in networked data: A toolkit and a univariate case study", "author": ["S. Macskassy", "F. Provost"], "venue": "The Journal of Machine Learning Research 8:935\u2013983.", "citeRegEx": "Macskassy and Provost,? 2007", "shortCiteRegEx": "Macskassy and Provost", "year": 2007}, {"title": "Human-level control through deep reinforcement learning", "author": ["V. Mnih", "K. Kavukcuoglu", "D. Silver", "A.A. Rusu", "J. Veness", "M.G. Bellemare", "A. Graves", "M. Riedmiller", "A.K. Fidjeland", "G Ostrovski"], "venue": "Nature", "citeRegEx": "Mnih et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Mnih et al\\.", "year": 2015}, {"title": "Recurrent neural collective classification", "author": ["D.D. Monner", "J Reggia"], "venue": "Neural Networks and Learning Systems, IEEE Transactions on 24(12):1932\u20131943", "citeRegEx": "Monner and Reggia,? \\Q2013\\E", "shortCiteRegEx": "Monner and Reggia", "year": 2013}, {"title": "The columnar organization of the neocortex", "author": ["V.B. Mountcastle"], "venue": "Brain 120(4):701\u2013722.", "citeRegEx": "Mountcastle,? 1997", "shortCiteRegEx": "Mountcastle", "year": 1997}, {"title": "Iterative classification in relational data", "author": ["J. Neville", "D. Jensen"], "venue": "Proc. AAAI-2000 Workshop on Learning Statistical Models from Relational Data, 13\u201320.", "citeRegEx": "Neville and Jensen,? 2000", "shortCiteRegEx": "Neville and Jensen", "year": 2000}, {"title": "Relational dependency networks", "author": ["J. Neville", "D. Jensen"], "venue": "Journal of Machine Learning Research 8(Mar):653\u2013 692.", "citeRegEx": "Neville and Jensen,? 2007", "shortCiteRegEx": "Neville and Jensen", "year": 2007}, {"title": "Advanced mean field methods: Theory and practice", "author": ["M. Opper", "D. Saad"], "venue": "Massachusetts Institute of Technology Press (MIT Press).", "citeRegEx": "Opper and Saad,? 2001", "shortCiteRegEx": "Opper and Saad", "year": 2001}, {"title": "Probabilistic Reasoning in Intelligent Systems: Networks of Plausible Inference", "author": ["J. Pearl"], "venue": "San Francisco, CA: Morgan Kaufmann.", "citeRegEx": "Pearl,? 1988", "shortCiteRegEx": "Pearl", "year": 1988}, {"title": "Faster training of very deep networks via p-norm gates", "author": ["T. Pham", "T. Tran", "D. Phung", "S. Venkatesh"], "venue": "ICPR\u201916.", "citeRegEx": "Pham et al\\.,? 2016", "shortCiteRegEx": "Pham et al\\.", "year": 2016}, {"title": "Markov logic networks", "author": ["M. Richardson", "P. Domingos"], "venue": "Machine Learning 62:107\u2013136.", "citeRegEx": "Richardson and Domingos,? 2006", "shortCiteRegEx": "Richardson and Domingos", "year": 2006}, {"title": "Deep learning in neural networks: An overview", "author": ["J. Schmidhuber"], "venue": "Neural Networks 61:85\u2013117.", "citeRegEx": "Schmidhuber,? 2015", "shortCiteRegEx": "Schmidhuber", "year": 2015}, {"title": "Collective classification in network data", "author": ["P. Sen", "G. Namata", "M. Bilgic", "L. Getoor", "B. Galligher", "T. Eliassi-Rad"], "venue": "AI magazine 29(3):93.", "citeRegEx": "Sen et al\\.,? 2008", "shortCiteRegEx": "Sen et al\\.", "year": 2008}, {"title": "Training very deep networks", "author": ["R.K. Srivastava", "K. Greff", "J. Schmidhuber"], "venue": "Advances in neural information processing systems, 2377\u20132385.", "citeRegEx": "Srivastava et al\\.,? 2015", "shortCiteRegEx": "Srivastava et al\\.", "year": 2015}, {"title": "Piecewise pseudolikelihood for efficient CRF training", "author": ["C. Sutton", "A. McCallum"], "venue": "Proceedings of the International Conference on Machine Learning (ICML), 863\u2013 870.", "citeRegEx": "Sutton and McCallum,? 2007", "shortCiteRegEx": "Sutton and McCallum", "year": 2007}, {"title": "Discriminative probabilistic models for relational data", "author": ["B. Taskar", "A. Pieter", "D. Koller"], "venue": "Proceedings of the 18th Conference on Uncertainty in Artificial Intelligence (UAI), 485\u201349. Morgan Kaufmann.", "citeRegEx": "Taskar et al\\.,? 2002", "shortCiteRegEx": "Taskar et al\\.", "year": 2002}, {"title": "Joint training of a convolutional network and a graphical model for human pose estimation", "author": ["J.J. Tompson", "A. Jain", "Y. LeCun", "C. Bregler"], "venue": "Advances in neural information processing systems, 1799\u20131807.", "citeRegEx": "Tompson et al\\.,? 2014", "shortCiteRegEx": "Tompson et al\\.", "year": 2014}, {"title": "Tree-based Iterated Local Search for Markov Random Fields with Applications in Image Analysis", "author": ["T. Tran", "S.V. Dinh Phung"], "venue": "Journal of Heuristics DOI:10.1007/s10732014-9270-1.", "citeRegEx": "Tran and Phung,? 2014", "shortCiteRegEx": "Tran and Phung", "year": 2014}, {"title": "Neural choice by elimination via highway networks", "author": ["T. Tran", "D. Phung", "S. Venkatesh"], "venue": "Trends and Applications in Knowledge Discovery and Data Mining, Lecture Notes in Computer Science 9794:15\u201325.", "citeRegEx": "Tran et al\\.,? 2016", "shortCiteRegEx": "Tran et al\\.", "year": 2016}, {"title": "Sequential labeling using deep-structured conditional random fields", "author": ["D. Yu", "S. Wang", "L. Deng"], "venue": "Selected Topics in Signal Processing, IEEE Journal of 4(6):965\u2013973.", "citeRegEx": "Yu et al\\.,? 2010", "shortCiteRegEx": "Yu et al\\.", "year": 2010}, {"title": "Conditional random fields as recurrent neural networks", "author": ["S. Zheng", "S. Jayasumana", "B. Romera-Paredes", "V. Vineet", "Z. Su", "D. Du", "C. Huang", "P.H. Torr"], "venue": "Proceedings of the IEEE International Conference on Computer Vision, 1529\u20131537.", "citeRegEx": "Zheng et al\\.,? 2015", "shortCiteRegEx": "Zheng et al\\.", "year": 2015}], "referenceMentions": [{"referenceID": 12, "context": "A canonical task in learning from this data type is collective classification in which networked data instances are classified simultaneously rather than independently to exploit the dependencies in the data (Macskassy and Provost 2007; Neville and Jensen 2007; Richardson and Domingos 2006; Sen et al. 2008).", "startOffset": 208, "endOffset": 308}, {"referenceID": 17, "context": "A canonical task in learning from this data type is collective classification in which networked data instances are classified simultaneously rather than independently to exploit the dependencies in the data (Macskassy and Provost 2007; Neville and Jensen 2007; Richardson and Domingos 2006; Sen et al. 2008).", "startOffset": 208, "endOffset": 308}, {"referenceID": 21, "context": "A canonical task in learning from this data type is collective classification in which networked data instances are classified simultaneously rather than independently to exploit the dependencies in the data (Macskassy and Provost 2007; Neville and Jensen 2007; Richardson and Domingos 2006; Sen et al. 2008).", "startOffset": 208, "endOffset": 308}, {"referenceID": 23, "context": "A canonical task in learning from this data type is collective classification in which networked data instances are classified simultaneously rather than independently to exploit the dependencies in the data (Macskassy and Provost 2007; Neville and Jensen 2007; Richardson and Domingos 2006; Sen et al. 2008).", "startOffset": 208, "endOffset": 308}, {"referenceID": 25, "context": "For tractable learning, we often resort to surrogate loss functions such as (structured) pseudo-likelihood (Sutton and McCallum 2007), approximate gradient (Hinton 2002), or iterative schemes, stacked learning (Choetkiertikul et al.", "startOffset": 107, "endOffset": 133}, {"referenceID": 7, "context": "For tractable learning, we often resort to surrogate loss functions such as (structured) pseudo-likelihood (Sutton and McCallum 2007), approximate gradient (Hinton 2002), or iterative schemes, stacked learning (Choetkiertikul et al.", "startOffset": 156, "endOffset": 169}, {"referenceID": 1, "context": "For tractable learning, we often resort to surrogate loss functions such as (structured) pseudo-likelihood (Sutton and McCallum 2007), approximate gradient (Hinton 2002), or iterative schemes, stacked learning (Choetkiertikul et al. 2015; Kou and Cohen 2007; Macskassy and Provost 2007; Neville and Jensen 2000).", "startOffset": 210, "endOffset": 311}, {"referenceID": 8, "context": "For tractable learning, we often resort to surrogate loss functions such as (structured) pseudo-likelihood (Sutton and McCallum 2007), approximate gradient (Hinton 2002), or iterative schemes, stacked learning (Choetkiertikul et al. 2015; Kou and Cohen 2007; Macskassy and Provost 2007; Neville and Jensen 2000).", "startOffset": 210, "endOffset": 311}, {"referenceID": 12, "context": "For tractable learning, we often resort to surrogate loss functions such as (structured) pseudo-likelihood (Sutton and McCallum 2007), approximate gradient (Hinton 2002), or iterative schemes, stacked learning (Choetkiertikul et al. 2015; Kou and Cohen 2007; Macskassy and Provost 2007; Neville and Jensen 2000).", "startOffset": 210, "endOffset": 311}, {"referenceID": 16, "context": "For tractable learning, we often resort to surrogate loss functions such as (structured) pseudo-likelihood (Sutton and McCallum 2007), approximate gradient (Hinton 2002), or iterative schemes, stacked learning (Choetkiertikul et al. 2015; Kou and Cohen 2007; Macskassy and Provost 2007; Neville and Jensen 2000).", "startOffset": 210, "endOffset": 311}, {"referenceID": 13, "context": "Deep neural networks, on the other hand, offer automatic feature learning, which is arguably the key behind recent record-breaking successes in vision, speech, games and NLP (LeCun, Bengio, and Hinton 2015; Mnih et al. 2015).", "startOffset": 174, "endOffset": 224}, {"referenceID": 0, "context": "With known challenges in relational learning, can we design a deep neural network that is efficient and accurate for collective classification? There has been recent work that combines deep learning with structured prediction but the main learning and inference problems for general multirelational settings remain open (Belanger and McCallum 2016; Do, Arti, and others 2010; Tompson et al. 2014; Yu, Wang, and Deng 2010; Zheng et al. 2015).", "startOffset": 320, "endOffset": 440}, {"referenceID": 27, "context": "With known challenges in relational learning, can we design a deep neural network that is efficient and accurate for collective classification? There has been recent work that combines deep learning with structured prediction but the main learning and inference problems for general multirelational settings remain open (Belanger and McCallum 2016; Do, Arti, and others 2010; Tompson et al. 2014; Yu, Wang, and Deng 2010; Zheng et al. 2015).", "startOffset": 320, "endOffset": 440}, {"referenceID": 31, "context": "With known challenges in relational learning, can we design a deep neural network that is efficient and accurate for collective classification? There has been recent work that combines deep learning with structured prediction but the main learning and inference problems for general multirelational settings remain open (Belanger and McCallum 2016; Do, Arti, and others 2010; Tompson et al. 2014; Yu, Wang, and Deng 2010; Zheng et al. 2015).", "startOffset": 320, "endOffset": 440}, {"referenceID": 15, "context": "The design of CLN is partly inspired by the columnar organization of neocortex (Mountcastle 1997), in which cortical neurons are organized in vertical, layered mini-columns, each of which is responsible for a small receptive field.", "startOffset": 79, "endOffset": 97}, {"referenceID": 11, "context": "But unlike the original highway nets, CLN\u2019s hidden layers share the same set of parameters, allowing the depth to grow without introducing new parameters (Liao and Poggio 2016; Pham et al. 2016).", "startOffset": 154, "endOffset": 194}, {"referenceID": 20, "context": "But unlike the original highway nets, CLN\u2019s hidden layers share the same set of parameters, allowing the depth to grow without introducing new parameters (Liao and Poggio 2016; Pham et al. 2016).", "startOffset": 154, "endOffset": 194}, {"referenceID": 12, "context": "A popular strategy is to employ approximate but efficient iterative methods (Macskassy and Provost 2007).", "startOffset": 76, "endOffset": 104}, {"referenceID": 1, "context": "1) is a multi-step learning procedure for collective classification (Choetkiertikul et al. 2015; Kou and Cohen 2007; Yu, Wang, and Deng 2010).", "startOffset": 68, "endOffset": 141}, {"referenceID": 8, "context": "1) is a multi-step learning procedure for collective classification (Choetkiertikul et al. 2015; Kou and Cohen 2007; Yu, Wang, and Deng 2010).", "startOffset": 68, "endOffset": 141}, {"referenceID": 1, "context": "In (Choetkiertikul et al. 2015), each relation produces one set of contextual features, where all features of the same relation are averaged:", "startOffset": 3, "endOffset": 31}, {"referenceID": 15, "context": "Inspired by the columnar organization in neocortex (Mountcastle 1997), the CLN has one mini-column per entity (or data instance), which is akin to a sensory receptive field.", "startOffset": 51, "endOffset": 69}, {"referenceID": 20, "context": "Other gating options exists, for example, the p-norm gates where \u03b1p1 +\u03b1 p 2 = 1 for p > 0 (Pham et al. 2016).", "startOffset": 90, "endOffset": 108}, {"referenceID": 11, "context": "There has been empirical evidence supporting this strategy in non-relational data (Liao and Poggio 2016; Pham et al. 2016).", "startOffset": 82, "endOffset": 122}, {"referenceID": 20, "context": "There has been empirical evidence supporting this strategy in non-relational data (Liao and Poggio 2016; Pham et al. 2016).", "startOffset": 82, "endOffset": 122}, {"referenceID": 12, "context": "For the former, we used NetKit1, an open source toolkit for classification in networked data (Macskassy and Provost 2007).", "startOffset": 93, "endOffset": 121}, {"referenceID": 11, "context": "For deep neural nets, following the latest results in (Liao and Poggio 2016; Pham et al. 2016), we implemented highway network with shared parameters among layers (HWNnoRel).", "startOffset": 54, "endOffset": 94}, {"referenceID": 20, "context": "For deep neural nets, following the latest results in (Liao and Poggio 2016; Pham et al. 2016), we implemented highway network with shared parameters among layers (HWNnoRel).", "startOffset": 54, "endOffset": 94}, {"referenceID": 1, "context": "This task is to predict potential delay for an issue, which is an unit of task in an iterative software development lifecycle (Choetkiertikul et al. 2015).", "startOffset": 126, "endOffset": 154}, {"referenceID": 1, "context": "We use the largest dataset reported in (Choetkiertikul et al. 2015), the JBoss, which contains 8,206 issues.", "startOffset": 39, "endOffset": 67}, {"referenceID": 1, "context": "(*) Result reported in (Choetkiertikul et al. 2015).", "startOffset": 23, "endOffset": 51}, {"referenceID": 6, "context": "We used the MovieLens Latest Dataset (Harper and Konstan 2016) which consists of 33,000 movies.", "startOffset": 37, "endOffset": 62}, {"referenceID": 5, "context": "Started in the late 1990s, SRL has advanced significantly with noticeable works such as Probabilistic Relational Models (Getoor and Sahami 1999), Conditional Random Fields (Lafferty, McCallum, and Pereira 2001), Relational Markov Network (Taskar, Pieter, and Koller 2002) and Markov Logic Networks (Richardson and Domingos 2006).", "startOffset": 120, "endOffset": 144}, {"referenceID": 21, "context": "Started in the late 1990s, SRL has advanced significantly with noticeable works such as Probabilistic Relational Models (Getoor and Sahami 1999), Conditional Random Fields (Lafferty, McCallum, and Pereira 2001), Relational Markov Network (Taskar, Pieter, and Koller 2002) and Markov Logic Networks (Richardson and Domingos 2006).", "startOffset": 298, "endOffset": 328}, {"referenceID": 2, "context": "Collective classification is a canonical task in SRL, also known in various forms as structured prediction (Dietterich et al. 2008) and classification on networked data (Macskassy and Provost 2007).", "startOffset": 107, "endOffset": 131}, {"referenceID": 12, "context": "2008) and classification on networked data (Macskassy and Provost 2007).", "startOffset": 43, "endOffset": 71}, {"referenceID": 12, "context": "Two key components of collective classifiers are relational classifier and collective inference (Macskassy and Provost 2007).", "startOffset": 96, "endOffset": 124}, {"referenceID": 12, "context": "Examples are wvRN (Macskassy and Provost 2007), logistic based (Domke 2013) or stacked graphical learning (Choetkiertikul et al.", "startOffset": 18, "endOffset": 46}, {"referenceID": 4, "context": "Examples are wvRN (Macskassy and Provost 2007), logistic based (Domke 2013) or stacked graphical learning (Choetkiertikul et al.", "startOffset": 63, "endOffset": 75}, {"referenceID": 1, "context": "Examples are wvRN (Macskassy and Provost 2007), logistic based (Domke 2013) or stacked graphical learning (Choetkiertikul et al. 2015; Kou and Cohen 2007).", "startOffset": 106, "endOffset": 154}, {"referenceID": 8, "context": "Examples are wvRN (Macskassy and Provost 2007), logistic based (Domke 2013) or stacked graphical learning (Choetkiertikul et al. 2015; Kou and Cohen 2007).", "startOffset": 106, "endOffset": 154}, {"referenceID": 19, "context": "This is a subject of AI with abundance of solutions including message passing algorithms (Pearl 1988), variational meanfield (Opper and Saad 2001) and discrete optimization (Tran and Dinh Phung 2014).", "startOffset": 89, "endOffset": 101}, {"referenceID": 18, "context": "This is a subject of AI with abundance of solutions including message passing algorithms (Pearl 1988), variational meanfield (Opper and Saad 2001) and discrete optimization (Tran and Dinh Phung 2014).", "startOffset": 125, "endOffset": 146}, {"referenceID": 8, "context": "Among existing collective classifiers, the closest to ours is stacked graphical learning where collective inference is bypassed through stacking (Kou and Cohen 2007; Yu, Wang, and Deng 2010).", "startOffset": 145, "endOffset": 190}, {"referenceID": 22, "context": "The other area is Deep Learning (DL), where the current wave has offered compact and efficient ways to build multilayered networks for function approximation (via feedforward networks) and program construction (via recurrent networks) (LeCun, Bengio, and Hinton 2015; Schmidhuber 2015).", "startOffset": 235, "endOffset": 285}, {"referenceID": 0, "context": "However, much less attention has been paid to general networked data (Monner, Reggia, and others 2013), although there has been work on pairing structured outputs with deep networks (Belanger and McCallum 2016; Do, Arti, and others 2010; Tompson et al. 2014; Yu, Wang, and Deng 2010; Zheng et al. 2015).", "startOffset": 182, "endOffset": 302}, {"referenceID": 27, "context": "However, much less attention has been paid to general networked data (Monner, Reggia, and others 2013), although there has been work on pairing structured outputs with deep networks (Belanger and McCallum 2016; Do, Arti, and others 2010; Tompson et al. 2014; Yu, Wang, and Deng 2010; Zheng et al. 2015).", "startOffset": 182, "endOffset": 302}, {"referenceID": 31, "context": "However, much less attention has been paid to general networked data (Monner, Reggia, and others 2013), although there has been work on pairing structured outputs with deep networks (Belanger and McCallum 2016; Do, Arti, and others 2010; Tompson et al. 2014; Yu, Wang, and Deng 2010; Zheng et al. 2015).", "startOffset": 182, "endOffset": 302}, {"referenceID": 11, "context": "Parameter sharing in feedforward networks was recently analyzed in (Liao and Poggio 2016; Pham et al. 2016).", "startOffset": 67, "endOffset": 107}, {"referenceID": 20, "context": "Parameter sharing in feedforward networks was recently analyzed in (Liao and Poggio 2016; Pham et al. 2016).", "startOffset": 67, "endOffset": 107}, {"referenceID": 15, "context": "It somewhat resembles the columnar structure in neocortex (Mountcastle 1997), where each narrow deep network plays a role of a mini-column.", "startOffset": 58, "endOffset": 76}, {"referenceID": 20, "context": "We wish to emphasize that although we use highway networks in actual implementation due to its excellent performance (Pham et al. 2016; Srivastava, Greff, and Schmidhuber 2015; Tran, Phung, and Venkatesh 2016), any feedforward networks can be potentially be used in our architecture.", "startOffset": 117, "endOffset": 209}], "year": 2016, "abstractText": "Relational learning deals with data that are characterized by relational structures. An important task is collective classification, which is to jointly classify networked objects. While it holds a great promise to produce a better accuracy than non-collective classifiers, collective classification is computationally challenging and has not leveraged on the recent breakthroughs of deep learning. We present Column Network (CLN), a novel deep learning model for collective classification in multi-relational domains. CLN has many desirable theoretical properties: (i) it encodes multi-relations between any two instances; (ii) it is deep and compact, allowing complex functions to be approximated at the network level with a small set of free parameters; (iii) local and relational features are learned simultaneously; (iv) long-range, higher-order dependencies between instances are supported naturally; and (v) crucially, learning and inference are efficient with linear complexity in the size of the network and the number of relations. We evaluate CLN on multiple real-world applications: (a) delay prediction in software projects, (b) PubMed Diabetes publication classification and (c) film genre classification. In all of these applications, CLN demonstrates a higher accuracy than state-of-the-art rivals.", "creator": "TeX"}}}