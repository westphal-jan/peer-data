{"id": "1609.05396", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "17-Sep-2016", "title": "A Deep Metric for Multimodal Registration", "abstract": "Multimodal registration is a challenging problem in medical imaging due the high variability of tissue appearance under different imaging modalities. The crucial component here is the choice of the right similarity measure. We make a step towards a general learning-based solution that can be adapted to specific situations and present a metric based on a convolutional neural network. Our network can be trained from scratch even from a few aligned image pairs. The metric is validated on intersubject deformable registration on a dataset different from the one used for training, demonstrating good generalization. In this task, we outperform mutual information by a significant margin. In contrast, in general, the task for which the number of images was not specified, has improved by several orders of magnitude. We present a new approach that provides a simple approach to training. A second approach which allows us to model the same task and the same group of images is the same task as the one used in training.\n\n\nThe basic principles are of the two basic principle: the basic principle is to obtain information from images with high quality images or with high quality images (for example, a small number of images would be good images to illustrate). The goal of this method is to obtain data from an image in order to calculate the total number of images in order to show that the number of images are good images to illustrate. The objective of this method is to generate a small image, with high quality images in order to demonstrate the maximum number of images available. This method is very flexible and will require large images. For example, in our sample image processing technique, the pixel size is a few hundred pixels.\nWe chose the smallest image to represent the image. It is only for the small size, and the images are smaller. We used a combination of the two principles, which are simple to use:\nThe minimum size and maximum size are the minimum size of images. This algorithm provides a set of parameters that can be determined on a variety of images. For example, we will specify the maximum size of the image on the screen, and the maximum size of the image on the display. The minimum size of the image on the screen is specified in the size of the image on the screen.\nWhen a image is made with low quality images, the size is calculated with a set of parameters that can be determined on a variety of images. For example, in our sample image processing technique, the maximum size of the image on the screen is specified in the size of the image on the screen.\n", "histories": [["v1", "Sat, 17 Sep 2016 21:46:21 GMT  (752kb,D)", "http://arxiv.org/abs/1609.05396v1", "Accepted to MICCAI 2016; extended version"]], "COMMENTS": "Accepted to MICCAI 2016; extended version", "reviews": [], "SUBJECTS": "cs.CV cs.LG cs.NE", "authors": ["martin simonovsky", "benjam\\'in guti\\'errez-becker", "diana mateus", "nassir navab", "nikos komodakis"], "accepted": false, "id": "1609.05396"}, "pdf": {"name": "1609.05396.pdf", "metadata": {"source": "CRF", "title": "A Deep Metric for Multimodal Registration", "authors": ["Martin Simonovsky", "Benjam\u0131\u0301n Guti\u00e9rrez-Becker", "Diana Mateus", "Nassir Navab", "Nikos Komodakis"], "emails": ["martin.simonovsky@enpc.fr", "nikos.komodakis@enpc.fr"], "sections": [{"heading": "1 Introduction", "text": "Multimodal registration is a very challenging problem in medical imaging commonly faced during image-guided interventions and data fusion [13]. The main difficulty of the multimodal registration task comes from the great variability of tissue or organ appearance when imaged by different physical principles, which translates in the lack of a general rule to compare such images. Therefore, efforts to tackle this problem focus mainly on the design of multimodal similarity metrics.\nRecent works have explored the use of supervised methods to learn similarity metrics from a set of aligned examples [2,8,11], showing potential to outperform hand-crafted metrics in particular applications. However, a general method to learn similarity between any two modalities calls for higher capacity models.\nInspired by their success in computer vision, we propose to learn such general similarity metric based on Convolutional Neural Networks (CNNs). The problem is modelled as a classification task, where the goal is to discriminate between aligned and misaligned patches from different modalities. To the best of our knowledge, this is the first time that CNNs are used in the context of multimodal medical image registration.\nThe ability of our metric to obtain reliable registrations is demonstrated on the ALBERTs database of neonatal images [5], where we outperform Mutual Information [10]. Importantly, we train on a separate dataset (IXI database of adults [1]), demonstrating the capability to generalize to data acquired with\nar X\niv :1\n60 9.\n05 39\n6v 1\n[ cs\n.C V\n] 1\n7 Se\np 20\n16\nnote in caption: output = dissim map\ndifferent scanners an with demographic differences in the subjects. We also show that our method is able to learn reliable multimodal similarities even with a small training set, as is often the case in medical imaging applications."}, {"heading": "1.1 Related Work", "text": "The idea of using supervised learning to build a similarity metric for multimodal images has been explored in a number of works. On one side, there are probabilistic approaches which rely on modelling the joint-image distribution. For instance, Guetter et al. propose a generative method based on Kullback-Leibler Divergence [6]. Our work is closer to the discriminative concept proposed by Lee et al. [8] and Michel et al. [11], where the problem of learning a similarity metric is posed as binary classification. Here the goal is to discriminate between aligned and misaligned patches given pairs of aligned images. Lee et al. propose the use\nof a Structured Support Vector Machine while Michel et al. use a method based on Adaboost. Different to these approaches we rely on CNN as our learning method of choice as the suitable set of characteristics for each type of modality combinations can be directly learned from the training data.\nThe power of CNNs to capture complex relationships between multimodal medical images has been shown in the problem of modality synthesis [12], where CNNs are used to map MRI-T2 images to MRI-T1 images using jointly the appearance of a small patch together with its localization. Our work is arguably most similar to the approach of Cheng et al. [3] who train a multilayer fullyconnected network pretrained with autoencoder for estimating similarity of 2D CT-MR patch pairs. Our network is a CNN, which enables us to scale to 3D due to weight sharing and train from scratch. Moreover, we evaluate our metric on the actual task of registration, unlike Cheng et al."}, {"heading": "2 Method", "text": "Image registration is the task of estimating the best spatial transformation T : \u2126f 7\u2192 Rd between a fixed image If : \u2126f \u2282 Rd 7\u2192 R and a moving image Im : \u2126m \u2282 Rd 7\u2192 R. In our setting d = 3 and the images come each from a different modality. The problem is often solved by minimizing the energy\nE(\u03b8) = M(If , Im(T (\u03b8))) +R(T (\u03b8)) (1)\nwhere the first term M is a metric quantifying the cost of the alignment by transformation T parameterized by \u03b8 and the second term R is a regularization constraining the mapping. We denote the moving image resampled into \u2126f by T as the warped image I \u2032m = Im(T (\u03b8)) : \u2126f \u2282 Rd 7\u2192 R. The minimization is commonly solved in a continuous or discrete optimization framework [13], depending on the nature of \u03b8.\nIn this work we explore formulating M as a convolutional neural network. To this end we rely on network N(Pf , Pm) which outputs a scalar value estimating the dissimilarity between two image patches Pf \u2282 If and Pm \u2282 I \u2032m of the same size. Its incorporation into a continuous optimization framework is explained in Subsection 2.1. The architecture and training of N is described in Subsection 2.2."}, {"heading": "2.1 Continuous Optimization", "text": "Continuous optimization methods iteratively update parameters \u03b8 based on the gradient of the objective function E(\u03b8). We restrict ourselves to first-order methods and use gradient descent in particular. Our metric is defined to aggregate local patch comparisons as\nM(If , I \u2032 m) = \u2211 P\u2208P N(If (P ), I \u2032 m(P )) (2)\nwhere P is the set of patch domains P \u2282 \u2126f sampled on a dense uniform grid with significant overlaps. The method is illustrated in Figure 1.\nIts gradient \u2207M(\u03b8), which is required for \u2207E(\u03b8), can be computed by applying chain rule as follows:\n\u2202 \u2211\nP\u2208P N(If (P ), I \u2032 m(P ))\n\u2202\u03b8 = \u2211 x\u2208\u2126f \u2211 P\u2208Px \u2202N(If (P ), I \u2032 m(P )) \u2202I \u2032m(x) \u2202I \u2032m(x) \u2202\u03b8 = (3)\n= \u2211 x\u2208\u2126f \u2211 P\u2208Px \u2202N(If (P ), I \u2032 m(P )) \u2202I \u2032m(x) \u2202Im(T (\u03b8,x)) \u2202T (\u03b8,x) \u2202T (\u03b8,x) \u2202\u03b8 = (4)\n= \u2211 x\u2208\u2126f \u2202N(If , I \u2032 m) \u2202I \u2032m(x) \u2207Im(T (\u03b8,x))JT (x) (5)\nEquation (3) shows that the derivative of N w.r.t. the intensity of an input pixel x depends on all patches containing it, denoted as Px. Thus, high overlap of neighboring patches leads to smoother, more stable derivatives. We found that registration quality drops considerably unless the grid stride s of P is small. On the other hand, subsampling \u2126f to obtain a sparser set of samples x has a minor impact on performance.\nIn the transition from Equation (4) to (5), patch-wise evaluation of N is replaced by fully convolutional evaluation over the whole domain \u2126f . This makes the computation very efficient, as results in intermediate network layers can be shared among neighboring patches [9].\nUltimately, the contribution of each pixel x to \u2207M(\u03b8) is a product of three terms, c.f. Equation (5): the derivative \u2202N/\u2202I \u2032m(x) of the estimated dissimilarity of patches around x w.r.t. its intensity in the warped image, which can be readily computed by standard backpropagation, the gradient of the moving image \u2207Im, which can be precomputed, and the local Jacobian matrix JT of transformation T . Note that the choice of a particular transformation type is decoupled from the network, therefore a single network will work with any transformation.\nComputing one iteration thus requires resampling of the moving image and one forward and one backward pass in the network. All operations can be efficiently computed on a GPU."}, {"heading": "2.2 Network Architecture and Training", "text": "Architecture. A feed-forward convolutional neural network N is used to estimate the dissimilarity of two cubic patches of the same size of p\u00d7p\u00d7p pixels. The architecture is based on recent works on learning to compare patches, notably the 2-channel network of Zagoruyko and Komodakis [14]. The two patches are considered as a 2-channel 3D image (each channel represents a different modality), which is fed to the first layer of the network. The network consists of a series of volumetric convolutional layers with ReLU non-linearities finalized by a convolutional layer without any non-linearity, which produces a scalar score.\nTo gradually subsample the spatial domain within the network and increase spatial context (perceptive field), we prefer convolutions with non-unit output stride to pooling used in [14], as it has led to better performance. We hypothesize\nthat too much spatial invariance might be detrimental in our case of learning cross-modal identity, unlike aiming for robustness to distortions such as perspective deformation. The product of convolutional strides determines the overal network stride s used in the fully-convolutional mode.\nThe 2-channel architecture is powerful as it considers both patches jointly from the beginning. However, its evaluation does not exploit the fact that the fixed image If does not change during optimization and its deep representation could be precomputed in the form of descriptors and cached. We have therefore experimented on architectures with two independent input branches, such as the pseudo-siamese network in [14]. Unfortunately, we have observed consistent decrease in registration performance.\nTraining. We suppose to have a set of k aligned pairs of training images {(Aj , Bj)}kj=1 with Aj , Bj : \u2126j \u2282 Rd 7\u2192 R. We sample transformations Ti,Aj , Ti,Bj : \u2126j 7\u2192 \u2126j for j-th image pair for data augmentation by varying position, scale, rotation, and mirroring. Patch pairs Xi = (Aj(Ti,Aj (P )), Bj(Ti,Bj (P ))) with fixed-size domain P are used for training the network. Sample Xi is defined to be positive (labeled yi = \u22121) if Ti,Aj = Ti,Bj and negative (yi = 1) otherwise. Positive and negative samples are mined with equal probability. Imposing restrictions on negatives (such as minimum or maximum overlap of source patch domains) or on patch content (such as minimum contrast [8]) were experimentally shown detrimental to the registration quality.\nThe network is trained to classify training samples Xi by minimizing hinge loss L = \u2211 i max(0, 1\u2212 yiN(Xi)), which we found to perform better than crossentropy. We observed that softmax leads to overly flat gradients in continuous optimization, as shown in the bottom plots in Figure 3. SGD with learning rate 0.01, momentum 0.9 and batch size 128 is used to optimize the network.\nInstead of preparing a fixed dataset of patches like in [3], we sample Xi online. This, together with the augmentations described above, allows us to feed the network with practically unlimited amount of training data. Even for small k we observed no overfitting in learning (see also Subsection 3.2).\nImplementation. We use Torch with cuDNN library for deep learning, elastix for GPU-based image resampling, and ITK for registration3. Our network has 5 layers, 2M parameters, patch size p = 17, and stride s = 4. We plan to open source our implementation and the trained network.\n3 www.torch.ch, developer.nvidia.com/cudnn, elastix.isi.uu.nl, www.itk.org"}, {"heading": "3 Experiments and Results", "text": "We evaluate the effectiveness of the learned metric in registration experiments on a set of clinical brain images in Subsection 3.1 and conduct further experiments to demonstrate its interesting properties in Subsections 3.2 and 3.3."}, {"heading": "3.1 Deformable Registration of Neonatal Brain MRI Images", "text": "Datasets. We conducted intersubject deformable registration experiments on a set of neonatal brain image volumes taken from the publicly available brain atlases ALBERTs [5]. This database consists of T1 and T2-weighted MRI scans of 20 newborns. Each T1-T2 pair is aligned and annotated with a segmentation map of 50 anatomical regions, which allows us to evaluate registration quality in terms of overlap measures; we compute average Dice and Jaccard coefficients.\nTo make the experiment challenging and demonstrate good generalization of our learned metric (denoted CNN), we train on IXI [1], a completely independent dataset of adult brain images. Let us remark that there are structural differences between the brains of neonates and adults. The dataset contains about 600 approximately aligned T1-T2 image pairs and we use k = 557 for training and the rest for validation, although in Subsection 3.2 we demonstrate that much less is actually needed. Image intensities in both datasets are normalized to [0, 1].\nBaseline. Our baseline is mutual information (MI) [10], the standard metric for multimodal registration. We observed that MI perform better when image domains are restricted to the head region, thus we use a fixed intensity threshold of 0.01 for masking the background and denote this variant MI+M. Such masking made nearly no difference to our metric. Unfortunately, we could not compare to other learning-based metrics [8,11] as their implementation was not available.\nProtocol. We test on 18 subjects in ALBERTs and perform 68 intersubject registrations, half of them aligning T1 to T2 and half of them the other way round. We reserve the remaining 2 subjects for validating registration parameters and model selection. Both metrics are evaluated in exactly the same registration pipeline with the same transformation model and optimizer. The pipeline consists of multiresolution similarity transform registration followed by multiresolution B-spline registration (2 scales, 1000 control points on the fine scale, 200k image sampling points), optimized by gradient descent with regular step and 500 iterations per scale. MI is used with 75 histogram bins (validated optimum). An explicit regularization term R in Equation (1) was used neither for MI nor for CNN. Instead, we regularize implicitly by the design of the pipeline and the choice of its hyperparameters.\nResults. The results are listed in Table 1 and demonstrate statistically significant improvement of registration quality due to CNN by about 4 points in both coefficients (as by one-sided t-test with significance \u03b1 = 0.01). Figure 2 exhibits scatter plots of initial and final Dice scores for each registration run (Jaccard scores follow similar trend). We can see that while CNN has improved on the alignment in all runs, this is not the case for MI+M and especially MI, showing rather low precision. The highest accuracies achieved by both methods\nare rather similar (up to 0.8) and seem nearly independent on the initial level of misalignment. Furthermore, the registration using CNN is only about 2x slower than using MI (on Nvidia Titan Black), the difference mostly due to expensive resampling of moving image."}, {"heading": "3.2 Influence of Training Set Size", "text": "The huge number of aligned volumes in IXI dataset is rather exceptional in medical domain. We are therefore interested in how much we can decrease the training set size k without noticeable impact on the quality. To this end, we train networks with only k = 11, 6, and 3 random image pairs under the same setting as above. Table 1 shows that even with little training data the results are very good and only for k = 3 our metric does not significantly outperform MI+M. On one hand, this suggests that our online sampling and data augmentation methodology works well. On the other hand, either the inherent variability in the dataset is very low (especially compared to natural image recognition problems, where more data typically improves performance) or our network is not able to exploit it. We expect that the amount of necessary data will be higher for more challenging modalities, such as ultrasound."}, {"heading": "3.3 Plausibility of Metric and Its Derivatives", "text": "To investigate the behavior of metric value and its actual derivatives used for continuous optimization, we visualize these quantities by manually perturbing a single parameter of a transformation initialized to identity on an aligned validation image pair in IXI. Figure 3 suggests that the metric behaves reasonably as its curves are smooth with the correct local minima. The analytic derivatives, as in Equation (5), have the correct sign over a large range, albeit their magnitude is slightly noisy. Nevertheless, this was shown not to prevent the metric from obtaining good registration results."}, {"heading": "4 Conclusion", "text": "We have presented a similarity metric for multimodal 3D image registration based on a convolutional neural network. The network can be trained from scratch even from a few aligned image pairs, mostly due to our data sampling scheme. We have described the incorporation of this metric into first-order continuous optimization frameworks. The experimetal evaluation was perfomed on the task of intersubject T1-T2 deformable registration on a dataset different from the one used for training, demonstrating good generalization. In this task, we outperform mutual information by a significant margin.\nWe envision incorporating our network into a discrete optimization framework as an easy extension. In a MRF-based formulation, the local alignment cost is expressed by unary potentials over nodes in a graph [4]. In particular, a unary potential gn(un) related to the cost of assigning a label/translation un to node n might be defined as gn(un) = N(If (Pn), Im(T (\u03b8, Pn) + un)), where Pn \u2282 \u2126f is a patch domain centered at the control point of transformation T corresponding to node n. As such an optimization is derivative-free, only the forward pass in the network would be necessary.\nWe also plan to apply our method to more modalities, such as ultrasound.\nAcknowledgments. We gratefully acknowledge NVIDIA Corporation for the donated GPU used in this research. ALBERTs atlases are copyrighted by Imperial College of Science, Technology and Medicine and Ioannis S. Gousias 2013. B. Gutie\u0301rrez-Becker thanks the financial support of CONACYT and the DAAD. The official publication is available at Springer via http://dx.doi.org/10. 1007/978-3-319-46726-9_2."}, {"heading": "A Comparison with MIND", "text": "The Modality Independent Neighborhood Descriptor (MIND) is a state-of-theart multimodal descriptor by Heinrich et al. [7] based on the concept of selfsimilarity. We performed the same set of 68 registration as in Subsection 3.1 using the code from the authors\u2019 website4. After validating its three main hyperparameters (six-neighborhood search region, Gaussian weighting \u03c3 = 0.5, regularization \u03b1 = 0.2), we obtained Dice score of 0.610 \u00b1 0.073 (see also Figure 4), resp. Jaccard score of 0.458 \u00b1 0.070. The results are clearly inferior to both MI+M and our CNN-based approach, although the running time was much shorter. However, we stress that their deformable registration code does not follow our pipeline described in Subsection 3.1 (different regularization and optimization, no similarity transformation step), and therefore the comparison serves for rather illustrative purposes.\n4 http://www.ibme.ox.ac.uk/research/biomedia/julia-schnabel/files/symgn.zip"}], "references": [{"title": "Robust multimodal dictionary learning", "author": ["T. Cao", "V. Jojic", "S. Modla", "D. Powell", "K. Czymmek", "M. Niethammer"], "venue": "Mori, K., Sakuma, I., Sato, Y., Barillot, C., Navab, N. (eds.) MICCAI. LNCS, vol. 8149, pp. 259\u2013266. Springer", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2013}, {"title": "Deep similarity learning for multimodal medical images", "author": ["X. Cheng", "L. Zhang", "Y. Zheng"], "venue": "Computer Methods in Biomechanics and Biomedical Engineering: Imaging & Visualization. pp. 1\u20135", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2015}, {"title": "Deformable medical image registration: Setting the state of the art with discrete methods", "author": ["B. Glocker", "A. Sotiras", "N. Komodakis", "N. Paragios"], "venue": "Annual Review of Biomedical Engineering 13, 219\u2013244", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2011}, {"title": "Magnetic resonance imaging of the newborn brain: Manual segmentation of labelled atlases in term-born and preterm infants", "author": ["I. Gousias", "A. Edwards", "M. Rutherford", "S. Counsell", "J. Hajnal", "D. Rueckert", "A. Hammers"], "venue": "Neuroimage 62,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2012}, {"title": "Learning based non-rigid multi-modal image registration using Kullback-Leibler divergence", "author": ["C. Guetter", "C. Xu", "F. Sauer", "J. Hornegger"], "venue": "Duncan, J.S., Gerig, G. (eds.) MICCAI. LNCS, vol. 3750, pp. 255\u2013262. Springer", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2005}, {"title": "MIND: Modality independent neighbourhood descriptor for multi-modal deformable registration", "author": ["M.P. Heinrich", "M. Jenkinson", "M. Bhushan", "T. Matin", "F.V. Gleeson", "S.M. Brady", "J.A. Schnabel"], "venue": "Medical image analysis 16, 1423\u201335", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2012}, {"title": "Learning similarity measure for multi-modal 3D image registration", "author": ["D. Lee", "M. Hofmann", "F. Steinke", "Y. Altun", "N. Cahill", "B. Scholkopf"], "venue": "CVPR. pp. 186\u2013193", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2009}, {"title": "Fully convolutional networks for semantic segmentation", "author": ["J. Long", "E. Shelhamer", "T. Darrell"], "venue": "CVPR. pp. 3431\u20133440", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2015}, {"title": "Nonrigid multimodality image registration", "author": ["D. Mattes", "D.R. Haynor", "H. Vesselle", "T.K. Lewellyn", "W. Eubank"], "venue": "SPIE. vol. 4322, pp. 1609\u20131620", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2001}, {"title": "Boosted metric learning for 3d multi-modal deformable registration", "author": ["F. Michel", "M. Bronstein", "A. Bronstein", "N. Paragios"], "venue": "ISBI. pp. 1209\u20131214", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2011}, {"title": "Cross-domain synthesis of medical images using efficient location-sensitive deep network", "author": ["H.V. Nguyen", "S.K. Zhou", "R. Vemulapalli"], "venue": "Navab, N., Hornegger, J., Wells III, W.M., Frangi, A.F. (eds.) MICCAI. LNCS, vol. 9349, pp. 677\u2013684. Springer", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2015}, {"title": "Deformable medical image registration: A survey", "author": ["A. Sotiras", "C. Davatzikos", "N. Paragios"], "venue": "TMI 32(7), 1153\u20131190", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2013}, {"title": "Learning to compare image patches via convolutional neural networks", "author": ["S. Zagoruyko", "N. Komodakis"], "venue": "CVPR. pp. 4353\u20134361", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2015}], "referenceMentions": [{"referenceID": 11, "context": "Multimodal registration is a very challenging problem in medical imaging commonly faced during image-guided interventions and data fusion [13].", "startOffset": 138, "endOffset": 142}, {"referenceID": 0, "context": "Recent works have explored the use of supervised methods to learn similarity metrics from a set of aligned examples [2,8,11], showing potential to outperform hand-crafted metrics in particular applications.", "startOffset": 116, "endOffset": 124}, {"referenceID": 6, "context": "Recent works have explored the use of supervised methods to learn similarity metrics from a set of aligned examples [2,8,11], showing potential to outperform hand-crafted metrics in particular applications.", "startOffset": 116, "endOffset": 124}, {"referenceID": 9, "context": "Recent works have explored the use of supervised methods to learn similarity metrics from a set of aligned examples [2,8,11], showing potential to outperform hand-crafted metrics in particular applications.", "startOffset": 116, "endOffset": 124}, {"referenceID": 3, "context": "The ability of our metric to obtain reliable registrations is demonstrated on the ALBERTs database of neonatal images [5], where we outperform Mutual Information [10].", "startOffset": 118, "endOffset": 121}, {"referenceID": 8, "context": "The ability of our metric to obtain reliable registrations is demonstrated on the ALBERTs database of neonatal images [5], where we outperform Mutual Information [10].", "startOffset": 162, "endOffset": 166}, {"referenceID": 4, "context": "propose a generative method based on Kullback-Leibler Divergence [6].", "startOffset": 65, "endOffset": 68}, {"referenceID": 6, "context": "[8] and Michel et al.", "startOffset": 0, "endOffset": 3}, {"referenceID": 9, "context": "[11], where the problem of learning a similarity metric is posed as binary classification.", "startOffset": 0, "endOffset": 4}, {"referenceID": 10, "context": "The power of CNNs to capture complex relationships between multimodal medical images has been shown in the problem of modality synthesis [12], where CNNs are used to map MRI-T2 images to MRI-T1 images using jointly the appearance of a small patch together with its localization.", "startOffset": 137, "endOffset": 141}, {"referenceID": 1, "context": "[3] who train a multilayer fullyconnected network pretrained with autoencoder for estimating similarity of 2D CT-MR patch pairs.", "startOffset": 0, "endOffset": 3}, {"referenceID": 11, "context": "The minimization is commonly solved in a continuous or discrete optimization framework [13], depending on the nature of \u03b8.", "startOffset": 87, "endOffset": 91}, {"referenceID": 7, "context": "This makes the computation very efficient, as results in intermediate network layers can be shared among neighboring patches [9].", "startOffset": 125, "endOffset": 128}, {"referenceID": 12, "context": "The architecture is based on recent works on learning to compare patches, notably the 2-channel network of Zagoruyko and Komodakis [14].", "startOffset": 131, "endOffset": 135}, {"referenceID": 12, "context": "To gradually subsample the spatial domain within the network and increase spatial context (perceptive field), we prefer convolutions with non-unit output stride to pooling used in [14], as it has led to better performance.", "startOffset": 180, "endOffset": 184}, {"referenceID": 12, "context": "We have therefore experimented on architectures with two independent input branches, such as the pseudo-siamese network in [14].", "startOffset": 123, "endOffset": 127}, {"referenceID": 6, "context": "Imposing restrictions on negatives (such as minimum or maximum overlap of source patch domains) or on patch content (such as minimum contrast [8]) were experimentally shown detrimental to the registration quality.", "startOffset": 142, "endOffset": 145}, {"referenceID": 1, "context": "Instead of preparing a fixed dataset of patches like in [3], we sample Xi online.", "startOffset": 56, "endOffset": 59}, {"referenceID": 3, "context": "We conducted intersubject deformable registration experiments on a set of neonatal brain image volumes taken from the publicly available brain atlases ALBERTs [5].", "startOffset": 159, "endOffset": 162}, {"referenceID": 8, "context": "Our baseline is mutual information (MI) [10], the standard metric for multimodal registration.", "startOffset": 40, "endOffset": 44}, {"referenceID": 6, "context": "Unfortunately, we could not compare to other learning-based metrics [8,11] as their implementation was not available.", "startOffset": 68, "endOffset": 74}, {"referenceID": 9, "context": "Unfortunately, we could not compare to other learning-based metrics [8,11] as their implementation was not available.", "startOffset": 68, "endOffset": 74}, {"referenceID": 2, "context": "In a MRF-based formulation, the local alignment cost is expressed by unary potentials over nodes in a graph [4].", "startOffset": 108, "endOffset": 111}], "year": 2016, "abstractText": "Multimodal registration is a challenging problem in medical imaging due the high variability of tissue appearance under different imaging modalities. The crucial component here is the choice of the right similarity measure. We make a step towards a general learning-based solution that can be adapted to specific situations and present a metric based on a convolutional neural network. Our network can be trained from scratch even from a few aligned image pairs. The metric is validated on intersubject deformable registration on a dataset different from the one used for training, demonstrating good generalization. In this task, we outperform mutual information by a significant margin.", "creator": "LaTeX with hyperref package"}}}