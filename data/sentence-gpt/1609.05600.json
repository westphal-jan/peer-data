{"id": "1609.05600", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "19-Sep-2016", "title": "Graph-Structured Representations for Visual Question Answering", "abstract": "This paper proposes to improve visual question answering (VQA) with structured representations of both scene contents and questions. A key challenge in VQA is to require joint reasoning over the visual and text domains. The predominant CNN/LSTM-based approach to VQA is limited by monolithic vector representations that largely ignore structure in the scene and in the form of the question. CNN feature vectors cannot effectively capture situations as simple as multiple object instances, and LSTMs process questions as series of words, which does not reflect the true complexity of language structure. We instead propose to build graphs over the scene objects and over the question words, and we describe a deep neural network that exploits the structure in these representations. This shows significant benefit over the sequential processing of LSTMs. The overall efficacy of our approach is demonstrated by significant improvements over the state-of-the-art, from 71.2% to 74.4% in accuracy on the \"abstract scenes\" multiple-choice benchmark, and from 34.7% to 39.1% in accuracy over pairs of \"balanced\" scenes, i.e. images with fine-grained differences and opposite yes/no answers to a same question. It is a remarkable breakthrough, given the difficulty with describing LSTMs on visual-language and on the computational cost of computation.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n", "histories": [["v1", "Mon, 19 Sep 2016 05:21:36 GMT  (4616kb,D)", "http://arxiv.org/abs/1609.05600v1", null], ["v2", "Thu, 30 Mar 2017 04:26:26 GMT  (4618kb,D)", "http://arxiv.org/abs/1609.05600v2", null]], "reviews": [], "SUBJECTS": "cs.CV cs.AI cs.CL", "authors": ["damien teney", "lingqiao liu", "anton van den hengel"], "accepted": false, "id": "1609.05600"}, "pdf": {"name": "1609.05600.pdf", "metadata": {"source": "CRF", "title": "Graph-Structured Representations for Visual Question Answering", "authors": ["Damien Teney", "Lingqiao Liu", "Anton van den Hengel"], "emails": ["damien.teney@adelaide.edu.au", "lingqiao.liu@adelaide.edu.au", "anton.vandenhengel@adelaide.edu.au"], "sections": [{"heading": "1. Introduction", "text": "The task of Visual Question Answering has received growing interest in the recent years [18, 4, 26]. It involves multiple aspects of computer vision, natural language processing, and artificial intelligence. In its common form, a question is provided as text in natural language together with an image, and a correct answer must be predicted, typically in the form of a single word or a short phrase. In a multiple-choice setting, candidate answers are provided, alleviating evaluation issues related to synonyms and paraphrasing.\nMultiple datasets for VQA have been introduced with either real [4, 15, 18, 22, 32] or synthetic images [4, 31]. Our experiments uses the latter, which contains clip art or \u201ccartoon\u201d images created by humans to depict realistic scenes\n(they are usually referred to as \u201cabstract scenes\u201d, despite this being a misnomer). Our experiments focus on this dataset of clip art scenes, as they allow to focus on semantic reasoning and vision-language interactions, in isolation from the performance of visual recognition (see examples in Fig. 5.2). A particularly attractive version of this dataset was introduced in [31] by selecting only the questions with binary answers (e.g. yes/no) and pairing each image with a minimally-different complementary version that elicits the opposite (no/yes) answer (see examples in Fig. 5.2, bottom rows). This strongly contrasts with other VQA datasets of real images, where a correct answer is often obvious without looking at the image, by relying on systematic regularities of frequent questions and answers [4, 31]. The marginal improvements often reported on such datasets are difficult to interpret as actual progress in fine-grained scene understanding and reasoning, and this hampers the progress toward the greater goal of VQA. In our view, and despite obvious limitations of synthetic images, improvements on the aforementioned \u201cbalanced\u201d dataset constitute a more robust sign of progress, since subtle details of the scenes must be considered to answer questions correctly.\nChallenges The questions in the clip-art dataset vary greatly in their complexity. Some can be directly answered from observations of visual elements, e.g. Is there a dog in\n1\nar X\niv :1\n60 9.\n05 60\n0v 1\n[ cs\n.C V\n] 1\n9 Se\np 20\n16\nthe room ?, or Is the weather good ? with the image of a sunny day. Others require relating multiple facts or understanding complex actions, e.g. Is the boy going to catch the ball?, or Is it the winter? about a scene containing a fireplace and jackets on a coat rack. A major challenge is the sparsity of training data. Even a large number of training questions (almost 25,000 for the clip art scenes of [4]) cannot possibly cover the combinatorial diversity of possible objects and concepts. Adding to this challenge, most methods for VQA process the question through a recurrent neural network (such as an LSTM) trained from scratch solely on the training questions.\nLanguage representation The above reasons motivate us to take advantage of the extensive existing work in the natural language community to aid processing the questions. First, we identify the syntactic structure using a dependency parser [7]. This produces a graph representation of the question in which each node represents a word and each edge a particular type of dependency (e.g. determiner, nominal subject, direct object, etc.). Second, we associate each word (node) with a vector embedding pretrained on large corpora of text data [21]. This embedding maps the words to a space in which distances are semantically meaningful. Consequently, this essentially regularizes the remainder of the network to share learned concepts among related words and synonyms. This particularly helps dealing with rare words, and also allows questions to include words absent from the training questions/answers. Note that this pretraining and ad hoc processing of the language part mimics a practice common for the image part, in which visual features are usually obtained from a fixed CNN, itself pretrained on a larger dataset and with a different (supervised classification) objective.\nScene representation Each object in the scene corresponds to a node in the scene graph, which has an associated feature vector describing its appearance. The graph is fully connected, with each edge representing the relative position of the objects in the image.\nApplying Neural Networks to graphs The two graph representations feed into a deep neural network that we will describe in Section 4. The advantage of applying the Graph Neural Network (GNN) over text- and scene graphs, rather than on more typical representations is that the graphs can capture relationships between words and between objects. This enables the GNN to exploit (1) the unordered nature of some elements (the scene objects in particular) and (2) the semantic relationships between elements (grammatical relationships in particular). The unordered aspect contrasts with the typical approach of representing the image with CNN activations (which are sensitive to all object locations)\nand the question with an RNN (despite the fact that grammatical structure is very non-linear). The graph representation ignores the order in which elements are processed, but instead represents the relationships between different elements using different edge types. Our network uses multiple layers that iterate over the features associated with every node, then ultimately identifies a soft matching between nodes from the two graphs. This matching reflects the correspondences between the words in the question and the objects in the image. The features of the matched nodes then feed into a classifier to infer the answer to the question (Fig. 1).\nThe main contributions of this paper are four-fold. 1) We describe how to use graph representations of scene\nand question for VQA, and a neural network capable of processing these representations to infer an answer. 2) We show how the graph representation can take advantage of preprocessing the language part, using syntactic dependencies as edge features and word embeddings as nodes features. 3) We train the proposed model on the VQA \u201cabstract scenes\u201d benchmark [4] and demonstrate its efficacy by raising the state-of-the-art accuracy from 71.2% to 74.4% in the multiple-choice setting. On the \u201cbalanced\u201d version of the dataset, we raise the accuracy from 34.7% to 39.1% in the hardest setting (requiring a correct answer over pairs of scenes). 4) We evaluate the uncertainty in the model by presenting \u2013 for the first time on the task of VQA \u2013 precision/recall curves of predicted answers. Those curves provide more insight than the single accuracy metric and favorably show that the uncertainty estimated by the model about its predictions correlates with the ambiguity of the human-provided ground truth."}, {"heading": "2. Related work", "text": "The task of visual question answering has received increasing interest since the seminar paper of Antol et al. [4]. Most recent methods are based on the idea of a joint embedding of the image and the question using a deep neural network. The image is passed through a convolutional neural network (CNN) pretrained for image classification, from which intermediate features are extracted to describe the image. The question is typically passed through a recurrent neural network (RNN) such as an LSTM, which produces a fixed-size vector representing the sequence of words. These two representations are mapped to a joint space by one or several non-linear layers. They can then be fed into a classifier over an output vocabulary, predicting the final answer. Most recent papers on VQA propose improvements and variations on this basic idea. Consult [26] for a survey.\nA major improvement to the basic method is to use an attention mechanism [32, 28, 5, 13, 3, 29]. It models interactions between specific parts of the inputs (image and question) depending on their actual contents. The visual input is then typically represented a spatial feature map, instead of holistic, image-wide features. The feature map is used with the question to determine spatial weights that reflect the most relevant regions of the image. Our approach uses a similar weighting operation, which, with our graph representation, we equate to a subgraph matching. Graph nodes representing question words are associated with graph nodes representing scene objects and vice versa. Similarly, the co-attention model of Lu et al. [17] determines attention weights on both image regions and question words. Their best-performing approach proceeds in a sequential manner, starting with question-guided visual attention followed by image-guided question attention. In our case, we found that a joint, one-pass version performs better.\nA major contribution of our model is to use structured representations of the input scene and the question. This contrasts with typical CNN and RNN models which are limited to spatial feature maps and sequences of words respectively. The dynamic memory networks (DMN), applied to VQA in [27] also maintain a set-like representation of the input. As in our model, the DMN models interactions between different parts of the input. Our method can additionally take, as input, features characterizing arbitrary relations between parts of the input (the edge features in our graphs). This specifically allows making use of syntactic dependencies between words after pre-parsing the question.\nMost VQA systems are trained end-to-end from questions and images to answers, with the exception of the\nvisual feature extractor, which is typically a CNN pretrained for image classification. For the language processing part, some methods address the the semantic aspect with word embeddings pretrained on a language modeling task (e.g. [24, 10]). The syntactic aspect is often overlooked. In [31], hand-designed rules serve to identify primary and secondary objects of the questions. In the Neural Module Networks [3, 2], the question is processed by a dependency parser, and fragments of the parse, selected with ad hoc fixed rules are associated with modules, assembled into a full neural network. In contrast, our method is trained to make direct use of the output of a syntactic parser.\nSeveral recent works have independently proposed similar formulations for processing graphs within neural networks [9, 12, 16]. Defferrard et al. [8] describes a comprehensive framework for applying convolutions and pooling over graphs. Most similar to ours are the Gated Graph Sequence Neural Networks [16], which associate a gated recurrent unit (GRU [6]) to each node, and update the feature vector of each node by iteratively passing messages between neighbours. Also related is the work of Vinyals et al. for embedding a set into fixed-size vector [25]. They feed the entire set through a recurrent unit multiple times, so that the final state incorporates information from all elements of the set, while being invariant to the order in which they are represented. Each recurrent iteration uses an attention mechanism to focus on different parts of the input set. Our formulation similarly incorporates information from neighbours into each node feature over multiple iterations, but we did not find any advantage in using an attention mechanism within the recurrent unit."}, {"heading": "3. Graph representation of scenes and questions", "text": "The input data for each training or test instance is a text question and the description of a scene. The question is processed with the Stanford dependency parser [7], which outputs the following.\n\u2022 A set of NQ words that constitute the nodes of the question graph. Each word is represented by its index in the input vocabulary, a token xQi \u2208 Z (i \u2208 1..NQ). \u2022 A set of pairwise relations between words, which constitute the edges of our graph. An edge between words i and j is represented by eQij \u2208 Z, an index among the possible types of dependencies.\nThe scene is represented as follows.\n\u2022 A set of NS objects that constitute the nodes of the scene graph. Each node is represented by a vector xSi \u2208 RC of visual features (i \u2208 1..NS).\n\u2022 A set of pairwise relations between all objects. They form the edges of a fully-connected graph of the scene. The edge between objects i and j is represented by a vector eSij \u2208 RD that encodes relative spatial relationships.\nOur experiments are carried out on datasets of clip art scenes, in which descriptions of the scenes are provided in the form of lists of objects with their visual features. The method is equally applicable to real images, with the object list replaced by candidate object detections. Our experiments on clip art allows the effect of the proposed method to be isolated from the performance of the object detector. Please refer to the supplementary material for implementation details.\nThe features of all nodes and edges are projected to a vector space RH of common dimension (typically H=300). The question nodes and edges use vector embeddings implemented as look-up tables, and the scene nodes and edges use affine projections:\nx \u2032Q i =W1 [ xQi ]\ne \u2032Q ij =W2 [ eQij ]\n(1)\nx \u2032S i =W3x S i + b3 e \u2032S ij =W4e S ij + b4 (2)\nwith W1 the word embedding (usually pretrained, see supplementary material), W2 the embedding of dependencies, W3 \u2208 Rh\u00d7c and W4 \u2208 Rh\u00d7d weight matrices, and b3 \u2208 Rc and b4 \u2208 Rd biases."}, {"heading": "4. Processing graphs with neural networks", "text": "We now describe a deep neural network suitable for processing the question and scene graphs to infer an answer. See Fig. 2 for an overview.\nThe two graphs representing the question and the scene are processed independently in a recurrent architecture. We\ndrop the exponents S and Q for this paragraph as the same procedure applies to both graphs. Each node xi is associated with a gated recurrent unit (GRU [6]) and processed over a fixed number T of iterations (typically T=4):\nh0i = 0 (3) hti = GRU ( ht\u22121i , [x \u2032 i ; ni] ) t \u2208 [1, T ] (4)\nni = pool j\n( e\u2032ij \u25e6 x\u2032j ) (5)\nSquare brackets with a semicolon represent a concatenation of vectors and \u25e6 the Hadamard (element-wise) product. The final state of the GRU is used as the new representation of the nodes: x\u2032\u2032i = h T i . The pool operation transforms features from a variable number of neighbours (i.e. connected nodes) to a fixed-size representation. Any commutative operation can be used (e.g. sum, maximum). In our implementation, we found the best performance with the average function, taking care of averaging over the variable number of connected neighbours. An intuitive interpretation of the recurrent processing is to progressively integrate context information from connected neighbours into each node\u2019s own representation. Our formulation is similar but slightly different from the gated graph networks [16], as the propagation of information in our model is limited to the first order. Note that our graphs are typically densely connected.\nWe now introduce a form of attention into the model, which constitutes an essential part of the model. The motivation is two-fold: (1) to identify parts of the input data most relevant to produce the answer and (2) to align the elements of the question with those of the scene. Practically, we estimate the relevance of each possible pairwise combinations of words and objects. More precisely, we compute scalar \u201cmatching weights\u201d between node sets {xQi } and {xSi }. These weights are comparable to the \u201cattention weights\u201d in other models (e.g. [17]). Therefore, \u2200 i \u2208 1..NQ, j \u2208 1..NS:\naij = \u03c3 ( W5 ( x\u2032Qi \u2016x\u2032Qi \u2016 \u25e6 x \u2032S j \u2016x\u2032Sj \u2016 ) + b5 ) (6)\nwhere W5 \u2208 R1\u00d7h and b5 \u2208 R are learned weights and biases, and \u03c3 the logistic function that introduces a nonlinearity and bounds the weights to ]0, 1[. The formulation is similar to a cosine similarity with learned weights on the feature dimensions. Note that the weights are computed using the initial embedding of the node features (pre-GRU). We apply the scalar weights aij to the corresponding pairwise combinations of question and scene features, thereby focusing and giving more importance to the matched pairs (Eq. 7). We sum the weighted features over the scene elements (Eq. 8) then over the question elements (Eq. 9), interleaving the sums with affine projections\nand non-linearities to obtain a final prediction:\nyij = aij \u25e6 [xQi ; x S j ] (7) y\u2032i = f ( W6 \u2211NS j yij + b6 ) (8)\ny\u2032\u2032 = f \u2032 ( W7 \u2211NQ i y \u2032 i + b7 ) (9)\nwith W6, W7, b6, b7 learned weights and biases, f a ReLU, and f \u2032 a softmax or a logistic function (see experiments, Section 5.1). The summations over the scene elements and question elements is a form of pooling that brings the variable number of features (due to the variable number of words and objects in the input) to a fixed-size output. The final output vector y\u2032\u2032 \u2208 RT contains scores for the possible answers, and has a number of dimensions equal to 2 for the binary questions of the \u201cbalanced\u201d dataset, or to the number of all candidate answers in the \u201cabstract scenes\u201d dataset. The candidate answers are those appearing at least 5 times in the training set (see supplementary material for details)."}, {"heading": "5. Evaluation", "text": "Datasets Our evaluation use two datasets: the original \u201cabstract scenes\u201d from Antol et al. [4] and its \u201cbalanced\u201d extension [31]. They both contain scenes created by humans in a drag-and-drop interface for arranging clip art objects and figures. The original dataset contains 20k/10k/20k scenes (for training/validation/test respectively) and 60k/30k/60k questions, each with 10 humanprovided ground-truth answers. Questions are categorized based on the type of the correct answer into yes/no, number, and other, but the same method is used for all categories, the type of the test questions being unknown. The \u201cbalanced\u201d version of the dataset contains only the subset questions which have binary (yes/no) answers and, in addition, complementary scenes created to elicit the opposite answer to each question. This is significant because guessing the modal answer from the training set will the succeed only half of the time (slightly more than 50% in practice because of disagreement between annotators) and give 0% accuracy over complementary pairs. This contrasts with other VQA datasets where blind guessing can be very effective. The pairs of complementary scenes also typically differ by only one or two objects being displaced, removed, or slightly modified (see examples in Fig. 5.2, bottom rows). This makes the questions very challenging by requiring to take into account subtle details of the scenes.\nMetrics The main metric is the average \u201cVQA score\u201d [4], which is a soft accuracy that takes into account variability of ground truth answers from multiple human annotators. Let us refer to a test question by an index q = 1..M , and to each possible answer in the output vocabulary by an index a. The\nground truth score s(q, a) = 1.0 if the answer a was provided by m\u22653 annotators. Otherwise, s(q, a) = m/31. Our method outputs a predicted score s\u0302(q, a) for each question and answer (y\u2032\u2032 in Eq. 9) and the overall accuracy is the average ground truth score of the highest prediction per question, i.e. 1M \u2211M q s(q, argmaxa s\u0302(q, a)).\nIt has been argued that the \u201cbalanced\u201d dataset can better evaluate a method\u2019s level of visual understanding than other datasets, because it is less susceptible to the use of language priors and dataset regularities (i.e. guessing from the question[31]). Our initial experiments confirmed that the performances of various algorithms on the balanced dataset were indeed better separated, and we used it for our ablative analysis. We also focus on the hardest evaluation setting [31], which measures the accuracy over pairs of complementary scenes. This is the only metric in which blind models (guessing from the question) obtain null accuracy. This setting also does not consider pairs of test scenes deemed ambiguous because of disagreement between annotators. Each test scene is still evaluated independently however, so the model is unable to increase performance by forcing opposite answers to pairs of questions. The metric is then a standard \u201chard\u201d accuracy, i.e. all ground truth scores s(i, j) \u2208 {0, 1}. Please refer to the supplementary material for additional details."}, {"heading": "5.1. Evaluation on the \u201cbalanced\u201d dataset", "text": "We compare our method against the three models proposed in [31]. They all use an ensemble of models exploiting either an LSTM for processing the question, or an elaborate set of hand-designed rules to identify two objects as the focus of the question. The visual features in the three models are respectively empty (blind model), global (scenewide), or focused on the two objects identified from the question. These models are specifically designed for binary questions, whereas ours is generally applicable. Nevertheless, we obtain significantly better accuracy than all of the three (Table 1). Differences in performance are mostly visible in the \u201cpairs\u201d setting, which we believe is more reliable as it discards ambiguous test questions on which human annotators disagreed.\nDuring training, we take care of keeping pairs of complementary scenes together when forming mini-batches. This has a significant positive effect on the stability of the optimization. Interestingly, we did not notice any tendency toward overfitting when training on balanced scenes. We hypothesize that the pairs of complementary scenes have a strong regularizing effect that force the learned model to focus on relevant details of the scenes. In Fig. 5.2 (and in the supplementary material), we visualize the matching weights between question words and scene objects (Eq. 6). As expected, these tend to be larger between semantically related\n1Ground truth scores are also averaged in a 10\u2013choose\u20139 manner [4].\nelements (e.g. daytime\u2194sun, dog\u2194puppy, boy\u2194human) although some are more difficult to interpret.\nOur best performance of about 39% is still low in absolute terms, which is understandable from the wide range of concepts involved in the questions (see examples in\nFig. 5.2 and in the supplementary material). It seems unlikely that these concepts could be learned from training question/answers alone, and we suggest that any further significant improvement in performance will require external sources of information at training and/or test time.\nAblative evaluation We evaluated variants of our model to measure the impact of various design choices (see numbered rows in Table 1). On the question side, we evaluate (row 1) our graph approach without syntactic parsing, building question graphs with only two types of edges, previous/next and linking consecutive nodes. This shows the advantage of using the graph method together with syntactic parsing. Optimizing the word embeddings from scratch (row 2) rather than from pretrained Glove vectors [21] produces a significant drop in performance. On the scene side, we removed the edge features (row 3) by setting eSij = 1. It confirms that the model makes use of the spatial relations between objects encoded by the edges of the graph. In rows 4\u20136, we disabled the recurrent graph processing (x\u2032\u2032i = x \u2032 i) for the either the question, the scene, or both. Although the performance is affected in all cases, it is most significant for the question. The language part thus benefits the most from the graph processing. We finally tested the model with uniform matching weights (aij = 1, row 10). As expected, it performed poorly. Our weights act similarly to the attention mechanisms in other models (e.g. [32, 28, 5, 13, 29]) and our observations confirm that such mechanisms are crucial for good performance.\nPrecision/recall We are interested in assessing the confidence of our model in its predicted answers. Most existing VQA methods treat the answering as a hard classification over candidate answers, and almost all reported results consist of a single accuracy metric. To provide more insight, we produce precision/recall curves for predicted answers. A precision/recall point (p, r) is obtained by setting a thresh-\nold t on predicted scores such that\np =\n\u2211 i,j 1 ( s\u0302(i, j)>t ) s(i, j)\u2211\ni,j 1(s\u0302(i, j)>t) (10)\nr =\n\u2211 i,j 1 ( s\u0302(i, j)>t ) s(i, j)\u2211\ni,j s(i, j) (11)\nwhere 1(\u00b7) is the 0/1 indicator function. We plot precision/recall curves in Fig. 5 for both datasets2. The predicted score proves to be a reliable indicator of the model confidence, as a low threshold can achieve near-perfect accuracy (Fig. 5, left and middle) by filtering out harder and/or ambiguous test cases.\nWe compare models trained with either a softmax or a sigmoid as the final non-linearity (Eq. 9). The common practice is to train the softmax for a hard classification objective, using a cross-entropy loss and the answer of highest ground truth score as the target. In an attempt to make better use of the multiple human-provided answers, we propose to use the soft ground truth scores as the target with a logarithmic loss. This shows an advantage on the \u201cabstract scenes\u201d dataset (Fig. 5, left and middle). In that dataset, the soft target scores reflect frequent ambiguities in the questions and the scenes, and when synonyms constitute multiple acceptable answers. In those cases, we can avoid the potential confusion induced by a hard classification for one specific answer. The \u201cbalanced\u201d dataset, by nature, contains almost no such ambiguities, and there is no significant difference between the different training objectives (Fig. 5, right).\nEffect of training set size Our motivation for introducing language parsing and pretrained word embeddings is to better generalize the concepts learned from the limited training examples. Words representing semantically close concepts ideally get assigned close word embeddings. Simi-\n2The \u201cabstract scenes\u201d test set is not available publicly, and precision/recall can only be provided on its validation set.\nlarly, paraphrases of similar questions should produce parse graphs with more similarities than a simple concatenation of words would reveal (as in the input to traditional LSTMs).\nWe trained our model with limited subsets of the training data (see Fig. 5.1). Unsurprisingly, the performance grows steadily with the amount of training data, which suggests that larger datasets would improve performance. In our opinion however, it seems unlikely that sufficient data, covering all possible concepts, could be be collected in the form of question/answer examples. More data can however be brought in with other sources of information and supervision. Our use of parsing and word embeddings is a small step in that direction. Both techniques clearly improve generalization (Fig. 5.1). The effect may be particularly visible in our case because of the relatively small number of training examples (about 20k questions in the \u201cbalanced\u201d dataset). It is unclear whether huge VQA datasets could ultimately negate this advantage. Future experiments on larger datasets (e.g. [15]) may answer this question."}, {"heading": "5.2. Evaluation on the \u201cabstract scenes\u201d dataset", "text": "We report our results on the original \u201cabstract scenes\u201d dataset in Table 2. The evaluation is performed on an automated server that does not allow for an extensive ablative analysis. Anecdotally, performance on the validation set corroborates all findings presented above, in particular the strong benefit of pre-parsing, pretrained word embeddings, and graph processing with a GRU. At the time of our submission, our method occupies the top place on the leader board in both the open-ended and multiple choice settings. The advantage over existing method is most pronounced on the binary and the counting questions. Refer to Fig. 5.2 and to the supplementary for visualizations of the results."}, {"heading": "6. Conclusions", "text": "We presented a deep neural network for visual question answering that processes graph-structured representations of scenes and questions. This enables leveraging existing natural language processing tools, in particular pretrained word embeddings and syntactic parsing. The latter showed significant advantage over a traditional sequential processing of the questions, e.g. with LSTMs. In our opinion, VQA systems are unlikely to learn everything from question/answer examples alone. We believe that future leaps in performance will require additional sources of information and supervision. Our explicit processing of the language part is a small step in that direction. It was clearly shown to improve generalization without resting entirely on VQAspecific annotations. We have so far applied our method to datasets of clip art scenes. Its direct extension to real images will be addressed in future work, by replacing nodes in the input scene graph with proposals from pretrained object detectors."}, {"heading": "B. Additional results", "text": "We provide below additional example results in the same format as in Fig. 5.2.\nB.1. Additional results: abstract scenes dataset\nIs the woman exercising ? Answer: yes\nhu m\nan\nhu m\nan\nco at\nra ck\npl an\nt\nco uc\nh\nde sk do or ru g pi llo w\nis\nthe woman exercising\nWhat is the girl doing ? Answer: jumping jumping rope\nhu m\nan\nhu m\nan\nbe e cl ou\nd\nsu n m on\nke yb\nar s\nju m\npr op\ne\nwhat\nis\nthe girl doing\nDoes he walk like an idiot ? Answer: yes\nhu m\nan\nbu sh be nc\nh\ntr ee po nd si de\nw al\nk\nfo ot\nba ll\nso cc\ner\nju m\npr op\ne\ndoes\nhe walk\nlike an idiot\nIs the man sitting on the armrest ? Answer: no yes\nhu m\nan\npu pp\ny\nw in\ndo w\nch ai\nr\ntv do or fir ep\nla ce\nen dt\nab le\nis\nthe\nman\nsitting\non the armrest\nWho is sitting between toys ? Answer: baby\nhu m\nan\nw in\ndo w\nw in\ndo w\nru g co uc\nh\ndo ll do ll\nwho\nis sitting between\ntoys\nWhat color are the pillows on the couch ? Answer: brown\nhu m\nan\nki tte\nn\nco uc\nh\nw in\ndo w\npi ct\nur e\nru g no te\nbo ok\nwhat color\nare the pillows\non the couch\nMight they be dating ? Answer: yes\nhu m\nan\nhu m\nan\ngr ill tr ee bl an\nke t\nap pl\ne\nbo ttl\ne\nrib s ba sk\net\nm ar\nsh m\nal lo\nw\nmight\nthey\nbe dating\nWhere is the girl sitting ? Answer: sandbox bench\nhu m\nan\nhu m\nan\nbu tte\nrf ly\nbi rd sl id\ne\ntr ee tr ee be nc\nh\nsa nd\nbo x\nsh ov\nel\nwhere\nis\nthe girl sitting\nWhat is underneath the arched window ? Answer: rug plant\nhu m\nan\nfir ep\nla ce\npl an\nt\nru g w in\ndo w\nw in\ndo w\npi ct\nur e\nwhat\nis underneath\nthe arched window\nWhere is the slide facing ? Answer: sandbox left\nhu m\nan\nhu m\nan\nhu m\nan\npu pp\ny\nsi de\nw al\nk\nsl id\ne be nc\nh\nsa nd\nbo x\nho td\nog\nwhere\nis\nthe\nslide\nfacing\nHow many clouds ? Answer: 2\nhu m\nan\nhu m\nan\nhu m\nan\nhu m\nan\nbu tte\nrf ly\nbu tte\nrf ly\nbl ue\nja y\nbl an\nke t\nbu sh bu sh\nhow\nmany clouds\nIs the lady reading ? Answer: yes\nhu m\nan\nw in\ndo w\nso fa fir ep\nla ce\nbo ok\nsh el\nf\nno te\nbo ok\nbo ok bo ok bo ok\nis\nthe lady reading\nHow many flowers are in the room ? Answer: 0\nhu m\nan\nca t pl an\nt\nch ai\nr\nde sk ru g co uc\nh\npe tb\ned\nhow\nmany\nflowers\nare\nin\nthe room\nWhat is on the mantle ? Answer: wine toys\nhu m\nan\nca t do g do g w in\ndo w\nfir ep\nla ce\ndi ni\nng ta\nbl e\nst oo\nl\nst oo\nl\npi ct\nur e\nwhat\nis on the mantle\nCan this child climb on the couch ? no\nhu m\nan\nru g pi ct\nur e\npi ct\nur e\nso fa to y do llh\nou se\ncan this child climb\non the couch\nDoes the man look depressed ? Answer: yes\nhu m\nan\npu pp\ny\nfir ep\nla ce\nru g so fa pi ct\nur e\npi llo\nw\ndoes\nthe\nman look depressed\nWhat color is his shirt ? Answer: red black\nhu m\nan\ndo g tr ee cl ou\nd\npo nd lil yp\nad\nwhat color\nis\nhis\nshirt\nHow many clouds in the sky ? Answer: 3\nhu m\nan\ndo g cl ou\nd\ncl ou\nd\ncl ou\nd\nbu sh tr ee su n\nhow\nmany clouds\nin\nthe sky\nWhat is the man doing ? Answer: sitting watching tv\nhu m\nan\npu pp\ny\ntv ch ai\nr\nco at\nra ck\nta bl\ne\nwhat\nis\nthe\nman\ndoing\nWhat is the pattern on the curtains ? floral\nhu m\nan\nhu m\nan\nki tte\nn\nm ou\nse\nso fa so fa pi ct\nur e\nfir ep\nla ce\nw in\ndo w\nw in\ndo w\nwhat\nis\nthe pattern\non the curtains\nIs this an african american girl ? Answer: yes no\nhu m\nan\nki tte\nn\nca t w in\ndo w\nw in\ndo w\nco uc\nh\npi ct\nur e\nco ffe\net ab\nle\npl an\nt\nru g\nis this an african american\ngirl\nIs the grass green ? Answer: yes\nhu m\nan\npu pp\ny\ncl ou\nd\ncl ou\nd\ntr ee sa nd\nbo x\nsu n so cc\ner\nis\nthe grass green\nB.2. Additional results: balanced dataset\nIs the young lady tempted to pet the dog ? Answer: yes\nhu m\nan\npu pp\ny\npi ct\nur e\npi ct\nur e\ndo or ru g\nis\nthe young\nlady tempted\nto\npet the dog\nIs the young lady tempted to pet the dog ? Answer: no\nhu m\nan\npu pp\ny\npi ct\nur e\npi ct\nur e\ndo or ru g\nis\nthe young\nlady tempted\nto\npet the dog\nIs she running to help him ? Answer: yes\nhu m\nan\nhu m\nan\npi ct\nur e\nw in\ndo w\nw in\ndo w\nru g co uc\nh\nis she running\nto help him\nIs she running to help him ? Answer: yes no\nhu m\nan\nhu m\nan\npi ct\nur e\nw in\ndo w\nw in\ndo w\nru g co uc\nh\nis she running\nto help him\nIs the man close to the left window ? Answer: yes no\nhu m\nan\nru g pi ct\nur e\npi ct\nur e\nco at\nra ck\nw in\ndo w\nw in\ndo w\nis\nthe\nman close\nto\nthe left window\nIs the man close to the left window ? Answer: no yes\nhu m\nan\nru g pi ct\nur e\npi ct\nur e\nco at\nra ck\nw in\ndo w\nw in\ndo w\nis\nthe\nman close\nto\nthe left window\nIs the dog white ? Answer: yes\nhu m\nan\nhu m\nan\npu pp\ny\nco uc\nh\npi ct\nur e\npi ct\nur e\nru g\nis\nthe dog\nwhite\nIs the dog white ? Answer: no\nhu m\nan\nhu m\nan\npu pp\ny\nco uc\nh\npi ct\nur e\npi ct\nur e\nru g\nis\nthe dog\nwhite\nIs this man hungry ? Answer: no yes\nhu m\nan\nco uc\nh\ndi ni\nng ta\nbl e\ndo or bo ok\nsh el\nf\npe tb\ned\nsa nd\nw ic\nh\nsa nd\nw ic\nh\npo pc\nan\ndo ll\nis this man hungry\nIs this man hungry ? Answer: yes no\nhu m\nan\nco uc\nh\ndi ni\nng ta\nbl e\ndo or bo ok\nsh el\nf\npe tb\ned\nsa nd\nw ic\nh\nsa nd\nw ic\nh\npo pc\nan\ndo ll\nis this man hungry\nIs the girl about to kick the soccer ball ? Answer: no yes\nhu m\nan\nhu m\nan\nm on\nke yb\nar s\nsu n sl id\ne so cc\ner\nis\nthe girl about\nto kick the soccer\nball\nIs the girl about to kick the soccer ball ? Answer: no\nhu m\nan\nhu m\nan\nm on\nke yb\nar s\nsu n sl id\ne fo ot\nba ll\nis\nthe girl about\nto kick the soccer\nball\nIs there a dog in the dog bed ? Answer: no yes\npu pp\ny\nco uc\nh\npl an\nt\nw in\ndo w\nw in\ndo w\npe tb\ned\nis there\na dog\nin\nthe dog bed\nIs there a dog in the dog bed ? Answer: no\npu pp\ny\nco uc\nh\npl an\nt\nw in\ndo w\nw in\ndo w\npe tb\ned\nis there\na dog\nin\nthe dog bed\nIs the young man dropping a football ? Answer: yes\nhu m\nan\ncl ou\nd\nsu n tr ee fo ot\nba ll\nm us\nhr oo\nm\nm us\nhr oo\nm\nis\nthe young\nman dropping\na football\nIs the young man dropping a football ? Answer: yes no\nhu m\nan\ncl ou\nd\nsu n tr ee fo ot\nba ll\nm us\nhr oo\nm\nm us\nhr oo\nm\nis\nthe young\nman dropping\na football\nIs the sun out ? Answer: yes\npi ge\non\nbu tte\nrf ly\nbu tte\nrf ly\nbu tte\nrf ly\nbu tte\nrf ly\nbi rd bi rd bi rd su n bu sh\nis\nthe sun out\nIs the sun out ? Answer: no\npi ge\non\nbu tte\nrf ly\nbu tte\nrf ly\nbu tte\nrf ly\nbu tte\nrf ly\nbi rd bi rd bi rd bu sh tr ee\nis\nthe sun out\nIs there a dog ? Answer: yes\nch ip\nm un\nk\ndo g du ck du ck ko i po nd su n bu sh bu sh bu sh\nis there\na dog\nIs there a dog ? Answer: no\nch ip\nm un\nk\ndu ck du ck ko i po nd su n bu sh bu sh bu sh bu sh\nis there\na dog\nIs the man jump roping ? Answer: no yes\nhu m\nan\nhu m\nan\ncl ou\nd\ncl ou\nd\ntr ee si de\nw al\nk\nju m\npr op\ne\nis\nthe\nman jump\nroping\nIs the man jump roping ? Answer: no\nhu m\nan\nhu m\nan\ncl ou\nd\ncl ou\nd\ntr ee si de\nw al\nk\nis\nthe\nman jump\nroping"}], "references": [], "referenceMentions": [], "year": 2016, "abstractText": "This paper proposes to improve visual question answer-<lb>ing (VQA) with structured representations of both scene<lb>contents and questions. A key challenge in VQA is to require<lb>joint reasoning over the visual and text domains. The pre-<lb>dominant CNN/LSTM-based approach to VQA is limited by<lb>monolithic vector representations that largely ignore struc-<lb>ture in the scene and in the form of the question. CNN fea-<lb>ture vectors cannot effectively capture situations as simple<lb>as multiple object instances, and LSTMs process questions<lb>as series of words, which does not reflect the true complexity<lb>of language structure. We instead propose to build graphs<lb>over the scene objects and over the question words, and we<lb>describe a deep neural network that exploits the structure<lb>in these representations. This shows significant benefit over<lb>the sequential processing of LSTMs. The overall efficacy of<lb>our approach is demonstrated by significant improvements<lb>over the state-of-the-art, from 71.2% to 74.4% in accuracy<lb>on the \u201cabstract scenes\u201d multiple-choice benchmark, and<lb>from 34.7% to 39.1% in accuracy over pairs of \u201cbalanced\u201d<lb>scenes, i.e. images with fine-grained differences and oppo-<lb>site yes/no answers to a same question.", "creator": "LaTeX with hyperref package"}}}