{"id": "1609.05650", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "19-Sep-2016", "title": "Multi-view Dimensionality Reduction for Dialect Identification of Arabic Broadcast Speech", "abstract": "In this work, we present a new Vector Space Model (VSM) of speech utterances for the task of spoken dialect identification. Generally, DID systems are built using two sets of features that are extracted from speech utterances; acoustic and phonetic. The acoustic and phonetic features are used to form vector representations of speech utterances in an attempt to encode information about the spoken dialects. The Phonotactic and Acoustic VSMs, thus formed, are used for the task of DID. The aim of this paper is to construct a single VSM that encodes information about spoken dialects from both the Phonotactic and Acoustic VSMs. Given the two views of the data, we make use of a well known multi-view dimensionality reduction technique known as Canonical Correlation Analysis (CCA), to form a single vector representation for each speech utterance that encodes dialect specific discriminative information from both the phonetic and acoustic representations. We refer to this approach as feature space combination approach and show that our CCA based feature vector representation performs better on the Arabic DID task than the phonetic and acoustic feature representations used alone. We also present the feature space combination approach as a viable alternative to the model based combination approach, where two DID systems are built using the two VSMs (Phonotactic and Acoustic) and the final prediction score is the output score combination from the two systems. In order to create an ideal design, we can use two different approach-specific models. Here we show that the two VSMs, each in a similar manner, represent the speech utterances with the full range of their articulation. We conclude that the two VSMs represent the speech utterances with the full range of their articulation. For example, it is possible to derive the VSM of speech utterances using the only three data structures for each speech utterance (Phonotactic and Acoustic VSMs), as shown in Fig. 1. This means that the VSM of speech utterances has a large size and does not represent speech utterances with all its parts.\n\n\n\nThe VSM of speech utterances is a multibillion-bit computer computer program. It can be used to analyze speech utterances using various techniques including: CCS, an ANOVA, ANOVA, ANOVA, ANOVA, ANOVA, ANOVA, ANOVA, ANOVA, ANOVA, ANOVA, ANOVA, ANOVA, ANOVA, ANOVA, ANOVA, AN", "histories": [["v1", "Mon, 19 Sep 2016 09:44:54 GMT  (1471kb,D)", "http://arxiv.org/abs/1609.05650v1", null]], "reviews": [], "SUBJECTS": "cs.CL", "authors": ["sameer khurana", "ahmed ali", "steve renals"], "accepted": false, "id": "1609.05650"}, "pdf": {"name": "1609.05650.pdf", "metadata": {"source": "CRF", "title": "MULTI-VIEW DIMENSIONALITY REDUCTION FOR DIALECT IDENTIFICATION OF ARABIC BROADCAST SPEECH", "authors": ["Sameer Khurana", "Ahmed Ali", "Steve Renals"], "emails": [], "sections": [{"heading": null, "text": "Index Terms\u2014 Canonical Correlation Analysis (CCA), Multi-view Dimensionality Reduction, Vector Space Model (VSM), Arabic Dialect Identification (DID)"}, {"heading": "1. INTRODUCTION", "text": "Dialect Identification (DID) problem is a special case of the more general problem of Language Identification (LID). LID refers to the process of automatically identifying the language class for given speech segment or text document, while DID classifies between dialects within the same language class, making it a more challenging task than LID. A good DID system used as a front-end to an automatic speech recognition system, can help improve the recognition performance\nby providing dialectal data for acoustic and language model adaptation to the specific dialect being spoken [1].\nIn this work, we focus on Arabic DID which can can be posed as a five class classification problem, given that the Arabic language can be divided into five major dialects; Egyptian (EGY), Gulf (GLF), Lavantine (LAV), Modern Standard Arabic (MSA) and North African (NOR) [2].\nOver the past decade, great advances have been made in the field of automatic language identification (LID). Research effort has focused on coming up with mathematical representations of speech utterances, that encodes the information about the language being spoken. These approaches are also known as Vector Space Modeling approaches [3], where speech utterances are represented by a continuous vector of high dimensions. Two predominant Vector Space Modeling approaches are Phonotactic and Acoustic. Phonotactic approaches attempt to model the n-gram phone statistics of speech. Phone sequences for each utterance are extracted using one or multiple phone recognisers. A Vector Space Model (VSM) is then constructed using a term-document matrix [4], followed by an unsupervised dimensionality reduction technique, such as Principal Component Analysis (PCA) [5] to map the high dimensional feature space to a low dimensional Vector Subspace (Section 2.1), giving a Phonotactic VSM. In other cases, a phone n-gram language model is used to model the phone statistics instead of a VSM [6, 7, 8]. On the other hand, Acoustic approaches attempt to extract dialect discriminative information from speech using low level acoustic features, such as pitch, prosody, shifted delta ceptral coefficients, bottleneck features [9, 10]. One of the most successful acoustic approaches is, the use of i-Vector framework for LID, where i-Vectors are extracted for each speech utterance, using an i-Vector extractor that consists of a GMM-UBM trained on top of BNF, followed by a Total Variability Subspace Model [2, 11]. The extracted i-Vectors give an Acoustic VSM (Section 2.2). These methods are also used for DID.\nEach of the two VSMs is used as an input to a back-end discriminative classifier, which is trained to find a suitable decision boundary in these vector spaces. This gives us two DID\nar X\niv :1\n60 9.\n05 65\n0v 1\n[ cs\n.C L\n] 1\n9 Se\np 20\nsystems built using the Acoustic and Phonotactic VSMs. At prediction time, output scores from the two DID systems are combined to give a final score, on the basis of which classification decision is made. This model combination approach has been shown to give performace improvements on the DID task [2]. This also shows that the two systems are complementary to each other, which leads us to investigate a feature space combination approach i.e. to construct a single VSM by combining Phonotactic and Acoustic VSMs, in an attempt to encode useful discriminative information in that single VSM.\nIn this work, we present a feature space combination approach. We form a combined VSM that incorporates useful information, necessary for DID, from both the Phonotactic and Acoustic VSMs. To achieve this goal, we make use of the well known multi-view dimensionality reduction technique known as Canonical Correlation Analysis (CCA), devloped by H. Hotelling [12] (Section 2.3). We show the performance of the combined VSM on Arabic DID task and compare it against the performance of Phonotactic and Acoustic VSMs used alone (Section 5). CCA VSM shows superior performance. The advantages of our feature space combination approach over model combination are two fold: Only one backend classifier needs to be trained and; Unlabeled data from other domain can easily be used in CCA framework to construct the single VSM. In this work, we do not experiment with unlabeled data and leave it as an extension to our current work."}, {"heading": "2. VECTOR SPACE MODELS", "text": "This section gives details about the construction of combined VSM, also referred to as CCA VSM, ZC. We start by presenting the Phonotactic VSM, XP and Acoustic VSM, XA, used in this work, followed by the section on CCA VSM, ZC ."}, {"heading": "2.1. Phonotactic VSM; XP", "text": "Phonotactic VSM is constructed by modeling the n-gram phone statistics of the phone sequences that are extracted using an Arabic phone recognizer. Details about the phone recognizer can be found in [2]. VSM is constructed in two steps; 1) Construct a term-document matrix, X \u2208 RN\u00d7d (See Fig 1), where each speech utterance in represented by a Phonotactic feature vector, p = (f(p, s1), f(p, s2), . . . , f(p, sd)) \u2208 Rd\u00d71, where N is the number of speech utterances and f(p, s) is the number of times a phone n-gram (term) s appears in the utterance (document) p and 2) Perform Truncated Singular Value Decomposition (SVD) (Equation 2) on X to learn a lower dimensional linear manifold, \u03a0 \u2208 Rd\u00d7k, where k << d. SVD attempts to discover the latent structure in the high dimensional feature space. Note that, k is the number of largest singular values. X is projected down to \u03a0 to get the Phonotactic VSM, XP (Equation 2).\nX\ufe38\ufe37\ufe37\ufe38 N\u00d7d = U\ufe38\ufe37\ufe37\ufe38 N\u00d7k \u2022 S\ufe38\ufe37\ufe37\ufe38 k\u00d7k \u2022 \u03a0T\ufe38\ufe37\ufe37\ufe38 k\u00d7d\n(1)\nXP\ufe38\ufe37\ufe37\ufe38 N\u00d7k = X\ufe38\ufe37\ufe37\ufe38 N\u00d7d \u2022 \u03a0\ufe38\ufe37\ufe37\ufe38 d\u00d7k\n(2)\nIn our case, the n-gram dictionary consisted of phone 2- grams and 3-grams with a total of 8K features i.e. d = 8K. The dimensionality of the VSM, XP , was chosen to be 1200, i.e. k = 1200. 1200 was the optimal value chosen experimentally."}, {"heading": "2.2. Acoustic VSM; XA", "text": "Acoustic VSM is constructed in two steps; 1) Extracting the bottleneck features (BNF) from speech and 2) Modeling BNF using the i-Vector extraction framework.\nWe use the same Deep Neural Network (DNN) based ASR system to extract the BNF as in our previous works [2, 13]. Two DNNs are used with 5 hidden layers and 1 Bottleneck Layer, all having sigmoidal neurons. Tied-phone states are used as the target to the DNNs. The target labels of dimension 3040 are provided by a GMM-HMM baseline system trained on 60 hours of Arabic Broadcast speech [14]. Input to the DNN consists of 11 consecutive frames stacked together, where for each frame 23 fbank features along with pitch and voicing probability are extracted. The output of the BN layer from the first DNN are fed as inputs to the second DNN, which acts as a correction DNN for the first model. Time offsets at the input layer of the second DNN are -10, -5, 0, 5 and 10, giving an overall context of 31 frames at the input of the second DNN. The BNF from the first DNN are used in the i-Vector modeling framework.\ni-Vector modeling framework consists of building a GMM-UBM on a large amount of data using acoustic features (BNF), to model the dialectal feature space. The sufficient statistics of the GMM-UBM give a general idea of the data spread in the high dimensional Vector Space. GMM-UBM mean supervector is updated while adapting it to each utterance. This update information is encoded in a low dimensional latent vector known as an i-Vector. The latent variable model used to extract i-Vector is called Total Variability Subspace Model and is given by the equation:\nM = u+ Tv\nwhere u is GMM-UBM mean supervector. v is the latent vector, known as the i \u2212 V ector and T is the lower dimensional Vector Subspace. The parameters of the model are estimated using Maximum Likelihood training criterion. For a detailed explanation of i-Vector modeling framework, reader is directed to excellent work in [15, 11].\nIn this work, GMM-UBM model has 2048 gaussian components, MFCC features are extracted using a 25 ms window and the i-Vectors are 400 dimensional [2].\nFinally, we construct the acoustic VSM, XA \u2208 RN\u00d7400, where the ith row is the 400 dimensional i-Vector representation corresponding to the speech utterance, ai. We also perform Linear Discriminant Analysis (LDA) and Within Class Co-variance Normalization (WCCN) on the Acoustic Vector Space, to increase the discriminative strength of the VSM. This method has been shown to improve DID (LID) performance [2, 11]."}, {"heading": "2.3. CCA VSM, ZC", "text": ""}, {"heading": "2.3.1. Brief Overview; CCA", "text": "Here, we give a brief overview of the mathematical foundations of the CCA. Fig 2 gives a probabilistic graphical model of CCA. Nodes of the graph represent Random Variables (RVs) and the structure encodes conditional independence assumptions. XP and XA are the Random Variables corresponding to the Phonotactic and Acoustic views of the data. Each data view is associated with two latent variables; 1) ZC , which is shared, and is the variable of interest that will form the final combined VSM, ZC and 2) \u03c6p and \u03c6a, which are the subspaces associated with the Phontactic and Acoustic views, respectively. CCA attempts to estimate \u03c6p and \u03c6a such that the correlation between the projections of the phonotactic feature vectors, p, on \u03c6p and acoustic feature vectors, a, on \u03c6a are maximized. Hence, CCA can be posed as the following optimization problem [16].\n[\u03c6p, \u03c6a] = arg max \u03c6p,\u03c6a \u03c6Tp CXPXA\u03c6a\u221a \u03c6Tp CXPXP \u03c6a \u221a \u03c6TaCXAXA\u03c6a\nThe above optimization formualtion can be massaged into the following eigenvalue problem. For details see [17].\nC\u22121XPXPCXPXAC \u22121 XAXACXAXP \u03c6p = \u03bb\u03c6p C\u22121XAXACXAXPC \u22121 XPXPCXPXA\u03c6a = \u03bb\u03c6a\nAn equivalent SVD formulation of the above eigenvalue problem is given below, which allows us to find \u03c6a and \u03c6p by performing SVD of C\u22121/2XPXPCXPXAC \u22121/2 XAXA . For the proof of this formulation, reader is referred to [16].\nC \u22121/2 XPXPCXPXAC \u22121/2 XAXA = \u03c6p\u039b\u03c6 T a (3)\nWe use the above formulation in this paper to learn the latent Vector Subspaces, \u03c6p and \u03c6a."}, {"heading": "2.3.2. Modeling", "text": "Given the two views, XP \u2208 RN\u00d71200 and XA \u2208 RN\u00d7400, for our speech data, we form a shared VSM, ZC \u2208 RN\u00d72c using CCA formulation discussed in Section 2.3.1. Note that, XP , XA and ZC are instantiations of the RVs XP , XA and ZC respectively as given in Fig 2. ZC\u2019s construction can be concisely given by the following two equations. \u03c6p\ufe38\ufe37\ufe37\ufe38\n1200\u00d7c , \u03c6a\ufe38\ufe37\ufe37\ufe38 c\u00d7400  = CCA  XP\ufe38\ufe37\ufe37\ufe38 N\u00d71200 , XA\ufe38\ufe37\ufe37\ufe38 N\u00d7400  (4) ZC\ufe38\ufe37\ufe37\ufe38 N\u00d72c = XP \u2022 \u03c6p\ufe38 \ufe37\ufe37 \ufe38 N\u00d7c \u2016XA \u2022 \u03c6Ta\ufe38 \ufe37\ufe37 \ufe38 N\u00d7c (5)\nwhere, CCA is performed by using the SVD formulation of equation 3.\nIn our case, the shared VSM\u2019s dimensionality is 600, i.e. c = 300. This value is the optimal VSM dimensionality that is experimentally decided. We also perform LDA and WCCN to increase the discriminability of the shared Vector space."}, {"heading": "3. DATA USED", "text": "Training and test data used in this work is the same as used in [2]. Table 1 gives the number of hours of data available for each dialect for training and testing.\nTable 2 shows the number of speech utterances that are available for training and testing the DID system.\nTraining data consist of recording from the Arabic Broadcast domain and contains utterances spoken in all the five dialects; EGY, GLF, LAV, MSA and NOR.\nThe test set is from the same broadcast domain but is collected from Al-Jazeera and hence, unlike training data set, the recording are of high quality. The test set is labeled using CrowdFlower, a crowd source platform, by QCRI and is publicly available on their web portal1. More details about the train and test data can be found in [2, 18]."}, {"heading": "4. SYSTEM DESCRIPTION", "text": "Fig 3 gives an overview of our DID system, which can be seen as a combination of two broad components; 1) Vector Space Modeling Component and 2) Back-end classifier.\nThe most important pieces of the DID system are the four latent Vector Subspaces; 1) \u03a0: which is learned by performing SVD on the n-gram phootactic term-document matrix (Section 2.1), 2) T: is the total variability subspace that is learned in an unsupervised manner in the i-Vector framework (Section 2.2), 3) \u03c6p: is the latent vector subspace corresponding to the Phonotactic VSM learned in the CCA Vector Space modeling framework and 4) \u03c6a: is the latent vector subspace corresponding to Acoustic VSM learned in the CCA vector space modeling framework (Section 2.3). The shared VSM is then fed to a back-end discriminative classifier. In our case, we use multi-class logistic regression, also known as softmax classification for DID [19, 20].\nThe parameter settings used for the softmax classifier can be found in Table 3. We use elastic net regularization i.e. using both an L1 and L2 regularizer. Stochastic Gradient Descent is used for learning the model parameters. Log-entropy loss function is used as the model training objective.\n1https://github.com/Qatar-Computing-Research-Institute/dialectID"}, {"heading": "5. EXPERIMENTS AND RESULTS", "text": ""}, {"heading": "5.1. Dialect Identification Results", "text": "Table 4 gives performance of different VSMs on Arabic DID task. The standalone Phonotactic VSM, XP, performs rather poorly as compared to the standalone Acoustic VSM, XA. CCA VSM, ZC, which is formed by combining XP and XA using CCA performs better than both standalone VSMs, which is an expected outcome because ZC carries dialect discriminative information from both XP and XA.\nFurther improvements in performance are due to performing LDA and WCCN on ZC. LDA is a supervised dimensionality reduction technique that form a lower dimensional Vector Subspace such that the class separability between the data points is maximized, and hence the performance gain that we see due to LDA is to be expected. An interesting observation to note is that the LDA based Acoustic VSM, XA+LDA+WCCN, performs at par with the LDA based CCA VSM, ZC+LDA+WCCN. This shows that some of the useful discriminative information from the Acoustic VSM is lost while performing CCA and hence we finally concatenate the two LDA based VSMs to get the final accuracy of 60% on the DID task."}, {"heading": "5.2. Confusion", "text": "Table 5 gives the confusion matrix for the Arabic DID task. We can infer the following: 1) EGY is confused most often with LAV, 2) GLF is confused most often with LAV and EGY, 3) LAV most often confused with EGY and GLF, 4) MSA is pretty well discriminated, which can also be confirmed by the VSM projection given by Fig 5.1, 5) NOR is most confused with EGY and LAV, which can also be seen in the VSM projection, where LAV is given by cyan region while EGY and LAV are given by blue and green dots."}, {"heading": "6. CONCLUSIONS", "text": "In this work, we showed our innovative approach to construct a single VSM for DID that carries dialect discriminative information from both the Acoustic and Phonotactic VSMs. To that end, we use a well known multi-view dimensionality reduction known as Canonical Correlation Analysis (CCA). The single VSM constructed performed better than any of the Phonotcatic or Acoustic VSMs alone, but the LDA based CCA and Acoustic VSMs performed at par on the DID taks, while their combination gave us our best VSM for Arabic DID. We conclude that some dialect specific discriminative information is lost while performing CCA between Acoustic and Phonotactic VSMs and hence the final combination performs better. CCA is an unsupervised method and can easily incorporate unlabeled data from a different domain and act as a domain adaptation or semi-supervised learning method such as co-training as shown in [21]. We leave co-training and domain adaptation using CCA for our future work."}, {"heading": "7. REFERENCES", "text": "[1] Fadi Biadsy, Automatic dialect and accent recognition and its application to speech recognition, Ph.D. thesis, Columbia University, 2011.\n[2] Ahmed Ali, Najim Dehak, Patrick Cardinal, Sameer Khurana, Sree Harsha Yella, James Glass, Peter Bell, and Steve Renals, \u201cAutomatic dialect detection in arabic broadcast speech,\u201d in Interspeech 2016, 2016, pp. 2934\u20132938.\n[3] Haizhou Li, Bin Ma, and Chin-Hui Lee, \u201cA vector space modeling approach to spoken language identification,\u201d IEEE Transactions on Audio, Speech, and Language Processing, vol. 15, no. 1, pp. 271\u2013284, 2007.\n[4] Scott Deerwester, Susan T Dumais, George W Furnas, Thomas K Landauer, and Richard Harshman, \u201cIndexing by latent semantic analysis,\u201d Journal of the American society for information science, vol. 41, no. 6, pp. 391, 1990.\n[5] Bruce Moore, \u201cPrincipal component analysis in linear systems: Controllability, observability, and model reduction,\u201d IEEE transactions on automatic control, vol. 26, no. 1, pp. 17\u201332, 1981.\n[6] Marc A Zissman and Kay M Berkling, \u201cAutomatic language identification,\u201d Speech Communication, vol. 35, no. 1, pp. 115\u2013124, 2001.\n[7] Fadi Biadsy, Julia Hirschberg, and Nizar Habash, \u201cSpoken arabic dialect identification using phonotactic modeling,\u201d in Proceedings of the eacl 2009 workshop on computational approaches to semitic languages. Association for Computational Linguistics, 2009, pp. 53\u201361.\n[8] Fadi Biadsy, Julia Hirschberg, and Daniel PW Ellis, \u201cDialect and accent recognition using phoneticsegmentation supervectors.,\u201d in INTERSPEECH, 2011, pp. 745\u2013748.\n[9] Pedro A Torres-Carrasquillo, Elliot Singer, Mary A Kohler, Richard J Greene, Douglas A Reynolds, and JR Deller Jr, \u201cApproaches to language identification using gaussian mixture models and shifted delta cepstral features x, i,\u201d .\n[10] David Mart\u0131\u0301nez, Luka\u0301s\u030c Burget, Luciana Ferrer, and Nicolas Scheffer, \u201civector-based prosodic system for language identification,\u201d in 2012 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). IEEE, 2012, pp. 4861\u20134864.\n[11] Najim Dehak, Pedro A Torres-Carrasquillo, Douglas A Reynolds, and Reda Dehak, \u201cLanguage recognition via i-vectors and dimensionality reduction.,\u201d in INTERSPEECH. Citeseer, 2011, pp. 857\u2013860.\n[12] H. Hotelling, \u201cCanonical correlation analysis (cca),\u201d in Journal of Educational Psychology, 1935.\n[13] Patrick Cardinal, Najim Dehak, Yu Zhang, and James Glass, \u201cSpeaker adaptation using the i-vector technique for bottleneck features,\u201d Proceedings of Interspeech, vol. 2015, 2015.\n[14] Ahmed Ali, Yifan Zhang, Patrick Cardinal, Najim Dahak, Stephan Vogel, and James Glass, \u201cA complete kaldi recipe for building arabic speech recognition systems,\u201d in Spoken Language Technology Workshop (SLT), 2014 IEEE. IEEE, 2014, pp. 525\u2013529.\n[15] Patrick Kenny, Pierre Ouellet, Najim Dehak, Vishwa Gupta, and Pierre Dumouchel, \u201cA study of interspeaker variability in speaker verification,\u201d IEEE Transactions on Audio, Speech, and Language Processing, vol. 16, no. 5, pp. 980\u2013988, 2008.\n[16] Paramveer S Dhillon, Dean P Foster, and Lyle H Ungar, \u201cEigenwords: Spectral word embeddings,\u201d The Journal of Machine Learning Research, vol. 16, no. 1, pp. 3035\u2013 3078, 2015.\n[17] David R Hardoon, Sandor Szedmak, and John ShaweTaylor, \u201cCanonical correlation analysis; an overview with application to learning methods,\u201d 2003.\n[18] Samantha Wray and Ahmed Ali, \u201cCrowdsource a little to label a lot: Labeling a speech corpus of dialectal arabic,\u201d in INTERSPEECH, 2015.\n[19] Kevin P Murphy, Machine learning: a probabilistic perspective, MIT press, 2012.\n[20] Bianca Zadrozny and Charles Elkan, \u201cTransforming classifier scores into accurate multiclass probability estimates,\u201d in Proceedings of the eighth ACM SIGKDD international conference on Knowledge discovery and data mining. ACM, 2002, pp. 694\u2013699.\n[21] Dean P Foster, Sham M Kakade, and Tong Zhang, \u201cMulti-view dimensionality reduction via canonical correlation analysis,\u201d ."}], "references": [{"title": "Automatic dialect and accent recognition and its application to speech recognition", "author": ["Fadi Biadsy"], "venue": "Ph.D. thesis, Columbia University,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2011}, {"title": "Automatic dialect detection in arabic broadcast speech", "author": ["Ahmed Ali", "Najim Dehak", "Patrick Cardinal", "Sameer Khurana", "Sree Harsha Yella", "James Glass", "Peter Bell", "Steve Renals"], "venue": "Interspeech 2016, 2016, pp. 2934\u20132938.", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2016}, {"title": "A vector space modeling approach to spoken language identification", "author": ["Haizhou Li", "Bin Ma", "Chin-Hui Lee"], "venue": "IEEE Transactions on Audio, Speech, and Language Processing, vol. 15, no. 1, pp. 271\u2013284, 2007.", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2007}, {"title": "Indexing by latent semantic analysis", "author": ["Scott Deerwester", "Susan T Dumais", "George W Furnas", "Thomas K Landauer", "Richard Harshman"], "venue": "Journal of the American society for information science, vol. 41, no. 6, pp. 391, 1990.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 1990}, {"title": "Principal component analysis in linear systems: Controllability, observability, and model reduction", "author": ["Bruce Moore"], "venue": "IEEE transactions on automatic control, vol. 26, no. 1, pp. 17\u201332, 1981.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 1981}, {"title": "Automatic language identification", "author": ["Marc A Zissman", "Kay M Berkling"], "venue": "Speech Communication, vol. 35, no. 1, pp. 115\u2013124, 2001.", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2001}, {"title": "Spoken arabic dialect identification using phonotactic modeling", "author": ["Fadi Biadsy", "Julia Hirschberg", "Nizar Habash"], "venue": "Proceedings of the eacl 2009 workshop on computational approaches to semitic languages. Association for Computational Linguistics, 2009, pp. 53\u201361.", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2009}, {"title": "Dialect and accent recognition using phoneticsegmentation supervectors", "author": ["Fadi Biadsy", "Julia Hirschberg", "Daniel PW Ellis"], "venue": "INTERSPEECH, 2011, pp. 745\u2013748.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2011}, {"title": "Approaches to language identification using gaussian mixture models and shifted delta cepstral features x, i", "author": ["Pedro A Torres-Carrasquillo", "Elliot Singer", "Mary A Kohler", "Richard J Greene", "Douglas A Reynolds", "JR Deller Jr"], "venue": ".", "citeRegEx": "9", "shortCiteRegEx": null, "year": 0}, {"title": "ivector-based prosodic system for language identification", "author": ["David Mart\u0131\u0301nez", "Luk\u00e1\u0161 Burget", "Luciana Ferrer", "Nicolas Scheffer"], "venue": "2012 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). IEEE, 2012, pp. 4861\u20134864.", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2012}, {"title": "Language recognition via i-vectors and dimensionality reduction", "author": ["Najim Dehak", "Pedro A Torres-Carrasquillo", "Douglas A Reynolds", "Reda Dehak"], "venue": "INTER- SPEECH. Citeseer, 2011, pp. 857\u2013860.", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2011}, {"title": "Canonical correlation analysis (cca)", "author": ["H. Hotelling"], "venue": "Journal of Educational Psychology, 1935.", "citeRegEx": "12", "shortCiteRegEx": null, "year": 1935}, {"title": "Speaker adaptation using the i-vector technique for bottleneck features", "author": ["Patrick Cardinal", "Najim Dehak", "Yu Zhang", "James Glass"], "venue": "Proceedings of Interspeech, vol. 2015, 2015.", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2015}, {"title": "A complete kaldi recipe for building arabic speech recognition systems", "author": ["Ahmed Ali", "Yifan Zhang", "Patrick Cardinal", "Najim Dahak", "Stephan Vogel", "James Glass"], "venue": "Spoken Language Technology Workshop (SLT), 2014 IEEE. IEEE, 2014, pp. 525\u2013529.", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2014}, {"title": "A study of interspeaker variability in speaker verification", "author": ["Patrick Kenny", "Pierre Ouellet", "Najim Dehak", "Vishwa Gupta", "Pierre Dumouchel"], "venue": "IEEE Transactions on Audio, Speech, and Language Processing, vol. 16, no. 5, pp. 980\u2013988, 2008.", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2008}, {"title": "Eigenwords: Spectral word embeddings", "author": ["Paramveer S Dhillon", "Dean P Foster", "Lyle H Ungar"], "venue": "The Journal of Machine Learning Research, vol. 16, no. 1, pp. 3035\u2013 3078, 2015.", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2015}, {"title": "Canonical correlation analysis; an overview with application to learning methods", "author": ["David R Hardoon", "Sandor Szedmak", "John Shawe- Taylor"], "venue": "2003.", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2003}, {"title": "Crowdsource a little to label a lot: Labeling a speech corpus of dialectal arabic", "author": ["Samantha Wray", "Ahmed Ali"], "venue": "INTERSPEECH, 2015.", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2015}, {"title": "Machine learning: a probabilistic perspective", "author": ["Kevin P Murphy"], "venue": "MIT press,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2012}, {"title": "Transforming classifier scores into accurate multiclass probability estimates", "author": ["Bianca Zadrozny", "Charles Elkan"], "venue": "Proceedings of the eighth ACM SIGKDD international conference on Knowledge discovery and data mining. ACM, 2002, pp. 694\u2013699.", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2002}, {"title": "Multi-view dimensionality reduction via canonical correlation analysis", "author": ["Dean P Foster", "Sham M Kakade", "Tong Zhang"], "venue": ".", "citeRegEx": "21", "shortCiteRegEx": null, "year": 0}], "referenceMentions": [{"referenceID": 0, "context": "A good DID system used as a front-end to an automatic speech recognition system, can help improve the recognition performance by providing dialectal data for acoustic and language model adaptation to the specific dialect being spoken [1].", "startOffset": 234, "endOffset": 237}, {"referenceID": 1, "context": "In this work, we focus on Arabic DID which can can be posed as a five class classification problem, given that the Arabic language can be divided into five major dialects; Egyptian (EGY), Gulf (GLF), Lavantine (LAV), Modern Standard Arabic (MSA) and North African (NOR) [2].", "startOffset": 270, "endOffset": 273}, {"referenceID": 2, "context": "These approaches are also known as Vector Space Modeling approaches [3], where speech utterances are represented by a continuous vector of high dimensions.", "startOffset": 68, "endOffset": 71}, {"referenceID": 3, "context": "A Vector Space Model (VSM) is then constructed using a term-document matrix [4], followed by an unsupervised dimensionality reduction technique, such as Principal Component Analysis (PCA) [5] to map the high dimensional feature space to a low dimensional Vector Subspace (Section 2.", "startOffset": 76, "endOffset": 79}, {"referenceID": 4, "context": "A Vector Space Model (VSM) is then constructed using a term-document matrix [4], followed by an unsupervised dimensionality reduction technique, such as Principal Component Analysis (PCA) [5] to map the high dimensional feature space to a low dimensional Vector Subspace (Section 2.", "startOffset": 188, "endOffset": 191}, {"referenceID": 5, "context": "In other cases, a phone n-gram language model is used to model the phone statistics instead of a VSM [6, 7, 8].", "startOffset": 101, "endOffset": 110}, {"referenceID": 6, "context": "In other cases, a phone n-gram language model is used to model the phone statistics instead of a VSM [6, 7, 8].", "startOffset": 101, "endOffset": 110}, {"referenceID": 7, "context": "In other cases, a phone n-gram language model is used to model the phone statistics instead of a VSM [6, 7, 8].", "startOffset": 101, "endOffset": 110}, {"referenceID": 8, "context": "On the other hand, Acoustic approaches attempt to extract dialect discriminative information from speech using low level acoustic features, such as pitch, prosody, shifted delta ceptral coefficients, bottleneck features [9, 10].", "startOffset": 220, "endOffset": 227}, {"referenceID": 9, "context": "On the other hand, Acoustic approaches attempt to extract dialect discriminative information from speech using low level acoustic features, such as pitch, prosody, shifted delta ceptral coefficients, bottleneck features [9, 10].", "startOffset": 220, "endOffset": 227}, {"referenceID": 1, "context": "One of the most successful acoustic approaches is, the use of i-Vector framework for LID, where i-Vectors are extracted for each speech utterance, using an i-Vector extractor that consists of a GMM-UBM trained on top of BNF, followed by a Total Variability Subspace Model [2, 11].", "startOffset": 272, "endOffset": 279}, {"referenceID": 10, "context": "One of the most successful acoustic approaches is, the use of i-Vector framework for LID, where i-Vectors are extracted for each speech utterance, using an i-Vector extractor that consists of a GMM-UBM trained on top of BNF, followed by a Total Variability Subspace Model [2, 11].", "startOffset": 272, "endOffset": 279}, {"referenceID": 1, "context": "This model combination approach has been shown to give performace improvements on the DID task [2].", "startOffset": 95, "endOffset": 98}, {"referenceID": 11, "context": "Hotelling [12] (Section 2.", "startOffset": 10, "endOffset": 14}, {"referenceID": 1, "context": "Details about the phone recognizer can be found in [2].", "startOffset": 51, "endOffset": 54}, {"referenceID": 1, "context": "We use the same Deep Neural Network (DNN) based ASR system to extract the BNF as in our previous works [2, 13].", "startOffset": 103, "endOffset": 110}, {"referenceID": 12, "context": "We use the same Deep Neural Network (DNN) based ASR system to extract the BNF as in our previous works [2, 13].", "startOffset": 103, "endOffset": 110}, {"referenceID": 13, "context": "The target labels of dimension 3040 are provided by a GMM-HMM baseline system trained on 60 hours of Arabic Broadcast speech [14].", "startOffset": 125, "endOffset": 129}, {"referenceID": 14, "context": "For a detailed explanation of i-Vector modeling framework, reader is directed to excellent work in [15, 11].", "startOffset": 99, "endOffset": 107}, {"referenceID": 10, "context": "For a detailed explanation of i-Vector modeling framework, reader is directed to excellent work in [15, 11].", "startOffset": 99, "endOffset": 107}, {"referenceID": 1, "context": "In this work, GMM-UBM model has 2048 gaussian components, MFCC features are extracted using a 25 ms window and the i-Vectors are 400 dimensional [2].", "startOffset": 145, "endOffset": 148}, {"referenceID": 1, "context": "This method has been shown to improve DID (LID) performance [2, 11].", "startOffset": 60, "endOffset": 67}, {"referenceID": 10, "context": "This method has been shown to improve DID (LID) performance [2, 11].", "startOffset": 60, "endOffset": 67}, {"referenceID": 15, "context": "Hence, CCA can be posed as the following optimization problem [16].", "startOffset": 62, "endOffset": 66}, {"referenceID": 16, "context": "For details see [17].", "startOffset": 16, "endOffset": 20}, {"referenceID": 15, "context": "For the proof of this formulation, reader is referred to [16].", "startOffset": 57, "endOffset": 61}, {"referenceID": 1, "context": "Training and test data used in this work is the same as used in [2].", "startOffset": 64, "endOffset": 67}, {"referenceID": 1, "context": "More details about the train and test data can be found in [2, 18].", "startOffset": 59, "endOffset": 66}, {"referenceID": 17, "context": "More details about the train and test data can be found in [2, 18].", "startOffset": 59, "endOffset": 66}, {"referenceID": 18, "context": "In our case, we use multi-class logistic regression, also known as softmax classification for DID [19, 20].", "startOffset": 98, "endOffset": 106}, {"referenceID": 19, "context": "In our case, we use multi-class logistic regression, also known as softmax classification for DID [19, 20].", "startOffset": 98, "endOffset": 106}, {"referenceID": 20, "context": "CCA is an unsupervised method and can easily incorporate unlabeled data from a different domain and act as a domain adaptation or semi-supervised learning method such as co-training as shown in [21].", "startOffset": 194, "endOffset": 198}], "year": 2016, "abstractText": "In this work, we present a new Vector Space Model (VSM) of speech utterances for the task of spoken dialect identification. Generally, DID systems are built using two sets of features that are extracted from speech utterances; acoustic and phonetic. The acoustic and phonetic features are used to form vector representations of speech utterances in an attempt to encode information about the spoken dialects. The Phonotactic and Acoustic VSMs, thus formed, are used for the task of DID. The aim of this paper is to construct a single VSM that encodes information about spoken dialects from both the Phonotactic and Acoustic VSMs. Given the two views of the data, we make use of a well known multi-view dimensionality reduction technique known as Canonical Correlation Analysis (CCA), to form a single vector representation for each speech utterance that encodes dialect specific discriminative information from both the phonetic and acoustic representations. We refer to this approach as feature space combination approach and show that our CCA based feature vector representation performs better on the Arabic DID task than the phonetic and acoustic feature representations used alone. We also present the feature space combination approach as a viable alternative to the model based combination approach, where two DID systems are built using the two VSMs (Phonotactic and Acoustic) and the final prediction score is the output score combination from the two systems.", "creator": "LaTeX with hyperref package"}}}