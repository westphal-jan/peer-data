{"id": "1609.06026", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "20-Sep-2016", "title": "An Approach for Self-Training Audio Event Detectors Using Web Data", "abstract": "Audio event detection in the era of Big Data has the constraint of lacking annotations to train robust models that match the scale of class diversity. This is mainly due to the expensive and time-consuming process of manually annotating sound events in isolation or as segments within audio recordings. In this paper, we propose an approach for semi-supervised self-training of audio event detectors using unlabeled web data. We started with a small annotated dataset and trained sound events detectors. Then, we crawl and collect thousands of web videos and extract their soundtrack. The segmented soundtracks are run by the detectors and different selection techniques were used to determine whether a segment should be used for self-training the detectors. The original detectors were compared to the self-trained detectors and the results showed a performance improvement by the latter when evaluated on the annotated test set.\n\n\n\n\nWe also present a general view of how audio recordings are structured within the software. In this paper we introduce the algorithm to train models based on real time spatial information generated on the world record. We show a new algorithm for the encoding and encoding of audio from the self-trained speakers. Using a simple algorithm, we use the algorithm for the encoding, decoding, and processing of each individual sounds. We use two separate versions of the algorithm and the three versions of the algorithm for the encoding and decoding of audio from the self-trained speakers. We describe how the algorithm produces sound, the source of the sound, and the resulting audio output.\n\n\nThe algorithm is implemented as a preloaded tool with an interface that stores a set of unblabeled web files and then parses the raw audio for the self-trained listeners. The program consists of a library that contains audio in it which can be decompiled by importing audio from a streaming system (such as Audiotape, MP3, MP3 or MP3).\nThis library provides the following examples. The source of the audio is an Open source extension of OpenSUSE's web-data library (OBS), which was made available as a source of OpenSUSE's web-data library (TLS). The source of the audio is also available as a source of a library for using, for the recording of audio, for recording of sound, and for storing data, to convert and store data. For example, the audio source is found in the audio sources of this library.\nThe source of the audio is found in the audio sources of the audio and is not a library, and therefore", "histories": [["v1", "Tue, 20 Sep 2016 05:52:06 GMT  (65kb,D)", "https://arxiv.org/abs/1609.06026v1", "5 pages"], ["v2", "Fri, 23 Sep 2016 14:15:13 GMT  (65kb,D)", "http://arxiv.org/abs/1609.06026v2", "5 pages"], ["v3", "Tue, 27 Jun 2017 17:09:59 GMT  (1760kb,D)", "http://arxiv.org/abs/1609.06026v3", "5 pages"]], "COMMENTS": "5 pages", "reviews": [], "SUBJECTS": "cs.SD cs.LG cs.MM", "authors": ["benjamin elizalde", "ankit shah", "siddharth dalmia", "min hun lee", "rohan badlani", "anurag kumar", "bhiksha raj", "ian lane"], "accepted": false, "id": "1609.06026"}, "pdf": {"name": "1609.06026.pdf", "metadata": {"source": "CRF", "title": "An Approach for Self-Training Audio Event Detectors Using Web Data", "authors": ["Benjamin Elizalde", "Ankit Shah", "Siddharth Dalmia", "Min Hun Lee", "Rohan Badlani", "Anurag Kumar", "Bhiksha Raj", "Ian Lane"], "emails": ["bmartin1@andrew.cmu.edu,", "ankit.tronix@gmail.com,", "sdalmia@andrew.cmu.edu,", "mhlee@cmu.edu,", "rohan.badlani@gmail.com,", "alnu@andrew.cmu.edu,", "bhiksha@cs.cmu.edu,", "lane@cs.cmu.edu"], "sections": [{"heading": null, "text": "I. INTRODUCTION AND RELATED WORK\nSounds are essential to how humans perceive and interact with the world. Audio content is captured in recordings and shared on the web on a minute-by-minute basis. Academia and industry exploits this acoustic information throughout multiple applications. The dominant application is multimedia video content analysis, where audio is combined with images and text [1], [2] to index, search and retrieve videos. Another task is human-robot interaction [3], [4], where sounds complement speech as non-verbal communication. Recently, a growing application is in smart cities [5], where sounds are used to detect sources of noise pollution. All of these applications rely on Audio Event Detection (AED) to recognize the occurrence of sounds within audio and video recordings.\nThe related work on AED has mainly focused on using available datasets to train machine learning models in a supervised manner [6], [5], [7], [8], [9]. However, the largest data set, ESC-50 [8], contains only 40 samples per class. The numbers strongly contrast with Imagenet, the computer vision counterpart, which has hundreds of samples per class. Hence, training a model which reflects the acoustic diversity of an audio event class is limited. The common solution is to have humans annotating more data. However, the process is costly and slow and thus, other solutions should be explored.\n\u00a7 First six authors contributed equally.\nAnother solution is to combine the small amount of labeled data with a large amount of unlabeled data. A particular method is semi-supervised self-training, which is an algorithm that iteratively re-trains a model. First, the model is trained using the labeled data set. Then, at each iteration and under a certain criteria, a portion of the unlabeled set could be labeled as any of the known classes. Lastly, using the newly labeled data, the model is re-trained. This approach has been explored for audio events in two papers [10], [11]. Particularly in [10], the authors collected 17,000 labeled audio-only recordings from FindSounds.com. Two thirds were used to train and test a classifier and the rest was treated as unlabeled audio for re-training. The result was an improvement of 1.4% precision over the baseline, suggesting a valid alternative to improve models. Moreover, the authors pointed out the challenges of utilizing audio-only web recordings.\nIn our paper, we followed a similar framework of semisupervised self-learning, but with the following differences:\n\u2022 We employed UrbanSounds8k as the labeled set for training and testing. However, we collected YouTube videos as the unlabeled set for re-training, creating mismatch conditions. \u2022 For re-training, we used 30 times more audio files. \u2022 The unlabeled audio is extracted from videos as opposed\nto audio-only recordings. Hence, posing challenges during the collection process. For instance, it is not possible to guarantee that the YouTube audio will actually contain any of the sounds.\nThe paper is structured as follows, in Section II we describe the flow of our self-training approach for sound detectors. Within this Section we describe the sound event dataset and YouTube video collection process and how we pre-processed the audio recordings. Then, we explain how we trained our two machine learning based detectors to compare performance. In Section III we compare the baseline performance and the selftraining performance obtained with different techniques.\nar X\niv :1\n60 9.\n06 02\n6v 3\n[ cs\n.S D\n] 2\n7 Ju\nn 20"}, {"heading": "II. SEMI-SUPERVISED SELF-TRAINING OF AUDIO EVENT DETECTORS", "text": "Semi-supervised self-training is an algorithm that iteratively re-trains a model and our particular framework is illustrated in Figure 1. First, using the labeled dataset, the ten class detectors are trained and tested to compute a baseline performance. Second, the unlabeled data is run by the detectors to obtain a class label with its corresponding confidence score. Third, we applied a threshold based on the confidence score to determine candidates for self-training the detectors. Fourth, the detectors are re-trained and again tested on the labeled data to compute the new performance and compare it with the previous. Lastly, steps two, three and four are performed iteratively until the performance converges."}, {"heading": "A. Datasets: Labeled and Unlabeled", "text": "Labeled Data (Training and Testing): UrbanSound8K The UrbanSound8K (US8K) dataset [5] has 10 classes: air conditioner, car horn, children playing, dog bark, and street music, gun shot, drilling, engine idling, siren, jackhammer. The content of the audio may have other overlapping sounds and the target sound may occur in the background or in the foreground. The dataset has about 8,732 audio segments of 3.5 sec average duration. These files are distributed into 10 stratified cross-validation folds.\nUnlabeled Data (Self-Training): YouTube videos The unlabeled audio comes from YouTube videos and videos are the largest source of audio. The website was chosen because it offers a wide diversity of class samples. The soundtracks of the videos were crawled and downloaded using\nthe Pafy API1. The audio roughly corresponds to the 10 classes from US8k. The acoustic content is unstructured, and commonly the target sound is occluded by multiple factors such as noise, overlap with other sounds, and channel effects. The web set has 200,000 segments of 3.5 seconds. We converted all of the audio files into raw 16 bit encoding, mono-channel, and 16 kHz sampling rate.\nChallenges of Unlabeled Data Collection Audio from videos poses collection challenges. YouTube contains years of videos and in order to process and evaluate audio containing the target sound, the query should serve as a filter. Hence, the query formulation aims to filter in videos roughly matching the ten classes in US8K. Typing a query composed by a noun such as air conditioner will not necessarily fetch a video containing such sound event. This happens because the associated tags and metadata are mainly inspired by the video\u2019s visual content; contrary to what happens in audioonly websites such as freesounds.org. Therefore, we modified the query to be a combination of keywords: \u201c<audio event label>sound\u201d, for example,\u201cair conditioner sound\u201d. Although the results empirically improved, another issue was that the audio event was not guaranteed to occur and if present it most likely occurred with a short duration within whole recording. Therefore, we restricted the video length to be larger than five seconds and shorter than ten minutes to reduce the amount of irrelevant audio."}, {"heading": "B. Data Preparation", "text": "Extracting Low-level Features: MFCCs The Mel Frequency Cepstral Coefficients (MFCCs) have been widely used in audio event detection [12], [5], [8]. The parameters are standard, such as 10 ms shifts, window of 25 ms and 20 cepstral coefficients including delta and doubledelta (time dynamics) for a total of 60 coefficients for each time window or vector.\nExtracting Intermediate Features: BoAWs An effective approach for characterizing audio events is the Bag-of-Audio-Words (BoAWs) feature representation, which is usually built over low-level features such as MFCCs. The method we followed to compute BoAWs features is broadly illustrated in Figure 2 and detailed in these papers [13], [14]. In the first step, we put all the MFCCs in the training set together. In the second step, we learn an \u201caudio vocabulary\u201d by grouping the features into \u201caudio words\u201d. In contrast to conventional approaches which uses clustering for grouping words, our method adapts the MFCCs to a Gaussian Mixture Model (GMM) using Expectation Maximization where each mixture represents a word. The third step is quantization, which uses the created vocabulary to turn the MFCC matrix of a given recording into a BoAW histogram-vector of the size of the vocabulary. The conventional quantization process computes the distance of each MFCCs frame to all the audio words and sums the value of one only on the histogram bin that\n1https://pypi.python.org/pypi/pafy\ncorresponds to the closest word. However, our approach uses soft-quantization, which sums probabilities for all the words.\nFormally, let the MFCC vectors of a recording be represented as ~xt. ~xt is tth D dimensional MFCC frame, t = 1 to T . Here, the GMM G = {wk, N(~\u00b5k,\u03a3k), k = 1 to M}, is learned over the MFCCs of training data. wk, ~\u00b5k and \u03a3k are the mixture weight, mean and co-variance parameters respectively, of the kth Gaussian in G. We train GMM with diagonal co-variance matrices. To obtain the bag of audio word feature representation for any given recording, we first compute the probabilistic assignment to kth Gaussian for each MFCC frame of that recording as in Equation 1. This soft assignment is then summed and normalized over all MFCC frames for kth Gaussian as in Eq 2.\nPr(k|~xt) = wkN(~xt; ~\u00b5k,\u03a3k) M\u2211 j=1 wjN(~xt; ~\u00b5k,\u03a3k)\n(1)\nP (k) = 1\nT T\u2211 i=1 Pr(k|~xt) (2)\nThe final soft-count histogram feature representation, represented as ~\u03b1 is ~\u03b1M = [P (1), ..P (k)..P (M)]T . ~\u03b1M features are an M -dimensional (M=128) feature representation for any given recording. During testing, the BoAW features are computed in a similar manner however, using the created vocabulary from training.\n1) Training Detectors: Positive and Negative Classes: We chose detectors\u2013binary classifiers, because are able to recognize the presence or absence of a particular audio event in a recording. The binary setup also aims to simulate the imbalance ratio of small amount of target sound vs a large amount of non-target sounds, which is common in web retrieval tasks. The audio samples belonging to the target class are referred to as positives and those samples not belonging to the target are referred to as negatives. Each of the ten detectors is trained with both, a positive and a negative class. Positive contains class samples and negative contains samples from the rest of the classes. For instance, the detector for jackhammer has all the samples corresponding to jackhammer as positives and all the samples corresponding to the other 9 sounds as negatives.\n2) Training Detectors: SVM: One round of experiments was performed with Support Vector Machines (SVMs) because SVMs have been widely explored for sound events [6]. The ten SVM-based detectors used linear decision boundaries to fit the data and were trained with the intermediate features. To relax\nthe constraints defining the margin of the decision boundary, the parameter \u201cC\u201d was tuned and set to 0.01. Then, the trained detectors were evaluated using the test set. Although conventional SVMs could employ other techniques to allow non-linear decision boundaries, the problem happens when new audio segments are added for re-training. Each iteration means that the SVM has to be re-trained from scratch using all the train data. The consequence is a bottleneck, which worsens as more segments and iterations are added.\n3) Training Detectors: NN: Considering the previous issue, the second round of experiments was performed with Neural Networks (NNs). The NNs are more suitable for the iterative nature of self-training. For example, in order to add a new audio segment for training, the NN does not need to be retrained from scratch and a quick updating process suffice. The NN-based detectors are also binary classifiers. More precisely, for the NN we utilized a Multi-Layer Perceptron (MLP), with tuned hyper-parameters such as number of layers, neurons, activation function, regularization and loss function. The final architecture consisted of an input of size 128\u2013BoW features dimensionality, one hidden layer of 100 neurons, and two output units\u2013 class or not class. The activation function was \u201ctanh\u201d, the regularization method was dropout (p=0.5), the loss function was cross-entropy and the number of epochs was 10. Then, the trained detectors were evaluated using the labeled test set."}, {"heading": "III. EXPERIMENTS AND EVALUATION OF METHODS", "text": ""}, {"heading": "A. Computing the Baseline Performance and Running Detectors on Unlabeled Data", "text": "The initial performance computed by our two machine learning algorithms defined the baseline to improve after selftraining. The detectors used the labeled data from US8k, which comes divided in 10 stratified folds. We used 9 folds as training data and tested on the left-out fold. This is done in 10 different ways, resulting in 100 runs for all the 10 events classes and 10 folds. We evaluated our detectors using average precision as we wanted to detect reliable positive or negative samples. For every class, the average precision (AP) over each fold is computed, as well as the mean AP across all folds referred as Mean AP. Afterwards, the detectors where run on the unlabeled dataset to obtain confidence scores and labels for each of the 200,000 segments. Note that the unlabeled data was carefully handled to be consistent with the 10 fold cross-validation setup. For example, the detectors trained using the first 9 folds may not yield the same performance as the detectors trained with any other fold combination."}, {"heading": "B. Selecting Candidates and Self-Training Detectors", "text": "We employed a high confidence threshold to select audio segments as candidates for self-training. The candidates were used for self-training the detectors in combination to the supervised audio segments. Once the detectors were re-trained, they were ran on the supervised test set and their performance was computed. The Mean AP value was compared with the\nbaseline and the whole process was repeated iteratively until the Mean AP converged.\nA key step in the self-training process is the selection of candidates. We tried three main approaches: Detector\u2019s output scores, precision and clarity index.\nScore-based Under this approach, the output of the detector is a probability score that can be interpreted as a confidence value and has been used for self-training in the paper [10]. A score threshold of greater or equal than 0.95 was selected to filter in any segment, where 0 means the lowest confidence and 1 means the strongest confidence.\nPrecision High precision means that the detector returned more relevant results than irrelevant ones. A precision threshold of greater or equal than 0.95 was set. The value range is the same as the score-based.\nClarity Index Clarity Index (CI), based on the paper [15], aims to determine those segments that are the most confusing for the detector. CI is based on two losses called relevance loss and irrelevance loss. To understand these losses let us assume that the training data is D = {(x1, y1), (x2, y2)..., (xn, yn)} and the detector mapping function is denoted by f . Let xu be an unlabeled data point. The Relevance Loss (RL) and the Irrelevance Loss (IL) are defined as\nRL(xu, f) = 1 |D0| \u2211\nxi\u2208D0\nI(f(xi)\u2212 f(xu)) (3)\nIL(xu, f) = 1 |D1| \u2211\nxi\u2208D1\nI(f(xu)\u2212 f(xi)) (4)\nThe relevance loss is expected to be low if xu is relevant (positive) and irrelevance loss is expected to be low irrelevant (negative). The difference of the two losses CI = IL\u2212RL is expected to be high (close to 1) for positive instances and low (close to -1) for negative instance. Overall, the CI helps us rank unlabeled segments to choose better segments for self-training. Higher CI implies that Xu is more likely to be positive. An unlabeled point with very high CI would have outscored a large number of training points and hence is expected to be positive. Similarly, lower CI implies the instance is most likely negative."}, {"heading": "IV. RESULTS AND DISCUSSION", "text": ""}, {"heading": "A. Baseline", "text": "The NN outperformed the SVM by an absolute 8.5% in the baseline performance. The Mean AP score was 57.8% for SVM and 66.3% for NN and are shown in Table I. One reason for to justify the better performance of the NN, is that it employed nonlinear decision boundaries to fit the data unlike the SVM, which used linear boundaries. As mentioned before, the SVM can also support nonlinear decision boundaries by using kernels, but the computation time was an issue for processing 200,000 segments for the 10 fold combinations, and the re-training."}, {"heading": "B. Self-Training", "text": "The main results of this paper are the improvement gain by self-training shown in Table I and labeled as SVM Best and NN Best. The overall Mean AP improvement was 1.2% for both classifiers. Except for SVM\u2019s dog barking, all the audio events improved their performance. Particularly, air conditioning and jackhammer benefited the most with about 3%. More importantly, the performance did not degrade, which is expected to happen when audio that not belonging to the target class is added by re-training. The SVM Best and NN Best results correspond to different threshold types\u2013 CI and Pecision respectively. The performance of the three threshold types was similar (0.5%-1.4%) and we cannot say that one should be preferred.\nFor the three threshold types, tuning affected differently the overall selection of candidates and the detection performance. The number of candidates varied between class from 0 to 2,000 on each iteration. In general, stricter values (greater than 0.9) reduced the number of candidates to two digits. Regarding the detection performance, stricter values (greater than 0.95) and loose values (approximately 0.5) degraded Mean AP, but values close to 0.9 yielded the reported gain. The threshold also defined the number of iterations the algorithm took to converged or stop improving. For our value of 0.9, our algorithm iterated three times. Afterwards, the Mean AP performance converged and then slowly decreased. In general, most of our experiments degraded its performance after several iterations. Two possible explanation are the mismatch conditions and the lack of useful files for self-training.\nMismatch conditions are unavoidable if web audio is intended to be exploited through semi-supervised approaches. There is no control over the recording methods for unlabeled web audio and thus it will most likely be different than the control methods from labeled datasets. In our case, the dataset US8k has different collection methods and acoustic characteristics, which do not match the user-generated YouTube audio. In our experiments, the detectors were self-trained using only the newly labeled \u201cpositive\u201d segments from YouTube. After each iteration, more and more YouTube data was added to the detectors on the positive category but not on the negative. However, the improvement was limited and often degraded. After inspecting some of the rejected files, it seemed that the detectors were discriminating YouTube vs non-YouTube audio, rather than positive vs negative. On the contrary, when \u201cpositive and negative\u201d segments were added, the performance improved.\nA manual inspection on some of the candidates helped us better understand the audio content used to re-train the detectors. Thumbnails examples are in Figure 3, illustrating interesting cases. For instance, some videos may have the presence of the sound even though the image didn\u2019t corresponded. The first thumbnail-video had the siren sound, but the image in the video was just a radio-like box. Another example was when sounds were acoustically similar but semantically different. The third thumbnail-video showed a scene from the\nmovie \u201cCaptain America\u201d, where the audio was similar to \u201cair conditioner\u201d, but there was no such item. These examples does not necessarily degrade the quality of the detector as shown in [16]. In a similar manner, gun shot had, among some of the candidates, object banging sounds."}, {"heading": "V. LIMITATIONS AND FUTURE WORK", "text": "Classifier bias Semi-supervised approaches have limitations related to what extent they can help, as discussed in [17]. Especially, self-training has an inherent detector bias issue which happens when a detector is trained with an initial set of data. The detector then, is ran on the unlabeled data and the confidence score depends on the initial model. Once we add new segments, we are enforcing the acoustic characteristics of the previous model and not necessarily making our models more robust. Addressing the issue was out of scope, but could be a reason for the fast convergence in our results.\nThreshold type The set of thresholds utilized are a reasonable approach supported in the literature. However, a more elaborated objective function should be considered to better select candidates."}, {"heading": "VI. CONCLUSIONS", "text": "In this work we proposed a framework of semi-supervised self-training of audio event detectors, where the detectors were\ntrained with the annotated US8K dataset, and the self-training employed unlabeled audio from YouTube videos. The NN detectors yielded a higher baseline performance than SVM. Both detectors and almost all the classes benefited from selftraining. Despite the audio mismatch conditions and thepossibility of having few or no target sounds to be candidates, the performance after self-training did not degrade. Further exploration to select candidates offers a valuable opportunity. Unlabeled audio from videos can help audio event detection."}], "references": [{"title": "Multimedia information retrieval: content-based information retrieval from large text and audio databases", "author": ["P. Sch\u00e4uble"], "venue": "Springer Science & Business Media,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2012}, {"title": "Content-based multimedia information retrieval: State of the art and challenges", "author": ["M.S. Lew", "N. Sebe", "C. Djeraba", "R. Jain"], "venue": "ACM Transactions on Multimedia Computing, Communications, and Applications (TOMM), vol. 2, no. 1, pp. 1\u201319, 2006.", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2006}, {"title": "Sound representation and classification benchmark for domestic robots", "author": ["J. Maxime", "X. Alameda-Pineda", "L. Girin", "R. Horaud"], "venue": "2014 IEEE International Conference on Robotics and Automation (ICRA). IEEE, 2014, pp. 6285\u20136292.", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2014}, {"title": "Soundevent recognition with a companion humanoid", "author": ["M. Janvier", "X. Alameda-Pineda", "L. Girinz", "R. Horaud"], "venue": "2012 12th IEEE- RAS International Conference on Humanoid Robots (Humanoids 2012). IEEE, 2012, pp. 104\u2013111.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2012}, {"title": "A dataset and taxonomy for urban sound research", "author": ["J. Salamon", "C. Jacoby", "J.P. Bello"], "venue": "22st ACM International Conference on Multimedia (ACM-MM\u201914), Orlando, FL, USA, Nov. 2014.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2014}, {"title": "Detection and classification of acoustic scenes and events", "author": ["D. Stowell", "D. Giannoulis", "E. Benetos", "M. Lagrange", "M.D. Plumbley"], "venue": "IEEE Transactions on Multimedia, vol. 17, no. 10, pp. 1733\u20131746, 2015.", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2015}, {"title": "Audio concept classification with hierarchical deep neural networks", "author": ["M. Ravanelli", "B. Elizalde", "K. Ni", "G. Friedland"], "venue": "Proceedings of EUSIPCO, 2014.", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2014}, {"title": "Environmental sound classification with convolutional neural networks", "author": ["K.J. Piczak"], "venue": "2015 IEEE 25th International Workshop on Machine Learning for Signal Processing (MLSP). IEEE, 2015, pp. 1\u20136.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2015}, {"title": "Proceedings of the Detection and Classification of Acoustic Scenes and Events 2016 Workshop (DCASE2016)", "author": ["T. Virtanen", "A. Mesaros", "T. Heittola", "M. Plumbley", "P. Foster", "E. Benetos", "M. Lagrange"], "venue": "Tampere University of Technology. Department of Signal Processing,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2016}, {"title": "Semi-supervised active learning for sound classification in hybrid learning environments", "author": ["W. Han", "E. Coutinho", "H. Ruan", "H. Li", "B. Schuller", "X. Yu", "X. Zhu"], "venue": "PloS one, vol. 11, no. 9, p. e0162075, 2016.", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2016}, {"title": "Semi-supervised learning helps in sound event classification", "author": ["Z. Zhang", "B. Schuller"], "venue": "Acoustics, Speech and Signal Processing (ICASSP), 2012 IEEE International Conference on. IEEE, 2012, pp. 333\u2013336.", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2012}, {"title": "Improved audio features for largescale multimedia event detection", "author": ["F. Metze", "S. Rawat", "Y. Wang"], "venue": "Multimedia and Expo (ICME), 2014 IEEE International Conference on. IEEE, 2014, pp. 1\u20136.", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2014}, {"title": "The perceived position of moving objects: Transcranial magnetic stimulation of area MT+ reduces the flash-lag effect", "author": ["F.-F. Li", "P. Perona"], "venue": "IEEE CVPR, vol. 2, 2005.", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2005}, {"title": "Experiments on the dcase challenge 2016: Acoustic scene classification and sound event detection in real life recording", "author": ["B. Elizalde", "A. Kumar", "A. Shah", "R. Badlani", "E. Vincent", "B. Raj", "I. Lane"], "venue": "DCASE2016 Workshop on Detection and Classification of Acoustic Scenes and Events, 2016.", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2016}, {"title": "Active learning for interactive multimedia retrieval", "author": ["T.S. Huang", "C.K. Dagli", "S. Rajaram", "E.Y. Chang", "M.I. Mandel", "G.E. Poliner", "D.P. Ellis"], "venue": "Proceedings of the IEEE, vol. 96, no. 4, pp. 648\u2013667, 2008.", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2008}, {"title": "Audio concept ranking for video event detection on user-generated content", "author": ["B. Elizalde", "M. Ravanelli", "G. Friedland"], "venue": "Multimedia (SLAM 2013).", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2013}, {"title": "Unlabeled data: Now it helps, now it doesn\u2019t", "author": ["A. Singh", "R. Nowak", "X. Zhu"], "venue": "Advances in neural information processing systems, 2009, pp. 1513\u20131520.", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2009}], "referenceMentions": [{"referenceID": 0, "context": "The dominant application is multimedia video content analysis, where audio is combined with images and text [1], [2] to index, search and retrieve videos.", "startOffset": 108, "endOffset": 111}, {"referenceID": 1, "context": "The dominant application is multimedia video content analysis, where audio is combined with images and text [1], [2] to index, search and retrieve videos.", "startOffset": 113, "endOffset": 116}, {"referenceID": 2, "context": "Another task is human-robot interaction [3], [4], where sounds complement speech as non-verbal communication.", "startOffset": 40, "endOffset": 43}, {"referenceID": 3, "context": "Another task is human-robot interaction [3], [4], where sounds complement speech as non-verbal communication.", "startOffset": 45, "endOffset": 48}, {"referenceID": 4, "context": "Recently, a growing application is in smart cities [5], where sounds are used to detect sources of noise pollution.", "startOffset": 51, "endOffset": 54}, {"referenceID": 5, "context": "The related work on AED has mainly focused on using available datasets to train machine learning models in a supervised manner [6], [5], [7], [8], [9].", "startOffset": 127, "endOffset": 130}, {"referenceID": 4, "context": "The related work on AED has mainly focused on using available datasets to train machine learning models in a supervised manner [6], [5], [7], [8], [9].", "startOffset": 132, "endOffset": 135}, {"referenceID": 6, "context": "The related work on AED has mainly focused on using available datasets to train machine learning models in a supervised manner [6], [5], [7], [8], [9].", "startOffset": 137, "endOffset": 140}, {"referenceID": 7, "context": "The related work on AED has mainly focused on using available datasets to train machine learning models in a supervised manner [6], [5], [7], [8], [9].", "startOffset": 142, "endOffset": 145}, {"referenceID": 8, "context": "The related work on AED has mainly focused on using available datasets to train machine learning models in a supervised manner [6], [5], [7], [8], [9].", "startOffset": 147, "endOffset": 150}, {"referenceID": 7, "context": "data set, ESC-50 [8], contains only 40 samples per class.", "startOffset": 17, "endOffset": 20}, {"referenceID": 9, "context": "This approach has been explored for audio events in two papers [10], [11].", "startOffset": 63, "endOffset": 67}, {"referenceID": 10, "context": "This approach has been explored for audio events in two papers [10], [11].", "startOffset": 69, "endOffset": 73}, {"referenceID": 9, "context": "Particularly in [10], the authors collected 17,000 labeled audio-only recordings from FindSounds.", "startOffset": 16, "endOffset": 20}, {"referenceID": 4, "context": "The UrbanSound8K (US8K) dataset [5] has 10 classes: air", "startOffset": 32, "endOffset": 35}, {"referenceID": 11, "context": "The Mel Frequency Cepstral Coefficients (MFCCs) have been widely used in audio event detection [12], [5], [8].", "startOffset": 95, "endOffset": 99}, {"referenceID": 4, "context": "The Mel Frequency Cepstral Coefficients (MFCCs) have been widely used in audio event detection [12], [5], [8].", "startOffset": 101, "endOffset": 104}, {"referenceID": 7, "context": "The Mel Frequency Cepstral Coefficients (MFCCs) have been widely used in audio event detection [12], [5], [8].", "startOffset": 106, "endOffset": 109}, {"referenceID": 12, "context": "The method we followed to compute BoAWs features is broadly illustrated in Figure 2 and detailed in these papers [13], [14].", "startOffset": 113, "endOffset": 117}, {"referenceID": 13, "context": "The method we followed to compute BoAWs features is broadly illustrated in Figure 2 and detailed in these papers [13], [14].", "startOffset": 119, "endOffset": 123}, {"referenceID": 5, "context": "2) Training Detectors: SVM: One round of experiments was performed with Support Vector Machines (SVMs) because SVMs have been widely explored for sound events [6].", "startOffset": 159, "endOffset": 162}, {"referenceID": 9, "context": "Score-based Under this approach, the output of the detector is a probability score that can be interpreted as a confidence value and has been used for self-training in the paper [10].", "startOffset": 178, "endOffset": 182}, {"referenceID": 14, "context": "Clarity Index Clarity Index (CI), based on the paper [15], aims to determine those segments that are the most confusing for the detector.", "startOffset": 53, "endOffset": 57}, {"referenceID": 15, "context": "These examples does not necessarily degrade the quality of the detector as shown in [16].", "startOffset": 84, "endOffset": 88}, {"referenceID": 16, "context": "Classifier bias Semi-supervised approaches have limitations related to what extent they can help, as discussed in [17].", "startOffset": 114, "endOffset": 118}], "year": 2017, "abstractText": "Audio Event Detection (AED) aims to recognize sounds within audio and video recordings. AED employs machine learning algorithms commonly trained and tested on annotated datasets. However, available datasets are limited in number of samples and hence it is difficult to model acoustic diversity. Therefore, we propose combining labeled audio from a dataset and unlabeled audio from the web to improve the sound models. The audio event detectors are trained on the labeled audio and ran on the unlabeled audio downloaded from YouTube. Whenever the detectors recognized any of the known sounds with high confidence, the unlabeled audio was use to re-train the detectors. The performance of the re-trained detectors is compared to the one from the original detectors using the annotated test set. Results showed an improvement of the AED, and uncovered challenges of using web audio from videos.", "creator": "LaTeX with hyperref package"}}}