{"id": "1609.06049", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "20-Sep-2016", "title": "Automatic Quality Assessment for Speech Translation Using Joint ASR and MT Features", "abstract": "This paper addresses automatic quality assessment of spoken language translation (SLT). This relatively new task is defined and formalized as a sequence labeling problem where each word in the SLT hypothesis is tagged as good or bad according to a large feature set. We propose several word confidence estimators (WCE) based on our automatic evaluation of transcription (ASR) quality, translation (MT) quality, or both (combined ASR+MT). This research work is possible because we built a specific corpus which contains 6.7k utterances for which a quintuplet containing: ASR output, verbatim transcript, text translation, speech translation and post-edition of translation is built. The conclusion of our multiple experiments using joint ASR and MT features for WCE is that MT features remain the most influent while ASR feature can bring interesting complementary information. Our robust quality estimators for SLT can be used for re-scoring speech translation graphs or for providing feedback to the user in interactive speech translation or computer-assisted speech-to-text scenarios.", "histories": [["v1", "Tue, 20 Sep 2016 08:04:33 GMT  (1624kb,D)", "http://arxiv.org/abs/1609.06049v1", "submitted to MT Journal (special issue on spoken language translation)"]], "COMMENTS": "submitted to MT Journal (special issue on spoken language translation)", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["ngoc-tien le", "benjamin lecouteux", "laurent besacier"], "accepted": false, "id": "1609.06049"}, "pdf": {"name": "1609.06049.pdf", "metadata": {"source": "CRF", "title": "Automatic Quality Assessment for Speech Translation Using Joint ASR and MT Features", "authors": ["Benjamin Lecouteux", "Laurent Besacier"], "emails": ["firstname.lastname@imag.fr"], "sections": [{"heading": null, "text": "Keywords Quality estimation \u00b7Word confidence estimation (WCE) \u00b7 Spoken Language Translation (SLT) \u00b7 Joint Features \u00b7 Feature Selection"}, {"heading": "1 Introduction", "text": "Automatic quality assessment of spoken language translation (SLT), also named confidence estimation (CE), is an important topic because it allows to know if a system produces (or not) user-acceptable outputs. In interactive speech to speech translation, CE helps to judge if a translated turn is uncertain\nFirstname Lastname Laboratoire d\u2019Informatique de Grenoble, University of Grenoble Alpes, France Building IMAG, 700 Centrale, 38401 Saint Martin d\u2019He\u0300res Tel. : +33 457421454 E-mail: firstname.lastname@imag.fr\nar X\niv :1\n60 9.\n06 04\n9v 1\n[ cs\n.C L\n] 2\n0 Se\np 20\n16\n(and ask the speaker to rephrase or repeat). For speech-to-text applications, CE may tell us if output translations are worth being corrected or if they require retranslation from scratch. Moreover, an accurate CE can also help to improve SLT itself through a second-pass N-best list re-ranking or search graph re-decoding, as it has already been done for text translation in [2] and [19], or for speech translation in [4]. Consequently, building a method which is capable of pointing out the correct parts as well as detecting the errors in a speech translated output is crucial to tackle above issues.\nGiven signal xf in the source language, spoken language translation (SLT) consists in finding the most probable target language sequence e\u0302 = (e1, e2, ..., eN ) so that\ne\u0302 = argmax e {p(e/xf , f)} (1)\nwhere f = (f1, f2, ..., fM ) is the transcription of xf . Now, if we perform confidence estimation at the \u201cwords\u201d level, the problem is called Word-level Confidence Estimation (WCE) and we can represent this information as a sequence q (same length N of e\u0302) where q = (q1, q2, ..., qN ) and qi \u2208 {good, bad} 1.\nThen, integrating automatic quality assessment in our SLT process can be done as following:\ne\u0302 = argmax e \u2211 q p(e, q/xf , f) (2)\ne\u0302 = argmax e \u2211 q p(q/xf , f, e) \u2217 p(e/xf , f) (3) e\u0302 \u2248 argmax e {max q {p(q/xf , f, e) \u2217 p(e/xf , f)}} (4)\nIn the product of (4), the SLT component p(e/xf , f) and the WCE component p(q/xf , f, e) contribute together to find the best translation output e\u0302. In the past, WCE has been treated separately in ASR or MT contexts and we propose here a joint estimation of word confidence for a spoken language translation (SLT) task involving both ASR and MT.\nThis journal paper is an extended version of a paper published at ASRU 2015 last year [4] but we focus more on the WCE component and on the best approaches to estimate p(q/xf , f, e) accurately.\nContributions The main contributions of this journal paper are the following:\n\u2013 A corpus (distributed to the research community 2) dedicated to WCE for SLT was initially published in [3]. We present, in this paper, its extension from 2643 to 6693 speech utterances.\n1. qi could be also more than 2 labels, or even scores but this paper only deals with error detection (binary set of labels)\n2. https://github.com/besacier/WCE-SLT-LIG\n\u2013 While our previous work on quality assessment was based on two separate WCE classifiers (one for quality assessment in ASR and one for quality assessment in MT), we propose here a unique joint model based on different feature types (ASR and MT features). \u2013 This joint model allows us to operate feature selection and analyze which features (from ASR or MT) are the most efficient for quality assessment in speech translation. \u2013 We also experiment with two ASR systems that have different performance in order to analyze the behavior of our SLT quality assessment algorithms at different levels of word error rate (WER).\nOutline The outline of this paper goes simply as follows: section 2 reviews the state-of-the-art on confidence estimation for ASR and MT. Our WCE system using multiple features is then described in section 3. The experimental setup (notably our specific WCE corpus) is presented in section 4 while section 5 evaluates our joint WCE system. Feature selection for quality assessment in speech translation is analyzed in section 6 and finally, section 7 concludes this work and gives some perspectives."}, {"heading": "2 Related work on confidence estimation for ASR and MT", "text": "Several previous works tried to propose effective confidence measures in order to detect errors on ASR outputs. Confidence measures are introduced for Out-Of-Vocabulary (OOV) detection by [1]. [27] extends the previous work and introduces the use of word posterior probability (WPP) as a confidence measure for speech recognition. Posterior probability of a word is most of the time computed using the hypothesis word graph [10]. Also, more recent approaches [16] for confidence measure estimation use side-information extracted from the recognizer: normalized likelihoods (WPP), the number of competitors at the end of a word (hypothesis density), decoding process behavior, linguistic features, acoustic features (acoustic stability, duration features) and semantic features.\nIn parallel, the Workshop on Machine Translation (WMT) introduced in 2013 a WCE task for machine translation. [9] [21] employed the Conditional Random Fields (CRF) [12] model as their machine learning method to address the problem as a sequence labelling task. Meanwhile, [5] extended their initial proposition by dynamic training with adaptive weight updates in their neural network classifier. As far as prediction indicators are concerned, [5] proposed seven word feature types and found among them the \u201ccommon cover links\u201d (the links that point from the leaf node containing this word to other leaf nodes in the same subtree of the syntactic tree) the most outstanding. [9] focused only on various n-gram combinations of target words. Inheriting most of previously-recognized features, [21] integrated a number of new indicators relying on graph topology, pseudo reference, syntactic behavior (constituent label, distance to the semantic tree root) and polysemy characteristic. The estimation of the confidence score uses mainly classifiers like Conditional Random\nFields [9,18], Support Vector Machines [13] or Perceptron [5]. Some investigations were also conducted to determine which features seem to be the most relevant. [13] proposed to filter features using a forward-backward algorithm to discard linearly correlated features. Using Boosting as learning algorithm, [20] was able to take advantage of the most significant features.\nFinally, several toolkits for WCE were recently proposed: TranscRater for ASR 3, Marmot for MT 4 as well as WCE-LIG [25] 5 that will be used to extract MT features in the experiments of this journal paper.\nTo our knowledge, the first attempt to design WCE for speech translation, using both ASR and MT features, is our own work [3,4] which is further extended in this journal paper submission."}, {"heading": "3 Building an efficient quality assessment (WCE) system", "text": "The WCE component solves the equation:\nq\u0302 = argmax q {pSLT (q/xf , f, e)} (5)\nwhere q = (q1, q2, ..., qN ) is the sequence of quality labels on the target language. This is a sequence labelling task that can be solved with several machine learning techniques such as Conditional Random Fields (CRF) [12]. However, for that, we need a large amount of training data for which a quadruplet (xf , f, e, q) is available. In this work, we will use a corpus extended from [3] which contains 6.7k utterances. We will investigate if this amount of data is enough to evaluate and test a joint model pSLT (q/xf , f, e).\nAs it is much easier to obtain data containing either the triplet (xf , f, q) (automatically transcribed speech with manual references and quality labels infered from word error rate estimation) or the triplet (f, e, q) (automatically translated text with manual post-editions and quality labels infered using tools such as TERpA [26]) we can also recast the WCE problem with the following equation:\nq\u0302 = argmax q {pASR(q/xf , f)\u03b1 \u2217 pMT (q/e, f)1\u2212\u03b1} (6)\nwhere \u03b1 is a weight giving more or less importance to WCEASR (quality assesment on transcription) compared to WCEMT (quality assesment on translation). It is important to note that pASR(q/xf , f) corresponds to the quality estimation of the words in the target language based on features calculated on the source language (ASR). For that, what we do is projecting source quality scores to the target using word-alignment information between e and f sequences. This alternative approach (equation 6 ) will be also evaluated in this work.\n3. https://github.com/hlt-mt/TranscRater 4. https://github.com/qe-team/marmot 5. https://github.com/besacier/WCE-LIG\nIn both approaches \u2013 joint (pSLT (q/xf , f, e)) and combined (pASR(q/xf , f) + pMT (q/e, f)) \u2013 some features need to be extracted from ASR and MT modules. They are more precisely detailed in next subsections.\n3.1 WCE features for speech transcription (ASR)\nIn this work, we extract several types of features, which come from the ASR graph, from language model scores and from a morphosyntactic analysis. These features are listed below (more details can be found in [3]):\n\u2013 Acoustic features: word duration (F-dur). \u2013 Graph features (extracted from the ASR word confusion networks): num-\nber of alternative (F-alt) paths between two nodes; word posterior probability (F-post). \u2013 Linguistic features (based on probabilities by the language model): word itself (F-word), 3-gram probability (F-3g), log probability (F-log), back-off level of the word (F-back), as proposed in [6], \u2013 Lexical Features: Part-Of-Speech (POS) of the word (F-POS), \u2013 Context Features: Part-Of-Speech tags in the neighborhood of a given\nword (F-context). For each word in the ASR hypothesis, we estimate the 9 features (F-Word; F-3g; F-back; F-log; F-alt; F-post; F-dur; F-POS; F-context) previously described.\nIn a preliminary experiment, we will evaluate these features for quality assessment in ASR only (WCEASR task). Two different classifiers will be used: a variant of boosting classification algorithm called bonzaiboost [14] (implementing the boosting algorithm Adaboost.MH over deeper trees) and the Conditional Random Fields [12].\n3.2 WCE features for machine translation (MT)\nA number of knowledge sources are employed for extracting features, in a total of 24 major feature types, see Table 1.\nIt is important to note that we extract features regarding tokens in the machine translation (MT) hypothesis sentence. In other words, one feature is\nextracted for each token in the MT output. So, in the Table 1, target refers to the feature coming from the MT hypothesis and source refers to a feature extracted from the source word aligned to the considered target word. More details on some of these features are given in the next subsections."}, {"heading": "3.2.1 Internal Features", "text": "These features are given by the Machine Translation system, which outputs additional data like N -best list.\nWord Posterior Probability (WPP) and Nodes features are extracted from a confusion network, which comes from the output of the machine translation N -best list. WPP Exact is the WPP value for each word concerned at the exact same position in the graph. WPP Any extracts the same information at any position in the graph. WPP Min gives the smallest WPP value concerned by the transition and WPP Max its maximum."}, {"heading": "3.2.2 External Features", "text": "Below is the list of the external features used:\n\u2013 Proper Name: indicates if a word is a proper name (same binary features are extracted to know if a token is Numerical, Punctuation or Stop Word). \u2013 Unknown Stem: informs whether the stem of the considered word is known or not. \u2013 Number of Word/Stem Occurrences: counts the occurrences of a word/stem in the sentence. \u2013 Alignment context features: these features (#11-13 in Table 1) are based on collocations and proposed by [2]. Collocations could be an indicator for judging if a target word is generated by a particular source word. We also apply the reverse, the collocations regarding the source side (#7 in Table 1 - simply called Alignment Features): \u2022 Source alignment context features: the combinations of the target word,\nthe source word (with which it is aligned), and one source word before and one source word after (left and right contexts, respectively). \u2022 Target alignment context features: the combinations of the source word,\nthe target word (with which it is aligned), and one target word before and one target word after. \u2013 Longest Target (or Source) N-gram Length: we seek to get the length (n+1) of the longest left sequence (wi\u2212n) concerned by the current word (wi) and known by the language model (LM) concerned (source and target sides). For example, if the longest left sequence wi\u22122, wi\u22121, wi appears in the target LM, the longest target n-gram value for wi will be 3. This value ranges from 0 to the max order of the LM concerned. We also extract a redundant feature called Backoff Behavior Target.\n\u2013 The target word\u2019s constituent label (Constituent Label) and its depth in the constituent tree (Distance to Root) are extracted using a syntactic parser. \u2013 Target Polysemy Count: we extract the polysemy count, which is the number of meanings of a word in a given language. \u2013 Occurences in Google Translate and Occurences in Bing Translator: in the translation hypothesis, we (optionally) test the presence of the target word in on-line translations given respectively by Google Translate and Bing Translator 6.\nA very similar feature set was used for a simple WCEMT task (English - Spanish MT, WMT 2013, 2014 quality estimation shared task) and obtained very good performances [17]. This preliminary experience in participating to the WCE shared task in 2013 and 2014 lead us to the following observation: while feature processing is very important to achieve good performance, it requires to call a set of heterogeneous NLP tools (for lexical, syntactic, semantic analyses). Thus, we recently proposed to unify the feature processing, together with the call of machine learning algorithms, in order to facilitate the design of confidence estimation systems. The open-source toolkit proposed (written in Python and made available on github 7) integrates some standard as well as in-house features that have proven useful for WCE (based on our experience in WMT 2013 and 2014).\nIn this paper, we will use only Conditional Random Fields [12] (CRFs) as our machine learning method, with WAPITI toolkit [15], to train our WCE estimator based on MT features.\n4 Experimental setup\n4.1 Dataset"}, {"heading": "4.1.1 Starting point: an existing MT Post-edition corpus", "text": "For a French-English translation task, we used our SMT system to obtain the translation hypothesis for 10,881 source sentences taken from news corpora of the WMT (Workshop on Machine Translation) evaluation campaign (from 2006 to 2010). Post-editions were obtained from non professional translators using a crowdsourcing platform. More details on the baseline SMT system used can be found in [22] and more details on the post-edited corpus can be found in [23]. It is worth mentionning, however, that a sub-set (311 sentences) of these collected post-editions was assessed by a professional translator and 87.1% of post-editions were judged to improve the hypothesis\n6. Using this kind of feature is controversial, however we observed that such features are available in general use case scenarios, so we decided to include them in our experiments. Contrastive results without these 2 features will be also given later on.\n7. http://github.com/besacier/WCE-LIG\nThen, the word label setting for WCE was done using TERp-A toolkit [26]. Table 2 illustrates the labels generated by TERp-A for one hypothesis and post-edition pair. Each word or phrase in the hypothesis is aligned to a word or phrase in the post-edition with different types of edit: \u201cI\u201d (insertions), \u201cS\u201d (substitutions), \u201cT\u201d (stem matches), \u201cY\u201d (synonym matches), and \u201cP\u201d (phrasal substitutions). The lack of a symbol indicates an exact match and will be replaced by \u201cE\u201d thereafter. We do not consider the words marked with \u201cD\u201d (deletions) since they appear only in the reference. However, later on, we will have to train binary classifiers (good/bad) so we re-categorize the obtained 6-label set into binary set: The E, T and Y belong to the good (G), whereas the S, P and I belong to the bad (B) category."}, {"heading": "4.1.2 Extending the corpus with speech recordings and transcripts", "text": "The dev set and tst set of this corpus were recorded by french native speakers. Each sentence was uttered by 3 speakers, leading to 2643 and 4050 speech recordings for dev set and tst set, respectively. For each speech utterance, a quintuplet containing: ASR output (fhyp), verbatim transcript (fref ), English text translation output (ehypmt), speech translation output (ehypslt) and postedition of translation (eref ), was made available. This corpus is available on a github repository 8. More details are given in table 3. The total length of the dev and tst speech corpus obtained are 16h52, since some utterances were pretty long.\n4.2 ASR Systems\nTo obtain the speech transcripts (fhyp), we built a French ASR system based on KALDI toolkit [24]. Acoustic models are trained using several corpora (ESTER, REPERE, ETAPE and BREF120) representing more than 600 hours of french transcribed speech.\nThe baseline GMM system is based on mel-frequency cepstral coefficient (MFCC) acoustic features (13 coefficients expanded with delta and double delta features and energy : 40 features) with various feature transformations including linear discriminant analysis (LDA), maximum likelihood linear transformation (MLLT), and feature space maximum likelihood linear regression (fMLLR) with speaker adaptive training (SAT). The GMM acoustic model makes initial phoneme alignments of the training data set for the following DNN acoustic model training.\nThe speech transcription process is carried out in two passes: an automatic transcript is generated with a GMM-HMM model of 43182 states and 250000 Gaussians. Then word graphs outputs obtained during the first pass are used to compute a fMLLR-SAT transform on each speaker. The second pass is performed using DNN acoustic model trained on acoustic features normalized with the fMLLR matrix.\nCD-DNN-HMM acoustic models are trained (43 182 context-dependent states) using GMM-HMM topology.\nWe propose to use two 3-gram language models trained on French ESTER corpus [8] as well as on French Gigaword (vocabulary size are respectively 62k and 95k). The ASR systems LM weight parameters are tuned through WER on the dev corpus. Details on these two language models can be found in table 4.\nIn our experiments we propose two ASR systems based on the previously described language models. The first system (ASR1) uses the small language model allowing a fast ASR system (about 2x Real Time), while in the second system lattices are rescored with a big language model (about 10x Real Time) during a third pass.\nTable 5 presents the performances obtained by two above ASR systems.\nThese WER may appear as rather high according to the task (transcribing read news). A deeper analysis shows that these news contain a lot of foreign named entities, especially in our dev set. This part of the data is extracted from French medias dealing with european economy in EU. This could also explain why the scores are significantly different between dev and test sets. In addition, automatic post-processing is applied to ASR output in order to match requirements of standard input for machine translation.\n4.3 SMT System\nWe used moses phrase-based translation toolkit [11] to translate French ASR into English (ehyp). This medium-size system was trained using a subset of data provided for IWSLT 2012 evaluation [7]: Europarl, Ted and NewsCommentary corpora. The total amount is about 60M words. We used an adapted target language model trained on specific data (News Crawled corpora) similar to our evaluation corpus (see [22]). This standard SMT system will be used in all experiments reported in this paper.\n4.4 Obtaining quality assessment labels for SLT\nAfter building an ASR system, we have a new element of our desired quintuplet: the ASR output fhyp. It is the noisy version of our already available verbatim transcripts called fref . This ASR output (fhyp) is then translated by the exact same SMT system [22] already mentionned in subsection 4.3. This new output translation is called ehypslt and it is a degraded version of ehypmt (translation of fref ).\nAt this point, a strong assumption we made has to be revealed: we re-used the post-editions obtained from the text translation task (called eref ), to infer the quality (G, B) labels of our speech translation output ehypslt . The word label setting for WCE is also done using TERp-A toolkit [26] between ehypslt and eref . This assumption, and the fact that initial MT post-edition can be also used to infer labels of a SLT task, is reasonnable regarding results (later presented in table 8 and table 9) where it is shown that there is not a huge difference between the MT and SLT performance (evaluated with BLEU).\nThe remark above is important and this is what makes the value of this corpus. For instance, other corpora such as the TED corpus compiled by LIUM 9 contain also a quintuplet with ASR output, verbatim transcript, MT output, SLT output and target translation. But there are 2 main differences: first, the target translation is a manual translation of the prior subtitles so this is not a post-edition of an automatic translation (and we have no guarantee that the good/bad labels extracted from this will be reliable for WCE training and\n9. http://www-lium.univ-lemans.fr/fr/content/corpus-ted-lium\ntesting); secondly, in our corpus, each sentence is uttered by 3 different speakers which introduces speaker variability in the database and allows us to deal with different ASR outputs for a single source sentence.\n4.5 Final corpus statistics\nThe final corpus obtained is summarized in table 6, where we also clarify how the WCE labels were obtained. For the test set, we now have all the data needed to evaluate WCE for 3 tasks:\n\u2013 ASR: extract good/bad labels by calculating WER between fhyp and fref , \u2013 MT: extract good/bad labels by calculating TERp-A between ehypmt and eref , \u2013 SLT: extract good/bad labels by calculating TERp-A between ehypslt and eref .\nTable 7 gives an example of the quintuplet available in our corpus. One transcript (fhyp1) has 1 error while the other one (fhyp2) has 4. This leads to respectively 2 B labels (ehypslt1) and 4 B labels (ehypslt2) in the speech translation output, while ehypmt has only one B label.\nTable 8 and table 9 summarize baseline ASR, MT and SLT performances obtained on our corpora, as well as the distribution of good (G) and bad (B) labels inferred for both tasks. Logically, the percentage of (B) labels increases from MT to SLT task in the same conditions."}, {"heading": "5 Experiments on WCE for SLT", "text": "5.1 SLT quality assessment using only MT or ASR features\nWe first report in Table 10 the baseline WCE results obtained using MT or ASR features separately. In short, we evaluate the performance of 4 WCE systems for different tasks:\n\u2013 The first and second systems (WCE for ASR / ASR feat.) use ASR features described in section 3.1 with two different classifiers (CRF or Boosting). \u2013 The third system (WCE for SLT / MT feat.) uses only MT features described in section 3.2 with CRF classifier. \u2013 The fourth system (WCE for SLT / ASR feat.) uses only ASR features described in section 3.1 with CRF classifier (so this is predicting SLT output confidence using only ASR confidence features!). Word alignment information between fhyp and ehyp is used to project the WCE scores coming from ASR, to the SLT output,\nIn all experiments reported in this paper, we evaluate the performance of our classifiers by using the average between the F-measure for good labels and the F-measure for bad labels that are calculated by the common evaluation metrics: Precision, Recall and F-measure for good/bad labels. Since two ASR\nsystems are available, F-mes1 is obtained for SLT based on ASR1 whereas F-mes2 is obtained for SLT based on ASR2. For the results of Table 10, the classifier is evaluated on the tst part of our corpus and trained on the dev part.\nConcerning WCE for ASR, we observe that Fmeasure decreases when ASR WER is lower (F-mes2<F-mes1 while WERASR2 < WERASR1). So quality assessment in ASR seems to become harder as the ASR system improves. This could be due to the fact that the ASR1 errors recovered by bigger LM in ASR2 system were easier to detect. Anyway, this conclusion should be considered with caution since both results (F-mes1 and F-mes2 ) are not directly comparable because they are evaluated on different references (proportion of good/bad labels differ as ASR system differ). The effect of the classifier (CRF or Boosting) is not conclusive since CRF is better for F-mes1 and worse for F-mes2. Anyway, we decide to use CRF for all our future experiments since this is the classifier integrated in WCE-LIG [25] toolkit.\nConcerning WCE for SLT, we observe that Fmeasure is better using MT features rather than ASR features (quality assessment for SLT more dependent of MT features than ASR features). Again, Fmeasure decreases when ASR WER is lower (F-mes2<F-mes1 while WERASR2 < WERASR1). For MT features, removing OccurInGoogleTranslate and OccurInBingTranslate features lead to 59.40% and 58.11% for F-mes1 and F-mes2 respectively.\nIn the next subsection, we try to see if the use of both MT and ASR features improves quality assessment for SLT.\n5.2 SLT quality assessment using both MT and ASR features\nWe now report in Table 12 WCE for SLT results obtained using both MT and ASR features. More precisely we evaluate two different approaches (combination and joint):\n\u2013 The first system (WCE for SLT / MT+ASR feat.) combines the output of two separate classifiers based on ASR and MT features. In this approach, ASR-based confidence score of the source is projected to the target SLT output and combined with the MT-based confidence score as shown in equation 6 (we did not tune the \u03b1 coefficient and set it a priori to 0.5).\ntask WCE for SLT WCE for SLT WCE for SLT WCE for SLT feat. type MT+ASR feat. Joint feat. 1 Joint feat. 2 Joint feat. 3\npASR(q/xf , f) \u03b1 p(q/xf , f, e) p(q/xf , f, e) p(q/xf , f, e) \u2217pMT (q/e, f)1\u2212\u03b1\nF-mes1 52.99% 60.29%* 60.17% 60.23% F-mes2 48.46% 59.23%* 59.20% 58.99%\nThe results of Table 12 show that joint ASR and MT features do not improve WCE performance: F-mes1 and F-mes2 are slightly worse than those of table 9 (WCE for SLT / MT features only). We also observe that simple combination (MT+ASR) degrades the WCE performance. This latter observation may be due to different behaviors of WCEMT and WCEASR classifiers which makes the weighted combination ineffective. Moreover, the disappoint-\ning performance of our joint classifier may be due to an insufficient training set (only 2683 utterances in dev !). Finally, removing OccurInGoogleTranslate and OccurInBingTranslate features for Joint lowered F-mes between 1% and 1.5%.\nThese observations lead us to investigate the behaviour of our WCE approaches for a large range of good/bad decision threshold and with a new protocol where we reverse dev and tst. So, in the next experiments of this subsection, we will report WCE evaluation results obtained on dev (2683 utt.) with classifiers trained on tst (4050 utt.). Finally, the different strategies used to project ASR features when a target word is aligned to more than one source word do not lead to very different performance: we will use strategy joint 1 in the future.\nWhile the previous tables provided WCE performance for a single point of interest (good/bad decision threshold set to 0.5), the curves of figures 1 and 2 show the full picture of our WCE systems (for SLT) using speech transcriptions systems ASR1 and ASR2, respectively. We observe that the classifier based on ASR features has a very different behaviour than the classifier based on MT features which explains why their simple combination (MT+ASR) does not\nwork very well for the default decision threshold (0.5). However, for threshold above 0.5, the use of both ASR and MT features is beneficial. This is interesting because higher thresholds improves the Fmeasure on bad labels (so improves error detection). Both curves are similar whatever the ASR system used. These results suggest that with enough development data for appropriate threshold tuning (which we do not have for this very new task), the use of both ASR and MT features should improve error detection in speech translation (blue and red curves are above the green curve for higher decision threshold 10). We also analyzed the Fmeasure curves for bad and good labels separately 11: if we consider, for instance ASR1 system, for decision threshold equals to 0.75, the Fmeasure on bad labels is equivalent (60%) for 3 systems (Joint, MT+ASR and MT ) while the Fmeasure on good labels is 61% when using MT features only, 66% when using Joint features and 68% when using MT+ASR features. In other words, for a fixed performance on bad labels, the Fmeasure on good labels is improved using all information available (ASR and MT features).\n10. Corresponding to optimization of the Fmeasure on bad labels (errors) 11. Not reported here due to space constraints.\nFinally, if we focus on Joint versus MT+ASR, we notice that the range of the threshold where performance are stable is larger for Joint than for MT+ASR."}, {"heading": "6 Feature Selection", "text": "In this section, we try to better understand the contribution of each (ASR or MT) feature by applying feature selection on our joint WCE classifier. In these experiments, we decide to keep OccurInGoogleTranslate and OccurInBingTranslate features.\nWe choose the Sequential Backward Selection (SBS) algorithm which is a top-down algorithm starting from a feature set noted Yk (which denotes the set of all features) and sequentially removing the most irrelevant one (x) that maximizes the Mean F-Measure, MF (Yk \u2212 x). In our work, we examine until the set Yk contains only one remaining feature. Algorithm 1 summarizes the whole process.\nAlgorithm 1 Sequential Backward Selection (SBS) algorithm for feature selection. Yk denotes the set of all features and x is the feature removed at each step of the algorithm\nwhile size of Yk > 0 do maxval = 0 for x \u2208 Yk do\nif maxval < MF (Yk \u2212 x) then maxval\u2190MF (Yk \u2212 x) worstfeat\u2190 x\nend if end for remove worstfeat from Yk\nend while\nThe results of the SBS algorithm can be found in table 13 which ranks all joint features used in WCE for SLT by order of importance after applying the algorithm on dev. We can see that the SBS algorithm is not very stable and is clearly influenced by the ASR system (ASR1 or ASR2) considered in SLT. Anyway, if we focus on the features that are in the top-10 best in both cases, we find that the most relevant ones are:\n\u2013 Occur in Google Translate and Occur in Bing Translate (diagnostic from other MT systems), \u2013 Longest Source N-gram Length, Target Backoff Behaviour (source or target N-gram features) \u2013 Stem Context Alignment (source-target alignment feature) We also observe that the most relevant ASR features (in bold in table 13) are F-3g, F-POS and F-back (lexical and linguistic features) whereas ASR acoustic and graph based features are among the worst (F-post, F-alt, F-dur). So, in our experimental setting, it seems that MT features are more influent than ASR features. Another surprising result is the relatively low rank of word\nposterior probability (WPP) features whereas we were expecting to see them among the top features (as shown in [20] where WPP Any is among the best features for WCE in MT).\nFigure 3 and Figure 4 present the evolution of WCE performance for dev and tst corpora when feature selection using SBS algorithm is made on dev, for ASR1 and ASR2 systems, respectively. In other words, for these two figures, we apply our SBS algorithm on dev which means that feature selection is done on dev with classifiers trained on tst. After that, the best feature subsets (using 33, 32, 31 until 1 feature only) are applied on tst corpus (with classifiers trained on dev) 12.\nOn both figures, we observe that half of the features only contribute to the WCE process since best performances are observed with 10 to 15 features only. We also notice that optimal WCE performance is not necessarily obtained with the full feature set but it can be obtained with a subset of it.\n7 Conclusion\n7.1 Main contributions\nIn this paper, we introduced a new quality assessment task: word confidence estimation (WCE) for spoken language translation (SLT). A specific\n12. 3 data sets would have been needed to (a) train classifiers, (b) apply feature selection, (c) evaluate WCE performance. Since we only have a dev and a tst set, we found this procedure acceptable\ncorpus, distributed to the research community 13 was built for this purpose. We formalized WCE for SLT and proposed several approaches based on several types of features: machine translation (MT) based features, automatic speech recognition (ASR) based features, as well as combined or joint features using ASR and MT information. The proposition of a unique joint classifier based on different feature types (ASR and MT features) allowed us to operate feature selection and analyze which features (from ASR or MT) are the most efficient for quality assessment in speech translation. Our conclusion is that MT features remain the most influential while ASR feature can bring interesting complementary information. In all our experiments, we systematically evaluated with two ASR systems that have different performance in order to analyze the behavior of our quality assessment algorithms at different levels of word error rate (WER). This allowed us to observe that WCE performance decreases as ASR system improves. For reproducible research, most features 14 and algorithms used in this paper are available through our toolkit called WCE-LIG. This package is made available on a GitHub repository 15 under the licence GPL V3. We hope that the availability of our corpus and toolkit could lead, in a near future, to a new shared task dedicated to quality estima-\n13. https://github.com/besacier/WCE-SLT-LIG 14. MT features already available, ASR features available soon 15. https://github.com/besacier/WCE-LIG\ntion for speech translation. Such a shared task could be proposed in avenues such as IWSLT (International Workshop on Spoken Language Translation) or WMT (Workshop on Machine Translation) for instance.\n7.2 SLT redecoding using WCE\nA direct application of this work is the use of WCE labels to re-decode speech translation graphs and (hopefully) improve speech translation performance. Preliminary results were already obtained and recently published by the authors of this paper [4]. The main idea is to carry a second speech translation pass by considering every word and its quality assessment label, as shown in equation 4. The speech translation graph is redecoded following the following principle: words labeled as good in the search graph should be \u201crewarded\u201d by reducing their cost; on the contrary, those labeled as bad should be \u201cpenalized\u201d. To illustrate this direct application of our work, we present examples of speech translation hypotheses (SLT) obtained with or without graph re-decoding in table 14 (table taken from [4]).\nExample 1 illustrates a first case where re-decoding allows slightly improving the translation hypothesis. Analysis of the labels from the confidence estimator indicates that the words a (start of sentence) and penalty were labeled as bad here. Thus, a better hypothesis arised from the second pass, although\nthe transcription error could not be recovered. In example 2, the confidence estimator labeled as bad the following word sequences: it has, speech that was and post route. Better translation hypothesis is found after re-decoding (correct pronoun, better quality at the end of sentence). Finally, example 3 shows a case where, this time, the end of the first pass translation deteriorated after re-decoding. Analysis of confidence estimator output shows that the phrase to open was (correctly) labeled as bad, but the re-decoding gave rise to an even worse hypothesis. The reason is that the system could not recover the named entity opel since this word was not in the speech translation graph.\n7.3 Other perspectives\nIn addition to re-decode SLT graphs, our quality assessment system can be used in interactive speech translation scenarios such as news or lectures subtitling, to improve human translator productivity by giving him/her feedback on automatic transcription and translation quality. Another application would be the adaptation of our WCE system to interactive speech-to-speech translation scenarios where feedback on transcription and translation modules\nis needed to improve communication. On these latter subjects, it would also be nice to move from a binary (good or bad labels) to a 3-class decision problem (good, asr-error, mt-error). The outcome material of this paper (corpus, toolkit) can be definitely used to address such a new problem."}], "references": [{"title": "Automatic detection of new words in a large vocabulary continuous speech recognition system", "author": ["A. Asadi", "R. Schwartz", "J. Makhoul"], "venue": "Proc. of International Conference on Acoustics, Speech and Signal Processing", "citeRegEx": "1", "shortCiteRegEx": null, "year": 1990}, {"title": "Goodness: A method for measuring machine translation confidence", "author": ["N. Bach", "F. Huang", "Y. Al-Onaizan"], "venue": "Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics, pp. 211\u2013219. Portland, Oregon", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2011}, {"title": "Word confidence estimation for speech translation", "author": ["L. Besacier", "B. Lecouteux", "N.Q. Luong", "K. Hour", "M. Hadjsalah"], "venue": "Proceedings of The International Workshop on Spoken Language Translation (IWSLT). Lake Tahoe, USA", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2014}, {"title": "Spoken language translation graphs re-decoding using automatic quality assessment", "author": ["L. Besacier", "B. Lecouteux", "N.Q. Luong", "N.T. Le"], "venue": "IEEE Workshop on Automatic Speech Recognition and Understanding (ASRU). Scotsdale, Arizona, United States", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2015}, {"title": "Referential translation machines for quality estimation", "author": ["E. Bicici"], "venue": "Proceedings of the Eighth Workshop on Statistical Machine Translation, pp. 343\u2013351. Association for Computational Linguistics, Sofia, Bulgaria", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2013}, {"title": "Crf-based combination of contextual features to improve a posteriori word-level confidence measures", "author": ["J. Fayolle", "F. Moreau", "C. Raymond", "G. Gravier", "P. Gros"], "venue": "Interspeech", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2010}, {"title": "Overview of the IWSLT 2012 evaluation campaign", "author": ["M. Federico", "M. Cettolo", "L. Bentivogli", "M. Paul", "S. St\u00fcker"], "venue": "In proceedings of the 9th International Workshop on Spoken Language Translation (IWSLT)", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2012}, {"title": "Corpus description of the ester evaluation campaign for the rich transcription of french broadcast news", "author": ["S. Galliano", "E. Geoffrois", "G. Gravier", "J.F. Bonastre", "D. Mostefa", "K. Choukri"], "venue": "In Proceedings of the 5th international Conference on Language Resources and Evaluation (LREC 2006), pp. 315\u2013320", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2006}, {"title": "Quality estimation for machine translation using the joint method of evaluation criteria and statistical modeling", "author": ["A.L.F. Han", "Y. Lu", "D.F. Wong", "L.S. Chao", "L. He", "J. Xing"], "venue": "Proceedings of the Eighth Workshop on Statistical Machine Translation, pp. 365\u2013372. Association for Computational Linguistics, Sofia, Bulgaria", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2013}, {"title": "Estimating confidence using word lattices", "author": ["T. Kemp", "T. Schaaf"], "venue": "Proc. of European Conference on Speech Communication Technology pp. 827\u2013830", "citeRegEx": "10", "shortCiteRegEx": null, "year": 1997}, {"title": "Moses: Open source toolkit for statistical machine translation", "author": ["P. Koehn", "H. Hoang", "A. Birch", "C. Callison-Burch", "M. Federico", "N. Bertoldi", "B. Cowan", "W. Shen", "C. Moran", "R. Zens", "C. Dyer", "O. Bojar", "A. Constantin", "E. Herbst"], "venue": "Proceedings of the 45th Annual Meeting of the Association for Computational Linguistics, pp. 177\u2013180. Prague, Czech Republic", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2007}, {"title": "Conditional random fields: Probabilistic models for segmenting et labeling sequence data", "author": ["J. Lafferty", "A. McCallum", "F. Pereira"], "venue": "Proceedings of ICML-01, pp. 282\u2013289", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2001}, {"title": "Loria system for the wmt12 quality estimation shared task", "author": ["D. Langlois", "S. Raybaud", "K. Sm\u00e4\u0131li"], "venue": "Proceedings of the Seventh Workshop on Statistical Machine Translation, pp. 114\u2013119. Baltimore, Maryland USA", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2012}, {"title": "Boosting bonsai trees for efficient features combination : application to speaker role identification", "author": ["A. Laurent", "N. Camelin", "C. Raymond"], "venue": "Interspeech", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2014}, {"title": "Practical very large scale crfs", "author": ["T. Lavergne", "O. Capp\u00e9", "F. Yvon"], "venue": "Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pp. 504\u2013513", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2010}, {"title": "Combined low level and high level features for out-of-vocabulary word detection", "author": ["B. Lecouteux", "G. Linar\u00e8s", "B. Favre"], "venue": "INTERSPEECH", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2009}, {"title": "Word confidence estimation and its integration in sentence quality estimation for machine translation", "author": ["N.Q. Luong", "L. Besacier", "B. Lecouteux"], "venue": "Proceedings of The Fifth International Conference on Knowledge and Systems Engineering (KSE 2013). Hanoi, Vietnam", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2013}, {"title": "LIG System for Word Level QE task at WMT14", "author": ["N.Q. Luong", "L. Besacier", "B. Lecouteux"], "venue": "Proceedings of the Ninth Workshop on Statistical Machine Translation, pp. 335\u2013341. Baltimore, Maryland USA", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2014}, {"title": "Word Confidence Estimation for SMT Nbest List Re-ranking", "author": ["N.Q. Luong", "L. Besacier", "B. Lecouteux"], "venue": "Proceedings of the Workshop on Humans and Computerassisted Translation (HaCaT) during EACL. Gothenburg, Su\u00e8de", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2014}, {"title": "Towards accurate predictors of word quality for machine translation: Lessons learned on french - english and english - spanish systems", "author": ["N.Q. Luong", "L. Besacier", "B. Lecouteux"], "venue": "Data and Knowledge Engineering p. 11", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2015}, {"title": "LIG system for WMT13 QE task: Investigating the usefulness of features in word confidence estimation for MT", "author": ["N.Q. Luong", "B. Lecouteux", "L. Besacier"], "venue": "Proceedings of the Eighth Workshop on Statistical Machine Translation, pp. 396\u2013391. Association for Computational Linguistics, Sofia, Bulgaria", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2013}, {"title": "The lig machine translation system for wmt 2010", "author": ["M. Potet", "L. Besacier", "H. Blanchon"], "venue": "A. Workshop (ed.) Proceedings of the joint fifth Workshop on Statistical Machine Translation and Metrics MATR (WMT2010). Uppsala, Sweden", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2010}, {"title": "Collection of a large database of french-english smt output corrections", "author": ["M. Potet", "R. Emmanuelle E", "L. Besacier", "H. Blanchon"], "venue": "Proceedings of the eighth international conference on Language Resources and Evaluation (LREC). Istanbul, Turkey", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2012}, {"title": "The kaldi speech recognition toolkit", "author": ["D. Povey", "A. Ghoshal", "G. Boulianne", "L. Burget", "O. Glembek", "N. Goel", "M. Hannemann", "P. Motlicek", "Y. Qian", "P. Schwarz", "J. Silovsky", "G. Stemmer", "K. Vesely"], "venue": "IEEE 2011 Workshop on Automatic Speech Recognition and Understanding. IEEE Signal Processing Society", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2011}, {"title": "An Open Source Toolkit for Word-level Confidence Estimation in Machine Translation", "author": ["C. Servan", "N.T. Le", "N.Q. Luong", "B. Lecouteux", "L. Besacier"], "venue": "The 12th International Workshop on Spoken Language Translation (IWSLT\u201915). Da Nang, Vietnam", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2015}, {"title": "Terp system description", "author": ["M. Snover", "N. Madnani", "B. Dorr", "R. Schwartz"], "venue": "MetricsMATR workshop at AMTA", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2008}, {"title": "Recognition confidence measures: Detection of misrecognitions and out-ofvocabulary words", "author": ["S.R. Young"], "venue": "Proc. of International Conference on Acoustics, Speech and Signal Processing pp. 21\u201324", "citeRegEx": "27", "shortCiteRegEx": null, "year": 1994}], "referenceMentions": [{"referenceID": 1, "context": "Moreover, an accurate CE can also help to improve SLT itself through a second-pass N-best list re-ranking or search graph re-decoding, as it has already been done for text translation in [2] and [19], or for speech translation in [4].", "startOffset": 187, "endOffset": 190}, {"referenceID": 18, "context": "Moreover, an accurate CE can also help to improve SLT itself through a second-pass N-best list re-ranking or search graph re-decoding, as it has already been done for text translation in [2] and [19], or for speech translation in [4].", "startOffset": 195, "endOffset": 199}, {"referenceID": 3, "context": "Moreover, an accurate CE can also help to improve SLT itself through a second-pass N-best list re-ranking or search graph re-decoding, as it has already been done for text translation in [2] and [19], or for speech translation in [4].", "startOffset": 230, "endOffset": 233}, {"referenceID": 3, "context": "This journal paper is an extended version of a paper published at ASRU 2015 last year [4] but we focus more on the WCE component and on the best approaches to estimate p(q/xf , f, e) accurately.", "startOffset": 86, "endOffset": 89}, {"referenceID": 2, "context": "Contributions The main contributions of this journal paper are the following: \u2013 A corpus (distributed to the research community ) dedicated to WCE for SLT was initially published in [3].", "startOffset": 182, "endOffset": 185}, {"referenceID": 0, "context": "Confidence measures are introduced for Out-Of-Vocabulary (OOV) detection by [1].", "startOffset": 76, "endOffset": 79}, {"referenceID": 26, "context": "[27] extends the previous work and introduces the use of word posterior probability (WPP) as a confidence measure for speech recognition.", "startOffset": 0, "endOffset": 4}, {"referenceID": 9, "context": "Posterior probability of a word is most of the time computed using the hypothesis word graph [10].", "startOffset": 93, "endOffset": 97}, {"referenceID": 15, "context": "Also, more recent approaches [16] for confidence measure estimation use side-information extracted from the recognizer: normalized likelihoods (WPP), the number of competitors at the end of a word (hypothesis density), decoding process behavior, linguistic features, acoustic features (acoustic stability, duration features) and semantic features.", "startOffset": 29, "endOffset": 33}, {"referenceID": 8, "context": "[9] [21] employed the Conditional Random Fields (CRF) [12] model as their machine learning method to address the problem as a sequence labelling task.", "startOffset": 0, "endOffset": 3}, {"referenceID": 20, "context": "[9] [21] employed the Conditional Random Fields (CRF) [12] model as their machine learning method to address the problem as a sequence labelling task.", "startOffset": 4, "endOffset": 8}, {"referenceID": 11, "context": "[9] [21] employed the Conditional Random Fields (CRF) [12] model as their machine learning method to address the problem as a sequence labelling task.", "startOffset": 54, "endOffset": 58}, {"referenceID": 4, "context": "Meanwhile, [5] extended their initial proposition by dynamic training with adaptive weight updates in their neural network classifier.", "startOffset": 11, "endOffset": 14}, {"referenceID": 4, "context": "As far as prediction indicators are concerned, [5] proposed seven word feature types and found among them the \u201ccommon cover links\u201d (the links that point from the leaf node containing this word to other leaf nodes in the same subtree of the syntactic tree) the most outstanding.", "startOffset": 47, "endOffset": 50}, {"referenceID": 8, "context": "[9] focused only on various n-gram combinations of target words.", "startOffset": 0, "endOffset": 3}, {"referenceID": 20, "context": "Inheriting most of previously-recognized features, [21] integrated a number of new indicators relying on graph topology, pseudo reference, syntactic behavior (constituent label, distance to the semantic tree root) and polysemy characteristic.", "startOffset": 51, "endOffset": 55}, {"referenceID": 8, "context": "Fields [9,18], Support Vector Machines [13] or Perceptron [5].", "startOffset": 7, "endOffset": 13}, {"referenceID": 17, "context": "Fields [9,18], Support Vector Machines [13] or Perceptron [5].", "startOffset": 7, "endOffset": 13}, {"referenceID": 12, "context": "Fields [9,18], Support Vector Machines [13] or Perceptron [5].", "startOffset": 39, "endOffset": 43}, {"referenceID": 4, "context": "Fields [9,18], Support Vector Machines [13] or Perceptron [5].", "startOffset": 58, "endOffset": 61}, {"referenceID": 12, "context": "[13] proposed to filter features using a forward-backward algorithm to discard linearly correlated features.", "startOffset": 0, "endOffset": 4}, {"referenceID": 19, "context": "Using Boosting as learning algorithm, [20] was able to take advantage of the most significant features.", "startOffset": 38, "endOffset": 42}, {"referenceID": 24, "context": "Finally, several toolkits for WCE were recently proposed: TranscRater for ASR , Marmot for MT 4 as well as WCE-LIG [25] 5 that will be used to extract MT features in the experiments of this journal paper.", "startOffset": 115, "endOffset": 119}, {"referenceID": 2, "context": "To our knowledge, the first attempt to design WCE for speech translation, using both ASR and MT features, is our own work [3,4] which is further extended in this journal paper submission.", "startOffset": 122, "endOffset": 127}, {"referenceID": 3, "context": "To our knowledge, the first attempt to design WCE for speech translation, using both ASR and MT features, is our own work [3,4] which is further extended in this journal paper submission.", "startOffset": 122, "endOffset": 127}, {"referenceID": 11, "context": "This is a sequence labelling task that can be solved with several machine learning techniques such as Conditional Random Fields (CRF) [12].", "startOffset": 134, "endOffset": 138}, {"referenceID": 2, "context": "In this work, we will use a corpus extended from [3] which contains 6.", "startOffset": 49, "endOffset": 52}, {"referenceID": 25, "context": "As it is much easier to obtain data containing either the triplet (xf , f, q) (automatically transcribed speech with manual references and quality labels infered from word error rate estimation) or the triplet (f, e, q) (automatically translated text with manual post-editions and quality labels infered using tools such as TERpA [26]) we can also recast the WCE problem with the following equation:", "startOffset": 330, "endOffset": 334}, {"referenceID": 2, "context": "These features are listed below (more details can be found in [3]): \u2013 Acoustic features: word duration (F-dur).", "startOffset": 62, "endOffset": 65}, {"referenceID": 5, "context": "\u2013 Linguistic features (based on probabilities by the language model): word itself (F-word), 3-gram probability (F-3g), log probability (F-log), back-off level of the word (F-back), as proposed in [6], \u2013 Lexical Features: Part-Of-Speech (POS) of the word (F-POS), \u2013 Context Features: Part-Of-Speech tags in the neighborhood of a given word (F-context).", "startOffset": 196, "endOffset": 199}, {"referenceID": 13, "context": "Two different classifiers will be used: a variant of boosting classification algorithm called bonzaiboost [14] (implementing the boosting algorithm Adaboost.", "startOffset": 106, "endOffset": 110}, {"referenceID": 11, "context": "MH over deeper trees) and the Conditional Random Fields [12].", "startOffset": 56, "endOffset": 60}, {"referenceID": 1, "context": "\u2013 Alignment context features: these features (#11-13 in Table 1) are based on collocations and proposed by [2].", "startOffset": 107, "endOffset": 110}, {"referenceID": 16, "context": "A very similar feature set was used for a simple WCEMT task (English Spanish MT, WMT 2013, 2014 quality estimation shared task) and obtained very good performances [17].", "startOffset": 164, "endOffset": 168}, {"referenceID": 11, "context": "In this paper, we will use only Conditional Random Fields [12] (CRFs) as our machine learning method, with WAPITI toolkit [15], to train our WCE estimator based on MT features.", "startOffset": 58, "endOffset": 62}, {"referenceID": 14, "context": "In this paper, we will use only Conditional Random Fields [12] (CRFs) as our machine learning method, with WAPITI toolkit [15], to train our WCE estimator based on MT features.", "startOffset": 122, "endOffset": 126}, {"referenceID": 21, "context": "More details on the baseline SMT system used can be found in [22] and more details on the post-edited corpus can be found in [23].", "startOffset": 61, "endOffset": 65}, {"referenceID": 22, "context": "More details on the baseline SMT system used can be found in [22] and more details on the post-edited corpus can be found in [23].", "startOffset": 125, "endOffset": 129}, {"referenceID": 25, "context": "Then, the word label setting for WCE was done using TERp-A toolkit [26].", "startOffset": 67, "endOffset": 71}, {"referenceID": 23, "context": "To obtain the speech transcripts (fhyp), we built a French ASR system based on KALDI toolkit [24].", "startOffset": 93, "endOffset": 97}, {"referenceID": 7, "context": "We propose to use two 3-gram language models trained on French ESTER corpus [8] as well as on French Gigaword (vocabulary size are respectively 62k and 95k).", "startOffset": 76, "endOffset": 79}, {"referenceID": 10, "context": "We used moses phrase-based translation toolkit [11] to translate French ASR into English (ehyp).", "startOffset": 47, "endOffset": 51}, {"referenceID": 6, "context": "This medium-size system was trained using a subset of data provided for IWSLT 2012 evaluation [7]: Europarl, Ted and NewsCommentary corpora.", "startOffset": 94, "endOffset": 97}, {"referenceID": 21, "context": "We used an adapted target language model trained on specific data (News Crawled corpora) similar to our evaluation corpus (see [22]).", "startOffset": 127, "endOffset": 131}, {"referenceID": 21, "context": "This ASR output (fhyp) is then translated by the exact same SMT system [22] already mentionned in subsection 4.", "startOffset": 71, "endOffset": 75}, {"referenceID": 25, "context": "The word label setting for WCE is also done using TERp-A toolkit [26] between ehypslt and eref .", "startOffset": 65, "endOffset": 69}, {"referenceID": 24, "context": "Anyway, we decide to use CRF for all our future experiments since this is the classifier integrated in WCE-LIG [25] toolkit.", "startOffset": 111, "endOffset": 115}, {"referenceID": 19, "context": "posterior probability (WPP) features whereas we were expecting to see them among the top features (as shown in [20] where WPP Any is among the best features for WCE in MT).", "startOffset": 111, "endOffset": 115}, {"referenceID": 3, "context": "Preliminary results were already obtained and recently published by the authors of this paper [4].", "startOffset": 94, "endOffset": 97}, {"referenceID": 3, "context": "To illustrate this direct application of our work, we present examples of speech translation hypotheses (SLT) obtained with or without graph re-decoding in table 14 (table taken from [4]).", "startOffset": 183, "endOffset": 186}, {"referenceID": 3, "context": "Table 14 Examples of French SLT hyp with and w/o graph re-decoding - table taken from [4].", "startOffset": 86, "endOffset": 89}], "year": 2016, "abstractText": "This paper addresses automatic quality assessment of spoken language translation (SLT). This relatively new task is defined and formalized as a sequence labeling problem where each word in the SLT hypothesis is tagged as good or bad according to a large feature set. We propose several word confidence estimators (WCE) based on our automatic evaluation of transcription (ASR) quality, translation (MT) quality, or both (combined ASR+MT). This research work is possible because we built a specific corpus which contains 6.7k utterances for which a quintuplet containing: ASR output, verbatim transcript, text translation, speech translation and post-edition of translation is built. The conclusion of our multiple experiments using joint ASR and MT features for WCE is that MT features remain the most influent while ASR feature can bring interesting complementary information. Our robust quality estimators for SLT can be used for re-scoring speech translation graphs or for providing feedback to the user in interactive speech translation or computer-assisted speech-to-text scenarios.", "creator": "LaTeX with hyperref package"}}}