{"id": "1609.06354", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "20-Sep-2016", "title": "Recognizing Detailed Human Context In-the-Wild from Smartphones and Smartwatches", "abstract": "We demonstrate that a person's behavioral and environmental context can be automatically recognized by harnessing the sensors built into smartphones and smartwatches. We propose a generic system that can simultaneously recognize many contextual attributes from diverse behavioral domains. By fusing complementary information from different types of sensors our system successfully recognizes fine details of work and leisure activities, body movement, transportation, and more, and will facilitate a more personalized understanding of the health and environmental effects of wearable devices. Our system will also inform health management for older users who are living longer. It will allow the user to better assess whether they have health problems that require physical intervention and how quickly they can manage them. It will also assist users to manage their health and their health by using a predictive health function that will alert the user to a significant risk of developing symptoms.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n", "histories": [["v1", "Tue, 20 Sep 2016 20:56:07 GMT  (2312kb,D)", "http://arxiv.org/abs/1609.06354v1", "This paper was submitted to IEEE Pervasive Computing magazine on September 20th, 2016"], ["v2", "Thu, 29 Dec 2016 22:47:22 GMT  (2314kb,D)", "http://arxiv.org/abs/1609.06354v2", "This paper was originally submitted to IEEE Pervasive Computing magazine on September 20th, 2016. This version is the major revision (following first review). This revision was submitted to IEEE Pervasive Computing on December 28th, 2016"], ["v3", "Wed, 17 May 2017 21:44:18 GMT  (640kb,D)", "http://arxiv.org/abs/1609.06354v3", "This paper was originally submitted to IEEE Pervasive Computing magazine on September 20th, 2016. After a major revision and a minor revision, the paper was accepted in April 2017. It is currently under the editing process"], ["v4", "Sat, 30 Sep 2017 15:25:23 GMT  (640kb,D)", "http://arxiv.org/abs/1609.06354v4", "This paper was accepted and is to appear in IEEE Pervasive Computing, vol. 16, no. 4, October-December 2017, pp. 62-74"]], "COMMENTS": "This paper was submitted to IEEE Pervasive Computing magazine on September 20th, 2016", "reviews": [], "SUBJECTS": "cs.AI cs.CY cs.HC cs.LG", "authors": ["yonatan vaizman", "katherine ellis", "gert lanckriet"], "accepted": false, "id": "1609.06354"}, "pdf": {"name": "1609.06354.pdf", "metadata": {"source": "CRF", "title": "Recognizing Detailed Human Context In-the-Wild from Smartphones and Smartwatches", "authors": ["Yonatan Vaizman", "Katherine Ellis"], "emails": ["yvaizman@eng.ucsd.edu"], "sections": [{"heading": null, "text": "Index Terms\u2014Context awareness, Mobile sensors, Artificial intelligence, Machine learning, Human activity recognition.\nF"}, {"heading": "1 INTRODUCTION", "text": "The ability to automatically recognize a person\u2019s context (i.e., where they are, what they are doing, who they are with, how they feel, etc.) is greatly beneficial in many domains. Health monitoring and lifestyle interventions that have traditionally been based on manual, subjective reporting [1], sometimes by relying on recall with an end-of-day survey [2], can be improved with automatic (frequent, realtime, effortless and objective) detection of behaviors like exercise, eating [3], going to sleep [4], or mental states like stress [5] and hypomania [6]. Just-in-time interventions (e.g. for addiction) often prompt the patient at arbitrary times of the day, possibly missing times when the patient needs support the most [7]. Adding (automatically recognized) context information will help detect critical times and offer immediate support (e.g. an alcoholic patient may be in high risk of craving or lapse when the context is \u201cat a bar, with friends\u201d). The biomedical research community is acknowledging the effects of behavior, life style and environment on health, disease and treatments, as evident by the human Exposome project [8] and the governmental promotion of precision medicine [9]. Automatic context recognition tools will be essential to incorporate the behavioral and exposure aspects into large scale studies and to tailor appropriate treatment for patients. The range of measured exposures should be broad and cover diverse life style and environmental aspects. Commercial tools that offer superficial recognition (e.g. of walking, running and driving) will not suffice. Personal assistant systems can better serve the user by adjusting to the real-time context\n\u2022 Y. Vaizman, K. Ellis and G. Lanckriet were with the Department of Electrical and Computer Engineering, University of California, San Diego.\n\u2022 \u2217 Y. Vaizman and K. Ellis are equal contributors. \u2022 \u2020 Corresponding author. Email: yvaizman@eng.ucsd.edu\nof a user\u2019s situation (e.g., using speech while driving or recommending products while shopping). Aging care programs can use automated logging of older adults\u2019 physical, social and mental activities to detect early signs of cognitive impairment, monitor functional independence and support aging at home [10]. These applications and many more will benefit from technological advances that continuously sense and \u201cunderstand\u201d people\u2019s environments, activities and behaviors."}, {"heading": "Related work", "text": "Previous research efforts have used mobile sensors to recognize small sets of specific targeted activities, such as body movement, modes of transportation, gymnasium activities or eating [3], [11], [12], [13], [14], [15], [16], [17], [18], [19], [20], [21], [22], [23], [24], [25], [26], [27], and seldom addressed a wider range of activities [28]. In many studies the recognition setup involved cumbersome devices that had to be worn in a specific way [5], [12], [13], [14], [15], [17], [22], [23], [25], [28], [29]. Although the placement and orientation of the sensors have a great influence on the success of recognition [30], restricting the users how to wear their devices makes it infeasible for applications to have broad deployment \u2014 the variability in how people use devices is a challenge that should be overcome. The increasing popularity of smartphones and smartwatches, and the advances in their sensing and computing power, promotes using them as convenient and natural agents for context recognition systems [6], [18], [19], [20], [21], [22], [24], [26], [27], [31], [32]. Commercial entities such as Apple and Google are interested in context-awareness and have started offering functionality in a limited scope, e.g., detecting basic transportation modes (on foot, in a vehicle, on a bicycle) from smartphone measurements. ar X iv :1\n60 9.\n06 35\n4v 1\n[ cs\n.A I]\n2 0\nSe p\n20 16\n2 Holistic context recognition in-the-wild In this work our goal is to capture a holistic understanding of a person\u2019s context by recognizing combinations of over 50 context labels from various domains, describing body movement, transportation, basic needs, work and leisure activities, environments and more. These combinations describe countless detailed situations (e.g., \u201cwalking, outside, at school, with phone in pocket\u201d or \u201cstanding in place, at home, taking a shower, while the phone is on a table\u201d). We achieve this with high accuracy by learning context classifiers that combine multi-modal sensor information available in smartphones and smartwatches. We show how sensors of different modalities (motion, location, audio, etc.) are sensitive to different attributes of behavior and complement each other. In situations where one sensor has a weak signal (e.g., the phone\u2019s accelerometer when the phone is motionless), other sensors (e.g., watch accelerometer, audio) compensate and provide missing information. Our suggested system is generic (not domain specific), and can be extended to many more labels of interest.\nIn order to promote development of practical applications for broad adoption we emphasize in-the-wild conditions in this work, in three aspects:\n\u2022 Popular devices. All the sensors used are sensors that are built into popular devices, namely smartphones and smartwatches. These are devices that people naturally carry with them. We used both iPhone and Android smartphones (users used their own personal phone, to promote natural behavior) and Pebble smartwatch. \u2022 Device placement. We gave no requirements on how to use the phone. Users were free to carry their phone however is comfortable and natural to them. \u2022 Behavioral content. The users were instructed to do their natural regular behavior (for about a week). No tasks were instructed.\nThese free-living conditions bring the research closer to real life applications, but they also introduce variability and noise, which make the recognition task harder compared to controlled conditions. Overcoming this variability requires the collection of large training datasets."}, {"heading": "CONTEXT RECOGNITION SYSTEM", "text": "Figure 1 illustrates the flow of our recognition system. The system is based on measurements from five sensors in a smartphone: accelerometer (Acc), gyroscope (Gyro), location (Loc), audio (Aud), and phone state (PS), as well as accelerometer measurements from a smartwatch (WAcc). For a given minute the system samples measurements from these six sensors as input and the task is to detect the combination of relevant context labels (Figure 1(A)), i.e. declare for each label l a binary decision: yl = 1 (the label is relevant to this minute) or yl = 0 (not relevant).\nSingle-sensor classifiers. Single-sensor classifiers use sensor-specific features to help us understand how informative each sensor can be, independently of the other sensors, for a given context label (Figure 1(B)). The following procedure was performed for a given sensor s and a given label l: (a) For each example, compute a ds-dimensional feature\nvector xs. Each sensor has a different set of relevant features. (b) Standardize each feature by subtracting precomputed (estimated from the training set) mean and dividing by precomputed standard deviation. (c) Learn a ds-dimensional logistic regression classifier from the training set. (d) Apply the logistic regression classifier to a test example to obtain a binary classification yl and probability value P (yl = 1|xs)."}, {"heading": "Sensor fusion", "text": "The system further combines the information from different sensors in several alternative ways.\nEarly fusion (EF) classifiers combine information from multiple sensors prior to the classification stage (Figure 1(C)). The following procedure was performed for a given label l: (a) Start with the sensor-specific feature vectors xs for each of the ns sensors. (b) Concatenate the (standardized) sensor-specific feature vectors into a single vector x of dimension d = \u2211ns s=1 ds (c) Learn a d-dimensional logistic regression classifier from the training set. (d) Apply the logistic regression classifier to a test example to obtain a binary classification yl and probability value P (yl = 1|x).\nLate fusion classifiers. Late fusion methods use the ns single-sensor classifiers and combine the probability predictions P (yl = 1|xs) made by those classifiers. We explore two methods for late fusion: Late fusion using average probability (LFA) (Figure 1(D)) simply averages the probability values from all the singlesensor classifiers to obtain a final probability value, i.e., P (yl = 1|x1, x2, . . . , xns) = 1ns \u2211ns s=1 P (yl = 1|xs). No additional training is performed after the single-sensor classifiers are learned. This method grants equal weight to each sensor, implicitly assuming a sensor that has a strong signal will classify with higher confidence (probability close to 0 or close to 1) and will influence the final decision more than \u201cindecisive\u201d sensors (probability close to 0.5).\nHowever, it is also reasonable to assume that certain sensors will consistently be better suited for certain labels. For example, the accelerometer might better predict activities characterized by specific movements, while audio might better predict environmental contexts that have a characteristic sound. Late fusion using learned weights (LFL) (Figure 1(E)) is a second type of late fusion that places varying weight on each sensor. This method learns a second layer of ns-dimensional logistic regression model. The second layer\u2019s input is the probability outputs of the single-sensor models and it outputs a final decision yl and probability output."}, {"heading": "DATA COLLECTION", "text": "For the purpose of data collection in a large scale we developed a mobile application (app) called ExtraSensory, with versions for both iPhone and Android smartphones, and a companion application for the Pebble smartwatch that integrates with both. The app was used to collect both sensor measurements and ground truth context labels. The app records a 20sec window of high frequency measurements every minute. The flexible user interface provided the user with many mechanisms to self-report the relevant context labels with minimal effort and time (see Figure 2).\n3 walking listening to music at the beach Sensor Measurements Classifier Context Labels\noutdoors\nacceleration gyroscope\nlocation\naudio\nphone state\nSensor Measurements Features Binary Classifier Label Probability\nx11 x12 . . .\n+\n- Pr(walking|x1)\nx21 x22 . . .\nx31 x32 . . .\nx41 x42 . . .\nPr(walking|x2)\nPr(walking|x3)\nPr(walking|x4)\n-\n-\n-\n+\n+\n+\nx11 x12 . . .\nPr(walking|x)\n-\n+\nA\nB\nC D E\nx21 x22 . . .\nPr(walking|x) Pr(walking|x)\n-\n+ Pr(walking|x1) Pr(walking|x2)\nPr(walking|x3)\n...\nAverage\nPr(walking|x1)\nPr(walking|x2)\nPr(walking|x3) ...\nLabel Probability\nSensorspecific\nProbability\n+\nLabel Probability Sensorspecific\nProbability\nBinary Classifier Binary Classifier Label Probability Concatenated Features\nFig. 1. Automatic context recognition. (A) While the person is engaged in natural behavior the system uses sensor measurements from the smartphone and smartwatch to automatically recognize the person\u2019s detailed context. (B) The classification pipeline. Appropriate features are extracted from each sensor. For a given context label, classification can be done based on each sensor independently. (C) Under Early fusion (EF), features from multiple sensors are concatenated to form a long feature vector. (D) Late fusion using averaging (LFA) simply averages the output probabilities of the single-sensor classifiers. (E) Late fusion with learned weights (LFL) learns how much to \u201clisten\u201d to each sensor when making the final classification.\n4 Sixty subjects (users) were recruited using fliers posted around the UC San Diego campus and campus-based email lists. Thirty-four of the subjects were iPhone users, with iPhone devices of generations from iPhone4 to iPhone6 and with operating system (iOS) versions 7, 8 and 9. Twentysix subjects were Android users, with devices by Samsung, Nexus, Motorola, Sony, HTC, Amazon Fire-Phone and PlusOne. The subjects were from diverse ethnic backgrounds (each subject self-reported their ethnicity in a survey), including Chinese, Mexican, Indian, Caucasian, AfricanAmerican and more. The majority of the subjects (93%) were right handed, and chose to wear the smartwatch on their left wrist. The dataset is homogeneous with regard to occupation; almost all the subjects were students (undergraduate or graduate) or research assistants. Thirty-four subjects were female and twenty-six were male. Table 1 describes additional subject characteristics. We installed the app on each subject\u2019s personal phone (to maximize authenticity of natural behavior) and provided the watch to the subject (56 out of the 60 agreed to wear the watch). The subject then engaged in their usual behaviors for approximately a week, while keeping the app running in the background on their phone as much as was possible and convenient. The subject was asked to report as many labels as possible without interfering too much with their natural behavior. More details are available in the Supplementary material.\nThe dataset, titled ExtraSensory, contains a total of 308,320 labeled examples (minutes) from the sixty users. Not all the sensors were available at all times. Table 2 specifies how many labeled examples have measurements from each sensor. The dataset is publicly available and researchers are encouraged to use it for development and comparison of methods for context recognition [33]."}, {"heading": "EVALUATION AND RESULTS", "text": "We evaluated classification performance using five-fold cross validation (each fold has 48 users in train set and the other 12 users in test set) and measured performance using the balanced accuracy (BA) and F1 metrics. Our system reached balance accuracy of 80% over a wide variety of context labels. Figure 3(A,C) shows performance for a selected set of labels (see Tables S1\u2013S4 in Supplementary material for results from all 51 labels). While all the single-sensor classifiers had average performance far above chance level, they each have their weaknesses \u2014 specific labels where they performed poorly. On the other hand, sensor fusion (especially late fusion) consistently performed well, in many cases improving compared to all the single-sensors. Still, in specific cases, sensor fusion failed to improve over the best single-sensor classifier (e.g. \u201crunning\u201d and \u201cshower\u201d were well detected using only the watch \u2014 adding information from the other sensors only distracted from the useful signal). Such issues can be overcome by collecting more training data from a variety of participants to improve generalization. In such cases it may also be advantageous to decide in advance which sensor is most fitting to detect a specific label. Figure 3(B) depicts additional labels for which we collected fewer training examples and shows that reasonable recognition rates can still be achieved when the appropriate sensor is selected.\nUnderstanding why sensor fusion helps The performance of single-sensor classifiers on selected labels (Figure 4(A)) demonstrates how the most informative sensor may vary depending on the context. For example, in detecting sleeping it is clear that the watch carries a stronger signal than the phone\u2019s motion sensors (Acc, Gyro) \u2014 during sleep the phone may be lying motionless on a nightstand, while the watch records the person\u2019s wrist movement. Similarly, contexts such as \u201cshower\u201d or \u201cin a meeting\u201d have unique acoustic signatures (e.g., water running, voices) that allow the audio-based classifier to perform strongly. When showering it is reasonable that the phone will be in a different room than the person, in which case the watch is an important indicator of the activity. Other activities, such as bicycling, achieved good recognition rates with all sensors. These insights provide intuition to the advantages of the learned late fusion method (LFL), which fuses sensor information by learning to weight each sensor\u2019s prediction appropriately for a given context label. Figure 4(A) demonstrates that the LFL method assigns reasonable weights to the six sensors \u2014 sensors that perform more strongly for a given label are given higher weight.\nInvestigating where misclassification occurs helps to understand the predictive ability of the system. Figure 4(B\u2013 G) shows confusion matrices that depict misclassification rates between related context labels. For example, a classifier using the phone\u2019s motion sensors (Acc and Gyro) (Figure 4(B)) to discriminate between body movement/posture states confuses even dissimilar labels (e.g., \u201crunning\u201d vs. \u201clying down\u201d). Such errors may be caused by the fact that people don\u2019t always carry their phone in their pocket \u2014 subjects were sometimes running on a treadmill with their phone next to them, motionless. The smartwatch is an ideal\n5\nFig. 2. Screenshots from the ExtraSensory mobile application (iPhone version). (A) On the history tab the user can see a daily log of activities and contexts. The server\u2019s real-time predictions of the main activity appear with question marks and help the user organize the log and recall when their activity may have changed. The user can update the labels to the history records (including adding secondary labels like \u201cat home\u201d and \u201ceating\u201d), merge consecutive records into a longer period, and split records. (B) Using the \u201cactive feedback\u201d button the user can report they will be engaged in a specific context starting immediately and valid for a set period of time. (C) Periodic notifications remind the user to provide labels. If the user is engaged in the same context as they recently reported they simply need to reply \u201ccorrect\u201d to the question. If any element of the context has changed they can press \u201cnot exactly\u201d and be directed to a screen where they can update the labels of the recent time period. These notifications appear on the watch as well, which enables easier responses.\naddition \u2014 while being convenient and natural to wear it also adds a strong signal about the motion of the user\u2019s wrist, even when the phone is motionless. Indeed, when watch accelerometer features were added to the classifier (Figure 4(C)), the confusion between activities was reduced.\nAudio signal from the smartphone (Figure 4(D)) is informative for labels related to the environmental context. We see a hierarchy of misclassification: while there is some confusion between labels that share similar acoustic properties (e.g., toilet vs. shower, class vs. meeting), there is a sharper distinction between label groups from different domains (e.g., toilet and shower vs. class and meeting vs. restaurant).\nAdditionally, the position of the phone with respect to the user provides cues about the user\u2019s activity (e.g. when the phone is lying on a table it is more likely the user is showering than walking to work; when the phone is in the pocket the user is probably not sleeping). The ability to recognize the phone\u2019s position will improve overall context recognition. A single modality is not sufficient to fully identify phone position. A classifier based on the phone\u2019s motion sensors is sensitive to movement, so when the phone was in a bag (possibly motionless) it was often mistaken for being on a table (Figure 4(E)). On the other hand, a classifier using audio alone is more sensitive to whether the phone is enclosed or exposed to environmental sounds, so with this classifier cases of \u201cphone in bag\u201d were mistaken for \u201cphone in pocket\u201d and \u201cphone in hand\u201d was often mislabeled as\n\u201cphone on table\u201d (Figure 4(F)). However, by combining motion and audio modalities, the classifier synthesized these two dimensions of discrimination to better recognize phone position (Figure 4(G))."}, {"heading": "USER PERSONALIZATION.", "text": "Since everyone moves, behaves and uses their phone slightly differently, a system that is fine-tuned to a specific user should outperform a more general model. To explore the potential of personalization we performed experiments with a single test user. We compared three models: (1) universal (trained on data from other users), (2) individual (trained on half of the data from the same test user) and (3) adapted (merges both other users and the same user\u2019s information). We tested the three models on the same, unseen, data (the second half from the test user).\nFigure 5 shows the results of these experiments. The universal model demonstrates good performance (average balanced accuracy of 0.85 and F1 of 0.56 over the 15 tested labels), showing the basic ability of a universally-trained product to recognize a new user\u2019s context. As suspected, the individual model performed better than the universal model for some of the labels (\u201clying down\u201d, \u201csitting\u201d, \u201csleeping\u201d, \u201cat home\u201d, \u201ccomputer work\u201d and \u201cat main workplace\u201d). The disadvantage of a strictly-individual model is the lack of training data. For many context labels there are no examples\n6 0.0 0.2 0.4 0.6 0.8 1.0\nBalanced Accuracy\nLying Down\nSitting\nWalking\nRunning\nBicycling\nSleeping\nLab Work\nIn Class\nIn a Meeting\nMain Workplace\nIndoors\nOutside\nIn a Car\nOn a Bus\nDrive (I'm the Driver)\nDrive (I'm a Passenger)\nHome\nRestaurant\nPhone in Pocket\nExercise\nCooking\nShopping\nStrolling\nDrinking (Alcohol)\nBathing (Shower)\nA\n0.0 0.2 0.4 0.6 0.8 1.0\nBalanced Accuracy\nStairs (Going Up)\nStairs (Going Down)\nElevator\nCleaning\nDoing Laundry\nWashing Dishes\nSinging\nAt a Party\nBeach\nAt a Bar\nB"}, {"heading": "Recall Prec F1 BA", "text": "Average\n0.0\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\nC\np99 Acc Gyro WAcc Loc Aud PS EF LFA LFL\nFig. 3. Overall performance of the single-sensor classifiers (Acc, Gyro, WAcc, Loc, Aud and PS) and the sensor-fusion classifiers (EF, LFA and LFL). (A) Balanced accuracy (BA, random chance level is always 0.5) scores for selected labels from diverse domains. For most labels late fusion classifiers (LFA and LFL) performed better than single-sensor classifiers. (B) BA scores for additional labels that had fewer examples. For each label here there was one very informative sensor (e.g. Gyroscope for \u201cElevator\u201d or audio for \u201cAt a party\u201d) but sensor-fusion failed to improve, possibly due to having less training examples. (C) Average performance (recall, precision, F1 and BA) over a wide range of context labels (from A). All average scores were well above the p99 value, which marks the 99th percentile of random score \u2014 scores above the p99 value have less than 1% probability of being achieved randomly (p99 was estimated from 1000 random simulations).\n7 0 2 4 6 le a rn e d w e ig h t Bicycling\nA cc G y ro\nW A\ncc Lo c A u d P S\n0.5\n0.7\n0.9\nB A\nSleeping\nA cc G y ro\nW A\ncc Lo c A u d P S\nAt main workplace\nA cc G y ro\nW A\ncc Lo c A u d P S\nIn a meeting\nA cc G y ro\nW A\ncc Lo c A u d P S\nIn a car\nA cc G y ro\nW A\ncc Lo c A u d P S\nBathing - shower\nA cc G y ro\nW A\ncc Lo c A u d P S\nA\n8 available from the individual user and for some labels there are only a limited number of examples we can acquire in a few days, which risks over-fitting to these few examples. This demonstrates the advantage of using a universal model that was pre-trained on plenty of data from many users. The user-adapted model has benefits of both the universal and the individual models, demonstrated by an improvement in recognition performance (average BA of 0.87 and F1 of 0.60). LFA is a simple heuristic that manages to demonstrate this advantage. For each label, when there is not enough data to train an individual model, the adapted model relies only on the universal model. When there is enough data to train an individual model, the adapted model \u201clistens\u201d to the universal and individual models, in some cases achieving better performance than either model on its own (e.g. \u201csleeping\u201d, \u201cat home\u201d).\nIn practical systems, the logistics of implementing personalization may not be an obvious task. For medical applications, the clinician or patient may decide that their cause is important and worth dedicating some effort to provide individual labeled data for a few days, in order to better adapt the model. However, in commercial applications the users (clients) may not be motivated to invest the extra effort in labeling. In such cases semi-supervised methods can still be used to make the most of unlabeled data from the individual user and personalize the model."}, {"heading": "DAILY ACTIVITY SCHEDULE.", "text": "To illustrate the potential usage of having detailed and frequent behavioral context information we present a daily activity schedule analysis. Figure 6 shows the distribution of eight activities around the daily (24-hour) clock for two groups of users: users who go to sleep before 11:45pm (26 users, total of 142,587 minutes. Figure 6(A,C)) and after 11:45pm (26 users, total of 141,443 minutes. Figure 6(B,D)) (see Supplementary material). The first group spends overall statistically significantly more time sleeping than the second group, and less time in the toilet than the second group. For the other activities the overall time spent is not statistically significantly different between the groups, but the distribution over the hours of day is distinctively different. Time spent at main workplace is more concentrated at 9am\u20136pm for the first group and more spread for the second. Eating times show a clearer pattern of breakfast, lunch and dinner for the first group. \u201cToilet\u201d, \u201crunning\u201d and transportation (car or bus) activities are clearly concentrated in morning and evening times for the first group, whereas in the second group they are spread all over the daytime and even during night time. It seems that the \u201cearly bed-timers\u201d form a more homogeneous, regular and routine group than the \u201clate sleepers\u201d. It should be noted that our data\u2019s subject group is not representative of the general population and this analysis is merely an example of the potential usage of detailed contextual information. Researchers in health or social science who are interested in differences in activity patterns (e.g. [2]) can perform similar analyses using automated context recognition logs and compare specific populations (e.g. smoking vs. non-smoking, treatment vs. control, different age groups or different ethnic groups)."}, {"heading": "FUTURE DIRECTIONS", "text": "Context aware intelligent technologies raise the issue of user privacy and many approaches can be used to let the user control the balance between the private behavioral information they expose to the system and the utility they gain from it. The context recognition system can be implemented either on \u201cthe cloud\u201d or on the phone device itself \u2014 preventing sensitive information from being sent out. Recognition can be limited to focus on certain behavioral aspects (e.g. physical exercise) while ignoring others. Depending on the application, sufficient information can be passed downstream without exposing the fully recognized behavioral context. For instance, if a medical application is designed to monitor the total time of walking and running done every day, these statistics can be accumulated during the day and sent at the end of the day, without divulging the specific schedule of when (and where) the user did which activity.\nIn this work we used simple computational methods. Better recognition can further be achieved by researching feature extraction methods, modeling inter-label correlation, using information from the near past, analyzing mood context, using online learning and active learning to improve models in real time, and leveraging the ample quantities of unlabeled data to personalize models. Differential privacy and similar approaches can be explored for privacy preservation of behavioral data. The public dataset we have collected enables the exploration of these research directions and many more, and will serve the research community as a standard by which to compare algorithm performance."}, {"heading": "SUPPLEMENTARY MATERIAL", "text": "Supplementary material has technical details about the following components of the work:\n\u2022 Mobile app \u2022 Data collection procedure \u2022 Sensor measurements \u2022 Extracted features \u2022 Label processing \u2022 Classification methods \u2022 Performance evaluation \u2022 User personalization assessment \u2022 Daily activity schedule analysis \u2022 Detailed results tables"}, {"heading": "Mobile app", "text": "For the purpose of data collection in a large scale we developed a mobile application called ExtraSensory, with versions for both iPhone and Android smartphones, and a companion application for the Pebble smartwatch that integrates with both. The app was used for supervised data collection, meaning it collects both sensor measurements and ground truth context labels. The app is scheduled every minute to automatically record measurements for 20 seconds from the sensors. Sensors are sampled in frequencies appropriate for their domain, and include motion-responsive sensors, location services, audio, environment sensors, as well as bits of information about the phone\u2019s state. When the watch is available (within Bluetooth range and paired with the phone) measurements from the watch are also collected by the app during the 20 second recording session. More details about the sensors are provided in \u201cSensor measurements\u201d. At the end of the 20 second recording session the measurements are bundled in a zip file and sent through the internet (if a WiFi network is available) to our lab\u2019s server, which runs a quick calculation and replies with an initial prediction of the activity (e.g. sitting, walking, running). All communication between the app and the server is secure and encrypted, and identified only by a unique universal identifier (UUID) that was randomly generated for each user.\nIn addition to collecting sensor measurements, the app\u2019s interface provides several mechanisms for the user to report labels about their context. This was a crucial part of the research design and we had to overcome a basic trade-off: on one hand we wanted to collect ground truth labels for as many examples (minutes) as possible and with much detail (combination of all the relevant context labels). On the other hand we did not want the subject to interact with the app every minute to report labels, both because it would be an extreme inconvenience for the subject and because it would impact the natural behavior of the subject and miss the point of collecting data in-the-wild. To balance this tradeoff, we designed a flexible interface that helps minimize the user-app interaction time. The following label-reporting mechanisms were included:\n\u2022 A history journal presents the user\u2019s activities chronologically as a calendar and enables the user to easily edit the context labels of time ranges in the past (up to one day in the past). The user can easily\nmerge a sequence of minutes to a single \u201cevent\u201d with the same context labels, or split a calendar event to describe a change in context. See Figure 2(A). The real-time predictions from the server assist the user to recall when their activity changed \u2014 consecutive minutes with the same prediction from the server are merged to a single item on the history calendar. \u2022 The user can also initiate active feedback by reporting labels describing their context in the immediate future (starting \u201cnow\u201d for up to half an hour in the future). See Figure 2 (B). \u2022 Every x minutes (by default, x is 10 minutes, but can be set by the user) the app presents a notification to remind the user to provide labels. If the the user has recently provided labels, the notification asks whether the user was still engaged in the same activities \u2014 allowing for a quick and easy response if the answer is \u201cyes\u201d. See Figure 2(C). \u2022 The notifications also appear on the smartwatch, allowing for an easier response with a click of a button on the watch, without using the phone itself. \u2022 When selecting labels from the menu, a side-bar index allows quick search of the relevant labels, either by categories (e.g. sports, work, company) or through a \u201cfrequently used labels\u201d menu, which presents labels that the user has applied in the past. The category in which a label was presented in the menu does not matter, and a label can appear under different categories (e.g. \u201cskateboarding\u201d appears under \u201csports\u201d, \u201cleisure\u201d and \u201ctransportation\u201d) \u2014 the only reason for these categories is to make it easy for the user to find the relevant label quickly."}, {"heading": "Data collection procedure", "text": "The study\u2019s research plan and consent form were approved by the university\u2019s institutional review board (IRB). Human subjects were recruited for the study via fliers across campus, university mailing lists and word of mouth. Every subject read and signed the consent form. The researchers installed the app on each subject\u2019s personal phone (to maximize authenticity of natural behavior). The subject then engaged in their usual behaviors for approximately a week, while keeping the app running in the background on their phone as much as was possible and convenient. The subject was asked to report as many labels as possible without interfering too much with their natural behavior. Subjects varied in their level of rigorousness with respect to providing labels: some wanted to be very precise (with specific detailed combinations of labels, and trying to keep minute-to-minute precision) and others tended to be less specific and to dedicate less effort. The subjects who used the watch, which we supplied them with, were told that it is fine to get it wet (wash hands, shower) but not submerge it (swimming). They were also asked to turn off the watch app whenever they removed the watch from their wrist and to turn it back on when they wore the watch \u2014 so we can generally assume that whenever watch measurements are available they were taken from the subject\u2019s wrist.\nUsing the app consumes the phone\u2019s battery more quickly than normal. To make up for this, the researchers\n12\nprovided participants with an external portable battery, which provides one extra charge during the day. The researchers also provided the subject with the Pebble smartwatch (56 of the subjects agreed to use the watch). The external battery and the smartwatch were returned at the end of the study. Each subject was compensated for their participation. The basic compensation was in the amount of $40, and an additional amount was calculated based on the amount of labeled data that the subject contributed (as an incentive to encourage reporting many labels). The total compensation per subject was between $40 and $75."}, {"heading": "Sensor measurements", "text": "Raw sensor measurements are provided in the publicly available dataset [33]. High frequency measurements: Each sensor (and pseudo-sensor) in the following list was sampled at 40Hz during the \u223c20 second recording session to produce a time series of \u223c800 time points. The sampling relies on the design of the phone\u2019s hardware and operating system and the sampling rate was not guaranteed to be accurate (especially for the Android devices). For that reason the time reference of each sample in a time series was also recorded; the differences between consecutive time references were approximately 25 milliseconds.\n\u2022 Accelerometer. Time series of 3-axis vectors of acceleration according to standard axes of phone devices. \u2022 Gyroscope. Time series of 3-axis vectors of rotation rate around each of the phone\u2019s 3 axes. \u2022 Magnetometer. Time series of 3-axis vectors of the magnetic field. \u2022 Processed signals. Both iPhone and Android operating systems provide processed versions of the signals: The raw acceleration is split to the gravity acceleration (estimated direction of gravity at every moment, the magnitude is always 1G) and the usergenerated acceleration (subtraction of the gravity signal from the raw acceleration). For the gyroscope the OS has a calibrated version that attempts to remove drift effects. For the magnetometer the OS has an unbiased version that subtracts the estimated bias of the magnetic field created by the device itself.\nIn this paper we used the raw acceleration signal (which includes the effects of gravity) and the calibrated version of the gyroscope signal. Acceleration is reported in units of G (gravitational acceleration on the surface of the Earth) on iPhone and in units of m/s2 on Android. Before extracting features we converted the Android acceleration measurements to units of G.\nWatch measurements: From the Pebble smartwatch we collected signals from the two available sensors\u2014 accelerometer and compass. Acceleration was sampled at 25Hz and describes a 3-axis vector of acceleration (in units of mG) relative to the watch\u2019s axes-system. The compass does not have a constant sampling rate; it was requested to provide an update of the heading whenever a change of more than one degree was detected. The compass takes some time to calibrate before providing measurements, so some examples that have watch acceleration measurements are missing compass measurements.\nLocation measurements: Both iPhone and Android provide location services (based on a combination of GPS, WiFi and cellular communications). The app samples location data in a non-constant rate, as the location service updates each time a movement is detected. This creates a time series of varying length (sometimes just a single time point in a recording session, sometimes more than 20 points) of location updates. Each update has a relative time reference and the estimated location measurements: latitude coordinate, longitude coordinate, altitude, speed, vertical accuracy and horizontal accuracy (these accuracies describe the range of reasonable error in location). Some of these values may be missing at times (e.g. when the phone is in a place with weak signals). In addition to the time series of location updates, the app calculates on the phone some basic heuristic location features: standard deviation of latitude values, standard deviation of longitude values, total change of latitude (last value minus first value), total change of longitude, average absolute latitude derivative and average absolute longitude derivative (as proxy to the speed of the user).\nTo protect our study subjects\u2019 privacy (collected examples with label \u201cat home\u201d that also include the exact location coordinates may reveal the subject\u2019s identity) the app has an option to select a location (typically their home) they would like to disguise. For the subjects that opted to use this option, whenever they were within 500 meters of their chosen location, the app would not send the latitude and longitude coordinates from the current recording (but it would send the other estimated location values such as altitude, speed, as well as the basic heuristic location features).\nLow frequency measurements: These measurements were sampled just once in a recording session (approximately once per minute). Some of them describe the phone state (PS): app state (foreground/background), WiFi connectivity status, battery status (charging, discharging), battery level, or phone call status. Other low frequency measurements are taken from sensors built in to the phone, if available: proximity sensor, ambient light, temperature, humidity, air pressure.\nAudio data: Audio was recorded from the phone\u2019s microphone in 22,050 Hz for the duration of the recording session (\u223c20 seconds). Audio was not available for recording when the phone was being used for a phone call. In order to maintain the privacy of the subjects, the raw audio recording was not sent to the server. Instead, standard audio processing features (Mel Frequency Cepstral Coefficients (MFCC)) were calculated on the phone itself and only the features were sent to the server. The MFCCs were calculated for half-overlapping windows of 2048 samples, based on 40 Mel scaled frequency bands and 13 cepstral coefficients (including the 0th coefficient). As part of the preprocessing of the recorded audio the raw audio signal was normalized to have maximal magnitude of 1 (dividing by the maximum absolute value of the sound wave). This normalizing factor is also sent as a measurement separately from the calculated MFCC features."}, {"heading": "Extracted features", "text": "For the experiments in this work we focused on six sensors: accelerometer, gyroscope, watch accelerometer, location, au-\n13\ndio and phone state. Other sensors\u2019 measurements are available in the public dataset. Every sensor measures different physical or virtual properties and has a different form of raw measurements. Therefore we designed specific features for each sensor. The published dataset includes files with these features pre-computed for all the users [33].\nAccelerometer and Gyroscope (26 features each): Since in natural behavior the phone\u2019s position is not controlled we cannot assume it is oriented in a particular way, and it also may be changing its axes-system with respect to the ground (and with respect to the person). For that reason we had little reason to assume that any of the phone\u2019s axes will have a particular coherent correspondence to many behavioral patterns, and we extracted most of the features based on the overall magnitude of the signal. We calculated the vector magnitude signal as the euclidean norm of the 3-axis acceleration measurement at each point in time, i.e., a[t] = \u221a ax[t]2 + ay[t]2 + az[t]2. We extracted the following features:\n\u2022 Nine statistics of the magnitude signal: mean, standard deviation, third moment, fourth moment, 25th\npercentile, 50th percentile, 75th percentile, valueentropy (entropy calculated from a histogram of quantization of the magnitude values to 20 bins) and time-entropy (entropy calculated from normalizing the magnitude signal and treating it as a probability distribution, which is designed to detect peakiness in time\u2014sudden bursts of magnitude). \u2022 Six spectral features of the magnitude signal: log energies in 5 sub-bands (0\u20130.5Hz, 0.5\u20131Hz, 1\u20133Hz, 3\u20135Hz, >5Hz) and spectral entropy. \u2022 Two autocorrelation features from the magnitude signal. The average of the magnitude signal (DC component) was subtracted and the autocorrelation function was computed and normalized such that the autocorrelation value at lag 0 will be 1. The highest value after the main lobe was located. The corresponding period (in seconds) was calculated as the dominant periodicity and its normalized autocorrelation value was also extracted. \u2022 Nine statistics of the 3-axis time series: the mean and standard deviation of each axis and the 3 inter-axis correlation coefficients.\nWatch accelerometer (46 features): From the watch acceleration we extracted the same 26 features as from the phone accelerometer or gyroscope. Since the watch is positioned in a more controlled way than the phone (it is firmly fixed to the wrist), its axes have a strong meaning (e.g. motion along the x-axis of the watch describes a different kind of movement than motion along the z-axis of the watch). Hence we added 15 more axis-specific features\u2014log energies in the same 5 sub-bands as before, this time calculated for each axis\u2019 signal separately. In addition, to account for the changes in watch orientation during the recording we calculated 5 relative-direction features in the following way: we first calculate the cosine-similarity between the acceleration directions of any two time points in the time series (value of 1 meaning same direction, value of -1 meaning opposite directions and value of 0 meaning orthogonal directions). Then we averaged these cosine similarity values\nin 5 different ranges of time-lag between the compared time points (0\u20130.5sec, 0.5\u20131sec, 1\u20135sec, 5\u201310sec, >10sec).\nLocation (17 features): In this work we used location features that were based only on relative locations, and not on absolute latitude/longitude coordinates. This was in order to avoid over-fitting to our location-homogeneous training set that will not generalize well to the outside world (e.g., mistakenly learning that a specific location in the UCSD campus always corresponds to \u201cat work\u201d). Six features were calculated on the phone \u2014 this was in order to have some basic location features in cases where the subjects opted to hide their absolute location. These quick features included standard deviation of latitude, standard deviation of longitude, change in latitude (last value minus first value), change in longitude, average absolute value of derivative of latitude and average absolute value of derivative of longitude.\nThe transmitted location measurements were further processed to extract the following 11 features: number of updates (indicating how much the location changed during the 20 second recording), log of latitude-range (if latitudes were transmitted), log of longitude-range (if longitudes were transmitted), minimum altitude, maximum altitude, minimum speed, maximum speed, best (lowest) vertical accuracy, best (lowest) horizontal accuracy and diameter (maximum distance between two locations in the recording session, in meters).\nAudio (26 features): From the time series of 13- dimensional MFCC vectors (typically around 400 time frames) we calculated the average and standard deviation of each of the 13 coefficients.\nPhone State (34 features): For this work we used only the discrete phone state measurements. We represented them with a 26-dimensional one-hot representation\u2014for each property we created a binary indicator for each of the possible values the property can take, plus one indicator denoting missing data. This representation is a redundant coding of the phone state, but it facilitates the use of simple, linear classifiers over this long binary vector representation. The keys were: app state (3 options: active, inactive, background), battery plugged (3 options: AC, USB, wireless), battery state (6 options: unknown, unplugged, not charging, discharging, charging, full), in a phone call (2 options: false, true), ringer mode (3 options: normal, silent no vibrate, silent with vibrate) and WiFi status (3 options: not reachable, reachable via WiFi, reachable via WWAN).\nIn addition, we added a set of features indicating timeof-day information. We used the timestamp of every example and (using San Diego local time) extracted the hour component (one out of 24 discrete values). In order to get a flexible, useful representation we defined 8 half-overlapping time ranges: midnight-6am, 3am-9am, 6am-midday, 9am3pm, midday-6pm, 3pm-9pm, 6pm-midnight and 9pm-3am. Then we represented each example\u2019s hour with an 8-bit binary value, where 2 bins will be active for 1 relevant time range."}, {"heading": "Label processing", "text": "Since the labels are obtained by subjects self-reporting their own behaviors, the reliability of annotation is not perfect.\n14\nIn some cases, this was the result of the subject reporting labels some time after the activity had occurred and misremembering the exact time. More common are cases where the subject neglected to report labels when relevant activities occurred (perhaps because the subject was distracted, did not have time to specify all the relevant labels, or was not aware of another relevant label in the vocabulary). As part of cleaning the data, we created adjusted versions for some labels using two methods: based on location data and based on other labels.\nLocation adjusted labels. We collected absolute location coordinates of the examples that had location measurements (selecting the location update with best horizontal accuracy from each example) and visualized them on a map. This made it easier to correct some labels which were clearly reported incorrectly. In examples without location data the original label was maintained.\n\u2022 \u201cAt the beach\u201d. According to the few examples that reported being at the beach we marked areas that should be regarded as beach (and manually verified their validity by viewing them on a map). We then adjusted the label by applying \u201cAt the beach\u201d to any example with a location within these areas. \u2022 \u201cAt home\u201d. For each subject we identified the location of their home (by visualizing on a map all locations of examples where the subject reported being at home) and marked the coordinates of a visual centroid. This was only done when it was clear that we had indeed identified a location of a home. Three subjects reported being at home in two different houses, in which case we marked the two locations as locations of home. Two subjects never reported being at home but it was clear from their location to identify their location of residence. Some subjects had none or very few examples of \u201cat home\u201d with location data, so no home location was noted and their original reported labels were used. To define the adjusted version of the label \u201cat home\u201d, whenever a subject\u2019s location was within 15 meters of their marked home location (or either of the two marked home locations), the adjusted value was set to \u201ctrue\u201d; whenever a subject\u2019s location was farther than 100 meters from all the subject\u2019s marked home locations the adjusted value was set to \u201cfalse\u201d. In other cases (when the location was between 15 and 100 meters from a home location, or when there was no location data available) we retained the subject\u2019s originally reported value for \u201cat home\u201d. This adjustment removed some obviously false reports of \u201cat home\u201d (e.g., when the subject was clearly on a drive on a freeway). The adjustment manifested mostly by adding the missing label \u201cat home\u201d to many examples where the subject was clearly at home but failed to report it. \u2022 \u201cAt main workplace\u201d. Similarly to home label we identified for each subject (if they used the original label \u201cat work\u201d) the centroid location of their main workplace and created a new label \u2014 \u201cat main workplace\u201d \u2014 in a similar way. Some subjects reported being at work in different locations, so the original\nlabel \u201cat work\u201d is still valid for analysis and may have a different meaning than \u201cat main workplace\u201d (which was designed to capture behavioral patterns typical to the most common place that a person works in). This adjustment removed some examples where the label \u201cAt work\u201d was probably incorrectly reported, but more importantly, it added the missing label in cases where the subject was clearly present at their most common workplace.\nLabels corrected using other labels. We used reported values of other labels to adjust some labels. In a few cases it was clear that the reported labels were mistakes (because the combination of labels was unreasonable). In other cases a relevant label was simply not reported, even though it clearly should be relevant according to the other reported labels.\n\u2022 \u201cWalking\u201d. We identified a few cases where subjects reported walking together with labels related to driving. In cases where location data was available, it was clear on the map that the correct activity was the drive and not the walk. In the adjusted version of \u201cwalking\u201d we changed the value to \u201cfalse\u201d whenever the subject reported \u201con a bus\u201d, \u201cin a car\u201d, \u201cdrive (I\u2019m the driver)\u201d, \u201cdrive (I\u2019m a passenger)\u201d, \u201cmotorbike\u201d, \u201cskateboarding\u201d or \u201cat the pool\u201d. \u2022 \u201cRunning\u201d. The adjusted version was set to \u201cfalse\u201d for the same activities as in the adjusted \u201cwalking\u201d label, plus in cases where the subject reported \u201cplaying baseball\u201d or \u201cplaying frisbee\u201d. Although these cases are likely still valid (because the subject decided to report they were running during these playing activities), we decided to create the adjusted \u201crunning\u201d version to represent a more coherent running activity (assuming that the playing activities involve a mixture of running, walking and standing intermittently). While the adjusted versions of \u201cwalking\u201d and \u201crunning\u201d may have a few misses (e.g., some minutes during a baseball game when the subject was purely running), these misses don\u2019t harm the integrity of the multi-class experiments, which used only examples that had positive labels of \u201crunning\u201d, \u201cwalking\u201d, \u201csitting\u201d, etc.. \u2022 \u201cExercise\u201d. The adjusted version was set to \u201ctrue\u201d whenever the subject reported \u201cexercising\u201d, \u201crunning\u201d, \u201cbicycling\u201d, \u201clifting weights\u201d, \u201celliptical machine\u201d, \u201ctreadmill\u201d, \u201cstationary bike\u201d or \u201cat the gym\u201d. This adjustment boosted the representation of the exercise behavior and also took advantage of reported specific activities without enough examples to be analyzed on their own. \u2022 \u201cIndoors\u201d. The adjusted version was set to \u201ctrue\u201d whenever the subject reported \u201cindoors\u201d, \u201csleeping\u201d, \u201ctoilet\u201d, \u201cbathing \u2014 bath\u201d, \u201cbathing \u2014 shower\u201d, \u201cin class\u201d, \u201cat home\u201d, \u201cat a bar\u201d, \u201cat the gym\u201d or \u201celevator\u201d. It is reasonable that many subjects simply did not bother to report being indoors every time they did an activity indoors. \u2022 \u201cOutside\u201d. The adjusted version was set to \u201ctrue\u201d whenever the subject reported \u201coutside\u201d, \u201cskateboarding\u201d, \u201cplaying baseball\u201d, \u201cplaying frisbee\u201d,\n15\n\u201cgardening\u201d, \u201craking leaves\u201d, \u201cstrolling\u201d, \u201chiking\u201d, \u201cat the beach\u201d, \u201cat sea\u201d or \u201cmotorbike\u201d. \u2022 \u201cAt a restaurant\u201d. In the adjusted version we changed the value to \u201cfalse\u201d whenever the subject reported \u201con a bus\u201d, \u201cin a car\u201d, \u201cdrive (I\u2019m the driver)\u201d, \u201cdrive (I\u2019m a passenger)\u201d or \u201cmotorbike\u201d."}, {"heading": "Classification methods", "text": "Our system uses binary logistic regression classifiers (with a fitted intercept). Logistic regression provides a real-valued output, interpreted as the probability of the relevance of the label (value larger than 0.5 yielding a decision of \u201crelevant\u201d). For each context label we used an independent model. We first randomly partitioned the training examples into internal training and validation subsets, allocating one third of the training examples for the validation subset, while maintaining the same proportion of positive vs. negative examples in both subsets. We then used grid search to select the cost parameter C for logistic regression: for each value (out of {0.001, 0.01, 0.1, 1, 10, 100}) we trained a logistic regression model on the internal train subset and tested the model on the validation subset. We selected the value of C that resulted in highest F1 measure on the validation subset. We then re-trained a logistic regression model with the selected value on the entire training set. The learned weights from LFL for a set of selected labels that are presented in Figure 4(A) are taken from the first (of five) training set of the cross validation evaluation. To look at misclassifications and to produce the confusion matrices in Figure 4(B\u2013G) we used the multiclass (one-versus-rest) version of logistic regression, with a fixed cost value of C = 1. Each multiclass experiment used the set of examples annotated with exactly one label from the examined label subset and with data from all of the sensors of interest (so an experiment with only accelerometer and gyroscope sensors might have more examples than an experiment with accelerometer, gyroscope and watch accelerometer). These experiments were more fitting than binary classification in cases where missing labels are common. For example, labels describing the phone\u2019s position were not always consistently annotated. A binary classifier will use all negative examples to learn a decision boundary, including examples the subject forgot to label, which may skew the results if there are many missing labels."}, {"heading": "Performance evaluation", "text": "In order to make a fair comparison among the different sensors, evaluation was done on the subset of examples with data from all six core sensors available (\u223c177k examples from 51 subjects). In the training phase, however, a singlesensor classifier was allowed to use all examples available (e.g., all examples in the dataset had phone state data, so the PS single-sensor classifier was trained with all examples). While the early fusion system benefited from the advantage of modeling correlations between features of different sensors, it was limited to being trained only on examples with all sensor data available. The late fusion systems, on the other hand, had the advantage of using single-sensor classifiers that were trained on many more examples.\nClassifier performance was evaluated using 5-fold cross validation. The subjects were randomly partitioned once\ninto 5 folds, while equalizing the proportion of iPhone vs. Android users between folds (To keep a fair evaluation it was important to partition the subjects, and not randomly partition the pool of examples, in order to avoid having examples from the same subject appear in both the train set and the test set). The cross validation procedure repeats the following for each fold: (1) hold out the selected fold to act as the test set (2) train a classifier on the remaining folds (3) apply the classifier to the held out test set. For each fold and for each label, we counted the numbers of true positives (TP. Examples that were correctly classified as positive), true negatives (TN. Examples that were correctly classified as negative), false positives (FP. Examples that were wrongfully classified as positive) and false negatives (FN. Examples that were wrongfully classified as negative). At the end of the 5-fold procedure we summed up the total numbers of TP, TN, FP and FN over the entire evaluation set and calculated the following performance metrics:\n\u2022 True positive rate (TPR, also called sensitivity or recall) is the proportion of positive examples that were correctly classified as positive: recall=TPR=TP/(TP+FN). \u2022 True negative rate (TNR, also called specificity) is the proportion of negative examples that were correctly classified as negative: TNR=TN/(TN+FP). \u2022 Precision (prec) is the proportion of correctly classified examples out of the examples that the classifier declared as positive: precision=TP/(TP+FP). \u2022 Balanced accuracy is a measure that accounts for the tradeoff between true positive rate and true negative rate: BA=(TPR+TNR)/2. \u2022 The F1 measure is another such measure, which takes the harmonic mean of precision and recall: F1=(2*TPR*prec)/(TPR+prec).\nWhile the balanced accuracy is easy to interpret (chance level is always 0.5 and perfect performance is 1) the F1 measure is very sensitive to how rare the positive examples are, so for each label a typical F1 value is different. The 5-fold subject partition is available with the dataset, and we encourage researchers using the dataset to evaluate new methods to use the same 5-fold partition, in order to promote fair comparisons.\nRandom chance. To assess the statistical significance of the performance scores we achieved, we evaluated a distribution of performance scores achieved by a random classifier. The random classifier declares \u201crelevant\u201d with probability 0.5 independently for each example and for each label. To estimate the distribution of scores that such a classifier obtains, we ran 1000 simulations (each time the classifier randomly assigned binary predictions and the performance scores were calculated over the entire evaluation dataset). Chance level (expected value of the random classifier) of balanced accuracy is 0.5 for every label. For the F1 measure the chance level for each label is dependent on the proportion of positive and negative examples in the data. For each performance measure and for each label we estimated a value which we call p99, the 99th percentile among the 1000 scores achieved in the 1000 simulations. Hence the probability of obtaining a score greater than p99 by chance is less than 1%. For average (over a set of labels)\n16\nscores the p99 value was calculated similarly (computing the average score for each of the 1000 simulations)."}, {"heading": "User personalization assessment", "text": "To assess the advantages of user personalization, we selected a single test subject that had provided relatively many examples and many labels. We partitioned this user\u2019s examples into the first half and second half of the examples (according to their recording timestamps), to simulate an adaptation training period (the first half) and a deployment period (the second half). We used early fusion (EF) classifiers to combine the features from all 6 sensors. The universal model was the one used in previous experiments, taken from the fold where the test user was part of the cross validation test set (so the universal model was trained on 48 other users). The individual model was trained only on data from the test user, taken from the first half of the subject\u2019s examples. The adapted model was a combination of both the universal and individual models using the LFA method (i.e., averaging the probability outputs of both models). All three models were tested on the same set of unseen test examples (the second half of the subject\u2019s examples). For some labels, an insufficient number of examples to train an individual classifier resulted in a trivial classifier (always declaring the same answer). In those cases the performance was reported as chance level (BA of 0.5 and F1 of 0).\nDaily activity schedule analysis For the analysis of daily activity schedule we used the ground truth labels collected for eight selected labels. Using the timestamp of each minute-example we extracted the time of day (hour and minute) of every example. The criterion to partition the users to two groups was based on the time of \u201cgoing to sleep\u201d. We first detected all the minutes of \u201cstarting to sleep\u201d (where the user reported sleeping and did not report sleeping the previous minute). We then discarded sleep-start-times that were between 5am and 8pm (in order to focus on night time sleep). We were left with 52 users (other users didn\u2019t have examples of night time sleeping), for each we calculated the average sleepstart-time (with minute precision).\nWe partitioned the users to group 1 (shown on left side of figure 6) \u2014 users whose average bedtime is earlier than 11:45pm (26 users, total of 142,587 minutes) and group 2 (shown on right side of the figure) \u2014 users whose average bedtime is later than 11:45pm (26 users, total of 141,443 minutes). For each of the eight activities and for each of the two groups we calculated a histogram counting how many minute-examples of that activity were reported in each of the 24 hour-slots of the day. For each activity these 48 count values were scaled proportionally, to make the maximal count appear filling 0.9 of the ring representing that activity in the clock plot. So the heights of the 48 hour-bars with the same color (same activity) are comparable to each other, but not comparable to hour-bars of other activities.\nFor each activity and for each user we also calculated the portion of time (portion of labeled minute-examples) spent in that activity. The color legend presents the portion of time averaged over the 26 users of group 1 (left) vs. averaged over the 26 users of group 2 (right). A 2-tailed t-test (independent\nsets, equal variance) was conducted comparing the two sets of 26 values of portion-of-time, and when the result had pvalue less than 0.05 an asterisk (*) is shown in the legend.\n17\nDETAILED RESULTS TABLES\n18\nne ns p99 Acc Gyro WAcc Loc Aud PS EF LFA LFL Lying down 54359 47 0.38 0.61 0.59 0.71 0.55 0.69 0.78 0.81 0.79 0.82 Sitting 82904 50 0.49 0.58 0.58 0.67 0.59 0.62 0.72 0.75 0.75 0.74 Walking 11892 50 0.12 0.38 0.38 0.31 0.22 0.19 0.22 0.38 0.39 0.38 Running 675 19 0.01 0.03 0.03 0.04 0.01 0.01 0.01 0.03 0.04 0.04 Bicycling 3523 22 0.04 0.19 0.16 0.23 0.18 0.12 0.17 0.31 0.25 0.26 Sleeping 42920 40 0.33 0.57 0.53 0.65 0.44 0.63 0.75 0.79 0.77 0.81 Lab work 2898 8 0.03 0.08 0.06 0.06 0.11 0.09 0.11 0.21 0.16 0.19 In class 2872 13 0.03 0.05 0.06 0.04 0.07 0.10 0.06 0.13 0.12 0.14 In a meeting 2904 34 0.03 0.05 0.04 0.05 0.05 0.11 0.07 0.17 0.10 0.14 At main workplace 20382 26 0.19 0.23 0.18 0.28 0.41 0.31 0.42 0.49 0.47 0.50 Indoors 107944 51 0.55 0.74 0.73 0.70 0.68 0.75 0.71 0.78 0.79 0.78 Outside 7629 36 0.08 0.20 0.20 0.18 0.15 0.15 0.20 0.23 0.26 0.25 In a car 3635 24 0.04 0.15 0.07 0.10 0.27 0.13 0.16 0.23 0.22 0.23 On a bus 1185 24 0.01 0.04 0.03 0.03 0.07 0.04 0.06 0.07 0.07 0.06 Drive (I\u2019m the driver) 5034 24 0.06 0.21 0.09 0.15 0.37 0.16 0.23 0.31 0.31 0.31 Drive (I\u2019m a passenger) 1655 19 0.02 0.07 0.04 0.04 0.15 0.07 0.08 0.14 0.12 0.12 At home 83977 50 0.49 0.66 0.65 0.64 0.63 0.70 0.67 0.74 0.76 0.77 At a restaurant 1320 16 0.02 0.02 0.03 0.03 0.02 0.08 0.05 0.11 0.07 0.09 Phone in pocket 15301 31 0.15 0.28 0.33 0.26 0.21 0.25 0.28 0.38 0.36 0.37 Exercise 5384 36 0.06 0.21 0.18 0.24 0.14 0.13 0.19 0.27 0.26 0.25 Cooking 2257 33 0.03 0.03 0.03 0.05 0.03 0.04 0.06 0.09 0.07 0.08 Shopping 896 18 0.01 0.03 0.03 0.02 0.01 0.02 0.04 0.04 0.04 0.04 Strolling 434 8 0.01 0.02 0.02 0.01 0.01 0.01 0.02 0.03 0.03 0.03 Drinking (alcohol) 864 10 0.01 0.03 0.02 0.01 0.01 0.04 0.03 0.07 0.07 0.06 Bathing - shower 1186 27 0.01 0.01 0.02 0.04 0.01 0.02 0.01 0.04 0.04 0.05 Average 0.13 0.22 0.20 0.22 0.22 0.22 0.24 0.30 0.29 0.30\nTABLE S1 F1 performance measure for several classifiers on each label (part 1). For each label, ne is the number of examples and ns is the number of\nsubjects in the test phase. p99 represents the 99th percentile of random scores \u2014 a score above the p99 value has less than 1 % probability of being achieved randomly.\n19\nne ns p99 Acc Gyro WAcc Loc Aud PS EF LFA LFL Cleaning 1839 22 0.02 0.05 0.05 0.05 0.01 0.03 0.02 0.05 0.06 0.05 Laundry 473 12 0.01 0.01 0.01 0.01 0.00 0.01 0.01 0.02 0.01 0.02 Washing dishes 851 17 0.01 0.01 0.01 0.03 0.01 0.02 0.01 0.03 0.03 0.04 Watching TV 9412 28 0.10 0.14 0.11 0.12 0.12 0.17 0.18 0.21 0.22 0.22 Surfing the internet 11641 28 0.12 0.15 0.14 0.17 0.13 0.17 0.15 0.18 0.19 0.20 At a party 404 3 0.01 0.01 0.01 0.00 0.01 0.03 0.01 0.04 0.03 0.04 At a bar 520 4 0.01 0.00 0.01 0.01 0.01 0.00 0.09 0.00 0.03 0.06 At the beach 122 5 0.00 0.00 0.00 0.00 0.01 0.00 0.01 0.00 0.02 0.02 Singing 384 6 0.00 0.01 0.01 0.00 0.01 0.01 0.01 0.00 0.01 0.01 Talking 18976 44 0.18 0.25 0.26 0.24 0.21 0.29 0.27 0.29 0.30 0.30 Computer work 23692 38 0.21 0.26 0.25 0.30 0.31 0.30 0.35 0.38 0.39 0.39 Eating 10169 49 0.11 0.15 0.14 0.15 0.11 0.16 0.15 0.19 0.18 0.17 Toilet 1646 33 0.02 0.03 0.02 0.03 0.02 0.03 0.03 0.04 0.04 0.04 Grooming 1847 25 0.02 0.02 0.02 0.03 0.03 0.04 0.03 0.05 0.04 0.05 Dressing 1308 27 0.02 0.02 0.02 0.03 0.02 0.03 0.03 0.04 0.04 0.04 At the gym 906 6 0.01 0.01 0.01 0.02 0.01 0.02 0.02 0.03 0.03 0.03 Stairs - going up 399 17 0.01 0.02 0.02 0.01 0.01 0.01 0.00 0.01 0.02 0.01 Stairs - going down 390 15 0.00 0.02 0.02 0.01 0.01 0.01 0.00 0.01 0.02 0.01 Elevator 124 8 0.00 0.01 0.01 0.00 0.00 0.00 0.00 0.00 0.01 0.01 Standing 22766 51 0.21 0.28 0.27 0.35 0.23 0.27 0.29 0.36 0.34 0.35 At school 25840 39 0.23 0.30 0.29 0.30 0.38 0.34 0.39 0.41 0.41 0.41 Phone in hand 8595 37 0.09 0.17 0.17 0.11 0.13 0.13 0.13 0.16 0.17 0.16 Phone in bag 5589 22 0.06 0.10 0.08 0.07 0.09 0.11 0.12 0.15 0.14 0.14 Phone on table 70611 43 0.45 0.58 0.58 0.51 0.50 0.51 0.56 0.56 0.59 0.58 With co-workers 4139 17 0.05 0.06 0.06 0.07 0.06 0.11 0.08 0.13 0.12 0.13 With friends 12865 25 0.13 0.15 0.17 0.14 0.15 0.18 0.18 0.15 0.19 0.18 Average 0.08 0.11 0.11 0.11 0.10 0.11 0.12 0.13 0.14 0.14\nTABLE S2 F1 performance measure for several classifiers on each label (part 2). For each label, ne is the number of examples and ns is the number of\nsubjects in the test phase. p99 represents the 99th percentile of random scores \u2014 a score above the p99 value has less than 1 % probability of being achieved randomly.\n20\nne ns p99 Acc Gyro WAcc Loc Aud PS EF LFA LFL Lying down 54359 47 0.50 0.72 0.69 0.81 0.66 0.79 0.85 0.87 0.86 0.88 Sitting 82904 50 0.50 0.63 0.61 0.68 0.61 0.65 0.69 0.76 0.75 0.75 Walking 11892 50 0.51 0.77 0.80 0.75 0.66 0.63 0.70 0.80 0.80 0.80 Running 675 19 0.52 0.69 0.66 0.80 0.56 0.48 0.58 0.62 0.69 0.71 Bicycling 3523 22 0.51 0.81 0.81 0.84 0.81 0.77 0.83 0.87 0.87 0.87 Sleeping 42920 40 0.50 0.75 0.70 0.81 0.62 0.79 0.87 0.88 0.88 0.89 Lab work 2898 8 0.51 0.71 0.62 0.65 0.84 0.71 0.81 0.84 0.87 0.85 In class 2872 13 0.51 0.60 0.63 0.57 0.74 0.76 0.67 0.70 0.77 0.80 In a meeting 2904 34 0.51 0.60 0.57 0.62 0.63 0.79 0.73 0.80 0.79 0.82 At main workplace 20382 26 0.50 0.57 0.49 0.63 0.76 0.65 0.78 0.80 0.80 0.81 Indoors 107944 51 0.50 0.66 0.66 0.67 0.63 0.71 0.72 0.75 0.75 0.76 Outside 7629 36 0.51 0.70 0.73 0.70 0.66 0.66 0.73 0.74 0.77 0.78 In a car 3635 24 0.51 0.79 0.65 0.71 0.81 0.77 0.84 0.85 0.86 0.86 On a bus 1185 24 0.52 0.73 0.69 0.67 0.75 0.74 0.82 0.77 0.84 0.83 Drive (I\u2019m the driver) 5034 24 0.51 0.79 0.61 0.75 0.82 0.74 0.83 0.84 0.86 0.87 Drive (I\u2019m a passenger) 1655 19 0.51 0.76 0.71 0.64 0.79 0.76 0.81 0.84 0.84 0.85 At home 83977 50 0.50 0.65 0.63 0.66 0.63 0.71 0.70 0.75 0.77 0.78 At a restaurant 1320 16 0.52 0.62 0.67 0.68 0.58 0.85 0.77 0.76 0.83 0.81 Phone in pocket 15301 31 0.50 0.69 0.75 0.67 0.61 0.64 0.72 0.77 0.77 0.77 Exercise 5384 36 0.51 0.73 0.73 0.77 0.71 0.70 0.77 0.81 0.80 0.81 Cooking 2257 33 0.51 0.52 0.53 0.68 0.57 0.62 0.68 0.71 0.71 0.72 Shopping 896 18 0.52 0.70 0.70 0.69 0.54 0.59 0.79 0.69 0.76 0.80 Strolling 434 8 0.53 0.67 0.74 0.72 0.67 0.64 0.75 0.66 0.77 0.74 Drinking (alcohol) 864 10 0.52 0.71 0.69 0.50 0.56 0.80 0.74 0.70 0.82 0.81 Bathing - shower 1186 27 0.52 0.53 0.55 0.73 0.47 0.63 0.47 0.64 0.67 0.70 Average 0.50 0.68 0.66 0.70 0.67 0.70 0.75 0.77 0.80 0.80\nTABLE S3 BA performance measure for several classifiers on each label (part 1). For each label, ne is the number of examples and ns is the number of\nsubjects in the test phase. p99 represents the 99th percentile of random scores \u2014 a score above the p99 value has less than 1 % probability of being achieved randomly.\n21\nne ns p99 Acc Gyro WAcc Loc Aud PS EF LFA LFL Cleaning 1839 22 0.51 0.63 0.64 0.71 0.41 0.60 0.51 0.60 0.70 0.68 Laundry 473 12 0.53 0.65 0.66 0.66 0.38 0.53 0.65 0.58 0.63 0.63 Washing dishes 851 17 0.52 0.40 0.52 0.70 0.58 0.60 0.57 0.65 0.70 0.70 Watching TV 9412 28 0.51 0.61 0.54 0.56 0.56 0.64 0.67 0.65 0.70 0.68 Surfing the internet 11641 28 0.51 0.56 0.55 0.60 0.54 0.60 0.57 0.59 0.63 0.63 At a party 404 3 0.53 0.74 0.71 0.49 0.54 0.81 0.56 0.54 0.76 0.75 At a bar 520 4 0.52 0.45 0.66 0.53 0.60 0.49 0.93 0.50 0.61 0.66 At the beach 122 5 0.55 0.62 0.48 0.47 0.72 0.58 0.70 0.50 0.71 0.70 Singing 384 6 0.53 0.57 0.64 0.46 0.61 0.68 0.59 0.50 0.65 0.53 Talking 18976 44 0.50 0.60 0.61 0.60 0.54 0.65 0.65 0.65 0.67 0.67 Computer work 23692 38 0.50 0.57 0.56 0.62 0.63 0.61 0.68 0.68 0.71 0.70 Eating 10169 49 0.51 0.59 0.58 0.60 0.51 0.61 0.62 0.66 0.65 0.65 Toilet 1646 33 0.51 0.57 0.51 0.58 0.57 0.64 0.59 0.65 0.66 0.66 Grooming 1847 25 0.51 0.44 0.49 0.62 0.59 0.63 0.58 0.60 0.63 0.63 Dressing 1308 27 0.52 0.51 0.52 0.64 0.54 0.65 0.61 0.64 0.67 0.67 At the gym 906 6 0.52 0.50 0.55 0.58 0.57 0.65 0.70 0.54 0.64 0.61 Stairs - going up 399 17 0.53 0.70 0.73 0.65 0.55 0.55 0.51 0.58 0.69 0.67 Stairs - going down 390 15 0.53 0.71 0.73 0.66 0.55 0.55 0.51 0.58 0.71 0.66 Elevator 124 8 0.55 0.72 0.76 0.44 0.54 0.71 0.51 0.49 0.73 0.73 Standing 22766 51 0.50 0.60 0.59 0.67 0.54 0.59 0.63 0.68 0.67 0.68 At school 25840 39 0.50 0.59 0.59 0.59 0.66 0.64 0.68 0.70 0.70 0.70 Phone in hand 8595 37 0.51 0.65 0.68 0.56 0.59 0.59 0.61 0.64 0.67 0.66 Phone in bag 5589 22 0.51 0.59 0.56 0.55 0.59 0.64 0.69 0.67 0.68 0.69 Phone on table 70611 43 0.50 0.60 0.61 0.56 0.53 0.55 0.61 0.61 0.62 0.62 With co-workers 4139 17 0.51 0.57 0.57 0.61 0.58 0.68 0.67 0.69 0.71 0.72 With friends 12865 25 0.51 0.55 0.58 0.53 0.54 0.60 0.60 0.55 0.61 0.58 Average 0.50 0.59 0.60 0.59 0.56 0.62 0.62 0.60 0.67 0.66\nTABLE S4 BA performance measure for several classifiers on each label (part 2). For each label, ne is the number of examples and ns is the number of\nsubjects in the test phase. p99 represents the 99th percentile of random scores \u2014 a score above the p99 value has less than 1 % probability of being achieved randomly."}], "references": [], "referenceMentions": [], "year": 2017, "abstractText": "We demonstrate that a person\u2019s behavioral and environmental context can be automatically recognized by harnessing the sensors built into smartphones and smartwatches. We propose a generic system that can simultaneously recognize many contextual attributes from diverse behavioral domains. By fusing complementary information from different types of sensors our system successfully recognizes fine details of work and leisure activities, body movement, transportation, and more. Health monitoring, clinical intervention, aging care, personal assistance and many more applications will benefit from automatic, frequent and detailed context", "creator": "LaTeX with hyperref package"}}}