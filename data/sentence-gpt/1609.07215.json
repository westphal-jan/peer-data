{"id": "1609.07215", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "23-Sep-2016", "title": "A Novel Progressive Multi-label Classifier for Classincremental Data", "abstract": "In this paper, a progressive learning algorithm for multi-label classification to learn new labels while retaining the knowledge of previous labels is designed. New output neurons corresponding to new labels are added and the neural network connections and parameters are automatically restructured as if the label has been introduced from the beginning. This work is the first of the kind in multi-label classifier for class-incremental learning. It also addresses the issue of missing data. It is also based on this paper by Professor K\u00fcnmei-Passela (Dahlstrom-Hohrer and colleagues) and other collaborators of the L'Grontera and S. V\u00f6stman (S\u00f6stman and K\u00fcnmei-Passela; Dahlstrom-Hohrer and colleagues; S\u00f6stman and K\u00fcnmei-Passela; and S\u00f6stman and K\u00fcnmei-Passela) and was published in Nature in September 2009. This paper was funded by the Department of Science and Engineering in Germany, and was supported by the grant of the European Union's National Science Foundation (Grant $16.05, 2009).", "histories": [["v1", "Fri, 23 Sep 2016 03:09:24 GMT  (1194kb)", "http://arxiv.org/abs/1609.07215v1", "5 pages, 3 figures, 4 tables"]], "COMMENTS": "5 pages, 3 figures, 4 tables", "reviews": [], "SUBJECTS": "cs.LG cs.NE", "authors": ["mihika dave", "sahil tapiawala", "meng joo er", "rajasekar venkatesan"], "accepted": false, "id": "1609.07215"}, "pdf": {"name": "1609.07215.pdf", "metadata": {"source": "CRF", "title": "A Novel Progressive Multi-label Classifier for Class- incremental Data", "authors": ["Mihika Dave", "Sahil Tapiawala", "Meng Joo Er", "Rajasekar Venkatesan"], "emails": [], "sections": [{"heading": null, "text": "multi-label classification to learn new labels while retaining the knowledge of previous labels is designed. New output neurons corresponding to new labels are added and the neural network connections and parameters are automatically restructured as if the label has been introduced from the beginning. This work is the first of the kind in multi-label classifier for class-incremental learning. It is useful for real-world applications in applied fields such as robotics where streaming data are available and the number of labels is often unknown. Based on the Extreme Learning Machine framework, a novel universal classifier with plug and play capabilities for progressive multi-label classification is developed. Experimental results on various benchmark synthetic and real datasets validate the efficiency and effectiveness of our proposed algorithm.\nKeywords\u2014extreme learning machines; multi-label\nclassification; on-line learning\nI. INTRODUCTION\nIn the modern context of machine intelligence, the growing importance of classification has motivated the development of several algorithms which are scalable [1][2]. A number of tasks right from the identification of objects in images to the study of emotion-associated brainwaves belong to this category. Classification, which is the identification of the target categories a data sample belongs to, can be divided into single-label and multi-label classification. Single-label classification involves binary and multi-class classification where a data sample is associated with one label only. On the contrary, multi-label classification involves data samples which are simultaneously associated with multiple labels. The learning algorithms are generally of two types: Batch learning and Sequential learning. Batch learning requires pre-collection of training data, and the parameters of the network are calculated by processing all the training data concurrently. While, in online/sequential learning algorithms, the network parameters are updated as and when a new training data arrives [3], [4].\nMulti-label classification has become significant due to its rapidly increasing application areas such as text categorization [6]-[8], bioinformatics [9], [10], medical diagnosis [11], scene classification [12], genomics, map labeling [13], marketing , multimedia, music categorization, etc. The rising significance of multi-label classification has spurred a recent growth in its theoretical analysis [14], [15] and development of algorithms for practical applications [16], [17].\nThe existing multi-label classifiers once trained to classify a specific number of labels, cannot learn new labels without retraining all the labels anew again. Such classifiers work well with the pre-known dataset, but they may not be appropriate for applications such as cognitive robotics or real-time applications of big data where the nature of training data is unknown. For such real-time data, the learning technique must be self-adapting to suit the dynamic needs. Class-incremental Extreme Learning Machines (ELM) has been proposed for multi-class classification [18] but there is no significant work done for multi-label classification. To overcome this shortcoming, a novel learning paradigm based on Extreme Learning Machine is proposed, called the \u201cprogressive-ELM multi-label classifier\u201d (Pro-EMLC). This is the first method for incremental learning in multi-label classification. It has been successfully tested on benchmark datasets like Scene, Corel5k, and Medical. The promising results we obtained validate the efficacy of our approach. Next section gives a brief description of ELM and Online Sequential-ELM. Section 3 describes the proposed method. Section 4 presents the results obtained and section 5 states the conclusion.\nII. BRIEF OVERVIEW OF ELM AND OS-ELM\nELM considers a \u2018generalized\u2019 Single Hidden Layer Feedforward Neural Network (SLFN) architecture consisting of n input neurons and m output neurons, with N hidden layer neurons in the second layer. Most neural networks are considered to be universal classifiers or function approximators [19], [20] when all the parameters of the neural network are tuned. It has been previously shown that ELM works for the SLFN architecture without tuning the hidden layer parameters (feature mapping parameters) [21]. We discuss the ELM paradigm in brief in the following paragraphs.\nConsider there are N\u2019 hidden layer neurons, n is the number of features and m is the number of labels . The training data of size N samples is of the form {(xi, yi)|xi \u03f5 R n, yi \u03f5 R m, i = 1,\u2026N}, where xi represents input feature vector and yi represents the output label vector. Label space L = {Y1, Y2,\u2026,YM}. The predicted output of SLFN \u2018oj\u2019 for j = 1,\u2026., N is:\n\u2211 \ud835\udefd\ud835\udc56\ud835\udc54(\ud835\udc98\ud835\udc56 . \ud835\udc99\ud835\udc57 + \ud835\udc4f\ud835\udc56) = \ud835\udc90\ud835\udc57 \ud835\udc41\u2032 \ud835\udc56=1 (1)\nWhere, g(x) is the activation function, wi = [wi1, wi2, \u2026, win] T is the input weight, \u03b2i = [\u03b2i1, \u03b2i2,\u2026\u03b2im] T is the output weight, and bi is the hidden layer bias.\nIn ELM, the input weights and hidden layer bias are assigned randomly. To minimize the difference between the actual and predicted output, \u03b2i should be found such that output class is equal to the target class.\n\u2211 \u2016\ud835\udc90\ud835\udc8b \u2212 \ud835\udc9a\ud835\udc8b \u2016 \ud835\udc41\u2032 \ud835\udc57=1 = 0 (2)\nThus, ELM output network can be written as\n\u2211 \ud835\udefd\ud835\udc56\ud835\udc54(\ud835\udc98\ud835\udc56 . \ud835\udc99\ud835\udc57 + \ud835\udc4f\ud835\udc56) = \ud835\udc9a\ud835\udc57 \ud835\udc41\u2032 \ud835\udc56=1 , j=1,\u2026, N (3)\nIn matrix form,\n\ud835\udc6f\ud835\udefd = \ud835\udc80 (4)\nWhere,\n\ud835\udc6f = [ \ud835\udc54(\ud835\udc981 . \ud835\udc991 + \ud835\udc4f1) \u22ef \ud835\udc54(\ud835\udc98\ud835\udc41\u2032 . \ud835\udc991 + \ud835\udc4f\ud835\udc41\u2032 )\n\u22ee \u22f1 \u22ee \ud835\udc54(\ud835\udc981 . \ud835\udc99\ud835\udc41 + \ud835\udc4f1) \u22ef \ud835\udc54(\ud835\udc98\ud835\udc41\u2032 . \ud835\udc99\ud835\udc41 + \ud835\udc4f\ud835\udc41\u2032 ) ]\n\ud835\udc41\ud835\udc4b\ud835\udc41\u2032\n\ud835\udefd = [ \ud835\udefd1\n\ud835\udc47\n\u22ee \ud835\udefd\ud835\udc41\u2032 \ud835\udc47 ]\n\ud835\udc41\u2032\ud835\udc4b\ud835\udc5a\nand \ud835\udc80 = [ \ud835\udc661\n\ud835\udc47\n\u22ee \ud835\udc66\ud835\udc41 \ud835\udc47 ]\n\ud835\udc41\ud835\udc4b\ud835\udc5a\nUsing the Moore-Penrose generalized inverse ( \ud835\udc3b+ ) of hidden layer matrix H, we can get the output feature mapping matrix \u03b2 of the ELM network.\n\ud835\udefd = \ud835\udc6f+ \ud835\udc80 (5)\nWhere, \ud835\udc6f+ = (\ud835\udc6f\ud835\udc47 \ud835\udc6f)\u22121\ud835\udc6f\ud835\udc47\nIn real time applications, the complete training data is seldom available and as the data arrives sequentially, the ELM has to be retrained with a combination of the new data and the previously available data. There is no provision to retain the learning from the previous data and train the network for only the new data. On-line Sequential Extreme Learning Machine (OS-ELM) retains the knowledge of the previous training data and can learn data over previously available data chunk-bychunk with varying chunk sizes [22]. The above-mentioned algorithm of ELM is extended to OS-ELM as described below.\nLet the initial block of training data have N0 samples. For\nthe initial block, compute\n\ud835\udc740 = (\ud835\udc6f0 \ud835\udc47 \ud835\udc6f0) \u22121 (6)\n\ud835\udefd0 = \ud835\udc740\ud835\udc6f0 \ud835\udc47 \ud835\udc800 (7)\nWhere H0 = [h1 ... hN0] T is the feature mapping (hidden layer) matrix computed as above and Y0 = [y1,\u2026,yN0] T output vector for N0 samples\nFor the incoming (k+1) th sequential data, Recursive Least Squares (RLS) approximation is used to retain the learning. Updating can be done as,\n\ud835\udc74\ud835\udc58+1 = \ud835\udc74\ud835\udc58 \u2212 \ud835\udc74\ud835\udc58\ud835\udc89\ud835\udc58+1 \ud835\udc89\ud835\udc58+1\n\ud835\udc47 \ud835\udc74\ud835\udc58\n1 +\ud835\udc89\ud835\udc58+1 \ud835\udc47 \ud835\udc74\ud835\udc58\ud835\udc89\ud835\udc58+1\n(8)\n\ud835\udefd\ud835\udc58+1 = \ud835\udefd\ud835\udc58 + \ud835\udc74\ud835\udc58+1\ud835\udc89\ud835\udc58+1(\ud835\udc80\ud835\udc58 +1 \ud835\udc47 \u2212 \ud835\udc89\ud835\udc58 +1 \ud835\udc47 \ud835\udefd\ud835\udc58) (9)\nWhere k = 0, 1, 2,.., N-N0-1.\nThe calculated value of \ud835\udefd is used for predicting the output matrix. The detailed theory and mathematics involved in ELM and OS-ELM are given in [22] [23].\nIII. PROPOSED METHOD\nThe steps of the proposed \u2018progress ive-ELM multi-label classifier\u2019 are:\n1) Processing of input: The raw input data is processed so that the output label, corresponding to each input sample, is an m-tuple with -1 or 1 representing the belongingness to each of the labels in the label space L.\n2) Initialization: The input weights and the hidden layer bias are assigned at random in accordance with the ELM learning paradigm. The number of hidden layer neurons N\u2019 is fixed such that over-fitting does not occur. The optimal value N\u2019 is found by carrying out different epochs with varying number of hidden neurons, plotting it w.r.t. the training and cross-validation accuracy and choosing the optimal number of hidden neurons.\n3) ELM training \u2013 Initial learning: The hidden layer output matrix H0 is calculated for an initial block of N0 data samples.\nH0 = [h1,\u2026.hN0] T (10)\nWhere hi=[g(w1.xi+b1),\u2026.g(wN\u2019.xi+bN\u2019)] T, i = 1,2\u2026N0.\nUsing H0, the initial values of M0 and \u03b20 are estimated as\n\ud835\udc740 = (\ud835\udc6f0 \ud835\udc47 \ud835\udc6f0 ) \u22121 (11)\n\ud835\udefd0 = \ud835\udc740\ud835\udc6f0 \ud835\udc47 \ud835\udc800 (12)\n4) ELM training \u2013 Sequential Learning: The subsequent data arriving at the network can be trained one-by-one or chunkby-chunk. Let the chunk size be \u2018b\u2019. For b=1, data is trained oneby-one. The incoming data may have the presence of a new label(s), indicated by the increase in tuple size. Presence/Absence of a new label is handled as below:\na) Absence: In the absence of a new label, no special computations are required and step 5 is executed directly.\nb) Presence: In the presence of a new label, the network is to be recalibrated to accommodate a new label, while retaining the old knowledge. Let \u2018c\u2019 new labels be introduced and m labels of data are currently learnt. Introducing \u2018c\u2019 new labels at any instant k+1 modifies the dimensions of the output weight matrix \ud835\udefd\ud835\udc41\u2032\ud835\udc4b\ud835\udc5a to \ud835\udefd\ud835\udc41\u2032\ud835\udc4b\ud835\udc5a +\ud835\udc50 . Transformed output weight matrix \ud835\udefd\ud835\u0303\udc58 is given as \ud835\udefd\ud835\u0303\udc58 = (\ud835\udefd\ud835\udc58 )\ud835\udc41\u2032\ud835\udc4b\ud835\udc5a \ud835\udc3c\ud835\udc5a\ud835\udc4b\ud835\udc5a+\ud835\udc50 where, \ud835\udc3c\ud835\udc5a\ud835\udc4b\ud835\udc5a+\ud835\udc50 is m X m+c dimensional rectangular identity matrix.\n\ud835\udefd\ud835\u0303\udc58 \ud835\udc41\u2032\ud835\udc4b\ud835\udc5a+\ud835\udc50 = (\ud835\udefd\ud835\udc58 )\ud835\udc41\u2032\ud835\udc4b\ud835\udc5a [ 1 0 \u2026 0 1 \u2026 0 0 0 0 \u2026 \u2026 0 0 0 0 ]\n\ud835\udc5a\ud835\udc4b\ud835\udc5a +\ud835\udc50\n(13)\n\ud835\udefd\ud835\u0303\udc58 \ud835\udc41\u2032\ud835\udc4b\ud835\udc5a+\ud835\udc50 = [(\ud835\udefd\ud835\udc58 )\ud835\udc41\u2032\ud835\udc4b\ud835\udc5a \ud835\udc42\ud835\udc41\u2032\ud835\udc4b\ud835\udc50 ]\ud835\udc41\u2032\ud835\udc4b\ud835\udc5a+\ud835\udc50 (14)\nwhere \ud835\udc42\ud835\udc41\u2032\ud835\udc4b\ud835\udc50 is a zero matrix.\nThe output values corresponding to new labels is -1 for previously learnt samples. Therefore, the k-learning step update for the \u2018c\u2019 new labels ((\u2206\u03b2k)N\u2032Xc(\u2206\ud835\udefd\ud835\udc58)\ud835\udc41\u2032 \ud835\udc4b\ud835\udc50 ) can be expressed as,\n(\u2206\ud835\udefd\ud835\udc58)\ud835\udc41\u2032\ud835\udc4b\ud835\udc50 = (\ud835\udc74\ud835\udc58)\ud835\udc41\u2032\ud835\udc4b\ud835\udc41\u2032 (\ud835\udc89\ud835\udc58 \ud835\udc47 ) \ud835\udc41\u2032\ud835\udc4b\ud835\udc4f [ \u22121 \u22ef \u22121\n\u22ee \u22f1 \u22ee \u22121 \u22ef \u22121\n]\n\ud835\udc4f\ud835\udc4b\ud835\udc50\n(15)\n(\u2206\ud835\udefd\ud835\udc58)\ud835\udc41\u2032\ud835\udc4b\ud835\udc50 = \u2212(\ud835\udc74\ud835\udc58 )\ud835\udc41\u2032\ud835\udc4b\ud835\udc41\u2032 (\ud835\udc89\ud835\udc58 \ud835\udc47 ) \ud835\udc41\u2032\ud835\udc4b\ud835\udc4f \ud835\udc3d\ud835\udc4f\ud835\udc4b\ud835\udc50 (16)\nWhere, \ud835\udc3d\ud835\udc4f\ud835\udc4b\ud835\udc50 is an all-ones matrix of size b x c.\n(\u2206\ud835\udefd\ud835\udc58 )\ud835\udc41\u2032\ud835\udc4b\ud835\udc5a+\ud835\udc50 = [\ud835\udc42\ud835\udc41\u2032\ud835\udc4b\ud835\udc5a \u2212(\ud835\udc74\ud835\udc58 )\ud835\udc41\u2032\ud835\udc4b\ud835\udc41\u2032 (\ud835\udc89\ud835\udc58 \ud835\udc47 ) \ud835\udc41\u2032 \ud835\udc4b\ud835\udc4f \ud835\udc3d\ud835\udc4f\ud835\udc4b\ud835\udc50 ] (17)\nThe recalibrated output weight matrix (\ud835\udefd\ud835\udc58 )\ud835\udc41\u2032\ud835\udc4b(\ud835\udc5a+\ud835\udc50) is calculated as,\n(\ud835\udefd\ud835\udc58)\ud835\udc41\u2032\ud835\udc4b(\ud835\udc5a+\ud835\udc50) = \ud835\udefd\ud835\u0303\udc58\ud835\udc41\u2032\ud835\udc4b\ud835\udc5a +\ud835\udc50 + (\u2206\ud835\udefd\ud835\udc58 )\ud835\udc41\u2032\ud835\udc4b\ud835\udc5a+\ud835\udc50 (18)\nUpon simplification, (\ud835\udefd\ud835\udc58 )\ud835\udc41\u2032\ud835\udc4b(\ud835\udc41\u2032+\ud835\udc50)can be expressed as,\n(\ud835\udefd\ud835\udc58)\ud835\udc41\u2032\ud835\udc4b(\ud835\udc5a+\ud835\udc50) = [ (\ud835\udefd\ud835\udc58 )\ud835\udc41\u2032\ud835\udc4b\ud835\udc5a (\u2206\ud835\udefd\ud835\udc58 )\ud835\udc41\u2032 \ud835\udc4b\ud835\udc50 ] (19)\nThe hidden layer output vector \u210e\ud835\udc58+1 is calculated. The output weight is updated according to the Recursive Least Square\nalgorithm [22], \ud835\udc74\ud835\udc58+1 = \ud835\udc74\ud835\udc58 \u2212 \ud835\udc74\ud835\udc58\ud835\udc89\ud835\udc58 +1 \ud835\udc47 (\ud835\udc3c + \ud835\udc89\ud835\udc58 +1\ud835\udc74\ud835\udc58\ud835\udc89\ud835\udc58+1 \ud835\udc47 )\u22121\ud835\udc89\ud835\udc58+1\ud835\udc74\ud835\udc58 (20)\n\ud835\udefd\ud835\udc58+1 = \ud835\udefd\ud835\udc58 + \ud835\udc74\ud835\udc58+1\ud835\udc89\ud835\udc58+1(\ud835\udc80\ud835\udc58 +1 \ud835\udc47 \u2212 \ud835\udc89\ud835\udc58 +1 \ud835\udc47 \ud835\udefd\ud835\udc58) (21)\n5) ELM testing and thresholding: Raw output matrix Y of\nthe testing data samples is calculated using \ud835\udc4c = \ud835\udc3b\ud835\udefd. Since the\nnumber of labels a samples belongs to is unknown, thresholding\nof raw output is suggested. Thresholding refers to the\napplication of a threshold value, based on the separation\nbetween two categories of labels (Labels that the data sample\nbelongs to and labels the data sample does not belong to), to\nidentify the number of labels and the target class labels\ncorresponding to the input data sample. As a trivial case, zero\nthreshold is chosen. Passing the raw output through bipolar step\nfunction gives the samples\u2019 association to the label(s).\nThus, the proposed algorithm allows for class-incremental\ntraining of multi-label data.\nIV. EXPERIMENTAL RESULTS\nThe proposed method is tested on benchmark datasets, namely Scene, Corel5k and Medical, having a varied number of labels (6 to 374) and belonging to diverse domains like multimedia and text. The degree of \u2018multi-labelness\u2019 of datasets is measured by label density (LD) and label cardinality (LC) as defined by Tsoumakas et al [24]. Label density of the tested datasets ranges from 0.009 to 0.178 and label cardinality ranges from 1.07 to 3.52. The characteristics of these datasets are given\nin Table I. To perform testing for progressive learning, the data samples are redistributed such that the number of labels present in the initial block of data is less than the total number of labels present in the dataset. New labels can be introduced in the streaming data one-by-one or in groups.\nThe learning curve or the hamming loss is plotted for Scene dataset (having 6 labels) in Fig. 1. The hamming loss decreases with increasing number of samples. Till point A, when only five\nlabels are learnt, the overall hamming loss is higher since the prediction for the 6th label is incorrect. After introducing the remaining label (label 6), at point A, in the sequential phase, there is a sharp decrease in hamming loss suggesting that the prediction for label 6 has improved. The hamming loss for the label 6 alone is also shown in Fig. 1. The learning curves for Medical and Corel5k datasets are shown in Fig. 2. For the Medical dataset, 39 out of 45 labels were introduced initially. To verify the behavior towards incremental labels, two labels are introduced at point A, two labels at point B, one label at point C, and finally the last label at point D for the Medical dataset. The falling hamming loss is evidence of the improvement in the\nlearning resulting in better prediction. This pattern of introducing new labels is represented as 39+2+2+1+1 in Table II. Similar protocol was followed for Corel5k dataset with the introduction pattern being 368+2+1+1+2.\nA 10-fold cross-validation is carried out for various combinations of label introduction in Scene (6 labels), Medical (45 labels) and Corel5k (374 labels) data sets. Various measures to evaluate the performance of multi-label classifiers, as explained by Tsoukamas et al [24] are calculated for the proposed algorithm. The results are presented in Table II. Label introduction pattern is the number of labels introduced in the initial phase followed by the number of those in the sequential phase.\nThe proposed algorithm was compared to the state-of-the-art online learning multi-label algorithms mentioned in Table III.\nDespite being an online-class incremental algorithm, the hamming loss of the proposed Progressive - ELM Multi-label Classifier (Pro-EMLC), as shown in Fig. 3, was comparable to the state-of-the-art multi-label classification techniques for batch learning described by Madjarov et al. [25], for all the tested datasets. A similar comparision of training time was also made as given in Table IV. Since the algorithm is based on ELM, the training and testing speed is very high, making it highly suitable for real-time applications of big data analysis.This suggests that the performance of the algorithm is superior for a class incremental algorithm and hence can be used widely.\nV. CONCLUSIONS\nThe proposed progressive-ELM multi-label classifier is the first of its kind. It can be used for online/streaming big data applications with known number of labels as well as for realtime applications such as cognitive robotics where the number of labels is unknown. It has shown high speed and high performance metric for all the 3 tested benchmark datasets. Based on these promising results, the proposed ELM-based classifier can be considered fit for achieving high performance with speed for multi-label classification, especially in incremental learning areas.\nACKNOWLEDGEMENT\nThe authors would like to acknowledge the funding support from the Ministry of Education, Singapore (Tier 1 AcRF, RG30/14).\nREFERENCES\n[1] G. B. Huang, Z. Hongming, D. Xiaojian and Z. Rui , \"Extreme Learning Machine for Regression and Multiclass Classification,\" IEEE Transactions on Systems, Man and Cybernetics , Part B, vol. 42, no. 2, pp. 513-529, 2012\n[2] K. Crammer and C. Gentile, \"Multiclass classification with bandit feedback using adaptive regularization,\" Machine Learning, vol. 90, no. 3, pp. 347-383, 2013.\n[3] M. Pratama, S. G. Anavatti, and J. Lu. Recurrent classifier based on an incremental metacognitive-based scaffolding algorithm. IEEE Transactions on Fuzzy Systems, 23(6):2048-2066, 2015.\n[4] M. Pratama, J. Lu, and G. Zhang. Evolving Type-2 Fuzzy Classifier. IEEE Transactions on Fuzzy Systems, PP(99):1-1, 2015.\n[5] G. Tsoumakas, I. Katakis and I. Vlahavas. Mining Multi-label Data. In O. Maimon and L. Rokach, editors, Data Mining and Knowledge Discovery Handbook, pages 667-685, Springer US, 2010.\n[6] X. Luo and A. N. Zincir-Heywood. Evaluation of Two Systems on Multiclass Multi-label document classification. In Hacid M.S., Murray N.V., Ras Z.W. and Tsumoto S, editors, Foundations of Intelligent Systems, volume 3488 of Lecture Notes in Computer Science, pages 161-169. Springer Berlin Heidelberg, 2005.\n[7] D. Tikk and G Biro. Experiments with multi-label text classifier on the Reuters collection. In Proceedings of the international conference on computational cybernetics (ICCC 03), pages 33-38, 2003.\n[8] K. Yu, S. Yu and V. Tresp. Multi-label informed latent semantic indexing. In Proceedings of the 28th annual international ACM SIGIR conference on Research and Development in information retrieval, SIGIR\u201905, pages 258-265, New York, NY, USA, 2005.\n[9] A. Elisseeff and J. Weston. A kernel method for multi-labelled classification. In Advances in Neural Information Processing Systems, pages 681-687, 2001.\n[10] M. L. Zhang and Z. H. Zhou, A k-nearest neighbour based algorithm for multi-label classification. In 2005 IEEE International Conference on Granular Computing, volume 2, pages 718-721, 2005.\n[11] A. Karalic and V. Pirnat. Significance level based multiple tree classification. Informatica, 15(5):12, 1991.\n[12] M. Boutell, X. Shen, J. Luo and C. Brouwn. Multi-label semantic scene classification. Technical Report, Department of Computer Science, University of Rochester, USA, 2003.\n[13] B. Zhu and C. K. Poon. Efficient Approximation Algorithms for Multilabel Map Labelling. In Algorithms and Computation, volume 1741 of Lecture Notes in Computer Science, pages 143-152. Springer Berlin Heidelberg, 1999.\n[14] K. Dembczy, W. Waegeman, W. Cheng and E. Hullermeier, \"On Label dependence and Loss Minimization in Multilabel Classification,\" Machine Learning, vol. 88, no. 2, pp. 5-45, 2012\n[15] W. Gao and Z.-H. Zhou, \"On the consistency of Multi-label Classification,\" Artificial Intelligence, vol. 199, pp. 22-44, 2013\n[16] J. Petterson and T . Caetano, \"Submodular Multi-label Learning,\" in Advances in Neural Information Processing Systems, Granada, Spain, 2011.\n[17] H.-F. Yu, P. Jain, P. Kar and I. Dhillon, \"Large-scale multi-label learning with missing labels,\" in Proceedings of the 31st International Conference on Machine Learning, Beijing, China, 2014.\n[18] Z. Zhao, Z. Chen, Y. Chen, S. Wang, and H. Wang. A class incremental extreme learning machine for activity recognition. Cognitive Computation, 6(3):423-431, 2014.\n[19] K. Hornik, \"Approximation capabilities of multilayer feedforward networks,\" Neural Network, vol. 4, pp. 251-257, 1991.\n[20] M. Leshno, V. Y. Lin, A. Pinkus and S. Schocken, \"Multilayer feedforward networks with a nonpolynomial activation function can approximate any function,\" Neural Network, vol. 6, no. 6, pp. 861-867, 1993.\n[21] G.-B. Huang, L. Chen and C.-K. Siew, \"Universal approximation using incremental constructive feedforward networks with random hidden nodes,\" IEEE Transactions on Neural Networks, vol. 17, no. 4, pp. 879- 892, 2006.\n[22] N. Y. Liang, G. B. Huang, P. Saratchandran and N. Sundararajan. A Fast and Accurate Online Sequential Learning Algorithm for Feedforward Networks. IEEE Transactions on Neural Networks, 17(6):1411-1423, 2006.\n[23] G. B. Huang, D. H. Wang and Y. Lan. Extreme Learning Machines: A Survey. International Journal of Machine Learning and Cybernetics, 2(2):107-122, 2011.\n[24] G. Tsoumakas and I. Katakis. Multi-label Classification: An Overview, International Journal of Data Warehousing and Mining , 3(3): 1-13, 2007.\n[25] G. Madjarov, D. Kocev, D. Gjorgjevikj and S. Dzeroski. An extensive experimental comparison of methods for multi-label learning. Pattern Recognition, 45(9): 3084-3104, 2012."}], "references": [{"title": "Extreme Learning Machine for Regression and Multiclass Classification", "author": ["G.B. Huang", "Z. Hongming", "D. Xiaojian", "Z. Rui"], "venue": "IEEE Transactions on Systems, Man and Cybernetics , Part B, vol. 42, no. 2, pp. 513-529, 2012", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2012}, {"title": "Multiclass classification with bandit feedback using adaptive regularization", "author": ["K. Crammer", "C. Gentile"], "venue": "Machine Learning, vol. 90, no. 3, pp. 347-383, 2013.", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2013}, {"title": "Recurrent classifier based on an incremental metacognitive-based scaffolding algorithm", "author": ["M. Pratama", "S.G. Anavatti", "J. Lu"], "venue": "IEEE Transactions on Fuzzy Systems,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2015}, {"title": "Evolving Type-2 Fuzzy Classifier", "author": ["M. Pratama", "J. Lu", "G. Zhang"], "venue": "IEEE Transactions on Fuzzy Systems,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2015}, {"title": "Mining Multi-label Data", "author": ["G. Tsoumakas", "I. Katakis", "I. Vlahavas"], "venue": "Data Mining and Knowledge Discovery Handbook,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2010}, {"title": "Evaluation of Two Systems on Multiclass Multi-label document classification", "author": ["X. Luo", "A.N. Zincir-Heywood"], "venue": "Foundations of Intelligent Systems,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2005}, {"title": "Experiments with multi-label text classifier on the Reuters collection", "author": ["D. Tikk", "G Biro"], "venue": "In Proceedings of the international conference on computational cybernetics (ICCC", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2003}, {"title": "Multi-label informed latent semantic indexing", "author": ["K. Yu", "S. Yu", "V. Tresp"], "venue": "In Proceedings of the 28th annual international ACM SIGIR conference on Research and Development in information retrieval,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2005}, {"title": "A kernel method for multi-labelled classification", "author": ["A. Elisseeff", "J. Weston"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2001}, {"title": "A k-nearest neighbour based algorithm for multi-label classification", "author": ["M.L. Zhang", "Z.H. Zhou"], "venue": "IEEE International Conference on Granular Computing,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2005}, {"title": "Significance level based multiple tree classification", "author": ["A. Karalic", "V. Pirnat"], "venue": null, "citeRegEx": "11", "shortCiteRegEx": "11", "year": 1991}, {"title": "Multi-label semantic scene classification", "author": ["M. Boutell", "X. Shen", "J. Luo", "C. Brouwn"], "venue": "Technical Report,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2003}, {"title": "Efficient Approximation Algorithms for Multilabel Map Labelling", "author": ["B. Zhu", "C.K. Poon"], "venue": null, "citeRegEx": "13", "shortCiteRegEx": "13", "year": 1999}, {"title": "On Label dependence and Loss Minimization in Multilabel Classification", "author": ["K. Dembczy", "W. Waegeman", "W. Cheng", "E. Hullermeier"], "venue": "Machine Learning, vol. 88, no. 2, pp. 5-45, 2012", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2012}, {"title": "On the consistency of Multi-label Classification", "author": ["W. Gao", "Z.-H. Zhou"], "venue": "Artificial Intelligence, vol. 199, pp. 22-44, 2013", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2013}, {"title": "Submodular Multi-label Learning", "author": ["J. Petterson", "T . Caetano"], "venue": "Advances in Neural Information Processing Systems, Granada, Spain, 2011.", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2011}, {"title": "Large-scale multi-label learning with missing labels", "author": ["H.-F. Yu", "P. Jain", "P. Kar", "I. Dhillon"], "venue": "Proceedings of the 31st International Conference on Machine Learning, Beijing, China, 2014.", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2014}, {"title": "A class incremental extreme learning machine for activity recognition", "author": ["Z. Zhao", "Z. Chen", "Y. Chen", "S. Wang", "H. Wang"], "venue": "Cognitive Computation,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2014}, {"title": "Approximation capabilities of multilayer feedforward networks", "author": ["K. Hornik"], "venue": "Neural Network, vol. 4, pp. 251-257, 1991.", "citeRegEx": "19", "shortCiteRegEx": null, "year": 1991}, {"title": "Multilayer feedforward networks with a nonpolynomial activation function can approximate any function", "author": ["M. Leshno", "V.Y. Lin", "A. Pinkus", "S. Schocken"], "venue": "Neural Network, vol. 6, no. 6, pp. 861-867, 1993.", "citeRegEx": "20", "shortCiteRegEx": null, "year": 1993}, {"title": "Universal approximation using incremental constructive feedforward networks with random hidden nodes", "author": ["G.-B. Huang", "L. Chen", "C.-K. Siew"], "venue": "IEEE Transactions on Neural Networks, vol. 17, no. 4, pp. 879- 892, 2006.", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2006}, {"title": "A Fast and Accurate Online Sequential Learning Algorithm for Feedforward Networks", "author": ["N.Y. Liang", "G.B. Huang", "P. Saratchandran", "N. Sundararajan"], "venue": "IEEE Transactions on Neural Networks,", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2006}, {"title": "Extreme Learning Machines: A Survey", "author": ["G.B. Huang", "D.H. Wang", "Y. Lan"], "venue": "International Journal of Machine Learning and Cybernetics,", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2011}, {"title": "Multi-label Classification: An Overview, International Journal of Data Warehousing and Mining", "author": ["G. Tsoumakas", "I. Katakis"], "venue": null, "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2007}, {"title": "An extensive experimental comparison of methods for multi-label learning", "author": ["G. Madjarov", "D. Kocev", "D. Gjorgjevikj", "S. Dzeroski"], "venue": "Pattern Recognition,", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2012}], "referenceMentions": [{"referenceID": 0, "context": "INTRODUCTION In the modern context of machine intelligence, the growing importance of classification has motivated the development of several algorithms which are scalable [1][2].", "startOffset": 172, "endOffset": 175}, {"referenceID": 1, "context": "INTRODUCTION In the modern context of machine intelligence, the growing importance of classification has motivated the development of several algorithms which are scalable [1][2].", "startOffset": 175, "endOffset": 178}, {"referenceID": 2, "context": "While, in online/sequential learning algorithms, the network parameters are updated as and when a new training data arrives [3], [4].", "startOffset": 124, "endOffset": 127}, {"referenceID": 3, "context": "While, in online/sequential learning algorithms, the network parameters are updated as and when a new training data arrives [3], [4].", "startOffset": 129, "endOffset": 132}, {"referenceID": 5, "context": "Multi-label classification has become significant due to its rapidly increasing application areas such as text categorization [6]-[8], bioinformatics [9], [10], medical diagnosis [11], scene classification [12], genomics, map labeling [13], marketing , multimedia, music categorization, etc.", "startOffset": 126, "endOffset": 129}, {"referenceID": 7, "context": "Multi-label classification has become significant due to its rapidly increasing application areas such as text categorization [6]-[8], bioinformatics [9], [10], medical diagnosis [11], scene classification [12], genomics, map labeling [13], marketing , multimedia, music categorization, etc.", "startOffset": 130, "endOffset": 133}, {"referenceID": 8, "context": "Multi-label classification has become significant due to its rapidly increasing application areas such as text categorization [6]-[8], bioinformatics [9], [10], medical diagnosis [11], scene classification [12], genomics, map labeling [13], marketing , multimedia, music categorization, etc.", "startOffset": 150, "endOffset": 153}, {"referenceID": 9, "context": "Multi-label classification has become significant due to its rapidly increasing application areas such as text categorization [6]-[8], bioinformatics [9], [10], medical diagnosis [11], scene classification [12], genomics, map labeling [13], marketing , multimedia, music categorization, etc.", "startOffset": 155, "endOffset": 159}, {"referenceID": 10, "context": "Multi-label classification has become significant due to its rapidly increasing application areas such as text categorization [6]-[8], bioinformatics [9], [10], medical diagnosis [11], scene classification [12], genomics, map labeling [13], marketing , multimedia, music categorization, etc.", "startOffset": 179, "endOffset": 183}, {"referenceID": 11, "context": "Multi-label classification has become significant due to its rapidly increasing application areas such as text categorization [6]-[8], bioinformatics [9], [10], medical diagnosis [11], scene classification [12], genomics, map labeling [13], marketing , multimedia, music categorization, etc.", "startOffset": 206, "endOffset": 210}, {"referenceID": 12, "context": "Multi-label classification has become significant due to its rapidly increasing application areas such as text categorization [6]-[8], bioinformatics [9], [10], medical diagnosis [11], scene classification [12], genomics, map labeling [13], marketing , multimedia, music categorization, etc.", "startOffset": 235, "endOffset": 239}, {"referenceID": 13, "context": "The rising significance of multi-label classification has spurred a recent growth in its theoretical analysis [14], [15] and development of algorithms for practical applications [16], [17].", "startOffset": 110, "endOffset": 114}, {"referenceID": 14, "context": "The rising significance of multi-label classification has spurred a recent growth in its theoretical analysis [14], [15] and development of algorithms for practical applications [16], [17].", "startOffset": 116, "endOffset": 120}, {"referenceID": 15, "context": "The rising significance of multi-label classification has spurred a recent growth in its theoretical analysis [14], [15] and development of algorithms for practical applications [16], [17].", "startOffset": 178, "endOffset": 182}, {"referenceID": 16, "context": "The rising significance of multi-label classification has spurred a recent growth in its theoretical analysis [14], [15] and development of algorithms for practical applications [16], [17].", "startOffset": 184, "endOffset": 188}, {"referenceID": 17, "context": "Class-incremental Extreme Learning Machines (ELM) has been proposed for multi-class classification [18] but there is no significant work done for multi-label classification.", "startOffset": 99, "endOffset": 103}, {"referenceID": 18, "context": "Most neural networks are considered to be universal classifiers or function approximators [19], [20] when all the parameters of the neural network are tuned.", "startOffset": 90, "endOffset": 94}, {"referenceID": 19, "context": "Most neural networks are considered to be universal classifiers or function approximators [19], [20] when all the parameters of the neural network are tuned.", "startOffset": 96, "endOffset": 100}, {"referenceID": 20, "context": "It has been previously shown that ELM works for the SLFN architecture without tuning the hidden layer parameters (feature mapping parameters) [21].", "startOffset": 142, "endOffset": 146}, {"referenceID": 21, "context": "On-line Sequential Extreme Learning Machine (OS-ELM) retains the knowledge of the previous training data and can learn data over previously available data chunk-bychunk with varying chunk sizes [22].", "startOffset": 194, "endOffset": 198}, {"referenceID": 21, "context": "The detailed theory and mathematics involved in ELM and OS-ELM are given in [22] [23].", "startOffset": 76, "endOffset": 80}, {"referenceID": 22, "context": "The detailed theory and mathematics involved in ELM and OS-ELM are given in [22] [23].", "startOffset": 81, "endOffset": 85}, {"referenceID": 21, "context": "The output weight is updated according to the Recursive Least Square algorithm [22], Mk+1 = Mk \u2212 Mkhk +1 T (I + hk +1Mkhk+1 T )hk+1Mk (20) \u03b2k+1 = \u03b2k + Mk+1hk+1(Yk +1 T \u2212 hk +1 T \u03b2k) (21)", "startOffset": 79, "endOffset": 83}, {"referenceID": 23, "context": "The degree of \u2018multi-labelness\u2019 of datasets is measured by label density (LD) and label cardinality (LC) as defined by Tsoumakas et al [24].", "startOffset": 135, "endOffset": 139}, {"referenceID": 23, "context": "Various measures to evaluate the performance of multi-label classifiers, as explained by Tsoukamas et al [24] are calculated for the proposed algorithm.", "startOffset": 105, "endOffset": 109}, {"referenceID": 24, "context": "[25], for all the tested datasets.", "startOffset": 0, "endOffset": 4}], "year": 2016, "abstractText": "In this paper, a progressive learning algorithm for multi-label classification to learn new labels while retaining the knowledge of previous labels is designed. New output neurons corresponding to new labels are added and the neural network connections and parameters are automatically restructured as if the label has been introduced from the beginning. This work is the first of the kind in multi-label classifier for class-incremental learning. It is useful for real-world applications in applied fields such as robotics where streaming data are available and the number of labels is often unknown. Based on the Extreme Learning Machine framework, a novel universal classifier with plug and play capabilities for progressive multi-label classification is developed. Experimental results on various benchmark synthetic and real datasets validate the efficiency and effectiveness of our proposed algorithm. Keywords\u2014extreme learning machines; multi-label classification; on-line learning", "creator": "Microsoft\u00ae Word 2016"}}}