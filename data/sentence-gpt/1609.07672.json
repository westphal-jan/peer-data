{"id": "1609.07672", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "24-Sep-2016", "title": "Information-Theoretic Methods for Planning and Learning in Partially Observable Markov Decision Processes", "abstract": "Bounded agents are limited by intrinsic constraints on their ability to process information that is available in their sensors and memory and choose actions and memory updates. In this dissertation, we model these constraints as information-rate constraints on communication channels connecting these various internal components of the agent. We demonstrate how these constraints enable us to improve on the complexity of their actions and perform actions, thereby avoiding the limitations of our current model.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n", "histories": [["v1", "Sat, 24 Sep 2016 20:45:37 GMT  (9274kb,D)", "http://arxiv.org/abs/1609.07672v1", "PhD thesis, Hebrew University of Jerusalem, 9/2016"], ["v2", "Thu, 30 Mar 2017 04:57:49 GMT  (9077kb,D)", "http://arxiv.org/abs/1609.07672v2", "PhD thesis, Hebrew University of Jerusalem, 9/2016"]], "COMMENTS": "PhD thesis, Hebrew University of Jerusalem, 9/2016", "reviews": [], "SUBJECTS": "cs.LG", "authors": ["roy fox"], "accepted": false, "id": "1609.07672"}, "pdf": {"name": "1609.07672.pdf", "metadata": {"source": "CRF", "title": "Information-Theoretic Methods for Planning and Learning in Partially Observable Markov Decision Processes", "authors": ["Naftali Tishby"], "emails": [], "sections": [{"heading": null, "text": "Information-Theoretic Methods for Planning and Learning in Partially\nObservable Markov Decision Processes\nThesis submitted for the degree of \u201cDoctor of Philosophy\u201d\nby Roy Fox\nSubmitted to the Senate of the Hebrew University of Jerusalem 9/2016\nar X\niv :1\n60 9.\n07 67\n2v 1\n[ cs\n.L G\n] 2\n4 Se\np 20\n16\nThis work was carried out under the supervision of Professor Naftali Tishby\nAcknowledgements\nFirst and foremost, I would like to express my deep gratitude to my advisor, Prof. Naftali Tishby, for instilling in me a passion for science, for sharing with me his tremendous insight, for empowering me to ask the important questions, and for supporting me in my first steps as an independent researcher.\nDuring my exchange program at Columbia University, I was hosted by Prof. Larry Abbott and Prof. Liam Paninski, and I greatly appreciate their kind help and generous support.\nMany thanks to my collaborators, Ari Pakman, Josh Merel, Prof. Liam Paninski and Prof. Tony Jebara, for our pleasant, professional and productive discussions.\nI am privileged to have had so many fascinating discussions and exchanges of truly exciting ideas with my cherished friends and colleagues, Prof. Daniel Polani, Noga Zaslavsky, Pedro Ortega, David Pfau, Ari Pakman, Stas Tiomkin, Nori Jacoby, Michal Moshkovitz, Nadav Amir and Hadar Levi.\nI would like to thank my advisory committee members, Prof. Amir Globerson and Prof. Shie Mannor, for their advice and guidance.\nI am forever grateful to my beloved parents and brother, Miri, Amit and Ken, for their continuing support in all my endeavors.\nLastly, my very special thanks belong to my dear partner, Noga Zaslavsky, whose ideas, insight, advice, collaboration and support have been making this journey possible and worthwhile.\nAbstract\nWe model the interaction of an intelligent agent with its environment as a Partially Observable Markov Decision Process (POMDP), where the joint dynamics of the internal state of the agent and the external state of the world are subject to extrinsic and intrinsic constraints. Extrinsic constraints of partial observability and partial controllability specify how the agent\u2019s input observation depends on the world state and how the latter depends on the agent\u2019s output action. The agent also incurs an extrinsic cost, based on the world states reached and the actions taken in them.\nBounded agents are also limited by intrinsic constraints on their ability to process information that is available in their sensors and memory and choose actions and memory updates. In this dissertation, we model these constraints as information-rate constraints on communication channels connecting these various internal components of the agent.\nThe simplest is to first consider reactive (memoryless) agents, with a channel connecting their sensors to their actuators. The problem of optimizing such an agent, under a constraint on the information rate between the input and the output, is a sequential rate-distortion problem. The marginal distribution of the observation can be computed by a forward inference process, whereas the expected cost-to-go of an action can be computed by a backward control process. Given this source distribution and this effective distortion, respectively, each step can be optimized by solving a rate-distortion problem that trades off the extrinsic cost with the intrinsic information rate.\nRetentive (memory-utilizing) agents can be reduced to reactive agents by interpreting the state of the memory component as part of the external world state. The memory reader can then be thought of as another sensor and the memory writer as another actuator and they are limited by the same informational constraint between inputs and outputs.\nIn this dissertation we make four major contributions detailed below and many smaller contributions detailed in each section.\nFirst, we formulate the problem of optimizing the agent under both extrinsic and intrinsic constraints and develop the main tools for solving it. This optimization problem is highly non-convex, with many local optima. Its difficulty is mostly due to the coupling of the forward inference process and the backward control process. The inference policy and the control policy can be optimal given each other but still jointly suboptimal as a pair. For example, if some information is not attended to it cannot be used and if it is not used it should optimally not be attended to.\nSecond, we identify another reason for the challenging convergence properties of the optimization algorithm, which is the bifurcation structure of the update operator near phase transitions. We show that the update operator may undergo period doubling, after which the optimal policy is periodic and the optimal stationary policy is unstable. Any algorithm for planning in such domains must therefore allow for periodic policies, which may themselves be subject to an informational constraint on the clock signal.\nThird, we study the special case of linear-Gaussian dynamics and quadratic cost (LQG), where the optimal solution has a particularly simple and solvable form. Under informational constraints, the forward and the backward processes are not separable. However, we show that they do have a more explicitly solvable structure; namely, a sequential semidefinite program. This also allows us to analyze the structure of the retentive solution under the reduction to the reactive setting.\nFourth, we explore the learning task, where the model of the world dy-\n2\nnamics is unknown and sample-based updates are used instead. We focus on fully observable domains and measure the informational cost with the KL divergence, so that the problem can be solved with a backward-only algorithm. We suggest a schedule for the tradeoff coefficient, such that more emphasis is put on reducing the extrinsic cost and less on the simplicity of the solution, as uncertainty is gradually removed from the value function through learning. This leads to improved performance over existing reinforcement learning algorithms.\n3\nLetter of Contribution\nSection 4.1 of the thesis, \u201cTaming the Noise in Reinforcement Learning via Soft Updates\u201d, is a paper authored by Roy Fox, Ari Pakman and Naftali Tishby, and published in the Proceedings of the 32nd Conference on Uncertainty in Artificial Intelligence (UAI), 2016. RF and AP made an equal and major contribution to this work. RF contributed the original idea for the approach developed in this paper. RF and AP jointly developed the approach to its final published form. RF and AP collaborated on running the simulations needed for the development of the approach, with RF running the majority of simulations, as well as the ones included in the final submission."}, {"heading": "RF and AP jointly wrote the manuscript, with RF taking the lead on some parts, and making an equal contribution to other parts. All other sections of the thesis were authored by Roy Fox jointly with", "text": "advisor Naftali Tishby."}, {"heading": "Contents", "text": ""}, {"heading": "1 Introduction 6", "text": "1.1 Partially Observable Markov Decision Processes . . . . . . . . 6\n1.1.1 Setting . . . . . . . . . . . . . . . . . . . . . . . . . . . 7 1.1.2 Structure . . . . . . . . . . . . . . . . . . . . . . . . . 12 1.1.3 Methods . . . . . . . . . . . . . . . . . . . . . . . . . . 15 1.1.4 Challenges . . . . . . . . . . . . . . . . . . . . . . . . . 20\n1.2 Information Theory . . . . . . . . . . . . . . . . . . . . . . . . 23 1.2.1 Rate-Distortion Theory . . . . . . . . . . . . . . . . . . 23 1.2.2 Sequential Rate-Distortion . . . . . . . . . . . . . . . . 26\n1.3 Organization of the Dissertation . . . . . . . . . . . . . . . . . 30"}, {"heading": "2 Minimum-Information POMDP Planning 32", "text": "2.1 Bounded Planning in Passive POMDPs . . . . . . . . . . . . . 32 2.2 Optimal Selective Attention in Reactive Agents . . . . . . . . 41"}, {"heading": "3 Minimum-Information LQG Control 53", "text": "3.1 Part I: Memoryless Controllers . . . . . . . . . . . . . . . . . . 53 3.2 Part II: Retentive Controllers . . . . . . . . . . . . . . . . . . 61 3.3 Supplementary Material . . . . . . . . . . . . . . . . . . . . . 69"}, {"heading": "4 Minimum-KL Reinforcement Learning 75", "text": "4.1 Taming the Noise in Reinforcement Learning via Soft Updates 75"}, {"heading": "5 Discussion 86", "text": "Glossary 91\n5\nChapter 1"}, {"heading": "Introduction", "text": "In this chapter we introduce the conceptual framework that is the basis for the results presented in this thesis. Although reinforcement learning and information theory have both been studied intensively for many decades, some aspects of our approach to these fields are novel. The preliminaries are included here in somewhat non-standard notation and are accompanied by several new organizing principles and insights."}, {"heading": "1.1 Partially Observable Markov Decision", "text": "Processes\nA Partially Observable Markov Decision Process (POMDP) is a dynamical system, with outputs that partially reveal the state of the system, and inputs that partially control the state dynamics. This richly expressive model has numerous and diverse applications, from autonomous vehicles to ad displays [1]. POMDPs, and particularly the reinforcement learning paradigm for optimizing and learning them, have therefore enjoyed increasing attention from the research community in recent years.\n6"}, {"heading": "1.1.1 Setting", "text": ""}, {"heading": "Dynamics", "text": "A discrete-time dynamical system has a time-dependent state, and possibly stochastic dynamics that determine the distribution of each next state given each current state. A closed system has no input, and the dynamics are simply a Markov chain of states tstu, induced by the conditional probability distribution ppst`1|stq of the state transition. In open systems, which we consider in this thesis, an input control signal at P A, also called an action, can affect the dynamics of the state wt P W , which are now given by the distribution ppwt`1|wt, atq.\nThe system also emits an output signal ot P O, also called an observation, based on its state. The observation dynamics are given by the distribution pot|wtq. In the special case of a fully-observable Markov Decision Process, the observation space contains the state space, and pot|wtq \u201c ot\u201cwt .\nThe control signal is generated by an agent, based on past observations, according to some policy. A history-based policy is given by the distribution \u21e1pat|o\u00a7tq, where o\u00a7t denotes the observable history; i.e., the sequence of observations up to time t. Jointly with its environment, also called the world, the agent forms a larger dynamical system, which induces a stochastic process over twt, ot, atu.\nGeneric history-based policies are hard to optimize, implement, and even represent, since the space of observable histories grows exponentially in size with the length of the history. Instead, the agent is equipped with some memory mt P M, which summarizes the observable history, and on which the future actions are based. In addition to the control policy \u21e1pat|mtq, the agent now consists of an inference policy qpmt`1|mt, ot`1q, for updating the memory state using the new observation. This induces a stochastic process over twt, ot, mt, atu.\n7"}, {"heading": "Extrinsic Constraints", "text": "The world dynamics can be considered extrinsic limitations on how the agent can interact with the world state. Without these limitations, the agent could precisely observe the current state wt of the world, and completely determine its next state wt`1. In POMDPs, the observability is partial, in that the only information about wt that the agent can use is that given by o\u00a7t. Dually, controllability is also partial, in that the only future trajectory of states that the agent can effect are those induced by a\u2022t. Put another way, the state dynamics p limits how the agent can control the world\nP\u21e1pwt`1|wt, mtq \u201c Eat\u201e\u21e1p\u00a8|mtqrppwt`1|wt, atqs,\nso that the distribution of wt`1, given wt and mt, is a selected mixture \u21e1 of the fixed distributions p. Similarly, the observation dynamics limits how the memory state can adapt to the new world state\nPqpmt`1|mt, wt`1q \u201c Eot`1\u201e p\u00a8|wt`1qrqpmt`1|mt, ot`1qs,\nso that the distribution of mt`1, given mt and wt`1, is a fixed mixture of the selected distributions q.\nAnother limitation on the policy of the agent, which is often viewed as the target of the policy optimization, is to achieve low values of the expectation of some cost (or equivalently, high values of an expected reward). Without loss of generality, the cost is taken to be a function cpwt, atq of the world state and the action. On a long timescale, expected cost accumulates at a linear rate, and we are concerned with that asymptotic rate\nV\u21e1,q \u201c lim sup T \u00d18\n1\nT\nT \u00b41\u00ff t\u201c0 Ercpwt, atqs. (1)\n8"}, {"heading": "Stationary Processes", "text": "Let\nP\u21e1,qpwt`1, mt`1|wt, mtq \u201c P\u21e1pwt`1|wt, mtqPqpmt`1|mt, wt`1q.\nFor a marginal distribution p\u0304pstq over the joint state st \u201c pwt, mtq of the world and the agent, the forward operator\nP\u21e1,q : p\u0304 fi\u00d1 Est\u201ep\u0304rP\u21e1,qp\u00a8|stqs,\ninduces a Markov process on the joint state. The limit (1) is clearly related to fixed points of P\u21e1,q, called stationary distributions of the process: if at any point the process reaches a stationary distribution p\u0304, it remains in that distribution, and\nV\u21e1,q \u201c Epwt,mtq\u201ep\u0304 at\u201e\u21e1p\u00a8|mtq rcpwt, atqs. (2)\nHowever, the process does not always have a stationary distribution, and when it does it may not be unique, with the one actually reached depending on the initial distribution of s0 \u201c pw0, m0q.\nTo formulate the conditions under which there exists a unique stationary distribution, we require some results from ergodicity theory, an extensively researched filed which we address here only in a nutshell.\nWe say that the joint state s communicates with s1, and denote s \u00d1\u21e1,q s1, if s1 is reached from s with positive probability after some finite time tps, s1q\nP t\u21e1,qr ssps1q \u201c P\u21e1,qpst \u201c s1|s0 \u201c sq \u00b0 0.\nConsider the equivalence classes of the equivalence relation s \u00d8\u21e1,q s1 (i.e. s and s1 communicate with each other), and the partial order \u00d1\u21e1,q induced on the set of these communicating classes.\n9\nWe say that a communicating class is closed if the probability of leaving it is 0. A closed communicating class has a unique stationary distribution over its member states. Even if the process has period T , and the marginal joint distributions follow a limit cycle p\u03040, . . . , p\u0304T \u00b41, the limit average (1) is the expected cost (2), with respect to the stationary distribution p\u0304 \u201c 1\nT \u221eT \u00b41 t\u201c0 p\u0304t.\nIf the process has multiple closed communicating classes, the convex combination of their stationary distributions is also stationary, and thus the value of the policy depends on the initial state distribution. On the other hand, the process may have no closed communicating classes, or more generally the total probability of reaching any closed communicating class from the initial state may be less than 1. If a closed communicating class is not reached, the process goes through an infinite sequence of distinct communicating classes, which of course excludes stationarity. This is only possible when the state space is infinite, in the case where W or M is infinite.\nIn reinforcement learning, it is common to assume ergodic processes, consisting in particular of a single closed communicating class. However, it should be recalled that the process is induced jointly by the world dynamics and the agent policy, and some policies are not ergodic. Some of the complications this creates are explored in Section 2.2; however, further implications are beyond the scope of this dissertation. Here we restrict the discussion to policies that are well-behaved, in that the process they induce reaches a single closed communicating class with probability 1."}, {"heading": "Finite-Horizon Processes", "text": "In many reinforcement learning domains the process terminates upon reaching a terminating state. A terminating state can be modeled as persisting with probability 1 and cost 0, making it a closed communicating class, and the value of any well-behaved policy 0. For the comparison of policies in this episodic setting to be meaningful, we consider the total life-long expected\n10\ncost\nV\u21e1,q \u201c 8\u00ff\nt\u201c0 Ercpwt, atqs,\nrather than the average cost. A special case of this finite-horizon setting is the popular discounted set-\nting, although it is often mistakenly considered to have an infinite horizon [2]. In this setting, each transition has a fixed probability 0 \u2020 1\u00b4 \u00a7 1 of terminating, regardless of the current state or action. The horizon Tf is distributed geometrically with parameter 1 \u00b4 , and we have\nV\u21e1,q \u201c 8\u00ff\nT \u201c1 PpTf \u201c T q\nTf \u00b41\u00ff\nt\u201c0 Ercpwt, atqs\n\u201c 8\u00ff\nT \u201c1 p1 \u00b4 q T \u00b41\nT \u00b41\u00ff t\u201c0 Ercpwt, atqs\n\u201c 8\u00ff\nt\u201c0 Ercpwt, atq|Tf \u00b0 ts\n8\u00ff\nT \u201ct`1 p1 \u00b4 q T \u00b41\n\u201c 8\u00ff\nt\u201c0 tErcpwt, atq|Tf \u00b0 ts.\nIt may appear in this expression that the horizon is infinite and the costs are discounted exponentially by . However the contribution of later time steps to the total cost becomes negligible on the effective horizon, which is in the order of the expected termination time 1\n1\u00b4 . Another special case is the fixed finite horizon T\nV\u21e1,q \u201c T \u00b41\u00ff\nt\u201c0 Ercpwt, atqs.\nThis can be modeled by keeping track of the time index as a part of the state w1t \u201c pt, wtq, and terminating when t \u201c T . Policies in this setting are often\n11\nwt\u00b41\nat\u00b42\nmt\u00b42\not\u00b41\nmt\u00b41\nat\u00b41\nwt\not\nmt\nat\nwt`1"}, {"heading": "Symmetries", "text": ""}, {"heading": "1.1.2 Structure", "text": "the other; that is, actions are mapped to observations and vice versa. This symmetry underlines the fundamental distinction between the agent and the world: the dynamics p and of one remain fixed, as those of the other, q and \u21e1, are optimized. This suggests that in a closed system, the component whose dynamics is adaptive on shorter timescales can be thought of as an agent with respect to the rest of the system.\nSecond, there is a vertical (left/right) symmetry between the past and the future, that again maps the inputs to the outputs and vice versa. This symmetry underlines the role of causality in the process: while the outputs of wt, namely ot and wt`1, are independent given the state st \u201c pwt, mtq\not K wt`1 | wt, mt,\nthe inputs of wt`1, namely wt and at, may be dependent given s1t \u201c pmt, wt`1q\nat M wt | wt`1, mt.\nTo illustrate this, consider a light switch w that is either on or off. A noisy observation o of the state of the switch only affects the future through its perception by an agent m. On the other hand, given the next switch state w1, a change w \u2030 w1 is much more likely if the agent touches the switch than if it does not, so the actual action a carries information on w even beyond the mere intention m. Other similar causal asymmetries exist as well.\nNevertheless, this imperfect symmetry gives rise to an important duality between inference and control [4]. Inference is inherently a forward process, that is performed by computing the forward dynamics of the process while marginalizing and conditioning probabilities. Control is inherently a backward process, where earlier and earlier actions are selected based on their previously-computed future consequences. The two processes and the interchange between them are central in reinforcement learning, and the duality between them is thus highly insightful.\n13\nA third symmetry in the infinite-horizon setting is time shifting, which is the basis for the stationary analysis of this setting."}, {"heading": "Reactive Agents", "text": "An interesting restriction of the solution space is to only consider reactive (memoryless) agents. That is, when observing ot and deciding on an action at, we disallow any access to previously inferred statistics of the observable history, and restrict the policy to be of the form \u21e1pat|otq.\nRequiring the agent policy to be reactive may have a considerable impact on its optimal value, since the optimal reactive policy can be arbitrarily worse than the optimal retentive (memory-utilizing) policy. Nevertheless, there are many reasons to consider reactive policies. First, in many real cases there are optimal or near-optimal reactive policies. Fully-observable MDPs are one important class of such cases, but others exist as well.\nSecond, even when reactive policies are not near-optimal, they may be preferred for the simplicity of optimizing and implementing them. For example, reactive policies were recently successfully employed to play Atari games, with only the 4 most recent screen frames available as observation in each step [5]. Third, a good treatment of reactive policies can be a springboard for general policies.\nLast, but not least, in a certain sense, no generality is lost by restricting attention to reactive policies, because the general case of planning with retentive policies can be reduced to the problem of planning with reactive policies in an extended POMDP.\nThe extended POMDP is defined over the joint world-agent state space W\u0303 \u201c M \u02c6 W . The observation space is O\u0303 \u201c M \u02c6 O, and similarly the action space is A\u0303 \u201c M \u02c6 A. The extended state dynamics are\np\u0303ppmt, wt`1q|pmt\u00b41, wtq, pm1t, atqq \u201c mt\u201cm1tppwt`1|wt, atq,\n14\nthe observation dynamics are\n\u0303ppm1t\u00b41, otq|pmt\u00b41, wtqq \u201c m1t\u00b41\u201cmt\u00b41 pot|wtq,\nand the cost is\nc\u0303ppmt\u00b41, wtq, pmt, atqq \u201c cpwt, atq.\nThat is, the memory component of the world state is fully observable, fully controllable, and does not affect the cost.\nTo complete the reduction, we need to translate the solution reactive policy in the extended POMDP, \u21e1\u0303ppmt, atq|pmt\u00b41, otqq, back into a retentive policy in the original POMDP. This policy does not generally have the property that at is independent of pmt\u00b41, otq given mt, as it should according to our original notation, but this condition can usually be relaxed without practical implications (see Section 3.2). Alternatively, the retentive policy can have memory space M \u02c6 A, and\nqppmt, atq|pmt\u00b41, at\u00b41q, otq \u201c \u21e1\u0303ppmt, atq|pmt\u00b41, otqq \u21e1pa1t|pmt, atqq \u201c a1t\u201cat ."}, {"heading": "1.1.3 Methods", "text": ""}, {"heading": "Optimization Problem", "text": "We are interested in optimizing the value of the policy p\u21e1, qq\nV\u21e1,q \u201c lim sup T \u00d18\n1\nT\nT \u00b41\u00ff t\u201c0 Ercpwt, atqs \u201c Epwt,mtq\u201ep\u0304 at\u201e\u21e1p\u00a8|mtq rcpwt, atqs\n15\nunder a constraint on the observability and controllability allowed by the world dynamics pp, q, so that p\u0304 is a fixed point of the forward recursion\np\u0304pwt`1, mt`1q \u201c E pwt,mtq\u201ep\u0304 at\u201e\u21e1p\u00a8|mtq\not`1\u201e p\u00a8|wt`1q\nrppwt`1|wt, atqqpmt`1|mt, ot`1qs. (3)\nWe can formulate the optimization target as the Lagrangian\nLp\u0304,\u21e1,q,\u232b \u201c Epwt,mtq\u201ep\u0304 at\u201e\u21e1p\u00a8|mtq rcpwt, atqs\n` \u232b \u00a8 pE pwt,mtq\u201ep\u0304 at\u201e\u21e1p\u00a8|mtq\not`1\u201e p\u00a8|wt`1q\nrpp\u00a8|wt, atq b qp\u00a8|mt, ot`1qs \u00b4 p\u0304q\n\u201c Epwt,mtq\u201ep\u0304 \u201e Eat\u201e\u21e1p\u00a8|mtq \u201d cpwt, atq\n` E wt`1\u201epp\u00a8|wt,atq ot`1\u201e p\u00a8|wt`1q\nmt`1\u201eqp\u00a8|mt,ot`1q\nr\u232bpwt`1, mt`1qs \u0131 \u00b4 \u232bpwt, mtq \u21e2 ,\nwhere \u232bpwt, mtq is the Lagrange multiplier that corresponds to the constraint of the forward recursion (3). In the infinite-horizon setting, we also add the constraint that p\u0304 is a normalized probability distribution, Ep\u0304r1s \u201c 1, with multiplier .\nAlthough this optimization problem is highly non-convex, and many local optima exist, we chose a parameterization under which the Lagrangian is linear separately in each parameter. This enables us to easily find the gradient with respect to each parameter, and completely optimize over it with the other parameters fixed."}, {"heading": "Backward Operator", "text": "The gradient with respect to p\u0304 is\nBp\u0304pwt,mtq Lp\u0304,\u21e1,q,\u232b \u201c Ercpwt, atq ` \u232bpwt`1, mt`1q|wt, mts \u00b4 \u232bpwt, mtq \u00b4 .\n16\nA necessary condition for the solution to be optimal is that the gradient be 0. By taking an expectation on both sides with respect to a stationary distribution p\u0304pwt, mtq, must be the target expected cost Ercpwt, atqs, and\n\u232bpwt, mtq \u201c E at\u201e\u21e1p\u00a8|mtq wt`1\u201epp\u00a8|wt,atq ot`1\u201e p\u00a8|wt`1q\nmt`1\u201eqp\u00a8|mt,ot`1q\nrcpwt, atq ` \u232bpwt`1, mt`1qs \u00b4 . (4)\nThis is a backward recursion for the value of being in joint state pwt, mtq. It computes the cost-to-go for trajectories starting at that state, by accumulating backwards the cost, in a dynamic-programming scheme.\nIn the finite-horizon setting, we exclude the terminating state from our notation, and thus the probabilities are not normalized, since they exclude the probability of termination. The constraint that p\u0304 is normalized no longer holds, and we get \u201c 0, which gives the ordinary Bellman equation [2]."}, {"heading": "Optimal Policy", "text": "As for the agent policy, we have that optimally \u21e1 deterministically selects the action that optimizes the future value\na\u02dat pmtq \u201c arg min at Ercpwt, atq ` \u232bpwt`1, mt`1q|mts (5)\nThe expectation is with respect to the posterior distribution given the agent\u2019s memory state\nPp\u0304pwt|mtq \u201c p\u0304pwt, mtq Epw1t,m1tq\u201ep\u0304r m1t\u201cmts .\nThis is the objective belief that the agent should have about what the state of the world wt may be, when the agent itself is in state mt. To be able to perform this computation, the agent must be able to interpret mt as representing this belief. An inference policy that induces a subjective belief bmtpwtq which\n17\nequals the objective belief Pp\u0304pwt|mtq, is called objectively consistent. Similarly, the optimal q deterministically selects the next memory state\nthat optimizes the future value starting at the inference half-step\nm\u02dat`1pmt, ot`1q \u201c arg min mt`1 Er\u232bpwt`1, mt`1q|mt, ot`1s. (6)\nHere the expectation is with respect to the updated posterior\nPp\u0304,\u21e1pwt`1|mt, ot`1q \u201c p\u0304 1pmt, wt`1q pot`1|wt`1q\nEpm1t,w1t`1q\u201ep\u03041r m1t\u201cmt pot`1|w1t`1qs ,\nwhere p\u03041 is the half-step phased stationary distribution\np\u03041pmt, wt`1q \u201c Epw1t,m1tq\u201ep\u0304 a1t\u201e\u21e1p\u00a8|m1tq r m1t\u201cmtppwt`1|w1t, a1tqs.\nIf the memory state space is unlimited, inference is optimized by choosing a state mt`1 that represents this updated posterior; i.e., having\nbmt`1pwt`1q \u201c Pp\u0304,\u21e1pwt`1|mt, ot`1q.\nThis is the Bayesian inference policy that is the standard in reinforcement learning [6]. It is deterministic and objectively consistent.\nIn summary, an analysis of the optimization problem gives us a forward recursion (3) on the marginal distributions, a backward recursion (4) on the cost-to-go function, and policy optimization equations (5), (6). These can be treated as update equations that allow the iterative update of each solution parameter given the others. Since each update brings a parameter to its optimum, the solution is monotonically improved in each iteration, and is guaranteed to converge in value, at least to a local optimum. This type of forward-backward algorithm is a central element in reinforcement learning and in this thesis.\n18"}, {"heading": "Model-Based vs. Sample-Based Learning", "text": "Our approach so far has been model-based, in that the world dynamics pp, q are needed to compute the forward (3) and backward (4) updates, as well as the inference policy (6). Indeed, general POMDPs are usually solved using model-based methods [7] [8] [9].\nWhen the model is known at the time of agent design, the task of optimizing its policy is called planning. When the model is unknown, the task is called learning, since the agent must learn something about the world before good behavior can be identified. The agent cannot simply exploit its partial knowledge of the world dynamics before it sufficiently explores unknown aspects of the world, because the unknown aspects could allow it to choose a much better policy. The agent must therefore trade off exploration and exploitation [2].\nHowever, the distinction is blurred in model-based methods, because at least in principle, the unknown dynamical parameters of the world can be considered part of the unknown state of the world. Exploration in this sense is not unlike active perception [10], where the actions are selected also for their benefit in better observing the world state.\nLearning and exploration are more pertinent to sample-based methods, where no model of the world is known or learned. Instead, the agent learns only the parameters representing the value function, the policy, or both. This is usually done in the fully observable setting, where no inference or forward computation is needed. In the backward recursion (4), instead of the expectation over unknown distributions, various sampling techniques are used [2]. This approach is employed in Section 4.1.\nValue Iteration vs. Gradient Methods\nThe method presented above utilizes a specific parameterization of the problem that makes the target linear separately in each parameter. This enables the global optimization of each parameter given the others, which is then\n19\niterated until convergence, an approach called value iteration [2] [11]. Gradient methods update the parameters in a different manner [12] [13].\nWhereas value iteration methods follow each coordinate of the gradient to convergence before moving on to another coordinate, gradient methods only take a small step in the direction of the gradient in each iteration. This makes these methods suitable for combining with the plethora of gradientbased parametric function learning methods developed in recent years in the optimization literature. Compared to value iteration methods, gradient methods follow a different trajectory in solution space, with implications for convergence that are a subject of ongoing research.\nIt is also possible to mix and match the approaches. For example, policy gradient methods often take small gradient steps with respect to the policy, but keep the forward-backward equations consistent."}, {"heading": "1.1.4 Challenges", "text": "The POMDP planning problem is provably computationally hard [14], and the optimization problem is highly non-convex. Nevertheless, the problem is important enough to merit the attention it has been getting. Inspired by the fact that natural agents do regularly solve instances of the problem, the research community has come up with useful insights and increasingly effective approaches for solving it, but has also faced significant challenges."}, {"heading": "Memory Space Identification", "text": "Central to these challenges is the identification of a good space of memory states. This is essentially a representation learning problem. Naively, since memory states represent belief states, we may be tempted to consider the entire space of distributions over world states. Unfortunately, this space is continuous, and its discretization requires a state space exponential in the number of world states, an explosion called the curse of dimensionality. To\n20\nput this in precise terms, the number of p|W |\u00b41q-dimensional simplexes with edge length \u270f needed to tile the simplex representing the distributions over W , is p?2{\u270fq|W|\u00b41.\nNot all Bayesian beliefs are reachable in a given POMDP. The Bayesian inference policy is deterministic, making the belief state a function of the observable history. The number of reachable beliefs is therefore bounded by the number of different histories, |O|T , which is unfortunately exponential in the horizon T , an explosion called the curse of history.\nWith a memory state space this large, it may not even be clear how to represent the solution policy, let alone compute a good one. There is therefore a crucial need to reduce the size of the memory space, and the number of representable beliefs. The set of all reachable beliefs needs to be clustered, perhaps implicitly, into these representable beliefs. This is the premise of all successful approaches to POMDP planning, such as point-based value iteration [11] and finite-state controllers [15].\nA remaining challenge with existing approaches is that they employ heuristics for choosing the subset of representable beliefs, often without even making these beliefs explicit. The representable beliefs are the centroids of the belief space clustering that the inference policy implements, and there is a defined cost for the information lost in this clustering. A more direct approach can compare this increase in cost to the belief compression it allows, and trade them off, as in rate-distortion theory. In a very fundamental sense, the complexity of POMDP planning really stems from a curse of information."}, {"heading": "Forward-Backward Coupling", "text": "This brings us to the final aspect of the challenge, which is the forwardbackward nature of the algorithms involved. Computing beliefs and information costs requires marginal distributions to be found using a forward process. At the same time, value functions are computed using a backward process. When the forward and backward processes are separable, the prob-\n21\nlem becomes much easier to solve. This is the case when observability is full, rendering the forward process\ntrivial. In domains with linear-Gaussian dynamics and quadratic cost (LQG), the Gaussian distribution and the quadratic function make the processes separable, and the problem easily solvable, despite the partial observability and controllability. This hinges on the important property that the reachable beliefs are themselves Gaussian distributions over the world state space, with fixed covariances. The memory space can optimally be parameterized by the means of the beliefs, and these parameters inferred by linear updates from observations.\nIn general POMDPs, the forward and backward processes are coupled, which manifests in the non-convexity of the problem, and contributes to its complexity. To illustrate this issue, consider a solution policy that neglects to infer from observations and to utilize in actions a useful piece of information about the hidden state. This solution may easily become a local optimum for optimization algorithms, since on the one hand the information cannot be used in the control policy (backward process) if it is not inferred, and on the other hand it is wasteful to commit the information to the limited memory in the inference policy (forward process) if it is never used. The control and inference policies may therefore be optimal for each other, but only locally optimal as a pair.\nThe coupling of the forward and the backward processes is mediated by the representation. Given the semantics of the memory states as the beliefs for which they stand, the processes become separable. This again emphasizes the importance of the challenge of finding good representations for reinforcement learning.\n22\nencoder qX|S\nchannel pY |X\ndecoder qS\u0302|Y\ns x y s\u0302"}, {"heading": "Source Coding and Channel Coding", "text": ""}, {"heading": "1.2.1 Rate-Distortion Theory", "text": ""}, {"heading": "1.2 Information Theory", "text": "and s\u0302, and the expected cost only depends on the distribution of x, this suggests a separation of source coding and channel coding. In the rate-distortion problem, we determine the distribution qS\u0302|Sps\u0302|sq induced by the source coding, so that it achieves low expected distortion while also compressing the signal to keep the information rate Irs; s\u0302s low. In capacity-cost problem, we determine the distribution qXpxq induced by the channel coding, so that it achieves low expected cost while also allowing a greater information capacity Irx; ys on the channel."}, {"heading": "Source-Channel Separation", "text": "Clearly, a solution to the joint source-channel coding problem is also feasible separately for each of the rate-distortion and capacity-cost problems. The optima of the subproblems therefore give a lower bound on the joint optimum. Classic coding theory shows that if we allow the encoders and decoders to map a large block of inputs, as one unit, into a large block of outputs, then asymptotically for large blocks the lower bound obtained by separation is tight [16]. This separation principle allows coding theorists and practitioners to focus their efforts on one of the subproblems at a time, without having to worry about combining the solutions into a joint solution, at least in this simple setting.\nWhen we come to apply this theory to reinforcement learning in Section 1.2.2, however, we find that encoding blocks of inputs is not an option. Our encoder and decoder are part of a controller that needs to take in a single observation and output a single action in each time step. In a large and rich system, it may be the case that each single observation or action is complex enough to treat it as a block in and of itself. More generally, however, single-letter coding is required.\nA characterization of sources and channels that are matched for singleletter coding is given in [17]. In such source-channel pairs there exists a single-letter coding that achieves the lower bound of separate source and\n24\nchannel coding. Luckily, when designing a reinforcement learning agent, it is often possible to choose the channel that is built into the agent, so that it matches the agent\u2019s information sources. Then the problem of internal agent communication can be separated into its source-coding and channel-matching parts, and the former treated as a sequential version of the rate-distortion problem, which we discuss in Section 1.2.2. For example, in Chapter 3 we rely on the result that if the optimal reconstruction distribution qS\u0302|S is Gaussian with linear mean and fixed covariance, the linear-Gaussian channel with quadratic channel cost matches the source."}, {"heading": "Optimal Lossy Source Coding", "text": "To trade off the expected distortion Erdps, s\u0302qs and the information rate Irs; s\u0302s, we can set one of these terms as our optimization target, with a constraint that the other is not too high. The Lagrangian of this optimization problem is\nFq,q\u0304; \u201c E s\u201ep s\u0302\u201eqp\u00a8|sq\n\u201e 1\nlog qps\u0302|sq q\u0304ps\u0302q ` dps, s\u0302q\n\u21e2 ,\nplus terms that constrain the distributions q and q\u0304 to be normalized. Here is a Lagrange multiplier that sets the relative marginal costs of the distortion and the information rate. This Lagrangian is also called the free energy, due to similarities to the quantity of that name in statistical physics, with the inverse temperature.\nNote that we did not constrain the distribution q\u0304ps\u0302q over the reconstruction to be the marginal that corresponds to ppsq and qps\u0302|sq. Instead, this required property will emerge as a necessary condition for a solution to be an optimum. The optimum must have gradient 0 with respect to all parameters,\n25\nwhich implies\nBqps\u0302|sq Fq,q\u0304; \u201c ppsq \u02c6 1\nlog qps\u0302|sq q\u0304ps\u0302q ` dps, s\u0302q ` 1\n` s \u02d9 \u201c 0\nBq\u0304ps\u0302q Fq,q\u0304; \u201c Es\u201eprqps\u0302|sqs q\u0304ps\u0302q ` \u201c 0,\nand thus\nqps\u0302|sq \u201c 1 Z psq q\u0304ps\u0302q expp\u00b4 dps, s\u0302qq\nq\u0304ps\u0302q \u201c Es\u201eprqps\u0302|sqs.\nHere Z is a normalizing partition function. The equations for q and q\u0304 provide us with a method for finding the optimal\nsolution. We can take them as update equations, and iteratively improve a solution until it converges. This algorithm, known as Blahut-Arimoto, turns out to be an alternating projection algorithm, where each step is a projection onto a convex set, guaranteeing convergence to a global optimum [18]."}, {"heading": "1.2.2 Sequential Rate-Distortion", "text": ""}, {"heading": "Problem Formulation", "text": "Rate-distortion theory gives us a way to model intrinsic limitations of bounded agents. Agents often operate under limited capacity of their internal storage and communication channels. Such constraints can also be used as a proxy for computational limitations caused by scarcity of information processing resources.\nIntrinsic costs on information rates between the various components of an agent limit the space of policies that can feasibly be implemented. The agent may be unable to pay attention to the entire observation available in its sensors, commit the entire observable history (or a sufficient statistic of\n26\nWorld wt\nObservation ot\nSensor encoder\nActuator decoder\nAction at\nwith\n\u21e1\u0304patq \u201c E wt\u201ep\u0304 ot\u201e p\u00a8|wtq r\u21e1pat|otqs.\nThe free energy of this sequential rate-distortion problem is\nFp\u0304,\u21e1,\u21e1\u0304,\u232b; \u201c Ewt\u201ep\u0304 \u00ab Eot\u201e p\u00a8|wtq\nat\u201e\u21e1p\u00a8|otq\n\u201e 1\nlog \u21e1pat|otq \u21e1\u0304patq ` cpwt, atq\n` Ewt`1\u201epp\u00a8|wt,atqr\u232bpwt`1qs \u21e2 \u00b4 \u232bpwtq ."}, {"heading": "Optimality Principle", "text": "The sequential rate-distortion Lagrangian is non-linear but convex in the policy parameters, and their global optimum given the other parameters is\n\u21e1pat|otq \u201c 1 Z potq \u21e1\u0304patq expp\u00b4 Er\u232bpwt`1q|ot, atsq (7)\n\u21e1\u0304patq \u201c E wt\u201ep\u0304 ot\u201e p\u00a8|wtq r\u21e1pat|otqs, (8)\nwhere for \u21e1 the expectation is with respect to the predictive posterior\nP\u21e1pwt`1|ot, atq \u201c Ewt\u201ep\u0304r pot|wtq\u21e1pat|otqppwt`1|wt, atqs Ewt\u201ep\u0304r pot|wtq\u21e1pat|otqs .\nThe backward recursion is similar to the unbounded case, except that the cost-to-go now also accumulates the intrinsic informational cost\n\u232bpwtq \u201c E ot\u201e p\u00a8|wtq at\u201e\u21e1p\u00a8|otq\nwt`1\u201epp\u00a8|wt,atq\n\u201e 1\nlog \u21e1pat|otq \u21e1\u0304patq ` cpwt, atq ` \u232bpwt`1q\n\u21e2 \u00b4 .\nNote that these updates are inherently forward-backward, even if we assume full observability, unlike the unbounded case which is backward-only\n28\nwhen observability is full. Both the forward and the backward processes are needed to compute the parameters for the single-step rate-distortion problem of optimizing the policy \u21e1 and its marginal \u21e1\u0304. The forward process computes the marginal p\u0304, which takes the role of the source distribution in rate-distortion theory, and is needed in the marginalization step (8) of the Blahut-Arimoto algorithm. The backward process computes the cost-to-go \u232b, which takes the role of the distortion between the source and its reconstruction\ndpot, atq \u201c Er\u232bpwt`1q|ot, ats,\nand is needed in the update step (7). The rate-distortion optimization of each step depends on past solutions through p\u0304 and on future solutions through \u232b."}, {"heading": "Extensions", "text": "One way to avoid the complication of the forward-backward coupling is to eliminate the optimization over the parameter \u21e1\u0304, which yields the marginal distribution, and replace it with a fixed prior \u21e1\u0304patq. More generally, we can allow this fixed prior to depend on the input as well, in the form \u21e2pat|otq. If we also restrict attention to fully observable domains, the state marginal p\u0304 is no longer needed, and the entire optimization can be performed by a backward algorithm. We use a sample-based version of this approach in Section 4.1.\nWe can generalize to retentive (memory-utilizing) policies by considering the reduction presented in Section 1.1.2. When the agent has an internal memory component, we can think of the memory reader as another sensor, and of the memory writer as another actuator. Then we can consider the joint information rate Irmt\u00b41, ot; mt, ats on the channel from the sensory and memory inputs to the control and memory outputs. This is the approach taken in Section 3.2.\nWe cannot always assume that all sensory inputs can be encoded together\n29\nby a single encoder, and the actuatory outputs decoded together. Different sensors, such as an external sensor and a memory reader, can be distributed between different components, and their inputs encoded separately by distinct encoders, and similarly for the outputs. The channels themselves can likewise be independent, each with its own capacity.\nThe tradeoffs involved in this more general setting are much more complicated. This is a special case of the diverse setting studied in network information theory [19], where tight bounds on the achievable rate-distortion regions are often unknown. We do not have a complete solution to this problem; however, the joint sensory-memory coding of the inputs to the inference policy can be formulated as a multiterminal source coding problem [20], where the solution exhibits an intriguing tradeoff between memory and sensing, as discussed in Section 2.1.\nFinally, we mention that our formulation leads to a sequential form of the source-coding problem and is missing its channel-coding counterpart. We conjecture that some form of a sequential capacity-cost problem may be relevant to a more general setting than the one discussed in this thesis and solvable using similar methods."}, {"heading": "1.3 Organization of the Dissertation", "text": "This dissertation is organized as follows. In Section 2.1 we present our approach to POMDP planning under informational constraints, and introduce the forward-backward algorithm for sequential rate-distortion. The setting in this section is restricted to passive POMDPs, where actions incur costs but do not affect the state of the world. The same algorithm can be implemented in general POMDPs, but the convergence properties are poorer. Section 2.2 explores one insightful aspect of these convergence challenges; namely, that the optimal solution can be either a limit cycle or an unstable fixed point of the update operator.\n30\nSections 3.1 and 3.2 study the important case of a POMDP over continuous state, observation and action spaces, where the dynamics are linear with Gaussian noise, and the extrinsic cost is quadratic. This LQG setting is often better-behaved than the general setting, making it useful in practice, and allowing insights into the properties of the theory that generalize to discrete and nonlinear domains. Section 3.1 focuses on reactive (memoryless) control policies, section 3.2 applies the reduction from retentive (memory-utilizing) policies to reactive ones to study the structure of the solution in the general case, and section 3.3 contains supplementary lemmas and proofs.\nIn Section 4.1 we turn to the learning setting, where sample-based updates are used instead of model-based ones. Focusing on fully observable domains and Kullback-Leibler (KL) costs allows the algorithm to be backward-only, with guaranteed convergence to the global optimum. The convergence rate, as well as other desirable attributes, are shown to improve on existing reinforcement learning algorithms.\nFinally, in Section 5 we discuss the contributions of this dissertation, and the results and insights obtained.\n31\nChapter 2"}, {"heading": "Minimum-Information POMDP", "text": ""}, {"heading": "Planning", "text": ""}, {"heading": "2.1 Bounded Planning in Passive POMDPs", "text": "Published: Roy Fox and Naftali Tishby, Bounded Planning in Passive POMDPs, In Proceedings of the 29th International Conference on Machine Learning (ICML), 2012.\n32\nBounded Planning in Passive POMDPs\nRoy Fox royf@cs.huji.ac.il Naftali Tishby tishby@cs.huji.ac.il\nSchool of Computer Science and Engineering The Hebrew University Jerusalem 91904, Israel\nAbstract\nIn Passive POMDPs actions do not a\u21b5ect the world state, but still incur costs. When the agent is bounded by information-processing constraints, it can only keep an approximation of the belief. We present a variational principle for the problem of maintaining the information which is most useful for minimizing the cost, and introduce an e cient and simple algorithm for finding an optimum.\n1. Introduction\n1.1. Passive POMDPs Planning\nPlanning in Partially Observable Markov Decision Processes (POMDPs) is an important task in reinforcement learning, which models an agent\u2019s interaction with its environment as a discrete-time stochastic process. The environment goes through a sequence of world states W1, . . . , Wn in a finite domain W. These states are hidden from the agent except for an observation Ot in a finite domain O, distributed by (Ot|Wt). In the standard POMDP, the agent then chooses an action, which a\u21b5ects the next world state and incurs a cost. Here we consider Passive POMDPs, in which the action a\u21b5ects the cost, but not the world state. We assume that the world itself is a Markov Chain, with states governed by a time-independent transition probability function p(Wt|Wt 1) and an initial distribution P1(W1).\nThe agent maintains an internal memory state Mt in a finite domain M. In each step the memory state is updated from the previous memory state and the current observation, according to a memory-state transition function qt(Mt|Mt 1, Ot) which serves as an inference policy. Figure 1 summarizes the stochastic process.\nAppearing in Proceedings of the 29 th International Conference on Machine Learning, Edinburgh, Scotland, UK, 2012. Copyright 2012 by the author(s)/owner(s).\nWt 1 Wt Wt+1\nOt 1 d Ot d Ot+1\nMt 1 Mt Mt+1\np p\nqt qt+1\nFigure 1. Structure of the Bayes network model of Passive POMDP planning\nThe agent\u2019s goal is to minimize the average expected cost of its actions. In this paper we take the agent\u2019s memory state to represent the action, and define a cost function d : W \u21e5M ! R on the world and memory states. The planning task is then to minimize\n1\nn\nnX\nt=1\nE Wt,Mt d(Wt, Mt)\ngiven the model parameters P1, p, and d.\nA Passive POMDP can be viewed as an HMM in which inference quality is measured by a cost function. Examples of Passive POMDPs include various gambling scenarios, such as the stock exchange or horse racing, where the betting does not a\u21b5ect the world state. In some settings, the reward depends directly on the amount of information that the agent has on the world state (Kelly gambling, see Cover & Thomas, 2006).\nWhen the agent is unbounded it has a simple deterministic optimal inference policy. It can maintain a belief Bt(Wt|O(t)), which is the posterior probability of the world state Wt given the observable history O(t) = O1, . . . , Ot. The belief is a minimal su cient statistic of O(t) for Wt, and therefore keeps all the relevant information. It can be computed sequentially by a forward algorithm, starting with B1(W1|O1) / P1(W1) (O1|W1), and at each step updating\nBt(Wt|O(t))\n/ X\nwt 1\nBt 1(wt 1|O(t 1))p(Wt|wt 1) (Ot|Wt),\nnormalized to be a probability vector.\n33\nBounded Planning in Passive POMDPs\n1.2. Information Constraints\nThe su ciency of the exact belief allows the agent to minimize the external cost, but it incurs significant internal costs. The amount of information which the agent needs to keep in memory can be large, and even each observation can be more than the agent can grasp. Anyway, not all of this information is equally useful in reducing external costs.\nIn general, the agent\u2019s information-processing capacity may be bounded in two ways:\n1. The capacity of the agent\u2019s memory may limit its information rate between Mt 1 and Mt, to RM .\n2. The capacity of the channel from the agent\u2019s sensors to its memory may limit the rate at which the agent is able to process the observation Ot while it is available, to RS .\nThe requirement that the agent keeps su cient statistics and exact beliefs is unrealistic. Rather, the agent\u2019s memory Mt must be a statistic of O(t) which is not sufficient, but is still \u201dgood\u201d in the sense that it keeps the external cost low. We also want it to be \u201dminimal\u201d for that level of quality, in terms of information-processing rates, so that the agent keeps only information which is useful enough. For each step individually, this is exactly the notion captured by rate-distortion theory, and here we have a sequential extension of it.\nThe main results of this paper are the formulation of the setting described above, and the introduction of an e cient and simple algorithm to solve it. We prove that the algorithm converges to a local optimum, and demonstrate in simulations the tradeo\u21b5 of memory and sensing intrinsic to this setting. The application of our results to previously studied problems, and a comparison to existing algorithms, are left for future work.\nThis paper is organized as follows. In section 2 we formulate out setting in information-theoretical terms. In section 3 we solve the problem for one step by finding a variational principle and an e cient optimization algorithm. In section 4 we analyze the complete sequential problem and introduce an algorithm to solve it. In section 5 we show two simulations of our solution.\n1.3. Related Work\nUnconstrained planning in Passive POMDPs is easily done by maintaining the exact belief, and choosing each action to minimize the subjective expected cost. Planning in general POMDPs is harder, in one aspect due to the size of the belief space. Many algorithms plan e ciently but approximately by focusing on a subset of this space.\nSeveral works do so by optimizing a finite-state controller of a given size (Poupart & Boutilier, 2003; Amato et al., 2010). The belief represented by each state of the controller is then the posterior probability of the world state given that memory state. A di\u21b5erent approach is to explicitly select a subset of beliefs, and use them to guide the iterations (Pineau et al., 2003). Another is to reduce the dimension of the belief space to its principle components (Roy & Gordon, 2002).\nIn this paper we present the novel setting of planning in Passive POMDPs which is constrained by information capacities. This setting allows treatment of reinforcement learning in an information-theoretic framework. It may also provide a principled method for belief approximation in general POMDPs. With a fixed action policy the POMDP becomes a Passive POMDP, and a bounded inference policy can be computed. This reduces the belief space, which in turn guides the action planning. This decoupling is similar to Chrisman (1992), and will be explored in future work.\nSome research treats POMDPs where the cost is the DKL between the distributions of the next world state when it is controlled and uncontrolled (Todorov, 2006; Kappen et al., 2009). This has interesting analogies to our setting. Our information-rate constraints define, in e\u21b5ect, components of the cost which are the DKL between the distribution of the next memory state and its marginals (see section 3.1). Tishby & Polani (2011) combine similar information-rate constraints of perception and action together. Future work will explore and exploit this symmetry in the special case where the memory information rate is unconstrained.\n2. Preliminaries\nAssume that the model parameters P1, p, and d are given. The agent strives to find an inference policy q(n) such that the average expected cost satisfies\n1\nn\nnX\nt=1\nE Wt,Mt\nd(Wt, Mt)  D.\nfor the minimal D possible. However, the agent operates under capacity constraints on the channels from Mt 1 and Ot to Mt. The external cost d parallels the distortion in rate-distortion theory, while the internal costs are information rates. The agent actually needs to minimize a combination of these costs.\nNote that the agent will generally have some information on the next observation even before seeing it, i.e. Mt 1 and Ot will not be independent. The agent therefore has some freedom in choosing what part of the information common to Mt 1 and Ot it remembers, and what part it forgets and observes anew.\n34\nBounded Planning in Passive POMDPs\nThe average information rate in both channels combined cannot exceed their total capacity, that is\n1\nn\nnX\nt=1\nI(Mt 1, Ot; Mt)  RM + RS .\nIn addition, in each step the portion of the above information that is absent from Ot may only be passed on the memory channel, and so\n1\nn\nnX\nt=1\nI(Mt 1; Mt|Ot)  RM .\nSimilarly, information absent from Mt 1 is subject to the sensory channel capacity\n1\nn\nnX\nt=1\nI(Ot; Mt|Mt 1)  RS .\nThe distortion constraint and the three informationrate constraints together form the problem of inference-planning in Passive POMDPs (Problem 1 ).\nThe emergence of three information-rate constraints for two channels is similar in spirit to multiterminal source coding (Berger, 1977). In their terminology, the agent needs to implement in each Mt a lossy coding of the correlated sources Mt 1 and Ot, under capacity constraints, so as to minimize an average expected distortion. The main di\u21b5erence is that here we chose to allow the encoding not to be distributed, in keeping with the ability of memory to interact with perception in biological agents (Laeng & Endestad, 2012).\n3. One-Step Optimization\n3.1. Variational Principle\nBefore we consider the long-term planning required of the agent in Problem 1, we focus on the choice of qn in the final step, given the other transitions, that is, given the joint distribution of Mn 1, Wn and On. We define the joint belief \u2713n(Mn 1, Wn) to be the joint distribution of Mn 1 and Wn, and have\nPr \u2713n\n(Mn 1, Wn, On) = \u2713n(Mn 1, Wn) (On|Wn).\nWe are interested in the rate-distortion region which includes all points (RM , RS , D) which are achievable, that is, for which there exists some qn(Mn|Mn 1, On) with\nD\u2713n(qn) def = E Wn,Mn d(Wn, Mn)  D\nIC,\u2713n(qn) def = I(Mn 1, On; Mn)  RM + RS\nIM,\u2713n(qn) def = I(Mn 1; Mn|On)  RM\nIS,\u2713n(qn) def = I(On; Mn|Mn 1)  RS .\nFor any information-rate pair (RM , RS), the minimal achievable D lies on the boundary D\u21e4\u2713n(RM , RS) of the rate-distortion region. When \u2713n and qn are clear from context, we refer to these quantities as D, IC , IM , IS and D\u21e4. We find D\u21e4(RM , RS) by minimizing the expected distortion under information-rate constraints. The minimum exists because all our formulas are continuous, and the solution space for qn is closed.\nLet q\u0304n(Mn|Mn 1), q\u0304n(Mn|On) and q\u0304n(Mn) be the marginals of qn(Mn|Mn 1, On). We expand the terms of the problem using these conditional probability distributions, to have\nmin qn,q\u0304n E Mn 1,Wn,On\nX\nmn\nqn(mn|Mn 1, On)d(Wn, mn)\nE Mn 1,On DKL(qn(Mn|Mn 1, On); q\u0304n(Mn))  RM + RS E\nMn 1,On DKL(qn(Mn|Mn 1, On); q\u0304n(Mn|On))  RM\nE Mn 1,On DKL(qn(Mn|Mn 1, On); q\u0304n(Mn|Mn 1))  RS under normalization constraints.1 We may waive the constraints of non-negative probabilities, which will essentially never be active as we shall see later. Also note that we optimize over qn and q\u0304n as distinct parameters. This is justified by theorem 1 which states that, at the optimum, q\u0304n are indeed the marginals of qn.\nLet the Lagrange multipliers for the constraints be C , M and S , and their sum = C + M + S . Leaving aside terms of log qn, the pointwise terms in the Lagrangian will be\nG(d, q\u0304n, Mn 1, Wn, On, Mn)\n= d(Wn, Mn) C log q\u0304n(Mn) M log q\u0304n(Mn|On) S log q\u0304n(Mn|Mn 1).\nIn the following analysis, several expectations of this function will be useful:\n\u2022 G\u2713n(d, q\u0304n, Mn 1, On, Mn) = E\nWn|Mn 1,On G(d, q\u0304n, Mn 1, Wn, On, Mn),\n\u2022 Gqn(d, q\u0304n, Mn 1, Wn) = E\nOn,Mn|Mn 1,Wn G(d, q\u0304n, Mn 1, Wn, On, Mn),\n\u2022 G\u2713n,qn(d, q\u0304n) = E\nMn 1,Wn,On,Mn G(d, q\u0304n, Mn 1, Wn, On, Mn)\n= D\u2713n(qn) + CH(q\u0304n(Mn)) + MH(q\u0304n(Mn|On)) + SH(q\u0304n(Mn|Mn 1)),\n1The information-rate constraints result from the n-step Problem 1 by fixing the first n 1 steps, if we consider that only two of the constraints are actually used in any instance (see corollary 3).\n35\nBounded Planning in Passive POMDPs\nwhere H is the entropy function. The Lagrangian of the problem, up to normalization terms and additive constants, can now be written as\nL1(qn, q\u0304n; \u2713n, C , M , S) = G\u2713n,qn(d, q\u0304n) H(qn).\n3.2. Properties of the One-Step Lagrangian\nTheorem 1. For any fixed \u2713n, L1 is convex in qn and q\u0304n. L1 is strictly convex in parameters which are conditional on mn 1 and on with Pr\u2713n(mn 1, on) > 0, and at the minimum these satisfy\nqn(Mn|Mn 1, On) (1)\n= exp( 1G\u2713n(d, q\u0304n, Mn 1, On, Mn))\nZn(Mn 1, On) ,\nwhere Zn is a normalizing partition function, and\nq\u0304n(Mn) = X\nmn 1,on\nPr \u2713n\n(mn 1, on)qn(Mn|mn 1, on)\nq\u0304n(Mn|On) = X\nmn 1\nPr \u2713n\n(mn 1|On)qn(Mn|mn 1, On)\nq\u0304n(Mn|Mn 1) = X\non\nPr \u2713n\n(on|Mn 1)qn(Mn|Mn 1, on).\n(2)\nProof. For any fixed \u2713n, L1 is convex since all its terms are convex. Non-zero terms only involve mn 1 and on with Pr\u2713n(mn 1, on) > 0. Focusing on these parameters, the distortion terms are linear, and the information terms strictly convex. The unique feasible extremum of L1 is then the global minimum. Di\u21b5erentiating by each parameter gives equations 1 and 2.\nIf follows from theorem 1 that complementary slackness conditions are su cient for optimality. Table 1 shows these conditions, the information rates (RM , RS) where the solution meets the boundary, and a subgradient of the boundary at that point. For example, if the minimum of L1 with M = S = 0 satisfies IC IM + IS , then for any information-rate pair in the interval [(IC IS , IS), (IM , IC IM )] the minimal achievable distortion is D and ( C , C) is a subgradient of the boundary.\nTheorem 2. For any joint belief \u2713n, the boundary D\u21e4\u2713n(RM , RS) of the rate-distortion region is continuous and convex. Any point (RM , RS , D) on the boundary at which ( \u21b5M , \u21b5S) is a subgradient, is achieved by minimizing L1 for multipliers\n( C , M , S) = 8 >>>>>>< >>>>>>: (0,\u21b5M ,\u21b5S) if IC  IM +IS (\u21b5M , 0,\u21b5S \u21b5M ) if IC IM +IS and \u21b5M  \u21b5S (\u21b5S ,\u21b5M \u21b5S , 0) if IC IM +IS\nand \u21b5M \u21b5S\nBounded Planning in Passive POMDPs\nAlgorithm 1 Last-Step Optimization\nInput: P1, p, , d, C , M , S , \u2713n Output: optimal qn\nr 0 Initialize some suggestion for qrn repeat\nCompute the marginals q\u0304rn of q r n (eq. 2) Compute a new value for qr+1n from q\u0304 r n (eq. 1)\nr r + 1 until qrn converges\nL1(qrn, q\u0304rn) L1(qr+1n , q\u0304rn) ! r !1 0.\nBut qr+1n is the unique minimum of the continuous Lagrangian. This implies that qrn also converges to a solution q\u21e4n with marginals q\u0304 \u21e4 n. By the continuity of the Lagrangian\u2019s derivatives, they are all 0 at this solution.\n4. Sequential Rate-Distortion\n4.1. Variational Principle\nReturning to the entire process of Problem 1, the sequence of joint beliefs \u2713(2,n) = \u27132, . . . , \u2713n depends recursively on \u27131 and the policy q(n). For each 1  t < n\n\u2713t+1(Mt, Wt+1) (3)\n= X\nmt 1,wt\n\u2713t(mt 1, wt) Pr qt\n(Mt, Wt+1|mt 1, wt),\nwith \u27131 given as the independent distribution of M0 and W1.\nAdding the constraints of equation 3 with multipliers \u232bt,mt,wt+1 , the Lagrangian of Problem 1 is\nLn(q(n), q\u0304(n), \u2713(2,n)) = 1\nn\nnX\nt=1\nL1(qt, q\u0304t; \u2713t, C , M , S)\n1 n\nn 1X\nt=1\nX\nmt,wt+1\n\u232bt,mt,wt+1 0 B@\u2713t+1(mt, wt+1)\nX\nmt 1,wt\n\u2713t(mt 1, wt) Pr qt\n(mt, wt+1|mt 1, wt) 1 CA\nup to normalization terms and additive constants.\nSolving Ln is much more di cult than L1. Ln is not convex, and each step may a\u21b5ect all future steps. Intuitively, remembering some feature of the sample in one step is less rewarding if this information is discarded in a future step, and vice versa. This leads to Ln having many local minima.\n4.2. Local Optimization Algorithm\nNevertheless, Problem 1 still has some structure which can be insightful to explore. In particular, it has some interesting similarities to the standard POMDP planning problem. Di\u21b5erentiating Ln by qt we now get\nqt(Mt|Mt 1, Ot) (4)\n= exp( 1G\u2713t(d~\u232btt , q\u0304t, Mt 1, Ot, Mt))\nZt(Mt 1, Ot) ,\nwith\nd~\u232btt (Wt, Mt) = d(Wt, Mt) + E Wt+1|Wt \u232bt,Mt,Wt+1 ,\nwhere ~\u232bn = 0. qt now depends on the future through the multiplier vector ~\u232bt. Note how the expectation of \u232bt,Mt,Wt+1 given Wt plays a parallel role to that of d(Wt, Mt).\nLn is linear in each \u2713t, and at the optimum must in fact be constant in every non-trivial component of \u2713t. This gives us a recursive formula for computing ~\u232bt 1 from ~\u232bt, qt and q\u0304t. For 1 < t  n, and whenever 0 < \u2713t(Mt 1, Wt) < 1, we have\n\u232bt 1,Mt 1,Wt = Gqt(d ~\u232bt t , q\u0304t, Mt 1, Wt) (5)\nE Ot|Wt H(qt(Mt|Mt 1, Ot)) + t,Wt . Note that equation 5 is a linear backward recursion for ~\u232bt. The multipliers ~ t come from the constraints that \u2713t is a probability distribution function. It has no consequence, however, since it is independent of Mt 1, and is normalized out when ~\u232bt 1 is used to compute qt 1 in equation 4.\nAt this point, we can introduce the following generalization of algorithm 1, which finds the optimal transition qt, given the joint belief \u2713t and the policy su x q(t+1,n) = qt+1, . . . , qn.\nAlgorithm 2 One-Step Optimization\nInput: P1, p, , d, C , M , S , \u2713t, q(t+1,n) Output: optimal qt\nr 0 Initialize some suggestion for qrt repeat\nCompute \u2713r(t+1,n) from \u2713t and q r (t,n 1) (eq. 3) Compute the marginals q\u0304r(t,n) of q r (t,n) (eq. 2) Compute ~\u232br(t,n 1) recursively backward (eq. 5) Compute qr+1t from \u2713 r t , q\u0304 r t and ~\u232b r t (eq. 4)\nr r + 1 until qrt converges\nThis is a forward-backward algorithm. In each iteration we compute \u2713(t+1,n) = \u2713t+1, . . . , \u2713n recursively\n37\nBounded Planning in Passive POMDPs\nforward, and then ~\u232b(t,n 1) = ~\u232bt, . . . ,~\u232bn 1 recursively backward. The algorithm is guaranteed to converge monotonically to an optimal solution, since Ln is still strictly convex in each qt separately. In fact, all our theorems and proofs regarding algorithm 1 carry over to this generalization.\n4.3. Joint-Belief MDP\nExpanding the recursion of ~\u232bt in equation 5 to a closed form, and disregarding ~ t, we find that for 1 < t  n and consistent parameters3\nLn t+1(q(t,n); \u2713t) (6)\n= 1 n t + 1 X\nmt 1,wt\n\u2713t(mt 1, wt)\u232bt 1,mt 1,wt .\nIf we extend the recursion by another step to define ~\u232b0, we get that our minimization target is\nLn(q(n); \u27131) = 1\nn E M0,W1 \u232b0,M0,W1 .\nThe minimization\nVt(\u2713t) = min q(t,n) E Mt 1,Wt \u232bt 1,Mt 1,Wt\ncan be looked at as the cost-to-go given the joint belief \u2713t before step t. Importantly, the recursive formula 5, when minimized over q(t,n), is a Bellman equation. It contains a recursive term\nE Mt,Wt+1|Mt 1,Wt \u232bt,Mt,Wt+1 ,\nwhich is the expected future cost, and other terms which are the expected immediate costs, internal and external, of implementing qt in step t.\nThis suggests viewing our problem as a joint-belief MDP. Here the states are the joint beliefs \u2713t, the actions are qt, and the next state always follows deterministically according to equation 3. This determinism allows us to use a time-dependent policy q(n), rather than a state-dependent one, and will prove useful in finding a solution.\nThe belief space of a standard POMDP can be looked at as the state space of a belief MDP, with the same actions and observations, and a linear transition function. If memory states are approximate beliefs, then our model is more like a further abstraction, where the MDP state space is the set of distributions over the belief space. Table 2 summarizes the main di\u21b5erences between this joint-belief MDP and the belief-MDP representation of discrete-action finite-horizon POMDPs.\n3When the Lagrangian is written in terms of the policy and the initial joint belief, the other parameters are taken to be consistent with them.\nBounded Planning in Passive POMDPs\nAlgorithm 3 Passive POMDP Bounded Planning\nInput: \u27131, p, , d, C , M , S , n Output: locally optimal q(n)\nr 0 Initialize some suggestion for qr(n) repeat \u2713r1 \u27131 Compute \u2713r(2,n) from \u2713 r 1 and q r (n 1) (eq. 3)\nfor t n to 1 do qr+1,t(t,n) arg minq(t,n) Ln t+1(q(t,n); \u2713rt ) s.t. q(t+1,n) 2 n qr+1,t+1(t+1,n) , q r (t+1,n) o (alg. 2) end for qr+1(n) q r+1,1 (n)\nr r + 1 until Ln(qr(n); \u27131) converges\nthe bounded-inference-planning problem (section 2), in the sense that for any 1  t  n, the global minimum given qr(t 1) and q r (t+1,n) is at most \u270f better than q r (n).\nProof. In iteration r, qr(n) from the previous iteration is feasible for qr+1(n) . Therefore the cost of q r (n) is nonincreasing in r and converges monotonically to a limit L\u21e4. Let qr(n) be within some \u270f > 0 of L\u21e4. Fix any 1  t  n, and let q\u21e4t achieve the global optimum given q r (t 1) and qr(t+1,n). Then\nLn(qr(n); \u27131) \u270f  Ln(qr+1(n) ; \u27131) (a)\n Ln((qr(t 1), qr+1,t(t,n) ); \u27131) (b)\n Ln((qr(t 1), q\u21e4t , qr(t+1,n)); \u27131), where\n(a) follows recursively from (qrt0 , q r+1,t0+1 (t0+1,n) ) being fea-\nsible for \u2713rt0 in iteration r, for each 1  t0 < t, and\n(b) follows from (q\u21e4t , q r (t+1,n)) being feasible for \u2713 r t in\niteration r.\nWhere algorithm 3 runs algorithm 2, it can initialize qt to q r t from the previous iteration. This may speed up each iteration, particularly when the algorithm has nearly converged. In addition, when running algorithm 3 with di\u21b5erent sets of multipliers, it converges much faster if each run is initialized with the previous result. Empirically, this also leads to much better local minima if the runs are sorted in order of decreasing multipliers.\nFigure 2. Boundary of the rate-distortion region for the sequential symmetric channel simulation The parts from left to right: M = 0; M = S = 0; S = 0\nFigure 3. Contour map of the rate-distortion boundary for the sequential symmetric channel simulation\n5. Simulations\n5.1. Symmetric Channel\nFigure 2 shows the boundary of the rate-distortion region for the 30-step sequential symmetric channel problem. The domains W, O and M are all binary. The agent observes the state correctly with probability 0.8. The state remains the same for the next step independently with probability 0.8. The distortion is the delta function.\nThe boundary consists of three parts as in corollary 3. They have M = 0 (left), M = S = 0 (middle) and S = 0 (right). Empirically, taking C = 0 is never feasible, as no optimal solution ever has IC  IM +IS . To clarify this further, figure 3 shows a colored contour map of the boundary. The lower the distortion, the higher the required information rates. The tradeo\u21b5 between memory and perception is illustrated by the negative slope of the contours.\n39\nBounded Planning in Passive POMDPs\nFigure 4. Contour map of the rate-distortion boundary for the Kelly gambling simulation\n5.2. Kelly Gambling\nThree horses are running in 10 races. Each horse has a fitness rating fi 2 {1, 2, 3}, and the winning horse is determined by softmax, i.e. horse i wins with probability proportional to exp(fi). Between the races, the fitness of each horse may independently grow by 1, with probability 0.1 if it is not maxed out, or drop by 1, with probability 0.1 if it is not depleted. Each horse keeps its fitness with the remaining probability.\nThe only observations are side races performed before each race: 2 random horses compete (with softmax) and the identities of the winner and the loser are announced. The memory state is a model of the world, consisting of the presumed fitness f\u0302i of each horse. The log-optimal proportional gambling strategy is used (Kelly gambling, see Cover & Thomas, 2006), betting on horse i a fraction of the wealth proportional to exp(f\u0302i). Each bet is double-or-nothing, and the distortion is the expected log return on the portfolio.\nFigure 4 shows the contour map, which is not convex in this instance.\n6. Conclusion\nWe have presented the problem of planning in Passive POMDPs with information-rate constraints. This problem takes the form of a sequential version of ratedistortion theory, and accordingly we were able to provide algorithms which globally optimize each step individually. Unfortunately, the full problem is not convex, and we expect that it has very hard instance sets.\nNevertheless, typical instances with some locality in their transitions and observations are expected to be easier. We have introduced an e cient and simple algorithm for finding a local minimum, and have used it to illustrate the problem with two simulations. In doing so, we have demonstrated the emergence of a\nmemory-perception tradeo\u21b5 in the problem.\nOur work has been motivated by the problem of planning in general POMDPs, which may benefit from belief approximation which is principled by information theory. The application of our current results to this problem is left for future work.\n7. Acknowledgement\nThis project is supported in part by the MSEE DARPA Project and by the Gatsby Charitable Foundation.\nReferences\nAmato, Christopher, Bernstein, Daniel S., and Zilberstein, Shlomo. Optimizing fixed-size stochastic controllers for POMDPs and decentralized POMDPs. Autonomous Agents and Multi-Agent Systems, 2010.\nBerger, Toby. Multiterminal source coding. The Information Theory Approach to Communications, 1977.\nChrisman, Lonnie. Reinforcement learning with perceptual aliasing: The perceptual distinctions approach. In Proceedings of the tenth national conference on Artificial intelligence, 1992.\nCover, Thomas M. and Thomas, Joy A. Elements of Information Theory. Wiley Series in Telecommunications and Signal Processing. 2006.\nKappen, Bert, Go\u0301mez, Vicenc\u0327, and Opper, Manfred. Optimal control as a graphical model inference problem. CoRR, 2009.\nLaeng, Bruno and Endestad, Tor. Bright illusions reduce the eye\u2019s pupil. Proceedings of the National Academy of Sciences, 2012.\nPineau, Joelle, Gordon, Geo\u21b5rey J., and Thrun, Sebastian. Point-based value iteration: An anytime algorithm for POMDPs. In IJCAI, 2003.\nPoupart, Pascal and Boutilier, Craig. Bounded finite state controllers. In NIPS, 2003.\nRoy, Nicholas and Gordon, Geo\u21b5rey J. Exponential family PCA for belief compression in POMDPs. In NIPS, 2002.\nTishby, Naftali and Polani, Daniel. Information theory of decisions and actions. In Cutsuridis, Vassilis, Hussain, Amir, and Taylor, John G. (eds.), PerceptionAction Cycle, pp. 601\u2013636. Springer, 2011.\nTodorov, Emanuel. Linearly-solvable Markov decision problems. In NIPS, 2006.\n40"}, {"heading": "2.2 Optimal Selective Attention in Reactive Agents", "text": "Unpublished: Roy Fox and Naftali Tishby, Optimal Selective Attention in Reactive Agents, Technical Report, 2015.\n41\nOptimal Selective Attention in Reactive Agents\nTechnical Report\nRoy Fox Naftali Tishby\nSchool of Computer Science and Engineering The Hebrew University\nAbstract\nIn POMDPs, information about the hidden state, delivered through observations, is both valuable to the agent, allowing it to base its actions on better informed internal states, and a \"curse\", exploding the size and diversity of the internal state space. One attempt to deal with this is to focus on reactive policies, that only base their actions on the most recent observation. However, even reactive policies can be demanding on resources, and agents need to pay selective attention to only some of the information available to them in observations. In this report we present the minimum-information principle for selective attention in reactive agents. We further motivate this approach by reducing the general problem of optimal control in POMDPs, to reactive control with complex observations. Lastly, we explore a newly discovered phenomenon of this optimization process \u2014 period doubling bifurcations. This necessitates periodic policies, and raises many more questions regarding stability, periodicity and chaos in optimal control."}, {"heading": "1 Introduction", "text": "For an intelligent agent interacting with its environment, information is valuable. By observing and retaining information about its environment, the agent can form beliefs and make predictions. It represents these beliefs in an internal state, on which it can then base its actions.\nIf information about some event in the world is unavailable to the agent, through the lack of observability or attention, its internal state is independent of that event, and so are its actions, potentially incurring otherwise avoidable costs. The same is true if the\ninformation is only partially available, limiting the extent to which the agent\u2019s actions can depend on the state of the world.\nHowever, information is also a \"curse\". Retaining much information about the world requires the agent to have a large and rich internal state space, representing diverse beliefs. This leads to complex policies for inference and control, which are computationally hard both to find and to apply. Designed agents should not be \u2014 and evolved agents are unlikely to be \u2014 more complex than is sufficient for them to perform well.\nThe \"curse of dimensionality\" [3] is the challenge of representing in the internal state space the entire belief space \u2014 the space of probability distributions over world states. The volume of this simplex is exponential in the number of world states, and approximate methods [19] [17] [1] [12] are required to explore and represent policies over this space.\nThe \"curse of history\" [14] results from representing only reachable Bayesian beliefs \u2014 posteriors of the world state given each possible observable history. The Bayesian belief is a sufficient statistic of the observable history for the world state, keeping all available information about it. Unfortunately, the size of this space can be exponential in the length of the history.\nThis realization immediately suggests the idea of truncating the observable history by forgetting older observations. Taken to the extreme, this leads to reactive agents [10] [20], in which each internal state can only take into account the most recent observation, discarding the previous internal state. The internal state space of reactive agents needs not be larger than the observation space, which removes the curse of history in domains where the set of observations is not too large.\nDefinition 1. A reactive agent bases its actions only on the most recent observation. In contrast, a retentive agent can base its actions on a memory state, which is updated with each observation, and thus sum-\n42\nmarizes the entire observable history.\nA drawback of this approach is that, since the history is no longer grounded in a known initial belief, a new challenge arises of identifying which beliefs these internal states represent. This challenge generally requires forward-backward algorithms [6], as opposed to fully observable Markov Decision Processes which are solvable by backward (dynamic programming) algorithms [3].\nIn addition, the original difficulty remains in domains where the observation space is still too large, such as the one presented in Section 3. In this sense, the curse of history is a special case of the following principle, which we might call the \"curse of information\".\nAn agent\u2019s input \u2014 its sensors, and its memory when available \u2014 usually contains too much information for the agent to process. For the agent to encode all of this information in its new internal state, an internal state space is required that is too large to be manageable and utilized by feasible policies. As a matter of practicality, an agent must have selective attention. A retentive agent must also have selective retention [6], which is beyond the scope of this paper. Definition 2. A reactive agent (similarly, a retentive agent), is said to have selective attention (resp. selective retention) if its internal state has less information about the world state than its observation (resp. observable history) does.\nReactive policies have been explored before in [10], with some of their challenges noted in [20]. A policygradient algorithm for finding such policies was presented in [7], which has the nice property of avoiding the forward-backward coupling. However, the local optimum it finds is not guaranteed to be a fixed point of the value recursion.\nInformation considerations in dynamical systems were presented in [24]. Algorithms were later introduced for trading off value and information in fully observable Markov Decision Processes [18] and in partially observable ones where actions have no external effect [6].\nThis paper offers three novel contributions, in each of the following sections.\nSection 3 shows that reactive policies are as expressive as retentive policies, under proper redefinition of the model. This motivates our focus on reactive agents, at the same time that it demands a more principled cure for the curse of information than simply discarding the memory.\nSection 4 provides such a principle, namely the minimum-information principle. We present the prin-\nworld\nsensor actuator\nobservation (o|s)\npolicy \u21e1(a|o)\naction p(s0|s, a)\nFigure 1: Schematic model of a reactive agent interacting with its environment\nciple and formalize it, discuss its relation to source coding, and give an algorithm for its numeric solution.\nSection 5 demonstrates a newly discovered phenomenon in optimal control, namely the occurrence of bifurcations when attention is traded off with external cost. This is illustrated using two examples.\nWe conclude with a short discussion of these contributions and their consequences."}, {"heading": "2 Preliminaries", "text": "We model the interaction of an intelligent agent with its environment using the formalism of Partially Observable Markov Decision Processes (POMDPs). A POMDP is a discrete-time dynamical system with state st 2 S. In time t, the system emits an observation ot 2 O with probability (ot|st). It then receives from the interacting agent an input action at 2 A, and transitions to a new state st+1 with probability p(st+1|st, at). For our purposes here, the sets S, O and A are finite, and we are only concerned with stationary (time-invariant) POMDPs, where the model parameters p and are fixed for every time step.\nA reactive agent has no internal memory state, and can only base its actions on the most recent observation. The agent consists of two modules, the sensor making the observation ot and the actuator taking the action at (Figure 1). The reactive policy \u21e1 of the agent is implemented by linking the two modules through a communication channel, such that the action at is taken with probability \u21e1t(at|ot) in reaction to observation ot. The policy is called periodic with period T if \u21e1t = \u21e1t+T for every time step t. The policy is called stationary if it has period 1, i.e. \u21e1t is fixed for every time step.\nThe model and the policy together induce a stochastic process over the variables {st, ot, at} (Figure 2). Due to the agent\u2019s lack of memory, the states {st} form a Markov chain. In the following we always assume that the process is ergodic. This implies that, if the agent policy has period T , then for each phase 0  t < T\n43\nst 1\not 1 at 1\nst\not at\nst+1\nFigure 2: Graphical model of a reactive agent interacting with its environment\nthere exists a unique marginal distribution p\u0304t(st) that is a fixed point of the T -step forward recursion\np\u0304t(st+T ) = X\nst\np\u0304t(st)Pt,\u21e1(st+T |st)\nwith\nPt,\u21e1(st+T |st) = X\nst+1,...,st+T 1\nt+T 1Y\n\u2327=t\nP\u21e1\u2327 (s\u2327+1|s\u2327 )\nand P\u21e1\u2327 (s\u2327+1|s\u2327 ) = X\no\u2327 ,a\u2327\n(o\u2327 |s\u2327 )\u21e1\u2327 (a\u2327 |o\u2327 )p(s\u2327+1|s\u2327 , a\u2327 ).\nThese marginal distributions are therefore periodic with the same period T , i.e. p\u0304t = p\u0304t+T , and inside a cycle the phases are linked through the 1-step forward recursion\np\u0304t+1(st+1) = X\nst\np\u0304t(st)P\u21e1t(st+1|st). (1)\nThe marginal distributions also induce beliefs\nbt(st|ot) = p\u0304t(st) (ot|st)\n\u0304t(ot) ,\nwith \u0304t(ot) = X\nst\np\u0304t(st) (ot|st).\nThe belief is the posterior distribution of the state given the observation.\nIn this paper we will have the agent incur an external nominal cost c(st, at) when it takes action at in state st, and measure the quality of a policy by the longterm average expected cost\nC = lim T!1\n1\nT\nT 1X\nt=0\nE[c(st, at)]\nin the stochastic process that the policy induces. If the policy has period T and the process is at its periodic marginal distribution, then\nC = 1T T 1X\nt=0\nX\nst,ot,at\np\u0304t(st) (ot|st)\u21e1t(at|ot)c(st, at).\nst 1\not 1\nmt 1\nat 1\nst\not\nmt\nat\nst+1\nFigure 3: Graphical model of a retentive agent interacting with its environment\nmt 1, st\nmt 1, ot mt\nmt, st\nmt at\nmt, st+1\nFigure 4: Reduction from a retentive policy to a reactive policy\nThis undiscounted expected cost is appropriate for studying stationary processes. In contrast, discounting the cost by t emphasizes transient effects, up to horizon O( 11 ). A related fault with discounting in reactive policies is discussed in [20]."}, {"heading": "3 Reduction from retentive to reactive policies", "text": "Consider a retentive agent [2] [6] interacting with a POMDP (Figure 3). The agent has an internal state mt 2 M, and an inference policy qt controlling it, such that with probability qt(mt|mt 1, ot) the memory state mt 1 is updated to mt upon observing ot in time step t. The control policy \u21e1t(at|mt) is allowed to depend not only on the most recent observation, but on the summary of the entire observable history represented in mt.\nIn a given POMDP, retentive policies (q,\u21e1) are much more expressive and powerful than reactive policies. Interestingly, however, there exists another (timevariant) POMDP in which \u21e10 = (q,\u21e1) can be implemented as a reactive policy (Figure 4). This new POMDP is similar in spirit to the cross-product MDP [11], and the distinction between them is discussed below.\nFormally, let the state space of the new POMDP be S 0 = M \u21e5 S, the observation space O0 = M \u21e5 (O [ {?}), where ? is some null-observation symbol, and the action space A0 = M [ A. Let the dynamics advance at twice the frequency, with each time step tak-\n44\ning half as long. The state at time t is s0t = (mt 1, st), and it emits an observation with distribution\n0t((mt 1, ot)|(mt 1, st)) = (ot|st).\nThe agent, upon observing (mt 1, ot), can apply its inference policy to generate the next memory state mt. It then takes the \"action\" of committing mt to \"external storage\"\np0t((mt, st)|(mt 1, st), mt) = 1.\nIn this new state at time t+ 12 , the committed memory state is observable\n0 t+\n1 2\n((mt,?)|(mt, st)) = 1,\nand the agent can apply its control policy to take the action at, thus controlling the transition\np0 t+\n1 2\n((mt, st+1)|(mt, st), at) = p(st+1|st, at).\nNote that it should be inadmissible for the agent to commit a memory state in a non-integer time step, or take an action in an integer one. This can be enforced by penalizing the wrong type of action, which is the main reason that the new POMDP needs to be timevariant.\nAssuming that the agent follows these restrictions, the new POMDP induces the same stochastic process over the variables {st, ot, mt, at} as the original one for any given policy, establishing the reduction.\nOur reduction is related to the cross-product MDP of [11]. However, the two models have different formulations that serve their different purposes \u2014 where the cross-product MDP creates structure to be exploited in planning algorithms, our formulation flattens this structure to reduce the problem to a simpler one. To achieve this, instead of the implicit restriction in [11] that policies depend only on the agent state, we model the same constraint explicitly as partial observability. Furthermore, by breaking each time step into two we avoid the exponential action space of the cross-product MDP.\nLastly, an important issue to consider is the memory state space M. The standard approach in the reinforcement learning literature is to have M be the belief space, the simplex of distributions over S, and q the Bayesian inference policy1. Such a choice would make S 0, O0 and A0 uncountable, as opposed to our usual assumption that these sets are finite.\n1It is also common to have actions as part of the observable history, which our notation allows but does not require.\nAlternatively, we can have in M only reachable beliefs. If the support of the inference policy q is finite2, then over a finite horizon only a finite number of beliefs are reachable. Unfortunately, due to the \"curse of history\", this number is exponential in the horizon, which renders this reduction \u2014 and indeed many existing approaches to POMDPs \u2014 impractical.\nThis difficulty underlines the need for selective attention. Theoretically, the support of mt needs never be larger than that of (mt 1, ot), at least in terms of sufficient inference. However, it should practically be much smaller than that \u2014 roughly the same size as the support of mt 1 \u2014 if the agent is to interact with the system for significant horizons without exploding in complexity. The ability of the agent to selectively attend to its input, whether from sensors or from memory, and to retain not all, but only the most useful information, is key to reducing this complexity.\nThis is the approach taken by Finite State Controllers (FSCs) [15], where the number of memory states is fixed. Several heuristic algorithms exist for finding a good FSC, however this problem is hard and highly non-convex. The policy of a FSC is time-invariant, and as we see in Section 5 a stationary Bellman-optimal solution is generally not stable."}, {"heading": "4 Minimum-information principle", "text": "Our guiding principle in formalizing selective attention is the reduction of information complexity, as measured by the Shannon mutual information between the observation ot and the action at. We first present the principle, and then justify it by relating it to source coding. We note that numerous other justifications and connections exist, some discussed previously [9] [24] [25] [16] [8], and some should be explored further, particularly in the context of POMDP planning.\nThe pointwise mutual information between ot and at in time step t is given by\nit(ot, at) = log \u21e1t(at|ot) \u21e1\u0304t(at) ,\nwith \u21e1\u0304t(at) = X\not\n\u0304t(ot)\u21e1t(at|ot). (2)\nThis can be thought of as the internal informational cost of choosing action at in reaction to observation ot. The long-term average expectation of this internal cost, similar to the external cost, is\nI = lim T!1\n1\nT\nT 1X\nt=0\nE[it(ot, at)].\n2For example, the Bayesian inference policy is deterministic.\n45"}, {"heading": "If the policy has period T and the process is at its periodic marginal distribution then", "text": "I = 1T T 1X\nt=0\nX\not,at\n\u0304t(ot)\u21e1t(at|ot)it(ot, at)\n= 1 T T 1X\nt=0\nDKL[\u21e1tk\u21e1\u0304t] = 1 T T 1X\nt=0\nI[ot; at].\nHere DKL[\u21e1tk\u21e1\u0304t] is the Kullback-Leibler divergence of \u21e1t from \u21e1\u0304t, and I[ot; at] is the Shannon mutual information between ot and at.\nDKL[\u21e1tk\u21e1\u0304t] is a measure of the cognitive effort required for the agent to diverge from a passive, uncontrolled policy \u21e1\u0304t to an active, controlled policy \u21e1t. Unlike [9] [24] [25], we allow the passive policy, as well as the active one, to be designed or evolved. The uncontrolled policy that minimizes the informational cost I is the appropriate marginal distribution of the action (2) [5].\nAmong agents incurring external cost C  C, the simplest agent, in some sense, minimizes the internal cost I. In other words, the agent needs to trade off its external and internal costs. To link these views, the Lagrange multiplier corresponding to the constraint C  C in the optimization of I is a conversion rate between the two types of cost. We can then write the total cost as F = 1 I + C."}, {"heading": "F is called the free energy, due to its similarity to the quantity of the same name in statistical physics, with taking the part of the inverse temperature.", "text": "For a given , the agent chooses its policy so as to minimize the free energy, under two constraints. First, the dynamics of the system follow the forward recursion (1). Second, p\u0304t, \u21e1t(\u00b7|ot) and \u21e1\u0304t need to be probability distributions, each summing to 1. The constraints that they are non-negative can be ignored, since they will be either inactive or weakly active.\nThis gives for horizon T the Lagrangian Lp\u0304,\u21e1,\u21e1\u0304\n= 1\nT\nT 1X\nt=0\nX\nst,ot,at\np\u0304t(st) (ot|st)\u21e1t(at|ot)ft(st, ot, at)\n+ X\nst+1\n\u232bt+1(st+1)\nX\nst\np\u0304t(st)P\u21e1t(st+1|st) p\u0304t+1(st+1) !\n't X\nst\np\u0304t(st) 1 ! + \u2318t X\nat\n\u21e1\u0304t(at) 1 !\n+ X\not\nt(ot)\nX\nat\n\u21e1t(at|ot) 1 !! ,\nwith\nft(st, ot, at) = 1 it(ot, at) + c(st, at)."}, {"heading": "4.1 Necessary conditions for optimality", "text": "This optimization problem is far from convex, and no efficient algorithm is known for finding the global optimum. Indeed, as tends to infinity, the agent\u2019s policy becomes deterministic, and some problems involving deterministic reactive policies are known to be NPcomplete [10].\nNevertheless, we can consider local minima by finding the first-order necessary conditions for a solution to be optimal. That is, we differentiate the Lagrangian by each of its parameters, and require that this derivative equals 0.\nFor p\u0304, this gives us a backward recursion\n\u232bt(st) = X\not,at\n(ot|st)\u21e1t(at|ot)ft(st, ot, at)\n+ X\nst+1\nP\u21e1t(st+1|st)\u232bt+1(st+1) 't. (3)\nDue to overconstraining, we have some degrees of freedom in choosing the multipliers to satisfy the KarushKuhn-Tucker conditions [4]. If the policy has period T , we will choose 't to also have period T and satisfy\n1 T T 1X\nt=0\n't = F ,\nso that \u232bt also has period T . Thus \u232bt(st) measures the fluctuation from the average free energy F of the state st in phase t of the cycle.\nThe first-order necessary conditions for \u21e1 are\n\u21e1t(at|ot) = 1\nZt(ot) \u21e1\u0304t(at) exp( dt(ot, at)) (4)\nwith\ndt(ot, at) = X\nst\nbt(st|ot)c(st, at)\n+ X\nst,st+1\nbt(st|ot)p(st+1|st, at)\u232bt+1(st+1)\nand the normalizing partition function\nZt(ot) = X\nat\n\u21e1\u0304t(at) exp( dt(ot, at)),\nand for \u21e1\u0304 we have (2) as promised."}, {"heading": "As tends to infinity, the optimal policy in (4) becomes deterministic. Together with (3), it becomes a Bellman equation [3].", "text": "For finite , on the other hand, the optimal policy is stochastic, which is a welcome outcome in many respects. The best deterministic reactive policy is generally arbitrarily worse than the optimal stochastic\n46\nreactive policy [20]. Optimality in reactive policies requires stochasticity. Unfortunately, many planning algorithms rely on the smaller space of deterministic policies (e.g. [14]), and others lack a principle by which to gauge the optimal amount of uncertainty in the agent\u2019s actions (e.g. [2]). We propose minimum information as such a principle.\nFurthermore, in practice the model used for planning is itself uncertain. Using deterministic policies could overfit to the available data, and hinder further learning [22]. Information considerations provide a principled way of fitting the uncertainty of the policy to the uncertainty of the model [18].\nIn reinforcement learning, it is common to use softmax to obtain stochastic planning and exploration policies [2] [22]. Note that soft-max is a special case of (4), with the uniform prior instead of the marginal \u21e1\u0304t. That information theory provides a better principle for stochasticity in optimization is further illustrated in the next subsection."}, {"heading": "4.2 Sequential rate-distortion", "text": "The form of (4) and (2) may be familiar as the solution to the rate-distortion problem of lossy source coding [5]. Indeed, minimum-information optimal control can be construed as a sequential rate-distortion problem [23].\nThe reactive agent\u2019s policy is a channel from its sensor to its actuator (Figure 5). Following the classic model of such a channel, the sensor can be considered an encoder which, upon observing the \"source\" ot, chooses a \"codeword\" mt. It transmits it to the actuator, a decoder which then \"reconstructs\" the intended at.\nFor a given time step t, and with a source distribution \u0304t(ot) and a distortion function dt(ot, at) fixed, this would be a standard source-coding problem. Let a feasible agent be one achieving at most D expected distortion\nX\not,at\n\u0304t(ot)\u21e1t(at|ot)dt(ot, at)  D.\nNow suppose we are interested in the feasible agent with the simplest internal state space, as measured by the size of the \"codebook\" M. The rate-distortion theorem [5] states that the simplest feasible agent is the one minimizing I[ot; at].\nIn a sequential rate-distortion problem, the solution \u21e1t in time step t affects future source distributions \u0304\u2327 in time steps \u2327 > t, as well as past distortions d\u2327 in time steps \u2327 < t. This creates a coupling between the forward inference process of computing marginal distributions, and the backward control process of computing value functions. This is further complicated in partially observable processes, where dt also depends on the forward inference process through the beliefs bt.\nCoupling makes sequential rate-distortion complex, both conceptually and computationally. Conceptually, the results of rate-distortion theory are no longer known to hold in the sequential case. If we nevertheless accept the minimum information as a solid guiding principle in our optimization, we find that this optimization is computationally hard. We can optimize the policy in each time step given the other time steps with algorithms like Blahut-Arimoto [13]. However the forward-backward algorithm for finding the overall policy is only guaranteed to converge to a local optimum [6]."}, {"heading": "4.3 Optimization algorithm", "text": "The forward recursion (1), the backward recursion (3), the optimal policy (4) and its marginal (2) are necessary conditions for a solution to be optimal. They also provide an algorithm for finding a good solution: iteratively plug the current solution in the right-hand side of one of the equations, to obtain a better solution, until (asymptotically) no such improvement is possible. Many existing algorithms employ a similar scheme. For example, in the Generalized Policy Iteration algorithm for planning in MDPs [22], there is some schedule for alternating between3 policy evaluation, a variant of (3), and policy improvement, a variant of (4) with !1. A sophisticated schedule can guarantee that the solution improves monotonically with each iteration [6]. Here we suggest the following simpler schedule, for which such a guarantee does not hold, but which empirically converges to good solutions in practice."}, {"heading": "Repeat until convergence:", "text": "1. Compute the marginal \u21e1\u0304 given the current solution for \u21e1, by applying (2).\n2. Compute the value function \u232b given the current solution for p\u0304, \u21e1 and \u21e1\u0304. This can be done by iteratively applying (3) until it converges, or by solving it as a system of linear equations.\n3A forward equation is not needed in fully observable problems if attention is not selective.\n47\n3. In a forward algorithm, until convergence to a limit cycle:\n(a) Compute the marginal p\u0304t given the current solution for p\u0304t 1 and \u21e1t 1, by applying (1).\n(b) Compute the optimal policy \u21e1t given the current solution for p\u0304t, \u21e1\u0304 and \u232b, by applying (4)."}, {"heading": "5 Periodicity in reactive policies", "text": "Throughout the previous sections, we always referred to periodic reactive policies rather than stationary ones, even though the POMDP itself is assumed to be stationary. Periodic reactive policies may seem to be a contradiction in terms, since their actions depend not only on the most recent observation, but also on the time t. They require a clock to be available to the actuator, with period that is a multiple of the policy period.\nWe argue that periodic policies must inevitably be a part of the solution concept of POMDPs with selective attention. When paying full attention to inputs, in the form of exact Bayesian inference, we can restrict the discussion to stationary policies [19]. When attention is partial, there are significant drawbacks to considering only stationary policies.\nOne drawback is that the best stationary policy is generally arbitrarily worse than the optimal periodic policy. Adapting the example in [20], consider the POMDP illustrated in Figure 6. This model has 2 states, 1 (uninformative) observation and 2 actions. The actions deterministically set the next state, and a reward (negative cost) is given for switching to the other state.\nThe optimal stationary retentive policy for this POMDP is to have two internal memory states, each indicating a different action, and switch between them in each time step. This policy gets the reward in each time step, but incurs 1 bit of internal cost4.\nOn the other hand, a stationary reactive policy in an unobservable POMDP is just a fixed distribution over the actions, and it can be no better in this instance than the uniform distribution. This policy yields only half the expected reward, but incurs no internal cost.\nLastly, the reactive policy of period 2 which alternates between the actions also receives the full reward, at seemingly no internal cost. In fact, this would seemingly also be the preferred retentive solution, if the internal cost is taken into consideration.\nOf course, counting no internal cost for a periodic pol-\n4See [6] for the definition of the internal cost of a retentive policy.\nicy is cheating. Instead of paying attention to its sensors or memory, the agent is paying attention to a clock, but that attention is still a burden on internal resources.\nSimilar to the informational cost between ot and at, we need to add a term for the informational cost between t and at. For a reactive policy with period T , this cost term can naturally be defined by\nI[t; at] = 1 T T 1X\nt=0\nDKL[\u21e1\u0304tk\u21e1\u0304]\n= 1 T T 1X\nt=0\nX\nat\n\u21e1\u0304t(at) log \u21e1\u0304t(at)\n\u21e1\u0304(at) ,\nwith\n\u21e1\u0304(a) = 1 T T 1X\nt=0\n\u21e1\u0304t(a).\nHere we use the fact that the phase of the cycle is distributed uniformly during the process.\nAdding the term I[t; at] to the free energy is equivalent to asserting that a clock is observable to the agent, and that attention to it is as costly as to any other part of the observation. The pointwise informational cost is now\ni\u0303t(ot, at) = log \u21e1t(at|ot) \u21e1\u0304(at) ,\nand the average expected internal cost is\nI\u0303 = 1T T 1X\nt=0\nI[ot; at] + I[t; at]\n= I[ot; at|t] + I[t; at] = I[t, ot; at]. The values of f\u0303t, \u232b\u0303t and d\u0303t change accordingly, and the optimal policy is now\n\u21e1t(at|ot) = 1\nZ\u0303t(ot) \u21e1\u0304(at) exp( d\u0303t(ot, at)),\nwith the proper partition function Z\u0303t(ot).\nThis allows us to consider policies which are \"softly periodic\", in that they attend to some but not all time information. Figure 7 shows the information-cost curve for the POMDP in Figure 6, and Figure 8 shows the final-state diagram for the iterative algorithm with the schedule in Section 4.3.\nInterestingly, this problem exhibits a bifurcation at = 1. Below this value, information is too costly, and the optimal solution is the stationary uniform policy. At = 1, the system undergoes a period-doubling bifurcation, and above this value the optimal policy becomes periodic with period 2 \u2014 the two phases of\n48\nthe cycle are given by the two branches in Figure 8. The \"hardness\" of this periodicity, as measured by the information I[t; at], grows continuously from 0, and tends to 1 bit as tends to infinity.\nAbove the critical point, a third solution exists, which is a fixed point of the optimization schedule (the dashed line in Figure 8). This solution is the optimal stationary reactive policy, but it is an unstable fixed point: starting the optimization from a small perturbation of this solution does not converge back to it, but diverges until it reaches the periodic solution. Thus we have a supercritical pitchfork bifurcation [21].\nThe instability of the stationary solution is another paramount reason for allowing periodic policies. It would be practically impossible to find a stationary solution using Bellman-like variational methods, as the one presented in this paper. In contrast, approaches such as policy-gradient methods [2] generally can find stationary solutions, but these are generally not fixed points of a Bellman recursion, and are thus not Bellman-optimal [3]."}, {"heading": "5.1 Robot example", "text": "As another example, consider the POMDP illustrated in Figure 9. Here a robot is engaged in moving items\n49\nfrom the left end of a corridor to the right one. The robot can be in one of 4 states: it can be at either end of the corridor, and it can be carrying an item or not. It has 4 actions: to move to the left end of the corridor, or to the right, or to pick up or put down an item. However, an action can fail with probability 0.2, leaving the robot at the same state. The robot can only pick up an item at the left end of the corridor, and it receives a reward for dropping an item at the right end.\nThe robot has 4 possible observations from two binary sensors, telling it its position and whether or not it is carrying an item. The location sensor is more reliable, showing the correct position with probability 0.88. The load sensor only shows the correct load state with probability 0.7. The parameters were selected for visual clarity of the results.\nFigure 10 shows the information-cost curve for this problem. Here there are two phase transitions, where the period doubles to 2, then again to 4 (Figure 11). When attention is scarce, the robot\u2019s actions are more uniformly random. In this situation, the sensors, although noisy, carry more relevant information than the clock, since they better correspond to the actual state.\nAs attention increases with , the robot relies more and more on its sensors. Conditional on the observation, the robot makes its actions less and less stochastic. At some point, the policy is reliable enough that the clock has more relevant information about the load state than the noisy sensor. At that point, a pitchfork bifurcation occurs, and the robot begins to rely mostly on the parity of the clock to decide when to move, and\non the location sensor to decide where to move and whether to load or unload.\nWith the parameters above, as keeps increasing, the clock eventually becomes even more reliable than the location sensor, and a second period doubling occurs, to period 4. Asymptotically as grows to infinity, the clock signal takes precedence over both sensors, and the agent unloads its cargo on schedule even if its sensors tells it that it is dislocated or empty handed."}, {"heading": "6 Discussion", "text": "In this paper we presented three novel results involving reactive agents interacting with partially observable systems. We have motivated the focus on reactive policies through a reduction from retentive policies, introduced a principle and an algorithm for optimizing reactive policies, and explored a surprising aspect of their phenomenology.\nWe conclude with a few remarks on the implication of each contribution."}, {"heading": "6.1 Selective attention as clustering", "text": "Information-constrained clustering can also be construed as source coding [16], so that the data to be clustered is considered the source, and the cluster centroids the reconstruction. Following the relation we show between selective attention and source coding, we can think of a reactive policy as a soft clustering of observations into actions.\nWith the information constraint removed, the clustering becomes hard, mapping each data point to its closest centroid. Similarly in our case, as grows the policy becomes more deterministic, until at ! 1 it always picks the optimal action for each observation.\nThe implication of viewing reactive policies as clustering is that actions should generally be simpler, and never more complex, than the observations on which they are based. Indeed, there is a duality between observations and actions, and between selective attention (the retained part of the observation) and selective action (the intended part of the action, as divergence from the prior \u21e1\u0304). Information that is not retained cannot be used for choosing actions, and there is no point in retaining information that is not used."}, {"heading": "6.2 Implications of selective attention for retentive agents", "text": "In this paper we have focused on reactive agents, and introduced the minimum-information principle for optimal selective attention. However, as the reduction\n50\nin Section 3 shows, this has implications for retentive agents as well.\nThe effect of selective attention is to make internal states less complex than their inputs, by discarding information that is not useful enough. When applied to the inference policy, this leads to approximate inference, that trades off the external value of information in guiding actions with its internal cost in information complexity. In fact, an inference process in POMDPs is equivalent to sequential clustering. With each new observation ot, the pair (mt 1, ot) is clustered into a new internal state mt.\nThe major challenge when planning in POMDPs is approximating the Bayesian belief in such a way that allows efficient planning and execution, while not losing too much value. Selective attention, and in this case retention, is precisely such a principle. The application of this approach to retentive agents is left for future work."}, {"heading": "6.3 Policy bifurcations and chaos theory", "text": "We have discovered the occurrence of bifurcations in the optimization process of reactive policies. It presents many of the characterizing features of chaos theory of iterated functions, such as period doubling and slow convergence near the bifurcation points. We expect to see many more such features in other, more complex systems. We conjecture that systems with more states, perhaps infinitely many, can present a cascade of bifurcations, leading to aperiodicity and chaos.\nA full investigation of the bearings of the theories of bifurcation and chaos to optimal control in dynamical systems is beyond the scope of this report. To the extent that such a connection exists, it could be of profound philosophical implications, as it could indicate that intelligent agents interacting with complex environments must choose among the following alternatives:\n\u2022 Plan with very little attention of their inputs\n\u2022 Plan for very short horizons\n\u2022 Plan with some degree of inability to identify their own value function or predict their own future actions.\n51"}, {"heading": "Minimum-Information LQG", "text": ""}, {"heading": "Control", "text": ""}, {"heading": "3.1 Part I: Memoryless Controllers", "text": "Accepted for publication: Roy Fox and Naftali Tishby, Minimum-Information LQG Control \u2014 Part I: Memoryless Controllers, In Proceedings of the 55th"}, {"heading": "IEEE Conference on Decision and Control (CDC), 2016.", "text": "53\nMinimum-Information LQG Control Part I: Memoryless Controllers\nRoy Fox\u2020 and Naftali Tishby\u2020\nAbstract\u2014 With the increased demand for power efficiency in feedback-control systems, communication is becoming a limiting factor, raising the need to trade off the external cost that they incur with the capacity of the controller\u2019s communication channels. With a proper design of the channels, this translates into a sequential rate-distortion problem, where we minimize the rate of information required for the controller\u2019s operation under a constraint on its external cost. Memoryless controllers are of particular interest both for the simplicity and frugality of their implementation and as a basis for studying more complex controllers. In this paper we present the optimality principle for memoryless linear controllers that utilize minimal information rates to achieve a guaranteed external-cost level. We also study the interesting and useful phenomenology of the optimal controller, such as the principled reduction of its order.\nI. INTRODUCTION\nThe modern technology industry is deploying artificial sensing-acting agents everywhere [1]. From smart-home devices to manufacturing robots to outdoor vehicles and from nanoscale machines to space rockets, these agents sense their environment and act on it in a perception-action cycle [2].\nWhen these agents are centrally controlled or when the sensors and the actuators are distributed, this control process relies on the ability to communicate the observations to the controller and the intended actions to the actuators. Autonomous agents likewise require sufficient capacity for the internal communication between their sensor and actuator components. As devices become smaller and more ubiquitous, power efficiency and physical restrictions dictate that communication become a limiting factor in the agent\u2019s operation.\nClassic optimal control theory [3] is unconcerned with the costs and the limitations of communicating the information needed for the controller\u2019s operation. In the past two decades, however, a large body of research has been dedicated to this issue ([4]\u2013[7] and references therein).\nThe perception-action cycle between a controller and its environment (Figure 1) consists of multiple channels and the capacity of any of them can be limited. Accordingly, various information rates can be considered. Our guiding principle in this work is to measure the information complexity of the controller\u2019s internal representation by asking \u201cHow much information does the controller have on the past?\u201d. The past is informative of the future [8] and some information in past\n\u2020School of Computer Science and Engineering, The Hebrew University, {royf,tishby}@cs.huji.ac.il\n\u21e4This work was supported by the DARPA MSEE Program, the Gatsby Charitable Foundation, the Israel Science Foundation and the Intel ICRI-CI Institute\nobservations is useful in controlling the future. We therefore seek a trade-off between the external cost incurred by the system and the internal cost of the communication resources spent by the controller in reducing that external cost. This trade-off is often formulated as an optimization problem, where one cost is constrained and the other minimized.\nWhen the controller has no internal memory, it can only attend to its most recent input observation, perhaps selectively. The degree of this attention, measured by the amount of Shannon information about the input observation that is utilized in the output control, is a lower bound on the required capacity of the communication channel between the controller\u2019s sensor and its actuator (see Figure 3).\nOur motivation in considering memoryless controllers is twofold. First, there are applications in which having any significant memory capacity within the controller is impractical. When the system is complex and the controller\u2019s hardware and resources are limited, they may be inadequate for maintaining any significant representation of the environment. In this case, a memoryless controller is the more cost-effective solution and sometimes the only feasible one. Memoryless controllers have been studied before, particularly in the contexts of delay [9]\u2013[11] and discrete state-spaces [12]\u2013[14].\nSecond, we show in Part II of this work [15] how to formulate the problem of optimizing a bounded retentive (memory-utilizing) controller as an equivalent problem of optimizing a bounded memoryless controller. This reduction enables us to reuse the solution derived in this paper in solving the bounded retentive control problem.\nMuch of the related existing research has been concerned with the issue of stabilizability of an unstable plant over communication channels that are limited in some way: quantization [16]\u2013[20], noise [21]\u2013[23], delay [24] and fading [25]. Our current work reduces in the stabilizable case to known results, and this analysis will be included in an upcoming paper.\nOther early publications proposed heuristic approximate solutions to the problem of optimal control with finite precision [26], [27]. More recently, the problem of optimal control over limited-capacity channels has been studied, with various information patterns in the sensor-side encoder and the actuator-side decoder: unlimited encoder and decoder memory with full feedback [28]\u2013[31], unlimited encoder memory and memoryless decoder [32], and unlimited decoder memory with some feedback to the encoder [33].\nA special case of our current work was studied in [34]. Their setting is fully observable and scalar, whereas we treat the much more general setting of partially observable vector\n54\nPlant xt\nObservation yt\nController zt\nControl ut\nFig. 1. Block diagram of a closed-loop control system\nspaces. Our main result reduces in this simple case to one of their solutions, implying that their other proposed solution is never optimal.\nIn this paper we make three contributions. First, we present a method for designing memoryless linear controllers that utilize minimal information rates to achieve a guaranteed external cost level. To our knowledge, this is the first treatment of information considerations in continuous-space control problems where neither the controller\u2019s sensor nor its actuator have unbounded memory capacity.\nSecond, we derive a solution that has a particularly explicit form, allowing direct numerical computation. Unlike classic controllers, which are designed by separable forward and backward Riccati equations [35], our forward and backward recursions are coupled. Yet each forward and backward step is given in closed form up to eigenvalue decomposition (EVD) operations. This is in contrast to the semidefinite programs (SDP) in [29], [31], [36], which require external solvers.\nThird, we study the interesting and useful phenomenology of the optimal controller. It manifests a water-filling effect [37], which is a principled criterion for the selection of the active controller modes and their magnitudes. By trading off external cost to reduce the controller\u2019s communication resources we also reduce its order in a principled way.\nIn Section II we define the LQG task that the controller should solve. In Section III we present the memoryless control model and the information considerations involved. In Section IV we find the conditions satisfied by the optimal linear solution and discuss its intriguing phenomenology. More discussion and an illustrative example can be found in Part II of this work [15].\nII. CONTROL TASK We consider the closed-loop control problem depicted schematically in Figure 1, where an agent (controller) is interacting with its environment (plant). When the plant is in state xt 2 Rn, it emits an observation yt 2 Rk, takes in a control input ut 2 R and undergoes a stochastic state transition. The goal of the controller is to reduce the longterm average expectation of some cost rate Jt(xt, ut).\nA controller \u21e1 defines the possibly stochastic mapping from the observable history yt = {y\u2327}\u2327 t into the control ut. The plant and the controller, under some initial conditions, jointly induce a stochastic process over the infinite sequence of variables {xt, yt, ut}.\nOur focus in this work is on discrete-time systems with linear dynamics, Gaussian noise and quadratic cost rate (LQG). For simplicity, all elements are taken to be homogeneous, i.e. centered at the origin, and time-invariant. We note that all our results hold without these assumptions, with the appropriate adjustments, as usual in LQG problems [3].\nDefinition 1: A linear-Gaussian time-invariant (LTI) plant A, B, C, , has state dynamics\nxt+1 = Axt + But + t; t \u21e0 N (0, ),\nwhere A 2 Rn n, B 2 Rn , 2 Sn+ is in the positive-semidefinite cone and t is independent of (xt, yt, ut) = {x\u2327 , y\u2327 , u\u2327}\u2327 t. The observation dynamics are\nyt = Cxt + \u270ft; \u270ft \u21e0 N (0, ),\nwhere C 2 Rk n, 2 Sk+ and \u270ft is independent of (yt 1, ut 1, xt).\nDefinition 2: A linear-quadratic-Gaussian (LQG) task A, B, C, , , Q, R involves a LTI plant and the cost rate\nJt = 12 (x t Qxt + u t Rut),\nwhere Q 2 Sn+ and R 2 S +. The task is to achieve a low long-term average expected cost rate, with respect to the distribution induced by the plant and the controller \u21e1\nJ\u21e1 = lim sup T!1\n1\nT\nTX\nt=1\nE\u21e1[Jt]. (1)\nWe are particularly interested in controllers which are time-invariant, i.e. have \u21e1(ut|yt) independent of t, and which induce a stationary process, independent of any initial conditions. In a stationary process, the marginal joint distribution of (xt, yt, ut) is time-invariant and we can replace the longterm average expected cost rate (1) with the expected cost rate in the stationary marginal distribution.\nWe denote by x 2 Sn+ and y 2 Sk+, respectively, the stationary covariances of the state and of the observation, assuming they exist and are finite. They are related through\ny = C x C + .\nIf xt and yt are jointly Gaussian with mean 0, they satisfy the reverse relation\nxt = Kyt + t; t \u21e0 N (0, ),\nwhere the residual state noise t is independent of yt (but not of the past of the process), and\nK = x C \u2020y\n= x x C \u2020y C x,\nwith \u00b7\u2020 the Moore-Penrose pseudoinverse. If the entire process has mean 0, the stationary expected cost rate (1) is given by\nJ\u21e1 = 12 (tr(Q x) + tr(R u)), (2)\nwhere u 2 S + is the stationary control covariance.\n55\nxt 1\nyt 1 ut 1\nxt\nyt ut\nxt+1\nFig. 2. Bayesian network of memoryless control\nPlant xt\nObservation yt\nSensor encoder\nActuator decoder\nControl ut\nzt\nr bits\nFig. 3. The communication channel from the sensor to the actuator\nIII. BOUNDED MEMORYLESS CONTROLLERS\nA. Control model\nIn this section we introduce memoryless controllers with bounded communication resources. A memoryless controller is simply a possibly stochastic mapping from its input observation yt into its output control ut without any memory of past observations.\nDefinition 3: A controller is memoryless if the control depends only on the most recent observation; that is, ut is independent of (yt 1, ut 1, xt) given yt.\nA system including a memoryless controller satisfies the Bayesian network in Figure 2.\nOptimization over the space of all measurable control laws is hard to analyze and the optimal controller can be hard to implement. It is therefore practical to require the control law to have a certain form, most commonly the linear-Gaussian time-invariant (LTI) form. LTI controllers induce, jointly with a LTI plant, a Gaussian stochastic process. When the process is stable, it has a unique stationary distribution that is independent of any initial conditions. Linear controllers with limited memory are known not to be optimal for all control problems [38], [39]. The conditions under which there exists an optimal memoryless controller which is LTI, so that no performance is lost by focusing our attention on such controllers, are beyond the scope of this paper.\nDefinition 4: A memoryless linear-Gaussian time-invariant (LTI) controller has control law of the form\nut = Hyt + \u2318t; \u2318t \u21e0 N (0, ), (3) where H 2 R k, 2 S + and \u2318t is independent of yt.\nB. Information considerations\nOur controller is bounded and operates under limitations on its capacity to process the observation and produce the control. To measure this internal complexity of the controller, we consider a memoryless communication channel from the sensor to the actuator with limited capacity (Figure 3).\nFor example, we can consider a noiseless binary channel and measure the controller complexity by the number r of bits per time step that it transmits from its sensor to its actuator. This requires the controller\u2019s sensor to perform lossy source coding of the observation yt by compressing it into a binary string representation zt 2 {0, 1}r. This representation is transmitted losslessly and reconstructed by the controller\u2019s actuator as a control ut. Since the controller is memoryless, both the encoder and the decoder are memoryless.\nIn this sense, the dynamical control problem can be thought of as a sequential rate-distortion (SRD) problem [28], [36]. Unlike the standard one-shot rate-distortion (RD) problem [37], [40], in a SRD problem the output distribution affects the future of the process. This often creates a coupling between the forward inference process that determines the marginal distributions and the backward control process that determines the cost-to-go, i.e. the distortion. We note that without control [36] the decoder only affects the controller part of the future trajectory; however, this distinction is of minor consequence for the SRD aspect of the problem [41].\nFollowing rate-distortion theory, we find that the bit rate r required for this process is linked to the Shannon mutual information between the observation and the control, defined by\nI[yt; ut] = E log\nf(yt, ut)\nf(yt)f(ut)\n,\nwhere f denotes the various probability density functions, as indicated by their arguments. The bit rate is bounded from below by the information rate due to the data-processing inequality [37]\nI[yt; ut]  I[yt; zt]  H[zt]  r log 2, where\nH[zt] = E[log Pr(zt)] is the discrete Shannon entropy of zt.\nIn classic information theory, this bound is made asymptotically tight by jointly encoding a long block of observations and jointly decoding a long block of controls. In our setting, this is impossible due to the causal nature of the plant-controller interaction. Thus, unfortunately, the bound is generally not tight for discrete channels. We can nevertheless expect it to be a good approximation, if we draw intuition from the stabilizability problem, where the informational lower bound is approximated by a known upper bound [42].\nIn applications, it is often possible to make design choices regarding the channel itself. If we can design the channel to be perfectly matched to the optimal LTI control law, no block coding will be needed [43]. When the controller is LTI, it is more practical to take the channel in Figure 3 to be itself linear-Gaussian instead of binary. There exists an additive Gaussian noise channel with a signal power cost that is perfectly matched to our optimal controller in Theorem 1. With such a channel, the information rate is optimally equal to the channel capacity and a constraint on\n56\nthe information rate I[yt; ut] is equivalent to a constraint on the expected power available for transmission on the channel. We develop these results in the Supplementary Material1 (SM), Appendix I. We are thus interested in a LTI controller \u21e1 that minimizes the long-term average\nI\u21e1 = lim sup T!1\n1\nT\nTX\nt=1\nIt (4)\nof the controller\u2019s internal information rate It = I[yt; ut], under the constraint that it achieves some guarantee level c of expected cost rate.\nProblem 1: Given a LQG task, the bounded memoryless LTI controller optimization problem is\nmin \u21e1 I\u21e1 s.t. J\u21e1  c,\nwith I\u21e1 as in (4), where It = I[yt; ut], and with ut as in (3). IV. MAIN RESULT\nA. Optimality conditions\nIn this section we derive the optimality conditions for a bounded memoryless LTI controller. These conditions are summarized in Theorem 1 below.\nAnalysis of Problem 1 starts with considering the minimum mean square error (MMSE) estimators\nx\u0302yt = E[xt|yt] = Kyt x\u0302ut = E[xt|ut] = x;u \u2020u ut,\nrespectively for the state given the observation and the control. Here x;u = E[xtu t ] is the covariance matrix between xt and ut. This implies that x\u0302yt and x\u0302ut are also 0-mean and jointly Gaussian with the other variables. At this point, it is useful to state a few properties of MMSE estimators of Gaussian variables.\nLemma 1: Let x and x\u0302 be 0-mean jointly Gaussian random variables. The following properties are equivalent:\n1) There exists a random variable u, jointly Gaussian with x, such that x\u0302(u) = arg minx\u0302 E[kx\u0302 xk2|u] = E[x|u]. 2) x\u0302;x = x\u0302. 3) x|x\u0302 = x x\u0302, where x|x\u0302 is the conditional co-\nvariance matrix of x given x\u0302, implying x x\u0302. 4) x\u0302 = E[x|x\u0302].\nSuch x\u0302 is called a minimum mean square error (MMSE) estimator (of u) for x.\nProof: See SM, Appendix II. Since the conditional covariance x|u of xt given ut is deterministic, i.e. is not a random variable, the conditional expectation of xt given ut, i.e. x\u0302ut , is a sufficient statistic of ut for xt, satisfying the Markov chain xt \u2014 x\u0302ut \u2014 ut. This suggests that the stochastic control process satisfies the Bayesian network in Figure 4, where the control is based on x\u0302ut instead of directly on yt.\n1Available at https://arxiv.org/abs/1606.01946\nxt\nut 1\nx\u0302ut 1\nyt\nx\u0302yt x\u0302ut\nut\nxt+1\nyt+1\nx\u0302yt+1\nFig. 4. Bayesian network of memoryless estimator-based control\nLemma 2: The bounded memoryless LTI controller optimization problem (Problem 1) is solved by a control law of the form\nx\u0302yt = Kyt (5a) x\u0302ut = Wx\u0302yt + !t; !t \u21e0 N (0, !) (5b) ut = Lx\u0302ut , (5c)\nwhere W 2 Rn n, ! 2 Rn n, L 2 R n, !t is independent of yt, x\u0302ut is a MMSE estimator for x\u0302yt and\nI[yt; ut] = I[x\u0302yt ; x\u0302ut ]. (6) Proof: See SM, Appendix III. Lemma 2 allows us to derive optimality conditions for Problem 1. The stationary state covariance satisfies\nx = A B\nx x;u u;x u A B + (7)\n= (A + BL) x\u0302u(A + BL) + A x|x\u0302u A + .\nThe mutual information between jointly Gaussian variables [37] is given by\nI[x\u0302yt ; x\u0302ut ] = 12 (log | x\u0302y |\u2020 log | x\u0302y|x\u0302u |\u2020), (8)\nwhere | \u00b7 |\u2020 is the pseudodeterminant, i.e. the product of the positive eigenvalues. This holds if x\u0302y and x\u0302y|x\u0302u have the same range and thus the same number of positive eigenvalues; otherwise, the mutual information between yt and ut is infinite.\nWith the target (8) and the constraints (7) and J\u21e1  c, where J\u21e1 is given by (2), the Lagrangian of Problem 1 can be written as\nF x, x\u0302u ,L,S; = 12 ( 1(log | x\u0302y |\u2020 log | x\u0302y|x\u0302u |\u2020) (9) + tr(Q x) + tr(RL x\u0302u L )\n+ tr(S((A + BL) x\u0302u(A + BL)\n+ A x|x\u0302u A + x))).\nHere > 0 is the Lagrange multiplier corresponding to the constraint J\u21e1  c and serving as the marginal tradeoff coefficient between the external cost and the information rate, 2 S 2 Rn n is the multiplier of the constraint (7) and for convenience the entire Lagrangian is divided by . As in rate-distortion theory, F can be minimized for any given value of . The that corresponds to a specific expected cost-rate guarantee level c can then be found using a binary\n57\nsearch. The case = 0 corresponds to the minimization of information without any cost constraint.\nTheorem 1: Given , the Lagrangian (9) is minimized by a controller satisfying the forward equations\nx = (A + BL) x\u0302u(A + BL) (10a)\n+ A x|x\u0302u A +\ny = C x C + (10b) K = x C \u2020y (10c)\nx\u0302y = K y K , (10d)\nthe backward equations\nM = 1C K ( \u2020x\u0302y|x\u0302u \u2020 x\u0302y )KC (10e)\nS = Q + A SA M, (10f) L = (R + B SB)\u2020B SA (10g) N = L (R + B SB)L (10h)\nand the control-based estimator covariance\nx\u0302u = 1/2 x\u0302y V DV 1/2 x\u0302y , (10i)\nthe latter determined by the eigenvalue decomposition (EVD)\nV V = 1/2 x\u0302y N 1/2 x\u0302y\n(10j)\nhaving V orthogonal with n rank( x\u0302y ) columns spanning the kernel of x\u0302y and = diag{ i} and by the active mode coefficient matrix\nD = diag 1 1 1i i > 1 0 i  1 . (10k) Proof: See SM, Appendix IV. The spectral analysis in (10j)\u2013(10k) implies that in (10e) the signal-to-noise-ratio (SNR) matrix Z = \u2020x\u0302y|x\u0302u \u2020 x\u0302y satisfies\nZ = \u2020x\u0302y|x\u0302u \u2020 x\u0302y = \u2020/2 x\u0302y V ((I D) 1 I)V \u2020/2x\u0302y =\n\u2020/2 x\u0302y V D V \u2020/2 x\u0302y\nand that the information rate is\nI\u21e1 = 12 (log | x\u0302y |\u2020 log | x\u0302y|x\u0302u |\u2020) (11) = log |I D| = X\ni\nmax(0, log i).\nAs shown in the SM, Appendix I, given an additive Gaussian noise channel wt ! w\u0302t with noise covariance I D, the optimal encoder and decoder are now given by\nwt = D 1/2V\n\u2020/2 x\u0302y x\u0302yt\nx\u0302ut = 1/2 x\u0302y V D 1/2w\u0302t,\nwhich can be summarized in the form (5b), with\nW = x\u0302u \u2020 x\u0302y\n! = 1/2 x\u0302y V D(I D)V 1/2x\u0302y .\nAlternatively, the controller can be given in the form (3), with\nH = LWK\n= L ! L .\nInterestingly, Theorem 1 also shows that S corresponds to the cost-to-go Hessian, with respect to the state, as in classic control theory. The difference is that here S also accumulates the non-quadratic information cost and is only the Hessian in an average sense. In the form given in Theorem 1, M is positive semidefinite, but S may not be. This is not problematic if we view S as the Lagrange multiplier of the equality constraint (7), but it is undesired for a cost-togo Hessian. The positive semidefiniteness of S is discussed further and restored in Part II [15, Section III-C].\nTheorem 1 gives the first-order necessary conditions for a solution to be optimal; namely, that the gradient of the Lagrangian (9) is 0 with respect to each parameter. It additionally includes two more conditions, one which is higher-order and the other non-necessary. First, the condition on x\u0302u (10i) is necessary but not first-order, being a solution to a semidefinite program (see SM, Appendix V). Second, the condition on L (10g) is the least-square solution of a possibly underdetermined system, which means that it may not hold for all optimal solutions but that it does hold for some globally optimal solution.\nProblem 1 is highly non-convex and has many local optima that satisfy the first-order necessary conditions. By including the two higher-order and non-necessary conditions, we exclude many of these local optima, although some remain (see Part II [15, Section IV]). This merits further study of the fixed-point structure of this problem.\nB. Phenomenology\nTo better understand the optimal solution of Theorem 1, consider its phenomenology as spans its range from 0 to 1. The following is the SRD extension of a standard result in one-shot RD theory [37].\nLemma 3: Let I(J ) be the minimal information rate achievable by a controller that incurs cost at rate at most J . This information-cost function is monotonically decreasing, convex, and its slope is\nJ I = , (12) for the Lagrange multiplier corresponding to the expected cost-rate guarantee level c = J .\nProof: For any , let\n\u21e1\u21e4 = arg min \u21e1 { 1I\u21e1 + J\u21e1}.\n\u21e1\u21e4 achieves the optimum in Problem 1 when c = J\u21e1\u21e4 . Take I = I\u21e1\u21e4 ; J = J\u21e1\u21e4 ; F = 1I + J .\nThen the slope equation follows by fixing while J and I vary and noting that at the optimum\nJ F = 1 J I + 1 = 0.\n58\nMonotonicity follows directly from the definition of Problem 1. Convexity can also be shown directly; however, it follows more easily from the slope equation (12) by considering that J is non-increasing in and thus\n2J 2 I = J 0. We now turn to consider how the controller order is increased as is increased from 0 to 1. This phenomenon is known as a water-filling effect [36], [37], and is made explicit in the form of the optimal information rate I\u21e1 (11). Note, however, that in the SRD problem the water-filling effect is self-consistent, in that itself depends on .\nDefinition 5: The order of a LTI controller is rank( x\u0302u). For the optimal solution (10i), this equals rank(D), the number of active modes.\nLet us consider a stable plant, having all eigenvalues of A inside the unit circle. We note that our results hold more generally and extend known results [18] when the plant is unstable but stabilizable and detectable. However, the analysis of this case when ! 0 is more involved and is presented separately in an upcoming paper.\nWhen = 0, we are only interested in minimizing I\u21e1 and therefore take an order-0 controller, having D = 0, x\u0302u = 0 and M = 0. x and S satisfy the uncontrolled Lyapunov equations\nx = A x A +\nS = Q + A SA.\nL and N can be set accordingly, despite the fact that no attention to the observation is spent and no control is possible. Computing the EVD of x\u0302y and applying (10j), we can retrieve .\nAs we increase , this uncontrolled solution remains constant as long as  11 , the inverse of the largest eigenvalue in . At that first critical point, the controller undergoes a phase transition, where its order increases from 0 to 1 (or higher if 1 is not unique in ).\nNote that contains the same eigenvalues as the matrix\nN1/2x\u0302y = N 1/2 x\u0302y N 1/2,\nwhich represents the value of the information that the observation has on the state, in terms of the cost reduction it allows. Thus an order-1 controller observes and controls the state mode that provides the largest decrease in cost per bit of observed information, in keeping with (12).\nBeyond the first phase transition, the optimal solution does change with and so does . Eventually, meets 1i ( ), for each i = 2, . . . , rank( N1/2x\u0302y ) in turn and further phase transitions occur, increasing the controller order until it reaches rank( N1/2x\u0302y ).\nAs long as is finite, even after the last phase transition, the information rate must be finite. Since the controller lacks the capacity to attend to any mode with perfect fidelity, it must maintain some uncertainty in all modes and accordingly D I and x\u0302u x\u0302y . As ! 1, the SNR matrix Z = \u2020x\u0302y|x\u0302u \u2020 x\u0302y\ngrows to infinity in modes having i > 0, as does the information rate in these modes.\nThe = 1 case marks a qualitative change in the optimization problem. We are no longer concerned with the information rate and only wish to minimize the expected cost rate J\u21e1 . The optimal solution here is underdetermined with respect to useless modes where i = 0. Despite having no value in decreasing J\u21e1 , at = 1 (but not for !1) these modes may be observed for free. This allows us to simplify the solution to\nD = I\nx\u0302u = x\u0302y M = C K \u2020/2 x\u0302y 1/2 x\u0302y N 1/2 x\u0302y \u2020/2 x\u0302y KC\n= C K NKC.\nIt is interesting to note the impact of the observability on M at = 1. When the plant is unobservable, we have C = K = 0 and thus M = 0. When observability is full, we have C = K = I and thus M = N . For partial observability models, N M is not necessarily positive semidefinite, which will become important in the reduced retentive control problem (see Part II [15, Section III-C]).\nIn the classic control problem, where observability is partial but the memory and the sensory capacities are unbounded, the memory state is maintained by the Kalman filter and we have M = N and\nS = Q + A SA N, independent of the forward inference process. Note, however, that S in that case is the Hessian of the certainty-equivalent cost-to-go with respect to x\u0302t, instead of xt.\nThus either full and unbounded ( = 1) observability or bounded ( < 1) sensing with unbounded memory [31] are sufficient for recovering the separation principle of classic control theory. In the more general case, the backward control process (10f) is coupled with the forward inference process (10a).\nV. DISCUSSION\nIn this paper we introduce the problem of optimal memoryless LQG control with bounded channel capacity. We present the solution and discuss some of its properties and phenomenology.\nPart of our motivation in considering memoryless controllers is that the problem of retentive (memory-utilizing) control can be reduced to the problem of memoryless control. This is discussed in detail in Part II of this work [15, Section III-B]. The two control models are also compared there (Section IV) using an illustrative example.\nOne attractive aspect of our solution is its principled reduction of the controller order. In many applications, the controller\u2019s information rate is a more natural measure of its complexity than the dimension of its support. Nevertheless, a hard constraint on the order is sometimes required, alongside a soft constraint on the information rate, leading to an algorithmically challenging open question.\nThe controllers considered in this paper have linearGaussian control laws. This class of controllers does not\n59\nsolve optimally all control problems and is particularly prone to suboptimality in memory-constrained settings [38], [39]. Nevertheless, we conjecture that there exist some moderately strong conditions under which the bounded memoryless control problem discussed here is solved optimally by an LTI controller.\nREFERENCES\n[1] R. M. Murray, Control in an Information Rich World: Report of the Panel on Future Directions in Control, Dynamics, and Systems. SIAM, 2003. [2] V. Cutsuridis, A. Hussain, and J. G. Taylor, Perception-Action Cycle: Models, Architectures, and Hardware, ser. Cognitive and Neural Systems. Springer, 2011. [3] D. P. Bertsekas, Dynamic Programming and Optimal Control. Athena Scientific, 1995, vol. I. [4] S. K. Mitter, \u201cControl with limited information,\u201d European Journal of Control, vol. 7, no. 2, pp. 122\u2013131, 2001. [5] G. N. Nair, F. Fagnani, S. Zampieri, and R. J. Evans, \u201cFeedback control under data rate constraints: An overview,\u201d Proceedings of the IEEE, vol. 95, no. 1, pp. 108\u2013137, 2007. [6] J. Hespanha, P. Naghshtabrizi, and Y. Xu, \u201cA survey of recent results in networked control systems,\u201d Proceedings of the IEEE, vol. 95, no. 1, pp. 138\u2013162, 2007. [7] A. S. Matveev and A. V. Savkin, Estimation and Control over Communication Networks. Birkh\u00e4user, 2009. [8] W. Bialek, I. Nemenman, and N. Tishby, \u201cPredictability, complexity, and learning,\u201d Neural computation, vol. 13, pp. 2409\u20132463, 2001. [9] G. Lipsa and N. C. Martins, \u201cFinite horizon optimal memoryless control of a delay in gaussian noise: A simple counterexample,\u201d in Proceedings of the 47th IEEE Conference on Decision and Control, 2008, pp. 1628\u20131635. [10] F. Carravetta, P. Palumbo, and P. Pepe, \u201cMemoryless solution to the optimal control problem for linear systems with delayed input,\u201d Kybernetika, vol. 49, no. 4, pp. 568\u2013589, 2013. [11] F. Cacace, F. Conte, and A. Germani, \u201cMemoryless approach to the LQ and LQG problems with variable input delay,\u201d IEEE Transactions on Automatic Control, vol. 61, no. 1, pp. 216\u2013221, 2016. [12] M. L. Littman, \u201cMemoryless policies: Theoretical limitations and practical results,\u201d in From Animals to Animats 3: Proceedings of the Third International Conference on Simulation of Adaptive Behavior, vol. 3. MIT Press, 1994, pp. 238\u2013245. [13] S. P. Singh, T. S. Jaakkola, and M. I. Jordan, \u201cLearning without state-estimation in partially observable Markovian decision processes.\u201d in Proceedings of the 11th International Conference on Machine Learning, 1994, pp. 284\u2013292. [14] R. Fox and N. Tishby, \u201cOptimal selective attention in reactive agents,\u201d arXiv preprint arXiv:1512.08575, 2015. [15] \u2014\u2014, \u201cMinimum-information LQG control \u2014 Part II: Retentive controllers,\u201d in Proceedings of the 55th IEEE Conference on Decision and Control (CDC), 2016. [Online]. Available: https: //arxiv.org/abs/1606.01947 [16] D. F. Delchamps, \u201cStabilizing a linear system with quantized state feedback,\u201d IEEE Transactions on Automatic Control, vol. 35, no. 8, pp. 916\u2013924, 1990. [17] W. S. Wong and R. W. Brockett, \u201cSystems with finite communication bandwidth constraints\u2014II: Stabilization with limited information feedback,\u201d IEEE Transactions on Automatic Control, vol. 44, no. 5, pp. 1049\u20131053, 1999. [18] G. N. Nair and R. J. Evans, \u201cStabilization with data-rate-limited feedback: tightest attainable bounds,\u201d Systems & Control Letters, vol. 41, pp. 49\u201356, 2000. [19] N. Elia and S. K. Mitter, \u201cStabilization of linear systems with limited information,\u201d IEEE Transactions on Automatic Control, vol. 46, no. 9, pp. 1384\u20131400, 2001. [20] J. Baillieul, \u201cFeedback designs in information-based control,\u201d in Stochastic Theory and Control, ser. Lecture Notes in Control and Information Sciences. Springer, 2002, vol. 280, pp. 35\u201357. [21] N. G. Dokuchaev and A. V. Savkin, \u201cA new class of hybrid dynamical systems: State estimators with bit-rate constraints,\u201d International Journal of Hybrid Systems, vol. 1, no. 1, pp. 33\u201350, 2001.\n[22] N. Elia, \u201cWhen Bode meets Shannon: Control-oriented feedback communication schemes,\u201d IEEE Transactions on Automatic Control, vol. 49, no. 9, pp. 1477\u20131488, 2004. [23] A. Sahai and S. K. Mitter, \u201cThe necessity and sufficiency of anytime capacity for stabilization of a linear system over a noisy communication link\u2014Part I: Scalar systems,\u201d IEEE Transactions on Information Theory, vol. 52, no. 8, pp. 3369\u20133395, 2006. [24] V. S. Borkar and S. K. Mitter, \u201cLQG control with communication constraints,\u201d in Communications, Computation, Control, and Signal Processing. Springer, 1997. [25] N. Elia, \u201cRemote stabilization over fading channels,\u201d Systems & Control Letters, vol. 54, pp. 237\u2013249, 2005. [26] D. Williamson and K. Kadiman, \u201cOptimal finite wordlength linear quadratic regulation,\u201d IEEE Transactions on Automatic Control, vol. 34, no. 12, pp. 1218\u20131228, 1989. [27] L. Xiao, M. Johansson, H. Hindi, S. Boyd, and A. Goldsmith, \u201cJoint optimization of communication rates and linear systems,\u201d IEEE Transactions on Automatic Control, vol. 48, no. 1, pp. 148\u2013153, 2003. [28] S. Tatikonda, A. Sahai, and S. Mitter, \u201cControl of LQG systems under communication constraints,\u201d in Proceedings of the 37th IEEE Conference on Decision and Control, vol. 1, 1998, pp. 1165\u20131170. [29] \u2014\u2014, \u201cStochastic linear control over a communication channel,\u201d IEEE Transactions on Automatic Control, vol. 49, no. 9, pp. 1549\u20131561, 2004. [30] T. Tanaka and H. Sandberg, \u201cSDP-based joint sensor and controller design for information-regularized optimal LQG control,\u201d in Proceedings of the 54th IEEE Conference on Decision and Control, 2015, pp. 4486\u20134491. [31] T. Tanaka, P. M. Esfahani, and S. K. Mitter, \u201cLQG control with minimal information: Three-stage separation principle and SDP-based solution synthesis,\u201d arXiv preprint arXiv:1510.04214, submitted for publication. [32] A. S. Matveev and A. V. Savkin, \u201cThe problem of LQG optimal control via a limited capacity communication channel,\u201d Systems & control letters, vol. 53, pp. 51\u201364, 2004. [33] L. Bao, M. Skoglund, and K. H. Johansson, \u201cIterative encodercontroller design for feedback control over noisy channels,\u201d IEEE Transactions on Automatic Control, vol. 56, no. 2, pp. 265\u2013278, 2011. [34] E. Shafieepoorfard and M. Raginsky, \u201cRational inattention in scalar LQG control,\u201d in Proceedings of the 52nd IEEE Conference on Decision and Control, 2013, pp. 5733\u20135739. [35] H. S. Witsenhausen, \u201cSeparation of estimation and control for discrete time systems,\u201d Proceedings of the IEEE, vol. 59, no. 11, pp. 1557\u2013 1566, 1971. [36] T. Tanaka, K.-K. K. Kim, P. A. Parrilo, and S. K. Mitter, \u201cSemidefinite programming approach to Gaussian sequential rate-distortion tradeoffs,\u201d arXiv preprint arXiv:1411.7632, submitted for publication. [37] T. M. Cover and J. A. Thomas, Elements of Information Theory. Wiley, 2012. [38] H. S. Witsenhausen, \u201cA counterexample in stochastic optimum control,\u201d SIAM Journal on Control, vol. 6, no. 1, pp. 131\u2013147, 1968. [39] S. Mitter and A. Sahai, \u201cInformation and control: Witsenhausen revisited,\u201d in Learning, Control and Hybrid Systems, ser. Lecture Notes in Control and Information Sciences. Springer, 1999, vol. 241, pp. 281\u2013293. [40] T. Berger, Rate Distortion Theory: A Mathematical Basis for Data Compression, ser. Information and System Sciences. Prentice-Hall, 1971. [41] R. Fox and N. Tishby, \u201cBounded planning in passive POMDPs,\u201d in Proceedings of the 29th International Conference on Machine Learning, 2012, pp. 1775\u20131782. [42] E. I. Silva, M. S. Derpich, and J. \u00d8stergaard, \u201cA framework for control system design subject to average data-rate constraints,\u201d IEEE Transactions on Automatic Control, vol. 56, no. 8, pp. 1886\u20131899, 2011. [43] M. Gastpar, B. Rimoldi, and M. Vetterli, \u201cTo code, or not to code: Lossy source-channel communication revisited,\u201d IEEE Transactions on Information Theory, vol. 49, no. 5, pp. 1147\u20131158, 2003.\n60"}, {"heading": "3.2 Part II: Retentive Controllers", "text": "Accepted for publication: Roy Fox and Naftali Tishby, Minimum-Information LQG Control \u2014 Part II: Retentive Controllers, In Proceedings of the 55th"}, {"heading": "IEEE Conference on Decision and Control (CDC), 2016.", "text": "61\nMinimum-Information LQG Control Part II: Retentive Controllers\nRoy Fox\u2020 and Naftali Tishby\u2020\nAbstract\u2014 Retentive (memory-utilizing) sensing-acting agents may operate under limitations on the communication between their sensing, memory and acting components, requiring them to trade off the external cost that they incur with the capacity of their communication channels. In this paper we formulate this problem as a sequential rate-distortion problem of minimizing the rate of information required for the controller\u2019s operation under a constraint on its external cost. We reduce this bounded retentive control problem to the memoryless one, studied in Part I of this work [1], by viewing the memory reader as one more sensor and the memory writer as one more actuator. We further investigate the structure of the resulting optimal solution and demonstrate its interesting phenomenology.\nI. INTRODUCTION\nIn a feedback-control system, the internal state of the agent interacts with the external state of the world through sensors that pay attention to the agent\u2019s environment and actuators that apply intention to it, in a perception-action cycle [2]. This interaction is limited by external constraints on observability and controllability, as well as internal constraints on the information-processing resources available to the controller.\nIn Part I of this work [1], we focused on memoryless controllers that have no internal memory and can only attend to their most recent input observation. We discussed how the communication from the sensor to the actuator is central to the agent\u2019s ability to act upon the perceived information. The degree of this attention, measured by the amount of Shannon information about the input observation that is utilized in the output control, is a lower bound on the required capacity of the communication channel between the controller\u2019s sensor and its actuator. When this capacity for internal communication is limited, the agent needs to trade off some external cost for reducing the rate at which it transmits information.\nA related but often overlooked resource is memory bandwidth. We can think of memory as a communication channel from the past internal state of the controller to its future internal state. When memory resources are remote, communication constraints apply to them as well. Even local memory is limited by its capacity to store information and by the capacity of the internal communication channels to and from the memory components. This limitation is evidenced by the hierarchical design of memory in modern digital\n\u2020School of Computer Science and Engineering, The Hebrew University, {royf,tishby}@cs.huji.ac.il\n\u21e4This work was supported by the DARPA MSEE Program, the Gatsby Charitable Foundation, the Israel Science Foundation and the Intel ICRI-CI Institute\ncomputers, which places larger capacity on the channels to closer but smaller cache memory components [3].\nWhen the controller is retentive (memory-utilizing), it does maintain an internal memory state which can have information on more than the most recent observation. As in Part I, our guiding principle in this work is to measure the information complexity of the controller\u2019s internal representation by asking \u201cHow much information does the controller have on the past?\u201d. The retentive controller receives information of the past through both memory and sensory channels (Figure 2) and the amount of information that it keeps of the past is a lower bound on the total capacity of both these channels [4].\nIn a sense, we can consider the reader of the memory state to be one more sensor and the writer of the memory state to be one more actuator. This suggests a reduction from the retentive case to the memoryless case, in which the memory state is considered external and part of the world state [5], [6]. This memory component is fully observable, fully controllable, has no process noise and incurs no cost. Rather than redevelop our results for the retentive controllers similarly to Part I, this reduction allows us to reuse those results and underlines the structure of the solution.\nIn this paper we make two contributions. First, we present a method for the design of controllers that are optimal under a constraint on both their memory and sensory channel capacity. To our knowledge, this is the first explicit treatment of the channel capacity of the memory process in the context of continuous state-space systems.\nSecond, we provide a reduction from the problem of bounded retentive control to the problem of bounded memoryless control. This reduction is conceptually convenient and constructive, allowing us to treat both problems using the same framework and providing insight into the structure of the optimal retentive controller.\nIn Section II we define the LQG task and restate the results of Part I. In Section III we present the retentive control model, its reduction to memoryless control and the structure of the resulting optimal solution. In Section IV we illustrate our results with an example.\nII. PRELIMINARIES\nA. Control task\nWe consider the same closed-loop control problem detailed in Part I [1, Section II]. In time t, a plant in state xt 2 Rn emits an observation yt 2 Rk, takes in a control input ut 2 R and undergoes a stochastic state transition. We focus on discrete-time systems with linear dynamics,\n62\nGaussian noise and quadratic cost rate (LQG). For simplicity, all elements are taken to be homogeneous, i.e. centered at the origin, and time-invariant. We note that all our results hold without these assumptions, with the appropriate adjustments, as usual in LQG problems [7].\nDefinition 1: A linear-Gaussian time-invariant (LTI) plant A, B, C, , has state dynamics\nxt+1 = Axt + But + t; t \u21e0 N (0, ), (1) where A 2 Rn n, B 2 Rn , 0 2 Sn+ and t is independent of (xt, yt, ut). The observation dynamics are\nyt = Cxt + \u270ft; \u270ft \u21e0 N (0, ), (2) where C 2 Rk n, 2 Sk+ and \u270ft is independent of (yt 1, ut 1, xt), where we denote xt = {x\u2327}\u2327 t, etc.\nDefinition 2: A linear-quadratic-Gaussian (LQG) task A, B, C, , , Q, R involves a LTI plant and the cost rate\nJt = 12 (x t Qxt + u t Rut),\nwhere Q 2 Sn+ and R 2 S +. The task is to achieve a low long-term average expected cost rate, with respect to the distribution induced by the plant and the controller \u21e1\nJ\u21e1 = lim sup T!1\n1\nT\nTX\nt=1 E\u21e1[Jt]. As motivated in Part I, we are particularly interested in linear-Gaussian time-invariant (LTI) controllers, which induce, jointly with a LTI plant, a stationary Gaussian process, independent of any initial conditions. With x 2 Sn+, y 2 Sk+ and u 2 S +, respectively the stationary covariances of the state, the observation and the control, we have\ny = C x C + ,\nand the reverse relation\nxt = Kyt + t; t \u21e0 N (0, ) K = x C\n\u2020y = x x C \u2020y C x,\nwith \u00b7\u2020 the Moore-Penrose pseudoinverse. Assuming that the process has mean 0, the stationary expected cost rate is\nJ\u21e1 = 12 (tr(Q x) + tr(R u)). B. Bounded memoryless control\nIn this section we restate the main result of Part I [1, Section IV].\nDefinition 3: A memoryless linear-Gaussian time-invariant (LTI) controller has control law of the form\nut = Hyt + \u2318t; \u2318t \u21e0 N (0, ), (3) where H 2 R k, 2 S + and \u2318t is independent of (ut 1, xt, yt).\nThe controller is bounded and operates under limitations on its capacity to process the observation and produce the control. Namely, with the Shannon information rate\nIt = I[yt; ut] = E log\nf(yt, ut)\nf(yt)f(ut)\n, (4)\nwhere f denotes the various probability density functions, as indicated by their arguments, we are interested in a LTI controller \u21e1 that minimizes the long-term average rate\nI\u21e1 = lim sup T!1\n1\nT\nTX\nt=1\nIt, (5)\nunder the constraint that it achieves some guarantee level c of expected cost rate.\nProblem 1: Given a LQG task, the bounded memoryless LTI controller optimization problem is\nmin \u21e1 I\u21e1 s.t. J\u21e1  c,\nwith I\u21e1 as in (5), where It = I[yt; ut], and with ut as in (3). To solve the optimization problem, we consider the minimum mean square error (MMSE) estimators\nx\u0302yt = E[xt|yt] = Kyt x\u0302ut = E[xt|ut] = x;u \u2020u ut,\nrespectively for the state given the observation and the control. Since x\u0302ut is a sufficient statistic of ut for xt, we can reverse their causality, basing ut on x\u0302ut instead of vice versa. This puts the control law in the form\nx\u0302yt = Kyt x\u0302ut = Wx\u0302yt + !t; !t \u21e0 N (0, !) ut = Lx\u0302ut .\nThe optimal memoryless controller satisfies the conditions of Theorem 1 in Part I, Section IV-A, restated below in algorithmic form. To numerically find the optimal solution, we can interpret these conditions as update equations, which we apply iteratively until a fixed point is reached.\nWe split the equations into three parts, a forward iteration (Algorithm 1) updating the marginal distributions, a backward iteration (Algorithm 2) updating the cost-to-go and the control policy, and an eigenvalue decomposition (EVD) for finding the control-based estimator covariance (Algorithm 3). We can alternate between Algorithms 1, 2 and 3, iterating until the solution converges to a fixed point of the equations.\nIII. BOUNDED RETENTIVE CONTROLLERS\nA. Control model\nIn this section we discuss retentive (memory-utilizing) controllers with bounded communication resources. A retentive controller has an internal memory state zt in some space Z . The memory allows the controller to output a control that indirectly depends on past input observations rather than only on the most recent observation. The controller takes as input an observation yt and outputs a control ut, while also making a memory state transition from zt 1 to zt. Thus, in each time step, there are two inputs, zt 1 and yt, and two outputs, zt and ut.\nDefinition 4: A controller is retentive if it satisfies the following independence properties:\n63\nAlgorithm 1 Forward iteration function FORWARD( x, x\u0302u , L)\nUpdate\nx (A + BL) x\u0302u(A + BL) + A( x x\u0302u)A +\ny C x C + K x C \u2020y\nx\u0302y K y K\nend function\nAlgorithm 2 Backward iteration function BACKWARD( x\u0302y , x\u0302u , K, S; )\nUpdate\nM 1C K ( \u2020x\u0302y|x\u0302u \u2020 x\u0302y )KC\nS Q + A SA M L (R + B SB)\u2020B SA N L (R + B SB)L\nend function\nAlgorithm 3 Activation of control-based estimator modes function ACTIVATION( x\u0302y , N ; )\nUpdate\nV, EVD( 1/2x\u0302y N 1/2 x\u0302y )\nwith n rank( x\u0302y ) columns of V spanning ker( x\u0302y )\nD diag 1 1 1i i > 1 0 i  1\nx\u0302u 1/2 x\u0302y V DV 1/2 x\u0302y\nend function\n1) The memory state depends only on the previous memory state and the current observation; that is, zt is independent of (zt 2, yt 1, ut 1, xt) given zt 1 and yt. 2) The control depends only on the memory state; that is, ut is independent of (zt 1, ut 1, xt, yt) given zt.\nA system including a retentive controller satisfies the Bayesian network in Figure 1.\nAs motivated in Part I for the memoryless case, we are particularly interested in controllers where both the memory state update and the control are linear-Gaussian and timeinvariant (LTI), since they are easier to optimize and implement. Linear controllers with limited memory are known not to be optimal for all control problems [8], [9]. The conditions under which such controllers are optimal for our bounded control problem are beyond our current scope.\nDefinition 5: A retentive linear-Gaussian time-invariant (LTI) controller has memory state space that is a vector space\nxt 1\nut 2\nzt 2\nyt 1\nzt 1\nut 1\nxt\nyt\nzt\nut\nxt+1\nFig. 1. Bayesian network of retentive control\nPlant xt\nObservation yt\nSensor encoder\nActuator decoder\nMemory (delay)\nControl ut\nzt\nr bits\nFig. 2. Block diagram of a closed-loop retentive control system, with a communication channel from the sensor-reader to the actuator-writer\nZ = Rd and control law of the form zt = Fzt 1 + Gyt + t; t \u21e0 N (0, ) (6a) ut = Lzt + \u232bt; \u232bt \u21e0 N (0, \u232b) (6b)\nwhere F 2 Rd d, G 2 Rd k, 2 Sd+, L 2 Rm d, \u232b 2 Sm+ , t is independent of (zt 1, yt) and \u232bt is independent of zt.\nWe are interested in reducing the information complexity of implementing this controller. To measure this complexity, we consider the capacity of a memoryless communication channel from the sensor-reader to the actuator-writer (Figure 2). The encoder and the decoder themselves are memoryless, but the memory component has perfect fidelity, making everything written by the actuator available for the sensor to read in the next step.\nWe could use Z = {0, 1}r, the set of r-bit strings, instead of the vector space Rd, to indicate that the controller can process at most r bits of information per time step\nI[zt 1, yt; zt, ut] = I[zt 1, yt; zt]  H[zt]  r log 2. As in the memoryless case (Part I [1, Section III-B]), the information rate is generally not a tight lower bound on the capacity of a discrete memory, but here again, if the controller is LTI, there exists a perfectly matched memoryless additive Gaussian noise channel. As shown in the Supplementary Material1 (SM), Appendix I, the capacity of this channel optimally equals the information rate I[zt 1, yt; zt, ut] and a constraint on the information rate is equivalent to a constraint on the power available for transmission on the channel.\nThe retentive controller optimization problem is therefore similar to Problem 1, but with the information rate including both the memory and the sensory channels.\n1Available at https://arxiv.org/abs/1606.01947\n64\nProblem 2: Given a LQG task, the bounded retentive LTI controller optimization problem is\nmin \u21e1 I\u21e1 s.t. J\u21e1  c,\nwith I\u21e1 as in (5), where\nIt = I[zt 1, yt; zt, ut], (7)\nand with zt and ut as in (6). Note that here there is no additional constraint or cost on the precision of ut given zt, implying that optimally \u232b = 0. There is an interesting connection between the retentive information rate I\u21e1 and the long-term average of the directed information rate [10], [11], defined by\nI[{yt}! {zt}] = lim sup T!1\n1 T I[yT ! zT ]\n= lim sup T!1\n1\nT\nTX\nt=1\nI[yt; zt|zt 1].\nBy the independence properties of the retentive controller and by the chain rule for information [12], we have\nI[zt 1, yt; zt, ut] = I[zt 1, yt; zt] = I[zt 1, yt; zt] = I[zt 1; zt] + I[yt; zt|zt 1].\nWe can thus define the following extension of the concept of directed information.\nDefinition 6: The retentive directed information from the sequence of observations yT to the sequence of memory states zT is\nI[yT zT ] = TX\nt=1\nI[zt 1, yt; zt].\nSince I[yT zT ] I[yT ! zT ], the retentive directed information rate is always a tighter lower bound on the capacity of the channel in Figure 2. Despite the apparent similarity to Figure 2 in [11], notice that their encoder and decoder have unlimited memory of zt and ut. This justifies their use of directed information, regardless of the residual term I[zt 1; zt] being infinite in their optimal controller.\nSome further properties of the retentive directed information can be found in the SM, Appendix VI.\nB. Reduction to memoryless controllers\nWe can analyze the bounded retentive control problem (Problem 2) directly using the same tools developed in Part I [1, Section IV-A] for Problem 1. Fortunately, there is no need to repeat that entire treatment, since a simple and insightful reduction will allow us to reuse the results obtained there.\nWe start by reformulating the problem. The following relaxation and Lemma 1 that shows its equivalence to the original problem allow us to reverse the causality between\nxt 1\nyt 1\nmt 1\nut 1\nmt\nxt\nyt ut\nmt+1\nxt+1\nFig. 3. Bayesian network of relaxed retentive control\nxt 1\nmt 1\nyt 1\nmt 1\nut 1\nmt\nxt\nmt\nyt\nmt\nut\nmt+1\nxt+1\nmt+1\nFig. 4. Bayesian network of relaxed retentive control, redrawn in the form of memoryless control\nut and zt. We need a new notation for the resulting timeshifted memory state sequence and define for each t\nmt = zt 1.\nDefinition 7: A retentive controller is relaxed if ut is not required to be independent of (mt, yt) given mt+1. Thus the relaxed controller satisfies the Bayesian network in Figure 3 and its control law is given by \u21e1(ut, mt+1|mt, yt).\nLemma 1: The relaxed controller optimization problem is equivalent to the original Problem 2.\nProof: The following proof does not assume that the controller is linear-Gaussian and holds for the LTI controller as a special case.\nLet \u21e1 be a controller satisfying the Bayesian network in Figure 3. We construct a controller \u21e1\u0303 with z\u0303t = (ut, mt+1) for each t, such that\n\u21e1\u0303(z\u0303t|z\u0303t 1, yt) = \u21e1(ut, mt+1|mt, yt) \u21e1\u0303(ut|z\u0303t) = z\u0303t=(ut,\u00b7).\nThis controller satisfies the Bayesian network in Figure 1 and\nI\u21e1\u0303[z\u0303t 1, yt; z\u0303t, ut] = I\u21e1[(ut 1, mt), yt; (ut, mt+1)] = I\u21e1[mt, yt; ut, mt+1].\nThus the controller \u21e1\u0303 is feasible for the unrelaxed Problem 2 and has the same performance as the relaxed controller \u21e1, since it induces a stochastic process with the same distribution and information rate.\nThe structure in Figure 3 can now be redrawn as in Figure 4. Comparing this Bayesian network to the one in Part I, Figure 2, we have clearly reduced the bounded retentive control problem to a special case of the bounded memoryless control problem, as stated formally in the following lemma.\nLemma 2: The bounded retentive LTI controller optimization problem (Problem 2) for the LQG task Ax, Bx;u, Cy;x, , , Qx, Ru is equivalent to the\n65\nbounded memoryless LTI controller optimization problem (Problem 1) for the LQG task A, B, C, \u0303, \u0303, Q, R , where\nA = Ax 0 0 0 ; B = Bx;u 0 0 I ; C = Cy;x 0 0 I\n\u0303 = 0 0 0 ; \u0303; = 0 0 0\nQ = Qx 0 0 0 ; R = Ru 0 0 0 .\nHere all matrices are extended by d rows and d columns. Proof: Given the retentive control stochastic process {xt, mt, yt, ut}, we consider the memoryless control stochastic process {x\u0303t, y\u0303t, u\u0303t} with\nx\u0303t = xt mt ; y\u0303t = yt mt ; u\u0303t = ut mt+1 .\nThe dynamics for this process can easily be seen to be given by (1), (2), with A, B, C, \u0303 and \u0303 as in the lemma. The cost rate applies only to the xt and ut parts\nJt = 12\nxt mt\nQx 0 0 0 xt mt\n+\nut\nmt+1\nRu 0 0 0\nut\nmt+1\n.\nThe information rate is\nIt = I[y\u0303t; u\u0303t] = I[mt, yt; ut, mt+1], where the left-hand side is taken as in (4) and the right-hand side as in (7), as required.\nC. Structure of the optimal solution\nWe can substitute the form of the reduction in Lemma 2 into the optimal solution in Section II-B, to study more explicitly the structure of the optimal solution in the retentive case. The detailed derivations can be found in the SM, Appendix VII.\nFor the backward process, it is useful to borrow notation from the forward process and denote\nS =\nSx Sx;m\nSm;x Sm\nSx|m = Sx Sx;mS\u2020mSm;x Su|m = R + B Sx|mB.\nThen we can find the feedback gain\nL = (R + B SB)\u2020B SA\n=\nLu;x|m 0\nS\u2020mSm;x(Ax + Bx;uLu;x|m) 0\n, (8)\nwith a memory-conditioned form of the classic feedback gain\nLu;x|m = S\u2020u|mB x;uSx|mAx. The memory-conditioned cost reduction matrix is\nN = L (R + B SB)L = Nx|m 0\n0 0\n,\nwith\nNx|m = A x(Sx Sx|m + Sx|mBx;uS\u2020u|mB x;uSx|m)Ax.\nThus rank(D)  rank(N)  n, with D the mode activation matrix (see Algorithm 3), implying that at most n modes can be active.\nThe d rightmost columns in (8) are 0, implying that u\u0303t depends only on the state estimator x\u0302u\u0303t = E[xt|u\u0303t] of xt and not on an estimator of the memory component mt. Since x\u0302y\u0303t = E[xt|y\u0303t] is a sufficient statistic of y\u0303t for xt, we also have the Markov chain\nxt \u2014 x\u0302y\u0303t \u2014 y\u0303t \u2014 \u02c6\u0303xy\u0303t \u2014 \u02c6\u0303xu\u0303t \u2014 x\u0302u\u0303t \u2014 u\u0303t,\nwith\n\u02c6\u0303xy\u0303t = E[x\u0303t|y\u0303t] = E xt mt\nyt mt\nand similarly for \u02c6\u0303xu\u0303t This implies that we need only consider the first component x\u0302y\u0303t of \u02c6\u0303xy\u0303t , which is obtained from the observation y\u0303t using\nK = x C \u2020y\u0303\n= Kx;y|m (I Kx;y|mCy;x) x;m \u2020m ,\nwhere\nKx;y|m = x|m C y;x \u2020 y|m\nis the Kalman gain that performs optimal inference in the classic LQG task [7].\nCrucially, we see that x\u0302y\u0303t depends on mt only through\nx\u0302mt = E[xt|mt] = x;m \u2020m mt. This implies that, for a controller \u21e1, we can design an equivalent controller \u21e10 whose memory state is the MMSE estimator m0t = x\u0302mt . The feedback gain for \u21e1 0 is\nL0 =\nI 0 0 x;m \u2020 m L.\nNote that, since m0t is a sufficient statistic of mt for xt, we have x|m0 = x|m and Kx;y|m0 = Kx;y|m. Thus\nK 0 = Kx;y|m I Kx;y|mCy;x ,\nwith x;m0 \u2020 m0 = m0 \u2020 m0 in the second component omitted due to its redundancy. The controllers \u21e1 and \u21e10 generate the same control ut and thus incur the same external cost. At the same time, since m0t is a function of mt, by the data-processing inequality the information rate of \u21e10 is at most that of \u21e1. Thus any controller can be converted into a MMSE controller without loss of performance, allowing us to consider the MMSE controller canonical. In particular, this proves again that d = n is always sufficient for representing the memory state.\nWe now diverge from the solution given in Section II-B, which has freedom in its choice of memory representation, and is therefore not guaranteed to be a MMSE controller. Instead, we explicitly constrain the controller to be MMSE, which in return enables us to relax some of the conditions\n66\nm1 m2\nk1\nc1\nk2\nc2\nk1\nc1\nx1 u1\nx2 u2\ncontinuous-time dynamics of this system are given by\nA =\n0 1 0 0\nk1+k2m1 c1+c2 m1 k2 m1\nc2 m1\n0 0 0 1 k2 m2 c2 m2 k1+k2m2 c1+c2 m2\nB =\n0 0 1\nm1 0\n0 0 0 1m2\nC = 1 0 0 0 0 0 1 0 ,\nwith m1 = 5 kg, m2 = 15 kg, k1 = 1 N/m, k2 = 0.5 N/m and c1 = c2 = 1 N\u00b7sec/m. We discretize the time using the Tustin transformation with sampling frequency 20Hz and consider the isotropic noises and cost rates\n= I = I Q = I R = I.\nFor the memoryless control problem, we initialize a solution with x = S = 0. For the retentive control problem, we apply the reduction in Lemma 2 to obtain a reduced plant and then initialize a solution using the classic LQG controller, as described in Section III-C. To the initial solution, we apply the forward-backward iterations of Section II-B, with fixed , until convergence to a fixed point, suspected as a global optimum. To improve running time, we employ a reverseannealing scheme, decreasing gradually over its range and using the fixed point for one value of to initialize the iterations for the next value of .\nFigures 6 and 7 show, respectively, the resulting cost-logbeta and cost-information curves, demonstrating that even this simple example exhibits interesting phenomenology.\nWe see that both the memoryless (blue) and the retentive (green) controllers undergo phase transitions as increases. The system is controllable and observable, allowing the retentive controller to undergo 4 phase transitions, until it fully remembers and controls all modes of the system. However, the rank-2 matrices B and C only allow the memoryless controller to undergo 2 phase transitions and reach order d = 2.\nIn the first phase transition, the controllers begin controlling a single mode, in order to reduce the external cost, at the expense of communication resources. This is not depicted in the cost-information plot (Figure 7), since below this critical point the information is 0 and the cost is fixed.\nThe second and fourth phase transitions involve memory and only occur in the retentive controller. Below these critical\npoint, a hypothetical order-2 retentive controller is worse than the order-1 controller, in terms of the target F , the total external and internal cost-to-go it incurs. At the critical point, the order-2 controller overtakes the order-1 controller, already with a significantly reduced cost rate and a significant information rate (see red dots in Figures 6 and 7). The critical point is where the ratio between these costs is 1 (see (12) in Part I [1, Section IV-B]).\nThe third phase transition is again common to the memoryless and the retentive controllers, although by now the retentive controller has committed to memory much valuable information, reducing the cost much beyond the capabilities of the memoryless controller.\nV. DISCUSSION\nIn this paper we introduce the problem of optimal LQG control with bounded channel capacity in both the memory and the sensory channels. We show how to reduce this problem to that of bounded memoryless LQG control, study the structure of the resulting solution and illustrate its interesting phenomenology with a simple example.\nOne aspect of this phenomenology that merits further study is the existence of suboptimal fixed points of the iterative algorithm (Section II-B). For example, around the second critical point in the double mass-spring-damper system (Section IV), both an order-1 controller and a retentive order-2 controller are fixed points. Before the phase transition, one of these solutions is stable, while the other is metastable and suboptimal, and at the phase transition they switch. This resembles well-studied phenomena in statistical physics.\nLQG control with constraints on the sensory channel capacity has now been studied in the regime of unlimited memory [11], no memory (Part I of this work [1]) and in this paper, a shared channel capacity for sensing and memory. More generally, the memory and the sensory channels can\nbe separate, with their relative costs ranging from 0 (no memory) to 1 (shared capacity) to1 (unlimited memory) including any intermediate value. This memory-sensory tradeoff has been studied in the context of finite-state systems [4] and further insight can be gained from studying this more general problem in the LQG context.\nREFERENCES [1] R. Fox and N. Tishby, \u201cMinimum-information LQG control \u2014\nPart I: Memoryless controllers,\u201d in Proceedings of the 55th IEEE Conference on Decision and Control (CDC), 2016. [Online]. Available: https://arxiv.org/abs/1606.01946 [2] V. Cutsuridis, A. Hussain, and J. G. Taylor, Perception-Action Cycle: Models, Architectures, and Hardware, ser. Cognitive and Neural Systems. Springer, 2011. [3] D. A. Patterson, \u201cLatency lags bandwith,\u201d Communications of the ACM, vol. 47, no. 10, pp. 71\u201375, 2004. [4] R. Fox and N. Tishby, \u201cBounded planning in passive POMDPs,\u201d in Proceedings of the 29th International Conference on Machine Learning, 2012, pp. 1775\u20131782. [5] \u2014\u2014, \u201cOptimal selective attention in reactive agents,\u201d arXiv preprint arXiv:1512.08575, 2015. [6] N. Meuleau, K.-E. Kim, L. P. Kaelbling, and A. R. Cassandra, \u201cSolving POMDPs by searching the space of finite policies,\u201d in Proceedings of the 15th conference on Uncertainty in Artificial Intelligence (UAI). Morgan Kaufmann Publishers Inc., 1999, pp. 417\u2013426. [7] D. P. Bertsekas, Dynamic Programming and Optimal Control. Athena Scientific, 1995, vol. I. [8] H. S. Witsenhausen, \u201cA counterexample in stochastic optimum control,\u201d SIAM Journal on Control, vol. 6, no. 1, pp. 131\u2013147, 1968. [9] S. Mitter and A. Sahai, \u201cInformation and control: Witsenhausen revisited,\u201d in Learning, Control and Hybrid Systems, ser. Lecture Notes in Control and Information Sciences. Springer, 1999, vol. 241, pp. 281\u2013293. [10] J. L. Massey, \u201cCausality, feedback and directed information,\u201d in Proceedings of the International Symposium on Information Theory and Its Applications, 1990, pp. 303\u2013305. [11] T. Tanaka, P. M. Esfahani, and S. K. Mitter, \u201cLQG control with minimal information: Three-stage separation principle and SDP-based solution synthesis,\u201d arXiv preprint arXiv:1510.04214, submitted for publication. [12] T. M. Cover and J. A. Thomas, Elements of Information Theory. Wiley, 2012. [13] F. Creutzig, A. Globerson, and N. Tishby, \u201cPast-future information bottleneck in dynamical systems,\u201d Physical Review E, vol. 79, no. 4, p. 041925, 2009.\n68"}, {"heading": "3.3 Supplementary Material", "text": ""}, {"heading": "Supplementary material for Part I and Part II.", "text": "69\nMinimum-Information LQG Control Supplementary Material\nRoy Fox\u2020 and Naftali Tishby\u2020\nAPPENDIX I PERFECTLY MATCHED CHANNEL\nIn this appendix we construct a channel that is perfectly matched to the sequential source code derived in Theorem 1, in Part I of this paper [1, Section III-B]. Recall that in a perfectly matched source-channel pair the optimal source coding and the optimal channel coding can be implemented jointly for single letters, without requiring longer blocks. This allows us to use them in a perception-action cycle, where we cannot accumulate a block of inputs before emitting an output.\nThe main results of [2], applied to our setting, can be summarized as follows. We wish to find a memoryless channel into which we can input an encoding wt = g(x\u0302yt), such that x\u0302ut = h(w\u0302t) can be decoded from the channel output w\u0302t. Suppose that we are concerned with the power needed to transmit wt and thus the input cost is w t wt. Then the source x\u0302yt and the channel wt ! w\u0302t are perfectly matched if there exist an encoder and a decoder such that\n1) The Kullback-Leibler divergence D[f(w\u0302t|wt)kf(w\u0302t)] between the conditional and marginal densities of w\u0302t, as a function of wt, equals c1w t wt + c2, for some\nconstants c1 0 and c2; and 2) f(x\u0302ut |x\u0302yt) satisfies the conditions in Theorem 1. To meet these conditions, we can choose the channel, the\nencoder and the decoder to have\nwt = D 1/2V\n\u2020/2 x\u0302y x\u0302yt\nw\u0302t = wt + t; t \u21e0 N (0, I D) x\u0302ut = 1/2 x\u0302y V D 1/2w\u0302t,\nwith D and V as in Theorem 1. Then\nw = D\nw\u0302 = I\nx\u0302u = 1/2 x\u0302y V DV 1/2 x\u0302y = x\u0302u;x\u0302y ,\nand it can be verified that\nD[f(w\u0302t|wt)kf(w\u0302t)] = 12w t 1 w\u0302 wt + const,\nas required. The capacity of the additive Gaussian noise channel with noise covariance I D, under the appropriate expected \u2020School of Computer Science and Engineering, The Hebrew University, {royf,tishby}@cs.huji.ac.il \u21e4This work was supported by the DARPA MSEE Program, the Gatsby Charitable Foundation, the Israel Science Foundation and the Intel ICRI-CI Institute\npower constraint, is indeed achieved by a Gaussian input with covariance D and is equal to the information rate in Theorem 1. As shown in [2], this means that constraining the expected power w is equivalent to constraining the information rate I[x\u0302yt ; x\u0302ut ].\nNote, however, that the matched channel noise covariance depends on the constraint, through the solution in Theorem 1. Moreover, this result is not applicable when the best channel available to the designer of the controller is not the matched channel above, in which case both the channel and the sequential source coding generally need to be adapted.\nAPPENDIX II PROOF OF LEMMA 1 OF PART I\nIn this appendix we restate and prove Lemma 1 of Part I [1, Section IV-A].\nLemma 1: Let x and x\u0302 be 0-mean jointly Gaussian random variables. The following properties are equivalent:\n1) There exists a random variable u, jointly Gaussian with x, such that x\u0302(u) = arg minx\u0302 E[kx\u0302 xk2|u] = E[x|u]. 2) x\u0302;x = x\u0302. 3) x|x\u0302 = x x\u0302, where x|x\u0302 is the conditional co-\nvariance matrix of x given x\u0302, implying x x\u0302. 4) x\u0302 = E[x|x\u0302].\nSuch x\u0302 is called a minimum mean square error (MMSE) estimator (of u) for x.\nProof: (1 = 2) Assume without loss of generality that u has mean 0. Then\nx\u0302 = x;u \u2020 u u,\nimplying\nx\u0302;x = x;u \u2020 u u;x = x\u0302 .\n(2 = 3)\nx|x\u0302 = x x;x\u0302 \u2020x\u0302 x\u0302;x = x x\u0302 .\n(3 = 4) Since x and x\u0302 are 0-mean and jointly Gaussian, we can write for some T\nx = T x\u0302 + ; \u21e0 N (0, x|x\u0302),\nimplying\nx = T x\u0302 T + x x\u0302,\nthus without loss of generality T = I .\n70\n(4 = 1) Taking u = x\u0302, we have arg min\nx\u03020 E[kx\u03020 xk2|u]\n= arg min x\u03020\n(x\u03020 x\u03020 2x\u03020 E[x|u]) + E[x x|u],\nwhich is optimized by x\u03020 = E[x|u]. APPENDIX III\nPROOF OF LEMMA 2 OF PART I\nIn this appendix we restate and prove Lemma 2 of Part I [1, Section IV-A].\nLemma 2: The bounded memoryless LTI controller optimization problem (Problem 1) is solved by a control law of the form\nx\u0302yt = Kyt (5a) x\u0302ut = Wx\u0302yt + !t; !t \u21e0 N (0, !) (5b) ut = Lx\u0302ut , (5c)\nwhere W 2 Rn n, ! 2 Rn n, L 2 R n, !t is independent of yt, x\u0302ut is a MMSE estimator for x\u0302yt and\nI[yt; ut] = I[x\u0302yt ; x\u0302ut ]. (6) Proof: Consider a LTI controller \u21e1 of the form\nut = Hyt + \u2318t; \u2318t \u21e0 N (0, ), (III.1) satisfying the Markov network\nxt \u2014 yt \u2014 ut | |\nx\u0302yt x\u0302ut .\nWe now construct a controller \u21e10 with control law u0t based on the estimator x\u03020ut by defining the Markov chain\nxt \u2014 yt \u2014 x\u0302yt \u2014 u 00 t \u2014 x\u0302 0 ut \u2014 u 0 t\nsuch that each consecutive pair of variables has the same joint distribution as their unprimed namesakes. Since x\u0302yt is a sufficient statistic of yt for xt, we have the Markov chain xt \u2014 x\u0302yt \u2014 yt \u2014 ut, implying that u 00 t has the same joint distribution with xt as ut does. Likewise, x\u03020ut has the same joint distribution with xt as x\u0302ut does. Since x\u0302ut is a sufficient statistic of ut for xt, we have that u0t also has the same joint distribution with xt as ut does.\nThus the controller \u21e10 induces the same stochastic process {xt, u0t} and the same external cost. Note that u0t may not have the same joint distribution with yt as ut does and due to the data-processing inequality [3]\nI[yt; ut] I[x\u0302yt ; ut] = I[x\u0302yt ; u00t ] I[x\u0302yt ; x\u03020ut ] I[yt; u0t].\nTherefore \u21e10 performs at least as well as \u21e1 and equally well when \u21e1 is optimal, proving (6).\nx\u03020ut is a MMSE estimator for x\u0302yt since\nE[x\u0302yt |x\u03020ut ] = E[E[xt|yt]|x\u0302u0t ] = E[xt|x\u03020ut ] = x\u03020ut ,\nwhere the second equality follows from xt \u2014 yt \u2014 x\u03020ut . Finally, it may not be clear from the above analysis that u0t is optimally deterministic in x\u0302 0 ut . If ut has covariance \u232b given x\u03020ut , the Lagrangian of the optimization problem ((9) in Part I) depends on \u232b only through the terms\n1 2 (tr(R \u232b) + tr(SB \u232b B )).\nSince R + B SB 0 is positive semidefinite, we can take \u232b = 0 without loss of performance, recovering the structure (5). Intuitively, the argument is that any noise added to u0t, beyond x\u0302 0 ut , is not helpful in compressing xt and can only increase the external cost without saving any communication cost.\nIn the other direction, let ut satisfy the form of Lemma 2. We can rewrite ut in the form (III.1), with\nH = LWK\n= L ! L .\nAPPENDIX IV PROOF OF THEOREM 1 OF PART I\nIn this appendix we restate and prove Theorem 1 of Part I [1, Section IV-A], which relies on the following Lagrangian developed there.\nF x, x\u0302u ,L,S; = 12 ( 1(log | x\u0302y |\u2020 log | x\u0302y|x\u0302u |\u2020) (9) + tr(Q x) + tr(RL x\u0302u L )\n+ tr(S((A + BL) x\u0302u(A + BL)\n+ A x|x\u0302u A + x))).\nTheorem 1: Given , the Lagrangian (9) is minimized by a controller satisfying the forward equations\nx = (A + BL) x\u0302u(A + BL) (10a)\n+ A x|x\u0302u A +\ny = C x C + (10b) K = x C \u2020y (10c)\nx\u0302y = K y K , (10d)\nthe backward equations\nM = 1C K ( \u2020x\u0302y|x\u0302u \u2020 x\u0302y )KC (10e)\nS = Q + A SA M, (10f) L = (R + B SB)\u2020B SA (10g) N = L (R + B SB)L (10h)\nand the control-based estimator covariance\nx\u0302u = 1/2 x\u0302y V DV 1/2 x\u0302y , (10i)\nthe latter determined by the eigenvalue decomposition (EVD)\nV V = 1/2 x\u0302y N 1/2 x\u0302y\n(10j)\nhaving V orthogonal with n rank( x\u0302y ) columns spanning the kernel of x\u0302y and = diag{ i} and by the active mode coefficient matrix\nD = diag 1 1 1i i > 1 0 i  1 . (10k)\n71\nProof: The minimum of the Lagrangian (9) must satisfy the first-order optimality conditions, i.e. that the gradient with respect to each parameter is 0 at the optimum. We start by differentiating F by the feedback gain L L F x, x\u0302u ,L,S; = RL x\u0302u +B S(A + BL) x\u0302u = 0,\nwhich we rewrite as\n(R + B SB)L x\u0302u = B SA x\u0302u . As this equation shows, L is underdetermined in the kernel of x\u0302u , since these modes are always 0 in x\u0302ut and have no effect on ut. L is also underdetermined in the kernel of R + B SB, since these modes have no cost (immediate or future) and can be controlled in any way without affecting the solution\u2019s performance. Thus without loss of performance we can take\nL = (R + B SB)\u2020B SA. We substitute this solution back into the Lagrangian, to\nget\nF x, x\u0302u ,S; = 12 ( 1(log | x\u0302y |\u2020 log | x\u0302y|x\u0302u |\u2020) (IV.1) + tr(M x) tr(N x\u0302u) + tr(S )),\nwith\nM = Q + A SA S N = L (R + B SB)L\n= A SB(R + B SB)\u2020B SA.\nThe problem of optimizing over x\u0302u given the other parameters can now be written, up to constants, as the semidefinite program (SDP)\nmax x\u0302u\nlog | x\u0302y x\u0302u |\u2020 + tr(N x\u0302u)\ns.t. 0 x\u0302u x\u0302y . By Lemma V.1 in Appendix V, the optimum is achieved when x\u0302u satisfies (10i)\u2013(10k).\nFinally, with P = x\u0302y \u2020 x\u0302y\nthe projection onto the support of x\u0302yt and since the range of x\u0302u is contained in that subspace, we have\n( x)i,j (log | x\u0302y |\u2020 log | x\u0302y|x\u0302u |\u2020) = ( x)i,j log |P x\u0302u \u2020x\u0302y |\u2020 = ( x)i,j log |I x\u0302u(P x\u0302y P )\u2020| = tr((I x\u0302u \u2020x\u0302y ) 1 x\u0302u ( x)i,j (P x\u0302y P ) \u2020).\nThe purpose of introducing P is to notice that even if the range of x\u0302y is increased, this has no effect on the Lagrangian, because these modes are orthogonal to the range of x\u0302u . This allows us to treat P as constant, so that the range of P x\u0302y P is constant in a neighborhood of the solution, and the derivative of the pseudoinverse is simplified in this case to\n( x)i,j (P x\u0302y P ) \u2020 = \u2020x\u0302y ( ( x)i,j x\u0302y ) \u2020 x\u0302y\n= \u2020x\u0302y KCJi,jC K \u2020x\u0302y ,\nwith Ji,j the matrix with 1 in position (i, j) and 0 elsewhere. This yields\nx F x, x\u0302u ,S; = 12 (M 1C K \u2020 x\u0302y (I x\u0302u \u2020x\u0302y ) 1 x\u0302u \u2020 x\u0302y KC) = 12 (M 1C K \u2020 x\u0302y ((I x\u0302u \u2020x\u0302y ) 1 I)KC) = 12 (M 1C K ( \u2020 x\u0302y|x\u0302u \u2020 x\u0302y )KC) = 0,\nimplying (10e).\nAPPENDIX V SEMIDEFINITE PROGRAM SOLUTION\nIn this appendix we state and prove the following solution to our SDP problem.\nLemma V.1: The semidefinite program\nmax X Sn+\nlog |M1 X|\u2020 + tr(M2X)\ns.t. X M1, with M1, M2 0, is optimized by\nX = M 1/2 1 V DV M 1/2 1 ,\nwith the eigenvalue decomposition (EVD)\nV V = M 1/2 1 M2M 1/2 1 ,\nsuch that V is orthogonal with n rank(M1) columns spanning the kernel of M1 and = diag{ i} and with\nD = diag 1 1i i > 1 0 i  1 .\nProof: Let the EVD of M1 be\nU U = M1,\nwith U orthogonal and diagonal, having\n= + 0 0 0(n m) (n m) ,\nwith m = rank(M1). Let\n\u2021 = \u2020 + I \u2020 = 1+ 0\n0 I\n.\nBy changing the variable to\nY = \u2021/2U XU \u2021/2,\nthe constraint of the SDP becomes\nY Im,n = Im m 0\n0 0(n m) (n m)\n.\nY must therefore be 0 outside the upper-left m\u21e5m block, and the SDP is equivalent, up to constants, to\nmax Y Sn+\nlog |Im,n Y |\u2020 + tr( 1/2U M2U 1/2Y )\ns.t. Y Im,n. Let the EVD of the linear coefficient be\nV\u0304 V\u0304 = 1/2U M2U 1/2,\n72\nyt 1\nzt 1\nyt\nzt\nyt+1\nzt+1\nwith\nV\u0304 = V\u0304+ 0 0 I(n m) (n m)\northogonal and preserving the kernel of and = diag{ i}. We can again change the variable to\nD = V\u0304 Y V\u0304 ,\nto get\nmax D Sn+\nlog |Im,n D|\u2020 + tr( D)\ns.t. D Im,n, which can easily be solved using Hadamard\u2019s inequality [3], to find\nD = diag 1 1i i > 1 0 i  1 .\nFinally, the lemma follows by unmaking the variable changes and taking\nV = UV\u0304 .\nAPPENDIX VI PROPERTIES OF THE RETENTIVE DIRECTED INFORMATION\nIn this appendix we show how the retentive directed information (Definition 6 of Part II [4, Section III-A]) relates to the multi-information of Bayesian networks [5].\nConsider the Bayesian network in Figure VI.1, which describes the process of online inference from a sequence of independent observations. The multi-information of this network, for horizon T , is equal to the retentive directed information\nI[yT , zT ] = E \" log\nf(yT , zT ) T t=1 f(yt)f(zt)\n#\n= TX\nt=1\nE log f(zt|zt 1, yt) f(zt) = I[yT zT ].\nAn important property of the directed information is that the mutual information between two sequences can be decomposed into the sum of directed information in both directions [6]\nI[xT ; zT ] = I[xT ! zT ] + I[zT ! xT ]. Interestingly, retentive directed information extends this property to the retentive control process (Figure 1 in Part II). This process can be thought of as consisting of four phases:\nwith\nKx;y|m = x|m C y;x \u2020 y|m .\nNow constraining the controller to be MMSE, we have the structure\nx\u0303 =\nx|m + m m\nm m\nK = Kx;y|m I Kx;y|mCy;x ,\nwhich we employ in differentiating F (IV.1), to get\nx|m F x|m, m, x\u0302u\u0303 ,S; = I 0\nx\u0303 F x\u0303, x\u0302u\u0303 ,S; I 0\n= 12 I 0 (M 1C K ZKC) I 0\n= 12\nI 0 M I 0\n1C y;xK x;y|mZKx;y|mCy;x = 0\nm F x|m, m, x\u0302u\u0303 ,S; = I I\nx\u0303 F x\u0303, x\u0302u\u0303 ,S; I I\n= 12 I I (M 1C K ZKC) I I\n= 12\nI I M I I\n1Z = 0,\nwith\nZ = \u2020x\u0302y\u0303|x\u0302u\u0303 \u2020 x\u0302y\u0303 .\nThis leaves M overparameterized and we can choose to give it the structure\nM = Mx|m + Mm Mm Mm Mm\nwith\nMx|m = 1Z\nMm = 1(C y;xK x;y|mZKx;y|mCy;x Z).\nREFERENCES [1] R. Fox and N. Tishby, \u201cMinimum-information LQG control \u2014\nPart I: Memoryless controllers,\u201d in Proceedings of the 55th IEEE Conference on Decision and Control (CDC), 2016. [Online]. Available: https://arxiv.org/abs/1606.01946 [2] M. Gastpar, B. Rimoldi, and M. Vetterli, \u201cTo code, or not to code: Lossy source-channel communication revisited,\u201d IEEE Transactions on Information Theory, vol. 49, no. 5, pp. 1147\u20131158, 2003. [3] T. M. Cover and J. A. Thomas, Elements of Information Theory. Wiley, 2012. [4] R. Fox and N. Tishby, \u201cMinimum-information LQG control \u2014 Part II: Retentive controllers,\u201d in Proceedings of the 55th IEEE Conference on Decision and Control (CDC), 2016. [Online]. Available: https://arxiv.org/abs/1606.01947 [5] N. Friedman, O. Mosenzon, N. Slonim, and N. Tishby, \u201cMultivariate information bottleneck,\u201d in Proceedings of the 17th Conference in Uncertainty in Artificial Intelligence, 2001, pp. 152\u2013161. [6] J. L. Massey, \u201cCausality, feedback and directed information,\u201d in Proceedings of the International Symposium on Information Theory and Its Applications, 1990, pp. 303\u2013305. [7] N. Tishby and D. Polani, \u201cInformation theory of decisions and actions,\u201d in Perception-Action Cycle, ser. Cognitive and Neural Systems. Springer, 2011, pp. 601\u2013636.\n74\nChapter 4"}, {"heading": "Minimum-KL Reinforcement", "text": ""}, {"heading": "Learning", "text": ""}, {"heading": "4.1 Taming the Noise in Reinforcement", "text": "Learning via Soft Updates\nPublished: Roy Fox\u02da, Ari Pakman\u02da and Naftali Tishby, Taming the Noise in Reinforcement Learning via Soft Updates, In Proceedings of the 32nd Conference on Uncertainty in Artificial Intelligence (UAI), 2016.\n\u02daThese authors contributed equally to this work.\n75\nTaming the Noise in Reinforcement Learning via Soft Updates\nRoy Fox\u21e4 Hebrew University\nAri Pakman\u21e4 Columbia University\nNaftali Tishby Hebrew University\nAbstract\nModel-free reinforcement learning algorithms, such as Q-learning, perform poorly in the early stages of learning in noisy environments, because much effort is spent unlearning biased estimates of the state-action value function. The bias results from selecting, among several noisy estimates, the apparent optimum, which may actually be suboptimal. We propose G-learning, a new off-policy learning algorithm that regularizes the value estimates by penalizing deterministic policies in the beginning of the learning process. We show that this method reduces the bias of the value-function estimation, leading to faster convergence to the optimal value and the optimal policy. Moreover, G-learning enables the natural incorporation of prior domain knowledge, when available. The stochastic nature of G-learning also makes it avoid some exploration costs, a property usually attributed only to on-policy algorithms. We illustrate these ideas in several examples, where G-learning results in significant improvements of the convergence rate and the cost of the learning process."}, {"heading": "1 INTRODUCTION", "text": "The need to separate signals from noise stands at the center of any learning task in a noisy environment. While a rich set of tools to regularize learned parameters has been developed for supervised and unsupervised learning problems, in areas such as reinforcement learning there still exists a vital need for techniques that tame the noise and avoid overfitting and local minima.\nOne of the central algorithms in reinforcement learning is Q-learning [1], a model-free off-policy algorithm, which attempts to estimate the optimal value function Q, the\n\u21e4These authors contributed equally to this work.\ncost-to-go of the optimal policy. To enable this estimation, a stochastic exploration policy is used by the learning agent to interact with its environment and explore the model. This approach is very successful and popular, and despite several alternative approaches developed in recent years [2, 3, 4], it is still being applied successfully in complex domains for which explicit models are lacking [5].\nHowever, in noisy domains, in early stages of the learning process, the min (or max) operator in Q-learning brings about a bias in the estimates. This problem is akin to the \u201cwinner\u2019s curse\u201d in auctions [6, 7, 8, 9]. With too little evidence, the biased estimates may lead to wrong decisions, which slow down the convergence of the learning process, and require subsequent unlearning of these suboptimal behaviors.\nIn this paper we present G-learning, a new off-policy information-theoretic approach to regularizing the stateaction value function learned by an agent interacting with its environment in model-free settings.\nThis is achieved by adding to the cost-to-go a term that penalizes deterministic policies which diverge from a simple stochastic prior policy [10]. With only a small sample to go by, G-learning prefers a more randomized policy, and as samples accumulate, it gradually shifts to a more deterministic and exploiting policy. This transition is managed by appropriately scheduling the coefficient of the penalty term as learning proceeds.\nIn Section 4 we discuss the theoretical and practical aspects of scheduling this coefficient, and suggest that a simple linear schedule can perform well. We show that Glearning with this schedule reduces the value estimation bias by avoiding overfitting in its selection of the update policy. We further establish empirically the link between bias reduction and learning performance, that has been the underlying assumption in many approaches to reinforcement learning [11, 12, 13, 14]. The examples in Section 6 demonstrate the significant improvement thus obtained.\nFurthermore, in domains where exploration incurs significantly higher costs than exploitation, such as the classic\n76\ncliff domain [2], G-learning with an \u270f-greedy exploration policy is exploration-aware, and chooses a less costly exploration policy, thus reducing the costs incurred during the learning process. Such awareness to the cost of exploration is usually attributed to on-policy algorithms, such as SARSA [2, 4] and Expected-SARSA [15, 16]. The remarkable finding that G-learning exhibits on-policy-like properties is illustrated in the example of Section 6.2.\nIn Section 2 we discuss the problem of learning in noisy environments. In Section 3 we introduce the penalty term, derive G-learning and prove its convergence. In Section 4 we determine a schedule for the coefficient of the information penalty term. In Section 5 we discuss related work. In Section 6 we illustrate the strengths of the algorithm through several examples."}, {"heading": "2 LEARNING IN NOISY ENVIRONMENTS", "text": ""}, {"heading": "2.1 NOTATION AND BACKGROUND", "text": "We consider the usual setting of a Markov Decision Process (MDP), in which an agent interacts with its environment by repeatedly observing its state s 2 S, taking an action a 2 A, with A and S finite, and incurring cost c 2 R. This induces a stochastic process s0, a0, c0, s1, . . ., where s0 is fixed, and where for t 0 we have the Markov properties indicated by the conditional distributions at \u21e0 \u21e1t(at|st), ct \u21e0 \u2713(ct|st, at) and st+1 \u21e0 p(st+1|st, at). The objective of the agent is to find a time-invariant policy \u21e1 that minimizes the total discounted expected cost\nV \u21e1(s) = X\nt 0 t E[ct|s0 = s], (1)\nsimultaneously for any s 2 S, for a given discount factor 0  < 1. For each t, the expectation above is over all trajectories of length t starting at s0 = s. A related quantity is the state-action value function\nQ\u21e1(s, a) = X\nt 0 t E[ct|s0 = s, a0 = a]\n= E\u2713[c|s, a] + Ep[V \u21e1(s0)|s, a], (2)\nwhich equals the total discounted expected cost that follows from choosing action a in state s, and then following the policy \u21e1.\nIf we know the distributions p and \u2713 (or at least E\u2713[c|s, a]), then it is easy to find the optimal state-action value function\nQ\u21e4(s, a) = min \u21e1 Q\u21e1(s, a) (3)\nusing standard techniques, such as Value Iteration [17]. Our interest is in model-free learning, where the model parameters are unknown. Instead, the agent obtains samples\nfrom p(st+1|st, at) and \u2713(ct|st, at) through its interaction with the environment. In this setting, the Q-learning algorithm [1] provides a method for estimating Q\u21e4. It starts with an arbitrary Q, and in step t upon observing st, at, ct and st+1, performs the update\nQ(st, at) (1 \u21b5t)Q(st, at) (4)\n+ \u21b5t ct +\nX\na0\n\u21e1(a0|st+1)Q(st+1, a0) ! ,\nwith some learning rate 0  \u21b5t  1, and the greedy policy for Q having\n\u21e1(a|s) = a,a\u21e4(s); a\u21e4(s) = arg min a Q(s, a). (5)\nQ(s, a) is unchanged for any (s, a) 6= (st, at). If the learning rate satisfies\nX\nt\n\u21b5t =1; X\nt\n\u21b52t <1, (6)\nand the interaction itself uses an exploration policy that returns to each state-action pair infinitely many times, then Q is a consistent estimator, converging to Q\u21e4 with probability 1 [1, 17]. Similarly, if the update rule (4) uses a fixed update policy \u21e1 = \u21e2, we call this algorithm Q\u21e2-learning, because Q converges to Q\u21e2 with probability 1."}, {"heading": "2.2 BIAS AND EARLY COMMITMENT", "text": "Despite the success of Q-learning in many situations, learning can proceed extremely slowly when there is noise in the distribution, given st and at, of either of the terms of (2), namely the cost ct and the value of the next state st+1. The source of this problem is a negative bias introduced by the min operator in the estimator mina0 Q(st+1, a0), when (5) is plugged into (4).\nTo illustrate this bias, assume that Q(s, a) is an unbiased but noisy estimate of the optimal Q\u21e4(s, a). Then Jensen\u2019s inequality for the concave min operator implies that\nE[min a Q(s, a)]  min a Q\u21e4(s, a), (7)\nwith equality only when Q already reveals the optimal policy by having arg mina Q(s, a) = arg mina Q\n\u21e4(s, a) with probability 1, so that no further learning is needed. The expectation in (7) is with respect to the learning process, including any randomness in state transition, cost, exploration and internal update, given the domain.\nThis is an optimistic bias, causing the cost-to-go to appear lower than it is (or the reward-to-go higher). It is the well known \u201cwinner\u2019s curse\u201d problem in economics and decision theory [6, 7, 8, 9], and in the context of Q-learning it was studied before in [3, 11, 12, 13]. A similar problem occurs when a function approximation scheme is used\n77\nfor Q instead of a table, even in the absence of transition or cost noise, because the approximation itself introduces noise [18].\nAs the sample size increases, the variance in Q(s, a) decreases, which in turn reduces the bias in (7). This makes the update policy (5) more optimal, and the update increasingly similar to Value Iteration."}, {"heading": "2.3 THE INTERPLAY OF VALUE BIAS AND POLICY SUBOPTIMALITY", "text": "It is insightful to consider the effect of the bias not only on the estimated value function, but also on the real value V \u21e1 of the greedy policy (5), since in many cases the latter is the actual output of the learning process. The central quantity of interest here is the gap Q\u21e4(s, a0) V \u21e4(s), in a given state s, between the value of a non-optimal action a0 and that of the optimal action.\nConsider first the case in which the gap is large compared to the noise in the estimation of the Q(s, a) values. In this case, a0 indeed appears suboptimal with high probability, as desired. Interestingly, when the gap is very small relative to the noise, the learning agent should not worry, either. Confusing such a0 for the optimal action has a limited effect on the value of the greedy policy, since choosing a0 is nearoptimal.\nWe conclude that the real value V \u21e1 of the greedy policy (5) is suboptimal only in the intermediate regime, when the gap is of the order of the noise, and neither is small. The effect of the noise can be made even worse by the propagation of bias between states, through updates. Such propagation can cause large-gap suboptimal actions to nevertheless appear optimal, if they lead to a region of state-space that is highly biased."}, {"heading": "2.4 A DYNAMIC OPTIMISM-UNCERTAINTY LOOP", "text": "The above considerations were agnostic to the exploration policy, but the bias reduction can be accelerated by an exploration policy that is close to being greedy. In this case, high-variance estimation is self-correcting: an estimated state value with optimistic bias draws exploration towards that state, leading to a decrease in the variance, which in turn reduces the optimistic bias. This is a dynamic form of optimism under uncertainty. While in the usual case the optimism is externally imposed as an initial condition [19], here it is spontaneously generated by the noise and selfcorrected through exploration.\nThe approach we propose below to reduce the variance is motivated by electing to represent the uncertainty explicitly, and not indirectly through an optimistic bias. We notice that although in the end of the learning process one obtains the deterministic greedy policy from Q(a, s) as\nin (5), during the learning itself the bias in Q can be ameliorated by avoiding the hard min operator, and refraining from committing to a deterministic greedy policy. This can be achieved by adding to Q, at the early learning stage, a term that penalizes deterministic policies, which we consider next."}, {"heading": "3 LEARNING WITH SOFT UPDATES", "text": ""}, {"heading": "3.1 THE FREE-ENERGY FUNCTION G AND G-LEARNING", "text": "Let us adopt, before any interaction with the environment, a simple stochastic prior policy \u21e2(a|s). For example, we can take the uniform distribution over the possible actions. The information cost of a learned policy \u21e1(a|s) is defined as\ng\u21e1(s, a) = log \u21e1(a|s)\u21e2(a|s) , (8)\nand its expectation over the policy \u21e1 is the KullbackLeibler (KL) divergence of \u21e1s = \u21e1(\u00b7|s) from \u21e2s = \u21e2(\u00b7|s),\nE\u21e1[g \u21e1(s, a)|s] = DKL[\u21e1sk\u21e2s]. (9)\nThe term (8) penalizes deviations from the prior policy and serves to regularize the optimal policy away from a deterministic action. In the context of the MDP dynamics p(st+1|st, at), similarly to (1), we consider the total discounted expected information cost\nI\u21e1(s) = X\nt 0 t E[g\u21e1(st, at)|s0 = s]. (10)\nThe discounting in (1) and (10) is justified by imagining a horizon T \u21e0 Geom(1 ), distributed geometrically with parameter 1 . Then the cost-to-go V \u21e1 in (1) and the information-to-go I\u21e1 in (10) are the total (undiscounted) expected T -step costs.\nAdding the penalty term (10) to the cost function (1) gives\nF\u21e1(s) = V \u21e1(s) + 1 I \u21e1(s), (11)\n= X\nt 0 t E[ 1 g \u21e1(st, at) + ct|s0 = s],\ncalled the free-energy function by analogy with a similar quantity in statistical mechanics [10].\nHere is a parameter that sets the relative weight between the two costs. For the moment, we assume that is fixed. In following sections, we let grow as the learning proceeds.\nIn analogy with the Q\u21e1 function (2), let us define the stateaction free-energy function G\u21e1(s, a) as\nG\u21e1(s, a) = E\u2713[c|s, a] + Ep[F\u21e1(s0)|s, a] (12) = X\nt 0 t E[ct + g \u21e1(st+1, at+1))|s0 = s, a0 = a],\n78\nand note that it does not involve the information term at time t = 0, since the action a0 = a is already known. From the definitions (11) and (12) it follows that\nF\u21e1(s) = X\na\n\u21e1(a|s) h 1 log\n\u21e1(a|s) \u21e2(a|s) + G\n\u21e1(s, a) i . (13)\nIt is easy to verify that, given the G function, the above expression for F\u21e1 has gradient 0 at\n\u21e1(a|s) = \u21e2(a|s)e G(s,a)\nP a0 \u21e2(a 0|s)e G(s,a0) , (14)\nwhich is therefore the optimal policy.\nThe policy (14) is the soft-min operator applied to G, with inverse-temperature . When is small, the information cost is dominant, and \u21e1 approaches the prior \u21e2. When is large, we are willing to diverge much from the prior to reduce the external cost, and \u21e1 approaches the deterministic greedy policy for G.\nEvaluated at the soft-greedy policy (14), the free energy (13) is\nF\u21e1(s) = 1 log X\na\n\u21e2(a|s)e G\u21e1(s,a), (15)\nand plugging this expression into (12), we get that the optimal G\u21e4 is a fixed point of the equation\nG\u21e4(s, a) = E\u2713[c|s, a] (16)\nEp \" log X\na0\n\u21e2(a0|s0)e G\u21e4(s0,a0) #\n\u2318 B\u21e4[G\u21e4](s,a). (17)\nBased on the above expression, we introduce G-learning as an off-policy TD-learning algorithm [2], that learns the optimal G\u21e4 from the interaction with the environment by applying the update rule\nG(st, at) (1 \u21b5t)G(st, at) (18)\n+ \u21b5t ct log\nX\na0\n\u21e2(a0|st+1)e G(st+1,a 0) !! ."}, {"heading": "3.2 THE ROLE OF THE PRIOR", "text": "Clearly the choice of the prior policy \u21e2 is significant in the performance of the algorithm. The prior policy can encode any prior knowledge that we have about the domain, and this can improve the convergence if done correctly. However an incorrect prior policy can hinder learning. We should therefore choose a prior policy that represents all of our prior knowledge, but nothing more. This prior policy has maximal entropy given the prior knowledge [20].\nIn our examples in Section 6, we use the uniform prior policy, representing no prior knowledge. Both in Q-learning\nand in G-learning, we could utilize the prior knowledge that moving into a wall is never a good action, by eliminating those actions. One advantage of G-learning is that it can utilize softer prior knowledge. For example, a prior policy that gives lower probability for moving into a wall represent the prior knowledge that such an action is usually (but not always) harmful, a type of knowledge that cannot be utilized in Q-learning.\nWe have presented G-learning in a fully parameterized formulation, where the function G is stored in a lookup table. Practical applications of Q-learning often resort to approximating the function Q through function approximations, such as linear expansions or neural networks [2, 3, 4, 21, 5]. Such an approximation generates inductive bias, which is another form of implicit prior knowledge. While Glearning is introduced here in its table form, preliminary results indicate that its benefits carry over to function approximations, despite the challenges posed by this extension."}, {"heading": "3.3 CONVERGENCE", "text": "In this section we study the convergence of G under the update rule (18). Recall that the supremum norm is defined as |x|1 = maxi |xi|. We need the following Lemma, proved in the Supplementary Material.\nLemma 1. The operator B\u21e4[G](s,a) defined in (17) is a contraction in the supremum norm,\nB\u21e4[G1] B\u21e4[G2] 1  G1 G2 1. (19)\nThe update equation (18) of the algorithm can be written as a stochastic iteration equation\nGt+1(st, at) = (1 \u21b5t)Gt(st, at) (20) + \u21b5t(B \u21e4[Gt](st,at) + zt(ct, st+1))\nwhere the random variable zt is\nzt(ct, st+1) = B\u21e4[Gt](st,at) (21) + ct log X\na0\n\u21e2(a0|st+1)e Gt(st+1,a 0).\nNote that zt has expectation 0. Many results exist for iterative equations of the type (20). In particular, given conditions (6) for \u21b5t, the contractive nature of B\u21e4, infinite visits to each pair (st, at) and assuming that |zt| < 1 , Gt is guaranteed to converge to the optimal G\u21e4 with probability 1 [17, 22]."}, {"heading": "4 SCHEDULING", "text": "In the previous section, we showed that running G-learning with a fixed converges, with probability 1, to the optimal G\u21e4 for that , given by the recursion in (12)\u2013(14).\n79\nWhen = 1, the equations for G\u21e4 and F \u21e4 degenerate into the equations for Q\u21e4 and V \u21e4, and G-learning becomes Q-learning. When = 0, the update policy \u21e1 in (14) is equal to the prior \u21e2. This case, denoted Q\u21e2-learning, converges to Q\u21e2.\nIn an early stage of learning, Q\u21e2-learning has an advantage over Q-learning, because it avoids committing to a deterministic policy based on a noisy Q function. In a later stage of learning, when Q is a more precise estimate of Q\u21e4, Q-learning gains the advantage by updating with a better policy than the prior. This is demonstrated in section 6.1.\nWe would therefore like to schedule so that G-learning makes a smooth transition from Q\u21e2-learning to Q-learning, just at the right pace to enjoy the early advantage of the former and the late advantage of the latter. As we argue below, such a always exists."}, {"heading": "4.1 ORACLE SCHEDULING", "text": "To consider the effect of the scheduling on the correction of the bias (7), suppose that during learning we reach some G that is an unbiased estimate of G\u21e4. G(st, at) would remain unbiased if we update it towards\nct + G(st+1, a \u21e4) (22)\nwith\na\u21e4 = arg min a0 G\u21e4(st+1, a 0), (23)\nbut we do not have access to this optimal action. If we use the update rule (18) with = 0, we update G(st, at) towards\nct + X\na0\n\u21e2(a0|st+1)G(st+1, a0), (24)\nwhich is always at least as large as (22), creating a positive bias. If we use =1, we update G(st, at) towards\nct + min a0\nG(st+1, a 0), (25)\nwhich creates a negative bias, as explained in Section 2.2. Since the right-hand side of (18) is continuous and monotonic in , there must be some for which this update rule is unbiased.\nThis is a non-constructive proof for the existence of a schedule that keeps the value estimators unbiased (or at least does not accumulate additional bias). We can imagine a scheduling oracle, and a protocol for the agent by which to consult the oracle and obtain the for its soft updates. At the very least, the oracle must be told the iteration index t, but it can also be useful to let depend on any other aspect of the learning process, particularly the current world state st."}, {"heading": "4.2 PRACTICAL SCHEDULING", "text": "A good schedule should increase as learning proceeds, because as more samples are gathered the variance of G decreases, allowing more deterministic policies. In the examples of Section 6 we adopted the linear schedule\nt = kt, (26)\nwith some constant k > 0. Another possibility that we explored was to make inversely proportional to a running average of the Bellman error, which decreases as learning progresses. The results were similar to the linear schedule.\nThe optimal parameter k can be obtained by performing initial runs with different values of k and picking the value whose learned policy gives empirically the lower cost-togo. Although this exploration would seem costly compared to other algorithms for which no parameter tuning is needed, these initial runs do not need to be carried for many iterations. Moreover, in many situations the agent is confronted with a class of similar domains, and tuning k in a few initial domains leads to an improved learning for the whole class. This is the case in the domain-generator example in Section 6.1."}, {"heading": "5 RELATED WORK", "text": "The connection between domain noise or function approximation, and the statistical bias in the Q function, was first discussed in [18, 3]. An interesting modification of Q-learning to address this problem is Double-Qlearning [11, 14], which uses two estimators for the Q function to alleviate the bias. Other modifications of Qlearning that attempt to reduce or correct the bias are suggested in [12, 13].\nAn early approach to Q-learning in continuous noisy domains was to learn, instead of the value function, the advantage function A(s, a) = Q(s, a) V (s) [23]. The algorithm represents A and V separately, and the optimal action is determined from A(s, a) as a\u21e4(s) = arg mina A(s, a). In noisy environments, learning A is shown in some examples to be faster than learning Q [23, 24].\nMore recently, it was shown that the advantage learning algorithm is a gap-increasing operator [25]. As discussed in Section 2.2, the action gap is a central factor in the generation of bias, and increasing the gap should also help reduce the bias. In Section 6.1 we compare our algorithm to the consistent Bellman operator TC , one of the gap-increasing algorithms introduced in [25].\nFor other works that study the effect of noise in Q-learning, although without identifying the bias (7), see [26, 27, 28].\nInformation considerations have received attention in recent years in various machine learning settings, with the free energy F\u21e1 and similar quantities used as a design\n80\nprinciple for policies in known MDPs [10, 29, 30]. Other works have used related methods for reinforcement learning [31, 32, 33, 34, 35]. A KL penalty similar to ours is used in [35], in settings with known reward and transition functions, to encourage \u201ccuriosity\u201d.\nSoft-greedy policies have been used before for exploration [2, 36], but to our knowledge G-learning is the first TD-learning algorithm to explicitly use soft-greedy policies in its updates.\nParticularly relevant to our work is the approach studied in [32]. There the policy is iteratively improved by optimizing it in each iteration under the constraint that it only diverges slightly, in terms of KL-divergence, from the empirical distribution generated by the previous policy. In contrast, in G-learning we measure the KL-divergence from a fixed prior policy, and in each iteration allow the divergence to grow larger by increasing . Thus the two methods follow different information-geodesics from the stochastic prior policy to more and more deterministic policies.\nThis distinction is best demonstrated by considering the - learning algorithm presented in [33, 34], based on the same approach as [32]. It employs the update rule\n(st, at) (st, at) (27) + \u21b5t(ct +  \u0304(st+1)  \u0304(st)),\nwith\n \u0304(s) = log X\na\n\u21e2(a|s)e (s,a), (28)\nwhich is closely related to our update of G in (18).\nApart from lacking a parameter, the most important difference is that the update of involves subtracting \u21b5t \u0304(st), whereas the update of G involves subtracting \u21b5tG(st, at). This seemingly minor modification has a large impact on the behavior of the two algorithms. The update of G is designed to pull it towards the optimal stateaction free energy G\u21e4, for all state-action pairs. In contrast, subtracting the log-partition  \u0304(st), in the long run pulls only (st, a\u21e4), with a\u21e4 the optimal action, towards its true value, while for the other actions the values grow to infinity. In this sense, the -learning update (27) is an informationtheoretic gap-increasing Bellman operator [25].\nThe growth to infinity of suboptimal values separates them from the optimal value, and drives the algorithm to convergence. In G-learning, this parallels the increase in with the accumulation of samples. However, there is a major benefit to keeping G reliable in all its parameters, and controlling it with a separate parameter. In -learning, the function penalizes actions it deems suboptimal. If early noise causes an error in this penalty, the algorithm needs to unlearn it - a similar drawback to that of Q-learning. In Section 6, we demonstrate the improvement offered by Glearning.\nFigure 1: Gridworld domain. The agent can choose an adjacent square as the target to move to, and then may end up stochastically in a square adjacent to that target. The color scale indicates the optimal values V \u21e4 with a fixed cost of 1 per step."}, {"heading": "6 EXAMPLES", "text": "This section illustrates how G-learning improves on existing model-free learning algorithms in several settings. The domains we use are clean and simple, to demonstrate that the advantages of G-learning are inherent to the algorithm itself.\nWe schedule the learning rate \u21b5t as\n\u21b5t = nt(st, at) ! , (29)\nwhere nt(st, at) is the number of times the pair (st, at) was visited. This scheme is widely used, and is consistent with (6) for ! 2 (1/2, 1]. We choose ! = 0.8, which is within the range suggested in [37].\nWe schedule linearly, as discussed in Section 4.2. In each case, we start with 5 preliminary runs of G-learning with various linear coefficients, and pick the coefficient with the lowest empirical cost. This coefficient is used in the subsequent test runs, whose results are plotted in Figure 2.\nIn all cases, we use a uniform prior policy \u21e2, a discount factor = 0.95, and 0 for the initial values (Q0 = 0 in Q-learning, and similarly in the other algorithms). Except when mentioned otherwise, we employ random exploration, where st and at are chosen uniformly at the beginning of each time step, independently of any previous sample. This exploration technique is useful when comparing update rules, while controlling for the exploration process."}, {"heading": "6.1 GRIDWORLD", "text": "Our first set of examples occurs in a gridworld of 8 \u21e5 8 squares, with some unavailable squares occupied by walls shown in black (Figure 1). The lightest square is the goal, and reaching it ends the episode.\nAt each time step, the agent can choose to move one square in any of the 8 directions (including diagonally), or stay in place. If the move is blocked by a wall or the edge of the\n81\nboard, it effectively attempts to stay in place. With some probability, the action performed by the agent is further followed by an additional random slide: with probability 0.15 to each vertically or horizontally adjacent available position, and with probability 0.05 to each diagonally adjacent available position.\nThe noise associated with these random transitions can be enhanced further by the possible variability in the costs incurred along the way. We consider three cases. In the first case, the cost in each step is fixed at 1. In the second case, the cost in each step is distributed normally i.i.d, with mean 1 and standard deviation 2. In the third case we define a distribution over domains, such that at the time of domaingeneration the mean cost for each state-action is distributed uniformly i.i.d over [1, 3]. Once the domain has been generated and interaction begins, the cost itself in each step is again distributed normally i.i.d, with the generated mean and standard deviation 4.\nWe attempt to learn these domains using various algorithms. Figure 2 summarizes the results for Q-learning, G-learning, Double-Q-learning [11], -learning [33, 34] and the consistent Bellman operator TC of [25]. We also include Q\u21e2-learning, which performs updates as in (4) towards the prior policy \u21e2. Comparison with Speedy-Qlearning [12] is omitted, since it showed no improvement over vanilla Q-learning in these settings. In our experiments, these algorithms had comparable running times.\nThe scheduling used in G-learning is linear, with the coefficient k equal to 10 3, 10 4, 5 \u00b7 10 5 and 10 6, respectively for the fixed-cost, noisy-cost, domain-generator and cliff domains (see Section 6.2).\nFor each case, Figure 2 shows the evolution over 250,000 algorithm iterations of the following three measures, averaged over N = 100 runs:\n1. Empirical bias, defined as\n1 Nn\nNX\ni=1\nnX\ns=1\n(Vi,t(s) V \u21e4i (s)), (30)\nwhere i indexes the N runs and s the n states. Here Vi,t is the greedy value based on the estimate obtained by each algorithm (Q, G, etc.), in iteration t of run i. The optimal value V \u21e4i , computed via Value Iteration, varies between runs in the domain-generator case.\n2. Mean absolute error in V\n1 Nn\nNX\ni=1\nnX\ns=1\n|Vi,t(s) V \u21e4i (s)|. (31)\nA low bias could result from the cancellation of terms with high positive and negative biases. A convergence in the absolute error is more indicative of the actual convergence of the value estimates.\n3. Increase in cost-to-go, relative to the optimal policy\n1 Nn\nNX\ni=1\nnX\ns=1\n(V \u21e1i,t(s) V \u21e4i (s)). (32)\nThis measures the quality of the learned policy. Here \u21e1i,t is the greedy policy based on the state-action value estimates, and V \u21e1i,t is its value in the model, computed via Value Iteration.\nAn algorithm is better when these measures reach zero faster. As is clear in Figure 2, in the domains with noisy cost (Rows 2 and 3), G-learning dominates over all the other competing algorithms by the three measures. The results are statistically significant, but plotting confidence intervals would clutter the figure.\nAn important and surprising point of Figure 2 is that Q\u21e2learning always outperforms Q-learning initially, before degrading. The reason is that the Q-learning updates initially rely on very few samples, so these harmful updates need to be undone by later updates. Q\u21e2-learning, on the other hand, updates in the direction of a uniform prior. This gives an early advantage in mapping out the local topology of the problem, before long-range effects start pulling the learning towards the suboptimal Q\u21e2.\nThe power of G-learning is that it enjoys the early advantage of Q\u21e2-learning, and smoothly transitions to the convergence advantage of Q-learning. When is small, the information cost gt (8) outweighs the external costs ct, and we update towards \u21e2. As samples keep coming in, and our estimates improve, increases, and the updates gradually lean more towards a cost-optimizing policy. Unlike early stages in Q-learning, at this point Gt is already a good estimate, and we avoid overfitting. As mentioned above, Figure 2 shows that this effect is more manifest in noisier scenarios.\nFinally, Figure 3 shows running averages of the Bellman error for the different algorithms considered. The Bellman error in G-learning is the coefficient multiplying \u21b5t in (18),\nGt \u2318 ct log X\na0\n\u21e2(a0|st+1)e Gt(st+1,a 0)\n!\nGt(st, a). (33) When learning ends and G = G\u21e4, the expectation of Gt is zero (see (16)). Similar definitions hold for the other learning algorithms we compare with. As is clear from Figure 3, G-learning reaches zero average Bellman error faster than the competing methods, even while is still increasing in order to make G\u21e4 converge to Q\u21e4."}, {"heading": "6.2 CLIFF WALKING", "text": "Cliff walking is a standard example in reinforcement learning [2], that demonstrates an advantage of on-policy algorithms such as SARSA [2, 4] and Expected-SARSA [15,\n82\nFigure 2: Gridworld (Rows 1-3): Comparison of Q-, G-, Q\u21e2-, Double-Q-, - and TC-learning. Row 1: The cost in each step is fixed at 1. Row 2: The cost in each step is distributed as N (1, 22). Row 3: In each run, the domain is generated by drawing each E[c|s, a] uniformly over [1, 3]. The cost in each step is distributed as N (E[c|s, a], 42). Note that in the noisy domains (Rows 2 and 3), G-learning dominates over all the other algorithms by the three measures. Cliff (Row 4): Comparison of Q- and G-learning, and Expected-SARSA. The cost in each step is 1, and falling off the cliff costs 5. Left: Empirical bias of V , relative to V \u21e4 (30). Middle: Mean absolute error between V and V \u21e4 (31). Right: Value of greedy policy, with the baseline V \u21e4 subtracted (32); except in Row 4, which shows the value of the exploration policy.\n16] over off-policy learning approaches such as Q-learning. We use it to show another interesting strength of Glearning.\nIn this example, the agent can walk on the grid in Figure 4 horizontally or vertically, with deterministic transitions. Each step costs 1, except when the agent walks off\nthe cliff (the bottom row), which costs 5, or reaches the goal (lower right corner), which costs 0. In either of these cases, the position resets to the lower left corner.\nExploration is now on-line, with st taken from the end of the previous step. The exploration policy in our simulations is \u270f-greedy with \u270f = 0.1, i.e. with probability \u270f the agent\n83\nFigure 3: Running average of the Bellman error in the gridworld domain-generator example for Q-, G-, Q\u21e2-, DoubleQ-, - and TC-learning. The results for the other two gridworlds of Figure 2 are similar.\nchooses a random action, and otherwise it takes deterministically the one that seems optimal. In practice, \u270f can be decreased after the learning phase, however it is also common to keep \u270f fixed for continued exploration [2].\nIn this setting, as shown in the bottom row of Figure 2, an off-policy algorithm like Q-learning performs poorly in terms of the value of its exploration policy, and the empirical cost it incurs. It learns a rough estimate of Q\u21e4 quickly, and then tends to use it and walk on the edge of the cliff. This leads to the agent occasionally exploring the possibility of falling off the cliff. In contrast, an on-policy algorithm like Expected-SARSA [15, 16] learns the value of its exploration policy, and quickly manages to avoid the cliff.\nFigure 4 compares Q-learning, G-learning and ExpectedSARSA in this domain, and shows that G-learning learns to avoid the cliff even better than an on-policy algorithm, although for a different reason. As an off-policy algorithm, G-learning does learn the value of the update policy, which prefers trajectories far from the cliff in the early stages of learning. This occurs because near the cliff, avoiding the cost of falling requires ruling out downward moves, which has a high information cost. On the other hand, trajectories far from the cliff, while paying a higher cost in overall distance to the goal, enjoy lower information cost because acting randomly is not costly for them.\nAs shown in the bottom row of Figure 2, by using a greedy policy for G as the basis of the \u270f-greedy exploration, we enjoy the benefits of being aware of the value of the exploration policy during the learning stage. At the same time, G-learning converges faster than either Q-learning or Expected-SARSA to the correct value function. In this case the \u201cnoise\u201d that G-learning mitigates is related to the variability associated with the exploration."}, {"heading": "7 CONCLUSIONS", "text": "The algorithm we have introduced successfully mitigates the slow learning problem of early stage Q-learning in\nFigure 4: Cliff domain. The agent can choose a horizontally or vertically adjacent square, and moves there deterministically. The color scale and the arrow lengths indicate, respectively, the frequency of visiting each state and of making each transition, in the first 250,000 iterations of Q-learning, Expected-SARSA and G-learning. The neargreedy exploration policy of Q-learning has higher chance of taking the shortest path near the edge of the cliff at the bottom, than that of G-learning. As an off-policy algorithm, Q-learning fails to optimize for the exploration policy, whereas G-learning succeeds.\nnoisy environments, that is caused by the bias generated by the hard optimization of the policy.\nAlthough we have focused on Q-learning as a baseline, we believe that early-stage information penalties can also be applied to advantage in more sophisticated model-free settings, such as TD( ), and combined with other incremental learning techniques, such as function approximation, experience replay and actor-critic methods.\nG-learning takes a Frequentist approach to estimating the optimal Q function. This is in contrast to Bayesian Q-learning [38], which explicitly models the uncertainty about the Q function as a posterior distribution. It would be interesting to study the bias that hard optimization causes in the mean of this posterior, and to consider its reduction using methods similar to G-learning.\nAn important next step is to apply G-learning to more challenging domains, where an approximation of the G function is necessary. The simplicity of our linear schedule (26) should facilitate such extensions, and allow Glearning to be combined with other schemes and algorithms. Further study should also address the optimal schedule for . We leave these important questions for future work."}, {"heading": "Acknowledgments", "text": "AP is supported by ONR grant N00014-14-1-0243 and IARPA via DoI/IBC contract number D16PC00003. RF and NT are supported by the DARPA MSEE Program, the Gatsby Charitable Foundation, the Israel Science Foundation and the Intel ICRI-CI Institute.\n84"}, {"heading": "Discussion", "text": "In this thesis we studied bounded agents that operate in dynamical systems under intrinsic informational constraints. This setting can be modeled as a sequential rate-distortion problem, and solved with a forward-backward algorithm. We investigated the convergence properties of the algorithm, exploited the structure of the special LQG case, and simplified the setting to be usable for learning. In this section we summarize and discuss some of the insights gained in our work."}, {"heading": "A Principle for the Tradeoff of Informational Resources and Costs", "text": "There are various ways to model bounded agents with limited informationprocessing resources. Our approach, introduced in Section 2.1, is to identify distinct components within the agent, such as sensors, memory and actuators, and consider the information rates on the communication channels between these components.\nThe reduction of extrinsic costs is usually taken as the optimization target, with extrinsic dynamical constraints, to which we add intrinsic constraints on the rates at which information can be communicated between sensors, memory and actuators. In the Lagrangian form of this optimization problem, the latter become intrinsic informational costs, which are traded off with extrinsic expectational costs.\nAs an alternative formulation of the optimization problem, we can set an upper constraint on the extrinsic cost, and seek the simplest agent that\n86\nachieves this cost level. With a fixed prior behavior, simplicity of a policy can be measured as the Kullback-Leibler (KL) divergence of the solution policy from the prior. On an evolutionary timescale, the prior itself is adaptive, and the optimization target becomes the information rate.\nInterpreting informational constraints as costs is insightful, in that it allows trading off the informational costs of various channels among themselves. For example, when memory resources are scarce, it may be easier to extract some information from observations again and again, rather than remember it. On the other hand, when memory has a high enough capacity, it can help process the sensory input by generating a good prediction of the observation, and only attending to some surprises.\nTo illustrate this point further, consider the optimal policy for a fully observable MDP. Without informational constraints, there is always an optimal policy which is reactive (memoryless), and simply depends on the current state [2]. However, if attending the state spends precious informationalprocessing resources, and thus incurs informational costs, the setting becomes partially attendable, even though it remains fully observable. If we allow a memory channel from past internal agent states to future ones that is cheaper than the sensory channel, it may be beneficial to make up for unattended sensory input using remembered information."}, {"heading": "Periodicity and Instability of the Optimization Principle", "text": "The algorithm presented in Section 2.1 is applicable to general POMDPs. However, it is only demonstrated there on passive POMDPs, where actions incur costs but do not affect the state of the world. Experiments with other types of examples exhibited poor convergence that seemed to be the result of periodicity or instability of the solution under the update operator, particularly after phase transitions that increase the support of the agent policy.\nWe have thus taken the first steps in the study of the bifurcation structure of the learning dynamics around critical values of the tradeoff param-\n87\neter . Section 2.2 analyzes examples that illustrate this structure. The phenomenology of planning in partial observability under informational constraints includes period doubling through supercritical pitchfork bifurcations. The optimal solution at these critical points becomes periodic, requiring the agent to start paying attention to a clock signal. The optimal stationary (aperiodic) solution remains a fixed point, but loses its stability to perturbations.\nThe conclusion is that for reinforcement learning algorithms to converge in partially observable domains (or, indeed, under partial attendability or approximate inference), they must allow for periodicity of the solution policy. This holds true for value iteration and gradient methods alike. We also note that the periodicity itself is a channel from the clock to the controller, and may be subject to information constraints."}, {"heading": "The Linear-Quadratic-Gaussian Case", "text": "The general algorithm presented in Section 2.1 is polynomial in the sizes of the world state, memory state, observation and action spaces involved. When these spaces are very large or continuous, we can no longer apply the algorithm in this tabular form. Instead, the solution must be parameterized in a tractable manner, and the gradient must be taken with respect to these parameters.\nA particularly important and insightful parametric family, studied in Chapter 3, is the Gaussian distributions (for p\u0304), the linear-Gaussian conditional distributions (for p, , q and \u21e1) and the quadratic functions (for c and \u232b). This family has special properties when considering unbounded agents. It is self-conjugate, meaning that under linear-Gaussian dynamics, a Gaussian marginal remains Gaussian, and a quadratic cost-to-go function remains quadratic when the cost rate is quadratic. There is also separation of the forward inference process and the backward control process.\nIt comes as no surprise that this case is also special when considering\n88\nbounded agents. Although the cost-to-go is not quadratic when considering informational costs, the solution method only involves its second-order expansion. The forward and backward processes are coupled, but many local optima of the type that plagues the discrete case are avoided by the tools available to treat second-order systems.\nIn particular, the sequential rate-distortion problem can be formulated in the LQG case as a sequential semidefinite program. Its solution provides not only first-order necessary conditions for a solution to be optimal, i.e. having gradient 0, but also higher-order necessary conditions. This prevents some local optima where the inference and control policies are optimal given each other, but jointly suboptimal as a pair."}, {"heading": "Learning and Scheduling", "text": "Learning is the process of gaining useful information about the world through interaction. An agent in interaction with an environment whose state is partially observable has to perform learning, whether or not it has a model of the dynamics. The setting where no such model is available is particularly interesting, since it illustrates how the tradeoff between cost and simplicity changes as the algorithm progresses, as shown in Section 4.1.\nThe maximum relative entropy principle states that a solution should minimize the KL divergence to a simple prior, under the constraint that it fits any additional information we have about the solution. In the MDP learning setting, this additional information is represented by the value function, which is iteratively improved by sample-based updates. An imperfect value function cannot generally be used to select an optimal policy, and we must settle for a suboptimal value guarantee. The policy used in each update should thus be the simplest one, in terms of KL divergence, under the constraint that this value guarantee level is achieved. As learning progresses, the value function becomes more accurate, the guarantee can be improved, and hence the tradeoff coefficient is increased to reflect a larger emphasis\n89\non the extrinsic cost. The principle that should generally increase as the learned parameters\nimprove is not unique to sample-based methods. The goal of any computation is to reduce the uncertainty about its output, and iterative algorithms generally reduce this uncertainty gradually. If the partially optimized solution is used to obtain an improved solution, it may be beneficial to consider soft-optimization, by taking the simplest solution under the constraint that a gradually increasing guarantee level is achieved. For example, scheduling can also be used in this manner to improve convergence in value iteration, and many other algorithms.\n90"}, {"heading": "Glossary", "text": "Action, Control signal Input to the POMDP state transition. Output of the agent control policy. Notation: at.\nBackward process Computation of the cost-to-go function through the application of the Bellman operator backward in time.\nBayesian network Graphical model of a distribution as a directed acyclic graph with a variable in each node. The joint distribution of all variable is given by the product of the distributions of each variable given its parents. Notation: ppx1, . . . , xnq \u201c \u00b1 i ppxi|parentspxiqq.\nBelief Probability distribution over world states that is represented in the agent memory state. Notation: objective: Ppw|mq; subjective: bmpwq.\nCapacity-cost problem Optimization of the tradeoff between the capacity Irx; ys for information on the channel and the expected cost Ercpxqs. Notation: inputs: pY |Xpy|xq, cpxq; output: qpxq.\nChannel Stochastic mapping of the channel input x to the channel output y. A cost cpxq on the channel input is sometimes also considered part of the definition of the channel. Notation: pY |Xpy|xq.\nChannel coding Encoding of an input signal s into the channel input x and decoding of the channel output y as a reconstruction signal s\u0302. Notation: encoder: x \u201c gpsq; decoder: s\u0302 \u201c hpyq.\n91\nControl policy Probability distribution of the agent\u2019s action given its memory state. Notation: \u21e1pat|mtq.\nCost Real function of the system\u2019s state, usually the world state and the action, whose expectation is used as the minimization target. Notation: cpwt, atq.\nExploitation Agent behavior aimed at achieving good value, based on known aspects of the world.\nExploration Agent behavior aimed at learning unknown aspects of the world.\nFinite horizon Minimization target that considers the total cost of the process. Meaningful when the process has a finite expected termination time, such as in the episodic, discounted and fixed-horizon settings. Notation: \u221e8 t\u201c0Ercpwt, atqs.\nForward process Computation of the marginal state distribution through the application of the dynamics operator forward in time.\nFull controllability Complete determination of the next state by the input action. Notation: ppwt`1|wt, atq \u201c wt`1\u201cat .\nFull observability Complete revelation of the state as an observation. Notation: pot|wtq \u201c ot\u201cwt .\nHistory-based policy Most general form of an agent policy, where its output action depends arbitrarily on its past inputs, the observable history. Notation: \u21e1pat|o\u00a7tq.\n92\nInference policy Probability distribution of the agent\u2019s next memory state given its current memory state and the new observation. Notation: qpmt`1|mt, ot`1q.\nInfinite horizon Minimization target that considers the long-term average cost of the process. Meaningful when the process is stationary. Notation: lim supT \u00d18 1T \u221eT \u00b41 t\u201c0 Ercpwt, atqs.\nMarginal distribution Distribution induced by a stochastic process on a subset of its random variables, often a state. Notation: p\u0304pstq, \u21e1\u0304patq.\nModel-based learning Learning based on update equations that involve a model of the world dynamics.\nObjectively consistent inference Inference policy that induces subjective beliefs which are consistent with the objective beliefs. Notation: bmpwq \u201c Ppw|mq.\nObservation Output emitted by the POMDP depending on its state. Input to the agent inference policy. Notation: ot.\nObservation dynamics Probability distribution of the observation given the state. Notation: pot|stq.\nPartial attendability Intrinsic limitation on the agent\u2019s ability to attend to its inputs when performing inference. For example, the constraint of a low mutual information Irmt, ot`1; mt`1s between the inputs and the outputs of the inference step.\nPartial controllability Extrinsic limitation on the agent\u2019s ability to determine the world state transition. In particular, the constraint that Ppwt`1|wt, mtq belongs to the convex hull of tppwt`1|wt, atq : at P Au.\n93\nPartial intendability Intrinsic limitation on the agent\u2019s ability to intend its outputs when performing control. For example, the constraint of a low mutual information Irmt; ats between the inputs and the outputs of the control step.\nPartial observability Extrinsic limitation on the agent\u2019s ability to observe the world state. In particular, the constraint that Ppmt`1|mt, wt`1q is induced by the mixture pot|wtq, applied to an inference policy.\nRate-distortion problem Optimization of the tradeoff between the rate Irs; s\u0302s of information that the reconstruction has on the source and the expected distortion Erdps, s\u0302qs. Notation: inputs: pSpsq, dps, s\u0302q; output: qps\u0302|sq.\nReactive, memoryless agent Agent without an internal memory state, consisting of a memoryless policy.\nReactive, memoryless policy Probability distribution of the agent\u2019s action given its most recent observation. Notation: \u21e1pat|otq.\nRetentive agent Memory-utilizing agent, consisting of an inference policy and a control policy.\nSample-based learning Learning based on update equations that utilize samples of the world dynamics.\nSource Probability distribution of a signal s. A distortion dps, s\u0302q between the signal and its reconstruction is sometimes also considered part of the definition of the source. Notation: pSpsq.\nSource coding Stochastic encoding of a source signal into an intermediate representation and decoding its reconstruction. Notation: encoder: gpz|sq; decoder: s\u0302 \u201c hpzq.\n94\nState Time-dependent property of a system that separates the past and the future of the system. Notation: closed system, joint world-agent system: st; world: wt; agent memory: mt.\nState dynamics, transition Probability distribution of the next state given the current state and any inputs. Notation: closed system: ppst`1|stq; open system: ppwt`1|wt, atq.\nStationary distribution Marginal distribution of the state that remains the same after a step of the dynamics.\n95"}], "references": [], "referenceMentions": [], "year": 2017, "abstractText": "We model the interaction of an intelligent agent with its environment as<lb>a Partially Observable Markov Decision Process (POMDP), where the joint<lb>dynamics of the internal state of the agent and the external state of the<lb>world are subject to extrinsic and intrinsic constraints. Extrinsic constraints<lb>of partial observability and partial controllability specify how the agent\u2019s<lb>input observation depends on the world state and how the latter depends on<lb>the agent\u2019s output action. The agent also incurs an extrinsic cost, based on<lb>the world states reached and the actions taken in them.<lb>Bounded agents are also limited by intrinsic constraints on their ability to<lb>process information that is available in their sensors and memory and choose<lb>actions and memory updates. In this dissertation, we model these constraints<lb>as information-rate constraints on communication channels connecting these<lb>various internal components of the agent.<lb>The simplest is to first consider reactive (memoryless) agents, with a chan-<lb>nel connecting their sensors to their actuators. The problem of optimizing<lb>such an agent, under a constraint on the information rate between the input<lb>and the output, is a sequential rate-distortion problem. The marginal distri-<lb>bution of the observation can be computed by a forward inference process,<lb>whereas the expected cost-to-go of an action can be computed by a backward<lb>control process. Given this source distribution and this effective distortion,<lb>respectively, each step can be optimized by solving a rate-distortion problem<lb>that trades off the extrinsic cost with the intrinsic information rate. Retentive (memory-utilizing) agents can be reduced to reactive agents<lb>by interpreting the state of the memory component as part of the external<lb>world state. The memory reader can then be thought of as another sensor<lb>and the memory writer as another actuator and they are limited by the same<lb>informational constraint between inputs and outputs.<lb>In this dissertation we make four major contributions detailed below and<lb>many smaller contributions detailed in each section.<lb>First, we formulate the problem of optimizing the agent under both ex-<lb>trinsic and intrinsic constraints and develop the main tools for solving it.<lb>This optimization problem is highly non-convex, with many local optima.<lb>Its difficulty is mostly due to the coupling of the forward inference process<lb>and the backward control process. The inference policy and the control pol-<lb>icy can be optimal given each other but still jointly suboptimal as a pair.<lb>For example, if some information is not attended to it cannot be used and if<lb>it is not used it should optimally not be attended to.<lb>Second, we identify another reason for the challenging convergence prop-<lb>erties of the optimization algorithm, which is the bifurcation structure of the<lb>update operator near phase transitions. We show that the update operator<lb>may undergo period doubling, after which the optimal policy is periodic and<lb>the optimal stationary policy is unstable. Any algorithm for planning in such<lb>domains must therefore allow for periodic policies, which may themselves be<lb>subject to an informational constraint on the clock signal.<lb>Third, we study the special case of linear-Gaussian dynamics and quadratic<lb>cost (LQG), where the optimal solution has a particularly simple and solv-<lb>able form. Under informational constraints, the forward and the backward<lb>processes are not separable. However, we show that they do have a more ex-<lb>plicitly solvable structure; namely, a sequential semidefinite program. This<lb>also allows us to analyze the structure of the retentive solution under the<lb>reduction to the reactive setting.<lb>Fourth, we explore the learning task, where the model of the world dy-", "creator": "LaTeX with hyperref package"}}}