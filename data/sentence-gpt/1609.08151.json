{"id": "1609.08151", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "25-Sep-2016", "title": "Nonnegative autoencoder with simplified random neural network", "abstract": "This paper proposes new nonnegative (shallow and multi-layer) autoencoders by combining the model of spiking Random Neural Network (RNN), the network architecture in the deep-learning area and the training technique in the nonnegative matrix factorization (NMF) area. The shallow autoencoder is a simplified RNN model, which is then stacked into a multi-layer architecture. The learning algorithms are based on the weight update rules in the NMF area, subjecting to the nonnegative and probability constraints in a RNN model. The neural networks are then trained in the LHC, the Nucleic acid processing layer, and a single layer of memory.\n\n\n\nIn the first section of the paper, I discuss new methods for generating deep learning neural networks using a deep neural network. In the second section of the paper, I discuss new methods for generating deep learning neural networks using a deep neural network. In the third section of the paper, I discuss new methods for generating deep learning neural networks using a deep neural network. In the fourth section of the paper, I discuss new methods for generating deep learning neural networks using a deep neural network. In the fourth section of the paper, I discuss new methods for generating deep learning neural networks using a deep neural network. In the fifth section of the paper, I discuss new methods for generating deep learning neural networks using a deep neural network. In the fourth section of the paper, I discuss new methods for generating deep learning neural networks using a deep neural network. In the fifth section of the paper, I discuss new methods for generating deep learning neural networks using a deep neural network. In the sixth section of the paper, I discuss new methods for generating deep learning neural networks using a deep neural network. In the sixth section of the paper, I discuss new methods for generating deep learning neural networks using a deep neural network. In the fourth section of the paper, I discuss new methods for generating deep learning neural networks using a deep neural network. In the fourth section of the paper, I discuss new methods for generating deep learning neural networks using a deep neural network. In the sixth section of the paper, I discuss new methods for generating deep learning neural networks using a deep neural network. In the sixth section of the paper, I discuss new methods for generating deep learning neural networks using a deep neural network. In the fifth section of the paper, I discuss new methods for generating deep learning neural networks using a deep neural network. In the fifth section of the paper, I", "histories": [["v1", "Sun, 25 Sep 2016 13:47:08 GMT  (603kb,D)", "https://arxiv.org/abs/1609.08151v1", "10 pages"], ["v2", "Thu, 29 Sep 2016 11:02:29 GMT  (603kb,D)", "http://arxiv.org/abs/1609.08151v2", "10 pages (a small edit to the abstract)"]], "COMMENTS": "10 pages", "reviews": [], "SUBJECTS": "cs.LG", "authors": ["yonghua yin", "erol gelenbe"], "accepted": false, "id": "1609.08151"}, "pdf": {"name": "1609.08151.pdf", "metadata": {"source": "CRF", "title": "Nonnegative autoencoder with simplified random neural network", "authors": ["Yonghua Yin", "Erol Gelenbe"], "emails": ["y.yin14@imperial.ac.uk,", "e.gelenbe@imperial.ac.uk"], "sections": [{"heading": "1 Introduction", "text": "A mathematical tool that has existed since 1989 [1\u20133], but is less well known in the machine-learning community, is the Random Neural Network (RNN), which is a stochastic integer-state \u201cintegrate and fire\u201d system and developed to mimic the behaviour of biological neurons in the brain. In an RNN, an arbitrarily large set of neurons interact with each other via excitatory and inhibitory spikes which modify each neuron\u2019s action potential in continuous time. The power of the RNN lays on the fact that, in steady state, the stochastic spiking behaviors of the network have a remarkable property called \u201cproduct form\u201d and that the state probability distribution is given by an easily solvable system of non-linear equations. The RNN has been used for numerous applications [4\u201315] that exploit its recurrent structure.\nDeep learning has achieved great success in machine learning [16,17]. Many computational models in deep learning exploit a feed-forward neural-network architecture that is composed of multi-processing layers, which allows the model to extract high-level representations from raw data. The feed-forward fully-connected multi-layer neural network could be difficult to train [18]. Pre-training the network layer by layer is a great advance [16, 19] and useful due to its wide adaptability, though recent literature shows that utilizing the rectified linear unit (ReLU) could train a deep neural network without pre-training [20]. The typical training procedure called stochastic gradient descent (SGD) provides a practical choice for handling large datasets [21].\nNonnegative matrix factorization (NMF) is also a popular topic in machine learning [22\u201326], which learns part-based representations of raw data. Lee [22] suggested that the perception of the whole in the brain may be based on these part-based representations (based on the physiological evidence [27]) and proposed simple yet effective update rules. Hoyer [24] combined sparse coding and NMF that allows control over sparseness. Ding investigated the equivalence between the NMF and K-means\nar X\niv :1\n60 9.\n08 15\n1v 2\n[ cs\n.L G\n] 2\n9 Se\np 20\nclustering in [26, 28] and presented simple update rules for orthogonal NMF. Wang [25] provided a comprehensive review on recent processes in the NMF area.\nThis paper first exploits the structure of the RNN equations as a quasi-linear structure. Using it in the feed-forward case, an RNN-based shallow nonnegative autoencoder is constructed. Then, this shallow autoencoder is stacked into a multi-layer feed-forward autoencoder following the network architecture in the deep learning area [16, 17, 19]. Since connecting weights in the RNN are products of firing rates and transition probabilities, they are subject to the constraints of nonnegativity and that the sum of probabilities is no larger than 1, which are called the RNN constraints in this paper. In view of that, the conventional gradient descent is not applicable for training such an autoencoder. By adapting the update rules from nonnegative graph embedding that can be seemed as a variant of NMF, applicable update rules are developed for the autoencoder that satisfy the first RNN constraint of nonnegativity. For the second RNN constraint, we impose a check-and-adjust procedure into the iterative learning process of the learning algorithms. The training procedure of SGD is also adapted into the algorithms. The efficacy of the nonnegative autoencoders equipped with the learning algorithms is well verified via numerical experiments on both typical image datasets including the MNIST [29], Yale face [30] and CIFAR-10 [31] datesets and 16 real-world datasets in different areas from the UCI machine learning repository [32]. Then, we simulate the spiking behaviors of the RNNbased autoencoder, where simulation results conform well with the corresponding numerical results, therefore demonstrating that this nonnegative autoencoder can be implemented in a highly-distributed and parallel manner."}, {"heading": "2 A quasi-linear simplified random neural network", "text": "An arbitrary neuron in the RNN can receive excitatory or inhibitory spikes from external sources, in which case they arrive according to independent Poisson processes. Excitatory or inhibitory spikes can also arrive from other neurons to a given neuron, in which case they arrive when the sending neuron fires, which happens only if that neuron\u2019s input state is positive (i.e. the neuron is excited) and inter-firing intervals from the same neuron v are exponentially distributed random variables with rate rv \u2265 0. Since the firing times depend on the internal state of the sensing neuron, the arrival process of neurons from other cells is not in general Poisson. From the preceding assumptions it was proved in [3] that for an arbitrary N neuron RNN, which may or may not be recurrent (i.e. containing feedback loops), the probability in steady-state that any cell h, located anywhere in the network, is excited is given by the expression:\nqh = min( \u03bb+h +\n\u2211N v=1 qvrvp + vh\nrh + \u03bb \u2212 h + \u2211N v=1 qvrvp \u2212 vh , 1), (1)\nfor h = 1, ... , N , where p+vh, p \u2212 vh are the probabilities that cell v may send excitatory or inhibitory spikes to cell h, and \u03bb+h , \u03bb \u2212 h are the external arrival rates of excitatory and inhibitory spikes to neuron h. Note that min(a, b) is a element-wise operation whose output is the smaller one between a and b. In [3], it was shown that the system of N non-linear equations (1) have a solution which is unique.\nBefore adapting the RNN as a non-negative autoencoder (Section 3), we will simplify the recurrent RNN model into the feed-forward structure shown in Figure 1. The simplified RNN has an input layer and a hidden layer. The V input neurons receive excitatory spikes from the outside world, and they fire excitatory spikes to the H hidden neurons.\nLet us denote by q\u0302v the probability that the vth input neuron (v = 1, \u00b7 \u00b7 \u00b7 , V ) is excited and qh the probability that the hth hidden neuron (h = 1, \u00b7 \u00b7 \u00b7 , H) is excited. According to [1] and (1), they are given by q\u0302v = min(\u039b\u0302+v /r\u0302v, 1), and qh = min(\u039b + h /rh, 1), where the quantities \u039b\u0302 + v and \u039b + h represent the total average arrival rates of excitatory spikes, r\u0302v and rh represent the firing rates of the neurons. Neurons in this model interact with each other in the following manner, where h = 1, \u00b7 \u00b7 \u00b7 , H and v = 1, \u00b7 \u00b7 \u00b7 , V . When the vth input neuron fires, it sends excitatory spikes to the hth hidden neuron with probability p+v,h \u2265 0. Clearly, \u2211H h=1 p + v,h \u2264 1. \u2022 The vth input neuron receives excitatory spikes from the outside world with rate xv \u2265 0. \u2022When the hth hidden neuron fires, it sends excitatory spikes outside the network. Let us denote wv,h = p+v,hr\u0302v. For simplicity, let us set the firing rates of all neurons to r\u0302v = rh = 1\nor that \u2211H\nh=1 wv,h \u2264 1. Then, \u039b\u0302+v = xv , r\u0302v = 1, \u039b + h = \u2211V v=1 wv,hq\u0302v , and using the fact that qh, qv\nare probabilities, we can write:\nq\u0302v = min(xv, 1), qh = min( V\u2211 v=1 wv,hq\u0302v, 1), (2)\nsubject to \u2211H\nh=1 wv,h \u2264 1. We can see from (2) that this simplified RNN is quasi linear. For the network shown in Figure 1, we call it a quasi-linear RNN (LRNN)."}, {"heading": "3 Shallow non-negative LRNN autoencoder", "text": "We add an output layer with O neurons on top of the hidden layer of the LRNN shown in Figure 1 to construct a shallow non-negative LRNN autoencoder. Let qo denote the probability that the oth output neuron is excited, and the oth output neurons interact with the LRNN in the following manner, where o = 1, \u00b7 \u00b7 \u00b7 , O. \u2022When the hth hidden neuron fires, it sends excitatory spikes to the oth output neuron with probability p+h,o \u2265 0. Also, \u2211O o=1 p + h,o \u2264 1. \u2022 The firing rate of the oth output neuron ro = 1. Let wh,o = p+h,orh = p + h,o. Then, \u2211O o=1 wh,o \u2264 1. The shallow LRNN autoencoder is described by\nq\u0302v = min(xv, 1), qh = min( V\u2211 v=1 wv,hq\u0302v, 1), qo = min( H\u2211 h=1 wh,oqh, 1), (3)\nwhere O = V and the input, hidden and output layers are the visual, encoding and decoding layers.\nSuppose there is a dataset represented by a nonnegative D \u00d7 V matrix X = [xd,v], where D is the number of instances, each instance has V attributes and xd,v is the vth attribute of the dth instance. We import X into the input layer of the LRNN autoencoder. Let q\u0302d,v, qd,h and qd,o respectively denote the values of q\u0302v , qd and qo for the dth instance.\nLet a D \u00d7 V -matrix Q\u0302 = [q\u0302d,v], a D \u00d7 H-matrix Q = [qd,h], a D \u00d7 O-matrix Q = [qd,o], a V \u00d7 U -matrix W = [wv,h] and a H \u00d7 O-matrix W = [wh,o]. Then, (3) can be rewritten as the following matrix manner:\nQ\u0302 = min(X, 1), Q = min(Q\u0302W, 1), Q = min(QW, 1), (4) subject to the RNN constraints W \u2265 0, W \u2265 0, \u2211H h=1 wv,h \u2264 1 and \u2211O\no=1 wh,o \u2264 1. The problem for the autoecoder to learn the dataset X can be described as\narg min W,W ||X \u2212Q||2, s.t. W \u2265 0,W \u2265 0, H\u2211 h=1 wv,h \u2264 1, O\u2211 o=1 wh,o \u2264 1. (5)\nWe use the following update rules to solve this problem, which are simplified from Liu\u2019s work [33]:\nwv,h \u2190 wv,h (XTXW\nT )v,h\n(XTXWWW T )v,h\n, (6)\nwh,o \u2190 wh,o (W TXTX)h,o\n(W TXTXWW )h,o , (7)\nwhere the symbol (\u00b7)v,h denotes the element in the vth row and hth column of a matrix. Note that, to avoid the division-by-zero problem, zero elements in the denominators of (6) and (7) are replaced with tiny positive values, (e.g., \u201ceps\u201d in MATLAB). After each update, adjustments need to be made such that W and W satisfy the RNN constraints. The procedure to train the shallow LRNN autoencoder (4) is given in Algorithm 1, where the operation max(W ) produces the maximal element in W , the operations of wv,h \u2190 wv,h/ \u2211H h=1 wv,h and wh,o \u2190 wh,o/ \u2211O o=1 wh,o guarantee that the weights satisfy the RNN constraints, and the operations W \u2190 W/max(X\u0304W ) and W \u2190 W/max(HW ) normalize the weights to reduce the number of neurons that are saturated.\nAlgorithm 1 Procedure for training a shallow nonnegatvie LRNN autoencder (4) Randomly initialize W and W that satisfy RNN constraints while terminal condition is not satisfied do\nfor each minibatch X\u0304 do update W with (6) for v = 1, \u00b7 \u00b7 \u00b7 , V do\nif \u2211H\nh=1 wv,h > 1 wv,h \u2190 wv,h/ \u2211H\nh=1 wv,h, for h = 1, \u00b7 \u00b7 \u00b7 , H W \u2190W/max(X\u0304W ) update W with (7) for h = 1, \u00b7 \u00b7 \u00b7 , H do\nif \u2211O\no=1 wh,o > 1 wh,o \u2190 wh,o/ \u2211O\no=1 wh,o, for o = 1, \u00b7 \u00b7 \u00b7 , O H = min(X\u0304W, 1) W \u2190W/max(HW )"}, {"heading": "4 Multi-layer non-negative LRNN autoencoder", "text": "We stack multi LRNNs to build a multi-layer non-negative LRNN autoencoder. Suppose the multilayer autoencoder has a visual layer, M encoding layers and M decoding layer (M \u2265 2), and they are connected in series with excitatory weights Wm and W with m = 1, \u00b7 \u00b7 \u00b7 ,M . We import a dataset X into the visual layer of the autoencoder. Let Hm and Om denote the numbers of neurons in the mth encoding layer and decoding layer, respectively. For the autoencoder, V = OM , Hm = OM\u2212m with m = 1, \u00b7 \u00b7 \u00b7 ,M \u2212 1.\nLet Q\u0302 denote the state of the visual layer, Qm denote the state of the mth encoding layer and Qm denote the state of the mth decoding layer. Then, the multi-layer LRNN autoencoder is described by{\nQ\u0302 = min(X, 1), Q1 = min(Q\u0302W1, 1), Qm = min(Qm\u22121Wm, 1), Q1 = min(QMW 1, 1), Qm = min(Qm\u22121Wm, 1), (8)\nwith m = 2, \u00b7 \u00b7 \u00b7 ,M . The RNN constraints for (8) are Wm \u2265 0, Wm \u2265 0 and the summation of each row in Wm and Wm is not larger than 1, where m = 1, \u00b7 \u00b7 \u00b7 ,M . The problem for the multi-layer LRNN autoencoder (8) to learn dataset X can be described as\narg min Wm,Wm\n||X \u2212QM ||2, (9)\nsubject to the RNN constraints, where m = 1, \u00b7 \u00b7 \u00b7 ,M . The procedure to train the multi-layer non-negative LRNN autoencder (8) is given in Algorithm 2.\nTo avoid loading the whole dataset into the computer memory, we could also use Algorithm 3 to train the autoencoder, where the update rules could be\nW1 \u2190W1 Q\u0302TQ\u0302W\nT M\nQ\u0302TQ\u0302W1WMW T M\n, Wm \u2190Wm QTm\u22121Qm\u22121W\nT M\u2212m+1\nQTm\u22121Qm\u22121WmWM\u2212m+1W T M\u2212m+1\n, (10)\nWM \u2190WM W T1 Q\u0302 TQ\u0302\nW T1 Q\u0302 TQ\u0302W1WM\n, WM\u2212m+1 \u2190WM\u2212m+1 W TmQ T m\u22121Qm\u22121\nW TmQ T m\u22121Qm\u22121WmWM\u2212m+1\n,\n(11)\nwith m = 2, \u00b7 \u00b7 \u00b7 ,M and the operation denoting element-wise product of two matrices. To avoid the division-by-zero problem, zero elements in denominators of (10) and (11) are replaced with tiny positive values. The operations of adjusting the weights to satisfy the RNN constraints and normalizing the weights are the same as those in Algorithm 1.\nAlgorithm 2 Proceduce for training a multi-layer LRNN-based non-negatvie autoencder (8) X1 = X for m = 1, \u00b7 \u00b7 \u00b7 ,M do\nTrain Wm and WM\u2212m+1 with Algorithm 1 that takes Xm as input dataset if m 6= M do Xm+1 = min(XmWm, 1)\nAlgorithm 3 Proceduce for training a multi-layer LRNN-based non-negatvie autoencder (8) (minibatch manner)\nRandomly initialize Wm and Wm that satisfy RNN constraints (with m = 1, \u00b7 \u00b7 \u00b7 ,M ) while terminal condition is not satisfied do\nfor each minibatch X\u0304 do for m = 1, \u00b7 \u00b7 \u00b7 ,M do\nupdate Wm with (10) adjust Wm to satisfy RNN constraints normalize Wm subject to X\u0304 update Wm with (11) adjust Wm to satisfy RNN constraints normalize Wm subject to X\u0304"}, {"heading": "5 Numerical Experiments", "text": ""}, {"heading": "5.1 Datasets", "text": "MNIST: The MNIST dataset of handwritten digits [29] contains 60,000 and 10,000 images in the training and test dataset. The number of input attributes is 784 (28\u00d7 28 images), which are in [0, 1]. Yale face: This database (http://vision.ucsd.edu/content/yale-face-database) contains 165 gray scale images of 15 individuals. Here we use the pre-processed dataset from [30], where each image is resized as 32\u00d7 32 (1024 pixels). CIFAR-10: The CIFAR-10 dataset consists of 60,000 32\u00d7 32 colour images [31]. Each image has 3072 attributes. It contains 50,000 and 10,000 images in the training and test dataset.\nUCI real-world datasets: In addition to image datasets, we also conduct numerical experiments on different real-world datasets in different areas from the UCI machine learning repository [32]. The names, attribute numbers and instance numbers of these datasets are listed in Table 1."}, {"heading": "5.2 Convergence and reconstruction performance", "text": "Results of MNIST: Let us first test the convergence and reconstruction performance of the shallow non-negative LRNN autoencoder. We use structures of 784 \u2192 100 (for simplicity, we use the encoding part to represent an autoencoder) and 784\u2192 50 and the MNIST dataset for experiments. The whole training dataset of 60,000 images is used for training. Figure 2(a) shows the curves of training error (mean square error) versus the number of iterations, where, in each iteration, a minibatch of size 100 is handled. Then, we use a multi-layer non-negative LRNN autoencoder with structure 784 \u2192 1000 \u2192 500 \u2192 250 \u2192 50, and the corresponding curve of training error versus iterations is also given in Figure 2(a). It can be seen from Figure 2(a) that reconstruction errors using the LRNN autoencoders equipped with the developed algorithms converge well for different structures. In addition, the lowest errors using the shallow and multi\u2013layer autoencoders are respectively 0.0204 and 0.0190. The results show that, for the same encoding dimension, the performances of the shallow and multi-layer structures are similar for this dataset.\nResults of Yale face: Attribute values are normalized into [0, 1] (by dividing by 255). The structures for the shallow and multi-layer LRNN autoencoders are respectively 1024\u2192 50 and 1024\u2192 500\u2192 100\u2192 50. The size of a minibatch is 5. Curves of reconstruction errors versus iterations are given in Figure 2(b). For this dataset, the shallow autoencoder seems more stable than the multi-layer one.\nResults of CIFAR-10: Attribute values of the dataset are also divided by 255 for normalization in range [0, 1]. The structures used are 3072 \u2192 150 and 3072 \u2192 1000 \u2192 500 \u2192 150. Both the training and testing dataset (total 60,000 images) are used for training the autoencoders. The size of minibatch is chosen as 100. The results are given in Figure 2(c). We can see that reconstruction errors for both structures converge as the number of iterations increases. In addition, the lowest reconstruction errors in using the shallow and multi-layer autoencoders are the same (0.0082). These results together with those with the MNIST and Yale face datasets (Figures 2(a) to 2(c)) verify the good convergence and reconstruction performance of both the shallow and multi-layer LRNN autoencoders for handling image datsets.\nResults of UCI real-world datasets: Let N denote the attribute number in a dataset. The structures of the LRNN autoencoders used are N \u2192 round(N/2), where the operation round(\u00b7) produces\nthe nearest integer number of the element. The attribute values are linear normalized in range [0, 1]. The size of mini-batches is set as 50 for all datasets. Curves of reconstruction errors versus iterations are given in Figure 3. We see that the reconstruction errors generally decrease as the number of iterations increases. These results also demonstrate the efficacy of the nonnegative LRNN autoencoders equipped with the training algorithms."}, {"heading": "6 Simulating the spiking random neural network", "text": "The advantage of a spiking model, such as the LRNN autoencoder, lays on its highly-distributed nature. In this section, rather than numerical calculation, we simulate the stochastic spiking behaviors of the LRNN autoencoder. The simulation in this section is based on the numerical experiment of Subsection 5.2. Specifically, in Subsection 5.2, we construct a LRNN autoencoder of structure 784\u2192 100 (with appropriate weights found), which has three layers: the visual layer (784 neurons), hidden layer (100 neurons) and output layer (784 neurons). First, an image with 28 \u00d7 28 = 784 attributes is taken from the MNIST dataset. Each visual neuron receives excitatory spikes from outside the network in a Poisson stream with the rate being the corresponding attribute value in the image. When activated, the visual neurons fire excitatory spikes to the hidden neurons according to the Poisson process with rate 1 (meaning wv,h = p+v,h). When the vth visual neuron fires to the hidden layer, the spike goes to the hth hidden neuron with probability p+v,h or it goes outside the\nnetwork with probability 1 \u2212 \u2211H\nh=1 p + v,h. The hidden neurons fire excitatory spikes to the output\nlayer in a similar manner subjecting to wh,o. The firing rate of output neurons is 1 and the spikes go outside the network with probability 1.\nIn the simulation, we call it an event whenever a spike gets in from outside the network or a neuron fires. During the simulation, we observe the potential (the level of activation) of each neuron once every 1,000 events. Let ki,b represent the bth observation of the ith neuron. We estimate the average potential of the ith neuron, denoted by k\u0304i, simply by averaging observations, i.e., k\u0304i \u2248 ( \u2211B b=1 ki,b)/B. Let qi denote the probability that the ith neuron is activated. The relation between qi and k\u0304i is known as k\u0304i = qi/(1\u2212 qi). Then, the value of qi can be estimated during the simulation as:\nqi = k\u0304i\n1 + k\u0304i \u2248\n( \u2211B\nb=1 ki,b)/B 1 + ( \u2211B b=1 ki,b)/B . (12)\nIn Figure 4, we visualize the estimated values of qi for all neurons in different layers after 10,000, 100,000 and 1,000,000 events during the simulation. For comparison, numerical results from Subsection 5.2 are also given in Figure 4. At the beginning, simulation results of only the visual layer are close to its numerical results. As time evolves, the simulation results of the hidden and output layers and their corresponding numerical results become more and more similar. These results demonstrate that the LRNN autoencoders have the potential to be implemented in a highly distributed and parallel manner."}, {"heading": "7 Conclusions", "text": "New nonnegative autoencoders (the shallow and multi-layer LRNN autoencoders) have been proposed based on the spiking RNN model, which adopt the feed-forword multi-layer network architecture in the deep-learning area. To comply the RNN constraints of nonnegativity and that the sum of probabilities is no larger than 1, learning algorithms have been developed by adapting weight update rules from the NMF area. Numerical results based on typical image datasets including the MNIST, Yale face and CIFAR-10 datesets and 16 real-world datasets from different areas have well verified the robust convergence and reconstruction performance of the LRNN autoencoder. In addition to numerical experiments, we have conducted simulations of the autoencoder where the stochastic spiking behaviors are simulated. Simulation results conform well with the corresponding numerical results. This demonstrates that the LRNN autoencoder can be implemented in a highly distributed and parallel manner."}], "references": [{"title": "Random neural networks with negative and positive signals and product form solution", "author": ["E. Gelenbe"], "venue": "Neural computation, vol. 1, no. 4, pp. 502\u2013510, 1989.", "citeRegEx": "1", "shortCiteRegEx": null, "year": 1989}, {"title": "Stability of the random neural network model", "author": ["\u2014\u2014"], "venue": "Neural computation, vol. 2, no. 2, pp. 239\u2013247, 1990.", "citeRegEx": "2", "shortCiteRegEx": null, "year": 1990}, {"title": "Learning in the recurrent random neural network", "author": ["\u2014\u2014"], "venue": "Neural Computation, vol. 5, pp. 154\u2013164, 1993.", "citeRegEx": "3", "shortCiteRegEx": null, "year": 1993}, {"title": "Dynamical random neural network approach to the traveling salesman problem", "author": ["E. Gelenbe", "V. Koubi", "F. Pekergin"], "venue": "Proceedings IEEE Symp. Systems, Man and Cybernetics. IEEE, 1993, p. 630\u2013635.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 1993}, {"title": "Video quality and traffic qos in learning-based subsampled and receiver-interpolated video sequences", "author": ["C.E. Cramer", "E. Gelenbe"], "venue": "Selected Areas in Communications, IEEE Journal on, vol. 18, no. 2, pp. 150\u2013167, 2000.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2000}, {"title": "Learning in the multiple class random neural network", "author": ["E. Gelenbe", "K.F. Hussain"], "venue": "Neural Networks, IEEE Transactions on, vol. 13, no. 6, pp. 1257\u20131267, 2002.", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2002}, {"title": "Steps toward self-aware networks", "author": ["E. Gelenbe"], "venue": "Communications of the ACM, vol. 52, no. 7, pp. 66\u201375, 2009.", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2009}, {"title": "Large scale simulation for human evacuation and rescue", "author": ["E. Gelenbe", "F.-J. Wu"], "venue": "Computers & Mathematics with Applications, vol. 64, no. 12, pp. 3869\u20133880, 2012.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2012}, {"title": "A new approach for the prediction of end-to-end performance of multimedia streams", "author": ["G. Rubino", "M. Varela"], "venue": "Quantitative Evaluation of Systems, 2004. QEST 2004. Proceedings. First International Conference on the. IEEE, 2004, pp. 110\u2013119.", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2004}, {"title": "A study of real-time packet video quality using random neural networks", "author": ["S. Mohamed", "G. Rubino"], "venue": "IEEE Transactions on Circuits and Systems for Video Technology, vol. 12, no. 12, pp. 1071\u20131083, 2002.", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2002}, {"title": "Evaluating perceived voice quality on packet networks using different random neural network architectures", "author": ["K. Radhakrishnan", "H. Larijani"], "venue": "Performance Evaluation, vol. 68, no. 4, pp. 347\u2013360, 2011.", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2011}, {"title": "Non-intrusive method for video quality prediction over lte using random neural networks (rnn)", "author": ["T. Ghalut", "H. Larijani"], "venue": "Communication Systems, Networks & Digital Signal Processing (CSNDSP), 2014 9th International Symposium on. IEEE, 2014, pp. 519\u2013524.", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2014}, {"title": "Pictorial information retrieval using the random neural network", "author": ["A. Stafylopatis", "A. Likas"], "venue": "IEEE Transactions on Software Engineering, vol. 18, no. 7, pp. 590\u2013600, 1992.", "citeRegEx": "13", "shortCiteRegEx": null, "year": 1992}, {"title": "Voice quality in voip networks based on random neural networks", "author": ["H. Larijani", "K. Radhakrishnan"], "venue": "Networks (ICN), 2010 Ninth International Conference on. IEEE, 2010, pp. 89\u201392.", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2010}, {"title": "A grasp algorithm using rnn for solving dynamics in a p2p live video streaming network", "author": ["M. Mart\u00ednez", "A. Mor\u00f3n", "F. Robledo", "P. Rodr\u00edguez-Bocca", "H. Cancela", "G. Rubino"], "venue": "Hybrid Intelligent Systems, 2008. HIS\u201908. Eighth International Conference on. IEEE, 2008, pp. 447\u2013452.", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2008}, {"title": "Reducing the dimensionality of data with neural networks", "author": ["G.E. Hinton", "R.R. Salakhutdinov"], "venue": "Science, vol. 313, no. 5786, pp. 504\u2013507, 2006.", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2006}, {"title": "Deep learning", "author": ["Y. LeCun", "Y. Bengio", "G. Hinton"], "venue": "Nature, vol. 521, no. 7553, pp. 436\u2013444, 2015.", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2015}, {"title": "Understanding the difficulty of training deep feedforward neural networks.", "author": ["X. Glorot", "Y. Bengio"], "venue": "in Aistats, vol", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2010}, {"title": "A fast learning algorithm for deep belief nets", "author": ["G.E. Hinton", "S. Osindero", "Y.-W. Teh"], "venue": "Neural computation, vol. 18, no. 7, pp. 1527\u20131554, 2006.", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2006}, {"title": "Deep sparse rectifier neural networks.", "author": ["X. Glorot", "A. Bordes", "Y. Bengio"], "venue": "in Aistats,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2011}, {"title": "The tradeoffs of large scale learning", "author": ["O. Bousquet", "L. Bottou"], "venue": "Advances in neural information processing systems, 2008, pp. 161\u2013168. 9", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2008}, {"title": "Learning the parts of objects by non-negative matrix factorization", "author": ["D.D. Lee", "H.S. Seung"], "venue": "Nature, vol. 401, no. 6755, pp. 788\u2013791, 1999.", "citeRegEx": "22", "shortCiteRegEx": null, "year": 1999}, {"title": "Projective nonnegative graph embedding", "author": ["X. Liu", "S. Yan", "H. Jin"], "venue": "IEEE Transactions on Image Processing, vol. 19, no. 5, pp. 1126\u20131137, 2010.", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2010}, {"title": "Non-negative sparse coding", "author": ["P.O. Hoyer"], "venue": "Neural Networks for Signal Processing, 2002. Proceedings of the 2002 12th IEEE Workshop on. IEEE, 2002, pp. 557\u2013565.", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2002}, {"title": "Nonnegative matrix factorization: A comprehensive review", "author": ["Y.-X. Wang", "Y.-J. Zhang"], "venue": "IEEE Transactions on Knowledge and Data Engineering, vol. 25, no. 6, pp. 1336\u20131353, 2013.", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2013}, {"title": "Orthogonal nonnegative matrix t-factorizations for clustering", "author": ["C. Ding", "T. Li", "W. Peng", "H. Park"], "venue": "Proceedings of the 12th ACM SIGKDD international conference on Knowledge discovery and data mining. ACM, 2006, pp. 126\u2013135.", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2006}, {"title": "Recognition of objects and their component parts: responses of single units in the temporal cortex of the macaque", "author": ["E. Wachsmuth", "M. Oram", "D. Perrett"], "venue": "Cerebral Cortex, vol. 4, no. 5, pp. 509\u2013522, 1994.", "citeRegEx": "27", "shortCiteRegEx": null, "year": 1994}, {"title": "On the equivalence of nonnegative matrix factorization and spectral clustering.", "author": ["C.H. Ding", "X. He", "H.D. Simon"], "venue": "in SDM,", "citeRegEx": "28", "shortCiteRegEx": "28", "year": 2005}, {"title": "Gradient-based learning applied to document recognition", "author": ["Y. LeCun", "L. Bottou", "Y. Bengio", "P. Haffner"], "venue": "Proceedings of the IEEE, vol. 86, no. 11, pp. 2278\u20132324, 1998.", "citeRegEx": "29", "shortCiteRegEx": null, "year": 1998}, {"title": "Learning a spatially smooth subspace for face recognition", "author": ["D. Cai", "X. He", "Y. Hu", "J. Han", "T. Huang"], "venue": "2007 IEEE Conference on Computer Vision and Pattern Recognition. IEEE, 2007, pp. 1\u20137.", "citeRegEx": "30", "shortCiteRegEx": null, "year": 2007}, {"title": "Learning multiple layers of features from tiny images", "author": ["A. Krizhevsky", "G. Hinton"], "venue": "2009.", "citeRegEx": "31", "shortCiteRegEx": null, "year": 2009}, {"title": "UCI machine learning repository", "author": ["M. Lichman"], "venue": "2013. [Online]. Available: http: //archive.ics.uci.edu/ml", "citeRegEx": "32", "shortCiteRegEx": null, "year": 2013}, {"title": "Projective nonnegative graph embedding", "author": ["X. Liu", "S. Yan", "H. Jin"], "venue": "Image Processing, IEEE Transactions on, vol. 19, no. 5, pp. 1126\u20131137, 2010.", "citeRegEx": "33", "shortCiteRegEx": null, "year": 2010}, {"title": "Pattern recognition via linear programming: Theory and application to medical diagnosis", "author": ["O.L. Mangasarian", "R. Setiono", "W. Wolberg"], "venue": "Large-scale numerical optimization, pp. 22\u201331, 1990.", "citeRegEx": "35", "shortCiteRegEx": null, "year": 1990}, {"title": "Robust linear programming discrimination of two linearly inseparable sets", "author": ["K.P. Bennett", "O.L. Mangasarian"], "venue": "Optimization methods and software, vol. 1, no. 1, pp. 23\u201334, 1992.", "citeRegEx": "36", "shortCiteRegEx": null, "year": 1992}, {"title": "Exploiting nonlinear recurrence and fractal scaling properties for voice disorder detection", "author": ["M.A. Little", "P.E. McSharry", "S.J. Roberts", "D.A. Costello", "I.M. Moroz"], "venue": "BioMedical Engineering OnLine, vol. 6, no. 1, p. 1, 2007.", "citeRegEx": "37", "shortCiteRegEx": null, "year": 2007}, {"title": "Short-term memory mechanisms in neural network learning of robot navigation tasks: A case study", "author": ["A.L. Freire", "G.A. Barreto", "M. Veloso", "A.T. Varela"], "venue": "Robotics Symposium (LARS), 2009 6th Latin American. IEEE, 2009, pp. 1\u20136.", "citeRegEx": "38", "shortCiteRegEx": null, "year": 2009}, {"title": "Classification of radar returns from the ionosphere using neural networks", "author": ["V.G. Sigillito", "S.P. Wing", "L.V. Hutton", "K.B. Baker"], "venue": "Johns Hopkins APL Technical Digest, vol. 10, no. 3, pp. 262\u2013266, 1989.", "citeRegEx": "39", "shortCiteRegEx": null, "year": 1989}, {"title": "Machine learning for first-order theorem proving", "author": ["J.P. Bridge", "S.B. Holden", "L.C. Paulson"], "venue": "Journal of automated reasoning, vol. 53, no. 2, pp. 141\u2013172, 2014.", "citeRegEx": "40", "shortCiteRegEx": null, "year": 2014}, {"title": "Analysis of hidden units in a layered network trained to classify sonar targets", "author": ["R.P. Gorman", "T.J. Sejnowski"], "venue": "Neural networks, vol. 1, no. 1, pp. 75\u201389, 1988.", "citeRegEx": "41", "shortCiteRegEx": null, "year": 1988}, {"title": "A supervised machine learning algorithm for arrhythmia analysis", "author": ["H.A. Guvenir", "B. Acar", "G. Demiroz", "A. Cekin"], "venue": "Computers in Cardiology 1997. IEEE, 1997, pp. 433\u2013436. 10", "citeRegEx": "42", "shortCiteRegEx": null, "year": 1997}], "referenceMentions": [{"referenceID": 0, "context": "A mathematical tool that has existed since 1989 [1\u20133], but is less well known in the machine-learning community, is the Random Neural Network (RNN), which is a stochastic integer-state \u201cintegrate and fire\u201d system and developed to mimic the behaviour of biological neurons in the brain.", "startOffset": 48, "endOffset": 53}, {"referenceID": 1, "context": "A mathematical tool that has existed since 1989 [1\u20133], but is less well known in the machine-learning community, is the Random Neural Network (RNN), which is a stochastic integer-state \u201cintegrate and fire\u201d system and developed to mimic the behaviour of biological neurons in the brain.", "startOffset": 48, "endOffset": 53}, {"referenceID": 2, "context": "A mathematical tool that has existed since 1989 [1\u20133], but is less well known in the machine-learning community, is the Random Neural Network (RNN), which is a stochastic integer-state \u201cintegrate and fire\u201d system and developed to mimic the behaviour of biological neurons in the brain.", "startOffset": 48, "endOffset": 53}, {"referenceID": 3, "context": "The RNN has been used for numerous applications [4\u201315] that exploit its recurrent structure.", "startOffset": 48, "endOffset": 54}, {"referenceID": 4, "context": "The RNN has been used for numerous applications [4\u201315] that exploit its recurrent structure.", "startOffset": 48, "endOffset": 54}, {"referenceID": 5, "context": "The RNN has been used for numerous applications [4\u201315] that exploit its recurrent structure.", "startOffset": 48, "endOffset": 54}, {"referenceID": 6, "context": "The RNN has been used for numerous applications [4\u201315] that exploit its recurrent structure.", "startOffset": 48, "endOffset": 54}, {"referenceID": 7, "context": "The RNN has been used for numerous applications [4\u201315] that exploit its recurrent structure.", "startOffset": 48, "endOffset": 54}, {"referenceID": 8, "context": "The RNN has been used for numerous applications [4\u201315] that exploit its recurrent structure.", "startOffset": 48, "endOffset": 54}, {"referenceID": 9, "context": "The RNN has been used for numerous applications [4\u201315] that exploit its recurrent structure.", "startOffset": 48, "endOffset": 54}, {"referenceID": 10, "context": "The RNN has been used for numerous applications [4\u201315] that exploit its recurrent structure.", "startOffset": 48, "endOffset": 54}, {"referenceID": 11, "context": "The RNN has been used for numerous applications [4\u201315] that exploit its recurrent structure.", "startOffset": 48, "endOffset": 54}, {"referenceID": 12, "context": "The RNN has been used for numerous applications [4\u201315] that exploit its recurrent structure.", "startOffset": 48, "endOffset": 54}, {"referenceID": 13, "context": "The RNN has been used for numerous applications [4\u201315] that exploit its recurrent structure.", "startOffset": 48, "endOffset": 54}, {"referenceID": 14, "context": "The RNN has been used for numerous applications [4\u201315] that exploit its recurrent structure.", "startOffset": 48, "endOffset": 54}, {"referenceID": 15, "context": "Deep learning has achieved great success in machine learning [16,17].", "startOffset": 61, "endOffset": 68}, {"referenceID": 16, "context": "Deep learning has achieved great success in machine learning [16,17].", "startOffset": 61, "endOffset": 68}, {"referenceID": 17, "context": "The feed-forward fully-connected multi-layer neural network could be difficult to train [18].", "startOffset": 88, "endOffset": 92}, {"referenceID": 15, "context": "Pre-training the network layer by layer is a great advance [16, 19] and useful due to its wide adaptability, though recent literature shows that utilizing the rectified linear unit (ReLU) could train a deep neural network without pre-training [20].", "startOffset": 59, "endOffset": 67}, {"referenceID": 18, "context": "Pre-training the network layer by layer is a great advance [16, 19] and useful due to its wide adaptability, though recent literature shows that utilizing the rectified linear unit (ReLU) could train a deep neural network without pre-training [20].", "startOffset": 59, "endOffset": 67}, {"referenceID": 19, "context": "Pre-training the network layer by layer is a great advance [16, 19] and useful due to its wide adaptability, though recent literature shows that utilizing the rectified linear unit (ReLU) could train a deep neural network without pre-training [20].", "startOffset": 243, "endOffset": 247}, {"referenceID": 20, "context": "The typical training procedure called stochastic gradient descent (SGD) provides a practical choice for handling large datasets [21].", "startOffset": 128, "endOffset": 132}, {"referenceID": 21, "context": "Nonnegative matrix factorization (NMF) is also a popular topic in machine learning [22\u201326], which learns part-based representations of raw data.", "startOffset": 83, "endOffset": 90}, {"referenceID": 22, "context": "Nonnegative matrix factorization (NMF) is also a popular topic in machine learning [22\u201326], which learns part-based representations of raw data.", "startOffset": 83, "endOffset": 90}, {"referenceID": 23, "context": "Nonnegative matrix factorization (NMF) is also a popular topic in machine learning [22\u201326], which learns part-based representations of raw data.", "startOffset": 83, "endOffset": 90}, {"referenceID": 24, "context": "Nonnegative matrix factorization (NMF) is also a popular topic in machine learning [22\u201326], which learns part-based representations of raw data.", "startOffset": 83, "endOffset": 90}, {"referenceID": 25, "context": "Nonnegative matrix factorization (NMF) is also a popular topic in machine learning [22\u201326], which learns part-based representations of raw data.", "startOffset": 83, "endOffset": 90}, {"referenceID": 21, "context": "Lee [22] suggested that the perception of the whole in the brain may be based on these part-based representations (based on the physiological evidence [27]) and proposed simple yet effective update rules.", "startOffset": 4, "endOffset": 8}, {"referenceID": 26, "context": "Lee [22] suggested that the perception of the whole in the brain may be based on these part-based representations (based on the physiological evidence [27]) and proposed simple yet effective update rules.", "startOffset": 151, "endOffset": 155}, {"referenceID": 23, "context": "Hoyer [24] combined sparse coding and NMF that allows control over sparseness.", "startOffset": 6, "endOffset": 10}, {"referenceID": 25, "context": "clustering in [26, 28] and presented simple update rules for orthogonal NMF.", "startOffset": 14, "endOffset": 22}, {"referenceID": 27, "context": "clustering in [26, 28] and presented simple update rules for orthogonal NMF.", "startOffset": 14, "endOffset": 22}, {"referenceID": 24, "context": "Wang [25] provided a comprehensive review on recent processes in the NMF area.", "startOffset": 5, "endOffset": 9}, {"referenceID": 15, "context": "Then, this shallow autoencoder is stacked into a multi-layer feed-forward autoencoder following the network architecture in the deep learning area [16, 17, 19].", "startOffset": 147, "endOffset": 159}, {"referenceID": 16, "context": "Then, this shallow autoencoder is stacked into a multi-layer feed-forward autoencoder following the network architecture in the deep learning area [16, 17, 19].", "startOffset": 147, "endOffset": 159}, {"referenceID": 18, "context": "Then, this shallow autoencoder is stacked into a multi-layer feed-forward autoencoder following the network architecture in the deep learning area [16, 17, 19].", "startOffset": 147, "endOffset": 159}, {"referenceID": 28, "context": "The efficacy of the nonnegative autoencoders equipped with the learning algorithms is well verified via numerical experiments on both typical image datasets including the MNIST [29], Yale face [30] and CIFAR-10 [31] datesets and 16 real-world datasets in different areas from the UCI machine learning repository [32].", "startOffset": 177, "endOffset": 181}, {"referenceID": 29, "context": "The efficacy of the nonnegative autoencoders equipped with the learning algorithms is well verified via numerical experiments on both typical image datasets including the MNIST [29], Yale face [30] and CIFAR-10 [31] datesets and 16 real-world datasets in different areas from the UCI machine learning repository [32].", "startOffset": 193, "endOffset": 197}, {"referenceID": 30, "context": "The efficacy of the nonnegative autoencoders equipped with the learning algorithms is well verified via numerical experiments on both typical image datasets including the MNIST [29], Yale face [30] and CIFAR-10 [31] datesets and 16 real-world datasets in different areas from the UCI machine learning repository [32].", "startOffset": 211, "endOffset": 215}, {"referenceID": 31, "context": "The efficacy of the nonnegative autoencoders equipped with the learning algorithms is well verified via numerical experiments on both typical image datasets including the MNIST [29], Yale face [30] and CIFAR-10 [31] datesets and 16 real-world datasets in different areas from the UCI machine learning repository [32].", "startOffset": 312, "endOffset": 316}, {"referenceID": 2, "context": "From the preceding assumptions it was proved in [3] that for an arbitrary N neuron RNN, which may or may not be recurrent (i.", "startOffset": 48, "endOffset": 51}, {"referenceID": 2, "context": "In [3], it was shown that the system of N non-linear equations (1) have a solution which is unique.", "startOffset": 3, "endOffset": 6}, {"referenceID": 0, "context": "According to [1] and (1), they are given by q\u0302v = min(\u039b\u0302v /r\u0302v, 1), and qh = min(\u039b + h /rh, 1), where the quantities \u039b\u0302 + v and \u039b + h represent the total average arrival rates of excitatory spikes, r\u0302v and rh represent the firing rates of the neurons.", "startOffset": 13, "endOffset": 16}, {"referenceID": 32, "context": "We use the following update rules to solve this problem, which are simplified from Liu\u2019s work [33]:", "startOffset": 94, "endOffset": 98}, {"referenceID": 28, "context": "MNIST: The MNIST dataset of handwritten digits [29] contains 60,000 and 10,000 images in the training and test dataset.", "startOffset": 47, "endOffset": 51}, {"referenceID": 0, "context": "The number of input attributes is 784 (28\u00d7 28 images), which are in [0, 1].", "startOffset": 68, "endOffset": 74}, {"referenceID": 29, "context": "Here we use the pre-processed dataset from [30], where each image is resized as 32\u00d7 32 (1024 pixels).", "startOffset": 43, "endOffset": 47}, {"referenceID": 30, "context": "CIFAR-10: The CIFAR-10 dataset consists of 60,000 32\u00d7 32 colour images [31].", "startOffset": 71, "endOffset": 75}, {"referenceID": 31, "context": "UCI real-world datasets: In addition to image datasets, we also conduct numerical experiments on different real-world datasets in different areas from the UCI machine learning repository [32].", "startOffset": 187, "endOffset": 191}, {"referenceID": 33, "context": "Iris 4 150 Teaching Assistant Evaluation (TAE) 5 151 Liver Disorders (LD) 5 345 Seeds 7 210 Pima Indians Diabetes (PID) 8 768 Breast Cancer Wisconsin (BC) [34\u201336] 9 699 Glass 9 214 Wine 13 178 Zoo 16 100 Parkinsons [37] 22 195 Wall-Following Robot Navigation (WFRN) [38] 24 5456 Ionosphere [39] 34 351 Soybean Large (SL) 35 186 First-Order Theorem Proving (FOTP) [40] 51 6118 Sonar [41] 60 208 Cardiac Arrhythmia (CA) [42] 279 452", "startOffset": 155, "endOffset": 162}, {"referenceID": 34, "context": "Iris 4 150 Teaching Assistant Evaluation (TAE) 5 151 Liver Disorders (LD) 5 345 Seeds 7 210 Pima Indians Diabetes (PID) 8 768 Breast Cancer Wisconsin (BC) [34\u201336] 9 699 Glass 9 214 Wine 13 178 Zoo 16 100 Parkinsons [37] 22 195 Wall-Following Robot Navigation (WFRN) [38] 24 5456 Ionosphere [39] 34 351 Soybean Large (SL) 35 186 First-Order Theorem Proving (FOTP) [40] 51 6118 Sonar [41] 60 208 Cardiac Arrhythmia (CA) [42] 279 452", "startOffset": 155, "endOffset": 162}, {"referenceID": 35, "context": "Iris 4 150 Teaching Assistant Evaluation (TAE) 5 151 Liver Disorders (LD) 5 345 Seeds 7 210 Pima Indians Diabetes (PID) 8 768 Breast Cancer Wisconsin (BC) [34\u201336] 9 699 Glass 9 214 Wine 13 178 Zoo 16 100 Parkinsons [37] 22 195 Wall-Following Robot Navigation (WFRN) [38] 24 5456 Ionosphere [39] 34 351 Soybean Large (SL) 35 186 First-Order Theorem Proving (FOTP) [40] 51 6118 Sonar [41] 60 208 Cardiac Arrhythmia (CA) [42] 279 452", "startOffset": 215, "endOffset": 219}, {"referenceID": 36, "context": "Iris 4 150 Teaching Assistant Evaluation (TAE) 5 151 Liver Disorders (LD) 5 345 Seeds 7 210 Pima Indians Diabetes (PID) 8 768 Breast Cancer Wisconsin (BC) [34\u201336] 9 699 Glass 9 214 Wine 13 178 Zoo 16 100 Parkinsons [37] 22 195 Wall-Following Robot Navigation (WFRN) [38] 24 5456 Ionosphere [39] 34 351 Soybean Large (SL) 35 186 First-Order Theorem Proving (FOTP) [40] 51 6118 Sonar [41] 60 208 Cardiac Arrhythmia (CA) [42] 279 452", "startOffset": 266, "endOffset": 270}, {"referenceID": 37, "context": "Iris 4 150 Teaching Assistant Evaluation (TAE) 5 151 Liver Disorders (LD) 5 345 Seeds 7 210 Pima Indians Diabetes (PID) 8 768 Breast Cancer Wisconsin (BC) [34\u201336] 9 699 Glass 9 214 Wine 13 178 Zoo 16 100 Parkinsons [37] 22 195 Wall-Following Robot Navigation (WFRN) [38] 24 5456 Ionosphere [39] 34 351 Soybean Large (SL) 35 186 First-Order Theorem Proving (FOTP) [40] 51 6118 Sonar [41] 60 208 Cardiac Arrhythmia (CA) [42] 279 452", "startOffset": 290, "endOffset": 294}, {"referenceID": 38, "context": "Iris 4 150 Teaching Assistant Evaluation (TAE) 5 151 Liver Disorders (LD) 5 345 Seeds 7 210 Pima Indians Diabetes (PID) 8 768 Breast Cancer Wisconsin (BC) [34\u201336] 9 699 Glass 9 214 Wine 13 178 Zoo 16 100 Parkinsons [37] 22 195 Wall-Following Robot Navigation (WFRN) [38] 24 5456 Ionosphere [39] 34 351 Soybean Large (SL) 35 186 First-Order Theorem Proving (FOTP) [40] 51 6118 Sonar [41] 60 208 Cardiac Arrhythmia (CA) [42] 279 452", "startOffset": 363, "endOffset": 367}, {"referenceID": 39, "context": "Iris 4 150 Teaching Assistant Evaluation (TAE) 5 151 Liver Disorders (LD) 5 345 Seeds 7 210 Pima Indians Diabetes (PID) 8 768 Breast Cancer Wisconsin (BC) [34\u201336] 9 699 Glass 9 214 Wine 13 178 Zoo 16 100 Parkinsons [37] 22 195 Wall-Following Robot Navigation (WFRN) [38] 24 5456 Ionosphere [39] 34 351 Soybean Large (SL) 35 186 First-Order Theorem Proving (FOTP) [40] 51 6118 Sonar [41] 60 208 Cardiac Arrhythmia (CA) [42] 279 452", "startOffset": 382, "endOffset": 386}, {"referenceID": 40, "context": "Iris 4 150 Teaching Assistant Evaluation (TAE) 5 151 Liver Disorders (LD) 5 345 Seeds 7 210 Pima Indians Diabetes (PID) 8 768 Breast Cancer Wisconsin (BC) [34\u201336] 9 699 Glass 9 214 Wine 13 178 Zoo 16 100 Parkinsons [37] 22 195 Wall-Following Robot Navigation (WFRN) [38] 24 5456 Ionosphere [39] 34 351 Soybean Large (SL) 35 186 First-Order Theorem Proving (FOTP) [40] 51 6118 Sonar [41] 60 208 Cardiac Arrhythmia (CA) [42] 279 452", "startOffset": 418, "endOffset": 422}, {"referenceID": 0, "context": "Results of Yale face: Attribute values are normalized into [0, 1] (by dividing by 255).", "startOffset": 59, "endOffset": 65}, {"referenceID": 0, "context": "Results of CIFAR-10: Attribute values of the dataset are also divided by 255 for normalization in range [0, 1].", "startOffset": 104, "endOffset": 110}, {"referenceID": 0, "context": "The attribute values are linear normalized in range [0, 1].", "startOffset": 52, "endOffset": 58}], "year": 2016, "abstractText": "This paper proposes new nonnegative (shallow and multi-layer) autoencoders by combining the spiking Random Neural Network (RNN) model, the network architecture typical used in deep-learning area and the training technique inspired from nonnegative matrix factorization (NMF). The shallow autoencoder is a simplified RNN model, which is then stacked into a multi-layer architecture. The learning algorithm is based on the weight update rules in NMF, subject to the nonnegative probability constraints of the RNN. The autoencoders equipped with this learning algorithm are tested on typical image datasets including the MNIST, Yale face and CIFAR-10 datasets, and also using 16 real-world datasets from different areas. The results obtained through these tests yield the desired high learning and recognition accuracy. Also, numerical simulations of the stochastic spiking behavior of this RNN auto encoder, show that it can be implemented in a highly-distributed manner.", "creator": "LaTeX with hyperref package"}}}