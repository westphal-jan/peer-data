{"id": "1609.08221", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "26-Sep-2016", "title": "Simultaneous Low-rank Component and Graph Estimation for High-dimensional Graph Signals: Application to Brain Imaging", "abstract": "We propose an algorithm to uncover the intrinsic low-rank component of a high-dimensional, graph-smooth and grossly-corrupted dataset, under the situations that the underlying graph is unknown. Based on a model with a low-rank component plus a sparse perturbation, and an initial graph estimation, our proposed algorithm simultaneously learns the low-rank component and refines the graph. Our evaluations using synthetic and real brain imaging data in unsupervised and supervised classification tasks demonstrate encouraging performance in terms of neural complexity and structural components (such as high-density-resolution maps).\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n", "histories": [["v1", "Mon, 26 Sep 2016 23:24:27 GMT  (542kb,D)", "https://arxiv.org/abs/1609.08221v1", "Submitted to ICASSP 2017"], ["v2", "Mon, 9 Jan 2017 03:23:43 GMT  (1172kb,D)", "http://arxiv.org/abs/1609.08221v2", "Accepted by ICASSP 2017"]], "COMMENTS": "Submitted to ICASSP 2017", "reviews": [], "SUBJECTS": "cs.CV cs.LG", "authors": ["rui liu", "hossein nejati", "ngai-man cheung"], "accepted": false, "id": "1609.08221"}, "pdf": {"name": "1609.08221.pdf", "metadata": {"source": "CRF", "title": "SIMULTANEOUS LOW-RANK COMPONENT AND GRAPH ESTIMATION FOR HIGH-DIMENSIONAL GRAPH SIGNALS: APPLICATION TO BRAIN IMAGING", "authors": ["Liu Rui", "Hossein Nejati", "Seyed Hamid Safavi", "Ngai-Man Cheung"], "emails": [], "sections": [{"heading": null, "text": "Index Terms\u2014 Graph Signal Processing, Low Rank, Dimensionality Reduction, Graph Learning, Brain Imaging"}, {"heading": "1. INTRODUCTION", "text": "We consider the problem of uncovering the intrinsic low rank component of a high-dimensional dataset. We further focus on data that resides on a certain graph and the data changes smoothly between the connected vertices [1]. In many problems, the underlying graph is unknown or inexact [2, 3, 4]. For example, the graph may be estimated from the input data which is grossly corrupted. We propose an algorithm to estimate the low-rank component of the data, using the graph smoothness assumption to assist the estimation. Our algorithm also simultaneously and iteratively refines the graph to improve the effectiveness of the graph smoothness constraint, thereby increasing the quality of low-rank component estimation.\nHigh-dimensional data is common in many engineering areas such as image / video processing, biomedical imaging, computer networks and transportation networks. We are specifically interested in automatic analysis of brain imaging data. In many cases, the task is to find the spatiotemporal neural signature of a task, by performing classification on cortical activations evoked by different stimuli [5, 6]. Common brain imaging techniques are Electroencephalography (EEG) and Magnetoencephalography (MEG). These measurements are high-dimensional spatiotemporal data. For instance, in our experiments, we use a recumbent Elekta MEG scanner with 306 sensors to record the brain activity for 1100 milliseconds. Furthermore, the measurements are degraded by various types of noise (e.g., sensor noise, ambient magnetic field noise, etc.) and the noise model is complicated (potentially non-Gaussian). The high-dimensionality and noise limit both the speed and accuracy of the signal analysis, that may result in unreliable signature modeling for classification. The high-dimensionality of these signals also increases the complexity of the classifier.\nNote that it has been recognized that there are patterns of anatomical links, or statistical dependencies or causal interactions between distinct units within a nervous system [7]. Some techniques have\nalso been developed to estimate this brain connectivity graph [8, 9]. However, this task is complicated and in many cases the estimated graph may not be accurate. Our contribution is to develop a robust algorithm to determine the reduced dimensionality components that include task-related information, under the assumption that the brain imaging data is graph-smooth but the knowledge of the graph is imperfect. Specifically, our contributions are: (i) based on a model with a low-rank component plus a sparse perturbation, and an initial graph estimation, we propose an algorithm to simultaneously learn the lowrank component and the graph; (ii) we derive the learning steps using ADMM [10]; (iii) we evaluate the algorithm using synthetic and real brain imaging data in a supervised classification task."}, {"heading": "1.1. Related Work", "text": "This work is inspired by [2]. While the focus of [2] is to learn the connectivity graph topology, their algorithm also estimates some noise-free version of the input data as by-product. Gaussian noise model and Frobenius norm optimization are employed in [2]. Therefore, their work is suitable for problem when noise is small. In our work, starting from a model with a low-rank component plus a sparse perturbation, and an initial graph estimation, we adopt the idea of [2] to incrementally refine the underlying connectivity graph, thereby better low-rank estimation of the data can be obtained. As will be shown in our experiment, our method can perform better with highdimensional graph data grossly corrupted by complicated noise, such as brain imaging signals. In addition to [2], learning a graph from smooth signals has attracted a fair amount of interests recently [3, 4]. These works focus on learning the graph, and advanced formulations (e.g., matrix optimization problem) have been derived. Estimation of the brain connectivity graph using a Gaussian noise model has been proposed in [11]. On the other hand, focusing on low-rank estimation, some works have proposed to incorporate spectral graph regularization [12, 13, 14]. Their graphs are fixed in their algorithms, pre-computed from the noisy input data. On the contrary, our algorithm uses the improved low-rank estimation to refine the graph, which in turn is used to improve the quality of the low-rank estimation. Besides, graph signal processing has been applied to a few different brain imaging tasks. In [15], graph Fourier transform is applied to decompose brain signals into low, medium, and high frequency components for analysis of functional brain networks properties. [16] uses the eigenvectors of the graph Laplacian to approximate the intrinsic subspace of high-dimensional brain imaging data. They experimented different brain connectivity estimations to compute the graph. [17] presents a graph based framework for fMRI brain activation mapping. Graph signal processing has also been shown to be useful in image compression [18], temperature data [2], wireless sensor data [19]. A few signal features motivated by graph signal processing have also been proposed [20, 21]. Moreover, several linear / nonlinear dimensionality methods have been proposed that make use of the graph ar X iv :1 60 9. 08 22 1v 2\n[ cs\n.C V\n] 9\nJ an\n2 01\n7\nLaplacian of the sample affinity graphs [22, 23, 24] These methods are geometrically motivated, aim to preserve the local structures of the data, and involve different algorithms compared to our work."}, {"heading": "2. SIMULTANEOUS LOW RANK AND GRAPH ESTIMATION", "text": "We consider X = (x1, . . . ,xn) \u2208 Rp\u00d7n, the high dimensional data matrix that consists of n p-dimensional data points. For our brain imaging data, X are the measurements by the p sensors at the n time instants, i.e., p time series. We assume the data points have low intrinsic dimensionality and lie near some low-dimensional subspace. We assume the following mathematical model for the data:\nX = L0 + M0 (1)\nL0 \u2208 Rp\u00d7n is the low-rank component of the data matrix that is of primary interest in this paper, and M0 \u2208 Rp\u00d7n is a perturbation matrix. We assume that M0 can have arbitrarily large magnitude but its support is sparse.\nPrincipal component analysis (PCA) is the most popular technique for determining the low-rank component with application domains ranging from image, video, signal, web content, to network. The classical PCA finds the projection QT \u2208 Rk\u00d7n of X in a kdimensional (k \u2264 p) linear space characterized by an orthogonal basis V \u2208 Rp\u00d7k, by solving the following optimization:\nminimize V,Q\n\u2016X\u2212VQT \u20162F\nsubject to VTV = I (2)\nThe V and QT matrices are known as principal components and projected data points, respectively. L = VQT \u2208 Rp\u00d7n is the approximation of the low-rank component. The classical PCA suffers from a few disadvantages. First, it is susceptible to grossly corrupted data in X. Second, it does not consider the implicit data manifold information.\nCandes et al. [25] addressed the first issue by designing Robust PCA, which is robust to outliers by directly recovering the low-rank matrix L from the grossly corrupted X:\nminimize L,M \u2016L\u2016\u2217 + \u03b4\u2016M\u20161 subject to X = L + M (3)\n\u2016.\u2016\u2217 denotes the nuclear norm which is used as a convex surrogate of rank.\nIn this work we propose to extend (3) with an additional graph smoothness regularization, while the underlying graph topology that captures the data correlation could be unknown or inexact (thus some refinement is needed):\nminimize L,M,\u03a6f\n\u2016L\u2016\u2217 + \u03b4\u2016M\u20161 + \u03b3tr(LT\u03a6fL) + \u03b2\u2016\u03a6f\u20162F\nsubject to X = L + M, \u03a6f \u2208 L\n(4)\nHere \u03a6f is the graph Laplacian of the feature graph G describing the correlation between individual features: G = (V, E ,W) consists of a finite set of vertices V , with |V| = p, a set of edges E , and a weighted adjacency matrix W = {Wi,j |Wi,j \u2265 0}, with Wi,j quantifying the similarity between the i-th and j-th features of the p-dimensional\nmeasurement vectors. \u03a6f = D \u2212W, with D being the diagonal degree matrix. L is the set of all valid p\u00d7 p graph Laplacian \u03a6:\nL = {\u03a6 : \u03a6ij = \u03a6ji \u2264 0,\u03a6ii = \u2212 \u2211 j 6=i \u03a6ij} (5)\nAs will be further discussed in Section 3, we solve (4) iteratively using alternating minimization with the following justifications:\n\u2022 L,M given \u03a6f : For a given \u03a6f (even a rough estimate), tr(LT\u03a6fL) imposes an additional constraint on the underlying (unknown) low-rank data L. Specifically,\ntr(LT\u03a6fL) = 1\n2 \u2211 i,j Wi,j\u2016l\u0303i \u2212 l\u0303j\u20162, (6)\nwhere l\u0303i \u2208 Rn is the i-th row vector of L. Therefore, tr(LT\u03a6fL) in (4) forces the row i and j of L to have similar values if Wi,j is large. Note that in our brain imaging data, individual rows represent the time series captured by sensors. Thus, tr(LT\u03a6fL) forces the low-rank representations of the time series to be similar for highly correlated sensors. Prior information regarding measurement correlation (such as the physical distance between the capturing sensors) can be incorporated as the initial \u03a6f to bootstrap the estimation of L.\n\u2022 \u03a6f given L,M: On the other hand, for a given estimate of the low-rank data L, tr(LT\u03a6fL) guides the refinement of \u03a6f and hence the underlying connectivity graph G. In particular, a graph G that is consistent with the signal variation in L is favored: large Wi,j if row i and j of L have similar values. In many problems, the given graph for a problem can be noisy (e.g., the graph is estimated from the noisy data itself [13, 14]). The proposed formulation iteratively improves \u03a6f using the refined low-rank data. The improved \u03a6f in turn facilitates the low-rank data estimation."}, {"heading": "3. LEARNING ALGORITHM", "text": "We propose to solve the problem in Eq (4) with alternating minimization scheme where, at each step, we fix one or two variables and update the other variable.\nAt the first step, for a given \u03a6f , we solve the following optimization problem using ADMM[10] with respect to L and M. It means given a graph, it estimates the low rank matrix:\nminimize L,M\n\u2016L\u2016\u2217 + \u03b4\u2016M\u20161 + \u03b3tr(LT\u03a6fL)\nsubject to X = L + M, (7)\nAt the second step, L and M are fixed and we solve the following optimization problem with respect to \u03a6f , which means that based on the low rank matrix we got in the previous step, it updates the graph.\nminimize \u03a6f\n\u03b3tr(LT\u03a6fL) + \u03b2\u2016\u03a6f\u20162F\nsubject to \u03a6f \u2208 L (8)\nFor equation (8), it can be written as:\nminimize \u03a6f\n\u03b3tr(LT\u03a6fL) + \u03b2\u2016z\u20162F\nsubject to \u03a6f \u2212 z = 0, \u03a6f \u2208 L\n(9)\nWe can form the augmented Lagrangian of (9) as:\nL\u03c1(\u03a6f , z,u) =\u03b3tr(L T\u03a6fL) + \u03b2\u2016\u03a6f\u20162F\n+ \u03c1\n2 \u2016z\u2212 \u03a6f\u20162F + \u3008u, z\u2212 \u03a6f \u3009\n(10)\nThen we can get the following formula to update for \u03a6f , z and u:\n\u03a6k+1f := \u03c1zk \u2212 \u03b3LTL + u\n\u03b2 2 + \u03c1 zk+1 := \u220f L (\u03a6k+1f \u2212 1 \u03c1 uk)\nuk+1 := uk + 1\nk (zk+1 \u2212 \u03a6k+1f )\n(11)\nwhere \u03c1 > 0 is the Lagrangian parameter and \u220f L is the Eu-\nclidean projection onto set L."}, {"heading": "4. EXPERIMENT", "text": ""}, {"heading": "4.1. Synthetic Experiment", "text": "In this synthetic experiment, we generate low-rank, graph-smooth and grossly-corrupted data. We generate synthetic data with the following model:\nX = L0 + M0 (12)\nwhere L0 \u2208 Rp\u00d7n is a low rank matrix with rank r and M0 is the sparse matrix.\nWe generate L0 as a product L0 = PYT where P \u2208 Rp\u00d7r and Y \u2208 Rn\u00d7r . L0 is also graph-smooth and generated as follows. The (ground-truth) graph consists of p nodes, with each pair of nodes having a probability of q to be connected together. The edge weights between different nodes are drew uniformly from 0 to 1 and presented in a p\u00d7p symmetric adjacency matrix W. We calculate the Laplacian matrix \u03a6f from W and compute the eigenvectors and eigenvalues of \u03a6f . The eigenvectors, corresponding to top r eigenvalues, are selected as the columns of P. For matrix Y, the entries are independently sampled from a N(0, 1/p) distribution. Therefore, L0 is low-rank and graph-smooth. We introduce k = \u2016M0\u20160/p2 errors in the matrix M0 from an i.i.d Bernoulli distribution. Each corrupted entry takes a value \u00b11 with a probability k/2.\nWe compare the proposed method, GL-SigRep[26], RPCAG[13], RPCA[25], and PCA on the data to estimate the low rank matrix and the graph matrix. The estimation accuracies are evaluated by the reconstruction errors: \u2016L\u0302\u2212L0\u2016F /\u2016L0\u2016F and \u2016\u03a6\u0302f\u2212\u03a6f\u2016F /\u2016\u03a6f\u2016F . All the methods are initialized with the same feature similarity graph (consider each row of X as a node) computed using the procedure in [14] with a K-nearest neighbor strategy (K = 10).\nTable 1 shows the results on synthetic data generated by the Eq (12) with p = 30, n = 50, r = 3, k = 40%, q = 0.2. With cross-validation, we set \u03b4 = 2.5\u221a\n50 , \u03b3 = 1.5, \u03b2 = 1.5. The low rank\napproximation of proposed method achieves the smallest error. For the estimate graph matrix, the proposed method also achieves the smallest estimation error. Synthetic experiment results show that the proposed method can achieve good performance on extracting low rank approximation and the underlying graph from non-Gaussian noisy data."}, {"heading": "4.2. Brain Imaging Data Experiment", "text": "We also apply our proposed method on a high-dimensional brain imaging dataset to extract the brain connectivity graph and the low\nrank approximation from the high dimensionality. This is practically useful for brain imaging studies: due to the high dimensionality of data, low signal-to-noise ratio, and small number of available samples, it is challenging to estimate the low rank approximation in these studies.\nThe brain imaging dataset used here is a set of magnetoencephalography (MEG) signal recordings of brain activities, in response to two categories of visual stimuli: 320 face images and 192 non-face images. These images were randomly selected and displayed passively with no task, and 16 subjects were asked to simply fixate at the center of the screen. All images were normalized for size and brightness among other factors, and were each displayed once for 300 ms with random inter-stimulus delay intervals of 1000 ms to 1500 ms. We used a recumbent Elekta MEG scanner with 306 sensors to record the brain activity for 1100 milliseconds (100 milliseconds before and 1000 milliseconds after the presentation) for each stimuli. The classification task in this experiment is to distinguish signals evoked by face images from signals evoked by non-face images."}, {"heading": "4.2.1. Initial graph matrix", "text": "In the proposed method, a suitable starting point is important for solving the optimization problem. We therefore initialize \u03a6f with the brain connectivity matrix generated with the resting state measurements. The resting state in our experiment is 100ms of signal recording before the stimuli presentation. Note that our method and all other methods are initialized with the same connectivity matrix.\nThree different types of brain connectivity graphs are commonly used in the literature: structural connectivity, functional connectivity and effective connectivity. Structural connectivity shows the anatomical structure in the brain; functional connectivity quantifies functional dependencies between different brain regions; and effective connectivity shows directed or causal relationship between different brain regions [7].\nIn this paper we use a coherence connectivity, a functional connectivity, quantifying oscillatory interdependency between different brain regions [16]. It is the frequency domain analog of the crosscorrelation coefficient. Given two series of signals at, bt and a frequency f , the first step is to spectrally decompose the signal at target f to obtain the instantaneous phase at each time point [27]. After band-pass filtering each signal between f \u00b1 5Hz, the convolution of f(t) with a Morlet wavelet centered at frequency f provides the instantaneous phase at time t. Thus, the two signals can be represented as: a = Ea(t)ej\u03c8a(t) and b = Eb(t)ej\u03c8b(t), where Ea(t) and Eb(t) are amplitudes. \u03c8a(t) and \u03c8b(t) are the phase for A and B at time t. Then the coherence connectivity edge is calculated as below:\nwA,B = \u2223\u2223\u2223\u2223\u2223\u2223 1 T \u2211T t=1 Ea(t)Eb(t)e j[\u03c8a(t)\u2212\u03c8b(t)]\u221a 1 T \u2211T t=1 Ea(t) 2 \u00b7 \u221a 1 T \u2211T t=1 Eb(t) 2 \u2223\u2223\u2223\u2223\u2223\u2223 (13)\nAfter we get the adjacency matrix W, we can calculate the Laplacian matrix \u03a6f = D\u2212W as the initialization."}, {"heading": "4.2.2. Brain Imaging Data Experiment Results", "text": "To evaluate the performance of proposed method, we use a supervised classifier, SVM, to classify the low-rank outputs into face and nonface classes. We compare these methods based on their classification accuracies as well as compatibility of their connectivity graph matrix to the related neuroscience findings on suggested cortical regions involving face processing.\nMEG and EEG components corresponding to the face/non-face distinction have been reported at latencies of approximately 100 ms, and more reliably at 170 ms (also known as N170 marker, reported at about 145ms in MEG studies), after visual stimulus onset (e.g. see [28, 29, 30, 31]). In this experiment, we therefore choose the data from two time slots, namely 96ms to 105ms and 141ms to 150ms after the stimuli presentation, to be able to compare our automate assessment with the related neuroscience literature.\nWe applied commonly employed mean subtraction on the timelocked data. In addition to this step, we refrained from providing any prior information to our method. This is due to the goal of this paper to showcase the capabilities of our proposed method, and therefore chose a pure data driven approach to the problem of estimation of underlying connectivity graph. For example, we did not enforce constraints in the optimization step on the sensor sensitivity to field spread (i.e. nearby electrodes record similar brain activities). Similarly, we did not differentiate between the magnetometers and gradiometers. With no prior information fed to the method along-side the data, the estimated underlying graph can be easily compared with the results of other methods. This is while more specific experiments can be conducted in the future to reveal the effects of prior information on the results.\nGiven the input data and the initial graph, our proposed method outputs some low rank estimation L. We decompose the low rank matrix L using SVD, select the components according to the rank of L, project the data onto the components to obtain low-dimensionality representations, and classify the reduced dimension data. We compare the proposed method with GL-SigRep, RPCAG, RPCA and PCA. Using cross-validation with \u03b4 \u2208 [1:0.5:8]\u221a\n50 , \u03b3 \u2208 10[\u22122:1:1], \u03b2 \u03b3 \u2208 [1 :\n5 : 10] (for step 2, only the ratio of \u03b2 and \u03b3 matters the results), we set \u03b4 = 1\u221a\n50 , \u03b3 = 0.1, \u03b2 = 0.5 for the first time slot and \u03b4 =\n1\u221a 50 , \u03b3 = 0.01, \u03b2 = 0.05 for the second time slot.Table 2 shows the classification results for different methods on the two time slots. The proposed method gives the best results for both time slots.\nFor another comparison, in Figure 1, we visualize the estimated graph matrix \u03a6f by our method as well as the GL-SigRep method, by projecting the graph connectivity weights on the MEG sensor locations. The first row of Figure 1 shows the initialization graph used for both method, the coherence connectivity graph obtained from the resting state data. The second row visualizes the output of the two methods for data at 100ms and 145ms time point.\nComparing the graph visualization results from GL-SigRep (Figure 1, b and d), one can see that using the data from 96-105ms, GL-SigRep indicates connectivities at the left temporal and middle and inferior frontal gyri. The estimated graph by GL-SigRep does not significantly change using the data from 141-150ms either. None of these regions high-lighted by GL-SigRep has apparent link with early visual processing described in neuroscientific literature (note that GL-SigRep assumes Gaussian noise and would not be appropriate for MEG data). At 100ms after projection of a visual stimuli\nlike a face image, neuroscientific literature seem to report the visual information is still being processed at early visual cortex at occipital and occipitotemporal regions (e.g. see [29, 31]). Unlike GL-SigRep results, at 96-105ms, the graph connectivity estimation by our method (Figure 1, a) tends to span more on the occipital and left occipitotemporal regions, and therefore seem to have reached a more successful estimation of the true underlying connectivity at this time-point.\nThe connectivity graph estimated by our method during 141ms to 150ms (Figure 1, c) converges on connections on the right occipitotemporal region. This graph connectivity is comparable to the neuroscientific findings on face perception, and specifically the N170 marker. In several studies such as [28, 30], the fusiform gyrus (at the occipitotemporal region) are suggested for processing face perception during about 145ms after presentation of a face image stimuli, also known as N170 marker (named after its first discovery at 170ms in EEG studies). In this work, our technique reveals almost the same regions as important graph connections for face perception. The compatibility of our estimated graph connectivity with neuroscientific literature further supports our proposed method over others."}, {"heading": "5. CONCLUSION", "text": "We propose an algorithm in learning the low rank component and graph simultaneously. It is suitable for cases where the perturbations on the low rank components are grossly but sparse. We showed that the proposed method on both synthetic data and brain imaging data is competitive. Our method achieves good performance on both low rank approximation and graph estimation. In addition, when applying to the brain imaging data, our method could recover a connectivity graph that is more compatible to the neuroscientific literature, indicating its better estimation of the true underlying graph. Future work applies the proposed algorithm for other high-dimensional data [32]."}, {"heading": "6. REFERENCES", "text": "[1] David Shuman, Sunil K Narang, Pascal Frossard, Antonio Ortega, Pierre Vandergheynst, et al., \u201cThe emerging field of signal processing on graphs: Extending high-dimensional data analysis to networks and other irregular domains,\u201d Signal Processing Magazine, IEEE, vol. 30, no. 3, pp. 83\u201398, 2013.\n[2] Xiaowen Dong, Dorina Thanou, Pascal Frossard, and Pierre Vandergheynst, \u201cLaplacian matrix learning for smooth graph signal representation,\u201d in Proc. ICASSP, 2015.\n[3] Eduardo Pavez and Antonio Ortega, \u201cGeneralized laplacian precision matrix estimation for graph signal processing,\u201d in Proc. ICASSP, 2016.\n[4] Vassilis Kalofolias, \u201cHow to learn a graph from smooth signals,\u201d in AISTATS 2016, 2016.\n[5] Kleovoulos Tsourides, Shahriar Shariat, Hossein Nejati, Tapan K Gandhi, Annie Cardinaux, Christopher T Simons, Ngai-Man Cheung, Vladimir Pavlovic, and Pawan Sinha, \u201cNeural correlates of the food/nonfood visual distinction,\u201d Biological Psychology, 2016.\n[6] H. Nejati, K. Tsourides, V. Pomponiu, E.C. Ehrenberg, Ngai-Man Cheung, and P. Sinha, \u201cTowards perception awareness: Perceptual event detection for brain computer interfaces,\u201d in EMBC2015, Aug 2015, pp. 1480\u20131483.\n[7] Ed Bullmore and Olaf Sporns, \u201cComplex brain networks: graph theoretical analysis of structural and functional systems,\u201d Nature Reviews Neuroscience, vol. 10, no. 3, pp. 186\u2013198, 2009.\n[8] James S Hyde and Andrzej Jesmanowicz, \u201cCross-correlation: an fmri signal-processing strategy,\u201d NeuroImage, vol. 62, no. 2, pp. 848\u2013851, 2012.\n[9] Andrea Brovelli, Mingzhou Ding, Anders Ledberg, Yonghong Chen, Richard Nakamura, and Steven L Bressler, \u201cBeta oscillations in a largescale sensorimotor cortical network: directional influences revealed by granger causality,\u201d Proceedings of the National Academy of Sciences of the United States of America, vol. 101, no. 26, pp. 9849\u20139854, 2004.\n[10] Stephen Boyd, Neal Parikh, Eric Chu, Borja Peleato, and Jonathan Eckstein, \u201cDistributed optimization and statistical learning via the alternating direction method of multipliers,\u201d Foundations and Trends in Machine Learning, vol. 3, no. 1, pp. 1\u2013122, 2011.\n[11] Chenhui Hu, Lin Cheng, Jorge Sepulcre, Georges El Fakhri, Yue M. Lu, and Quanzheng Li, \u201cA graph theoretical regression model for brain connectivity learning of alzheimer\u2019s disease,\u201d in ISBI2013, San Francisco, CA, 7-11 Apr. 2013.\n[12] Bo Jiang, Chris Ding, Bin Luo, and Jin Tang, \u201cGraph-Laplacian PCA: Closed-form solution and robustness,\u201d in CVPR, 2013, pp. 3490\u201334968.\n[13] Nauman Shahid, Vassilis Kalofolias, Xavier Bresson, Michael Bronstein, and Pierre Vandergheynst, \u201cRobust principal component analysis on graphs,\u201d in Proceedings of International Conference on Computer Vision, Santiago, Chile, 2015, pp. 2812\u20132820.\n[14] Nauman Shahid, Nathanael Perraudin, Vassilis Kalofolias, Gilles Puy, and Pierre Vandergheynst, \u201cFast robust PCA on graphs,\u201d arXiv:1507.08173v2 [cs.CV] 25 Jan 2016, pp. 1\u201317, 2016.\n[15] Weiyu Huang, Leah Goldsberry, Nicholas F Wymbs, Scott T Grafton, Danielle S Bassett, and Alejandro Ribeiro, \u201cGraph frequency analysis of brain signals,\u201d arXiv preprint arXiv:1512.00037v2, 2016.\n[16] Liu Rui, Hossein Nejati, and Ngai-Man Cheung, \u201cDimensionality reduction of brain imaging data using graph signal processing,\u201d in ICIP. IEEE, 2016, pp. 1329\u20131333.\n[17] Hamid Behjat, Nora Leonardi, Leif S\u00f6rnmo, and Dimitri Van De Ville, \u201cAnatomically-adapted graph wavelets for improved group-level fmri activation mapping,\u201d NeuroImage, vol. 123, pp. 185\u2013199, 2015.\n[18] Wei Hu, Gene Cheung, Antonio Ortega, and Oscar C Au, \u201cMultiresolution graph fourier transform for compression of piecewise smooth images,\u201d Image Processing, IEEE Transactions on, vol. 24, no. 1, pp. 419\u2013433, 2015.\n[19] Hilmi E Egilmez and Antonio Ortega, \u201cSpectral anomaly detection using graph-based filtering for wireless sensor networks,\u201d in ICASSP. IEEE, 2014, pp. 1085\u20131089.\n[20] Xiaowen Dong, Antonio Ortega, Pascal Frossard, and Pierre Vandergheynst, \u201cInference of mobility patterns via spectral graph wavelets,\u201d in ICASSP. IEEE, 2013, pp. 3118\u20133122.\n[21] Jieqi Kang, Shan Lu, Weibo Gong, and Patrick A Kelly, \u201cA complex network based feature extraction for image retrieval,\u201d in ICIP. IEEE, 2014, pp. 2051\u20132055.\n[22] M. Belkin and P. Niyogi, \u201cLaplacian eigenmaps for dimensionality reduction and data representation,\u201d Neural Computation, vol. 15, no. 6, pp. 1373\u20131396, 2003.\n[23] Xiaofei He and Partha Niyogi, \u201cLocality preserving projection,\u201d in Proceedings of NIPS, 2003.\n[24] Shuicheng Yan, Dong Xu, Benyu Zhang, and Stephen Lin, \u201cGraph embedding and extensions: a general framework for dimensionality reduction,\u201d IEEE Transactions on Pattern Analysis and Machine Intelligence, 2007.\n[25] Emmanuel J. Cand\u00e8s, Xiaodong Li, Yi Ma, and John Wright, \u201cRobust principal component analysis?,\u201d Journal of the ACM, vol. 58, no. 3, pp. 11:1\u201311:37, May 2011.\n[26] Xiaowen Dong, Dorina Thanou, Pascal Frossard, and Pierre Vandergheynst, \u201cLearning laplacian matrix in smooth graph signal representations,\u201d arXiv preprint arXiv:1406.7842v3, 2014.\n[27] Benjamin T Schmidt, Avniel S Ghuman, and Theodore J Huppert, \u201cWhole brain functional connectivity using phase locking measures of resting state magnetoencephalography,\u201d Front. Neurosci, vol. 8, no. 141, pp. 10\u20133389, 2014.\n[28] David I. Perrett, Amanda J. Mistlin, and Andrew J. Chitty, \u201cVisual neurones responsive to faces,\u201d Trends in Neurosciences, vol. 10, no. 9, pp. 358 \u2013 364, 1987.\n[29] S. Thorpe, D. Fize, and C. Marlot, \u201cSpeed of processing in the human visual system,\u201d Nature, vol. 381, no. 6582, pp. 520\u20132, 1996.\n[30] Jia Liu, Alison Harris, and Nancy Kanwisher, \u201cStages of processing in face perception: an MEG study.,\u201d Nature neuroscience, vol. 5, no. 9, pp. 910\u2013916, Sept. 2002.\n[31] R. Desimone, \u201cFace-selective cells in the temporal cortex of monkeys,\u201d Journal of Cognitive Neuroscience, vol. 3, no. 1, pp. 1\u20138, Jan. 2006.\n[32] Lu Fang, Ngai-Man Cheung, D Tian, A Vetro, H Sun, and O Au, \u201cAn analytical model for synthesis distortion estimation in 3d video,\u201d IEEE Transactions on Image Processing, vol. 23, no. 1, pp. 185\u2013199, 2014."}], "references": [{"title": "The emerging field of signal processing on graphs: Extending high-dimensional data analysis to networks and other irregular domains", "author": ["David Shuman", "Sunil K Narang", "Pascal Frossard", "Antonio Ortega", "Pierre Vandergheynst"], "venue": "Signal Processing Magazine, IEEE, vol. 30, no. 3, pp. 83\u201398, 2013.", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2013}, {"title": "Laplacian matrix learning for smooth graph signal representation", "author": ["Xiaowen Dong", "Dorina Thanou", "Pascal Frossard", "Pierre Vandergheynst"], "venue": "Proc. ICASSP, 2015.", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2015}, {"title": "Generalized laplacian precision matrix estimation for graph signal processing", "author": ["Eduardo Pavez", "Antonio Ortega"], "venue": "Proc. ICASSP, 2016.", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2016}, {"title": "How to learn a graph from smooth signals", "author": ["Vassilis Kalofolias"], "venue": "AISTATS 2016, 2016.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2016}, {"title": "Neural correlates of the food/nonfood visual distinction", "author": ["Kleovoulos Tsourides", "Shahriar Shariat", "Hossein Nejati", "Tapan K Gandhi", "Annie Cardinaux", "Christopher T Simons", "Ngai-Man Cheung", "Vladimir Pavlovic", "Pawan Sinha"], "venue": "Biological Psychology, 2016.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2016}, {"title": "Towards perception awareness: Perceptual event detection for brain computer interfaces", "author": ["H. Nejati", "K. Tsourides", "V. Pomponiu", "E.C. Ehrenberg", "Ngai-Man Cheung", "P. Sinha"], "venue": "EMBC2015, Aug 2015, pp. 1480\u20131483.", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2015}, {"title": "Complex brain networks: graph theoretical analysis of structural and functional systems", "author": ["Ed Bullmore", "Olaf Sporns"], "venue": "Nature Reviews Neuroscience, vol. 10, no. 3, pp. 186\u2013198, 2009.", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2009}, {"title": "Cross-correlation: an fmri signal-processing strategy", "author": ["James S Hyde", "Andrzej Jesmanowicz"], "venue": "NeuroImage, vol. 62, no. 2, pp. 848\u2013851, 2012.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2012}, {"title": "Beta oscillations in a largescale sensorimotor cortical network: directional influences revealed by granger causality", "author": ["Andrea Brovelli", "Mingzhou Ding", "Anders Ledberg", "Yonghong Chen", "Richard Nakamura", "Steven L Bressler"], "venue": "Proceedings of the National Academy of Sciences of the United States of America, vol. 101, no. 26, pp. 9849\u20139854, 2004.", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2004}, {"title": "Distributed optimization and statistical learning via the alternating direction method of multipliers", "author": ["Stephen Boyd", "Neal Parikh", "Eric Chu", "Borja Peleato", "Jonathan Eckstein"], "venue": "Foundations and Trends in Machine Learning, vol. 3, no. 1, pp. 1\u2013122, 2011.", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2011}, {"title": "A graph theoretical regression model for brain connectivity learning of alzheimer\u2019s disease", "author": ["Chenhui Hu", "Lin Cheng", "Jorge Sepulcre", "Georges El Fakhri", "Yue M. Lu", "Quanzheng Li"], "venue": "ISBI2013, San Francisco, CA, 7-11 Apr. 2013.", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2013}, {"title": "Graph-Laplacian PCA: Closed-form solution and robustness", "author": ["Bo Jiang", "Chris Ding", "Bin Luo", "Jin Tang"], "venue": "CVPR, 2013, pp. 3490\u201334968.", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2013}, {"title": "Robust principal component analysis on graphs", "author": ["Nauman Shahid", "Vassilis Kalofolias", "Xavier Bresson", "Michael Bronstein", "Pierre Vandergheynst"], "venue": "Proceedings of International Conference on Computer Vision, Santiago, Chile, 2015, pp. 2812\u20132820.", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2015}, {"title": "Fast robust PCA on graphs", "author": ["Nauman Shahid", "Nathanael Perraudin", "Vassilis Kalofolias", "Gilles Puy", "Pierre Vandergheynst"], "venue": "arXiv:1507.08173v2 [cs.CV] 25 Jan 2016, pp. 1\u201317, 2016.", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2016}, {"title": "Graph frequency analysis of brain signals", "author": ["Weiyu Huang", "Leah Goldsberry", "Nicholas F Wymbs", "Scott T Grafton", "Danielle S Bassett", "Alejandro Ribeiro"], "venue": "arXiv preprint arXiv:1512.00037v2, 2016.", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2016}, {"title": "Dimensionality reduction of brain imaging data using graph signal processing", "author": ["Liu Rui", "Hossein Nejati", "Ngai-Man Cheung"], "venue": "ICIP. IEEE, 2016, pp. 1329\u20131333.", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2016}, {"title": "Anatomically-adapted graph wavelets for improved group-level fmri activation mapping", "author": ["Hamid Behjat", "Nora Leonardi", "Leif S\u00f6rnmo", "Dimitri Van De Ville"], "venue": "NeuroImage, vol. 123, pp. 185\u2013199, 2015.", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2015}, {"title": "Multiresolution graph fourier transform for compression of piecewise smooth images", "author": ["Wei Hu", "Gene Cheung", "Antonio Ortega", "Oscar C Au"], "venue": "Image Processing, IEEE Transactions on, vol. 24, no. 1, pp. 419\u2013433, 2015.", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2015}, {"title": "Spectral anomaly detection using graph-based filtering for wireless sensor networks", "author": ["Hilmi E Egilmez", "Antonio Ortega"], "venue": "ICASSP. IEEE, 2014, pp. 1085\u20131089.", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2014}, {"title": "Inference of mobility patterns via spectral graph wavelets", "author": ["Xiaowen Dong", "Antonio Ortega", "Pascal Frossard", "Pierre Vandergheynst"], "venue": "ICASSP. IEEE, 2013, pp. 3118\u20133122.", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2013}, {"title": "A complex network based feature extraction for image retrieval", "author": ["Jieqi Kang", "Shan Lu", "Weibo Gong", "Patrick A Kelly"], "venue": "ICIP. IEEE, 2014, pp. 2051\u20132055.", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2014}, {"title": "Laplacian eigenmaps for dimensionality reduction and data representation", "author": ["M. Belkin", "P. Niyogi"], "venue": "Neural Computation, vol. 15, no. 6, pp. 1373\u20131396, 2003.", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2003}, {"title": "Locality preserving projection", "author": ["Xiaofei He", "Partha Niyogi"], "venue": "Proceedings of NIPS, 2003.", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2003}, {"title": "Graph embedding and extensions: a general framework for dimensionality reduction", "author": ["Shuicheng Yan", "Dong Xu", "Benyu Zhang", "Stephen Lin"], "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence, 2007.", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2007}, {"title": "Robust principal component analysis", "author": ["Emmanuel J. Cand\u00e8s", "Xiaodong Li", "Yi Ma", "John Wright"], "venue": "Journal of the ACM, vol. 58, no. 3, pp. 11:1\u201311:37, May 2011.", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2011}, {"title": "Learning laplacian matrix in smooth graph signal representations", "author": ["Xiaowen Dong", "Dorina Thanou", "Pascal Frossard", "Pierre Vandergheynst"], "venue": "arXiv preprint arXiv:1406.7842v3, 2014.", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2014}, {"title": "Whole brain functional connectivity using phase locking measures of resting state magnetoencephalography", "author": ["Benjamin T Schmidt", "Avniel S Ghuman", "Theodore J Huppert"], "venue": "Front. Neurosci, vol. 8, no. 141, pp. 10\u20133389, 2014.", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2014}, {"title": "Visual neurones responsive to faces", "author": ["David I. Perrett", "Amanda J. Mistlin", "Andrew J. Chitty"], "venue": "Trends in Neurosciences, vol. 10, no. 9, pp. 358 \u2013 364, 1987.", "citeRegEx": "28", "shortCiteRegEx": null, "year": 1987}, {"title": "Speed of processing in the human visual system", "author": ["S. Thorpe", "D. Fize", "C. Marlot"], "venue": "Nature, vol. 381, no. 6582, pp. 520\u20132, 1996.", "citeRegEx": "29", "shortCiteRegEx": null, "year": 1996}, {"title": "Stages of processing in face perception: an MEG study", "author": ["Jia Liu", "Alison Harris", "Nancy Kanwisher"], "venue": "Nature neuroscience, vol. 5, no. 9, pp. 910\u2013916, Sept. 2002.", "citeRegEx": "30", "shortCiteRegEx": null, "year": 2002}, {"title": "Face-selective cells in the temporal cortex of monkeys", "author": ["R. Desimone"], "venue": "Journal of Cognitive Neuroscience, vol. 3, no. 1, pp. 1\u20138, Jan. 2006.", "citeRegEx": "31", "shortCiteRegEx": null, "year": 2006}, {"title": "An analytical model for synthesis distortion estimation in 3d video", "author": ["Lu Fang", "Ngai-Man Cheung", "D Tian", "A Vetro", "H Sun", "O Au"], "venue": "IEEE Transactions on Image Processing, vol. 23, no. 1, pp. 185\u2013199, 2014.", "citeRegEx": "32", "shortCiteRegEx": null, "year": 2014}], "referenceMentions": [{"referenceID": 0, "context": "We further focus on data that resides on a certain graph and the data changes smoothly between the connected vertices [1].", "startOffset": 118, "endOffset": 121}, {"referenceID": 1, "context": "In many problems, the underlying graph is unknown or inexact [2, 3, 4].", "startOffset": 61, "endOffset": 70}, {"referenceID": 2, "context": "In many problems, the underlying graph is unknown or inexact [2, 3, 4].", "startOffset": 61, "endOffset": 70}, {"referenceID": 3, "context": "In many problems, the underlying graph is unknown or inexact [2, 3, 4].", "startOffset": 61, "endOffset": 70}, {"referenceID": 4, "context": "In many cases, the task is to find the spatiotemporal neural signature of a task, by performing classification on cortical activations evoked by different stimuli [5, 6].", "startOffset": 163, "endOffset": 169}, {"referenceID": 5, "context": "In many cases, the task is to find the spatiotemporal neural signature of a task, by performing classification on cortical activations evoked by different stimuli [5, 6].", "startOffset": 163, "endOffset": 169}, {"referenceID": 6, "context": "Note that it has been recognized that there are patterns of anatomical links, or statistical dependencies or causal interactions between distinct units within a nervous system [7].", "startOffset": 176, "endOffset": 179}, {"referenceID": 7, "context": "Some techniques have also been developed to estimate this brain connectivity graph [8, 9].", "startOffset": 83, "endOffset": 89}, {"referenceID": 8, "context": "Some techniques have also been developed to estimate this brain connectivity graph [8, 9].", "startOffset": 83, "endOffset": 89}, {"referenceID": 9, "context": "Specifically, our contributions are: (i) based on a model with a low-rank component plus a sparse perturbation, and an initial graph estimation, we propose an algorithm to simultaneously learn the lowrank component and the graph; (ii) we derive the learning steps using ADMM [10]; (iii) we evaluate the algorithm using synthetic and real brain imaging data in a supervised classification task.", "startOffset": 275, "endOffset": 279}, {"referenceID": 1, "context": "This work is inspired by [2].", "startOffset": 25, "endOffset": 28}, {"referenceID": 1, "context": "While the focus of [2] is to learn the connectivity graph topology, their algorithm also estimates some noise-free version of the input data as by-product.", "startOffset": 19, "endOffset": 22}, {"referenceID": 1, "context": "Gaussian noise model and Frobenius norm optimization are employed in [2].", "startOffset": 69, "endOffset": 72}, {"referenceID": 1, "context": "In our work, starting from a model with a low-rank component plus a sparse perturbation, and an initial graph estimation, we adopt the idea of [2] to incrementally refine the underlying connectivity graph, thereby better low-rank estimation of the data can be obtained.", "startOffset": 143, "endOffset": 146}, {"referenceID": 1, "context": "In addition to [2], learning a graph from smooth signals has attracted a fair amount of interests recently [3, 4].", "startOffset": 15, "endOffset": 18}, {"referenceID": 2, "context": "In addition to [2], learning a graph from smooth signals has attracted a fair amount of interests recently [3, 4].", "startOffset": 107, "endOffset": 113}, {"referenceID": 3, "context": "In addition to [2], learning a graph from smooth signals has attracted a fair amount of interests recently [3, 4].", "startOffset": 107, "endOffset": 113}, {"referenceID": 10, "context": "Estimation of the brain connectivity graph using a Gaussian noise model has been proposed in [11].", "startOffset": 93, "endOffset": 97}, {"referenceID": 11, "context": "On the other hand, focusing on low-rank estimation, some works have proposed to incorporate spectral graph regularization [12, 13, 14].", "startOffset": 122, "endOffset": 134}, {"referenceID": 12, "context": "On the other hand, focusing on low-rank estimation, some works have proposed to incorporate spectral graph regularization [12, 13, 14].", "startOffset": 122, "endOffset": 134}, {"referenceID": 13, "context": "On the other hand, focusing on low-rank estimation, some works have proposed to incorporate spectral graph regularization [12, 13, 14].", "startOffset": 122, "endOffset": 134}, {"referenceID": 14, "context": "In [15], graph Fourier transform is applied to decompose brain signals into low, medium, and high frequency components for analysis of functional brain networks properties.", "startOffset": 3, "endOffset": 7}, {"referenceID": 15, "context": "[16] uses the eigenvectors of the graph Laplacian to approximate the intrinsic subspace of high-dimensional brain imaging data.", "startOffset": 0, "endOffset": 4}, {"referenceID": 16, "context": "[17] presents a graph based framework for fMRI brain activation mapping.", "startOffset": 0, "endOffset": 4}, {"referenceID": 17, "context": "Graph signal processing has also been shown to be useful in image compression [18], temperature data [2], wireless sensor data [19].", "startOffset": 78, "endOffset": 82}, {"referenceID": 1, "context": "Graph signal processing has also been shown to be useful in image compression [18], temperature data [2], wireless sensor data [19].", "startOffset": 101, "endOffset": 104}, {"referenceID": 18, "context": "Graph signal processing has also been shown to be useful in image compression [18], temperature data [2], wireless sensor data [19].", "startOffset": 127, "endOffset": 131}, {"referenceID": 19, "context": "A few signal features motivated by graph signal processing have also been proposed [20, 21].", "startOffset": 83, "endOffset": 91}, {"referenceID": 20, "context": "A few signal features motivated by graph signal processing have also been proposed [20, 21].", "startOffset": 83, "endOffset": 91}, {"referenceID": 21, "context": "Laplacian of the sample affinity graphs [22, 23, 24] These methods are geometrically motivated, aim to preserve the local structures of the data, and involve different algorithms compared to our work.", "startOffset": 40, "endOffset": 52}, {"referenceID": 22, "context": "Laplacian of the sample affinity graphs [22, 23, 24] These methods are geometrically motivated, aim to preserve the local structures of the data, and involve different algorithms compared to our work.", "startOffset": 40, "endOffset": 52}, {"referenceID": 23, "context": "Laplacian of the sample affinity graphs [22, 23, 24] These methods are geometrically motivated, aim to preserve the local structures of the data, and involve different algorithms compared to our work.", "startOffset": 40, "endOffset": 52}, {"referenceID": 24, "context": "[25] addressed the first issue by designing Robust PCA, which is robust to outliers by directly recovering the low-rank matrix L from the grossly corrupted X:", "startOffset": 0, "endOffset": 4}, {"referenceID": 12, "context": ", the graph is estimated from the noisy data itself [13, 14]).", "startOffset": 52, "endOffset": 60}, {"referenceID": 13, "context": ", the graph is estimated from the noisy data itself [13, 14]).", "startOffset": 52, "endOffset": 60}, {"referenceID": 9, "context": "At the first step, for a given \u03a6f , we solve the following optimization problem using ADMM[10] with respect to L and M.", "startOffset": 90, "endOffset": 94}, {"referenceID": 25, "context": "We compare the proposed method, GL-SigRep[26], RPCAG[13], RPCA[25], and PCA on the data to estimate the low rank matrix and the graph matrix.", "startOffset": 41, "endOffset": 45}, {"referenceID": 12, "context": "We compare the proposed method, GL-SigRep[26], RPCAG[13], RPCA[25], and PCA on the data to estimate the low rank matrix and the graph matrix.", "startOffset": 52, "endOffset": 56}, {"referenceID": 24, "context": "We compare the proposed method, GL-SigRep[26], RPCAG[13], RPCA[25], and PCA on the data to estimate the low rank matrix and the graph matrix.", "startOffset": 62, "endOffset": 66}, {"referenceID": 13, "context": "All the methods are initialized with the same feature similarity graph (consider each row of X as a node) computed using the procedure in [14] with a K-nearest neighbor strategy (K = 10).", "startOffset": 138, "endOffset": 142}, {"referenceID": 6, "context": "Structural connectivity shows the anatomical structure in the brain; functional connectivity quantifies functional dependencies between different brain regions; and effective connectivity shows directed or causal relationship between different brain regions [7].", "startOffset": 258, "endOffset": 261}, {"referenceID": 15, "context": "In this paper we use a coherence connectivity, a functional connectivity, quantifying oscillatory interdependency between different brain regions [16].", "startOffset": 146, "endOffset": 150}, {"referenceID": 26, "context": "Given two series of signals at, bt and a frequency f , the first step is to spectrally decompose the signal at target f to obtain the instantaneous phase at each time point [27].", "startOffset": 173, "endOffset": 177}, {"referenceID": 27, "context": "see [28, 29, 30, 31]).", "startOffset": 4, "endOffset": 20}, {"referenceID": 28, "context": "see [28, 29, 30, 31]).", "startOffset": 4, "endOffset": 20}, {"referenceID": 29, "context": "see [28, 29, 30, 31]).", "startOffset": 4, "endOffset": 20}, {"referenceID": 30, "context": "see [28, 29, 30, 31]).", "startOffset": 4, "endOffset": 20}, {"referenceID": 28, "context": "see [29, 31]).", "startOffset": 4, "endOffset": 12}, {"referenceID": 30, "context": "see [29, 31]).", "startOffset": 4, "endOffset": 12}, {"referenceID": 27, "context": "In several studies such as [28, 30], the fusiform gyrus (at the occipitotemporal region) are suggested for processing face perception during about 145ms after presentation of a face image stimuli, also known as N170 marker (named after its first discovery at 170ms in EEG studies).", "startOffset": 27, "endOffset": 35}, {"referenceID": 29, "context": "In several studies such as [28, 30], the fusiform gyrus (at the occipitotemporal region) are suggested for processing face perception during about 145ms after presentation of a face image stimuli, also known as N170 marker (named after its first discovery at 170ms in EEG studies).", "startOffset": 27, "endOffset": 35}, {"referenceID": 31, "context": "Future work applies the proposed algorithm for other high-dimensional data [32].", "startOffset": 75, "endOffset": 79}], "year": 2017, "abstractText": "We propose an algorithm to uncover the intrinsic low-rank component of a high-dimensional, graph-smooth and grossly-corrupted dataset, under the situations that the underlying graph is unknown. Based on a model with a low-rank component plus a sparse perturbation, and an initial graph estimation, our proposed algorithm simultaneously learns the low-rank component and refines the graph. The refined graph improves the effectiveness of the graph smoothness constraint and increases the accuracy of the low-rank estimation. We derive the learning steps using ADMM. Our evaluations using synthetic and real brain imaging data in a supervised classification task demonstrate encouraging performance.", "creator": "LaTeX with hyperref package"}}}