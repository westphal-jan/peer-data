{"id": "1609.08419", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "27-Sep-2016", "title": "Decision Making Based on Cohort Scores for Speaker Verification", "abstract": "Decision making is an important component in a speaker verification system. For the conventional GMM-UBM architecture, the decision is usually conducted based on the log likelihood ratio of the test utterance against the GMM of the claimed speaker and the UBM. This single-score decision is simple but tends to be sensitive to the complex variations in speech signals (e.g. speech quality and accuracy; for example, in the speech with a large sample size) and the general accuracy of the test utterance versus the test utterance against the UBM. The GMM-UBM also has a number of variables, including speech speed and amplitude. The key to understanding the specific characteristics of the test utterance can be done by examining the input frequency. For example, in the example of the test utterance, a loudest voice, loudest voice, loudest voice, or loudest voice, is used to generate a voice count. In other words, the loudest voice is used to generate a voice count. Finally, the output is used for calculating the frequency of the test utterance to allow for the accuracy of the test utterance over the period of three to four years. The key to understanding the particular characteristics of the test utterance can be done using a variety of statistical methods. First, the GMM-UBM uses an input frequency to determine the correct pitch of the test utterance. This is a different way to compare the exact pitch of the test utterance between the test utterance and the UBM, which is the same as using the same inputs. When the pitch of the test utterance is reached, the test utterance is compared with the normal pitch. The pitch of the test utterance is compared with the normal pitch of the test utterance to give you an estimate of the number of the correct pitch. Finally, the test utterance can be compared with the normal pitch of the test utterance to give you an estimate of the number of the correct pitch.\n\n\n\n\nThe general results of the test utterance are not as important to anyone as they might appear in the first place. However, some authors consider the specific characteristics of the test utterance in general to be the following:\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022", "histories": [["v1", "Tue, 27 Sep 2016 13:29:12 GMT  (1477kb,D)", "http://arxiv.org/abs/1609.08419v1", "APSIPA ASC 2016"]], "COMMENTS": "APSIPA ASC 2016", "reviews": [], "SUBJECTS": "cs.SD cs.AI cs.LO", "authors": ["lantian li", "renyu wang", "gang wang", "caixia wang", "thomas fang zheng"], "accepted": false, "id": "1609.08419"}, "pdf": {"name": "1609.08419.pdf", "metadata": {"source": "CRF", "title": "Decision Making Based on Cohort Scores for Speaker Verification", "authors": ["Lantian Li", "Renyu Wang", "Gang Wang", "Caixia Wang", "Thomas Fang Zheng"], "emails": ["fzheng@tsinghua.edu.cn"], "sections": [{"heading": null, "text": "I. INTRODUCTION Speaker verification aims to verify claimed identities of speakers, and has gained great popularity in a wide range of applications including access control, forensic evidence provision and user authentication. After decades of research, lots of popular speaker verification approaches have been proposed, such as Gaussian mixture model-universal background model (GMM-UBM) [1], joint factor analysis (JFA) [2] and its \u2018simplified\u2019 version, the i-vector model [3]. Accompanied with these models, various back-end techniques have also been proposed to promote the discriminative capability for speakers, such as within-class covariance normalization (WCCN) [4], nuisance attribute projection (NAP) [5] and probabilistic LDA (PLDA) [6], etc. These methods have been demonstrated to be highly successful. Recently, deep learning has been applied to speaker verification and gained much interest [7], [8].\nWithin a speaker verification system, decision making is an important component [9]. To make a decision, the verification system first determines a score for the test utterance that reflects the confidence that the utterance is from the claimed speaker, and then compares the score with a predefined threshold. In a typical GMM-UBM system, the score is often computed as the log likelihood ratio that the test utterance being generated from the GMM of the claimed speaker and\nthe UBM. This single-score decision is simple and efficient, but it tends to be quite sensitive to variations in speech signals, e.g., in terms of text contents, channel conditions and speaking styles. This sensitivity means that choosing an appropriate threshold is rather difficult, or leading to error-pron decisions.\nTo deal with this score variation, various score normalization techniques have been proposed. Most of the normalization approaches, according to [10], can be explained using the Bayes\u2019 theorem. Among these approaches the cohort normalization is particular interesting. This approach chooses a set of cohort speakers who are close to the genuine speaker, and for each test utterance, it computes a set of \u2018cohort scores\u2019 on the models of these speakers. These cohort scores then replace the UBM to normalize the score of the test utterance against the claimed speaker [11], [12]. Using cohort models tends to model the alternative hypothesis more accurately, due to its more flexible structure compared to a single UBM. However, the existing methods based on cohort models do not fully utilize the information involved in the cohort scores: they are just simply averaged to normalize the target score, which is still a single-score approach.\nThis paper presents a new cohort approach that utilizes the cohort scores in a more effective way. Specifically, we propose to make decisions on the whole cohort scores (formulated as a score vector), and employ a powerful discriminative model to make the decision. Our assumption is that the knowledge involved in the cohort scores is more than a mean average, but as complex as their distributions, their ranks, spanning areas, etc. Fully utilization of these rich information results in a true multi-score decision making, which is expected to be more reliable than the traditional single-score approach.\nThe technique presented in this paper involves three steps: (1) Firstly, a set of cohort models is constructed by a clustering algorithm; (2) Secondly, for each test utterance, scores are estimated among the claimed speaker GMM, the global UBM and the cohort GMMs; (3) Finally, a classification model (SVM or DNNs) is employed to make the decision based on some features derived from the scores derived above.\nThe layout of this paper is organized as follows. Sec-\nar X\niv :1\n60 9.\n08 41\n9v 1\n[ cs\n.S D\n] 2\n7 Se\np 20\n16\ntion II presents the proposed cohort-based decision making framework. The experiments are presented in Section III, and Section IV concludes the paper."}, {"heading": "II. COHORT-BASED DECISION MAKING FRAMEWORK", "text": "In a typical GMM-UBM speaker verification system, the score likelihood ratio of a test utterance is computed over the GMM of the claimed speaker model and UBM. Then the likelihood ratio will be compared with a predefined threshold. If it is higher than the threshold, the test utterance will be accepted, else rejected. We argue that this naive decision making approach is unreliable and less robustness because this likelihood ratio only describes the distance between the claimed GMM and UBM, and it does not make use of the world speakers and corresponding score information. Therefore, we design a cohort-based decision making framework, as shown in Fig. 1. This framework is made up of three parts: cohort selection, feature design and discriminative model training."}, {"heading": "A. Cohort selection", "text": "A vector quantization (VQ) method [13] based on the Kmeans algorithm was utilized to conduct the speaker model clustering. The centroid of each cluster represents a reference speaker, and all the reference speakers build the \u2018cohort\u2019. We chose a weighted K-L distance to measure the distances among Gaussian mixture models, given by:\nD(\u03bb1, \u03bb2) = M\u2211 i=1 wiKL(N 1 i , N 2 i ) (1)\nKL(N1, N2) = 1\n2 (\u00b51 \u2212 \u00b52)T (\u03a3\u221211 \u2212 \u03a3 \u22121 2 )(\u00b51 \u2212 \u00b52) (2)\nwhere \u03bb1 and \u03bb2 are two Gaussian mixture models, and wi is the weight of ith Gaussian component. Note that, for fast computation, only the mean parameters are adapted in the GMM-MAP process, while the weights and variances of the GMMs are the same as UBM. Equ. 2 is used to measure the distance between two multi-dimensional Gaussian distributions.\nGiven a set of speaker GMMs \u03bb = (\u03bb1, \u03bb2, ..., \u03bbn) and \u03bbci that is the cluster centroid where speaker i is assigned to. The\noptimization objective is to minimize the within-class cost J , and finally each cluster centroid is regarded as one \u2018cohort\u2019 model.\nJ = 1\nN N\u2211 i=1 D(\u03bbi, \u03bbci) (3)"}, {"heading": "B. Feature design", "text": "Once the cohort models (CGMMs) have been determined, a set of cohort scores are calculated on the claimed speaker GMM, UBM and CGMMs respectively for each test utterance. We seek to use these cohort scores to explore some potential knowledge and design more discriminative features on genuine and imposter speaker models. In this part, three cohort-based score features are discussed.\n1) Cohort-based score normalization: The inspiration of this feature comes from the conventional score normalization techniques [10]. For a test feature vector X , the normalized score L\u0303\u03bb(X) is given as follows:\nL\u0303\u03bb(X) = L\u03bb(X)\u2212 \u00b5\u03bb\n\u03c3\u03bb (4)\nwhere \u03bb represents a claimed speaker model, and \u00b5\u03bb, \u03c3\u03bb is estimated from the cohort scores.\n2) Rank position: Assuming the size of cohort is K, for each test trial, a (1+K)-dimensional score vector is calculated based on GMM and CGMMs. And we think that the likelihood scores on the genuine speaker GMMs are at the top-rank position in the (1+K)-dimensional score vector, while for the imposter speakers, it lies in a random rank position.\n3) Rank of score differences: Similar assumption with the rank position, we also believe that the distribution of cohort scores on the genuine speaker models is different from that on imposter speaker models. For each test utterance, the score feature is calculated by subtracting the likelihood score on the claimed speaker GMM from that on each cohort CGMM. It describes a high-dimensional cohort-based score distribution instead of the UBM space. After ranking it, this score feature also covers the information of rank position, and has strong discriminability on genuine and imposter speaker models. This assumption will be verified in Section III-C."}, {"heading": "C. Discriminative model training", "text": "Based on these features derived from the cohort scores, discriminative models (e.g., support vector machine (SVM) and deep neural networks (DNNs) can be directly optimize with respect to the speaker verification task, i.e., the genuine/imposter speaker decision."}, {"heading": "III. EXPERIMENTS", "text": ""}, {"heading": "A. Database", "text": "The experiments are performed on a database called \u2018CSLTDSDB\u2019 (Digit String Database) that was jointly created by CSLT (Center for Speech and Language Technologies), Tsinghua University and Beijing d-Ear Technologies, Co. Ltd. The text of all recordings is the text-prompted digit strings.\nThe recordings were conducted using different mobile microphones, sampled at 16 kHz with 16-bit precision.\n\u2022 Training set: It contains an approximate size of 1 GB data (about 200 males and 200 females) recorded in an ordinary office environment. And it is used for the UBM training.\n\u2022 Development set: It contains 280 enrollment utterances covering 145 speakers and 2, 874 test utterances. And it is used for cohort selection and feature design.\n\u2022 Evaluation set: It involves 92 speakers. For each speaker, there are text-prompted digit strings of about 40 seconds in length for speaker model training; and 8-16 randomly generated digit strings each of which is an 8-digit string for verification. There are overall 1, 220 test utterances and 1, 220 target trials and 111, 020 non-target trials."}, {"heading": "B. Experimental setup", "text": "The acoustic feature was the conventional 39-dimensional Mel frequency cepstral coefficients (MFCC), which involves 13-dimensional static components plus the first and second order derivatives. The UBM consisted of 256 Gaussian components and was trained with the training set. Note that this setting is \u2018almost\u2019 optimal in our experiments, i.e., using more Gaussian components cannot improve system performance. And the baseline of GMM-UBM system on the evaluation set was 1.621% in terms of EER (Equal Error Rate).\nBesides, with the maximum a posterior (MAP) algorithm, 280 speaker GMMs were adapted from UBM. And The Kmeans algorithm was used to cluster the 280 speaker GMMs into a suitable cohort. Fig. 2 presents the function between the number of clusters and the clustering cost J . It can be observed that when the number of clusters exceeds 10, the clustering cost J has already been converged. Therefore, the size of cohort was set to 10.\nIn order to select the discriminative score feature, 6, 115 target trials and 5, 748 imposter trials were selected from the development set. Considering the unbalanced data problem 1,\n1The number of target samples and imposter samples will be highly unbalanced, one or some few target samples against large amount of imposter samples. And learning from such unbalanced data will result in biased SVM/DNNs models.\nonly the top two scores were selected from all the imposter speaker models."}, {"heading": "C. Feature design", "text": "1) Cohort-based score normalization: According to Equ. 4, the normalized score for each test was calculated, and the system performance was 1.639% in EER. It shows reasonable performance and can be considered as an available score feature.\n2) Rank position: From Fig. 3, we observed that this rank position has certain discriminability. Nearly all the likelihood scores on the genuine speaker GMMs are at the first rank position, while for imposter speaker models, the rank position distribution approximately satisfies a Gaussian distribution with the mean of 5.\n3) Rank of score differences: To provide an intuitive understanding of the discriminative capability of this feature, the rank of score differences of all the test trials are plotted in a two-dimensional space via T-SNE [14]. As shown in Fig. 4.\nIt can be seen that there exists a distinct non-linear boundary between genuine speaker models and imposter speaker models. That is to say, this \u2018rank of score differences\u2019 has strong discriminative capability on genuine and imposter speaker models."}, {"heading": "D. Discriminative model training", "text": "With these cohort-based score features, discriminative models can be optimized with respect to discriminating the genuine/imposter speakers. In this paper, both the SVM and DNNs models were trained as the decision maker for speaker verification system.\n1) SVM-based scoring: The SVMs were trained for each cohort-based score feature with the linear kernel function. Results are shown in Table I on condition of C1-C3. Note that \u2018norm\u2019 is the \u2018Cohort-based score normalization\u2019, \u2018r-pos\u2019 is the \u2018Rank position\u2019, \u2018r-diff\u2019 is the \u2018Rank of score differences\u2019 and\u221a\nrepresents that related features are chosen as the input of SVMs.\n2) DNN-based scoring: The DNN models were trained with these cohort-based score features, and the decision was made by logistic regression model at the soft-max layer. Note that for different input feature, the experimental results can be optimized with tuning of the DNNs structure such as the number of hidden units and hidden layers. Whereas, in order to unify the experimental configuration, we just set the number of hidden layer units 10 times as much as the dimension of input features, and there is only 1 hidden layers. The results are shown in Table II on the condition of C1-C3.\n3) Feature combination: From Table I and Table II, it can be seen that in condition C1-C3, both the SVM- and DNNbased scoring offer clear performance improvement than the GMM-UBM baseline 1.621%. Therefore, a feature combination scheme was proposed by concatenation these score features together. Experiment results are shown on condition of C4-C7. It can be observed that the performance of this simple feature combination is inconsistent, and we attribute it to the feature redundancy because all these features are embedded from the cohort scores. Besides, the overall feature combination C7 on DNN-based scoring system obtains the best performance."}, {"heading": "IV. CONCLUSIONS", "text": "This paper presents a decision making method based on cohort scores instead of the traditional single decision score. Some potential discriminative features are embedded from cohort scores, and then more powerful discriminative models are trained as the decision maker. Experimental results show that the proposed \u2018rank of score differences\u2019 with SVM/DNNbased scoring model can obtain stable and better system performance than the GMM-UBM baseline. Moreover, a feature combination scheme is proposed to further improve system performance. Future work involves designing more robustness score-level discriminative features and more reasonable cohort selection approaches."}, {"heading": "ACKNOWLEDGMENT", "text": "This work is supported by the National Natural Science Foundation of China under Grant No. 61371136 and No. 61271389, it was also supported by the National Basic Research Program (973 Program) of China under Grant No. 2013CB329302."}], "references": [{"title": "Speaker verification using adapted Gaussian mixture models", "author": ["D.A. Reynolds", "T.F. Quatieri", "R.B. Dunn"], "venue": "Digital Signal Processing, vol. 10, no. 1-3, pp. 19\u201341, 2000.", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2000}, {"title": "Joint factor analysis versus eigenchannels in speaker recognition", "author": ["P. Kenny", "G. Boulianne", "P. Ouellet", "P. Dumouchel"], "venue": "IEEE Transactions on Audio, Speech, and Language Processing, vol. 15, no. 4, pp. 1435\u20131447, 2007.", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2007}, {"title": "Front-end factor analysis for speaker verification", "author": ["N. Dehak", "P. Kenny", "R. Dehak", "P. Dumouchel", "P. Ouellet"], "venue": "IEEE Transactions on Audio, Speech, and Language Processing, vol. 19, no. 4, pp. 788\u2013798, 2011.", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2011}, {"title": "Within-class covariance normalization for svm-based speaker recognition", "author": ["A.O. Hatch", "S.S. Kajarekar", "A. Stolcke"], "venue": "Proc. INTER- SPEECH\u201906, 2006.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2006}, {"title": "Support vector machines using gmm supervectors for speaker verification", "author": ["W.M. Campbell", "D.E. Sturim", "D.A. Reynolds"], "venue": "Signal Processing Letters, vol. 13, no. 5, pp. 308\u2013311, 2006.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2006}, {"title": "Probabilistic linear discriminant analysis for inferences about identity", "author": ["S.J. Prince", "J.H. Elder"], "venue": "ICCV\u201907. IEEE, 2007, pp. 1\u20138.", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2007}, {"title": "Deep neural networks for extracting baum-welch statistics for speaker recognition", "author": ["P. Kenny", "V. Gupta", "T. Stafylakis", "P. Ouellet", "J. Alam"], "venue": "Odyseey\u20192014. Odyssey, 2014.", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2014}, {"title": "Deep neural networks for small footprint text-dependent speaker verification", "author": ["V. Ehsan", "L. Xin", "E. McDermott", "I.L. Moreno", "J. Gonzalez- Dominguez"], "venue": "IEEE International Conference on Acoustic, Speech and Signal Processing (ICASSP),2014. IEEE, 2014, pp. 4052\u2013 4056.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2014}, {"title": "Speaker recognition: A tutorial", "author": ["J.P. Campbell Jr"], "venue": "Proceedings of the IEEE, vol. 85, no. 9, pp. 1437\u20131462, 1997.", "citeRegEx": "9", "shortCiteRegEx": null, "year": 1997}, {"title": "Score normalization for text-independent speaker verification systems", "author": ["R. Auckenthaler", "M. Carey", "H. Lloyd-Thomas"], "venue": "Digital Signal Processing, vol. 10, no. 1-3, pp. 42\u201354, 2000.", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2000}, {"title": "Impostor cohort selection for score normalisation in speaker verification", "author": ["R.A. Finan", "R.I. Damperb", "A.T. Sapeluk"], "venue": "Pattern Recognition Letters, vol. 18, no. 9, 1997.", "citeRegEx": "11", "shortCiteRegEx": null, "year": 1997}, {"title": "The use of cohort normalized scores for speaker verification", "author": ["A.E. Rosenberg", "J. DeLong", "C.-H. Lee", "B.-H. Juang", "F.K. Soong"], "venue": "ICSLP\u201992, 1992, pp. 4052\u20134056.", "citeRegEx": "12", "shortCiteRegEx": null, "year": 1992}, {"title": "Vector quantization", "author": ["R. Gray"], "venue": "IEEE Assp Magazine, vol. 1, no. 2, pp. 4\u201329, 1994.", "citeRegEx": "13", "shortCiteRegEx": null, "year": 1994}, {"title": "Visualizing data using t-sne", "author": ["L. v. d. Maaten", "G. Hinton"], "venue": "Machine Learning Research, 2008.", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2008}], "referenceMentions": [{"referenceID": 0, "context": "After decades of research, lots of popular speaker verification approaches have been proposed, such as Gaussian mixture model-universal background model (GMM-UBM) [1], joint factor analysis (JFA) [2] and its \u2018simplified\u2019 version, the i-vector model [3].", "startOffset": 163, "endOffset": 166}, {"referenceID": 1, "context": "After decades of research, lots of popular speaker verification approaches have been proposed, such as Gaussian mixture model-universal background model (GMM-UBM) [1], joint factor analysis (JFA) [2] and its \u2018simplified\u2019 version, the i-vector model [3].", "startOffset": 196, "endOffset": 199}, {"referenceID": 2, "context": "After decades of research, lots of popular speaker verification approaches have been proposed, such as Gaussian mixture model-universal background model (GMM-UBM) [1], joint factor analysis (JFA) [2] and its \u2018simplified\u2019 version, the i-vector model [3].", "startOffset": 249, "endOffset": 252}, {"referenceID": 3, "context": "Accompanied with these models, various back-end techniques have also been proposed to promote the discriminative capability for speakers, such as within-class covariance normalization (WCCN) [4], nuisance attribute projection (NAP) [5] and probabilistic LDA (PLDA) [6], etc.", "startOffset": 191, "endOffset": 194}, {"referenceID": 4, "context": "Accompanied with these models, various back-end techniques have also been proposed to promote the discriminative capability for speakers, such as within-class covariance normalization (WCCN) [4], nuisance attribute projection (NAP) [5] and probabilistic LDA (PLDA) [6], etc.", "startOffset": 232, "endOffset": 235}, {"referenceID": 5, "context": "Accompanied with these models, various back-end techniques have also been proposed to promote the discriminative capability for speakers, such as within-class covariance normalization (WCCN) [4], nuisance attribute projection (NAP) [5] and probabilistic LDA (PLDA) [6], etc.", "startOffset": 265, "endOffset": 268}, {"referenceID": 6, "context": "Recently, deep learning has been applied to speaker verification and gained much interest [7], [8].", "startOffset": 90, "endOffset": 93}, {"referenceID": 7, "context": "Recently, deep learning has been applied to speaker verification and gained much interest [7], [8].", "startOffset": 95, "endOffset": 98}, {"referenceID": 8, "context": "Within a speaker verification system, decision making is an important component [9].", "startOffset": 80, "endOffset": 83}, {"referenceID": 9, "context": "Most of the normalization approaches, according to [10], can be explained using the Bayes\u2019 theorem.", "startOffset": 51, "endOffset": 55}, {"referenceID": 10, "context": "These cohort scores then replace the UBM to normalize the score of the test utterance against the claimed speaker [11], [12].", "startOffset": 114, "endOffset": 118}, {"referenceID": 11, "context": "These cohort scores then replace the UBM to normalize the score of the test utterance against the claimed speaker [11], [12].", "startOffset": 120, "endOffset": 124}, {"referenceID": 12, "context": "A vector quantization (VQ) method [13] based on the Kmeans algorithm was utilized to conduct the speaker model clustering.", "startOffset": 34, "endOffset": 38}, {"referenceID": 9, "context": "1) Cohort-based score normalization: The inspiration of this feature comes from the conventional score normalization techniques [10].", "startOffset": 128, "endOffset": 132}, {"referenceID": 13, "context": "3) Rank of score differences: To provide an intuitive understanding of the discriminative capability of this feature, the rank of score differences of all the test trials are plotted in a two-dimensional space via T-SNE [14].", "startOffset": 220, "endOffset": 224}], "year": 2016, "abstractText": "Decision making is an important component in a speaker verification system. For the conventional GMM-UBM architecture, the decision is usually conducted based on the log likelihood ratio of the test utterance against the GMM of the claimed speaker and the UBM. This single-score decision is simple but tends to be sensitive to the complex variations in speech signals (e.g. text content, channel, speaking style, etc.). In this paper, we propose a decision making approach based on multiple scores derived from a set of cohort GMMs (cohort scores). Importantly, these cohort scores are not simply averaged as in conventional cohort methods; instead, we employ a powerful discriminative model as the decision maker. Experimental results show that the proposed method delivers substantial performance improvement over the baseline system, especially when a deep neural network (DNN) is used as the decision maker, and the DNN input involves some statistical features derived from the cohort scores.", "creator": "LaTeX with hyperref package"}}}