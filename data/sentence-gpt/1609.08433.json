{"id": "1609.08433", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "27-Sep-2016", "title": "Local Training for PLDA in Speaker Verification", "abstract": "PLDA is a popular normalization approach for the i-vector model, and it has delivered state-of-the-art performance in speaker verification. However, PLDA training requires a large amount of labeled development data, which is highly expensive in most cases. A possible approach to mitigate the problem is various unsupervised adaptation methods, which use unlabeled data to adapt the PLDA scattering matrices to the target domain. To make this possible, PLDA training requires a set of labeled features, which can be combined with two methods:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n", "histories": [["v1", "Tue, 27 Sep 2016 13:37:13 GMT  (757kb,D)", "http://arxiv.org/abs/1609.08433v1", "O-COCOSDA 2016"]], "COMMENTS": "O-COCOSDA 2016", "reviews": [], "SUBJECTS": "cs.SD cs.CL", "authors": ["chenghui zhao", "lantian li", "dong wang", "april pu"], "accepted": false, "id": "1609.08433"}, "pdf": {"name": "1609.08433.pdf", "metadata": {"source": "CRF", "title": "Local Training for PLDA in Speaker Verification", "authors": ["Chenghui Zhao", "Lantian Li", "Dong Wang"], "emails": ["pu}@pachiratech.com", "lilt@cslt.riit.tsinghua.edu.cn", "wangdong99@mails.tsinghua.edu.cn"], "sections": [{"heading": null, "text": "In this paper, we present a new \u2018local training\u2019 approach that utilizes inaccurate but much cheaper local labels to train the PLDA model. These local labels discriminate speakers within a single conversion only, and so are much easier to obtain compared to the normal \u2018global labels\u2019. Our experiments show that the proposed approach can deliver significant performance improvement, particularly with limited globally-labeled data.\nIndex Terms\u2014PLDA, i-vector, speaker verification\nI. INTRODUCTION\nThe i-vector model plus various normalization approaches offers the standard framework for modern speaker verification systems [1], [2], [3], [4]. Basically, the i-vector model uses a Gaussian mixture model (GMM) or a deep neural network (DNN) to collect the Baum-Welch sufficient statistics of an utterance, and then projects it onto a low-dimensional total variability space. These low-dimensional representations, or i-vectors, involve mixed information from both speakers and channels, and therefore require some normalization techniques to separate speaker information from other undesired variability. Probabilistic linear discriminant analysis (PLDA) is one of the most popular normalization methods. It assumes that i-vectors of utterances of a particular speaker form a Gaussian distribution, with the mean vector following a normal distribution [2]. Scoring the hypothesis that two ivectors belong to the same speaker with the PLDA model involves marginalization of the prior distribution under the two hypothesises that the two i-vectors are from the same speaker or not. Combined with length normalization, PLDA delivers state-of-the-art performance in various test benchmarks [4].\nTraining a PLDA model requires a large amount of labeled data, usually thousands of speakers, each with multiple sessions. For example, in the two popular development databases Fisher [5] and Switchboard [6], there are 12,399 and 543 speakers, respectively. In many practical situations, collecting such a large amount of labeled data is very difficult and time\nconsuming. For instance, for a phone-call archive from a callcenter service, it is often highly difficult and time consuming to tell whether two calls are from the same speaker, and it is more difficult to cluster calls from customers into thousands of speakers. Therefore, a typical situation that we often encounter is: a small labeled database is available, but it is often outof-domain; on the other hand, there is a large amount of indomain data but the data are difficult to label. This situation is also reflected in the NIST i-vector challenge [7], where 36,572 unlabeled i-vectors were provided for system building. How to use unlabeled data is a critical problem in particular for practical systems.\nA number of techniques have been proposed to deal with the situation, most of them are based on unsupervised adaptation. For example, Garcia-Romero et al. [8] used an outof-domain PLDA to cluster in-domain data into some pseudo speakers, based on which the PLDA covariance matrices were adapted. Villalba and colleagues [9] proposed a variational Bayesian method where the unknown labels were treated as latent variables. Liu et al. [10] proposed an approach where unlabeled data (i-vectors) were treated as from a universal speaker. The i-vectors of the universal speaker and other speakers were pooled together to train the PLDA model. Wang et al. [11] proposed a domain-adaptation approach based on maximum likelihood linear transformation (MLLT), and Rahman et al. [12] proposed a dataset-invariant covariance normalization approach that normalized i-vectors by a global covariance matrix computed from both in-domain and outdomain data. This is equal to project i-vectors of in-domain and out-domain speakers onto a third dataset-invariant space, so the PLDA model trained with the projected i-vectors is more robust against data mismatch.\nIn this paper, we propose a new PLDA training approach that is different from the above methods. The basic idea is to use the prior knowledge that a conversation involves only a few participants, and these participants can be easily separated by listeners or any audio segment method, resulting in conversation-based labels. These labels, however, only valid within individual conversions as they do not consider anything about the labels in other conversations. To obtain labels that can be used for PLDA training, we further assume that all\nar X\niv :1\n60 9.\n08 43\n3v 1\n[ cs\n.S D\n] 2\n7 Se\np 20\n16\nspeakers in any two conversations are distinct, leading to speaker labels something like \u2018conversation-id:speaker-id-inconversation\u2019. This assumption, of course, is not certainly true, because it is very possible that one speaker appears in multiple conversations. However, we find that in some practical scenarios, the possibility that the same speaker appears in two or more conversions is rather low. For example, in a call center archive of one week, customers in different conversations are almost different. We call the speaker labels that only discriminates participants within individual conversions as local labels, and the conventional labels that accurately discriminate crossconversation speakers as global labels. The PLDA training with local labels is identical to the procedure with global labels. Note that a major difficulty for speaker labeling is the comparison for speakers within different conversations, which means local labels are much cheaper than global labels, although they are not thus accurate and can be only regarded as partial supervision.\nThis paper is structured as follows: Section II presents the local training approach, and Section III presents the experiments, followed by some conclusions in Section IV."}, {"heading": "II. LOCAL PLDA TRAINING", "text": "In this section, the conventional PLDA model is briefly reviewed, and our proposed local PLDA training approach is then presented in details."}, {"heading": "A. PLDA model", "text": "PLDA is an extension of the linear discriminative analysis (LDA), by introducing a Gaussian prior on the mean vector of classes. Combined with length normalization, PLDA has delivered state-of-the-art performance in speaker verification. Letting wij denote the i-vector of the jth utterance (session) of the ith speaker, the PLDA model can be formulated as follows:\nwij = u+ V yi + zij\nwhere u is the speaker-independent global factor, and yi and zij represent the speaker-level and utterance-level factors, respectively. The matrix V involves the bases of the speaker subspace. Note that both yi and zij are assumed to follow a full-rank Gaussian prior. The model can be trained via an EM algorithm [13], and the similarity of two i-vectors can be computed as the ratio of the evidence (likelihood) of two hypothesises: whether or not the two i-vectors belong to the same speaker [14]."}, {"heading": "B. Global training and local training", "text": "Training a PLDA model also requires a large amount of labeled data, usually thousands of speakers each with multiple sessions. These labels discriminate speakers within multiple sessions and so are global labels. Global labels are very expensive, because it is usually very hard to tell whether a voice from a new utterance is from a speaker in a set that involves thousands of speakers that are already known, or from a new speaker. Most of existing databases were collected following a\nregistration-and-recording approach, which identifies speaker identities by meta information, instead of manual labeling. This approach is cheap in data labeling, but is costly in hiring speakers and managing the recording. Furthermore, it is not applicable in many practical scenarios where some in-domain data are important and therefore should be collected in a reallife environment but the meta information is not available.\nSome unsupervised learning methods have been proposed to solve the problem, as discussed already in the introduction [8], [9], [10], [11], [12]. Basically, these methods focus on i-vector normalization or adaptation, so that the normalized or adapted i-vectors can be better discriminated by the PLDA model that has been trained already. In other words, they can not improve the discriminative capability of the PLDA model.\nWe propose a local training approach that can be regarded as a weak-supervised method. Basically we label speakers in a conversation-independent way, which means that the labels only discriminate speakers within the same conversion, and speakers in different conversations are simply assumed to be different. With the local labels obtained, PLDA is trained as usual. We call this training based on local labels local training. Although this supervision is not fully accurate, we hope it is still possible to improve PLDA.\nFig. 1 illustrates the difference between local labels and global labels, where each speaker is represented by a particular color. For global labels, the segments from the same speaker but different conversations are correctly labeled. For local labels, speakers in different conversations are labeled as distinct."}, {"heading": "III. EXPERIMENT", "text": "The proposed local training approach is tested on a speaker verification task with telephone speech from a call-center archives. The system is designed based on the GMM-ivector framework. We first present the data profile and then report the results. Some analysis will be given to show in which condition the local training is most effective."}, {"heading": "A. Databases and experimental setting", "text": "The training data used to train the GMM-ivector system are composed of 500 hours of conversational speech signals. These data are used to train the UBMs and the T matrix of the i-vector model. The development data used to train the PLDA model are divided into two sets: the Global set and the Local set, with global and local labels respectively. Note that, the environmental condition of the Local set is more close to the condition of the evaluation data, which means that the Local set can be regarded as in-domain data and the unsupervised learning would be helpful. More details about the development data are shown in Table I.\nThe evaluation set involves 1, 236 speakers and the enrollment speech for each speaker is 15 seconds long. The test is conducted in 3 conditions, where the length of the test utterances grows from 5 seconds to 15 seconds. The details of the evaluation data are shown in Table II.\nThe acoustic feature used in our experiments is the 60- dimensional Mel frequency cepstral coefficients (MFCCs), which involves 20-dimensional static components plus the first and second order derivatives. The frame size is 25 ms and the frame shift is 10 ms. The UBM involves 1, 024 Gaussian components and the dimensionality of the i-vectors is 100. The performance is evaluated in terms of Equal Error Rate (EER) [15]."}, {"heading": "B. Basic results", "text": "We test three training strategies for PLDA, as shown below: \u2022 Global training (GT): The conventional PLDA training\nwith the Global dev set. It is regarded as the baseline in our experiment. \u2022 Local training (LT): Local PLDA training with the Local dev set. \u2022 Pooled training (Pool): PLDA training with both the Global set and the Local set.\nThe results are shown in Table III. For comparison, the results with cosine scoring are also reported. We first observe that all the three PLDA training approaches obtained significant performance improvement compared to the cosine scoring. This is particular interesting for the LT approach, where only local labels are available. This confirms our conjecture that cheap local labels can be used to train PLDA and obtain performance improvement with little effort on data labeling.\nAt the same time, it can be observed that the global training (GT) is still the most effective, and the local training (LT) and the pool training (Pool) are unable to beat the GT. This should be attributed to the noise in local labels, caused by the fact that the same speakers appeared in different conversations are simply labeled as distinct speakers."}, {"heading": "C. Study on pooled training", "text": "The superior performance with GT over LT is expected, due to the more accurate supervision with global labels. However, the lower performance with the pooled training compared to the GT is a bit surprising. As we have argued, the supervision with local labels is noisy but informative, which can be seen from the LT results in Table III. One reason that the performance was deteriorated is that the global training is so strong (6000 speakers in the Global set) that the noisy local training is not necessary. More investigations are required to confirm the conjecture and experiment with the condition under which local training is effective.\nWe first investigate the performance change with different amount of locally labeled data, with the globally labeled data fixed. The results are shown in Fig. 2. For a clear presentation, we only show the results on the test condition C5; the performance on C10 and C15 show similar trends. In Fig. 2, the number of speakers of the globally labeled data (global speaker) is set to 0, 200 and 3000 respectively, corresponding to the three curves in the picture. The number of speakers of local labeled data (local speaker) varies from 0 to 2000. Note that the case of 0 global speaker is just the local training approach, and the case of 0 local speaker is simply the global training approach.\nFrom the results shown in Fig. 2, we first observe that the performance of the local training approach is monotonically improved with more locally labeled data. When the locally labeled data is sufficient, the performance of the pooled training seems saturated at a level close to the performance of global training with hundreds of global speakers. Moreover, it can be seen that when the number of global speakers is 200 (the dash line), involving locally labeled data is helpful. The performance is firstly improved when a small number of local speakers are involved, and then it is degraded a little when more local speakers are involved. This result indicates that the information conveyed by locally labeled data is useful when the globally labeled data are insufficient. When the number of global speakers is 3000 (the dot line), involving locally labeled data does not improve the performance; actually it may deteriorate the performance if the locally labeled data are too many.\nTo make the picture complete, we fix the number local\nspeakers, and vary the number of global speakers from 0 to 2000. The results are shown in Fig. 3. Again, only the results on the C5 test condition are presented. The four curves in Fig. 3 show the results with the number of local speakers set to 0, 100, 200 and 2000, respectively. The same information can be read as from Fig. 2.\nAs a summary, we find that the local training is mostly effective when the global training is weak, i.e., the number of global speakers is small. If the globally labeled data are sufficient, the local training is not very useful. In practice, it is often the case that globally labeled data are very limited, suggesting the potential value of the local training approach."}, {"heading": "IV. CONCLUSION", "text": "This paper proposed a local training approach for PLDA and verified its potential in speaker verification. Based on the assumption that speakers in different conversations tend to be distinct, local labels posses a high probability to be correct and so can be used as weak supervision to train PLDA. Experimental results demonstrated that the local training approach can improve system performance when globally labeled data are limit. A particular problem of the local training is the conversion independent assumption. Future work will investigate to what extent this assumption holds would result in an effective local training."}, {"heading": "ACKNOWLEDGMENT", "text": "This work was supported by the National Natural Science Foundation of China under Grant No.61271389 and NO.61371136 and the National Basic Research Program (973 Program) of China under Grant No.2013CB329302."}], "references": [{"title": "Front-end factor analysis for speaker verification", "author": ["N. Dehak", "P. Kenny", "R. Dehak", "P. Dumouchel", "P. Ouellet"], "venue": "IEEE Transactions on Audio, Speech, and Language Processing, vol. 19, no. 4, pp. 788\u2013798, 2011.", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2011}, {"title": "Analysis of i-vector length normalization in speaker recognition systems.", "author": ["D. Garcia-Romero", "C.Y. Espy-Wilson"], "venue": "in Proc. INTER- SPEECH\u201911,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2011}, {"title": "A novel scheme for speaker recognition using a phonetically-aware deep neural network", "author": ["Y. Lei", "L. Ferrer", "M. McLaren"], "venue": "Acoustics, Speech and Signal Processing (ICASSP), 2014 IEEE International Conference on. IEEE, 2014, pp. 1695\u20131699.", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2014}, {"title": "Bayesian speaker verification with heavy-tailed priors", "author": ["P. Kenny"], "venue": "Proc. Odyssey, 2010.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2010}, {"title": "The fisher corpus:a resource for the next generations of speech-to-text", "author": ["C. Cieri", "D. Miller", "K. Walker"], "venue": "The Fourth International Conference on Language Resources and Evaluation (LREC),2004, 2004, pp. 69\u201371.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2004}, {"title": "Improving speaker recognition performance in the domain adaptation challenge using deep neural networks", "author": ["D. Garcia-Romero", "X. Zhang", "A. McCree", "D. Povey"], "venue": "Spoken Language Technology Workshop (SLT), 2014 IEEE. IEEE, 2014, pp. 378\u2013383.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2014}, {"title": "Unsupervised adaptation of plda by using variational bayes methods", "author": ["J. Villalba", "E. Lleida"], "venue": "Acoustics, Speech and Signal Processing (ICASSP), 2014 IEEE International Conference on. IEEE, 2014, pp. 744\u2013748.", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2014}, {"title": "Utilization of unlabeled development data for speaker verification", "author": ["G. Liu", "C. Yu", "N. Shokouhi", "A. Misra", "H. Xing", "J.H. Hansen"], "venue": "Spoken Language Technology Workshop (SLT), 2014 IEEE. IEEE, 2014, pp. 418\u2013423.", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2014}, {"title": "Domain adaptation using maximum likelihood linear transformation for plda-based speaker verification", "author": ["Q. Wang", "H. Yamamoto", "T. Koshinaka"], "venue": "Acoustics, Speech and Signal Processing (ICASSP), 2016 IEEE International Conference on. IEEE, 2016, pp. 5110\u20135114.", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2016}, {"title": "Dataset-invariant covariance normalization for out-domain plda speaker verification", "author": ["M.H. Rahman", "A. Kanagasundaram", "D. Dean", "S. Sridharan"], "venue": "Proc. INTERSPEECH\u201915, 2015, pp. 1017\u20131021.", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2015}, {"title": "Probabilistic linear discriminant analysis for inferences about identity", "author": ["S.J. Prince", "J.H. Elder"], "venue": "ICCV\u201907. IEEE, 2007, pp. 1\u20138.", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2007}, {"title": "Sparse probabilistic linear discriminant analysis for speaker verification", "author": ["Y. Hai", "L. Yan", "X. Fei"], "venue": "Proc. INTERSPEECH\u201912, 2012, pp. 2658\u20132661.", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2012}, {"title": "The 2012 nist speaker recognition evaluation", "author": ["C.S. Greenberg", "V.M. Stanford", "A.F. Martin", "M. Yadagiri", "G.R. Doddington", "J.J. Godfrey", "J. Hernandez-Cordero"], "venue": "Proc. INTERSPEECH\u201913, 2013, pp. 1971\u20131975.", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2013}], "referenceMentions": [{"referenceID": 0, "context": "The i-vector model plus various normalization approaches offers the standard framework for modern speaker verification systems [1], [2], [3], [4].", "startOffset": 127, "endOffset": 130}, {"referenceID": 1, "context": "The i-vector model plus various normalization approaches offers the standard framework for modern speaker verification systems [1], [2], [3], [4].", "startOffset": 132, "endOffset": 135}, {"referenceID": 2, "context": "The i-vector model plus various normalization approaches offers the standard framework for modern speaker verification systems [1], [2], [3], [4].", "startOffset": 137, "endOffset": 140}, {"referenceID": 3, "context": "The i-vector model plus various normalization approaches offers the standard framework for modern speaker verification systems [1], [2], [3], [4].", "startOffset": 142, "endOffset": 145}, {"referenceID": 1, "context": "It assumes that i-vectors of utterances of a particular speaker form a Gaussian distribution, with the mean vector following a normal distribution [2].", "startOffset": 147, "endOffset": 150}, {"referenceID": 3, "context": "Combined with length normalization, PLDA delivers state-of-the-art performance in various test benchmarks [4].", "startOffset": 106, "endOffset": 109}, {"referenceID": 4, "context": "For example, in the two popular development databases Fisher [5] and Switchboard [6], there are 12,399 and 543 speakers, respectively.", "startOffset": 61, "endOffset": 64}, {"referenceID": 5, "context": "[8] used an outof-domain PLDA to cluster in-domain data into some pseudo speakers, based on which the PLDA covariance matrices were adapted.", "startOffset": 0, "endOffset": 3}, {"referenceID": 6, "context": "Villalba and colleagues [9] proposed a variational Bayesian method where the unknown labels were treated as latent variables.", "startOffset": 24, "endOffset": 27}, {"referenceID": 7, "context": "[10] proposed an approach where unlabeled data (i-vectors) were treated as from a universal speaker.", "startOffset": 0, "endOffset": 4}, {"referenceID": 8, "context": "[11] proposed a domain-adaptation approach based on maximum likelihood linear transformation (MLLT), and Rahman et al.", "startOffset": 0, "endOffset": 4}, {"referenceID": 9, "context": "[12] proposed a dataset-invariant covariance normalization approach that normalized i-vectors by a global covariance matrix computed from both in-domain and outdomain data.", "startOffset": 0, "endOffset": 4}, {"referenceID": 10, "context": "The model can be trained via an EM algorithm [13], and the similarity of two i-vectors can be computed as the ratio of the evidence (likelihood) of two hypothesises: whether or not the two i-vectors belong to the same speaker [14].", "startOffset": 45, "endOffset": 49}, {"referenceID": 11, "context": "The model can be trained via an EM algorithm [13], and the similarity of two i-vectors can be computed as the ratio of the evidence (likelihood) of two hypothesises: whether or not the two i-vectors belong to the same speaker [14].", "startOffset": 226, "endOffset": 230}, {"referenceID": 5, "context": "Some unsupervised learning methods have been proposed to solve the problem, as discussed already in the introduction [8], [9], [10], [11], [12].", "startOffset": 117, "endOffset": 120}, {"referenceID": 6, "context": "Some unsupervised learning methods have been proposed to solve the problem, as discussed already in the introduction [8], [9], [10], [11], [12].", "startOffset": 122, "endOffset": 125}, {"referenceID": 7, "context": "Some unsupervised learning methods have been proposed to solve the problem, as discussed already in the introduction [8], [9], [10], [11], [12].", "startOffset": 127, "endOffset": 131}, {"referenceID": 8, "context": "Some unsupervised learning methods have been proposed to solve the problem, as discussed already in the introduction [8], [9], [10], [11], [12].", "startOffset": 133, "endOffset": 137}, {"referenceID": 9, "context": "Some unsupervised learning methods have been proposed to solve the problem, as discussed already in the introduction [8], [9], [10], [11], [12].", "startOffset": 139, "endOffset": 143}, {"referenceID": 12, "context": "The performance is evaluated in terms of Equal Error Rate (EER) [15].", "startOffset": 64, "endOffset": 68}], "year": 2016, "abstractText": "PLDA is a popular normalization approach for the i-vector model, and it has delivered state-of-the-art performance in speaker verification. However, PLDA training requires a large amount of labeled development data, which is highly expensive in most cases. A possible approach to mitigate the problem is various unsupervised adaptation methods, which use unlabeled data to adapt the PLDA scattering matrices to the target domain. In this paper, we present a new \u2018local training\u2019 approach that utilizes inaccurate but much cheaper local labels to train the PLDA model. These local labels discriminate speakers within a single conversion only, and so are much easier to obtain compared to the normal \u2018global labels\u2019. Our experiments show that the proposed approach can deliver significant performance improvement, particularly with limited globally-labeled data.", "creator": "LaTeX with hyperref package"}}}