{"id": "1609.09405", "review": {"conference": "EMNLP", "VERSION": "v1", "DATE_OF_SUBMISSION": "29-Sep-2016", "title": "Evaluating Induced CCG Parsers on Grounded Semantic Parsing", "abstract": "We compare the effectiveness of four different syntactic CCG parsers for a semantic slot-filling task to explore how much syntactic supervision is required for downstream semantic analysis. This extrinsic, task-based evaluation also provides a unique window into the semantics captured (or missed) by unsupervised grammar induction systems. In this paper, we present the same constraints (which we use in a future paper), along with a more comprehensive, more extensive analysis of these constraints. We discuss how semantic processing in a semantic scanner works in a wide variety of contexts, such as contexts in which the semantic representation of text and language is constrained.", "histories": [["v1", "Thu, 29 Sep 2016 16:09:29 GMT  (377kb,D)", "http://arxiv.org/abs/1609.09405v1", "6 pages, short paper, EMNLP 2016"], ["v2", "Tue, 31 Jan 2017 16:25:39 GMT  (332kb,D)", "http://arxiv.org/abs/1609.09405v2", "EMNLP 2016, Table 2 erratum, Code and Freebase Semantic Parsing data URL"]], "COMMENTS": "6 pages, short paper, EMNLP 2016", "reviews": [], "SUBJECTS": "cs.CL cs.AI", "authors": ["yonatan bisk", "siva reddy", "john blitzer", "julia hockenmaier", "mark steedman"], "accepted": true, "id": "1609.09405"}, "pdf": {"name": "1609.09405.pdf", "metadata": {"source": "CRF", "title": "Evaluating Induced CCG Parsers on Grounded Semantic Parsing", "authors": ["Yonatan Bisk", "Siva Reddy", "John Blitzer", "Julia Hockenmaier", "Mark Steedman"], "emails": ["ybisk@isi.edu,", "siva.reddy@ed.ac.uk,", "blitzer@google.com,", "juliahmr@illinois.edu,", "steedman@inf.ed.ac.uk,"], "sections": [{"heading": "1 Introduction", "text": "The past several years have seen significant progress in unsupervised grammar induction (Carroll and Charniak, 1992; Yuret, 1998; Klein and Manning, 2004; Spitkovsky et al., 2010; Garrette et al., 2015; Bisk and Hockenmaier, 2015). But how useful are unsupervised syntactic parsers for downstream NLP tasks? What phenomena are they able to capture, and where would additional annotation be required? Instead of standard intrinsic evaluations \u2013 attachment scores that depend strongly on the particular annotation styles of the gold treebank \u2013 we examine the utility of unsupervised and weakly supervised parsers for semantics. We perform an extrinsic evaluation of unsupervised and weakly supervised CCG parsers on a grounded semantic parsing task that will shed light on the extent to which these systems recover semantic information. We focus on English to perform a direct comparison with supervised parsers (although unsupervised or weakly supervised approaches are likely to be most beneficial for domains or languages where supervised parsers are not available).\n\u2217Equal contribution\nSpecifically, we evaluate different parsing scenarios with varying amounts of supervision. These are designed to shed light on the question of how well syntactic knowledge correlates with performance on a semantic evaluation. We evaluate the following scenarios (all of which assume POS-tagged input): 1) no supervision; 2) a lexicon containing words mapped to CCG categories; 3) a lexicon containing POS tags mapped to CCG categories; 4) sentences annotated with CCG derivations (i.e., fully supervised). Our evaluation reveals which constructions are problematic for unsupervised parsers (and annotation efforts should focus on). Our results indicate that unsupervised syntax is useful for semantics, while a simple semi-supervised parser outperforms a fully unsupervised approach, and could hence be a viable option for low resource languages."}, {"heading": "2 CCG Intrinsic Evaluations", "text": "CCG (Steedman, 2000) is a lexicalized formalism in which words are assigned syntactic types, also known as supertags, encoding subcategorization information. Consider the sentence Google acquired Nest in 2014, and its CCG derivations shown in Figure 1. In (a) and (b), the supertag of acquired, (S\\NP)/NP, indicates that it has two arguments, and the prepositional phrase in 2014 is an adjunct, whereas in (c) the supertag ((S\\NP)/PP)/NP indicates acquired has three arguments including the prepositional phrase. In (a) and (b), depending on the supertag of in, the derivation differs. When trained on labeled treebanks, (a) is preferred. However note that all these derivations could lead to the same semantics (e.g., to the logical form in Equation 1). Without syntactic suar X\niv :1\n60 9.\n09 40\n5v 1\n[ cs\n.C L\n] 2\n9 Se\np 20\npervision, there may not be any reason for the parser to prefer one analysis over the other. One procedure to evaluate unsupervised induction methods has been to compare the assigned supertags to treebanked supertags, but this evaluation does not consider that multiple derivations could lead to the same semantics. This problem is also not solved by evaluating syntactic dependencies. Moreover, while many dependency standards agree on the head direction of simple constituents (e.g., noun phrases) they disagree on the most semantically useful ones (e.g., coordination and relative clauses).1"}, {"heading": "3 Our Proposed Evaluation", "text": "The above syntax-based evaluation metrics conceal the real performance differences and their effect on downstream tasks. Here we propose an extrinsic evaluation where we evaluate our ability to convert sentences to Freebase logical forms starting via CCG derivations. Our motivation is that most sentences can only have a single realization in Freebase, and any derivation that could lead to this realization is potentially a correct derivation. For example, the Freebase logical form for the example sentence in Figure 1 is shown below, and none of its derivations are penalized if they could result in this logical form.\n\u03bbe. business.acquisition(e)\n\u2227 acquiring company(e,GOOGLE) \u2227 company acquired(e,NEST) \u2227 date(e, 2014)\n(1)\nSince grammar induction systems are traditionally trained on declarative sentences, we would ideally require declarative sentences paired with Freebase logical forms. But such datasets do not exist in the Freebase semantic parsing literature (Cai and Yates, 2013; Berant et al., 2013). To alleviate this prob-\n1Please see Bisk and Hockenmaier (2013) for more details.\nlem, and yet perform Freebase semantic parsing, we propose an entity slot-filling task.\nEntity Slot-Filling Task. Given a declarative sentence containing mentions of Freebase entities, we randomly remove one of the mentions to create a blank slot. The task is to fill this slot by translating the declarative sentence into a Freebase query. Consider the following sentence where the entity Nest has been removed: Google acquired which was founded in Palo Alto\nTo correctly fill in the blank, one has to query Freebase for the entities acquired by Google (constraint 1) and founded in Palo Alto (constraint 2). If either of those constraints are not applied, there will be many entities as answers. For each question, we execute a single Freebase query containing all the constraints and retrieve a list of answer entities. From this list, we pick the first entity as our predicted answer, and consider the prediction as correct if the gold answer is the same as the predicted answer."}, {"heading": "4 Sentences to Freebase Logical Forms", "text": "CCG provides a clean interface between syntax and semantics, i.e. each argument of a words syntactic category corresponds to an argument of the lambda expression that defines its semantic interpretation (e.g., the lambda expression corresponding to the category (S\\NP)/NP of the verb acquired is \u03bbf.\u03bbg.\u03bbe.\u2203x.\u2203y.acquired(e) \u2227 f(x) \u2227 g(y) \u2227 arg1(e, y)\u2227arg2(e, x)), and the logical form for the complete sentence can be constructed by composing word level lambda expressions following the syntactic derivation (Bos et al., 2004). In Figure 2 we show two syntactic derivations for the same sentence, and the corresponding logical forms and equivalent graph representations derived by GRAPHPARSER (Reddy et al., 2014). The graph representations are possible because GRAPHPARSER assumes access to coindexations of input CCG categories. We provide\nco-indexation for all induced categories, including multiple co-indexations when an induced category is ambiguous. For example, (S\\N)/(S\\N) refers to either (Sx\\Ny)/(Sx\\Ny) indicating an auxiliary verb or (Sx\\Ny)/(Sz\\Ny) indicating a control verb. Initially, the predicates in the expression/graph will be based entirely on the surface form of the words in the sentence. This is the \u201cungrounded\u201d semantic representation.\nOur next step is to convert these ungrounded graphs to Freebase graphs.2 Like Reddy et al. (2014), we treat this problem as a graph matching problem. Using GRAPHPARSER we retrieve all the Freebase graphs that are isomorphic to the ungrounded graph, and select only the graphs that could correctly predict the blank slot, as candidate graphs. Using these candidate graphs, we train a structured perceptron that learns to rank grounded graphs for a given ungrounded graph.3 We use ungrounded predicate and Freebase predicate alignments as our features."}, {"heading": "5 Experiments", "text": ""}, {"heading": "5.1 Training and Evaluation Datasets", "text": "Our dataset SPADES (Semantic PArsing of DEclarative Sentences) is constructed from the declarative sentences collected by Reddy et al. (2014) from CLUEWEB09 (Gabrilovich et al., 2013) based on the following constraints: 1) There exists at least\n2Note that there is one-to-one correspondence between Freebase graphs and Freebase logical forms.\n3Please see Section 4.3 of Reddy et al. (2016) for details.\none isomorphic Freebase graph to the ungrounded representation of the input sentence; 2) There are no variable nodes in the ungrounded graph (e.g., Google acquired a company is discarded whereas Google acquired the company Nest is selected). We split this data into training (85%), development (5%) and testing (10%) sentences (Table 1). We introduce empty slots into these sentences by randomly removing an entity. SPADES can be downloaded at http:// github.com/sivareddyg/graph-parser.\nThere has been other recent interest in similar datasets for sentence completion (Zweig et al., 2012) and machine reading (Hermann et al., 2015), but unlike other corpora our data is tied directly to Freebase and requires the execution of a semantic parse to correctly predict the missing entity. This is made more explicit by the fact that one third of the entities in our test set are never seen during training, so without a general approach to query creation and execution there is a limit on a system\u2019s performance."}, {"heading": "5.2 Our Models", "text": "We use different CCG parsers varying in the amounts of supervision. For the UNSUPERVISED scenario, we use Bisk and Hockenmaier (2015)\u2019s parser which\nexploits a small set of universal rules to automatically induce and weight a large set of lexical categories. For the semi-supervised, we explore two options \u2013 SEMI-SUPERVISED-WORD and SEMI-SUPERVISEDPOS. We use Bisk et al. in both settings but we constrain its lexicon manually rather than inducing it from scratch. In the former, we restrict the top 200 words in English to occur only with the CCG categories that comprise 95% of the occurrences of a word\u2019s use in Section 22 of WSJ/CCGbank. In the latter, we restrict the POS tags instead of words. For the SUPERVISED scenario, we use EasyCCG (Lewis and Steedman, 2014) trained on CCGbank.\nFinally, in order to further demonstrate the amount of useful information being learned by our parsers, we present a competitive Bag-of-Words baseline, which is a perceptron classifier that performs \u201csemantic parsing\u201d by predicting either a Freebase or a null relation between the empty slot and every other entity in the sentence, using the words in the sentence as features. This naive approach is competitive on simple sentences with only two entities, rivaling even the fully supervised parser, but falters as complexity increases."}, {"heading": "5.3 Results and Discussion", "text": "Our primary focus is a comparison of intrinsic syntactic evaluation with our extrinsic semantic evaluation. To highlight the differences we present Section 23 parsing performance for our four models (Table 2). Dependency performance is evaluated on both the simplified labeled F1 of Bisk and Hockenmaier (2015) and Undirected Unlabeled F1.\nDespite the supervised parser performing almost twice as well as the semi-supervised parsers on CCGbank LF1 (53 vs 84), in our semantic evaluation we\nsee a comparatively small gain in performance (43 vs 46). It is interesting that such weakly supervised models are able to achieve over 90% of the performance of a fully supervised parser. To explore this further, we break down the semantics performance of all our models by the number of entities in a sentence. Each sentence has two, three, or four entities, one of which will be dropped for prediction. The more entities there are in a sentence, the more likely the models are to misanalyze a relation leading to their making the wrong prediction. These results are presented on the right side of Table 2. There are still notable discrepancies in performance, which we analyze more closely in the next section.\nAnother interesting result is the drop in performance by the Bag-of-Words Model. As the number of entities in the sentence increase, the model weakens, performing worse than the unsupervised parser on sentences with four entities. It becomes non-trivial for it to isolate which entities and relations should be used for prediction. This seems to indicate that the unsupervised grammar is capturing more useful syntactic/semantic information than what is available from the words alone. Ensemble systems that incorporate syntax and a Bag-of-Words baseline may yield even better performance."}, {"heading": "5.4 The Benefits of Annotation", "text": "The performance of SEMI-SUPERVISED-POS and SEMI-SUPERVISED-WORD suggests that when resources are scarce, it is beneficial to create a even a small lexicon of CCG categories. We analyze this further in Figure 3. Here we show how performance changes as a function of the number of labeled lexical types. Our values range from 0 to 1000 lexical types. We see syntactic improvements of 16pts and seman-\nTable 1 Annotated Words Syntax Semantics 0 37.1 37.3 100 48.49 40.9 200 53.5 43.2 500 53.36 42.4\n1000 49.87 42.4\n1\ntic gains of 6pts with 200 words, before performance degrades. It is possible that increasing annotation may only benefit fully supervised models. Finally, when computing the most frequent lexical types we excluded commas. We found a 3pt performance drop when restricting commas to the category , (they are commonly conj in our data). Additional in-domain knowledge might further improve performance."}, {"heading": "5.5 Common Errors", "text": "Bisk and Hockenmaier (2015) performed an in-depth analysis of the types of categories learned and correctly used by their models (the same models as this paper). Their analysis was based on syntactic evaluation against CCGbank. In particular, they found the most egregious \u201csemantic\u201d errors to be the misuse of verb chains, possessives and PP attachment (bottom of Table 3). Since we now have access to a purely semantic evaluation, we can therefore ask whether these errors exist here, and how common they are. We do this analysis in two steps. First, we manually analyzed parses for which the unsupervised model failed to predict the correct semantics, but where the supervised parser succeeded. The top of Table 3 presents several of the most common reasons for failure. These mistakes were more mundane (e.g. incorrect use of a conjunction) than failures to use complex CCG categories or analyze attachments.\nSecond, we can compare grammatical decisions made by the semi-supervised and unsupervised parsers against EasyCCG on sentences they successfully grounded. Bisk and Hockenmaier (2015) found that their unsupervised parser made mistakes on many very simple categories. We found the same\nresult. When evaluating our parsers against the treebank we found the unsupervised model only correctly predicted transitive verbs 20% of the time and adverbs 39% of the time. In contrast, on our data, we produced the correct transitive category (according to EasyCCG) 65% of the time, and the correct adverb 68% of the time. These correct parsing decisions also lead to improved performance across many other categories (e.g. prepositions). This is likely due to our corpus containing simpler constructions. In contrast, auxiliary verbs, relative clauses, and commas still proved difficult or harder than in the treebank. This implies that future work should tailor the annotation effort to their specific domain rather than relying on guidance solely from the treebank."}, {"heading": "6 Conclusion", "text": "Our goal in this paper was to present the first semantic evaluation of induced grammars in order to better understand their utility and strengths. We showed that induced grammars are learning more semantically useful structure than a Bag-of-Words model. Furthermore, we showed how minimal syntactic supervision can provide substantial gains in semantic evaluation. Our ongoing work explores creating a syntax-semantics loop where each benefits the other with no human (annotation) in the loop."}, {"heading": "Acknowledgments", "text": "This paper is partly based on work that was done when the first and second authors were interns at Google, and on work that that was supported by NSF grant 1053856 to JH, and a Google PhD Fellowship to SR."}], "references": [{"title": "Semantic parsing on Freebase from question-answer pairs", "author": ["Jonathan Berant", "Andrew Chou", "Roy Frostig", "Percy Liang."], "venue": "Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 1533\u20131544, Seattle, Washington,", "citeRegEx": "Berant et al\\.,? 2013", "shortCiteRegEx": "Berant et al\\.", "year": 2013}, {"title": "An HDP Model for Inducing Combinatory Categorial Grammars", "author": ["Yonatan Bisk", "Julia Hockenmaier."], "venue": "Transactions of the Association for Computational Linguistics, pages 75\u201388.", "citeRegEx": "Bisk and Hockenmaier.,? 2013", "shortCiteRegEx": "Bisk and Hockenmaier.", "year": 2013}, {"title": "Probing the linguistic strengths and limitations of unsupervised grammar induction", "author": ["Yonatan Bisk", "Julia Hockenmaier."], "venue": "Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics, Beijing,China, July.", "citeRegEx": "Bisk and Hockenmaier.,? 2015", "shortCiteRegEx": "Bisk and Hockenmaier.", "year": 2015}, {"title": "Wide-coverage semantic representations from a CCG parser", "author": ["Johan Bos", "Stephen Clark", "Mark Steedman", "James R Curran", "Julia Hockenmaier."], "venue": "Proceedings of the 20th international conference on Computational Linguistics, page 1240.", "citeRegEx": "Bos et al\\.,? 2004", "shortCiteRegEx": "Bos et al\\.", "year": 2004}, {"title": "Semantic parsing freebase: Towards open-domain semantic parsing", "author": ["Qingqing Cai", "Alexander Yates."], "venue": "Second Joint Conference on Lexical and Computational Semantics (*SEM), Volume 1: Proceedings of the Main Conference and the Shared Task: Semantic", "citeRegEx": "Cai and Yates.,? 2013", "shortCiteRegEx": "Cai and Yates.", "year": 2013}, {"title": "Two Experiments on Learning Probabilistic Dependency Grammars from Corpora", "author": ["Glenn Carroll", "Eugene Charniak."], "venue": "Working Notes of the Workshop Statistically-Based NLP Techniques, pages 1\u201315, March.", "citeRegEx": "Carroll and Charniak.,? 1992", "shortCiteRegEx": "Carroll and Charniak.", "year": 1992}, {"title": "FACC1: Freebase annotation of ClueWeb corpora, Version 1 (Release date 2013-06-26, Format version 1, Correction level", "author": ["Evgeniy Gabrilovich", "Michael Ringgaard", "Amarnag Subramanya"], "venue": null, "citeRegEx": "Gabrilovich et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Gabrilovich et al\\.", "year": 2013}, {"title": "Weakly-Supervised Grammar-Informed Bayesian CCG Parser Learning", "author": ["Dan Garrette", "Chris Dyer", "Jason Baldridge", "Noah A Smith."], "venue": "Proceedings of the Association for the Advancement of Artificial Intelligence.", "citeRegEx": "Garrette et al\\.,? 2015", "shortCiteRegEx": "Garrette et al\\.", "year": 2015}, {"title": "Teaching machines to read and comprehend", "author": ["Karl Moritz Hermann", "Tomas Kocisky", "Edward Grefenstette", "Lasse Espeholt", "Will Kay", "Mustafa Suleyman", "Phil Blunsom."], "venue": "Advances in Neural Information Processing Systems 28, pages 1693\u20131701.", "citeRegEx": "Hermann et al\\.,? 2015", "shortCiteRegEx": "Hermann et al\\.", "year": 2015}, {"title": "CorpusBased Induction of Syntactic Structure: Models of Dependency and Constituency", "author": ["Dan Klein", "Christopher D Manning."], "venue": "Proceedings of the 42nd Meeting of the Association for Computational Linguistics (ACL\u201904), Main Volume, pages 478\u2013485,", "citeRegEx": "Klein and Manning.,? 2004", "shortCiteRegEx": "Klein and Manning.", "year": 2004}, {"title": "A* CCG Parsing with a Supertag-factored Model", "author": ["Mike Lewis", "Mark Steedman."], "venue": "Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 990\u20131000, Doha, Qatar, October.", "citeRegEx": "Lewis and Steedman.,? 2014", "shortCiteRegEx": "Lewis and Steedman.", "year": 2014}, {"title": "Large-scale Semantic Parsing without QuestionAnswer Pairs", "author": ["Siva Reddy", "Mirella Lapata", "Mark Steedman."], "venue": "Transactions of the Association for Computational Linguistics, pages 1\u201316, June.", "citeRegEx": "Reddy et al\\.,? 2014", "shortCiteRegEx": "Reddy et al\\.", "year": 2014}, {"title": "Transforming Dependency Structures to Logical Forms for Semantic Parsing", "author": ["Siva Reddy", "Oscar T\u00e4ckstr\u00f6m", "Michael Collins", "Tom Kwiatkowski", "Dipanjan Das", "Mark Steedman", "Mirella Lapata."], "venue": "Transactions of the Association for Computational Lin-", "citeRegEx": "Reddy et al\\.,? 2016", "shortCiteRegEx": "Reddy et al\\.", "year": 2016}, {"title": "From Baby Steps to Leapfrog: How \u201cLess is More\u201d in Unsupervised Dependency Parsing", "author": ["Valentin I Spitkovsky", "Hiyan Alshawi", "Daniel Jurafsky."], "venue": "Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the Association", "citeRegEx": "Spitkovsky et al\\.,? 2010", "shortCiteRegEx": "Spitkovsky et al\\.", "year": 2010}, {"title": "The Syntactic Process", "author": ["Mark Steedman."], "venue": "The MIT Press, September.", "citeRegEx": "Steedman.,? 2000", "shortCiteRegEx": "Steedman.", "year": 2000}, {"title": "Discovery of Linguistic Relations Using Lexical Attraction", "author": ["Deniz Yuret."], "venue": "Ph.D. thesis, Massachusetts Institute of Technology.", "citeRegEx": "Yuret.,? 1998", "shortCiteRegEx": "Yuret.", "year": 1998}, {"title": "Computational approaches to sentence completion", "author": ["Geoffrey Zweig", "John C. Platt", "Christopher Meek", "Christopher J.C. Burges", "Ainur Yessenalina", "Qiang Liu."], "venue": "Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics (Volume 1:", "citeRegEx": "Zweig et al\\.,? 2012", "shortCiteRegEx": "Zweig et al\\.", "year": 2012}], "referenceMentions": [{"referenceID": 5, "context": "The past several years have seen significant progress in unsupervised grammar induction (Carroll and Charniak, 1992; Yuret, 1998; Klein and Manning, 2004; Spitkovsky et al., 2010; Garrette et al., 2015; Bisk and Hockenmaier, 2015).", "startOffset": 88, "endOffset": 230}, {"referenceID": 15, "context": "The past several years have seen significant progress in unsupervised grammar induction (Carroll and Charniak, 1992; Yuret, 1998; Klein and Manning, 2004; Spitkovsky et al., 2010; Garrette et al., 2015; Bisk and Hockenmaier, 2015).", "startOffset": 88, "endOffset": 230}, {"referenceID": 9, "context": "The past several years have seen significant progress in unsupervised grammar induction (Carroll and Charniak, 1992; Yuret, 1998; Klein and Manning, 2004; Spitkovsky et al., 2010; Garrette et al., 2015; Bisk and Hockenmaier, 2015).", "startOffset": 88, "endOffset": 230}, {"referenceID": 13, "context": "The past several years have seen significant progress in unsupervised grammar induction (Carroll and Charniak, 1992; Yuret, 1998; Klein and Manning, 2004; Spitkovsky et al., 2010; Garrette et al., 2015; Bisk and Hockenmaier, 2015).", "startOffset": 88, "endOffset": 230}, {"referenceID": 7, "context": "The past several years have seen significant progress in unsupervised grammar induction (Carroll and Charniak, 1992; Yuret, 1998; Klein and Manning, 2004; Spitkovsky et al., 2010; Garrette et al., 2015; Bisk and Hockenmaier, 2015).", "startOffset": 88, "endOffset": 230}, {"referenceID": 2, "context": "The past several years have seen significant progress in unsupervised grammar induction (Carroll and Charniak, 1992; Yuret, 1998; Klein and Manning, 2004; Spitkovsky et al., 2010; Garrette et al., 2015; Bisk and Hockenmaier, 2015).", "startOffset": 88, "endOffset": 230}, {"referenceID": 14, "context": "CCG (Steedman, 2000) is a lexicalized formalism in which words are assigned syntactic types, also known as supertags, encoding subcategorization information.", "startOffset": 4, "endOffset": 20}, {"referenceID": 4, "context": "But such datasets do not exist in the Freebase semantic parsing literature (Cai and Yates, 2013; Berant et al., 2013).", "startOffset": 75, "endOffset": 117}, {"referenceID": 0, "context": "But such datasets do not exist in the Freebase semantic parsing literature (Cai and Yates, 2013; Berant et al., 2013).", "startOffset": 75, "endOffset": 117}, {"referenceID": 1, "context": "Please see Bisk and Hockenmaier (2013) for more details.", "startOffset": 11, "endOffset": 39}, {"referenceID": 3, "context": "acquired(e) \u2227 f(x) \u2227 g(y) \u2227 arg1(e, y)\u2227arg2(e, x)), and the logical form for the complete sentence can be constructed by composing word level lambda expressions following the syntactic derivation (Bos et al., 2004).", "startOffset": 196, "endOffset": 214}, {"referenceID": 11, "context": "In Figure 2 we show two syntactic derivations for the same sentence, and the corresponding logical forms and equivalent graph representations derived by GRAPHPARSER (Reddy et al., 2014).", "startOffset": 165, "endOffset": 185}, {"referenceID": 11, "context": "2 Like Reddy et al. (2014), we treat this problem as a graph matching problem.", "startOffset": 7, "endOffset": 27}, {"referenceID": 6, "context": "(2014) from CLUEWEB09 (Gabrilovich et al., 2013) based on the following constraints: 1) There exists at least", "startOffset": 22, "endOffset": 48}, {"referenceID": 10, "context": "Our dataset SPADES (Semantic PArsing of DEclarative Sentences) is constructed from the declarative sentences collected by Reddy et al. (2014) from CLUEWEB09 (Gabrilovich et al.", "startOffset": 122, "endOffset": 142}, {"referenceID": 11, "context": "3 of Reddy et al. (2016) for details.", "startOffset": 5, "endOffset": 25}, {"referenceID": 16, "context": "There has been other recent interest in similar datasets for sentence completion (Zweig et al., 2012) and machine reading (Hermann et al.", "startOffset": 81, "endOffset": 101}, {"referenceID": 8, "context": ", 2012) and machine reading (Hermann et al., 2015), but unlike other corpora our data is tied directly to Freebase and requires the execution of a semantic parse to correctly predict the missing entity.", "startOffset": 28, "endOffset": 50}, {"referenceID": 1, "context": "For the UNSUPERVISED scenario, we use Bisk and Hockenmaier (2015)\u2019s parser which", "startOffset": 38, "endOffset": 66}, {"referenceID": 10, "context": "For the SUPERVISED scenario, we use EasyCCG (Lewis and Steedman, 2014) trained on CCGbank.", "startOffset": 44, "endOffset": 70}, {"referenceID": 1, "context": "Dependency performance is evaluated on both the simplified labeled F1 of Bisk and Hockenmaier (2015) and Undirected Unlabeled F1.", "startOffset": 73, "endOffset": 101}, {"referenceID": 1, "context": "Bisk and Hockenmaier (2015) found that their unsupervised parser made mistakes on many very simple categories.", "startOffset": 0, "endOffset": 28}], "year": 2017, "abstractText": "We compare the effectiveness of four different syntactic CCG parsers for a semantic slotfilling task to explore how much syntactic supervision is required for downstream semantic analysis. This extrinsic, task-based evaluation also provides a unique window into the semantics captured (or missed) by unsupervised grammar induction systems.", "creator": "LaTeX with hyperref package"}}}